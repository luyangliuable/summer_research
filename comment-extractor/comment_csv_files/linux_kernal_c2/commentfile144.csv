 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/locking/lockdep_proc.c: 1
 * kernel/lockdep_proc.c /Users/rubber/linux/kernel/locking/lockdep_proc.c: 3
 * Runtime locking correctness validator /Users/rubber/linux/kernel/locking/lockdep_proc.c: 5
 * Started by Ingo Molnar: /Users/rubber/linux/kernel/locking/lockdep_proc.c: 7
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/locking/lockdep_proc.c: 9
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/locking/lockdep_proc.c: 10
 * Code for /proc/lockdep and /proc/lockdep_stats: /Users/rubber/linux/kernel/locking/lockdep_proc.c: 12
	/* /Users/rubber/linux/kernel/locking/lockdep_proc.c: 269
	 * Total number of dependencies: /Users/rubber/linux/kernel/locking/lockdep_proc.c: 270
	 * /Users/rubber/linux/kernel/locking/lockdep_proc.c: 271
	 * All irq-safe locks may nest inside irq-unsafe locks, /Users/rubber/linux/kernel/locking/lockdep_proc.c: 272
	 * plus all the other known dependencies: /Users/rubber/linux/kernel/locking/lockdep_proc.c: 273
	/* /Users/rubber/linux/kernel/locking/lockdep_proc.c: 352
	 * Zapped classes and lockdep data buffers reuse statistics. /Users/rubber/linux/kernel/locking/lockdep_proc.c: 353
 * sort on absolute number of contentions /Users/rubber/linux/kernel/locking/lockdep_proc.c: 380
 * Debugging code for mutexes /Users/rubber/linux/kernel/locking/mutex-debug.c: 2
 * Started by Ingo Molnar: /Users/rubber/linux/kernel/locking/mutex-debug.c: 4
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/locking/mutex-debug.c: 6
 * lock debugging, locking tree, deadlock detection started by: /Users/rubber/linux/kernel/locking/mutex-debug.c: 8
 *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey /Users/rubber/linux/kernel/locking/mutex-debug.c: 10
 *  Released under the General Public License (GPL). /Users/rubber/linux/kernel/locking/mutex-debug.c: 11
 * Must be called with lock->wait_lock held. /Users/rubber/linux/kernel/locking/mutex-debug.c: 26
	/* /Users/rubber/linux/kernel/locking/mutex-debug.c: 83
	 * Make sure we are not reinitializing a held lock: /Users/rubber/linux/kernel/locking/mutex-debug.c: 84
 * mutex_destroy - mark a mutex unusable /Users/rubber/linux/kernel/locking/mutex-debug.c: 93
 * @lock: the mutex to be destroyed /Users/rubber/linux/kernel/locking/mutex-debug.c: 94
 * This function marks the mutex uninitialized, and any subsequent /Users/rubber/linux/kernel/locking/mutex-debug.c: 96
 * use of the mutex is forbidden. The mutex must not be locked when /Users/rubber/linux/kernel/locking/mutex-debug.c: 97
 * this function is called. /Users/rubber/linux/kernel/locking/mutex-debug.c: 98
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/locking/spinlock_rt.c: 1
 * PREEMPT_RT substitution for spin/rw_locks /Users/rubber/linux/kernel/locking/spinlock_rt.c: 3
 * spinlocks and rwlocks on RT are based on rtmutexes, with a few twists to /Users/rubber/linux/kernel/locking/spinlock_rt.c: 5
 * resemble the non RT semantics: /Users/rubber/linux/kernel/locking/spinlock_rt.c: 6
 * - Contrary to plain rtmutexes, spinlocks and rwlocks are state /Users/rubber/linux/kernel/locking/spinlock_rt.c: 8
 *   preserving. The task state is saved before blocking on the underlying /Users/rubber/linux/kernel/locking/spinlock_rt.c: 9
 *   rtmutex, and restored when the lock has been acquired. Regular wakeups /Users/rubber/linux/kernel/locking/spinlock_rt.c: 10
 *   during that time are redirected to the saved state so no wake up is /Users/rubber/linux/kernel/locking/spinlock_rt.c: 11
 *   missed. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 12
 * - Non RT spin/rwlocks disable preemption and eventually interrupts. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 14
 *   Disabling preemption has the side effect of disabling migration and /Users/rubber/linux/kernel/locking/spinlock_rt.c: 15
 *   preventing RCU grace periods. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 16
 *   The RT substitutions explicitly disable migration and take /Users/rubber/linux/kernel/locking/spinlock_rt.c: 18
 *   rcu_read_lock() across the lock held section. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 19
 * __might_resched() skips the state check as rtlocks are state /Users/rubber/linux/kernel/locking/spinlock_rt.c: 28
 * preserving. Take RCU nesting into account as spin/read/write_lock() can /Users/rubber/linux/kernel/locking/spinlock_rt.c: 29
 * legitimately nest into an RCU read side critical section. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 30
 * Wait for the lock to get unlocked: instead of polling for an unlock /Users/rubber/linux/kernel/locking/spinlock_rt.c: 88
 * (like raw spinlocks do), lock and unlock, to force the kernel to /Users/rubber/linux/kernel/locking/spinlock_rt.c: 89
 * schedule if there's contention: /Users/rubber/linux/kernel/locking/spinlock_rt.c: 90
 * RT-specific reader/writer locks /Users/rubber/linux/kernel/locking/spinlock_rt.c: 146
 * The common functions which get wrapped into the rwlock API. /Users/rubber/linux/kernel/locking/spinlock_rt.c: 192
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/kernel/locking/qrwlock.c: 1
 * Queued read/write locks /Users/rubber/linux/kernel/locking/qrwlock.c: 3
 * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P. /Users/rubber/linux/kernel/locking/qrwlock.c: 5
 * Authors: Waiman Long <waiman.long@hp.com> /Users/rubber/linux/kernel/locking/qrwlock.c: 7
 * queued_read_lock_slowpath - acquire read lock of a queue rwlock /Users/rubber/linux/kernel/locking/qrwlock.c: 17
 * @lock: Pointer to queue rwlock structure /Users/rubber/linux/kernel/locking/qrwlock.c: 18
	/* /Users/rubber/linux/kernel/locking/qrwlock.c: 22
	 * Readers come here when they cannot get the lock without waiting /Users/rubber/linux/kernel/locking/qrwlock.c: 23
		/* /Users/rubber/linux/kernel/locking/qrwlock.c: 26
		 * Readers in interrupt context will get the lock immediately /Users/rubber/linux/kernel/locking/qrwlock.c: 27
		 * if the writer is just waiting (not holding the lock yet), /Users/rubber/linux/kernel/locking/qrwlock.c: 28
		 * so spin with ACQUIRE semantics until the lock is available /Users/rubber/linux/kernel/locking/qrwlock.c: 29
		 * without waiting in the queue. /Users/rubber/linux/kernel/locking/qrwlock.c: 30
	/* /Users/rubber/linux/kernel/locking/qrwlock.c: 37
	 * Put the reader into the wait queue /Users/rubber/linux/kernel/locking/qrwlock.c: 38
	/* /Users/rubber/linux/kernel/locking/qrwlock.c: 43
	 * The ACQUIRE semantics of the following spinning code ensure /Users/rubber/linux/kernel/locking/qrwlock.c: 44
	 * that accesses can't leak upwards out of our subsequent critical /Users/rubber/linux/kernel/locking/qrwlock.c: 45
	 * section in the case that the lock is currently held for write. /Users/rubber/linux/kernel/locking/qrwlock.c: 46
	/* /Users/rubber/linux/kernel/locking/qrwlock.c: 50
	 * Signal the next one in queue to become queue head /Users/rubber/linux/kernel/locking/qrwlock.c: 51
 * queued_write_lock_slowpath - acquire write lock of a queue rwlock /Users/rubber/linux/kernel/locking/qrwlock.c: 58
 * @lock : Pointer to queue rwlock structure /Users/rubber/linux/kernel/locking/qrwlock.c: 59
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/locking/rwsem.c: 1
/* kernel/rwsem.c: R/W semaphores, public implementation /Users/rubber/linux/kernel/locking/rwsem.c: 2
 * Written by David Howells (dhowells@redhat.com). /Users/rubber/linux/kernel/locking/rwsem.c: 4
 * Derived from asm-i386/semaphore.h /Users/rubber/linux/kernel/locking/rwsem.c: 5
 * Writer lock-stealing by Alex Shi <alex.shi@intel.com> /Users/rubber/linux/kernel/locking/rwsem.c: 7
 * and Michel Lespinasse <walken@google.com> /Users/rubber/linux/kernel/locking/rwsem.c: 8
 * Optimistic spinning by Tim Chen <tim.c.chen@intel.com> /Users/rubber/linux/kernel/locking/rwsem.c: 10
 * and Davidlohr Bueso <davidlohr@hp.com>. Based on mutexes. /Users/rubber/linux/kernel/locking/rwsem.c: 11
 * Rwsem count bit fields re-definition and rwsem rearchitecture by /Users/rubber/linux/kernel/locking/rwsem.c: 13
 * Waiman Long <longman@redhat.com> and /Users/rubber/linux/kernel/locking/rwsem.c: 14
 * Peter Zijlstra <peterz@infradead.org>. /Users/rubber/linux/kernel/locking/rwsem.c: 15
 * The least significant 2 bits of the owner value has the following /Users/rubber/linux/kernel/locking/rwsem.c: 35
 * meanings when set. /Users/rubber/linux/kernel/locking/rwsem.c: 36
 *  - Bit 0: RWSEM_READER_OWNED - The rwsem is owned by readers /Users/rubber/linux/kernel/locking/rwsem.c: 37
 *  - Bit 1: RWSEM_NONSPINNABLE - Cannot spin on a reader-owned lock /Users/rubber/linux/kernel/locking/rwsem.c: 38
 * When the rwsem is reader-owned and a spinning writer has timed out, /Users/rubber/linux/kernel/locking/rwsem.c: 40
 * the nonspinnable bit will be set to disable optimistic spinning. /Users/rubber/linux/kernel/locking/rwsem.c: 41
 * When a writer acquires a rwsem, it puts its task_struct pointer /Users/rubber/linux/kernel/locking/rwsem.c: 43
 * into the owner field. It is cleared after an unlock. /Users/rubber/linux/kernel/locking/rwsem.c: 44
 * When a reader acquires a rwsem, it will also puts its task_struct /Users/rubber/linux/kernel/locking/rwsem.c: 46
 * pointer into the owner field with the RWSEM_READER_OWNED bit set. /Users/rubber/linux/kernel/locking/rwsem.c: 47
 * On unlock, the owner field will largely be left untouched. So /Users/rubber/linux/kernel/locking/rwsem.c: 48
 * for a free or reader-owned rwsem, the owner value may contain /Users/rubber/linux/kernel/locking/rwsem.c: 49
 * information about the last reader that acquires the rwsem. /Users/rubber/linux/kernel/locking/rwsem.c: 50
 * That information may be helpful in debugging cases where the system /Users/rubber/linux/kernel/locking/rwsem.c: 52
 * seems to hang on a reader owned rwsem especially if only one reader /Users/rubber/linux/kernel/locking/rwsem.c: 53
 * is involved. Ideally we would like to track all the readers that own /Users/rubber/linux/kernel/locking/rwsem.c: 54
 * a rwsem, but the overhead is simply too big. /Users/rubber/linux/kernel/locking/rwsem.c: 55
 * A fast path reader optimistic lock stealing is supported when the rwsem /Users/rubber/linux/kernel/locking/rwsem.c: 57
 * is previously owned by a writer and the following conditions are met: /Users/rubber/linux/kernel/locking/rwsem.c: 58
 *  - rwsem is not currently writer owned /Users/rubber/linux/kernel/locking/rwsem.c: 59
 *  - the handoff isn't set. /Users/rubber/linux/kernel/locking/rwsem.c: 60
 * On 64-bit architectures, the bit definitions of the count are: /Users/rubber/linux/kernel/locking/rwsem.c: 81
 * Bit  0    - writer locked bit /Users/rubber/linux/kernel/locking/rwsem.c: 83
 * Bit  1    - waiters present bit /Users/rubber/linux/kernel/locking/rwsem.c: 84
 * Bit  2    - lock handoff bit /Users/rubber/linux/kernel/locking/rwsem.c: 85
 * Bits 3-7  - reserved /Users/rubber/linux/kernel/locking/rwsem.c: 86
 * Bits 8-62 - 55-bit reader count /Users/rubber/linux/kernel/locking/rwsem.c: 87
 * Bit  63   - read fail bit /Users/rubber/linux/kernel/locking/rwsem.c: 88
 * On 32-bit architectures, the bit definitions of the count are: /Users/rubber/linux/kernel/locking/rwsem.c: 90
 * Bit  0    - writer locked bit /Users/rubber/linux/kernel/locking/rwsem.c: 92
 * Bit  1    - waiters present bit /Users/rubber/linux/kernel/locking/rwsem.c: 93
 * Bit  2    - lock handoff bit /Users/rubber/linux/kernel/locking/rwsem.c: 94
 * Bits 3-7  - reserved /Users/rubber/linux/kernel/locking/rwsem.c: 95
 * Bits 8-30 - 23-bit reader count /Users/rubber/linux/kernel/locking/rwsem.c: 96
 * Bit  31   - read fail bit /Users/rubber/linux/kernel/locking/rwsem.c: 97
 * It is not likely that the most significant bit (read fail bit) will ever /Users/rubber/linux/kernel/locking/rwsem.c: 99
 * be set. This guard bit is still checked anyway in the down_read() fastpath /Users/rubber/linux/kernel/locking/rwsem.c: 100
 * just in case we need to use up more of the reader bits for other purpose /Users/rubber/linux/kernel/locking/rwsem.c: 101
 * in the future. /Users/rubber/linux/kernel/locking/rwsem.c: 102
 * atomic_long_fetch_add() is used to obtain reader lock, whereas /Users/rubber/linux/kernel/locking/rwsem.c: 104
 * atomic_long_cmpxchg() will be used to obtain writer lock. /Users/rubber/linux/kernel/locking/rwsem.c: 105
 * There are three places where the lock handoff bit may be set or cleared. /Users/rubber/linux/kernel/locking/rwsem.c: 107
 * 1) rwsem_mark_wake() for readers. /Users/rubber/linux/kernel/locking/rwsem.c: 108
 * 2) rwsem_try_write_lock() for writers. /Users/rubber/linux/kernel/locking/rwsem.c: 109
 * 3) Error path of rwsem_down_write_slowpath(). /Users/rubber/linux/kernel/locking/rwsem.c: 110
 * For all the above cases, wait_lock will be held. A writer must also /Users/rubber/linux/kernel/locking/rwsem.c: 112
 * be the first one in the wait_list to be eligible for setting the handoff /Users/rubber/linux/kernel/locking/rwsem.c: 113
 * bit. So concurrent setting/clearing of handoff bit is not possible. /Users/rubber/linux/kernel/locking/rwsem.c: 114
 * All writes to owner are protected by WRITE_ONCE() to make sure that /Users/rubber/linux/kernel/locking/rwsem.c: 130
 * store tearing can't happen as optimistic spinners may read and use /Users/rubber/linux/kernel/locking/rwsem.c: 131
 * the owner value concurrently without lock. Read from owner, however, /Users/rubber/linux/kernel/locking/rwsem.c: 132
 * may not need READ_ONCE() as long as the pointer value is only used /Users/rubber/linux/kernel/locking/rwsem.c: 133
 * for comparison and isn't being dereferenced. /Users/rubber/linux/kernel/locking/rwsem.c: 134
 * Test the flags in the owner field. /Users/rubber/linux/kernel/locking/rwsem.c: 147
 * The task_struct pointer of the last owning reader will be left in /Users/rubber/linux/kernel/locking/rwsem.c: 155
 * the owner field. /Users/rubber/linux/kernel/locking/rwsem.c: 156
 * Note that the owner value just indicates the task has owned the rwsem /Users/rubber/linux/kernel/locking/rwsem.c: 158
 * previously, it may not be the real owner or one of the real owners /Users/rubber/linux/kernel/locking/rwsem.c: 159
 * anymore when that field is examined, so take it with a grain of salt. /Users/rubber/linux/kernel/locking/rwsem.c: 160
 * The reader non-spinnable bit is preserved. /Users/rubber/linux/kernel/locking/rwsem.c: 162
 * Return true if the rwsem is owned by a reader. /Users/rubber/linux/kernel/locking/rwsem.c: 179
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 184
	 * Check the count to see if it is write-locked. /Users/rubber/linux/kernel/locking/rwsem.c: 185
 * With CONFIG_DEBUG_RWSEMS configured, it will make sure that if there /Users/rubber/linux/kernel/locking/rwsem.c: 197
 * is a task pointer in owner of a reader-owned rwsem, it will be the /Users/rubber/linux/kernel/locking/rwsem.c: 198
 * real owner or one of the real owners. The only exception is when the /Users/rubber/linux/kernel/locking/rwsem.c: 199
 * unlock is done by up_read_non_owner(). /Users/rubber/linux/kernel/locking/rwsem.c: 200
 * Set the RWSEM_NONSPINNABLE bits if the RWSEM_READER_OWNED flag /Users/rubber/linux/kernel/locking/rwsem.c: 219
 * remains set. Otherwise, the operation will be aborted. /Users/rubber/linux/kernel/locking/rwsem.c: 220
 * Return just the real task structure pointer of the owner /Users/rubber/linux/kernel/locking/rwsem.c: 263
 * Return the real task structure pointer of the owner and the embedded /Users/rubber/linux/kernel/locking/rwsem.c: 272
 * flags in the owner. pflags must be non-NULL. /Users/rubber/linux/kernel/locking/rwsem.c: 273
 * Guide to the rw_semaphore's count field. /Users/rubber/linux/kernel/locking/rwsem.c: 285
 * When the RWSEM_WRITER_LOCKED bit in count is set, the lock is owned /Users/rubber/linux/kernel/locking/rwsem.c: 287
 * by a writer. /Users/rubber/linux/kernel/locking/rwsem.c: 288
 * The lock is owned by readers when /Users/rubber/linux/kernel/locking/rwsem.c: 290
 * (1) the RWSEM_WRITER_LOCKED isn't set in count, /Users/rubber/linux/kernel/locking/rwsem.c: 291
 * (2) some of the reader bits are set in count, and /Users/rubber/linux/kernel/locking/rwsem.c: 292
 * (3) the owner field has RWSEM_READ_OWNED bit set. /Users/rubber/linux/kernel/locking/rwsem.c: 293
 * Having some reader bits set is not enough to guarantee a readers owned /Users/rubber/linux/kernel/locking/rwsem.c: 295
 * lock as the readers may be in the process of backing out from the count /Users/rubber/linux/kernel/locking/rwsem.c: 296
 * and a writer has just released the lock. So another writer may steal /Users/rubber/linux/kernel/locking/rwsem.c: 297
 * the lock immediately after that. /Users/rubber/linux/kernel/locking/rwsem.c: 298
 * Initialize an rwsem: /Users/rubber/linux/kernel/locking/rwsem.c: 302
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 308
	 * Make sure we are not reinitializing a held semaphore: /Users/rubber/linux/kernel/locking/rwsem.c: 309
 * The typical HZ value is either 250 or 1000. So set the minimum waiting /Users/rubber/linux/kernel/locking/rwsem.c: 354
 * time to at least 4ms or 1 jiffy (if it is higher than 4ms) in the wait /Users/rubber/linux/kernel/locking/rwsem.c: 355
 * queue before initiating the handoff protocol. /Users/rubber/linux/kernel/locking/rwsem.c: 356
 * Magic number to batch-wakeup waiting readers, even when writers are /Users/rubber/linux/kernel/locking/rwsem.c: 361
 * also present in the queue. This both limits the amount of work the /Users/rubber/linux/kernel/locking/rwsem.c: 362
 * waking thread must do and also prevents any potential counter overflow, /Users/rubber/linux/kernel/locking/rwsem.c: 363
 * however unlikely. /Users/rubber/linux/kernel/locking/rwsem.c: 364
 * handle the lock release when processes blocked on it that can now run /Users/rubber/linux/kernel/locking/rwsem.c: 369
 * - if we come here from up_xxxx(), then the RWSEM_FLAG_WAITERS bit must /Users/rubber/linux/kernel/locking/rwsem.c: 370
 *   have been set. /Users/rubber/linux/kernel/locking/rwsem.c: 371
 * - there must be someone on the queue /Users/rubber/linux/kernel/locking/rwsem.c: 372
 * - the wait_lock must be held by the caller /Users/rubber/linux/kernel/locking/rwsem.c: 373
 * - tasks are marked for wakeup, the caller must later invoke wake_up_q() /Users/rubber/linux/kernel/locking/rwsem.c: 374
 *   to actually wakeup the blocked task(s) and drop the reference count, /Users/rubber/linux/kernel/locking/rwsem.c: 375
 *   preferably when the wait_lock is released /Users/rubber/linux/kernel/locking/rwsem.c: 376
 * - woken process blocks are discarded from the list after having task zeroed /Users/rubber/linux/kernel/locking/rwsem.c: 377
 * - writers are only marked woken if downgrading is false /Users/rubber/linux/kernel/locking/rwsem.c: 378
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 390
	 * Take a peek at the queue head waiter such that we can determine /Users/rubber/linux/kernel/locking/rwsem.c: 391
	 * the wakeup(s) to perform. /Users/rubber/linux/kernel/locking/rwsem.c: 392
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 398
			 * Mark writer at the front of the queue for wakeup. /Users/rubber/linux/kernel/locking/rwsem.c: 399
			 * Until the task is actually later awoken later by /Users/rubber/linux/kernel/locking/rwsem.c: 400
			 * the caller, other writers are able to steal it. /Users/rubber/linux/kernel/locking/rwsem.c: 401
			 * Readers, on the other hand, will block as they /Users/rubber/linux/kernel/locking/rwsem.c: 402
			 * will notice the queued writer. /Users/rubber/linux/kernel/locking/rwsem.c: 403
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 412
	 * No reader wakeup if there are too many of them already. /Users/rubber/linux/kernel/locking/rwsem.c: 413
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 418
	 * Writers might steal the lock before we grant it to the next reader. /Users/rubber/linux/kernel/locking/rwsem.c: 419
	 * We prefer to do the first reader grant before counting readers /Users/rubber/linux/kernel/locking/rwsem.c: 420
	 * so we can bail out early if a writer stole the lock. /Users/rubber/linux/kernel/locking/rwsem.c: 421
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 429
			 * When we've been waiting "too" long (for writers /Users/rubber/linux/kernel/locking/rwsem.c: 430
			 * to give up the lock), request a HANDOFF to /Users/rubber/linux/kernel/locking/rwsem.c: 431
			 * force the issue. /Users/rubber/linux/kernel/locking/rwsem.c: 432
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 443
		 * Set it to reader-owned to give spinners an early /Users/rubber/linux/kernel/locking/rwsem.c: 444
		 * indication that readers now have the lock. /Users/rubber/linux/kernel/locking/rwsem.c: 445
		 * The reader nonspinnable bit seen at slowpath entry of /Users/rubber/linux/kernel/locking/rwsem.c: 446
		 * the reader is copied over. /Users/rubber/linux/kernel/locking/rwsem.c: 447
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 453
	 * Grant up to MAX_READERS_WAKEUP read locks to all the readers in the /Users/rubber/linux/kernel/locking/rwsem.c: 454
	 * queue. We know that the woken will be at least 1 as we accounted /Users/rubber/linux/kernel/locking/rwsem.c: 455
	 * for above. Note we increment the 'active part' of the count by the /Users/rubber/linux/kernel/locking/rwsem.c: 456
	 * number of readers before waking any processes up. /Users/rubber/linux/kernel/locking/rwsem.c: 457
	 * /Users/rubber/linux/kernel/locking/rwsem.c: 458
	 * This is an adaptation of the phase-fair R/W locks where at the /Users/rubber/linux/kernel/locking/rwsem.c: 459
	 * reader phase (first waiter is a reader), all readers are eligible /Users/rubber/linux/kernel/locking/rwsem.c: 460
	 * to acquire the lock at the same time irrespective of their order /Users/rubber/linux/kernel/locking/rwsem.c: 461
	 * in the queue. The writers acquire the lock according to their /Users/rubber/linux/kernel/locking/rwsem.c: 462
	 * order in the queue. /Users/rubber/linux/kernel/locking/rwsem.c: 463
	 * /Users/rubber/linux/kernel/locking/rwsem.c: 464
	 * We have to do wakeup in 2 passes to prevent the possibility that /Users/rubber/linux/kernel/locking/rwsem.c: 465
	 * the reader count may be decremented before it is incremented. It /Users/rubber/linux/kernel/locking/rwsem.c: 466
	 * is because the to-be-woken waiter may not have slept yet. So it /Users/rubber/linux/kernel/locking/rwsem.c: 467
	 * may see waiter->task got cleared, finish its critical section and /Users/rubber/linux/kernel/locking/rwsem.c: 468
	 * do an unlock before the reader count increment. /Users/rubber/linux/kernel/locking/rwsem.c: 469
	 * /Users/rubber/linux/kernel/locking/rwsem.c: 470
	 * 1) Collect the read-waiters in a separate list, count them and /Users/rubber/linux/kernel/locking/rwsem.c: 471
	 *    fully increment the reader count in rwsem. /Users/rubber/linux/kernel/locking/rwsem.c: 472
	 * 2) For each waiters in the new list, clear waiter->task and /Users/rubber/linux/kernel/locking/rwsem.c: 473
	 *    put them into wake_q to be woken up later. /Users/rubber/linux/kernel/locking/rwsem.c: 474
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 484
		 * Limit # of readers that can be woken up per wakeup call. /Users/rubber/linux/kernel/locking/rwsem.c: 485
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 498
	 * When we've woken a reader, we no longer need to force writers /Users/rubber/linux/kernel/locking/rwsem.c: 499
	 * to give up the lock and we can clear HANDOFF. /Users/rubber/linux/kernel/locking/rwsem.c: 500
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 515
		 * Ensure calling get_task_struct() before setting the reader /Users/rubber/linux/kernel/locking/rwsem.c: 516
		 * waiter to nil such that rwsem_down_read_slowpath() cannot /Users/rubber/linux/kernel/locking/rwsem.c: 517
		 * race with do_exit() by always holding a reference count /Users/rubber/linux/kernel/locking/rwsem.c: 518
		 * to the task to wakeup. /Users/rubber/linux/kernel/locking/rwsem.c: 519
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 522
		 * Ensure issuing the wakeup (either by us or someone else) /Users/rubber/linux/kernel/locking/rwsem.c: 523
		 * after setting the reader waiter to nil. /Users/rubber/linux/kernel/locking/rwsem.c: 524
 * This function must be called with the sem->wait_lock held to prevent /Users/rubber/linux/kernel/locking/rwsem.c: 531
 * race conditions between checking the rwsem wait list and setting the /Users/rubber/linux/kernel/locking/rwsem.c: 532
 * sem->count accordingly. /Users/rubber/linux/kernel/locking/rwsem.c: 533
 * If wstate is WRITER_HANDOFF, it will make sure that either the handoff /Users/rubber/linux/kernel/locking/rwsem.c: 535
 * bit is set or the lock is acquired with handoff bit cleared. /Users/rubber/linux/kernel/locking/rwsem.c: 536
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 568
	 * We have either acquired the lock with handoff bit cleared or /Users/rubber/linux/kernel/locking/rwsem.c: 569
	 * set the handoff bit. /Users/rubber/linux/kernel/locking/rwsem.c: 570
 * The rwsem_spin_on_owner() function returns the following 4 values /Users/rubber/linux/kernel/locking/rwsem.c: 580
 * depending on the lock owner state. /Users/rubber/linux/kernel/locking/rwsem.c: 581
 *   OWNER_NULL  : owner is currently NULL /Users/rubber/linux/kernel/locking/rwsem.c: 582
 *   OWNER_WRITER: when owner changes and is a writer /Users/rubber/linux/kernel/locking/rwsem.c: 583
 *   OWNER_READER: when owner changes and the new owner may be a reader. /Users/rubber/linux/kernel/locking/rwsem.c: 584
 *   OWNER_NONSPINNABLE: /Users/rubber/linux/kernel/locking/rwsem.c: 585
 *		   when optimistic spinning has to stop because either the /Users/rubber/linux/kernel/locking/rwsem.c: 586
 *		   owner stops running, is unknown, or its timeslice has /Users/rubber/linux/kernel/locking/rwsem.c: 587
 *		   been used up. /Users/rubber/linux/kernel/locking/rwsem.c: 588
 * Try to acquire write lock before the writer has been put on wait queue. /Users/rubber/linux/kernel/locking/rwsem.c: 599
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 618
	 * As lock holder preemption issue, we both skip spinning if /Users/rubber/linux/kernel/locking/rwsem.c: 619
	 * task is not on cpu or its cpu is preempted /Users/rubber/linux/kernel/locking/rwsem.c: 620
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 637
	 * Disable preemption is equal to the RCU read-side crital section, /Users/rubber/linux/kernel/locking/rwsem.c: 638
	 * thus the task_strcut structure won't go away. /Users/rubber/linux/kernel/locking/rwsem.c: 639
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 642
	 * Don't check the read-owner as the entry may be stale. /Users/rubber/linux/kernel/locking/rwsem.c: 643
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 683
		 * When a waiting writer set the handoff flag, it may spin /Users/rubber/linux/kernel/locking/rwsem.c: 684
		 * on the owner as well. Once that writer acquires the lock, /Users/rubber/linux/kernel/locking/rwsem.c: 685
		 * we can spin on it. So we don't need to quit even when the /Users/rubber/linux/kernel/locking/rwsem.c: 686
		 * handoff bit is set. /Users/rubber/linux/kernel/locking/rwsem.c: 687
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 695
		 * Ensure we emit the owner->on_cpu, dereference _after_ /Users/rubber/linux/kernel/locking/rwsem.c: 696
		 * checking sem->owner still matches owner, if that fails, /Users/rubber/linux/kernel/locking/rwsem.c: 697
		 * owner might point to free()d memory, if it still matches, /Users/rubber/linux/kernel/locking/rwsem.c: 698
		 * our spinning context already disabled preemption which is /Users/rubber/linux/kernel/locking/rwsem.c: 699
		 * equal to RCU read-side crital section ensures the memory /Users/rubber/linux/kernel/locking/rwsem.c: 700
		 * stays valid. /Users/rubber/linux/kernel/locking/rwsem.c: 701
 * Calculate reader-owned rwsem spinning threshold for writer /Users/rubber/linux/kernel/locking/rwsem.c: 717
 * The more readers own the rwsem, the longer it will take for them to /Users/rubber/linux/kernel/locking/rwsem.c: 719
 * wind down and free the rwsem. So the empirical formula used to /Users/rubber/linux/kernel/locking/rwsem.c: 720
 * determine the actual spinning time limit here is: /Users/rubber/linux/kernel/locking/rwsem.c: 721
 *   Spinning threshold = (10 + nr_readers/2)us /Users/rubber/linux/kernel/locking/rwsem.c: 723
 * The limit is capped to a maximum of 25us (30 readers). This is just /Users/rubber/linux/kernel/locking/rwsem.c: 725
 * a heuristic and is subjected to change in the future. /Users/rubber/linux/kernel/locking/rwsem.c: 726
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 754
	 * Optimistically spin on the owner field and attempt to acquire the /Users/rubber/linux/kernel/locking/rwsem.c: 755
	 * lock whenever the owner changes. Spinning will be stopped when: /Users/rubber/linux/kernel/locking/rwsem.c: 756
	 *  1) the owning writer isn't running; or /Users/rubber/linux/kernel/locking/rwsem.c: 757
	 *  2) readers own the lock and spinning time has exceeded limit. /Users/rubber/linux/kernel/locking/rwsem.c: 758
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 767
		 * Try to acquire the lock /Users/rubber/linux/kernel/locking/rwsem.c: 768
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 775
		 * Time-based reader-owned rwsem optimistic spinning /Users/rubber/linux/kernel/locking/rwsem.c: 776
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 779
			 * Re-initialize rspin_threshold every time when /Users/rubber/linux/kernel/locking/rwsem.c: 780
			 * the owner state changes from non-reader to reader. /Users/rubber/linux/kernel/locking/rwsem.c: 781
			 * This allows a writer to steal the lock in between /Users/rubber/linux/kernel/locking/rwsem.c: 782
			 * 2 reader phases and have the threshold reset at /Users/rubber/linux/kernel/locking/rwsem.c: 783
			 * the beginning of the 2nd reader phase. /Users/rubber/linux/kernel/locking/rwsem.c: 784
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 793
			 * Check time threshold once every 16 iterations to /Users/rubber/linux/kernel/locking/rwsem.c: 794
			 * avoid calling sched_clock() too frequently so /Users/rubber/linux/kernel/locking/rwsem.c: 795
			 * as to reduce the average latency between the times /Users/rubber/linux/kernel/locking/rwsem.c: 796
			 * when the lock becomes free and when the spinner /Users/rubber/linux/kernel/locking/rwsem.c: 797
			 * is ready to do a trylock. /Users/rubber/linux/kernel/locking/rwsem.c: 798
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 807
		 * An RT task cannot do optimistic spinning if it cannot /Users/rubber/linux/kernel/locking/rwsem.c: 808
		 * be sure the lock holder is running or live-lock may /Users/rubber/linux/kernel/locking/rwsem.c: 809
		 * happen if the current task and the lock holder happen /Users/rubber/linux/kernel/locking/rwsem.c: 810
		 * to run in the same CPU. However, aborting optimistic /Users/rubber/linux/kernel/locking/rwsem.c: 811
		 * spinning while a NULL owner is detected may miss some /Users/rubber/linux/kernel/locking/rwsem.c: 812
		 * opportunity where spinning can continue without causing /Users/rubber/linux/kernel/locking/rwsem.c: 813
		 * problem. /Users/rubber/linux/kernel/locking/rwsem.c: 814
		 * /Users/rubber/linux/kernel/locking/rwsem.c: 815
		 * There are 2 possible cases where an RT task may be able /Users/rubber/linux/kernel/locking/rwsem.c: 816
		 * to continue spinning. /Users/rubber/linux/kernel/locking/rwsem.c: 817
		 * /Users/rubber/linux/kernel/locking/rwsem.c: 818
		 * 1) The lock owner is in the process of releasing the /Users/rubber/linux/kernel/locking/rwsem.c: 819
		 *    lock, sem->owner is cleared but the lock has not /Users/rubber/linux/kernel/locking/rwsem.c: 820
		 *    been released yet. /Users/rubber/linux/kernel/locking/rwsem.c: 821
		 * 2) The lock was free and owner cleared, but another /Users/rubber/linux/kernel/locking/rwsem.c: 822
		 *    task just comes in and acquire the lock before /Users/rubber/linux/kernel/locking/rwsem.c: 823
		 *    we try to get it. The new owner may be a spinnable /Users/rubber/linux/kernel/locking/rwsem.c: 824
		 *    writer. /Users/rubber/linux/kernel/locking/rwsem.c: 825
		 * /Users/rubber/linux/kernel/locking/rwsem.c: 826
		 * To take advantage of two scenarios listed above, the RT /Users/rubber/linux/kernel/locking/rwsem.c: 827
		 * task is made to retry one more time to see if it can /Users/rubber/linux/kernel/locking/rwsem.c: 828
		 * acquire the lock or continue spinning on the new owning /Users/rubber/linux/kernel/locking/rwsem.c: 829
		 * writer. Of course, if the time lag is long enough or the /Users/rubber/linux/kernel/locking/rwsem.c: 830
		 * new owner is not a writer or spinnable, the RT task will /Users/rubber/linux/kernel/locking/rwsem.c: 831
		 * quit spinning. /Users/rubber/linux/kernel/locking/rwsem.c: 832
		 * /Users/rubber/linux/kernel/locking/rwsem.c: 833
		 * If the owner is a writer, the need_resched() check is /Users/rubber/linux/kernel/locking/rwsem.c: 834
		 * done inside rwsem_spin_on_owner(). If the owner is not /Users/rubber/linux/kernel/locking/rwsem.c: 835
		 * a writer, need_resched() check needs to be done here. /Users/rubber/linux/kernel/locking/rwsem.c: 836
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 847
		 * The cpu_relax() call is a compiler barrier which forces /Users/rubber/linux/kernel/locking/rwsem.c: 848
		 * everything in this loop to be re-loaded. We don't need /Users/rubber/linux/kernel/locking/rwsem.c: 849
		 * memory barriers as we'll eventually observe the right /Users/rubber/linux/kernel/locking/rwsem.c: 850
		 * values at the cost of a few extra spins. /Users/rubber/linux/kernel/locking/rwsem.c: 851
 * Clear the owner's RWSEM_NONSPINNABLE bit if it is set. This should /Users/rubber/linux/kernel/locking/rwsem.c: 863
 * only be called when the reader count reaches 0. /Users/rubber/linux/kernel/locking/rwsem.c: 864
 * Wait for the read lock to be granted /Users/rubber/linux/kernel/locking/rwsem.c: 893
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 904
	 * To prevent a constant stream of readers from starving a sleeping /Users/rubber/linux/kernel/locking/rwsem.c: 905
	 * waiter, don't attempt optimistic lock stealing if the lock is /Users/rubber/linux/kernel/locking/rwsem.c: 906
	 * currently owned by readers. /Users/rubber/linux/kernel/locking/rwsem.c: 907
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 913
	 * Reader optimistic lock stealing. /Users/rubber/linux/kernel/locking/rwsem.c: 914
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 920
		 * Wake up other readers in the wait queue if it is /Users/rubber/linux/kernel/locking/rwsem.c: 921
		 * the first reader. /Users/rubber/linux/kernel/locking/rwsem.c: 922
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 942
		 * In case the wait queue is empty and the lock isn't owned /Users/rubber/linux/kernel/locking/rwsem.c: 943
		 * by a writer or has the handoff bit set, this reader can /Users/rubber/linux/kernel/locking/rwsem.c: 944
		 * exit the slowpath and return immediately as its /Users/rubber/linux/kernel/locking/rwsem.c: 945
		 * RWSEM_READER_BIAS has already been set in the count. /Users/rubber/linux/kernel/locking/rwsem.c: 946
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 964
	 * If there are no active locks, wake the front queued process(es). /Users/rubber/linux/kernel/locking/rwsem.c: 965
	 * /Users/rubber/linux/kernel/locking/rwsem.c: 966
	 * If there are no writers and we are first in the queue, /Users/rubber/linux/kernel/locking/rwsem.c: 967
	 * wake our own waiter to join the existing active readers ! /Users/rubber/linux/kernel/locking/rwsem.c: 968
 * Wait until we successfully acquire the write lock /Users/rubber/linux/kernel/locking/rwsem.c: 1017
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 1034
	 * Optimistic spinning failed, proceed to the slowpath /Users/rubber/linux/kernel/locking/rwsem.c: 1035
	 * and block until we can acquire the sem. /Users/rubber/linux/kernel/locking/rwsem.c: 1036
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 1053
		 * If there were already threads queued before us and: /Users/rubber/linux/kernel/locking/rwsem.c: 1054
		 *  1) there are no active locks, wake the front /Users/rubber/linux/kernel/locking/rwsem.c: 1055
		 *     queued process(es) as the handoff bit might be set. /Users/rubber/linux/kernel/locking/rwsem.c: 1056
		 *  2) there are no active writers and some readers, the lock /Users/rubber/linux/kernel/locking/rwsem.c: 1057
		 *     must be read owned; so we try to wake any read lock /Users/rubber/linux/kernel/locking/rwsem.c: 1058
		 *     waiters that were queued ahead of us. /Users/rubber/linux/kernel/locking/rwsem.c: 1059
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 1069
			 * We want to minimize wait_lock hold time especially /Users/rubber/linux/kernel/locking/rwsem.c: 1070
			 * when a large number of readers are to be woken up. /Users/rubber/linux/kernel/locking/rwsem.c: 1071
		/* /Users/rubber/linux/kernel/locking/rwsem.c: 1093
		 * After setting the handoff bit and failing to acquire /Users/rubber/linux/kernel/locking/rwsem.c: 1094
		 * the lock, attempt to spin on owner to accelerate lock /Users/rubber/linux/kernel/locking/rwsem.c: 1095
		 * transfer. If the previous owner is a on-cpu writer and it /Users/rubber/linux/kernel/locking/rwsem.c: 1096
		 * has just released the lock, OWNER_NULL will be returned. /Users/rubber/linux/kernel/locking/rwsem.c: 1097
		 * In this case, we attempt to acquire the lock again /Users/rubber/linux/kernel/locking/rwsem.c: 1098
		 * without sleeping. /Users/rubber/linux/kernel/locking/rwsem.c: 1099
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 1120
			 * If HANDOFF bit is set, unconditionally do /Users/rubber/linux/kernel/locking/rwsem.c: 1121
			 * a trylock. /Users/rubber/linux/kernel/locking/rwsem.c: 1122
			/* /Users/rubber/linux/kernel/locking/rwsem.c: 1135
			 * The setting of the handoff bit is deferred /Users/rubber/linux/kernel/locking/rwsem.c: 1136
			 * until rwsem_try_write_lock() is called. /Users/rubber/linux/kernel/locking/rwsem.c: 1137
 * handle waking up a waiter on the semaphore /Users/rubber/linux/kernel/locking/rwsem.c: 1176
 * - up_read/up_write has decremented the active part of count if we come here /Users/rubber/linux/kernel/locking/rwsem.c: 1177
 * downgrade a write lock into a read lock /Users/rubber/linux/kernel/locking/rwsem.c: 1196
 * - caller incremented waiting part of count and discovered it still negative /Users/rubber/linux/kernel/locking/rwsem.c: 1197
 * - just wake up any readers at the front of the queue /Users/rubber/linux/kernel/locking/rwsem.c: 1198
 * lock for reading /Users/rubber/linux/kernel/locking/rwsem.c: 1217
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 1252
	 * Optimize for the case when the rwsem is not locked at all. /Users/rubber/linux/kernel/locking/rwsem.c: 1253
 * lock for writing /Users/rubber/linux/kernel/locking/rwsem.c: 1267
 * unlock after reading /Users/rubber/linux/kernel/locking/rwsem.c: 1296
 * unlock after writing /Users/rubber/linux/kernel/locking/rwsem.c: 1316
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 1323
	 * sem->owner may differ from current if the ownership is transferred /Users/rubber/linux/kernel/locking/rwsem.c: 1324
	 * to an anonymous writer by setting the RWSEM_NONSPINNABLE bits. /Users/rubber/linux/kernel/locking/rwsem.c: 1325
 * downgrade write lock to read lock /Users/rubber/linux/kernel/locking/rwsem.c: 1337
	/* /Users/rubber/linux/kernel/locking/rwsem.c: 1343
	 * When downgrading from exclusive to shared ownership, /Users/rubber/linux/kernel/locking/rwsem.c: 1344
	 * anything inside the write-locked region cannot leak /Users/rubber/linux/kernel/locking/rwsem.c: 1345
	 * into the read side. In contrast, anything in the /Users/rubber/linux/kernel/locking/rwsem.c: 1346
	 * read-locked region is ok to be re-ordered into the /Users/rubber/linux/kernel/locking/rwsem.c: 1347
	 * write side. As such, rely on RELEASE semantics. /Users/rubber/linux/kernel/locking/rwsem.c: 1348
 * lock for reading /Users/rubber/linux/kernel/locking/rwsem.c: 1469
 * trylock for reading -- returns 1 if successful, 0 if contention /Users/rubber/linux/kernel/locking/rwsem.c: 1509
 * lock for writing /Users/rubber/linux/kernel/locking/rwsem.c: 1522
 * lock for writing /Users/rubber/linux/kernel/locking/rwsem.c: 1533
 * trylock for writing -- returns 1 if successful, 0 if contention /Users/rubber/linux/kernel/locking/rwsem.c: 1551
 * release a read lock /Users/rubber/linux/kernel/locking/rwsem.c: 1565
 * release a write lock /Users/rubber/linux/kernel/locking/rwsem.c: 1575
 * downgrade write lock to read lock /Users/rubber/linux/kernel/locking/rwsem.c: 1585
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/cputime.c: 1
 * Simple CPU accounting cgroup controller /Users/rubber/linux/kernel/sched/cputime.c: 3
 * There are no locks covering percpu hardirq/softirq time. /Users/rubber/linux/kernel/sched/cputime.c: 10
 * They are only modified in vtime_account, on corresponding CPU /Users/rubber/linux/kernel/sched/cputime.c: 11
 * with interrupts disabled. So, writes are safe. /Users/rubber/linux/kernel/sched/cputime.c: 12
 * They are read and saved off onto struct rq in update_rq_clock(). /Users/rubber/linux/kernel/sched/cputime.c: 13
 * This may result in other CPU reading this CPU's irq time and can /Users/rubber/linux/kernel/sched/cputime.c: 14
 * race with irq/vtime_account on this CPU. We would either get old /Users/rubber/linux/kernel/sched/cputime.c: 15
 * or new value with a side effect of accounting a slice of irq time to wrong /Users/rubber/linux/kernel/sched/cputime.c: 16
 * task when irq is in progress while we read rq->clock. That is a worthy /Users/rubber/linux/kernel/sched/cputime.c: 17
 * compromise in place of having locks on each irq in account_system_time. /Users/rubber/linux/kernel/sched/cputime.c: 18
 * Called after incrementing preempt_count on {soft,}irq_enter /Users/rubber/linux/kernel/sched/cputime.c: 47
 * and before decrementing preempt_count on {soft,}irq_exit. /Users/rubber/linux/kernel/sched/cputime.c: 48
	/* /Users/rubber/linux/kernel/sched/cputime.c: 65
	 * We do not account for softirq time from ksoftirqd here. /Users/rubber/linux/kernel/sched/cputime.c: 66
	 * We want to continue accounting softirq time to ksoftirqd thread /Users/rubber/linux/kernel/sched/cputime.c: 67
	 * in that case, so as not to confuse scheduler with a special task /Users/rubber/linux/kernel/sched/cputime.c: 68
	 * that do not consume any time, but still wants to run. /Users/rubber/linux/kernel/sched/cputime.c: 69
	/* /Users/rubber/linux/kernel/sched/cputime.c: 102
	 * Since all updates are sure to touch the root cgroup, we /Users/rubber/linux/kernel/sched/cputime.c: 103
	 * get ourselves ahead and touch it first. If the root cgroup /Users/rubber/linux/kernel/sched/cputime.c: 104
	 * is the only cgroup, then nothing else should be necessary. /Users/rubber/linux/kernel/sched/cputime.c: 105
	 * /Users/rubber/linux/kernel/sched/cputime.c: 106
 * Account user CPU time to a process. /Users/rubber/linux/kernel/sched/cputime.c: 114
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 115
 * @cputime: the CPU time spent in user space since the last update /Users/rubber/linux/kernel/sched/cputime.c: 116
 * Account guest CPU time to a process. /Users/rubber/linux/kernel/sched/cputime.c: 136
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 137
 * @cputime: the CPU time spent in virtual machine since the last update /Users/rubber/linux/kernel/sched/cputime.c: 138
 * Account system CPU time to a process and desired cpustat field /Users/rubber/linux/kernel/sched/cputime.c: 160
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 161
 * @cputime: the CPU time spent in kernel space since the last update /Users/rubber/linux/kernel/sched/cputime.c: 162
 * @index: pointer to cpustat field that has to be updated /Users/rubber/linux/kernel/sched/cputime.c: 163
 * Account system CPU time to a process. /Users/rubber/linux/kernel/sched/cputime.c: 180
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 181
 * @hardirq_offset: the offset to subtract from hardirq_count() /Users/rubber/linux/kernel/sched/cputime.c: 182
 * @cputime: the CPU time spent in kernel space since the last update /Users/rubber/linux/kernel/sched/cputime.c: 183
 * Account for involuntary wait time. /Users/rubber/linux/kernel/sched/cputime.c: 205
 * @cputime: the CPU time spent in involuntary wait /Users/rubber/linux/kernel/sched/cputime.c: 206
 * Account for idle time. /Users/rubber/linux/kernel/sched/cputime.c: 216
 * @cputime: the CPU time spent in idle wait /Users/rubber/linux/kernel/sched/cputime.c: 217
 * When a guest is interrupted for a longer amount of time, missed clock /Users/rubber/linux/kernel/sched/cputime.c: 231
 * ticks are not redelivered later. Due to that, this function may on /Users/rubber/linux/kernel/sched/cputime.c: 232
 * occasion account more time than the calling functions think elapsed. /Users/rubber/linux/kernel/sched/cputime.c: 233
 * Account how much elapsed time was spent in steal, irq, or softirq time. /Users/rubber/linux/kernel/sched/cputime.c: 254
 * Accumulate raw cputime values of dead tasks (sig->[us]time) and live /Users/rubber/linux/kernel/sched/cputime.c: 291
 * tasks (sum on group iteration) belonging to @tsk's group. /Users/rubber/linux/kernel/sched/cputime.c: 292
	/* /Users/rubber/linux/kernel/sched/cputime.c: 302
	 * Update current task runtime to account pending time since last /Users/rubber/linux/kernel/sched/cputime.c: 303
	 * scheduler action or thread_group_cputime() call. This thread group /Users/rubber/linux/kernel/sched/cputime.c: 304
	 * might have other running tasks on different CPUs, but updating /Users/rubber/linux/kernel/sched/cputime.c: 305
	 * their runtime can affect syscall performance, so we skip account /Users/rubber/linux/kernel/sched/cputime.c: 306
	 * those pending times and rely only on values updated on tick or /Users/rubber/linux/kernel/sched/cputime.c: 307
	 * other scheduler action. /Users/rubber/linux/kernel/sched/cputime.c: 308
 * Account a tick to a process and cpustat /Users/rubber/linux/kernel/sched/cputime.c: 338
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 339
 * @user_tick: is the tick from userspace /Users/rubber/linux/kernel/sched/cputime.c: 340
 * @rq: the pointer to rq /Users/rubber/linux/kernel/sched/cputime.c: 341
 * Tick demultiplexing follows the order /Users/rubber/linux/kernel/sched/cputime.c: 343
 * - pending hardirq update /Users/rubber/linux/kernel/sched/cputime.c: 344
 * - pending softirq update /Users/rubber/linux/kernel/sched/cputime.c: 345
 * - user_time /Users/rubber/linux/kernel/sched/cputime.c: 346
 * - idle_time /Users/rubber/linux/kernel/sched/cputime.c: 347
 * - system time /Users/rubber/linux/kernel/sched/cputime.c: 348
 *   - check for guest_time /Users/rubber/linux/kernel/sched/cputime.c: 349
 *   - else account as system_time /Users/rubber/linux/kernel/sched/cputime.c: 350
 * Check for hardirq is done both for system and user time as there is /Users/rubber/linux/kernel/sched/cputime.c: 352
 * no timer going off while we are on hardirq and hence we may never get an /Users/rubber/linux/kernel/sched/cputime.c: 353
 * opportunity to update it solely in system time. /Users/rubber/linux/kernel/sched/cputime.c: 354
 * p->stime and friends are only updated on system time and not on irq /Users/rubber/linux/kernel/sched/cputime.c: 355
 * softirq as those do not count in task exec_runtime any more. /Users/rubber/linux/kernel/sched/cputime.c: 356
	/* /Users/rubber/linux/kernel/sched/cputime.c: 363
	 * When returning from idle, many ticks can get accounted at /Users/rubber/linux/kernel/sched/cputime.c: 364
	 * once, including some ticks of steal, irq, and softirq time. /Users/rubber/linux/kernel/sched/cputime.c: 365
	 * Subtract those ticks from the amount of time accounted to /Users/rubber/linux/kernel/sched/cputime.c: 366
	 * idle, or potentially user or system time. Due to rounding, /Users/rubber/linux/kernel/sched/cputime.c: 367
	 * other time can exceed ticks occasionally. /Users/rubber/linux/kernel/sched/cputime.c: 368
		/* /Users/rubber/linux/kernel/sched/cputime.c: 377
		 * ksoftirqd time do not get accounted in cpu_softirq_time. /Users/rubber/linux/kernel/sched/cputime.c: 378
		 * So, we have to handle it separately here. /Users/rubber/linux/kernel/sched/cputime.c: 379
		 * Also, p->stime needs to be updated for ksoftirqd. /Users/rubber/linux/kernel/sched/cputime.c: 380
 * Use precise platform statistics if available: /Users/rubber/linux/kernel/sched/cputime.c: 405
 * Account a single tick of CPU time. /Users/rubber/linux/kernel/sched/cputime.c: 465
 * @p: the process that the CPU time gets accounted to /Users/rubber/linux/kernel/sched/cputime.c: 466
 * @user_tick: indicates if the tick is a user or a system tick /Users/rubber/linux/kernel/sched/cputime.c: 467
 * Account multiple ticks of idle time. /Users/rubber/linux/kernel/sched/cputime.c: 498
 * @ticks: number of stolen ticks /Users/rubber/linux/kernel/sched/cputime.c: 499
 * Adjust tick based cputime random precision against scheduler runtime /Users/rubber/linux/kernel/sched/cputime.c: 521
 * accounting. /Users/rubber/linux/kernel/sched/cputime.c: 522
 * Tick based cputime accounting depend on random scheduling timeslices of a /Users/rubber/linux/kernel/sched/cputime.c: 524
 * task to be interrupted or not by the timer.  Depending on these /Users/rubber/linux/kernel/sched/cputime.c: 525
 * circumstances, the number of these interrupts may be over or /Users/rubber/linux/kernel/sched/cputime.c: 526
 * under-optimistic, matching the real user and system cputime with a variable /Users/rubber/linux/kernel/sched/cputime.c: 527
 * precision. /Users/rubber/linux/kernel/sched/cputime.c: 528
 * Fix this by scaling these tick based values against the total runtime /Users/rubber/linux/kernel/sched/cputime.c: 530
 * accounted by the CFS scheduler. /Users/rubber/linux/kernel/sched/cputime.c: 531
 * This code provides the following guarantees: /Users/rubber/linux/kernel/sched/cputime.c: 533
 *   stime + utime == rtime /Users/rubber/linux/kernel/sched/cputime.c: 535
 *   stime_i+1 >= stime_i, utime_i+1 >= utime_i /Users/rubber/linux/kernel/sched/cputime.c: 536
 * Assuming that rtime_i+1 >= rtime_i. /Users/rubber/linux/kernel/sched/cputime.c: 538
	/* /Users/rubber/linux/kernel/sched/cputime.c: 550
	 * This is possible under two circumstances: /Users/rubber/linux/kernel/sched/cputime.c: 551
	 *  - rtime isn't monotonic after all (a bug); /Users/rubber/linux/kernel/sched/cputime.c: 552
	 *  - we got reordered by the lock. /Users/rubber/linux/kernel/sched/cputime.c: 553
	 * /Users/rubber/linux/kernel/sched/cputime.c: 554
	 * In both cases this acts as a filter such that the rest of the code /Users/rubber/linux/kernel/sched/cputime.c: 555
	 * can assume it is monotonic regardless of anything else. /Users/rubber/linux/kernel/sched/cputime.c: 556
	/* /Users/rubber/linux/kernel/sched/cputime.c: 564
	 * If either stime or utime are 0, assume all runtime is userspace. /Users/rubber/linux/kernel/sched/cputime.c: 565
	 * Once a task gets some ticks, the monotonicity code at 'update:' /Users/rubber/linux/kernel/sched/cputime.c: 566
	 * will ensure things converge to the observed ratio. /Users/rubber/linux/kernel/sched/cputime.c: 567
	/* /Users/rubber/linux/kernel/sched/cputime.c: 582
	 * Make sure stime doesn't go backwards; this preserves monotonicity /Users/rubber/linux/kernel/sched/cputime.c: 583
	 * for utime because rtime is monotonic. /Users/rubber/linux/kernel/sched/cputime.c: 584
	 * /Users/rubber/linux/kernel/sched/cputime.c: 585
	 *  utime_i+1 = rtime_i+1 - stime_i /Users/rubber/linux/kernel/sched/cputime.c: 586
	 *            = rtime_i+1 - (rtime_i - utime_i) /Users/rubber/linux/kernel/sched/cputime.c: 587
	 *            = (rtime_i+1 - rtime_i) + utime_i /Users/rubber/linux/kernel/sched/cputime.c: 588
	 *            >= utime_i /Users/rubber/linux/kernel/sched/cputime.c: 589
	/* /Users/rubber/linux/kernel/sched/cputime.c: 595
	 * Make sure utime doesn't go backwards; this still preserves /Users/rubber/linux/kernel/sched/cputime.c: 596
	 * monotonicity for stime, analogous argument to above. /Users/rubber/linux/kernel/sched/cputime.c: 597
	/* /Users/rubber/linux/kernel/sched/cputime.c: 649
	 * Unlike tick based timing, vtime based timing never has lost /Users/rubber/linux/kernel/sched/cputime.c: 650
	 * ticks, and no need for steal time accounting to make up for /Users/rubber/linux/kernel/sched/cputime.c: 651
	 * lost ticks. Vtime accounts a rounded version of actual /Users/rubber/linux/kernel/sched/cputime.c: 652
	 * elapsed time. Limit account_other_time to prevent rounding /Users/rubber/linux/kernel/sched/cputime.c: 653
	 * errors from causing elapsed vtime to go negative. /Users/rubber/linux/kernel/sched/cputime.c: 654
	/* /Users/rubber/linux/kernel/sched/cputime.c: 732
	 * The flags must be updated under the lock with /Users/rubber/linux/kernel/sched/cputime.c: 733
	 * the vtime_starttime flush and update. /Users/rubber/linux/kernel/sched/cputime.c: 734
	 * That enforces a right ordering and update sequence /Users/rubber/linux/kernel/sched/cputime.c: 735
	 * synchronization against the reader (task_gtime()) /Users/rubber/linux/kernel/sched/cputime.c: 736
	 * that can thus safely catch up with a tickless delta. /Users/rubber/linux/kernel/sched/cputime.c: 737
 * Fetch cputime raw values from fields of task_struct and /Users/rubber/linux/kernel/sched/cputime.c: 827
 * add up the pending nohz execution time since the last /Users/rubber/linux/kernel/sched/cputime.c: 828
 * cputime snapshot. /Users/rubber/linux/kernel/sched/cputime.c: 829
		/* /Users/rubber/linux/kernel/sched/cputime.c: 855
		 * Task runs either in user (including guest) or kernel space, /Users/rubber/linux/kernel/sched/cputime.c: 856
		 * add pending nohz time to the right place. /Users/rubber/linux/kernel/sched/cputime.c: 857
	/* /Users/rubber/linux/kernel/sched/cputime.c: 870
	 * We raced against a context switch, fetch the /Users/rubber/linux/kernel/sched/cputime.c: 871
	 * kcpustat task again. /Users/rubber/linux/kernel/sched/cputime.c: 872
	/* /Users/rubber/linux/kernel/sched/cputime.c: 877
	 * Two possible things here: /Users/rubber/linux/kernel/sched/cputime.c: 878
	 * 1) We are seeing the scheduling out task (prev) or any past one. /Users/rubber/linux/kernel/sched/cputime.c: 879
	 * 2) We are seeing the scheduling in task (next) but it hasn't /Users/rubber/linux/kernel/sched/cputime.c: 880
	 *    passed though vtime_task_switch() yet so the pending /Users/rubber/linux/kernel/sched/cputime.c: 881
	 *    cputime of the prev task may not be flushed yet. /Users/rubber/linux/kernel/sched/cputime.c: 882
	 * /Users/rubber/linux/kernel/sched/cputime.c: 883
	 * Case 1) is ok but 2) is not. So wait for a safe VTIME state. /Users/rubber/linux/kernel/sched/cputime.c: 884
		/* /Users/rubber/linux/kernel/sched/cputime.c: 920
		 * Nice VS unnice cputime accounting may be inaccurate if /Users/rubber/linux/kernel/sched/cputime.c: 921
		 * the nice value has changed since the last vtime update. /Users/rubber/linux/kernel/sched/cputime.c: 922
		 * But proper fix would involve interrupting target on nice /Users/rubber/linux/kernel/sched/cputime.c: 923
		 * updates which is a no go on nohz_full (although the scheduler /Users/rubber/linux/kernel/sched/cputime.c: 924
		 * may still interrupt the target if rescheduling is needed...) /Users/rubber/linux/kernel/sched/cputime.c: 925
		/* /Users/rubber/linux/kernel/sched/cputime.c: 1017
		 * Task runs either in user (including guest) or kernel space, /Users/rubber/linux/kernel/sched/cputime.c: 1018
		 * add pending nohz time to the right place. /Users/rubber/linux/kernel/sched/cputime.c: 1019
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/core_sched.c: 1
 * A simple wrapper around refcount. An allocated sched_core_cookie's /Users/rubber/linux/kernel/sched/core_sched.c: 7
 * address is used to compute the cookie of the task. /Users/rubber/linux/kernel/sched/core_sched.c: 8
 * sched_core_update_cookie - replace the cookie on a task /Users/rubber/linux/kernel/sched/core_sched.c: 47
 * @p: the task to update /Users/rubber/linux/kernel/sched/core_sched.c: 48
 * @cookie: the new cookie /Users/rubber/linux/kernel/sched/core_sched.c: 49
 * Effectively exchange the task cookie; caller is responsible for lifetimes on /Users/rubber/linux/kernel/sched/core_sched.c: 51
 * both ends. /Users/rubber/linux/kernel/sched/core_sched.c: 52
 * Returns: the old cookie /Users/rubber/linux/kernel/sched/core_sched.c: 54
	/* /Users/rubber/linux/kernel/sched/core_sched.c: 66
	 * Since creating a cookie implies sched_core_get(), and we cannot set /Users/rubber/linux/kernel/sched/core_sched.c: 67
	 * a cookie until after we've created it, similarly, we cannot destroy /Users/rubber/linux/kernel/sched/core_sched.c: 68
	 * a cookie until after we've removed it, we must have core scheduling /Users/rubber/linux/kernel/sched/core_sched.c: 69
	 * enabled here. /Users/rubber/linux/kernel/sched/core_sched.c: 70
	/* /Users/rubber/linux/kernel/sched/core_sched.c: 84
	 * If task is currently running, it may not be compatible anymore after /Users/rubber/linux/kernel/sched/core_sched.c: 85
	 * the cookie change, so enter the scheduler on its CPU to schedule it /Users/rubber/linux/kernel/sched/core_sched.c: 86
	 * away. /Users/rubber/linux/kernel/sched/core_sched.c: 87
	/* /Users/rubber/linux/kernel/sched/core_sched.c: 159
	 * Check if this process has the right to modify the specified /Users/rubber/linux/kernel/sched/core_sched.c: 160
	 * process. Use the regular "ptrace_may_access()" checks. /Users/rubber/linux/kernel/sched/core_sched.c: 161
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/stop_task.c: 1
 * stop-task scheduling class. /Users/rubber/linux/kernel/sched/stop_task.c: 3
 * The stop task is the highest priority task in the system, it preempts /Users/rubber/linux/kernel/sched/stop_task.c: 5
 * everything and will be preempted by nothing. /Users/rubber/linux/kernel/sched/stop_task.c: 6
 * See kernel/stop_machine.c /Users/rubber/linux/kernel/sched/stop_task.c: 8
 * scheduler tick hitting a task of our scheduling class. /Users/rubber/linux/kernel/sched/stop_task.c: 92
 * NOTE: This function can be called remotely by the tick offload that /Users/rubber/linux/kernel/sched/stop_task.c: 94
 * goes along full dynticks. Therefore no local assumption can be made /Users/rubber/linux/kernel/sched/stop_task.c: 95
 * and everything must be accessed through the @rq and @curr passed in /Users/rubber/linux/kernel/sched/stop_task.c: 96
 * parameters. /Users/rubber/linux/kernel/sched/stop_task.c: 97
 * Simple, special scheduling class for the per-CPU stop tasks: /Users/rubber/linux/kernel/sched/stop_task.c: 119
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/deadline.c: 1
 * Deadline Scheduling Class (SCHED_DEADLINE) /Users/rubber/linux/kernel/sched/deadline.c: 3
 * Earliest Deadline First (EDF) + Constant Bandwidth Server (CBS). /Users/rubber/linux/kernel/sched/deadline.c: 5
 * Tasks that periodically executes their instances for less than their /Users/rubber/linux/kernel/sched/deadline.c: 7
 * runtime won't miss any of their deadlines. /Users/rubber/linux/kernel/sched/deadline.c: 8
 * Tasks that are not periodic or sporadic or that tries to execute more /Users/rubber/linux/kernel/sched/deadline.c: 9
 * than their reserved bandwidth will be slowed down (and may potentially /Users/rubber/linux/kernel/sched/deadline.c: 10
 * miss some of their deadlines), and won't affect any other task. /Users/rubber/linux/kernel/sched/deadline.c: 11
 * Copyright (C) 2012 Dario Faggioli <raistlin@linux.it>, /Users/rubber/linux/kernel/sched/deadline.c: 13
 *                    Juri Lelli <juri.lelli@gmail.com>, /Users/rubber/linux/kernel/sched/deadline.c: 14
 *                    Michael Trimarchi <michael@amarulasolutions.com>, /Users/rubber/linux/kernel/sched/deadline.c: 15
 *                    Fabio Checconi <fchecconi@gmail.com> /Users/rubber/linux/kernel/sched/deadline.c: 16
 * XXX Fix: If 'rq->rd == def_root_domain' perform AC against capacity /Users/rubber/linux/kernel/sched/deadline.c: 110
 * of the CPU the task is running on rather rd's \Sum CPU capacity. /Users/rubber/linux/kernel/sched/deadline.c: 111
		/* /Users/rubber/linux/kernel/sched/deadline.c: 246
		 * If the timer handler is currently running and the /Users/rubber/linux/kernel/sched/deadline.c: 247
		 * timer cannot be canceled, inactive_task_timer() /Users/rubber/linux/kernel/sched/deadline.c: 248
		 * will see that dl_not_contending is not set, and /Users/rubber/linux/kernel/sched/deadline.c: 249
		 * will not touch the rq's active utilization, /Users/rubber/linux/kernel/sched/deadline.c: 250
		 * so we are still safe. /Users/rubber/linux/kernel/sched/deadline.c: 251
 * The utilization of a task cannot be immediately removed from /Users/rubber/linux/kernel/sched/deadline.c: 261
 * the rq active utilization (running_bw) when the task blocks. /Users/rubber/linux/kernel/sched/deadline.c: 262
 * Instead, we have to wait for the so called "0-lag time". /Users/rubber/linux/kernel/sched/deadline.c: 263
 * If a task blocks before the "0-lag time", a timer (the inactive /Users/rubber/linux/kernel/sched/deadline.c: 265
 * timer) is armed, and running_bw is decreased when the timer /Users/rubber/linux/kernel/sched/deadline.c: 266
 * fires. /Users/rubber/linux/kernel/sched/deadline.c: 267
 * If the task wakes up again before the inactive timer fires, /Users/rubber/linux/kernel/sched/deadline.c: 269
 * the timer is canceled, whereas if the task wakes up after the /Users/rubber/linux/kernel/sched/deadline.c: 270
 * inactive timer fired (and running_bw has been decreased) the /Users/rubber/linux/kernel/sched/deadline.c: 271
 * task's utilization has to be added to running_bw again. /Users/rubber/linux/kernel/sched/deadline.c: 272
 * A flag in the deadline scheduling entity (dl_non_contending) /Users/rubber/linux/kernel/sched/deadline.c: 273
 * is used to avoid race conditions between the inactive timer handler /Users/rubber/linux/kernel/sched/deadline.c: 274
 * and task wakeups. /Users/rubber/linux/kernel/sched/deadline.c: 275
 * The following diagram shows how running_bw is updated. A task is /Users/rubber/linux/kernel/sched/deadline.c: 277
 * "ACTIVE" when its utilization contributes to running_bw; an /Users/rubber/linux/kernel/sched/deadline.c: 278
 * "ACTIVE contending" task is in the TASK_RUNNING state, while an /Users/rubber/linux/kernel/sched/deadline.c: 279
 * "ACTIVE non contending" task is a blocked task for which the "0-lag time" /Users/rubber/linux/kernel/sched/deadline.c: 280
 * has not passed yet. An "INACTIVE" task is a task for which the "0-lag" /Users/rubber/linux/kernel/sched/deadline.c: 281
 * time already passed, which does not contribute to running_bw anymore. /Users/rubber/linux/kernel/sched/deadline.c: 282
 *                              +------------------+ /Users/rubber/linux/kernel/sched/deadline.c: 283
 *             wakeup           |    ACTIVE        | /Users/rubber/linux/kernel/sched/deadline.c: 284
 *          +------------------>+   contending     | /Users/rubber/linux/kernel/sched/deadline.c: 285
 *          | add_running_bw    |                  | /Users/rubber/linux/kernel/sched/deadline.c: 286
 *          |                   +----+------+------+ /Users/rubber/linux/kernel/sched/deadline.c: 287
 *          |                        |      ^ /Users/rubber/linux/kernel/sched/deadline.c: 288
 *          |                dequeue |      | /Users/rubber/linux/kernel/sched/deadline.c: 289
 * +--------+-------+                |      | /Users/rubber/linux/kernel/sched/deadline.c: 290
 * |                |   t >= 0-lag   |      | wakeup /Users/rubber/linux/kernel/sched/deadline.c: 291
 * |    INACTIVE    |<---------------+      | /Users/rubber/linux/kernel/sched/deadline.c: 292
 * |                | sub_running_bw |      | /Users/rubber/linux/kernel/sched/deadline.c: 293
 * +--------+-------+                |      | /Users/rubber/linux/kernel/sched/deadline.c: 294
 *          ^                        |      | /Users/rubber/linux/kernel/sched/deadline.c: 295
 *          |              t < 0-lag |      | /Users/rubber/linux/kernel/sched/deadline.c: 296
 *          |                        |      | /Users/rubber/linux/kernel/sched/deadline.c: 297
 *          |                        V      | /Users/rubber/linux/kernel/sched/deadline.c: 298
 *          |                   +----+------+------+ /Users/rubber/linux/kernel/sched/deadline.c: 299
 *          | sub_running_bw    |    ACTIVE        | /Users/rubber/linux/kernel/sched/deadline.c: 300
 *          +-------------------+                  | /Users/rubber/linux/kernel/sched/deadline.c: 301
 *            inactive timer    |  non contending  | /Users/rubber/linux/kernel/sched/deadline.c: 302
 *            fired             +------------------+ /Users/rubber/linux/kernel/sched/deadline.c: 303
 * The task_non_contending() function is invoked when a task /Users/rubber/linux/kernel/sched/deadline.c: 305
 * blocks, and checks if the 0-lag time already passed or /Users/rubber/linux/kernel/sched/deadline.c: 306
 * not (in the first case, it directly updates running_bw; /Users/rubber/linux/kernel/sched/deadline.c: 307
 * in the second case, it arms the inactive timer). /Users/rubber/linux/kernel/sched/deadline.c: 308
 * The task_contending() function is invoked when a task wakes /Users/rubber/linux/kernel/sched/deadline.c: 310
 * up, and checks if the task is still in the "ACTIVE non contending" /Users/rubber/linux/kernel/sched/deadline.c: 311
 * state or not (in the second case, it updates running_bw). /Users/rubber/linux/kernel/sched/deadline.c: 312
	/* /Users/rubber/linux/kernel/sched/deadline.c: 322
	 * If this is a non-deadline task that has been boosted, /Users/rubber/linux/kernel/sched/deadline.c: 323
	 * do nothing /Users/rubber/linux/kernel/sched/deadline.c: 324
	/* /Users/rubber/linux/kernel/sched/deadline.c: 338
	 * Using relative times instead of the absolute "0-lag time" /Users/rubber/linux/kernel/sched/deadline.c: 339
	 * allows to simplify the code /Users/rubber/linux/kernel/sched/deadline.c: 340
	/* /Users/rubber/linux/kernel/sched/deadline.c: 344
	 * If the "0-lag time" already passed, decrease the active /Users/rubber/linux/kernel/sched/deadline.c: 345
	 * utilization now, instead of starting a timer /Users/rubber/linux/kernel/sched/deadline.c: 346
	/* /Users/rubber/linux/kernel/sched/deadline.c: 374
	 * If this is a non-deadline task that has been boosted, /Users/rubber/linux/kernel/sched/deadline.c: 375
	 * do nothing /Users/rubber/linux/kernel/sched/deadline.c: 376
		/* /Users/rubber/linux/kernel/sched/deadline.c: 386
		 * If the timer handler is currently running and the /Users/rubber/linux/kernel/sched/deadline.c: 387
		 * timer cannot be canceled, inactive_task_timer() /Users/rubber/linux/kernel/sched/deadline.c: 388
		 * will see that dl_not_contending is not set, and /Users/rubber/linux/kernel/sched/deadline.c: 389
		 * will not touch the rq's active utilization, /Users/rubber/linux/kernel/sched/deadline.c: 390
		 * so we are still safe. /Users/rubber/linux/kernel/sched/deadline.c: 391
		/* /Users/rubber/linux/kernel/sched/deadline.c: 396
		 * Since "dl_non_contending" is not set, the /Users/rubber/linux/kernel/sched/deadline.c: 397
		 * task's utilization has already been removed from /Users/rubber/linux/kernel/sched/deadline.c: 398
		 * active utilization (either when the task blocked, /Users/rubber/linux/kernel/sched/deadline.c: 399
		 * when the "inactive timer" fired). /Users/rubber/linux/kernel/sched/deadline.c: 400
		 * So, add it back. /Users/rubber/linux/kernel/sched/deadline.c: 401
	/* /Users/rubber/linux/kernel/sched/deadline.c: 468
	 * Must be visible before the overload count is /Users/rubber/linux/kernel/sched/deadline.c: 469
	 * set (as in sched_rt.c). /Users/rubber/linux/kernel/sched/deadline.c: 470
	 * /Users/rubber/linux/kernel/sched/deadline.c: 471
	 * Matched by the barrier in pull_dl_task(). /Users/rubber/linux/kernel/sched/deadline.c: 472
 * The list of pushable -deadline task is not a plist, like in /Users/rubber/linux/kernel/sched/deadline.c: 529
 * sched_rt.c, it is an rb-tree with tasks ordered by deadline. /Users/rubber/linux/kernel/sched/deadline.c: 530
		/* /Users/rubber/linux/kernel/sched/deadline.c: 603
		 * If we cannot preempt any rq, fall back to pick any /Users/rubber/linux/kernel/sched/deadline.c: 604
		 * online CPU: /Users/rubber/linux/kernel/sched/deadline.c: 605
			/* /Users/rubber/linux/kernel/sched/deadline.c: 609
			 * Failed to find any suitable CPU. /Users/rubber/linux/kernel/sched/deadline.c: 610
			 * The task will never come back! /Users/rubber/linux/kernel/sched/deadline.c: 611
			/* /Users/rubber/linux/kernel/sched/deadline.c: 615
			 * If admission control is disabled we /Users/rubber/linux/kernel/sched/deadline.c: 616
			 * try a little harder to let the task /Users/rubber/linux/kernel/sched/deadline.c: 617
			 * run. /Users/rubber/linux/kernel/sched/deadline.c: 618
		/* /Users/rubber/linux/kernel/sched/deadline.c: 627
		 * Inactive timer is armed (or callback is running, but /Users/rubber/linux/kernel/sched/deadline.c: 628
		 * waiting for us to release rq locks). In any case, when it /Users/rubber/linux/kernel/sched/deadline.c: 629
		 * will fire (or continue), it will see running_bw of this /Users/rubber/linux/kernel/sched/deadline.c: 630
		 * task migrated to later_rq (and correctly handle it). /Users/rubber/linux/kernel/sched/deadline.c: 631
	/* /Users/rubber/linux/kernel/sched/deadline.c: 643
	 * And we finally need to fixup root_domain(s) bandwidth accounting, /Users/rubber/linux/kernel/sched/deadline.c: 644
	 * since p is still hanging out in the old (now moved to default) root /Users/rubber/linux/kernel/sched/deadline.c: 645
	 * domain. /Users/rubber/linux/kernel/sched/deadline.c: 646
 * We are being explicitly informed that a new instance is starting, /Users/rubber/linux/kernel/sched/deadline.c: 709
 * and this means that: /Users/rubber/linux/kernel/sched/deadline.c: 710
 *  - the absolute deadline of the entity has to be placed at /Users/rubber/linux/kernel/sched/deadline.c: 711
 *    current time + relative deadline; /Users/rubber/linux/kernel/sched/deadline.c: 712
 *  - the runtime of the entity has to be set to the maximum value. /Users/rubber/linux/kernel/sched/deadline.c: 713
 * The capability of specifying such event is useful whenever a -deadline /Users/rubber/linux/kernel/sched/deadline.c: 715
 * entity wants to (try to!) synchronize its behaviour with the scheduler's /Users/rubber/linux/kernel/sched/deadline.c: 716
 * one, and to (try to!) reconcile itself with its own scheduling /Users/rubber/linux/kernel/sched/deadline.c: 717
 * parameters. /Users/rubber/linux/kernel/sched/deadline.c: 718
	/* /Users/rubber/linux/kernel/sched/deadline.c: 728
	 * We are racing with the deadline timer. So, do nothing because /Users/rubber/linux/kernel/sched/deadline.c: 729
	 * the deadline timer handler will take care of properly recharging /Users/rubber/linux/kernel/sched/deadline.c: 730
	 * the runtime and postponing the deadline /Users/rubber/linux/kernel/sched/deadline.c: 731
	/* /Users/rubber/linux/kernel/sched/deadline.c: 736
	 * We use the regular wall clock time to set deadlines in the /Users/rubber/linux/kernel/sched/deadline.c: 737
	 * future; in fact, we must consider execution overheads (time /Users/rubber/linux/kernel/sched/deadline.c: 738
	 * spent on hardirq context, etc.). /Users/rubber/linux/kernel/sched/deadline.c: 739
 * Pure Earliest Deadline First (EDF) scheduling does not deal with the /Users/rubber/linux/kernel/sched/deadline.c: 746
 * possibility of a entity lasting more than what it declared, and thus /Users/rubber/linux/kernel/sched/deadline.c: 747
 * exhausting its runtime. /Users/rubber/linux/kernel/sched/deadline.c: 748
 * Here we are interested in making runtime overrun possible, but we do /Users/rubber/linux/kernel/sched/deadline.c: 750
 * not want a entity which is misbehaving to affect the scheduling of all /Users/rubber/linux/kernel/sched/deadline.c: 751
 * other entities. /Users/rubber/linux/kernel/sched/deadline.c: 752
 * Therefore, a budgeting strategy called Constant Bandwidth Server (CBS) /Users/rubber/linux/kernel/sched/deadline.c: 753
 * is used, in order to confine each entity within its own bandwidth. /Users/rubber/linux/kernel/sched/deadline.c: 754
 * This function deals exactly with that, and ensures that when the runtime /Users/rubber/linux/kernel/sched/deadline.c: 756
 * of a entity is replenished, its deadline is also postponed. That ensures /Users/rubber/linux/kernel/sched/deadline.c: 757
 * the overrunning entity can't interfere with other entity in the system and /Users/rubber/linux/kernel/sched/deadline.c: 758
 * can't make them miss their deadlines. Reasons why this kind of overruns /Users/rubber/linux/kernel/sched/deadline.c: 759
 * could happen are, typically, a entity voluntarily trying to overcome its /Users/rubber/linux/kernel/sched/deadline.c: 760
 * runtime, or it just underestimated it during sched_setattr(). /Users/rubber/linux/kernel/sched/deadline.c: 761
	/* /Users/rubber/linux/kernel/sched/deadline.c: 770
	 * This could be the case for a !-dl task that is boosted. /Users/rubber/linux/kernel/sched/deadline.c: 771
	 * Just go with full inherited parameters. /Users/rubber/linux/kernel/sched/deadline.c: 772
	/* /Users/rubber/linux/kernel/sched/deadline.c: 782
	 * We keep moving the deadline away until we get some /Users/rubber/linux/kernel/sched/deadline.c: 783
	 * available runtime for the entity. This ensures correct /Users/rubber/linux/kernel/sched/deadline.c: 784
	 * handling of situations where the runtime overrun is /Users/rubber/linux/kernel/sched/deadline.c: 785
	 * arbitrary large. /Users/rubber/linux/kernel/sched/deadline.c: 786
	/* /Users/rubber/linux/kernel/sched/deadline.c: 793
	 * At this point, the deadline really should be "in /Users/rubber/linux/kernel/sched/deadline.c: 794
	 * the future" with respect to rq->clock. If it's /Users/rubber/linux/kernel/sched/deadline.c: 795
	 * not, we are, for some reason, lagging too much! /Users/rubber/linux/kernel/sched/deadline.c: 796
	 * Anyway, after having warn userspace abut that, /Users/rubber/linux/kernel/sched/deadline.c: 797
	 * we still try to keep the things running by /Users/rubber/linux/kernel/sched/deadline.c: 798
	 * resetting the deadline and the budget of the /Users/rubber/linux/kernel/sched/deadline.c: 799
	 * entity. /Users/rubber/linux/kernel/sched/deadline.c: 800
 * Here we check if --at time t-- an entity (which is probably being /Users/rubber/linux/kernel/sched/deadline.c: 815
 * [re]activated or, in general, enqueued) can use its remaining runtime /Users/rubber/linux/kernel/sched/deadline.c: 816
 * and its current deadline _without_ exceeding the bandwidth it is /Users/rubber/linux/kernel/sched/deadline.c: 817
 * assigned (function returns true if it can't). We are in fact applying /Users/rubber/linux/kernel/sched/deadline.c: 818
 * one of the CBS rules: when a task wakes up, if the residual runtime /Users/rubber/linux/kernel/sched/deadline.c: 819
 * over residual deadline fits within the allocated bandwidth, then we /Users/rubber/linux/kernel/sched/deadline.c: 820
 * can keep the current (absolute) deadline and residual budget without /Users/rubber/linux/kernel/sched/deadline.c: 821
 * disrupting the schedulability of the system. Otherwise, we should /Users/rubber/linux/kernel/sched/deadline.c: 822
 * refill the runtime and set the deadline a period in the future, /Users/rubber/linux/kernel/sched/deadline.c: 823
 * because keeping the current (absolute) deadline of the task would /Users/rubber/linux/kernel/sched/deadline.c: 824
 * result in breaking guarantees promised to other tasks (refer to /Users/rubber/linux/kernel/sched/deadline.c: 825
 * Documentation/scheduler/sched-deadline.rst for more information). /Users/rubber/linux/kernel/sched/deadline.c: 826
 * This function returns true if: /Users/rubber/linux/kernel/sched/deadline.c: 828
 *   runtime / (deadline - t) > dl_runtime / dl_deadline , /Users/rubber/linux/kernel/sched/deadline.c: 830
 * IOW we can't recycle current parameters. /Users/rubber/linux/kernel/sched/deadline.c: 832
 * Notice that the bandwidth check is done against the deadline. For /Users/rubber/linux/kernel/sched/deadline.c: 834
 * task with deadline equal to period this is the same of using /Users/rubber/linux/kernel/sched/deadline.c: 835
 * dl_period instead of dl_deadline in the equation above. /Users/rubber/linux/kernel/sched/deadline.c: 836
	/* /Users/rubber/linux/kernel/sched/deadline.c: 842
	 * left and right are the two sides of the equation above, /Users/rubber/linux/kernel/sched/deadline.c: 843
	 * after a bit of shuffling to use multiplications instead /Users/rubber/linux/kernel/sched/deadline.c: 844
	 * of divisions. /Users/rubber/linux/kernel/sched/deadline.c: 845
	 * /Users/rubber/linux/kernel/sched/deadline.c: 846
	 * Note that none of the time values involved in the two /Users/rubber/linux/kernel/sched/deadline.c: 847
	 * multiplications are absolute: dl_deadline and dl_runtime /Users/rubber/linux/kernel/sched/deadline.c: 848
	 * are the relative deadline and the maximum runtime of each /Users/rubber/linux/kernel/sched/deadline.c: 849
	 * instance, runtime is the runtime left for the last instance /Users/rubber/linux/kernel/sched/deadline.c: 850
	 * and (deadline - t), since t is rq->clock, is the time left /Users/rubber/linux/kernel/sched/deadline.c: 851
	 * to the (absolute) deadline. Even if overflowing the u64 type /Users/rubber/linux/kernel/sched/deadline.c: 852
	 * is very unlikely to occur in both cases, here we scale down /Users/rubber/linux/kernel/sched/deadline.c: 853
	 * as we want to avoid that risk at all. Scaling down by 10 /Users/rubber/linux/kernel/sched/deadline.c: 854
	 * means that we reduce granularity to 1us. We are fine with it, /Users/rubber/linux/kernel/sched/deadline.c: 855
	 * since this is only a true/false check and, anyway, thinking /Users/rubber/linux/kernel/sched/deadline.c: 856
	 * of anything below microseconds resolution is actually fiction /Users/rubber/linux/kernel/sched/deadline.c: 857
	 * (but still we want to give the user that illusion >;). /Users/rubber/linux/kernel/sched/deadline.c: 858
 * Revised wakeup rule [1]: For self-suspending tasks, rather then /Users/rubber/linux/kernel/sched/deadline.c: 868
 * re-initializing task's runtime and deadline, the revised wakeup /Users/rubber/linux/kernel/sched/deadline.c: 869
 * rule adjusts the task's runtime to avoid the task to overrun its /Users/rubber/linux/kernel/sched/deadline.c: 870
 * density. /Users/rubber/linux/kernel/sched/deadline.c: 871
 * Reasoning: a task may overrun the density if: /Users/rubber/linux/kernel/sched/deadline.c: 873
 *    runtime / (deadline - t) > dl_runtime / dl_deadline /Users/rubber/linux/kernel/sched/deadline.c: 874
 * Therefore, runtime can be adjusted to: /Users/rubber/linux/kernel/sched/deadline.c: 876
 *     runtime = (dl_runtime / dl_deadline) * (deadline - t) /Users/rubber/linux/kernel/sched/deadline.c: 877
 * In such way that runtime will be equal to the maximum density /Users/rubber/linux/kernel/sched/deadline.c: 879
 * the task can use without breaking any rule. /Users/rubber/linux/kernel/sched/deadline.c: 880
 * [1] Luca Abeni, Giuseppe Lipari, and Juri Lelli. 2015. Constant /Users/rubber/linux/kernel/sched/deadline.c: 882
 * bandwidth server revisited. SIGBED Rev. 11, 4 (January 2015), 19-24. /Users/rubber/linux/kernel/sched/deadline.c: 883
	/* /Users/rubber/linux/kernel/sched/deadline.c: 890
	 * If the task has deadline < period, and the deadline is in the past, /Users/rubber/linux/kernel/sched/deadline.c: 891
	 * it should already be throttled before this check. /Users/rubber/linux/kernel/sched/deadline.c: 892
	 * /Users/rubber/linux/kernel/sched/deadline.c: 893
	 * See update_dl_entity() comments for further details. /Users/rubber/linux/kernel/sched/deadline.c: 894
 * Regarding the deadline, a task with implicit deadline has a relative /Users/rubber/linux/kernel/sched/deadline.c: 902
 * deadline == relative period. A task with constrained deadline has a /Users/rubber/linux/kernel/sched/deadline.c: 903
 * relative deadline <= relative period. /Users/rubber/linux/kernel/sched/deadline.c: 904
 * We support constrained deadline tasks. However, there are some restrictions /Users/rubber/linux/kernel/sched/deadline.c: 906
 * applied only for tasks which do not have an implicit deadline. See /Users/rubber/linux/kernel/sched/deadline.c: 907
 * update_dl_entity() to know more about such restrictions. /Users/rubber/linux/kernel/sched/deadline.c: 908
 * The dl_is_implicit() returns true if the task has an implicit deadline. /Users/rubber/linux/kernel/sched/deadline.c: 910
 * When a deadline entity is placed in the runqueue, its runtime and deadline /Users/rubber/linux/kernel/sched/deadline.c: 918
 * might need to be updated. This is done by a CBS wake up rule. There are two /Users/rubber/linux/kernel/sched/deadline.c: 919
 * different rules: 1) the original CBS; and 2) the Revisited CBS. /Users/rubber/linux/kernel/sched/deadline.c: 920
 * When the task is starting a new period, the Original CBS is used. In this /Users/rubber/linux/kernel/sched/deadline.c: 922
 * case, the runtime is replenished and a new absolute deadline is set. /Users/rubber/linux/kernel/sched/deadline.c: 923
 * When a task is queued before the begin of the next period, using the /Users/rubber/linux/kernel/sched/deadline.c: 925
 * remaining runtime and deadline could make the entity to overflow, see /Users/rubber/linux/kernel/sched/deadline.c: 926
 * dl_entity_overflow() to find more about runtime overflow. When such case /Users/rubber/linux/kernel/sched/deadline.c: 927
 * is detected, the runtime and deadline need to be updated. /Users/rubber/linux/kernel/sched/deadline.c: 928
 * If the task has an implicit deadline, i.e., deadline == period, the Original /Users/rubber/linux/kernel/sched/deadline.c: 930
 * CBS is applied. the runtime is replenished and a new absolute deadline is /Users/rubber/linux/kernel/sched/deadline.c: 931
 * set, as in the previous cases. /Users/rubber/linux/kernel/sched/deadline.c: 932
 * However, the Original CBS does not work properly for tasks with /Users/rubber/linux/kernel/sched/deadline.c: 934
 * deadline < period, which are said to have a constrained deadline. By /Users/rubber/linux/kernel/sched/deadline.c: 935
 * applying the Original CBS, a constrained deadline task would be able to run /Users/rubber/linux/kernel/sched/deadline.c: 936
 * runtime/deadline in a period. With deadline < period, the task would /Users/rubber/linux/kernel/sched/deadline.c: 937
 * overrun the runtime/period allowed bandwidth, breaking the admission test. /Users/rubber/linux/kernel/sched/deadline.c: 938
 * In order to prevent this misbehave, the Revisited CBS is used for /Users/rubber/linux/kernel/sched/deadline.c: 940
 * constrained deadline tasks when a runtime overflow is detected. In the /Users/rubber/linux/kernel/sched/deadline.c: 941
 * Revisited CBS, rather than replenishing & setting a new absolute deadline, /Users/rubber/linux/kernel/sched/deadline.c: 942
 * the remaining runtime of the task is reduced to avoid runtime overflow. /Users/rubber/linux/kernel/sched/deadline.c: 943
 * Please refer to the comments update_dl_revised_wakeup() function to find /Users/rubber/linux/kernel/sched/deadline.c: 944
 * more about the Revised CBS rule. /Users/rubber/linux/kernel/sched/deadline.c: 945
 * If the entity depleted all its runtime, and if we want it to sleep /Users/rubber/linux/kernel/sched/deadline.c: 973
 * while waiting for some new execution time to become available, we /Users/rubber/linux/kernel/sched/deadline.c: 974
 * set the bandwidth replenishment timer to the replenishment instant /Users/rubber/linux/kernel/sched/deadline.c: 975
 * and try to activate it. /Users/rubber/linux/kernel/sched/deadline.c: 976
 * Notice that it is important for the caller to know if the timer /Users/rubber/linux/kernel/sched/deadline.c: 978
 * actually started or not (i.e., the replenishment instant is in /Users/rubber/linux/kernel/sched/deadline.c: 979
 * the future or in the past). /Users/rubber/linux/kernel/sched/deadline.c: 980
	/* /Users/rubber/linux/kernel/sched/deadline.c: 992
	 * We want the timer to fire at the deadline, but considering /Users/rubber/linux/kernel/sched/deadline.c: 993
	 * that it is actually coming from rq->clock and not from /Users/rubber/linux/kernel/sched/deadline.c: 994
	 * hrtimer's time base reading. /Users/rubber/linux/kernel/sched/deadline.c: 995
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1002
	 * If the expiry time already passed, e.g., because the value /Users/rubber/linux/kernel/sched/deadline.c: 1003
	 * chosen as the deadline is too small, don't even try to /Users/rubber/linux/kernel/sched/deadline.c: 1004
	 * start the timer in the past! /Users/rubber/linux/kernel/sched/deadline.c: 1005
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1010
	 * !enqueued will guarantee another callback; even if one is already in /Users/rubber/linux/kernel/sched/deadline.c: 1011
	 * progress. This ensures a balanced {get,put}_task_struct(). /Users/rubber/linux/kernel/sched/deadline.c: 1012
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1013
	 * The race against __run_timer() clearing the enqueued state is /Users/rubber/linux/kernel/sched/deadline.c: 1014
	 * harmless because we're holding task_rq()->lock, therefore the timer /Users/rubber/linux/kernel/sched/deadline.c: 1015
	 * expiring after we've done the check will wait on its task_rq_lock() /Users/rubber/linux/kernel/sched/deadline.c: 1016
	 * and observe our state. /Users/rubber/linux/kernel/sched/deadline.c: 1017
 * This is the bandwidth enforcement timer callback. If here, we know /Users/rubber/linux/kernel/sched/deadline.c: 1028
 * a task is not on its dl_rq, since the fact that the timer was running /Users/rubber/linux/kernel/sched/deadline.c: 1029
 * means the task is throttled and needs a runtime replenishment. /Users/rubber/linux/kernel/sched/deadline.c: 1030
 * However, what we actually do depends on the fact the task is active, /Users/rubber/linux/kernel/sched/deadline.c: 1032
 * (it is on its rq) or has been removed from there by a call to /Users/rubber/linux/kernel/sched/deadline.c: 1033
 * dequeue_task_dl(). In the former case we must issue the runtime /Users/rubber/linux/kernel/sched/deadline.c: 1034
 * replenishment and add the task back to the dl_rq; in the latter, we just /Users/rubber/linux/kernel/sched/deadline.c: 1035
 * do nothing but clearing dl_throttled, so that runtime and deadline /Users/rubber/linux/kernel/sched/deadline.c: 1036
 * updating (and the queueing back to dl_rq) will be done by the /Users/rubber/linux/kernel/sched/deadline.c: 1037
 * next call to enqueue_task_dl(). /Users/rubber/linux/kernel/sched/deadline.c: 1038
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1051
	 * The task might have changed its scheduling policy to something /Users/rubber/linux/kernel/sched/deadline.c: 1052
	 * different than SCHED_DEADLINE (through switched_from_dl()). /Users/rubber/linux/kernel/sched/deadline.c: 1053
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1058
	 * The task might have been boosted by someone else and might be in the /Users/rubber/linux/kernel/sched/deadline.c: 1059
	 * boosting/deboosting path, its not throttled. /Users/rubber/linux/kernel/sched/deadline.c: 1060
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1065
	 * Spurious timer due to start_dl_timer() race; or we already received /Users/rubber/linux/kernel/sched/deadline.c: 1066
	 * a replenishment from rt_mutex_setprio(). /Users/rubber/linux/kernel/sched/deadline.c: 1067
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1075
	 * If the throttle happened during sched-out; like: /Users/rubber/linux/kernel/sched/deadline.c: 1076
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1077
	 *   schedule() /Users/rubber/linux/kernel/sched/deadline.c: 1078
	 *     deactivate_task() /Users/rubber/linux/kernel/sched/deadline.c: 1079
	 *       dequeue_task_dl() /Users/rubber/linux/kernel/sched/deadline.c: 1080
	 *         update_curr_dl() /Users/rubber/linux/kernel/sched/deadline.c: 1081
	 *           start_dl_timer() /Users/rubber/linux/kernel/sched/deadline.c: 1082
	 *         __dequeue_task_dl() /Users/rubber/linux/kernel/sched/deadline.c: 1083
	 *     prev->on_rq = 0; /Users/rubber/linux/kernel/sched/deadline.c: 1084
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1085
	 * We can be both throttled and !queued. Replenish the counter /Users/rubber/linux/kernel/sched/deadline.c: 1086
	 * but do not enqueue -- wait for our wakeup to do that. /Users/rubber/linux/kernel/sched/deadline.c: 1087
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1096
		 * If the runqueue is no longer available, migrate the /Users/rubber/linux/kernel/sched/deadline.c: 1097
		 * task elsewhere. This necessarily changes rq. /Users/rubber/linux/kernel/sched/deadline.c: 1098
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1105
		 * Now that the task has been migrated to the new RQ and we /Users/rubber/linux/kernel/sched/deadline.c: 1106
		 * have that locked, proceed as normal and enqueue the task /Users/rubber/linux/kernel/sched/deadline.c: 1107
		 * there. /Users/rubber/linux/kernel/sched/deadline.c: 1108
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1120
	 * Queueing this task back might have overloaded rq, check if we need /Users/rubber/linux/kernel/sched/deadline.c: 1121
	 * to kick someone away. /Users/rubber/linux/kernel/sched/deadline.c: 1122
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1125
		 * Nothing relies on rq->lock after this, so its safe to drop /Users/rubber/linux/kernel/sched/deadline.c: 1126
		 * rq->lock. /Users/rubber/linux/kernel/sched/deadline.c: 1127
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1138
	 * This can free the task_struct, including this hrtimer, do not touch /Users/rubber/linux/kernel/sched/deadline.c: 1139
	 * anything related to that after this. /Users/rubber/linux/kernel/sched/deadline.c: 1140
 * During the activation, CBS checks if it can reuse the current task's /Users/rubber/linux/kernel/sched/deadline.c: 1156
 * runtime and period. If the deadline of the task is in the past, CBS /Users/rubber/linux/kernel/sched/deadline.c: 1157
 * cannot use the runtime, and so it replenishes the task. This rule /Users/rubber/linux/kernel/sched/deadline.c: 1158
 * works fine for implicit deadline tasks (deadline == period), and the /Users/rubber/linux/kernel/sched/deadline.c: 1159
 * CBS was designed for implicit deadline tasks. However, a task with /Users/rubber/linux/kernel/sched/deadline.c: 1160
 * constrained deadline (deadline < period) might be awakened after the /Users/rubber/linux/kernel/sched/deadline.c: 1161
 * deadline, but before the next period. In this case, replenishing the /Users/rubber/linux/kernel/sched/deadline.c: 1162
 * task would allow it to run for runtime / deadline. As in this case /Users/rubber/linux/kernel/sched/deadline.c: 1163
 * deadline < period, CBS enables a task to run for more than the /Users/rubber/linux/kernel/sched/deadline.c: 1164
 * runtime / period. In a very loaded system, this can cause a domino /Users/rubber/linux/kernel/sched/deadline.c: 1165
 * effect, making other tasks miss their deadlines. /Users/rubber/linux/kernel/sched/deadline.c: 1166
 * To avoid this problem, in the activation of a constrained deadline /Users/rubber/linux/kernel/sched/deadline.c: 1168
 * task after the deadline but before the next period, throttle the /Users/rubber/linux/kernel/sched/deadline.c: 1169
 * task and set the replenishing timer to the begin of the next period, /Users/rubber/linux/kernel/sched/deadline.c: 1170
 * unless it is boosted. /Users/rubber/linux/kernel/sched/deadline.c: 1171
 * This function implements the GRUB accounting rule: /Users/rubber/linux/kernel/sched/deadline.c: 1197
 * according to the GRUB reclaiming algorithm, the runtime is /Users/rubber/linux/kernel/sched/deadline.c: 1198
 * not decreased as "dq = -dt", but as /Users/rubber/linux/kernel/sched/deadline.c: 1199
 * "dq = -max{u / Umax, (1 - Uinact - Uextra)} dt", /Users/rubber/linux/kernel/sched/deadline.c: 1200
 * where u is the utilization of the task, Umax is the maximum reclaimable /Users/rubber/linux/kernel/sched/deadline.c: 1201
 * utilization, Uinact is the (per-runqueue) inactive utilization, computed /Users/rubber/linux/kernel/sched/deadline.c: 1202
 * as the difference between the "total runqueue utilization" and the /Users/rubber/linux/kernel/sched/deadline.c: 1203
 * runqueue active utilization, and Uextra is the (per runqueue) extra /Users/rubber/linux/kernel/sched/deadline.c: 1204
 * reclaimable utilization. /Users/rubber/linux/kernel/sched/deadline.c: 1205
 * Since rq->dl.running_bw and rq->dl.this_bw contain utilizations /Users/rubber/linux/kernel/sched/deadline.c: 1206
 * multiplied by 2^BW_SHIFT, the result has to be shifted right by /Users/rubber/linux/kernel/sched/deadline.c: 1207
 * BW_SHIFT. /Users/rubber/linux/kernel/sched/deadline.c: 1208
 * Since rq->dl.bw_ratio contains 1 / Umax multiplied by 2^RATIO_SHIFT, /Users/rubber/linux/kernel/sched/deadline.c: 1209
 * dl_bw is multiped by rq->dl.bw_ratio and shifted right by RATIO_SHIFT. /Users/rubber/linux/kernel/sched/deadline.c: 1210
 * Since delta is a 64 bit variable, to have an overflow its value /Users/rubber/linux/kernel/sched/deadline.c: 1211
 * should be larger than 2^(64 - 20 - 8), which is more than 64 seconds. /Users/rubber/linux/kernel/sched/deadline.c: 1212
 * So, overflow is not an issue here. /Users/rubber/linux/kernel/sched/deadline.c: 1213
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1221
	 * Instead of computing max{u * bw_ratio, (1 - u_inact - u_extra)}, /Users/rubber/linux/kernel/sched/deadline.c: 1222
	 * we compare u_inact + rq->dl.extra_bw with /Users/rubber/linux/kernel/sched/deadline.c: 1223
	 * 1 - (u * rq->dl.bw_ratio >> RATIO_SHIFT), because /Users/rubber/linux/kernel/sched/deadline.c: 1224
	 * u_inact + rq->dl.extra_bw can be larger than /Users/rubber/linux/kernel/sched/deadline.c: 1225
	 * 1 * (so, 1 - u_inact - rq->dl.extra_bw would be negative /Users/rubber/linux/kernel/sched/deadline.c: 1226
	 * leading to wrong results) /Users/rubber/linux/kernel/sched/deadline.c: 1227
 * Update the current task's runtime statistics (provided it is still /Users/rubber/linux/kernel/sched/deadline.c: 1238
 * a -deadline task and has not been removed from the dl_rq). /Users/rubber/linux/kernel/sched/deadline.c: 1239
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1252
	 * Consumed budget is computed considering the time as /Users/rubber/linux/kernel/sched/deadline.c: 1253
	 * observed by schedulable tasks (excluding time spent /Users/rubber/linux/kernel/sched/deadline.c: 1254
	 * in hardirq context, etc.). Deadlines are instead /Users/rubber/linux/kernel/sched/deadline.c: 1255
	 * computed using hard walltime. This seems to be the more /Users/rubber/linux/kernel/sched/deadline.c: 1256
	 * natural solution, but the full ramifications of this /Users/rubber/linux/kernel/sched/deadline.c: 1257
	 * approach need further study. /Users/rubber/linux/kernel/sched/deadline.c: 1258
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1282
	 * For tasks that participate in GRUB, we implement GRUB-PA: the /Users/rubber/linux/kernel/sched/deadline.c: 1283
	 * spare reclaimed bandwidth is used to clock down frequency. /Users/rubber/linux/kernel/sched/deadline.c: 1284
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1285
	 * For the others, we still need to scale reservation parameters /Users/rubber/linux/kernel/sched/deadline.c: 1286
	 * according to current frequency and CPU maximum capacity. /Users/rubber/linux/kernel/sched/deadline.c: 1287
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1320
	 * Because -- for now -- we share the rt bandwidth, we need to /Users/rubber/linux/kernel/sched/deadline.c: 1321
	 * account our runtime there too, otherwise actual rt tasks /Users/rubber/linux/kernel/sched/deadline.c: 1322
	 * would be able to exceed the shared quota. /Users/rubber/linux/kernel/sched/deadline.c: 1323
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1324
	 * Account to the root rt group for now. /Users/rubber/linux/kernel/sched/deadline.c: 1325
	 * /Users/rubber/linux/kernel/sched/deadline.c: 1326
	 * The solution we're working towards is having the RT groups scheduled /Users/rubber/linux/kernel/sched/deadline.c: 1327
	 * using deadline servers -- however there's a few nasties to figure /Users/rubber/linux/kernel/sched/deadline.c: 1328
	 * out before that can happen. /Users/rubber/linux/kernel/sched/deadline.c: 1329
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1335
		 * We'll let actual RT tasks worry about the overflow here, we /Users/rubber/linux/kernel/sched/deadline.c: 1336
		 * have our own CBS to keep us inline; only account when RT /Users/rubber/linux/kernel/sched/deadline.c: 1337
		 * bandwidth is relevant. /Users/rubber/linux/kernel/sched/deadline.c: 1338
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1415
	 * Since we may have removed our earliest (and/or next earliest) /Users/rubber/linux/kernel/sched/deadline.c: 1416
	 * task we must recompute them. /Users/rubber/linux/kernel/sched/deadline.c: 1417
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1585
	 * If this is a wakeup or a new instance, the scheduling /Users/rubber/linux/kernel/sched/deadline.c: 1586
	 * parameters of the task might need updating. Otherwise, /Users/rubber/linux/kernel/sched/deadline.c: 1587
	 * we want a replenishment of its runtime. /Users/rubber/linux/kernel/sched/deadline.c: 1588
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1612
		 * Because of delays in the detection of the overrun of a /Users/rubber/linux/kernel/sched/deadline.c: 1613
		 * thread's runtime, it might be the case that a thread /Users/rubber/linux/kernel/sched/deadline.c: 1614
		 * goes to sleep in a rt mutex with negative runtime. As /Users/rubber/linux/kernel/sched/deadline.c: 1615
		 * a consequence, the thread will be throttled. /Users/rubber/linux/kernel/sched/deadline.c: 1616
		 * /Users/rubber/linux/kernel/sched/deadline.c: 1617
		 * While waiting for the mutex, this thread can also be /Users/rubber/linux/kernel/sched/deadline.c: 1618
		 * boosted via PI, resulting in a thread that is throttled /Users/rubber/linux/kernel/sched/deadline.c: 1619
		 * and boosted at the same time. /Users/rubber/linux/kernel/sched/deadline.c: 1620
		 * /Users/rubber/linux/kernel/sched/deadline.c: 1621
		 * In this case, the boost overrides the throttle. /Users/rubber/linux/kernel/sched/deadline.c: 1622
			/* /Users/rubber/linux/kernel/sched/deadline.c: 1625
			 * The replenish timer needs to be canceled. No /Users/rubber/linux/kernel/sched/deadline.c: 1626
			 * problem if it fires concurrently: boosted threads /Users/rubber/linux/kernel/sched/deadline.c: 1627
			 * are ignored in dl_task_timer(). /Users/rubber/linux/kernel/sched/deadline.c: 1628
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1634
		 * Special case in which we have a !SCHED_DEADLINE task that is going /Users/rubber/linux/kernel/sched/deadline.c: 1635
		 * to be deboosted, but exceeds its runtime while doing so. No point in /Users/rubber/linux/kernel/sched/deadline.c: 1636
		 * replenishing it, as it's going to return back to its original /Users/rubber/linux/kernel/sched/deadline.c: 1637
		 * scheduling class after this. If it has been throttled, we need to /Users/rubber/linux/kernel/sched/deadline.c: 1638
		 * clear the flag, otherwise the task may wake up as throttled after /Users/rubber/linux/kernel/sched/deadline.c: 1639
		 * being boosted again with no means to replenish the runtime and clear /Users/rubber/linux/kernel/sched/deadline.c: 1640
		 * the throttle. /Users/rubber/linux/kernel/sched/deadline.c: 1641
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1648
	 * Check if a constrained deadline task was activated /Users/rubber/linux/kernel/sched/deadline.c: 1649
	 * after the deadline but before the next period. /Users/rubber/linux/kernel/sched/deadline.c: 1650
	 * If that is the case, the task will be throttled and /Users/rubber/linux/kernel/sched/deadline.c: 1651
	 * the replenishment timer will be set to the next period. /Users/rubber/linux/kernel/sched/deadline.c: 1652
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1662
	 * If p is throttled, we do not enqueue it. In fact, if it exhausted /Users/rubber/linux/kernel/sched/deadline.c: 1663
	 * its budget it needs a replenishment and, since it now is on /Users/rubber/linux/kernel/sched/deadline.c: 1664
	 * its rq, the bandwidth timer callback (which clearly has not /Users/rubber/linux/kernel/sched/deadline.c: 1665
	 * run yet) will take care of this. /Users/rubber/linux/kernel/sched/deadline.c: 1666
	 * However, the active utilization does not depend on the fact /Users/rubber/linux/kernel/sched/deadline.c: 1667
	 * that the task is on the runqueue or not (but depends on the /Users/rubber/linux/kernel/sched/deadline.c: 1668
	 * task's state - in GRUB parlance, "inactive" vs "active contending"). /Users/rubber/linux/kernel/sched/deadline.c: 1669
	 * In other words, even if a task is throttled its utilization must /Users/rubber/linux/kernel/sched/deadline.c: 1670
	 * be counted in the active utilization; hence, we need to call /Users/rubber/linux/kernel/sched/deadline.c: 1671
	 * add_running_bw(). /Users/rubber/linux/kernel/sched/deadline.c: 1672
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1707
	 * This check allows to start the inactive timer (or to immediately /Users/rubber/linux/kernel/sched/deadline.c: 1708
	 * decrease the active utilization, if needed) in two cases: /Users/rubber/linux/kernel/sched/deadline.c: 1709
	 * when the task blocks and when it is terminating /Users/rubber/linux/kernel/sched/deadline.c: 1710
	 * (p->state == TASK_DEAD). We can handle the two cases in the same /Users/rubber/linux/kernel/sched/deadline.c: 1711
	 * way, because from GRUB's point of view the same thing is happening /Users/rubber/linux/kernel/sched/deadline.c: 1712
	 * (the task moves from "active contending" to "active non contending" /Users/rubber/linux/kernel/sched/deadline.c: 1713
	 * or "inactive") /Users/rubber/linux/kernel/sched/deadline.c: 1714
 * Yield task semantic for -deadline tasks is: /Users/rubber/linux/kernel/sched/deadline.c: 1721
 *   get off from the CPU until our next instance, with /Users/rubber/linux/kernel/sched/deadline.c: 1723
 *   a new runtime. This is of little use now, since we /Users/rubber/linux/kernel/sched/deadline.c: 1724
 *   don't have a bandwidth reclaiming mechanism. Anyway, /Users/rubber/linux/kernel/sched/deadline.c: 1725
 *   bandwidth reclaiming is planned for the future, and /Users/rubber/linux/kernel/sched/deadline.c: 1726
 *   yield_task_dl will indicate that some spare budget /Users/rubber/linux/kernel/sched/deadline.c: 1727
 *   is available for other task instances to use it. /Users/rubber/linux/kernel/sched/deadline.c: 1728
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1732
	 * We make the task go to sleep until its current deadline by /Users/rubber/linux/kernel/sched/deadline.c: 1733
	 * forcing its runtime to zero. This way, update_curr_dl() stops /Users/rubber/linux/kernel/sched/deadline.c: 1734
	 * it and the bandwidth timer will wake it up and will give it /Users/rubber/linux/kernel/sched/deadline.c: 1735
	 * new scheduling parameters (thanks to dl_yielded=1). /Users/rubber/linux/kernel/sched/deadline.c: 1736
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1742
	 * Tell update_rq_clock() that we've just updated, /Users/rubber/linux/kernel/sched/deadline.c: 1743
	 * so we don't do microscopic update in schedule() /Users/rubber/linux/kernel/sched/deadline.c: 1744
	 * and double the fastpath cost. /Users/rubber/linux/kernel/sched/deadline.c: 1745
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1769
	 * If we are dealing with a -deadline task, we must /Users/rubber/linux/kernel/sched/deadline.c: 1770
	 * decide where to wake it up. /Users/rubber/linux/kernel/sched/deadline.c: 1771
	 * If it has a later deadline and the current task /Users/rubber/linux/kernel/sched/deadline.c: 1772
	 * on this rq can't move (provided the waking task /Users/rubber/linux/kernel/sched/deadline.c: 1773
	 * can!) we prefer to send it somewhere else. On the /Users/rubber/linux/kernel/sched/deadline.c: 1774
	 * other hand, if it has a shorter deadline, we /Users/rubber/linux/kernel/sched/deadline.c: 1775
	 * try to make it stay here, it might be important. /Users/rubber/linux/kernel/sched/deadline.c: 1776
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1783
	 * Take the capacity of the CPU into account to /Users/rubber/linux/kernel/sched/deadline.c: 1784
	 * ensure it fits the requirement of the task. /Users/rubber/linux/kernel/sched/deadline.c: 1785
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1813
	 * Since p->state == TASK_WAKING, set_task_cpu() has been called /Users/rubber/linux/kernel/sched/deadline.c: 1814
	 * from try_to_wake_up(). Hence, p->pi_lock is locked, but /Users/rubber/linux/kernel/sched/deadline.c: 1815
	 * rq->lock is not... So, lock it /Users/rubber/linux/kernel/sched/deadline.c: 1816
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1823
		 * If the timer handler is currently running and the /Users/rubber/linux/kernel/sched/deadline.c: 1824
		 * timer cannot be canceled, inactive_task_timer() /Users/rubber/linux/kernel/sched/deadline.c: 1825
		 * will see that dl_not_contending is not set, and /Users/rubber/linux/kernel/sched/deadline.c: 1826
		 * will not touch the rq's active utilization, /Users/rubber/linux/kernel/sched/deadline.c: 1827
		 * so we are still safe. /Users/rubber/linux/kernel/sched/deadline.c: 1828
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1839
	 * Current can't be migrated, useless to reschedule, /Users/rubber/linux/kernel/sched/deadline.c: 1840
	 * let's hope p can move out. /Users/rubber/linux/kernel/sched/deadline.c: 1841
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1847
	 * p is migratable, so let's not schedule it and /Users/rubber/linux/kernel/sched/deadline.c: 1848
	 * see if it is pushed or pulled somewhere else. /Users/rubber/linux/kernel/sched/deadline.c: 1849
		/* /Users/rubber/linux/kernel/sched/deadline.c: 1861
		 * This is OK, because current is on_cpu, which avoids it being /Users/rubber/linux/kernel/sched/deadline.c: 1862
		 * picked for load-balance and preemption/IRQs are still /Users/rubber/linux/kernel/sched/deadline.c: 1863
		 * disabled avoiding further scheduler activity on it and we've /Users/rubber/linux/kernel/sched/deadline.c: 1864
		 * not yet started the picking loop. /Users/rubber/linux/kernel/sched/deadline.c: 1865
 * Only called when both the current and waking task are -deadline /Users/rubber/linux/kernel/sched/deadline.c: 1877
 * tasks. /Users/rubber/linux/kernel/sched/deadline.c: 1878
	/* /Users/rubber/linux/kernel/sched/deadline.c: 1889
	 * In the unlikely case current and p have the same deadline /Users/rubber/linux/kernel/sched/deadline.c: 1890
	 * let us try to decide what's the best thing to do... /Users/rubber/linux/kernel/sched/deadline.c: 1891
 * scheduler tick hitting a task of our scheduling class. /Users/rubber/linux/kernel/sched/deadline.c: 1988
 * NOTE: This function can be called remotely by the tick offload that /Users/rubber/linux/kernel/sched/deadline.c: 1990
 * goes along full dynticks. Therefore no local assumption can be made /Users/rubber/linux/kernel/sched/deadline.c: 1991
 * and everything must be accessed through the @rq and @curr passed in /Users/rubber/linux/kernel/sched/deadline.c: 1992
 * parameters. /Users/rubber/linux/kernel/sched/deadline.c: 1993
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2000
	 * Even when we have runtime, update_curr_dl() might have resulted in us /Users/rubber/linux/kernel/sched/deadline.c: 2001
	 * not being the leftmost task anymore. In that case NEED_RESCHED will /Users/rubber/linux/kernel/sched/deadline.c: 2002
	 * be set and schedule() will start a new hrtick for the next task. /Users/rubber/linux/kernel/sched/deadline.c: 2003
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2012
	 * SCHED_DEADLINE tasks cannot fork and this is achieved through /Users/rubber/linux/kernel/sched/deadline.c: 2013
	 * sched_fork() /Users/rubber/linux/kernel/sched/deadline.c: 2014
 * Return the earliest pushable rq's task, which is suitable to be executed /Users/rubber/linux/kernel/sched/deadline.c: 2032
 * on the CPU, NULL otherwise: /Users/rubber/linux/kernel/sched/deadline.c: 2033
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2073
	 * We have to consider system topology and task affinity /Users/rubber/linux/kernel/sched/deadline.c: 2074
	 * first, then we can look for a suitable CPU. /Users/rubber/linux/kernel/sched/deadline.c: 2075
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2080
	 * If we are here, some targets have been found, including /Users/rubber/linux/kernel/sched/deadline.c: 2081
	 * the most suitable which is, among the runqueues where the /Users/rubber/linux/kernel/sched/deadline.c: 2082
	 * current tasks have later deadlines than the task's one, the /Users/rubber/linux/kernel/sched/deadline.c: 2083
	 * rq with the latest possible one. /Users/rubber/linux/kernel/sched/deadline.c: 2084
	 * /Users/rubber/linux/kernel/sched/deadline.c: 2085
	 * Now we check how well this matches with task's /Users/rubber/linux/kernel/sched/deadline.c: 2086
	 * affinity and system topology. /Users/rubber/linux/kernel/sched/deadline.c: 2087
	 * /Users/rubber/linux/kernel/sched/deadline.c: 2088
	 * The last CPU where the task run is our first /Users/rubber/linux/kernel/sched/deadline.c: 2089
	 * guess, since it is most likely cache-hot there. /Users/rubber/linux/kernel/sched/deadline.c: 2090
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2094
	 * Check if this_cpu is to be skipped (i.e., it is /Users/rubber/linux/kernel/sched/deadline.c: 2095
	 * not in the mask) or not. /Users/rubber/linux/kernel/sched/deadline.c: 2096
			/* /Users/rubber/linux/kernel/sched/deadline.c: 2106
			 * If possible, preempting this_cpu is /Users/rubber/linux/kernel/sched/deadline.c: 2107
			 * cheaper than migrating. /Users/rubber/linux/kernel/sched/deadline.c: 2108
			/* /Users/rubber/linux/kernel/sched/deadline.c: 2118
			 * Last chance: if a CPU being in both later_mask /Users/rubber/linux/kernel/sched/deadline.c: 2119
			 * and current sd span is valid, that becomes our /Users/rubber/linux/kernel/sched/deadline.c: 2120
			 * choice. Of course, the latest possible CPU is /Users/rubber/linux/kernel/sched/deadline.c: 2121
			 * already under consideration through later_mask. /Users/rubber/linux/kernel/sched/deadline.c: 2122
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2132
	 * At this point, all our guesses failed, we just return /Users/rubber/linux/kernel/sched/deadline.c: 2133
	 * 'something', and let the caller sort the things out. /Users/rubber/linux/kernel/sched/deadline.c: 2134
			/* /Users/rubber/linux/kernel/sched/deadline.c: 2164
			 * Target rq has tasks of equal or earlier deadline, /Users/rubber/linux/kernel/sched/deadline.c: 2165
			 * retrying does not release any lock and is unlikely /Users/rubber/linux/kernel/sched/deadline.c: 2166
			 * to yield a different result. /Users/rubber/linux/kernel/sched/deadline.c: 2167
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2186
		 * If the rq we found has no -deadline task, or /Users/rubber/linux/kernel/sched/deadline.c: 2187
		 * its earliest one has a later deadline than our /Users/rubber/linux/kernel/sched/deadline.c: 2188
		 * task, the rq is a good one. /Users/rubber/linux/kernel/sched/deadline.c: 2189
 * See if the non running -deadline tasks on this rq /Users/rubber/linux/kernel/sched/deadline.c: 2225
 * can be sent to some other CPU where they can preempt /Users/rubber/linux/kernel/sched/deadline.c: 2226
 * and start executing. /Users/rubber/linux/kernel/sched/deadline.c: 2227
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2249
	 * If next_task preempts rq->curr, and rq->curr /Users/rubber/linux/kernel/sched/deadline.c: 2250
	 * can move away, it makes sense to just reschedule /Users/rubber/linux/kernel/sched/deadline.c: 2251
	 * without going further in pushing next_task. /Users/rubber/linux/kernel/sched/deadline.c: 2252
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2269
		 * We must check all this again, since /Users/rubber/linux/kernel/sched/deadline.c: 2270
		 * find_lock_later_rq releases rq->lock and it is /Users/rubber/linux/kernel/sched/deadline.c: 2271
		 * then possible that next_task has migrated. /Users/rubber/linux/kernel/sched/deadline.c: 2272
			/* /Users/rubber/linux/kernel/sched/deadline.c: 2276
			 * The task is still there. We don't try /Users/rubber/linux/kernel/sched/deadline.c: 2277
			 * again, some other CPU will pull it when ready. /Users/rubber/linux/kernel/sched/deadline.c: 2278
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2295
	 * Update the later_rq clock here, because the clock is used /Users/rubber/linux/kernel/sched/deadline.c: 2296
	 * by the cpufreq_update_util() inside __add_running_bw(). /Users/rubber/linux/kernel/sched/deadline.c: 2297
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2331
	 * Match the barrier from dl_set_overloaded; this guarantees that if we /Users/rubber/linux/kernel/sched/deadline.c: 2332
	 * see overloaded we must also see the dlo_mask bit. /Users/rubber/linux/kernel/sched/deadline.c: 2333
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2343
		 * It looks racy, abd it is! However, as in sched_rt.c, /Users/rubber/linux/kernel/sched/deadline.c: 2344
		 * we are fine with this. /Users/rubber/linux/kernel/sched/deadline.c: 2345
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2356
		 * If there are no more pullable tasks on the /Users/rubber/linux/kernel/sched/deadline.c: 2357
		 * rq, we're done with it. /Users/rubber/linux/kernel/sched/deadline.c: 2358
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2365
		 * We found a task to be pulled if: /Users/rubber/linux/kernel/sched/deadline.c: 2366
		 *  - it preempts our current (if there's one), /Users/rubber/linux/kernel/sched/deadline.c: 2367
		 *  - it will preempt the last one we pulled (if any). /Users/rubber/linux/kernel/sched/deadline.c: 2368
			/* /Users/rubber/linux/kernel/sched/deadline.c: 2377
			 * Then we pull iff p has actually an earlier /Users/rubber/linux/kernel/sched/deadline.c: 2378
			 * deadline than the current task of its runqueue. /Users/rubber/linux/kernel/sched/deadline.c: 2379
 * Since the task is not running and a reschedule is not going to happen /Users/rubber/linux/kernel/sched/deadline.c: 2413
 * anytime soon on its runqueue, we try pushing it away now. /Users/rubber/linux/kernel/sched/deadline.c: 2414
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2439
	 * Migrating a SCHED_DEADLINE task between exclusive /Users/rubber/linux/kernel/sched/deadline.c: 2440
	 * cpusets (different root_domains) entails a bandwidth /Users/rubber/linux/kernel/sched/deadline.c: 2441
	 * update. We already made space for us in the destination /Users/rubber/linux/kernel/sched/deadline.c: 2442
	 * domain (see cpuset_can_attach()). /Users/rubber/linux/kernel/sched/deadline.c: 2443
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2449
		 * We now free resources of the root_domain we are migrating /Users/rubber/linux/kernel/sched/deadline.c: 2450
		 * off. In the worst case, sched_setattr() may temporary fail /Users/rubber/linux/kernel/sched/deadline.c: 2451
		 * until we complete the update. /Users/rubber/linux/kernel/sched/deadline.c: 2452
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2529
	 * task_non_contending() can start the "inactive timer" (if the 0-lag /Users/rubber/linux/kernel/sched/deadline.c: 2530
	 * time is in the future). If the task switches back to dl before /Users/rubber/linux/kernel/sched/deadline.c: 2531
	 * the "inactive timer" fires, it can continue to consume its current /Users/rubber/linux/kernel/sched/deadline.c: 2532
	 * runtime using its current deadline. If it stays outside of /Users/rubber/linux/kernel/sched/deadline.c: 2533
	 * SCHED_DEADLINE until the 0-lag time passes, inactive_task_timer() /Users/rubber/linux/kernel/sched/deadline.c: 2534
	 * will reset the task parameters. /Users/rubber/linux/kernel/sched/deadline.c: 2535
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2541
		 * Inactive timer is armed. However, p is leaving DEADLINE and /Users/rubber/linux/kernel/sched/deadline.c: 2542
		 * might migrate away from this rq while continuing to run on /Users/rubber/linux/kernel/sched/deadline.c: 2543
		 * some other class. We need to remove its contribution from /Users/rubber/linux/kernel/sched/deadline.c: 2544
		 * this rq running_bw now, or sub_rq_bw (below) will complain. /Users/rubber/linux/kernel/sched/deadline.c: 2545
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2552
	 * We cannot use inactive_task_timer() to invoke sub_running_bw() /Users/rubber/linux/kernel/sched/deadline.c: 2553
	 * at the 0-lag time, because the task could have been migrated /Users/rubber/linux/kernel/sched/deadline.c: 2554
	 * while SCHED_OTHER in the meanwhile. /Users/rubber/linux/kernel/sched/deadline.c: 2555
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2560
	 * Since this might be the only -deadline task on the rq, /Users/rubber/linux/kernel/sched/deadline.c: 2561
	 * this is the right place to try to pull some other one /Users/rubber/linux/kernel/sched/deadline.c: 2562
	 * from an overloaded CPU, if any. /Users/rubber/linux/kernel/sched/deadline.c: 2563
 * When switching to -deadline, we may overload the rq, then /Users/rubber/linux/kernel/sched/deadline.c: 2572
 * we try to push someone off, if possible. /Users/rubber/linux/kernel/sched/deadline.c: 2573
 * If the scheduling parameters of a -deadline task changed, /Users/rubber/linux/kernel/sched/deadline.c: 2602
 * a push or pull operation might be needed. /Users/rubber/linux/kernel/sched/deadline.c: 2603
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2610
		 * This might be too much, but unfortunately /Users/rubber/linux/kernel/sched/deadline.c: 2611
		 * we don't have the old deadline value, and /Users/rubber/linux/kernel/sched/deadline.c: 2612
		 * we can't argue if the task is increasing /Users/rubber/linux/kernel/sched/deadline.c: 2613
		 * or lowering its prio, so... /Users/rubber/linux/kernel/sched/deadline.c: 2614
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2619
		 * If we now have a earlier deadline task than p, /Users/rubber/linux/kernel/sched/deadline.c: 2620
		 * then reschedule, provided p is still on this /Users/rubber/linux/kernel/sched/deadline.c: 2621
		 * runqueue. /Users/rubber/linux/kernel/sched/deadline.c: 2622
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2627
		 * Again, we don't know if p has a earlier /Users/rubber/linux/kernel/sched/deadline.c: 2628
		 * or later deadline, so let's blindly set a /Users/rubber/linux/kernel/sched/deadline.c: 2629
		 * (maybe not needed) rescheduling point. /Users/rubber/linux/kernel/sched/deadline.c: 2630
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2684
	 * Here we want to check the bandwidth not being set to some /Users/rubber/linux/kernel/sched/deadline.c: 2685
	 * value smaller than the currently allocated bandwidth in /Users/rubber/linux/kernel/sched/deadline.c: 2686
	 * any of the root_domains. /Users/rubber/linux/kernel/sched/deadline.c: 2687
 * We must be sure that accepting a new task (or allowing changing the /Users/rubber/linux/kernel/sched/deadline.c: 2760
 * parameters of an existing one) is consistent with the bandwidth /Users/rubber/linux/kernel/sched/deadline.c: 2761
 * constraints. If yes, this function also accordingly updates the currently /Users/rubber/linux/kernel/sched/deadline.c: 2762
 * allocated bandwidth to reflect the new situation. /Users/rubber/linux/kernel/sched/deadline.c: 2763
 * This function is called while holding p's rq->lock. /Users/rubber/linux/kernel/sched/deadline.c: 2765
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2784
	 * Either if a task, enters, leave, or stays -deadline but changes /Users/rubber/linux/kernel/sched/deadline.c: 2785
	 * its parameters, we may need to update accordingly the total /Users/rubber/linux/kernel/sched/deadline.c: 2786
	 * allocated bandwidth of the container. /Users/rubber/linux/kernel/sched/deadline.c: 2787
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2801
		 * XXX this is slightly incorrect: when the task /Users/rubber/linux/kernel/sched/deadline.c: 2802
		 * utilization decreases, we should delay the total /Users/rubber/linux/kernel/sched/deadline.c: 2803
		 * utilization change until the task's 0-lag point. /Users/rubber/linux/kernel/sched/deadline.c: 2804
		 * But this would require to set the task's "inactive /Users/rubber/linux/kernel/sched/deadline.c: 2805
		 * timer" when the task is not inactive. /Users/rubber/linux/kernel/sched/deadline.c: 2806
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2813
		 * Do not decrease the total deadline utilization here, /Users/rubber/linux/kernel/sched/deadline.c: 2814
		 * switched_from_dl() will take care to do it at the correct /Users/rubber/linux/kernel/sched/deadline.c: 2815
		 * (0-lag) time. /Users/rubber/linux/kernel/sched/deadline.c: 2816
 * This function initializes the sched_dl_entity of a newly becoming /Users/rubber/linux/kernel/sched/deadline.c: 2826
 * SCHED_DEADLINE task. /Users/rubber/linux/kernel/sched/deadline.c: 2827
 * Only the static values are considered here, the actual runtime and the /Users/rubber/linux/kernel/sched/deadline.c: 2829
 * absolute deadline will be properly calculated when the task is enqueued /Users/rubber/linux/kernel/sched/deadline.c: 2830
 * for the first time with its new policy. /Users/rubber/linux/kernel/sched/deadline.c: 2831
 * Default limits for DL period; on the top end we guard against small util /Users/rubber/linux/kernel/sched/deadline.c: 2858
 * tasks still getting ridiculously long effective runtimes, on the bottom end we /Users/rubber/linux/kernel/sched/deadline.c: 2859
 * guard against timer DoS. /Users/rubber/linux/kernel/sched/deadline.c: 2860
 * This function validates the new parameters of a -deadline task. /Users/rubber/linux/kernel/sched/deadline.c: 2866
 * We ask for the deadline not being zero, and greater or equal /Users/rubber/linux/kernel/sched/deadline.c: 2867
 * than the runtime, as well as the period of being zero or /Users/rubber/linux/kernel/sched/deadline.c: 2868
 * greater than deadline. Furthermore, we have to be sure that /Users/rubber/linux/kernel/sched/deadline.c: 2869
 * user parameters are above the internal resolution of 1us (we /Users/rubber/linux/kernel/sched/deadline.c: 2870
 * check sched_runtime only since it is always the smaller one) and /Users/rubber/linux/kernel/sched/deadline.c: 2871
 * below 2^63 ns (we have to check both sched_deadline and /Users/rubber/linux/kernel/sched/deadline.c: 2872
 * sched_period, as the latter can be zero). /Users/rubber/linux/kernel/sched/deadline.c: 2873
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2887
	 * Since we truncate DL_SCALE bits, make sure we're at least /Users/rubber/linux/kernel/sched/deadline.c: 2888
	 * that big. /Users/rubber/linux/kernel/sched/deadline.c: 2889
	/* /Users/rubber/linux/kernel/sched/deadline.c: 2894
	 * Since we use the MSB for wrap-around and sign issues, make /Users/rubber/linux/kernel/sched/deadline.c: 2895
	 * sure it's not set (mind that period can be equal to zero). /Users/rubber/linux/kernel/sched/deadline.c: 2896
 * This function clears the sched_dl_entity static params. /Users/rubber/linux/kernel/sched/deadline.c: 2921
		/* /Users/rubber/linux/kernel/sched/deadline.c: 2976
		 * We reserve space for this task in the destination /Users/rubber/linux/kernel/sched/deadline.c: 2977
		 * root_domain, as we can't fail after this point. /Users/rubber/linux/kernel/sched/deadline.c: 2978
		 * We will free resources in the source root_domain /Users/rubber/linux/kernel/sched/deadline.c: 2979
		 * later on (see set_cpus_allowed_dl()). /Users/rubber/linux/kernel/sched/deadline.c: 2980
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/cpudeadline.c: 1
 *  kernel/sched/cpudl.c /Users/rubber/linux/kernel/sched/cpudeadline.c: 3
 *  Global CPU deadline management /Users/rubber/linux/kernel/sched/cpudeadline.c: 5
 *  Author: Juri Lelli <j.lelli@sssup.it> /Users/rubber/linux/kernel/sched/cpudeadline.c: 7
 * cpudl_find - find the best (later-dl) CPU in the system /Users/rubber/linux/kernel/sched/cpudeadline.c: 110
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 111
 * @p: the task /Users/rubber/linux/kernel/sched/cpudeadline.c: 112
 * @later_mask: a mask to fill in with the selected CPUs (or NULL) /Users/rubber/linux/kernel/sched/cpudeadline.c: 113
 * Returns: int - CPUs were found /Users/rubber/linux/kernel/sched/cpudeadline.c: 115
 * cpudl_clear - remove a CPU from the cpudl max-heap /Users/rubber/linux/kernel/sched/cpudeadline.c: 166
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 167
 * @cpu: the target CPU /Users/rubber/linux/kernel/sched/cpudeadline.c: 168
 * Notes: assumes cpu_rq(cpu)->lock is locked /Users/rubber/linux/kernel/sched/cpudeadline.c: 170
 * Returns: (void) /Users/rubber/linux/kernel/sched/cpudeadline.c: 172
		/* /Users/rubber/linux/kernel/sched/cpudeadline.c: 185
		 * Nothing to remove if old_idx was invalid. /Users/rubber/linux/kernel/sched/cpudeadline.c: 186
		 * This could happen if a rq_offline_dl is /Users/rubber/linux/kernel/sched/cpudeadline.c: 187
		 * called for a CPU without -dl tasks running. /Users/rubber/linux/kernel/sched/cpudeadline.c: 188
 * cpudl_set - update the cpudl max-heap /Users/rubber/linux/kernel/sched/cpudeadline.c: 205
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 206
 * @cpu: the target CPU /Users/rubber/linux/kernel/sched/cpudeadline.c: 207
 * @dl: the new earliest deadline for this CPU /Users/rubber/linux/kernel/sched/cpudeadline.c: 208
 * Notes: assumes cpu_rq(cpu)->lock is locked /Users/rubber/linux/kernel/sched/cpudeadline.c: 210
 * Returns: (void) /Users/rubber/linux/kernel/sched/cpudeadline.c: 212
 * cpudl_set_freecpu - Set the cpudl.free_cpus /Users/rubber/linux/kernel/sched/cpudeadline.c: 241
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 242
 * @cpu: rd attached CPU /Users/rubber/linux/kernel/sched/cpudeadline.c: 243
 * cpudl_clear_freecpu - Clear the cpudl.free_cpus /Users/rubber/linux/kernel/sched/cpudeadline.c: 251
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 252
 * @cpu: rd attached CPU /Users/rubber/linux/kernel/sched/cpudeadline.c: 253
 * cpudl_init - initialize the cpudl structure /Users/rubber/linux/kernel/sched/cpudeadline.c: 261
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 262
 * cpudl_cleanup - clean up the cpudl structure /Users/rubber/linux/kernel/sched/cpudeadline.c: 289
 * @cp: the cpudl max-heap context /Users/rubber/linux/kernel/sched/cpudeadline.c: 290
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/autogroup.c: 1
 * Auto-group scheduling implementation: /Users/rubber/linux/kernel/sched/autogroup.c: 3
	/* /Users/rubber/linux/kernel/sched/autogroup.c: 80
	 * Autogroup RT tasks are redirected to the root task group /Users/rubber/linux/kernel/sched/autogroup.c: 81
	 * so we don't have to move tasks around upon policy change, /Users/rubber/linux/kernel/sched/autogroup.c: 82
	 * or flail around trying to allocate bandwidth on the fly. /Users/rubber/linux/kernel/sched/autogroup.c: 83
	 * A bandwidth exception in __sched_setscheduler() allows /Users/rubber/linux/kernel/sched/autogroup.c: 84
	 * the policy change to proceed. /Users/rubber/linux/kernel/sched/autogroup.c: 85
	/* /Users/rubber/linux/kernel/sched/autogroup.c: 111
	 * If we race with autogroup_move_group() the caller can use the old /Users/rubber/linux/kernel/sched/autogroup.c: 112
	 * value of signal->autogroup but in this case sched_move_task() will /Users/rubber/linux/kernel/sched/autogroup.c: 113
	 * be called again before autogroup_kref_put(). /Users/rubber/linux/kernel/sched/autogroup.c: 114
	 * /Users/rubber/linux/kernel/sched/autogroup.c: 115
	 * However, there is no way sched_autogroup_exit_task() could tell us /Users/rubber/linux/kernel/sched/autogroup.c: 116
	 * to avoid autogroup->tg, so we abuse PF_EXITING flag for this case. /Users/rubber/linux/kernel/sched/autogroup.c: 117
	/* /Users/rubber/linux/kernel/sched/autogroup.c: 127
	 * We are going to call exit_notify() and autogroup_move_group() can't /Users/rubber/linux/kernel/sched/autogroup.c: 128
	 * see this thread after that: we can no longer use signal->autogroup. /Users/rubber/linux/kernel/sched/autogroup.c: 129
	 * See the PF_EXITING check in task_wants_autogroup(). /Users/rubber/linux/kernel/sched/autogroup.c: 130
	/* /Users/rubber/linux/kernel/sched/autogroup.c: 151
	 * We can't avoid sched_move_task() after we changed signal->autogroup, /Users/rubber/linux/kernel/sched/autogroup.c: 152
	 * this process can already run with task_group() == prev->tg or we can /Users/rubber/linux/kernel/sched/autogroup.c: 153
	 * race with cgroup code which can read autogroup = prev under rq->lock. /Users/rubber/linux/kernel/sched/autogroup.c: 154
	 * In the latter case for_each_thread() can not miss a migrating thread, /Users/rubber/linux/kernel/sched/autogroup.c: 155
	 * cpu_cgroup_attach() must not be possible after cgroup_exit() and it /Users/rubber/linux/kernel/sched/autogroup.c: 156
	 * can't be removed from thread list, we hold ->siglock. /Users/rubber/linux/kernel/sched/autogroup.c: 157
	 * /Users/rubber/linux/kernel/sched/autogroup.c: 158
	 * If an exiting thread was already removed from thread list we rely on /Users/rubber/linux/kernel/sched/autogroup.c: 159
	 * sched_autogroup_exit_task(). /Users/rubber/linux/kernel/sched/autogroup.c: 160
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/topology.c: 1
 * Scheduler topology setup/handling methods /Users/rubber/linux/kernel/sched/topology.c: 3
 * EAS can be used on a root domain if it meets all the following conditions: /Users/rubber/linux/kernel/sched/topology.c: 325
 *    1. an Energy Model (EM) is available; /Users/rubber/linux/kernel/sched/topology.c: 326
 *    2. the SD_ASYM_CPUCAPACITY flag is set in the sched_domain hierarchy. /Users/rubber/linux/kernel/sched/topology.c: 327
 *    3. no SMT is detected. /Users/rubber/linux/kernel/sched/topology.c: 328
 *    4. the EM complexity is low enough to keep scheduling overheads low; /Users/rubber/linux/kernel/sched/topology.c: 329
 *    5. schedutil is driving the frequency of all CPUs of the rd; /Users/rubber/linux/kernel/sched/topology.c: 330
 *    6. frequency invariance support is present; /Users/rubber/linux/kernel/sched/topology.c: 331
 * The complexity of the Energy Model is defined as: /Users/rubber/linux/kernel/sched/topology.c: 333
 *              C = nr_pd * (nr_cpus + nr_ps) /Users/rubber/linux/kernel/sched/topology.c: 335
 * with parameters defined as: /Users/rubber/linux/kernel/sched/topology.c: 337
 *  - nr_pd:    the number of performance domains /Users/rubber/linux/kernel/sched/topology.c: 338
 *  - nr_cpus:  the number of CPUs /Users/rubber/linux/kernel/sched/topology.c: 339
 *  - nr_ps:    the sum of the number of performance states of all performance /Users/rubber/linux/kernel/sched/topology.c: 340
 *              domains (for example, on a system with 2 performance domains, /Users/rubber/linux/kernel/sched/topology.c: 341
 *              with 10 performance states each, nr_ps = 2 * 10 = 20). /Users/rubber/linux/kernel/sched/topology.c: 342
 * It is generally not a good idea to use such a model in the wake-up path on /Users/rubber/linux/kernel/sched/topology.c: 344
 * very complex platforms because of the associated scheduling overheads. The /Users/rubber/linux/kernel/sched/topology.c: 345
 * arbitrary constraint below prevents that. It makes EAS usable up to 16 CPUs /Users/rubber/linux/kernel/sched/topology.c: 346
 * with per-CPU DVFS and less than 8 performance states each, for example. /Users/rubber/linux/kernel/sched/topology.c: 347
		/* /Users/rubber/linux/kernel/sched/topology.c: 413
		 * Count performance domains and performance states for the /Users/rubber/linux/kernel/sched/topology.c: 414
		 * complexity check. /Users/rubber/linux/kernel/sched/topology.c: 415
		/* /Users/rubber/linux/kernel/sched/topology.c: 480
		 * If we dont want to free the old_rd yet then /Users/rubber/linux/kernel/sched/topology.c: 481
		 * set old_rd to NULL to skip the freeing later /Users/rubber/linux/kernel/sched/topology.c: 482
		 * in this function: /Users/rubber/linux/kernel/sched/topology.c: 483
 * By default the system creates a single root-domain with all CPUs as /Users/rubber/linux/kernel/sched/topology.c: 556
 * members (mimicking the global state we have today). /Users/rubber/linux/kernel/sched/topology.c: 557
	/* /Users/rubber/linux/kernel/sched/topology.c: 606
	 * A normal sched domain may have multiple group references, an /Users/rubber/linux/kernel/sched/topology.c: 607
	 * overlapping domain, having private groups, only one.  Iterate, /Users/rubber/linux/kernel/sched/topology.c: 608
	 * dropping group/capacity references, freeing where none remain. /Users/rubber/linux/kernel/sched/topology.c: 609
 * Keep a special pointer to the highest sched_domain that has /Users/rubber/linux/kernel/sched/topology.c: 636
 * SD_SHARE_PKG_RESOURCE set (Last Level Cache Domain) for this /Users/rubber/linux/kernel/sched/topology.c: 637
 * allows us to avoid some pointer chasing select_idle_sibling(). /Users/rubber/linux/kernel/sched/topology.c: 638
 * Also keep a unique ID per domain (we use the first CPU number in /Users/rubber/linux/kernel/sched/topology.c: 640
 * the cpumask of the domain), this allows us to quickly tell if /Users/rubber/linux/kernel/sched/topology.c: 641
 * two CPUs are in the same cache domain, see cpus_share_cache(). /Users/rubber/linux/kernel/sched/topology.c: 642
 * Attach the domain 'sd' to 'cpu' as its base domain. Callers must /Users/rubber/linux/kernel/sched/topology.c: 683
 * hold the hotplug lock. /Users/rubber/linux/kernel/sched/topology.c: 684
			/* /Users/rubber/linux/kernel/sched/topology.c: 702
			 * Transfer SD_PREFER_SIBLING down in case of a /Users/rubber/linux/kernel/sched/topology.c: 703
			 * degenerate parent; the spans match for this /Users/rubber/linux/kernel/sched/topology.c: 704
			 * so the property transfers. /Users/rubber/linux/kernel/sched/topology.c: 705
			/* /Users/rubber/linux/kernel/sched/topology.c: 721
			 * sched groups hold the flags of the child sched /Users/rubber/linux/kernel/sched/topology.c: 722
			 * domain for convenience. Clear such flags since /Users/rubber/linux/kernel/sched/topology.c: 723
			 * the child is being destroyed. /Users/rubber/linux/kernel/sched/topology.c: 724
 * Return the canonical balance CPU for this group, this is the first CPU /Users/rubber/linux/kernel/sched/topology.c: 758
 * of this group that's also in the balance mask. /Users/rubber/linux/kernel/sched/topology.c: 759
 * The balance mask are all those CPUs that could actually end up at this /Users/rubber/linux/kernel/sched/topology.c: 761
 * group. See build_balance_mask(). /Users/rubber/linux/kernel/sched/topology.c: 762
 * Also see should_we_balance(). /Users/rubber/linux/kernel/sched/topology.c: 764
 * NUMA topology (first read the regular topology blurb below) /Users/rubber/linux/kernel/sched/topology.c: 773
 * Given a node-distance table, for example: /Users/rubber/linux/kernel/sched/topology.c: 775
 *   node   0   1   2   3 /Users/rubber/linux/kernel/sched/topology.c: 777
 *     0:  10  20  30  20 /Users/rubber/linux/kernel/sched/topology.c: 778
 *     1:  20  10  20  30 /Users/rubber/linux/kernel/sched/topology.c: 779
 *     2:  30  20  10  20 /Users/rubber/linux/kernel/sched/topology.c: 780
 *     3:  20  30  20  10 /Users/rubber/linux/kernel/sched/topology.c: 781
 * which represents a 4 node ring topology like: /Users/rubber/linux/kernel/sched/topology.c: 783
 *   0 ----- 1 /Users/rubber/linux/kernel/sched/topology.c: 785
 *   |       | /Users/rubber/linux/kernel/sched/topology.c: 786
 *   |       | /Users/rubber/linux/kernel/sched/topology.c: 787
 *   |       | /Users/rubber/linux/kernel/sched/topology.c: 788
 *   3 ----- 2 /Users/rubber/linux/kernel/sched/topology.c: 789
 * We want to construct domains and groups to represent this. The way we go /Users/rubber/linux/kernel/sched/topology.c: 791
 * about doing this is to build the domains on 'hops'. For each NUMA level we /Users/rubber/linux/kernel/sched/topology.c: 792
 * construct the mask of all nodes reachable in @level hops. /Users/rubber/linux/kernel/sched/topology.c: 793
 * For the above NUMA topology that gives 3 levels: /Users/rubber/linux/kernel/sched/topology.c: 795
 * NUMA-2	0-3		0-3		0-3		0-3 /Users/rubber/linux/kernel/sched/topology.c: 797
 *  groups:	{0-1,3},{1-3}	{0-2},{0,2-3}	{1-3},{0-1,3}	{0,2-3},{0-2} /Users/rubber/linux/kernel/sched/topology.c: 798
 * NUMA-1	0-1,3		0-2		1-3		0,2-3 /Users/rubber/linux/kernel/sched/topology.c: 800
 *  groups:	{0},{1},{3}	{0},{1},{2}	{1},{2},{3}	{0},{2},{3} /Users/rubber/linux/kernel/sched/topology.c: 801
 * NUMA-0	0		1		2		3 /Users/rubber/linux/kernel/sched/topology.c: 803
 * As can be seen; things don't nicely line up as with the regular topology. /Users/rubber/linux/kernel/sched/topology.c: 806
 * When we iterate a domain in child domain chunks some nodes can be /Users/rubber/linux/kernel/sched/topology.c: 807
 * represented multiple times -- hence the "overlap" naming for this part of /Users/rubber/linux/kernel/sched/topology.c: 808
 * the topology. /Users/rubber/linux/kernel/sched/topology.c: 809
 * In order to minimize this overlap, we only build enough groups to cover the /Users/rubber/linux/kernel/sched/topology.c: 811
 * domain. For instance Node-0 NUMA-2 would only get groups: 0-1,3 and 1-3. /Users/rubber/linux/kernel/sched/topology.c: 812
 * Because: /Users/rubber/linux/kernel/sched/topology.c: 814
 *  - the first group of each domain is its child domain; this /Users/rubber/linux/kernel/sched/topology.c: 816
 *    gets us the first 0-1,3 /Users/rubber/linux/kernel/sched/topology.c: 817
 *  - the only uncovered node is 2, who's child domain is 1-3. /Users/rubber/linux/kernel/sched/topology.c: 818
 * However, because of the overlap, computing a unique CPU for each group is /Users/rubber/linux/kernel/sched/topology.c: 820
 * more complicated. Consider for instance the groups of NODE-1 NUMA-2, both /Users/rubber/linux/kernel/sched/topology.c: 821
 * groups include the CPUs of Node-0, while those CPUs would not in fact ever /Users/rubber/linux/kernel/sched/topology.c: 822
 * end up at those groups (they would end up in group: 0-1,3). /Users/rubber/linux/kernel/sched/topology.c: 823
 * To correct this we have to introduce the group balance mask. This mask /Users/rubber/linux/kernel/sched/topology.c: 825
 * will contain those CPUs in the group that can reach this group given the /Users/rubber/linux/kernel/sched/topology.c: 826
 * (child) domain tree. /Users/rubber/linux/kernel/sched/topology.c: 827
 * With this we can once again compute balance_cpu and sched_group_capacity /Users/rubber/linux/kernel/sched/topology.c: 829
 * relations. /Users/rubber/linux/kernel/sched/topology.c: 830
 * XXX include words on how balance_cpu is unique and therefore can be /Users/rubber/linux/kernel/sched/topology.c: 832
 * used for sched_group_capacity links. /Users/rubber/linux/kernel/sched/topology.c: 833
 * Another 'interesting' topology is: /Users/rubber/linux/kernel/sched/topology.c: 836
 *   node   0   1   2   3 /Users/rubber/linux/kernel/sched/topology.c: 838
 *     0:  10  20  20  30 /Users/rubber/linux/kernel/sched/topology.c: 839
 *     1:  20  10  20  20 /Users/rubber/linux/kernel/sched/topology.c: 840
 *     2:  20  20  10  20 /Users/rubber/linux/kernel/sched/topology.c: 841
 *     3:  30  20  20  10 /Users/rubber/linux/kernel/sched/topology.c: 842
 * Which looks a little like: /Users/rubber/linux/kernel/sched/topology.c: 844
 *   0 ----- 1 /Users/rubber/linux/kernel/sched/topology.c: 846
 *   |     / | /Users/rubber/linux/kernel/sched/topology.c: 847
 *   |   /   | /Users/rubber/linux/kernel/sched/topology.c: 848
 *   | /     | /Users/rubber/linux/kernel/sched/topology.c: 849
 *   2 ----- 3 /Users/rubber/linux/kernel/sched/topology.c: 850
 * This topology is asymmetric, nodes 1,2 are fully connected, but nodes 0,3 /Users/rubber/linux/kernel/sched/topology.c: 852
 * are not. /Users/rubber/linux/kernel/sched/topology.c: 853
 * This leads to a few particularly weird cases where the sched_domain's are /Users/rubber/linux/kernel/sched/topology.c: 855
 * not of the same number for each CPU. Consider: /Users/rubber/linux/kernel/sched/topology.c: 856
 * NUMA-2	0-3						0-3 /Users/rubber/linux/kernel/sched/topology.c: 858
 *  groups:	{0-2},{1-3}					{1-3},{0-2} /Users/rubber/linux/kernel/sched/topology.c: 859
 * NUMA-1	0-2		0-3		0-3		1-3 /Users/rubber/linux/kernel/sched/topology.c: 861
 * NUMA-0	0		1		2		3 /Users/rubber/linux/kernel/sched/topology.c: 863
 * Build the balance mask; it contains only those CPUs that can arrive at this /Users/rubber/linux/kernel/sched/topology.c: 869
 * group and should be considered to continue balancing. /Users/rubber/linux/kernel/sched/topology.c: 870
 * We do this during the group creation pass, therefore the group information /Users/rubber/linux/kernel/sched/topology.c: 872
 * isn't complete yet, however since each group represents a (child) domain we /Users/rubber/linux/kernel/sched/topology.c: 873
 * can fully construct this using the sched_domain bits (which are already /Users/rubber/linux/kernel/sched/topology.c: 874
 * complete). /Users/rubber/linux/kernel/sched/topology.c: 875
		/* /Users/rubber/linux/kernel/sched/topology.c: 890
		 * Can happen in the asymmetric case, where these siblings are /Users/rubber/linux/kernel/sched/topology.c: 891
		 * unused. The mask will not be empty because those CPUs that /Users/rubber/linux/kernel/sched/topology.c: 892
		 * do have the top domain _should_ span the domain. /Users/rubber/linux/kernel/sched/topology.c: 893
 * XXX: This creates per-node group entries; since the load-balancer will /Users/rubber/linux/kernel/sched/topology.c: 910
 * immediately access remote memory to construct this group's load-balance /Users/rubber/linux/kernel/sched/topology.c: 911
 * statistics having the groups node local is of dubious benefit. /Users/rubber/linux/kernel/sched/topology.c: 912
	/* /Users/rubber/linux/kernel/sched/topology.c: 955
	 * Initialize sgc->capacity such that even if we mess up the /Users/rubber/linux/kernel/sched/topology.c: 956
	 * domains and no possible iteration will get us here, we won't /Users/rubber/linux/kernel/sched/topology.c: 957
	 * die on a /0 trap. /Users/rubber/linux/kernel/sched/topology.c: 958
	/* /Users/rubber/linux/kernel/sched/topology.c: 969
	 * The proper descendant would be the one whose child won't span out /Users/rubber/linux/kernel/sched/topology.c: 970
	 * of sd /Users/rubber/linux/kernel/sched/topology.c: 971
	/* /Users/rubber/linux/kernel/sched/topology.c: 978
	 * As we are referencing sgc across different topology level, we need /Users/rubber/linux/kernel/sched/topology.c: 979
	 * to go down to skip those sched_domains which don't contribute to /Users/rubber/linux/kernel/sched/topology.c: 980
	 * scheduling because they will be degenerated in cpu_attach_domain /Users/rubber/linux/kernel/sched/topology.c: 981
		/* /Users/rubber/linux/kernel/sched/topology.c: 1011
		 * Asymmetric node setups can result in situations where the /Users/rubber/linux/kernel/sched/topology.c: 1012
		 * domain tree is of unequal depth, make sure to skip domains /Users/rubber/linux/kernel/sched/topology.c: 1013
		 * that already cover the entire range. /Users/rubber/linux/kernel/sched/topology.c: 1014
		 * /Users/rubber/linux/kernel/sched/topology.c: 1015
		 * In that case build_sched_domains() will have terminated the /Users/rubber/linux/kernel/sched/topology.c: 1016
		 * iteration early and our sibling sd spans will be empty. /Users/rubber/linux/kernel/sched/topology.c: 1017
		 * Domains should always include the CPU they're built on, so /Users/rubber/linux/kernel/sched/topology.c: 1018
		 * check that. /Users/rubber/linux/kernel/sched/topology.c: 1019
		/* /Users/rubber/linux/kernel/sched/topology.c: 1024
		 * Usually we build sched_group by sibling's child sched_domain /Users/rubber/linux/kernel/sched/topology.c: 1025
		 * But for machines whose NUMA diameter are 3 or above, we move /Users/rubber/linux/kernel/sched/topology.c: 1026
		 * to build sched_group by sibling's proper descendant's child /Users/rubber/linux/kernel/sched/topology.c: 1027
		 * domain because sibling's child sched_domain will span out of /Users/rubber/linux/kernel/sched/topology.c: 1028
		 * the sched_domain being built as below. /Users/rubber/linux/kernel/sched/topology.c: 1029
		 * /Users/rubber/linux/kernel/sched/topology.c: 1030
		 * Smallest diameter=3 topology is: /Users/rubber/linux/kernel/sched/topology.c: 1031
		 * /Users/rubber/linux/kernel/sched/topology.c: 1032
		 *   node   0   1   2   3 /Users/rubber/linux/kernel/sched/topology.c: 1033
		 *     0:  10  20  30  40 /Users/rubber/linux/kernel/sched/topology.c: 1034
		 *     1:  20  10  20  30 /Users/rubber/linux/kernel/sched/topology.c: 1035
		 *     2:  30  20  10  20 /Users/rubber/linux/kernel/sched/topology.c: 1036
		 *     3:  40  30  20  10 /Users/rubber/linux/kernel/sched/topology.c: 1037
		 * /Users/rubber/linux/kernel/sched/topology.c: 1038
		 *   0 --- 1 --- 2 --- 3 /Users/rubber/linux/kernel/sched/topology.c: 1039
		 * /Users/rubber/linux/kernel/sched/topology.c: 1040
		 * NUMA-3       0-3             N/A             N/A             0-3 /Users/rubber/linux/kernel/sched/topology.c: 1041
		 *  groups:     {0-2},{1-3}                                     {1-3},{0-2} /Users/rubber/linux/kernel/sched/topology.c: 1042
		 * /Users/rubber/linux/kernel/sched/topology.c: 1043
		 * NUMA-2       0-2             0-3             0-3             1-3 /Users/rubber/linux/kernel/sched/topology.c: 1044
		 *  groups:     {0-1},{1-3}     {0-2},{2-3}     {1-3},{0-1}     {2-3},{0-2} /Users/rubber/linux/kernel/sched/topology.c: 1045
		 * /Users/rubber/linux/kernel/sched/topology.c: 1046
		 * NUMA-1       0-1             0-2             1-3             2-3 /Users/rubber/linux/kernel/sched/topology.c: 1047
		 *  groups:     {0},{1}         {1},{2},{0}     {2},{3},{1}     {3},{2} /Users/rubber/linux/kernel/sched/topology.c: 1048
		 * /Users/rubber/linux/kernel/sched/topology.c: 1049
		 * NUMA-0       0               1               2               3 /Users/rubber/linux/kernel/sched/topology.c: 1050
		 * /Users/rubber/linux/kernel/sched/topology.c: 1051
		 * The NUMA-2 groups for nodes 0 and 3 are obviously buggered, as the /Users/rubber/linux/kernel/sched/topology.c: 1052
		 * group span isn't a subset of the domain span. /Users/rubber/linux/kernel/sched/topology.c: 1053
 * Package topology (also see the load-balance blurb in fair.c) /Users/rubber/linux/kernel/sched/topology.c: 1087
 * The scheduler builds a tree structure to represent a number of important /Users/rubber/linux/kernel/sched/topology.c: 1089
 * topology features. By default (default_topology[]) these include: /Users/rubber/linux/kernel/sched/topology.c: 1090
 *  - Simultaneous multithreading (SMT) /Users/rubber/linux/kernel/sched/topology.c: 1092
 *  - Multi-Core Cache (MC) /Users/rubber/linux/kernel/sched/topology.c: 1093
 *  - Package (DIE) /Users/rubber/linux/kernel/sched/topology.c: 1094
 * Where the last one more or less denotes everything up to a NUMA node. /Users/rubber/linux/kernel/sched/topology.c: 1096
 * The tree consists of 3 primary data structures: /Users/rubber/linux/kernel/sched/topology.c: 1098
 *	sched_domain -> sched_group -> sched_group_capacity /Users/rubber/linux/kernel/sched/topology.c: 1100
 *	    ^ ^             ^ ^ /Users/rubber/linux/kernel/sched/topology.c: 1101
 *          `-'             `-' /Users/rubber/linux/kernel/sched/topology.c: 1102
 * The sched_domains are per-CPU and have a two way link (parent & child) and /Users/rubber/linux/kernel/sched/topology.c: 1104
 * denote the ever growing mask of CPUs belonging to that level of topology. /Users/rubber/linux/kernel/sched/topology.c: 1105
 * Each sched_domain has a circular (double) linked list of sched_group's, each /Users/rubber/linux/kernel/sched/topology.c: 1107
 * denoting the domains of the level below (or individual CPUs in case of the /Users/rubber/linux/kernel/sched/topology.c: 1108
 * first domain level). The sched_group linked by a sched_domain includes the /Users/rubber/linux/kernel/sched/topology.c: 1109
 * CPU of that sched_domain [*]. /Users/rubber/linux/kernel/sched/topology.c: 1110
 * Take for instance a 2 threaded, 2 core, 2 cache cluster part: /Users/rubber/linux/kernel/sched/topology.c: 1112
 * CPU   0   1   2   3   4   5   6   7 /Users/rubber/linux/kernel/sched/topology.c: 1114
 * DIE  [                             ] /Users/rubber/linux/kernel/sched/topology.c: 1116
 * MC   [             ] [             ] /Users/rubber/linux/kernel/sched/topology.c: 1117
 * SMT  [     ] [     ] [     ] [     ] /Users/rubber/linux/kernel/sched/topology.c: 1118
 *  - or - /Users/rubber/linux/kernel/sched/topology.c: 1120
 * DIE  0-7 0-7 0-7 0-7 0-7 0-7 0-7 0-7 /Users/rubber/linux/kernel/sched/topology.c: 1122
 * MC	0-3 0-3 0-3 0-3 4-7 4-7 4-7 4-7 /Users/rubber/linux/kernel/sched/topology.c: 1123
 * SMT  0-1 0-1 2-3 2-3 4-5 4-5 6-7 6-7 /Users/rubber/linux/kernel/sched/topology.c: 1124
 * CPU   0   1   2   3   4   5   6   7 /Users/rubber/linux/kernel/sched/topology.c: 1126
 * One way to think about it is: sched_domain moves you up and down among these /Users/rubber/linux/kernel/sched/topology.c: 1128
 * topology levels, while sched_group moves you sideways through it, at child /Users/rubber/linux/kernel/sched/topology.c: 1129
 * domain granularity. /Users/rubber/linux/kernel/sched/topology.c: 1130
 * sched_group_capacity ensures each unique sched_group has shared storage. /Users/rubber/linux/kernel/sched/topology.c: 1132
 * There are two related construction problems, both require a CPU that /Users/rubber/linux/kernel/sched/topology.c: 1134
 * uniquely identify each group (for a given domain): /Users/rubber/linux/kernel/sched/topology.c: 1135
 *  - The first is the balance_cpu (see should_we_balance() and the /Users/rubber/linux/kernel/sched/topology.c: 1137
 *    load-balance blub in fair.c); for each group we only want 1 CPU to /Users/rubber/linux/kernel/sched/topology.c: 1138
 *    continue balancing at a higher domain. /Users/rubber/linux/kernel/sched/topology.c: 1139
 *  - The second is the sched_group_capacity; we want all identical groups /Users/rubber/linux/kernel/sched/topology.c: 1141
 *    to share a single sched_group_capacity. /Users/rubber/linux/kernel/sched/topology.c: 1142
 * Since these topologies are exclusive by construction. That is, its /Users/rubber/linux/kernel/sched/topology.c: 1144
 * impossible for an SMT thread to belong to multiple cores, and cores to /Users/rubber/linux/kernel/sched/topology.c: 1145
 * be part of multiple caches. There is a very clear and unique location /Users/rubber/linux/kernel/sched/topology.c: 1146
 * for each CPU in the hierarchy. /Users/rubber/linux/kernel/sched/topology.c: 1147
 * Therefore computing a unique CPU for each group is trivial (the iteration /Users/rubber/linux/kernel/sched/topology.c: 1149
 * mask is redundant and set all 1s; all CPUs in a group will end up at _that_ /Users/rubber/linux/kernel/sched/topology.c: 1150
 * group), we can simply pick the first CPU in each group. /Users/rubber/linux/kernel/sched/topology.c: 1151
 * [*] in other words, the first group of each domain is its child domain. /Users/rubber/linux/kernel/sched/topology.c: 1154
 * build_sched_groups will build a circular linked list of the groups /Users/rubber/linux/kernel/sched/topology.c: 1196
 * covered by the given span, will set each group's ->cpumask correctly, /Users/rubber/linux/kernel/sched/topology.c: 1197
 * and will initialize their ->sgc. /Users/rubber/linux/kernel/sched/topology.c: 1198
 * Assumes the sched_domain tree is fully constructed /Users/rubber/linux/kernel/sched/topology.c: 1200
 * Initialize sched groups cpu_capacity. /Users/rubber/linux/kernel/sched/topology.c: 1239
 * cpu_capacity indicates the capacity of sched group, which is used while /Users/rubber/linux/kernel/sched/topology.c: 1241
 * distributing the load between different sched groups in a sched domain. /Users/rubber/linux/kernel/sched/topology.c: 1242
 * Typically cpu_capacity for all the groups in a sched domain will be same /Users/rubber/linux/kernel/sched/topology.c: 1243
 * unless there are asymmetries in the topology. If there are asymmetries, /Users/rubber/linux/kernel/sched/topology.c: 1244
 * group having more cpu_capacity will pickup more load compared to the /Users/rubber/linux/kernel/sched/topology.c: 1245
 * group having less cpu_capacity. /Users/rubber/linux/kernel/sched/topology.c: 1246
 * Asymmetric CPU capacity bits /Users/rubber/linux/kernel/sched/topology.c: 1281
 * Set of available CPUs grouped by their corresponding capacities /Users/rubber/linux/kernel/sched/topology.c: 1290
 * Each list entry contains a CPU mask reflecting CPUs that share the same /Users/rubber/linux/kernel/sched/topology.c: 1291
 * capacity. /Users/rubber/linux/kernel/sched/topology.c: 1292
 * The lifespan of data is unlimited. /Users/rubber/linux/kernel/sched/topology.c: 1293
 * Verify whether there is any CPU capacity asymmetry in a given sched domain. /Users/rubber/linux/kernel/sched/topology.c: 1300
 * Provides sd_flags reflecting the asymmetry scope. /Users/rubber/linux/kernel/sched/topology.c: 1301
	/* /Users/rubber/linux/kernel/sched/topology.c: 1310
	 * Count how many unique CPU capacities this domain spans across /Users/rubber/linux/kernel/sched/topology.c: 1311
	 * (compare sched_domain CPUs mask with ones representing  available /Users/rubber/linux/kernel/sched/topology.c: 1312
	 * CPUs capacities). Take into account CPUs that might be offline: /Users/rubber/linux/kernel/sched/topology.c: 1313
	 * skip those. /Users/rubber/linux/kernel/sched/topology.c: 1314
 * Build-up/update list of CPUs grouped by their capacities /Users/rubber/linux/kernel/sched/topology.c: 1357
 * An update requires explicit request to rebuild sched domains /Users/rubber/linux/kernel/sched/topology.c: 1358
 * with state indicating CPU topology changes. /Users/rubber/linux/kernel/sched/topology.c: 1359
	/* /Users/rubber/linux/kernel/sched/topology.c: 1379
	 * Only one capacity value has been detected i.e. this system is symmetric. /Users/rubber/linux/kernel/sched/topology.c: 1380
	 * No need to keep this data around. /Users/rubber/linux/kernel/sched/topology.c: 1381
 * Initializers for schedule domains /Users/rubber/linux/kernel/sched/topology.c: 1391
 * Non-inlined to reduce accumulated stack pressure in build_sched_domains() /Users/rubber/linux/kernel/sched/topology.c: 1392
 * NULL the sd_data elements we've used to build the sched_domain and /Users/rubber/linux/kernel/sched/topology.c: 1465
 * sched_group structure so that the subsequent __free_domain_allocs() /Users/rubber/linux/kernel/sched/topology.c: 1466
 * will not free the data we're using. /Users/rubber/linux/kernel/sched/topology.c: 1467
 * SD_flags allowed in topology descriptions. /Users/rubber/linux/kernel/sched/topology.c: 1500
 * These flags are purely descriptive of the topology and do not prescribe /Users/rubber/linux/kernel/sched/topology.c: 1502
 * behaviour. Behaviour is artificial and mapped in the below sd_init() /Users/rubber/linux/kernel/sched/topology.c: 1503
 * function: /Users/rubber/linux/kernel/sched/topology.c: 1504
 *   SD_SHARE_CPUCAPACITY   - describes SMT topologies /Users/rubber/linux/kernel/sched/topology.c: 1506
 *   SD_SHARE_PKG_RESOURCES - describes shared caches /Users/rubber/linux/kernel/sched/topology.c: 1507
 *   SD_NUMA                - describes NUMA topologies /Users/rubber/linux/kernel/sched/topology.c: 1508
 * Odd one out, which beside describing the topology has a quirk also /Users/rubber/linux/kernel/sched/topology.c: 1510
 * prescribes the desired behaviour that goes along with it: /Users/rubber/linux/kernel/sched/topology.c: 1511
 *   SD_ASYM_PACKING        - describes SMT quirks /Users/rubber/linux/kernel/sched/topology.c: 1513
	/* /Users/rubber/linux/kernel/sched/topology.c: 1532
	 * Ugly hack to pass state to sd_numa_mask()... /Users/rubber/linux/kernel/sched/topology.c: 1533
	/* /Users/rubber/linux/kernel/sched/topology.c: 1587
	 * Convert topological properties into behaviour. /Users/rubber/linux/kernel/sched/topology.c: 1588
	/* /Users/rubber/linux/kernel/sched/topology.c: 1618
	 * For all levels sharing cache; connect a sched_domain_shared /Users/rubber/linux/kernel/sched/topology.c: 1619
	 * instance. /Users/rubber/linux/kernel/sched/topology.c: 1620
 * Topology list, bottom-up. /Users/rubber/linux/kernel/sched/topology.c: 1634
 * A system can have three types of NUMA topology: /Users/rubber/linux/kernel/sched/topology.c: 1710
 * NUMA_DIRECT: all nodes are directly connected, or not a NUMA system /Users/rubber/linux/kernel/sched/topology.c: 1711
 * NUMA_GLUELESS_MESH: some nodes reachable through intermediary nodes /Users/rubber/linux/kernel/sched/topology.c: 1712
 * NUMA_BACKPLANE: nodes can reach other nodes through a backplane /Users/rubber/linux/kernel/sched/topology.c: 1713
 * The difference between a glueless mesh topology and a backplane /Users/rubber/linux/kernel/sched/topology.c: 1715
 * topology lies in whether communication between not directly /Users/rubber/linux/kernel/sched/topology.c: 1716
 * connected nodes goes through intermediary nodes (where programs /Users/rubber/linux/kernel/sched/topology.c: 1717
 * could run), or through backplane controllers. This affects /Users/rubber/linux/kernel/sched/topology.c: 1718
 * placement of programs. /Users/rubber/linux/kernel/sched/topology.c: 1719
 * The type of topology can be discerned with the following tests: /Users/rubber/linux/kernel/sched/topology.c: 1721
 * - If the maximum distance between any nodes is 1 hop, the system /Users/rubber/linux/kernel/sched/topology.c: 1722
 *   is directly connected. /Users/rubber/linux/kernel/sched/topology.c: 1723
 * - If for two nodes A and B, located N > 1 hops away from each other, /Users/rubber/linux/kernel/sched/topology.c: 1724
 *   there is an intermediary node C, which is < N hops away from both /Users/rubber/linux/kernel/sched/topology.c: 1725
 *   nodes A and B, the system is a glueless mesh. /Users/rubber/linux/kernel/sched/topology.c: 1726
	/* /Users/rubber/linux/kernel/sched/topology.c: 1771
	 * O(nr_nodes^2) deduplicating selection sort -- in order to find the /Users/rubber/linux/kernel/sched/topology.c: 1772
	 * unique distances in the node_distance() table. /Users/rubber/linux/kernel/sched/topology.c: 1773
	/* /Users/rubber/linux/kernel/sched/topology.c: 1792
	 * We can now figure out how many unique distance values there are and /Users/rubber/linux/kernel/sched/topology.c: 1793
	 * allocate memory accordingly. /Users/rubber/linux/kernel/sched/topology.c: 1794
	/* /Users/rubber/linux/kernel/sched/topology.c: 1811
	 * 'nr_levels' contains the number of unique distances /Users/rubber/linux/kernel/sched/topology.c: 1812
	 * /Users/rubber/linux/kernel/sched/topology.c: 1813
	 * The sched_domains_numa_distance[] array includes the actual distance /Users/rubber/linux/kernel/sched/topology.c: 1814
	 * numbers. /Users/rubber/linux/kernel/sched/topology.c: 1815
	/* /Users/rubber/linux/kernel/sched/topology.c: 1818
	 * Here, we should temporarily reset sched_domains_numa_levels to 0. /Users/rubber/linux/kernel/sched/topology.c: 1819
	 * If it fails to allocate memory for array sched_domains_numa_masks[][], /Users/rubber/linux/kernel/sched/topology.c: 1820
	 * the array will contain less then 'nr_levels' members. This could be /Users/rubber/linux/kernel/sched/topology.c: 1821
	 * dangerous when we use it to iterate array sched_domains_numa_masks[][] /Users/rubber/linux/kernel/sched/topology.c: 1822
	 * in other functions. /Users/rubber/linux/kernel/sched/topology.c: 1823
	 * /Users/rubber/linux/kernel/sched/topology.c: 1824
	 * We reset it to 'nr_levels' at the end of this function. /Users/rubber/linux/kernel/sched/topology.c: 1825
	/* /Users/rubber/linux/kernel/sched/topology.c: 1833
	 * Now for each level, construct a mask per node which contains all /Users/rubber/linux/kernel/sched/topology.c: 1834
	 * CPUs of nodes that are that many hops away from us. /Users/rubber/linux/kernel/sched/topology.c: 1835
				/* /Users/rubber/linux/kernel/sched/topology.c: 1853
				 * Distance information can be unreliable for /Users/rubber/linux/kernel/sched/topology.c: 1854
				 * offline nodes, defer building the node /Users/rubber/linux/kernel/sched/topology.c: 1855
				 * masks to its bringup. /Users/rubber/linux/kernel/sched/topology.c: 1856
				 * This relies on all unique distance values /Users/rubber/linux/kernel/sched/topology.c: 1857
				 * still being visible at init time. /Users/rubber/linux/kernel/sched/topology.c: 1858
	/* /Users/rubber/linux/kernel/sched/topology.c: 1882
	 * Copy the default topology bits.. /Users/rubber/linux/kernel/sched/topology.c: 1883
	/* /Users/rubber/linux/kernel/sched/topology.c: 1888
	 * Add the NUMA identity distance, aka single NODE. /Users/rubber/linux/kernel/sched/topology.c: 1889
	/* /Users/rubber/linux/kernel/sched/topology.c: 1897
	 * .. and append 'j' levels of NUMA goodness. /Users/rubber/linux/kernel/sched/topology.c: 1898
	/* /Users/rubber/linux/kernel/sched/topology.c: 1930
	 * NUMA masks are not built for offline nodes in sched_init_numa(). /Users/rubber/linux/kernel/sched/topology.c: 1931
	 * Thus, when a CPU of a never-onlined-before node gets plugged in, /Users/rubber/linux/kernel/sched/topology.c: 1932
	 * adding that new CPU to the right NUMA masks is not sufficient: the /Users/rubber/linux/kernel/sched/topology.c: 1933
	 * masks of that CPU's node must also be updated. /Users/rubber/linux/kernel/sched/topology.c: 1934
	/* /Users/rubber/linux/kernel/sched/topology.c: 1956
	 * A new node has been brought up, potentially changing the topology /Users/rubber/linux/kernel/sched/topology.c: 1957
	 * classification. /Users/rubber/linux/kernel/sched/topology.c: 1958
	 * /Users/rubber/linux/kernel/sched/topology.c: 1959
	 * Note that this is racy vs any use of sched_numa_topology_type :/ /Users/rubber/linux/kernel/sched/topology.c: 1960
 * sched_numa_find_closest() - given the NUMA topology, find the cpu /Users/rubber/linux/kernel/sched/topology.c: 1995
 *                             closest to @cpu from @cpumask. /Users/rubber/linux/kernel/sched/topology.c: 1996
 * cpumask: cpumask to find a cpu from /Users/rubber/linux/kernel/sched/topology.c: 1997
 * cpu: cpu to be close to /Users/rubber/linux/kernel/sched/topology.c: 1998
 * returns: cpu, or nr_cpu_ids when nothing found. /Users/rubber/linux/kernel/sched/topology.c: 2000
 * Ensure topology masks are sane, i.e. there are no conflicts (overlaps) for /Users/rubber/linux/kernel/sched/topology.c: 2152
 * any two given CPUs at this (non-NUMA) topology level. /Users/rubber/linux/kernel/sched/topology.c: 2153
	/* /Users/rubber/linux/kernel/sched/topology.c: 2164
	 * Non-NUMA levels cannot partially overlap - they must be either /Users/rubber/linux/kernel/sched/topology.c: 2165
	 * completely equal or completely disjoint. Otherwise we can end up /Users/rubber/linux/kernel/sched/topology.c: 2166
	 * breaking the sched_group lists - i.e. a later get_group() pass /Users/rubber/linux/kernel/sched/topology.c: 2167
	 * breaks the linking done for an earlier span. /Users/rubber/linux/kernel/sched/topology.c: 2168
		/* /Users/rubber/linux/kernel/sched/topology.c: 2173
		 * We should 'and' all those masks with 'cpu_map' to exactly /Users/rubber/linux/kernel/sched/topology.c: 2174
		 * match the topology we're about to build, but that can only /Users/rubber/linux/kernel/sched/topology.c: 2175
		 * remove CPUs, which only lessens our ability to detect /Users/rubber/linux/kernel/sched/topology.c: 2176
		 * overlaps /Users/rubber/linux/kernel/sched/topology.c: 2177
 * Build sched domains for a given set of CPUs and attach the sched domains /Users/rubber/linux/kernel/sched/topology.c: 2188
 * to the individual CPUs /Users/rubber/linux/kernel/sched/topology.c: 2189
 * Special case: If a kmalloc() of a doms_cur partition (array of /Users/rubber/linux/kernel/sched/topology.c: 2295
 * cpumask) fails, then fallback to a single sched domain, /Users/rubber/linux/kernel/sched/topology.c: 2296
 * as determined by the single cpumask fallback_doms. /Users/rubber/linux/kernel/sched/topology.c: 2297
 * arch_update_cpu_topology lets virtualized architectures update the /Users/rubber/linux/kernel/sched/topology.c: 2302
 * CPU core maps. It is supposed to return 1 if the topology changed /Users/rubber/linux/kernel/sched/topology.c: 2303
 * or 0 if it stayed the same. /Users/rubber/linux/kernel/sched/topology.c: 2304
 * Set up scheduler domains and groups.  For now this just excludes isolated /Users/rubber/linux/kernel/sched/topology.c: 2337
 * CPUs, but could be used to exclude other special cases in the future. /Users/rubber/linux/kernel/sched/topology.c: 2338
 * Detach sched domains from a group of CPUs specified in cpu_map /Users/rubber/linux/kernel/sched/topology.c: 2361
 * These CPUs will now be attached to the NULL domain /Users/rubber/linux/kernel/sched/topology.c: 2362
 * Partition sched domains as specified by the 'ndoms_new' /Users/rubber/linux/kernel/sched/topology.c: 2396
 * cpumasks in the array doms_new[] of cpumasks. This compares /Users/rubber/linux/kernel/sched/topology.c: 2397
 * doms_new[] to the current sched domain partitioning, doms_cur[]. /Users/rubber/linux/kernel/sched/topology.c: 2398
 * It destroys each deleted domain and builds each new domain. /Users/rubber/linux/kernel/sched/topology.c: 2399
 * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'. /Users/rubber/linux/kernel/sched/topology.c: 2401
 * The masks don't intersect (don't overlap.) We should setup one /Users/rubber/linux/kernel/sched/topology.c: 2402
 * sched domain for each mask. CPUs not in any of the cpumasks will /Users/rubber/linux/kernel/sched/topology.c: 2403
 * not be load balanced. If the same cpumask appears both in the /Users/rubber/linux/kernel/sched/topology.c: 2404
 * current 'doms_cur' domains and in the new 'doms_new', we can leave /Users/rubber/linux/kernel/sched/topology.c: 2405
 * it as it is. /Users/rubber/linux/kernel/sched/topology.c: 2406
 * The passed in 'doms_new' should be allocated using /Users/rubber/linux/kernel/sched/topology.c: 2408
 * alloc_sched_domains.  This routine takes ownership of it and will /Users/rubber/linux/kernel/sched/topology.c: 2409
 * free_sched_domains it when done with it. If the caller failed the /Users/rubber/linux/kernel/sched/topology.c: 2410
 * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1, /Users/rubber/linux/kernel/sched/topology.c: 2411
 * and partition_sched_domains() will fallback to the single partition /Users/rubber/linux/kernel/sched/topology.c: 2412
 * 'fallback_doms', it also forces the domains to be rebuilt. /Users/rubber/linux/kernel/sched/topology.c: 2413
 * If doms_new == NULL it will be replaced with cpu_online_mask. /Users/rubber/linux/kernel/sched/topology.c: 2415
 * ndoms_new == 0 is a special case for destroying existing domains, /Users/rubber/linux/kernel/sched/topology.c: 2416
 * and it will not create the default domain. /Users/rubber/linux/kernel/sched/topology.c: 2417
 * Call with hotplug lock and sched_domains_mutex held /Users/rubber/linux/kernel/sched/topology.c: 2419
				/* /Users/rubber/linux/kernel/sched/topology.c: 2456
				 * This domain won't be destroyed and as such /Users/rubber/linux/kernel/sched/topology.c: 2457
				 * its dl_bw->total_bw needs to be cleared.  It /Users/rubber/linux/kernel/sched/topology.c: 2458
				 * will be recomputed in function /Users/rubber/linux/kernel/sched/topology.c: 2459
				 * update_tasks_root_domain(). /Users/rubber/linux/kernel/sched/topology.c: 2460
 * Call with hotplug lock held /Users/rubber/linux/kernel/sched/topology.c: 2525
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/stats.c: 1
 * /proc/schedstat implementation /Users/rubber/linux/kernel/sched/stats.c: 3
			/* /Users/rubber/linux/kernel/sched/stats.c: 28
			 * Preserve migrating task's wait time so wait_start /Users/rubber/linux/kernel/sched/stats.c: 29
			 * time stamp can be adjusted to accumulate wait time /Users/rubber/linux/kernel/sched/stats.c: 30
			 * prior to migration. /Users/rubber/linux/kernel/sched/stats.c: 31
			/* /Users/rubber/linux/kernel/sched/stats.c: 96
			 * Blocking time is in units of nanosecs, so shift by /Users/rubber/linux/kernel/sched/stats.c: 97
			 * 20 to get a milliseconds-range estimation of the /Users/rubber/linux/kernel/sched/stats.c: 98
			 * amount of time that the task spent sleeping: /Users/rubber/linux/kernel/sched/stats.c: 99
 * Current schedstat API version. /Users/rubber/linux/kernel/sched/stats.c: 112
 * Bump this up when changing the output format or the meaning of an existing /Users/rubber/linux/kernel/sched/stats.c: 114
 * format, so that tools can adapt (or abort) /Users/rubber/linux/kernel/sched/stats.c: 115
 * This iterator needs some explanation. /Users/rubber/linux/kernel/sched/stats.c: 181
 * It returns 1 for the header position. /Users/rubber/linux/kernel/sched/stats.c: 182
 * This means 2 is cpu 0. /Users/rubber/linux/kernel/sched/stats.c: 183
 * In a hotplugged system some CPUs, including cpu 0, may be missing so we have /Users/rubber/linux/kernel/sched/stats.c: 184
 * to use cpumask_* to iterate over the CPUs. /Users/rubber/linux/kernel/sched/stats.c: 185
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/clock.c: 1
 * sched_clock() for unstable CPU clocks /Users/rubber/linux/kernel/sched/clock.c: 3
 *  Copyright (C) 2008 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/sched/clock.c: 5
 *  Updates and enhancements: /Users/rubber/linux/kernel/sched/clock.c: 7
 *    Copyright (C) 2008 Red Hat, Inc. Steven Rostedt <srostedt@redhat.com> /Users/rubber/linux/kernel/sched/clock.c: 8
 * Based on code by: /Users/rubber/linux/kernel/sched/clock.c: 10
 *   Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/sched/clock.c: 11
 *   Guillaume Chazarain <guichaz@gmail.com> /Users/rubber/linux/kernel/sched/clock.c: 12
 * What this file implements: /Users/rubber/linux/kernel/sched/clock.c: 15
 * cpu_clock(i) provides a fast (execution time) high resolution /Users/rubber/linux/kernel/sched/clock.c: 17
 * clock with bounded drift between CPUs. The value of cpu_clock(i) /Users/rubber/linux/kernel/sched/clock.c: 18
 * is monotonic for constant i. The timestamp returned is in nanoseconds. /Users/rubber/linux/kernel/sched/clock.c: 19
 * ######################### BIG FAT WARNING ########################## /Users/rubber/linux/kernel/sched/clock.c: 21
 * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can # /Users/rubber/linux/kernel/sched/clock.c: 22
 * # go backwards !!                                                  # /Users/rubber/linux/kernel/sched/clock.c: 23
 * #################################################################### /Users/rubber/linux/kernel/sched/clock.c: 24
 * There is no strict promise about the base, although it tends to start /Users/rubber/linux/kernel/sched/clock.c: 26
 * at 0 on boot (but people really shouldn't rely on that). /Users/rubber/linux/kernel/sched/clock.c: 27
 * cpu_clock(i)       -- can be used from any context, including NMI. /Users/rubber/linux/kernel/sched/clock.c: 29
 * local_clock()      -- is cpu_clock() on the current CPU. /Users/rubber/linux/kernel/sched/clock.c: 30
 * sched_clock_cpu(i) /Users/rubber/linux/kernel/sched/clock.c: 32
 * How it is implemented: /Users/rubber/linux/kernel/sched/clock.c: 34
 * The implementation either uses sched_clock() when /Users/rubber/linux/kernel/sched/clock.c: 36
 * !CONFIG_HAVE_UNSTABLE_SCHED_CLOCK, which means in that case the /Users/rubber/linux/kernel/sched/clock.c: 37
 * sched_clock() is assumed to provide these properties (mostly it means /Users/rubber/linux/kernel/sched/clock.c: 38
 * the architecture provides a globally synchronized highres time source). /Users/rubber/linux/kernel/sched/clock.c: 39
 * Otherwise it tries to create a semi stable clock from a mixture of other /Users/rubber/linux/kernel/sched/clock.c: 41
 * clocks, including: /Users/rubber/linux/kernel/sched/clock.c: 42
 *  - GTOD (clock monotonic) /Users/rubber/linux/kernel/sched/clock.c: 44
 *  - sched_clock() /Users/rubber/linux/kernel/sched/clock.c: 45
 *  - explicit idle events /Users/rubber/linux/kernel/sched/clock.c: 46
 * We use GTOD as base and use sched_clock() deltas to improve resolution. The /Users/rubber/linux/kernel/sched/clock.c: 48
 * deltas are filtered to provide monotonicity and keeping it within an /Users/rubber/linux/kernel/sched/clock.c: 49
 * expected window. /Users/rubber/linux/kernel/sched/clock.c: 50
 * Furthermore, explicit sleep and wakeup hooks allow us to account for time /Users/rubber/linux/kernel/sched/clock.c: 52
 * that is otherwise invisible (TSC gets stopped). /Users/rubber/linux/kernel/sched/clock.c: 53
 * Scheduler clock - returns current time in nanosec units. /Users/rubber/linux/kernel/sched/clock.c: 60
 * This is default implementation. /Users/rubber/linux/kernel/sched/clock.c: 61
 * Architectures and sub-architectures can override this. /Users/rubber/linux/kernel/sched/clock.c: 62
 * We must start with !__sched_clock_stable because the unstable -> stable /Users/rubber/linux/kernel/sched/clock.c: 75
 * transition is accurate, while the stable -> unstable transition is not. /Users/rubber/linux/kernel/sched/clock.c: 76
 * Similarly we start with __sched_clock_stable_early, thereby assuming we /Users/rubber/linux/kernel/sched/clock.c: 78
 * will become stable, such that there's only a single 1 -> 0 transition. /Users/rubber/linux/kernel/sched/clock.c: 79
 * We want: ktime_get_ns() + __gtod_offset == sched_clock() + __sched_clock_offset /Users/rubber/linux/kernel/sched/clock.c: 85
	/* /Users/rubber/linux/kernel/sched/clock.c: 123
	 * Since we're still unstable and the tick is already running, we have /Users/rubber/linux/kernel/sched/clock.c: 124
	 * to disable IRQs in order to get a consistent scd->tick* reading. /Users/rubber/linux/kernel/sched/clock.c: 125
	/* /Users/rubber/linux/kernel/sched/clock.c: 129
	 * Attempt to make the (initial) unstable->stable transition continuous. /Users/rubber/linux/kernel/sched/clock.c: 130
 * If we ever get here, we're screwed, because we found out -- typically after /Users/rubber/linux/kernel/sched/clock.c: 144
 * the fact -- that TSC wasn't good. This means all our clocksources (including /Users/rubber/linux/kernel/sched/clock.c: 145
 * ktime) could have reported wrong values. /Users/rubber/linux/kernel/sched/clock.c: 146
 * What we do here is an attempt to fix up and continue sort of where we left /Users/rubber/linux/kernel/sched/clock.c: 148
 * off in a coherent manner. /Users/rubber/linux/kernel/sched/clock.c: 149
 * The only way to fully avoid random clock jumps is to boot with: /Users/rubber/linux/kernel/sched/clock.c: 151
 * "tsc=unstable". /Users/rubber/linux/kernel/sched/clock.c: 152
	/* /Users/rubber/linux/kernel/sched/clock.c: 209
	 * Set __gtod_offset such that once we mark sched_clock_running, /Users/rubber/linux/kernel/sched/clock.c: 210
	 * sched_clock_tick() continues where sched_clock() left off. /Users/rubber/linux/kernel/sched/clock.c: 211
	 * /Users/rubber/linux/kernel/sched/clock.c: 212
	 * Even if TSC is buggered, we're still UP at this point so it /Users/rubber/linux/kernel/sched/clock.c: 213
	 * can't really be out of sync. /Users/rubber/linux/kernel/sched/clock.c: 214
 * We run this as late_initcall() such that it runs after all built-in drivers, /Users/rubber/linux/kernel/sched/clock.c: 223
 * notably: acpi_processor and intel_idle, which can mark the TSC as unstable. /Users/rubber/linux/kernel/sched/clock.c: 224
	/* /Users/rubber/linux/kernel/sched/clock.c: 229
	 * Ensure that it is impossible to not do a static_key update. /Users/rubber/linux/kernel/sched/clock.c: 230
	 * /Users/rubber/linux/kernel/sched/clock.c: 231
	 * Either {set,clear}_sched_clock_stable() must see sched_clock_running /Users/rubber/linux/kernel/sched/clock.c: 232
	 * and do the update, or we must see their __sched_clock_stable_early /Users/rubber/linux/kernel/sched/clock.c: 233
	 * and do the update, or both. /Users/rubber/linux/kernel/sched/clock.c: 234
 * min, max except they take wrapping into account /Users/rubber/linux/kernel/sched/clock.c: 246
 * update the percpu scd from the raw @now value /Users/rubber/linux/kernel/sched/clock.c: 260
 *  - filter out backward motion /Users/rubber/linux/kernel/sched/clock.c: 262
 *  - use the GTOD tick value to create a window to filter crazy TSC values /Users/rubber/linux/kernel/sched/clock.c: 263
	/* /Users/rubber/linux/kernel/sched/clock.c: 278
	 * scd->clock = clamp(scd->tick_gtod + delta, /Users/rubber/linux/kernel/sched/clock.c: 279
	 *		      max(scd->tick_gtod, scd->clock), /Users/rubber/linux/kernel/sched/clock.c: 280
	 *		      scd->tick_gtod + TICK_NSEC); /Users/rubber/linux/kernel/sched/clock.c: 281
	/* /Users/rubber/linux/kernel/sched/clock.c: 306
	 * Careful here: The local and the remote clock values need to /Users/rubber/linux/kernel/sched/clock.c: 307
	 * be read out atomic as we need to compare the values and /Users/rubber/linux/kernel/sched/clock.c: 308
	 * then update either the local or the remote side. So the /Users/rubber/linux/kernel/sched/clock.c: 309
	 * cmpxchg64 below only protects one readout. /Users/rubber/linux/kernel/sched/clock.c: 310
	 * /Users/rubber/linux/kernel/sched/clock.c: 311
	 * We must reread via sched_clock_local() in the retry case on /Users/rubber/linux/kernel/sched/clock.c: 312
	 * 32-bit kernels as an NMI could use sched_clock_local() via the /Users/rubber/linux/kernel/sched/clock.c: 313
	 * tracer and hit between the readout of /Users/rubber/linux/kernel/sched/clock.c: 314
	 * the low 32-bit and the high 32-bit portion. /Users/rubber/linux/kernel/sched/clock.c: 315
	/* /Users/rubber/linux/kernel/sched/clock.c: 318
	 * We must enforce atomic readout on 32-bit, otherwise the /Users/rubber/linux/kernel/sched/clock.c: 319
	 * update on the remote CPU can hit inbetween the readout of /Users/rubber/linux/kernel/sched/clock.c: 320
	 * the low 32-bit and the high 32-bit portion. /Users/rubber/linux/kernel/sched/clock.c: 321
	/* /Users/rubber/linux/kernel/sched/clock.c: 325
	 * On 64-bit kernels the read of [my]scd->clock is atomic versus the /Users/rubber/linux/kernel/sched/clock.c: 326
	 * update, so we can avoid the above 32-bit dance. /Users/rubber/linux/kernel/sched/clock.c: 327
	/* /Users/rubber/linux/kernel/sched/clock.c: 335
	 * Use the opportunity that we have both locks /Users/rubber/linux/kernel/sched/clock.c: 336
	 * taken to couple the two clocks: we take the /Users/rubber/linux/kernel/sched/clock.c: 337
	 * larger time as the latest time for both /Users/rubber/linux/kernel/sched/clock.c: 338
	 * runqueues. (this creates monotonic movement) /Users/rubber/linux/kernel/sched/clock.c: 339
		/* /Users/rubber/linux/kernel/sched/clock.c: 346
		 * Should be rare, but possible: /Users/rubber/linux/kernel/sched/clock.c: 347
 * Similar to cpu_clock(), but requires local IRQs to be disabled. /Users/rubber/linux/kernel/sched/clock.c: 361
 * See cpu_clock(). /Users/rubber/linux/kernel/sched/clock.c: 363
	/* /Users/rubber/linux/kernel/sched/clock.c: 411
	 * Called under watchdog_lock. /Users/rubber/linux/kernel/sched/clock.c: 412
	 * /Users/rubber/linux/kernel/sched/clock.c: 413
	 * The watchdog just found this TSC to (still) be stable, so now is a /Users/rubber/linux/kernel/sched/clock.c: 414
	 * good moment to update our __gtod_offset. Because once we find the /Users/rubber/linux/kernel/sched/clock.c: 415
	 * TSC to be unstable, any computation will be computing crap. /Users/rubber/linux/kernel/sched/clock.c: 416
 * We are going deep-idle (irqs are disabled): /Users/rubber/linux/kernel/sched/clock.c: 424
 * We just idled; resync with ktime. /Users/rubber/linux/kernel/sched/clock.c: 433
 * Running clock - returns the time that has elapsed while a guest has been /Users/rubber/linux/kernel/sched/clock.c: 472
 * running. /Users/rubber/linux/kernel/sched/clock.c: 473
 * On a guest this value should be local_clock minus the time the guest was /Users/rubber/linux/kernel/sched/clock.c: 474
 * suspended by the hypervisor (for any reason). /Users/rubber/linux/kernel/sched/clock.c: 475
 * On bare metal this function should return the same as local_clock. /Users/rubber/linux/kernel/sched/clock.c: 476
 * Architectures and sub-architectures can override this. /Users/rubber/linux/kernel/sched/clock.c: 477
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/core.c: 1
 *  kernel/sched/core.c /Users/rubber/linux/kernel/sched/core.c: 3
 *  Core kernel scheduler code and related syscalls /Users/rubber/linux/kernel/sched/core.c: 5
 *  Copyright (C) 1991-2002  Linus Torvalds /Users/rubber/linux/kernel/sched/core.c: 7
 * Export tracepoints that act as a bare tracehook (ie: have no trace event /Users/rubber/linux/kernel/sched/core.c: 31
 * associated with them) to allow external modules to probe them. /Users/rubber/linux/kernel/sched/core.c: 32
 * Debugging: various feature bits /Users/rubber/linux/kernel/sched/core.c: 49
 * If SCHED_DEBUG is disabled, each compilation unit has its own copy of /Users/rubber/linux/kernel/sched/core.c: 51
 * sysctl_sched_features, defined in sched.h, to allow constants propagation /Users/rubber/linux/kernel/sched/core.c: 52
 * at compile time and compiler optimization based on features default. /Users/rubber/linux/kernel/sched/core.c: 53
 * Print a warning if need_resched is set for the given duration (if /Users/rubber/linux/kernel/sched/core.c: 63
 * LATENCY_WARN is enabled). /Users/rubber/linux/kernel/sched/core.c: 64
 * If sysctl_resched_latency_warn_once is set, only one warning will be shown /Users/rubber/linux/kernel/sched/core.c: 66
 * per boot. /Users/rubber/linux/kernel/sched/core.c: 67
 * Number of tasks to iterate in a single balance run. /Users/rubber/linux/kernel/sched/core.c: 74
 * Limited because this is done with IRQs disabled. /Users/rubber/linux/kernel/sched/core.c: 75
 * period over which we measure -rt task CPU usage in us. /Users/rubber/linux/kernel/sched/core.c: 84
 * default: 1s /Users/rubber/linux/kernel/sched/core.c: 85
 * l(a,b) /Users/rubber/linux/kernel/sched/core.c: 111
 * le(a,b) := !l(b,a) /Users/rubber/linux/kernel/sched/core.c: 112
 * g(a,b)  := l(b,a) /Users/rubber/linux/kernel/sched/core.c: 113
 * ge(a,b) := !l(a,b) /Users/rubber/linux/kernel/sched/core.c: 114
 * Find left-most (aka, highest priority) task matching @cookie. /Users/rubber/linux/kernel/sched/core.c: 196
	/* /Users/rubber/linux/kernel/sched/core.c: 203
	 * The idle task always matches any cookie! /Users/rubber/linux/kernel/sched/core.c: 204
 * Magic required such that: /Users/rubber/linux/kernel/sched/core.c: 228
 *	raw_spin_rq_lock(rq); /Users/rubber/linux/kernel/sched/core.c: 230
 *	... /Users/rubber/linux/kernel/sched/core.c: 231
 *	raw_spin_rq_unlock(rq); /Users/rubber/linux/kernel/sched/core.c: 232
 * ends up locking and unlocking the _same_ lock, and all CPUs /Users/rubber/linux/kernel/sched/core.c: 234
 * always agree on what rq has what lock. /Users/rubber/linux/kernel/sched/core.c: 235
 * XXX entirely possible to selectively enable cores, don't bother for now. /Users/rubber/linux/kernel/sched/core.c: 237
	/* /Users/rubber/linux/kernel/sched/core.c: 271
	 * Toggle the online cores, one by one. /Users/rubber/linux/kernel/sched/core.c: 272
	/* /Users/rubber/linux/kernel/sched/core.c: 288
	 * Toggle the offline CPUs. /Users/rubber/linux/kernel/sched/core.c: 289
	/* /Users/rubber/linux/kernel/sched/core.c: 311
	 * Ensure all previous instances of raw_spin_rq_*lock() have finished /Users/rubber/linux/kernel/sched/core.c: 312
	 * and future ones will observe !sched_core_disabled(). /Users/rubber/linux/kernel/sched/core.c: 313
	/* /Users/rubber/linux/kernel/sched/core.c: 353
	 * "There can be only one" /Users/rubber/linux/kernel/sched/core.c: 354
	 * /Users/rubber/linux/kernel/sched/core.c: 355
	 * Either this is the last one, or we don't actually need to do any /Users/rubber/linux/kernel/sched/core.c: 356
	 * 'work'. If it is the last *again*, we rely on /Users/rubber/linux/kernel/sched/core.c: 357
	 * WORK_STRUCT_PENDING_BIT. /Users/rubber/linux/kernel/sched/core.c: 358
 * part of the period that we allow rt tasks to run in us. /Users/rubber/linux/kernel/sched/core.c: 372
 * default: 0.95s /Users/rubber/linux/kernel/sched/core.c: 373
 * Serialization rules: /Users/rubber/linux/kernel/sched/core.c: 379
 * Lock order: /Users/rubber/linux/kernel/sched/core.c: 381
 *   p->pi_lock /Users/rubber/linux/kernel/sched/core.c: 383
 *     rq->lock /Users/rubber/linux/kernel/sched/core.c: 384
 *       hrtimer_cpu_base->lock (hrtimer_start() for bandwidth controls) /Users/rubber/linux/kernel/sched/core.c: 385
 *  rq1->lock /Users/rubber/linux/kernel/sched/core.c: 387
 *    rq2->lock  where: rq1 < rq2 /Users/rubber/linux/kernel/sched/core.c: 388
 * Regular state: /Users/rubber/linux/kernel/sched/core.c: 390
 * Normal scheduling state is serialized by rq->lock. __schedule() takes the /Users/rubber/linux/kernel/sched/core.c: 392
 * local CPU's rq->lock, it optionally removes the task from the runqueue and /Users/rubber/linux/kernel/sched/core.c: 393
 * always looks at the local rq data structures to find the most eligible task /Users/rubber/linux/kernel/sched/core.c: 394
 * to run next. /Users/rubber/linux/kernel/sched/core.c: 395
 * Task enqueue is also under rq->lock, possibly taken from another CPU. /Users/rubber/linux/kernel/sched/core.c: 397
 * Wakeups from another LLC domain might use an IPI to transfer the enqueue to /Users/rubber/linux/kernel/sched/core.c: 398
 * the local CPU to avoid bouncing the runqueue state around [ see /Users/rubber/linux/kernel/sched/core.c: 399
 * ttwu_queue_wakelist() ] /Users/rubber/linux/kernel/sched/core.c: 400
 * Task wakeup, specifically wakeups that involve migration, are horribly /Users/rubber/linux/kernel/sched/core.c: 402
 * complicated to avoid having to take two rq->locks. /Users/rubber/linux/kernel/sched/core.c: 403
 * Special state: /Users/rubber/linux/kernel/sched/core.c: 405
 * System-calls and anything external will use task_rq_lock() which acquires /Users/rubber/linux/kernel/sched/core.c: 407
 * both p->pi_lock and rq->lock. As a consequence the state they change is /Users/rubber/linux/kernel/sched/core.c: 408
 * stable while holding either lock: /Users/rubber/linux/kernel/sched/core.c: 409
 *  - sched_setaffinity()/ /Users/rubber/linux/kernel/sched/core.c: 411
 *    set_cpus_allowed_ptr():	p->cpus_ptr, p->nr_cpus_allowed /Users/rubber/linux/kernel/sched/core.c: 412
 *  - set_user_nice():		p->se.load, p->*prio /Users/rubber/linux/kernel/sched/core.c: 413
 *  - __sched_setscheduler():	p->sched_class, p->policy, p->*prio, /Users/rubber/linux/kernel/sched/core.c: 414
 *				p->se.load, p->rt_priority, /Users/rubber/linux/kernel/sched/core.c: 415
 *				p->dl.dl_{runtime, deadline, period, flags, bw, density} /Users/rubber/linux/kernel/sched/core.c: 416
 *  - sched_setnuma():		p->numa_preferred_nid /Users/rubber/linux/kernel/sched/core.c: 417
 *  - sched_move_task()/ /Users/rubber/linux/kernel/sched/core.c: 418
 *    cpu_cgroup_fork():	p->sched_task_group /Users/rubber/linux/kernel/sched/core.c: 419
 *  - uclamp_update_active()	p->uclamp* /Users/rubber/linux/kernel/sched/core.c: 420
 * p->state <- TASK_*: /Users/rubber/linux/kernel/sched/core.c: 422
 *   is changed locklessly using set_current_state(), __set_current_state() or /Users/rubber/linux/kernel/sched/core.c: 424
 *   set_special_state(), see their respective comments, or by /Users/rubber/linux/kernel/sched/core.c: 425
 *   try_to_wake_up(). This latter uses p->pi_lock to serialize against /Users/rubber/linux/kernel/sched/core.c: 426
 *   concurrent self. /Users/rubber/linux/kernel/sched/core.c: 427
 * p->on_rq <- { 0, 1 = TASK_ON_RQ_QUEUED, 2 = TASK_ON_RQ_MIGRATING }: /Users/rubber/linux/kernel/sched/core.c: 429
 *   is set by activate_task() and cleared by deactivate_task(), under /Users/rubber/linux/kernel/sched/core.c: 431
 *   rq->lock. Non-zero indicates the task is runnable, the special /Users/rubber/linux/kernel/sched/core.c: 432
 *   ON_RQ_MIGRATING state is used for migration without holding both /Users/rubber/linux/kernel/sched/core.c: 433
 *   rq->locks. It indicates task_cpu() is not stable, see task_rq_lock(). /Users/rubber/linux/kernel/sched/core.c: 434
 * p->on_cpu <- { 0, 1 }: /Users/rubber/linux/kernel/sched/core.c: 436
 *   is set by prepare_task() and cleared by finish_task() such that it will be /Users/rubber/linux/kernel/sched/core.c: 438
 *   set before p is scheduled-in and cleared after p is scheduled-out, both /Users/rubber/linux/kernel/sched/core.c: 439
 *   under rq->lock. Non-zero indicates the task is running on its CPU. /Users/rubber/linux/kernel/sched/core.c: 440
 *   [ The astute reader will observe that it is possible for two tasks on one /Users/rubber/linux/kernel/sched/core.c: 442
 *     CPU to have ->on_cpu = 1 at the same time. ] /Users/rubber/linux/kernel/sched/core.c: 443
 * task_cpu(p): is changed by set_task_cpu(), the rules are: /Users/rubber/linux/kernel/sched/core.c: 445
 *  - Don't call set_task_cpu() on a blocked task: /Users/rubber/linux/kernel/sched/core.c: 447
 *    We don't care what CPU we're not running on, this simplifies hotplug, /Users/rubber/linux/kernel/sched/core.c: 449
 *    the CPU assignment of blocked tasks isn't required to be valid. /Users/rubber/linux/kernel/sched/core.c: 450
 *  - for try_to_wake_up(), called under p->pi_lock: /Users/rubber/linux/kernel/sched/core.c: 452
 *    This allows try_to_wake_up() to only take one rq->lock, see its comment. /Users/rubber/linux/kernel/sched/core.c: 454
 *  - for migration called under rq->lock: /Users/rubber/linux/kernel/sched/core.c: 456
 *    [ see task_on_rq_migrating() in task_rq_lock() ] /Users/rubber/linux/kernel/sched/core.c: 457
 *    o move_queued_task() /Users/rubber/linux/kernel/sched/core.c: 459
 *    o detach_task() /Users/rubber/linux/kernel/sched/core.c: 460
 *  - for migration called under double_rq_lock(): /Users/rubber/linux/kernel/sched/core.c: 462
 *    o __migrate_swap_task() /Users/rubber/linux/kernel/sched/core.c: 464
 *    o push_rt_task() / pull_rt_task() /Users/rubber/linux/kernel/sched/core.c: 465
 *    o push_dl_task() / pull_dl_task() /Users/rubber/linux/kernel/sched/core.c: 466
 *    o dl_task_offline_migration() /Users/rubber/linux/kernel/sched/core.c: 467
 * double_rq_lock - safely lock two runqueues /Users/rubber/linux/kernel/sched/core.c: 527
 * __task_rq_lock - lock the rq @p resides on. /Users/rubber/linux/kernel/sched/core.c: 545
 * task_rq_lock - lock p->pi_lock and lock the rq @p resides on. /Users/rubber/linux/kernel/sched/core.c: 569
		/* /Users/rubber/linux/kernel/sched/core.c: 581
		 *	move_queued_task()		task_rq_lock() /Users/rubber/linux/kernel/sched/core.c: 582
		 * /Users/rubber/linux/kernel/sched/core.c: 583
		 *	ACQUIRE (rq->lock) /Users/rubber/linux/kernel/sched/core.c: 584
		 *	[S] ->on_rq = MIGRATING		[L] rq = task_rq() /Users/rubber/linux/kernel/sched/core.c: 585
		 *	WMB (__set_task_cpu())		ACQUIRE (rq->lock); /Users/rubber/linux/kernel/sched/core.c: 586
		 *	[S] ->cpu = new_cpu		[L] task_rq() /Users/rubber/linux/kernel/sched/core.c: 587
		 *					[L] ->on_rq /Users/rubber/linux/kernel/sched/core.c: 588
		 *	RELEASE (rq->lock) /Users/rubber/linux/kernel/sched/core.c: 589
		 * /Users/rubber/linux/kernel/sched/core.c: 590
		 * If we observe the old CPU in task_rq_lock(), the acquire of /Users/rubber/linux/kernel/sched/core.c: 591
		 * the old rq->lock will fully serialize against the stores. /Users/rubber/linux/kernel/sched/core.c: 592
		 * /Users/rubber/linux/kernel/sched/core.c: 593
		 * If we observe the new CPU in task_rq_lock(), the address /Users/rubber/linux/kernel/sched/core.c: 594
		 * dependency headed by '[L] rq = task_rq()' and the acquire /Users/rubber/linux/kernel/sched/core.c: 595
		 * will pair with the WMB to ensure we then also see migrating. /Users/rubber/linux/kernel/sched/core.c: 596
 * RQ-clock updating methods: /Users/rubber/linux/kernel/sched/core.c: 611
 * In theory, the compile should just see 0 here, and optimize out the call /Users/rubber/linux/kernel/sched/core.c: 617
 * to sched_rt_avg_update. But I don't trust it... /Users/rubber/linux/kernel/sched/core.c: 618
	/* /Users/rubber/linux/kernel/sched/core.c: 625
	 * Since irq_time is only updated on {soft,}irq_exit, we might run into /Users/rubber/linux/kernel/sched/core.c: 626
	 * this case when a previous update_rq_clock() happened inside a /Users/rubber/linux/kernel/sched/core.c: 627
	 * {soft,}irq region. /Users/rubber/linux/kernel/sched/core.c: 628
	 * /Users/rubber/linux/kernel/sched/core.c: 629
	 * When this happens, we stop ->clock_task and only update the /Users/rubber/linux/kernel/sched/core.c: 630
	 * prev_irq_time stamp to account for the part that fit, so that a next /Users/rubber/linux/kernel/sched/core.c: 631
	 * update will consume the rest. This ensures ->clock_task is /Users/rubber/linux/kernel/sched/core.c: 632
	 * monotonic. /Users/rubber/linux/kernel/sched/core.c: 633
	 * /Users/rubber/linux/kernel/sched/core.c: 634
	 * It does however cause some slight miss-attribution of {soft,}irq /Users/rubber/linux/kernel/sched/core.c: 635
	 * time, a more accurate solution would be to update the irq_time using /Users/rubber/linux/kernel/sched/core.c: 636
	 * the current rq->clock timestamp, except that would require using /Users/rubber/linux/kernel/sched/core.c: 637
	 * atomic ops. /Users/rubber/linux/kernel/sched/core.c: 638
 * Use HR-timers to deliver accurate preemption points. /Users/rubber/linux/kernel/sched/core.c: 692
 * High-resolution timer tick. /Users/rubber/linux/kernel/sched/core.c: 702
 * Runs from hardirq context with interrupts disabled. /Users/rubber/linux/kernel/sched/core.c: 703
 * called from hardirq (IPI) context /Users/rubber/linux/kernel/sched/core.c: 731
 * Called to set the hrtick timer state. /Users/rubber/linux/kernel/sched/core.c: 744
 * called with rq->lock held and irqs disabled /Users/rubber/linux/kernel/sched/core.c: 746
	/* /Users/rubber/linux/kernel/sched/core.c: 753
	 * Don't schedule slices shorter than 10000ns, that just /Users/rubber/linux/kernel/sched/core.c: 754
	 * doesn't make sense and can cause timer DoS. /Users/rubber/linux/kernel/sched/core.c: 755
 * Called to set the hrtick timer state. /Users/rubber/linux/kernel/sched/core.c: 768
 * called with rq->lock held and irqs disabled /Users/rubber/linux/kernel/sched/core.c: 770
	/* /Users/rubber/linux/kernel/sched/core.c: 774
	 * Don't schedule slices shorter than 10000ns, that just /Users/rubber/linux/kernel/sched/core.c: 775
	 * doesn't make sense. Rely on vruntime for fairness. /Users/rubber/linux/kernel/sched/core.c: 776
 * cmpxchg based fetch_or, macro so it works for different integer types /Users/rubber/linux/kernel/sched/core.c: 804
 * Atomically set TIF_NEED_RESCHED and test for TIF_POLLING_NRFLAG, /Users/rubber/linux/kernel/sched/core.c: 823
 * this avoids any races wrt polling state changes and thereby avoids /Users/rubber/linux/kernel/sched/core.c: 824
 * spurious IPIs. /Users/rubber/linux/kernel/sched/core.c: 825
 * Atomically set TIF_NEED_RESCHED if TIF_POLLING_NRFLAG is set. /Users/rubber/linux/kernel/sched/core.c: 834
 * If this returns true, then the idle task promises to call /Users/rubber/linux/kernel/sched/core.c: 836
 * sched_ttwu_pending() and reschedule soon. /Users/rubber/linux/kernel/sched/core.c: 837
	/* /Users/rubber/linux/kernel/sched/core.c: 876
	 * Atomically grab the task, if ->wake_q is !nil already it means /Users/rubber/linux/kernel/sched/core.c: 877
	 * it's already queued (either by us or someone else) and will get the /Users/rubber/linux/kernel/sched/core.c: 878
	 * wakeup due to that. /Users/rubber/linux/kernel/sched/core.c: 879
	 * /Users/rubber/linux/kernel/sched/core.c: 880
	 * In order to ensure that a pending wakeup will observe our pending /Users/rubber/linux/kernel/sched/core.c: 881
	 * state, even in the failed case, an explicit smp_mb() must be used. /Users/rubber/linux/kernel/sched/core.c: 882
	/* /Users/rubber/linux/kernel/sched/core.c: 888
	 * The head is context local, there can be no concurrency. /Users/rubber/linux/kernel/sched/core.c: 889
 * wake_q_add() - queue a wakeup for 'later' waking. /Users/rubber/linux/kernel/sched/core.c: 897
 * @head: the wake_q_head to add @task to /Users/rubber/linux/kernel/sched/core.c: 898
 * @task: the task to queue for 'later' wakeup /Users/rubber/linux/kernel/sched/core.c: 899
 * Queue a task for later wakeup, most likely by the wake_up_q() call in the /Users/rubber/linux/kernel/sched/core.c: 901
 * same context, _HOWEVER_ this is not guaranteed, the wakeup can come /Users/rubber/linux/kernel/sched/core.c: 902
 * instantly. /Users/rubber/linux/kernel/sched/core.c: 903
 * This function must be used as-if it were wake_up_process(); IOW the task /Users/rubber/linux/kernel/sched/core.c: 905
 * must be ready to be woken at this location. /Users/rubber/linux/kernel/sched/core.c: 906
 * wake_q_add_safe() - safely queue a wakeup for 'later' waking. /Users/rubber/linux/kernel/sched/core.c: 915
 * @head: the wake_q_head to add @task to /Users/rubber/linux/kernel/sched/core.c: 916
 * @task: the task to queue for 'later' wakeup /Users/rubber/linux/kernel/sched/core.c: 917
 * Queue a task for later wakeup, most likely by the wake_up_q() call in the /Users/rubber/linux/kernel/sched/core.c: 919
 * same context, _HOWEVER_ this is not guaranteed, the wakeup can come /Users/rubber/linux/kernel/sched/core.c: 920
 * instantly. /Users/rubber/linux/kernel/sched/core.c: 921
 * This function must be used as-if it were wake_up_process(); IOW the task /Users/rubber/linux/kernel/sched/core.c: 923
 * must be ready to be woken at this location. /Users/rubber/linux/kernel/sched/core.c: 924
 * This function is essentially a task-safe equivalent to wake_q_add(). Callers /Users/rubber/linux/kernel/sched/core.c: 926
 * that already hold reference to @task can call the 'safe' version and trust /Users/rubber/linux/kernel/sched/core.c: 927
 * wake_q to do the right thing depending whether or not the @task is already /Users/rubber/linux/kernel/sched/core.c: 928
 * queued for wakeup. /Users/rubber/linux/kernel/sched/core.c: 929
		/* /Users/rubber/linux/kernel/sched/core.c: 949
		 * wake_up_process() executes a full barrier, which pairs with /Users/rubber/linux/kernel/sched/core.c: 950
		 * the queueing in wake_q_add() so as not to miss wakeups. /Users/rubber/linux/kernel/sched/core.c: 951
 * resched_curr - mark rq's current task 'to be rescheduled now'. /Users/rubber/linux/kernel/sched/core.c: 959
 * On UP this means the setting of the need_resched flag, on SMP it /Users/rubber/linux/kernel/sched/core.c: 961
 * might also involve a cross-CPU call to trigger the scheduler on /Users/rubber/linux/kernel/sched/core.c: 962
 * the target CPU. /Users/rubber/linux/kernel/sched/core.c: 963
 * In the semi idle case, use the nearest busy CPU for migrating timers /Users/rubber/linux/kernel/sched/core.c: 1003
 * from an idle CPU.  This is good for power-savings. /Users/rubber/linux/kernel/sched/core.c: 1004
 * We don't do similar optimization for completely idle system, as /Users/rubber/linux/kernel/sched/core.c: 1006
 * selecting an idle CPU will add more delays to the timers than intended /Users/rubber/linux/kernel/sched/core.c: 1007
 * (as that CPU's timer base may not be uptodate wrt jiffies etc). /Users/rubber/linux/kernel/sched/core.c: 1008
 * When add_timer_on() enqueues a timer into the timer wheel of an /Users/rubber/linux/kernel/sched/core.c: 1046
 * idle CPU then this timer might expire before the next timer event /Users/rubber/linux/kernel/sched/core.c: 1047
 * which is scheduled to wake up that CPU. In case of a completely /Users/rubber/linux/kernel/sched/core.c: 1048
 * idle system the next event might even be infinite time into the /Users/rubber/linux/kernel/sched/core.c: 1049
 * future. wake_up_idle_cpu() ensures that the CPU is woken up and /Users/rubber/linux/kernel/sched/core.c: 1050
 * leaves the inner idle loop so the newly added timer is taken into /Users/rubber/linux/kernel/sched/core.c: 1051
 * account when the CPU goes back to idle and evaluates the timer /Users/rubber/linux/kernel/sched/core.c: 1052
 * wheel for the next timer event. /Users/rubber/linux/kernel/sched/core.c: 1053
	/* /Users/rubber/linux/kernel/sched/core.c: 1070
	 * We just need the target to call irq_exit() and re-evaluate /Users/rubber/linux/kernel/sched/core.c: 1071
	 * the next tick. The nohz full kick at least implies that. /Users/rubber/linux/kernel/sched/core.c: 1072
	 * If needed we can still optimize that later with an /Users/rubber/linux/kernel/sched/core.c: 1073
	 * empty IRQ. /Users/rubber/linux/kernel/sched/core.c: 1074
 * Wake up the specified CPU.  If the CPU is going offline, it is the /Users/rubber/linux/kernel/sched/core.c: 1089
 * caller's responsibility to deal with the lost wakeup, for example, /Users/rubber/linux/kernel/sched/core.c: 1090
 * by hooking into the CPU_DEAD notifier like timers and hrtimers do. /Users/rubber/linux/kernel/sched/core.c: 1091
	/* /Users/rubber/linux/kernel/sched/core.c: 1105
	 * Release the rq::nohz_csd. /Users/rubber/linux/kernel/sched/core.c: 1106
	/* /Users/rubber/linux/kernel/sched/core.c: 1129
	 * If there are more than one RR tasks, we need the tick to affect the /Users/rubber/linux/kernel/sched/core.c: 1130
	 * actual RR behaviour. /Users/rubber/linux/kernel/sched/core.c: 1131
	/* /Users/rubber/linux/kernel/sched/core.c: 1140
	 * If there's no RR tasks, but FIFO tasks, we can skip the tick, no /Users/rubber/linux/kernel/sched/core.c: 1141
	 * forced preemption between FIFO tasks. /Users/rubber/linux/kernel/sched/core.c: 1142
	/* /Users/rubber/linux/kernel/sched/core.c: 1148
	 * If there are no DL,RR/FIFO tasks, there must only be CFS tasks left; /Users/rubber/linux/kernel/sched/core.c: 1149
	 * if there's more than one we need the tick for involuntary /Users/rubber/linux/kernel/sched/core.c: 1150
	 * preemption. /Users/rubber/linux/kernel/sched/core.c: 1151
 * Iterate task_group tree rooted at *from, calling @down when first entering a /Users/rubber/linux/kernel/sched/core.c: 1164
 * node and @up when leaving it for the final time. /Users/rubber/linux/kernel/sched/core.c: 1165
 * Caller must hold rcu_lock or sufficient equivalent. /Users/rubber/linux/kernel/sched/core.c: 1167
	/* /Users/rubber/linux/kernel/sched/core.c: 1211
	 * SCHED_IDLE tasks get minimal weight: /Users/rubber/linux/kernel/sched/core.c: 1212
	/* /Users/rubber/linux/kernel/sched/core.c: 1220
	 * SCHED_OTHER tasks have to update their load when changing their /Users/rubber/linux/kernel/sched/core.c: 1221
	 * weight /Users/rubber/linux/kernel/sched/core.c: 1222
 * Serializes updates of utilization clamp values /Users/rubber/linux/kernel/sched/core.c: 1234
 * The (slow-path) user-space triggers utilization clamp value updates which /Users/rubber/linux/kernel/sched/core.c: 1236
 * can require updates on (fast-path) scheduler's data structures used to /Users/rubber/linux/kernel/sched/core.c: 1237
 * support enqueue/dequeue operations. /Users/rubber/linux/kernel/sched/core.c: 1238
 * While the per-CPU rq lock protects fast-path update operations, user-space /Users/rubber/linux/kernel/sched/core.c: 1239
 * requests are serialized using a mutex to reduce the risk of conflicting /Users/rubber/linux/kernel/sched/core.c: 1240
 * updates or API abuses. /Users/rubber/linux/kernel/sched/core.c: 1241
 * By default RT tasks run at the maximum performance point/capacity of the /Users/rubber/linux/kernel/sched/core.c: 1252
 * system. Uclamp enforces this by always setting UCLAMP_MIN of RT tasks to /Users/rubber/linux/kernel/sched/core.c: 1253
 * SCHED_CAPACITY_SCALE. /Users/rubber/linux/kernel/sched/core.c: 1254
 * This knob allows admins to change the default behavior when uclamp is being /Users/rubber/linux/kernel/sched/core.c: 1256
 * used. In battery powered devices, particularly, running at the maximum /Users/rubber/linux/kernel/sched/core.c: 1257
 * capacity and frequency will increase energy consumption and shorten the /Users/rubber/linux/kernel/sched/core.c: 1258
 * battery life. /Users/rubber/linux/kernel/sched/core.c: 1259
 * This knob only affects RT tasks that their uclamp_se->user_defined == false. /Users/rubber/linux/kernel/sched/core.c: 1261
 * This knob will not override the system default sched_util_clamp_min defined /Users/rubber/linux/kernel/sched/core.c: 1263
 * above. /Users/rubber/linux/kernel/sched/core.c: 1264
 * This static key is used to reduce the uclamp overhead in the fast path. It /Users/rubber/linux/kernel/sched/core.c: 1272
 * primarily disables the call to uclamp_rq_{inc, dec}() in /Users/rubber/linux/kernel/sched/core.c: 1273
 * enqueue/dequeue_task(). /Users/rubber/linux/kernel/sched/core.c: 1274
 * This allows users to continue to enable uclamp in their kernel config with /Users/rubber/linux/kernel/sched/core.c: 1276
 * minimum uclamp overhead in the fast path. /Users/rubber/linux/kernel/sched/core.c: 1277
 * As soon as userspace modifies any of the uclamp knobs, the static key is /Users/rubber/linux/kernel/sched/core.c: 1279
 * enabled, since we have an actual users that make use of uclamp /Users/rubber/linux/kernel/sched/core.c: 1280
 * functionality. /Users/rubber/linux/kernel/sched/core.c: 1281
 * The knobs that would enable this static key are: /Users/rubber/linux/kernel/sched/core.c: 1283
 *   * A task modifying its uclamp value with sched_setattr(). /Users/rubber/linux/kernel/sched/core.c: 1285
 *   * An admin modifying the sysctl_sched_uclamp_{min, max} via procfs. /Users/rubber/linux/kernel/sched/core.c: 1286
 *   * An admin modifying the cgroup cpu.uclamp.{min, max} /Users/rubber/linux/kernel/sched/core.c: 1287
	/* /Users/rubber/linux/kernel/sched/core.c: 1321
	 * Avoid blocked utilization pushing up the frequency when we go /Users/rubber/linux/kernel/sched/core.c: 1322
	 * idle (which drops the max-clamp) by retaining the last known /Users/rubber/linux/kernel/sched/core.c: 1323
	 * max-clamp. /Users/rubber/linux/kernel/sched/core.c: 1324
	/* /Users/rubber/linux/kernel/sched/core.c: 1351
	 * Since both min and max clamps are max aggregated, find the /Users/rubber/linux/kernel/sched/core.c: 1352
	 * top most bucket with tasks in. /Users/rubber/linux/kernel/sched/core.c: 1353
	/* /Users/rubber/linux/kernel/sched/core.c: 1400
	 * copy_process()			sysctl_uclamp /Users/rubber/linux/kernel/sched/core.c: 1401
	 *					  uclamp_min_rt = X; /Users/rubber/linux/kernel/sched/core.c: 1402
	 *   write_lock(&tasklist_lock)		  read_lock(&tasklist_lock) /Users/rubber/linux/kernel/sched/core.c: 1403
	 *   // link thread			  smp_mb__after_spinlock() /Users/rubber/linux/kernel/sched/core.c: 1404
	 *   write_unlock(&tasklist_lock)	  read_unlock(&tasklist_lock); /Users/rubber/linux/kernel/sched/core.c: 1405
	 *   sched_post_fork()			  for_each_process_thread() /Users/rubber/linux/kernel/sched/core.c: 1406
	 *     __uclamp_sync_rt()		    __uclamp_sync_rt() /Users/rubber/linux/kernel/sched/core.c: 1407
	 * /Users/rubber/linux/kernel/sched/core.c: 1408
	 * Ensures that either sched_post_fork() will observe the new /Users/rubber/linux/kernel/sched/core.c: 1409
	 * uclamp_min_rt or for_each_process_thread() will observe the new /Users/rubber/linux/kernel/sched/core.c: 1410
	 * task. /Users/rubber/linux/kernel/sched/core.c: 1411
	/* /Users/rubber/linux/kernel/sched/core.c: 1431
	 * Tasks in autogroups or root task group will be /Users/rubber/linux/kernel/sched/core.c: 1432
	 * restricted by system defaults. /Users/rubber/linux/kernel/sched/core.c: 1433
 * The effective clamp bucket index of a task depends on, by increasing /Users/rubber/linux/kernel/sched/core.c: 1451
 * priority: /Users/rubber/linux/kernel/sched/core.c: 1452
 * - the task specific clamp value, when explicitly requested from userspace /Users/rubber/linux/kernel/sched/core.c: 1453
 * - the task group effective clamp value, for tasks not either in the root /Users/rubber/linux/kernel/sched/core.c: 1454
 *   group or in an autogroup /Users/rubber/linux/kernel/sched/core.c: 1455
 * - the system default clamp value, defined by the sysadmin /Users/rubber/linux/kernel/sched/core.c: 1456
 * When a task is enqueued on a rq, the clamp bucket currently defined by the /Users/rubber/linux/kernel/sched/core.c: 1485
 * task's uclamp::bucket_id is refcounted on that rq. This also immediately /Users/rubber/linux/kernel/sched/core.c: 1486
 * updates the rq's clamp value if required. /Users/rubber/linux/kernel/sched/core.c: 1487
 * Tasks can have a task-specific value requested from user-space, track /Users/rubber/linux/kernel/sched/core.c: 1489
 * within each bucket the maximum value for tasks refcounted in it. /Users/rubber/linux/kernel/sched/core.c: 1490
 * This "local max aggregation" allows to track the exact "requested" value /Users/rubber/linux/kernel/sched/core.c: 1491
 * for each bucket when all its RUNNABLE tasks require the same clamp. /Users/rubber/linux/kernel/sched/core.c: 1492
	/* /Users/rubber/linux/kernel/sched/core.c: 1512
	 * Local max aggregation: rq buckets always track the max /Users/rubber/linux/kernel/sched/core.c: 1513
	 * "requested" clamp value of its RUNNABLE tasks. /Users/rubber/linux/kernel/sched/core.c: 1514
 * When a task is dequeued from a rq, the clamp bucket refcounted by the task /Users/rubber/linux/kernel/sched/core.c: 1524
 * is released. If this is the last task reference counting the rq's max /Users/rubber/linux/kernel/sched/core.c: 1525
 * active clamp value, then the rq's clamp value is updated. /Users/rubber/linux/kernel/sched/core.c: 1526
 * Both refcounted tasks and rq's cached clamp values are expected to be /Users/rubber/linux/kernel/sched/core.c: 1528
 * always valid. If it's detected they are not, as defensive programming, /Users/rubber/linux/kernel/sched/core.c: 1529
 * enforce the expected state and warn. /Users/rubber/linux/kernel/sched/core.c: 1530
	/* /Users/rubber/linux/kernel/sched/core.c: 1543
	 * If sched_uclamp_used was enabled after task @p was enqueued, /Users/rubber/linux/kernel/sched/core.c: 1544
	 * we could end up with unbalanced call to uclamp_rq_dec_id(). /Users/rubber/linux/kernel/sched/core.c: 1545
	 * /Users/rubber/linux/kernel/sched/core.c: 1546
	 * In this case the uc_se->active flag should be false since no uclamp /Users/rubber/linux/kernel/sched/core.c: 1547
	 * accounting was performed at enqueue time and we can just return /Users/rubber/linux/kernel/sched/core.c: 1548
	 * here. /Users/rubber/linux/kernel/sched/core.c: 1549
	 * /Users/rubber/linux/kernel/sched/core.c: 1550
	 * Need to be careful of the following enqueue/dequeue ordering /Users/rubber/linux/kernel/sched/core.c: 1551
	 * problem too /Users/rubber/linux/kernel/sched/core.c: 1552
	 * /Users/rubber/linux/kernel/sched/core.c: 1553
	 *	enqueue(taskA) /Users/rubber/linux/kernel/sched/core.c: 1554
	 *	// sched_uclamp_used gets enabled /Users/rubber/linux/kernel/sched/core.c: 1555
	 *	enqueue(taskB) /Users/rubber/linux/kernel/sched/core.c: 1556
	 *	dequeue(taskA) /Users/rubber/linux/kernel/sched/core.c: 1557
	 *	// Must not decrement bucket->tasks here /Users/rubber/linux/kernel/sched/core.c: 1558
	 *	dequeue(taskB) /Users/rubber/linux/kernel/sched/core.c: 1559
	 * /Users/rubber/linux/kernel/sched/core.c: 1560
	 * where we could end up with stale data in uc_se and /Users/rubber/linux/kernel/sched/core.c: 1561
	 * bucket[uc_se->bucket_id]. /Users/rubber/linux/kernel/sched/core.c: 1562
	 * /Users/rubber/linux/kernel/sched/core.c: 1563
	 * The following check here eliminates the possibility of such race. /Users/rubber/linux/kernel/sched/core.c: 1564
	/* /Users/rubber/linux/kernel/sched/core.c: 1577
	 * Keep "local max aggregation" simple and accept to (possibly) /Users/rubber/linux/kernel/sched/core.c: 1578
	 * overboost some RUNNABLE tasks in the same bucket. /Users/rubber/linux/kernel/sched/core.c: 1579
	 * The rq clamp bucket value is reset to its base value whenever /Users/rubber/linux/kernel/sched/core.c: 1580
	 * there are no more RUNNABLE tasks refcounting it. /Users/rubber/linux/kernel/sched/core.c: 1581
	/* /Users/rubber/linux/kernel/sched/core.c: 1587
	 * Defensive programming: this should never happen. If it happens, /Users/rubber/linux/kernel/sched/core.c: 1588
	 * e.g. due to future modification, warn and fixup the expected value. /Users/rubber/linux/kernel/sched/core.c: 1589
	/* /Users/rubber/linux/kernel/sched/core.c: 1602
	 * Avoid any overhead until uclamp is actually used by the userspace. /Users/rubber/linux/kernel/sched/core.c: 1603
	 * /Users/rubber/linux/kernel/sched/core.c: 1604
	 * The condition is constructed such that a NOP is generated when /Users/rubber/linux/kernel/sched/core.c: 1605
	 * sched_uclamp_used is disabled. /Users/rubber/linux/kernel/sched/core.c: 1606
	/* /Users/rubber/linux/kernel/sched/core.c: 1626
	 * Avoid any overhead until uclamp is actually used by the userspace. /Users/rubber/linux/kernel/sched/core.c: 1627
	 * /Users/rubber/linux/kernel/sched/core.c: 1628
	 * The condition is constructed such that a NOP is generated when /Users/rubber/linux/kernel/sched/core.c: 1629
	 * sched_uclamp_used is disabled. /Users/rubber/linux/kernel/sched/core.c: 1630
	/* /Users/rubber/linux/kernel/sched/core.c: 1651
	 * Make sure to clear the idle flag if we've transiently reached 0 /Users/rubber/linux/kernel/sched/core.c: 1652
	 * active tasks on rq. /Users/rubber/linux/kernel/sched/core.c: 1653
	/* /Users/rubber/linux/kernel/sched/core.c: 1666
	 * Lock the task and the rq where the task is (or was) queued. /Users/rubber/linux/kernel/sched/core.c: 1667
	 * /Users/rubber/linux/kernel/sched/core.c: 1668
	 * We might lock the (previous) rq of a !RUNNABLE task, but that's the /Users/rubber/linux/kernel/sched/core.c: 1669
	 * price to pay to safely serialize util_{min,max} updates with /Users/rubber/linux/kernel/sched/core.c: 1670
	 * enqueues, dequeues and migration operations. /Users/rubber/linux/kernel/sched/core.c: 1671
	 * This is the same locking schema used by __set_cpus_allowed_ptr(). /Users/rubber/linux/kernel/sched/core.c: 1672
	/* /Users/rubber/linux/kernel/sched/core.c: 1676
	 * Setting the clamp bucket is serialized by task_rq_lock(). /Users/rubber/linux/kernel/sched/core.c: 1677
	 * If the task is not yet RUNNABLE and its task_struct is not /Users/rubber/linux/kernel/sched/core.c: 1678
	 * affecting a valid clamp bucket, the next time it's enqueued, /Users/rubber/linux/kernel/sched/core.c: 1679
	 * it will already see the updated clamp bucket value. /Users/rubber/linux/kernel/sched/core.c: 1680
	/* /Users/rubber/linux/kernel/sched/core.c: 1766
	 * We update all RUNNABLE tasks only when task groups are in use. /Users/rubber/linux/kernel/sched/core.c: 1767
	 * Otherwise, keep it simple and do just a lazy update at each next /Users/rubber/linux/kernel/sched/core.c: 1768
	 * task enqueue time. /Users/rubber/linux/kernel/sched/core.c: 1769
	/* /Users/rubber/linux/kernel/sched/core.c: 1807
	 * We have valid uclamp attributes; make sure uclamp is enabled. /Users/rubber/linux/kernel/sched/core.c: 1808
	 * /Users/rubber/linux/kernel/sched/core.c: 1809
	 * We need to do that here, because enabling static branches is a /Users/rubber/linux/kernel/sched/core.c: 1810
	 * blocking operation which obviously cannot be done while holding /Users/rubber/linux/kernel/sched/core.c: 1811
	 * scheduler locks. /Users/rubber/linux/kernel/sched/core.c: 1812
		/* /Users/rubber/linux/kernel/sched/core.c: 1856
		 * RT by default have a 100% boost value that could be modified /Users/rubber/linux/kernel/sched/core.c: 1857
		 * at runtime. /Users/rubber/linux/kernel/sched/core.c: 1858
	/* /Users/rubber/linux/kernel/sched/core.c: 1889
	 * We don't need to hold task_rq_lock() when updating p->uclamp_* here /Users/rubber/linux/kernel/sched/core.c: 1890
	 * as the task is still at its early fork stages. /Users/rubber/linux/kernel/sched/core.c: 1891
 * Calculate the expected normal priority: i.e. priority /Users/rubber/linux/kernel/sched/core.c: 2051
 * without taking RT-inheritance into account. Might be /Users/rubber/linux/kernel/sched/core.c: 2052
 * boosted by interactivity modifiers. Changes upon fork, /Users/rubber/linux/kernel/sched/core.c: 2053
 * setprio syscalls, and whenever the interactivity /Users/rubber/linux/kernel/sched/core.c: 2054
 * estimator recalculates. /Users/rubber/linux/kernel/sched/core.c: 2055
 * Calculate the current priority, i.e. the priority /Users/rubber/linux/kernel/sched/core.c: 2063
 * taken into account by the scheduler. This value might /Users/rubber/linux/kernel/sched/core.c: 2064
 * be boosted by RT tasks, or might be boosted by /Users/rubber/linux/kernel/sched/core.c: 2065
 * interactivity modifiers. Will be RT if the task got /Users/rubber/linux/kernel/sched/core.c: 2066
 * RT-boosted. If not then it returns p->normal_prio. /Users/rubber/linux/kernel/sched/core.c: 2067
	/* /Users/rubber/linux/kernel/sched/core.c: 2072
	 * If we are RT tasks or we were boosted to RT priority, /Users/rubber/linux/kernel/sched/core.c: 2073
	 * keep the priority unchanged. Otherwise, update priority /Users/rubber/linux/kernel/sched/core.c: 2074
	 * to the normal priority: /Users/rubber/linux/kernel/sched/core.c: 2075
 * task_curr - is this task currently executing on a CPU? /Users/rubber/linux/kernel/sched/core.c: 2083
 * @p: the task in question. /Users/rubber/linux/kernel/sched/core.c: 2084
 * Return: 1 if the task is currently executing. 0 otherwise. /Users/rubber/linux/kernel/sched/core.c: 2086
 * switched_from, switched_to and prio_changed must _NOT_ drop rq->lock, /Users/rubber/linux/kernel/sched/core.c: 2094
 * use the balance_callback list if you want balancing. /Users/rubber/linux/kernel/sched/core.c: 2095
 * this means any call to check_class_changed() must be followed by a call to /Users/rubber/linux/kernel/sched/core.c: 2097
 * balance_callback(). /Users/rubber/linux/kernel/sched/core.c: 2098
	/* /Users/rubber/linux/kernel/sched/core.c: 2120
	 * A queue event has occurred, and we're going to schedule.  In /Users/rubber/linux/kernel/sched/core.c: 2121
	 * this case, we can save a useless back to back clock update. /Users/rubber/linux/kernel/sched/core.c: 2122
	/* /Users/rubber/linux/kernel/sched/core.c: 2145
	 * Violates locking rules! see comment in __do_set_cpus_allowed(). /Users/rubber/linux/kernel/sched/core.c: 2146
	/* /Users/rubber/linux/kernel/sched/core.c: 2176
	 * Ensure stop_task runs either before or after this, and that /Users/rubber/linux/kernel/sched/core.c: 2177
	 * __set_cpus_allowed_ptr(SCA_MIGRATE_ENABLE) doesn't schedule(). /Users/rubber/linux/kernel/sched/core.c: 2178
	/* /Users/rubber/linux/kernel/sched/core.c: 2183
	 * Mustn't clear migration_disabled() until cpus_ptr points back at the /Users/rubber/linux/kernel/sched/core.c: 2184
	 * regular cpus_mask, otherwise things that race (eg. /Users/rubber/linux/kernel/sched/core.c: 2185
	 * select_fallback_rq) get confused. /Users/rubber/linux/kernel/sched/core.c: 2186
 * Per-CPU kthreads are allowed to run on !active && online CPUs, see /Users/rubber/linux/kernel/sched/core.c: 2201
 * __set_cpus_allowed_ptr() and select_fallback_rq(). /Users/rubber/linux/kernel/sched/core.c: 2202
 * This is how migration works: /Users/rubber/linux/kernel/sched/core.c: 2231
 * 1) we invoke migration_cpu_stop() on the target CPU using /Users/rubber/linux/kernel/sched/core.c: 2233
 *    stop_one_cpu(). /Users/rubber/linux/kernel/sched/core.c: 2234
 * 2) stopper starts to run (implicitly forcing the migrated thread /Users/rubber/linux/kernel/sched/core.c: 2235
 *    off the CPU) /Users/rubber/linux/kernel/sched/core.c: 2236
 * 3) it checks whether the migrated task is still in the wrong runqueue. /Users/rubber/linux/kernel/sched/core.c: 2237
 * 4) if it's in the wrong runqueue then the migration thread removes /Users/rubber/linux/kernel/sched/core.c: 2238
 *    it and puts it into the right queue. /Users/rubber/linux/kernel/sched/core.c: 2239
 * 5) stopper completes and stop_one_cpu() returns and the migration /Users/rubber/linux/kernel/sched/core.c: 2240
 *    is done. /Users/rubber/linux/kernel/sched/core.c: 2241
 * move_queued_task - move a queued task to new rq. /Users/rubber/linux/kernel/sched/core.c: 2245
 * Returns (locked) new rq. Old rq's lock is released. /Users/rubber/linux/kernel/sched/core.c: 2247
 * @refs: number of wait_for_completion() /Users/rubber/linux/kernel/sched/core.c: 2275
 * @stop_pending: is @stop_work in use /Users/rubber/linux/kernel/sched/core.c: 2276
 * Move (not current) task off this CPU, onto the destination CPU. We're doing /Users/rubber/linux/kernel/sched/core.c: 2287
 * this because either it can't run here any more (set_cpus_allowed() /Users/rubber/linux/kernel/sched/core.c: 2288
 * away from this CPU, or CPU going down), or because we're /Users/rubber/linux/kernel/sched/core.c: 2289
 * attempting to rebalance this task on exec (sched_exec). /Users/rubber/linux/kernel/sched/core.c: 2290
 * So we race with normal scheduler movements, but that's OK, as long /Users/rubber/linux/kernel/sched/core.c: 2292
 * as the task is no longer on this CPU. /Users/rubber/linux/kernel/sched/core.c: 2293
 * migration_cpu_stop - this will be executed by a highprio stopper thread /Users/rubber/linux/kernel/sched/core.c: 2309
 * and performs thread migration by bumping thread off CPU then /Users/rubber/linux/kernel/sched/core.c: 2310
 * 'pushing' onto another runqueue. /Users/rubber/linux/kernel/sched/core.c: 2311
	/* /Users/rubber/linux/kernel/sched/core.c: 2322
	 * The original target CPU might have gone down and we might /Users/rubber/linux/kernel/sched/core.c: 2323
	 * be on another CPU but it doesn't matter. /Users/rubber/linux/kernel/sched/core.c: 2324
	/* /Users/rubber/linux/kernel/sched/core.c: 2327
	 * We need to explicitly wake pending tasks before running /Users/rubber/linux/kernel/sched/core.c: 2328
	 * __migrate_task() such that we will not miss enforcing cpus_ptr /Users/rubber/linux/kernel/sched/core.c: 2329
	 * during wakeups, see set_cpus_allowed_ptr()'s TASK_WAKING test. /Users/rubber/linux/kernel/sched/core.c: 2330
	/* /Users/rubber/linux/kernel/sched/core.c: 2337
	 * If we were passed a pending, then ->stop_pending was set, thus /Users/rubber/linux/kernel/sched/core.c: 2338
	 * p->migration_pending must have remained stable. /Users/rubber/linux/kernel/sched/core.c: 2339
	/* /Users/rubber/linux/kernel/sched/core.c: 2343
	 * If task_rq(p) != rq, it cannot be migrated here, because we're /Users/rubber/linux/kernel/sched/core.c: 2344
	 * holding rq->lock, if p->on_rq == 0 it cannot get enqueued because /Users/rubber/linux/kernel/sched/core.c: 2345
	 * we're holding p->pi_lock. /Users/rubber/linux/kernel/sched/core.c: 2346
		/* /Users/rubber/linux/kernel/sched/core.c: 2365
		 * XXX __migrate_task() can fail, at which point we might end /Users/rubber/linux/kernel/sched/core.c: 2366
		 * up running on a dodgy CPU, AFAICT this can only happen /Users/rubber/linux/kernel/sched/core.c: 2367
		 * during CPU hotplug, at which point we'll get pushed out /Users/rubber/linux/kernel/sched/core.c: 2368
		 * anyway, so it's probably not a big deal. /Users/rubber/linux/kernel/sched/core.c: 2369
		/* /Users/rubber/linux/kernel/sched/core.c: 2373
		 * This happens when we get migrated between migrate_enable()'s /Users/rubber/linux/kernel/sched/core.c: 2374
		 * preempt_enable() and scheduling the stopper task. At that /Users/rubber/linux/kernel/sched/core.c: 2375
		 * point we're a regular task again and not current anymore. /Users/rubber/linux/kernel/sched/core.c: 2376
		 * /Users/rubber/linux/kernel/sched/core.c: 2377
		 * A !PREEMPT kernel has a giant hole here, which makes it far /Users/rubber/linux/kernel/sched/core.c: 2378
		 * more likely. /Users/rubber/linux/kernel/sched/core.c: 2379
		/* /Users/rubber/linux/kernel/sched/core.c: 2382
		 * The task moved before the stopper got to run. We're holding /Users/rubber/linux/kernel/sched/core.c: 2383
		 * ->pi_lock, so the allowed mask is stable - if it got /Users/rubber/linux/kernel/sched/core.c: 2384
		 * somewhere allowed, we're done. /Users/rubber/linux/kernel/sched/core.c: 2385
		/* /Users/rubber/linux/kernel/sched/core.c: 2393
		 * When migrate_enable() hits a rq mis-match we can't reliably /Users/rubber/linux/kernel/sched/core.c: 2394
		 * determine is_migration_disabled() and so have to chase after /Users/rubber/linux/kernel/sched/core.c: 2395
		 * it. /Users/rubber/linux/kernel/sched/core.c: 2396
 XXX validate p is still the highest prio task /Users/rubber/linux/kernel/sched/core.c: 2439
 * sched_class::set_cpus_allowed must do the below, but is not required to /Users/rubber/linux/kernel/sched/core.c: 2459
 * actually call this function. /Users/rubber/linux/kernel/sched/core.c: 2460
	/* /Users/rubber/linux/kernel/sched/core.c: 2479
	 * This here violates the locking rules for affinity, since we're only /Users/rubber/linux/kernel/sched/core.c: 2480
	 * supposed to change these variables while holding both rq->lock and /Users/rubber/linux/kernel/sched/core.c: 2481
	 * p->pi_lock. /Users/rubber/linux/kernel/sched/core.c: 2482
	 * /Users/rubber/linux/kernel/sched/core.c: 2483
	 * HOWEVER, it magically works, because ttwu() is the only code that /Users/rubber/linux/kernel/sched/core.c: 2484
	 * accesses these variables under p->pi_lock and only does so after /Users/rubber/linux/kernel/sched/core.c: 2485
	 * smp_cond_load_acquire(&p->on_cpu, !VAL), and we're in __schedule() /Users/rubber/linux/kernel/sched/core.c: 2486
	 * before finish_task(). /Users/rubber/linux/kernel/sched/core.c: 2487
	 * /Users/rubber/linux/kernel/sched/core.c: 2488
	 * XXX do further audits, this smells like something putrid. /Users/rubber/linux/kernel/sched/core.c: 2489
		/* /Users/rubber/linux/kernel/sched/core.c: 2500
		 * Because __kthread_bind() calls this on blocked tasks without /Users/rubber/linux/kernel/sched/core.c: 2501
		 * holding rq->lock. /Users/rubber/linux/kernel/sched/core.c: 2502
 * This function is wildly self concurrent; here be dragons. /Users/rubber/linux/kernel/sched/core.c: 2552
 * When given a valid mask, __set_cpus_allowed_ptr() must block until the /Users/rubber/linux/kernel/sched/core.c: 2555
 * designated task is enqueued on an allowed CPU. If that task is currently /Users/rubber/linux/kernel/sched/core.c: 2556
 * running, we have to kick it out using the CPU stopper. /Users/rubber/linux/kernel/sched/core.c: 2557
 * Migrate-Disable comes along and tramples all over our nice sandcastle. /Users/rubber/linux/kernel/sched/core.c: 2559
 * Consider: /Users/rubber/linux/kernel/sched/core.c: 2560
 *     Initial conditions: P0->cpus_mask = [0, 1] /Users/rubber/linux/kernel/sched/core.c: 2562
 *     P0@CPU0                  P1 /Users/rubber/linux/kernel/sched/core.c: 2564
 *     migrate_disable(); /Users/rubber/linux/kernel/sched/core.c: 2566
 *     <preempted> /Users/rubber/linux/kernel/sched/core.c: 2567
 *                              set_cpus_allowed_ptr(P0, [1]); /Users/rubber/linux/kernel/sched/core.c: 2568
 * P1 *cannot* return from this set_cpus_allowed_ptr() call until P0 executes /Users/rubber/linux/kernel/sched/core.c: 2570
 * its outermost migrate_enable() (i.e. it exits its Migrate-Disable region). /Users/rubber/linux/kernel/sched/core.c: 2571
 * This means we need the following scheme: /Users/rubber/linux/kernel/sched/core.c: 2572
 *     P0@CPU0                  P1 /Users/rubber/linux/kernel/sched/core.c: 2574
 *     migrate_disable(); /Users/rubber/linux/kernel/sched/core.c: 2576
 *     <preempted> /Users/rubber/linux/kernel/sched/core.c: 2577
 *                              set_cpus_allowed_ptr(P0, [1]); /Users/rubber/linux/kernel/sched/core.c: 2578
 *                                <blocks> /Users/rubber/linux/kernel/sched/core.c: 2579
 *     <resumes> /Users/rubber/linux/kernel/sched/core.c: 2580
 *     migrate_enable(); /Users/rubber/linux/kernel/sched/core.c: 2581
 *       __set_cpus_allowed_ptr(); /Users/rubber/linux/kernel/sched/core.c: 2582
 *       <wakes local stopper> /Users/rubber/linux/kernel/sched/core.c: 2583
 *                         `--> <woken on migration completion> /Users/rubber/linux/kernel/sched/core.c: 2584
 * Now the fun stuff: there may be several P1-like tasks, i.e. multiple /Users/rubber/linux/kernel/sched/core.c: 2586
 * concurrent set_cpus_allowed_ptr(P0, [*]) calls. CPU affinity changes of any /Users/rubber/linux/kernel/sched/core.c: 2587
 * task p are serialized by p->pi_lock, which we can leverage: the one that /Users/rubber/linux/kernel/sched/core.c: 2588
 * should come into effect at the end of the Migrate-Disable region is the last /Users/rubber/linux/kernel/sched/core.c: 2589
 * one. This means we only need to track a single cpumask (i.e. p->cpus_mask), /Users/rubber/linux/kernel/sched/core.c: 2590
 * but we still need to properly signal those waiting tasks at the appropriate /Users/rubber/linux/kernel/sched/core.c: 2591
 * moment. /Users/rubber/linux/kernel/sched/core.c: 2592
 * This is implemented using struct set_affinity_pending. The first /Users/rubber/linux/kernel/sched/core.c: 2594
 * __set_cpus_allowed_ptr() caller within a given Migrate-Disable region will /Users/rubber/linux/kernel/sched/core.c: 2595
 * setup an instance of that struct and install it on the targeted task_struct. /Users/rubber/linux/kernel/sched/core.c: 2596
 * Any and all further callers will reuse that instance. Those then wait for /Users/rubber/linux/kernel/sched/core.c: 2597
 * a completion signaled at the tail of the CPU stopper callback (1), triggered /Users/rubber/linux/kernel/sched/core.c: 2598
 * on the end of the Migrate-Disable region (i.e. outermost migrate_enable()). /Users/rubber/linux/kernel/sched/core.c: 2599
 * (1) In the cases covered above. There is one more where the completion is /Users/rubber/linux/kernel/sched/core.c: 2602
 * signaled within affine_move_task() itself: when a subsequent affinity request /Users/rubber/linux/kernel/sched/core.c: 2603
 * occurs after the stopper bailed out due to the targeted task still being /Users/rubber/linux/kernel/sched/core.c: 2604
 * Migrate-Disable. Consider: /Users/rubber/linux/kernel/sched/core.c: 2605
 *     Initial conditions: P0->cpus_mask = [0, 1] /Users/rubber/linux/kernel/sched/core.c: 2607
 *     CPU0		  P1				P2 /Users/rubber/linux/kernel/sched/core.c: 2609
 *     <P0> /Users/rubber/linux/kernel/sched/core.c: 2610
 *       migrate_disable(); /Users/rubber/linux/kernel/sched/core.c: 2611
 *       <preempted> /Users/rubber/linux/kernel/sched/core.c: 2612
 *                        set_cpus_allowed_ptr(P0, [1]); /Users/rubber/linux/kernel/sched/core.c: 2613
 *                          <blocks> /Users/rubber/linux/kernel/sched/core.c: 2614
 *     <migration/0> /Users/rubber/linux/kernel/sched/core.c: 2615
 *       migration_cpu_stop() /Users/rubber/linux/kernel/sched/core.c: 2616
 *         is_migration_disabled() /Users/rubber/linux/kernel/sched/core.c: 2617
 *           <bails> /Users/rubber/linux/kernel/sched/core.c: 2618
 *                                                       set_cpus_allowed_ptr(P0, [0, 1]); /Users/rubber/linux/kernel/sched/core.c: 2619
 *                                                         <signal completion> /Users/rubber/linux/kernel/sched/core.c: 2620
 *                          <awakes> /Users/rubber/linux/kernel/sched/core.c: 2621
 * Note that the above is safe vs a concurrent migrate_enable(), as any /Users/rubber/linux/kernel/sched/core.c: 2623
 * pending affinity completion is preceded by an uninstallation of /Users/rubber/linux/kernel/sched/core.c: 2624
 * p->migration_pending done with p->pi_lock held. /Users/rubber/linux/kernel/sched/core.c: 2625
		/* /Users/rubber/linux/kernel/sched/core.c: 2643
		 * If there are pending waiters, but no pending stop_work, /Users/rubber/linux/kernel/sched/core.c: 2644
		 * then complete now. /Users/rubber/linux/kernel/sched/core.c: 2645
			/* /Users/rubber/linux/kernel/sched/core.c: 2682
			 * Affinity has changed, but we've already installed a /Users/rubber/linux/kernel/sched/core.c: 2683
			 * pending. migration_cpu_stop() *must* see this, else /Users/rubber/linux/kernel/sched/core.c: 2684
			 * we risk a completion of the pending despite having a /Users/rubber/linux/kernel/sched/core.c: 2685
			 * task on a disallowed CPU. /Users/rubber/linux/kernel/sched/core.c: 2686
			 * /Users/rubber/linux/kernel/sched/core.c: 2687
			 * Serialized by p->pi_lock, so this is safe. /Users/rubber/linux/kernel/sched/core.c: 2688
	/* /Users/rubber/linux/kernel/sched/core.c: 2694
	 * - !MIGRATE_ENABLE: /Users/rubber/linux/kernel/sched/core.c: 2695
	 *   we'll have installed a pending if there wasn't one already. /Users/rubber/linux/kernel/sched/core.c: 2696
	 * /Users/rubber/linux/kernel/sched/core.c: 2697
	 * - MIGRATE_ENABLE: /Users/rubber/linux/kernel/sched/core.c: 2698
	 *   we're here because the current CPU isn't matching anymore, /Users/rubber/linux/kernel/sched/core.c: 2699
	 *   the only way that can happen is because of a concurrent /Users/rubber/linux/kernel/sched/core.c: 2700
	 *   set_cpus_allowed_ptr() call, which should then still be /Users/rubber/linux/kernel/sched/core.c: 2701
	 *   pending completion. /Users/rubber/linux/kernel/sched/core.c: 2702
	 * /Users/rubber/linux/kernel/sched/core.c: 2703
	 * Either way, we really should have a @pending here. /Users/rubber/linux/kernel/sched/core.c: 2704
		/* /Users/rubber/linux/kernel/sched/core.c: 2712
		 * MIGRATE_ENABLE gets here because 'p == current', but for /Users/rubber/linux/kernel/sched/core.c: 2713
		 * anything else we cannot do is_migration_disabled(), punt /Users/rubber/linux/kernel/sched/core.c: 2714
		 * and have the stopper function handle it all race-free. /Users/rubber/linux/kernel/sched/core.c: 2715
	/* /Users/rubber/linux/kernel/sched/core.c: 2755
	 * Block the original owner of &pending until all subsequent callers /Users/rubber/linux/kernel/sched/core.c: 2756
	 * have seen the completion and decremented the refcount /Users/rubber/linux/kernel/sched/core.c: 2757
 * Called with both p->pi_lock and rq->lock held; drops both before returning. /Users/rubber/linux/kernel/sched/core.c: 2768
		/* /Users/rubber/linux/kernel/sched/core.c: 2788
		 * Kernel threads are allowed on online && !active CPUs, /Users/rubber/linux/kernel/sched/core.c: 2789
		 * however, during cpu-hot-unplug, even these might get pushed /Users/rubber/linux/kernel/sched/core.c: 2790
		 * away if not KTHREAD_IS_PER_CPU. /Users/rubber/linux/kernel/sched/core.c: 2791
		 * /Users/rubber/linux/kernel/sched/core.c: 2792
		 * Specifically, migration_disabled() tasks must not fail the /Users/rubber/linux/kernel/sched/core.c: 2793
		 * cpumask_any_and_distribute() pick below, esp. so on /Users/rubber/linux/kernel/sched/core.c: 2794
		 * SCA_MIGRATE_ENABLE, otherwise we'll not call /Users/rubber/linux/kernel/sched/core.c: 2795
		 * set_cpus_allowed_common() and actually reset p->cpus_ptr. /Users/rubber/linux/kernel/sched/core.c: 2796
	/* /Users/rubber/linux/kernel/sched/core.c: 2806
	 * Must re-check here, to close a race against __kthread_bind(), /Users/rubber/linux/kernel/sched/core.c: 2807
	 * sched_setaffinity() is not guaranteed to observe the flag. /Users/rubber/linux/kernel/sched/core.c: 2808
	/* /Users/rubber/linux/kernel/sched/core.c: 2827
	 * Picking a ~random cpu helps in cases where we are changing affinity /Users/rubber/linux/kernel/sched/core.c: 2828
	 * for groups of tasks (ie. cpuset), so that load balancing is not /Users/rubber/linux/kernel/sched/core.c: 2829
	 * immediately required to distribute the tasks within their new mask. /Users/rubber/linux/kernel/sched/core.c: 2830
 * Change a given task's CPU affinity. Migrate the thread to a /Users/rubber/linux/kernel/sched/core.c: 2856
 * proper CPU and schedule it away if the CPU it's executing on /Users/rubber/linux/kernel/sched/core.c: 2857
 * is removed from the allowed bitmask. /Users/rubber/linux/kernel/sched/core.c: 2858
 * NOTE: the caller must have a valid reference to the task, the /Users/rubber/linux/kernel/sched/core.c: 2860
 * task must not exit() & deallocate itself prematurely. The /Users/rubber/linux/kernel/sched/core.c: 2861
 * call is not atomic; no spinlocks may be held. /Users/rubber/linux/kernel/sched/core.c: 2862
 * Change a given task's CPU affinity to the intersection of its current /Users/rubber/linux/kernel/sched/core.c: 2881
 * affinity mask and @subset_mask, writing the resulting mask to @new_mask /Users/rubber/linux/kernel/sched/core.c: 2882
 * and pointing @p->user_cpus_ptr to a copy of the old mask. /Users/rubber/linux/kernel/sched/core.c: 2883
 * If the resulting mask is empty, leave the affinity unchanged and return /Users/rubber/linux/kernel/sched/core.c: 2884
 * -EINVAL. /Users/rubber/linux/kernel/sched/core.c: 2885
	/* /Users/rubber/linux/kernel/sched/core.c: 2904
	 * Forcefully restricting the affinity of a deadline task is /Users/rubber/linux/kernel/sched/core.c: 2905
	 * likely to cause problems, so fail and noisily override the /Users/rubber/linux/kernel/sched/core.c: 2906
	 * mask entirely. /Users/rubber/linux/kernel/sched/core.c: 2907
	/* /Users/rubber/linux/kernel/sched/core.c: 2919
	 * We're about to butcher the task affinity, so keep track of what /Users/rubber/linux/kernel/sched/core.c: 2920
	 * the user asked for in case we're able to restore it later on. /Users/rubber/linux/kernel/sched/core.c: 2921
 * Restrict the CPU affinity of task @p so that it is a subset of /Users/rubber/linux/kernel/sched/core.c: 2937
 * task_cpu_possible_mask() and point @p->user_cpu_ptr to a copy of the /Users/rubber/linux/kernel/sched/core.c: 2938
 * old affinity mask. If the resulting mask is empty, we warn and walk /Users/rubber/linux/kernel/sched/core.c: 2939
 * up the cpuset hierarchy until we find a suitable mask. /Users/rubber/linux/kernel/sched/core.c: 2940
	/* /Users/rubber/linux/kernel/sched/core.c: 2949
	 * __migrate_task() can fail silently in the face of concurrent /Users/rubber/linux/kernel/sched/core.c: 2950
	 * offlining of the chosen destination CPU, so take the hotplug /Users/rubber/linux/kernel/sched/core.c: 2951
	 * lock to ensure that the migration succeeds. /Users/rubber/linux/kernel/sched/core.c: 2952
	/* /Users/rubber/linux/kernel/sched/core.c: 2961
	 * We failed to find a valid subset of the affinity mask for the /Users/rubber/linux/kernel/sched/core.c: 2962
	 * task, so override it based on its cpuset hierarchy. /Users/rubber/linux/kernel/sched/core.c: 2963
 * Restore the affinity of a task @p which was previously restricted by a /Users/rubber/linux/kernel/sched/core.c: 2985
 * call to force_compatible_cpus_allowed_ptr(). This will clear (and free) /Users/rubber/linux/kernel/sched/core.c: 2986
 * @p->user_cpus_ptr. /Users/rubber/linux/kernel/sched/core.c: 2987
 * It is the caller's responsibility to serialise this with any calls to /Users/rubber/linux/kernel/sched/core.c: 2989
 * force_compatible_cpus_allowed_ptr(@p). /Users/rubber/linux/kernel/sched/core.c: 2990
	/* /Users/rubber/linux/kernel/sched/core.c: 2997
	 * Try to restore the old affinity mask. If this fails, then /Users/rubber/linux/kernel/sched/core.c: 2998
	 * we free the mask explicitly to avoid it being inherited across /Users/rubber/linux/kernel/sched/core.c: 2999
	 * a subsequent fork(). /Users/rubber/linux/kernel/sched/core.c: 3000
	/* /Users/rubber/linux/kernel/sched/core.c: 3017
	 * We should never call set_task_cpu() on a blocked task, /Users/rubber/linux/kernel/sched/core.c: 3018
	 * ttwu() will sort out the placement. /Users/rubber/linux/kernel/sched/core.c: 3019
	/* /Users/rubber/linux/kernel/sched/core.c: 3023
	 * Migrating fair class task must have p->on_rq = TASK_ON_RQ_MIGRATING, /Users/rubber/linux/kernel/sched/core.c: 3024
	 * because schedstat_wait_{start,end} rebase migrating task's wait_start /Users/rubber/linux/kernel/sched/core.c: 3025
	 * time relying on p->on_rq. /Users/rubber/linux/kernel/sched/core.c: 3026
	/* /Users/rubber/linux/kernel/sched/core.c: 3033
	 * The caller should hold either p->pi_lock or rq->lock, when changing /Users/rubber/linux/kernel/sched/core.c: 3034
	 * a task's CPU. ->pi_lock for waking tasks, rq->lock for runnable tasks. /Users/rubber/linux/kernel/sched/core.c: 3035
	 * /Users/rubber/linux/kernel/sched/core.c: 3036
	 * sched_move_task() holds both and thus holding either pins the cgroup, /Users/rubber/linux/kernel/sched/core.c: 3037
	 * see task_group(). /Users/rubber/linux/kernel/sched/core.c: 3038
	 * /Users/rubber/linux/kernel/sched/core.c: 3039
	 * Furthermore, all task_rq users should acquire both locks, see /Users/rubber/linux/kernel/sched/core.c: 3040
	 * task_rq_lock(). /Users/rubber/linux/kernel/sched/core.c: 3041
	/* /Users/rubber/linux/kernel/sched/core.c: 3046
	 * Clearly, migrating tasks to offline CPUs is a fairly daft thing. /Users/rubber/linux/kernel/sched/core.c: 3047
		/* /Users/rubber/linux/kernel/sched/core.c: 3089
		 * Task isn't running anymore; make it appear like we migrated /Users/rubber/linux/kernel/sched/core.c: 3090
		 * it before it went to sleep. This means on wakeup we make the /Users/rubber/linux/kernel/sched/core.c: 3091
		 * previous CPU our target instead of where it really is. /Users/rubber/linux/kernel/sched/core.c: 3092
 * Cross migrate two tasks /Users/rubber/linux/kernel/sched/core.c: 3145
	/* /Users/rubber/linux/kernel/sched/core.c: 3163
	 * These three tests are all lockless; this is OK since all of them /Users/rubber/linux/kernel/sched/core.c: 3164
	 * will be re-checked with proper locks held further down the line. /Users/rubber/linux/kernel/sched/core.c: 3165
 * wait_task_inactive - wait for a thread to unschedule. /Users/rubber/linux/kernel/sched/core.c: 3185
 * If @match_state is nonzero, it's the @p->state value just checked and /Users/rubber/linux/kernel/sched/core.c: 3187
 * not expected to change.  If it changes, i.e. @p might have woken up, /Users/rubber/linux/kernel/sched/core.c: 3188
 * then return zero.  When we succeed in waiting for @p to be off its CPU, /Users/rubber/linux/kernel/sched/core.c: 3189
 * we return a positive number (its total switch count).  If a second call /Users/rubber/linux/kernel/sched/core.c: 3190
 * a short while later returns the same number, the caller can be sure that /Users/rubber/linux/kernel/sched/core.c: 3191
 * @p has remained unscheduled the whole time. /Users/rubber/linux/kernel/sched/core.c: 3192
 * The caller must ensure that the task *will* unschedule sometime soon, /Users/rubber/linux/kernel/sched/core.c: 3194
 * else this function might spin for a *long* time. This function can't /Users/rubber/linux/kernel/sched/core.c: 3195
 * be called with interrupts off, or it may introduce deadlock with /Users/rubber/linux/kernel/sched/core.c: 3196
 * smp_call_function() if an IPI is sent by the same process we are /Users/rubber/linux/kernel/sched/core.c: 3197
 * waiting to become inactive. /Users/rubber/linux/kernel/sched/core.c: 3198
		/* /Users/rubber/linux/kernel/sched/core.c: 3208
		 * We do the initial early heuristics without holding /Users/rubber/linux/kernel/sched/core.c: 3209
		 * any task-queue locks at all. We'll only try to get /Users/rubber/linux/kernel/sched/core.c: 3210
		 * the runqueue lock when things look like they will /Users/rubber/linux/kernel/sched/core.c: 3211
		 * work out! /Users/rubber/linux/kernel/sched/core.c: 3212
		/* /Users/rubber/linux/kernel/sched/core.c: 3216
		 * If the task is actively running on another CPU /Users/rubber/linux/kernel/sched/core.c: 3217
		 * still, just relax and busy-wait without holding /Users/rubber/linux/kernel/sched/core.c: 3218
		 * any locks. /Users/rubber/linux/kernel/sched/core.c: 3219
		 * /Users/rubber/linux/kernel/sched/core.c: 3220
		 * NOTE! Since we don't hold any locks, it's not /Users/rubber/linux/kernel/sched/core.c: 3221
		 * even sure that "rq" stays as the right runqueue! /Users/rubber/linux/kernel/sched/core.c: 3222
		 * But we don't care, since "task_running()" will /Users/rubber/linux/kernel/sched/core.c: 3223
		 * return false if the runqueue has changed and p /Users/rubber/linux/kernel/sched/core.c: 3224
		 * is actually now running somewhere else! /Users/rubber/linux/kernel/sched/core.c: 3225
		/* /Users/rubber/linux/kernel/sched/core.c: 3233
		 * Ok, time to look more closely! We need the rq /Users/rubber/linux/kernel/sched/core.c: 3234
		 * lock now, to be *sure*. If we're wrong, we'll /Users/rubber/linux/kernel/sched/core.c: 3235
		 * just go back and repeat. /Users/rubber/linux/kernel/sched/core.c: 3236
		/* /Users/rubber/linux/kernel/sched/core.c: 3247
		 * If it changed from the expected state, bail out now. /Users/rubber/linux/kernel/sched/core.c: 3248
		/* /Users/rubber/linux/kernel/sched/core.c: 3253
		 * Was it really running after all now that we /Users/rubber/linux/kernel/sched/core.c: 3254
		 * checked with the proper locks actually held? /Users/rubber/linux/kernel/sched/core.c: 3255
		 * /Users/rubber/linux/kernel/sched/core.c: 3256
		 * Oops. Go back and try again.. /Users/rubber/linux/kernel/sched/core.c: 3257
		/* /Users/rubber/linux/kernel/sched/core.c: 3264
		 * It's not enough that it's not actively running, /Users/rubber/linux/kernel/sched/core.c: 3265
		 * it must be off the runqueue _entirely_, and not /Users/rubber/linux/kernel/sched/core.c: 3266
		 * preempted! /Users/rubber/linux/kernel/sched/core.c: 3267
		 * /Users/rubber/linux/kernel/sched/core.c: 3268
		 * So if it was still runnable (but just not actively /Users/rubber/linux/kernel/sched/core.c: 3269
		 * running right now), it's preempted, and we should /Users/rubber/linux/kernel/sched/core.c: 3270
		 * yield - it could be a while. /Users/rubber/linux/kernel/sched/core.c: 3271
		/* /Users/rubber/linux/kernel/sched/core.c: 3281
		 * Ahh, all good. It wasn't running, and it wasn't /Users/rubber/linux/kernel/sched/core.c: 3282
		 * runnable, which means that it will never become /Users/rubber/linux/kernel/sched/core.c: 3283
		 * running in the future either. We're all done! /Users/rubber/linux/kernel/sched/core.c: 3284
 * kick_process - kick a running thread to enter/exit the kernel /Users/rubber/linux/kernel/sched/core.c: 3293
 * @p: the to-be-kicked thread /Users/rubber/linux/kernel/sched/core.c: 3294
 * Cause a process which is running on another CPU to enter /Users/rubber/linux/kernel/sched/core.c: 3296
 * kernel-mode, without any delay. (to get signals handled.) /Users/rubber/linux/kernel/sched/core.c: 3297
 * NOTE: this function doesn't have to take the runqueue lock, /Users/rubber/linux/kernel/sched/core.c: 3299
 * because all it wants to ensure is that the remote task enters /Users/rubber/linux/kernel/sched/core.c: 3300
 * the kernel. If the IPI races and the task has been migrated /Users/rubber/linux/kernel/sched/core.c: 3301
 * to another CPU then no harm is done and the purpose has been /Users/rubber/linux/kernel/sched/core.c: 3302
 * achieved as well. /Users/rubber/linux/kernel/sched/core.c: 3303
 * ->cpus_ptr is protected by both rq->lock and p->pi_lock /Users/rubber/linux/kernel/sched/core.c: 3318
 * A few notes on cpu_active vs cpu_online: /Users/rubber/linux/kernel/sched/core.c: 3320
 *  - cpu_active must be a subset of cpu_online /Users/rubber/linux/kernel/sched/core.c: 3322
 *  - on CPU-up we allow per-CPU kthreads on the online && !active CPU, /Users/rubber/linux/kernel/sched/core.c: 3324
 *    see __set_cpus_allowed_ptr(). At this point the newly online /Users/rubber/linux/kernel/sched/core.c: 3325
 *    CPU isn't yet part of the sched domains, and balancing will not /Users/rubber/linux/kernel/sched/core.c: 3326
 *    see it. /Users/rubber/linux/kernel/sched/core.c: 3327
 *  - on CPU-down we clear cpu_active() to mask the sched domains and /Users/rubber/linux/kernel/sched/core.c: 3329
 *    avoid the load balancer to place new tasks on the to be removed /Users/rubber/linux/kernel/sched/core.c: 3330
 *    CPU. Existing tasks will remain running there and will be taken /Users/rubber/linux/kernel/sched/core.c: 3331
 *    off. /Users/rubber/linux/kernel/sched/core.c: 3332
 * This means that fallback selection must not select !active CPUs. /Users/rubber/linux/kernel/sched/core.c: 3334
 * And can assume that any active CPU must be online. Conversely /Users/rubber/linux/kernel/sched/core.c: 3335
 * select_task_rq() below may allow selection of !active CPUs in order /Users/rubber/linux/kernel/sched/core.c: 3336
 * to satisfy the above rules. /Users/rubber/linux/kernel/sched/core.c: 3337
	/* /Users/rubber/linux/kernel/sched/core.c: 3346
	 * If the node that the CPU is on has been offlined, cpu_to_node() /Users/rubber/linux/kernel/sched/core.c: 3347
	 * will return -1. There is no CPU on the node, and we should /Users/rubber/linux/kernel/sched/core.c: 3348
	 * select the CPU on the other node. /Users/rubber/linux/kernel/sched/core.c: 3349
			/* /Users/rubber/linux/kernel/sched/core.c: 3379
			 * XXX When called from select_task_rq() we only /Users/rubber/linux/kernel/sched/core.c: 3380
			 * hold p->pi_lock and again violate locking order. /Users/rubber/linux/kernel/sched/core.c: 3381
			 * /Users/rubber/linux/kernel/sched/core.c: 3382
			 * More yuck to audit. /Users/rubber/linux/kernel/sched/core.c: 3383
		/* /Users/rubber/linux/kernel/sched/core.c: 3396
		 * Don't tell them about moving exiting tasks or /Users/rubber/linux/kernel/sched/core.c: 3397
		 * kernel threads (both mm NULL), since they never /Users/rubber/linux/kernel/sched/core.c: 3398
		 * leave kernel. /Users/rubber/linux/kernel/sched/core.c: 3399
 * The caller (fork, wakeup) owns p->pi_lock, ->cpus_ptr is stable. /Users/rubber/linux/kernel/sched/core.c: 3411
	/* /Users/rubber/linux/kernel/sched/core.c: 3423
	 * In order not to call set_task_cpu() on a blocking task we need /Users/rubber/linux/kernel/sched/core.c: 3424
	 * to rely on ttwu() to place the task on a valid ->cpus_ptr /Users/rubber/linux/kernel/sched/core.c: 3425
	 * CPU. /Users/rubber/linux/kernel/sched/core.c: 3426
	 * /Users/rubber/linux/kernel/sched/core.c: 3427
	 * Since this is common to all placement strategies, this lives here. /Users/rubber/linux/kernel/sched/core.c: 3428
	 * /Users/rubber/linux/kernel/sched/core.c: 3429
	 * [ this allows ->select_task() to simply return task_cpu(p) and /Users/rubber/linux/kernel/sched/core.c: 3430
	 *   not worry about this generic constraint ] /Users/rubber/linux/kernel/sched/core.c: 3431
		/* /Users/rubber/linux/kernel/sched/core.c: 3446
		 * Make it appear like a SCHED_FIFO task, its something /Users/rubber/linux/kernel/sched/core.c: 3447
		 * userspace knows about and won't get confused about. /Users/rubber/linux/kernel/sched/core.c: 3448
		 * /Users/rubber/linux/kernel/sched/core.c: 3449
		 * Also, it will make PI more or less work without too /Users/rubber/linux/kernel/sched/core.c: 3450
		 * much confusion -- but then, stop work should not /Users/rubber/linux/kernel/sched/core.c: 3451
		 * rely on PI working anyway. /Users/rubber/linux/kernel/sched/core.c: 3452
		/* /Users/rubber/linux/kernel/sched/core.c: 3458
		 * The PI code calls rt_mutex_setprio() with ->pi_lock held to /Users/rubber/linux/kernel/sched/core.c: 3459
		 * adjust the effective priority of a task. As a result, /Users/rubber/linux/kernel/sched/core.c: 3460
		 * rt_mutex_setprio() can trigger (RT) balancing operations, /Users/rubber/linux/kernel/sched/core.c: 3461
		 * which can then trigger wakeups of the stop thread to push /Users/rubber/linux/kernel/sched/core.c: 3462
		 * around the current task. /Users/rubber/linux/kernel/sched/core.c: 3463
		 * /Users/rubber/linux/kernel/sched/core.c: 3464
		 * The stop task itself will never be part of the PI-chain, it /Users/rubber/linux/kernel/sched/core.c: 3465
		 * never blocks, therefore that ->pi_lock recursion is safe. /Users/rubber/linux/kernel/sched/core.c: 3466
		 * Tell lockdep about this by placing the stop->pi_lock in its /Users/rubber/linux/kernel/sched/core.c: 3467
		 * own class. /Users/rubber/linux/kernel/sched/core.c: 3468
		/* /Users/rubber/linux/kernel/sched/core.c: 3476
		 * Reset it back to a normal scheduling class so that /Users/rubber/linux/kernel/sched/core.c: 3477
		 * it can die in pieces. /Users/rubber/linux/kernel/sched/core.c: 3478
 * Mark the task runnable and perform wakeup-preemption. /Users/rubber/linux/kernel/sched/core.c: 3542
		/* /Users/rubber/linux/kernel/sched/core.c: 3553
		 * Our task @p is fully woken up and running; so it's safe to /Users/rubber/linux/kernel/sched/core.c: 3554
		 * drop the rq->lock, hereafter rq is only used for statistics. /Users/rubber/linux/kernel/sched/core.c: 3555
 * Consider @p being inside a wait loop: /Users/rubber/linux/kernel/sched/core.c: 3605
 *   for (;;) { /Users/rubber/linux/kernel/sched/core.c: 3607
 *      set_current_state(TASK_UNINTERRUPTIBLE); /Users/rubber/linux/kernel/sched/core.c: 3608
 *      if (CONDITION) /Users/rubber/linux/kernel/sched/core.c: 3610
 *         break; /Users/rubber/linux/kernel/sched/core.c: 3611
 *      schedule(); /Users/rubber/linux/kernel/sched/core.c: 3613
 *   } /Users/rubber/linux/kernel/sched/core.c: 3614
 *   __set_current_state(TASK_RUNNING); /Users/rubber/linux/kernel/sched/core.c: 3615
 * between set_current_state() and schedule(). In this case @p is still /Users/rubber/linux/kernel/sched/core.c: 3617
 * runnable, so all that needs doing is change p->state back to TASK_RUNNING in /Users/rubber/linux/kernel/sched/core.c: 3618
 * an atomic manner. /Users/rubber/linux/kernel/sched/core.c: 3619
 * By taking task_rq(p)->lock we serialize against schedule(), if @p->on_rq /Users/rubber/linux/kernel/sched/core.c: 3621
 * then schedule() must still happen and p->state can be changed to /Users/rubber/linux/kernel/sched/core.c: 3622
 * TASK_RUNNING. Otherwise we lost the race, schedule() has happened, and we /Users/rubber/linux/kernel/sched/core.c: 3623
 * need to do a full wakeup with enqueue. /Users/rubber/linux/kernel/sched/core.c: 3624
 * Returns: %true when the wakeup is done, /Users/rubber/linux/kernel/sched/core.c: 3626
 *          %false otherwise. /Users/rubber/linux/kernel/sched/core.c: 3627
	/* /Users/rubber/linux/kernel/sched/core.c: 3658
	 * rq::ttwu_pending racy indication of out-standing wakeups. /Users/rubber/linux/kernel/sched/core.c: 3659
	 * Races such that false-negatives are possible, since they /Users/rubber/linux/kernel/sched/core.c: 3660
	 * are shorter lived that false-positives would be. /Users/rubber/linux/kernel/sched/core.c: 3661
 * Queue a task on the target CPUs wake_list and wake the CPU via IPI if /Users/rubber/linux/kernel/sched/core.c: 3692
 * necessary. The wakee CPU on receipt of the IPI will queue the task /Users/rubber/linux/kernel/sched/core.c: 3693
 * via sched_ttwu_wakeup() for activation so the wakee incurs the cost /Users/rubber/linux/kernel/sched/core.c: 3694
 * of the wakeup instead of the waker. /Users/rubber/linux/kernel/sched/core.c: 3695
	/* /Users/rubber/linux/kernel/sched/core.c: 3737
	 * Do not complicate things with the async wake_list while the CPU is /Users/rubber/linux/kernel/sched/core.c: 3738
	 * in hotplug state. /Users/rubber/linux/kernel/sched/core.c: 3739
	/* /Users/rubber/linux/kernel/sched/core.c: 3744
	 * If the CPU does not share cache, then queue the task on the /Users/rubber/linux/kernel/sched/core.c: 3745
	 * remote rqs wakelist to avoid accessing remote data. /Users/rubber/linux/kernel/sched/core.c: 3746
	/* /Users/rubber/linux/kernel/sched/core.c: 3751
	 * If the task is descheduling and the only running task on the /Users/rubber/linux/kernel/sched/core.c: 3752
	 * CPU then use the wakelist to offload the task activation to /Users/rubber/linux/kernel/sched/core.c: 3753
	 * the soon-to-be-idle CPU as the current CPU is likely busy. /Users/rubber/linux/kernel/sched/core.c: 3754
	 * nr_running is checked to avoid unnecessary task stacking. /Users/rubber/linux/kernel/sched/core.c: 3755
 * Invoked from try_to_wake_up() to check whether the task can be woken up. /Users/rubber/linux/kernel/sched/core.c: 3801
 * The caller holds p::pi_lock if p != current or has preemption /Users/rubber/linux/kernel/sched/core.c: 3803
 * disabled when p == current. /Users/rubber/linux/kernel/sched/core.c: 3804
 * The rules of PREEMPT_RT saved_state: /Users/rubber/linux/kernel/sched/core.c: 3806
 *   The related locking code always holds p::pi_lock when updating /Users/rubber/linux/kernel/sched/core.c: 3808
 *   p::saved_state, which means the code is fully serialized in both cases. /Users/rubber/linux/kernel/sched/core.c: 3809
 *   The lock wait and lock wakeups happen via TASK_RTLOCK_WAIT. No other /Users/rubber/linux/kernel/sched/core.c: 3811
 *   bits set. This allows to distinguish all wakeup scenarios. /Users/rubber/linux/kernel/sched/core.c: 3812
	/* /Users/rubber/linux/kernel/sched/core.c: 3828
	 * Saved state preserves the task state across blocking on /Users/rubber/linux/kernel/sched/core.c: 3829
	 * an RT lock.  If the state matches, set p::saved_state to /Users/rubber/linux/kernel/sched/core.c: 3830
	 * TASK_RUNNING, but do not wake the task because it waits /Users/rubber/linux/kernel/sched/core.c: 3831
	 * for a lock wakeup. Also indicate success because from /Users/rubber/linux/kernel/sched/core.c: 3832
	 * the regular waker's point of view this has succeeded. /Users/rubber/linux/kernel/sched/core.c: 3833
	 * /Users/rubber/linux/kernel/sched/core.c: 3834
	 * After acquiring the lock the task will restore p::__state /Users/rubber/linux/kernel/sched/core.c: 3835
	 * from p::saved_state which ensures that the regular /Users/rubber/linux/kernel/sched/core.c: 3836
	 * wakeup is not lost. The restore will also set /Users/rubber/linux/kernel/sched/core.c: 3837
	 * p::saved_state to TASK_RUNNING so any further tests will /Users/rubber/linux/kernel/sched/core.c: 3838
	 * not result in false positives vs. @success /Users/rubber/linux/kernel/sched/core.c: 3839
 * Notes on Program-Order guarantees on SMP systems. /Users/rubber/linux/kernel/sched/core.c: 3850
 *  MIGRATION /Users/rubber/linux/kernel/sched/core.c: 3852
 * The basic program-order guarantee on SMP systems is that when a task [t] /Users/rubber/linux/kernel/sched/core.c: 3854
 * migrates, all its activity on its old CPU [c0] happens-before any subsequent /Users/rubber/linux/kernel/sched/core.c: 3855
 * execution on its new CPU [c1]. /Users/rubber/linux/kernel/sched/core.c: 3856
 * For migration (of runnable tasks) this is provided by the following means: /Users/rubber/linux/kernel/sched/core.c: 3858
 *  A) UNLOCK of the rq(c0)->lock scheduling out task t /Users/rubber/linux/kernel/sched/core.c: 3860
 *  B) migration for t is required to synchronize *both* rq(c0)->lock and /Users/rubber/linux/kernel/sched/core.c: 3861
 *     rq(c1)->lock (if not at the same time, then in that order). /Users/rubber/linux/kernel/sched/core.c: 3862
 *  C) LOCK of the rq(c1)->lock scheduling in task /Users/rubber/linux/kernel/sched/core.c: 3863
 * Release/acquire chaining guarantees that B happens after A and C after B. /Users/rubber/linux/kernel/sched/core.c: 3865
 * Note: the CPU doing B need not be c0 or c1 /Users/rubber/linux/kernel/sched/core.c: 3866
 * Example: /Users/rubber/linux/kernel/sched/core.c: 3868
 *   CPU0            CPU1            CPU2 /Users/rubber/linux/kernel/sched/core.c: 3870
 *   LOCK rq(0)->lock /Users/rubber/linux/kernel/sched/core.c: 3872
 *   sched-out X /Users/rubber/linux/kernel/sched/core.c: 3873
 *   sched-in Y /Users/rubber/linux/kernel/sched/core.c: 3874
 *   UNLOCK rq(0)->lock /Users/rubber/linux/kernel/sched/core.c: 3875
 *                                   LOCK rq(0)->lock // orders against CPU0 /Users/rubber/linux/kernel/sched/core.c: 3877
 *                                   dequeue X /Users/rubber/linux/kernel/sched/core.c: 3878
 *                                   UNLOCK rq(0)->lock /Users/rubber/linux/kernel/sched/core.c: 3879
 *                                   LOCK rq(1)->lock /Users/rubber/linux/kernel/sched/core.c: 3881
 *                                   enqueue X /Users/rubber/linux/kernel/sched/core.c: 3882
 *                                   UNLOCK rq(1)->lock /Users/rubber/linux/kernel/sched/core.c: 3883
 *                   LOCK rq(1)->lock // orders against CPU2 /Users/rubber/linux/kernel/sched/core.c: 3885
 *                   sched-out Z /Users/rubber/linux/kernel/sched/core.c: 3886
 *                   sched-in X /Users/rubber/linux/kernel/sched/core.c: 3887
 *                   UNLOCK rq(1)->lock /Users/rubber/linux/kernel/sched/core.c: 3888
 *  BLOCKING -- aka. SLEEP + WAKEUP /Users/rubber/linux/kernel/sched/core.c: 3891
 * For blocking we (obviously) need to provide the same guarantee as for /Users/rubber/linux/kernel/sched/core.c: 3893
 * migration. However the means are completely different as there is no lock /Users/rubber/linux/kernel/sched/core.c: 3894
 * chain to provide order. Instead we do: /Users/rubber/linux/kernel/sched/core.c: 3895
 *   1) smp_store_release(X->on_cpu, 0)   -- finish_task() /Users/rubber/linux/kernel/sched/core.c: 3897
 *   2) smp_cond_load_acquire(!X->on_cpu) -- try_to_wake_up() /Users/rubber/linux/kernel/sched/core.c: 3898
 * Example: /Users/rubber/linux/kernel/sched/core.c: 3900
 *   CPU0 (schedule)  CPU1 (try_to_wake_up) CPU2 (schedule) /Users/rubber/linux/kernel/sched/core.c: 3902
 *   LOCK rq(0)->lock LOCK X->pi_lock /Users/rubber/linux/kernel/sched/core.c: 3904
 *   dequeue X /Users/rubber/linux/kernel/sched/core.c: 3905
 *   sched-out X /Users/rubber/linux/kernel/sched/core.c: 3906
 *   smp_store_release(X->on_cpu, 0); /Users/rubber/linux/kernel/sched/core.c: 3907
 *                    smp_cond_load_acquire(&X->on_cpu, !VAL); /Users/rubber/linux/kernel/sched/core.c: 3909
 *                    X->state = WAKING /Users/rubber/linux/kernel/sched/core.c: 3910
 *                    set_task_cpu(X,2) /Users/rubber/linux/kernel/sched/core.c: 3911
 *                    LOCK rq(2)->lock /Users/rubber/linux/kernel/sched/core.c: 3913
 *                    enqueue X /Users/rubber/linux/kernel/sched/core.c: 3914
 *                    X->state = RUNNING /Users/rubber/linux/kernel/sched/core.c: 3915
 *                    UNLOCK rq(2)->lock /Users/rubber/linux/kernel/sched/core.c: 3916
 *                                          LOCK rq(2)->lock // orders against CPU1 /Users/rubber/linux/kernel/sched/core.c: 3918
 *                                          sched-out Z /Users/rubber/linux/kernel/sched/core.c: 3919
 *                                          sched-in X /Users/rubber/linux/kernel/sched/core.c: 3920
 *                                          UNLOCK rq(2)->lock /Users/rubber/linux/kernel/sched/core.c: 3921
 *                    UNLOCK X->pi_lock /Users/rubber/linux/kernel/sched/core.c: 3923
 *   UNLOCK rq(0)->lock /Users/rubber/linux/kernel/sched/core.c: 3924
 * However, for wakeups there is a second guarantee we must provide, namely we /Users/rubber/linux/kernel/sched/core.c: 3927
 * must ensure that CONDITION=1 done by the caller can not be reordered with /Users/rubber/linux/kernel/sched/core.c: 3928
 * accesses to the task state; see try_to_wake_up() and set_current_state(). /Users/rubber/linux/kernel/sched/core.c: 3929
 * try_to_wake_up - wake up a thread /Users/rubber/linux/kernel/sched/core.c: 3933
 * @p: the thread to be awakened /Users/rubber/linux/kernel/sched/core.c: 3934
 * @state: the mask of task states that can be woken /Users/rubber/linux/kernel/sched/core.c: 3935
 * @wake_flags: wake modifier flags (WF_*) /Users/rubber/linux/kernel/sched/core.c: 3936
 * Conceptually does: /Users/rubber/linux/kernel/sched/core.c: 3938
 *   If (@state & @p->state) @p->state = TASK_RUNNING. /Users/rubber/linux/kernel/sched/core.c: 3940
 * If the task was not queued/runnable, also place it back on a runqueue. /Users/rubber/linux/kernel/sched/core.c: 3942
 * This function is atomic against schedule() which would dequeue the task. /Users/rubber/linux/kernel/sched/core.c: 3944
 * It issues a full memory barrier before accessing @p->state, see the comment /Users/rubber/linux/kernel/sched/core.c: 3946
 * with set_current_state(). /Users/rubber/linux/kernel/sched/core.c: 3947
 * Uses p->pi_lock to serialize against concurrent wake-ups. /Users/rubber/linux/kernel/sched/core.c: 3949
 * Relies on p->pi_lock stabilizing: /Users/rubber/linux/kernel/sched/core.c: 3951
 *  - p->sched_class /Users/rubber/linux/kernel/sched/core.c: 3952
 *  - p->cpus_ptr /Users/rubber/linux/kernel/sched/core.c: 3953
 *  - p->sched_task_group /Users/rubber/linux/kernel/sched/core.c: 3954
 * in order to do migration, see its use of select_task_rq()/set_task_cpu(). /Users/rubber/linux/kernel/sched/core.c: 3955
 * Tries really hard to only take one task_rq(p)->lock for performance. /Users/rubber/linux/kernel/sched/core.c: 3957
 * Takes rq->lock in: /Users/rubber/linux/kernel/sched/core.c: 3958
 *  - ttwu_runnable()    -- old rq, unavoidable, see comment there; /Users/rubber/linux/kernel/sched/core.c: 3959
 *  - ttwu_queue()       -- new rq, for enqueue of the task; /Users/rubber/linux/kernel/sched/core.c: 3960
 *  - psi_ttwu_dequeue() -- much sadness :-( accounting will kill us. /Users/rubber/linux/kernel/sched/core.c: 3961
 * As a consequence we race really badly with just about everything. See the /Users/rubber/linux/kernel/sched/core.c: 3963
 * many memory barriers and their comments for details. /Users/rubber/linux/kernel/sched/core.c: 3964
 * Return: %true if @p->state changes (an actual wakeup was done), /Users/rubber/linux/kernel/sched/core.c: 3966
 *	   %false otherwise. /Users/rubber/linux/kernel/sched/core.c: 3967
		/* /Users/rubber/linux/kernel/sched/core.c: 3977
		 * We're waking current, this means 'p->on_rq' and 'task_cpu(p) /Users/rubber/linux/kernel/sched/core.c: 3978
		 * == smp_processor_id()'. Together this means we can special /Users/rubber/linux/kernel/sched/core.c: 3979
		 * case the whole 'p->on_rq && ttwu_runnable()' case below /Users/rubber/linux/kernel/sched/core.c: 3980
		 * without taking any locks. /Users/rubber/linux/kernel/sched/core.c: 3981
		 * /Users/rubber/linux/kernel/sched/core.c: 3982
		 * In particular: /Users/rubber/linux/kernel/sched/core.c: 3983
		 *  - we rely on Program-Order guarantees for all the ordering, /Users/rubber/linux/kernel/sched/core.c: 3984
		 *  - we're serialized against set_special_state() by virtue of /Users/rubber/linux/kernel/sched/core.c: 3985
		 *    it disabling IRQs (this allows not taking ->pi_lock). /Users/rubber/linux/kernel/sched/core.c: 3986
	/* /Users/rubber/linux/kernel/sched/core.c: 3997
	 * If we are going to wake up a thread waiting for CONDITION we /Users/rubber/linux/kernel/sched/core.c: 3998
	 * need to ensure that CONDITION=1 done by the caller can not be /Users/rubber/linux/kernel/sched/core.c: 3999
	 * reordered with p->state check below. This pairs with smp_store_mb() /Users/rubber/linux/kernel/sched/core.c: 4000
	 * in set_current_state() that the waiting thread does. /Users/rubber/linux/kernel/sched/core.c: 4001
	/* /Users/rubber/linux/kernel/sched/core.c: 4010
	 * Ensure we load p->on_rq _after_ p->state, otherwise it would /Users/rubber/linux/kernel/sched/core.c: 4011
	 * be possible to, falsely, observe p->on_rq == 0 and get stuck /Users/rubber/linux/kernel/sched/core.c: 4012
	 * in smp_cond_load_acquire() below. /Users/rubber/linux/kernel/sched/core.c: 4013
	 * /Users/rubber/linux/kernel/sched/core.c: 4014
	 * sched_ttwu_pending()			try_to_wake_up() /Users/rubber/linux/kernel/sched/core.c: 4015
	 *   STORE p->on_rq = 1			  LOAD p->state /Users/rubber/linux/kernel/sched/core.c: 4016
	 *   UNLOCK rq->lock /Users/rubber/linux/kernel/sched/core.c: 4017
	 * /Users/rubber/linux/kernel/sched/core.c: 4018
	 * __schedule() (switch to task 'p') /Users/rubber/linux/kernel/sched/core.c: 4019
	 *   LOCK rq->lock			  smp_rmb(); /Users/rubber/linux/kernel/sched/core.c: 4020
	 *   smp_mb__after_spinlock(); /Users/rubber/linux/kernel/sched/core.c: 4021
	 *   UNLOCK rq->lock /Users/rubber/linux/kernel/sched/core.c: 4022
	 * /Users/rubber/linux/kernel/sched/core.c: 4023
	 * [task p] /Users/rubber/linux/kernel/sched/core.c: 4024
	 *   STORE p->state = UNINTERRUPTIBLE	  LOAD p->on_rq /Users/rubber/linux/kernel/sched/core.c: 4025
	 * /Users/rubber/linux/kernel/sched/core.c: 4026
	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in /Users/rubber/linux/kernel/sched/core.c: 4027
	 * __schedule().  See the comment for smp_mb__after_spinlock(). /Users/rubber/linux/kernel/sched/core.c: 4028
	 * /Users/rubber/linux/kernel/sched/core.c: 4029
	 * A similar smb_rmb() lives in try_invoke_on_locked_down_task(). /Users/rubber/linux/kernel/sched/core.c: 4030
	/* /Users/rubber/linux/kernel/sched/core.c: 4037
	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be /Users/rubber/linux/kernel/sched/core.c: 4038
	 * possible to, falsely, observe p->on_cpu == 0. /Users/rubber/linux/kernel/sched/core.c: 4039
	 * /Users/rubber/linux/kernel/sched/core.c: 4040
	 * One must be running (->on_cpu == 1) in order to remove oneself /Users/rubber/linux/kernel/sched/core.c: 4041
	 * from the runqueue. /Users/rubber/linux/kernel/sched/core.c: 4042
	 * /Users/rubber/linux/kernel/sched/core.c: 4043
	 * __schedule() (switch to task 'p')	try_to_wake_up() /Users/rubber/linux/kernel/sched/core.c: 4044
	 *   STORE p->on_cpu = 1		  LOAD p->on_rq /Users/rubber/linux/kernel/sched/core.c: 4045
	 *   UNLOCK rq->lock /Users/rubber/linux/kernel/sched/core.c: 4046
	 * /Users/rubber/linux/kernel/sched/core.c: 4047
	 * __schedule() (put 'p' to sleep) /Users/rubber/linux/kernel/sched/core.c: 4048
	 *   LOCK rq->lock			  smp_rmb(); /Users/rubber/linux/kernel/sched/core.c: 4049
	 *   smp_mb__after_spinlock(); /Users/rubber/linux/kernel/sched/core.c: 4050
	 *   STORE p->on_rq = 0			  LOAD p->on_cpu /Users/rubber/linux/kernel/sched/core.c: 4051
	 * /Users/rubber/linux/kernel/sched/core.c: 4052
	 * Pairs with the LOCK+smp_mb__after_spinlock() on rq->lock in /Users/rubber/linux/kernel/sched/core.c: 4053
	 * __schedule().  See the comment for smp_mb__after_spinlock(). /Users/rubber/linux/kernel/sched/core.c: 4054
	 * /Users/rubber/linux/kernel/sched/core.c: 4055
	 * Form a control-dep-acquire with p->on_rq == 0 above, to ensure /Users/rubber/linux/kernel/sched/core.c: 4056
	 * schedule()'s deactivate_task() has 'happened' and p will no longer /Users/rubber/linux/kernel/sched/core.c: 4057
	 * care about it's own p->state. See the comment in __schedule(). /Users/rubber/linux/kernel/sched/core.c: 4058
	/* /Users/rubber/linux/kernel/sched/core.c: 4062
	 * We're doing the wakeup (@success == 1), they did a dequeue (p->on_rq /Users/rubber/linux/kernel/sched/core.c: 4063
	 * == 0), which means we need to do an enqueue, change p->state to /Users/rubber/linux/kernel/sched/core.c: 4064
	 * TASK_WAKING such that we can unlock p->pi_lock before doing the /Users/rubber/linux/kernel/sched/core.c: 4065
	 * enqueue, such as ttwu_queue_wakelist(). /Users/rubber/linux/kernel/sched/core.c: 4066
	/* /Users/rubber/linux/kernel/sched/core.c: 4070
	 * If the owning (remote) CPU is still in the middle of schedule() with /Users/rubber/linux/kernel/sched/core.c: 4071
	 * this task as prev, considering queueing p on the remote CPUs wake_list /Users/rubber/linux/kernel/sched/core.c: 4072
	 * which potentially sends an IPI instead of spinning on p->on_cpu to /Users/rubber/linux/kernel/sched/core.c: 4073
	 * let the waker make forward progress. This is safe because IRQs are /Users/rubber/linux/kernel/sched/core.c: 4074
	 * disabled and the IPI will deliver after on_cpu is cleared. /Users/rubber/linux/kernel/sched/core.c: 4075
	 * /Users/rubber/linux/kernel/sched/core.c: 4076
	 * Ensure we load task_cpu(p) after p->on_cpu: /Users/rubber/linux/kernel/sched/core.c: 4077
	 * /Users/rubber/linux/kernel/sched/core.c: 4078
	 * set_task_cpu(p, cpu); /Users/rubber/linux/kernel/sched/core.c: 4079
	 *   STORE p->cpu = @cpu /Users/rubber/linux/kernel/sched/core.c: 4080
	 * __schedule() (switch to task 'p') /Users/rubber/linux/kernel/sched/core.c: 4081
	 *   LOCK rq->lock /Users/rubber/linux/kernel/sched/core.c: 4082
	 *   smp_mb__after_spin_lock()		smp_cond_load_acquire(&p->on_cpu) /Users/rubber/linux/kernel/sched/core.c: 4083
	 *   STORE p->on_cpu = 1		LOAD p->cpu /Users/rubber/linux/kernel/sched/core.c: 4084
	 * /Users/rubber/linux/kernel/sched/core.c: 4085
	 * to ensure we observe the correct CPU on which the task is currently /Users/rubber/linux/kernel/sched/core.c: 4086
	 * scheduling. /Users/rubber/linux/kernel/sched/core.c: 4087
	/* /Users/rubber/linux/kernel/sched/core.c: 4093
	 * If the owning (remote) CPU is still in the middle of schedule() with /Users/rubber/linux/kernel/sched/core.c: 4094
	 * this task as prev, wait until it's done referencing the task. /Users/rubber/linux/kernel/sched/core.c: 4095
	 * /Users/rubber/linux/kernel/sched/core.c: 4096
	 * Pairs with the smp_store_release() in finish_task(). /Users/rubber/linux/kernel/sched/core.c: 4097
	 * /Users/rubber/linux/kernel/sched/core.c: 4098
	 * This ensures that tasks getting woken will be fully ordered against /Users/rubber/linux/kernel/sched/core.c: 4099
	 * their previous state and preserve Program Order. /Users/rubber/linux/kernel/sched/core.c: 4100
 * task_call_func - Invoke a function on task in fixed state /Users/rubber/linux/kernel/sched/core.c: 4131
 * @p: Process for which the function is to be invoked, can be @current. /Users/rubber/linux/kernel/sched/core.c: 4132
 * @func: Function to invoke. /Users/rubber/linux/kernel/sched/core.c: 4133
 * @arg: Argument to function. /Users/rubber/linux/kernel/sched/core.c: 4134
 * Fix the task in it's current state by avoiding wakeups and or rq operations /Users/rubber/linux/kernel/sched/core.c: 4136
 * and call @func(@arg) on it.  This function can use ->on_rq and task_curr() /Users/rubber/linux/kernel/sched/core.c: 4137
 * to work out what the state is, if required.  Given that @func can be invoked /Users/rubber/linux/kernel/sched/core.c: 4138
 * with a runqueue lock held, it had better be quite lightweight. /Users/rubber/linux/kernel/sched/core.c: 4139
 * Returns: /Users/rubber/linux/kernel/sched/core.c: 4141
 *   Whatever @func returns /Users/rubber/linux/kernel/sched/core.c: 4142
	/* /Users/rubber/linux/kernel/sched/core.c: 4155
	 * Ensure we load p->on_rq after p->__state, otherwise it would be /Users/rubber/linux/kernel/sched/core.c: 4156
	 * possible to, falsely, observe p->on_rq == 0. /Users/rubber/linux/kernel/sched/core.c: 4157
	 * /Users/rubber/linux/kernel/sched/core.c: 4158
	 * See try_to_wake_up() for a longer comment. /Users/rubber/linux/kernel/sched/core.c: 4159
	/* /Users/rubber/linux/kernel/sched/core.c: 4163
	 * Since pi->lock blocks try_to_wake_up(), we don't need rq->lock when /Users/rubber/linux/kernel/sched/core.c: 4164
	 * the task is blocked. Make sure to check @state since ttwu() can drop /Users/rubber/linux/kernel/sched/core.c: 4165
	 * locks at the end, see ttwu_queue_wakelist(). /Users/rubber/linux/kernel/sched/core.c: 4166
	/* /Users/rubber/linux/kernel/sched/core.c: 4171
	 * At this point the task is pinned; either: /Users/rubber/linux/kernel/sched/core.c: 4172
	 *  - blocked and we're holding off wakeups	 (pi->lock) /Users/rubber/linux/kernel/sched/core.c: 4173
	 *  - woken, and we're holding off enqueue	 (rq->lock) /Users/rubber/linux/kernel/sched/core.c: 4174
	 *  - queued, and we're holding off schedule	 (rq->lock) /Users/rubber/linux/kernel/sched/core.c: 4175
	 *  - running, and we're holding off de-schedule (rq->lock) /Users/rubber/linux/kernel/sched/core.c: 4176
	 * /Users/rubber/linux/kernel/sched/core.c: 4177
	 * The called function (@func) can use: task_curr(), p->on_rq and /Users/rubber/linux/kernel/sched/core.c: 4178
	 * p->__state to differentiate between these states. /Users/rubber/linux/kernel/sched/core.c: 4179
 * wake_up_process - Wake up a specific process /Users/rubber/linux/kernel/sched/core.c: 4191
 * @p: The process to be woken up. /Users/rubber/linux/kernel/sched/core.c: 4192
 * Attempt to wake up the nominated process and move it to the set of runnable /Users/rubber/linux/kernel/sched/core.c: 4194
 * processes. /Users/rubber/linux/kernel/sched/core.c: 4195
 * Return: 1 if the process was woken up, 0 if it was already running. /Users/rubber/linux/kernel/sched/core.c: 4197
 * This function executes a full memory barrier before accessing the task state. /Users/rubber/linux/kernel/sched/core.c: 4199
 * Perform scheduler related setup for a newly forked process p. /Users/rubber/linux/kernel/sched/core.c: 4213
 * p is forked by current. /Users/rubber/linux/kernel/sched/core.c: 4214
 * __sched_fork() is basic setup used by init_idle() too: /Users/rubber/linux/kernel/sched/core.c: 4216
 * fork()/clone()-time setup: /Users/rubber/linux/kernel/sched/core.c: 4364
	/* /Users/rubber/linux/kernel/sched/core.c: 4369
	 * We mark the process as NEW here. This guarantees that /Users/rubber/linux/kernel/sched/core.c: 4370
	 * nobody will actually run it, and a signal or other external /Users/rubber/linux/kernel/sched/core.c: 4371
	 * event cannot wake it up and insert it on the runqueue either. /Users/rubber/linux/kernel/sched/core.c: 4372
	/* /Users/rubber/linux/kernel/sched/core.c: 4376
	 * Make sure we do not leak PI boosting priority to the child. /Users/rubber/linux/kernel/sched/core.c: 4377
	/* /Users/rubber/linux/kernel/sched/core.c: 4383
	 * Revert to default priority/policy on fork if requested. /Users/rubber/linux/kernel/sched/core.c: 4384
		/* /Users/rubber/linux/kernel/sched/core.c: 4397
		 * We don't need the reset flag anymore after the fork. It has /Users/rubber/linux/kernel/sched/core.c: 4398
		 * fulfilled its duty: /Users/rubber/linux/kernel/sched/core.c: 4399
	/* /Users/rubber/linux/kernel/sched/core.c: 4442
	 * We're setting the CPU for the first time, we don't migrate, /Users/rubber/linux/kernel/sched/core.c: 4443
	 * so use __set_task_cpu(). /Users/rubber/linux/kernel/sched/core.c: 4444
	/* /Users/rubber/linux/kernel/sched/core.c: 4459
	 * Doing this here saves a lot of checks in all /Users/rubber/linux/kernel/sched/core.c: 4460
	 * the calling paths, and returning zero seems /Users/rubber/linux/kernel/sched/core.c: 4461
	 * safe for them anyway. /Users/rubber/linux/kernel/sched/core.c: 4462
 * wake_up_new_task - wake up a newly created task for the first time. /Users/rubber/linux/kernel/sched/core.c: 4471
 * This function will do some initial scheduler statistics housekeeping /Users/rubber/linux/kernel/sched/core.c: 4473
 * that must be done for every newly created context, then puts the task /Users/rubber/linux/kernel/sched/core.c: 4474
 * on the runqueue and wakes it. /Users/rubber/linux/kernel/sched/core.c: 4475
	/* /Users/rubber/linux/kernel/sched/core.c: 4485
	 * Fork balancing, do it here and not earlier because: /Users/rubber/linux/kernel/sched/core.c: 4486
	 *  - cpus_ptr can change in the fork path /Users/rubber/linux/kernel/sched/core.c: 4487
	 *  - any previously selected CPU might disappear through hotplug /Users/rubber/linux/kernel/sched/core.c: 4488
	 * /Users/rubber/linux/kernel/sched/core.c: 4489
	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq, /Users/rubber/linux/kernel/sched/core.c: 4490
	 * as we're not fully set-up yet. /Users/rubber/linux/kernel/sched/core.c: 4491
		/* /Users/rubber/linux/kernel/sched/core.c: 4506
		 * Nothing relies on rq->lock after this, so it's fine to /Users/rubber/linux/kernel/sched/core.c: 4507
		 * drop it. /Users/rubber/linux/kernel/sched/core.c: 4508
 * preempt_notifier_register - tell me when current is being preempted & rescheduled /Users/rubber/linux/kernel/sched/core.c: 4535
 * @notifier: notifier struct to register /Users/rubber/linux/kernel/sched/core.c: 4536
 * preempt_notifier_unregister - no longer interested in preemption notifications /Users/rubber/linux/kernel/sched/core.c: 4548
 * @notifier: notifier struct to unregister /Users/rubber/linux/kernel/sched/core.c: 4549
 * This is *not* safe to call from within a preemption notifier. /Users/rubber/linux/kernel/sched/core.c: 4551
	/* /Users/rubber/linux/kernel/sched/core.c: 4608
	 * Claim the task as running, we do this before switching to it /Users/rubber/linux/kernel/sched/core.c: 4609
	 * such that any running task will have this set. /Users/rubber/linux/kernel/sched/core.c: 4610
	 * /Users/rubber/linux/kernel/sched/core.c: 4611
	 * See the ttwu() WF_ON_CPU case and its ordering comment. /Users/rubber/linux/kernel/sched/core.c: 4612
	/* /Users/rubber/linux/kernel/sched/core.c: 4621
	 * This must be the very last reference to @prev from this CPU. After /Users/rubber/linux/kernel/sched/core.c: 4622
	 * p->on_cpu is cleared, the task can be moved to a different CPU. We /Users/rubber/linux/kernel/sched/core.c: 4623
	 * must ensure this doesn't happen until the switch is completely /Users/rubber/linux/kernel/sched/core.c: 4624
	 * finished. /Users/rubber/linux/kernel/sched/core.c: 4625
	 * /Users/rubber/linux/kernel/sched/core.c: 4626
	 * In particular, the load of prev->state in finish_task_switch() must /Users/rubber/linux/kernel/sched/core.c: 4627
	 * happen before this. /Users/rubber/linux/kernel/sched/core.c: 4628
	 * /Users/rubber/linux/kernel/sched/core.c: 4629
	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up(). /Users/rubber/linux/kernel/sched/core.c: 4630
	/* /Users/rubber/linux/kernel/sched/core.c: 4709
	 * Since the runqueue lock will be released by the next /Users/rubber/linux/kernel/sched/core.c: 4710
	 * task (which is an invalid locking op but in the case /Users/rubber/linux/kernel/sched/core.c: 4711
	 * of the scheduler it's an obvious special-case), so we /Users/rubber/linux/kernel/sched/core.c: 4712
	 * do an early lockdep release here: /Users/rubber/linux/kernel/sched/core.c: 4713
	/* /Users/rubber/linux/kernel/sched/core.c: 4725
	 * If we are tracking spinlock dependencies then we have to /Users/rubber/linux/kernel/sched/core.c: 4726
	 * fix up the runqueue lock - which gets 'carried over' from /Users/rubber/linux/kernel/sched/core.c: 4727
	 * prev into current: /Users/rubber/linux/kernel/sched/core.c: 4728
 * NOP if the arch has not defined these: /Users/rubber/linux/kernel/sched/core.c: 4736
 * prepare_task_switch - prepare to switch tasks /Users/rubber/linux/kernel/sched/core.c: 4764
 * @rq: the runqueue preparing to switch /Users/rubber/linux/kernel/sched/core.c: 4765
 * @prev: the current task that is being switched out /Users/rubber/linux/kernel/sched/core.c: 4766
 * @next: the task we are going to switch to. /Users/rubber/linux/kernel/sched/core.c: 4767
 * This is called with the rq lock held and interrupts off. It must /Users/rubber/linux/kernel/sched/core.c: 4769
 * be paired with a subsequent finish_task_switch after the context /Users/rubber/linux/kernel/sched/core.c: 4770
 * switch. /Users/rubber/linux/kernel/sched/core.c: 4771
 * prepare_task_switch sets up locking and calls architecture specific /Users/rubber/linux/kernel/sched/core.c: 4773
 * hooks. /Users/rubber/linux/kernel/sched/core.c: 4774
 * finish_task_switch - clean up after a task-switch /Users/rubber/linux/kernel/sched/core.c: 4791
 * @prev: the thread we just switched away from. /Users/rubber/linux/kernel/sched/core.c: 4792
 * finish_task_switch must be called after the context switch, paired /Users/rubber/linux/kernel/sched/core.c: 4794
 * with a prepare_task_switch call before the context switch. /Users/rubber/linux/kernel/sched/core.c: 4795
 * finish_task_switch will reconcile locking set up by prepare_task_switch, /Users/rubber/linux/kernel/sched/core.c: 4796
 * and do any other architecture-specific cleanup actions. /Users/rubber/linux/kernel/sched/core.c: 4797
 * Note that we may have delayed dropping an mm in context_switch(). If /Users/rubber/linux/kernel/sched/core.c: 4799
 * so, we finish that here outside of the runqueue lock. (Doing it /Users/rubber/linux/kernel/sched/core.c: 4800
 * with the lock held can cause deadlocks; see schedule() for /Users/rubber/linux/kernel/sched/core.c: 4801
 * details.) /Users/rubber/linux/kernel/sched/core.c: 4802
 * The context switch have flipped the stack from under us and restored the /Users/rubber/linux/kernel/sched/core.c: 4804
 * local variables which were saved when this task called schedule() in the /Users/rubber/linux/kernel/sched/core.c: 4805
 * past. prev == current is still correct but we need to recalculate this_rq /Users/rubber/linux/kernel/sched/core.c: 4806
 * because prev may have moved to another CPU. /Users/rubber/linux/kernel/sched/core.c: 4807
	/* /Users/rubber/linux/kernel/sched/core.c: 4816
	 * The previous task will have left us with a preempt_count of 2 /Users/rubber/linux/kernel/sched/core.c: 4817
	 * because it left us after: /Users/rubber/linux/kernel/sched/core.c: 4818
	 * /Users/rubber/linux/kernel/sched/core.c: 4819
	 *	schedule() /Users/rubber/linux/kernel/sched/core.c: 4820
	 *	  preempt_disable();			// 1 /Users/rubber/linux/kernel/sched/core.c: 4821
	 *	  __schedule() /Users/rubber/linux/kernel/sched/core.c: 4822
	 *	    raw_spin_lock_irq(&rq->lock)	// 2 /Users/rubber/linux/kernel/sched/core.c: 4823
	 * /Users/rubber/linux/kernel/sched/core.c: 4824
	 * Also, see FORK_PREEMPT_COUNT. /Users/rubber/linux/kernel/sched/core.c: 4825
	/* /Users/rubber/linux/kernel/sched/core.c: 4834
	 * A task struct has one reference for the use as "current". /Users/rubber/linux/kernel/sched/core.c: 4835
	 * If a task dies, then it sets TASK_DEAD in tsk->state and calls /Users/rubber/linux/kernel/sched/core.c: 4836
	 * schedule one last time. The schedule call will never return, and /Users/rubber/linux/kernel/sched/core.c: 4837
	 * the scheduled task must drop that reference. /Users/rubber/linux/kernel/sched/core.c: 4838
	 * /Users/rubber/linux/kernel/sched/core.c: 4839
	 * We must observe prev->state before clearing prev->on_cpu (in /Users/rubber/linux/kernel/sched/core.c: 4840
	 * finish_task), otherwise a concurrent wakeup can get prev /Users/rubber/linux/kernel/sched/core.c: 4841
	 * running on another CPU and we could rave with its RUNNING -> DEAD /Users/rubber/linux/kernel/sched/core.c: 4842
	 * transition, resulting in a double drop. /Users/rubber/linux/kernel/sched/core.c: 4843
	/* /Users/rubber/linux/kernel/sched/core.c: 4853
	 * kmap_local_sched_out() is invoked with rq::lock held and /Users/rubber/linux/kernel/sched/core.c: 4854
	 * interrupts disabled. There is no requirement for that, but the /Users/rubber/linux/kernel/sched/core.c: 4855
	 * sched out code does not have an interrupt enabled section. /Users/rubber/linux/kernel/sched/core.c: 4856
	 * Restoring the maps on sched in does not require interrupts being /Users/rubber/linux/kernel/sched/core.c: 4857
	 * disabled either. /Users/rubber/linux/kernel/sched/core.c: 4858
	/* /Users/rubber/linux/kernel/sched/core.c: 4863
	 * When switching through a kernel thread, the loop in /Users/rubber/linux/kernel/sched/core.c: 4864
	 * membarrier_{private,global}_expedited() may have observed that /Users/rubber/linux/kernel/sched/core.c: 4865
	 * kernel thread and not issued an IPI. It is therefore possible to /Users/rubber/linux/kernel/sched/core.c: 4866
	 * schedule between user->kernel->user threads without passing though /Users/rubber/linux/kernel/sched/core.c: 4867
	 * switch_mm(). Membarrier requires a barrier after storing to /Users/rubber/linux/kernel/sched/core.c: 4868
	 * rq->curr, before returning to userspace, so provide them here: /Users/rubber/linux/kernel/sched/core.c: 4869
	 * /Users/rubber/linux/kernel/sched/core.c: 4870
	 * - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly /Users/rubber/linux/kernel/sched/core.c: 4871
	 *   provided by mmdrop(), /Users/rubber/linux/kernel/sched/core.c: 4872
	 * - a sync_core for SYNC_CORE. /Users/rubber/linux/kernel/sched/core.c: 4873
 * schedule_tail - first thing a freshly forked thread must call. /Users/rubber/linux/kernel/sched/core.c: 4893
 * @prev: the thread we just switched away from. /Users/rubber/linux/kernel/sched/core.c: 4894
	/* /Users/rubber/linux/kernel/sched/core.c: 4899
	 * New tasks start with FORK_PREEMPT_COUNT, see there and /Users/rubber/linux/kernel/sched/core.c: 4900
	 * finish_task_switch() for details. /Users/rubber/linux/kernel/sched/core.c: 4901
	 * /Users/rubber/linux/kernel/sched/core.c: 4902
	 * finish_task_switch() will drop rq->lock() and lower preempt_count /Users/rubber/linux/kernel/sched/core.c: 4903
	 * and the preempt_enable() will end up enabling preemption (on /Users/rubber/linux/kernel/sched/core.c: 4904
	 * PREEMPT_COUNT kernels). /Users/rubber/linux/kernel/sched/core.c: 4905
 * context_switch - switch to the new MM and the new thread's register state. /Users/rubber/linux/kernel/sched/core.c: 4918
	/* /Users/rubber/linux/kernel/sched/core.c: 4926
	 * For paravirt, this is coupled with an exit in switch_to to /Users/rubber/linux/kernel/sched/core.c: 4927
	 * combine the page table reload and the switch backend into /Users/rubber/linux/kernel/sched/core.c: 4928
	 * one hypercall. /Users/rubber/linux/kernel/sched/core.c: 4929
	/* /Users/rubber/linux/kernel/sched/core.c: 4933
	 * kernel -> kernel   lazy + transfer active /Users/rubber/linux/kernel/sched/core.c: 4934
	 *   user -> kernel   lazy + mmgrab() active /Users/rubber/linux/kernel/sched/core.c: 4935
	 * /Users/rubber/linux/kernel/sched/core.c: 4936
	 * kernel ->   user   switch + mmdrop() active /Users/rubber/linux/kernel/sched/core.c: 4937
	 *   user ->   user   switch /Users/rubber/linux/kernel/sched/core.c: 4938
 to kernel /Users/rubber/linux/kernel/sched/core.c: 4940
 from user /Users/rubber/linux/kernel/sched/core.c: 4944
 to user /Users/rubber/linux/kernel/sched/core.c: 4948
		/* /Users/rubber/linux/kernel/sched/core.c: 4950
		 * sys_membarrier() requires an smp_mb() between setting /Users/rubber/linux/kernel/sched/core.c: 4951
		 * rq->curr / membarrier_switch_mm() and returning to userspace. /Users/rubber/linux/kernel/sched/core.c: 4952
		 * /Users/rubber/linux/kernel/sched/core.c: 4953
		 * The below provides this either through switch_mm(), or in /Users/rubber/linux/kernel/sched/core.c: 4954
		 * case 'prev->active_mm == next->mm' through /Users/rubber/linux/kernel/sched/core.c: 4955
		 * finish_task_switch()'s mmdrop(). /Users/rubber/linux/kernel/sched/core.c: 4956
 from kernel /Users/rubber/linux/kernel/sched/core.c: 4960
 * nr_running and nr_context_switches: /Users/rubber/linux/kernel/sched/core.c: 4979
 * externally visible scheduler statistics: current number of runnable /Users/rubber/linux/kernel/sched/core.c: 4981
 * threads, total number of context switches performed since bootup. /Users/rubber/linux/kernel/sched/core.c: 4982
 * Check if only the current task is running on the CPU. /Users/rubber/linux/kernel/sched/core.c: 4995
 * Caution: this function does not check that the caller has disabled /Users/rubber/linux/kernel/sched/core.c: 4997
 * preemption, thus the result might have a time-of-check-to-time-of-use /Users/rubber/linux/kernel/sched/core.c: 4998
 * race.  The caller is responsible to use it correctly, for example: /Users/rubber/linux/kernel/sched/core.c: 4999
 * - from a non-preemptible section (of course) /Users/rubber/linux/kernel/sched/core.c: 5001
 * - from a thread that is bound to a single CPU /Users/rubber/linux/kernel/sched/core.c: 5003
 * - in a loop with very short iterations (e.g. a polling loop) /Users/rubber/linux/kernel/sched/core.c: 5005
 * Consumers of these two interfaces, like for example the cpuidle menu /Users/rubber/linux/kernel/sched/core.c: 5025
 * governor, are using nonsensical data. Preferring shallow idle state selection /Users/rubber/linux/kernel/sched/core.c: 5026
 * for a CPU that has IO-wait which might not even end up running the task when /Users/rubber/linux/kernel/sched/core.c: 5027
 * it does become runnable. /Users/rubber/linux/kernel/sched/core.c: 5028
 * IO-wait accounting, and how it's mostly bollocks (on SMP). /Users/rubber/linux/kernel/sched/core.c: 5037
 * The idea behind IO-wait account is to account the idle time that we could /Users/rubber/linux/kernel/sched/core.c: 5039
 * have spend running if it were not for IO. That is, if we were to improve the /Users/rubber/linux/kernel/sched/core.c: 5040
 * storage performance, we'd have a proportional reduction in IO-wait time. /Users/rubber/linux/kernel/sched/core.c: 5041
 * This all works nicely on UP, where, when a task blocks on IO, we account /Users/rubber/linux/kernel/sched/core.c: 5043
 * idle time as IO-wait, because if the storage were faster, it could've been /Users/rubber/linux/kernel/sched/core.c: 5044
 * running and we'd not be idle. /Users/rubber/linux/kernel/sched/core.c: 5045
 * This has been extended to SMP, by doing the same for each CPU. This however /Users/rubber/linux/kernel/sched/core.c: 5047
 * is broken. /Users/rubber/linux/kernel/sched/core.c: 5048
 * Imagine for instance the case where two tasks block on one CPU, only the one /Users/rubber/linux/kernel/sched/core.c: 5050
 * CPU will have IO-wait accounted, while the other has regular idle. Even /Users/rubber/linux/kernel/sched/core.c: 5051
 * though, if the storage were faster, both could've ran at the same time, /Users/rubber/linux/kernel/sched/core.c: 5052
 * utilising both CPUs. /Users/rubber/linux/kernel/sched/core.c: 5053
 * This means, that when looking globally, the current IO-wait accounting on /Users/rubber/linux/kernel/sched/core.c: 5055
 * SMP is a lower bound, by reason of under accounting. /Users/rubber/linux/kernel/sched/core.c: 5056
 * Worse, since the numbers are provided per CPU, they are sometimes /Users/rubber/linux/kernel/sched/core.c: 5058
 * interpreted per CPU, and that is nonsensical. A blocked task isn't strictly /Users/rubber/linux/kernel/sched/core.c: 5059
 * associated with any one particular CPU, it can wake to another CPU than it /Users/rubber/linux/kernel/sched/core.c: 5060
 * blocked on. This means the per CPU IO-wait number is meaningless. /Users/rubber/linux/kernel/sched/core.c: 5061
 * Task CPU affinities can make all that even more 'interesting'. /Users/rubber/linux/kernel/sched/core.c: 5063
 * sched_exec - execve() is a valuable balancing opportunity, because at /Users/rubber/linux/kernel/sched/core.c: 5079
 * this point the task has the smallest effective memory and cache footprint. /Users/rubber/linux/kernel/sched/core.c: 5080
 * The function fair_sched_class.update_curr accesses the struct curr /Users/rubber/linux/kernel/sched/core.c: 5113
 * and its field curr->exec_start; when called from task_sched_runtime(), /Users/rubber/linux/kernel/sched/core.c: 5114
 * we observe a high rate of cache misses in practice. /Users/rubber/linux/kernel/sched/core.c: 5115
 * Prefetching this data results in improved performance. /Users/rubber/linux/kernel/sched/core.c: 5116
 * Return accounted runtime for the task. /Users/rubber/linux/kernel/sched/core.c: 5130
 * In case the task is currently running, return the runtime plus current's /Users/rubber/linux/kernel/sched/core.c: 5131
 * pending runtime that have not been accounted yet. /Users/rubber/linux/kernel/sched/core.c: 5132
	/* /Users/rubber/linux/kernel/sched/core.c: 5141
	 * 64-bit doesn't need locks to atomically read a 64-bit value. /Users/rubber/linux/kernel/sched/core.c: 5142
	 * So we have a optimization chance when the task's delta_exec is 0. /Users/rubber/linux/kernel/sched/core.c: 5143
	 * Reading ->on_cpu is racy, but this is ok. /Users/rubber/linux/kernel/sched/core.c: 5144
	 * /Users/rubber/linux/kernel/sched/core.c: 5145
	 * If we race with it leaving CPU, we'll take a lock. So we're correct. /Users/rubber/linux/kernel/sched/core.c: 5146
	 * If we race with it entering CPU, unaccounted time is 0. This is /Users/rubber/linux/kernel/sched/core.c: 5147
	 * indistinguishable from the read occurring a few cycles earlier. /Users/rubber/linux/kernel/sched/core.c: 5148
	 * If we see ->on_cpu without ->on_rq, the task is leaving, and has /Users/rubber/linux/kernel/sched/core.c: 5149
	 * been accounted, so we're correct here as well. /Users/rubber/linux/kernel/sched/core.c: 5150
	/* /Users/rubber/linux/kernel/sched/core.c: 5157
	 * Must be ->curr _and_ ->on_rq.  If dequeued, we would /Users/rubber/linux/kernel/sched/core.c: 5158
	 * project cycles that may never be accounted to this /Users/rubber/linux/kernel/sched/core.c: 5159
	 * thread, breaking clock_gettime(). /Users/rubber/linux/kernel/sched/core.c: 5160
 * This function gets called by the timer code, with HZ frequency. /Users/rubber/linux/kernel/sched/core.c: 5223
 * We call it with interrupts disabled. /Users/rubber/linux/kernel/sched/core.c: 5224
 * State diagram for ->state: /Users/rubber/linux/kernel/sched/core.c: 5274
 *          TICK_SCHED_REMOTE_OFFLINE /Users/rubber/linux/kernel/sched/core.c: 5277
 *                    |   ^ /Users/rubber/linux/kernel/sched/core.c: 5278
 *                    |   | /Users/rubber/linux/kernel/sched/core.c: 5279
 *                    |   | sched_tick_remote() /Users/rubber/linux/kernel/sched/core.c: 5280
 *                    |   | /Users/rubber/linux/kernel/sched/core.c: 5281
 *                    |   | /Users/rubber/linux/kernel/sched/core.c: 5282
 *                    +--TICK_SCHED_REMOTE_OFFLINING /Users/rubber/linux/kernel/sched/core.c: 5283
 *                    |   ^ /Users/rubber/linux/kernel/sched/core.c: 5284
 *                    |   | /Users/rubber/linux/kernel/sched/core.c: 5285
 * sched_tick_start() |   | sched_tick_stop() /Users/rubber/linux/kernel/sched/core.c: 5286
 *                    |   | /Users/rubber/linux/kernel/sched/core.c: 5287
 *                    V   | /Users/rubber/linux/kernel/sched/core.c: 5288
 *          TICK_SCHED_REMOTE_RUNNING /Users/rubber/linux/kernel/sched/core.c: 5289
 * Other transitions get WARN_ON_ONCE(), except that sched_tick_remote() /Users/rubber/linux/kernel/sched/core.c: 5292
 * and sched_tick_start() are happy to leave the state in RUNNING. /Users/rubber/linux/kernel/sched/core.c: 5293
	/* /Users/rubber/linux/kernel/sched/core.c: 5309
	 * Handle the tick only if it appears the remote CPU is running in full /Users/rubber/linux/kernel/sched/core.c: 5310
	 * dynticks mode. The check is racy by nature, but missing a tick or /Users/rubber/linux/kernel/sched/core.c: 5311
	 * having one too much is no big deal because the scheduler tick updates /Users/rubber/linux/kernel/sched/core.c: 5312
	 * statistics and checks timeslices in a time-independent way, regardless /Users/rubber/linux/kernel/sched/core.c: 5313
	 * of when exactly it is running. /Users/rubber/linux/kernel/sched/core.c: 5314
		/* /Users/rubber/linux/kernel/sched/core.c: 5327
		 * Make sure the next tick runs within a reasonable /Users/rubber/linux/kernel/sched/core.c: 5328
		 * amount of time. /Users/rubber/linux/kernel/sched/core.c: 5329
	/* /Users/rubber/linux/kernel/sched/core.c: 5341
	 * Run the remote tick once per second (1Hz). This arbitrary /Users/rubber/linux/kernel/sched/core.c: 5342
	 * frequency is large enough to avoid overload but short enough /Users/rubber/linux/kernel/sched/core.c: 5343
	 * to keep scheduler internal stats reasonably up to date.  But /Users/rubber/linux/kernel/sched/core.c: 5344
	 * first update state to reflect hotplug activity if required. /Users/rubber/linux/kernel/sched/core.c: 5345
 * If the value passed in is equal to the current preempt count /Users/rubber/linux/kernel/sched/core.c: 5407
 * then we just disabled preemption. Start timing the latency. /Users/rubber/linux/kernel/sched/core.c: 5408
	/* /Users/rubber/linux/kernel/sched/core.c: 5424
	 * Underflow? /Users/rubber/linux/kernel/sched/core.c: 5425
	/* /Users/rubber/linux/kernel/sched/core.c: 5432
	 * Spinlock count overflowing soon? /Users/rubber/linux/kernel/sched/core.c: 5433
 * If the value passed in equals to the current preempt count /Users/rubber/linux/kernel/sched/core.c: 5444
 * then we just enabled preemption. Stop timing the latency. /Users/rubber/linux/kernel/sched/core.c: 5445
	/* /Users/rubber/linux/kernel/sched/core.c: 5456
	 * Underflow? /Users/rubber/linux/kernel/sched/core.c: 5457
	/* /Users/rubber/linux/kernel/sched/core.c: 5461
	 * Is the spinlock portion underflowing? /Users/rubber/linux/kernel/sched/core.c: 5462
 * Print scheduling while atomic bug: /Users/rubber/linux/kernel/sched/core.c: 5490
 * Various schedule()-time debugging checks and statistics: /Users/rubber/linux/kernel/sched/core.c: 5520
	/* /Users/rubber/linux/kernel/sched/core.c: 5558
	 * We must do the balancing pass before put_prev_task(), such /Users/rubber/linux/kernel/sched/core.c: 5559
	 * that when we release the rq->lock the task is in the same /Users/rubber/linux/kernel/sched/core.c: 5560
	 * state as before we took rq->lock. /Users/rubber/linux/kernel/sched/core.c: 5561
	 * /Users/rubber/linux/kernel/sched/core.c: 5562
	 * We can terminate the balance pass as soon as we know there is /Users/rubber/linux/kernel/sched/core.c: 5563
	 * a runnable task of @class priority or higher. /Users/rubber/linux/kernel/sched/core.c: 5564
 * Pick up the highest-prio task: /Users/rubber/linux/kernel/sched/core.c: 5576
	/* /Users/rubber/linux/kernel/sched/core.c: 5584
	 * Optimization: we know that if all tasks are in the fair class we can /Users/rubber/linux/kernel/sched/core.c: 5585
	 * call that function directly, but only if the @prev task wasn't of a /Users/rubber/linux/kernel/sched/core.c: 5586
	 * higher scheduling class, because otherwise those lose the /Users/rubber/linux/kernel/sched/core.c: 5587
	 * opportunity to pull in more work from other CPUs. /Users/rubber/linux/kernel/sched/core.c: 5588
		/* /Users/rubber/linux/kernel/sched/core.c: 5671
		 * Reset core_pick so that we don't enter the fastpath when /Users/rubber/linux/kernel/sched/core.c: 5672
		 * coming online. core_pick would already be migrated to /Users/rubber/linux/kernel/sched/core.c: 5673
		 * another cpu during offline. /Users/rubber/linux/kernel/sched/core.c: 5674
	/* /Users/rubber/linux/kernel/sched/core.c: 5680
	 * If there were no {en,de}queues since we picked (IOW, the task /Users/rubber/linux/kernel/sched/core.c: 5681
	 * pointers are all still valid), and we haven't scheduled the last /Users/rubber/linux/kernel/sched/core.c: 5682
	 * pick yet, do so now. /Users/rubber/linux/kernel/sched/core.c: 5683
	 * /Users/rubber/linux/kernel/sched/core.c: 5684
	 * rq->core_pick can be NULL if no selection was made for a CPU because /Users/rubber/linux/kernel/sched/core.c: 5685
	 * it was either offline or went offline during a sibling's core-wide /Users/rubber/linux/kernel/sched/core.c: 5686
	 * selection. In this case, do a core-wide selection. /Users/rubber/linux/kernel/sched/core.c: 5687
	/* /Users/rubber/linux/kernel/sched/core.c: 5717
	 * core->core_task_seq, core->core_pick_seq, rq->core_sched_seq /Users/rubber/linux/kernel/sched/core.c: 5718
	 * /Users/rubber/linux/kernel/sched/core.c: 5719
	 * @task_seq guards the task state ({en,de}queues) /Users/rubber/linux/kernel/sched/core.c: 5720
	 * @pick_seq is the @task_seq we did a selection on /Users/rubber/linux/kernel/sched/core.c: 5721
	 * @sched_seq is the @pick_seq we scheduled /Users/rubber/linux/kernel/sched/core.c: 5722
	 * /Users/rubber/linux/kernel/sched/core.c: 5723
	 * However, preemptions can cause multiple picks on the same task set. /Users/rubber/linux/kernel/sched/core.c: 5724
	 * 'Fix' this by also increasing @task_seq for every pick. /Users/rubber/linux/kernel/sched/core.c: 5725
	/* /Users/rubber/linux/kernel/sched/core.c: 5729
	 * Optimize for common case where this CPU has no cookies /Users/rubber/linux/kernel/sched/core.c: 5730
	 * and there are no cookied tasks running on siblings. /Users/rubber/linux/kernel/sched/core.c: 5731
			/* /Users/rubber/linux/kernel/sched/core.c: 5737
			 * For robustness, update the min_vruntime_fi for /Users/rubber/linux/kernel/sched/core.c: 5738
			 * unconstrained picks as well. /Users/rubber/linux/kernel/sched/core.c: 5739
	/* /Users/rubber/linux/kernel/sched/core.c: 5747
	 * For each thread: do the regular task pick and find the max prio task /Users/rubber/linux/kernel/sched/core.c: 5748
	 * amongst them. /Users/rubber/linux/kernel/sched/core.c: 5749
	 * /Users/rubber/linux/kernel/sched/core.c: 5750
	 * Tie-break prio towards the current CPU /Users/rubber/linux/kernel/sched/core.c: 5751
	/* /Users/rubber/linux/kernel/sched/core.c: 5766
	 * For each thread: try and find a runnable task that matches @max or /Users/rubber/linux/kernel/sched/core.c: 5767
	 * force idle. /Users/rubber/linux/kernel/sched/core.c: 5768
	/* /Users/rubber/linux/kernel/sched/core.c: 5802
	 * Reschedule siblings /Users/rubber/linux/kernel/sched/core.c: 5803
	 * /Users/rubber/linux/kernel/sched/core.c: 5804
	 * NOTE: L1TF -- at this point we're no longer running the old task and /Users/rubber/linux/kernel/sched/core.c: 5805
	 * sending an IPI (below) ensures the sibling will no longer be running /Users/rubber/linux/kernel/sched/core.c: 5806
	 * their task. This ensures there is no inter-sibling overlap between /Users/rubber/linux/kernel/sched/core.c: 5807
	 * non-matching user state. /Users/rubber/linux/kernel/sched/core.c: 5808
		/* /Users/rubber/linux/kernel/sched/core.c: 5813
		 * An online sibling might have gone offline before a task /Users/rubber/linux/kernel/sched/core.c: 5814
		 * could be picked for it, or it might be offline but later /Users/rubber/linux/kernel/sched/core.c: 5815
		 * happen to come online, but its too late and nothing was /Users/rubber/linux/kernel/sched/core.c: 5816
		 * picked for it.  That's Ok - it will pick tasks for itself, /Users/rubber/linux/kernel/sched/core.c: 5817
		 * so ignore it. /Users/rubber/linux/kernel/sched/core.c: 5818
		/* /Users/rubber/linux/kernel/sched/core.c: 5823
		 * Update for new !FI->FI transitions, or if continuing to be in !FI: /Users/rubber/linux/kernel/sched/core.c: 5824
		 * fi_before     fi      update? /Users/rubber/linux/kernel/sched/core.c: 5825
		 *  0            0       1 /Users/rubber/linux/kernel/sched/core.c: 5826
		 *  0            1       1 /Users/rubber/linux/kernel/sched/core.c: 5827
		 *  1            0       1 /Users/rubber/linux/kernel/sched/core.c: 5828
		 *  1            1       0 /Users/rubber/linux/kernel/sched/core.c: 5829
 * Constants for the sched_mode argument of __schedule(). /Users/rubber/linux/kernel/sched/core.c: 6075
 * The mode argument allows RT enabled kernels to differentiate a /Users/rubber/linux/kernel/sched/core.c: 6077
 * preemption from blocking on an 'sleeping' spin/rwlock. Note that /Users/rubber/linux/kernel/sched/core.c: 6078
 * SM_MASK_PREEMPT for !RT has all bits set, which allows the compiler to /Users/rubber/linux/kernel/sched/core.c: 6079
 * optimize the AND operation out and just check for zero. /Users/rubber/linux/kernel/sched/core.c: 6080
 * __schedule() is the main scheduler function. /Users/rubber/linux/kernel/sched/core.c: 6093
 * The main means of driving the scheduler and thus entering this function are: /Users/rubber/linux/kernel/sched/core.c: 6095
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc. /Users/rubber/linux/kernel/sched/core.c: 6097
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return /Users/rubber/linux/kernel/sched/core.c: 6099
 *      paths. For example, see arch/x86/entry_64.S. /Users/rubber/linux/kernel/sched/core.c: 6100
 *      To drive preemption between tasks, the scheduler sets the flag in timer /Users/rubber/linux/kernel/sched/core.c: 6102
 *      interrupt handler scheduler_tick(). /Users/rubber/linux/kernel/sched/core.c: 6103
 *   3. Wakeups don't really cause entry into schedule(). They add a /Users/rubber/linux/kernel/sched/core.c: 6105
 *      task to the run-queue and that's it. /Users/rubber/linux/kernel/sched/core.c: 6106
 *      Now, if the new task added to the run-queue preempts the current /Users/rubber/linux/kernel/sched/core.c: 6108
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets /Users/rubber/linux/kernel/sched/core.c: 6109
 *      called on the nearest possible occasion: /Users/rubber/linux/kernel/sched/core.c: 6110
 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y): /Users/rubber/linux/kernel/sched/core.c: 6112
 *         - in syscall or exception context, at the next outmost /Users/rubber/linux/kernel/sched/core.c: 6114
 *           preempt_enable(). (this might be as soon as the wake_up()'s /Users/rubber/linux/kernel/sched/core.c: 6115
 *           spin_unlock()!) /Users/rubber/linux/kernel/sched/core.c: 6116
 *         - in IRQ context, return from interrupt-handler to /Users/rubber/linux/kernel/sched/core.c: 6118
 *           preemptible context /Users/rubber/linux/kernel/sched/core.c: 6119
 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set) /Users/rubber/linux/kernel/sched/core.c: 6121
 *         then at the next: /Users/rubber/linux/kernel/sched/core.c: 6122
 *          - cond_resched() call /Users/rubber/linux/kernel/sched/core.c: 6124
 *          - explicit schedule() call /Users/rubber/linux/kernel/sched/core.c: 6125
 *          - return from syscall or exception to user-space /Users/rubber/linux/kernel/sched/core.c: 6126
 *          - return from interrupt-handler to user-space /Users/rubber/linux/kernel/sched/core.c: 6127
 * WARNING: must be called with preemption disabled! /Users/rubber/linux/kernel/sched/core.c: 6129
	/* /Users/rubber/linux/kernel/sched/core.c: 6152
	 * Make sure that signal_pending_state()->signal_pending() below /Users/rubber/linux/kernel/sched/core.c: 6153
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE) /Users/rubber/linux/kernel/sched/core.c: 6154
	 * done by the caller to avoid the race with signal_wake_up(): /Users/rubber/linux/kernel/sched/core.c: 6155
	 * /Users/rubber/linux/kernel/sched/core.c: 6156
	 * __set_current_state(@state)		signal_wake_up() /Users/rubber/linux/kernel/sched/core.c: 6157
	 * schedule()				  set_tsk_thread_flag(p, TIF_SIGPENDING) /Users/rubber/linux/kernel/sched/core.c: 6158
	 *					  wake_up_state(p, state) /Users/rubber/linux/kernel/sched/core.c: 6159
	 *   LOCK rq->lock			    LOCK p->pi_state /Users/rubber/linux/kernel/sched/core.c: 6160
	 *   smp_mb__after_spinlock()		    smp_mb__after_spinlock() /Users/rubber/linux/kernel/sched/core.c: 6161
	 *     if (signal_pending_state())	    if (p->state & @state) /Users/rubber/linux/kernel/sched/core.c: 6162
	 * /Users/rubber/linux/kernel/sched/core.c: 6163
	 * Also, the membarrier system call requires a full memory barrier /Users/rubber/linux/kernel/sched/core.c: 6164
	 * after coming from user-space, before storing to rq->curr. /Users/rubber/linux/kernel/sched/core.c: 6165
	/* /Users/rubber/linux/kernel/sched/core.c: 6176
	 * We must load prev->state once (task_struct::state is volatile), such /Users/rubber/linux/kernel/sched/core.c: 6177
	 * that: /Users/rubber/linux/kernel/sched/core.c: 6178
	 * /Users/rubber/linux/kernel/sched/core.c: 6179
	 *  - we form a control dependency vs deactivate_task() below. /Users/rubber/linux/kernel/sched/core.c: 6180
	 *  - ptrace_{,un}freeze_traced() can change ->state underneath us. /Users/rubber/linux/kernel/sched/core.c: 6181
			/* /Users/rubber/linux/kernel/sched/core.c: 6196
			 * __schedule()			ttwu() /Users/rubber/linux/kernel/sched/core.c: 6197
			 *   prev_state = prev->state;    if (p->on_rq && ...) /Users/rubber/linux/kernel/sched/core.c: 6198
			 *   if (prev_state)		    goto out; /Users/rubber/linux/kernel/sched/core.c: 6199
			 *     p->on_rq = 0;		  smp_acquire__after_ctrl_dep(); /Users/rubber/linux/kernel/sched/core.c: 6200
			 *				  p->state = TASK_WAKING /Users/rubber/linux/kernel/sched/core.c: 6201
			 * /Users/rubber/linux/kernel/sched/core.c: 6202
			 * Where __schedule() and ttwu() have matching control dependencies. /Users/rubber/linux/kernel/sched/core.c: 6203
			 * /Users/rubber/linux/kernel/sched/core.c: 6204
			 * After this, schedule() must not care about p->state any more. /Users/rubber/linux/kernel/sched/core.c: 6205
		/* /Users/rubber/linux/kernel/sched/core.c: 6226
		 * RCU users of rcu_dereference(rq->curr) may not see /Users/rubber/linux/kernel/sched/core.c: 6227
		 * changes to task_struct made by pick_next_task(). /Users/rubber/linux/kernel/sched/core.c: 6228
		/* /Users/rubber/linux/kernel/sched/core.c: 6231
		 * The membarrier system call requires each architecture /Users/rubber/linux/kernel/sched/core.c: 6232
		 * to have a full memory barrier after updating /Users/rubber/linux/kernel/sched/core.c: 6233
		 * rq->curr, before returning to user-space. /Users/rubber/linux/kernel/sched/core.c: 6234
		 * /Users/rubber/linux/kernel/sched/core.c: 6235
		 * Here are the schemes providing that barrier on the /Users/rubber/linux/kernel/sched/core.c: 6236
		 * various architectures: /Users/rubber/linux/kernel/sched/core.c: 6237
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC. /Users/rubber/linux/kernel/sched/core.c: 6238
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC. /Users/rubber/linux/kernel/sched/core.c: 6239
		 * - finish_lock_switch() for weakly-ordered /Users/rubber/linux/kernel/sched/core.c: 6240
		 *   architectures where spin_unlock is a full barrier, /Users/rubber/linux/kernel/sched/core.c: 6241
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock /Users/rubber/linux/kernel/sched/core.c: 6242
		 *   is a RELEASE barrier), /Users/rubber/linux/kernel/sched/core.c: 6243
	/* /Users/rubber/linux/kernel/sched/core.c: 6287
	 * If a worker goes to sleep, notify and ask workqueue whether it /Users/rubber/linux/kernel/sched/core.c: 6288
	 * wants to wake up a task to maintain concurrency. /Users/rubber/linux/kernel/sched/core.c: 6289
	/* /Users/rubber/linux/kernel/sched/core.c: 6301
	 * If we are going to sleep and we have plugged IO queued, /Users/rubber/linux/kernel/sched/core.c: 6302
	 * make sure to submit it to avoid deadlocks. /Users/rubber/linux/kernel/sched/core.c: 6303
 * synchronize_rcu_tasks() makes sure that no task is stuck in preempted /Users/rubber/linux/kernel/sched/core.c: 6334
 * state (have scheduled out non-voluntarily) by making sure that all /Users/rubber/linux/kernel/sched/core.c: 6335
 * tasks have either left the run queue or have gone into user space. /Users/rubber/linux/kernel/sched/core.c: 6336
 * As idle tasks do not do either, they must not ever be preempted /Users/rubber/linux/kernel/sched/core.c: 6337
 * (schedule out non-voluntarily). /Users/rubber/linux/kernel/sched/core.c: 6338
 * schedule_idle() is similar to schedule_preempt_disable() except that it /Users/rubber/linux/kernel/sched/core.c: 6340
 * never enables preemption because it does not call sched_submit_work(). /Users/rubber/linux/kernel/sched/core.c: 6341
	/* /Users/rubber/linux/kernel/sched/core.c: 6345
	 * As this skips calling sched_submit_work(), which the idle task does /Users/rubber/linux/kernel/sched/core.c: 6346
	 * regardless because that function is a nop when the task is in a /Users/rubber/linux/kernel/sched/core.c: 6347
	 * TASK_RUNNING state, make sure this isn't used someplace that the /Users/rubber/linux/kernel/sched/core.c: 6348
	 * current task can be in any other state. Note, idle is always in the /Users/rubber/linux/kernel/sched/core.c: 6349
	 * TASK_RUNNING state. /Users/rubber/linux/kernel/sched/core.c: 6350
	/* /Users/rubber/linux/kernel/sched/core.c: 6361
	 * If we come here after a random call to set_need_resched(), /Users/rubber/linux/kernel/sched/core.c: 6362
	 * or we have been woken up remotely but the IPI has not yet arrived, /Users/rubber/linux/kernel/sched/core.c: 6363
	 * we haven't yet exited the RCU idle mode. Do it here manually until /Users/rubber/linux/kernel/sched/core.c: 6364
	 * we find a better solution. /Users/rubber/linux/kernel/sched/core.c: 6365
	 * /Users/rubber/linux/kernel/sched/core.c: 6366
	 * NB: There are buggy callers of this function.  Ideally we /Users/rubber/linux/kernel/sched/core.c: 6367
	 * should warn if prev_state != CONTEXT_USER, but that will trigger /Users/rubber/linux/kernel/sched/core.c: 6368
	 * too frequently to make sense yet. /Users/rubber/linux/kernel/sched/core.c: 6369
 * schedule_preempt_disabled - called with preemption disabled /Users/rubber/linux/kernel/sched/core.c: 6378
 * Returns with preemption disabled. Note: preempt_count must be 1 /Users/rubber/linux/kernel/sched/core.c: 6380
		/* /Users/rubber/linux/kernel/sched/core.c: 6404
		 * Because the function tracer can trace preempt_count_sub() /Users/rubber/linux/kernel/sched/core.c: 6405
		 * and it also uses preempt_enable/disable_notrace(), if /Users/rubber/linux/kernel/sched/core.c: 6406
		 * NEED_RESCHED is set, the preempt_enable_notrace() called /Users/rubber/linux/kernel/sched/core.c: 6407
		 * by the function tracer will call this function again and /Users/rubber/linux/kernel/sched/core.c: 6408
		 * cause infinite recursion. /Users/rubber/linux/kernel/sched/core.c: 6409
		 * /Users/rubber/linux/kernel/sched/core.c: 6410
		 * Preemption must be disabled here before the function /Users/rubber/linux/kernel/sched/core.c: 6411
		 * tracer can trace. Break up preempt_disable() into two /Users/rubber/linux/kernel/sched/core.c: 6412
		 * calls. One to disable preemption without fear of being /Users/rubber/linux/kernel/sched/core.c: 6413
		 * traced. The other to still record the preemption latency, /Users/rubber/linux/kernel/sched/core.c: 6414
		 * which can also be traced by the function tracer. /Users/rubber/linux/kernel/sched/core.c: 6415
		/* /Users/rubber/linux/kernel/sched/core.c: 6423
		 * Check again in case we missed a preemption opportunity /Users/rubber/linux/kernel/sched/core.c: 6424
		 * between schedule and now. /Users/rubber/linux/kernel/sched/core.c: 6425
 * This is the entry point to schedule() from in-kernel preemption /Users/rubber/linux/kernel/sched/core.c: 6432
 * off of preempt_enable. /Users/rubber/linux/kernel/sched/core.c: 6433
	/* /Users/rubber/linux/kernel/sched/core.c: 6437
	 * If there is a non-zero preempt_count or interrupts are disabled, /Users/rubber/linux/kernel/sched/core.c: 6438
	 * we do not want to preempt the current task. Just return.. /Users/rubber/linux/kernel/sched/core.c: 6439
 * preempt_schedule_notrace - preempt_schedule called by tracing /Users/rubber/linux/kernel/sched/core.c: 6456
 * The tracing infrastructure uses preempt_enable_notrace to prevent /Users/rubber/linux/kernel/sched/core.c: 6458
 * recursion and tracing preempt enabling caused by the tracing /Users/rubber/linux/kernel/sched/core.c: 6459
 * infrastructure itself. But as tracing can happen in areas coming /Users/rubber/linux/kernel/sched/core.c: 6460
 * from userspace or just about to enter userspace, a preempt enable /Users/rubber/linux/kernel/sched/core.c: 6461
 * can occur before user_exit() is called. This will cause the scheduler /Users/rubber/linux/kernel/sched/core.c: 6462
 * to be called when the system is still in usermode. /Users/rubber/linux/kernel/sched/core.c: 6463
 * To prevent this, the preempt_enable_notrace will use this function /Users/rubber/linux/kernel/sched/core.c: 6465
 * instead of preempt_schedule() to exit user context if needed before /Users/rubber/linux/kernel/sched/core.c: 6466
 * calling the scheduler. /Users/rubber/linux/kernel/sched/core.c: 6467
		/* /Users/rubber/linux/kernel/sched/core.c: 6477
		 * Because the function tracer can trace preempt_count_sub() /Users/rubber/linux/kernel/sched/core.c: 6478
		 * and it also uses preempt_enable/disable_notrace(), if /Users/rubber/linux/kernel/sched/core.c: 6479
		 * NEED_RESCHED is set, the preempt_enable_notrace() called /Users/rubber/linux/kernel/sched/core.c: 6480
		 * by the function tracer will call this function again and /Users/rubber/linux/kernel/sched/core.c: 6481
		 * cause infinite recursion. /Users/rubber/linux/kernel/sched/core.c: 6482
		 * /Users/rubber/linux/kernel/sched/core.c: 6483
		 * Preemption must be disabled here before the function /Users/rubber/linux/kernel/sched/core.c: 6484
		 * tracer can trace. Break up preempt_disable() into two /Users/rubber/linux/kernel/sched/core.c: 6485
		 * calls. One to disable preemption without fear of being /Users/rubber/linux/kernel/sched/core.c: 6486
		 * traced. The other to still record the preemption latency, /Users/rubber/linux/kernel/sched/core.c: 6487
		 * which can also be traced by the function tracer. /Users/rubber/linux/kernel/sched/core.c: 6488
		/* /Users/rubber/linux/kernel/sched/core.c: 6492
		 * Needs preempt disabled in case user_exit() is traced /Users/rubber/linux/kernel/sched/core.c: 6493
		 * and the tracer calls preempt_enable_notrace() causing /Users/rubber/linux/kernel/sched/core.c: 6494
		 * an infinite recursion. /Users/rubber/linux/kernel/sched/core.c: 6495
 * SC:cond_resched /Users/rubber/linux/kernel/sched/core.c: 6519
 * SC:might_resched /Users/rubber/linux/kernel/sched/core.c: 6520
 * SC:preempt_schedule /Users/rubber/linux/kernel/sched/core.c: 6521
 * SC:preempt_schedule_notrace /Users/rubber/linux/kernel/sched/core.c: 6522
 * SC:irqentry_exit_cond_resched /Users/rubber/linux/kernel/sched/core.c: 6523
 * NONE: /Users/rubber/linux/kernel/sched/core.c: 6526
 *   cond_resched               <- __cond_resched /Users/rubber/linux/kernel/sched/core.c: 6527
 *   might_resched              <- RET0 /Users/rubber/linux/kernel/sched/core.c: 6528
 *   preempt_schedule           <- NOP /Users/rubber/linux/kernel/sched/core.c: 6529
 *   preempt_schedule_notrace   <- NOP /Users/rubber/linux/kernel/sched/core.c: 6530
 *   irqentry_exit_cond_resched <- NOP /Users/rubber/linux/kernel/sched/core.c: 6531
 * VOLUNTARY: /Users/rubber/linux/kernel/sched/core.c: 6533
 *   cond_resched               <- __cond_resched /Users/rubber/linux/kernel/sched/core.c: 6534
 *   might_resched              <- __cond_resched /Users/rubber/linux/kernel/sched/core.c: 6535
 *   preempt_schedule           <- NOP /Users/rubber/linux/kernel/sched/core.c: 6536
 *   preempt_schedule_notrace   <- NOP /Users/rubber/linux/kernel/sched/core.c: 6537
 *   irqentry_exit_cond_resched <- NOP /Users/rubber/linux/kernel/sched/core.c: 6538
 * FULL: /Users/rubber/linux/kernel/sched/core.c: 6540
 *   cond_resched               <- RET0 /Users/rubber/linux/kernel/sched/core.c: 6541
 *   might_resched              <- RET0 /Users/rubber/linux/kernel/sched/core.c: 6542
 *   preempt_schedule           <- preempt_schedule /Users/rubber/linux/kernel/sched/core.c: 6543
 *   preempt_schedule_notrace   <- preempt_schedule_notrace /Users/rubber/linux/kernel/sched/core.c: 6544
 *   irqentry_exit_cond_resched <- irqentry_exit_cond_resched /Users/rubber/linux/kernel/sched/core.c: 6545
	/* /Users/rubber/linux/kernel/sched/core.c: 6573
	 * Avoid {NONE,VOLUNTARY} -> FULL transitions from ever ending up in /Users/rubber/linux/kernel/sched/core.c: 6574
	 * the ZERO state, which is invalid. /Users/rubber/linux/kernel/sched/core.c: 6575
 * This is the entry point to schedule() from kernel preemption /Users/rubber/linux/kernel/sched/core.c: 6651
 * off of irq context. /Users/rubber/linux/kernel/sched/core.c: 6652
 * Note, that this is called and return with irqs disabled. This will /Users/rubber/linux/kernel/sched/core.c: 6653
 * protect us against recursive calling from irq. /Users/rubber/linux/kernel/sched/core.c: 6654
 * rt_mutex_setprio - set the current priority of a task /Users/rubber/linux/kernel/sched/core.c: 6714
 * @p: task to boost /Users/rubber/linux/kernel/sched/core.c: 6715
 * @pi_task: donor task /Users/rubber/linux/kernel/sched/core.c: 6716
 * This function changes the 'effective' priority of a task. It does /Users/rubber/linux/kernel/sched/core.c: 6718
 * not touch ->normal_prio like __setscheduler(). /Users/rubber/linux/kernel/sched/core.c: 6719
 * Used by the rt_mutex code to implement priority inheritance /Users/rubber/linux/kernel/sched/core.c: 6721
 * logic. Call site only calls if the priority of the task changed. /Users/rubber/linux/kernel/sched/core.c: 6722
	/* /Users/rubber/linux/kernel/sched/core.c: 6735
	 * If nothing changed; bail early. /Users/rubber/linux/kernel/sched/core.c: 6736
	/* /Users/rubber/linux/kernel/sched/core.c: 6743
	 * Set under pi_lock && rq->lock, such that the value can be used under /Users/rubber/linux/kernel/sched/core.c: 6744
	 * either lock. /Users/rubber/linux/kernel/sched/core.c: 6745
	 * /Users/rubber/linux/kernel/sched/core.c: 6746
	 * Note that there is loads of tricky to make this pointer cache work /Users/rubber/linux/kernel/sched/core.c: 6747
	 * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to /Users/rubber/linux/kernel/sched/core.c: 6748
	 * ensure a task is de-boosted (pi_task is set to NULL) before the /Users/rubber/linux/kernel/sched/core.c: 6749
	 * task is allowed to run again (and can exit). This ensures the pointer /Users/rubber/linux/kernel/sched/core.c: 6750
	 * points to a blocked task -- which guarantees the task is present. /Users/rubber/linux/kernel/sched/core.c: 6751
	/* /Users/rubber/linux/kernel/sched/core.c: 6755
	 * For FIFO/RR we only need to set prio, if that matches we're done. /Users/rubber/linux/kernel/sched/core.c: 6756
	/* /Users/rubber/linux/kernel/sched/core.c: 6761
	 * Idle task boosting is a nono in general. There is one /Users/rubber/linux/kernel/sched/core.c: 6762
	 * exception, when PREEMPT_RT and NOHZ is active: /Users/rubber/linux/kernel/sched/core.c: 6763
	 * /Users/rubber/linux/kernel/sched/core.c: 6764
	 * The idle task calls get_next_timer_interrupt() and holds /Users/rubber/linux/kernel/sched/core.c: 6765
	 * the timer wheel base->lock on the CPU and another CPU wants /Users/rubber/linux/kernel/sched/core.c: 6766
	 * to access the timer (probably to cancel it). We can safely /Users/rubber/linux/kernel/sched/core.c: 6767
	 * ignore the boosting request, as the idle CPU runs this code /Users/rubber/linux/kernel/sched/core.c: 6768
	 * with interrupts disabled and will complete the lock /Users/rubber/linux/kernel/sched/core.c: 6769
	 * protected section without being interrupted. So there is no /Users/rubber/linux/kernel/sched/core.c: 6770
	 * real need to boost. /Users/rubber/linux/kernel/sched/core.c: 6771
	/* /Users/rubber/linux/kernel/sched/core.c: 6793
	 * Boosting condition are: /Users/rubber/linux/kernel/sched/core.c: 6794
	 * 1. -rt task is running and holds mutex A /Users/rubber/linux/kernel/sched/core.c: 6795
	 *      --> -dl task blocks on mutex A /Users/rubber/linux/kernel/sched/core.c: 6796
	 * /Users/rubber/linux/kernel/sched/core.c: 6797
	 * 2. -dl task is running and holds mutex A /Users/rubber/linux/kernel/sched/core.c: 6798
	 *      --> -dl task blocks on mutex A and could preempt the /Users/rubber/linux/kernel/sched/core.c: 6799
	 *          running task /Users/rubber/linux/kernel/sched/core.c: 6800
	/* /Users/rubber/linux/kernel/sched/core.c: 6857
	 * We have to be careful, if called from sys_setpriority(), /Users/rubber/linux/kernel/sched/core.c: 6858
	 * the task might be in the middle of scheduling on another CPU. /Users/rubber/linux/kernel/sched/core.c: 6859
	/* /Users/rubber/linux/kernel/sched/core.c: 6864
	 * The RT priorities are set via sched_setscheduler(), but we still /Users/rubber/linux/kernel/sched/core.c: 6865
	 * allow the 'normal' nice value to be set - but as expected /Users/rubber/linux/kernel/sched/core.c: 6866
	 * it won't have any effect on scheduling until the task is /Users/rubber/linux/kernel/sched/core.c: 6867
	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR: /Users/rubber/linux/kernel/sched/core.c: 6868
	/* /Users/rubber/linux/kernel/sched/core.c: 6891
	 * If the task increased its priority or is running and /Users/rubber/linux/kernel/sched/core.c: 6892
	 * lowered its priority, then reschedule its CPU: /Users/rubber/linux/kernel/sched/core.c: 6893
 * can_nice - check if a task can reduce its nice value /Users/rubber/linux/kernel/sched/core.c: 6903
 * @p: task /Users/rubber/linux/kernel/sched/core.c: 6904
 * @nice: nice value /Users/rubber/linux/kernel/sched/core.c: 6905
 * sys_nice - change the priority of the current process. /Users/rubber/linux/kernel/sched/core.c: 6919
 * @increment: priority increment /Users/rubber/linux/kernel/sched/core.c: 6920
 * sys_setpriority is a more generic, but much slower function that /Users/rubber/linux/kernel/sched/core.c: 6922
 * does similar things. /Users/rubber/linux/kernel/sched/core.c: 6923
	/* /Users/rubber/linux/kernel/sched/core.c: 6929
	 * Setpriority might change our priority at the same moment. /Users/rubber/linux/kernel/sched/core.c: 6930
	 * We don't have to worry. Conceptually one call occurs first /Users/rubber/linux/kernel/sched/core.c: 6931
	 * and we have a single winner. /Users/rubber/linux/kernel/sched/core.c: 6932
 * task_prio - return the priority value of a given task. /Users/rubber/linux/kernel/sched/core.c: 6952
 * @p: the task in question. /Users/rubber/linux/kernel/sched/core.c: 6953
 * Return: The priority value as seen by users in /proc. /Users/rubber/linux/kernel/sched/core.c: 6955
 * sched policy         return value   kernel prio    user prio/nice /Users/rubber/linux/kernel/sched/core.c: 6957
 * normal, batch, idle     [0 ... 39]  [100 ... 139]          0/[-20 ... 19] /Users/rubber/linux/kernel/sched/core.c: 6959
 * fifo, rr             [-2 ... -100]     [98 ... 0]  [1 ... 99] /Users/rubber/linux/kernel/sched/core.c: 6960
 * deadline                     -101             -1           0 /Users/rubber/linux/kernel/sched/core.c: 6961
 * idle_cpu - is a given CPU idle currently? /Users/rubber/linux/kernel/sched/core.c: 6969
 * @cpu: the processor in question. /Users/rubber/linux/kernel/sched/core.c: 6970
 * Return: 1 if the CPU is currently idle. 0 otherwise. /Users/rubber/linux/kernel/sched/core.c: 6972
 * available_idle_cpu - is a given CPU idle for enqueuing work. /Users/rubber/linux/kernel/sched/core.c: 6993
 * @cpu: the CPU in question. /Users/rubber/linux/kernel/sched/core.c: 6994
 * Return: 1 if the CPU is currently idle. 0 otherwise. /Users/rubber/linux/kernel/sched/core.c: 6996
 * idle_task - return the idle task for a given CPU. /Users/rubber/linux/kernel/sched/core.c: 7010
 * @cpu: the processor in question. /Users/rubber/linux/kernel/sched/core.c: 7011
 * Return: The idle task for the CPU @cpu. /Users/rubber/linux/kernel/sched/core.c: 7013
 * This function computes an effective utilization for the given CPU, to be /Users/rubber/linux/kernel/sched/core.c: 7022
 * used for frequency selection given the linear relation: f = u * f_max. /Users/rubber/linux/kernel/sched/core.c: 7023
 * The scheduler tracks the following metrics: /Users/rubber/linux/kernel/sched/core.c: 7025
 *   cpu_util_{cfs,rt,dl,irq}() /Users/rubber/linux/kernel/sched/core.c: 7027
 *   cpu_bw_dl() /Users/rubber/linux/kernel/sched/core.c: 7028
 * Where the cfs,rt and dl util numbers are tracked with the same metric and /Users/rubber/linux/kernel/sched/core.c: 7030
 * synchronized windows and are thus directly comparable. /Users/rubber/linux/kernel/sched/core.c: 7031
 * The cfs,rt,dl utilization are the running times measured with rq->clock_task /Users/rubber/linux/kernel/sched/core.c: 7033
 * which excludes things like IRQ and steal-time. These latter are then accrued /Users/rubber/linux/kernel/sched/core.c: 7034
 * in the irq utilization. /Users/rubber/linux/kernel/sched/core.c: 7035
 * The DL bandwidth number otoh is not a measured metric but a value computed /Users/rubber/linux/kernel/sched/core.c: 7037
 * based on the task model parameters and gives the minimal utilization /Users/rubber/linux/kernel/sched/core.c: 7038
 * required to meet deadlines. /Users/rubber/linux/kernel/sched/core.c: 7039
	/* /Users/rubber/linux/kernel/sched/core.c: 7053
	 * Early check to see if IRQ/steal time saturates the CPU, can be /Users/rubber/linux/kernel/sched/core.c: 7054
	 * because of inaccuracies in how we track these -- see /Users/rubber/linux/kernel/sched/core.c: 7055
	 * update_irq_load_avg(). /Users/rubber/linux/kernel/sched/core.c: 7056
	/* /Users/rubber/linux/kernel/sched/core.c: 7062
	 * Because the time spend on RT/DL tasks is visible as 'lost' time to /Users/rubber/linux/kernel/sched/core.c: 7063
	 * CFS tasks and we use the same metric to track the effective /Users/rubber/linux/kernel/sched/core.c: 7064
	 * utilization (PELT windows are synchronized) we can directly add them /Users/rubber/linux/kernel/sched/core.c: 7065
	 * to obtain the CPU's actual utilization. /Users/rubber/linux/kernel/sched/core.c: 7066
	 * /Users/rubber/linux/kernel/sched/core.c: 7067
	 * CFS and RT utilization can be boosted or capped, depending on /Users/rubber/linux/kernel/sched/core.c: 7068
	 * utilization clamp constraints requested by currently RUNNABLE /Users/rubber/linux/kernel/sched/core.c: 7069
	 * tasks. /Users/rubber/linux/kernel/sched/core.c: 7070
	 * When there are no CFS RUNNABLE tasks, clamps are released and /Users/rubber/linux/kernel/sched/core.c: 7071
	 * frequency will be gracefully reduced with the utilization decay. /Users/rubber/linux/kernel/sched/core.c: 7072
	/* /Users/rubber/linux/kernel/sched/core.c: 7080
	 * For frequency selection we do not make cpu_util_dl() a permanent part /Users/rubber/linux/kernel/sched/core.c: 7081
	 * of this sum because we want to use cpu_bw_dl() later on, but we need /Users/rubber/linux/kernel/sched/core.c: 7082
	 * to check if the CFS+RT+DL sum is saturated (ie. no idle time) such /Users/rubber/linux/kernel/sched/core.c: 7083
	 * that we select f_max when there is no idle time. /Users/rubber/linux/kernel/sched/core.c: 7084
	 * /Users/rubber/linux/kernel/sched/core.c: 7085
	 * NOTE: numerical errors or stop class might cause us to not quite hit /Users/rubber/linux/kernel/sched/core.c: 7086
	 * saturation when we should -- something for later. /Users/rubber/linux/kernel/sched/core.c: 7087
	/* /Users/rubber/linux/kernel/sched/core.c: 7092
	 * OTOH, for energy computation we need the estimated running time, so /Users/rubber/linux/kernel/sched/core.c: 7093
	 * include util_dl and ignore dl_bw. /Users/rubber/linux/kernel/sched/core.c: 7094
	/* /Users/rubber/linux/kernel/sched/core.c: 7099
	 * There is still idle time; further improve the number by using the /Users/rubber/linux/kernel/sched/core.c: 7100
	 * irq metric. Because IRQ/steal time is hidden from the task clock we /Users/rubber/linux/kernel/sched/core.c: 7101
	 * need to scale the task numbers: /Users/rubber/linux/kernel/sched/core.c: 7102
	 * /Users/rubber/linux/kernel/sched/core.c: 7103
	 *              max - irq /Users/rubber/linux/kernel/sched/core.c: 7104
	 *   U' = irq + --------- * U /Users/rubber/linux/kernel/sched/core.c: 7105
	 *                 max /Users/rubber/linux/kernel/sched/core.c: 7106
	/* /Users/rubber/linux/kernel/sched/core.c: 7111
	 * Bandwidth required by DEADLINE must always be granted while, for /Users/rubber/linux/kernel/sched/core.c: 7112
	 * FAIR and RT, we use blocked utilization of IDLE CPUs as a mechanism /Users/rubber/linux/kernel/sched/core.c: 7113
	 * to gracefully reduce the frequency when no tasks show up for longer /Users/rubber/linux/kernel/sched/core.c: 7114
	 * periods of time. /Users/rubber/linux/kernel/sched/core.c: 7115
	 * /Users/rubber/linux/kernel/sched/core.c: 7116
	 * Ideally we would like to set bw_dl as min/guaranteed freq and util + /Users/rubber/linux/kernel/sched/core.c: 7117
	 * bw_dl as requested freq. However, cpufreq is not yet ready for such /Users/rubber/linux/kernel/sched/core.c: 7118
	 * an interface. So, we only do the latter for now. /Users/rubber/linux/kernel/sched/core.c: 7119
 * find_process_by_pid - find a process with a matching PID value. /Users/rubber/linux/kernel/sched/core.c: 7135
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7136
 * The task of @pid, if found. %NULL otherwise. /Users/rubber/linux/kernel/sched/core.c: 7138
 * sched_setparam() passes in -1 for its policy, to let the functions /Users/rubber/linux/kernel/sched/core.c: 7146
 * it calls know not to change it. /Users/rubber/linux/kernel/sched/core.c: 7147
	/* /Users/rubber/linux/kernel/sched/core.c: 7166
	 * __sched_setscheduler() ensures attr->sched_priority == 0 when /Users/rubber/linux/kernel/sched/core.c: 7167
	 * !rt_policy. Always setting this ensures that things like /Users/rubber/linux/kernel/sched/core.c: 7168
	 * getparam()/getattr() don't report silly values for !rt tasks. /Users/rubber/linux/kernel/sched/core.c: 7169
 * Check the target process has a UID that matches the current process's: /Users/rubber/linux/kernel/sched/core.c: 7177
	/* /Users/rubber/linux/kernel/sched/core.c: 7222
	 * Valid priorities for SCHED_FIFO and SCHED_RR are /Users/rubber/linux/kernel/sched/core.c: 7223
	 * 1..MAX_RT_PRIO-1, valid priority for SCHED_NORMAL, /Users/rubber/linux/kernel/sched/core.c: 7224
	 * SCHED_BATCH and SCHED_IDLE is 0. /Users/rubber/linux/kernel/sched/core.c: 7225
	/* /Users/rubber/linux/kernel/sched/core.c: 7233
	 * Allow unprivileged RT tasks to decrease priority: /Users/rubber/linux/kernel/sched/core.c: 7234
		 /* /Users/rubber/linux/kernel/sched/core.c: 7257
		  * Can't set/change SCHED_DEADLINE policy at all for now /Users/rubber/linux/kernel/sched/core.c: 7258
		  * (safest behavior); in the future we would like to allow /Users/rubber/linux/kernel/sched/core.c: 7259
		  * unprivileged DL tasks to increase their relative deadline /Users/rubber/linux/kernel/sched/core.c: 7260
		  * or reduce their runtime (both ways reducing utilization) /Users/rubber/linux/kernel/sched/core.c: 7261
		/* /Users/rubber/linux/kernel/sched/core.c: 7266
		 * Treat SCHED_IDLE as nice 20. Only allow a switch to /Users/rubber/linux/kernel/sched/core.c: 7267
		 * SCHED_NORMAL if the RLIMIT_NICE would normally permit it. /Users/rubber/linux/kernel/sched/core.c: 7268
	/* /Users/rubber/linux/kernel/sched/core.c: 7303
	 * Make sure no PI-waiters arrive (or leave) while we are /Users/rubber/linux/kernel/sched/core.c: 7304
	 * changing the priority of the task: /Users/rubber/linux/kernel/sched/core.c: 7305
	 * /Users/rubber/linux/kernel/sched/core.c: 7306
	 * To be able to change p->policy safely, the appropriate /Users/rubber/linux/kernel/sched/core.c: 7307
	 * runqueue lock must be held. /Users/rubber/linux/kernel/sched/core.c: 7308
	/* /Users/rubber/linux/kernel/sched/core.c: 7313
	 * Changing the policy of the stop threads its a very bad idea: /Users/rubber/linux/kernel/sched/core.c: 7314
	/* /Users/rubber/linux/kernel/sched/core.c: 7321
	 * If not changing anything there's no need to proceed further, /Users/rubber/linux/kernel/sched/core.c: 7322
	 * but store a possible modification of reset_on_fork. /Users/rubber/linux/kernel/sched/core.c: 7323
		/* /Users/rubber/linux/kernel/sched/core.c: 7343
		 * Do not allow realtime tasks into groups that have no runtime /Users/rubber/linux/kernel/sched/core.c: 7344
		 * assigned. /Users/rubber/linux/kernel/sched/core.c: 7345
			/* /Users/rubber/linux/kernel/sched/core.c: 7359
			 * Don't allow tasks with an affinity mask smaller than /Users/rubber/linux/kernel/sched/core.c: 7360
			 * the entire root_domain to become SCHED_DEADLINE. We /Users/rubber/linux/kernel/sched/core.c: 7361
			 * will also fail if there's no bandwidth available. /Users/rubber/linux/kernel/sched/core.c: 7362
	/* /Users/rubber/linux/kernel/sched/core.c: 7382
	 * If setscheduling to SCHED_DEADLINE (or changing the parameters /Users/rubber/linux/kernel/sched/core.c: 7383
	 * of a SCHED_DEADLINE task) we need to check if enough bandwidth /Users/rubber/linux/kernel/sched/core.c: 7384
	 * is available. /Users/rubber/linux/kernel/sched/core.c: 7385
		/* /Users/rubber/linux/kernel/sched/core.c: 7397
		 * Take priority boosted tasks into account. If the new /Users/rubber/linux/kernel/sched/core.c: 7398
		 * effective priority is unchanged, we just store the new /Users/rubber/linux/kernel/sched/core.c: 7399
		 * normal parameters and do not touch the scheduler class and /Users/rubber/linux/kernel/sched/core.c: 7400
		 * the runqueue. This will be done when the task deboost /Users/rubber/linux/kernel/sched/core.c: 7401
		 * itself. /Users/rubber/linux/kernel/sched/core.c: 7402
		/* /Users/rubber/linux/kernel/sched/core.c: 7425
		 * We enqueue to tail when the priority of a task is /Users/rubber/linux/kernel/sched/core.c: 7426
		 * increased (user space view). /Users/rubber/linux/kernel/sched/core.c: 7427
 * sched_setscheduler - change the scheduling policy and/or RT priority of a thread. /Users/rubber/linux/kernel/sched/core.c: 7481
 * @p: the task in question. /Users/rubber/linux/kernel/sched/core.c: 7482
 * @policy: new policy. /Users/rubber/linux/kernel/sched/core.c: 7483
 * @param: structure containing the new RT priority. /Users/rubber/linux/kernel/sched/core.c: 7484
 * Use sched_set_fifo(), read its comment. /Users/rubber/linux/kernel/sched/core.c: 7486
 * Return: 0 on success. An error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 7488
 * NOTE that the task may be already dead. /Users/rubber/linux/kernel/sched/core.c: 7490
 * sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace. /Users/rubber/linux/kernel/sched/core.c: 7510
 * @p: the task in question. /Users/rubber/linux/kernel/sched/core.c: 7511
 * @policy: new policy. /Users/rubber/linux/kernel/sched/core.c: 7512
 * @param: structure containing the new RT priority. /Users/rubber/linux/kernel/sched/core.c: 7513
 * Just like sched_setscheduler, only don't bother checking if the /Users/rubber/linux/kernel/sched/core.c: 7515
 * current context has permission.  For example, this is needed in /Users/rubber/linux/kernel/sched/core.c: 7516
 * stop_machine(): we create temporary high priority worker threads, /Users/rubber/linux/kernel/sched/core.c: 7517
 * but our caller might not have that capability. /Users/rubber/linux/kernel/sched/core.c: 7518
 * Return: 0 on success. An error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 7520
 * SCHED_FIFO is a broken scheduler model; that is, it is fundamentally /Users/rubber/linux/kernel/sched/core.c: 7529
 * incapable of resource management, which is the one thing an OS really should /Users/rubber/linux/kernel/sched/core.c: 7530
 * be doing. /Users/rubber/linux/kernel/sched/core.c: 7531
 * This is of course the reason it is limited to privileged users only. /Users/rubber/linux/kernel/sched/core.c: 7533
 * Worse still; it is fundamentally impossible to compose static priority /Users/rubber/linux/kernel/sched/core.c: 7535
 * workloads. You cannot take two correctly working static prio workloads /Users/rubber/linux/kernel/sched/core.c: 7536
 * and smash them together and still expect them to work. /Users/rubber/linux/kernel/sched/core.c: 7537
 * For this reason 'all' FIFO tasks the kernel creates are basically at: /Users/rubber/linux/kernel/sched/core.c: 7539
 *   MAX_RT_PRIO / 2 /Users/rubber/linux/kernel/sched/core.c: 7541
 * The administrator _MUST_ configure the system, the kernel simply doesn't /Users/rubber/linux/kernel/sched/core.c: 7543
 * know enough information to make a sensible choice. /Users/rubber/linux/kernel/sched/core.c: 7544
 * For when you don't much care about FIFO, but want to be above SCHED_NORMAL. /Users/rubber/linux/kernel/sched/core.c: 7554
 * Mimics kernel/events/core.c perf_copy_attr(). /Users/rubber/linux/kernel/sched/core.c: 7601
	/* /Users/rubber/linux/kernel/sched/core.c: 7632
	 * XXX: Do we want to be lenient like existing syscalls; or do we want /Users/rubber/linux/kernel/sched/core.c: 7633
	 * to be strict and return an error on out-of-bounds values? /Users/rubber/linux/kernel/sched/core.c: 7634
 * sys_sched_setscheduler - set/change the scheduler policy and RT priority /Users/rubber/linux/kernel/sched/core.c: 7656
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7657
 * @policy: new policy. /Users/rubber/linux/kernel/sched/core.c: 7658
 * @param: structure containing the new RT priority. /Users/rubber/linux/kernel/sched/core.c: 7659
 * Return: 0 on success. An error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 7661
 * sys_sched_setparam - set/change the RT priority of a thread /Users/rubber/linux/kernel/sched/core.c: 7672
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7673
 * @param: structure containing the new RT priority. /Users/rubber/linux/kernel/sched/core.c: 7674
 * Return: 0 on success. An error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 7676
 * sys_sched_setattr - same as above, but with extended sched_attr /Users/rubber/linux/kernel/sched/core.c: 7684
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7685
 * @uattr: structure containing the extended parameters. /Users/rubber/linux/kernel/sched/core.c: 7686
 * @flags: for future extension. /Users/rubber/linux/kernel/sched/core.c: 7687
 * sys_sched_getscheduler - get the policy (scheduling class) of a thread /Users/rubber/linux/kernel/sched/core.c: 7726
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7727
 * Return: On success, the policy of the thread. Otherwise, a negative error /Users/rubber/linux/kernel/sched/core.c: 7729
 * code. /Users/rubber/linux/kernel/sched/core.c: 7730
 * sys_sched_getparam - get the RT priority of a thread /Users/rubber/linux/kernel/sched/core.c: 7754
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7755
 * @param: structure containing the RT priority. /Users/rubber/linux/kernel/sched/core.c: 7756
 * Return: On success, 0 and the RT priority is in @param. Otherwise, an error /Users/rubber/linux/kernel/sched/core.c: 7758
 * code. /Users/rubber/linux/kernel/sched/core.c: 7759
	/* /Users/rubber/linux/kernel/sched/core.c: 7784
	 * This one might sleep, we cannot do it with a spinlock held ... /Users/rubber/linux/kernel/sched/core.c: 7785
 * Copy the kernel size attribute structure (which might be larger /Users/rubber/linux/kernel/sched/core.c: 7797
 * than what user-space knows about) to user-space. /Users/rubber/linux/kernel/sched/core.c: 7798
 * Note that all cases are valid: user-space buffer can be larger or /Users/rubber/linux/kernel/sched/core.c: 7800
 * smaller than the kernel-space buffer. The usual case is that both /Users/rubber/linux/kernel/sched/core.c: 7801
 * have the same size. /Users/rubber/linux/kernel/sched/core.c: 7802
	/* /Users/rubber/linux/kernel/sched/core.c: 7814
	 * sched_getattr() ABI forwards and backwards compatibility: /Users/rubber/linux/kernel/sched/core.c: 7815
	 * /Users/rubber/linux/kernel/sched/core.c: 7816
	 * If usize == ksize then we just copy everything to user-space and all is good. /Users/rubber/linux/kernel/sched/core.c: 7817
	 * /Users/rubber/linux/kernel/sched/core.c: 7818
	 * If usize < ksize then we only copy as much as user-space has space for, /Users/rubber/linux/kernel/sched/core.c: 7819
	 * this keeps ABI compatibility as well. We skip the rest. /Users/rubber/linux/kernel/sched/core.c: 7820
	 * /Users/rubber/linux/kernel/sched/core.c: 7821
	 * If usize > ksize then user-space is using a newer version of the ABI, /Users/rubber/linux/kernel/sched/core.c: 7822
	 * which part the kernel doesn't know about. Just ignore it - tooling can /Users/rubber/linux/kernel/sched/core.c: 7823
	 * detect the kernel's knowledge of attributes from the attr->size value /Users/rubber/linux/kernel/sched/core.c: 7824
	 * which is set to ksize in this case. /Users/rubber/linux/kernel/sched/core.c: 7825
 * sys_sched_getattr - similar to sched_getparam, but with sched_attr /Users/rubber/linux/kernel/sched/core.c: 7836
 * @pid: the pid in question. /Users/rubber/linux/kernel/sched/core.c: 7837
 * @uattr: structure containing the extended parameters. /Users/rubber/linux/kernel/sched/core.c: 7838
 * @usize: sizeof(attr) for fwd/bwd comp. /Users/rubber/linux/kernel/sched/core.c: 7839
 * @flags: for future extension. /Users/rubber/linux/kernel/sched/core.c: 7840
	/* /Users/rubber/linux/kernel/sched/core.c: 7870
	 * This could race with another potential updater, but this is fine /Users/rubber/linux/kernel/sched/core.c: 7871
	 * because it'll correctly read the old or the new value. We don't need /Users/rubber/linux/kernel/sched/core.c: 7872
	 * to guarantee who wins the race as long as it doesn't return garbage. /Users/rubber/linux/kernel/sched/core.c: 7873
	/* /Users/rubber/linux/kernel/sched/core.c: 7893
	 * If the task isn't a deadline task or admission control is /Users/rubber/linux/kernel/sched/core.c: 7894
	 * disabled then we don't care about affinity changes. /Users/rubber/linux/kernel/sched/core.c: 7895
	/* /Users/rubber/linux/kernel/sched/core.c: 7900
	 * Since bandwidth control happens on root_domain basis, /Users/rubber/linux/kernel/sched/core.c: 7901
	 * if admission test is enabled, we only admit -deadline /Users/rubber/linux/kernel/sched/core.c: 7902
	 * tasks allowed to run on all the CPUs in the task's /Users/rubber/linux/kernel/sched/core.c: 7903
	 * root_domain. /Users/rubber/linux/kernel/sched/core.c: 7904
		/* /Users/rubber/linux/kernel/sched/core.c: 7941
		 * We must have raced with a concurrent cpuset update. /Users/rubber/linux/kernel/sched/core.c: 7942
		 * Just reset the cpumask to the cpuset's cpus_allowed. /Users/rubber/linux/kernel/sched/core.c: 7943
 * sys_sched_setaffinity - set the CPU affinity of a process /Users/rubber/linux/kernel/sched/core.c: 8010
 * @pid: pid of the process /Users/rubber/linux/kernel/sched/core.c: 8011
 * @len: length in bytes of the bitmask pointed to by user_mask_ptr /Users/rubber/linux/kernel/sched/core.c: 8012
 * @user_mask_ptr: user-space pointer to the new CPU mask /Users/rubber/linux/kernel/sched/core.c: 8013
 * Return: 0 on success. An error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 8015
 * sys_sched_getaffinity - get the CPU affinity of a process /Users/rubber/linux/kernel/sched/core.c: 8061
 * @pid: pid of the process /Users/rubber/linux/kernel/sched/core.c: 8062
 * @len: length in bytes of the bitmask pointed to by user_mask_ptr /Users/rubber/linux/kernel/sched/core.c: 8063
 * @user_mask_ptr: user-space pointer to hold the current CPU mask /Users/rubber/linux/kernel/sched/core.c: 8064
 * Return: size of CPU mask copied to user_mask_ptr on success. An /Users/rubber/linux/kernel/sched/core.c: 8066
 * error code otherwise. /Users/rubber/linux/kernel/sched/core.c: 8067
 * sys_sched_yield - yield the current processor to other threads. /Users/rubber/linux/kernel/sched/core.c: 8115
 * This function yields the current CPU to other tasks. If there are no /Users/rubber/linux/kernel/sched/core.c: 8117
 * other threads running on this CPU then this function will return. /Users/rubber/linux/kernel/sched/core.c: 8118
 * Return: 0. /Users/rubber/linux/kernel/sched/core.c: 8120
	/* /Users/rubber/linux/kernel/sched/core.c: 8135
	 * In preemptible kernels, ->rcu_read_lock_nesting tells the tick /Users/rubber/linux/kernel/sched/core.c: 8136
	 * whether the current CPU is in an RCU read-side critical section, /Users/rubber/linux/kernel/sched/core.c: 8137
	 * so the tick can report quiescent states even for CPUs looping /Users/rubber/linux/kernel/sched/core.c: 8138
	 * in kernel context.  In contrast, in non-preemptible kernels, /Users/rubber/linux/kernel/sched/core.c: 8139
	 * RCU readers leave no in-memory hints, which means that CPU-bound /Users/rubber/linux/kernel/sched/core.c: 8140
	 * processes executing in kernel context might never report an /Users/rubber/linux/kernel/sched/core.c: 8141
	 * RCU quiescent state.  Therefore, the following code causes /Users/rubber/linux/kernel/sched/core.c: 8142
	 * cond_resched() to report a quiescent state, but only when RCU /Users/rubber/linux/kernel/sched/core.c: 8143
	 * is in urgent need of one. /Users/rubber/linux/kernel/sched/core.c: 8144
 * __cond_resched_lock() - if a reschedule is pending, drop the given lock, /Users/rubber/linux/kernel/sched/core.c: 8163
 * call schedule, and on return reacquire the lock. /Users/rubber/linux/kernel/sched/core.c: 8164
 * This works OK both with and without CONFIG_PREEMPTION. We do strange low-level /Users/rubber/linux/kernel/sched/core.c: 8166
 * operations here to prevent schedule() from being called twice (once via /Users/rubber/linux/kernel/sched/core.c: 8167
 * spin_unlock(), once by hand). /Users/rubber/linux/kernel/sched/core.c: 8168
 * yield - yield the current processor to other threads. /Users/rubber/linux/kernel/sched/core.c: 8231
 * Do not ever use this function, there's a 99% chance you're doing it wrong. /Users/rubber/linux/kernel/sched/core.c: 8233
 * The scheduler is at all times free to pick the calling task as the most /Users/rubber/linux/kernel/sched/core.c: 8235
 * eligible task to run, if removing the yield() call from your code breaks /Users/rubber/linux/kernel/sched/core.c: 8236
 * it, it's already broken. /Users/rubber/linux/kernel/sched/core.c: 8237
 * Typical broken usage is: /Users/rubber/linux/kernel/sched/core.c: 8239
 * while (!event) /Users/rubber/linux/kernel/sched/core.c: 8241
 *	yield(); /Users/rubber/linux/kernel/sched/core.c: 8242
 * where one assumes that yield() will let 'the other' process run that will /Users/rubber/linux/kernel/sched/core.c: 8244
 * make event true. If the current task is a SCHED_FIFO task that will never /Users/rubber/linux/kernel/sched/core.c: 8245
 * happen. Never use yield() as a progress guarantee!! /Users/rubber/linux/kernel/sched/core.c: 8246
 * If you want to use yield() to wait for something, use wait_event(). /Users/rubber/linux/kernel/sched/core.c: 8248
 * If you want to use yield() to be 'nice' for others, use cond_resched(). /Users/rubber/linux/kernel/sched/core.c: 8249
 * If you still want to use yield(), do not! /Users/rubber/linux/kernel/sched/core.c: 8250
 * yield_to - yield the current processor to another thread in /Users/rubber/linux/kernel/sched/core.c: 8260
 * your thread group, or accelerate that thread toward the /Users/rubber/linux/kernel/sched/core.c: 8261
 * processor it's on. /Users/rubber/linux/kernel/sched/core.c: 8262
 * @p: target task /Users/rubber/linux/kernel/sched/core.c: 8263
 * @preempt: whether task preemption is allowed or not /Users/rubber/linux/kernel/sched/core.c: 8264
 * It's the caller's job to ensure that the target task struct /Users/rubber/linux/kernel/sched/core.c: 8266
 * can't go away on us before we can do any checks. /Users/rubber/linux/kernel/sched/core.c: 8267
 * Return: /Users/rubber/linux/kernel/sched/core.c: 8269
 *	true (>0) if we indeed boosted the target task. /Users/rubber/linux/kernel/sched/core.c: 8270
 *	false (0) if we failed to boost the target. /Users/rubber/linux/kernel/sched/core.c: 8271
 *	-ESRCH if there's no task to yield to. /Users/rubber/linux/kernel/sched/core.c: 8272
	/* /Users/rubber/linux/kernel/sched/core.c: 8286
	 * If we're the only runnable task on the rq and target rq also /Users/rubber/linux/kernel/sched/core.c: 8287
	 * has only one task, there's absolutely no point in yielding. /Users/rubber/linux/kernel/sched/core.c: 8288
		/* /Users/rubber/linux/kernel/sched/core.c: 8313
		 * Make p's CPU reschedule; pick_next_entity takes care of /Users/rubber/linux/kernel/sched/core.c: 8314
		 * fairness. /Users/rubber/linux/kernel/sched/core.c: 8315
 * This task is about to go to sleep on IO. Increment rq->nr_iowait so /Users/rubber/linux/kernel/sched/core.c: 8350
 * that process accounting knows that this is a task in IO wait state. /Users/rubber/linux/kernel/sched/core.c: 8351
 * sys_sched_get_priority_max - return maximum RT priority. /Users/rubber/linux/kernel/sched/core.c: 8377
 * @policy: scheduling class. /Users/rubber/linux/kernel/sched/core.c: 8378
 * Return: On success, this syscall returns the maximum /Users/rubber/linux/kernel/sched/core.c: 8380
 * rt_priority that can be used by a given scheduling class. /Users/rubber/linux/kernel/sched/core.c: 8381
 * On failure, a negative error code is returned. /Users/rubber/linux/kernel/sched/core.c: 8382
 * sys_sched_get_priority_min - return minimum RT priority. /Users/rubber/linux/kernel/sched/core.c: 8404
 * @policy: scheduling class. /Users/rubber/linux/kernel/sched/core.c: 8405
 * Return: On success, this syscall returns the minimum /Users/rubber/linux/kernel/sched/core.c: 8407
 * rt_priority that can be used by a given scheduling class. /Users/rubber/linux/kernel/sched/core.c: 8408
 * On failure, a negative error code is returned. /Users/rubber/linux/kernel/sched/core.c: 8409
 * sys_sched_rr_get_interval - return the default timeslice of a process. /Users/rubber/linux/kernel/sched/core.c: 8466
 * @pid: pid of the process. /Users/rubber/linux/kernel/sched/core.c: 8467
 * @interval: userspace pointer to the timeslice value. /Users/rubber/linux/kernel/sched/core.c: 8468
 * this syscall writes the default timeslice value of a given process /Users/rubber/linux/kernel/sched/core.c: 8470
 * into the user-space timespec buffer. A value of '0' means infinity. /Users/rubber/linux/kernel/sched/core.c: 8471
 * Return: On success, 0 and the timeslice is in @interval. Otherwise, /Users/rubber/linux/kernel/sched/core.c: 8473
 * an error code. /Users/rubber/linux/kernel/sched/core.c: 8474
	/* /Users/rubber/linux/kernel/sched/core.c: 8545
	 * When looking for TASK_UNINTERRUPTIBLE skip TASK_IDLE (allows /Users/rubber/linux/kernel/sched/core.c: 8546
	 * TASK_KILLABLE). /Users/rubber/linux/kernel/sched/core.c: 8547
		/* /Users/rubber/linux/kernel/sched/core.c: 8562
		 * reset the NMI-timeout, listing all files on a slow /Users/rubber/linux/kernel/sched/core.c: 8563
		 * console might take a lot of time: /Users/rubber/linux/kernel/sched/core.c: 8564
		 * Also, reset softlockup watchdogs on all CPUs, because /Users/rubber/linux/kernel/sched/core.c: 8565
		 * another CPU might be blocked waiting for us to process /Users/rubber/linux/kernel/sched/core.c: 8566
		 * an IPI. /Users/rubber/linux/kernel/sched/core.c: 8567
	/* /Users/rubber/linux/kernel/sched/core.c: 8580
	 * Only show locks if all tasks are dumped: /Users/rubber/linux/kernel/sched/core.c: 8581
 * init_idle - set up an idle thread for a given CPU /Users/rubber/linux/kernel/sched/core.c: 8588
 * @idle: task in question /Users/rubber/linux/kernel/sched/core.c: 8589
 * @cpu: CPU the idle task belongs to /Users/rubber/linux/kernel/sched/core.c: 8590
 * NOTE: this function does not set the idle thread's NEED_RESCHED /Users/rubber/linux/kernel/sched/core.c: 8592
 * flag, to make booting more robust. /Users/rubber/linux/kernel/sched/core.c: 8593
	/* /Users/rubber/linux/kernel/sched/core.c: 8602
	 * The idle task doesn't need the kthread struct to function, but it /Users/rubber/linux/kernel/sched/core.c: 8603
	 * is dressed up as a per-CPU kthread and thus needs to play the part /Users/rubber/linux/kernel/sched/core.c: 8604
	 * if we want to avoid special-casing it in code that deals with per-CPU /Users/rubber/linux/kernel/sched/core.c: 8605
	 * kthreads. /Users/rubber/linux/kernel/sched/core.c: 8606
	/* /Users/rubber/linux/kernel/sched/core.c: 8615
	 * PF_KTHREAD should already be set at this point; regardless, make it /Users/rubber/linux/kernel/sched/core.c: 8616
	 * look like a proper per-CPU kthread. /Users/rubber/linux/kernel/sched/core.c: 8617
	/* /Users/rubber/linux/kernel/sched/core.c: 8626
	 * It's possible that init_idle() gets called multiple times on a task, /Users/rubber/linux/kernel/sched/core.c: 8627
	 * in that case do_set_cpus_allowed() will not do the right thing. /Users/rubber/linux/kernel/sched/core.c: 8628
	 * /Users/rubber/linux/kernel/sched/core.c: 8629
	 * And since this is boot we can forgo the serialization. /Users/rubber/linux/kernel/sched/core.c: 8630
	/* /Users/rubber/linux/kernel/sched/core.c: 8634
	 * We're having a chicken and egg problem, even though we are /Users/rubber/linux/kernel/sched/core.c: 8635
	 * holding rq->lock, the CPU isn't yet set to this CPU so the /Users/rubber/linux/kernel/sched/core.c: 8636
	 * lockdep check in task_group() will fail. /Users/rubber/linux/kernel/sched/core.c: 8637
	 * /Users/rubber/linux/kernel/sched/core.c: 8638
	 * Similar case to sched_fork(). / Alternatively we could /Users/rubber/linux/kernel/sched/core.c: 8639
	 * use task_rq_lock() here and obtain the other rq->lock. /Users/rubber/linux/kernel/sched/core.c: 8640
	 * /Users/rubber/linux/kernel/sched/core.c: 8641
	 * Silence PROVE_RCU /Users/rubber/linux/kernel/sched/core.c: 8642
	/* /Users/rubber/linux/kernel/sched/core.c: 8660
	 * The idle tasks have their own, simple scheduling class: /Users/rubber/linux/kernel/sched/core.c: 8661
	/* /Users/rubber/linux/kernel/sched/core.c: 8691
	 * Kthreads which disallow setaffinity shouldn't be moved /Users/rubber/linux/kernel/sched/core.c: 8692
	 * to a new cpuset; we don't want to change their CPU /Users/rubber/linux/kernel/sched/core.c: 8693
	 * affinity and isolating such threads by their set of /Users/rubber/linux/kernel/sched/core.c: 8694
	 * allowed nodes is unnecessary.  Thus, cpusets are not /Users/rubber/linux/kernel/sched/core.c: 8695
	 * applicable for such threads.  This prevents checking for /Users/rubber/linux/kernel/sched/core.c: 8696
	 * success of set_cpus_allowed_ptr() on all attached tasks /Users/rubber/linux/kernel/sched/core.c: 8697
	 * before cpus_mask may be changed. /Users/rubber/linux/kernel/sched/core.c: 8698
 * Requeue a task on a given node and accurately track the number of NUMA /Users/rubber/linux/kernel/sched/core.c: 8735
 * tasks on the runqueues /Users/rubber/linux/kernel/sched/core.c: 8736
 * Ensure that the idle task is using init_mm right before its CPU goes /Users/rubber/linux/kernel/sched/core.c: 8765
 * offline. /Users/rubber/linux/kernel/sched/core.c: 8766
 * Ensure we only run per-cpu kthreads once the CPU goes !active. /Users/rubber/linux/kernel/sched/core.c: 8812
 * This is enabled below SCHED_AP_ACTIVE; when !cpu_active(), but only /Users/rubber/linux/kernel/sched/core.c: 8814
 * effective when the hotplug motion is down. /Users/rubber/linux/kernel/sched/core.c: 8815
	/* /Users/rubber/linux/kernel/sched/core.c: 8823
	 * Ensure the thing is persistent until balance_push_set(.on = false); /Users/rubber/linux/kernel/sched/core.c: 8824
	/* /Users/rubber/linux/kernel/sched/core.c: 8828
	 * Only active while going offline and when invoked on the outgoing /Users/rubber/linux/kernel/sched/core.c: 8829
	 * CPU. /Users/rubber/linux/kernel/sched/core.c: 8830
	/* /Users/rubber/linux/kernel/sched/core.c: 8835
	 * Both the cpu-hotplug and stop task are in this case and are /Users/rubber/linux/kernel/sched/core.c: 8836
	 * required to complete the hotplug process. /Users/rubber/linux/kernel/sched/core.c: 8837
		/* /Users/rubber/linux/kernel/sched/core.c: 8842
		 * If this is the idle task on the outgoing CPU try to wake /Users/rubber/linux/kernel/sched/core.c: 8843
		 * up the hotplug control thread which might wait for the /Users/rubber/linux/kernel/sched/core.c: 8844
		 * last task to vanish. The rcuwait_active() check is /Users/rubber/linux/kernel/sched/core.c: 8845
		 * accurate here because the waiter is pinned on this CPU /Users/rubber/linux/kernel/sched/core.c: 8846
		 * and can't obviously be running in parallel. /Users/rubber/linux/kernel/sched/core.c: 8847
		 * /Users/rubber/linux/kernel/sched/core.c: 8848
		 * On RT kernels this also has to check whether there are /Users/rubber/linux/kernel/sched/core.c: 8849
		 * pinned and scheduled out tasks on the runqueue. They /Users/rubber/linux/kernel/sched/core.c: 8850
		 * need to leave the migrate disabled section first. /Users/rubber/linux/kernel/sched/core.c: 8851
	/* /Users/rubber/linux/kernel/sched/core.c: 8863
	 * Temporarily drop rq->lock such that we can wake-up the stop task. /Users/rubber/linux/kernel/sched/core.c: 8864
	 * Both preemption and IRQs are still disabled. /Users/rubber/linux/kernel/sched/core.c: 8865
	/* /Users/rubber/linux/kernel/sched/core.c: 8870
	 * At this point need_resched() is true and we'll take the loop in /Users/rubber/linux/kernel/sched/core.c: 8871
	 * schedule(). The next pick is obviously going to be the stop task /Users/rubber/linux/kernel/sched/core.c: 8872
	 * which kthread_is_per_cpu() and will push this task away. /Users/rubber/linux/kernel/sched/core.c: 8873
 * Invoked from a CPUs hotplug control thread after the CPU has been marked /Users/rubber/linux/kernel/sched/core.c: 8894
 * inactive. All tasks which are not per CPU kernel threads are either /Users/rubber/linux/kernel/sched/core.c: 8895
 * pushed off this CPU now via balance_push() or placed on a different CPU /Users/rubber/linux/kernel/sched/core.c: 8896
 * during wakeup. Wait until the CPU is quiescent. /Users/rubber/linux/kernel/sched/core.c: 8897
 * used to mark begin/end of suspend/resume: /Users/rubber/linux/kernel/sched/core.c: 8955
 * Update cpusets according to cpu_active mask.  If cpusets are /Users/rubber/linux/kernel/sched/core.c: 8960
 * disabled, cpuset_update_active_cpus() becomes a simple wrapper /Users/rubber/linux/kernel/sched/core.c: 8961
 * around partition_sched_domains(). /Users/rubber/linux/kernel/sched/core.c: 8962
 * If we come here as part of a suspend/resume, don't touch cpusets because we /Users/rubber/linux/kernel/sched/core.c: 8964
 * want to restore it back to its original state upon resume anyway. /Users/rubber/linux/kernel/sched/core.c: 8965
		/* /Users/rubber/linux/kernel/sched/core.c: 8970
		 * num_cpus_frozen tracks how many CPUs are involved in suspend /Users/rubber/linux/kernel/sched/core.c: 8971
		 * resume sequence. As long as this is not the last online /Users/rubber/linux/kernel/sched/core.c: 8972
		 * operation in the resume sequence, just build a single sched /Users/rubber/linux/kernel/sched/core.c: 8973
		 * domain, ignoring cpusets. /Users/rubber/linux/kernel/sched/core.c: 8974
		/* /Users/rubber/linux/kernel/sched/core.c: 8979
		 * This is the last CPU online operation. So fall through and /Users/rubber/linux/kernel/sched/core.c: 8980
		 * restore the original sched domains by considering the /Users/rubber/linux/kernel/sched/core.c: 8981
		 * cpuset configurations. /Users/rubber/linux/kernel/sched/core.c: 8982
	/* /Users/rubber/linux/kernel/sched/core.c: 9007
	 * Clear the balance_push callback and prepare to schedule /Users/rubber/linux/kernel/sched/core.c: 9008
	 * regular tasks. /Users/rubber/linux/kernel/sched/core.c: 9009
	/* /Users/rubber/linux/kernel/sched/core.c: 9014
	 * When going up, increment the number of cores with SMT present. /Users/rubber/linux/kernel/sched/core.c: 9015
	/* /Users/rubber/linux/kernel/sched/core.c: 9027
	 * Put the rq online, if not already. This happens: /Users/rubber/linux/kernel/sched/core.c: 9028
	 * /Users/rubber/linux/kernel/sched/core.c: 9029
	 * 1) In the early boot process, because we build the real domains /Users/rubber/linux/kernel/sched/core.c: 9030
	 *    after all CPUs have been brought up. /Users/rubber/linux/kernel/sched/core.c: 9031
	 * /Users/rubber/linux/kernel/sched/core.c: 9032
	 * 2) At runtime, if cpuset_cpu_active() fails to rebuild the /Users/rubber/linux/kernel/sched/core.c: 9033
	 *    domains. /Users/rubber/linux/kernel/sched/core.c: 9034
	/* /Users/rubber/linux/kernel/sched/core.c: 9052
	 * Remove CPU from nohz.idle_cpus_mask to prevent participating in /Users/rubber/linux/kernel/sched/core.c: 9053
	 * load balancing when not active /Users/rubber/linux/kernel/sched/core.c: 9054
	/* /Users/rubber/linux/kernel/sched/core.c: 9060
	 * From this point forward, this CPU will refuse to run any task that /Users/rubber/linux/kernel/sched/core.c: 9061
	 * is not: migrate_disable() or KTHREAD_IS_PER_CPU, and will actively /Users/rubber/linux/kernel/sched/core.c: 9062
	 * push those tasks away until this gets cleared, see /Users/rubber/linux/kernel/sched/core.c: 9063
	 * sched_cpu_dying(). /Users/rubber/linux/kernel/sched/core.c: 9064
	/* /Users/rubber/linux/kernel/sched/core.c: 9068
	 * We've cleared cpu_active_mask / set balance_push, wait for all /Users/rubber/linux/kernel/sched/core.c: 9069
	 * preempt-disabled and RCU users of this state to go away such that /Users/rubber/linux/kernel/sched/core.c: 9070
	 * all new such users will observe it. /Users/rubber/linux/kernel/sched/core.c: 9071
	 * /Users/rubber/linux/kernel/sched/core.c: 9072
	 * Specifically, we rely on ttwu to no longer target this CPU, see /Users/rubber/linux/kernel/sched/core.c: 9073
	 * ttwu_queue_cond() and is_cpu_allowed(). /Users/rubber/linux/kernel/sched/core.c: 9074
	 * /Users/rubber/linux/kernel/sched/core.c: 9075
	 * Do sync before park smpboot threads to take care the rcu boost case. /Users/rubber/linux/kernel/sched/core.c: 9076
	/* /Users/rubber/linux/kernel/sched/core.c: 9089
	 * When going down, decrement the number of cores with SMT present. /Users/rubber/linux/kernel/sched/core.c: 9090
 * Invoked immediately before the stopper thread is invoked to bring the /Users/rubber/linux/kernel/sched/core.c: 9130
 * CPU down completely. At this point all per CPU kthreads except the /Users/rubber/linux/kernel/sched/core.c: 9131
 * hotplug thread (current) and the stopper thread (inactive) have been /Users/rubber/linux/kernel/sched/core.c: 9132
 * either parked or have been unbound from the outgoing CPU. Ensure that /Users/rubber/linux/kernel/sched/core.c: 9133
 * any of those which might be on the way out are gone. /Users/rubber/linux/kernel/sched/core.c: 9134
 * If after this point a bound task is being woken on this CPU then the /Users/rubber/linux/kernel/sched/core.c: 9136
 * responsible hotplug callback has failed to do it's job. /Users/rubber/linux/kernel/sched/core.c: 9137
 * sched_cpu_dying() will catch it with the appropriate fireworks. /Users/rubber/linux/kernel/sched/core.c: 9138
 * Since this CPU is going 'away' for a while, fold any nr_active delta we /Users/rubber/linux/kernel/sched/core.c: 9147
 * might have. Called from the CPU stopper task after ensuring that the /Users/rubber/linux/kernel/sched/core.c: 9148
 * stopper is the last running task on the CPU, so nr_active count is /Users/rubber/linux/kernel/sched/core.c: 9149
 * stable. We need to take the teardown thread which is calling this into /Users/rubber/linux/kernel/sched/core.c: 9150
 * account, so we hand in adjust = 1 to the load calculation. /Users/rubber/linux/kernel/sched/core.c: 9151
 * Also see the comment "Global load-average calculations". /Users/rubber/linux/kernel/sched/core.c: 9153
	/* /Users/rubber/linux/kernel/sched/core.c: 9209
	 * There's no userspace yet to cause hotplug operations; hence all the /Users/rubber/linux/kernel/sched/core.c: 9210
	 * CPU masks are stable and all blatant races in the below code cannot /Users/rubber/linux/kernel/sched/core.c: 9211
	 * happen. /Users/rubber/linux/kernel/sched/core.c: 9212
 * Default task group. /Users/rubber/linux/kernel/sched/core.c: 9253
 * Every task in system belongs to this group at bootup. /Users/rubber/linux/kernel/sched/core.c: 9254
		/* /Users/rubber/linux/kernel/sched/core.c: 9353
		 * How much CPU bandwidth does root_task_group get? /Users/rubber/linux/kernel/sched/core.c: 9354
		 * /Users/rubber/linux/kernel/sched/core.c: 9355
		 * In case of task-groups formed thr' the cgroup filesystem, it /Users/rubber/linux/kernel/sched/core.c: 9356
		 * gets 100% of the CPU resources in the system. This overall /Users/rubber/linux/kernel/sched/core.c: 9357
		 * system CPU resource is divided among the tasks of /Users/rubber/linux/kernel/sched/core.c: 9358
		 * root_task_group and its child task-groups in a fair manner, /Users/rubber/linux/kernel/sched/core.c: 9359
		 * based on each entity's (task or task-group's) weight /Users/rubber/linux/kernel/sched/core.c: 9360
		 * (se->load.weight). /Users/rubber/linux/kernel/sched/core.c: 9361
		 * /Users/rubber/linux/kernel/sched/core.c: 9362
		 * In other words, if root_task_group has 10 tasks of weight /Users/rubber/linux/kernel/sched/core.c: 9363
		 * 1024) and two child groups A0 and A1 (of weight 1024 each), /Users/rubber/linux/kernel/sched/core.c: 9364
		 * then A0's share of the CPU resource is: /Users/rubber/linux/kernel/sched/core.c: 9365
		 * /Users/rubber/linux/kernel/sched/core.c: 9366
		 *	A0's bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33% /Users/rubber/linux/kernel/sched/core.c: 9367
		 * /Users/rubber/linux/kernel/sched/core.c: 9368
		 * We achieve this by letting root_task_group's tasks sit /Users/rubber/linux/kernel/sched/core.c: 9369
		 * directly in rq->cfs (i.e root_task_group->se[] = NULL). /Users/rubber/linux/kernel/sched/core.c: 9370
	/* /Users/rubber/linux/kernel/sched/core.c: 9424
	 * The boot idle thread does lazy MMU switching as well: /Users/rubber/linux/kernel/sched/core.c: 9425
	/* /Users/rubber/linux/kernel/sched/core.c: 9430
	 * Make us the idle thread. Technically, schedule() should not be /Users/rubber/linux/kernel/sched/core.c: 9431
	 * called from this thread, however somewhere below it might be, /Users/rubber/linux/kernel/sched/core.c: 9432
	 * but because we are the idle thread, we just pick up running again /Users/rubber/linux/kernel/sched/core.c: 9433
	 * when this runqueue becomes "idle". /Users/rubber/linux/kernel/sched/core.c: 9434
	/* /Users/rubber/linux/kernel/sched/core.c: 9460
	 * Blocking primitives will set (and therefore destroy) current->state, /Users/rubber/linux/kernel/sched/core.c: 9461
	 * since we will exit with TASK_RUNNING make sure we enter with it, /Users/rubber/linux/kernel/sched/core.c: 9462
	 * otherwise we will destroy state. /Users/rubber/linux/kernel/sched/core.c: 9463
		/* /Users/rubber/linux/kernel/sched/core.c: 9619
		 * Only normalize user tasks: /Users/rubber/linux/kernel/sched/core.c: 9620
			/* /Users/rubber/linux/kernel/sched/core.c: 9631
			 * Renice negative nice level userspace /Users/rubber/linux/kernel/sched/core.c: 9632
			 * tasks back to 0: /Users/rubber/linux/kernel/sched/core.c: 9633
 * These functions are only useful for the IA64 MCA handling, or kdb. /Users/rubber/linux/kernel/sched/core.c: 9649
 * They can only be called when the whole system has been /Users/rubber/linux/kernel/sched/core.c: 9651
 * stopped - every CPU needs to be quiescent, and no scheduling /Users/rubber/linux/kernel/sched/core.c: 9652
 * activity can take place. Using them for anything else would /Users/rubber/linux/kernel/sched/core.c: 9653
 * be a serious bug, and as a result, they aren't even visible /Users/rubber/linux/kernel/sched/core.c: 9654
 * under any other configuration. /Users/rubber/linux/kernel/sched/core.c: 9655
 * curr_task - return the current task for a given CPU. /Users/rubber/linux/kernel/sched/core.c: 9659
 * @cpu: the processor in question. /Users/rubber/linux/kernel/sched/core.c: 9660
 * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! /Users/rubber/linux/kernel/sched/core.c: 9662
 * Return: The current task for @cpu. /Users/rubber/linux/kernel/sched/core.c: 9664
 * ia64_set_curr_task - set the current task for a given CPU. /Users/rubber/linux/kernel/sched/core.c: 9675
 * @cpu: the processor in question. /Users/rubber/linux/kernel/sched/core.c: 9676
 * @p: the task pointer to set. /Users/rubber/linux/kernel/sched/core.c: 9677
 * Description: This function must only be used when non-maskable interrupts /Users/rubber/linux/kernel/sched/core.c: 9679
 * are serviced on a separate stack. It allows the architecture to switch the /Users/rubber/linux/kernel/sched/core.c: 9680
 * notion of the current task on a CPU in a non-blocking manner. This function /Users/rubber/linux/kernel/sched/core.c: 9681
 * must be called with all CPU's synchronized, and interrupts disabled, the /Users/rubber/linux/kernel/sched/core.c: 9682
 * and caller must save the original value of the current task (see /Users/rubber/linux/kernel/sched/core.c: 9683
 * curr_task() above) and restore that value before reenabling interrupts and /Users/rubber/linux/kernel/sched/core.c: 9684
 * re-starting the system. /Users/rubber/linux/kernel/sched/core.c: 9685
 * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! /Users/rubber/linux/kernel/sched/core.c: 9687
	/* /Users/rubber/linux/kernel/sched/core.c: 9731
	 * We have to wait for yet another RCU grace period to expire, as /Users/rubber/linux/kernel/sched/core.c: 9732
	 * print_cfs_stats() might run concurrently. /Users/rubber/linux/kernel/sched/core.c: 9733
	/* /Users/rubber/linux/kernel/sched/core.c: 9797
	 * Unlink first, to avoid walk_tg_tree_from() from finding us (via /Users/rubber/linux/kernel/sched/core.c: 9798
	 * sched_cfs_period_timer()). /Users/rubber/linux/kernel/sched/core.c: 9799
	 * /Users/rubber/linux/kernel/sched/core.c: 9800
	 * For this to be effective, we have to wait for all pending users of /Users/rubber/linux/kernel/sched/core.c: 9801
	 * this task group to leave their RCU critical section to ensure no new /Users/rubber/linux/kernel/sched/core.c: 9802
	 * user will see our dying task group any more. Specifically ensure /Users/rubber/linux/kernel/sched/core.c: 9803
	 * that tg_unthrottle_up() won't add decayed cfs_rq's to it. /Users/rubber/linux/kernel/sched/core.c: 9804
	 * /Users/rubber/linux/kernel/sched/core.c: 9805
	 * We therefore defer calling unregister_fair_sched_group() to /Users/rubber/linux/kernel/sched/core.c: 9806
	 * sched_unregister_group() which is guarantied to get called only after the /Users/rubber/linux/kernel/sched/core.c: 9807
	 * current RCU grace period has expired. /Users/rubber/linux/kernel/sched/core.c: 9808
	/* /Users/rubber/linux/kernel/sched/core.c: 9820
	 * All callers are synchronized by task_rq_lock(); we do not use RCU /Users/rubber/linux/kernel/sched/core.c: 9821
	 * which is pointless here. Thus, we pass "true" to task_css_check() /Users/rubber/linux/kernel/sched/core.c: 9822
	 * to prevent lockdep warnings. /Users/rubber/linux/kernel/sched/core.c: 9823
 * Change task's runqueue when it moves between groups. /Users/rubber/linux/kernel/sched/core.c: 9839
 * The caller of this function should have put the task in its new group by /Users/rubber/linux/kernel/sched/core.c: 9841
 * now. This function just updates tsk->se.cfs_rq and tsk->se.parent to reflect /Users/rubber/linux/kernel/sched/core.c: 9842
 * its new group. /Users/rubber/linux/kernel/sched/core.c: 9843
		/* /Users/rubber/linux/kernel/sched/core.c: 9869
		 * After changing group, the running task may have joined a /Users/rubber/linux/kernel/sched/core.c: 9870
		 * throttled one but it's still the running task. Trigger a /Users/rubber/linux/kernel/sched/core.c: 9871
		 * resched to make sure that task can still run. /Users/rubber/linux/kernel/sched/core.c: 9872
	/* /Users/rubber/linux/kernel/sched/core.c: 9935
	 * Relies on the RCU grace period between css_released() and this. /Users/rubber/linux/kernel/sched/core.c: 9936
 * This is called before wake_up_new_task(), therefore we really only /Users/rubber/linux/kernel/sched/core.c: 9942
 * have to set its group bits, all the other stuff does not apply. /Users/rubber/linux/kernel/sched/core.c: 9943
		/* /Users/rubber/linux/kernel/sched/core.c: 9969
		 * Serialize against wake_up_new_task() such that if it's /Users/rubber/linux/kernel/sched/core.c: 9970
		 * running, we're sure to observe its full state. /Users/rubber/linux/kernel/sched/core.c: 9971
		/* /Users/rubber/linux/kernel/sched/core.c: 9974
		 * Avoid calling sched_move_task() before wake_up_new_task() /Users/rubber/linux/kernel/sched/core.c: 9975
		 * has happened. This would lead to problems with PELT, due to /Users/rubber/linux/kernel/sched/core.c: 9976
		 * move wanting to detach+attach while we're not attached yet. /Users/rubber/linux/kernel/sched/core.c: 9977
 * Integer 10^N with a given N exponent by casting to integer the literal "1eN" /Users/rubber/linux/kernel/sched/core.c: 10048
 * C expression. Since there is no way to convert a macro argument (N) into a /Users/rubber/linux/kernel/sched/core.c: 10049
 * character constant, use two levels of macros. /Users/rubber/linux/kernel/sched/core.c: 10050
	/* /Users/rubber/linux/kernel/sched/core.c: 10110
	 * Because of not recoverable conversion rounding we keep track of the /Users/rubber/linux/kernel/sched/core.c: 10111
	 * exact requested value /Users/rubber/linux/kernel/sched/core.c: 10112
	/* /Users/rubber/linux/kernel/sched/core.c: 10211
	 * Ensure we have at some amount of bandwidth every period.  This is /Users/rubber/linux/kernel/sched/core.c: 10212
	 * to prevent reaching a state of large arrears when throttled via /Users/rubber/linux/kernel/sched/core.c: 10213
	 * entity_tick() resulting in prolonged exit starvation. /Users/rubber/linux/kernel/sched/core.c: 10214
	/* /Users/rubber/linux/kernel/sched/core.c: 10219
	 * Likewise, bound things on the other side by preventing insane quota /Users/rubber/linux/kernel/sched/core.c: 10220
	 * periods.  This also allows us to normalize in computing quota /Users/rubber/linux/kernel/sched/core.c: 10221
	 * feasibility. /Users/rubber/linux/kernel/sched/core.c: 10222
	/* /Users/rubber/linux/kernel/sched/core.c: 10227
	 * Bound quota to defend quota against overflow during bandwidth shift. /Users/rubber/linux/kernel/sched/core.c: 10228
	/* /Users/rubber/linux/kernel/sched/core.c: 10237
	 * Prevent race between setting of cfs_rq->runtime_enabled and /Users/rubber/linux/kernel/sched/core.c: 10238
	 * unthrottle_offline_cfs_rqs(). /Users/rubber/linux/kernel/sched/core.c: 10239
	/* /Users/rubber/linux/kernel/sched/core.c: 10249
	 * If we need to toggle cfs_bandwidth_used, off->on must occur /Users/rubber/linux/kernel/sched/core.c: 10250
	 * before making related changes, and on->off must occur afterwards /Users/rubber/linux/kernel/sched/core.c: 10251
 * normalize group quota/period to be quota/max_period /Users/rubber/linux/kernel/sched/core.c: 10409
 * note: units are usecs /Users/rubber/linux/kernel/sched/core.c: 10410
		/* /Users/rubber/linux/kernel/sched/core.c: 10446
		 * Ensure max(child_quota) <= parent_quota.  On cgroup2, /Users/rubber/linux/kernel/sched/core.c: 10447
		 * always take the min.  On cgroup1, only inherit when no /Users/rubber/linux/kernel/sched/core.c: 10448
		 * limit is set: /Users/rubber/linux/kernel/sched/core.c: 10449
	/* /Users/rubber/linux/kernel/sched/core.c: 10658
	 * cgroup weight knobs should use the common MIN, DFL and MAX /Users/rubber/linux/kernel/sched/core.c: 10659
	 * values which are 1, 100 and 10000 respectively.  While it loses /Users/rubber/linux/kernel/sched/core.c: 10660
	 * a bit of range on both ends, it maps pretty well onto the shares /Users/rubber/linux/kernel/sched/core.c: 10661
	 * value used by scheduler and the round-trip conversions preserve /Users/rubber/linux/kernel/sched/core.c: 10662
	 * the original value over the entire range. /Users/rubber/linux/kernel/sched/core.c: 10663
 * Nice levels are multiplicative, with a gentle 10% change for every /Users/rubber/linux/kernel/sched/core.c: 10841
 * nice level changed. I.e. when a CPU-bound task goes from nice 0 to /Users/rubber/linux/kernel/sched/core.c: 10842
 * nice 1, it will get ~10% less CPU time than another CPU-bound task /Users/rubber/linux/kernel/sched/core.c: 10843
 * that remained on nice 0. /Users/rubber/linux/kernel/sched/core.c: 10844
 * The "10% effect" is relative and cumulative: from _any_ nice level, /Users/rubber/linux/kernel/sched/core.c: 10846
 * if you go up 1 level, it's -10% CPU usage, if you go down 1 level /Users/rubber/linux/kernel/sched/core.c: 10847
 * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25. /Users/rubber/linux/kernel/sched/core.c: 10848
 * If a task goes up by ~10% and another task goes down by ~10% then /Users/rubber/linux/kernel/sched/core.c: 10849
 * the relative distance between them is ~25%.) /Users/rubber/linux/kernel/sched/core.c: 10850
 -20 */     88761,     71755,     56483,     46273,     36291, /Users/rubber/linux/kernel/sched/core.c: 10853
 -15 */     29154,     23254,     18705,     14949,     11916, /Users/rubber/linux/kernel/sched/core.c: 10854
 -10 */      9548,      7620,      6100,      4904,      3906, /Users/rubber/linux/kernel/sched/core.c: 10855
  -5 */      3121,      2501,      1991,      1586,      1277, /Users/rubber/linux/kernel/sched/core.c: 10856
   0 */      1024,       820,       655,       526,       423, /Users/rubber/linux/kernel/sched/core.c: 10857
   5 */       335,       272,       215,       172,       137, /Users/rubber/linux/kernel/sched/core.c: 10858
  10 */       110,        87,        70,        56,        45, /Users/rubber/linux/kernel/sched/core.c: 10859
  15 */        36,        29,        23,        18,        15, /Users/rubber/linux/kernel/sched/core.c: 10860
 * Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated. /Users/rubber/linux/kernel/sched/core.c: 10864
 * In cases where the weight does not change often, we can use the /Users/rubber/linux/kernel/sched/core.c: 10866
 * precalculated inverse to speed up arithmetics by turning divisions /Users/rubber/linux/kernel/sched/core.c: 10867
 * into multiplications: /Users/rubber/linux/kernel/sched/core.c: 10868
 -20 */     48388,     59856,     76040,     92818,    118348, /Users/rubber/linux/kernel/sched/core.c: 10871
 -15 */    147320,    184698,    229616,    287308,    360437, /Users/rubber/linux/kernel/sched/core.c: 10872
 -10 */    449829,    563644,    704093,    875809,   1099582, /Users/rubber/linux/kernel/sched/core.c: 10873
  -5 */   1376151,   1717300,   2157191,   2708050,   3363326, /Users/rubber/linux/kernel/sched/core.c: 10874
   0 */   4194304,   5237765,   6557202,   8165337,  10153587, /Users/rubber/linux/kernel/sched/core.c: 10875
   5 */  12820798,  15790321,  19976592,  24970740,  31350126, /Users/rubber/linux/kernel/sched/core.c: 10876
  10 */  39045157,  49367440,  61356676,  76695844,  95443717, /Users/rubber/linux/kernel/sched/core.c: 10877
  15 */ 119304647, 148102320, 186737708, 238609294, 286331153, /Users/rubber/linux/kernel/sched/core.c: 10878
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/debug.c: 1
 * kernel/sched/debug.c /Users/rubber/linux/kernel/sched/debug.c: 3
 * Print the CFS rbtree and other debugging details /Users/rubber/linux/kernel/sched/debug.c: 5
 * Copyright(C) 2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/sched/debug.c: 7
 * This allows printing both to /proc/sched_debug and /Users/rubber/linux/kernel/sched/debug.c: 12
 * to the console /Users/rubber/linux/kernel/sched/debug.c: 13
 * Ease the printing of nsec fields: /Users/rubber/linux/kernel/sched/debug.c: 24
	/* /Users/rubber/linux/kernel/sched/debug.c: 398
	 * This can unfortunately be invoked before sched_debug_init() creates /Users/rubber/linux/kernel/sched/debug.c: 399
	 * the debug directory. Don't touch sd_sysctl_cpus until then. /Users/rubber/linux/kernel/sched/debug.c: 400
 * Only 1 SEQ_printf_task_group_path() caller can use the full length /Users/rubber/linux/kernel/sched/debug.c: 508
 * group_path[] for cgroup path. Other simultaneous callers will have /Users/rubber/linux/kernel/sched/debug.c: 509
 * to use a shorter stack buffer. A "..." suffix is appended at the end /Users/rubber/linux/kernel/sched/debug.c: 510
 * of the stack buffer so that it will show up in case the output length /Users/rubber/linux/kernel/sched/debug.c: 511
 * matches the given buffer size to indicate possible path name truncation. /Users/rubber/linux/kernel/sched/debug.c: 512
		/* /Users/rubber/linux/kernel/sched/debug.c: 854
		 * Need to reset softlockup watchdogs on all CPUs, because /Users/rubber/linux/kernel/sched/debug.c: 855
		 * another CPU might be blocked waiting for us to process /Users/rubber/linux/kernel/sched/debug.c: 856
		 * an IPI or stop_machine. /Users/rubber/linux/kernel/sched/debug.c: 857
 * This iterator needs some explanation. /Users/rubber/linux/kernel/sched/debug.c: 866
 * It returns 1 for the header position. /Users/rubber/linux/kernel/sched/debug.c: 867
 * This means 2 is CPU 0. /Users/rubber/linux/kernel/sched/debug.c: 868
 * In a hotplugged system some CPUs, including CPU 0, may be missing so we have /Users/rubber/linux/kernel/sched/debug.c: 869
 * to use cpumask_* to iterate over the CPUs. /Users/rubber/linux/kernel/sched/debug.c: 870
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/completion.c: 1
 * Generic wait-for-completion handler; /Users/rubber/linux/kernel/sched/completion.c: 3
 * It differs from semaphores in that their default case is the opposite, /Users/rubber/linux/kernel/sched/completion.c: 5
 * wait_for_completion default blocks whereas semaphore default non-block. The /Users/rubber/linux/kernel/sched/completion.c: 6
 * interface also makes it easy to 'complete' multiple waiting threads, /Users/rubber/linux/kernel/sched/completion.c: 7
 * something which isn't entirely natural for semaphores. /Users/rubber/linux/kernel/sched/completion.c: 8
 * But more importantly, the primitive documents the usage. Semaphores would /Users/rubber/linux/kernel/sched/completion.c: 10
 * typically be used for exclusion which gives rise to priority inversion. /Users/rubber/linux/kernel/sched/completion.c: 11
 * Waiting for completion is a typically sync point, but not an exclusion point. /Users/rubber/linux/kernel/sched/completion.c: 12
 * complete: - signals a single thread waiting on this completion /Users/rubber/linux/kernel/sched/completion.c: 17
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 18
 * This will wake up a single thread waiting on this completion. Threads will be /Users/rubber/linux/kernel/sched/completion.c: 20
 * awakened in the same order in which they were queued. /Users/rubber/linux/kernel/sched/completion.c: 21
 * See also complete_all(), wait_for_completion() and related routines. /Users/rubber/linux/kernel/sched/completion.c: 23
 * If this function wakes up a task, it executes a full memory barrier before /Users/rubber/linux/kernel/sched/completion.c: 25
 * accessing the task state. /Users/rubber/linux/kernel/sched/completion.c: 26
 * complete_all: - signals all threads waiting on this completion /Users/rubber/linux/kernel/sched/completion.c: 42
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 43
 * This will wake up all threads waiting on this particular completion event. /Users/rubber/linux/kernel/sched/completion.c: 45
 * If this function wakes up a task, it executes a full memory barrier before /Users/rubber/linux/kernel/sched/completion.c: 47
 * accessing the task state. /Users/rubber/linux/kernel/sched/completion.c: 48
 * Since complete_all() sets the completion of @x permanently to done /Users/rubber/linux/kernel/sched/completion.c: 50
 * to allow multiple waiters to finish, a call to reinit_completion() /Users/rubber/linux/kernel/sched/completion.c: 51
 * must be used on @x if @x is to be used again. The code must make /Users/rubber/linux/kernel/sched/completion.c: 52
 * sure that all waiters have woken and finished before reinitializing /Users/rubber/linux/kernel/sched/completion.c: 53
 * @x. Also note that the function completion_done() can not be used /Users/rubber/linux/kernel/sched/completion.c: 54
 * to know if there are still waiters after complete_all() has been called. /Users/rubber/linux/kernel/sched/completion.c: 55
 * wait_for_completion: - waits for completion of a task /Users/rubber/linux/kernel/sched/completion.c: 127
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 128
 * This waits to be signaled for completion of a specific task. It is NOT /Users/rubber/linux/kernel/sched/completion.c: 130
 * interruptible and there is no timeout. /Users/rubber/linux/kernel/sched/completion.c: 131
 * See also similar routines (i.e. wait_for_completion_timeout()) with timeout /Users/rubber/linux/kernel/sched/completion.c: 133
 * and interrupt capability. Also see complete(). /Users/rubber/linux/kernel/sched/completion.c: 134
 * wait_for_completion_timeout: - waits for completion of a task (w/timeout) /Users/rubber/linux/kernel/sched/completion.c: 143
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 144
 * @timeout:  timeout value in jiffies /Users/rubber/linux/kernel/sched/completion.c: 145
 * This waits for either a completion of a specific task to be signaled or for a /Users/rubber/linux/kernel/sched/completion.c: 147
 * specified timeout to expire. The timeout is in jiffies. It is not /Users/rubber/linux/kernel/sched/completion.c: 148
 * interruptible. /Users/rubber/linux/kernel/sched/completion.c: 149
 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left /Users/rubber/linux/kernel/sched/completion.c: 151
 * till timeout) if completed. /Users/rubber/linux/kernel/sched/completion.c: 152
 * wait_for_completion_io: - waits for completion of a task /Users/rubber/linux/kernel/sched/completion.c: 162
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 163
 * This waits to be signaled for completion of a specific task. It is NOT /Users/rubber/linux/kernel/sched/completion.c: 165
 * interruptible and there is no timeout. The caller is accounted as waiting /Users/rubber/linux/kernel/sched/completion.c: 166
 * for IO (which traditionally means blkio only). /Users/rubber/linux/kernel/sched/completion.c: 167
 * wait_for_completion_io_timeout: - waits for completion of a task (w/timeout) /Users/rubber/linux/kernel/sched/completion.c: 176
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 177
 * @timeout:  timeout value in jiffies /Users/rubber/linux/kernel/sched/completion.c: 178
 * This waits for either a completion of a specific task to be signaled or for a /Users/rubber/linux/kernel/sched/completion.c: 180
 * specified timeout to expire. The timeout is in jiffies. It is not /Users/rubber/linux/kernel/sched/completion.c: 181
 * interruptible. The caller is accounted as waiting for IO (which traditionally /Users/rubber/linux/kernel/sched/completion.c: 182
 * means blkio only). /Users/rubber/linux/kernel/sched/completion.c: 183
 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left /Users/rubber/linux/kernel/sched/completion.c: 185
 * till timeout) if completed. /Users/rubber/linux/kernel/sched/completion.c: 186
 * wait_for_completion_interruptible: - waits for completion of a task (w/intr) /Users/rubber/linux/kernel/sched/completion.c: 196
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 197
 * This waits for completion of a specific task to be signaled. It is /Users/rubber/linux/kernel/sched/completion.c: 199
 * interruptible. /Users/rubber/linux/kernel/sched/completion.c: 200
 * Return: -ERESTARTSYS if interrupted, 0 if completed. /Users/rubber/linux/kernel/sched/completion.c: 202
 * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr)) /Users/rubber/linux/kernel/sched/completion.c: 214
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 215
 * @timeout:  timeout value in jiffies /Users/rubber/linux/kernel/sched/completion.c: 216
 * This waits for either a completion of a specific task to be signaled or for a /Users/rubber/linux/kernel/sched/completion.c: 218
 * specified timeout to expire. It is interruptible. The timeout is in jiffies. /Users/rubber/linux/kernel/sched/completion.c: 219
 * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1, /Users/rubber/linux/kernel/sched/completion.c: 221
 * or number of jiffies left till timeout) if completed. /Users/rubber/linux/kernel/sched/completion.c: 222
 * wait_for_completion_killable: - waits for completion of a task (killable) /Users/rubber/linux/kernel/sched/completion.c: 233
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 234
 * This waits to be signaled for completion of a specific task. It can be /Users/rubber/linux/kernel/sched/completion.c: 236
 * interrupted by a kill signal. /Users/rubber/linux/kernel/sched/completion.c: 237
 * Return: -ERESTARTSYS if interrupted, 0 if completed. /Users/rubber/linux/kernel/sched/completion.c: 239
 * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable)) /Users/rubber/linux/kernel/sched/completion.c: 251
 * @x:  holds the state of this particular completion /Users/rubber/linux/kernel/sched/completion.c: 252
 * @timeout:  timeout value in jiffies /Users/rubber/linux/kernel/sched/completion.c: 253
 * This waits for either a completion of a specific task to be /Users/rubber/linux/kernel/sched/completion.c: 255
 * signaled or for a specified timeout to expire. It can be /Users/rubber/linux/kernel/sched/completion.c: 256
 * interrupted by a kill signal. The timeout is in jiffies. /Users/rubber/linux/kernel/sched/completion.c: 257
 * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1, /Users/rubber/linux/kernel/sched/completion.c: 259
 * or number of jiffies left till timeout) if completed. /Users/rubber/linux/kernel/sched/completion.c: 260
 *	try_wait_for_completion - try to decrement a completion without blocking /Users/rubber/linux/kernel/sched/completion.c: 271
 *	@x:	completion structure /Users/rubber/linux/kernel/sched/completion.c: 272
 *	Return: 0 if a decrement cannot be done without blocking /Users/rubber/linux/kernel/sched/completion.c: 274
 *		 1 if a decrement succeeded. /Users/rubber/linux/kernel/sched/completion.c: 275
 *	If a completion is being used as a counting completion, /Users/rubber/linux/kernel/sched/completion.c: 277
 *	attempt to decrement the counter without blocking. This /Users/rubber/linux/kernel/sched/completion.c: 278
 *	enables us to avoid waiting if the resource the completion /Users/rubber/linux/kernel/sched/completion.c: 279
 *	is protecting is not available. /Users/rubber/linux/kernel/sched/completion.c: 280
	/* /Users/rubber/linux/kernel/sched/completion.c: 287
	 * Since x->done will need to be locked only /Users/rubber/linux/kernel/sched/completion.c: 288
	 * in the non-blocking case, we check x->done /Users/rubber/linux/kernel/sched/completion.c: 289
	 * first without taking the lock so we can /Users/rubber/linux/kernel/sched/completion.c: 290
	 * return early in the blocking case. /Users/rubber/linux/kernel/sched/completion.c: 291
 *	completion_done - Test to see if a completion has any waiters /Users/rubber/linux/kernel/sched/completion.c: 307
 *	@x:	completion structure /Users/rubber/linux/kernel/sched/completion.c: 308
 *	Return: 0 if there are waiters (wait_for_completion() in progress) /Users/rubber/linux/kernel/sched/completion.c: 310
 *		 1 if there are no waiters. /Users/rubber/linux/kernel/sched/completion.c: 311
 *	Note, this will always return true if complete_all() was called on @X. /Users/rubber/linux/kernel/sched/completion.c: 313
	/* /Users/rubber/linux/kernel/sched/completion.c: 322
	 * If ->done, we need to wait for complete() to release ->wait.lock /Users/rubber/linux/kernel/sched/completion.c: 323
	 * otherwise we can end up freeing the completion before complete() /Users/rubber/linux/kernel/sched/completion.c: 324
	 * is done referencing it. /Users/rubber/linux/kernel/sched/completion.c: 325
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/cpufreq.c: 1
 * Scheduler code and data structures related to cpufreq. /Users/rubber/linux/kernel/sched/cpufreq.c: 3
 * Copyright (C) 2016, Intel Corporation /Users/rubber/linux/kernel/sched/cpufreq.c: 5
 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com> /Users/rubber/linux/kernel/sched/cpufreq.c: 6
 * cpufreq_add_update_util_hook - Populate the CPU's update_util_data pointer. /Users/rubber/linux/kernel/sched/cpufreq.c: 15
 * @cpu: The CPU to set the pointer for. /Users/rubber/linux/kernel/sched/cpufreq.c: 16
 * @data: New pointer value. /Users/rubber/linux/kernel/sched/cpufreq.c: 17
 * @func: Callback function to set for the CPU. /Users/rubber/linux/kernel/sched/cpufreq.c: 18
 * Set and publish the update_util_data pointer for the given CPU. /Users/rubber/linux/kernel/sched/cpufreq.c: 20
 * The update_util_data pointer of @cpu is set to @data and the callback /Users/rubber/linux/kernel/sched/cpufreq.c: 22
 * function pointer in the target struct update_util_data is set to @func. /Users/rubber/linux/kernel/sched/cpufreq.c: 23
 * That function will be called by cpufreq_update_util() from RCU-sched /Users/rubber/linux/kernel/sched/cpufreq.c: 24
 * read-side critical sections, so it must not sleep.  @data will always be /Users/rubber/linux/kernel/sched/cpufreq.c: 25
 * passed to it as the first argument which allows the function to get to the /Users/rubber/linux/kernel/sched/cpufreq.c: 26
 * target update_util_data structure and its container. /Users/rubber/linux/kernel/sched/cpufreq.c: 27
 * The update_util_data pointer of @cpu must be NULL when this function is /Users/rubber/linux/kernel/sched/cpufreq.c: 29
 * called or it will WARN() and return with no effect. /Users/rubber/linux/kernel/sched/cpufreq.c: 30
 * cpufreq_remove_update_util_hook - Clear the CPU's update_util_data pointer. /Users/rubber/linux/kernel/sched/cpufreq.c: 48
 * @cpu: The CPU to clear the pointer for. /Users/rubber/linux/kernel/sched/cpufreq.c: 49
 * Clear the update_util_data pointer for the given CPU. /Users/rubber/linux/kernel/sched/cpufreq.c: 51
 * Callers must use RCU callbacks to free any memory that might be /Users/rubber/linux/kernel/sched/cpufreq.c: 53
 * accessed via the old update_util_data pointer or invoke synchronize_rcu() /Users/rubber/linux/kernel/sched/cpufreq.c: 54
 * right after this function to avoid use-after-free. /Users/rubber/linux/kernel/sched/cpufreq.c: 55
 * cpufreq_this_cpu_can_update - Check if cpufreq policy can be updated. /Users/rubber/linux/kernel/sched/cpufreq.c: 64
 * @policy: cpufreq policy to check. /Users/rubber/linux/kernel/sched/cpufreq.c: 65
 * Return 'true' if: /Users/rubber/linux/kernel/sched/cpufreq.c: 67
 * - the local and remote CPUs share @policy, /Users/rubber/linux/kernel/sched/cpufreq.c: 68
 * - dvfs_possible_from_any_cpu is set in @policy and the local CPU is not going /Users/rubber/linux/kernel/sched/cpufreq.c: 69
 *   offline (in which case it is not expected to run cpufreq updates any more). /Users/rubber/linux/kernel/sched/cpufreq.c: 70
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/pelt.c: 1
 * Per Entity Load Tracking /Users/rubber/linux/kernel/sched/pelt.c: 3
 *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/sched/pelt.c: 5
 *  Interactivity improvements by Mike Galbraith /Users/rubber/linux/kernel/sched/pelt.c: 7
 *  (C) 2007 Mike Galbraith <efault@gmx.de> /Users/rubber/linux/kernel/sched/pelt.c: 8
 *  Various enhancements by Dmitry Adamushko. /Users/rubber/linux/kernel/sched/pelt.c: 10
 *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com> /Users/rubber/linux/kernel/sched/pelt.c: 11
 *  Group scheduling enhancements by Srivatsa Vaddagiri /Users/rubber/linux/kernel/sched/pelt.c: 13
 *  Copyright IBM Corporation, 2007 /Users/rubber/linux/kernel/sched/pelt.c: 14
 *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com> /Users/rubber/linux/kernel/sched/pelt.c: 15
 *  Scaled math optimizations by Thomas Gleixner /Users/rubber/linux/kernel/sched/pelt.c: 17
 *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/sched/pelt.c: 18
 *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra /Users/rubber/linux/kernel/sched/pelt.c: 20
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/sched/pelt.c: 21
 *  Move PELT related code from fair.c into this pelt.c file /Users/rubber/linux/kernel/sched/pelt.c: 23
 *  Author: Vincent Guittot <vincent.guittot@linaro.org> /Users/rubber/linux/kernel/sched/pelt.c: 24
 * Approximate: /Users/rubber/linux/kernel/sched/pelt.c: 32
 *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period) /Users/rubber/linux/kernel/sched/pelt.c: 33
	/* /Users/rubber/linux/kernel/sched/pelt.c: 45
	 * As y^PERIOD = 1/2, we can combine /Users/rubber/linux/kernel/sched/pelt.c: 46
	 *    y^n = 1/2^(n/PERIOD) * y^(n%PERIOD) /Users/rubber/linux/kernel/sched/pelt.c: 47
	 * With a look-up table which covers y^n (n<PERIOD) /Users/rubber/linux/kernel/sched/pelt.c: 48
	 * /Users/rubber/linux/kernel/sched/pelt.c: 49
	 * To achieve constant time decay_load. /Users/rubber/linux/kernel/sched/pelt.c: 50
	/* /Users/rubber/linux/kernel/sched/pelt.c: 65
	 * c1 = d1 y^p /Users/rubber/linux/kernel/sched/pelt.c: 66
	/* /Users/rubber/linux/kernel/sched/pelt.c: 70
	 *            p-1 /Users/rubber/linux/kernel/sched/pelt.c: 71
	 * c2 = 1024 \Sum y^n /Users/rubber/linux/kernel/sched/pelt.c: 72
	 *            n=1 /Users/rubber/linux/kernel/sched/pelt.c: 73
	 * /Users/rubber/linux/kernel/sched/pelt.c: 74
	 *              inf        inf /Users/rubber/linux/kernel/sched/pelt.c: 75
	 *    = 1024 ( \Sum y^n - \Sum y^n - y^0 ) /Users/rubber/linux/kernel/sched/pelt.c: 76
	 *              n=0        n=p /Users/rubber/linux/kernel/sched/pelt.c: 77
 * Accumulate the three separate parts of the sum; d1 the remainder /Users/rubber/linux/kernel/sched/pelt.c: 85
 * of the last (incomplete) period, d2 the span of full periods and d3 /Users/rubber/linux/kernel/sched/pelt.c: 86
 * the remainder of the (incomplete) current period. /Users/rubber/linux/kernel/sched/pelt.c: 87
 *           d1          d2           d3 /Users/rubber/linux/kernel/sched/pelt.c: 89
 *           ^           ^            ^ /Users/rubber/linux/kernel/sched/pelt.c: 90
 *           |           |            | /Users/rubber/linux/kernel/sched/pelt.c: 91
 *         |<->|<----------------->|<--->| /Users/rubber/linux/kernel/sched/pelt.c: 92
 * ... |---x---|------| ... |------|-----x (now) /Users/rubber/linux/kernel/sched/pelt.c: 93
 *                           p-1 /Users/rubber/linux/kernel/sched/pelt.c: 95
 * u' = (u + d1) y^p + 1024 \Sum y^n + d3 y^0 /Users/rubber/linux/kernel/sched/pelt.c: 96
 *                           n=1 /Users/rubber/linux/kernel/sched/pelt.c: 97
 *    = u y^p +					(Step 1) /Users/rubber/linux/kernel/sched/pelt.c: 99
 *                     p-1 /Users/rubber/linux/kernel/sched/pelt.c: 101
 *      d1 y^p + 1024 \Sum y^n + d3 y^0		(Step 2) /Users/rubber/linux/kernel/sched/pelt.c: 102
 *                     n=1 /Users/rubber/linux/kernel/sched/pelt.c: 103
	/* /Users/rubber/linux/kernel/sched/pelt.c: 115
	 * Step 1: decay old *_sum if we crossed period boundaries. /Users/rubber/linux/kernel/sched/pelt.c: 116
		/* /Users/rubber/linux/kernel/sched/pelt.c: 124
		 * Step 2 /Users/rubber/linux/kernel/sched/pelt.c: 125
			/* /Users/rubber/linux/kernel/sched/pelt.c: 129
			 * This relies on the: /Users/rubber/linux/kernel/sched/pelt.c: 130
			 * /Users/rubber/linux/kernel/sched/pelt.c: 131
			 * if (!load) /Users/rubber/linux/kernel/sched/pelt.c: 132
			 *	runnable = running = 0; /Users/rubber/linux/kernel/sched/pelt.c: 133
			 * /Users/rubber/linux/kernel/sched/pelt.c: 134
			 * clause from ___update_load_sum(); this results in /Users/rubber/linux/kernel/sched/pelt.c: 135
			 * the below usage of @contrib to disappear entirely, /Users/rubber/linux/kernel/sched/pelt.c: 136
			 * so no point in calculating it. /Users/rubber/linux/kernel/sched/pelt.c: 137
 * We can represent the historical contribution to runnable average as the /Users/rubber/linux/kernel/sched/pelt.c: 156
 * coefficients of a geometric series.  To do this we sub-divide our runnable /Users/rubber/linux/kernel/sched/pelt.c: 157
 * history into segments of approximately 1ms (1024us); label the segment that /Users/rubber/linux/kernel/sched/pelt.c: 158
 * occurred N-ms ago p_N, with p_0 corresponding to the current period, e.g. /Users/rubber/linux/kernel/sched/pelt.c: 159
 * [<- 1024us ->|<- 1024us ->|<- 1024us ->| ... /Users/rubber/linux/kernel/sched/pelt.c: 161
 *      p0            p1           p2 /Users/rubber/linux/kernel/sched/pelt.c: 162
 *     (now)       (~1ms ago)  (~2ms ago) /Users/rubber/linux/kernel/sched/pelt.c: 163
 * Let u_i denote the fraction of p_i that the entity was runnable. /Users/rubber/linux/kernel/sched/pelt.c: 165
 * We then designate the fractions u_i as our co-efficients, yielding the /Users/rubber/linux/kernel/sched/pelt.c: 167
 * following representation of historical load: /Users/rubber/linux/kernel/sched/pelt.c: 168
 *   u_0 + u_1*y + u_2*y^2 + u_3*y^3 + ... /Users/rubber/linux/kernel/sched/pelt.c: 169
 * We choose y based on the with of a reasonably scheduling period, fixing: /Users/rubber/linux/kernel/sched/pelt.c: 171
 *   y^32 = 0.5 /Users/rubber/linux/kernel/sched/pelt.c: 172
 * This means that the contribution to load ~32ms ago (u_32) will be weighted /Users/rubber/linux/kernel/sched/pelt.c: 174
 * approximately half as much as the contribution to load within the last ms /Users/rubber/linux/kernel/sched/pelt.c: 175
 * (u_0). /Users/rubber/linux/kernel/sched/pelt.c: 176
 * When a period "rolls over" and we have new u_0`, multiplying the previous /Users/rubber/linux/kernel/sched/pelt.c: 178
 * sum again by y is sufficient to update: /Users/rubber/linux/kernel/sched/pelt.c: 179
 *   load_avg = u_0` + y*(u_0 + u_1*y + u_2*y^2 + ... ) /Users/rubber/linux/kernel/sched/pelt.c: 180
 *            = u_0 + u_1*y + u_2*y^2 + ... [re-labeling u_i --> u_{i+1}] /Users/rubber/linux/kernel/sched/pelt.c: 181
	/* /Users/rubber/linux/kernel/sched/pelt.c: 190
	 * This should only happen when time goes backwards, which it /Users/rubber/linux/kernel/sched/pelt.c: 191
	 * unfortunately does during sched clock init when we swap over to TSC. /Users/rubber/linux/kernel/sched/pelt.c: 192
	/* /Users/rubber/linux/kernel/sched/pelt.c: 199
	 * Use 1024ns as the unit of measurement since it's a reasonable /Users/rubber/linux/kernel/sched/pelt.c: 200
	 * approximation of 1us and fast to compute. /Users/rubber/linux/kernel/sched/pelt.c: 201
	/* /Users/rubber/linux/kernel/sched/pelt.c: 209
	 * running is a subset of runnable (weight) so running can't be set if /Users/rubber/linux/kernel/sched/pelt.c: 210
	 * runnable is clear. But there are some corner cases where the current /Users/rubber/linux/kernel/sched/pelt.c: 211
	 * se has been already dequeued but cfs_rq->curr still points to it. /Users/rubber/linux/kernel/sched/pelt.c: 212
	 * This means that weight will be 0 but not running for a sched_entity /Users/rubber/linux/kernel/sched/pelt.c: 213
	 * but also for a cfs_rq if the latter becomes idle. As an example, /Users/rubber/linux/kernel/sched/pelt.c: 214
	 * this happens during idle_balance() which calls /Users/rubber/linux/kernel/sched/pelt.c: 215
	 * update_blocked_averages(). /Users/rubber/linux/kernel/sched/pelt.c: 216
	 * /Users/rubber/linux/kernel/sched/pelt.c: 217
	 * Also see the comment in accumulate_sum(). /Users/rubber/linux/kernel/sched/pelt.c: 218
	/* /Users/rubber/linux/kernel/sched/pelt.c: 223
	 * Now we know we crossed measurement unit boundaries. The *_avg /Users/rubber/linux/kernel/sched/pelt.c: 224
	 * accrues by two steps: /Users/rubber/linux/kernel/sched/pelt.c: 225
	 * /Users/rubber/linux/kernel/sched/pelt.c: 226
	 * Step 1: accumulate *_sum since last_update_time. If we haven't /Users/rubber/linux/kernel/sched/pelt.c: 227
	 * crossed period boundaries, finish. /Users/rubber/linux/kernel/sched/pelt.c: 228
 * When syncing *_avg with *_sum, we must take into account the current /Users/rubber/linux/kernel/sched/pelt.c: 237
 * position in the PELT segment otherwise the remaining part of the segment /Users/rubber/linux/kernel/sched/pelt.c: 238
 * will be considered as idle time whereas it's not yet elapsed and this will /Users/rubber/linux/kernel/sched/pelt.c: 239
 * generate unwanted oscillation in the range [1002..1024[. /Users/rubber/linux/kernel/sched/pelt.c: 240
 * The max value of *_sum varies with the position in the time segment and is /Users/rubber/linux/kernel/sched/pelt.c: 242
 * equals to : /Users/rubber/linux/kernel/sched/pelt.c: 243
 *   LOAD_AVG_MAX*y + sa->period_contrib /Users/rubber/linux/kernel/sched/pelt.c: 245
 * which can be simplified into: /Users/rubber/linux/kernel/sched/pelt.c: 247
 *   LOAD_AVG_MAX - 1024 + sa->period_contrib /Users/rubber/linux/kernel/sched/pelt.c: 249
 * because LOAD_AVG_MAX*y == LOAD_AVG_MAX-1024 /Users/rubber/linux/kernel/sched/pelt.c: 251
 * The same care must be taken when a sched entity is added, updated or /Users/rubber/linux/kernel/sched/pelt.c: 253
 * removed from a cfs_rq and we need to update sched_avg. Scheduler entities /Users/rubber/linux/kernel/sched/pelt.c: 254
 * and the cfs rq, to which they are attached, have the same position in the /Users/rubber/linux/kernel/sched/pelt.c: 255
 * time segment because they use the same clock. This means that we can use /Users/rubber/linux/kernel/sched/pelt.c: 256
 * the period_contrib of cfs_rq when updating the sched_avg of a sched_entity /Users/rubber/linux/kernel/sched/pelt.c: 257
 * if it's more convenient. /Users/rubber/linux/kernel/sched/pelt.c: 258
	/* /Users/rubber/linux/kernel/sched/pelt.c: 265
	 * Step 2: update *_avg. /Users/rubber/linux/kernel/sched/pelt.c: 266
 * sched_entity: /Users/rubber/linux/kernel/sched/pelt.c: 274
 *   task: /Users/rubber/linux/kernel/sched/pelt.c: 276
 *     se_weight()   = se->load.weight /Users/rubber/linux/kernel/sched/pelt.c: 277
 *     se_runnable() = !!on_rq /Users/rubber/linux/kernel/sched/pelt.c: 278
 *   group: [ see update_cfs_group() ] /Users/rubber/linux/kernel/sched/pelt.c: 280
 *     se_weight()   = tg->weight * grq->load_avg / tg->load_avg /Users/rubber/linux/kernel/sched/pelt.c: 281
 *     se_runnable() = grq->h_nr_running /Users/rubber/linux/kernel/sched/pelt.c: 282
 *   runnable_sum = se_runnable() * runnable = grq->runnable_sum /Users/rubber/linux/kernel/sched/pelt.c: 284
 *   runnable_avg = runnable_sum /Users/rubber/linux/kernel/sched/pelt.c: 285
 *   load_sum := runnable /Users/rubber/linux/kernel/sched/pelt.c: 287
 *   load_avg = se_weight(se) * load_sum /Users/rubber/linux/kernel/sched/pelt.c: 288
 * cfq_rq: /Users/rubber/linux/kernel/sched/pelt.c: 290
 *   runnable_sum = \Sum se->avg.runnable_sum /Users/rubber/linux/kernel/sched/pelt.c: 292
 *   runnable_avg = \Sum se->avg.runnable_avg /Users/rubber/linux/kernel/sched/pelt.c: 293
 *   load_sum = \Sum se_weight(se) * se->avg.load_sum /Users/rubber/linux/kernel/sched/pelt.c: 295
 *   load_avg = \Sum se->avg.load_avg /Users/rubber/linux/kernel/sched/pelt.c: 296
 * rt_rq: /Users/rubber/linux/kernel/sched/pelt.c: 340
 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked /Users/rubber/linux/kernel/sched/pelt.c: 342
 *   util_sum = cpu_scale * load_sum /Users/rubber/linux/kernel/sched/pelt.c: 343
 *   runnable_sum = util_sum /Users/rubber/linux/kernel/sched/pelt.c: 344
 *   load_avg and runnable_avg are not supported and meaningless. /Users/rubber/linux/kernel/sched/pelt.c: 346
 * dl_rq: /Users/rubber/linux/kernel/sched/pelt.c: 366
 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked /Users/rubber/linux/kernel/sched/pelt.c: 368
 *   util_sum = cpu_scale * load_sum /Users/rubber/linux/kernel/sched/pelt.c: 369
 *   runnable_sum = util_sum /Users/rubber/linux/kernel/sched/pelt.c: 370
 *   load_avg and runnable_avg are not supported and meaningless. /Users/rubber/linux/kernel/sched/pelt.c: 372
 * thermal: /Users/rubber/linux/kernel/sched/pelt.c: 393
 *   load_sum = \Sum se->avg.load_sum but se->avg.load_sum is not tracked /Users/rubber/linux/kernel/sched/pelt.c: 395
 *   util_avg and runnable_load_avg are not supported and meaningless. /Users/rubber/linux/kernel/sched/pelt.c: 397
 * Unlike rt/dl utilization tracking that track time spent by a cpu /Users/rubber/linux/kernel/sched/pelt.c: 399
 * running a rt/dl task through util_avg, the average thermal pressure is /Users/rubber/linux/kernel/sched/pelt.c: 400
 * tracked through load_avg. This is because thermal pressure signal is /Users/rubber/linux/kernel/sched/pelt.c: 401
 * time weighted "delta" capacity unlike util_avg which is binary. /Users/rubber/linux/kernel/sched/pelt.c: 402
 * "delta capacity" =  actual capacity  - /Users/rubber/linux/kernel/sched/pelt.c: 403
 *			capped capacity a cpu due to a thermal event. /Users/rubber/linux/kernel/sched/pelt.c: 404
 * irq: /Users/rubber/linux/kernel/sched/pelt.c: 424
 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked /Users/rubber/linux/kernel/sched/pelt.c: 426
 *   util_sum = cpu_scale * load_sum /Users/rubber/linux/kernel/sched/pelt.c: 427
 *   runnable_sum = util_sum /Users/rubber/linux/kernel/sched/pelt.c: 428
 *   load_avg and runnable_avg are not supported and meaningless. /Users/rubber/linux/kernel/sched/pelt.c: 430
	/* /Users/rubber/linux/kernel/sched/pelt.c: 438
	 * We can't use clock_pelt because irq time is not accounted in /Users/rubber/linux/kernel/sched/pelt.c: 439
	 * clock_task. Instead we directly scale the running time to /Users/rubber/linux/kernel/sched/pelt.c: 440
	 * reflect the real amount of computation /Users/rubber/linux/kernel/sched/pelt.c: 441
	/* /Users/rubber/linux/kernel/sched/pelt.c: 446
	 * We know the time that has been used by interrupt since last update /Users/rubber/linux/kernel/sched/pelt.c: 447
	 * but we don't when. Let be pessimistic and assume that interrupt has /Users/rubber/linux/kernel/sched/pelt.c: 448
	 * happened just before the update. This is not so far from reality /Users/rubber/linux/kernel/sched/pelt.c: 449
	 * because interrupt will most probably wake up task and trig an update /Users/rubber/linux/kernel/sched/pelt.c: 450
	 * of rq clock during which the metric is updated. /Users/rubber/linux/kernel/sched/pelt.c: 451
	 * We start to decay with normal context time and then we add the /Users/rubber/linux/kernel/sched/pelt.c: 452
	 * interrupt context time. /Users/rubber/linux/kernel/sched/pelt.c: 453
	 * We can safely remove running from rq->clock because /Users/rubber/linux/kernel/sched/pelt.c: 454
	 * rq->clock += delta with delta >= running /Users/rubber/linux/kernel/sched/pelt.c: 455
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/wait_bit.c: 1
 * The implementation of the wait_bit*() and related waiting APIs: /Users/rubber/linux/kernel/sched/wait_bit.c: 3
 * To allow interruptible waiting and asynchronous (i.e. nonblocking) /Users/rubber/linux/kernel/sched/wait_bit.c: 36
 * waiting, the actions of __wait_on_bit() and __wait_on_bit_lock() are /Users/rubber/linux/kernel/sched/wait_bit.c: 37
 * permitted return codes. Nonzero return codes halt waiting and return. /Users/rubber/linux/kernel/sched/wait_bit.c: 38
			/* /Users/rubber/linux/kernel/sched/wait_bit.c: 91
			 * See the comment in prepare_to_wait_event(). /Users/rubber/linux/kernel/sched/wait_bit.c: 92
			 * finish_wait() does not necessarily takes wwq_head->lock, /Users/rubber/linux/kernel/sched/wait_bit.c: 93
			 * but test_and_set_bit() implies mb() which pairs with /Users/rubber/linux/kernel/sched/wait_bit.c: 94
			 * smp_mb__after_atomic() before wake_up_page(). /Users/rubber/linux/kernel/sched/wait_bit.c: 95
 * wake_up_bit - wake up a waiter on a bit /Users/rubber/linux/kernel/sched/wait_bit.c: 131
 * @word: the word being waited on, a kernel virtual address /Users/rubber/linux/kernel/sched/wait_bit.c: 132
 * @bit: the bit of the word being waited on /Users/rubber/linux/kernel/sched/wait_bit.c: 133
 * There is a standard hashed waitqueue table for generic use. This /Users/rubber/linux/kernel/sched/wait_bit.c: 135
 * is the part of the hashtable's accessor API that wakes up waiters /Users/rubber/linux/kernel/sched/wait_bit.c: 136
 * on a bit. For instance, if one were to have waiters on a bitflag, /Users/rubber/linux/kernel/sched/wait_bit.c: 137
 * one would call wake_up_bit() after clearing the bit. /Users/rubber/linux/kernel/sched/wait_bit.c: 138
 * In order for this to function properly, as it uses waitqueue_active() /Users/rubber/linux/kernel/sched/wait_bit.c: 140
 * internally, some kind of memory barrier must be done prior to calling /Users/rubber/linux/kernel/sched/wait_bit.c: 141
 * this. Typically, this will be smp_mb__after_atomic(), but in some /Users/rubber/linux/kernel/sched/wait_bit.c: 142
 * cases where bitflags are manipulated non-atomically under a lock, one /Users/rubber/linux/kernel/sched/wait_bit.c: 143
 * may need to use a less regular barrier, such fs/inode.c's smp_mb(), /Users/rubber/linux/kernel/sched/wait_bit.c: 144
 * because spin_unlock() does not guarantee a memory barrier. /Users/rubber/linux/kernel/sched/wait_bit.c: 145
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/rt.c: 1
 * Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR /Users/rubber/linux/kernel/sched/rt.c: 3
 * policies) /Users/rubber/linux/kernel/sched/rt.c: 4
		/* /Users/rubber/linux/kernel/sched/rt.c: 63
		 * SCHED_DEADLINE updates the bandwidth, as a run away /Users/rubber/linux/kernel/sched/rt.c: 64
		 * RT task with a DL task could hog a CPU. But DL does /Users/rubber/linux/kernel/sched/rt.c: 65
		 * not reset the period. If a deadline task was running /Users/rubber/linux/kernel/sched/rt.c: 66
		 * without an RT task running, it can cause RT tasks to /Users/rubber/linux/kernel/sched/rt.c: 67
		 * throttle when they start up. Kick the timer right away /Users/rubber/linux/kernel/sched/rt.c: 68
		 * to update the period. /Users/rubber/linux/kernel/sched/rt.c: 69
	/* /Users/rubber/linux/kernel/sched/rt.c: 288
	 * Make sure the mask is visible before we set /Users/rubber/linux/kernel/sched/rt.c: 289
	 * the overload count. That is checked to determine /Users/rubber/linux/kernel/sched/rt.c: 290
	 * if we should look at the mask. It would be a shame /Users/rubber/linux/kernel/sched/rt.c: 291
	 * if we looked at the mask, but the mask was not /Users/rubber/linux/kernel/sched/rt.c: 292
	 * updated yet. /Users/rubber/linux/kernel/sched/rt.c: 293
	 * /Users/rubber/linux/kernel/sched/rt.c: 294
	 * Matched by the barrier in pull_rt_task(). /Users/rubber/linux/kernel/sched/rt.c: 295
 * Verify the fitness of task @p to run on @cpu taking into account the uclamp /Users/rubber/linux/kernel/sched/rt.c: 451
 * settings. /Users/rubber/linux/kernel/sched/rt.c: 452
 * This check is only important for heterogeneous systems where uclamp_min value /Users/rubber/linux/kernel/sched/rt.c: 454
 * is higher than the capacity of a @cpu. For non-heterogeneous system this /Users/rubber/linux/kernel/sched/rt.c: 455
 * function will always return true. /Users/rubber/linux/kernel/sched/rt.c: 456
 * The function will return true if the capacity of the @cpu is >= the /Users/rubber/linux/kernel/sched/rt.c: 458
 * uclamp_min and false otherwise. /Users/rubber/linux/kernel/sched/rt.c: 459
 * Note that uclamp_min will be clamped to uclamp_max if uclamp_min /Users/rubber/linux/kernel/sched/rt.c: 461
 * > uclamp_max. /Users/rubber/linux/kernel/sched/rt.c: 462
 * We ran out of runtime, see if we can borrow some from our neighbours. /Users/rubber/linux/kernel/sched/rt.c: 685
		/* /Users/rubber/linux/kernel/sched/rt.c: 706
		 * Either all rqs have inf runtime and there's nothing to steal /Users/rubber/linux/kernel/sched/rt.c: 707
		 * or __disable_runtime() below sets a specific rq to inf to /Users/rubber/linux/kernel/sched/rt.c: 708
		 * indicate its been disabled and disallow stealing. /Users/rubber/linux/kernel/sched/rt.c: 709
		/* /Users/rubber/linux/kernel/sched/rt.c: 714
		 * From runqueues with spare time, take 1/n part of their /Users/rubber/linux/kernel/sched/rt.c: 715
		 * spare time, but no more than our period. /Users/rubber/linux/kernel/sched/rt.c: 716
 * Ensure this RQ takes back all the runtime it lend to its neighbours. /Users/rubber/linux/kernel/sched/rt.c: 737
		/* /Users/rubber/linux/kernel/sched/rt.c: 755
		 * Either we're all inf and nobody needs to borrow, or we're /Users/rubber/linux/kernel/sched/rt.c: 756
		 * already disabled and thus have nothing to do, or we have /Users/rubber/linux/kernel/sched/rt.c: 757
		 * exactly the right amount of runtime to take out. /Users/rubber/linux/kernel/sched/rt.c: 758
		/* /Users/rubber/linux/kernel/sched/rt.c: 765
		 * Calculate the difference between what we started out with /Users/rubber/linux/kernel/sched/rt.c: 766
		 * and what we current have, that's the amount of runtime /Users/rubber/linux/kernel/sched/rt.c: 767
		 * we lend and now have to reclaim. /Users/rubber/linux/kernel/sched/rt.c: 768
		/* /Users/rubber/linux/kernel/sched/rt.c: 772
		 * Greedy reclaim, take back as much as we can. /Users/rubber/linux/kernel/sched/rt.c: 773
			/* /Users/rubber/linux/kernel/sched/rt.c: 779
			 * Can't reclaim from ourselves or disabled runqueues. /Users/rubber/linux/kernel/sched/rt.c: 780
		/* /Users/rubber/linux/kernel/sched/rt.c: 801
		 * We cannot be left wanting - that would mean some runtime /Users/rubber/linux/kernel/sched/rt.c: 802
		 * leaked out of the system. /Users/rubber/linux/kernel/sched/rt.c: 803
		/* /Users/rubber/linux/kernel/sched/rt.c: 807
		 * Disable all the borrow logic by pretending we have inf /Users/rubber/linux/kernel/sched/rt.c: 808
		 * runtime - in which case borrowing doesn't make sense. /Users/rubber/linux/kernel/sched/rt.c: 809
	/* /Users/rubber/linux/kernel/sched/rt.c: 829
	 * Reset each runqueue's bandwidth settings /Users/rubber/linux/kernel/sched/rt.c: 830
	/* /Users/rubber/linux/kernel/sched/rt.c: 867
	 * FIXME: isolated CPUs should really leave the root task group, /Users/rubber/linux/kernel/sched/rt.c: 868
	 * whether they are isolcpus or were isolated via cpusets, lest /Users/rubber/linux/kernel/sched/rt.c: 869
	 * the timer run on a CPU which does not service all runqueues, /Users/rubber/linux/kernel/sched/rt.c: 870
	 * potentially leaving other CPUs indefinitely throttled.  If /Users/rubber/linux/kernel/sched/rt.c: 871
	 * isolation is really required, the user will turn the throttle /Users/rubber/linux/kernel/sched/rt.c: 872
	 * off to kill the perturbations it causes anyway.  Meanwhile, /Users/rubber/linux/kernel/sched/rt.c: 873
	 * this maintains functionality for boot and/or troubleshooting. /Users/rubber/linux/kernel/sched/rt.c: 874
		/* /Users/rubber/linux/kernel/sched/rt.c: 885
		 * When span == cpu_online_mask, taking each rq->lock /Users/rubber/linux/kernel/sched/rt.c: 886
		 * can be time-consuming. Try to avoid it when possible. /Users/rubber/linux/kernel/sched/rt.c: 887
				/* /Users/rubber/linux/kernel/sched/rt.c: 912
				 * When we're idle and a woken (rt) task is /Users/rubber/linux/kernel/sched/rt.c: 913
				 * throttled check_preempt_curr() will set /Users/rubber/linux/kernel/sched/rt.c: 914
				 * skip_update and the time between the wakeup /Users/rubber/linux/kernel/sched/rt.c: 915
				 * and this unthrottle will get accounted as /Users/rubber/linux/kernel/sched/rt.c: 916
				 * 'runtime'. /Users/rubber/linux/kernel/sched/rt.c: 917
		/* /Users/rubber/linux/kernel/sched/rt.c: 974
		 * Don't actually throttle groups that have no runtime assigned /Users/rubber/linux/kernel/sched/rt.c: 975
		 * but accrue some time due to boosting. /Users/rubber/linux/kernel/sched/rt.c: 976
			/* /Users/rubber/linux/kernel/sched/rt.c: 982
			 * In case we did anyway, make it go away, /Users/rubber/linux/kernel/sched/rt.c: 983
			 * replenishment is a joke, since it will replenish us /Users/rubber/linux/kernel/sched/rt.c: 984
			 * with exactly 0 ns. /Users/rubber/linux/kernel/sched/rt.c: 985
 * Update the current task's runtime statistics. Skip current tasks that /Users/rubber/linux/kernel/sched/rt.c: 1000
 * are not in our scheduling class. /Users/rubber/linux/kernel/sched/rt.c: 1001
	/* /Users/rubber/linux/kernel/sched/rt.c: 1092
	 * Change rq's cpupri only if rt_rq is the top queue. /Users/rubber/linux/kernel/sched/rt.c: 1093
	/* /Users/rubber/linux/kernel/sched/rt.c: 1108
	 * Change rq's cpupri only if rt_rq is the top queue. /Users/rubber/linux/kernel/sched/rt.c: 1109
		/* /Users/rubber/linux/kernel/sched/rt.c: 1148
		 * This may have been our highest task, and therefore /Users/rubber/linux/kernel/sched/rt.c: 1149
		 * we may have some recomputation to do /Users/rubber/linux/kernel/sched/rt.c: 1150
 * Change rt_se->run_list location unless SAVE && !MOVE /Users/rubber/linux/kernel/sched/rt.c: 1260
 * assumes ENQUEUE/DEQUEUE flags match /Users/rubber/linux/kernel/sched/rt.c: 1262
	/* /Users/rubber/linux/kernel/sched/rt.c: 1395
	 * Don't enqueue the group if its throttled, or when empty. /Users/rubber/linux/kernel/sched/rt.c: 1396
	 * The latter is a consequence of the former when a child group /Users/rubber/linux/kernel/sched/rt.c: 1397
	 * get throttled and the current group doesn't have any other /Users/rubber/linux/kernel/sched/rt.c: 1398
	 * active members. /Users/rubber/linux/kernel/sched/rt.c: 1399
 * Because the prio of an upper entry depends on the lower /Users/rubber/linux/kernel/sched/rt.c: 1437
 * entries, we must remove entries top - down. /Users/rubber/linux/kernel/sched/rt.c: 1438
 * Adding/removing a task to/from a priority array: /Users/rubber/linux/kernel/sched/rt.c: 1487
 * Put task to the head or the end of the run list without the overhead of /Users/rubber/linux/kernel/sched/rt.c: 1517
 * dequeue followed by enqueue. /Users/rubber/linux/kernel/sched/rt.c: 1518
	/* /Users/rubber/linux/kernel/sched/rt.c: 1569
	 * If the current task on @p's runqueue is an RT task, then /Users/rubber/linux/kernel/sched/rt.c: 1570
	 * try to see if we can wake this RT task up on another /Users/rubber/linux/kernel/sched/rt.c: 1571
	 * runqueue. Otherwise simply start this RT task /Users/rubber/linux/kernel/sched/rt.c: 1572
	 * on its current runqueue. /Users/rubber/linux/kernel/sched/rt.c: 1573
	 * /Users/rubber/linux/kernel/sched/rt.c: 1574
	 * We want to avoid overloading runqueues. If the woken /Users/rubber/linux/kernel/sched/rt.c: 1575
	 * task is a higher priority, then it will stay on this CPU /Users/rubber/linux/kernel/sched/rt.c: 1576
	 * and the lower prio task should be moved to another CPU. /Users/rubber/linux/kernel/sched/rt.c: 1577
	 * Even though this will probably make the lower prio task /Users/rubber/linux/kernel/sched/rt.c: 1578
	 * lose its cache, we do not want to bounce a higher task /Users/rubber/linux/kernel/sched/rt.c: 1579
	 * around just because it gave up its CPU, perhaps for a /Users/rubber/linux/kernel/sched/rt.c: 1580
	 * lock? /Users/rubber/linux/kernel/sched/rt.c: 1581
	 * /Users/rubber/linux/kernel/sched/rt.c: 1582
	 * For equal prio tasks, we just let the scheduler sort it out. /Users/rubber/linux/kernel/sched/rt.c: 1583
	 * /Users/rubber/linux/kernel/sched/rt.c: 1584
	 * Otherwise, just let it ride on the affined RQ and the /Users/rubber/linux/kernel/sched/rt.c: 1585
	 * post-schedule router will push the preempted task away /Users/rubber/linux/kernel/sched/rt.c: 1586
	 * /Users/rubber/linux/kernel/sched/rt.c: 1587
	 * This test is optimistic, if we get it wrong the load-balancer /Users/rubber/linux/kernel/sched/rt.c: 1588
	 * will have to sort it out. /Users/rubber/linux/kernel/sched/rt.c: 1589
	 * /Users/rubber/linux/kernel/sched/rt.c: 1590
	 * We take into account the capacity of the CPU to ensure it fits the /Users/rubber/linux/kernel/sched/rt.c: 1591
	 * requirement of the task - which is only important on heterogeneous /Users/rubber/linux/kernel/sched/rt.c: 1592
	 * systems like big.LITTLE. /Users/rubber/linux/kernel/sched/rt.c: 1593
		/* /Users/rubber/linux/kernel/sched/rt.c: 1602
		 * Bail out if we were forcing a migration to find a better /Users/rubber/linux/kernel/sched/rt.c: 1603
		 * fitting CPU but our search failed. /Users/rubber/linux/kernel/sched/rt.c: 1604
		/* /Users/rubber/linux/kernel/sched/rt.c: 1609
		 * Don't bother moving it if the destination CPU is /Users/rubber/linux/kernel/sched/rt.c: 1610
		 * not running a lower priority task. /Users/rubber/linux/kernel/sched/rt.c: 1611
	/* /Users/rubber/linux/kernel/sched/rt.c: 1627
	 * Current can't be migrated, useless to reschedule, /Users/rubber/linux/kernel/sched/rt.c: 1628
	 * let's hope p can move out. /Users/rubber/linux/kernel/sched/rt.c: 1629
	/* /Users/rubber/linux/kernel/sched/rt.c: 1635
	 * p is migratable, so let's not schedule it and /Users/rubber/linux/kernel/sched/rt.c: 1636
	 * see if it is pushed or pulled somewhere else. /Users/rubber/linux/kernel/sched/rt.c: 1637
	/* /Users/rubber/linux/kernel/sched/rt.c: 1643
	 * There appear to be other CPUs that can accept /Users/rubber/linux/kernel/sched/rt.c: 1644
	 * the current task but none can run 'p', so lets reschedule /Users/rubber/linux/kernel/sched/rt.c: 1645
	 * to try and push the current task away: /Users/rubber/linux/kernel/sched/rt.c: 1646
		/* /Users/rubber/linux/kernel/sched/rt.c: 1655
		 * This is OK, because current is on_cpu, which avoids it being /Users/rubber/linux/kernel/sched/rt.c: 1656
		 * picked for load-balance and preemption/IRQs are still /Users/rubber/linux/kernel/sched/rt.c: 1657
		 * disabled avoiding further scheduler activity on it and we've /Users/rubber/linux/kernel/sched/rt.c: 1658
		 * not yet started the picking loop. /Users/rubber/linux/kernel/sched/rt.c: 1659
 * Preempt the current task with a newly woken task if needed: /Users/rubber/linux/kernel/sched/rt.c: 1671
	/* /Users/rubber/linux/kernel/sched/rt.c: 1681
	 * If: /Users/rubber/linux/kernel/sched/rt.c: 1682
	 * /Users/rubber/linux/kernel/sched/rt.c: 1683
	 * - the newly woken task is of equal priority to the current task /Users/rubber/linux/kernel/sched/rt.c: 1684
	 * - the newly woken task is non-migratable while current is migratable /Users/rubber/linux/kernel/sched/rt.c: 1685
	 * - current will be preempted on the next reschedule /Users/rubber/linux/kernel/sched/rt.c: 1686
	 * /Users/rubber/linux/kernel/sched/rt.c: 1687
	 * we should check to see if current can readily move to a different /Users/rubber/linux/kernel/sched/rt.c: 1688
	 * cpu.  If so, we will reschedule to allow the push logic to try /Users/rubber/linux/kernel/sched/rt.c: 1689
	 * to move current somewhere else, making room for our non-migratable /Users/rubber/linux/kernel/sched/rt.c: 1690
	 * task. /Users/rubber/linux/kernel/sched/rt.c: 1691
	/* /Users/rubber/linux/kernel/sched/rt.c: 1713
	 * If prev task was rt, put_prev_task() has already updated the /Users/rubber/linux/kernel/sched/rt.c: 1714
	 * utilization. We only care of the case where we start to schedule a /Users/rubber/linux/kernel/sched/rt.c: 1715
	 * rt task /Users/rubber/linux/kernel/sched/rt.c: 1716
	/* /Users/rubber/linux/kernel/sched/rt.c: 1789
	 * The previous task needs to be made eligible for pushing /Users/rubber/linux/kernel/sched/rt.c: 1790
	 * if it is still active /Users/rubber/linux/kernel/sched/rt.c: 1791
 * Return the highest pushable rq's task, which is suitable to be executed /Users/rubber/linux/kernel/sched/rt.c: 1812
 * on the CPU, NULL otherwise /Users/rubber/linux/kernel/sched/rt.c: 1813
	/* /Users/rubber/linux/kernel/sched/rt.c: 1848
	 * If we're on asym system ensure we consider the different capacities /Users/rubber/linux/kernel/sched/rt.c: 1849
	 * of the CPUs when searching for the lowest_mask. /Users/rubber/linux/kernel/sched/rt.c: 1850
	/* /Users/rubber/linux/kernel/sched/rt.c: 1866
	 * At this point we have built a mask of CPUs representing the /Users/rubber/linux/kernel/sched/rt.c: 1867
	 * lowest priority tasks in the system.  Now we want to elect /Users/rubber/linux/kernel/sched/rt.c: 1868
	 * the best one based on our affinity and topology. /Users/rubber/linux/kernel/sched/rt.c: 1869
	 * /Users/rubber/linux/kernel/sched/rt.c: 1870
	 * We prioritize the last CPU that the task executed on since /Users/rubber/linux/kernel/sched/rt.c: 1871
	 * it is most likely cache-hot in that location. /Users/rubber/linux/kernel/sched/rt.c: 1872
	/* /Users/rubber/linux/kernel/sched/rt.c: 1877
	 * Otherwise, we consult the sched_domains span maps to figure /Users/rubber/linux/kernel/sched/rt.c: 1878
	 * out which CPU is logically closest to our hot cache data. /Users/rubber/linux/kernel/sched/rt.c: 1879
			/* /Users/rubber/linux/kernel/sched/rt.c: 1889
			 * "this_cpu" is cheaper to preempt than a /Users/rubber/linux/kernel/sched/rt.c: 1890
			 * remote processor. /Users/rubber/linux/kernel/sched/rt.c: 1891
	/* /Users/rubber/linux/kernel/sched/rt.c: 1909
	 * And finally, if there were no matches within the domains /Users/rubber/linux/kernel/sched/rt.c: 1910
	 * just give the caller *something* to work with from the compatible /Users/rubber/linux/kernel/sched/rt.c: 1911
	 * locations. /Users/rubber/linux/kernel/sched/rt.c: 1912
			/* /Users/rubber/linux/kernel/sched/rt.c: 1940
			 * Target rq has tasks of equal or higher priority, /Users/rubber/linux/kernel/sched/rt.c: 1941
			 * retrying does not release any lock and is unlikely /Users/rubber/linux/kernel/sched/rt.c: 1942
			 * to yield a different result. /Users/rubber/linux/kernel/sched/rt.c: 1943
			/* /Users/rubber/linux/kernel/sched/rt.c: 1951
			 * We had to unlock the run queue. In /Users/rubber/linux/kernel/sched/rt.c: 1952
			 * the mean time, task could have /Users/rubber/linux/kernel/sched/rt.c: 1953
			 * migrated already or had its affinity changed. /Users/rubber/linux/kernel/sched/rt.c: 1954
			 * Also make sure that it wasn't scheduled on its rq. /Users/rubber/linux/kernel/sched/rt.c: 1955
 * If the current CPU has more than one RT task, see if the non /Users/rubber/linux/kernel/sched/rt.c: 2002
 * running task can migrate over to a CPU that is running a task /Users/rubber/linux/kernel/sched/rt.c: 2003
 * of lesser priority. /Users/rubber/linux/kernel/sched/rt.c: 2004
		/* /Users/rubber/linux/kernel/sched/rt.c: 2031
		 * Given we found a CPU with lower priority than @next_task, /Users/rubber/linux/kernel/sched/rt.c: 2032
		 * therefore it should be running. However we cannot migrate it /Users/rubber/linux/kernel/sched/rt.c: 2033
		 * to this other CPU, instead attempt to push the current /Users/rubber/linux/kernel/sched/rt.c: 2034
		 * running task on this CPU away. /Users/rubber/linux/kernel/sched/rt.c: 2035
	/* /Users/rubber/linux/kernel/sched/rt.c: 2051
	 * It's possible that the next_task slipped in of /Users/rubber/linux/kernel/sched/rt.c: 2052
	 * higher priority than current. If that's the case /Users/rubber/linux/kernel/sched/rt.c: 2053
	 * just reschedule current. /Users/rubber/linux/kernel/sched/rt.c: 2054
		/* /Users/rubber/linux/kernel/sched/rt.c: 2068
		 * find_lock_lowest_rq releases rq->lock /Users/rubber/linux/kernel/sched/rt.c: 2069
		 * so it is possible that next_task has migrated. /Users/rubber/linux/kernel/sched/rt.c: 2070
		 * /Users/rubber/linux/kernel/sched/rt.c: 2071
		 * We need to make sure that the task is still on the same /Users/rubber/linux/kernel/sched/rt.c: 2072
		 * run-queue and is also still the next task eligible for /Users/rubber/linux/kernel/sched/rt.c: 2073
		 * pushing. /Users/rubber/linux/kernel/sched/rt.c: 2074
			/* /Users/rubber/linux/kernel/sched/rt.c: 2078
			 * The task hasn't migrated, and is still the next /Users/rubber/linux/kernel/sched/rt.c: 2079
			 * eligible task, but we failed to find a run-queue /Users/rubber/linux/kernel/sched/rt.c: 2080
			 * to push it to.  Do not retry in this case, since /Users/rubber/linux/kernel/sched/rt.c: 2081
			 * other CPUs will pull from us when ready. /Users/rubber/linux/kernel/sched/rt.c: 2082
		/* /Users/rubber/linux/kernel/sched/rt.c: 2091
		 * Something has shifted, try again. /Users/rubber/linux/kernel/sched/rt.c: 2092
 * When a high priority task schedules out from a CPU and a lower priority /Users/rubber/linux/kernel/sched/rt.c: 2122
 * task is scheduled in, a check is made to see if there's any RT tasks /Users/rubber/linux/kernel/sched/rt.c: 2123
 * on other CPUs that are waiting to run because a higher priority RT task /Users/rubber/linux/kernel/sched/rt.c: 2124
 * is currently running on its CPU. In this case, the CPU with multiple RT /Users/rubber/linux/kernel/sched/rt.c: 2125
 * tasks queued on it (overloaded) needs to be notified that a CPU has opened /Users/rubber/linux/kernel/sched/rt.c: 2126
 * up that may be able to run one of its non-running queued RT tasks. /Users/rubber/linux/kernel/sched/rt.c: 2127
 * All CPUs with overloaded RT tasks need to be notified as there is currently /Users/rubber/linux/kernel/sched/rt.c: 2129
 * no way to know which of these CPUs have the highest priority task waiting /Users/rubber/linux/kernel/sched/rt.c: 2130
 * to run. Instead of trying to take a spinlock on each of these CPUs, /Users/rubber/linux/kernel/sched/rt.c: 2131
 * which has shown to cause large latency when done on machines with many /Users/rubber/linux/kernel/sched/rt.c: 2132
 * CPUs, sending an IPI to the CPUs to have them push off the overloaded /Users/rubber/linux/kernel/sched/rt.c: 2133
 * RT tasks waiting to run. /Users/rubber/linux/kernel/sched/rt.c: 2134
 * Just sending an IPI to each of the CPUs is also an issue, as on large /Users/rubber/linux/kernel/sched/rt.c: 2136
 * count CPU machines, this can cause an IPI storm on a CPU, especially /Users/rubber/linux/kernel/sched/rt.c: 2137
 * if its the only CPU with multiple RT tasks queued, and a large number /Users/rubber/linux/kernel/sched/rt.c: 2138
 * of CPUs scheduling a lower priority task at the same time. /Users/rubber/linux/kernel/sched/rt.c: 2139
 * Each root domain has its own irq work function that can iterate over /Users/rubber/linux/kernel/sched/rt.c: 2141
 * all CPUs with RT overloaded tasks. Since all CPUs with overloaded RT /Users/rubber/linux/kernel/sched/rt.c: 2142
 * task must be checked if there's one or many CPUs that are lowering /Users/rubber/linux/kernel/sched/rt.c: 2143
 * their priority, there's a single irq work iterator that will try to /Users/rubber/linux/kernel/sched/rt.c: 2144
 * push off RT tasks that are waiting to run. /Users/rubber/linux/kernel/sched/rt.c: 2145
 * When a CPU schedules a lower priority task, it will kick off the /Users/rubber/linux/kernel/sched/rt.c: 2147
 * irq work iterator that will jump to each CPU with overloaded RT tasks. /Users/rubber/linux/kernel/sched/rt.c: 2148
 * As it only takes the first CPU that schedules a lower priority task /Users/rubber/linux/kernel/sched/rt.c: 2149
 * to start the process, the rto_start variable is incremented and if /Users/rubber/linux/kernel/sched/rt.c: 2150
 * the atomic result is one, then that CPU will try to take the rto_lock. /Users/rubber/linux/kernel/sched/rt.c: 2151
 * This prevents high contention on the lock as the process handles all /Users/rubber/linux/kernel/sched/rt.c: 2152
 * CPUs scheduling lower priority tasks. /Users/rubber/linux/kernel/sched/rt.c: 2153
 * All CPUs that are scheduling a lower priority task will increment the /Users/rubber/linux/kernel/sched/rt.c: 2155
 * rt_loop_next variable. This will make sure that the irq work iterator /Users/rubber/linux/kernel/sched/rt.c: 2156
 * checks all RT overloaded CPUs whenever a CPU schedules a new lower /Users/rubber/linux/kernel/sched/rt.c: 2157
 * priority task, even if the iterator is in the middle of a scan. Incrementing /Users/rubber/linux/kernel/sched/rt.c: 2158
 * the rt_loop_next will cause the iterator to perform another scan. /Users/rubber/linux/kernel/sched/rt.c: 2159
	/* /Users/rubber/linux/kernel/sched/rt.c: 2167
	 * When starting the IPI RT pushing, the rto_cpu is set to -1, /Users/rubber/linux/kernel/sched/rt.c: 2168
	 * rt_next_cpu() will simply return the first CPU found in /Users/rubber/linux/kernel/sched/rt.c: 2169
	 * the rto_mask. /Users/rubber/linux/kernel/sched/rt.c: 2170
	 * /Users/rubber/linux/kernel/sched/rt.c: 2171
	 * If rto_next_cpu() is called with rto_cpu is a valid CPU, it /Users/rubber/linux/kernel/sched/rt.c: 2172
	 * will return the next CPU found in the rto_mask. /Users/rubber/linux/kernel/sched/rt.c: 2173
	 * /Users/rubber/linux/kernel/sched/rt.c: 2174
	 * If there are no more CPUs left in the rto_mask, then a check is made /Users/rubber/linux/kernel/sched/rt.c: 2175
	 * against rto_loop and rto_loop_next. rto_loop is only updated with /Users/rubber/linux/kernel/sched/rt.c: 2176
	 * the rto_lock held, but any CPU may increment the rto_loop_next /Users/rubber/linux/kernel/sched/rt.c: 2177
	 * without any locking. /Users/rubber/linux/kernel/sched/rt.c: 2178
		/* /Users/rubber/linux/kernel/sched/rt.c: 2192
		 * ACQUIRE ensures we see the @rto_mask changes /Users/rubber/linux/kernel/sched/rt.c: 2193
		 * made prior to the @next value observed. /Users/rubber/linux/kernel/sched/rt.c: 2194
		 * /Users/rubber/linux/kernel/sched/rt.c: 2195
		 * Matches WMB in rt_set_overload(). /Users/rubber/linux/kernel/sched/rt.c: 2196
	/* /Users/rubber/linux/kernel/sched/rt.c: 2232
	 * The rto_cpu is updated under the lock, if it has a valid CPU /Users/rubber/linux/kernel/sched/rt.c: 2233
	 * then the IPI is still running and will continue due to the /Users/rubber/linux/kernel/sched/rt.c: 2234
	 * update to loop_next, and nothing needs to be done here. /Users/rubber/linux/kernel/sched/rt.c: 2235
	 * Otherwise it is finishing up and an ipi needs to be sent. /Users/rubber/linux/kernel/sched/rt.c: 2236
	/* /Users/rubber/linux/kernel/sched/rt.c: 2262
	 * We do not need to grab the lock to check for has_pushable_tasks. /Users/rubber/linux/kernel/sched/rt.c: 2263
	 * When it gets updated, a check is made if a push is possible. /Users/rubber/linux/kernel/sched/rt.c: 2264
	/* /Users/rubber/linux/kernel/sched/rt.c: 2301
	 * Match the barrier from rt_set_overloaded; this guarantees that if we /Users/rubber/linux/kernel/sched/rt.c: 2302
	 * see overloaded we must also see the rto_mask bit. /Users/rubber/linux/kernel/sched/rt.c: 2303
		/* /Users/rubber/linux/kernel/sched/rt.c: 2325
		 * Don't bother taking the src_rq->lock if the next highest /Users/rubber/linux/kernel/sched/rt.c: 2326
		 * task is known to be lower-priority than our current task. /Users/rubber/linux/kernel/sched/rt.c: 2327
		 * This may look racy, but if this value is about to go /Users/rubber/linux/kernel/sched/rt.c: 2328
		 * logically higher, the src_rq will push this task away. /Users/rubber/linux/kernel/sched/rt.c: 2329
		 * And if its going logically lower, we do not care /Users/rubber/linux/kernel/sched/rt.c: 2330
		/* /Users/rubber/linux/kernel/sched/rt.c: 2336
		 * We can potentially drop this_rq's lock in /Users/rubber/linux/kernel/sched/rt.c: 2337
		 * double_lock_balance, and another CPU could /Users/rubber/linux/kernel/sched/rt.c: 2338
		 * alter this_rq /Users/rubber/linux/kernel/sched/rt.c: 2339
		/* /Users/rubber/linux/kernel/sched/rt.c: 2344
		 * We can pull only a task, which is pushable /Users/rubber/linux/kernel/sched/rt.c: 2345
		 * on its rq, and no others. /Users/rubber/linux/kernel/sched/rt.c: 2346
		/* /Users/rubber/linux/kernel/sched/rt.c: 2350
		 * Do we have an RT task that preempts /Users/rubber/linux/kernel/sched/rt.c: 2351
		 * the to-be-scheduled task? /Users/rubber/linux/kernel/sched/rt.c: 2352
			/* /Users/rubber/linux/kernel/sched/rt.c: 2358
			 * There's a chance that p is higher in priority /Users/rubber/linux/kernel/sched/rt.c: 2359
			 * than what's currently running on its CPU. /Users/rubber/linux/kernel/sched/rt.c: 2360
			 * This is just that p is waking up and hasn't /Users/rubber/linux/kernel/sched/rt.c: 2361
			 * had a chance to schedule. We only pull /Users/rubber/linux/kernel/sched/rt.c: 2362
			 * p if it is lower in priority than the /Users/rubber/linux/kernel/sched/rt.c: 2363
			 * current task on the run queue /Users/rubber/linux/kernel/sched/rt.c: 2364
			/* /Users/rubber/linux/kernel/sched/rt.c: 2377
			 * We continue with the search, just in /Users/rubber/linux/kernel/sched/rt.c: 2378
			 * case there's an even higher prio task /Users/rubber/linux/kernel/sched/rt.c: 2379
			 * in another runqueue. (low likelihood /Users/rubber/linux/kernel/sched/rt.c: 2380
			 * but possible) /Users/rubber/linux/kernel/sched/rt.c: 2381
 * If we are not running and we are not going to reschedule soon, we should /Users/rubber/linux/kernel/sched/rt.c: 2400
 * try to push tasks away now /Users/rubber/linux/kernel/sched/rt.c: 2401
 * When switch from the rt queue, we bring ourselves to a position /Users/rubber/linux/kernel/sched/rt.c: 2439
 * that we might want to pull RT tasks from other runqueues. /Users/rubber/linux/kernel/sched/rt.c: 2440
	/* /Users/rubber/linux/kernel/sched/rt.c: 2444
	 * If there are other RT tasks then we will reschedule /Users/rubber/linux/kernel/sched/rt.c: 2445
	 * and the scheduling of the other RT tasks will handle /Users/rubber/linux/kernel/sched/rt.c: 2446
	 * the balancing. But if we are the last RT task /Users/rubber/linux/kernel/sched/rt.c: 2447
	 * we may need to handle the pulling of RT tasks /Users/rubber/linux/kernel/sched/rt.c: 2448
	 * now. /Users/rubber/linux/kernel/sched/rt.c: 2449
 * When switching a task to RT, we may overload the runqueue /Users/rubber/linux/kernel/sched/rt.c: 2469
 * with RT tasks. In this case we try to push them off to /Users/rubber/linux/kernel/sched/rt.c: 2470
 * other runqueues. /Users/rubber/linux/kernel/sched/rt.c: 2471
	/* /Users/rubber/linux/kernel/sched/rt.c: 2475
	 * If we are running, update the avg_rt tracking, as the running time /Users/rubber/linux/kernel/sched/rt.c: 2476
	 * will now on be accounted into the latter. /Users/rubber/linux/kernel/sched/rt.c: 2477
	/* /Users/rubber/linux/kernel/sched/rt.c: 2484
	 * If we are not running we may need to preempt the current /Users/rubber/linux/kernel/sched/rt.c: 2485
	 * running task. If that current running task is also an RT task /Users/rubber/linux/kernel/sched/rt.c: 2486
	 * then see if we can move to another run queue. /Users/rubber/linux/kernel/sched/rt.c: 2487
 * Priority of the task has changed. This may cause /Users/rubber/linux/kernel/sched/rt.c: 2500
 * us to initiate a push or pull. /Users/rubber/linux/kernel/sched/rt.c: 2501
		/* /Users/rubber/linux/kernel/sched/rt.c: 2511
		 * If our priority decreases while running, we /Users/rubber/linux/kernel/sched/rt.c: 2512
		 * may need to pull tasks to this runqueue. /Users/rubber/linux/kernel/sched/rt.c: 2513
		/* /Users/rubber/linux/kernel/sched/rt.c: 2518
		 * If there's a higher priority task waiting to run /Users/rubber/linux/kernel/sched/rt.c: 2519
		 * then reschedule. /Users/rubber/linux/kernel/sched/rt.c: 2520
		/* /Users/rubber/linux/kernel/sched/rt.c: 2530
		 * This task is not running, but if it is /Users/rubber/linux/kernel/sched/rt.c: 2531
		 * greater than the current running task /Users/rubber/linux/kernel/sched/rt.c: 2532
		 * then reschedule. /Users/rubber/linux/kernel/sched/rt.c: 2533
 * scheduler tick hitting a task of our scheduling class. /Users/rubber/linux/kernel/sched/rt.c: 2569
 * NOTE: This function can be called remotely by the tick offload that /Users/rubber/linux/kernel/sched/rt.c: 2571
 * goes along full dynticks. Therefore no local assumption can be made /Users/rubber/linux/kernel/sched/rt.c: 2572
 * and everything must be accessed through the @rq and @curr passed in /Users/rubber/linux/kernel/sched/rt.c: 2573
 * parameters. /Users/rubber/linux/kernel/sched/rt.c: 2574
	/* /Users/rubber/linux/kernel/sched/rt.c: 2585
	 * RR tasks need a special form of timeslice management. /Users/rubber/linux/kernel/sched/rt.c: 2586
	 * FIFO tasks have no timeslices. /Users/rubber/linux/kernel/sched/rt.c: 2587
	/* /Users/rubber/linux/kernel/sched/rt.c: 2597
	 * Requeue to the end of queue if we (and all of our ancestors) are not /Users/rubber/linux/kernel/sched/rt.c: 2598
	 * the only element on the queue /Users/rubber/linux/kernel/sched/rt.c: 2599
	/* /Users/rubber/linux/kernel/sched/rt.c: 2612
	 * Time slice is 0 for SCHED_FIFO tasks /Users/rubber/linux/kernel/sched/rt.c: 2613
 * Ensure that the real time constraints are schedulable. /Users/rubber/linux/kernel/sched/rt.c: 2661
	/* /Users/rubber/linux/kernel/sched/rt.c: 2671
	 * Autogroups do not have RT tasks; see autogroup_create(). /Users/rubber/linux/kernel/sched/rt.c: 2672
	/* /Users/rubber/linux/kernel/sched/rt.c: 2706
	 * Cannot have more runtime than the period. /Users/rubber/linux/kernel/sched/rt.c: 2707
	/* /Users/rubber/linux/kernel/sched/rt.c: 2712
	 * Ensure we don't starve existing RT tasks if runtime turns zero. /Users/rubber/linux/kernel/sched/rt.c: 2713
	/* /Users/rubber/linux/kernel/sched/rt.c: 2721
	 * Nobody can have more than the global setting allows. /Users/rubber/linux/kernel/sched/rt.c: 2722
	/* /Users/rubber/linux/kernel/sched/rt.c: 2727
	 * The sum of our children's runtime should not exceed our own. /Users/rubber/linux/kernel/sched/rt.c: 2728
	/* /Users/rubber/linux/kernel/sched/rt.c: 2770
	 * Disallowing the root group RT runtime is BAD, it would disallow the /Users/rubber/linux/kernel/sched/rt.c: 2771
	 * kernel creating (and or operating) RT threads. /Users/rubber/linux/kernel/sched/rt.c: 2772
	/* /Users/rubber/linux/kernel/sched/rt.c: 2781
	 * Bound quota to defend quota against overflow during bandwidth shift. /Users/rubber/linux/kernel/sched/rt.c: 2782
	/* /Users/rubber/linux/kernel/sched/rt.c: 2965
	 * Make sure that internally we keep jiffies. /Users/rubber/linux/kernel/sched/rt.c: 2966
	 * Also, writing zero resets the timeslice to default: /Users/rubber/linux/kernel/sched/rt.c: 2967
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/cpupri.c: 1
 *  kernel/sched/cpupri.c /Users/rubber/linux/kernel/sched/cpupri.c: 3
 *  CPU priority management /Users/rubber/linux/kernel/sched/cpupri.c: 5
 *  Copyright (C) 2007-2008 Novell /Users/rubber/linux/kernel/sched/cpupri.c: 7
 *  Author: Gregory Haskins <ghaskins@novell.com> /Users/rubber/linux/kernel/sched/cpupri.c: 9
 *  This code tracks the priority of each CPU so that global migration /Users/rubber/linux/kernel/sched/cpupri.c: 11
 *  decisions are easy to calculate.  Each CPU can be in a state as follows: /Users/rubber/linux/kernel/sched/cpupri.c: 12
 *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER /Users/rubber/linux/kernel/sched/cpupri.c: 14
 *  going from the lowest priority to the highest.  CPUs in the INVALID state /Users/rubber/linux/kernel/sched/cpupri.c: 16
 *  are not eligible for routing.  The system maintains this state with /Users/rubber/linux/kernel/sched/cpupri.c: 17
 *  a 2 dimensional bitmap (the first for priority class, the second for CPUs /Users/rubber/linux/kernel/sched/cpupri.c: 18
 *  in that class).  Therefore a typical application without affinity /Users/rubber/linux/kernel/sched/cpupri.c: 19
 *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit /Users/rubber/linux/kernel/sched/cpupri.c: 20
 *  searches).  For tasks with affinity restrictions, the algorithm has a /Users/rubber/linux/kernel/sched/cpupri.c: 21
 *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that /Users/rubber/linux/kernel/sched/cpupri.c: 22
 *  yields the worst case search is fairly contrived. /Users/rubber/linux/kernel/sched/cpupri.c: 23
 * p->rt_priority   p->prio   newpri   cpupri /Users/rubber/linux/kernel/sched/cpupri.c: 28
 *				  -1       -1 (CPUPRI_INVALID) /Users/rubber/linux/kernel/sched/cpupri.c: 30
 *				  99        0 (CPUPRI_NORMAL) /Users/rubber/linux/kernel/sched/cpupri.c: 32
 *		1        98       98        1 /Users/rubber/linux/kernel/sched/cpupri.c: 34
 *	      ... /Users/rubber/linux/kernel/sched/cpupri.c: 35
 *	       49        50       50       49 /Users/rubber/linux/kernel/sched/cpupri.c: 36
 *	       50        49       49       50 /Users/rubber/linux/kernel/sched/cpupri.c: 37
 *	      ... /Users/rubber/linux/kernel/sched/cpupri.c: 38
 *	       99         0        0       99 /Users/rubber/linux/kernel/sched/cpupri.c: 39
 *				 100	  100 (CPUPRI_HIGHER) /Users/rubber/linux/kernel/sched/cpupri.c: 41
	/* /Users/rubber/linux/kernel/sched/cpupri.c: 76
	 * When looking at the vector, we need to read the counter, /Users/rubber/linux/kernel/sched/cpupri.c: 77
	 * do a memory barrier, then read the mask. /Users/rubber/linux/kernel/sched/cpupri.c: 78
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 79
	 * Note: This is still all racy, but we can deal with it. /Users/rubber/linux/kernel/sched/cpupri.c: 80
	 *  Ideally, we only want to look at masks that are set. /Users/rubber/linux/kernel/sched/cpupri.c: 81
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 82
	 *  If a mask is not set, then the only thing wrong is that we /Users/rubber/linux/kernel/sched/cpupri.c: 83
	 *  did a little more work than necessary. /Users/rubber/linux/kernel/sched/cpupri.c: 84
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 85
	 *  If we read a zero count but the mask is set, because of the /Users/rubber/linux/kernel/sched/cpupri.c: 86
	 *  memory barriers, that can only happen when the highest prio /Users/rubber/linux/kernel/sched/cpupri.c: 87
	 *  task for a run queue has left the run queue, in which case, /Users/rubber/linux/kernel/sched/cpupri.c: 88
	 *  it will be followed by a pull. If the task we are processing /Users/rubber/linux/kernel/sched/cpupri.c: 89
	 *  fails to find a proper place to go, that pull request will /Users/rubber/linux/kernel/sched/cpupri.c: 90
	 *  pull this task if the run queue is running at a lower /Users/rubber/linux/kernel/sched/cpupri.c: 91
	 *  priority. /Users/rubber/linux/kernel/sched/cpupri.c: 92
		/* /Users/rubber/linux/kernel/sched/cpupri.c: 106
		 * We have to ensure that we have at least one bit /Users/rubber/linux/kernel/sched/cpupri.c: 107
		 * still set in the array, since the map could have /Users/rubber/linux/kernel/sched/cpupri.c: 108
		 * been concurrently emptied between the first and /Users/rubber/linux/kernel/sched/cpupri.c: 109
		 * second reads of vec->mask.  If we hit this /Users/rubber/linux/kernel/sched/cpupri.c: 110
		 * condition, simply act as though we never hit this /Users/rubber/linux/kernel/sched/cpupri.c: 111
		 * priority level and continue on. /Users/rubber/linux/kernel/sched/cpupri.c: 112
 * cpupri_find_fitness - find the best (lowest-pri) CPU in the system /Users/rubber/linux/kernel/sched/cpupri.c: 128
 * @cp: The cpupri context /Users/rubber/linux/kernel/sched/cpupri.c: 129
 * @p: The task /Users/rubber/linux/kernel/sched/cpupri.c: 130
 * @lowest_mask: A mask to fill in with selected CPUs (or NULL) /Users/rubber/linux/kernel/sched/cpupri.c: 131
 * @fitness_fn: A pointer to a function to do custom checks whether the CPU /Users/rubber/linux/kernel/sched/cpupri.c: 132
 *              fits a specific criteria so that we only return those CPUs. /Users/rubber/linux/kernel/sched/cpupri.c: 133
 * Note: This function returns the recommended CPUs as calculated during the /Users/rubber/linux/kernel/sched/cpupri.c: 135
 * current invocation.  By the time the call returns, the CPUs may have in /Users/rubber/linux/kernel/sched/cpupri.c: 136
 * fact changed priorities any number of times.  While not ideal, it is not /Users/rubber/linux/kernel/sched/cpupri.c: 137
 * an issue of correctness since the normal rebalancer logic will correct /Users/rubber/linux/kernel/sched/cpupri.c: 138
 * any discrepancies created by racing against the uncertainty of the current /Users/rubber/linux/kernel/sched/cpupri.c: 139
 * priority configuration. /Users/rubber/linux/kernel/sched/cpupri.c: 140
 * Return: (int)bool - CPUs were found /Users/rubber/linux/kernel/sched/cpupri.c: 142
		/* /Users/rubber/linux/kernel/sched/cpupri.c: 167
		 * If no CPU at the current priority can fit the task /Users/rubber/linux/kernel/sched/cpupri.c: 168
		 * continue looking /Users/rubber/linux/kernel/sched/cpupri.c: 169
	/* /Users/rubber/linux/kernel/sched/cpupri.c: 177
	 * If we failed to find a fitting lowest_mask, kick off a new search /Users/rubber/linux/kernel/sched/cpupri.c: 178
	 * but without taking into account any fitness criteria this time. /Users/rubber/linux/kernel/sched/cpupri.c: 179
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 180
	 * This rule favours honouring priority over fitting the task in the /Users/rubber/linux/kernel/sched/cpupri.c: 181
	 * correct CPU (Capacity Awareness being the only user now). /Users/rubber/linux/kernel/sched/cpupri.c: 182
	 * The idea is that if a higher priority task can run, then it should /Users/rubber/linux/kernel/sched/cpupri.c: 183
	 * run even if this ends up being on unfitting CPU. /Users/rubber/linux/kernel/sched/cpupri.c: 184
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 185
	 * The cost of this trade-off is not entirely clear and will probably /Users/rubber/linux/kernel/sched/cpupri.c: 186
	 * be good for some workloads and bad for others. /Users/rubber/linux/kernel/sched/cpupri.c: 187
	 * /Users/rubber/linux/kernel/sched/cpupri.c: 188
	 * The main idea here is that if some CPUs were over-committed, we try /Users/rubber/linux/kernel/sched/cpupri.c: 189
	 * to spread which is what the scheduler traditionally did. Sys admins /Users/rubber/linux/kernel/sched/cpupri.c: 190
	 * must do proper RT planning to avoid overloading the system if they /Users/rubber/linux/kernel/sched/cpupri.c: 191
	 * really care. /Users/rubber/linux/kernel/sched/cpupri.c: 192
 * cpupri_set - update the CPU priority setting /Users/rubber/linux/kernel/sched/cpupri.c: 201
 * @cp: The cpupri context /Users/rubber/linux/kernel/sched/cpupri.c: 202
 * @cpu: The target CPU /Users/rubber/linux/kernel/sched/cpupri.c: 203
 * @newpri: The priority (INVALID,NORMAL,RT1-RT99,HIGHER) to assign to this CPU /Users/rubber/linux/kernel/sched/cpupri.c: 204
 * Note: Assumes cpu_rq(cpu)->lock is locked /Users/rubber/linux/kernel/sched/cpupri.c: 206
 * Returns: (void) /Users/rubber/linux/kernel/sched/cpupri.c: 208
	/* /Users/rubber/linux/kernel/sched/cpupri.c: 223
	 * If the CPU was currently mapped to a different value, we /Users/rubber/linux/kernel/sched/cpupri.c: 224
	 * need to map it to the new value then remove the old value. /Users/rubber/linux/kernel/sched/cpupri.c: 225
	 * Note, we must add the new value first, otherwise we risk the /Users/rubber/linux/kernel/sched/cpupri.c: 226
	 * cpu being missed by the priority loop in cpupri_find. /Users/rubber/linux/kernel/sched/cpupri.c: 227
		/* /Users/rubber/linux/kernel/sched/cpupri.c: 233
		 * When adding a new vector, we update the mask first, /Users/rubber/linux/kernel/sched/cpupri.c: 234
		 * do a write memory barrier, and then update the count, to /Users/rubber/linux/kernel/sched/cpupri.c: 235
		 * make sure the vector is visible when count is set. /Users/rubber/linux/kernel/sched/cpupri.c: 236
		/* /Users/rubber/linux/kernel/sched/cpupri.c: 245
		 * Because the order of modification of the vec->count /Users/rubber/linux/kernel/sched/cpupri.c: 246
		 * is important, we must make sure that the update /Users/rubber/linux/kernel/sched/cpupri.c: 247
		 * of the new prio is seen before we decrement the /Users/rubber/linux/kernel/sched/cpupri.c: 248
		 * old prio. This makes sure that the loop sees /Users/rubber/linux/kernel/sched/cpupri.c: 249
		 * one or the other when we raise the priority of /Users/rubber/linux/kernel/sched/cpupri.c: 250
		 * the run queue. We don't care about when we lower the /Users/rubber/linux/kernel/sched/cpupri.c: 251
		 * priority, as that will trigger an rt pull anyway. /Users/rubber/linux/kernel/sched/cpupri.c: 252
		 * /Users/rubber/linux/kernel/sched/cpupri.c: 253
		 * We only need to do a memory barrier if we updated /Users/rubber/linux/kernel/sched/cpupri.c: 254
		 * the new priority vec. /Users/rubber/linux/kernel/sched/cpupri.c: 255
		/* /Users/rubber/linux/kernel/sched/cpupri.c: 260
		 * When removing from the vector, we decrement the counter first /Users/rubber/linux/kernel/sched/cpupri.c: 261
		 * do a memory barrier and then clear the mask. /Users/rubber/linux/kernel/sched/cpupri.c: 262
 * cpupri_init - initialize the cpupri structure /Users/rubber/linux/kernel/sched/cpupri.c: 273
 * @cp: The cpupri context /Users/rubber/linux/kernel/sched/cpupri.c: 274
 * Return: -ENOMEM on memory allocation failure. /Users/rubber/linux/kernel/sched/cpupri.c: 276
 * cpupri_cleanup - clean up the cpupri structure /Users/rubber/linux/kernel/sched/cpupri.c: 306
 * @cp: The cpupri context /Users/rubber/linux/kernel/sched/cpupri.c: 307
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/isolation.c: 1
 *  Housekeeping management. Manage the targets for routine code that can run on /Users/rubber/linux/kernel/sched/isolation.c: 3
 *  any CPU: unbound workqueues, timers, kthreads and any offloadable work. /Users/rubber/linux/kernel/sched/isolation.c: 4
 * Copyright (C) 2017 Red Hat, Inc., Frederic Weisbecker /Users/rubber/linux/kernel/sched/isolation.c: 6
 * Copyright (C) 2017-2018 SUSE, Frederic Weisbecker /Users/rubber/linux/kernel/sched/isolation.c: 7
		/* /Users/rubber/linux/kernel/sched/isolation.c: 174
		 * Skip unknown sub-parameter and validate that it is not /Users/rubber/linux/kernel/sched/isolation.c: 175
		 * containing an invalid character. /Users/rubber/linux/kernel/sched/isolation.c: 176
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/cpuacct.c: 1
 * CPU accounting code for task groups. /Users/rubber/linux/kernel/sched/cpuacct.c: 3
 * Based on the work by Paul Menage (menage@google.com) and Balbir Singh /Users/rubber/linux/kernel/sched/cpuacct.c: 5
 * (balbir@in.ibm.com). /Users/rubber/linux/kernel/sched/cpuacct.c: 6
	/* /Users/rubber/linux/kernel/sched/cpuacct.c: 105
	 * We allow index == CPUACCT_STAT_NSTATS here to read /Users/rubber/linux/kernel/sched/cpuacct.c: 106
	 * the sum of usages. /Users/rubber/linux/kernel/sched/cpuacct.c: 107
	/* /Users/rubber/linux/kernel/sched/cpuacct.c: 112
	 * Take rq->lock to make 64-bit read safe on 32-bit platforms. /Users/rubber/linux/kernel/sched/cpuacct.c: 113
	/* /Users/rubber/linux/kernel/sched/cpuacct.c: 141
	 * Take rq->lock to make 64-bit write safe on 32-bit platforms. /Users/rubber/linux/kernel/sched/cpuacct.c: 142
	/* /Users/rubber/linux/kernel/sched/cpuacct.c: 192
	 * Only allow '0' here to do a reset. /Users/rubber/linux/kernel/sched/cpuacct.c: 193
			/* /Users/rubber/linux/kernel/sched/cpuacct.c: 252
			 * Take rq->lock to make 64-bit read safe on 32-bit /Users/rubber/linux/kernel/sched/cpuacct.c: 253
			 * platforms. /Users/rubber/linux/kernel/sched/cpuacct.c: 254
 * charge this task's execution time to its accounting group. /Users/rubber/linux/kernel/sched/cpuacct.c: 335
 * called with rq->lock held. /Users/rubber/linux/kernel/sched/cpuacct.c: 337
 * Add user/system time to cpuacct. /Users/rubber/linux/kernel/sched/cpuacct.c: 357
 * Note: it's the caller that updates the account of the root cgroup. /Users/rubber/linux/kernel/sched/cpuacct.c: 359
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/kernel/sched/membarrier.c: 1
 * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com> /Users/rubber/linux/kernel/sched/membarrier.c: 3
 * membarrier system call /Users/rubber/linux/kernel/sched/membarrier.c: 5
 * For documentation purposes, here are some membarrier ordering /Users/rubber/linux/kernel/sched/membarrier.c: 10
 * scenarios to keep in mind: /Users/rubber/linux/kernel/sched/membarrier.c: 11
 * A) Userspace thread execution after IPI vs membarrier's memory /Users/rubber/linux/kernel/sched/membarrier.c: 13
 *    barrier before sending the IPI /Users/rubber/linux/kernel/sched/membarrier.c: 14
 * Userspace variables: /Users/rubber/linux/kernel/sched/membarrier.c: 16
 * int x = 0, y = 0; /Users/rubber/linux/kernel/sched/membarrier.c: 18
 * The memory barrier at the start of membarrier() on CPU0 is necessary in /Users/rubber/linux/kernel/sched/membarrier.c: 20
 * order to enforce the guarantee that any writes occurring on CPU0 before /Users/rubber/linux/kernel/sched/membarrier.c: 21
 * the membarrier() is executed will be visible to any code executing on /Users/rubber/linux/kernel/sched/membarrier.c: 22
 * CPU1 after the IPI-induced memory barrier: /Users/rubber/linux/kernel/sched/membarrier.c: 23
 *         CPU0                              CPU1 /Users/rubber/linux/kernel/sched/membarrier.c: 25
 *         x = 1 /Users/rubber/linux/kernel/sched/membarrier.c: 27
 *         membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 28
 *           a: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 29
 *           b: send IPI                       IPI-induced mb /Users/rubber/linux/kernel/sched/membarrier.c: 30
 *           c: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 31
 *         r2 = y /Users/rubber/linux/kernel/sched/membarrier.c: 32
 *                                           y = 1 /Users/rubber/linux/kernel/sched/membarrier.c: 33
 *                                           barrier() /Users/rubber/linux/kernel/sched/membarrier.c: 34
 *                                           r1 = x /Users/rubber/linux/kernel/sched/membarrier.c: 35
 *                     BUG_ON(r1 == 0 && r2 == 0) /Users/rubber/linux/kernel/sched/membarrier.c: 37
 * The write to y and load from x by CPU1 are unordered by the hardware, /Users/rubber/linux/kernel/sched/membarrier.c: 39
 * so it's possible to have "r1 = x" reordered before "y = 1" at any /Users/rubber/linux/kernel/sched/membarrier.c: 40
 * point after (b).  If the memory barrier at (a) is omitted, then "x = 1" /Users/rubber/linux/kernel/sched/membarrier.c: 41
 * can be reordered after (a) (although not after (c)), so we get r1 == 0 /Users/rubber/linux/kernel/sched/membarrier.c: 42
 * and r2 == 0.  This violates the guarantee that membarrier() is /Users/rubber/linux/kernel/sched/membarrier.c: 43
 * supposed by provide. /Users/rubber/linux/kernel/sched/membarrier.c: 44
 * The timing of the memory barrier at (a) has to ensure that it executes /Users/rubber/linux/kernel/sched/membarrier.c: 46
 * before the IPI-induced memory barrier on CPU1. /Users/rubber/linux/kernel/sched/membarrier.c: 47
 * B) Userspace thread execution before IPI vs membarrier's memory /Users/rubber/linux/kernel/sched/membarrier.c: 49
 *    barrier after completing the IPI /Users/rubber/linux/kernel/sched/membarrier.c: 50
 * Userspace variables: /Users/rubber/linux/kernel/sched/membarrier.c: 52
 * int x = 0, y = 0; /Users/rubber/linux/kernel/sched/membarrier.c: 54
 * The memory barrier at the end of membarrier() on CPU0 is necessary in /Users/rubber/linux/kernel/sched/membarrier.c: 56
 * order to enforce the guarantee that any writes occurring on CPU1 before /Users/rubber/linux/kernel/sched/membarrier.c: 57
 * the membarrier() is executed will be visible to any code executing on /Users/rubber/linux/kernel/sched/membarrier.c: 58
 * CPU0 after the membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 59
 *         CPU0                              CPU1 /Users/rubber/linux/kernel/sched/membarrier.c: 61
 *                                           x = 1 /Users/rubber/linux/kernel/sched/membarrier.c: 63
 *                                           barrier() /Users/rubber/linux/kernel/sched/membarrier.c: 64
 *                                           y = 1 /Users/rubber/linux/kernel/sched/membarrier.c: 65
 *         r2 = y /Users/rubber/linux/kernel/sched/membarrier.c: 66
 *         membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 67
 *           a: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 68
 *           b: send IPI                       IPI-induced mb /Users/rubber/linux/kernel/sched/membarrier.c: 69
 *           c: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 70
 *         r1 = x /Users/rubber/linux/kernel/sched/membarrier.c: 71
 *         BUG_ON(r1 == 0 && r2 == 1) /Users/rubber/linux/kernel/sched/membarrier.c: 72
 * The writes to x and y are unordered by the hardware, so it's possible to /Users/rubber/linux/kernel/sched/membarrier.c: 74
 * have "r2 = 1" even though the write to x doesn't execute until (b).  If /Users/rubber/linux/kernel/sched/membarrier.c: 75
 * the memory barrier at (c) is omitted then "r1 = x" can be reordered /Users/rubber/linux/kernel/sched/membarrier.c: 76
 * before (b) (although not before (a)), so we get "r1 = 0".  This violates /Users/rubber/linux/kernel/sched/membarrier.c: 77
 * the guarantee that membarrier() is supposed to provide. /Users/rubber/linux/kernel/sched/membarrier.c: 78
 * The timing of the memory barrier at (c) has to ensure that it executes /Users/rubber/linux/kernel/sched/membarrier.c: 80
 * after the IPI-induced memory barrier on CPU1. /Users/rubber/linux/kernel/sched/membarrier.c: 81
 * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier /Users/rubber/linux/kernel/sched/membarrier.c: 83
 *           CPU0                            CPU1 /Users/rubber/linux/kernel/sched/membarrier.c: 85
 *           membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 87
 *           a: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 88
 *                                           d: switch to kthread (includes mb) /Users/rubber/linux/kernel/sched/membarrier.c: 89
 *           b: read rq->curr->mm == NULL /Users/rubber/linux/kernel/sched/membarrier.c: 90
 *                                           e: switch to user (includes mb) /Users/rubber/linux/kernel/sched/membarrier.c: 91
 *           c: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 92
 * Using the scenario from (A), we can show that (a) needs to be paired /Users/rubber/linux/kernel/sched/membarrier.c: 94
 * with (e). Using the scenario from (B), we can show that (c) needs to /Users/rubber/linux/kernel/sched/membarrier.c: 95
 * be paired with (d). /Users/rubber/linux/kernel/sched/membarrier.c: 96
 * D) exit_mm vs membarrier /Users/rubber/linux/kernel/sched/membarrier.c: 98
 * Two thread groups are created, A and B.  Thread group B is created by /Users/rubber/linux/kernel/sched/membarrier.c: 100
 * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD. /Users/rubber/linux/kernel/sched/membarrier.c: 101
 * Let's assume we have a single thread within each thread group (Thread A /Users/rubber/linux/kernel/sched/membarrier.c: 102
 * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1. /Users/rubber/linux/kernel/sched/membarrier.c: 103
 *           CPU0                            CPU1 /Users/rubber/linux/kernel/sched/membarrier.c: 105
 *           membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 107
 *             a: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 108
 *                                           exit_mm(): /Users/rubber/linux/kernel/sched/membarrier.c: 109
 *                                             d: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 110
 *                                             e: current->mm = NULL /Users/rubber/linux/kernel/sched/membarrier.c: 111
 *             b: read rq->curr->mm == NULL /Users/rubber/linux/kernel/sched/membarrier.c: 112
 *             c: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 113
 * Using scenario (B), we can show that (c) needs to be paired with (d). /Users/rubber/linux/kernel/sched/membarrier.c: 115
 * E) kthread_{use,unuse}_mm vs membarrier /Users/rubber/linux/kernel/sched/membarrier.c: 117
 *           CPU0                            CPU1 /Users/rubber/linux/kernel/sched/membarrier.c: 119
 *           membarrier(): /Users/rubber/linux/kernel/sched/membarrier.c: 121
 *           a: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 122
 *                                           kthread_unuse_mm() /Users/rubber/linux/kernel/sched/membarrier.c: 123
 *                                             d: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 124
 *                                             e: current->mm = NULL /Users/rubber/linux/kernel/sched/membarrier.c: 125
 *           b: read rq->curr->mm == NULL /Users/rubber/linux/kernel/sched/membarrier.c: 126
 *                                           kthread_use_mm() /Users/rubber/linux/kernel/sched/membarrier.c: 127
 *                                             f: current->mm = mm /Users/rubber/linux/kernel/sched/membarrier.c: 128
 *                                             g: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 129
 *           c: smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 130
 * Using the scenario from (A), we can show that (a) needs to be paired /Users/rubber/linux/kernel/sched/membarrier.c: 132
 * with (g). Using the scenario from (B), we can show that (c) needs to /Users/rubber/linux/kernel/sched/membarrier.c: 133
 * be paired with (d). /Users/rubber/linux/kernel/sched/membarrier.c: 134
 * Bitmask made from a "or" of all commands within enum membarrier_cmd, /Users/rubber/linux/kernel/sched/membarrier.c: 138
 * except MEMBARRIER_CMD_QUERY. /Users/rubber/linux/kernel/sched/membarrier.c: 139
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 171
	 * The smp_mb() in membarrier after all the IPIs is supposed to /Users/rubber/linux/kernel/sched/membarrier.c: 172
	 * ensure that memory on remote CPUs that occur before the IPI /Users/rubber/linux/kernel/sched/membarrier.c: 173
	 * become visible to membarrier()'s caller -- see scenario B in /Users/rubber/linux/kernel/sched/membarrier.c: 174
	 * the big comment at the top of this file. /Users/rubber/linux/kernel/sched/membarrier.c: 175
	 * /Users/rubber/linux/kernel/sched/membarrier.c: 176
	 * A sync_core() would provide this guarantee, but /Users/rubber/linux/kernel/sched/membarrier.c: 177
	 * sync_core_before_usermode() might end up being deferred until /Users/rubber/linux/kernel/sched/membarrier.c: 178
	 * after membarrier()'s smp_mb(). /Users/rubber/linux/kernel/sched/membarrier.c: 179
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 188
	 * Ensure that all stores done by the calling thread are visible /Users/rubber/linux/kernel/sched/membarrier.c: 189
	 * to the current task before the current task resumes.  We could /Users/rubber/linux/kernel/sched/membarrier.c: 190
	 * probably optimize this away on most architectures, but by the /Users/rubber/linux/kernel/sched/membarrier.c: 191
	 * time we've already sent an IPI, the cost of the extra smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 192
	 * is negligible. /Users/rubber/linux/kernel/sched/membarrier.c: 193
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 207
	 * Issue a memory barrier after setting /Users/rubber/linux/kernel/sched/membarrier.c: 208
	 * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to /Users/rubber/linux/kernel/sched/membarrier.c: 209
	 * guarantee that no memory access following registration is reordered /Users/rubber/linux/kernel/sched/membarrier.c: 210
	 * before registration. /Users/rubber/linux/kernel/sched/membarrier.c: 211
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 218
	 * Issue a memory barrier before clearing membarrier_state to /Users/rubber/linux/kernel/sched/membarrier.c: 219
	 * guarantee that no memory access prior to exec is reordered after /Users/rubber/linux/kernel/sched/membarrier.c: 220
	 * clearing this state. /Users/rubber/linux/kernel/sched/membarrier.c: 221
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 225
	 * Keep the runqueue membarrier_state in sync with this mm /Users/rubber/linux/kernel/sched/membarrier.c: 226
	 * membarrier_state. /Users/rubber/linux/kernel/sched/membarrier.c: 227
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 252
	 * Matches memory barriers around rq->curr modification in /Users/rubber/linux/kernel/sched/membarrier.c: 253
	 * scheduler. /Users/rubber/linux/kernel/sched/membarrier.c: 254
		/* /Users/rubber/linux/kernel/sched/membarrier.c: 266
		 * Skipping the current CPU is OK even through we can be /Users/rubber/linux/kernel/sched/membarrier.c: 267
		 * migrated at any point. The current CPU, at the point /Users/rubber/linux/kernel/sched/membarrier.c: 268
		 * where we read raw_smp_processor_id(), is ensured to /Users/rubber/linux/kernel/sched/membarrier.c: 269
		 * be in program order with respect to the caller /Users/rubber/linux/kernel/sched/membarrier.c: 270
		 * thread. Therefore, we can skip this CPU from the /Users/rubber/linux/kernel/sched/membarrier.c: 271
		 * iteration. /Users/rubber/linux/kernel/sched/membarrier.c: 272
		/* /Users/rubber/linux/kernel/sched/membarrier.c: 281
		 * Skip the CPU if it runs a kernel thread which is not using /Users/rubber/linux/kernel/sched/membarrier.c: 282
		 * a task mm. /Users/rubber/linux/kernel/sched/membarrier.c: 283
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 300
	 * Memory barrier on the caller thread _after_ we finished /Users/rubber/linux/kernel/sched/membarrier.c: 301
	 * waiting for the last IPI. Matches memory barriers around /Users/rubber/linux/kernel/sched/membarrier.c: 302
	 * rq->curr modification in scheduler. /Users/rubber/linux/kernel/sched/membarrier.c: 303
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 340
	 * Matches memory barriers around rq->curr modification in /Users/rubber/linux/kernel/sched/membarrier.c: 341
	 * scheduler. /Users/rubber/linux/kernel/sched/membarrier.c: 342
		/* /Users/rubber/linux/kernel/sched/membarrier.c: 378
		 * smp_call_function_single() will call ipi_func() if cpu_id /Users/rubber/linux/kernel/sched/membarrier.c: 379
		 * is the calling CPU. /Users/rubber/linux/kernel/sched/membarrier.c: 380
		/* /Users/rubber/linux/kernel/sched/membarrier.c: 384
		 * For regular membarrier, we can save a few cycles by /Users/rubber/linux/kernel/sched/membarrier.c: 385
		 * skipping the current cpu -- we're about to do smp_mb() /Users/rubber/linux/kernel/sched/membarrier.c: 386
		 * below, and if we migrate to a different cpu, this cpu /Users/rubber/linux/kernel/sched/membarrier.c: 387
		 * and the new cpu will execute a full barrier in the /Users/rubber/linux/kernel/sched/membarrier.c: 388
		 * scheduler. /Users/rubber/linux/kernel/sched/membarrier.c: 389
		 * /Users/rubber/linux/kernel/sched/membarrier.c: 390
		 * For SYNC_CORE, we do need a barrier on the current cpu -- /Users/rubber/linux/kernel/sched/membarrier.c: 391
		 * otherwise, if we are migrated and replaced by a different /Users/rubber/linux/kernel/sched/membarrier.c: 392
		 * task in the same mm just before, during, or after /Users/rubber/linux/kernel/sched/membarrier.c: 393
		 * membarrier, we will end up with some thread in the mm /Users/rubber/linux/kernel/sched/membarrier.c: 394
		 * running without a core sync. /Users/rubber/linux/kernel/sched/membarrier.c: 395
		 * /Users/rubber/linux/kernel/sched/membarrier.c: 396
		 * For RSEQ, don't rseq_preempt() the caller.  User code /Users/rubber/linux/kernel/sched/membarrier.c: 397
		 * is not supposed to issue syscalls at all from inside an /Users/rubber/linux/kernel/sched/membarrier.c: 398
		 * rseq critical section. /Users/rubber/linux/kernel/sched/membarrier.c: 399
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 415
	 * Memory barrier on the caller thread _after_ we finished /Users/rubber/linux/kernel/sched/membarrier.c: 416
	 * waiting for the last IPI. Matches memory barriers around /Users/rubber/linux/kernel/sched/membarrier.c: 417
	 * rq->curr modification in scheduler. /Users/rubber/linux/kernel/sched/membarrier.c: 418
		/* /Users/rubber/linux/kernel/sched/membarrier.c: 434
		 * For single mm user, we can simply issue a memory barrier /Users/rubber/linux/kernel/sched/membarrier.c: 435
		 * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the /Users/rubber/linux/kernel/sched/membarrier.c: 436
		 * mm and in the current runqueue to guarantee that no memory /Users/rubber/linux/kernel/sched/membarrier.c: 437
		 * access following registration is reordered before /Users/rubber/linux/kernel/sched/membarrier.c: 438
		 * registration. /Users/rubber/linux/kernel/sched/membarrier.c: 439
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 448
	 * For mm with multiple users, we need to ensure all future /Users/rubber/linux/kernel/sched/membarrier.c: 449
	 * scheduler executions will observe @mm's new membarrier /Users/rubber/linux/kernel/sched/membarrier.c: 450
	 * state. /Users/rubber/linux/kernel/sched/membarrier.c: 451
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 455
	 * For each cpu runqueue, if the task's mm match @mm, ensure that all /Users/rubber/linux/kernel/sched/membarrier.c: 456
	 * @mm's membarrier state set bits are also set in the runqueue's /Users/rubber/linux/kernel/sched/membarrier.c: 457
	 * membarrier state. This ensures that a runqueue scheduling /Users/rubber/linux/kernel/sched/membarrier.c: 458
	 * between threads which are users of @mm has its membarrier state /Users/rubber/linux/kernel/sched/membarrier.c: 459
	 * updated. /Users/rubber/linux/kernel/sched/membarrier.c: 460
	/* /Users/rubber/linux/kernel/sched/membarrier.c: 523
	 * We need to consider threads belonging to different thread /Users/rubber/linux/kernel/sched/membarrier.c: 524
	 * groups, which use the same mm. (CLONE_VM but not /Users/rubber/linux/kernel/sched/membarrier.c: 525
	 * CLONE_THREAD). /Users/rubber/linux/kernel/sched/membarrier.c: 526
 * sys_membarrier - issue memory barriers on a set of threads /Users/rubber/linux/kernel/sched/membarrier.c: 544
 * @cmd:    Takes command values defined in enum membarrier_cmd. /Users/rubber/linux/kernel/sched/membarrier.c: 545
 * @flags:  Currently needs to be 0 for all commands other than /Users/rubber/linux/kernel/sched/membarrier.c: 546
 *          MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ: in the latter /Users/rubber/linux/kernel/sched/membarrier.c: 547
 *          case it can be MEMBARRIER_CMD_FLAG_CPU, indicating that @cpu_id /Users/rubber/linux/kernel/sched/membarrier.c: 548
 *          contains the CPU on which to interrupt (= restart) /Users/rubber/linux/kernel/sched/membarrier.c: 549
 *          the RSEQ critical section. /Users/rubber/linux/kernel/sched/membarrier.c: 550
 * @cpu_id: if @flags == MEMBARRIER_CMD_FLAG_CPU, indicates the cpu on which /Users/rubber/linux/kernel/sched/membarrier.c: 551
 *          RSEQ CS should be interrupted (@cmd must be /Users/rubber/linux/kernel/sched/membarrier.c: 552
 *          MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ). /Users/rubber/linux/kernel/sched/membarrier.c: 553
 * If this system call is not implemented, -ENOSYS is returned. If the /Users/rubber/linux/kernel/sched/membarrier.c: 555
 * command specified does not exist, not available on the running /Users/rubber/linux/kernel/sched/membarrier.c: 556
 * kernel, or if the command argument is invalid, this system call /Users/rubber/linux/kernel/sched/membarrier.c: 557
 * returns -EINVAL. For a given command, with flags argument set to 0, /Users/rubber/linux/kernel/sched/membarrier.c: 558
 * if this system call returns -ENOSYS or -EINVAL, it is guaranteed to /Users/rubber/linux/kernel/sched/membarrier.c: 559
 * always return the same value until reboot. In addition, it can return /Users/rubber/linux/kernel/sched/membarrier.c: 560
 * -ENOMEM if there is not enough memory available to perform the system /Users/rubber/linux/kernel/sched/membarrier.c: 561
 * call. /Users/rubber/linux/kernel/sched/membarrier.c: 562
 * All memory accesses performed in program order from each targeted thread /Users/rubber/linux/kernel/sched/membarrier.c: 564
 * is guaranteed to be ordered with respect to sys_membarrier(). If we use /Users/rubber/linux/kernel/sched/membarrier.c: 565
 * the semantic "barrier()" to represent a compiler barrier forcing memory /Users/rubber/linux/kernel/sched/membarrier.c: 566
 * accesses to be performed in program order across the barrier, and /Users/rubber/linux/kernel/sched/membarrier.c: 567
 * smp_mb() to represent explicit memory barriers forcing full memory /Users/rubber/linux/kernel/sched/membarrier.c: 568
 * ordering across the barrier, we have the following ordering table for /Users/rubber/linux/kernel/sched/membarrier.c: 569
 * each pair of barrier(), sys_membarrier() and smp_mb(): /Users/rubber/linux/kernel/sched/membarrier.c: 570
 * The pair ordering is detailed as (O: ordered, X: not ordered): /Users/rubber/linux/kernel/sched/membarrier.c: 572
 *                        barrier()   smp_mb() sys_membarrier() /Users/rubber/linux/kernel/sched/membarrier.c: 574
 *        barrier()          X           X            O /Users/rubber/linux/kernel/sched/membarrier.c: 575
 *        smp_mb()           X           O            O /Users/rubber/linux/kernel/sched/membarrier.c: 576
 *        sys_membarrier()   O           O            O /Users/rubber/linux/kernel/sched/membarrier.c: 577
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/wait.c: 1
 * Generic waiting primitives. /Users/rubber/linux/kernel/sched/wait.c: 3
 * (C) 2004 Nadia Yvette Chambers, Oracle /Users/rubber/linux/kernel/sched/wait.c: 5
 * Scan threshold to break wait queue walk. /Users/rubber/linux/kernel/sched/wait.c: 62
 * This allows a waker to take a break from holding the /Users/rubber/linux/kernel/sched/wait.c: 63
 * wait queue lock during the wait queue walk. /Users/rubber/linux/kernel/sched/wait.c: 64
 * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just /Users/rubber/linux/kernel/sched/wait.c: 69
 * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve /Users/rubber/linux/kernel/sched/wait.c: 70
 * number) then we wake that number of exclusive tasks, and potentially all /Users/rubber/linux/kernel/sched/wait.c: 71
 * the non-exclusive tasks. Normally, exclusive tasks will be at the end of /Users/rubber/linux/kernel/sched/wait.c: 72
 * the list and any non-exclusive tasks will be woken first. A priority task /Users/rubber/linux/kernel/sched/wait.c: 73
 * may be at the head of the list, and can consume the event without any other /Users/rubber/linux/kernel/sched/wait.c: 74
 * tasks being woken. /Users/rubber/linux/kernel/sched/wait.c: 75
 * There are circumstances in which we can try to wake a task which has already /Users/rubber/linux/kernel/sched/wait.c: 77
 * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns /Users/rubber/linux/kernel/sched/wait.c: 78
 * zero in this (rare) case, and we handle it by continuing to scan the queue. /Users/rubber/linux/kernel/sched/wait.c: 79
 * __wake_up - wake up threads blocked on a waitqueue. /Users/rubber/linux/kernel/sched/wait.c: 145
 * @wq_head: the waitqueue /Users/rubber/linux/kernel/sched/wait.c: 146
 * @mode: which threads /Users/rubber/linux/kernel/sched/wait.c: 147
 * @nr_exclusive: how many wake-one or wake-many threads to wake up /Users/rubber/linux/kernel/sched/wait.c: 148
 * @key: is directly passed to the wakeup function /Users/rubber/linux/kernel/sched/wait.c: 149
 * If this function wakes up a task, it executes a full memory barrier before /Users/rubber/linux/kernel/sched/wait.c: 151
 * accessing the task state. /Users/rubber/linux/kernel/sched/wait.c: 152
 * Same as __wake_up but called with the spinlock in wait_queue_head_t held. /Users/rubber/linux/kernel/sched/wait.c: 162
 * __wake_up_sync_key - wake up threads blocked on a waitqueue. /Users/rubber/linux/kernel/sched/wait.c: 184
 * @wq_head: the waitqueue /Users/rubber/linux/kernel/sched/wait.c: 185
 * @mode: which threads /Users/rubber/linux/kernel/sched/wait.c: 186
 * @key: opaque value to be passed to wakeup targets /Users/rubber/linux/kernel/sched/wait.c: 187
 * The sync wakeup differs that the waker knows that it will schedule /Users/rubber/linux/kernel/sched/wait.c: 189
 * away soon, so while the target thread will be woken up, it will not /Users/rubber/linux/kernel/sched/wait.c: 190
 * be migrated to another CPU - ie. the two threads are 'synchronized' /Users/rubber/linux/kernel/sched/wait.c: 191
 * with each other. This can prevent needless bouncing between CPUs. /Users/rubber/linux/kernel/sched/wait.c: 192
 * On UP it can prevent extra preemption. /Users/rubber/linux/kernel/sched/wait.c: 194
 * If this function wakes up a task, it executes a full memory barrier before /Users/rubber/linux/kernel/sched/wait.c: 196
 * accessing the task state. /Users/rubber/linux/kernel/sched/wait.c: 197
 * __wake_up_locked_sync_key - wake up a thread blocked on a locked waitqueue. /Users/rubber/linux/kernel/sched/wait.c: 210
 * @wq_head: the waitqueue /Users/rubber/linux/kernel/sched/wait.c: 211
 * @mode: which threads /Users/rubber/linux/kernel/sched/wait.c: 212
 * @key: opaque value to be passed to wakeup targets /Users/rubber/linux/kernel/sched/wait.c: 213
 * The sync wakeup differs in that the waker knows that it will schedule /Users/rubber/linux/kernel/sched/wait.c: 215
 * away soon, so while the target thread will be woken up, it will not /Users/rubber/linux/kernel/sched/wait.c: 216
 * be migrated to another CPU - ie. the two threads are 'synchronized' /Users/rubber/linux/kernel/sched/wait.c: 217
 * with each other. This can prevent needless bouncing between CPUs. /Users/rubber/linux/kernel/sched/wait.c: 218
 * On UP it can prevent extra preemption. /Users/rubber/linux/kernel/sched/wait.c: 220
 * If this function wakes up a task, it executes a full memory barrier before /Users/rubber/linux/kernel/sched/wait.c: 222
 * accessing the task state. /Users/rubber/linux/kernel/sched/wait.c: 223
 * __wake_up_sync - see __wake_up_sync_key() /Users/rubber/linux/kernel/sched/wait.c: 233
 * Note: we use "set_current_state()" _after_ the wait-queue add, /Users/rubber/linux/kernel/sched/wait.c: 242
 * because we need a memory barrier there on SMP, so that any /Users/rubber/linux/kernel/sched/wait.c: 243
 * wake-function that tests for the wait-queue being active /Users/rubber/linux/kernel/sched/wait.c: 244
 * will be guaranteed to see waitqueue addition _or_ subsequent /Users/rubber/linux/kernel/sched/wait.c: 245
 * tests in this thread will see the wakeup having taken place. /Users/rubber/linux/kernel/sched/wait.c: 246
 * The spin_unlock() itself is semi-permeable and only protects /Users/rubber/linux/kernel/sched/wait.c: 248
 * one way (it only protects stuff inside the critical region and /Users/rubber/linux/kernel/sched/wait.c: 249
 * stops them from bleeding out - it would still allow subsequent /Users/rubber/linux/kernel/sched/wait.c: 250
 * loads to move into the critical region). /Users/rubber/linux/kernel/sched/wait.c: 251
		/* /Users/rubber/linux/kernel/sched/wait.c: 302
		 * Exclusive waiter must not fail if it was selected by wakeup, /Users/rubber/linux/kernel/sched/wait.c: 303
		 * it should "consume" the condition we were waiting for. /Users/rubber/linux/kernel/sched/wait.c: 304
		 * /Users/rubber/linux/kernel/sched/wait.c: 305
		 * The caller will recheck the condition and return success if /Users/rubber/linux/kernel/sched/wait.c: 306
		 * we were already woken up, we can not miss the event because /Users/rubber/linux/kernel/sched/wait.c: 307
		 * wakeup locks/unlocks the same wq_head->lock. /Users/rubber/linux/kernel/sched/wait.c: 308
		 * /Users/rubber/linux/kernel/sched/wait.c: 309
		 * But we need to ensure that set-condition + wakeup after that /Users/rubber/linux/kernel/sched/wait.c: 310
		 * can't see us, it should wake up another exclusive waiter if /Users/rubber/linux/kernel/sched/wait.c: 311
		 * we fail. /Users/rubber/linux/kernel/sched/wait.c: 312
 * Note! These two wait functions are entered with the /Users/rubber/linux/kernel/sched/wait.c: 332
 * wait-queue lock held (and interrupts off in the _irq /Users/rubber/linux/kernel/sched/wait.c: 333
 * case), so there is no race with testing the wakeup /Users/rubber/linux/kernel/sched/wait.c: 334
 * condition in the caller before they add the wait /Users/rubber/linux/kernel/sched/wait.c: 335
 * entry to the wake queue. /Users/rubber/linux/kernel/sched/wait.c: 336
 * finish_wait - clean up after waiting in a queue /Users/rubber/linux/kernel/sched/wait.c: 373
 * @wq_head: waitqueue waited on /Users/rubber/linux/kernel/sched/wait.c: 374
 * @wq_entry: wait descriptor /Users/rubber/linux/kernel/sched/wait.c: 375
 * Sets current thread back to running state and removes /Users/rubber/linux/kernel/sched/wait.c: 377
 * the wait descriptor from the given waitqueue if still /Users/rubber/linux/kernel/sched/wait.c: 378
 * queued. /Users/rubber/linux/kernel/sched/wait.c: 379
	/* /Users/rubber/linux/kernel/sched/wait.c: 386
	 * We can check for list emptiness outside the lock /Users/rubber/linux/kernel/sched/wait.c: 387
	 * IFF: /Users/rubber/linux/kernel/sched/wait.c: 388
	 *  - we use the "careful" check that verifies both /Users/rubber/linux/kernel/sched/wait.c: 389
	 *    the next and prev pointers, so that there cannot /Users/rubber/linux/kernel/sched/wait.c: 390
	 *    be any half-pending updates in progress on other /Users/rubber/linux/kernel/sched/wait.c: 391
	 *    CPU's that we haven't seen yet (and that might /Users/rubber/linux/kernel/sched/wait.c: 392
	 *    still change the stack area. /Users/rubber/linux/kernel/sched/wait.c: 393
	 * and /Users/rubber/linux/kernel/sched/wait.c: 394
	 *  - all other users take the lock (ie we can only /Users/rubber/linux/kernel/sched/wait.c: 395
	 *    have _one_ other CPU that looks at or modifies /Users/rubber/linux/kernel/sched/wait.c: 396
	 *    the list). /Users/rubber/linux/kernel/sched/wait.c: 397
 * DEFINE_WAIT_FUNC(wait, woken_wake_func); /Users/rubber/linux/kernel/sched/wait.c: 424
 * add_wait_queue(&wq_head, &wait); /Users/rubber/linux/kernel/sched/wait.c: 426
 * for (;;) { /Users/rubber/linux/kernel/sched/wait.c: 427
 *     if (condition) /Users/rubber/linux/kernel/sched/wait.c: 428
 *         break; /Users/rubber/linux/kernel/sched/wait.c: 429
 *     // in wait_woken()			// in woken_wake_function() /Users/rubber/linux/kernel/sched/wait.c: 431
 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN; /Users/rubber/linux/kernel/sched/wait.c: 433
 *     smp_mb(); // A				try_to_wake_up(): /Users/rubber/linux/kernel/sched/wait.c: 434
 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier> /Users/rubber/linux/kernel/sched/wait.c: 435
 *         schedule()				   if (p->state & mode) /Users/rubber/linux/kernel/sched/wait.c: 436
 *     p->state = TASK_RUNNING;			      p->state = TASK_RUNNING; /Users/rubber/linux/kernel/sched/wait.c: 437
 *     wq_entry->flags &= ~WQ_FLAG_WOKEN;	~~~~~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/sched/wait.c: 438
 *     smp_mb(); // B				condition = true; /Users/rubber/linux/kernel/sched/wait.c: 439
 * }						smp_mb(); // C /Users/rubber/linux/kernel/sched/wait.c: 440
 * remove_wait_queue(&wq_head, &wait);		wq_entry->flags |= WQ_FLAG_WOKEN; /Users/rubber/linux/kernel/sched/wait.c: 441
	/* /Users/rubber/linux/kernel/sched/wait.c: 445
	 * The below executes an smp_mb(), which matches with the full barrier /Users/rubber/linux/kernel/sched/wait.c: 446
	 * executed by the try_to_wake_up() in woken_wake_function() such that /Users/rubber/linux/kernel/sched/wait.c: 447
	 * either we see the store to wq_entry->flags in woken_wake_function() /Users/rubber/linux/kernel/sched/wait.c: 448
	 * or woken_wake_function() sees our store to current->state. /Users/rubber/linux/kernel/sched/wait.c: 449
	/* /Users/rubber/linux/kernel/sched/wait.c: 456
	 * The below executes an smp_mb(), which matches with the smp_mb() (C) /Users/rubber/linux/kernel/sched/wait.c: 457
	 * in woken_wake_function() such that either we see the wait condition /Users/rubber/linux/kernel/sched/wait.c: 458
	 * being true or the store to wq_entry->flags in woken_wake_function() /Users/rubber/linux/kernel/sched/wait.c: 459
	 * follows ours in the coherence order. /Users/rubber/linux/kernel/sched/wait.c: 460
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/fair.c: 1
 * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH) /Users/rubber/linux/kernel/sched/fair.c: 3
 *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/sched/fair.c: 5
 *  Interactivity improvements by Mike Galbraith /Users/rubber/linux/kernel/sched/fair.c: 7
 *  (C) 2007 Mike Galbraith <efault@gmx.de> /Users/rubber/linux/kernel/sched/fair.c: 8
 *  Various enhancements by Dmitry Adamushko. /Users/rubber/linux/kernel/sched/fair.c: 10
 *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com> /Users/rubber/linux/kernel/sched/fair.c: 11
 *  Group scheduling enhancements by Srivatsa Vaddagiri /Users/rubber/linux/kernel/sched/fair.c: 13
 *  Copyright IBM Corporation, 2007 /Users/rubber/linux/kernel/sched/fair.c: 14
 *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com> /Users/rubber/linux/kernel/sched/fair.c: 15
 *  Scaled math optimizations by Thomas Gleixner /Users/rubber/linux/kernel/sched/fair.c: 17
 *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/sched/fair.c: 18
 *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra /Users/rubber/linux/kernel/sched/fair.c: 20
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/sched/fair.c: 21
 * Targeted preemption latency for CPU-bound tasks: /Users/rubber/linux/kernel/sched/fair.c: 26
 * NOTE: this latency value is not the same as the concept of /Users/rubber/linux/kernel/sched/fair.c: 28
 * 'timeslice length' - timeslices in CFS are of variable length /Users/rubber/linux/kernel/sched/fair.c: 29
 * and have no persistent notion like in traditional, time-slice /Users/rubber/linux/kernel/sched/fair.c: 30
 * based scheduling concepts. /Users/rubber/linux/kernel/sched/fair.c: 31
 * (to see the precise effective timeslice length of your workload, /Users/rubber/linux/kernel/sched/fair.c: 33
 *  run vmstat and monitor the context-switches (cs) field) /Users/rubber/linux/kernel/sched/fair.c: 34
 * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds) /Users/rubber/linux/kernel/sched/fair.c: 36
 * The initial- and re-scaling of tunables is configurable /Users/rubber/linux/kernel/sched/fair.c: 42
 * Options are: /Users/rubber/linux/kernel/sched/fair.c: 44
 *   SCHED_TUNABLESCALING_NONE - unscaled, always *1 /Users/rubber/linux/kernel/sched/fair.c: 46
 *   SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus) /Users/rubber/linux/kernel/sched/fair.c: 47
 *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus /Users/rubber/linux/kernel/sched/fair.c: 48
 * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus)) /Users/rubber/linux/kernel/sched/fair.c: 50
 * Minimal preemption granularity for CPU-bound tasks: /Users/rubber/linux/kernel/sched/fair.c: 55
 * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds) /Users/rubber/linux/kernel/sched/fair.c: 57
 * Minimal preemption granularity for CPU-bound SCHED_IDLE tasks. /Users/rubber/linux/kernel/sched/fair.c: 63
 * Applies only when SCHED_IDLE tasks compete with normal tasks. /Users/rubber/linux/kernel/sched/fair.c: 64
 * (default: 0.75 msec) /Users/rubber/linux/kernel/sched/fair.c: 66
 * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity /Users/rubber/linux/kernel/sched/fair.c: 71
 * After fork, child runs first. If set to 0 (default) then /Users/rubber/linux/kernel/sched/fair.c: 76
 * parent will (try to) run first. /Users/rubber/linux/kernel/sched/fair.c: 77
 * SCHED_OTHER wake-up granularity. /Users/rubber/linux/kernel/sched/fair.c: 82
 * This option delays the preemption effects of decoupled workloads /Users/rubber/linux/kernel/sched/fair.c: 84
 * and reduces their over-scheduling. Synchronous workloads will still /Users/rubber/linux/kernel/sched/fair.c: 85
 * have immediate wakeup/sleep latencies. /Users/rubber/linux/kernel/sched/fair.c: 86
 * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds) /Users/rubber/linux/kernel/sched/fair.c: 88
 * For asym packing, by default the lower numbered CPU has higher priority. /Users/rubber/linux/kernel/sched/fair.c: 110
 * The margin used when comparing utilization with CPU capacity. /Users/rubber/linux/kernel/sched/fair.c: 118
 * (default: ~20%) /Users/rubber/linux/kernel/sched/fair.c: 120
 * The margin used when comparing CPU capacities. /Users/rubber/linux/kernel/sched/fair.c: 125
 * is 'cap1' noticeably greater than 'cap2' /Users/rubber/linux/kernel/sched/fair.c: 126
 * (default: ~5%) /Users/rubber/linux/kernel/sched/fair.c: 128
 * Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool /Users/rubber/linux/kernel/sched/fair.c: 135
 * each time a cfs_rq requests quota. /Users/rubber/linux/kernel/sched/fair.c: 136
 * Note: in the case that the slice exceeds the runtime remaining (either due /Users/rubber/linux/kernel/sched/fair.c: 138
 * to consumption or the quota being specified to be smaller than the slice) /Users/rubber/linux/kernel/sched/fair.c: 139
 * we will always only issue the remaining available time. /Users/rubber/linux/kernel/sched/fair.c: 140
 * (default: 5 msec, units: microseconds) /Users/rubber/linux/kernel/sched/fair.c: 142
 * Increase the granularity value when there are more CPUs, /Users/rubber/linux/kernel/sched/fair.c: 166
 * because with more CPUs the 'effective latency' as visible /Users/rubber/linux/kernel/sched/fair.c: 167
 * to users decreases. But the relationship is not linear, /Users/rubber/linux/kernel/sched/fair.c: 168
 * so pick a second-best guess by going with the log2 of the /Users/rubber/linux/kernel/sched/fair.c: 169
 * number of CPUs. /Users/rubber/linux/kernel/sched/fair.c: 170
 * This idea comes from the SD scheduler of Con Kolivas: /Users/rubber/linux/kernel/sched/fair.c: 172
 * delta_exec * weight / lw.weight /Users/rubber/linux/kernel/sched/fair.c: 233
 *   OR /Users/rubber/linux/kernel/sched/fair.c: 234
 * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT /Users/rubber/linux/kernel/sched/fair.c: 235
 * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case /Users/rubber/linux/kernel/sched/fair.c: 237
 * we're guaranteed shift stays positive because inv_weight is guaranteed to /Users/rubber/linux/kernel/sched/fair.c: 238
 * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22. /Users/rubber/linux/kernel/sched/fair.c: 239
 * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus /Users/rubber/linux/kernel/sched/fair.c: 241
 * weight/lw.weight <= 1, and therefore our shift will also be positive. /Users/rubber/linux/kernel/sched/fair.c: 242
 * CFS operations on generic schedulable entities: /Users/rubber/linux/kernel/sched/fair.c: 275
	/* /Users/rubber/linux/kernel/sched/fair.c: 307
	 * Ensure we either appear before our parent (if already /Users/rubber/linux/kernel/sched/fair.c: 308
	 * enqueued) or force our parent to appear after us when it is /Users/rubber/linux/kernel/sched/fair.c: 309
	 * enqueued. The fact that we always enqueue bottom-up /Users/rubber/linux/kernel/sched/fair.c: 310
	 * reduces this to two cases and a special case for the root /Users/rubber/linux/kernel/sched/fair.c: 311
	 * cfs_rq. Furthermore, it also means that we will always reset /Users/rubber/linux/kernel/sched/fair.c: 312
	 * tmp_alone_branch either when the branch is connected /Users/rubber/linux/kernel/sched/fair.c: 313
	 * to a tree or when we reach the top of the tree /Users/rubber/linux/kernel/sched/fair.c: 314
		/* /Users/rubber/linux/kernel/sched/fair.c: 318
		 * If parent is already on the list, we add the child /Users/rubber/linux/kernel/sched/fair.c: 319
		 * just before. Thanks to circular linked property of /Users/rubber/linux/kernel/sched/fair.c: 320
		 * the list, this means to put the child at the tail /Users/rubber/linux/kernel/sched/fair.c: 321
		 * of the list that starts by parent. /Users/rubber/linux/kernel/sched/fair.c: 322
		/* /Users/rubber/linux/kernel/sched/fair.c: 326
		 * The branch is now connected to its tree so we can /Users/rubber/linux/kernel/sched/fair.c: 327
		 * reset tmp_alone_branch to the beginning of the /Users/rubber/linux/kernel/sched/fair.c: 328
		 * list. /Users/rubber/linux/kernel/sched/fair.c: 329
		/* /Users/rubber/linux/kernel/sched/fair.c: 336
		 * cfs rq without parent should be put /Users/rubber/linux/kernel/sched/fair.c: 337
		 * at the tail of the list. /Users/rubber/linux/kernel/sched/fair.c: 338
		/* /Users/rubber/linux/kernel/sched/fair.c: 342
		 * We have reach the top of a tree so we can reset /Users/rubber/linux/kernel/sched/fair.c: 343
		 * tmp_alone_branch to the beginning of the list. /Users/rubber/linux/kernel/sched/fair.c: 344
	/* /Users/rubber/linux/kernel/sched/fair.c: 350
	 * The parent has not already been added so we want to /Users/rubber/linux/kernel/sched/fair.c: 351
	 * make sure that it will be put after us. /Users/rubber/linux/kernel/sched/fair.c: 352
	 * tmp_alone_branch points to the begin of the branch /Users/rubber/linux/kernel/sched/fair.c: 353
	 * where we will add parent. /Users/rubber/linux/kernel/sched/fair.c: 354
	/* /Users/rubber/linux/kernel/sched/fair.c: 357
	 * update tmp_alone_branch to points to the new begin /Users/rubber/linux/kernel/sched/fair.c: 358
	 * of the branch /Users/rubber/linux/kernel/sched/fair.c: 359
		/* /Users/rubber/linux/kernel/sched/fair.c: 370
		 * With cfs_rq being unthrottled/throttled during an enqueue, /Users/rubber/linux/kernel/sched/fair.c: 371
		 * it can happen the tmp_alone_branch points the a leaf that /Users/rubber/linux/kernel/sched/fair.c: 372
		 * we finally want to del. In this case, tmp_alone_branch moves /Users/rubber/linux/kernel/sched/fair.c: 373
		 * to the prev element but it will point to rq->leaf_cfs_rq_list /Users/rubber/linux/kernel/sched/fair.c: 374
		 * at the end of the enqueue. /Users/rubber/linux/kernel/sched/fair.c: 375
	/* /Users/rubber/linux/kernel/sched/fair.c: 415
	 * preemption test can be made between sibling entities who are in the /Users/rubber/linux/kernel/sched/fair.c: 416
	 * same cfs_rq i.e who have a common parent. Walk up the hierarchy of /Users/rubber/linux/kernel/sched/fair.c: 417
	 * both tasks until we find their ancestors who are siblings of common /Users/rubber/linux/kernel/sched/fair.c: 418
	 * parent. /Users/rubber/linux/kernel/sched/fair.c: 419
 * Scheduling class tree data structure manipulation methods: /Users/rubber/linux/kernel/sched/fair.c: 517
 * Enqueue an entity into the rb-tree: /Users/rubber/linux/kernel/sched/fair.c: 584
 * Scheduling class statistics methods: /Users/rubber/linux/kernel/sched/fair.c: 628
 * delta /= w /Users/rubber/linux/kernel/sched/fair.c: 650
 * The idea is to set a period in which each task runs once. /Users/rubber/linux/kernel/sched/fair.c: 661
 * When there are too many tasks (sched_nr_latency) we have to stretch /Users/rubber/linux/kernel/sched/fair.c: 663
 * this period because otherwise the slices get too small. /Users/rubber/linux/kernel/sched/fair.c: 664
 * p = (nr <= nl) ? l : l*nr/nl /Users/rubber/linux/kernel/sched/fair.c: 666
 * We calculate the wall-time slice from the period by taking a part /Users/rubber/linux/kernel/sched/fair.c: 679
 * proportional to the weight. /Users/rubber/linux/kernel/sched/fair.c: 680
 * s = p*P[w/rw] /Users/rubber/linux/kernel/sched/fair.c: 682
 * We calculate the vruntime slice of a to-be-inserted task. /Users/rubber/linux/kernel/sched/fair.c: 726
 * vs = s/w /Users/rubber/linux/kernel/sched/fair.c: 728
	/* /Users/rubber/linux/kernel/sched/fair.c: 749
	 * Tasks are initialized with full load to be seen as heavy tasks until /Users/rubber/linux/kernel/sched/fair.c: 750
	 * they get a chance to stabilize to their real load level. /Users/rubber/linux/kernel/sched/fair.c: 751
	 * Group entities are initialized with zero load to reflect the fact that /Users/rubber/linux/kernel/sched/fair.c: 752
	 * nothing has been attached to the task group yet. /Users/rubber/linux/kernel/sched/fair.c: 753
 * With new tasks being created, their initial util_avgs are extrapolated /Users/rubber/linux/kernel/sched/fair.c: 764
 * based on the cfs_rq's current util_avg: /Users/rubber/linux/kernel/sched/fair.c: 765
 *   util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight /Users/rubber/linux/kernel/sched/fair.c: 767
 * However, in many cases, the above util_avg does not give a desired /Users/rubber/linux/kernel/sched/fair.c: 769
 * value. Moreover, the sum of the util_avgs may be divergent, such /Users/rubber/linux/kernel/sched/fair.c: 770
 * as when the series is a harmonic series. /Users/rubber/linux/kernel/sched/fair.c: 771
 * To solve this problem, we also cap the util_avg of successive tasks to /Users/rubber/linux/kernel/sched/fair.c: 773
 * only 1/2 of the left utilization budget: /Users/rubber/linux/kernel/sched/fair.c: 774
 *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n /Users/rubber/linux/kernel/sched/fair.c: 776
 * where n denotes the nth task and cpu_scale the CPU capacity. /Users/rubber/linux/kernel/sched/fair.c: 778
 * For example, for a CPU with 1024 of capacity, a simplest series from /Users/rubber/linux/kernel/sched/fair.c: 780
 * the beginning would be like: /Users/rubber/linux/kernel/sched/fair.c: 781
 *  task  util_avg: 512, 256, 128,  64,  32,   16,    8, ... /Users/rubber/linux/kernel/sched/fair.c: 783
 * cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ... /Users/rubber/linux/kernel/sched/fair.c: 784
 * Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap) /Users/rubber/linux/kernel/sched/fair.c: 786
 * if util_avg > util_avg_cap. /Users/rubber/linux/kernel/sched/fair.c: 787
		/* /Users/rubber/linux/kernel/sched/fair.c: 812
		 * For !fair tasks do: /Users/rubber/linux/kernel/sched/fair.c: 813
		 * /Users/rubber/linux/kernel/sched/fair.c: 814
		update_cfs_rq_load_avg(now, cfs_rq); /Users/rubber/linux/kernel/sched/fair.c: 815
		attach_entity_load_avg(cfs_rq, se); /Users/rubber/linux/kernel/sched/fair.c: 816
		switched_from_fair(rq, p); /Users/rubber/linux/kernel/sched/fair.c: 817
		 * /Users/rubber/linux/kernel/sched/fair.c: 818
		 * such that the next switched_to_fair() has the /Users/rubber/linux/kernel/sched/fair.c: 819
		 * expected state. /Users/rubber/linux/kernel/sched/fair.c: 820
 * Update the current task's runtime statistics. /Users/rubber/linux/kernel/sched/fair.c: 842
	/* /Users/rubber/linux/kernel/sched/fair.c: 917
	 * When the sched_schedstat changes from 0 to 1, some sched se /Users/rubber/linux/kernel/sched/fair.c: 918
	 * maybe already in the runqueue, the se->statistics.wait_start /Users/rubber/linux/kernel/sched/fair.c: 919
	 * will be 0.So it will let the delta wrong. We need to avoid this /Users/rubber/linux/kernel/sched/fair.c: 920
	 * scenario. /Users/rubber/linux/kernel/sched/fair.c: 921
 * Task is being enqueued - update stats: /Users/rubber/linux/kernel/sched/fair.c: 950
	/* /Users/rubber/linux/kernel/sched/fair.c: 958
	 * Are we enqueueing a waiting task? (for current tasks /Users/rubber/linux/kernel/sched/fair.c: 959
	 * a dequeue/enqueue event is a NOP) /Users/rubber/linux/kernel/sched/fair.c: 960
	/* /Users/rubber/linux/kernel/sched/fair.c: 976
	 * Mark the end of the wait period if dequeueing a /Users/rubber/linux/kernel/sched/fair.c: 977
	 * waiting task: /Users/rubber/linux/kernel/sched/fair.c: 978
 * We are picking a new current task - update its stats: /Users/rubber/linux/kernel/sched/fair.c: 999
	/* /Users/rubber/linux/kernel/sched/fair.c: 1004
	 * We are starting a new run period: /Users/rubber/linux/kernel/sched/fair.c: 1005
 * Scheduling class queueing methods: /Users/rubber/linux/kernel/sched/fair.c: 1011
 * Approximate time to scan a full NUMA task in ms. The task scan period is /Users/rubber/linux/kernel/sched/fair.c: 1016
 * calculated based on the tasks virtual memory size and /Users/rubber/linux/kernel/sched/fair.c: 1017
 * numa_balancing_scan_size. /Users/rubber/linux/kernel/sched/fair.c: 1018
	/* /Users/rubber/linux/kernel/sched/fair.c: 1040
	 * faults[] array is split into two regions: faults_mem and faults_cpu. /Users/rubber/linux/kernel/sched/fair.c: 1041
	 * /Users/rubber/linux/kernel/sched/fair.c: 1042
	 * Faults_cpu is used to decide whether memory should move /Users/rubber/linux/kernel/sched/fair.c: 1043
	 * towards the CPU. As a consequence, these stats are weighted /Users/rubber/linux/kernel/sched/fair.c: 1044
	 * more by CPU use than by memory faults. /Users/rubber/linux/kernel/sched/fair.c: 1045
 * For functions that can be called in multiple contexts that permit reading /Users/rubber/linux/kernel/sched/fair.c: 1051
 * ->numa_group (see struct task_struct for locking rules). /Users/rubber/linux/kernel/sched/fair.c: 1052
	/* /Users/rubber/linux/kernel/sched/fair.c: 1073
	 * Calculations based on RSS as non-present and empty pages are skipped /Users/rubber/linux/kernel/sched/fair.c: 1074
	 * by the PTE scanner and NUMA hinting faults should be trapped based /Users/rubber/linux/kernel/sched/fair.c: 1075
	 * on resident pages /Users/rubber/linux/kernel/sched/fair.c: 1076
 * The averaged statistics, shared & private, memory & CPU, /Users/rubber/linux/kernel/sched/fair.c: 1188
 * occupy the first half of the array. The second half of the /Users/rubber/linux/kernel/sched/fair.c: 1189
 * array is for current counters, which are averaged into the /Users/rubber/linux/kernel/sched/fair.c: 1190
 * first set by task_numa_placement. /Users/rubber/linux/kernel/sched/fair.c: 1191
 * A node triggering more than 1/3 as many NUMA faults as the maximum is /Users/rubber/linux/kernel/sched/fair.c: 1249
 * considered part of a numa group's pseudo-interleaving set. Migrations /Users/rubber/linux/kernel/sched/fair.c: 1250
 * between these nodes are slowed down, to allow things to settle down. /Users/rubber/linux/kernel/sched/fair.c: 1251
	/* /Users/rubber/linux/kernel/sched/fair.c: 1267
	 * All nodes are directly connected, and the same distance /Users/rubber/linux/kernel/sched/fair.c: 1268
	 * from each other. No need for fancy placement algorithms. /Users/rubber/linux/kernel/sched/fair.c: 1269
	/* /Users/rubber/linux/kernel/sched/fair.c: 1274
	 * This code is called for each node, introducing N^2 complexity, /Users/rubber/linux/kernel/sched/fair.c: 1275
	 * which should be ok given the number of nodes rarely exceeds 8. /Users/rubber/linux/kernel/sched/fair.c: 1276
		/* /Users/rubber/linux/kernel/sched/fair.c: 1282
		 * The furthest away nodes in the system are not interesting /Users/rubber/linux/kernel/sched/fair.c: 1283
		 * for placement; nid was already counted. /Users/rubber/linux/kernel/sched/fair.c: 1284
		/* /Users/rubber/linux/kernel/sched/fair.c: 1289
		 * On systems with a backplane NUMA topology, compare groups /Users/rubber/linux/kernel/sched/fair.c: 1290
		 * of nodes, and move tasks towards the group with the most /Users/rubber/linux/kernel/sched/fair.c: 1291
		 * memory accesses. When comparing two nodes at distance /Users/rubber/linux/kernel/sched/fair.c: 1292
		 * "hoplimit", only nodes closer by than "hoplimit" are part /Users/rubber/linux/kernel/sched/fair.c: 1293
		 * of each group. Skip other nodes. /Users/rubber/linux/kernel/sched/fair.c: 1294
		/* /Users/rubber/linux/kernel/sched/fair.c: 1306
		 * On systems with a glueless mesh NUMA topology, there are /Users/rubber/linux/kernel/sched/fair.c: 1307
		 * no fixed "groups of nodes". Instead, nodes that are not /Users/rubber/linux/kernel/sched/fair.c: 1308
		 * directly connected bounce traffic through intermediate /Users/rubber/linux/kernel/sched/fair.c: 1309
		 * nodes; a numa_group can occupy any set of nodes. /Users/rubber/linux/kernel/sched/fair.c: 1310
		 * The further away a node is, the less the faults count. /Users/rubber/linux/kernel/sched/fair.c: 1311
		 * This seems to result in good task placement. /Users/rubber/linux/kernel/sched/fair.c: 1312
 * These return the fraction of accesses done by a particular task, or /Users/rubber/linux/kernel/sched/fair.c: 1326
 * task group, on a particular numa node.  The group weight is given a /Users/rubber/linux/kernel/sched/fair.c: 1327
 * larger multiplier, in order to group tasks together that are almost /Users/rubber/linux/kernel/sched/fair.c: 1328
 * evenly spread out between numa nodes. /Users/rubber/linux/kernel/sched/fair.c: 1329
	/* /Users/rubber/linux/kernel/sched/fair.c: 1380
	 * Allow first faults or private faults to migrate immediately early in /Users/rubber/linux/kernel/sched/fair.c: 1381
	 * the lifetime of a task. The magic number 4 is based on waiting for /Users/rubber/linux/kernel/sched/fair.c: 1382
	 * two full passes of the "multi-stage node selection" test that is /Users/rubber/linux/kernel/sched/fair.c: 1383
	 * executed below. /Users/rubber/linux/kernel/sched/fair.c: 1384
	/* /Users/rubber/linux/kernel/sched/fair.c: 1390
	 * Multi-stage node selection is used in conjunction with a periodic /Users/rubber/linux/kernel/sched/fair.c: 1391
	 * migration fault to build a temporal task<->page relation. By using /Users/rubber/linux/kernel/sched/fair.c: 1392
	 * a two-stage filter we remove short/unlikely relations. /Users/rubber/linux/kernel/sched/fair.c: 1393
	 * /Users/rubber/linux/kernel/sched/fair.c: 1394
	 * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate /Users/rubber/linux/kernel/sched/fair.c: 1395
	 * a task's usage of a particular page (n_p) per total usage of this /Users/rubber/linux/kernel/sched/fair.c: 1396
	 * page (n_t) (in a given time-span) to a probability. /Users/rubber/linux/kernel/sched/fair.c: 1397
	 * /Users/rubber/linux/kernel/sched/fair.c: 1398
	 * Our periodic faults will sample this probability and getting the /Users/rubber/linux/kernel/sched/fair.c: 1399
	 * same result twice in a row, given these samples are fully /Users/rubber/linux/kernel/sched/fair.c: 1400
	 * independent, is then given by P(n)^2, provided our sample period /Users/rubber/linux/kernel/sched/fair.c: 1401
	 * is sufficiently short compared to the usage pattern. /Users/rubber/linux/kernel/sched/fair.c: 1402
	 * /Users/rubber/linux/kernel/sched/fair.c: 1403
	 * This quadric squishes small probabilities, making it less likely we /Users/rubber/linux/kernel/sched/fair.c: 1404
	 * act on an unlikely task<->page relation. /Users/rubber/linux/kernel/sched/fair.c: 1405
	/* /Users/rubber/linux/kernel/sched/fair.c: 1419
	 * Destination node is much more heavily used than the source /Users/rubber/linux/kernel/sched/fair.c: 1420
	 * node? Allow migration. /Users/rubber/linux/kernel/sched/fair.c: 1421
	/* /Users/rubber/linux/kernel/sched/fair.c: 1427
	 * Distribute memory according to CPU & memory use on each node, /Users/rubber/linux/kernel/sched/fair.c: 1428
	 * with 3/4 hysteresis to avoid unnecessary memory migrations: /Users/rubber/linux/kernel/sched/fair.c: 1429
	 * /Users/rubber/linux/kernel/sched/fair.c: 1430
	 * faults_cpu(dst)   3   faults_cpu(src) /Users/rubber/linux/kernel/sched/fair.c: 1431
	 * --------------- * - > --------------- /Users/rubber/linux/kernel/sched/fair.c: 1432
	 * faults_mem(dst)   4   faults_mem(src) /Users/rubber/linux/kernel/sched/fair.c: 1433
 * 'numa_type' describes the node at the moment of load balancing. /Users/rubber/linux/kernel/sched/fair.c: 1440
	/* /Users/rubber/linux/kernel/sched/fair.c: 1445
	 * The node is fully used and the tasks don't compete for more CPU /Users/rubber/linux/kernel/sched/fair.c: 1446
	 * cycles. Nevertheless, some tasks might wait before running. /Users/rubber/linux/kernel/sched/fair.c: 1447
	/* /Users/rubber/linux/kernel/sched/fair.c: 1450
	 * The node is overloaded and can't provide expected CPU cycles to all /Users/rubber/linux/kernel/sched/fair.c: 1451
	 * tasks. /Users/rubber/linux/kernel/sched/fair.c: 1452
	/* /Users/rubber/linux/kernel/sched/fair.c: 1535
	 * Prefer cores instead of packing HT siblings /Users/rubber/linux/kernel/sched/fair.c: 1536
	 * and triggering future load balancing. /Users/rubber/linux/kernel/sched/fair.c: 1537
 * Gather all necessary information to make NUMA balancing placement /Users/rubber/linux/kernel/sched/fair.c: 1552
 * decisions that are compatible with standard load balancer. This /Users/rubber/linux/kernel/sched/fair.c: 1553
 * borrows code and logic from update_sg_lb_stats but sharing a /Users/rubber/linux/kernel/sched/fair.c: 1554
 * common implementation is impractical. /Users/rubber/linux/kernel/sched/fair.c: 1555
	/* /Users/rubber/linux/kernel/sched/fair.c: 1625
	 * Clear previous best_cpu/rq numa-migrate flag, since task now /Users/rubber/linux/kernel/sched/fair.c: 1626
	 * found a better CPU to move/swap. /Users/rubber/linux/kernel/sched/fair.c: 1627
	/* /Users/rubber/linux/kernel/sched/fair.c: 1651
	 * The load is corrected for the CPU capacity available on each node. /Users/rubber/linux/kernel/sched/fair.c: 1652
	 * /Users/rubber/linux/kernel/sched/fair.c: 1653
	 * src_load        dst_load /Users/rubber/linux/kernel/sched/fair.c: 1654
	 * ------------ vs --------- /Users/rubber/linux/kernel/sched/fair.c: 1655
	 * src_capacity    dst_capacity /Users/rubber/linux/kernel/sched/fair.c: 1656
 * Maximum NUMA importance can be 1998 (2*999); /Users/rubber/linux/kernel/sched/fair.c: 1673
 * SMALLIMP @ 30 would be close to 1998/64. /Users/rubber/linux/kernel/sched/fair.c: 1674
 * Used to deter task migration. /Users/rubber/linux/kernel/sched/fair.c: 1675
 * This checks if the overall compute and NUMA accesses of the system would /Users/rubber/linux/kernel/sched/fair.c: 1680
 * be improved if the source tasks was migrated to the target dst_cpu taking /Users/rubber/linux/kernel/sched/fair.c: 1681
 * into account that it might be best if task running on the dst_cpu should /Users/rubber/linux/kernel/sched/fair.c: 1682
 * be exchanged with the source task /Users/rubber/linux/kernel/sched/fair.c: 1683
	/* /Users/rubber/linux/kernel/sched/fair.c: 1706
	 * Because we have preemption enabled we can get migrated around and /Users/rubber/linux/kernel/sched/fair.c: 1707
	 * end try selecting ourselves (current == env->p) as a swap candidate. /Users/rubber/linux/kernel/sched/fair.c: 1708
	/* /Users/rubber/linux/kernel/sched/fair.c: 1726
	 * Skip this swap candidate if it is not moving to its preferred /Users/rubber/linux/kernel/sched/fair.c: 1727
	 * node and the best task is. /Users/rubber/linux/kernel/sched/fair.c: 1728
	/* /Users/rubber/linux/kernel/sched/fair.c: 1736
	 * "imp" is the fault differential for the source task between the /Users/rubber/linux/kernel/sched/fair.c: 1737
	 * source and destination node. Calculate the total differential for /Users/rubber/linux/kernel/sched/fair.c: 1738
	 * the source task and potential destination task. The more negative /Users/rubber/linux/kernel/sched/fair.c: 1739
	 * the value is, the more remote accesses that would be expected to /Users/rubber/linux/kernel/sched/fair.c: 1740
	 * be incurred if the tasks were swapped. /Users/rubber/linux/kernel/sched/fair.c: 1741
	 * /Users/rubber/linux/kernel/sched/fair.c: 1742
	 * If dst and source tasks are in the same NUMA group, or not /Users/rubber/linux/kernel/sched/fair.c: 1743
	 * in any group then look only at task weights. /Users/rubber/linux/kernel/sched/fair.c: 1744
		/* /Users/rubber/linux/kernel/sched/fair.c: 1750
		 * Add some hysteresis to prevent swapping the /Users/rubber/linux/kernel/sched/fair.c: 1751
		 * tasks within a group over tiny differences. /Users/rubber/linux/kernel/sched/fair.c: 1752
		/* /Users/rubber/linux/kernel/sched/fair.c: 1757
		 * Compare the group weights. If a task is all by itself /Users/rubber/linux/kernel/sched/fair.c: 1758
		 * (not part of a group), use the task weight instead. /Users/rubber/linux/kernel/sched/fair.c: 1759
	/* /Users/rubber/linux/kernel/sched/fair.c: 1773
	 * Encourage picking a task that moves to its preferred node. /Users/rubber/linux/kernel/sched/fair.c: 1774
	 * This potentially makes imp larger than it's maximum of /Users/rubber/linux/kernel/sched/fair.c: 1775
	 * 1998 (see SMALLIMP and task_weight for why) but in this /Users/rubber/linux/kernel/sched/fair.c: 1776
	 * case, it does not matter. /Users/rubber/linux/kernel/sched/fair.c: 1777
	/* /Users/rubber/linux/kernel/sched/fair.c: 1788
	 * Prefer swapping with a task moving to its preferred node over a /Users/rubber/linux/kernel/sched/fair.c: 1789
	 * task that is not. /Users/rubber/linux/kernel/sched/fair.c: 1790
	/* /Users/rubber/linux/kernel/sched/fair.c: 1797
	 * If the NUMA importance is less than SMALLIMP, /Users/rubber/linux/kernel/sched/fair.c: 1798
	 * task migration might only result in ping pong /Users/rubber/linux/kernel/sched/fair.c: 1799
	 * of tasks and also hurt performance due to cache /Users/rubber/linux/kernel/sched/fair.c: 1800
	 * misses. /Users/rubber/linux/kernel/sched/fair.c: 1801
	/* /Users/rubber/linux/kernel/sched/fair.c: 1806
	 * In the overloaded case, try and keep the load balanced. /Users/rubber/linux/kernel/sched/fair.c: 1807
		/* /Users/rubber/linux/kernel/sched/fair.c: 1828
		 * If the CPU is no longer truly idle and the previous best CPU /Users/rubber/linux/kernel/sched/fair.c: 1829
		 * is, keep using it. /Users/rubber/linux/kernel/sched/fair.c: 1830
	/* /Users/rubber/linux/kernel/sched/fair.c: 1842
	 * If a move to idle is allowed because there is capacity or load /Users/rubber/linux/kernel/sched/fair.c: 1843
	 * balance improves then stop the search. While a better swap /Users/rubber/linux/kernel/sched/fair.c: 1844
	 * candidate may exist, a search is not free. /Users/rubber/linux/kernel/sched/fair.c: 1845
	/* /Users/rubber/linux/kernel/sched/fair.c: 1850
	 * If a swap candidate must be identified and the current best task /Users/rubber/linux/kernel/sched/fair.c: 1851
	 * moves its preferred node then stop the search. /Users/rubber/linux/kernel/sched/fair.c: 1852
	/* /Users/rubber/linux/kernel/sched/fair.c: 1870
	 * If dst node has spare capacity, then check if there is an /Users/rubber/linux/kernel/sched/fair.c: 1871
	 * imbalance that would be overruled by the load balancer. /Users/rubber/linux/kernel/sched/fair.c: 1872
		/* /Users/rubber/linux/kernel/sched/fair.c: 1878
		 * Would movement cause an imbalance? Note that if src has /Users/rubber/linux/kernel/sched/fair.c: 1879
		 * more running tasks that the imbalance is ignored as the /Users/rubber/linux/kernel/sched/fair.c: 1880
		 * move improves the imbalance from the perspective of the /Users/rubber/linux/kernel/sched/fair.c: 1881
		 * CPU load balancer. /Users/rubber/linux/kernel/sched/fair.c: 1882
		/* /Users/rubber/linux/kernel/sched/fair.c: 1901
		 * If the improvement from just moving env->p direction is better /Users/rubber/linux/kernel/sched/fair.c: 1902
		 * than swapping tasks around, check if a move is possible. /Users/rubber/linux/kernel/sched/fair.c: 1903
	/* /Users/rubber/linux/kernel/sched/fair.c: 1943
	 * Pick the lowest SD_NUMA domain, as that would have the smallest /Users/rubber/linux/kernel/sched/fair.c: 1944
	 * imbalance and would be the first to start moving tasks about. /Users/rubber/linux/kernel/sched/fair.c: 1945
	 * /Users/rubber/linux/kernel/sched/fair.c: 1946
	 * And we want to avoid any moving of tasks about, as that would create /Users/rubber/linux/kernel/sched/fair.c: 1947
	 * random movement of tasks -- counter the numa conditions we're trying /Users/rubber/linux/kernel/sched/fair.c: 1948
	 * to satisfy here. /Users/rubber/linux/kernel/sched/fair.c: 1949
	/* /Users/rubber/linux/kernel/sched/fair.c: 1957
	 * Cpusets can break the scheduler domain tree into smaller /Users/rubber/linux/kernel/sched/fair.c: 1958
	 * balance domains, some of which do not cross NUMA boundaries. /Users/rubber/linux/kernel/sched/fair.c: 1959
	 * Tasks that are "trapped" in such domains cannot be migrated /Users/rubber/linux/kernel/sched/fair.c: 1960
	 * elsewhere, so there is no point in (re)trying. /Users/rubber/linux/kernel/sched/fair.c: 1961
	/* /Users/rubber/linux/kernel/sched/fair.c: 1980
	 * Look at other nodes in these cases: /Users/rubber/linux/kernel/sched/fair.c: 1981
	 * - there is no space available on the preferred_nid /Users/rubber/linux/kernel/sched/fair.c: 1982
	 * - the task is part of a numa_group that is interleaved across /Users/rubber/linux/kernel/sched/fair.c: 1983
	 *   multiple NUMA nodes; in order to better consolidate the group, /Users/rubber/linux/kernel/sched/fair.c: 1984
	 *   we need to check other locations. /Users/rubber/linux/kernel/sched/fair.c: 1985
	/* /Users/rubber/linux/kernel/sched/fair.c: 2013
	 * If the task is part of a workload that spans multiple NUMA nodes, /Users/rubber/linux/kernel/sched/fair.c: 2014
	 * and is migrating into one of the workload's active nodes, remember /Users/rubber/linux/kernel/sched/fair.c: 2015
	 * this node as the task's preferred numa node, so the workload can /Users/rubber/linux/kernel/sched/fair.c: 2016
	 * settle down. /Users/rubber/linux/kernel/sched/fair.c: 2017
	 * A task that migrated to a second choice node will be better off /Users/rubber/linux/kernel/sched/fair.c: 2018
	 * trying for a better one later. Do not set the preferred node here. /Users/rubber/linux/kernel/sched/fair.c: 2019
 * Find out how many nodes the workload is actively running on. Do this by /Users/rubber/linux/kernel/sched/fair.c: 2077
 * tracking the nodes from which NUMA hinting faults are triggered. This can /Users/rubber/linux/kernel/sched/fair.c: 2078
 * be different from the set of nodes where the workload's memory is currently /Users/rubber/linux/kernel/sched/fair.c: 2079
 * located. /Users/rubber/linux/kernel/sched/fair.c: 2080
 * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS /Users/rubber/linux/kernel/sched/fair.c: 2104
 * increments. The more local the fault statistics are, the higher the scan /Users/rubber/linux/kernel/sched/fair.c: 2105
 * period will be for the next scan window. If local/(local+remote) ratio is /Users/rubber/linux/kernel/sched/fair.c: 2106
 * below NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) /Users/rubber/linux/kernel/sched/fair.c: 2107
 * the scan period will decrease. Aim for 70% local accesses. /Users/rubber/linux/kernel/sched/fair.c: 2108
 * Increase the scan period (slow down scanning) if the majority of /Users/rubber/linux/kernel/sched/fair.c: 2114
 * our memory is already on our local node, or if the majority of /Users/rubber/linux/kernel/sched/fair.c: 2115
 * the page accesses are shared with other processes. /Users/rubber/linux/kernel/sched/fair.c: 2116
 * Otherwise, decrease the scan period. /Users/rubber/linux/kernel/sched/fair.c: 2117
	/* /Users/rubber/linux/kernel/sched/fair.c: 2129
	 * If there were no record hinting faults then either the task is /Users/rubber/linux/kernel/sched/fair.c: 2130
	 * completely idle or all activity is in areas that are not of interest /Users/rubber/linux/kernel/sched/fair.c: 2131
	 * to automatic numa balancing. Related to that, if there were failed /Users/rubber/linux/kernel/sched/fair.c: 2132
	 * migration then it implies we are migrating too quickly or the local /Users/rubber/linux/kernel/sched/fair.c: 2133
	 * node is overloaded. In either case, scan slower /Users/rubber/linux/kernel/sched/fair.c: 2134
	/* /Users/rubber/linux/kernel/sched/fair.c: 2146
	 * Prepare to scale scan period relative to the current period. /Users/rubber/linux/kernel/sched/fair.c: 2147
	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same /Users/rubber/linux/kernel/sched/fair.c: 2148
	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster) /Users/rubber/linux/kernel/sched/fair.c: 2149
	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower) /Users/rubber/linux/kernel/sched/fair.c: 2150
		/* /Users/rubber/linux/kernel/sched/fair.c: 2157
		 * Most memory accesses are local. There is no need to /Users/rubber/linux/kernel/sched/fair.c: 2158
		 * do fast NUMA scanning, since memory is already local. /Users/rubber/linux/kernel/sched/fair.c: 2159
		/* /Users/rubber/linux/kernel/sched/fair.c: 2166
		 * Most memory accesses are shared with other tasks. /Users/rubber/linux/kernel/sched/fair.c: 2167
		 * There is no point in continuing fast NUMA scanning, /Users/rubber/linux/kernel/sched/fair.c: 2168
		 * since other tasks may just move the memory elsewhere. /Users/rubber/linux/kernel/sched/fair.c: 2169
		/* /Users/rubber/linux/kernel/sched/fair.c: 2176
		 * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS, /Users/rubber/linux/kernel/sched/fair.c: 2177
		 * yet they are not on the local NUMA node. Speed up /Users/rubber/linux/kernel/sched/fair.c: 2178
		 * NUMA scanning to get the memory moved over. /Users/rubber/linux/kernel/sched/fair.c: 2179
 * Get the fraction of time the task has been running since the last /Users/rubber/linux/kernel/sched/fair.c: 2191
 * NUMA placement cycle. The scheduler keeps similar statistics, but /Users/rubber/linux/kernel/sched/fair.c: 2192
 * decays those on a 32ms period, which is orders of magnitude off /Users/rubber/linux/kernel/sched/fair.c: 2193
 * from the dozens-of-seconds NUMA balancing period. Use the scheduler /Users/rubber/linux/kernel/sched/fair.c: 2194
 * stats only if the task is so new there are no NUMA statistics yet. /Users/rubber/linux/kernel/sched/fair.c: 2195
 * Determine the preferred nid for a task in a numa_group. This needs to /Users/rubber/linux/kernel/sched/fair.c: 2223
 * be done in a way that produces consistent results with group_weight, /Users/rubber/linux/kernel/sched/fair.c: 2224
 * otherwise workloads might not converge. /Users/rubber/linux/kernel/sched/fair.c: 2225
	/* /Users/rubber/linux/kernel/sched/fair.c: 2236
	 * On a system with glueless mesh NUMA topology, group_weight /Users/rubber/linux/kernel/sched/fair.c: 2237
	 * scores nodes according to the number of NUMA hinting faults on /Users/rubber/linux/kernel/sched/fair.c: 2238
	 * both the node itself, and on nearby nodes. /Users/rubber/linux/kernel/sched/fair.c: 2239
	/* /Users/rubber/linux/kernel/sched/fair.c: 2257
	 * Finding the preferred nid in a system with NUMA backplane /Users/rubber/linux/kernel/sched/fair.c: 2258
	 * interconnect topology is more involved. The goal is to locate /Users/rubber/linux/kernel/sched/fair.c: 2259
	 * tasks from numa_groups near each other in the system, and /Users/rubber/linux/kernel/sched/fair.c: 2260
	 * untangle workloads from different sides of the system. This requires /Users/rubber/linux/kernel/sched/fair.c: 2261
	 * searching down the hierarchy of node groups, recursively searching /Users/rubber/linux/kernel/sched/fair.c: 2262
	 * inside the highest scoring group of nodes. The nodemask tricks /Users/rubber/linux/kernel/sched/fair.c: 2263
	 * keep the complexity of the search down. /Users/rubber/linux/kernel/sched/fair.c: 2264
				/* /Users/rubber/linux/kernel/sched/fair.c: 2294
				 * subtle: at the smallest distance there is /Users/rubber/linux/kernel/sched/fair.c: 2295
				 * just one node left in each "group", the /Users/rubber/linux/kernel/sched/fair.c: 2296
				 * winner is the preferred nid. /Users/rubber/linux/kernel/sched/fair.c: 2297
	/* /Users/rubber/linux/kernel/sched/fair.c: 2320
	 * The p->mm->numa_scan_seq field gets updated without /Users/rubber/linux/kernel/sched/fair.c: 2321
	 * exclusive access. Use READ_ONCE() here to ensure /Users/rubber/linux/kernel/sched/fair.c: 2322
	 * that the field is read in a single access: /Users/rubber/linux/kernel/sched/fair.c: 2323
			/* /Users/rubber/linux/kernel/sched/fair.c: 2362
			 * Normalize the faults_from, so all tasks in a group /Users/rubber/linux/kernel/sched/fair.c: 2363
			 * count according to CPU use, instead of by the raw /Users/rubber/linux/kernel/sched/fair.c: 2364
			 * number of faults. Tasks with little runtime have /Users/rubber/linux/kernel/sched/fair.c: 2365
			 * little over-all impact on throughput, and thus their /Users/rubber/linux/kernel/sched/fair.c: 2366
			 * faults are less important. /Users/rubber/linux/kernel/sched/fair.c: 2367
				/* /Users/rubber/linux/kernel/sched/fair.c: 2380
				 * safe because we can only change our own group /Users/rubber/linux/kernel/sched/fair.c: 2381
				 * /Users/rubber/linux/kernel/sched/fair.c: 2382
				 * mem_idx represents the offset for a given /Users/rubber/linux/kernel/sched/fair.c: 2383
				 * nid and priv in a specific region because it /Users/rubber/linux/kernel/sched/fair.c: 2384
				 * is at the beginning of the numa_faults array. /Users/rubber/linux/kernel/sched/fair.c: 2385
	/* /Users/rubber/linux/kernel/sched/fair.c: 2478
	 * Only join the other group if its bigger; if we're the bigger group, /Users/rubber/linux/kernel/sched/fair.c: 2479
	 * the other task will join us. /Users/rubber/linux/kernel/sched/fair.c: 2480
	/* /Users/rubber/linux/kernel/sched/fair.c: 2485
	 * Tie-break on the grp address. /Users/rubber/linux/kernel/sched/fair.c: 2486
 * Get rid of NUMA statistics associated with a task (either current or dead). /Users/rubber/linux/kernel/sched/fair.c: 2537
 * If @final is set, the task is dead and has reached refcount zero, so we can /Users/rubber/linux/kernel/sched/fair.c: 2538
 * safely free all relevant data structures. Otherwise, there might be /Users/rubber/linux/kernel/sched/fair.c: 2539
 * concurrent reads from places like load balancing and procfs, and we should /Users/rubber/linux/kernel/sched/fair.c: 2540
 * reset the data back to default state without freeing ->numa_faults. /Users/rubber/linux/kernel/sched/fair.c: 2541
 * Got a PROT_NONE fault for a page on @node. /Users/rubber/linux/kernel/sched/fair.c: 2577
	/* /Users/rubber/linux/kernel/sched/fair.c: 2608
	 * First accesses are treated as private, otherwise consider accesses /Users/rubber/linux/kernel/sched/fair.c: 2609
	 * to be private if the accessing pid has not changed /Users/rubber/linux/kernel/sched/fair.c: 2610
	/* /Users/rubber/linux/kernel/sched/fair.c: 2620
	 * If a workload spans multiple NUMA nodes, a shared fault that /Users/rubber/linux/kernel/sched/fair.c: 2621
	 * occurs wholly within the set of nodes that the workload is /Users/rubber/linux/kernel/sched/fair.c: 2622
	 * actively using should be counted as local. This allows the /Users/rubber/linux/kernel/sched/fair.c: 2623
	 * scan rate to slow down when a workload has settled down. /Users/rubber/linux/kernel/sched/fair.c: 2624
	/* /Users/rubber/linux/kernel/sched/fair.c: 2632
	 * Retry to migrate task to preferred node periodically, in case it /Users/rubber/linux/kernel/sched/fair.c: 2633
	 * previously failed, or the scheduler moved us. /Users/rubber/linux/kernel/sched/fair.c: 2634
	/* /Users/rubber/linux/kernel/sched/fair.c: 2653
	 * We only did a read acquisition of the mmap sem, so /Users/rubber/linux/kernel/sched/fair.c: 2654
	 * p->mm->numa_scan_seq is written to without exclusive access /Users/rubber/linux/kernel/sched/fair.c: 2655
	 * and the update is not guaranteed to be atomic. That's not /Users/rubber/linux/kernel/sched/fair.c: 2656
	 * much of an issue though, since this is just used for /Users/rubber/linux/kernel/sched/fair.c: 2657
	 * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not /Users/rubber/linux/kernel/sched/fair.c: 2658
	 * expensive, to avoid any form of compiler optimizations: /Users/rubber/linux/kernel/sched/fair.c: 2659
 * The expensive part of numa migration is done from task_work context. /Users/rubber/linux/kernel/sched/fair.c: 2666
 * Triggered from task_tick_numa(). /Users/rubber/linux/kernel/sched/fair.c: 2667
	/* /Users/rubber/linux/kernel/sched/fair.c: 2683
	 * Who cares about NUMA placement when they're dying. /Users/rubber/linux/kernel/sched/fair.c: 2684
	 * /Users/rubber/linux/kernel/sched/fair.c: 2685
	 * NOTE: make sure not to dereference p->mm before this check, /Users/rubber/linux/kernel/sched/fair.c: 2686
	 * exit_task_work() happens _after_ exit_mm() so we could be called /Users/rubber/linux/kernel/sched/fair.c: 2687
	 * without p->mm even though we still had it when we enqueued this /Users/rubber/linux/kernel/sched/fair.c: 2688
	 * work. /Users/rubber/linux/kernel/sched/fair.c: 2689
	/* /Users/rubber/linux/kernel/sched/fair.c: 2699
	 * Enforce maximal scan/migration frequency.. /Users/rubber/linux/kernel/sched/fair.c: 2700
	/* /Users/rubber/linux/kernel/sched/fair.c: 2715
	 * Delay this task enough that another task of this mm will likely win /Users/rubber/linux/kernel/sched/fair.c: 2716
	 * the next time around. /Users/rubber/linux/kernel/sched/fair.c: 2717
		/* /Users/rubber/linux/kernel/sched/fair.c: 2743
		 * Shared library pages mapped by multiple processes are not /Users/rubber/linux/kernel/sched/fair.c: 2744
		 * migrated as it is expected they are cache replicated. Avoid /Users/rubber/linux/kernel/sched/fair.c: 2745
		 * hinting faults in read-only file-backed mappings or the vdso /Users/rubber/linux/kernel/sched/fair.c: 2746
		 * as migrating the pages will be of marginal benefit. /Users/rubber/linux/kernel/sched/fair.c: 2747
		/* /Users/rubber/linux/kernel/sched/fair.c: 2753
		 * Skip inaccessible VMAs to avoid any confusion between /Users/rubber/linux/kernel/sched/fair.c: 2754
		 * PROT_NONE and NUMA hinting ptes /Users/rubber/linux/kernel/sched/fair.c: 2755
			/* /Users/rubber/linux/kernel/sched/fair.c: 2766
			 * Try to scan sysctl_numa_balancing_size worth of /Users/rubber/linux/kernel/sched/fair.c: 2767
			 * hpages that have at least one present PTE that /Users/rubber/linux/kernel/sched/fair.c: 2768
			 * is not already pte-numa. If the VMA contains /Users/rubber/linux/kernel/sched/fair.c: 2769
			 * areas that are unused or already full of prot_numa /Users/rubber/linux/kernel/sched/fair.c: 2770
			 * PTEs, scan up to virtpages, to skip through those /Users/rubber/linux/kernel/sched/fair.c: 2771
			 * areas faster. /Users/rubber/linux/kernel/sched/fair.c: 2772
	/* /Users/rubber/linux/kernel/sched/fair.c: 2787
	 * It is possible to reach the end of the VMA list but the last few /Users/rubber/linux/kernel/sched/fair.c: 2788
	 * VMAs are not guaranteed to the vma_migratable. If they are not, we /Users/rubber/linux/kernel/sched/fair.c: 2789
	 * would find the !migratable VMA on the next scan but not reset the /Users/rubber/linux/kernel/sched/fair.c: 2790
	 * scanner to the start so check it now. /Users/rubber/linux/kernel/sched/fair.c: 2791
	/* /Users/rubber/linux/kernel/sched/fair.c: 2799
	 * Make sure tasks use at least 32x as much time to run other code /Users/rubber/linux/kernel/sched/fair.c: 2800
	 * than they used here, to limit NUMA PTE scanning overhead to 3% max. /Users/rubber/linux/kernel/sched/fair.c: 2801
	 * Usually update_task_scan_period slows down scanning enough; on an /Users/rubber/linux/kernel/sched/fair.c: 2802
	 * overloaded system we need to limit overhead on a per task basis. /Users/rubber/linux/kernel/sched/fair.c: 2803
	/* /Users/rubber/linux/kernel/sched/fair.c: 2841
	 * New thread, keep existing numa_preferred_nid which should be copied /Users/rubber/linux/kernel/sched/fair.c: 2842
	 * already by arch_dup_task_struct but stagger when scans start. /Users/rubber/linux/kernel/sched/fair.c: 2843
 * Drive the periodic memory faults.. /Users/rubber/linux/kernel/sched/fair.c: 2856
	/* /Users/rubber/linux/kernel/sched/fair.c: 2863
	 * We don't care about NUMA placement if we don't have memory. /Users/rubber/linux/kernel/sched/fair.c: 2864
	/* /Users/rubber/linux/kernel/sched/fair.c: 2869
	 * Using runtime rather than walltime has the dual advantage that /Users/rubber/linux/kernel/sched/fair.c: 2870
	 * we (mostly) drive the selection from busy threads and that the /Users/rubber/linux/kernel/sched/fair.c: 2871
	 * task needs to have done some actual work before we bother with /Users/rubber/linux/kernel/sched/fair.c: 2872
	 * NUMA placement. /Users/rubber/linux/kernel/sched/fair.c: 2873
	/* /Users/rubber/linux/kernel/sched/fair.c: 2902
	 * Allow resets if faults have been trapped before one scan /Users/rubber/linux/kernel/sched/fair.c: 2903
	 * has completed. This is most likely due to a new task that /Users/rubber/linux/kernel/sched/fair.c: 2904
	 * is pulled cross-node due to wakeups or load balancing. /Users/rubber/linux/kernel/sched/fair.c: 2905
		/* /Users/rubber/linux/kernel/sched/fair.c: 2908
		 * Avoid scan adjustments if moving to the preferred /Users/rubber/linux/kernel/sched/fair.c: 2909
		 * node or if the task was not previously running on /Users/rubber/linux/kernel/sched/fair.c: 2910
		 * the preferred node. /Users/rubber/linux/kernel/sched/fair.c: 2911
 * Signed add and clamp on underflow. /Users/rubber/linux/kernel/sched/fair.c: 2974
 * Explicitly do a load-store to ensure the intermediate value never hits /Users/rubber/linux/kernel/sched/fair.c: 2976
 * memory. This allows lockless observations without ever seeing the negative /Users/rubber/linux/kernel/sched/fair.c: 2977
 * values. /Users/rubber/linux/kernel/sched/fair.c: 2978
 * Unsigned subtract and clamp on underflow. /Users/rubber/linux/kernel/sched/fair.c: 2994
 * Explicitly do a load-store to ensure the intermediate value never hits /Users/rubber/linux/kernel/sched/fair.c: 2996
 * memory. This allows lockless observations without ever seeing the negative /Users/rubber/linux/kernel/sched/fair.c: 2997
 * values. /Users/rubber/linux/kernel/sched/fair.c: 2998
 * Remove and clamp on negative, from a local variable. /Users/rubber/linux/kernel/sched/fair.c: 3011
 * A variant of sub_positive(), which does not use explicit load-store /Users/rubber/linux/kernel/sched/fair.c: 3013
 * and is thus optimized for local variable updates. /Users/rubber/linux/kernel/sched/fair.c: 3014
 * All this does is approximate the hierarchical proportion which includes that /Users/rubber/linux/kernel/sched/fair.c: 3084
 * global sum we all love to hate. /Users/rubber/linux/kernel/sched/fair.c: 3085
 * That is, the weight of a group entity, is the proportional share of the /Users/rubber/linux/kernel/sched/fair.c: 3087
 * group weight based on the group runqueue weights. That is: /Users/rubber/linux/kernel/sched/fair.c: 3088
 *                     tg->weight * grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3090
 *   ge->load.weight = -----------------------------               (1) /Users/rubber/linux/kernel/sched/fair.c: 3091
 *                       \Sum grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3092
 * Now, because computing that sum is prohibitively expensive to compute (been /Users/rubber/linux/kernel/sched/fair.c: 3094
 * there, done that) we approximate it with this average stuff. The average /Users/rubber/linux/kernel/sched/fair.c: 3095
 * moves slower and therefore the approximation is cheaper and more stable. /Users/rubber/linux/kernel/sched/fair.c: 3096
 * So instead of the above, we substitute: /Users/rubber/linux/kernel/sched/fair.c: 3098
 *   grq->load.weight -> grq->avg.load_avg                         (2) /Users/rubber/linux/kernel/sched/fair.c: 3100
 * which yields the following: /Users/rubber/linux/kernel/sched/fair.c: 3102
 *                     tg->weight * grq->avg.load_avg /Users/rubber/linux/kernel/sched/fair.c: 3104
 *   ge->load.weight = ------------------------------              (3) /Users/rubber/linux/kernel/sched/fair.c: 3105
 *                             tg->load_avg /Users/rubber/linux/kernel/sched/fair.c: 3106
 * Where: tg->load_avg ~= \Sum grq->avg.load_avg /Users/rubber/linux/kernel/sched/fair.c: 3108
 * That is shares_avg, and it is right (given the approximation (2)). /Users/rubber/linux/kernel/sched/fair.c: 3110
 * The problem with it is that because the average is slow -- it was designed /Users/rubber/linux/kernel/sched/fair.c: 3112
 * to be exactly that of course -- this leads to transients in boundary /Users/rubber/linux/kernel/sched/fair.c: 3113
 * conditions. In specific, the case where the group was idle and we start the /Users/rubber/linux/kernel/sched/fair.c: 3114
 * one task. It takes time for our CPU's grq->avg.load_avg to build up, /Users/rubber/linux/kernel/sched/fair.c: 3115
 * yielding bad latency etc.. /Users/rubber/linux/kernel/sched/fair.c: 3116
 * Now, in that special case (1) reduces to: /Users/rubber/linux/kernel/sched/fair.c: 3118
 *                     tg->weight * grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3120
 *   ge->load.weight = ----------------------------- = tg->weight   (4) /Users/rubber/linux/kernel/sched/fair.c: 3121
 *                         grp->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3122
 * That is, the sum collapses because all other CPUs are idle; the UP scenario. /Users/rubber/linux/kernel/sched/fair.c: 3124
 * So what we do is modify our approximation (3) to approach (4) in the (near) /Users/rubber/linux/kernel/sched/fair.c: 3126
 * UP case, like: /Users/rubber/linux/kernel/sched/fair.c: 3127
 *   ge->load.weight = /Users/rubber/linux/kernel/sched/fair.c: 3129
 *              tg->weight * grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3131
 *     ---------------------------------------------------         (5) /Users/rubber/linux/kernel/sched/fair.c: 3132
 *     tg->load_avg - grq->avg.load_avg + grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3133
 * But because grq->load.weight can drop to 0, resulting in a divide by zero, /Users/rubber/linux/kernel/sched/fair.c: 3135
 * we need to use grq->avg.load_avg as its lower bound, which then gives: /Users/rubber/linux/kernel/sched/fair.c: 3136
 *                     tg->weight * grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3139
 *   ge->load.weight = -----------------------------		   (6) /Users/rubber/linux/kernel/sched/fair.c: 3140
 *                             tg_load_avg' /Users/rubber/linux/kernel/sched/fair.c: 3141
 * Where: /Users/rubber/linux/kernel/sched/fair.c: 3143
 *   tg_load_avg' = tg->load_avg - grq->avg.load_avg + /Users/rubber/linux/kernel/sched/fair.c: 3145
 *                  max(grq->load.weight, grq->avg.load_avg) /Users/rubber/linux/kernel/sched/fair.c: 3146
 * And that is shares_weight and is icky. In the (near) UP case it approaches /Users/rubber/linux/kernel/sched/fair.c: 3148
 * (4) while in the normal case it approaches (3). It consistently /Users/rubber/linux/kernel/sched/fair.c: 3149
 * overestimates the ge->load.weight and therefore: /Users/rubber/linux/kernel/sched/fair.c: 3150
 *   \Sum ge->load.weight >= tg->weight /Users/rubber/linux/kernel/sched/fair.c: 3152
 * hence icky! /Users/rubber/linux/kernel/sched/fair.c: 3154
	/* /Users/rubber/linux/kernel/sched/fair.c: 3175
	 * MIN_SHARES has to be unscaled here to support per-CPU partitioning /Users/rubber/linux/kernel/sched/fair.c: 3176
	 * of a group with small tg->shares value. It is a floor value which is /Users/rubber/linux/kernel/sched/fair.c: 3177
	 * assigned as a minimum load.weight to the sched_entity representing /Users/rubber/linux/kernel/sched/fair.c: 3178
	 * the group on a CPU. /Users/rubber/linux/kernel/sched/fair.c: 3179
	 * /Users/rubber/linux/kernel/sched/fair.c: 3180
	 * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024 /Users/rubber/linux/kernel/sched/fair.c: 3181
	 * on an 8-core system with 8 tasks each runnable on one CPU shares has /Users/rubber/linux/kernel/sched/fair.c: 3182
	 * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In /Users/rubber/linux/kernel/sched/fair.c: 3183
	 * case no task is runnable on a CPU MIN_SHARES=2 should be returned /Users/rubber/linux/kernel/sched/fair.c: 3184
	 * instead of 0. /Users/rubber/linux/kernel/sched/fair.c: 3185
 * Recomputes the group entity based on the current state of its group /Users/rubber/linux/kernel/sched/fair.c: 3194
 * runqueue. /Users/rubber/linux/kernel/sched/fair.c: 3195
		/* /Users/rubber/linux/kernel/sched/fair.c: 3231
		 * There are a few boundary cases this might miss but it should /Users/rubber/linux/kernel/sched/fair.c: 3232
		 * get called often enough that that should (hopefully) not be /Users/rubber/linux/kernel/sched/fair.c: 3233
		 * a real problem. /Users/rubber/linux/kernel/sched/fair.c: 3234
		 * /Users/rubber/linux/kernel/sched/fair.c: 3235
		 * It will not get called when we go idle, because the idle /Users/rubber/linux/kernel/sched/fair.c: 3236
		 * thread is a different class (!fair), nor will the utilization /Users/rubber/linux/kernel/sched/fair.c: 3237
		 * number include things like RT tasks. /Users/rubber/linux/kernel/sched/fair.c: 3238
		 * /Users/rubber/linux/kernel/sched/fair.c: 3239
		 * As is, the util number is not freq-invariant (we'd have to /Users/rubber/linux/kernel/sched/fair.c: 3240
		 * implement arch_scale_freq_capacity() for that). /Users/rubber/linux/kernel/sched/fair.c: 3241
		 * /Users/rubber/linux/kernel/sched/fair.c: 3242
		 * See cpu_util(). /Users/rubber/linux/kernel/sched/fair.c: 3243
 * Because list_add_leaf_cfs_rq always places a child cfs_rq on the list /Users/rubber/linux/kernel/sched/fair.c: 3252
 * immediately before a parent cfs_rq, and cfs_rqs are removed from the list /Users/rubber/linux/kernel/sched/fair.c: 3253
 * bottom-up, we only have to test whether the cfs_rq before us on the list /Users/rubber/linux/kernel/sched/fair.c: 3254
 * is our child. /Users/rubber/linux/kernel/sched/fair.c: 3255
 * If cfs_rq is not on the list, test whether a child needs its to be added to /Users/rubber/linux/kernel/sched/fair.c: 3256
 * connect a branch to the tree  * (see list_add_leaf_cfs_rq() for details). /Users/rubber/linux/kernel/sched/fair.c: 3257
	/* /Users/rubber/linux/kernel/sched/fair.c: 3294
	 * _avg must be null when _sum are null because _avg = _sum / divider /Users/rubber/linux/kernel/sched/fair.c: 3295
	 * Make sure that rounding and/or propagation of PELT values never /Users/rubber/linux/kernel/sched/fair.c: 3296
	 * break this. /Users/rubber/linux/kernel/sched/fair.c: 3297
 * update_tg_load_avg - update the tg's load avg /Users/rubber/linux/kernel/sched/fair.c: 3307
 * @cfs_rq: the cfs_rq whose avg changed /Users/rubber/linux/kernel/sched/fair.c: 3308
 * This function 'ensures': tg->load_avg := \Sum tg->cfs_rq[]->avg.load. /Users/rubber/linux/kernel/sched/fair.c: 3310
 * However, because tg->load_avg is a global value there are performance /Users/rubber/linux/kernel/sched/fair.c: 3311
 * considerations. /Users/rubber/linux/kernel/sched/fair.c: 3312
 * In order to avoid having to look at the other cfs_rq's, we use a /Users/rubber/linux/kernel/sched/fair.c: 3314
 * differential update where we store the last value we propagated. This in /Users/rubber/linux/kernel/sched/fair.c: 3315
 * turn allows skipping updates if the differential is 'small'. /Users/rubber/linux/kernel/sched/fair.c: 3316
 * Updating tg's load_avg is necessary before update_cfs_share(). /Users/rubber/linux/kernel/sched/fair.c: 3318
	/* /Users/rubber/linux/kernel/sched/fair.c: 3324
	 * No need to update load_avg for root_task_group as it is not used. /Users/rubber/linux/kernel/sched/fair.c: 3325
 * Called within set_task_rq() right before setting a task's CPU. The /Users/rubber/linux/kernel/sched/fair.c: 3337
 * caller only guarantees p->pi_lock is held; no other assumptions, /Users/rubber/linux/kernel/sched/fair.c: 3338
 * including the state of rq->lock, should be made. /Users/rubber/linux/kernel/sched/fair.c: 3339
	/* /Users/rubber/linux/kernel/sched/fair.c: 3350
	 * We are supposed to update the task to "current" time, then its up to /Users/rubber/linux/kernel/sched/fair.c: 3351
	 * date and ready to go to new CPU/cfs_rq. But we have difficulty in /Users/rubber/linux/kernel/sched/fair.c: 3352
	 * getting what current time is, so simply throw away the out-of-date /Users/rubber/linux/kernel/sched/fair.c: 3353
	 * time. This will result in the wakee task is less decayed, but giving /Users/rubber/linux/kernel/sched/fair.c: 3354
	 * the wakee more load sounds not bad. /Users/rubber/linux/kernel/sched/fair.c: 3355
 * When on migration a sched_entity joins/leaves the PELT hierarchy, we need to /Users/rubber/linux/kernel/sched/fair.c: 3387
 * propagate its contribution. The key to this propagation is the invariant /Users/rubber/linux/kernel/sched/fair.c: 3388
 * that for each group: /Users/rubber/linux/kernel/sched/fair.c: 3389
 *   ge->avg == grq->avg						(1) /Users/rubber/linux/kernel/sched/fair.c: 3391
 * _IFF_ we look at the pure running and runnable sums. Because they /Users/rubber/linux/kernel/sched/fair.c: 3393
 * represent the very same entity, just at different points in the hierarchy. /Users/rubber/linux/kernel/sched/fair.c: 3394
 * Per the above update_tg_cfs_util() and update_tg_cfs_runnable() are trivial /Users/rubber/linux/kernel/sched/fair.c: 3396
 * and simply copies the running/runnable sum over (but still wrong, because /Users/rubber/linux/kernel/sched/fair.c: 3397
 * the group entity and group rq do not have their PELT windows aligned). /Users/rubber/linux/kernel/sched/fair.c: 3398
 * However, update_tg_cfs_load() is more complex. So we have: /Users/rubber/linux/kernel/sched/fair.c: 3400
 *   ge->avg.load_avg = ge->load.weight * ge->avg.runnable_avg		(2) /Users/rubber/linux/kernel/sched/fair.c: 3402
 * And since, like util, the runnable part should be directly transferable, /Users/rubber/linux/kernel/sched/fair.c: 3404
 * the following would _appear_ to be the straight forward approach: /Users/rubber/linux/kernel/sched/fair.c: 3405
 *   grq->avg.load_avg = grq->load.weight * grq->avg.runnable_avg	(3) /Users/rubber/linux/kernel/sched/fair.c: 3407
 * And per (1) we have: /Users/rubber/linux/kernel/sched/fair.c: 3409
 *   ge->avg.runnable_avg == grq->avg.runnable_avg /Users/rubber/linux/kernel/sched/fair.c: 3411
 * Which gives: /Users/rubber/linux/kernel/sched/fair.c: 3413
 *                      ge->load.weight * grq->avg.load_avg /Users/rubber/linux/kernel/sched/fair.c: 3415
 *   ge->avg.load_avg = -----------------------------------		(4) /Users/rubber/linux/kernel/sched/fair.c: 3416
 *                               grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3417
 * Except that is wrong! /Users/rubber/linux/kernel/sched/fair.c: 3419
 * Because while for entities historical weight is not important and we /Users/rubber/linux/kernel/sched/fair.c: 3421
 * really only care about our future and therefore can consider a pure /Users/rubber/linux/kernel/sched/fair.c: 3422
 * runnable sum, runqueues can NOT do this. /Users/rubber/linux/kernel/sched/fair.c: 3423
 * We specifically want runqueues to have a load_avg that includes /Users/rubber/linux/kernel/sched/fair.c: 3425
 * historical weights. Those represent the blocked load, the load we expect /Users/rubber/linux/kernel/sched/fair.c: 3426
 * to (shortly) return to us. This only works by keeping the weights as /Users/rubber/linux/kernel/sched/fair.c: 3427
 * integral part of the sum. We therefore cannot decompose as per (3). /Users/rubber/linux/kernel/sched/fair.c: 3428
 * Another reason this doesn't work is that runnable isn't a 0-sum entity. /Users/rubber/linux/kernel/sched/fair.c: 3430
 * Imagine a rq with 2 tasks that each are runnable 2/3 of the time. Then the /Users/rubber/linux/kernel/sched/fair.c: 3431
 * rq itself is runnable anywhere between 2/3 and 1 depending on how the /Users/rubber/linux/kernel/sched/fair.c: 3432
 * runnable section of these tasks overlap (or not). If they were to perfectly /Users/rubber/linux/kernel/sched/fair.c: 3433
 * align the rq as a whole would be runnable 2/3 of the time. If however we /Users/rubber/linux/kernel/sched/fair.c: 3434
 * always have at least 1 runnable task, the rq as a whole is always runnable. /Users/rubber/linux/kernel/sched/fair.c: 3435
 * So we'll have to approximate.. :/ /Users/rubber/linux/kernel/sched/fair.c: 3437
 * Given the constraint: /Users/rubber/linux/kernel/sched/fair.c: 3439
 *   ge->avg.running_sum <= ge->avg.runnable_sum <= LOAD_AVG_MAX /Users/rubber/linux/kernel/sched/fair.c: 3441
 * We can construct a rule that adds runnable to a rq by assuming minimal /Users/rubber/linux/kernel/sched/fair.c: 3443
 * overlap. /Users/rubber/linux/kernel/sched/fair.c: 3444
 * On removal, we'll assume each task is equally runnable; which yields: /Users/rubber/linux/kernel/sched/fair.c: 3446
 *   grq->avg.runnable_sum = grq->avg.load_sum / grq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 3448
 * XXX: only do this for the part of runnable > running ? /Users/rubber/linux/kernel/sched/fair.c: 3450
	/* /Users/rubber/linux/kernel/sched/fair.c: 3464
	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se. /Users/rubber/linux/kernel/sched/fair.c: 3465
	 * See ___update_load_avg() for details. /Users/rubber/linux/kernel/sched/fair.c: 3466
	/* /Users/rubber/linux/kernel/sched/fair.c: 3489
	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se. /Users/rubber/linux/kernel/sched/fair.c: 3490
	 * See ___update_load_avg() for details. /Users/rubber/linux/kernel/sched/fair.c: 3491
	/* /Users/rubber/linux/kernel/sched/fair.c: 3517
	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se. /Users/rubber/linux/kernel/sched/fair.c: 3518
	 * See ___update_load_avg() for details. /Users/rubber/linux/kernel/sched/fair.c: 3519
		/* /Users/rubber/linux/kernel/sched/fair.c: 3524
		 * Add runnable; clip at LOAD_AVG_MAX. Reflects that until /Users/rubber/linux/kernel/sched/fair.c: 3525
		 * the CPU is saturated running == runnable. /Users/rubber/linux/kernel/sched/fair.c: 3526
		/* /Users/rubber/linux/kernel/sched/fair.c: 3531
		 * Estimate the new unweighted runnable_sum of the gcfs_rq by /Users/rubber/linux/kernel/sched/fair.c: 3532
		 * assuming all tasks are equally runnable. /Users/rubber/linux/kernel/sched/fair.c: 3533
	/* /Users/rubber/linux/kernel/sched/fair.c: 3544
	 * runnable_sum can't be lower than running_sum /Users/rubber/linux/kernel/sched/fair.c: 3545
	 * Rescale running sum to be in the same range as runnable sum /Users/rubber/linux/kernel/sched/fair.c: 3546
	 * running_sum is in [0 : LOAD_AVG_MAX <<  SCHED_CAPACITY_SHIFT] /Users/rubber/linux/kernel/sched/fair.c: 3547
	 * runnable_sum is in [0 : LOAD_AVG_MAX] /Users/rubber/linux/kernel/sched/fair.c: 3548
 * Check if we need to update the load and the utilization of a blocked /Users/rubber/linux/kernel/sched/fair.c: 3603
 * group_entity: /Users/rubber/linux/kernel/sched/fair.c: 3604
	/* /Users/rubber/linux/kernel/sched/fair.c: 3610
	 * If sched_entity still have not zero load or utilization, we have to /Users/rubber/linux/kernel/sched/fair.c: 3611
	 * decay it: /Users/rubber/linux/kernel/sched/fair.c: 3612
	/* /Users/rubber/linux/kernel/sched/fair.c: 3617
	 * If there is a pending propagation, we have to update the load and /Users/rubber/linux/kernel/sched/fair.c: 3618
	 * the utilization of the sched_entity: /Users/rubber/linux/kernel/sched/fair.c: 3619
	/* /Users/rubber/linux/kernel/sched/fair.c: 3624
	 * Otherwise, the load and the utilization of the sched_entity is /Users/rubber/linux/kernel/sched/fair.c: 3625
	 * already zero and there is no pending propagation, so it will be a /Users/rubber/linux/kernel/sched/fair.c: 3626
	 * waste of time to try to decay it: /Users/rubber/linux/kernel/sched/fair.c: 3627
 * update_cfs_rq_load_avg - update the cfs_rq's load/util averages /Users/rubber/linux/kernel/sched/fair.c: 3646
 * @now: current time, as per cfs_rq_clock_pelt() /Users/rubber/linux/kernel/sched/fair.c: 3647
 * @cfs_rq: cfs_rq to update /Users/rubber/linux/kernel/sched/fair.c: 3648
 * The cfs_rq avg is the direct sum of all its entities (blocked and runnable) /Users/rubber/linux/kernel/sched/fair.c: 3650
 * avg. The immediate corollary is that all (fair) tasks must be attached, see /Users/rubber/linux/kernel/sched/fair.c: 3651
 * post_init_entity_util_avg(). /Users/rubber/linux/kernel/sched/fair.c: 3652
 * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example. /Users/rubber/linux/kernel/sched/fair.c: 3654
 * Returns true if the load decayed or we removed load. /Users/rubber/linux/kernel/sched/fair.c: 3656
 * Since both these conditions indicate a changed cfs_rq->avg.load we should /Users/rubber/linux/kernel/sched/fair.c: 3658
 * call update_tg_load_avg() when this function returns true. /Users/rubber/linux/kernel/sched/fair.c: 3659
		/* /Users/rubber/linux/kernel/sched/fair.c: 3691
		 * removed_runnable is the unweighted version of removed_load so we /Users/rubber/linux/kernel/sched/fair.c: 3692
		 * can use it to estimate removed_load_sum. /Users/rubber/linux/kernel/sched/fair.c: 3693
 * attach_entity_load_avg - attach this entity to its cfs_rq load avg /Users/rubber/linux/kernel/sched/fair.c: 3712
 * @cfs_rq: cfs_rq to attach to /Users/rubber/linux/kernel/sched/fair.c: 3713
 * @se: sched_entity to attach /Users/rubber/linux/kernel/sched/fair.c: 3714
 * Must call update_cfs_rq_load_avg() before this, since we rely on /Users/rubber/linux/kernel/sched/fair.c: 3716
 * cfs_rq->avg.last_update_time being current. /Users/rubber/linux/kernel/sched/fair.c: 3717
	/* /Users/rubber/linux/kernel/sched/fair.c: 3721
	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se. /Users/rubber/linux/kernel/sched/fair.c: 3722
	 * See ___update_load_avg() for details. /Users/rubber/linux/kernel/sched/fair.c: 3723
	/* /Users/rubber/linux/kernel/sched/fair.c: 3727
	 * When we attach the @se to the @cfs_rq, we must align the decay /Users/rubber/linux/kernel/sched/fair.c: 3728
	 * window because without that, really weird and wonderful things can /Users/rubber/linux/kernel/sched/fair.c: 3729
	 * happen. /Users/rubber/linux/kernel/sched/fair.c: 3730
	 * /Users/rubber/linux/kernel/sched/fair.c: 3731
	 * XXX illustrate /Users/rubber/linux/kernel/sched/fair.c: 3732
	/* /Users/rubber/linux/kernel/sched/fair.c: 3737
	 * Hell(o) Nasty stuff.. we need to recompute _sum based on the new /Users/rubber/linux/kernel/sched/fair.c: 3738
	 * period_contrib. This isn't strictly correct, but since we're /Users/rubber/linux/kernel/sched/fair.c: 3739
	 * entirely outside of the PELT hierarchy, nobody cares if we truncate /Users/rubber/linux/kernel/sched/fair.c: 3740
	 * _sum a little. /Users/rubber/linux/kernel/sched/fair.c: 3741
 * detach_entity_load_avg - detach this entity from its cfs_rq load avg /Users/rubber/linux/kernel/sched/fair.c: 3767
 * @cfs_rq: cfs_rq to detach from /Users/rubber/linux/kernel/sched/fair.c: 3768
 * @se: sched_entity to detach /Users/rubber/linux/kernel/sched/fair.c: 3769
 * Must call update_cfs_rq_load_avg() before this, since we rely on /Users/rubber/linux/kernel/sched/fair.c: 3771
 * cfs_rq->avg.last_update_time being current. /Users/rubber/linux/kernel/sched/fair.c: 3772
	/* /Users/rubber/linux/kernel/sched/fair.c: 3776
	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se. /Users/rubber/linux/kernel/sched/fair.c: 3777
	 * See ___update_load_avg() for details. /Users/rubber/linux/kernel/sched/fair.c: 3778
 * Optional action to be done while updating the load average /Users/rubber/linux/kernel/sched/fair.c: 3796
	/* /Users/rubber/linux/kernel/sched/fair.c: 3808
	 * Track task load average for carrying it to new CPU after migrated, and /Users/rubber/linux/kernel/sched/fair.c: 3809
	 * track group sched_entity load average for task_h_load calc in migration /Users/rubber/linux/kernel/sched/fair.c: 3810
		/* /Users/rubber/linux/kernel/sched/fair.c: 3820
		 * DO_ATTACH means we're here from enqueue_entity(). /Users/rubber/linux/kernel/sched/fair.c: 3821
		 * !last_update_time means we've passed through /Users/rubber/linux/kernel/sched/fair.c: 3822
		 * migrate_task_rq_fair() indicating we migrated. /Users/rubber/linux/kernel/sched/fair.c: 3823
		 * /Users/rubber/linux/kernel/sched/fair.c: 3824
		 * IOW we're enqueueing a task on a new CPU. /Users/rubber/linux/kernel/sched/fair.c: 3825
 * Synchronize entity load avg of dequeued entity without locking /Users/rubber/linux/kernel/sched/fair.c: 3860
 * the previous rq. /Users/rubber/linux/kernel/sched/fair.c: 3861
 * Task first catches up with cfs_rq, and then subtract /Users/rubber/linux/kernel/sched/fair.c: 3873
 * itself from the cfs_rq (task must be off the queue now). /Users/rubber/linux/kernel/sched/fair.c: 3874
	/* /Users/rubber/linux/kernel/sched/fair.c: 3881
	 * tasks cannot exit without having gone through wake_up_new_task() -> /Users/rubber/linux/kernel/sched/fair.c: 3882
	 * post_init_entity_util_avg() which will have added things to the /Users/rubber/linux/kernel/sched/fair.c: 3883
	 * cfs_rq, so we can remove unconditionally. /Users/rubber/linux/kernel/sched/fair.c: 3884
 * Check if a (signed) value is within a specified (unsigned) margin, /Users/rubber/linux/kernel/sched/fair.c: 3975
 * based on the observation that: /Users/rubber/linux/kernel/sched/fair.c: 3976
 *     abs(x) < y := (unsigned)(x + y - 1) < (2 * y - 1) /Users/rubber/linux/kernel/sched/fair.c: 3978
 * NOTE: this only works when value + margin < INT_MAX. /Users/rubber/linux/kernel/sched/fair.c: 3980
	/* /Users/rubber/linux/kernel/sched/fair.c: 3997
	 * Skip update of task's estimated utilization when the task has not /Users/rubber/linux/kernel/sched/fair.c: 3998
	 * yet completed an activation, e.g. being migrated. /Users/rubber/linux/kernel/sched/fair.c: 3999
	/* /Users/rubber/linux/kernel/sched/fair.c: 4004
	 * If the PELT values haven't changed since enqueue time, /Users/rubber/linux/kernel/sched/fair.c: 4005
	 * skip the util_est update. /Users/rubber/linux/kernel/sched/fair.c: 4006
	/* /Users/rubber/linux/kernel/sched/fair.c: 4014
	 * Reset EWMA on utilization increases, the moving average is used only /Users/rubber/linux/kernel/sched/fair.c: 4015
	 * to smooth utilization decreases. /Users/rubber/linux/kernel/sched/fair.c: 4016
	/* /Users/rubber/linux/kernel/sched/fair.c: 4026
	 * Skip update of task's estimated utilization when its members are /Users/rubber/linux/kernel/sched/fair.c: 4027
	 * already ~1% close to its last activation value. /Users/rubber/linux/kernel/sched/fair.c: 4028
	/* /Users/rubber/linux/kernel/sched/fair.c: 4039
	 * To avoid overestimation of actual task utilization, skip updates if /Users/rubber/linux/kernel/sched/fair.c: 4040
	 * we cannot grant there is idle time in this CPU. /Users/rubber/linux/kernel/sched/fair.c: 4041
	/* /Users/rubber/linux/kernel/sched/fair.c: 4046
	 * Update Task's estimated utilization /Users/rubber/linux/kernel/sched/fair.c: 4047
	 * /Users/rubber/linux/kernel/sched/fair.c: 4048
	 * When *p completes an activation we can consolidate another sample /Users/rubber/linux/kernel/sched/fair.c: 4049
	 * of the task size. This is done by storing the current PELT value /Users/rubber/linux/kernel/sched/fair.c: 4050
	 * as ue.enqueued and by using this value to update the Exponential /Users/rubber/linux/kernel/sched/fair.c: 4051
	 * Weighted Moving Average (EWMA): /Users/rubber/linux/kernel/sched/fair.c: 4052
	 * /Users/rubber/linux/kernel/sched/fair.c: 4053
	 *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1) /Users/rubber/linux/kernel/sched/fair.c: 4054
	 *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1) /Users/rubber/linux/kernel/sched/fair.c: 4055
	 *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1) /Users/rubber/linux/kernel/sched/fair.c: 4056
	 *          = w * (      last_ewma_diff            ) +     ewma(t-1) /Users/rubber/linux/kernel/sched/fair.c: 4057
	 *          = w * (last_ewma_diff  +  ewma(t-1) / w) /Users/rubber/linux/kernel/sched/fair.c: 4058
	 * /Users/rubber/linux/kernel/sched/fair.c: 4059
	 * Where 'w' is the weight of new samples, which is configured to be /Users/rubber/linux/kernel/sched/fair.c: 4060
	 * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT) /Users/rubber/linux/kernel/sched/fair.c: 4061
	/* /Users/rubber/linux/kernel/sched/fair.c: 4093
	 * Make sure that misfit_task_load will not be null even if /Users/rubber/linux/kernel/sched/fair.c: 4094
	 * task_h_load() returns 0. /Users/rubber/linux/kernel/sched/fair.c: 4095
	/* /Users/rubber/linux/kernel/sched/fair.c: 4159
	 * The 'current' period is already promised to the current tasks, /Users/rubber/linux/kernel/sched/fair.c: 4160
	 * however the extra weight of the new task will slow them down a /Users/rubber/linux/kernel/sched/fair.c: 4161
	 * little, place the new task so that it fits in the slot that /Users/rubber/linux/kernel/sched/fair.c: 4162
	 * stays open at the end. /Users/rubber/linux/kernel/sched/fair.c: 4163
		/* /Users/rubber/linux/kernel/sched/fair.c: 4177
		 * Halve their sleep time's effect, to allow /Users/rubber/linux/kernel/sched/fair.c: 4178
		 * for a gentler effect of sleepers: /Users/rubber/linux/kernel/sched/fair.c: 4179
 * MIGRATION /Users/rubber/linux/kernel/sched/fair.c: 4196
 *	dequeue /Users/rubber/linux/kernel/sched/fair.c: 4198
 *	  update_curr() /Users/rubber/linux/kernel/sched/fair.c: 4199
 *	    update_min_vruntime() /Users/rubber/linux/kernel/sched/fair.c: 4200
 *	  vruntime -= min_vruntime /Users/rubber/linux/kernel/sched/fair.c: 4201
 *	enqueue /Users/rubber/linux/kernel/sched/fair.c: 4203
 *	  update_curr() /Users/rubber/linux/kernel/sched/fair.c: 4204
 *	    update_min_vruntime() /Users/rubber/linux/kernel/sched/fair.c: 4205
 *	  vruntime += min_vruntime /Users/rubber/linux/kernel/sched/fair.c: 4206
 * this way the vruntime transition between RQs is done when both /Users/rubber/linux/kernel/sched/fair.c: 4208
 * min_vruntime are up-to-date. /Users/rubber/linux/kernel/sched/fair.c: 4209
 * WAKEUP (remote) /Users/rubber/linux/kernel/sched/fair.c: 4211
 *	->migrate_task_rq_fair() (p->state == TASK_WAKING) /Users/rubber/linux/kernel/sched/fair.c: 4213
 *	  vruntime -= min_vruntime /Users/rubber/linux/kernel/sched/fair.c: 4214
 *	enqueue /Users/rubber/linux/kernel/sched/fair.c: 4216
 *	  update_curr() /Users/rubber/linux/kernel/sched/fair.c: 4217
 *	    update_min_vruntime() /Users/rubber/linux/kernel/sched/fair.c: 4218
 *	  vruntime += min_vruntime /Users/rubber/linux/kernel/sched/fair.c: 4219
 * this way we don't have the most up-to-date min_vruntime on the originating /Users/rubber/linux/kernel/sched/fair.c: 4221
 * CPU and an up-to-date min_vruntime on the destination CPU. /Users/rubber/linux/kernel/sched/fair.c: 4222
	/* /Users/rubber/linux/kernel/sched/fair.c: 4231
	 * If we're the current task, we must renormalise before calling /Users/rubber/linux/kernel/sched/fair.c: 4232
	 * update_curr(). /Users/rubber/linux/kernel/sched/fair.c: 4233
	/* /Users/rubber/linux/kernel/sched/fair.c: 4240
	 * Otherwise, renormalise after, such that we're placed at the current /Users/rubber/linux/kernel/sched/fair.c: 4241
	 * moment in time, instead of some random moment in the past. Being /Users/rubber/linux/kernel/sched/fair.c: 4242
	 * placed in the past could significantly boost this task to the /Users/rubber/linux/kernel/sched/fair.c: 4243
	 * fairness detriment of existing tasks. /Users/rubber/linux/kernel/sched/fair.c: 4244
	/* /Users/rubber/linux/kernel/sched/fair.c: 4249
	 * When enqueuing a sched_entity, we must: /Users/rubber/linux/kernel/sched/fair.c: 4250
	 *   - Update loads to have both entity and cfs_rq synced with now. /Users/rubber/linux/kernel/sched/fair.c: 4251
	 *   - Add its load to cfs_rq->runnable_avg /Users/rubber/linux/kernel/sched/fair.c: 4252
	 *   - For group_entity, update its weight to reflect the new share of /Users/rubber/linux/kernel/sched/fair.c: 4253
	 *     its group cfs_rq /Users/rubber/linux/kernel/sched/fair.c: 4254
	 *   - Add its new weight to cfs_rq->load.weight /Users/rubber/linux/kernel/sched/fair.c: 4255
	/* /Users/rubber/linux/kernel/sched/fair.c: 4272
	 * When bandwidth control is enabled, cfs might have been removed /Users/rubber/linux/kernel/sched/fair.c: 4273
	 * because of a parent been throttled but cfs->nr_running > 1. Try to /Users/rubber/linux/kernel/sched/fair.c: 4274
	 * add it unconditionally. /Users/rubber/linux/kernel/sched/fair.c: 4275
	/* /Users/rubber/linux/kernel/sched/fair.c: 4334
	 * Update run-time statistics of the 'current'. /Users/rubber/linux/kernel/sched/fair.c: 4335
	/* /Users/rubber/linux/kernel/sched/fair.c: 4339
	 * When dequeuing a sched_entity, we must: /Users/rubber/linux/kernel/sched/fair.c: 4340
	 *   - Update loads to have both entity and cfs_rq synced with now. /Users/rubber/linux/kernel/sched/fair.c: 4341
	 *   - Subtract its load from the cfs_rq->runnable_avg. /Users/rubber/linux/kernel/sched/fair.c: 4342
	 *   - Subtract its previous weight from cfs_rq->load.weight. /Users/rubber/linux/kernel/sched/fair.c: 4343
	 *   - For group entity, update its weight to reflect the new share /Users/rubber/linux/kernel/sched/fair.c: 4344
	 *     of its group cfs_rq. /Users/rubber/linux/kernel/sched/fair.c: 4345
	/* /Users/rubber/linux/kernel/sched/fair.c: 4359
	 * Normalize after update_curr(); which will also have moved /Users/rubber/linux/kernel/sched/fair.c: 4360
	 * min_vruntime if @se is the one holding it back. But before doing /Users/rubber/linux/kernel/sched/fair.c: 4361
	 * update_min_vruntime() again, which will discount @se's position and /Users/rubber/linux/kernel/sched/fair.c: 4362
	 * can move min_vruntime forward still more. /Users/rubber/linux/kernel/sched/fair.c: 4363
	/* /Users/rubber/linux/kernel/sched/fair.c: 4373
	 * Now advance min_vruntime if @se was the entity holding it back, /Users/rubber/linux/kernel/sched/fair.c: 4374
	 * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be /Users/rubber/linux/kernel/sched/fair.c: 4375
	 * put back on, and if we advance min_vruntime, we'll be placed back /Users/rubber/linux/kernel/sched/fair.c: 4376
	 * further than we started -- ie. we'll be penalized. /Users/rubber/linux/kernel/sched/fair.c: 4377
 * Preempt the current task with a newly woken task if needed: /Users/rubber/linux/kernel/sched/fair.c: 4384
		/* /Users/rubber/linux/kernel/sched/fair.c: 4397
		 * The current task ran long enough, ensure it doesn't get /Users/rubber/linux/kernel/sched/fair.c: 4398
		 * re-elected due to buddy favours. /Users/rubber/linux/kernel/sched/fair.c: 4399
	/* /Users/rubber/linux/kernel/sched/fair.c: 4405
	 * Ensure that a task that missed wakeup preemption by a /Users/rubber/linux/kernel/sched/fair.c: 4406
	 * narrow margin doesn't have to wait for a full slice. /Users/rubber/linux/kernel/sched/fair.c: 4407
	 * This also mitigates buddy induced latencies under load. /Users/rubber/linux/kernel/sched/fair.c: 4408
		/* /Users/rubber/linux/kernel/sched/fair.c: 4430
		 * Any task has to be enqueued before it get to execute on /Users/rubber/linux/kernel/sched/fair.c: 4431
		 * a CPU. So account for the time it spent waiting on the /Users/rubber/linux/kernel/sched/fair.c: 4432
		 * runqueue. /Users/rubber/linux/kernel/sched/fair.c: 4433
	/* /Users/rubber/linux/kernel/sched/fair.c: 4443
	 * Track our maximum slice length, if the CPU's load is at /Users/rubber/linux/kernel/sched/fair.c: 4444
	 * least twice that of our own weight (i.e. dont track it /Users/rubber/linux/kernel/sched/fair.c: 4445
	 * when there are only lesser-weight tasks around): /Users/rubber/linux/kernel/sched/fair.c: 4446
 * Pick the next process, keeping these things in mind, in this order: /Users/rubber/linux/kernel/sched/fair.c: 4465
 * 1) keep things fair between processes/task groups /Users/rubber/linux/kernel/sched/fair.c: 4466
 * 2) pick the "next" process, since someone really wants that to run /Users/rubber/linux/kernel/sched/fair.c: 4467
 * 3) pick the "last" process, for cache locality /Users/rubber/linux/kernel/sched/fair.c: 4468
 * 4) do not run the "skip" process, if something else is available /Users/rubber/linux/kernel/sched/fair.c: 4469
	/* /Users/rubber/linux/kernel/sched/fair.c: 4477
	 * If curr is set we have to see if its left of the leftmost entity /Users/rubber/linux/kernel/sched/fair.c: 4478
	 * still in the tree, provided there was anything in the tree at all. /Users/rubber/linux/kernel/sched/fair.c: 4479
	/* /Users/rubber/linux/kernel/sched/fair.c: 4486
	 * Avoid running the skip buddy, if running something else can /Users/rubber/linux/kernel/sched/fair.c: 4487
	 * be done without getting too unfair. /Users/rubber/linux/kernel/sched/fair.c: 4488
		/* /Users/rubber/linux/kernel/sched/fair.c: 4506
		 * Someone really wants this to run. If it's not unfair, run it. /Users/rubber/linux/kernel/sched/fair.c: 4507
		/* /Users/rubber/linux/kernel/sched/fair.c: 4511
		 * Prefer last buddy, try to return the CPU to a preempted task. /Users/rubber/linux/kernel/sched/fair.c: 4512
	/* /Users/rubber/linux/kernel/sched/fair.c: 4524
	 * If still on the runqueue then deactivate_task() /Users/rubber/linux/kernel/sched/fair.c: 4525
	 * was not called and update_curr() has to be done: /Users/rubber/linux/kernel/sched/fair.c: 4526
	/* /Users/rubber/linux/kernel/sched/fair.c: 4549
	 * Update run-time statistics of the 'current'. /Users/rubber/linux/kernel/sched/fair.c: 4550
	/* /Users/rubber/linux/kernel/sched/fair.c: 4554
	 * Ensure that runnable average is periodically updated. /Users/rubber/linux/kernel/sched/fair.c: 4555
	/* /Users/rubber/linux/kernel/sched/fair.c: 4561
	 * queued ticks are scheduled to match the slice, so don't bother /Users/rubber/linux/kernel/sched/fair.c: 4562
	 * validating it and just reschedule. /Users/rubber/linux/kernel/sched/fair.c: 4563
	/* /Users/rubber/linux/kernel/sched/fair.c: 4569
	 * don't let the period tick interfere with the hrtick preemption /Users/rubber/linux/kernel/sched/fair.c: 4570
 * CFS bandwidth control machinery /Users/rubber/linux/kernel/sched/fair.c: 4583
 * default period for cfs group bandwidth. /Users/rubber/linux/kernel/sched/fair.c: 4616
 * default: 0.1s, units: nanoseconds /Users/rubber/linux/kernel/sched/fair.c: 4617
 * Replenish runtime according to assigned quota. We use sched_clock_cpu /Users/rubber/linux/kernel/sched/fair.c: 4630
 * directly instead of rq->clock to avoid adding additional synchronization /Users/rubber/linux/kernel/sched/fair.c: 4631
 * around rq->lock. /Users/rubber/linux/kernel/sched/fair.c: 4632
 * requires cfs_b->lock /Users/rubber/linux/kernel/sched/fair.c: 4634
	/* /Users/rubber/linux/kernel/sched/fair.c: 4710
	 * if we're unable to extend our runtime we resched so that the active /Users/rubber/linux/kernel/sched/fair.c: 4711
	 * hierarchy can be throttled /Users/rubber/linux/kernel/sched/fair.c: 4712
 * Ensure that neither of the group entities corresponding to src_cpu or /Users/rubber/linux/kernel/sched/fair.c: 4739
 * dest_cpu are members of a throttled hierarchy when performing group /Users/rubber/linux/kernel/sched/fair.c: 4740
 * load-balance operations. /Users/rubber/linux/kernel/sched/fair.c: 4741
		/* /Users/rubber/linux/kernel/sched/fair.c: 4798
		 * We have raced with bandwidth becoming available, and if we /Users/rubber/linux/kernel/sched/fair.c: 4799
		 * actually throttled the timer might not unthrottle us for an /Users/rubber/linux/kernel/sched/fair.c: 4800
		 * entire period. We additionally needed to make sure that any /Users/rubber/linux/kernel/sched/fair.c: 4801
		 * subsequent check_cfs_rq_runtime calls agree not to throttle /Users/rubber/linux/kernel/sched/fair.c: 4802
		 * us, as we may commit to do cfs put_prev+pick_next, so we ask /Users/rubber/linux/kernel/sched/fair.c: 4803
		 * for 1ns of runtime rather than just check cfs_b. /Users/rubber/linux/kernel/sched/fair.c: 4804
	/* /Users/rubber/linux/kernel/sched/fair.c: 4866
	 * Note: distribution will already see us throttled via the /Users/rubber/linux/kernel/sched/fair.c: 4867
	 * throttled-list.  rq->lock protects completion. /Users/rubber/linux/kernel/sched/fair.c: 4868
		/* /Users/rubber/linux/kernel/sched/fair.c: 4939
		 * One parent has been throttled and cfs_rq removed from the /Users/rubber/linux/kernel/sched/fair.c: 4940
		 * list. Add it back to not break the leaf list. /Users/rubber/linux/kernel/sched/fair.c: 4941
	/* /Users/rubber/linux/kernel/sched/fair.c: 4951
	 * The cfs_rq_throttled() breaks in the above iteration can result in /Users/rubber/linux/kernel/sched/fair.c: 4952
	 * incomplete leaf list maintenance, resulting in triggering the /Users/rubber/linux/kernel/sched/fair.c: 4953
	 * assertion below. /Users/rubber/linux/kernel/sched/fair.c: 4954
 * Responsible for refilling a task_group's bandwidth and unthrottling its /Users/rubber/linux/kernel/sched/fair.c: 5012
 * cfs_rqs as appropriate. If there has been no activity within the last /Users/rubber/linux/kernel/sched/fair.c: 5013
 * period the timer is deactivated until scheduling resumes; cfs_b->idle is /Users/rubber/linux/kernel/sched/fair.c: 5014
 * used to track this state. /Users/rubber/linux/kernel/sched/fair.c: 5015
	/* /Users/rubber/linux/kernel/sched/fair.c: 5031
	 * idle depends on !throttled (for the case of a large deficit), and if /Users/rubber/linux/kernel/sched/fair.c: 5032
	 * we're going inactive then everything else can be deferred /Users/rubber/linux/kernel/sched/fair.c: 5033
	/* /Users/rubber/linux/kernel/sched/fair.c: 5047
	 * This check is repeated as we release cfs_b->lock while we unthrottle. /Users/rubber/linux/kernel/sched/fair.c: 5048
	/* /Users/rubber/linux/kernel/sched/fair.c: 5059
	 * While we are ensured activity in the period following an /Users/rubber/linux/kernel/sched/fair.c: 5060
	 * unthrottle, this also covers the case in which the new bandwidth is /Users/rubber/linux/kernel/sched/fair.c: 5061
	 * insufficient to cover the existing bandwidth deficit.  (Forcing the /Users/rubber/linux/kernel/sched/fair.c: 5062
	 * timer to remain active while there are any throttled entities.) /Users/rubber/linux/kernel/sched/fair.c: 5063
 * Are we near the end of the current quota period? /Users/rubber/linux/kernel/sched/fair.c: 5081
 * Requires cfs_b->lock for hrtimer_expires_remaining to be safe against the /Users/rubber/linux/kernel/sched/fair.c: 5083
 * hrtimer base being cleared by hrtimer_start. In the case of /Users/rubber/linux/kernel/sched/fair.c: 5084
 * migrate_hrtimers, base is never cleared, so we are fine. /Users/rubber/linux/kernel/sched/fair.c: 5085
 * This is done with a timer (instead of inline with bandwidth return) since /Users/rubber/linux/kernel/sched/fair.c: 5158
 * it's necessary to juggle rq->locks to unthrottle their respective cfs_rqs. /Users/rubber/linux/kernel/sched/fair.c: 5159
 * When a group wakes up we want to make sure that its quota is not already /Users/rubber/linux/kernel/sched/fair.c: 5187
 * expired/exceeded, otherwise it may be allowed to steal additional ticks of /Users/rubber/linux/kernel/sched/fair.c: 5188
 * runtime as update_curr() throttling can not trigger until it's on-rq. /Users/rubber/linux/kernel/sched/fair.c: 5189
	/* /Users/rubber/linux/kernel/sched/fair.c: 5236
	 * it's possible for a throttled entity to be forced into a running /Users/rubber/linux/kernel/sched/fair.c: 5237
	 * state (e.g. set_curr_task), in this case we're finished. /Users/rubber/linux/kernel/sched/fair.c: 5238
			/* /Users/rubber/linux/kernel/sched/fair.c: 5278
			 * Grow period by a factor of 2 to avoid losing precision. /Users/rubber/linux/kernel/sched/fair.c: 5279
			 * Precision loss in the quota/period ratio can cause __cfs_schedulable /Users/rubber/linux/kernel/sched/fair.c: 5280
			 * to fail. /Users/rubber/linux/kernel/sched/fair.c: 5281
 * Both these CPU hotplug callbacks race against unregister_fair_sched_group() /Users/rubber/linux/kernel/sched/fair.c: 5358
 * The race is harmless, since modifying bandwidth settings of unhooked group /Users/rubber/linux/kernel/sched/fair.c: 5360
 * bits doesn't do much. /Users/rubber/linux/kernel/sched/fair.c: 5361
		/* /Users/rubber/linux/kernel/sched/fair.c: 5397
		 * clock_task is not advancing so we just need to make sure /Users/rubber/linux/kernel/sched/fair.c: 5398
		 * there's some valid quota amount /Users/rubber/linux/kernel/sched/fair.c: 5399
		/* /Users/rubber/linux/kernel/sched/fair.c: 5402
		 * Offline rq is schedulable till CPU is completely disabled /Users/rubber/linux/kernel/sched/fair.c: 5403
		 * in take_cpu_down(), so we prevent new cfs throttling here. /Users/rubber/linux/kernel/sched/fair.c: 5404
 * CFS operations on tasks: /Users/rubber/linux/kernel/sched/fair.c: 5460
 * called from enqueue/dequeue and updates the hrtick when the /Users/rubber/linux/kernel/sched/fair.c: 5486
 * current task is from our class and nr_running is low enough /Users/rubber/linux/kernel/sched/fair.c: 5487
 * to matter. /Users/rubber/linux/kernel/sched/fair.c: 5488
 * Returns true if cfs_rq only has SCHED_IDLE entities enqueued. Note the use /Users/rubber/linux/kernel/sched/fair.c: 5538
 * of idle_nr_running, which does not consider idle descendants of normal /Users/rubber/linux/kernel/sched/fair.c: 5539
 * entities. /Users/rubber/linux/kernel/sched/fair.c: 5540
 * The enqueue_task method is called before nr_running is /Users/rubber/linux/kernel/sched/fair.c: 5556
 * increased. Here we update the fair scheduling stats and /Users/rubber/linux/kernel/sched/fair.c: 5557
 * then put the task into the rbtree: /Users/rubber/linux/kernel/sched/fair.c: 5558
	/* /Users/rubber/linux/kernel/sched/fair.c: 5568
	 * The code below (indirectly) updates schedutil which looks at /Users/rubber/linux/kernel/sched/fair.c: 5569
	 * the cfs_rq utilization to select a frequency. /Users/rubber/linux/kernel/sched/fair.c: 5570
	 * Let's add the task's estimated utilization to the cfs_rq's /Users/rubber/linux/kernel/sched/fair.c: 5571
	 * estimated utilization, before we update schedutil. /Users/rubber/linux/kernel/sched/fair.c: 5572
	/* /Users/rubber/linux/kernel/sched/fair.c: 5576
	 * If in_iowait is set, the code below may not trigger any cpufreq /Users/rubber/linux/kernel/sched/fair.c: 5577
	 * utilization updates, so do it here explicitly with the IOWAIT flag /Users/rubber/linux/kernel/sched/fair.c: 5578
	 * passed. /Users/rubber/linux/kernel/sched/fair.c: 5579
                * One parent has been throttled and cfs_rq removed from the /Users/rubber/linux/kernel/sched/fair.c: 5621
                * list. Add it back to not break the leaf list. /Users/rubber/linux/kernel/sched/fair.c: 5622
	/* /Users/rubber/linux/kernel/sched/fair.c: 5631
	 * Since new tasks are assigned an initial util_avg equal to /Users/rubber/linux/kernel/sched/fair.c: 5632
	 * half of the spare capacity of their CPU, tiny tasks have the /Users/rubber/linux/kernel/sched/fair.c: 5633
	 * ability to cross the overutilized threshold, which will /Users/rubber/linux/kernel/sched/fair.c: 5634
	 * result in the load balancer ruining all the task placement /Users/rubber/linux/kernel/sched/fair.c: 5635
	 * done by EAS. As a way to mitigate that effect, do not account /Users/rubber/linux/kernel/sched/fair.c: 5636
	 * for the first enqueue operation of new tasks during the /Users/rubber/linux/kernel/sched/fair.c: 5637
	 * overutilized flag detection. /Users/rubber/linux/kernel/sched/fair.c: 5638
	 * /Users/rubber/linux/kernel/sched/fair.c: 5639
	 * A better way of solving this problem would be to wait for /Users/rubber/linux/kernel/sched/fair.c: 5640
	 * the PELT signals of tasks to converge before taking them /Users/rubber/linux/kernel/sched/fair.c: 5641
	 * into account, but that is not straightforward to implement, /Users/rubber/linux/kernel/sched/fair.c: 5642
	 * and the following generally works well enough in practice. /Users/rubber/linux/kernel/sched/fair.c: 5643
		/* /Users/rubber/linux/kernel/sched/fair.c: 5650
		 * When bandwidth control is enabled; the cfs_rq_throttled() /Users/rubber/linux/kernel/sched/fair.c: 5651
		 * breaks in the above iteration can result in incomplete /Users/rubber/linux/kernel/sched/fair.c: 5652
		 * leaf list maintenance, resulting in triggering the assertion /Users/rubber/linux/kernel/sched/fair.c: 5653
		 * below. /Users/rubber/linux/kernel/sched/fair.c: 5654
 * The dequeue_task method is called before nr_running is /Users/rubber/linux/kernel/sched/fair.c: 5672
 * decreased. We remove the task from the rbtree and /Users/rubber/linux/kernel/sched/fair.c: 5673
 * update the fair scheduling stats: /Users/rubber/linux/kernel/sched/fair.c: 5674
			/* /Users/rubber/linux/kernel/sched/fair.c: 5704
			 * Bias pick_next to pick a task from this cfs_rq, as /Users/rubber/linux/kernel/sched/fair.c: 5705
			 * p is sleeping when it is within its sched_slice. /Users/rubber/linux/kernel/sched/fair.c: 5706
 * cpu_load_without - compute CPU load without any contributions from *p /Users/rubber/linux/kernel/sched/fair.c: 5771
 * @cpu: the CPU which load is requested /Users/rubber/linux/kernel/sched/fair.c: 5772
 * @p: the task which load should be discounted /Users/rubber/linux/kernel/sched/fair.c: 5773
 * The load of a CPU is defined by the load of tasks currently enqueued on that /Users/rubber/linux/kernel/sched/fair.c: 5775
 * CPU as well as tasks which are currently sleeping after an execution on that /Users/rubber/linux/kernel/sched/fair.c: 5776
 * CPU. /Users/rubber/linux/kernel/sched/fair.c: 5777
 * This method returns the load of the specified CPU by discounting the load of /Users/rubber/linux/kernel/sched/fair.c: 5779
 * the specified task, whenever the task is currently contributing to the CPU /Users/rubber/linux/kernel/sched/fair.c: 5780
 * load. /Users/rubber/linux/kernel/sched/fair.c: 5781
	/* /Users/rubber/linux/kernel/sched/fair.c: 5831
	 * Only decay a single time; tasks that have less then 1 wakeup per /Users/rubber/linux/kernel/sched/fair.c: 5832
	 * jiffy will not have built up many flips. /Users/rubber/linux/kernel/sched/fair.c: 5833
 * Detect M:N waker/wakee relationships via a switching-frequency heuristic. /Users/rubber/linux/kernel/sched/fair.c: 5847
 * A waker of many should wake a different task than the one last awakened /Users/rubber/linux/kernel/sched/fair.c: 5849
 * at a frequency roughly N times higher than one of its wakees. /Users/rubber/linux/kernel/sched/fair.c: 5850
 * In order to determine whether we should let the load spread vs consolidating /Users/rubber/linux/kernel/sched/fair.c: 5852
 * to shared cache, we look for a minimum 'flip' frequency of llc_size in one /Users/rubber/linux/kernel/sched/fair.c: 5853
 * partner, and a factor of lls_size higher frequency in the other. /Users/rubber/linux/kernel/sched/fair.c: 5854
 * With both conditions met, we can be relatively sure that the relationship is /Users/rubber/linux/kernel/sched/fair.c: 5856
 * non-monogamous, with partner count exceeding socket size. /Users/rubber/linux/kernel/sched/fair.c: 5857
 * Waker/wakee being client/server, worker/dispatcher, interrupt source or /Users/rubber/linux/kernel/sched/fair.c: 5859
 * whatever is irrelevant, spread criteria is apparent partner count exceeds /Users/rubber/linux/kernel/sched/fair.c: 5860
 * socket size. /Users/rubber/linux/kernel/sched/fair.c: 5861
 * The purpose of wake_affine() is to quickly determine on which CPU we can run /Users/rubber/linux/kernel/sched/fair.c: 5877
 * soonest. For the purpose of speed we only consider the waking and previous /Users/rubber/linux/kernel/sched/fair.c: 5878
 * CPU. /Users/rubber/linux/kernel/sched/fair.c: 5879
 * wake_affine_idle() - only considers 'now', it check if the waking CPU is /Users/rubber/linux/kernel/sched/fair.c: 5881
 *			cache-affine and is (or	will be) idle. /Users/rubber/linux/kernel/sched/fair.c: 5882
 * wake_affine_weight() - considers the weight to reflect the average /Users/rubber/linux/kernel/sched/fair.c: 5884
 *			  scheduling latency of the CPUs. This seems to work /Users/rubber/linux/kernel/sched/fair.c: 5885
 *			  for the overloaded case. /Users/rubber/linux/kernel/sched/fair.c: 5886
	/* /Users/rubber/linux/kernel/sched/fair.c: 5891
	 * If this_cpu is idle, it implies the wakeup is from interrupt /Users/rubber/linux/kernel/sched/fair.c: 5892
	 * context. Only allow the move if cache is shared. Otherwise an /Users/rubber/linux/kernel/sched/fair.c: 5893
	 * interrupt intensive workload could force all tasks onto one /Users/rubber/linux/kernel/sched/fair.c: 5894
	 * node depending on the IO topology or IRQ affinity settings. /Users/rubber/linux/kernel/sched/fair.c: 5895
	 * /Users/rubber/linux/kernel/sched/fair.c: 5896
	 * If the prev_cpu is idle and cache affine then avoid a migration. /Users/rubber/linux/kernel/sched/fair.c: 5897
	 * There is no guarantee that the cache hot data from an interrupt /Users/rubber/linux/kernel/sched/fair.c: 5898
	 * is more important than cache hot data on the prev_cpu and from /Users/rubber/linux/kernel/sched/fair.c: 5899
	 * a cpufreq perspective, it's better to have higher utilisation /Users/rubber/linux/kernel/sched/fair.c: 5900
	 * on one CPU. /Users/rubber/linux/kernel/sched/fair.c: 5901
	/* /Users/rubber/linux/kernel/sched/fair.c: 5946
	 * If sync, adjust the weight of prev_eff_load such that if /Users/rubber/linux/kernel/sched/fair.c: 5947
	 * prev_eff == this_eff that select_idle_sibling() will consider /Users/rubber/linux/kernel/sched/fair.c: 5948
	 * stacking the wakee on top of the waker if no other CPU is /Users/rubber/linux/kernel/sched/fair.c: 5949
	 * idle. /Users/rubber/linux/kernel/sched/fair.c: 5950
 * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group. /Users/rubber/linux/kernel/sched/fair.c: 5982
				/* /Users/rubber/linux/kernel/sched/fair.c: 6011
				 * We give priority to a CPU whose idle state /Users/rubber/linux/kernel/sched/fair.c: 6012
				 * has the smallest exit latency irrespective /Users/rubber/linux/kernel/sched/fair.c: 6013
				 * of any idle timestamp. /Users/rubber/linux/kernel/sched/fair.c: 6014
				/* /Users/rubber/linux/kernel/sched/fair.c: 6021
				 * If equal or no active idle state, then /Users/rubber/linux/kernel/sched/fair.c: 6022
				 * the most recently idled CPU might have /Users/rubber/linux/kernel/sched/fair.c: 6023
				 * a warmer cache. /Users/rubber/linux/kernel/sched/fair.c: 6024
	/* /Users/rubber/linux/kernel/sched/fair.c: 6049
	 * We need task's util for cpu_util_without, sync it up to /Users/rubber/linux/kernel/sched/fair.c: 6050
	 * prev_cpu's last_update_time. /Users/rubber/linux/kernel/sched/fair.c: 6051
 * Scans the local SMT mask to see if the entire core is idle, and records this /Users/rubber/linux/kernel/sched/fair.c: 6128
 * information in sd_llc_shared->has_idle_cores. /Users/rubber/linux/kernel/sched/fair.c: 6129
 * Since SMT siblings share all cache levels, inspecting this limited remote /Users/rubber/linux/kernel/sched/fair.c: 6131
 * state should be fairly cheap. /Users/rubber/linux/kernel/sched/fair.c: 6132
 * Scan the entire LLC domain for idle cores; this dynamically switches off if /Users/rubber/linux/kernel/sched/fair.c: 6157
 * there are no idle cores left in the system; tracked through /Users/rubber/linux/kernel/sched/fair.c: 6158
 * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above. /Users/rubber/linux/kernel/sched/fair.c: 6159
 * Scan the local SMT mask for idle CPUs. /Users/rubber/linux/kernel/sched/fair.c: 6193
 * Scan the LLC domain for idle CPUs; this is dynamically regulated by /Users/rubber/linux/kernel/sched/fair.c: 6234
 * comparing the average scan cost (tracked in sd->avg_scan_cost) against the /Users/rubber/linux/kernel/sched/fair.c: 6235
 * average idle time for this rq (as found in rq->avg_idle). /Users/rubber/linux/kernel/sched/fair.c: 6236
		/* /Users/rubber/linux/kernel/sched/fair.c: 6257
		 * If we're busy, the assumption that the last idle period /Users/rubber/linux/kernel/sched/fair.c: 6258
		 * predicts the future is flawed; age away the remaining /Users/rubber/linux/kernel/sched/fair.c: 6259
		 * predicted idle time. /Users/rubber/linux/kernel/sched/fair.c: 6260
		/* /Users/rubber/linux/kernel/sched/fair.c: 6302
		 * Account for the scan cost of wakeups against the average /Users/rubber/linux/kernel/sched/fair.c: 6303
		 * idle time. /Users/rubber/linux/kernel/sched/fair.c: 6304
 * Scan the asym_capacity domain for idle CPUs; pick the first idle one on which /Users/rubber/linux/kernel/sched/fair.c: 6315
 * the task fits. If no CPU is big enough, but there are idle ones, try to /Users/rubber/linux/kernel/sched/fair.c: 6316
 * maximize capacity. /Users/rubber/linux/kernel/sched/fair.c: 6317
 * Try and locate an idle core/thread in the LLC cache domain. /Users/rubber/linux/kernel/sched/fair.c: 6357
	/* /Users/rubber/linux/kernel/sched/fair.c: 6366
	 * On asymmetric system, update task utilization because we will check /Users/rubber/linux/kernel/sched/fair.c: 6367
	 * that the task fits with cpu's capacity. /Users/rubber/linux/kernel/sched/fair.c: 6368
	/* /Users/rubber/linux/kernel/sched/fair.c: 6375
	 * per-cpu select_idle_mask usage /Users/rubber/linux/kernel/sched/fair.c: 6376
	/* /Users/rubber/linux/kernel/sched/fair.c: 6384
	 * If the previous CPU is cache affine and idle, don't be stupid: /Users/rubber/linux/kernel/sched/fair.c: 6385
	/* /Users/rubber/linux/kernel/sched/fair.c: 6392
	 * Allow a per-cpu kthread to stack with the wakee if the /Users/rubber/linux/kernel/sched/fair.c: 6393
	 * kworker thread and the tasks previous CPUs are the same. /Users/rubber/linux/kernel/sched/fair.c: 6394
	 * The assumption is that the wakee queued work for the /Users/rubber/linux/kernel/sched/fair.c: 6395
	 * per-cpu kthread that is now complete and the wakeup is /Users/rubber/linux/kernel/sched/fair.c: 6396
	 * essentially a sync wakeup. An obvious example of this /Users/rubber/linux/kernel/sched/fair.c: 6397
	 * pattern is IO completions. /Users/rubber/linux/kernel/sched/fair.c: 6398
	/* /Users/rubber/linux/kernel/sched/fair.c: 6418
	 * For asymmetric CPU capacity systems, our domain of interest is /Users/rubber/linux/kernel/sched/fair.c: 6419
	 * sd_asym_cpucapacity rather than sd_llc. /Users/rubber/linux/kernel/sched/fair.c: 6420
		/* /Users/rubber/linux/kernel/sched/fair.c: 6424
		 * On an asymmetric CPU capacity system where an exclusive /Users/rubber/linux/kernel/sched/fair.c: 6425
		 * cpuset defines a symmetric island (i.e. one unique /Users/rubber/linux/kernel/sched/fair.c: 6426
		 * capacity_orig value through the cpuset), the key will be set /Users/rubber/linux/kernel/sched/fair.c: 6427
		 * but the CPUs within that cpuset will not have a domain with /Users/rubber/linux/kernel/sched/fair.c: 6428
		 * SD_ASYM_CPUCAPACITY. These should follow the usual symmetric /Users/rubber/linux/kernel/sched/fair.c: 6429
		 * capacity path. /Users/rubber/linux/kernel/sched/fair.c: 6430
 * cpu_util - Estimates the amount of capacity of a CPU used by CFS tasks. /Users/rubber/linux/kernel/sched/fair.c: 6460
 * @cpu: the CPU to get the utilization of /Users/rubber/linux/kernel/sched/fair.c: 6461
 * The unit of the return value must be the one of capacity so we can compare /Users/rubber/linux/kernel/sched/fair.c: 6463
 * the utilization with the capacity of the CPU that is available for CFS task /Users/rubber/linux/kernel/sched/fair.c: 6464
 * (ie cpu_capacity). /Users/rubber/linux/kernel/sched/fair.c: 6465
 * cfs_rq.avg.util_avg is the sum of running time of runnable tasks plus the /Users/rubber/linux/kernel/sched/fair.c: 6467
 * recent utilization of currently non-runnable tasks on a CPU. It represents /Users/rubber/linux/kernel/sched/fair.c: 6468
 * the amount of utilization of a CPU in the range [0..capacity_orig] where /Users/rubber/linux/kernel/sched/fair.c: 6469
 * capacity_orig is the cpu_capacity available at the highest frequency /Users/rubber/linux/kernel/sched/fair.c: 6470
 * (arch_scale_freq_capacity()). /Users/rubber/linux/kernel/sched/fair.c: 6471
 * The utilization of a CPU converges towards a sum equal to or less than the /Users/rubber/linux/kernel/sched/fair.c: 6472
 * current capacity (capacity_curr <= capacity_orig) of the CPU because it is /Users/rubber/linux/kernel/sched/fair.c: 6473
 * the running time on this CPU scaled by capacity_curr. /Users/rubber/linux/kernel/sched/fair.c: 6474
 * The estimated utilization of a CPU is defined to be the maximum between its /Users/rubber/linux/kernel/sched/fair.c: 6476
 * cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks /Users/rubber/linux/kernel/sched/fair.c: 6477
 * currently RUNNABLE on that CPU. /Users/rubber/linux/kernel/sched/fair.c: 6478
 * This allows to properly represent the expected utilization of a CPU which /Users/rubber/linux/kernel/sched/fair.c: 6479
 * has just got a big task running since a long sleep period. At the same time /Users/rubber/linux/kernel/sched/fair.c: 6480
 * however it preserves the benefits of the "blocked utilization" in /Users/rubber/linux/kernel/sched/fair.c: 6481
 * describing the potential for other tasks waking up on the same CPU. /Users/rubber/linux/kernel/sched/fair.c: 6482
 * Nevertheless, cfs_rq.avg.util_avg can be higher than capacity_curr or even /Users/rubber/linux/kernel/sched/fair.c: 6484
 * higher than capacity_orig because of unfortunate rounding in /Users/rubber/linux/kernel/sched/fair.c: 6485
 * cfs.avg.util_avg or just after migrating tasks and new task wakeups until /Users/rubber/linux/kernel/sched/fair.c: 6486
 * the average stabilizes with the new running time. We need to check that the /Users/rubber/linux/kernel/sched/fair.c: 6487
 * utilization stays within the range of [0..capacity_orig] and cap it if /Users/rubber/linux/kernel/sched/fair.c: 6488
 * necessary. Without utilization capping, a group could be seen as overloaded /Users/rubber/linux/kernel/sched/fair.c: 6489
 * (CPU0 utilization at 121% + CPU1 utilization at 80%) whereas CPU1 has 20% of /Users/rubber/linux/kernel/sched/fair.c: 6490
 * available capacity. We allow utilization to overshoot capacity_curr (but not /Users/rubber/linux/kernel/sched/fair.c: 6491
 * capacity_orig) as it useful for predicting the capacity required after task /Users/rubber/linux/kernel/sched/fair.c: 6492
 * migrations (scheduler-driven DVFS). /Users/rubber/linux/kernel/sched/fair.c: 6493
 * Return: the (estimated) utilization for the specified CPU /Users/rubber/linux/kernel/sched/fair.c: 6495
 * cpu_util_without: compute cpu utilization without any contributions from *p /Users/rubber/linux/kernel/sched/fair.c: 6512
 * @cpu: the CPU which utilization is requested /Users/rubber/linux/kernel/sched/fair.c: 6513
 * @p: the task which utilization should be discounted /Users/rubber/linux/kernel/sched/fair.c: 6514
 * The utilization of a CPU is defined by the utilization of tasks currently /Users/rubber/linux/kernel/sched/fair.c: 6516
 * enqueued on that CPU as well as tasks which are currently sleeping after an /Users/rubber/linux/kernel/sched/fair.c: 6517
 * execution on that CPU. /Users/rubber/linux/kernel/sched/fair.c: 6518
 * This method returns the utilization of the specified CPU by discounting the /Users/rubber/linux/kernel/sched/fair.c: 6520
 * utilization of the specified task, whenever the task is currently /Users/rubber/linux/kernel/sched/fair.c: 6521
 * contributing to the CPU utilization. /Users/rubber/linux/kernel/sched/fair.c: 6522
	/* /Users/rubber/linux/kernel/sched/fair.c: 6539
	 * Covered cases: /Users/rubber/linux/kernel/sched/fair.c: 6540
	 * /Users/rubber/linux/kernel/sched/fair.c: 6541
	 * a) if *p is the only task sleeping on this CPU, then: /Users/rubber/linux/kernel/sched/fair.c: 6542
	 *      cpu_util (== task_util) > util_est (== 0) /Users/rubber/linux/kernel/sched/fair.c: 6543
	 *    and thus we return: /Users/rubber/linux/kernel/sched/fair.c: 6544
	 *      cpu_util_without = (cpu_util - task_util) = 0 /Users/rubber/linux/kernel/sched/fair.c: 6545
	 * /Users/rubber/linux/kernel/sched/fair.c: 6546
	 * b) if other tasks are SLEEPING on this CPU, which is now exiting /Users/rubber/linux/kernel/sched/fair.c: 6547
	 *    IDLE, then: /Users/rubber/linux/kernel/sched/fair.c: 6548
	 *      cpu_util >= task_util /Users/rubber/linux/kernel/sched/fair.c: 6549
	 *      cpu_util > util_est (== 0) /Users/rubber/linux/kernel/sched/fair.c: 6550
	 *    and thus we discount *p's blocked utilization to return: /Users/rubber/linux/kernel/sched/fair.c: 6551
	 *      cpu_util_without = (cpu_util - task_util) >= 0 /Users/rubber/linux/kernel/sched/fair.c: 6552
	 * /Users/rubber/linux/kernel/sched/fair.c: 6553
	 * c) if other tasks are RUNNABLE on that CPU and /Users/rubber/linux/kernel/sched/fair.c: 6554
	 *      util_est > cpu_util /Users/rubber/linux/kernel/sched/fair.c: 6555
	 *    then we use util_est since it returns a more restrictive /Users/rubber/linux/kernel/sched/fair.c: 6556
	 *    estimation of the spare capacity on that CPU, by just /Users/rubber/linux/kernel/sched/fair.c: 6557
	 *    considering the expected utilization of tasks already /Users/rubber/linux/kernel/sched/fair.c: 6558
	 *    runnable on that CPU. /Users/rubber/linux/kernel/sched/fair.c: 6559
	 * /Users/rubber/linux/kernel/sched/fair.c: 6560
	 * Cases a) and b) are covered by the above code, while case c) is /Users/rubber/linux/kernel/sched/fair.c: 6561
	 * covered by the following code when estimated utilization is /Users/rubber/linux/kernel/sched/fair.c: 6562
	 * enabled. /Users/rubber/linux/kernel/sched/fair.c: 6563
		/* /Users/rubber/linux/kernel/sched/fair.c: 6569
		 * Despite the following checks we still have a small window /Users/rubber/linux/kernel/sched/fair.c: 6570
		 * for a possible race, when an execl's select_task_rq_fair() /Users/rubber/linux/kernel/sched/fair.c: 6571
		 * races with LB's detach_task(): /Users/rubber/linux/kernel/sched/fair.c: 6572
		 * /Users/rubber/linux/kernel/sched/fair.c: 6573
		 *   detach_task() /Users/rubber/linux/kernel/sched/fair.c: 6574
		 *     p->on_rq = TASK_ON_RQ_MIGRATING; /Users/rubber/linux/kernel/sched/fair.c: 6575
		 *     ---------------------------------- A /Users/rubber/linux/kernel/sched/fair.c: 6576
		 *     deactivate_task()                   \ /Users/rubber/linux/kernel/sched/fair.c: 6577
		 *       dequeue_task()                     + RaceTime /Users/rubber/linux/kernel/sched/fair.c: 6578
		 *         util_est_dequeue()              / /Users/rubber/linux/kernel/sched/fair.c: 6579
		 *     ---------------------------------- B /Users/rubber/linux/kernel/sched/fair.c: 6580
		 * /Users/rubber/linux/kernel/sched/fair.c: 6581
		 * The additional check on "current == p" it's required to /Users/rubber/linux/kernel/sched/fair.c: 6582
		 * properly fix the execl regression and it helps in further /Users/rubber/linux/kernel/sched/fair.c: 6583
		 * reducing the chances for the above race. /Users/rubber/linux/kernel/sched/fair.c: 6584
	/* /Users/rubber/linux/kernel/sched/fair.c: 6592
	 * Utilization (estimated) can exceed the CPU capacity, thus let's /Users/rubber/linux/kernel/sched/fair.c: 6593
	 * clamp to the maximum CPU capacity to ensure consistency with /Users/rubber/linux/kernel/sched/fair.c: 6594
	 * the cpu_util call. /Users/rubber/linux/kernel/sched/fair.c: 6595
 * Predicts what cpu_util(@cpu) would return if @p was migrated (and enqueued) /Users/rubber/linux/kernel/sched/fair.c: 6601
 * to @dst_cpu. /Users/rubber/linux/kernel/sched/fair.c: 6602
	/* /Users/rubber/linux/kernel/sched/fair.c: 6609
	 * If @p migrates from @cpu to another, remove its contribution. Or, /Users/rubber/linux/kernel/sched/fair.c: 6610
	 * if @p migrates from another CPU to @cpu, add its contribution. In /Users/rubber/linux/kernel/sched/fair.c: 6611
	 * the other cases, @cpu is not impacted by the migration, so the /Users/rubber/linux/kernel/sched/fair.c: 6612
	 * util_avg should already be correct. /Users/rubber/linux/kernel/sched/fair.c: 6613
		/* /Users/rubber/linux/kernel/sched/fair.c: 6623
		 * During wake-up, the task isn't enqueued yet and doesn't /Users/rubber/linux/kernel/sched/fair.c: 6624
		 * appear in the cfs_rq->avg.util_est.enqueued of any rq, /Users/rubber/linux/kernel/sched/fair.c: 6625
		 * so just add it (if needed) to "simulate" what will be /Users/rubber/linux/kernel/sched/fair.c: 6626
		 * cpu_util() after the task has been enqueued. /Users/rubber/linux/kernel/sched/fair.c: 6627
 * compute_energy(): Estimates the energy that @pd would consume if @p was /Users/rubber/linux/kernel/sched/fair.c: 6639
 * migrated to @dst_cpu. compute_energy() predicts what will be the utilization /Users/rubber/linux/kernel/sched/fair.c: 6640
 * landscape of @pd's CPUs after the task migration, and uses the Energy Model /Users/rubber/linux/kernel/sched/fair.c: 6641
 * to compute what would be the energy if we decided to actually migrate that /Users/rubber/linux/kernel/sched/fair.c: 6642
 * task. /Users/rubber/linux/kernel/sched/fair.c: 6643
	/* /Users/rubber/linux/kernel/sched/fair.c: 6656
	 * The capacity state of CPUs of the current rd can be driven by CPUs /Users/rubber/linux/kernel/sched/fair.c: 6657
	 * of another rd if they belong to the same pd. So, account for the /Users/rubber/linux/kernel/sched/fair.c: 6658
	 * utilization of these CPUs too by masking pd with cpu_online_mask /Users/rubber/linux/kernel/sched/fair.c: 6659
	 * instead of the rd span. /Users/rubber/linux/kernel/sched/fair.c: 6660
	 * /Users/rubber/linux/kernel/sched/fair.c: 6661
	 * If an entire pd is outside of the current rd, it will not appear in /Users/rubber/linux/kernel/sched/fair.c: 6662
	 * its pd list and will not be accounted by compute_energy(). /Users/rubber/linux/kernel/sched/fair.c: 6663
		/* /Users/rubber/linux/kernel/sched/fair.c: 6670
		 * When @p is placed on @cpu: /Users/rubber/linux/kernel/sched/fair.c: 6671
		 * /Users/rubber/linux/kernel/sched/fair.c: 6672
		 * util_running = max(cpu_util, cpu_util_est) + /Users/rubber/linux/kernel/sched/fair.c: 6673
		 *		  max(task_util, _task_util_est) /Users/rubber/linux/kernel/sched/fair.c: 6674
		 * /Users/rubber/linux/kernel/sched/fair.c: 6675
		 * while cpu_util_next is: max(cpu_util + task_util, /Users/rubber/linux/kernel/sched/fair.c: 6676
		 *			       cpu_util_est + _task_util_est) /Users/rubber/linux/kernel/sched/fair.c: 6677
		/* /Users/rubber/linux/kernel/sched/fair.c: 6685
		 * Busy time computation: utilization clamping is not /Users/rubber/linux/kernel/sched/fair.c: 6686
		 * required since the ratio (sum_util / cpu_capacity) /Users/rubber/linux/kernel/sched/fair.c: 6687
		 * is already enough to scale the EM reported power /Users/rubber/linux/kernel/sched/fair.c: 6688
		 * consumption at the (eventually clamped) cpu_capacity. /Users/rubber/linux/kernel/sched/fair.c: 6689
		/* /Users/rubber/linux/kernel/sched/fair.c: 6696
		 * Performance domain frequency: utilization clamping /Users/rubber/linux/kernel/sched/fair.c: 6697
		 * must be considered since it affects the selection /Users/rubber/linux/kernel/sched/fair.c: 6698
		 * of the performance domain frequency. /Users/rubber/linux/kernel/sched/fair.c: 6699
		 * NOTE: in case RT tasks are running, by default the /Users/rubber/linux/kernel/sched/fair.c: 6700
		 * FREQUENCY_UTIL's utilization can be max OPP. /Users/rubber/linux/kernel/sched/fair.c: 6701
 * find_energy_efficient_cpu(): Find most energy-efficient target CPU for the /Users/rubber/linux/kernel/sched/fair.c: 6712
 * waking task. find_energy_efficient_cpu() looks for the CPU with maximum /Users/rubber/linux/kernel/sched/fair.c: 6713
 * spare capacity in each performance domain and uses it as a potential /Users/rubber/linux/kernel/sched/fair.c: 6714
 * candidate to execute the task. Then, it uses the Energy Model to figure /Users/rubber/linux/kernel/sched/fair.c: 6715
 * out which of the CPU candidates is the most energy-efficient. /Users/rubber/linux/kernel/sched/fair.c: 6716
 * The rationale for this heuristic is as follows. In a performance domain, /Users/rubber/linux/kernel/sched/fair.c: 6718
 * all the most energy efficient CPU candidates (according to the Energy /Users/rubber/linux/kernel/sched/fair.c: 6719
 * Model) are those for which we'll request a low frequency. When there are /Users/rubber/linux/kernel/sched/fair.c: 6720
 * several CPUs for which the frequency request will be the same, we don't /Users/rubber/linux/kernel/sched/fair.c: 6721
 * have enough data to break the tie between them, because the Energy Model /Users/rubber/linux/kernel/sched/fair.c: 6722
 * only includes active power costs. With this model, if we assume that /Users/rubber/linux/kernel/sched/fair.c: 6723
 * frequency requests follow utilization (e.g. using schedutil), the CPU with /Users/rubber/linux/kernel/sched/fair.c: 6724
 * the maximum spare capacity in a performance domain is guaranteed to be among /Users/rubber/linux/kernel/sched/fair.c: 6725
 * the best candidates of the performance domain. /Users/rubber/linux/kernel/sched/fair.c: 6726
 * In practice, it could be preferable from an energy standpoint to pack /Users/rubber/linux/kernel/sched/fair.c: 6728
 * small tasks on a CPU in order to let other CPUs go in deeper idle states, /Users/rubber/linux/kernel/sched/fair.c: 6729
 * but that could also hurt our chances to go cluster idle, and we have no /Users/rubber/linux/kernel/sched/fair.c: 6730
 * ways to tell with the current Energy Model if this is actually a good /Users/rubber/linux/kernel/sched/fair.c: 6731
 * idea or not. So, find_energy_efficient_cpu() basically favors /Users/rubber/linux/kernel/sched/fair.c: 6732
 * cluster-packing, and spreading inside a cluster. That should at least be /Users/rubber/linux/kernel/sched/fair.c: 6733
 * a good thing for latency, and this is consistent with the idea that most /Users/rubber/linux/kernel/sched/fair.c: 6734
 * of the energy savings of EAS come from the asymmetry of the system, and /Users/rubber/linux/kernel/sched/fair.c: 6735
 * not so much from breaking the tie between identical CPUs. That's also the /Users/rubber/linux/kernel/sched/fair.c: 6736
 * reason why EAS is enabled in the topology code only for systems where /Users/rubber/linux/kernel/sched/fair.c: 6737
 * SD_ASYM_CPUCAPACITY is set. /Users/rubber/linux/kernel/sched/fair.c: 6738
 * NOTE: Forkees are not accepted in the energy-aware wake-up path because /Users/rubber/linux/kernel/sched/fair.c: 6740
 * they don't have any useful utilization data yet and it's not possible to /Users/rubber/linux/kernel/sched/fair.c: 6741
 * forecast their impact on energy consumption. Consequently, they will be /Users/rubber/linux/kernel/sched/fair.c: 6742
 * placed by find_idlest_cpu() on the least loaded CPU, which might turn out /Users/rubber/linux/kernel/sched/fair.c: 6743
 * to be energy-inefficient in some use-cases. The alternative would be to /Users/rubber/linux/kernel/sched/fair.c: 6744
 * bias new tasks towards specific types of CPUs first, or to try to infer /Users/rubber/linux/kernel/sched/fair.c: 6745
 * their util_avg from the parent task, but those heuristics could hurt /Users/rubber/linux/kernel/sched/fair.c: 6746
 * other use-cases too. So, until someone finds a better way to solve this, /Users/rubber/linux/kernel/sched/fair.c: 6747
 * let's keep things simple by re-using the existing slow path. /Users/rubber/linux/kernel/sched/fair.c: 6748
	/* /Users/rubber/linux/kernel/sched/fair.c: 6764
	 * Energy-aware wake-up happens on the lowest sched_domain starting /Users/rubber/linux/kernel/sched/fair.c: 6765
	 * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu. /Users/rubber/linux/kernel/sched/fair.c: 6766
			/* /Users/rubber/linux/kernel/sched/fair.c: 6795
			 * Skip CPUs that cannot satisfy the capacity request. /Users/rubber/linux/kernel/sched/fair.c: 6796
			 * IOW, placing the task there would make the CPU /Users/rubber/linux/kernel/sched/fair.c: 6797
			 * overutilized. Take uclamp into account to see how /Users/rubber/linux/kernel/sched/fair.c: 6798
			 * much capacity we can get out of the CPU; this is /Users/rubber/linux/kernel/sched/fair.c: 6799
			 * aligned with sched_cpu_util(). /Users/rubber/linux/kernel/sched/fair.c: 6800
				/* /Users/rubber/linux/kernel/sched/fair.c: 6810
				 * Find the CPU with the maximum spare capacity /Users/rubber/linux/kernel/sched/fair.c: 6811
				 * in the performance domain. /Users/rubber/linux/kernel/sched/fair.c: 6812
	/* /Users/rubber/linux/kernel/sched/fair.c: 6849
	 * Pick the best CPU if prev_cpu cannot be used, or if it saves at /Users/rubber/linux/kernel/sched/fair.c: 6850
	 * least 6% of the energy used by prev_cpu. /Users/rubber/linux/kernel/sched/fair.c: 6851
 * select_task_rq_fair: Select target runqueue for the waking task in domains /Users/rubber/linux/kernel/sched/fair.c: 6866
 * that have the relevant SD flag set. In practice, this is SD_BALANCE_WAKE, /Users/rubber/linux/kernel/sched/fair.c: 6867
 * SD_BALANCE_FORK, or SD_BALANCE_EXEC. /Users/rubber/linux/kernel/sched/fair.c: 6868
 * Balances load by selecting the idlest CPU in the idlest group, or under /Users/rubber/linux/kernel/sched/fair.c: 6870
 * certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set. /Users/rubber/linux/kernel/sched/fair.c: 6871
 * Returns the target CPU number. /Users/rubber/linux/kernel/sched/fair.c: 6873
	/* /Users/rubber/linux/kernel/sched/fair.c: 6886
	 * required for stable ->cpus_allowed /Users/rubber/linux/kernel/sched/fair.c: 6887
		/* /Users/rubber/linux/kernel/sched/fair.c: 6905
		 * If both 'cpu' and 'prev_cpu' are part of this domain, /Users/rubber/linux/kernel/sched/fair.c: 6906
		 * cpu is a valid SD_WAKE_AFFINE target. /Users/rubber/linux/kernel/sched/fair.c: 6907
 * Called immediately before a task is migrated to a new CPU; task_cpu(p) and /Users/rubber/linux/kernel/sched/fair.c: 6939
 * cfs_rq_of(p) references at time of call are still valid and identify the /Users/rubber/linux/kernel/sched/fair.c: 6940
 * previous CPU. The caller guarantees p->pi_lock or task_rq(p)->lock is held. /Users/rubber/linux/kernel/sched/fair.c: 6941
	/* /Users/rubber/linux/kernel/sched/fair.c: 6945
	 * As blocked tasks retain absolute vruntime the migration needs to /Users/rubber/linux/kernel/sched/fair.c: 6946
	 * deal with this by subtracting the old and adding the new /Users/rubber/linux/kernel/sched/fair.c: 6947
	 * min_vruntime -- the latter is done by enqueue_entity() when placing /Users/rubber/linux/kernel/sched/fair.c: 6948
	 * the task on the new runqueue. /Users/rubber/linux/kernel/sched/fair.c: 6949
		/* /Users/rubber/linux/kernel/sched/fair.c: 6972
		 * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old' /Users/rubber/linux/kernel/sched/fair.c: 6973
		 * rq->lock and can modify state directly. /Users/rubber/linux/kernel/sched/fair.c: 6974
		/* /Users/rubber/linux/kernel/sched/fair.c: 6980
		 * We are supposed to update the task to "current" time, then /Users/rubber/linux/kernel/sched/fair.c: 6981
		 * its up to date and ready to go to new CPU/cfs_rq. But we /Users/rubber/linux/kernel/sched/fair.c: 6982
		 * have difficulty in getting what current time is, so simply /Users/rubber/linux/kernel/sched/fair.c: 6983
		 * throw away the out-of-date time. This will result in the /Users/rubber/linux/kernel/sched/fair.c: 6984
		 * wakee task is less decayed, but giving the wakee more load /Users/rubber/linux/kernel/sched/fair.c: 6985
		 * sounds not bad. /Users/rubber/linux/kernel/sched/fair.c: 6986
	/* /Users/rubber/linux/kernel/sched/fair.c: 7019
	 * Since its curr running now, convert the gran from real-time /Users/rubber/linux/kernel/sched/fair.c: 7020
	 * to virtual-time in his units. /Users/rubber/linux/kernel/sched/fair.c: 7021
	 * /Users/rubber/linux/kernel/sched/fair.c: 7022
	 * By using 'se' instead of 'curr' we penalize light tasks, so /Users/rubber/linux/kernel/sched/fair.c: 7023
	 * they get preempted easier. That is, if 'se' < 'curr' then /Users/rubber/linux/kernel/sched/fair.c: 7024
	 * the resulting gran will be larger, therefore penalizing the /Users/rubber/linux/kernel/sched/fair.c: 7025
	 * lighter, if otoh 'se' > 'curr' then the resulting gran will /Users/rubber/linux/kernel/sched/fair.c: 7026
	 * be smaller, again penalizing the lighter task. /Users/rubber/linux/kernel/sched/fair.c: 7027
	 * /Users/rubber/linux/kernel/sched/fair.c: 7028
	 * This is especially important for buddies when the leftmost /Users/rubber/linux/kernel/sched/fair.c: 7029
	 * task is higher priority than the buddy. /Users/rubber/linux/kernel/sched/fair.c: 7030
 * Should 'se' preempt 'curr'. /Users/rubber/linux/kernel/sched/fair.c: 7036
 *             |s1 /Users/rubber/linux/kernel/sched/fair.c: 7038
 *        |s2 /Users/rubber/linux/kernel/sched/fair.c: 7039
 *   |s3 /Users/rubber/linux/kernel/sched/fair.c: 7040
 *         g /Users/rubber/linux/kernel/sched/fair.c: 7041
 *      |<--->|c /Users/rubber/linux/kernel/sched/fair.c: 7042
 *  w(c, s1) = -1 /Users/rubber/linux/kernel/sched/fair.c: 7044
 *  w(c, s2) =  0 /Users/rubber/linux/kernel/sched/fair.c: 7045
 *  w(c, s3) =  1 /Users/rubber/linux/kernel/sched/fair.c: 7046
 * Preempt the current task with a newly woken task if needed: /Users/rubber/linux/kernel/sched/fair.c: 7093
	/* /Users/rubber/linux/kernel/sched/fair.c: 7107
	 * This is possible from callers such as attach_tasks(), in which we /Users/rubber/linux/kernel/sched/fair.c: 7108
	 * unconditionally check_preempt_curr() after an enqueue (which may have /Users/rubber/linux/kernel/sched/fair.c: 7109
	 * lead to a throttle).  This both saves work and prevents false /Users/rubber/linux/kernel/sched/fair.c: 7110
	 * next-buddy nomination below. /Users/rubber/linux/kernel/sched/fair.c: 7111
	/* /Users/rubber/linux/kernel/sched/fair.c: 7121
	 * We can come here with TIF_NEED_RESCHED already set from new task /Users/rubber/linux/kernel/sched/fair.c: 7122
	 * wake up path. /Users/rubber/linux/kernel/sched/fair.c: 7123
	 * /Users/rubber/linux/kernel/sched/fair.c: 7124
	 * Note: this also catches the edge-case of curr being in a throttled /Users/rubber/linux/kernel/sched/fair.c: 7125
	 * group (e.g. via set_curr_task), since update_curr() (in the /Users/rubber/linux/kernel/sched/fair.c: 7126
	 * enqueue of curr) will have resulted in resched being set.  This /Users/rubber/linux/kernel/sched/fair.c: 7127
	 * prevents us from potentially nominating it as a false LAST_BUDDY /Users/rubber/linux/kernel/sched/fair.c: 7128
	 * below. /Users/rubber/linux/kernel/sched/fair.c: 7129
	/* /Users/rubber/linux/kernel/sched/fair.c: 7139
	 * Batch and idle tasks do not preempt non-idle tasks (their preemption /Users/rubber/linux/kernel/sched/fair.c: 7140
	 * is driven by the tick): /Users/rubber/linux/kernel/sched/fair.c: 7141
	/* /Users/rubber/linux/kernel/sched/fair.c: 7152
	 * Preempt an idle group in favor of a non-idle group (and don't preempt /Users/rubber/linux/kernel/sched/fair.c: 7153
	 * in the inverse case). /Users/rubber/linux/kernel/sched/fair.c: 7154
		/* /Users/rubber/linux/kernel/sched/fair.c: 7163
		 * Bias pick_next to pick the sched entity that is /Users/rubber/linux/kernel/sched/fair.c: 7164
		 * triggering this preemption. /Users/rubber/linux/kernel/sched/fair.c: 7165
	/* /Users/rubber/linux/kernel/sched/fair.c: 7176
	 * Only set the backward buddy when the current task is still /Users/rubber/linux/kernel/sched/fair.c: 7177
	 * on the rq. This can happen when a wakeup gets interleaved /Users/rubber/linux/kernel/sched/fair.c: 7178
	 * with schedule on the ->pre_schedule() or idle_balance() /Users/rubber/linux/kernel/sched/fair.c: 7179
	 * point, either of which can * drop the rq lock. /Users/rubber/linux/kernel/sched/fair.c: 7180
	 * /Users/rubber/linux/kernel/sched/fair.c: 7181
	 * Also, during early boot the idle thread is in the fair class, /Users/rubber/linux/kernel/sched/fair.c: 7182
	 * for obvious reasons its a bad idea to schedule back to it. /Users/rubber/linux/kernel/sched/fair.c: 7183
	/* /Users/rubber/linux/kernel/sched/fair.c: 7241
	 * Because of the set_next_buddy() in dequeue_task_fair() it is rather /Users/rubber/linux/kernel/sched/fair.c: 7242
	 * likely that a next task is from the same cgroup as the current. /Users/rubber/linux/kernel/sched/fair.c: 7243
	 * /Users/rubber/linux/kernel/sched/fair.c: 7244
	 * Therefore attempt to avoid putting and setting the entire cgroup /Users/rubber/linux/kernel/sched/fair.c: 7245
	 * hierarchy, only change the part that actually changes. /Users/rubber/linux/kernel/sched/fair.c: 7246
		/* /Users/rubber/linux/kernel/sched/fair.c: 7252
		 * Since we got here without doing put_prev_entity() we also /Users/rubber/linux/kernel/sched/fair.c: 7253
		 * have to consider cfs_rq->curr. If it is still a runnable /Users/rubber/linux/kernel/sched/fair.c: 7254
		 * entity, update_curr() will update its vruntime, otherwise /Users/rubber/linux/kernel/sched/fair.c: 7255
		 * forget we've ever seen it. /Users/rubber/linux/kernel/sched/fair.c: 7256
			/* /Users/rubber/linux/kernel/sched/fair.c: 7264
			 * This call to check_cfs_rq_runtime() will do the /Users/rubber/linux/kernel/sched/fair.c: 7265
			 * throttle and dequeue its entity in the parent(s). /Users/rubber/linux/kernel/sched/fair.c: 7266
			 * Therefore the nr_running test will indeed /Users/rubber/linux/kernel/sched/fair.c: 7267
			 * be correct. /Users/rubber/linux/kernel/sched/fair.c: 7268
	/* /Users/rubber/linux/kernel/sched/fair.c: 7286
	 * Since we haven't yet done put_prev_entity and if the selected task /Users/rubber/linux/kernel/sched/fair.c: 7287
	 * is a different task than we started out with, try and touch the /Users/rubber/linux/kernel/sched/fair.c: 7288
	 * least amount of cfs_rqs. /Users/rubber/linux/kernel/sched/fair.c: 7289
	/* /Users/rubber/linux/kernel/sched/fair.c: 7328
	 * Move the next running task to the front of /Users/rubber/linux/kernel/sched/fair.c: 7329
	 * the list, so our cfs_tasks list becomes MRU /Users/rubber/linux/kernel/sched/fair.c: 7330
	 * one. /Users/rubber/linux/kernel/sched/fair.c: 7331
	/* /Users/rubber/linux/kernel/sched/fair.c: 7349
	 * Because newidle_balance() releases (and re-acquires) rq->lock, it is /Users/rubber/linux/kernel/sched/fair.c: 7350
	 * possible for any higher priority task to appear. In that case we /Users/rubber/linux/kernel/sched/fair.c: 7351
	 * must re-start the pick_next_entity() loop. /Users/rubber/linux/kernel/sched/fair.c: 7352
	/* /Users/rubber/linux/kernel/sched/fair.c: 7360
	 * rq is about to be idle, check if we need to update the /Users/rubber/linux/kernel/sched/fair.c: 7361
	 * lost_idle_time of clock_pelt /Users/rubber/linux/kernel/sched/fair.c: 7362
 * Account for a descheduled task: /Users/rubber/linux/kernel/sched/fair.c: 7375
 * sched_yield() is very simple /Users/rubber/linux/kernel/sched/fair.c: 7389
 * The magic of dealing with the ->skip buddy is in pick_next_entity. /Users/rubber/linux/kernel/sched/fair.c: 7391
	/* /Users/rubber/linux/kernel/sched/fair.c: 7399
	 * Are we the only task in the tree? /Users/rubber/linux/kernel/sched/fair.c: 7400
		/* /Users/rubber/linux/kernel/sched/fair.c: 7409
		 * Update run-time statistics of the 'current'. /Users/rubber/linux/kernel/sched/fair.c: 7410
		/* /Users/rubber/linux/kernel/sched/fair.c: 7413
		 * Tell update_rq_clock() that we've just updated, /Users/rubber/linux/kernel/sched/fair.c: 7414
		 * so we don't do microscopic update in schedule() /Users/rubber/linux/kernel/sched/fair.c: 7415
		 * and double the fastpath cost. /Users/rubber/linux/kernel/sched/fair.c: 7416
 * Fair scheduling class load-balancing methods. /Users/rubber/linux/kernel/sched/fair.c: 7442
 * BASICS /Users/rubber/linux/kernel/sched/fair.c: 7444
 * The purpose of load-balancing is to achieve the same basic fairness the /Users/rubber/linux/kernel/sched/fair.c: 7446
 * per-CPU scheduler provides, namely provide a proportional amount of compute /Users/rubber/linux/kernel/sched/fair.c: 7447
 * time to each task. This is expressed in the following equation: /Users/rubber/linux/kernel/sched/fair.c: 7448
 *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1) /Users/rubber/linux/kernel/sched/fair.c: 7450
 * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight /Users/rubber/linux/kernel/sched/fair.c: 7452
 * W_i,0 is defined as: /Users/rubber/linux/kernel/sched/fair.c: 7453
 *   W_i,0 = \Sum_j w_i,j                                             (2) /Users/rubber/linux/kernel/sched/fair.c: 7455
 * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight /Users/rubber/linux/kernel/sched/fair.c: 7457
 * is derived from the nice value as per sched_prio_to_weight[]. /Users/rubber/linux/kernel/sched/fair.c: 7458
 * The weight average is an exponential decay average of the instantaneous /Users/rubber/linux/kernel/sched/fair.c: 7460
 * weight: /Users/rubber/linux/kernel/sched/fair.c: 7461
 *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3) /Users/rubber/linux/kernel/sched/fair.c: 7463
 * C_i is the compute capacity of CPU i, typically it is the /Users/rubber/linux/kernel/sched/fair.c: 7465
 * fraction of 'recent' time available for SCHED_OTHER task execution. But it /Users/rubber/linux/kernel/sched/fair.c: 7466
 * can also include other factors [XXX]. /Users/rubber/linux/kernel/sched/fair.c: 7467
 * To achieve this balance we define a measure of imbalance which follows /Users/rubber/linux/kernel/sched/fair.c: 7469
 * directly from (1): /Users/rubber/linux/kernel/sched/fair.c: 7470
 *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4) /Users/rubber/linux/kernel/sched/fair.c: 7472
 * We them move tasks around to minimize the imbalance. In the continuous /Users/rubber/linux/kernel/sched/fair.c: 7474
 * function space it is obvious this converges, in the discrete case we get /Users/rubber/linux/kernel/sched/fair.c: 7475
 * a few fun cases generally called infeasible weight scenarios. /Users/rubber/linux/kernel/sched/fair.c: 7476
 * [XXX expand on: /Users/rubber/linux/kernel/sched/fair.c: 7478
 *     - infeasible weights; /Users/rubber/linux/kernel/sched/fair.c: 7479
 *     - local vs global optima in the discrete case. ] /Users/rubber/linux/kernel/sched/fair.c: 7480
 * SCHED DOMAINS /Users/rubber/linux/kernel/sched/fair.c: 7483
 * In order to solve the imbalance equation (4), and avoid the obvious O(n^2) /Users/rubber/linux/kernel/sched/fair.c: 7485
 * for all i,j solution, we create a tree of CPUs that follows the hardware /Users/rubber/linux/kernel/sched/fair.c: 7486
 * topology where each level pairs two lower groups (or better). This results /Users/rubber/linux/kernel/sched/fair.c: 7487
 * in O(log n) layers. Furthermore we reduce the number of CPUs going up the /Users/rubber/linux/kernel/sched/fair.c: 7488
 * tree to only the first of the previous level and we decrease the frequency /Users/rubber/linux/kernel/sched/fair.c: 7489
 * of load-balance at each level inv. proportional to the number of CPUs in /Users/rubber/linux/kernel/sched/fair.c: 7490
 * the groups. /Users/rubber/linux/kernel/sched/fair.c: 7491
 * This yields: /Users/rubber/linux/kernel/sched/fair.c: 7493
 *     log_2 n     1     n /Users/rubber/linux/kernel/sched/fair.c: 7495
 *   \Sum       { --- * --- * 2^i } = O(n)                            (5) /Users/rubber/linux/kernel/sched/fair.c: 7496
 *     i = 0      2^i   2^i /Users/rubber/linux/kernel/sched/fair.c: 7497
 *                               `- size of each group /Users/rubber/linux/kernel/sched/fair.c: 7498
 *         |         |     `- number of CPUs doing load-balance /Users/rubber/linux/kernel/sched/fair.c: 7499
 *         |         `- freq /Users/rubber/linux/kernel/sched/fair.c: 7500
 *         `- sum over all levels /Users/rubber/linux/kernel/sched/fair.c: 7501
 * Coupled with a limit on how many tasks we can migrate every balance pass, /Users/rubber/linux/kernel/sched/fair.c: 7503
 * this makes (5) the runtime complexity of the balancer. /Users/rubber/linux/kernel/sched/fair.c: 7504
 * An important property here is that each CPU is still (indirectly) connected /Users/rubber/linux/kernel/sched/fair.c: 7506
 * to every other CPU in at most O(log n) steps: /Users/rubber/linux/kernel/sched/fair.c: 7507
 * The adjacency matrix of the resulting graph is given by: /Users/rubber/linux/kernel/sched/fair.c: 7509
 *             log_2 n /Users/rubber/linux/kernel/sched/fair.c: 7511
 *   A_i,j = \Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6) /Users/rubber/linux/kernel/sched/fair.c: 7512
 *             k = 0 /Users/rubber/linux/kernel/sched/fair.c: 7513
 * And you'll find that: /Users/rubber/linux/kernel/sched/fair.c: 7515
 *   A^(log_2 n)_i,j != 0  for all i,j                                (7) /Users/rubber/linux/kernel/sched/fair.c: 7517
 * Showing there's indeed a path between every CPU in at most O(log n) steps. /Users/rubber/linux/kernel/sched/fair.c: 7519
 * The task movement gives a factor of O(m), giving a convergence complexity /Users/rubber/linux/kernel/sched/fair.c: 7520
 * of: /Users/rubber/linux/kernel/sched/fair.c: 7521
 *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8) /Users/rubber/linux/kernel/sched/fair.c: 7523
 * WORK CONSERVING /Users/rubber/linux/kernel/sched/fair.c: 7526
 * In order to avoid CPUs going idle while there's still work to do, new idle /Users/rubber/linux/kernel/sched/fair.c: 7528
 * balancing is more aggressive and has the newly idle CPU iterate up the domain /Users/rubber/linux/kernel/sched/fair.c: 7529
 * tree itself instead of relying on other CPUs to bring it work. /Users/rubber/linux/kernel/sched/fair.c: 7530
 * This adds some complexity to both (5) and (8) but it reduces the total idle /Users/rubber/linux/kernel/sched/fair.c: 7532
 * time. /Users/rubber/linux/kernel/sched/fair.c: 7533
 * [XXX more?] /Users/rubber/linux/kernel/sched/fair.c: 7535
 * CGROUPS /Users/rubber/linux/kernel/sched/fair.c: 7538
 * Cgroups make a horror show out of (2), instead of a simple sum we get: /Users/rubber/linux/kernel/sched/fair.c: 7540
 *                                s_k,i /Users/rubber/linux/kernel/sched/fair.c: 7542
 *   W_i,0 = \Sum_j \Prod_k w_k * -----                               (9) /Users/rubber/linux/kernel/sched/fair.c: 7543
 *                                 S_k /Users/rubber/linux/kernel/sched/fair.c: 7544
 * Where /Users/rubber/linux/kernel/sched/fair.c: 7546
 *   s_k,i = \Sum_j w_i,j,k  and  S_k = \Sum_i s_k,i                 (10) /Users/rubber/linux/kernel/sched/fair.c: 7548
 * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i. /Users/rubber/linux/kernel/sched/fair.c: 7550
 * The big problem is S_k, its a global sum needed to compute a local (W_i) /Users/rubber/linux/kernel/sched/fair.c: 7552
 * property. /Users/rubber/linux/kernel/sched/fair.c: 7553
 * [XXX write more on how we solve this.. _after_ merging pjt's patches that /Users/rubber/linux/kernel/sched/fair.c: 7555
 *      rewrite all of this once again.] /Users/rubber/linux/kernel/sched/fair.c: 7556
 * 'group_type' describes the group of CPUs at the moment of load balancing. /Users/rubber/linux/kernel/sched/fair.c: 7564
 * The enum is ordered by pulling priority, with the group with lowest priority /Users/rubber/linux/kernel/sched/fair.c: 7566
 * first so the group_type can simply be compared when selecting the busiest /Users/rubber/linux/kernel/sched/fair.c: 7567
 * group. See update_sd_pick_busiest(). /Users/rubber/linux/kernel/sched/fair.c: 7568
	/* /Users/rubber/linux/kernel/sched/fair.c: 7573
	 * The group is fully used and the tasks don't compete for more CPU /Users/rubber/linux/kernel/sched/fair.c: 7574
	 * cycles. Nevertheless, some tasks might wait before running. /Users/rubber/linux/kernel/sched/fair.c: 7575
	/* /Users/rubber/linux/kernel/sched/fair.c: 7578
	 * SD_ASYM_CPUCAPACITY only: One task doesn't fit with CPU's capacity /Users/rubber/linux/kernel/sched/fair.c: 7579
	 * and must be migrated to a more powerful CPU. /Users/rubber/linux/kernel/sched/fair.c: 7580
	/* /Users/rubber/linux/kernel/sched/fair.c: 7583
	 * SD_ASYM_PACKING only: One local CPU with higher capacity is available, /Users/rubber/linux/kernel/sched/fair.c: 7584
	 * and the task should be migrated to it instead of running on the /Users/rubber/linux/kernel/sched/fair.c: 7585
	 * current CPU. /Users/rubber/linux/kernel/sched/fair.c: 7586
	/* /Users/rubber/linux/kernel/sched/fair.c: 7589
	 * The tasks' affinity constraints previously prevented the scheduler /Users/rubber/linux/kernel/sched/fair.c: 7590
	 * from balancing the load across the system. /Users/rubber/linux/kernel/sched/fair.c: 7591
	/* /Users/rubber/linux/kernel/sched/fair.c: 7594
	 * The CPU is overloaded and can't provide expected CPU cycles to all /Users/rubber/linux/kernel/sched/fair.c: 7595
	 * tasks. /Users/rubber/linux/kernel/sched/fair.c: 7596
 * Is this task likely cache-hot: /Users/rubber/linux/kernel/sched/fair.c: 7642
	/* /Users/rubber/linux/kernel/sched/fair.c: 7660
	 * Buddy candidates are cache hot: /Users/rubber/linux/kernel/sched/fair.c: 7661
	/* /Users/rubber/linux/kernel/sched/fair.c: 7671
	 * Don't migrate task if the task's cookie does not match /Users/rubber/linux/kernel/sched/fair.c: 7672
	 * with the destination CPU's core cookie. /Users/rubber/linux/kernel/sched/fair.c: 7673
 * Returns 1, if task migration degrades locality /Users/rubber/linux/kernel/sched/fair.c: 7688
 * Returns 0, if task migration improves locality i.e migration preferred. /Users/rubber/linux/kernel/sched/fair.c: 7689
 * Returns -1, if task migration is not affected by locality. /Users/rubber/linux/kernel/sched/fair.c: 7690
 * can_migrate_task - may task p from runqueue rq be migrated to this_cpu? /Users/rubber/linux/kernel/sched/fair.c: 7747
	/* /Users/rubber/linux/kernel/sched/fair.c: 7756
	 * We do not migrate tasks that are: /Users/rubber/linux/kernel/sched/fair.c: 7757
	 * 1) throttled_lb_pair, or /Users/rubber/linux/kernel/sched/fair.c: 7758
	 * 2) cannot be migrated to this CPU due to cpus_ptr, or /Users/rubber/linux/kernel/sched/fair.c: 7759
	 * 3) running (obviously), or /Users/rubber/linux/kernel/sched/fair.c: 7760
	 * 4) are cache-hot on their current CPU. /Users/rubber/linux/kernel/sched/fair.c: 7761
		/* /Users/rubber/linux/kernel/sched/fair.c: 7777
		 * Remember if this task can be migrated to any other CPU in /Users/rubber/linux/kernel/sched/fair.c: 7778
		 * our sched_group. We may want to revisit it if we couldn't /Users/rubber/linux/kernel/sched/fair.c: 7779
		 * meet load balance goals by pulling other tasks on src_cpu. /Users/rubber/linux/kernel/sched/fair.c: 7780
		 * /Users/rubber/linux/kernel/sched/fair.c: 7781
		 * Avoid computing new_dst_cpu /Users/rubber/linux/kernel/sched/fair.c: 7782
		 * - for NEWLY_IDLE /Users/rubber/linux/kernel/sched/fair.c: 7783
		 * - if we have already computed one in current iteration /Users/rubber/linux/kernel/sched/fair.c: 7784
		 * - if it's an active balance /Users/rubber/linux/kernel/sched/fair.c: 7785
	/* /Users/rubber/linux/kernel/sched/fair.c: 7811
	 * Aggressive migration if: /Users/rubber/linux/kernel/sched/fair.c: 7812
	 * 1) active balance /Users/rubber/linux/kernel/sched/fair.c: 7813
	 * 2) destination numa is preferred /Users/rubber/linux/kernel/sched/fair.c: 7814
	 * 3) task is cache cold, or /Users/rubber/linux/kernel/sched/fair.c: 7815
	 * 4) too many balance attempts have failed. /Users/rubber/linux/kernel/sched/fair.c: 7816
 * detach_task() -- detach the task for the migration specified in env /Users/rubber/linux/kernel/sched/fair.c: 7839
 * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as /Users/rubber/linux/kernel/sched/fair.c: 7850
 * part of active balancing operations within "domain". /Users/rubber/linux/kernel/sched/fair.c: 7851
 * Returns a task if successful and NULL otherwise. /Users/rubber/linux/kernel/sched/fair.c: 7853
		/* /Users/rubber/linux/kernel/sched/fair.c: 7868
		 * Right now, this is only the second place where /Users/rubber/linux/kernel/sched/fair.c: 7869
		 * lb_gained[env->idle] is updated (other is detach_tasks) /Users/rubber/linux/kernel/sched/fair.c: 7870
		 * so we can safely collect stats here rather than /Users/rubber/linux/kernel/sched/fair.c: 7871
		 * inside detach_tasks(). /Users/rubber/linux/kernel/sched/fair.c: 7872
 * detach_tasks() -- tries to detach up to imbalance load/util/tasks from /Users/rubber/linux/kernel/sched/fair.c: 7883
 * busiest_rq, as part of a balancing operation within domain "sd". /Users/rubber/linux/kernel/sched/fair.c: 7884
 * Returns number of detached tasks if successful and 0 otherwise. /Users/rubber/linux/kernel/sched/fair.c: 7886
	/* /Users/rubber/linux/kernel/sched/fair.c: 7897
	 * Source run queue has been emptied by another CPU, clear /Users/rubber/linux/kernel/sched/fair.c: 7898
	 * LBF_ALL_PINNED flag as we will not test any task. /Users/rubber/linux/kernel/sched/fair.c: 7899
		/* /Users/rubber/linux/kernel/sched/fair.c: 7910
		 * We don't want to steal all, otherwise we may be treated likewise, /Users/rubber/linux/kernel/sched/fair.c: 7911
		 * which could at worst lead to a livelock crash. /Users/rubber/linux/kernel/sched/fair.c: 7912
			/* /Users/rubber/linux/kernel/sched/fair.c: 7936
			 * Depending of the number of CPUs and tasks and the /Users/rubber/linux/kernel/sched/fair.c: 7937
			 * cgroup hierarchy, task_h_load() can return a null /Users/rubber/linux/kernel/sched/fair.c: 7938
			 * value. Make sure that env->imbalance decreases /Users/rubber/linux/kernel/sched/fair.c: 7939
			 * otherwise detach_tasks() will stop only after /Users/rubber/linux/kernel/sched/fair.c: 7940
			 * detaching up to loop_max tasks. /Users/rubber/linux/kernel/sched/fair.c: 7941
			/* /Users/rubber/linux/kernel/sched/fair.c: 7949
			 * Make sure that we don't migrate too much load. /Users/rubber/linux/kernel/sched/fair.c: 7950
			 * Nevertheless, let relax the constraint if /Users/rubber/linux/kernel/sched/fair.c: 7951
			 * scheduler fails to find a good waiting task to /Users/rubber/linux/kernel/sched/fair.c: 7952
			 * migrate. /Users/rubber/linux/kernel/sched/fair.c: 7953
		/* /Users/rubber/linux/kernel/sched/fair.c: 7989
		 * NEWIDLE balancing is a source of latency, so preemptible /Users/rubber/linux/kernel/sched/fair.c: 7990
		 * kernels will stop after the first task is detached to minimize /Users/rubber/linux/kernel/sched/fair.c: 7991
		 * the critical section. /Users/rubber/linux/kernel/sched/fair.c: 7992
		/* /Users/rubber/linux/kernel/sched/fair.c: 7998
		 * We only want to steal up to the prescribed amount of /Users/rubber/linux/kernel/sched/fair.c: 7999
		 * load/util/tasks. /Users/rubber/linux/kernel/sched/fair.c: 8000
	/* /Users/rubber/linux/kernel/sched/fair.c: 8010
	 * Right now, this is one of only two places we collect this stat /Users/rubber/linux/kernel/sched/fair.c: 8011
	 * so we can safely collect detach_one_task() stats here rather /Users/rubber/linux/kernel/sched/fair.c: 8012
	 * than inside detach_one_task(). /Users/rubber/linux/kernel/sched/fair.c: 8013
 * attach_task() -- attach the task detached by detach_task() to its new rq. /Users/rubber/linux/kernel/sched/fair.c: 8021
 * attach_one_task() -- attaches the task returned from detach_one_task() to /Users/rubber/linux/kernel/sched/fair.c: 8033
 * its new rq. /Users/rubber/linux/kernel/sched/fair.c: 8034
 * attach_tasks() -- attaches all tasks detached by detach_tasks() to their /Users/rubber/linux/kernel/sched/fair.c: 8047
 * new rq. /Users/rubber/linux/kernel/sched/fair.c: 8048
	/* /Users/rubber/linux/kernel/sched/fair.c: 8124
	 * update_load_avg() can call cpufreq_update_util(). Make sure that RT, /Users/rubber/linux/kernel/sched/fair.c: 8125
	 * DL and IRQ signals have been updated before updating CFS. /Users/rubber/linux/kernel/sched/fair.c: 8126
	/* /Users/rubber/linux/kernel/sched/fair.c: 8151
	 * Iterates the task_group tree in a bottom up fashion, see /Users/rubber/linux/kernel/sched/fair.c: 8152
	 * list_add_leaf_cfs_rq() for details. /Users/rubber/linux/kernel/sched/fair.c: 8153
		/* /Users/rubber/linux/kernel/sched/fair.c: 8170
		 * There can be a lot of idle CPU cgroups.  Don't let fully /Users/rubber/linux/kernel/sched/fair.c: 8171
		 * decayed cfs_rqs linger on the list. /Users/rubber/linux/kernel/sched/fair.c: 8172
 * Compute the hierarchical load factor for cfs_rq and all its ascendants. /Users/rubber/linux/kernel/sched/fair.c: 8186
 * This needs to be done in a top-down fashion because the load of a child /Users/rubber/linux/kernel/sched/fair.c: 8187
 * group is a fraction of its parents load. /Users/rubber/linux/kernel/sched/fair.c: 8188
 * sg_lb_stats - stats of a sched_group required for load_balancing /Users/rubber/linux/kernel/sched/fair.c: 8272
 * sd_lb_stats - Structure to store the statistics of a sched_domain /Users/rubber/linux/kernel/sched/fair.c: 8294
 *		 during load balancing. /Users/rubber/linux/kernel/sched/fair.c: 8295
	/* /Users/rubber/linux/kernel/sched/fair.c: 8311
	 * Skimp on the clearing to avoid duplicate work. We can avoid clearing /Users/rubber/linux/kernel/sched/fair.c: 8312
	 * local_stat because update_sg_lb_stats() does a full clear/assignment. /Users/rubber/linux/kernel/sched/fair.c: 8313
	 * We must however set busiest_stat::group_type and /Users/rubber/linux/kernel/sched/fair.c: 8314
	 * busiest_stat::idle_cpus to the worst busiest group because /Users/rubber/linux/kernel/sched/fair.c: 8315
	 * update_sd_pick_busiest() reads these before assignment. /Users/rubber/linux/kernel/sched/fair.c: 8316
	/* /Users/rubber/linux/kernel/sched/fair.c: 8342
	 * avg_rt.util_avg and avg_dl.util_avg track binary signals /Users/rubber/linux/kernel/sched/fair.c: 8343
	 * (running and not running) with weights 0 and 1024 respectively. /Users/rubber/linux/kernel/sched/fair.c: 8344
	 * avg_thermal.load_avg tracks thermal pressure and the weighted /Users/rubber/linux/kernel/sched/fair.c: 8345
	 * average uses the actual delta max capacity(load). /Users/rubber/linux/kernel/sched/fair.c: 8346
		/* /Users/rubber/linux/kernel/sched/fair.c: 8399
		 * SD_OVERLAP domains cannot assume that child groups /Users/rubber/linux/kernel/sched/fair.c: 8400
		 * span the current group. /Users/rubber/linux/kernel/sched/fair.c: 8401
		/* /Users/rubber/linux/kernel/sched/fair.c: 8412
		 * !SD_OVERLAP domains can assume that child groups /Users/rubber/linux/kernel/sched/fair.c: 8413
		 * span the current group. /Users/rubber/linux/kernel/sched/fair.c: 8414
 * Check whether the capacity of the rq has been noticeably reduced by side /Users/rubber/linux/kernel/sched/fair.c: 8434
 * activity. The imbalance_pct is used for the threshold. /Users/rubber/linux/kernel/sched/fair.c: 8435
 * Return true is the capacity is reduced /Users/rubber/linux/kernel/sched/fair.c: 8436
 * Check whether a rq has a misfit task and if it looks like we can actually /Users/rubber/linux/kernel/sched/fair.c: 8446
 * help that task: we can migrate the task to a CPU of higher capacity, or /Users/rubber/linux/kernel/sched/fair.c: 8447
 * the task's current CPU is heavily pressured. /Users/rubber/linux/kernel/sched/fair.c: 8448
 * Group imbalance indicates (and tries to solve) the problem where balancing /Users/rubber/linux/kernel/sched/fair.c: 8458
 * groups is inadequate due to ->cpus_ptr constraints. /Users/rubber/linux/kernel/sched/fair.c: 8459
 * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a /Users/rubber/linux/kernel/sched/fair.c: 8461
 * cpumask covering 1 CPU of the first group and 3 CPUs of the second group. /Users/rubber/linux/kernel/sched/fair.c: 8462
 * Something like: /Users/rubber/linux/kernel/sched/fair.c: 8463
 *	{ 0 1 2 3 } { 4 5 6 7 } /Users/rubber/linux/kernel/sched/fair.c: 8465
 *	        *     * * * /Users/rubber/linux/kernel/sched/fair.c: 8466
 * If we were to balance group-wise we'd place two tasks in the first group and /Users/rubber/linux/kernel/sched/fair.c: 8468
 * two tasks in the second group. Clearly this is undesired as it will overload /Users/rubber/linux/kernel/sched/fair.c: 8469
 * cpu 3 and leave one of the CPUs in the second group unused. /Users/rubber/linux/kernel/sched/fair.c: 8470
 * The current solution to this issue is detecting the skew in the first group /Users/rubber/linux/kernel/sched/fair.c: 8472
 * by noticing the lower domain failed to reach balance and had difficulty /Users/rubber/linux/kernel/sched/fair.c: 8473
 * moving tasks due to affinity constraints. /Users/rubber/linux/kernel/sched/fair.c: 8474
 * When this is so detected; this group becomes a candidate for busiest; see /Users/rubber/linux/kernel/sched/fair.c: 8476
 * update_sd_pick_busiest(). And calculate_imbalance() and /Users/rubber/linux/kernel/sched/fair.c: 8477
 * find_busiest_group() avoid some of the usual balance conditions to allow it /Users/rubber/linux/kernel/sched/fair.c: 8478
 * to create an effective group imbalance. /Users/rubber/linux/kernel/sched/fair.c: 8479
 * This is a somewhat tricky proposition since the next run might not find the /Users/rubber/linux/kernel/sched/fair.c: 8481
 * group imbalance and decide the groups need to be balanced again. A most /Users/rubber/linux/kernel/sched/fair.c: 8482
 * subtle and fragile situation. /Users/rubber/linux/kernel/sched/fair.c: 8483
 * group_has_capacity returns true if the group has spare capacity that could /Users/rubber/linux/kernel/sched/fair.c: 8492
 * be used by some tasks. /Users/rubber/linux/kernel/sched/fair.c: 8493
 * We consider that a group has spare capacity if the  * number of task is /Users/rubber/linux/kernel/sched/fair.c: 8494
 * smaller than the number of CPUs or if the utilization is lower than the /Users/rubber/linux/kernel/sched/fair.c: 8495
 * available capacity for CFS tasks. /Users/rubber/linux/kernel/sched/fair.c: 8496
 * For the latter, we use a threshold to stabilize the state, to take into /Users/rubber/linux/kernel/sched/fair.c: 8497
 * account the variance of the tasks' load and to return true if the available /Users/rubber/linux/kernel/sched/fair.c: 8498
 * capacity in meaningful for the load balancer. /Users/rubber/linux/kernel/sched/fair.c: 8499
 * As an example, an available capacity of 1% can appear but it doesn't make /Users/rubber/linux/kernel/sched/fair.c: 8500
 * any benefit for the load balance. /Users/rubber/linux/kernel/sched/fair.c: 8501
 *  group_is_overloaded returns true if the group has more tasks than it can /Users/rubber/linux/kernel/sched/fair.c: 8521
 *  handle. /Users/rubber/linux/kernel/sched/fair.c: 8522
 *  group_is_overloaded is not equals to !group_has_capacity because a group /Users/rubber/linux/kernel/sched/fair.c: 8523
 *  with the exact right number of tasks, has no more spare capacity but is not /Users/rubber/linux/kernel/sched/fair.c: 8524
 *  overloaded so both group_has_capacity and group_is_overloaded return /Users/rubber/linux/kernel/sched/fair.c: 8525
 *  false. /Users/rubber/linux/kernel/sched/fair.c: 8526
 * asym_smt_can_pull_tasks - Check whether the load balancing CPU can pull tasks /Users/rubber/linux/kernel/sched/fair.c: 8569
 * @dst_cpu:	Destination CPU of the load balancing /Users/rubber/linux/kernel/sched/fair.c: 8570
 * @sds:	Load-balancing data with statistics of the local group /Users/rubber/linux/kernel/sched/fair.c: 8571
 * @sgs:	Load-balancing statistics of the candidate busiest group /Users/rubber/linux/kernel/sched/fair.c: 8572
 * @sg:		The candidate busiest group /Users/rubber/linux/kernel/sched/fair.c: 8573
 * Check the state of the SMT siblings of both @sds::local and @sg and decide /Users/rubber/linux/kernel/sched/fair.c: 8575
 * if @dst_cpu can pull tasks. /Users/rubber/linux/kernel/sched/fair.c: 8576
 * If @dst_cpu does not have SMT siblings, it can pull tasks if two or more of /Users/rubber/linux/kernel/sched/fair.c: 8578
 * the SMT siblings of @sg are busy. If only one CPU in @sg is busy, pull tasks /Users/rubber/linux/kernel/sched/fair.c: 8579
 * only if @dst_cpu has higher priority. /Users/rubber/linux/kernel/sched/fair.c: 8580
 * If both @dst_cpu and @sg have SMT siblings, and @sg has exactly one more /Users/rubber/linux/kernel/sched/fair.c: 8582
 * busy CPU than @sds::local, let @dst_cpu pull tasks if it has higher priority. /Users/rubber/linux/kernel/sched/fair.c: 8583
 * Bigger imbalances in the number of busy CPUs will be dealt with in /Users/rubber/linux/kernel/sched/fair.c: 8584
 * update_sd_pick_busiest(). /Users/rubber/linux/kernel/sched/fair.c: 8585
 * If @sg does not have SMT siblings, only pull tasks if all of the SMT siblings /Users/rubber/linux/kernel/sched/fair.c: 8587
 * of @dst_cpu are idle and @sg has lower priority. /Users/rubber/linux/kernel/sched/fair.c: 8588
		/* /Users/rubber/linux/kernel/sched/fair.c: 8604
		 * If we are here, @dst_cpu is idle and does not have SMT /Users/rubber/linux/kernel/sched/fair.c: 8605
		 * siblings. Pull tasks if candidate group has two or more /Users/rubber/linux/kernel/sched/fair.c: 8606
		 * busy CPUs. /Users/rubber/linux/kernel/sched/fair.c: 8607
		/* /Users/rubber/linux/kernel/sched/fair.c: 8612
		 * @dst_cpu does not have SMT siblings. @sg may have SMT /Users/rubber/linux/kernel/sched/fair.c: 8613
		 * siblings and only one is busy. In such case, @dst_cpu /Users/rubber/linux/kernel/sched/fair.c: 8614
		 * can help if it has higher priority and is idle (i.e., /Users/rubber/linux/kernel/sched/fair.c: 8615
		 * it has no running tasks). /Users/rubber/linux/kernel/sched/fair.c: 8616
	/* /Users/rubber/linux/kernel/sched/fair.c: 8634
	 * @sg does not have SMT siblings. Ensure that @sds::local does not end /Users/rubber/linux/kernel/sched/fair.c: 8635
	 * up with more than one busy SMT sibling and only pull tasks if there /Users/rubber/linux/kernel/sched/fair.c: 8636
	 * are not busy CPUs (i.e., no CPU has running tasks). /Users/rubber/linux/kernel/sched/fair.c: 8637
 * update_sg_lb_stats - Update sched_group's statistics for load balancing. /Users/rubber/linux/kernel/sched/fair.c: 8662
 * @env: The load balancing environment. /Users/rubber/linux/kernel/sched/fair.c: 8663
 * @group: sched_group whose statistics are to be updated. /Users/rubber/linux/kernel/sched/fair.c: 8664
 * @sgs: variable to hold the statistics for this group. /Users/rubber/linux/kernel/sched/fair.c: 8665
 * @sg_status: Holds flag indicating the status of the sched_group /Users/rubber/linux/kernel/sched/fair.c: 8666
		/* /Users/rubber/linux/kernel/sched/fair.c: 8701
		 * No need to call idle_cpu() if nr_running is not 0 /Users/rubber/linux/kernel/sched/fair.c: 8702
 * update_sd_pick_busiest - return 1 on busiest group /Users/rubber/linux/kernel/sched/fair.c: 8741
 * @env: The load balancing environment. /Users/rubber/linux/kernel/sched/fair.c: 8742
 * @sds: sched_domain statistics /Users/rubber/linux/kernel/sched/fair.c: 8743
 * @sg: sched_group candidate to be checked for being the busiest /Users/rubber/linux/kernel/sched/fair.c: 8744
 * @sgs: sched_group statistics /Users/rubber/linux/kernel/sched/fair.c: 8745
 * Determine if @sg is a busier group than the previously selected /Users/rubber/linux/kernel/sched/fair.c: 8747
 * busiest group. /Users/rubber/linux/kernel/sched/fair.c: 8748
 * Return: %true if @sg is a busier group than the previously selected /Users/rubber/linux/kernel/sched/fair.c: 8750
 * busiest group. %false otherwise. /Users/rubber/linux/kernel/sched/fair.c: 8751
	/* /Users/rubber/linux/kernel/sched/fair.c: 8764
	 * Don't try to pull misfit tasks we can't help. /Users/rubber/linux/kernel/sched/fair.c: 8765
	 * We can use max_capacity here as reduction in capacity on some /Users/rubber/linux/kernel/sched/fair.c: 8766
	 * CPUs in the group should either be possible to resolve /Users/rubber/linux/kernel/sched/fair.c: 8767
	 * internally or be covered by avg_load imbalance (eventually). /Users/rubber/linux/kernel/sched/fair.c: 8768
	/* /Users/rubber/linux/kernel/sched/fair.c: 8781
	 * The candidate and the current busiest group are the same type of /Users/rubber/linux/kernel/sched/fair.c: 8782
	 * group. Let check which one is the busiest according to the type. /Users/rubber/linux/kernel/sched/fair.c: 8783
		/* /Users/rubber/linux/kernel/sched/fair.c: 8794
		 * Select the 1st imbalanced group as we don't have any way to /Users/rubber/linux/kernel/sched/fair.c: 8795
		 * choose one more than another. /Users/rubber/linux/kernel/sched/fair.c: 8796
		/* /Users/rubber/linux/kernel/sched/fair.c: 8807
		 * If we have more than one misfit sg go with the biggest /Users/rubber/linux/kernel/sched/fair.c: 8808
		 * misfit. /Users/rubber/linux/kernel/sched/fair.c: 8809
		/* /Users/rubber/linux/kernel/sched/fair.c: 8816
		 * Select the fully busy group with highest avg_load. In /Users/rubber/linux/kernel/sched/fair.c: 8817
		 * theory, there is no need to pull task from such kind of /Users/rubber/linux/kernel/sched/fair.c: 8818
		 * group because tasks have all compute capacity that they need /Users/rubber/linux/kernel/sched/fair.c: 8819
		 * but we can still improve the overall throughput by reducing /Users/rubber/linux/kernel/sched/fair.c: 8820
		 * contention when accessing shared HW resources. /Users/rubber/linux/kernel/sched/fair.c: 8821
		 * /Users/rubber/linux/kernel/sched/fair.c: 8822
		 * XXX for now avg_load is not computed and always 0 so we /Users/rubber/linux/kernel/sched/fair.c: 8823
		 * select the 1st one. /Users/rubber/linux/kernel/sched/fair.c: 8824
		/* /Users/rubber/linux/kernel/sched/fair.c: 8831
		 * Select not overloaded group with lowest number of idle cpus /Users/rubber/linux/kernel/sched/fair.c: 8832
		 * and highest number of running tasks. We could also compare /Users/rubber/linux/kernel/sched/fair.c: 8833
		 * the spare capacity which is more stable but it can end up /Users/rubber/linux/kernel/sched/fair.c: 8834
		 * that the group has less spare capacity but finally more idle /Users/rubber/linux/kernel/sched/fair.c: 8835
		 * CPUs which means less opportunity to pull tasks. /Users/rubber/linux/kernel/sched/fair.c: 8836
	/* /Users/rubber/linux/kernel/sched/fair.c: 8847
	 * Candidate sg has no more than one task per CPU and has higher /Users/rubber/linux/kernel/sched/fair.c: 8848
	 * per-CPU capacity. Migrating tasks to less capable CPUs may harm /Users/rubber/linux/kernel/sched/fair.c: 8849
	 * throughput. Maximize throughput, power/energy consequences are not /Users/rubber/linux/kernel/sched/fair.c: 8850
	 * considered. /Users/rubber/linux/kernel/sched/fair.c: 8851
 * task_running_on_cpu - return 1 if @p is running on @cpu. /Users/rubber/linux/kernel/sched/fair.c: 8895
 * idle_cpu_without - would a given CPU be idle without p ? /Users/rubber/linux/kernel/sched/fair.c: 8911
 * @cpu: the processor on which idleness is tested. /Users/rubber/linux/kernel/sched/fair.c: 8912
 * @p: task which should be ignored. /Users/rubber/linux/kernel/sched/fair.c: 8913
 * Return: 1 if the CPU would be idle. 0 otherwise. /Users/rubber/linux/kernel/sched/fair.c: 8915
	/* /Users/rubber/linux/kernel/sched/fair.c: 8924
	 * rq->nr_running can't be used but an updated version without the /Users/rubber/linux/kernel/sched/fair.c: 8925
	 * impact of p on cpu must be used instead. The updated nr_running /Users/rubber/linux/kernel/sched/fair.c: 8926
	 * be computed and tested before calling idle_cpu_without(). /Users/rubber/linux/kernel/sched/fair.c: 8927
 * update_sg_wakeup_stats - Update sched_group's statistics for wakeup. /Users/rubber/linux/kernel/sched/fair.c: 8939
 * @sd: The sched_domain level to look for idlest group. /Users/rubber/linux/kernel/sched/fair.c: 8940
 * @group: sched_group whose statistics are to be updated. /Users/rubber/linux/kernel/sched/fair.c: 8941
 * @sgs: variable to hold the statistics for this group. /Users/rubber/linux/kernel/sched/fair.c: 8942
 * @p: The task for which we look for the idlest group/CPU. /Users/rubber/linux/kernel/sched/fair.c: 8943
		/* /Users/rubber/linux/kernel/sched/fair.c: 8967
		 * No need to call idle_cpu_without() if nr_running is not 0 /Users/rubber/linux/kernel/sched/fair.c: 8968
	/* /Users/rubber/linux/kernel/sched/fair.c: 8987
	 * Computing avg_load makes sense only when group is fully busy or /Users/rubber/linux/kernel/sched/fair.c: 8988
	 * overloaded /Users/rubber/linux/kernel/sched/fair.c: 8989
	/* /Users/rubber/linux/kernel/sched/fair.c: 9008
	 * The candidate and the current idlest group are the same type of /Users/rubber/linux/kernel/sched/fair.c: 9009
	 * group. Let check which one is the idlest according to the type. /Users/rubber/linux/kernel/sched/fair.c: 9010
 * Allow a NUMA imbalance if busy CPUs is less than 25% of the domain. /Users/rubber/linux/kernel/sched/fair.c: 9049
 * This is an approximation as the number of running tasks may not be /Users/rubber/linux/kernel/sched/fair.c: 9050
 * related to the number of busy CPUs due to sched_setaffinity. /Users/rubber/linux/kernel/sched/fair.c: 9051
 * find_idlest_group() finds and returns the least busy CPU group within the /Users/rubber/linux/kernel/sched/fair.c: 9059
 * domain. /Users/rubber/linux/kernel/sched/fair.c: 9060
 * Assumes p is allowed on at least one CPU in sd. /Users/rubber/linux/kernel/sched/fair.c: 9062
	/* /Users/rubber/linux/kernel/sched/fair.c: 9116
	 * If the local group is idler than the selected idlest group /Users/rubber/linux/kernel/sched/fair.c: 9117
	 * don't try and push the task. /Users/rubber/linux/kernel/sched/fair.c: 9118
	/* /Users/rubber/linux/kernel/sched/fair.c: 9123
	 * If the local group is busier than the selected idlest group /Users/rubber/linux/kernel/sched/fair.c: 9124
	 * try and push the task. /Users/rubber/linux/kernel/sched/fair.c: 9125
		/* /Users/rubber/linux/kernel/sched/fair.c: 9138
		 * When comparing groups across NUMA domains, it's possible for /Users/rubber/linux/kernel/sched/fair.c: 9139
		 * the local domain to be very lightly loaded relative to the /Users/rubber/linux/kernel/sched/fair.c: 9140
		 * remote domains but "imbalance" skews the comparison making /Users/rubber/linux/kernel/sched/fair.c: 9141
		 * remote CPUs look much more favourable. When considering /Users/rubber/linux/kernel/sched/fair.c: 9142
		 * cross-domain, add imbalance to the load on the remote node /Users/rubber/linux/kernel/sched/fair.c: 9143
		 * and consider staying local. /Users/rubber/linux/kernel/sched/fair.c: 9144
		/* /Users/rubber/linux/kernel/sched/fair.c: 9151
		 * If the local group is less loaded than the selected /Users/rubber/linux/kernel/sched/fair.c: 9152
		 * idlest group don't try and push any tasks. /Users/rubber/linux/kernel/sched/fair.c: 9153
			/* /Users/rubber/linux/kernel/sched/fair.c: 9177
			 * If there is spare capacity at NUMA, try to select /Users/rubber/linux/kernel/sched/fair.c: 9178
			 * the preferred node /Users/rubber/linux/kernel/sched/fair.c: 9179
			/* /Users/rubber/linux/kernel/sched/fair.c: 9188
			 * Otherwise, keep the task on this node to stay close /Users/rubber/linux/kernel/sched/fair.c: 9189
			 * its wakeup source and improve locality. If there is /Users/rubber/linux/kernel/sched/fair.c: 9190
			 * a real need of migration, periodic load balance will /Users/rubber/linux/kernel/sched/fair.c: 9191
			 * take care of it. /Users/rubber/linux/kernel/sched/fair.c: 9192
		/* /Users/rubber/linux/kernel/sched/fair.c: 9198
		 * Select group with highest number of idle CPUs. We could also /Users/rubber/linux/kernel/sched/fair.c: 9199
		 * compare the utilization which is more stable but it can end /Users/rubber/linux/kernel/sched/fair.c: 9200
		 * up that the group has less spare capacity but finally more /Users/rubber/linux/kernel/sched/fair.c: 9201
		 * idle CPUs which means more opportunity to run task. /Users/rubber/linux/kernel/sched/fair.c: 9202
 * update_sd_lb_stats - Update sched_domain's statistics for load balancing. /Users/rubber/linux/kernel/sched/fair.c: 9213
 * @env: The load balancing environment. /Users/rubber/linux/kernel/sched/fair.c: 9214
 * @sds: variable to hold the statistics for this sched_domain. /Users/rubber/linux/kernel/sched/fair.c: 9215
	/* /Users/rubber/linux/kernel/sched/fair.c: 9291
	 * Allow a small imbalance based on a simple pair of communicating /Users/rubber/linux/kernel/sched/fair.c: 9292
	 * tasks that remain local when the destination is lightly loaded. /Users/rubber/linux/kernel/sched/fair.c: 9293
 * calculate_imbalance - Calculate the amount of imbalance present within the /Users/rubber/linux/kernel/sched/fair.c: 9302
 *			 groups of a given sched_domain during load balance. /Users/rubber/linux/kernel/sched/fair.c: 9303
 * @env: load balance environment /Users/rubber/linux/kernel/sched/fair.c: 9304
 * @sds: statistics of the sched_domain whose imbalance is to be calculated. /Users/rubber/linux/kernel/sched/fair.c: 9305
		/* /Users/rubber/linux/kernel/sched/fair.c: 9322
		 * In case of asym capacity, we will try to migrate all load to /Users/rubber/linux/kernel/sched/fair.c: 9323
		 * the preferred CPU. /Users/rubber/linux/kernel/sched/fair.c: 9324
		/* /Users/rubber/linux/kernel/sched/fair.c: 9332
		 * In the group_imb case we cannot rely on group-wide averages /Users/rubber/linux/kernel/sched/fair.c: 9333
		 * to ensure CPU-load equilibrium, try to move any task to fix /Users/rubber/linux/kernel/sched/fair.c: 9334
		 * the imbalance. The next load balance will take care of /Users/rubber/linux/kernel/sched/fair.c: 9335
		 * balancing back the system. /Users/rubber/linux/kernel/sched/fair.c: 9336
	/* /Users/rubber/linux/kernel/sched/fair.c: 9343
	 * Try to use spare capacity of local group without overloading it or /Users/rubber/linux/kernel/sched/fair.c: 9344
	 * emptying busiest. /Users/rubber/linux/kernel/sched/fair.c: 9345
			/* /Users/rubber/linux/kernel/sched/fair.c: 9350
			 * If busiest is overloaded, try to fill spare /Users/rubber/linux/kernel/sched/fair.c: 9351
			 * capacity. This might end up creating spare capacity /Users/rubber/linux/kernel/sched/fair.c: 9352
			 * in busiest or busiest still being overloaded but /Users/rubber/linux/kernel/sched/fair.c: 9353
			 * there is no simple way to directly compute the /Users/rubber/linux/kernel/sched/fair.c: 9354
			 * amount of load to migrate in order to balance the /Users/rubber/linux/kernel/sched/fair.c: 9355
			 * system. /Users/rubber/linux/kernel/sched/fair.c: 9356
			/* /Users/rubber/linux/kernel/sched/fair.c: 9362
			 * In some cases, the group's utilization is max or even /Users/rubber/linux/kernel/sched/fair.c: 9363
			 * higher than capacity because of migrations but the /Users/rubber/linux/kernel/sched/fair.c: 9364
			 * local CPU is (newly) idle. There is at least one /Users/rubber/linux/kernel/sched/fair.c: 9365
			 * waiting task in this overloaded busiest group. Let's /Users/rubber/linux/kernel/sched/fair.c: 9366
			 * try to pull it. /Users/rubber/linux/kernel/sched/fair.c: 9367
			/* /Users/rubber/linux/kernel/sched/fair.c: 9379
			 * When prefer sibling, evenly spread running tasks on /Users/rubber/linux/kernel/sched/fair.c: 9380
			 * groups. /Users/rubber/linux/kernel/sched/fair.c: 9381
			/* /Users/rubber/linux/kernel/sched/fair.c: 9388
			 * If there is no overload, we just want to even the number of /Users/rubber/linux/kernel/sched/fair.c: 9389
			 * idle cpus. /Users/rubber/linux/kernel/sched/fair.c: 9390
	/* /Users/rubber/linux/kernel/sched/fair.c: 9406
	 * Local is fully busy but has to take more load to relieve the /Users/rubber/linux/kernel/sched/fair.c: 9407
	 * busiest group /Users/rubber/linux/kernel/sched/fair.c: 9408
		/* /Users/rubber/linux/kernel/sched/fair.c: 9411
		 * Local will become overloaded so the avg_load metrics are /Users/rubber/linux/kernel/sched/fair.c: 9412
		 * finally needed. /Users/rubber/linux/kernel/sched/fair.c: 9413
		/* /Users/rubber/linux/kernel/sched/fair.c: 9421
		 * If the local group is more loaded than the selected /Users/rubber/linux/kernel/sched/fair.c: 9422
		 * busiest group don't try to pull any tasks. /Users/rubber/linux/kernel/sched/fair.c: 9423
	/* /Users/rubber/linux/kernel/sched/fair.c: 9431
	 * Both group are or will become overloaded and we're trying to get all /Users/rubber/linux/kernel/sched/fair.c: 9432
	 * the CPUs to the average_load, so we don't want to push ourselves /Users/rubber/linux/kernel/sched/fair.c: 9433
	 * above the average load, nor do we wish to reduce the max loaded CPU /Users/rubber/linux/kernel/sched/fair.c: 9434
	 * below the average load. At the same time, we also don't want to /Users/rubber/linux/kernel/sched/fair.c: 9435
	 * reduce the group load below the group capacity. Thus we look for /Users/rubber/linux/kernel/sched/fair.c: 9436
	 * the minimum possible imbalance. /Users/rubber/linux/kernel/sched/fair.c: 9437
 * Decision matrix according to the local and busiest group type: /Users/rubber/linux/kernel/sched/fair.c: 9449
 * busiest \ local has_spare fully_busy misfit asym imbalanced overloaded /Users/rubber/linux/kernel/sched/fair.c: 9451
 * has_spare        nr_idle   balanced   N/A    N/A  balanced   balanced /Users/rubber/linux/kernel/sched/fair.c: 9452
 * fully_busy       nr_idle   nr_idle    N/A    N/A  balanced   balanced /Users/rubber/linux/kernel/sched/fair.c: 9453
 * misfit_task      force     N/A        N/A    N/A  force      force /Users/rubber/linux/kernel/sched/fair.c: 9454
 * asym_packing     force     force      N/A    N/A  force      force /Users/rubber/linux/kernel/sched/fair.c: 9455
 * imbalanced       force     force      N/A    N/A  force      force /Users/rubber/linux/kernel/sched/fair.c: 9456
 * overloaded       force     force      N/A    N/A  force      avg_load /Users/rubber/linux/kernel/sched/fair.c: 9457
 * N/A :      Not Applicable because already filtered while updating /Users/rubber/linux/kernel/sched/fair.c: 9459
 *            statistics. /Users/rubber/linux/kernel/sched/fair.c: 9460
 * balanced : The system is balanced for these 2 groups. /Users/rubber/linux/kernel/sched/fair.c: 9461
 * force :    Calculate the imbalance as load migration is probably needed. /Users/rubber/linux/kernel/sched/fair.c: 9462
 * avg_load : Only if imbalance is significant enough. /Users/rubber/linux/kernel/sched/fair.c: 9463
 * nr_idle :  dst_cpu is not busy and the number of idle CPUs is quite /Users/rubber/linux/kernel/sched/fair.c: 9464
 *            different in groups. /Users/rubber/linux/kernel/sched/fair.c: 9465
 * find_busiest_group - Returns the busiest group within the sched_domain /Users/rubber/linux/kernel/sched/fair.c: 9469
 * if there is an imbalance. /Users/rubber/linux/kernel/sched/fair.c: 9470
 * Also calculates the amount of runnable load which should be moved /Users/rubber/linux/kernel/sched/fair.c: 9472
 * to restore balance. /Users/rubber/linux/kernel/sched/fair.c: 9473
 * @env: The load balancing environment. /Users/rubber/linux/kernel/sched/fair.c: 9475
 * Return:	- The busiest group if imbalance exists. /Users/rubber/linux/kernel/sched/fair.c: 9477
	/* /Users/rubber/linux/kernel/sched/fair.c: 9486
	 * Compute the various statistics relevant for load balancing at /Users/rubber/linux/kernel/sched/fair.c: 9487
	 * this level. /Users/rubber/linux/kernel/sched/fair.c: 9488
	/* /Users/rubber/linux/kernel/sched/fair.c: 9514
	 * If the busiest group is imbalanced the below checks don't /Users/rubber/linux/kernel/sched/fair.c: 9515
	 * work because they assume all things are equal, which typically /Users/rubber/linux/kernel/sched/fair.c: 9516
	 * isn't true due to cpus_ptr constraints and the like. /Users/rubber/linux/kernel/sched/fair.c: 9517
	/* /Users/rubber/linux/kernel/sched/fair.c: 9522
	 * If the local group is busier than the selected busiest group /Users/rubber/linux/kernel/sched/fair.c: 9523
	 * don't try and pull any tasks. /Users/rubber/linux/kernel/sched/fair.c: 9524
	/* /Users/rubber/linux/kernel/sched/fair.c: 9529
	 * When groups are overloaded, use the avg_load to ensure fairness /Users/rubber/linux/kernel/sched/fair.c: 9530
	 * between tasks. /Users/rubber/linux/kernel/sched/fair.c: 9531
		/* /Users/rubber/linux/kernel/sched/fair.c: 9534
		 * If the local group is more loaded than the selected /Users/rubber/linux/kernel/sched/fair.c: 9535
		 * busiest group don't try to pull any tasks. /Users/rubber/linux/kernel/sched/fair.c: 9536
		/* /Users/rubber/linux/kernel/sched/fair.c: 9545
		 * Don't pull any tasks if this group is already above the /Users/rubber/linux/kernel/sched/fair.c: 9546
		 * domain average load. /Users/rubber/linux/kernel/sched/fair.c: 9547
		/* /Users/rubber/linux/kernel/sched/fair.c: 9552
		 * If the busiest group is more loaded, use imbalance_pct to be /Users/rubber/linux/kernel/sched/fair.c: 9553
		 * conservative. /Users/rubber/linux/kernel/sched/fair.c: 9554
			/* /Users/rubber/linux/kernel/sched/fair.c: 9568
			 * If the busiest group is not overloaded (and as a /Users/rubber/linux/kernel/sched/fair.c: 9569
			 * result the local one too) but this CPU is already /Users/rubber/linux/kernel/sched/fair.c: 9570
			 * busy, let another idle CPU try to pull task. /Users/rubber/linux/kernel/sched/fair.c: 9571
			/* /Users/rubber/linux/kernel/sched/fair.c: 9577
			 * If the busiest group is not overloaded /Users/rubber/linux/kernel/sched/fair.c: 9578
			 * and there is no imbalance between this and busiest /Users/rubber/linux/kernel/sched/fair.c: 9579
			 * group wrt idle CPUs, it is balanced. The imbalance /Users/rubber/linux/kernel/sched/fair.c: 9580
			 * becomes significant if the diff is greater than 1 /Users/rubber/linux/kernel/sched/fair.c: 9581
			 * otherwise we might end up to just move the imbalance /Users/rubber/linux/kernel/sched/fair.c: 9582
			 * on another group. Of course this applies only if /Users/rubber/linux/kernel/sched/fair.c: 9583
			 * there is more than 1 CPU per group. /Users/rubber/linux/kernel/sched/fair.c: 9584
			/* /Users/rubber/linux/kernel/sched/fair.c: 9589
			 * busiest doesn't have any tasks waiting to run /Users/rubber/linux/kernel/sched/fair.c: 9590
 * find_busiest_queue - find the busiest runqueue among the CPUs in the group. /Users/rubber/linux/kernel/sched/fair.c: 9606
		/* /Users/rubber/linux/kernel/sched/fair.c: 9624
		 * We classify groups/runqueues into three groups: /Users/rubber/linux/kernel/sched/fair.c: 9625
		 *  - regular: there are !numa tasks /Users/rubber/linux/kernel/sched/fair.c: 9626
		 *  - remote:  there are numa tasks that run on the 'wrong' node /Users/rubber/linux/kernel/sched/fair.c: 9627
		 *  - all:     there is no distinction /Users/rubber/linux/kernel/sched/fair.c: 9628
		 * /Users/rubber/linux/kernel/sched/fair.c: 9629
		 * In order to avoid migrating ideally placed numa tasks, /Users/rubber/linux/kernel/sched/fair.c: 9630
		 * ignore those when there's better options. /Users/rubber/linux/kernel/sched/fair.c: 9631
		 * /Users/rubber/linux/kernel/sched/fair.c: 9632
		 * If we ignore the actual busiest queue to migrate another /Users/rubber/linux/kernel/sched/fair.c: 9633
		 * task, the next balance pass can still reduce the busiest /Users/rubber/linux/kernel/sched/fair.c: 9634
		 * queue by moving tasks around inside the node. /Users/rubber/linux/kernel/sched/fair.c: 9635
		 * /Users/rubber/linux/kernel/sched/fair.c: 9636
		 * If we cannot move enough load due to this classification /Users/rubber/linux/kernel/sched/fair.c: 9637
		 * the next pass will adjust the group classification and /Users/rubber/linux/kernel/sched/fair.c: 9638
		 * allow migration of more tasks. /Users/rubber/linux/kernel/sched/fair.c: 9639
		 * /Users/rubber/linux/kernel/sched/fair.c: 9640
		 * Both cases only affect the total convergence complexity. /Users/rubber/linux/kernel/sched/fair.c: 9641
		/* /Users/rubber/linux/kernel/sched/fair.c: 9652
		 * For ASYM_CPUCAPACITY domains, don't pick a CPU that could /Users/rubber/linux/kernel/sched/fair.c: 9653
		 * eventually lead to active_balancing high->low capacity. /Users/rubber/linux/kernel/sched/fair.c: 9654
		 * Higher per-CPU capacity is considered better than balancing /Users/rubber/linux/kernel/sched/fair.c: 9655
		 * average load. /Users/rubber/linux/kernel/sched/fair.c: 9656
			/* /Users/rubber/linux/kernel/sched/fair.c: 9671
			 * When comparing with load imbalance, use cpu_load() /Users/rubber/linux/kernel/sched/fair.c: 9672
			 * which is not scaled with the CPU capacity. /Users/rubber/linux/kernel/sched/fair.c: 9673
			/* /Users/rubber/linux/kernel/sched/fair.c: 9681
			 * For the load comparisons with the other CPUs, /Users/rubber/linux/kernel/sched/fair.c: 9682
			 * consider the cpu_load() scaled with the CPU /Users/rubber/linux/kernel/sched/fair.c: 9683
			 * capacity, so that the load can be moved away /Users/rubber/linux/kernel/sched/fair.c: 9684
			 * from the CPU that is potentially running at a /Users/rubber/linux/kernel/sched/fair.c: 9685
			 * lower capacity. /Users/rubber/linux/kernel/sched/fair.c: 9686
			 * /Users/rubber/linux/kernel/sched/fair.c: 9687
			 * Thus we're looking for max(load_i / capacity_i), /Users/rubber/linux/kernel/sched/fair.c: 9688
			 * crosswise multiplication to rid ourselves of the /Users/rubber/linux/kernel/sched/fair.c: 9689
			 * division works out to: /Users/rubber/linux/kernel/sched/fair.c: 9690
			 * load_i * capacity_j > load_j * capacity_i; /Users/rubber/linux/kernel/sched/fair.c: 9691
			 * where j is our previous maximum. /Users/rubber/linux/kernel/sched/fair.c: 9692
			/* /Users/rubber/linux/kernel/sched/fair.c: 9704
			 * Don't try to pull utilization from a CPU with one /Users/rubber/linux/kernel/sched/fair.c: 9705
			 * running task. Whatever its utilization, we will fail /Users/rubber/linux/kernel/sched/fair.c: 9706
			 * detach the task. /Users/rubber/linux/kernel/sched/fair.c: 9707
			/* /Users/rubber/linux/kernel/sched/fair.c: 9726
			 * For ASYM_CPUCAPACITY domains with misfit tasks we /Users/rubber/linux/kernel/sched/fair.c: 9727
			 * simply seek the "biggest" misfit task. /Users/rubber/linux/kernel/sched/fair.c: 9728
 * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but /Users/rubber/linux/kernel/sched/fair.c: 9744
 * so long as it is large enough. /Users/rubber/linux/kernel/sched/fair.c: 9745
	/* /Users/rubber/linux/kernel/sched/fair.c: 9752
	 * ASYM_PACKING needs to force migrate tasks from busy but /Users/rubber/linux/kernel/sched/fair.c: 9753
	 * lower priority CPUs in order to pack all tasks in the /Users/rubber/linux/kernel/sched/fair.c: 9754
	 * highest priority CPUs. /Users/rubber/linux/kernel/sched/fair.c: 9755
	/* /Users/rubber/linux/kernel/sched/fair.c: 9766
	 * The imbalanced case includes the case of pinned tasks preventing a fair /Users/rubber/linux/kernel/sched/fair.c: 9767
	 * distribution of the load on the system but also the even distribution of the /Users/rubber/linux/kernel/sched/fair.c: 9768
	 * threads on a system with spare capacity /Users/rubber/linux/kernel/sched/fair.c: 9769
	/* /Users/rubber/linux/kernel/sched/fair.c: 9788
	 * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task. /Users/rubber/linux/kernel/sched/fair.c: 9789
	 * It's worth migrating the task if the src_cpu's capacity is reduced /Users/rubber/linux/kernel/sched/fair.c: 9790
	 * because of other sched_class or IRQs if more capacity stays /Users/rubber/linux/kernel/sched/fair.c: 9791
	 * available on dst_cpu. /Users/rubber/linux/kernel/sched/fair.c: 9792
	/* /Users/rubber/linux/kernel/sched/fair.c: 9814
	 * Ensure the balancing environment is consistent; can happen /Users/rubber/linux/kernel/sched/fair.c: 9815
	 * when the softirq triggers 'during' hotplug. /Users/rubber/linux/kernel/sched/fair.c: 9816
	/* /Users/rubber/linux/kernel/sched/fair.c: 9821
	 * In the newly idle case, we will allow all the CPUs /Users/rubber/linux/kernel/sched/fair.c: 9822
	 * to do the newly idle load balance. /Users/rubber/linux/kernel/sched/fair.c: 9823
 * Check this_cpu to ensure it is balanced within domain. Attempt to move /Users/rubber/linux/kernel/sched/fair.c: 9842
 * tasks if there is an imbalance. /Users/rubber/linux/kernel/sched/fair.c: 9843
		/* /Users/rubber/linux/kernel/sched/fair.c: 9901
		 * Attempt to move tasks. If find_busiest_group has found /Users/rubber/linux/kernel/sched/fair.c: 9902
		 * an imbalance but busiest->nr_running <= 1, the group is /Users/rubber/linux/kernel/sched/fair.c: 9903
		 * still unbalanced. ld_moved simply stays zero, so it is /Users/rubber/linux/kernel/sched/fair.c: 9904
		 * correctly treated as an imbalance. /Users/rubber/linux/kernel/sched/fair.c: 9905
		/* /Users/rubber/linux/kernel/sched/fair.c: 9913
		 * cur_ld_moved - load moved in current iteration /Users/rubber/linux/kernel/sched/fair.c: 9914
		 * ld_moved     - cumulative load moved across iterations /Users/rubber/linux/kernel/sched/fair.c: 9915
		/* /Users/rubber/linux/kernel/sched/fair.c: 9919
		 * We've detached some tasks from busiest_rq. Every /Users/rubber/linux/kernel/sched/fair.c: 9920
		 * task is masked "TASK_ON_RQ_MIGRATING", so we can safely /Users/rubber/linux/kernel/sched/fair.c: 9921
		 * unlock busiest->lock, and we are able to be sure /Users/rubber/linux/kernel/sched/fair.c: 9922
		 * that nobody can manipulate the tasks in parallel. /Users/rubber/linux/kernel/sched/fair.c: 9923
		 * See task_rq_lock() family for the details. /Users/rubber/linux/kernel/sched/fair.c: 9924
		/* /Users/rubber/linux/kernel/sched/fair.c: 9941
		 * Revisit (affine) tasks on src_cpu that couldn't be moved to /Users/rubber/linux/kernel/sched/fair.c: 9942
		 * us and move them to an alternate dst_cpu in our sched_group /Users/rubber/linux/kernel/sched/fair.c: 9943
		 * where they can run. The upper limit on how many times we /Users/rubber/linux/kernel/sched/fair.c: 9944
		 * iterate on same src_cpu is dependent on number of CPUs in our /Users/rubber/linux/kernel/sched/fair.c: 9945
		 * sched_group. /Users/rubber/linux/kernel/sched/fair.c: 9946
		 * /Users/rubber/linux/kernel/sched/fair.c: 9947
		 * This changes load balance semantics a bit on who can move /Users/rubber/linux/kernel/sched/fair.c: 9948
		 * load to a given_cpu. In addition to the given_cpu itself /Users/rubber/linux/kernel/sched/fair.c: 9949
		 * (or a ilb_cpu acting on its behalf where given_cpu is /Users/rubber/linux/kernel/sched/fair.c: 9950
		 * nohz-idle), we now have balance_cpu in a position to move /Users/rubber/linux/kernel/sched/fair.c: 9951
		 * load to given_cpu. In rare situations, this may cause /Users/rubber/linux/kernel/sched/fair.c: 9952
		 * conflicts (balance_cpu and given_cpu/ilb_cpu deciding /Users/rubber/linux/kernel/sched/fair.c: 9953
		 * _independently_ and at _same_ time to move some load to /Users/rubber/linux/kernel/sched/fair.c: 9954
		 * given_cpu) causing excess load to be moved to given_cpu. /Users/rubber/linux/kernel/sched/fair.c: 9955
		 * This however should not happen so much in practice and /Users/rubber/linux/kernel/sched/fair.c: 9956
		 * moreover subsequent load balance cycles should correct the /Users/rubber/linux/kernel/sched/fair.c: 9957
		 * excess load moved. /Users/rubber/linux/kernel/sched/fair.c: 9958
			/* /Users/rubber/linux/kernel/sched/fair.c: 9971
			 * Go back to "more_balance" rather than "redo" since we /Users/rubber/linux/kernel/sched/fair.c: 9972
			 * need to continue with same src_cpu. /Users/rubber/linux/kernel/sched/fair.c: 9973
		/* /Users/rubber/linux/kernel/sched/fair.c: 9978
		 * We failed to reach balance because of affinity. /Users/rubber/linux/kernel/sched/fair.c: 9979
			/* /Users/rubber/linux/kernel/sched/fair.c: 9991
			 * Attempting to continue load balancing at the current /Users/rubber/linux/kernel/sched/fair.c: 9992
			 * sched_domain level only makes sense if there are /Users/rubber/linux/kernel/sched/fair.c: 9993
			 * active CPUs remaining as possible busiest CPUs to /Users/rubber/linux/kernel/sched/fair.c: 9994
			 * pull load from which are not contained within the /Users/rubber/linux/kernel/sched/fair.c: 9995
			 * destination group that is receiving any migrated /Users/rubber/linux/kernel/sched/fair.c: 9996
			 * load. /Users/rubber/linux/kernel/sched/fair.c: 9997
		/* /Users/rubber/linux/kernel/sched/fair.c: 10010
		 * Increment the failure counter only on periodic balance. /Users/rubber/linux/kernel/sched/fair.c: 10011
		 * We do not want newidle balance, which can be very /Users/rubber/linux/kernel/sched/fair.c: 10012
		 * frequent, pollute the failure counter causing /Users/rubber/linux/kernel/sched/fair.c: 10013
		 * excessive cache_hot migrations and active balances. /Users/rubber/linux/kernel/sched/fair.c: 10014
			/* /Users/rubber/linux/kernel/sched/fair.c: 10024
			 * Don't kick the active_load_balance_cpu_stop, /Users/rubber/linux/kernel/sched/fair.c: 10025
			 * if the curr task on busiest CPU can't be /Users/rubber/linux/kernel/sched/fair.c: 10026
			 * moved to this_cpu: /Users/rubber/linux/kernel/sched/fair.c: 10027
			/* /Users/rubber/linux/kernel/sched/fair.c: 10037
			 * ->active_balance synchronizes accesses to /Users/rubber/linux/kernel/sched/fair.c: 10038
			 * ->active_balance_work.  Once set, it's cleared /Users/rubber/linux/kernel/sched/fair.c: 10039
			 * only after active load balance is finished. /Users/rubber/linux/kernel/sched/fair.c: 10040
	/* /Users/rubber/linux/kernel/sched/fair.c: 10067
	 * We reach balance although we may have faced some affinity /Users/rubber/linux/kernel/sched/fair.c: 10068
	 * constraints. Clear the imbalance flag only if other tasks got /Users/rubber/linux/kernel/sched/fair.c: 10069
	 * a chance to move and fix the imbalance. /Users/rubber/linux/kernel/sched/fair.c: 10070
	/* /Users/rubber/linux/kernel/sched/fair.c: 10080
	 * We reach balance because all tasks are pinned at this level so /Users/rubber/linux/kernel/sched/fair.c: 10081
	 * we can't migrate them. Let the imbalance flag set so parent level /Users/rubber/linux/kernel/sched/fair.c: 10082
	 * can try to migrate them. /Users/rubber/linux/kernel/sched/fair.c: 10083
	/* /Users/rubber/linux/kernel/sched/fair.c: 10092
	 * newidle_balance() disregards balance intervals, so we could /Users/rubber/linux/kernel/sched/fair.c: 10093
	 * repeatedly reach this code, which would lead to balance_interval /Users/rubber/linux/kernel/sched/fair.c: 10094
	 * skyrocketing in a short amount of time. Skip the balance_interval /Users/rubber/linux/kernel/sched/fair.c: 10095
	 * increase logic to avoid that. /Users/rubber/linux/kernel/sched/fair.c: 10096
	/* /Users/rubber/linux/kernel/sched/fair.c: 10121
	 * Reduce likelihood of busy balancing at higher domains racing with /Users/rubber/linux/kernel/sched/fair.c: 10122
	 * balancing at lower domains by preventing their balancing periods /Users/rubber/linux/kernel/sched/fair.c: 10123
	 * from being multiples of each other. /Users/rubber/linux/kernel/sched/fair.c: 10124
 * active_load_balance_cpu_stop is run by the CPU stopper. It pushes /Users/rubber/linux/kernel/sched/fair.c: 10148
 * running tasks off the busiest CPU onto idle CPUs. It requires at /Users/rubber/linux/kernel/sched/fair.c: 10149
 * least 1 task to be running on each physical CPU where possible, and /Users/rubber/linux/kernel/sched/fair.c: 10150
 * avoids physical / logical imbalances. /Users/rubber/linux/kernel/sched/fair.c: 10151
	/* /Users/rubber/linux/kernel/sched/fair.c: 10164
	 * Between queueing the stop-work and running it is a hole in which /Users/rubber/linux/kernel/sched/fair.c: 10165
	 * CPUs can become inactive. We should not move tasks from or to /Users/rubber/linux/kernel/sched/fair.c: 10166
	 * inactive CPUs. /Users/rubber/linux/kernel/sched/fair.c: 10167
	/* /Users/rubber/linux/kernel/sched/fair.c: 10181
	 * This condition is "impossible", if it occurs /Users/rubber/linux/kernel/sched/fair.c: 10182
	 * we need to fix it. Originally reported by /Users/rubber/linux/kernel/sched/fair.c: 10183
	 * Bjorn Helgaas on a 128-CPU setup. /Users/rubber/linux/kernel/sched/fair.c: 10184
 * Scale the max load_balance interval with the number of CPUs in the system. /Users/rubber/linux/kernel/sched/fair.c: 10234
 * This trades load-balance latency on larger machines for less cross talk. /Users/rubber/linux/kernel/sched/fair.c: 10235
		/* /Users/rubber/linux/kernel/sched/fair.c: 10245
		 * Track max cost of a domain to make sure to not delay the /Users/rubber/linux/kernel/sched/fair.c: 10246
		 * next wakeup on the CPU. /Users/rubber/linux/kernel/sched/fair.c: 10247
		/* /Users/rubber/linux/kernel/sched/fair.c: 10252
		 * Decay the newidle max times by ~1% per second to ensure that /Users/rubber/linux/kernel/sched/fair.c: 10253
		 * it is not outdated and the current max cost is actually /Users/rubber/linux/kernel/sched/fair.c: 10254
		 * shorter. /Users/rubber/linux/kernel/sched/fair.c: 10255
 * It checks each scheduling domain to see if it is due to be balanced, /Users/rubber/linux/kernel/sched/fair.c: 10267
 * and initiates a balancing operation if so. /Users/rubber/linux/kernel/sched/fair.c: 10268
 * Balancing parameters are set up in init_sched_domains. /Users/rubber/linux/kernel/sched/fair.c: 10270
		/* /Users/rubber/linux/kernel/sched/fair.c: 10287
		 * Decay the newidle max times here because this is a regular /Users/rubber/linux/kernel/sched/fair.c: 10288
		 * visit to all the domains. /Users/rubber/linux/kernel/sched/fair.c: 10289
		/* /Users/rubber/linux/kernel/sched/fair.c: 10294
		 * Stop the load balance at this level. There is another /Users/rubber/linux/kernel/sched/fair.c: 10295
		 * CPU in our sched group which is doing load balancing more /Users/rubber/linux/kernel/sched/fair.c: 10296
		 * actively. /Users/rubber/linux/kernel/sched/fair.c: 10297
				/* /Users/rubber/linux/kernel/sched/fair.c: 10315
				 * The LBF_DST_PINNED logic could have changed /Users/rubber/linux/kernel/sched/fair.c: 10316
				 * env->dst_cpu, so we can't know our idle /Users/rubber/linux/kernel/sched/fair.c: 10317
				 * state even if we migrated tasks. Update it. /Users/rubber/linux/kernel/sched/fair.c: 10318
		/* /Users/rubber/linux/kernel/sched/fair.c: 10335
		 * Ensure the rq-wide value also decays but keep it at a /Users/rubber/linux/kernel/sched/fair.c: 10336
		 * reasonable floor to avoid funnies with rq->avg_idle. /Users/rubber/linux/kernel/sched/fair.c: 10337
	/* /Users/rubber/linux/kernel/sched/fair.c: 10344
	 * next_balance will be updated only when there is a need. /Users/rubber/linux/kernel/sched/fair.c: 10345
	 * When the cpu is attached to null domain for ex, it will not be /Users/rubber/linux/kernel/sched/fair.c: 10346
	 * updated. /Users/rubber/linux/kernel/sched/fair.c: 10347
 * idle load balancing details /Users/rubber/linux/kernel/sched/fair.c: 10361
 * - When one of the busy CPUs notice that there may be an idle rebalancing /Users/rubber/linux/kernel/sched/fair.c: 10362
 *   needed, they will kick the idle load balancer, which then does idle /Users/rubber/linux/kernel/sched/fair.c: 10363
 *   load balancing for all the idle CPUs. /Users/rubber/linux/kernel/sched/fair.c: 10364
 * - HK_FLAG_MISC CPUs are used for this task, because HK_FLAG_SCHED not set /Users/rubber/linux/kernel/sched/fair.c: 10365
 *   anywhere yet. /Users/rubber/linux/kernel/sched/fair.c: 10366
 * Kick a CPU to do the nohz balancing, if it is time for it. We pick any /Users/rubber/linux/kernel/sched/fair.c: 10389
 * idle CPU in the HK_FLAG_MISC housekeeping set (if there is one). /Users/rubber/linux/kernel/sched/fair.c: 10390
	/* /Users/rubber/linux/kernel/sched/fair.c: 10396
	 * Increase nohz.next_balance only when if full ilb is triggered but /Users/rubber/linux/kernel/sched/fair.c: 10397
	 * not if we only update stats. /Users/rubber/linux/kernel/sched/fair.c: 10398
	/* /Users/rubber/linux/kernel/sched/fair.c: 10408
	 * Access to rq::nohz_csd is serialized by NOHZ_KICK_MASK; he who sets /Users/rubber/linux/kernel/sched/fair.c: 10409
	 * the first flag owns it; cleared by nohz_csd_func(). /Users/rubber/linux/kernel/sched/fair.c: 10410
	/* /Users/rubber/linux/kernel/sched/fair.c: 10416
	 * This way we generate an IPI on the target CPU which /Users/rubber/linux/kernel/sched/fair.c: 10417
	 * is idle. And the softirq performing nohz idle load balance /Users/rubber/linux/kernel/sched/fair.c: 10418
	 * will be run before returning from the IPI. /Users/rubber/linux/kernel/sched/fair.c: 10419
 * Current decision point for kicking the idle load balancer in the presence /Users/rubber/linux/kernel/sched/fair.c: 10425
 * of idle CPUs in the system. /Users/rubber/linux/kernel/sched/fair.c: 10426
	/* /Users/rubber/linux/kernel/sched/fair.c: 10439
	 * We may be recently in ticked or tickless idle mode. At the first /Users/rubber/linux/kernel/sched/fair.c: 10440
	 * busy tick after returning from idle, we will update the busy stats. /Users/rubber/linux/kernel/sched/fair.c: 10441
	/* /Users/rubber/linux/kernel/sched/fair.c: 10445
	 * None are in tickless mode and hence no need for NOHZ idle load /Users/rubber/linux/kernel/sched/fair.c: 10446
	 * balancing. /Users/rubber/linux/kernel/sched/fair.c: 10447
		/* /Users/rubber/linux/kernel/sched/fair.c: 10468
		 * If there's a CFS task and the current CPU has reduced /Users/rubber/linux/kernel/sched/fair.c: 10469
		 * capacity; kick the ILB to see if there's a better CPU to run /Users/rubber/linux/kernel/sched/fair.c: 10470
		 * on. /Users/rubber/linux/kernel/sched/fair.c: 10471
		/* /Users/rubber/linux/kernel/sched/fair.c: 10481
		 * When ASYM_PACKING; see if there's a more preferred CPU /Users/rubber/linux/kernel/sched/fair.c: 10482
		 * currently idle; in which case, kick the ILB to move tasks /Users/rubber/linux/kernel/sched/fair.c: 10483
		 * around. /Users/rubber/linux/kernel/sched/fair.c: 10484
		/* /Users/rubber/linux/kernel/sched/fair.c: 10496
		 * When ASYM_CPUCAPACITY; see if there's a higher capacity CPU /Users/rubber/linux/kernel/sched/fair.c: 10497
		 * to run the misfit task on. /Users/rubber/linux/kernel/sched/fair.c: 10498
		/* /Users/rubber/linux/kernel/sched/fair.c: 10505
		 * For asymmetric systems, we do not want to nicely balance /Users/rubber/linux/kernel/sched/fair.c: 10506
		 * cache use, instead we want to embrace asymmetry and only /Users/rubber/linux/kernel/sched/fair.c: 10507
		 * ensure tasks have enough CPU capacity. /Users/rubber/linux/kernel/sched/fair.c: 10508
		 * /Users/rubber/linux/kernel/sched/fair.c: 10509
		 * Skip the LLC logic because it's not relevant in that case. /Users/rubber/linux/kernel/sched/fair.c: 10510
		/* /Users/rubber/linux/kernel/sched/fair.c: 10517
		 * If there is an imbalance between LLC domains (IOW we could /Users/rubber/linux/kernel/sched/fair.c: 10518
		 * increase the overall cache use), we need some less-loaded LLC /Users/rubber/linux/kernel/sched/fair.c: 10519
		 * domain to pull some load. Likewise, we may need to spread /Users/rubber/linux/kernel/sched/fair.c: 10520
		 * load within the current LLC domain (e.g. packed SMT cores but /Users/rubber/linux/kernel/sched/fair.c: 10521
		 * other CPUs are idle). We can't really know from here how busy /Users/rubber/linux/kernel/sched/fair.c: 10522
		 * the others are - so just get a nohz balance going if it looks /Users/rubber/linux/kernel/sched/fair.c: 10523
		 * like this LLC domain has tasks we could move. /Users/rubber/linux/kernel/sched/fair.c: 10524
 * This routine will record that the CPU is going idle with tick stopped. /Users/rubber/linux/kernel/sched/fair.c: 10589
 * This info will be used in performing idle load balancing in the future. /Users/rubber/linux/kernel/sched/fair.c: 10590
	/* /Users/rubber/linux/kernel/sched/fair.c: 10606
	 * Can be set safely without rq->lock held /Users/rubber/linux/kernel/sched/fair.c: 10607
	 * If a clear happens, it will have evaluated last additions because /Users/rubber/linux/kernel/sched/fair.c: 10608
	 * rq->lock is held during the check and the clear /Users/rubber/linux/kernel/sched/fair.c: 10609
	/* /Users/rubber/linux/kernel/sched/fair.c: 10613
	 * The tick is still stopped but load could have been added in the /Users/rubber/linux/kernel/sched/fair.c: 10614
	 * meantime. We set the nohz.has_blocked flag to trig a check of the /Users/rubber/linux/kernel/sched/fair.c: 10615
	 * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear /Users/rubber/linux/kernel/sched/fair.c: 10616
	 * of nohz.has_blocked can only happen after checking the new load /Users/rubber/linux/kernel/sched/fair.c: 10617
	/* /Users/rubber/linux/kernel/sched/fair.c: 10631
	 * Ensures that if nohz_idle_balance() fails to observe our /Users/rubber/linux/kernel/sched/fair.c: 10632
	 * @idle_cpus_mask store, it must observe the @has_blocked /Users/rubber/linux/kernel/sched/fair.c: 10633
	 * and @needs_update stores. /Users/rubber/linux/kernel/sched/fair.c: 10634
	/* /Users/rubber/linux/kernel/sched/fair.c: 10642
	 * Each time a cpu enter idle, we assume that it has blocked load and /Users/rubber/linux/kernel/sched/fair.c: 10643
	 * enable the periodic update of the load of idle cpus /Users/rubber/linux/kernel/sched/fair.c: 10644
 * Internal function that runs load balance for all idle cpus. The load balance /Users/rubber/linux/kernel/sched/fair.c: 10668
 * can be a simple update of blocked load or a complete load balance with /Users/rubber/linux/kernel/sched/fair.c: 10669
 * tasks movement depending of flags. /Users/rubber/linux/kernel/sched/fair.c: 10670
	/* /Users/rubber/linux/kernel/sched/fair.c: 10686
	 * We assume there will be no idle load after this update and clear /Users/rubber/linux/kernel/sched/fair.c: 10687
	 * the has_blocked flag. If a cpu enters idle in the mean time, it will /Users/rubber/linux/kernel/sched/fair.c: 10688
	 * set the has_blocked flag and trigger another update of idle load. /Users/rubber/linux/kernel/sched/fair.c: 10689
	 * Because a cpu that becomes idle, is added to idle_cpus_mask before /Users/rubber/linux/kernel/sched/fair.c: 10690
	 * setting the flag, we are sure to not clear the state and not /Users/rubber/linux/kernel/sched/fair.c: 10691
	 * check the load of an idle cpu. /Users/rubber/linux/kernel/sched/fair.c: 10692
	 * /Users/rubber/linux/kernel/sched/fair.c: 10693
	 * Same applies to idle_cpus_mask vs needs_update. /Users/rubber/linux/kernel/sched/fair.c: 10694
	/* /Users/rubber/linux/kernel/sched/fair.c: 10701
	 * Ensures that if we miss the CPU, we must see the has_blocked /Users/rubber/linux/kernel/sched/fair.c: 10702
	 * store from nohz_balance_enter_idle(). /Users/rubber/linux/kernel/sched/fair.c: 10703
	/* /Users/rubber/linux/kernel/sched/fair.c: 10707
	 * Start with the next CPU after this_cpu so we will end with this_cpu and let a /Users/rubber/linux/kernel/sched/fair.c: 10708
	 * chance for other idle cpu to pull load. /Users/rubber/linux/kernel/sched/fair.c: 10709
		/* /Users/rubber/linux/kernel/sched/fair.c: 10715
		 * If this CPU gets work to do, stop the load balancing /Users/rubber/linux/kernel/sched/fair.c: 10716
		 * work being done for other CPUs. Next load /Users/rubber/linux/kernel/sched/fair.c: 10717
		 * balancing owner will pick it up. /Users/rubber/linux/kernel/sched/fair.c: 10718
		/* /Users/rubber/linux/kernel/sched/fair.c: 10733
		 * If time for next balance is due, /Users/rubber/linux/kernel/sched/fair.c: 10734
		 * do the balance. /Users/rubber/linux/kernel/sched/fair.c: 10735
	/* /Users/rubber/linux/kernel/sched/fair.c: 10754
	 * next_balance will be updated only when there is a need. /Users/rubber/linux/kernel/sched/fair.c: 10755
	 * When the CPU is attached to null domain for ex, it will not be /Users/rubber/linux/kernel/sched/fair.c: 10756
	 * updated. /Users/rubber/linux/kernel/sched/fair.c: 10757
 * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the /Users/rubber/linux/kernel/sched/fair.c: 10773
 * rebalancing for all the cpus for whom scheduler ticks are stopped. /Users/rubber/linux/kernel/sched/fair.c: 10774
 * Check if we need to run the ILB for updating blocked load before entering /Users/rubber/linux/kernel/sched/fair.c: 10794
 * idle state. /Users/rubber/linux/kernel/sched/fair.c: 10795
	/* /Users/rubber/linux/kernel/sched/fair.c: 10803
	 * Update the blocked load only if no SCHED_SOFTIRQ is about to happen /Users/rubber/linux/kernel/sched/fair.c: 10804
	 * (ie NOHZ_STATS_KICK set) and will do the same. /Users/rubber/linux/kernel/sched/fair.c: 10805
	/* /Users/rubber/linux/kernel/sched/fair.c: 10815
	 * This CPU doesn't want to be disturbed by scheduler /Users/rubber/linux/kernel/sched/fair.c: 10816
	 * housekeeping /Users/rubber/linux/kernel/sched/fair.c: 10817
	/* /Users/rubber/linux/kernel/sched/fair.c: 10831
	 * Set the need to trigger ILB in order to update blocked load /Users/rubber/linux/kernel/sched/fair.c: 10832
	 * before entering idle state. /Users/rubber/linux/kernel/sched/fair.c: 10833
 * newidle_balance is called by schedule() if this_cpu is about to become /Users/rubber/linux/kernel/sched/fair.c: 10850
 * idle. Attempts to pull tasks from other CPUs. /Users/rubber/linux/kernel/sched/fair.c: 10851
 * Returns: /Users/rubber/linux/kernel/sched/fair.c: 10853
 *   < 0 - we released the lock and there are !fair tasks present /Users/rubber/linux/kernel/sched/fair.c: 10854
 *     0 - failed, no new tasks /Users/rubber/linux/kernel/sched/fair.c: 10855
 *   > 0 - success, new (fair) tasks present /Users/rubber/linux/kernel/sched/fair.c: 10856
	/* /Users/rubber/linux/kernel/sched/fair.c: 10868
	 * There is a task waiting to run. No need to search for one. /Users/rubber/linux/kernel/sched/fair.c: 10869
	 * Return 0; the task will be enqueued when switching to idle. /Users/rubber/linux/kernel/sched/fair.c: 10870
	/* /Users/rubber/linux/kernel/sched/fair.c: 10875
	 * We must set idle_stamp _before_ calling idle_balance(), such that we /Users/rubber/linux/kernel/sched/fair.c: 10876
	 * measure the duration of idle_balance() as idle time. /Users/rubber/linux/kernel/sched/fair.c: 10877
	/* /Users/rubber/linux/kernel/sched/fair.c: 10881
	 * Do not pull tasks towards !active CPUs... /Users/rubber/linux/kernel/sched/fair.c: 10882
	/* /Users/rubber/linux/kernel/sched/fair.c: 10887
	 * This is OK, because current is on_cpu, which avoids it being picked /Users/rubber/linux/kernel/sched/fair.c: 10888
	 * for load-balance and preemption/IRQs are still disabled avoiding /Users/rubber/linux/kernel/sched/fair.c: 10889
	 * further scheduler activity on it and we're being very careful to /Users/rubber/linux/kernel/sched/fair.c: 10890
	 * re-start the picking loop. /Users/rubber/linux/kernel/sched/fair.c: 10891
		/* /Users/rubber/linux/kernel/sched/fair.c: 10938
		 * Stop searching for tasks to pull if there are /Users/rubber/linux/kernel/sched/fair.c: 10939
		 * now runnable tasks on this rq. /Users/rubber/linux/kernel/sched/fair.c: 10940
	/* /Users/rubber/linux/kernel/sched/fair.c: 10953
	 * While browsing the domains, we released the rq lock, a task could /Users/rubber/linux/kernel/sched/fair.c: 10954
	 * have been enqueued in the meantime. Since we're not going idle, /Users/rubber/linux/kernel/sched/fair.c: 10955
	 * pretend we pulled a task. /Users/rubber/linux/kernel/sched/fair.c: 10956
 * run_rebalance_domains is triggered when needed from the scheduler tick. /Users/rubber/linux/kernel/sched/fair.c: 10981
 * Also triggered for nohz idle balancing (with nohz_balancing_kick set). /Users/rubber/linux/kernel/sched/fair.c: 10982
	/* /Users/rubber/linux/kernel/sched/fair.c: 10990
	 * If this CPU has a pending nohz_balance_kick, then do the /Users/rubber/linux/kernel/sched/fair.c: 10991
	 * balancing on behalf of the other idle CPUs whose ticks are /Users/rubber/linux/kernel/sched/fair.c: 10992
	 * stopped. Do nohz_idle_balance *before* rebalance_domains to /Users/rubber/linux/kernel/sched/fair.c: 10993
	 * give the idle CPUs a chance to load balance. Else we may /Users/rubber/linux/kernel/sched/fair.c: 10994
	 * load balance only within the local sched_domain hierarchy /Users/rubber/linux/kernel/sched/fair.c: 10995
	 * and abort nohz_idle_balance altogether if we pull some load. /Users/rubber/linux/kernel/sched/fair.c: 10996
 * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing. /Users/rubber/linux/kernel/sched/fair.c: 11007
	/* /Users/rubber/linux/kernel/sched/fair.c: 11011
	 * Don't need to rebalance while attached to NULL domain or /Users/rubber/linux/kernel/sched/fair.c: 11012
	 * runqueue CPU is not active /Users/rubber/linux/kernel/sched/fair.c: 11013
	/* /Users/rubber/linux/kernel/sched/fair.c: 11057
	 * If runqueue has only one task which used up its slice and /Users/rubber/linux/kernel/sched/fair.c: 11058
	 * if the sibling is forced idle, then trigger schedule to /Users/rubber/linux/kernel/sched/fair.c: 11059
	 * give forced idle task a chance. /Users/rubber/linux/kernel/sched/fair.c: 11060
	 * /Users/rubber/linux/kernel/sched/fair.c: 11061
	 * sched_slice() considers only this active rq and it gets the /Users/rubber/linux/kernel/sched/fair.c: 11062
	 * whole slice. But during force idle, we have siblings acting /Users/rubber/linux/kernel/sched/fair.c: 11063
	 * like a single runqueue and hence we need to consider runnable /Users/rubber/linux/kernel/sched/fair.c: 11064
	 * tasks on this CPU and the forced idle CPU. Ideally, we should /Users/rubber/linux/kernel/sched/fair.c: 11065
	 * go through the forced idle rq, but that would be a perf hit. /Users/rubber/linux/kernel/sched/fair.c: 11066
	 * We can assume that the forced idle CPU has at least /Users/rubber/linux/kernel/sched/fair.c: 11067
	 * MIN_NR_TASKS_DURING_FORCEIDLE - 1 tasks and use that to check /Users/rubber/linux/kernel/sched/fair.c: 11068
	 * if we need to give up the CPU. /Users/rubber/linux/kernel/sched/fair.c: 11069
 * se_fi_update - Update the cfs_rq->min_vruntime_fi in a CFS hierarchy if needed. /Users/rubber/linux/kernel/sched/fair.c: 11077
	/* /Users/rubber/linux/kernel/sched/fair.c: 11116
	 * Find an se in the hierarchy for tasks a and b, such that the se's /Users/rubber/linux/kernel/sched/fair.c: 11117
	 * are immediate siblings. /Users/rubber/linux/kernel/sched/fair.c: 11118
	/* /Users/rubber/linux/kernel/sched/fair.c: 11140
	 * Find delta after normalizing se's vruntime with its cfs_rq's /Users/rubber/linux/kernel/sched/fair.c: 11141
	 * min_vruntime_fi, which would have been updated in prior calls /Users/rubber/linux/kernel/sched/fair.c: 11142
	 * to se_fi_update(). /Users/rubber/linux/kernel/sched/fair.c: 11143
 * scheduler tick hitting a task of our scheduling class. /Users/rubber/linux/kernel/sched/fair.c: 11155
 * NOTE: This function can be called remotely by the tick offload that /Users/rubber/linux/kernel/sched/fair.c: 11157
 * goes along full dynticks. Therefore no local assumption can be made /Users/rubber/linux/kernel/sched/fair.c: 11158
 * and everything must be accessed through the @rq and @curr passed in /Users/rubber/linux/kernel/sched/fair.c: 11159
 * parameters. /Users/rubber/linux/kernel/sched/fair.c: 11160
 * called on fork with the child task as argument from the parent's context /Users/rubber/linux/kernel/sched/fair.c: 11182
 *  - child not yet on the tasklist /Users/rubber/linux/kernel/sched/fair.c: 11183
 *  - preemption disabled /Users/rubber/linux/kernel/sched/fair.c: 11184
		/* /Users/rubber/linux/kernel/sched/fair.c: 11205
		 * Upon rescheduling, sched_class::put_prev_task() will place /Users/rubber/linux/kernel/sched/fair.c: 11206
		 * 'current' within the tree based on its new key value. /Users/rubber/linux/kernel/sched/fair.c: 11207
 * Priority of the task has changed. Check to see if we preempt /Users/rubber/linux/kernel/sched/fair.c: 11218
 * the current task. /Users/rubber/linux/kernel/sched/fair.c: 11219
	/* /Users/rubber/linux/kernel/sched/fair.c: 11230
	 * Reschedule if we are currently running on this runqueue and /Users/rubber/linux/kernel/sched/fair.c: 11231
	 * our priority decreased, or if we are not currently running on /Users/rubber/linux/kernel/sched/fair.c: 11232
	 * this runqueue and our priority is higher than the current's /Users/rubber/linux/kernel/sched/fair.c: 11233
	/* /Users/rubber/linux/kernel/sched/fair.c: 11246
	 * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases, /Users/rubber/linux/kernel/sched/fair.c: 11247
	 * the dequeue_entity(.flags=0) will already have normalized the /Users/rubber/linux/kernel/sched/fair.c: 11248
	 * vruntime. /Users/rubber/linux/kernel/sched/fair.c: 11249
	/* /Users/rubber/linux/kernel/sched/fair.c: 11254
	 * When !on_rq, vruntime of the task has usually NOT been normalized. /Users/rubber/linux/kernel/sched/fair.c: 11255
	 * But there are some cases where it has already been normalized: /Users/rubber/linux/kernel/sched/fair.c: 11256
	 * /Users/rubber/linux/kernel/sched/fair.c: 11257
	 * - A forked child which is waiting for being woken up by /Users/rubber/linux/kernel/sched/fair.c: 11258
	 *   wake_up_new_task(). /Users/rubber/linux/kernel/sched/fair.c: 11259
	 * - A task which has been woken up by try_to_wake_up() and /Users/rubber/linux/kernel/sched/fair.c: 11260
	 *   waiting for actually being woken up by sched_ttwu_pending(). /Users/rubber/linux/kernel/sched/fair.c: 11261
 * Propagate the changes of the sched_entity across the tg tree to make it /Users/rubber/linux/kernel/sched/fair.c: 11272
 * visible to the root /Users/rubber/linux/kernel/sched/fair.c: 11273
	/* /Users/rubber/linux/kernel/sched/fair.c: 11317
	 * Since the real-depth could have been changed (only FAIR /Users/rubber/linux/kernel/sched/fair.c: 11318
	 * class maintain depth value), reset depth properly. /Users/rubber/linux/kernel/sched/fair.c: 11319
		/* /Users/rubber/linux/kernel/sched/fair.c: 11337
		 * Fix up our vruntime so that the current sleep doesn't /Users/rubber/linux/kernel/sched/fair.c: 11338
		 * cause 'unlimited' sleep bonus. /Users/rubber/linux/kernel/sched/fair.c: 11339
		/* /Users/rubber/linux/kernel/sched/fair.c: 11369
		 * We were most likely switched from sched_rt, so /Users/rubber/linux/kernel/sched/fair.c: 11370
		 * kick off the schedule if running, otherwise just see /Users/rubber/linux/kernel/sched/fair.c: 11371
		 * if we can still preempt the current task. /Users/rubber/linux/kernel/sched/fair.c: 11372
/* Account for a task changing its policy or group. /Users/rubber/linux/kernel/sched/fair.c: 11381
 * This routine is mostly called to set cfs_rq->curr field when a task /Users/rubber/linux/kernel/sched/fair.c: 11383
 * migrates between groups/classes. /Users/rubber/linux/kernel/sched/fair.c: 11384
		/* /Users/rubber/linux/kernel/sched/fair.c: 11392
		 * Move the next running task to the front of the list, so our /Users/rubber/linux/kernel/sched/fair.c: 11393
		 * cfs_tasks list becomes MRU one. /Users/rubber/linux/kernel/sched/fair.c: 11394
		/* /Users/rubber/linux/kernel/sched/fair.c: 11541
		 * Only empty task groups can be destroyed; so we can speculatively /Users/rubber/linux/kernel/sched/fair.c: 11542
		 * check on_list without danger of it being re-added. /Users/rubber/linux/kernel/sched/fair.c: 11543
	/* /Users/rubber/linux/kernel/sched/fair.c: 11595
	 * We can't change the weight of the root cgroup. /Users/rubber/linux/kernel/sched/fair.c: 11596
	/* /Users/rubber/linux/kernel/sched/fair.c: 11733
	 * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise /Users/rubber/linux/kernel/sched/fair.c: 11734
	 * idle runqueue: /Users/rubber/linux/kernel/sched/fair.c: 11735
 * All the scheduling class methods: /Users/rubber/linux/kernel/sched/fair.c: 11744
 * Helper functions to facilitate extracting info from tracepoints. /Users/rubber/linux/kernel/sched/fair.c: 11843
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/sched/idle.c: 1
 * Generic entry points for the idle threads and /Users/rubber/linux/kernel/sched/idle.c: 3
 * implementation of the idle task scheduling class. /Users/rubber/linux/kernel/sched/idle.c: 4
 * (NOTE: these are not related to SCHED_IDLE batch scheduled /Users/rubber/linux/kernel/sched/idle.c: 6
 *        tasks which are handled in sched/fair.c ) /Users/rubber/linux/kernel/sched/idle.c: 7
 * sched_idle_set_state - Record idle state for the current CPU. /Users/rubber/linux/kernel/sched/idle.c: 17
 * @idle_state: State to record. /Users/rubber/linux/kernel/sched/idle.c: 18
 * default_idle_call - Default CPU idle routine. /Users/rubber/linux/kernel/sched/idle.c: 85
 * To use when the cpuidle framework cannot be used. /Users/rubber/linux/kernel/sched/idle.c: 87
		/* /Users/rubber/linux/kernel/sched/idle.c: 98
		 * arch_cpu_idle() is supposed to enable IRQs, however /Users/rubber/linux/kernel/sched/idle.c: 99
		 * we can't do that because of RCU and tracing. /Users/rubber/linux/kernel/sched/idle.c: 100
		 * /Users/rubber/linux/kernel/sched/idle.c: 101
		 * Trace IRQs enable here, then switch off RCU, and have /Users/rubber/linux/kernel/sched/idle.c: 102
		 * arch_cpu_idle() use raw_local_irq_enable(). Note that /Users/rubber/linux/kernel/sched/idle.c: 103
		 * rcu_idle_enter() relies on lockdep IRQ state, so switch that /Users/rubber/linux/kernel/sched/idle.c: 104
		 * last -- this is very similar to the entry code. /Users/rubber/linux/kernel/sched/idle.c: 105
		/* /Users/rubber/linux/kernel/sched/idle.c: 114
		 * OK, so IRQs are enabled here, but RCU needs them disabled to /Users/rubber/linux/kernel/sched/idle.c: 115
		 * turn itself back on.. funny thing is that disabling IRQs /Users/rubber/linux/kernel/sched/idle.c: 116
		 * will cause tracing, which needs RCU. Jump through hoops to /Users/rubber/linux/kernel/sched/idle.c: 117
		 * make it 'work'. /Users/rubber/linux/kernel/sched/idle.c: 118
	/* /Users/rubber/linux/kernel/sched/idle.c: 143
	 * The idle task must be scheduled, it is pointless to go to idle, just /Users/rubber/linux/kernel/sched/idle.c: 144
	 * update no idle residency and return. /Users/rubber/linux/kernel/sched/idle.c: 145
	/* /Users/rubber/linux/kernel/sched/idle.c: 153
	 * Enter the idle state previously returned by the governor decision. /Users/rubber/linux/kernel/sched/idle.c: 154
	 * This function will block until an interrupt occurs and will take /Users/rubber/linux/kernel/sched/idle.c: 155
	 * care of re-enabling the local interrupts /Users/rubber/linux/kernel/sched/idle.c: 156
 * cpuidle_idle_call - the main idle function /Users/rubber/linux/kernel/sched/idle.c: 162
 * NOTE: no locks or semaphores should be used here /Users/rubber/linux/kernel/sched/idle.c: 164
 * On architectures that support TIF_POLLING_NRFLAG, is called with polling /Users/rubber/linux/kernel/sched/idle.c: 166
 * set, and it returns with polling set.  If it ever stops polling, it /Users/rubber/linux/kernel/sched/idle.c: 167
 * must clear the polling bit. /Users/rubber/linux/kernel/sched/idle.c: 168
	/* /Users/rubber/linux/kernel/sched/idle.c: 176
	 * Check if the idle task must be rescheduled. If it is the /Users/rubber/linux/kernel/sched/idle.c: 177
	 * case, exit the function after re-enabling the local irq. /Users/rubber/linux/kernel/sched/idle.c: 178
	/* /Users/rubber/linux/kernel/sched/idle.c: 185
	 * The RCU framework needs to be told that we are entering an idle /Users/rubber/linux/kernel/sched/idle.c: 186
	 * section, so no more rcu read side critical sections and one more /Users/rubber/linux/kernel/sched/idle.c: 187
	 * step to the grace period /Users/rubber/linux/kernel/sched/idle.c: 188
	/* /Users/rubber/linux/kernel/sched/idle.c: 198
	 * Suspend-to-idle ("s2idle") is a system state in which all user space /Users/rubber/linux/kernel/sched/idle.c: 199
	 * has been frozen, all I/O devices have been suspended and the only /Users/rubber/linux/kernel/sched/idle.c: 200
	 * activity happens here and in interrupts (if any). In that case bypass /Users/rubber/linux/kernel/sched/idle.c: 201
	 * the cpuidle governor and go straight for the deepest idle state /Users/rubber/linux/kernel/sched/idle.c: 202
	 * available.  Possibly also suspend the local tick and the entire /Users/rubber/linux/kernel/sched/idle.c: 203
	 * timekeeping to prevent timer interrupts from kicking us out of idle /Users/rubber/linux/kernel/sched/idle.c: 204
	 * until a proper wakeup interrupt happens. /Users/rubber/linux/kernel/sched/idle.c: 205
		/* /Users/rubber/linux/kernel/sched/idle.c: 229
		 * Ask the cpuidle framework to choose a convenient idle state. /Users/rubber/linux/kernel/sched/idle.c: 230
		/* /Users/rubber/linux/kernel/sched/idle.c: 240
		 * Give the governor an opportunity to reflect on the outcome /Users/rubber/linux/kernel/sched/idle.c: 241
	/* /Users/rubber/linux/kernel/sched/idle.c: 249
	 * It is up to the idle functions to reenable local interrupts /Users/rubber/linux/kernel/sched/idle.c: 250
 * Generic idle loop implementation /Users/rubber/linux/kernel/sched/idle.c: 257
 * Called with polling cleared. /Users/rubber/linux/kernel/sched/idle.c: 259
	/* /Users/rubber/linux/kernel/sched/idle.c: 265
	 * Check if we need to update blocked load /Users/rubber/linux/kernel/sched/idle.c: 266
	/* /Users/rubber/linux/kernel/sched/idle.c: 270
	 * If the arch has a polling bit, we maintain an invariant: /Users/rubber/linux/kernel/sched/idle.c: 271
	 * /Users/rubber/linux/kernel/sched/idle.c: 272
	 * Our polling bit is clear if we're not scheduled (i.e. if rq->curr != /Users/rubber/linux/kernel/sched/idle.c: 273
	 * rq->idle). This means that, if rq->idle has the polling bit set, /Users/rubber/linux/kernel/sched/idle.c: 274
	 * then setting need_resched is guaranteed to cause the CPU to /Users/rubber/linux/kernel/sched/idle.c: 275
	 * reschedule. /Users/rubber/linux/kernel/sched/idle.c: 276
		/* /Users/rubber/linux/kernel/sched/idle.c: 296
		 * In poll mode we reenable interrupts and spin. Also if we /Users/rubber/linux/kernel/sched/idle.c: 297
		 * detected in the wakeup from idle path that the tick /Users/rubber/linux/kernel/sched/idle.c: 298
		 * broadcast device expired for us, we don't want to go deep /Users/rubber/linux/kernel/sched/idle.c: 299
		 * idle as we know that the IPI is going to arrive right away. /Users/rubber/linux/kernel/sched/idle.c: 300
	/* /Users/rubber/linux/kernel/sched/idle.c: 311
	 * Since we fell out of the loop above, we know TIF_NEED_RESCHED must /Users/rubber/linux/kernel/sched/idle.c: 312
	 * be set, propagate it into PREEMPT_NEED_RESCHED. /Users/rubber/linux/kernel/sched/idle.c: 313
	 * /Users/rubber/linux/kernel/sched/idle.c: 314
	 * This is required because for polling idle loops we will not have had /Users/rubber/linux/kernel/sched/idle.c: 315
	 * an IPI to fold the state for us. /Users/rubber/linux/kernel/sched/idle.c: 316
	/* /Users/rubber/linux/kernel/sched/idle.c: 322
	 * We promise to call sched_ttwu_pending() and reschedule if /Users/rubber/linux/kernel/sched/idle.c: 323
	 * need_resched() is set while polling is set. That means that clearing /Users/rubber/linux/kernel/sched/idle.c: 324
	 * polling needs to be visible before doing these things. /Users/rubber/linux/kernel/sched/idle.c: 325
	/* /Users/rubber/linux/kernel/sched/idle.c: 329
	 * RCU relies on this call to be done outside of an RCU read-side /Users/rubber/linux/kernel/sched/idle.c: 330
	 * critical section. /Users/rubber/linux/kernel/sched/idle.c: 331
	/* /Users/rubber/linux/kernel/sched/idle.c: 365
	 * Only FIFO tasks can disable the tick since they don't need the forced /Users/rubber/linux/kernel/sched/idle.c: 366
	 * preemption. /Users/rubber/linux/kernel/sched/idle.c: 367
 * idle-task scheduling class. /Users/rubber/linux/kernel/sched/idle.c: 407
 * Idle tasks are unconditionally rescheduled: /Users/rubber/linux/kernel/sched/idle.c: 425
 * It is not legal to sleep in the idle task - print a warning /Users/rubber/linux/kernel/sched/idle.c: 460
 * message if some code attempts to do it: /Users/rubber/linux/kernel/sched/idle.c: 461
 * scheduler tick hitting a task of our scheduling class. /Users/rubber/linux/kernel/sched/idle.c: 473
 * NOTE: This function can be called remotely by the tick offload that /Users/rubber/linux/kernel/sched/idle.c: 475
 * goes along full dynticks. Therefore no local assumption can be made /Users/rubber/linux/kernel/sched/idle.c: 476
 * and everything must be accessed through the @rq and @curr passed in /Users/rubber/linux/kernel/sched/idle.c: 477
 * parameters. /Users/rubber/linux/kernel/sched/idle.c: 478
 * Simple, special scheduling class for the per-CPU idle tasks: /Users/rubber/linux/kernel/sched/idle.c: 500
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 1
 * CPUFreq governor based on scheduler-provided CPU utilization data. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 3
 * Copyright (C) 2016, Intel Corporation /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 5
 * Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com> /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 6
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 74
	 * Since cpufreq_update_util() is called with rq->lock held for /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 75
	 * the @target_cpu, our per-CPU data is fully serialized. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 76
	 * /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 77
	 * However, drivers cannot in general deal with cross-CPU /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 78
	 * requests, so while get_next_freq() will work, our /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 79
	 * sugov_update_commit() call may not for the fast switching platforms. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 80
	 * /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 81
	 * Hence stop here for remote requests if they aren't supported /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 82
	 * by the hardware, as calculating the frequency is pointless if /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 83
	 * we cannot in fact act on it. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 84
	 * /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 85
	 * This is needed on the slow switching platforms too to prevent CPUs /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 86
	 * going offline from leaving stale IRQ work items behind. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 87
 * get_next_freq - Compute a new frequency for a given cpufreq policy. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 126
 * @sg_policy: schedutil policy object to compute the new frequency for. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 127
 * @util: Current CPU utilization. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 128
 * @max: CPU capacity. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 129
 * If the utilization is frequency-invariant, choose the new frequency to be /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 131
 * proportional to it, that is /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 132
 * next_freq = C * max_freq * util / max /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 134
 * Otherwise, approximate the would-be frequency-invariant utilization by /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 136
 * util_raw * (curr_freq / max_freq) which leads to /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 137
 * next_freq = C * curr_freq * util_raw / max /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 139
 * Take C = 1.25 for the frequency tipping point at (util / max) = 0.8. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 141
 * The lowest driver-supported frequency which is equal or greater than the raw /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 143
 * next_freq (as calculated above) is returned, subject to policy min/max and /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 144
 * cpufreq driver limitations. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 145
 * sugov_iowait_reset() - Reset the IO boost status of a CPU. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 176
 * @sg_cpu: the sugov data for the CPU to boost /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 177
 * @time: the update time from the caller /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 178
 * @set_iowait_boost: true if an IO boost has been requested /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 179
 * The IO wait boost of a task is disabled after a tick since the last update /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 181
 * of a CPU. If a new IO wait boost is requested after more then a tick, then /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 182
 * we enable the boost starting from IOWAIT_BOOST_MIN, which improves energy /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 183
 * efficiency by ignoring sporadic wakeups from IO. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 184
 * sugov_iowait_boost() - Updates the IO boost status of a CPU. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 202
 * @sg_cpu: the sugov data for the CPU to boost /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 203
 * @time: the update time from the caller /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 204
 * @flags: SCHED_CPUFREQ_IOWAIT if the task is waking up after an IO wait /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 205
 * Each time a task wakes up after an IO operation, the CPU utilization can be /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 207
 * boosted to a certain utilization which doubles at each "frequent and /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 208
 * successive" wakeup from IO, ranging from IOWAIT_BOOST_MIN to the utilization /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 209
 * of the maximum OPP. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 210
 * To keep doubling, an IO boost has to be requested at least once per tick, /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 212
 * otherwise we restart from the utilization of the minimum OPP. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 213
 * sugov_iowait_apply() - Apply the IO boost to a CPU. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 246
 * @sg_cpu: the sugov data for the cpu to boost /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 247
 * @time: the update time from the caller /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 248
 * A CPU running a task which woken up after an IO operation can have its /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 250
 * utilization boosted to speed up the completion of those IO operations. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 251
 * The IO boost value is increased each time a task wakes up from IO, in /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 252
 * sugov_iowait_apply(), and it's instead decreased by this function, /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 253
 * each time an increase has not been requested (!iowait_boost_pending). /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 254
 * A CPU which also appears to have been idle for at least one tick has also /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 256
 * its IO boost utilization reset. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 257
 * This mechanism is designed to boost high frequently IO waiting tasks, while /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 259
 * being more conservative on tasks which does sporadic IO operations. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 260
		/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 275
		 * No boost pending; reduce the boost value. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 276
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 287
	 * sg_cpu->util is already in capacity scale; convert iowait_boost /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 288
	 * into the same scale so we can compare. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 289
 * Make sugov_should_update_freq() ignore the rate limit when DL /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 310
 * has increased the utilization. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 311
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 348
	 * Do not reduce the frequency if the CPU has not been idle /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 349
	 * recently, as the reduction is likely to be premature then. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 350
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 362
	 * This code runs under rq->lock for the target CPU, so it won't run /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 363
	 * concurrently on two different CPUs for the same target and it is not /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 364
	 * necessary to acquire the lock in the fast switch case. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 365
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 382
	 * Fall back to the "frequency" path if frequency invariance is not /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 383
	 * supported, because the direct mapping between the utilization and /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 384
	 * the performance levels depends on the frequency invariance. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 385
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 395
	 * Do not reduce the target performance level if the CPU has not been /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 396
	 * idle recently, as the reduction is likely to be premature then. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 397
	/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 468
	 * Hold sg_policy->update_lock shortly to handle the case where: /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 469
	 * in case sg_policy->next_freq is read here, and then updated by /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 470
	 * sugov_deferred_update() just before work_in_progress is set to false /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 471
	 * here, we may miss queueing the new update. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 472
	 * /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 473
	 * Note: If a work was queued after the update_lock is released, /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 474
	 * sugov_work() will just be called again by kthread_work code; and the /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 475
	 * request will be proceed before the sugov thread sleeps. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 476
		/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 584
		 * Fake (unused) bandwidth; workaround to "fix" /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 585
		 * priority inheritance. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 586
 * EAS shouldn't be attempted without sugov, so rebuild the sched_domains /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 847
 * on governor changes to make sure the scheduler knows about it. /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 848
		/* /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 854
		 * When called from the cpufreq_register_driver() path, the /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 855
		 * cpu_hotplug_lock is already held, so use a work item to /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 856
		 * avoid nested locking in rebuild_sched_domains(). /Users/rubber/linux/kernel/sched/cpufreq_schedutil.c: 857
 * Pressure stall information for CPU, memory and IO /Users/rubber/linux/kernel/sched/psi.c: 2
 * Copyright (c) 2018 Facebook, Inc. /Users/rubber/linux/kernel/sched/psi.c: 4
 * Author: Johannes Weiner <hannes@cmpxchg.org> /Users/rubber/linux/kernel/sched/psi.c: 5
 * Polling support by Suren Baghdasaryan <surenb@google.com> /Users/rubber/linux/kernel/sched/psi.c: 7
 * Copyright (c) 2018 Google, Inc. /Users/rubber/linux/kernel/sched/psi.c: 8
 * When CPU, memory and IO are contended, tasks experience delays that /Users/rubber/linux/kernel/sched/psi.c: 10
 * reduce throughput and introduce latencies into the workload. Memory /Users/rubber/linux/kernel/sched/psi.c: 11
 * and IO contention, in addition, can cause a full loss of forward /Users/rubber/linux/kernel/sched/psi.c: 12
 * progress in which the CPU goes idle. /Users/rubber/linux/kernel/sched/psi.c: 13
 * This code aggregates individual task delays into resource pressure /Users/rubber/linux/kernel/sched/psi.c: 15
 * metrics that indicate problems with both workload health and /Users/rubber/linux/kernel/sched/psi.c: 16
 * resource utilization. /Users/rubber/linux/kernel/sched/psi.c: 17
 *			Model /Users/rubber/linux/kernel/sched/psi.c: 19
 * The time in which a task can execute on a CPU is our baseline for /Users/rubber/linux/kernel/sched/psi.c: 21
 * productivity. Pressure expresses the amount of time in which this /Users/rubber/linux/kernel/sched/psi.c: 22
 * potential cannot be realized due to resource contention. /Users/rubber/linux/kernel/sched/psi.c: 23
 * This concept of productivity has two components: the workload and /Users/rubber/linux/kernel/sched/psi.c: 25
 * the CPU. To measure the impact of pressure on both, we define two /Users/rubber/linux/kernel/sched/psi.c: 26
 * contention states for a resource: SOME and FULL. /Users/rubber/linux/kernel/sched/psi.c: 27
 * In the SOME state of a given resource, one or more tasks are /Users/rubber/linux/kernel/sched/psi.c: 29
 * delayed on that resource. This affects the workload's ability to /Users/rubber/linux/kernel/sched/psi.c: 30
 * perform work, but the CPU may still be executing other tasks. /Users/rubber/linux/kernel/sched/psi.c: 31
 * In the FULL state of a given resource, all non-idle tasks are /Users/rubber/linux/kernel/sched/psi.c: 33
 * delayed on that resource such that nobody is advancing and the CPU /Users/rubber/linux/kernel/sched/psi.c: 34
 * goes idle. This leaves both workload and CPU unproductive. /Users/rubber/linux/kernel/sched/psi.c: 35
 * Naturally, the FULL state doesn't exist for the CPU resource at the /Users/rubber/linux/kernel/sched/psi.c: 37
 * system level, but exist at the cgroup level, means all non-idle tasks /Users/rubber/linux/kernel/sched/psi.c: 38
 * in a cgroup are delayed on the CPU resource which used by others outside /Users/rubber/linux/kernel/sched/psi.c: 39
 * of the cgroup or throttled by the cgroup cpu.max configuration. /Users/rubber/linux/kernel/sched/psi.c: 40
 *	SOME = nr_delayed_tasks != 0 /Users/rubber/linux/kernel/sched/psi.c: 42
 *	FULL = nr_delayed_tasks != 0 && nr_running_tasks == 0 /Users/rubber/linux/kernel/sched/psi.c: 43
 * The percentage of wallclock time spent in those compound stall /Users/rubber/linux/kernel/sched/psi.c: 45
 * states gives pressure numbers between 0 and 100 for each resource, /Users/rubber/linux/kernel/sched/psi.c: 46
 * where the SOME percentage indicates workload slowdowns and the FULL /Users/rubber/linux/kernel/sched/psi.c: 47
 * percentage indicates reduced CPU utilization: /Users/rubber/linux/kernel/sched/psi.c: 48
 *	%SOME = time(SOME) / period /Users/rubber/linux/kernel/sched/psi.c: 50
 *	%FULL = time(FULL) / period /Users/rubber/linux/kernel/sched/psi.c: 51
 *			Multiple CPUs /Users/rubber/linux/kernel/sched/psi.c: 53
 * The more tasks and available CPUs there are, the more work can be /Users/rubber/linux/kernel/sched/psi.c: 55
 * performed concurrently. This means that the potential that can go /Users/rubber/linux/kernel/sched/psi.c: 56
 * unrealized due to resource contention *also* scales with non-idle /Users/rubber/linux/kernel/sched/psi.c: 57
 * tasks and CPUs. /Users/rubber/linux/kernel/sched/psi.c: 58
 * Consider a scenario where 257 number crunching tasks are trying to /Users/rubber/linux/kernel/sched/psi.c: 60
 * run concurrently on 256 CPUs. If we simply aggregated the task /Users/rubber/linux/kernel/sched/psi.c: 61
 * states, we would have to conclude a CPU SOME pressure number of /Users/rubber/linux/kernel/sched/psi.c: 62
 * 100%, since *somebody* is waiting on a runqueue at all /Users/rubber/linux/kernel/sched/psi.c: 63
 * times. However, that is clearly not the amount of contention the /Users/rubber/linux/kernel/sched/psi.c: 64
 * workload is experiencing: only one out of 256 possible execution /Users/rubber/linux/kernel/sched/psi.c: 65
 * threads will be contended at any given time, or about 0.4%. /Users/rubber/linux/kernel/sched/psi.c: 66
 * Conversely, consider a scenario of 4 tasks and 4 CPUs where at any /Users/rubber/linux/kernel/sched/psi.c: 68
 * given time *one* of the tasks is delayed due to a lack of memory. /Users/rubber/linux/kernel/sched/psi.c: 69
 * Again, looking purely at the task state would yield a memory FULL /Users/rubber/linux/kernel/sched/psi.c: 70
 * pressure number of 0%, since *somebody* is always making forward /Users/rubber/linux/kernel/sched/psi.c: 71
 * progress. But again this wouldn't capture the amount of execution /Users/rubber/linux/kernel/sched/psi.c: 72
 * potential lost, which is 1 out of 4 CPUs, or 25%. /Users/rubber/linux/kernel/sched/psi.c: 73
 * To calculate wasted potential (pressure) with multiple processors, /Users/rubber/linux/kernel/sched/psi.c: 75
 * we have to base our calculation on the number of non-idle tasks in /Users/rubber/linux/kernel/sched/psi.c: 76
 * conjunction with the number of available CPUs, which is the number /Users/rubber/linux/kernel/sched/psi.c: 77
 * of potential execution threads. SOME becomes then the proportion of /Users/rubber/linux/kernel/sched/psi.c: 78
 * delayed tasks to possible threads, and FULL is the share of possible /Users/rubber/linux/kernel/sched/psi.c: 79
 * threads that are unproductive due to delays: /Users/rubber/linux/kernel/sched/psi.c: 80
 *	threads = min(nr_nonidle_tasks, nr_cpus) /Users/rubber/linux/kernel/sched/psi.c: 82
 *	   SOME = min(nr_delayed_tasks / threads, 1) /Users/rubber/linux/kernel/sched/psi.c: 83
 *	   FULL = (threads - min(nr_running_tasks, threads)) / threads /Users/rubber/linux/kernel/sched/psi.c: 84
 * For the 257 number crunchers on 256 CPUs, this yields: /Users/rubber/linux/kernel/sched/psi.c: 86
 *	threads = min(257, 256) /Users/rubber/linux/kernel/sched/psi.c: 88
 *	   SOME = min(1 / 256, 1)             = 0.4% /Users/rubber/linux/kernel/sched/psi.c: 89
 *	   FULL = (256 - min(257, 256)) / 256 = 0% /Users/rubber/linux/kernel/sched/psi.c: 90
 * For the 1 out of 4 memory-delayed tasks, this yields: /Users/rubber/linux/kernel/sched/psi.c: 92
 *	threads = min(4, 4) /Users/rubber/linux/kernel/sched/psi.c: 94
 *	   SOME = min(1 / 4, 1)               = 25% /Users/rubber/linux/kernel/sched/psi.c: 95
 *	   FULL = (4 - min(3, 4)) / 4         = 25% /Users/rubber/linux/kernel/sched/psi.c: 96
 * [ Substitute nr_cpus with 1, and you can see that it's a natural /Users/rubber/linux/kernel/sched/psi.c: 98
 *   extension of the single-CPU model. ] /Users/rubber/linux/kernel/sched/psi.c: 99
 *			Implementation /Users/rubber/linux/kernel/sched/psi.c: 101
 * To assess the precise time spent in each such state, we would have /Users/rubber/linux/kernel/sched/psi.c: 103
 * to freeze the system on task changes and start/stop the state /Users/rubber/linux/kernel/sched/psi.c: 104
 * clocks accordingly. Obviously that doesn't scale in practice. /Users/rubber/linux/kernel/sched/psi.c: 105
 * Because the scheduler aims to distribute the compute load evenly /Users/rubber/linux/kernel/sched/psi.c: 107
 * among the available CPUs, we can track task state locally to each /Users/rubber/linux/kernel/sched/psi.c: 108
 * CPU and, at much lower frequency, extrapolate the global state for /Users/rubber/linux/kernel/sched/psi.c: 109
 * the cumulative stall times and the running averages. /Users/rubber/linux/kernel/sched/psi.c: 110
 * For each runqueue, we track: /Users/rubber/linux/kernel/sched/psi.c: 112
 *	   tSOME[cpu] = time(nr_delayed_tasks[cpu] != 0) /Users/rubber/linux/kernel/sched/psi.c: 114
 *	   tFULL[cpu] = time(nr_delayed_tasks[cpu] && !nr_running_tasks[cpu]) /Users/rubber/linux/kernel/sched/psi.c: 115
 *	tNONIDLE[cpu] = time(nr_nonidle_tasks[cpu] != 0) /Users/rubber/linux/kernel/sched/psi.c: 116
 * and then periodically aggregate: /Users/rubber/linux/kernel/sched/psi.c: 118
 *	tNONIDLE = sum(tNONIDLE[i]) /Users/rubber/linux/kernel/sched/psi.c: 120
 *	   tSOME = sum(tSOME[i] * tNONIDLE[i]) / tNONIDLE /Users/rubber/linux/kernel/sched/psi.c: 122
 *	   tFULL = sum(tFULL[i] * tNONIDLE[i]) / tNONIDLE /Users/rubber/linux/kernel/sched/psi.c: 123
 *	   %SOME = tSOME / period /Users/rubber/linux/kernel/sched/psi.c: 125
 *	   %FULL = tFULL / period /Users/rubber/linux/kernel/sched/psi.c: 126
 * This gives us an approximation of pressure that is practical /Users/rubber/linux/kernel/sched/psi.c: 128
 * cost-wise, yet way more sensitive and accurate than periodic /Users/rubber/linux/kernel/sched/psi.c: 129
 * sampling of the aggregate task states would be. /Users/rubber/linux/kernel/sched/psi.c: 130
		/* /Users/rubber/linux/kernel/sched/psi.c: 273
		 * In addition to already concluded states, we also /Users/rubber/linux/kernel/sched/psi.c: 274
		 * incorporate currently active states on the CPU, /Users/rubber/linux/kernel/sched/psi.c: 275
		 * since states may last for many sampling periods. /Users/rubber/linux/kernel/sched/psi.c: 276
		 * /Users/rubber/linux/kernel/sched/psi.c: 277
		 * This way we keep our delta sampling buckets small /Users/rubber/linux/kernel/sched/psi.c: 278
		 * (u32) and our reported pressure close to what's /Users/rubber/linux/kernel/sched/psi.c: 279
		 * actually happening. /Users/rubber/linux/kernel/sched/psi.c: 280
	/* /Users/rubber/linux/kernel/sched/psi.c: 324
	 * Collect the per-cpu time buckets and average them into a /Users/rubber/linux/kernel/sched/psi.c: 325
	 * single time sample that is normalized to wallclock time. /Users/rubber/linux/kernel/sched/psi.c: 326
	 * /Users/rubber/linux/kernel/sched/psi.c: 327
	 * For averaging, each CPU is weighted by its non-idle time in /Users/rubber/linux/kernel/sched/psi.c: 328
	 * the sampling period. This eliminates artifacts from uneven /Users/rubber/linux/kernel/sched/psi.c: 329
	 * loading, or even entirely idle CPUs. /Users/rubber/linux/kernel/sched/psi.c: 330
	/* /Users/rubber/linux/kernel/sched/psi.c: 348
	 * Integrate the sample into the running statistics that are /Users/rubber/linux/kernel/sched/psi.c: 349
	 * reported to userspace: the cumulative stall times and the /Users/rubber/linux/kernel/sched/psi.c: 350
	 * decaying averages. /Users/rubber/linux/kernel/sched/psi.c: 351
	 * /Users/rubber/linux/kernel/sched/psi.c: 352
	 * Pressure percentages are sampled at PSI_FREQ. We might be /Users/rubber/linux/kernel/sched/psi.c: 353
	 * called more often when the user polls more frequently than /Users/rubber/linux/kernel/sched/psi.c: 354
	 * that; we might be called less often when there is no task /Users/rubber/linux/kernel/sched/psi.c: 355
	 * activity, thus no data, and clock ticks are sporadic. The /Users/rubber/linux/kernel/sched/psi.c: 356
	 * below handles both. /Users/rubber/linux/kernel/sched/psi.c: 357
	/* /Users/rubber/linux/kernel/sched/psi.c: 381
	 * The periodic clock tick can get delayed for various /Users/rubber/linux/kernel/sched/psi.c: 382
	 * reasons, especially on loaded systems. To avoid clock /Users/rubber/linux/kernel/sched/psi.c: 383
	 * drift, we schedule the clock in fixed psi_period intervals. /Users/rubber/linux/kernel/sched/psi.c: 384
	 * But the deltas we sample out of the per-cpu buckets above /Users/rubber/linux/kernel/sched/psi.c: 385
	 * are based on the actual time elapsing between clock ticks. /Users/rubber/linux/kernel/sched/psi.c: 386
		/* /Users/rubber/linux/kernel/sched/psi.c: 396
		 * Due to the lockless sampling of the time buckets, /Users/rubber/linux/kernel/sched/psi.c: 397
		 * recorded time deltas can slip into the next period, /Users/rubber/linux/kernel/sched/psi.c: 398
		 * which under full pressure can result in samples in /Users/rubber/linux/kernel/sched/psi.c: 399
		 * excess of the period length. /Users/rubber/linux/kernel/sched/psi.c: 400
		 * /Users/rubber/linux/kernel/sched/psi.c: 401
		 * We don't want to report non-sensical pressures in /Users/rubber/linux/kernel/sched/psi.c: 402
		 * excess of 100%, nor do we want to drop such events /Users/rubber/linux/kernel/sched/psi.c: 403
		 * on the floor. Instead we punt any overage into the /Users/rubber/linux/kernel/sched/psi.c: 404
		 * future until pressure subsides. By doing this we /Users/rubber/linux/kernel/sched/psi.c: 405
		 * don't underreport the occurring pressure curve, we /Users/rubber/linux/kernel/sched/psi.c: 406
		 * just report it delayed by one period length. /Users/rubber/linux/kernel/sched/psi.c: 407
		 * /Users/rubber/linux/kernel/sched/psi.c: 408
		 * The error isn't cumulative. As soon as another /Users/rubber/linux/kernel/sched/psi.c: 409
		 * delta slips from a period P to P+1, by definition /Users/rubber/linux/kernel/sched/psi.c: 410
		 * it frees up its time T in P. /Users/rubber/linux/kernel/sched/psi.c: 411
	/* /Users/rubber/linux/kernel/sched/psi.c: 439
	 * If there is task activity, periodically fold the per-cpu /Users/rubber/linux/kernel/sched/psi.c: 440
	 * times and feed samples into the running averages. If things /Users/rubber/linux/kernel/sched/psi.c: 441
	 * are idle and there is no data to process, stop the clock. /Users/rubber/linux/kernel/sched/psi.c: 442
	 * Once restarted, we'll catch up the running averages in one /Users/rubber/linux/kernel/sched/psi.c: 443
	 * go - see calc_avgs() and missed_periods. /Users/rubber/linux/kernel/sched/psi.c: 444
 * PSI growth tracking window update and growth calculation routine. /Users/rubber/linux/kernel/sched/psi.c: 467
 * This approximates a sliding tracking window by interpolating /Users/rubber/linux/kernel/sched/psi.c: 469
 * partially elapsed windows using historical growth data from the /Users/rubber/linux/kernel/sched/psi.c: 470
 * previous intervals. This minimizes memory requirements (by not storing /Users/rubber/linux/kernel/sched/psi.c: 471
 * all the intermediate values in the previous window) and simplifies /Users/rubber/linux/kernel/sched/psi.c: 472
 * the calculations. It works well because PSI signal changes only in /Users/rubber/linux/kernel/sched/psi.c: 473
 * positive direction and over relatively small window sizes the growth /Users/rubber/linux/kernel/sched/psi.c: 474
 * is close to linear. /Users/rubber/linux/kernel/sched/psi.c: 475
	/* /Users/rubber/linux/kernel/sched/psi.c: 484
	 * After each tracking window passes win->start_value and /Users/rubber/linux/kernel/sched/psi.c: 485
	 * win->start_time get reset and win->prev_growth stores /Users/rubber/linux/kernel/sched/psi.c: 486
	 * the average per-window growth of the previous window. /Users/rubber/linux/kernel/sched/psi.c: 487
	 * win->prev_growth is then used to interpolate additional /Users/rubber/linux/kernel/sched/psi.c: 488
	 * growth from the previous window assuming it was linear. /Users/rubber/linux/kernel/sched/psi.c: 489
	/* /Users/rubber/linux/kernel/sched/psi.c: 521
	 * On subsequent updates, calculate growth deltas and let /Users/rubber/linux/kernel/sched/psi.c: 522
	 * watchers know when their specified thresholds are exceeded. /Users/rubber/linux/kernel/sched/psi.c: 523
		/* /Users/rubber/linux/kernel/sched/psi.c: 532
		 * Multiple triggers might be looking at the same state, /Users/rubber/linux/kernel/sched/psi.c: 533
		 * remember to update group->polling_total[] once we've /Users/rubber/linux/kernel/sched/psi.c: 534
		 * been through all of them. Also remember to extend the /Users/rubber/linux/kernel/sched/psi.c: 535
		 * polling time if we see new stall activity. /Users/rubber/linux/kernel/sched/psi.c: 536
	/* /Users/rubber/linux/kernel/sched/psi.c: 567
	 * Do not reschedule if already scheduled. /Users/rubber/linux/kernel/sched/psi.c: 568
	 * Possible race with a timer scheduled after this check but before /Users/rubber/linux/kernel/sched/psi.c: 569
	 * mod_timer below can be tolerated because group->polling_next_update /Users/rubber/linux/kernel/sched/psi.c: 570
	 * will keep updates on schedule. /Users/rubber/linux/kernel/sched/psi.c: 571
	/* /Users/rubber/linux/kernel/sched/psi.c: 579
	 * kworker might be NULL in case psi_trigger_destroy races with /Users/rubber/linux/kernel/sched/psi.c: 580
	 * psi_task_change (hotpath) which can't use locks /Users/rubber/linux/kernel/sched/psi.c: 581
		/* /Users/rubber/linux/kernel/sched/psi.c: 605
		 * Keep the monitor active for at least the duration of the /Users/rubber/linux/kernel/sched/psi.c: 606
		 * minimum tracking window as long as monitor states are /Users/rubber/linux/kernel/sched/psi.c: 607
		 * changing. /Users/rubber/linux/kernel/sched/psi.c: 608
	/* /Users/rubber/linux/kernel/sched/psi.c: 695
	 * First we assess the aggregate resource states this CPU's /Users/rubber/linux/kernel/sched/psi.c: 696
	 * tasks have been in since the last change, and account any /Users/rubber/linux/kernel/sched/psi.c: 697
	 * SOME and FULL time these may have resulted in. /Users/rubber/linux/kernel/sched/psi.c: 698
	 * /Users/rubber/linux/kernel/sched/psi.c: 699
	 * Then we update the task counts according to the state /Users/rubber/linux/kernel/sched/psi.c: 700
	 * change requested through the @clear and @set bits. /Users/rubber/linux/kernel/sched/psi.c: 701
	/* /Users/rubber/linux/kernel/sched/psi.c: 731
	 * Since we care about lost potential, a memstall is FULL /Users/rubber/linux/kernel/sched/psi.c: 732
	 * when there are no other working tasks, but also when /Users/rubber/linux/kernel/sched/psi.c: 733
	 * the CPU is actively reclaiming and nothing productive /Users/rubber/linux/kernel/sched/psi.c: 734
	 * could run even if it were runnable. So when the current /Users/rubber/linux/kernel/sched/psi.c: 735
	 * task in a cgroup is in_memstall, the corresponding groupc /Users/rubber/linux/kernel/sched/psi.c: 736
	 * on that cpu is in PSI_MEM_FULL state. /Users/rubber/linux/kernel/sched/psi.c: 737
	/* /Users/rubber/linux/kernel/sched/psi.c: 806
	 * Periodic aggregation shuts off if there is a period of no /Users/rubber/linux/kernel/sched/psi.c: 807
	 * task changes, so we wake it back up if necessary. However, /Users/rubber/linux/kernel/sched/psi.c: 808
	 * don't do this if the task change is the aggregation worker /Users/rubber/linux/kernel/sched/psi.c: 809
	 * itself going to sleep, or we'll ping-pong forever. /Users/rubber/linux/kernel/sched/psi.c: 810
		/* /Users/rubber/linux/kernel/sched/psi.c: 833
		 * When switching between tasks that have an identical /Users/rubber/linux/kernel/sched/psi.c: 834
		 * runtime state, the cgroup that contains both tasks /Users/rubber/linux/kernel/sched/psi.c: 835
		 * runtime state, the cgroup that contains both tasks /Users/rubber/linux/kernel/sched/psi.c: 836
		 * we reach the first common ancestor. Iterate @next's /Users/rubber/linux/kernel/sched/psi.c: 837
		 * ancestors only until we encounter @prev's ONCPU. /Users/rubber/linux/kernel/sched/psi.c: 838
		/* /Users/rubber/linux/kernel/sched/psi.c: 856
		 * When we're going to sleep, psi_dequeue() lets us handle /Users/rubber/linux/kernel/sched/psi.c: 857
		 * TSK_RUNNING and TSK_IOWAIT here, where we can combine it /Users/rubber/linux/kernel/sched/psi.c: 858
		 * with TSK_ONCPU and save walking common ancestors twice. /Users/rubber/linux/kernel/sched/psi.c: 859
		/* /Users/rubber/linux/kernel/sched/psi.c: 873
		 * TSK_ONCPU is handled up to the common ancestor. If we're tasked /Users/rubber/linux/kernel/sched/psi.c: 874
		 * with dequeuing too, finish that for the rest of the hierarchy. /Users/rubber/linux/kernel/sched/psi.c: 875
 * psi_memstall_enter - mark the beginning of a memory stall section /Users/rubber/linux/kernel/sched/psi.c: 886
 * @flags: flags to handle nested sections /Users/rubber/linux/kernel/sched/psi.c: 887
 * Marks the calling task as being stalled due to a lack of memory, /Users/rubber/linux/kernel/sched/psi.c: 889
 * such as waiting for a refault or performing reclaim. /Users/rubber/linux/kernel/sched/psi.c: 890
	/* /Users/rubber/linux/kernel/sched/psi.c: 903
	 * in_memstall setting & accounting needs to be atomic wrt /Users/rubber/linux/kernel/sched/psi.c: 904
	 * changes to the task's scheduling state, otherwise we can /Users/rubber/linux/kernel/sched/psi.c: 905
	 * race with CPU migration. /Users/rubber/linux/kernel/sched/psi.c: 906
 * psi_memstall_leave - mark the end of an memory stall section /Users/rubber/linux/kernel/sched/psi.c: 917
 * @flags: flags to handle nested memdelay sections /Users/rubber/linux/kernel/sched/psi.c: 918
 * Marks the calling task as no longer stalled due to lack of memory. /Users/rubber/linux/kernel/sched/psi.c: 920
	/* /Users/rubber/linux/kernel/sched/psi.c: 932
	 * in_memstall clearing & accounting needs to be atomic wrt /Users/rubber/linux/kernel/sched/psi.c: 933
	 * changes to the task's scheduling state, otherwise we could /Users/rubber/linux/kernel/sched/psi.c: 934
	 * race with CPU migration. /Users/rubber/linux/kernel/sched/psi.c: 935
 * cgroup_move_task - move task to a different cgroup /Users/rubber/linux/kernel/sched/psi.c: 970
 * @task: the task /Users/rubber/linux/kernel/sched/psi.c: 971
 * @to: the target css_set /Users/rubber/linux/kernel/sched/psi.c: 972
 * Move task to a new cgroup and safely migrate its associated stall /Users/rubber/linux/kernel/sched/psi.c: 974
 * state between the different groups. /Users/rubber/linux/kernel/sched/psi.c: 975
 * This function acquires the task's rq lock to lock out concurrent /Users/rubber/linux/kernel/sched/psi.c: 977
 * changes to the task's scheduling state and - in case the task is /Users/rubber/linux/kernel/sched/psi.c: 978
 * running - concurrent changes to its stall state. /Users/rubber/linux/kernel/sched/psi.c: 979
		/* /Users/rubber/linux/kernel/sched/psi.c: 988
		 * Lame to do this here, but the scheduler cannot be locked /Users/rubber/linux/kernel/sched/psi.c: 989
		 * from the outside, so we move cgroups from inside sched/. /Users/rubber/linux/kernel/sched/psi.c: 990
	/* /Users/rubber/linux/kernel/sched/psi.c: 998
	 * We may race with schedule() dropping the rq lock between /Users/rubber/linux/kernel/sched/psi.c: 999
	 * deactivating prev and switching to next. Because the psi /Users/rubber/linux/kernel/sched/psi.c: 1000
	 * updates from the deactivation are deferred to the switch /Users/rubber/linux/kernel/sched/psi.c: 1001
	 * callback to save cgroup tree updates, the task's scheduling /Users/rubber/linux/kernel/sched/psi.c: 1002
	 * state here is not coherent with its psi state: /Users/rubber/linux/kernel/sched/psi.c: 1003
	 * /Users/rubber/linux/kernel/sched/psi.c: 1004
	 * schedule()                   cgroup_move_task() /Users/rubber/linux/kernel/sched/psi.c: 1005
	 *   rq_lock() /Users/rubber/linux/kernel/sched/psi.c: 1006
	 *   deactivate_task() /Users/rubber/linux/kernel/sched/psi.c: 1007
	 *     p->on_rq = 0 /Users/rubber/linux/kernel/sched/psi.c: 1008
	 *     psi_dequeue() // defers TSK_RUNNING & TSK_IOWAIT updates /Users/rubber/linux/kernel/sched/psi.c: 1009
	 *   pick_next_task() /Users/rubber/linux/kernel/sched/psi.c: 1010
	 *     rq_unlock() /Users/rubber/linux/kernel/sched/psi.c: 1011
	 *                                rq_lock() /Users/rubber/linux/kernel/sched/psi.c: 1012
	 *                                psi_task_change() // old cgroup /Users/rubber/linux/kernel/sched/psi.c: 1013
	 *                                task->cgroups = to /Users/rubber/linux/kernel/sched/psi.c: 1014
	 *                                psi_task_change() // new cgroup /Users/rubber/linux/kernel/sched/psi.c: 1015
	 *                                rq_unlock() /Users/rubber/linux/kernel/sched/psi.c: 1016
	 *     rq_lock() /Users/rubber/linux/kernel/sched/psi.c: 1017
	 *   psi_sched_switch() // does deferred updates in new cgroup /Users/rubber/linux/kernel/sched/psi.c: 1018
	 * /Users/rubber/linux/kernel/sched/psi.c: 1019
	 * Don't rely on the scheduling state. Use psi_flags instead. /Users/rubber/linux/kernel/sched/psi.c: 1020
	/* /Users/rubber/linux/kernel/sched/psi.c: 1192
	 * Wakeup waiters to stop polling. Can happen if cgroup is deleted /Users/rubber/linux/kernel/sched/psi.c: 1193
	 * from under a polling process. /Users/rubber/linux/kernel/sched/psi.c: 1194
	/* /Users/rubber/linux/kernel/sched/psi.c: 1226
	 * Wait for both *trigger_ptr from psi_trigger_replace and /Users/rubber/linux/kernel/sched/psi.c: 1227
	 * poll_task RCUs to complete their read-side critical sections /Users/rubber/linux/kernel/sched/psi.c: 1228
	 * before destroying the trigger and optionally the poll_task /Users/rubber/linux/kernel/sched/psi.c: 1229
	/* /Users/rubber/linux/kernel/sched/psi.c: 1232
	 * Stop kthread 'psimon' after releasing trigger_lock to prevent a /Users/rubber/linux/kernel/sched/psi.c: 1233
	 * deadlock while waiting for psi_poll_work to acquire trigger_lock /Users/rubber/linux/kernel/sched/psi.c: 1234
		/* /Users/rubber/linux/kernel/sched/psi.c: 1237
		 * After the RCU grace period has expired, the worker /Users/rubber/linux/kernel/sched/psi.c: 1238
		 * can no longer be found through group->poll_task. /Users/rubber/linux/kernel/sched/psi.c: 1239
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/loadavg.c: 1
 * kernel/sched/loadavg.c /Users/rubber/linux/kernel/sched/loadavg.c: 3
 * This file contains the magic bits required to compute the global loadavg /Users/rubber/linux/kernel/sched/loadavg.c: 5
 * figure. Its a silly number but people think its important. We go through /Users/rubber/linux/kernel/sched/loadavg.c: 6
 * great pains to make it work on big machines and tickless kernels. /Users/rubber/linux/kernel/sched/loadavg.c: 7
 * Global load-average calculations /Users/rubber/linux/kernel/sched/loadavg.c: 12
 * We take a distributed and async approach to calculating the global load-avg /Users/rubber/linux/kernel/sched/loadavg.c: 14
 * in order to minimize overhead. /Users/rubber/linux/kernel/sched/loadavg.c: 15
 * The global load average is an exponentially decaying average of nr_running + /Users/rubber/linux/kernel/sched/loadavg.c: 17
 * nr_uninterruptible. /Users/rubber/linux/kernel/sched/loadavg.c: 18
 * Once every LOAD_FREQ: /Users/rubber/linux/kernel/sched/loadavg.c: 20
 *   nr_active = 0; /Users/rubber/linux/kernel/sched/loadavg.c: 22
 *   for_each_possible_cpu(cpu) /Users/rubber/linux/kernel/sched/loadavg.c: 23
 *	nr_active += cpu_of(cpu)->nr_running + cpu_of(cpu)->nr_uninterruptible; /Users/rubber/linux/kernel/sched/loadavg.c: 24
 *   avenrun[n] = avenrun[0] * exp_n + nr_active * (1 - exp_n) /Users/rubber/linux/kernel/sched/loadavg.c: 26
 * Due to a number of reasons the above turns in the mess below: /Users/rubber/linux/kernel/sched/loadavg.c: 28
 *  - for_each_possible_cpu() is prohibitively expensive on machines with /Users/rubber/linux/kernel/sched/loadavg.c: 30
 *    serious number of CPUs, therefore we need to take a distributed approach /Users/rubber/linux/kernel/sched/loadavg.c: 31
 *    to calculating nr_active. /Users/rubber/linux/kernel/sched/loadavg.c: 32
 *        \Sum_i x_i(t) = \Sum_i x_i(t) - x_i(t_0) | x_i(t_0) := 0 /Users/rubber/linux/kernel/sched/loadavg.c: 34
 *                      = \Sum_i { \Sum_j=1 x_i(t_j) - x_i(t_j-1) } /Users/rubber/linux/kernel/sched/loadavg.c: 35
 *    So assuming nr_active := 0 when we start out -- true per definition, we /Users/rubber/linux/kernel/sched/loadavg.c: 37
 *    can simply take per-CPU deltas and fold those into a global accumulate /Users/rubber/linux/kernel/sched/loadavg.c: 38
 *    to obtain the same result. See calc_load_fold_active(). /Users/rubber/linux/kernel/sched/loadavg.c: 39
 *    Furthermore, in order to avoid synchronizing all per-CPU delta folding /Users/rubber/linux/kernel/sched/loadavg.c: 41
 *    across the machine, we assume 10 ticks is sufficient time for every /Users/rubber/linux/kernel/sched/loadavg.c: 42
 *    CPU to have completed this task. /Users/rubber/linux/kernel/sched/loadavg.c: 43
 *    This places an upper-bound on the IRQ-off latency of the machine. Then /Users/rubber/linux/kernel/sched/loadavg.c: 45
 *    again, being late doesn't loose the delta, just wrecks the sample. /Users/rubber/linux/kernel/sched/loadavg.c: 46
 *  - cpu_rq()->nr_uninterruptible isn't accurately tracked per-CPU because /Users/rubber/linux/kernel/sched/loadavg.c: 48
 *    this would add another cross-CPU cacheline miss and atomic operation /Users/rubber/linux/kernel/sched/loadavg.c: 49
 *    to the wakeup path. Instead we increment on whatever CPU the task ran /Users/rubber/linux/kernel/sched/loadavg.c: 50
 *    when it went into uninterruptible state and decrement on whatever CPU /Users/rubber/linux/kernel/sched/loadavg.c: 51
 *    did the wakeup. This means that only the sum of nr_uninterruptible over /Users/rubber/linux/kernel/sched/loadavg.c: 52
 *    all CPUs yields the correct result. /Users/rubber/linux/kernel/sched/loadavg.c: 53
 *  This covers the NO_HZ=n code, for extra head-aches, see the comment below. /Users/rubber/linux/kernel/sched/loadavg.c: 55
 * get_avenrun - get the load average array /Users/rubber/linux/kernel/sched/loadavg.c: 65
 * @loads:	pointer to dest load array /Users/rubber/linux/kernel/sched/loadavg.c: 66
 * @offset:	offset to add /Users/rubber/linux/kernel/sched/loadavg.c: 67
 * @shift:	shift count to shift the result left /Users/rubber/linux/kernel/sched/loadavg.c: 68
 * These values are estimates at best, so no need for locking. /Users/rubber/linux/kernel/sched/loadavg.c: 70
 * fixed_power_int - compute: x^n, in O(log n) time /Users/rubber/linux/kernel/sched/loadavg.c: 95
 * @x:         base of the power /Users/rubber/linux/kernel/sched/loadavg.c: 97
 * @frac_bits: fractional bits of @x /Users/rubber/linux/kernel/sched/loadavg.c: 98
 * @n:         power to raise @x to. /Users/rubber/linux/kernel/sched/loadavg.c: 99
 * By exploiting the relation between the definition of the natural power /Users/rubber/linux/kernel/sched/loadavg.c: 101
 * function: x^n := x*x*...*x (x multiplied by itself for n times), and /Users/rubber/linux/kernel/sched/loadavg.c: 102
 * the binary encoding of numbers used by computers: n := \Sum n_i * 2^i, /Users/rubber/linux/kernel/sched/loadavg.c: 103
 * (where: n_i \elem {0, 1}, the binary vector representing n), /Users/rubber/linux/kernel/sched/loadavg.c: 104
 * we find: x^n := x^(\Sum n_i * 2^i) := \Prod x^(n_i * 2^i), which is /Users/rubber/linux/kernel/sched/loadavg.c: 105
 * of course trivially computable in O(log_2 n), the length of our binary /Users/rubber/linux/kernel/sched/loadavg.c: 106
 * vector. /Users/rubber/linux/kernel/sched/loadavg.c: 107
 * a1 = a0 * e + a * (1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 134
 * a2 = a1 * e + a * (1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 136
 *    = (a0 * e + a * (1 - e)) * e + a * (1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 137
 *    = a0 * e^2 + a * (1 - e) * (1 + e) /Users/rubber/linux/kernel/sched/loadavg.c: 138
 * a3 = a2 * e + a * (1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 140
 *    = (a0 * e^2 + a * (1 - e) * (1 + e)) * e + a * (1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 141
 *    = a0 * e^3 + a * (1 - e) * (1 + e + e^2) /Users/rubber/linux/kernel/sched/loadavg.c: 142
 *  ... /Users/rubber/linux/kernel/sched/loadavg.c: 144
 * an = a0 * e^n + a * (1 - e) * (1 + e + ... + e^n-1) [1] /Users/rubber/linux/kernel/sched/loadavg.c: 146
 *    = a0 * e^n + a * (1 - e) * (1 - e^n)/(1 - e) /Users/rubber/linux/kernel/sched/loadavg.c: 147
 *    = a0 * e^n + a * (1 - e^n) /Users/rubber/linux/kernel/sched/loadavg.c: 148
 * [1] application of the geometric series: /Users/rubber/linux/kernel/sched/loadavg.c: 150
 *              n         1 - x^(n+1) /Users/rubber/linux/kernel/sched/loadavg.c: 152
 *     S_n := \Sum x^i = ------------- /Users/rubber/linux/kernel/sched/loadavg.c: 153
 *             i=0          1 - x /Users/rubber/linux/kernel/sched/loadavg.c: 154
 * Handle NO_HZ for the global load-average. /Users/rubber/linux/kernel/sched/loadavg.c: 165
 * Since the above described distributed algorithm to compute the global /Users/rubber/linux/kernel/sched/loadavg.c: 167
 * load-average relies on per-CPU sampling from the tick, it is affected by /Users/rubber/linux/kernel/sched/loadavg.c: 168
 * NO_HZ. /Users/rubber/linux/kernel/sched/loadavg.c: 169
 * The basic idea is to fold the nr_active delta into a global NO_HZ-delta upon /Users/rubber/linux/kernel/sched/loadavg.c: 171
 * entering NO_HZ state such that we can include this as an 'extra' CPU delta /Users/rubber/linux/kernel/sched/loadavg.c: 172
 * when we read the global state. /Users/rubber/linux/kernel/sched/loadavg.c: 173
 * Obviously reality has to ruin such a delightfully simple scheme: /Users/rubber/linux/kernel/sched/loadavg.c: 175
 *  - When we go NO_HZ idle during the window, we can negate our sample /Users/rubber/linux/kernel/sched/loadavg.c: 177
 *    contribution, causing under-accounting. /Users/rubber/linux/kernel/sched/loadavg.c: 178
 *    We avoid this by keeping two NO_HZ-delta counters and flipping them /Users/rubber/linux/kernel/sched/loadavg.c: 180
 *    when the window starts, thus separating old and new NO_HZ load. /Users/rubber/linux/kernel/sched/loadavg.c: 181
 *    The only trick is the slight shift in index flip for read vs write. /Users/rubber/linux/kernel/sched/loadavg.c: 183
 *        0s            5s            10s           15s /Users/rubber/linux/kernel/sched/loadavg.c: 185
 *          +10           +10           +10           +10 /Users/rubber/linux/kernel/sched/loadavg.c: 186
 *        |-|-----------|-|-----------|-|-----------|-| /Users/rubber/linux/kernel/sched/loadavg.c: 187
 *    r:0 0 1           1 0           0 1           1 0 /Users/rubber/linux/kernel/sched/loadavg.c: 188
 *    w:0 1 1           0 0           1 1           0 0 /Users/rubber/linux/kernel/sched/loadavg.c: 189
 *    This ensures we'll fold the old NO_HZ contribution in this window while /Users/rubber/linux/kernel/sched/loadavg.c: 191
 *    accumulating the new one. /Users/rubber/linux/kernel/sched/loadavg.c: 192
 *  - When we wake up from NO_HZ during the window, we push up our /Users/rubber/linux/kernel/sched/loadavg.c: 194
 *    contribution, since we effectively move our sample point to a known /Users/rubber/linux/kernel/sched/loadavg.c: 195
 *    busy state. /Users/rubber/linux/kernel/sched/loadavg.c: 196
 *    This is solved by pushing the window forward, and thus skipping the /Users/rubber/linux/kernel/sched/loadavg.c: 198
 *    sample, for this CPU (effectively using the NO_HZ-delta for this CPU which /Users/rubber/linux/kernel/sched/loadavg.c: 199
 *    was in effect at the time the window opened). This also solves the issue /Users/rubber/linux/kernel/sched/loadavg.c: 200
 *    of having to deal with a CPU having been in NO_HZ for multiple LOAD_FREQ /Users/rubber/linux/kernel/sched/loadavg.c: 201
 *    intervals. /Users/rubber/linux/kernel/sched/loadavg.c: 202
 * When making the ILB scale, we should try to pull this in as well. /Users/rubber/linux/kernel/sched/loadavg.c: 204
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 213
	 * See calc_global_nohz(), if we observe the new index, we also /Users/rubber/linux/kernel/sched/loadavg.c: 214
	 * need to observe the new update time. /Users/rubber/linux/kernel/sched/loadavg.c: 215
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 219
	 * If the folding window started, make sure we start writing in the /Users/rubber/linux/kernel/sched/loadavg.c: 220
	 * next NO_HZ-delta. /Users/rubber/linux/kernel/sched/loadavg.c: 221
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 248
	 * We're going into NO_HZ mode, if there's any pending delta, fold it /Users/rubber/linux/kernel/sched/loadavg.c: 249
	 * into the pending NO_HZ delta. /Users/rubber/linux/kernel/sched/loadavg.c: 250
 * Keep track of the load for NOHZ_FULL, must be called between /Users/rubber/linux/kernel/sched/loadavg.c: 256
 * calc_load_nohz_{start,stop}(). /Users/rubber/linux/kernel/sched/loadavg.c: 257
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 268
	 * If we're still before the pending sample window, we're done. /Users/rubber/linux/kernel/sched/loadavg.c: 269
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 275
	 * We woke inside or after the sample window, this means we're already /Users/rubber/linux/kernel/sched/loadavg.c: 276
	 * accounted through the nohz accounting, so skip the entire deal and /Users/rubber/linux/kernel/sched/loadavg.c: 277
	 * sync up for the next window. /Users/rubber/linux/kernel/sched/loadavg.c: 278
 * NO_HZ can leave us missing all per-CPU ticks calling /Users/rubber/linux/kernel/sched/loadavg.c: 296
 * calc_load_fold_active(), but since a NO_HZ CPU folds its delta into /Users/rubber/linux/kernel/sched/loadavg.c: 297
 * calc_load_nohz per calc_load_nohz_start(), all we need to do is fold /Users/rubber/linux/kernel/sched/loadavg.c: 298
 * in the pending NO_HZ delta if our NO_HZ period crossed a load cycle boundary. /Users/rubber/linux/kernel/sched/loadavg.c: 299
 * Once we've updated the global active value, we need to apply the exponential /Users/rubber/linux/kernel/sched/loadavg.c: 301
 * weights adjusted to the number of cycles missed. /Users/rubber/linux/kernel/sched/loadavg.c: 302
		/* /Users/rubber/linux/kernel/sched/loadavg.c: 311
		 * Catch-up, fold however many we are behind still /Users/rubber/linux/kernel/sched/loadavg.c: 312
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 327
	 * Flip the NO_HZ index... /Users/rubber/linux/kernel/sched/loadavg.c: 328
	 * /Users/rubber/linux/kernel/sched/loadavg.c: 329
	 * Make sure we first write the new time then flip the index, so that /Users/rubber/linux/kernel/sched/loadavg.c: 330
	 * calc_load_write_idx() will see the new time when it reads the new /Users/rubber/linux/kernel/sched/loadavg.c: 331
	 * index, this avoids a double flip messing things up. /Users/rubber/linux/kernel/sched/loadavg.c: 332
 * calc_load - update the avenrun load estimates 10 ticks after the /Users/rubber/linux/kernel/sched/loadavg.c: 345
 * CPUs have updated calc_load_tasks. /Users/rubber/linux/kernel/sched/loadavg.c: 346
 * Called from the global timer code. /Users/rubber/linux/kernel/sched/loadavg.c: 348
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 359
	 * Fold the 'old' NO_HZ-delta to include all NO_HZ CPUs. /Users/rubber/linux/kernel/sched/loadavg.c: 360
	/* /Users/rubber/linux/kernel/sched/loadavg.c: 375
	 * In case we went to NO_HZ for multiple LOAD_FREQ intervals /Users/rubber/linux/kernel/sched/loadavg.c: 376
	 * catch up in bulk. /Users/rubber/linux/kernel/sched/loadavg.c: 377
 * Called from scheduler_tick() to periodically update this CPU's /Users/rubber/linux/kernel/sched/loadavg.c: 383
 * active count. /Users/rubber/linux/kernel/sched/loadavg.c: 384
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/sched/swait.c: 1
 * <linux/swait.h> (simple wait queues ) implementation: /Users/rubber/linux/kernel/sched/swait.c: 3
 * The thing about the wake_up_state() return value; I think we can ignore it. /Users/rubber/linux/kernel/sched/swait.c: 17
 * If for some reason it would return 0, that means the previously waiting /Users/rubber/linux/kernel/sched/swait.c: 19
 * task is already running, so it will observe condition true (or has already). /Users/rubber/linux/kernel/sched/swait.c: 20
 * Wake up all waiters. This is an interface which is solely exposed for /Users/rubber/linux/kernel/sched/swait.c: 36
 * completions and not for general usage. /Users/rubber/linux/kernel/sched/swait.c: 37
 * It is intentionally different from swake_up_all() to allow usage from /Users/rubber/linux/kernel/sched/swait.c: 39
 * hard interrupt context and interrupt disabled regions. /Users/rubber/linux/kernel/sched/swait.c: 40
 * Does not allow usage from IRQ disabled, since we must be able to /Users/rubber/linux/kernel/sched/swait.c: 59
 * release IRQs to guarantee bounded hold time. /Users/rubber/linux/kernel/sched/swait.c: 60
		/* /Users/rubber/linux/kernel/sched/swait.c: 110
		 * See prepare_to_wait_event(). TL;DR, subsequent swake_up_one() /Users/rubber/linux/kernel/sched/swait.c: 111
		 * must not see us. /Users/rubber/linux/kernel/sched/swait.c: 112
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/cgroup/rdma.c: 1
 * RDMA resource limiting controller for cgroups. /Users/rubber/linux/kernel/cgroup/rdma.c: 3
 * Used to allow a cgroup hierarchy to stop processes from consuming /Users/rubber/linux/kernel/cgroup/rdma.c: 5
 * additional RDMA resources after a certain limit is reached. /Users/rubber/linux/kernel/cgroup/rdma.c: 6
 * Copyright (C) 2016 Parav Pandit <pandit.parav@gmail.com> /Users/rubber/linux/kernel/cgroup/rdma.c: 8
 * Protects list of resource pools maintained on per cgroup basis /Users/rubber/linux/kernel/cgroup/rdma.c: 21
 * and rdma device list. /Users/rubber/linux/kernel/cgroup/rdma.c: 22
 * resource table definition as to be seen by the user. /Users/rubber/linux/kernel/cgroup/rdma.c: 33
 * Need to add entries to it when more resources are /Users/rubber/linux/kernel/cgroup/rdma.c: 34
 * added/defined at IB verb/core layer. /Users/rubber/linux/kernel/cgroup/rdma.c: 35
 * resource pool object which represents per cgroup, per device /Users/rubber/linux/kernel/cgroup/rdma.c: 49
 * resources. There are multiple instances of this object per cgroup, /Users/rubber/linux/kernel/cgroup/rdma.c: 50
 * therefore it cannot be embedded within rdma_cgroup structure. It /Users/rubber/linux/kernel/cgroup/rdma.c: 51
 * is maintained as list. /Users/rubber/linux/kernel/cgroup/rdma.c: 52
 * uncharge_cg_locked - uncharge resource for rdma cgroup /Users/rubber/linux/kernel/cgroup/rdma.c: 152
 * @cg: pointer to cg to uncharge and all parents in hierarchy /Users/rubber/linux/kernel/cgroup/rdma.c: 153
 * @device: pointer to rdmacg device /Users/rubber/linux/kernel/cgroup/rdma.c: 154
 * @index: index of the resource to uncharge in cg (resource pool) /Users/rubber/linux/kernel/cgroup/rdma.c: 155
 * It also frees the resource pool which was created as part of /Users/rubber/linux/kernel/cgroup/rdma.c: 157
 * charging operation when there are no resources attached to /Users/rubber/linux/kernel/cgroup/rdma.c: 158
 * resource pool. /Users/rubber/linux/kernel/cgroup/rdma.c: 159
	/* /Users/rubber/linux/kernel/cgroup/rdma.c: 170
	 * rpool cannot be null at this stage. Let kernel operate in case /Users/rubber/linux/kernel/cgroup/rdma.c: 171
	 * if there a bug in IB stack or rdma controller, instead of crashing /Users/rubber/linux/kernel/cgroup/rdma.c: 172
	 * the system. /Users/rubber/linux/kernel/cgroup/rdma.c: 173
	/* /Users/rubber/linux/kernel/cgroup/rdma.c: 182
	 * A negative count (or overflow) is invalid, /Users/rubber/linux/kernel/cgroup/rdma.c: 183
	 * it indicates a bug in the rdma controller. /Users/rubber/linux/kernel/cgroup/rdma.c: 184
		/* /Users/rubber/linux/kernel/cgroup/rdma.c: 190
		 * No user of the rpool and all entries are set to max, so /Users/rubber/linux/kernel/cgroup/rdma.c: 191
		 * safe to delete this rpool. /Users/rubber/linux/kernel/cgroup/rdma.c: 192
 * rdmacg_uncharge_hierarchy - hierarchically uncharge rdma resource count /Users/rubber/linux/kernel/cgroup/rdma.c: 199
 * @device: pointer to rdmacg device /Users/rubber/linux/kernel/cgroup/rdma.c: 200
 * @stop_cg: while traversing hirerchy, when meet with stop_cg cgroup /Users/rubber/linux/kernel/cgroup/rdma.c: 201
 *           stop uncharging /Users/rubber/linux/kernel/cgroup/rdma.c: 202
 * @index: index of the resource to uncharge in cg in given resource pool /Users/rubber/linux/kernel/cgroup/rdma.c: 203
 * rdmacg_uncharge - hierarchically uncharge rdma resource count /Users/rubber/linux/kernel/cgroup/rdma.c: 223
 * @device: pointer to rdmacg device /Users/rubber/linux/kernel/cgroup/rdma.c: 224
 * @index: index of the resource to uncharge in cgroup in given resource pool /Users/rubber/linux/kernel/cgroup/rdma.c: 225
 * rdmacg_try_charge - hierarchically try to charge the rdma resource /Users/rubber/linux/kernel/cgroup/rdma.c: 239
 * @rdmacg: pointer to rdma cgroup which will own this resource /Users/rubber/linux/kernel/cgroup/rdma.c: 240
 * @device: pointer to rdmacg device /Users/rubber/linux/kernel/cgroup/rdma.c: 241
 * @index: index of the resource to charge in cgroup (resource pool) /Users/rubber/linux/kernel/cgroup/rdma.c: 242
 * This function follows charging resource in hierarchical way. /Users/rubber/linux/kernel/cgroup/rdma.c: 244
 * It will fail if the charge would cause the new value to exceed the /Users/rubber/linux/kernel/cgroup/rdma.c: 245
 * hierarchical limit. /Users/rubber/linux/kernel/cgroup/rdma.c: 246
 * Returns 0 if the charge succeeded, otherwise -EAGAIN, -ENOMEM or -EINVAL. /Users/rubber/linux/kernel/cgroup/rdma.c: 247
 * Returns pointer to rdmacg for this resource when charging is successful. /Users/rubber/linux/kernel/cgroup/rdma.c: 248
 * Charger needs to account resources on two criteria. /Users/rubber/linux/kernel/cgroup/rdma.c: 250
 * (a) per cgroup & (b) per device resource usage. /Users/rubber/linux/kernel/cgroup/rdma.c: 251
 * Per cgroup resource usage ensures that tasks of cgroup doesn't cross /Users/rubber/linux/kernel/cgroup/rdma.c: 252
 * the configured limits. Per device provides granular configuration /Users/rubber/linux/kernel/cgroup/rdma.c: 253
 * in multi device usage. It allocates resource pool in the hierarchy /Users/rubber/linux/kernel/cgroup/rdma.c: 254
 * for each parent it come across for first resource. Later on resource /Users/rubber/linux/kernel/cgroup/rdma.c: 255
 * pool will be available. Therefore it will be much faster thereon /Users/rubber/linux/kernel/cgroup/rdma.c: 256
 * to charge/uncharge. /Users/rubber/linux/kernel/cgroup/rdma.c: 257
	/* /Users/rubber/linux/kernel/cgroup/rdma.c: 271
	 * hold on to css, as cgroup can be removed but resource /Users/rubber/linux/kernel/cgroup/rdma.c: 272
	 * accounting happens on css. /Users/rubber/linux/kernel/cgroup/rdma.c: 273
 * rdmacg_register_device - register rdmacg device to rdma controller. /Users/rubber/linux/kernel/cgroup/rdma.c: 307
 * @device: pointer to rdmacg device whose resources need to be accounted. /Users/rubber/linux/kernel/cgroup/rdma.c: 308
 * If IB stack wish a device to participate in rdma cgroup resource /Users/rubber/linux/kernel/cgroup/rdma.c: 310
 * tracking, it must invoke this API to register with rdma cgroup before /Users/rubber/linux/kernel/cgroup/rdma.c: 311
 * any user space application can start using the RDMA resources. /Users/rubber/linux/kernel/cgroup/rdma.c: 312
 * rdmacg_unregister_device - unregister rdmacg device from rdma controller. /Users/rubber/linux/kernel/cgroup/rdma.c: 326
 * @device: pointer to rdmacg device which was previously registered with rdma /Users/rubber/linux/kernel/cgroup/rdma.c: 327
 *          controller using rdmacg_register_device(). /Users/rubber/linux/kernel/cgroup/rdma.c: 328
 * IB stack must invoke this after all the resources of the IB device /Users/rubber/linux/kernel/cgroup/rdma.c: 330
 * are destroyed and after ensuring that no more resources will be created /Users/rubber/linux/kernel/cgroup/rdma.c: 331
 * when this API is invoked. /Users/rubber/linux/kernel/cgroup/rdma.c: 332
	/* /Users/rubber/linux/kernel/cgroup/rdma.c: 338
	 * Synchronize with any active resource settings, /Users/rubber/linux/kernel/cgroup/rdma.c: 339
	 * usage query happening via configfs. /Users/rubber/linux/kernel/cgroup/rdma.c: 340
	/* /Users/rubber/linux/kernel/cgroup/rdma.c: 345
	 * Now that this device is off the cgroup list, its safe to free /Users/rubber/linux/kernel/cgroup/rdma.c: 346
	 * all the rpool resources. /Users/rubber/linux/kernel/cgroup/rdma.c: 347
		/* /Users/rubber/linux/kernel/cgroup/rdma.c: 475
		 * No user of the rpool and all entries are set to max, so /Users/rubber/linux/kernel/cgroup/rdma.c: 476
		 * safe to delete this rpool. /Users/rubber/linux/kernel/cgroup/rdma.c: 477
 * rdmacg_css_offline - cgroup css_offline callback /Users/rubber/linux/kernel/cgroup/rdma.c: 583
 * @css: css of interest /Users/rubber/linux/kernel/cgroup/rdma.c: 584
 * This function is called when @css is about to go away and responsible /Users/rubber/linux/kernel/cgroup/rdma.c: 586
 * for shooting down all rdmacg associated with @css. As part of that it /Users/rubber/linux/kernel/cgroup/rdma.c: 587
 * marks all the resource pool entries to max value, so that when resources are /Users/rubber/linux/kernel/cgroup/rdma.c: 588
 * uncharged, associated resource pool can be freed as well. /Users/rubber/linux/kernel/cgroup/rdma.c: 589
 *  kernel/cpuset.c /Users/rubber/linux/kernel/cgroup/cpuset.c: 2
 *  Processor and Memory placement constraints for sets of tasks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 4
 *  Copyright (C) 2003 BULL SA. /Users/rubber/linux/kernel/cgroup/cpuset.c: 6
 *  Copyright (C) 2004-2007 Silicon Graphics, Inc. /Users/rubber/linux/kernel/cgroup/cpuset.c: 7
 *  Copyright (C) 2006 Google, Inc /Users/rubber/linux/kernel/cgroup/cpuset.c: 8
 *  Portions derived from Patrick Mochel's sysfs code. /Users/rubber/linux/kernel/cgroup/cpuset.c: 10
 *  sysfs is Copyright (c) 2001-3 Patrick Mochel /Users/rubber/linux/kernel/cgroup/cpuset.c: 11
 *  2003-10-10 Written by Simon Derr. /Users/rubber/linux/kernel/cgroup/cpuset.c: 13
 *  2003-10-22 Updates by Stephen Hemminger. /Users/rubber/linux/kernel/cgroup/cpuset.c: 14
 *  2004 May-July Rework by Paul Jackson. /Users/rubber/linux/kernel/cgroup/cpuset.c: 15
 *  2006 Rework by Paul Menage to use generic cgroups /Users/rubber/linux/kernel/cgroup/cpuset.c: 16
 *  2008 Rework of the scheduler domains and CPU hotplug handling /Users/rubber/linux/kernel/cgroup/cpuset.c: 17
 *       by Max Krasnyansky /Users/rubber/linux/kernel/cgroup/cpuset.c: 18
 *  This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/cgroup/cpuset.c: 20
 *  License.  See the file COPYING in the main directory of the Linux /Users/rubber/linux/kernel/cgroup/cpuset.c: 21
 *  distribution for more details. /Users/rubber/linux/kernel/cgroup/cpuset.c: 22
 * There could be abnormal cpuset configurations for cpu or memory /Users/rubber/linux/kernel/cgroup/cpuset.c: 73
 * node binding, add this key to provide a quick low-cost judgement /Users/rubber/linux/kernel/cgroup/cpuset.c: 74
 * of the situation. /Users/rubber/linux/kernel/cgroup/cpuset.c: 75
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 93
	 * On default hierarchy: /Users/rubber/linux/kernel/cgroup/cpuset.c: 94
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 95
	 * The user-configured masks can only be changed by writing to /Users/rubber/linux/kernel/cgroup/cpuset.c: 96
	 * cpuset.cpus and cpuset.mems, and won't be limited by the /Users/rubber/linux/kernel/cgroup/cpuset.c: 97
	 * parent masks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 98
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 99
	 * The effective masks is the real masks that apply to the tasks /Users/rubber/linux/kernel/cgroup/cpuset.c: 100
	 * in the cpuset. They may be changed if the configured masks are /Users/rubber/linux/kernel/cgroup/cpuset.c: 101
	 * changed or hotplug happens. /Users/rubber/linux/kernel/cgroup/cpuset.c: 102
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 103
	 * effective_mask == configured_mask & parent's effective_mask, /Users/rubber/linux/kernel/cgroup/cpuset.c: 104
	 * and if it ends up empty, it will inherit the parent's mask. /Users/rubber/linux/kernel/cgroup/cpuset.c: 105
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 106
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 107
	 * On legacy hierarchy: /Users/rubber/linux/kernel/cgroup/cpuset.c: 108
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 109
	 * The user-configured masks are always the same with effective masks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 110
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 121
	 * CPUs allocated to child sub-partitions (default hierarchy only) /Users/rubber/linux/kernel/cgroup/cpuset.c: 122
	 * - CPUs granted by the parent = effective_cpus U subparts_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 123
	 * - effective_cpus and subparts_cpus are mutually exclusive. /Users/rubber/linux/kernel/cgroup/cpuset.c: 124
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 125
	 * effective_cpus contains only onlined CPUs, but subparts_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 126
	 * may have offlined ones. /Users/rubber/linux/kernel/cgroup/cpuset.c: 127
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 131
	 * This is old Memory Nodes tasks took on. /Users/rubber/linux/kernel/cgroup/cpuset.c: 132
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 133
	 * - top_cpuset.old_mems_allowed is initialized to mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 134
	 * - A new cpuset's old_mems_allowed is initialized when some /Users/rubber/linux/kernel/cgroup/cpuset.c: 135
	 *   task is moved into it. /Users/rubber/linux/kernel/cgroup/cpuset.c: 136
	 * - old_mems_allowed is used in cpuset_migrate_mm() when we change /Users/rubber/linux/kernel/cgroup/cpuset.c: 137
	 *   cpuset.mems_allowed and have tasks' nodemask updated, and /Users/rubber/linux/kernel/cgroup/cpuset.c: 138
	 *   then old_mems_allowed is updated to mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 139
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 145
	 * Tasks are being attached to this cpuset.  Used to prevent /Users/rubber/linux/kernel/cgroup/cpuset.c: 146
	 * zeroing cpus/mems_allowed between ->can_attach() and ->attach(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 147
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 163
	 * Default hierarchy only: /Users/rubber/linux/kernel/cgroup/cpuset.c: 164
	 * use_parent_ecpus - set if using parent's effective_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 165
	 * child_ecpus_count - # of children with use_parent_ecpus set /Users/rubber/linux/kernel/cgroup/cpuset.c: 166
 * Partition root states: /Users/rubber/linux/kernel/cgroup/cpuset.c: 176
 *   0 - not a partition root /Users/rubber/linux/kernel/cgroup/cpuset.c: 178
 *   1 - partition root /Users/rubber/linux/kernel/cgroup/cpuset.c: 180
 *  -1 - invalid partition root /Users/rubber/linux/kernel/cgroup/cpuset.c: 182
 *       None of the cpus in cpus_allowed can be put into the parent's /Users/rubber/linux/kernel/cgroup/cpuset.c: 183
 *       subparts_cpus. In this case, the cpuset is not a real partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 184
 *       root anymore.  However, the CPU_EXCLUSIVE bit will still be set /Users/rubber/linux/kernel/cgroup/cpuset.c: 185
 *       and the cpuset can be restored back to a partition root if the /Users/rubber/linux/kernel/cgroup/cpuset.c: 186
 *       parent cpuset can give more CPUs back to this child cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 187
 * Temporary cpumasks for working with partitions that are passed among /Users/rubber/linux/kernel/cgroup/cpuset.c: 194
 * functions to avoid memory allocation in inner functions. /Users/rubber/linux/kernel/cgroup/cpuset.c: 195
 * Send notification event of whenever partition_root_state changes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 277
 * cpuset_for_each_child - traverse online children of a cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 293
 * @child_cs: loop cursor pointing to the current child /Users/rubber/linux/kernel/cgroup/cpuset.c: 294
 * @pos_css: used for iteration /Users/rubber/linux/kernel/cgroup/cpuset.c: 295
 * @parent_cs: target cpuset to walk children of /Users/rubber/linux/kernel/cgroup/cpuset.c: 296
 * Walk @child_cs through the online children of @parent_cs.  Must be used /Users/rubber/linux/kernel/cgroup/cpuset.c: 298
 * with RCU read locked. /Users/rubber/linux/kernel/cgroup/cpuset.c: 299
 * cpuset_for_each_descendant_pre - pre-order walk of a cpuset's descendants /Users/rubber/linux/kernel/cgroup/cpuset.c: 306
 * @des_cs: loop cursor pointing to the current descendant /Users/rubber/linux/kernel/cgroup/cpuset.c: 307
 * @pos_css: used for iteration /Users/rubber/linux/kernel/cgroup/cpuset.c: 308
 * @root_cs: target cpuset to walk ancestor of /Users/rubber/linux/kernel/cgroup/cpuset.c: 309
 * Walk @des_cs through the online descendants of @root_cs.  Must be used /Users/rubber/linux/kernel/cgroup/cpuset.c: 311
 * with RCU read locked.  The caller may modify @pos_css by calling /Users/rubber/linux/kernel/cgroup/cpuset.c: 312
 * css_rightmost_descendant() to skip subtree.  @root_cs is included in the /Users/rubber/linux/kernel/cgroup/cpuset.c: 313
 * iteration and the first node to be visited. /Users/rubber/linux/kernel/cgroup/cpuset.c: 314
 * There are two global locks guarding cpuset structures - cpuset_rwsem and /Users/rubber/linux/kernel/cgroup/cpuset.c: 321
 * callback_lock. We also require taking task_lock() when dereferencing a /Users/rubber/linux/kernel/cgroup/cpuset.c: 322
 * task's cpuset pointer. See "The task_lock() exception", at the end of this /Users/rubber/linux/kernel/cgroup/cpuset.c: 323
 * comment.  The cpuset code uses only cpuset_rwsem write lock.  Other /Users/rubber/linux/kernel/cgroup/cpuset.c: 324
 * kernel subsystems can use cpuset_read_lock()/cpuset_read_unlock() to /Users/rubber/linux/kernel/cgroup/cpuset.c: 325
 * prevent change to cpuset structures. /Users/rubber/linux/kernel/cgroup/cpuset.c: 326
 * A task must hold both locks to modify cpusets.  If a task holds /Users/rubber/linux/kernel/cgroup/cpuset.c: 328
 * cpuset_rwsem, it blocks others wanting that rwsem, ensuring that it /Users/rubber/linux/kernel/cgroup/cpuset.c: 329
 * is the only task able to also acquire callback_lock and be able to /Users/rubber/linux/kernel/cgroup/cpuset.c: 330
 * modify cpusets.  It can perform various checks on the cpuset structure /Users/rubber/linux/kernel/cgroup/cpuset.c: 331
 * first, knowing nothing will change.  It can also allocate memory while /Users/rubber/linux/kernel/cgroup/cpuset.c: 332
 * just holding cpuset_rwsem.  While it is performing these checks, various /Users/rubber/linux/kernel/cgroup/cpuset.c: 333
 * callback routines can briefly acquire callback_lock to query cpusets. /Users/rubber/linux/kernel/cgroup/cpuset.c: 334
 * Once it is ready to make the changes, it takes callback_lock, blocking /Users/rubber/linux/kernel/cgroup/cpuset.c: 335
 * everyone else. /Users/rubber/linux/kernel/cgroup/cpuset.c: 336
 * Calls to the kernel memory allocator can not be made while holding /Users/rubber/linux/kernel/cgroup/cpuset.c: 338
 * callback_lock, as that would risk double tripping on callback_lock /Users/rubber/linux/kernel/cgroup/cpuset.c: 339
 * from one of the callbacks into the cpuset code from within /Users/rubber/linux/kernel/cgroup/cpuset.c: 340
 * __alloc_pages(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 341
 * If a task is only holding callback_lock, then it has read-only /Users/rubber/linux/kernel/cgroup/cpuset.c: 343
 * access to cpusets. /Users/rubber/linux/kernel/cgroup/cpuset.c: 344
 * Now, the task_struct fields mems_allowed and mempolicy may be changed /Users/rubber/linux/kernel/cgroup/cpuset.c: 346
 * by other task, we use alloc_lock in the task_struct fields to protect /Users/rubber/linux/kernel/cgroup/cpuset.c: 347
 * them. /Users/rubber/linux/kernel/cgroup/cpuset.c: 348
 * The cpuset_common_file_read() handlers only hold callback_lock across /Users/rubber/linux/kernel/cgroup/cpuset.c: 350
 * small pieces of code, such as when reading out possibly multi-word /Users/rubber/linux/kernel/cgroup/cpuset.c: 351
 * cpumasks and nodemasks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 352
 * Accessing a task's cpuset should be done in accordance with the /Users/rubber/linux/kernel/cgroup/cpuset.c: 354
 * guidelines for accessing subsystem state in kernel/cgroup.c /Users/rubber/linux/kernel/cgroup/cpuset.c: 355
 * CPU / memory hotplug is handled asynchronously. /Users/rubber/linux/kernel/cgroup/cpuset.c: 375
 * Cgroup v2 behavior is used on the "cpus" and "mems" control files when /Users/rubber/linux/kernel/cgroup/cpuset.c: 394
 * on default hierarchy or when the cpuset_v2_mode flag is set by mounting /Users/rubber/linux/kernel/cgroup/cpuset.c: 395
 * the v1 cpuset cgroup filesystem with the "cpuset_v2_mode" mount option. /Users/rubber/linux/kernel/cgroup/cpuset.c: 396
 * With v2 behavior, "cpus" and "mems" are always what the users have /Users/rubber/linux/kernel/cgroup/cpuset.c: 397
 * requested and won't be changed by hotplug events. Only the effective /Users/rubber/linux/kernel/cgroup/cpuset.c: 398
 * cpus or mems will be affected. /Users/rubber/linux/kernel/cgroup/cpuset.c: 399
 * Return in pmask the portion of a task's cpusets's cpus_allowed that /Users/rubber/linux/kernel/cgroup/cpuset.c: 408
 * are online and are capable of running the task.  If none are found, /Users/rubber/linux/kernel/cgroup/cpuset.c: 409
 * walk up the cpuset hierarchy until we find one that does have some /Users/rubber/linux/kernel/cgroup/cpuset.c: 410
 * appropriate cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 411
 * One way or another, we guarantee to return some non-empty subset /Users/rubber/linux/kernel/cgroup/cpuset.c: 413
 * of cpu_online_mask. /Users/rubber/linux/kernel/cgroup/cpuset.c: 414
 * Call with callback_lock or cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 416
			/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 433
			 * The top cpuset doesn't have any online cpu as a /Users/rubber/linux/kernel/cgroup/cpuset.c: 434
			 * consequence of a race between cpuset_hotplug_work /Users/rubber/linux/kernel/cgroup/cpuset.c: 435
			 * and cpu hotplug notifier.  But we know the top /Users/rubber/linux/kernel/cgroup/cpuset.c: 436
			 * cpuset's effective_cpus is on its way to be /Users/rubber/linux/kernel/cgroup/cpuset.c: 437
			 * identical to cpu_online_mask. /Users/rubber/linux/kernel/cgroup/cpuset.c: 438
 * Return in *pmask the portion of a cpusets's mems_allowed that /Users/rubber/linux/kernel/cgroup/cpuset.c: 450
 * are online, with memory.  If none are online with memory, walk /Users/rubber/linux/kernel/cgroup/cpuset.c: 451
 * up the cpuset hierarchy until we find one that does have some /Users/rubber/linux/kernel/cgroup/cpuset.c: 452
 * online mems.  The top cpuset always has some mems online. /Users/rubber/linux/kernel/cgroup/cpuset.c: 453
 * One way or another, we guarantee to return some non-empty subset /Users/rubber/linux/kernel/cgroup/cpuset.c: 455
 * of node_states[N_MEMORY]. /Users/rubber/linux/kernel/cgroup/cpuset.c: 456
 * Call with callback_lock or cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 458
 * update task's spread flag if cpuset's page/slab spread flag is set /Users/rubber/linux/kernel/cgroup/cpuset.c: 468
 * Call with callback_lock or cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 470
 * is_cpuset_subset(p, q) - Is cpuset p a subset of cpuset q? /Users/rubber/linux/kernel/cgroup/cpuset.c: 487
 * One cpuset is a subset of another if all its allowed CPUs and /Users/rubber/linux/kernel/cgroup/cpuset.c: 489
 * Memory Nodes are a subset of the other, and its exclusive flags /Users/rubber/linux/kernel/cgroup/cpuset.c: 490
 * are only set if the other's are set.  Call holding cpuset_rwsem. /Users/rubber/linux/kernel/cgroup/cpuset.c: 491
 * alloc_cpumasks - allocate three cpumasks for cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 503
 * @cs:  the cpuset that have cpumasks to be allocated. /Users/rubber/linux/kernel/cgroup/cpuset.c: 504
 * @tmp: the tmpmasks structure pointer /Users/rubber/linux/kernel/cgroup/cpuset.c: 505
 * Return: 0 if successful, -ENOMEM otherwise. /Users/rubber/linux/kernel/cgroup/cpuset.c: 506
 * Only one of the two input arguments should be non-NULL. /Users/rubber/linux/kernel/cgroup/cpuset.c: 508
 * free_cpumasks - free cpumasks in a tmpmasks structure /Users/rubber/linux/kernel/cgroup/cpuset.c: 543
 * @cs:  the cpuset that have cpumasks to be free. /Users/rubber/linux/kernel/cgroup/cpuset.c: 544
 * @tmp: the tmpmasks structure pointer /Users/rubber/linux/kernel/cgroup/cpuset.c: 545
 * alloc_trial_cpuset - allocate a trial cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 562
 * @cs: the cpuset that the trial cpuset duplicates /Users/rubber/linux/kernel/cgroup/cpuset.c: 563
 * free_cpuset - free the cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 584
 * @cs: the cpuset to be freed /Users/rubber/linux/kernel/cgroup/cpuset.c: 585
 * validate_change() - Used to validate that any proposed cpuset change /Users/rubber/linux/kernel/cgroup/cpuset.c: 594
 *		       follows the structural rules for cpusets. /Users/rubber/linux/kernel/cgroup/cpuset.c: 595
 * If we replaced the flag and mask values of the current cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 597
 * (cur) with those values in the trial cpuset (trial), would /Users/rubber/linux/kernel/cgroup/cpuset.c: 598
 * our various subset and exclusive rules still be valid?  Presumes /Users/rubber/linux/kernel/cgroup/cpuset.c: 599
 * cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 600
 * 'cur' is the address of an actual, in-use cpuset.  Operations /Users/rubber/linux/kernel/cgroup/cpuset.c: 602
 * such as list traversal that depend on the actual address of the /Users/rubber/linux/kernel/cgroup/cpuset.c: 603
 * cpuset in the list must use cur below, not trial. /Users/rubber/linux/kernel/cgroup/cpuset.c: 604
 * 'trial' is the address of bulk structure copy of cur, with /Users/rubber/linux/kernel/cgroup/cpuset.c: 606
 * perhaps one or more of the fields cpus_allowed, mems_allowed, /Users/rubber/linux/kernel/cgroup/cpuset.c: 607
 * or flags changed to new, trial values. /Users/rubber/linux/kernel/cgroup/cpuset.c: 608
 * Return 0 if valid, -errno if not. /Users/rubber/linux/kernel/cgroup/cpuset.c: 610
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 639
	 * If either I or some sibling (!= me) is exclusive, we can't /Users/rubber/linux/kernel/cgroup/cpuset.c: 640
	 * overlap /Users/rubber/linux/kernel/cgroup/cpuset.c: 641
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 655
	 * Cpusets with tasks - existing or newly being attached - can't /Users/rubber/linux/kernel/cgroup/cpuset.c: 656
	 * be changed to have empty cpus_allowed or mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 657
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 669
	 * We can't shrink if we won't have enough room for SCHED_DEADLINE /Users/rubber/linux/kernel/cgroup/cpuset.c: 670
	 * tasks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 671
 * Helper routine for generate_sched_domains(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 687
 * Do cpusets a, b have overlapping effective cpus_allowed masks? /Users/rubber/linux/kernel/cgroup/cpuset.c: 688
 * generate_sched_domains() /Users/rubber/linux/kernel/cgroup/cpuset.c: 731
 * This function builds a partial partition of the systems CPUs /Users/rubber/linux/kernel/cgroup/cpuset.c: 733
 * A 'partial partition' is a set of non-overlapping subsets whose /Users/rubber/linux/kernel/cgroup/cpuset.c: 734
 * union is a subset of that set. /Users/rubber/linux/kernel/cgroup/cpuset.c: 735
 * The output of this function needs to be passed to kernel/sched/core.c /Users/rubber/linux/kernel/cgroup/cpuset.c: 736
 * partition_sched_domains() routine, which will rebuild the scheduler's /Users/rubber/linux/kernel/cgroup/cpuset.c: 737
 * load balancing domains (sched domains) as specified by that partial /Users/rubber/linux/kernel/cgroup/cpuset.c: 738
 * partition. /Users/rubber/linux/kernel/cgroup/cpuset.c: 739
 * See "What is sched_load_balance" in Documentation/admin-guide/cgroup-v1/cpusets.rst /Users/rubber/linux/kernel/cgroup/cpuset.c: 741
 * for a background explanation of this. /Users/rubber/linux/kernel/cgroup/cpuset.c: 742
 * Does not return errors, on the theory that the callers of this /Users/rubber/linux/kernel/cgroup/cpuset.c: 744
 * routine would rather not worry about failures to rebuild sched /Users/rubber/linux/kernel/cgroup/cpuset.c: 745
 * domains when operating in the severe memory shortage situations /Users/rubber/linux/kernel/cgroup/cpuset.c: 746
 * that could cause allocation failures below. /Users/rubber/linux/kernel/cgroup/cpuset.c: 747
 * Must be called with cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 749
 * The three key local variables below are: /Users/rubber/linux/kernel/cgroup/cpuset.c: 751
 *    cp - cpuset pointer, used (together with pos_css) to perform a /Users/rubber/linux/kernel/cgroup/cpuset.c: 752
 *	   top-down scan of all cpusets. For our purposes, rebuilding /Users/rubber/linux/kernel/cgroup/cpuset.c: 753
 *	   the schedulers sched domains, we can ignore !is_sched_load_ /Users/rubber/linux/kernel/cgroup/cpuset.c: 754
 *	   balance cpusets. /Users/rubber/linux/kernel/cgroup/cpuset.c: 755
 *  csa  - (for CpuSet Array) Array of pointers to all the cpusets /Users/rubber/linux/kernel/cgroup/cpuset.c: 756
 *	   that need to be load balanced, for convenient iterative /Users/rubber/linux/kernel/cgroup/cpuset.c: 757
 *	   access by the subsequent code that finds the best partition, /Users/rubber/linux/kernel/cgroup/cpuset.c: 758
 *	   i.e the set of domains (subsets) of CPUs such that the /Users/rubber/linux/kernel/cgroup/cpuset.c: 759
 *	   cpus_allowed of every cpuset marked is_sched_load_balance /Users/rubber/linux/kernel/cgroup/cpuset.c: 760
 *	   is a subset of one of these domains, while there are as /Users/rubber/linux/kernel/cgroup/cpuset.c: 761
 *	   many such domains as possible, each as small as possible. /Users/rubber/linux/kernel/cgroup/cpuset.c: 762
 * doms  - Conversion of 'csa' to an array of cpumasks, for passing to /Users/rubber/linux/kernel/cgroup/cpuset.c: 763
 *	   the kernel/sched/core.c routine partition_sched_domains() in a /Users/rubber/linux/kernel/cgroup/cpuset.c: 764
 *	   convenient format, that can be easily compared to the prior /Users/rubber/linux/kernel/cgroup/cpuset.c: 765
 *	   value to determine what partition elements (sched domains) /Users/rubber/linux/kernel/cgroup/cpuset.c: 766
 *	   were changed (added or removed.) /Users/rubber/linux/kernel/cgroup/cpuset.c: 767
 * Finding the best partition (set of domains): /Users/rubber/linux/kernel/cgroup/cpuset.c: 769
 *	The triple nested loops below over i, j, k scan over the /Users/rubber/linux/kernel/cgroup/cpuset.c: 770
 *	load balanced cpusets (using the array of cpuset pointers in /Users/rubber/linux/kernel/cgroup/cpuset.c: 771
 *	csa[]) looking for pairs of cpusets that have overlapping /Users/rubber/linux/kernel/cgroup/cpuset.c: 772
 *	cpus_allowed, but which don't have the same 'pn' partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 773
 *	number and gives them in the same partition number.  It keeps /Users/rubber/linux/kernel/cgroup/cpuset.c: 774
 *	looping on the 'restart' label until it can no longer find /Users/rubber/linux/kernel/cgroup/cpuset.c: 775
 *	any such pairs. /Users/rubber/linux/kernel/cgroup/cpuset.c: 776
 *	The union of the cpus_allowed masks from the set of /Users/rubber/linux/kernel/cgroup/cpuset.c: 778
 *	all cpusets having the same 'pn' value then form the one /Users/rubber/linux/kernel/cgroup/cpuset.c: 779
 *	element of the partition (one sched domain) to be passed to /Users/rubber/linux/kernel/cgroup/cpuset.c: 780
 *	partition_sched_domains(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 781
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 830
		 * Continue traversing beyond @cp iff @cp has some CPUs and /Users/rubber/linux/kernel/cgroup/cpuset.c: 831
		 * isn't load balancing.  The former is obvious.  The /Users/rubber/linux/kernel/cgroup/cpuset.c: 832
		 * latter: All child cpusets contain a subset of the /Users/rubber/linux/kernel/cgroup/cpuset.c: 833
		 * parent's cpus, so just skip them, and then we call /Users/rubber/linux/kernel/cgroup/cpuset.c: 834
		 * update_domain_attr_tree() to calc relax_domain_level of /Users/rubber/linux/kernel/cgroup/cpuset.c: 835
		 * the corresponding sched domain. /Users/rubber/linux/kernel/cgroup/cpuset.c: 836
		 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 837
		 * If root is load-balancing, we can skip @cp if it /Users/rubber/linux/kernel/cgroup/cpuset.c: 838
		 * is a subset of the root's effective_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 839
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 888
	 * Now we know how many domains to create. /Users/rubber/linux/kernel/cgroup/cpuset.c: 889
	 * Convert <csn, csa> to <ndoms, doms> and populate cpu masks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 890
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 896
	 * The rest of the code, including the scheduler, can deal with /Users/rubber/linux/kernel/cgroup/cpuset.c: 897
	 * dattr==NULL case. No need to abort if alloc fails. /Users/rubber/linux/kernel/cgroup/cpuset.c: 898
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 948
	 * Fallback to the default domain if kmalloc() failed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 949
	 * See comments in partition_sched_domains(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 950
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 984
	 * Clear default root domain DL accounting, it will be computed again /Users/rubber/linux/kernel/cgroup/cpuset.c: 985
	 * if a task belongs to it. /Users/rubber/linux/kernel/cgroup/cpuset.c: 986
 * Rebuild scheduler domains. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1020
 * If the flag 'sched_load_balance' of any cpuset with non-empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 1022
 * 'cpus' changes, or if the 'cpus' allowed changes in any cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1023
 * which has that flag enabled, or if any cpuset with a non-empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 1024
 * 'cpus' is removed, then call this routine to rebuild the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1025
 * scheduler's dynamic sched domains. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1026
 * Call with cpuset_rwsem held.  Takes cpus_read_lock(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 1028
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1041
	 * If we have raced with CPU hotplug, return early to avoid /Users/rubber/linux/kernel/cgroup/cpuset.c: 1042
	 * passing doms with offlined cpu to partition_sched_domains(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 1043
	 * Anyways, cpuset_hotplug_workfn() will rebuild sched domains. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1044
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 1045
	 * With no CPUs in any subpartitions, top_cpuset's effective CPUs /Users/rubber/linux/kernel/cgroup/cpuset.c: 1046
	 * should be the same as the active CPUs, so checking only top_cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1047
	 * is enough to detect racing CPU offlines. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1048
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1054
	 * With subpartition CPUs, however, the effective CPUs of a partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 1055
	 * root should be only a subset of the active CPUs.  Since a CPU in any /Users/rubber/linux/kernel/cgroup/cpuset.c: 1056
	 * partition root could be offlined, all must be checked. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1057
 * update_tasks_cpumask - Update the cpumasks of tasks in the cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1097
 * @cs: the cpuset in which each task's cpus_allowed mask needs to be changed /Users/rubber/linux/kernel/cgroup/cpuset.c: 1098
 * Iterate through each task of @cs updating its cpus_allowed to the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1100
 * effective cpuset's.  As this function is called with cpuset_rwsem held, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1101
 * cpuset membership stays stable. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1102
 * compute_effective_cpumask - Compute the effective cpumask of the cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1116
 * @new_cpus: the temp variable for the new effective_cpus mask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1117
 * @cs: the cpuset the need to recompute the new effective_cpus mask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1118
 * @parent: the parent cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1119
 * If the parent has subpartition CPUs, include them in the list of /Users/rubber/linux/kernel/cgroup/cpuset.c: 1121
 * allowable CPUs in computing the new effective_cpus mask. Since offlined /Users/rubber/linux/kernel/cgroup/cpuset.c: 1122
 * CPUs are not removed from subparts_cpus, we have to use cpu_active_mask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1123
 * to mask those out. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1124
 * Commands for update_parent_subparts_cpumask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1140
 * update_parent_subparts_cpumask - update subparts_cpus mask of parent cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1149
 * @cpuset:  The cpuset that requests change in partition root state /Users/rubber/linux/kernel/cgroup/cpuset.c: 1150
 * @cmd:     Partition root state change command /Users/rubber/linux/kernel/cgroup/cpuset.c: 1151
 * @newmask: Optional new cpumask for partcmd_update /Users/rubber/linux/kernel/cgroup/cpuset.c: 1152
 * @tmp:     Temporary addmask and delmask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1153
 * Return:   0, 1 or an error code /Users/rubber/linux/kernel/cgroup/cpuset.c: 1154
 * For partcmd_enable, the cpuset is being transformed from a non-partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 1156
 * root to a partition root. The cpus_allowed mask of the given cpuset will /Users/rubber/linux/kernel/cgroup/cpuset.c: 1157
 * be put into parent's subparts_cpus and taken away from parent's /Users/rubber/linux/kernel/cgroup/cpuset.c: 1158
 * effective_cpus. The function will return 0 if all the CPUs listed in /Users/rubber/linux/kernel/cgroup/cpuset.c: 1159
 * cpus_allowed can be granted or an error code will be returned. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1160
 * For partcmd_disable, the cpuset is being transofrmed from a partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 1162
 * root back to a non-partition root. Any CPUs in cpus_allowed that are in /Users/rubber/linux/kernel/cgroup/cpuset.c: 1163
 * parent's subparts_cpus will be taken away from that cpumask and put back /Users/rubber/linux/kernel/cgroup/cpuset.c: 1164
 * into parent's effective_cpus. 0 should always be returned. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1165
 * For partcmd_update, if the optional newmask is specified, the cpu /Users/rubber/linux/kernel/cgroup/cpuset.c: 1167
 * list is to be changed from cpus_allowed to newmask. Otherwise, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1168
 * cpus_allowed is assumed to remain the same. The cpuset should either /Users/rubber/linux/kernel/cgroup/cpuset.c: 1169
 * be a partition root or an invalid partition root. The partition root /Users/rubber/linux/kernel/cgroup/cpuset.c: 1170
 * state may change if newmask is NULL and none of the requested CPUs can /Users/rubber/linux/kernel/cgroup/cpuset.c: 1171
 * be granted by the parent. The function will return 1 if changes to /Users/rubber/linux/kernel/cgroup/cpuset.c: 1172
 * parent's subparts_cpus and effective_cpus happen or 0 otherwise. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1173
 * Error code should only be returned when newmask is non-NULL. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1174
 * The partcmd_enable and partcmd_disable commands are used by /Users/rubber/linux/kernel/cgroup/cpuset.c: 1176
 * update_prstate(). The partcmd_update command is used by /Users/rubber/linux/kernel/cgroup/cpuset.c: 1177
 * update_cpumasks_hier() with newmask NULL and update_cpumask() with /Users/rubber/linux/kernel/cgroup/cpuset.c: 1178
 * newmask set. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1179
 * The checking is more strict when enabling partition root than the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1181
 * other two commands. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1182
 * Because of the implicit cpu exclusive nature of a partition root, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1184
 * cpumask changes that violates the cpu exclusivity rule will not be /Users/rubber/linux/kernel/cgroup/cpuset.c: 1185
 * permitted when checked by validate_change(). The validate_change() /Users/rubber/linux/kernel/cgroup/cpuset.c: 1186
 * function will also prevent any changes to the cpu list if it is not /Users/rubber/linux/kernel/cgroup/cpuset.c: 1187
 * a superset of children's cpu lists. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1188
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1202
	 * The parent must be a partition root. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1203
	 * The new cpumask, if present, or the current cpus_allowed must /Users/rubber/linux/kernel/cgroup/cpuset.c: 1204
	 * not be empty. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1205
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1212
	 * Enabling/disabling partition root is not allowed if there are /Users/rubber/linux/kernel/cgroup/cpuset.c: 1213
	 * online children. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1214
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1219
	 * Enabling partition root is not allowed if not all the CPUs /Users/rubber/linux/kernel/cgroup/cpuset.c: 1220
	 * can be granted from parent's effective_cpus or at least one /Users/rubber/linux/kernel/cgroup/cpuset.c: 1221
	 * CPU will be left after that. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1222
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1229
	 * A cpumask update cannot make parent's effective_cpus become empty. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1230
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1241
		 * partcmd_update with newmask: /Users/rubber/linux/kernel/cgroup/cpuset.c: 1242
		 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 1243
		 * delmask = cpus_allowed & ~newmask & parent->subparts_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1244
		 * addmask = newmask & parent->effective_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1245
		 *		     & ~parent->subparts_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1246
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1255
		 * Return error if the new effective_cpus could become empty. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1256
			/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1262
			 * As some of the CPUs in subparts_cpus might have /Users/rubber/linux/kernel/cgroup/cpuset.c: 1263
			 * been offlined, we need to compute the real delmask /Users/rubber/linux/kernel/cgroup/cpuset.c: 1264
			 * to confirm that. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1265
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1273
		 * partcmd_update w/o newmask: /Users/rubber/linux/kernel/cgroup/cpuset.c: 1274
		 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 1275
		 * addmask = cpus_allowed & parent->effective_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1276
		 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 1277
		 * Note that parent's subparts_cpus may have been /Users/rubber/linux/kernel/cgroup/cpuset.c: 1278
		 * pre-shrunk in case there is a change in the cpu list. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1279
		 * So no deletion is needed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1280
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1291
		 * Check for possible transition between PRS_ENABLED /Users/rubber/linux/kernel/cgroup/cpuset.c: 1292
		 * and PRS_ERROR. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1293
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1305
		 * Set part_error if previously in invalid state. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1306
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1315
		 * Remove all its cpus from parent's subparts_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1316
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1326
	 * Change the parent's subparts_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1327
	 * Newly added CPUs will be removed from effective_cpus and /Users/rubber/linux/kernel/cgroup/cpuset.c: 1328
	 * newly deleted ones will be added back to effective_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1329
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1341
		 * Some of the CPUs in subparts_cpus might have been offlined. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1342
 * update_cpumasks_hier - Update effective cpumasks and tasks in the subtree /Users/rubber/linux/kernel/cgroup/cpuset.c: 1361
 * @cs:  the cpuset to consider /Users/rubber/linux/kernel/cgroup/cpuset.c: 1362
 * @tmp: temp variables for calculating effective_cpus & partition setup /Users/rubber/linux/kernel/cgroup/cpuset.c: 1363
 * When configured cpumask is changed, the effective cpumasks of this cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1365
 * and all its descendants need to be updated. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1366
 * On legacy hierarchy, effective_cpus will be the same with cpu_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1368
 * Called with cpuset_rwsem held /Users/rubber/linux/kernel/cgroup/cpuset.c: 1370
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1385
		 * If it becomes empty, inherit the effective mask of the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1386
		 * parent, which is guaranteed to have some CPUs. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1387
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1401
		 * Skip the whole subtree if the cpumask remains the same /Users/rubber/linux/kernel/cgroup/cpuset.c: 1402
		 * and has no partition root state. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1403
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1411
		 * update_parent_subparts_cpumask() should have been called /Users/rubber/linux/kernel/cgroup/cpuset.c: 1412
		 * for cs already in update_cpumask(). We should also call /Users/rubber/linux/kernel/cgroup/cpuset.c: 1413
		 * update_tasks_cpumask() again for tasks in the parent /Users/rubber/linux/kernel/cgroup/cpuset.c: 1414
		 * cpuset if the parent's subparts_cpus changes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1415
				/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1421
				 * If parent is not a partition root or an /Users/rubber/linux/kernel/cgroup/cpuset.c: 1422
				 * invalid partition root, clear its state /Users/rubber/linux/kernel/cgroup/cpuset.c: 1423
				 * and its CS_CPU_EXCLUSIVE flag. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1424
				/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1430
				 * clear_bit() is an atomic operation and /Users/rubber/linux/kernel/cgroup/cpuset.c: 1431
				 * readers aren't interested in the state /Users/rubber/linux/kernel/cgroup/cpuset.c: 1432
				 * of CS_CPU_EXCLUSIVE anyway. So we can /Users/rubber/linux/kernel/cgroup/cpuset.c: 1433
				 * just update the flag without holding /Users/rubber/linux/kernel/cgroup/cpuset.c: 1434
				 * the callback_lock. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1435
				/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1446
				 * When parent is invalid, it has to be too. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1447
			/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1465
			 * Make sure that effective_cpus & subparts_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1466
			 * are mutually exclusive. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1467
			 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 1468
			 * In the unlikely event that effective_cpus /Users/rubber/linux/kernel/cgroup/cpuset.c: 1469
			 * becomes empty. we clear cp->nr_subparts_cpus and /Users/rubber/linux/kernel/cgroup/cpuset.c: 1470
			 * let its child partition roots to compete for /Users/rubber/linux/kernel/cgroup/cpuset.c: 1471
			 * CPUs again. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1472
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1500
		 * On legacy hierarchy, if the effective cpumask of any non- /Users/rubber/linux/kernel/cgroup/cpuset.c: 1501
		 * empty cpuset is changed, we need to rebuild sched domains. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1502
		 * On default hierarchy, the cpuset needs to be a partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 1503
		 * root as well. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1504
 * update_sibling_cpumasks - Update siblings cpumasks /Users/rubber/linux/kernel/cgroup/cpuset.c: 1522
 * @parent:  Parent cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1523
 * @cs:      Current cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1524
 * @tmp:     Temp variables /Users/rubber/linux/kernel/cgroup/cpuset.c: 1525
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1533
	 * Check all its siblings and call update_cpumasks_hier() /Users/rubber/linux/kernel/cgroup/cpuset.c: 1534
	 * if their use_parent_ecpus flag is set in order for them /Users/rubber/linux/kernel/cgroup/cpuset.c: 1535
	 * to use the right effective_cpus value. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1536
 * update_cpumask - update the cpus_allowed mask of a cpuset and all tasks in it /Users/rubber/linux/kernel/cgroup/cpuset.c: 1551
 * @cs: the cpuset to consider /Users/rubber/linux/kernel/cgroup/cpuset.c: 1552
 * @trialcs: trial cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1553
 * @buf: buffer of cpu numbers written to this cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1554
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1566
	 * An empty cpus_allowed is ok only if the cpuset has no tasks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1567
	 * Since cpulist_parse() fails on an empty mask, we special case /Users/rubber/linux/kernel/cgroup/cpuset.c: 1568
	 * that parsing.  The validate_change() call ensures that cpusets /Users/rubber/linux/kernel/cgroup/cpuset.c: 1569
	 * with tasks have cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1570
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1593
	 * Use the cpumasks in trialcs for tmpmasks when they are pointers /Users/rubber/linux/kernel/cgroup/cpuset.c: 1594
	 * to allocated cpumasks. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1595
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1614
	 * Make sure that subparts_cpus is a subset of cpus_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1615
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1629
		 * For partition root, update the cpumasks of sibling /Users/rubber/linux/kernel/cgroup/cpuset.c: 1630
		 * cpusets if they use parent's effective_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1631
 * Migrate memory region from one set of nodes to another.  This is /Users/rubber/linux/kernel/cgroup/cpuset.c: 1640
 * performed asynchronously as it can be called from process migration path /Users/rubber/linux/kernel/cgroup/cpuset.c: 1641
 * holding locks involved in process management.  All mm migrations are /Users/rubber/linux/kernel/cgroup/cpuset.c: 1642
 * performed in the queued order and can be waited for by flushing /Users/rubber/linux/kernel/cgroup/cpuset.c: 1643
 * cpuset_migrate_mm_wq. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1644
 * cpuset_change_task_nodemask - change task's mems_allowed and mempolicy /Users/rubber/linux/kernel/cgroup/cpuset.c: 1693
 * @tsk: the task to change /Users/rubber/linux/kernel/cgroup/cpuset.c: 1694
 * @newmems: new nodes that the task will be set /Users/rubber/linux/kernel/cgroup/cpuset.c: 1695
 * We use the mems_allowed_seq seqlock to safely update both tsk->mems_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 1697
 * and rebind an eventual tasks' mempolicy. If the task is allocating in /Users/rubber/linux/kernel/cgroup/cpuset.c: 1698
 * parallel, it might temporarily see an empty intersection, which results in /Users/rubber/linux/kernel/cgroup/cpuset.c: 1699
 * a seqlock check and retry before OOM or allocation failure. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1700
 * update_tasks_nodemask - Update the nodemasks of tasks in the cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1723
 * @cs: the cpuset in which each task's mems_allowed mask needs to be changed /Users/rubber/linux/kernel/cgroup/cpuset.c: 1724
 * Iterate through each task of @cs updating its mems_allowed to the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1726
 * effective cpuset's.  As this function is called with cpuset_rwsem held, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1727
 * cpuset membership stays stable. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1728
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1740
	 * The mpol_rebind_mm() call takes mmap_lock, which we couldn't /Users/rubber/linux/kernel/cgroup/cpuset.c: 1741
	 * take while holding tasklist_lock.  Forks can happen - the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1742
	 * mpol_dup() cpuset_being_rebound check will catch such forks, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1743
	 * and rebind their vma mempolicies too.  Because we still hold /Users/rubber/linux/kernel/cgroup/cpuset.c: 1744
	 * the global cpuset_rwsem, we know that no other rebind effort /Users/rubber/linux/kernel/cgroup/cpuset.c: 1745
	 * will be contending for the global variable cpuset_being_rebound. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1746
	 * It's ok if we rebind the same mm twice; mpol_rebind_mm() /Users/rubber/linux/kernel/cgroup/cpuset.c: 1747
	 * is idempotent.  Also migrate pages in each mm to new nodes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1748
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1771
	 * All the tasks' nodemasks have been updated, update /Users/rubber/linux/kernel/cgroup/cpuset.c: 1772
	 * cs->old_mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1773
 * update_nodemasks_hier - Update effective nodemasks and tasks in the subtree /Users/rubber/linux/kernel/cgroup/cpuset.c: 1782
 * @cs: the cpuset to consider /Users/rubber/linux/kernel/cgroup/cpuset.c: 1783
 * @new_mems: a temp variable for calculating new effective_mems /Users/rubber/linux/kernel/cgroup/cpuset.c: 1784
 * When configured nodemask is changed, the effective nodemasks of this cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 1786
 * and all its descendants need to be updated. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1787
 * On legacy hierarchy, effective_mems will be the same with mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1789
 * Called with cpuset_rwsem held /Users/rubber/linux/kernel/cgroup/cpuset.c: 1791
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1804
		 * If it becomes empty, inherit the effective mask of the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1805
		 * parent, which is guaranteed to have some MEMs. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1806
 * Handle user request to change the 'mems' memory placement /Users/rubber/linux/kernel/cgroup/cpuset.c: 1837
 * of a cpuset.  Needs to validate the request, update the /Users/rubber/linux/kernel/cgroup/cpuset.c: 1838
 * cpusets mems_allowed, and for each task in the cpuset, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1839
 * update mems_allowed and rebind task's mempolicy and any vma /Users/rubber/linux/kernel/cgroup/cpuset.c: 1840
 * mempolicies and if the cpuset is marked 'memory_migrate', /Users/rubber/linux/kernel/cgroup/cpuset.c: 1841
 * migrate the tasks pages to the new memory. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1842
 * Call with cpuset_rwsem held. May take callback_lock during call. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1844
 * Will take tasklist_lock, scan tasklist for tasks in cpuset cs, /Users/rubber/linux/kernel/cgroup/cpuset.c: 1845
 * lock each such tasks mm->mmap_lock, scan its vma's and rebind /Users/rubber/linux/kernel/cgroup/cpuset.c: 1846
 * their mempolicies to the cpusets new mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1847
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1854
	 * top_cpuset.mems_allowed tracks node_stats[N_MEMORY]; /Users/rubber/linux/kernel/cgroup/cpuset.c: 1855
	 * it's read-only /Users/rubber/linux/kernel/cgroup/cpuset.c: 1856
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 1863
	 * An empty mems_allowed is ok iff there are no tasks in the cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1864
	 * Since nodelist_parse() fails on an empty mask, we special case /Users/rubber/linux/kernel/cgroup/cpuset.c: 1865
	 * that parsing.  The validate_change() call ensures that cpusets /Users/rubber/linux/kernel/cgroup/cpuset.c: 1866
	 * with tasks have memory. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1867
 * update_tasks_flags - update the spread flags of tasks in the cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1932
 * @cs: the cpuset in which each task's spread flags needs to be changed /Users/rubber/linux/kernel/cgroup/cpuset.c: 1933
 * Iterate through each task of @cs updating its spread flags.  As this /Users/rubber/linux/kernel/cgroup/cpuset.c: 1935
 * function is called with cpuset_rwsem held, cpuset membership stays /Users/rubber/linux/kernel/cgroup/cpuset.c: 1936
 * stable. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1937
 * update_flag - read a 0 or a 1 in a file and update associated flag /Users/rubber/linux/kernel/cgroup/cpuset.c: 1951
 * bit:		the bit to update (see cpuset_flagbits_t) /Users/rubber/linux/kernel/cgroup/cpuset.c: 1952
 * cs:		the cpuset to update /Users/rubber/linux/kernel/cgroup/cpuset.c: 1953
 * turning_on: 	whether the flag is being set or cleared /Users/rubber/linux/kernel/cgroup/cpuset.c: 1954
 * Call with cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 1956
 * update_prstate - update partititon_root_state /Users/rubber/linux/kernel/cgroup/cpuset.c: 2001
 * cs: the cpuset to update /Users/rubber/linux/kernel/cgroup/cpuset.c: 2002
 * new_prs: new partition root state /Users/rubber/linux/kernel/cgroup/cpuset.c: 2003
 * Call with cpuset_rwsem held. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2005
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2016
	 * Cannot force a partial or invalid partition root to a full /Users/rubber/linux/kernel/cgroup/cpuset.c: 2017
	 * partition root. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2018
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2028
		 * Turning on partition root requires setting the /Users/rubber/linux/kernel/cgroup/cpuset.c: 2029
		 * CS_CPU_EXCLUSIVE bit implicitly as well and cpus_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 2030
		 * cannot be NULL. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2031
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2047
		 * Turning off partition root will clear the /Users/rubber/linux/kernel/cgroup/cpuset.c: 2048
		 * CS_CPU_EXCLUSIVE bit. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2049
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2066
	 * Update cpumask of parent's tasks except when it is the top /Users/rubber/linux/kernel/cgroup/cpuset.c: 2067
	 * cpuset as some system daemons cannot be mapped to other CPUs. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2068
 * Frequency meter - How fast is some event occurring? /Users/rubber/linux/kernel/cgroup/cpuset.c: 2090
 * These routines manage a digitally filtered, constant time based, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2092
 * event frequency meter.  There are four routines: /Users/rubber/linux/kernel/cgroup/cpuset.c: 2093
 *   fmeter_init() - initialize a frequency meter. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2094
 *   fmeter_markevent() - called each time the event happens. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2095
 *   fmeter_getrate() - returns the recent rate of such events. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2096
 *   fmeter_update() - internal routine used to update fmeter. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2097
 * A common data structure is passed to each of these routines, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2099
 * which is used to keep track of the state required to manage the /Users/rubber/linux/kernel/cgroup/cpuset.c: 2100
 * frequency meter and its digital filter. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2101
 * The filter works on the number of events marked per unit time. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2103
 * The filter is single-pole low-pass recursive (IIR).  The time unit /Users/rubber/linux/kernel/cgroup/cpuset.c: 2104
 * is 1 second.  Arithmetic is done using 32-bit integers scaled to /Users/rubber/linux/kernel/cgroup/cpuset.c: 2105
 * simulate 3 decimal digits of precision (multiplied by 1000). /Users/rubber/linux/kernel/cgroup/cpuset.c: 2106
 * With an FM_COEF of 933, and a time base of 1 second, the filter /Users/rubber/linux/kernel/cgroup/cpuset.c: 2108
 * has a half-life of 10 seconds, meaning that if the events quit /Users/rubber/linux/kernel/cgroup/cpuset.c: 2109
 * happening, then the rate returned from the fmeter_getrate() /Users/rubber/linux/kernel/cgroup/cpuset.c: 2110
 * will be cut in half each 10 seconds, until it converges to zero. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2111
 * It is not worth doing a real infinitely recursive filter.  If more /Users/rubber/linux/kernel/cgroup/cpuset.c: 2113
 * than FM_MAXTICKS ticks have elapsed since the last filter event, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2114
 * just compute FM_MAXTICKS ticks worth, by which point the level /Users/rubber/linux/kernel/cgroup/cpuset.c: 2115
 * will be stable. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2116
 * Limit the count of unprocessed events to FM_MAXCNT, so as to avoid /Users/rubber/linux/kernel/cgroup/cpuset.c: 2118
 * arithmetic overflow in the fmeter_update() routine. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2119
 * Given the simple 32 bit integer arithmetic used, this meter works /Users/rubber/linux/kernel/cgroup/cpuset.c: 2121
 * best for reporting rates between one per millisecond (msec) and /Users/rubber/linux/kernel/cgroup/cpuset.c: 2122
 * one per 32 (approx) seconds.  At constant rates faster than one /Users/rubber/linux/kernel/cgroup/cpuset.c: 2123
 * per msec it maxes out at values just under 1,000,000.  At constant /Users/rubber/linux/kernel/cgroup/cpuset.c: 2124
 * rates between one per msec, and one per second it will stabilize /Users/rubber/linux/kernel/cgroup/cpuset.c: 2125
 * to a value N*1000, where N is the rate of events per second. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2126
 * At constant rates between one per second and one per 32 seconds, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2127
 * it will be choppy, moving up on the seconds that have an event, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2128
 * and then decaying until the next event.  At rates slower than /Users/rubber/linux/kernel/cgroup/cpuset.c: 2129
 * about one in 32 seconds, it decays all the way back to zero between /Users/rubber/linux/kernel/cgroup/cpuset.c: 2130
 * each event. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2131
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2221
	 * Mark attach is in progress.  This makes validate_change() fail /Users/rubber/linux/kernel/cgroup/cpuset.c: 2222
	 * changes which zero cpus/mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2223
 * Protected by cpuset_rwsem.  cpus_attach is used only by cpuset_attach() /Users/rubber/linux/kernel/cgroup/cpuset.c: 2244
 * but we can't allocate it dynamically there.  Define it global and /Users/rubber/linux/kernel/cgroup/cpuset.c: 2245
 * allocate from cpuset_init(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 2246
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2272
		 * can_attach beforehand should guarantee that this doesn't /Users/rubber/linux/kernel/cgroup/cpuset.c: 2273
		 * fail.  TODO: have a better way to handle failure here /Users/rubber/linux/kernel/cgroup/cpuset.c: 2274
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2282
	 * Change mm for all threadgroup leaders. This is expensive and may /Users/rubber/linux/kernel/cgroup/cpuset.c: 2283
	 * sleep and should be moved outside migration path proper. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2284
			/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2293
			 * old_mems_allowed is the same with mems_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 2294
			 * here, except if this task is being moved /Users/rubber/linux/kernel/cgroup/cpuset.c: 2295
			 * automatically due to hotplug.  In that case /Users/rubber/linux/kernel/cgroup/cpuset.c: 2296
			 * @mems_allowed has been updated and is empty, so /Users/rubber/linux/kernel/cgroup/cpuset.c: 2297
			 * @old_mems_allowed is the right nodesets that we /Users/rubber/linux/kernel/cgroup/cpuset.c: 2298
			 * migrate mm from. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2299
 * Common handling for a write to a "cpus" or "mems" file. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2415
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2426
	 * CPU or memory hotunplug may leave @cs w/o any execution /Users/rubber/linux/kernel/cgroup/cpuset.c: 2427
	 * resources, in which case the hotplug code asynchronously updates /Users/rubber/linux/kernel/cgroup/cpuset.c: 2428
	 * configuration and transfers all tasks to the nearest ancestor /Users/rubber/linux/kernel/cgroup/cpuset.c: 2429
	 * which can execute. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2430
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 2431
	 * As writes to "cpus" or "mems" may restore @cs's execution /Users/rubber/linux/kernel/cgroup/cpuset.c: 2432
	 * resources, wait for the previously scheduled operations before /Users/rubber/linux/kernel/cgroup/cpuset.c: 2433
	 * proceeding, so that we don't end up keep removing tasks added /Users/rubber/linux/kernel/cgroup/cpuset.c: 2434
	 * after execution capability is restored. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2435
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 2436
	 * cpuset_hotplug_work calls back into cgroup core via /Users/rubber/linux/kernel/cgroup/cpuset.c: 2437
	 * cgroup_transfer_tasks() and waiting for it from a cgroupfs /Users/rubber/linux/kernel/cgroup/cpuset.c: 2438
	 * operation like this one can lead to a deadlock through kernfs /Users/rubber/linux/kernel/cgroup/cpuset.c: 2439
	 * active_ref protection.  Let's break the protection.  Losing the /Users/rubber/linux/kernel/cgroup/cpuset.c: 2440
	 * protection is okay as we check whether @cs is online after /Users/rubber/linux/kernel/cgroup/cpuset.c: 2441
	 * grabbing cpuset_rwsem anyway.  This only happens on the legacy /Users/rubber/linux/kernel/cgroup/cpuset.c: 2442
	 * hierarchies. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2443
 * These ascii lists should be read in a single call, by using a user /Users/rubber/linux/kernel/cgroup/cpuset.c: 2483
 * buffer large enough to hold the entire map.  If read in smaller /Users/rubber/linux/kernel/cgroup/cpuset.c: 2484
 * chunks, there is no guarantee of atomicity.  Since the display format /Users/rubber/linux/kernel/cgroup/cpuset.c: 2485
 * used, list of ranges of sequential numbers, is variable length, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2486
 * and since these maps can change value dynamically, one could read /Users/rubber/linux/kernel/cgroup/cpuset.c: 2487
 * gibberish by doing partial reads while a list was changing. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2488
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2595
	 * Convert "root" to ENABLED, and convert "member" to DISABLED. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2596
 * for the common functions, 'private' gives the type of file /Users/rubber/linux/kernel/cgroup/cpuset.c: 2620
 * This is currently a minimal set for the default hierarchy. It can be /Users/rubber/linux/kernel/cgroup/cpuset.c: 2726
 * expanded later on by migrating more features and control files from v1. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2727
 *	cpuset_css_alloc - allocate a cpuset css /Users/rubber/linux/kernel/cgroup/cpuset.c: 2781
 *	cgrp:	control group that the new cpuset will be part of /Users/rubber/linux/kernel/cgroup/cpuset.c: 2782
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 2848
	 * Clone @parent's configuration if CGRP_CPUSET_CLONE_CHILDREN is /Users/rubber/linux/kernel/cgroup/cpuset.c: 2849
	 * set.  This flag handling is implemented in cgroup core for /Users/rubber/linux/kernel/cgroup/cpuset.c: 2850
	 * histrical reasons - the flag may be specified during mount. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2851
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 2852
	 * Currently, if any sibling cpusets have exclusive cpus or mem, we /Users/rubber/linux/kernel/cgroup/cpuset.c: 2853
	 * refuse to clone the configuration - thereby refusing the task to /Users/rubber/linux/kernel/cgroup/cpuset.c: 2854
	 * be entered, and as a result refusing the sys_unshare() or /Users/rubber/linux/kernel/cgroup/cpuset.c: 2855
	 * clone() which initiated it.  If this becomes a problem for some /Users/rubber/linux/kernel/cgroup/cpuset.c: 2856
	 * users who wish to allow that scenario, then this could be /Users/rubber/linux/kernel/cgroup/cpuset.c: 2857
	 * changed to grant parent->cpus_allowed-sibling_cpus_exclusive /Users/rubber/linux/kernel/cgroup/cpuset.c: 2858
	 * (and likewise for mems) to the new cgroup. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2859
 * If the cpuset being removed has its flag 'sched_load_balance' /Users/rubber/linux/kernel/cgroup/cpuset.c: 2883
 * enabled, then simulate turning sched_load_balance off, which /Users/rubber/linux/kernel/cgroup/cpuset.c: 2884
 * will call rebuild_sched_domains_locked(). That is not needed /Users/rubber/linux/kernel/cgroup/cpuset.c: 2885
 * in the default hierarchy where only changes in partition /Users/rubber/linux/kernel/cgroup/cpuset.c: 2886
 * will cause repartitioning. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2887
 * If the cpuset has the 'sched.partition' flag enabled, simulate /Users/rubber/linux/kernel/cgroup/cpuset.c: 2889
 * turning 'sched.partition" off. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2890
 * Make sure the new task conform to the current state of its parent, /Users/rubber/linux/kernel/cgroup/cpuset.c: 2947
 * which could have been changed by cpuset just after it inherits the /Users/rubber/linux/kernel/cgroup/cpuset.c: 2948
 * state from the parent and before it sits on the cgroup's task list. /Users/rubber/linux/kernel/cgroup/cpuset.c: 2949
 * cpuset_init - initialize cpusets at system boot /Users/rubber/linux/kernel/cgroup/cpuset.c: 2978
 * Description: Initialize top_cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 2980
 * If CPU and/or memory hotplug handlers, below, unplug any CPUs /Users/rubber/linux/kernel/cgroup/cpuset.c: 3006
 * or memory nodes, we need to walk over the cpuset hierarchy, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3007
 * removing that CPU or node from all cpusets.  If this removes the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3008
 * last CPU or node from a cpuset, then move the tasks in the empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 3009
 * cpuset to its next-highest non-empty parent. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3010
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3016
	 * Find its next-highest non-empty parent, (top cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3017
	 * has online cpus, so can't be empty). /Users/rubber/linux/kernel/cgroup/cpuset.c: 3018
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3046
	 * Don't call update_tasks_cpumask() if the cpuset becomes empty, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3047
	 * as the tasks will be migratecd to an ancestor. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3048
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3060
	 * Move tasks to the nearest ancestor with execution resources, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3061
	 * This is full cgroup operation which will also call back into /Users/rubber/linux/kernel/cgroup/cpuset.c: 3062
	 * cpuset. Should be done outside any lock. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3063
 * cpuset_hotplug_update_tasks - update tasks in a cpuset for hotunplug /Users/rubber/linux/kernel/cgroup/cpuset.c: 3100
 * @cs: cpuset in interest /Users/rubber/linux/kernel/cgroup/cpuset.c: 3101
 * @tmp: the tmpmasks structure pointer /Users/rubber/linux/kernel/cgroup/cpuset.c: 3102
 * Compare @cs's cpu and mem masks against top_cpuset and if some have gone /Users/rubber/linux/kernel/cgroup/cpuset.c: 3104
 * offline, update @cs accordingly.  If @cs ends up with no CPU or memory, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3105
 * all its tasks are moved to the nearest ancestor with both resources. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3106
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3120
	 * We have raced with task attaching. We wait until attaching /Users/rubber/linux/kernel/cgroup/cpuset.c: 3121
	 * is finished, so we won't attach a task to an empty cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3122
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3134
		 * Make sure that CPUs allocated to child partitions /Users/rubber/linux/kernel/cgroup/cpuset.c: 3135
		 * do not show up in effective_cpus. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3136
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3143
	 * In the unlikely event that a partition root has empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 3144
	 * effective_cpus or its parent becomes erroneous, we have to /Users/rubber/linux/kernel/cgroup/cpuset.c: 3145
	 * transition it to the erroneous state. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3146
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3158
		 * If the effective_cpus is empty because the child /Users/rubber/linux/kernel/cgroup/cpuset.c: 3159
		 * partitions take away all the CPUs, we can keep /Users/rubber/linux/kernel/cgroup/cpuset.c: 3160
		 * the current partition and let the child partitions /Users/rubber/linux/kernel/cgroup/cpuset.c: 3161
		 * fight for available CPUs. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3162
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3181
	 * On the other hand, an erroneous partition root may be transitioned /Users/rubber/linux/kernel/cgroup/cpuset.c: 3182
	 * back to a regular one or a partition root with no CPU allocated /Users/rubber/linux/kernel/cgroup/cpuset.c: 3183
	 * from the parent may change to erroneous. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3184
 * cpuset_hotplug_workfn - handle CPU/memory hotunplug for a cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3210
 * This function is called after either CPU or memory configuration has /Users/rubber/linux/kernel/cgroup/cpuset.c: 3212
 * changed and updates cpuset accordingly.  The top_cpuset is always /Users/rubber/linux/kernel/cgroup/cpuset.c: 3213
 * synchronized to cpu_active_mask and N_MEMORY, which is necessary in /Users/rubber/linux/kernel/cgroup/cpuset.c: 3214
 * order to make cpusets transparent (of no affect) on systems that are /Users/rubber/linux/kernel/cgroup/cpuset.c: 3215
 * actively using CPU hotplug but making no active use of cpusets. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3216
 * Non-root cpusets are only affected by offlining.  If any CPUs or memory /Users/rubber/linux/kernel/cgroup/cpuset.c: 3218
 * nodes have been taken down, cpuset_hotplug_update_tasks() is invoked on /Users/rubber/linux/kernel/cgroup/cpuset.c: 3219
 * all descendants. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3220
 * Note that CPU offlining during suspend is ignored.  We don't modify /Users/rubber/linux/kernel/cgroup/cpuset.c: 3222
 * cpusets across suspend/resume cycles at all. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3223
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3242
	 * If subparts_cpus is populated, it is likely that the check below /Users/rubber/linux/kernel/cgroup/cpuset.c: 3243
	 * will produce a false positive on cpus_updated when the cpu list /Users/rubber/linux/kernel/cgroup/cpuset.c: 3244
	 * isn't changed. It is extra work, but it is better to be safe. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3245
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3250
	 * In the rare case that hotplug removes all the cpus in subparts_cpus, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3251
	 * we assumed that cpus are updated. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3252
		/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3262
		 * Make sure that CPUs allocated to child partitions /Users/rubber/linux/kernel/cgroup/cpuset.c: 3263
		 * do not show up in effective_cpus. If no CPU is left, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3264
		 * we clear the subparts_cpus & let the child partitions /Users/rubber/linux/kernel/cgroup/cpuset.c: 3265
		 * fight for the CPUs again. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3266
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3325
	 * We're inside cpu hotplug critical region which usually nests /Users/rubber/linux/kernel/cgroup/cpuset.c: 3326
	 * inside cgroup synchronization.  Bounce actual hotplug processing /Users/rubber/linux/kernel/cgroup/cpuset.c: 3327
	 * to a work item to avoid reverse locking order. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3328
 * Keep top_cpuset.mems_allowed tracking node_states[N_MEMORY]. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3339
 * Call this routine anytime after node_states[N_MEMORY] changes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3340
 * See cpuset_update_active_cpus() for CPU hotplug handling. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3341
 * cpuset_init_smp - initialize cpus_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 3356
 * Description: Finish top cpuset after cpu, node maps are initialized /Users/rubber/linux/kernel/cgroup/cpuset.c: 3358
 * cpuset_cpus_allowed - return cpus_allowed mask from a tasks cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3376
 * @tsk: pointer to task_struct from which to obtain cpuset->cpus_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3377
 * @pmask: pointer to struct cpumask variable to receive cpus_allowed set. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3378
 * Description: Returns the cpumask_var_t cpus_allowed of the cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3380
 * attached to the specified @tsk.  Guaranteed to return some non-empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 3381
 * subset of cpu_online_mask, even if this means going outside the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3382
 * tasks cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3383
 * cpuset_cpus_allowed_fallback - final fallback before complete catastrophe. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3396
 * @tsk: pointer to task_struct with which the scheduler is struggling /Users/rubber/linux/kernel/cgroup/cpuset.c: 3397
 * Description: In the case that the scheduler cannot find an allowed cpu in /Users/rubber/linux/kernel/cgroup/cpuset.c: 3399
 * tsk->cpus_allowed, we fall back to task_cs(tsk)->cpus_allowed. In legacy /Users/rubber/linux/kernel/cgroup/cpuset.c: 3400
 * mode however, this value is the same as task_cs(tsk)->effective_cpus, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3401
 * which will not contain a sane cpumask during cases such as cpu hotplugging. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3402
 * This is the absolute last resort for the scheduler and it is only used if /Users/rubber/linux/kernel/cgroup/cpuset.c: 3403
 * _every_ other avenue has been traveled. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3404
 * Returns true if the affinity of @tsk was changed, false otherwise. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3406
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3423
	 * We own tsk->cpus_allowed, nobody can change it under us. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3424
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 3425
	 * But we used cs && cs->cpus_allowed lockless and thus can /Users/rubber/linux/kernel/cgroup/cpuset.c: 3426
	 * race with cgroup_attach_task() or update_cpumask() and get /Users/rubber/linux/kernel/cgroup/cpuset.c: 3427
	 * the wrong tsk->cpus_allowed. However, both cases imply the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3428
	 * subsequent cpuset_change_cpumask()->set_cpus_allowed_ptr() /Users/rubber/linux/kernel/cgroup/cpuset.c: 3429
	 * which takes task_rq_lock(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 3430
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 3431
	 * If we are called after it dropped the lock we must see all /Users/rubber/linux/kernel/cgroup/cpuset.c: 3432
	 * changes in tsk_cs()->cpus_allowed. Otherwise we can temporary /Users/rubber/linux/kernel/cgroup/cpuset.c: 3433
	 * set any mask even if it is not right from task_cs() pov, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3434
	 * the pending set_cpus_allowed_ptr() will fix things. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3435
	 * /Users/rubber/linux/kernel/cgroup/cpuset.c: 3436
	 * select_fallback_rq() will fix things ups and set cpu_possible_mask /Users/rubber/linux/kernel/cgroup/cpuset.c: 3437
	 * if required. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3438
 * cpuset_mems_allowed - return mems_allowed mask from a tasks cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3449
 * @tsk: pointer to task_struct from which to obtain cpuset->mems_allowed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3450
 * Description: Returns the nodemask_t mems_allowed of the cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3452
 * attached to the specified @tsk.  Guaranteed to return some non-empty /Users/rubber/linux/kernel/cgroup/cpuset.c: 3453
 * subset of node_states[N_MEMORY], even if this means going outside the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3454
 * tasks cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3455
 * cpuset_nodemask_valid_mems_allowed - check nodemask vs. current mems_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 3473
 * @nodemask: the nodemask to be checked /Users/rubber/linux/kernel/cgroup/cpuset.c: 3474
 * Are any of the nodes in the nodemask allowed in current->mems_allowed? /Users/rubber/linux/kernel/cgroup/cpuset.c: 3476
 * nearest_hardwall_ancestor() - Returns the nearest mem_exclusive or /Users/rubber/linux/kernel/cgroup/cpuset.c: 3484
 * mem_hardwall ancestor to the specified cpuset.  Call holding /Users/rubber/linux/kernel/cgroup/cpuset.c: 3485
 * callback_lock.  If no ancestor is mem_exclusive or mem_hardwall /Users/rubber/linux/kernel/cgroup/cpuset.c: 3486
 * (an unusual configuration), then returns the root cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3487
 * cpuset_node_allowed - Can we allocate on a memory node? /Users/rubber/linux/kernel/cgroup/cpuset.c: 3497
 * @node: is this an allowed node? /Users/rubber/linux/kernel/cgroup/cpuset.c: 3498
 * @gfp_mask: memory allocation flags /Users/rubber/linux/kernel/cgroup/cpuset.c: 3499
 * If we're in interrupt, yes, we can always allocate.  If @node is set in /Users/rubber/linux/kernel/cgroup/cpuset.c: 3501
 * current's mems_allowed, yes.  If it's not a __GFP_HARDWALL request and this /Users/rubber/linux/kernel/cgroup/cpuset.c: 3502
 * node is set in the nearest hardwalled cpuset ancestor to current's cpuset, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3503
 * yes.  If current has access to memory reserves as an oom victim, yes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3504
 * Otherwise, no. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3505
 * GFP_USER allocations are marked with the __GFP_HARDWALL bit, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3507
 * and do not allow allocations outside the current tasks cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3508
 * unless the task has been OOM killed. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3509
 * GFP_KERNEL allocations are not so marked, so can escape to the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3510
 * nearest enclosing hardwalled ancestor cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3511
 * Scanning up parent cpusets requires callback_lock.  The /Users/rubber/linux/kernel/cgroup/cpuset.c: 3513
 * __alloc_pages() routine only calls here with __GFP_HARDWALL bit /Users/rubber/linux/kernel/cgroup/cpuset.c: 3514
 * _not_ set if it's a GFP_KERNEL allocation, and all nodes in the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3515
 * current tasks mems_allowed came up empty on the first pass over /Users/rubber/linux/kernel/cgroup/cpuset.c: 3516
 * the zonelist.  So only GFP_KERNEL allocations, if all nodes in the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3517
 * cpuset are short of memory, might require taking the callback_lock. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3518
 * The first call here from mm/page_alloc:get_page_from_freelist() /Users/rubber/linux/kernel/cgroup/cpuset.c: 3520
 * has __GFP_HARDWALL set in gfp_mask, enforcing hardwall cpusets, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3521
 * so no allocation on a node outside the cpuset is allowed (unless /Users/rubber/linux/kernel/cgroup/cpuset.c: 3522
 * in interrupt, of course). /Users/rubber/linux/kernel/cgroup/cpuset.c: 3523
 * The second pass through get_page_from_freelist() doesn't even call /Users/rubber/linux/kernel/cgroup/cpuset.c: 3525
 * here for GFP_ATOMIC calls.  For those calls, the __alloc_pages() /Users/rubber/linux/kernel/cgroup/cpuset.c: 3526
 * variable 'wait' is not set, and the bit ALLOC_CPUSET is not set /Users/rubber/linux/kernel/cgroup/cpuset.c: 3527
 * in alloc_flags.  That logic and the checks below have the combined /Users/rubber/linux/kernel/cgroup/cpuset.c: 3528
 * affect that: /Users/rubber/linux/kernel/cgroup/cpuset.c: 3529
 *	in_interrupt - any node ok (current task context irrelevant) /Users/rubber/linux/kernel/cgroup/cpuset.c: 3530
 *	GFP_ATOMIC   - any node ok /Users/rubber/linux/kernel/cgroup/cpuset.c: 3531
 *	tsk_is_oom_victim   - any node ok /Users/rubber/linux/kernel/cgroup/cpuset.c: 3532
 *	GFP_KERNEL   - any node in enclosing hardwalled cpuset ok /Users/rubber/linux/kernel/cgroup/cpuset.c: 3533
 *	GFP_USER     - only nodes in current tasks mems allowed ok. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3534
	/* /Users/rubber/linux/kernel/cgroup/cpuset.c: 3546
	 * Allow tasks that have access to memory reserves because they have /Users/rubber/linux/kernel/cgroup/cpuset.c: 3547
	 * been OOM killed to get memory anywhere. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3548
 * cpuset_mem_spread_node() - On which node to begin search for a file page /Users/rubber/linux/kernel/cgroup/cpuset.c: 3571
 * cpuset_slab_spread_node() - On which node to begin search for a slab page /Users/rubber/linux/kernel/cgroup/cpuset.c: 3572
 * If a task is marked PF_SPREAD_PAGE or PF_SPREAD_SLAB (as for /Users/rubber/linux/kernel/cgroup/cpuset.c: 3574
 * tasks in a cpuset with is_spread_page or is_spread_slab set), /Users/rubber/linux/kernel/cgroup/cpuset.c: 3575
 * and if the memory allocation used cpuset_mem_spread_node() /Users/rubber/linux/kernel/cgroup/cpuset.c: 3576
 * to determine on which node to start looking, as it will for /Users/rubber/linux/kernel/cgroup/cpuset.c: 3577
 * certain page cache or slab cache pages such as used for file /Users/rubber/linux/kernel/cgroup/cpuset.c: 3578
 * system buffers and inode caches, then instead of starting on the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3579
 * local node to look for a free page, rather spread the starting /Users/rubber/linux/kernel/cgroup/cpuset.c: 3580
 * node around the tasks mems_allowed nodes. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3581
 * We don't have to worry about the returned node being offline /Users/rubber/linux/kernel/cgroup/cpuset.c: 3583
 * because "it can't happen", and even if it did, it would be ok. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3584
 * The routines calling guarantee_online_mems() are careful to /Users/rubber/linux/kernel/cgroup/cpuset.c: 3586
 * only set nodes in task->mems_allowed that are online.  So it /Users/rubber/linux/kernel/cgroup/cpuset.c: 3587
 * should not be possible for the following code to return an /Users/rubber/linux/kernel/cgroup/cpuset.c: 3588
 * offline node.  But if it did, that would be ok, as this routine /Users/rubber/linux/kernel/cgroup/cpuset.c: 3589
 * is not returning the node where the allocation must be, only /Users/rubber/linux/kernel/cgroup/cpuset.c: 3590
 * the node where the search should start.  The zonelist passed to /Users/rubber/linux/kernel/cgroup/cpuset.c: 3591
 * __alloc_pages() will include all nodes.  If the slab allocator /Users/rubber/linux/kernel/cgroup/cpuset.c: 3592
 * is passed an offline node, it will fall back to the local node. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3593
 * See kmem_cache_alloc_node(). /Users/rubber/linux/kernel/cgroup/cpuset.c: 3594
 * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's? /Users/rubber/linux/kernel/cgroup/cpuset.c: 3623
 * @tsk1: pointer to task_struct of some task. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3624
 * @tsk2: pointer to task_struct of some other task. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3625
 * Description: Return true if @tsk1's mems_allowed intersects the /Users/rubber/linux/kernel/cgroup/cpuset.c: 3627
 * mems_allowed of @tsk2.  Used by the OOM killer to determine if /Users/rubber/linux/kernel/cgroup/cpuset.c: 3628
 * one of the task's memory usage might impact the memory available /Users/rubber/linux/kernel/cgroup/cpuset.c: 3629
 * to the other. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3630
 * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed /Users/rubber/linux/kernel/cgroup/cpuset.c: 3640
 * Description: Prints current's name, cpuset name, and cached copy of its /Users/rubber/linux/kernel/cgroup/cpuset.c: 3642
 * mems_allowed to the kernel log. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3643
 * Collection of memory_pressure is suppressed unless /Users/rubber/linux/kernel/cgroup/cpuset.c: 3661
 * this flag is enabled by writing "1" to the special /Users/rubber/linux/kernel/cgroup/cpuset.c: 3662
 * cpuset file 'memory_pressure_enabled' in the root cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3663
 * cpuset_memory_pressure_bump - keep stats of per-cpuset reclaims. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3669
 * Keep a running average of the rate of synchronous (direct) /Users/rubber/linux/kernel/cgroup/cpuset.c: 3671
 * page reclaim efforts initiated by tasks in each cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3672
 * This represents the rate at which some task in the cpuset /Users/rubber/linux/kernel/cgroup/cpuset.c: 3674
 * ran low on memory on all nodes it was allowed to use, and /Users/rubber/linux/kernel/cgroup/cpuset.c: 3675
 * had to enter the kernels page reclaim code in an effort to /Users/rubber/linux/kernel/cgroup/cpuset.c: 3676
 * create more free memory by tossing clean pages or swapping /Users/rubber/linux/kernel/cgroup/cpuset.c: 3677
 * or writing dirty pages. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3678
 * Display to user space in the per-cpuset read-only file /Users/rubber/linux/kernel/cgroup/cpuset.c: 3680
 * "memory_pressure".  Value displayed is an integer /Users/rubber/linux/kernel/cgroup/cpuset.c: 3681
 * representing the recent rate of entry into the synchronous /Users/rubber/linux/kernel/cgroup/cpuset.c: 3682
 * (direct) page reclaim by any task attached to the cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3683
 * proc_cpuset_show() /Users/rubber/linux/kernel/cgroup/cpuset.c: 3695
 *  - Print tasks cpuset path into seq_file. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3696
 *  - Used for /proc/<pid>/cpuset. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3697
 *  - No need to task_lock(tsk) on this tsk->cpuset reference, as it /Users/rubber/linux/kernel/cgroup/cpuset.c: 3698
 *    doesn't really matter if tsk->cpuset changes after we read it, /Users/rubber/linux/kernel/cgroup/cpuset.c: 3699
 *    and we take cpuset_rwsem, keeping cpuset_attach() from changing it /Users/rubber/linux/kernel/cgroup/cpuset.c: 3700
 *    anyway. /Users/rubber/linux/kernel/cgroup/cpuset.c: 3701
 * cgroup_freezer.c -  control group freezer subsystem /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 2
 * Copyright IBM Corporation, 2007 /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 4
 * Author : Cedric Le Goater <clg@fr.ibm.com> /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 6
 * This program is free software; you can redistribute it and/or modify it /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 8
 * under the terms of version 2.1 of the GNU Lesser General Public License /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 9
 * as published by the Free Software Foundation. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 10
 * This program is distributed in the hope that it would be useful, but /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 12
 * WITHOUT ANY WARRANTY; without even the implied warranty of /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 13
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 14
 * A cgroup is freezing if any FREEZING flags are set.  FREEZING_SELF is /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 27
 * set if "FROZEN" is written to freezer.state cgroupfs file, and cleared /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 28
 * for "THAWED".  FREEZING_PARENT is set if the parent freezer is FREEZING /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 29
 * for whatever reason.  IOW, a cgroup has FREEZING_PARENT set if one of /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 30
 * its ancestors has FREEZING_SELF set. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 31
 * freezer_css_online - commit creation of a freezer css /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 98
 * @css: css being created /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 99
 * We're committing to creation of @css.  Mark it online and inherit /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 101
 * parent's freezing state while holding both parent's and our /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 102
 * freezer->lock. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 103
 * freezer_css_offline - initiate destruction of a freezer css /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 124
 * @css: css being destroyed /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 125
 * @css is going away.  Mark it dead and decrement system_freezing_count if /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 127
 * it was holding one. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 128
 * Tasks can be migrated into a different freezer anytime regardless of its /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 150
 * current state.  freezer_attach() is responsible for making new tasks /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 151
 * conform to the current state. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 152
 * Freezer state changes and task migration are synchronized via /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 154
 * @freezer->lock.  freezer_attach() makes the new tasks conform to the /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 155
 * current state and all following state changes can see the new tasks. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 156
	/* /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 165
	 * Make the new tasks conform to the current state of @new_css. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 166
	 * For simplicity, when migrating any task to a FROZEN cgroup, we /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 167
	 * revert it to FREEZING and let update_if_frozen() determine the /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 168
	 * correct state later. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 169
	 * /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 170
	 * Tasks in @tset are on @new_css but may not conform to its /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 171
	 * current state before executing the following - !frozen tasks may /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 172
	 * be visible in a FROZEN cgroup and frozen tasks in a THAWED one. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 173
 * freezer_fork - cgroup post fork callback /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 194
 * @task: a task which has just been forked /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 195
 * @task has just been created and should conform to the current state of /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 197
 * the cgroup_freezer it belongs to.  This function may race against /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 198
 * freezer_attach().  Losing to freezer_attach() means that we don't have /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 199
 * to do anything as freezer_attach() will put @task into the appropriate /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 200
 * state. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 201
	/* /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 207
	 * The root cgroup is non-freezable, so we can skip locking the /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 208
	 * freezer.  This is safe regardless of race with task migration. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 209
	 * If we didn't race or won, skipping is obviously the right thing /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 210
	 * to do.  If we lost and root is the new cgroup, noop is still the /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 211
	 * right thing to do. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 212
 * update_if_frozen - update whether a cgroup finished freezing /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 229
 * @css: css of interest /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 230
 * Once FREEZING is initiated, transition to FROZEN is lazily updated by /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 232
 * calling this function.  If the current state is FREEZING but not FROZEN, /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 233
 * this function checks whether all tasks of this cgroup and the descendant /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 234
 * cgroups finished freezing and, if so, sets FROZEN. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 235
 * The caller is responsible for grabbing RCU read lock and calling /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 237
 * update_if_frozen() on all descendants prior to invoking this function. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 238
 * Task states and freezer state might disagree while tasks are being /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 240
 * migrated into or out of @css, so we can't verify task states against /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 241
 * @freezer state here.  See freezer_attach() for details. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 242
			/* /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 275
			 * freezer_should_skip() indicates that the task /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 276
			 * should be skipped when determining freezing /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 277
			 * completion.  Consider it frozen in addition to /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 278
			 * the usual frozen condition. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 279
 * freezer_apply_state - apply state change to a single cgroup_freezer /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 341
 * @freezer: freezer to apply state change to /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 342
 * @freeze: whether to freeze or unfreeze /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 343
 * @state: CGROUP_FREEZING_* flag to set or clear /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 344
 * Set or clear @state on @cgroup according to @freeze, and perform /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 346
 * freezing or thawing as necessary. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 347
 * freezer_change_state - change the freezing state of a cgroup_freezer /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 378
 * @freezer: freezer of interest /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 379
 * @freeze: whether to freeze or thaw /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 380
 * Freeze or thaw @freezer according to @freeze.  The operations are /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 382
 * recursive - all descendants of @freezer will be affected. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 383
	/* /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 389
	 * Update all its descendants in pre-order traversal.  Each /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 390
	 * descendant will try to inherit its parent's FREEZING state as /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 391
	 * CGROUP_FREEZING_PARENT. /Users/rubber/linux/kernel/cgroup/legacy_freezer.c: 392
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/cgroup/debug.c: 1
 * Debug controller /Users/rubber/linux/kernel/cgroup/debug.c: 3
 * WARNING: This controller is for cgroup core debugging only. /Users/rubber/linux/kernel/cgroup/debug.c: 5
 * Its interfaces are unstable and subject to changes at any time. /Users/rubber/linux/kernel/cgroup/debug.c: 6
 * debug_taskcount_read - return the number of tasks in a cgroup. /Users/rubber/linux/kernel/cgroup/debug.c: 31
 * @cgrp: the cgroup in question /Users/rubber/linux/kernel/cgroup/debug.c: 32
	/* /Users/rubber/linux/kernel/cgroup/debug.c: 60
	 * Print the css'es stored in the current css_set. /Users/rubber/linux/kernel/cgroup/debug.c: 61
		/* /Users/rubber/linux/kernel/cgroup/debug.c: 128
		 * Print out the proc_cset and threaded_cset relationship /Users/rubber/linux/kernel/cgroup/debug.c: 129
		 * and highlight difference between refcount and task_count. /Users/rubber/linux/kernel/cgroup/debug.c: 130
				/* /Users/rubber/linux/kernel/cgroup/debug.c: 153
				 * Take out the one additional reference in /Users/rubber/linux/kernel/cgroup/debug.c: 154
				 * init_css_set. /Users/rubber/linux/kernel/cgroup/debug.c: 155
 * On v2, debug is an implicit controller enabled by "cgroup_debug" boot /Users/rubber/linux/kernel/cgroup/debug.c: 373
 * parameter. /Users/rubber/linux/kernel/cgroup/debug.c: 374
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/cgroup/pids.c: 1
 * Process number limiting controller for cgroups. /Users/rubber/linux/kernel/cgroup/pids.c: 3
 * Used to allow a cgroup hierarchy to stop any new processes from fork()ing /Users/rubber/linux/kernel/cgroup/pids.c: 5
 * after a certain limit is reached. /Users/rubber/linux/kernel/cgroup/pids.c: 6
 * Since it is trivial to hit the task limit without hitting any kmemcg limits /Users/rubber/linux/kernel/cgroup/pids.c: 8
 * in place, PIDs are a fundamental resource. As such, PID exhaustion must be /Users/rubber/linux/kernel/cgroup/pids.c: 9
 * preventable in the scope of a cgroup hierarchy by allowing resource limiting /Users/rubber/linux/kernel/cgroup/pids.c: 10
 * of the number of tasks in a cgroup. /Users/rubber/linux/kernel/cgroup/pids.c: 11
 * In order to use the `pids` controller, set the maximum number of tasks in /Users/rubber/linux/kernel/cgroup/pids.c: 13
 * pids.max (this is not available in the root cgroup for obvious reasons). The /Users/rubber/linux/kernel/cgroup/pids.c: 14
 * number of processes currently in the cgroup is given by pids.current. /Users/rubber/linux/kernel/cgroup/pids.c: 15
 * Organisational operations are not blocked by cgroup policies, so it is /Users/rubber/linux/kernel/cgroup/pids.c: 16
 * possible to have pids.current > pids.max. However, it is not possible to /Users/rubber/linux/kernel/cgroup/pids.c: 17
 * violate a cgroup policy through fork(). fork() will return -EAGAIN if forking /Users/rubber/linux/kernel/cgroup/pids.c: 18
 * would cause a cgroup policy to be violated. /Users/rubber/linux/kernel/cgroup/pids.c: 19
 * To set a cgroup to have no limit, set pids.max to "max". This is the default /Users/rubber/linux/kernel/cgroup/pids.c: 21
 * for all new cgroups (N.B. that PID limits are hierarchical, so the most /Users/rubber/linux/kernel/cgroup/pids.c: 22
 * stringent limit in the hierarchy is followed). /Users/rubber/linux/kernel/cgroup/pids.c: 23
 * pids.current tracks all child cgroup hierarchies, so parent/pids.current is /Users/rubber/linux/kernel/cgroup/pids.c: 25
 * a superset of parent/child/pids.current. /Users/rubber/linux/kernel/cgroup/pids.c: 26
 * Copyright (C) 2015 Aleksa Sarai <cyphar@cyphar.com> /Users/rubber/linux/kernel/cgroup/pids.c: 28
	/* /Users/rubber/linux/kernel/cgroup/pids.c: 44
	 * Use 64-bit types so that we can safely represent "max" as /Users/rubber/linux/kernel/cgroup/pids.c: 45
	 * %PIDS_MAX = (%PID_MAX_LIMIT + 1). /Users/rubber/linux/kernel/cgroup/pids.c: 46
 * pids_cancel - uncharge the local pid count /Users/rubber/linux/kernel/cgroup/pids.c: 89
 * @pids: the pid cgroup state /Users/rubber/linux/kernel/cgroup/pids.c: 90
 * @num: the number of pids to cancel /Users/rubber/linux/kernel/cgroup/pids.c: 91
 * This function will WARN if the pid count goes under 0, because such a case is /Users/rubber/linux/kernel/cgroup/pids.c: 93
 * a bug in the pids controller proper. /Users/rubber/linux/kernel/cgroup/pids.c: 94
	/* /Users/rubber/linux/kernel/cgroup/pids.c: 98
	 * A negative count (or overflow for that matter) is invalid, /Users/rubber/linux/kernel/cgroup/pids.c: 99
	 * and indicates a bug in the `pids` controller proper. /Users/rubber/linux/kernel/cgroup/pids.c: 100
 * pids_uncharge - hierarchically uncharge the pid count /Users/rubber/linux/kernel/cgroup/pids.c: 106
 * @pids: the pid cgroup state /Users/rubber/linux/kernel/cgroup/pids.c: 107
 * @num: the number of pids to uncharge /Users/rubber/linux/kernel/cgroup/pids.c: 108
 * pids_charge - hierarchically charge the pid count /Users/rubber/linux/kernel/cgroup/pids.c: 119
 * @pids: the pid cgroup state /Users/rubber/linux/kernel/cgroup/pids.c: 120
 * @num: the number of pids to charge /Users/rubber/linux/kernel/cgroup/pids.c: 121
 * This function does *not* follow the pid limit set. It cannot fail and the new /Users/rubber/linux/kernel/cgroup/pids.c: 123
 * pid count may exceed the limit. This is only used for reverting failed /Users/rubber/linux/kernel/cgroup/pids.c: 124
 * attaches, where there is no other way out than violating the limit. /Users/rubber/linux/kernel/cgroup/pids.c: 125
 * pids_try_charge - hierarchically try to charge the pid count /Users/rubber/linux/kernel/cgroup/pids.c: 136
 * @pids: the pid cgroup state /Users/rubber/linux/kernel/cgroup/pids.c: 137
 * @num: the number of pids to charge /Users/rubber/linux/kernel/cgroup/pids.c: 138
 * This function follows the set limit. It will fail if the charge would cause /Users/rubber/linux/kernel/cgroup/pids.c: 140
 * the new value to exceed the hierarchical limit. Returns 0 if the charge /Users/rubber/linux/kernel/cgroup/pids.c: 141
 * succeeded, otherwise -EAGAIN. /Users/rubber/linux/kernel/cgroup/pids.c: 142
		/* /Users/rubber/linux/kernel/cgroup/pids.c: 152
		 * Since new is capped to the maximum number of pid_t, if /Users/rubber/linux/kernel/cgroup/pids.c: 153
		 * p->limit is %PIDS_MAX then we know that this test will never /Users/rubber/linux/kernel/cgroup/pids.c: 154
		 * fail. /Users/rubber/linux/kernel/cgroup/pids.c: 155
		/* /Users/rubber/linux/kernel/cgroup/pids.c: 181
		 * No need to pin @old_css between here and cancel_attach() /Users/rubber/linux/kernel/cgroup/pids.c: 182
		 * because cgroup core protects it from being freed before /Users/rubber/linux/kernel/cgroup/pids.c: 183
		 * the migration completes or fails. /Users/rubber/linux/kernel/cgroup/pids.c: 184
 * task_css_check(true) in pids_can_fork() and pids_cancel_fork() relies /Users/rubber/linux/kernel/cgroup/pids.c: 215
 * on cgroup_threadgroup_change_begin() held by the copy_process(). /Users/rubber/linux/kernel/cgroup/pids.c: 216
	/* /Users/rubber/linux/kernel/cgroup/pids.c: 284
	 * Limit updates don't need to be mutex'd, since it isn't /Users/rubber/linux/kernel/cgroup/pids.c: 285
	 * critical that any racing fork()s follow the new limit. /Users/rubber/linux/kernel/cgroup/pids.c: 286
SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/cgroup/freezer.c: 1
 * Propagate the cgroup frozen state upwards by the cgroup tree. /Users/rubber/linux/kernel/cgroup/freezer.c: 12
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 18
	 * If the new state is frozen, some freezing ancestor cgroups may change /Users/rubber/linux/kernel/cgroup/freezer.c: 19
	 * their state too, depending on if all their descendants are frozen. /Users/rubber/linux/kernel/cgroup/freezer.c: 20
	 * /Users/rubber/linux/kernel/cgroup/freezer.c: 21
	 * Otherwise, all ancestor cgroups are forced into the non-frozen state. /Users/rubber/linux/kernel/cgroup/freezer.c: 22
 * Revisit the cgroup frozen state. /Users/rubber/linux/kernel/cgroup/freezer.c: 49
 * Checks if the cgroup is really frozen and perform all state transitions. /Users/rubber/linux/kernel/cgroup/freezer.c: 50
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 58
	 * If the cgroup has to be frozen (CGRP_FREEZE bit set), /Users/rubber/linux/kernel/cgroup/freezer.c: 59
	 * and all tasks are frozen and/or stopped, let's consider /Users/rubber/linux/kernel/cgroup/freezer.c: 60
	 * the cgroup frozen. Otherwise it's not frozen. /Users/rubber/linux/kernel/cgroup/freezer.c: 61
 * Increment cgroup's nr_frozen_tasks. /Users/rubber/linux/kernel/cgroup/freezer.c: 87
 * Decrement cgroup's nr_frozen_tasks. /Users/rubber/linux/kernel/cgroup/freezer.c: 95
 * Enter frozen/stopped state, if not yet there. Update cgroup's counters, /Users/rubber/linux/kernel/cgroup/freezer.c: 104
 * and revisit the state of the cgroup, if necessary. /Users/rubber/linux/kernel/cgroup/freezer.c: 105
 * Conditionally leave frozen/stopped state. Update cgroup's counters, /Users/rubber/linux/kernel/cgroup/freezer.c: 123
 * and revisit the state of the cgroup, if necessary. /Users/rubber/linux/kernel/cgroup/freezer.c: 124
 * If always_leave is not set, and the cgroup is freezing, /Users/rubber/linux/kernel/cgroup/freezer.c: 126
 * we're racing with the cgroup freezing. In this case, we don't /Users/rubber/linux/kernel/cgroup/freezer.c: 127
 * drop the frozen counter to avoid a transient switch to /Users/rubber/linux/kernel/cgroup/freezer.c: 128
 * the unfrozen state. /Users/rubber/linux/kernel/cgroup/freezer.c: 129
 * Freeze or unfreeze the task by setting or clearing the JOBCTL_TRAP_FREEZE /Users/rubber/linux/kernel/cgroup/freezer.c: 152
 * jobctl bit. /Users/rubber/linux/kernel/cgroup/freezer.c: 153
 * Freeze or unfreeze all tasks in the given cgroup. /Users/rubber/linux/kernel/cgroup/freezer.c: 175
		/* /Users/rubber/linux/kernel/cgroup/freezer.c: 198
		 * Ignore kernel threads here. Freezing cgroups containing /Users/rubber/linux/kernel/cgroup/freezer.c: 199
		 * kthreads isn't supported. /Users/rubber/linux/kernel/cgroup/freezer.c: 200
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 208
	 * Cgroup state should be revisited here to cover empty leaf cgroups /Users/rubber/linux/kernel/cgroup/freezer.c: 209
	 * and cgroups which descendants are already in the desired state. /Users/rubber/linux/kernel/cgroup/freezer.c: 210
 * Adjust the task state (freeze or unfreeze) and revisit the state of /Users/rubber/linux/kernel/cgroup/freezer.c: 219
 * source and destination cgroups. /Users/rubber/linux/kernel/cgroup/freezer.c: 220
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 227
	 * Kernel threads are not supposed to be frozen at all. /Users/rubber/linux/kernel/cgroup/freezer.c: 228
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 233
	 * It's not necessary to do changes if both of the src and dst cgroups /Users/rubber/linux/kernel/cgroup/freezer.c: 234
	 * are not freezing and task is not frozen. /Users/rubber/linux/kernel/cgroup/freezer.c: 235
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 242
	 * Adjust counters of freezing and frozen tasks. /Users/rubber/linux/kernel/cgroup/freezer.c: 243
	 * Note, that if the task is frozen, but the destination cgroup is not /Users/rubber/linux/kernel/cgroup/freezer.c: 244
	 * frozen, we bump both counters to keep them balanced. /Users/rubber/linux/kernel/cgroup/freezer.c: 245
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 254
	 * Force the task to the desired state. /Users/rubber/linux/kernel/cgroup/freezer.c: 255
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 268
	 * Nothing changed? Just exit. /Users/rubber/linux/kernel/cgroup/freezer.c: 269
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 276
	 * Propagate changes downwards the cgroup tree. /Users/rubber/linux/kernel/cgroup/freezer.c: 277
			/* /Users/rubber/linux/kernel/cgroup/freezer.c: 287
			 * Already frozen because of ancestor's settings? /Users/rubber/linux/kernel/cgroup/freezer.c: 288
			/* /Users/rubber/linux/kernel/cgroup/freezer.c: 294
			 * Still frozen because of ancestor's settings? /Users/rubber/linux/kernel/cgroup/freezer.c: 295
		/* /Users/rubber/linux/kernel/cgroup/freezer.c: 303
		 * Do change actual state: freeze or unfreeze. /Users/rubber/linux/kernel/cgroup/freezer.c: 304
	/* /Users/rubber/linux/kernel/cgroup/freezer.c: 310
	 * Even if the actual state hasn't changed, let's notify a user. /Users/rubber/linux/kernel/cgroup/freezer.c: 311
	 * The state can be enforced by an ancestor cgroup: the cgroup /Users/rubber/linux/kernel/cgroup/freezer.c: 312
	 * can already be in the desired state or it can be locked in the /Users/rubber/linux/kernel/cgroup/freezer.c: 313
	 * opposite state, so that the transition will never happen. /Users/rubber/linux/kernel/cgroup/freezer.c: 314
	 * In both cases it's better to notify a user, that there is /Users/rubber/linux/kernel/cgroup/freezer.c: 315
	 * nothing to wait for. /Users/rubber/linux/kernel/cgroup/freezer.c: 316
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/cgroup/rstat.c: 1
 * cgroup_rstat_updated - keep track of updated rstat_cpu /Users/rubber/linux/kernel/cgroup/rstat.c: 17
 * @cgrp: target cgroup /Users/rubber/linux/kernel/cgroup/rstat.c: 18
 * @cpu: cpu on which rstat_cpu was updated /Users/rubber/linux/kernel/cgroup/rstat.c: 19
 * @cgrp's rstat_cpu on @cpu was updated.  Put it on the parent's matching /Users/rubber/linux/kernel/cgroup/rstat.c: 21
 * rstat_cpu->updated_children list.  See the comment on top of /Users/rubber/linux/kernel/cgroup/rstat.c: 22
 * cgroup_rstat_cpu definition for details. /Users/rubber/linux/kernel/cgroup/rstat.c: 23
	/* /Users/rubber/linux/kernel/cgroup/rstat.c: 30
	 * Speculative already-on-list test. This may race leading to /Users/rubber/linux/kernel/cgroup/rstat.c: 31
	 * temporary inaccuracies, which is fine. /Users/rubber/linux/kernel/cgroup/rstat.c: 32
	 * /Users/rubber/linux/kernel/cgroup/rstat.c: 33
	 * Because @parent's updated_children is terminated with @parent /Users/rubber/linux/kernel/cgroup/rstat.c: 34
	 * instead of NULL, we can tell whether @cgrp is on the list by /Users/rubber/linux/kernel/cgroup/rstat.c: 35
	 * testing the next pointer for NULL. /Users/rubber/linux/kernel/cgroup/rstat.c: 36
		/* /Users/rubber/linux/kernel/cgroup/rstat.c: 49
		 * Both additions and removals are bottom-up.  If a cgroup /Users/rubber/linux/kernel/cgroup/rstat.c: 50
		 * is already in the tree, all ancestors are. /Users/rubber/linux/kernel/cgroup/rstat.c: 51
 * cgroup_rstat_cpu_pop_updated - iterate and dismantle rstat_cpu updated tree /Users/rubber/linux/kernel/cgroup/rstat.c: 73
 * @pos: current position /Users/rubber/linux/kernel/cgroup/rstat.c: 74
 * @root: root of the tree to traversal /Users/rubber/linux/kernel/cgroup/rstat.c: 75
 * @cpu: target cpu /Users/rubber/linux/kernel/cgroup/rstat.c: 76
 * Walks the updated rstat_cpu tree on @cpu from @root.  %NULL @pos starts /Users/rubber/linux/kernel/cgroup/rstat.c: 78
 * the traversal and %NULL return indicates the end.  During traversal, /Users/rubber/linux/kernel/cgroup/rstat.c: 79
 * each returned cgroup is unlinked from the tree.  Must be called with the /Users/rubber/linux/kernel/cgroup/rstat.c: 80
 * matching cgroup_rstat_cpu_lock held. /Users/rubber/linux/kernel/cgroup/rstat.c: 81
 * The only ordering guarantee is that, for a parent and a child pair /Users/rubber/linux/kernel/cgroup/rstat.c: 83
 * covered by a given traversal, if a child is visited, its parent is /Users/rubber/linux/kernel/cgroup/rstat.c: 84
 * guaranteed to be visited afterwards. /Users/rubber/linux/kernel/cgroup/rstat.c: 85
	/* /Users/rubber/linux/kernel/cgroup/rstat.c: 95
	 * We're gonna walk down to the first leaf and visit/remove it.  We /Users/rubber/linux/kernel/cgroup/rstat.c: 96
	 * can pick whatever unvisited node as the starting point. /Users/rubber/linux/kernel/cgroup/rstat.c: 97
	/* /Users/rubber/linux/kernel/cgroup/rstat.c: 112
	 * Unlink @pos from the tree.  As the updated_children list is /Users/rubber/linux/kernel/cgroup/rstat.c: 113
	 * singly linked, we have to walk it to find the removal point. /Users/rubber/linux/kernel/cgroup/rstat.c: 114
	 * However, due to the way we traverse, @pos will be the first /Users/rubber/linux/kernel/cgroup/rstat.c: 115
	 * child in most cases. The only exception is @root. /Users/rubber/linux/kernel/cgroup/rstat.c: 116
 * cgroup_rstat_flush - flush stats in @cgrp's subtree /Users/rubber/linux/kernel/cgroup/rstat.c: 186
 * @cgrp: target cgroup /Users/rubber/linux/kernel/cgroup/rstat.c: 187
 * Collect all per-cpu stats in @cgrp's subtree into the global counters /Users/rubber/linux/kernel/cgroup/rstat.c: 189
 * and propagate them upwards.  After this function returns, all cgroups in /Users/rubber/linux/kernel/cgroup/rstat.c: 190
 * the subtree have up-to-date ->stat. /Users/rubber/linux/kernel/cgroup/rstat.c: 191
 * This also gets all cgroups in the subtree including @cgrp off the /Users/rubber/linux/kernel/cgroup/rstat.c: 193
 * ->updated_children lists. /Users/rubber/linux/kernel/cgroup/rstat.c: 194
 * This function may block. /Users/rubber/linux/kernel/cgroup/rstat.c: 196
 * cgroup_rstat_flush_irqsafe - irqsafe version of cgroup_rstat_flush() /Users/rubber/linux/kernel/cgroup/rstat.c: 208
 * @cgrp: target cgroup /Users/rubber/linux/kernel/cgroup/rstat.c: 209
 * This function can be called from any context. /Users/rubber/linux/kernel/cgroup/rstat.c: 211
 * cgroup_rstat_flush_hold - flush stats in @cgrp's subtree and hold /Users/rubber/linux/kernel/cgroup/rstat.c: 223
 * @cgrp: target cgroup /Users/rubber/linux/kernel/cgroup/rstat.c: 224
 * Flush stats in @cgrp's subtree and prevent further flushes.  Must be /Users/rubber/linux/kernel/cgroup/rstat.c: 226
 * paired with cgroup_rstat_flush_release(). /Users/rubber/linux/kernel/cgroup/rstat.c: 227
 * This function may block. /Users/rubber/linux/kernel/cgroup/rstat.c: 229
 * cgroup_rstat_flush_release - release cgroup_rstat_flush_hold() /Users/rubber/linux/kernel/cgroup/rstat.c: 240
 * Functions for cgroup basic resource statistics implemented on top of /Users/rubber/linux/kernel/cgroup/rstat.c: 298
 * rstat. /Users/rubber/linux/kernel/cgroup/rstat.c: 299
 * compute the cputime for the root cgroup by getting the per cpu data /Users/rubber/linux/kernel/cgroup/rstat.c: 404
 * at a global level, then categorizing the fields in a manner consistent /Users/rubber/linux/kernel/cgroup/rstat.c: 405
 * with how it is done by __cgroup_account_cputime_field for each bit of /Users/rubber/linux/kernel/cgroup/rstat.c: 406
 * cpu time attributed to a cgroup. /Users/rubber/linux/kernel/cgroup/rstat.c: 407
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/cgroup/misc.c: 1
 * Miscellaneous cgroup controller /Users/rubber/linux/kernel/cgroup/misc.c: 3
 * Copyright 2020 Google LLC /Users/rubber/linux/kernel/cgroup/misc.c: 5
 * Author: Vipin Sharma <vipinsh@google.com> /Users/rubber/linux/kernel/cgroup/misc.c: 6
 * Miscellaneous resources capacity for the entire machine. 0 capacity means /Users/rubber/linux/kernel/cgroup/misc.c: 33
 * resource is not initialized or not present in the host. /Users/rubber/linux/kernel/cgroup/misc.c: 34
 * root_cg.max and capacity are independent of each other. root_cg.max can be /Users/rubber/linux/kernel/cgroup/misc.c: 36
 * more than the actual capacity. We are using Limits resource distribution /Users/rubber/linux/kernel/cgroup/misc.c: 37
 * model of cgroup for miscellaneous controller. /Users/rubber/linux/kernel/cgroup/misc.c: 38
 * parent_misc() - Get the parent of the passed misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 43
 * @cgroup: cgroup whose parent needs to be fetched. /Users/rubber/linux/kernel/cgroup/misc.c: 44
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 46
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 47
 * * struct misc_cg* - Parent of the @cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 48
 * * %NULL - If @cgroup is null or the passed cgroup does not have a parent. /Users/rubber/linux/kernel/cgroup/misc.c: 49
 * valid_type() - Check if @type is valid or not. /Users/rubber/linux/kernel/cgroup/misc.c: 57
 * @type: misc res type. /Users/rubber/linux/kernel/cgroup/misc.c: 58
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 60
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 61
 * * true - If valid type. /Users/rubber/linux/kernel/cgroup/misc.c: 62
 * * false - If not valid type. /Users/rubber/linux/kernel/cgroup/misc.c: 63
 * misc_cg_res_total_usage() - Get the current total usage of the resource. /Users/rubber/linux/kernel/cgroup/misc.c: 71
 * @type: misc res type. /Users/rubber/linux/kernel/cgroup/misc.c: 72
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 74
 * Return: Current total usage of the resource. /Users/rubber/linux/kernel/cgroup/misc.c: 75
 * misc_cg_set_capacity() - Set the capacity of the misc cgroup res. /Users/rubber/linux/kernel/cgroup/misc.c: 87
 * @type: Type of the misc res. /Users/rubber/linux/kernel/cgroup/misc.c: 88
 * @capacity: Supported capacity of the misc res on the host. /Users/rubber/linux/kernel/cgroup/misc.c: 89
 * If capacity is 0 then the charging a misc cgroup fails for that type. /Users/rubber/linux/kernel/cgroup/misc.c: 91
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 93
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 94
 * * %0 - Successfully registered the capacity. /Users/rubber/linux/kernel/cgroup/misc.c: 95
 * * %-EINVAL - If @type is invalid. /Users/rubber/linux/kernel/cgroup/misc.c: 96
 * misc_cg_cancel_charge() - Cancel the charge from the misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 109
 * @type: Misc res type in misc cg to cancel the charge from. /Users/rubber/linux/kernel/cgroup/misc.c: 110
 * @cg: Misc cgroup to cancel charge from. /Users/rubber/linux/kernel/cgroup/misc.c: 111
 * @amount: Amount to cancel. /Users/rubber/linux/kernel/cgroup/misc.c: 112
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 114
 * misc_cg_try_charge() - Try charging the misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 125
 * @type: Misc res type to charge. /Users/rubber/linux/kernel/cgroup/misc.c: 126
 * @cg: Misc cgroup which will be charged. /Users/rubber/linux/kernel/cgroup/misc.c: 127
 * @amount: Amount to charge. /Users/rubber/linux/kernel/cgroup/misc.c: 128
 * Charge @amount to the misc cgroup. Caller must use the same cgroup during /Users/rubber/linux/kernel/cgroup/misc.c: 130
 * the uncharge call. /Users/rubber/linux/kernel/cgroup/misc.c: 131
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 133
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 134
 * * %0 - If successfully charged. /Users/rubber/linux/kernel/cgroup/misc.c: 135
 * * -EINVAL - If @type is invalid or misc res has 0 capacity. /Users/rubber/linux/kernel/cgroup/misc.c: 136
 * * -EBUSY - If max limit will be crossed or total usage will be more than the /Users/rubber/linux/kernel/cgroup/misc.c: 137
 *	      capacity. /Users/rubber/linux/kernel/cgroup/misc.c: 138
 * misc_cg_uncharge() - Uncharge the misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 180
 * @type: Misc res type which was charged. /Users/rubber/linux/kernel/cgroup/misc.c: 181
 * @cg: Misc cgroup which will be uncharged. /Users/rubber/linux/kernel/cgroup/misc.c: 182
 * @amount: Charged amount. /Users/rubber/linux/kernel/cgroup/misc.c: 183
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 185
 * misc_cg_max_show() - Show the misc cgroup max limit. /Users/rubber/linux/kernel/cgroup/misc.c: 201
 * @sf: Interface file /Users/rubber/linux/kernel/cgroup/misc.c: 202
 * @v: Arguments passed /Users/rubber/linux/kernel/cgroup/misc.c: 203
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 205
 * Return: 0 to denote successful print. /Users/rubber/linux/kernel/cgroup/misc.c: 206
 * misc_cg_max_write() - Update the maximum limit of the cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 229
 * @of: Handler for the file. /Users/rubber/linux/kernel/cgroup/misc.c: 230
 * @buf: Data from the user. It should be either "max", 0, or a positive /Users/rubber/linux/kernel/cgroup/misc.c: 231
 *	 integer. /Users/rubber/linux/kernel/cgroup/misc.c: 232
 * @nbytes: Number of bytes of the data. /Users/rubber/linux/kernel/cgroup/misc.c: 233
 * @off: Offset in the file. /Users/rubber/linux/kernel/cgroup/misc.c: 234
 * User can pass data like: /Users/rubber/linux/kernel/cgroup/misc.c: 236
 * echo sev 23 > misc.max, OR /Users/rubber/linux/kernel/cgroup/misc.c: 237
 * echo sev max > misc.max /Users/rubber/linux/kernel/cgroup/misc.c: 238
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 240
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 241
 * * >= 0 - Number of bytes processed in the input. /Users/rubber/linux/kernel/cgroup/misc.c: 242
 * * -EINVAL - If buf is not valid. /Users/rubber/linux/kernel/cgroup/misc.c: 243
 * * -ERANGE - If number is bigger than the unsigned long capacity. /Users/rubber/linux/kernel/cgroup/misc.c: 244
 * misc_cg_current_show() - Show the current usage of the misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 290
 * @sf: Interface file /Users/rubber/linux/kernel/cgroup/misc.c: 291
 * @v: Arguments passed /Users/rubber/linux/kernel/cgroup/misc.c: 292
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 294
 * Return: 0 to denote successful print. /Users/rubber/linux/kernel/cgroup/misc.c: 295
 * misc_cg_capacity_show() - Show the total capacity of misc res on the host. /Users/rubber/linux/kernel/cgroup/misc.c: 313
 * @sf: Interface file /Users/rubber/linux/kernel/cgroup/misc.c: 314
 * @v: Arguments passed /Users/rubber/linux/kernel/cgroup/misc.c: 315
 * Only present in the root cgroup directory. /Users/rubber/linux/kernel/cgroup/misc.c: 317
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 319
 * Return: 0 to denote successful print. /Users/rubber/linux/kernel/cgroup/misc.c: 320
 * misc_cg_alloc() - Allocate misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 377
 * @parent_css: Parent cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 378
 * Context: Process context. /Users/rubber/linux/kernel/cgroup/misc.c: 380
 * Return: /Users/rubber/linux/kernel/cgroup/misc.c: 381
 * * struct cgroup_subsys_state* - css of the allocated cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 382
 * * ERR_PTR(-ENOMEM) - No memory available to allocate. /Users/rubber/linux/kernel/cgroup/misc.c: 383
 * misc_cg_free() - Free the misc cgroup. /Users/rubber/linux/kernel/cgroup/misc.c: 408
 * @css: cgroup subsys object. /Users/rubber/linux/kernel/cgroup/misc.c: 409
 * Context: Any context. /Users/rubber/linux/kernel/cgroup/misc.c: 411
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1
 * pidlists linger the following amount before being destroyed.  The goal /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 22
 * is avoiding frequent destruction in the middle of consecutive read calls /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 23
 * Expiring in the middle is a performance problem not a correctness one. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 24
 * 1 sec should be enough. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 25
 * pidlist destructions need to be flushed on cgroup destruction.  Use a /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 36
 * separate workqueue as flush domain. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 37
 * cgroup_attach_task_all - attach task 'tsk' to all cgroups of task 'from' /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 50
 * @from: attach to all cgroups of a given task /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 51
 * @tsk: the task to be attached /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 52
 * Return: %0 on success or a negative errno code on failure /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 54
 * cgroup_transfer_tasks - move tasks from one cgroup to another /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 82
 * @to: cgroup to which the tasks will be moved /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 83
 * @from: cgroup in which the tasks currently reside /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 84
 * Locking rules between cgroup_post_fork() and the migration path /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 86
 * guarantee that, if a task is forking while being migrated, the new child /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 87
 * is guaranteed to be either visible in the source cgroup after the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 88
 * parent's migration is complete or put into the target cgroup.  No task /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 89
 * can slip out of migration through forking. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 90
 * Return: %0 on success or a negative errno code on failure /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 92
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 123
	 * Migrate tasks one-by-one until @from is empty.  This fails iff /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 124
	 * ->can_attach() fails. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 125
 * Stuff for reading the 'tasks'/'procs' files. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 153
 * Reading this file can return large amounts of data if a cgroup has /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 155
 * *lots* of attached tasks. So it may need several calls to read(), /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 156
 * but we cannot guarantee that the information we produce is correct /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 157
 * unless we produce it entirely atomically. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 158
 * A pidlist is a list of pids that virtually represents the contents of one /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 169
 * of the cgroup files ("procs" or "tasks"). We keep a list of such pidlists, /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 170
 * a pair (one each for procs, tasks) for each pid namespace that's relevant /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 171
 * to the cgroup. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 172
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 175
	 * used to find which pidlist is wanted. doesn't change as long as /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 176
	 * this particular list stays in the list. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 177
 * Used to destroy all pidlists lingering waiting for destroy timer.  None /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 193
 * should be left afterwards. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 194
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 218
	 * Destroy iff we didn't get queued again.  The state won't change /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 219
	 * as destroy_dwork can only be queued while locked. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 220
 * pidlist_uniq - given a kmalloc()ed list, strip out all duplicate entries /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 234
 * Returns the number of unique elements. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 235
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 241
	 * we presume the 0th element is unique, so i starts at 1. trivial /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 242
	 * edge cases first; no work needs to be done for either /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 243
 * The two pid files - task and cgroup.procs - guaranteed that the result /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 264
 * is sorted, which forced this whole pidlist fiasco.  As pid order is /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 265
 * different per namespace, each namespace needs differently sorted list, /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 266
 * making it impossible to use, for example, single rbtree of member tasks /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 267
 * sorted by task pointer.  As pidlists can be fairly large, allocating one /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 268
 * per open file is dangerous, so cgroup had to implement shared pool of /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 269
 * pidlists keyed by cgroup and namespace. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 270
 * find the appropriate pidlist for our purpose (given procs vs tasks) /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 293
 * returns with the lock on that pidlist already held, and takes care /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 294
 * of the use count, or returns NULL with no locks held if we're out of /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 295
 * memory. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 296
 * Load a cgroup's pidarray with either procs' tgids or tasks' pids /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 324
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 338
	 * If cgroup gets more users after we read count, we won't have /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 339
	 * enough space - tough.  This race is indistinguishable to the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 340
	 * caller from the case that the additional cgroup users didn't /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 341
	 * show up until sometime later on. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 342
 * seq_file methods for the tasks/procs files. The seq_file position is the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 383
 * next pid to display; the seq_file iterator is a pointer to the pid /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 384
 * in the cgroup->l->list array. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 385
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 390
	 * Initially we receive a position value that corresponds to /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 391
	 * one more than the last pid shown (or 0 on the first call or /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 392
	 * after a seek to the start). Use a binary-search to find the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 393
	 * next pid to display, if any /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 394
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 405
	 * !NULL @of->priv indicates that this isn't the first start() /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 406
	 * after open.  If the matching pidlist is around, we can use that. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 407
	 * Look for it.  Note that @of->priv can't be used directly.  It /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 408
	 * could already have been destroyed. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 409
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 414
	 * Either this is the first start() after open or the matching /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 415
	 * pidlist has been destroyed inbetween.  Create a new one. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 416
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 466
	 * Advance to the next pid in the array. If this goes off the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 467
	 * end, we're done /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 468
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 506
	 * Even if we're attaching all tasks in the thread group, we only /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 507
	 * need to check permissions on one of them. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 508
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 661
	 * Grab the subsystems state racily. No need to add avenue to /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 662
	 * cgroup_mutex contention. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 663
 * cgroupstats_build - build and fill cgroupstats /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 676
 * @stats: cgroupstats to fill information into /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 677
 * @dentry: A dentry entry belonging to the cgroup for which stats have /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 678
 * been requested. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 679
 * Build and fill cgroupstats so that taskstats can export it to user /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 681
 * space. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 682
 * Return: %0 on success or a negative errno code on failure /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 684
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 698
	 * We aren't being called from kernfs and there's no guarantee on /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 699
	 * @kn->priv's validity.  For this and css_tryget_online_from_dir(), /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 700
	 * @kn->priv is RCU safe.  Let's do the RCU dancing. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 701
 * Notify userspace when a cgroup is released, by running the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 746
 * configured release agent with the name of the cgroup (path /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 747
 * relative to the root of cgroup file system) as the argument. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 748
 * Most likely, this user command will try to rmdir this cgroup. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 750
 * This races with the possibility that some other task will be /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 752
 * attached to this cgroup before it is removed, or that some other /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 753
 * user task will 'mkdir' a child cgroup of this cgroup.  That's ok. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 754
 * The presumed 'rmdir' will fail quietly if this cgroup is no longer /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 755
 * unused, and this cgroup will be reprieved from its death sentence, /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 756
 * to continue to serve a useful existence.  Next time it's released, /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 757
 * we will get notified again, if it still has 'notify_on_release' set. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 758
 * The final arg to call_usermodehelper() is UMH_WAIT_EXEC, which /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 760
 * means only wait until the task is successfully execve()'d.  The /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 761
 * separate release agent task is forked by call_usermodehelper(), /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 762
 * then control in this thread returns here, without waiting for the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 763
 * release agent task.  We don't bother to wait because the caller of /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 764
 * this routine has no use for the exit status of the release agent /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 765
 * task, so no sense holding our caller up for that. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 766
 * cgroup_rename - Only allow simple rename of directories in place. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 812
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 829
	 * We're gonna grab cgroup_mutex which nests outside kernfs /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 830
	 * active_ref.  kernfs_rename() doesn't require active_ref /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 831
	 * protection.  Break them before grabbing cgroup_mutex. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 832
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1002
	 * In absence of 'none', 'name=' and subsystem name options, /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1003
	 * let's default to 'all'. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1004
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1017
	 * We either have to specify by name or by subsystems. (So all /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1018
	 * empty hierarchies must have a name). /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1019
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1024
	 * Option noprefix was introduced just for backward compatibility /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1025
	 * with the old cpuset, so we allow noprefix only if mounting just /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1026
	 * the cpuset subsystem. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1027
 * The guts of cgroup1 mount - find or create cgroup_root to use. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1104
 * Called with cgroup_mutex held; returns 0 on success, -E... on /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1105
 * error and positive - in case when the candidate is busy dying. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1106
 * On success it stashes a reference to cgroup_root into given /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1107
 * cgroup_fs_context; that reference is *NOT* counting towards the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1108
 * cgroup_root refcount. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1109
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1123
	 * Destruction of cgroup root is asynchronous, so subsystems may /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1124
	 * still be dying after the previous unmount.  Let's drain the /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1125
	 * dying subsystems.  We just need to ensure that the ones /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1126
	 * unmounted previously finish dying and don't care about new ones /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1127
	 * starting.  Testing ref liveliness is good enough. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1128
		/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1146
		 * If we asked for a name then it must match.  Also, if /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1147
		 * name matches but sybsys_mask doesn't, we should fail. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1148
		 * Remember whether name matched. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1149
		/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1157
		 * If we asked for subsystems (or explicitly for no /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1158
		 * subsystems) then they must match. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1159
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1175
	 * No such thing, create a new one.  name= matching without subsys /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1176
	 * specification is allowed for already existing hierarchies but we /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1177
	 * can't create new one without subsys specification. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1178
	/* /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1234
	 * Used to destroy pidlists and separate to serve as flush domain. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1235
	 * Cap @max_active to 1 too. /Users/rubber/linux/kernel/cgroup/cgroup-v1.c: 1236
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/cgroup/namespace.c: 1
 *  Generic process-grouping system. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2
 *  Based originally on the cpuset system, extracted by Paul Menage /Users/rubber/linux/kernel/cgroup/cgroup.c: 4
 *  Copyright (C) 2006 Google, Inc /Users/rubber/linux/kernel/cgroup/cgroup.c: 5
 *  Notifications support /Users/rubber/linux/kernel/cgroup/cgroup.c: 7
 *  Copyright (C) 2009 Nokia Corporation /Users/rubber/linux/kernel/cgroup/cgroup.c: 8
 *  Author: Kirill A. Shutemov /Users/rubber/linux/kernel/cgroup/cgroup.c: 9
 *  Copyright notices from the original cpuset code: /Users/rubber/linux/kernel/cgroup/cgroup.c: 11
 *  -------------------------------------------------- /Users/rubber/linux/kernel/cgroup/cgroup.c: 12
 *  Copyright (C) 2003 BULL SA. /Users/rubber/linux/kernel/cgroup/cgroup.c: 13
 *  Copyright (C) 2004-2006 Silicon Graphics, Inc. /Users/rubber/linux/kernel/cgroup/cgroup.c: 14
 *  Portions derived from Patrick Mochel's sysfs code. /Users/rubber/linux/kernel/cgroup/cgroup.c: 16
 *  sysfs is Copyright (c) 2001-3 Patrick Mochel /Users/rubber/linux/kernel/cgroup/cgroup.c: 17
 *  2003-10-10 Written by Simon Derr. /Users/rubber/linux/kernel/cgroup/cgroup.c: 19
 *  2003-10-22 Updates by Stephen Hemminger. /Users/rubber/linux/kernel/cgroup/cgroup.c: 20
 *  2004 May-July Rework by Paul Jackson. /Users/rubber/linux/kernel/cgroup/cgroup.c: 21
 *  --------------------------------------------------- /Users/rubber/linux/kernel/cgroup/cgroup.c: 22
 *  This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/cgroup/cgroup.c: 24
 *  License.  See the file COPYING in the main directory of the Linux /Users/rubber/linux/kernel/cgroup/cgroup.c: 25
 *  distribution for more details. /Users/rubber/linux/kernel/cgroup/cgroup.c: 26
 * To avoid confusing the compiler (and generating warnings) with code /Users/rubber/linux/kernel/cgroup/cgroup.c: 71
 * that attempts to access what would be a 0-element array (i.e. sized /Users/rubber/linux/kernel/cgroup/cgroup.c: 72
 * to a potentially empty array when CGROUP_SUBSYS_COUNT == 0), this /Users/rubber/linux/kernel/cgroup/cgroup.c: 73
 * constant expression can be added. /Users/rubber/linux/kernel/cgroup/cgroup.c: 74
 * cgroup_mutex is the master lock.  Any modification to cgroup or its /Users/rubber/linux/kernel/cgroup/cgroup.c: 79
 * hierarchy must be performed while holding it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 80
 * css_set_lock protects task->cgroups pointer, the list of css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 82
 * objects, and the chain of tasks off each css_set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 83
 * These locks are exported if CONFIG_PROVE_RCU so that accessors in /Users/rubber/linux/kernel/cgroup/cgroup.c: 85
 * cgroup.h can use them for lockdep annotations. /Users/rubber/linux/kernel/cgroup/cgroup.c: 86
 * Protects cgroup_idr and css_idr so that IDs can be released without /Users/rubber/linux/kernel/cgroup/cgroup.c: 101
 * grabbing cgroup_mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 102
 * Protects cgroup_file->kn for !self csses.  It synchronizes notifications /Users/rubber/linux/kernel/cgroup/cgroup.c: 107
 * against file removal/re-creation across css hiding. /Users/rubber/linux/kernel/cgroup/cgroup.c: 108
 * cgroup destruction makes heavy use of work items and there can be a lot /Users/rubber/linux/kernel/cgroup/cgroup.c: 120
 * of concurrent destructions.  Use a separate workqueue so that cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 121
 * destruction work items don't end up filling up max_active of system_wq /Users/rubber/linux/kernel/cgroup/cgroup.c: 122
 * which may lead to deadlock. /Users/rubber/linux/kernel/cgroup/cgroup.c: 123
 * The default hierarchy always exists but is hidden until mounted for the /Users/rubber/linux/kernel/cgroup/cgroup.c: 169
 * first time.  This is for backward compatibility. /Users/rubber/linux/kernel/cgroup/cgroup.c: 170
 * Assign a monotonically increasing serial number to csses.  It guarantees /Users/rubber/linux/kernel/cgroup/cgroup.c: 191
 * cgroups with bigger numbers are newer than those with smaller numbers. /Users/rubber/linux/kernel/cgroup/cgroup.c: 192
 * Also, as csses are always appended to the parent's ->children list, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 193
 * guarantees that sibling csses are always sorted in the ascending serial /Users/rubber/linux/kernel/cgroup/cgroup.c: 194
 * number order on the list.  Protected by cgroup_mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 195
 * These bitmasks identify subsystems with specific features to avoid /Users/rubber/linux/kernel/cgroup/cgroup.c: 200
 * having to do iterative checks repeatedly. /Users/rubber/linux/kernel/cgroup/cgroup.c: 201
 * cgroup_ssid_enabled - cgroup subsys enabled test by subsys ID /Users/rubber/linux/kernel/cgroup/cgroup.c: 250
 * @ssid: subsys ID of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 251
 * cgroup_subsys_enabled() can only be used with literal subsys names which /Users/rubber/linux/kernel/cgroup/cgroup.c: 253
 * is fine for individual subsystems but unsuitable for cgroup core.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 254
 * is slower static_key_enabled() based test indexed by @ssid. /Users/rubber/linux/kernel/cgroup/cgroup.c: 255
 * cgroup_on_dfl - test whether a cgroup is on the default hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 266
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 267
 * The default hierarchy is the v2 interface of cgroup and this function /Users/rubber/linux/kernel/cgroup/cgroup.c: 269
 * can be used to test whether a cgroup is on the default hierarchy for /Users/rubber/linux/kernel/cgroup/cgroup.c: 270
 * cases where a subsystem should behave differently depending on the /Users/rubber/linux/kernel/cgroup/cgroup.c: 271
 * interface version. /Users/rubber/linux/kernel/cgroup/cgroup.c: 272
 * List of changed behaviors: /Users/rubber/linux/kernel/cgroup/cgroup.c: 274
 * - Mount options "noprefix", "xattr", "clone_children", "release_agent" /Users/rubber/linux/kernel/cgroup/cgroup.c: 276
 *   and "name" are disallowed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 277
 * - When mounting an existing superblock, mount options should match. /Users/rubber/linux/kernel/cgroup/cgroup.c: 279
 * - Remount is disallowed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 281
 * - rename(2) is disallowed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 283
 * - "tasks" is removed.  Everything should be at process granularity.  Use /Users/rubber/linux/kernel/cgroup/cgroup.c: 285
 *   "cgroup.procs" instead. /Users/rubber/linux/kernel/cgroup/cgroup.c: 286
 * - "cgroup.procs" is not sorted.  pids will be unique unless they got /Users/rubber/linux/kernel/cgroup/cgroup.c: 288
 *   recycled in-between reads. /Users/rubber/linux/kernel/cgroup/cgroup.c: 289
 * - "release_agent" and "notify_on_release" are removed.  Replacement /Users/rubber/linux/kernel/cgroup/cgroup.c: 291
 *   notification mechanism will be implemented. /Users/rubber/linux/kernel/cgroup/cgroup.c: 292
 * - "cgroup.clone_children" is removed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 294
 * - "cgroup.subtree_populated" is available.  Its value is 0 if the cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 296
 *   and its descendants contain no task; otherwise, 1.  The file also /Users/rubber/linux/kernel/cgroup/cgroup.c: 297
 *   generates kernfs notification which can be monitored through poll and /Users/rubber/linux/kernel/cgroup/cgroup.c: 298
 *   [di]notify when the value of the file changes. /Users/rubber/linux/kernel/cgroup/cgroup.c: 299
 * - cpuset: tasks will be kept in empty cpusets when hotplug happens and /Users/rubber/linux/kernel/cgroup/cgroup.c: 301
 *   take masks of ancestors with non-empty cpus/mems, instead of being /Users/rubber/linux/kernel/cgroup/cgroup.c: 302
 *   moved to an ancestor. /Users/rubber/linux/kernel/cgroup/cgroup.c: 303
 * - cpuset: a task can be moved into an empty cpuset, and again it takes /Users/rubber/linux/kernel/cgroup/cgroup.c: 305
 *   masks of ancestors. /Users/rubber/linux/kernel/cgroup/cgroup.c: 306
 * - blkcg: blk-throttle becomes properly hierarchical. /Users/rubber/linux/kernel/cgroup/cgroup.c: 308
 * - debug: disallowed on the default hierarchy. /Users/rubber/linux/kernel/cgroup/cgroup.c: 310
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 361
	 * Root isn't under domain level resource control exempting it from /Users/rubber/linux/kernel/cgroup/cgroup.c: 362
	 * the no-internal-process constraint, so it can serve as a thread /Users/rubber/linux/kernel/cgroup/cgroup.c: 363
	 * root and a parent of resource domains at the same time. /Users/rubber/linux/kernel/cgroup/cgroup.c: 364
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 402
	 * A domain which has tasks and explicit threaded controllers /Users/rubber/linux/kernel/cgroup/cgroup.c: 403
	 * enabled is a thread root. /Users/rubber/linux/kernel/cgroup/cgroup.c: 404
 * cgroup_css - obtain a cgroup's css for the specified subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 470
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 471
 * @ss: the subsystem of interest (%NULL returns @cgrp->self) /Users/rubber/linux/kernel/cgroup/cgroup.c: 472
 * Return @cgrp's css (cgroup_subsys_state) associated with @ss.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 474
 * function must be called either under cgroup_mutex or rcu_read_lock() and /Users/rubber/linux/kernel/cgroup/cgroup.c: 475
 * the caller is responsible for pinning the returned css if it wants to /Users/rubber/linux/kernel/cgroup/cgroup.c: 476
 * keep accessing it outside the said locks.  This function may return /Users/rubber/linux/kernel/cgroup/cgroup.c: 477
 * %NULL if @cgrp doesn't have @subsys_id enabled. /Users/rubber/linux/kernel/cgroup/cgroup.c: 478
 * cgroup_tryget_css - try to get a cgroup's css for the specified subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 491
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 492
 * @ss: the subsystem of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 493
 * Find and get @cgrp's css associated with @ss.  If the css doesn't exist /Users/rubber/linux/kernel/cgroup/cgroup.c: 495
 * or is offline, %NULL is returned. /Users/rubber/linux/kernel/cgroup/cgroup.c: 496
 * cgroup_e_css_by_mask - obtain a cgroup's effective css for the specified ss /Users/rubber/linux/kernel/cgroup/cgroup.c: 513
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 514
 * @ss: the subsystem of interest (%NULL returns @cgrp->self) /Users/rubber/linux/kernel/cgroup/cgroup.c: 515
 * Similar to cgroup_css() but returns the effective css, which is defined /Users/rubber/linux/kernel/cgroup/cgroup.c: 517
 * as the matching css of the nearest ancestor including self which has @ss /Users/rubber/linux/kernel/cgroup/cgroup.c: 518
 * enabled.  If @ss is associated with the hierarchy @cgrp is on, this /Users/rubber/linux/kernel/cgroup/cgroup.c: 519
 * function is guaranteed to return non-NULL css. /Users/rubber/linux/kernel/cgroup/cgroup.c: 520
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 530
	 * This function is used while updating css associations and thus /Users/rubber/linux/kernel/cgroup/cgroup.c: 531
	 * can't test the csses directly.  Test ss_mask. /Users/rubber/linux/kernel/cgroup/cgroup.c: 532
 * cgroup_e_css - obtain a cgroup's effective css for the specified subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 544
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 545
 * @ss: the subsystem of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 546
 * Find and get the effective css of @cgrp for @ss.  The effective css is /Users/rubber/linux/kernel/cgroup/cgroup.c: 548
 * defined as the matching css of the nearest ancestor including self which /Users/rubber/linux/kernel/cgroup/cgroup.c: 549
 * has @ss enabled.  If @ss is not mounted on the hierarchy @cgrp is on, /Users/rubber/linux/kernel/cgroup/cgroup.c: 550
 * the root css is returned, so this function always returns a valid css. /Users/rubber/linux/kernel/cgroup/cgroup.c: 551
 * The returned css is not guaranteed to be online, and therefore it is the /Users/rubber/linux/kernel/cgroup/cgroup.c: 553
 * callers responsibility to try get a reference for it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 554
 * cgroup_get_e_css - get a cgroup's effective css for the specified subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 576
 * @cgrp: the cgroup of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 577
 * @ss: the subsystem of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 578
 * Find and get the effective css of @cgrp for @ss.  The effective css is /Users/rubber/linux/kernel/cgroup/cgroup.c: 580
 * defined as the matching css of the nearest ancestor including self which /Users/rubber/linux/kernel/cgroup/cgroup.c: 581
 * has @ss enabled.  If @ss is not mounted on the hierarchy @cgrp is on, /Users/rubber/linux/kernel/cgroup/cgroup.c: 582
 * the root css is returned, so this function always returns a valid css. /Users/rubber/linux/kernel/cgroup/cgroup.c: 583
 * The returned css must be put using css_put(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 584
 * __cgroup_task_count - count the number of tasks in a cgroup. The caller /Users/rubber/linux/kernel/cgroup/cgroup.c: 619
 * is responsible for taking the css_set_lock. /Users/rubber/linux/kernel/cgroup/cgroup.c: 620
 * @cgrp: the cgroup in question /Users/rubber/linux/kernel/cgroup/cgroup.c: 621
 * cgroup_task_count - count the number of tasks in a cgroup. /Users/rubber/linux/kernel/cgroup/cgroup.c: 637
 * @cgrp: the cgroup in question /Users/rubber/linux/kernel/cgroup/cgroup.c: 638
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 656
	 * This is open and unprotected implementation of cgroup_css(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 657
	 * seq_css() is only called from a kernfs file operation which has /Users/rubber/linux/kernel/cgroup/cgroup.c: 658
	 * an active reference on the file.  Because all the subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 659
	 * files are drained before a css is disassociated with a cgroup, /Users/rubber/linux/kernel/cgroup/cgroup.c: 660
	 * the matching css from the cgroup's subsys table is guaranteed to /Users/rubber/linux/kernel/cgroup/cgroup.c: 661
	 * be and stay valid until the enclosing operation is complete. /Users/rubber/linux/kernel/cgroup/cgroup.c: 662
 * for_each_css - iterate all css's of a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 672
 * @css: the iteration cursor /Users/rubber/linux/kernel/cgroup/cgroup.c: 673
 * @ssid: the index of the subsystem, CGROUP_SUBSYS_COUNT after reaching the end /Users/rubber/linux/kernel/cgroup/cgroup.c: 674
 * @cgrp: the target cgroup to iterate css's of /Users/rubber/linux/kernel/cgroup/cgroup.c: 675
 * Should be called under cgroup_[tree_]mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 677
 * for_each_e_css - iterate all effective css's of a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 687
 * @css: the iteration cursor /Users/rubber/linux/kernel/cgroup/cgroup.c: 688
 * @ssid: the index of the subsystem, CGROUP_SUBSYS_COUNT after reaching the end /Users/rubber/linux/kernel/cgroup/cgroup.c: 689
 * @cgrp: the target cgroup to iterate css's of /Users/rubber/linux/kernel/cgroup/cgroup.c: 690
 * Should be called under cgroup_[tree_]mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 692
 * do_each_subsys_mask - filter for_each_subsys with a bitmask /Users/rubber/linux/kernel/cgroup/cgroup.c: 702
 * @ss: the iteration cursor /Users/rubber/linux/kernel/cgroup/cgroup.c: 703
 * @ssid: the index of @ss, CGROUP_SUBSYS_COUNT after reaching the end /Users/rubber/linux/kernel/cgroup/cgroup.c: 704
 * @ss_mask: the bitmask /Users/rubber/linux/kernel/cgroup/cgroup.c: 705
 * The block will only run for cases where the ssid-th bit (1 << ssid) of /Users/rubber/linux/kernel/cgroup/cgroup.c: 707
 * @ss_mask is set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 708
 * The default css_set - used by init and its children prior to any /Users/rubber/linux/kernel/cgroup/cgroup.c: 752
 * hierarchies being mounted. It contains a pointer to the root state /Users/rubber/linux/kernel/cgroup/cgroup.c: 753
 * for each subsystem. Also used to anchor the list of css_sets. Not /Users/rubber/linux/kernel/cgroup/cgroup.c: 754
 * reference-counted, to improve performance when child cgroups /Users/rubber/linux/kernel/cgroup/cgroup.c: 755
 * haven't been created. /Users/rubber/linux/kernel/cgroup/cgroup.c: 756
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 770
	 * The following field is re-initialized when this cset gets linked /Users/rubber/linux/kernel/cgroup/cgroup.c: 771
	 * in cgroup_init().  However, let's initialize the field /Users/rubber/linux/kernel/cgroup/cgroup.c: 772
	 * statically too so that the default cgroup can be accessed safely /Users/rubber/linux/kernel/cgroup/cgroup.c: 773
	 * early during boot. /Users/rubber/linux/kernel/cgroup/cgroup.c: 774
 * css_set_populated - does a css_set contain any tasks? /Users/rubber/linux/kernel/cgroup/cgroup.c: 787
 * @cset: target css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 788
 * css_set_populated() should be the same as !!cset->nr_tasks at steady /Users/rubber/linux/kernel/cgroup/cgroup.c: 790
 * state. However, css_set_populated() can be called while a task is being /Users/rubber/linux/kernel/cgroup/cgroup.c: 791
 * added to or removed from the linked list before the nr_tasks is /Users/rubber/linux/kernel/cgroup/cgroup.c: 792
 * properly updated. Hence, we can't just look at ->nr_tasks here. /Users/rubber/linux/kernel/cgroup/cgroup.c: 793
 * cgroup_update_populated - update the populated count of a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 803
 * @cgrp: the target cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 804
 * @populated: inc or dec populated count /Users/rubber/linux/kernel/cgroup/cgroup.c: 805
 * One of the css_sets associated with @cgrp is either getting its first /Users/rubber/linux/kernel/cgroup/cgroup.c: 807
 * task or losing the last.  Update @cgrp->nr_populated_* accordingly.  The /Users/rubber/linux/kernel/cgroup/cgroup.c: 808
 * count is propagated towards root so that a given cgroup's /Users/rubber/linux/kernel/cgroup/cgroup.c: 809
 * nr_populated_children is zero iff none of its descendants contain any /Users/rubber/linux/kernel/cgroup/cgroup.c: 810
 * tasks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 811
 * @cgrp's interface file "cgroup.populated" is zero if both /Users/rubber/linux/kernel/cgroup/cgroup.c: 813
 * @cgrp->nr_populated_csets and @cgrp->nr_populated_children are zero and /Users/rubber/linux/kernel/cgroup/cgroup.c: 814
 * 1 otherwise.  When the sum changes from or to zero, userland is notified /Users/rubber/linux/kernel/cgroup/cgroup.c: 815
 * that the content of the interface file has changed.  This can be used to /Users/rubber/linux/kernel/cgroup/cgroup.c: 816
 * detect when @cgrp and its descendants become populated or empty. /Users/rubber/linux/kernel/cgroup/cgroup.c: 817
 * css_set_update_populated - update populated state of a css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 852
 * @cset: target css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 853
 * @populated: whether @cset is populated or depopulated /Users/rubber/linux/kernel/cgroup/cgroup.c: 854
 * @cset is either getting the first task or losing the last.  Update the /Users/rubber/linux/kernel/cgroup/cgroup.c: 856
 * populated counters of all associated cgroups accordingly. /Users/rubber/linux/kernel/cgroup/cgroup.c: 857
 * @task is leaving, advance task iterators which are pointing to it so /Users/rubber/linux/kernel/cgroup/cgroup.c: 870
 * that they can resume at the next position.  Advancing an iterator might /Users/rubber/linux/kernel/cgroup/cgroup.c: 871
 * remove it from the list, use safe walk.  See css_task_iter_skip() for /Users/rubber/linux/kernel/cgroup/cgroup.c: 872
 * details. /Users/rubber/linux/kernel/cgroup/cgroup.c: 873
 * css_set_move_task - move a task from one css_set to another /Users/rubber/linux/kernel/cgroup/cgroup.c: 885
 * @task: task being moved /Users/rubber/linux/kernel/cgroup/cgroup.c: 886
 * @from_cset: css_set @task currently belongs to (may be NULL) /Users/rubber/linux/kernel/cgroup/cgroup.c: 887
 * @to_cset: new css_set @task is being moved to (may be NULL) /Users/rubber/linux/kernel/cgroup/cgroup.c: 888
 * @use_mg_tasks: move to @to_cset->mg_tasks instead of ->tasks /Users/rubber/linux/kernel/cgroup/cgroup.c: 889
 * Move @task from @from_cset to @to_cset.  If @task didn't belong to any /Users/rubber/linux/kernel/cgroup/cgroup.c: 891
 * css_set, @from_cset can be NULL.  If @task is being disassociated /Users/rubber/linux/kernel/cgroup/cgroup.c: 892
 * instead of moved, @to_cset can be NULL. /Users/rubber/linux/kernel/cgroup/cgroup.c: 893
 * This function automatically handles populated counter updates and /Users/rubber/linux/kernel/cgroup/cgroup.c: 895
 * css_task_iter adjustments but the caller is responsible for managing /Users/rubber/linux/kernel/cgroup/cgroup.c: 896
 * @from_cset and @to_cset's reference counts. /Users/rubber/linux/kernel/cgroup/cgroup.c: 897
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 920
		 * We are synchronized through cgroup_threadgroup_rwsem /Users/rubber/linux/kernel/cgroup/cgroup.c: 921
		 * against PF_EXITING setting such that we can't race /Users/rubber/linux/kernel/cgroup/cgroup.c: 922
		 * against cgroup_exit()/cgroup_free() dropping the css_set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 923
 * hash table for cgroup groups. This improves the performance to find /Users/rubber/linux/kernel/cgroup/cgroup.c: 934
 * an existing css_set. This hash doesn't (currently) take into /Users/rubber/linux/kernel/cgroup/cgroup.c: 935
 * account cgroups in empty hierarchies. /Users/rubber/linux/kernel/cgroup/cgroup.c: 936
 * compare_css_sets - helper function for find_existing_css_set(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 992
 * @cset: candidate css_set being tested /Users/rubber/linux/kernel/cgroup/cgroup.c: 993
 * @old_cset: existing css_set for a task /Users/rubber/linux/kernel/cgroup/cgroup.c: 994
 * @new_cgrp: cgroup that's being entered by the task /Users/rubber/linux/kernel/cgroup/cgroup.c: 995
 * @template: desired set of css pointers in css_set (pre-calculated) /Users/rubber/linux/kernel/cgroup/cgroup.c: 996
 * Returns true if "cset" matches "old_cset" except for the hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 998
 * which "new_cgrp" belongs to, for which it should match "new_cgrp". /Users/rubber/linux/kernel/cgroup/cgroup.c: 999
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1009
	 * On the default hierarchy, there can be csets which are /Users/rubber/linux/kernel/cgroup/cgroup.c: 1010
	 * associated with the same set of cgroups but different csses. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1011
	 * Let's first ensure that csses match. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1012
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1027
	 * Compare cgroup pointers in order to distinguish between /Users/rubber/linux/kernel/cgroup/cgroup.c: 1028
	 * different cgroups in hierarchies.  As different cgroups may /Users/rubber/linux/kernel/cgroup/cgroup.c: 1029
	 * share the same effective css, this comparison is always /Users/rubber/linux/kernel/cgroup/cgroup.c: 1030
	 * necessary. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1031
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1056
		 * If this hierarchy is the hierarchy of the cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1057
		 * that's changing, then we need to check that this /Users/rubber/linux/kernel/cgroup/cgroup.c: 1058
		 * css_set points to the new cgroup; if it's any other /Users/rubber/linux/kernel/cgroup/cgroup.c: 1059
		 * hierarchy, then this css_set should point to the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1060
		 * same cgroup as the old css_set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1061
 * find_existing_css_set - init css array and find the matching css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 1075
 * @old_cset: the css_set that we're using before the cgroup transition /Users/rubber/linux/kernel/cgroup/cgroup.c: 1076
 * @cgrp: the cgroup that we're moving into /Users/rubber/linux/kernel/cgroup/cgroup.c: 1077
 * @template: out param for the new set of csses, should be clear on entry /Users/rubber/linux/kernel/cgroup/cgroup.c: 1078
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1090
	 * Build the set of subsystem state objects that we want to see in the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1091
	 * new css_set. While subsystems can change globally, the entries here /Users/rubber/linux/kernel/cgroup/cgroup.c: 1092
	 * won't change, so no need for locking. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1093
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1097
			 * @ss is in this hierarchy, so we want the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1098
			 * effective css from @cgrp. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1099
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1103
			 * @ss is not in this hierarchy, so we don't want /Users/rubber/linux/kernel/cgroup/cgroup.c: 1104
			 * to change the css. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1105
 * allocate_cgrp_cset_links - allocate cgrp_cset_links /Users/rubber/linux/kernel/cgroup/cgroup.c: 1135
 * @count: the number of links to allocate /Users/rubber/linux/kernel/cgroup/cgroup.c: 1136
 * @tmp_links: list_head the allocated links are put on /Users/rubber/linux/kernel/cgroup/cgroup.c: 1137
 * Allocate @count cgrp_cset_link structures and chain them on @tmp_links /Users/rubber/linux/kernel/cgroup/cgroup.c: 1139
 * through ->cset_link.  Returns 0 on success or -errno. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1140
 * link_css_set - a helper function to link a css_set to a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1161
 * @tmp_links: cgrp_cset_link objects allocated by allocate_cgrp_cset_links() /Users/rubber/linux/kernel/cgroup/cgroup.c: 1162
 * @cset: the css_set to be linked /Users/rubber/linux/kernel/cgroup/cgroup.c: 1163
 * @cgrp: the destination cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1164
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1180
	 * Always add links to the tail of the lists so that the lists are /Users/rubber/linux/kernel/cgroup/cgroup.c: 1181
	 * in chronological order. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1182
 * find_css_set - return a new css_set with one cgroup updated /Users/rubber/linux/kernel/cgroup/cgroup.c: 1192
 * @old_cset: the baseline css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 1193
 * @cgrp: the cgroup to be updated /Users/rubber/linux/kernel/cgroup/cgroup.c: 1194
 * Return a new css_set that's equivalent to @old_cset, but with @cgrp /Users/rubber/linux/kernel/cgroup/cgroup.c: 1196
 * substituted into the appropriate hierarchy. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1197
	/* First see if we already have a cgroup group that matches /Users/rubber/linux/kernel/cgroup/cgroup.c: 1212
	/* Copy the set of subsystem state objects generated in /Users/rubber/linux/kernel/cgroup/cgroup.c: 1245
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1277
	 * If @cset should be threaded, look up the matching dom_cset and /Users/rubber/linux/kernel/cgroup/cgroup.c: 1278
	 * link them up.  We first fully initialize @cset then look for the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1279
	 * dom_cset.  It's simpler this way and safe as @cset is guaranteed /Users/rubber/linux/kernel/cgroup/cgroup.c: 1280
	 * to stay empty until we return. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1281
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1350
	 * Release all the links from cset_links to this hierarchy's /Users/rubber/linux/kernel/cgroup/cgroup.c: 1351
	 * root cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1352
 * look up cgroup associated with current task's cgroup namespace on the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1379
 * specified hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 1380
 * Return the cgroup for "task" from the given hierarchy. Must be /Users/rubber/linux/kernel/cgroup/cgroup.c: 1446
 * called with cgroup_mutex and css_set_lock held. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1447
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1452
	 * No need to lock the task - since we hold css_set_lock the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1453
	 * task can't change groups. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1454
 * A task must hold cgroup_mutex to modify cgroups. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1460
 * Any task can increment and decrement the count field without lock. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1462
 * So in general, code holding cgroup_mutex can't rely on the count /Users/rubber/linux/kernel/cgroup/cgroup.c: 1463
 * field not changing.  However, if the count goes to zero, then only /Users/rubber/linux/kernel/cgroup/cgroup.c: 1464
 * cgroup_attach_task() can increment it again.  Because a count of zero /Users/rubber/linux/kernel/cgroup/cgroup.c: 1465
 * means that no tasks are currently attached, therefore there is no /Users/rubber/linux/kernel/cgroup/cgroup.c: 1466
 * way a task attached to that cgroup can fork (the other way to /Users/rubber/linux/kernel/cgroup/cgroup.c: 1467
 * increment the count).  So code holding cgroup_mutex can safely /Users/rubber/linux/kernel/cgroup/cgroup.c: 1468
 * assume that if the count is zero, it will stay zero. Similarly, if /Users/rubber/linux/kernel/cgroup/cgroup.c: 1469
 * a task holds cgroup_mutex on a cgroup with zero count, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 1470
 * knows that the cgroup won't be removed, as cgroup_rmdir() /Users/rubber/linux/kernel/cgroup/cgroup.c: 1471
 * needs that mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1472
 * A cgroup can only be deleted if both its 'count' of using tasks /Users/rubber/linux/kernel/cgroup/cgroup.c: 1474
 * is zero, and its list of 'children' cgroups is empty.  Since all /Users/rubber/linux/kernel/cgroup/cgroup.c: 1475
 * tasks in the system use _some_ cgroup, and since there is always at /Users/rubber/linux/kernel/cgroup/cgroup.c: 1476
 * least one task in the system (init, pid == 1), therefore, root cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1477
 * always has either children cgroups and/or using tasks.  So we don't /Users/rubber/linux/kernel/cgroup/cgroup.c: 1478
 * need a special hack to ensure that root cgroup cannot be deleted. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1479
 * P.S.  One more locking exception.  RCU is used to guard the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1481
 * update of a tasks cgroup pointer by cgroup_attach_task() /Users/rubber/linux/kernel/cgroup/cgroup.c: 1482
 * cgroup_file_mode - deduce file mode of a control file /Users/rubber/linux/kernel/cgroup/cgroup.c: 1506
 * @cft: the control file in question /Users/rubber/linux/kernel/cgroup/cgroup.c: 1507
 * S_IRUGO for read, S_IWUSR for write. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1509
 * cgroup_calc_subtree_ss_mask - calculate subtree_ss_mask /Users/rubber/linux/kernel/cgroup/cgroup.c: 1529
 * @subtree_control: the new subtree_control mask to consider /Users/rubber/linux/kernel/cgroup/cgroup.c: 1530
 * @this_ss_mask: available subsystems /Users/rubber/linux/kernel/cgroup/cgroup.c: 1531
 * On the default hierarchy, a subsystem may request other subsystems to be /Users/rubber/linux/kernel/cgroup/cgroup.c: 1533
 * enabled together through its ->depends_on mask.  In such cases, more /Users/rubber/linux/kernel/cgroup/cgroup.c: 1534
 * subsystems than specified in "cgroup.subtree_control" may be enabled. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1535
 * This function calculates which subsystems need to be enabled if /Users/rubber/linux/kernel/cgroup/cgroup.c: 1537
 * @subtree_control is to be applied while restricted to @this_ss_mask. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1538
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1557
		 * Mask out subsystems which aren't available.  This can /Users/rubber/linux/kernel/cgroup/cgroup.c: 1558
		 * happen only if some depended-upon subsystems were bound /Users/rubber/linux/kernel/cgroup/cgroup.c: 1559
		 * to non-default hierarchies. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1560
 * cgroup_kn_unlock - unlocking helper for cgroup kernfs methods /Users/rubber/linux/kernel/cgroup/cgroup.c: 1573
 * @kn: the kernfs_node being serviced /Users/rubber/linux/kernel/cgroup/cgroup.c: 1574
 * This helper undoes cgroup_kn_lock_live() and should be invoked before /Users/rubber/linux/kernel/cgroup/cgroup.c: 1576
 * the method finishes if locking succeeded.  Note that once this function /Users/rubber/linux/kernel/cgroup/cgroup.c: 1577
 * returns the cgroup returned by cgroup_kn_lock_live() may become /Users/rubber/linux/kernel/cgroup/cgroup.c: 1578
 * inaccessible any time.  If the caller intends to continue to access the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1579
 * cgroup, it should pin it before invoking this function. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1580
 * cgroup_kn_lock_live - locking helper for cgroup kernfs methods /Users/rubber/linux/kernel/cgroup/cgroup.c: 1598
 * @kn: the kernfs_node being serviced /Users/rubber/linux/kernel/cgroup/cgroup.c: 1599
 * @drain_offline: perform offline draining on the cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1600
 * This helper is to be used by a cgroup kernfs method currently servicing /Users/rubber/linux/kernel/cgroup/cgroup.c: 1602
 * @kn.  It breaks the active protection, performs cgroup locking and /Users/rubber/linux/kernel/cgroup/cgroup.c: 1603
 * verifies that the associated cgroup is alive.  Returns the cgroup if /Users/rubber/linux/kernel/cgroup/cgroup.c: 1604
 * alive; otherwise, %NULL.  A successful return should be undone by a /Users/rubber/linux/kernel/cgroup/cgroup.c: 1605
 * matching cgroup_kn_unlock() invocation.  If @drain_offline is %true, the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1606
 * cgroup is drained of offlining csses before return. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1607
 * Any cgroup kernfs method implementation which requires locking the /Users/rubber/linux/kernel/cgroup/cgroup.c: 1609
 * associated cgroup should use this helper.  It avoids nesting cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 1610
 * locking under kernfs active protection and allows all kernfs operations /Users/rubber/linux/kernel/cgroup/cgroup.c: 1611
 * including self-removal. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1612
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1623
	 * We're gonna grab cgroup_mutex which nests outside kernfs /Users/rubber/linux/kernel/cgroup/cgroup.c: 1624
	 * active_ref.  cgroup liveliness check alone provides enough /Users/rubber/linux/kernel/cgroup/cgroup.c: 1625
	 * protection against removal.  Ensure @cgrp stays accessible and /Users/rubber/linux/kernel/cgroup/cgroup.c: 1626
	 * break the active_ref protection. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1627
 * css_clear_dir - remove subsys files in a cgroup directory /Users/rubber/linux/kernel/cgroup/cgroup.c: 1666
 * @css: target css /Users/rubber/linux/kernel/cgroup/cgroup.c: 1667
 * css_populate_dir - create subsys files in a cgroup directory /Users/rubber/linux/kernel/cgroup/cgroup.c: 1693
 * @css: target css /Users/rubber/linux/kernel/cgroup/cgroup.c: 1694
 * On failure, no file is added. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1696
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1748
		 * If @ss has non-root csses attached to it, can't move. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1749
		 * If @ss is an implicit controller, it is exempt from this /Users/rubber/linux/kernel/cgroup/cgroup.c: 1750
		 * rule and can be stolen. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1751
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1761
		 * Collect ssid's that need to be disabled from default /Users/rubber/linux/kernel/cgroup/cgroup.c: 1762
		 * hierarchy. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1763
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 1773
		 * Controllers from default hierarchy that need to be rebound /Users/rubber/linux/kernel/cgroup/cgroup.c: 1774
		 * are all disabled together in one go. /Users/rubber/linux/kernel/cgroup/cgroup.c: 1775
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2000
	 * We're accessing css_set_count without locking css_set_lock here, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2001
	 * but that's OK - it can only be increased by someone holding /Users/rubber/linux/kernel/cgroup/cgroup.c: 2002
	 * cgroup_lock, and that's us.  Later rebinding may disable /Users/rubber/linux/kernel/cgroup/cgroup.c: 2003
	 * controllers on the default hierarchy and thus create new csets, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2004
	 * which can't be more than the existing ones.  Allocate 2x. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2005
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2048
	 * There must be no failure case after here, since rebinding takes /Users/rubber/linux/kernel/cgroup/cgroup.c: 2049
	 * care of subsystems' refcounts, which are explicitly dropped in /Users/rubber/linux/kernel/cgroup/cgroup.c: 2050
	 * the failure exit path. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2051
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2056
	 * Link the root cgroup in this hierarchy into all the css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 2057
	 * objects. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2058
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2100
	 * In non-init cgroup namespace, instead of root cgroup's dentry, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2101
	 * we return the dentry corresponding to the cgroupns->root_cgrp. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2102
 * Destroy a cgroup filesystem context. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2134
 * Initialise the cgroup filesystem creation/reconfiguration context.  Notably, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2177
 * we select the namespace we're going to use. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2178
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2206
	 * If @root doesn't have any children, start killing it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2207
	 * This prevents new mounts by disabling percpu_ref_tryget_live(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 2208
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 2209
	 * And don't kill the default root. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2210
 * This is ugly, but preserves the userspace API for existing cpuset /Users/rubber/linux/kernel/cgroup/cgroup.c: 2244
 * users. If someone tries to mount the "cpuset" filesystem, we /Users/rubber/linux/kernel/cgroup/cgroup.c: 2245
 * silently switch it to mount "cgroup" instead /Users/rubber/linux/kernel/cgroup/cgroup.c: 2246
 * task_cgroup_path - cgroup path of a task in the first cgroup hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 2307
 * @task: target task /Users/rubber/linux/kernel/cgroup/cgroup.c: 2308
 * @buf: the buffer to write the path into /Users/rubber/linux/kernel/cgroup/cgroup.c: 2309
 * @buflen: the length of the buffer /Users/rubber/linux/kernel/cgroup/cgroup.c: 2310
 * Determine @task's cgroup on the first (the one with the lowest non-zero /Users/rubber/linux/kernel/cgroup/cgroup.c: 2312
 * hierarchy_id) cgroup hierarchy and copy its path into @buf.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 2313
 * function grabs cgroup_mutex and shouldn't be used inside locks used by /Users/rubber/linux/kernel/cgroup/cgroup.c: 2314
 * cgroup controller callbacks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2315
 * Return value is the same as kernfs_path(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 2317
 * cgroup_migrate_add_task - add a migration target task to a migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2346
 * @task: target task /Users/rubber/linux/kernel/cgroup/cgroup.c: 2347
 * @mgctx: target migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2348
 * Add @task, which is a migration target, to @mgctx->tset.  This function /Users/rubber/linux/kernel/cgroup/cgroup.c: 2350
 * becomes noop if @task doesn't need to be migrated.  @task's css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 2351
 * should have been added as a migration source and @task->cg_list will be /Users/rubber/linux/kernel/cgroup/cgroup.c: 2352
 * moved from the css_set's tasks list to mg_tasks one. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2353
 * cgroup_taskset_first - reset taskset and return the first task /Users/rubber/linux/kernel/cgroup/cgroup.c: 2385
 * @tset: taskset of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 2386
 * @dst_cssp: output variable for the destination css /Users/rubber/linux/kernel/cgroup/cgroup.c: 2387
 * @tset iteration is initialized and the first task is returned. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2389
 * cgroup_taskset_next - iterate to the next task in taskset /Users/rubber/linux/kernel/cgroup/cgroup.c: 2401
 * @tset: taskset of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 2402
 * @dst_cssp: output variable for the destination css /Users/rubber/linux/kernel/cgroup/cgroup.c: 2403
 * Return the next task in @tset.  Iteration must have been initialized /Users/rubber/linux/kernel/cgroup/cgroup.c: 2405
 * with cgroup_taskset_first(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 2406
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2425
			 * This function may be called both before and /Users/rubber/linux/kernel/cgroup/cgroup.c: 2426
			 * after cgroup_taskset_migrate().  The two cases /Users/rubber/linux/kernel/cgroup/cgroup.c: 2427
			 * can be distinguished by looking at whether @cset /Users/rubber/linux/kernel/cgroup/cgroup.c: 2428
			 * has its ->mg_dst_cset set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2429
 * cgroup_migrate_execute - migrate a taskset /Users/rubber/linux/kernel/cgroup/cgroup.c: 2447
 * @mgctx: migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2448
 * Migrate tasks in @mgctx as setup by migration preparation functions. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2450
 * This function fails iff one of the ->can_attach callbacks fails and /Users/rubber/linux/kernel/cgroup/cgroup.c: 2451
 * guarantees that either all or none of the tasks in @mgctx are migrated. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2452
 * @mgctx is consumed regardless of success. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2453
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2477
	 * Now that we're guaranteed success, proceed to move all tasks to /Users/rubber/linux/kernel/cgroup/cgroup.c: 2478
	 * the new cgroup.  There are no failure cases after here, so this /Users/rubber/linux/kernel/cgroup/cgroup.c: 2479
	 * is the commit point. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2480
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2492
			 * If the source or destination cgroup is frozen, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2493
			 * the task might require to change its state. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2494
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2504
	 * Migration is committed, all target tasks are now on dst_csets. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2505
	 * Nothing is sensitive to fork() after this point.  Notify /Users/rubber/linux/kernel/cgroup/cgroup.c: 2506
	 * controllers that migration is complete. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2507
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2543
	 * Re-initialize the cgroup_taskset structure in case it is reused /Users/rubber/linux/kernel/cgroup/cgroup.c: 2544
	 * again in another cgroup_migrate_add_task()/cgroup_migrate_execute() /Users/rubber/linux/kernel/cgroup/cgroup.c: 2545
	 * iteration. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2546
 * cgroup_migrate_vet_dst - verify whether a cgroup can be migration destination /Users/rubber/linux/kernel/cgroup/cgroup.c: 2554
 * @dst_cgrp: destination cgroup to test /Users/rubber/linux/kernel/cgroup/cgroup.c: 2555
 * On the default hierarchy, except for the mixable, (possible) thread root /Users/rubber/linux/kernel/cgroup/cgroup.c: 2557
 * and threaded cgroups, subtree_control must be zero for migration /Users/rubber/linux/kernel/cgroup/cgroup.c: 2558
 * destination cgroups with tasks so that child cgroups don't compete /Users/rubber/linux/kernel/cgroup/cgroup.c: 2559
 * against tasks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2560
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2576
	 * If @dst_cgrp is already or can become a thread root or is /Users/rubber/linux/kernel/cgroup/cgroup.c: 2577
	 * threaded, it doesn't matter. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2578
 * cgroup_migrate_finish - cleanup after attach /Users/rubber/linux/kernel/cgroup/cgroup.c: 2591
 * @mgctx: migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2592
 * Undo cgroup_migrate_add_src() and cgroup_migrate_prepare_dst().  See /Users/rubber/linux/kernel/cgroup/cgroup.c: 2594
 * those functions for details. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2595
 * cgroup_migrate_add_src - add a migration source css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 2621
 * @src_cset: the source css_set to add /Users/rubber/linux/kernel/cgroup/cgroup.c: 2622
 * @dst_cgrp: the destination cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 2623
 * @mgctx: migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2624
 * Tasks belonging to @src_cset are about to be migrated to @dst_cgrp.  Pin /Users/rubber/linux/kernel/cgroup/cgroup.c: 2626
 * @src_cset and add it to @mgctx->src_csets, which should later be cleaned /Users/rubber/linux/kernel/cgroup/cgroup.c: 2627
 * up by cgroup_migrate_finish(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 2628
 * This function may be called without holding cgroup_threadgroup_rwsem /Users/rubber/linux/kernel/cgroup/cgroup.c: 2630
 * even if the target is a process.  Threads may be created and destroyed /Users/rubber/linux/kernel/cgroup/cgroup.c: 2631
 * but as long as cgroup_mutex is not dropped, no new css_set can be put /Users/rubber/linux/kernel/cgroup/cgroup.c: 2632
 * into play and the preloaded css_sets are guaranteed to cover all /Users/rubber/linux/kernel/cgroup/cgroup.c: 2633
 * migrations. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2634
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2645
	 * If ->dead, @src_set is associated with one or more dead cgroups /Users/rubber/linux/kernel/cgroup/cgroup.c: 2646
	 * and doesn't contain any migratable tasks.  Ignore it early so /Users/rubber/linux/kernel/cgroup/cgroup.c: 2647
	 * that the rest of migration path doesn't get confused by it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2648
 * cgroup_migrate_prepare_dst - prepare destination css_sets for migration /Users/rubber/linux/kernel/cgroup/cgroup.c: 2670
 * @mgctx: migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2671
 * Tasks are about to be moved and all the source css_sets have been /Users/rubber/linux/kernel/cgroup/cgroup.c: 2673
 * preloaded to @mgctx->preloaded_src_csets.  This function looks up and /Users/rubber/linux/kernel/cgroup/cgroup.c: 2674
 * pins all destination css_sets, links each to its source, and append them /Users/rubber/linux/kernel/cgroup/cgroup.c: 2675
 * to @mgctx->preloaded_dst_csets. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2676
 * This function must be called after cgroup_migrate_add_src() has been /Users/rubber/linux/kernel/cgroup/cgroup.c: 2678
 * called on each migration source css_set.  After migration is performed /Users/rubber/linux/kernel/cgroup/cgroup.c: 2679
 * using cgroup_migrate(), cgroup_migrate_finish() must be called on /Users/rubber/linux/kernel/cgroup/cgroup.c: 2680
 * @mgctx. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2681
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2702
		 * If src cset equals dst, it's noop.  Drop the src. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2703
		 * cgroup_migrate() will skip the cset too.  Note that we /Users/rubber/linux/kernel/cgroup/cgroup.c: 2704
		 * can't handle src == dst as some nodes are used by both. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2705
 * cgroup_migrate - migrate a process or task to a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 2733
 * @leader: the leader of the process or the task to migrate /Users/rubber/linux/kernel/cgroup/cgroup.c: 2734
 * @threadgroup: whether @leader points to the whole process or a single task /Users/rubber/linux/kernel/cgroup/cgroup.c: 2735
 * @mgctx: migration context /Users/rubber/linux/kernel/cgroup/cgroup.c: 2736
 * Migrate a process or task denoted by @leader.  If migrating a process, /Users/rubber/linux/kernel/cgroup/cgroup.c: 2738
 * the caller must be holding cgroup_threadgroup_rwsem.  The caller is also /Users/rubber/linux/kernel/cgroup/cgroup.c: 2739
 * responsible for invoking cgroup_migrate_add_src() and /Users/rubber/linux/kernel/cgroup/cgroup.c: 2740
 * cgroup_migrate_prepare_dst() on the targets before invoking this /Users/rubber/linux/kernel/cgroup/cgroup.c: 2741
 * function and following up with cgroup_migrate_finish(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 2742
 * As long as a controller's ->can_attach() doesn't fail, this function is /Users/rubber/linux/kernel/cgroup/cgroup.c: 2744
 * guaranteed to succeed.  This means that, excluding ->can_attach() /Users/rubber/linux/kernel/cgroup/cgroup.c: 2745
 * failure, when migrating multiple targets, the success or failure can be /Users/rubber/linux/kernel/cgroup/cgroup.c: 2746
 * decided for all targets by invoking group_migrate_prepare_dst() before /Users/rubber/linux/kernel/cgroup/cgroup.c: 2747
 * actually starting migrating. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2748
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2755
	 * Prevent freeing of tasks while we take a snapshot. Tasks that are /Users/rubber/linux/kernel/cgroup/cgroup.c: 2756
	 * already PF_EXITING could be freed from underneath us unless we /Users/rubber/linux/kernel/cgroup/cgroup.c: 2757
	 * take an rcu_read_lock. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2758
 * cgroup_attach_task - attach a task or a whole threadgroup to a cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 2775
 * @dst_cgrp: the cgroup to attach to /Users/rubber/linux/kernel/cgroup/cgroup.c: 2776
 * @leader: the task or the leader of the threadgroup to be attached /Users/rubber/linux/kernel/cgroup/cgroup.c: 2777
 * @threadgroup: attach the whole threadgroup? /Users/rubber/linux/kernel/cgroup/cgroup.c: 2778
 * Call holding cgroup_mutex and cgroup_threadgroup_rwsem. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2780
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2824
	 * If we migrate a single thread, we don't care about threadgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 2825
	 * stability. If the thread is `current`, it won't exit(2) under our /Users/rubber/linux/kernel/cgroup/cgroup.c: 2826
	 * hands or change PID through exec(2). We exclude /Users/rubber/linux/kernel/cgroup/cgroup.c: 2827
	 * cgroup_update_dfl_csses and other cgroup_{proc,thread}s_write /Users/rubber/linux/kernel/cgroup/cgroup.c: 2828
	 * callers by cgroup_mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2829
	 * Therefore, we can skip the global lock. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2830
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 2854
	 * kthreads may acquire PF_NO_SETAFFINITY during initialization. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2855
	 * If userland migrates such a kthread to a non-root cgroup, it can /Users/rubber/linux/kernel/cgroup/cgroup.c: 2856
	 * become trapped in a cpuset, or RT kthread may be born in a /Users/rubber/linux/kernel/cgroup/cgroup.c: 2857
	 * cgroup with no rt_runtime allocated.  Just say no. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2858
 * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 2929
 * @cgrp: root of the subtree to update csses for /Users/rubber/linux/kernel/cgroup/cgroup.c: 2930
 * @cgrp's control masks have changed and its subtree's css associations /Users/rubber/linux/kernel/cgroup/cgroup.c: 2932
 * need to be updated accordingly.  This function looks up all css_sets /Users/rubber/linux/kernel/cgroup/cgroup.c: 2933
 * which are attached to the subtree, creates the matching updated css_sets /Users/rubber/linux/kernel/cgroup/cgroup.c: 2934
 * and migrates the tasks to the new ones. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2935
 * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses /Users/rubber/linux/kernel/cgroup/cgroup.c: 2982
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 2983
 * Because css offlining is asynchronous, userland may try to re-enable a /Users/rubber/linux/kernel/cgroup/cgroup.c: 2985
 * controller while the previous css is still around.  This function grabs /Users/rubber/linux/kernel/cgroup/cgroup.c: 2986
 * cgroup_mutex and drains the previous css instances of @cgrp's subtree. /Users/rubber/linux/kernel/cgroup/cgroup.c: 2987
 * cgroup_save_control - save control masks and dom_cgrp of a subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3023
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3024
 * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the /Users/rubber/linux/kernel/cgroup/cgroup.c: 3026
 * respective old_ prefixed fields for @cgrp's subtree including @cgrp /Users/rubber/linux/kernel/cgroup/cgroup.c: 3027
 * itself. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3028
 * cgroup_propagate_control - refresh control masks of a subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3043
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3044
 * For @cgrp and its subtree, ensure ->subtree_ss_mask matches /Users/rubber/linux/kernel/cgroup/cgroup.c: 3046
 * ->subtree_control and propagate controller availability through the /Users/rubber/linux/kernel/cgroup/cgroup.c: 3047
 * subtree so that descendants don't have unavailable controllers enabled. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3048
 * cgroup_restore_control - restore control masks and dom_cgrp of a subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3064
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3065
 * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the /Users/rubber/linux/kernel/cgroup/cgroup.c: 3067
 * respective old_ prefixed fields for @cgrp's subtree including @cgrp /Users/rubber/linux/kernel/cgroup/cgroup.c: 3068
 * itself. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3069
 * cgroup_apply_control_enable - enable or show csses according to control /Users/rubber/linux/kernel/cgroup/cgroup.c: 3096
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3097
 * Walk @cgrp's subtree and create new csses or make the existing ones /Users/rubber/linux/kernel/cgroup/cgroup.c: 3099
 * visible.  A css is created invisible if it's being implicitly enabled /Users/rubber/linux/kernel/cgroup/cgroup.c: 3100
 * through dependency.  An invisible css is made visible when the userland /Users/rubber/linux/kernel/cgroup/cgroup.c: 3101
 * explicitly enables it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3102
 * Returns 0 on success, -errno on failure.  On failure, csses which have /Users/rubber/linux/kernel/cgroup/cgroup.c: 3104
 * been processed already aren't cleaned up.  The caller is responsible for /Users/rubber/linux/kernel/cgroup/cgroup.c: 3105
 * cleaning up with cgroup_apply_control_disable(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 3106
 * cgroup_apply_control_disable - kill or hide csses according to control /Users/rubber/linux/kernel/cgroup/cgroup.c: 3142
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3143
 * Walk @cgrp's subtree and kill and hide csses so that they match /Users/rubber/linux/kernel/cgroup/cgroup.c: 3145
 * cgroup_ss_mask() and cgroup_visible_mask(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 3146
 * A css is hidden when the userland requests it to be disabled while other /Users/rubber/linux/kernel/cgroup/cgroup.c: 3148
 * subsystems are still depending on it.  The css must not actively control /Users/rubber/linux/kernel/cgroup/cgroup.c: 3149
 * resources and be in the vanilla state if it's made visible again later. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3150
 * Controllers which may be depended upon should provide ->css_reset() for /Users/rubber/linux/kernel/cgroup/cgroup.c: 3151
 * this purpose. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3152
 * cgroup_apply_control - apply control mask updates to the subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3183
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3184
 * subsystems can be enabled and disabled in a subtree using the following /Users/rubber/linux/kernel/cgroup/cgroup.c: 3186
 * steps. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3187
 * 1. Call cgroup_save_control() to stash the current state. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3189
 * 2. Update ->subtree_control masks in the subtree as desired. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3190
 * 3. Call cgroup_apply_control() to apply the changes. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3191
 * 4. Optionally perform other related operations. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3192
 * 5. Call cgroup_finalize_control() to finish up. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3193
 * This function implements step 3 and propagates the mask changes /Users/rubber/linux/kernel/cgroup/cgroup.c: 3195
 * throughout @cgrp's subtree, updates csses accordingly and perform /Users/rubber/linux/kernel/cgroup/cgroup.c: 3196
 * process migrations. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3197
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3209
	 * At this point, cgroup_e_css_by_mask() results reflect the new csses /Users/rubber/linux/kernel/cgroup/cgroup.c: 3210
	 * making the following cgroup_update_dfl_csses() properly update /Users/rubber/linux/kernel/cgroup/cgroup.c: 3211
	 * css associations of all tasks in the subtree. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3212
 * cgroup_finalize_control - finalize control mask update /Users/rubber/linux/kernel/cgroup/cgroup.c: 3222
 * @cgrp: root of the target subtree /Users/rubber/linux/kernel/cgroup/cgroup.c: 3223
 * @ret: the result of the update /Users/rubber/linux/kernel/cgroup/cgroup.c: 3224
 * Finalize control mask update.  See cgroup_apply_control() for more info. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3226
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3259
		 * Threaded controllers can handle internal competitions /Users/rubber/linux/kernel/cgroup/cgroup.c: 3260
		 * and are always allowed inside a (prospective) thread /Users/rubber/linux/kernel/cgroup/cgroup.c: 3261
		 * subtree. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3262
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3268
	 * Controllers can't be enabled for a cgroup with tasks to avoid /Users/rubber/linux/kernel/cgroup/cgroup.c: 3269
	 * child cgroups competing against tasks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3270
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3289
	 * Parse input - space separated list of subsystem names prefixed /Users/rubber/linux/kernel/cgroup/cgroup.c: 3290
	 * with either + or -. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3291
 * cgroup_enable_threaded - make @cgrp threaded /Users/rubber/linux/kernel/cgroup/cgroup.c: 3375
 * @cgrp: the target cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 3376
 * Called when "threaded" is written to the cgroup.type interface file and /Users/rubber/linux/kernel/cgroup/cgroup.c: 3378
 * tries to make @cgrp threaded and join the parent's resource domain. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3379
 * This function is never called on the root cgroup as cgroup.type doesn't /Users/rubber/linux/kernel/cgroup/cgroup.c: 3380
 * exist on it. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3381
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3397
	 * If @cgroup is populated or has domain controllers enabled, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 3398
	 * can't be switched.  While the below cgroup_can_be_thread_root() /Users/rubber/linux/kernel/cgroup/cgroup.c: 3399
	 * test can catch the same conditions, that's only when @parent is /Users/rubber/linux/kernel/cgroup/cgroup.c: 3400
	 * not mixable, so let's check it explicitly. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3401
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3412
	 * The following shouldn't cause actual migrations and should /Users/rubber/linux/kernel/cgroup/cgroup.c: 3413
	 * always succeed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3414
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3796
	 * Killing is a process directed operation, i.e. the whole thread-group /Users/rubber/linux/kernel/cgroup/cgroup.c: 3797
	 * is taken down so act like we do for cgroup.procs and only make this /Users/rubber/linux/kernel/cgroup/cgroup.c: 3798
	 * writable in non-threaded cgroups. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3799
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3840
	 * If namespaces are delegation boundaries, disallow writes to /Users/rubber/linux/kernel/cgroup/cgroup.c: 3841
	 * files in an non-init namespace root from inside the namespace /Users/rubber/linux/kernel/cgroup/cgroup.c: 3842
	 * except for the files explicitly marked delegatable - /Users/rubber/linux/kernel/cgroup/cgroup.c: 3843
	 * cgroup.procs and cgroup.subtree_control. /Users/rubber/linux/kernel/cgroup/cgroup.c: 3844
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 3854
	 * kernfs guarantees that a file isn't deleted with operations in /Users/rubber/linux/kernel/cgroup/cgroup.c: 3855
	 * flight, which means that the matching css is and stays alive and /Users/rubber/linux/kernel/cgroup/cgroup.c: 3856
	 * doesn't need to be pinned.  The RCU locking is not necessary /Users/rubber/linux/kernel/cgroup/cgroup.c: 3857
	 * either.  It's just for the convenience of using cgroup_css(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 3858
 * cgroup_addrm_files - add or remove files to a cgroup directory /Users/rubber/linux/kernel/cgroup/cgroup.c: 4004
 * @css: the target css /Users/rubber/linux/kernel/cgroup/cgroup.c: 4005
 * @cgrp: the target cgroup (usually css->cgroup) /Users/rubber/linux/kernel/cgroup/cgroup.c: 4006
 * @cfts: array of cftypes to be added /Users/rubber/linux/kernel/cgroup/cgroup.c: 4007
 * @is_add: whether to add or remove /Users/rubber/linux/kernel/cgroup/cgroup.c: 4008
 * Depending on @is_add, add or remove files defined by @cfts on @cgrp. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4010
 * For removals, this function never fails. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4011
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4112
		 * Ugh... if @cft wants a custom max_write_len, we need to /Users/rubber/linux/kernel/cgroup/cgroup.c: 4113
		 * make a copy of kf_ops to set its atomic_write_len. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4114
 * cgroup_rm_cftypes - remove an array of cftypes from a subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 4146
 * @cfts: zero-length name terminated array of cftypes /Users/rubber/linux/kernel/cgroup/cgroup.c: 4147
 * Unregister @cfts.  Files described by @cfts are removed from all /Users/rubber/linux/kernel/cgroup/cgroup.c: 4149
 * existing cgroups and all future cgroups won't have them either.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 4150
 * function can be called anytime whether @cfts' subsys is attached or not. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4151
 * Returns 0 on successful unregistration, -ENOENT if @cfts is not /Users/rubber/linux/kernel/cgroup/cgroup.c: 4153
 * registered. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4154
 * cgroup_add_cftypes - add an array of cftypes to a subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 4167
 * @ss: target cgroup subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 4168
 * @cfts: zero-length name terminated array of cftypes /Users/rubber/linux/kernel/cgroup/cgroup.c: 4169
 * Register @cfts to @ss.  Files described by @cfts are created for all /Users/rubber/linux/kernel/cgroup/cgroup.c: 4171
 * existing cgroups to which @ss is attached and all future cgroups will /Users/rubber/linux/kernel/cgroup/cgroup.c: 4172
 * have them too.  This function can be called anytime whether @ss is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4173
 * attached or not. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4174
 * Returns 0 on successful registration, -errno on failure.  Note that this /Users/rubber/linux/kernel/cgroup/cgroup.c: 4176
 * function currently returns 0 as long as @cfts registration is successful /Users/rubber/linux/kernel/cgroup/cgroup.c: 4177
 * even if some file creation attempts on existing cgroups fail. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4178
 * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 4206
 * @ss: target cgroup subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 4207
 * @cfts: zero-length name terminated array of cftypes /Users/rubber/linux/kernel/cgroup/cgroup.c: 4208
 * Similar to cgroup_add_cftypes() but the added files are only used for /Users/rubber/linux/kernel/cgroup/cgroup.c: 4210
 * the default hierarchy. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4211
 * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies /Users/rubber/linux/kernel/cgroup/cgroup.c: 4223
 * @ss: target cgroup subsystem /Users/rubber/linux/kernel/cgroup/cgroup.c: 4224
 * @cfts: zero-length name terminated array of cftypes /Users/rubber/linux/kernel/cgroup/cgroup.c: 4225
 * Similar to cgroup_add_cftypes() but the added files are only used for /Users/rubber/linux/kernel/cgroup/cgroup.c: 4227
 * the legacy hierarchies. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4228
 * cgroup_file_notify - generate a file modified event for a cgroup_file /Users/rubber/linux/kernel/cgroup/cgroup.c: 4240
 * @cfile: target cgroup_file /Users/rubber/linux/kernel/cgroup/cgroup.c: 4241
 * @cfile must have been obtained by setting cftype->file_offset. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4243
 * css_next_child - find the next child of a given css /Users/rubber/linux/kernel/cgroup/cgroup.c: 4265
 * @pos: the current position (%NULL to initiate traversal) /Users/rubber/linux/kernel/cgroup/cgroup.c: 4266
 * @parent: css whose children to walk /Users/rubber/linux/kernel/cgroup/cgroup.c: 4267
 * This function returns the next child of @parent and should be called /Users/rubber/linux/kernel/cgroup/cgroup.c: 4269
 * under either cgroup_mutex or RCU read lock.  The only requirement is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4270
 * that @parent and @pos are accessible.  The next sibling is guaranteed to /Users/rubber/linux/kernel/cgroup/cgroup.c: 4271
 * be returned regardless of their states. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4272
 * If a subsystem synchronizes ->css_online() and the start of iteration, a /Users/rubber/linux/kernel/cgroup/cgroup.c: 4274
 * css which finished ->css_online() is guaranteed to be visible in the /Users/rubber/linux/kernel/cgroup/cgroup.c: 4275
 * future iterations and will stay visible until the last reference is put. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4276
 * A css which hasn't finished ->css_online() or already finished /Users/rubber/linux/kernel/cgroup/cgroup.c: 4277
 * ->css_offline() may show up during traversal.  It's each subsystem's /Users/rubber/linux/kernel/cgroup/cgroup.c: 4278
 * responsibility to synchronize against on/offlining. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4279
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4288
	 * @pos could already have been unlinked from the sibling list. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4289
	 * Once a cgroup is removed, its ->sibling.next is no longer /Users/rubber/linux/kernel/cgroup/cgroup.c: 4290
	 * updated when its next sibling changes.  CSS_RELEASED is set when /Users/rubber/linux/kernel/cgroup/cgroup.c: 4291
	 * @pos is taken off list, at which time its next pointer is valid, /Users/rubber/linux/kernel/cgroup/cgroup.c: 4292
	 * and, as releases are serialized, the one pointed to by the next /Users/rubber/linux/kernel/cgroup/cgroup.c: 4293
	 * pointer is guaranteed to not have started release yet.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 4294
	 * implies that if we observe !CSS_RELEASED on @pos in this RCU /Users/rubber/linux/kernel/cgroup/cgroup.c: 4295
	 * critical section, the one pointed to by its next pointer is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4296
	 * guaranteed to not have finished its RCU grace period even if we /Users/rubber/linux/kernel/cgroup/cgroup.c: 4297
	 * have dropped rcu_read_lock() in-between iterations. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4298
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 4299
	 * If @pos has CSS_RELEASED set, its next pointer can't be /Users/rubber/linux/kernel/cgroup/cgroup.c: 4300
	 * dereferenced; however, as each css is given a monotonically /Users/rubber/linux/kernel/cgroup/cgroup.c: 4301
	 * increasing unique serial number and always appended to the /Users/rubber/linux/kernel/cgroup/cgroup.c: 4302
	 * sibling list, the next one can be found by walking the parent's /Users/rubber/linux/kernel/cgroup/cgroup.c: 4303
	 * children until the first css with higher serial number than /Users/rubber/linux/kernel/cgroup/cgroup.c: 4304
	 * @pos's.  While this path can be slower, it happens iff iteration /Users/rubber/linux/kernel/cgroup/cgroup.c: 4305
	 * races against release and the race window is very small. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4306
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4319
	 * @next, if not pointing to the head, can be dereferenced and is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4320
	 * the next sibling. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4321
 * css_next_descendant_pre - find the next descendant for pre-order walk /Users/rubber/linux/kernel/cgroup/cgroup.c: 4329
 * @pos: the current position (%NULL to initiate traversal) /Users/rubber/linux/kernel/cgroup/cgroup.c: 4330
 * @root: css whose descendants to walk /Users/rubber/linux/kernel/cgroup/cgroup.c: 4331
 * To be used by css_for_each_descendant_pre().  Find the next descendant /Users/rubber/linux/kernel/cgroup/cgroup.c: 4333
 * to visit for pre-order traversal of @root's descendants.  @root is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4334
 * included in the iteration and the first node to be visited. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4335
 * While this function requires cgroup_mutex or RCU read locking, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 4337
 * doesn't require the whole traversal to be contained in a single critical /Users/rubber/linux/kernel/cgroup/cgroup.c: 4338
 * section.  This function will return the correct next descendant as long /Users/rubber/linux/kernel/cgroup/cgroup.c: 4339
 * as both @pos and @root are accessible and @pos is a descendant of @root. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4340
 * If a subsystem synchronizes ->css_online() and the start of iteration, a /Users/rubber/linux/kernel/cgroup/cgroup.c: 4342
 * css which finished ->css_online() is guaranteed to be visible in the /Users/rubber/linux/kernel/cgroup/cgroup.c: 4343
 * future iterations and will stay visible until the last reference is put. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4344
 * A css which hasn't finished ->css_online() or already finished /Users/rubber/linux/kernel/cgroup/cgroup.c: 4345
 * ->css_offline() may show up during traversal.  It's each subsystem's /Users/rubber/linux/kernel/cgroup/cgroup.c: 4346
 * responsibility to synchronize against on/offlining. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4347
 * css_rightmost_descendant - return the rightmost descendant of a css /Users/rubber/linux/kernel/cgroup/cgroup.c: 4379
 * @pos: css of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 4380
 * Return the rightmost descendant of @pos.  If there's no descendant, @pos /Users/rubber/linux/kernel/cgroup/cgroup.c: 4382
 * is returned.  This can be used during pre-order traversal to skip /Users/rubber/linux/kernel/cgroup/cgroup.c: 4383
 * subtree of @pos. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4384
 * While this function requires cgroup_mutex or RCU read locking, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 4386
 * doesn't require the whole traversal to be contained in a single critical /Users/rubber/linux/kernel/cgroup/cgroup.c: 4387
 * section.  This function will return the correct rightmost descendant as /Users/rubber/linux/kernel/cgroup/cgroup.c: 4388
 * long as @pos is accessible. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4389
 * css_next_descendant_post - find the next descendant for post-order walk /Users/rubber/linux/kernel/cgroup/cgroup.c: 4423
 * @pos: the current position (%NULL to initiate traversal) /Users/rubber/linux/kernel/cgroup/cgroup.c: 4424
 * @root: css whose descendants to walk /Users/rubber/linux/kernel/cgroup/cgroup.c: 4425
 * To be used by css_for_each_descendant_post().  Find the next descendant /Users/rubber/linux/kernel/cgroup/cgroup.c: 4427
 * to visit for post-order traversal of @root's descendants.  @root is /Users/rubber/linux/kernel/cgroup/cgroup.c: 4428
 * included in the iteration and the last node to be visited. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4429
 * While this function requires cgroup_mutex or RCU read locking, it /Users/rubber/linux/kernel/cgroup/cgroup.c: 4431
 * doesn't require the whole traversal to be contained in a single critical /Users/rubber/linux/kernel/cgroup/cgroup.c: 4432
 * section.  This function will return the correct next descendant as long /Users/rubber/linux/kernel/cgroup/cgroup.c: 4433
 * as both @pos and @cgroup are accessible and @pos is a descendant of /Users/rubber/linux/kernel/cgroup/cgroup.c: 4434
 * @cgroup. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4435
 * If a subsystem synchronizes ->css_online() and the start of iteration, a /Users/rubber/linux/kernel/cgroup/cgroup.c: 4437
 * css which finished ->css_online() is guaranteed to be visible in the /Users/rubber/linux/kernel/cgroup/cgroup.c: 4438
 * future iterations and will stay visible until the last reference is put. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4439
 * A css which hasn't finished ->css_online() or already finished /Users/rubber/linux/kernel/cgroup/cgroup.c: 4440
 * ->css_offline() may show up during traversal.  It's each subsystem's /Users/rubber/linux/kernel/cgroup/cgroup.c: 4441
 * responsibility to synchronize against on/offlining. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4442
 * css_has_online_children - does a css have online children /Users/rubber/linux/kernel/cgroup/cgroup.c: 4470
 * @css: the target css /Users/rubber/linux/kernel/cgroup/cgroup.c: 4471
 * Returns %true if @css has any online children; otherwise, %false.  This /Users/rubber/linux/kernel/cgroup/cgroup.c: 4473
 * function can be called from any context but the caller is responsible /Users/rubber/linux/kernel/cgroup/cgroup.c: 4474
 * for synchronizing against on/offlining as necessary. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4475
 * css_task_iter_advance_css_set - advance a task iterator to the next css_set /Users/rubber/linux/kernel/cgroup/cgroup.c: 4546
 * @it: the iterator to advance /Users/rubber/linux/kernel/cgroup/cgroup.c: 4547
 * Advance @it to the next css_set to walk. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4549
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4576
	 * We don't keep css_sets locked across iteration steps and thus /Users/rubber/linux/kernel/cgroup/cgroup.c: 4577
	 * need to take steps to ensure that iteration can be resumed after /Users/rubber/linux/kernel/cgroup/cgroup.c: 4578
	 * the lock is re-acquired.  Iteration is performed at two levels - /Users/rubber/linux/kernel/cgroup/cgroup.c: 4579
	 * css_sets and tasks in them. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4580
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 4581
	 * Once created, a css_set never leaves its cgroup lists, so a /Users/rubber/linux/kernel/cgroup/cgroup.c: 4582
	 * pinned css_set is guaranteed to stay put and we can resume /Users/rubber/linux/kernel/cgroup/cgroup.c: 4583
	 * iteration afterwards. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4584
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 4585
	 * Tasks may leave @cset across iteration steps.  This is resolved /Users/rubber/linux/kernel/cgroup/cgroup.c: 4586
	 * by registering each iterator with the css_set currently being /Users/rubber/linux/kernel/cgroup/cgroup.c: 4587
	 * walked and making css_set_move_task() advance iterators whose /Users/rubber/linux/kernel/cgroup/cgroup.c: 4588
	 * next task is leaving. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4589
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4618
		 * Advance iterator to find next entry. We go through cset /Users/rubber/linux/kernel/cgroup/cgroup.c: 4619
		 * tasks, mg_tasks and dying_tasks, when consumed we move onto /Users/rubber/linux/kernel/cgroup/cgroup.c: 4620
		 * the next cset. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4621
 * css_task_iter_start - initiate task iteration /Users/rubber/linux/kernel/cgroup/cgroup.c: 4665
 * @css: the css to walk tasks of /Users/rubber/linux/kernel/cgroup/cgroup.c: 4666
 * @flags: CSS_TASK_ITER_* flags /Users/rubber/linux/kernel/cgroup/cgroup.c: 4667
 * @it: the task iterator to use /Users/rubber/linux/kernel/cgroup/cgroup.c: 4668
 * Initiate iteration through the tasks of @css.  The caller can call /Users/rubber/linux/kernel/cgroup/cgroup.c: 4670
 * css_task_iter_next() to walk through the tasks until the function /Users/rubber/linux/kernel/cgroup/cgroup.c: 4671
 * returns NULL.  On completion of iteration, css_task_iter_end() must be /Users/rubber/linux/kernel/cgroup/cgroup.c: 4672
 * called. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4673
 * css_task_iter_next - return the next task for the iterator /Users/rubber/linux/kernel/cgroup/cgroup.c: 4698
 * @it: the task iterator being iterated /Users/rubber/linux/kernel/cgroup/cgroup.c: 4699
 * The "next" function for task iteration.  @it should have been /Users/rubber/linux/kernel/cgroup/cgroup.c: 4701
 * initialized via css_task_iter_start().  Returns NULL when the iteration /Users/rubber/linux/kernel/cgroup/cgroup.c: 4702
 * reaches the end. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4703
 * css_task_iter_end - finish task iteration /Users/rubber/linux/kernel/cgroup/cgroup.c: 4731
 * @it: the task iterator to finish /Users/rubber/linux/kernel/cgroup/cgroup.c: 4732
 * Finish task iteration started by css_task_iter_start(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 4734
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4778
	 * When a seq_file is seeked, it's always traversed sequentially /Users/rubber/linux/kernel/cgroup/cgroup.c: 4779
	 * from position 0, so we can simply keep iterating on !0 *pos. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4780
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4804
	 * All processes of a threaded subtree belong to the domain cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 4805
	 * of the subtree.  Only threads can be distributed across the /Users/rubber/linux/kernel/cgroup/cgroup.c: 4806
	 * subtree.  Reject reads on cgroup.procs in the subtree proper. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4807
	 * They're always empty anyway. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4808
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 4858
	 * If namespaces are delegation boundaries, %current must be able /Users/rubber/linux/kernel/cgroup/cgroup.c: 4859
	 * to see both source and destination cgroups from its namespace. /Users/rubber/linux/kernel/cgroup/cgroup.c: 4860
 * css destruction is four-stage process. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5047
 * 1. Destruction starts.  Killing of the percpu_ref is initiated. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5049
 *    Implemented in kill_css(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5050
 * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs /Users/rubber/linux/kernel/cgroup/cgroup.c: 5052
 *    and thus css_tryget_online() is guaranteed to fail, the css can be /Users/rubber/linux/kernel/cgroup/cgroup.c: 5053
 *    offlined by invoking offline_css().  After offlining, the base ref is /Users/rubber/linux/kernel/cgroup/cgroup.c: 5054
 *    put.  Implemented in css_killed_work_fn(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5055
 * 3. When the percpu_ref reaches zero, the only possible remaining /Users/rubber/linux/kernel/cgroup/cgroup.c: 5057
 *    accessors are inside RCU read sections.  css_release() schedules the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5058
 *    RCU callback. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5059
 * 4. After the grace period, the css can be freed.  Implemented in /Users/rubber/linux/kernel/cgroup/cgroup.c: 5061
 *    css_free_work_fn(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5062
 * It is actually hairier because both step 2 and 4 require process context /Users/rubber/linux/kernel/cgroup/cgroup.c: 5064
 * and thus involve punting to css->destroy_work adding two additional /Users/rubber/linux/kernel/cgroup/cgroup.c: 5065
 * steps to the already complex sequence. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5066
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5095
			 * We get a ref to the parent, and put the ref when /Users/rubber/linux/kernel/cgroup/cgroup.c: 5096
			 * this cgroup is being freed, so it's guaranteed /Users/rubber/linux/kernel/cgroup/cgroup.c: 5097
			 * that the parent won't be destroyed before its /Users/rubber/linux/kernel/cgroup/cgroup.c: 5098
			 * children. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5099
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5107
			 * This is root cgroup's refcnt reaching zero, /Users/rubber/linux/kernel/cgroup/cgroup.c: 5108
			 * which indicates that the root should be /Users/rubber/linux/kernel/cgroup/cgroup.c: 5109
			 * released. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5110
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5153
		 * There are two control paths which try to determine /Users/rubber/linux/kernel/cgroup/cgroup.c: 5154
		 * cgroup from dentry without going through kernfs - /Users/rubber/linux/kernel/cgroup/cgroup.c: 5155
		 * cgroupstats_build() and css_tryget_online_from_dir(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5156
		 * Those are supported by RCU protecting clearing of /Users/rubber/linux/kernel/cgroup/cgroup.c: 5157
		 * cgrp->kn->priv backpointer. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5158
 * css_create - create a cgroup_subsys_state /Users/rubber/linux/kernel/cgroup/cgroup.c: 5249
 * @cgrp: the cgroup new css will be associated with /Users/rubber/linux/kernel/cgroup/cgroup.c: 5250
 * @ss: the subsys of new css /Users/rubber/linux/kernel/cgroup/cgroup.c: 5251
 * Create a new css associated with @cgrp - @ss pair.  On success, the new /Users/rubber/linux/kernel/cgroup/cgroup.c: 5253
 * css is online and installed in @cgrp.  This function doesn't create the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5254
 * interface files.  Returns 0 on success, -errno on failure. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5255
 * The returned cgroup is fully initialized including its control mask, but /Users/rubber/linux/kernel/cgroup/cgroup.c: 5304
 * it isn't associated with its kernfs_node and doesn't have the control /Users/rubber/linux/kernel/cgroup/cgroup.c: 5305
 * mask applied. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5306
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5353
	 * New cgroup inherits effective freeze counter, and /Users/rubber/linux/kernel/cgroup/cgroup.c: 5354
	 * if the parent has to be frozen, the child has too. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5355
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5359
		 * Set the CGRP_FREEZE flag, so when a process will be /Users/rubber/linux/kernel/cgroup/cgroup.c: 5360
		 * attached to the child cgroup, it will become frozen. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5361
		 * At this point the new cgroup is unpopulated, so we can /Users/rubber/linux/kernel/cgroup/cgroup.c: 5362
		 * consider it frozen immediately. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5363
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5376
			 * If the new cgroup is frozen, all ancestor cgroups /Users/rubber/linux/kernel/cgroup/cgroup.c: 5377
			 * get a new frozen descendant, but their state can't /Users/rubber/linux/kernel/cgroup/cgroup.c: 5378
			 * change because of this. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5379
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5400
	 * On the default hierarchy, a child doesn't automatically inherit /Users/rubber/linux/kernel/cgroup/cgroup.c: 5401
	 * subtree_control from the parent.  Each is configured manually. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5402
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5471
	 * This extra ref will be put in cgroup_free_fn() and guarantees /Users/rubber/linux/kernel/cgroup/cgroup.c: 5472
	 * that @cgrp->kn is always accessible. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5473
 * This is called when the refcnt of a css is confirmed to be killed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5505
 * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to /Users/rubber/linux/kernel/cgroup/cgroup.c: 5506
 * initiate destruction and put the css ref from kill_css(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5507
 * kill_css - destroy a css /Users/rubber/linux/kernel/cgroup/cgroup.c: 5539
 * @css: css to destroy /Users/rubber/linux/kernel/cgroup/cgroup.c: 5540
 * This function initiates destruction of @css by removing cgroup interface /Users/rubber/linux/kernel/cgroup/cgroup.c: 5542
 * files and putting its base reference.  ->css_offline() will be invoked /Users/rubber/linux/kernel/cgroup/cgroup.c: 5543
 * asynchronously once css_tryget_online() is guaranteed to fail and when /Users/rubber/linux/kernel/cgroup/cgroup.c: 5544
 * the reference count reaches zero, @css will be released. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5545
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5556
	 * This must happen before css is disassociated with its cgroup. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5557
	 * See seq_css() for details. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5558
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5562
	 * Killing would put the base ref, but we need to keep it alive /Users/rubber/linux/kernel/cgroup/cgroup.c: 5563
	 * until after ->css_offline(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 5564
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5568
	 * cgroup core guarantees that, by the time ->css_offline() is /Users/rubber/linux/kernel/cgroup/cgroup.c: 5569
	 * invoked, no new css reference will be given out via /Users/rubber/linux/kernel/cgroup/cgroup.c: 5570
	 * css_tryget_online().  We can't simply call percpu_ref_kill() and /Users/rubber/linux/kernel/cgroup/cgroup.c: 5571
	 * proceed to offlining css's because percpu_ref_kill() doesn't /Users/rubber/linux/kernel/cgroup/cgroup.c: 5572
	 * guarantee that the ref is seen as killed on all CPUs on return. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5573
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 5574
	 * Use percpu_ref_kill_and_confirm() to get notifications as each /Users/rubber/linux/kernel/cgroup/cgroup.c: 5575
	 * css is confirmed to be seen as killed on all CPUs. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5576
 * cgroup_destroy_locked - the first stage of cgroup destruction /Users/rubber/linux/kernel/cgroup/cgroup.c: 5582
 * @cgrp: cgroup to be destroyed /Users/rubber/linux/kernel/cgroup/cgroup.c: 5583
 * css's make use of percpu refcnts whose killing latency shouldn't be /Users/rubber/linux/kernel/cgroup/cgroup.c: 5585
 * exposed to userland and are RCU protected.  Also, cgroup core needs to /Users/rubber/linux/kernel/cgroup/cgroup.c: 5586
 * guarantee that css_tryget_online() won't succeed by the time /Users/rubber/linux/kernel/cgroup/cgroup.c: 5587
 * ->css_offline() is invoked.  To satisfy all the requirements, /Users/rubber/linux/kernel/cgroup/cgroup.c: 5588
 * destruction is implemented in the following two steps. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5589
 * s1. Verify @cgrp can be destroyed and mark it dying.  Remove all /Users/rubber/linux/kernel/cgroup/cgroup.c: 5591
 *     userland visible parts and start killing the percpu refcnts of /Users/rubber/linux/kernel/cgroup/cgroup.c: 5592
 *     css's.  Set up so that the next stage will be kicked off once all /Users/rubber/linux/kernel/cgroup/cgroup.c: 5593
 *     the percpu refcnts are confirmed to be killed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5594
 * s2. Invoke ->css_offline(), mark the cgroup dead and proceed with the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5596
 *     rest of destruction.  Once all cgroup references are gone, the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5597
 *     cgroup is RCU-freed. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5598
 * This function implements s1.  After this step, @cgrp is gone as far as /Users/rubber/linux/kernel/cgroup/cgroup.c: 5600
 * the userland is concerned and a new cgroup with the same name may be /Users/rubber/linux/kernel/cgroup/cgroup.c: 5601
 * created.  As cgroup doesn't care about the names internally, this /Users/rubber/linux/kernel/cgroup/cgroup.c: 5602
 * doesn't cause any problem. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5603
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5615
	 * Only migration can raise populated from zero and we're already /Users/rubber/linux/kernel/cgroup/cgroup.c: 5616
	 * holding cgroup_mutex. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5617
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5622
	 * Make sure there's no live children.  We can't test emptiness of /Users/rubber/linux/kernel/cgroup/cgroup.c: 5623
	 * ->self.children as dead children linger on it while being /Users/rubber/linux/kernel/cgroup/cgroup.c: 5624
	 * drained; otherwise, "rmdir parent/child parent" may fail. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5625
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5630
	 * Mark @cgrp and the associated csets dead.  The former prevents /Users/rubber/linux/kernel/cgroup/cgroup.c: 5631
	 * further task migration and child creation by disabling /Users/rubber/linux/kernel/cgroup/cgroup.c: 5632
	 * cgroup_lock_live_group().  The latter makes the csets ignored by /Users/rubber/linux/kernel/cgroup/cgroup.c: 5633
	 * the migration path. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5634
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5658
		 * If the dying cgroup is frozen, decrease frozen descendants /Users/rubber/linux/kernel/cgroup/cgroup.c: 5659
		 * counters of ancestor cgroups. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5660
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5719
	 * Root csses are never destroyed and we can't initialize /Users/rubber/linux/kernel/cgroup/cgroup.c: 5720
	 * percpu_ref during early init.  Disable refcnting. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5721
	/* Update the init_css_set to contain a subsys /Users/rubber/linux/kernel/cgroup/cgroup.c: 5733
	 * pointer to this state - since the subsystem is /Users/rubber/linux/kernel/cgroup/cgroup.c: 5734
	 * newly registered, all tasks and hence the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5735
	/* At system boot, before all subsystems have been /Users/rubber/linux/kernel/cgroup/cgroup.c: 5744
	 * registered, no tasks have been forked, so we don't /Users/rubber/linux/kernel/cgroup/cgroup.c: 5745
 * cgroup_init_early - cgroup initialization at system boot /Users/rubber/linux/kernel/cgroup/cgroup.c: 5755
 * Initialize cgroups at system boot, and initialize any /Users/rubber/linux/kernel/cgroup/cgroup.c: 5757
 * subsystems that request early init. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5758
 * cgroup_init - cgroup initialization /Users/rubber/linux/kernel/cgroup/cgroup.c: 5792
 * Register cgroup filesystem and /proc file, and initialize /Users/rubber/linux/kernel/cgroup/cgroup.c: 5794
 * any subsystems that didn't request early init. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5795
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5808
	 * The latency of the synchronize_rcu() is too high for cgroups, /Users/rubber/linux/kernel/cgroup/cgroup.c: 5809
	 * avoid it at the cost of forcing all readers into the slow path. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5810
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5818
	 * Add init_css_set to the hash table so that dfl_root can link to /Users/rubber/linux/kernel/cgroup/cgroup.c: 5819
	 * it during init. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5820
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5844
		 * Setting dfl_root subsys_mask needs to consider the /Users/rubber/linux/kernel/cgroup/cgroup.c: 5845
		 * disabled flag and cftype registration needs kmalloc, /Users/rubber/linux/kernel/cgroup/cgroup.c: 5846
		 * both of which aren't available during early_init. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5847
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5902
	 * There isn't much point in executing destruction path in /Users/rubber/linux/kernel/cgroup/cgroup.c: 5903
	 * parallel.  Good chunk is serialized with cgroup_mutex anyway. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5904
	 * Use 1 for @max_active. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5905
	 * /Users/rubber/linux/kernel/cgroup/cgroup.c: 5906
	 * We would prefer to do this in cgroup_init() above, but that /Users/rubber/linux/kernel/cgroup/cgroup.c: 5907
	 * is called before init_workqueues(): so leave this until after. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5908
 * cgroup_get_from_id : get the cgroup associated with cgroup id /Users/rubber/linux/kernel/cgroup/cgroup.c: 5928
 * @id: cgroup id /Users/rubber/linux/kernel/cgroup/cgroup.c: 5929
 * On success return the cgrp, on failure return NULL /Users/rubber/linux/kernel/cgroup/cgroup.c: 5930
 * proc_cgroup_show() /Users/rubber/linux/kernel/cgroup/cgroup.c: 5956
 *  - Print task's cgroup paths into seq_file, one line for each hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 5957
 *  - Used for /proc/<pid>/cgroup. /Users/rubber/linux/kernel/cgroup/cgroup.c: 5958
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 5996
		 * On traditional hierarchies, all zombie tasks show up as /Users/rubber/linux/kernel/cgroup/cgroup.c: 5997
		 * belonging to the root cgroup.  On the default hierarchy, /Users/rubber/linux/kernel/cgroup/cgroup.c: 5998
		 * while a zombie doesn't show up in "cgroup.procs" and /Users/rubber/linux/kernel/cgroup/cgroup.c: 5999
		 * thus can't be migrated, its /proc/PID/cgroup keeps /Users/rubber/linux/kernel/cgroup/cgroup.c: 6000
		 * reporting the cgroup it belonged to before exiting.  If /Users/rubber/linux/kernel/cgroup/cgroup.c: 6001
		 * the cgroup is removed before the zombie is reaped, /Users/rubber/linux/kernel/cgroup/cgroup.c: 6002
		 * " (deleted)" is appended to the cgroup path. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6003
 * cgroup_fork - initialize cgroup related fields during copy_process() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6034
 * @child: pointer to task_struct of forking parent process. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6035
 * A task is associated with the init_css_set until cgroup_post_fork() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6037
 * attaches it to the target css_set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6038
 * cgroup_css_set_fork - find or create a css_set for a child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6065
 * @kargs: the arguments passed to create the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6066
 * This functions finds or creates a new css_set which the child /Users/rubber/linux/kernel/cgroup/cgroup.c: 6068
 * process will be attached to in cgroup_post_fork(). By default, /Users/rubber/linux/kernel/cgroup/cgroup.c: 6069
 * the child process will be given the same css_set as its parent. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6070
 * If CLONE_INTO_CGROUP is specified this function will try to find an /Users/rubber/linux/kernel/cgroup/cgroup.c: 6072
 * existing css_set which includes the requested cgroup and if not create /Users/rubber/linux/kernel/cgroup/cgroup.c: 6073
 * a new css_set that the child will be attached to later. If this function /Users/rubber/linux/kernel/cgroup/cgroup.c: 6074
 * succeeds it will hold cgroup_threadgroup_rwsem on return. If /Users/rubber/linux/kernel/cgroup/cgroup.c: 6075
 * CLONE_INTO_CGROUP is requested this function will grab cgroup mutex /Users/rubber/linux/kernel/cgroup/cgroup.c: 6076
 * before grabbing cgroup_threadgroup_rwsem and will hold a reference /Users/rubber/linux/kernel/cgroup/cgroup.c: 6077
 * to the target cgroup. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6078
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6123
	 * Verify that we the target cgroup is writable for us. This is /Users/rubber/linux/kernel/cgroup/cgroup.c: 6124
	 * usually done by the vfs layer but since we're not going through /Users/rubber/linux/kernel/cgroup/cgroup.c: 6125
	 * the vfs layer here we need to do it "manually". /Users/rubber/linux/kernel/cgroup/cgroup.c: 6126
 * cgroup_css_set_put_fork - drop references we took during fork /Users/rubber/linux/kernel/cgroup/cgroup.c: 6162
 * @kargs: the arguments passed to create the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6163
 * Drop references to the prepared css_set and target cgroup if /Users/rubber/linux/kernel/cgroup/cgroup.c: 6165
 * CLONE_INTO_CGROUP was requested. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6166
 * cgroup_can_fork - called on a new task before the process is exposed /Users/rubber/linux/kernel/cgroup/cgroup.c: 6192
 * @child: the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6193
 * This prepares a new css_set for the child process which the child will /Users/rubber/linux/kernel/cgroup/cgroup.c: 6195
 * be attached to in cgroup_post_fork(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 6196
 * This calls the subsystem can_fork() callbacks. If the cgroup_can_fork() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6197
 * callback returns an error, the fork aborts with that error code. This /Users/rubber/linux/kernel/cgroup/cgroup.c: 6198
 * allows for a cgroup subsystem to conditionally allow or deny new forks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6199
 * cgroup_cancel_fork - called if a fork failed after cgroup_can_fork() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6232
 * @child: the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6233
 * @kargs: the arguments passed to create the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6234
 * This calls the cancel_fork() callbacks if a fork failed *after* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6236
 * cgroup_can_fork() succeeded and cleans up references we took to /Users/rubber/linux/kernel/cgroup/cgroup.c: 6237
 * prepare a new css_set for the child process in cgroup_can_fork(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 6238
 * cgroup_post_fork - finalize cgroup setup for the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6254
 * @child: the child process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6255
 * Attach the child process to its css_set calling the subsystem fork() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6257
 * callbacks. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6258
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6292
			 * If the cgroup has to be frozen, the new task has /Users/rubber/linux/kernel/cgroup/cgroup.c: 6293
			 * too. Let's set the JOBCTL_TRAP_FREEZE jobctl bit to /Users/rubber/linux/kernel/cgroup/cgroup.c: 6294
			 * get the task into the frozen state. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6295
			/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6302
			 * Calling cgroup_update_frozen() isn't required here, /Users/rubber/linux/kernel/cgroup/cgroup.c: 6303
			 * because it will be called anyway a bit later from /Users/rubber/linux/kernel/cgroup/cgroup.c: 6304
			 * do_freezer_trap(). So we avoid cgroup's transient /Users/rubber/linux/kernel/cgroup/cgroup.c: 6305
			 * switch from the frozen state and back. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6306
		/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6310
		 * If the cgroup is to be killed notice it now and take the /Users/rubber/linux/kernel/cgroup/cgroup.c: 6311
		 * child down right after we finished preparing it for /Users/rubber/linux/kernel/cgroup/cgroup.c: 6312
		 * userspace. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6313
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6320
	 * Call ss->fork().  This must happen after @child is linked on /Users/rubber/linux/kernel/cgroup/cgroup.c: 6321
	 * css_set; otherwise, @child might change state between ->fork() /Users/rubber/linux/kernel/cgroup/cgroup.c: 6322
	 * and addition to css_set. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6323
 * cgroup_exit - detach cgroup from exiting task /Users/rubber/linux/kernel/cgroup/cgroup.c: 6346
 * @tsk: pointer to task_struct of exiting process /Users/rubber/linux/kernel/cgroup/cgroup.c: 6347
 * Description: Detach cgroup from @tsk. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6349
 * css_tryget_online_from_dir - get corresponding css from a cgroup dentry /Users/rubber/linux/kernel/cgroup/cgroup.c: 6444
 * @dentry: directory dentry of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 6445
 * @ss: subsystem of interest /Users/rubber/linux/kernel/cgroup/cgroup.c: 6446
 * If @dentry is a directory for a cgroup which has @ss enabled on it, try /Users/rubber/linux/kernel/cgroup/cgroup.c: 6448
 * to get the corresponding css and return it.  If such css doesn't exist /Users/rubber/linux/kernel/cgroup/cgroup.c: 6449
 * or can't be pinned, an ERR_PTR value is returned. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6450
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6467
	 * This path doesn't originate from kernfs and @kn could already /Users/rubber/linux/kernel/cgroup/cgroup.c: 6468
	 * have been or be removed at any point.  @kn->priv is RCU /Users/rubber/linux/kernel/cgroup/cgroup.c: 6469
	 * protected for this access.  See css_release_work_fn() for details. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6470
 * css_from_id - lookup css by id /Users/rubber/linux/kernel/cgroup/cgroup.c: 6484
 * @id: the cgroup id /Users/rubber/linux/kernel/cgroup/cgroup.c: 6485
 * @ss: cgroup subsys to be looked into /Users/rubber/linux/kernel/cgroup/cgroup.c: 6486
 * Returns the css if there's valid one with @id, otherwise returns NULL. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6488
 * Should be called under rcu_read_lock(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 6489
 * cgroup_get_from_path - lookup and get a cgroup from its default hierarchy path /Users/rubber/linux/kernel/cgroup/cgroup.c: 6498
 * @path: path on the default hierarchy /Users/rubber/linux/kernel/cgroup/cgroup.c: 6499
 * Find the cgroup at @path on the default hierarchy, increment its /Users/rubber/linux/kernel/cgroup/cgroup.c: 6501
 * reference count and return it.  Returns pointer to the found cgroup on /Users/rubber/linux/kernel/cgroup/cgroup.c: 6502
 * success, ERR_PTR(-ENOENT) if @path doesn't exist or if the cgroup has already /Users/rubber/linux/kernel/cgroup/cgroup.c: 6503
 * been released and ERR_PTR(-ENOTDIR) if @path points to a non-directory. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6504
 * cgroup_get_from_fd - get a cgroup pointer from a fd /Users/rubber/linux/kernel/cgroup/cgroup.c: 6536
 * @fd: fd obtained by open(cgroup2_dir) /Users/rubber/linux/kernel/cgroup/cgroup.c: 6537
 * Find the cgroup from a fd which should be obtained /Users/rubber/linux/kernel/cgroup/cgroup.c: 6539
 * by opening a cgroup directory.  Returns a pointer to the /Users/rubber/linux/kernel/cgroup/cgroup.c: 6540
 * cgroup on success. ERR_PTR is returned if the cgroup /Users/rubber/linux/kernel/cgroup/cgroup.c: 6541
 * cannot be found. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6542
 * cgroup_parse_float - parse a floating number /Users/rubber/linux/kernel/cgroup/cgroup.c: 6568
 * @input: input string /Users/rubber/linux/kernel/cgroup/cgroup.c: 6569
 * @dec_shift: number of decimal digits to shift /Users/rubber/linux/kernel/cgroup/cgroup.c: 6570
 * @v: output /Users/rubber/linux/kernel/cgroup/cgroup.c: 6571
 * Parse a decimal floating point number in @input and store the result in /Users/rubber/linux/kernel/cgroup/cgroup.c: 6573
 * @v with decimal point right shifted @dec_shift times.  For example, if /Users/rubber/linux/kernel/cgroup/cgroup.c: 6574
 * @input is "12.3456" and @dec_shift is 3, *@v will be set to 12345. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6575
 * Returns 0 on success, -errno otherwise. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6576
 * There's nothing cgroup specific about this function except that it's /Users/rubber/linux/kernel/cgroup/cgroup.c: 6578
 * currently the only user. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6579
 * sock->sk_cgrp_data handling.  For more info, see sock_cgroup_data /Users/rubber/linux/kernel/cgroup/cgroup.c: 6602
 * definition in cgroup-defs.h. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6603
	/* /Users/rubber/linux/kernel/cgroup/cgroup.c: 6639
	 * We might be cloning a socket which is left in an empty /Users/rubber/linux/kernel/cgroup/cgroup.c: 6640
	 * cgroup and the cgroup might have already been rmdir'd. /Users/rubber/linux/kernel/cgroup/cgroup.c: 6641
	 * Don't use cgroup_get_live(). /Users/rubber/linux/kernel/cgroup/cgroup.c: 6642
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/ntp.c: 1
 * NTP state machine interfaces and logic. /Users/rubber/linux/kernel/time/ntp.c: 3
 * This code was mainly moved from kernel/timer.c and kernel/time.c /Users/rubber/linux/kernel/time/ntp.c: 5
 * Please see those files for relevant copyright info and historical /Users/rubber/linux/kernel/time/ntp.c: 6
 * changelogs. /Users/rubber/linux/kernel/time/ntp.c: 7
 * NTP timekeeping variables: /Users/rubber/linux/kernel/time/ntp.c: 27
 * Note: All of the NTP state is protected by the timekeeping locks. /Users/rubber/linux/kernel/time/ntp.c: 29
 * phase-lock loop variables /Users/rubber/linux/kernel/time/ntp.c: 49
 * clock synchronization status /Users/rubber/linux/kernel/time/ntp.c: 53
 * (TIME_ERROR prevents overwriting the CMOS clock) /Users/rubber/linux/kernel/time/ntp.c: 55
 * The following variables are used when a pulse-per-second (PPS) signal /Users/rubber/linux/kernel/time/ntp.c: 91
 * is available. They establish the engineering parameters of the clock /Users/rubber/linux/kernel/time/ntp.c: 92
 * discipline loop when controlled by the PPS signal. /Users/rubber/linux/kernel/time/ntp.c: 93
#define PPS_INTCOUNT	4	/* number of consecutive good intervals to /Users/rubber/linux/kernel/time/ntp.c: 99
				   increase pps_shift or consecutive bad /Users/rubber/linux/kernel/time/ntp.c: 100
 * PPS signal quality monitors /Users/rubber/linux/kernel/time/ntp.c: 114
/* PPS kernel consumer compensates the whole phase error immediately. /Users/rubber/linux/kernel/time/ntp.c: 122
 * Otherwise, reduce the offset by a fixed factor times the time constant. /Users/rubber/linux/kernel/time/ntp.c: 123
	/* the PPS calibration interval may end /Users/rubber/linux/kernel/time/ntp.c: 135
 * pps_clear - Clears the PPS state variables /Users/rubber/linux/kernel/time/ntp.c: 142
/* Decrease pps_valid to indicate that another second has passed since /Users/rubber/linux/kernel/time/ntp.c: 154
 * the last PPS signal. When it reaches 0, indicate that PPS signal is /Users/rubber/linux/kernel/time/ntp.c: 155
 * missing. /Users/rubber/linux/kernel/time/ntp.c: 156
		/* PPS signal lost when either PPS time or /Users/rubber/linux/kernel/time/ntp.c: 177
		 * PPS frequency synchronization requested /Users/rubber/linux/kernel/time/ntp.c: 178
		/* PPS jitter exceeded when /Users/rubber/linux/kernel/time/ntp.c: 182
		/* PPS wander exceeded or calibration error when /Users/rubber/linux/kernel/time/ntp.c: 186
		 * PPS frequency synchronization requested /Users/rubber/linux/kernel/time/ntp.c: 187
 * ntp_synced - Returns 1 if the NTP status is not UNSYNC /Users/rubber/linux/kernel/time/ntp.c: 242
 * NTP methods: /Users/rubber/linux/kernel/time/ntp.c: 252
 * Update (tick_length, tick_length_base, tick_nsec), based /Users/rubber/linux/kernel/time/ntp.c: 256
 * on (tick_usec, ntp_tick_adj, time_freq): /Users/rubber/linux/kernel/time/ntp.c: 257
	/* /Users/rubber/linux/kernel/time/ntp.c: 273
	 * Don't wait for the next second_overflow, apply /Users/rubber/linux/kernel/time/ntp.c: 274
	 * the change to the tick length immediately: /Users/rubber/linux/kernel/time/ntp.c: 275
	/* /Users/rubber/linux/kernel/time/ntp.c: 311
	 * Scale the phase adjustment and /Users/rubber/linux/kernel/time/ntp.c: 312
	 * clamp to the operating range. /Users/rubber/linux/kernel/time/ntp.c: 313
	/* /Users/rubber/linux/kernel/time/ntp.c: 317
	 * Select how the frequency is to be controlled /Users/rubber/linux/kernel/time/ntp.c: 318
	 * and in which mode (PLL or FLL). /Users/rubber/linux/kernel/time/ntp.c: 319
	/* /Users/rubber/linux/kernel/time/ntp.c: 330
	 * Clamp update interval to reduce PLL gain with low /Users/rubber/linux/kernel/time/ntp.c: 331
	 * sampling rate (e.g. intermittent network connection) /Users/rubber/linux/kernel/time/ntp.c: 332
	 * to avoid instability. /Users/rubber/linux/kernel/time/ntp.c: 333
 * ntp_clear - Clears the NTP state variables /Users/rubber/linux/kernel/time/ntp.c: 349
 * ntp_get_next_leap - Returns the next leapsecond in CLOCK_REALTIME ktime_t /Users/rubber/linux/kernel/time/ntp.c: 375
 * Provides the time of the next leapsecond against CLOCK_REALTIME in /Users/rubber/linux/kernel/time/ntp.c: 377
 * a ktime_t format. Returns KTIME_MAX if no leapsecond is pending. /Users/rubber/linux/kernel/time/ntp.c: 378
 * this routine handles the overflow of the microsecond field /Users/rubber/linux/kernel/time/ntp.c: 391
 * The tricky bits of code to handle the accurate clock support /Users/rubber/linux/kernel/time/ntp.c: 393
 * were provided by Dave Mills (Mills@UDEL.EDU) of NTP fame. /Users/rubber/linux/kernel/time/ntp.c: 394
 * They were originally developed for SUN and DEC kernels. /Users/rubber/linux/kernel/time/ntp.c: 395
 * All the kudos should go to Dave for this stuff. /Users/rubber/linux/kernel/time/ntp.c: 396
 * Also handles leap second processing, and returns leap offset /Users/rubber/linux/kernel/time/ntp.c: 398
	/* /Users/rubber/linux/kernel/time/ntp.c: 406
	 * Leap second processing. If in leap-insert state at the end of the /Users/rubber/linux/kernel/time/ntp.c: 407
	 * day, the system clock is set back one second; if in leap-delete /Users/rubber/linux/kernel/time/ntp.c: 408
	 * state, the system clock is set ahead one second. /Users/rubber/linux/kernel/time/ntp.c: 409
 * Check whether @now is correct versus the required time to update the RTC /Users/rubber/linux/kernel/time/ntp.c: 523
 * and calculate the value which needs to be written to the RTC so that the /Users/rubber/linux/kernel/time/ntp.c: 524
 * next seconds increment of the RTC after the write is aligned with the next /Users/rubber/linux/kernel/time/ntp.c: 525
 * seconds increment of clock REALTIME. /Users/rubber/linux/kernel/time/ntp.c: 526
 * tsched     t1 write(t2.tv_sec - 1sec))	t2 RTC increments seconds /Users/rubber/linux/kernel/time/ntp.c: 528
 * t2.tv_nsec == 0 /Users/rubber/linux/kernel/time/ntp.c: 530
 * tsched = t2 - set_offset_nsec /Users/rubber/linux/kernel/time/ntp.c: 531
 * newval = t2 - NSEC_PER_SEC /Users/rubber/linux/kernel/time/ntp.c: 532
 * ==> neval = tsched + set_offset_nsec - NSEC_PER_SEC /Users/rubber/linux/kernel/time/ntp.c: 534
 * As the execution of this code is not guaranteed to happen exactly at /Users/rubber/linux/kernel/time/ntp.c: 536
 * tsched this allows it to happen within a fuzzy region: /Users/rubber/linux/kernel/time/ntp.c: 537
 *	abs(now - tsched) < FUZZ /Users/rubber/linux/kernel/time/ntp.c: 539
 * If @now is not inside the allowed window the function returns false. /Users/rubber/linux/kernel/time/ntp.c: 541
 * If we have an externally synchronized Linux clock, then update RTC clock /Users/rubber/linux/kernel/time/ntp.c: 615
 * accordingly every ~11 minutes. Generally RTCs can only store second /Users/rubber/linux/kernel/time/ntp.c: 616
 * precision, but many RTCs will adjust the phase of their second tick to /Users/rubber/linux/kernel/time/ntp.c: 617
 * match the moment of update. This infrastructure arranges to call to the RTC /Users/rubber/linux/kernel/time/ntp.c: 618
 * set at the correct moment to phase synchronize the RTC second tick over /Users/rubber/linux/kernel/time/ntp.c: 619
 * with the kernel clock. /Users/rubber/linux/kernel/time/ntp.c: 620
	/* /Users/rubber/linux/kernel/time/ntp.c: 624
	 * The default synchronization offset is 500ms for the deprecated /Users/rubber/linux/kernel/time/ntp.c: 625
	 * update_persistent_clock64() under the assumption that it uses /Users/rubber/linux/kernel/time/ntp.c: 626
	 * the infamous CMOS clock (MC146818). /Users/rubber/linux/kernel/time/ntp.c: 627
	/* /Users/rubber/linux/kernel/time/ntp.c: 633
	 * Don't update if STA_UNSYNC is set and if ntp_notify_cmos_timer() /Users/rubber/linux/kernel/time/ntp.c: 634
	 * managed to schedule the work between the timer firing and the /Users/rubber/linux/kernel/time/ntp.c: 635
	 * work being able to rearm the timer. Wait for the timer to expire. /Users/rubber/linux/kernel/time/ntp.c: 636
	/* /Users/rubber/linux/kernel/time/ntp.c: 665
	 * When the work is currently executed but has not yet the timer /Users/rubber/linux/kernel/time/ntp.c: 666
	 * rearmed this queues the work immediately again. No big issue, /Users/rubber/linux/kernel/time/ntp.c: 667
	 * just a pointless work scheduled. /Users/rubber/linux/kernel/time/ntp.c: 668
 * Propagate a new txc->status value into the NTP state: /Users/rubber/linux/kernel/time/ntp.c: 684
	/* /Users/rubber/linux/kernel/time/ntp.c: 696
	 * If we turn on PLL adjustments then reset the /Users/rubber/linux/kernel/time/ntp.c: 697
	 * reference time to current time. /Users/rubber/linux/kernel/time/ntp.c: 698
 * adjtimex mainly allows reading (and writing, if superuser) of /Users/rubber/linux/kernel/time/ntp.c: 759
 * kernel time-keeping variables. used by xntpd. /Users/rubber/linux/kernel/time/ntp.c: 760
/* actually struct pps_normtime is good old struct timespec, but it is /Users/rubber/linux/kernel/time/ntp.c: 850
 * semantically different (and it is the reason why it was invented): /Users/rubber/linux/kernel/time/ntp.c: 851
 * pps_normtime.nsec has a range of ( -NSEC_PER_SEC / 2, NSEC_PER_SEC / 2 ] /Users/rubber/linux/kernel/time/ntp.c: 852
/* normalize the timestamp so that nsec is in the /Users/rubber/linux/kernel/time/ntp.c: 859
/* decrease frequency calibration interval length. /Users/rubber/linux/kernel/time/ntp.c: 895
 * It is halved after four consecutive unstable intervals. /Users/rubber/linux/kernel/time/ntp.c: 896
/* increase frequency calibration interval length. /Users/rubber/linux/kernel/time/ntp.c: 909
 * It is doubled after four consecutive stable intervals. /Users/rubber/linux/kernel/time/ntp.c: 910
/* update clock frequency based on MONOTONIC_RAW clock PPS signal /Users/rubber/linux/kernel/time/ntp.c: 923
 * timestamps /Users/rubber/linux/kernel/time/ntp.c: 924
 * At the end of the calibration interval the difference between the /Users/rubber/linux/kernel/time/ntp.c: 926
 * first and last MONOTONIC_RAW clock timestamps divided by the length /Users/rubber/linux/kernel/time/ntp.c: 927
 * of the interval becomes the frequency update. If the interval was /Users/rubber/linux/kernel/time/ntp.c: 928
 * too long, the data are discarded. /Users/rubber/linux/kernel/time/ntp.c: 929
 * Returns the difference between old and new frequency values. /Users/rubber/linux/kernel/time/ntp.c: 930
	/* here the raw frequency offset and wander (stability) is /Users/rubber/linux/kernel/time/ntp.c: 948
	 * calculated. If the wander is less than the wander threshold /Users/rubber/linux/kernel/time/ntp.c: 949
	 * the interval is increased; otherwise it is decreased. /Users/rubber/linux/kernel/time/ntp.c: 950
	/* the stability metric is calculated as the average of recent /Users/rubber/linux/kernel/time/ntp.c: 966
	 * frequency changes, but is used only for performance /Users/rubber/linux/kernel/time/ntp.c: 967
	 * monitoring /Users/rubber/linux/kernel/time/ntp.c: 968
	/* Nominal jitter is due to PPS signal noise. If it exceeds the /Users/rubber/linux/kernel/time/ntp.c: 997
	 * threshold, the sample is discarded; otherwise, if so enabled, /Users/rubber/linux/kernel/time/ntp.c: 998
	 * the time offset is updated. /Users/rubber/linux/kernel/time/ntp.c: 999
 * __hardpps() - discipline CPU clock oscillator to external PPS signal /Users/rubber/linux/kernel/time/ntp.c: 1019
 * This routine is called at each PPS signal arrival in order to /Users/rubber/linux/kernel/time/ntp.c: 1021
 * discipline the CPU clock oscillator to the PPS signal. It takes two /Users/rubber/linux/kernel/time/ntp.c: 1022
 * parameters: REALTIME and MONOTONIC_RAW clock timestamps. The former /Users/rubber/linux/kernel/time/ntp.c: 1023
 * is used to correct clock phase error and the latter is used to /Users/rubber/linux/kernel/time/ntp.c: 1024
 * correct the frequency. /Users/rubber/linux/kernel/time/ntp.c: 1025
 * This code is based on David Mills's reference nanokernel /Users/rubber/linux/kernel/time/ntp.c: 1027
 * implementation. It was mostly rewritten but keeps the same idea. /Users/rubber/linux/kernel/time/ntp.c: 1028
	/* when called for the first time, /Users/rubber/linux/kernel/time/ntp.c: 1043
	/* check that the signal is in the range /Users/rubber/linux/kernel/time/ntp.c: 1053
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/timekeeping_debug.c: 1
 * debugfs file to track time spent in suspend /Users/rubber/linux/kernel/time/timekeeping_debug.c: 3
 * Copyright (c) 2011, Google, Inc. /Users/rubber/linux/kernel/time/timekeeping_debug.c: 5
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/test_udelay.c: 1
 * udelay() test kernel module /Users/rubber/linux/kernel/time/test_udelay.c: 3
 * Test is executed by writing and reading to /sys/kernel/debug/udelay_test /Users/rubber/linux/kernel/time/test_udelay.c: 5
 * Tests are configured by writing: USECS ITERATIONS /Users/rubber/linux/kernel/time/test_udelay.c: 6
 * Tests are executed by reading from the same file. /Users/rubber/linux/kernel/time/test_udelay.c: 7
 * Specifying usecs of 0 or negative values will run multiples tests. /Users/rubber/linux/kernel/time/test_udelay.c: 8
 * Copyright (C) 2014 Google, Inc. /Users/rubber/linux/kernel/time/test_udelay.c: 10
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/timer.c: 1
 *  Kernel internal timers /Users/rubber/linux/kernel/time/timer.c: 3
 *  Copyright (C) 1991, 1992  Linus Torvalds /Users/rubber/linux/kernel/time/timer.c: 5
 *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better. /Users/rubber/linux/kernel/time/timer.c: 7
 *  1997-09-10  Updated NTP code according to technical memorandum Jan '96 /Users/rubber/linux/kernel/time/timer.c: 9
 *              "A Kernel Model for Precision Timekeeping" by Dave Mills /Users/rubber/linux/kernel/time/timer.c: 10
 *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to /Users/rubber/linux/kernel/time/timer.c: 11
 *              serialize accesses to xtime/lost_ticks). /Users/rubber/linux/kernel/time/timer.c: 12
 *                              Copyright (C) 1998  Andrea Arcangeli /Users/rubber/linux/kernel/time/timer.c: 13
 *  1999-03-10  Improved NTP compatibility by Ulrich Windl /Users/rubber/linux/kernel/time/timer.c: 14
 *  2002-05-31	Move sys_sysinfo here and make its locking sane, Robert Love /Users/rubber/linux/kernel/time/timer.c: 15
 *  2000-10-05  Implemented scalable SMP per-CPU timer handling. /Users/rubber/linux/kernel/time/timer.c: 16
 *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar /Users/rubber/linux/kernel/time/timer.c: 17
 *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar /Users/rubber/linux/kernel/time/timer.c: 18
 * The timer wheel has LVL_DEPTH array levels. Each level provides an array of /Users/rubber/linux/kernel/time/timer.c: 64
 * LVL_SIZE buckets. Each level is driven by its own clock and therefor each /Users/rubber/linux/kernel/time/timer.c: 65
 * level has a different granularity. /Users/rubber/linux/kernel/time/timer.c: 66
 * The level granularity is:		LVL_CLK_DIV ^ lvl /Users/rubber/linux/kernel/time/timer.c: 68
 * The level clock frequency is:	HZ / (LVL_CLK_DIV ^ level) /Users/rubber/linux/kernel/time/timer.c: 69
 * The array level of a newly armed timer depends on the relative expiry /Users/rubber/linux/kernel/time/timer.c: 71
 * time. The farther the expiry time is away the higher the array level and /Users/rubber/linux/kernel/time/timer.c: 72
 * therefor the granularity becomes. /Users/rubber/linux/kernel/time/timer.c: 73
 * Contrary to the original timer wheel implementation, which aims for 'exact' /Users/rubber/linux/kernel/time/timer.c: 75
 * expiry of the timers, this implementation removes the need for recascading /Users/rubber/linux/kernel/time/timer.c: 76
 * the timers into the lower array levels. The previous 'classic' timer wheel /Users/rubber/linux/kernel/time/timer.c: 77
 * implementation of the kernel already violated the 'exact' expiry by adding /Users/rubber/linux/kernel/time/timer.c: 78
 * slack to the expiry time to provide batched expiration. The granularity /Users/rubber/linux/kernel/time/timer.c: 79
 * levels provide implicit batching. /Users/rubber/linux/kernel/time/timer.c: 80
 * This is an optimization of the original timer wheel implementation for the /Users/rubber/linux/kernel/time/timer.c: 82
 * majority of the timer wheel use cases: timeouts. The vast majority of /Users/rubber/linux/kernel/time/timer.c: 83
 * timeout timers (networking, disk I/O ...) are canceled before expiry. If /Users/rubber/linux/kernel/time/timer.c: 84
 * the timeout expires it indicates that normal operation is disturbed, so it /Users/rubber/linux/kernel/time/timer.c: 85
 * does not matter much whether the timeout comes with a slight delay. /Users/rubber/linux/kernel/time/timer.c: 86
 * The only exception to this are networking timers with a small expiry /Users/rubber/linux/kernel/time/timer.c: 88
 * time. They rely on the granularity. Those fit into the first wheel level, /Users/rubber/linux/kernel/time/timer.c: 89
 * which has HZ granularity. /Users/rubber/linux/kernel/time/timer.c: 90
 * We don't have cascading anymore. timers with a expiry time above the /Users/rubber/linux/kernel/time/timer.c: 92
 * capacity of the last wheel level are force expired at the maximum timeout /Users/rubber/linux/kernel/time/timer.c: 93
 * value of the last wheel level. From data sampling we know that the maximum /Users/rubber/linux/kernel/time/timer.c: 94
 * value observed is 5 days (network connection tracking), so this should not /Users/rubber/linux/kernel/time/timer.c: 95
 * be an issue. /Users/rubber/linux/kernel/time/timer.c: 96
 * The currently chosen array constants values are a good compromise between /Users/rubber/linux/kernel/time/timer.c: 98
 * array size and granularity. /Users/rubber/linux/kernel/time/timer.c: 99
 * This results in the following granularity and range levels: /Users/rubber/linux/kernel/time/timer.c: 101
 * HZ 1000 steps /Users/rubber/linux/kernel/time/timer.c: 103
 * Level Offset  Granularity            Range /Users/rubber/linux/kernel/time/timer.c: 104
 *  0      0         1 ms                0 ms -         63 ms /Users/rubber/linux/kernel/time/timer.c: 105
 *  1     64         8 ms               64 ms -        511 ms /Users/rubber/linux/kernel/time/timer.c: 106
 *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s) /Users/rubber/linux/kernel/time/timer.c: 107
 *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s) /Users/rubber/linux/kernel/time/timer.c: 108
 *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m) /Users/rubber/linux/kernel/time/timer.c: 109
 *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m) /Users/rubber/linux/kernel/time/timer.c: 110
 *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h) /Users/rubber/linux/kernel/time/timer.c: 111
 *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d) /Users/rubber/linux/kernel/time/timer.c: 112
 *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d) /Users/rubber/linux/kernel/time/timer.c: 113
 * HZ  300 /Users/rubber/linux/kernel/time/timer.c: 115
 * Level Offset  Granularity            Range /Users/rubber/linux/kernel/time/timer.c: 116
 *  0	   0         3 ms                0 ms -        210 ms /Users/rubber/linux/kernel/time/timer.c: 117
 *  1	  64        26 ms              213 ms -       1703 ms (213ms - ~1s) /Users/rubber/linux/kernel/time/timer.c: 118
 *  2	 128       213 ms             1706 ms -      13650 ms (~1s - ~13s) /Users/rubber/linux/kernel/time/timer.c: 119
 *  3	 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m) /Users/rubber/linux/kernel/time/timer.c: 120
 *  4	 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m) /Users/rubber/linux/kernel/time/timer.c: 121
 *  5	 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h) /Users/rubber/linux/kernel/time/timer.c: 122
 *  6	 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h) /Users/rubber/linux/kernel/time/timer.c: 123
 *  7	 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d) /Users/rubber/linux/kernel/time/timer.c: 124
 *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d) /Users/rubber/linux/kernel/time/timer.c: 125
 * HZ  250 /Users/rubber/linux/kernel/time/timer.c: 127
 * Level Offset  Granularity            Range /Users/rubber/linux/kernel/time/timer.c: 128
 *  0	   0         4 ms                0 ms -        255 ms /Users/rubber/linux/kernel/time/timer.c: 129
 *  1	  64        32 ms              256 ms -       2047 ms (256ms - ~2s) /Users/rubber/linux/kernel/time/timer.c: 130
 *  2	 128       256 ms             2048 ms -      16383 ms (~2s - ~16s) /Users/rubber/linux/kernel/time/timer.c: 131
 *  3	 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m) /Users/rubber/linux/kernel/time/timer.c: 132
 *  4	 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m) /Users/rubber/linux/kernel/time/timer.c: 133
 *  5	 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h) /Users/rubber/linux/kernel/time/timer.c: 134
 *  6	 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h) /Users/rubber/linux/kernel/time/timer.c: 135
 *  7	 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d) /Users/rubber/linux/kernel/time/timer.c: 136
 *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d) /Users/rubber/linux/kernel/time/timer.c: 137
 * HZ  100 /Users/rubber/linux/kernel/time/timer.c: 139
 * Level Offset  Granularity            Range /Users/rubber/linux/kernel/time/timer.c: 140
 *  0	   0         10 ms               0 ms -        630 ms /Users/rubber/linux/kernel/time/timer.c: 141
 *  1	  64         80 ms             640 ms -       5110 ms (640ms - ~5s) /Users/rubber/linux/kernel/time/timer.c: 142
 *  2	 128        640 ms            5120 ms -      40950 ms (~5s - ~40s) /Users/rubber/linux/kernel/time/timer.c: 143
 *  3	 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m) /Users/rubber/linux/kernel/time/timer.c: 144
 *  4	 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m) /Users/rubber/linux/kernel/time/timer.c: 145
 *  5	 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h) /Users/rubber/linux/kernel/time/timer.c: 146
 *  6	 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d) /Users/rubber/linux/kernel/time/timer.c: 147
 *  7	 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d) /Users/rubber/linux/kernel/time/timer.c: 148
 * The time start value for each level to select the bucket at enqueue /Users/rubber/linux/kernel/time/timer.c: 159
 * time. We start from the last possible delta of the previous level /Users/rubber/linux/kernel/time/timer.c: 160
 * so that we can later add an extra LVL_GRAN(n) to n (see calc_index()). /Users/rubber/linux/kernel/time/timer.c: 161
 * The resulting wheel size. If NOHZ is configured we allocate two /Users/rubber/linux/kernel/time/timer.c: 183
 * wheels so we have a separate storage for the deferrable timers. /Users/rubber/linux/kernel/time/timer.c: 184
	/* /Users/rubber/linux/kernel/time/timer.c: 281
	 * We don't want all cpus firing their timers at once hitting the /Users/rubber/linux/kernel/time/timer.c: 282
	 * same lock or cachelines, so we skew each extra cpu with an extra /Users/rubber/linux/kernel/time/timer.c: 283
	 * 3 jiffies. This 3 jiffies came originally from the mm/ code which /Users/rubber/linux/kernel/time/timer.c: 284
	 * already did this. /Users/rubber/linux/kernel/time/timer.c: 285
	 * The skew is done by adding 3*cpunr, then round, then subtract this /Users/rubber/linux/kernel/time/timer.c: 286
	 * extra offset again. /Users/rubber/linux/kernel/time/timer.c: 287
	/* /Users/rubber/linux/kernel/time/timer.c: 293
	 * If the target jiffie is just after a whole second (which can happen /Users/rubber/linux/kernel/time/timer.c: 294
	 * due to delays of the timer irq, long irq off times etc etc) then /Users/rubber/linux/kernel/time/timer.c: 295
	 * we should round down to the whole second, not up. Use 1/4th second /Users/rubber/linux/kernel/time/timer.c: 296
	 * as cutoff for this rounding as an extreme upper bound for this. /Users/rubber/linux/kernel/time/timer.c: 297
	 * But never round down if @force_up is set. /Users/rubber/linux/kernel/time/timer.c: 298
	/* /Users/rubber/linux/kernel/time/timer.c: 308
	 * Make sure j is still in the future. Otherwise return the /Users/rubber/linux/kernel/time/timer.c: 309
	 * unmodified value. /Users/rubber/linux/kernel/time/timer.c: 310
 * __round_jiffies - function to round jiffies to a full second /Users/rubber/linux/kernel/time/timer.c: 316
 * @j: the time in (absolute) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 317
 * @cpu: the processor number on which the timeout will happen /Users/rubber/linux/kernel/time/timer.c: 318
 * __round_jiffies() rounds an absolute time in the future (in jiffies) /Users/rubber/linux/kernel/time/timer.c: 320
 * up or down to (approximately) full seconds. This is useful for timers /Users/rubber/linux/kernel/time/timer.c: 321
 * for which the exact time they fire does not matter too much, as long as /Users/rubber/linux/kernel/time/timer.c: 322
 * they fire approximately every X seconds. /Users/rubber/linux/kernel/time/timer.c: 323
 * By rounding these timers to whole seconds, all such timers will fire /Users/rubber/linux/kernel/time/timer.c: 325
 * at the same time, rather than at various times spread out. The goal /Users/rubber/linux/kernel/time/timer.c: 326
 * of this is to have the CPU wake up less, which saves power. /Users/rubber/linux/kernel/time/timer.c: 327
 * The exact rounding is skewed for each processor to avoid all /Users/rubber/linux/kernel/time/timer.c: 329
 * processors firing at the exact same time, which could lead /Users/rubber/linux/kernel/time/timer.c: 330
 * to lock contention or spurious cache line bouncing. /Users/rubber/linux/kernel/time/timer.c: 331
 * The return value is the rounded version of the @j parameter. /Users/rubber/linux/kernel/time/timer.c: 333
 * __round_jiffies_relative - function to round jiffies to a full second /Users/rubber/linux/kernel/time/timer.c: 342
 * @j: the time in (relative) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 343
 * @cpu: the processor number on which the timeout will happen /Users/rubber/linux/kernel/time/timer.c: 344
 * __round_jiffies_relative() rounds a time delta  in the future (in jiffies) /Users/rubber/linux/kernel/time/timer.c: 346
 * up or down to (approximately) full seconds. This is useful for timers /Users/rubber/linux/kernel/time/timer.c: 347
 * for which the exact time they fire does not matter too much, as long as /Users/rubber/linux/kernel/time/timer.c: 348
 * they fire approximately every X seconds. /Users/rubber/linux/kernel/time/timer.c: 349
 * By rounding these timers to whole seconds, all such timers will fire /Users/rubber/linux/kernel/time/timer.c: 351
 * at the same time, rather than at various times spread out. The goal /Users/rubber/linux/kernel/time/timer.c: 352
 * of this is to have the CPU wake up less, which saves power. /Users/rubber/linux/kernel/time/timer.c: 353
 * The exact rounding is skewed for each processor to avoid all /Users/rubber/linux/kernel/time/timer.c: 355
 * processors firing at the exact same time, which could lead /Users/rubber/linux/kernel/time/timer.c: 356
 * to lock contention or spurious cache line bouncing. /Users/rubber/linux/kernel/time/timer.c: 357
 * The return value is the rounded version of the @j parameter. /Users/rubber/linux/kernel/time/timer.c: 359
 * round_jiffies - function to round jiffies to a full second /Users/rubber/linux/kernel/time/timer.c: 371
 * @j: the time in (absolute) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 372
 * round_jiffies() rounds an absolute time in the future (in jiffies) /Users/rubber/linux/kernel/time/timer.c: 374
 * up or down to (approximately) full seconds. This is useful for timers /Users/rubber/linux/kernel/time/timer.c: 375
 * for which the exact time they fire does not matter too much, as long as /Users/rubber/linux/kernel/time/timer.c: 376
 * they fire approximately every X seconds. /Users/rubber/linux/kernel/time/timer.c: 377
 * By rounding these timers to whole seconds, all such timers will fire /Users/rubber/linux/kernel/time/timer.c: 379
 * at the same time, rather than at various times spread out. The goal /Users/rubber/linux/kernel/time/timer.c: 380
 * of this is to have the CPU wake up less, which saves power. /Users/rubber/linux/kernel/time/timer.c: 381
 * The return value is the rounded version of the @j parameter. /Users/rubber/linux/kernel/time/timer.c: 383
 * round_jiffies_relative - function to round jiffies to a full second /Users/rubber/linux/kernel/time/timer.c: 392
 * @j: the time in (relative) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 393
 * round_jiffies_relative() rounds a time delta  in the future (in jiffies) /Users/rubber/linux/kernel/time/timer.c: 395
 * up or down to (approximately) full seconds. This is useful for timers /Users/rubber/linux/kernel/time/timer.c: 396
 * for which the exact time they fire does not matter too much, as long as /Users/rubber/linux/kernel/time/timer.c: 397
 * they fire approximately every X seconds. /Users/rubber/linux/kernel/time/timer.c: 398
 * By rounding these timers to whole seconds, all such timers will fire /Users/rubber/linux/kernel/time/timer.c: 400
 * at the same time, rather than at various times spread out. The goal /Users/rubber/linux/kernel/time/timer.c: 401
 * of this is to have the CPU wake up less, which saves power. /Users/rubber/linux/kernel/time/timer.c: 402
 * The return value is the rounded version of the @j parameter. /Users/rubber/linux/kernel/time/timer.c: 404
 * __round_jiffies_up - function to round jiffies up to a full second /Users/rubber/linux/kernel/time/timer.c: 413
 * @j: the time in (absolute) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 414
 * @cpu: the processor number on which the timeout will happen /Users/rubber/linux/kernel/time/timer.c: 415
 * This is the same as __round_jiffies() except that it will never /Users/rubber/linux/kernel/time/timer.c: 417
 * round down.  This is useful for timeouts for which the exact time /Users/rubber/linux/kernel/time/timer.c: 418
 * of firing does not matter too much, as long as they don't fire too /Users/rubber/linux/kernel/time/timer.c: 419
 * early. /Users/rubber/linux/kernel/time/timer.c: 420
 * __round_jiffies_up_relative - function to round jiffies up to a full second /Users/rubber/linux/kernel/time/timer.c: 429
 * @j: the time in (relative) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 430
 * @cpu: the processor number on which the timeout will happen /Users/rubber/linux/kernel/time/timer.c: 431
 * This is the same as __round_jiffies_relative() except that it will never /Users/rubber/linux/kernel/time/timer.c: 433
 * round down.  This is useful for timeouts for which the exact time /Users/rubber/linux/kernel/time/timer.c: 434
 * of firing does not matter too much, as long as they don't fire too /Users/rubber/linux/kernel/time/timer.c: 435
 * early. /Users/rubber/linux/kernel/time/timer.c: 436
 * round_jiffies_up - function to round jiffies up to a full second /Users/rubber/linux/kernel/time/timer.c: 448
 * @j: the time in (absolute) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 449
 * This is the same as round_jiffies() except that it will never /Users/rubber/linux/kernel/time/timer.c: 451
 * round down.  This is useful for timeouts for which the exact time /Users/rubber/linux/kernel/time/timer.c: 452
 * of firing does not matter too much, as long as they don't fire too /Users/rubber/linux/kernel/time/timer.c: 453
 * early. /Users/rubber/linux/kernel/time/timer.c: 454
 * round_jiffies_up_relative - function to round jiffies up to a full second /Users/rubber/linux/kernel/time/timer.c: 463
 * @j: the time in (relative) jiffies that should be rounded /Users/rubber/linux/kernel/time/timer.c: 464
 * This is the same as round_jiffies_relative() except that it will never /Users/rubber/linux/kernel/time/timer.c: 466
 * round down.  This is useful for timeouts for which the exact time /Users/rubber/linux/kernel/time/timer.c: 467
 * of firing does not matter too much, as long as they don't fire too /Users/rubber/linux/kernel/time/timer.c: 468
 * early. /Users/rubber/linux/kernel/time/timer.c: 469
 * Helper function to calculate the array index for a given expiry /Users/rubber/linux/kernel/time/timer.c: 490
 * time. /Users/rubber/linux/kernel/time/timer.c: 491
	/* /Users/rubber/linux/kernel/time/timer.c: 497
	 * The timer wheel has to guarantee that a timer does not fire /Users/rubber/linux/kernel/time/timer.c: 498
	 * early. Early expiry can happen due to: /Users/rubber/linux/kernel/time/timer.c: 499
	 * - Timer is armed at the edge of a tick /Users/rubber/linux/kernel/time/timer.c: 500
	 * - Truncation of the expiry time in the outer wheel levels /Users/rubber/linux/kernel/time/timer.c: 501
	 * /Users/rubber/linux/kernel/time/timer.c: 502
	 * Round up with level granularity to prevent this. /Users/rubber/linux/kernel/time/timer.c: 503
		/* /Users/rubber/linux/kernel/time/timer.c: 536
		 * Force expire obscene large timeouts to expire at the /Users/rubber/linux/kernel/time/timer.c: 537
		 * capacity limit of the wheel. /Users/rubber/linux/kernel/time/timer.c: 538
	/* /Users/rubber/linux/kernel/time/timer.c: 554
	 * TODO: This wants some optimizing similar to the code below, but we /Users/rubber/linux/kernel/time/timer.c: 555
	 * will do that when we switch from push to pull for deferrable timers. /Users/rubber/linux/kernel/time/timer.c: 556
	/* /Users/rubber/linux/kernel/time/timer.c: 564
	 * We might have to IPI the remote CPU if the base is idle and the /Users/rubber/linux/kernel/time/timer.c: 565
	 * timer is not deferrable. If the other CPU is on the way to idle /Users/rubber/linux/kernel/time/timer.c: 566
	 * then it can't set base->is_idle as we hold the base lock: /Users/rubber/linux/kernel/time/timer.c: 567
 * Enqueue the timer into the hash bucket, mark it pending in /Users/rubber/linux/kernel/time/timer.c: 574
 * the bitmap, store the index in the timer flags then wake up /Users/rubber/linux/kernel/time/timer.c: 575
 * the target CPU if needed. /Users/rubber/linux/kernel/time/timer.c: 576
	/* /Users/rubber/linux/kernel/time/timer.c: 588
	 * Check whether this is the new first expiring timer. The /Users/rubber/linux/kernel/time/timer.c: 589
	 * effective expiry time of the timer is required here /Users/rubber/linux/kernel/time/timer.c: 590
	 * (bucket_expiry) instead of timer->expires. /Users/rubber/linux/kernel/time/timer.c: 591
		/* /Users/rubber/linux/kernel/time/timer.c: 594
		 * Set the next expiry time and kick the CPU so it /Users/rubber/linux/kernel/time/timer.c: 595
		 * can reevaluate the wheel: /Users/rubber/linux/kernel/time/timer.c: 596
 * fixup_init is called when: /Users/rubber/linux/kernel/time/timer.c: 632
 * - an active object is initialized /Users/rubber/linux/kernel/time/timer.c: 633
 * fixup_activate is called when: /Users/rubber/linux/kernel/time/timer.c: 656
 * - an active object is activated /Users/rubber/linux/kernel/time/timer.c: 657
 * - an unknown non-static object is activated /Users/rubber/linux/kernel/time/timer.c: 658
 * fixup_free is called when: /Users/rubber/linux/kernel/time/timer.c: 678
 * - an active object is freed /Users/rubber/linux/kernel/time/timer.c: 679
 * fixup_assert_init is called when: /Users/rubber/linux/kernel/time/timer.c: 696
 * - an untracked/uninit-ed object is found /Users/rubber/linux/kernel/time/timer.c: 697
 * init_timer_key - initialize a timer /Users/rubber/linux/kernel/time/timer.c: 801
 * @timer: the timer to be initialized /Users/rubber/linux/kernel/time/timer.c: 802
 * @func: timer callback function /Users/rubber/linux/kernel/time/timer.c: 803
 * @flags: timer flags /Users/rubber/linux/kernel/time/timer.c: 804
 * @name: name of the timer /Users/rubber/linux/kernel/time/timer.c: 805
 * @key: lockdep class key of the fake lock used for tracking timer /Users/rubber/linux/kernel/time/timer.c: 806
 *       sync lock dependencies /Users/rubber/linux/kernel/time/timer.c: 807
 * init_timer_key() must be done to a timer prior calling *any* of the /Users/rubber/linux/kernel/time/timer.c: 809
 * other timer functions. /Users/rubber/linux/kernel/time/timer.c: 810
	/* /Users/rubber/linux/kernel/time/timer.c: 854
	 * If the timer is deferrable and NO_HZ_COMMON is set then we need /Users/rubber/linux/kernel/time/timer.c: 855
	 * to use the deferrable base. /Users/rubber/linux/kernel/time/timer.c: 856
	/* /Users/rubber/linux/kernel/time/timer.c: 867
	 * If the timer is deferrable and NO_HZ_COMMON is set then we need /Users/rubber/linux/kernel/time/timer.c: 868
	 * to use the deferrable base. /Users/rubber/linux/kernel/time/timer.c: 869
	/* /Users/rubber/linux/kernel/time/timer.c: 896
	 * No need to forward if we are close enough below jiffies. /Users/rubber/linux/kernel/time/timer.c: 897
	 * Also while executing timers, base->clk is 1 offset ahead /Users/rubber/linux/kernel/time/timer.c: 898
	 * of jiffies to avoid endless requeuing to current jiffies. /Users/rubber/linux/kernel/time/timer.c: 899
	/* /Users/rubber/linux/kernel/time/timer.c: 904
	 * If the next expiry value is > jiffies, then we fast forward to /Users/rubber/linux/kernel/time/timer.c: 905
	 * jiffies otherwise we forward to the next expiry value. /Users/rubber/linux/kernel/time/timer.c: 906
 * We are using hashed locking: Holding per_cpu(timer_bases[x]).lock means /Users/rubber/linux/kernel/time/timer.c: 919
 * that all timers which are tied to this base are locked, and the base itself /Users/rubber/linux/kernel/time/timer.c: 920
 * is locked too. /Users/rubber/linux/kernel/time/timer.c: 921
 * So __run_timers/migrate_timers can safely modify all timers which could /Users/rubber/linux/kernel/time/timer.c: 923
 * be found in the base->vectors array. /Users/rubber/linux/kernel/time/timer.c: 924
 * When a timer is migrating then the TIMER_MIGRATING flag is set and we need /Users/rubber/linux/kernel/time/timer.c: 926
 * to wait until the migration is done. /Users/rubber/linux/kernel/time/timer.c: 927
		/* /Users/rubber/linux/kernel/time/timer.c: 937
		 * We need to use READ_ONCE() here, otherwise the compiler /Users/rubber/linux/kernel/time/timer.c: 938
		 * might re-read @tf between the check for TIMER_MIGRATING /Users/rubber/linux/kernel/time/timer.c: 939
		 * and spin_lock(). /Users/rubber/linux/kernel/time/timer.c: 940
	/* /Users/rubber/linux/kernel/time/timer.c: 969
	 * This is a common optimization triggered by the networking code - if /Users/rubber/linux/kernel/time/timer.c: 970
	 * the timer is re-modified to have the same timeout or ends up in the /Users/rubber/linux/kernel/time/timer.c: 971
	 * same array bucket then just return: /Users/rubber/linux/kernel/time/timer.c: 972
		/* /Users/rubber/linux/kernel/time/timer.c: 975
		 * The downside of this optimization is that it can result in /Users/rubber/linux/kernel/time/timer.c: 976
		 * larger granularity than you would get from adding a new /Users/rubber/linux/kernel/time/timer.c: 977
		 * timer with this expiry. /Users/rubber/linux/kernel/time/timer.c: 978
		/* /Users/rubber/linux/kernel/time/timer.c: 987
		 * We lock timer base and calculate the bucket index right /Users/rubber/linux/kernel/time/timer.c: 988
		 * here. If the timer ends up in the same bucket, then we /Users/rubber/linux/kernel/time/timer.c: 989
		 * just update the expiry time and avoid the whole /Users/rubber/linux/kernel/time/timer.c: 990
		 * dequeue/enqueue dance. /Users/rubber/linux/kernel/time/timer.c: 991
		/* /Users/rubber/linux/kernel/time/timer.c: 1005
		 * Retrieve and compare the array index of the pending /Users/rubber/linux/kernel/time/timer.c: 1006
		 * timer. If it matches set the expiry to the new value so a /Users/rubber/linux/kernel/time/timer.c: 1007
		 * subsequent call will exit in the expires check above. /Users/rubber/linux/kernel/time/timer.c: 1008
		/* /Users/rubber/linux/kernel/time/timer.c: 1030
		 * We are trying to schedule the timer on the new base. /Users/rubber/linux/kernel/time/timer.c: 1031
		 * However we can't change timer's base while it is running, /Users/rubber/linux/kernel/time/timer.c: 1032
		 * otherwise del_timer_sync() can't detect that the timer's /Users/rubber/linux/kernel/time/timer.c: 1033
		 * handler yet has not finished. This also guarantees that the /Users/rubber/linux/kernel/time/timer.c: 1034
		 * timer is serialized wrt itself. /Users/rubber/linux/kernel/time/timer.c: 1035
	/* /Users/rubber/linux/kernel/time/timer.c: 1053
	 * If 'idx' was calculated above and the base time did not advance /Users/rubber/linux/kernel/time/timer.c: 1054
	 * between calculating 'idx' and possibly switching the base, only /Users/rubber/linux/kernel/time/timer.c: 1055
	 * enqueue_timer() is required. Otherwise we need to (re)calculate /Users/rubber/linux/kernel/time/timer.c: 1056
	 * the wheel index via internal_add_timer(). /Users/rubber/linux/kernel/time/timer.c: 1057
 * mod_timer_pending - modify a pending timer's timeout /Users/rubber/linux/kernel/time/timer.c: 1071
 * @timer: the pending timer to be modified /Users/rubber/linux/kernel/time/timer.c: 1072
 * @expires: new timeout in jiffies /Users/rubber/linux/kernel/time/timer.c: 1073
 * mod_timer_pending() is the same for pending timers as mod_timer(), /Users/rubber/linux/kernel/time/timer.c: 1075
 * but will not re-activate and modify already deleted timers. /Users/rubber/linux/kernel/time/timer.c: 1076
 * It is useful for unserialized use of timers. /Users/rubber/linux/kernel/time/timer.c: 1078
 * mod_timer - modify a timer's timeout /Users/rubber/linux/kernel/time/timer.c: 1087
 * @timer: the timer to be modified /Users/rubber/linux/kernel/time/timer.c: 1088
 * @expires: new timeout in jiffies /Users/rubber/linux/kernel/time/timer.c: 1089
 * mod_timer() is a more efficient way to update the expire field of an /Users/rubber/linux/kernel/time/timer.c: 1091
 * active timer (if the timer is inactive it will be activated) /Users/rubber/linux/kernel/time/timer.c: 1092
 * mod_timer(timer, expires) is equivalent to: /Users/rubber/linux/kernel/time/timer.c: 1094
 *     del_timer(timer); timer->expires = expires; add_timer(timer); /Users/rubber/linux/kernel/time/timer.c: 1096
 * Note that if there are multiple unserialized concurrent users of the /Users/rubber/linux/kernel/time/timer.c: 1098
 * same timer, then mod_timer() is the only safe way to modify the timeout, /Users/rubber/linux/kernel/time/timer.c: 1099
 * since add_timer() cannot modify an already running timer. /Users/rubber/linux/kernel/time/timer.c: 1100
 * The function returns whether it has modified a pending timer or not. /Users/rubber/linux/kernel/time/timer.c: 1102
 * (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an /Users/rubber/linux/kernel/time/timer.c: 1103
 * active timer returns 1.) /Users/rubber/linux/kernel/time/timer.c: 1104
 * timer_reduce - Modify a timer's timeout if it would reduce the timeout /Users/rubber/linux/kernel/time/timer.c: 1113
 * @timer:	The timer to be modified /Users/rubber/linux/kernel/time/timer.c: 1114
 * @expires:	New timeout in jiffies /Users/rubber/linux/kernel/time/timer.c: 1115
 * timer_reduce() is very similar to mod_timer(), except that it will only /Users/rubber/linux/kernel/time/timer.c: 1117
 * modify a running timer if that would reduce the expiration time (it will /Users/rubber/linux/kernel/time/timer.c: 1118
 * start a timer that isn't running). /Users/rubber/linux/kernel/time/timer.c: 1119
 * add_timer - start a timer /Users/rubber/linux/kernel/time/timer.c: 1128
 * @timer: the timer to be added /Users/rubber/linux/kernel/time/timer.c: 1129
 * The kernel will do a ->function(@timer) callback from the /Users/rubber/linux/kernel/time/timer.c: 1131
 * timer interrupt at the ->expires point in the future. The /Users/rubber/linux/kernel/time/timer.c: 1132
 * current time is 'jiffies'. /Users/rubber/linux/kernel/time/timer.c: 1133
 * The timer's ->expires, ->function fields must be set prior calling this /Users/rubber/linux/kernel/time/timer.c: 1135
 * function. /Users/rubber/linux/kernel/time/timer.c: 1136
 * Timers with an ->expires field in the past will be executed in the next /Users/rubber/linux/kernel/time/timer.c: 1138
 * timer tick. /Users/rubber/linux/kernel/time/timer.c: 1139
 * add_timer_on - start a timer on a particular CPU /Users/rubber/linux/kernel/time/timer.c: 1149
 * @timer: the timer to be added /Users/rubber/linux/kernel/time/timer.c: 1150
 * @cpu: the CPU to start it on /Users/rubber/linux/kernel/time/timer.c: 1151
 * This is not very scalable on SMP. Double adds are not possible. /Users/rubber/linux/kernel/time/timer.c: 1153
	/* /Users/rubber/linux/kernel/time/timer.c: 1164
	 * If @timer was on a different CPU, it should be migrated with the /Users/rubber/linux/kernel/time/timer.c: 1165
	 * old base locked to prevent other operations proceeding with the /Users/rubber/linux/kernel/time/timer.c: 1166
	 * wrong base locked.  See lock_timer_base(). /Users/rubber/linux/kernel/time/timer.c: 1167
 * del_timer - deactivate a timer. /Users/rubber/linux/kernel/time/timer.c: 1188
 * @timer: the timer to be deactivated /Users/rubber/linux/kernel/time/timer.c: 1189
 * del_timer() deactivates a timer - this works on both active and inactive /Users/rubber/linux/kernel/time/timer.c: 1191
 * timers. /Users/rubber/linux/kernel/time/timer.c: 1192
 * The function returns whether it has deactivated a pending timer or not. /Users/rubber/linux/kernel/time/timer.c: 1194
 * (ie. del_timer() of an inactive timer returns 0, del_timer() of an /Users/rubber/linux/kernel/time/timer.c: 1195
 * active timer returns 1.) /Users/rubber/linux/kernel/time/timer.c: 1196
 * try_to_del_timer_sync - Try to deactivate a timer /Users/rubber/linux/kernel/time/timer.c: 1217
 * @timer: timer to delete /Users/rubber/linux/kernel/time/timer.c: 1218
 * This function tries to deactivate a timer. Upon successful (ret >= 0) /Users/rubber/linux/kernel/time/timer.c: 1220
 * exit the timer is not queued and the handler is not running on any CPU. /Users/rubber/linux/kernel/time/timer.c: 1221
 * The counterpart to del_timer_wait_running(). /Users/rubber/linux/kernel/time/timer.c: 1259
 * If there is a waiter for base->expiry_lock, then it was waiting for the /Users/rubber/linux/kernel/time/timer.c: 1261
 * timer callback to finish. Drop expiry_lock and reacquire it. That allows /Users/rubber/linux/kernel/time/timer.c: 1262
 * the waiter to acquire the lock and make progress. /Users/rubber/linux/kernel/time/timer.c: 1263
 * This function is called on PREEMPT_RT kernels when the fast path /Users/rubber/linux/kernel/time/timer.c: 1276
 * deletion of a timer failed because the timer callback function was /Users/rubber/linux/kernel/time/timer.c: 1277
 * running. /Users/rubber/linux/kernel/time/timer.c: 1278
 * This prevents priority inversion, if the softirq thread on a remote CPU /Users/rubber/linux/kernel/time/timer.c: 1280
 * got preempted, and it prevents a life lock when the task which tries to /Users/rubber/linux/kernel/time/timer.c: 1281
 * delete a timer preempted the softirq thread running the timer callback /Users/rubber/linux/kernel/time/timer.c: 1282
 * function. /Users/rubber/linux/kernel/time/timer.c: 1283
		/* /Users/rubber/linux/kernel/time/timer.c: 1293
		 * Mark the base as contended and grab the expiry lock, /Users/rubber/linux/kernel/time/timer.c: 1294
		 * which is held by the softirq across the timer /Users/rubber/linux/kernel/time/timer.c: 1295
		 * callback. Drop the lock immediately so the softirq can /Users/rubber/linux/kernel/time/timer.c: 1296
		 * expire the next timer. In theory the timer could already /Users/rubber/linux/kernel/time/timer.c: 1297
		 * be running again, but that's more than unlikely and just /Users/rubber/linux/kernel/time/timer.c: 1298
		 * causes another wait loop. /Users/rubber/linux/kernel/time/timer.c: 1299
 * del_timer_sync - deactivate a timer and wait for the handler to finish. /Users/rubber/linux/kernel/time/timer.c: 1317
 * @timer: the timer to be deactivated /Users/rubber/linux/kernel/time/timer.c: 1318
 * This function only differs from del_timer() on SMP: besides deactivating /Users/rubber/linux/kernel/time/timer.c: 1320
 * the timer it also makes sure the handler has finished executing on other /Users/rubber/linux/kernel/time/timer.c: 1321
 * CPUs. /Users/rubber/linux/kernel/time/timer.c: 1322
 * Synchronization rules: Callers must prevent restarting of the timer, /Users/rubber/linux/kernel/time/timer.c: 1324
 * otherwise this function is meaningless. It must not be called from /Users/rubber/linux/kernel/time/timer.c: 1325
 * interrupt contexts unless the timer is an irqsafe one. The caller must /Users/rubber/linux/kernel/time/timer.c: 1326
 * not hold locks which would prevent completion of the timer's /Users/rubber/linux/kernel/time/timer.c: 1327
 * handler. The timer's handler must not call add_timer_on(). Upon exit the /Users/rubber/linux/kernel/time/timer.c: 1328
 * timer is not queued and the handler is not running on any CPU. /Users/rubber/linux/kernel/time/timer.c: 1329
 * Note: For !irqsafe timers, you must not hold locks that are held in /Users/rubber/linux/kernel/time/timer.c: 1331
 *   interrupt context while calling this function. Even if the lock has /Users/rubber/linux/kernel/time/timer.c: 1332
 *   nothing to do with the timer in question.  Here's why:: /Users/rubber/linux/kernel/time/timer.c: 1333
 *    CPU0                             CPU1 /Users/rubber/linux/kernel/time/timer.c: 1335
 *    ----                             ---- /Users/rubber/linux/kernel/time/timer.c: 1336
 *                                     <SOFTIRQ> /Users/rubber/linux/kernel/time/timer.c: 1337
 *                                       call_timer_fn(); /Users/rubber/linux/kernel/time/timer.c: 1338
 *                                       base->running_timer = mytimer; /Users/rubber/linux/kernel/time/timer.c: 1339
 *    spin_lock_irq(somelock); /Users/rubber/linux/kernel/time/timer.c: 1340
 *                                     <IRQ> /Users/rubber/linux/kernel/time/timer.c: 1341
 *                                        spin_lock(somelock); /Users/rubber/linux/kernel/time/timer.c: 1342
 *    del_timer_sync(mytimer); /Users/rubber/linux/kernel/time/timer.c: 1343
 *    while (base->running_timer == mytimer); /Users/rubber/linux/kernel/time/timer.c: 1344
 * Now del_timer_sync() will never return and never release somelock. /Users/rubber/linux/kernel/time/timer.c: 1346
 * The interrupt on the other CPU is waiting to grab somelock but /Users/rubber/linux/kernel/time/timer.c: 1347
 * it has interrupted the softirq that CPU0 is waiting to finish. /Users/rubber/linux/kernel/time/timer.c: 1348
 * The function returns whether it has deactivated a pending timer or not. /Users/rubber/linux/kernel/time/timer.c: 1350
	/* /Users/rubber/linux/kernel/time/timer.c: 1359
	 * If lockdep gives a backtrace here, please reference /Users/rubber/linux/kernel/time/timer.c: 1360
	 * the synchronization rules above. /Users/rubber/linux/kernel/time/timer.c: 1361
	/* /Users/rubber/linux/kernel/time/timer.c: 1368
	 * don't use it in hardirq context, because it /Users/rubber/linux/kernel/time/timer.c: 1369
	 * could lead to deadlock. /Users/rubber/linux/kernel/time/timer.c: 1370
	/* /Users/rubber/linux/kernel/time/timer.c: 1374
	 * Must be able to sleep on PREEMPT_RT because of the slowpath in /Users/rubber/linux/kernel/time/timer.c: 1375
	 * del_timer_wait_running(). /Users/rubber/linux/kernel/time/timer.c: 1376
	/* /Users/rubber/linux/kernel/time/timer.c: 1402
	 * It is permissible to free the timer from inside the /Users/rubber/linux/kernel/time/timer.c: 1403
	 * function that is called from it, this we need to take into /Users/rubber/linux/kernel/time/timer.c: 1404
	 * account for lockdep too. To avoid bogus "held lock freed" /Users/rubber/linux/kernel/time/timer.c: 1405
	 * warnings as well as problems when looking into /Users/rubber/linux/kernel/time/timer.c: 1406
	 * timer->lockdep_map, make a copy and use that here. /Users/rubber/linux/kernel/time/timer.c: 1407
	/* /Users/rubber/linux/kernel/time/timer.c: 1413
	 * Couple the lock chain with the lock chain at /Users/rubber/linux/kernel/time/timer.c: 1414
	 * del_timer_sync() by acquiring the lock_map around the fn() /Users/rubber/linux/kernel/time/timer.c: 1415
	 * call here and in del_timer_sync(). /Users/rubber/linux/kernel/time/timer.c: 1416
		/* /Users/rubber/linux/kernel/time/timer.c: 1429
		 * Restore the preempt count. That gives us a decent /Users/rubber/linux/kernel/time/timer.c: 1430
		 * chance to survive and extract information. If the /Users/rubber/linux/kernel/time/timer.c: 1431
		 * callback kept a lock held, bad luck, but not worse /Users/rubber/linux/kernel/time/timer.c: 1432
		 * than the BUG() we had. /Users/rubber/linux/kernel/time/timer.c: 1433
	/* /Users/rubber/linux/kernel/time/timer.c: 1441
	 * This value is required only for tracing. base->clk was /Users/rubber/linux/kernel/time/timer.c: 1442
	 * incremented directly before expire_timers was called. But expiry /Users/rubber/linux/kernel/time/timer.c: 1443
	 * is related to the old base->clk value. /Users/rubber/linux/kernel/time/timer.c: 1444
 * Find the next pending bucket of a level. Search from level start (@offset) /Users/rubber/linux/kernel/time/timer.c: 1500
 * + @clk upwards and if nothing there, search from start of the level /Users/rubber/linux/kernel/time/timer.c: 1501
 * (@offset) up to @offset + clk. /Users/rubber/linux/kernel/time/timer.c: 1502
 * Search the first expiring timer in the various clock levels. Caller must /Users/rubber/linux/kernel/time/timer.c: 1519
 * hold base->lock. /Users/rubber/linux/kernel/time/timer.c: 1520
			/* /Users/rubber/linux/kernel/time/timer.c: 1540
			 * If the next expiration happens before we reach /Users/rubber/linux/kernel/time/timer.c: 1541
			 * the next level, no need to check further. /Users/rubber/linux/kernel/time/timer.c: 1542
		/* /Users/rubber/linux/kernel/time/timer.c: 1547
		 * Clock for the next level. If the current level clock lower /Users/rubber/linux/kernel/time/timer.c: 1548
		 * bits are zero, we look at the next level as is. If not we /Users/rubber/linux/kernel/time/timer.c: 1549
		 * need to advance it by one because that's going to be the /Users/rubber/linux/kernel/time/timer.c: 1550
		 * next expiring bucket in that level. base->clk is the next /Users/rubber/linux/kernel/time/timer.c: 1551
		 * expiring jiffie. So in case of: /Users/rubber/linux/kernel/time/timer.c: 1552
		 * /Users/rubber/linux/kernel/time/timer.c: 1553
		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0 /Users/rubber/linux/kernel/time/timer.c: 1554
		 *  0    0    0    0    0    0 /Users/rubber/linux/kernel/time/timer.c: 1555
		 * /Users/rubber/linux/kernel/time/timer.c: 1556
		 * we have to look at all levels @index 0. With /Users/rubber/linux/kernel/time/timer.c: 1557
		 * /Users/rubber/linux/kernel/time/timer.c: 1558
		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0 /Users/rubber/linux/kernel/time/timer.c: 1559
		 *  0    0    0    0    0    2 /Users/rubber/linux/kernel/time/timer.c: 1560
		 * /Users/rubber/linux/kernel/time/timer.c: 1561
		 * LVL0 has the next expiring bucket @index 2. The upper /Users/rubber/linux/kernel/time/timer.c: 1562
		 * levels have the next expiring bucket @index 1. /Users/rubber/linux/kernel/time/timer.c: 1563
		 * /Users/rubber/linux/kernel/time/timer.c: 1564
		 * In case that the propagation wraps the next level the same /Users/rubber/linux/kernel/time/timer.c: 1565
		 * rules apply: /Users/rubber/linux/kernel/time/timer.c: 1566
		 * /Users/rubber/linux/kernel/time/timer.c: 1567
		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0 /Users/rubber/linux/kernel/time/timer.c: 1568
		 *  0    0    0    0    F    2 /Users/rubber/linux/kernel/time/timer.c: 1569
		 * /Users/rubber/linux/kernel/time/timer.c: 1570
		 * So after looking at LVL0 we get: /Users/rubber/linux/kernel/time/timer.c: 1571
		 * /Users/rubber/linux/kernel/time/timer.c: 1572
		 * LVL5 LVL4 LVL3 LVL2 LVL1 /Users/rubber/linux/kernel/time/timer.c: 1573
		 *  0    0    0    1    0 /Users/rubber/linux/kernel/time/timer.c: 1574
		 * /Users/rubber/linux/kernel/time/timer.c: 1575
		 * So no propagation from LVL1 to LVL2 because that happened /Users/rubber/linux/kernel/time/timer.c: 1576
		 * with the add already, but then we need to propagate further /Users/rubber/linux/kernel/time/timer.c: 1577
		 * from LVL2 to LVL3. /Users/rubber/linux/kernel/time/timer.c: 1578
		 * /Users/rubber/linux/kernel/time/timer.c: 1579
		 * So the simple check whether the lower bits of the current /Users/rubber/linux/kernel/time/timer.c: 1580
		 * level are 0 or not is sufficient for all cases. /Users/rubber/linux/kernel/time/timer.c: 1581
 * Check, if the next hrtimer event is before the next timer wheel /Users/rubber/linux/kernel/time/timer.c: 1596
 * event: /Users/rubber/linux/kernel/time/timer.c: 1597
	/* /Users/rubber/linux/kernel/time/timer.c: 1603
	 * If high resolution timers are enabled /Users/rubber/linux/kernel/time/timer.c: 1604
	 * hrtimer_get_next_event() returns KTIME_MAX. /Users/rubber/linux/kernel/time/timer.c: 1605
	/* /Users/rubber/linux/kernel/time/timer.c: 1610
	 * If the next timer is already expired, return the tick base /Users/rubber/linux/kernel/time/timer.c: 1611
	 * time so the tick is fired immediately. /Users/rubber/linux/kernel/time/timer.c: 1612
	/* /Users/rubber/linux/kernel/time/timer.c: 1617
	 * Round up to the next jiffie. High resolution timers are /Users/rubber/linux/kernel/time/timer.c: 1618
	 * off, so the hrtimers are expired in the tick and we need to /Users/rubber/linux/kernel/time/timer.c: 1619
	 * make sure that this tick really expires the timer to avoid /Users/rubber/linux/kernel/time/timer.c: 1620
	 * a ping pong of the nohz stop code. /Users/rubber/linux/kernel/time/timer.c: 1621
	 * /Users/rubber/linux/kernel/time/timer.c: 1622
	 * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3 /Users/rubber/linux/kernel/time/timer.c: 1623
 * get_next_timer_interrupt - return the time (clock mono) of the next timer /Users/rubber/linux/kernel/time/timer.c: 1629
 * @basej:	base time jiffies /Users/rubber/linux/kernel/time/timer.c: 1630
 * @basem:	base time clock monotonic /Users/rubber/linux/kernel/time/timer.c: 1631
 * Returns the tick aligned clock monotonic time of the next pending /Users/rubber/linux/kernel/time/timer.c: 1633
 * timer or KTIME_MAX if no timer is pending. /Users/rubber/linux/kernel/time/timer.c: 1634
	/* /Users/rubber/linux/kernel/time/timer.c: 1642
	 * Pretend that there is no timer pending if the cpu is offline. /Users/rubber/linux/kernel/time/timer.c: 1643
	 * Possible pending timers will be migrated later to an active cpu. /Users/rubber/linux/kernel/time/timer.c: 1644
	/* /Users/rubber/linux/kernel/time/timer.c: 1654
	 * We have a fresh next event. Check whether we can forward the /Users/rubber/linux/kernel/time/timer.c: 1655
	 * base. We can only do that when @basej is past base->clk /Users/rubber/linux/kernel/time/timer.c: 1656
	 * otherwise we might rewind base->clk. /Users/rubber/linux/kernel/time/timer.c: 1657
		/* /Users/rubber/linux/kernel/time/timer.c: 1672
		 * If we expect to sleep more than a tick, mark the base idle. /Users/rubber/linux/kernel/time/timer.c: 1673
		 * Also the tick is stopped so any added timer must forward /Users/rubber/linux/kernel/time/timer.c: 1674
		 * the base clk itself to keep granularity small. This idle /Users/rubber/linux/kernel/time/timer.c: 1675
		 * logic is only maintained for the BASE_STD base, deferrable /Users/rubber/linux/kernel/time/timer.c: 1676
		 * timers may still see large granularity skew (by design). /Users/rubber/linux/kernel/time/timer.c: 1677
 * timer_clear_idle - Clear the idle state of the timer base /Users/rubber/linux/kernel/time/timer.c: 1688
 * Called with interrupts disabled /Users/rubber/linux/kernel/time/timer.c: 1690
	/* /Users/rubber/linux/kernel/time/timer.c: 1696
	 * We do this unlocked. The worst outcome is a remote enqueue sending /Users/rubber/linux/kernel/time/timer.c: 1697
	 * a pointless IPI, but taking the lock would just make the window for /Users/rubber/linux/kernel/time/timer.c: 1698
	 * sending the IPI a few instructions smaller for the cost of taking /Users/rubber/linux/kernel/time/timer.c: 1699
	 * the lock in the exit from idle path. /Users/rubber/linux/kernel/time/timer.c: 1700
 * __run_timers - run all expired timers (if any) on this CPU. /Users/rubber/linux/kernel/time/timer.c: 1707
 * @base: the timer vector to be processed. /Users/rubber/linux/kernel/time/timer.c: 1708
		/* /Users/rubber/linux/kernel/time/timer.c: 1724
		 * The only possible reason for not finding any expired /Users/rubber/linux/kernel/time/timer.c: 1725
		 * timer at this clk is that all matching timers have been /Users/rubber/linux/kernel/time/timer.c: 1726
		 * dequeued. /Users/rubber/linux/kernel/time/timer.c: 1727
 * This function runs timers and the timer-tq in bottom half context. /Users/rubber/linux/kernel/time/timer.c: 1741
 * Called by the local, per-CPU timer interrupt on SMP. /Users/rubber/linux/kernel/time/timer.c: 1753
 * Called from the timer interrupt handler to charge one tick to the current /Users/rubber/linux/kernel/time/timer.c: 1773
 * process.  user_tick is 1 if the tick is user time, 0 for system. /Users/rubber/linux/kernel/time/timer.c: 1774
 * Since schedule_timeout()'s timer is defined on the stack, it must store /Users/rubber/linux/kernel/time/timer.c: 1796
 * the target task on the stack as well. /Users/rubber/linux/kernel/time/timer.c: 1797
 * schedule_timeout - sleep until timeout /Users/rubber/linux/kernel/time/timer.c: 1812
 * @timeout: timeout value in jiffies /Users/rubber/linux/kernel/time/timer.c: 1813
 * Make the current task sleep until @timeout jiffies have elapsed. /Users/rubber/linux/kernel/time/timer.c: 1815
 * The function behavior depends on the current task state /Users/rubber/linux/kernel/time/timer.c: 1816
 * (see also set_current_state() description): /Users/rubber/linux/kernel/time/timer.c: 1817
 * %TASK_RUNNING - the scheduler is called, but the task does not sleep /Users/rubber/linux/kernel/time/timer.c: 1819
 * at all. That happens because sched_submit_work() does nothing for /Users/rubber/linux/kernel/time/timer.c: 1820
 * tasks in %TASK_RUNNING state. /Users/rubber/linux/kernel/time/timer.c: 1821
 * %TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to /Users/rubber/linux/kernel/time/timer.c: 1823
 * pass before the routine returns unless the current task is explicitly /Users/rubber/linux/kernel/time/timer.c: 1824
 * woken up, (e.g. by wake_up_process()). /Users/rubber/linux/kernel/time/timer.c: 1825
 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is /Users/rubber/linux/kernel/time/timer.c: 1827
 * delivered to the current task or the current task is explicitly woken /Users/rubber/linux/kernel/time/timer.c: 1828
 * up. /Users/rubber/linux/kernel/time/timer.c: 1829
 * The current task state is guaranteed to be %TASK_RUNNING when this /Users/rubber/linux/kernel/time/timer.c: 1831
 * routine returns. /Users/rubber/linux/kernel/time/timer.c: 1832
 * Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule /Users/rubber/linux/kernel/time/timer.c: 1834
 * the CPU away without a bound on the timeout. In this case the return /Users/rubber/linux/kernel/time/timer.c: 1835
 * value will be %MAX_SCHEDULE_TIMEOUT. /Users/rubber/linux/kernel/time/timer.c: 1836
 * Returns 0 when the timer has expired otherwise the remaining time in /Users/rubber/linux/kernel/time/timer.c: 1838
 * jiffies will be returned. In all cases the return value is guaranteed /Users/rubber/linux/kernel/time/timer.c: 1839
 * to be non-negative. /Users/rubber/linux/kernel/time/timer.c: 1840
		/* /Users/rubber/linux/kernel/time/timer.c: 1850
		 * These two special cases are useful to be comfortable /Users/rubber/linux/kernel/time/timer.c: 1851
		 * in the caller. Nothing more. We could take /Users/rubber/linux/kernel/time/timer.c: 1852
		 * MAX_SCHEDULE_TIMEOUT from one of the negative value /Users/rubber/linux/kernel/time/timer.c: 1853
		 * but I' d like to return a valid offset (>=0) to allow /Users/rubber/linux/kernel/time/timer.c: 1854
		 * the caller to do everything it want with the retval. /Users/rubber/linux/kernel/time/timer.c: 1855
		/* /Users/rubber/linux/kernel/time/timer.c: 1860
		 * Another bit of PARANOID. Note that the retval will be /Users/rubber/linux/kernel/time/timer.c: 1861
		 * 0 since no piece of kernel is supposed to do a check /Users/rubber/linux/kernel/time/timer.c: 1862
		 * for a negative retval of schedule_timeout() (since it /Users/rubber/linux/kernel/time/timer.c: 1863
		 * should never happens anyway). You just have the printk() /Users/rubber/linux/kernel/time/timer.c: 1864
		 * that will tell you if something is gone wrong and where. /Users/rubber/linux/kernel/time/timer.c: 1865
 * We can use __set_current_state() here because schedule_timeout() calls /Users/rubber/linux/kernel/time/timer.c: 1895
 * schedule() unconditionally. /Users/rubber/linux/kernel/time/timer.c: 1896
 * Like schedule_timeout_uninterruptible(), except this task will not contribute /Users/rubber/linux/kernel/time/timer.c: 1920
 * to load average. /Users/rubber/linux/kernel/time/timer.c: 1921
		/* /Users/rubber/linux/kernel/time/timer.c: 1970
		 * The caller is globally serialized and nobody else /Users/rubber/linux/kernel/time/timer.c: 1971
		 * takes two locks at once, deadlock is not possible. /Users/rubber/linux/kernel/time/timer.c: 1972
		/* /Users/rubber/linux/kernel/time/timer.c: 1977
		 * The current CPUs base clock might be stale. Update it /Users/rubber/linux/kernel/time/timer.c: 1978
		 * before moving the timers over. /Users/rubber/linux/kernel/time/timer.c: 1979
 * msleep - sleep safely even with waitqueue interruptions /Users/rubber/linux/kernel/time/timer.c: 2028
 * @msecs: Time in milliseconds to sleep for /Users/rubber/linux/kernel/time/timer.c: 2029
 * msleep_interruptible - sleep waiting for signals /Users/rubber/linux/kernel/time/timer.c: 2042
 * @msecs: Time in milliseconds to sleep for /Users/rubber/linux/kernel/time/timer.c: 2043
 * usleep_range - Sleep for an approximate time /Users/rubber/linux/kernel/time/timer.c: 2057
 * @min: Minimum time in usecs to sleep /Users/rubber/linux/kernel/time/timer.c: 2058
 * @max: Maximum time in usecs to sleep /Users/rubber/linux/kernel/time/timer.c: 2059
 * In non-atomic context where the exact wakeup time is flexible, use /Users/rubber/linux/kernel/time/timer.c: 2061
 * usleep_range() instead of udelay().  The sleep improves responsiveness /Users/rubber/linux/kernel/time/timer.c: 2062
 * by avoiding the CPU-hogging busy-wait of udelay(), and the range reduces /Users/rubber/linux/kernel/time/timer.c: 2063
 * power usage by allowing hrtimers to take advantage of an already- /Users/rubber/linux/kernel/time/timer.c: 2064
 * scheduled interrupt instead of scheduling a new one just for this sleep. /Users/rubber/linux/kernel/time/timer.c: 2065
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/timecounter.c: 1
 * Based on clocksource code. See commit 74d23cc704d1 /Users/rubber/linux/kernel/time/timecounter.c: 3
 * timecounter_read_delta - get nanoseconds since last call of this function /Users/rubber/linux/kernel/time/timecounter.c: 21
 * @tc:         Pointer to time counter /Users/rubber/linux/kernel/time/timecounter.c: 22
 * When the underlying cycle counter runs over, this will be handled /Users/rubber/linux/kernel/time/timecounter.c: 24
 * correctly as long as it does not run over more than once between /Users/rubber/linux/kernel/time/timecounter.c: 25
 * calls. /Users/rubber/linux/kernel/time/timecounter.c: 26
 * The first call to this function for a new time counter initializes /Users/rubber/linux/kernel/time/timecounter.c: 28
 * the time tracking and returns an undefined result. /Users/rubber/linux/kernel/time/timecounter.c: 29
 * This is like cyclecounter_cyc2ns(), but it is used for computing a /Users/rubber/linux/kernel/time/timecounter.c: 66
 * time previous to the time stored in the cycle counter. /Users/rubber/linux/kernel/time/timecounter.c: 67
	/* /Users/rubber/linux/kernel/time/timecounter.c: 85
	 * Instead of always treating cycle_tstamp as more recent /Users/rubber/linux/kernel/time/timecounter.c: 86
	 * than tc->cycle_last, detect when it is too far in the /Users/rubber/linux/kernel/time/timecounter.c: 87
	 * future and treat it as old time stamp instead. /Users/rubber/linux/kernel/time/timecounter.c: 88
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-broadcast.c: 1
 * This file contains functions which emulate a local clock-event /Users/rubber/linux/kernel/time/tick-broadcast.c: 3
 * device via a broadcast event source. /Users/rubber/linux/kernel/time/tick-broadcast.c: 4
 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/tick-broadcast.c: 6
 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/tick-broadcast.c: 7
 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/tick-broadcast.c: 8
 * Broadcast support for broken x86 hardware, where the local apic /Users/rubber/linux/kernel/time/tick-broadcast.c: 23
 * timer stops in C3 state. /Users/rubber/linux/kernel/time/tick-broadcast.c: 24
 * Debugging: see timer_list.c /Users/rubber/linux/kernel/time/tick-broadcast.c: 54
 * Start the device in periodic mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 74
 * Check, if the device can be utilized as broadcast device: /Users/rubber/linux/kernel/time/tick-broadcast.c: 83
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 108
	 * If we woke up early and the tick was reprogrammed in the /Users/rubber/linux/kernel/time/tick-broadcast.c: 109
	 * meantime then this may be spurious but harmless. /Users/rubber/linux/kernel/time/tick-broadcast.c: 110
 * Conditionally install/replace broadcast device /Users/rubber/linux/kernel/time/tick-broadcast.c: 160
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 185
	 * If the system already runs in oneshot mode, switch the newly /Users/rubber/linux/kernel/time/tick-broadcast.c: 186
	 * registered broadcast device to oneshot mode explicitly. /Users/rubber/linux/kernel/time/tick-broadcast.c: 187
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 194
	 * Inform all cpus about this. We might be in a situation /Users/rubber/linux/kernel/time/tick-broadcast.c: 195
	 * where we did not switch to oneshot mode because the per cpu /Users/rubber/linux/kernel/time/tick-broadcast.c: 196
	 * devices are affected by CLOCK_EVT_FEAT_C3STOP and the lack /Users/rubber/linux/kernel/time/tick-broadcast.c: 197
	 * of a oneshot capable broadcast device. Without that /Users/rubber/linux/kernel/time/tick-broadcast.c: 198
	 * notification the systems stays stuck in periodic mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 199
	 * forever. /Users/rubber/linux/kernel/time/tick-broadcast.c: 200
 * Check, if the device is the broadcast device /Users/rubber/linux/kernel/time/tick-broadcast.c: 206
 * Check, if the device is dysfunctional and a placeholder, which /Users/rubber/linux/kernel/time/tick-broadcast.c: 243
 * needs to be handled by the broadcast device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 244
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 254
	 * Devices might be registered with both periodic and oneshot /Users/rubber/linux/kernel/time/tick-broadcast.c: 255
	 * mode disabled. This signals, that the device needs to be /Users/rubber/linux/kernel/time/tick-broadcast.c: 256
	 * operated from the broadcast device and is a placeholder for /Users/rubber/linux/kernel/time/tick-broadcast.c: 257
	 * the cpu local device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 258
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 270
		 * Clear the broadcast bit for this cpu if the /Users/rubber/linux/kernel/time/tick-broadcast.c: 271
		 * device is not power state affected. /Users/rubber/linux/kernel/time/tick-broadcast.c: 272
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 279
		 * Clear the broadcast bit if the CPU is not in /Users/rubber/linux/kernel/time/tick-broadcast.c: 280
		 * periodic broadcast on state. /Users/rubber/linux/kernel/time/tick-broadcast.c: 281
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 288
			 * If the system is in oneshot mode we can /Users/rubber/linux/kernel/time/tick-broadcast.c: 289
			 * unconditionally clear the oneshot mask bit, /Users/rubber/linux/kernel/time/tick-broadcast.c: 290
			 * because the CPU is running and therefore /Users/rubber/linux/kernel/time/tick-broadcast.c: 291
			 * not in an idle state which causes the power /Users/rubber/linux/kernel/time/tick-broadcast.c: 292
			 * state affected device to stop. Let the /Users/rubber/linux/kernel/time/tick-broadcast.c: 293
			 * caller initialize the device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 294
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 301
			 * If the system is in periodic mode, check /Users/rubber/linux/kernel/time/tick-broadcast.c: 302
			 * whether the broadcast device can be /Users/rubber/linux/kernel/time/tick-broadcast.c: 303
			 * switched off now. /Users/rubber/linux/kernel/time/tick-broadcast.c: 304
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 308
			 * If we kept the cpu in the broadcast mask, /Users/rubber/linux/kernel/time/tick-broadcast.c: 309
			 * tell the caller to leave the per cpu device /Users/rubber/linux/kernel/time/tick-broadcast.c: 310
			 * in shutdown state. The periodic interrupt /Users/rubber/linux/kernel/time/tick-broadcast.c: 311
			 * is delivered by the broadcast device, if /Users/rubber/linux/kernel/time/tick-broadcast.c: 312
			 * the broadcast device exists and is not /Users/rubber/linux/kernel/time/tick-broadcast.c: 313
			 * hrtimer based. /Users/rubber/linux/kernel/time/tick-broadcast.c: 314
 * Broadcast the event to the cpus, which are set in the mask (mangled). /Users/rubber/linux/kernel/time/tick-broadcast.c: 343
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 351
	 * Check, if the current cpu is in the mask /Users/rubber/linux/kernel/time/tick-broadcast.c: 352
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 358
		 * We only run the local handler, if the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 359
		 * device is not hrtimer based. Otherwise we run into /Users/rubber/linux/kernel/time/tick-broadcast.c: 360
		 * a hrtimer recursion. /Users/rubber/linux/kernel/time/tick-broadcast.c: 361
		 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 362
		 * local timer_interrupt() /Users/rubber/linux/kernel/time/tick-broadcast.c: 363
		 *   local_handler() /Users/rubber/linux/kernel/time/tick-broadcast.c: 364
		 *     expire_hrtimers() /Users/rubber/linux/kernel/time/tick-broadcast.c: 365
		 *       bc_handler() /Users/rubber/linux/kernel/time/tick-broadcast.c: 366
		 *         local_handler() /Users/rubber/linux/kernel/time/tick-broadcast.c: 367
		 *	     expire_hrtimers() /Users/rubber/linux/kernel/time/tick-broadcast.c: 368
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 374
		 * It might be necessary to actually check whether the devices /Users/rubber/linux/kernel/time/tick-broadcast.c: 375
		 * have different broadcast functions. For now, just use the /Users/rubber/linux/kernel/time/tick-broadcast.c: 376
		 * one of the first device. This works as long as we have this /Users/rubber/linux/kernel/time/tick-broadcast.c: 377
		 * misfeature only on x86 (lapic) /Users/rubber/linux/kernel/time/tick-broadcast.c: 378
 * Periodic broadcast: /Users/rubber/linux/kernel/time/tick-broadcast.c: 387
 * - invoke the broadcast handlers /Users/rubber/linux/kernel/time/tick-broadcast.c: 388
 * Event handler for periodic broadcast ticks /Users/rubber/linux/kernel/time/tick-broadcast.c: 397
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 421
	 * We run the handler of the local cpu after dropping /Users/rubber/linux/kernel/time/tick-broadcast.c: 422
	 * tick_broadcast_lock because the handler might deadlock when /Users/rubber/linux/kernel/time/tick-broadcast.c: 423
	 * trying to switch to oneshot mode. /Users/rubber/linux/kernel/time/tick-broadcast.c: 424
 * tick_broadcast_control - Enable/disable or force broadcast mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 431
 * @mode:	The selected broadcast mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 432
 * Called when the system enters a state where affected tick devices /Users/rubber/linux/kernel/time/tick-broadcast.c: 434
 * might stop. Note: TICK_BROADCAST_FORCE cannot be undone. /Users/rubber/linux/kernel/time/tick-broadcast.c: 435
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 449
	 * Is the device not affected by the powerstate ? /Users/rubber/linux/kernel/time/tick-broadcast.c: 450
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 469
			 * Only shutdown the cpu local device, if: /Users/rubber/linux/kernel/time/tick-broadcast.c: 470
			 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 471
			 * - the broadcast device exists /Users/rubber/linux/kernel/time/tick-broadcast.c: 472
			 * - the broadcast device is not a hrtimer based one /Users/rubber/linux/kernel/time/tick-broadcast.c: 473
			 * - the broadcast device is in periodic mode to /Users/rubber/linux/kernel/time/tick-broadcast.c: 474
			 *   avoid a hiccup during switch to oneshot mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 475
 * Set the periodic handler depending on broadcast on/off /Users/rubber/linux/kernel/time/tick-broadcast.c: 512
 * Remove a CPU from broadcasting /Users/rubber/linux/kernel/time/tick-broadcast.c: 534
 * This is called from tick_resume_local() on a resuming CPU. That's /Users/rubber/linux/kernel/time/tick-broadcast.c: 563
 * called from the core resume function, tick_unfreeze() and the magic XEN /Users/rubber/linux/kernel/time/tick-broadcast.c: 564
 * resume hackery. /Users/rubber/linux/kernel/time/tick-broadcast.c: 565
 * In none of these cases the broadcast device mode can change and the /Users/rubber/linux/kernel/time/tick-broadcast.c: 567
 * bit of the resuming CPU in the broadcast mask is safe as well. /Users/rubber/linux/kernel/time/tick-broadcast.c: 568
 * Exposed for debugging: see timer_list.c /Users/rubber/linux/kernel/time/tick-broadcast.c: 611
 * Called before going idle with interrupts disabled. Checks whether a /Users/rubber/linux/kernel/time/tick-broadcast.c: 619
 * broadcast event from the other core is about to happen. We detected /Users/rubber/linux/kernel/time/tick-broadcast.c: 620
 * that in tick_broadcast_oneshot_control(). The callsite can use this /Users/rubber/linux/kernel/time/tick-broadcast.c: 621
 * to avoid a deep idle transition as we are about to get the /Users/rubber/linux/kernel/time/tick-broadcast.c: 622
 * broadcast IPI right away. /Users/rubber/linux/kernel/time/tick-broadcast.c: 623
 * Set broadcast interrupt affinity /Users/rubber/linux/kernel/time/tick-broadcast.c: 631
 * Called from irq_enter() when idle was interrupted to reenable the /Users/rubber/linux/kernel/time/tick-broadcast.c: 662
 * per cpu device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 663
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 670
		 * We might be in the middle of switching over from /Users/rubber/linux/kernel/time/tick-broadcast.c: 671
		 * periodic to oneshot. If the CPU has not yet /Users/rubber/linux/kernel/time/tick-broadcast.c: 672
		 * switched over, leave the device alone. /Users/rubber/linux/kernel/time/tick-broadcast.c: 673
 * Handle oneshot mode broadcasting /Users/rubber/linux/kernel/time/tick-broadcast.c: 683
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 699
		 * Required for !SMP because for_each_cpu() reports /Users/rubber/linux/kernel/time/tick-broadcast.c: 700
		 * unconditionally CPU0 as set on UP kernels. /Users/rubber/linux/kernel/time/tick-broadcast.c: 701
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 710
			 * Mark the remote cpu in the pending mask, so /Users/rubber/linux/kernel/time/tick-broadcast.c: 711
			 * it can avoid reprogramming the cpu local /Users/rubber/linux/kernel/time/tick-broadcast.c: 712
			 * timer in tick_broadcast_oneshot_control(). /Users/rubber/linux/kernel/time/tick-broadcast.c: 713
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 722
	 * Remove the current cpu from the pending mask. The event is /Users/rubber/linux/kernel/time/tick-broadcast.c: 723
	 * delivered immediately in tick_do_broadcast() ! /Users/rubber/linux/kernel/time/tick-broadcast.c: 724
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 732
	 * Sanity check. Catch the case where we try to broadcast to /Users/rubber/linux/kernel/time/tick-broadcast.c: 733
	 * offline cpus. /Users/rubber/linux/kernel/time/tick-broadcast.c: 734
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 739
	 * Wakeup the cpus which have an expired event. /Users/rubber/linux/kernel/time/tick-broadcast.c: 740
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 744
	 * Two reasons for reprogram: /Users/rubber/linux/kernel/time/tick-broadcast.c: 745
	 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 746
	 * - The global event did not expire any CPU local /Users/rubber/linux/kernel/time/tick-broadcast.c: 747
	 * events. This happens in dyntick mode, as the maximum PIT /Users/rubber/linux/kernel/time/tick-broadcast.c: 748
	 * delta is quite small. /Users/rubber/linux/kernel/time/tick-broadcast.c: 749
	 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 750
	 * - There are pending events on sleeping CPUs which were not /Users/rubber/linux/kernel/time/tick-broadcast.c: 751
	 * in the event mask /Users/rubber/linux/kernel/time/tick-broadcast.c: 752
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 777
	 * For hrtimer based broadcasting we cannot shutdown the cpu /Users/rubber/linux/kernel/time/tick-broadcast.c: 778
	 * local device if our own event is the first one to expire or /Users/rubber/linux/kernel/time/tick-broadcast.c: 779
	 * if we own the broadcast timer. /Users/rubber/linux/kernel/time/tick-broadcast.c: 780
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 803
		 * If the current CPU owns the hrtimer broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 804
		 * mechanism, it cannot go deep idle and we do not add /Users/rubber/linux/kernel/time/tick-broadcast.c: 805
		 * the CPU to the broadcast mask. We don't have to go /Users/rubber/linux/kernel/time/tick-broadcast.c: 806
		 * through the EXIT path as the local timer is not /Users/rubber/linux/kernel/time/tick-broadcast.c: 807
		 * shutdown. /Users/rubber/linux/kernel/time/tick-broadcast.c: 808
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 814
		 * If the broadcast device is in periodic mode, we /Users/rubber/linux/kernel/time/tick-broadcast.c: 815
		 * return. /Users/rubber/linux/kernel/time/tick-broadcast.c: 816
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 831
			 * We only reprogram the broadcast timer if we /Users/rubber/linux/kernel/time/tick-broadcast.c: 832
			 * did not mark ourself in the force mask and /Users/rubber/linux/kernel/time/tick-broadcast.c: 833
			 * if the cpu local event is earlier than the /Users/rubber/linux/kernel/time/tick-broadcast.c: 834
			 * broadcast event. If the current CPU is in /Users/rubber/linux/kernel/time/tick-broadcast.c: 835
			 * the force mask, then we are going to be /Users/rubber/linux/kernel/time/tick-broadcast.c: 836
			 * woken by the IPI right away; we return /Users/rubber/linux/kernel/time/tick-broadcast.c: 837
			 * busy, so the CPU does not try to go deep /Users/rubber/linux/kernel/time/tick-broadcast.c: 838
			 * idle. /Users/rubber/linux/kernel/time/tick-broadcast.c: 839
				/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 845
				 * In case of hrtimer broadcasts the /Users/rubber/linux/kernel/time/tick-broadcast.c: 846
				 * programming might have moved the /Users/rubber/linux/kernel/time/tick-broadcast.c: 847
				 * timer to this cpu. If yes, remove /Users/rubber/linux/kernel/time/tick-broadcast.c: 848
				 * us from the broadcast mask and /Users/rubber/linux/kernel/time/tick-broadcast.c: 849
				 * return busy. /Users/rubber/linux/kernel/time/tick-broadcast.c: 850
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 862
			 * The cpu which was handling the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 863
			 * timer marked this cpu in the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 864
			 * pending mask and fired the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 865
			 * IPI. So we are going to handle the expired /Users/rubber/linux/kernel/time/tick-broadcast.c: 866
			 * event anyway via the broadcast IPI /Users/rubber/linux/kernel/time/tick-broadcast.c: 867
			 * handler. No need to reprogram the timer /Users/rubber/linux/kernel/time/tick-broadcast.c: 868
			 * with an already expired event. /Users/rubber/linux/kernel/time/tick-broadcast.c: 869
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 875
			 * Bail out if there is no next event. /Users/rubber/linux/kernel/time/tick-broadcast.c: 876
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 880
			 * If the pending bit is not set, then we are /Users/rubber/linux/kernel/time/tick-broadcast.c: 881
			 * either the CPU handling the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 882
			 * interrupt or we got woken by something else. /Users/rubber/linux/kernel/time/tick-broadcast.c: 883
			 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 884
			 * We are no longer in the broadcast mask, so /Users/rubber/linux/kernel/time/tick-broadcast.c: 885
			 * if the cpu local expiry time is already /Users/rubber/linux/kernel/time/tick-broadcast.c: 886
			 * reached, we would reprogram the cpu local /Users/rubber/linux/kernel/time/tick-broadcast.c: 887
			 * timer with an already expired event. /Users/rubber/linux/kernel/time/tick-broadcast.c: 888
			 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 889
			 * This can lead to a ping-pong when we return /Users/rubber/linux/kernel/time/tick-broadcast.c: 890
			 * to idle and therefore rearm the broadcast /Users/rubber/linux/kernel/time/tick-broadcast.c: 891
			 * timer before the cpu local timer was able /Users/rubber/linux/kernel/time/tick-broadcast.c: 892
			 * to fire. This happens because the forced /Users/rubber/linux/kernel/time/tick-broadcast.c: 893
			 * reprogramming makes sure that the event /Users/rubber/linux/kernel/time/tick-broadcast.c: 894
			 * will happen in the future and depending on /Users/rubber/linux/kernel/time/tick-broadcast.c: 895
			 * the min_delta setting this might be far /Users/rubber/linux/kernel/time/tick-broadcast.c: 896
			 * enough out that the ping-pong starts. /Users/rubber/linux/kernel/time/tick-broadcast.c: 897
			 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 898
			 * If the cpu local next_event has expired /Users/rubber/linux/kernel/time/tick-broadcast.c: 899
			 * then we know that the broadcast timer /Users/rubber/linux/kernel/time/tick-broadcast.c: 900
			 * next_event has expired as well and /Users/rubber/linux/kernel/time/tick-broadcast.c: 901
			 * broadcast is about to be handled. So we /Users/rubber/linux/kernel/time/tick-broadcast.c: 902
			 * avoid reprogramming and enforce that the /Users/rubber/linux/kernel/time/tick-broadcast.c: 903
			 * broadcast handler, which did not run yet, /Users/rubber/linux/kernel/time/tick-broadcast.c: 904
			 * will invoke the cpu local handler. /Users/rubber/linux/kernel/time/tick-broadcast.c: 905
			 * /Users/rubber/linux/kernel/time/tick-broadcast.c: 906
			 * We cannot call the handler directly from /Users/rubber/linux/kernel/time/tick-broadcast.c: 907
			 * here, because we might be in a NOHZ phase /Users/rubber/linux/kernel/time/tick-broadcast.c: 908
			 * and we did not go through the irq_enter() /Users/rubber/linux/kernel/time/tick-broadcast.c: 909
			 * nohz fixups. /Users/rubber/linux/kernel/time/tick-broadcast.c: 910
			/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 917
			 * We got woken by something else. Reprogram /Users/rubber/linux/kernel/time/tick-broadcast.c: 918
			 * the cpu local timer device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 919
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 969
	 * If there is no broadcast or wakeup device, tell the caller not /Users/rubber/linux/kernel/time/tick-broadcast.c: 970
	 * to go into deep idle. /Users/rubber/linux/kernel/time/tick-broadcast.c: 971
 * Reset the one shot broadcast for a cpu /Users/rubber/linux/kernel/time/tick-broadcast.c: 977
 * Called with tick_broadcast_lock held /Users/rubber/linux/kernel/time/tick-broadcast.c: 979
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 1004
	 * Protect against concurrent updates (store /load tearing on /Users/rubber/linux/kernel/time/tick-broadcast.c: 1005
	 * 32bit). It does not matter if the time is already in the /Users/rubber/linux/kernel/time/tick-broadcast.c: 1006
	 * past. The broadcast device which is about to be programmed will /Users/rubber/linux/kernel/time/tick-broadcast.c: 1007
	 * fire in any case. /Users/rubber/linux/kernel/time/tick-broadcast.c: 1008
 * tick_broadcast_setup_oneshot - setup the broadcast device /Users/rubber/linux/kernel/time/tick-broadcast.c: 1017
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 1032
		 * We must be careful here. There might be other CPUs /Users/rubber/linux/kernel/time/tick-broadcast.c: 1033
		 * waiting for periodic broadcast. We need to set the /Users/rubber/linux/kernel/time/tick-broadcast.c: 1034
		 * oneshot_mask bits for those and program the /Users/rubber/linux/kernel/time/tick-broadcast.c: 1035
		 * broadcast device to fire. /Users/rubber/linux/kernel/time/tick-broadcast.c: 1036
		/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 1052
		 * The first cpu which switches to oneshot mode sets /Users/rubber/linux/kernel/time/tick-broadcast.c: 1053
		 * the bit for all other cpus which are in the general /Users/rubber/linux/kernel/time/tick-broadcast.c: 1054
		 * (periodic) broadcast mask. So the bit is set and /Users/rubber/linux/kernel/time/tick-broadcast.c: 1055
		 * would prevent the first broadcast enter after this /Users/rubber/linux/kernel/time/tick-broadcast.c: 1056
		 * to program the bc device. /Users/rubber/linux/kernel/time/tick-broadcast.c: 1057
 * Select oneshot operating mode for the broadcast device /Users/rubber/linux/kernel/time/tick-broadcast.c: 1064
 * Remove a dying CPU from broadcasting /Users/rubber/linux/kernel/time/tick-broadcast.c: 1098
	/* /Users/rubber/linux/kernel/time/tick-broadcast.c: 1105
	 * Clear the broadcast masks for the dead cpu, but do not stop /Users/rubber/linux/kernel/time/tick-broadcast.c: 1106
	 * the broadcast device! /Users/rubber/linux/kernel/time/tick-broadcast.c: 1107
 * Check, whether the broadcast device is in one shot mode /Users/rubber/linux/kernel/time/tick-broadcast.c: 1116
 * Check whether the broadcast device supports oneshot. /Users/rubber/linux/kernel/time/tick-broadcast.c: 1124
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1
 * Implement CPU time clocks for the POSIX clock interface. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 3
 * Called after updating RLIMIT_CPU to run cpu timer and update /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 33
 * tsk->signal->posix_cputimers.bases[clock].nextevt expiration cache if /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 34
 * necessary. Needs siglock protection since other code may update the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 35
 * expiration cache as well. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 36
 * Functions for validating access to tasks. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 48
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 59
	 * If the encoded PID is 0, then the timer is targeted at current /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 60
	 * or the process to which current belongs. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 61
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 75
	 * For clock_gettime(PROCESS) allow finding the process by /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 76
	 * with the pid of the current task.  The code needs the tgid /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 77
	 * of the process so that pid_task(pid, PIDTYPE_TGID) can be /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 78
	 * used to find the process. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 79
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 84
	 * For processes require that pid identifies a process. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 85
 * Update expiry time from increment, and increase overrun count, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 112
 * given the current clock sample. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 113
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 161
			 * If sched_clock is using a cycle counter, we /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 162
			 * don't have any idea of its true resolution /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 163
			 * exported, but it is much more than 1s/HZ. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 164
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 177
	 * You can never reset a CPU clock, but we check for other errors /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 178
	 * in the call before failing with EPERM. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 179
 * Sample a per-thread clock for the given task. clkid is validated. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 185
 * Set cputime to sum_cputime if sum_cputime > cputime. Use cmpxchg /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 234
 * to avoid race conditions with concurrent updates to cputime. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 235
 * thread_group_sample_cputime - Sample cputime for a given task /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 257
 * @tsk:	Task for which cputime needs to be started /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 258
 * @samples:	Storage for time samples /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 259
 * Called from sys_getitimer() to calculate the expiry time of an active /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 261
 * timer. That means group cputime accounting is already active. Called /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 262
 * with task sighand lock held. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 263
 * Updates @times with an uptodate sample of the thread group cputimes. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 265
 * thread_group_start_cputime - Start cputime and return a sample /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 278
 * @tsk:	Task for which cputime needs to be started /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 279
 * @samples:	Storage for time samples /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 280
 * The thread group cputime accounting is avoided when there are no posix /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 282
 * CPU timers armed. Before starting a timer it's required to check whether /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 283
 * the time accounting is active. If not, a full update of the atomic /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 284
 * accounting store needs to be done and the accounting enabled. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 285
 * Updates @times with an uptodate sample of the thread group cputimes. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 287
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 300
		 * The POSIX timer interface allows for absolute time expiry /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 301
		 * values through the TIMER_ABSTIME flag, therefore we have /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 302
		 * to synchronize the timer to the clock every time we start it. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 303
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 308
		 * We're setting timers_active without a lock. Ensure this /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 309
		 * only gets written to in one operation. We set it after /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 310
		 * update_gt_cputime() as a small optimization, but /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 311
		 * barriers are not required because update_gt_cputime() /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 312
		 * can handle concurrent updates. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 313
 * Sample a process (thread group) clock for the given task clkid. If the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 329
 * group's cputime accounting is already enabled, read the atomic /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 330
 * store. Otherwise a full update is required.  clkid is already validated. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 331
 * Validate the clockid_t for a new CPU-clock timer, and initialize the timer. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 376
 * This is called from sys_timer_create() and do_cpu_nanosleep() with the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 377
 * new timer already all-zeros initialized. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 378
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 392
	 * If posix timer expiry is handled in task work context then /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 393
	 * timer::it_lock can be taken without disabling interrupts as all /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 394
	 * other locking happens in task context. This requires a separate /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 395
	 * lock class key otherwise regular posix timer expiry would record /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 396
	 * the lock class being taken in interrupt context and generate a /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 397
	 * false positive warning. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 398
 * Force recalculating the base earliest expiration on the next tick. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 422
 * This will also re-evaluate the need to keep around the process wide /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 423
 * cputime counter and tick dependency and eventually shut these down /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 424
 * if necessary. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 425
 * Dequeue the timer and reset the base if it was its earliest expiration. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 436
 * It makes sure the next tick recalculates the base next expiration so we /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 437
 * don't keep the costly process wide cputime counter around for a random /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 438
 * amount of time, along with the tick dependency. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 439
 * If another timer gets queued between this and the next tick, its /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 441
 * expiration will update the base next event if necessary on the next /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 442
 * tick. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 443
 * Clean up a CPU-clock timer that is about to be destroyed. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 460
 * This is called from timer deletion with the timer already locked. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 461
 * If we return TIMER_RETRY, it's necessary to release the timer's lock /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 462
 * and try again.  (This happens when the timer is in the middle of firing.) /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 463
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 478
	 * Protect against sighand release/switch in exit/exec and process/ /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 479
	 * thread timer list entry concurrent read/writes. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 480
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 484
		 * This raced with the reaping of the task. The exit cleanup /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 485
		 * should have removed this timer from the timer queue. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 486
 * Clean out CPU timers which are still armed when a thread exits. The /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 519
 * timers are only removed from the list. No other updates are done. The /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 520
 * corresponding posix timers are still accessible, but cannot be rearmed. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 521
 * This must be called with the siglock held. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 523
 * These are both called with the siglock held, when the current thread /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 533
 * is being reaped.  When the final (leader) thread in the group is reaped, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 534
 * posix_cpu_timers_exit_group will be called after posix_cpu_timers_exit. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 535
 * Insert the timer on the appropriate list before any timers that /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 547
 * expire later.  This must be called with the sighand lock held. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 548
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 559
	 * We are the new earliest-expiring POSIX 1.b timer, hence /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 560
	 * need to update expiration cache. Take into account that /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 561
	 * for process timers we share expiration cache with itimers /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 562
	 * and RLIMIT_CPU and for thread timers with RLIMIT_RTTIME. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 563
 * The timer is locked, fire it and arrange for its reload. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 575
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 582
		 * User don't want any signal. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 583
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 587
		 * This a special case for clock_nanosleep, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 588
		 * not a normal timer from sys_timer_create. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 589
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 594
		 * One-shot timer.  Clear it as soon as it's fired. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 595
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 600
		 * The signal did not get queued because the signal /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 601
		 * was ignored, so we won't get any callback to /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 602
		 * reload the timer.  But we need to keep it /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 603
		 * ticking in case the signal is deliverable next time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 604
 * Guts of sys_timer_settime for CPU timers. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 612
 * This is called with the timer locked and interrupts disabled. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 613
 * If we return TIMER_RETRY, it's necessary to release the timer's lock /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 614
 * and try again.  (This happens when the timer is in the middle of firing.) /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 615
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 631
		 * If p has just been reaped, we can no /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 632
		 * longer get any information about it at all. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 633
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 639
	 * Use the to_ktime conversion because that clamps the maximum /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 640
	 * value to KTIME_MAX and avoid multiplication overflows. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 641
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 645
	 * Protect against sighand release/switch in exit/exec and p->cpu_timers /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 646
	 * and p->signal->cpu_timers read/write in arm_timer() /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 647
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 650
	 * If p has just been reaped, we can no /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 651
	 * longer get any information about it at all. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 652
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 659
	 * Disarm any old timer after extracting its expiry time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 660
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 672
	 * We need to sample the current value to convert the new /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 673
	 * value from to relative and absolute, and to convert the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 674
	 * old value from absolute to relative.  To set a process /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 675
	 * timer, we need a sample to balance the thread expiry /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 676
	 * times (in arm_timer).  With an absolute time, we must /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 677
	 * check if it's already passed.  In short, we need a sample. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 678
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 690
			 * Update the timer in case it has overrun already. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 691
			 * If it has, we'll report it as having overrun and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 692
			 * with the next reloaded timer already ticking, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 693
			 * though we are swallowing that pending /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 694
			 * notification here to install the new setting. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 695
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 710
		 * We are colliding with the timer actually firing. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 711
		 * Punt after filling in the timer's old value, and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 712
		 * disable this firing since we are already reporting /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 713
		 * it as an overrun (thanks to bump_cpu_timer above). /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 714
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 724
	 * Install the new expiry time (or zero). /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 725
	 * For a timer with no notification action, we don't actually /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 726
	 * arm the timer (we'll just fake it for timer_gettime). /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 727
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 735
	 * Install the new reload setting, and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 736
	 * set up the signal and overrun bookkeeping. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 737
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 741
	 * This acts as a modification timestamp for the timer, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 742
	 * so any automatic reload attempt will punt on seeing /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 743
	 * that we have reset the timer manually. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 744
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 753
			 * The designated time already passed, so we notify /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 754
			 * immediately, even if the thread never runs to /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 755
			 * accumulate more time on this clock. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 756
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 761
		 * Make sure we don't keep around the process wide cputime /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 762
		 * counter or the tick dependency if they are not necessary. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 763
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 794
	 * Easy part: convert the reload time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 795
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 802
	 * Sample the clock to take the difference with the expiry time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 803
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 813
		 * The timer should have expired already, but the firing /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 814
		 * hasn't taken place yet.  Say it's just about to expire. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 815
 * Check for any per-thread CPU timers that have fired and move them off /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 885
 * the tsk->cpu_timers[N] list onto the firing list.  Here we update the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 886
 * tsk->it_*_expires values to reflect the remaining thread CPU timers. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 887
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 905
	 * Check for the special case thread timers. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 906
 * Check for any per-thread CPU timers that have fired and move them /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 962
 * off the tsk->*_timers list onto the firing list.  Per-thread timers /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 963
 * have already been taken off. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 964
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 974
	 * If there are no active process wide timers (POSIX 1.b, itimers, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 975
	 * RLIMIT_CPU) nothing to check. Also skip the process wide timer /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 976
	 * processing when there is already another task handling them. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 977
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 982
	 * Signify that a thread is checking for process timers. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 983
	 * Write access to this field is protected by the sighand lock. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 984
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 988
	 * Collect the current process totals. Group accounting is active /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 989
	 * so the sample can be taken directly. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 990
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 995
	 * Check for the special case process timers. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 996
 * This is called from the signal code (via posixtimer_rearm) /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1036
 * when the last timer signal was delivered and we have to reload the timer. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1037
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1057
	 * Fetch the current sample and update the timer's expiry time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1058
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1067
	 * Now re-arm for the new expiry time. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1068
 * task_cputimers_expired - Check whether posix CPU timers are expired /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1077
 * @samples:	Array of current samples for the CPUCLOCK clocks /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1079
 * @pct:	Pointer to a posix_cputimers container /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1080
 * Returns true if any member of @samples is greater than the corresponding /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1082
 * member of @pct->bases[CLK].nextevt. False otherwise /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1083
 * fastpath_timer_check - POSIX CPU timers fast path. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1098
 * @tsk:	The task (thread) being checked. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1100
 * Check the task and thread group timers.  If both are zero (there are no /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1102
 * timers set) return false.  Otherwise snapshot the task and thread group /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1103
 * timers and compare them with the corresponding expiration times.  Return /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1104
 * true if a timer has expired, else return false. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1105
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1122
	 * Check if thread group timers expired when timers are active and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1123
	 * no other thread in the group is already handling expiry for /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1124
	 * thread group cputimers. These fields are read without the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1125
	 * sighand lock. However, this is fine because this is meant to be /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1126
	 * a fastpath heuristic to determine whether we should try to /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1127
	 * acquire the sighand lock to handle timer expiry. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1128
	 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1129
	 * In the worst case scenario, if concurrently timers_active is set /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1130
	 * or expiry_active is cleared, but the current thread doesn't see /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1131
	 * the change yet, the timer checks are delayed until the next /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1132
	 * thread in the group gets a scheduler interrupt to handle the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1133
	 * timer. This isn't an issue in practice because these types of /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1134
	 * delays with signals actually getting sent are expected. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1135
 * Clear existing posix CPU timers task work. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1162
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1166
	 * A copied work entry from the old task is not meaningful, clear it. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1167
	 * N.B. init_task_work will not do this. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1168
 * Initialize posix CPU timers task work in init task. Out of line to /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1178
 * keep the callback static and to avoid header recursion hell. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1179
 * Note: All operations on tsk->posix_cputimer_work.scheduled happen either /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1187
 * in hard interrupt context or in task context with interrupts /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1188
 * disabled. Aside of that the writer/reader interaction is always in the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1189
 * context of the current task, which means they are strict per CPU. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1190
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1212
	 * On !RT kernels interrupts are disabled while collecting expired /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1213
	 * timers, so no tick can happen and the fast path check can be /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1214
	 * reenabled without further checks. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1215
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1222
	 * On RT enabled kernels ticks can happen while the expired timers /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1223
	 * are collected under sighand lock. But any tick which observes /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1224
	 * the CPUTIMERS_WORK_SCHEDULED bit set, does not run the fastpath /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1225
	 * checks. So reenabling the tick work has do be done carefully: /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1226
	 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1227
	 * Disable interrupts and run the fast path check if jiffies have /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1228
	 * advanced since the collecting of expired timers started. If /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1229
	 * jiffies have not advanced or the fast path check did not find /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1230
	 * newly expired timers, reenable the fast path check in the timer /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1231
	 * interrupt. If there are newly expired timers, return false and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1232
	 * let the collection loop repeat. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1233
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1274
		 * On RT locking sighand lock does not disable interrupts, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1275
		 * so this needs to be careful vs. ticks. Store the current /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1276
		 * jiffies value. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1277
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1282
		 * Here we take off tsk->signal->cpu_timers[N] and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1283
		 * tsk->cpu_timers[N] all the timers that are firing, and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1284
		 * put them on the firing list. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1285
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1291
		 * The above timer checks have updated the expiry cache and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1292
		 * because nothing can have queued or modified timers after /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1293
		 * sighand lock was taken above it is guaranteed to be /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1294
		 * consistent. So the next timer interrupt fastpath check /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1295
		 * will find valid data. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1296
		 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1297
		 * If timer expiry runs in the timer interrupt context then /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1298
		 * the loop is not relevant as timers will be directly /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1299
		 * expired in interrupt context. The stub function below /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1300
		 * returns always true which allows the compiler to /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1301
		 * optimize the loop out. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1302
		 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1303
		 * If timer expiry is deferred to task work context then /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1304
		 * the following rules apply: /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1305
		 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1306
		 * - On !RT kernels no tick can have happened on this CPU /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1307
		 *   after sighand lock was acquired because interrupts are /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1308
		 *   disabled. So reenabling task work before dropping /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1309
		 *   sighand lock and reenabling interrupts is race free. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1310
		 * /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1311
		 * - On RT kernels ticks might have happened but the tick /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1312
		 *   work ignored posix CPU timer handling because the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1313
		 *   CPUTIMERS_WORK_SCHEDULED bit is set. Reenabling work /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1314
		 *   must be done very carefully including a check whether /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1315
		 *   ticks have happened since the start of the timer /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1316
		 *   expiry checks. posix_cpu_timers_enable_work() takes /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1317
		 *   care of that and eventually lets the expiry checks /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1318
		 *   run again. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1319
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1323
	 * We must release sighand lock before taking any timer's lock. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1324
	 * There is a potential race with timer deletion here, as the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1325
	 * siglock now protects our private firing list.  We have set /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1326
	 * the firing flag in each timer, so that a deletion attempt /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1327
	 * that gets the timer lock before we do will give it up and /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1328
	 * spin until we've taken care of that timer below. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1329
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1333
	 * Now that all the timers on our list have the firing flag, /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1334
	 * no one will touch their list entries but us.  We'll take /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1335
	 * each timer's lock before clearing its firing flag, so no /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1336
	 * timer call will interfere. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1337
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1342
		 * spin_lock() is sufficient here even independent of the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1343
		 * expiry context. If expiry happens in hard interrupt /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1344
		 * context it's obvious. For task work context it's safe /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1345
		 * because all other operations on timer::it_lock happen in /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1346
		 * task context (syscall or exit). /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1347
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1353
		 * The firing flag is -1 if we collided with a reset /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1354
		 * of the timer, which already reported this /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1355
		 * almost-firing as an overrun.  So don't generate an event. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1356
 * This is called from the timer interrupt handler.  The irq handler has /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1365
 * already updated our counts.  We need to check if any timers fire now. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1366
 * Interrupts are disabled. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1367
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1375
	 * If the actual expiry is deferred to task work context and the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1376
	 * work is already scheduled there is no point to do anything here. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1377
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1382
	 * The fast path checks that there are no expired thread or thread /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1383
	 * group timers.  If that's so, just return. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1384
 * Set one of the process-wide special case CPU timers or RLIMIT_CPU. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1393
 * The tsk->sighand->siglock must be held by the caller. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1394
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1408
		 * We are setting itimer. The *oldval is absolute and we update /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1409
		 * it to be relative, *newval argument is relative and we update /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1410
		 * it to be absolute. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1411
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1426
	 * Update expiration cache if this is the earliest timer. CPUCLOCK_PROF /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1427
	 * expiry cache is also used by RLIMIT_CPU!. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1428
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1444
	 * Set up a temporary timer and then wait for it to go off. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1445
				/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1470
				 * Our timer fired and was reset, below /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1471
				 * deletion can not fail. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1472
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1479
			 * Block until cpu_timer_fire (or a signal) wakes us. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1480
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1488
		 * We were interrupted by a signal. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1489
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1494
			 * Timer is now unarmed, deletion can not fail. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1495
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1502
			 * We need to handle case when timer was or is in the /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1503
			 * middle of firing. In other cases we already freed /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1504
			 * resources. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1505
			/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1513
			 * It actually did fire already. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1514
		/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1520
		 * Report back to the user the time still remaining. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1521
	/* /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1540
	 * Diagnose required errors first. /Users/rubber/linux/kernel/time/posix-cpu-timers.c: 1541
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/posix-stubs.c: 1
 * Dummy stubs used when CONFIG_POSIX_TIMERS=n /Users/rubber/linux/kernel/time/posix-stubs.c: 3
 * Created by:  Nicolas Pitre, July 2016 /Users/rubber/linux/kernel/time/posix-stubs.c: 5
 * Copyright:   (C) 2016 Linaro Limited /Users/rubber/linux/kernel/time/posix-stubs.c: 6
 * We preserve minimal support for CLOCK_REALTIME and CLOCK_MONOTONIC /Users/rubber/linux/kernel/time/posix-stubs.c: 55
 * as it is easy to remain compatible with little code. CLOCK_BOOTTIME /Users/rubber/linux/kernel/time/posix-stubs.c: 56
 * is also included for convenience as at least systemd uses it. /Users/rubber/linux/kernel/time/posix-stubs.c: 57
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 1
 * Emulate a local clock event device via a pseudo clock device. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 3
	/* /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 22
	 * Note, we cannot cancel the timer here as we might /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 23
	 * run into the following live lock scenario: /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 24
	 * /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 25
	 * cpu 0		cpu1 /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 26
	 * lock(broadcast_lock); /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 27
	 *			hrtimer_interrupt() /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 28
	 *			bc_handler() /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 29
	 *			   tick_handle_oneshot_broadcast(); /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 30
	 *			    lock(broadcast_lock); /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 31
	 * hrtimer_cancel() /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 32
	 *  wait_for_callback() /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 33
 * This is called from the guts of the broadcast code when the cpu /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 40
 * which is about to enter idle has the earliest broadcast timer event. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 41
	/* /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 45
	 * This is called either from enter/exit idle code or from the /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 46
	 * broadcast handler. In all cases tick_broadcast_lock is held. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 47
	 * /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 48
	 * hrtimer_cancel() cannot be called here neither from the /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 49
	 * broadcast handler nor from the enter/exit idle code. The idle /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 50
	 * code can run into the problem described in bc_shutdown() and the /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 51
	 * broadcast handler cannot wait for itself to complete for obvious /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 52
	 * reasons. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 53
	 * /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 54
	 * Each caller tries to arm the hrtimer on its own CPU, but if the /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 55
	 * hrtimer callback function is currently running, then /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 56
	 * hrtimer_start() cannot move it and the timer stays on the CPU on /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 57
	 * which it is assigned at the moment. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 58
	 * /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 59
	 * As this can be called from idle code, the hrtimer_start() /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 60
	 * invocation has to be wrapped with RCU_NONIDLE() as /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 61
	 * hrtimer_start() can call into tracing. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 62
		/* /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 66
		 * The core tick broadcast mode expects bc->bound_on to be set /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 67
		 * correctly to prevent a CPU which has the broadcast hrtimer /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 68
		 * armed from going deep idle. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 69
		 * /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 70
		 * As tick_broadcast_lock is held, nothing can change the cpu /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 71
		 * base which was just established in hrtimer_start() above. So /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 72
		 * the below access is safe even without holding the hrtimer /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 73
		 * base lock. /Users/rubber/linux/kernel/time/tick-broadcast-hrtimer.c: 74
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 1
 * Unit test for the clocksource watchdog. /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 3
 * Copyright (C) 2021 Facebook, Inc. /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 5
 * Author: Paul E. McKenney <paulmck@kernel.org> /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 7
	/* /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 112
	 * Verify that jiffies-like clocksources get the manually /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 113
	 * specified uncertainty margin. /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 114
	/* /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 127
	 * Verify that tsc-like clocksources are assigned a reasonable /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 128
	 * uncertainty margin. /Users/rubber/linux/kernel/time/clocksource-wdtest.c: 129
 SPDX-License-Identifier: LGPL-2.0+ /Users/rubber/linux/kernel/time/timeconv.c: 1
 * Copyright (C) 1993, 1994, 1995, 1996, 1997 Free Software Foundation, Inc. /Users/rubber/linux/kernel/time/timeconv.c: 3
 * This file is part of the GNU C Library. /Users/rubber/linux/kernel/time/timeconv.c: 4
 * Contributed by Paul Eggert (eggert@twinsun.com). /Users/rubber/linux/kernel/time/timeconv.c: 5
 * The GNU C Library is free software; you can redistribute it and/or /Users/rubber/linux/kernel/time/timeconv.c: 7
 * modify it under the terms of the GNU Library General Public License as /Users/rubber/linux/kernel/time/timeconv.c: 8
 * published by the Free Software Foundation; either version 2 of the /Users/rubber/linux/kernel/time/timeconv.c: 9
 * License, or (at your option) any later version. /Users/rubber/linux/kernel/time/timeconv.c: 10
 * The GNU C Library is distributed in the hope that it will be useful, /Users/rubber/linux/kernel/time/timeconv.c: 12
 * but WITHOUT ANY WARRANTY; without even the implied warranty of /Users/rubber/linux/kernel/time/timeconv.c: 13
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU /Users/rubber/linux/kernel/time/timeconv.c: 14
 * Library General Public License for more details. /Users/rubber/linux/kernel/time/timeconv.c: 15
 * You should have received a copy of the GNU Library General Public /Users/rubber/linux/kernel/time/timeconv.c: 17
 * License along with the GNU C Library; see the file COPYING.LIB.  If not, /Users/rubber/linux/kernel/time/timeconv.c: 18
 * write to the Free Software Foundation, Inc., 59 Temple Place - Suite 330, /Users/rubber/linux/kernel/time/timeconv.c: 19
 * Boston, MA 02111-1307, USA. /Users/rubber/linux/kernel/time/timeconv.c: 20
 * Converts the calendar time to broken-down time representation /Users/rubber/linux/kernel/time/timeconv.c: 24
 * 2009-7-14: /Users/rubber/linux/kernel/time/timeconv.c: 26
 *   Moved from glibc-2.6 to kernel by Zhaolei<zhaolei@cn.fujitsu.com> /Users/rubber/linux/kernel/time/timeconv.c: 27
 * 2021-06-02: /Users/rubber/linux/kernel/time/timeconv.c: 28
 *   Reimplemented by Cassio Neri <cassio.neri@gmail.com> /Users/rubber/linux/kernel/time/timeconv.c: 29
 * time64_to_tm - converts the calendar time to local broken-down time /Users/rubber/linux/kernel/time/timeconv.c: 40
 * @totalsecs:	the number of seconds elapsed since 00:00:00 on January 1, 1970, /Users/rubber/linux/kernel/time/timeconv.c: 42
 *		Coordinated Universal Time (UTC). /Users/rubber/linux/kernel/time/timeconv.c: 43
 * @offset:	offset seconds adding to totalsecs. /Users/rubber/linux/kernel/time/timeconv.c: 44
 * @result:	pointer to struct tm variable to receive broken-down time /Users/rubber/linux/kernel/time/timeconv.c: 45
	/* /Users/rubber/linux/kernel/time/timeconv.c: 77
	 * The following algorithm is, basically, Proposition 6.3 of Neri /Users/rubber/linux/kernel/time/timeconv.c: 78
	 * and Schneider [1]. In a few words: it works on the computational /Users/rubber/linux/kernel/time/timeconv.c: 79
	 * (fictitious) calendar where the year starts in March, month = 2 /Users/rubber/linux/kernel/time/timeconv.c: 80
	 * (*), and finishes in February, month = 13. This calendar is /Users/rubber/linux/kernel/time/timeconv.c: 81
	 * mathematically convenient because the day of the year does not /Users/rubber/linux/kernel/time/timeconv.c: 82
	 * depend on whether the year is leap or not. For instance: /Users/rubber/linux/kernel/time/timeconv.c: 83
	 * /Users/rubber/linux/kernel/time/timeconv.c: 84
	 * March 1st		0-th day of the year; /Users/rubber/linux/kernel/time/timeconv.c: 85
	 * ... /Users/rubber/linux/kernel/time/timeconv.c: 86
	 * April 1st		31-st day of the year; /Users/rubber/linux/kernel/time/timeconv.c: 87
	 * ... /Users/rubber/linux/kernel/time/timeconv.c: 88
	 * January 1st		306-th day of the year; (Important!) /Users/rubber/linux/kernel/time/timeconv.c: 89
	 * ... /Users/rubber/linux/kernel/time/timeconv.c: 90
	 * February 28th	364-th day of the year; /Users/rubber/linux/kernel/time/timeconv.c: 91
	 * February 29th	365-th day of the year (if it exists). /Users/rubber/linux/kernel/time/timeconv.c: 92
	 * /Users/rubber/linux/kernel/time/timeconv.c: 93
	 * After having worked out the date in the computational calendar /Users/rubber/linux/kernel/time/timeconv.c: 94
	 * (using just arithmetics) it's easy to convert it to the /Users/rubber/linux/kernel/time/timeconv.c: 95
	 * corresponding date in the Gregorian calendar. /Users/rubber/linux/kernel/time/timeconv.c: 96
	 * /Users/rubber/linux/kernel/time/timeconv.c: 97
	 * [1] "Euclidean Affine Functions and Applications to Calendar /Users/rubber/linux/kernel/time/timeconv.c: 98
	 * Algorithms". https://arxiv.org/abs/2102.06959 /Users/rubber/linux/kernel/time/timeconv.c: 99
	 * /Users/rubber/linux/kernel/time/timeconv.c: 100
	 * (*) The numbering of months follows tm more closely and thus, /Users/rubber/linux/kernel/time/timeconv.c: 101
	 * is slightly different from [1]. /Users/rubber/linux/kernel/time/timeconv.c: 102
	/* /Users/rubber/linux/kernel/time/timeconv.c: 123
	 * Recall that January 1st is the 306-th day of the year in the /Users/rubber/linux/kernel/time/timeconv.c: 124
	 * computational (not Gregorian) calendar. /Users/rubber/linux/kernel/time/timeconv.c: 125
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/vsyscall.c: 1
 * Copyright 2019 ARM Ltd. /Users/rubber/linux/kernel/time/vsyscall.c: 3
 * Generic implementation of update_vsyscall and update_vsyscall_tz. /Users/rubber/linux/kernel/time/vsyscall.c: 5
 * Based on the x86 specific implementation. /Users/rubber/linux/kernel/time/vsyscall.c: 7
	/* /Users/rubber/linux/kernel/time/vsyscall.c: 103
	 * Read without the seqlock held by clock_getres(). /Users/rubber/linux/kernel/time/vsyscall.c: 104
	 * Note: No need to have a second copy. /Users/rubber/linux/kernel/time/vsyscall.c: 105
	/* /Users/rubber/linux/kernel/time/vsyscall.c: 109
	 * If the current clocksource is not VDSO capable, then spare the /Users/rubber/linux/kernel/time/vsyscall.c: 110
	 * update of the high resolution parts. /Users/rubber/linux/kernel/time/vsyscall.c: 111
 * vdso_update_begin - Start of a VDSO update section /Users/rubber/linux/kernel/time/vsyscall.c: 134
 * Allows architecture code to safely update the architecture specific VDSO /Users/rubber/linux/kernel/time/vsyscall.c: 136
 * data. Disables interrupts, acquires timekeeper lock to serialize against /Users/rubber/linux/kernel/time/vsyscall.c: 137
 * concurrent updates from timekeeping and invalidates the VDSO data /Users/rubber/linux/kernel/time/vsyscall.c: 138
 * sequence counter to prevent concurrent readers from accessing /Users/rubber/linux/kernel/time/vsyscall.c: 139
 * inconsistent data. /Users/rubber/linux/kernel/time/vsyscall.c: 140
 * Returns: Saved interrupt flags which need to be handed in to /Users/rubber/linux/kernel/time/vsyscall.c: 142
 * vdso_update_end(). /Users/rubber/linux/kernel/time/vsyscall.c: 143
 * vdso_update_end - End of a VDSO update section /Users/rubber/linux/kernel/time/vsyscall.c: 156
 * @flags:	Interrupt flags as returned from vdso_update_begin() /Users/rubber/linux/kernel/time/vsyscall.c: 157
 * Pairs with vdso_update_begin(). Marks vdso data consistent, invokes data /Users/rubber/linux/kernel/time/vsyscall.c: 159
 * synchronization if the architecture requires it, drops timekeeper lock /Users/rubber/linux/kernel/time/vsyscall.c: 160
 * and restores interrupt flags. /Users/rubber/linux/kernel/time/vsyscall.c: 161
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/sched_clock.c: 1
 * Generic sched_clock() support, to extend low level hardware time /Users/rubber/linux/kernel/time/sched_clock.c: 3
 * counters to full 64-bit ns values. /Users/rubber/linux/kernel/time/sched_clock.c: 4
 * struct clock_data - all data needed for sched_clock() (including /Users/rubber/linux/kernel/time/sched_clock.c: 23
 *                     registration of a new clock source) /Users/rubber/linux/kernel/time/sched_clock.c: 24
 * @seq:		Sequence counter for protecting updates. The lowest /Users/rubber/linux/kernel/time/sched_clock.c: 26
 *			bit is the index for @read_data. /Users/rubber/linux/kernel/time/sched_clock.c: 27
 * @read_data:		Data required to read from sched_clock. /Users/rubber/linux/kernel/time/sched_clock.c: 28
 * @wrap_kt:		Duration for which clock can run before wrapping. /Users/rubber/linux/kernel/time/sched_clock.c: 29
 * @rate:		Tick rate of the registered clock. /Users/rubber/linux/kernel/time/sched_clock.c: 30
 * @actual_read_sched_clock: Registered hardware level clock read function. /Users/rubber/linux/kernel/time/sched_clock.c: 31
 * The ordering of this structure has been chosen to optimize cache /Users/rubber/linux/kernel/time/sched_clock.c: 33
 * performance. In particular 'seq' and 'read_data[0]' (combined) should fit /Users/rubber/linux/kernel/time/sched_clock.c: 34
 * into a single 64-byte cache line. /Users/rubber/linux/kernel/time/sched_clock.c: 35
	/* /Users/rubber/linux/kernel/time/sched_clock.c: 53
	 * We don't need to use get_jiffies_64 on 32-bit arches here /Users/rubber/linux/kernel/time/sched_clock.c: 54
	 * because we register with BITS_PER_LONG /Users/rubber/linux/kernel/time/sched_clock.c: 55
 * Updating the data required to read the clock. /Users/rubber/linux/kernel/time/sched_clock.c: 100
 * sched_clock() will never observe mis-matched data even if called from /Users/rubber/linux/kernel/time/sched_clock.c: 102
 * an NMI. We do this by maintaining an odd/even copy of the data and /Users/rubber/linux/kernel/time/sched_clock.c: 103
 * steering sched_clock() to one or the other using a sequence counter. /Users/rubber/linux/kernel/time/sched_clock.c: 104
 * In order to preserve the data cache profile of sched_clock() as much /Users/rubber/linux/kernel/time/sched_clock.c: 105
 * as possible the system reverts back to the even copy when the update /Users/rubber/linux/kernel/time/sched_clock.c: 106
 * completes; the odd copy is used *only* during an update. /Users/rubber/linux/kernel/time/sched_clock.c: 107
 * Atomically update the sched_clock() epoch. /Users/rubber/linux/kernel/time/sched_clock.c: 125
	/* /Users/rubber/linux/kernel/time/sched_clock.c: 230
	 * If no sched_clock() function has been provided at that point, /Users/rubber/linux/kernel/time/sched_clock.c: 231
	 * make it the final one. /Users/rubber/linux/kernel/time/sched_clock.c: 232
	/* /Users/rubber/linux/kernel/time/sched_clock.c: 239
	 * Start the timer to keep sched_clock() properly updated and /Users/rubber/linux/kernel/time/sched_clock.c: 240
	 * sets the initial epoch. /Users/rubber/linux/kernel/time/sched_clock.c: 241
 * Clock read function for use when the clock is suspended. /Users/rubber/linux/kernel/time/sched_clock.c: 249
 * This function makes it appear to sched_clock() as if the clock /Users/rubber/linux/kernel/time/sched_clock.c: 251
 * stopped counting at its last update. /Users/rubber/linux/kernel/time/sched_clock.c: 252
 * This function must only be called from the critical /Users/rubber/linux/kernel/time/sched_clock.c: 254
 * section in sched_clock(). It relies on the read_seqcount_retry() /Users/rubber/linux/kernel/time/sched_clock.c: 255
 * at the end of the critical section to be sure we observe the /Users/rubber/linux/kernel/time/sched_clock.c: 256
 * correct copy of 'epoch_cyc'. /Users/rubber/linux/kernel/time/sched_clock.c: 257
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-common.c: 1
 * This file contains the base functions to manage periodic tick /Users/rubber/linux/kernel/time/tick-common.c: 3
 * related events. /Users/rubber/linux/kernel/time/tick-common.c: 4
 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/tick-common.c: 6
 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/tick-common.c: 7
 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/tick-common.c: 8
 * Tick devices /Users/rubber/linux/kernel/time/tick-common.c: 26
 * Tick next event: keeps track of the tick time. It's updated by the /Users/rubber/linux/kernel/time/tick-common.c: 30
 * CPU which handles the tick and protected by jiffies_lock. There is /Users/rubber/linux/kernel/time/tick-common.c: 31
 * no requirement to write hold the jiffies seqcount for it. /Users/rubber/linux/kernel/time/tick-common.c: 32
 * tick_do_timer_cpu is a timer core internal variable which holds the CPU NR /Users/rubber/linux/kernel/time/tick-common.c: 37
 * which is responsible for calling do_timer(), i.e. the timekeeping stuff. This /Users/rubber/linux/kernel/time/tick-common.c: 38
 * variable has two functions: /Users/rubber/linux/kernel/time/tick-common.c: 39
 * 1) Prevent a thundering herd issue of a gazillion of CPUs trying to grab the /Users/rubber/linux/kernel/time/tick-common.c: 41
 *    timekeeping lock all at once. Only the CPU which is assigned to do the /Users/rubber/linux/kernel/time/tick-common.c: 42
 *    update is handling it. /Users/rubber/linux/kernel/time/tick-common.c: 43
 * 2) Hand off the duty in the NOHZ idle case by setting the value to /Users/rubber/linux/kernel/time/tick-common.c: 45
 *    TICK_DO_TIMER_NONE, i.e. a non existing CPU. So the next cpu which looks /Users/rubber/linux/kernel/time/tick-common.c: 46
 *    at it will take over and keep the time keeping alive.  The handover /Users/rubber/linux/kernel/time/tick-common.c: 47
 *    procedure also covers cpu hotplug. /Users/rubber/linux/kernel/time/tick-common.c: 48
 * tick_do_timer_boot_cpu indicates the boot CPU temporarily owns /Users/rubber/linux/kernel/time/tick-common.c: 53
 * tick_do_timer_cpu and it should be taken over by an eligible secondary /Users/rubber/linux/kernel/time/tick-common.c: 54
 * when one comes online. /Users/rubber/linux/kernel/time/tick-common.c: 55
 * Debugging: see timer_list.c /Users/rubber/linux/kernel/time/tick-common.c: 61
 * tick_is_oneshot_available - check for a oneshot capable event device /Users/rubber/linux/kernel/time/tick-common.c: 69
 * Periodic tick /Users/rubber/linux/kernel/time/tick-common.c: 83
 * Event handler for periodic ticks /Users/rubber/linux/kernel/time/tick-common.c: 105
	/* /Users/rubber/linux/kernel/time/tick-common.c: 115
	 * The cpu might have transitioned to HIGHRES or NOHZ mode via /Users/rubber/linux/kernel/time/tick-common.c: 116
	 * update_process_times() -> run_local_timers() -> /Users/rubber/linux/kernel/time/tick-common.c: 117
	 * hrtimer_run_queues(). /Users/rubber/linux/kernel/time/tick-common.c: 118
		/* /Users/rubber/linux/kernel/time/tick-common.c: 127
		 * Setup the next period for devices, which do not have /Users/rubber/linux/kernel/time/tick-common.c: 128
		 * periodic mode: /Users/rubber/linux/kernel/time/tick-common.c: 129
		/* /Users/rubber/linux/kernel/time/tick-common.c: 135
		 * Have to be careful here. If we're in oneshot mode, /Users/rubber/linux/kernel/time/tick-common.c: 136
		 * before we call tick_periodic() in a loop, we need /Users/rubber/linux/kernel/time/tick-common.c: 137
		 * to be sure we're using a real hardware clocksource. /Users/rubber/linux/kernel/time/tick-common.c: 138
		 * Otherwise we could get trapped in an infinite /Users/rubber/linux/kernel/time/tick-common.c: 139
		 * loop, as the tick_periodic() increments jiffies, /Users/rubber/linux/kernel/time/tick-common.c: 140
		 * which then will increment time, possibly causing /Users/rubber/linux/kernel/time/tick-common.c: 141
		 * the loop to trigger again and again. /Users/rubber/linux/kernel/time/tick-common.c: 142
 * Setup the device for a periodic tick /Users/rubber/linux/kernel/time/tick-common.c: 150
 * Setup the tick device /Users/rubber/linux/kernel/time/tick-common.c: 203
	/* /Users/rubber/linux/kernel/time/tick-common.c: 212
	 * First device setup ? /Users/rubber/linux/kernel/time/tick-common.c: 213
		/* /Users/rubber/linux/kernel/time/tick-common.c: 216
		 * If no cpu took the do_timer update, assign it to /Users/rubber/linux/kernel/time/tick-common.c: 217
		 * this cpu: /Users/rubber/linux/kernel/time/tick-common.c: 218
			/* /Users/rubber/linux/kernel/time/tick-common.c: 225
			 * The boot CPU may be nohz_full, in which case set /Users/rubber/linux/kernel/time/tick-common.c: 226
			 * tick_do_timer_boot_cpu so the first housekeeping /Users/rubber/linux/kernel/time/tick-common.c: 227
			 * secondary that comes up will take do_timer from /Users/rubber/linux/kernel/time/tick-common.c: 228
			 * us. /Users/rubber/linux/kernel/time/tick-common.c: 229
		/* /Users/rubber/linux/kernel/time/tick-common.c: 242
		 * Startup in periodic mode first. /Users/rubber/linux/kernel/time/tick-common.c: 243
	/* /Users/rubber/linux/kernel/time/tick-common.c: 254
	 * When the device is not per cpu, pin the interrupt to the /Users/rubber/linux/kernel/time/tick-common.c: 255
	 * current cpu: /Users/rubber/linux/kernel/time/tick-common.c: 256
	/* /Users/rubber/linux/kernel/time/tick-common.c: 261
	 * When global broadcasting is active, check if the current /Users/rubber/linux/kernel/time/tick-common.c: 262
	 * device is registered as a placeholder for broadcast mode. /Users/rubber/linux/kernel/time/tick-common.c: 263
	 * This allows us to handle this x86 misfeature in a generic /Users/rubber/linux/kernel/time/tick-common.c: 264
	 * way. This function also returns !=0 when we keep the /Users/rubber/linux/kernel/time/tick-common.c: 265
	 * current active broadcast state for this CPU. /Users/rubber/linux/kernel/time/tick-common.c: 266
	/* /Users/rubber/linux/kernel/time/tick-common.c: 315
	 * Use the higher rated one, but prefer a CPU local device with a lower /Users/rubber/linux/kernel/time/tick-common.c: 316
	 * rating than a non-CPU local device /Users/rubber/linux/kernel/time/tick-common.c: 317
 * Check whether the new device is a better fit than curdev. curdev /Users/rubber/linux/kernel/time/tick-common.c: 325
 * can be NULL ! /Users/rubber/linux/kernel/time/tick-common.c: 326
 * Check, if the new registered device should be used. Called with /Users/rubber/linux/kernel/time/tick-common.c: 338
 * clockevents_lock held and interrupts disabled. /Users/rubber/linux/kernel/time/tick-common.c: 339
	/* /Users/rubber/linux/kernel/time/tick-common.c: 357
	 * Replace the eventually existing device by the new /Users/rubber/linux/kernel/time/tick-common.c: 358
	 * device. If the current device is the broadcast device, do /Users/rubber/linux/kernel/time/tick-common.c: 359
	 * not give it back to the clockevents layer ! /Users/rubber/linux/kernel/time/tick-common.c: 360
	/* /Users/rubber/linux/kernel/time/tick-common.c: 373
	 * Can the new device be used as a broadcast device ? /Users/rubber/linux/kernel/time/tick-common.c: 374
 * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode /Users/rubber/linux/kernel/time/tick-common.c: 380
 * @state:	The target state (enter/exit) /Users/rubber/linux/kernel/time/tick-common.c: 381
 * The system enters/leaves a state, where affected devices might stop /Users/rubber/linux/kernel/time/tick-common.c: 383
 * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups. /Users/rubber/linux/kernel/time/tick-common.c: 384
 * Called with interrupts disabled, so clockevents_lock is not /Users/rubber/linux/kernel/time/tick-common.c: 386
 * required here because the local clock event device cannot go away /Users/rubber/linux/kernel/time/tick-common.c: 387
 * under us. /Users/rubber/linux/kernel/time/tick-common.c: 388
 * Transfer the do_timer job away from a dying cpu. /Users/rubber/linux/kernel/time/tick-common.c: 403
 * Called with interrupts disabled. No locking required. If /Users/rubber/linux/kernel/time/tick-common.c: 405
 * tick_do_timer_cpu is owned by this cpu, nothing can change it. /Users/rubber/linux/kernel/time/tick-common.c: 406
 * Shutdown an event device on a given cpu: /Users/rubber/linux/kernel/time/tick-common.c: 415
 * This is called on a life CPU, when a CPU is dead. So we cannot /Users/rubber/linux/kernel/time/tick-common.c: 417
 * access the hardware device itself. /Users/rubber/linux/kernel/time/tick-common.c: 418
 * We just set the mode and remove it from the lists. /Users/rubber/linux/kernel/time/tick-common.c: 419
		/* /Users/rubber/linux/kernel/time/tick-common.c: 428
		 * Prevent that the clock events layer tries to call /Users/rubber/linux/kernel/time/tick-common.c: 429
		 * the set mode function! /Users/rubber/linux/kernel/time/tick-common.c: 430
 * tick_suspend_local - Suspend the local tick device /Users/rubber/linux/kernel/time/tick-common.c: 441
 * Called from the local cpu for freeze with interrupts disabled. /Users/rubber/linux/kernel/time/tick-common.c: 443
 * No locks required. Nothing can change the per cpu device. /Users/rubber/linux/kernel/time/tick-common.c: 445
 * tick_resume_local - Resume the local tick device /Users/rubber/linux/kernel/time/tick-common.c: 455
 * Called from the local CPU for unfreeze or XEN resume magic. /Users/rubber/linux/kernel/time/tick-common.c: 457
 * No locks required. Nothing can change the per cpu device. /Users/rubber/linux/kernel/time/tick-common.c: 459
	/* /Users/rubber/linux/kernel/time/tick-common.c: 474
	 * Ensure that hrtimers are up to date and the clockevents device /Users/rubber/linux/kernel/time/tick-common.c: 475
	 * is reprogrammed correctly when high resolution timers are /Users/rubber/linux/kernel/time/tick-common.c: 476
	 * enabled. /Users/rubber/linux/kernel/time/tick-common.c: 477
 * tick_suspend - Suspend the tick and the broadcast device /Users/rubber/linux/kernel/time/tick-common.c: 483
 * Called from syscore_suspend() via timekeeping_suspend with only one /Users/rubber/linux/kernel/time/tick-common.c: 485
 * CPU online and interrupts disabled or from tick_unfreeze() under /Users/rubber/linux/kernel/time/tick-common.c: 486
 * tick_freeze_lock. /Users/rubber/linux/kernel/time/tick-common.c: 487
 * No locks required. Nothing can change the per cpu device. /Users/rubber/linux/kernel/time/tick-common.c: 489
 * tick_resume - Resume the tick and the broadcast device /Users/rubber/linux/kernel/time/tick-common.c: 498
 * Called from syscore_resume() via timekeeping_resume with only one /Users/rubber/linux/kernel/time/tick-common.c: 500
 * CPU online and interrupts disabled. /Users/rubber/linux/kernel/time/tick-common.c: 501
 * No locks required. Nothing can change the per cpu device. /Users/rubber/linux/kernel/time/tick-common.c: 503
 * tick_freeze - Suspend the local tick and (possibly) timekeeping. /Users/rubber/linux/kernel/time/tick-common.c: 516
 * Check if this is the last online CPU executing the function and if so, /Users/rubber/linux/kernel/time/tick-common.c: 518
 * suspend timekeeping.  Otherwise suspend the local tick. /Users/rubber/linux/kernel/time/tick-common.c: 519
 * Call with interrupts disabled.  Must be balanced with %tick_unfreeze(). /Users/rubber/linux/kernel/time/tick-common.c: 521
 * Interrupts must not be enabled before the subsequent %tick_unfreeze(). /Users/rubber/linux/kernel/time/tick-common.c: 522
 * tick_unfreeze - Resume the local tick and (possibly) timekeeping. /Users/rubber/linux/kernel/time/tick-common.c: 543
 * Check if this is the first CPU executing the function and if so, resume /Users/rubber/linux/kernel/time/tick-common.c: 545
 * timekeeping.  Otherwise resume the local tick. /Users/rubber/linux/kernel/time/tick-common.c: 546
 * Call with interrupts disabled.  Must be balanced with %tick_freeze(). /Users/rubber/linux/kernel/time/tick-common.c: 548
 * Interrupts must not be enabled after the preceding %tick_freeze(). /Users/rubber/linux/kernel/time/tick-common.c: 549
 * tick_init - initialize the tick control /Users/rubber/linux/kernel/time/tick-common.c: 573
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/posix-clock.c: 1
 * Support for dynamic clock devices /Users/rubber/linux/kernel/time/posix-clock.c: 3
 * Copyright (C) 2010 OMICRON electronics GmbH /Users/rubber/linux/kernel/time/posix-clock.c: 5
 * Returns NULL if the posix_clock instance attached to 'fp' is old and stale. /Users/rubber/linux/kernel/time/posix-clock.c: 18
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/timekeeping.c: 1
 *  Kernel timekeeping code and accessor functions. Based on code from /Users/rubber/linux/kernel/time/timekeeping.c: 3
 *  timer.c, moved in commit 8524070b7982. /Users/rubber/linux/kernel/time/timekeeping.c: 4
 * The most important data for readout fits into a single 64 byte /Users/rubber/linux/kernel/time/timekeeping.c: 45
 * cache line. /Users/rubber/linux/kernel/time/timekeeping.c: 46
 * struct tk_fast - NMI safe timekeeper /Users/rubber/linux/kernel/time/timekeeping.c: 61
 * @seq:	Sequence counter for protecting updates. The lowest bit /Users/rubber/linux/kernel/time/timekeeping.c: 62
 *		is the index for the tk_read_base array /Users/rubber/linux/kernel/time/timekeeping.c: 63
 * @base:	tk_read_base array. Access is indexed by the lowest bit of /Users/rubber/linux/kernel/time/timekeeping.c: 64
 *		@seq. /Users/rubber/linux/kernel/time/timekeeping.c: 65
 * See @update_fast_timekeeper() below. /Users/rubber/linux/kernel/time/timekeeping.c: 67
 * Boot time initialization which allows local_clock() to be utilized /Users/rubber/linux/kernel/time/timekeeping.c: 89
 * during early boot when clocksources are not available. local_clock() /Users/rubber/linux/kernel/time/timekeeping.c: 90
 * returns nanoseconds already so no conversion is required, hence mult=1 /Users/rubber/linux/kernel/time/timekeeping.c: 91
 * and shift=0. When the first proper clocksource is installed then /Users/rubber/linux/kernel/time/timekeeping.c: 92
 * the fast time keepers are updated with the correct values. /Users/rubber/linux/kernel/time/timekeeping.c: 93
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 153
	 * Verify consistency of: offset_real = -wall_to_monotonic /Users/rubber/linux/kernel/time/timekeeping.c: 154
	 * before modifying anything /Users/rubber/linux/kernel/time/timekeeping.c: 155
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 169
	 * Timespec representation for VDSO update to avoid 64bit division /Users/rubber/linux/kernel/time/timekeeping.c: 170
	 * on every update. /Users/rubber/linux/kernel/time/timekeeping.c: 171
 * tk_clock_read - atomic clocksource read() helper /Users/rubber/linux/kernel/time/timekeeping.c: 177
 * This helper is necessary to use in the read paths because, while the /Users/rubber/linux/kernel/time/timekeeping.c: 179
 * seqcount ensures we don't return a bad value while structures are updated, /Users/rubber/linux/kernel/time/timekeeping.c: 180
 * it doesn't protect from potential crashes. There is the possibility that /Users/rubber/linux/kernel/time/timekeeping.c: 181
 * the tkr's clocksource may change between the read reference, and the /Users/rubber/linux/kernel/time/timekeeping.c: 182
 * clock reference passed to the read function.  This can cause crashes if /Users/rubber/linux/kernel/time/timekeeping.c: 183
 * the wrong clocksource is passed to the wrong read function. /Users/rubber/linux/kernel/time/timekeeping.c: 184
 * This isn't necessary to use when holding the timekeeper_lock or doing /Users/rubber/linux/kernel/time/timekeeping.c: 185
 * a read of the fast-timekeeper tkrs (which is protected by its own locking /Users/rubber/linux/kernel/time/timekeeping.c: 186
 * and update logic). /Users/rubber/linux/kernel/time/timekeeping.c: 187
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 244
	 * Since we're called holding a seqcount, the data may shift /Users/rubber/linux/kernel/time/timekeeping.c: 245
	 * under us while we're doing the calculation. This can cause /Users/rubber/linux/kernel/time/timekeeping.c: 246
	 * false positives, since we'd note a problem but throw the /Users/rubber/linux/kernel/time/timekeeping.c: 247
	 * results away. So nest another seqcount here to atomically /Users/rubber/linux/kernel/time/timekeeping.c: 248
	 * grab the points we are checking with. /Users/rubber/linux/kernel/time/timekeeping.c: 249
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 261
	 * Try to catch underflows by checking if we are seeing small /Users/rubber/linux/kernel/time/timekeeping.c: 262
	 * mask-relative negative values. /Users/rubber/linux/kernel/time/timekeeping.c: 263
 * tk_setup_internals - Set up internals to use clocksource clock. /Users/rubber/linux/kernel/time/timekeeping.c: 297
 * @tk:		The target timekeeper to setup. /Users/rubber/linux/kernel/time/timekeeping.c: 299
 * @clock:		Pointer to clocksource. /Users/rubber/linux/kernel/time/timekeeping.c: 300
 * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment /Users/rubber/linux/kernel/time/timekeeping.c: 302
 * pair and interval request. /Users/rubber/linux/kernel/time/timekeeping.c: 303
 * Unless you're the timekeeping code, you should not be using this! /Users/rubber/linux/kernel/time/timekeeping.c: 305
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 359
	 * The timekeeper keeps its own mult values for the currently /Users/rubber/linux/kernel/time/timekeeping.c: 360
	 * active clocksource. These value will be adjusted via NTP /Users/rubber/linux/kernel/time/timekeeping.c: 361
	 * to counteract clock drifting. /Users/rubber/linux/kernel/time/timekeeping.c: 362
 * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper. /Users/rubber/linux/kernel/time/timekeeping.c: 400
 * @tkr: Timekeeping readout base from which we take the update /Users/rubber/linux/kernel/time/timekeeping.c: 401
 * @tkf: Pointer to NMI safe timekeeper /Users/rubber/linux/kernel/time/timekeeping.c: 402
 * We want to use this from any context including NMI and tracing / /Users/rubber/linux/kernel/time/timekeeping.c: 404
 * instrumenting the timekeeping code itself. /Users/rubber/linux/kernel/time/timekeeping.c: 405
 * Employ the latch technique; see @raw_write_seqcount_latch. /Users/rubber/linux/kernel/time/timekeeping.c: 407
 * So if a NMI hits the update of base[0] then it will use base[1] /Users/rubber/linux/kernel/time/timekeeping.c: 409
 * which is still consistent. In the worst case this can result is a /Users/rubber/linux/kernel/time/timekeeping.c: 410
 * slightly wrong timestamp (a few nanoseconds). See /Users/rubber/linux/kernel/time/timekeeping.c: 411
 * @ktime_get_mono_fast_ns. /Users/rubber/linux/kernel/time/timekeeping.c: 412
 * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic /Users/rubber/linux/kernel/time/timekeeping.c: 454
 * This timestamp is not guaranteed to be monotonic across an update. /Users/rubber/linux/kernel/time/timekeeping.c: 456
 * The timestamp is calculated by: /Users/rubber/linux/kernel/time/timekeeping.c: 457
 *	now = base_mono + clock_delta * slope /Users/rubber/linux/kernel/time/timekeeping.c: 459
 * So if the update lowers the slope, readers who are forced to the /Users/rubber/linux/kernel/time/timekeeping.c: 461
 * not yet updated second array are still using the old steeper slope. /Users/rubber/linux/kernel/time/timekeeping.c: 462
 * tmono /Users/rubber/linux/kernel/time/timekeeping.c: 464
 * ^ /Users/rubber/linux/kernel/time/timekeeping.c: 465
 * |    o  n /Users/rubber/linux/kernel/time/timekeeping.c: 466
 * |   o n /Users/rubber/linux/kernel/time/timekeeping.c: 467
 * |  u /Users/rubber/linux/kernel/time/timekeeping.c: 468
 * | o /Users/rubber/linux/kernel/time/timekeeping.c: 469
 * |o /Users/rubber/linux/kernel/time/timekeeping.c: 470
 * |12345678---> reader order /Users/rubber/linux/kernel/time/timekeeping.c: 471
 * o = old slope /Users/rubber/linux/kernel/time/timekeeping.c: 473
 * u = update /Users/rubber/linux/kernel/time/timekeeping.c: 474
 * n = new slope /Users/rubber/linux/kernel/time/timekeeping.c: 475
 * So reader 6 will observe time going backwards versus reader 5. /Users/rubber/linux/kernel/time/timekeeping.c: 477
 * While other CPUs are likely to be able to observe that, the only way /Users/rubber/linux/kernel/time/timekeeping.c: 479
 * for a CPU local observation is when an NMI hits in the middle of /Users/rubber/linux/kernel/time/timekeeping.c: 480
 * the update. Timestamps taken from that NMI context might be ahead /Users/rubber/linux/kernel/time/timekeeping.c: 481
 * of the following timestamps. Callers need to be aware of that and /Users/rubber/linux/kernel/time/timekeeping.c: 482
 * deal with it. /Users/rubber/linux/kernel/time/timekeeping.c: 483
 * ktime_get_raw_fast_ns - Fast NMI safe access to clock monotonic raw /Users/rubber/linux/kernel/time/timekeeping.c: 492
 * Contrary to ktime_get_mono_fast_ns() this is always correct because the /Users/rubber/linux/kernel/time/timekeeping.c: 494
 * conversion factor is not affected by NTP/PTP correction. /Users/rubber/linux/kernel/time/timekeeping.c: 495
 * ktime_get_boot_fast_ns - NMI safe and fast access to boot clock. /Users/rubber/linux/kernel/time/timekeeping.c: 504
 * To keep it NMI safe since we're accessing from tracing, we're not using a /Users/rubber/linux/kernel/time/timekeeping.c: 506
 * separate timekeeper with updates to monotonic clock and boot offset /Users/rubber/linux/kernel/time/timekeeping.c: 507
 * protected with seqcounts. This has the following minor side effects: /Users/rubber/linux/kernel/time/timekeeping.c: 508
 * (1) Its possible that a timestamp be taken after the boot offset is updated /Users/rubber/linux/kernel/time/timekeeping.c: 510
 * but before the timekeeper is updated. If this happens, the new boot offset /Users/rubber/linux/kernel/time/timekeeping.c: 511
 * is added to the old timekeeping making the clock appear to update slightly /Users/rubber/linux/kernel/time/timekeeping.c: 512
 * earlier: /Users/rubber/linux/kernel/time/timekeeping.c: 513
 *    CPU 0                                        CPU 1 /Users/rubber/linux/kernel/time/timekeeping.c: 514
 *    timekeeping_inject_sleeptime64() /Users/rubber/linux/kernel/time/timekeeping.c: 515
 *    __timekeeping_inject_sleeptime(tk, delta); /Users/rubber/linux/kernel/time/timekeeping.c: 516
 *                                                 timestamp(); /Users/rubber/linux/kernel/time/timekeeping.c: 517
 *    timekeeping_update(tk, TK_CLEAR_NTP...); /Users/rubber/linux/kernel/time/timekeeping.c: 518
 * (2) On 32-bit systems, the 64-bit boot offset (tk->offs_boot) may be /Users/rubber/linux/kernel/time/timekeeping.c: 520
 * partially updated.  Since the tk->offs_boot update is a rare event, this /Users/rubber/linux/kernel/time/timekeeping.c: 521
 * should be a rare occurrence which postprocessing should be able to handle. /Users/rubber/linux/kernel/time/timekeeping.c: 522
 * The caveats vs. timestamp ordering as documented for ktime_get_fast_ns() /Users/rubber/linux/kernel/time/timekeeping.c: 524
 * apply as well. /Users/rubber/linux/kernel/time/timekeeping.c: 525
 * ktime_get_real_fast_ns: - NMI safe and fast access to clock realtime. /Users/rubber/linux/kernel/time/timekeeping.c: 558
 * See ktime_get_fast_ns() for documentation of the time stamp ordering. /Users/rubber/linux/kernel/time/timekeeping.c: 560
 * ktime_get_fast_timestamps: - NMI safe timestamps /Users/rubber/linux/kernel/time/timekeeping.c: 569
 * @snapshot:	Pointer to timestamp storage /Users/rubber/linux/kernel/time/timekeeping.c: 570
 * Stores clock monotonic, boottime and realtime timestamps. /Users/rubber/linux/kernel/time/timekeeping.c: 572
 * Boot time is a racy access on 32bit systems if the sleep time injection /Users/rubber/linux/kernel/time/timekeeping.c: 574
 * happens late during resume and not in timekeeping_resume(). That could /Users/rubber/linux/kernel/time/timekeeping.c: 575
 * be avoided by expanding struct tk_read_base with boot offset for 32bit /Users/rubber/linux/kernel/time/timekeeping.c: 576
 * and adding more overhead to the update. As this is a hard to observe /Users/rubber/linux/kernel/time/timekeeping.c: 577
 * once per resume event which can be filtered with reasonable effort using /Users/rubber/linux/kernel/time/timekeeping.c: 578
 * the accurate mono/real timestamps, it's probably not worth the trouble. /Users/rubber/linux/kernel/time/timekeeping.c: 579
 * Aside of that it might be possible on 32 and 64 bit to observe the /Users/rubber/linux/kernel/time/timekeeping.c: 581
 * following when the sleep time injection happens late: /Users/rubber/linux/kernel/time/timekeeping.c: 582
 * CPU 0				CPU 1 /Users/rubber/linux/kernel/time/timekeeping.c: 584
 * timekeeping_resume() /Users/rubber/linux/kernel/time/timekeeping.c: 585
 * ktime_get_fast_timestamps() /Users/rubber/linux/kernel/time/timekeeping.c: 586
 *	mono, real = __ktime_get_real_fast() /Users/rubber/linux/kernel/time/timekeeping.c: 587
 *					inject_sleep_time() /Users/rubber/linux/kernel/time/timekeeping.c: 588
 *					   update boot offset /Users/rubber/linux/kernel/time/timekeeping.c: 589
 *	boot = mono + bootoffset; /Users/rubber/linux/kernel/time/timekeeping.c: 590
 * That means that boot time already has the sleep time adjustment, but /Users/rubber/linux/kernel/time/timekeeping.c: 592
 * real time does not. On the next readout both are in sync again. /Users/rubber/linux/kernel/time/timekeeping.c: 593
 * Preventing this for 64bit is not really feasible without destroying the /Users/rubber/linux/kernel/time/timekeeping.c: 595
 * careful cache layout of the timekeeper because the sequence count and /Users/rubber/linux/kernel/time/timekeeping.c: 596
 * struct tk_read_base would then need two cache lines instead of one. /Users/rubber/linux/kernel/time/timekeeping.c: 597
 * Access to the time keeper clock source is disabled across the innermost /Users/rubber/linux/kernel/time/timekeeping.c: 599
 * steps of suspend/resume. The accessors still work, but the timestamps /Users/rubber/linux/kernel/time/timekeeping.c: 600
 * are frozen until time keeping is resumed which happens very early. /Users/rubber/linux/kernel/time/timekeeping.c: 601
 * For regular suspend/resume there is no observable difference vs. sched /Users/rubber/linux/kernel/time/timekeeping.c: 603
 * clock, but it might affect some of the nasty low level debug printks. /Users/rubber/linux/kernel/time/timekeeping.c: 604
 * OTOH, access to sched clock is not guaranteed across suspend/resume on /Users/rubber/linux/kernel/time/timekeeping.c: 606
 * all systems either so it depends on the hardware in use. /Users/rubber/linux/kernel/time/timekeeping.c: 607
 * If that turns out to be a real problem then this could be mitigated by /Users/rubber/linux/kernel/time/timekeeping.c: 609
 * using sched clock in a similar way as during early boot. But it's not as /Users/rubber/linux/kernel/time/timekeeping.c: 610
 * trivial as on early boot because it needs some careful protection /Users/rubber/linux/kernel/time/timekeeping.c: 611
 * against the clock monotonic timestamp jumping backwards on resume. /Users/rubber/linux/kernel/time/timekeeping.c: 612
 * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource. /Users/rubber/linux/kernel/time/timekeeping.c: 623
 * @tk: Timekeeper to snapshot. /Users/rubber/linux/kernel/time/timekeeping.c: 624
 * It generally is unsafe to access the clocksource after timekeeping has been /Users/rubber/linux/kernel/time/timekeeping.c: 626
 * suspended, so take a snapshot of the readout base of @tk and use it as the /Users/rubber/linux/kernel/time/timekeeping.c: 627
 * fast timekeeper's readout base while suspended.  It will return the same /Users/rubber/linux/kernel/time/timekeeping.c: 628
 * number of cycles every time until timekeeping is resumed at which time the /Users/rubber/linux/kernel/time/timekeeping.c: 629
 * proper readout base for the fast timekeeper will be restored automatically. /Users/rubber/linux/kernel/time/timekeeping.c: 630
 * pvclock_gtod_register_notifier - register a pvclock timedata update listener /Users/rubber/linux/kernel/time/timekeeping.c: 657
 * @nb: Pointer to the notifier block to register /Users/rubber/linux/kernel/time/timekeeping.c: 658
 * pvclock_gtod_unregister_notifier - unregister a pvclock /Users/rubber/linux/kernel/time/timekeeping.c: 676
 * timedata update listener /Users/rubber/linux/kernel/time/timekeeping.c: 677
 * @nb: Pointer to the notifier block to unregister /Users/rubber/linux/kernel/time/timekeeping.c: 678
 * tk_update_leap_state - helper to update the next_leap_ktime /Users/rubber/linux/kernel/time/timekeeping.c: 694
 * Update the ktime_t based scalar nsec members of the timekeeper /Users/rubber/linux/kernel/time/timekeeping.c: 705
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 712
	 * The xtime based monotonic readout is: /Users/rubber/linux/kernel/time/timekeeping.c: 713
	 *	nsec = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec + now(); /Users/rubber/linux/kernel/time/timekeeping.c: 714
	 * The ktime based monotonic readout is: /Users/rubber/linux/kernel/time/timekeeping.c: 715
	 *	nsec = base_mono + now(); /Users/rubber/linux/kernel/time/timekeeping.c: 716
	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec /Users/rubber/linux/kernel/time/timekeeping.c: 717
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 723
	 * The sum of the nanoseconds portions of xtime and /Users/rubber/linux/kernel/time/timekeeping.c: 724
	 * wall_to_monotonic can be greater/equal one second. Take /Users/rubber/linux/kernel/time/timekeeping.c: 725
	 * this into account before updating tk->ktime_sec. /Users/rubber/linux/kernel/time/timekeeping.c: 726
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 757
	 * The mirroring of the data to the shadow-timekeeper needs /Users/rubber/linux/kernel/time/timekeeping.c: 758
	 * to happen last here to ensure we don't over-write the /Users/rubber/linux/kernel/time/timekeeping.c: 759
	 * timekeeper structure on the next update with stale data /Users/rubber/linux/kernel/time/timekeeping.c: 760
 * timekeeping_forward_now - update clock to the current time /Users/rubber/linux/kernel/time/timekeeping.c: 768
 * @tk:		Pointer to the timekeeper to update /Users/rubber/linux/kernel/time/timekeeping.c: 769
 * Forward the current clock to update its state since the last call to /Users/rubber/linux/kernel/time/timekeeping.c: 771
 * update_wall_time(). This is useful before significant clock changes, /Users/rubber/linux/kernel/time/timekeeping.c: 772
 * as it avoids having to deal with this time offset explicitly. /Users/rubber/linux/kernel/time/timekeeping.c: 773
 * ktime_get_real_ts64 - Returns the time of day in a timespec64. /Users/rubber/linux/kernel/time/timekeeping.c: 791
 * @ts:		pointer to the timespec to be set /Users/rubber/linux/kernel/time/timekeeping.c: 792
 * Returns the time of day in a timespec64 (WARN if suspended). /Users/rubber/linux/kernel/time/timekeeping.c: 794
 * ktime_mono_to_any() - convert monotonic time to any other time /Users/rubber/linux/kernel/time/timekeeping.c: 902
 * @tmono:	time to convert. /Users/rubber/linux/kernel/time/timekeeping.c: 903
 * @offs:	which offset to use /Users/rubber/linux/kernel/time/timekeeping.c: 904
 * ktime_get_raw - Returns the raw monotonic time in ktime_t format /Users/rubber/linux/kernel/time/timekeeping.c: 922
 * ktime_get_ts64 - get the monotonic clock in timespec64 format /Users/rubber/linux/kernel/time/timekeeping.c: 943
 * @ts:		pointer to timespec variable /Users/rubber/linux/kernel/time/timekeeping.c: 944
 * The function calculates the monotonic clock from the realtime /Users/rubber/linux/kernel/time/timekeeping.c: 946
 * clock and the wall_to_monotonic offset and stores the result /Users/rubber/linux/kernel/time/timekeeping.c: 947
 * in normalized timespec64 format in the variable pointed to by @ts. /Users/rubber/linux/kernel/time/timekeeping.c: 948
 * ktime_get_seconds - Get the seconds portion of CLOCK_MONOTONIC /Users/rubber/linux/kernel/time/timekeeping.c: 974
 * Returns the seconds portion of CLOCK_MONOTONIC with a single non /Users/rubber/linux/kernel/time/timekeeping.c: 976
 * serialized read. tk->ktime_sec is of type 'unsigned long' so this /Users/rubber/linux/kernel/time/timekeeping.c: 977
 * works on both 32 and 64 bit systems. On 32 bit systems the readout /Users/rubber/linux/kernel/time/timekeeping.c: 978
 * covers ~136 years of uptime which should be enough to prevent /Users/rubber/linux/kernel/time/timekeeping.c: 979
 * premature wrap arounds. /Users/rubber/linux/kernel/time/timekeeping.c: 980
 * ktime_get_real_seconds - Get the seconds portion of CLOCK_REALTIME /Users/rubber/linux/kernel/time/timekeeping.c: 992
 * Returns the wall clock seconds since 1970. /Users/rubber/linux/kernel/time/timekeeping.c: 994
 * For 64bit systems the fast access to tk->xtime_sec is preserved. On /Users/rubber/linux/kernel/time/timekeeping.c: 996
 * 32bit systems the access must be protected with the sequence /Users/rubber/linux/kernel/time/timekeeping.c: 997
 * counter to provide "atomic" access to the 64bit tk->xtime_sec /Users/rubber/linux/kernel/time/timekeeping.c: 998
 * value. /Users/rubber/linux/kernel/time/timekeeping.c: 999
 * __ktime_get_real_seconds - The same as ktime_get_real_seconds /Users/rubber/linux/kernel/time/timekeeping.c: 1021
 * but without the sequence counter protect. This internal function /Users/rubber/linux/kernel/time/timekeeping.c: 1022
 * is called just when timekeeping lock is already held. /Users/rubber/linux/kernel/time/timekeeping.c: 1023
 * ktime_get_snapshot - snapshots the realtime/monotonic raw clocks with counter /Users/rubber/linux/kernel/time/timekeeping.c: 1033
 * @systime_snapshot:	pointer to struct receiving the system time snapshot /Users/rubber/linux/kernel/time/timekeeping.c: 1034
 * adjust_historical_crosststamp - adjust crosstimestamp previous to current interval /Users/rubber/linux/kernel/time/timekeeping.c: 1085
 * @history:			Snapshot representing start of history /Users/rubber/linux/kernel/time/timekeeping.c: 1086
 * @partial_history_cycles:	Cycle offset into history (fractional part) /Users/rubber/linux/kernel/time/timekeeping.c: 1087
 * @total_history_cycles:	Total history length in cycles /Users/rubber/linux/kernel/time/timekeeping.c: 1088
 * @discontinuity:		True indicates clock was set on history period /Users/rubber/linux/kernel/time/timekeeping.c: 1089
 * @ts:				Cross timestamp that should be adjusted using /Users/rubber/linux/kernel/time/timekeeping.c: 1090
 *	partial/total ratio /Users/rubber/linux/kernel/time/timekeeping.c: 1091
 * Helper function used by get_device_system_crosststamp() to correct the /Users/rubber/linux/kernel/time/timekeeping.c: 1093
 * crosstimestamp corresponding to the start of the current interval to the /Users/rubber/linux/kernel/time/timekeeping.c: 1094
 * system counter value (timestamp point) provided by the driver. The /Users/rubber/linux/kernel/time/timekeeping.c: 1095
 * total_history_* quantities are the total history starting at the provided /Users/rubber/linux/kernel/time/timekeeping.c: 1096
 * reference point and ending at the start of the current interval. The cycle /Users/rubber/linux/kernel/time/timekeeping.c: 1097
 * count between the driver timestamp point and the start of the current /Users/rubber/linux/kernel/time/timekeeping.c: 1098
 * interval is partial_history_cycles. /Users/rubber/linux/kernel/time/timekeeping.c: 1099
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1121
	 * Scale the monotonic raw time delta by: /Users/rubber/linux/kernel/time/timekeeping.c: 1122
	 *	partial_history_cycles / total_history_cycles /Users/rubber/linux/kernel/time/timekeeping.c: 1123
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1132
	 * If there is a discontinuity in the history, scale monotonic raw /Users/rubber/linux/kernel/time/timekeeping.c: 1133
	 *	correction by: /Users/rubber/linux/kernel/time/timekeeping.c: 1134
	 *	mult(real)/mult(raw) yielding the realtime correction /Users/rubber/linux/kernel/time/timekeeping.c: 1135
	 * Otherwise, calculate the realtime correction similar to monotonic /Users/rubber/linux/kernel/time/timekeeping.c: 1136
	 *	raw calculation /Users/rubber/linux/kernel/time/timekeeping.c: 1137
 * cycle_between - true if test occurs chronologically between before and after /Users/rubber/linux/kernel/time/timekeeping.c: 1164
 * get_device_system_crosststamp - Synchronously capture system/device timestamp /Users/rubber/linux/kernel/time/timekeeping.c: 1176
 * @get_time_fn:	Callback to get simultaneous device time and /Users/rubber/linux/kernel/time/timekeeping.c: 1177
 *	system counter from the device driver /Users/rubber/linux/kernel/time/timekeeping.c: 1178
 * @ctx:		Context passed to get_time_fn() /Users/rubber/linux/kernel/time/timekeeping.c: 1179
 * @history_begin:	Historical reference point used to interpolate system /Users/rubber/linux/kernel/time/timekeeping.c: 1180
 *	time when counter provided by the driver is before the current interval /Users/rubber/linux/kernel/time/timekeeping.c: 1181
 * @xtstamp:		Receives simultaneously captured system and device time /Users/rubber/linux/kernel/time/timekeeping.c: 1182
 * Reads a timestamp from a device and correlates it to system time /Users/rubber/linux/kernel/time/timekeeping.c: 1184
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 1207
		 * Try to synchronously capture device time and a system /Users/rubber/linux/kernel/time/timekeeping.c: 1208
		 * counter value calling back into the device driver /Users/rubber/linux/kernel/time/timekeeping.c: 1209
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 1215
		 * Verify that the clocksource associated with the captured /Users/rubber/linux/kernel/time/timekeeping.c: 1216
		 * system counter value is the same as the currently installed /Users/rubber/linux/kernel/time/timekeeping.c: 1217
		 * timekeeper clocksource /Users/rubber/linux/kernel/time/timekeeping.c: 1218
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 1224
		 * Check whether the system counter value provided by the /Users/rubber/linux/kernel/time/timekeeping.c: 1225
		 * device driver is on the current timekeeping interval. /Users/rubber/linux/kernel/time/timekeeping.c: 1226
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1252
	 * Interpolate if necessary, adjusting back from the start of the /Users/rubber/linux/kernel/time/timekeeping.c: 1253
	 * current interval /Users/rubber/linux/kernel/time/timekeeping.c: 1254
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 1260
		 * Check that the counter value occurs after the provided /Users/rubber/linux/kernel/time/timekeeping.c: 1261
		 * history reference and that the history doesn't cross a /Users/rubber/linux/kernel/time/timekeeping.c: 1262
		 * clocksource change /Users/rubber/linux/kernel/time/timekeeping.c: 1263
 * do_settimeofday64 - Sets the time of day. /Users/rubber/linux/kernel/time/timekeeping.c: 1288
 * @ts:     pointer to the timespec64 variable containing the new time /Users/rubber/linux/kernel/time/timekeeping.c: 1289
 * Sets the time of day to the new time and update NTP and notify hrtimers /Users/rubber/linux/kernel/time/timekeeping.c: 1291
 * timekeeping_inject_offset - Adds or subtracts from the current time. /Users/rubber/linux/kernel/time/timekeeping.c: 1337
 * @ts:		Pointer to the timespec variable containing the offset /Users/rubber/linux/kernel/time/timekeeping.c: 1338
 * Adds or subtracts an offset value from the current time. /Users/rubber/linux/kernel/time/timekeeping.c: 1340
 * Indicates if there is an offset between the system clock and the hardware /Users/rubber/linux/kernel/time/timekeeping.c: 1381
 * clock/persistent clock/rtc. /Users/rubber/linux/kernel/time/timekeeping.c: 1382
 * Adjust the time obtained from the CMOS to be UTC time instead of /Users/rubber/linux/kernel/time/timekeeping.c: 1387
 * local time. /Users/rubber/linux/kernel/time/timekeeping.c: 1388
 * This is ugly, but preferable to the alternatives.  Otherwise we /Users/rubber/linux/kernel/time/timekeeping.c: 1390
 * would either need to write a program to do it in /etc/rc (and risk /Users/rubber/linux/kernel/time/timekeeping.c: 1391
 * confusion if the program gets run more than once; it would also be /Users/rubber/linux/kernel/time/timekeeping.c: 1392
 * hard to make the program warp the clock precisely n hours)  or /Users/rubber/linux/kernel/time/timekeeping.c: 1393
 * compile in the timezone information into the kernel.  Bad, bad.... /Users/rubber/linux/kernel/time/timekeeping.c: 1394
 *						- TYT, 1992-01-01 /Users/rubber/linux/kernel/time/timekeeping.c: 1396
 * The best thing to do is to keep the CMOS clock in universal time (UTC) /Users/rubber/linux/kernel/time/timekeeping.c: 1398
 * as real UNIX machines always do it. This avoids all headaches about /Users/rubber/linux/kernel/time/timekeeping.c: 1399
 * daylight saving times and warping kernel clocks. /Users/rubber/linux/kernel/time/timekeeping.c: 1400
 * __timekeeping_set_tai_offset - Sets the TAI offset from UTC and monotonic /Users/rubber/linux/kernel/time/timekeeping.c: 1415
 * change_clocksource - Swaps clocksources if a new one is available /Users/rubber/linux/kernel/time/timekeeping.c: 1424
 * Accumulates current time interval and initializes new clocksource /Users/rubber/linux/kernel/time/timekeeping.c: 1426
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1437
	 * If the cs is in module, get a module reference. Succeeds /Users/rubber/linux/kernel/time/timekeeping.c: 1438
	 * for built-in code (owner == NULL) as well. /Users/rubber/linux/kernel/time/timekeeping.c: 1439
 * timekeeping_notify - Install a new clock source /Users/rubber/linux/kernel/time/timekeeping.c: 1474
 * @clock:		pointer to the clock source /Users/rubber/linux/kernel/time/timekeeping.c: 1475
 * This function is called from clocksource.c after a new, better clock /Users/rubber/linux/kernel/time/timekeeping.c: 1477
 * source has been registered. The caller holds the clocksource_mutex. /Users/rubber/linux/kernel/time/timekeeping.c: 1478
 * ktime_get_raw_ts64 - Returns the raw monotonic time in a timespec /Users/rubber/linux/kernel/time/timekeeping.c: 1492
 * @ts:		pointer to the timespec64 to be set /Users/rubber/linux/kernel/time/timekeeping.c: 1493
 * Returns the raw monotonic time (completely un-modified by ntp) /Users/rubber/linux/kernel/time/timekeeping.c: 1495
 * timekeeping_valid_for_hres - Check if timekeeping is suitable for hres /Users/rubber/linux/kernel/time/timekeeping.c: 1517
 * timekeeping_max_deferment - Returns max time the clocksource can be deferred /Users/rubber/linux/kernel/time/timekeeping.c: 1536
 * read_persistent_clock64 -  Return time from the persistent clock. /Users/rubber/linux/kernel/time/timekeeping.c: 1555
 * @ts: Pointer to the storage for the readout value /Users/rubber/linux/kernel/time/timekeeping.c: 1556
 * Weak dummy function for arches that do not yet support it. /Users/rubber/linux/kernel/time/timekeeping.c: 1558
 * Reads the time from the battery backed persistent clock. /Users/rubber/linux/kernel/time/timekeeping.c: 1559
 * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported. /Users/rubber/linux/kernel/time/timekeeping.c: 1560
 *  XXX - Do be sure to remove it once all arches implement it. /Users/rubber/linux/kernel/time/timekeeping.c: 1562
 * read_persistent_wall_and_boot_offset - Read persistent clock, and also offset /Users/rubber/linux/kernel/time/timekeeping.c: 1571
 *                                        from the boot. /Users/rubber/linux/kernel/time/timekeeping.c: 1572
 * Weak dummy function for arches that do not yet support it. /Users/rubber/linux/kernel/time/timekeeping.c: 1574
 * @wall_time:	- current time as returned by persistent clock /Users/rubber/linux/kernel/time/timekeeping.c: 1575
 * @boot_offset: - offset that is defined as wall_time - boot_time /Users/rubber/linux/kernel/time/timekeeping.c: 1576
 * The default function calculates offset based on the current value of /Users/rubber/linux/kernel/time/timekeeping.c: 1578
 * local_clock(). This way architectures that support sched_clock() but don't /Users/rubber/linux/kernel/time/timekeeping.c: 1579
 * support dedicated boot time clock will provide the best estimate of the /Users/rubber/linux/kernel/time/timekeeping.c: 1580
 * boot time. /Users/rubber/linux/kernel/time/timekeeping.c: 1581
 * Flag reflecting whether timekeeping_resume() has injected sleeptime. /Users/rubber/linux/kernel/time/timekeeping.c: 1592
 * The flag starts of false and is only set when a suspend reaches /Users/rubber/linux/kernel/time/timekeeping.c: 1594
 * timekeeping_suspend(), timekeeping_resume() sets it to false when the /Users/rubber/linux/kernel/time/timekeeping.c: 1595
 * timekeeper clocksource is not stopping across suspend and has been /Users/rubber/linux/kernel/time/timekeeping.c: 1596
 * used to update sleep time. If the timekeeper clocksource has stopped /Users/rubber/linux/kernel/time/timekeeping.c: 1597
 * then the flag stays true and is used by the RTC resume code to decide /Users/rubber/linux/kernel/time/timekeeping.c: 1598
 * whether sleeptime must be injected and if so the flag gets false then. /Users/rubber/linux/kernel/time/timekeeping.c: 1599
 * If a suspend fails before reaching timekeeping_resume() then the flag /Users/rubber/linux/kernel/time/timekeeping.c: 1601
 * stays false and prevents erroneous sleeptime injection. /Users/rubber/linux/kernel/time/timekeeping.c: 1602
 * timekeeping_init - Initializes the clocksource and common timekeeping values /Users/rubber/linux/kernel/time/timekeeping.c: 1610
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1631
	 * We want set wall_to_mono, so the following is true: /Users/rubber/linux/kernel/time/timekeeping.c: 1632
	 * wall time + wall_to_mono = boot time /Users/rubber/linux/kernel/time/timekeeping.c: 1633
 * __timekeeping_inject_sleeptime - Internal function to add sleep interval /Users/rubber/linux/kernel/time/timekeeping.c: 1661
 * @tk:		Pointer to the timekeeper to be updated /Users/rubber/linux/kernel/time/timekeeping.c: 1662
 * @delta:	Pointer to the delta value in timespec64 format /Users/rubber/linux/kernel/time/timekeeping.c: 1663
 * Takes a timespec offset measuring a suspend interval and properly /Users/rubber/linux/kernel/time/timekeeping.c: 1665
 * adds the sleep offset to the timekeeping variables. /Users/rubber/linux/kernel/time/timekeeping.c: 1666
 * We have three kinds of time sources to use for sleep time /Users/rubber/linux/kernel/time/timekeeping.c: 1685
 * injection, the preference order is: /Users/rubber/linux/kernel/time/timekeeping.c: 1686
 * 1) non-stop clocksource /Users/rubber/linux/kernel/time/timekeeping.c: 1687
 * 2) persistent clock (ie: RTC accessible when irqs are off) /Users/rubber/linux/kernel/time/timekeeping.c: 1688
 * 3) RTC /Users/rubber/linux/kernel/time/timekeeping.c: 1689
 * 1) and 2) are used by timekeeping, 3) by RTC subsystem. /Users/rubber/linux/kernel/time/timekeeping.c: 1691
 * If system has neither 1) nor 2), 3) will be used finally. /Users/rubber/linux/kernel/time/timekeeping.c: 1692
 * If timekeeping has injected sleeptime via either 1) or 2), /Users/rubber/linux/kernel/time/timekeeping.c: 1695
 * 3) becomes needless, so in this case we don't need to call /Users/rubber/linux/kernel/time/timekeeping.c: 1696
 * rtc_resume(), and this is what timekeeping_rtc_skipresume() /Users/rubber/linux/kernel/time/timekeeping.c: 1697
 * means. /Users/rubber/linux/kernel/time/timekeeping.c: 1698
 * 1) can be determined whether to use or not only when doing /Users/rubber/linux/kernel/time/timekeeping.c: 1706
 * timekeeping_resume() which is invoked after rtc_suspend(), /Users/rubber/linux/kernel/time/timekeeping.c: 1707
 * so we can't skip rtc_suspend() surely if system has 1). /Users/rubber/linux/kernel/time/timekeeping.c: 1708
 * But if system has 2), 2) will definitely be used, so in this /Users/rubber/linux/kernel/time/timekeeping.c: 1710
 * case we don't need to call rtc_suspend(), and this is what /Users/rubber/linux/kernel/time/timekeeping.c: 1711
 * timekeeping_rtc_skipsuspend() means. /Users/rubber/linux/kernel/time/timekeeping.c: 1712
 * timekeeping_inject_sleeptime64 - Adds suspend interval to timeekeeping values /Users/rubber/linux/kernel/time/timekeeping.c: 1720
 * @delta: pointer to a timespec64 delta value /Users/rubber/linux/kernel/time/timekeeping.c: 1721
 * This hook is for architectures that cannot support read_persistent_clock64 /Users/rubber/linux/kernel/time/timekeeping.c: 1723
 * because their RTC/persistent clock is only accessible when irqs are enabled. /Users/rubber/linux/kernel/time/timekeeping.c: 1724
 * and also don't have an effective nonstop clocksource. /Users/rubber/linux/kernel/time/timekeeping.c: 1725
 * This function should only be called by rtc_resume(), and allows /Users/rubber/linux/kernel/time/timekeeping.c: 1727
 * a suspend offset to be injected into the timekeeping values. /Users/rubber/linux/kernel/time/timekeeping.c: 1728
 * timekeeping_resume - Resumes the generic timekeeping subsystem. /Users/rubber/linux/kernel/time/timekeeping.c: 1755
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1774
	 * After system resumes, we need to calculate the suspended time and /Users/rubber/linux/kernel/time/timekeeping.c: 1775
	 * compensate it for the OS time. There are 3 sources that could be /Users/rubber/linux/kernel/time/timekeeping.c: 1776
	 * used: Nonstop clocksource during suspend, persistent clock and rtc /Users/rubber/linux/kernel/time/timekeeping.c: 1777
	 * device. /Users/rubber/linux/kernel/time/timekeeping.c: 1778
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1779
	 * One specific platform may have 1 or 2 or all of them, and the /Users/rubber/linux/kernel/time/timekeeping.c: 1780
	 * preference will be: /Users/rubber/linux/kernel/time/timekeeping.c: 1781
	 *	suspend-nonstop clocksource -> persistent clock -> rtc /Users/rubber/linux/kernel/time/timekeeping.c: 1782
	 * The less preferred source will only be tried if there is no better /Users/rubber/linux/kernel/time/timekeeping.c: 1783
	 * usable source. The rtc part is handled separately in rtc core code. /Users/rubber/linux/kernel/time/timekeeping.c: 1784
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1830
	 * On some systems the persistent_clock can not be detected at /Users/rubber/linux/kernel/time/timekeeping.c: 1831
	 * timekeeping_init by its return value, so if we see a valid /Users/rubber/linux/kernel/time/timekeeping.c: 1832
	 * value returned, update the persistent_clock_exists flag. /Users/rubber/linux/kernel/time/timekeeping.c: 1833
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1845
	 * Since we've called forward_now, cycle_last stores the value /Users/rubber/linux/kernel/time/timekeeping.c: 1846
	 * just read from the current clocksource. Save this to potentially /Users/rubber/linux/kernel/time/timekeeping.c: 1847
	 * use in suspend timing. /Users/rubber/linux/kernel/time/timekeeping.c: 1848
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 1855
		 * To avoid drift caused by repeated suspend/resumes, /Users/rubber/linux/kernel/time/timekeeping.c: 1856
		 * which each can add ~1 second drift error, /Users/rubber/linux/kernel/time/timekeeping.c: 1857
		 * try to compensate so the difference in system time /Users/rubber/linux/kernel/time/timekeeping.c: 1858
		 * and persistent_clock time stays close to constant. /Users/rubber/linux/kernel/time/timekeeping.c: 1859
			/* /Users/rubber/linux/kernel/time/timekeeping.c: 1864
			 * if delta_delta is too large, assume time correction /Users/rubber/linux/kernel/time/timekeeping.c: 1865
			 * has occurred and set old_delta to the current delta. /Users/rubber/linux/kernel/time/timekeeping.c: 1866
 * Apply a multiplier adjustment to the timekeeper /Users/rubber/linux/kernel/time/timekeeping.c: 1902
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1920
	 * So the following can be confusing. /Users/rubber/linux/kernel/time/timekeeping.c: 1921
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1922
	 * To keep things simple, lets assume mult_adj == 1 for now. /Users/rubber/linux/kernel/time/timekeeping.c: 1923
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1924
	 * When mult_adj != 1, remember that the interval and offset values /Users/rubber/linux/kernel/time/timekeeping.c: 1925
	 * have been appropriately scaled so the math is the same. /Users/rubber/linux/kernel/time/timekeeping.c: 1926
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1927
	 * The basic idea here is that we're increasing the multiplier /Users/rubber/linux/kernel/time/timekeeping.c: 1928
	 * by one, this causes the xtime_interval to be incremented by /Users/rubber/linux/kernel/time/timekeeping.c: 1929
	 * one cycle_interval. This is because: /Users/rubber/linux/kernel/time/timekeeping.c: 1930
	 *	xtime_interval = cycle_interval * mult /Users/rubber/linux/kernel/time/timekeeping.c: 1931
	 * So if mult is being incremented by one: /Users/rubber/linux/kernel/time/timekeeping.c: 1932
	 *	xtime_interval = cycle_interval * (mult + 1) /Users/rubber/linux/kernel/time/timekeeping.c: 1933
	 * Its the same as: /Users/rubber/linux/kernel/time/timekeeping.c: 1934
	 *	xtime_interval = (cycle_interval * mult) + cycle_interval /Users/rubber/linux/kernel/time/timekeeping.c: 1935
	 * Which can be shortened to: /Users/rubber/linux/kernel/time/timekeeping.c: 1936
	 *	xtime_interval += cycle_interval /Users/rubber/linux/kernel/time/timekeeping.c: 1937
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1938
	 * So offset stores the non-accumulated cycles. Thus the current /Users/rubber/linux/kernel/time/timekeeping.c: 1939
	 * time (in shifted nanoseconds) is: /Users/rubber/linux/kernel/time/timekeeping.c: 1940
	 *	now = (offset * adj) + xtime_nsec /Users/rubber/linux/kernel/time/timekeeping.c: 1941
	 * Now, even though we're adjusting the clock frequency, we have /Users/rubber/linux/kernel/time/timekeeping.c: 1942
	 * to keep time consistent. In other words, we can't jump back /Users/rubber/linux/kernel/time/timekeeping.c: 1943
	 * in time, and we also want to avoid jumping forward in time. /Users/rubber/linux/kernel/time/timekeeping.c: 1944
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 1945
	 * So given the same offset value, we need the time to be the same /Users/rubber/linux/kernel/time/timekeeping.c: 1946
	 * both before and after the freq adjustment. /Users/rubber/linux/kernel/time/timekeeping.c: 1947
	 *	now = (offset * adj_1) + xtime_nsec_1 /Users/rubber/linux/kernel/time/timekeeping.c: 1948
	 *	now = (offset * adj_2) + xtime_nsec_2 /Users/rubber/linux/kernel/time/timekeeping.c: 1949
	 * So: /Users/rubber/linux/kernel/time/timekeeping.c: 1950
	 *	(offset * adj_1) + xtime_nsec_1 = /Users/rubber/linux/kernel/time/timekeeping.c: 1951
	 *		(offset * adj_2) + xtime_nsec_2 /Users/rubber/linux/kernel/time/timekeeping.c: 1952
	 * And we know: /Users/rubber/linux/kernel/time/timekeeping.c: 1953
	 *	adj_2 = adj_1 + 1 /Users/rubber/linux/kernel/time/timekeeping.c: 1954
	 * So: /Users/rubber/linux/kernel/time/timekeeping.c: 1955
	 *	(offset * adj_1) + xtime_nsec_1 = /Users/rubber/linux/kernel/time/timekeeping.c: 1956
	 *		(offset * (adj_1+1)) + xtime_nsec_2 /Users/rubber/linux/kernel/time/timekeeping.c: 1957
	 *	(offset * adj_1) + xtime_nsec_1 = /Users/rubber/linux/kernel/time/timekeeping.c: 1958
	 *		(offset * adj_1) + offset + xtime_nsec_2 /Users/rubber/linux/kernel/time/timekeeping.c: 1959
	 * Canceling the sides: /Users/rubber/linux/kernel/time/timekeeping.c: 1960
	 *	xtime_nsec_1 = offset + xtime_nsec_2 /Users/rubber/linux/kernel/time/timekeeping.c: 1961
	 * Which gives us: /Users/rubber/linux/kernel/time/timekeeping.c: 1962
	 *	xtime_nsec_2 = xtime_nsec_1 - offset /Users/rubber/linux/kernel/time/timekeeping.c: 1963
	 * Which simplifies to: /Users/rubber/linux/kernel/time/timekeeping.c: 1964
	 *	xtime_nsec -= offset /Users/rubber/linux/kernel/time/timekeeping.c: 1965
 * Adjust the timekeeper's multiplier to the correct frequency /Users/rubber/linux/kernel/time/timekeeping.c: 1979
 * and also to reduce the accumulated error value. /Users/rubber/linux/kernel/time/timekeeping.c: 1980
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1986
	 * Determine the multiplier from the current NTP tick length. /Users/rubber/linux/kernel/time/timekeeping.c: 1987
	 * Avoid expensive division when the tick length doesn't change. /Users/rubber/linux/kernel/time/timekeeping.c: 1988
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 1998
	 * If the clock is behind the NTP time, increase the multiplier by 1 /Users/rubber/linux/kernel/time/timekeeping.c: 1999
	 * to catch up with it. If it's ahead and there was a remainder in the /Users/rubber/linux/kernel/time/timekeeping.c: 2000
	 * tick division, the clock will slow down. Otherwise it will stay /Users/rubber/linux/kernel/time/timekeeping.c: 2001
	 * ahead until the tick length changes to a non-divisible value. /Users/rubber/linux/kernel/time/timekeeping.c: 2002
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 2018
	 * It may be possible that when we entered this function, xtime_nsec /Users/rubber/linux/kernel/time/timekeeping.c: 2019
	 * was very small.  Further, if we're slightly speeding the clocksource /Users/rubber/linux/kernel/time/timekeeping.c: 2020
	 * in the code above, its possible the required corrective factor to /Users/rubber/linux/kernel/time/timekeeping.c: 2021
	 * xtime_nsec could cause it to underflow. /Users/rubber/linux/kernel/time/timekeeping.c: 2022
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 2023
	 * Now, since we have already accumulated the second and the NTP /Users/rubber/linux/kernel/time/timekeeping.c: 2024
	 * subsystem has been notified via second_overflow(), we need to skip /Users/rubber/linux/kernel/time/timekeeping.c: 2025
	 * the next update. /Users/rubber/linux/kernel/time/timekeeping.c: 2026
 * accumulate_nsecs_to_secs - Accumulates nsecs into secs /Users/rubber/linux/kernel/time/timekeeping.c: 2037
 * Helper function that accumulates the nsecs greater than a second /Users/rubber/linux/kernel/time/timekeeping.c: 2039
 * from the xtime_nsec field to the xtime_secs field. /Users/rubber/linux/kernel/time/timekeeping.c: 2040
 * It also calls into the NTP code to handle leapsecond processing. /Users/rubber/linux/kernel/time/timekeeping.c: 2041
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 2054
		 * Skip NTP update if this second was accumulated before, /Users/rubber/linux/kernel/time/timekeeping.c: 2055
		 * i.e. xtime_nsec underflowed in timekeeping_adjust() /Users/rubber/linux/kernel/time/timekeeping.c: 2056
 * logarithmic_accumulation - shifted accumulation of cycles /Users/rubber/linux/kernel/time/timekeeping.c: 2084
 * This functions accumulates a shifted interval of cycles into /Users/rubber/linux/kernel/time/timekeeping.c: 2086
 * a shifted interval nanoseconds. Allows for O(log) accumulation /Users/rubber/linux/kernel/time/timekeeping.c: 2087
 * loop. /Users/rubber/linux/kernel/time/timekeeping.c: 2088
 * Returns the unconsumed cycles. /Users/rubber/linux/kernel/time/timekeeping.c: 2090
 * timekeeping_advance - Updates the timekeeper to the current time and /Users/rubber/linux/kernel/time/timekeeping.c: 2127
 * current NTP tick length /Users/rubber/linux/kernel/time/timekeeping.c: 2128
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 2155
	 * With NO_HZ we may have to accumulate many cycle_intervals /Users/rubber/linux/kernel/time/timekeeping.c: 2156
	 * (think "ticks") worth of time at once. To do this efficiently, /Users/rubber/linux/kernel/time/timekeeping.c: 2157
	 * we calculate the largest doubling multiple of cycle_intervals /Users/rubber/linux/kernel/time/timekeeping.c: 2158
	 * that is smaller than the offset.  We then accumulate that /Users/rubber/linux/kernel/time/timekeeping.c: 2159
	 * chunk in one go, and then try to consume the next smaller /Users/rubber/linux/kernel/time/timekeeping.c: 2160
	 * doubled multiple. /Users/rubber/linux/kernel/time/timekeeping.c: 2161
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 2178
	 * Finally, make sure that after the rounding /Users/rubber/linux/kernel/time/timekeeping.c: 2179
	 * xtime_nsec isn't larger than NSEC_PER_SEC /Users/rubber/linux/kernel/time/timekeeping.c: 2180
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 2185
	 * Update the real timekeeper. /Users/rubber/linux/kernel/time/timekeeping.c: 2186
	 * /Users/rubber/linux/kernel/time/timekeeping.c: 2187
	 * We could avoid this memcpy by switching pointers, but that /Users/rubber/linux/kernel/time/timekeeping.c: 2188
	 * requires changes to all other timekeeper usage sites as /Users/rubber/linux/kernel/time/timekeeping.c: 2189
	 * well, i.e. move the timekeeper pointer getter into the /Users/rubber/linux/kernel/time/timekeeping.c: 2190
	 * spinlocked/seqcount protected sections. And we trade this /Users/rubber/linux/kernel/time/timekeeping.c: 2191
	 * memcpy under the tk_core.seq against one before we start /Users/rubber/linux/kernel/time/timekeeping.c: 2192
	 * updating. /Users/rubber/linux/kernel/time/timekeeping.c: 2193
 * update_wall_time - Uses the current clocksource to increment the wall time /Users/rubber/linux/kernel/time/timekeeping.c: 2206
 * getboottime64 - Return the real time of system boot. /Users/rubber/linux/kernel/time/timekeeping.c: 2216
 * @ts:		pointer to the timespec64 to be set /Users/rubber/linux/kernel/time/timekeeping.c: 2217
 * Returns the wall-time of boot in a timespec64. /Users/rubber/linux/kernel/time/timekeeping.c: 2219
 * This is based on the wall_to_monotonic offset and the total suspend /Users/rubber/linux/kernel/time/timekeeping.c: 2221
 * time. Calls to settimeofday will affect the value returned (which /Users/rubber/linux/kernel/time/timekeeping.c: 2222
 * basically means that however wrong your real time clock is at boot time, /Users/rubber/linux/kernel/time/timekeeping.c: 2223
 * you get the right time here). /Users/rubber/linux/kernel/time/timekeeping.c: 2224
 * Must hold jiffies_lock /Users/rubber/linux/kernel/time/timekeeping.c: 2267
 * ktime_get_update_offsets_now - hrtimer helper /Users/rubber/linux/kernel/time/timekeeping.c: 2276
 * @cwsseq:	pointer to check and store the clock was set sequence number /Users/rubber/linux/kernel/time/timekeeping.c: 2277
 * @offs_real:	pointer to storage for monotonic -> realtime offset /Users/rubber/linux/kernel/time/timekeeping.c: 2278
 * @offs_boot:	pointer to storage for monotonic -> boottime offset /Users/rubber/linux/kernel/time/timekeeping.c: 2279
 * @offs_tai:	pointer to storage for monotonic -> clock tai offset /Users/rubber/linux/kernel/time/timekeeping.c: 2280
 * Returns current monotonic time and updates the offsets if the /Users/rubber/linux/kernel/time/timekeeping.c: 2282
 * sequence number in @cwsseq and timekeeper.clock_was_set_seq are /Users/rubber/linux/kernel/time/timekeeping.c: 2283
 * different. /Users/rubber/linux/kernel/time/timekeeping.c: 2284
 * Called from hrtimer_interrupt() or retrigger_next_event() /Users/rubber/linux/kernel/time/timekeeping.c: 2286
 * timekeeping_validate_timex - Ensures the timex is ok for use in do_adjtimex /Users/rubber/linux/kernel/time/timekeeping.c: 2320
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 2335
		 * if the quartz is off by more than 10% then /Users/rubber/linux/kernel/time/timekeeping.c: 2336
		 * something is VERY wrong! /Users/rubber/linux/kernel/time/timekeeping.c: 2337
		/* /Users/rubber/linux/kernel/time/timekeeping.c: 2350
		 * Validate if a timespec/timeval used to inject a time /Users/rubber/linux/kernel/time/timekeeping.c: 2351
		 * offset is valid.  Offsets can be positive or negative, so /Users/rubber/linux/kernel/time/timekeeping.c: 2352
		 * we don't check tv_sec. The value of the timeval/timespec /Users/rubber/linux/kernel/time/timekeeping.c: 2353
		 * is the sum of its fields,but *NOTE*: /Users/rubber/linux/kernel/time/timekeeping.c: 2354
		 * The field tv_usec/tv_nsec must always be non-negative and /Users/rubber/linux/kernel/time/timekeeping.c: 2355
		 * we can't have more nanoseconds/microseconds than a second. /Users/rubber/linux/kernel/time/timekeeping.c: 2356
	/* /Users/rubber/linux/kernel/time/timekeeping.c: 2370
	 * Check for potential multiplication overflows that can /Users/rubber/linux/kernel/time/timekeeping.c: 2371
	 * only happen on 64-bit systems: /Users/rubber/linux/kernel/time/timekeeping.c: 2372
 * do_adjtimex() - Accessor function to NTP __do_adjtimex function /Users/rubber/linux/kernel/time/timekeeping.c: 2386
 * hardpps() - Accessor function to NTP __hardpps function /Users/rubber/linux/kernel/time/timekeeping.c: 2452
 SPDX-License-Identifier: LGPL-2.1+ /Users/rubber/linux/kernel/time/time_test.c: 1
 * Traditional implementation of leap year evaluation. /Users/rubber/linux/kernel/time/time_test.c: 7
 * Gets the last day of a month. /Users/rubber/linux/kernel/time/time_test.c: 15
 * Advances a date by one day. /Users/rubber/linux/kernel/time/time_test.c: 27
 * Checks every day in a 160000 years interval centered at 1970-01-01 /Users/rubber/linux/kernel/time/time_test.c: 50
 * against the expected result. /Users/rubber/linux/kernel/time/time_test.c: 51
	/* /Users/rubber/linux/kernel/time/time_test.c: 55
	 * 80000 years	= (80000 / 400) * 400 years /Users/rubber/linux/kernel/time/time_test.c: 56
	 *		= (80000 / 400) * 146097 days /Users/rubber/linux/kernel/time/time_test.c: 57
	 *		= (80000 / 400) * 146097 * 86400 seconds /Users/rubber/linux/kernel/time/time_test.c: 58
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/clocksource.c: 1
 * This file contains the functions which manage clocksource drivers. /Users/rubber/linux/kernel/time/clocksource.c: 3
 * Copyright (C) 2004, 2005 IBM, John Stultz (johnstul@us.ibm.com) /Users/rubber/linux/kernel/time/clocksource.c: 5
 * clocks_calc_mult_shift - calculate mult/shift factors for scaled math of clocks /Users/rubber/linux/kernel/time/clocksource.c: 24
 * @mult:	pointer to mult variable /Users/rubber/linux/kernel/time/clocksource.c: 25
 * @shift:	pointer to shift variable /Users/rubber/linux/kernel/time/clocksource.c: 26
 * @from:	frequency to convert from /Users/rubber/linux/kernel/time/clocksource.c: 27
 * @to:		frequency to convert to /Users/rubber/linux/kernel/time/clocksource.c: 28
 * @maxsec:	guaranteed runtime conversion range in seconds /Users/rubber/linux/kernel/time/clocksource.c: 29
 * The function evaluates the shift/mult pair for the scaled math /Users/rubber/linux/kernel/time/clocksource.c: 31
 * operations of clocksources and clockevents. /Users/rubber/linux/kernel/time/clocksource.c: 32
 * @to and @from are frequency values in HZ. For clock sources @to is /Users/rubber/linux/kernel/time/clocksource.c: 34
 * NSEC_PER_SEC == 1GHz and @from is the counter frequency. For clock /Users/rubber/linux/kernel/time/clocksource.c: 35
 * event @to is the counter frequency and @from is NSEC_PER_SEC. /Users/rubber/linux/kernel/time/clocksource.c: 36
 * The @maxsec conversion range argument controls the time frame in /Users/rubber/linux/kernel/time/clocksource.c: 38
 * seconds which must be covered by the runtime conversion with the /Users/rubber/linux/kernel/time/clocksource.c: 39
 * calculated mult and shift factors. This guarantees that no 64bit /Users/rubber/linux/kernel/time/clocksource.c: 40
 * overflow happens when the input value of the conversion is /Users/rubber/linux/kernel/time/clocksource.c: 41
 * multiplied with the calculated mult factor. Larger ranges may /Users/rubber/linux/kernel/time/clocksource.c: 42
 * reduce the conversion accuracy by choosing smaller mult and shift /Users/rubber/linux/kernel/time/clocksource.c: 43
 * factors. /Users/rubber/linux/kernel/time/clocksource.c: 44
	/* /Users/rubber/linux/kernel/time/clocksource.c: 52
	 * Calculate the shift factor which is limiting the conversion /Users/rubber/linux/kernel/time/clocksource.c: 53
	 * range: /Users/rubber/linux/kernel/time/clocksource.c: 54
	/* /Users/rubber/linux/kernel/time/clocksource.c: 62
	 * Find the conversion shift/mult pair which has the best /Users/rubber/linux/kernel/time/clocksource.c: 63
	 * accuracy and fits the maxsec conversion range: /Users/rubber/linux/kernel/time/clocksource.c: 64
/*[Clocksource internal variables]--------- /Users/rubber/linux/kernel/time/clocksource.c: 78
 * curr_clocksource: /Users/rubber/linux/kernel/time/clocksource.c: 79
 *	currently selected clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 80
 * suspend_clocksource: /Users/rubber/linux/kernel/time/clocksource.c: 81
 *	used to calculate the suspend time. /Users/rubber/linux/kernel/time/clocksource.c: 82
 * clocksource_list: /Users/rubber/linux/kernel/time/clocksource.c: 83
 *	linked list with the registered clocksources /Users/rubber/linux/kernel/time/clocksource.c: 84
 * clocksource_mutex: /Users/rubber/linux/kernel/time/clocksource.c: 85
 *	protects manipulations to curr_clocksource and the clocksource_list /Users/rubber/linux/kernel/time/clocksource.c: 86
 * override_name: /Users/rubber/linux/kernel/time/clocksource.c: 87
 *	Name of the user-specified clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 88
 * Threshold: 0.0312s, when doubled: 0.0625s. /Users/rubber/linux/kernel/time/clocksource.c: 99
 * Also a default for cs->uncertainty_margin when registering clocks. /Users/rubber/linux/kernel/time/clocksource.c: 100
 * Maximum permissible delay between two readouts of the watchdog /Users/rubber/linux/kernel/time/clocksource.c: 105
 * clocksource surrounding a read of the clocksource being validated. /Users/rubber/linux/kernel/time/clocksource.c: 106
 * This delay could be due to SMIs, NMIs, or to VCPU preemptions.  Used as /Users/rubber/linux/kernel/time/clocksource.c: 107
 * a lower bound for cs->uncertainty_margin values when registering clocks. /Users/rubber/linux/kernel/time/clocksource.c: 108
 * Interval: 0.5sec. /Users/rubber/linux/kernel/time/clocksource.c: 138
	/* /Users/rubber/linux/kernel/time/clocksource.c: 144
	 * We cannot directly run clocksource_watchdog_kthread() here, because /Users/rubber/linux/kernel/time/clocksource.c: 145
	 * clocksource_select() calls timekeeping_notify() which uses /Users/rubber/linux/kernel/time/clocksource.c: 146
	 * stop_machine(). One cannot use stop_machine() from a workqueue() due /Users/rubber/linux/kernel/time/clocksource.c: 147
	 * lock inversions wrt CPU hotplug. /Users/rubber/linux/kernel/time/clocksource.c: 148
	 * /Users/rubber/linux/kernel/time/clocksource.c: 149
	 * Also, we only ever run this work once or twice during the lifetime /Users/rubber/linux/kernel/time/clocksource.c: 150
	 * of the kernel, so there is no point in creating a more permanent /Users/rubber/linux/kernel/time/clocksource.c: 151
	 * kthread for this. /Users/rubber/linux/kernel/time/clocksource.c: 152
	 * /Users/rubber/linux/kernel/time/clocksource.c: 153
	 * If kthread_run fails the next watchdog scan over the /Users/rubber/linux/kernel/time/clocksource.c: 154
	 * watchdog_list will find the unstable clock again. /Users/rubber/linux/kernel/time/clocksource.c: 155
	/* /Users/rubber/linux/kernel/time/clocksource.c: 165
	 * If the clocksource is registered clocksource_watchdog_kthread() will /Users/rubber/linux/kernel/time/clocksource.c: 166
	 * re-rate and re-select. /Users/rubber/linux/kernel/time/clocksource.c: 167
 * clocksource_mark_unstable - mark clocksource unstable via watchdog /Users/rubber/linux/kernel/time/clocksource.c: 183
 * @cs:		clocksource to be marked unstable /Users/rubber/linux/kernel/time/clocksource.c: 184
 * This function is called by the x86 TSC code to mark clocksources as unstable; /Users/rubber/linux/kernel/time/clocksource.c: 186
 * it defers demotion and re-selection to a kthread. /Users/rubber/linux/kernel/time/clocksource.c: 187
	/* /Users/rubber/linux/kernel/time/clocksource.c: 271
	 * Randomly select the specified number of CPUs.  If the same /Users/rubber/linux/kernel/time/clocksource.c: 272
	 * CPU is selected multiple times, that CPU is checked only once, /Users/rubber/linux/kernel/time/clocksource.c: 273
	 * and no replacement CPU is selected.  This gracefully handles /Users/rubber/linux/kernel/time/clocksource.c: 274
	 * situations where verify_n_cpus is greater than the number of /Users/rubber/linux/kernel/time/clocksource.c: 275
	 * CPUs that are currently online. /Users/rubber/linux/kernel/time/clocksource.c: 276
			/* /Users/rubber/linux/kernel/time/clocksource.c: 433
			 * clocksource_done_booting() will sort it if /Users/rubber/linux/kernel/time/clocksource.c: 434
			 * finished_booting is not set yet. /Users/rubber/linux/kernel/time/clocksource.c: 435
			/* /Users/rubber/linux/kernel/time/clocksource.c: 440
			 * If this is not the current clocksource let /Users/rubber/linux/kernel/time/clocksource.c: 441
			 * the watchdog thread reselect it. Due to the /Users/rubber/linux/kernel/time/clocksource.c: 442
			 * change to high res this clocksource might /Users/rubber/linux/kernel/time/clocksource.c: 443
			 * be preferred now. If it is the current /Users/rubber/linux/kernel/time/clocksource.c: 444
			 * clocksource let the tick code know about /Users/rubber/linux/kernel/time/clocksource.c: 445
			 * that change. /Users/rubber/linux/kernel/time/clocksource.c: 446
	/* /Users/rubber/linux/kernel/time/clocksource.c: 457
	 * We only clear the watchdog_reset_pending, when we did a /Users/rubber/linux/kernel/time/clocksource.c: 458
	 * full cycle through all clocksources. /Users/rubber/linux/kernel/time/clocksource.c: 459
	/* /Users/rubber/linux/kernel/time/clocksource.c: 464
	 * Cycle through CPUs to check if the CPUs stay synchronized /Users/rubber/linux/kernel/time/clocksource.c: 465
	 * to each other. /Users/rubber/linux/kernel/time/clocksource.c: 466
	/* /Users/rubber/linux/kernel/time/clocksource.c: 472
	 * Arm timer if not already pending: could race with concurrent /Users/rubber/linux/kernel/time/clocksource.c: 473
	 * pair clocksource_stop_watchdog() clocksource_start_watchdog(). /Users/rubber/linux/kernel/time/clocksource.c: 474
	/* /Users/rubber/linux/kernel/time/clocksource.c: 651
	 * Skip the clocksource which will be stopped in suspend state. /Users/rubber/linux/kernel/time/clocksource.c: 652
	/* /Users/rubber/linux/kernel/time/clocksource.c: 657
	 * The nonstop clocksource can be selected as the suspend clocksource to /Users/rubber/linux/kernel/time/clocksource.c: 658
	 * calculate the suspend time, so it should not supply suspend/resume /Users/rubber/linux/kernel/time/clocksource.c: 659
	 * interfaces to suspend the nonstop clocksource when system suspends. /Users/rubber/linux/kernel/time/clocksource.c: 660
 * clocksource_suspend_select - Select the best clocksource for suspend timing /Users/rubber/linux/kernel/time/clocksource.c: 673
 * @fallback:	if select a fallback clocksource /Users/rubber/linux/kernel/time/clocksource.c: 674
 * clocksource_start_suspend_timing - Start measuring the suspend timing /Users/rubber/linux/kernel/time/clocksource.c: 694
 * @cs:			current clocksource from timekeeping /Users/rubber/linux/kernel/time/clocksource.c: 695
 * @start_cycles:	current cycles from timekeeping /Users/rubber/linux/kernel/time/clocksource.c: 696
 * This function will save the start cycle values of suspend timer to calculate /Users/rubber/linux/kernel/time/clocksource.c: 698
 * the suspend time when resuming system. /Users/rubber/linux/kernel/time/clocksource.c: 699
 * This function is called late in the suspend process from timekeeping_suspend(), /Users/rubber/linux/kernel/time/clocksource.c: 701
 * that means processes are frozen, non-boot cpus and interrupts are disabled /Users/rubber/linux/kernel/time/clocksource.c: 702
 * now. It is therefore possible to start the suspend timer without taking the /Users/rubber/linux/kernel/time/clocksource.c: 703
 * clocksource mutex. /Users/rubber/linux/kernel/time/clocksource.c: 704
	/* /Users/rubber/linux/kernel/time/clocksource.c: 711
	 * If current clocksource is the suspend timer, we should use the /Users/rubber/linux/kernel/time/clocksource.c: 712
	 * tkr_mono.cycle_last value as suspend_start to avoid same reading /Users/rubber/linux/kernel/time/clocksource.c: 713
	 * from suspend timer. /Users/rubber/linux/kernel/time/clocksource.c: 714
 * clocksource_stop_suspend_timing - Stop measuring the suspend timing /Users/rubber/linux/kernel/time/clocksource.c: 731
 * @cs:		current clocksource from timekeeping /Users/rubber/linux/kernel/time/clocksource.c: 732
 * @cycle_now:	current cycles from timekeeping /Users/rubber/linux/kernel/time/clocksource.c: 733
 * This function will calculate the suspend time from suspend timer. /Users/rubber/linux/kernel/time/clocksource.c: 735
 * Returns nanoseconds since suspend started, 0 if no usable suspend clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 737
 * This function is called early in the resume process from timekeeping_resume(), /Users/rubber/linux/kernel/time/clocksource.c: 739
 * that means there is only one cpu, no processes are running and the interrupts /Users/rubber/linux/kernel/time/clocksource.c: 740
 * are disabled. It is therefore possible to stop the suspend timer without /Users/rubber/linux/kernel/time/clocksource.c: 741
 * taking the clocksource mutex. /Users/rubber/linux/kernel/time/clocksource.c: 742
	/* /Users/rubber/linux/kernel/time/clocksource.c: 751
	 * If current clocksource is the suspend timer, we should use the /Users/rubber/linux/kernel/time/clocksource.c: 752
	 * tkr_mono.cycle_last value from timekeeping as current cycle to /Users/rubber/linux/kernel/time/clocksource.c: 753
	 * avoid same reading from suspend timer. /Users/rubber/linux/kernel/time/clocksource.c: 754
	/* /Users/rubber/linux/kernel/time/clocksource.c: 768
	 * Disable the suspend timer to save power if current clocksource is /Users/rubber/linux/kernel/time/clocksource.c: 769
	 * not the suspend timer. /Users/rubber/linux/kernel/time/clocksource.c: 770
 * clocksource_suspend - suspend the clocksource(s) /Users/rubber/linux/kernel/time/clocksource.c: 779
 * clocksource_resume - resume the clocksource(s) /Users/rubber/linux/kernel/time/clocksource.c: 791
 * clocksource_touch_watchdog - Update watchdog /Users/rubber/linux/kernel/time/clocksource.c: 805
 * Update the watchdog after exception contexts such as kgdb so as not /Users/rubber/linux/kernel/time/clocksource.c: 807
 * to incorrectly trip the watchdog. This might fail when the kernel /Users/rubber/linux/kernel/time/clocksource.c: 808
 * was stopped in code which holds watchdog_lock. /Users/rubber/linux/kernel/time/clocksource.c: 809
 * clocksource_max_adjustment- Returns max adjustment amount /Users/rubber/linux/kernel/time/clocksource.c: 817
 * @cs:         Pointer to clocksource /Users/rubber/linux/kernel/time/clocksource.c: 818
	/* /Users/rubber/linux/kernel/time/clocksource.c: 824
	 * We won't try to correct for more than 11% adjustments (110,000 ppm), /Users/rubber/linux/kernel/time/clocksource.c: 825
 * clocks_calc_max_nsecs - Returns maximum nanoseconds that can be converted /Users/rubber/linux/kernel/time/clocksource.c: 833
 * @mult:	cycle to nanosecond multiplier /Users/rubber/linux/kernel/time/clocksource.c: 834
 * @shift:	cycle to nanosecond divisor (power of two) /Users/rubber/linux/kernel/time/clocksource.c: 835
 * @maxadj:	maximum adjustment value to mult (~11%) /Users/rubber/linux/kernel/time/clocksource.c: 836
 * @mask:	bitmask for two's complement subtraction of non 64 bit counters /Users/rubber/linux/kernel/time/clocksource.c: 837
 * @max_cyc:	maximum cycle value before potential overflow (does not include /Users/rubber/linux/kernel/time/clocksource.c: 838
 *		any safety margin) /Users/rubber/linux/kernel/time/clocksource.c: 839
 * NOTE: This function includes a safety margin of 50%, in other words, we /Users/rubber/linux/kernel/time/clocksource.c: 841
 * return half the number of nanoseconds the hardware counter can technically /Users/rubber/linux/kernel/time/clocksource.c: 842
 * cover. This is done so that we can potentially detect problems caused by /Users/rubber/linux/kernel/time/clocksource.c: 843
 * delayed timers or bad hardware, which might result in time intervals that /Users/rubber/linux/kernel/time/clocksource.c: 844
 * are larger than what the math used can handle without overflows. /Users/rubber/linux/kernel/time/clocksource.c: 845
	/* /Users/rubber/linux/kernel/time/clocksource.c: 851
	 * Calculate the maximum number of cycles that we can pass to the /Users/rubber/linux/kernel/time/clocksource.c: 852
	 * cyc2ns() function without overflowing a 64-bit result. /Users/rubber/linux/kernel/time/clocksource.c: 853
	/* /Users/rubber/linux/kernel/time/clocksource.c: 858
	 * The actual maximum number of cycles we can defer the clocksource is /Users/rubber/linux/kernel/time/clocksource.c: 859
	 * determined by the minimum of max_cycles and mask. /Users/rubber/linux/kernel/time/clocksource.c: 860
	 * Note: Here we subtract the maxadj to make sure we don't sleep for /Users/rubber/linux/kernel/time/clocksource.c: 861
	 * too long if there's a large negative adjustment. /Users/rubber/linux/kernel/time/clocksource.c: 862
 * clocksource_update_max_deferment - Updates the clocksource max_idle_ns & max_cycles /Users/rubber/linux/kernel/time/clocksource.c: 878
 * @cs:         Pointer to clocksource to be updated /Users/rubber/linux/kernel/time/clocksource.c: 879
	/* /Users/rubber/linux/kernel/time/clocksource.c: 896
	 * We pick the clocksource with the highest rating. If oneshot /Users/rubber/linux/kernel/time/clocksource.c: 897
	 * mode is active, we pick the highres valid clocksource with /Users/rubber/linux/kernel/time/clocksource.c: 898
	 * the best rating. /Users/rubber/linux/kernel/time/clocksource.c: 899
		/* /Users/rubber/linux/kernel/time/clocksource.c: 930
		 * Check to make sure we don't switch to a non-highres /Users/rubber/linux/kernel/time/clocksource.c: 931
		 * capable clocksource if the tick code is in oneshot /Users/rubber/linux/kernel/time/clocksource.c: 932
		 * mode (highres or nohz) /Users/rubber/linux/kernel/time/clocksource.c: 933
				/* /Users/rubber/linux/kernel/time/clocksource.c: 942
				 * The override cannot be currently verified. /Users/rubber/linux/kernel/time/clocksource.c: 943
				 * Deferring to let the watchdog check. /Users/rubber/linux/kernel/time/clocksource.c: 944
 * clocksource_select - Select the best clocksource available /Users/rubber/linux/kernel/time/clocksource.c: 963
 * Private function. Must hold clocksource_mutex when called. /Users/rubber/linux/kernel/time/clocksource.c: 965
 * Select the clocksource with the best rating, or the clocksource, /Users/rubber/linux/kernel/time/clocksource.c: 967
 * which is selected by userspace override. /Users/rubber/linux/kernel/time/clocksource.c: 968
 * clocksource_done_booting - Called near the end of core bootup /Users/rubber/linux/kernel/time/clocksource.c: 981
 * Hack to avoid lots of clocksource churn at boot time. /Users/rubber/linux/kernel/time/clocksource.c: 983
 * We use fs_initcall because we want this to start before /Users/rubber/linux/kernel/time/clocksource.c: 984
 * device_initcall but after subsys_initcall. /Users/rubber/linux/kernel/time/clocksource.c: 985
	/* /Users/rubber/linux/kernel/time/clocksource.c: 992
	 * Run the watchdog first to eliminate unstable clock sources /Users/rubber/linux/kernel/time/clocksource.c: 993
 * Enqueue the clocksource sorted by rating /Users/rubber/linux/kernel/time/clocksource.c: 1003
 * __clocksource_update_freq_scale - Used update clocksource with new freq /Users/rubber/linux/kernel/time/clocksource.c: 1020
 * @cs:		clocksource to be registered /Users/rubber/linux/kernel/time/clocksource.c: 1021
 * @scale:	Scale factor multiplied against freq to get clocksource hz /Users/rubber/linux/kernel/time/clocksource.c: 1022
 * @freq:	clocksource frequency (cycles per second) divided by scale /Users/rubber/linux/kernel/time/clocksource.c: 1023
 * This should only be called from the clocksource->enable() method. /Users/rubber/linux/kernel/time/clocksource.c: 1025
 * This *SHOULD NOT* be called directly! Please use the /Users/rubber/linux/kernel/time/clocksource.c: 1027
 * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper /Users/rubber/linux/kernel/time/clocksource.c: 1028
 * functions. /Users/rubber/linux/kernel/time/clocksource.c: 1029
	/* /Users/rubber/linux/kernel/time/clocksource.c: 1035
	 * Default clocksources are *special* and self-define their mult/shift. /Users/rubber/linux/kernel/time/clocksource.c: 1036
	 * But, you're not special, so you should specify a freq value. /Users/rubber/linux/kernel/time/clocksource.c: 1037
		/* /Users/rubber/linux/kernel/time/clocksource.c: 1040
		 * Calc the maximum number of seconds which we can run before /Users/rubber/linux/kernel/time/clocksource.c: 1041
		 * wrapping around. For clocksources which have a mask > 32-bit /Users/rubber/linux/kernel/time/clocksource.c: 1042
		 * we need to limit the max sleep time to have a good /Users/rubber/linux/kernel/time/clocksource.c: 1043
		 * conversion precision. 10 minutes is still a reasonable /Users/rubber/linux/kernel/time/clocksource.c: 1044
		 * amount. That results in a shift value of 24 for a /Users/rubber/linux/kernel/time/clocksource.c: 1045
		 * clocksource with mask >= 40-bit and f >= 4GHz. That maps to /Users/rubber/linux/kernel/time/clocksource.c: 1046
		 * ~ 0.06ppm granularity for NTP. /Users/rubber/linux/kernel/time/clocksource.c: 1047
	/* /Users/rubber/linux/kernel/time/clocksource.c: 1061
	 * If the uncertainty margin is not specified, calculate it. /Users/rubber/linux/kernel/time/clocksource.c: 1062
	 * If both scale and freq are non-zero, calculate the clock /Users/rubber/linux/kernel/time/clocksource.c: 1063
	 * period, but bound below at 2*WATCHDOG_MAX_SKEW.  However, /Users/rubber/linux/kernel/time/clocksource.c: 1064
	 * if either of scale or freq is zero, be very conservative and /Users/rubber/linux/kernel/time/clocksource.c: 1065
	 * take the tens-of-milliseconds WATCHDOG_THRESHOLD value for the /Users/rubber/linux/kernel/time/clocksource.c: 1066
	 * uncertainty margin.  Allow stupidly small uncertainty margins /Users/rubber/linux/kernel/time/clocksource.c: 1067
	 * to be specified by the caller for testing purposes, but warn /Users/rubber/linux/kernel/time/clocksource.c: 1068
	 * to discourage production use of this capability. /Users/rubber/linux/kernel/time/clocksource.c: 1069
	/* /Users/rubber/linux/kernel/time/clocksource.c: 1080
	 * Ensure clocksources that have large 'mult' values don't overflow /Users/rubber/linux/kernel/time/clocksource.c: 1081
	 * when adjusted. /Users/rubber/linux/kernel/time/clocksource.c: 1082
	/* /Users/rubber/linux/kernel/time/clocksource.c: 1092
	 * Only warn for *special* clocksources that self-define /Users/rubber/linux/kernel/time/clocksource.c: 1093
	 * their mult/shift values and don't specify a freq. /Users/rubber/linux/kernel/time/clocksource.c: 1094
 * __clocksource_register_scale - Used to install new clocksources /Users/rubber/linux/kernel/time/clocksource.c: 1108
 * @cs:		clocksource to be registered /Users/rubber/linux/kernel/time/clocksource.c: 1109
 * @scale:	Scale factor multiplied against freq to get clocksource hz /Users/rubber/linux/kernel/time/clocksource.c: 1110
 * @freq:	clocksource frequency (cycles per second) divided by scale /Users/rubber/linux/kernel/time/clocksource.c: 1111
 * Returns -EBUSY if registration fails, zero otherwise. /Users/rubber/linux/kernel/time/clocksource.c: 1113
 * This *SHOULD NOT* be called directly! Please use the /Users/rubber/linux/kernel/time/clocksource.c: 1115
 * clocksource_register_hz() or clocksource_register_khz helper functions. /Users/rubber/linux/kernel/time/clocksource.c: 1116
 * clocksource_change_rating - Change the rating of a registered clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1160
 * @cs:		clocksource to be changed /Users/rubber/linux/kernel/time/clocksource.c: 1161
 * @rating:	new rating /Users/rubber/linux/kernel/time/clocksource.c: 1162
 * Unbind clocksource @cs. Called with clocksource_mutex held /Users/rubber/linux/kernel/time/clocksource.c: 1181
		/* /Users/rubber/linux/kernel/time/clocksource.c: 1202
		 * Select and try to install a replacement suspend clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 1203
		 * If no replacement suspend clocksource, we will just let the /Users/rubber/linux/kernel/time/clocksource.c: 1204
		 * clocksource go and have no suspend clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 1205
 * clocksource_unregister - remove a registered clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1219
 * @cs:	clocksource to be unregistered /Users/rubber/linux/kernel/time/clocksource.c: 1220
 * current_clocksource_show - sysfs interface for current clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1236
 * @dev:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1237
 * @attr:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1238
 * @buf:	char buffer to be filled with clocksource list /Users/rubber/linux/kernel/time/clocksource.c: 1239
 * Provides sysfs interface for listing current clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 1241
 * current_clocksource_store - interface for manually overriding clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1274
 * @dev:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1275
 * @attr:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1276
 * @buf:	name of override clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1277
 * @count:	length of buffer /Users/rubber/linux/kernel/time/clocksource.c: 1278
 * Takes input from sysfs interface for manually overriding the default /Users/rubber/linux/kernel/time/clocksource.c: 1280
 * clocksource selection. /Users/rubber/linux/kernel/time/clocksource.c: 1281
 * unbind_clocksource_store - interface for manually unbinding clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1302
 * @dev:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1303
 * @attr:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1304
 * @buf:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1305
 * @count:	length of buffer /Users/rubber/linux/kernel/time/clocksource.c: 1306
 * Takes input from sysfs interface for manually unbinding a clocksource. /Users/rubber/linux/kernel/time/clocksource.c: 1308
 * available_clocksource_show - sysfs interface for listing clocksource /Users/rubber/linux/kernel/time/clocksource.c: 1337
 * @dev:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1338
 * @attr:	unused /Users/rubber/linux/kernel/time/clocksource.c: 1339
 * @buf:	char buffer to be filled with clocksource list /Users/rubber/linux/kernel/time/clocksource.c: 1340
 * Provides sysfs interface for listing registered clocksources /Users/rubber/linux/kernel/time/clocksource.c: 1342
		/* /Users/rubber/linux/kernel/time/clocksource.c: 1353
		 * Don't show non-HRES clocksource if the tick code is /Users/rubber/linux/kernel/time/clocksource.c: 1354
		 * in one shot mode (highres=on or nohz=on) /Users/rubber/linux/kernel/time/clocksource.c: 1355
 * boot_override_clocksource - boot clock override /Users/rubber/linux/kernel/time/clocksource.c: 1405
 * @str:	override name /Users/rubber/linux/kernel/time/clocksource.c: 1406
 * Takes a clocksource= boot argument and uses it /Users/rubber/linux/kernel/time/clocksource.c: 1408
 * as the clocksource override name. /Users/rubber/linux/kernel/time/clocksource.c: 1409
 * boot_override_clock - Compatibility layer for deprecated boot option /Users/rubber/linux/kernel/time/clocksource.c: 1423
 * @str:	override name /Users/rubber/linux/kernel/time/clocksource.c: 1424
 * DEPRECATED! Takes a clock= boot argument and uses it /Users/rubber/linux/kernel/time/clocksource.c: 1426
 * as the clocksource override name /Users/rubber/linux/kernel/time/clocksource.c: 1427
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/clockevents.c: 1
 * This file contains functions which manage clock event devices. /Users/rubber/linux/kernel/time/clockevents.c: 3
 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/clockevents.c: 5
 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/clockevents.c: 6
 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/clockevents.c: 7
	/* /Users/rubber/linux/kernel/time/clockevents.c: 42
	 * Upper bound sanity check. If the backwards conversion is /Users/rubber/linux/kernel/time/clockevents.c: 43
	 * not equal latch, we know that the above shift overflowed. /Users/rubber/linux/kernel/time/clockevents.c: 44
	/* /Users/rubber/linux/kernel/time/clockevents.c: 49
	 * Scaled math oddities: /Users/rubber/linux/kernel/time/clockevents.c: 50
	 * /Users/rubber/linux/kernel/time/clockevents.c: 51
	 * For mult <= (1 << shift) we can safely add mult - 1 to /Users/rubber/linux/kernel/time/clockevents.c: 52
	 * prevent integer rounding loss. So the backwards conversion /Users/rubber/linux/kernel/time/clockevents.c: 53
	 * from nsec to device ticks will be correct. /Users/rubber/linux/kernel/time/clockevents.c: 54
	 * /Users/rubber/linux/kernel/time/clockevents.c: 55
	 * For mult > (1 << shift), i.e. device frequency is > 1GHz we /Users/rubber/linux/kernel/time/clockevents.c: 56
	 * need to be careful. Adding mult - 1 will result in a value /Users/rubber/linux/kernel/time/clockevents.c: 57
	 * which when converted back to device ticks can be larger /Users/rubber/linux/kernel/time/clockevents.c: 58
	 * than latch by up to (mult - 1) >> shift. For the min_delta /Users/rubber/linux/kernel/time/clockevents.c: 59
	 * calculation we still want to apply this in order to stay /Users/rubber/linux/kernel/time/clockevents.c: 60
	 * above the minimum device ticks limit. For the upper limit /Users/rubber/linux/kernel/time/clockevents.c: 61
	 * we would end up with a latch value larger than the upper /Users/rubber/linux/kernel/time/clockevents.c: 62
	 * limit of the device, so we omit the add to stay below the /Users/rubber/linux/kernel/time/clockevents.c: 63
	 * device upper boundary. /Users/rubber/linux/kernel/time/clockevents.c: 64
	 * /Users/rubber/linux/kernel/time/clockevents.c: 65
	 * Also omit the add if it would overflow the u64 boundary. /Users/rubber/linux/kernel/time/clockevents.c: 66
 * clockevents_delta2ns - Convert a latch value (device ticks) to nanoseconds /Users/rubber/linux/kernel/time/clockevents.c: 79
 * @latch:	value to convert /Users/rubber/linux/kernel/time/clockevents.c: 80
 * @evt:	pointer to clock event device descriptor /Users/rubber/linux/kernel/time/clockevents.c: 81
 * Math helper, returns latch value converted to nanoseconds (bound checked) /Users/rubber/linux/kernel/time/clockevents.c: 83
 * clockevents_switch_state - set the operating state of a clock event device /Users/rubber/linux/kernel/time/clockevents.c: 141
 * @dev:	device to modify /Users/rubber/linux/kernel/time/clockevents.c: 142
 * @state:	new state /Users/rubber/linux/kernel/time/clockevents.c: 143
 * Must be called with interrupts disabled ! /Users/rubber/linux/kernel/time/clockevents.c: 145
		/* /Users/rubber/linux/kernel/time/clockevents.c: 156
		 * A nsec2cyc multiplicator of 0 is invalid and we'd crash /Users/rubber/linux/kernel/time/clockevents.c: 157
		 * on it, so fix it up and emit a warning: /Users/rubber/linux/kernel/time/clockevents.c: 158
 * clockevents_shutdown - shutdown the device and clear next_event /Users/rubber/linux/kernel/time/clockevents.c: 168
 * @dev:	device to shutdown /Users/rubber/linux/kernel/time/clockevents.c: 169
 * clockevents_tick_resume -	Resume the tick device before using it again /Users/rubber/linux/kernel/time/clockevents.c: 178
 * @dev:			device to resume /Users/rubber/linux/kernel/time/clockevents.c: 179
 * clockevents_increase_min_delta - raise minimum delta of a clock event device /Users/rubber/linux/kernel/time/clockevents.c: 197
 * @dev:       device to increase the minimum delta /Users/rubber/linux/kernel/time/clockevents.c: 198
 * Returns 0 on success, -ETIME when the minimum delta reached the limit. /Users/rubber/linux/kernel/time/clockevents.c: 200
 * clockevents_program_min_delta - Set clock event device to the minimum delay. /Users/rubber/linux/kernel/time/clockevents.c: 228
 * @dev:	device to program /Users/rubber/linux/kernel/time/clockevents.c: 229
 * Returns 0 on success, -ETIME when the retry loop failed. /Users/rubber/linux/kernel/time/clockevents.c: 231
			/* /Users/rubber/linux/kernel/time/clockevents.c: 252
			 * We tried 3 times to program the device with the /Users/rubber/linux/kernel/time/clockevents.c: 253
			 * given min_delta_ns. Try to increase the minimum /Users/rubber/linux/kernel/time/clockevents.c: 254
			 * delta, if that fails as well get out of here. /Users/rubber/linux/kernel/time/clockevents.c: 255
 * clockevents_program_min_delta - Set clock event device to the minimum delay. /Users/rubber/linux/kernel/time/clockevents.c: 267
 * @dev:	device to program /Users/rubber/linux/kernel/time/clockevents.c: 268
 * Returns 0 on success, -ETIME when the retry loop failed. /Users/rubber/linux/kernel/time/clockevents.c: 270
 * clockevents_program_event - Reprogram the clock event device. /Users/rubber/linux/kernel/time/clockevents.c: 296
 * @dev:	device to program /Users/rubber/linux/kernel/time/clockevents.c: 297
 * @expires:	absolute expiry time (monotonic clock) /Users/rubber/linux/kernel/time/clockevents.c: 298
 * @force:	program minimum delay if expires can not be set /Users/rubber/linux/kernel/time/clockevents.c: 299
 * Returns 0 on success, -ETIME when the event is in the past. /Users/rubber/linux/kernel/time/clockevents.c: 301
 * Called after a notify add to make devices available which were /Users/rubber/linux/kernel/time/clockevents.c: 340
 * released from the notifier call. /Users/rubber/linux/kernel/time/clockevents.c: 341
 * Try to install a replacement clock event device /Users/rubber/linux/kernel/time/clockevents.c: 356
 * Called with clockevents_mutex and clockevents_lock held /Users/rubber/linux/kernel/time/clockevents.c: 384
 * SMP function call to unbind a device /Users/rubber/linux/kernel/time/clockevents.c: 398
 * Issues smp function call to unbind a per cpu device. Called with /Users/rubber/linux/kernel/time/clockevents.c: 414
 * clockevents_mutex held. /Users/rubber/linux/kernel/time/clockevents.c: 415
 * Unbind a clockevents device. /Users/rubber/linux/kernel/time/clockevents.c: 426
 * clockevents_register_device - register a clock event device /Users/rubber/linux/kernel/time/clockevents.c: 440
 * @dev:	device to register /Users/rubber/linux/kernel/time/clockevents.c: 441
	/* /Users/rubber/linux/kernel/time/clockevents.c: 478
	 * Calculate the maximum number of seconds we can sleep. Limit /Users/rubber/linux/kernel/time/clockevents.c: 479
	 * to 10 minutes for hardware which can program more than /Users/rubber/linux/kernel/time/clockevents.c: 480
	 * 32bit ticks so we still get reasonable conversion values. /Users/rubber/linux/kernel/time/clockevents.c: 481
 * clockevents_config_and_register - Configure and register a clock event device /Users/rubber/linux/kernel/time/clockevents.c: 496
 * @dev:	device to register /Users/rubber/linux/kernel/time/clockevents.c: 497
 * @freq:	The clock frequency /Users/rubber/linux/kernel/time/clockevents.c: 498
 * @min_delta:	The minimum clock ticks to program in oneshot mode /Users/rubber/linux/kernel/time/clockevents.c: 499
 * @max_delta:	The maximum clock ticks to program in oneshot mode /Users/rubber/linux/kernel/time/clockevents.c: 500
 * min/max_delta can be 0 for devices which do not support oneshot mode. /Users/rubber/linux/kernel/time/clockevents.c: 502
 * clockevents_update_freq - Update frequency and reprogram a clock event device. /Users/rubber/linux/kernel/time/clockevents.c: 529
 * @dev:	device to modify /Users/rubber/linux/kernel/time/clockevents.c: 530
 * @freq:	new device frequency /Users/rubber/linux/kernel/time/clockevents.c: 531
 * Reconfigure and reprogram a clock event device in oneshot /Users/rubber/linux/kernel/time/clockevents.c: 533
 * mode. Must be called on the cpu for which the device delivers per /Users/rubber/linux/kernel/time/clockevents.c: 534
 * cpu timer events. If called for the broadcast device the core takes /Users/rubber/linux/kernel/time/clockevents.c: 535
 * care of serialization. /Users/rubber/linux/kernel/time/clockevents.c: 536
 * Returns 0 on success, -ETIME when the event is in the past. /Users/rubber/linux/kernel/time/clockevents.c: 538
 * Noop handler when we shut down an event device /Users/rubber/linux/kernel/time/clockevents.c: 554
 * clockevents_exchange_device - release and request clock devices /Users/rubber/linux/kernel/time/clockevents.c: 561
 * @old:	device to release (can be NULL) /Users/rubber/linux/kernel/time/clockevents.c: 562
 * @new:	device to request (can be NULL) /Users/rubber/linux/kernel/time/clockevents.c: 563
 * Called from various tick functions with clockevents_lock held and /Users/rubber/linux/kernel/time/clockevents.c: 565
 * interrupts disabled. /Users/rubber/linux/kernel/time/clockevents.c: 566
	/* /Users/rubber/linux/kernel/time/clockevents.c: 571
	 * Caller releases a clock event device. We queue it into the /Users/rubber/linux/kernel/time/clockevents.c: 572
	 * released list and do a notify add later. /Users/rubber/linux/kernel/time/clockevents.c: 573
 * clockevents_suspend - suspend clock devices /Users/rubber/linux/kernel/time/clockevents.c: 588
 * clockevents_resume - resume clock devices /Users/rubber/linux/kernel/time/clockevents.c: 600
 * tick_offline_cpu - Take CPU out of the broadcast mechanism /Users/rubber/linux/kernel/time/clockevents.c: 615
 * @cpu:	The outgoing CPU /Users/rubber/linux/kernel/time/clockevents.c: 616
 * Called on the outgoing CPU after it took itself offline. /Users/rubber/linux/kernel/time/clockevents.c: 618
 * tick_cleanup_dead_cpu - Cleanup the tick and clockevents of a dead cpu /Users/rubber/linux/kernel/time/clockevents.c: 629
 * @cpu:	The dead CPU /Users/rubber/linux/kernel/time/clockevents.c: 630
	/* /Users/rubber/linux/kernel/time/clockevents.c: 640
	 * Unregister the clock event devices which were /Users/rubber/linux/kernel/time/clockevents.c: 641
	 * released from the users in the notify chain. /Users/rubber/linux/kernel/time/clockevents.c: 642
	/* /Users/rubber/linux/kernel/time/clockevents.c: 646
	 * Now check whether the CPU has left unused per cpu devices /Users/rubber/linux/kernel/time/clockevents.c: 647
	/* /Users/rubber/linux/kernel/time/clockevents.c: 708
	 * We hold clockevents_mutex, so ce can't go away /Users/rubber/linux/kernel/time/clockevents.c: 709
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/posix-timers.c: 1
 * 2002-10-15  Posix Clocks & timers /Users/rubber/linux/kernel/time/posix-timers.c: 3
 *                           by George Anzinger george@mvista.com /Users/rubber/linux/kernel/time/posix-timers.c: 4
 *			     Copyright (C) 2002 2003 by MontaVista Software. /Users/rubber/linux/kernel/time/posix-timers.c: 5
 * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug. /Users/rubber/linux/kernel/time/posix-timers.c: 7
 *			     Copyright (C) 2004 Boris Hu /Users/rubber/linux/kernel/time/posix-timers.c: 8
 * These are all the functions necessary to implement POSIX clocks & timers /Users/rubber/linux/kernel/time/posix-timers.c: 10
 * Management arrays for POSIX timers. Timers are now kept in static hash table /Users/rubber/linux/kernel/time/posix-timers.c: 39
 * with 512 entries. /Users/rubber/linux/kernel/time/posix-timers.c: 40
 * Timer ids are allocated by local routine, which selects proper hash head by /Users/rubber/linux/kernel/time/posix-timers.c: 41
 * key, constructed from current->signal address and per signal struct counter. /Users/rubber/linux/kernel/time/posix-timers.c: 42
 * This keeps timer ids unique per process, but now they can intersect between /Users/rubber/linux/kernel/time/posix-timers.c: 43
 * processes. /Users/rubber/linux/kernel/time/posix-timers.c: 44
 * Lets keep our timers in a slab cache :-) /Users/rubber/linux/kernel/time/posix-timers.c: 48
 * we assume that the new SIGEV_THREAD_ID shares no bits with the other /Users/rubber/linux/kernel/time/posix-timers.c: 60
 * SIGEV values.  Here we put out an error if this assumption fails. /Users/rubber/linux/kernel/time/posix-timers.c: 61
 * The timer ID is turned into a timer address by idr_find(). /Users/rubber/linux/kernel/time/posix-timers.c: 69
 * Verifying a valid ID consists of: /Users/rubber/linux/kernel/time/posix-timers.c: 70
 * a) checking that idr_find() returns other than -1. /Users/rubber/linux/kernel/time/posix-timers.c: 72
 * b) checking that the timer id matches the one in the timer itself. /Users/rubber/linux/kernel/time/posix-timers.c: 73
 * c) that the timer owner is in the callers thread group. /Users/rubber/linux/kernel/time/posix-timers.c: 74
 * CLOCKs: The POSIX standard calls for a couple of clocks and allows us /Users/rubber/linux/kernel/time/posix-timers.c: 78
 *	    to implement others.  This structure defines the various /Users/rubber/linux/kernel/time/posix-timers.c: 79
 *	    clocks. /Users/rubber/linux/kernel/time/posix-timers.c: 80
 * RESOLUTION: Clock resolution is used to round up timer and interval /Users/rubber/linux/kernel/time/posix-timers.c: 82
 *	    times, NOT to report clock times, which are reported with as /Users/rubber/linux/kernel/time/posix-timers.c: 83
 *	    much resolution as the system can muster.  In some cases this /Users/rubber/linux/kernel/time/posix-timers.c: 84
 *	    resolution may depend on the underlying clock hardware and /Users/rubber/linux/kernel/time/posix-timers.c: 85
 *	    may not be quantifiable until run time, and only then is the /Users/rubber/linux/kernel/time/posix-timers.c: 86
 *	    necessary code is written.	The standard says we should say /Users/rubber/linux/kernel/time/posix-timers.c: 87
 *	    something about this issue in the documentation... /Users/rubber/linux/kernel/time/posix-timers.c: 88
 * FUNCTIONS: The CLOCKs structure defines possible functions to /Users/rubber/linux/kernel/time/posix-timers.c: 90
 *	    handle various clock functions. /Users/rubber/linux/kernel/time/posix-timers.c: 91
 *	    The standard POSIX timer management code assumes the /Users/rubber/linux/kernel/time/posix-timers.c: 93
 *	    following: 1.) The k_itimer struct (sched.h) is used for /Users/rubber/linux/kernel/time/posix-timers.c: 94
 *	    the timer.  2.) The list, it_lock, it_clock, it_id and /Users/rubber/linux/kernel/time/posix-timers.c: 95
 *	    it_pid fields are not modified by timer code. /Users/rubber/linux/kernel/time/posix-timers.c: 96
 * Permissions: It is assumed that the clock_settime() function defined /Users/rubber/linux/kernel/time/posix-timers.c: 98
 *	    for each clock will take care of permission checks.	 Some /Users/rubber/linux/kernel/time/posix-timers.c: 99
 *	    clocks may be set able by any user (i.e. local process /Users/rubber/linux/kernel/time/posix-timers.c: 100
 *	    clocks) others not.	 Currently the only set able clock we /Users/rubber/linux/kernel/time/posix-timers.c: 101
 *	    have is CLOCK_REALTIME and its high res counter part, both of /Users/rubber/linux/kernel/time/posix-timers.c: 102
 *	    which we beg off on and pass to do_sys_settimeofday(). /Users/rubber/linux/kernel/time/posix-timers.c: 103
 * Get monotonic time for posix timers /Users/rubber/linux/kernel/time/posix-timers.c: 195
 * Get monotonic-raw time for posix timers /Users/rubber/linux/kernel/time/posix-timers.c: 210
 * Initialize everything, well, just everything in Posix clocks/timers ;) /Users/rubber/linux/kernel/time/posix-timers.c: 271
 * The siginfo si_overrun field and the return value of timer_getoverrun(2) /Users/rubber/linux/kernel/time/posix-timers.c: 283
 * are of type int. Clamp the overrun value to INT_MAX /Users/rubber/linux/kernel/time/posix-timers.c: 284
 * This function is exported for use by the signal deliver code.  It is /Users/rubber/linux/kernel/time/posix-timers.c: 303
 * called just prior to the info block being released and passes that /Users/rubber/linux/kernel/time/posix-timers.c: 304
 * block to us.  It's function is to update the overrun entry AND to /Users/rubber/linux/kernel/time/posix-timers.c: 305
 * restart the timer.  It should only be called if the timer is to be /Users/rubber/linux/kernel/time/posix-timers.c: 306
 * restarted (i.e. we have flagged this in the sys_private entry of the /Users/rubber/linux/kernel/time/posix-timers.c: 307
 * info block). /Users/rubber/linux/kernel/time/posix-timers.c: 308
 * To protect against the timer going away while the interrupt is queued, /Users/rubber/linux/kernel/time/posix-timers.c: 310
 * we require that the it_requeue_pending flag be set. /Users/rubber/linux/kernel/time/posix-timers.c: 311
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 340
	 * FIXME: if ->sigq is queued we can race with /Users/rubber/linux/kernel/time/posix-timers.c: 341
	 * dequeue_signal()->posixtimer_rearm(). /Users/rubber/linux/kernel/time/posix-timers.c: 342
	 * /Users/rubber/linux/kernel/time/posix-timers.c: 343
	 * If dequeue_signal() sees the "right" value of /Users/rubber/linux/kernel/time/posix-timers.c: 344
	 * si_sys_private it calls posixtimer_rearm(). /Users/rubber/linux/kernel/time/posix-timers.c: 345
	 * We re-queue ->sigq and drop ->it_lock(). /Users/rubber/linux/kernel/time/posix-timers.c: 346
	 * posixtimer_rearm() locks the timer /Users/rubber/linux/kernel/time/posix-timers.c: 347
	 * and re-schedules it while ->sigq is pending. /Users/rubber/linux/kernel/time/posix-timers.c: 348
	 * Not really bad, but not that we want. /Users/rubber/linux/kernel/time/posix-timers.c: 349
 * This function gets called when a POSIX.1b interval timer expires.  It /Users/rubber/linux/kernel/time/posix-timers.c: 360
 * is used as a callback from the kernel internal timer.  The /Users/rubber/linux/kernel/time/posix-timers.c: 361
 * run_timer_list code ALWAYS calls with interrupts on. /Users/rubber/linux/kernel/time/posix-timers.c: 362
 * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers. /Users/rubber/linux/kernel/time/posix-timers.c: 364
		/* /Users/rubber/linux/kernel/time/posix-timers.c: 381
		 * signal was not sent because of sig_ignor /Users/rubber/linux/kernel/time/posix-timers.c: 382
		 * we will not get a call back to restart it AND /Users/rubber/linux/kernel/time/posix-timers.c: 383
		 * it should be restarted. /Users/rubber/linux/kernel/time/posix-timers.c: 384
			/* /Users/rubber/linux/kernel/time/posix-timers.c: 389
			 * FIXME: What we really want, is to stop this /Users/rubber/linux/kernel/time/posix-timers.c: 390
			 * timer completely and restart it in case the /Users/rubber/linux/kernel/time/posix-timers.c: 391
			 * SIG_IGN is removed. This is a non trivial /Users/rubber/linux/kernel/time/posix-timers.c: 392
			 * change which involves sighand locking /Users/rubber/linux/kernel/time/posix-timers.c: 393
			 * (sigh !), which we don't want to do late in /Users/rubber/linux/kernel/time/posix-timers.c: 394
			 * the release cycle. /Users/rubber/linux/kernel/time/posix-timers.c: 395
			 * /Users/rubber/linux/kernel/time/posix-timers.c: 396
			 * For now we just let timers with an interval /Users/rubber/linux/kernel/time/posix-timers.c: 397
			 * less than a jiffie expire every jiffie to /Users/rubber/linux/kernel/time/posix-timers.c: 398
			 * avoid softirq starvation in case of SIG_IGN /Users/rubber/linux/kernel/time/posix-timers.c: 399
			 * and a very small interval, which would put /Users/rubber/linux/kernel/time/posix-timers.c: 400
			 * the timer right back on the softirq pending /Users/rubber/linux/kernel/time/posix-timers.c: 401
			 * list. By moving now ahead of time we trick /Users/rubber/linux/kernel/time/posix-timers.c: 402
			 * hrtimer_forward() to expire the timer /Users/rubber/linux/kernel/time/posix-timers.c: 403
			 * later, while we still maintain the overrun /Users/rubber/linux/kernel/time/posix-timers.c: 404
			 * accuracy, but have some inconsistency in /Users/rubber/linux/kernel/time/posix-timers.c: 405
			 * the timer_gettime() case. This is at least /Users/rubber/linux/kernel/time/posix-timers.c: 406
			 * better than a starved softirq. A more /Users/rubber/linux/kernel/time/posix-timers.c: 407
			 * complex fix which solves also another related /Users/rubber/linux/kernel/time/posix-timers.c: 408
			 * inconsistency is already in the pipeline. /Users/rubber/linux/kernel/time/posix-timers.c: 409
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 566
	 * In the case of the timer belonging to another task, after /Users/rubber/linux/kernel/time/posix-timers.c: 567
	 * the task is unlocked, the timer is owned by the other task /Users/rubber/linux/kernel/time/posix-timers.c: 568
	 * and may cease to exist at any time.  Don't use or modify /Users/rubber/linux/kernel/time/posix-timers.c: 569
	 * new_timer after the unlock call. /Users/rubber/linux/kernel/time/posix-timers.c: 570
 * Locking issues: We need to protect the result of the id look up until /Users/rubber/linux/kernel/time/posix-timers.c: 608
 * we get the timer locked down so it is not deleted under us.  The /Users/rubber/linux/kernel/time/posix-timers.c: 609
 * removal is done under the idr spinlock so we use that here to bridge /Users/rubber/linux/kernel/time/posix-timers.c: 610
 * the find to the timer lock.  To avoid a dead lock, the timer id MUST /Users/rubber/linux/kernel/time/posix-timers.c: 611
 * be release with out holding the timer lock. /Users/rubber/linux/kernel/time/posix-timers.c: 612
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 618
	 * timer_t could be any type >= int and we want to make sure any /Users/rubber/linux/kernel/time/posix-timers.c: 619
	 * @timer_id outside positive int range fails lookup. /Users/rubber/linux/kernel/time/posix-timers.c: 620
 * Get the time remaining on a POSIX.1b interval timer.  This function /Users/rubber/linux/kernel/time/posix-timers.c: 655
 * is ALWAYS called with spin_lock_irq on the timer, thus it must not /Users/rubber/linux/kernel/time/posix-timers.c: 656
 * mess with irq. /Users/rubber/linux/kernel/time/posix-timers.c: 657
 * We have a couple of messes to clean up here.  First there is the case /Users/rubber/linux/kernel/time/posix-timers.c: 659
 * of a timer that has a requeue pending.  These timers should appear to /Users/rubber/linux/kernel/time/posix-timers.c: 660
 * be in the timer list with an expiry as if we were to requeue them /Users/rubber/linux/kernel/time/posix-timers.c: 661
 * now. /Users/rubber/linux/kernel/time/posix-timers.c: 662
 * The second issue is the SIGEV_NONE timer which may be active but is /Users/rubber/linux/kernel/time/posix-timers.c: 664
 * not really ever put in the timer list (to save system resources). /Users/rubber/linux/kernel/time/posix-timers.c: 665
 * This timer may be expired, and if so, we will do it here.  Otherwise /Users/rubber/linux/kernel/time/posix-timers.c: 666
 * it is the same as a requeue pending timer WRT to what we should /Users/rubber/linux/kernel/time/posix-timers.c: 667
 * report. /Users/rubber/linux/kernel/time/posix-timers.c: 668
		/* /Users/rubber/linux/kernel/time/posix-timers.c: 683
		 * SIGEV_NONE oneshot timers are never queued. Check them /Users/rubber/linux/kernel/time/posix-timers.c: 684
		 * below. /Users/rubber/linux/kernel/time/posix-timers.c: 685
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 693
	 * When a requeue is pending or this is a SIGEV_NONE timer move the /Users/rubber/linux/kernel/time/posix-timers.c: 694
	 * expiry time forward by intervals, so expiry is > now. /Users/rubber/linux/kernel/time/posix-timers.c: 695
		/* /Users/rubber/linux/kernel/time/posix-timers.c: 703
		 * A single shot SIGEV_NONE timer must return 0, when /Users/rubber/linux/kernel/time/posix-timers.c: 704
		 * it is expired ! /Users/rubber/linux/kernel/time/posix-timers.c: 705
 * Get the number of overruns of a POSIX.1b interval timer.  This is to /Users/rubber/linux/kernel/time/posix-timers.c: 769
 * be the overrun of the timer last delivered.  At the same time we are /Users/rubber/linux/kernel/time/posix-timers.c: 770
 * accumulating overruns on the next timer.  The overrun is frozen when /Users/rubber/linux/kernel/time/posix-timers.c: 771
 * the signal is delivered, either at the notify time (if the info block /Users/rubber/linux/kernel/time/posix-timers.c: 772
 * is not queued) or at the actual delivery time (as we are informed by /Users/rubber/linux/kernel/time/posix-timers.c: 773
 * the call back to posixtimer_rearm().  So all we need to do is /Users/rubber/linux/kernel/time/posix-timers.c: 774
 * to pick up the frozen overrun. /Users/rubber/linux/kernel/time/posix-timers.c: 775
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 800
	 * Posix magic: Relative CLOCK_REALTIME timers are not affected by /Users/rubber/linux/kernel/time/posix-timers.c: 801
	 * clock modifications, so they become CLOCK_MONOTONIC based under the /Users/rubber/linux/kernel/time/posix-timers.c: 802
	 * hood. See hrtimer_init(). Update timr->kclock, so the generic /Users/rubber/linux/kernel/time/posix-timers.c: 803
	 * functions which use timr->kclock->clock_get_*() work. /Users/rubber/linux/kernel/time/posix-timers.c: 804
	 * /Users/rubber/linux/kernel/time/posix-timers.c: 805
	 * Note: it_clock stays unmodified, because the next timer_set() might /Users/rubber/linux/kernel/time/posix-timers.c: 806
	 * use ABSTIME, so it needs to switch back. /Users/rubber/linux/kernel/time/posix-timers.c: 807
 * On PREEMPT_RT this prevent priority inversion against softirq kthread in /Users/rubber/linux/kernel/time/posix-timers.c: 834
 * case it gets preempted while executing a timer callback. See comments in /Users/rubber/linux/kernel/time/posix-timers.c: 835
 * hrtimer_cancel_wait_running. For PREEMPT_RT=n this just results in a /Users/rubber/linux/kernel/time/posix-timers.c: 836
 * cpu_relax(). /Users/rubber/linux/kernel/time/posix-timers.c: 837
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 871
	 * Careful here. On SMP systems the timer expiry function could be /Users/rubber/linux/kernel/time/posix-timers.c: 872
	 * active and spinning on timr->it_lock. /Users/rubber/linux/kernel/time/posix-timers.c: 873
 We already got the old time... /Users/rubber/linux/kernel/time/posix-timers.c: 926
	/* /Users/rubber/linux/kernel/time/posix-timers.c: 1024
	 * This keeps any tasks waiting on the spin lock from thinking /Users/rubber/linux/kernel/time/posix-timers.c: 1025
	 * they got something (see the lock code above). /Users/rubber/linux/kernel/time/posix-timers.c: 1026
 * return timer owned by the process, used by exit_itimers /Users/rubber/linux/kernel/time/posix-timers.c: 1036
 * This is called by do_exit or de_thread, only when there are no more /Users/rubber/linux/kernel/time/posix-timers.c: 1054
 * references to the shared signal_struct. /Users/rubber/linux/kernel/time/posix-timers.c: 1055
 * nanosleep for monotonic and realtime clocks /Users/rubber/linux/kernel/time/posix-timers.c: 1220
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-sched.c: 1
 *  Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/tick-sched.c: 3
 *  Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/tick-sched.c: 4
 *  Copyright(C) 2006-2007  Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/tick-sched.c: 5
 *  No idle tick implementation for low and high resolution timers /Users/rubber/linux/kernel/time/tick-sched.c: 7
 *  Started by: Thomas Gleixner and Ingo Molnar /Users/rubber/linux/kernel/time/tick-sched.c: 9
 * Per-CPU nohz control structure /Users/rubber/linux/kernel/time/tick-sched.c: 37
 * The time, when the last jiffy update happened. Write access must hold /Users/rubber/linux/kernel/time/tick-sched.c: 48
 * jiffies_lock and jiffies_seq. tick_nohz_next_event() needs to get a /Users/rubber/linux/kernel/time/tick-sched.c: 49
 * consistent view of jiffies and last_jiffies_update. /Users/rubber/linux/kernel/time/tick-sched.c: 50
 * Must be called with interrupts disabled ! /Users/rubber/linux/kernel/time/tick-sched.c: 55
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 62
	 * 64bit can do a quick check without holding jiffies lock and /Users/rubber/linux/kernel/time/tick-sched.c: 63
	 * without looking at the sequence count. The smp_load_acquire() /Users/rubber/linux/kernel/time/tick-sched.c: 64
	 * pairs with the update done later in this function. /Users/rubber/linux/kernel/time/tick-sched.c: 65
	 * /Users/rubber/linux/kernel/time/tick-sched.c: 66
	 * 32bit cannot do that because the store of tick_next_period /Users/rubber/linux/kernel/time/tick-sched.c: 67
	 * consists of two 32bit stores and the first store could move it /Users/rubber/linux/kernel/time/tick-sched.c: 68
	 * to a random point in the future. /Users/rubber/linux/kernel/time/tick-sched.c: 69
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 77
		 * Avoid contention on jiffies_lock and protect the quick /Users/rubber/linux/kernel/time/tick-sched.c: 78
		 * check with the sequence count. /Users/rubber/linux/kernel/time/tick-sched.c: 79
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 92
	 * Reevaluate with the lock held. Another CPU might have done the /Users/rubber/linux/kernel/time/tick-sched.c: 93
	 * update already. /Users/rubber/linux/kernel/time/tick-sched.c: 94
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 120
	 * Keep the tick_next_period variable up to date. /Users/rubber/linux/kernel/time/tick-sched.c: 121
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 126
		 * Pairs with smp_load_acquire() in the lockless quick /Users/rubber/linux/kernel/time/tick-sched.c: 127
		 * check above and ensures that the update to jiffies_64 is /Users/rubber/linux/kernel/time/tick-sched.c: 128
		 * not reordered vs. the store to tick_next_period, neither /Users/rubber/linux/kernel/time/tick-sched.c: 129
		 * by the compiler nor by the CPU. /Users/rubber/linux/kernel/time/tick-sched.c: 130
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 134
		 * A plain store is good enough on 32bit as the quick check /Users/rubber/linux/kernel/time/tick-sched.c: 135
		 * above is protected by the sequence count. /Users/rubber/linux/kernel/time/tick-sched.c: 136
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 141
	 * Release the sequence count. calc_global_load() below is not /Users/rubber/linux/kernel/time/tick-sched.c: 142
	 * protected by it, but jiffies_lock needs to be held to prevent /Users/rubber/linux/kernel/time/tick-sched.c: 143
	 * concurrent invocations. /Users/rubber/linux/kernel/time/tick-sched.c: 144
 * Initialize and return retrieve the jiffies update. /Users/rubber/linux/kernel/time/tick-sched.c: 155
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 177
	 * Check if the do_timer duty was dropped. We don't care about /Users/rubber/linux/kernel/time/tick-sched.c: 178
	 * concurrency: This happens only when the CPU in charge went /Users/rubber/linux/kernel/time/tick-sched.c: 179
	 * into a long sleep. If two CPUs happen to assign themselves to /Users/rubber/linux/kernel/time/tick-sched.c: 180
	 * this duty, then the jiffies update is still serialized by /Users/rubber/linux/kernel/time/tick-sched.c: 181
	 * jiffies_lock. /Users/rubber/linux/kernel/time/tick-sched.c: 182
	 * /Users/rubber/linux/kernel/time/tick-sched.c: 183
	 * If nohz_full is enabled, this should not happen because the /Users/rubber/linux/kernel/time/tick-sched.c: 184
	 * tick_do_timer_cpu never relinquishes. /Users/rubber/linux/kernel/time/tick-sched.c: 185
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 206
	 * When we are idle and the tick is stopped, we have to touch /Users/rubber/linux/kernel/time/tick-sched.c: 207
	 * the watchdog as we might not schedule for a really long /Users/rubber/linux/kernel/time/tick-sched.c: 208
	 * time. This happens on complete idle SMP systems while /Users/rubber/linux/kernel/time/tick-sched.c: 209
	 * waiting on the login prompt. We also increment the "start of /Users/rubber/linux/kernel/time/tick-sched.c: 210
	 * idle" jiffy stamp so the idle accounting adjustment we do /Users/rubber/linux/kernel/time/tick-sched.c: 211
	 * when we go busy again does not account too much ticks. /Users/rubber/linux/kernel/time/tick-sched.c: 212
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 218
		 * In case the current tick fired too early past its expected /Users/rubber/linux/kernel/time/tick-sched.c: 219
		 * expiration, make sure we don't bypass the next clock reprogramming /Users/rubber/linux/kernel/time/tick-sched.c: 220
		 * to the same deadline. /Users/rubber/linux/kernel/time/tick-sched.c: 221
 * Kick this CPU if it's full dynticks in order to force it to /Users/rubber/linux/kernel/time/tick-sched.c: 301
 * re-evaluate its dependency on the tick and restart it if necessary. /Users/rubber/linux/kernel/time/tick-sched.c: 302
 * This kick, unlike tick_nohz_full_kick_cpu() and tick_nohz_full_kick_all(), /Users/rubber/linux/kernel/time/tick-sched.c: 303
 * is NMI safe. /Users/rubber/linux/kernel/time/tick-sched.c: 304
 * Kick the CPU if it's full dynticks in order to force it to /Users/rubber/linux/kernel/time/tick-sched.c: 315
 * re-evaluate its dependency on the tick and restart it if necessary. /Users/rubber/linux/kernel/time/tick-sched.c: 316
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 330
	 * If the task is not running, run_posix_cpu_timers() /Users/rubber/linux/kernel/time/tick-sched.c: 331
	 * has nothing to elapse, IPI can then be spared. /Users/rubber/linux/kernel/time/tick-sched.c: 332
	 * /Users/rubber/linux/kernel/time/tick-sched.c: 333
	 * activate_task()                      STORE p->tick_dep_mask /Users/rubber/linux/kernel/time/tick-sched.c: 334
	 *   STORE p->on_rq /Users/rubber/linux/kernel/time/tick-sched.c: 335
	 * __schedule() (switch to task 'p')    smp_mb() (atomic_fetch_or()) /Users/rubber/linux/kernel/time/tick-sched.c: 336
	 *   LOCK rq->lock                      LOAD p->on_rq /Users/rubber/linux/kernel/time/tick-sched.c: 337
	 *   smp_mb__after_spin_lock() /Users/rubber/linux/kernel/time/tick-sched.c: 338
	 *   tick_nohz_task_switch() /Users/rubber/linux/kernel/time/tick-sched.c: 339
	 *     LOAD p->tick_dep_mask /Users/rubber/linux/kernel/time/tick-sched.c: 340
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 345
	 * If the task concurrently migrates to another CPU, /Users/rubber/linux/kernel/time/tick-sched.c: 346
	 * we guarantee it sees the new tick dependency upon /Users/rubber/linux/kernel/time/tick-sched.c: 347
	 * schedule. /Users/rubber/linux/kernel/time/tick-sched.c: 348
	 * /Users/rubber/linux/kernel/time/tick-sched.c: 349
	 * set_task_cpu(p, cpu); /Users/rubber/linux/kernel/time/tick-sched.c: 350
	 *   STORE p->cpu = @cpu /Users/rubber/linux/kernel/time/tick-sched.c: 351
	 * __schedule() (switch to task 'p') /Users/rubber/linux/kernel/time/tick-sched.c: 352
	 *   LOCK rq->lock /Users/rubber/linux/kernel/time/tick-sched.c: 353
	 *   smp_mb__after_spin_lock()          STORE p->tick_dep_mask /Users/rubber/linux/kernel/time/tick-sched.c: 354
	 *   tick_nohz_task_switch()            smp_mb() (atomic_fetch_or()) /Users/rubber/linux/kernel/time/tick-sched.c: 355
	 *      LOAD p->tick_dep_mask           LOAD p->cpu /Users/rubber/linux/kernel/time/tick-sched.c: 356
 * Kick all full dynticks CPUs in order to force these to re-evaluate /Users/rubber/linux/kernel/time/tick-sched.c: 367
 * their dependency on the tick and restart it if necessary. /Users/rubber/linux/kernel/time/tick-sched.c: 368
 * Set a global tick dependency. Used by perf events that rely on freq and /Users/rubber/linux/kernel/time/tick-sched.c: 394
 * by unstable clock. /Users/rubber/linux/kernel/time/tick-sched.c: 395
 * Set per-CPU tick dependency. Used by scheduler and perf events in order to /Users/rubber/linux/kernel/time/tick-sched.c: 408
 * manage events throttling. /Users/rubber/linux/kernel/time/tick-sched.c: 409
 * Set a per-task tick dependency. RCU need this. Also posix CPU timers /Users/rubber/linux/kernel/time/tick-sched.c: 443
 * in order to elapse per task timers. /Users/rubber/linux/kernel/time/tick-sched.c: 444
 * Set a per-taskgroup tick dependency. Posix CPU timers need this in order to elapse /Users/rubber/linux/kernel/time/tick-sched.c: 460
 * per process timers. /Users/rubber/linux/kernel/time/tick-sched.c: 461
 * Re-evaluate the need for the tick as we switch the current task. /Users/rubber/linux/kernel/time/tick-sched.c: 485
 * It might need the tick due to per task/process properties: /Users/rubber/linux/kernel/time/tick-sched.c: 486
 * perf events, posix CPU timers, ... /Users/rubber/linux/kernel/time/tick-sched.c: 487
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 516
	 * The tick_do_timer_cpu CPU handles housekeeping duty (unbound /Users/rubber/linux/kernel/time/tick-sched.c: 517
	 * timers, workqueues, timekeeping, ...) on behalf of full dynticks /Users/rubber/linux/kernel/time/tick-sched.c: 518
	 * CPUs. It must remain online when nohz full is enabled. /Users/rubber/linux/kernel/time/tick-sched.c: 519
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 533
	 * Full dynticks uses irq work to drive the tick rescheduling on safe /Users/rubber/linux/kernel/time/tick-sched.c: 534
	 * locking contexts. But then we need irq work to raise its own /Users/rubber/linux/kernel/time/tick-sched.c: 535
	 * interrupts to avoid circular dependency on the tick /Users/rubber/linux/kernel/time/tick-sched.c: 536
 * NOHZ - aka dynamic tick functionality /Users/rubber/linux/kernel/time/tick-sched.c: 569
 * NO HZ enabled ? /Users/rubber/linux/kernel/time/tick-sched.c: 573
 * Enable / Disable tickless mode /Users/rubber/linux/kernel/time/tick-sched.c: 578
 * tick_nohz_update_jiffies - update jiffies when idle was interrupted /Users/rubber/linux/kernel/time/tick-sched.c: 602
 * Called from interrupt entry when the CPU was idle /Users/rubber/linux/kernel/time/tick-sched.c: 604
 * In case the sched_tick was stopped on this CPU, we have to check if jiffies /Users/rubber/linux/kernel/time/tick-sched.c: 606
 * must be updated. Otherwise an interrupt handler could use a stale jiffy /Users/rubber/linux/kernel/time/tick-sched.c: 607
 * value. We do this unconditionally on any CPU, as we don't know whether the /Users/rubber/linux/kernel/time/tick-sched.c: 608
 * CPU, which has the update task assigned is in a long sleep. /Users/rubber/linux/kernel/time/tick-sched.c: 609
 * Updates the per-CPU time idle statistics counters /Users/rubber/linux/kernel/time/tick-sched.c: 625
 * get_cpu_idle_time_us - get the total idle time of a CPU /Users/rubber/linux/kernel/time/tick-sched.c: 662
 * @cpu: CPU number to query /Users/rubber/linux/kernel/time/tick-sched.c: 663
 * @last_update_time: variable to store update time in. Do not update /Users/rubber/linux/kernel/time/tick-sched.c: 664
 * counters if NULL. /Users/rubber/linux/kernel/time/tick-sched.c: 665
 * Return the cumulative idle time (since boot) for a given /Users/rubber/linux/kernel/time/tick-sched.c: 667
 * CPU, in microseconds. /Users/rubber/linux/kernel/time/tick-sched.c: 668
 * This time is measured via accounting rather than sampling, /Users/rubber/linux/kernel/time/tick-sched.c: 670
 * and is as accurate as ktime_get() is. /Users/rubber/linux/kernel/time/tick-sched.c: 671
 * This function returns -1 if NOHZ is not enabled. /Users/rubber/linux/kernel/time/tick-sched.c: 673
 * get_cpu_iowait_time_us - get the total iowait time of a CPU /Users/rubber/linux/kernel/time/tick-sched.c: 703
 * @cpu: CPU number to query /Users/rubber/linux/kernel/time/tick-sched.c: 704
 * @last_update_time: variable to store update time in. Do not update /Users/rubber/linux/kernel/time/tick-sched.c: 705
 * counters if NULL. /Users/rubber/linux/kernel/time/tick-sched.c: 706
 * Return the cumulative iowait time (since boot) for a given /Users/rubber/linux/kernel/time/tick-sched.c: 708
 * CPU, in microseconds. /Users/rubber/linux/kernel/time/tick-sched.c: 709
 * This time is measured via accounting rather than sampling, /Users/rubber/linux/kernel/time/tick-sched.c: 711
 * and is as accurate as ktime_get() is. /Users/rubber/linux/kernel/time/tick-sched.c: 712
 * This function returns -1 if NOHZ is not enabled. /Users/rubber/linux/kernel/time/tick-sched.c: 714
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 757
	 * Reset to make sure next tick stop doesn't get fooled by past /Users/rubber/linux/kernel/time/tick-sched.c: 758
	 * cached clock deadline. /Users/rubber/linux/kernel/time/tick-sched.c: 759
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 784
	 * Keep the periodic tick, when RCU, architecture or irq_work /Users/rubber/linux/kernel/time/tick-sched.c: 785
	 * requests it. /Users/rubber/linux/kernel/time/tick-sched.c: 786
	 * Aside of that check whether the local timer softirq is /Users/rubber/linux/kernel/time/tick-sched.c: 787
	 * pending. If so its a bad idea to call get_next_timer_interrupt() /Users/rubber/linux/kernel/time/tick-sched.c: 788
	 * because there is an already expired timer, so it will request /Users/rubber/linux/kernel/time/tick-sched.c: 789
	 * immediate expiry, which rearms the hardware timer with a /Users/rubber/linux/kernel/time/tick-sched.c: 790
	 * minimal delta which brings us back to this place /Users/rubber/linux/kernel/time/tick-sched.c: 791
	 * immediately. Lather, rinse and repeat... /Users/rubber/linux/kernel/time/tick-sched.c: 792
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 798
		 * Get the next pending timer. If high resolution /Users/rubber/linux/kernel/time/tick-sched.c: 799
		 * timers are enabled this only takes the timer wheel /Users/rubber/linux/kernel/time/tick-sched.c: 800
		 * timers into account. If high resolution timers are /Users/rubber/linux/kernel/time/tick-sched.c: 801
		 * disabled this also looks at the next expiring /Users/rubber/linux/kernel/time/tick-sched.c: 802
		 * hrtimer. /Users/rubber/linux/kernel/time/tick-sched.c: 803
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 811
	 * If the tick is due in the next period, keep it ticking or /Users/rubber/linux/kernel/time/tick-sched.c: 812
	 * force prod the timer. /Users/rubber/linux/kernel/time/tick-sched.c: 813
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 817
		 * Tell the timer code that the base is not idle, i.e. undo /Users/rubber/linux/kernel/time/tick-sched.c: 818
		 * the effect of get_next_timer_interrupt(): /Users/rubber/linux/kernel/time/tick-sched.c: 819
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 822
		 * We've not stopped the tick yet, and there's a timer in the /Users/rubber/linux/kernel/time/tick-sched.c: 823
		 * next period, so no point in stopping it either, bail. /Users/rubber/linux/kernel/time/tick-sched.c: 824
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 832
	 * If this CPU is the one which had the do_timer() duty last, we limit /Users/rubber/linux/kernel/time/tick-sched.c: 833
	 * the sleep time to the timekeeping max_deferment value. /Users/rubber/linux/kernel/time/tick-sched.c: 834
	 * Otherwise we can sleep as long as we want. /Users/rubber/linux/kernel/time/tick-sched.c: 835
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 864
	 * If this CPU is the one which updates jiffies, then give up /Users/rubber/linux/kernel/time/tick-sched.c: 865
	 * the assignment and let it be taken by the CPU which runs /Users/rubber/linux/kernel/time/tick-sched.c: 866
	 * the tick timer next, which might be this CPU as well. If we /Users/rubber/linux/kernel/time/tick-sched.c: 867
	 * don't drop this here the jiffies might be stale and /Users/rubber/linux/kernel/time/tick-sched.c: 868
	 * do_timer() never invoked. Keep track of the fact that it /Users/rubber/linux/kernel/time/tick-sched.c: 869
	 * was the one which had the do_timer() duty last. /Users/rubber/linux/kernel/time/tick-sched.c: 870
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 891
	 * nohz_stop_sched_tick can be called several times before /Users/rubber/linux/kernel/time/tick-sched.c: 892
	 * the nohz_restart_sched_tick is called. This happens when /Users/rubber/linux/kernel/time/tick-sched.c: 893
	 * interrupts arrive which do not cause a reschedule. In the /Users/rubber/linux/kernel/time/tick-sched.c: 894
	 * first call we save the current tick time, so we can restart /Users/rubber/linux/kernel/time/tick-sched.c: 895
	 * the scheduler tick in nohz_restart_sched_tick. /Users/rubber/linux/kernel/time/tick-sched.c: 896
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 909
	 * If the expiration time == KTIME_MAX, then we simply stop /Users/rubber/linux/kernel/time/tick-sched.c: 910
	 * the tick timer. /Users/rubber/linux/kernel/time/tick-sched.c: 911
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 948
	 * Clear the timer idle flag, so we avoid IPIs on remote queueing and /Users/rubber/linux/kernel/time/tick-sched.c: 949
	 * the clock forward checks in the enqueue path: /Users/rubber/linux/kernel/time/tick-sched.c: 950
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 956
	 * Cancel the scheduled timer and restore the tick /Users/rubber/linux/kernel/time/tick-sched.c: 957
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 989
	 * If this CPU is offline and it is the one which updates /Users/rubber/linux/kernel/time/tick-sched.c: 990
	 * jiffies, then give up the assignment and let it be taken by /Users/rubber/linux/kernel/time/tick-sched.c: 991
	 * the CPU which runs the tick timer next. If we don't drop /Users/rubber/linux/kernel/time/tick-sched.c: 992
	 * this here the jiffies might be stale and do_timer() never /Users/rubber/linux/kernel/time/tick-sched.c: 993
	 * invoked. /Users/rubber/linux/kernel/time/tick-sched.c: 994
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 999
		 * Make sure the CPU doesn't get fooled by obsolete tick /Users/rubber/linux/kernel/time/tick-sched.c: 1000
		 * deadline if it comes back online later. /Users/rubber/linux/kernel/time/tick-sched.c: 1001
		/* /Users/rubber/linux/kernel/time/tick-sched.c: 1026
		 * Keep the tick alive to guarantee timekeeping progression /Users/rubber/linux/kernel/time/tick-sched.c: 1027
		 * if there are full dynticks CPUs around /Users/rubber/linux/kernel/time/tick-sched.c: 1028
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1046
	 * If tick_nohz_get_sleep_length() ran tick_nohz_next_event(), the /Users/rubber/linux/kernel/time/tick-sched.c: 1047
	 * tick timer expiration time is known already. /Users/rubber/linux/kernel/time/tick-sched.c: 1048
 * tick_nohz_idle_stop_tick - stop the idle tick from the idle task /Users/rubber/linux/kernel/time/tick-sched.c: 1077
 * When the next event is more than a tick into the future, stop the idle tick /Users/rubber/linux/kernel/time/tick-sched.c: 1079
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1089
	 * Undo the effect of get_next_timer_interrupt() called from /Users/rubber/linux/kernel/time/tick-sched.c: 1090
	 * tick_nohz_next_event(). /Users/rubber/linux/kernel/time/tick-sched.c: 1091
 * tick_nohz_idle_enter - prepare for entering idle on the current CPU /Users/rubber/linux/kernel/time/tick-sched.c: 1097
 * Called when we start the idle loop. /Users/rubber/linux/kernel/time/tick-sched.c: 1099
 * tick_nohz_irq_exit - update next tick event from interrupt exit /Users/rubber/linux/kernel/time/tick-sched.c: 1120
 * When an interrupt fires while we are idle and it doesn't cause /Users/rubber/linux/kernel/time/tick-sched.c: 1122
 * a reschedule, it may still add, modify or delete a timer, enqueue /Users/rubber/linux/kernel/time/tick-sched.c: 1123
 * an RCU callback, etc... /Users/rubber/linux/kernel/time/tick-sched.c: 1124
 * So we need to re-calculate and reprogram the next tick event. /Users/rubber/linux/kernel/time/tick-sched.c: 1125
 * tick_nohz_idle_got_tick - Check whether or not the tick handler has run /Users/rubber/linux/kernel/time/tick-sched.c: 1138
 * tick_nohz_get_next_hrtimer - return the next expiration time for the hrtimer /Users/rubber/linux/kernel/time/tick-sched.c: 1152
 * or the tick, whatever that expires first. Note that, if the tick has been /Users/rubber/linux/kernel/time/tick-sched.c: 1153
 * stopped, it returns the next hrtimer. /Users/rubber/linux/kernel/time/tick-sched.c: 1154
 * Called from power state control code with interrupts disabled /Users/rubber/linux/kernel/time/tick-sched.c: 1156
 * tick_nohz_get_sleep_length - return the expected length of the current sleep /Users/rubber/linux/kernel/time/tick-sched.c: 1164
 * @delta_next: duration until the next event if the tick cannot be stopped /Users/rubber/linux/kernel/time/tick-sched.c: 1165
 * Called from power state control code with interrupts disabled. /Users/rubber/linux/kernel/time/tick-sched.c: 1167
 * The return value of this function and/or the value returned by it through the /Users/rubber/linux/kernel/time/tick-sched.c: 1169
 * @delta_next pointer can be negative which must be taken into account by its /Users/rubber/linux/kernel/time/tick-sched.c: 1170
 * callers. /Users/rubber/linux/kernel/time/tick-sched.c: 1171
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1178
	 * The idle entry time is expected to be a sufficient approximation of /Users/rubber/linux/kernel/time/tick-sched.c: 1179
	 * the current time at this point. /Users/rubber/linux/kernel/time/tick-sched.c: 1180
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1196
	 * If the next highres timer to expire is earlier than next_event, the /Users/rubber/linux/kernel/time/tick-sched.c: 1197
	 * idle governor needs to know that. /Users/rubber/linux/kernel/time/tick-sched.c: 1198
 * tick_nohz_get_idle_calls_cpu - return the current idle calls counter value /Users/rubber/linux/kernel/time/tick-sched.c: 1207
 * for a particular CPU. /Users/rubber/linux/kernel/time/tick-sched.c: 1208
 * Called from the schedutil frequency scaling governor in scheduler context. /Users/rubber/linux/kernel/time/tick-sched.c: 1210
 * tick_nohz_get_idle_calls - return the current idle calls counter value /Users/rubber/linux/kernel/time/tick-sched.c: 1220
 * Called from the schedutil frequency scaling governor in scheduler context. /Users/rubber/linux/kernel/time/tick-sched.c: 1222
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1240
	 * We stopped the tick in idle. Update process times would miss the /Users/rubber/linux/kernel/time/tick-sched.c: 1241
	 * time we slept as update_process_times does only a 1 tick /Users/rubber/linux/kernel/time/tick-sched.c: 1242
	 * accounting. Enforce that this is accounted to idle ! /Users/rubber/linux/kernel/time/tick-sched.c: 1243
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1246
	 * We might be one off. Do not randomly account a huge number of ticks! /Users/rubber/linux/kernel/time/tick-sched.c: 1247
 * tick_nohz_idle_exit - restart the idle tick from the idle task /Users/rubber/linux/kernel/time/tick-sched.c: 1275
 * Restart the idle tick when the CPU is woken up from idle /Users/rubber/linux/kernel/time/tick-sched.c: 1277
 * This also exit the RCU extended quiescent state. The CPU /Users/rubber/linux/kernel/time/tick-sched.c: 1278
 * can use RCU again after this function is called. /Users/rubber/linux/kernel/time/tick-sched.c: 1279
 * The nohz low res interrupt handler /Users/rubber/linux/kernel/time/tick-sched.c: 1309
 * tick_nohz_switch_to_nohz - switch to nohz mode /Users/rubber/linux/kernel/time/tick-sched.c: 1341
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1354
	 * Recycle the hrtimer in ts, so we can share the /Users/rubber/linux/kernel/time/tick-sched.c: 1355
	 * hrtimer_forward with the highres code. /Users/rubber/linux/kernel/time/tick-sched.c: 1356
 * Called from irq_enter to notify about the possible interruption of idle() /Users/rubber/linux/kernel/time/tick-sched.c: 1391
 * High resolution timer specific code /Users/rubber/linux/kernel/time/tick-sched.c: 1400
 * We rearm the timer until we get disabled by the idle code. /Users/rubber/linux/kernel/time/tick-sched.c: 1404
 * Called with interrupts disabled. /Users/rubber/linux/kernel/time/tick-sched.c: 1405
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1416
	 * Do not call, when we are not in irq context and have /Users/rubber/linux/kernel/time/tick-sched.c: 1417
	 * no valid regs pointer /Users/rubber/linux/kernel/time/tick-sched.c: 1418
 * tick_setup_sched_timer - setup the tick emulation timer /Users/rubber/linux/kernel/time/tick-sched.c: 1445
	/* /Users/rubber/linux/kernel/time/tick-sched.c: 1452
	 * Emulate tick processing via per-CPU hrtimers: /Users/rubber/linux/kernel/time/tick-sched.c: 1453
 * Async notification about clocksource changes /Users/rubber/linux/kernel/time/tick-sched.c: 1490
 * Async notification about clock event changes /Users/rubber/linux/kernel/time/tick-sched.c: 1501
 * Check, if a change happened, which makes oneshot possible. /Users/rubber/linux/kernel/time/tick-sched.c: 1511
 * Called cyclic from the hrtimer softirq (driven by the timer /Users/rubber/linux/kernel/time/tick-sched.c: 1513
 * softirq) allow_nohz signals, that we can switch into low-res nohz /Users/rubber/linux/kernel/time/tick-sched.c: 1514
 * mode, because high resolution timers are disabled (either compile /Users/rubber/linux/kernel/time/tick-sched.c: 1515
 * or runtime). Called with interrupts disabled. /Users/rubber/linux/kernel/time/tick-sched.c: 1516
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/alarmtimer.c: 1
 * Alarmtimer interface /Users/rubber/linux/kernel/time/alarmtimer.c: 3
 * This interface provides a timer which is similar to hrtimers, /Users/rubber/linux/kernel/time/alarmtimer.c: 5
 * but triggers a RTC alarm if the box is suspend. /Users/rubber/linux/kernel/time/alarmtimer.c: 6
 * This interface is influenced by the Android RTC Alarm timer /Users/rubber/linux/kernel/time/alarmtimer.c: 8
 * interface. /Users/rubber/linux/kernel/time/alarmtimer.c: 9
 * Copyright (C) 2010 IBM Corporation /Users/rubber/linux/kernel/time/alarmtimer.c: 11
 * Author: John Stultz <john.stultz@linaro.org> /Users/rubber/linux/kernel/time/alarmtimer.c: 13
 * struct alarm_base - Alarm timer bases /Users/rubber/linux/kernel/time/alarmtimer.c: 37
 * @lock:		Lock for syncrhonized access to the base /Users/rubber/linux/kernel/time/alarmtimer.c: 38
 * @timerqueue:		Timerqueue head managing the list of events /Users/rubber/linux/kernel/time/alarmtimer.c: 39
 * @get_ktime:		Function to read the time correlating to the base /Users/rubber/linux/kernel/time/alarmtimer.c: 40
 * @get_timespec:	Function to read the namespace time correlating to the base /Users/rubber/linux/kernel/time/alarmtimer.c: 41
 * @base_clockid:	clockid for the base /Users/rubber/linux/kernel/time/alarmtimer.c: 42
 * alarmtimer_get_rtcdev - Return selected rtcdevice /Users/rubber/linux/kernel/time/alarmtimer.c: 67
 * This function returns the rtc device to use for wakealarms. /Users/rubber/linux/kernel/time/alarmtimer.c: 69
 * alarmtimer_enqueue - Adds an alarm timer to an alarm_base timerqueue /Users/rubber/linux/kernel/time/alarmtimer.c: 152
 * @base: pointer to the base where the timer is being run /Users/rubber/linux/kernel/time/alarmtimer.c: 153
 * @alarm: pointer to alarm being enqueued. /Users/rubber/linux/kernel/time/alarmtimer.c: 154
 * Adds alarm to a alarm_base timerqueue /Users/rubber/linux/kernel/time/alarmtimer.c: 156
 * Must hold base->lock when calling. /Users/rubber/linux/kernel/time/alarmtimer.c: 158
 * alarmtimer_dequeue - Removes an alarm timer from an alarm_base timerqueue /Users/rubber/linux/kernel/time/alarmtimer.c: 170
 * @base: pointer to the base where the timer is running /Users/rubber/linux/kernel/time/alarmtimer.c: 171
 * @alarm: pointer to alarm being removed /Users/rubber/linux/kernel/time/alarmtimer.c: 172
 * Removes alarm to a alarm_base timerqueue /Users/rubber/linux/kernel/time/alarmtimer.c: 174
 * Must hold base->lock when calling. /Users/rubber/linux/kernel/time/alarmtimer.c: 176
 * alarmtimer_fired - Handles alarm hrtimer being fired. /Users/rubber/linux/kernel/time/alarmtimer.c: 189
 * @timer: pointer to hrtimer being run /Users/rubber/linux/kernel/time/alarmtimer.c: 190
 * When a alarm timer fires, this runs through the timerqueue to /Users/rubber/linux/kernel/time/alarmtimer.c: 192
 * see which alarms expired, and runs those. If there are more alarm /Users/rubber/linux/kernel/time/alarmtimer.c: 193
 * timers queued for the future, we set the hrtimer to fire when /Users/rubber/linux/kernel/time/alarmtimer.c: 194
 * the next future alarm timer expires. /Users/rubber/linux/kernel/time/alarmtimer.c: 195
 * alarmtimer_suspend - Suspend time callback /Users/rubber/linux/kernel/time/alarmtimer.c: 234
 * @dev: unused /Users/rubber/linux/kernel/time/alarmtimer.c: 235
 * When we are going into suspend, we look through the bases /Users/rubber/linux/kernel/time/alarmtimer.c: 237
 * to see which is the soonest timer to expire. We then /Users/rubber/linux/kernel/time/alarmtimer.c: 238
 * set an rtc timer to fire that far into the future, which /Users/rubber/linux/kernel/time/alarmtimer.c: 239
 * will wake us from suspend. /Users/rubber/linux/kernel/time/alarmtimer.c: 240
 * alarm_init - Initialize an alarm structure /Users/rubber/linux/kernel/time/alarmtimer.c: 337
 * @alarm: ptr to alarm to be initialized /Users/rubber/linux/kernel/time/alarmtimer.c: 338
 * @type: the type of the alarm /Users/rubber/linux/kernel/time/alarmtimer.c: 339
 * @function: callback that is run when the alarm fires /Users/rubber/linux/kernel/time/alarmtimer.c: 340
 * alarm_start - Sets an absolute alarm to fire /Users/rubber/linux/kernel/time/alarmtimer.c: 352
 * @alarm: ptr to alarm to set /Users/rubber/linux/kernel/time/alarmtimer.c: 353
 * @start: time to run the alarm /Users/rubber/linux/kernel/time/alarmtimer.c: 354
 * alarm_start_relative - Sets a relative alarm to fire /Users/rubber/linux/kernel/time/alarmtimer.c: 372
 * @alarm: ptr to alarm to set /Users/rubber/linux/kernel/time/alarmtimer.c: 373
 * @start: time relative to now to run the alarm /Users/rubber/linux/kernel/time/alarmtimer.c: 374
 * alarm_try_to_cancel - Tries to cancel an alarm timer /Users/rubber/linux/kernel/time/alarmtimer.c: 399
 * @alarm: ptr to alarm to be canceled /Users/rubber/linux/kernel/time/alarmtimer.c: 400
 * Returns 1 if the timer was canceled, 0 if it was not running, /Users/rubber/linux/kernel/time/alarmtimer.c: 402
 * and -1 if the callback was running /Users/rubber/linux/kernel/time/alarmtimer.c: 403
 * alarm_cancel - Spins trying to cancel an alarm timer until it is done /Users/rubber/linux/kernel/time/alarmtimer.c: 424
 * @alarm: ptr to alarm to be canceled /Users/rubber/linux/kernel/time/alarmtimer.c: 425
 * Returns 1 if the timer was canceled, 0 if it was not active. /Users/rubber/linux/kernel/time/alarmtimer.c: 427
		/* /Users/rubber/linux/kernel/time/alarmtimer.c: 461
		 * This (and the ktime_add() below) is the /Users/rubber/linux/kernel/time/alarmtimer.c: 462
		 * correction for exact: /Users/rubber/linux/kernel/time/alarmtimer.c: 463
 * clock2alarm - helper that converts from clockid to alarmtypes /Users/rubber/linux/kernel/time/alarmtimer.c: 515
 * @clockid: clockid. /Users/rubber/linux/kernel/time/alarmtimer.c: 516
 * alarm_handle_timer - Callback for posix timers /Users/rubber/linux/kernel/time/alarmtimer.c: 528
 * @alarm: alarm that fired /Users/rubber/linux/kernel/time/alarmtimer.c: 529
 * @now: time at the timer expiration /Users/rubber/linux/kernel/time/alarmtimer.c: 530
 * Posix timer callback for expired alarm timers. /Users/rubber/linux/kernel/time/alarmtimer.c: 532
 * Return: whether the timer is to be restarted /Users/rubber/linux/kernel/time/alarmtimer.c: 534
		/* /Users/rubber/linux/kernel/time/alarmtimer.c: 552
		 * Handle ignored signals and rearm the timer. This will go /Users/rubber/linux/kernel/time/alarmtimer.c: 553
		 * away once we handle ignored signals proper. /Users/rubber/linux/kernel/time/alarmtimer.c: 554
 * alarm_timer_rearm - Posix timer callback for rearming timer /Users/rubber/linux/kernel/time/alarmtimer.c: 567
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 568
 * alarm_timer_forward - Posix timer callback for forwarding timer /Users/rubber/linux/kernel/time/alarmtimer.c: 579
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 580
 * @now:	Current time to forward the timer against /Users/rubber/linux/kernel/time/alarmtimer.c: 581
 * alarm_timer_remaining - Posix timer callback to retrieve remaining time /Users/rubber/linux/kernel/time/alarmtimer.c: 591
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 592
 * @now:	Current time to calculate against /Users/rubber/linux/kernel/time/alarmtimer.c: 593
 * alarm_timer_try_to_cancel - Posix timer callback to cancel a timer /Users/rubber/linux/kernel/time/alarmtimer.c: 603
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 604
 * alarm_timer_wait_running - Posix timer callback to wait for a timer /Users/rubber/linux/kernel/time/alarmtimer.c: 612
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 613
 * Called from the core code when timer cancel detected that the callback /Users/rubber/linux/kernel/time/alarmtimer.c: 615
 * is running. @timr is unlocked and rcu read lock is held to prevent it /Users/rubber/linux/kernel/time/alarmtimer.c: 616
 * from being freed. /Users/rubber/linux/kernel/time/alarmtimer.c: 617
 * alarm_timer_arm - Posix timer callback to arm a timer /Users/rubber/linux/kernel/time/alarmtimer.c: 625
 * @timr:	Pointer to the posixtimer data struct /Users/rubber/linux/kernel/time/alarmtimer.c: 626
 * @expires:	The new expiry time /Users/rubber/linux/kernel/time/alarmtimer.c: 627
 * @absolute:	Expiry value is absolute time /Users/rubber/linux/kernel/time/alarmtimer.c: 628
 * @sigev_none:	Posix timer does not deliver signals /Users/rubber/linux/kernel/time/alarmtimer.c: 629
 * alarm_clock_getres - posix getres interface /Users/rubber/linux/kernel/time/alarmtimer.c: 646
 * @which_clock: clockid /Users/rubber/linux/kernel/time/alarmtimer.c: 647
 * @tp: timespec to fill /Users/rubber/linux/kernel/time/alarmtimer.c: 648
 * Returns the granularity of underlying alarm base clock /Users/rubber/linux/kernel/time/alarmtimer.c: 650
 * alarm_clock_get_timespec - posix clock_get_timespec interface /Users/rubber/linux/kernel/time/alarmtimer.c: 663
 * @which_clock: clockid /Users/rubber/linux/kernel/time/alarmtimer.c: 664
 * @tp: timespec to fill. /Users/rubber/linux/kernel/time/alarmtimer.c: 665
 * Provides the underlying alarm base time in a tasks time namespace. /Users/rubber/linux/kernel/time/alarmtimer.c: 667
 * alarm_clock_get_ktime - posix clock_get_ktime interface /Users/rubber/linux/kernel/time/alarmtimer.c: 682
 * @which_clock: clockid /Users/rubber/linux/kernel/time/alarmtimer.c: 683
 * Provides the underlying alarm base time in the root namespace. /Users/rubber/linux/kernel/time/alarmtimer.c: 685
 * alarm_timer_create - posix timer_create interface /Users/rubber/linux/kernel/time/alarmtimer.c: 698
 * @new_timer: k_itimer pointer to manage /Users/rubber/linux/kernel/time/alarmtimer.c: 699
 * Initializes the k_itimer structure. /Users/rubber/linux/kernel/time/alarmtimer.c: 701
 * alarmtimer_nsleep_wakeup - Wakeup function for alarm_timer_nsleep /Users/rubber/linux/kernel/time/alarmtimer.c: 719
 * @alarm: ptr to alarm that fired /Users/rubber/linux/kernel/time/alarmtimer.c: 720
 * @now: time at the timer expiration /Users/rubber/linux/kernel/time/alarmtimer.c: 721
 * Wakes up the task that set the alarmtimer /Users/rubber/linux/kernel/time/alarmtimer.c: 723
 * Return: ALARMTIMER_NORESTART /Users/rubber/linux/kernel/time/alarmtimer.c: 725
 * alarmtimer_do_nsleep - Internal alarmtimer nsleep implementation /Users/rubber/linux/kernel/time/alarmtimer.c: 739
 * @alarm: ptr to alarmtimer /Users/rubber/linux/kernel/time/alarmtimer.c: 740
 * @absexp: absolute expiration time /Users/rubber/linux/kernel/time/alarmtimer.c: 741
 * @type: alarm type (BOOTTIME/REALTIME). /Users/rubber/linux/kernel/time/alarmtimer.c: 742
 * Sets the alarm timer and sleeps until it is fired or interrupted. /Users/rubber/linux/kernel/time/alarmtimer.c: 744
 * alarm_timer_nsleep_restart - restartblock alarmtimer nsleep /Users/rubber/linux/kernel/time/alarmtimer.c: 795
 * @restart: ptr to restart block /Users/rubber/linux/kernel/time/alarmtimer.c: 796
 * Handles restarted clock_nanosleep calls /Users/rubber/linux/kernel/time/alarmtimer.c: 798
 * alarm_timer_nsleep - alarmtimer nanosleep /Users/rubber/linux/kernel/time/alarmtimer.c: 812
 * @which_clock: clockid /Users/rubber/linux/kernel/time/alarmtimer.c: 813
 * @flags: determines abstime or relative /Users/rubber/linux/kernel/time/alarmtimer.c: 814
 * @tsreq: requested sleep time (abs or rel) /Users/rubber/linux/kernel/time/alarmtimer.c: 815
 * Handles clock_nanosleep calls against _ALARM clockids /Users/rubber/linux/kernel/time/alarmtimer.c: 817
 * alarmtimer_init - Initialize alarm timer code /Users/rubber/linux/kernel/time/alarmtimer.c: 902
 * This function initializes the alarm bases and registers /Users/rubber/linux/kernel/time/alarmtimer.c: 904
 * the posix clock ids. /Users/rubber/linux/kernel/time/alarmtimer.c: 905
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/time.c: 1
 *  Copyright (C) 1991, 1992  Linus Torvalds /Users/rubber/linux/kernel/time/time.c: 3
 *  This file contains the interface functions for the various time related /Users/rubber/linux/kernel/time/time.c: 5
 *  system calls: time, stime, gettimeofday, settimeofday, adjtime /Users/rubber/linux/kernel/time/time.c: 6
 * Modification history: /Users/rubber/linux/kernel/time/time.c: 8
 * 1993-09-02    Philip Gladstone /Users/rubber/linux/kernel/time/time.c: 10
 *      Created file with time related functions from sched/core.c and adjtimex() /Users/rubber/linux/kernel/time/time.c: 11
 * 1993-10-08    Torsten Duwe /Users/rubber/linux/kernel/time/time.c: 12
 *      adjtime interface update and CMOS clock write code /Users/rubber/linux/kernel/time/time.c: 13
 * 1995-08-13    Torsten Duwe /Users/rubber/linux/kernel/time/time.c: 14
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589) /Users/rubber/linux/kernel/time/time.c: 15
 * 1999-01-16    Ulrich Windl /Users/rubber/linux/kernel/time/time.c: 16
 *	Introduced error checking for many cases in adjtimex(). /Users/rubber/linux/kernel/time/time.c: 17
 *	Updated NTP code according to technical memorandum Jan '96 /Users/rubber/linux/kernel/time/time.c: 18
 *	"A Kernel Model for Precision Timekeeping" by Dave Mills /Users/rubber/linux/kernel/time/time.c: 19
 *	Allow time_constant larger than MAXTC(6) for NTP v4 (MAXTC == 10) /Users/rubber/linux/kernel/time/time.c: 20
 *	(Even though the technical memorandum forbids it) /Users/rubber/linux/kernel/time/time.c: 21
 * 2004-07-14	 Christoph Lameter /Users/rubber/linux/kernel/time/time.c: 22
 *	Added getnstimeofday to allow the posix timer functions to return /Users/rubber/linux/kernel/time/time.c: 23
 *	with nanosecond accuracy /Users/rubber/linux/kernel/time/time.c: 24
 * The timezone where the local system is located.  Used as a default by some /Users/rubber/linux/kernel/time/time.c: 47
 * programs who obtain this value by using gettimeofday. /Users/rubber/linux/kernel/time/time.c: 48
 * sys_time() can be implemented in user-level using /Users/rubber/linux/kernel/time/time.c: 57
 * sys_gettimeofday().  Is this for backwards compatibility?  If so, /Users/rubber/linux/kernel/time/time.c: 58
 * why not move it into the appropriate arch directory (for those /Users/rubber/linux/kernel/time/time.c: 59
 * architectures that need it). /Users/rubber/linux/kernel/time/time.c: 60
 * sys_stime() can be implemented in user-level using /Users/rubber/linux/kernel/time/time.c: 75
 * sys_settimeofday().  Is this for backwards compatibility?  If so, /Users/rubber/linux/kernel/time/time.c: 76
 * why not move it into the appropriate arch directory (for those /Users/rubber/linux/kernel/time/time.c: 77
 * architectures that need it). /Users/rubber/linux/kernel/time/time.c: 78
 * In case for some reason the CMOS clock has not already been running /Users/rubber/linux/kernel/time/time.c: 159
 * in UTC, but in some local time: The first time we set the timezone, /Users/rubber/linux/kernel/time/time.c: 160
 * we will warp the clock so that it is ticking UTC time instead of /Users/rubber/linux/kernel/time/time.c: 161
 * local time. Presumably, if someone is setting the timezone then we /Users/rubber/linux/kernel/time/time.c: 162
 * are running in an environment where the programs understand about /Users/rubber/linux/kernel/time/time.c: 163
 * timezones. This should be done at boot time in the /etc/rc script, /Users/rubber/linux/kernel/time/time.c: 164
 * as soon as possible, so that the clock can be set right. Otherwise, /Users/rubber/linux/kernel/time/time.c: 165
 * various programs will get confused when the clock gets warped. /Users/rubber/linux/kernel/time/time.c: 166
	/* Copy the user data space into the kernel copy /Users/rubber/linux/kernel/time/time.c: 274
	 * structure. But bear in mind that the structures /Users/rubber/linux/kernel/time/time.c: 275
	 * may change /Users/rubber/linux/kernel/time/time.c: 276
 * Convert jiffies to milliseconds and back. /Users/rubber/linux/kernel/time/time.c: 369
 * Avoid unnecessary multiplications/divisions in the /Users/rubber/linux/kernel/time/time.c: 371
 * two most common HZ cases: /Users/rubber/linux/kernel/time/time.c: 372
	/* /Users/rubber/linux/kernel/time/time.c: 393
	 * Hz usually doesn't go much further MSEC_PER_SEC. /Users/rubber/linux/kernel/time/time.c: 394
	 * jiffies_to_usecs() and usecs_to_jiffies() depend on that. /Users/rubber/linux/kernel/time/time.c: 395
 * mktime64 - Converts date to seconds. /Users/rubber/linux/kernel/time/time.c: 412
 * Converts Gregorian date to seconds since 1970-01-01 00:00:00. /Users/rubber/linux/kernel/time/time.c: 413
 * Assumes input in normal date format, i.e. 1980-12-31 23:59:59 /Users/rubber/linux/kernel/time/time.c: 414
 * => year=1980, mon=12, day=31, hour=23, min=59, sec=59. /Users/rubber/linux/kernel/time/time.c: 415
 * [For the Julian calendar (which was used in Russia before 1917, /Users/rubber/linux/kernel/time/time.c: 417
 * Britain & colonies before 1752, anywhere else before 1582, /Users/rubber/linux/kernel/time/time.c: 418
 * and is still in use by some communities) leave out the /Users/rubber/linux/kernel/time/time.c: 419
 * -year/100+year/400 terms, and add 10.] /Users/rubber/linux/kernel/time/time.c: 420
 * This algorithm was first published by Gauss (I think). /Users/rubber/linux/kernel/time/time.c: 422
 * A leap second can be indicated by calling this function with sec as /Users/rubber/linux/kernel/time/time.c: 424
 * 60 (allowable under ISO 8601).  The leap second is treated the same /Users/rubber/linux/kernel/time/time.c: 425
 * as the following second since they don't exist in UNIX time. /Users/rubber/linux/kernel/time/time.c: 426
 * An encoding of midnight at the end of the day as 24:00:00 - ie. midnight /Users/rubber/linux/kernel/time/time.c: 428
 * tomorrow - (allowable under ISO 8601) is supported. /Users/rubber/linux/kernel/time/time.c: 429
 * set_normalized_timespec - set timespec sec and nsec parts and normalize /Users/rubber/linux/kernel/time/time.c: 465
 * @ts:		pointer to timespec variable to be set /Users/rubber/linux/kernel/time/time.c: 467
 * @sec:	seconds to set /Users/rubber/linux/kernel/time/time.c: 468
 * @nsec:	nanoseconds to set /Users/rubber/linux/kernel/time/time.c: 469
 * Set seconds and nanoseconds field of a timespec variable and /Users/rubber/linux/kernel/time/time.c: 471
 * normalize to the timespec storage format /Users/rubber/linux/kernel/time/time.c: 472
 * Note: The tv_nsec part is always in the range of /Users/rubber/linux/kernel/time/time.c: 474
 *	0 <= tv_nsec < NSEC_PER_SEC /Users/rubber/linux/kernel/time/time.c: 475
 * For negative values only the tv_sec field is negative ! /Users/rubber/linux/kernel/time/time.c: 476
		/* /Users/rubber/linux/kernel/time/time.c: 481
		 * The following asm() prevents the compiler from /Users/rubber/linux/kernel/time/time.c: 482
		 * optimising this loop into a modulo operation. See /Users/rubber/linux/kernel/time/time.c: 483
		 * also __iter_div_u64_rem() in include/linux/time.h /Users/rubber/linux/kernel/time/time.c: 484
 * ns_to_timespec64 - Convert nanoseconds to timespec64 /Users/rubber/linux/kernel/time/time.c: 501
 * @nsec:       the nanoseconds value to be converted /Users/rubber/linux/kernel/time/time.c: 502
 * Returns the timespec64 representation of the nsec parameter. /Users/rubber/linux/kernel/time/time.c: 504
		/* /Users/rubber/linux/kernel/time/time.c: 515
		 * With negative times, tv_sec points to the earlier /Users/rubber/linux/kernel/time/time.c: 516
		 * second, and tv_nsec counts the nanoseconds since /Users/rubber/linux/kernel/time/time.c: 517
		 * then, so tv_nsec is always a positive number. /Users/rubber/linux/kernel/time/time.c: 518
 * msecs_to_jiffies: - convert milliseconds to jiffies /Users/rubber/linux/kernel/time/time.c: 529
 * @m:	time in milliseconds /Users/rubber/linux/kernel/time/time.c: 530
 * conversion is done as follows: /Users/rubber/linux/kernel/time/time.c: 532
 * - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET) /Users/rubber/linux/kernel/time/time.c: 534
 * - 'too large' values [that would result in larger than /Users/rubber/linux/kernel/time/time.c: 536
 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too. /Users/rubber/linux/kernel/time/time.c: 537
 * - all other values are converted to jiffies by either multiplying /Users/rubber/linux/kernel/time/time.c: 539
 *   the input value by a factor or dividing it with a factor and /Users/rubber/linux/kernel/time/time.c: 540
 *   handling any 32-bit overflows. /Users/rubber/linux/kernel/time/time.c: 541
 *   for the details see __msecs_to_jiffies() /Users/rubber/linux/kernel/time/time.c: 542
 * msecs_to_jiffies() checks for the passed in value being a constant /Users/rubber/linux/kernel/time/time.c: 544
 * via __builtin_constant_p() allowing gcc to eliminate most of the /Users/rubber/linux/kernel/time/time.c: 545
 * code, __msecs_to_jiffies() is called if the value passed does not /Users/rubber/linux/kernel/time/time.c: 546
 * allow constant folding and the actual conversion must be done at /Users/rubber/linux/kernel/time/time.c: 547
 * runtime. /Users/rubber/linux/kernel/time/time.c: 548
 * the _msecs_to_jiffies helpers are the HZ dependent conversion /Users/rubber/linux/kernel/time/time.c: 549
 * routines found in include/linux/jiffies.h /Users/rubber/linux/kernel/time/time.c: 550
	/* /Users/rubber/linux/kernel/time/time.c: 554
	 * Negative value, means infinite timeout: /Users/rubber/linux/kernel/time/time.c: 555
 * The TICK_NSEC - 1 rounds up the value to the next resolution.  Note /Users/rubber/linux/kernel/time/time.c: 572
 * that a remainder subtract here would not do the right thing as the /Users/rubber/linux/kernel/time/time.c: 573
 * resolution values don't fall on second boundaries.  I.e. the line: /Users/rubber/linux/kernel/time/time.c: 574
 * nsec -= nsec % TICK_NSEC; is NOT a correct resolution rounding. /Users/rubber/linux/kernel/time/time.c: 575
 * Note that due to the small error in the multiplier here, this /Users/rubber/linux/kernel/time/time.c: 576
 * rounding is incorrect for sufficiently large values of tv_nsec, but /Users/rubber/linux/kernel/time/time.c: 577
 * well formed timespecs should have tv_nsec < NSEC_PER_SEC, so we're /Users/rubber/linux/kernel/time/time.c: 578
 * OK. /Users/rubber/linux/kernel/time/time.c: 579
 * Rather, we just shift the bits off the right. /Users/rubber/linux/kernel/time/time.c: 581
 * The >> (NSEC_JIFFIE_SC - SEC_JIFFIE_SC) converts the scaled nsec /Users/rubber/linux/kernel/time/time.c: 583
 * value to a scaled second value. /Users/rubber/linux/kernel/time/time.c: 584
	/* /Users/rubber/linux/kernel/time/time.c: 607
	 * Convert jiffies to nanoseconds and separate with /Users/rubber/linux/kernel/time/time.c: 608
	 * one divide. /Users/rubber/linux/kernel/time/time.c: 609
 * Convert jiffies/jiffies_64 to clock_t and back. /Users/rubber/linux/kernel/time/time.c: 619
	/* /Users/rubber/linux/kernel/time/time.c: 663
	 * There are better ways that don't overflow early, /Users/rubber/linux/kernel/time/time.c: 664
	 * but even this doesn't overflow in hundreds of years /Users/rubber/linux/kernel/time/time.c: 665
	 * in 64 bits, so.. /Users/rubber/linux/kernel/time/time.c: 666
	/* /Users/rubber/linux/kernel/time/time.c: 681
         * max relative error 5.7e-8 (1.8s per year) for USER_HZ <= 1024, /Users/rubber/linux/kernel/time/time.c: 682
         * overflow after 64.99 years. /Users/rubber/linux/kernel/time/time.c: 683
         * exact for HZ=60, 72, 90, 120, 144, 180, 300, 600, 900, ... /Users/rubber/linux/kernel/time/time.c: 684
 * nsecs_to_jiffies64 - Convert nsecs in u64 to jiffies64 /Users/rubber/linux/kernel/time/time.c: 711
 * @n:	nsecs in u64 /Users/rubber/linux/kernel/time/time.c: 713
 * Unlike {m,u}secs_to_jiffies, type of input is not unsigned int but u64. /Users/rubber/linux/kernel/time/time.c: 715
 * And this doesn't return MAX_JIFFY_OFFSET since this function is designed /Users/rubber/linux/kernel/time/time.c: 716
 * for scheduler, not for use in device drivers to calculate timeout value. /Users/rubber/linux/kernel/time/time.c: 717
 * note: /Users/rubber/linux/kernel/time/time.c: 719
 *   NSEC_PER_SEC = 10^9 = (5^9 * 2^9) = (1953125 * 512) /Users/rubber/linux/kernel/time/time.c: 720
 *   ULLONG_MAX ns = 18446744073.709551615 secs = about 584 years /Users/rubber/linux/kernel/time/time.c: 721
	/* /Users/rubber/linux/kernel/time/time.c: 732
	 * Generic case - optimized for cases where HZ is a multiple of 3. /Users/rubber/linux/kernel/time/time.c: 733
	 * overflow after 64.99 years, exact for HZ = 60, 72, 90, 120 etc. /Users/rubber/linux/kernel/time/time.c: 734
 * nsecs_to_jiffies - Convert nsecs in u64 to jiffies /Users/rubber/linux/kernel/time/time.c: 742
 * @n:	nsecs in u64 /Users/rubber/linux/kernel/time/time.c: 744
 * Unlike {m,u}secs_to_jiffies, type of input is not unsigned int but u64. /Users/rubber/linux/kernel/time/time.c: 746
 * And this doesn't return MAX_JIFFY_OFFSET since this function is designed /Users/rubber/linux/kernel/time/time.c: 747
 * for scheduler, not for use in device drivers to calculate timeout value. /Users/rubber/linux/kernel/time/time.c: 748
 * note: /Users/rubber/linux/kernel/time/time.c: 750
 *   NSEC_PER_SEC = 10^9 = (5^9 * 2^9) = (1953125 * 512) /Users/rubber/linux/kernel/time/time.c: 751
 *   ULLONG_MAX ns = 18446744073.709551615 secs = about 584 years /Users/rubber/linux/kernel/time/time.c: 752
 * Add two timespec64 values and do a safety check for overflow. /Users/rubber/linux/kernel/time/time.c: 761
 * It's assumed that both values are valid (>= 0). /Users/rubber/linux/kernel/time/time.c: 762
 * And, each timespec64 is in normalized form. /Users/rubber/linux/kernel/time/time.c: 763
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/time/jiffies.c: 1
 * This file contains the jiffies based clocksource. /Users/rubber/linux/kernel/time/jiffies.c: 3
 * Copyright (C) 2004, 2005 IBM, John Stultz (johnstul@us.ibm.com) /Users/rubber/linux/kernel/time/jiffies.c: 5
 * The Jiffies based clocksource is the lowest common /Users/rubber/linux/kernel/time/jiffies.c: 22
 * denominator clock source which should function on /Users/rubber/linux/kernel/time/jiffies.c: 23
 * all systems. It has the same coarse resolution as /Users/rubber/linux/kernel/time/jiffies.c: 24
 * the timer interrupt frequency HZ and it suffers /Users/rubber/linux/kernel/time/jiffies.c: 25
 * inaccuracies caused by missed or lost timer /Users/rubber/linux/kernel/time/jiffies.c: 26
 * interrupts and the inability for the timer /Users/rubber/linux/kernel/time/jiffies.c: 27
 * interrupt hardware to accurately tick at the /Users/rubber/linux/kernel/time/jiffies.c: 28
 * requested HZ value. It is also not recommended /Users/rubber/linux/kernel/time/jiffies.c: 29
 * for "tick-less" systems. /Users/rubber/linux/kernel/time/jiffies.c: 30
/* Adjustment factor when a ceiling value is used.  Use as: /Users/rubber/linux/kernel/time/timeconst.bc: 20
/* Compute the appropriate mul/adj values as well as a shift count, /Users/rubber/linux/kernel/time/timeconst.bc: 29
   which brings the mul value into the range 2^b-1 <= x < 2^b.  Such /Users/rubber/linux/kernel/time/timeconst.bc: 30
   a shift value will be correct in the signed integer range and off /Users/rubber/linux/kernel/time/timeconst.bc: 31
 Automatically generated by kernel/time/timeconst.bc */\n" /Users/rubber/linux/kernel/time/timeconst.bc: 44
 Time conversion constants for HZ == ", hz, " */\n" /Users/rubber/linux/kernel/time/timeconst.bc: 45
 KERNEL_TIMECONST_H */\n" /Users/rubber/linux/kernel/time/timeconst.bc: 111
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/timer_list.c: 1
 * List pending timers /Users/rubber/linux/kernel/time/timer_list.c: 3
 * Copyright(C) 2006, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/timer_list.c: 5
 * This allows printing both to /proc/timer_list and /Users/rubber/linux/kernel/time/timer_list.c: 27
 * to the console (on SysRq-Q): /Users/rubber/linux/kernel/time/timer_list.c: 28
	/* /Users/rubber/linux/kernel/time/timer_list.c: 76
	 * Crude but we have to do this O(N*N) thing, because /Users/rubber/linux/kernel/time/timer_list.c: 77
	 * we have to unlock the base when printing: /Users/rubber/linux/kernel/time/timer_list.c: 78
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/itimer.c: 1
 * Copyright (C) 1992 Darren Senn /Users/rubber/linux/kernel/time/itimer.c: 3
 * itimer_get_remtime - get remaining time for the timer /Users/rubber/linux/kernel/time/itimer.c: 22
 * @timer: the timer to read /Users/rubber/linux/kernel/time/itimer.c: 24
 * Returns the delta between the expiry time and now, which can be /Users/rubber/linux/kernel/time/itimer.c: 26
 * less than zero or 1usec for an pending expired timer /Users/rubber/linux/kernel/time/itimer.c: 27
	/* /Users/rubber/linux/kernel/time/itimer.c: 33
	 * Racy but safe: if the itimer expires after the above /Users/rubber/linux/kernel/time/itimer.c: 34
	 * hrtimer_get_remtime() call but before this condition /Users/rubber/linux/kernel/time/itimer.c: 35
	 * then we return 0 - which is correct. /Users/rubber/linux/kernel/time/itimer.c: 36
 * The timer is automagically restarted, when interval != 0 /Users/rubber/linux/kernel/time/itimer.c: 154
 * Returns true if the timeval is in canonical form /Users/rubber/linux/kernel/time/itimer.c: 201
 * alarm_setitimer - set alarm in seconds /Users/rubber/linux/kernel/time/itimer.c: 266
 * @seconds:	number of seconds until alarm /Users/rubber/linux/kernel/time/itimer.c: 268
 *		0 disables the alarm /Users/rubber/linux/kernel/time/itimer.c: 269
 * Returns the remaining time in seconds of a pending timer or 0 when /Users/rubber/linux/kernel/time/itimer.c: 271
 * the timer is not active. /Users/rubber/linux/kernel/time/itimer.c: 272
 * On 32 bit machines the seconds value is limited to (INT_MAX/2) to avoid /Users/rubber/linux/kernel/time/itimer.c: 274
 * negative timeval settings which would cause immediate expiry. /Users/rubber/linux/kernel/time/itimer.c: 275
	/* /Users/rubber/linux/kernel/time/itimer.c: 291
	 * We can't return 0 if we have an alarm pending ...  And we'd /Users/rubber/linux/kernel/time/itimer.c: 292
	 * better return too much than too little anyway /Users/rubber/linux/kernel/time/itimer.c: 293
 * For backwards compatibility?  This can be done in libc so Alpha /Users/rubber/linux/kernel/time/itimer.c: 303
 * and all newer ports shouldn't need it. /Users/rubber/linux/kernel/time/itimer.c: 304
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-legacy.c: 1
 * Timer tick function for architectures that lack generic clockevents, /Users/rubber/linux/kernel/time/tick-legacy.c: 3
 * consolidated here from m68k/ia64/parisc/arm. /Users/rubber/linux/kernel/time/tick-legacy.c: 4
 * legacy_timer_tick() - advances the timekeeping infrastructure /Users/rubber/linux/kernel/time/tick-legacy.c: 14
 * @ticks:	number of ticks, that have elapsed since the last call. /Users/rubber/linux/kernel/time/tick-legacy.c: 15
 * This is used by platforms that have not been converted to /Users/rubber/linux/kernel/time/tick-legacy.c: 17
 * generic clockevents. /Users/rubber/linux/kernel/time/tick-legacy.c: 18
 * If 'ticks' is zero, the CPU is not handling timekeeping, so /Users/rubber/linux/kernel/time/tick-legacy.c: 20
 * only perform process accounting and profiling. /Users/rubber/linux/kernel/time/tick-legacy.c: 21
 * Must be called with interrupts disabled. /Users/rubber/linux/kernel/time/tick-legacy.c: 23
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/tick-oneshot.c: 1
 * This file contains functions which manage high resolution tick /Users/rubber/linux/kernel/time/tick-oneshot.c: 3
 * related events. /Users/rubber/linux/kernel/time/tick-oneshot.c: 4
 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/tick-oneshot.c: 6
 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/tick-oneshot.c: 7
 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/tick-oneshot.c: 8
 * tick_program_event /Users/rubber/linux/kernel/time/tick-oneshot.c: 21
		/* /Users/rubber/linux/kernel/time/tick-oneshot.c: 28
		 * We don't need the clock event device any more, stop it. /Users/rubber/linux/kernel/time/tick-oneshot.c: 29
		/* /Users/rubber/linux/kernel/time/tick-oneshot.c: 37
		 * We need the clock event again, configure it in ONESHOT mode /Users/rubber/linux/kernel/time/tick-oneshot.c: 38
		 * before using it. /Users/rubber/linux/kernel/time/tick-oneshot.c: 39
 * tick_resume_oneshot - resume oneshot mode /Users/rubber/linux/kernel/time/tick-oneshot.c: 48
 * tick_setup_oneshot - setup the event device for oneshot mode (hres or nohz) /Users/rubber/linux/kernel/time/tick-oneshot.c: 59
 * tick_switch_to_oneshot - switch to oneshot mode /Users/rubber/linux/kernel/time/tick-oneshot.c: 71
 * tick_check_oneshot_mode - check whether the system is in oneshot mode /Users/rubber/linux/kernel/time/tick-oneshot.c: 102
 * returns 1 when either nohz or highres are enabled. otherwise 0. /Users/rubber/linux/kernel/time/tick-oneshot.c: 104
 * tick_init_highres - switch to high resolution mode /Users/rubber/linux/kernel/time/tick-oneshot.c: 120
 * Called with interrupts disabled. /Users/rubber/linux/kernel/time/tick-oneshot.c: 122
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/hrtimer.c: 1
 *  Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/time/hrtimer.c: 3
 *  Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/time/hrtimer.c: 4
 *  Copyright(C) 2006-2007  Timesys Corp., Thomas Gleixner /Users/rubber/linux/kernel/time/hrtimer.c: 5
 *  High-resolution kernel timers /Users/rubber/linux/kernel/time/hrtimer.c: 7
 *  In contrast to the low-resolution timeout API, aka timer wheel, /Users/rubber/linux/kernel/time/hrtimer.c: 9
 *  hrtimers provide finer resolution and accuracy depending on system /Users/rubber/linux/kernel/time/hrtimer.c: 10
 *  configuration and capabilities. /Users/rubber/linux/kernel/time/hrtimer.c: 11
 *  Started by: Thomas Gleixner and Ingo Molnar /Users/rubber/linux/kernel/time/hrtimer.c: 13
 *  Credits: /Users/rubber/linux/kernel/time/hrtimer.c: 15
 *	Based on the original timer wheel code /Users/rubber/linux/kernel/time/hrtimer.c: 16
 *	Help, testing, suggestions, bugfixes, improvements were /Users/rubber/linux/kernel/time/hrtimer.c: 18
 *	provided by: /Users/rubber/linux/kernel/time/hrtimer.c: 19
 *	George Anzinger, Andrew Morton, Steven Rostedt, Roman Zippel /Users/rubber/linux/kernel/time/hrtimer.c: 21
 *	et. al. /Users/rubber/linux/kernel/time/hrtimer.c: 22
 * Masks for selecting the soft and hard context timers from /Users/rubber/linux/kernel/time/hrtimer.c: 52
 * cpu_base->active /Users/rubber/linux/kernel/time/hrtimer.c: 53
 * The timer bases: /Users/rubber/linux/kernel/time/hrtimer.c: 61
 * There are more clockids than hrtimer bases. Thus, we index /Users/rubber/linux/kernel/time/hrtimer.c: 63
 * into the timer bases by the hrtimer_base_type enum. When trying /Users/rubber/linux/kernel/time/hrtimer.c: 64
 * to reach a base using a clockid, hrtimer_clockid_to_base() /Users/rubber/linux/kernel/time/hrtimer.c: 65
 * is used to convert from clockid to the proper hrtimer_base_type. /Users/rubber/linux/kernel/time/hrtimer.c: 66
 * Functions and macros which are different for UP/SMP systems are kept in a /Users/rubber/linux/kernel/time/hrtimer.c: 127
 * single place /Users/rubber/linux/kernel/time/hrtimer.c: 128
 * We require the migration_base for lock_hrtimer_base()/switch_hrtimer_base() /Users/rubber/linux/kernel/time/hrtimer.c: 133
 * such that hrtimer_callback_running() can unconditionally dereference /Users/rubber/linux/kernel/time/hrtimer.c: 134
 * timer->base->cpu_base /Users/rubber/linux/kernel/time/hrtimer.c: 135
 * We are using hashed locking: holding per_cpu(hrtimer_bases)[n].lock /Users/rubber/linux/kernel/time/hrtimer.c: 153
 * means that all timers which are tied to this base via timer->base are /Users/rubber/linux/kernel/time/hrtimer.c: 154
 * locked, and the base itself is locked too. /Users/rubber/linux/kernel/time/hrtimer.c: 155
 * So __run_timers/migrate_timers can safely modify all timers which could /Users/rubber/linux/kernel/time/hrtimer.c: 157
 * be found on the lists/queues. /Users/rubber/linux/kernel/time/hrtimer.c: 158
 * When the timer's base is locked, and the timer removed from list, it is /Users/rubber/linux/kernel/time/hrtimer.c: 160
 * possible to set timer->base = &migration_base and drop the lock: the timer /Users/rubber/linux/kernel/time/hrtimer.c: 161
 * remains locked. /Users/rubber/linux/kernel/time/hrtimer.c: 162
 * We do not migrate the timer when it is expiring before the next /Users/rubber/linux/kernel/time/hrtimer.c: 184
 * event on the target cpu. When high resolution is enabled, we cannot /Users/rubber/linux/kernel/time/hrtimer.c: 185
 * reprogram the target cpu hardware and we would cause it to fire /Users/rubber/linux/kernel/time/hrtimer.c: 186
 * late. To keep it simple, we handle the high resolution enabled and /Users/rubber/linux/kernel/time/hrtimer.c: 187
 * disabled case similar. /Users/rubber/linux/kernel/time/hrtimer.c: 188
 * Called with cpu_base->lock of target cpu held. /Users/rubber/linux/kernel/time/hrtimer.c: 190
 * We switch the timer base to a power-optimized selected CPU target, /Users/rubber/linux/kernel/time/hrtimer.c: 213
 * if: /Users/rubber/linux/kernel/time/hrtimer.c: 214
 *	- NO_HZ_COMMON is enabled /Users/rubber/linux/kernel/time/hrtimer.c: 215
 *	- timer migration is enabled /Users/rubber/linux/kernel/time/hrtimer.c: 216
 *	- the timer callback is not running /Users/rubber/linux/kernel/time/hrtimer.c: 217
 *	- the timer is not the first expiring timer on the new target /Users/rubber/linux/kernel/time/hrtimer.c: 218
 * If one of the above requirements is not fulfilled we move the timer /Users/rubber/linux/kernel/time/hrtimer.c: 220
 * to the current CPU or leave it on the previously assigned CPU if /Users/rubber/linux/kernel/time/hrtimer.c: 221
 * the timer callback is currently running. /Users/rubber/linux/kernel/time/hrtimer.c: 222
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 238
		 * We are trying to move timer to new_base. /Users/rubber/linux/kernel/time/hrtimer.c: 239
		 * However we can't change timer's base while it is running, /Users/rubber/linux/kernel/time/hrtimer.c: 240
		 * so we keep it on the same CPU. No hassle vs. reprogramming /Users/rubber/linux/kernel/time/hrtimer.c: 241
		 * the event source in the high resolution case. The softirq /Users/rubber/linux/kernel/time/hrtimer.c: 242
		 * code will take care of this when the timer function has /Users/rubber/linux/kernel/time/hrtimer.c: 243
		 * completed. There is no conflict as we hold the lock until /Users/rubber/linux/kernel/time/hrtimer.c: 244
		 * the timer is enqueued. /Users/rubber/linux/kernel/time/hrtimer.c: 245
 * Functions for the union type storage format of ktime_t which are /Users/rubber/linux/kernel/time/hrtimer.c: 296
 * too large for inlining: /Users/rubber/linux/kernel/time/hrtimer.c: 297
 * Divide a ktime value by a nanosecond value /Users/rubber/linux/kernel/time/hrtimer.c: 301
 * Add two ktime values and do a safety check for overflow: /Users/rubber/linux/kernel/time/hrtimer.c: 325
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 331
	 * We use KTIME_SEC_MAX here, the maximum timeout which we can /Users/rubber/linux/kernel/time/hrtimer.c: 332
	 * return to user space in a timespec: /Users/rubber/linux/kernel/time/hrtimer.c: 333
 * fixup_init is called when: /Users/rubber/linux/kernel/time/hrtimer.c: 353
 * - an active object is initialized /Users/rubber/linux/kernel/time/hrtimer.c: 354
 * fixup_activate is called when: /Users/rubber/linux/kernel/time/hrtimer.c: 371
 * - an active object is activated /Users/rubber/linux/kernel/time/hrtimer.c: 372
 * - an unknown non-static object is activated /Users/rubber/linux/kernel/time/hrtimer.c: 373
 * fixup_free is called when: /Users/rubber/linux/kernel/time/hrtimer.c: 387
 * - an active object is freed /Users/rubber/linux/kernel/time/hrtimer.c: 388
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 538
	 * clock_was_set() might have changed base->offset of any of /Users/rubber/linux/kernel/time/hrtimer.c: 539
	 * the clock bases so the result might be negative. Fix it up /Users/rubber/linux/kernel/time/hrtimer.c: 540
	 * to prevent a false positive in clockevents_program_event(). /Users/rubber/linux/kernel/time/hrtimer.c: 541
 * Recomputes cpu_base::*next_timer and returns the earliest expires_next /Users/rubber/linux/kernel/time/hrtimer.c: 549
 * but does not set cpu_base::*expires_next, that is done by /Users/rubber/linux/kernel/time/hrtimer.c: 550
 * hrtimer[_force]_reprogram and hrtimer_interrupt only. When updating /Users/rubber/linux/kernel/time/hrtimer.c: 551
 * cpu_base::*expires_next right away, reprogramming logic would no longer /Users/rubber/linux/kernel/time/hrtimer.c: 552
 * work. /Users/rubber/linux/kernel/time/hrtimer.c: 553
 * When a softirq is pending, we can ignore the HRTIMER_ACTIVE_SOFT bases, /Users/rubber/linux/kernel/time/hrtimer.c: 555
 * those timers will get run whenever the softirq gets handled, at the end of /Users/rubber/linux/kernel/time/hrtimer.c: 556
 * hrtimer_run_softirq(), hrtimer_update_softirq_timer() will re-add these bases. /Users/rubber/linux/kernel/time/hrtimer.c: 557
 * Therefore softirq values are those from the HRTIMER_ACTIVE_SOFT clock bases. /Users/rubber/linux/kernel/time/hrtimer.c: 559
 * The !softirq values are the minima across HRTIMER_ACTIVE_ALL, unless an actual /Users/rubber/linux/kernel/time/hrtimer.c: 560
 * softirq is pending, in which case they're the minima of HRTIMER_ACTIVE_HARD. /Users/rubber/linux/kernel/time/hrtimer.c: 561
 * @active_mask must be one of: /Users/rubber/linux/kernel/time/hrtimer.c: 563
 *  - HRTIMER_ACTIVE_ALL, /Users/rubber/linux/kernel/time/hrtimer.c: 564
 *  - HRTIMER_ACTIVE_SOFT, or /Users/rubber/linux/kernel/time/hrtimer.c: 565
 *  - HRTIMER_ACTIVE_HARD. /Users/rubber/linux/kernel/time/hrtimer.c: 566
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 598
	 * If the soft interrupt has already been activated, ignore the /Users/rubber/linux/kernel/time/hrtimer.c: 599
	 * soft bases. They will be handled in the already raised soft /Users/rubber/linux/kernel/time/hrtimer.c: 600
	 * interrupt. /Users/rubber/linux/kernel/time/hrtimer.c: 601
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 605
		 * Update the soft expiry time. clock_settime() might have /Users/rubber/linux/kernel/time/hrtimer.c: 606
		 * affected it. /Users/rubber/linux/kernel/time/hrtimer.c: 607
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 613
	 * If a softirq timer is expiring first, update cpu_base->next_timer /Users/rubber/linux/kernel/time/hrtimer.c: 614
	 * and program the hardware with the soft expiry time. /Users/rubber/linux/kernel/time/hrtimer.c: 615
 * Is the high resolution mode active ? /Users/rubber/linux/kernel/time/hrtimer.c: 642
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 661
	 * If hres is not active, hardware does not have to be /Users/rubber/linux/kernel/time/hrtimer.c: 662
	 * reprogrammed yet. /Users/rubber/linux/kernel/time/hrtimer.c: 663
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 664
	 * If a hang was detected in the last timer interrupt then we /Users/rubber/linux/kernel/time/hrtimer.c: 665
	 * leave the hang delay active in the hardware. We want the /Users/rubber/linux/kernel/time/hrtimer.c: 666
	 * system to make progress. That also prevents the following /Users/rubber/linux/kernel/time/hrtimer.c: 667
	 * scenario: /Users/rubber/linux/kernel/time/hrtimer.c: 668
	 * T1 expires 50ms from now /Users/rubber/linux/kernel/time/hrtimer.c: 669
	 * T2 expires 5s from now /Users/rubber/linux/kernel/time/hrtimer.c: 670
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 671
	 * T1 is removed, so this code is called and would reprogram /Users/rubber/linux/kernel/time/hrtimer.c: 672
	 * the hardware to 5s from now. Any hrtimer_start after that /Users/rubber/linux/kernel/time/hrtimer.c: 673
	 * will not reprogram the hardware due to hang_detected being /Users/rubber/linux/kernel/time/hrtimer.c: 674
	 * set. So we'd effectively block all timers until the T2 event /Users/rubber/linux/kernel/time/hrtimer.c: 675
	 * fires. /Users/rubber/linux/kernel/time/hrtimer.c: 676
 * Reprogram the event source with checking both queues for the /Users/rubber/linux/kernel/time/hrtimer.c: 685
 * next event /Users/rubber/linux/kernel/time/hrtimer.c: 686
 * Called with interrupts disabled and base->lock held /Users/rubber/linux/kernel/time/hrtimer.c: 687
 * High resolution timer enabled ? /Users/rubber/linux/kernel/time/hrtimer.c: 706
 * Enable / Disable high resolution mode /Users/rubber/linux/kernel/time/hrtimer.c: 713
 * hrtimer_high_res_enabled - query, if the highres mode is enabled /Users/rubber/linux/kernel/time/hrtimer.c: 723
 * Switch to high resolution mode /Users/rubber/linux/kernel/time/hrtimer.c: 733
 * Retrigger next event is called after clock was set with interrupts /Users/rubber/linux/kernel/time/hrtimer.c: 759
 * disabled through an SMP function call or directly from low level /Users/rubber/linux/kernel/time/hrtimer.c: 760
 * resume code. /Users/rubber/linux/kernel/time/hrtimer.c: 761
 * This is only invoked when: /Users/rubber/linux/kernel/time/hrtimer.c: 763
 *	- CONFIG_HIGH_RES_TIMERS is enabled. /Users/rubber/linux/kernel/time/hrtimer.c: 764
 *	- CONFIG_NOHZ_COMMON is enabled /Users/rubber/linux/kernel/time/hrtimer.c: 765
 * For the other cases this function is empty and because the call sites /Users/rubber/linux/kernel/time/hrtimer.c: 767
 * are optimized out it vanishes as well, i.e. no need for lots of /Users/rubber/linux/kernel/time/hrtimer.c: 768
 * #ifdeffery. /Users/rubber/linux/kernel/time/hrtimer.c: 769
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 775
	 * When high resolution mode or nohz is active, then the offsets of /Users/rubber/linux/kernel/time/hrtimer.c: 776
	 * CLOCK_REALTIME/TAI/BOOTTIME have to be updated. Otherwise the /Users/rubber/linux/kernel/time/hrtimer.c: 777
	 * next tick will take care of that. /Users/rubber/linux/kernel/time/hrtimer.c: 778
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 779
	 * If high resolution mode is active then the next expiring timer /Users/rubber/linux/kernel/time/hrtimer.c: 780
	 * must be reevaluated and the clock event device reprogrammed if /Users/rubber/linux/kernel/time/hrtimer.c: 781
	 * necessary. /Users/rubber/linux/kernel/time/hrtimer.c: 782
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 783
	 * In the NOHZ case the update of the offset and the reevaluation /Users/rubber/linux/kernel/time/hrtimer.c: 784
	 * of the next expiring timer is enough. The return from the SMP /Users/rubber/linux/kernel/time/hrtimer.c: 785
	 * function call will take care of the reprogramming in case the /Users/rubber/linux/kernel/time/hrtimer.c: 786
	 * CPU was in a NOHZ idle sleep. /Users/rubber/linux/kernel/time/hrtimer.c: 787
 * When a timer is enqueued and expires earlier than the already enqueued /Users/rubber/linux/kernel/time/hrtimer.c: 802
 * timers, we have to check, whether it expires earlier than the timer for /Users/rubber/linux/kernel/time/hrtimer.c: 803
 * which the clock event device was armed. /Users/rubber/linux/kernel/time/hrtimer.c: 804
 * Called with interrupts disabled and base->cpu_base.lock held /Users/rubber/linux/kernel/time/hrtimer.c: 806
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 816
	 * CLOCK_REALTIME timer might be requested with an absolute /Users/rubber/linux/kernel/time/hrtimer.c: 817
	 * expiry time which is less than base->offset. Set it to 0. /Users/rubber/linux/kernel/time/hrtimer.c: 818
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 824
		 * soft hrtimer could be started on a remote CPU. In this /Users/rubber/linux/kernel/time/hrtimer.c: 825
		 * case softirq_expires_next needs to be updated on the /Users/rubber/linux/kernel/time/hrtimer.c: 826
		 * remote CPU. The soft hrtimer will not expire before the /Users/rubber/linux/kernel/time/hrtimer.c: 827
		 * first hard hrtimer on the remote CPU - /Users/rubber/linux/kernel/time/hrtimer.c: 828
		 * hrtimer_check_target() prevents this case. /Users/rubber/linux/kernel/time/hrtimer.c: 829
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 847
	 * If the timer is not on the current cpu, we cannot reprogram /Users/rubber/linux/kernel/time/hrtimer.c: 848
	 * the other cpus clock event device. /Users/rubber/linux/kernel/time/hrtimer.c: 849
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 857
	 * If the hrtimer interrupt is running, then it will reevaluate the /Users/rubber/linux/kernel/time/hrtimer.c: 858
	 * clock bases and reprogram the clock event device. /Users/rubber/linux/kernel/time/hrtimer.c: 859
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 876
	 * Update the base offsets unconditionally so the following /Users/rubber/linux/kernel/time/hrtimer.c: 877
	 * checks whether the SMP function call is required works. /Users/rubber/linux/kernel/time/hrtimer.c: 878
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 879
	 * The update is safe even when the remote CPU is in the hrtimer /Users/rubber/linux/kernel/time/hrtimer.c: 880
	 * interrupt or the hrtimer soft interrupt and expiring affected /Users/rubber/linux/kernel/time/hrtimer.c: 881
	 * bases. Either it will see the update before handling a base or /Users/rubber/linux/kernel/time/hrtimer.c: 882
	 * it will see it when it finishes the processing and reevaluates /Users/rubber/linux/kernel/time/hrtimer.c: 883
	 * the next expiring timer. /Users/rubber/linux/kernel/time/hrtimer.c: 884
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 889
	 * If the sequence did not change over the update then the /Users/rubber/linux/kernel/time/hrtimer.c: 890
	 * remote CPU already handled it. /Users/rubber/linux/kernel/time/hrtimer.c: 891
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 896
	 * If the remote CPU is currently handling an hrtimer interrupt, it /Users/rubber/linux/kernel/time/hrtimer.c: 897
	 * will reevaluate the first expiring timer of all clock bases /Users/rubber/linux/kernel/time/hrtimer.c: 898
	 * before reprogramming. Nothing to do here. /Users/rubber/linux/kernel/time/hrtimer.c: 899
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 904
	 * Walk the affected clock bases and check whether the first expiring /Users/rubber/linux/kernel/time/hrtimer.c: 905
	 * timer in a clock base is moving ahead of the first expiring timer of /Users/rubber/linux/kernel/time/hrtimer.c: 906
	 * @cpu_base. If so, the IPI must be invoked because per CPU clock /Users/rubber/linux/kernel/time/hrtimer.c: 907
	 * event devices cannot be remotely reprogrammed. /Users/rubber/linux/kernel/time/hrtimer.c: 908
 * Clock was set. This might affect CLOCK_REALTIME, CLOCK_TAI and /Users/rubber/linux/kernel/time/hrtimer.c: 932
 * CLOCK_BOOTTIME (for late sleep time injection). /Users/rubber/linux/kernel/time/hrtimer.c: 933
 * This requires to update the offsets for these clocks /Users/rubber/linux/kernel/time/hrtimer.c: 935
 * vs. CLOCK_MONOTONIC. When high resolution timers are enabled, then this /Users/rubber/linux/kernel/time/hrtimer.c: 936
 * also requires to eventually reprogram the per CPU clock event devices /Users/rubber/linux/kernel/time/hrtimer.c: 937
 * when the change moves an affected timer ahead of the first expiring /Users/rubber/linux/kernel/time/hrtimer.c: 938
 * timer on that CPU. Obviously remote per CPU clock event devices cannot /Users/rubber/linux/kernel/time/hrtimer.c: 939
 * be reprogrammed. The other reason why an IPI has to be sent is when the /Users/rubber/linux/kernel/time/hrtimer.c: 940
 * system is in !HIGH_RES and NOHZ mode. The NOHZ mode updates the offsets /Users/rubber/linux/kernel/time/hrtimer.c: 941
 * in the tick, which obviously might be stopped, so this has to bring out /Users/rubber/linux/kernel/time/hrtimer.c: 942
 * the remote CPU which might sleep in idle to get this sorted. /Users/rubber/linux/kernel/time/hrtimer.c: 943
 * Called from timekeeping code to reprogram the hrtimer interrupt device /Users/rubber/linux/kernel/time/hrtimer.c: 991
 * on all cpus and to notify timerfd. /Users/rubber/linux/kernel/time/hrtimer.c: 992
 * Called during resume either directly from via timekeeping_resume() /Users/rubber/linux/kernel/time/hrtimer.c: 1000
 * or in the case of s2idle from tick_unfreeze() to ensure that the /Users/rubber/linux/kernel/time/hrtimer.c: 1001
 * hrtimers are up to date. /Users/rubber/linux/kernel/time/hrtimer.c: 1002
 * Counterpart to lock_hrtimer_base above: /Users/rubber/linux/kernel/time/hrtimer.c: 1012
 * hrtimer_forward - forward the timer expiry /Users/rubber/linux/kernel/time/hrtimer.c: 1021
 * @timer:	hrtimer to forward /Users/rubber/linux/kernel/time/hrtimer.c: 1022
 * @now:	forward past this time /Users/rubber/linux/kernel/time/hrtimer.c: 1023
 * @interval:	the interval to forward /Users/rubber/linux/kernel/time/hrtimer.c: 1024
 * Forward the timer expiry so it will expire in the future. /Users/rubber/linux/kernel/time/hrtimer.c: 1026
 * Returns the number of overruns. /Users/rubber/linux/kernel/time/hrtimer.c: 1027
 * Can be safely called from the callback function of @timer. If /Users/rubber/linux/kernel/time/hrtimer.c: 1029
 * called from other contexts @timer must neither be enqueued nor /Users/rubber/linux/kernel/time/hrtimer.c: 1030
 * running the callback and the caller needs to take care of /Users/rubber/linux/kernel/time/hrtimer.c: 1031
 * serialization. /Users/rubber/linux/kernel/time/hrtimer.c: 1032
 * Note: This only updates the timer expiry value and does not requeue /Users/rubber/linux/kernel/time/hrtimer.c: 1034
 * the timer. /Users/rubber/linux/kernel/time/hrtimer.c: 1035
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 1060
		 * This (and the ktime_add() below) is the /Users/rubber/linux/kernel/time/hrtimer.c: 1061
		 * correction for exact: /Users/rubber/linux/kernel/time/hrtimer.c: 1062
 * enqueue_hrtimer - internal function to (re)start a timer /Users/rubber/linux/kernel/time/hrtimer.c: 1073
 * The timer is inserted in expiry order. Insertion into the /Users/rubber/linux/kernel/time/hrtimer.c: 1075
 * red black tree is O(log(n)). Must hold the base lock. /Users/rubber/linux/kernel/time/hrtimer.c: 1076
 * Returns 1 when the new timer is the leftmost timer in the tree. /Users/rubber/linux/kernel/time/hrtimer.c: 1078
 * __remove_hrtimer - internal function to remove a timer /Users/rubber/linux/kernel/time/hrtimer.c: 1095
 * Caller must hold the base lock. /Users/rubber/linux/kernel/time/hrtimer.c: 1097
 * High resolution timer mode reprograms the clock event device when the /Users/rubber/linux/kernel/time/hrtimer.c: 1099
 * timer is the one which expires next. The caller can disable this by setting /Users/rubber/linux/kernel/time/hrtimer.c: 1100
 * reprogram to zero. This is useful, when the context does a reprogramming /Users/rubber/linux/kernel/time/hrtimer.c: 1101
 * anyway (e.g. timer interrupt) /Users/rubber/linux/kernel/time/hrtimer.c: 1102
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1119
	 * Note: If reprogram is false we do not update /Users/rubber/linux/kernel/time/hrtimer.c: 1120
	 * cpu_base->next_timer. This happens when we remove the first /Users/rubber/linux/kernel/time/hrtimer.c: 1121
	 * timer on a remote cpu. No harm as we never dereference /Users/rubber/linux/kernel/time/hrtimer.c: 1122
	 * cpu_base->next_timer. So the worst thing what can happen is /Users/rubber/linux/kernel/time/hrtimer.c: 1123
	 * an superfluous call to hrtimer_force_reprogram() on the /Users/rubber/linux/kernel/time/hrtimer.c: 1124
	 * remote cpu later on if the same timer gets enqueued again. /Users/rubber/linux/kernel/time/hrtimer.c: 1125
 * remove hrtimer, called with base lock held /Users/rubber/linux/kernel/time/hrtimer.c: 1132
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 1143
		 * Remove the timer and force reprogramming when high /Users/rubber/linux/kernel/time/hrtimer.c: 1144
		 * resolution mode is active and the timer is on the current /Users/rubber/linux/kernel/time/hrtimer.c: 1145
		 * CPU. If we remove a timer on another CPU, reprogramming is /Users/rubber/linux/kernel/time/hrtimer.c: 1146
		 * skipped. The interrupt event on this CPU is fired and /Users/rubber/linux/kernel/time/hrtimer.c: 1147
		 * reprogramming happens in the interrupt handler. This is a /Users/rubber/linux/kernel/time/hrtimer.c: 1148
		 * rare case and less expensive than a smp call. /Users/rubber/linux/kernel/time/hrtimer.c: 1149
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 1154
		 * If the timer is not restarted then reprogramming is /Users/rubber/linux/kernel/time/hrtimer.c: 1155
		 * required if the timer is local. If it is local and about /Users/rubber/linux/kernel/time/hrtimer.c: 1156
		 * to be restarted, avoid programming it twice (on removal /Users/rubber/linux/kernel/time/hrtimer.c: 1157
		 * and a moment later when it's requeued). /Users/rubber/linux/kernel/time/hrtimer.c: 1158
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1175
	 * CONFIG_TIME_LOW_RES indicates that the system has no way to return /Users/rubber/linux/kernel/time/hrtimer.c: 1176
	 * granular time values. For relative timers we add hrtimer_resolution /Users/rubber/linux/kernel/time/hrtimer.c: 1177
	 * (i.e. one jiffie) to prevent short timeouts. /Users/rubber/linux/kernel/time/hrtimer.c: 1178
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1192
	 * Find the next SOFT expiration. /Users/rubber/linux/kernel/time/hrtimer.c: 1193
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1197
	 * reprogramming needs to be triggered, even if the next soft /Users/rubber/linux/kernel/time/hrtimer.c: 1198
	 * hrtimer expires at the same time than the next hard /Users/rubber/linux/kernel/time/hrtimer.c: 1199
	 * hrtimer. cpu_base->softirq_expires_next needs to be updated! /Users/rubber/linux/kernel/time/hrtimer.c: 1200
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1205
	 * cpu_base->*next_timer is recomputed by __hrtimer_get_next_event() /Users/rubber/linux/kernel/time/hrtimer.c: 1206
	 * cpu_base->*expires_next is only set by hrtimer_reprogram() /Users/rubber/linux/kernel/time/hrtimer.c: 1207
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1219
	 * If the timer is on the local cpu base and is the first expiring /Users/rubber/linux/kernel/time/hrtimer.c: 1220
	 * timer then this might end up reprogramming the hardware twice /Users/rubber/linux/kernel/time/hrtimer.c: 1221
	 * (on removal and on enqueue). To avoid that by prevent the /Users/rubber/linux/kernel/time/hrtimer.c: 1222
	 * reprogram on removal, keep the timer local to the current CPU /Users/rubber/linux/kernel/time/hrtimer.c: 1223
	 * and enforce reprogramming after it is queued no matter whether /Users/rubber/linux/kernel/time/hrtimer.c: 1224
	 * it is the new first expiring timer again or not. /Users/rubber/linux/kernel/time/hrtimer.c: 1225
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1230
	 * Remove an active timer from the queue. In case it is not queued /Users/rubber/linux/kernel/time/hrtimer.c: 1231
	 * on the current CPU, make sure that remove_hrtimer() updates the /Users/rubber/linux/kernel/time/hrtimer.c: 1232
	 * remote data correctly. /Users/rubber/linux/kernel/time/hrtimer.c: 1233
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1234
	 * If it's on the current CPU and the first expiring timer, then /Users/rubber/linux/kernel/time/hrtimer.c: 1235
	 * skip reprogramming, keep the timer local and enforce /Users/rubber/linux/kernel/time/hrtimer.c: 1236
	 * reprogramming later if it was the first expiring timer.  This /Users/rubber/linux/kernel/time/hrtimer.c: 1237
	 * avoids programming the underlying clock event twice (once at /Users/rubber/linux/kernel/time/hrtimer.c: 1238
	 * removal and once after enqueue). /Users/rubber/linux/kernel/time/hrtimer.c: 1239
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1262
	 * Timer was forced to stay on the current CPU to avoid /Users/rubber/linux/kernel/time/hrtimer.c: 1263
	 * reprogramming on removal and enqueue. Force reprogram the /Users/rubber/linux/kernel/time/hrtimer.c: 1264
	 * hardware by evaluating the new first expiring timer. /Users/rubber/linux/kernel/time/hrtimer.c: 1265
 * hrtimer_start_range_ns - (re)start an hrtimer /Users/rubber/linux/kernel/time/hrtimer.c: 1272
 * @timer:	the timer to be added /Users/rubber/linux/kernel/time/hrtimer.c: 1273
 * @tim:	expiry time /Users/rubber/linux/kernel/time/hrtimer.c: 1274
 * @delta_ns:	"slack" range for the timer /Users/rubber/linux/kernel/time/hrtimer.c: 1275
 * @mode:	timer mode: absolute (HRTIMER_MODE_ABS) or /Users/rubber/linux/kernel/time/hrtimer.c: 1276
 *		relative (HRTIMER_MODE_REL), and pinned (HRTIMER_MODE_PINNED); /Users/rubber/linux/kernel/time/hrtimer.c: 1277
 *		softirq based mode is considered for debug purpose only! /Users/rubber/linux/kernel/time/hrtimer.c: 1278
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1286
	 * Check whether the HRTIMER_MODE_SOFT bit and hrtimer.is_soft /Users/rubber/linux/kernel/time/hrtimer.c: 1287
	 * match on CONFIG_PREEMPT_RT = n. With PREEMPT_RT check the hard /Users/rubber/linux/kernel/time/hrtimer.c: 1288
	 * expiry mode because unmarked timers are moved to softirq expiry. /Users/rubber/linux/kernel/time/hrtimer.c: 1289
 * hrtimer_try_to_cancel - try to deactivate a timer /Users/rubber/linux/kernel/time/hrtimer.c: 1306
 * @timer:	hrtimer to stop /Users/rubber/linux/kernel/time/hrtimer.c: 1307
 * Returns: /Users/rubber/linux/kernel/time/hrtimer.c: 1309
 *  *  0 when the timer was not active /Users/rubber/linux/kernel/time/hrtimer.c: 1311
 *  *  1 when the timer was active /Users/rubber/linux/kernel/time/hrtimer.c: 1312
 *  * -1 when the timer is currently executing the callback function and /Users/rubber/linux/kernel/time/hrtimer.c: 1313
 *    cannot be stopped /Users/rubber/linux/kernel/time/hrtimer.c: 1314
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1322
	 * Check lockless first. If the timer is not active (neither /Users/rubber/linux/kernel/time/hrtimer.c: 1323
	 * enqueued nor running the callback, nothing to do here.  The /Users/rubber/linux/kernel/time/hrtimer.c: 1324
	 * base lock does not serialize against a concurrent enqueue, /Users/rubber/linux/kernel/time/hrtimer.c: 1325
	 * so we can avoid taking it. /Users/rubber/linux/kernel/time/hrtimer.c: 1326
 * The counterpart to hrtimer_cancel_wait_running(). /Users/rubber/linux/kernel/time/hrtimer.c: 1360
 * If there is a waiter for cpu_base->expiry_lock, then it was waiting for /Users/rubber/linux/kernel/time/hrtimer.c: 1362
 * the timer callback to finish. Drop expiry_lock and reacquire it. That /Users/rubber/linux/kernel/time/hrtimer.c: 1363
 * allows the waiter to acquire the lock and make progress. /Users/rubber/linux/kernel/time/hrtimer.c: 1364
 * This function is called on PREEMPT_RT kernels when the fast path /Users/rubber/linux/kernel/time/hrtimer.c: 1378
 * deletion of a timer failed because the timer callback function was /Users/rubber/linux/kernel/time/hrtimer.c: 1379
 * running. /Users/rubber/linux/kernel/time/hrtimer.c: 1380
 * This prevents priority inversion: if the soft irq thread is preempted /Users/rubber/linux/kernel/time/hrtimer.c: 1382
 * in the middle of a timer callback, then calling del_timer_sync() can /Users/rubber/linux/kernel/time/hrtimer.c: 1383
 * lead to two issues: /Users/rubber/linux/kernel/time/hrtimer.c: 1384
 *  - If the caller is on a remote CPU then it has to spin wait for the timer /Users/rubber/linux/kernel/time/hrtimer.c: 1386
 *    handler to complete. This can result in unbound priority inversion. /Users/rubber/linux/kernel/time/hrtimer.c: 1387
 *  - If the caller originates from the task which preempted the timer /Users/rubber/linux/kernel/time/hrtimer.c: 1389
 *    handler on the same CPU, then spin waiting for the timer handler to /Users/rubber/linux/kernel/time/hrtimer.c: 1390
 *    complete is never going to end. /Users/rubber/linux/kernel/time/hrtimer.c: 1391
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1398
	 * Just relax if the timer expires in hard interrupt context or if /Users/rubber/linux/kernel/time/hrtimer.c: 1399
	 * it is currently on the migration base. /Users/rubber/linux/kernel/time/hrtimer.c: 1400
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1407
	 * Mark the base as contended and grab the expiry lock, which is /Users/rubber/linux/kernel/time/hrtimer.c: 1408
	 * held by the softirq across the timer callback. Drop the lock /Users/rubber/linux/kernel/time/hrtimer.c: 1409
	 * immediately so the softirq can expire the next timer. In theory /Users/rubber/linux/kernel/time/hrtimer.c: 1410
	 * the timer could already be running again, but that's more than /Users/rubber/linux/kernel/time/hrtimer.c: 1411
	 * unlikely and just causes another wait loop. /Users/rubber/linux/kernel/time/hrtimer.c: 1412
 * hrtimer_cancel - cancel a timer and wait for the handler to finish. /Users/rubber/linux/kernel/time/hrtimer.c: 1431
 * @timer:	the timer to be cancelled /Users/rubber/linux/kernel/time/hrtimer.c: 1432
 * Returns: /Users/rubber/linux/kernel/time/hrtimer.c: 1434
 *  0 when the timer was not active /Users/rubber/linux/kernel/time/hrtimer.c: 1435
 *  1 when the timer was active /Users/rubber/linux/kernel/time/hrtimer.c: 1436
 * __hrtimer_get_remaining - get remaining time for the timer /Users/rubber/linux/kernel/time/hrtimer.c: 1453
 * @timer:	the timer to read /Users/rubber/linux/kernel/time/hrtimer.c: 1454
 * @adjust:	adjust relative timers when CONFIG_TIME_LOW_RES=y /Users/rubber/linux/kernel/time/hrtimer.c: 1455
 * hrtimer_get_next_event - get the time until next expiry event /Users/rubber/linux/kernel/time/hrtimer.c: 1475
 * Returns the next expiry time or KTIME_MAX if no timer is pending. /Users/rubber/linux/kernel/time/hrtimer.c: 1477
 * hrtimer_next_event_without - time until next expiry event w/o one timer /Users/rubber/linux/kernel/time/hrtimer.c: 1496
 * @exclude:	timer to exclude /Users/rubber/linux/kernel/time/hrtimer.c: 1497
 * Returns the next expiry time over all timers except for the @exclude one or /Users/rubber/linux/kernel/time/hrtimer.c: 1499
 * KTIME_MAX if none of them is pending. /Users/rubber/linux/kernel/time/hrtimer.c: 1500
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1548
	 * On PREEMPT_RT enabled kernels hrtimers which are not explicitly /Users/rubber/linux/kernel/time/hrtimer.c: 1549
	 * marked for hard interrupt expiry mode are moved into soft /Users/rubber/linux/kernel/time/hrtimer.c: 1550
	 * interrupt context for latency reasons and because the callbacks /Users/rubber/linux/kernel/time/hrtimer.c: 1551
	 * can invoke functions which might sleep on RT, e.g. spin_lock(). /Users/rubber/linux/kernel/time/hrtimer.c: 1552
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1561
	 * POSIX magic: Relative CLOCK_REALTIME timers are not affected by /Users/rubber/linux/kernel/time/hrtimer.c: 1562
	 * clock modifications, so they needs to become CLOCK_MONOTONIC to /Users/rubber/linux/kernel/time/hrtimer.c: 1563
	 * ensure POSIX compliance. /Users/rubber/linux/kernel/time/hrtimer.c: 1564
 * hrtimer_init - initialize a timer to the given clock /Users/rubber/linux/kernel/time/hrtimer.c: 1578
 * @timer:	the timer to be initialized /Users/rubber/linux/kernel/time/hrtimer.c: 1579
 * @clock_id:	the clock to be used /Users/rubber/linux/kernel/time/hrtimer.c: 1580
 * @mode:       The modes which are relevant for initialization: /Users/rubber/linux/kernel/time/hrtimer.c: 1581
 *              HRTIMER_MODE_ABS, HRTIMER_MODE_REL, HRTIMER_MODE_ABS_SOFT, /Users/rubber/linux/kernel/time/hrtimer.c: 1582
 *              HRTIMER_MODE_REL_SOFT /Users/rubber/linux/kernel/time/hrtimer.c: 1583
 *              The PINNED variants of the above can be handed in, /Users/rubber/linux/kernel/time/hrtimer.c: 1585
 *              but the PINNED bit is ignored as pinning happens /Users/rubber/linux/kernel/time/hrtimer.c: 1586
 *              when the hrtimer is started /Users/rubber/linux/kernel/time/hrtimer.c: 1587
 * A timer is active, when it is enqueued into the rbtree or the /Users/rubber/linux/kernel/time/hrtimer.c: 1598
 * callback function is running or it's in the state of being migrated /Users/rubber/linux/kernel/time/hrtimer.c: 1599
 * to another cpu. /Users/rubber/linux/kernel/time/hrtimer.c: 1600
 * It is important for this function to not return a false negative. /Users/rubber/linux/kernel/time/hrtimer.c: 1602
 * The write_seqcount_barrier()s in __run_hrtimer() split the thing into 3 /Users/rubber/linux/kernel/time/hrtimer.c: 1625
 * distinct sections: /Users/rubber/linux/kernel/time/hrtimer.c: 1626
 *  - queued:	the timer is queued /Users/rubber/linux/kernel/time/hrtimer.c: 1628
 *  - callback:	the timer is being ran /Users/rubber/linux/kernel/time/hrtimer.c: 1629
 *  - post:	the timer is inactive or (re)queued /Users/rubber/linux/kernel/time/hrtimer.c: 1630
 * On the read side we ensure we observe timer->state and cpu_base->running /Users/rubber/linux/kernel/time/hrtimer.c: 1632
 * from the same section, if anything changed while we looked at it, we retry. /Users/rubber/linux/kernel/time/hrtimer.c: 1633
 * This includes timer->base changing because sequence numbers alone are /Users/rubber/linux/kernel/time/hrtimer.c: 1634
 * insufficient for that. /Users/rubber/linux/kernel/time/hrtimer.c: 1635
 * The sequence numbers are required because otherwise we could still observe /Users/rubber/linux/kernel/time/hrtimer.c: 1637
 * a false negative if the read side got smeared over multiple consecutive /Users/rubber/linux/kernel/time/hrtimer.c: 1638
 * __run_hrtimer() invocations. /Users/rubber/linux/kernel/time/hrtimer.c: 1639
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1656
	 * Separate the ->running assignment from the ->state assignment. /Users/rubber/linux/kernel/time/hrtimer.c: 1657
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1658
	 * As with a regular write barrier, this ensures the read side in /Users/rubber/linux/kernel/time/hrtimer.c: 1659
	 * hrtimer_active() cannot observe base->running == NULL && /Users/rubber/linux/kernel/time/hrtimer.c: 1660
	 * timer->state == INACTIVE. /Users/rubber/linux/kernel/time/hrtimer.c: 1661
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1668
	 * Clear the 'is relative' flag for the TIME_LOW_RES case. If the /Users/rubber/linux/kernel/time/hrtimer.c: 1669
	 * timer is restarted with a period then it becomes an absolute /Users/rubber/linux/kernel/time/hrtimer.c: 1670
	 * timer. If its not restarted it does not matter. /Users/rubber/linux/kernel/time/hrtimer.c: 1671
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1676
	 * The timer is marked as running in the CPU base, so it is /Users/rubber/linux/kernel/time/hrtimer.c: 1677
	 * protected against migration to a different CPU even if the lock /Users/rubber/linux/kernel/time/hrtimer.c: 1678
	 * is dropped. /Users/rubber/linux/kernel/time/hrtimer.c: 1679
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1691
	 * Note: We clear the running state after enqueue_hrtimer and /Users/rubber/linux/kernel/time/hrtimer.c: 1692
	 * we do not reprogram the event hardware. Happens either in /Users/rubber/linux/kernel/time/hrtimer.c: 1693
	 * hrtimer_start_range_ns() or in hrtimer_interrupt() /Users/rubber/linux/kernel/time/hrtimer.c: 1694
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1695
	 * Note: Because we dropped the cpu_base->lock above, /Users/rubber/linux/kernel/time/hrtimer.c: 1696
	 * hrtimer_start_range_ns() can have popped in and enqueued the timer /Users/rubber/linux/kernel/time/hrtimer.c: 1697
	 * for us already. /Users/rubber/linux/kernel/time/hrtimer.c: 1698
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1704
	 * Separate the ->running assignment from the ->state assignment. /Users/rubber/linux/kernel/time/hrtimer.c: 1705
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1706
	 * As with a regular write barrier, this ensures the read side in /Users/rubber/linux/kernel/time/hrtimer.c: 1707
	 * hrtimer_active() cannot observe base->running.timer == NULL && /Users/rubber/linux/kernel/time/hrtimer.c: 1708
	 * timer->state == INACTIVE. /Users/rubber/linux/kernel/time/hrtimer.c: 1709
			/* /Users/rubber/linux/kernel/time/hrtimer.c: 1734
			 * The immediate goal for using the softexpires is /Users/rubber/linux/kernel/time/hrtimer.c: 1735
			 * minimizing wakeups, not running timers at the /Users/rubber/linux/kernel/time/hrtimer.c: 1736
			 * earliest interrupt after their soft expiration. /Users/rubber/linux/kernel/time/hrtimer.c: 1737
			 * This allows us to avoid using a Priority Search /Users/rubber/linux/kernel/time/hrtimer.c: 1738
			 * Tree, which can answer a stabbing query for /Users/rubber/linux/kernel/time/hrtimer.c: 1739
			 * overlapping intervals and instead use the simple /Users/rubber/linux/kernel/time/hrtimer.c: 1740
			 * BST we already have. /Users/rubber/linux/kernel/time/hrtimer.c: 1741
			 * We don't add extra wakeups by delaying timers that /Users/rubber/linux/kernel/time/hrtimer.c: 1742
			 * are right-of a not yet expired timer, because that /Users/rubber/linux/kernel/time/hrtimer.c: 1743
			 * timer will have to trigger a wakeup anyway. /Users/rubber/linux/kernel/time/hrtimer.c: 1744
 * High resolution timer interrupt /Users/rubber/linux/kernel/time/hrtimer.c: 1778
 * Called with interrupts disabled /Users/rubber/linux/kernel/time/hrtimer.c: 1779
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1796
	 * We set expires_next to KTIME_MAX here with cpu_base->lock /Users/rubber/linux/kernel/time/hrtimer.c: 1797
	 * held to prevent that a timer is enqueued in our queue via /Users/rubber/linux/kernel/time/hrtimer.c: 1798
	 * the migration code. This does not affect enqueueing of /Users/rubber/linux/kernel/time/hrtimer.c: 1799
	 * timers which run their callback and need to be requeued on /Users/rubber/linux/kernel/time/hrtimer.c: 1800
	 * this CPU. /Users/rubber/linux/kernel/time/hrtimer.c: 1801
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1815
	 * Store the new expiry value so the migration code can verify /Users/rubber/linux/kernel/time/hrtimer.c: 1816
	 * against it. /Users/rubber/linux/kernel/time/hrtimer.c: 1817
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1829
	 * The next timer was already expired due to: /Users/rubber/linux/kernel/time/hrtimer.c: 1830
	 * - tracing /Users/rubber/linux/kernel/time/hrtimer.c: 1831
	 * - long lasting callbacks /Users/rubber/linux/kernel/time/hrtimer.c: 1832
	 * - being scheduled away when running in a VM /Users/rubber/linux/kernel/time/hrtimer.c: 1833
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1834
	 * We need to prevent that we loop forever in the hrtimer /Users/rubber/linux/kernel/time/hrtimer.c: 1835
	 * interrupt routine. We give it 3 attempts to avoid /Users/rubber/linux/kernel/time/hrtimer.c: 1836
	 * overreacting on some spurious event. /Users/rubber/linux/kernel/time/hrtimer.c: 1837
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1838
	 * Acquire base lock for updating the offsets and retrieving /Users/rubber/linux/kernel/time/hrtimer.c: 1839
	 * the current time. /Users/rubber/linux/kernel/time/hrtimer.c: 1840
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1847
	 * Give the system a chance to do something else than looping /Users/rubber/linux/kernel/time/hrtimer.c: 1848
	 * here. We stored the entry time, so we know exactly how long /Users/rubber/linux/kernel/time/hrtimer.c: 1849
	 * we spent here. We schedule the next event this amount of /Users/rubber/linux/kernel/time/hrtimer.c: 1850
	 * time away. /Users/rubber/linux/kernel/time/hrtimer.c: 1851
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1860
	 * Limit it to a sensible value as we enforce a longer /Users/rubber/linux/kernel/time/hrtimer.c: 1861
	 * delay. Give the CPU at least 100ms to catch up. /Users/rubber/linux/kernel/time/hrtimer.c: 1862
 * Called from run_local_timers in hardirq context every jiffy /Users/rubber/linux/kernel/time/hrtimer.c: 1892
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1903
	 * This _is_ ugly: We have to check periodically, whether we /Users/rubber/linux/kernel/time/hrtimer.c: 1904
	 * can switch to highres and / or nohz mode. The clocksource /Users/rubber/linux/kernel/time/hrtimer.c: 1905
	 * switch happens with xtime_lock held. Notification from /Users/rubber/linux/kernel/time/hrtimer.c: 1906
	 * there only sets the check bit in the tick_oneshot code, /Users/rubber/linux/kernel/time/hrtimer.c: 1907
	 * otherwise we might deadlock vs. xtime_lock. /Users/rubber/linux/kernel/time/hrtimer.c: 1908
 * Sleep related functions: /Users/rubber/linux/kernel/time/hrtimer.c: 1929
 * hrtimer_sleeper_start_expires - Start a hrtimer sleeper timer /Users/rubber/linux/kernel/time/hrtimer.c: 1945
 * @sl:		sleeper to be started /Users/rubber/linux/kernel/time/hrtimer.c: 1946
 * @mode:	timer mode abs/rel /Users/rubber/linux/kernel/time/hrtimer.c: 1947
 * Wrapper around hrtimer_start_expires() for hrtimer_sleeper based timers /Users/rubber/linux/kernel/time/hrtimer.c: 1949
 * to allow PREEMPT_RT to tweak the delivery mode (soft/hardirq context) /Users/rubber/linux/kernel/time/hrtimer.c: 1950
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1955
	 * Make the enqueue delivery mode check work on RT. If the sleeper /Users/rubber/linux/kernel/time/hrtimer.c: 1956
	 * was initialized for hard interrupt delivery, force the mode bit. /Users/rubber/linux/kernel/time/hrtimer.c: 1957
	 * This is a special case for hrtimer_sleepers because /Users/rubber/linux/kernel/time/hrtimer.c: 1958
	 * hrtimer_init_sleeper() determines the delivery mode on RT so the /Users/rubber/linux/kernel/time/hrtimer.c: 1959
	 * fiddling with this decision is avoided at the call sites. /Users/rubber/linux/kernel/time/hrtimer.c: 1960
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 1972
	 * On PREEMPT_RT enabled kernels hrtimers which are not explicitly /Users/rubber/linux/kernel/time/hrtimer.c: 1973
	 * marked for hard interrupt expiry mode are moved into soft /Users/rubber/linux/kernel/time/hrtimer.c: 1974
	 * interrupt context either for latency reasons or because the /Users/rubber/linux/kernel/time/hrtimer.c: 1975
	 * hrtimer callback takes regular spinlocks or invokes other /Users/rubber/linux/kernel/time/hrtimer.c: 1976
	 * functions which are not suitable for hard interrupt context on /Users/rubber/linux/kernel/time/hrtimer.c: 1977
	 * PREEMPT_RT. /Users/rubber/linux/kernel/time/hrtimer.c: 1978
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1979
	 * The hrtimer_sleeper callback is RT compatible in hard interrupt /Users/rubber/linux/kernel/time/hrtimer.c: 1980
	 * context, but there is a latency concern: Untrusted userspace can /Users/rubber/linux/kernel/time/hrtimer.c: 1981
	 * spawn many threads which arm timers for the same expiry time on /Users/rubber/linux/kernel/time/hrtimer.c: 1982
	 * the same CPU. That causes a latency spike due to the wakeup of /Users/rubber/linux/kernel/time/hrtimer.c: 1983
	 * a gazillion threads. /Users/rubber/linux/kernel/time/hrtimer.c: 1984
	 * /Users/rubber/linux/kernel/time/hrtimer.c: 1985
	 * OTOH, privileged real-time user space applications rely on the /Users/rubber/linux/kernel/time/hrtimer.c: 1986
	 * low latency of hard interrupt wakeups. If the current task is in /Users/rubber/linux/kernel/time/hrtimer.c: 1987
	 * a real-time scheduling class, mark the mode for hard interrupt /Users/rubber/linux/kernel/time/hrtimer.c: 1988
	 * expiry. /Users/rubber/linux/kernel/time/hrtimer.c: 1989
 * hrtimer_init_sleeper - initialize sleeper to the given clock /Users/rubber/linux/kernel/time/hrtimer.c: 2002
 * @sl:		sleeper to be initialized /Users/rubber/linux/kernel/time/hrtimer.c: 2003
 * @clock_id:	the clock to be used /Users/rubber/linux/kernel/time/hrtimer.c: 2004
 * @mode:	timer mode abs/rel /Users/rubber/linux/kernel/time/hrtimer.c: 2005
 * Functions related to boot-time initialization: /Users/rubber/linux/kernel/time/hrtimer.c: 2158
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 2198
		 * Mark it as ENQUEUED not INACTIVE otherwise the /Users/rubber/linux/kernel/time/hrtimer.c: 2199
		 * timer could be seen as !active and just vanish away /Users/rubber/linux/kernel/time/hrtimer.c: 2200
		 * under us on another CPU /Users/rubber/linux/kernel/time/hrtimer.c: 2201
		/* /Users/rubber/linux/kernel/time/hrtimer.c: 2205
		 * Enqueue the timers on the new cpu. This does not /Users/rubber/linux/kernel/time/hrtimer.c: 2206
		 * reprogram the event device in case the timer /Users/rubber/linux/kernel/time/hrtimer.c: 2207
		 * expires before the earliest on this CPU, but we run /Users/rubber/linux/kernel/time/hrtimer.c: 2208
		 * hrtimer_interrupt after we migrated everything to /Users/rubber/linux/kernel/time/hrtimer.c: 2209
		 * sort out already expired timers and reprogram the /Users/rubber/linux/kernel/time/hrtimer.c: 2210
		 * event device. /Users/rubber/linux/kernel/time/hrtimer.c: 2211
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 2225
	 * this BH disable ensures that raise_softirq_irqoff() does /Users/rubber/linux/kernel/time/hrtimer.c: 2226
	 * not wakeup ksoftirqd (and acquire the pi-lock) while /Users/rubber/linux/kernel/time/hrtimer.c: 2227
	 * holding the cpu_base lock /Users/rubber/linux/kernel/time/hrtimer.c: 2228
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 2234
	 * The caller is globally serialized and nobody else /Users/rubber/linux/kernel/time/hrtimer.c: 2235
	 * takes two locks at once, deadlock is not possible. /Users/rubber/linux/kernel/time/hrtimer.c: 2236
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 2246
	 * The migration might have changed the first expiring softirq /Users/rubber/linux/kernel/time/hrtimer.c: 2247
	 * timer on this CPU. Update it. /Users/rubber/linux/kernel/time/hrtimer.c: 2248
 * schedule_hrtimeout_range_clock - sleep until timeout /Users/rubber/linux/kernel/time/hrtimer.c: 2271
 * @expires:	timeout value (ktime_t) /Users/rubber/linux/kernel/time/hrtimer.c: 2272
 * @delta:	slack in expires timeout (ktime_t) /Users/rubber/linux/kernel/time/hrtimer.c: 2273
 * @mode:	timer mode /Users/rubber/linux/kernel/time/hrtimer.c: 2274
 * @clock_id:	timer clock to be used /Users/rubber/linux/kernel/time/hrtimer.c: 2275
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 2283
	 * Optimize when a zero timeout value is given. It does not /Users/rubber/linux/kernel/time/hrtimer.c: 2284
	 * matter whether this is an absolute or a relative time. /Users/rubber/linux/kernel/time/hrtimer.c: 2285
	/* /Users/rubber/linux/kernel/time/hrtimer.c: 2292
	 * A NULL parameter means "infinite" /Users/rubber/linux/kernel/time/hrtimer.c: 2293
 * schedule_hrtimeout_range - sleep until timeout /Users/rubber/linux/kernel/time/hrtimer.c: 2316
 * @expires:	timeout value (ktime_t) /Users/rubber/linux/kernel/time/hrtimer.c: 2317
 * @delta:	slack in expires timeout (ktime_t) /Users/rubber/linux/kernel/time/hrtimer.c: 2318
 * @mode:	timer mode /Users/rubber/linux/kernel/time/hrtimer.c: 2319
 * Make the current task sleep until the given expiry time has /Users/rubber/linux/kernel/time/hrtimer.c: 2321
 * elapsed. The routine will return immediately unless /Users/rubber/linux/kernel/time/hrtimer.c: 2322
 * the current task state has been set (see set_current_state()). /Users/rubber/linux/kernel/time/hrtimer.c: 2323
 * The @delta argument gives the kernel the freedom to schedule the /Users/rubber/linux/kernel/time/hrtimer.c: 2325
 * actual wakeup to a time that is both power and performance friendly. /Users/rubber/linux/kernel/time/hrtimer.c: 2326
 * The kernel give the normal best effort behavior for "@expires+@delta", /Users/rubber/linux/kernel/time/hrtimer.c: 2327
 * but may decide to fire the timer earlier, but no earlier than @expires. /Users/rubber/linux/kernel/time/hrtimer.c: 2328
 * You can set the task state as follows - /Users/rubber/linux/kernel/time/hrtimer.c: 2330
 * %TASK_UNINTERRUPTIBLE - at least @timeout time is guaranteed to /Users/rubber/linux/kernel/time/hrtimer.c: 2332
 * pass before the routine returns unless the current task is explicitly /Users/rubber/linux/kernel/time/hrtimer.c: 2333
 * woken up, (e.g. by wake_up_process()). /Users/rubber/linux/kernel/time/hrtimer.c: 2334
 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is /Users/rubber/linux/kernel/time/hrtimer.c: 2336
 * delivered to the current task or the current task is explicitly woken /Users/rubber/linux/kernel/time/hrtimer.c: 2337
 * up. /Users/rubber/linux/kernel/time/hrtimer.c: 2338
 * The current task state is guaranteed to be TASK_RUNNING when this /Users/rubber/linux/kernel/time/hrtimer.c: 2340
 * routine returns. /Users/rubber/linux/kernel/time/hrtimer.c: 2341
 * Returns 0 when the timer has expired. If the task was woken before the /Users/rubber/linux/kernel/time/hrtimer.c: 2343
 * timer expired by a signal (only possible in state TASK_INTERRUPTIBLE) or /Users/rubber/linux/kernel/time/hrtimer.c: 2344
 * by an explicit wakeup, it returns -EINTR. /Users/rubber/linux/kernel/time/hrtimer.c: 2345
 * schedule_hrtimeout - sleep until timeout /Users/rubber/linux/kernel/time/hrtimer.c: 2356
 * @expires:	timeout value (ktime_t) /Users/rubber/linux/kernel/time/hrtimer.c: 2357
 * @mode:	timer mode /Users/rubber/linux/kernel/time/hrtimer.c: 2358
 * Make the current task sleep until the given expiry time has /Users/rubber/linux/kernel/time/hrtimer.c: 2360
 * elapsed. The routine will return immediately unless /Users/rubber/linux/kernel/time/hrtimer.c: 2361
 * the current task state has been set (see set_current_state()). /Users/rubber/linux/kernel/time/hrtimer.c: 2362
 * You can set the task state as follows - /Users/rubber/linux/kernel/time/hrtimer.c: 2364
 * %TASK_UNINTERRUPTIBLE - at least @timeout time is guaranteed to /Users/rubber/linux/kernel/time/hrtimer.c: 2366
 * pass before the routine returns unless the current task is explicitly /Users/rubber/linux/kernel/time/hrtimer.c: 2367
 * woken up, (e.g. by wake_up_process()). /Users/rubber/linux/kernel/time/hrtimer.c: 2368
 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is /Users/rubber/linux/kernel/time/hrtimer.c: 2370
 * delivered to the current task or the current task is explicitly woken /Users/rubber/linux/kernel/time/hrtimer.c: 2371
 * up. /Users/rubber/linux/kernel/time/hrtimer.c: 2372
 * The current task state is guaranteed to be TASK_RUNNING when this /Users/rubber/linux/kernel/time/hrtimer.c: 2374
 * routine returns. /Users/rubber/linux/kernel/time/hrtimer.c: 2375
 * Returns 0 when the timer has expired. If the task was woken before the /Users/rubber/linux/kernel/time/hrtimer.c: 2377
 * timer expired by a signal (only possible in state TASK_INTERRUPTIBLE) or /Users/rubber/linux/kernel/time/hrtimer.c: 2378
 * by an explicit wakeup, it returns -EINTR. /Users/rubber/linux/kernel/time/hrtimer.c: 2379
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/time/namespace.c: 1
 * Author: Andrei Vagin <avagin@openvz.org> /Users/rubber/linux/kernel/time/namespace.c: 3
 * Author: Dmitry Safonov <dima@arista.com> /Users/rubber/linux/kernel/time/namespace.c: 4
	/* /Users/rubber/linux/kernel/time/namespace.c: 40
	 * Check that @tim value is in [offset, KTIME_MAX + offset] /Users/rubber/linux/kernel/time/namespace.c: 41
	 * and subtract offset. /Users/rubber/linux/kernel/time/namespace.c: 42
		/* /Users/rubber/linux/kernel/time/namespace.c: 45
		 * User can specify @tim *absolute* value - if it's lesser than /Users/rubber/linux/kernel/time/namespace.c: 46
		 * the time namespace's offset - it's already expired. /Users/rubber/linux/kernel/time/namespace.c: 47
 * clone_time_ns - Clone a time namespace /Users/rubber/linux/kernel/time/namespace.c: 70
 * @user_ns:	User namespace which owns a new namespace. /Users/rubber/linux/kernel/time/namespace.c: 71
 * @old_ns:	Namespace to clone /Users/rubber/linux/kernel/time/namespace.c: 72
 * Clone @old_ns and set the clone refcount to 1 /Users/rubber/linux/kernel/time/namespace.c: 74
 * Return: The new namespace or ERR_PTR. /Users/rubber/linux/kernel/time/namespace.c: 76
 * copy_time_ns - Create timens_for_children from @old_ns /Users/rubber/linux/kernel/time/namespace.c: 123
 * @flags:	Cloning flags /Users/rubber/linux/kernel/time/namespace.c: 124
 * @user_ns:	User namespace which owns a new namespace. /Users/rubber/linux/kernel/time/namespace.c: 125
 * @old_ns:	Namespace to clone /Users/rubber/linux/kernel/time/namespace.c: 126
 * If CLONE_NEWTIME specified in @flags, creates a new timens_for_children; /Users/rubber/linux/kernel/time/namespace.c: 128
 * adds a refcounter to @old_ns otherwise. /Users/rubber/linux/kernel/time/namespace.c: 129
 * Return: timens_for_children namespace or ERR_PTR. /Users/rubber/linux/kernel/time/namespace.c: 131
 * A time namespace VVAR page has the same layout as the VVAR page which /Users/rubber/linux/kernel/time/namespace.c: 153
 * contains the system wide VDSO data. /Users/rubber/linux/kernel/time/namespace.c: 154
 * For a normal task the VVAR pages are installed in the normal ordering: /Users/rubber/linux/kernel/time/namespace.c: 156
 *     VVAR /Users/rubber/linux/kernel/time/namespace.c: 157
 *     PVCLOCK /Users/rubber/linux/kernel/time/namespace.c: 158
 *     HVCLOCK /Users/rubber/linux/kernel/time/namespace.c: 159
 *     TIMENS   <- Not really required /Users/rubber/linux/kernel/time/namespace.c: 160
 * Now for a timens task the pages are installed in the following order: /Users/rubber/linux/kernel/time/namespace.c: 162
 *     TIMENS /Users/rubber/linux/kernel/time/namespace.c: 163
 *     PVCLOCK /Users/rubber/linux/kernel/time/namespace.c: 164
 *     HVCLOCK /Users/rubber/linux/kernel/time/namespace.c: 165
 *     VVAR /Users/rubber/linux/kernel/time/namespace.c: 166
 * The check for vdso_data->clock_mode is in the unlikely path of /Users/rubber/linux/kernel/time/namespace.c: 168
 * the seq begin magic. So for the non-timens case most of the time /Users/rubber/linux/kernel/time/namespace.c: 169
 * 'seq' is even, so the branch is not taken. /Users/rubber/linux/kernel/time/namespace.c: 170
 * If 'seq' is odd, i.e. a concurrent update is in progress, the extra check /Users/rubber/linux/kernel/time/namespace.c: 172
 * for vdso_data->clock_mode is a non-issue. The task is spin waiting for the /Users/rubber/linux/kernel/time/namespace.c: 173
 * update to finish and for 'seq' to become even anyway. /Users/rubber/linux/kernel/time/namespace.c: 174
 * Timens page has vdso_data->clock_mode set to VDSO_CLOCKMODE_TIMENS which /Users/rubber/linux/kernel/time/namespace.c: 176
 * enforces the time namespace handling path. /Users/rubber/linux/kernel/time/namespace.c: 177
 * Protects possibly multiple offsets writers racing each other /Users/rubber/linux/kernel/time/namespace.c: 196
 * and tasks entering the namespace. /Users/rubber/linux/kernel/time/namespace.c: 197
		/* /Users/rubber/linux/kernel/time/namespace.c: 402
		 * KTIME_SEC_MAX is divided by 2 to be sure that KTIME_MAX is /Users/rubber/linux/kernel/time/namespace.c: 403
		 * still unreachable. /Users/rubber/linux/kernel/time/namespace.c: 404
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/rcuscale.c: 1
 * Read-Copy Update module-based scalability-test facility /Users/rubber/linux/kernel/rcu/rcuscale.c: 3
 * Copyright (C) IBM Corporation, 2015 /Users/rubber/linux/kernel/rcu/rcuscale.c: 5
 * Authors: Paul E. McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/rcuscale.c: 7
 * The intended use cases for the nreaders and nwriters module parameters /Users/rubber/linux/kernel/rcu/rcuscale.c: 57
 * are as follows: /Users/rubber/linux/kernel/rcu/rcuscale.c: 58
 * 1.	Specify only the nr_cpus kernel boot parameter.  This will /Users/rubber/linux/kernel/rcu/rcuscale.c: 60
 *	set both nreaders and nwriters to the value specified by /Users/rubber/linux/kernel/rcu/rcuscale.c: 61
 *	nr_cpus for a mixed reader/writer test. /Users/rubber/linux/kernel/rcu/rcuscale.c: 62
 * 2.	Specify the nr_cpus kernel boot parameter, but set /Users/rubber/linux/kernel/rcu/rcuscale.c: 64
 *	rcuscale.nreaders to zero.  This will set nwriters to the /Users/rubber/linux/kernel/rcu/rcuscale.c: 65
 *	value specified by nr_cpus for an update-only test. /Users/rubber/linux/kernel/rcu/rcuscale.c: 66
 * 3.	Specify the nr_cpus kernel boot parameter, but set /Users/rubber/linux/kernel/rcu/rcuscale.c: 68
 *	rcuscale.nwriters to zero.  This will set nreaders to the /Users/rubber/linux/kernel/rcu/rcuscale.c: 69
 *	value specified by nr_cpus for a read-only test. /Users/rubber/linux/kernel/rcu/rcuscale.c: 70
 * Various other use cases may of course be specified. /Users/rubber/linux/kernel/rcu/rcuscale.c: 72
 * Note that this test's readers are intended only as a test load for /Users/rubber/linux/kernel/rcu/rcuscale.c: 74
 * the writers.  The reader scalability statistics will be overly /Users/rubber/linux/kernel/rcu/rcuscale.c: 75
 * pessimistic due to the per-critical-section interrupt disabling, /Users/rubber/linux/kernel/rcu/rcuscale.c: 76
 * test-end checks, and the pair of calls through pointers. /Users/rubber/linux/kernel/rcu/rcuscale.c: 77
 * Operations vector for selecting different types of tests. /Users/rubber/linux/kernel/rcu/rcuscale.c: 125
 * Definitions for rcu scalability testing. /Users/rubber/linux/kernel/rcu/rcuscale.c: 147
 * Definitions for srcu scalability testing. /Users/rubber/linux/kernel/rcu/rcuscale.c: 186
 * Definitions for RCU-tasks scalability testing. /Users/rubber/linux/kernel/rcu/rcuscale.c: 272
 * Definitions for RCU-tasks-trace scalability testing. /Users/rubber/linux/kernel/rcu/rcuscale.c: 299
 * If scalability tests complete, wait for shutdown to commence. /Users/rubber/linux/kernel/rcu/rcuscale.c: 335
 * RCU scalability reader kthread.  Repeatedly does empty RCU read-side /Users/rubber/linux/kernel/rcu/rcuscale.c: 347
 * critical section, minimizing update-side interference.  However, the /Users/rubber/linux/kernel/rcu/rcuscale.c: 348
 * point of this test is not to evaluate reader scalability, but instead /Users/rubber/linux/kernel/rcu/rcuscale.c: 349
 * to serve as a test load for update-side scalability testing. /Users/rubber/linux/kernel/rcu/rcuscale.c: 350
 * Callback function for asynchronous grace periods from rcu_scale_writer(). /Users/rubber/linux/kernel/rcu/rcuscale.c: 376
 * RCU scale writer kthread.  Repeatedly does a grace period. /Users/rubber/linux/kernel/rcu/rcuscale.c: 385
	/* /Users/rubber/linux/kernel/rcu/rcuscale.c: 407
	 * Wait until rcu_end_inkernel_boot() is called for normal GP tests /Users/rubber/linux/kernel/rcu/rcuscale.c: 408
	 * so that RCU is not always expedited for normal GP tests. /Users/rubber/linux/kernel/rcu/rcuscale.c: 409
	 * The system_state test is approximate, but works well in practice. /Users/rubber/linux/kernel/rcu/rcuscale.c: 410
	/* /Users/rubber/linux/kernel/rcu/rcuscale.c: 512
	 * Would like warning at start, but everything is expedited /Users/rubber/linux/kernel/rcu/rcuscale.c: 513
	 * during the mid-boot phase, so have to wait till the end. /Users/rubber/linux/kernel/rcu/rcuscale.c: 514
 * Return the number if non-negative.  If -1, the number of CPUs. /Users/rubber/linux/kernel/rcu/rcuscale.c: 587
 * If less than -1, that much less than the number of CPUs, but /Users/rubber/linux/kernel/rcu/rcuscale.c: 588
 * at least one. /Users/rubber/linux/kernel/rcu/rcuscale.c: 589
 * RCU scalability shutdown kthread.  Just waits to be awakened, then shuts /Users/rubber/linux/kernel/rcu/rcuscale.c: 606
 * down system. /Users/rubber/linux/kernel/rcu/rcuscale.c: 607
 * kfree_rcu() scalability tests: Start a kfree_rcu() loop on all CPUs for number /Users/rubber/linux/kernel/rcu/rcuscale.c: 621
 * of iterations and measure total time and number of GP for all iterations to complete. /Users/rubber/linux/kernel/rcu/rcuscale.c: 622
 By default kfree_rcu_test_single and kfree_rcu_test_double are /Users/rubber/linux/kernel/rcu/rcuscale.c: 678
 initialized to false. If both have the same value (false or true) /Users/rubber/linux/kernel/rcu/rcuscale.c: 679
 both are randomly tested, otherwise only the one with value true /Users/rubber/linux/kernel/rcu/rcuscale.c: 680
 is tested. /Users/rubber/linux/kernel/rcu/rcuscale.c: 681
 * shutdown kthread.  Just waits to be awakened, then shuts down system. /Users/rubber/linux/kernel/rcu/rcuscale.c: 734
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/srcutiny.c: 1
 * Sleepable Read-Copy Update mechanism for mutual exclusion, /Users/rubber/linux/kernel/rcu/srcutiny.c: 3
 *	tiny version for non-preemptible single-CPU use. /Users/rubber/linux/kernel/rcu/srcutiny.c: 4
 * Copyright (C) IBM Corporation, 2017 /Users/rubber/linux/kernel/rcu/srcutiny.c: 6
 * Author: Paul McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/srcutiny.c: 8
 * init_srcu_struct - initialize a sleep-RCU structure /Users/rubber/linux/kernel/rcu/srcutiny.c: 58
 * @ssp: structure to initialize. /Users/rubber/linux/kernel/rcu/srcutiny.c: 59
 * Must invoke this on a given srcu_struct before passing that srcu_struct /Users/rubber/linux/kernel/rcu/srcutiny.c: 61
 * to any other function.  Each srcu_struct represents a separate domain /Users/rubber/linux/kernel/rcu/srcutiny.c: 62
 * of SRCU protection. /Users/rubber/linux/kernel/rcu/srcutiny.c: 63
 * cleanup_srcu_struct - deconstruct a sleep-RCU structure /Users/rubber/linux/kernel/rcu/srcutiny.c: 74
 * @ssp: structure to clean up. /Users/rubber/linux/kernel/rcu/srcutiny.c: 75
 * Must invoke this after you are finished using a given srcu_struct that /Users/rubber/linux/kernel/rcu/srcutiny.c: 77
 * was initialized via init_srcu_struct(), else you leak memory. /Users/rubber/linux/kernel/rcu/srcutiny.c: 78
 * Removes the count for the old reader from the appropriate element of /Users/rubber/linux/kernel/rcu/srcutiny.c: 94
 * the srcu_struct. /Users/rubber/linux/kernel/rcu/srcutiny.c: 95
 * Workqueue handler to drive one grace period and invoke any callbacks /Users/rubber/linux/kernel/rcu/srcutiny.c: 108
 * that become ready as a result.  Single-CPU and !PREEMPTION operation /Users/rubber/linux/kernel/rcu/srcutiny.c: 109
 * means that we get away with murder on synchronization.  ;-) /Users/rubber/linux/kernel/rcu/srcutiny.c: 110
	/* /Users/rubber/linux/kernel/rcu/srcutiny.c: 146
	 * Enable rescheduling, and if there are more callbacks, /Users/rubber/linux/kernel/rcu/srcutiny.c: 147
	 * reschedule ourselves.  This can race with a call_srcu() /Users/rubber/linux/kernel/rcu/srcutiny.c: 148
	 * at interrupt level, but the ->srcu_gp_running checks will /Users/rubber/linux/kernel/rcu/srcutiny.c: 149
	 * straighten that out. /Users/rubber/linux/kernel/rcu/srcutiny.c: 150
 * Enqueue an SRCU callback on the specified srcu_struct structure, /Users/rubber/linux/kernel/rcu/srcutiny.c: 175
 * initiating grace-period processing if it is not already running. /Users/rubber/linux/kernel/rcu/srcutiny.c: 176
 * synchronize_srcu - wait for prior SRCU read-side critical-section completion /Users/rubber/linux/kernel/rcu/srcutiny.c: 194
 * get_state_synchronize_srcu - Provide an end-of-grace-period cookie /Users/rubber/linux/kernel/rcu/srcutiny.c: 209
 * start_poll_synchronize_srcu - Provide cookie and start grace period /Users/rubber/linux/kernel/rcu/srcutiny.c: 223
 * The difference between this and get_state_synchronize_srcu() is that /Users/rubber/linux/kernel/rcu/srcutiny.c: 225
 * this function ensures that the poll_state_synchronize_srcu() will /Users/rubber/linux/kernel/rcu/srcutiny.c: 226
 * eventually return the value true. /Users/rubber/linux/kernel/rcu/srcutiny.c: 227
 * poll_state_synchronize_srcu - Has cookie's grace period ended? /Users/rubber/linux/kernel/rcu/srcutiny.c: 239
 * Queue work for srcu_struct structures with early boot callbacks. /Users/rubber/linux/kernel/rcu/srcutiny.c: 257
 * The work won't actually execute until the workqueue initialization /Users/rubber/linux/kernel/rcu/srcutiny.c: 258
 * phase that takes place after the scheduler starts. /Users/rubber/linux/kernel/rcu/srcutiny.c: 259
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/tiny.c: 1
 * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition. /Users/rubber/linux/kernel/rcu/tiny.c: 3
 * Copyright IBM Corporation, 2008 /Users/rubber/linux/kernel/rcu/tiny.c: 5
 * Author: Paul E. McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/tiny.c: 7
 * For detailed explanation of Read-Copy Update mechanism see - /Users/rubber/linux/kernel/rcu/tiny.c: 9
 *		Documentation/RCU /Users/rubber/linux/kernel/rcu/tiny.c: 10
 * Check to see if the scheduling-clock interrupt came from an extended /Users/rubber/linux/kernel/rcu/tiny.c: 66
 * quiescent state, and, if so, tell RCU about it.  This function must /Users/rubber/linux/kernel/rcu/tiny.c: 67
 * be called from hardirq context.  It is normally called from the /Users/rubber/linux/kernel/rcu/tiny.c: 68
 * scheduling-clock interrupt. /Users/rubber/linux/kernel/rcu/tiny.c: 69
 * Reclaim the specified callback, either by invoking it for non-kfree cases or /Users/rubber/linux/kernel/rcu/tiny.c: 82
 * freeing it directly (for kfree). Return true if kfreeing, false otherwise. /Users/rubber/linux/kernel/rcu/tiny.c: 83
 * Wait for a grace period to elapse.  But it is illegal to invoke /Users/rubber/linux/kernel/rcu/tiny.c: 140
 * synchronize_rcu() from within an RCU read-side critical section. /Users/rubber/linux/kernel/rcu/tiny.c: 141
 * Therefore, any legal call to synchronize_rcu() is a quiescent /Users/rubber/linux/kernel/rcu/tiny.c: 142
 * state, and so on a UP system, synchronize_rcu() need do nothing. /Users/rubber/linux/kernel/rcu/tiny.c: 143
 * (But Lai Jiangshan points out the benefits of doing might_sleep() /Users/rubber/linux/kernel/rcu/tiny.c: 144
 * to reduce latency.) /Users/rubber/linux/kernel/rcu/tiny.c: 145
 * Cool, huh?  (Due to Josh Triplett.) /Users/rubber/linux/kernel/rcu/tiny.c: 147
 * Post an RCU callback to be invoked after the end of an RCU grace /Users/rubber/linux/kernel/rcu/tiny.c: 159
 * period.  But since we have but one CPU, that would be after any /Users/rubber/linux/kernel/rcu/tiny.c: 160
 * quiescent state. /Users/rubber/linux/kernel/rcu/tiny.c: 161
 * Return a grace-period-counter "cookie".  For more information, /Users/rubber/linux/kernel/rcu/tiny.c: 184
 * see the Tree RCU header comment. /Users/rubber/linux/kernel/rcu/tiny.c: 185
 * Return a grace-period-counter "cookie" and ensure that a future grace /Users/rubber/linux/kernel/rcu/tiny.c: 194
 * period completes.  For more information, see the Tree RCU header comment. /Users/rubber/linux/kernel/rcu/tiny.c: 195
 * Return true if the grace period corresponding to oldstate has completed /Users/rubber/linux/kernel/rcu/tiny.c: 210
 * and false otherwise.  For more information, see the Tree RCU header /Users/rubber/linux/kernel/rcu/tiny.c: 211
 * comment. /Users/rubber/linux/kernel/rcu/tiny.c: 212
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/update.c: 1
 * Read-Copy Update mechanism for mutual exclusion /Users/rubber/linux/kernel/rcu/update.c: 3
 * Copyright IBM Corporation, 2001 /Users/rubber/linux/kernel/rcu/update.c: 5
 * Authors: Dipankar Sarma <dipankar@in.ibm.com> /Users/rubber/linux/kernel/rcu/update.c: 7
 *	    Manfred Spraul <manfred@colorfullife.com> /Users/rubber/linux/kernel/rcu/update.c: 8
 * Based on the original work by Paul McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/update.c: 10
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen. /Users/rubber/linux/kernel/rcu/update.c: 11
 * Papers: /Users/rubber/linux/kernel/rcu/update.c: 12
 * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf /Users/rubber/linux/kernel/rcu/update.c: 13
 * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001) /Users/rubber/linux/kernel/rcu/update.c: 14
 * For detailed explanation of Read-Copy Update mechanism see - /Users/rubber/linux/kernel/rcu/update.c: 16
 *		http://lse.sourceforge.net/locking/rcupdate.html /Users/rubber/linux/kernel/rcu/update.c: 17
 * rcu_read_lock_held_common() - might we be in RCU-sched read-side critical section? /Users/rubber/linux/kernel/rcu/update.c: 67
 * @ret:	Best guess answer if lockdep cannot be relied on /Users/rubber/linux/kernel/rcu/update.c: 68
 * Returns true if lockdep must be ignored, in which case ``*ret`` contains /Users/rubber/linux/kernel/rcu/update.c: 70
 * the best guess described below.  Otherwise returns false, in which /Users/rubber/linux/kernel/rcu/update.c: 71
 * case ``*ret`` tells the caller nothing and the caller should instead /Users/rubber/linux/kernel/rcu/update.c: 72
 * consult lockdep. /Users/rubber/linux/kernel/rcu/update.c: 73
 * If CONFIG_DEBUG_LOCK_ALLOC is selected, set ``*ret`` to nonzero iff in an /Users/rubber/linux/kernel/rcu/update.c: 75
 * RCU-sched read-side critical section.  In absence of /Users/rubber/linux/kernel/rcu/update.c: 76
 * CONFIG_DEBUG_LOCK_ALLOC, this assumes we are in an RCU-sched read-side /Users/rubber/linux/kernel/rcu/update.c: 77
 * critical section unless it can prove otherwise.  Note that disabling /Users/rubber/linux/kernel/rcu/update.c: 78
 * of preemption (including disabling irqs) counts as an RCU-sched /Users/rubber/linux/kernel/rcu/update.c: 79
 * read-side critical section.  This is useful for debug checks in functions /Users/rubber/linux/kernel/rcu/update.c: 80
 * that required that they be called within an RCU-sched read-side /Users/rubber/linux/kernel/rcu/update.c: 81
 * critical section. /Users/rubber/linux/kernel/rcu/update.c: 82
 * Check debug_lockdep_rcu_enabled() to prevent false positives during boot /Users/rubber/linux/kernel/rcu/update.c: 84
 * and while lockdep is disabled. /Users/rubber/linux/kernel/rcu/update.c: 85
 * Note that if the CPU is in the idle loop from an RCU point of view (ie: /Users/rubber/linux/kernel/rcu/update.c: 87
 * that we are in the section between rcu_idle_enter() and rcu_idle_exit()) /Users/rubber/linux/kernel/rcu/update.c: 88
 * then rcu_read_lock_held() sets ``*ret`` to false even if the CPU did an /Users/rubber/linux/kernel/rcu/update.c: 89
 * rcu_read_lock().  The reason for this is that RCU ignores CPUs that are /Users/rubber/linux/kernel/rcu/update.c: 90
 * in such a section, considering these as in extended quiescent state, /Users/rubber/linux/kernel/rcu/update.c: 91
 * so such a CPU is effectively never in an RCU read-side critical section /Users/rubber/linux/kernel/rcu/update.c: 92
 * regardless of what RCU primitives it invokes.  This state of affairs is /Users/rubber/linux/kernel/rcu/update.c: 93
 * required --- we need to keep an RCU-free window in idle where the CPU may /Users/rubber/linux/kernel/rcu/update.c: 94
 * possibly enter into low power mode. This way we can notice an extended /Users/rubber/linux/kernel/rcu/update.c: 95
 * quiescent state to other CPUs that started a grace period. Otherwise /Users/rubber/linux/kernel/rcu/update.c: 96
 * we would delay any grace period as long as we run in the idle task. /Users/rubber/linux/kernel/rcu/update.c: 97
 * Similarly, we avoid claiming an RCU read lock held if the current /Users/rubber/linux/kernel/rcu/update.c: 99
 * CPU is offline. /Users/rubber/linux/kernel/rcu/update.c: 100
 * Should expedited grace-period primitives always fall back to their /Users/rubber/linux/kernel/rcu/update.c: 133
 * non-expedited counterparts?  Intended for use within RCU.  Note /Users/rubber/linux/kernel/rcu/update.c: 134
 * that if the user specifies both rcu_expedited and rcu_normal, then /Users/rubber/linux/kernel/rcu/update.c: 135
 * rcu_normal wins.  (Except during the time period during boot from /Users/rubber/linux/kernel/rcu/update.c: 136
 * when the first task is spawned until the rcu_set_runtime_mode() /Users/rubber/linux/kernel/rcu/update.c: 137
 * core_initcall() is invoked, at which point everything is expedited.) /Users/rubber/linux/kernel/rcu/update.c: 138
 * Should normal grace-period primitives be expedited?  Intended for /Users/rubber/linux/kernel/rcu/update.c: 150
 * use within RCU.  Note that this function takes the rcu_expedited /Users/rubber/linux/kernel/rcu/update.c: 151
 * sysfs/boot variable and rcu_scheduler_active into account as well /Users/rubber/linux/kernel/rcu/update.c: 152
 * as the rcu_expedite_gp() nesting.  So looping on rcu_unexpedite_gp() /Users/rubber/linux/kernel/rcu/update.c: 153
 * until rcu_gp_is_expedited() returns false is a -really- bad idea. /Users/rubber/linux/kernel/rcu/update.c: 154
 * rcu_expedite_gp - Expedite future RCU grace periods /Users/rubber/linux/kernel/rcu/update.c: 163
 * After a call to this function, future calls to synchronize_rcu() and /Users/rubber/linux/kernel/rcu/update.c: 165
 * friends act as the corresponding synchronize_rcu_expedited() function /Users/rubber/linux/kernel/rcu/update.c: 166
 * had instead been called. /Users/rubber/linux/kernel/rcu/update.c: 167
 * rcu_unexpedite_gp - Cancel prior rcu_expedite_gp() invocation /Users/rubber/linux/kernel/rcu/update.c: 176
 * Undo a prior call to rcu_expedite_gp().  If all prior calls to /Users/rubber/linux/kernel/rcu/update.c: 178
 * rcu_expedite_gp() are undone by a subsequent call to rcu_unexpedite_gp(), /Users/rubber/linux/kernel/rcu/update.c: 179
 * and if the rcu_expedited sysfs/boot parameter is not set, then all /Users/rubber/linux/kernel/rcu/update.c: 180
 * subsequent calls to synchronize_rcu() and friends will return to /Users/rubber/linux/kernel/rcu/update.c: 181
 * their normal non-expedited behavior. /Users/rubber/linux/kernel/rcu/update.c: 182
 * Inform RCU of the end of the in-kernel boot sequence. /Users/rubber/linux/kernel/rcu/update.c: 193
 * Let rcutorture know when it is OK to turn it up to eleven. /Users/rubber/linux/kernel/rcu/update.c: 204
 * Test each non-SRCU synchronous grace-period wait API.  This is /Users/rubber/linux/kernel/rcu/update.c: 215
 * useful just after a change in mode for these primitives, and /Users/rubber/linux/kernel/rcu/update.c: 216
 * during early boot. /Users/rubber/linux/kernel/rcu/update.c: 217
 * Switch to run-time mode once RCU has fully initialized. /Users/rubber/linux/kernel/rcu/update.c: 230
 Tell lockdep when RCU callbacks are being invoked. /Users/rubber/linux/kernel/rcu/update.c: 272
 * rcu_read_lock_held() - might we be in RCU read-side critical section? /Users/rubber/linux/kernel/rcu/update.c: 286
 * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an RCU /Users/rubber/linux/kernel/rcu/update.c: 288
 * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC, /Users/rubber/linux/kernel/rcu/update.c: 289
 * this assumes we are in an RCU read-side critical section unless it can /Users/rubber/linux/kernel/rcu/update.c: 290
 * prove otherwise.  This is useful for debug checks in functions that /Users/rubber/linux/kernel/rcu/update.c: 291
 * require that they be called within an RCU read-side critical section. /Users/rubber/linux/kernel/rcu/update.c: 292
 * Checks debug_lockdep_rcu_enabled() to prevent false positives during boot /Users/rubber/linux/kernel/rcu/update.c: 294
 * and while lockdep is disabled. /Users/rubber/linux/kernel/rcu/update.c: 295
 * Note that rcu_read_lock() and the matching rcu_read_unlock() must /Users/rubber/linux/kernel/rcu/update.c: 297
 * occur in the same context, for example, it is illegal to invoke /Users/rubber/linux/kernel/rcu/update.c: 298
 * rcu_read_unlock() in process context if the matching rcu_read_lock() /Users/rubber/linux/kernel/rcu/update.c: 299
 * was invoked from within an irq handler. /Users/rubber/linux/kernel/rcu/update.c: 300
 * Note that rcu_read_lock() is disallowed if the CPU is either idle or /Users/rubber/linux/kernel/rcu/update.c: 302
 * offline from an RCU perspective, so check for those as well. /Users/rubber/linux/kernel/rcu/update.c: 303
 * rcu_read_lock_bh_held() - might we be in RCU-bh read-side critical section? /Users/rubber/linux/kernel/rcu/update.c: 316
 * Check for bottom half being disabled, which covers both the /Users/rubber/linux/kernel/rcu/update.c: 318
 * CONFIG_PROVE_RCU and not cases.  Note that if someone uses /Users/rubber/linux/kernel/rcu/update.c: 319
 * rcu_read_lock_bh(), but then later enables BH, lockdep (if enabled) /Users/rubber/linux/kernel/rcu/update.c: 320
 * will show the situation.  This is useful for debug checks in functions /Users/rubber/linux/kernel/rcu/update.c: 321
 * that require that they be called within an RCU read-side critical /Users/rubber/linux/kernel/rcu/update.c: 322
 * section. /Users/rubber/linux/kernel/rcu/update.c: 323
 * Check debug_lockdep_rcu_enabled() to prevent false positives during boot. /Users/rubber/linux/kernel/rcu/update.c: 325
 * Note that rcu_read_lock_bh() is disallowed if the CPU is either idle or /Users/rubber/linux/kernel/rcu/update.c: 327
 * offline from an RCU perspective, so check for those as well. /Users/rubber/linux/kernel/rcu/update.c: 328
 * wakeme_after_rcu() - Callback function to awaken a task after grace period /Users/rubber/linux/kernel/rcu/update.c: 357
 * @head: Pointer to rcu_head member within rcu_synchronize structure /Users/rubber/linux/kernel/rcu/update.c: 358
 * Awaken the corresponding task now that a grace period has elapsed. /Users/rubber/linux/kernel/rcu/update.c: 360
 * init_rcu_head_on_stack() - initialize on-stack rcu_head for debugobjects /Users/rubber/linux/kernel/rcu/update.c: 429
 * @head: pointer to rcu_head structure to be initialized /Users/rubber/linux/kernel/rcu/update.c: 430
 * This function informs debugobjects of a new rcu_head structure that /Users/rubber/linux/kernel/rcu/update.c: 432
 * has been allocated as an auto variable on the stack.  This function /Users/rubber/linux/kernel/rcu/update.c: 433
 * is not required for rcu_head structures that are statically defined or /Users/rubber/linux/kernel/rcu/update.c: 434
 * that are dynamically allocated on the heap.  This function has no /Users/rubber/linux/kernel/rcu/update.c: 435
 * effect for !CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds. /Users/rubber/linux/kernel/rcu/update.c: 436
 * destroy_rcu_head_on_stack() - destroy on-stack rcu_head for debugobjects /Users/rubber/linux/kernel/rcu/update.c: 445
 * @head: pointer to rcu_head structure to be initialized /Users/rubber/linux/kernel/rcu/update.c: 446
 * This function informs debugobjects that an on-stack rcu_head structure /Users/rubber/linux/kernel/rcu/update.c: 448
 * is about to go out of scope.  As with init_rcu_head_on_stack(), this /Users/rubber/linux/kernel/rcu/update.c: 449
 * function is not required for rcu_head structures that are statically /Users/rubber/linux/kernel/rcu/update.c: 450
 * defined or that are dynamically allocated on the heap.  Also as with /Users/rubber/linux/kernel/rcu/update.c: 451
 * init_rcu_head_on_stack(), this function has no effect for /Users/rubber/linux/kernel/rcu/update.c: 452
 * !CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds. /Users/rubber/linux/kernel/rcu/update.c: 453
 !0 = suppress stall warnings. /Users/rubber/linux/kernel/rcu/update.c: 497
 Suppress boot-time RCU CPU stall warnings and rcutorture writer stall /Users/rubber/linux/kernel/rcu/update.c: 504
 warnings.  Also used by rcutorture even if stall warnings are excluded. /Users/rubber/linux/kernel/rcu/update.c: 505
 !0 = suppress boot stalls. /Users/rubber/linux/kernel/rcu/update.c: 506
 * Early boot self test parameters. /Users/rubber/linux/kernel/rcu/update.c: 513
 * Print any significant non-default boot-time settings. /Users/rubber/linux/kernel/rcu/update.c: 589
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/refscale.c: 1
 Scalability test comparing RCU vs other mechanisms /Users/rubber/linux/kernel/rcu/refscale.c: 3
 for acquiring references on objects. /Users/rubber/linux/kernel/rcu/refscale.c: 4
 Copyright (C) Google, 2020. /Users/rubber/linux/kernel/rcu/refscale.c: 6
 Author: Joel Fernandes <joel@joelfernandes.org> /Users/rubber/linux/kernel/rcu/refscale.c: 8
 Wait until there are multiple CPUs before starting test. /Users/rubber/linux/kernel/rcu/refscale.c: 74
 Number of loops per experiment, all readers execute operations concurrently. /Users/rubber/linux/kernel/rcu/refscale.c: 77
 Number of readers, with -1 defaulting to about 75% of the CPUs. /Users/rubber/linux/kernel/rcu/refscale.c: 79
 Number of runs. /Users/rubber/linux/kernel/rcu/refscale.c: 81
 Reader delay in nanoseconds, 0 for no delay. /Users/rubber/linux/kernel/rcu/refscale.c: 83
 Number of readers that are part of the current experiment. /Users/rubber/linux/kernel/rcu/refscale.c: 111
 Use to wait for all threads to start. /Users/rubber/linux/kernel/rcu/refscale.c: 114
 Track which experiment is currently running. /Users/rubber/linux/kernel/rcu/refscale.c: 120
 Operations vector for selecting different types of tests. /Users/rubber/linux/kernel/rcu/refscale.c: 123
 Definitions for SRCU ref scale testing. /Users/rubber/linux/kernel/rcu/refscale.c: 174
 Definitions for RCU Tasks ref scale testing: Empty read markers. /Users/rubber/linux/kernel/rcu/refscale.c: 208
 These definitions also work for RCU Rude readers. /Users/rubber/linux/kernel/rcu/refscale.c: 209
 Definitions for RCU Tasks Trace ref scale testing. /Users/rubber/linux/kernel/rcu/refscale.c: 233
 Definitions for reference count /Users/rubber/linux/kernel/rcu/refscale.c: 262
 Definitions for rwlock /Users/rubber/linux/kernel/rcu/refscale.c: 293
 Definitions for rwsem /Users/rubber/linux/kernel/rcu/refscale.c: 329
 Definitions for global spinlock /Users/rubber/linux/kernel/rcu/refscale.c: 365
 Definitions for global irq-save spinlock /Users/rubber/linux/kernel/rcu/refscale.c: 399
 Definitions acquire-release. /Users/rubber/linux/kernel/rcu/refscale.c: 434
 Reader kthread.  Repeatedly does empty RCU read-side /Users/rubber/linux/kernel/rcu/refscale.c: 512
 critical section, minimizing update-side interference. /Users/rubber/linux/kernel/rcu/refscale.c: 513
 Wait for signal that this reader can start. /Users/rubber/linux/kernel/rcu/refscale.c: 532
 Make sure that the CPU is affinitized appropriately during testing. /Users/rubber/linux/kernel/rcu/refscale.c: 539
 To reduce noise, do an initial cache-warming invocation, check /Users/rubber/linux/kernel/rcu/refscale.c: 550
 in, and then keep warming until everyone has checked in. /Users/rubber/linux/kernel/rcu/refscale.c: 551
 Also keep interrupts disabled.  This also has the effect /Users/rubber/linux/kernel/rcu/refscale.c: 556
 of preventing entries into slow path for rcu_read_unlock(). /Users/rubber/linux/kernel/rcu/refscale.c: 557
 To reduce runtime-skew noise, do maintain-load invocations until /Users/rubber/linux/kernel/rcu/refscale.c: 567
 everyone is done. /Users/rubber/linux/kernel/rcu/refscale.c: 568
 Print the results of each reader and return the sum of all their durations. /Users/rubber/linux/kernel/rcu/refscale.c: 598
 The main_func is the main orchestrator, it performs a bunch of /Users/rubber/linux/kernel/rcu/refscale.c: 632
 experiments.  For every experiment, it orders all the readers /Users/rubber/linux/kernel/rcu/refscale.c: 633
 involved to start and waits for them to finish the experiment. It /Users/rubber/linux/kernel/rcu/refscale.c: 634
 then reads their timestamps and starts the next experiment. Each /Users/rubber/linux/kernel/rcu/refscale.c: 635
 experiment progresses from 1 concurrent reader to N of them at which /Users/rubber/linux/kernel/rcu/refscale.c: 636
 point all the timestamps are printed. /Users/rubber/linux/kernel/rcu/refscale.c: 637
 Wait for all threads to start. /Users/rubber/linux/kernel/rcu/refscale.c: 659
 Start exp readers up per experiment /Users/rubber/linux/kernel/rcu/refscale.c: 664
 Print the average of all experiments /Users/rubber/linux/kernel/rcu/refscale.c: 698
 This will shutdown everything including us. /Users/rubber/linux/kernel/rcu/refscale.c: 721
 Wait for torture to stop us /Users/rubber/linux/kernel/rcu/refscale.c: 727
 Do scale-type-specific cleanup operations. /Users/rubber/linux/kernel/rcu/refscale.c: 769
 Shutdown kthread.  Just waits to be awakened, then shuts down system. /Users/rubber/linux/kernel/rcu/refscale.c: 776
 Wake before output. /Users/rubber/linux/kernel/rcu/refscale.c: 782
 Shutdown task /Users/rubber/linux/kernel/rcu/refscale.c: 822
 Reader tasks (default to ~75% of online CPUs). /Users/rubber/linux/kernel/rcu/refscale.c: 832
 Main Task /Users/rubber/linux/kernel/rcu/refscale.c: 860
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/rcutorture.c: 1
 * Read-Copy Update module-based torture test facility /Users/rubber/linux/kernel/rcu/rcutorture.c: 3
 * Copyright (C) IBM Corporation, 2005, 2006 /Users/rubber/linux/kernel/rcu/rcutorture.c: 5
 * Authors: Paul E. McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/rcutorture.c: 7
 *	  Josh Triplett <josh@joshtriplett.org> /Users/rubber/linux/kernel/rcu/rcutorture.c: 8
 * See also:  Documentation/RCU/torture.rst /Users/rubber/linux/kernel/rcu/rcutorture.c: 10
 Mailbox-like structure to check RCU global memory ordering. /Users/rubber/linux/kernel/rcu/rcutorture.c: 150
 Update-side data structure used to check RCU readers. /Users/rubber/linux/kernel/rcu/rcutorture.c: 159
 * Stop aggressive CPU-hog tests a bit before the end of the test in order /Users/rubber/linux/kernel/rcu/rcutorture.c: 264
 * to avoid interfering with test shutdown. /Users/rubber/linux/kernel/rcu/rcutorture.c: 265
 * Allocate an element from the rcu_tortures pool. /Users/rubber/linux/kernel/rcu/rcutorture.c: 284
 * Free an element to the rcu_tortures pool. /Users/rubber/linux/kernel/rcu/rcutorture.c: 305
 * Operations vector for selecting different types of tests. /Users/rubber/linux/kernel/rcu/rcutorture.c: 317
 * Definitions for rcu torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 355
	/* We want a short delay sometimes to make a reader delay the grace /Users/rubber/linux/kernel/rcu/rcutorture.c: 378
	 * period, and we want a long delay occasionally to trigger /Users/rubber/linux/kernel/rcu/rcutorture.c: 379
 * Update callback in the pipe.  This should be invoked after a grace period. /Users/rubber/linux/kernel/rcu/rcutorture.c: 411
 Pair with smp_load_acquire(). /Users/rubber/linux/kernel/rcu/rcutorture.c: 421
 * Update all callbacks in the pipe.  Suitable for synchronous grace-period /Users/rubber/linux/kernel/rcu/rcutorture.c: 436
 * primitives. /Users/rubber/linux/kernel/rcu/rcutorture.c: 437
 * Don't even think about trying any of these in real life!!! /Users/rubber/linux/kernel/rcu/rcutorture.c: 516
 * The names includes "busted", and they really means it! /Users/rubber/linux/kernel/rcu/rcutorture.c: 517
 * The only purpose of these functions is to provide a buggy RCU /Users/rubber/linux/kernel/rcu/rcutorture.c: 518
 * implementation to make sure that rcutorture correctly emits /Users/rubber/linux/kernel/rcu/rcutorture.c: 519
 * buggy-RCU error messages. /Users/rubber/linux/kernel/rcu/rcutorture.c: 520
 * Definitions for srcu torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 560
 * Definitions for RCU-tasks torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 728
 * Definitions for trivial CONFIG_PREEMPT=n-only torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 771
 * This implementation does not necessarily work well with CPU hotplug. /Users/rubber/linux/kernel/rcu/rcutorture.c: 772
 * Definitions for rude RCU-tasks torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 813
 * Definitions for tracing RCU-tasks torture testing. /Users/rubber/linux/kernel/rcu/rcutorture.c: 841
 * RCU torture priority-boost testing.  Runs one real-time thread per /Users/rubber/linux/kernel/rcu/rcutorture.c: 889
 * CPU for moderate bursts, repeatedly starting grace periods and waiting /Users/rubber/linux/kernel/rcu/rcutorture.c: 890
 * for them to complete.  If a given grace period takes too long, we assume /Users/rubber/linux/kernel/rcu/rcutorture.c: 891
 * that priority inversion has occurred. /Users/rubber/linux/kernel/rcu/rcutorture.c: 892
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 899
	 * Disable RT throttling so that rcutorture's boost threads don't get /Users/rubber/linux/kernel/rcu/rcutorture.c: 900
	 * throttled. Only possible if rcutorture is built-in otherwise the /Users/rubber/linux/kernel/rcu/rcutorture.c: 901
	 * user should manually do this by setting the sched_rt_period_us and /Users/rubber/linux/kernel/rcu/rcutorture.c: 902
	 * sched_rt_runtime sysctls. /Users/rubber/linux/kernel/rcu/rcutorture.c: 903
 Recheck after checking time to avoid false positives. /Users/rubber/linux/kernel/rcu/rcutorture.c: 933
 Time check before grace-period check. /Users/rubber/linux/kernel/rcu/rcutorture.c: 934
 passed, though perhaps just barely /Users/rubber/linux/kernel/rcu/rcutorture.c: 936
 At most one persisted message per boost test. /Users/rubber/linux/kernel/rcu/rcutorture.c: 938
 passed on a technicality /Users/rubber/linux/kernel/rcu/rcutorture.c: 943
 Recheck after print to flag grace period ending during splat. /Users/rubber/linux/kernel/rcu/rcutorture.c: 951
 failed /Users/rubber/linux/kernel/rcu/rcutorture.c: 958
 passed /Users/rubber/linux/kernel/rcu/rcutorture.c: 963
 Test failed already in this test interval /Users/rubber/linux/kernel/rcu/rcutorture.c: 980
 Do one boost-test interval. /Users/rubber/linux/kernel/rcu/rcutorture.c: 996
 Has current GP gone too long? /Users/rubber/linux/kernel/rcu/rcutorture.c: 999
 If we don't have a grace period in flight, start one. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1002
 If the grace period already ended, /Users/rubber/linux/kernel/rcu/rcutorture.c: 1010
 we don't know when that happened, so /Users/rubber/linux/kernel/rcu/rcutorture.c: 1011
 start over. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1012
 In case the grace period extended beyond the end of the loop. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1020
		/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 1024
		 * Set the start time of the next test interval. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1025
		 * Yes, this is vulnerable to long delays, but such /Users/rubber/linux/kernel/rcu/rcutorture.c: 1026
		 * delays simply cause a false negative for the next /Users/rubber/linux/kernel/rcu/rcutorture.c: 1027
		 * interval.  Besides, we are running at RT priority, /Users/rubber/linux/kernel/rcu/rcutorture.c: 1028
		 * so delays should be relatively rare. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1029
 * RCU torture force-quiescent-state kthread.  Repeatedly induces /Users/rubber/linux/kernel/rcu/rcutorture.c: 1058
 * bursts of calls to force_quiescent_state(), increasing the probability /Users/rubber/linux/kernel/rcu/rcutorture.c: 1059
 * of occurrence of some important types of race conditions. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1060
 Used by writers to randomly choose from the available grace-period /Users/rubber/linux/kernel/rcu/rcutorture.c: 1090
 primitives.  The only purpose of the initialization is to size the array. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1091
 * Determine which grace-period primitives are available. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1096
 * RCU torture writer kthread.  Repeatedly substitutes a new structure /Users/rubber/linux/kernel/rcu/rcutorture.c: 1139
 * for that pointed to by rcu_torture_current, freeing the old structure /Users/rubber/linux/kernel/rcu/rcutorture.c: 1140
 * after a series of grace periods (the "pipeline"). /Users/rubber/linux/kernel/rcu/rcutorture.c: 1141
		/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 1166
		 * No updates primitives, so don't try updating. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1167
		 * The resulting test won't be testing much, hence the /Users/rubber/linux/kernel/rcu/rcutorture.c: 1168
		 * above WARN_ONCE(). /Users/rubber/linux/kernel/rcu/rcutorture.c: 1169
 Let stats task know that we are done. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1280
 * RCU torture fake writer kthread.  Repeatedly calls sync, with a random /Users/rubber/linux/kernel/rcu/rcutorture.c: 1297
 * delay between calls. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1298
 Set up and carry out testing of RCU's global memory ordering /Users/rubber/linux/kernel/rcu/rcutorture.c: 1353
 Me. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1361
 Assigned us to do checking. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1362
 Reader being checked. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1363
 Reader doing checking when not me. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1364
 Don't try this from timer handlers. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1367
 Increment my counter. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1369
 Attempt to assign someone else some checking work. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1373
 Pairs with smp_store_release below. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1379
 Pairs with smp_store_release below. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1381
 This gets set after the grace period ends. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1385
 Back out. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1388
 If assigned some completed work, do it! /Users/rubber/linux/kernel/rcu/rcutorture.c: 1391
 No work or work not yet ready. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1394
 Someone else can assign us work. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1405
 Assigner can again assign. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1406
 * Do one extension of an RCU read-side critical section using the /Users/rubber/linux/kernel/rcu/rcutorture.c: 1410
 * current reader state in readstate (set to zero for initial entry /Users/rubber/linux/kernel/rcu/rcutorture.c: 1411
 * to extended critical section), set the new state as specified by /Users/rubber/linux/kernel/rcu/rcutorture.c: 1412
 * newstate (set to zero for final exit from extended critical section), /Users/rubber/linux/kernel/rcu/rcutorture.c: 1413
 * and random-number-generator state in trsp.  If this is neither the /Users/rubber/linux/kernel/rcu/rcutorture.c: 1414
 * beginning or end of the critical section and if there was actually a /Users/rubber/linux/kernel/rcu/rcutorture.c: 1415
 * change, do a ->read_delay(). /Users/rubber/linux/kernel/rcu/rcutorture.c: 1416
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 1446
	 * Next, remove old protection, in decreasing order of strength /Users/rubber/linux/kernel/rcu/rcutorture.c: 1447
	 * to avoid unlock paths that aren't safe in the stronger /Users/rubber/linux/kernel/rcu/rcutorture.c: 1448
	 * context. Namely: BH can not be enabled with disabled interrupts. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1449
	 * Additionally PREEMPT_RT requires that BH is enabled in preemptible /Users/rubber/linux/kernel/rcu/rcutorture.c: 1450
	 * context. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1451
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 1516
	 * Can't enable bh w/irq disabled. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1517
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 1522
	 * Ideally these sequences would be detected in debug builds /Users/rubber/linux/kernel/rcu/rcutorture.c: 1523
	 * (regardless of RT), but until then don't stop testing /Users/rubber/linux/kernel/rcu/rcutorture.c: 1524
	 * them on non-RT. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1525
 * Do a randomly selected number of extensions of an existing RCU read-side /Users/rubber/linux/kernel/rcu/rcutorture.c: 1539
 * critical section. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1540
 * Do one read-side critical section, returning false if there was /Users/rubber/linux/kernel/rcu/rcutorture.c: 1564
 * no data to read.  Can be invoked both from process context and /Users/rubber/linux/kernel/rcu/rcutorture.c: 1565
 * from a timer handler. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1566
 This next splat is expected behavior if leakpointer, especially /Users/rubber/linux/kernel/rcu/rcutorture.c: 1630
 for CONFIG_RCU_STRICT_GRACE_PERIOD=y kernels. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1631
 * RCU torture reader from timer handler.  Dereferences rcu_torture_current, /Users/rubber/linux/kernel/rcu/rcutorture.c: 1648
 * incrementing the corresponding element of the pipeline array.  The /Users/rubber/linux/kernel/rcu/rcutorture.c: 1649
 * counter in the element should never be greater than 1, otherwise, the /Users/rubber/linux/kernel/rcu/rcutorture.c: 1650
 * RCU implementation is broken. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1651
 * RCU torture reader kthread.  Repeatedly dereferences rcu_torture_current, /Users/rubber/linux/kernel/rcu/rcutorture.c: 1668
 * incrementing the corresponding element of the pipeline array.  The /Users/rubber/linux/kernel/rcu/rcutorture.c: 1669
 * counter in the element should never be greater than 1, otherwise, the /Users/rubber/linux/kernel/rcu/rcutorture.c: 1670
 * RCU implementation is broken. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1671
 * Randomly Toggle CPUs' callback-offload state.  This uses hrtimers to /Users/rubber/linux/kernel/rcu/rcutorture.c: 1712
 * increase race probabilities and fuzzes the interval between toggling. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1713
 * Print torture statistics.  Caller must ensure that there is only /Users/rubber/linux/kernel/rcu/rcutorture.c: 1759
 * one call to this function at a given time!!!  This is normally /Users/rubber/linux/kernel/rcu/rcutorture.c: 1760
 * accomplished by relying on the module system to only have one copy /Users/rubber/linux/kernel/rcu/rcutorture.c: 1761
 * of the module loaded, and then by giving the rcu_torture_stats /Users/rubber/linux/kernel/rcu/rcutorture.c: 1762
 * kthread full control (or the init/cleanup functions when rcu_torture_stats /Users/rubber/linux/kernel/rcu/rcutorture.c: 1763
 * thread is not running). /Users/rubber/linux/kernel/rcu/rcutorture.c: 1764
 Statistic. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1814
 rcu_barrier() /Users/rubber/linux/kernel/rcu/rcutorture.c: 1828
 no boost kthread /Users/rubber/linux/kernel/rcu/rcutorture.c: 1829
 can't set RT prio /Users/rubber/linux/kernel/rcu/rcutorture.c: 1830
 boost failed (TIMER_SOFTIRQ RT prio?) /Users/rubber/linux/kernel/rcu/rcutorture.c: 1831
 Too-short grace period /Users/rubber/linux/kernel/rcu/rcutorture.c: 1832
 * Periodically prints torture statistics, if periodic statistics printing /Users/rubber/linux/kernel/rcu/rcutorture.c: 1882
 * was specified via the stat_interval module parameter. /Users/rubber/linux/kernel/rcu/rcutorture.c: 1883
 * CPU-stall kthread.  It waits as specified by stall_cpu_holdoff, then /Users/rubber/linux/kernel/rcu/rcutorture.c: 2013
 * induces a CPU stall for the time specified by stall_cpu. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2014
 * Forward-progress self-propagating RCU callback function.  Because /Users/rubber/linux/kernel/rcu/rcutorture.c: 2084
 * callbacks run from softirq, this function is an implicit RCU read-side /Users/rubber/linux/kernel/rcu/rcutorture.c: 2085
 * critical section. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2086
 Give the scheduler a chance, even on nohz_full CPUs. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2180
 Real call_rcu() floods hit userspace, so emulate that. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2184
 No userspace emulation: CB invocation throttles call_rcu() /Users/rubber/linux/kernel/rcu/rcutorture.c: 2189
 * Free all callbacks on the rcu_fwd_cb_head list, either because the /Users/rubber/linux/kernel/rcu/rcutorture.c: 2194
 * test is over or because we hit an OOM event. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2195
 Cannot do need_resched() forward progress testing without ->sync. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2242
 Hoist initialization for multi-kthread /Users/rubber/linux/kernel/rcu/rcutorture.c: 2323
 * OOM notifier, but this only prints diagnostic information for the /Users/rubber/linux/kernel/rcu/rcutorture.c: 2391
 * current forward-progress test. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2392
		/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 2543
		 * The above smp_load_acquire() ensures barrier_phase load /Users/rubber/linux/kernel/rcu/rcutorture.c: 2544
		 * is ordered before the following ->call(). /Users/rubber/linux/kernel/rcu/rcutorture.c: 2545
 IPI failed, so use direct call from current CPU. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2549
 Wait manually for the remaining callbacks /Users/rubber/linux/kernel/rcu/rcutorture.c: 2588
 Can't trust ordering if broken. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2598
 Child kthread which just does an rcutorture reader and exits. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2697
 Minimize time between reading and exiting. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2703
 Parent kthread which creates and destroys read-exit child kthreads. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2710
 Allocate and initialize. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2719
 Each pass through this loop does one read-exit episode. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2723
 Wait for task_struct free, avoid OOM. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2727
 Spawn child. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2739
 Clean up and exit. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2755
 After reaping. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2756
 Store before wakeup. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2757
 Above write before wait. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2781
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 2849
	 * Wait for all RCU callbacks to fire, then do torture-type-specific /Users/rubber/linux/kernel/rcu/rcutorture.c: 2850
	 * cleanup operations. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2851
	/* /Users/rubber/linux/kernel/rcu/rcutorture.c: 2906
	 * This -might- happen due to race conditions, but is unlikely. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2907
	 * The scenario that leads to this happening is that the /Users/rubber/linux/kernel/rcu/rcutorture.c: 2908
	 * first of the pair of duplicate callbacks is queued, /Users/rubber/linux/kernel/rcu/rcutorture.c: 2909
	 * someone else starts a grace period that includes that /Users/rubber/linux/kernel/rcu/rcutorture.c: 2910
	 * callback, then the second of the pair must wait for the /Users/rubber/linux/kernel/rcu/rcutorture.c: 2911
	 * next grace period.  Unlikely, but can happen.  If it /Users/rubber/linux/kernel/rcu/rcutorture.c: 2912
	 * does happen, the debug-objects subsystem won't have splatted. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2913
 * Verify that double-free causes debug-objects to complain, but only /Users/rubber/linux/kernel/rcu/rcutorture.c: 2920
 * if CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.  Otherwise, say that the test /Users/rubber/linux/kernel/rcu/rcutorture.c: 2921
 * cannot be carried out. /Users/rubber/linux/kernel/rcu/rcutorture.c: 2922
 Testing RCU priority boosting requires rcutorture do /Users/rubber/linux/kernel/rcu/rcutorture.c: 3163
 some serious abuse.  Counter this by running ksoftirqd /Users/rubber/linux/kernel/rcu/rcutorture.c: 3164
 at higher priority. /Users/rubber/linux/kernel/rcu/rcutorture.c: 3165
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/srcutree.c: 1
 * Sleepable Read-Copy Update mechanism for mutual exclusion. /Users/rubber/linux/kernel/rcu/srcutree.c: 3
 * Copyright (C) IBM Corporation, 2006 /Users/rubber/linux/kernel/rcu/srcutree.c: 5
 * Copyright (C) Fujitsu, 2012 /Users/rubber/linux/kernel/rcu/srcutree.c: 6
 * Authors: Paul McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/srcutree.c: 8
 *	   Lai Jiangshan <laijs@cn.fujitsu.com> /Users/rubber/linux/kernel/rcu/srcutree.c: 9
 * For detailed explanation of Read-Copy Update mechanism see - /Users/rubber/linux/kernel/rcu/srcutree.c: 11
 *		Documentation/RCU/ *.txt /Users/rubber/linux/kernel/rcu/srcutree.c: 12
 * Initialize SRCU combining tree.  Note that statically allocated /Users/rubber/linux/kernel/rcu/srcutree.c: 78
 * srcu_struct structures might already have srcu_read_lock() and /Users/rubber/linux/kernel/rcu/srcutree.c: 79
 * srcu_read_unlock() running against them.  So if the is_static parameter /Users/rubber/linux/kernel/rcu/srcutree.c: 80
 * is set, don't initialize ->srcu_lock_count[] and ->srcu_unlock_count[]. /Users/rubber/linux/kernel/rcu/srcutree.c: 81
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 128
	 * Initialize the per-CPU srcu_data array, which feeds into the /Users/rubber/linux/kernel/rcu/srcutree.c: 129
	 * leaves of the srcu_node tree. /Users/rubber/linux/kernel/rcu/srcutree.c: 130
 * Initialize non-compile-time initialized fields, including the /Users/rubber/linux/kernel/rcu/srcutree.c: 158
 * associated srcu_node and srcu_data structures.  The is_static /Users/rubber/linux/kernel/rcu/srcutree.c: 159
 * parameter is passed through to init_srcu_struct_nodes(), and /Users/rubber/linux/kernel/rcu/srcutree.c: 160
 * also tells us that ->sda has already been wired up to srcu_data. /Users/rubber/linux/kernel/rcu/srcutree.c: 161
 * init_srcu_struct - initialize a sleep-RCU structure /Users/rubber/linux/kernel/rcu/srcutree.c: 200
 * @ssp: structure to initialize. /Users/rubber/linux/kernel/rcu/srcutree.c: 201
 * Must invoke this on a given srcu_struct before passing that srcu_struct /Users/rubber/linux/kernel/rcu/srcutree.c: 203
 * to any other function.  Each srcu_struct represents a separate domain /Users/rubber/linux/kernel/rcu/srcutree.c: 204
 * of SRCU protection. /Users/rubber/linux/kernel/rcu/srcutree.c: 205
 * First-use initialization of statically allocated srcu_struct /Users/rubber/linux/kernel/rcu/srcutree.c: 217
 * structure.  Wiring up the combining tree is more than can be /Users/rubber/linux/kernel/rcu/srcutree.c: 218
 * done with compile-time initialization, so this check is added /Users/rubber/linux/kernel/rcu/srcutree.c: 219
 * to each update-side SRCU primitive.  Use ssp->lock, which -is- /Users/rubber/linux/kernel/rcu/srcutree.c: 220
 * compile-time initialized, to resolve races involving multiple /Users/rubber/linux/kernel/rcu/srcutree.c: 221
 * CPUs trying to garner first-use privileges. /Users/rubber/linux/kernel/rcu/srcutree.c: 222
 * Returns approximate total of the readers' ->srcu_lock_count[] values /Users/rubber/linux/kernel/rcu/srcutree.c: 241
 * for the rank of per-CPU counters specified by idx. /Users/rubber/linux/kernel/rcu/srcutree.c: 242
 * Returns approximate total of the readers' ->srcu_unlock_count[] values /Users/rubber/linux/kernel/rcu/srcutree.c: 258
 * for the rank of per-CPU counters specified by idx. /Users/rubber/linux/kernel/rcu/srcutree.c: 259
 * Return true if the number of pre-existing readers is determined to /Users/rubber/linux/kernel/rcu/srcutree.c: 275
 * be zero. /Users/rubber/linux/kernel/rcu/srcutree.c: 276
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 284
	 * Make sure that a lock is always counted if the corresponding /Users/rubber/linux/kernel/rcu/srcutree.c: 285
	 * unlock is counted. Needs to be a smp_mb() as the read side may /Users/rubber/linux/kernel/rcu/srcutree.c: 286
	 * contain a read from a variable that is written to before the /Users/rubber/linux/kernel/rcu/srcutree.c: 287
	 * synchronize_srcu() in the write side. In this case smp_mb()s /Users/rubber/linux/kernel/rcu/srcutree.c: 288
	 * A and B act like the store buffering pattern. /Users/rubber/linux/kernel/rcu/srcutree.c: 289
	 * /Users/rubber/linux/kernel/rcu/srcutree.c: 290
	 * This smp_mb() also pairs with smp_mb() C to prevent accesses /Users/rubber/linux/kernel/rcu/srcutree.c: 291
	 * after the synchronize_srcu() from being executed before the /Users/rubber/linux/kernel/rcu/srcutree.c: 292
	 * grace period ends. /Users/rubber/linux/kernel/rcu/srcutree.c: 293
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 297
	 * If the locks are the same as the unlocks, then there must have /Users/rubber/linux/kernel/rcu/srcutree.c: 298
	 * been no readers on this index at some time in between. This does /Users/rubber/linux/kernel/rcu/srcutree.c: 299
	 * not mean that there are no more readers, as one could have read /Users/rubber/linux/kernel/rcu/srcutree.c: 300
	 * the current index but not have incremented the lock counter yet. /Users/rubber/linux/kernel/rcu/srcutree.c: 301
	 * /Users/rubber/linux/kernel/rcu/srcutree.c: 302
	 * So suppose that the updater is preempted here for so long /Users/rubber/linux/kernel/rcu/srcutree.c: 303
	 * that more than ULONG_MAX non-nested readers come and go in /Users/rubber/linux/kernel/rcu/srcutree.c: 304
	 * the meantime.  It turns out that this cannot result in overflow /Users/rubber/linux/kernel/rcu/srcutree.c: 305
	 * because if a reader modifies its unlock count after we read it /Users/rubber/linux/kernel/rcu/srcutree.c: 306
	 * above, then that reader's next load of ->srcu_idx is guaranteed /Users/rubber/linux/kernel/rcu/srcutree.c: 307
	 * to get the new value, which will cause it to operate on the /Users/rubber/linux/kernel/rcu/srcutree.c: 308
	 * other bank of counters, where it cannot contribute to the /Users/rubber/linux/kernel/rcu/srcutree.c: 309
	 * overflow of these counters.  This means that there is a maximum /Users/rubber/linux/kernel/rcu/srcutree.c: 310
	 * of 2*NR_CPUS increments, which cannot overflow given current /Users/rubber/linux/kernel/rcu/srcutree.c: 311
	 * systems, especially not on 64-bit systems. /Users/rubber/linux/kernel/rcu/srcutree.c: 312
	 * /Users/rubber/linux/kernel/rcu/srcutree.c: 313
	 * OK, how about nesting?  This does impose a limit on nesting /Users/rubber/linux/kernel/rcu/srcutree.c: 314
	 * of floor(ULONG_MAX/NR_CPUS/2), which should be sufficient, /Users/rubber/linux/kernel/rcu/srcutree.c: 315
	 * especially on 64-bit systems. /Users/rubber/linux/kernel/rcu/srcutree.c: 316
 * srcu_readers_active - returns true if there are readers. and false /Users/rubber/linux/kernel/rcu/srcutree.c: 322
 *                       otherwise /Users/rubber/linux/kernel/rcu/srcutree.c: 323
 * @ssp: which srcu_struct to count active readers (holding srcu_read_lock). /Users/rubber/linux/kernel/rcu/srcutree.c: 324
 * Note that this is not an atomic primitive, and can therefore suffer /Users/rubber/linux/kernel/rcu/srcutree.c: 326
 * severe errors when invoked on an active srcu_struct.  That said, it /Users/rubber/linux/kernel/rcu/srcutree.c: 327
 * can be useful as an error check at cleanup time. /Users/rubber/linux/kernel/rcu/srcutree.c: 328
 * Return grace-period delay, zero if there are expedited grace /Users/rubber/linux/kernel/rcu/srcutree.c: 349
 * periods pending, SRCU_INTERVAL otherwise. /Users/rubber/linux/kernel/rcu/srcutree.c: 350
 * cleanup_srcu_struct - deconstruct a sleep-RCU structure /Users/rubber/linux/kernel/rcu/srcutree.c: 361
 * @ssp: structure to clean up. /Users/rubber/linux/kernel/rcu/srcutree.c: 362
 * Must invoke this after you are finished using a given srcu_struct that /Users/rubber/linux/kernel/rcu/srcutree.c: 364
 * was initialized via init_srcu_struct(), else you leak memory. /Users/rubber/linux/kernel/rcu/srcutree.c: 365
 * Counts the new reader in the appropriate per-CPU element of the /Users/rubber/linux/kernel/rcu/srcutree.c: 396
 * srcu_struct. /Users/rubber/linux/kernel/rcu/srcutree.c: 397
 * Returns an index that must be passed to the matching srcu_read_unlock(). /Users/rubber/linux/kernel/rcu/srcutree.c: 398
 Avoid leaking the critical section. * /Users/rubber/linux/kernel/rcu/srcutree.c: 406
 * Removes the count for the old reader from the appropriate per-CPU /Users/rubber/linux/kernel/rcu/srcutree.c: 412
 * element of the srcu_struct.  Note that this may well be a different /Users/rubber/linux/kernel/rcu/srcutree.c: 413
 * CPU than that which was incremented by the corresponding srcu_read_lock(). /Users/rubber/linux/kernel/rcu/srcutree.c: 414
 Avoid leaking the critical section. * /Users/rubber/linux/kernel/rcu/srcutree.c: 418
 * We use an adaptive strategy for synchronize_srcu() and especially for /Users/rubber/linux/kernel/rcu/srcutree.c: 424
 * synchronize_srcu_expedited().  We spin for a fixed time period /Users/rubber/linux/kernel/rcu/srcutree.c: 425
 * (defined below) to allow SRCU readers to exit their read-side critical /Users/rubber/linux/kernel/rcu/srcutree.c: 426
 * sections.  If there are still some readers after a few microseconds, /Users/rubber/linux/kernel/rcu/srcutree.c: 427
 * we repeatedly block for 1-millisecond time periods. /Users/rubber/linux/kernel/rcu/srcutree.c: 428
 * Start an SRCU grace period. /Users/rubber/linux/kernel/rcu/srcutree.c: 433
 * Schedule callback invocation for the specified srcu_data structure, /Users/rubber/linux/kernel/rcu/srcutree.c: 474
 * if possible, on the corresponding CPU. /Users/rubber/linux/kernel/rcu/srcutree.c: 475
 * Schedule callback invocation for all srcu_data structures associated /Users/rubber/linux/kernel/rcu/srcutree.c: 483
 * with the specified srcu_node structure that have callbacks for the /Users/rubber/linux/kernel/rcu/srcutree.c: 484
 * just-completed grace period, the one corresponding to idx.  If possible, /Users/rubber/linux/kernel/rcu/srcutree.c: 485
 * schedule this invocation on the corresponding CPUs. /Users/rubber/linux/kernel/rcu/srcutree.c: 486
 * Note the end of an SRCU grace period.  Initiates callback invocation /Users/rubber/linux/kernel/rcu/srcutree.c: 501
 * and starts a new grace period if needed. /Users/rubber/linux/kernel/rcu/srcutree.c: 502
 * The ->srcu_cb_mutex acquisition does not protect any data, but /Users/rubber/linux/kernel/rcu/srcutree.c: 504
 * instead prevents more than one grace period from starting while we /Users/rubber/linux/kernel/rcu/srcutree.c: 505
 * are initiating callback invocation.  This allows the ->srcu_have_cbs[] /Users/rubber/linux/kernel/rcu/srcutree.c: 506
 * array to have a finite number of elements. /Users/rubber/linux/kernel/rcu/srcutree.c: 507
 * Funnel-locking scheme to scalably mediate many concurrent expedited /Users/rubber/linux/kernel/rcu/srcutree.c: 589
 * grace-period requests.  This function is invoked for the first known /Users/rubber/linux/kernel/rcu/srcutree.c: 590
 * expedited request for a grace period that has already been requested, /Users/rubber/linux/kernel/rcu/srcutree.c: 591
 * but without expediting.  To start a completely new grace period, /Users/rubber/linux/kernel/rcu/srcutree.c: 592
 * whether expedited or not, use srcu_funnel_gp_start() instead. /Users/rubber/linux/kernel/rcu/srcutree.c: 593
 * Funnel-locking scheme to scalably mediate many concurrent grace-period /Users/rubber/linux/kernel/rcu/srcutree.c: 619
 * requests.  The winner has to do the work of actually starting grace /Users/rubber/linux/kernel/rcu/srcutree.c: 620
 * period s.  Losers must either ensure that their desired grace-period /Users/rubber/linux/kernel/rcu/srcutree.c: 621
 * number is recorded on at least their leaf srcu_node structure, or they /Users/rubber/linux/kernel/rcu/srcutree.c: 622
 * must take steps to invoke their own callbacks. /Users/rubber/linux/kernel/rcu/srcutree.c: 623
 * Note that this function also does the work of srcu_funnel_exp_start(), /Users/rubber/linux/kernel/rcu/srcutree.c: 625
 * in some cases by directly invoking it. /Users/rubber/linux/kernel/rcu/srcutree.c: 626
		/* /Users/rubber/linux/kernel/rcu/srcutree.c: 667
		 * Record need for grace period s.  Pair with load /Users/rubber/linux/kernel/rcu/srcutree.c: 668
		 * acquire setting up for initialization. /Users/rubber/linux/kernel/rcu/srcutree.c: 669
 * Wait until all readers counted by array index idx complete, but /Users/rubber/linux/kernel/rcu/srcutree.c: 691
 * loop an additional time if there is an expedited grace period pending. /Users/rubber/linux/kernel/rcu/srcutree.c: 692
 * The caller must ensure that ->srcu_idx is not changed while checking. /Users/rubber/linux/kernel/rcu/srcutree.c: 693
 * Increment the ->srcu_idx counter so that future SRCU readers will /Users/rubber/linux/kernel/rcu/srcutree.c: 707
 * use the other rank of the ->srcu_(un)lock_count[] arrays.  This allows /Users/rubber/linux/kernel/rcu/srcutree.c: 708
 * us to wait for pre-existing readers in a starvation-free manner. /Users/rubber/linux/kernel/rcu/srcutree.c: 709
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 713
	 * Ensure that if this updater saw a given reader's increment /Users/rubber/linux/kernel/rcu/srcutree.c: 714
	 * from __srcu_read_lock(), that reader was using an old value /Users/rubber/linux/kernel/rcu/srcutree.c: 715
	 * of ->srcu_idx.  Also ensure that if a given reader sees the /Users/rubber/linux/kernel/rcu/srcutree.c: 716
	 * new value of ->srcu_idx, this updater's earlier scans cannot /Users/rubber/linux/kernel/rcu/srcutree.c: 717
	 * have seen that reader's increments (which is OK, because this /Users/rubber/linux/kernel/rcu/srcutree.c: 718
	 * grace period need not wait on that reader). /Users/rubber/linux/kernel/rcu/srcutree.c: 719
 Pairs with B and C. * /Users/rubber/linux/kernel/rcu/srcutree.c: 721
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 725
	 * Ensure that if the updater misses an __srcu_read_unlock() /Users/rubber/linux/kernel/rcu/srcutree.c: 726
	 * increment, that task's next __srcu_read_lock() will see the /Users/rubber/linux/kernel/rcu/srcutree.c: 727
	 * above counter update.  Note that both this memory barrier /Users/rubber/linux/kernel/rcu/srcutree.c: 728
	 * and the one in srcu_readers_active_idx_check() provide the /Users/rubber/linux/kernel/rcu/srcutree.c: 729
	 * guarantee for __srcu_read_lock(). /Users/rubber/linux/kernel/rcu/srcutree.c: 730
 Pairs with C. * /Users/rubber/linux/kernel/rcu/srcutree.c: 732
 * If SRCU is likely idle, return true, otherwise return false. /Users/rubber/linux/kernel/rcu/srcutree.c: 736
 * Note that it is OK for several current from-idle requests for a new /Users/rubber/linux/kernel/rcu/srcutree.c: 738
 * grace period from idle to specify expediting because they will all end /Users/rubber/linux/kernel/rcu/srcutree.c: 739
 * up requesting the same grace period anyhow.  So no loss. /Users/rubber/linux/kernel/rcu/srcutree.c: 740
 * Note also that if any CPU (including the current one) is still invoking /Users/rubber/linux/kernel/rcu/srcutree.c: 742
 * callbacks, this function will nevertheless say "idle".  This is not /Users/rubber/linux/kernel/rcu/srcutree.c: 743
 * ideal, but the overhead of checking all CPUs' callback lists is even /Users/rubber/linux/kernel/rcu/srcutree.c: 744
 * less ideal, especially on large systems.  Furthermore, the wakeup /Users/rubber/linux/kernel/rcu/srcutree.c: 745
 * can happen before the callback is fully removed, so we have no choice /Users/rubber/linux/kernel/rcu/srcutree.c: 746
 * but to accept this type of error. /Users/rubber/linux/kernel/rcu/srcutree.c: 747
 * This function is also subject to counter-wrap errors, but let's face /Users/rubber/linux/kernel/rcu/srcutree.c: 749
 * it, if this function was preempted for enough time for the counters /Users/rubber/linux/kernel/rcu/srcutree.c: 750
 * to wrap, it really doesn't matter whether or not we expedite the grace /Users/rubber/linux/kernel/rcu/srcutree.c: 751
 * period.  The extra overhead of a needlessly expedited grace period is /Users/rubber/linux/kernel/rcu/srcutree.c: 752
 * negligible when amortized over that time period, and the extra latency /Users/rubber/linux/kernel/rcu/srcutree.c: 753
 * of a needlessly non-expedited grace period is similarly negligible. /Users/rubber/linux/kernel/rcu/srcutree.c: 754
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 774
	 * No local callbacks, so probabilistically probe global state. /Users/rubber/linux/kernel/rcu/srcutree.c: 775
	 * Exact information would require acquiring locks, which would /Users/rubber/linux/kernel/rcu/srcutree.c: 776
	 * kill scalability, hence the probabilistic nature of the probe. /Users/rubber/linux/kernel/rcu/srcutree.c: 777
 * SRCU callback function to leak a callback. /Users/rubber/linux/kernel/rcu/srcutree.c: 799
 * Start an SRCU grace period, and also queue the callback if non-NULL. /Users/rubber/linux/kernel/rcu/srcutree.c: 806
 * Enqueue an SRCU callback on the srcu_data structure associated with /Users/rubber/linux/kernel/rcu/srcutree.c: 846
 * the current CPU and the specified srcu_struct structure, initiating /Users/rubber/linux/kernel/rcu/srcutree.c: 847
 * grace-period processing if it is not already running. /Users/rubber/linux/kernel/rcu/srcutree.c: 848
 * Note that all CPUs must agree that the grace period extended beyond /Users/rubber/linux/kernel/rcu/srcutree.c: 850
 * all pre-existing SRCU read-side critical section.  On systems with /Users/rubber/linux/kernel/rcu/srcutree.c: 851
 * more than one CPU, this means that when "func()" is invoked, each CPU /Users/rubber/linux/kernel/rcu/srcutree.c: 852
 * is guaranteed to have executed a full memory barrier since the end of /Users/rubber/linux/kernel/rcu/srcutree.c: 853
 * its last corresponding SRCU read-side critical section whose beginning /Users/rubber/linux/kernel/rcu/srcutree.c: 854
 * preceded the call to call_srcu().  It also means that each CPU executing /Users/rubber/linux/kernel/rcu/srcutree.c: 855
 * an SRCU read-side critical section that continues beyond the start of /Users/rubber/linux/kernel/rcu/srcutree.c: 856
 * "func()" must have executed a memory barrier after the call_srcu() /Users/rubber/linux/kernel/rcu/srcutree.c: 857
 * but before the beginning of that SRCU read-side critical section. /Users/rubber/linux/kernel/rcu/srcutree.c: 858
 * Note that these guarantees include CPUs that are offline, idle, or /Users/rubber/linux/kernel/rcu/srcutree.c: 859
 * executing in user mode, as well as CPUs that are executing in the kernel. /Users/rubber/linux/kernel/rcu/srcutree.c: 860
 * Furthermore, if CPU A invoked call_srcu() and CPU B invoked the /Users/rubber/linux/kernel/rcu/srcutree.c: 862
 * resulting SRCU callback function "func()", then both CPU A and CPU /Users/rubber/linux/kernel/rcu/srcutree.c: 863
 * B are guaranteed to execute a full memory barrier during the time /Users/rubber/linux/kernel/rcu/srcutree.c: 864
 * interval between the call to call_srcu() and the invocation of "func()". /Users/rubber/linux/kernel/rcu/srcutree.c: 865
 * This guarantee applies even if CPU A and CPU B are the same CPU (but /Users/rubber/linux/kernel/rcu/srcutree.c: 866
 * again only if the system has more than one CPU). /Users/rubber/linux/kernel/rcu/srcutree.c: 867
 * Of course, these guarantees apply only for invocations of call_srcu(), /Users/rubber/linux/kernel/rcu/srcutree.c: 869
 * srcu_read_lock(), and srcu_read_unlock() that are all passed the same /Users/rubber/linux/kernel/rcu/srcutree.c: 870
 * srcu_struct structure. /Users/rubber/linux/kernel/rcu/srcutree.c: 871
 * call_srcu() - Queue a callback for invocation after an SRCU grace period /Users/rubber/linux/kernel/rcu/srcutree.c: 887
 * @ssp: srcu_struct in queue the callback /Users/rubber/linux/kernel/rcu/srcutree.c: 888
 * @rhp: structure to be used for queueing the SRCU callback. /Users/rubber/linux/kernel/rcu/srcutree.c: 889
 * @func: function to be invoked after the SRCU grace period /Users/rubber/linux/kernel/rcu/srcutree.c: 890
 * The callback function will be invoked some time after a full SRCU /Users/rubber/linux/kernel/rcu/srcutree.c: 892
 * grace period elapses, in other words after all pre-existing SRCU /Users/rubber/linux/kernel/rcu/srcutree.c: 893
 * read-side critical sections have completed.  However, the callback /Users/rubber/linux/kernel/rcu/srcutree.c: 894
 * function might well execute concurrently with other SRCU read-side /Users/rubber/linux/kernel/rcu/srcutree.c: 895
 * critical sections that started after call_srcu() was invoked.  SRCU /Users/rubber/linux/kernel/rcu/srcutree.c: 896
 * read-side critical sections are delimited by srcu_read_lock() and /Users/rubber/linux/kernel/rcu/srcutree.c: 897
 * srcu_read_unlock(), and may be nested. /Users/rubber/linux/kernel/rcu/srcutree.c: 898
 * The callback will be invoked from process context, but must nevertheless /Users/rubber/linux/kernel/rcu/srcutree.c: 900
 * be fast and must not block. /Users/rubber/linux/kernel/rcu/srcutree.c: 901
 * Helper function for synchronize_srcu() and synchronize_srcu_expedited(). /Users/rubber/linux/kernel/rcu/srcutree.c: 911
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 933
	 * Make sure that later code is ordered after the SRCU grace /Users/rubber/linux/kernel/rcu/srcutree.c: 934
	 * period.  This pairs with the spin_lock_irq_rcu_node() /Users/rubber/linux/kernel/rcu/srcutree.c: 935
	 * in srcu_invoke_callbacks().  Unlike Tree RCU, this is needed /Users/rubber/linux/kernel/rcu/srcutree.c: 936
	 * because the current CPU might have been totally uninvolved with /Users/rubber/linux/kernel/rcu/srcutree.c: 937
	 * (and thus unordered against) that grace period. /Users/rubber/linux/kernel/rcu/srcutree.c: 938
 * synchronize_srcu_expedited - Brute-force SRCU grace period /Users/rubber/linux/kernel/rcu/srcutree.c: 944
 * @ssp: srcu_struct with which to synchronize. /Users/rubber/linux/kernel/rcu/srcutree.c: 945
 * Wait for an SRCU grace period to elapse, but be more aggressive about /Users/rubber/linux/kernel/rcu/srcutree.c: 947
 * spinning rather than blocking when waiting. /Users/rubber/linux/kernel/rcu/srcutree.c: 948
 * Note that synchronize_srcu_expedited() has the same deadlock and /Users/rubber/linux/kernel/rcu/srcutree.c: 950
 * memory-ordering properties as does synchronize_srcu(). /Users/rubber/linux/kernel/rcu/srcutree.c: 951
 * synchronize_srcu - wait for prior SRCU read-side critical-section completion /Users/rubber/linux/kernel/rcu/srcutree.c: 960
 * @ssp: srcu_struct with which to synchronize. /Users/rubber/linux/kernel/rcu/srcutree.c: 961
 * Wait for the count to drain to zero of both indexes. To avoid the /Users/rubber/linux/kernel/rcu/srcutree.c: 963
 * possible starvation of synchronize_srcu(), it waits for the count of /Users/rubber/linux/kernel/rcu/srcutree.c: 964
 * the index=((->srcu_idx & 1) ^ 1) to drain to zero at first, /Users/rubber/linux/kernel/rcu/srcutree.c: 965
 * and then flip the srcu_idx and wait for the count of the other index. /Users/rubber/linux/kernel/rcu/srcutree.c: 966
 * Can block; must be called from process context. /Users/rubber/linux/kernel/rcu/srcutree.c: 968
 * Note that it is illegal to call synchronize_srcu() from the corresponding /Users/rubber/linux/kernel/rcu/srcutree.c: 970
 * SRCU read-side critical section; doing so will result in deadlock. /Users/rubber/linux/kernel/rcu/srcutree.c: 971
 * However, it is perfectly legal to call synchronize_srcu() on one /Users/rubber/linux/kernel/rcu/srcutree.c: 972
 * srcu_struct from some other srcu_struct's read-side critical section, /Users/rubber/linux/kernel/rcu/srcutree.c: 973
 * as long as the resulting graph of srcu_structs is acyclic. /Users/rubber/linux/kernel/rcu/srcutree.c: 974
 * There are memory-ordering constraints implied by synchronize_srcu(). /Users/rubber/linux/kernel/rcu/srcutree.c: 976
 * On systems with more than one CPU, when synchronize_srcu() returns, /Users/rubber/linux/kernel/rcu/srcutree.c: 977
 * each CPU is guaranteed to have executed a full memory barrier since /Users/rubber/linux/kernel/rcu/srcutree.c: 978
 * the end of its last corresponding SRCU read-side critical section /Users/rubber/linux/kernel/rcu/srcutree.c: 979
 * whose beginning preceded the call to synchronize_srcu().  In addition, /Users/rubber/linux/kernel/rcu/srcutree.c: 980
 * each CPU having an SRCU read-side critical section that extends beyond /Users/rubber/linux/kernel/rcu/srcutree.c: 981
 * the return from synchronize_srcu() is guaranteed to have executed a /Users/rubber/linux/kernel/rcu/srcutree.c: 982
 * full memory barrier after the beginning of synchronize_srcu() and before /Users/rubber/linux/kernel/rcu/srcutree.c: 983
 * the beginning of that SRCU read-side critical section.  Note that these /Users/rubber/linux/kernel/rcu/srcutree.c: 984
 * guarantees include CPUs that are offline, idle, or executing in user mode, /Users/rubber/linux/kernel/rcu/srcutree.c: 985
 * as well as CPUs that are executing in the kernel. /Users/rubber/linux/kernel/rcu/srcutree.c: 986
 * Furthermore, if CPU A invoked synchronize_srcu(), which returned /Users/rubber/linux/kernel/rcu/srcutree.c: 988
 * to its caller on CPU B, then both CPU A and CPU B are guaranteed /Users/rubber/linux/kernel/rcu/srcutree.c: 989
 * to have executed a full memory barrier during the execution of /Users/rubber/linux/kernel/rcu/srcutree.c: 990
 * synchronize_srcu().  This guarantee applies even if CPU A and CPU B /Users/rubber/linux/kernel/rcu/srcutree.c: 991
 * are the same CPU, but again only if the system has more than one CPU. /Users/rubber/linux/kernel/rcu/srcutree.c: 992
 * Of course, these memory-ordering guarantees apply only when /Users/rubber/linux/kernel/rcu/srcutree.c: 994
 * synchronize_srcu(), srcu_read_lock(), and srcu_read_unlock() are /Users/rubber/linux/kernel/rcu/srcutree.c: 995
 * passed the same srcu_struct structure. /Users/rubber/linux/kernel/rcu/srcutree.c: 996
 * Implementation of these memory-ordering guarantees is similar to /Users/rubber/linux/kernel/rcu/srcutree.c: 998
 * that of synchronize_rcu(). /Users/rubber/linux/kernel/rcu/srcutree.c: 999
 * If SRCU is likely idle, expedite the first request.  This semantic /Users/rubber/linux/kernel/rcu/srcutree.c: 1001
 * was provided by Classic SRCU, and is relied upon by its users, so TREE /Users/rubber/linux/kernel/rcu/srcutree.c: 1002
 * SRCU must also provide it.  Note that detecting idleness is heuristic /Users/rubber/linux/kernel/rcu/srcutree.c: 1003
 * and subject to both false positives and negatives. /Users/rubber/linux/kernel/rcu/srcutree.c: 1004
 * get_state_synchronize_srcu - Provide an end-of-grace-period cookie /Users/rubber/linux/kernel/rcu/srcutree.c: 1016
 * @ssp: srcu_struct to provide cookie for. /Users/rubber/linux/kernel/rcu/srcutree.c: 1017
 * This function returns a cookie that can be passed to /Users/rubber/linux/kernel/rcu/srcutree.c: 1019
 * poll_state_synchronize_srcu(), which will return true if a full grace /Users/rubber/linux/kernel/rcu/srcutree.c: 1020
 * period has elapsed in the meantime.  It is the caller's responsibility /Users/rubber/linux/kernel/rcu/srcutree.c: 1021
 * to make sure that grace period happens, for example, by invoking /Users/rubber/linux/kernel/rcu/srcutree.c: 1022
 * call_srcu() after return from get_state_synchronize_srcu(). /Users/rubber/linux/kernel/rcu/srcutree.c: 1023
 Any prior manipulation of SRCU-protected data must happen /Users/rubber/linux/kernel/rcu/srcutree.c: 1027
 before the load from ->srcu_gp_seq. /Users/rubber/linux/kernel/rcu/srcutree.c: 1028
 * start_poll_synchronize_srcu - Provide cookie and start grace period /Users/rubber/linux/kernel/rcu/srcutree.c: 1035
 * @ssp: srcu_struct to provide cookie for. /Users/rubber/linux/kernel/rcu/srcutree.c: 1036
 * This function returns a cookie that can be passed to /Users/rubber/linux/kernel/rcu/srcutree.c: 1038
 * poll_state_synchronize_srcu(), which will return true if a full grace /Users/rubber/linux/kernel/rcu/srcutree.c: 1039
 * period has elapsed in the meantime.  Unlike get_state_synchronize_srcu(), /Users/rubber/linux/kernel/rcu/srcutree.c: 1040
 * this function also ensures that any needed SRCU grace period will be /Users/rubber/linux/kernel/rcu/srcutree.c: 1041
 * started.  This convenience does come at a cost in terms of CPU overhead. /Users/rubber/linux/kernel/rcu/srcutree.c: 1042
 * poll_state_synchronize_srcu - Has cookie's grace period ended? /Users/rubber/linux/kernel/rcu/srcutree.c: 1051
 * @ssp: srcu_struct to provide cookie for. /Users/rubber/linux/kernel/rcu/srcutree.c: 1052
 * @cookie: Return value from get_state_synchronize_srcu() or start_poll_synchronize_srcu(). /Users/rubber/linux/kernel/rcu/srcutree.c: 1053
 * This function takes the cookie that was returned from either /Users/rubber/linux/kernel/rcu/srcutree.c: 1055
 * get_state_synchronize_srcu() or start_poll_synchronize_srcu(), and /Users/rubber/linux/kernel/rcu/srcutree.c: 1056
 * returns @true if an SRCU grace period elapsed since the time that the /Users/rubber/linux/kernel/rcu/srcutree.c: 1057
 * cookie was created. /Users/rubber/linux/kernel/rcu/srcutree.c: 1058
 * Because cookies are finite in size, wrapping/overflow is possible. /Users/rubber/linux/kernel/rcu/srcutree.c: 1060
 * This is more pronounced on 32-bit systems where cookies are 32 bits, /Users/rubber/linux/kernel/rcu/srcutree.c: 1061
 * where in theory wrapping could happen in about 14 hours assuming /Users/rubber/linux/kernel/rcu/srcutree.c: 1062
 * 25-microsecond expedited SRCU grace periods.  However, a more likely /Users/rubber/linux/kernel/rcu/srcutree.c: 1063
 * overflow lower bound is on the order of 24 days in the case of /Users/rubber/linux/kernel/rcu/srcutree.c: 1064
 * one-millisecond SRCU grace periods.  Of course, wrapping in a 64-bit /Users/rubber/linux/kernel/rcu/srcutree.c: 1065
 * system requires geologic timespans, as in more than seven million years /Users/rubber/linux/kernel/rcu/srcutree.c: 1066
 * even for expedited SRCU grace periods. /Users/rubber/linux/kernel/rcu/srcutree.c: 1067
 * Wrapping/overflow is much more of an issue for CONFIG_SMP=n systems /Users/rubber/linux/kernel/rcu/srcutree.c: 1069
 * that also have CONFIG_PREEMPTION=n, which selects Tiny SRCU.  This uses /Users/rubber/linux/kernel/rcu/srcutree.c: 1070
 * a 16-bit cookie, which rcutorture routinely wraps in a matter of a /Users/rubber/linux/kernel/rcu/srcutree.c: 1071
 * few minutes.  If this proves to be a problem, this counter will be /Users/rubber/linux/kernel/rcu/srcutree.c: 1072
 * expanded to the same size as for Tree SRCU. /Users/rubber/linux/kernel/rcu/srcutree.c: 1073
 Ensure that the end of the SRCU grace period happens before /Users/rubber/linux/kernel/rcu/srcutree.c: 1079
 any subsequent code that the caller might execute. /Users/rubber/linux/kernel/rcu/srcutree.c: 1080
 ^^^ /Users/rubber/linux/kernel/rcu/srcutree.c: 1081
 * Callback function for srcu_barrier() use. /Users/rubber/linux/kernel/rcu/srcutree.c: 1087
 * srcu_barrier - Wait until all in-flight call_srcu() callbacks complete. /Users/rubber/linux/kernel/rcu/srcutree.c: 1101
 * @ssp: srcu_struct on which to wait for in-flight callbacks. /Users/rubber/linux/kernel/rcu/srcutree.c: 1102
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1123
	 * Each pass through this loop enqueues a callback, but only /Users/rubber/linux/kernel/rcu/srcutree.c: 1124
	 * on CPUs already having callbacks enqueued.  Note that if /Users/rubber/linux/kernel/rcu/srcutree.c: 1125
	 * a CPU already has callbacks enqueue, it must have already /Users/rubber/linux/kernel/rcu/srcutree.c: 1126
	 * registered the need for a future grace period, so all we /Users/rubber/linux/kernel/rcu/srcutree.c: 1127
	 * need do is enqueue a callback that will use the same /Users/rubber/linux/kernel/rcu/srcutree.c: 1128
	 * grace period as the last callback already in the queue. /Users/rubber/linux/kernel/rcu/srcutree.c: 1129
 * srcu_batches_completed - return batches completed. /Users/rubber/linux/kernel/rcu/srcutree.c: 1156
 * @ssp: srcu_struct on which to report batch completion. /Users/rubber/linux/kernel/rcu/srcutree.c: 1157
 * Report the number of batches, correlated with, but not necessarily /Users/rubber/linux/kernel/rcu/srcutree.c: 1159
 * precisely the same as, the number of grace periods that have elapsed. /Users/rubber/linux/kernel/rcu/srcutree.c: 1160
 * Core SRCU state machine.  Push state bits of ->srcu_gp_seq /Users/rubber/linux/kernel/rcu/srcutree.c: 1169
 * to SRCU_STATE_SCAN2, and invoke srcu_gp_end() when scan has /Users/rubber/linux/kernel/rcu/srcutree.c: 1170
 * completed in that state. /Users/rubber/linux/kernel/rcu/srcutree.c: 1171
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1179
	 * Because readers might be delayed for an extended period after /Users/rubber/linux/kernel/rcu/srcutree.c: 1180
	 * fetching ->srcu_idx for their index, at any point in time there /Users/rubber/linux/kernel/rcu/srcutree.c: 1181
	 * might well be readers using both idx=0 and idx=1.  We therefore /Users/rubber/linux/kernel/rcu/srcutree.c: 1182
	 * need to wait for readers to clear from both index values before /Users/rubber/linux/kernel/rcu/srcutree.c: 1183
	 * invoking a callback. /Users/rubber/linux/kernel/rcu/srcutree.c: 1184
	 * /Users/rubber/linux/kernel/rcu/srcutree.c: 1185
	 * The load-acquire ensures that we see the accesses performed /Users/rubber/linux/kernel/rcu/srcutree.c: 1186
	 * by the prior grace period. /Users/rubber/linux/kernel/rcu/srcutree.c: 1187
		/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1222
		 * SRCU read-side critical sections are normally short, /Users/rubber/linux/kernel/rcu/srcutree.c: 1223
		 * so check at least twice in quick succession after a flip. /Users/rubber/linux/kernel/rcu/srcutree.c: 1224
 * Invoke a limited number of SRCU callbacks that have passed through /Users/rubber/linux/kernel/rcu/srcutree.c: 1236
 * their grace period.  If there are more to do, SRCU will reschedule /Users/rubber/linux/kernel/rcu/srcutree.c: 1237
 * the workqueue.  Note that needed memory barriers have been executed /Users/rubber/linux/kernel/rcu/srcutree.c: 1238
 * in this task's context by srcu_readers_active_idx_check(). /Users/rubber/linux/kernel/rcu/srcutree.c: 1239
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1277
	 * Update counts, accelerate new callbacks, and if needed, /Users/rubber/linux/kernel/rcu/srcutree.c: 1278
	 * schedule another round of callback invocation. /Users/rubber/linux/kernel/rcu/srcutree.c: 1279
 * Finished one round of SRCU grace period.  Start another if there are /Users/rubber/linux/kernel/rcu/srcutree.c: 1293
 * more SRCU callbacks queued, otherwise put SRCU into not-running state. /Users/rubber/linux/kernel/rcu/srcutree.c: 1294
 * This is the work-queue function that handles SRCU grace periods. /Users/rubber/linux/kernel/rcu/srcutree.c: 1317
		/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1359
		 * Make sure that a lock is always counted if the corresponding /Users/rubber/linux/kernel/rcu/srcutree.c: 1360
		 * unlock is counted. /Users/rubber/linux/kernel/rcu/srcutree.c: 1361
	/* /Users/rubber/linux/kernel/rcu/srcutree.c: 1393
	 * Once that is set, call_srcu() can follow the normal path and /Users/rubber/linux/kernel/rcu/srcutree.c: 1394
	 * queue delayed work. This must follow RCU workqueues creation /Users/rubber/linux/kernel/rcu/srcutree.c: 1395
	 * and timers initialization. /Users/rubber/linux/kernel/rcu/srcutree.c: 1396
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 1
 * RCU segmented callback lists, function definitions /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 3
 * Copyright IBM Corporation, 2017 /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 5
 * Authors: Paul E. McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 7
 * Enqueue an rcu_head structure onto the specified callback list. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 26
 * Flush the second rcu_cblist structure onto the first one, obliterating /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 36
 * any contents of the first.  If rhp is non-NULL, enqueue it as the sole /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 37
 * element of the second rcu_cblist structure, but ensuring that the second /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 38
 * rcu_cblist structure, if initially non-empty, always appears non-empty /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 39
 * throughout the process.  If rdp is NULL, the second rcu_cblist structure /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 40
 * is instead initialized to empty. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 41
 * Dequeue the oldest rcu_head structure from the specified callback /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 64
 * list. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 65
 * Increase the numeric length of an rcu_segcblist structure by the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 144
 * specified amount, which can be negative.  This can cause the ->len /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 145
 * field to disagree with the actual number of callbacks on the structure. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 146
 * This increase is fully ordered with respect to the callers accesses /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 147
 * both before and after. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 148
 * So why on earth is a memory barrier required both before and after /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 150
 * the update to the ->len field??? /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 151
 * The reason is that rcu_barrier() locklessly samples each CPU's ->len /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 153
 * field, and if a given CPU's field is zero, avoids IPIing that CPU. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 154
 * This can of course race with both queuing and invoking of callbacks. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 155
 * Failing to correctly handle either of these races could result in /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 156
 * rcu_barrier() failing to IPI a CPU that actually had callbacks queued /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 157
 * which rcu_barrier() was obligated to wait on.  And if rcu_barrier() /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 158
 * failed to wait on such a callback, unloading certain kernel modules /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 159
 * would result in calls to functions whose code was no longer present in /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 160
 * the kernel, for but one example. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 161
 * Therefore, ->len transitions from 1->0 and 0->1 have to be carefully /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 163
 * ordered with respect with both list modifications and the rcu_barrier(). /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 164
 * The queuing case is CASE 1 and the invoking case is CASE 2. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 166
 * CASE 1: Suppose that CPU 0 has no callbacks queued, but invokes /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 168
 * call_rcu() just as CPU 1 invokes rcu_barrier().  CPU 0's ->len field /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 169
 * will transition from 0->1, which is one of the transitions that must /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 170
 * be handled carefully.  Without the full memory barriers after the ->len /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 171
 * update and at the beginning of rcu_barrier(), the following could happen: /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 172
 * CPU 0				CPU 1 /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 174
 * call_rcu(). /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 176
 *					rcu_barrier() sees ->len as 0. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 177
 * set ->len = 1. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 178
 *					rcu_barrier() does nothing. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 179
 *					module is unloaded. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 180
 * callback invokes unloaded function! /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 181
 * With the full barriers, any case where rcu_barrier() sees ->len as 0 will /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 183
 * have unambiguously preceded the return from the racing call_rcu(), which /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 184
 * means that this call_rcu() invocation is OK to not wait on.  After all, /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 185
 * you are supposed to make sure that any problematic call_rcu() invocations /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 186
 * happen before the rcu_barrier(). /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 187
 * CASE 2: Suppose that CPU 0 is invoking its last callback just as /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 190
 * CPU 1 invokes rcu_barrier().  CPU 0's ->len field will transition from /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 191
 * 1->0, which is one of the transitions that must be handled carefully. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 192
 * Without the full memory barriers before the ->len update and at the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 193
 * end of rcu_barrier(), the following could happen: /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 194
 * CPU 0				CPU 1 /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 196
 * start invoking last callback /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 198
 * set ->len = 0 (reordered) /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 199
 *					rcu_barrier() sees ->len as 0 /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 200
 *					rcu_barrier() does nothing. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 201
 *					module is unloaded /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 202
 * callback executing after unloaded! /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 203
 * With the full barriers, any case where rcu_barrier() sees ->len as 0 /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 205
 * will be fully ordered after the completion of the callback function, /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 206
 * so that the module unloading operation is completely safe. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 207
 Read header comment above. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 213
 Read header comment above. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 215
 Read header comment above. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 217
 Read header comment above. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 219
 * Increase the numeric length of an rcu_segcblist structure by one. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 224
 * This can cause the ->len field to disagree with the actual number of /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 225
 * callbacks on the structure.  This increase is fully ordered with respect /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 226
 * to the callers accesses both before and after. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 227
 * Initialize an rcu_segcblist structure. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 235
 * Disable the specified rcu_segcblist structure, so that callbacks can /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 253
 * no longer be posted to it.  This structure must be empty. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 254
 * Mark the specified rcu_segcblist structure as offloaded. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 264
 * Does the specified rcu_segcblist structure contain callbacks that /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 277
 * are ready to be invoked? /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 278
 * Does the specified rcu_segcblist structure contain callbacks that /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 287
 * are still pending, that is, not yet ready to be invoked? /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 288
 * Return a pointer to the first callback in the specified rcu_segcblist /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 297
 * structure.  This is useful for diagnostics. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 298
 * Return a pointer to the first pending callback in the specified /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 308
 * rcu_segcblist structure.  This is useful just after posting a given /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 309
 * callback -- if that callback is the first pending callback, then /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 310
 * you cannot rely on someone else having already started up the required /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 311
 * grace period. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 312
 * Return false if there are no CBs awaiting grace periods, otherwise, /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 322
 * return true and store the nearest waited-upon grace period into *lp. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 323
 * Enqueue the specified callback onto the specified rcu_segcblist /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 334
 * structure, updating accounting as needed.  Note that the ->len /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 335
 * field may be accessed locklessly, hence the WRITE_ONCE(). /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 336
 * The ->len field is used by rcu_barrier() and friends to determine /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 337
 * if it must post a callback on this structure, and it is OK /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 338
 * for rcu_barrier() to sometimes post callbacks needlessly, but /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 339
 * absolutely not OK for it to ever miss posting a callback. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 340
 * Entrain the specified callback onto the specified rcu_segcblist at /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 353
 * the end of the last non-empty segment.  If the entire rcu_segcblist /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 354
 * is empty, make no change, but return false. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 355
 * This is intended for use by rcu_barrier()-like primitives, -not- /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 357
 * for normal grace-period use.  IMPORTANT:  The callback you enqueue /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 358
 * will wait for all prior callbacks, NOT necessarily for a grace /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 359
 * period.  You have been warned. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 360
 * Extract only those callbacks ready to be invoked from the specified /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 383
 * rcu_segcblist structure and place them in the specified rcu_cblist /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 384
 * structure. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 385
 * Extract only those callbacks still pending (not yet ready to be /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 406
 * invoked) from the specified rcu_segcblist structure and place them in /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 407
 * the specified rcu_cblist structure.  Note that this loses information /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 408
 * about any callbacks that might have been partway done waiting for /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 409
 * their grace period.  Too bad!  They will have to start over. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 410
 * Insert counts from the specified rcu_cblist structure in the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 431
 * specified rcu_segcblist structure. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 432
 * Move callbacks from the specified rcu_cblist to the beginning of the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 441
 * done-callbacks segment of the specified rcu_segcblist. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 442
 * Move callbacks from the specified rcu_cblist to the end of the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 464
 * new-callbacks segment of the specified rcu_segcblist. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 465
 * Advance the callbacks in the specified rcu_segcblist structure based /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 479
 * on the current value passed in for the grace-period counter. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 480
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 490
	 * Find all callbacks whose ->gp_seq numbers indicate that they /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 491
	 * are ready to invoke, and put them into the RCU_DONE_TAIL segment. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 492
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 509
	 * Callbacks moved, so clean up the misordered ->tails[] pointers /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 510
	 * that now point into the middle of the list of ready-to-invoke /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 511
	 * callbacks.  The overall effect is to copy down the later pointers /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 512
	 * into the gap that was created by the now-ready segments. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 513
 * "Accelerate" callbacks based on more-accurate grace-period information. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 525
 * The reason for this is that RCU does not synchronize the beginnings and /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 526
 * ends of grace periods, and that callbacks are posted locally.  This in /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 527
 * turn means that the callbacks must be labelled conservatively early /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 528
 * on, as getting exact information would degrade both performance and /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 529
 * scalability.  When more accurate grace-period information becomes /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 530
 * available, previously posted callbacks can be "accelerated", marking /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 531
 * them to complete at the end of the earlier grace period. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 532
 * This function operates on an rcu_segcblist structure, and also the /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 534
 * grace-period sequence number seq at which new callbacks would become /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 535
 * ready to invoke.  Returns true if there are callbacks that won't be /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 536
 * ready to invoke until seq, false otherwise. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 537
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 547
	 * Find the segment preceding the oldest segment of callbacks /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 548
	 * whose ->gp_seq[] completion is at or after that passed in via /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 549
	 * "seq", skipping any empty segments.  This oldest segment, along /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 550
	 * with any later segments, can be merged in with any newly arrived /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 551
	 * callbacks in the RCU_NEXT_TAIL segment, and assigned "seq" /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 552
	 * as their ->gp_seq[] grace-period completion sequence number. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 553
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 560
	 * If all the segments contain callbacks that correspond to /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 561
	 * earlier grace-period sequence numbers than "seq", leave. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 562
	 * Assuming that the rcu_segcblist structure has enough /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 563
	 * segments in its arrays, this can only happen if some of /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 564
	 * the non-done segments contain callbacks that really are /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 565
	 * ready to invoke.  This situation will get straightened /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 566
	 * out by the next call to rcu_segcblist_advance(). /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 567
	 * /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 568
	 * Also advance to the oldest segment of callbacks whose /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 569
	 * ->gp_seq[] completion is at or after that passed in via "seq", /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 570
	 * skipping any empty segments. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 571
	 * /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 572
	 * Note that segment "i" (and any lower-numbered segments /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 573
	 * containing older callbacks) will be unaffected, and their /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 574
	 * grace-period numbers remain unchanged.  For example, if i == /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 575
	 * WAIT_TAIL, then neither WAIT_TAIL nor DONE_TAIL will be touched. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 576
	 * Instead, the CBs in NEXT_TAIL will be merged with those in /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 577
	 * NEXT_READY_TAIL and the grace-period number of NEXT_READY_TAIL /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 578
	 * would be updated.  NEXT_TAIL would then be empty. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 579
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 588
	 * Merge all later callbacks, including newly arrived callbacks, /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 589
	 * into the segment located by the for-loop above.  Assign "seq" /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 590
	 * as the ->gp_seq[] value in order to correctly handle the case /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 591
	 * where there were no pending callbacks in the rcu_segcblist /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 592
	 * structure other than in the RCU_NEXT_TAIL segment. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 593
 * Merge the source rcu_segcblist structure into the destination /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 603
 * rcu_segcblist structure, then initialize the source.  Any pending /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 604
 * callbacks from the source get to start over.  It is best to /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 605
 * advance and accelerate both the destination and the source /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 606
 * before merging. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 607
	/* /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 623
	 * No need smp_mb() before setting length to 0, because CPU hotplug /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 624
	 * lock excludes rcu_barrier. /Users/rubber/linux/kernel/rcu/rcu_segcblist.c: 625
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/sync.c: 1
 * RCU-based infrastructure for lightweight reader-writer locking /Users/rubber/linux/kernel/rcu/sync.c: 3
 * Copyright (c) 2015, Red Hat, Inc. /Users/rubber/linux/kernel/rcu/sync.c: 5
 * Author: Oleg Nesterov <oleg@redhat.com> /Users/rubber/linux/kernel/rcu/sync.c: 7
 * rcu_sync_init() - Initialize an rcu_sync structure /Users/rubber/linux/kernel/rcu/sync.c: 18
 * @rsp: Pointer to rcu_sync structure to be initialized /Users/rubber/linux/kernel/rcu/sync.c: 19
 * rcu_sync_enter_start - Force readers onto slow path for multiple updates /Users/rubber/linux/kernel/rcu/sync.c: 28
 * @rsp: Pointer to rcu_sync structure to use for synchronization /Users/rubber/linux/kernel/rcu/sync.c: 29
 * Must be called after rcu_sync_init() and before first use. /Users/rubber/linux/kernel/rcu/sync.c: 31
 * Ensures rcu_sync_is_idle() returns false and rcu_sync_{enter,exit}() /Users/rubber/linux/kernel/rcu/sync.c: 33
 * pairs turn into NO-OPs. /Users/rubber/linux/kernel/rcu/sync.c: 34
 * rcu_sync_func() - Callback function managing reader access to fastpath /Users/rubber/linux/kernel/rcu/sync.c: 51
 * @rhp: Pointer to rcu_head in rcu_sync structure to use for synchronization /Users/rubber/linux/kernel/rcu/sync.c: 52
 * This function is passed to call_rcu() function by rcu_sync_enter() and /Users/rubber/linux/kernel/rcu/sync.c: 54
 * rcu_sync_exit(), so that it is invoked after a grace period following the /Users/rubber/linux/kernel/rcu/sync.c: 55
 * that invocation of enter/exit. /Users/rubber/linux/kernel/rcu/sync.c: 56
 * If it is called by rcu_sync_enter() it signals that all the readers were /Users/rubber/linux/kernel/rcu/sync.c: 58
 * switched onto slow path. /Users/rubber/linux/kernel/rcu/sync.c: 59
 * If it is called by rcu_sync_exit() it takes action based on events that /Users/rubber/linux/kernel/rcu/sync.c: 61
 * have taken place in the meantime, so that closely spaced rcu_sync_enter() /Users/rubber/linux/kernel/rcu/sync.c: 62
 * and rcu_sync_exit() pairs need not wait for a grace period. /Users/rubber/linux/kernel/rcu/sync.c: 63
 * If another rcu_sync_enter() is invoked before the grace period /Users/rubber/linux/kernel/rcu/sync.c: 65
 * ended, reset state to allow the next rcu_sync_exit() to let the /Users/rubber/linux/kernel/rcu/sync.c: 66
 * readers back onto their fastpaths (after a grace period).  If both /Users/rubber/linux/kernel/rcu/sync.c: 67
 * another rcu_sync_enter() and its matching rcu_sync_exit() are invoked /Users/rubber/linux/kernel/rcu/sync.c: 68
 * before the grace period ended, re-invoke call_rcu() on behalf of that /Users/rubber/linux/kernel/rcu/sync.c: 69
 * rcu_sync_exit().  Otherwise, set all state back to idle so that readers /Users/rubber/linux/kernel/rcu/sync.c: 70
 * can again use their fastpaths. /Users/rubber/linux/kernel/rcu/sync.c: 71
		/* /Users/rubber/linux/kernel/rcu/sync.c: 83
		 * We're at least a GP after the GP_IDLE->GP_ENTER transition. /Users/rubber/linux/kernel/rcu/sync.c: 84
		/* /Users/rubber/linux/kernel/rcu/sync.c: 89
		 * A new rcu_sync_exit() has happened; requeue the callback to /Users/rubber/linux/kernel/rcu/sync.c: 90
		 * catch a later GP. /Users/rubber/linux/kernel/rcu/sync.c: 91
		/* /Users/rubber/linux/kernel/rcu/sync.c: 96
		 * We're at least a GP after the last rcu_sync_exit(); everybody /Users/rubber/linux/kernel/rcu/sync.c: 97
		 * will now have observed the write side critical section. /Users/rubber/linux/kernel/rcu/sync.c: 98
		 * Let 'em rip! /Users/rubber/linux/kernel/rcu/sync.c: 99
 * rcu_sync_enter() - Force readers onto slowpath /Users/rubber/linux/kernel/rcu/sync.c: 107
 * @rsp: Pointer to rcu_sync structure to use for synchronization /Users/rubber/linux/kernel/rcu/sync.c: 108
 * This function is used by updaters who need readers to make use of /Users/rubber/linux/kernel/rcu/sync.c: 110
 * a slowpath during the update.  After this function returns, all /Users/rubber/linux/kernel/rcu/sync.c: 111
 * subsequent calls to rcu_sync_is_idle() will return false, which /Users/rubber/linux/kernel/rcu/sync.c: 112
 * tells readers to stay off their fastpaths.  A later call to /Users/rubber/linux/kernel/rcu/sync.c: 113
 * rcu_sync_exit() re-enables reader slowpaths. /Users/rubber/linux/kernel/rcu/sync.c: 114
 * When called in isolation, rcu_sync_enter() must wait for a grace /Users/rubber/linux/kernel/rcu/sync.c: 116
 * period, however, closely spaced calls to rcu_sync_enter() can /Users/rubber/linux/kernel/rcu/sync.c: 117
 * optimize away the grace-period wait via a state machine implemented /Users/rubber/linux/kernel/rcu/sync.c: 118
 * by rcu_sync_enter(), rcu_sync_exit(), and rcu_sync_func(). /Users/rubber/linux/kernel/rcu/sync.c: 119
		/* /Users/rubber/linux/kernel/rcu/sync.c: 130
		 * Note that we could simply do rcu_sync_call(rsp) here and /Users/rubber/linux/kernel/rcu/sync.c: 131
		 * avoid the "if (gp_state == GP_IDLE)" block below. /Users/rubber/linux/kernel/rcu/sync.c: 132
		 * /Users/rubber/linux/kernel/rcu/sync.c: 133
		 * However, synchronize_rcu() can be faster if rcu_expedited /Users/rubber/linux/kernel/rcu/sync.c: 134
		 * or rcu_blocking_is_gp() is true. /Users/rubber/linux/kernel/rcu/sync.c: 135
		 * /Users/rubber/linux/kernel/rcu/sync.c: 136
		 * Another reason is that we can't wait for rcu callback if /Users/rubber/linux/kernel/rcu/sync.c: 137
		 * we are called at early boot time but this shouldn't happen. /Users/rubber/linux/kernel/rcu/sync.c: 138
		/* /Users/rubber/linux/kernel/rcu/sync.c: 145
		 * See the comment above, this simply does the "synchronous" /Users/rubber/linux/kernel/rcu/sync.c: 146
		 * call_rcu(rcu_sync_func) which does GP_ENTER -> GP_PASSED. /Users/rubber/linux/kernel/rcu/sync.c: 147
 * rcu_sync_exit() - Allow readers back onto fast path after grace period /Users/rubber/linux/kernel/rcu/sync.c: 159
 * @rsp: Pointer to rcu_sync structure to use for synchronization /Users/rubber/linux/kernel/rcu/sync.c: 160
 * This function is used by updaters who have completed, and can therefore /Users/rubber/linux/kernel/rcu/sync.c: 162
 * now allow readers to make use of their fastpaths after a grace period /Users/rubber/linux/kernel/rcu/sync.c: 163
 * has elapsed.  After this grace period has completed, all subsequent /Users/rubber/linux/kernel/rcu/sync.c: 164
 * calls to rcu_sync_is_idle() will return true, which tells readers that /Users/rubber/linux/kernel/rcu/sync.c: 165
 * they can once again use their fastpaths. /Users/rubber/linux/kernel/rcu/sync.c: 166
 * rcu_sync_dtor() - Clean up an rcu_sync structure /Users/rubber/linux/kernel/rcu/sync.c: 186
 * @rsp: Pointer to rcu_sync structure to be cleaned up /Users/rubber/linux/kernel/rcu/sync.c: 187
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/rcu/tree.c: 1
 * Read-Copy Update mechanism for mutual exclusion (tree-based version) /Users/rubber/linux/kernel/rcu/tree.c: 3
 * Copyright IBM Corporation, 2008 /Users/rubber/linux/kernel/rcu/tree.c: 5
 * Authors: Dipankar Sarma <dipankar@in.ibm.com> /Users/rubber/linux/kernel/rcu/tree.c: 7
 *	    Manfred Spraul <manfred@colorfullife.com> /Users/rubber/linux/kernel/rcu/tree.c: 8
 *	    Paul E. McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/tree.c: 9
 * Based on the original work by Paul McKenney <paulmck@linux.ibm.com> /Users/rubber/linux/kernel/rcu/tree.c: 11
 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen. /Users/rubber/linux/kernel/rcu/tree.c: 12
 * For detailed explanation of Read-Copy Update mechanism see - /Users/rubber/linux/kernel/rcu/tree.c: 14
 *	Documentation/RCU /Users/rubber/linux/kernel/rcu/tree.c: 15
 * The rcu_scheduler_active variable is initialized to the value /Users/rubber/linux/kernel/rcu/tree.c: 117
 * RCU_SCHEDULER_INACTIVE and transitions RCU_SCHEDULER_INIT just before the /Users/rubber/linux/kernel/rcu/tree.c: 118
 * first task is spawned.  So when this variable is RCU_SCHEDULER_INACTIVE, /Users/rubber/linux/kernel/rcu/tree.c: 119
 * RCU can assume that there is but one task, allowing RCU to (for example) /Users/rubber/linux/kernel/rcu/tree.c: 120
 * optimize synchronize_rcu() to a simple barrier().  When this variable /Users/rubber/linux/kernel/rcu/tree.c: 121
 * is RCU_SCHEDULER_INIT, RCU must actually do all the hard work required /Users/rubber/linux/kernel/rcu/tree.c: 122
 * to detect real grace periods.  This variable is also used to suppress /Users/rubber/linux/kernel/rcu/tree.c: 123
 * boot-time false positives from lockdep-RCU error checking.  Finally, it /Users/rubber/linux/kernel/rcu/tree.c: 124
 * transitions from RCU_SCHEDULER_INIT to RCU_SCHEDULER_RUNNING after RCU /Users/rubber/linux/kernel/rcu/tree.c: 125
 * is fully initialized, including all of its kthreads having been spawned. /Users/rubber/linux/kernel/rcu/tree.c: 126
 * The rcu_scheduler_fully_active variable transitions from zero to one /Users/rubber/linux/kernel/rcu/tree.c: 132
 * during the early_initcall() processing, which is after the scheduler /Users/rubber/linux/kernel/rcu/tree.c: 133
 * is capable of creating new tasks.  So RCU processing (for example, /Users/rubber/linux/kernel/rcu/tree.c: 134
 * creating tasks for RCU priority boosting) must be delayed until after /Users/rubber/linux/kernel/rcu/tree.c: 135
 * rcu_scheduler_fully_active transitions from zero to one.  We also /Users/rubber/linux/kernel/rcu/tree.c: 136
 * currently delay invocation of any RCU callbacks until after this point. /Users/rubber/linux/kernel/rcu/tree.c: 137
 * It might later prove better for people registering RCU callbacks during /Users/rubber/linux/kernel/rcu/tree.c: 139
 * early boot to take responsibility for these callbacks, but one step at /Users/rubber/linux/kernel/rcu/tree.c: 140
 * a time. /Users/rubber/linux/kernel/rcu/tree.c: 141
 Add delay to rcu_read_unlock() for strict grace periods. /Users/rubber/linux/kernel/rcu/tree.c: 169
 * This rcu parameter is runtime-read-only. It reflects /Users/rubber/linux/kernel/rcu/tree.c: 176
 * a minimum allowed number of objects which can be cached /Users/rubber/linux/kernel/rcu/tree.c: 177
 * per-CPU. Object size is equal to one page. This value /Users/rubber/linux/kernel/rcu/tree.c: 178
 * can be changed at boot time. /Users/rubber/linux/kernel/rcu/tree.c: 179
 A page shrinker can ask for pages to be freed to make them /Users/rubber/linux/kernel/rcu/tree.c: 184
 available for other parts of the system. This usually happens /Users/rubber/linux/kernel/rcu/tree.c: 185
 under low memory conditions, and in that case we should also /Users/rubber/linux/kernel/rcu/tree.c: 186
 defer page-cache filling for a short time period. /Users/rubber/linux/kernel/rcu/tree.c: 187
 The default value is 5 seconds, which is long enough to reduce /Users/rubber/linux/kernel/rcu/tree.c: 189
 interference with the shrinker while it asks other systems to /Users/rubber/linux/kernel/rcu/tree.c: 190
 drain their caches. /Users/rubber/linux/kernel/rcu/tree.c: 191
 * Number of grace periods between delays, normalized by the duration of /Users/rubber/linux/kernel/rcu/tree.c: 203
 * the delay.  The longer the delay, the more the grace periods between /Users/rubber/linux/kernel/rcu/tree.c: 204
 * each delay.  The reason for this normalization is that it means that, /Users/rubber/linux/kernel/rcu/tree.c: 205
 * for non-zero delays, the overall slowdown of grace periods is constant /Users/rubber/linux/kernel/rcu/tree.c: 206
 * regardless of the duration of the delay.  This arrangement balances /Users/rubber/linux/kernel/rcu/tree.c: 207
 * the need for long delays to increase some race probabilities with the /Users/rubber/linux/kernel/rcu/tree.c: 208
 * need for fast grace periods to increase other race probabilities. /Users/rubber/linux/kernel/rcu/tree.c: 209
 * Compute the mask of online CPUs for the specified rcu_node structure. /Users/rubber/linux/kernel/rcu/tree.c: 214
 * This will not be stable unless the rcu_node structure's ->lock is /Users/rubber/linux/kernel/rcu/tree.c: 215
 * held, but the bit corresponding to the current CPU will be stable /Users/rubber/linux/kernel/rcu/tree.c: 216
 * in most contexts. /Users/rubber/linux/kernel/rcu/tree.c: 217
 * Return true if an RCU grace period is in progress.  The READ_ONCE()s /Users/rubber/linux/kernel/rcu/tree.c: 225
 * permit this function to be invoked without holding the root rcu_node /Users/rubber/linux/kernel/rcu/tree.c: 226
 * structure's ->lock, but of course results can be subject to change. /Users/rubber/linux/kernel/rcu/tree.c: 227
 * Return the number of callbacks queued on the specified CPU. /Users/rubber/linux/kernel/rcu/tree.c: 235
 * Handles both the nocbs and normal cases. /Users/rubber/linux/kernel/rcu/tree.c: 236
 * Increment the current CPU's rcu_data structure's ->dynticks field /Users/rubber/linux/kernel/rcu/tree.c: 255
 * with ordering.  Return the new value. /Users/rubber/linux/kernel/rcu/tree.c: 256
 * Record entry into an extended quiescent state.  This is only to be /Users/rubber/linux/kernel/rcu/tree.c: 264
 * called when not already in an extended quiescent state, that is, /Users/rubber/linux/kernel/rcu/tree.c: 265
 * RCU is watching prior to the call to this function and is no longer /Users/rubber/linux/kernel/rcu/tree.c: 266
 * watching upon return. /Users/rubber/linux/kernel/rcu/tree.c: 267
	/* /Users/rubber/linux/kernel/rcu/tree.c: 273
	 * CPUs seeing atomic_add_return() must see prior RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 274
	 * critical sections, and we also must force ordering with the /Users/rubber/linux/kernel/rcu/tree.c: 275
	 * next idle sojourn. /Users/rubber/linux/kernel/rcu/tree.c: 276
 Before ->dynticks update! /Users/rubber/linux/kernel/rcu/tree.c: 278
 RCU is no longer watching.  Better be in extended quiescent state! /Users/rubber/linux/kernel/rcu/tree.c: 280
 * Record exit from an extended quiescent state.  This is only to be /Users/rubber/linux/kernel/rcu/tree.c: 285
 * called from an extended quiescent state, that is, RCU is not watching /Users/rubber/linux/kernel/rcu/tree.c: 286
 * prior to the call to this function and is watching upon return. /Users/rubber/linux/kernel/rcu/tree.c: 287
	/* /Users/rubber/linux/kernel/rcu/tree.c: 293
	 * CPUs seeing atomic_add_return() must see prior idle sojourns, /Users/rubber/linux/kernel/rcu/tree.c: 294
	 * and we also must force ordering with the next RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 295
	 * critical section. /Users/rubber/linux/kernel/rcu/tree.c: 296
 RCU is now watching.  Better not be in an extended quiescent state! /Users/rubber/linux/kernel/rcu/tree.c: 299
 After ->dynticks update! /Users/rubber/linux/kernel/rcu/tree.c: 300
 * Reset the current CPU's ->dynticks counter to indicate that the /Users/rubber/linux/kernel/rcu/tree.c: 305
 * newly onlined CPU is no longer in an extended quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 306
 * This will either leave the counter unchanged, or increment it /Users/rubber/linux/kernel/rcu/tree.c: 307
 * to the next non-quiescent value. /Users/rubber/linux/kernel/rcu/tree.c: 308
 * The non-atomic test/increment sequence works because the upper bits /Users/rubber/linux/kernel/rcu/tree.c: 310
 * of the ->dynticks counter are manipulated only by the corresponding CPU, /Users/rubber/linux/kernel/rcu/tree.c: 311
 * or when the corresponding CPU is offline. /Users/rubber/linux/kernel/rcu/tree.c: 312
 * Is the current CPU in an extended quiescent state? /Users/rubber/linux/kernel/rcu/tree.c: 324
 * No ordering, as we are sampling CPU-local information. /Users/rubber/linux/kernel/rcu/tree.c: 326
 * Snapshot the ->dynticks counter with full ordering so as to allow /Users/rubber/linux/kernel/rcu/tree.c: 334
 * stable comparison of this counter with past and future snapshots. /Users/rubber/linux/kernel/rcu/tree.c: 335
 Fundamental RCU ordering guarantee. /Users/rubber/linux/kernel/rcu/tree.c: 339
 * Return true if the snapshot returned from rcu_dynticks_snap() /Users/rubber/linux/kernel/rcu/tree.c: 344
 * indicates that RCU is in an extended quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 345
 * Return true if the CPU corresponding to the specified rcu_data /Users/rubber/linux/kernel/rcu/tree.c: 361
 * structure has spent some time in an extended quiescent state since /Users/rubber/linux/kernel/rcu/tree.c: 362
 * rcu_dynticks_snap() returned the specified snapshot. /Users/rubber/linux/kernel/rcu/tree.c: 363
 * Return true if the referenced integer is zero while the specified /Users/rubber/linux/kernel/rcu/tree.c: 371
 * CPU remains within a single extended quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 372
 If not quiescent, force back to earlier extended quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 379
 Order ->dynticks and *vp reads. /Users/rubber/linux/kernel/rcu/tree.c: 382
 Non-zero, so report failure; /Users/rubber/linux/kernel/rcu/tree.c: 384
 Order *vp read and ->dynticks re-read. /Users/rubber/linux/kernel/rcu/tree.c: 385
 If still in the same extended quiescent state, we are good! /Users/rubber/linux/kernel/rcu/tree.c: 387
 * Let the RCU core know that this CPU has gone through the scheduler, /Users/rubber/linux/kernel/rcu/tree.c: 392
 * which is a quiescent state.  This is called when the need for a /Users/rubber/linux/kernel/rcu/tree.c: 393
 * quiescent state is urgent, so we burn an atomic operation and full /Users/rubber/linux/kernel/rcu/tree.c: 394
 * memory barriers to let the RCU core know about it, regardless of what /Users/rubber/linux/kernel/rcu/tree.c: 395
 * this CPU might (or might not) do in the near future. /Users/rubber/linux/kernel/rcu/tree.c: 396
 * We inform the RCU core by emulating a zero-duration dyntick-idle period. /Users/rubber/linux/kernel/rcu/tree.c: 398
 * The caller must have disabled interrupts and must not be idle. /Users/rubber/linux/kernel/rcu/tree.c: 400
 * rcu_is_cpu_rrupt_from_idle - see if 'interrupted' from idle /Users/rubber/linux/kernel/rcu/tree.c: 415
 * If the current CPU is idle and running at a first-level (not nested) /Users/rubber/linux/kernel/rcu/tree.c: 417
 * interrupt, or directly, from idle, return true. /Users/rubber/linux/kernel/rcu/tree.c: 418
 * The caller must have at least disabled IRQs. /Users/rubber/linux/kernel/rcu/tree.c: 420
	/* /Users/rubber/linux/kernel/rcu/tree.c: 426
	 * Usually called from the tick; but also used from smp_function_call() /Users/rubber/linux/kernel/rcu/tree.c: 427
	 * for expedited grace periods. This latter can result in running from /Users/rubber/linux/kernel/rcu/tree.c: 428
	 * the idle task, instead of an actual IPI. /Users/rubber/linux/kernel/rcu/tree.c: 429
	/* /Users/rubber/linux/kernel/rcu/tree.c: 444
	 * If we're not in an interrupt, we must be in the idle task! /Users/rubber/linux/kernel/rcu/tree.c: 445
 Maximum callbacks per rcu_do_batch ... /Users/rubber/linux/kernel/rcu/tree.c: 454
 ... even during callback flood. /Users/rubber/linux/kernel/rcu/tree.c: 455
 If this many pending, ignore blimit. /Users/rubber/linux/kernel/rcu/tree.c: 457
 Once only this many pending, use blimit. /Users/rubber/linux/kernel/rcu/tree.c: 459
 If this many pending, hammer QS. /Users/rubber/linux/kernel/rcu/tree.c: 463
 No pre-initialization lock acquisitions! /Users/rubber/linux/kernel/rcu/tree.c: 464
 * How long the grace period must be before we start recruiting /Users/rubber/linux/kernel/rcu/tree.c: 482
 * quiescent-state help from rcu_note_context_switch(). /Users/rubber/linux/kernel/rcu/tree.c: 483
 * Make sure that we give the grace-period kthread time to detect any /Users/rubber/linux/kernel/rcu/tree.c: 491
 * idle CPUs before taking active measures to force quiescent states. /Users/rubber/linux/kernel/rcu/tree.c: 492
 * However, don't go below 100 milliseconds, adjusted upwards for really /Users/rubber/linux/kernel/rcu/tree.c: 493
 * large systems. /Users/rubber/linux/kernel/rcu/tree.c: 494
 * Return the number of RCU GPs completed thus far for debug & stats. /Users/rubber/linux/kernel/rcu/tree.c: 556
 * Return the number of RCU expedited batches completed thus far for /Users/rubber/linux/kernel/rcu/tree.c: 565
 * debug & stats.  Odd numbers mean that a batch is in progress, even /Users/rubber/linux/kernel/rcu/tree.c: 566
 * numbers mean idle.  The value returned will thus be roughly double /Users/rubber/linux/kernel/rcu/tree.c: 567
 * the cumulative batches since boot. /Users/rubber/linux/kernel/rcu/tree.c: 568
 * Return the root node of the rcu_state structure. /Users/rubber/linux/kernel/rcu/tree.c: 577
 * Send along grace-period-related data for rcutorture diagnostics. /Users/rubber/linux/kernel/rcu/tree.c: 585
 * Enter an RCU extended quiescent state, which can be either the /Users/rubber/linux/kernel/rcu/tree.c: 602
 * idle loop or adaptive-tickless usermode execution. /Users/rubber/linux/kernel/rcu/tree.c: 603
 * We crowbar the ->dynticks_nmi_nesting field to zero to allow for /Users/rubber/linux/kernel/rcu/tree.c: 605
 * the possibility of usermode upcalls having messed up our count /Users/rubber/linux/kernel/rcu/tree.c: 606
 * of interrupt nesting level during the prior busy period. /Users/rubber/linux/kernel/rcu/tree.c: 607
 RCU will still be watching, so just do accounting and leave. /Users/rubber/linux/kernel/rcu/tree.c: 618
 instrumentation for the noinstr rcu_dynticks_eqs_enter() /Users/rubber/linux/kernel/rcu/tree.c: 630
 RCU is watching here ... /Users/rubber/linux/kernel/rcu/tree.c: 635
 ... but is no longer watching here. /Users/rubber/linux/kernel/rcu/tree.c: 637
 * rcu_idle_enter - inform RCU that current CPU is entering idle /Users/rubber/linux/kernel/rcu/tree.c: 642
 * Enter idle mode, in other words, -leave- the mode in which RCU /Users/rubber/linux/kernel/rcu/tree.c: 644
 * read-side critical sections can occur.  (Though RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 645
 * critical sections can occur in irq handlers in idle, a possibility /Users/rubber/linux/kernel/rcu/tree.c: 646
 * handled by irq_enter() and irq_exit().) /Users/rubber/linux/kernel/rcu/tree.c: 647
 * If you add or remove a call to rcu_idle_enter(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 649
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 650
 * An empty function that will trigger a reschedule on /Users/rubber/linux/kernel/rcu/tree.c: 663
 * IRQ tail once IRQs get re-enabled on userspace/guest resume. /Users/rubber/linux/kernel/rcu/tree.c: 664
 * If either: /Users/rubber/linux/kernel/rcu/tree.c: 674
 * 1) the task is about to enter in guest mode and $ARCH doesn't support KVM generic work /Users/rubber/linux/kernel/rcu/tree.c: 676
 * 2) the task is about to enter in user mode and $ARCH doesn't support generic entry. /Users/rubber/linux/kernel/rcu/tree.c: 677
 * In these cases the late RCU wake ups aren't supported in the resched loops and our /Users/rubber/linux/kernel/rcu/tree.c: 679
 * last resort is to fire a local irq_work that will trigger a reschedule once IRQs /Users/rubber/linux/kernel/rcu/tree.c: 680
 * get re-enabled again. /Users/rubber/linux/kernel/rcu/tree.c: 681
 * rcu_user_enter - inform RCU that we are resuming userspace. /Users/rubber/linux/kernel/rcu/tree.c: 705
 * Enter RCU idle mode right before resuming userspace.  No use of RCU /Users/rubber/linux/kernel/rcu/tree.c: 707
 * is permitted between this call and rcu_user_exit(). This way the /Users/rubber/linux/kernel/rcu/tree.c: 708
 * CPU doesn't need to maintain the tick for RCU maintenance purposes /Users/rubber/linux/kernel/rcu/tree.c: 709
 * when the CPU runs in userspace. /Users/rubber/linux/kernel/rcu/tree.c: 710
 * If you add or remove a call to rcu_user_enter(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 712
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 713
	/* /Users/rubber/linux/kernel/rcu/tree.c: 719
	 * Other than generic entry implementation, we may be past the last /Users/rubber/linux/kernel/rcu/tree.c: 720
	 * rescheduling opportunity in the entry code. Trigger a self IPI /Users/rubber/linux/kernel/rcu/tree.c: 721
	 * that will fire and reschedule once we resume in user/guest mode. /Users/rubber/linux/kernel/rcu/tree.c: 722
 * rcu_nmi_exit - inform RCU of exit from NMI context /Users/rubber/linux/kernel/rcu/tree.c: 731
 * If we are returning from the outermost NMI handler that interrupted an /Users/rubber/linux/kernel/rcu/tree.c: 733
 * RCU-idle period, update rdp->dynticks and rdp->dynticks_nmi_nesting /Users/rubber/linux/kernel/rcu/tree.c: 734
 * to let the RCU grace-period handling know that the CPU is back to /Users/rubber/linux/kernel/rcu/tree.c: 735
 * being RCU-idle. /Users/rubber/linux/kernel/rcu/tree.c: 736
 * If you add or remove a call to rcu_nmi_exit(), be sure to test /Users/rubber/linux/kernel/rcu/tree.c: 738
 * with CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 739
	/* /Users/rubber/linux/kernel/rcu/tree.c: 746
	 * Check for ->dynticks_nmi_nesting underflow and bad ->dynticks. /Users/rubber/linux/kernel/rcu/tree.c: 747
	 * (We are exiting an NMI handler, so RCU better be paying attention /Users/rubber/linux/kernel/rcu/tree.c: 748
	 * to us!) /Users/rubber/linux/kernel/rcu/tree.c: 749
	/* /Users/rubber/linux/kernel/rcu/tree.c: 754
	 * If the nesting level is not 1, the CPU wasn't RCU-idle, so /Users/rubber/linux/kernel/rcu/tree.c: 755
	 * leave it in non-RCU-idle state. /Users/rubber/linux/kernel/rcu/tree.c: 756
 instrumentation for the noinstr rcu_dynticks_eqs_enter() /Users/rubber/linux/kernel/rcu/tree.c: 774
 RCU is watching here ... /Users/rubber/linux/kernel/rcu/tree.c: 778
 ... but is no longer watching here. /Users/rubber/linux/kernel/rcu/tree.c: 780
 * rcu_irq_exit - inform RCU that current CPU is exiting irq towards idle /Users/rubber/linux/kernel/rcu/tree.c: 787
 * Exit from an interrupt handler, which might possibly result in entering /Users/rubber/linux/kernel/rcu/tree.c: 789
 * idle mode, in other words, leaving the mode in which read-side critical /Users/rubber/linux/kernel/rcu/tree.c: 790
 * sections can occur.  The caller must have disabled interrupts. /Users/rubber/linux/kernel/rcu/tree.c: 791
 * This code assumes that the idle loop never does anything that might /Users/rubber/linux/kernel/rcu/tree.c: 793
 * result in unbalanced calls to irq_enter() and irq_exit().  If your /Users/rubber/linux/kernel/rcu/tree.c: 794
 * architecture's idle loop violates this assumption, RCU will give you what /Users/rubber/linux/kernel/rcu/tree.c: 795
 * you deserve, good and hard.  But very infrequently and irreproducibly. /Users/rubber/linux/kernel/rcu/tree.c: 796
 * Use things like work queues to work around this limitation. /Users/rubber/linux/kernel/rcu/tree.c: 798
 * You have been warned. /Users/rubber/linux/kernel/rcu/tree.c: 800
 * If you add or remove a call to rcu_irq_exit(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 802
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 803
 * rcu_irq_exit_check_preempt - Validate that scheduling is possible /Users/rubber/linux/kernel/rcu/tree.c: 813
 * Wrapper for rcu_irq_exit() where interrupts are enabled. /Users/rubber/linux/kernel/rcu/tree.c: 830
 * If you add or remove a call to rcu_irq_exit_irqson(), be sure to test /Users/rubber/linux/kernel/rcu/tree.c: 832
 * with CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 833
 * Exit an RCU extended quiescent state, which can be either the /Users/rubber/linux/kernel/rcu/tree.c: 845
 * idle loop or adaptive-tickless usermode execution. /Users/rubber/linux/kernel/rcu/tree.c: 846
 * We crowbar the ->dynticks_nmi_nesting field to DYNTICK_IRQ_NONIDLE to /Users/rubber/linux/kernel/rcu/tree.c: 848
 * allow for the possibility of usermode upcalls messing up our count of /Users/rubber/linux/kernel/rcu/tree.c: 849
 * interrupt nesting level during the busy period that is just now starting. /Users/rubber/linux/kernel/rcu/tree.c: 850
 RCU was already watching, so just do accounting and leave. /Users/rubber/linux/kernel/rcu/tree.c: 862
 RCU is not watching here ... /Users/rubber/linux/kernel/rcu/tree.c: 867
 ... but is watching here. /Users/rubber/linux/kernel/rcu/tree.c: 869
 instrumentation for the noinstr rcu_dynticks_eqs_exit() /Users/rubber/linux/kernel/rcu/tree.c: 872
 * rcu_idle_exit - inform RCU that current CPU is leaving idle /Users/rubber/linux/kernel/rcu/tree.c: 885
 * Exit idle mode, in other words, -enter- the mode in which RCU /Users/rubber/linux/kernel/rcu/tree.c: 887
 * read-side critical sections can occur. /Users/rubber/linux/kernel/rcu/tree.c: 888
 * If you add or remove a call to rcu_idle_exit(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 890
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 891
 * rcu_user_exit - inform RCU that we are exiting userspace. /Users/rubber/linux/kernel/rcu/tree.c: 905
 * Exit RCU idle mode while entering the kernel because it can /Users/rubber/linux/kernel/rcu/tree.c: 907
 * run a RCU read side critical section anytime. /Users/rubber/linux/kernel/rcu/tree.c: 908
 * If you add or remove a call to rcu_user_exit(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 910
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 911
 * __rcu_irq_enter_check_tick - Enable scheduler tick on CPU if RCU needs it. /Users/rubber/linux/kernel/rcu/tree.c: 919
 * The scheduler tick is not normally enabled when CPUs enter the kernel /Users/rubber/linux/kernel/rcu/tree.c: 921
 * from nohz_full userspace execution.  After all, nohz_full userspace /Users/rubber/linux/kernel/rcu/tree.c: 922
 * execution is an RCU quiescent state and the time executing in the kernel /Users/rubber/linux/kernel/rcu/tree.c: 923
 * is quite short.  Except of course when it isn't.  And it is not hard to /Users/rubber/linux/kernel/rcu/tree.c: 924
 * cause a large system to spend tens of seconds or even minutes looping /Users/rubber/linux/kernel/rcu/tree.c: 925
 * in the kernel, which can cause a number of problems, include RCU CPU /Users/rubber/linux/kernel/rcu/tree.c: 926
 * stall warnings. /Users/rubber/linux/kernel/rcu/tree.c: 927
 * Therefore, if a nohz_full CPU fails to report a quiescent state /Users/rubber/linux/kernel/rcu/tree.c: 929
 * in a timely manner, the RCU grace-period kthread sets that CPU's /Users/rubber/linux/kernel/rcu/tree.c: 930
 * ->rcu_urgent_qs flag with the expectation that the next interrupt or /Users/rubber/linux/kernel/rcu/tree.c: 931
 * exception will invoke this function, which will turn on the scheduler /Users/rubber/linux/kernel/rcu/tree.c: 932
 * tick, which will enable RCU to detect that CPU's quiescent states, /Users/rubber/linux/kernel/rcu/tree.c: 933
 * for example, due to cond_resched() calls in CONFIG_PREEMPT=n kernels. /Users/rubber/linux/kernel/rcu/tree.c: 934
 * The tick will be disabled once a quiescent state is reported for /Users/rubber/linux/kernel/rcu/tree.c: 935
 * this CPU. /Users/rubber/linux/kernel/rcu/tree.c: 936
 * Of course, in carefully tuned systems, there might never be an /Users/rubber/linux/kernel/rcu/tree.c: 938
 * interrupt or exception.  In that case, the RCU grace-period kthread /Users/rubber/linux/kernel/rcu/tree.c: 939
 * will eventually cause one to happen.  However, in less carefully /Users/rubber/linux/kernel/rcu/tree.c: 940
 * controlled environments, this function allows RCU to get what it /Users/rubber/linux/kernel/rcu/tree.c: 941
 * needs without creating otherwise useless interruptions. /Users/rubber/linux/kernel/rcu/tree.c: 942
 If we're here from NMI there's nothing to do. /Users/rubber/linux/kernel/rcu/tree.c: 948
 RCU doesn't need nohz_full help from this CPU, or it is /Users/rubber/linux/kernel/rcu/tree.c: 958
 already getting that help. /Users/rubber/linux/kernel/rcu/tree.c: 959
 We get here only when not in an extended quiescent state and /Users/rubber/linux/kernel/rcu/tree.c: 963
 from interrupts (as opposed to NMIs).  Therefore, (1) RCU is /Users/rubber/linux/kernel/rcu/tree.c: 964
 already watching and (2) The fact that we are in an interrupt /Users/rubber/linux/kernel/rcu/tree.c: 965
 handler and that the rcu_node lock is an irq-disabled lock /Users/rubber/linux/kernel/rcu/tree.c: 966
 prevents self-deadlock.  So we can safely recheck under the lock. /Users/rubber/linux/kernel/rcu/tree.c: 967
 Note that the nohz_full state currently cannot change. /Users/rubber/linux/kernel/rcu/tree.c: 968
 A nohz_full CPU is in the kernel and RCU needs a /Users/rubber/linux/kernel/rcu/tree.c: 971
 quiescent state.  Turn on the tick! /Users/rubber/linux/kernel/rcu/tree.c: 972
 * rcu_nmi_enter - inform RCU of entry to NMI context /Users/rubber/linux/kernel/rcu/tree.c: 981
 * If the CPU was idle from RCU's viewpoint, update rdp->dynticks and /Users/rubber/linux/kernel/rcu/tree.c: 983
 * rdp->dynticks_nmi_nesting to let the RCU grace-period handling know /Users/rubber/linux/kernel/rcu/tree.c: 984
 * that the CPU is active.  This implementation permits nested NMIs, as /Users/rubber/linux/kernel/rcu/tree.c: 985
 * long as the nesting level does not overflow an int.  (You will probably /Users/rubber/linux/kernel/rcu/tree.c: 986
 * run out of stack space first.) /Users/rubber/linux/kernel/rcu/tree.c: 987
 * If you add or remove a call to rcu_nmi_enter(), be sure to test /Users/rubber/linux/kernel/rcu/tree.c: 989
 * with CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 990
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1000
	 * If idle from RCU viewpoint, atomically increment ->dynticks /Users/rubber/linux/kernel/rcu/tree.c: 1001
	 * to mark non-idle and increment ->dynticks_nmi_nesting by one. /Users/rubber/linux/kernel/rcu/tree.c: 1002
	 * Otherwise, increment ->dynticks_nmi_nesting by two.  This means /Users/rubber/linux/kernel/rcu/tree.c: 1003
	 * if ->dynticks_nmi_nesting is equal to one, we are guaranteed /Users/rubber/linux/kernel/rcu/tree.c: 1004
	 * to be in the outermost NMI handler that interrupted an RCU-idle /Users/rubber/linux/kernel/rcu/tree.c: 1005
	 * period (observation due to Andy Lutomirski). /Users/rubber/linux/kernel/rcu/tree.c: 1006
 RCU is not watching here ... /Users/rubber/linux/kernel/rcu/tree.c: 1013
 ... but is watching here. /Users/rubber/linux/kernel/rcu/tree.c: 1015
 instrumentation for the noinstr rcu_dynticks_curr_cpu_in_eqs() /Users/rubber/linux/kernel/rcu/tree.c: 1024
 instrumentation for the noinstr rcu_dynticks_eqs_exit() /Users/rubber/linux/kernel/rcu/tree.c: 1026
 * rcu_irq_enter - inform RCU that current CPU is entering irq away from idle /Users/rubber/linux/kernel/rcu/tree.c: 1047
 * Enter an interrupt handler, which might possibly result in exiting /Users/rubber/linux/kernel/rcu/tree.c: 1049
 * idle mode, in other words, entering the mode in which read-side critical /Users/rubber/linux/kernel/rcu/tree.c: 1050
 * sections can occur.  The caller must have disabled interrupts. /Users/rubber/linux/kernel/rcu/tree.c: 1051
 * Note that the Linux kernel is fully capable of entering an interrupt /Users/rubber/linux/kernel/rcu/tree.c: 1053
 * handler that it never exits, for example when doing upcalls to user mode! /Users/rubber/linux/kernel/rcu/tree.c: 1054
 * This code assumes that the idle loop never does upcalls to user mode. /Users/rubber/linux/kernel/rcu/tree.c: 1055
 * If your architecture's idle loop does do upcalls to user mode (or does /Users/rubber/linux/kernel/rcu/tree.c: 1056
 * anything else that results in unbalanced calls to the irq_enter() and /Users/rubber/linux/kernel/rcu/tree.c: 1057
 * irq_exit() functions), RCU will give you what you deserve, good and hard. /Users/rubber/linux/kernel/rcu/tree.c: 1058
 * But very infrequently and irreproducibly. /Users/rubber/linux/kernel/rcu/tree.c: 1059
 * Use things like work queues to work around this limitation. /Users/rubber/linux/kernel/rcu/tree.c: 1061
 * You have been warned. /Users/rubber/linux/kernel/rcu/tree.c: 1063
 * If you add or remove a call to rcu_irq_enter(), be sure to test with /Users/rubber/linux/kernel/rcu/tree.c: 1065
 * CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 1066
 * Wrapper for rcu_irq_enter() where interrupts are enabled. /Users/rubber/linux/kernel/rcu/tree.c: 1075
 * If you add or remove a call to rcu_irq_enter_irqson(), be sure to test /Users/rubber/linux/kernel/rcu/tree.c: 1077
 * with CONFIG_RCU_EQS_DEBUG=y. /Users/rubber/linux/kernel/rcu/tree.c: 1078
 * If any sort of urgency was applied to the current CPU (for example, /Users/rubber/linux/kernel/rcu/tree.c: 1090
 * the scheduler-clock interrupt was enabled on a nohz_full CPU) in order /Users/rubber/linux/kernel/rcu/tree.c: 1091
 * to get to a quiescent state, disable it. /Users/rubber/linux/kernel/rcu/tree.c: 1092
 * rcu_is_watching - see if RCU thinks that the current CPU is not idle /Users/rubber/linux/kernel/rcu/tree.c: 1106
 * Return true if RCU is watching the running CPU, which means that this /Users/rubber/linux/kernel/rcu/tree.c: 1108
 * CPU can safely enter RCU read-side critical sections.  In other words, /Users/rubber/linux/kernel/rcu/tree.c: 1109
 * if the current CPU is not in its idle loop or is in an interrupt or /Users/rubber/linux/kernel/rcu/tree.c: 1110
 * NMI handler, return true. /Users/rubber/linux/kernel/rcu/tree.c: 1111
 * Make notrace because it can be called by the internal functions of /Users/rubber/linux/kernel/rcu/tree.c: 1113
 * ftrace, and making this notrace removes unnecessary recursion calls. /Users/rubber/linux/kernel/rcu/tree.c: 1114
 * If a holdout task is actually running, request an urgent quiescent /Users/rubber/linux/kernel/rcu/tree.c: 1128
 * state from its CPU.  This is unsynchronized, so migrations can cause /Users/rubber/linux/kernel/rcu/tree.c: 1129
 * the request to go to the wrong CPU.  Which is OK, all that will happen /Users/rubber/linux/kernel/rcu/tree.c: 1130
 * is that the CPU's next context switch will be a bit slower and next /Users/rubber/linux/kernel/rcu/tree.c: 1131
 * time around this task will generate another request. /Users/rubber/linux/kernel/rcu/tree.c: 1132
 * Is the current CPU online as far as RCU is concerned? /Users/rubber/linux/kernel/rcu/tree.c: 1148
 * Disable preemption to avoid false positives that could otherwise /Users/rubber/linux/kernel/rcu/tree.c: 1150
 * happen due to the current CPU number being sampled, this task being /Users/rubber/linux/kernel/rcu/tree.c: 1151
 * preempted, its old CPU being taken offline, resuming on some other CPU, /Users/rubber/linux/kernel/rcu/tree.c: 1152
 * then determining that its old CPU is now offline. /Users/rubber/linux/kernel/rcu/tree.c: 1153
 * Disable checking if in an NMI handler because we cannot safely /Users/rubber/linux/kernel/rcu/tree.c: 1155
 * report errors from NMI handlers anyway.  In addition, it is OK to use /Users/rubber/linux/kernel/rcu/tree.c: 1156
 * RCU on an offline processor during initial boot, hence the check for /Users/rubber/linux/kernel/rcu/tree.c: 1157
 * rcu_scheduler_fully_active. /Users/rubber/linux/kernel/rcu/tree.c: 1158
 * When trying to report a quiescent state on behalf of some other CPU, /Users/rubber/linux/kernel/rcu/tree.c: 1181
 * it is our responsibility to check for and handle potential overflow /Users/rubber/linux/kernel/rcu/tree.c: 1182
 * of the rcu_node ->gp_seq counter with respect to the rcu_data counters. /Users/rubber/linux/kernel/rcu/tree.c: 1183
 * After all, the CPU might be in deep idle state, and thus executing no /Users/rubber/linux/kernel/rcu/tree.c: 1184
 * code whatsoever. /Users/rubber/linux/kernel/rcu/tree.c: 1185
 * Snapshot the specified CPU's dynticks counter so that we can later /Users/rubber/linux/kernel/rcu/tree.c: 1198
 * credit them with an implicit quiescent state.  Return 1 if this CPU /Users/rubber/linux/kernel/rcu/tree.c: 1199
 * is in dynticks idle mode, which is an extended quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 1200
 * Return true if the specified CPU has passed through a quiescent /Users/rubber/linux/kernel/rcu/tree.c: 1214
 * state by virtue of being in or having passed through an dynticks /Users/rubber/linux/kernel/rcu/tree.c: 1215
 * idle state since the last call to dyntick_save_progress_counter() /Users/rubber/linux/kernel/rcu/tree.c: 1216
 * for this same CPU, or by virtue of having been offline. /Users/rubber/linux/kernel/rcu/tree.c: 1217
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1224
	 * If the CPU passed through or entered a dynticks idle phase with /Users/rubber/linux/kernel/rcu/tree.c: 1225
	 * no active irq/NMI handlers, then we can safely pretend that the CPU /Users/rubber/linux/kernel/rcu/tree.c: 1226
	 * already acknowledged the request to pass through a quiescent /Users/rubber/linux/kernel/rcu/tree.c: 1227
	 * state.  Either way, that CPU cannot possibly be in an RCU /Users/rubber/linux/kernel/rcu/tree.c: 1228
	 * read-side critical section that started before the beginning /Users/rubber/linux/kernel/rcu/tree.c: 1229
	 * of the current RCU grace period. /Users/rubber/linux/kernel/rcu/tree.c: 1230
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1238
	 * Complain if a CPU that is considered to be offline from RCU's /Users/rubber/linux/kernel/rcu/tree.c: 1239
	 * perspective has not yet reported a quiescent state.  After all, /Users/rubber/linux/kernel/rcu/tree.c: 1240
	 * the offline CPU should have reported a quiescent state during /Users/rubber/linux/kernel/rcu/tree.c: 1241
	 * the CPU-offline process, or, failing that, by rcu_gp_init() /Users/rubber/linux/kernel/rcu/tree.c: 1242
	 * if it ran concurrently with either the CPU going offline or the /Users/rubber/linux/kernel/rcu/tree.c: 1243
	 * last task on a leaf rcu_node structure exiting its RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 1244
	 * critical section while all CPUs corresponding to that structure /Users/rubber/linux/kernel/rcu/tree.c: 1245
	 * are offline.  This added warning detects bugs in any of these /Users/rubber/linux/kernel/rcu/tree.c: 1246
	 * code paths. /Users/rubber/linux/kernel/rcu/tree.c: 1247
	 * /Users/rubber/linux/kernel/rcu/tree.c: 1248
	 * The rcu_node structure's ->lock is held here, which excludes /Users/rubber/linux/kernel/rcu/tree.c: 1249
	 * the relevant portions the CPU-hotplug code, the grace-period /Users/rubber/linux/kernel/rcu/tree.c: 1250
	 * initialization code, and the rcu_read_unlock() code paths. /Users/rubber/linux/kernel/rcu/tree.c: 1251
	 * /Users/rubber/linux/kernel/rcu/tree.c: 1252
	 * For more detail, please refer to the "Hotplug CPU" section /Users/rubber/linux/kernel/rcu/tree.c: 1253
	 * of RCU's Requirements documentation. /Users/rubber/linux/kernel/rcu/tree.c: 1254
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1274
	 * A CPU running for an extended time within the kernel can /Users/rubber/linux/kernel/rcu/tree.c: 1275
	 * delay RCU grace periods: (1) At age jiffies_to_sched_qs, /Users/rubber/linux/kernel/rcu/tree.c: 1276
	 * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set /Users/rubber/linux/kernel/rcu/tree.c: 1277
	 * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the /Users/rubber/linux/kernel/rcu/tree.c: 1278
	 * unsynchronized assignments to the per-CPU rcu_need_heavy_qs /Users/rubber/linux/kernel/rcu/tree.c: 1279
	 * variable are safe because the assignments are repeated if this /Users/rubber/linux/kernel/rcu/tree.c: 1280
	 * CPU failed to pass through a quiescent state.  This code /Users/rubber/linux/kernel/rcu/tree.c: 1281
	 * also checks .jiffies_resched in case jiffies_to_sched_qs /Users/rubber/linux/kernel/rcu/tree.c: 1282
	 * is set way high. /Users/rubber/linux/kernel/rcu/tree.c: 1283
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1297
	 * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq! /Users/rubber/linux/kernel/rcu/tree.c: 1298
	 * The above code handles this, but only for straight cond_resched(). /Users/rubber/linux/kernel/rcu/tree.c: 1299
	 * And some in-kernel loops check need_resched() before calling /Users/rubber/linux/kernel/rcu/tree.c: 1300
	 * cond_resched(), which defeats the above code for CPUs that are /Users/rubber/linux/kernel/rcu/tree.c: 1301
	 * running in-kernel with scheduling-clock interrupts disabled. /Users/rubber/linux/kernel/rcu/tree.c: 1302
	 * So hit them over the head with the resched_cpu() hammer! /Users/rubber/linux/kernel/rcu/tree.c: 1303
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1313
	 * If more than halfway to RCU CPU stall-warning time, invoke /Users/rubber/linux/kernel/rcu/tree.c: 1314
	 * resched_cpu() more frequently to try to loosen things up a bit. /Users/rubber/linux/kernel/rcu/tree.c: 1315
	 * Also check to see if the CPU is getting hammered with interrupts, /Users/rubber/linux/kernel/rcu/tree.c: 1316
	 * but only once per grace period, just to keep the IPIs down to /Users/rubber/linux/kernel/rcu/tree.c: 1317
	 * a dull roar. /Users/rubber/linux/kernel/rcu/tree.c: 1318
 * rcu_start_this_gp - Request the start of a particular grace period /Users/rubber/linux/kernel/rcu/tree.c: 1348
 * @rnp_start: The leaf node of the CPU from which to start. /Users/rubber/linux/kernel/rcu/tree.c: 1349
 * @rdp: The rcu_data corresponding to the CPU from which to start. /Users/rubber/linux/kernel/rcu/tree.c: 1350
 * @gp_seq_req: The gp_seq of the grace period to start. /Users/rubber/linux/kernel/rcu/tree.c: 1351
 * Start the specified grace period, as needed to handle newly arrived /Users/rubber/linux/kernel/rcu/tree.c: 1353
 * callbacks.  The required future grace periods are recorded in each /Users/rubber/linux/kernel/rcu/tree.c: 1354
 * rcu_node structure's ->gp_seq_needed field.  Returns true if there /Users/rubber/linux/kernel/rcu/tree.c: 1355
 * is reason to awaken the grace-period kthread. /Users/rubber/linux/kernel/rcu/tree.c: 1356
 * The caller must hold the specified rcu_node structure's ->lock, which /Users/rubber/linux/kernel/rcu/tree.c: 1358
 * is why the caller is responsible for waking the grace-period kthread. /Users/rubber/linux/kernel/rcu/tree.c: 1359
 * Returns true if the GP thread needs to be awakened else false. /Users/rubber/linux/kernel/rcu/tree.c: 1361
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1369
	 * Use funnel locking to either acquire the root rcu_node /Users/rubber/linux/kernel/rcu/tree.c: 1370
	 * structure's lock or bail out if the need for this grace period /Users/rubber/linux/kernel/rcu/tree.c: 1371
	 * has already been recorded -- or if that grace period has in /Users/rubber/linux/kernel/rcu/tree.c: 1372
	 * fact already started.  If there is already a grace period in /Users/rubber/linux/kernel/rcu/tree.c: 1373
	 * progress in a non-leaf node, no recording is needed because the /Users/rubber/linux/kernel/rcu/tree.c: 1374
	 * end of the grace period will scan the leaf rcu_node structures. /Users/rubber/linux/kernel/rcu/tree.c: 1375
	 * Note that rnp_start->lock must not be released. /Users/rubber/linux/kernel/rcu/tree.c: 1376
			/* /Users/rubber/linux/kernel/rcu/tree.c: 1393
			 * We just marked the leaf or internal node, and a /Users/rubber/linux/kernel/rcu/tree.c: 1394
			 * grace period is in progress, which means that /Users/rubber/linux/kernel/rcu/tree.c: 1395
			 * rcu_gp_cleanup() will see the marking.  Bail to /Users/rubber/linux/kernel/rcu/tree.c: 1396
			 * reduce contention. /Users/rubber/linux/kernel/rcu/tree.c: 1397
 * Clean up any old requests for the just-ended grace period.  Also return /Users/rubber/linux/kernel/rcu/tree.c: 1435
 * whether any additional grace periods have been requested. /Users/rubber/linux/kernel/rcu/tree.c: 1436
 * Awaken the grace-period kthread.  Don't do a self-awaken (unless in an /Users/rubber/linux/kernel/rcu/tree.c: 1452
 * interrupt or softirq handler, in which case we just might immediately /Users/rubber/linux/kernel/rcu/tree.c: 1453
 * sleep upon return, resulting in a grace-period hang), and don't bother /Users/rubber/linux/kernel/rcu/tree.c: 1454
 * awakening when there is nothing for the grace-period kthread to do /Users/rubber/linux/kernel/rcu/tree.c: 1455
 * (as in several CPUs raced to awaken, we lost), and finally don't try /Users/rubber/linux/kernel/rcu/tree.c: 1456
 * to awaken a kthread that has not yet been created.  If all those checks /Users/rubber/linux/kernel/rcu/tree.c: 1457
 * are passed, track some debug information and awaken. /Users/rubber/linux/kernel/rcu/tree.c: 1458
 * So why do the self-wakeup when in an interrupt or softirq handler /Users/rubber/linux/kernel/rcu/tree.c: 1460
 * in the grace-period kthread's context?  Because the kthread might have /Users/rubber/linux/kernel/rcu/tree.c: 1461
 * been interrupted just as it was going to sleep, and just after the final /Users/rubber/linux/kernel/rcu/tree.c: 1462
 * pre-sleep check of the awaken condition.  In this case, a wakeup really /Users/rubber/linux/kernel/rcu/tree.c: 1463
 * is required, and is therefore supplied. /Users/rubber/linux/kernel/rcu/tree.c: 1464
 * If there is room, assign a ->gp_seq number to any callbacks on this /Users/rubber/linux/kernel/rcu/tree.c: 1479
 * CPU that have not already been assigned.  Also accelerate any callbacks /Users/rubber/linux/kernel/rcu/tree.c: 1480
 * that were previously assigned a ->gp_seq number that has since proven /Users/rubber/linux/kernel/rcu/tree.c: 1481
 * to be too conservative, which can happen if callbacks get assigned a /Users/rubber/linux/kernel/rcu/tree.c: 1482
 * ->gp_seq number while RCU is idle, but with reference to a non-root /Users/rubber/linux/kernel/rcu/tree.c: 1483
 * rcu_node structure.  This function is idempotent, so it does not hurt /Users/rubber/linux/kernel/rcu/tree.c: 1484
 * to call it repeatedly.  Returns an flag saying that we should awaken /Users/rubber/linux/kernel/rcu/tree.c: 1485
 * the RCU grace-period kthread. /Users/rubber/linux/kernel/rcu/tree.c: 1486
 * The caller must hold rnp->lock with interrupts disabled. /Users/rubber/linux/kernel/rcu/tree.c: 1488
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1504
	 * Callbacks are often registered with incomplete grace-period /Users/rubber/linux/kernel/rcu/tree.c: 1505
	 * information.  Something about the fact that getting exact /Users/rubber/linux/kernel/rcu/tree.c: 1506
	 * information requires acquiring a global lock...  RCU therefore /Users/rubber/linux/kernel/rcu/tree.c: 1507
	 * makes a conservative estimate of the grace period number at which /Users/rubber/linux/kernel/rcu/tree.c: 1508
	 * a given callback will become ready to invoke.	The following /Users/rubber/linux/kernel/rcu/tree.c: 1509
	 * code checks this estimate and improves it when possible, thus /Users/rubber/linux/kernel/rcu/tree.c: 1510
	 * accelerating callback invocation to an earlier grace-period /Users/rubber/linux/kernel/rcu/tree.c: 1511
	 * number. /Users/rubber/linux/kernel/rcu/tree.c: 1512
 * Similar to rcu_accelerate_cbs(), but does not require that the leaf /Users/rubber/linux/kernel/rcu/tree.c: 1530
 * rcu_node structure's ->lock be held.  It consults the cached value /Users/rubber/linux/kernel/rcu/tree.c: 1531
 * of ->gp_seq_needed in the rcu_data structure, and if that indicates /Users/rubber/linux/kernel/rcu/tree.c: 1532
 * that a new grace-period request be made, invokes rcu_accelerate_cbs() /Users/rubber/linux/kernel/rcu/tree.c: 1533
 * while holding the leaf rcu_node structure's ->lock. /Users/rubber/linux/kernel/rcu/tree.c: 1534
 * Move any callbacks whose grace period has completed to the /Users/rubber/linux/kernel/rcu/tree.c: 1557
 * RCU_DONE_TAIL sublist, then compact the remaining sublists and /Users/rubber/linux/kernel/rcu/tree.c: 1558
 * assign ->gp_seq numbers to any callbacks in the RCU_NEXT_TAIL /Users/rubber/linux/kernel/rcu/tree.c: 1559
 * sublist.  This function is idempotent, so it does not hurt to /Users/rubber/linux/kernel/rcu/tree.c: 1560
 * invoke it repeatedly.  As long as it is not invoked -too- often... /Users/rubber/linux/kernel/rcu/tree.c: 1561
 * Returns true if the RCU grace-period kthread needs to be awakened. /Users/rubber/linux/kernel/rcu/tree.c: 1562
 * The caller must hold rnp->lock with interrupts disabled. /Users/rubber/linux/kernel/rcu/tree.c: 1564
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1575
	 * Find all callbacks whose ->gp_seq numbers indicate that they /Users/rubber/linux/kernel/rcu/tree.c: 1576
	 * are ready to invoke, and put them into the RCU_DONE_TAIL sublist. /Users/rubber/linux/kernel/rcu/tree.c: 1577
 * Move and classify callbacks, but only if doing so won't require /Users/rubber/linux/kernel/rcu/tree.c: 1586
 * that the RCU grace-period kthread be awakened. /Users/rubber/linux/kernel/rcu/tree.c: 1587
 * In CONFIG_RCU_STRICT_GRACE_PERIOD=y kernels, attempt to generate a /Users/rubber/linux/kernel/rcu/tree.c: 1601
 * quiescent state.  This is intended to be invoked when the CPU notices /Users/rubber/linux/kernel/rcu/tree.c: 1602
 * a new grace period. /Users/rubber/linux/kernel/rcu/tree.c: 1603
 * Update CPU-local rcu_data state to record the beginnings and ends of /Users/rubber/linux/kernel/rcu/tree.c: 1614
 * grace periods.  The caller must hold the ->lock of the leaf rcu_node /Users/rubber/linux/kernel/rcu/tree.c: 1615
 * structure corresponding to the current CPU, and must have irqs disabled. /Users/rubber/linux/kernel/rcu/tree.c: 1616
 * Returns true if the grace-period kthread needs to be awakened. /Users/rubber/linux/kernel/rcu/tree.c: 1617
		/* /Users/rubber/linux/kernel/rcu/tree.c: 1647
		 * If the current grace period is waiting for this CPU, /Users/rubber/linux/kernel/rcu/tree.c: 1648
		 * set up to detect a quiescent state, otherwise don't /Users/rubber/linux/kernel/rcu/tree.c: 1649
		 * go looking for one. /Users/rubber/linux/kernel/rcu/tree.c: 1650
 * Handler for on_each_cpu() to invoke the target CPU's RCU core /Users/rubber/linux/kernel/rcu/tree.c: 1721
 * processing. /Users/rubber/linux/kernel/rcu/tree.c: 1722
 * Initialize a new grace period.  Return false if no grace period required. /Users/rubber/linux/kernel/rcu/tree.c: 1730
		/* /Users/rubber/linux/kernel/rcu/tree.c: 1751
		 * Grace period already in progress, don't start another. /Users/rubber/linux/kernel/rcu/tree.c: 1752
		 * Not supposed to be able to happen. /Users/rubber/linux/kernel/rcu/tree.c: 1753
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1767
	 * Apply per-leaf buffered online and offline operations to /Users/rubber/linux/kernel/rcu/tree.c: 1768
	 * the rcu_node tree. Note that this new grace period need not /Users/rubber/linux/kernel/rcu/tree.c: 1769
	 * wait for subsequent online CPUs, and that RCU hooks in the CPU /Users/rubber/linux/kernel/rcu/tree.c: 1770
	 * offlining path, when combined with checks in this function, /Users/rubber/linux/kernel/rcu/tree.c: 1771
	 * will handle CPUs that are currently going offline or that will /Users/rubber/linux/kernel/rcu/tree.c: 1772
	 * go offline later.  Please also refer to "Hotplug CPU" section /Users/rubber/linux/kernel/rcu/tree.c: 1773
	 * of RCU's Requirements documentation. /Users/rubber/linux/kernel/rcu/tree.c: 1774
 Wait for CPU-hotplug operations that might have /Users/rubber/linux/kernel/rcu/tree.c: 1778
 started before this grace period did. /Users/rubber/linux/kernel/rcu/tree.c: 1779
 Pair with barriers used when updating ->ofl_seq to odd values. /Users/rubber/linux/kernel/rcu/tree.c: 1780
 Can't wake unless RCU is watching. /Users/rubber/linux/kernel/rcu/tree.c: 1784
 Pair with barriers used when updating ->ofl_seq to even values. /Users/rubber/linux/kernel/rcu/tree.c: 1785
		/* /Users/rubber/linux/kernel/rcu/tree.c: 1812
		 * If all waited-on tasks from prior grace period are /Users/rubber/linux/kernel/rcu/tree.c: 1813
		 * done, and if all this rcu_node structure's CPUs are /Users/rubber/linux/kernel/rcu/tree.c: 1814
		 * still offline, propagate up the rcu_node tree and /Users/rubber/linux/kernel/rcu/tree.c: 1815
		 * clear ->wait_blkd_tasks.  Otherwise, if one of this /Users/rubber/linux/kernel/rcu/tree.c: 1816
		 * rcu_node structure's CPUs has since come back online, /Users/rubber/linux/kernel/rcu/tree.c: 1817
		 * simply clear ->wait_blkd_tasks. /Users/rubber/linux/kernel/rcu/tree.c: 1818
	/* /Users/rubber/linux/kernel/rcu/tree.c: 1832
	 * Set the quiescent-state-needed bits in all the rcu_node /Users/rubber/linux/kernel/rcu/tree.c: 1833
	 * structures for all currently online CPUs in breadth-first /Users/rubber/linux/kernel/rcu/tree.c: 1834
	 * order, starting from the root rcu_node structure, relying on the /Users/rubber/linux/kernel/rcu/tree.c: 1835
	 * layout of the tree within the rcu_state.node[] array.  Note that /Users/rubber/linux/kernel/rcu/tree.c: 1836
	 * other CPUs will access only the leaves of the hierarchy, thus /Users/rubber/linux/kernel/rcu/tree.c: 1837
	 * seeing that no grace period is in progress, at least until the /Users/rubber/linux/kernel/rcu/tree.c: 1838
	 * corresponding leaf node has been initialized. /Users/rubber/linux/kernel/rcu/tree.c: 1839
	 * /Users/rubber/linux/kernel/rcu/tree.c: 1840
	 * The grace period cannot complete until the initialization /Users/rubber/linux/kernel/rcu/tree.c: 1841
	 * process finishes, because this kthread handles both. /Users/rubber/linux/kernel/rcu/tree.c: 1842
 If strict, make all CPUs aware of new grace period. /Users/rubber/linux/kernel/rcu/tree.c: 1869
 * Helper function for swait_event_idle_exclusive() wakeup at force-quiescent-state /Users/rubber/linux/kernel/rcu/tree.c: 1877
 * time. /Users/rubber/linux/kernel/rcu/tree.c: 1878
 If under overload conditions, force an immediate FQS scan. /Users/rubber/linux/kernel/rcu/tree.c: 1884
 Someone like call_rcu() requested a force-quiescent-state scan. /Users/rubber/linux/kernel/rcu/tree.c: 1888
 The current grace period has completed. /Users/rubber/linux/kernel/rcu/tree.c: 1893
 * Do one round of quiescent-state forcing. /Users/rubber/linux/kernel/rcu/tree.c: 1901
 * Loop doing repeated quiescent-state forcing until the grace period ends. /Users/rubber/linux/kernel/rcu/tree.c: 1926
			/* /Users/rubber/linux/kernel/rcu/tree.c: 1944
			 * jiffies_force_qs before RCU_GP_WAIT_FQS state /Users/rubber/linux/kernel/rcu/tree.c: 1945
			 * update; required for stall checks. /Users/rubber/linux/kernel/rcu/tree.c: 1946
 * Clean up after the old grace period. /Users/rubber/linux/kernel/rcu/tree.c: 2000
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2020
	 * We know the grace period is complete, but to everyone else /Users/rubber/linux/kernel/rcu/tree.c: 2021
	 * it appears to still be ongoing.  But it is also the case /Users/rubber/linux/kernel/rcu/tree.c: 2022
	 * that to everyone else it looks like there is nothing that /Users/rubber/linux/kernel/rcu/tree.c: 2023
	 * they can do to advance the grace period.  It is therefore /Users/rubber/linux/kernel/rcu/tree.c: 2024
	 * safe for us to drop the lock in order to mark the grace /Users/rubber/linux/kernel/rcu/tree.c: 2025
	 * period as completed in all of the rcu_node structures. /Users/rubber/linux/kernel/rcu/tree.c: 2026
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2030
	 * Propagate new ->gp_seq value to rcu_node structures so that /Users/rubber/linux/kernel/rcu/tree.c: 2031
	 * other CPUs don't have to wait until the start of the next grace /Users/rubber/linux/kernel/rcu/tree.c: 2032
	 * period to process their callbacks.  This also avoids some nasty /Users/rubber/linux/kernel/rcu/tree.c: 2033
	 * RCU grace-period initialization races by forcing the end of /Users/rubber/linux/kernel/rcu/tree.c: 2034
	 * the current grace period to be completely recorded in all of /Users/rubber/linux/kernel/rcu/tree.c: 2035
	 * the rcu_node structures before the beginning of the next grace /Users/rubber/linux/kernel/rcu/tree.c: 2036
	 * period is recorded in any of the rcu_node structures. /Users/rubber/linux/kernel/rcu/tree.c: 2037
 Reset overload indication for CPUs no longer overloaded /Users/rubber/linux/kernel/rcu/tree.c: 2052
 If strict, make all CPUs aware of the end of the old grace period. /Users/rubber/linux/kernel/rcu/tree.c: 2094
 * Body of kthread that handles grace periods. /Users/rubber/linux/kernel/rcu/tree.c: 2100
 * Report a full set of quiescent states to the rcu_state data structure. /Users/rubber/linux/kernel/rcu/tree.c: 2138
 * Invoke rcu_gp_kthread_wake() to awaken the grace-period kthread if /Users/rubber/linux/kernel/rcu/tree.c: 2139
 * another grace period is required.  Whether we wake the grace-period /Users/rubber/linux/kernel/rcu/tree.c: 2140
 * kthread or it awakens itself for the next round of quiescent-state /Users/rubber/linux/kernel/rcu/tree.c: 2141
 * forcing, that kthread will clean up after the just-completed grace /Users/rubber/linux/kernel/rcu/tree.c: 2142
 * period.  Note that the caller must hold rnp->lock, which is released /Users/rubber/linux/kernel/rcu/tree.c: 2143
 * before return. /Users/rubber/linux/kernel/rcu/tree.c: 2144
 * Similar to rcu_report_qs_rdp(), for which it is a helper function. /Users/rubber/linux/kernel/rcu/tree.c: 2158
 * Allows quiescent states for a group of CPUs to be reported at one go /Users/rubber/linux/kernel/rcu/tree.c: 2159
 * to the specified rcu_node structure, though all the CPUs in the group /Users/rubber/linux/kernel/rcu/tree.c: 2160
 * must be represented by the same rcu_node structure (which need not be a /Users/rubber/linux/kernel/rcu/tree.c: 2161
 * leaf rcu_node structure, though it often will be).  The gps parameter /Users/rubber/linux/kernel/rcu/tree.c: 2162
 * is the grace-period snapshot, which means that the quiescent states /Users/rubber/linux/kernel/rcu/tree.c: 2163
 * are valid only if rnp->gp_seq is equal to gps.  That structure's lock /Users/rubber/linux/kernel/rcu/tree.c: 2164
 * must be held upon entry, and it is released before return. /Users/rubber/linux/kernel/rcu/tree.c: 2165
 * As a special case, if mask is zero, the bit-already-cleared check is /Users/rubber/linux/kernel/rcu/tree.c: 2167
 * disabled.  This allows propagating quiescent state due to resumed tasks /Users/rubber/linux/kernel/rcu/tree.c: 2168
 * during grace-period initialization. /Users/rubber/linux/kernel/rcu/tree.c: 2169
			/* /Users/rubber/linux/kernel/rcu/tree.c: 2184
			 * Our bit has already been cleared, or the /Users/rubber/linux/kernel/rcu/tree.c: 2185
			 * relevant grace period is already over, so done. /Users/rubber/linux/kernel/rcu/tree.c: 2186
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2220
	 * Get here if we are the last CPU to pass through a quiescent /Users/rubber/linux/kernel/rcu/tree.c: 2221
	 * state for this grace period.  Invoke rcu_report_qs_rsp() /Users/rubber/linux/kernel/rcu/tree.c: 2222
	 * to clean up and start the next grace period if one is needed. /Users/rubber/linux/kernel/rcu/tree.c: 2223
 * Record a quiescent state for all tasks that were previously queued /Users/rubber/linux/kernel/rcu/tree.c: 2229
 * on the specified rcu_node structure and that were blocking the current /Users/rubber/linux/kernel/rcu/tree.c: 2230
 * RCU grace period.  The caller must hold the corresponding rnp->lock with /Users/rubber/linux/kernel/rcu/tree.c: 2231
 * irqs disabled, and this lock is released upon return, but irqs remain /Users/rubber/linux/kernel/rcu/tree.c: 2232
 * disabled. /Users/rubber/linux/kernel/rcu/tree.c: 2233
		/* /Users/rubber/linux/kernel/rcu/tree.c: 2254
		 * Only one rcu_node structure in the tree, so don't /Users/rubber/linux/kernel/rcu/tree.c: 2255
		 * try to report up to its nonexistent parent! /Users/rubber/linux/kernel/rcu/tree.c: 2256
 * Record a quiescent state for the specified CPU to that CPU's rcu_data /Users/rubber/linux/kernel/rcu/tree.c: 2271
 * structure.  This must be called from the specified CPU. /Users/rubber/linux/kernel/rcu/tree.c: 2272
		/* /Users/rubber/linux/kernel/rcu/tree.c: 2289
		 * The grace period in which this quiescent state was /Users/rubber/linux/kernel/rcu/tree.c: 2290
		 * recorded has ended, so don't report it upwards. /Users/rubber/linux/kernel/rcu/tree.c: 2291
		 * We will instead need a new quiescent state that lies /Users/rubber/linux/kernel/rcu/tree.c: 2292
		 * within the current grace period. /Users/rubber/linux/kernel/rcu/tree.c: 2293
		/* /Users/rubber/linux/kernel/rcu/tree.c: 2304
		 * This GP can't end until cpu checks in, so all of our /Users/rubber/linux/kernel/rcu/tree.c: 2305
		 * callbacks can be processed during the next GP. /Users/rubber/linux/kernel/rcu/tree.c: 2306
 * Check to see if there is a new grace period of which this CPU /Users/rubber/linux/kernel/rcu/tree.c: 2320
 * is not yet aware, and if so, set up local rcu_data state for it. /Users/rubber/linux/kernel/rcu/tree.c: 2321
 * Otherwise, see if this CPU has just passed through its first /Users/rubber/linux/kernel/rcu/tree.c: 2322
 * quiescent state for this grace period, and record that fact if so. /Users/rubber/linux/kernel/rcu/tree.c: 2323
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2331
	 * Does this CPU still need to do its part for current grace period? /Users/rubber/linux/kernel/rcu/tree.c: 2332
	 * If no, return and let the other CPUs do their part as well. /Users/rubber/linux/kernel/rcu/tree.c: 2333
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2338
	 * Was there a quiescent state since the beginning of the grace /Users/rubber/linux/kernel/rcu/tree.c: 2339
	 * period? If no, then exit and wait for the next call. /Users/rubber/linux/kernel/rcu/tree.c: 2340
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2345
	 * Tell RCU we are done (but rcu_report_qs_rdp() will be the /Users/rubber/linux/kernel/rcu/tree.c: 2346
	 * judge of that). /Users/rubber/linux/kernel/rcu/tree.c: 2347
 * Near the end of the offline process.  Trace the fact that this CPU /Users/rubber/linux/kernel/rcu/tree.c: 2353
 * is going offline. /Users/rubber/linux/kernel/rcu/tree.c: 2354
 * All CPUs for the specified rcu_node structure have gone offline, /Users/rubber/linux/kernel/rcu/tree.c: 2372
 * and all tasks that were preempted within an RCU read-side critical /Users/rubber/linux/kernel/rcu/tree.c: 2373
 * section while running on one of those CPUs have since exited their RCU /Users/rubber/linux/kernel/rcu/tree.c: 2374
 * read-side critical section.  Some other CPU is reporting this fact with /Users/rubber/linux/kernel/rcu/tree.c: 2375
 * the specified rcu_node structure's ->lock held and interrupts disabled. /Users/rubber/linux/kernel/rcu/tree.c: 2376
 * This function therefore goes up the tree of rcu_node structures, /Users/rubber/linux/kernel/rcu/tree.c: 2377
 * clearing the corresponding bits in the ->qsmaskinit fields.  Note that /Users/rubber/linux/kernel/rcu/tree.c: 2378
 * the leaf rcu_node structure's ->qsmaskinit field has already been /Users/rubber/linux/kernel/rcu/tree.c: 2379
 * updated. /Users/rubber/linux/kernel/rcu/tree.c: 2380
 * This function does check that the specified rcu_node structure has /Users/rubber/linux/kernel/rcu/tree.c: 2382
 * all CPUs offline and no blocked tasks, so it is OK to invoke it /Users/rubber/linux/kernel/rcu/tree.c: 2383
 * prematurely.  That said, invoking it after the fact will cost you /Users/rubber/linux/kernel/rcu/tree.c: 2384
 * a needless lock acquisition.  So once it has done its work, don't /Users/rubber/linux/kernel/rcu/tree.c: 2385
 * invoke it again. /Users/rubber/linux/kernel/rcu/tree.c: 2386
 * The CPU has been completely removed, and some other CPU is reporting /Users/rubber/linux/kernel/rcu/tree.c: 2417
 * this fact from process context.  Do the remainder of the cleanup. /Users/rubber/linux/kernel/rcu/tree.c: 2418
 * There can only be one CPU hotplug operation at a time, so no need for /Users/rubber/linux/kernel/rcu/tree.c: 2419
 * explicit locking. /Users/rubber/linux/kernel/rcu/tree.c: 2420
 Stop-machine done, so allow nohz_full to disable tick. /Users/rubber/linux/kernel/rcu/tree.c: 2433
 * Invoke any RCU callbacks that have made it to the end of their grace /Users/rubber/linux/kernel/rcu/tree.c: 2439
 * period.  Throttle as specified by rdp->blimit. /Users/rubber/linux/kernel/rcu/tree.c: 2440
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2464
	 * Extract the list of ready callbacks, disabling to prevent /Users/rubber/linux/kernel/rcu/tree.c: 2465
	 * races with call_rcu() from interrupt handlers.  Leave the /Users/rubber/linux/kernel/rcu/tree.c: 2466
	 * callback counts, as rcu_barrier() needs to be conservative. /Users/rubber/linux/kernel/rcu/tree.c: 2467
		/* /Users/rubber/linux/kernel/rcu/tree.c: 2510
		 * Stop only if limit reached and CPU has something to do. /Users/rubber/linux/kernel/rcu/tree.c: 2511
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2555
	 * The following usually indicates a double call_rcu().  To track /Users/rubber/linux/kernel/rcu/tree.c: 2556
	 * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y. /Users/rubber/linux/kernel/rcu/tree.c: 2557
 * This function is invoked from each scheduling-clock interrupt, /Users/rubber/linux/kernel/rcu/tree.c: 2575
 * and checks to see if this CPU is in a non-context-switch quiescent /Users/rubber/linux/kernel/rcu/tree.c: 2576
 * state, for example, user mode or idle loop.  It also schedules RCU /Users/rubber/linux/kernel/rcu/tree.c: 2577
 * core processing.  If the current grace period has gone on too long, /Users/rubber/linux/kernel/rcu/tree.c: 2578
 * it will ask the scheduler to manufacture a context switch for the sole /Users/rubber/linux/kernel/rcu/tree.c: 2579
 * purpose of providing the needed quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 2580
 * Scan the leaf rcu_node structures.  For each structure on which all /Users/rubber/linux/kernel/rcu/tree.c: 2605
 * CPUs have reported a quiescent state and on which there are tasks /Users/rubber/linux/kernel/rcu/tree.c: 2606
 * blocking the current grace period, initiate RCU priority boosting. /Users/rubber/linux/kernel/rcu/tree.c: 2607
 * Otherwise, invoke the specified function to check dyntick state for /Users/rubber/linux/kernel/rcu/tree.c: 2608
 * each CPU that has not yet reported a quiescent state. /Users/rubber/linux/kernel/rcu/tree.c: 2609
				/* /Users/rubber/linux/kernel/rcu/tree.c: 2628
				 * No point in scanning bits because they /Users/rubber/linux/kernel/rcu/tree.c: 2629
				 * are all zero.  But we might need to /Users/rubber/linux/kernel/rcu/tree.c: 2630
				 * priority-boost blocked readers. /Users/rubber/linux/kernel/rcu/tree.c: 2631
 * Force quiescent states on reluctant CPUs, and also detect which /Users/rubber/linux/kernel/rcu/tree.c: 2658
 * CPUs are in dyntick-idle mode. /Users/rubber/linux/kernel/rcu/tree.c: 2659
 Workqueue handler for an RCU reader for kernels enforcing struct RCU /Users/rubber/linux/kernel/rcu/tree.c: 2695
 grace periods. /Users/rubber/linux/kernel/rcu/tree.c: 2696
 If strict GPs, schedule an RCU reader in a clean environment. /Users/rubber/linux/kernel/rcu/tree.c: 2747
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2759
	 * If the thread is yielding, only wake it when this /Users/rubber/linux/kernel/rcu/tree.c: 2760
	 * is invoked from idle /Users/rubber/linux/kernel/rcu/tree.c: 2761
 * Wake up this CPU's rcuc kthread to do RCU core processing. /Users/rubber/linux/kernel/rcu/tree.c: 2781
 * Per-CPU kernel thread that invokes RCU callbacks.  This replaces /Users/rubber/linux/kernel/rcu/tree.c: 2804
 * the RCU softirq used in configurations of RCU that do not support RCU /Users/rubber/linux/kernel/rcu/tree.c: 2805
 * priority boosting. /Users/rubber/linux/kernel/rcu/tree.c: 2806
 * Spawn per-CPU RCU core processing kthreads. /Users/rubber/linux/kernel/rcu/tree.c: 2848
 * Handle any core-RCU processing required by a call_rcu() invocation. /Users/rubber/linux/kernel/rcu/tree.c: 2864
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2869
	 * If called from an extended quiescent state, invoke the RCU /Users/rubber/linux/kernel/rcu/tree.c: 2870
	 * core in order to force a re-evaluation of RCU's idleness. /Users/rubber/linux/kernel/rcu/tree.c: 2871
	/* /Users/rubber/linux/kernel/rcu/tree.c: 2880
	 * Force the grace period if too many callbacks or too long waiting. /Users/rubber/linux/kernel/rcu/tree.c: 2881
	 * Enforce hysteresis, and don't invoke rcu_force_quiescent_state() /Users/rubber/linux/kernel/rcu/tree.c: 2882
	 * if some other CPU has recently done so.  Also, don't bother /Users/rubber/linux/kernel/rcu/tree.c: 2883
	 * invoking rcu_force_quiescent_state() if the newly enqueued callback /Users/rubber/linux/kernel/rcu/tree.c: 2884
	 * is the only one waiting for a grace period to complete. /Users/rubber/linux/kernel/rcu/tree.c: 2885
 * RCU callback function to leak a callback. /Users/rubber/linux/kernel/rcu/tree.c: 2909
 * Check and if necessary update the leaf rcu_node structure's /Users/rubber/linux/kernel/rcu/tree.c: 2916
 * ->cbovldmask bit corresponding to the current CPU based on that CPU's /Users/rubber/linux/kernel/rcu/tree.c: 2917
 * number of queued RCU callbacks.  The caller must hold the leaf rcu_node /Users/rubber/linux/kernel/rcu/tree.c: 2918
 * structure's ->lock. /Users/rubber/linux/kernel/rcu/tree.c: 2919
 Early boot and wildcard value set. /Users/rubber/linux/kernel/rcu/tree.c: 2925
 * Check and if necessary update the leaf rcu_node structure's /Users/rubber/linux/kernel/rcu/tree.c: 2933
 * ->cbovldmask bit corresponding to the current CPU based on that CPU's /Users/rubber/linux/kernel/rcu/tree.c: 2934
 * number of queued RCU callbacks.  No locks need be held, but the /Users/rubber/linux/kernel/rcu/tree.c: 2935
 * caller must have disabled interrupts. /Users/rubber/linux/kernel/rcu/tree.c: 2936
 * Note that this function ignores the possibility that there are a lot /Users/rubber/linux/kernel/rcu/tree.c: 2938
 * of callbacks all of which have already seen the end of their respective /Users/rubber/linux/kernel/rcu/tree.c: 2939
 * grace periods.  This omission is due to the need for no-CBs CPUs to /Users/rubber/linux/kernel/rcu/tree.c: 2940
 * be holding ->nocb_lock to do this check, which is too heavy for a /Users/rubber/linux/kernel/rcu/tree.c: 2941
 * common-case operation. /Users/rubber/linux/kernel/rcu/tree.c: 2942
 Early boot wildcard value or already set correctly. /Users/rubber/linux/kernel/rcu/tree.c: 2951
		/* /Users/rubber/linux/kernel/rcu/tree.c: 2970
		 * Probable double call_rcu(), so leak the callback. /Users/rubber/linux/kernel/rcu/tree.c: 2971
		 * Use rcu:rcu_callback trace event to find the previous /Users/rubber/linux/kernel/rcu/tree.c: 2972
		 * time callback was passed to __call_rcu(). /Users/rubber/linux/kernel/rcu/tree.c: 2973
 This can trigger due to call_rcu() from offline CPU: /Users/rubber/linux/kernel/rcu/tree.c: 2990
 Very early boot, before rcu_init().  Initialize if needed /Users/rubber/linux/kernel/rcu/tree.c: 2993
 and then drop through to queue the callback. /Users/rubber/linux/kernel/rcu/tree.c: 2994
 Enqueued onto ->nocb_bypass, so just leave. /Users/rubber/linux/kernel/rcu/tree.c: 3001
 If no-CBs CPU gets here, rcu_nocb_try_bypass() acquired ->nocb_lock. /Users/rubber/linux/kernel/rcu/tree.c: 3002
 * call_rcu() - Queue an RCU callback for invocation after a grace period. /Users/rubber/linux/kernel/rcu/tree.c: 3024
 * @head: structure to be used for queueing the RCU updates. /Users/rubber/linux/kernel/rcu/tree.c: 3025
 * @func: actual callback function to be invoked after the grace period /Users/rubber/linux/kernel/rcu/tree.c: 3026
 * The callback function will be invoked some time after a full grace /Users/rubber/linux/kernel/rcu/tree.c: 3028
 * period elapses, in other words after all pre-existing RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 3029
 * critical sections have completed.  However, the callback function /Users/rubber/linux/kernel/rcu/tree.c: 3030
 * might well execute concurrently with RCU read-side critical sections /Users/rubber/linux/kernel/rcu/tree.c: 3031
 * that started after call_rcu() was invoked. /Users/rubber/linux/kernel/rcu/tree.c: 3032
 * RCU read-side critical sections are delimited by rcu_read_lock() /Users/rubber/linux/kernel/rcu/tree.c: 3034
 * and rcu_read_unlock(), and may be nested.  In addition, but only in /Users/rubber/linux/kernel/rcu/tree.c: 3035
 * v5.0 and later, regions of code across which interrupts, preemption, /Users/rubber/linux/kernel/rcu/tree.c: 3036
 * or softirqs have been disabled also serve as RCU read-side critical /Users/rubber/linux/kernel/rcu/tree.c: 3037
 * sections.  This includes hardware interrupt handlers, softirq handlers, /Users/rubber/linux/kernel/rcu/tree.c: 3038
 * and NMI handlers. /Users/rubber/linux/kernel/rcu/tree.c: 3039
 * Note that all CPUs must agree that the grace period extended beyond /Users/rubber/linux/kernel/rcu/tree.c: 3041
 * all pre-existing RCU read-side critical section.  On systems with more /Users/rubber/linux/kernel/rcu/tree.c: 3042
 * than one CPU, this means that when "func()" is invoked, each CPU is /Users/rubber/linux/kernel/rcu/tree.c: 3043
 * guaranteed to have executed a full memory barrier since the end of its /Users/rubber/linux/kernel/rcu/tree.c: 3044
 * last RCU read-side critical section whose beginning preceded the call /Users/rubber/linux/kernel/rcu/tree.c: 3045
 * to call_rcu().  It also means that each CPU executing an RCU read-side /Users/rubber/linux/kernel/rcu/tree.c: 3046
 * critical section that continues beyond the start of "func()" must have /Users/rubber/linux/kernel/rcu/tree.c: 3047
 * executed a memory barrier after the call_rcu() but before the beginning /Users/rubber/linux/kernel/rcu/tree.c: 3048
 * of that RCU read-side critical section.  Note that these guarantees /Users/rubber/linux/kernel/rcu/tree.c: 3049
 * include CPUs that are offline, idle, or executing in user mode, as /Users/rubber/linux/kernel/rcu/tree.c: 3050
 * well as CPUs that are executing in the kernel. /Users/rubber/linux/kernel/rcu/tree.c: 3051
 * Furthermore, if CPU A invoked call_rcu() and CPU B invoked the /Users/rubber/linux/kernel/rcu/tree.c: 3053
 * resulting RCU callback function "func()", then both CPU A and CPU B are /Users/rubber/linux/kernel/rcu/tree.c: 3054
 * guaranteed to execute a full memory barrier during the time interval /Users/rubber/linux/kernel/rcu/tree.c: 3055
 * between the call to call_rcu() and the invocation of "func()" -- even /Users/rubber/linux/kernel/rcu/tree.c: 3056
 * if CPU A and CPU B are the same CPU (but again only if the system has /Users/rubber/linux/kernel/rcu/tree.c: 3057
 * more than one CPU). /Users/rubber/linux/kernel/rcu/tree.c: 3058
 * Implementation of these memory-ordering guarantees is described here: /Users/rubber/linux/kernel/rcu/tree.c: 3060
 * Documentation/RCU/Design/Memory-Ordering/Tree-RCU-Memory-Ordering.rst. /Users/rubber/linux/kernel/rcu/tree.c: 3061
 * struct kvfree_rcu_bulk_data - single block to store kvfree_rcu() pointers /Users/rubber/linux/kernel/rcu/tree.c: 3076
 * @nr_records: Number of active pointers in the array /Users/rubber/linux/kernel/rcu/tree.c: 3077
 * @next: Next bulk object in the block chain /Users/rubber/linux/kernel/rcu/tree.c: 3078
 * @records: Array of the kvfree_rcu() pointers /Users/rubber/linux/kernel/rcu/tree.c: 3079
 * This macro defines how many entries the "records" array /Users/rubber/linux/kernel/rcu/tree.c: 3088
 * will contain. It is based on the fact that the size of /Users/rubber/linux/kernel/rcu/tree.c: 3089
 * kvfree_rcu_bulk_data structure becomes exactly one page. /Users/rubber/linux/kernel/rcu/tree.c: 3090
 * struct kfree_rcu_cpu_work - single batch of kfree_rcu() requests /Users/rubber/linux/kernel/rcu/tree.c: 3096
 * @rcu_work: Let queue_rcu_work() invoke workqueue handler after grace period /Users/rubber/linux/kernel/rcu/tree.c: 3097
 * @head_free: List of kfree_rcu() objects waiting for a grace period /Users/rubber/linux/kernel/rcu/tree.c: 3098
 * @bkvhead_free: Bulk-List of kvfree_rcu() objects waiting for a grace period /Users/rubber/linux/kernel/rcu/tree.c: 3099
 * @krcp: Pointer to @kfree_rcu_cpu structure /Users/rubber/linux/kernel/rcu/tree.c: 3100
 * struct kfree_rcu_cpu - batch up kfree_rcu() requests for RCU grace period /Users/rubber/linux/kernel/rcu/tree.c: 3111
 * @head: List of kfree_rcu() objects not yet waiting for a grace period /Users/rubber/linux/kernel/rcu/tree.c: 3112
 * @bkvhead: Bulk-List of kvfree_rcu() objects not yet waiting for a grace period /Users/rubber/linux/kernel/rcu/tree.c: 3113
 * @krw_arr: Array of batches of kfree_rcu() objects waiting for a grace period /Users/rubber/linux/kernel/rcu/tree.c: 3114
 * @lock: Synchronize access to this structure /Users/rubber/linux/kernel/rcu/tree.c: 3115
 * @monitor_work: Promote @head to @head_free after KFREE_DRAIN_JIFFIES /Users/rubber/linux/kernel/rcu/tree.c: 3116
 * @monitor_todo: Tracks whether a @monitor_work delayed work is pending /Users/rubber/linux/kernel/rcu/tree.c: 3117
 * @initialized: The @rcu_work fields have been initialized /Users/rubber/linux/kernel/rcu/tree.c: 3118
 * @count: Number of objects for which GP not started /Users/rubber/linux/kernel/rcu/tree.c: 3119
 * @bkvcache: /Users/rubber/linux/kernel/rcu/tree.c: 3120
 *	A simple cache list that contains objects for reuse purpose. /Users/rubber/linux/kernel/rcu/tree.c: 3121
 *	In order to save some per-cpu space the list is singular. /Users/rubber/linux/kernel/rcu/tree.c: 3122
 *	Even though it is lockless an access has to be protected by the /Users/rubber/linux/kernel/rcu/tree.c: 3123
 *	per-cpu lock. /Users/rubber/linux/kernel/rcu/tree.c: 3124
 * @page_cache_work: A work to refill the cache when it is empty /Users/rubber/linux/kernel/rcu/tree.c: 3125
 * @backoff_page_cache_fill: Delay cache refills /Users/rubber/linux/kernel/rcu/tree.c: 3126
 * @work_in_progress: Indicates that page_cache_work is running /Users/rubber/linux/kernel/rcu/tree.c: 3127
 * @hrtimer: A hrtimer for scheduling a page_cache_work /Users/rubber/linux/kernel/rcu/tree.c: 3128
 * @nr_bkv_objs: number of allocated objects at @bkvcache. /Users/rubber/linux/kernel/rcu/tree.c: 3129
 * This is a per-CPU structure.  The reason that it is not included in /Users/rubber/linux/kernel/rcu/tree.c: 3131
 * the rcu_data structure is to permit this code to be extracted from /Users/rubber/linux/kernel/rcu/tree.c: 3132
 * the RCU files.  Such extraction could allow further optimization of /Users/rubber/linux/kernel/rcu/tree.c: 3133
 * the interactions with the slab allocators. /Users/rubber/linux/kernel/rcu/tree.c: 3134
 For safely calling this_cpu_ptr(). /Users/rubber/linux/kernel/rcu/tree.c: 3175
 Check the limit. /Users/rubber/linux/kernel/rcu/tree.c: 3203
 * This function is invoked in workqueue context after a grace period. /Users/rubber/linux/kernel/rcu/tree.c: 3233
 * It frees all the objects queued on ->bkvhead_free or ->head_free. /Users/rubber/linux/kernel/rcu/tree.c: 3234
 Channels 1 and 2. /Users/rubber/linux/kernel/rcu/tree.c: 3250
 Channel 3. /Users/rubber/linux/kernel/rcu/tree.c: 3256
 Handle the first two channels. /Users/rubber/linux/kernel/rcu/tree.c: 3261
 kmalloc() / kfree(). /Users/rubber/linux/kernel/rcu/tree.c: 3268
 vmalloc() / vfree(). /Users/rubber/linux/kernel/rcu/tree.c: 3275
	/* /Users/rubber/linux/kernel/rcu/tree.c: 3298
	 * This is used when the "bulk" path can not be used for the /Users/rubber/linux/kernel/rcu/tree.c: 3299
	 * double-argument of kvfree_rcu().  This happens when the /Users/rubber/linux/kernel/rcu/tree.c: 3300
	 * page-cache is empty, which means that objects are instead /Users/rubber/linux/kernel/rcu/tree.c: 3301
	 * queued on a linked list through their rcu_head structures. /Users/rubber/linux/kernel/rcu/tree.c: 3302
	 * This list is named "Channel 3". /Users/rubber/linux/kernel/rcu/tree.c: 3303
 * This function is invoked after the KFREE_DRAIN_JIFFIES timeout. /Users/rubber/linux/kernel/rcu/tree.c: 3323
 Attempt to start a new batch. /Users/rubber/linux/kernel/rcu/tree.c: 3334
 Try to detach bkvhead or head and attach it over any /Users/rubber/linux/kernel/rcu/tree.c: 3338
 available corresponding free channel. It can be that /Users/rubber/linux/kernel/rcu/tree.c: 3339
 a previous RCU batch is in progress, it means that /Users/rubber/linux/kernel/rcu/tree.c: 3340
 immediately to queue another one is not possible so /Users/rubber/linux/kernel/rcu/tree.c: 3341
 in that case the monitor work is rearmed. /Users/rubber/linux/kernel/rcu/tree.c: 3342
 Channel 1 corresponds to the SLAB-pointer bulk path. /Users/rubber/linux/kernel/rcu/tree.c: 3346
 Channel 2 corresponds to vmalloc-pointer bulk path. /Users/rubber/linux/kernel/rcu/tree.c: 3347
 Channel 3 corresponds to both SLAB and vmalloc /Users/rubber/linux/kernel/rcu/tree.c: 3355
 objects queued on the linked list. /Users/rubber/linux/kernel/rcu/tree.c: 3356
 One work is per one batch, so there are three /Users/rubber/linux/kernel/rcu/tree.c: 3364
 "free channels", the batch can handle. It can /Users/rubber/linux/kernel/rcu/tree.c: 3365
 be that the work is in the pending state when /Users/rubber/linux/kernel/rcu/tree.c: 3366
 channels have been detached following by each /Users/rubber/linux/kernel/rcu/tree.c: 3367
 other. /Users/rubber/linux/kernel/rcu/tree.c: 3368
 If there is nothing to detach, it means that our job is /Users/rubber/linux/kernel/rcu/tree.c: 3373
 successfully done here. In case of having at least one /Users/rubber/linux/kernel/rcu/tree.c: 3374
 of the channels that is still busy we should rearm the /Users/rubber/linux/kernel/rcu/tree.c: 3375
 work to repeat an attempt. Because previous batches are /Users/rubber/linux/kernel/rcu/tree.c: 3376
 still in progress. /Users/rubber/linux/kernel/rcu/tree.c: 3377
 Record ptr in a page managed by krcp, with the pre-krc_this_cpu_lock() /Users/rubber/linux/kernel/rcu/tree.c: 3447
 state specified by flags.  If can_alloc is true, the caller must /Users/rubber/linux/kernel/rcu/tree.c: 3448
 be schedulable and not be holding any locks or mutexes that might be /Users/rubber/linux/kernel/rcu/tree.c: 3449
 acquired by the memory allocator or anything that it might invoke. /Users/rubber/linux/kernel/rcu/tree.c: 3450
 Returns true if ptr was successfully recorded, else the caller must /Users/rubber/linux/kernel/rcu/tree.c: 3451
 use a fallback. /Users/rubber/linux/kernel/rcu/tree.c: 3452
 __GFP_NORETRY - allows a light-weight direct reclaim /Users/rubber/linux/kernel/rcu/tree.c: 3473
 what is OK from minimizing of fallback hitting point of /Users/rubber/linux/kernel/rcu/tree.c: 3474
 view. Apart of that it forbids any OOM invoking what is /Users/rubber/linux/kernel/rcu/tree.c: 3475
 also beneficial since we are about to release memory soon. /Users/rubber/linux/kernel/rcu/tree.c: 3476
 __GFP_NOMEMALLOC - prevents from consuming of all the /Users/rubber/linux/kernel/rcu/tree.c: 3478
 memory reserves. Please note we have a fallback path. /Users/rubber/linux/kernel/rcu/tree.c: 3479
 __GFP_NOWARN - it is supposed that an allocation can /Users/rubber/linux/kernel/rcu/tree.c: 3481
 be failed under low memory or high memory pressure /Users/rubber/linux/kernel/rcu/tree.c: 3482
 scenarios. /Users/rubber/linux/kernel/rcu/tree.c: 3483
 * Queue a request for lazy invocation of the appropriate free routine /Users/rubber/linux/kernel/rcu/tree.c: 3508
 * after a grace period.  Please note that three paths are maintained, /Users/rubber/linux/kernel/rcu/tree.c: 3509
 * two for the common case using arrays of pointers and a third one that /Users/rubber/linux/kernel/rcu/tree.c: 3510
 * is used only when the main paths cannot be used, for example, due to /Users/rubber/linux/kernel/rcu/tree.c: 3511
 * memory pressure. /Users/rubber/linux/kernel/rcu/tree.c: 3512
 * Each kvfree_call_rcu() request is added to a batch. The batch will be drained /Users/rubber/linux/kernel/rcu/tree.c: 3514
 * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch will /Users/rubber/linux/kernel/rcu/tree.c: 3515
 * be free'd in workqueue context. This allows us to: batch requests together to /Users/rubber/linux/kernel/rcu/tree.c: 3516
 * reduce the number of grace periods during heavy kfree_rcu()/kvfree_rcu() load. /Users/rubber/linux/kernel/rcu/tree.c: 3517
		/* /Users/rubber/linux/kernel/rcu/tree.c: 3529
		 * Please note there is a limitation for the head-less /Users/rubber/linux/kernel/rcu/tree.c: 3530
		 * variant, that is why there is a clear rule for such /Users/rubber/linux/kernel/rcu/tree.c: 3531
		 * objects: it can be used from might_sleep() context /Users/rubber/linux/kernel/rcu/tree.c: 3532
		 * only. For other places please embed an rcu_head to /Users/rubber/linux/kernel/rcu/tree.c: 3533
		 * your data. /Users/rubber/linux/kernel/rcu/tree.c: 3534
 Queue the object but don't yet schedule the batch. /Users/rubber/linux/kernel/rcu/tree.c: 3540
 Probable double kfree_rcu(), just leak. /Users/rubber/linux/kernel/rcu/tree.c: 3542
 Mark as success and leave. /Users/rubber/linux/kernel/rcu/tree.c: 3546
 Inline if kvfree_rcu(one_arg) call. /Users/rubber/linux/kernel/rcu/tree.c: 3556
 Set timer to drain after KFREE_DRAIN_JIFFIES. /Users/rubber/linux/kernel/rcu/tree.c: 3567
	/* /Users/rubber/linux/kernel/rcu/tree.c: 3577
	 * Inline kvfree() after synchronize_rcu(). We can do /Users/rubber/linux/kernel/rcu/tree.c: 3578
	 * it from might_sleep() context only, so the current /Users/rubber/linux/kernel/rcu/tree.c: 3579
	 * CPU can pass the QS state. /Users/rubber/linux/kernel/rcu/tree.c: 3580
 * During early boot, any blocking grace-period wait automatically /Users/rubber/linux/kernel/rcu/tree.c: 3660
 * implies a grace period.  Later on, this is never the case for PREEMPTION. /Users/rubber/linux/kernel/rcu/tree.c: 3661
 * However, because a context switch is a grace period for !PREEMPTION, any /Users/rubber/linux/kernel/rcu/tree.c: 3663
 * blocking grace-period wait automatically implies a grace period if /Users/rubber/linux/kernel/rcu/tree.c: 3664
 * there is only one CPU online at any point time during execution of /Users/rubber/linux/kernel/rcu/tree.c: 3665
 * either synchronize_rcu() or synchronize_rcu_expedited().  It is OK to /Users/rubber/linux/kernel/rcu/tree.c: 3666
 * occasionally incorrectly indicate that there are multiple CPUs online /Users/rubber/linux/kernel/rcu/tree.c: 3667
 * when there was in fact only one the whole time, as this just adds some /Users/rubber/linux/kernel/rcu/tree.c: 3668
 * overhead: RCU still operates correctly. /Users/rubber/linux/kernel/rcu/tree.c: 3669
	/* /Users/rubber/linux/kernel/rcu/tree.c: 3679
	 * If the rcu_state.n_online_cpus counter is equal to one, /Users/rubber/linux/kernel/rcu/tree.c: 3680
	 * there is only one CPU, and that CPU sees all prior accesses /Users/rubber/linux/kernel/rcu/tree.c: 3681
	 * made by any CPU that was online at the time of its access. /Users/rubber/linux/kernel/rcu/tree.c: 3682
	 * Furthermore, if this counter is equal to one, its value cannot /Users/rubber/linux/kernel/rcu/tree.c: 3683
	 * change until after the preempt_enable() below. /Users/rubber/linux/kernel/rcu/tree.c: 3684
	 * /Users/rubber/linux/kernel/rcu/tree.c: 3685
	 * Furthermore, if rcu_state.n_online_cpus is equal to one here, /Users/rubber/linux/kernel/rcu/tree.c: 3686
	 * all later CPUs (both this one and any that come online later /Users/rubber/linux/kernel/rcu/tree.c: 3687
	 * on) are guaranteed to see all accesses prior to this point /Users/rubber/linux/kernel/rcu/tree.c: 3688
	 * in the code, without the need for additional memory barriers. /Users/rubber/linux/kernel/rcu/tree.c: 3689
	 * Those memory barriers are provided by CPU-hotplug code. /Users/rubber/linux/kernel/rcu/tree.c: 3690
 * synchronize_rcu - wait until a grace period has elapsed. /Users/rubber/linux/kernel/rcu/tree.c: 3698
 * Control will return to the caller some time after a full grace /Users/rubber/linux/kernel/rcu/tree.c: 3700
 * period has elapsed, in other words after all currently executing RCU /Users/rubber/linux/kernel/rcu/tree.c: 3701
 * read-side critical sections have completed.  Note, however, that /Users/rubber/linux/kernel/rcu/tree.c: 3702
 * upon return from synchronize_rcu(), the caller might well be executing /Users/rubber/linux/kernel/rcu/tree.c: 3703
 * concurrently with new RCU read-side critical sections that began while /Users/rubber/linux/kernel/rcu/tree.c: 3704
 * synchronize_rcu() was waiting. /Users/rubber/linux/kernel/rcu/tree.c: 3705
 * RCU read-side critical sections are delimited by rcu_read_lock() /Users/rubber/linux/kernel/rcu/tree.c: 3707
 * and rcu_read_unlock(), and may be nested.  In addition, but only in /Users/rubber/linux/kernel/rcu/tree.c: 3708
 * v5.0 and later, regions of code across which interrupts, preemption, /Users/rubber/linux/kernel/rcu/tree.c: 3709
 * or softirqs have been disabled also serve as RCU read-side critical /Users/rubber/linux/kernel/rcu/tree.c: 3710
 * sections.  This includes hardware interrupt handlers, softirq handlers, /Users/rubber/linux/kernel/rcu/tree.c: 3711
 * and NMI handlers. /Users/rubber/linux/kernel/rcu/tree.c: 3712
 * Note that this guarantee implies further memory-ordering guarantees. /Users/rubber/linux/kernel/rcu/tree.c: 3714
 * On systems with more than one CPU, when synchronize_rcu() returns, /Users/rubber/linux/kernel/rcu/tree.c: 3715
 * each CPU is guaranteed to have executed a full memory barrier since /Users/rubber/linux/kernel/rcu/tree.c: 3716
 * the end of its last RCU read-side critical section whose beginning /Users/rubber/linux/kernel/rcu/tree.c: 3717
 * preceded the call to synchronize_rcu().  In addition, each CPU having /Users/rubber/linux/kernel/rcu/tree.c: 3718
 * an RCU read-side critical section that extends beyond the return from /Users/rubber/linux/kernel/rcu/tree.c: 3719
 * synchronize_rcu() is guaranteed to have executed a full memory barrier /Users/rubber/linux/kernel/rcu/tree.c: 3720
 * after the beginning of synchronize_rcu() and before the beginning of /Users/rubber/linux/kernel/rcu/tree.c: 3721
 * that RCU read-side critical section.  Note that these guarantees include /Users/rubber/linux/kernel/rcu/tree.c: 3722
 * CPUs that are offline, idle, or executing in user mode, as well as CPUs /Users/rubber/linux/kernel/rcu/tree.c: 3723
 * that are executing in the kernel. /Users/rubber/linux/kernel/rcu/tree.c: 3724
 * Furthermore, if CPU A invoked synchronize_rcu(), which returned /Users/rubber/linux/kernel/rcu/tree.c: 3726
 * to its caller on CPU B, then both CPU A and CPU B are guaranteed /Users/rubber/linux/kernel/rcu/tree.c: 3727
 * to have executed a full memory barrier during the execution of /Users/rubber/linux/kernel/rcu/tree.c: 3728
 * synchronize_rcu() -- even if CPU A and CPU B are the same CPU (but /Users/rubber/linux/kernel/rcu/tree.c: 3729
 * again only if the system has more than one CPU). /Users/rubber/linux/kernel/rcu/tree.c: 3730
 * Implementation of these memory-ordering guarantees is described here: /Users/rubber/linux/kernel/rcu/tree.c: 3732
 * Documentation/RCU/Design/Memory-Ordering/Tree-RCU-Memory-Ordering.rst. /Users/rubber/linux/kernel/rcu/tree.c: 3733
 Context allows vacuous grace periods. /Users/rubber/linux/kernel/rcu/tree.c: 3742
 * get_state_synchronize_rcu - Snapshot current RCU state /Users/rubber/linux/kernel/rcu/tree.c: 3751
 * Returns a cookie that is used by a later call to cond_synchronize_rcu() /Users/rubber/linux/kernel/rcu/tree.c: 3753
 * or poll_state_synchronize_rcu() to determine whether or not a full /Users/rubber/linux/kernel/rcu/tree.c: 3754
 * grace period has elapsed in the meantime. /Users/rubber/linux/kernel/rcu/tree.c: 3755
	/* /Users/rubber/linux/kernel/rcu/tree.c: 3759
	 * Any prior manipulation of RCU-protected data must happen /Users/rubber/linux/kernel/rcu/tree.c: 3760
	 * before the load from ->gp_seq. /Users/rubber/linux/kernel/rcu/tree.c: 3761
 * start_poll_synchronize_rcu - Snapshot and start RCU grace period /Users/rubber/linux/kernel/rcu/tree.c: 3769
 * Returns a cookie that is used by a later call to cond_synchronize_rcu() /Users/rubber/linux/kernel/rcu/tree.c: 3771
 * or poll_state_synchronize_rcu() to determine whether or not a full /Users/rubber/linux/kernel/rcu/tree.c: 3772
 * grace period has elapsed in the meantime.  If the needed grace period /Users/rubber/linux/kernel/rcu/tree.c: 3773
 * is not already slated to start, notifies RCU core of the need for that /Users/rubber/linux/kernel/rcu/tree.c: 3774
 * grace period. /Users/rubber/linux/kernel/rcu/tree.c: 3775
 * Interrupts must be enabled for the case where it is necessary to awaken /Users/rubber/linux/kernel/rcu/tree.c: 3777
 * the grace-period kthread. /Users/rubber/linux/kernel/rcu/tree.c: 3778
 irqs already disabled. /Users/rubber/linux/kernel/rcu/tree.c: 3792
 * poll_state_synchronize_rcu - Conditionally wait for an RCU grace period /Users/rubber/linux/kernel/rcu/tree.c: 3802
 * @oldstate: value from get_state_synchronize_rcu() or start_poll_synchronize_rcu() /Users/rubber/linux/kernel/rcu/tree.c: 3804
 * If a full RCU grace period has elapsed since the earlier call from /Users/rubber/linux/kernel/rcu/tree.c: 3806
 * which oldstate was obtained, return @true, otherwise return @false. /Users/rubber/linux/kernel/rcu/tree.c: 3807
 * If @false is returned, it is the caller's responsibility to invoke this /Users/rubber/linux/kernel/rcu/tree.c: 3808
 * function later on until it does return @true.  Alternatively, the caller /Users/rubber/linux/kernel/rcu/tree.c: 3809
 * can explicitly wait for a grace period, for example, by passing @oldstate /Users/rubber/linux/kernel/rcu/tree.c: 3810
 * to cond_synchronize_rcu() or by directly invoking synchronize_rcu(). /Users/rubber/linux/kernel/rcu/tree.c: 3811
 * Yes, this function does not take counter wrap into account. /Users/rubber/linux/kernel/rcu/tree.c: 3813
 * But counter wrap is harmless.  If the counter wraps, we have waited for /Users/rubber/linux/kernel/rcu/tree.c: 3814
 * more than 2 billion grace periods (and way more on a 64-bit system!). /Users/rubber/linux/kernel/rcu/tree.c: 3815
 * Those needing to keep oldstate values for very long time periods /Users/rubber/linux/kernel/rcu/tree.c: 3816
 * (many hours even on 32-bit systems) should check them occasionally /Users/rubber/linux/kernel/rcu/tree.c: 3817
 * and either refresh them or set a flag indicating that the grace period /Users/rubber/linux/kernel/rcu/tree.c: 3818
 * has completed. /Users/rubber/linux/kernel/rcu/tree.c: 3819
 * This function provides the same memory-ordering guarantees that /Users/rubber/linux/kernel/rcu/tree.c: 3821
 * would be provided by a synchronize_rcu() that was invoked at the call /Users/rubber/linux/kernel/rcu/tree.c: 3822
 * to the function that provided @oldstate, and that returned at the end /Users/rubber/linux/kernel/rcu/tree.c: 3823
 * of this function. /Users/rubber/linux/kernel/rcu/tree.c: 3824
 * cond_synchronize_rcu - Conditionally wait for an RCU grace period /Users/rubber/linux/kernel/rcu/tree.c: 3837
 * @oldstate: value from get_state_synchronize_rcu() or start_poll_synchronize_rcu() /Users/rubber/linux/kernel/rcu/tree.c: 3839
 * If a full RCU grace period has elapsed since the earlier call to /Users/rubber/linux/kernel/rcu/tree.c: 3841
 * get_state_synchronize_rcu() or start_poll_synchronize_rcu(), just return. /Users/rubber/linux/kernel/rcu/tree.c: 3842
 * Otherwise, invoke synchronize_rcu() to wait for a full grace period. /Users/rubber/linux/kernel/rcu/tree.c: 3843
 * Yes, this function does not take counter wrap into account.  But /Users/rubber/linux/kernel/rcu/tree.c: 3845
 * counter wrap is harmless.  If the counter wraps, we have waited for /Users/rubber/linux/kernel/rcu/tree.c: 3846
 * more than 2 billion grace periods (and way more on a 64-bit system!), /Users/rubber/linux/kernel/rcu/tree.c: 3847
 * so waiting for one additional grace period should be just fine. /Users/rubber/linux/kernel/rcu/tree.c: 3848
 * This function provides the same memory-ordering guarantees that /Users/rubber/linux/kernel/rcu/tree.c: 3850
 * would be provided by a synchronize_rcu() that was invoked at the call /Users/rubber/linux/kernel/rcu/tree.c: 3851
 * to the function that provided @oldstate, and that returned at the end /Users/rubber/linux/kernel/rcu/tree.c: 3852
 * of this function. /Users/rubber/linux/kernel/rcu/tree.c: 3853
 * Check to see if there is any immediate RCU-related work to be done by /Users/rubber/linux/kernel/rcu/tree.c: 3863
 * the current CPU, returning 1 if so and zero otherwise.  The checks are /Users/rubber/linux/kernel/rcu/tree.c: 3864
 * in order of increasing expense: checks that can be carried out against /Users/rubber/linux/kernel/rcu/tree.c: 3865
 * CPU-local state are performed first.  However, we must check for CPU /Users/rubber/linux/kernel/rcu/tree.c: 3866
 * stalls first, else we might not get a chance. /Users/rubber/linux/kernel/rcu/tree.c: 3867
 * Helper function for rcu_barrier() tracing.  If tracing is disabled, /Users/rubber/linux/kernel/rcu/tree.c: 3914
 * the compiler is expected to optimize this away. /Users/rubber/linux/kernel/rcu/tree.c: 3915
 * RCU callback function for rcu_barrier().  If we are last, wake /Users/rubber/linux/kernel/rcu/tree.c: 3924
 * up the task executing rcu_barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 3925
 * Note that the value of rcu_state.barrier_sequence must be captured /Users/rubber/linux/kernel/rcu/tree.c: 3927
 * before the atomic_dec_and_test().  Otherwise, if this CPU is not last, /Users/rubber/linux/kernel/rcu/tree.c: 3928
 * other CPUs might count the value down to zero before this CPU gets /Users/rubber/linux/kernel/rcu/tree.c: 3929
 * around to invoking rcu_barrier_trace(), which might result in bogus /Users/rubber/linux/kernel/rcu/tree.c: 3930
 * data from the next instance of rcu_barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 3931
 * Called with preemption disabled, and from cross-cpu IRQ context. /Users/rubber/linux/kernel/rcu/tree.c: 3946
 * rcu_barrier - Wait until all in-flight call_rcu() callbacks complete. /Users/rubber/linux/kernel/rcu/tree.c: 3969
 * Note that this primitive does not necessarily wait for an RCU grace period /Users/rubber/linux/kernel/rcu/tree.c: 3971
 * to complete.  For example, if there are no RCU callbacks queued anywhere /Users/rubber/linux/kernel/rcu/tree.c: 3972
 * in the system, then rcu_barrier() is within its rights to return /Users/rubber/linux/kernel/rcu/tree.c: 3973
 * immediately, without waiting for anything, much less an RCU grace period. /Users/rubber/linux/kernel/rcu/tree.c: 3974
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4000
	 * Initialize the count to two rather than to zero in order /Users/rubber/linux/kernel/rcu/tree.c: 4001
	 * to avoid a too-soon return to zero in case of an immediate /Users/rubber/linux/kernel/rcu/tree.c: 4002
	 * invocation of the just-enqueued callback (or preemption of /Users/rubber/linux/kernel/rcu/tree.c: 4003
	 * this task).  Exclude CPU-hotplug operations to ensure that no /Users/rubber/linux/kernel/rcu/tree.c: 4004
	 * offline non-offloaded CPU has callbacks queued. /Users/rubber/linux/kernel/rcu/tree.c: 4005
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4011
	 * Force each CPU with callbacks to register a new callback. /Users/rubber/linux/kernel/rcu/tree.c: 4012
	 * When that callback is invoked, we will know that all of the /Users/rubber/linux/kernel/rcu/tree.c: 4013
	 * corresponding CPU's preceding callbacks have been invoked. /Users/rubber/linux/kernel/rcu/tree.c: 4014
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4042
	 * Now that we have an rcu_barrier_callback() callback on each /Users/rubber/linux/kernel/rcu/tree.c: 4043
	 * CPU, and thus each counted, remove the initial count. /Users/rubber/linux/kernel/rcu/tree.c: 4044
 * Propagate ->qsinitmask bits up the rcu_node tree to account for the /Users/rubber/linux/kernel/rcu/tree.c: 4062
 * first CPU in a given leaf rcu_node structure coming online.  The caller /Users/rubber/linux/kernel/rcu/tree.c: 4063
 * must hold the corresponding leaf rcu_node ->lock with interrupts /Users/rubber/linux/kernel/rcu/tree.c: 4064
 * disabled. /Users/rubber/linux/kernel/rcu/tree.c: 4065
 * Do boot-time initialization of a CPU's per-CPU RCU data. /Users/rubber/linux/kernel/rcu/tree.c: 4090
 * Invoked early in the CPU-online process, when pretty much all services /Users/rubber/linux/kernel/rcu/tree.c: 4111
 * are available.  The incoming CPU is not present. /Users/rubber/linux/kernel/rcu/tree.c: 4112
 * Initializes a CPU's per-CPU RCU data.  Note that only one online or /Users/rubber/linux/kernel/rcu/tree.c: 4114
 * offline event can be happening at a given time.  Note also that we can /Users/rubber/linux/kernel/rcu/tree.c: 4115
 * accept some slop in the rsp->gp_seq access due to the fact that this /Users/rubber/linux/kernel/rcu/tree.c: 4116
 * CPU cannot possibly have any non-offloaded RCU callbacks in flight yet. /Users/rubber/linux/kernel/rcu/tree.c: 4117
 * And any offloaded callbacks are being numbered elsewhere. /Users/rubber/linux/kernel/rcu/tree.c: 4118
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4134
	 * Only non-NOCB CPUs that didn't have early-boot callbacks need to be /Users/rubber/linux/kernel/rcu/tree.c: 4135
	 * (re-)initialized. /Users/rubber/linux/kernel/rcu/tree.c: 4136
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4141
	 * Add CPU to leaf rcu_node pending-online bitmask.  Any needed /Users/rubber/linux/kernel/rcu/tree.c: 4142
	 * propagation up the rcu_node tree will happen at the beginning /Users/rubber/linux/kernel/rcu/tree.c: 4143
	 * of the next grace period. /Users/rubber/linux/kernel/rcu/tree.c: 4144
 * Update RCU priority boot kthread affinity for CPU-hotplug changes. /Users/rubber/linux/kernel/rcu/tree.c: 4166
 * Near the end of the CPU-online process.  Pretty much all services /Users/rubber/linux/kernel/rcu/tree.c: 4176
 * enabled, and the CPU is now very much alive. /Users/rubber/linux/kernel/rcu/tree.c: 4177
 Stop-machine done, so allow nohz_full to disable tick. /Users/rubber/linux/kernel/rcu/tree.c: 4195
 * Near the beginning of the process.  The CPU is still very much alive /Users/rubber/linux/kernel/rcu/tree.c: 4201
 * with pretty much all services enabled. /Users/rubber/linux/kernel/rcu/tree.c: 4202
 nohz_full CPUs need the tick for stop-machine to work quickly /Users/rubber/linux/kernel/rcu/tree.c: 4218
 * Mark the specified CPU as being online so that subsequent grace periods /Users/rubber/linux/kernel/rcu/tree.c: 4224
 * (both expedited and normal) will wait on it.  Note that this means that /Users/rubber/linux/kernel/rcu/tree.c: 4225
 * incoming CPUs are not allowed to use RCU read-side critical sections /Users/rubber/linux/kernel/rcu/tree.c: 4226
 * until this function is called.  Failing to observe this restriction /Users/rubber/linux/kernel/rcu/tree.c: 4227
 * will result in lockdep splats. /Users/rubber/linux/kernel/rcu/tree.c: 4228
 * Note that this function is special in that it is invoked directly /Users/rubber/linux/kernel/rcu/tree.c: 4230
 * from the incoming CPU rather than from the cpuhp_step mechanism. /Users/rubber/linux/kernel/rcu/tree.c: 4231
 * This is because this function must be invoked at a precise location. /Users/rubber/linux/kernel/rcu/tree.c: 4232
 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 4252
 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 4272
 * The outgoing function has no further need of RCU, so remove it from /Users/rubber/linux/kernel/rcu/tree.c: 4279
 * the rcu_node tree's ->qsmaskinitnext bit masks. /Users/rubber/linux/kernel/rcu/tree.c: 4280
 * Note that this function is special in that it is invoked directly /Users/rubber/linux/kernel/rcu/tree.c: 4282
 * from the outgoing CPU rather than from the cpuhp_step mechanism. /Users/rubber/linux/kernel/rcu/tree.c: 4283
 * This is because this function must be invoked at a precise location. /Users/rubber/linux/kernel/rcu/tree.c: 4284
 Do any dangling deferred wakeups. /Users/rubber/linux/kernel/rcu/tree.c: 4293
 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 4304
 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier(). /Users/rubber/linux/kernel/rcu/tree.c: 4317
 * The outgoing CPU has just passed through the dying-idle state, and we /Users/rubber/linux/kernel/rcu/tree.c: 4326
 * are being invoked from the CPU that was IPIed to continue the offline /Users/rubber/linux/kernel/rcu/tree.c: 4327
 * operation.  Migrate the outgoing CPU's callbacks to the current CPU. /Users/rubber/linux/kernel/rcu/tree.c: 4328
 * On non-huge systems, use expedited RCU grace periods to make suspend /Users/rubber/linux/kernel/rcu/tree.c: 4375
 * and hibernation run faster. /Users/rubber/linux/kernel/rcu/tree.c: 4376
 * Spawn the kthreads that handle RCU's grace periods. /Users/rubber/linux/kernel/rcu/tree.c: 4397
 Reset .gp_activity and .gp_req_activity before setting .gp_kthread. /Users/rubber/linux/kernel/rcu/tree.c: 4434
 * This function is invoked towards the end of the scheduler's /Users/rubber/linux/kernel/rcu/tree.c: 4446
 * initialization process.  Before this is called, the idle task might /Users/rubber/linux/kernel/rcu/tree.c: 4447
 * contain synchronous grace-period primitives (during which time, this idle /Users/rubber/linux/kernel/rcu/tree.c: 4448
 * task is booting the system, and such primitives are no-ops).  After this /Users/rubber/linux/kernel/rcu/tree.c: 4449
 * function is called, any synchronous grace-period primitives are run as /Users/rubber/linux/kernel/rcu/tree.c: 4450
 * expedited, with the requesting task driving the grace period forward. /Users/rubber/linux/kernel/rcu/tree.c: 4451
 * A later core_initcall() rcu_set_runtime_mode() will switch to full /Users/rubber/linux/kernel/rcu/tree.c: 4452
 * runtime RCU functionality. /Users/rubber/linux/kernel/rcu/tree.c: 4453
 * Helper function for rcu_init() that initializes the rcu_state structure. /Users/rubber/linux/kernel/rcu/tree.c: 4465
 * Compute the rcu_node tree geometry from kernel parameters.  This cannot /Users/rubber/linux/kernel/rcu/tree.c: 4547
 * replace the definitions in tree.h because those are needed to size /Users/rubber/linux/kernel/rcu/tree.c: 4548
 * the ->node array in the rcu_state structure. /Users/rubber/linux/kernel/rcu/tree.c: 4549
		/* /Users/rubber/linux/kernel/rcu/tree.c: 4560
		 * Warn if setup_nr_cpu_ids() had not yet been invoked, /Users/rubber/linux/kernel/rcu/tree.c: 4561
		 * unless nr_cpus_ids == NR_CPUS, in which case who cares? /Users/rubber/linux/kernel/rcu/tree.c: 4562
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4571
	 * Initialize any unspecified boot parameters. /Users/rubber/linux/kernel/rcu/tree.c: 4572
	 * The default values of jiffies_till_first_fqs and /Users/rubber/linux/kernel/rcu/tree.c: 4573
	 * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS /Users/rubber/linux/kernel/rcu/tree.c: 4574
	 * value, which is a function of HZ, then adding one for each /Users/rubber/linux/kernel/rcu/tree.c: 4575
	 * RCU_JIFFIES_FQS_DIV CPUs that might be on the system. /Users/rubber/linux/kernel/rcu/tree.c: 4576
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4592
	 * The boot-time rcu_fanout_leaf parameter must be at least two /Users/rubber/linux/kernel/rcu/tree.c: 4593
	 * and cannot exceed the number of bits in the rcu_node masks. /Users/rubber/linux/kernel/rcu/tree.c: 4594
	 * Complain and fall back to the compile-time values if this /Users/rubber/linux/kernel/rcu/tree.c: 4595
	 * limit is exceeded. /Users/rubber/linux/kernel/rcu/tree.c: 4596
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4605
	 * Compute number of nodes that can be handled an rcu_node tree /Users/rubber/linux/kernel/rcu/tree.c: 4606
	 * with the given number of levels. /Users/rubber/linux/kernel/rcu/tree.c: 4607
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4613
	 * The tree must be able to accommodate the configured number of CPUs. /Users/rubber/linux/kernel/rcu/tree.c: 4614
	 * If this limit is exceeded, fall back to the compile-time values. /Users/rubber/linux/kernel/rcu/tree.c: 4615
 * Dump out the structure of the rcu_node combining tree associated /Users/rubber/linux/kernel/rcu/tree.c: 4641
 * with the rcu_state structure. /Users/rubber/linux/kernel/rcu/tree.c: 4642
	/* /Users/rubber/linux/kernel/rcu/tree.c: 4713
	 * We don't need protection against CPU-hotplug here because /Users/rubber/linux/kernel/rcu/tree.c: 4714
	 * this is called early in boot, before either interrupts /Users/rubber/linux/kernel/rcu/tree.c: 4715
	 * or the scheduler are operational. /Users/rubber/linux/kernel/rcu/tree.c: 4716
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/dma/swiotlb.c: 1
 * Dynamic DMA mapping support. /Users/rubber/linux/kernel/dma/swiotlb.c: 3
 * This implementation is a fallback for platforms that do not support /Users/rubber/linux/kernel/dma/swiotlb.c: 5
 * I/O TLBs (aka DMA address translation hardware). /Users/rubber/linux/kernel/dma/swiotlb.c: 6
 * Copyright (C) 2000 Asit Mallick <Asit.K.Mallick@intel.com> /Users/rubber/linux/kernel/dma/swiotlb.c: 7
 * Copyright (C) 2000 Goutham Rao <goutham.rao@intel.com> /Users/rubber/linux/kernel/dma/swiotlb.c: 8
 * Copyright (C) 2000, 2003 Hewlett-Packard Co /Users/rubber/linux/kernel/dma/swiotlb.c: 9
 *	David Mosberger-Tang <davidm@hpl.hp.com> /Users/rubber/linux/kernel/dma/swiotlb.c: 10
 * 03/05/07 davidm	Switch from PCI-DMA to generic device DMA API. /Users/rubber/linux/kernel/dma/swiotlb.c: 12
 * 00/12/13 davidm	Rename to swiotlb.c and add mark_clean() to avoid /Users/rubber/linux/kernel/dma/swiotlb.c: 13
 *			unnecessary i-cache flushing. /Users/rubber/linux/kernel/dma/swiotlb.c: 14
 * 04/07/.. ak		Better overflow handling. Assorted fixes. /Users/rubber/linux/kernel/dma/swiotlb.c: 15
 * 05/09/10 linville	Add support for syncing ranges, support syncing for /Users/rubber/linux/kernel/dma/swiotlb.c: 16
 *			DMA_BIDIRECTIONAL mappings, miscellaneous cleanup. /Users/rubber/linux/kernel/dma/swiotlb.c: 17
 * 08/12/11 beckyb	Add highmem support /Users/rubber/linux/kernel/dma/swiotlb.c: 18
 * Minimum IO TLB size to bother booting with.  Systems with mainly /Users/rubber/linux/kernel/dma/swiotlb.c: 63
 * 64bit capable cards will only lightly use the swiotlb.  If we can't /Users/rubber/linux/kernel/dma/swiotlb.c: 64
 * allocate a contiguous 1MB, we're probably in trouble anyway. /Users/rubber/linux/kernel/dma/swiotlb.c: 65
 * Max segment that we can provide which (if pages are contingous) will /Users/rubber/linux/kernel/dma/swiotlb.c: 76
 * not be bounced (unless SWIOTLB_FORCE is set). /Users/rubber/linux/kernel/dma/swiotlb.c: 77
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 123
	 * If swiotlb parameter has not been specified, give a chance to /Users/rubber/linux/kernel/dma/swiotlb.c: 124
	 * architectures such as those supporting memory encryption to /Users/rubber/linux/kernel/dma/swiotlb.c: 125
	 * adjust/expand SWIOTLB size for their use. /Users/rubber/linux/kernel/dma/swiotlb.c: 126
 * Early SWIOTLB allocation may be too early to allow an architecture to /Users/rubber/linux/kernel/dma/swiotlb.c: 159
 * perform the desired operations.  This function allows the architecture to /Users/rubber/linux/kernel/dma/swiotlb.c: 160
 * call SWIOTLB when the operations are possible.  It needs to be called /Users/rubber/linux/kernel/dma/swiotlb.c: 161
 * before the SWIOTLB memory is used. /Users/rubber/linux/kernel/dma/swiotlb.c: 162
 * Statically reserve bounce buffer space and initialize bounce buffer data /Users/rubber/linux/kernel/dma/swiotlb.c: 229
 * structures for the software IO TLB used to implement the DMA API. /Users/rubber/linux/kernel/dma/swiotlb.c: 230
 * Systems with larger DMA zones (those that don't support ISA) can /Users/rubber/linux/kernel/dma/swiotlb.c: 256
 * initialize the swiotlb later using the slab allocator if needed. /Users/rubber/linux/kernel/dma/swiotlb.c: 257
 * This should be just like above, but with some error catching. /Users/rubber/linux/kernel/dma/swiotlb.c: 258
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 273
	 * Get IO TLB memory from the low pages /Users/rubber/linux/kernel/dma/swiotlb.c: 274
 * Return the offset into a iotlb slot required to keep the device happy. /Users/rubber/linux/kernel/dma/swiotlb.c: 356
 * Bounce: copy the swiotlb buffer from or back to the original dma location /Users/rubber/linux/kernel/dma/swiotlb.c: 364
 * Carefully handle integer overflow which can occur when boundary_mask == ~0UL. /Users/rubber/linux/kernel/dma/swiotlb.c: 441
 * Find a suitable number of IO TLB entries size that will fit this request and /Users/rubber/linux/kernel/dma/swiotlb.c: 458
 * allocate a buffer from that IO TLB pool. /Users/rubber/linux/kernel/dma/swiotlb.c: 459
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 478
	 * For mappings with an alignment requirement don't bother looping to /Users/rubber/linux/kernel/dma/swiotlb.c: 479
	 * unaligned slots once we found an aligned one.  For allocations of /Users/rubber/linux/kernel/dma/swiotlb.c: 480
	 * PAGE_SIZE or larger only look for page aligned allocations. /Users/rubber/linux/kernel/dma/swiotlb.c: 481
		/* /Users/rubber/linux/kernel/dma/swiotlb.c: 501
		 * If we find a slot that indicates we have 'nslots' number of /Users/rubber/linux/kernel/dma/swiotlb.c: 502
		 * contiguous buffers, we allocate the buffers from that slot /Users/rubber/linux/kernel/dma/swiotlb.c: 503
		 * and mark the entries as '0' indicating unavailable. /Users/rubber/linux/kernel/dma/swiotlb.c: 504
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 530
	 * Update the indices to avoid searching in the next round. /Users/rubber/linux/kernel/dma/swiotlb.c: 531
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 576
	 * Save away the mapping from the original address to the DMA address. /Users/rubber/linux/kernel/dma/swiotlb.c: 577
	 * This is needed when we sync the memory.  Then we sync the buffer if /Users/rubber/linux/kernel/dma/swiotlb.c: 578
	 * needed. /Users/rubber/linux/kernel/dma/swiotlb.c: 579
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 599
	 * Return the buffer to the free list by setting the corresponding /Users/rubber/linux/kernel/dma/swiotlb.c: 600
	 * entries to indicate the number of contiguous entries available. /Users/rubber/linux/kernel/dma/swiotlb.c: 601
	 * While returning the entries to the free list, we merge the entries /Users/rubber/linux/kernel/dma/swiotlb.c: 602
	 * with slots below and above the pool being returned. /Users/rubber/linux/kernel/dma/swiotlb.c: 603
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 611
	 * Step 1: return the slots to the free list, merging the slots with /Users/rubber/linux/kernel/dma/swiotlb.c: 612
	 * superceeding slots /Users/rubber/linux/kernel/dma/swiotlb.c: 613
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 621
	 * Step 2: merge the returned slots with the preceding slots, if /Users/rubber/linux/kernel/dma/swiotlb.c: 622
	 * available (non zero) /Users/rubber/linux/kernel/dma/swiotlb.c: 623
 * tlb_addr is the physical address of the bounce buffer to unmap. /Users/rubber/linux/kernel/dma/swiotlb.c: 634
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 640
	 * First, sync the memory before unmapping the entry /Users/rubber/linux/kernel/dma/swiotlb.c: 641
 * Create a swiotlb mapping for the buffer at @paddr, and in case of DMAing /Users/rubber/linux/kernel/dma/swiotlb.c: 669
 * to the device copy the data into it as well. /Users/rubber/linux/kernel/dma/swiotlb.c: 670
	/* /Users/rubber/linux/kernel/dma/swiotlb.c: 792
	 * Since multiple devices can share the same pool, the private data, /Users/rubber/linux/kernel/dma/swiotlb.c: 793
	 * io_tlb_mem struct, will be initialized by the first device attached /Users/rubber/linux/kernel/dma/swiotlb.c: 794
	 * to it. /Users/rubber/linux/kernel/dma/swiotlb.c: 795
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/dma/contiguous.c: 1
 * Contiguous Memory Allocator for DMA mapping framework /Users/rubber/linux/kernel/dma/contiguous.c: 3
 * Copyright (c) 2010-2011 by Samsung Electronics. /Users/rubber/linux/kernel/dma/contiguous.c: 4
 * Written by: /Users/rubber/linux/kernel/dma/contiguous.c: 5
 *	Marek Szyprowski <m.szyprowski@samsung.com> /Users/rubber/linux/kernel/dma/contiguous.c: 6
 *	Michal Nazarewicz <mina86@mina86.com> /Users/rubber/linux/kernel/dma/contiguous.c: 7
 * Contiguous Memory Allocator /Users/rubber/linux/kernel/dma/contiguous.c: 9
 *   The Contiguous Memory Allocator (CMA) makes it possible to /Users/rubber/linux/kernel/dma/contiguous.c: 11
 *   allocate big contiguous chunks of memory after the system has /Users/rubber/linux/kernel/dma/contiguous.c: 12
 *   booted. /Users/rubber/linux/kernel/dma/contiguous.c: 13
 * Why is it needed? /Users/rubber/linux/kernel/dma/contiguous.c: 15
 *   Various devices on embedded systems have no scatter-getter and/or /Users/rubber/linux/kernel/dma/contiguous.c: 17
 *   IO map support and require contiguous blocks of memory to /Users/rubber/linux/kernel/dma/contiguous.c: 18
 *   operate.  They include devices such as cameras, hardware video /Users/rubber/linux/kernel/dma/contiguous.c: 19
 *   coders, etc. /Users/rubber/linux/kernel/dma/contiguous.c: 20
 *   Such devices often require big memory buffers (a full HD frame /Users/rubber/linux/kernel/dma/contiguous.c: 22
 *   is, for instance, more than 2 mega pixels large, i.e. more than 6 /Users/rubber/linux/kernel/dma/contiguous.c: 23
 *   MB of memory), which makes mechanisms such as kmalloc() or /Users/rubber/linux/kernel/dma/contiguous.c: 24
 *   alloc_page() ineffective. /Users/rubber/linux/kernel/dma/contiguous.c: 25
 *   At the same time, a solution where a big memory region is /Users/rubber/linux/kernel/dma/contiguous.c: 27
 *   reserved for a device is suboptimal since often more memory is /Users/rubber/linux/kernel/dma/contiguous.c: 28
 *   reserved then strictly required and, moreover, the memory is /Users/rubber/linux/kernel/dma/contiguous.c: 29
 *   inaccessible to page system even if device drivers don't use it. /Users/rubber/linux/kernel/dma/contiguous.c: 30
 *   CMA tries to solve this issue by operating on memory regions /Users/rubber/linux/kernel/dma/contiguous.c: 32
 *   where only movable pages can be allocated from.  This way, kernel /Users/rubber/linux/kernel/dma/contiguous.c: 33
 *   can use the memory for pagecache and when device driver requests /Users/rubber/linux/kernel/dma/contiguous.c: 34
 *   it, allocated pages can be migrated. /Users/rubber/linux/kernel/dma/contiguous.c: 35
 * Default global CMA area size can be defined in kernel's .config. /Users/rubber/linux/kernel/dma/contiguous.c: 63
 * This is useful mainly for distro maintainers to create a kernel /Users/rubber/linux/kernel/dma/contiguous.c: 64
 * that works correctly for most supported systems. /Users/rubber/linux/kernel/dma/contiguous.c: 65
 * The size can be set in bytes or as a percentage of the total memory /Users/rubber/linux/kernel/dma/contiguous.c: 66
 * in the system. /Users/rubber/linux/kernel/dma/contiguous.c: 67
 * Users, who want to set the size of global CMA area for their system /Users/rubber/linux/kernel/dma/contiguous.c: 69
 * should use cma= kernel parameter. /Users/rubber/linux/kernel/dma/contiguous.c: 70
 * dma_contiguous_reserve() - reserve area(s) for contiguous memory handling /Users/rubber/linux/kernel/dma/contiguous.c: 159
 * @limit: End address of the reserved memory (optional, 0 for any). /Users/rubber/linux/kernel/dma/contiguous.c: 160
 * This function reserves memory from early allocator. It should be /Users/rubber/linux/kernel/dma/contiguous.c: 162
 * called by arch specific code once the early allocator (memblock or bootmem) /Users/rubber/linux/kernel/dma/contiguous.c: 163
 * has been activated and all other subsystems have already allocated/reserved /Users/rubber/linux/kernel/dma/contiguous.c: 164
 * memory. /Users/rubber/linux/kernel/dma/contiguous.c: 165
 * dma_contiguous_reserve_area() - reserve custom contiguous area /Users/rubber/linux/kernel/dma/contiguous.c: 211
 * @size: Size of the reserved area (in bytes), /Users/rubber/linux/kernel/dma/contiguous.c: 212
 * @base: Base address of the reserved area optional, use 0 for any /Users/rubber/linux/kernel/dma/contiguous.c: 213
 * @limit: End address of the reserved memory (optional, 0 for any). /Users/rubber/linux/kernel/dma/contiguous.c: 214
 * @res_cma: Pointer to store the created cma region. /Users/rubber/linux/kernel/dma/contiguous.c: 215
 * @fixed: hint about where to place the reserved area /Users/rubber/linux/kernel/dma/contiguous.c: 216
 * This function reserves memory from early allocator. It should be /Users/rubber/linux/kernel/dma/contiguous.c: 218
 * called by arch specific code once the early allocator (memblock or bootmem) /Users/rubber/linux/kernel/dma/contiguous.c: 219
 * has been activated and all other subsystems have already allocated/reserved /Users/rubber/linux/kernel/dma/contiguous.c: 220
 * memory. This function allows to create custom reserved areas for specific /Users/rubber/linux/kernel/dma/contiguous.c: 221
 * devices. /Users/rubber/linux/kernel/dma/contiguous.c: 222
 * If @fixed is true, reserve contiguous area at exactly @base.  If false, /Users/rubber/linux/kernel/dma/contiguous.c: 224
 * reserve in range from @base to @limit. /Users/rubber/linux/kernel/dma/contiguous.c: 225
 * dma_alloc_from_contiguous() - allocate pages from contiguous area /Users/rubber/linux/kernel/dma/contiguous.c: 246
 * @dev:   Pointer to device for which the allocation is performed. /Users/rubber/linux/kernel/dma/contiguous.c: 247
 * @count: Requested number of pages. /Users/rubber/linux/kernel/dma/contiguous.c: 248
 * @align: Requested alignment of pages (in PAGE_SIZE order). /Users/rubber/linux/kernel/dma/contiguous.c: 249
 * @no_warn: Avoid printing message about failed allocation. /Users/rubber/linux/kernel/dma/contiguous.c: 250
 * This function allocates memory buffer for specified device. It uses /Users/rubber/linux/kernel/dma/contiguous.c: 252
 * device specific contiguous memory area if available or the default /Users/rubber/linux/kernel/dma/contiguous.c: 253
 * global one. Requires architecture specific dev_get_cma_area() helper /Users/rubber/linux/kernel/dma/contiguous.c: 254
 * function. /Users/rubber/linux/kernel/dma/contiguous.c: 255
 * dma_release_from_contiguous() - release allocated pages /Users/rubber/linux/kernel/dma/contiguous.c: 267
 * @dev:   Pointer to device for which the pages were allocated. /Users/rubber/linux/kernel/dma/contiguous.c: 268
 * @pages: Allocated pages. /Users/rubber/linux/kernel/dma/contiguous.c: 269
 * @count: Number of allocated pages. /Users/rubber/linux/kernel/dma/contiguous.c: 270
 * This function releases memory allocated by dma_alloc_from_contiguous(). /Users/rubber/linux/kernel/dma/contiguous.c: 272
 * It returns false when provided pages do not belong to contiguous area and /Users/rubber/linux/kernel/dma/contiguous.c: 273
 * true otherwise. /Users/rubber/linux/kernel/dma/contiguous.c: 274
 * dma_alloc_contiguous() - allocate contiguous pages /Users/rubber/linux/kernel/dma/contiguous.c: 290
 * @dev:   Pointer to device for which the allocation is performed. /Users/rubber/linux/kernel/dma/contiguous.c: 291
 * @size:  Requested allocation size. /Users/rubber/linux/kernel/dma/contiguous.c: 292
 * @gfp:   Allocation flags. /Users/rubber/linux/kernel/dma/contiguous.c: 293
 * tries to use device specific contiguous memory area if available, or it /Users/rubber/linux/kernel/dma/contiguous.c: 295
 * tries to use per-numa cma, if the allocation fails, it will fallback to /Users/rubber/linux/kernel/dma/contiguous.c: 296
 * try default global one. /Users/rubber/linux/kernel/dma/contiguous.c: 297
 * Note that it bypass one-page size of allocations from the per-numa and /Users/rubber/linux/kernel/dma/contiguous.c: 299
 * global area as the addresses within one page are always contiguous, so /Users/rubber/linux/kernel/dma/contiguous.c: 300
 * there is no need to waste CMA pages for that kind; it also helps reduce /Users/rubber/linux/kernel/dma/contiguous.c: 301
 * fragmentations. /Users/rubber/linux/kernel/dma/contiguous.c: 302
 * dma_free_contiguous() - release allocated pages /Users/rubber/linux/kernel/dma/contiguous.c: 337
 * @dev:   Pointer to device for which the pages were allocated. /Users/rubber/linux/kernel/dma/contiguous.c: 338
 * @page:  Pointer to the allocated pages. /Users/rubber/linux/kernel/dma/contiguous.c: 339
 * @size:  Size of allocated pages. /Users/rubber/linux/kernel/dma/contiguous.c: 340
 * This function releases memory allocated by dma_alloc_contiguous(). As the /Users/rubber/linux/kernel/dma/contiguous.c: 342
 * cma_release returns false when provided pages do not belong to contiguous /Users/rubber/linux/kernel/dma/contiguous.c: 343
 * area and true otherwise, this function then does a fallback __free_pages() /Users/rubber/linux/kernel/dma/contiguous.c: 344
 * upon a false-return. /Users/rubber/linux/kernel/dma/contiguous.c: 345
		/* /Users/rubber/linux/kernel/dma/contiguous.c: 356
		 * otherwise, page is from either per-numa cma or default cma /Users/rubber/linux/kernel/dma/contiguous.c: 357
 * Support for reserved memory regions defined in device tree /Users/rubber/linux/kernel/dma/contiguous.c: 373
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/direct.c: 1
 * Copyright (C) 2018-2020 Christoph Hellwig. /Users/rubber/linux/kernel/dma/direct.c: 3
 * DMA operations that map physical memory directly without using an IOMMU. /Users/rubber/linux/kernel/dma/direct.c: 5
 * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use /Users/rubber/linux/kernel/dma/direct.c: 19
 * it for entirely different regions. In that case the arch code needs to /Users/rubber/linux/kernel/dma/direct.c: 20
 * override the variable below for dma-direct to work properly. /Users/rubber/linux/kernel/dma/direct.c: 21
	/* /Users/rubber/linux/kernel/dma/direct.c: 52
	 * Optimistically try the zone that the physical address mask falls /Users/rubber/linux/kernel/dma/direct.c: 53
	 * into first.  If that returns memory that isn't actually addressable /Users/rubber/linux/kernel/dma/direct.c: 54
	 * we will fallback to the next lower zone and try again. /Users/rubber/linux/kernel/dma/direct.c: 55
	 * /Users/rubber/linux/kernel/dma/direct.c: 56
	 * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding /Users/rubber/linux/kernel/dma/direct.c: 57
	 * zones. /Users/rubber/linux/kernel/dma/direct.c: 58
	/* /Users/rubber/linux/kernel/dma/direct.c: 187
	 * Remapping or decrypting memory may block. If either is required and /Users/rubber/linux/kernel/dma/direct.c: 188
	 * we can't block, allocate the memory from the atomic pools. /Users/rubber/linux/kernel/dma/direct.c: 189
	 * If restricted DMA (i.e., is_swiotlb_for_alloc) is required, one must /Users/rubber/linux/kernel/dma/direct.c: 190
	 * set up another device coherent pool by shared-dma-pool and use /Users/rubber/linux/kernel/dma/direct.c: 191
	 * dma_alloc_from_dev_coherent instead. /Users/rubber/linux/kernel/dma/direct.c: 192
		/* /Users/rubber/linux/kernel/dma/direct.c: 230
		 * Depending on the cma= arguments and per-arch setup /Users/rubber/linux/kernel/dma/direct.c: 231
		 * dma_alloc_contiguous could return highmem pages. /Users/rubber/linux/kernel/dma/direct.c: 232
		 * Without remapping there is no way to return them here, /Users/rubber/linux/kernel/dma/direct.c: 233
		 * so log an error and fail. /Users/rubber/linux/kernel/dma/direct.c: 234
		/* /Users/rubber/linux/kernel/dma/direct.c: 333
		 * Depending on the cma= arguments and per-arch setup /Users/rubber/linux/kernel/dma/direct.c: 334
		 * dma_alloc_contiguous could return highmem pages. /Users/rubber/linux/kernel/dma/direct.c: 335
		 * Without remapping there is no way to return them here, /Users/rubber/linux/kernel/dma/direct.c: 336
		 * so log an error and fail. /Users/rubber/linux/kernel/dma/direct.c: 337
	/* /Users/rubber/linux/kernel/dma/direct.c: 518
	 * Because 32-bit DMA masks are so common we expect every architecture /Users/rubber/linux/kernel/dma/direct.c: 519
	 * to be able to satisfy them - either by not supporting more physical /Users/rubber/linux/kernel/dma/direct.c: 520
	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the /Users/rubber/linux/kernel/dma/direct.c: 521
	 * architecture needs to use an IOMMU instead of the direct mapping. /Users/rubber/linux/kernel/dma/direct.c: 522
	/* /Users/rubber/linux/kernel/dma/direct.c: 527
	 * This check needs to be against the actual bit mask value, so use /Users/rubber/linux/kernel/dma/direct.c: 528
	 * phys_to_dma_unencrypted() here so that the SME encryption mask isn't /Users/rubber/linux/kernel/dma/direct.c: 529
	 * part of the check. /Users/rubber/linux/kernel/dma/direct.c: 530
 * dma_direct_set_offset - Assign scalar offset for a single DMA range. /Users/rubber/linux/kernel/dma/direct.c: 553
 * @dev:	device pointer; needed to "own" the alloced memory. /Users/rubber/linux/kernel/dma/direct.c: 554
 * @cpu_start:  beginning of memory region covered by this offset. /Users/rubber/linux/kernel/dma/direct.c: 555
 * @dma_start:  beginning of DMA/PCI region covered by this offset. /Users/rubber/linux/kernel/dma/direct.c: 556
 * @size:	size of the region. /Users/rubber/linux/kernel/dma/direct.c: 557
 * This is for the simple case of a uniform offset which cannot /Users/rubber/linux/kernel/dma/direct.c: 559
 * be discovered by "dma-ranges". /Users/rubber/linux/kernel/dma/direct.c: 560
 * It returns -ENOMEM if out of memory, -EINVAL if a map /Users/rubber/linux/kernel/dma/direct.c: 562
 * already exists, 0 otherwise. /Users/rubber/linux/kernel/dma/direct.c: 563
 * Note: any call to this from a driver is a bug.  The mapping needs /Users/rubber/linux/kernel/dma/direct.c: 565
 * to be described by the device tree or other firmware interfaces. /Users/rubber/linux/kernel/dma/direct.c: 566
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/coherent.c: 1
 * Coherent per-device memory handling. /Users/rubber/linux/kernel/dma/coherent.c: 3
 * Borrowed from i386 /Users/rubber/linux/kernel/dma/coherent.c: 4
 * Declare a region of memory to be handed out by dma_alloc_coherent() when it /Users/rubber/linux/kernel/dma/coherent.c: 101
 * is asked for coherent memory for this device.  This shall only be used /Users/rubber/linux/kernel/dma/coherent.c: 102
 * from platform code, usually based on the device tree description. /Users/rubber/linux/kernel/dma/coherent.c: 103
 * phys_addr is the CPU physical address to which the memory is currently /Users/rubber/linux/kernel/dma/coherent.c: 105
 * assigned (this will be ioremapped so the CPU can access the region). /Users/rubber/linux/kernel/dma/coherent.c: 106
 * device_addr is the DMA address the device needs to be programmed with to /Users/rubber/linux/kernel/dma/coherent.c: 108
 * actually address this memory (this will be handed out as the dma_addr_t in /Users/rubber/linux/kernel/dma/coherent.c: 109
 * dma_alloc_coherent()). /Users/rubber/linux/kernel/dma/coherent.c: 110
 * size is the size of the area (must be a multiple of PAGE_SIZE). /Users/rubber/linux/kernel/dma/coherent.c: 112
 * As a simplification for the platforms, only *one* such region of memory may /Users/rubber/linux/kernel/dma/coherent.c: 114
 * be declared per device. /Users/rubber/linux/kernel/dma/coherent.c: 115
	/* /Users/rubber/linux/kernel/dma/coherent.c: 151
	 * Memory was found in the coherent area. /Users/rubber/linux/kernel/dma/coherent.c: 152
 * dma_alloc_from_dev_coherent() - allocate memory from device coherent pool /Users/rubber/linux/kernel/dma/coherent.c: 166
 * @dev:	device from which we allocate memory /Users/rubber/linux/kernel/dma/coherent.c: 167
 * @size:	size of requested memory area /Users/rubber/linux/kernel/dma/coherent.c: 168
 * @dma_handle:	This will be filled with the correct dma handle /Users/rubber/linux/kernel/dma/coherent.c: 169
 * @ret:	This pointer will be filled with the virtual address /Users/rubber/linux/kernel/dma/coherent.c: 170
 *		to allocated area. /Users/rubber/linux/kernel/dma/coherent.c: 171
 * This function should be only called from per-arch dma_alloc_coherent() /Users/rubber/linux/kernel/dma/coherent.c: 173
 * to support allocation from per-device coherent memory pools. /Users/rubber/linux/kernel/dma/coherent.c: 174
 * Returns 0 if dma_alloc_coherent should continue with allocating from /Users/rubber/linux/kernel/dma/coherent.c: 176
 * generic memory areas, or !0 if dma_alloc_coherent should return @ret. /Users/rubber/linux/kernel/dma/coherent.c: 177
 * dma_release_from_dev_coherent() - free memory to device coherent memory pool /Users/rubber/linux/kernel/dma/coherent.c: 208
 * @dev:	device from which the memory was allocated /Users/rubber/linux/kernel/dma/coherent.c: 209
 * @order:	the order of pages allocated /Users/rubber/linux/kernel/dma/coherent.c: 210
 * @vaddr:	virtual address of allocated pages /Users/rubber/linux/kernel/dma/coherent.c: 211
 * This checks whether the memory was allocated from the per-device /Users/rubber/linux/kernel/dma/coherent.c: 213
 * coherent memory pool and if so, releases that memory. /Users/rubber/linux/kernel/dma/coherent.c: 214
 * Returns 1 if we correctly released the memory, or 0 if the caller should /Users/rubber/linux/kernel/dma/coherent.c: 216
 * proceed with releasing memory from generic pools. /Users/rubber/linux/kernel/dma/coherent.c: 217
 * dma_mmap_from_dev_coherent() - mmap memory from the device coherent pool /Users/rubber/linux/kernel/dma/coherent.c: 249
 * @dev:	device from which the memory was allocated /Users/rubber/linux/kernel/dma/coherent.c: 250
 * @vma:	vm_area for the userspace memory /Users/rubber/linux/kernel/dma/coherent.c: 251
 * @vaddr:	cpu address returned by dma_alloc_from_dev_coherent /Users/rubber/linux/kernel/dma/coherent.c: 252
 * @size:	size of the memory buffer allocated /Users/rubber/linux/kernel/dma/coherent.c: 253
 * @ret:	result from remap_pfn_range() /Users/rubber/linux/kernel/dma/coherent.c: 254
 * This checks whether the memory was allocated from the per-device /Users/rubber/linux/kernel/dma/coherent.c: 256
 * coherent memory pool and if so, maps that memory to the provided vma. /Users/rubber/linux/kernel/dma/coherent.c: 257
 * Returns 1 if @vaddr belongs to the device coherent pool and the caller /Users/rubber/linux/kernel/dma/coherent.c: 259
 * should return @ret, or 0 if they should proceed with mapping memory from /Users/rubber/linux/kernel/dma/coherent.c: 260
 * generic areas. /Users/rubber/linux/kernel/dma/coherent.c: 261
 * Support for reserved memory regions defined in device tree /Users/rubber/linux/kernel/dma/coherent.c: 317
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/dummy.c: 1
 * Dummy DMA ops that always fail. /Users/rubber/linux/kernel/dma/dummy.c: 3
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/ops_helpers.c: 1
 * Helpers for DMA ops implementations.  These generally rely on the fact that /Users/rubber/linux/kernel/dma/ops_helpers.c: 3
 * the allocated memory contains normal pages in the direct kernel mapping. /Users/rubber/linux/kernel/dma/ops_helpers.c: 4
 * Create scatter-list for the already allocated DMA buffer. /Users/rubber/linux/kernel/dma/ops_helpers.c: 16
 * Create userspace mapping for the DMA-coherent memory. /Users/rubber/linux/kernel/dma/ops_helpers.c: 32
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/dma/debug.c: 1
 * Copyright (C) 2008 Advanced Micro Devices, Inc. /Users/rubber/linux/kernel/dma/debug.c: 3
 * Author: Joerg Roedel <joerg.roedel@amd.com> /Users/rubber/linux/kernel/dma/debug.c: 5
 * struct dma_debug_entry - track a dma_map* or dma_alloc_coherent mapping /Users/rubber/linux/kernel/dma/debug.c: 53
 * @list: node on pre-allocated free_entries list /Users/rubber/linux/kernel/dma/debug.c: 54
 * @dev: 'dev' argument to dma_map_{page|single|sg} or dma_alloc_coherent /Users/rubber/linux/kernel/dma/debug.c: 55
 * @size: length of the mapping /Users/rubber/linux/kernel/dma/debug.c: 56
 * @type: single, page, sg, coherent /Users/rubber/linux/kernel/dma/debug.c: 57
 * @direction: enum dma_data_direction /Users/rubber/linux/kernel/dma/debug.c: 58
 * @sg_call_ents: 'nents' from dma_map_sg /Users/rubber/linux/kernel/dma/debug.c: 59
 * @sg_mapped_ents: 'mapped_ents' from dma_map_sg /Users/rubber/linux/kernel/dma/debug.c: 60
 * @pfn: page frame of the start address /Users/rubber/linux/kernel/dma/debug.c: 61
 * @offset: offset of mapping relative to pfn /Users/rubber/linux/kernel/dma/debug.c: 62
 * @map_err_type: track whether dma_mapping_error() was checked /Users/rubber/linux/kernel/dma/debug.c: 63
 * @stacktrace: support backtraces when a violation is detected /Users/rubber/linux/kernel/dma/debug.c: 64
 * The access to some variables in this macro is racy. We can't use atomic_t /Users/rubber/linux/kernel/dma/debug.c: 154
 * here because all these variables are exported to debugfs. Some of them even /Users/rubber/linux/kernel/dma/debug.c: 155
 * writeable. This is also the reason why a lock won't help much. But anyway, /Users/rubber/linux/kernel/dma/debug.c: 156
 * the races are no big deal. Here is why: /Users/rubber/linux/kernel/dma/debug.c: 157
 *   error_count: the addition is racy, but the worst thing that can happen is /Users/rubber/linux/kernel/dma/debug.c: 159
 *                that we don't count some errors /Users/rubber/linux/kernel/dma/debug.c: 160
 *   show_num_errors: the subtraction is racy. Also no big deal because in /Users/rubber/linux/kernel/dma/debug.c: 161
 *                    worst case this will result in one warning more in the /Users/rubber/linux/kernel/dma/debug.c: 162
 *                    system log than the user configured. This variable is /Users/rubber/linux/kernel/dma/debug.c: 163
 *                    writeable via debugfs. /Users/rubber/linux/kernel/dma/debug.c: 164
 * Hash related functions /Users/rubber/linux/kernel/dma/debug.c: 231
 * Every DMA-API request is saved into a struct dma_debug_entry. To /Users/rubber/linux/kernel/dma/debug.c: 233
 * have quick access to these structs they are stored into a hash. /Users/rubber/linux/kernel/dma/debug.c: 234
	/* /Users/rubber/linux/kernel/dma/debug.c: 238
	 * Hash function is based on the dma address. /Users/rubber/linux/kernel/dma/debug.c: 239
	 * We use bits 20-27 here as the index into the hash /Users/rubber/linux/kernel/dma/debug.c: 240
 * Request exclusive access to a hash bucket for a given dma_debug_entry. /Users/rubber/linux/kernel/dma/debug.c: 246
 * Give up exclusive access to the hash bucket /Users/rubber/linux/kernel/dma/debug.c: 261
 * Search a given entry in the hash bucket list /Users/rubber/linux/kernel/dma/debug.c: 290
		/* /Users/rubber/linux/kernel/dma/debug.c: 303
		 * Some drivers map the same physical address multiple /Users/rubber/linux/kernel/dma/debug.c: 304
		 * times. Without a hardware IOMMU this results in the /Users/rubber/linux/kernel/dma/debug.c: 305
		 * same device addresses being put into the dma-debug /Users/rubber/linux/kernel/dma/debug.c: 306
		 * hash multiple times too. This can result in false /Users/rubber/linux/kernel/dma/debug.c: 307
		 * positives being reported. Therefore we implement a /Users/rubber/linux/kernel/dma/debug.c: 308
		 * best-fit algorithm here which returns the entry from /Users/rubber/linux/kernel/dma/debug.c: 309
		 * the hash which fits best to the reference value /Users/rubber/linux/kernel/dma/debug.c: 310
		 * instead of the first-fit. /Users/rubber/linux/kernel/dma/debug.c: 311
			/* /Users/rubber/linux/kernel/dma/debug.c: 324
			 * We found an entry that fits better then the /Users/rubber/linux/kernel/dma/debug.c: 325
			 * previous one or it is the 1st match. /Users/rubber/linux/kernel/dma/debug.c: 326
	/* /Users/rubber/linux/kernel/dma/debug.c: 333
	 * If we have multiple matches but no perfect-fit, just return /Users/rubber/linux/kernel/dma/debug.c: 334
	 * NULL. /Users/rubber/linux/kernel/dma/debug.c: 335
		/* /Users/rubber/linux/kernel/dma/debug.c: 363
		 * Nothing found, go back a hash bucket /Users/rubber/linux/kernel/dma/debug.c: 364
 * Add an entry to a hash bucket /Users/rubber/linux/kernel/dma/debug.c: 376
 * Remove entry from a hash bucket list /Users/rubber/linux/kernel/dma/debug.c: 385
 * Dump mapping entries for debugging purposes /Users/rubber/linux/kernel/dma/debug.c: 401
 * For each mapping (initial cacheline in the case of /Users/rubber/linux/kernel/dma/debug.c: 432
 * dma_alloc_coherent/dma_map_page, initial cacheline in each page of a /Users/rubber/linux/kernel/dma/debug.c: 433
 * scatterlist, or the cacheline specified in dma_map_single) insert /Users/rubber/linux/kernel/dma/debug.c: 434
 * into this tree using the cacheline as the key. At /Users/rubber/linux/kernel/dma/debug.c: 435
 * dma_unmap_{single|sg|page} or dma_free_coherent delete the entry.  If /Users/rubber/linux/kernel/dma/debug.c: 436
 * the entry already exists at insertion time add a tag as a reference /Users/rubber/linux/kernel/dma/debug.c: 437
 * count for the overlapping mappings.  For now, the overlap tracking /Users/rubber/linux/kernel/dma/debug.c: 438
 * just ensures that 'unmaps' balance 'maps' before marking the /Users/rubber/linux/kernel/dma/debug.c: 439
 * cacheline idle, but we should also be flagging overlaps as an API /Users/rubber/linux/kernel/dma/debug.c: 440
 * violation. /Users/rubber/linux/kernel/dma/debug.c: 441
 * Memory usage is mostly constrained by the maximum number of available /Users/rubber/linux/kernel/dma/debug.c: 443
 * dma-debug entries in that we need a free dma_debug_entry before /Users/rubber/linux/kernel/dma/debug.c: 444
 * inserting into the tree.  In the case of dma_map_page and /Users/rubber/linux/kernel/dma/debug.c: 445
 * dma_alloc_coherent there is only one dma_debug_entry and one /Users/rubber/linux/kernel/dma/debug.c: 446
 * dma_active_cacheline entry to track per event.  dma_map_sg(), on the /Users/rubber/linux/kernel/dma/debug.c: 447
 * other hand, consumes a single dma_debug_entry, but inserts 'nents' /Users/rubber/linux/kernel/dma/debug.c: 448
 * entries into the tree. /Users/rubber/linux/kernel/dma/debug.c: 449
	/* If we overflowed the overlap counter then we're potentially /Users/rubber/linux/kernel/dma/debug.c: 495
	 * leaking dma-mappings. /Users/rubber/linux/kernel/dma/debug.c: 496
	/* If the device is not writing memory then we don't have any /Users/rubber/linux/kernel/dma/debug.c: 516
	 * concerns about the cpu consuming stale data.  This mitigates /Users/rubber/linux/kernel/dma/debug.c: 517
	 * legitimate usages of overlapping mappings. /Users/rubber/linux/kernel/dma/debug.c: 518
	/* since we are counting overlaps the final put of the /Users/rubber/linux/kernel/dma/debug.c: 542
	 * cacheline will occur when the overlap count is 0. /Users/rubber/linux/kernel/dma/debug.c: 543
	 * active_cacheline_dec_overlap() returns -1 in that case /Users/rubber/linux/kernel/dma/debug.c: 544
 * Wrapper function for adding an entry to the hash. /Users/rubber/linux/kernel/dma/debug.c: 552
 * This function takes care of locking itself. /Users/rubber/linux/kernel/dma/debug.c: 553
/* struct dma_entry allocator /Users/rubber/linux/kernel/dma/debug.c: 620
 * The next two functions implement the allocator for /Users/rubber/linux/kernel/dma/debug.c: 622
 * struct dma_debug_entries. /Users/rubber/linux/kernel/dma/debug.c: 623
	/* /Users/rubber/linux/kernel/dma/debug.c: 659
	 * add to beginning of the list - this way the entries are /Users/rubber/linux/kernel/dma/debug.c: 660
	 * more likely cache hot when they are reallocated. /Users/rubber/linux/kernel/dma/debug.c: 661
 * DMA-API debugging init code /Users/rubber/linux/kernel/dma/debug.c: 670
 * The init code does two things: /Users/rubber/linux/kernel/dma/debug.c: 672
 *   1. Initialize core data structures /Users/rubber/linux/kernel/dma/debug.c: 673
 *   2. Preallocate a given number of dma_debug_entry structs /Users/rubber/linux/kernel/dma/debug.c: 674
	/* /Users/rubber/linux/kernel/dma/debug.c: 687
	 * We can't copy to userspace directly because current_driver_name can /Users/rubber/linux/kernel/dma/debug.c: 688
	 * only be read under the driver_name_lock with irqs disabled. So /Users/rubber/linux/kernel/dma/debug.c: 689
	 * create a temporary copy first. /Users/rubber/linux/kernel/dma/debug.c: 690
	/* /Users/rubber/linux/kernel/dma/debug.c: 707
	 * We can't copy from userspace directly. Access to /Users/rubber/linux/kernel/dma/debug.c: 708
	 * current_driver_name is protected with a write_lock with irqs /Users/rubber/linux/kernel/dma/debug.c: 709
	 * disabled. Since copy_from_user can fault and may sleep we /Users/rubber/linux/kernel/dma/debug.c: 710
	 * need to copy to temporary buffer first /Users/rubber/linux/kernel/dma/debug.c: 711
	/* /Users/rubber/linux/kernel/dma/debug.c: 721
	 * Now handle the string we got from userspace very carefully. /Users/rubber/linux/kernel/dma/debug.c: 722
	 * The rules are: /Users/rubber/linux/kernel/dma/debug.c: 723
	 *         - only use the first token we got /Users/rubber/linux/kernel/dma/debug.c: 724
	 *         - token delimiter is everything looking like a space /Users/rubber/linux/kernel/dma/debug.c: 725
	 *           character (' ', '\n', '\t' ...) /Users/rubber/linux/kernel/dma/debug.c: 726
	 * /Users/rubber/linux/kernel/dma/debug.c: 727
		/* /Users/rubber/linux/kernel/dma/debug.c: 730
		 * If the first character userspace gave us is not /Users/rubber/linux/kernel/dma/debug.c: 731
		 * alphanumerical then assume the filter should be /Users/rubber/linux/kernel/dma/debug.c: 732
		 * switched off. /Users/rubber/linux/kernel/dma/debug.c: 733
	/* /Users/rubber/linux/kernel/dma/debug.c: 742
	 * Now parse out the first token and use it as the name for the /Users/rubber/linux/kernel/dma/debug.c: 743
	 * driver to filter for. /Users/rubber/linux/kernel/dma/debug.c: 744
	/* Do not use dma_debug_initialized here, since we really want to be /Users/rubber/linux/kernel/dma/debug.c: 886
	 * called to set dma_debug_initialized /Users/rubber/linux/kernel/dma/debug.c: 887
	/* /Users/rubber/linux/kernel/dma/debug.c: 1007
	 * This may be no bug in reality - but most implementations of the /Users/rubber/linux/kernel/dma/debug.c: 1008
	 * DMA API don't handle this properly, so check for it here /Users/rubber/linux/kernel/dma/debug.c: 1009
	/* /Users/rubber/linux/kernel/dma/debug.c: 1021
	 * Drivers should use dma_mapping_error() to check the returned /Users/rubber/linux/kernel/dma/debug.c: 1022
	 * addresses of dma_map_single() and dma_map_page(). /Users/rubber/linux/kernel/dma/debug.c: 1023
	 * If not, print this warning message. See Documentation/core-api/dma-api.rst. /Users/rubber/linux/kernel/dma/debug.c: 1024
	/* /Users/rubber/linux/kernel/dma/debug.c: 1157
	 * Either the driver forgot to set dma_parms appropriately, or /Users/rubber/linux/kernel/dma/debug.c: 1158
	 * whoever generated the list forgot to check them. /Users/rubber/linux/kernel/dma/debug.c: 1159
	/* /Users/rubber/linux/kernel/dma/debug.c: 1164
	 * In some cases this could potentially be the DMA API /Users/rubber/linux/kernel/dma/debug.c: 1165
	 * implementation's fault, but it would usually imply that /Users/rubber/linux/kernel/dma/debug.c: 1166
	 * the scatterlist was built inappropriately to begin with. /Users/rubber/linux/kernel/dma/debug.c: 1167
		/* /Users/rubber/linux/kernel/dma/debug.c: 1247
		 * The same physical address can be mapped multiple /Users/rubber/linux/kernel/dma/debug.c: 1248
		 * times. Without a hardware IOMMU this results in the /Users/rubber/linux/kernel/dma/debug.c: 1249
		 * same device addresses being put into the dma-debug /Users/rubber/linux/kernel/dma/debug.c: 1250
		 * hash multiple times too. This can result in false /Users/rubber/linux/kernel/dma/debug.c: 1251
		 * positives being reported. Therefore we implement a /Users/rubber/linux/kernel/dma/debug.c: 1252
		 * best-fit algorithm here which updates the first entry /Users/rubber/linux/kernel/dma/debug.c: 1253
		 * from the hash which fits the reference value and is /Users/rubber/linux/kernel/dma/debug.c: 1254
		 * not currently listed as being checked. /Users/rubber/linux/kernel/dma/debug.c: 1255
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/remap.c: 1
 * Copyright (c) 2014 The Linux Foundation /Users/rubber/linux/kernel/dma/remap.c: 3
 * Remaps an array of PAGE_SIZE pages into another vm_area. /Users/rubber/linux/kernel/dma/remap.c: 19
 * Cannot be used in non-sleeping contexts /Users/rubber/linux/kernel/dma/remap.c: 20
 * Remaps an allocated contiguous region into another vm_area. /Users/rubber/linux/kernel/dma/remap.c: 35
 * Cannot be used in non-sleeping contexts /Users/rubber/linux/kernel/dma/remap.c: 36
 * Unmaps a range previously mapped by dma_common_*_remap /Users/rubber/linux/kernel/dma/remap.c: 58
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/pool.c: 1
 * Copyright (C) 2012 ARM Ltd. /Users/rubber/linux/kernel/dma/pool.c: 3
 * Copyright (C) 2020 Google LLC /Users/rubber/linux/kernel/dma/pool.c: 4
	/* /Users/rubber/linux/kernel/dma/pool.c: 112
	 * Memory in the atomic DMA pools must be unencrypted, the pools do not /Users/rubber/linux/kernel/dma/pool.c: 113
	 * shrink so no re-encryption occurs in dma_direct_free(). /Users/rubber/linux/kernel/dma/pool.c: 114
	/* /Users/rubber/linux/kernel/dma/pool.c: 191
	 * If coherent_pool was not used on the command line, default the pool /Users/rubber/linux/kernel/dma/pool.c: 192
	 * sizes to 128KB per 1GB of memory, min 128KB, max MAX_ORDER-1. /Users/rubber/linux/kernel/dma/pool.c: 193
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/dma/mapping.c: 1
 * arch-independent dma-mapping routines /Users/rubber/linux/kernel/dma/mapping.c: 3
 * Copyright (c) 2006  SUSE Linux Products GmbH /Users/rubber/linux/kernel/dma/mapping.c: 5
 * Copyright (c) 2006  Tejun Heo <teheo@suse.de> /Users/rubber/linux/kernel/dma/mapping.c: 6
 * Managed DMA API /Users/rubber/linux/kernel/dma/mapping.c: 22
 * dmam_free_coherent - Managed dma_free_coherent() /Users/rubber/linux/kernel/dma/mapping.c: 52
 * @dev: Device to free coherent memory for /Users/rubber/linux/kernel/dma/mapping.c: 53
 * @size: Size of allocation /Users/rubber/linux/kernel/dma/mapping.c: 54
 * @vaddr: Virtual address of the memory to free /Users/rubber/linux/kernel/dma/mapping.c: 55
 * @dma_handle: DMA handle of the memory to free /Users/rubber/linux/kernel/dma/mapping.c: 56
 * Managed dma_free_coherent(). /Users/rubber/linux/kernel/dma/mapping.c: 58
 * dmam_alloc_attrs - Managed dma_alloc_attrs() /Users/rubber/linux/kernel/dma/mapping.c: 71
 * @dev: Device to allocate non_coherent memory for /Users/rubber/linux/kernel/dma/mapping.c: 72
 * @size: Size of allocation /Users/rubber/linux/kernel/dma/mapping.c: 73
 * @dma_handle: Out argument for allocated DMA handle /Users/rubber/linux/kernel/dma/mapping.c: 74
 * @gfp: Allocation flags /Users/rubber/linux/kernel/dma/mapping.c: 75
 * @attrs: Flags in the DMA_ATTR_* namespace. /Users/rubber/linux/kernel/dma/mapping.c: 76
 * Managed dma_alloc_attrs().  Memory allocated using this function will be /Users/rubber/linux/kernel/dma/mapping.c: 78
 * automatically released on driver detach. /Users/rubber/linux/kernel/dma/mapping.c: 79
 * RETURNS: /Users/rubber/linux/kernel/dma/mapping.c: 81
 * Pointer to allocated memory on success, NULL on failure. /Users/rubber/linux/kernel/dma/mapping.c: 82
 * Check if the devices uses a direct mapping for streaming DMA operations. /Users/rubber/linux/kernel/dma/mapping.c: 126
 * This allows IOMMU drivers to set a bypass mode if the DMA mask is large /Users/rubber/linux/kernel/dma/mapping.c: 127
 * enough. /Users/rubber/linux/kernel/dma/mapping.c: 128
 * dma_map_sg_attrs - Map the given buffer for DMA /Users/rubber/linux/kernel/dma/mapping.c: 207
 * @dev:	The device for which to perform the DMA operation /Users/rubber/linux/kernel/dma/mapping.c: 208
 * @sg:		The sg_table object describing the buffer /Users/rubber/linux/kernel/dma/mapping.c: 209
 * @nents:	Number of entries to map /Users/rubber/linux/kernel/dma/mapping.c: 210
 * @dir:	DMA direction /Users/rubber/linux/kernel/dma/mapping.c: 211
 * @attrs:	Optional DMA attributes for the map operation /Users/rubber/linux/kernel/dma/mapping.c: 212
 * Maps a buffer described by a scatterlist passed in the sg argument with /Users/rubber/linux/kernel/dma/mapping.c: 214
 * nents segments for the @dir DMA operation by the @dev device. /Users/rubber/linux/kernel/dma/mapping.c: 215
 * Returns the number of mapped entries (which can be less than nents) /Users/rubber/linux/kernel/dma/mapping.c: 217
 * on success. Zero is returned for any error. /Users/rubber/linux/kernel/dma/mapping.c: 218
 * dma_unmap_sg_attrs() should be used to unmap the buffer with the /Users/rubber/linux/kernel/dma/mapping.c: 220
 * original sg and original nents (not the value returned by this funciton). /Users/rubber/linux/kernel/dma/mapping.c: 221
 * dma_map_sgtable - Map the given buffer for DMA /Users/rubber/linux/kernel/dma/mapping.c: 236
 * @dev:	The device for which to perform the DMA operation /Users/rubber/linux/kernel/dma/mapping.c: 237
 * @sgt:	The sg_table object describing the buffer /Users/rubber/linux/kernel/dma/mapping.c: 238
 * @dir:	DMA direction /Users/rubber/linux/kernel/dma/mapping.c: 239
 * @attrs:	Optional DMA attributes for the map operation /Users/rubber/linux/kernel/dma/mapping.c: 240
 * Maps a buffer described by a scatterlist stored in the given sg_table /Users/rubber/linux/kernel/dma/mapping.c: 242
 * object for the @dir DMA operation by the @dev device. After success, the /Users/rubber/linux/kernel/dma/mapping.c: 243
 * ownership for the buffer is transferred to the DMA domain.  One has to /Users/rubber/linux/kernel/dma/mapping.c: 244
 * call dma_sync_sgtable_for_cpu() or dma_unmap_sgtable() to move the /Users/rubber/linux/kernel/dma/mapping.c: 245
 * ownership of the buffer back to the CPU domain before touching the /Users/rubber/linux/kernel/dma/mapping.c: 246
 * buffer by the CPU. /Users/rubber/linux/kernel/dma/mapping.c: 247
 * Returns 0 on success or a negative error code on error. The following /Users/rubber/linux/kernel/dma/mapping.c: 249
 * error codes are supported with the given meaning: /Users/rubber/linux/kernel/dma/mapping.c: 250
 *   -EINVAL	An invalid argument, unaligned access or other error /Users/rubber/linux/kernel/dma/mapping.c: 252
 *		in usage. Will not succeed if retried. /Users/rubber/linux/kernel/dma/mapping.c: 253
 *   -ENOMEM	Insufficient resources (like memory or IOVA space) to /Users/rubber/linux/kernel/dma/mapping.c: 254
 *		complete the mapping. Should succeed if retried later. /Users/rubber/linux/kernel/dma/mapping.c: 255
 *   -EIO	Legacy error code with an unknown meaning. eg. this is /Users/rubber/linux/kernel/dma/mapping.c: 256
 *		returned if a lower level call returned DMA_MAPPING_ERROR. /Users/rubber/linux/kernel/dma/mapping.c: 257
 * The whole dma_get_sgtable() idea is fundamentally unsafe - it seems /Users/rubber/linux/kernel/dma/mapping.c: 378
 * that the intention is to allow exporting memory allocated via the /Users/rubber/linux/kernel/dma/mapping.c: 379
 * coherent DMA APIs through the dma_buf API, which only accepts a /Users/rubber/linux/kernel/dma/mapping.c: 380
 * scattertable.  This presents a couple of problems: /Users/rubber/linux/kernel/dma/mapping.c: 381
 * 1. Not all memory allocated via the coherent DMA APIs is backed by /Users/rubber/linux/kernel/dma/mapping.c: 382
 *    a struct page /Users/rubber/linux/kernel/dma/mapping.c: 383
 * 2. Passing coherent DMA memory into the streaming APIs is not allowed /Users/rubber/linux/kernel/dma/mapping.c: 384
 *    as we will try to flush the memory through a different alias to that /Users/rubber/linux/kernel/dma/mapping.c: 385
 *    actually being used (and the flushes are redundant.) /Users/rubber/linux/kernel/dma/mapping.c: 386
 * Return the page attributes used for mapping dma_alloc_* memory, either in /Users/rubber/linux/kernel/dma/mapping.c: 405
 * kernel space if remapping is needed, or to userspace through dma_mmap_*. /Users/rubber/linux/kernel/dma/mapping.c: 406
 * dma_can_mmap - check if a given device supports dma_mmap_* /Users/rubber/linux/kernel/dma/mapping.c: 423
 * @dev: device to check /Users/rubber/linux/kernel/dma/mapping.c: 424
 * Returns %true if @dev supports dma_mmap_coherent() and dma_mmap_attrs() to /Users/rubber/linux/kernel/dma/mapping.c: 426
 * map DMA allocations to userspace. /Users/rubber/linux/kernel/dma/mapping.c: 427
 * dma_mmap_attrs - map a coherent DMA allocation into user space /Users/rubber/linux/kernel/dma/mapping.c: 440
 * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices /Users/rubber/linux/kernel/dma/mapping.c: 441
 * @vma: vm_area_struct describing requested user mapping /Users/rubber/linux/kernel/dma/mapping.c: 442
 * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs /Users/rubber/linux/kernel/dma/mapping.c: 443
 * @dma_addr: device-view address returned from dma_alloc_attrs /Users/rubber/linux/kernel/dma/mapping.c: 444
 * @size: size of memory originally requested in dma_alloc_attrs /Users/rubber/linux/kernel/dma/mapping.c: 445
 * @attrs: attributes of mapping properties requested in dma_alloc_attrs /Users/rubber/linux/kernel/dma/mapping.c: 446
 * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user /Users/rubber/linux/kernel/dma/mapping.c: 448
 * space.  The coherent DMA buffer must not be freed by the driver until the /Users/rubber/linux/kernel/dma/mapping.c: 449
 * user space mapping has been released. /Users/rubber/linux/kernel/dma/mapping.c: 450
	/* /Users/rubber/linux/kernel/dma/mapping.c: 476
	 * We require every DMA ops implementation to at least support a 32-bit /Users/rubber/linux/kernel/dma/mapping.c: 477
	 * DMA mask (and use bounce buffering if that isn't supported in /Users/rubber/linux/kernel/dma/mapping.c: 478
	 * hardware).  As the direct mapping code has its own routine to /Users/rubber/linux/kernel/dma/mapping.c: 479
	 * actually report an optimal mask we default to 32-bit here as that /Users/rubber/linux/kernel/dma/mapping.c: 480
	 * is the right thing for most IOMMUs, and at least not actively /Users/rubber/linux/kernel/dma/mapping.c: 481
	 * harmful in general. /Users/rubber/linux/kernel/dma/mapping.c: 482
	/* /Users/rubber/linux/kernel/dma/mapping.c: 521
	 * On non-coherent platforms which implement DMA-coherent buffers via /Users/rubber/linux/kernel/dma/mapping.c: 522
	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting /Users/rubber/linux/kernel/dma/mapping.c: 523
	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to /Users/rubber/linux/kernel/dma/mapping.c: 524
	 * sleep on some machines, and b) an indication that the driver is /Users/rubber/linux/kernel/dma/mapping.c: 525
	 * probably misusing the coherent API anyway. /Users/rubber/linux/kernel/dma/mapping.c: 526
	/* /Users/rubber/linux/kernel/dma/mapping.c: 713
	 * ->dma_supported sets the bypass flag, so we must always call /Users/rubber/linux/kernel/dma/mapping.c: 714
	 * into the method here unless the device is truly direct mapped. /Users/rubber/linux/kernel/dma/mapping.c: 715
	/* /Users/rubber/linux/kernel/dma/mapping.c: 733
	 * Truncate the mask to the actually supported dma_addr_t width to /Users/rubber/linux/kernel/dma/mapping.c: 734
	 * avoid generating unsupportable addresses. /Users/rubber/linux/kernel/dma/mapping.c: 735
	/* /Users/rubber/linux/kernel/dma/mapping.c: 751
	 * Truncate the mask to the actually supported dma_addr_t width to /Users/rubber/linux/kernel/dma/mapping.c: 752
	 * avoid generating unsupportable addresses. /Users/rubber/linux/kernel/dma/mapping.c: 753
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/dma/map_benchmark.c: 1
 * Copyright (C) 2020 HiSilicon Limited. /Users/rubber/linux/kernel/dma/map_benchmark.c: 3
		/* /Users/rubber/linux/kernel/dma/map_benchmark.c: 75
		 * for a non-coherent device, if we don't stain them in the /Users/rubber/linux/kernel/dma/map_benchmark.c: 76
		 * cache, this will give an underestimate of the real-world /Users/rubber/linux/kernel/dma/map_benchmark.c: 77
		 * overhead of BIDIRECTIONAL or TO_DEVICE mappings; /Users/rubber/linux/kernel/dma/map_benchmark.c: 78
		 * 66 means evertything goes well! 66 is lucky. /Users/rubber/linux/kernel/dma/map_benchmark.c: 79
		/* /Users/rubber/linux/kernel/dma/map_benchmark.c: 271
		 * restore the original dma_mask as many devices' dma_mask are /Users/rubber/linux/kernel/dma/map_benchmark.c: 272
		 * set by architectures, acpi, busses. When we bind them back /Users/rubber/linux/kernel/dma/map_benchmark.c: 273
		 * to their original drivers, those drivers shouldn't see /Users/rubber/linux/kernel/dma/map_benchmark.c: 274
		 * dma_mask changed by benchmark /Users/rubber/linux/kernel/dma/map_benchmark.c: 275
	/* /Users/rubber/linux/kernel/dma/map_benchmark.c: 318
	 * we only permit a device bound with this driver, 2nd probe /Users/rubber/linux/kernel/dma/map_benchmark.c: 319
	 * will fail /Users/rubber/linux/kernel/dma/map_benchmark.c: 320
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/events/callchain.c: 1
 * Performance events callchain code, extracted from core.c: /Users/rubber/linux/kernel/events/callchain.c: 3
 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/events/callchain.c: 5
 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/events/callchain.c: 6
 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/events/callchain.c: 7
 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com> /Users/rubber/linux/kernel/events/callchain.c: 8
	/* /Users/rubber/linux/kernel/events/callchain.c: 76
	 * We can't use the percpu allocation API for data that can be /Users/rubber/linux/kernel/events/callchain.c: 77
	 * accessed from NMI. Use a temporary manual per cpu allocation /Users/rubber/linux/kernel/events/callchain.c: 78
	 * until that gets sorted out. /Users/rubber/linux/kernel/events/callchain.c: 79
	/* /Users/rubber/linux/kernel/events/callchain.c: 121
	 * If requesting per event more than the global cap, /Users/rubber/linux/kernel/events/callchain.c: 122
	 * return a different error to help userspace figure /Users/rubber/linux/kernel/events/callchain.c: 123
	 * this out. /Users/rubber/linux/kernel/events/callchain.c: 124
	 * /Users/rubber/linux/kernel/events/callchain.c: 125
	 * And also do it here so that we have &callchain_mutex held. /Users/rubber/linux/kernel/events/callchain.c: 126
 * Used for sysctl_perf_event_max_stack and /Users/rubber/linux/kernel/events/callchain.c: 233
 * sysctl_perf_event_max_contexts_per_stack. /Users/rubber/linux/kernel/events/callchain.c: 234
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/events/core.c: 1
 * Performance events core code: /Users/rubber/linux/kernel/events/core.c: 3
 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/events/core.c: 5
 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/events/core.c: 6
 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/events/core.c: 7
 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com> /Users/rubber/linux/kernel/events/core.c: 8
		/* /Users/rubber/linux/kernel/events/core.c: 81
		 * Now that we're on right CPU with IRQs disabled, we can test /Users/rubber/linux/kernel/events/core.c: 82
		 * if we hit the right task without races. /Users/rubber/linux/kernel/events/core.c: 83
 * task_function_call - call a function on the cpu on which a task runs /Users/rubber/linux/kernel/events/core.c: 95
 * @p:		the task to evaluate /Users/rubber/linux/kernel/events/core.c: 96
 * @func:	the function to be called /Users/rubber/linux/kernel/events/core.c: 97
 * @info:	the function call argument /Users/rubber/linux/kernel/events/core.c: 98
 * Calls the function @func when the task is currently running. This might /Users/rubber/linux/kernel/events/core.c: 100
 * be on the current CPU, which just calls the function directly.  This will /Users/rubber/linux/kernel/events/core.c: 101
 * retry due to any failures in smp_call_function_single(), such as if the /Users/rubber/linux/kernel/events/core.c: 102
 * task_cpu() goes offline concurrently. /Users/rubber/linux/kernel/events/core.c: 103
 * returns @func return value or -ESRCH or -ENXIO when the process isn't running /Users/rubber/linux/kernel/events/core.c: 105
 * cpu_function_call - call a function on the cpu /Users/rubber/linux/kernel/events/core.c: 134
 * @cpu:	target cpu to queue this function /Users/rubber/linux/kernel/events/core.c: 135
 * @func:	the function to be called /Users/rubber/linux/kernel/events/core.c: 136
 * @info:	the function call argument /Users/rubber/linux/kernel/events/core.c: 137
 * Calls the function @func on the remote cpu. /Users/rubber/linux/kernel/events/core.c: 139
 * returns: @func return value or -ENXIO when the cpu is offline /Users/rubber/linux/kernel/events/core.c: 141
 * On task ctx scheduling... /Users/rubber/linux/kernel/events/core.c: 187
 * When !ctx->nr_events a task context will not be scheduled. This means /Users/rubber/linux/kernel/events/core.c: 189
 * we can disable the scheduler hooks (for performance) without leaving /Users/rubber/linux/kernel/events/core.c: 190
 * pending task ctx state. /Users/rubber/linux/kernel/events/core.c: 191
 * This however results in two special cases: /Users/rubber/linux/kernel/events/core.c: 193
 *  - removing the last event from a task ctx; this is relatively straight /Users/rubber/linux/kernel/events/core.c: 195
 *    forward and is done in __perf_remove_from_context. /Users/rubber/linux/kernel/events/core.c: 196
 *  - adding the first event to a task ctx; this is tricky because we cannot /Users/rubber/linux/kernel/events/core.c: 198
 *    rely on ctx->is_active and therefore cannot use event_function_call(). /Users/rubber/linux/kernel/events/core.c: 199
 *    See perf_install_in_context(). /Users/rubber/linux/kernel/events/core.c: 200
 * If ctx->nr_events, then ctx->is_active and cpuctx->task_ctx are set. /Users/rubber/linux/kernel/events/core.c: 202
	/* /Users/rubber/linux/kernel/events/core.c: 226
	 * Since we do the IPI call without holding ctx->lock things can have /Users/rubber/linux/kernel/events/core.c: 227
	 * changed, double check we hit the task we set out to hit. /Users/rubber/linux/kernel/events/core.c: 228
		/* /Users/rubber/linux/kernel/events/core.c: 236
		 * We only use event_function_call() on established contexts, /Users/rubber/linux/kernel/events/core.c: 237
		 * and event_function() is only ever called when active (or /Users/rubber/linux/kernel/events/core.c: 238
		 * rather, we'll have bailed in task_function_call() or the /Users/rubber/linux/kernel/events/core.c: 239
		 * above ctx->task != current test), therefore we must have /Users/rubber/linux/kernel/events/core.c: 240
		 * ctx->is_active here. /Users/rubber/linux/kernel/events/core.c: 241
		/* /Users/rubber/linux/kernel/events/core.c: 244
		 * And since we have ctx->is_active, cpuctx->task_ctx must /Users/rubber/linux/kernel/events/core.c: 245
		 * match. /Users/rubber/linux/kernel/events/core.c: 246
		/* /Users/rubber/linux/kernel/events/core.c: 271
		 * If this is a !child event, we must hold ctx::mutex to /Users/rubber/linux/kernel/events/core.c: 272
		 * stabilize the event->ctx relation. See /Users/rubber/linux/kernel/events/core.c: 273
		 * perf_event_ctx_lock(). /Users/rubber/linux/kernel/events/core.c: 274
	/* /Users/rubber/linux/kernel/events/core.c: 292
	 * Reload the task pointer, it might have been changed by /Users/rubber/linux/kernel/events/core.c: 293
	 * a concurrent perf_event_context_sched_out(). /Users/rubber/linux/kernel/events/core.c: 294
 * Similar to event_function_call() + event_function(), but hard assumes IRQs /Users/rubber/linux/kernel/events/core.c: 310
 * are already disabled and we're on the right CPU. /Users/rubber/linux/kernel/events/core.c: 311
		/* /Users/rubber/linux/kernel/events/core.c: 336
		 * We must be either inactive or active and the right task, /Users/rubber/linux/kernel/events/core.c: 337
		 * otherwise we're screwed, since we cannot IPI to somewhere /Users/rubber/linux/kernel/events/core.c: 338
		 * else. /Users/rubber/linux/kernel/events/core.c: 339
 * branch priv levels that need permission checks /Users/rubber/linux/kernel/events/core.c: 363
 * perf_sched_events : >0 events exist /Users/rubber/linux/kernel/events/core.c: 379
 * perf_cgroup_events: >0 per-cpu cgroup events exist on this cpu /Users/rubber/linux/kernel/events/core.c: 380
 * perf event paranoia level: /Users/rubber/linux/kernel/events/core.c: 412
 *  -1 - not paranoid at all /Users/rubber/linux/kernel/events/core.c: 413
 *   0 - disallow raw tracepoint access for unpriv /Users/rubber/linux/kernel/events/core.c: 414
 *   1 - disallow cpu events for unpriv /Users/rubber/linux/kernel/events/core.c: 415
 *   2 - disallow kernel profiling for unpriv /Users/rubber/linux/kernel/events/core.c: 416
 * max perf event sample rate /Users/rubber/linux/kernel/events/core.c: 424
	/* /Users/rubber/linux/kernel/events/core.c: 457
	 * If throttling is disabled don't allow the write: /Users/rubber/linux/kernel/events/core.c: 458
 * perf samples are done in some very critical code paths (NMIs). /Users/rubber/linux/kernel/events/core.c: 497
 * If they take too much CPU time, the system can lock up and not /Users/rubber/linux/kernel/events/core.c: 498
 * get any real work done.  This will drop the sample rate when /Users/rubber/linux/kernel/events/core.c: 499
 * we detect that events are taking too long. /Users/rubber/linux/kernel/events/core.c: 500
	/* /Users/rubber/linux/kernel/events/core.c: 535
	 * Note: this will be biased artifically low until we have /Users/rubber/linux/kernel/events/core.c: 536
	 * seen NR_ACCUMULATED_SAMPLES. Doing it this way keeps us /Users/rubber/linux/kernel/events/core.c: 537
	 * from having to maintain a count. /Users/rubber/linux/kernel/events/core.c: 538
	/* /Users/rubber/linux/kernel/events/core.c: 547
	 * Compute a throttle threshold 25% below the current duration. /Users/rubber/linux/kernel/events/core.c: 548
 * State based event timekeeping... /Users/rubber/linux/kernel/events/core.c: 596
 * The basic idea is to use event->state to determine which (if any) time /Users/rubber/linux/kernel/events/core.c: 598
 * fields to increment with the current delta. This means we only need to /Users/rubber/linux/kernel/events/core.c: 599
 * update timestamps when we change state or when they are explicitly requested /Users/rubber/linux/kernel/events/core.c: 600
 * (read). /Users/rubber/linux/kernel/events/core.c: 601
 * Event groups make things a little more complicated, but not terribly so. The /Users/rubber/linux/kernel/events/core.c: 603
 * rules for a group are that if the group leader is OFF the entire group is /Users/rubber/linux/kernel/events/core.c: 604
 * OFF, irrespecive of what the group member states are. This results in /Users/rubber/linux/kernel/events/core.c: 605
 * __perf_effective_state(). /Users/rubber/linux/kernel/events/core.c: 606
 * A futher ramification is that when a group leader flips between OFF and /Users/rubber/linux/kernel/events/core.c: 608
 * !OFF, we need to update all group member times. /Users/rubber/linux/kernel/events/core.c: 609
 * NOTE: perf_event_time() is based on the (cgroup) context time, and thus we /Users/rubber/linux/kernel/events/core.c: 612
 * need to make sure the relevant context time is updated before we try and /Users/rubber/linux/kernel/events/core.c: 613
 * update our timestamps. /Users/rubber/linux/kernel/events/core.c: 614
	/* /Users/rubber/linux/kernel/events/core.c: 667
	 * If a group leader gets enabled/disabled all its siblings /Users/rubber/linux/kernel/events/core.c: 668
	 * are affected too. /Users/rubber/linux/kernel/events/core.c: 669
	/* /Users/rubber/linux/kernel/events/core.c: 693
	 * Cgroup scoping is recursive.  An event enabled for a cgroup is /Users/rubber/linux/kernel/events/core.c: 694
	 * also enabled for all its descendant cgroups.  If @cpuctx's /Users/rubber/linux/kernel/events/core.c: 695
	 * cgroup is a descendant of @event's (the test covers identity /Users/rubber/linux/kernel/events/core.c: 696
	 * case), it's a match. /Users/rubber/linux/kernel/events/core.c: 697
	/* /Users/rubber/linux/kernel/events/core.c: 752
	 * ensure we access cgroup data only when needed and /Users/rubber/linux/kernel/events/core.c: 753
	 * when we know the cgroup is pinned (css_get) /Users/rubber/linux/kernel/events/core.c: 754
	/* /Users/rubber/linux/kernel/events/core.c: 760
	 * Do not update time when cgroup is not active /Users/rubber/linux/kernel/events/core.c: 761
	/* /Users/rubber/linux/kernel/events/core.c: 775
	 * ctx->lock held by caller /Users/rubber/linux/kernel/events/core.c: 776
	 * ensure we do not access cgroup data /Users/rubber/linux/kernel/events/core.c: 777
	 * unless we have the cgroup pinned (css_get) /Users/rubber/linux/kernel/events/core.c: 778
 * reschedule events based on the cgroup constraint of task. /Users/rubber/linux/kernel/events/core.c: 798
 * mode SWOUT : schedule out everything /Users/rubber/linux/kernel/events/core.c: 800
 * mode SWIN : schedule in based on cgroup for next /Users/rubber/linux/kernel/events/core.c: 801
	/* /Users/rubber/linux/kernel/events/core.c: 809
	 * Disable interrupts and preemption to avoid this CPU's /Users/rubber/linux/kernel/events/core.c: 810
	 * cgrp_cpuctx_entry to change under us. /Users/rubber/linux/kernel/events/core.c: 811
			/* /Users/rubber/linux/kernel/events/core.c: 824
			 * must not be done before ctxswout due /Users/rubber/linux/kernel/events/core.c: 825
			 * to event_filter_match() in event_sched_out() /Users/rubber/linux/kernel/events/core.c: 826
			/* /Users/rubber/linux/kernel/events/core.c: 833
			 * set cgrp before ctxsw in to allow /Users/rubber/linux/kernel/events/core.c: 834
			 * event_filter_match() to not have to pass /Users/rubber/linux/kernel/events/core.c: 835
			 * task around /Users/rubber/linux/kernel/events/core.c: 836
			 * we pass the cpuctx->ctx to perf_cgroup_from_task() /Users/rubber/linux/kernel/events/core.c: 837
			 * because cgorup events are only per-cpu /Users/rubber/linux/kernel/events/core.c: 838
	/* /Users/rubber/linux/kernel/events/core.c: 858
	 * we come here when we know perf_cgroup_events > 0 /Users/rubber/linux/kernel/events/core.c: 859
	 * we do not need to pass the ctx here because we know /Users/rubber/linux/kernel/events/core.c: 860
	 * we are holding the rcu lock /Users/rubber/linux/kernel/events/core.c: 861
	/* /Users/rubber/linux/kernel/events/core.c: 866
	 * only schedule out current cgroup events if we know /Users/rubber/linux/kernel/events/core.c: 867
	 * that we are switching to a different cgroup. Otherwise, /Users/rubber/linux/kernel/events/core.c: 868
	 * do no touch the cgroup events. /Users/rubber/linux/kernel/events/core.c: 869
	/* /Users/rubber/linux/kernel/events/core.c: 884
	 * we come here when we know perf_cgroup_events > 0 /Users/rubber/linux/kernel/events/core.c: 885
	 * we do not need to pass the ctx here because we know /Users/rubber/linux/kernel/events/core.c: 886
	 * we are holding the rcu lock /Users/rubber/linux/kernel/events/core.c: 887
	/* /Users/rubber/linux/kernel/events/core.c: 892
	 * only need to schedule in cgroup events if we are changing /Users/rubber/linux/kernel/events/core.c: 893
	 * cgroup during ctxsw. Cgroup events were not scheduled /Users/rubber/linux/kernel/events/core.c: 894
	 * out of ctxsw out if that was not the case. /Users/rubber/linux/kernel/events/core.c: 895
	/* /Users/rubber/linux/kernel/events/core.c: 910
	 * Allow storage to have sufficent space for an iterator for each /Users/rubber/linux/kernel/events/core.c: 911
	 * possibly nested cgroup plus an iterator for events with no cgroup. /Users/rubber/linux/kernel/events/core.c: 912
	/* /Users/rubber/linux/kernel/events/core.c: 970
	 * all events in a group must monitor /Users/rubber/linux/kernel/events/core.c: 971
	 * the same cgroup because a task belongs /Users/rubber/linux/kernel/events/core.c: 972
	 * to only one perf cgroup at a time /Users/rubber/linux/kernel/events/core.c: 973
	/* /Users/rubber/linux/kernel/events/core.c: 1000
	 * Because cgroup events are always per-cpu events, /Users/rubber/linux/kernel/events/core.c: 1001
	 * @ctx == &cpuctx->ctx. /Users/rubber/linux/kernel/events/core.c: 1002
	/* /Users/rubber/linux/kernel/events/core.c: 1006
	 * Since setting cpuctx->cgrp is conditional on the current @cgrp /Users/rubber/linux/kernel/events/core.c: 1007
	 * matching the event's cgroup, we must do this for every new event, /Users/rubber/linux/kernel/events/core.c: 1008
	 * because if the first would mismatch, the second would not try again /Users/rubber/linux/kernel/events/core.c: 1009
	 * and we would leave cpuctx->cgrp unset. /Users/rubber/linux/kernel/events/core.c: 1010
	/* /Users/rubber/linux/kernel/events/core.c: 1034
	 * Because cgroup events are always per-cpu events, /Users/rubber/linux/kernel/events/core.c: 1035
	 * @ctx == &cpuctx->ctx. /Users/rubber/linux/kernel/events/core.c: 1036
 * set default to be dependent on timer tick just /Users/rubber/linux/kernel/events/core.c: 1123
 * like original code /Users/rubber/linux/kernel/events/core.c: 1124
 * function must be called with interrupts disabled /Users/rubber/linux/kernel/events/core.c: 1128
	/* /Users/rubber/linux/kernel/events/core.c: 1160
	 * check default is sane, if not set then force to /Users/rubber/linux/kernel/events/core.c: 1161
	 * default interval (1/tick) /Users/rubber/linux/kernel/events/core.c: 1162
 * perf_event_ctx_activate(), perf_event_ctx_deactivate(), and /Users/rubber/linux/kernel/events/core.c: 1213
 * perf_event_task_tick() are fully serialized because they're strictly cpu /Users/rubber/linux/kernel/events/core.c: 1214
 * affine and perf_event_ctx{activate,deactivate} are called with IRQs /Users/rubber/linux/kernel/events/core.c: 1215
 * disabled, while perf_event_task_tick is called from IRQ context. /Users/rubber/linux/kernel/events/core.c: 1216
 * Because of perf_event::ctx migration in sys_perf_event_open::move_group and /Users/rubber/linux/kernel/events/core.c: 1278
 * perf_pmu_migrate_context() we need some magic. /Users/rubber/linux/kernel/events/core.c: 1279
 * Those places that change perf_event::ctx will hold both /Users/rubber/linux/kernel/events/core.c: 1281
 * perf_event_ctx::mutex of the 'old' and 'new' ctx value. /Users/rubber/linux/kernel/events/core.c: 1282
 * Lock ordering is by mutex address. There are two other sites where /Users/rubber/linux/kernel/events/core.c: 1284
 * perf_event_context::mutex nests and those are: /Users/rubber/linux/kernel/events/core.c: 1285
 *  - perf_event_exit_task_context()	[ child , 0 ] /Users/rubber/linux/kernel/events/core.c: 1287
 *      perf_event_exit_event() /Users/rubber/linux/kernel/events/core.c: 1288
 *        put_event()			[ parent, 1 ] /Users/rubber/linux/kernel/events/core.c: 1289
 *  - perf_event_init_context()		[ parent, 0 ] /Users/rubber/linux/kernel/events/core.c: 1291
 *      inherit_task_group() /Users/rubber/linux/kernel/events/core.c: 1292
 *        inherit_group() /Users/rubber/linux/kernel/events/core.c: 1293
 *          inherit_event() /Users/rubber/linux/kernel/events/core.c: 1294
 *            perf_event_alloc() /Users/rubber/linux/kernel/events/core.c: 1295
 *              perf_init_event() /Users/rubber/linux/kernel/events/core.c: 1296
 *                perf_try_init_event()	[ child , 1 ] /Users/rubber/linux/kernel/events/core.c: 1297
 * While it appears there is an obvious deadlock here -- the parent and child /Users/rubber/linux/kernel/events/core.c: 1299
 * nesting levels are inverted between the two. This is in fact safe because /Users/rubber/linux/kernel/events/core.c: 1300
 * life-time rules separate them. That is an exiting task cannot fork, and a /Users/rubber/linux/kernel/events/core.c: 1301
 * spawning task cannot (yet) exit. /Users/rubber/linux/kernel/events/core.c: 1302
 * But remember that these are parent<->child context relations, and /Users/rubber/linux/kernel/events/core.c: 1304
 * migration does not affect children, therefore these two orderings should not /Users/rubber/linux/kernel/events/core.c: 1305
 * interact. /Users/rubber/linux/kernel/events/core.c: 1306
 * The change in perf_event::ctx does not affect children (as claimed above) /Users/rubber/linux/kernel/events/core.c: 1308
 * because the sys_perf_event_open() case will install a new event and break /Users/rubber/linux/kernel/events/core.c: 1309
 * the ctx parent<->child relation, and perf_pmu_migrate_context() is only /Users/rubber/linux/kernel/events/core.c: 1310
 * concerned with cpuctx and that doesn't have children. /Users/rubber/linux/kernel/events/core.c: 1311
 * The places that change perf_event::ctx will issue: /Users/rubber/linux/kernel/events/core.c: 1313
 *   perf_remove_from_context(); /Users/rubber/linux/kernel/events/core.c: 1315
 *   synchronize_rcu(); /Users/rubber/linux/kernel/events/core.c: 1316
 *   perf_install_in_context(); /Users/rubber/linux/kernel/events/core.c: 1317
 * to affect the change. The remove_from_context() + synchronize_rcu() should /Users/rubber/linux/kernel/events/core.c: 1319
 * quiesce the event, after which we can install it in the new location. This /Users/rubber/linux/kernel/events/core.c: 1320
 * means that only external vectors (perf_fops, prctl) can perturb the event /Users/rubber/linux/kernel/events/core.c: 1321
 * while in transit. Therefore all such accessors should also acquire /Users/rubber/linux/kernel/events/core.c: 1322
 * perf_event_context::mutex to serialize against this. /Users/rubber/linux/kernel/events/core.c: 1323
 * However; because event->ctx can change while we're waiting to acquire /Users/rubber/linux/kernel/events/core.c: 1325
 * ctx->mutex we must be careful and use the below perf_event_ctx_lock() /Users/rubber/linux/kernel/events/core.c: 1326
 * function. /Users/rubber/linux/kernel/events/core.c: 1327
 * Lock order: /Users/rubber/linux/kernel/events/core.c: 1329
 *    exec_update_lock /Users/rubber/linux/kernel/events/core.c: 1330
 *	task_struct::perf_event_mutex /Users/rubber/linux/kernel/events/core.c: 1331
 *	  perf_event_context::mutex /Users/rubber/linux/kernel/events/core.c: 1332
 *	    perf_event::child_mutex; /Users/rubber/linux/kernel/events/core.c: 1333
 *	      perf_event_context::lock /Users/rubber/linux/kernel/events/core.c: 1334
 *	    perf_event::mmap_mutex /Users/rubber/linux/kernel/events/core.c: 1335
 *	    mmap_lock /Users/rubber/linux/kernel/events/core.c: 1336
 *	      perf_addr_filters_head::lock /Users/rubber/linux/kernel/events/core.c: 1337
 *    cpu_hotplug_lock /Users/rubber/linux/kernel/events/core.c: 1339
 *      pmus_lock /Users/rubber/linux/kernel/events/core.c: 1340
 *	  cpuctx->mutex / perf_event_context::mutex /Users/rubber/linux/kernel/events/core.c: 1341
 * This must be done under the ctx->lock, such as to serialize against /Users/rubber/linux/kernel/events/core.c: 1381
 * context_equiv(), therefore we cannot call put_ctx() since that might end up /Users/rubber/linux/kernel/events/core.c: 1382
 * calling scheduler related locks and ctx->lock nests inside those. /Users/rubber/linux/kernel/events/core.c: 1383
	/* /Users/rubber/linux/kernel/events/core.c: 1403
	 * only top level events have the pid namespace they were created in /Users/rubber/linux/kernel/events/core.c: 1404
 * If we inherit events we want to return the parent event id /Users/rubber/linux/kernel/events/core.c: 1427
 * to userspace. /Users/rubber/linux/kernel/events/core.c: 1428
 * Get the perf_event_context for a task and lock it. /Users/rubber/linux/kernel/events/core.c: 1441
 * This has to cope with the fact that until it is locked, /Users/rubber/linux/kernel/events/core.c: 1443
 * the context could get moved to another task. /Users/rubber/linux/kernel/events/core.c: 1444
	/* /Users/rubber/linux/kernel/events/core.c: 1452
	 * One of the few rules of preemptible RCU is that one cannot do /Users/rubber/linux/kernel/events/core.c: 1453
	 * rcu_read_unlock() while holding a scheduler (or nested) lock when /Users/rubber/linux/kernel/events/core.c: 1454
	 * part of the read side critical section was irqs-enabled -- see /Users/rubber/linux/kernel/events/core.c: 1455
	 * rcu_read_unlock_special(). /Users/rubber/linux/kernel/events/core.c: 1456
	 * /Users/rubber/linux/kernel/events/core.c: 1457
	 * Since ctx->lock nests under rq->lock we must ensure the entire read /Users/rubber/linux/kernel/events/core.c: 1458
	 * side critical section has interrupts disabled. /Users/rubber/linux/kernel/events/core.c: 1459
		/* /Users/rubber/linux/kernel/events/core.c: 1465
		 * If this context is a clone of another, it might /Users/rubber/linux/kernel/events/core.c: 1466
		 * get swapped for another underneath us by /Users/rubber/linux/kernel/events/core.c: 1467
		 * perf_event_task_sched_out, though the /Users/rubber/linux/kernel/events/core.c: 1468
		 * rcu_read_lock() protects us from any context /Users/rubber/linux/kernel/events/core.c: 1469
		 * getting freed.  Lock the context and check if it /Users/rubber/linux/kernel/events/core.c: 1470
		 * got swapped before we could get the lock, and retry /Users/rubber/linux/kernel/events/core.c: 1471
		 * if so.  If we locked the right context, then it /Users/rubber/linux/kernel/events/core.c: 1472
		 * can't get swapped on us any more. /Users/rubber/linux/kernel/events/core.c: 1473
 * Get the context for a task and increment its pin_count so it /Users/rubber/linux/kernel/events/core.c: 1498
 * can't get swapped to another task.  This also increments its /Users/rubber/linux/kernel/events/core.c: 1499
 * reference count so that the context can't get freed. /Users/rubber/linux/kernel/events/core.c: 1500
 * Update the record of the current time in a context. /Users/rubber/linux/kernel/events/core.c: 1526
	/* /Users/rubber/linux/kernel/events/core.c: 1553
	 * It's 'group type', really, because if our group leader is /Users/rubber/linux/kernel/events/core.c: 1554
	 * pinned, so are we. /Users/rubber/linux/kernel/events/core.c: 1555
 * Helper function to initialize event group nodes. /Users/rubber/linux/kernel/events/core.c: 1568
 * Extract pinned or flexible groups from the context /Users/rubber/linux/kernel/events/core.c: 1577
 * based on event attrs bits. /Users/rubber/linux/kernel/events/core.c: 1578
 * Helper function to initializes perf_event_group trees. /Users/rubber/linux/kernel/events/core.c: 1590
 * Compare function for event groups; /Users/rubber/linux/kernel/events/core.c: 1611
 * Implements complex key that first sorts by CPU and then by virtual index /Users/rubber/linux/kernel/events/core.c: 1613
 * which provides ordering when rotating groups for the same CPU. /Users/rubber/linux/kernel/events/core.c: 1614
				/* /Users/rubber/linux/kernel/events/core.c: 1631
				 * Left has no cgroup but right does, no /Users/rubber/linux/kernel/events/core.c: 1632
				 * cgroups come first. /Users/rubber/linux/kernel/events/core.c: 1633
				/* /Users/rubber/linux/kernel/events/core.c: 1638
				 * Right has no cgroup but left does, no /Users/rubber/linux/kernel/events/core.c: 1639
				 * cgroups come first. /Users/rubber/linux/kernel/events/core.c: 1640
 * Insert @event into @groups' tree; using {@event->cpu, ++@groups->index} for /Users/rubber/linux/kernel/events/core.c: 1686
 * key (see perf_event_groups_less). This places it last inside the CPU /Users/rubber/linux/kernel/events/core.c: 1687
 * subtree. /Users/rubber/linux/kernel/events/core.c: 1688
 * Helper function to insert event into the pinned or flexible groups. /Users/rubber/linux/kernel/events/core.c: 1700
 * Delete a group from a tree. /Users/rubber/linux/kernel/events/core.c: 1712
 * Helper function to delete event from its groups. /Users/rubber/linux/kernel/events/core.c: 1726
 * Get the leftmost event in the cpu/cgroup subtree. /Users/rubber/linux/kernel/events/core.c: 1738
 * Like rb_entry_next_safe() for the @cpu subtree. /Users/rubber/linux/kernel/events/core.c: 1758
 * Iterate through the whole groups tree. /Users/rubber/linux/kernel/events/core.c: 1777
 * Add an event from the lists for its context. /Users/rubber/linux/kernel/events/core.c: 1786
 * Must be called with ctx->mutex and ctx->lock held. /Users/rubber/linux/kernel/events/core.c: 1787
	/* /Users/rubber/linux/kernel/events/core.c: 1799
	 * If we're a stand alone event or group leader, we go to the context /Users/rubber/linux/kernel/events/core.c: 1800
	 * list, group events are kept attached to the group so that /Users/rubber/linux/kernel/events/core.c: 1801
	 * perf_group_detach can, at all times, locate all siblings. /Users/rubber/linux/kernel/events/core.c: 1802
 * Initialize event state based on the perf_event_attr::disabled. /Users/rubber/linux/kernel/events/core.c: 1821
 * Called at perf_event creation and when events are attached/detached from a /Users/rubber/linux/kernel/events/core.c: 1895
 * group. /Users/rubber/linux/kernel/events/core.c: 1896
	/* /Users/rubber/linux/kernel/events/core.c: 1934
	 * The values computed here will be over-written when we actually /Users/rubber/linux/kernel/events/core.c: 1935
	 * attach the event. /Users/rubber/linux/kernel/events/core.c: 1936
	/* /Users/rubber/linux/kernel/events/core.c: 1942
	 * Sum the lot; should not exceed the 64k limit we have on records. /Users/rubber/linux/kernel/events/core.c: 1943
	 * Conservative limit to allow for callchains and other variable fields. /Users/rubber/linux/kernel/events/core.c: 1944
	/* /Users/rubber/linux/kernel/events/core.c: 1959
	 * We can have double attach due to group movement in perf_event_open. /Users/rubber/linux/kernel/events/core.c: 1960
 * Remove an event from the lists for its context. /Users/rubber/linux/kernel/events/core.c: 1984
 * Must be called with ctx->mutex and ctx->lock held. /Users/rubber/linux/kernel/events/core.c: 1985
	/* /Users/rubber/linux/kernel/events/core.c: 1993
	 * We can have double detach due to exit/hot-unplug + close. /Users/rubber/linux/kernel/events/core.c: 1994
	/* /Users/rubber/linux/kernel/events/core.c: 2010
	 * If event was in error state, then keep it /Users/rubber/linux/kernel/events/core.c: 2011
	 * that way, otherwise bogus counts will be /Users/rubber/linux/kernel/events/core.c: 2012
	 * returned on read(). The only way to get out /Users/rubber/linux/kernel/events/core.c: 2013
	 * of error state is by explicit re-enabling /Users/rubber/linux/kernel/events/core.c: 2014
	 * of the event /Users/rubber/linux/kernel/events/core.c: 2015
	/* /Users/rubber/linux/kernel/events/core.c: 2048
	 * If event uses aux_event tear down the link /Users/rubber/linux/kernel/events/core.c: 2049
	/* /Users/rubber/linux/kernel/events/core.c: 2058
	 * If the event is an aux_event, tear down all links to /Users/rubber/linux/kernel/events/core.c: 2059
	 * it from other events. /Users/rubber/linux/kernel/events/core.c: 2060
		/* /Users/rubber/linux/kernel/events/core.c: 2069
		 * If it's ACTIVE, schedule it out and put it into ERROR /Users/rubber/linux/kernel/events/core.c: 2070
		 * state so that we don't try to schedule it again. Note /Users/rubber/linux/kernel/events/core.c: 2071
		 * that perf_event_enable() will clear the ERROR status. /Users/rubber/linux/kernel/events/core.c: 2072
	/* /Users/rubber/linux/kernel/events/core.c: 2087
	 * Our group leader must be an aux event if we want to be /Users/rubber/linux/kernel/events/core.c: 2088
	 * an aux_output. This way, the aux event will precede its /Users/rubber/linux/kernel/events/core.c: 2089
	 * aux_output events in the group, and therefore will always /Users/rubber/linux/kernel/events/core.c: 2090
	 * schedule first. /Users/rubber/linux/kernel/events/core.c: 2091
	/* /Users/rubber/linux/kernel/events/core.c: 2096
	 * aux_output and aux_sample_size are mutually exclusive. /Users/rubber/linux/kernel/events/core.c: 2097
	/* /Users/rubber/linux/kernel/events/core.c: 2112
	 * Link aux_outputs to their aux event; this is undone in /Users/rubber/linux/kernel/events/core.c: 2113
	 * perf_group_detach() by perf_put_aux_event(). When the /Users/rubber/linux/kernel/events/core.c: 2114
	 * group in torn down, the aux_output events loose their /Users/rubber/linux/kernel/events/core.c: 2115
	 * link to the aux_event and can't schedule any more. /Users/rubber/linux/kernel/events/core.c: 2116
 * Events that have PERF_EV_CAP_SIBLING require being part of a group and /Users/rubber/linux/kernel/events/core.c: 2130
 * cannot exist on their own, schedule them out and move them into the ERROR /Users/rubber/linux/kernel/events/core.c: 2131
 * state. Also see _perf_event_enable(), it will not be able to recover /Users/rubber/linux/kernel/events/core.c: 2132
 * this ERROR state. /Users/rubber/linux/kernel/events/core.c: 2133
	/* /Users/rubber/linux/kernel/events/core.c: 2152
	 * We can have double detach due to exit/hot-unplug + close. /Users/rubber/linux/kernel/events/core.c: 2153
	/* /Users/rubber/linux/kernel/events/core.c: 2162
	 * If this is a sibling, remove it from its group. /Users/rubber/linux/kernel/events/core.c: 2163
	/* /Users/rubber/linux/kernel/events/core.c: 2171
	 * If this was a group event with sibling events then /Users/rubber/linux/kernel/events/core.c: 2172
	 * upgrade the siblings to singleton events by adding them /Users/rubber/linux/kernel/events/core.c: 2173
	 * to whatever list we are on. /Users/rubber/linux/kernel/events/core.c: 2174
 * Check whether we should attempt to schedule an event group based on /Users/rubber/linux/kernel/events/core.c: 2236
 * PMU-specific filtering. An event group can consist of HW and SW events, /Users/rubber/linux/kernel/events/core.c: 2237
 * potentially with a SW leader, so we must check all the filters, to /Users/rubber/linux/kernel/events/core.c: 2238
 * determine whether a group is schedulable: /Users/rubber/linux/kernel/events/core.c: 2239
	/* /Users/rubber/linux/kernel/events/core.c: 2276
	 * Asymmetry; we only schedule events _IN_ through ctx_sched_in(), but /Users/rubber/linux/kernel/events/core.c: 2277
	 * we can schedule events _OUT_ individually through things like /Users/rubber/linux/kernel/events/core.c: 2278
	 * __perf_remove_from_context(). /Users/rubber/linux/kernel/events/core.c: 2279
	/* /Users/rubber/linux/kernel/events/core.c: 2321
	 * Schedule out siblings (if any): /Users/rubber/linux/kernel/events/core.c: 2322
 * Cross CPU call to remove a performance event /Users/rubber/linux/kernel/events/core.c: 2334
 * We disable the event on the hardware level first. After that we /Users/rubber/linux/kernel/events/core.c: 2336
 * remove it from the context list. /Users/rubber/linux/kernel/events/core.c: 2337
 * Remove the event from a task's (or a CPU's) list of events. /Users/rubber/linux/kernel/events/core.c: 2370
 * If event->ctx is a cloned context, callers must make sure that /Users/rubber/linux/kernel/events/core.c: 2372
 * every task struct that event->ctx->task could possibly point to /Users/rubber/linux/kernel/events/core.c: 2373
 * remains valid.  This is OK when called from perf_release since /Users/rubber/linux/kernel/events/core.c: 2374
 * that only calls us on the top-level context, which can't be a clone. /Users/rubber/linux/kernel/events/core.c: 2375
 * When called from perf_event_exit_task, it's OK because the /Users/rubber/linux/kernel/events/core.c: 2376
 * context has been detached from its task. /Users/rubber/linux/kernel/events/core.c: 2377
	/* /Users/rubber/linux/kernel/events/core.c: 2385
	 * Because of perf_event_exit_task(), perf_remove_from_context() ought /Users/rubber/linux/kernel/events/core.c: 2386
	 * to work in the face of TASK_TOMBSTONE, unlike every other /Users/rubber/linux/kernel/events/core.c: 2387
	 * event_function_call() user. /Users/rubber/linux/kernel/events/core.c: 2388
 * Cross CPU call to disable a performance event /Users/rubber/linux/kernel/events/core.c: 2403
 * Disable an event. /Users/rubber/linux/kernel/events/core.c: 2428
 * If event->ctx is a cloned context, callers must make sure that /Users/rubber/linux/kernel/events/core.c: 2430
 * every task struct that event->ctx->task could possibly point to /Users/rubber/linux/kernel/events/core.c: 2431
 * remains valid.  This condition is satisfied when called through /Users/rubber/linux/kernel/events/core.c: 2432
 * perf_event_for_each_child or perf_event_for_each because they /Users/rubber/linux/kernel/events/core.c: 2433
 * hold the top-level event's child_mutex, so any descendant that /Users/rubber/linux/kernel/events/core.c: 2434
 * goes to exit will block in perf_event_exit_event(). /Users/rubber/linux/kernel/events/core.c: 2435
 * When called from perf_pending_event it's OK because event->ctx /Users/rubber/linux/kernel/events/core.c: 2437
 * is the current context on this CPU and preemption is disabled, /Users/rubber/linux/kernel/events/core.c: 2438
 * hence we can't get into perf_event_task_sched_out for this context. /Users/rubber/linux/kernel/events/core.c: 2439
 * Strictly speaking kernel users cannot create groups and therefore this /Users/rubber/linux/kernel/events/core.c: 2461
 * interface does not need the perf_event_ctx_lock() magic. /Users/rubber/linux/kernel/events/core.c: 2462
	/* /Users/rubber/linux/kernel/events/core.c: 2484
	 * use the correct time source for the time snapshot /Users/rubber/linux/kernel/events/core.c: 2485
	 * /Users/rubber/linux/kernel/events/core.c: 2486
	 * We could get by without this by leveraging the /Users/rubber/linux/kernel/events/core.c: 2487
	 * fact that to get to this function, the caller /Users/rubber/linux/kernel/events/core.c: 2488
	 * has most likely already called update_context_time() /Users/rubber/linux/kernel/events/core.c: 2489
	 * and update_cgrp_time_xx() and thus both timestamp /Users/rubber/linux/kernel/events/core.c: 2490
	 * are identical (or very close). Given that tstamp is, /Users/rubber/linux/kernel/events/core.c: 2491
	 * already adjusted for cgroup, we could say that: /Users/rubber/linux/kernel/events/core.c: 2492
	 *    tstamp - ctx->timestamp /Users/rubber/linux/kernel/events/core.c: 2493
	 * is equivalent to /Users/rubber/linux/kernel/events/core.c: 2494
	 *    tstamp - cgrp->timestamp. /Users/rubber/linux/kernel/events/core.c: 2495
	 * /Users/rubber/linux/kernel/events/core.c: 2496
	 * Then, in perf_output_read(), the calculation would /Users/rubber/linux/kernel/events/core.c: 2497
	 * work with no changes because: /Users/rubber/linux/kernel/events/core.c: 2498
	 * - event is guaranteed scheduled in /Users/rubber/linux/kernel/events/core.c: 2499
	 * - no scheduled out in between /Users/rubber/linux/kernel/events/core.c: 2500
	 * - thus the timestamp would be the same /Users/rubber/linux/kernel/events/core.c: 2501
	 * /Users/rubber/linux/kernel/events/core.c: 2502
	 * But this is a bit hairy. /Users/rubber/linux/kernel/events/core.c: 2503
	 * /Users/rubber/linux/kernel/events/core.c: 2504
	 * So instead, we have an explicit cgroup call to remain /Users/rubber/linux/kernel/events/core.c: 2505
	 * within the time source all along. We believe it /Users/rubber/linux/kernel/events/core.c: 2506
	 * is cleaner and simpler to understand. /Users/rubber/linux/kernel/events/core.c: 2507
	/* /Users/rubber/linux/kernel/events/core.c: 2535
	 * Order event::oncpu write to happen before the ACTIVE state is /Users/rubber/linux/kernel/events/core.c: 2536
	 * visible. This allows perf_event_{stop,read}() to observe the correct /Users/rubber/linux/kernel/events/core.c: 2537
	 * ->oncpu if it sees ACTIVE. /Users/rubber/linux/kernel/events/core.c: 2538
	/* /Users/rubber/linux/kernel/events/core.c: 2543
	 * Unthrottle events, since we scheduled we might have missed several /Users/rubber/linux/kernel/events/core.c: 2544
	 * ticks already, also for a heavily scheduling task there is little /Users/rubber/linux/kernel/events/core.c: 2545
	 * guarantee it'll get a tick in a timely manner. /Users/rubber/linux/kernel/events/core.c: 2546
	/* /Users/rubber/linux/kernel/events/core.c: 2598
	 * Schedule in siblings as one group (if any): /Users/rubber/linux/kernel/events/core.c: 2599
	/* /Users/rubber/linux/kernel/events/core.c: 2612
	 * Groups can be scheduled in as one unit only, so undo any /Users/rubber/linux/kernel/events/core.c: 2613
	 * partial group before returning: /Users/rubber/linux/kernel/events/core.c: 2614
	 * The events up to the failed event are scheduled out normally. /Users/rubber/linux/kernel/events/core.c: 2615
 * Work out whether we can put this event group on the CPU now. /Users/rubber/linux/kernel/events/core.c: 2631
	/* /Users/rubber/linux/kernel/events/core.c: 2637
	 * Groups consisting entirely of software events can always go on. /Users/rubber/linux/kernel/events/core.c: 2638
	/* /Users/rubber/linux/kernel/events/core.c: 2642
	 * If an exclusive group is already on, no other hardware /Users/rubber/linux/kernel/events/core.c: 2643
	 * events can go on. /Users/rubber/linux/kernel/events/core.c: 2644
	/* /Users/rubber/linux/kernel/events/core.c: 2648
	 * If this group is exclusive and there are already /Users/rubber/linux/kernel/events/core.c: 2649
	 * events on the CPU, it can't go on. /Users/rubber/linux/kernel/events/core.c: 2650
	/* /Users/rubber/linux/kernel/events/core.c: 2654
	 * Otherwise, try to add it if all previous groups were able /Users/rubber/linux/kernel/events/core.c: 2655
	 * to go on. /Users/rubber/linux/kernel/events/core.c: 2656
 * We want to maintain the following priority of scheduling: /Users/rubber/linux/kernel/events/core.c: 2703
 *  - CPU pinned (EVENT_CPU | EVENT_PINNED) /Users/rubber/linux/kernel/events/core.c: 2704
 *  - task pinned (EVENT_PINNED) /Users/rubber/linux/kernel/events/core.c: 2705
 *  - CPU flexible (EVENT_CPU | EVENT_FLEXIBLE) /Users/rubber/linux/kernel/events/core.c: 2706
 *  - task flexible (EVENT_FLEXIBLE). /Users/rubber/linux/kernel/events/core.c: 2707
 * In order to avoid unscheduling and scheduling back in everything every /Users/rubber/linux/kernel/events/core.c: 2709
 * time an event is added, only do it for the groups of equal priority and /Users/rubber/linux/kernel/events/core.c: 2710
 * below. /Users/rubber/linux/kernel/events/core.c: 2711
 * This can be called after a batch operation on task events, in which case /Users/rubber/linux/kernel/events/core.c: 2713
 * event_type is a bit mask of the types of events involved. For CPU events, /Users/rubber/linux/kernel/events/core.c: 2714
 * event_type is only either EVENT_PINNED or EVENT_FLEXIBLE. /Users/rubber/linux/kernel/events/core.c: 2715
	/* /Users/rubber/linux/kernel/events/core.c: 2724
	 * If pinned groups are involved, flexible groups also need to be /Users/rubber/linux/kernel/events/core.c: 2725
	 * scheduled out. /Users/rubber/linux/kernel/events/core.c: 2726
	/* /Users/rubber/linux/kernel/events/core.c: 2737
	 * Decide which cpu ctx groups to schedule out based on the types /Users/rubber/linux/kernel/events/core.c: 2738
	 * of events that caused rescheduling: /Users/rubber/linux/kernel/events/core.c: 2739
	 *  - EVENT_CPU: schedule out corresponding groups; /Users/rubber/linux/kernel/events/core.c: 2740
	 *  - EVENT_PINNED task events: schedule out EVENT_FLEXIBLE groups; /Users/rubber/linux/kernel/events/core.c: 2741
	 *  - otherwise, do nothing more. /Users/rubber/linux/kernel/events/core.c: 2742
 * Cross CPU call to install and enable a performance event /Users/rubber/linux/kernel/events/core.c: 2764
 * Very similar to remote_function() + event_function() but cannot assume that /Users/rubber/linux/kernel/events/core.c: 2766
 * things like ctx->is_active and cpuctx->task_ctx are set. /Users/rubber/linux/kernel/events/core.c: 2767
		/* /Users/rubber/linux/kernel/events/core.c: 2785
		 * If the task is running, it must be running on this CPU, /Users/rubber/linux/kernel/events/core.c: 2786
		 * otherwise we cannot reprogram things. /Users/rubber/linux/kernel/events/core.c: 2787
		 * /Users/rubber/linux/kernel/events/core.c: 2788
		 * If its not running, we don't care, ctx->lock will /Users/rubber/linux/kernel/events/core.c: 2789
		 * serialize against it becoming runnable. /Users/rubber/linux/kernel/events/core.c: 2790
		/* /Users/rubber/linux/kernel/events/core.c: 2804
		 * If the current cgroup doesn't match the event's /Users/rubber/linux/kernel/events/core.c: 2805
		 * cgroup, we should not try to schedule it. /Users/rubber/linux/kernel/events/core.c: 2806
 * Attach a performance event to a context. /Users/rubber/linux/kernel/events/core.c: 2832
 * Very similar to event_function_call, see comment there. /Users/rubber/linux/kernel/events/core.c: 2834
	/* /Users/rubber/linux/kernel/events/core.c: 2850
	 * Ensures that if we can observe event->ctx, both the event and ctx /Users/rubber/linux/kernel/events/core.c: 2851
	 * will be 'complete'. See perf_iterate_sb_cpu(). /Users/rubber/linux/kernel/events/core.c: 2852
	/* /Users/rubber/linux/kernel/events/core.c: 2856
	 * perf_event_attr::disabled events will not run and can be initialized /Users/rubber/linux/kernel/events/core.c: 2857
	 * without IPI. Except when this is the first event for the context, in /Users/rubber/linux/kernel/events/core.c: 2858
	 * that case we need the magic of the IPI to set ctx->is_active. /Users/rubber/linux/kernel/events/core.c: 2859
	 * /Users/rubber/linux/kernel/events/core.c: 2860
	 * The IOC_ENABLE that is sure to follow the creation of a disabled /Users/rubber/linux/kernel/events/core.c: 2861
	 * event will issue the IPI and reprogram the hardware. /Users/rubber/linux/kernel/events/core.c: 2862
	/* /Users/rubber/linux/kernel/events/core.c: 2880
	 * Should not happen, we validate the ctx is still alive before calling. /Users/rubber/linux/kernel/events/core.c: 2881
	/* /Users/rubber/linux/kernel/events/core.c: 2886
	 * Installing events is tricky because we cannot rely on ctx->is_active /Users/rubber/linux/kernel/events/core.c: 2887
	 * to be set in case this is the nr_events 0 -> 1 transition. /Users/rubber/linux/kernel/events/core.c: 2888
	 * /Users/rubber/linux/kernel/events/core.c: 2889
	 * Instead we use task_curr(), which tells us if the task is running. /Users/rubber/linux/kernel/events/core.c: 2890
	 * However, since we use task_curr() outside of rq::lock, we can race /Users/rubber/linux/kernel/events/core.c: 2891
	 * against the actual state. This means the result can be wrong. /Users/rubber/linux/kernel/events/core.c: 2892
	 * /Users/rubber/linux/kernel/events/core.c: 2893
	 * If we get a false positive, we retry, this is harmless. /Users/rubber/linux/kernel/events/core.c: 2894
	 * /Users/rubber/linux/kernel/events/core.c: 2895
	 * If we get a false negative, things are complicated. If we are after /Users/rubber/linux/kernel/events/core.c: 2896
	 * perf_event_context_sched_in() ctx::lock will serialize us, and the /Users/rubber/linux/kernel/events/core.c: 2897
	 * value must be correct. If we're before, it doesn't matter since /Users/rubber/linux/kernel/events/core.c: 2898
	 * perf_event_context_sched_in() will program the counter. /Users/rubber/linux/kernel/events/core.c: 2899
	 * /Users/rubber/linux/kernel/events/core.c: 2900
	 * However, this hinges on the remote context switch having observed /Users/rubber/linux/kernel/events/core.c: 2901
	 * our task->perf_event_ctxp[] store, such that it will in fact take /Users/rubber/linux/kernel/events/core.c: 2902
	 * ctx::lock in perf_event_context_sched_in(). /Users/rubber/linux/kernel/events/core.c: 2903
	 * /Users/rubber/linux/kernel/events/core.c: 2904
	 * We do this by task_function_call(), if the IPI fails to hit the task /Users/rubber/linux/kernel/events/core.c: 2905
	 * we know any future context switch of task must see the /Users/rubber/linux/kernel/events/core.c: 2906
	 * perf_event_ctpx[] store. /Users/rubber/linux/kernel/events/core.c: 2907
	/* /Users/rubber/linux/kernel/events/core.c: 2910
	 * This smp_mb() orders the task->perf_event_ctxp[] store with the /Users/rubber/linux/kernel/events/core.c: 2911
	 * task_cpu() load, such that if the IPI then does not find the task /Users/rubber/linux/kernel/events/core.c: 2912
	 * running, a future context switch of that task must observe the /Users/rubber/linux/kernel/events/core.c: 2913
	 * store. /Users/rubber/linux/kernel/events/core.c: 2914
		/* /Users/rubber/linux/kernel/events/core.c: 2924
		 * Cannot happen because we already checked above (which also /Users/rubber/linux/kernel/events/core.c: 2925
		 * cannot happen), and we hold ctx->mutex, which serializes us /Users/rubber/linux/kernel/events/core.c: 2926
		 * against perf_event_exit_task_context(). /Users/rubber/linux/kernel/events/core.c: 2927
	/* /Users/rubber/linux/kernel/events/core.c: 2932
	 * If the task is not running, ctx->lock will avoid it becoming so, /Users/rubber/linux/kernel/events/core.c: 2933
	 * thus we can safely install the event. /Users/rubber/linux/kernel/events/core.c: 2934
 * Cross CPU call to enable a performance event /Users/rubber/linux/kernel/events/core.c: 2945
	/* /Users/rubber/linux/kernel/events/core.c: 2973
	 * If the event is in a group and isn't the group leader, /Users/rubber/linux/kernel/events/core.c: 2974
	 * then don't put it on unless the group is on. /Users/rubber/linux/kernel/events/core.c: 2975
 * Enable an event. /Users/rubber/linux/kernel/events/core.c: 2990
 * If event->ctx is a cloned context, callers must make sure that /Users/rubber/linux/kernel/events/core.c: 2992
 * every task struct that event->ctx->task could possibly point to /Users/rubber/linux/kernel/events/core.c: 2993
 * remains valid.  This condition is satisfied when called through /Users/rubber/linux/kernel/events/core.c: 2994
 * perf_event_for_each_child or perf_event_for_each as described /Users/rubber/linux/kernel/events/core.c: 2995
 * for perf_event_disable. /Users/rubber/linux/kernel/events/core.c: 2996
	/* /Users/rubber/linux/kernel/events/core.c: 3010
	 * If the event is in error state, clear that first. /Users/rubber/linux/kernel/events/core.c: 3011
	 * /Users/rubber/linux/kernel/events/core.c: 3012
	 * That way, if we see the event in error state below, we know that it /Users/rubber/linux/kernel/events/core.c: 3013
	 * has gone back into error state, as distinct from the task having /Users/rubber/linux/kernel/events/core.c: 3014
	 * been scheduled away before the cross-call arrived. /Users/rubber/linux/kernel/events/core.c: 3015
		/* /Users/rubber/linux/kernel/events/core.c: 3018
		 * Detached SIBLING events cannot leave ERROR state. /Users/rubber/linux/kernel/events/core.c: 3019
 * See perf_event_disable(); /Users/rubber/linux/kernel/events/core.c: 3033
	/* /Users/rubber/linux/kernel/events/core.c: 3062
	 * There is a window with interrupts enabled before we get here, /Users/rubber/linux/kernel/events/core.c: 3063
	 * so we need to check again lest we try to stop another CPU's event. /Users/rubber/linux/kernel/events/core.c: 3064
	/* /Users/rubber/linux/kernel/events/core.c: 3071
	 * May race with the actual stop (through perf_pmu_output_stop()), /Users/rubber/linux/kernel/events/core.c: 3072
	 * but it is only used for events with AUX ring buffer, and such /Users/rubber/linux/kernel/events/core.c: 3073
	 * events will refuse to restart because of rb::aux_mmap_count==0, /Users/rubber/linux/kernel/events/core.c: 3074
	 * see comments in perf_aux_output_begin(). /Users/rubber/linux/kernel/events/core.c: 3075
	 * /Users/rubber/linux/kernel/events/core.c: 3076
	 * Since this is happening on an event-local CPU, no trace is lost /Users/rubber/linux/kernel/events/core.c: 3077
	 * while restarting. /Users/rubber/linux/kernel/events/core.c: 3078
		/* /Users/rubber/linux/kernel/events/core.c: 3101
		 * We only want to restart ACTIVE events, so if the event goes /Users/rubber/linux/kernel/events/core.c: 3102
		 * inactive here (event->oncpu==-1), there's nothing more to do; /Users/rubber/linux/kernel/events/core.c: 3103
		 * fall through with ret==-ENXIO. /Users/rubber/linux/kernel/events/core.c: 3104
 * In order to contain the amount of racy and tricky in the address filter /Users/rubber/linux/kernel/events/core.c: 3114
 * configuration management, it is a two part process: /Users/rubber/linux/kernel/events/core.c: 3115
 * (p1) when userspace mappings change as a result of (1) or (2) or (3) below, /Users/rubber/linux/kernel/events/core.c: 3117
 *      we update the addresses of corresponding vmas in /Users/rubber/linux/kernel/events/core.c: 3118
 *	event::addr_filter_ranges array and bump the event::addr_filters_gen; /Users/rubber/linux/kernel/events/core.c: 3119
 * (p2) when an event is scheduled in (pmu::add), it calls /Users/rubber/linux/kernel/events/core.c: 3120
 *      perf_event_addr_filters_sync() which calls pmu::addr_filters_sync() /Users/rubber/linux/kernel/events/core.c: 3121
 *      if the generation has changed since the previous call. /Users/rubber/linux/kernel/events/core.c: 3122
 * If (p1) happens while the event is active, we restart it to force (p2). /Users/rubber/linux/kernel/events/core.c: 3124
 * (1) perf_addr_filters_apply(): adjusting filters' offsets based on /Users/rubber/linux/kernel/events/core.c: 3126
 *     pre-existing mappings, called once when new filters arrive via SET_FILTER /Users/rubber/linux/kernel/events/core.c: 3127
 *     ioctl; /Users/rubber/linux/kernel/events/core.c: 3128
 * (2) perf_addr_filters_adjust(): adjusting filters' offsets based on newly /Users/rubber/linux/kernel/events/core.c: 3129
 *     registered mapping, called for every new mmap(), with mm::mmap_lock down /Users/rubber/linux/kernel/events/core.c: 3130
 *     for reading; /Users/rubber/linux/kernel/events/core.c: 3131
 * (3) perf_event_addr_filters_exec(): clearing filters' offsets in the process /Users/rubber/linux/kernel/events/core.c: 3132
 *     of exec. /Users/rubber/linux/kernel/events/core.c: 3133
	/* /Users/rubber/linux/kernel/events/core.c: 3153
	 * not supported on inherited events /Users/rubber/linux/kernel/events/core.c: 3154
 * See perf_event_disable() /Users/rubber/linux/kernel/events/core.c: 3166
		/* /Users/rubber/linux/kernel/events/core.c: 3241
		 * See __perf_remove_from_context(). /Users/rubber/linux/kernel/events/core.c: 3242
	/* /Users/rubber/linux/kernel/events/core.c: 3260
	 * Always update time if it was set; not only when it changes. /Users/rubber/linux/kernel/events/core.c: 3261
	 * Otherwise we can 'forget' to update time for any but the last /Users/rubber/linux/kernel/events/core.c: 3262
	 * context we sched out. For example: /Users/rubber/linux/kernel/events/core.c: 3263
	 * /Users/rubber/linux/kernel/events/core.c: 3264
	 *   ctx_sched_out(.event_type = EVENT_FLEXIBLE) /Users/rubber/linux/kernel/events/core.c: 3265
	 *   ctx_sched_out(.event_type = EVENT_PINNED) /Users/rubber/linux/kernel/events/core.c: 3266
	 * /Users/rubber/linux/kernel/events/core.c: 3267
	 * would only update time for the pinned events. /Users/rubber/linux/kernel/events/core.c: 3268
		/* /Users/rubber/linux/kernel/events/core.c: 3291
		 * Since we cleared EVENT_FLEXIBLE, also clear /Users/rubber/linux/kernel/events/core.c: 3292
		 * rotate_necessary, is will be reset by /Users/rubber/linux/kernel/events/core.c: 3293
		 * ctx_flexible_sched_in() when needed. /Users/rubber/linux/kernel/events/core.c: 3294
 * Test whether two contexts are equivalent, i.e. whether they have both been /Users/rubber/linux/kernel/events/core.c: 3302
 * cloned from the same version of the same context. /Users/rubber/linux/kernel/events/core.c: 3303
 * Equivalence is measured using a generation number in the context that is /Users/rubber/linux/kernel/events/core.c: 3305
 * incremented on each modification to it; see unclone_ctx(), list_add_event() /Users/rubber/linux/kernel/events/core.c: 3306
 * and list_del_event(). /Users/rubber/linux/kernel/events/core.c: 3307
	/* /Users/rubber/linux/kernel/events/core.c: 3327
	 * If ctx1 and ctx2 have the same parent; we flatten the parent /Users/rubber/linux/kernel/events/core.c: 3328
	 * hierarchy, see perf_event_init_context(). /Users/rubber/linux/kernel/events/core.c: 3329
	/* /Users/rubber/linux/kernel/events/core.c: 3347
	 * Update the event value, we cannot use perf_event_read() /Users/rubber/linux/kernel/events/core.c: 3348
	 * because we're in the middle of a context switch and have IRQs /Users/rubber/linux/kernel/events/core.c: 3349
	 * disabled, which upsets smp_call_function_single(), however /Users/rubber/linux/kernel/events/core.c: 3350
	 * we know the event must be on the current CPU, therefore we /Users/rubber/linux/kernel/events/core.c: 3351
	 * don't need to use it. /Users/rubber/linux/kernel/events/core.c: 3352
	/* /Users/rubber/linux/kernel/events/core.c: 3359
	 * In order to keep per-task stats reliable we need to flip the event /Users/rubber/linux/kernel/events/core.c: 3360
	 * values when we flip the contexts. /Users/rubber/linux/kernel/events/core.c: 3361
	/* /Users/rubber/linux/kernel/events/core.c: 3370
	 * Since we swizzled the values, update the user visible data too. /Users/rubber/linux/kernel/events/core.c: 3371
		/* /Users/rubber/linux/kernel/events/core.c: 3434
		 * Looks like the two contexts are clones, so we might be /Users/rubber/linux/kernel/events/core.c: 3435
		 * able to optimize the context switch.  We lock both /Users/rubber/linux/kernel/events/core.c: 3436
		 * contexts and check that they are clones under the /Users/rubber/linux/kernel/events/core.c: 3437
		 * lock (including re-checking that neither has been /Users/rubber/linux/kernel/events/core.c: 3438
		 * uncloned in the meantime).  It doesn't matter which /Users/rubber/linux/kernel/events/core.c: 3439
		 * order we take the locks because no other cpu could /Users/rubber/linux/kernel/events/core.c: 3440
		 * be trying to lock both of these tasks. /Users/rubber/linux/kernel/events/core.c: 3441
			/* /Users/rubber/linux/kernel/events/core.c: 3455
			 * PMU specific parts of task perf context can require /Users/rubber/linux/kernel/events/core.c: 3456
			 * additional synchronization. As an example of such /Users/rubber/linux/kernel/events/core.c: 3457
			 * synchronization see implementation details of Intel /Users/rubber/linux/kernel/events/core.c: 3458
			 * LBR call stack data profiling; /Users/rubber/linux/kernel/events/core.c: 3459
			/* /Users/rubber/linux/kernel/events/core.c: 3468
			 * RCU_INIT_POINTER here is safe because we've not /Users/rubber/linux/kernel/events/core.c: 3469
			 * modified the ctx and the above modification of /Users/rubber/linux/kernel/events/core.c: 3470
			 * ctx->task and ctx->task_ctx_data are immaterial /Users/rubber/linux/kernel/events/core.c: 3471
			 * since those values are always verified under /Users/rubber/linux/kernel/events/core.c: 3472
			 * ctx->lock which we're now holding. /Users/rubber/linux/kernel/events/core.c: 3473
 * This function provides the context switch callback to the lower code /Users/rubber/linux/kernel/events/core.c: 3525
 * layer. It is invoked ONLY when the context switch callback is enabled. /Users/rubber/linux/kernel/events/core.c: 3526
 * This callback is relevant even to per-cpu events; for example multi event /Users/rubber/linux/kernel/events/core.c: 3528
 * PEBS requires this to provide PID/TID information. This requires we flush /Users/rubber/linux/kernel/events/core.c: 3529
 * all queued PEBS records before we context switch to a new task. /Users/rubber/linux/kernel/events/core.c: 3530
 * Called from scheduler to remove the events of the current task, /Users/rubber/linux/kernel/events/core.c: 3575
 * with interrupts disabled. /Users/rubber/linux/kernel/events/core.c: 3576
 * We stop each event and update the event value in event->count. /Users/rubber/linux/kernel/events/core.c: 3578
 * This does not protect us against NMI, but disable() /Users/rubber/linux/kernel/events/core.c: 3580
 * sets the disabled bit in the control field of event _before_ /Users/rubber/linux/kernel/events/core.c: 3581
 * accessing the event control register. If a NMI hits, then it will /Users/rubber/linux/kernel/events/core.c: 3582
 * not restart the event. /Users/rubber/linux/kernel/events/core.c: 3583
	/* /Users/rubber/linux/kernel/events/core.c: 3599
	 * if cgroup events exist on this CPU, then we need /Users/rubber/linux/kernel/events/core.c: 3600
	 * to check if we have to switch out PMU state. /Users/rubber/linux/kernel/events/core.c: 3601
	 * cgroup event are system-wide mode only /Users/rubber/linux/kernel/events/core.c: 3602
 * Called with IRQs disabled /Users/rubber/linux/kernel/events/core.c: 3609
	/* /Users/rubber/linux/kernel/events/core.c: 3824
	 * First go through the list and put on any pinned groups /Users/rubber/linux/kernel/events/core.c: 3825
	 * in order to give them the best chance of going on. /Users/rubber/linux/kernel/events/core.c: 3826
	/* /Users/rubber/linux/kernel/events/core.c: 3853
	 * HACK: for HETEROGENEOUS the task context might have switched to a /Users/rubber/linux/kernel/events/core.c: 3854
	 * different PMU, force (re)set the context, /Users/rubber/linux/kernel/events/core.c: 3855
	/* /Users/rubber/linux/kernel/events/core.c: 3866
	 * We must check ctx->nr_events while holding ctx->lock, such /Users/rubber/linux/kernel/events/core.c: 3867
	 * that we serialize against perf_install_in_context(). /Users/rubber/linux/kernel/events/core.c: 3868
	/* /Users/rubber/linux/kernel/events/core.c: 3874
	 * We want to keep the following priority order: /Users/rubber/linux/kernel/events/core.c: 3875
	 * cpu pinned (that don't need to move), task pinned, /Users/rubber/linux/kernel/events/core.c: 3876
	 * cpu flexible, task flexible. /Users/rubber/linux/kernel/events/core.c: 3877
	 * /Users/rubber/linux/kernel/events/core.c: 3878
	 * However, if task's ctx is not carrying any pinned /Users/rubber/linux/kernel/events/core.c: 3879
	 * events, no need to flip the cpuctx's events around. /Users/rubber/linux/kernel/events/core.c: 3880
 * Called from scheduler to add the events of the current task /Users/rubber/linux/kernel/events/core.c: 3896
 * with interrupts disabled. /Users/rubber/linux/kernel/events/core.c: 3897
 * We restore the event value and then enable it. /Users/rubber/linux/kernel/events/core.c: 3899
 * This does not protect us against NMI, but enable() /Users/rubber/linux/kernel/events/core.c: 3901
 * sets the enabled bit in the control field of event _before_ /Users/rubber/linux/kernel/events/core.c: 3902
 * accessing the event control register. If a NMI hits, then it will /Users/rubber/linux/kernel/events/core.c: 3903
 * keep the event running. /Users/rubber/linux/kernel/events/core.c: 3904
	/* /Users/rubber/linux/kernel/events/core.c: 3912
	 * If cgroup events exist on this CPU, then we need to check if we have /Users/rubber/linux/kernel/events/core.c: 3913
	 * to switch in PMU state; cgroup event are system-wide mode only. /Users/rubber/linux/kernel/events/core.c: 3914
	 * /Users/rubber/linux/kernel/events/core.c: 3915
	 * Since cgroup events are CPU events, we must schedule these in before /Users/rubber/linux/kernel/events/core.c: 3916
	 * we schedule in the task events. /Users/rubber/linux/kernel/events/core.c: 3917
	/* /Users/rubber/linux/kernel/events/core.c: 3950
	 * We got @count in @nsec, with a target of sample_freq HZ /Users/rubber/linux/kernel/events/core.c: 3951
	 * the target period becomes: /Users/rubber/linux/kernel/events/core.c: 3952
	 * /Users/rubber/linux/kernel/events/core.c: 3953
	 *             @count * 10^9 /Users/rubber/linux/kernel/events/core.c: 3954
	 * period = ------------------- /Users/rubber/linux/kernel/events/core.c: 3955
	 *          @nsec * sample_freq /Users/rubber/linux/kernel/events/core.c: 3956
	 * /Users/rubber/linux/kernel/events/core.c: 3957
	/* /Users/rubber/linux/kernel/events/core.c: 3960
	 * Reduce accuracy by one bit such that @a and @b converge /Users/rubber/linux/kernel/events/core.c: 3961
	 * to a similar magnitude. /Users/rubber/linux/kernel/events/core.c: 3962
	/* /Users/rubber/linux/kernel/events/core.c: 3975
	 * Reduce accuracy until either term fits in a u64, then proceed with /Users/rubber/linux/kernel/events/core.c: 3976
	 * the other, so that finally we can do a u64/u64 division. /Users/rubber/linux/kernel/events/core.c: 3977
 * combine freq adjustment with unthrottling to avoid two passes over the /Users/rubber/linux/kernel/events/core.c: 4043
 * events. At the same time, make sure, having freq events does not change /Users/rubber/linux/kernel/events/core.c: 4044
 * the rate of unthrottling as that would introduce bias. /Users/rubber/linux/kernel/events/core.c: 4045
	/* /Users/rubber/linux/kernel/events/core.c: 4055
	 * only need to iterate over all events iff: /Users/rubber/linux/kernel/events/core.c: 4056
	 * - context have events in frequency mode (needs freq adjust) /Users/rubber/linux/kernel/events/core.c: 4057
	 * - there are events to unthrottle on this cpu /Users/rubber/linux/kernel/events/core.c: 4058
		/* /Users/rubber/linux/kernel/events/core.c: 4086
		 * stop the event and update event->count /Users/rubber/linux/kernel/events/core.c: 4087
		/* /Users/rubber/linux/kernel/events/core.c: 4095
		 * restart the event /Users/rubber/linux/kernel/events/core.c: 4096
		 * reload only if value has changed /Users/rubber/linux/kernel/events/core.c: 4097
		 * we have stopped the event so tell that /Users/rubber/linux/kernel/events/core.c: 4098
		 * to perf_adjust_period() to avoid stopping it /Users/rubber/linux/kernel/events/core.c: 4099
		 * twice. /Users/rubber/linux/kernel/events/core.c: 4100
 * Move @event to the tail of the @ctx's elegible events. /Users/rubber/linux/kernel/events/core.c: 4115
	/* /Users/rubber/linux/kernel/events/core.c: 4119
	 * Rotate the first entry last of non-pinned groups. Rotation might be /Users/rubber/linux/kernel/events/core.c: 4120
	 * disabled by the inheritance code. /Users/rubber/linux/kernel/events/core.c: 4121
	/* /Users/rubber/linux/kernel/events/core.c: 4146
	 * Unconditionally clear rotate_necessary; if ctx_flexible_sched_in() /Users/rubber/linux/kernel/events/core.c: 4147
	 * finds there are unschedulable events, it will set it again. /Users/rubber/linux/kernel/events/core.c: 4148
	/* /Users/rubber/linux/kernel/events/core.c: 4161
	 * Since we run this from IRQ context, nobody can install new /Users/rubber/linux/kernel/events/core.c: 4162
	 * events, thus the event count values are stable. /Users/rubber/linux/kernel/events/core.c: 4163
	/* /Users/rubber/linux/kernel/events/core.c: 4181
	 * As per the order given at ctx_resched() first 'pop' task flexible /Users/rubber/linux/kernel/events/core.c: 4182
	 * and then, if needed CPU flexible. /Users/rubber/linux/kernel/events/core.c: 4183
 * Enable all of a task's events that have been marked enable-on-exec. /Users/rubber/linux/kernel/events/core.c: 4235
 * This expects task == current. /Users/rubber/linux/kernel/events/core.c: 4236
	/* /Users/rubber/linux/kernel/events/core.c: 4260
	 * Unclone and reschedule this context if we enabled any event. /Users/rubber/linux/kernel/events/core.c: 4261
 * Removes all events from the current task that have been marked /Users/rubber/linux/kernel/events/core.c: 4283
 * remove-on-exec, and feeds their values back to parent events. /Users/rubber/linux/kernel/events/core.c: 4284
 * Cross CPU call to read the hardware event /Users/rubber/linux/kernel/events/core.c: 4353
	/* /Users/rubber/linux/kernel/events/core.c: 4363
	 * If this is a task context, we need to check whether it is /Users/rubber/linux/kernel/events/core.c: 4364
	 * the current task context of this cpu.  If not it has been /Users/rubber/linux/kernel/events/core.c: 4365
	 * scheduled out before the smp call arrived.  In that case /Users/rubber/linux/kernel/events/core.c: 4366
	 * event->count would have been updated to a recent sample /Users/rubber/linux/kernel/events/core.c: 4367
	 * when the event was scheduled out. /Users/rubber/linux/kernel/events/core.c: 4368
			/* /Users/rubber/linux/kernel/events/core.c: 4398
			 * Use sibling's PMU rather than @event's since /Users/rubber/linux/kernel/events/core.c: 4399
			 * sibling could be on different (eg: software) PMU. /Users/rubber/linux/kernel/events/core.c: 4400
 * NMI-safe method to read a local event, that is an event that /Users/rubber/linux/kernel/events/core.c: 4418
 * is: /Users/rubber/linux/kernel/events/core.c: 4419
 *   - either for the current task, or for this CPU /Users/rubber/linux/kernel/events/core.c: 4420
 *   - does not have inherit set, for inherited task events /Users/rubber/linux/kernel/events/core.c: 4421
 *     will not be local and we cannot read them atomically /Users/rubber/linux/kernel/events/core.c: 4422
 *   - must not have a pmu::count method /Users/rubber/linux/kernel/events/core.c: 4423
	/* /Users/rubber/linux/kernel/events/core.c: 4431
	 * Disabling interrupts avoids all counter scheduling (context /Users/rubber/linux/kernel/events/core.c: 4432
	 * switches, timer based rotation and IPIs). /Users/rubber/linux/kernel/events/core.c: 4433
	/* /Users/rubber/linux/kernel/events/core.c: 4437
	 * It must not be an event with inherit set, we cannot read /Users/rubber/linux/kernel/events/core.c: 4438
	 * all child counters from atomic context. /Users/rubber/linux/kernel/events/core.c: 4439
	/* /Users/rubber/linux/kernel/events/core.c: 4466
	 * If the event is currently on this CPU, its either a per-task event, /Users/rubber/linux/kernel/events/core.c: 4467
	 * or local to this CPU. Furthermore it means its ACTIVE (otherwise /Users/rubber/linux/kernel/events/core.c: 4468
	 * oncpu == -1). /Users/rubber/linux/kernel/events/core.c: 4469
	/* /Users/rubber/linux/kernel/events/core.c: 4496
	 * If event is enabled and currently active on a CPU, update the /Users/rubber/linux/kernel/events/core.c: 4497
	 * value in the event structure: /Users/rubber/linux/kernel/events/core.c: 4498
		/* /Users/rubber/linux/kernel/events/core.c: 4504
		 * Orders the ->state and ->oncpu loads such that if we see /Users/rubber/linux/kernel/events/core.c: 4505
		 * ACTIVE we must also see the right ->oncpu. /Users/rubber/linux/kernel/events/core.c: 4506
		 * /Users/rubber/linux/kernel/events/core.c: 4507
		 * Matches the smp_wmb() from event_sched_in(). /Users/rubber/linux/kernel/events/core.c: 4508
		/* /Users/rubber/linux/kernel/events/core.c: 4525
		 * Purposely ignore the smp_call_function_single() return /Users/rubber/linux/kernel/events/core.c: 4526
		 * value. /Users/rubber/linux/kernel/events/core.c: 4527
		 * /Users/rubber/linux/kernel/events/core.c: 4528
		 * If event_cpu isn't a valid CPU it means the event got /Users/rubber/linux/kernel/events/core.c: 4529
		 * scheduled out and that will have updated the event count. /Users/rubber/linux/kernel/events/core.c: 4530
		 * /Users/rubber/linux/kernel/events/core.c: 4531
		 * Therefore, either way, we'll have an up-to-date event count /Users/rubber/linux/kernel/events/core.c: 4532
		 * after this. /Users/rubber/linux/kernel/events/core.c: 4533
		/* /Users/rubber/linux/kernel/events/core.c: 4550
		 * May read while context is not active (e.g., thread is /Users/rubber/linux/kernel/events/core.c: 4551
		 * blocked), in that case we cannot update context time /Users/rubber/linux/kernel/events/core.c: 4552
 * Initialize the perf_event context in a task_struct: /Users/rubber/linux/kernel/events/core.c: 4569
 * Returns a matching context with refcount and pincount. /Users/rubber/linux/kernel/events/core.c: 4622
		/* /Users/rubber/linux/kernel/events/core.c: 4691
		 * If it has already passed perf_event_exit_task(). /Users/rubber/linux/kernel/events/core.c: 4692
		 * we must see PF_EXITING, it takes this mutex too. /Users/rubber/linux/kernel/events/core.c: 4693
 * The following implement mutual exclusion of events on "exclusive" pmus /Users/rubber/linux/kernel/events/core.c: 4861
 * (PERF_PMU_CAP_EXCLUSIVE). Such pmus can only have one event scheduled /Users/rubber/linux/kernel/events/core.c: 4862
 * at a time, so we disallow creating events that might conflict, namely: /Users/rubber/linux/kernel/events/core.c: 4863
 *  1) cpu-wide events in the presence of per-task events, /Users/rubber/linux/kernel/events/core.c: 4865
 *  2) per-task events in the presence of cpu-wide events, /Users/rubber/linux/kernel/events/core.c: 4866
 *  3) two matching events on the same context. /Users/rubber/linux/kernel/events/core.c: 4867
 * The former two cases are handled in the allocation path (perf_event_alloc(), /Users/rubber/linux/kernel/events/core.c: 4869
 * _free_event()), the latter -- before the first perf_install_in_context(). /Users/rubber/linux/kernel/events/core.c: 4870
	/* /Users/rubber/linux/kernel/events/core.c: 4879
	 * Prevent co-existence of per-task and cpu-wide events on the /Users/rubber/linux/kernel/events/core.c: 4880
	 * same exclusive pmu. /Users/rubber/linux/kernel/events/core.c: 4881
	 * /Users/rubber/linux/kernel/events/core.c: 4882
	 * Negative pmu::exclusive_cnt means there are cpu-wide /Users/rubber/linux/kernel/events/core.c: 4883
	 * events on this "exclusive" pmu, positive means there are /Users/rubber/linux/kernel/events/core.c: 4884
	 * per-task events. /Users/rubber/linux/kernel/events/core.c: 4885
	 * /Users/rubber/linux/kernel/events/core.c: 4886
	 * Since this is called in perf_event_alloc() path, event::ctx /Users/rubber/linux/kernel/events/core.c: 4887
	 * doesn't exist yet; it is, however, safe to use PERF_ATTACH_TASK /Users/rubber/linux/kernel/events/core.c: 4888
	 * to mean "per-task event", because unlike other attach states it /Users/rubber/linux/kernel/events/core.c: 4889
	 * never gets cleared. /Users/rubber/linux/kernel/events/core.c: 4890
		/* /Users/rubber/linux/kernel/events/core.c: 4958
		 * Can happen when we close an event with re-directed output. /Users/rubber/linux/kernel/events/core.c: 4959
		 * /Users/rubber/linux/kernel/events/core.c: 4960
		 * Since we have a 0 refcount, perf_mmap_close() will skip /Users/rubber/linux/kernel/events/core.c: 4961
		 * over us; possibly making our ring_buffer_put() the last. /Users/rubber/linux/kernel/events/core.c: 4962
	/* /Users/rubber/linux/kernel/events/core.c: 4984
	 * Must be after ->destroy(), due to uprobe_perf_close() using /Users/rubber/linux/kernel/events/core.c: 4985
	 * hw.target. /Users/rubber/linux/kernel/events/core.c: 4986
	/* /Users/rubber/linux/kernel/events/core.c: 4991
	 * perf_event_free_task() relies on put_ctx() being 'last', in particular /Users/rubber/linux/kernel/events/core.c: 4992
	 * all task references must be cleaned up. /Users/rubber/linux/kernel/events/core.c: 4993
 * Used to free events which have a known refcount of 1, such as in error paths /Users/rubber/linux/kernel/events/core.c: 5005
 * where the event isn't exposed yet and inherited events. /Users/rubber/linux/kernel/events/core.c: 5006
 * Remove user event from the owner task. /Users/rubber/linux/kernel/events/core.c: 5021
	/* /Users/rubber/linux/kernel/events/core.c: 5028
	 * Matches the smp_store_release() in perf_event_exit_task(). If we /Users/rubber/linux/kernel/events/core.c: 5029
	 * observe !owner it means the list deletion is complete and we can /Users/rubber/linux/kernel/events/core.c: 5030
	 * indeed free this event, otherwise we need to serialize on /Users/rubber/linux/kernel/events/core.c: 5031
	 * owner->perf_event_mutex. /Users/rubber/linux/kernel/events/core.c: 5032
		/* /Users/rubber/linux/kernel/events/core.c: 5036
		 * Since delayed_put_task_struct() also drops the last /Users/rubber/linux/kernel/events/core.c: 5037
		 * task reference we can safely take a new reference /Users/rubber/linux/kernel/events/core.c: 5038
		 * while holding the rcu_read_lock(). /Users/rubber/linux/kernel/events/core.c: 5039
		/* /Users/rubber/linux/kernel/events/core.c: 5046
		 * If we're here through perf_event_exit_task() we're already /Users/rubber/linux/kernel/events/core.c: 5047
		 * holding ctx->mutex which would be an inversion wrt. the /Users/rubber/linux/kernel/events/core.c: 5048
		 * normal lock order. /Users/rubber/linux/kernel/events/core.c: 5049
		 * /Users/rubber/linux/kernel/events/core.c: 5050
		 * However we can safely take this lock because its the child /Users/rubber/linux/kernel/events/core.c: 5051
		 * ctx->mutex. /Users/rubber/linux/kernel/events/core.c: 5052
		/* /Users/rubber/linux/kernel/events/core.c: 5056
		 * We have to re-check the event->owner field, if it is cleared /Users/rubber/linux/kernel/events/core.c: 5057
		 * we raced with perf_event_exit_task(), acquiring the mutex /Users/rubber/linux/kernel/events/core.c: 5058
		 * ensured they're done, and we can proceed with freeing the /Users/rubber/linux/kernel/events/core.c: 5059
		 * event. /Users/rubber/linux/kernel/events/core.c: 5060
 * Kill an event dead; while event:refcount will preserve the event /Users/rubber/linux/kernel/events/core.c: 5080
 * object, it will not preserve its functionality. Once the last 'user' /Users/rubber/linux/kernel/events/core.c: 5081
 * gives up the object, we'll destroy the thing. /Users/rubber/linux/kernel/events/core.c: 5082
	/* /Users/rubber/linux/kernel/events/core.c: 5090
	 * If we got here through err_file: fput(event_file); we will not have /Users/rubber/linux/kernel/events/core.c: 5091
	 * attached to a context yet. /Users/rubber/linux/kernel/events/core.c: 5092
	/* /Users/rubber/linux/kernel/events/core.c: 5108
	 * Mark this event as STATE_DEAD, there is no external reference to it /Users/rubber/linux/kernel/events/core.c: 5109
	 * anymore. /Users/rubber/linux/kernel/events/core.c: 5110
	 * /Users/rubber/linux/kernel/events/core.c: 5111
	 * Anybody acquiring event->child_mutex after the below loop _must_ /Users/rubber/linux/kernel/events/core.c: 5112
	 * also see this, most importantly inherit_event() which will avoid /Users/rubber/linux/kernel/events/core.c: 5113
	 * placing more children on the list. /Users/rubber/linux/kernel/events/core.c: 5114
	 * /Users/rubber/linux/kernel/events/core.c: 5115
	 * Thus this guarantees that we will in fact observe and kill _ALL_ /Users/rubber/linux/kernel/events/core.c: 5116
	 * child events. /Users/rubber/linux/kernel/events/core.c: 5117
		/* /Users/rubber/linux/kernel/events/core.c: 5128
		 * Cannot change, child events are not migrated, see the /Users/rubber/linux/kernel/events/core.c: 5129
		 * comment with perf_event_ctx_lock_nested(). /Users/rubber/linux/kernel/events/core.c: 5130
		/* /Users/rubber/linux/kernel/events/core.c: 5133
		 * Since child_mutex nests inside ctx::mutex, we must jump /Users/rubber/linux/kernel/events/core.c: 5134
		 * through hoops. We start by grabbing a reference on the ctx. /Users/rubber/linux/kernel/events/core.c: 5135
		 * /Users/rubber/linux/kernel/events/core.c: 5136
		 * Since the event cannot get freed while we hold the /Users/rubber/linux/kernel/events/core.c: 5137
		 * child_mutex, the context must also exist and have a !0 /Users/rubber/linux/kernel/events/core.c: 5138
		 * reference count. /Users/rubber/linux/kernel/events/core.c: 5139
		/* /Users/rubber/linux/kernel/events/core.c: 5143
		 * Now that we have a ctx ref, we can drop child_mutex, and /Users/rubber/linux/kernel/events/core.c: 5144
		 * acquire ctx::mutex without fear of it going away. Then we /Users/rubber/linux/kernel/events/core.c: 5145
		 * can re-acquire child_mutex. /Users/rubber/linux/kernel/events/core.c: 5146
		/* /Users/rubber/linux/kernel/events/core.c: 5152
		 * Now that we hold ctx::mutex and child_mutex, revalidate our /Users/rubber/linux/kernel/events/core.c: 5153
		 * state, if child is still the first entry, it didn't get freed /Users/rubber/linux/kernel/events/core.c: 5154
		 * and we can continue doing so. /Users/rubber/linux/kernel/events/core.c: 5155
			/* /Users/rubber/linux/kernel/events/core.c: 5162
			 * This matches the refcount bump in inherit_event(); /Users/rubber/linux/kernel/events/core.c: 5163
			 * this can't be the last reference. /Users/rubber/linux/kernel/events/core.c: 5164
		/* /Users/rubber/linux/kernel/events/core.c: 5182
		 * Wake any perf_event_free_task() waiting for this event to be /Users/rubber/linux/kernel/events/core.c: 5183
		 * freed. /Users/rubber/linux/kernel/events/core.c: 5184
 * Called when the last reference to the file is gone. /Users/rubber/linux/kernel/events/core.c: 5197
	/* /Users/rubber/linux/kernel/events/core.c: 5262
	 * Since we co-schedule groups, {enabled,running} times of siblings /Users/rubber/linux/kernel/events/core.c: 5263
	 * will be identical to those of the leader, so we only publish one /Users/rubber/linux/kernel/events/core.c: 5264
	 * set. /Users/rubber/linux/kernel/events/core.c: 5265
	/* /Users/rubber/linux/kernel/events/core.c: 5277
	 * Write {count,id} tuples for every sibling. /Users/rubber/linux/kernel/events/core.c: 5278
	/* /Users/rubber/linux/kernel/events/core.c: 5310
	 * By locking the child_mutex of the leader we effectively /Users/rubber/linux/kernel/events/core.c: 5311
	 * lock the child list of all siblings.. XXX explain how. /Users/rubber/linux/kernel/events/core.c: 5312
 * Read the performance event - simple non blocking version for now /Users/rubber/linux/kernel/events/core.c: 5375
	/* /Users/rubber/linux/kernel/events/core.c: 5383
	 * Return end-of-file for a read on an event that is in /Users/rubber/linux/kernel/events/core.c: 5384
	 * error state (i.e. because it was pinned but it couldn't be /Users/rubber/linux/kernel/events/core.c: 5385
	 * scheduled on to the CPU at some point). /Users/rubber/linux/kernel/events/core.c: 5386
	/* /Users/rubber/linux/kernel/events/core.c: 5432
	 * Pin the event->rb by taking event->mmap_mutex; otherwise /Users/rubber/linux/kernel/events/core.c: 5433
	 * perf_event_set_output() can swizzle our rb and make us miss wakeups. /Users/rubber/linux/kernel/events/core.c: 5434
 * Holding the top-level event's child_mutex means that any /Users/rubber/linux/kernel/events/core.c: 5470
 * descendant process that has inherited this event will block /Users/rubber/linux/kernel/events/core.c: 5471
 * in perf_event_exit_event() if it goes to exit, thus satisfying the /Users/rubber/linux/kernel/events/core.c: 5472
 * task existence requirements of perf_event_enable/disable. /Users/rubber/linux/kernel/events/core.c: 5473
		/* /Users/rubber/linux/kernel/events/core.c: 5522
		 * We could be throttled; unthrottle now to avoid the tick /Users/rubber/linux/kernel/events/core.c: 5523
		 * trying to unthrottle while we already re-started the event. /Users/rubber/linux/kernel/events/core.c: 5524
 * Callers need to ensure there can be no nesting of this function, otherwise /Users/rubber/linux/kernel/events/core.c: 5841
 * the seqlock logic goes bad. We can not serialize this because the arch /Users/rubber/linux/kernel/events/core.c: 5842
 * code calls this from NMI context. /Users/rubber/linux/kernel/events/core.c: 5843
	/* /Users/rubber/linux/kernel/events/core.c: 5856
	 * compute total_time_enabled, total_time_running /Users/rubber/linux/kernel/events/core.c: 5857
	 * based on snapshot values taken when the event /Users/rubber/linux/kernel/events/core.c: 5858
	 * was last scheduled in. /Users/rubber/linux/kernel/events/core.c: 5859
	 * /Users/rubber/linux/kernel/events/core.c: 5860
	 * we cannot simply called update_context_time() /Users/rubber/linux/kernel/events/core.c: 5861
	 * because of locking issue as we can be called in /Users/rubber/linux/kernel/events/core.c: 5862
	 * NMI context /Users/rubber/linux/kernel/events/core.c: 5863
	/* /Users/rubber/linux/kernel/events/core.c: 5868
	 * Disable preemption to guarantee consistent time stamps are stored to /Users/rubber/linux/kernel/events/core.c: 5869
	 * the user page. /Users/rubber/linux/kernel/events/core.c: 5870
		/* /Users/rubber/linux/kernel/events/core.c: 5938
		 * Should be impossible, we set this when removing /Users/rubber/linux/kernel/events/core.c: 5939
		 * event->rb_entry and wait/clear when adding event->rb_entry. /Users/rubber/linux/kernel/events/core.c: 5940
	/* /Users/rubber/linux/kernel/events/core.c: 5964
	 * Avoid racing with perf_mmap_close(AUX): stop the event /Users/rubber/linux/kernel/events/core.c: 5965
	 * before swizzling the event::rb pointer; if it's getting /Users/rubber/linux/kernel/events/core.c: 5966
	 * unmapped, its aux_mmap_count will be 0 and it won't /Users/rubber/linux/kernel/events/core.c: 5967
	 * restart. See the comment in __perf_pmu_output_stop(). /Users/rubber/linux/kernel/events/core.c: 5968
	 * /Users/rubber/linux/kernel/events/core.c: 5969
	 * Data will inevitably be lost when set_output is done in /Users/rubber/linux/kernel/events/core.c: 5970
	 * mid-air, but then again, whoever does it like this is /Users/rubber/linux/kernel/events/core.c: 5971
	 * not in for the data anyway. /Users/rubber/linux/kernel/events/core.c: 5972
		/* /Users/rubber/linux/kernel/events/core.c: 5981
		 * Since we detached before setting the new rb, so that we /Users/rubber/linux/kernel/events/core.c: 5982
		 * could attach the new rb, we could have missed a wakeup. /Users/rubber/linux/kernel/events/core.c: 5983
		 * Provide it now. /Users/rubber/linux/kernel/events/core.c: 5984
 * A buffer can be mmap()ed multiple times; either directly through the same /Users/rubber/linux/kernel/events/core.c: 6045
 * event, or through other events by use of perf_event_set_output(). /Users/rubber/linux/kernel/events/core.c: 6046
 * In order to undo the VM accounting done by perf_mmap() we need to destroy /Users/rubber/linux/kernel/events/core.c: 6048
 * the buffer here, where we still have a VM context. This means we need /Users/rubber/linux/kernel/events/core.c: 6049
 * to detach all events redirecting to us. /Users/rubber/linux/kernel/events/core.c: 6050
	/* /Users/rubber/linux/kernel/events/core.c: 6064
	 * rb->aux_mmap_count will always drop before rb->mmap_count and /Users/rubber/linux/kernel/events/core.c: 6065
	 * event->mmap_count, so it is ok to use event->mmap_mutex to /Users/rubber/linux/kernel/events/core.c: 6066
	 * serialize with perf_mmap here. /Users/rubber/linux/kernel/events/core.c: 6067
		/* /Users/rubber/linux/kernel/events/core.c: 6071
		 * Stop all AUX events that are writing to this buffer, /Users/rubber/linux/kernel/events/core.c: 6072
		 * so that we can free its AUX pages and corresponding PMU /Users/rubber/linux/kernel/events/core.c: 6073
		 * data. Note that after rb::aux_mmap_count dropped to zero, /Users/rubber/linux/kernel/events/core.c: 6074
		 * they won't start any more (see perf_aux_output_begin()). /Users/rubber/linux/kernel/events/core.c: 6075
	/* /Users/rubber/linux/kernel/events/core.c: 6103
	 * No other mmap()s, detach from all other events that might redirect /Users/rubber/linux/kernel/events/core.c: 6104
	 * into the now unreachable buffer. Somewhat complicated by the /Users/rubber/linux/kernel/events/core.c: 6105
	 * fact that rb::event_lock otherwise nests inside mmap_mutex. /Users/rubber/linux/kernel/events/core.c: 6106
			/* /Users/rubber/linux/kernel/events/core.c: 6112
			 * This event is en-route to free_event() which will /Users/rubber/linux/kernel/events/core.c: 6113
			 * detach it and remove it from the list. /Users/rubber/linux/kernel/events/core.c: 6114
		/* /Users/rubber/linux/kernel/events/core.c: 6121
		 * Check we didn't race with perf_event_set_output() which can /Users/rubber/linux/kernel/events/core.c: 6122
		 * swizzle the rb from under us while we were waiting to /Users/rubber/linux/kernel/events/core.c: 6123
		 * acquire mmap_mutex. /Users/rubber/linux/kernel/events/core.c: 6124
		 * /Users/rubber/linux/kernel/events/core.c: 6125
		 * If we find a different rb; ignore this event, a next /Users/rubber/linux/kernel/events/core.c: 6126
		 * iteration will no longer find it on the list. We have to /Users/rubber/linux/kernel/events/core.c: 6127
		 * still restart the iteration to make sure we're not now /Users/rubber/linux/kernel/events/core.c: 6128
		 * iterating the wrong list. /Users/rubber/linux/kernel/events/core.c: 6129
		/* /Users/rubber/linux/kernel/events/core.c: 6137
		 * Restart the iteration; either we're on the wrong list or /Users/rubber/linux/kernel/events/core.c: 6138
		 * destroyed its integrity by doing a deletion. /Users/rubber/linux/kernel/events/core.c: 6139
	/* /Users/rubber/linux/kernel/events/core.c: 6145
	 * It could be there's still a few 0-ref events on the list; they'll /Users/rubber/linux/kernel/events/core.c: 6146
	 * get cleaned up by free_event() -- they'll also still have their /Users/rubber/linux/kernel/events/core.c: 6147
	 * ref on the rb and will free it whenever they are done with it. /Users/rubber/linux/kernel/events/core.c: 6148
	 * /Users/rubber/linux/kernel/events/core.c: 6149
	 * Aside from that, this buffer is 'fully' detached and unmapped, /Users/rubber/linux/kernel/events/core.c: 6150
	 * undo the VM accounting. /Users/rubber/linux/kernel/events/core.c: 6151
	/* /Users/rubber/linux/kernel/events/core.c: 6182
	 * Don't allow mmap() of inherited per-task counters. This would /Users/rubber/linux/kernel/events/core.c: 6183
	 * create a performance issue due to all children writing to the /Users/rubber/linux/kernel/events/core.c: 6184
	 * same rb. /Users/rubber/linux/kernel/events/core.c: 6185
		/* /Users/rubber/linux/kernel/events/core.c: 6202
		 * AUX area mapping: if rb->aux_nr_pages != 0, it's already /Users/rubber/linux/kernel/events/core.c: 6203
		 * mapped, all subsequent mappings should have the same size /Users/rubber/linux/kernel/events/core.c: 6204
		 * and offset. Must be above the normal perf buffer. /Users/rubber/linux/kernel/events/core.c: 6205
	/* /Users/rubber/linux/kernel/events/core.c: 6259
	 * If we have rb pages ensure they're a power-of-two number, so we /Users/rubber/linux/kernel/events/core.c: 6260
	 * can do bitmasks instead of modulo. /Users/rubber/linux/kernel/events/core.c: 6261
			/* /Users/rubber/linux/kernel/events/core.c: 6279
			 * Raced against perf_mmap_close() through /Users/rubber/linux/kernel/events/core.c: 6280
			 * perf_event_set_output(). Try again, hope for better /Users/rubber/linux/kernel/events/core.c: 6281
			 * luck. /Users/rubber/linux/kernel/events/core.c: 6282
	/* /Users/rubber/linux/kernel/events/core.c: 6296
	 * Increase the limit linearly with more CPUs: /Users/rubber/linux/kernel/events/core.c: 6297
	/* /Users/rubber/linux/kernel/events/core.c: 6303
	 * sysctl_perf_event_mlock may have changed, so that /Users/rubber/linux/kernel/events/core.c: 6304
	 *     user->locked_vm > user_lock_limit /Users/rubber/linux/kernel/events/core.c: 6305
		/* /Users/rubber/linux/kernel/events/core.c: 6312
		 * charge locked_vm until it hits user_lock_limit; /Users/rubber/linux/kernel/events/core.c: 6313
		 * charge the rest from pinned_vm /Users/rubber/linux/kernel/events/core.c: 6314
	/* /Users/rubber/linux/kernel/events/core.c: 6374
	 * Since pinned accounting is per vm we cannot allow fork() to copy our /Users/rubber/linux/kernel/events/core.c: 6375
	 * vma. /Users/rubber/linux/kernel/events/core.c: 6376
 * Perf event wakeup /Users/rubber/linux/kernel/events/core.c: 6415
 * If there's data, ensure we set the poll() state and publish everything /Users/rubber/linux/kernel/events/core.c: 6417
 * to user-space before waking everybody up. /Users/rubber/linux/kernel/events/core.c: 6418
	/* /Users/rubber/linux/kernel/events/core.c: 6441
	 * We'd expect this to only occur if the irq_work is delayed and either /Users/rubber/linux/kernel/events/core.c: 6442
	 * ctx->task or current has changed in the meantime. This can be the /Users/rubber/linux/kernel/events/core.c: 6443
	 * case on architectures that do not implement arch_irq_work_raise(). /Users/rubber/linux/kernel/events/core.c: 6444
	/* /Users/rubber/linux/kernel/events/core.c: 6449
	 * perf_pending_event() can race with the task exiting. /Users/rubber/linux/kernel/events/core.c: 6450
	/* /Users/rubber/linux/kernel/events/core.c: 6479
	 *  CPU-A			CPU-B /Users/rubber/linux/kernel/events/core.c: 6480
	 * /Users/rubber/linux/kernel/events/core.c: 6481
	 *  perf_event_disable_inatomic() /Users/rubber/linux/kernel/events/core.c: 6482
	 *    @pending_disable = CPU-A; /Users/rubber/linux/kernel/events/core.c: 6483
	 *    irq_work_queue(); /Users/rubber/linux/kernel/events/core.c: 6484
	 * /Users/rubber/linux/kernel/events/core.c: 6485
	 *  sched-out /Users/rubber/linux/kernel/events/core.c: 6486
	 *    @pending_disable = -1; /Users/rubber/linux/kernel/events/core.c: 6487
	 * /Users/rubber/linux/kernel/events/core.c: 6488
	 *				sched-in /Users/rubber/linux/kernel/events/core.c: 6489
	 *				perf_event_disable_inatomic() /Users/rubber/linux/kernel/events/core.c: 6490
	 *				  @pending_disable = CPU-B; /Users/rubber/linux/kernel/events/core.c: 6491
	 *				  irq_work_queue(); // FAILS /Users/rubber/linux/kernel/events/core.c: 6492
	 * /Users/rubber/linux/kernel/events/core.c: 6493
	 *  irq_work_run() /Users/rubber/linux/kernel/events/core.c: 6494
	 *    perf_pending_event() /Users/rubber/linux/kernel/events/core.c: 6495
	 * /Users/rubber/linux/kernel/events/core.c: 6496
	 * But the event runs on CPU-B and wants disabling there. /Users/rubber/linux/kernel/events/core.c: 6497
	/* /Users/rubber/linux/kernel/events/core.c: 6508
	 * If we 'fail' here, that's OK, it means recursion is already disabled /Users/rubber/linux/kernel/events/core.c: 6509
	 * and we won't recurse 'further'. /Users/rubber/linux/kernel/events/core.c: 6510
 * We assume there is only KVM supporting the callbacks. /Users/rubber/linux/kernel/events/core.c: 6525
 * Later on, we might change it to a list if there is /Users/rubber/linux/kernel/events/core.c: 6526
 * another virtualization implementation supporting the callbacks. /Users/rubber/linux/kernel/events/core.c: 6527
 * Get remaining task size from user stack pointer. /Users/rubber/linux/kernel/events/core.c: 6584
 * It'd be better to take stack vma map and limit this more /Users/rubber/linux/kernel/events/core.c: 6586
 * precisely, but there's no way to get it safely under interrupt, /Users/rubber/linux/kernel/events/core.c: 6587
 * so using TASK_SIZE as limit. /Users/rubber/linux/kernel/events/core.c: 6588
	/* /Users/rubber/linux/kernel/events/core.c: 6610
	 * Check if we fit in with the requested stack size into the: /Users/rubber/linux/kernel/events/core.c: 6611
	 * - TASK_SIZE /Users/rubber/linux/kernel/events/core.c: 6612
	 *   If we don't, we limit the size to the TASK_SIZE. /Users/rubber/linux/kernel/events/core.c: 6613
	 * /Users/rubber/linux/kernel/events/core.c: 6614
	 * - remaining sample size /Users/rubber/linux/kernel/events/core.c: 6615
	 *   If we don't, we customize the stack size to /Users/rubber/linux/kernel/events/core.c: 6616
	 *   fit in to the remaining sample size. /Users/rubber/linux/kernel/events/core.c: 6617
		/* /Users/rubber/linux/kernel/events/core.c: 6628
		 * If we overflow the maximum size for the sample, /Users/rubber/linux/kernel/events/core.c: 6629
		 * we customize the stack dump size to fit in. /Users/rubber/linux/kernel/events/core.c: 6630
		/* /Users/rubber/linux/kernel/events/core.c: 6653
		 * We dump: /Users/rubber/linux/kernel/events/core.c: 6654
		 * static size /Users/rubber/linux/kernel/events/core.c: 6655
		 *   - the size requested by user or the best one we can fit /Users/rubber/linux/kernel/events/core.c: 6656
		 *     in to the sample max size /Users/rubber/linux/kernel/events/core.c: 6657
		 * data /Users/rubber/linux/kernel/events/core.c: 6658
		 *   - user stack dump data /Users/rubber/linux/kernel/events/core.c: 6659
		 * dynamic size /Users/rubber/linux/kernel/events/core.c: 6660
		 *   - the actual dumped size /Users/rubber/linux/kernel/events/core.c: 6661
	/* /Users/rubber/linux/kernel/events/core.c: 6703
	 * If this is an NMI hit inside sampling code, don't take /Users/rubber/linux/kernel/events/core.c: 6704
	 * the sample. See also perf_aux_sample_output(). /Users/rubber/linux/kernel/events/core.c: 6705
	/* /Users/rubber/linux/kernel/events/core.c: 6727
	 * Normal ->start()/->stop() callbacks run in IRQ mode in scheduler /Users/rubber/linux/kernel/events/core.c: 6728
	 * paths. If we start calling them in NMI context, they may race with /Users/rubber/linux/kernel/events/core.c: 6729
	 * the IRQ ones, that is, for example, re-starting an event that's just /Users/rubber/linux/kernel/events/core.c: 6730
	 * been stopped, which is why we're using a separate callback that /Users/rubber/linux/kernel/events/core.c: 6731
	 * doesn't change the event state. /Users/rubber/linux/kernel/events/core.c: 6732
	 * /Users/rubber/linux/kernel/events/core.c: 6733
	 * IRQs need to be disabled to prevent IPIs from racing with us. /Users/rubber/linux/kernel/events/core.c: 6734
	/* /Users/rubber/linux/kernel/events/core.c: 6737
	 * Guard against NMI hits inside the critical section; /Users/rubber/linux/kernel/events/core.c: 6738
	 * see also perf_prepare_sample_aux(). /Users/rubber/linux/kernel/events/core.c: 6739
	/* /Users/rubber/linux/kernel/events/core.c: 6771
	 * An error here means that perf_output_copy() failed (returned a /Users/rubber/linux/kernel/events/core.c: 6772
	 * non-zero surplus that it didn't copy), which in its current /Users/rubber/linux/kernel/events/core.c: 6773
	 * enlightened implementation is not possible. If that changes, we'd /Users/rubber/linux/kernel/events/core.c: 6774
	 * like to know. /Users/rubber/linux/kernel/events/core.c: 6775
	/* /Users/rubber/linux/kernel/events/core.c: 6780
	 * The pad comes from ALIGN()ing data->aux_size up to u64 in /Users/rubber/linux/kernel/events/core.c: 6781
	 * perf_prepare_sample_aux(), so should not be more than that. /Users/rubber/linux/kernel/events/core.c: 6782
 * XXX PERF_SAMPLE_READ vs inherited events seems difficult. /Users/rubber/linux/kernel/events/core.c: 6936
 * The problem is that its both hard and excessively expensive to iterate the /Users/rubber/linux/kernel/events/core.c: 6938
 * child list, not to mention that its impossible to IPI the children running /Users/rubber/linux/kernel/events/core.c: 6939
 * on another CPU, from interrupt/NMI context. /Users/rubber/linux/kernel/events/core.c: 6940
	/* /Users/rubber/linux/kernel/events/core.c: 6948
	 * compute total_time_enabled, total_time_running /Users/rubber/linux/kernel/events/core.c: 6949
	 * based on snapshot values taken when the event /Users/rubber/linux/kernel/events/core.c: 6950
	 * was last scheduled in. /Users/rubber/linux/kernel/events/core.c: 6951
	 * /Users/rubber/linux/kernel/events/core.c: 6952
	 * we cannot simply called update_context_time() /Users/rubber/linux/kernel/events/core.c: 6953
	 * because of locking issue as we are called in /Users/rubber/linux/kernel/events/core.c: 6954
	 * NMI context /Users/rubber/linux/kernel/events/core.c: 6955
			/* /Users/rubber/linux/kernel/events/core.c: 7063
			 * we always store at least the value of nr /Users/rubber/linux/kernel/events/core.c: 7064
		/* /Users/rubber/linux/kernel/events/core.c: 7074
		 * If there are no regs to dump, notice it through /Users/rubber/linux/kernel/events/core.c: 7075
		 * first u64 being zero (PERF_SAMPLE_REGS_ABI_NONE). /Users/rubber/linux/kernel/events/core.c: 7076
		/* /Users/rubber/linux/kernel/events/core.c: 7105
		 * If there are no regs to dump, notice it through /Users/rubber/linux/kernel/events/core.c: 7106
		 * first u64 being zero (PERF_SAMPLE_REGS_ABI_NONE). /Users/rubber/linux/kernel/events/core.c: 7107
		/* /Users/rubber/linux/kernel/events/core.c: 7167
		 * Walking the pages tables for user address. /Users/rubber/linux/kernel/events/core.c: 7168
		 * Interrupts are disabled, so it prevents any tear down /Users/rubber/linux/kernel/events/core.c: 7169
		 * of the page tables. /Users/rubber/linux/kernel/events/core.c: 7170
		 * Try IRQ-safe get_user_page_fast_only first. /Users/rubber/linux/kernel/events/core.c: 7171
		 * If failed, leave phys_addr as 0. /Users/rubber/linux/kernel/events/core.c: 7172
 * Return the pagetable size of a given virtual address. /Users/rubber/linux/kernel/events/core.c: 7190
	/* /Users/rubber/linux/kernel/events/core.c: 7254
	 * Software page-table walkers must disable IRQs, /Users/rubber/linux/kernel/events/core.c: 7255
	 * which prevents any tear down of the page tables. /Users/rubber/linux/kernel/events/core.c: 7256
		/* /Users/rubber/linux/kernel/events/core.c: 7262
		 * For kernel threads and the like, use init_mm so that /Users/rubber/linux/kernel/events/core.c: 7263
		 * we can find kernel memory. /Users/rubber/linux/kernel/events/core.c: 7264
		/* /Users/rubber/linux/kernel/events/core.c: 7378
		 * Either we need PERF_SAMPLE_STACK_USER bit to be always /Users/rubber/linux/kernel/events/core.c: 7379
		 * processed as the last one or have additional check added /Users/rubber/linux/kernel/events/core.c: 7380
		 * in case new sample type is added, because we could eat /Users/rubber/linux/kernel/events/core.c: 7381
		 * up the rest of the sample size. /Users/rubber/linux/kernel/events/core.c: 7382
		/* /Users/rubber/linux/kernel/events/core.c: 7390
		 * If there is something to dump, add space for the dump /Users/rubber/linux/kernel/events/core.c: 7391
		 * itself and for the field that tells the dynamic size, /Users/rubber/linux/kernel/events/core.c: 7392
		 * which is how many have been actually dumped. /Users/rubber/linux/kernel/events/core.c: 7393
	/* /Users/rubber/linux/kernel/events/core.c: 7430
	 * PERF_DATA_PAGE_SIZE requires PERF_SAMPLE_ADDR. If the user doesn't /Users/rubber/linux/kernel/events/core.c: 7431
	 * require PERF_SAMPLE_ADDR, kernel implicitly retrieve the data->addr, /Users/rubber/linux/kernel/events/core.c: 7432
	 * but the value will not dump to the userspace. /Users/rubber/linux/kernel/events/core.c: 7433
		/* /Users/rubber/linux/kernel/events/core.c: 7446
		 * Given the 16bit nature of header::size, an AUX sample can /Users/rubber/linux/kernel/events/core.c: 7447
		 * easily overflow it, what with all the preceding sample bits. /Users/rubber/linux/kernel/events/core.c: 7448
		 * Make sure this doesn't happen by using up to U16_MAX bytes /Users/rubber/linux/kernel/events/core.c: 7449
		 * per sample in total (rounded down to 8 byte boundary). /Users/rubber/linux/kernel/events/core.c: 7450
	/* /Users/rubber/linux/kernel/events/core.c: 7460
	 * If you're adding more sample types here, you likely need to do /Users/rubber/linux/kernel/events/core.c: 7461
	 * something about the overflowing header::size, like repurpose the /Users/rubber/linux/kernel/events/core.c: 7462
	 * lowest 3 bits of size, which should be always zero at the moment. /Users/rubber/linux/kernel/events/core.c: 7463
	 * This raises a more important question, do we really need 512k sized /Users/rubber/linux/kernel/events/core.c: 7464
	 * samples and why, so good argumentation is in order for whatever you /Users/rubber/linux/kernel/events/core.c: 7465
	 * do here next. /Users/rubber/linux/kernel/events/core.c: 7466
 * read event_id /Users/rubber/linux/kernel/events/core.c: 7527
		/* /Users/rubber/linux/kernel/events/core.c: 7593
		 * Skip events that are not fully formed yet; ensure that /Users/rubber/linux/kernel/events/core.c: 7594
		 * if we observe event->ctx, both event and ctx will be /Users/rubber/linux/kernel/events/core.c: 7595
		 * complete enough. See perf_install_in_context(). /Users/rubber/linux/kernel/events/core.c: 7596
 * Iterate all events that need to receive side-band events. /Users/rubber/linux/kernel/events/core.c: 7610
 * For new callers; ensure that account_pmu_sb_event() includes /Users/rubber/linux/kernel/events/core.c: 7612
 * your event, otherwise it might not get delivered. /Users/rubber/linux/kernel/events/core.c: 7613
	/* /Users/rubber/linux/kernel/events/core.c: 7625
	 * If we have task_ctx != NULL we only notify the task context itself. /Users/rubber/linux/kernel/events/core.c: 7626
	 * The task_ctx is set only for EXIT events before releasing task /Users/rubber/linux/kernel/events/core.c: 7627
	 * context. /Users/rubber/linux/kernel/events/core.c: 7628
 * Clear all file-based filters at exec, they'll have to be /Users/rubber/linux/kernel/events/core.c: 7648
 * re-instated when/if these objects are mmapped again. /Users/rubber/linux/kernel/events/core.c: 7649
	/* /Users/rubber/linux/kernel/events/core.c: 7719
	 * In case of inheritance, it will be the parent that links to the /Users/rubber/linux/kernel/events/core.c: 7720
	 * ring-buffer, but it will be the child that's actually using it. /Users/rubber/linux/kernel/events/core.c: 7721
	 * /Users/rubber/linux/kernel/events/core.c: 7722
	 * We are using event::rb to determine if the event should be stopped, /Users/rubber/linux/kernel/events/core.c: 7723
	 * however this may race with ring_buffer_attach() (through set_output), /Users/rubber/linux/kernel/events/core.c: 7724
	 * which will make us skip the event that actually needs to be stopped. /Users/rubber/linux/kernel/events/core.c: 7725
	 * So ring_buffer_attach() has to stop an aux event before re-assigning /Users/rubber/linux/kernel/events/core.c: 7726
	 * its rb pointer. /Users/rubber/linux/kernel/events/core.c: 7727
		/* /Users/rubber/linux/kernel/events/core.c: 7760
		 * For per-CPU events, we need to make sure that neither they /Users/rubber/linux/kernel/events/core.c: 7761
		 * nor their children are running; for cpu==-1 events it's /Users/rubber/linux/kernel/events/core.c: 7762
		 * sufficient to stop the event itself if it's active, since /Users/rubber/linux/kernel/events/core.c: 7763
		 * it can't have children. /Users/rubber/linux/kernel/events/core.c: 7764
 * task tracking -- fork/exit /Users/rubber/linux/kernel/events/core.c: 7783
 * enabled by: attr.comm | attr.mmap | attr.mmap2 | attr.mmap_data | attr.task /Users/rubber/linux/kernel/events/core.c: 7785
 * comm tracking /Users/rubber/linux/kernel/events/core.c: 7893
 * namespaces tracking /Users/rubber/linux/kernel/events/core.c: 7992
 * cgroup tracking /Users/rubber/linux/kernel/events/core.c: 8122
enomem"; /Users/rubber/linux/kernel/events/core.c: 8172
	/* /Users/rubber/linux/kernel/events/core.c: 8199
	 * Since our buffer works in 8 byte units we need to align our string /Users/rubber/linux/kernel/events/core.c: 8200
	 * size to a multiple of 8. However, we must guarantee the tail end is /Users/rubber/linux/kernel/events/core.c: 8201
	 * zero'd out to avoid leaking random bits to userspace. /Users/rubber/linux/kernel/events/core.c: 8202
 * mmap tracking /Users/rubber/linux/kernel/events/core.c: 8221
enomem"; /Users/rubber/linux/kernel/events/core.c: 8360
		/* /Users/rubber/linux/kernel/events/core.c: 8363
		 * d_path() works from the end of the rb backwards, so we /Users/rubber/linux/kernel/events/core.c: 8364
		 * need to add enough zero bytes after the string to handle /Users/rubber/linux/kernel/events/core.c: 8365
		 * the 64bit alignment we do later. /Users/rubber/linux/kernel/events/core.c: 8366
toolong"; /Users/rubber/linux/kernel/events/core.c: 8370
anon"; /Users/rubber/linux/kernel/events/core.c: 8403
	/* /Users/rubber/linux/kernel/events/core.c: 8411
	 * Since our buffer works in 8 byte units we need to align our string /Users/rubber/linux/kernel/events/core.c: 8412
	 * size to a multiple of 8. However, we must guarantee the tail end is /Users/rubber/linux/kernel/events/core.c: 8413
	 * zero'd out to avoid leaking random bits to userspace. /Users/rubber/linux/kernel/events/core.c: 8414
 * Check whether inode and address range match filter criteria. /Users/rubber/linux/kernel/events/core.c: 8445
 * Adjust all task's events' filters to the new vma /Users/rubber/linux/kernel/events/core.c: 8521
	/* /Users/rubber/linux/kernel/events/core.c: 8528
	 * Data tracing isn't supported yet and as such there is no need /Users/rubber/linux/kernel/events/core.c: 8529
	 * to keep track of anything that isn't related to executable code: /Users/rubber/linux/kernel/events/core.c: 8530
 * Lost/dropped samples logging /Users/rubber/linux/kernel/events/core.c: 8616
 * context_switch tracking /Users/rubber/linux/kernel/events/core.c: 8649
 * IRQ throttle logging /Users/rubber/linux/kernel/events/core.c: 8737
 * ksymbol register/unregister tracking /Users/rubber/linux/kernel/events/core.c: 8778
 * bpf program load/unload tracking /Users/rubber/linux/kernel/events/core.c: 8869
 * Generic event overflow handling, sampling. /Users/rubber/linux/kernel/events/core.c: 9174
	/* /Users/rubber/linux/kernel/events/core.c: 9184
	 * Non-sampling counters might still use the PMI to fold short /Users/rubber/linux/kernel/events/core.c: 9185
	 * hardware counters, ignore those. /Users/rubber/linux/kernel/events/core.c: 9186
	/* /Users/rubber/linux/kernel/events/core.c: 9193
	 * XXX event_limit might not quite work as expected on inherited /Users/rubber/linux/kernel/events/core.c: 9194
	 * events /Users/rubber/linux/kernel/events/core.c: 9195
 * Generic software event infrastructure /Users/rubber/linux/kernel/events/core.c: 9225
 * We directly increment event->count and keep a second value in /Users/rubber/linux/kernel/events/core.c: 9240
 * event->hw.period_left to count intervals. This period event /Users/rubber/linux/kernel/events/core.c: 9241
 * is kept in the range [-sample_period, 0] so that we can use the /Users/rubber/linux/kernel/events/core.c: 9242
 * sign as trigger. /Users/rubber/linux/kernel/events/core.c: 9243
			/* /Users/rubber/linux/kernel/events/core.c: 9285
			 * We inhibit the overflow from happening when /Users/rubber/linux/kernel/events/core.c: 9286
			 * hwc->interrupts == MAX_INTERRUPTS. /Users/rubber/linux/kernel/events/core.c: 9287
	/* /Users/rubber/linux/kernel/events/core.c: 9395
	 * Event scheduling is always serialized against hlist allocation /Users/rubber/linux/kernel/events/core.c: 9396
	 * and release. Which makes the protected version suitable here. /Users/rubber/linux/kernel/events/core.c: 9397
	 * The context lock guarantees that. /Users/rubber/linux/kernel/events/core.c: 9398
	/* /Users/rubber/linux/kernel/events/core.c: 9622
	 * no branch sampling for software events /Users/rubber/linux/kernel/events/core.c: 9623
	/* /Users/rubber/linux/kernel/events/core.c: 9689
	 * If exclude_kernel, only trace user-space tracepoints (uprobes) /Users/rubber/linux/kernel/events/core.c: 9690
	/* /Users/rubber/linux/kernel/events/core.c: 9742
	 * If we got specified a target task, also iterate its context and /Users/rubber/linux/kernel/events/core.c: 9743
	 * deliver this event there too. /Users/rubber/linux/kernel/events/core.c: 9744
	/* /Users/rubber/linux/kernel/events/core.c: 9785
	 * no branch sampling for tracepoint events /Users/rubber/linux/kernel/events/core.c: 9786
 * Flags in config, used by dynamic PMU kprobe and uprobe /Users/rubber/linux/kernel/events/core.c: 9813
 * The flags should match following PMU_FORMAT_ATTR(). /Users/rubber/linux/kernel/events/core.c: 9814
 * PERF_PROBE_CONFIG_IS_RETPROBE if set, create kretprobe/uretprobe /Users/rubber/linux/kernel/events/core.c: 9816
 *                               if not set, create kprobe/uprobe /Users/rubber/linux/kernel/events/core.c: 9817
 * The following values specify a reference counter (or semaphore in the /Users/rubber/linux/kernel/events/core.c: 9819
 * terminology of tools like dtrace, systemtap, etc.) Userspace Statically /Users/rubber/linux/kernel/events/core.c: 9820
 * Defined Tracepoints (USDT). Currently, we use 40 bit for the offset. /Users/rubber/linux/kernel/events/core.c: 9821
 * PERF_UPROBE_REF_CTR_OFFSET_BITS	# of bits in config as th offset /Users/rubber/linux/kernel/events/core.c: 9823
 * PERF_UPROBE_REF_CTR_OFFSET_SHIFT	# of bits to shift left /Users/rubber/linux/kernel/events/core.c: 9824
	/* /Users/rubber/linux/kernel/events/core.c: 9874
	 * no branch sampling for probe events /Users/rubber/linux/kernel/events/core.c: 9875
	/* /Users/rubber/linux/kernel/events/core.c: 9934
	 * no branch sampling for probe events /Users/rubber/linux/kernel/events/core.c: 9935
		/* /Users/rubber/linux/kernel/events/core.c: 10015
		 * On perf_event with precise_ip, calling bpf_get_stack() /Users/rubber/linux/kernel/events/core.c: 10016
		 * may trigger unwinder warnings and occasional crashes. /Users/rubber/linux/kernel/events/core.c: 10017
		 * bpf_get_[stack|stackid] works around this issue by using /Users/rubber/linux/kernel/events/core.c: 10018
		 * callchain attached to perf_sample_data. If the /Users/rubber/linux/kernel/events/core.c: 10019
		 * perf_event does not full (kernel and user) callchain /Users/rubber/linux/kernel/events/core.c: 10020
		 * attached to perf_sample_data, do not allow attaching BPF /Users/rubber/linux/kernel/events/core.c: 10021
		 * program that calls bpf_get_[stack|stackid]. /Users/rubber/linux/kernel/events/core.c: 10022
 * returns true if the event is a tracepoint, or a kprobe/upprobe created /Users/rubber/linux/kernel/events/core.c: 10058
 * with perf_event_open() /Users/rubber/linux/kernel/events/core.c: 10059
 * Allocate a new address filter /Users/rubber/linux/kernel/events/core.c: 10155
 * Free existing address filters and optionally install new ones /Users/rubber/linux/kernel/events/core.c: 10185
 * Scan through mm's vmas and see if one of them matches the /Users/rubber/linux/kernel/events/core.c: 10212
 * @filter; if so, adjust filter's address range. /Users/rubber/linux/kernel/events/core.c: 10213
 * Called with mm::mmap_lock down for reading. /Users/rubber/linux/kernel/events/core.c: 10214
 * Update event's address range filters based on the /Users/rubber/linux/kernel/events/core.c: 10232
 * task's existing mappings, if any. /Users/rubber/linux/kernel/events/core.c: 10233
	/* /Users/rubber/linux/kernel/events/core.c: 10244
	 * We may observe TASK_TOMBSTONE, which means that the event tear-down /Users/rubber/linux/kernel/events/core.c: 10245
	 * will stop on the parent's child_mutex that our caller is also holding /Users/rubber/linux/kernel/events/core.c: 10246
			/* /Users/rubber/linux/kernel/events/core.c: 10262
			 * Adjust base offset if the filter is associated to a /Users/rubber/linux/kernel/events/core.c: 10263
			 * binary that needs to be mapped: /Users/rubber/linux/kernel/events/core.c: 10264
 * Address range filtering: limiting the data to certain /Users/rubber/linux/kernel/events/core.c: 10292
 * instruction address ranges. Filters are ioctl()ed to us from /Users/rubber/linux/kernel/events/core.c: 10293
 * userspace as ascii strings. /Users/rubber/linux/kernel/events/core.c: 10294
 * Filter string format: /Users/rubber/linux/kernel/events/core.c: 10296
 * ACTION RANGE_SPEC /Users/rubber/linux/kernel/events/core.c: 10298
 * where ACTION is one of the /Users/rubber/linux/kernel/events/core.c: 10299
 *  * "filter": limit the trace to this region /Users/rubber/linux/kernel/events/core.c: 10300
 *  * "start": start tracing from this address /Users/rubber/linux/kernel/events/core.c: 10301
 *  * "stop": stop tracing at this address/region; /Users/rubber/linux/kernel/events/core.c: 10302
 * RANGE_SPEC is /Users/rubber/linux/kernel/events/core.c: 10303
 *  * for kernel addresses: <start address>[/<size>] /Users/rubber/linux/kernel/events/core.c: 10304
 *  * for object files:     <start address>[/<size>]@</path/to/object/file> /Users/rubber/linux/kernel/events/core.c: 10305
 * if <size> is not specified or is zero, the range is treated as a single /Users/rubber/linux/kernel/events/core.c: 10307
 * address; not valid for ACTION=="filter". /Users/rubber/linux/kernel/events/core.c: 10308
 * Address filter string parser /Users/rubber/linux/kernel/events/core.c: 10339
		/* /Users/rubber/linux/kernel/events/core.c: 10426
		 * Filter definition is fully parsed, validate and install it. /Users/rubber/linux/kernel/events/core.c: 10427
		 * Make sure that it doesn't contradict itself or the event's /Users/rubber/linux/kernel/events/core.c: 10428
		 * attribute. /Users/rubber/linux/kernel/events/core.c: 10429
			/* /Users/rubber/linux/kernel/events/core.c: 10436
			 * ACTION "filter" must have a non-zero length region /Users/rubber/linux/kernel/events/core.c: 10437
			 * specified. /Users/rubber/linux/kernel/events/core.c: 10438
				/* /Users/rubber/linux/kernel/events/core.c: 10448
				 * For now, we only support file-based filters /Users/rubber/linux/kernel/events/core.c: 10449
				 * in per-task events; doing so for CPU-wide /Users/rubber/linux/kernel/events/core.c: 10450
				 * events requires additional context switching /Users/rubber/linux/kernel/events/core.c: 10451
				 * trickery, since same object code will be /Users/rubber/linux/kernel/events/core.c: 10452
				 * mapped at different virtual addresses in /Users/rubber/linux/kernel/events/core.c: 10453
				 * different processes. /Users/rubber/linux/kernel/events/core.c: 10454
	/* /Users/rubber/linux/kernel/events/core.c: 10503
	 * Since this is called in perf_ioctl() path, we're already holding /Users/rubber/linux/kernel/events/core.c: 10504
	 * ctx::mutex. /Users/rubber/linux/kernel/events/core.c: 10505
		/* /Users/rubber/linux/kernel/events/core.c: 10550
		 * Beware, here be dragons!! /Users/rubber/linux/kernel/events/core.c: 10551
		 * /Users/rubber/linux/kernel/events/core.c: 10552
		 * the tracepoint muck will deadlock against ctx->mutex, but /Users/rubber/linux/kernel/events/core.c: 10553
		 * the tracepoint stuff does not actually need it. So /Users/rubber/linux/kernel/events/core.c: 10554
		 * temporarily drop ctx->mutex. As per perf_event_ctx_lock() we /Users/rubber/linux/kernel/events/core.c: 10555
		 * already have a reference on ctx. /Users/rubber/linux/kernel/events/core.c: 10556
		 * /Users/rubber/linux/kernel/events/core.c: 10557
		 * This can result in event getting moved to a different ctx, /Users/rubber/linux/kernel/events/core.c: 10558
		 * but that does not affect the tracepoint state. /Users/rubber/linux/kernel/events/core.c: 10559
 * hrtimer based swevent callback /Users/rubber/linux/kernel/events/core.c: 10574
	/* /Users/rubber/linux/kernel/events/core.c: 10650
	 * Since hrtimers have a fixed rate, we can do a static freq->period /Users/rubber/linux/kernel/events/core.c: 10651
	 * mapping and avoid the whole period adjust feedback stuff. /Users/rubber/linux/kernel/events/core.c: 10652
 * Software event: cpu wall time clock /Users/rubber/linux/kernel/events/core.c: 10666
	/* /Users/rubber/linux/kernel/events/core.c: 10718
	 * no branch sampling for software events /Users/rubber/linux/kernel/events/core.c: 10719
 * Software event: task time clock /Users/rubber/linux/kernel/events/core.c: 10743
	/* /Users/rubber/linux/kernel/events/core.c: 10799
	 * no branch sampling for software events /Users/rubber/linux/kernel/events/core.c: 10800
 * Ensures all contexts with the same task_ctx_nr have the same /Users/rubber/linux/kernel/events/core.c: 10884
 * pmu_cpu_context too. /Users/rubber/linux/kernel/events/core.c: 10885
	/* /Users/rubber/linux/kernel/events/core.c: 10904
	 * Static contexts such as perf_sw_context have a global lifetime /Users/rubber/linux/kernel/events/core.c: 10905
	 * and may be shared between different PMUs. Avoid freeing them /Users/rubber/linux/kernel/events/core.c: 10906
	 * when a single PMU is going away. /Users/rubber/linux/kernel/events/core.c: 10907
 * Let userspace know that this PMU supports address range filtering: /Users/rubber/linux/kernel/events/core.c: 10916
		/* /Users/rubber/linux/kernel/events/core.c: 11095
		 * Other than systems with heterogeneous CPUs, it never makes /Users/rubber/linux/kernel/events/core.c: 11096
		 * sense for two PMUs to share perf_hw_context. PMUs which are /Users/rubber/linux/kernel/events/core.c: 11097
		 * uncore must use perf_invalid_context. /Users/rubber/linux/kernel/events/core.c: 11098
			/* /Users/rubber/linux/kernel/events/core.c: 11135
			 * If we have pmu_enable/pmu_disable calls, install /Users/rubber/linux/kernel/events/core.c: 11136
			 * transaction stubs that use that to try and batch /Users/rubber/linux/kernel/events/core.c: 11137
			 * hardware accesses. /Users/rubber/linux/kernel/events/core.c: 11138
	/* /Users/rubber/linux/kernel/events/core.c: 11161
	 * Ensure the TYPE_SOFTWARE PMUs are at the head of the list, /Users/rubber/linux/kernel/events/core.c: 11162
	 * since these cannot be in the IDR. This way the linear search /Users/rubber/linux/kernel/events/core.c: 11163
	 * is fast, provided a valid software event is provided. /Users/rubber/linux/kernel/events/core.c: 11164
	/* /Users/rubber/linux/kernel/events/core.c: 11197
	 * We dereference the pmu list under both SRCU and regular RCU, so /Users/rubber/linux/kernel/events/core.c: 11198
	 * synchronize against both of those. /Users/rubber/linux/kernel/events/core.c: 11199
	/* /Users/rubber/linux/kernel/events/core.c: 11232
	 * A number of pmu->event_init() methods iterate the sibling_list to, /Users/rubber/linux/kernel/events/core.c: 11233
	 * for example, validate if the group fits on the PMU. Therefore, /Users/rubber/linux/kernel/events/core.c: 11234
	 * if this is a sibling event, acquire the ctx->mutex to protect /Users/rubber/linux/kernel/events/core.c: 11235
	 * the sibling_list. /Users/rubber/linux/kernel/events/core.c: 11236
		/* /Users/rubber/linux/kernel/events/core.c: 11239
		 * This ctx->mutex can nest when we're called through /Users/rubber/linux/kernel/events/core.c: 11240
		 * inheritance. See the perf_event_ctx_lock_nested() comment. /Users/rubber/linux/kernel/events/core.c: 11241
	/* /Users/rubber/linux/kernel/events/core.c: 11289
	 * PERF_TYPE_HARDWARE and PERF_TYPE_HW_CACHE /Users/rubber/linux/kernel/events/core.c: 11290
	 * are often aliases for PERF_TYPE_RAW. /Users/rubber/linux/kernel/events/core.c: 11291
 * We keep a list of all !task (and therefore per-cpu) events /Users/rubber/linux/kernel/events/core.c: 11353
 * that need to receive side-band records. /Users/rubber/linux/kernel/events/core.c: 11354
 * This avoids having to scan all the various PMU per-cpu contexts /Users/rubber/linux/kernel/events/core.c: 11356
 * looking for them. /Users/rubber/linux/kernel/events/core.c: 11357
		/* /Users/rubber/linux/kernel/events/core.c: 11434
		 * We need the mutex here because static_branch_enable() /Users/rubber/linux/kernel/events/core.c: 11435
		 * must complete *before* the perf_sched_count increment /Users/rubber/linux/kernel/events/core.c: 11436
		 * becomes visible. /Users/rubber/linux/kernel/events/core.c: 11437
			/* /Users/rubber/linux/kernel/events/core.c: 11445
			 * Guarantee that all CPUs observe they key change and /Users/rubber/linux/kernel/events/core.c: 11446
			 * call the perf scheduling hooks before proceeding to /Users/rubber/linux/kernel/events/core.c: 11447
			 * install events that need them. /Users/rubber/linux/kernel/events/core.c: 11448
		/* /Users/rubber/linux/kernel/events/core.c: 11452
		 * Now that we have waited for the sync_sched(), allow further /Users/rubber/linux/kernel/events/core.c: 11453
		 * increments to by-pass the mutex. /Users/rubber/linux/kernel/events/core.c: 11454
 * Allocate and initialize an event structure /Users/rubber/linux/kernel/events/core.c: 11467
	/* /Users/rubber/linux/kernel/events/core.c: 11498
	 * Single events are their own group leaders, with an /Users/rubber/linux/kernel/events/core.c: 11499
	 * empty sibling list: /Users/rubber/linux/kernel/events/core.c: 11500
		/* /Users/rubber/linux/kernel/events/core.c: 11544
		 * XXX pmu::event_init needs to know what task to account to /Users/rubber/linux/kernel/events/core.c: 11545
		 * and we cannot use the ctx information because we need the /Users/rubber/linux/kernel/events/core.c: 11546
		 * pmu before we get a ctx. /Users/rubber/linux/kernel/events/core.c: 11547
	/* /Users/rubber/linux/kernel/events/core.c: 11594
	 * We currently do not support PERF_SAMPLE_READ on inherited events. /Users/rubber/linux/kernel/events/core.c: 11595
	 * See perf_output_read(). /Users/rubber/linux/kernel/events/core.c: 11596
	/* /Users/rubber/linux/kernel/events/core.c: 11610
	 * Disallow uncore-cgroup events, they don't make sense as the cgroup will /Users/rubber/linux/kernel/events/core.c: 11611
	 * be different on other CPUs in the uncore mask. /Users/rubber/linux/kernel/events/core.c: 11612
		/* /Users/rubber/linux/kernel/events/core.c: 11644
		 * Clone the parent's vma offsets: they are valid until exec() /Users/rubber/linux/kernel/events/core.c: 11645
		 * even if the mm is not shared with the parent. /Users/rubber/linux/kernel/events/core.c: 11646
			/* /Users/rubber/linux/kernel/events/core.c: 11766
			 * adjust user setting (for HW filter setup) /Users/rubber/linux/kernel/events/core.c: 11767
		/* /Users/rubber/linux/kernel/events/core.c: 11789
		 * We have __u32 type for the size, but so far /Users/rubber/linux/kernel/events/core.c: 11790
		 * we can only use __u16 as maximum due to the /Users/rubber/linux/kernel/events/core.c: 11791
		 * __u16 sample size limit. /Users/rubber/linux/kernel/events/core.c: 11792
	/* /Users/rubber/linux/kernel/events/core.c: 11845
	 * Don't allow cross-cpu buffers /Users/rubber/linux/kernel/events/core.c: 11846
	/* /Users/rubber/linux/kernel/events/core.c: 11851
	 * If its not a per-cpu rb, it must be the same task. /Users/rubber/linux/kernel/events/core.c: 11852
	/* /Users/rubber/linux/kernel/events/core.c: 11857
	 * Mixing clocks in the same buffer is trouble you don't need. /Users/rubber/linux/kernel/events/core.c: 11858
	/* /Users/rubber/linux/kernel/events/core.c: 11863
	 * Either writing ring buffer from beginning or from end. /Users/rubber/linux/kernel/events/core.c: 11864
	 * Mixing is not allowed. /Users/rubber/linux/kernel/events/core.c: 11865
	/* /Users/rubber/linux/kernel/events/core.c: 11870
	 * If both events generate aux data, they must be on the same PMU /Users/rubber/linux/kernel/events/core.c: 11871
 * Variation on perf_event_ctx_lock_nested(), except we take two context /Users/rubber/linux/kernel/events/core.c: 11947
 * mutexes. /Users/rubber/linux/kernel/events/core.c: 11948
		/* /Users/rubber/linux/kernel/events/core.c: 11984
		 * perf_event_attr::sigtrap sends signals to the other task. /Users/rubber/linux/kernel/events/core.c: 11985
		 * Require the current task to also have CAP_KILL. /Users/rubber/linux/kernel/events/core.c: 11986
		/* /Users/rubber/linux/kernel/events/core.c: 11992
		 * If the required capabilities aren't available, checks for /Users/rubber/linux/kernel/events/core.c: 11993
		 * ptrace permissions: upgrade to ATTACH, since sending signals /Users/rubber/linux/kernel/events/core.c: 11994
		 * can effectively change the target task. /Users/rubber/linux/kernel/events/core.c: 11995
	/* /Users/rubber/linux/kernel/events/core.c: 12000
	 * Preserve ptrace permission check for backwards compatibility. The /Users/rubber/linux/kernel/events/core.c: 12001
	 * ptrace check also includes checks that the current task and other /Users/rubber/linux/kernel/events/core.c: 12002
	 * task have matching uids, and is therefore not done here explicitly. /Users/rubber/linux/kernel/events/core.c: 12003
 * sys_perf_event_open - open a performance event, associate it to a task/cpu /Users/rubber/linux/kernel/events/core.c: 12009
 * @attr_uptr:	event_id type attributes for monitoring/sampling /Users/rubber/linux/kernel/events/core.c: 12011
 * @pid:		target pid /Users/rubber/linux/kernel/events/core.c: 12012
 * @cpu:		target cpu /Users/rubber/linux/kernel/events/core.c: 12013
 * @group_fd:		group leader event fd /Users/rubber/linux/kernel/events/core.c: 12014
 * @flags:		perf event open flags /Users/rubber/linux/kernel/events/core.c: 12015
	/* /Users/rubber/linux/kernel/events/core.c: 12081
	 * In cgroup mode, the pid argument is used to pass the fd /Users/rubber/linux/kernel/events/core.c: 12082
	 * opened to the cgroup directory in cgroupfs. The cpu argument /Users/rubber/linux/kernel/events/core.c: 12083
	 * designates the cpu on which to monitor threads from that /Users/rubber/linux/kernel/events/core.c: 12084
	 * cgroup. /Users/rubber/linux/kernel/events/core.c: 12085
	/* /Users/rubber/linux/kernel/events/core.c: 12139
	 * Special case software events and allow them to be part of /Users/rubber/linux/kernel/events/core.c: 12140
	 * any hardware group. /Users/rubber/linux/kernel/events/core.c: 12141
			/* /Users/rubber/linux/kernel/events/core.c: 12157
			 * If the event is a sw event, but the group_leader /Users/rubber/linux/kernel/events/core.c: 12158
			 * is on hw context. /Users/rubber/linux/kernel/events/core.c: 12159
			 * /Users/rubber/linux/kernel/events/core.c: 12160
			 * Allow the addition of software events to hw /Users/rubber/linux/kernel/events/core.c: 12161
			 * groups, this is safe because software events /Users/rubber/linux/kernel/events/core.c: 12162
			 * never fail to schedule. /Users/rubber/linux/kernel/events/core.c: 12163
			/* /Users/rubber/linux/kernel/events/core.c: 12169
			 * In case the group is a pure software group, and we /Users/rubber/linux/kernel/events/core.c: 12170
			 * try to add a hardware event, move the whole group to /Users/rubber/linux/kernel/events/core.c: 12171
			 * the hardware context. /Users/rubber/linux/kernel/events/core.c: 12172
	/* /Users/rubber/linux/kernel/events/core.c: 12178
	 * Get the target context (task or percpu): /Users/rubber/linux/kernel/events/core.c: 12179
	/* /Users/rubber/linux/kernel/events/core.c: 12187
	 * Look up the group leader (we will attach this event to it): /Users/rubber/linux/kernel/events/core.c: 12188
		/* /Users/rubber/linux/kernel/events/core.c: 12193
		 * Do not allow a recursive hierarchy (this new sibling /Users/rubber/linux/kernel/events/core.c: 12194
		 * becoming part of another group-sibling): /Users/rubber/linux/kernel/events/core.c: 12195
		/* /Users/rubber/linux/kernel/events/core.c: 12204
		 * Make sure we're both events for the same CPU; /Users/rubber/linux/kernel/events/core.c: 12205
		 * grouping events for different CPUs is broken; since /Users/rubber/linux/kernel/events/core.c: 12206
		 * you can never concurrently schedule them anyhow. /Users/rubber/linux/kernel/events/core.c: 12207
		/* /Users/rubber/linux/kernel/events/core.c: 12212
		 * Make sure we're both on the same task, or both /Users/rubber/linux/kernel/events/core.c: 12213
		 * per-CPU events. /Users/rubber/linux/kernel/events/core.c: 12214
		/* /Users/rubber/linux/kernel/events/core.c: 12219
		 * Do not allow to attach to a group in a different task /Users/rubber/linux/kernel/events/core.c: 12220
		 * or CPU context. If we're moving SW events, we'll fix /Users/rubber/linux/kernel/events/core.c: 12221
		 * this up later, so allow that. /Users/rubber/linux/kernel/events/core.c: 12222
		/* /Users/rubber/linux/kernel/events/core.c: 12227
		 * Only a group leader can be exclusive or pinned /Users/rubber/linux/kernel/events/core.c: 12228
		/* /Users/rubber/linux/kernel/events/core.c: 12253
		 * We must hold exec_update_lock across this and any potential /Users/rubber/linux/kernel/events/core.c: 12254
		 * perf_install_in_context() call for this new event to /Users/rubber/linux/kernel/events/core.c: 12255
		 * serialize against exec() altering our credentials (and the /Users/rubber/linux/kernel/events/core.c: 12256
		 * perf_event_exit_task() that could imply). /Users/rubber/linux/kernel/events/core.c: 12257
		/* /Users/rubber/linux/kernel/events/core.c: 12272
		 * Check if we raced against another sys_perf_event_open() call /Users/rubber/linux/kernel/events/core.c: 12273
		 * moving the software group underneath us. /Users/rubber/linux/kernel/events/core.c: 12274
			/* /Users/rubber/linux/kernel/events/core.c: 12277
			 * If someone moved the group out from under us, check /Users/rubber/linux/kernel/events/core.c: 12278
			 * if this new event wound up on the same ctx, if so /Users/rubber/linux/kernel/events/core.c: 12279
			 * its the regular !move_group case, otherwise fail. /Users/rubber/linux/kernel/events/core.c: 12280
		/* /Users/rubber/linux/kernel/events/core.c: 12291
		 * Failure to create exclusive events returns -EBUSY. /Users/rubber/linux/kernel/events/core.c: 12292
		/* /Users/rubber/linux/kernel/events/core.c: 12317
		 * Check if the @cpu we're creating an event for is online. /Users/rubber/linux/kernel/events/core.c: 12318
		 * /Users/rubber/linux/kernel/events/core.c: 12319
		 * We use the perf_cpu_context::ctx::mutex to serialize against /Users/rubber/linux/kernel/events/core.c: 12320
		 * the hotplug notifiers. See perf_event_{init,exit}_cpu(). /Users/rubber/linux/kernel/events/core.c: 12321
	/* /Users/rubber/linux/kernel/events/core.c: 12337
	 * Must be under the same ctx::mutex as perf_install_in_context(), /Users/rubber/linux/kernel/events/core.c: 12338
	 * because we need to serialize with concurrent event creation. /Users/rubber/linux/kernel/events/core.c: 12339
	/* /Users/rubber/linux/kernel/events/core.c: 12348
	 * This is the point on no return; we cannot fail hereafter. This is /Users/rubber/linux/kernel/events/core.c: 12349
	 * where we start modifying current state. /Users/rubber/linux/kernel/events/core.c: 12350
		/* /Users/rubber/linux/kernel/events/core.c: 12354
		 * See perf_event_ctx_lock() for comments on the details /Users/rubber/linux/kernel/events/core.c: 12355
		 * of swizzling perf_event::ctx. /Users/rubber/linux/kernel/events/core.c: 12356
		/* /Users/rubber/linux/kernel/events/core.c: 12366
		 * Wait for everybody to stop referencing the events through /Users/rubber/linux/kernel/events/core.c: 12367
		 * the old lists, before installing it on new lists. /Users/rubber/linux/kernel/events/core.c: 12368
		/* /Users/rubber/linux/kernel/events/core.c: 12372
		 * Install the group siblings before the group leader. /Users/rubber/linux/kernel/events/core.c: 12373
		 * /Users/rubber/linux/kernel/events/core.c: 12374
		 * Because a group leader will try and install the entire group /Users/rubber/linux/kernel/events/core.c: 12375
		 * (through the sibling list, which is still in-tact), we can /Users/rubber/linux/kernel/events/core.c: 12376
		 * end up with siblings installed in the wrong context. /Users/rubber/linux/kernel/events/core.c: 12377
		 * /Users/rubber/linux/kernel/events/core.c: 12378
		 * By installing siblings first we NO-OP because they're not /Users/rubber/linux/kernel/events/core.c: 12379
		 * reachable through the group lists. /Users/rubber/linux/kernel/events/core.c: 12380
		/* /Users/rubber/linux/kernel/events/core.c: 12388
		 * Removing from the context ends up with disabled /Users/rubber/linux/kernel/events/core.c: 12389
		 * event. What we want here is event in the initial /Users/rubber/linux/kernel/events/core.c: 12390
		 * startup state, ready to be add into new context. /Users/rubber/linux/kernel/events/core.c: 12391
	/* /Users/rubber/linux/kernel/events/core.c: 12398
	 * Precalculate sample_data sizes; do while holding ctx::mutex such /Users/rubber/linux/kernel/events/core.c: 12399
	 * that we're serialized against further additions and before /Users/rubber/linux/kernel/events/core.c: 12400
	 * perf_install_in_context() which is the point the event is active and /Users/rubber/linux/kernel/events/core.c: 12401
	 * can use these values. /Users/rubber/linux/kernel/events/core.c: 12402
	/* /Users/rubber/linux/kernel/events/core.c: 12425
	 * Drop the reference on the group_event after placing the /Users/rubber/linux/kernel/events/core.c: 12426
	 * new event on the sibling_list. This ensures destruction /Users/rubber/linux/kernel/events/core.c: 12427
	 * of the group leader will find the pointer to itself in /Users/rubber/linux/kernel/events/core.c: 12428
	 * perf_group_detach(). /Users/rubber/linux/kernel/events/core.c: 12429
	/* /Users/rubber/linux/kernel/events/core.c: 12448
	 * If event_file is set, the fput() above will have called ->release() /Users/rubber/linux/kernel/events/core.c: 12449
	 * and that will take care of freeing the event. /Users/rubber/linux/kernel/events/core.c: 12450
 * perf_event_create_kernel_counter /Users/rubber/linux/kernel/events/core.c: 12465
 * @attr: attributes of the counter to create /Users/rubber/linux/kernel/events/core.c: 12467
 * @cpu: cpu in which the counter is bound /Users/rubber/linux/kernel/events/core.c: 12468
 * @task: task to profile (NULL for percpu) /Users/rubber/linux/kernel/events/core.c: 12469
 * @overflow_handler: callback to trigger when we hit the event /Users/rubber/linux/kernel/events/core.c: 12470
 * @context: context data could be used in overflow_handler callback /Users/rubber/linux/kernel/events/core.c: 12471
	/* /Users/rubber/linux/kernel/events/core.c: 12483
	 * Grouping is not supported for kernel events, neither is 'AUX', /Users/rubber/linux/kernel/events/core.c: 12484
	 * make sure the caller's intentions are adjusted. /Users/rubber/linux/kernel/events/core.c: 12485
	/* /Users/rubber/linux/kernel/events/core.c: 12500
	 * Get the target context (task or percpu): /Users/rubber/linux/kernel/events/core.c: 12501
		/* /Users/rubber/linux/kernel/events/core.c: 12517
		 * Check if the @cpu we're creating an event for is online. /Users/rubber/linux/kernel/events/core.c: 12518
		 * /Users/rubber/linux/kernel/events/core.c: 12519
		 * We use the perf_cpu_context::ctx::mutex to serialize against /Users/rubber/linux/kernel/events/core.c: 12520
		 * the hotplug notifiers. See perf_event_{init,exit}_cpu(). /Users/rubber/linux/kernel/events/core.c: 12521
	/* /Users/rubber/linux/kernel/events/core.c: 12563
	 * See perf_event_ctx_lock() for comments on the details /Users/rubber/linux/kernel/events/core.c: 12564
	 * of swizzling perf_event::ctx. /Users/rubber/linux/kernel/events/core.c: 12565
	/* /Users/rubber/linux/kernel/events/core.c: 12576
	 * Wait for the events to quiesce before re-instating them. /Users/rubber/linux/kernel/events/core.c: 12577
	/* /Users/rubber/linux/kernel/events/core.c: 12581
	 * Re-instate events in 2 passes. /Users/rubber/linux/kernel/events/core.c: 12582
	 * /Users/rubber/linux/kernel/events/core.c: 12583
	 * Skip over group leaders and only install siblings on this first /Users/rubber/linux/kernel/events/core.c: 12584
	 * pass, siblings will not get enabled without a leader, however a /Users/rubber/linux/kernel/events/core.c: 12585
	 * leader will enable its siblings, even if those are still on the old /Users/rubber/linux/kernel/events/core.c: 12586
	 * context. /Users/rubber/linux/kernel/events/core.c: 12587
	/* /Users/rubber/linux/kernel/events/core.c: 12601
	 * Once all the siblings are setup properly, install the group leaders /Users/rubber/linux/kernel/events/core.c: 12602
	 * to make it go. /Users/rubber/linux/kernel/events/core.c: 12603
	/* /Users/rubber/linux/kernel/events/core.c: 12632
	 * Add back the child's count to the parent's count: /Users/rubber/linux/kernel/events/core.c: 12633
		/* /Users/rubber/linux/kernel/events/core.c: 12649
		 * Do not destroy the 'original' grouping; because of the /Users/rubber/linux/kernel/events/core.c: 12650
		 * context switch optimization the original events could've /Users/rubber/linux/kernel/events/core.c: 12651
		 * ended up in a random child task. /Users/rubber/linux/kernel/events/core.c: 12652
		 * /Users/rubber/linux/kernel/events/core.c: 12653
		 * If we were to destroy the original group, all group related /Users/rubber/linux/kernel/events/core.c: 12654
		 * operations would cease to function properly after this /Users/rubber/linux/kernel/events/core.c: 12655
		 * random child dies. /Users/rubber/linux/kernel/events/core.c: 12656
		 * /Users/rubber/linux/kernel/events/core.c: 12657
		 * Do destroy all inherited groups, we don't care about those /Users/rubber/linux/kernel/events/core.c: 12658
		 * and being thorough is better. /Users/rubber/linux/kernel/events/core.c: 12659
	/* /Users/rubber/linux/kernel/events/core.c: 12672
	 * Child events can be freed. /Users/rubber/linux/kernel/events/core.c: 12673
		/* /Users/rubber/linux/kernel/events/core.c: 12677
		 * Kick perf_poll() for is_event_hup(); /Users/rubber/linux/kernel/events/core.c: 12678
	/* /Users/rubber/linux/kernel/events/core.c: 12686
	 * Parent events are governed by their filedesc, retain them. /Users/rubber/linux/kernel/events/core.c: 12687
	/* /Users/rubber/linux/kernel/events/core.c: 12703
	 * In order to reduce the amount of tricky in ctx tear-down, we hold /Users/rubber/linux/kernel/events/core.c: 12704
	 * ctx::mutex over the entire thing. This serializes against almost /Users/rubber/linux/kernel/events/core.c: 12705
	 * everything that wants to access the ctx. /Users/rubber/linux/kernel/events/core.c: 12706
	 * /Users/rubber/linux/kernel/events/core.c: 12707
	 * The exception is sys_perf_event_open() / /Users/rubber/linux/kernel/events/core.c: 12708
	 * perf_event_create_kernel_count() which does find_get_context() /Users/rubber/linux/kernel/events/core.c: 12709
	 * without ctx::mutex (it cannot because of the move_group double mutex /Users/rubber/linux/kernel/events/core.c: 12710
	 * lock thing). See the comments in perf_install_in_context(). /Users/rubber/linux/kernel/events/core.c: 12711
	/* /Users/rubber/linux/kernel/events/core.c: 12715
	 * In a single ctx::lock section, de-schedule the events and detach the /Users/rubber/linux/kernel/events/core.c: 12716
	 * context from the task such that we cannot ever get it scheduled back /Users/rubber/linux/kernel/events/core.c: 12717
	 * in. /Users/rubber/linux/kernel/events/core.c: 12718
	/* /Users/rubber/linux/kernel/events/core.c: 12723
	 * Now that the context is inactive, destroy the task <-> ctx relation /Users/rubber/linux/kernel/events/core.c: 12724
	 * and mark the context dead. /Users/rubber/linux/kernel/events/core.c: 12725
	/* /Users/rubber/linux/kernel/events/core.c: 12738
	 * Report the task dead after unscheduling the events so that we /Users/rubber/linux/kernel/events/core.c: 12739
	 * won't get any samples after PERF_RECORD_EXIT. We can however still /Users/rubber/linux/kernel/events/core.c: 12740
	 * get a few PERF_RECORD_READ events. /Users/rubber/linux/kernel/events/core.c: 12741
 * When a child task exits, feed back event values to parent events. /Users/rubber/linux/kernel/events/core.c: 12754
 * Can be called with exec_update_lock held when called from /Users/rubber/linux/kernel/events/core.c: 12756
 * setup_new_exec(). /Users/rubber/linux/kernel/events/core.c: 12757
		/* /Users/rubber/linux/kernel/events/core.c: 12769
		 * Ensure the list deletion is visible before we clear /Users/rubber/linux/kernel/events/core.c: 12770
		 * the owner, closes a race against perf_release() where /Users/rubber/linux/kernel/events/core.c: 12771
		 * we need to serialize on the owner->perf_event_mutex. /Users/rubber/linux/kernel/events/core.c: 12772
	/* /Users/rubber/linux/kernel/events/core.c: 12781
	 * The perf_event_exit_task_context calls perf_event_task /Users/rubber/linux/kernel/events/core.c: 12782
	 * with child's task_ctx, which generates EXIT events for /Users/rubber/linux/kernel/events/core.c: 12783
	 * child contexts and sets child->perf_event_ctxp[] to NULL. /Users/rubber/linux/kernel/events/core.c: 12784
	 * At this point we need to send EXIT events to cpu contexts. /Users/rubber/linux/kernel/events/core.c: 12785
 * Free a context as created by inheritance by perf_event_init_task() below, /Users/rubber/linux/kernel/events/core.c: 12812
 * used by fork() in case of fail. /Users/rubber/linux/kernel/events/core.c: 12813
 * Even though the task has never lived, the context and events have been /Users/rubber/linux/kernel/events/core.c: 12815
 * exposed through the child_list, so we must take care tearing it all down. /Users/rubber/linux/kernel/events/core.c: 12816
		/* /Users/rubber/linux/kernel/events/core.c: 12831
		 * Destroy the task <-> ctx relation and mark the context dead. /Users/rubber/linux/kernel/events/core.c: 12832
		 * /Users/rubber/linux/kernel/events/core.c: 12833
		 * This is important because even though the task hasn't been /Users/rubber/linux/kernel/events/core.c: 12834
		 * exposed yet the context has been (through child_list). /Users/rubber/linux/kernel/events/core.c: 12835
		/* /Users/rubber/linux/kernel/events/core.c: 12847
		 * perf_event_release_kernel() could've stolen some of our /Users/rubber/linux/kernel/events/core.c: 12848
		 * child events and still have them on its free_list. In that /Users/rubber/linux/kernel/events/core.c: 12849
		 * case we must wait for these events to have been freed (in /Users/rubber/linux/kernel/events/core.c: 12850
		 * particular all their references to this task must've been /Users/rubber/linux/kernel/events/core.c: 12851
		 * dropped). /Users/rubber/linux/kernel/events/core.c: 12852
		 * /Users/rubber/linux/kernel/events/core.c: 12853
		 * Without this copy_process() will unconditionally free this /Users/rubber/linux/kernel/events/core.c: 12854
		 * task (irrespective of its reference count) and /Users/rubber/linux/kernel/events/core.c: 12855
		 * _free_event()'s put_task_struct(event->hw.target) will be a /Users/rubber/linux/kernel/events/core.c: 12856
		 * use-after-free. /Users/rubber/linux/kernel/events/core.c: 12857
		 * /Users/rubber/linux/kernel/events/core.c: 12858
		 * Wait for all events to drop their context reference. /Users/rubber/linux/kernel/events/core.c: 12859
 * Inherit an event from parent task to child task. /Users/rubber/linux/kernel/events/core.c: 12905
 * Returns: /Users/rubber/linux/kernel/events/core.c: 12907
 *  - valid pointer on success /Users/rubber/linux/kernel/events/core.c: 12908
 *  - NULL for orphaned events /Users/rubber/linux/kernel/events/core.c: 12909
 *  - IS_ERR() on error /Users/rubber/linux/kernel/events/core.c: 12910
	/* /Users/rubber/linux/kernel/events/core.c: 12924
	 * Instead of creating recursive hierarchies of events, /Users/rubber/linux/kernel/events/core.c: 12925
	 * we link inherited events back to the original parent, /Users/rubber/linux/kernel/events/core.c: 12926
	 * which has a filp for sure, which we use as the reference /Users/rubber/linux/kernel/events/core.c: 12927
	 * count: /Users/rubber/linux/kernel/events/core.c: 12928
	/* /Users/rubber/linux/kernel/events/core.c: 12953
	 * is_orphaned_event() and list_add_tail(&parent_event->child_list) /Users/rubber/linux/kernel/events/core.c: 12954
	 * must be under the same lock in order to serialize against /Users/rubber/linux/kernel/events/core.c: 12955
	 * perf_event_release_kernel(), such that either we must observe /Users/rubber/linux/kernel/events/core.c: 12956
	 * is_orphaned_event() or they will observe us on the child_list. /Users/rubber/linux/kernel/events/core.c: 12957
	/* /Users/rubber/linux/kernel/events/core.c: 12970
	 * Make the child state follow the state of the parent event, /Users/rubber/linux/kernel/events/core.c: 12971
	 * not its attr.disabled bit.  We hold the parent's mutex, /Users/rubber/linux/kernel/events/core.c: 12972
	 * so we won't race with perf_event_{en, dis}able_family. /Users/rubber/linux/kernel/events/core.c: 12973
	/* /Users/rubber/linux/kernel/events/core.c: 12995
	 * Precalculate sample_data sizes /Users/rubber/linux/kernel/events/core.c: 12996
	/* /Users/rubber/linux/kernel/events/core.c: 13001
	 * Link it up in the child's context: /Users/rubber/linux/kernel/events/core.c: 13002
	/* /Users/rubber/linux/kernel/events/core.c: 13009
	 * Link this into the parent event's child list /Users/rubber/linux/kernel/events/core.c: 13010
 * Inherits an event group. /Users/rubber/linux/kernel/events/core.c: 13019
 * This will quietly suppress orphaned events; !inherit_event() is not an error. /Users/rubber/linux/kernel/events/core.c: 13021
 * This matches with perf_event_release_kernel() removing all child events. /Users/rubber/linux/kernel/events/core.c: 13022
 * Returns: /Users/rubber/linux/kernel/events/core.c: 13024
 *  - 0 on success /Users/rubber/linux/kernel/events/core.c: 13025
 *  - <0 on error /Users/rubber/linux/kernel/events/core.c: 13026
	/* /Users/rubber/linux/kernel/events/core.c: 13042
	 * @leader can be NULL here because of is_orphaned_event(). In this /Users/rubber/linux/kernel/events/core.c: 13043
	 * case inherit_event() will create individual events, similar to what /Users/rubber/linux/kernel/events/core.c: 13044
	 * perf_group_detach() would do anyway. /Users/rubber/linux/kernel/events/core.c: 13045
 * Creates the child task context and tries to inherit the event-group. /Users/rubber/linux/kernel/events/core.c: 13061
 * Clears @inherited_all on !attr.inherited or error. Note that we'll leave /Users/rubber/linux/kernel/events/core.c: 13063
 * inherited_all set when we 'fail' to inherit an orphaned event; this is /Users/rubber/linux/kernel/events/core.c: 13064
 * consistent with perf_event_release_kernel() removing all child events. /Users/rubber/linux/kernel/events/core.c: 13065
 * Returns: /Users/rubber/linux/kernel/events/core.c: 13067
 *  - 0 on success /Users/rubber/linux/kernel/events/core.c: 13068
 *  - <0 on error /Users/rubber/linux/kernel/events/core.c: 13069
		/* /Users/rubber/linux/kernel/events/core.c: 13090
		 * This is executed from the parent task context, so /Users/rubber/linux/kernel/events/core.c: 13091
		 * inherit events that have been marked for cloning. /Users/rubber/linux/kernel/events/core.c: 13092
		 * First allocate and initialize a context for the /Users/rubber/linux/kernel/events/core.c: 13093
		 * child. /Users/rubber/linux/kernel/events/core.c: 13094
 * Initialize the perf_event context in task_struct /Users/rubber/linux/kernel/events/core.c: 13113
	/* /Users/rubber/linux/kernel/events/core.c: 13129
	 * If the parent's context is a clone, pin it so it won't get /Users/rubber/linux/kernel/events/core.c: 13130
	 * swapped under us. /Users/rubber/linux/kernel/events/core.c: 13131
	/* /Users/rubber/linux/kernel/events/core.c: 13137
	 * No need to check if parent_ctx != NULL here; since we saw /Users/rubber/linux/kernel/events/core.c: 13138
	 * it non-NULL earlier, the only reason for it to become NULL /Users/rubber/linux/kernel/events/core.c: 13139
	 * is if we exit, and since we're currently in the middle of /Users/rubber/linux/kernel/events/core.c: 13140
	 * a fork we can't be exiting at the same time. /Users/rubber/linux/kernel/events/core.c: 13141
	/* /Users/rubber/linux/kernel/events/core.c: 13144
	 * Lock the parent list. No need to lock the child - not PID /Users/rubber/linux/kernel/events/core.c: 13145
	 * hashed yet and not running, so nobody can access it. /Users/rubber/linux/kernel/events/core.c: 13146
	/* /Users/rubber/linux/kernel/events/core.c: 13150
	 * We dont have to disable NMIs - we are only looking at /Users/rubber/linux/kernel/events/core.c: 13151
	 * the list, not manipulating it: /Users/rubber/linux/kernel/events/core.c: 13152
	/* /Users/rubber/linux/kernel/events/core.c: 13162
	 * We can't hold ctx->lock when iterating the ->flexible_group list due /Users/rubber/linux/kernel/events/core.c: 13163
	 * to allocations, but we need to prevent rotation because /Users/rubber/linux/kernel/events/core.c: 13164
	 * rotate_ctx() will change the list from interrupt context. /Users/rubber/linux/kernel/events/core.c: 13165
		/* /Users/rubber/linux/kernel/events/core.c: 13185
		 * Mark the child context as a clone of the parent /Users/rubber/linux/kernel/events/core.c: 13186
		 * context, or of whatever the parent is a clone of. /Users/rubber/linux/kernel/events/core.c: 13187
		 * /Users/rubber/linux/kernel/events/core.c: 13188
		 * Note that if the parent is a clone, the holding of /Users/rubber/linux/kernel/events/core.c: 13189
		 * parent_ctx->lock avoids it from being uncloned. /Users/rubber/linux/kernel/events/core.c: 13190
 * Initialize the perf_event context in task_struct /Users/rubber/linux/kernel/events/core.c: 13214
 * Run the perf reboot notifier at the very last possible moment so that /Users/rubber/linux/kernel/events/core.c: 13352
 * the generic watchdog code runs as long as possible. /Users/rubber/linux/kernel/events/core.c: 13353
	/* /Users/rubber/linux/kernel/events/core.c: 13380
	 * Build time assertion that we keep the data_head at the intended /Users/rubber/linux/kernel/events/core.c: 13381
	 * location.  IOW, validation we got the __reserved[] size right. /Users/rubber/linux/kernel/events/core.c: 13382
	/* /Users/rubber/linux/kernel/events/core.c: 13485
	 * Implicitly enable on dfl hierarchy so that perf events can /Users/rubber/linux/kernel/events/core.c: 13486
	 * always be filtered by cgroup2 path as long as perf_event /Users/rubber/linux/kernel/events/core.c: 13487
	 * controller is not mounted on a legacy hierarchy. /Users/rubber/linux/kernel/events/core.c: 13488
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/events/uprobes.c: 1
 * User-space Probes (UProbes) /Users/rubber/linux/kernel/events/uprobes.c: 3
 * Copyright (C) IBM Corporation, 2008-2012 /Users/rubber/linux/kernel/events/uprobes.c: 5
 * Authors: /Users/rubber/linux/kernel/events/uprobes.c: 6
 *	Srikar Dronamraju /Users/rubber/linux/kernel/events/uprobes.c: 7
 *	Jim Keniston /Users/rubber/linux/kernel/events/uprobes.c: 8
 * Copyright (C) 2011-2012 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/events/uprobes.c: 9
 * allows us to skip the uprobe_mmap if there are no uprobe events active /Users/rubber/linux/kernel/events/uprobes.c: 38
 * at this time.  Probably a fine grained per inode count is better? /Users/rubber/linux/kernel/events/uprobes.c: 39
	/* /Users/rubber/linux/kernel/events/uprobes.c: 67
	 * The generic code assumes that it has two members of unknown type /Users/rubber/linux/kernel/events/uprobes.c: 68
	 * owned by the arch-specific code: /Users/rubber/linux/kernel/events/uprobes.c: 69
	 * /Users/rubber/linux/kernel/events/uprobes.c: 70
	 * 	insn -	copy_insn() saves the original instruction here for /Users/rubber/linux/kernel/events/uprobes.c: 71
	 *		arch_uprobe_analyze_insn(). /Users/rubber/linux/kernel/events/uprobes.c: 72
	 * /Users/rubber/linux/kernel/events/uprobes.c: 73
	 *	ixol -	potentially modified instruction to execute out of /Users/rubber/linux/kernel/events/uprobes.c: 74
	 *		line, copied to xol_area by xol_get_insn_slot(). /Users/rubber/linux/kernel/events/uprobes.c: 75
 * Execute out of line area: anonymous executable mapping installed /Users/rubber/linux/kernel/events/uprobes.c: 90
 * by the probed task to execute the copy of the original instruction /Users/rubber/linux/kernel/events/uprobes.c: 91
 * mangled by set_swbp(). /Users/rubber/linux/kernel/events/uprobes.c: 92
 * On a breakpoint hit, thread contests for a slot.  It frees the /Users/rubber/linux/kernel/events/uprobes.c: 94
 * slot after singlestep. Currently a fixed number of slots are /Users/rubber/linux/kernel/events/uprobes.c: 95
 * allocated. /Users/rubber/linux/kernel/events/uprobes.c: 96
	/* /Users/rubber/linux/kernel/events/uprobes.c: 105
	 * We keep the vma's vm_start rather than a pointer to the vma /Users/rubber/linux/kernel/events/uprobes.c: 106
	 * itself.  The probed process or a naughty kernel module could make /Users/rubber/linux/kernel/events/uprobes.c: 107
	 * the vma go away, and we must handle that reasonably gracefully. /Users/rubber/linux/kernel/events/uprobes.c: 108
 * valid_vma: Verify if the specified vma is an executable vma /Users/rubber/linux/kernel/events/uprobes.c: 114
 * Relax restrictions while unregistering: vm_flags might have /Users/rubber/linux/kernel/events/uprobes.c: 115
 * changed after breakpoint was inserted. /Users/rubber/linux/kernel/events/uprobes.c: 116
 *	- is_register: indicates if we are in register context. /Users/rubber/linux/kernel/events/uprobes.c: 117
 *	- Return 1 if the specified virtual address is in an /Users/rubber/linux/kernel/events/uprobes.c: 118
 *	  executable vma. /Users/rubber/linux/kernel/events/uprobes.c: 119
 * __replace_page - replace page in vma by new page. /Users/rubber/linux/kernel/events/uprobes.c: 142
 * based on replace_page in mm/ksm.c /Users/rubber/linux/kernel/events/uprobes.c: 143
 * @vma:      vma that holds the pte pointing to page /Users/rubber/linux/kernel/events/uprobes.c: 145
 * @addr:     address the old @page is mapped at /Users/rubber/linux/kernel/events/uprobes.c: 146
 * @old_page: the page we are replacing by new_page /Users/rubber/linux/kernel/events/uprobes.c: 147
 * @new_page: the modified page we replace page by /Users/rubber/linux/kernel/events/uprobes.c: 148
 * If @new_page is NULL, only unmap @old_page. /Users/rubber/linux/kernel/events/uprobes.c: 150
 * Returns 0 on success, negative error code otherwise. /Users/rubber/linux/kernel/events/uprobes.c: 152
 * is_swbp_insn - check if instruction is breakpoint instruction. /Users/rubber/linux/kernel/events/uprobes.c: 221
 * @insn: instruction to be checked. /Users/rubber/linux/kernel/events/uprobes.c: 222
 * Default implementation of is_swbp_insn /Users/rubber/linux/kernel/events/uprobes.c: 223
 * Returns true if @insn is a breakpoint instruction. /Users/rubber/linux/kernel/events/uprobes.c: 224
 * is_trap_insn - check if instruction is breakpoint instruction. /Users/rubber/linux/kernel/events/uprobes.c: 232
 * @insn: instruction to be checked. /Users/rubber/linux/kernel/events/uprobes.c: 233
 * Default implementation of is_trap_insn /Users/rubber/linux/kernel/events/uprobes.c: 234
 * Returns true if @insn is a breakpoint instruction. /Users/rubber/linux/kernel/events/uprobes.c: 235
 * This function is needed for the case where an architecture has multiple /Users/rubber/linux/kernel/events/uprobes.c: 237
 * trap instructions (like powerpc). /Users/rubber/linux/kernel/events/uprobes.c: 238
	/* /Users/rubber/linux/kernel/events/uprobes.c: 264
	 * Note: We only check if the old_opcode is UPROBE_SWBP_INSN here. /Users/rubber/linux/kernel/events/uprobes.c: 265
	 * We do not check if it is any other 'trap variant' which could /Users/rubber/linux/kernel/events/uprobes.c: 266
	 * be conditional trap instruction such as the one powerpc supports. /Users/rubber/linux/kernel/events/uprobes.c: 267
	 * /Users/rubber/linux/kernel/events/uprobes.c: 268
	 * The logic is that we do not care if the underlying instruction /Users/rubber/linux/kernel/events/uprobes.c: 269
	 * is a trap variant; uprobes always wins over any other (gdb) /Users/rubber/linux/kernel/events/uprobes.c: 270
	 * breakpoint. /Users/rubber/linux/kernel/events/uprobes.c: 271
		/* /Users/rubber/linux/kernel/events/uprobes.c: 383
		 * We are asking for 1 page. If get_user_pages_remote() fails, /Users/rubber/linux/kernel/events/uprobes.c: 384
		 * it may return 0, in that case we have to return error. /Users/rubber/linux/kernel/events/uprobes.c: 385
 * NOTE: /Users/rubber/linux/kernel/events/uprobes.c: 448
 * Expect the breakpoint instruction to be the smallest size instruction for /Users/rubber/linux/kernel/events/uprobes.c: 449
 * the architecture. If an arch has variable length instruction and the /Users/rubber/linux/kernel/events/uprobes.c: 450
 * breakpoint instruction is not of the smallest length instruction /Users/rubber/linux/kernel/events/uprobes.c: 451
 * supported by that architecture then we need to modify is_trap_at_addr and /Users/rubber/linux/kernel/events/uprobes.c: 452
 * uprobe_write_opcode accordingly. This would never be a problem for archs /Users/rubber/linux/kernel/events/uprobes.c: 453
 * that have fixed length instructions. /Users/rubber/linux/kernel/events/uprobes.c: 454
 * uprobe_write_opcode - write the opcode at a given virtual address. /Users/rubber/linux/kernel/events/uprobes.c: 456
 * @auprobe: arch specific probepoint information. /Users/rubber/linux/kernel/events/uprobes.c: 457
 * @mm: the probed process address space. /Users/rubber/linux/kernel/events/uprobes.c: 458
 * @vaddr: the virtual address to store the opcode. /Users/rubber/linux/kernel/events/uprobes.c: 459
 * @opcode: opcode to be written at @vaddr. /Users/rubber/linux/kernel/events/uprobes.c: 460
 * Called with mm->mmap_lock held for write. /Users/rubber/linux/kernel/events/uprobes.c: 462
 * Return 0 (success) or a negative errno. /Users/rubber/linux/kernel/events/uprobes.c: 463
 * set_swbp - store breakpoint at a given address. /Users/rubber/linux/kernel/events/uprobes.c: 568
 * @auprobe: arch specific probepoint information. /Users/rubber/linux/kernel/events/uprobes.c: 569
 * @mm: the probed process address space. /Users/rubber/linux/kernel/events/uprobes.c: 570
 * @vaddr: the virtual address to insert the opcode. /Users/rubber/linux/kernel/events/uprobes.c: 571
 * For mm @mm, store the breakpoint instruction at @vaddr. /Users/rubber/linux/kernel/events/uprobes.c: 573
 * Return 0 (success) or a negative errno. /Users/rubber/linux/kernel/events/uprobes.c: 574
 * set_orig_insn - Restore the original instruction. /Users/rubber/linux/kernel/events/uprobes.c: 582
 * @mm: the probed process address space. /Users/rubber/linux/kernel/events/uprobes.c: 583
 * @auprobe: arch specific probepoint information. /Users/rubber/linux/kernel/events/uprobes.c: 584
 * @vaddr: the virtual address to insert the opcode. /Users/rubber/linux/kernel/events/uprobes.c: 585
 * For mm @mm, restore the original opcode (opcode) at @vaddr. /Users/rubber/linux/kernel/events/uprobes.c: 587
 * Return 0 (success) or a negative errno. /Users/rubber/linux/kernel/events/uprobes.c: 588
		/* /Users/rubber/linux/kernel/events/uprobes.c: 606
		 * If application munmap(exec_vma) before uprobe_unregister() /Users/rubber/linux/kernel/events/uprobes.c: 607
		 * gets called, we don't get a chance to remove uprobe from /Users/rubber/linux/kernel/events/uprobes.c: 608
		 * delayed_uprobe_list from remove_breakpoint(). Do it here. /Users/rubber/linux/kernel/events/uprobes.c: 609
 * Find a uprobe corresponding to a given inode:offset /Users/rubber/linux/kernel/events/uprobes.c: 672
 * Acquires uprobes_treelock /Users/rubber/linux/kernel/events/uprobes.c: 673
 * Acquire uprobes_treelock. /Users/rubber/linux/kernel/events/uprobes.c: 700
 * Matching uprobe already exists in rbtree; /Users/rubber/linux/kernel/events/uprobes.c: 701
 *	increment (access refcount) and return the matching uprobe. /Users/rubber/linux/kernel/events/uprobes.c: 702
 * No matching uprobe; insert the uprobe in rb_tree; /Users/rubber/linux/kernel/events/uprobes.c: 704
 *	get a double refcount (access + creation) and return NULL. /Users/rubber/linux/kernel/events/uprobes.c: 705
 * For uprobe @uprobe, delete the consumer @uc. /Users/rubber/linux/kernel/events/uprobes.c: 769
 * Return true if the @uc is deleted successfully /Users/rubber/linux/kernel/events/uprobes.c: 770
 * or return false. /Users/rubber/linux/kernel/events/uprobes.c: 771
	/* /Users/rubber/linux/kernel/events/uprobes.c: 795
	 * Ensure that the page that has the original instruction is populated /Users/rubber/linux/kernel/events/uprobes.c: 796
	 * and in page-cache. If ->readpage == NULL it must be shmem_mapping(), /Users/rubber/linux/kernel/events/uprobes.c: 797
	 * see uprobe_register(). /Users/rubber/linux/kernel/events/uprobes.c: 798
	/* /Users/rubber/linux/kernel/events/uprobes.c: 907
	 * set MMF_HAS_UPROBES in advance for uprobe_pre_sstep_notifier(), /Users/rubber/linux/kernel/events/uprobes.c: 908
	 * the task can hit this breakpoint right after __replace_page(). /Users/rubber/linux/kernel/events/uprobes.c: 909
 * There could be threads that have already hit the breakpoint. They /Users/rubber/linux/kernel/events/uprobes.c: 936
 * will recheck the current insn and restart if find_uprobe() fails. /Users/rubber/linux/kernel/events/uprobes.c: 937
 * See find_active_uprobe(). /Users/rubber/linux/kernel/events/uprobes.c: 938
			/* /Users/rubber/linux/kernel/events/uprobes.c: 982
			 * Needs GFP_NOWAIT to avoid i_mmap_rwsem recursion through /Users/rubber/linux/kernel/events/uprobes.c: 983
			 * reclaim. This is optimistic, no harm done if it fails. /Users/rubber/linux/kernel/events/uprobes.c: 984
 * uprobe_unregister - unregister an already registered probe. /Users/rubber/linux/kernel/events/uprobes.c: 1104
 * @inode: the file in which the probe has to be removed. /Users/rubber/linux/kernel/events/uprobes.c: 1105
 * @offset: offset from the start of the file. /Users/rubber/linux/kernel/events/uprobes.c: 1106
 * @uc: identify which probe if multiple probes are colocated. /Users/rubber/linux/kernel/events/uprobes.c: 1107
 * __uprobe_register - register a probe /Users/rubber/linux/kernel/events/uprobes.c: 1125
 * @inode: the file in which the probe has to be placed. /Users/rubber/linux/kernel/events/uprobes.c: 1126
 * @offset: offset from the start of the file. /Users/rubber/linux/kernel/events/uprobes.c: 1127
 * @uc: information on howto handle the probe.. /Users/rubber/linux/kernel/events/uprobes.c: 1128
 * Apart from the access refcount, __uprobe_register() takes a creation /Users/rubber/linux/kernel/events/uprobes.c: 1130
 * refcount (thro alloc_uprobe) if and only if this @uprobe is getting /Users/rubber/linux/kernel/events/uprobes.c: 1131
 * inserted into the rbtree (i.e first consumer for a @inode:@offset /Users/rubber/linux/kernel/events/uprobes.c: 1132
 * tuple).  Creation refcount stops uprobe_unregister from freeing the /Users/rubber/linux/kernel/events/uprobes.c: 1133
 * @uprobe even before the register operation is complete. Creation /Users/rubber/linux/kernel/events/uprobes.c: 1134
 * refcount is released when the last @uc for the @uprobe /Users/rubber/linux/kernel/events/uprobes.c: 1135
 * unregisters. Caller of __uprobe_register() is required to keep @inode /Users/rubber/linux/kernel/events/uprobes.c: 1136
 * (and the containing mount) referenced. /Users/rubber/linux/kernel/events/uprobes.c: 1137
 * Return errno if it cannot successully install probes /Users/rubber/linux/kernel/events/uprobes.c: 1139
 * else return 0 (success) /Users/rubber/linux/kernel/events/uprobes.c: 1140
	/* /Users/rubber/linux/kernel/events/uprobes.c: 1159
	 * This ensures that copy_from_page(), copy_to_page() and /Users/rubber/linux/kernel/events/uprobes.c: 1160
	 * __update_ref_ctr() can't cross page boundary. /Users/rubber/linux/kernel/events/uprobes.c: 1161
	/* /Users/rubber/linux/kernel/events/uprobes.c: 1175
	 * We can race with uprobe_unregister()->delete_uprobe(). /Users/rubber/linux/kernel/events/uprobes.c: 1176
	 * Check uprobe_is_active() and retry if it is false. /Users/rubber/linux/kernel/events/uprobes.c: 1177
 * uprobe_apply - unregister an already registered probe. /Users/rubber/linux/kernel/events/uprobes.c: 1210
 * @inode: the file in which the probe has to be removed. /Users/rubber/linux/kernel/events/uprobes.c: 1211
 * @offset: offset from the start of the file. /Users/rubber/linux/kernel/events/uprobes.c: 1212
 * @uc: consumer which wants to add more or remove some breakpoints /Users/rubber/linux/kernel/events/uprobes.c: 1213
 * @add: add or remove the breakpoints /Users/rubber/linux/kernel/events/uprobes.c: 1214
 * For a given range in vma, build a list of probes that need to be inserted. /Users/rubber/linux/kernel/events/uprobes.c: 1291
 * Called from mmap_region/vma_adjust with mm->mmap_lock acquired. /Users/rubber/linux/kernel/events/uprobes.c: 1357
 * Currently we ignore all errors and always return 0, the callers /Users/rubber/linux/kernel/events/uprobes.c: 1359
 * can't handle the failure anyway. /Users/rubber/linux/kernel/events/uprobes.c: 1360
	/* /Users/rubber/linux/kernel/events/uprobes.c: 1385
	 * We can race with uprobe_unregister(), this uprobe can be already /Users/rubber/linux/kernel/events/uprobes.c: 1386
	 * removed. But in this case filter_chain() must return false, all /Users/rubber/linux/kernel/events/uprobes.c: 1387
	 * consumers have gone away. /Users/rubber/linux/kernel/events/uprobes.c: 1388
 * Called in context of a munmap of a vma. /Users/rubber/linux/kernel/events/uprobes.c: 1423
 * get_xol_area - Allocate process's xol_area if necessary. /Users/rubber/linux/kernel/events/uprobes.c: 1525
 * This area will be used for storing instructions for execution out of line. /Users/rubber/linux/kernel/events/uprobes.c: 1526
 * Returns the allocated area or NULL. /Users/rubber/linux/kernel/events/uprobes.c: 1528
 * uprobe_clear_state - Free the area allocated for slots. /Users/rubber/linux/kernel/events/uprobes.c: 1544
 *  - search for a free slot. /Users/rubber/linux/kernel/events/uprobes.c: 1582
 * xol_get_insn_slot - allocate a slot for xol. /Users/rubber/linux/kernel/events/uprobes.c: 1608
 * Returns the allocated slot address or 0. /Users/rubber/linux/kernel/events/uprobes.c: 1609
 * xol_free_insn_slot - If slot was earlier allocated by /Users/rubber/linux/kernel/events/uprobes.c: 1631
 * @xol_get_insn_slot(), make the slot available for /Users/rubber/linux/kernel/events/uprobes.c: 1632
 * subsequent requests. /Users/rubber/linux/kernel/events/uprobes.c: 1633
	/* /Users/rubber/linux/kernel/events/uprobes.c: 1675
	 * We probably need flush_icache_user_page() but it needs vma. /Users/rubber/linux/kernel/events/uprobes.c: 1676
	 * This should work on most of architectures by default. If /Users/rubber/linux/kernel/events/uprobes.c: 1677
	 * architecture needs to do something different it can define /Users/rubber/linux/kernel/events/uprobes.c: 1678
	 * its own version of the function. /Users/rubber/linux/kernel/events/uprobes.c: 1679
 * uprobe_get_swbp_addr - compute address of swbp given post-swbp regs /Users/rubber/linux/kernel/events/uprobes.c: 1685
 * @regs: Reflects the saved state of the task after it has hit a breakpoint /Users/rubber/linux/kernel/events/uprobes.c: 1686
 * instruction. /Users/rubber/linux/kernel/events/uprobes.c: 1687
 * Return the address of the breakpoint instruction. /Users/rubber/linux/kernel/events/uprobes.c: 1688
 * Called with no locks held. /Users/rubber/linux/kernel/events/uprobes.c: 1714
 * Called in context of an exiting or an exec-ing thread. /Users/rubber/linux/kernel/events/uprobes.c: 1715
 * Allocate a uprobe_task object for the task if necessary. /Users/rubber/linux/kernel/events/uprobes.c: 1738
 * Called when the thread hits a breakpoint. /Users/rubber/linux/kernel/events/uprobes.c: 1739
 * Returns: /Users/rubber/linux/kernel/events/uprobes.c: 1741
 * - pointer to new uprobe_task on success /Users/rubber/linux/kernel/events/uprobes.c: 1742
 * - NULL otherwise /Users/rubber/linux/kernel/events/uprobes.c: 1743
 * Called in context of a new clone/fork from copy_process. /Users/rubber/linux/kernel/events/uprobes.c: 1797
 * Current area->vaddr notion assume the trampoline address is always /Users/rubber/linux/kernel/events/uprobes.c: 1830
 * equal area->vaddr. /Users/rubber/linux/kernel/events/uprobes.c: 1831
 * Returns -1 in case the xol_area is not allocated. /Users/rubber/linux/kernel/events/uprobes.c: 1833
	/* /Users/rubber/linux/kernel/events/uprobes.c: 1895
	 * We don't want to keep trampoline address in stack, rather keep the /Users/rubber/linux/kernel/events/uprobes.c: 1896
	 * original return address of first caller thru all the consequent /Users/rubber/linux/kernel/events/uprobes.c: 1897
	 * instances. This also makes breakpoint unwrapping easier. /Users/rubber/linux/kernel/events/uprobes.c: 1898
			/* /Users/rubber/linux/kernel/events/uprobes.c: 1902
			 * This situation is not possible. Likely we have an /Users/rubber/linux/kernel/events/uprobes.c: 1903
			 * attack from user-space. /Users/rubber/linux/kernel/events/uprobes.c: 1904
 * If we are singlestepping, then ensure this thread is not connected to /Users/rubber/linux/kernel/events/uprobes.c: 1958
 * non-fatal signals until completion of singlestep.  When xol insn itself /Users/rubber/linux/kernel/events/uprobes.c: 1959
 * triggers the signal,  restart the original insn even if the task is /Users/rubber/linux/kernel/events/uprobes.c: 1960
 * already SIGKILL'ed (since coredump should report the correct ip).  This /Users/rubber/linux/kernel/events/uprobes.c: 1961
 * is even more important if the task has a handler for SIGSEGV/etc, The /Users/rubber/linux/kernel/events/uprobes.c: 1962
 * _same_ instruction should be repeated again after return from the signal /Users/rubber/linux/kernel/events/uprobes.c: 1963
 * handler, and SSTEP can never finish in this case. /Users/rubber/linux/kernel/events/uprobes.c: 1964
		/* /Users/rubber/linux/kernel/events/uprobes.c: 1997
		 * This is not strictly accurate, we can race with /Users/rubber/linux/kernel/events/uprobes.c: 1998
		 * uprobe_unregister() and see the already removed /Users/rubber/linux/kernel/events/uprobes.c: 1999
		 * uprobe if delete_uprobe() was not yet called. /Users/rubber/linux/kernel/events/uprobes.c: 2000
		 * Or this uprobe can be filtered out. /Users/rubber/linux/kernel/events/uprobes.c: 2001
	/* /Users/rubber/linux/kernel/events/uprobes.c: 2026
	 * The NULL 'tsk' here ensures that any faults that occur here /Users/rubber/linux/kernel/events/uprobes.c: 2027
	 * will not be accounted to the task.  'mm' *is* current->mm, /Users/rubber/linux/kernel/events/uprobes.c: 2028
	 * but we treat this as a 'remote' access since it is /Users/rubber/linux/kernel/events/uprobes.c: 2029
	 * essentially a kernel access to the memory. /Users/rubber/linux/kernel/events/uprobes.c: 2030
		/* /Users/rubber/linux/kernel/events/uprobes.c: 2146
		 * We should throw out the frames invalidated by longjmp(). /Users/rubber/linux/kernel/events/uprobes.c: 2147
		 * If this chain is valid, then the next one should be alive /Users/rubber/linux/kernel/events/uprobes.c: 2148
		 * or NULL; the latter case means that nobody but ri->func /Users/rubber/linux/kernel/events/uprobes.c: 2149
		 * could hit this trampoline on return. TODO: sigaltstack(). /Users/rubber/linux/kernel/events/uprobes.c: 2150
 * Run handler and ask thread to singlestep. /Users/rubber/linux/kernel/events/uprobes.c: 2185
 * Ensure all non-fatal signals cannot interrupt thread while it singlesteps. /Users/rubber/linux/kernel/events/uprobes.c: 2186
			/* /Users/rubber/linux/kernel/events/uprobes.c: 2204
			 * Either we raced with uprobe_unregister() or we can't /Users/rubber/linux/kernel/events/uprobes.c: 2205
			 * access this memory. The latter is only possible if /Users/rubber/linux/kernel/events/uprobes.c: 2206
			 * another thread plays with our ->mm. In both cases /Users/rubber/linux/kernel/events/uprobes.c: 2207
			 * we can simply restart. If this vma was unmapped we /Users/rubber/linux/kernel/events/uprobes.c: 2208
			 * can pretend this insn was not executed yet and get /Users/rubber/linux/kernel/events/uprobes.c: 2209
			 * the (correct) SIGSEGV after restart. /Users/rubber/linux/kernel/events/uprobes.c: 2210
	/* /Users/rubber/linux/kernel/events/uprobes.c: 2220
	 * TODO: move copy_insn/etc into _register and remove this hack. /Users/rubber/linux/kernel/events/uprobes.c: 2221
	 * After we hit the bp, _unregister + _register can install the /Users/rubber/linux/kernel/events/uprobes.c: 2222
	 * new and not-yet-analyzed uprobe at the same address, restart. /Users/rubber/linux/kernel/events/uprobes.c: 2223
	/* /Users/rubber/linux/kernel/events/uprobes.c: 2228
	 * Pairs with the smp_wmb() in prepare_uprobe(). /Users/rubber/linux/kernel/events/uprobes.c: 2229
	 * /Users/rubber/linux/kernel/events/uprobes.c: 2230
	 * Guarantees that if we see the UPROBE_COPY_INSN bit set, then /Users/rubber/linux/kernel/events/uprobes.c: 2231
	 * we must also see the stores to &uprobe->arch performed by the /Users/rubber/linux/kernel/events/uprobes.c: 2232
	 * prepare_uprobe() call. /Users/rubber/linux/kernel/events/uprobes.c: 2233
 * Perform required fix-ups and disable singlestep. /Users/rubber/linux/kernel/events/uprobes.c: 2258
 * Allow pending signals to take effect. /Users/rubber/linux/kernel/events/uprobes.c: 2259
 * On breakpoint hit, breakpoint notifier sets the TIF_UPROBE flag and /Users/rubber/linux/kernel/events/uprobes.c: 2290
 * allows the thread to return from interrupt. After that handle_swbp() /Users/rubber/linux/kernel/events/uprobes.c: 2291
 * sets utask->active_uprobe. /Users/rubber/linux/kernel/events/uprobes.c: 2292
 * On singlestep exception, singlestep notifier sets the TIF_UPROBE flag /Users/rubber/linux/kernel/events/uprobes.c: 2294
 * and allows the thread to return from interrupt. /Users/rubber/linux/kernel/events/uprobes.c: 2295
 * While returning to userspace, thread notices the TIF_UPROBE flag and calls /Users/rubber/linux/kernel/events/uprobes.c: 2297
 * uprobe_notify_resume(). /Users/rubber/linux/kernel/events/uprobes.c: 2298
 * uprobe_pre_sstep_notifier gets called from interrupt context as part of /Users/rubber/linux/kernel/events/uprobes.c: 2314
 * notifier mechanism. Set TIF_UPROBE flag and indicate breakpoint hit. /Users/rubber/linux/kernel/events/uprobes.c: 2315
 * uprobe_post_sstep_notifier gets called in interrupt context as part of notifier /Users/rubber/linux/kernel/events/uprobes.c: 2331
 * mechanism. Set TIF_UPROBE flag and indicate completion of singlestep. /Users/rubber/linux/kernel/events/uprobes.c: 2332
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/events/ring_buffer.c: 1
 * Performance events ring-buffer code: /Users/rubber/linux/kernel/events/ring_buffer.c: 3
 *  Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de> /Users/rubber/linux/kernel/events/ring_buffer.c: 5
 *  Copyright (C) 2008-2011 Red Hat, Inc., Ingo Molnar /Users/rubber/linux/kernel/events/ring_buffer.c: 6
 *  Copyright (C) 2008-2011 Red Hat, Inc., Peter Zijlstra /Users/rubber/linux/kernel/events/ring_buffer.c: 7
 *  Copyright  ©  2009 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com> /Users/rubber/linux/kernel/events/ring_buffer.c: 8
 * We need to ensure a later event_id doesn't publish a head when a former /Users/rubber/linux/kernel/events/ring_buffer.c: 29
 * event isn't done writing. However since we need to deal with NMIs we /Users/rubber/linux/kernel/events/ring_buffer.c: 30
 * cannot fully serialize things. /Users/rubber/linux/kernel/events/ring_buffer.c: 31
 * We only publish the head (and generate a wakeup) when the outer-most /Users/rubber/linux/kernel/events/ring_buffer.c: 33
 * event completes. /Users/rubber/linux/kernel/events/ring_buffer.c: 34
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 42
	 * Avoid an explicit LOAD/STORE such that architectures with memops /Users/rubber/linux/kernel/events/ring_buffer.c: 43
	 * can use them. /Users/rubber/linux/kernel/events/ring_buffer.c: 44
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 56
	 * If this isn't the outermost nesting, we don't have to update /Users/rubber/linux/kernel/events/ring_buffer.c: 57
	 * @rb->user_page->data_head. /Users/rubber/linux/kernel/events/ring_buffer.c: 58
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 67
	 * In order to avoid publishing a head value that goes backwards, /Users/rubber/linux/kernel/events/ring_buffer.c: 68
	 * we must ensure the load of @rb->head happens after we've /Users/rubber/linux/kernel/events/ring_buffer.c: 69
	 * incremented @rb->nest. /Users/rubber/linux/kernel/events/ring_buffer.c: 70
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 71
	 * Otherwise we can observe a @rb->head value before one published /Users/rubber/linux/kernel/events/ring_buffer.c: 72
	 * by an IRQ/NMI happening between the load and the increment. /Users/rubber/linux/kernel/events/ring_buffer.c: 73
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 78
	 * IRQ/NMI can happen here and advance @rb->head, causing our /Users/rubber/linux/kernel/events/ring_buffer.c: 79
	 * load above to be stale. /Users/rubber/linux/kernel/events/ring_buffer.c: 80
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 83
	 * Since the mmap() consumer (userspace) can run on a different CPU: /Users/rubber/linux/kernel/events/ring_buffer.c: 84
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 85
	 *   kernel				user /Users/rubber/linux/kernel/events/ring_buffer.c: 86
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 87
	 *   if (LOAD ->data_tail) {		LOAD ->data_head /Users/rubber/linux/kernel/events/ring_buffer.c: 88
	 *			(A)		smp_rmb()	(C) /Users/rubber/linux/kernel/events/ring_buffer.c: 89
	 *	STORE $data			LOAD $data /Users/rubber/linux/kernel/events/ring_buffer.c: 90
	 *	smp_wmb()	(B)		smp_mb()	(D) /Users/rubber/linux/kernel/events/ring_buffer.c: 91
	 *	STORE ->data_head		STORE ->data_tail /Users/rubber/linux/kernel/events/ring_buffer.c: 92
	 *   } /Users/rubber/linux/kernel/events/ring_buffer.c: 93
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 94
	 * Where A pairs with D, and B pairs with C. /Users/rubber/linux/kernel/events/ring_buffer.c: 95
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 96
	 * In our case (A) is a control dependency that separates the load of /Users/rubber/linux/kernel/events/ring_buffer.c: 97
	 * the ->data_tail and the stores of $data. In case ->data_tail /Users/rubber/linux/kernel/events/ring_buffer.c: 98
	 * indicates there is no room in the buffer to store $data we do not. /Users/rubber/linux/kernel/events/ring_buffer.c: 99
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 100
	 * D needs to be a full barrier since it separates the data READ /Users/rubber/linux/kernel/events/ring_buffer.c: 101
	 * from the tail WRITE. /Users/rubber/linux/kernel/events/ring_buffer.c: 102
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 103
	 * For B a WMB is sufficient since it separates two WRITEs, and for C /Users/rubber/linux/kernel/events/ring_buffer.c: 104
	 * an RMB is sufficient since it separates two READs. /Users/rubber/linux/kernel/events/ring_buffer.c: 105
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 106
	 * See perf_output_begin(). /Users/rubber/linux/kernel/events/ring_buffer.c: 107
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 112
	 * We must publish the head before decrementing the nest count, /Users/rubber/linux/kernel/events/ring_buffer.c: 113
	 * otherwise an IRQ/NMI can publish a more recent head value and our /Users/rubber/linux/kernel/events/ring_buffer.c: 114
	 * write will (temporarily) publish a stale value. /Users/rubber/linux/kernel/events/ring_buffer.c: 115
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 120
	 * Ensure we decrement @rb->nest before we validate the @rb->head. /Users/rubber/linux/kernel/events/ring_buffer.c: 121
	 * Otherwise we cannot be sure we caught the 'last' nested update. /Users/rubber/linux/kernel/events/ring_buffer.c: 122
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 164
	 * For inherited events we send all the output towards the parent. /Users/rubber/linux/kernel/events/ring_buffer.c: 165
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 202
		 * The above forms a control dependency barrier separating the /Users/rubber/linux/kernel/events/ring_buffer.c: 203
		 * @tail load above from the data stores below. Since the @tail /Users/rubber/linux/kernel/events/ring_buffer.c: 204
		 * load is required to compute the branch to fail below. /Users/rubber/linux/kernel/events/ring_buffer.c: 205
		 * /Users/rubber/linux/kernel/events/ring_buffer.c: 206
		 * A, matches D; the full memory barrier userspace SHOULD issue /Users/rubber/linux/kernel/events/ring_buffer.c: 207
		 * after reading the data and before storing the new tail /Users/rubber/linux/kernel/events/ring_buffer.c: 208
		 * position. /Users/rubber/linux/kernel/events/ring_buffer.c: 209
		 * /Users/rubber/linux/kernel/events/ring_buffer.c: 210
		 * See perf_output_put_handle(). /Users/rubber/linux/kernel/events/ring_buffer.c: 211
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 225
	 * We rely on the implied barrier() by local_cmpxchg() to ensure /Users/rubber/linux/kernel/events/ring_buffer.c: 226
	 * none of the data stores below can be lifted up by the compiler. /Users/rubber/linux/kernel/events/ring_buffer.c: 227
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 326
	 * perf_output_begin() only checks rb->paused, therefore /Users/rubber/linux/kernel/events/ring_buffer.c: 327
	 * rb->paused must be true if we have no pages for output. /Users/rubber/linux/kernel/events/ring_buffer.c: 328
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 336
	 * OVERWRITE is determined by perf_aux_output_end() and can't /Users/rubber/linux/kernel/events/ring_buffer.c: 337
	 * be passed in directly. /Users/rubber/linux/kernel/events/ring_buffer.c: 338
 * This is called before hardware starts writing to the AUX area to /Users/rubber/linux/kernel/events/ring_buffer.c: 348
 * obtain an output handle and make sure there's room in the buffer. /Users/rubber/linux/kernel/events/ring_buffer.c: 349
 * When the capture completes, call perf_aux_output_end() to commit /Users/rubber/linux/kernel/events/ring_buffer.c: 350
 * the recorded data to the buffer. /Users/rubber/linux/kernel/events/ring_buffer.c: 351
 * The ordering is similar to that of perf_output_{begin,end}, with /Users/rubber/linux/kernel/events/ring_buffer.c: 353
 * the exception of (B), which should be taken care of by the pmu /Users/rubber/linux/kernel/events/ring_buffer.c: 354
 * driver, since ordering rules will differ depending on hardware. /Users/rubber/linux/kernel/events/ring_buffer.c: 355
 * Call this from pmu::start(); see the comment in perf_aux_output_end() /Users/rubber/linux/kernel/events/ring_buffer.c: 357
 * about its use in pmu callbacks. Both can also be called from the PMI /Users/rubber/linux/kernel/events/ring_buffer.c: 358
 * handler if needed. /Users/rubber/linux/kernel/events/ring_buffer.c: 359
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 372
	 * Since this will typically be open across pmu::add/pmu::del, we /Users/rubber/linux/kernel/events/ring_buffer.c: 373
	 * grab ring_buffer's refcount instead of holding rcu read lock /Users/rubber/linux/kernel/events/ring_buffer.c: 374
	 * to make sure it doesn't disappear under us. /Users/rubber/linux/kernel/events/ring_buffer.c: 375
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 384
	 * If aux_mmap_count is zero, the aux buffer is in perf_mmap_close(), /Users/rubber/linux/kernel/events/ring_buffer.c: 385
	 * about to get freed, so we leave immediately. /Users/rubber/linux/kernel/events/ring_buffer.c: 386
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 387
	 * Checking rb::aux_mmap_count and rb::refcount has to be done in /Users/rubber/linux/kernel/events/ring_buffer.c: 388
	 * the same order, see perf_mmap_close. Otherwise we end up freeing /Users/rubber/linux/kernel/events/ring_buffer.c: 389
	 * aux pages in this path, which is a bug, because in_atomic(). /Users/rubber/linux/kernel/events/ring_buffer.c: 390
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 399
	 * Nesting is not supported for AUX area, make sure nested /Users/rubber/linux/kernel/events/ring_buffer.c: 400
	 * writers are caught early /Users/rubber/linux/kernel/events/ring_buffer.c: 401
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 416
	 * In overwrite mode, AUX data stores do not depend on aux_tail, /Users/rubber/linux/kernel/events/ring_buffer.c: 417
	 * therefore (A) control dependency barrier does not exist. The /Users/rubber/linux/kernel/events/ring_buffer.c: 418
	 * (B) <-> (C) ordering is still observed by the pmu driver. /Users/rubber/linux/kernel/events/ring_buffer.c: 419
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 427
		 * handle->size computation depends on aux_tail load; this forms a /Users/rubber/linux/kernel/events/ring_buffer.c: 428
		 * control dependency barrier separating aux_tail load from aux data /Users/rubber/linux/kernel/events/ring_buffer.c: 429
		 * store that will be enabled on successful return /Users/rubber/linux/kernel/events/ring_buffer.c: 430
 * Commit the data written by hardware into the ring buffer by adjusting /Users/rubber/linux/kernel/events/ring_buffer.c: 468
 * aux_head and posting a PERF_RECORD_AUX into the perf buffer. It is the /Users/rubber/linux/kernel/events/ring_buffer.c: 469
 * pmu driver's responsibility to observe ordering rules of the hardware, /Users/rubber/linux/kernel/events/ring_buffer.c: 470
 * so that all the data is externally visible before this is called. /Users/rubber/linux/kernel/events/ring_buffer.c: 471
 * Note: this has to be called from pmu::stop() callback, as the assumption /Users/rubber/linux/kernel/events/ring_buffer.c: 473
 * of the AUX buffer management code is that after pmu::stop(), the AUX /Users/rubber/linux/kernel/events/ring_buffer.c: 474
 * transaction must be stopped and therefore drop the AUX reference count. /Users/rubber/linux/kernel/events/ring_buffer.c: 475
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 496
	 * Only send RECORD_AUX if we have something useful to communicate /Users/rubber/linux/kernel/events/ring_buffer.c: 497
	 * /Users/rubber/linux/kernel/events/ring_buffer.c: 498
	 * Note: the OVERWRITE records by themselves are not considered /Users/rubber/linux/kernel/events/ring_buffer.c: 499
	 * useful, as they don't communicate any *new* information, /Users/rubber/linux/kernel/events/ring_buffer.c: 500
	 * aside from the short-lived offset, that becomes history at /Users/rubber/linux/kernel/events/ring_buffer.c: 501
	 * the next event sched-in and therefore isn't useful. /Users/rubber/linux/kernel/events/ring_buffer.c: 502
	 * The userspace that needs to copy out AUX data in overwrite /Users/rubber/linux/kernel/events/ring_buffer.c: 503
	 * mode should know to use user_page::aux_head for the actual /Users/rubber/linux/kernel/events/ring_buffer.c: 504
	 * offset. So, from now on we don't output AUX records that /Users/rubber/linux/kernel/events/ring_buffer.c: 505
	 * have *only* OVERWRITE flag set. /Users/rubber/linux/kernel/events/ring_buffer.c: 506
 * Skip over a given number of bytes in the AUX buffer, due to, for example, /Users/rubber/linux/kernel/events/ring_buffer.c: 532
 * hardware's alignment constraints. /Users/rubber/linux/kernel/events/ring_buffer.c: 533
 * Copy out AUX data from an AUX handle. /Users/rubber/linux/kernel/events/ring_buffer.c: 568
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 617
		 * Communicate the allocation size to the driver: /Users/rubber/linux/kernel/events/ring_buffer.c: 618
		 * if we managed to secure a high-order allocation, /Users/rubber/linux/kernel/events/ring_buffer.c: 619
		 * set its first page's private to this order; /Users/rubber/linux/kernel/events/ring_buffer.c: 620
		 * !PagePrivate(page) means it's just a normal page. /Users/rubber/linux/kernel/events/ring_buffer.c: 621
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 644
	 * Should never happen, the last reference should be dropped from /Users/rubber/linux/kernel/events/ring_buffer.c: 645
	 * perf_mmap_close() path, which first stops aux transactions (which /Users/rubber/linux/kernel/events/ring_buffer.c: 646
	 * in turn are the atomic holders of aux_refcount) and then does the /Users/rubber/linux/kernel/events/ring_buffer.c: 647
	 * last rb_free_aux(). /Users/rubber/linux/kernel/events/ring_buffer.c: 648
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 678
		 * Watermark defaults to half the buffer, and so does the /Users/rubber/linux/kernel/events/ring_buffer.c: 679
		 * max_order, to aid PMU drivers in double buffering. /Users/rubber/linux/kernel/events/ring_buffer.c: 680
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 685
		 * Use aux_watermark as the basis for chunking to /Users/rubber/linux/kernel/events/ring_buffer.c: 686
		 * help PMU drivers honor the watermark. /Users/rubber/linux/kernel/events/ring_buffer.c: 687
		/* /Users/rubber/linux/kernel/events/ring_buffer.c: 691
		 * We need to start with the max_order that fits in nr_pages, /Users/rubber/linux/kernel/events/ring_buffer.c: 692
		 * not the other way around, hence ilog2() and not get_order. /Users/rubber/linux/kernel/events/ring_buffer.c: 693
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 719
	 * In overwrite mode, PMUs that don't support SG may not handle more /Users/rubber/linux/kernel/events/ring_buffer.c: 720
	 * than one contiguous allocation, since they rely on PMI to do double /Users/rubber/linux/kernel/events/ring_buffer.c: 721
	 * buffering. In this case, the entire buffer has to be one contiguous /Users/rubber/linux/kernel/events/ring_buffer.c: 722
	 * chunk. /Users/rubber/linux/kernel/events/ring_buffer.c: 723
	/* /Users/rubber/linux/kernel/events/ring_buffer.c: 740
	 * aux_pages (and pmu driver's private data, aux_priv) will be /Users/rubber/linux/kernel/events/ring_buffer.c: 741
	 * referenced in both producer's and consumer's contexts, thus /Users/rubber/linux/kernel/events/ring_buffer.c: 742
	 * we keep a refcount here to make sure either of the two can /Users/rubber/linux/kernel/events/ring_buffer.c: 743
	 * reference them safely. /Users/rubber/linux/kernel/events/ring_buffer.c: 744
 * Back perf_mmap() with regular GFP_KERNEL-0 pages. /Users/rubber/linux/kernel/events/ring_buffer.c: 769
 SPDX-License-Identifier: GPL-2.0+ /Users/rubber/linux/kernel/events/hw_breakpoint.c: 1
 * Copyright (C) 2007 Alan Stern /Users/rubber/linux/kernel/events/hw_breakpoint.c: 3
 * Copyright (C) IBM Corporation, 2009 /Users/rubber/linux/kernel/events/hw_breakpoint.c: 4
 * Copyright (C) 2009, Frederic Weisbecker <fweisbec@gmail.com> /Users/rubber/linux/kernel/events/hw_breakpoint.c: 5
 * Thanks to Ingo Molnar for his many suggestions. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 7
 * Authors: Alan Stern <stern@rowland.harvard.edu> /Users/rubber/linux/kernel/events/hw_breakpoint.c: 9
 *          K.Prasad <prasad@linux.vnet.ibm.com> /Users/rubber/linux/kernel/events/hw_breakpoint.c: 10
 *          Frederic Weisbecker <fweisbec@gmail.com> /Users/rubber/linux/kernel/events/hw_breakpoint.c: 11
 * HW_breakpoint: a unified kernel/user-space hardware breakpoint facility, /Users/rubber/linux/kernel/events/hw_breakpoint.c: 15
 * using the CPU's debug registers. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 16
 * This file contains the arch-independent routines. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 17
 * Constraints data /Users/rubber/linux/kernel/events/hw_breakpoint.c: 38
 * Report the maximum number of pinned breakpoints a task /Users/rubber/linux/kernel/events/hw_breakpoint.c: 85
 * have in this cpu /Users/rubber/linux/kernel/events/hw_breakpoint.c: 86
 * Count the number of breakpoints of the same type and same task. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 102
 * The given event must be not on the list. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 103
 * Report the number of pinned/un-pinned breakpoints we have in /Users/rubber/linux/kernel/events/hw_breakpoint.c: 129
 * a given cpu (cpu > -1) or in all of them (cpu = -1). /Users/rubber/linux/kernel/events/hw_breakpoint.c: 130
 * For now, continue to consider flexible as pinned, until we can /Users/rubber/linux/kernel/events/hw_breakpoint.c: 159
 * ensure no flexible event can ever be scheduled before a pinned event /Users/rubber/linux/kernel/events/hw_breakpoint.c: 160
 * in a same cpu. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 161
 * Add a pinned breakpoint for the given task in our constraint table /Users/rubber/linux/kernel/events/hw_breakpoint.c: 170
 * Add/remove the given breakpoint in our constraint table /Users/rubber/linux/kernel/events/hw_breakpoint.c: 188
 * Function to perform processor-specific cleanup during unregistration /Users/rubber/linux/kernel/events/hw_breakpoint.c: 226
	/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 230
	 * A weak stub function here for those archs that don't define /Users/rubber/linux/kernel/events/hw_breakpoint.c: 231
	 * it inside arch/.../kernel/hw_breakpoint.c /Users/rubber/linux/kernel/events/hw_breakpoint.c: 232
 * Constraints to check before allowing this new breakpoint counter: /Users/rubber/linux/kernel/events/hw_breakpoint.c: 237
 *  == Non-pinned counter == (Considered as pinned for now) /Users/rubber/linux/kernel/events/hw_breakpoint.c: 239
 *   - If attached to a single cpu, check: /Users/rubber/linux/kernel/events/hw_breakpoint.c: 241
 *       (per_cpu(info->flexible, cpu) || (per_cpu(info->cpu_pinned, cpu) /Users/rubber/linux/kernel/events/hw_breakpoint.c: 243
 *           + max(per_cpu(info->tsk_pinned, cpu)))) < HBP_NUM /Users/rubber/linux/kernel/events/hw_breakpoint.c: 244
 *       -> If there are already non-pinned counters in this cpu, it means /Users/rubber/linux/kernel/events/hw_breakpoint.c: 246
 *          there is already a free slot for them. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 247
 *          Otherwise, we check that the maximum number of per task /Users/rubber/linux/kernel/events/hw_breakpoint.c: 248
 *          breakpoints (for this cpu) plus the number of per cpu breakpoint /Users/rubber/linux/kernel/events/hw_breakpoint.c: 249
 *          (for this cpu) doesn't cover every registers. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 250
 *   - If attached to every cpus, check: /Users/rubber/linux/kernel/events/hw_breakpoint.c: 252
 *       (per_cpu(info->flexible, *) || (max(per_cpu(info->cpu_pinned, *)) /Users/rubber/linux/kernel/events/hw_breakpoint.c: 254
 *           + max(per_cpu(info->tsk_pinned, *)))) < HBP_NUM /Users/rubber/linux/kernel/events/hw_breakpoint.c: 255
 *       -> This is roughly the same, except we check the number of per cpu /Users/rubber/linux/kernel/events/hw_breakpoint.c: 257
 *          bp for every cpu and we keep the max one. Same for the per tasks /Users/rubber/linux/kernel/events/hw_breakpoint.c: 258
 *          breakpoints. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 259
 * == Pinned counter == /Users/rubber/linux/kernel/events/hw_breakpoint.c: 262
 *   - If attached to a single cpu, check: /Users/rubber/linux/kernel/events/hw_breakpoint.c: 264
 *       ((per_cpu(info->flexible, cpu) > 1) + per_cpu(info->cpu_pinned, cpu) /Users/rubber/linux/kernel/events/hw_breakpoint.c: 266
 *            + max(per_cpu(info->tsk_pinned, cpu))) < HBP_NUM /Users/rubber/linux/kernel/events/hw_breakpoint.c: 267
 *       -> Same checks as before. But now the info->flexible, if any, must keep /Users/rubber/linux/kernel/events/hw_breakpoint.c: 269
 *          one register at least (or they will never be fed). /Users/rubber/linux/kernel/events/hw_breakpoint.c: 270
 *   - If attached to every cpus, check: /Users/rubber/linux/kernel/events/hw_breakpoint.c: 272
 *       ((per_cpu(info->flexible, *) > 1) + max(per_cpu(info->cpu_pinned, *)) /Users/rubber/linux/kernel/events/hw_breakpoint.c: 274
 *            + max(per_cpu(info->tsk_pinned, *))) < HBP_NUM /Users/rubber/linux/kernel/events/hw_breakpoint.c: 275
	/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 297
	 * Simulate the addition of this breakpoint to the constraints /Users/rubber/linux/kernel/events/hw_breakpoint.c: 298
	 * and see the result. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 299
		/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 359
		 * Reserve the old_type slot back in case /Users/rubber/linux/kernel/events/hw_breakpoint.c: 360
		 * there's no space for the new type. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 361
		 * /Users/rubber/linux/kernel/events/hw_breakpoint.c: 362
		 * This must succeed, because we just released /Users/rubber/linux/kernel/events/hw_breakpoint.c: 363
		 * the old_type slot in the __release_bp_slot /Users/rubber/linux/kernel/events/hw_breakpoint.c: 364
		 * call above. If not, something is broken. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 365
 * Allow the kernel debugger to reserve breakpoint slots without /Users/rubber/linux/kernel/events/hw_breakpoint.c: 384
 * taking a lock using the dbg_* variant of for the reserve and /Users/rubber/linux/kernel/events/hw_breakpoint.c: 385
 * release breakpoint slots. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 386
		/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 419
		 * Don't let unprivileged users set a breakpoint in the trap /Users/rubber/linux/kernel/events/hw_breakpoint.c: 420
		 * path to avoid trap recursion attacks. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 421
 * register_user_hw_breakpoint - register a hardware breakpoint for user space /Users/rubber/linux/kernel/events/hw_breakpoint.c: 451
 * @attr: breakpoint attributes /Users/rubber/linux/kernel/events/hw_breakpoint.c: 452
 * @triggered: callback to trigger when we hit the breakpoint /Users/rubber/linux/kernel/events/hw_breakpoint.c: 453
 * @context: context data could be used in the triggered callback /Users/rubber/linux/kernel/events/hw_breakpoint.c: 454
 * @tsk: pointer to 'task_struct' of the process to which the address belongs /Users/rubber/linux/kernel/events/hw_breakpoint.c: 455
 * modify_user_hw_breakpoint - modify a user-space hardware breakpoint /Users/rubber/linux/kernel/events/hw_breakpoint.c: 510
 * @bp: the breakpoint structure to modify /Users/rubber/linux/kernel/events/hw_breakpoint.c: 511
 * @attr: new breakpoint attributes /Users/rubber/linux/kernel/events/hw_breakpoint.c: 512
	/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 518
	 * modify_user_hw_breakpoint can be invoked with IRQs disabled and hence it /Users/rubber/linux/kernel/events/hw_breakpoint.c: 519
	 * will not be possible to raise IPIs that invoke __perf_event_disable. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 520
	 * So call the function directly after making sure we are targeting the /Users/rubber/linux/kernel/events/hw_breakpoint.c: 521
	 * current task. /Users/rubber/linux/kernel/events/hw_breakpoint.c: 522
 * unregister_hw_breakpoint - unregister a user-space hardware breakpoint /Users/rubber/linux/kernel/events/hw_breakpoint.c: 539
 * @bp: the breakpoint structure to unregister /Users/rubber/linux/kernel/events/hw_breakpoint.c: 540
 * register_wide_hw_breakpoint - register a wide breakpoint in the kernel /Users/rubber/linux/kernel/events/hw_breakpoint.c: 551
 * @attr: breakpoint attributes /Users/rubber/linux/kernel/events/hw_breakpoint.c: 552
 * @triggered: callback to trigger when we hit the breakpoint /Users/rubber/linux/kernel/events/hw_breakpoint.c: 553
 * @context: context data could be used in the triggered callback /Users/rubber/linux/kernel/events/hw_breakpoint.c: 554
 * @return a set of per_cpu pointers to perf events /Users/rubber/linux/kernel/events/hw_breakpoint.c: 556
 * unregister_wide_hw_breakpoint - unregister a wide breakpoint in the kernel /Users/rubber/linux/kernel/events/hw_breakpoint.c: 593
 * @cpu_events: the per cpu set of events to unregister /Users/rubber/linux/kernel/events/hw_breakpoint.c: 594
	/* /Users/rubber/linux/kernel/events/hw_breakpoint.c: 625
	 * no branch sampling for breakpoint events /Users/rubber/linux/kernel/events/hw_breakpoint.c: 626
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/printk/braille.c: 1
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/printk/printk.c: 1
 *  linux/kernel/printk.c /Users/rubber/linux/kernel/printk/printk.c: 3
 *  Copyright (C) 1991, 1992  Linus Torvalds /Users/rubber/linux/kernel/printk/printk.c: 5
 * Modified to make sys_syslog() more flexible: added commands to /Users/rubber/linux/kernel/printk/printk.c: 7
 * return the last 4k of kernel messages, regardless of whether /Users/rubber/linux/kernel/printk/printk.c: 8
 * they've been read or not.  Added option to suppress kernel printk's /Users/rubber/linux/kernel/printk/printk.c: 9
 * to the console.  Added hook for sending the console messages /Users/rubber/linux/kernel/printk/printk.c: 10
 * elsewhere, in preparation for a serial line console (someday). /Users/rubber/linux/kernel/printk/printk.c: 11
 * Ted Ts'o, 2/11/93. /Users/rubber/linux/kernel/printk/printk.c: 12
 * Modified for sysctl support, 1/8/97, Chris Horn. /Users/rubber/linux/kernel/printk/printk.c: 13
 * Fixed SMP synchronization, 08/08/99, Manfred Spraul /Users/rubber/linux/kernel/printk/printk.c: 14
 *     manfred@colorfullife.com /Users/rubber/linux/kernel/printk/printk.c: 15
 * Rewrote bits to get rid of console_lock /Users/rubber/linux/kernel/printk/printk.c: 16
 *	01Mar01 Andrew Morton /Users/rubber/linux/kernel/printk/printk.c: 17
 * Low level drivers may need that to know if they can schedule in /Users/rubber/linux/kernel/printk/printk.c: 75
 * their unblank() callback or not. So let's export it. /Users/rubber/linux/kernel/printk/printk.c: 76
 * console_sem protects the console_drivers list, and also /Users/rubber/linux/kernel/printk/printk.c: 82
 * provides serialisation for access to the entire console /Users/rubber/linux/kernel/printk/printk.c: 83
 * driver system. /Users/rubber/linux/kernel/printk/printk.c: 84
 * System may need to suppress printk message under certain /Users/rubber/linux/kernel/printk/printk.c: 91
 * circumstances, like after kernel panic happens. /Users/rubber/linux/kernel/printk/printk.c: 92
	/* /Users/rubber/linux/kernel/printk/printk.c: 152
	 * Set sysctl string accordingly: /Users/rubber/linux/kernel/printk/printk.c: 153
	/* /Users/rubber/linux/kernel/printk/printk.c: 161
	 * Sysctl cannot change it anymore. The kernel command line setting of /Users/rubber/linux/kernel/printk/printk.c: 162
	 * this parameter is to force the setting to be permanent throughout the /Users/rubber/linux/kernel/printk/printk.c: 163
	 * runtime of the system. This is a precation measure against userspace /Users/rubber/linux/kernel/printk/printk.c: 164
	 * trying to be a smarta** and attempting to change it up on us. /Users/rubber/linux/kernel/printk/printk.c: 165
		/* /Users/rubber/linux/kernel/printk/printk.c: 197
		 * Do not accept an unknown string OR a known string with /Users/rubber/linux/kernel/printk/printk.c: 198
		 * trailing crap... /Users/rubber/linux/kernel/printk/printk.c: 199
 * Helper macros to handle lockdep when locking/unlocking console_sem. We use /Users/rubber/linux/kernel/printk/printk.c: 218
 * macros instead of functions so that _RET_IP_ contains useful information. /Users/rubber/linux/kernel/printk/printk.c: 219
	/* /Users/rubber/linux/kernel/printk/printk.c: 231
	 * Here and in __up_console_sem() we need to be in safe mode, /Users/rubber/linux/kernel/printk/printk.c: 232
	 * because spindump/WARN/etc from under console ->lock will /Users/rubber/linux/kernel/printk/printk.c: 233
	 * deadlock in printk()->down_trylock_console_sem() otherwise. /Users/rubber/linux/kernel/printk/printk.c: 234
 * This is used for debugging the mess that is the VT code by /Users/rubber/linux/kernel/printk/printk.c: 260
 * keeping track if we have the console semaphore held. It's /Users/rubber/linux/kernel/printk/printk.c: 261
 * definitely not the perfect debug tool (we don't know if _WE_ /Users/rubber/linux/kernel/printk/printk.c: 262
 * hold it and are racing, but it helps tracking those weird code /Users/rubber/linux/kernel/printk/printk.c: 263
 * paths in the console code where we end up in places I want /Users/rubber/linux/kernel/printk/printk.c: 264
 * locked without the console semaphore held). /Users/rubber/linux/kernel/printk/printk.c: 265
 * If exclusive_console is non-NULL then only this console is to be printed to. /Users/rubber/linux/kernel/printk/printk.c: 270
 *	Array of consoles built from command line options (console=) /Users/rubber/linux/kernel/printk/printk.c: 275
 * The printk log buffer consists of a sequenced collection of records, each /Users/rubber/linux/kernel/printk/printk.c: 298
 * containing variable length message text. Every record also contains its /Users/rubber/linux/kernel/printk/printk.c: 299
 * own meta-data (@info). /Users/rubber/linux/kernel/printk/printk.c: 300
 * Every record meta-data carries the timestamp in microseconds, as well as /Users/rubber/linux/kernel/printk/printk.c: 302
 * the standard userspace syslog level and syslog facility. The usual kernel /Users/rubber/linux/kernel/printk/printk.c: 303
 * messages use LOG_KERN; userspace-injected messages always carry a matching /Users/rubber/linux/kernel/printk/printk.c: 304
 * syslog facility, by default LOG_USER. The origin of every message can be /Users/rubber/linux/kernel/printk/printk.c: 305
 * reliably determined that way. /Users/rubber/linux/kernel/printk/printk.c: 306
 * The human readable log message of a record is available in @text, the /Users/rubber/linux/kernel/printk/printk.c: 308
 * length of the message text in @text_len. The stored message is not /Users/rubber/linux/kernel/printk/printk.c: 309
 * terminated. /Users/rubber/linux/kernel/printk/printk.c: 310
 * Optionally, a record can carry a dictionary of properties (key/value /Users/rubber/linux/kernel/printk/printk.c: 312
 * pairs), to provide userspace with a machine-readable message context. /Users/rubber/linux/kernel/printk/printk.c: 313
 * Examples for well-defined, commonly used property names are: /Users/rubber/linux/kernel/printk/printk.c: 315
 *   DEVICE=b12:8               device identifier /Users/rubber/linux/kernel/printk/printk.c: 316
 *                                b12:8         block dev_t /Users/rubber/linux/kernel/printk/printk.c: 317
 *                                c127:3        char dev_t /Users/rubber/linux/kernel/printk/printk.c: 318
 *                                n8            netdev ifindex /Users/rubber/linux/kernel/printk/printk.c: 319
 *                                +sound:card0  subsystem:devname /Users/rubber/linux/kernel/printk/printk.c: 320
 *   SUBSYSTEM=pci              driver-core subsystem name /Users/rubber/linux/kernel/printk/printk.c: 321
 * Valid characters in property names are [a-zA-Z0-9.-_]. Property names /Users/rubber/linux/kernel/printk/printk.c: 323
 * and values are terminated by a '\0' character. /Users/rubber/linux/kernel/printk/printk.c: 324
 * Example of record values: /Users/rubber/linux/kernel/printk/printk.c: 326
 *   record.text_buf                = "it's a line" (unterminated) /Users/rubber/linux/kernel/printk/printk.c: 327
 *   record.info.seq                = 56 /Users/rubber/linux/kernel/printk/printk.c: 328
 *   record.info.ts_nsec            = 36863 /Users/rubber/linux/kernel/printk/printk.c: 329
 *   record.info.text_len           = 11 /Users/rubber/linux/kernel/printk/printk.c: 330
 *   record.info.facility           = 0 (LOG_KERN) /Users/rubber/linux/kernel/printk/printk.c: 331
 *   record.info.flags              = 0 /Users/rubber/linux/kernel/printk/printk.c: 332
 *   record.info.level              = 3 (LOG_ERR) /Users/rubber/linux/kernel/printk/printk.c: 333
 *   record.info.caller_id          = 299 (task 299) /Users/rubber/linux/kernel/printk/printk.c: 334
 *   record.info.dev_info.subsystem = "pci" (terminated) /Users/rubber/linux/kernel/printk/printk.c: 335
 *   record.info.dev_info.device    = "+pci:0000:00:01.0" (terminated) /Users/rubber/linux/kernel/printk/printk.c: 336
 * The 'struct printk_info' buffer must never be directly exported to /Users/rubber/linux/kernel/printk/printk.c: 338
 * userspace, it is a kernel-private implementation detail that might /Users/rubber/linux/kernel/printk/printk.c: 339
 * need to be changed in the future, when the requirements change. /Users/rubber/linux/kernel/printk/printk.c: 340
 * /dev/kmsg exports the structured data in the following line format: /Users/rubber/linux/kernel/printk/printk.c: 342
 *   "<level>,<sequnum>,<timestamp>,<contflag>[,additional_values, ... ];<message text>\n" /Users/rubber/linux/kernel/printk/printk.c: 343
 * Users of the export format should ignore possible additional values /Users/rubber/linux/kernel/printk/printk.c: 345
 * separated by ',', and find the message after the ';' character. /Users/rubber/linux/kernel/printk/printk.c: 346
 * The optional key/value pairs are attached as continuation lines starting /Users/rubber/linux/kernel/printk/printk.c: 348
 * with a space character and terminated by a newline. All possible /Users/rubber/linux/kernel/printk/printk.c: 349
 * non-prinatable characters are escaped in the "\xff" notation. /Users/rubber/linux/kernel/printk/printk.c: 350
 * The next printk record to read after the last 'clear' command. There are /Users/rubber/linux/kernel/printk/printk.c: 376
 * two copies (updated with seqcount_latch) so that reads can locklessly /Users/rubber/linux/kernel/printk/printk.c: 377
 * access a valid value. Writers are synchronized by @syslog_lock. /Users/rubber/linux/kernel/printk/printk.c: 378
 * Define the average message size. This only affects the number of /Users/rubber/linux/kernel/printk/printk.c: 410
 * descriptors that will be available. Underestimating is better than /Users/rubber/linux/kernel/printk/printk.c: 411
 * overestimating (too many available descriptors is better than not enough). /Users/rubber/linux/kernel/printk/printk.c: 412
 * We cannot access per-CPU data (e.g. per-CPU flush irq_work) before /Users/rubber/linux/kernel/printk/printk.c: 427
 * per_cpu_areas are initialised. This variable is set to true when /Users/rubber/linux/kernel/printk/printk.c: 428
 * it's safe to access per-CPU data. /Users/rubber/linux/kernel/printk/printk.c: 429
 * Define how much of the log buffer we could take at maximum. The value /Users/rubber/linux/kernel/printk/printk.c: 476
 * must be greater than two. Note that only half of the buffer is available /Users/rubber/linux/kernel/printk/printk.c: 477
 * when the index points to the middle. /Users/rubber/linux/kernel/printk/printk.c: 478
	/* /Users/rubber/linux/kernel/printk/printk.c: 485
	 * The message should not take the whole buffer. Otherwise, it might /Users/rubber/linux/kernel/printk/printk.c: 486
	 * get removed too soon. /Users/rubber/linux/kernel/printk/printk.c: 487
	/* /Users/rubber/linux/kernel/printk/printk.c: 508
	 * Unless restricted, we allow "read all" and "get buffer size" /Users/rubber/linux/kernel/printk/printk.c: 509
	 * for everybody. /Users/rubber/linux/kernel/printk/printk.c: 510
	/* /Users/rubber/linux/kernel/printk/printk.c: 518
	 * If this is from /proc/kmsg and we've already opened it, then we've /Users/rubber/linux/kernel/printk/printk.c: 519
	 * already done the capabilities checks at open time. /Users/rubber/linux/kernel/printk/printk.c: 520
		/* /Users/rubber/linux/kernel/printk/printk.c: 528
		 * For historical reasons, accept CAP_SYS_ADMIN too, with /Users/rubber/linux/kernel/printk/printk.c: 529
		 * a warning. /Users/rubber/linux/kernel/printk/printk.c: 530
	/* /Users/rubber/linux/kernel/printk/printk.c: 686
	 * Extract and skip the syslog prefix <[0-9]*>. Coming from userspace /Users/rubber/linux/kernel/printk/printk.c: 687
	 * the decimal value represents 32bit, the lower 3 bit are the log /Users/rubber/linux/kernel/printk/printk.c: 688
	 * level, the rest are the log facility. /Users/rubber/linux/kernel/printk/printk.c: 689
	 * /Users/rubber/linux/kernel/printk/printk.c: 690
	 * If no prefix or no userspace facility is specified, we /Users/rubber/linux/kernel/printk/printk.c: 691
	 * enforce LOG_USER, to be able to reliably distinguish /Users/rubber/linux/kernel/printk/printk.c: 692
	 * kernel-generated messages from userspace-injected ones. /Users/rubber/linux/kernel/printk/printk.c: 693
 * Be careful when modifying this function!!! /Users/rubber/linux/kernel/printk/printk.c: 772
 * Only few operations are supported because the device works only with the /Users/rubber/linux/kernel/printk/printk.c: 774
 * entire variable length messages (records). Non-standard values are /Users/rubber/linux/kernel/printk/printk.c: 775
 * returned in the other cases and has been this way for quite some time. /Users/rubber/linux/kernel/printk/printk.c: 776
 * User space applications might depend on this behavior. /Users/rubber/linux/kernel/printk/printk.c: 777
		/* /Users/rubber/linux/kernel/printk/printk.c: 795
		 * The first record after the last SYSLOG_ACTION_CLEAR, /Users/rubber/linux/kernel/printk/printk.c: 796
		 * like issued by 'dmesg -c'. Reading /dev/kmsg itself /Users/rubber/linux/kernel/printk/printk.c: 797
		 * changes no global state, and does not clear anything. /Users/rubber/linux/kernel/printk/printk.c: 798
 * This appends the listed symbols to /proc/vmcore /Users/rubber/linux/kernel/printk/printk.c: 893
 * /proc/vmcore is used by various utilities, like crash and makedumpfile to /Users/rubber/linux/kernel/printk/printk.c: 895
 * obtain access to symbols that are otherwise very difficult to locate.  These /Users/rubber/linux/kernel/printk/printk.c: 896
 * symbols are specifically used so that utilities can access and extract the /Users/rubber/linux/kernel/printk/printk.c: 897
 * dmesg log from a vmcore file after a crash. /Users/rubber/linux/kernel/printk/printk.c: 898
	/* /Users/rubber/linux/kernel/printk/printk.c: 908
	 * Export struct size and field offsets. User space tools can /Users/rubber/linux/kernel/printk/printk.c: 909
	 * parse it and detect any changes to structure down the line. /Users/rubber/linux/kernel/printk/printk.c: 910
	/* /Users/rubber/linux/kernel/printk/printk.c: 1000
	 * archs should set up cpu_possible_bits properly with /Users/rubber/linux/kernel/printk/printk.c: 1001
	 * set_cpu_possible() after setup_arch() but just in /Users/rubber/linux/kernel/printk/printk.c: 1002
	 * case lets ensure this is valid. /Users/rubber/linux/kernel/printk/printk.c: 1003
	/* /Users/rubber/linux/kernel/printk/printk.c: 1073
	 * Some archs call setup_log_buf() multiple times - first is very /Users/rubber/linux/kernel/printk/printk.c: 1074
	 * early, e.g. from setup_arch(), and second - when percpu_areas /Users/rubber/linux/kernel/printk/printk.c: 1075
	 * are initialised. /Users/rubber/linux/kernel/printk/printk.c: 1076
	/* /Users/rubber/linux/kernel/printk/printk.c: 1145
	 * Copy any remaining messages that might have appeared from /Users/rubber/linux/kernel/printk/printk.c: 1146
	 * NMI context after copying but before switching to the /Users/rubber/linux/kernel/printk/printk.c: 1147
	 * dynamic buffer. /Users/rubber/linux/kernel/printk/printk.c: 1148
		/* /Users/rubber/linux/kernel/printk/printk.c: 1233
		 * use (volatile) jiffies to prevent /Users/rubber/linux/kernel/printk/printk.c: 1234
		 * compiler reduction; loop termination via jiffies /Users/rubber/linux/kernel/printk/printk.c: 1235
		 * is secondary and may or may not happen. /Users/rubber/linux/kernel/printk/printk.c: 1236
 * Prepare the record for printing. The text is shifted within the given /Users/rubber/linux/kernel/printk/printk.c: 1300
 * buffer to avoid a need for another one. The following operations are /Users/rubber/linux/kernel/printk/printk.c: 1301
 * done: /Users/rubber/linux/kernel/printk/printk.c: 1302
 *   - Add prefix for each line. /Users/rubber/linux/kernel/printk/printk.c: 1304
 *   - Drop truncated lines that no longer fit into the buffer. /Users/rubber/linux/kernel/printk/printk.c: 1305
 *   - Add the trailing newline that has been removed in vprintk_store(). /Users/rubber/linux/kernel/printk/printk.c: 1306
 *   - Add a string terminator. /Users/rubber/linux/kernel/printk/printk.c: 1307
 * Since the produced string is always terminated, the maximum possible /Users/rubber/linux/kernel/printk/printk.c: 1309
 * return value is @r->text_buf_size - 1; /Users/rubber/linux/kernel/printk/printk.c: 1310
 * Return: The length of the updated/prepared text, including the added /Users/rubber/linux/kernel/printk/printk.c: 1312
 * prefixes and the newline. The terminator is not counted. The dropped /Users/rubber/linux/kernel/printk/printk.c: 1313
 * line(s) are not counted. /Users/rubber/linux/kernel/printk/printk.c: 1314
	/* /Users/rubber/linux/kernel/printk/printk.c: 1329
	 * If the message was truncated because the buffer was not large /Users/rubber/linux/kernel/printk/printk.c: 1330
	 * enough, treat the available text as if it were the full text. /Users/rubber/linux/kernel/printk/printk.c: 1331
	/* /Users/rubber/linux/kernel/printk/printk.c: 1338
	 * @text_len: bytes of unprocessed text /Users/rubber/linux/kernel/printk/printk.c: 1339
	 * @line_len: bytes of current line _without_ newline /Users/rubber/linux/kernel/printk/printk.c: 1340
	 * @text:     pointer to beginning of current line /Users/rubber/linux/kernel/printk/printk.c: 1341
	 * @len:      number of bytes prepared in r->text_buf /Users/rubber/linux/kernel/printk/printk.c: 1342
		/* /Users/rubber/linux/kernel/printk/printk.c: 1355
		 * Truncate the text if there is not enough space to add the /Users/rubber/linux/kernel/printk/printk.c: 1356
		 * prefix and a trailing newline and a terminator. /Users/rubber/linux/kernel/printk/printk.c: 1357
		/* /Users/rubber/linux/kernel/printk/printk.c: 1371
		 * Increment the prepared length to include the text and /Users/rubber/linux/kernel/printk/printk.c: 1372
		 * prefix that were just moved+copied. Also increment for the /Users/rubber/linux/kernel/printk/printk.c: 1373
		 * newline at the end of this line. If this is the last line, /Users/rubber/linux/kernel/printk/printk.c: 1374
		 * there is no newline, but it will be added immediately below. /Users/rubber/linux/kernel/printk/printk.c: 1375
			/* /Users/rubber/linux/kernel/printk/printk.c: 1379
			 * This is the last line. Add the trailing newline /Users/rubber/linux/kernel/printk/printk.c: 1380
			 * removed in vprintk_store(). /Users/rubber/linux/kernel/printk/printk.c: 1381
		/* /Users/rubber/linux/kernel/printk/printk.c: 1387
		 * Advance beyond the added prefix and the related line with /Users/rubber/linux/kernel/printk/printk.c: 1388
		 * its newline. /Users/rubber/linux/kernel/printk/printk.c: 1389
		/* /Users/rubber/linux/kernel/printk/printk.c: 1393
		 * The remaining text has only decreased by the line with its /Users/rubber/linux/kernel/printk/printk.c: 1394
		 * newline. /Users/rubber/linux/kernel/printk/printk.c: 1395
		 * /Users/rubber/linux/kernel/printk/printk.c: 1396
		 * Note that @text_len can become zero. It happens when @text /Users/rubber/linux/kernel/printk/printk.c: 1397
		 * ended with a newline (either due to truncation or the /Users/rubber/linux/kernel/printk/printk.c: 1398
		 * original string ending with "\n\n"). The loop is correctly /Users/rubber/linux/kernel/printk/printk.c: 1399
		 * repeated and (if not truncated) an empty line with a prefix /Users/rubber/linux/kernel/printk/printk.c: 1400
		 * will be prepared. /Users/rubber/linux/kernel/printk/printk.c: 1401
	/* /Users/rubber/linux/kernel/printk/printk.c: 1406
	 * If a buffer was provided, it will be terminated. Space for the /Users/rubber/linux/kernel/printk/printk.c: 1407
	 * string terminator is guaranteed to be available. The terminator is /Users/rubber/linux/kernel/printk/printk.c: 1408
	 * not counted in the return value. /Users/rubber/linux/kernel/printk/printk.c: 1409
	/* /Users/rubber/linux/kernel/printk/printk.c: 1426
	 * Each line will be preceded with a prefix. The intermediate /Users/rubber/linux/kernel/printk/printk.c: 1427
	 * newlines are already within the text, but a final trailing /Users/rubber/linux/kernel/printk/printk.c: 1428
	 * newline will be added. /Users/rubber/linux/kernel/printk/printk.c: 1429
 * Beginning with @start_seq, find the first record where it and all following /Users/rubber/linux/kernel/printk/printk.c: 1435
 * records up to (but not including) @max_seq fit into @size. /Users/rubber/linux/kernel/printk/printk.c: 1436
 * @max_seq is simply an upper bound and does not need to exist. If the caller /Users/rubber/linux/kernel/printk/printk.c: 1438
 * does not require an upper bound, -1 can be used for @max_seq. /Users/rubber/linux/kernel/printk/printk.c: 1439
	/* /Users/rubber/linux/kernel/printk/printk.c: 1456
	 * Adjust the upper bound for the next loop to avoid subtracting /Users/rubber/linux/kernel/printk/printk.c: 1457
	 * lengths that were never added. /Users/rubber/linux/kernel/printk/printk.c: 1458
	/* /Users/rubber/linux/kernel/printk/printk.c: 1463
	 * Move first record forward until length fits into the buffer. Ignore /Users/rubber/linux/kernel/printk/printk.c: 1464
	 * newest messages that were not counted in the above cycle. Messages /Users/rubber/linux/kernel/printk/printk.c: 1465
	 * might appear and get lost in the meantime. This is a best effort /Users/rubber/linux/kernel/printk/printk.c: 1466
	 * that prevents an infinite loop that could occur with a retry. /Users/rubber/linux/kernel/printk/printk.c: 1467
	/* /Users/rubber/linux/kernel/printk/printk.c: 1495
	 * Wait for the @syslog_seq record to be available. @syslog_seq may /Users/rubber/linux/kernel/printk/printk.c: 1496
	 * change while waiting. /Users/rubber/linux/kernel/printk/printk.c: 1497
	/* /Users/rubber/linux/kernel/printk/printk.c: 1510
	 * Copy records that fit into the buffer. The above cycle makes sure /Users/rubber/linux/kernel/printk/printk.c: 1511
	 * that the first record is always available. /Users/rubber/linux/kernel/printk/printk.c: 1512
		/* /Users/rubber/linux/kernel/printk/printk.c: 1528
		 * To keep reading/counting partial line consistent, /Users/rubber/linux/kernel/printk/printk.c: 1529
		 * use printk_time value as of the beginning of a line. /Users/rubber/linux/kernel/printk/printk.c: 1530
	/* /Users/rubber/linux/kernel/printk/printk.c: 1586
	 * Find first record that fits, including all following records, /Users/rubber/linux/kernel/printk/printk.c: 1587
	 * into the user-provided buffer for this dump. /Users/rubber/linux/kernel/printk/printk.c: 1588
			/* /Users/rubber/linux/kernel/printk/printk.c: 1712
			 * Short-cut for poll(/"proc/kmsg") which simply checks /Users/rubber/linux/kernel/printk/printk.c: 1713
			 * for pending data, not the size; return the count of /Users/rubber/linux/kernel/printk/printk.c: 1714
			 * records, not the length. /Users/rubber/linux/kernel/printk/printk.c: 1715
 * Special console_lock variants that help to reduce the risk of soft-lockups. /Users/rubber/linux/kernel/printk/printk.c: 1751
 * They allow to pass console_lock to another printk() call using a busy wait. /Users/rubber/linux/kernel/printk/printk.c: 1752
 * console_lock_spinning_enable - mark beginning of code where another /Users/rubber/linux/kernel/printk/printk.c: 1766
 *	thread might safely busy wait /Users/rubber/linux/kernel/printk/printk.c: 1767
 * This basically converts console_lock into a spinlock. This marks /Users/rubber/linux/kernel/printk/printk.c: 1769
 * the section where the console_lock owner can not sleep, because /Users/rubber/linux/kernel/printk/printk.c: 1770
 * there may be a waiter spinning (like a spinlock). Also it must be /Users/rubber/linux/kernel/printk/printk.c: 1771
 * ready to hand over the lock at the end of the section. /Users/rubber/linux/kernel/printk/printk.c: 1772
 * console_lock_spinning_disable_and_check - mark end of code where another /Users/rubber/linux/kernel/printk/printk.c: 1785
 *	thread was able to busy wait and check if there is a waiter /Users/rubber/linux/kernel/printk/printk.c: 1786
 * This is called at the end of the section where spinning is allowed. /Users/rubber/linux/kernel/printk/printk.c: 1788
 * It has two functions. First, it is a signal that it is no longer /Users/rubber/linux/kernel/printk/printk.c: 1789
 * safe to start busy waiting for the lock. Second, it checks if /Users/rubber/linux/kernel/printk/printk.c: 1790
 * there is a busy waiter and passes the lock rights to her. /Users/rubber/linux/kernel/printk/printk.c: 1791
 * Important: Callers lose the lock if there was a busy waiter. /Users/rubber/linux/kernel/printk/printk.c: 1793
 *	They must not touch items synchronized by console_lock /Users/rubber/linux/kernel/printk/printk.c: 1794
 *	in this case. /Users/rubber/linux/kernel/printk/printk.c: 1795
 * Return: 1 if the lock rights were passed, 0 otherwise. /Users/rubber/linux/kernel/printk/printk.c: 1797
	/* /Users/rubber/linux/kernel/printk/printk.c: 1818
	 * Hand off console_lock to waiter. The waiter will perform /Users/rubber/linux/kernel/printk/printk.c: 1819
	 * the up(). After this, the waiter is the console_lock owner. /Users/rubber/linux/kernel/printk/printk.c: 1820
 * console_trylock_spinning - try to get console_lock by busy waiting /Users/rubber/linux/kernel/printk/printk.c: 1827
 * This allows to busy wait for the console_lock when the current /Users/rubber/linux/kernel/printk/printk.c: 1829
 * owner is running in specially marked sections. It means that /Users/rubber/linux/kernel/printk/printk.c: 1830
 * the current owner is running and cannot reschedule until it /Users/rubber/linux/kernel/printk/printk.c: 1831
 * is ready to lose the lock. /Users/rubber/linux/kernel/printk/printk.c: 1832
 * Return: 1 if we got the lock, 0 othrewise /Users/rubber/linux/kernel/printk/printk.c: 1834
	/* /Users/rubber/linux/kernel/printk/printk.c: 1857
	 * If there is an active printk() writing to the /Users/rubber/linux/kernel/printk/printk.c: 1858
	 * consoles, instead of having it write our data too, /Users/rubber/linux/kernel/printk/printk.c: 1859
	 * see if we can offload that load from the active /Users/rubber/linux/kernel/printk/printk.c: 1860
	 * printer, and do some printing ourselves. /Users/rubber/linux/kernel/printk/printk.c: 1861
	 * Go into a spin only if there isn't already a waiter /Users/rubber/linux/kernel/printk/printk.c: 1862
	 * spinning, and there is an active printer, and /Users/rubber/linux/kernel/printk/printk.c: 1863
	 * that active printer isn't us (recursive printk?). /Users/rubber/linux/kernel/printk/printk.c: 1864
	/* /Users/rubber/linux/kernel/printk/printk.c: 1879
	 * The owner passed the console lock to us. /Users/rubber/linux/kernel/printk/printk.c: 1880
	 * Since we did not spin on console lock, annotate /Users/rubber/linux/kernel/printk/printk.c: 1881
	 * this as a trylock. Otherwise lockdep will /Users/rubber/linux/kernel/printk/printk.c: 1882
	 * complain. /Users/rubber/linux/kernel/printk/printk.c: 1883
 * Call the console drivers, asking them to write out /Users/rubber/linux/kernel/printk/printk.c: 1891
 * log_buf[start] to log_buf[end - 1]. /Users/rubber/linux/kernel/printk/printk.c: 1892
 * The console_lock must be held. /Users/rubber/linux/kernel/printk/printk.c: 1893
 * Recursion is tracked separately on each CPU. If NMIs are supported, an /Users/rubber/linux/kernel/printk/printk.c: 1935
 * additional NMI context per CPU is also separately tracked. Until per-CPU /Users/rubber/linux/kernel/printk/printk.c: 1936
 * is available, a separate "early tracking" is performed. /Users/rubber/linux/kernel/printk/printk.c: 1937
 * Recursion is limited to keep the output sane. printk() should not require /Users/rubber/linux/kernel/printk/printk.c: 1947
 * more than 1 level of recursion (allowing, for example, printk() to trigger /Users/rubber/linux/kernel/printk/printk.c: 1948
 * a WARN), but a higher value is used in case some printk-internal errors /Users/rubber/linux/kernel/printk/printk.c: 1949
 * exist, such as the ringbuffer validation checks failing. /Users/rubber/linux/kernel/printk/printk.c: 1950
 * Return a pointer to the dedicated counter for the CPU+context of the /Users/rubber/linux/kernel/printk/printk.c: 1955
 * caller. /Users/rubber/linux/kernel/printk/printk.c: 1956
 * Enter recursion tracking. Interrupts are disabled to simplify tracking. /Users/rubber/linux/kernel/printk/printk.c: 1973
 * The caller must check the boolean return value to see if the recursion is /Users/rubber/linux/kernel/printk/printk.c: 1974
 * allowed. On failure, interrupts are not disabled. /Users/rubber/linux/kernel/printk/printk.c: 1975
 * @recursion_ptr must be a variable of type (u8 *) and is the same variable /Users/rubber/linux/kernel/printk/printk.c: 1977
 * that is passed to printk_exit_irqrestore(). /Users/rubber/linux/kernel/printk/printk.c: 1978
 * printk_parse_prefix - Parse level and control flags. /Users/rubber/linux/kernel/printk/printk.c: 2025
 * @text:     The terminated text message. /Users/rubber/linux/kernel/printk/printk.c: 2027
 * @level:    A pointer to the current level value, will be updated. /Users/rubber/linux/kernel/printk/printk.c: 2028
 * @flags:    A pointer to the current printk_info flags, will be updated. /Users/rubber/linux/kernel/printk/printk.c: 2029
 * @level may be NULL if the caller is not interested in the parsed value. /Users/rubber/linux/kernel/printk/printk.c: 2031
 * Otherwise the variable pointed to by @level must be set to /Users/rubber/linux/kernel/printk/printk.c: 2032
 * LOGLEVEL_DEFAULT in order to be updated with the parsed value. /Users/rubber/linux/kernel/printk/printk.c: 2033
 * @flags may be NULL if the caller is not interested in the parsed value. /Users/rubber/linux/kernel/printk/printk.c: 2035
 * Otherwise the variable pointed to by @flags will be OR'd with the parsed /Users/rubber/linux/kernel/printk/printk.c: 2036
 * value. /Users/rubber/linux/kernel/printk/printk.c: 2037
 * Return: The length of the parsed level and control flags. /Users/rubber/linux/kernel/printk/printk.c: 2039
	/* /Users/rubber/linux/kernel/printk/printk.c: 2117
	 * Since the duration of printk() can vary depending on the message /Users/rubber/linux/kernel/printk/printk.c: 2118
	 * and state of the ringbuffer, grab the timestamp now so that it is /Users/rubber/linux/kernel/printk/printk.c: 2119
	 * close to the call of printk(). This provides a more deterministic /Users/rubber/linux/kernel/printk/printk.c: 2120
	 * timestamp with respect to the caller. /Users/rubber/linux/kernel/printk/printk.c: 2121
	/* /Users/rubber/linux/kernel/printk/printk.c: 2128
	 * The sprintf needs to come first since the syslog prefix might be /Users/rubber/linux/kernel/printk/printk.c: 2129
	 * passed in as a parameter. An extra byte must be reserved so that /Users/rubber/linux/kernel/printk/printk.c: 2130
	 * later the vscnprintf() into the reserved buffer has room for the /Users/rubber/linux/kernel/printk/printk.c: 2131
	 * terminating '\0', which is not counted by vsnprintf(). /Users/rubber/linux/kernel/printk/printk.c: 2132
	/* /Users/rubber/linux/kernel/printk/printk.c: 2170
	 * Explicitly initialize the record before every prb_reserve() call. /Users/rubber/linux/kernel/printk/printk.c: 2171
	 * prb_reserve_in_last() and prb_reserve() purposely invalidate the /Users/rubber/linux/kernel/printk/printk.c: 2172
	 * structure when they fail. /Users/rubber/linux/kernel/printk/printk.c: 2173
		/* /Users/rubber/linux/kernel/printk/printk.c: 2233
		 * Disable preemption to avoid being preempted while holding /Users/rubber/linux/kernel/printk/printk.c: 2234
		 * console_sem which would prevent anyone from printing to /Users/rubber/linux/kernel/printk/printk.c: 2235
		 * console /Users/rubber/linux/kernel/printk/printk.c: 2236
		/* /Users/rubber/linux/kernel/printk/printk.c: 2239
		 * Try to acquire and then immediately release the console /Users/rubber/linux/kernel/printk/printk.c: 2240
		 * semaphore.  The release will print out buffers and wake up /Users/rubber/linux/kernel/printk/printk.c: 2241
		 * /dev/kmsg and syslog() users. /Users/rubber/linux/kernel/printk/printk.c: 2242
	/* /Users/rubber/linux/kernel/printk/printk.c: 2333
	 *	See if this tty is not yet registered, and /Users/rubber/linux/kernel/printk/printk.c: 2334
	 *	if we have a slot free. /Users/rubber/linux/kernel/printk/printk.c: 2335
 * Set up a console.  Called via do_early_param() in init/main.c /Users/rubber/linux/kernel/printk/printk.c: 2372
 * for each "console=" parameter in the boot command line. /Users/rubber/linux/kernel/printk/printk.c: 2373
	/* /Users/rubber/linux/kernel/printk/printk.c: 2381
	 * console="" or console=null have been suggested as a way to /Users/rubber/linux/kernel/printk/printk.c: 2382
	 * disable console output. Use ttynull that has been created /Users/rubber/linux/kernel/printk/printk.c: 2383
	 * for exactly this purpose. /Users/rubber/linux/kernel/printk/printk.c: 2384
	/* /Users/rubber/linux/kernel/printk/printk.c: 2394
	 * Decode str into name, index, options. /Users/rubber/linux/kernel/printk/printk.c: 2395
 * add_preferred_console - add a device to the list of preferred consoles. /Users/rubber/linux/kernel/printk/printk.c: 2426
 * @name: device name /Users/rubber/linux/kernel/printk/printk.c: 2427
 * @idx: device index /Users/rubber/linux/kernel/printk/printk.c: 2428
 * @options: options for this console /Users/rubber/linux/kernel/printk/printk.c: 2429
 * The last preferred console added will be used for kernel messages /Users/rubber/linux/kernel/printk/printk.c: 2431
 * and stdin/out/err for init.  Normally this is used by console_setup /Users/rubber/linux/kernel/printk/printk.c: 2432
 * above to handle user-supplied console arguments; however it can also /Users/rubber/linux/kernel/printk/printk.c: 2433
 * be used by arch-specific code either to override the user or more /Users/rubber/linux/kernel/printk/printk.c: 2434
 * commonly to provide a default console (ie from PROM variables) when /Users/rubber/linux/kernel/printk/printk.c: 2435
 * the user has not supplied one. /Users/rubber/linux/kernel/printk/printk.c: 2436
 * suspend_console - suspend the console subsystem /Users/rubber/linux/kernel/printk/printk.c: 2470
 * This disables printk() while we go into suspend states /Users/rubber/linux/kernel/printk/printk.c: 2472
 * console_cpu_notify - print deferred console messages after CPU hotplug /Users/rubber/linux/kernel/printk/printk.c: 2494
 * @cpu: unused /Users/rubber/linux/kernel/printk/printk.c: 2495
 * If printk() is called from a CPU that is not online yet, the messages /Users/rubber/linux/kernel/printk/printk.c: 2497
 * will be printed on the console only if there are CON_ANYTIME consoles. /Users/rubber/linux/kernel/printk/printk.c: 2498
 * This function is called when a new CPU comes online (or fails to come /Users/rubber/linux/kernel/printk/printk.c: 2499
 * up) or goes offline. /Users/rubber/linux/kernel/printk/printk.c: 2500
 * console_lock - lock the console system for exclusive use. /Users/rubber/linux/kernel/printk/printk.c: 2513
 * Acquires a lock which guarantees that the caller has /Users/rubber/linux/kernel/printk/printk.c: 2515
 * exclusive access to the console system and the console_drivers list. /Users/rubber/linux/kernel/printk/printk.c: 2516
 * Can sleep, returns nothing. /Users/rubber/linux/kernel/printk/printk.c: 2518
 * console_trylock - try to lock the console system for exclusive use. /Users/rubber/linux/kernel/printk/printk.c: 2533
 * Try to acquire a lock which guarantees that the caller has exclusive /Users/rubber/linux/kernel/printk/printk.c: 2535
 * access to the console system and the console_drivers list. /Users/rubber/linux/kernel/printk/printk.c: 2536
 * returns 1 on success, and 0 on failure to acquire the lock. /Users/rubber/linux/kernel/printk/printk.c: 2538
 * Check if we have any console that is capable of printing while cpu is /Users/rubber/linux/kernel/printk/printk.c: 2561
 * booting or shutting down. Requires console_sem. /Users/rubber/linux/kernel/printk/printk.c: 2562
 * Can we actually use the console at this time on this cpu? /Users/rubber/linux/kernel/printk/printk.c: 2577
 * Console drivers may assume that per-cpu resources have been allocated. So /Users/rubber/linux/kernel/printk/printk.c: 2579
 * unless they're explicitly marked as being able to cope (CON_ANYTIME) don't /Users/rubber/linux/kernel/printk/printk.c: 2580
 * call them until this CPU is officially up. /Users/rubber/linux/kernel/printk/printk.c: 2581
 * console_unlock - unlock the console system /Users/rubber/linux/kernel/printk/printk.c: 2589
 * Releases the console_lock which the caller holds on the console system /Users/rubber/linux/kernel/printk/printk.c: 2591
 * and the console driver list. /Users/rubber/linux/kernel/printk/printk.c: 2592
 * While the console_lock was held, console output may have been buffered /Users/rubber/linux/kernel/printk/printk.c: 2594
 * by printk().  If this is the case, console_unlock(); emits /Users/rubber/linux/kernel/printk/printk.c: 2595
 * the output prior to releasing the lock. /Users/rubber/linux/kernel/printk/printk.c: 2596
 * If there is output waiting, we wake /dev/kmsg and syslog() users. /Users/rubber/linux/kernel/printk/printk.c: 2598
 * console_unlock(); may be called from any context. /Users/rubber/linux/kernel/printk/printk.c: 2600
	/* /Users/rubber/linux/kernel/printk/printk.c: 2619
	 * Console drivers are called with interrupts disabled, so /Users/rubber/linux/kernel/printk/printk.c: 2620
	 * @console_may_schedule should be cleared before; however, we may /Users/rubber/linux/kernel/printk/printk.c: 2621
	 * end up dumping a lot of lines, for example, if called from /Users/rubber/linux/kernel/printk/printk.c: 2622
	 * console registration path, and should invoke cond_resched() /Users/rubber/linux/kernel/printk/printk.c: 2623
	 * between lines if allowable.  Not doing so can cause a very long /Users/rubber/linux/kernel/printk/printk.c: 2624
	 * scheduling stall on a slow console leading to RCU stall and /Users/rubber/linux/kernel/printk/printk.c: 2625
	 * softlockup warnings which exacerbate the issue with more /Users/rubber/linux/kernel/printk/printk.c: 2626
	 * messages practically incapacitating the system. /Users/rubber/linux/kernel/printk/printk.c: 2627
	 * /Users/rubber/linux/kernel/printk/printk.c: 2628
	 * console_trylock() is not able to detect the preemptive /Users/rubber/linux/kernel/printk/printk.c: 2629
	 * context reliably. Therefore the value must be stored before /Users/rubber/linux/kernel/printk/printk.c: 2630
	 * and cleared after the "again" goto label. /Users/rubber/linux/kernel/printk/printk.c: 2631
	/* /Users/rubber/linux/kernel/printk/printk.c: 2637
	 * We released the console_sem lock, so we need to recheck if /Users/rubber/linux/kernel/printk/printk.c: 2638
	 * cpu is online and (if not) is there at least one CON_ANYTIME /Users/rubber/linux/kernel/printk/printk.c: 2639
	 * console. /Users/rubber/linux/kernel/printk/printk.c: 2640
			/* /Users/rubber/linux/kernel/printk/printk.c: 2663
			 * Skip record we have buffered and already printed /Users/rubber/linux/kernel/printk/printk.c: 2664
			 * directly to the console when we received it, and /Users/rubber/linux/kernel/printk/printk.c: 2665
			 * record that has level above the console loglevel. /Users/rubber/linux/kernel/printk/printk.c: 2666
		/* /Users/rubber/linux/kernel/printk/printk.c: 2678
		 * Handle extended console text first because later /Users/rubber/linux/kernel/printk/printk.c: 2679
		 * record_print_text() will modify the record buffer in-place. /Users/rubber/linux/kernel/printk/printk.c: 2680
		/* /Users/rubber/linux/kernel/printk/printk.c: 2697
		 * While actively printing out messages, if another printk() /Users/rubber/linux/kernel/printk/printk.c: 2698
		 * were to occur on another CPU, it may wait for this one to /Users/rubber/linux/kernel/printk/printk.c: 2699
		 * finish. This task can not be preempted if there is a /Users/rubber/linux/kernel/printk/printk.c: 2700
		 * waiter waiting to take over. /Users/rubber/linux/kernel/printk/printk.c: 2701
		 * /Users/rubber/linux/kernel/printk/printk.c: 2702
		 * Interrupts are disabled because the hand over to a waiter /Users/rubber/linux/kernel/printk/printk.c: 2703
		 * must not be interrupted until the hand over is completed /Users/rubber/linux/kernel/printk/printk.c: 2704
		 * (@console_waiter is cleared). /Users/rubber/linux/kernel/printk/printk.c: 2705
	/* /Users/rubber/linux/kernel/printk/printk.c: 2729
	 * Someone could have filled up the buffer again, so re-check if there's /Users/rubber/linux/kernel/printk/printk.c: 2730
	 * something to flush. In case we cannot trylock the console_sem again, /Users/rubber/linux/kernel/printk/printk.c: 2731
	 * there's a new owner and the console_unlock() from them will do the /Users/rubber/linux/kernel/printk/printk.c: 2732
	 * flush, no worries. /Users/rubber/linux/kernel/printk/printk.c: 2733
 * console_conditional_schedule - yield the CPU if required /Users/rubber/linux/kernel/printk/printk.c: 2742
 * If the console code is currently allowed to sleep, and /Users/rubber/linux/kernel/printk/printk.c: 2744
 * if this CPU should yield the CPU to another task, do /Users/rubber/linux/kernel/printk/printk.c: 2745
 * so here. /Users/rubber/linux/kernel/printk/printk.c: 2746
 * Must be called within console_lock();. /Users/rubber/linux/kernel/printk/printk.c: 2748
	/* /Users/rubber/linux/kernel/printk/printk.c: 2761
	 * console_unblank can no longer be called in interrupt context unless /Users/rubber/linux/kernel/printk/printk.c: 2762
	 * oops_in_progress is set to 1.. /Users/rubber/linux/kernel/printk/printk.c: 2763
 * console_flush_on_panic - flush console content on panic /Users/rubber/linux/kernel/printk/printk.c: 2780
 * @mode: flush all messages in buffer or just the pending ones /Users/rubber/linux/kernel/printk/printk.c: 2781
 * Immediately output all pending messages no matter what. /Users/rubber/linux/kernel/printk/printk.c: 2783
	/* /Users/rubber/linux/kernel/printk/printk.c: 2787
	 * If someone else is holding the console lock, trylock will fail /Users/rubber/linux/kernel/printk/printk.c: 2788
	 * and may_schedule may be set.  Ignore and proceed to unlock so /Users/rubber/linux/kernel/printk/printk.c: 2789
	 * that messages are flushed out.  As this can be called from any /Users/rubber/linux/kernel/printk/printk.c: 2790
	 * context and we don't want to get preempted while flushing, /Users/rubber/linux/kernel/printk/printk.c: 2791
	 * ensure may_schedule is cleared. /Users/rubber/linux/kernel/printk/printk.c: 2792
 * Return the console tty driver structure and its associated index /Users/rubber/linux/kernel/printk/printk.c: 2803
 * Prevent further output on the passed console device so that (for example) /Users/rubber/linux/kernel/printk/printk.c: 2823
 * serial drivers can disable console output before suspending a port, and can /Users/rubber/linux/kernel/printk/printk.c: 2824
 * re-enable output afterwards. /Users/rubber/linux/kernel/printk/printk.c: 2825
 * This is called by register_console() to try to match /Users/rubber/linux/kernel/printk/printk.c: 2856
 * the newly registered console with any of the ones selected /Users/rubber/linux/kernel/printk/printk.c: 2857
 * by either the command line or add_preferred_console() and /Users/rubber/linux/kernel/printk/printk.c: 2858
 * setup/enable it. /Users/rubber/linux/kernel/printk/printk.c: 2859
 * Care need to be taken with consoles that are statically /Users/rubber/linux/kernel/printk/printk.c: 2861
 * enabled such as netconsole /Users/rubber/linux/kernel/printk/printk.c: 2862
	/* /Users/rubber/linux/kernel/printk/printk.c: 2901
	 * Some consoles, such as pstore and netconsole, can be enabled even /Users/rubber/linux/kernel/printk/printk.c: 2902
	 * without matching. Accept the pre-enabled consoles only when match() /Users/rubber/linux/kernel/printk/printk.c: 2903
	 * and setup() had a chance to be called. /Users/rubber/linux/kernel/printk/printk.c: 2904
 * The console driver calls this routine during kernel initialization /Users/rubber/linux/kernel/printk/printk.c: 2913
 * to register the console printing procedure with printk() and to /Users/rubber/linux/kernel/printk/printk.c: 2914
 * print any messages that were printed by the kernel before the /Users/rubber/linux/kernel/printk/printk.c: 2915
 * console driver was initialized. /Users/rubber/linux/kernel/printk/printk.c: 2916
 * This can happen pretty early during the boot process (because of /Users/rubber/linux/kernel/printk/printk.c: 2918
 * early_printk) - sometimes before setup_arch() completes - be careful /Users/rubber/linux/kernel/printk/printk.c: 2919
 * of what kernel features are used - they may not be initialised yet. /Users/rubber/linux/kernel/printk/printk.c: 2920
 * There are two types of consoles - bootconsoles (early_printk) and /Users/rubber/linux/kernel/printk/printk.c: 2922
 * "real" consoles (everything which is not a bootconsole) which are /Users/rubber/linux/kernel/printk/printk.c: 2923
 * handled differently. /Users/rubber/linux/kernel/printk/printk.c: 2924
 *  - Any number of bootconsoles can be registered at any time. /Users/rubber/linux/kernel/printk/printk.c: 2925
 *  - As soon as a "real" console is registered, all bootconsoles /Users/rubber/linux/kernel/printk/printk.c: 2926
 *    will be unregistered automatically. /Users/rubber/linux/kernel/printk/printk.c: 2927
 *  - Once a "real" console is registered, any attempt to register a /Users/rubber/linux/kernel/printk/printk.c: 2928
 *    bootconsoles will be rejected /Users/rubber/linux/kernel/printk/printk.c: 2929
	/* /Users/rubber/linux/kernel/printk/printk.c: 2942
	 * before we register a new CON_BOOT console, make sure we don't /Users/rubber/linux/kernel/printk/printk.c: 2943
	 * already have a valid console /Users/rubber/linux/kernel/printk/printk.c: 2944
	/* /Users/rubber/linux/kernel/printk/printk.c: 2962
	 *	See if we want to use this console driver. If we /Users/rubber/linux/kernel/printk/printk.c: 2963
	 *	didn't select a console we take the first one /Users/rubber/linux/kernel/printk/printk.c: 2964
	 *	that registers here. /Users/rubber/linux/kernel/printk/printk.c: 2965
	/* /Users/rubber/linux/kernel/printk/printk.c: 2991
	 * If we have a bootconsole, and are switching to a real console, /Users/rubber/linux/kernel/printk/printk.c: 2992
	 * don't print everything out again, since when the boot console, and /Users/rubber/linux/kernel/printk/printk.c: 2993
	 * the real console are the same physical device, it's annoying to /Users/rubber/linux/kernel/printk/printk.c: 2994
	 * see the beginning boot messages twice /Users/rubber/linux/kernel/printk/printk.c: 2995
	/* /Users/rubber/linux/kernel/printk/printk.c: 3000
	 *	Put this console in the list - keep the /Users/rubber/linux/kernel/printk/printk.c: 3001
	 *	preferred driver at the head of the list. /Users/rubber/linux/kernel/printk/printk.c: 3002
		/* /Users/rubber/linux/kernel/printk/printk.c: 3021
		 * console_unlock(); will print out the buffered messages /Users/rubber/linux/kernel/printk/printk.c: 3022
		 * for us. /Users/rubber/linux/kernel/printk/printk.c: 3023
		 * /Users/rubber/linux/kernel/printk/printk.c: 3024
		 * We're about to replay the log buffer.  Only do this to the /Users/rubber/linux/kernel/printk/printk.c: 3025
		 * just-registered console to avoid excessive message spam to /Users/rubber/linux/kernel/printk/printk.c: 3026
		 * the already-registered consoles. /Users/rubber/linux/kernel/printk/printk.c: 3027
		 * /Users/rubber/linux/kernel/printk/printk.c: 3028
		 * Set exclusive_console with disabled interrupts to reduce /Users/rubber/linux/kernel/printk/printk.c: 3029
		 * race window with eventual console_flush_on_panic() that /Users/rubber/linux/kernel/printk/printk.c: 3030
		 * ignores console_lock. /Users/rubber/linux/kernel/printk/printk.c: 3031
	/* /Users/rubber/linux/kernel/printk/printk.c: 3044
	 * By unregistering the bootconsoles after we enable the real console /Users/rubber/linux/kernel/printk/printk.c: 3045
	 * we get the "console xxx enabled" message on all the consoles - /Users/rubber/linux/kernel/printk/printk.c: 3046
	 * boot consoles, real consoles, etc - this is to ensure that end /Users/rubber/linux/kernel/printk/printk.c: 3047
	 * users know there might be something in the kernel's log buffer that /Users/rubber/linux/kernel/printk/printk.c: 3048
	 * went to the bootconsole (that they do not see on the real console) /Users/rubber/linux/kernel/printk/printk.c: 3049
		/* We need to iterate through all boot consoles, to make /Users/rubber/linux/kernel/printk/printk.c: 3057
		 * sure we print everything out, before we unregister them. /Users/rubber/linux/kernel/printk/printk.c: 3058
	/* /Users/rubber/linux/kernel/printk/printk.c: 3103
	 * If this isn't the last console and it has CON_CONSDEV set, we /Users/rubber/linux/kernel/printk/printk.c: 3104
	 * need to set it on the next preferred console. /Users/rubber/linux/kernel/printk/printk.c: 3105
 * Initialize the console device. This is called *early*, so /Users/rubber/linux/kernel/printk/printk.c: 3128
 * we can't necessarily depend on lots of kernel help here. /Users/rubber/linux/kernel/printk/printk.c: 3129
 * Just do some early initializations, and do the complex setup /Users/rubber/linux/kernel/printk/printk.c: 3130
 * later. /Users/rubber/linux/kernel/printk/printk.c: 3131
	/* /Users/rubber/linux/kernel/printk/printk.c: 3142
	 * set up the console device so that later boot sequences can /Users/rubber/linux/kernel/printk/printk.c: 3143
	 * inform about problems etc.. /Users/rubber/linux/kernel/printk/printk.c: 3144
 * Some boot consoles access data that is in the init section and which will /Users/rubber/linux/kernel/printk/printk.c: 3158
 * be discarded after the initcalls have been run. To make sure that no code /Users/rubber/linux/kernel/printk/printk.c: 3159
 * will access this data, unregister the boot consoles in a late initcall. /Users/rubber/linux/kernel/printk/printk.c: 3160
 * If for some reason, such as deferred probe or the driver being a loadable /Users/rubber/linux/kernel/printk/printk.c: 3162
 * module, the real console hasn't registered yet at this point, there will /Users/rubber/linux/kernel/printk/printk.c: 3163
 * be a brief interval in which no messages are logged to the console, which /Users/rubber/linux/kernel/printk/printk.c: 3164
 * makes it difficult to diagnose problems that occur during this time. /Users/rubber/linux/kernel/printk/printk.c: 3165
 * To mitigate this problem somewhat, only unregister consoles whose memory /Users/rubber/linux/kernel/printk/printk.c: 3167
 * intersects with the init section. Note that all other boot consoles will /Users/rubber/linux/kernel/printk/printk.c: 3168
 * get unregistered when the real preferred console is registered. /Users/rubber/linux/kernel/printk/printk.c: 3169
			/* /Users/rubber/linux/kernel/printk/printk.c: 3187
			 * Please, consider moving the reported consoles out /Users/rubber/linux/kernel/printk/printk.c: 3188
			 * of the init section. /Users/rubber/linux/kernel/printk/printk.c: 3189
 * Delayed printk version, for scheduler-internal messages: /Users/rubber/linux/kernel/printk/printk.c: 3208
 * printk rate limiting, lifted from the networking subsystem. /Users/rubber/linux/kernel/printk/printk.c: 3284
 * This enforces a rate limit: not more than 10 kernel messages /Users/rubber/linux/kernel/printk/printk.c: 3286
 * every 5s to make a denial-of-service attack impossible. /Users/rubber/linux/kernel/printk/printk.c: 3287
 * printk_timed_ratelimit - caller-controlled printk ratelimiting /Users/rubber/linux/kernel/printk/printk.c: 3298
 * @caller_jiffies: pointer to caller's state /Users/rubber/linux/kernel/printk/printk.c: 3299
 * @interval_msecs: minimum interval between prints /Users/rubber/linux/kernel/printk/printk.c: 3300
 * printk_timed_ratelimit() returns true if more than @interval_msecs /Users/rubber/linux/kernel/printk/printk.c: 3302
 * milliseconds have elapsed since the last time printk_timed_ratelimit() /Users/rubber/linux/kernel/printk/printk.c: 3303
 * returned true. /Users/rubber/linux/kernel/printk/printk.c: 3304
 * kmsg_dump_register - register a kernel log dumper. /Users/rubber/linux/kernel/printk/printk.c: 3323
 * @dumper: pointer to the kmsg_dumper structure /Users/rubber/linux/kernel/printk/printk.c: 3324
 * Adds a kernel log dumper to the system. The dump callback in the /Users/rubber/linux/kernel/printk/printk.c: 3326
 * structure will be called when the kernel oopses or panics and must be /Users/rubber/linux/kernel/printk/printk.c: 3327
 * set. Returns zero on success and %-EINVAL or %-EBUSY otherwise. /Users/rubber/linux/kernel/printk/printk.c: 3328
 * kmsg_dump_unregister - unregister a kmsg dumper. /Users/rubber/linux/kernel/printk/printk.c: 3353
 * @dumper: pointer to the kmsg_dumper structure /Users/rubber/linux/kernel/printk/printk.c: 3354
 * Removes a dump device from the system. Returns zero on success and /Users/rubber/linux/kernel/printk/printk.c: 3356
 * %-EINVAL otherwise. /Users/rubber/linux/kernel/printk/printk.c: 3357
 * kmsg_dump - dump kernel log to kernel message dumpers. /Users/rubber/linux/kernel/printk/printk.c: 3398
 * @reason: the reason (oops, panic etc) for dumping /Users/rubber/linux/kernel/printk/printk.c: 3399
 * Call each of the registered dumper's dump() callback, which can /Users/rubber/linux/kernel/printk/printk.c: 3401
 * retrieve the kmsg records with kmsg_dump_get_line() or /Users/rubber/linux/kernel/printk/printk.c: 3402
 * kmsg_dump_get_buffer(). /Users/rubber/linux/kernel/printk/printk.c: 3403
		/* /Users/rubber/linux/kernel/printk/printk.c: 3413
		 * If client has not provided a specific max_reason, default /Users/rubber/linux/kernel/printk/printk.c: 3414
		 * to KMSG_DUMP_OOPS, unless always_kmsg_dump was set. /Users/rubber/linux/kernel/printk/printk.c: 3415
 * kmsg_dump_get_line - retrieve one kmsg log line /Users/rubber/linux/kernel/printk/printk.c: 3431
 * @iter: kmsg dump iterator /Users/rubber/linux/kernel/printk/printk.c: 3432
 * @syslog: include the "<4>" prefixes /Users/rubber/linux/kernel/printk/printk.c: 3433
 * @line: buffer to copy the line to /Users/rubber/linux/kernel/printk/printk.c: 3434
 * @size: maximum size of the buffer /Users/rubber/linux/kernel/printk/printk.c: 3435
 * @len: length of line placed into buffer /Users/rubber/linux/kernel/printk/printk.c: 3436
 * Start at the beginning of the kmsg buffer, with the oldest kmsg /Users/rubber/linux/kernel/printk/printk.c: 3438
 * record, and copy one record into the provided buffer. /Users/rubber/linux/kernel/printk/printk.c: 3439
 * Consecutive calls will return the next available record moving /Users/rubber/linux/kernel/printk/printk.c: 3441
 * towards the end of the buffer with the youngest messages. /Users/rubber/linux/kernel/printk/printk.c: 3442
 * A return value of FALSE indicates that there are no more records to /Users/rubber/linux/kernel/printk/printk.c: 3444
 * read. /Users/rubber/linux/kernel/printk/printk.c: 3445
 * kmsg_dump_get_buffer - copy kmsg log lines /Users/rubber/linux/kernel/printk/printk.c: 3487
 * @iter: kmsg dump iterator /Users/rubber/linux/kernel/printk/printk.c: 3488
 * @syslog: include the "<4>" prefixes /Users/rubber/linux/kernel/printk/printk.c: 3489
 * @buf: buffer to copy the line to /Users/rubber/linux/kernel/printk/printk.c: 3490
 * @size: maximum size of the buffer /Users/rubber/linux/kernel/printk/printk.c: 3491
 * @len_out: length of line placed into buffer /Users/rubber/linux/kernel/printk/printk.c: 3492
 * Start at the end of the kmsg buffer and fill the provided buffer /Users/rubber/linux/kernel/printk/printk.c: 3494
 * with as many of the *youngest* kmsg records that fit into it. /Users/rubber/linux/kernel/printk/printk.c: 3495
 * If the buffer is large enough, all available kmsg records will be /Users/rubber/linux/kernel/printk/printk.c: 3496
 * copied with a single call. /Users/rubber/linux/kernel/printk/printk.c: 3497
 * Consecutive calls will fill the buffer with the next block of /Users/rubber/linux/kernel/printk/printk.c: 3499
 * available older records, not including the earlier retrieved ones. /Users/rubber/linux/kernel/printk/printk.c: 3500
 * A return value of FALSE indicates that there are no more records to /Users/rubber/linux/kernel/printk/printk.c: 3502
 * read. /Users/rubber/linux/kernel/printk/printk.c: 3503
	/* /Users/rubber/linux/kernel/printk/printk.c: 3534
	 * Find first record that fits, including all following records, /Users/rubber/linux/kernel/printk/printk.c: 3535
	 * into the user-provided buffer for this dump. Pass in size-1 /Users/rubber/linux/kernel/printk/printk.c: 3536
	 * because this function (by way of record_print_text()) will /Users/rubber/linux/kernel/printk/printk.c: 3537
	 * not write more than size-1 bytes of text into @buf. /Users/rubber/linux/kernel/printk/printk.c: 3538
	/* /Users/rubber/linux/kernel/printk/printk.c: 3543
	 * Next kmsg_dump_get_buffer() invocation will dump block of /Users/rubber/linux/kernel/printk/printk.c: 3544
	 * older records stored right before this one. /Users/rubber/linux/kernel/printk/printk.c: 3545
 * kmsg_dump_rewind - reset the iterator /Users/rubber/linux/kernel/printk/printk.c: 3572
 * @iter: kmsg dump iterator /Users/rubber/linux/kernel/printk/printk.c: 3573
 * Reset the dumper's iterator so that kmsg_dump_get_line() and /Users/rubber/linux/kernel/printk/printk.c: 3575
 * kmsg_dump_get_buffer() can be called again and used multiple /Users/rubber/linux/kernel/printk/printk.c: 3576
 * times within the same dumper.dump() callback. /Users/rubber/linux/kernel/printk/printk.c: 3577
 * __printk_wait_on_cpu_lock() - Busy wait until the printk cpu-reentrant /Users/rubber/linux/kernel/printk/printk.c: 3593
 *                               spinning lock is not owned by any CPU. /Users/rubber/linux/kernel/printk/printk.c: 3594
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk.c: 3596
 * __printk_cpu_trylock() - Try to acquire the printk cpu-reentrant /Users/rubber/linux/kernel/printk/printk.c: 3607
 *                          spinning lock. /Users/rubber/linux/kernel/printk/printk.c: 3608
 * If no processor has the lock, the calling processor takes the lock and /Users/rubber/linux/kernel/printk/printk.c: 3610
 * becomes the owner. If the calling processor is already the owner of the /Users/rubber/linux/kernel/printk/printk.c: 3611
 * lock, this function succeeds immediately. /Users/rubber/linux/kernel/printk/printk.c: 3612
 * Context: Any context. Expects interrupts to be disabled. /Users/rubber/linux/kernel/printk/printk.c: 3614
 * Return: 1 on success, otherwise 0. /Users/rubber/linux/kernel/printk/printk.c: 3615
	/* /Users/rubber/linux/kernel/printk/printk.c: 3624
	 * Guarantee loads and stores from this CPU when it is the lock owner /Users/rubber/linux/kernel/printk/printk.c: 3625
	 * are _not_ visible to the previous lock owner. This pairs with /Users/rubber/linux/kernel/printk/printk.c: 3626
	 * __printk_cpu_unlock:B. /Users/rubber/linux/kernel/printk/printk.c: 3627
	 * /Users/rubber/linux/kernel/printk/printk.c: 3628
	 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk.c: 3629
	 * /Users/rubber/linux/kernel/printk/printk.c: 3630
	 * If __printk_cpu_trylock:A reads from __printk_cpu_unlock:B, then /Users/rubber/linux/kernel/printk/printk.c: 3631
	 * __printk_cpu_unlock:A can never read from __printk_cpu_trylock:B. /Users/rubber/linux/kernel/printk/printk.c: 3632
	 * /Users/rubber/linux/kernel/printk/printk.c: 3633
	 * Relies on: /Users/rubber/linux/kernel/printk/printk.c: 3634
	 * /Users/rubber/linux/kernel/printk/printk.c: 3635
	 * RELEASE from __printk_cpu_unlock:A to __printk_cpu_unlock:B /Users/rubber/linux/kernel/printk/printk.c: 3636
	 * of the previous CPU /Users/rubber/linux/kernel/printk/printk.c: 3637
	 *    matching /Users/rubber/linux/kernel/printk/printk.c: 3638
	 * ACQUIRE from __printk_cpu_trylock:A to __printk_cpu_trylock:B /Users/rubber/linux/kernel/printk/printk.c: 3639
	 * of this CPU /Users/rubber/linux/kernel/printk/printk.c: 3640
		/* /Users/rubber/linux/kernel/printk/printk.c: 3645
		 * This CPU is now the owner and begins loading/storing /Users/rubber/linux/kernel/printk/printk.c: 3646
		 * data: LMM(__printk_cpu_trylock:B) /Users/rubber/linux/kernel/printk/printk.c: 3647
 * __printk_cpu_unlock() - Release the printk cpu-reentrant spinning lock. /Users/rubber/linux/kernel/printk/printk.c: 3662
 * The calling processor must be the owner of the lock. /Users/rubber/linux/kernel/printk/printk.c: 3664
 * Context: Any context. Expects interrupts to be disabled. /Users/rubber/linux/kernel/printk/printk.c: 3666
	/* /Users/rubber/linux/kernel/printk/printk.c: 3675
	 * This CPU is finished loading/storing data: /Users/rubber/linux/kernel/printk/printk.c: 3676
	 * LMM(__printk_cpu_unlock:A) /Users/rubber/linux/kernel/printk/printk.c: 3677
	/* /Users/rubber/linux/kernel/printk/printk.c: 3680
	 * Guarantee loads and stores from this CPU when it was the /Users/rubber/linux/kernel/printk/printk.c: 3681
	 * lock owner are visible to the next lock owner. This pairs /Users/rubber/linux/kernel/printk/printk.c: 3682
	 * with __printk_cpu_trylock:A. /Users/rubber/linux/kernel/printk/printk.c: 3683
	 * /Users/rubber/linux/kernel/printk/printk.c: 3684
	 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk.c: 3685
	 * /Users/rubber/linux/kernel/printk/printk.c: 3686
	 * If __printk_cpu_trylock:A reads from __printk_cpu_unlock:B, /Users/rubber/linux/kernel/printk/printk.c: 3687
	 * then __printk_cpu_trylock:B reads from __printk_cpu_unlock:A. /Users/rubber/linux/kernel/printk/printk.c: 3688
	 * /Users/rubber/linux/kernel/printk/printk.c: 3689
	 * Relies on: /Users/rubber/linux/kernel/printk/printk.c: 3690
	 * /Users/rubber/linux/kernel/printk/printk.c: 3691
	 * RELEASE from __printk_cpu_unlock:A to __printk_cpu_unlock:B /Users/rubber/linux/kernel/printk/printk.c: 3692
	 * of this CPU /Users/rubber/linux/kernel/printk/printk.c: 3693
	 *    matching /Users/rubber/linux/kernel/printk/printk.c: 3694
	 * ACQUIRE from __printk_cpu_trylock:A to __printk_cpu_trylock:B /Users/rubber/linux/kernel/printk/printk.c: 3695
	 * of the next CPU /Users/rubber/linux/kernel/printk/printk.c: 3696
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/kernel/printk/printk_safe.c: 1
 * printk_safe.c - Safe printk for printk-deadlock-prone contexts /Users/rubber/linux/kernel/printk/printk_safe.c: 3
	/* /Users/rubber/linux/kernel/printk/printk_safe.c: 37
	 * Use the main logbuf even in NMI. But avoid calling console /Users/rubber/linux/kernel/printk/printk_safe.c: 38
	 * drivers that might have their own locks. /Users/rubber/linux/kernel/printk/printk_safe.c: 39
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/printk/index.c: 1
 * Userspace indexing of printk formats /Users/rubber/linux/kernel/printk/index.c: 3
	/* /Users/rubber/linux/kernel/printk/index.c: 55
	 * Make show() print the header line. Do not update *pos because /Users/rubber/linux/kernel/printk/index.c: 56
	 * pi_next() still has to return the entry at index 0 later. /Users/rubber/linux/kernel/printk/index.c: 57
 * We need both ESCAPE_ANY and explicit characters from ESCAPE_SPECIAL in @only /Users/rubber/linux/kernel/printk/index.c: 66
 * because otherwise ESCAPE_NAP will cause double quotes and backslashes to be /Users/rubber/linux/kernel/printk/index.c: 67
 * ignored for quoting. /Users/rubber/linux/kernel/printk/index.c: 68
		/* /Users/rubber/linux/kernel/printk/index.c: 95
		 * LOGLEVEL_DEFAULT here means "use the same level as the /Users/rubber/linux/kernel/printk/index.c: 96
		 * message we're continuing from", not the default message /Users/rubber/linux/kernel/printk/index.c: 97
		 * loglevel, so don't display it as such. /Users/rubber/linux/kernel/printk/index.c: 98
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1
 * DOC: printk_ringbuffer overview /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 11
 * Data Structure /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 13
 * -------------- /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 14
 * The printk_ringbuffer is made up of 3 internal ringbuffers: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 15
 *   desc_ring /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 17
 *     A ring of descriptors and their meta data (such as sequence number, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 18
 *     timestamp, loglevel, etc.) as well as internal state information about /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 19
 *     the record and logical positions specifying where in the other /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 20
 *     ringbuffer the text strings are located. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 21
 *   text_data_ring /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 23
 *     A ring of data blocks. A data block consists of an unsigned long /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 24
 *     integer (ID) that maps to a desc_ring index followed by the text /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 25
 *     string of the record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 26
 * The internal state information of a descriptor is the key element to allow /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 28
 * readers and writers to locklessly synchronize access to the data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 29
 * Implementation /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 31
 * -------------- /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 32
 * Descriptor Ring /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 34
 * ~~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 35
 * The descriptor ring is an array of descriptors. A descriptor contains /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 36
 * essential meta data to track the data of a printk record using /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 37
 * blk_lpos structs pointing to associated text data blocks (see /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 38
 * "Data Rings" below). Each descriptor is assigned an ID that maps /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 39
 * directly to index values of the descriptor array and has a state. The ID /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 40
 * and the state are bitwise combined into a single descriptor field named /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 41
 * @state_var, allowing ID and state to be synchronously and atomically /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 42
 * updated. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 43
 * Descriptors have four states: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 45
 *   reserved /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 47
 *     A writer is modifying the record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 48
 *   committed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 50
 *     The record and all its data are written. A writer can reopen the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 51
 *     descriptor (transitioning it back to reserved), but in the committed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 52
 *     state the data is consistent. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 53
 *   finalized /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 55
 *     The record and all its data are complete and available for reading. A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 56
 *     writer cannot reopen the descriptor. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 57
 *   reusable /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 59
 *     The record exists, but its text and/or meta data may no longer be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 60
 *     available. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 61
 * Querying the @state_var of a record requires providing the ID of the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 63
 * descriptor to query. This can yield a possible fifth (pseudo) state: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 64
 *   miss /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 66
 *     The descriptor being queried has an unexpected ID. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 67
 * The descriptor ring has a @tail_id that contains the ID of the oldest /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 69
 * descriptor and @head_id that contains the ID of the newest descriptor. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 70
 * When a new descriptor should be created (and the ring is full), the tail /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 72
 * descriptor is invalidated by first transitioning to the reusable state and /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 73
 * then invalidating all tail data blocks up to and including the data blocks /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 74
 * associated with the tail descriptor (for the text ring). Then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 75
 * @tail_id is advanced, followed by advancing @head_id. And finally the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 76
 * @state_var of the new descriptor is initialized to the new ID and reserved /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 77
 * state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 78
 * The @tail_id can only be advanced if the new @tail_id would be in the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 80
 * committed or reusable queried state. This makes it possible that a valid /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 81
 * sequence number of the tail is always available. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 82
 * Descriptor Finalization /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 84
 * ~~~~~~~~~~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 85
 * When a writer calls the commit function prb_commit(), record data is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 86
 * fully stored and is consistent within the ringbuffer. However, a writer can /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 87
 * reopen that record, claiming exclusive access (as with prb_reserve()), and /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 88
 * modify that record. When finished, the writer must again commit the record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 89
 * In order for a record to be made available to readers (and also become /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 91
 * recyclable for writers), it must be finalized. A finalized record cannot be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 92
 * reopened and can never become "unfinalized". Record finalization can occur /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 93
 * in three different scenarios: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 94
 *   1) A writer can simultaneously commit and finalize its record by calling /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 96
 *      prb_final_commit() instead of prb_commit(). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 97
 *   2) When a new record is reserved and the previous record has been /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 99
 *      committed via prb_commit(), that previous record is automatically /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 100
 *      finalized. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 101
 *   3) When a record is committed via prb_commit() and a newer record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 103
 *      already exists, the record being committed is automatically finalized. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 104
 * Data Ring /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 106
 * ~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 107
 * The text data ring is a byte array composed of data blocks. Data blocks are /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 108
 * referenced by blk_lpos structs that point to the logical position of the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 109
 * beginning of a data block and the beginning of the next adjacent data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 110
 * block. Logical positions are mapped directly to index values of the byte /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 111
 * array ringbuffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 112
 * Each data block consists of an ID followed by the writer data. The ID is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 114
 * the identifier of a descriptor that is associated with the data block. A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 115
 * given data block is considered valid if all of the following conditions /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 116
 * are met: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 117
 *   1) The descriptor associated with the data block is in the committed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 119
 *      or finalized queried state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 120
 *   2) The blk_lpos struct within the descriptor associated with the data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 122
 *      block references back to the same data block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 123
 *   3) The data block is within the head/tail logical position range. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 125
 * If the writer data of a data block would extend beyond the end of the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 127
 * byte array, only the ID of the data block is stored at the logical /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 128
 * position and the full data block (ID and writer data) is stored at the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 129
 * beginning of the byte array. The referencing blk_lpos will point to the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 130
 * ID before the wrap and the next data block will be at the logical /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 131
 * position adjacent the full data block after the wrap. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 132
 * Data rings have a @tail_lpos that points to the beginning of the oldest /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 134
 * data block and a @head_lpos that points to the logical position of the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 135
 * next (not yet existing) data block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 136
 * When a new data block should be created (and the ring is full), tail data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 138
 * blocks will first be invalidated by putting their associated descriptors /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 139
 * into the reusable state and then pushing the @tail_lpos forward beyond /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 140
 * them. Then the @head_lpos is pushed forward and is associated with a new /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 141
 * descriptor. If a data block is not valid, the @tail_lpos cannot be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 142
 * advanced beyond it. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 143
 * Info Array /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 145
 * ~~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 146
 * The general meta data of printk records are stored in printk_info structs, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 147
 * stored in an array with the same number of elements as the descriptor ring. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 148
 * Each info corresponds to the descriptor of the same index in the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 149
 * descriptor ring. Info validity is confirmed by evaluating the corresponding /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 150
 * descriptor before and after loading the info. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 151
 * Usage /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 153
 * ----- /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 154
 * Here are some simple examples demonstrating writers and readers. For the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 155
 * examples a global ringbuffer (test_rb) is available (which is not the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 156
 * actual ringbuffer used by printk):: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 157
 *	DEFINE_PRINTKRB(test_rb, 15, 5); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 159
 * This ringbuffer allows up to 32768 records (2 ^ 15) and has a size of /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 161
 * 1 MiB (2 ^ (15 + 5)) for text data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 162
 * Sample writer code:: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 164
 *	const char *textstr = "message text"; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 166
 *	struct prb_reserved_entry e; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 167
 *	struct printk_record r; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 168
 *	// specify how much to allocate /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 170
 *	prb_rec_init_wr(&r, strlen(textstr) + 1); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 171
 *	if (prb_reserve(&e, &test_rb, &r)) { /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 173
 *		snprintf(r.text_buf, r.text_buf_size, "%s", textstr); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 174
 *		r.info->text_len = strlen(textstr); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 176
 *		r.info->ts_nsec = local_clock(); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 177
 *		r.info->caller_id = printk_caller_id(); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 178
 *		// commit and finalize the record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 180
 *		prb_final_commit(&e); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 181
 *	} /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 182
 * Note that additional writer functions are available to extend a record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 184
 * after it has been committed but not yet finalized. This can be done as /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 185
 * long as no new records have been reserved and the caller is the same. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 186
 * Sample writer code (record extending):: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 188
 *		// alternate rest of previous example /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 190
 *		r.info->text_len = strlen(textstr); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 192
 *		r.info->ts_nsec = local_clock(); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 193
 *		r.info->caller_id = printk_caller_id(); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 194
 *		// commit the record (but do not finalize yet) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 196
 *		prb_commit(&e); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 197
 *	} /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 198
 *	... /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 200
 *	// specify additional 5 bytes text space to extend /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 202
 *	prb_rec_init_wr(&r, 5); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 203
 *	// try to extend, but only if it does not exceed 32 bytes /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 205
 *	if (prb_reserve_in_last(&e, &test_rb, &r, printk_caller_id()), 32) { /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 206
 *		snprintf(&r.text_buf[r.info->text_len], /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 207
 *			 r.text_buf_size - r.info->text_len, "hello"); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 208
 *		r.info->text_len += 5; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 210
 *		// commit and finalize the record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 212
 *		prb_final_commit(&e); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 213
 *	} /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 214
 * Sample reader code:: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 216
 *	struct printk_info info; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 218
 *	struct printk_record r; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 219
 *	char text_buf[32]; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 220
 *	u64 seq; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 221
 *	prb_rec_init_rd(&r, &info, &text_buf[0], sizeof(text_buf)); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 223
 *	prb_for_each_record(0, &test_rb, &seq, &r) { /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 225
 *		if (info.seq != seq) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 226
 *			pr_warn("lost %llu records\n", info.seq - seq); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 227
 *		if (info.text_len > r.text_buf_size) { /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 229
 *			pr_warn("record %llu text truncated\n", info.seq); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 230
 *			text_buf[r.text_buf_size - 1] = 0; /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 231
 *		} /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 232
 *		pr_info("%llu: %llu: %s\n", info.seq, info.ts_nsec, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 234
 *			&text_buf[0]); /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 235
 *	} /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 236
 * Note that additional less convenient reader functions are available to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 238
 * allow complex record access. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 239
 * ABA Issues /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 241
 * ~~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 242
 * To help avoid ABA issues, descriptors are referenced by IDs (array index /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 243
 * values combined with tagged bits counting array wraps) and data blocks are /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 244
 * referenced by logical positions (array index values combined with tagged /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 245
 * bits counting array wraps). However, on 32-bit systems the number of /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 246
 * tagged bits is relatively small such that an ABA incident is (at least /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 247
 * theoretically) possible. For example, if 4 million maximally sized (1KiB) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 248
 * printk messages were to occur in NMI context on a 32-bit system, the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 249
 * interrupted context would not be able to recognize that the 32-bit integer /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 250
 * completely wrapped and thus represents a different data block than the one /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 251
 * the interrupted context expects. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 252
 * To help combat this possibility, additional state checking is performed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 254
 * (such as using cmpxchg() even though set() would suffice). These extra /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 255
 * checks are commented as such and will hopefully catch any ABA issue that /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 256
 * a 32-bit system might experience. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 257
 * Memory Barriers /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 259
 * ~~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 260
 * Multiple memory barriers are used. To simplify proving correctness and /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 261
 * generating litmus tests, lines of code related to memory barriers /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 262
 * (loads, stores, and the associated memory barriers) are labeled:: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 263
 *	LMM(function:letter) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 265
 * Comments reference the labels using only the "function:letter" part. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 267
 * The memory barrier pairs and their ordering are: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 269
 *   desc_reserve:D / desc_reserve:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 271
 *     push descriptor tail (id), then push descriptor head (id) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 272
 *   desc_reserve:D / data_push_tail:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 274
 *     push data tail (lpos), then set new descriptor reserved (state) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 275
 *   desc_reserve:D / desc_push_tail:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 277
 *     push descriptor tail (id), then set new descriptor reserved (state) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 278
 *   desc_reserve:D / prb_first_seq:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 280
 *     push descriptor tail (id), then set new descriptor reserved (state) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 281
 *   desc_reserve:F / desc_read:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 283
 *     set new descriptor id and reserved (state), then allow writer changes /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 284
 *   data_alloc:A (or data_realloc:A) / desc_read:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 286
 *     set old descriptor reusable (state), then modify new data block area /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 287
 *   data_alloc:A (or data_realloc:A) / data_push_tail:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 289
 *     push data tail (lpos), then modify new data block area /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 290
 *   _prb_commit:B / desc_read:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 292
 *     store writer changes, then set new descriptor committed (state) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 293
 *   desc_reopen_last:A / _prb_commit:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 295
 *     set descriptor reserved (state), then read descriptor data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 296
 *   _prb_commit:B / desc_reserve:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 298
 *     set new descriptor committed (state), then check descriptor head (id) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 299
 *   data_push_tail:D / data_push_tail:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 301
 *     set descriptor reusable (state), then push data tail (lpos) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 302
 *   desc_push_tail:B / desc_reserve:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 304
 *     set descriptor reusable (state), then push descriptor tail (id) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 305
 * A data block: mapped directly to the beginning of the data block area /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 337
 * specified as a logical position within the data ring. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 338
 * @id:   the ID of the associated descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 340
 * @data: the writer data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 341
 * Note that the size of a data block is only known by its associated /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 343
 * descriptor. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 344
 * Return the descriptor associated with @n. @n can be either a /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 352
 * descriptor ID or a sequence number. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 353
 * Return the printk_info associated with @n. @n can be either a /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 361
 * descriptor ID or a sequence number. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 362
 * Increase the data size to account for data block meta data plus any /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 376
 * padding so that the adjacent data block is aligned on the ID size. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 377
 * Sanity checker for reserve size. The ringbuffer code assumes that a data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 389
 * block does not exceed the maximum possible size that could fit within the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 390
 * ringbuffer. This function provides that basic size check so that the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 391
 * assumption is safe. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 392
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 401
	 * Ensure the alignment padded size could possibly fit in the data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 402
	 * array. The largest possible data block must still leave room for /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 403
	 * at least the ID of the next block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 404
 * Get a copy of a specified descriptor and return its queried state. If the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 424
 * descriptor is in an inconsistent state (miss or reserved), the caller can /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 425
 * only expect the descriptor's @state_var field to be valid. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 426
 * The sequence number and caller_id can be optionally retrieved. Like all /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 428
 * non-state_var data, they are only valid if the descriptor is in a /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 429
 * consistent state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 430
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 446
		 * The descriptor is in an inconsistent state. Set at least /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 447
		 * @state_var so that the caller can see the details of /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 448
		 * the inconsistent state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 449
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 454
	 * Guarantee the state is loaded before copying the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 455
	 * content. This avoids copying obsolete descriptor content that might /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 456
	 * not apply to the descriptor state. This pairs with _prb_commit:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 457
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 458
	 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 459
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 460
	 * If desc_read:A reads from _prb_commit:B, then desc_read:C reads /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 461
	 * from _prb_commit:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 462
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 463
	 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 464
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 465
	 * WMB from _prb_commit:A to _prb_commit:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 466
	 *    matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 467
	 * RMB from desc_read:A to desc_read:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 468
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 472
	 * Copy the descriptor data. The data is not valid until the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 473
	 * state has been re-checked. A memcpy() for all of @desc /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 474
	 * cannot be used because of the atomic_t @state_var field. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 475
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 484
	 * 1. Guarantee the descriptor content is loaded before re-checking /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 485
	 *    the state. This avoids reading an obsolete descriptor state /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 486
	 *    that may not apply to the copied content. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 487
	 *    desc_reserve:F. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 488
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 489
	 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 490
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 491
	 *    If desc_read:C reads from desc_reserve:G, then desc_read:E /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 492
	 *    reads from desc_reserve:F. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 493
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 494
	 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 495
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 496
	 *    WMB from desc_reserve:F to desc_reserve:G /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 497
	 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 498
	 *    RMB from desc_read:C to desc_read:E /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 499
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 500
	 * 2. Guarantee the record data is loaded before re-checking the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 501
	 *    state. This avoids reading an obsolete descriptor state that may /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 502
	 *    not apply to the copied data. This pairs with data_alloc:A and /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 503
	 *    data_realloc:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 504
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 505
	 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 506
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 507
	 *    If copy_data:A reads from data_alloc:B, then desc_read:E /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 508
	 *    reads from desc_make_reusable:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 509
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 510
	 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 511
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 512
	 *    MB from desc_make_reusable:A to data_alloc:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 513
	 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 514
	 *    RMB from desc_read:C to desc_read:E /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 515
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 516
	 *    Note: desc_make_reusable:A and data_alloc:B can be different /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 517
	 *          CPUs. However, the data_alloc:B CPU (which performs the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 518
	 *          full memory barrier) must have previously seen /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 519
	 *          desc_make_reusable:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 520
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 524
	 * The data has been copied. Return the current descriptor state, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 525
	 * which may have changed since the load above. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 526
 * Take a specified descriptor out of the finalized state by attempting /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 536
 * the transition from finalized to reusable. Either this context or some /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 537
 * other context will have been successful. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 538
 * Given the text data ring, put the associated descriptor of each /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 553
 * data block from @lpos_begin until @lpos_end into the reusable state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 554
 * If there is any problem making the associated descriptor reusable, either /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 556
 * the descriptor has not yet been finalized or another writer context has /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 557
 * already pushed the tail lpos past the problematic data block. Regardless, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 558
 * on error the caller can re-load the tail lpos to determine the situation. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 559
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 579
		 * Load the block ID from the data block. This is a data race /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 580
		 * against a writer that may have newly reserved this data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 581
		 * area. If the loaded value matches a valid descriptor ID, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 582
		 * the blk_lpos of that descriptor will be checked to make /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 583
		 * sure it points back to this data block. If the check fails, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 584
		 * the data area has been recycled by another writer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 585
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 598
			 * This data block is invalid if the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 599
			 * does not point back to it. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 600
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 607
			 * This data block is invalid if the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 608
			 * does not point back to it. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 609
 * Advance the data ring tail to at least @lpos. This function puts /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 625
 * descriptors into the reusable state if the tail is pushed beyond /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 626
 * their associated data block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 627
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 640
	 * Any descriptor states that have transitioned to reusable due to the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 641
	 * data tail being pushed to this loaded value will be visible to this /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 642
	 * CPU. This pairs with data_push_tail:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 643
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 644
	 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 645
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 646
	 * If data_push_tail:A reads from data_push_tail:D, then this CPU can /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 647
	 * see desc_make_reusable:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 648
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 649
	 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 650
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 651
	 * MB from desc_make_reusable:A to data_push_tail:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 652
	 *    matches /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 653
	 * READFROM from data_push_tail:D to data_push_tail:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 654
	 *    thus /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 655
	 * READFROM from desc_make_reusable:A to this CPU /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 656
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 660
	 * Loop until the tail lpos is at or beyond @lpos. This condition /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 661
	 * may already be satisfied, resulting in no full memory barrier /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 662
	 * from data_push_tail:D being performed. However, since this CPU /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 663
	 * sees the new tail lpos, any descriptor states that transitioned to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 664
	 * the reusable state must already be visible. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 665
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 668
		 * Make all descriptors reusable that are associated with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 669
		 * data blocks before @lpos. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 670
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 673
			 * 1. Guarantee the block ID loaded in /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 674
			 *    data_make_reusable() is performed before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 675
			 *    reloading the tail lpos. The failed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 676
			 *    data_make_reusable() may be due to a newly /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 677
			 *    recycled data area causing the tail lpos to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 678
			 *    have been previously pushed. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 679
			 *    data_alloc:A and data_realloc:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 680
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 681
			 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 682
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 683
			 *    If data_make_reusable:A reads from data_alloc:B, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 684
			 *    then data_push_tail:C reads from /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 685
			 *    data_push_tail:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 686
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 687
			 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 688
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 689
			 *    MB from data_push_tail:D to data_alloc:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 690
			 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 691
			 *    RMB from data_make_reusable:A to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 692
			 *    data_push_tail:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 693
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 694
			 *    Note: data_push_tail:D and data_alloc:B can be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 695
			 *          different CPUs. However, the data_alloc:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 696
			 *          CPU (which performs the full memory /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 697
			 *          barrier) must have previously seen /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 698
			 *          data_push_tail:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 699
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 700
			 * 2. Guarantee the descriptor state loaded in /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 701
			 *    data_make_reusable() is performed before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 702
			 *    reloading the tail lpos. The failed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 703
			 *    data_make_reusable() may be due to a newly /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 704
			 *    recycled descriptor causing the tail lpos to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 705
			 *    have been previously pushed. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 706
			 *    desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 707
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 708
			 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 709
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 710
			 *    If data_make_reusable:B reads from /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 711
			 *    desc_reserve:F, then data_push_tail:C reads /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 712
			 *    from data_push_tail:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 713
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 714
			 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 715
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 716
			 *    MB from data_push_tail:D to desc_reserve:F /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 717
			 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 718
			 *    RMB from data_make_reusable:B to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 719
			 *    data_push_tail:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 720
			 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 721
			 *    Note: data_push_tail:D and desc_reserve:F can /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 722
			 *          be different CPUs. However, the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 723
			 *          desc_reserve:F CPU (which performs the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 724
			 *          full memory barrier) must have previously /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 725
			 *          seen data_push_tail:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 726
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 740
		 * Guarantee any descriptor states that have transitioned to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 741
		 * reusable are stored before pushing the tail lpos. A full /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 742
		 * memory barrier is needed since other CPUs may have made /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 743
		 * the descriptor states reusable. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 744
		 * data_push_tail:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 745
 * Advance the desc ring tail. This function advances the tail by one /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 757
 * descriptor, thus invalidating the oldest descriptor. Before advancing /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 758
 * the tail, the tail descriptor is made reusable and all data blocks up to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 759
 * and including the descriptor's data block are invalidated (i.e. the data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 760
 * ring tail is pushed past the data block of the descriptor being made /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 761
 * reusable). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 762
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 775
		 * If the ID is exactly 1 wrap behind the expected, it is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 776
		 * in the process of being reserved by another writer and /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 777
		 * must be considered reserved. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 778
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 785
		 * The ID has changed. Another writer must have pushed the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 786
		 * tail and recycled the descriptor already. Success is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 787
		 * returned because the caller is only interested in the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 788
		 * specified tail being pushed, which it was. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 789
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 802
	 * Data blocks must be invalidated before their associated /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 803
	 * descriptor can be made available for recycling. Invalidating /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 804
	 * them later is not possible because there is no way to trust /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 805
	 * data blocks once their associated descriptor is gone. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 806
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 812
	 * Check the next descriptor after @tail_id before pushing the tail /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 813
	 * to it because the tail must always be in a finalized or reusable /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 814
	 * state. The implementation of prb_first_seq() relies on this. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 815
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 816
	 * A successful read implies that the next descriptor is less than or /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 817
	 * equal to @head_id so there is no risk of pushing the tail past the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 818
	 * head. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 819
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 825
		 * Guarantee any descriptor states that have transitioned to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 826
		 * reusable are stored before pushing the tail ID. This allows /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 827
		 * verifying the recycled descriptor state. A full memory /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 828
		 * barrier is needed since other CPUs may have made the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 829
		 * descriptor states reusable. This pairs with desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 830
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 835
		 * Guarantee the last state load from desc_read() is before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 836
		 * reloading @tail_id in order to see a new tail ID in the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 837
		 * case that the descriptor has been recycled. This pairs /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 838
		 * with desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 839
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 840
		 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 841
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 842
		 * If desc_push_tail:A reads from desc_reserve:F, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 843
		 * desc_push_tail:D reads from desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 844
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 845
		 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 846
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 847
		 * MB from desc_push_tail:B to desc_reserve:F /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 848
		 *    matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 849
		 * RMB from desc_push_tail:A to desc_push_tail:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 850
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 851
		 * Note: desc_push_tail:B and desc_reserve:F can be different /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 852
		 *       CPUs. However, the desc_reserve:F CPU (which performs /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 853
		 *       the full memory barrier) must have previously seen /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 854
		 *       desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 855
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 859
		 * Re-check the tail ID. The descriptor following @tail_id is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 860
		 * not in an allowed tail state. But if the tail has since /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 861
		 * been moved by another CPU, then it does not matter. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 862
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 887
		 * Guarantee the head ID is read before reading the tail ID. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 888
		 * Since the tail ID is updated before the head ID, this /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 889
		 * guarantees that @id_prev_wrap is never ahead of the tail /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 890
		 * ID. This pairs with desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 891
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 892
		 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 893
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 894
		 * If desc_reserve:A reads from desc_reserve:D, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 895
		 * desc_reserve:C reads from desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 896
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 897
		 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 898
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 899
		 * MB from desc_push_tail:B to desc_reserve:D /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 900
		 *    matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 901
		 * RMB from desc_reserve:A to desc_reserve:C /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 902
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 903
		 * Note: desc_push_tail:B and desc_reserve:D can be different /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 904
		 *       CPUs. However, the desc_reserve:D CPU (which performs /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 905
		 *       the full memory barrier) must have previously seen /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 906
		 *       desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 907
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 913
			 * Make space for the new descriptor by /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 914
			 * advancing the tail. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 915
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 921
		 * 1. Guarantee the tail ID is read before validating the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 922
		 *    recycled descriptor state. A read memory barrier is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 923
		 *    sufficient for this. This pairs with desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 924
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 925
		 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 926
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 927
		 *    If desc_reserve:C reads from desc_push_tail:B, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 928
		 *    desc_reserve:E reads from desc_make_reusable:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 929
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 930
		 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 931
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 932
		 *    MB from desc_make_reusable:A to desc_push_tail:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 933
		 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 934
		 *    RMB from desc_reserve:C to desc_reserve:E /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 935
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 936
		 *    Note: desc_make_reusable:A and desc_push_tail:B can be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 937
		 *          different CPUs. However, the desc_push_tail:B CPU /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 938
		 *          (which performs the full memory barrier) must have /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 939
		 *          previously seen desc_make_reusable:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 940
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 941
		 * 2. Guarantee the tail ID is stored before storing the head /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 942
		 *    ID. This pairs with desc_reserve:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 943
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 944
		 * 3. Guarantee any data ring tail changes are stored before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 945
		 *    recycling the descriptor. Data ring tail changes can /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 946
		 *    happen via desc_push_tail()->data_push_tail(). A full /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 947
		 *    memory barrier is needed since another CPU may have /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 948
		 *    pushed the data ring tails. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 949
		 *    data_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 950
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 951
		 * 4. Guarantee a new tail ID is stored before recycling the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 952
		 *    descriptor. A full memory barrier is needed since /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 953
		 *    another CPU may have pushed the tail ID. This pairs /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 954
		 *    with desc_push_tail:C and this also pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 955
		 *    prb_first_seq:C. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 956
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 957
		 * 5. Guarantee the head ID is stored before trying to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 958
		 *    finalize the previous descriptor. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 959
		 *    _prb_commit:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 960
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 967
	 * If the descriptor has been recycled, verify the old state val. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 968
	 * See "ABA Issues" about why this verification is performed. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 969
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 978
	 * Assign the descriptor a new ID and set its state to reserved. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 979
	 * See "ABA Issues" about why cmpxchg() instead of set() is used. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 980
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 981
	 * Guarantee the new descriptor ID and state is stored before making /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 982
	 * any other changes. A write memory barrier is sufficient for this. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 983
	 * This pairs with desc_read:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 984
 * Allocate a new data block, invalidating the oldest data block(s) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1017
 * if necessary. This function also associates the data block with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1018
 * a specified descriptor. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1019
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1050
		 * 1. Guarantee any descriptor states that have transitioned /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1051
		 *    to reusable are stored before modifying the newly /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1052
		 *    allocated data area. A full memory barrier is needed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1053
		 *    since other CPUs may have made the descriptor states /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1054
		 *    reusable. See data_push_tail:A about why the reusable /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1055
		 *    states are visible. This pairs with desc_read:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1056
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1057
		 * 2. Guarantee any updated tail lpos is stored before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1058
		 *    modifying the newly allocated data area. Another CPU may /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1059
		 *    be in data_make_reusable() and is reading a block ID /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1060
		 *    from this area. data_make_reusable() can handle reading /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1061
		 *    a garbage block ID value, but then it must be able to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1062
		 *    load a new tail lpos. A full memory barrier is needed /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1063
		 *    since other CPUs may have updated the tail lpos. This /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1064
		 *    pairs with data_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1065
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1077
		 * Store the ID on the wrapped block for consistency. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1078
		 * The printk_ringbuffer does not actually use it. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1079
 * Try to resize an existing data block associated with the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1091
 * specified by @id. If the resized data block should become wrapped, it /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1092
 * copies the old data to the new data block. If @size yields a data block /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1093
 * with the same or less size, the data block is left as is. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1094
 * Fail if this is not the last allocated data block or if there is not /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1096
 * enough space or it is not possible make enough space. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1097
 * Return a pointer to the beginning of the entire data buffer or NULL on /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1099
 * failure. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1100
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1149
		 * Store the ID on the wrapped block for consistency. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1150
		 * The printk_ringbuffer does not actually use it. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1151
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1156
			 * Since the allocated space is now in the newly /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1157
			 * created wrapping data block, copy the content /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1158
			 * from the old data block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1159
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1185
	 * For wrapping data blocks, the trailing (wasted) space is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1186
	 * also counted. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1187
 * Given @blk_lpos, return a pointer to the writer data from the data block /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1194
 * and calculate the size of the data part. A NULL pointer is returned if /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1195
 * @blk_lpos specifies values that could never be legal. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1196
 * This function (used by readers) performs strict validation on the lpos /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1198
 * values to possibly detect bugs in the writer code. A WARN_ON_ONCE() is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1199
 * triggered if an internal error is detected. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1200
 * Attempt to transition the newest descriptor from committed back to reserved /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1252
 * so that the record can be modified by a writer again. This is only possible /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1253
 * if the descriptor is not yet finalized and the provided @caller_id matches. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1254
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1268
	 * To reduce unnecessarily reopening, first check if the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1269
	 * state and caller ID are correct. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1270
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1280
	 * Guarantee the reserved state is stored before reading any /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1281
	 * record data. A full memory barrier is needed because @state_var /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1282
	 * modification is followed by reading. This pairs with _prb_commit:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1283
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1284
	 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1285
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1286
	 * If desc_reopen_last:A reads from _prb_commit:B, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1287
	 * prb_reserve_in_last:A reads from _prb_commit:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1288
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1289
	 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1290
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1291
	 * WMB from _prb_commit:A to _prb_commit:B /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1292
	 *    matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1293
	 * MB If desc_reopen_last:A to prb_reserve_in_last:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1294
 * prb_reserve_in_last() - Re-reserve and extend the space in the ringbuffer /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1306
 *                         used by the newest record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1307
 * @e:         The entry structure to setup. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1309
 * @rb:        The ringbuffer to re-reserve and extend data in. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1310
 * @r:         The record structure to allocate buffers for. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1311
 * @caller_id: The caller ID of the caller (reserving writer). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1312
 * @max_size:  Fail if the extended size would be greater than this. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1313
 * This is the public function available to writers to re-reserve and extend /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1315
 * data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1316
 * The writer specifies the text size to extend (not the new total size) by /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1318
 * setting the @text_buf_size field of @r. To ensure proper initialization /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1319
 * of @r, prb_rec_init_wr() should be used. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1320
 * This function will fail if @caller_id does not match the caller ID of the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1322
 * newest record. In that case the caller must reserve new data using /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1323
 * prb_reserve(). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1324
 * Context: Any context. Disables local interrupts on success. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1326
 * Return: true if text data could be extended, otherwise false. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1327
 * On success: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1329
 *   - @r->text_buf points to the beginning of the entire text buffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1331
 *   - @r->text_buf_size is set to the new total size of the buffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1333
 *   - @r->info is not touched so that @r->info->text_len could be used /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1335
 *     to append the text. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1336
 *   - prb_record_text_space() can be used on @e to query the new /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1338
 *     actually used space. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1339
 * Important: All @r->info fields will already be set with the current values /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1341
 *            for the record. I.e. @r->info->text_len will be less than /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1342
 *            @text_buf_size. Writers can use @r->info->text_len to know /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1343
 *            where concatenation begins and writers should update /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1344
 *            @r->info->text_len after concatenating. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1345
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1369
	 * Set the @e fields here so that prb_commit() can be used if /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1370
	 * anything fails from now on. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1371
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1376
	 * desc_reopen_last() checked the caller_id, but there was no /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1377
	 * exclusive access at that point. The descriptor may have /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1378
	 * changed since then. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1379
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1403
		 * Increase the buffer size to include the original size. If /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1404
		 * the meta data (@text_len) is not sane, use the full data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1405
		 * block size. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1406
 * Attempt to finalize a specified descriptor. If this fails, the descriptor /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1442
 * is either already final or it will finalize itself when the writer commits. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1443
 * prb_reserve() - Reserve space in the ringbuffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1455
 * @e:  The entry structure to setup. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1457
 * @rb: The ringbuffer to reserve data in. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1458
 * @r:  The record structure to allocate buffers for. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1459
 * This is the public function available to writers to reserve data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1461
 * The writer specifies the text size to reserve by setting the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1463
 * @text_buf_size field of @r. To ensure proper initialization of @r, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1464
 * prb_rec_init_wr() should be used. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1465
 * Context: Any context. Disables local interrupts on success. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1467
 * Return: true if at least text data could be allocated, otherwise false. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1468
 * On success, the fields @info and @text_buf of @r will be set by this /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1470
 * function and should be filled in by the writer before committing. Also /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1471
 * on success, prb_record_text_space() can be used on @e to query the actual /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1472
 * space used for the text data block. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1473
 * Important: @info->text_len needs to be set correctly by the writer in /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1475
 *            order for data to be readable and/or extended. Its value /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1476
 *            is initialized to 0. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1477
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1491
	 * Descriptors in the reserved state act as blockers to all further /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1492
	 * reservations once the desc_ring has fully wrapped. Disable /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1493
	 * interrupts during the reserve/commit window in order to minimize /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1494
	 * the likelihood of this happening. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1495
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1509
	 * All @info fields (except @seq) are cleared and must be filled in /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1510
	 * by the writer. Save @seq before clearing because it is used to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1511
	 * determine the new sequence number. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1512
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1517
	 * Set the @e fields here so that prb_commit() can be used if /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1518
	 * text data allocation fails. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1519
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1524
	 * Initialize the sequence number if it has "never been set". /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1525
	 * Otherwise just increment it by a full wrap. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1526
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1527
	 * @seq is considered "never been set" if it has a value of 0, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1528
	 * _except_ for @infos[0], which was specially setup by the ringbuffer /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1529
	 * initializer and therefore is always considered as set. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1530
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1531
	 * See the "Bootstrap" comment block in printk_ringbuffer.h for /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1532
	 * details about how the initializer bootstraps the descriptors. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1533
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1540
	 * New data is about to be reserved. Once that happens, previous /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1541
	 * descriptors are no longer able to be extended. Finalize the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1542
	 * previous descriptor now so that it can be made available to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1543
	 * readers. (For seq==0 there is no previous descriptor.) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1544
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1578
	 * Set the descriptor as committed. See "ABA Issues" about why /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1579
	 * cmpxchg() instead of set() is used. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1580
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1581
	 * 1  Guarantee all record data is stored before the descriptor state /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1582
	 *    is stored as committed. A write memory barrier is sufficient /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1583
	 *    for this. This pairs with desc_read:B and desc_reopen_last:A. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1584
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1585
	 * 2. Guarantee the descriptor state is stored as committed before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1586
	 *    re-checking the head ID in order to possibly finalize this /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1587
	 *    descriptor. This pairs with desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1588
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1589
	 *    Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1590
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1591
	 *    If prb_commit:A reads from desc_reserve:D, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1592
	 *    desc_make_final:A reads from _prb_commit:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1593
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1594
	 *    Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1595
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1596
	 *    MB _prb_commit:B to prb_commit:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1597
	 *       matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1598
	 *    MB desc_reserve:D to desc_make_final:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1599
 * prb_commit() - Commit (previously reserved) data to the ringbuffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1611
 * @e: The entry containing the reserved data information. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1613
 * This is the public function available to writers to commit data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1615
 * Note that the data is not yet available to readers until it is finalized. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1617
 * Finalizing happens automatically when space for the next record is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1618
 * reserved. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1619
 * See prb_final_commit() for a version of this function that finalizes /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1621
 * immediately. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1622
 * Context: Any context. Enables local interrupts. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1624
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1633
	 * If this descriptor is no longer the head (i.e. a new record has /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1634
	 * been allocated), extending the data for this record is no longer /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1635
	 * allowed and therefore it must be finalized. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1636
 * prb_final_commit() - Commit and finalize (previously reserved) data to /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1644
 *                      the ringbuffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1645
 * @e: The entry containing the reserved data information. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1647
 * This is the public function available to writers to commit+finalize data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1649
 * By finalizing, the data is made immediately available to readers. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1651
 * This function should only be used if there are no intentions of extending /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1653
 * this data using prb_reserve_in_last(). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1654
 * Context: Any context. Enables local interrupts. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1656
 * Count the number of lines in provided text. All text has at least 1 line /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1664
 * (even if @text_size is 0). Each '\n' processed is counted as an additional /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1665
 * line. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1666
 * Given @blk_lpos, copy an expected @len of data into the provided buffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1687
 * If @line_count is provided, count the number of lines in the data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1688
 * This function (used by readers) performs strict validation on the data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1690
 * size to possibly detect bugs in the writer code. A WARN_ON_ONCE() is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1691
 * triggered if an internal error is detected. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1692
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1709
	 * Actual cannot be less than expected. It can be more than expected /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1710
	 * because of the trailing alignment padding. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1711
	 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1712
	 * Note that invalid @len values can occur because the caller loads /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1713
	 * the value during an allowed data race. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1714
 * This is an extended version of desc_read(). It gets a copy of a specified /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1734
 * descriptor. However, it also verifies that the record is finalized and has /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1735
 * the sequence number @seq. On success, 0 is returned. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1736
 * Error return values: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1738
 * -EINVAL: A finalized record with sequence number @seq does not exist. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1739
 * -ENOENT: A finalized record with sequence number @seq exists, but its data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1740
 *          is not available. This is a valid record, so readers should /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1741
 *          continue with the next record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1742
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1754
	 * An unexpected @id (desc_miss) or @seq mismatch means the record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1755
	 * does not exist. A descriptor in the reserved or committed state /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1756
	 * means the record does not yet exist for the reader. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1757
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1766
	 * A descriptor in the reusable state may no longer have its data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1767
	 * available; report it as existing but with lost data. Or the record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1768
	 * may actually be a record with lost data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1769
 * Copy the ringbuffer data from the record with @seq to the provided /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1780
 * @r buffer. On success, 0 is returned. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1781
 * See desc_read_finalized_seq() for error return values. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1783
	/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1802
	 * If @r is NULL, the caller is only interested in the availability /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1803
	 * of the record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1804
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1837
		 * This loop will not be infinite because the tail is /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1838
		 * _always_ in the finalized or reusable state. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1839
		/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1844
		 * Guarantee the last state load from desc_read() is before /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1845
		 * reloading @tail_id in order to see a new tail in the case /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1846
		 * that the descriptor has been recycled. This pairs with /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1847
		 * desc_reserve:D. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1848
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1849
		 * Memory barrier involvement: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1850
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1851
		 * If prb_first_seq:B reads from desc_reserve:F, then /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1852
		 * prb_first_seq:A reads from desc_push_tail:B. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1853
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1854
		 * Relies on: /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1855
		 * /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1856
		 * MB from desc_push_tail:B to desc_reserve:F /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1857
		 *    matching /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1858
		 * RMB prb_first_seq:B to prb_first_seq:A /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1859
 * Non-blocking read of a record. Updates @seq to the last finalized record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1868
 * (which may have no data available). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1869
 * See the description of prb_read_valid() and prb_read_valid_info() /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1871
 * for details. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1872
			/* /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1884
			 * Behind the tail. Catch up and try again. This /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1885
			 * can happen for -ENOENT and -EINVAL cases. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1886
 * prb_read_valid() - Non-blocking read of a requested record or (if gone) /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1904
 *                    the next available record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1905
 * @rb:  The ringbuffer to read from. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1907
 * @seq: The sequence number of the record to read. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1908
 * @r:   A record data buffer to store the read record to. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1909
 * This is the public function available to readers to read a record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1911
 * The reader provides the @info and @text_buf buffers of @r to be /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1913
 * filled in. Any of the buffer pointers can be set to NULL if the reader /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1914
 * is not interested in that data. To ensure proper initialization of @r, /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1915
 * prb_rec_init_rd() should be used. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1916
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1918
 * Return: true if a record was read, otherwise false. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1919
 * On success, the reader must check r->info.seq to see which record was /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1921
 * actually read. This allows the reader to detect dropped records. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1922
 * Failure means @seq refers to a not yet written record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1924
 * prb_read_valid_info() - Non-blocking read of meta data for a requested /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1933
 *                         record or (if gone) the next available record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1934
 * @rb:         The ringbuffer to read from. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1936
 * @seq:        The sequence number of the record to read. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1937
 * @info:       A buffer to store the read record meta data to. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1938
 * @line_count: A buffer to store the number of lines in the record text. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1939
 * This is the public function available to readers to read only the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1941
 * meta data of a record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1942
 * The reader provides the @info, @line_count buffers to be filled in. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1944
 * Either of the buffer pointers can be set to NULL if the reader is not /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1945
 * interested in that data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1946
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1948
 * Return: true if a record's meta data was read, otherwise false. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1949
 * On success, the reader must check info->seq to see which record meta data /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1951
 * was actually read. This allows the reader to detect dropped records. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1952
 * Failure means @seq refers to a not yet written record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1954
 * prb_first_valid_seq() - Get the sequence number of the oldest available /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1967
 *                         record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1968
 * @rb: The ringbuffer to get the sequence number from. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1970
 * This is the public function available to readers to see what the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1972
 * first/oldest valid sequence number is. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1973
 * This provides readers a starting point to begin iterating the ringbuffer. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1975
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1977
 * Return: The sequence number of the first/oldest record or, if the /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1978
 *         ringbuffer is empty, 0 is returned. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1979
 * prb_next_seq() - Get the sequence number after the last available record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1992
 * @rb:  The ringbuffer to get the sequence number from. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1994
 * This is the public function available to readers to see what the next /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1996
 * newest sequence number available to readers will be. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1997
 * This provides readers a sequence number to jump to if all currently /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 1999
 * available records should be skipped. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2000
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2002
 * Return: The sequence number of the next newest (not yet available) record /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2003
 *         for readers. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2004
 * prb_init() - Initialize a ringbuffer to use provided external buffers. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2018
 * @rb:       The ringbuffer to initialize. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2020
 * @text_buf: The data buffer for text data. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2021
 * @textbits: The size of @text_buf as a power-of-2 value. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2022
 * @descs:    The descriptor buffer for ringbuffer records. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2023
 * @descbits: The count of @descs items as a power-of-2 value. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2024
 * @infos:    The printk_info buffer for ringbuffer records. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2025
 * This is the public function available to writers to setup a ringbuffer /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2027
 * during runtime using provided buffers. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2028
 * This must match the initialization of DEFINE_PRINTKRB(). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2030
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2032
 * prb_record_text_space() - Query the full actual used ringbuffer space for /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2064
 *                           the text data of a reserved entry. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2065
 * @e: The successfully reserved entry to query. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2067
 * This is the public function available to writers to see how much actual /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2069
 * space is used in the ringbuffer to store the text data of the specified /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2070
 * entry. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2071
 * This function is only valid if @e has been successfully reserved using /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2073
 * prb_reserve(). /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2074
 * Context: Any context. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2076
 * Return: The size in bytes used by the text data of the associated record. /Users/rubber/linux/kernel/printk/printk_ringbuffer.c: 2077
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/prog_iter.c: 1
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/arraymap.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/arraymap.c: 2
 * Copyright (c) 2016,2017 Facebook /Users/rubber/linux/kernel/bpf/arraymap.c: 3
		/* if value_size is bigger, the user space won't be able to /Users/rubber/linux/kernel/bpf/arraymap.c: 73
		 * access the elements. /Users/rubber/linux/kernel/bpf/arraymap.c: 74
	/* On 32 bit archs roundup_pow_of_two() with max_entries that has /Users/rubber/linux/kernel/bpf/arraymap.c: 94
	 * upper most bit set in u32 space is undefined behavior due to /Users/rubber/linux/kernel/bpf/arraymap.c: 95
	 * resulting 1U << 32, so do it manually here in u64 space. /Users/rubber/linux/kernel/bpf/arraymap.c: 96
		/* round up array size to nearest power of 2, /Users/rubber/linux/kernel/bpf/arraymap.c: 104
		 * since cpu will speculate within index_mask limits /Users/rubber/linux/kernel/bpf/arraymap.c: 105
		/* rely on vmalloc() to return page-aligned memory and /Users/rubber/linux/kernel/bpf/arraymap.c: 117
		 * ensure array->value is exactly page-aligned /Users/rubber/linux/kernel/bpf/arraymap.c: 118
	/* per_cpu areas are zero-filled and bpf programs can only /Users/rubber/linux/kernel/bpf/arraymap.c: 256
	 * access 'value_size' of them, so copying rounded areas /Users/rubber/linux/kernel/bpf/arraymap.c: 257
	 * will not leak any kernel data /Users/rubber/linux/kernel/bpf/arraymap.c: 258
	/* the user space will provide round_up(value_size, 8) bytes that /Users/rubber/linux/kernel/bpf/arraymap.c: 356
	 * will be copied into per-cpu area. bpf programs can only access /Users/rubber/linux/kernel/bpf/arraymap.c: 357
	 * value_size of it. During lookup the same extra bytes will be /Users/rubber/linux/kernel/bpf/arraymap.c: 358
	 * returned or zeros which were zero-filled by percpu_alloc, /Users/rubber/linux/kernel/bpf/arraymap.c: 359
	 * so no kernel data leaks possible /Users/rubber/linux/kernel/bpf/arraymap.c: 360
	/* bpf array can only take a u32 key. This check makes sure /Users/rubber/linux/kernel/bpf/arraymap.c: 478
	 * that the btf matches the attr used during map_create. /Users/rubber/linux/kernel/bpf/arraymap.c: 479
	/* We must track the program's aux info at this point in time /Users/rubber/linux/kernel/bpf/arraymap.c: 923
	 * since the program pointer itself may not be stable yet, see /Users/rubber/linux/kernel/bpf/arraymap.c: 924
	 * also comment in prog_array_map_poke_run(). /Users/rubber/linux/kernel/bpf/arraymap.c: 925
			/* Few things to be aware of: /Users/rubber/linux/kernel/bpf/arraymap.c: 971
			 * /Users/rubber/linux/kernel/bpf/arraymap.c: 972
			 * 1) We can only ever access aux in this context, but /Users/rubber/linux/kernel/bpf/arraymap.c: 973
			 *    not aux->prog since it might not be stable yet and /Users/rubber/linux/kernel/bpf/arraymap.c: 974
			 *    there could be danger of use after free otherwise. /Users/rubber/linux/kernel/bpf/arraymap.c: 975
			 * 2) Initially when we start tracking aux, the program /Users/rubber/linux/kernel/bpf/arraymap.c: 976
			 *    is not JITed yet and also does not have a kallsyms /Users/rubber/linux/kernel/bpf/arraymap.c: 977
			 *    entry. We skip these as poke->tailcall_target_stable /Users/rubber/linux/kernel/bpf/arraymap.c: 978
			 *    is not active yet. The JIT will do the final fixup /Users/rubber/linux/kernel/bpf/arraymap.c: 979
			 *    before setting it stable. The various /Users/rubber/linux/kernel/bpf/arraymap.c: 980
			 *    poke->tailcall_target_stable are successively /Users/rubber/linux/kernel/bpf/arraymap.c: 981
			 *    activated, so tail call updates can arrive from here /Users/rubber/linux/kernel/bpf/arraymap.c: 982
			 *    while JIT is still finishing its final fixup for /Users/rubber/linux/kernel/bpf/arraymap.c: 983
			 *    non-activated poke entries. /Users/rubber/linux/kernel/bpf/arraymap.c: 984
			 * 3) On program teardown, the program's kallsym entry gets /Users/rubber/linux/kernel/bpf/arraymap.c: 985
			 *    removed out of RCU callback, but we can only untrack /Users/rubber/linux/kernel/bpf/arraymap.c: 986
			 *    from sleepable context, therefore bpf_arch_text_poke() /Users/rubber/linux/kernel/bpf/arraymap.c: 987
			 *    might not see that this is in BPF text section and /Users/rubber/linux/kernel/bpf/arraymap.c: 988
			 *    bails out with -EINVAL. As these are unreachable since /Users/rubber/linux/kernel/bpf/arraymap.c: 989
			 *    RCU grace period already passed, we simply skip them. /Users/rubber/linux/kernel/bpf/arraymap.c: 990
			 * 4) Also programs reaching refcount of zero while patching /Users/rubber/linux/kernel/bpf/arraymap.c: 991
			 *    is in progress is okay since we're protected under /Users/rubber/linux/kernel/bpf/arraymap.c: 992
			 *    poke_mutex and untrack the programs before the JIT /Users/rubber/linux/kernel/bpf/arraymap.c: 993
			 *    buffer is freed. When we're still in the middle of /Users/rubber/linux/kernel/bpf/arraymap.c: 994
			 *    patching and suddenly kallsyms entry of the program /Users/rubber/linux/kernel/bpf/arraymap.c: 995
			 *    gets evicted, we just skip the rest which is fine due /Users/rubber/linux/kernel/bpf/arraymap.c: 996
			 *    to point 3). /Users/rubber/linux/kernel/bpf/arraymap.c: 997
			 * 5) Any other error happening below from bpf_arch_text_poke() /Users/rubber/linux/kernel/bpf/arraymap.c: 998
			 *    is a unexpected bug. /Users/rubber/linux/kernel/bpf/arraymap.c: 999
				/* let other CPUs finish the execution of program /Users/rubber/linux/kernel/bpf/arraymap.c: 1031
				 * so that it will not possible to expose them /Users/rubber/linux/kernel/bpf/arraymap.c: 1032
				 * to invalid nop, stack unwind, nop state /Users/rubber/linux/kernel/bpf/arraymap.c: 1033
/* prog_array->aux->{type,jited} is a runtime binding. /Users/rubber/linux/kernel/bpf/arraymap.c: 1102
 * Doing static check alone in the verifier is not enough. /Users/rubber/linux/kernel/bpf/arraymap.c: 1103
 * Thus, prog_array_map cannot be used as an inner_map /Users/rubber/linux/kernel/bpf/arraymap.c: 1104
 * and map_meta_equal is not implemented. /Users/rubber/linux/kernel/bpf/arraymap.c: 1105
 not used */, /Users/rubber/linux/kernel/bpf/arraymap.c: 1232
	/* map->inner_map_meta is only accessed by syscall which /Users/rubber/linux/kernel/bpf/arraymap.c: 1288
	 * is protected by fdget/fdput. /Users/rubber/linux/kernel/bpf/arraymap.c: 1289
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 1
 * Copyright (c) 2019 Facebook /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 3
 * Copyright 2020 Google LLC. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 4
	/* Neither the bpf_prog nor the bpf-map's syscall /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 75
	 * could be modifying the local_storage->list now. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 76
	 * Thus, no elem can be added-to or deleted-from the /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 77
	 * local_storage->list by the bpf_prog or by the bpf-map's syscall. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 78
	 * /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 79
	 * It is racing with bpf_local_storage_map_free() alone /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 80
	 * when unlinking elem from the local_storage->list and /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 81
	 * the map's bucket->list. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 82
		/* Always unlink from map before unlinking from /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 86
		 * local_storage. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 87
	/* free_inoode_storage should always be true as long as /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 96
	 * local_storage->list was non-empty. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 97
	/* explicitly check that the inode_storage_ptr is not /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 178
	 * NULL as inode_storage_lookup returns NULL in this case and /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 179
	 * bpf_local_storage_update expects the owner to have a /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 180
	 * valid storage pointer. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 181
	/* This helper must only called from where the inode is guaranteed /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 190
	 * to have a refcount and cannot be freed. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 191
	/* This helper must only called from where the inode is guaranteed /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 210
	 * to have a refcount and cannot be freed. /Users/rubber/linux/kernel/bpf/bpf_inode_storage.c: 211
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 1
 * queue_stack_maps.c: BPF queue and stack maps /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 3
 * Copyright (c) 2018 Politecnico di Torino /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 5
		/* if value_size is bigger, the user space won't be able to /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 59
		 * access the elements. /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 60
	/* BPF_EXIST is used to force making room for a new element in case the /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 192
	 * map is full /Users/rubber/linux/kernel/bpf/queue_stack_maps.c: 193
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/dispatcher.c: 1
/* The BPF dispatcher is a multiway branch code generator. The /Users/rubber/linux/kernel/bpf/dispatcher.c: 8
 * dispatcher is a mechanism to avoid the performance penalty of an /Users/rubber/linux/kernel/bpf/dispatcher.c: 9
 * indirect call, which is expensive when retpolines are enabled. A /Users/rubber/linux/kernel/bpf/dispatcher.c: 10
 * dispatch client registers a BPF program into the dispatcher, and if /Users/rubber/linux/kernel/bpf/dispatcher.c: 11
 * there is available room in the dispatcher a direct call to the BPF /Users/rubber/linux/kernel/bpf/dispatcher.c: 12
 * program will be generated. All calls to the BPF programs called via /Users/rubber/linux/kernel/bpf/dispatcher.c: 13
 * the dispatcher will then be a direct call, instead of an /Users/rubber/linux/kernel/bpf/dispatcher.c: 14
 * indirect. The dispatcher hijacks a trampoline function it via the /Users/rubber/linux/kernel/bpf/dispatcher.c: 15
 * __fentry__ of the trampoline. The trampoline function has the /Users/rubber/linux/kernel/bpf/dispatcher.c: 16
 * following signature: /Users/rubber/linux/kernel/bpf/dispatcher.c: 17
 * unsigned int trampoline(const void *ctx, const struct bpf_insn *insnsi, /Users/rubber/linux/kernel/bpf/dispatcher.c: 19
 *                         unsigned int (*bpf_func)(const void *, /Users/rubber/linux/kernel/bpf/dispatcher.c: 20
 *                                                  const struct bpf_insn *)); /Users/rubber/linux/kernel/bpf/dispatcher.c: 21
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/net_namespace.c: 1
 * Functions to manage BPF programs attached to netns /Users/rubber/linux/kernel/bpf/net_namespace.c: 8
	/* We don't hold a ref to net in order to auto-detach the link /Users/rubber/linux/kernel/bpf/net_namespace.c: 16
	 * when netns is going away. Instead we rely on pernet /Users/rubber/linux/kernel/bpf/net_namespace.c: 17
	 * pre_exit callback to clear this pointer. Must be accessed /Users/rubber/linux/kernel/bpf/net_namespace.c: 18
	 * with netns_bpf_mutex held. /Users/rubber/linux/kernel/bpf/net_namespace.c: 19
	/* We can race with cleanup_net, but if we see a non-NULL /Users/rubber/linux/kernel/bpf/net_namespace.c: 112
	 * struct net pointer, pre_exit has not run yet and wait for /Users/rubber/linux/kernel/bpf/net_namespace.c: 113
	 * netns_bpf_mutex. /Users/rubber/linux/kernel/bpf/net_namespace.c: 114
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/devmap.c: 1
/* Copyright (c) 2017 Covalent IO, Inc. http://covalent.io /Users/rubber/linux/kernel/bpf/devmap.c: 2
/* Devmaps primary use is as a backend map for XDP BPF helper call /Users/rubber/linux/kernel/bpf/devmap.c: 5
 * bpf_redirect_map(). Because XDP is mostly concerned with performance we /Users/rubber/linux/kernel/bpf/devmap.c: 6
 * spent some effort to ensure the datapath with redirect maps does not use /Users/rubber/linux/kernel/bpf/devmap.c: 7
 * any locking. This is a quick note on the details. /Users/rubber/linux/kernel/bpf/devmap.c: 8
 * We have three possible paths to get into the devmap control plane bpf /Users/rubber/linux/kernel/bpf/devmap.c: 10
 * syscalls, bpf programs, and driver side xmit/flush operations. A bpf syscall /Users/rubber/linux/kernel/bpf/devmap.c: 11
 * will invoke an update, delete, or lookup operation. To ensure updates and /Users/rubber/linux/kernel/bpf/devmap.c: 12
 * deletes appear atomic from the datapath side xchg() is used to modify the /Users/rubber/linux/kernel/bpf/devmap.c: 13
 * netdev_map array. Then because the datapath does a lookup into the netdev_map /Users/rubber/linux/kernel/bpf/devmap.c: 14
 * array (read-only) from an RCU critical section we use call_rcu() to wait for /Users/rubber/linux/kernel/bpf/devmap.c: 15
 * an rcu grace period before free'ing the old data structures. This ensures the /Users/rubber/linux/kernel/bpf/devmap.c: 16
 * datapath always has a valid copy. However, the datapath does a "flush" /Users/rubber/linux/kernel/bpf/devmap.c: 17
 * operation that pushes any pending packets in the driver outside the RCU /Users/rubber/linux/kernel/bpf/devmap.c: 18
 * critical section. Each bpf_dtab_netdev tracks these pending operations using /Users/rubber/linux/kernel/bpf/devmap.c: 19
 * a per-cpu flush list. The bpf_dtab_netdev object will not be destroyed  until /Users/rubber/linux/kernel/bpf/devmap.c: 20
 * this list is empty, indicating outstanding flush operations have completed. /Users/rubber/linux/kernel/bpf/devmap.c: 21
 * BPF syscalls may race with BPF program calls on any of the update, delete /Users/rubber/linux/kernel/bpf/devmap.c: 23
 * or lookup operations. As noted above the xchg() operation also keep the /Users/rubber/linux/kernel/bpf/devmap.c: 24
 * netdev_map consistent in this case. From the devmap side BPF programs /Users/rubber/linux/kernel/bpf/devmap.c: 25
 * calling into these operations are the same as multiple user space threads /Users/rubber/linux/kernel/bpf/devmap.c: 26
 * making system calls. /Users/rubber/linux/kernel/bpf/devmap.c: 27
 * Finally, any of the above may race with a netdev_unregister notifier. The /Users/rubber/linux/kernel/bpf/devmap.c: 29
 * unregister notifier must search for net devices in the map structure that /Users/rubber/linux/kernel/bpf/devmap.c: 30
 * contain a reference to the net device and remove them. This is a two step /Users/rubber/linux/kernel/bpf/devmap.c: 31
 * process (a) dereference the bpf_dtab_netdev object in netdev_map and (b) /Users/rubber/linux/kernel/bpf/devmap.c: 32
 * check to see if the ifindex is the same as the net_device being removed. /Users/rubber/linux/kernel/bpf/devmap.c: 33
 * When removing the dev a cmpxchg() is used to ensure the correct dev is /Users/rubber/linux/kernel/bpf/devmap.c: 34
 * removed, in the case of a concurrent update or delete operation it is /Users/rubber/linux/kernel/bpf/devmap.c: 35
 * possible that the initially referenced dev is no longer in the map. As the /Users/rubber/linux/kernel/bpf/devmap.c: 36
 * notifier hook walks the map we know that new dev references can not be /Users/rubber/linux/kernel/bpf/devmap.c: 37
 * added by the user because core infrastructure ensures dev_get_by_index() /Users/rubber/linux/kernel/bpf/devmap.c: 38
 * calls will fail at this point. /Users/rubber/linux/kernel/bpf/devmap.c: 39
 * The devmap_hash type is a map type which interprets keys as ifindexes and /Users/rubber/linux/kernel/bpf/devmap.c: 41
 * indexes these using a hashmap. This allows maps that use ifindex as key to be /Users/rubber/linux/kernel/bpf/devmap.c: 42
 * densely packed instead of having holes in the lookup array for unused /Users/rubber/linux/kernel/bpf/devmap.c: 43
 * ifindexes. The setup and packet enqueue/send code is shared between the two /Users/rubber/linux/kernel/bpf/devmap.c: 44
 * types of devmap; only the lookup and insertion is different. /Users/rubber/linux/kernel/bpf/devmap.c: 45
	/* check sanity of attributes. 2 value sizes supported: /Users/rubber/linux/kernel/bpf/devmap.c: 114
	 * 4 bytes: ifindex /Users/rubber/linux/kernel/bpf/devmap.c: 115
	 * 8 bytes: ifindex + prog fd /Users/rubber/linux/kernel/bpf/devmap.c: 116
	/* Lookup returns a pointer straight to dev->ifindex, so make sure the /Users/rubber/linux/kernel/bpf/devmap.c: 124
	 * verifier prevents writes from the BPF side /Users/rubber/linux/kernel/bpf/devmap.c: 125
	/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0, /Users/rubber/linux/kernel/bpf/devmap.c: 187
	 * so the programs (can be more than one that used this map) were /Users/rubber/linux/kernel/bpf/devmap.c: 188
	 * disconnected from events. The following synchronize_rcu() guarantees /Users/rubber/linux/kernel/bpf/devmap.c: 189
	 * both rcu read critical sections complete and waits for /Users/rubber/linux/kernel/bpf/devmap.c: 190
	 * preempt-disable regions (NAPI being the relevant context here) so we /Users/rubber/linux/kernel/bpf/devmap.c: 191
	 * are certain there will be no further reads against the netdev_map and /Users/rubber/linux/kernel/bpf/devmap.c: 192
	 * all flush operations are complete. Flush operations can only be done /Users/rubber/linux/kernel/bpf/devmap.c: 193
	 * from NAPI context for this reason. /Users/rubber/linux/kernel/bpf/devmap.c: 194
/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or /Users/rubber/linux/kernel/bpf/devmap.c: 262
 * by local_bh_disable() (from XDP calls inside NAPI). The /Users/rubber/linux/kernel/bpf/devmap.c: 263
 * rcu_read_lock_bh_held() below makes lockdep accept both. /Users/rubber/linux/kernel/bpf/devmap.c: 264
		/* If ndo_xdp_xmit fails with an errno, no frames have /Users/rubber/linux/kernel/bpf/devmap.c: 389
		 * been xmit'ed. /Users/rubber/linux/kernel/bpf/devmap.c: 390
	/* If not all frames have been transmitted, it is our /Users/rubber/linux/kernel/bpf/devmap.c: 396
	 * responsibility to free them /Users/rubber/linux/kernel/bpf/devmap.c: 397
/* __dev_flush is called from xdp_do_flush() which _must_ be signalled from the /Users/rubber/linux/kernel/bpf/devmap.c: 407
 * driver before returning from its napi->poll() routine. See the comment above /Users/rubber/linux/kernel/bpf/devmap.c: 408
 * xdp_do_flush() in filter.c. /Users/rubber/linux/kernel/bpf/devmap.c: 409
/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or /Users/rubber/linux/kernel/bpf/devmap.c: 424
 * by local_bh_disable() (from XDP calls inside NAPI). The /Users/rubber/linux/kernel/bpf/devmap.c: 425
 * rcu_read_lock_bh_held() below makes lockdep accept both. /Users/rubber/linux/kernel/bpf/devmap.c: 426
/* Runs in NAPI, i.e., softirq under local_bh_disable(). Thus, safe percpu /Users/rubber/linux/kernel/bpf/devmap.c: 441
 * variable access, and map elements stick around. See comment above /Users/rubber/linux/kernel/bpf/devmap.c: 442
 * xdp_do_flush() in filter.c. /Users/rubber/linux/kernel/bpf/devmap.c: 443
	/* Ingress dev_rx will be the same for all xdp_frame's in /Users/rubber/linux/kernel/bpf/devmap.c: 454
	 * bulk_queue, because bq stored per-CPU and must be flushed /Users/rubber/linux/kernel/bpf/devmap.c: 455
	 * from net_device drivers NAPI func end. /Users/rubber/linux/kernel/bpf/devmap.c: 456
	 * /Users/rubber/linux/kernel/bpf/devmap.c: 457
	 * Do the same with xdp_prog and flush_list since these fields /Users/rubber/linux/kernel/bpf/devmap.c: 458
	 * are only ever modified together. /Users/rubber/linux/kernel/bpf/devmap.c: 459
/* Get ifindex of each upper device. 'indexes' must be able to hold at /Users/rubber/linux/kernel/bpf/devmap.c: 573
 * least MAX_NEST_DEV elements. /Users/rubber/linux/kernel/bpf/devmap.c: 574
 * Returns the number of ifindexes added. /Users/rubber/linux/kernel/bpf/devmap.c: 575
	/* Redirect has already succeeded semantically at this point, so we just /Users/rubber/linux/kernel/bpf/devmap.c: 677
	 * return 0 even if packet is dropped. Helper below takes care of /Users/rubber/linux/kernel/bpf/devmap.c: 678
	 * freeing skb. /Users/rubber/linux/kernel/bpf/devmap.c: 679
	/* Use call_rcu() here to ensure rcu critical sections have completed /Users/rubber/linux/kernel/bpf/devmap.c: 925
	 * Remembering the driver side flush operation will happen before the /Users/rubber/linux/kernel/bpf/devmap.c: 926
	 * net device is removed. /Users/rubber/linux/kernel/bpf/devmap.c: 927
		/* This rcu_read_lock/unlock pair is needed because /Users/rubber/linux/kernel/bpf/devmap.c: 1094
		 * dev_map_list is an RCU list AND to ensure a delete /Users/rubber/linux/kernel/bpf/devmap.c: 1095
		 * operation does not free a netdev_map entry while we /Users/rubber/linux/kernel/bpf/devmap.c: 1096
		 * are comparing it against the netdev being unregistered. /Users/rubber/linux/kernel/bpf/devmap.c: 1097
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/tnum.c: 1
/* tnum: tracked (or tristate) numbers /Users/rubber/linux/kernel/bpf/tnum.c: 2
 * A tnum tracks knowledge about the bits of a value.  Each bit can be either /Users/rubber/linux/kernel/bpf/tnum.c: 4
 * known (0 or 1), or unknown (x).  Arithmetic operations on tnums will /Users/rubber/linux/kernel/bpf/tnum.c: 5
 * propagate the unknown bits such that the tnum result represents all the /Users/rubber/linux/kernel/bpf/tnum.c: 6
 * possible results for possible values of the operands. /Users/rubber/linux/kernel/bpf/tnum.c: 7
	/* e.g. if chi = 4, bits = 3, delta = (1<<3) - 1 = 7. /Users/rubber/linux/kernel/bpf/tnum.c: 29
	 * if chi = 0, bits = 0, delta = (1<<0) - 1 = 0, so we return /Users/rubber/linux/kernel/bpf/tnum.c: 30
	 *  constant min (since min == max). /Users/rubber/linux/kernel/bpf/tnum.c: 31
	/* if a.value is negative, arithmetic shifting by minimum shift /Users/rubber/linux/kernel/bpf/tnum.c: 49
	 * will have larger negative offset compared to more shifting. /Users/rubber/linux/kernel/bpf/tnum.c: 50
	 * If a.value is nonnegative, arithmetic shifting by minimum shift /Users/rubber/linux/kernel/bpf/tnum.c: 51
	 * will have larger positive offset compare to more shifting. /Users/rubber/linux/kernel/bpf/tnum.c: 52
/* Generate partial products by multiplying each bit in the multiplier (tnum a) /Users/rubber/linux/kernel/bpf/tnum.c: 114
 * with the multiplicand (tnum b), and add the partial products after /Users/rubber/linux/kernel/bpf/tnum.c: 115
 * appropriately bit-shifting them. Instead of directly performing tnum addition /Users/rubber/linux/kernel/bpf/tnum.c: 116
 * on the generated partial products, equivalenty, decompose each partial /Users/rubber/linux/kernel/bpf/tnum.c: 117
 * product into two tnums, consisting of the value-sum (acc_v) and the /Users/rubber/linux/kernel/bpf/tnum.c: 118
 * mask-sum (acc_m) and then perform tnum addition on them. The following paper /Users/rubber/linux/kernel/bpf/tnum.c: 119
 * explains the algorithm in more detail: https://arxiv.org/abs/2105.05398. /Users/rubber/linux/kernel/bpf/tnum.c: 120
/* Note that if a and b disagree - i.e. one has a 'known 1' where the other has /Users/rubber/linux/kernel/bpf/tnum.c: 141
 * a 'known 0' - this will return a 'known 1' for that bit. /Users/rubber/linux/kernel/bpf/tnum.c: 142
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/percpu_freelist.c: 1
/* Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/percpu_freelist.c: 2
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 1
/* Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 2
	/* If the removing node is the next_inactive_rotation candidate, /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 69
	 * move the next_inactive_rotation pointer also. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 70
/* Move nodes between or within active and inactive list (like /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 96
 * active to inactive, inactive to active or tail of active back to /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 97
 * the head of active). /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 98
	/* If the moving node is the next_inactive_rotation candidate, /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 115
	 * move the next_inactive_rotation pointer also. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 116
/* Rotate the active list: /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 130
 * 1. Start from tail /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 131
 * 2. If the node has the ref bit set, it will be rotated /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 132
 *    back to the head of active list with the ref bit cleared. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 133
 *    Give this node one more chance to survive in the active list. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 134
 * 3. If the ref bit is not set, move it to the head of the /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 135
 *    inactive list. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 136
 * 4. It will at most scan nr_scans nodes /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 137
/* Rotate the inactive list.  It starts from the next_inactive_rotation /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 158
 * 1. If the node has ref bit set, it will be moved to the head /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 159
 *    of active list with the ref bit cleared. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 160
 * 2. If the node does not have ref bit set, it will leave it /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 161
 *    at its current location (i.e. do nothing) so that it can /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 162
 *    be considered during the next inactive_shrink. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 163
 * 3. It will at most scan nr_scans nodes /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 164
/* Shrink the inactive list.  It starts from the tail of the /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 201
 * inactive list and only move the nodes without the ref bit /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 202
 * set to the designated free list. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 203
/* 1. Rotate the active list (if needed) /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 234
 * 2. Always rotate the inactive list /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 235
/* Calls __bpf_lru_list_shrink_inactive() to shrink some /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 245
 * ref-bit-cleared nodes and move them to the designated /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 246
 * free list. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 247
 * If it cannot get a free node after calling /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 249
 * __bpf_lru_list_shrink_inactive().  It will just remove /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 250
 * one node from either inactive or active list without /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 251
 * honoring the ref-bit.  It prefers inactive list to active /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 252
 * list in this situation. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 253
	/* No free nodes found from the local free list and /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 459
	 * the global LRU list. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 460
	 * /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 461
	 * Steal from the local free/pending list of the /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 462
	 * current CPU and remote CPU in RR.  It starts /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 463
	 * with the loc_l->next_steal CPU. /Users/rubber/linux/kernel/bpf/bpf_lru_list.c: 464
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/hashtab.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/hashtab.c: 2
 * Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/hashtab.c: 3
 * The bucket lock has two protection scopes: /Users/rubber/linux/kernel/bpf/hashtab.c: 32
 * 1) Serializing concurrent operations from BPF programs on different /Users/rubber/linux/kernel/bpf/hashtab.c: 34
 *    CPUs /Users/rubber/linux/kernel/bpf/hashtab.c: 35
 * 2) Serializing concurrent operations from BPF programs and sys_bpf() /Users/rubber/linux/kernel/bpf/hashtab.c: 37
 * BPF programs can execute in any context including perf, kprobes and /Users/rubber/linux/kernel/bpf/hashtab.c: 39
 * tracing. As there are almost no limits where perf, kprobes and tracing /Users/rubber/linux/kernel/bpf/hashtab.c: 40
 * can be invoked from the lock operations need to be protected against /Users/rubber/linux/kernel/bpf/hashtab.c: 41
 * deadlocks. Deadlocks can be caused by recursion and by an invocation in /Users/rubber/linux/kernel/bpf/hashtab.c: 42
 * the lock held section when functions which acquire this lock are invoked /Users/rubber/linux/kernel/bpf/hashtab.c: 43
 * from sys_bpf(). BPF recursion is prevented by incrementing the per CPU /Users/rubber/linux/kernel/bpf/hashtab.c: 44
 * variable bpf_prog_active, which prevents BPF programs attached to perf /Users/rubber/linux/kernel/bpf/hashtab.c: 45
 * events, kprobes and tracing to be invoked before the prior invocation /Users/rubber/linux/kernel/bpf/hashtab.c: 46
 * from one of these contexts completed. sys_bpf() uses the same mechanism /Users/rubber/linux/kernel/bpf/hashtab.c: 47
 * by pinning the task to the current CPU and incrementing the recursion /Users/rubber/linux/kernel/bpf/hashtab.c: 48
 * protection across the map operation. /Users/rubber/linux/kernel/bpf/hashtab.c: 49
 * This has subtle implications on PREEMPT_RT. PREEMPT_RT forbids certain /Users/rubber/linux/kernel/bpf/hashtab.c: 51
 * operations like memory allocations (even with GFP_ATOMIC) from atomic /Users/rubber/linux/kernel/bpf/hashtab.c: 52
 * contexts. This is required because even with GFP_ATOMIC the memory /Users/rubber/linux/kernel/bpf/hashtab.c: 53
 * allocator calls into code paths which acquire locks with long held lock /Users/rubber/linux/kernel/bpf/hashtab.c: 54
 * sections. To ensure the deterministic behaviour these locks are regular /Users/rubber/linux/kernel/bpf/hashtab.c: 55
 * spinlocks, which are converted to 'sleepable' spinlocks on RT. The only /Users/rubber/linux/kernel/bpf/hashtab.c: 56
 * true atomic contexts on an RT kernel are the low level hardware /Users/rubber/linux/kernel/bpf/hashtab.c: 57
 * handling, scheduling, low level interrupt handling, NMIs etc. None of /Users/rubber/linux/kernel/bpf/hashtab.c: 58
 * these contexts should ever do memory allocations. /Users/rubber/linux/kernel/bpf/hashtab.c: 59
 * As regular device interrupt handlers and soft interrupts are forced into /Users/rubber/linux/kernel/bpf/hashtab.c: 61
 * thread context, the existing code which does /Users/rubber/linux/kernel/bpf/hashtab.c: 62
 *   spin_lock*(); alloc(GPF_ATOMIC); spin_unlock*(); /Users/rubber/linux/kernel/bpf/hashtab.c: 63
 * just works. /Users/rubber/linux/kernel/bpf/hashtab.c: 64
 * In theory the BPF locks could be converted to regular spinlocks as well, /Users/rubber/linux/kernel/bpf/hashtab.c: 66
 * but the bucket locks and percpu_freelist locks can be taken from /Users/rubber/linux/kernel/bpf/hashtab.c: 67
 * arbitrary contexts (perf, kprobes, tracepoints) which are required to be /Users/rubber/linux/kernel/bpf/hashtab.c: 68
 * atomic contexts even on RT. These mechanisms require preallocated maps, /Users/rubber/linux/kernel/bpf/hashtab.c: 69
 * so there is no need to invoke memory allocations within the lock held /Users/rubber/linux/kernel/bpf/hashtab.c: 70
 * sections. /Users/rubber/linux/kernel/bpf/hashtab.c: 71
 * BPF maps which need dynamic allocation are only used from (forced) /Users/rubber/linux/kernel/bpf/hashtab.c: 73
 * thread context on RT and can therefore use regular spinlocks which in /Users/rubber/linux/kernel/bpf/hashtab.c: 74
 * turn allows to invoke memory allocations from the lock held section. /Users/rubber/linux/kernel/bpf/hashtab.c: 75
 * On a non RT kernel this distinction is neither possible nor required. /Users/rubber/linux/kernel/bpf/hashtab.c: 77
 * spinlock maps to raw_spinlock and the extra code is optimized out by the /Users/rubber/linux/kernel/bpf/hashtab.c: 78
 * compiler. /Users/rubber/linux/kernel/bpf/hashtab.c: 79
/* The LRU list has a lock (lru_lock). Each htab bucket has a lock /Users/rubber/linux/kernel/bpf/hashtab.c: 276
 * (bucket_lock). If both locks need to be acquired together, the lock /Users/rubber/linux/kernel/bpf/hashtab.c: 277
 * order is always lru_lock -> bucket_lock and this only happens in /Users/rubber/linux/kernel/bpf/hashtab.c: 278
 * bpf_lru_list.c logic. For example, certain code path of /Users/rubber/linux/kernel/bpf/hashtab.c: 279
 * bpf_lru_pop_free(), which is called by function prealloc_lru_pop(), /Users/rubber/linux/kernel/bpf/hashtab.c: 280
 * will acquire lru_lock first followed by acquiring bucket_lock. /Users/rubber/linux/kernel/bpf/hashtab.c: 281
 * In hashtab.c, to avoid deadlock, lock acquisition of /Users/rubber/linux/kernel/bpf/hashtab.c: 283
 * bucket_lock followed by lru_lock is not allowed. In such cases, /Users/rubber/linux/kernel/bpf/hashtab.c: 284
 * bucket_lock needs to be released first before acquiring lru_lock. /Users/rubber/linux/kernel/bpf/hashtab.c: 285
		/* pop will succeed, since prealloc_init() /Users/rubber/linux/kernel/bpf/hashtab.c: 388
		 * preallocated extra num_possible_cpus elements /Users/rubber/linux/kernel/bpf/hashtab.c: 389
	/* percpu_lru means each cpu has its own LRU list. /Users/rubber/linux/kernel/bpf/hashtab.c: 405
	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where /Users/rubber/linux/kernel/bpf/hashtab.c: 406
	 * the map's value itself is percpu.  percpu_lru has /Users/rubber/linux/kernel/bpf/hashtab.c: 407
	 * nothing to do with the map's value. /Users/rubber/linux/kernel/bpf/hashtab.c: 408
		/* LRU implementation is much complicated than other /Users/rubber/linux/kernel/bpf/hashtab.c: 421
		 * maps.  Hence, limit to CAP_BPF. /Users/rubber/linux/kernel/bpf/hashtab.c: 422
	/* check sanity of attributes. /Users/rubber/linux/kernel/bpf/hashtab.c: 443
	 * value_size == 0 may be allowed in the future to use map as a set /Users/rubber/linux/kernel/bpf/hashtab.c: 444
		/* if key_size + value_size is bigger, the user space won't be /Users/rubber/linux/kernel/bpf/hashtab.c: 452
		 * able to access the elements via bpf syscall. This check /Users/rubber/linux/kernel/bpf/hashtab.c: 453
		 * also makes sure that the elem_size doesn't overflow and it's /Users/rubber/linux/kernel/bpf/hashtab.c: 454
		 * kmalloc-able later in htab_map_update_elem() /Users/rubber/linux/kernel/bpf/hashtab.c: 455
	/* percpu_lru means each cpu has its own LRU list. /Users/rubber/linux/kernel/bpf/hashtab.c: 468
	 * it is different from BPF_MAP_TYPE_PERCPU_HASH where /Users/rubber/linux/kernel/bpf/hashtab.c: 469
	 * the map's value itself is percpu.  percpu_lru has /Users/rubber/linux/kernel/bpf/hashtab.c: 470
	 * nothing to do with the map's value. /Users/rubber/linux/kernel/bpf/hashtab.c: 471
		/* ensure each CPU's lru list has >=1 elements. /Users/rubber/linux/kernel/bpf/hashtab.c: 487
		 * since we are at it, make each lru list has the same /Users/rubber/linux/kernel/bpf/hashtab.c: 488
		 * number of elements. /Users/rubber/linux/kernel/bpf/hashtab.c: 489
			/* lru itself can remove the least used element, so /Users/rubber/linux/kernel/bpf/hashtab.c: 543
			 * there is no need for an extra elem during map_update. /Users/rubber/linux/kernel/bpf/hashtab.c: 544
/* can be called without bucket lock. it will repeat the loop in /Users/rubber/linux/kernel/bpf/hashtab.c: 595
 * the unlikely event when elements moved from one bucket into another /Users/rubber/linux/kernel/bpf/hashtab.c: 596
 * while link list is being walked /Users/rubber/linux/kernel/bpf/hashtab.c: 597
/* Called from syscall or from eBPF program directly, so /Users/rubber/linux/kernel/bpf/hashtab.c: 617
 * arguments have to match bpf_map_lookup_elem() exactly. /Users/rubber/linux/kernel/bpf/hashtab.c: 618
 * The return value is adjusted by BPF instructions /Users/rubber/linux/kernel/bpf/hashtab.c: 619
 * in htab_map_gen_lookup(). /Users/rubber/linux/kernel/bpf/hashtab.c: 620
/* inline bpf_map_lookup_elem() call. /Users/rubber/linux/kernel/bpf/hashtab.c: 653
 * Instead of: /Users/rubber/linux/kernel/bpf/hashtab.c: 654
 * bpf_prog /Users/rubber/linux/kernel/bpf/hashtab.c: 655
 *   bpf_map_lookup_elem /Users/rubber/linux/kernel/bpf/hashtab.c: 656
 *     map->ops->map_lookup_elem /Users/rubber/linux/kernel/bpf/hashtab.c: 657
 *       htab_map_lookup_elem /Users/rubber/linux/kernel/bpf/hashtab.c: 658
 *         __htab_map_lookup_elem /Users/rubber/linux/kernel/bpf/hashtab.c: 659
 * do: /Users/rubber/linux/kernel/bpf/hashtab.c: 660
 * bpf_prog /Users/rubber/linux/kernel/bpf/hashtab.c: 661
 *   __htab_map_lookup_elem /Users/rubber/linux/kernel/bpf/hashtab.c: 662
/* It is called from the bpf_lru_list when the LRU needs to delete /Users/rubber/linux/kernel/bpf/hashtab.c: 736
 * older elements from the htab. /Users/rubber/linux/kernel/bpf/hashtab.c: 737
	/* When using prealloc and not setting the initial value on all cpus, /Users/rubber/linux/kernel/bpf/hashtab.c: 890
	 * zero-fill element values for other cpus (just as what happens when /Users/rubber/linux/kernel/bpf/hashtab.c: 891
	 * not using prealloc). Otherwise, bpf program has no way to ensure /Users/rubber/linux/kernel/bpf/hashtab.c: 892
	 * known initial values for cpus other than current one /Users/rubber/linux/kernel/bpf/hashtab.c: 893
	 * (onallcpus=false always when coming from bpf prog). /Users/rubber/linux/kernel/bpf/hashtab.c: 894
			/* if we're updating the existing element, /Users/rubber/linux/kernel/bpf/hashtab.c: 931
			 * use per-cpu extra elems to avoid freelist_pop/push /Users/rubber/linux/kernel/bpf/hashtab.c: 932
				/* when map is full and update() is replacing /Users/rubber/linux/kernel/bpf/hashtab.c: 949
				 * old element, it's ok to allocate, since /Users/rubber/linux/kernel/bpf/hashtab.c: 950
				 * old element will be freed immediately. /Users/rubber/linux/kernel/bpf/hashtab.c: 951
				 * Otherwise return an error /Users/rubber/linux/kernel/bpf/hashtab.c: 952
		/* fall through, grab the bucket lock and lookup again. /Users/rubber/linux/kernel/bpf/hashtab.c: 1060
		 * 99.9% chance that the element won't be found, /Users/rubber/linux/kernel/bpf/hashtab.c: 1061
		 * but second lookup under lock has to be done. /Users/rubber/linux/kernel/bpf/hashtab.c: 1062
		/* first lookup without the bucket lock didn't find the element, /Users/rubber/linux/kernel/bpf/hashtab.c: 1077
		 * but second lookup with the bucket lock found it. /Users/rubber/linux/kernel/bpf/hashtab.c: 1078
		 * This case is highly unlikely, but has to be dealt with: /Users/rubber/linux/kernel/bpf/hashtab.c: 1079
		 * grab the element lock in addition to the bucket lock /Users/rubber/linux/kernel/bpf/hashtab.c: 1080
		 * and update element in place /Users/rubber/linux/kernel/bpf/hashtab.c: 1081
	/* add new element to the head of the list, so that /Users/rubber/linux/kernel/bpf/hashtab.c: 1098
	 * concurrent search will find it before old elem /Users/rubber/linux/kernel/bpf/hashtab.c: 1099
	/* For LRU, we need to alloc before taking bucket's /Users/rubber/linux/kernel/bpf/hashtab.c: 1146
	 * spinlock because getting free nodes from LRU may need /Users/rubber/linux/kernel/bpf/hashtab.c: 1147
	 * to remove older elements from htab and this removal /Users/rubber/linux/kernel/bpf/hashtab.c: 1148
	 * operation will need a bucket lock. /Users/rubber/linux/kernel/bpf/hashtab.c: 1149
	/* add new element to the head of the list, so that /Users/rubber/linux/kernel/bpf/hashtab.c: 1167
	 * concurrent search will find it before old elem /Users/rubber/linux/kernel/bpf/hashtab.c: 1168
	/* For LRU, we need to alloc before taking bucket's /Users/rubber/linux/kernel/bpf/hashtab.c: 1269
	 * spinlock because LRU's elem alloc may need /Users/rubber/linux/kernel/bpf/hashtab.c: 1270
	 * to remove older elem from htab and this removal /Users/rubber/linux/kernel/bpf/hashtab.c: 1271
	 * operation will need a bucket lock. /Users/rubber/linux/kernel/bpf/hashtab.c: 1272
	/* bpf_free_used_maps() or close(map_fd) will trigger this map_free callback. /Users/rubber/linux/kernel/bpf/hashtab.c: 1447
	 * bpf_free_used_maps() is called after bpf prog is no longer executing. /Users/rubber/linux/kernel/bpf/hashtab.c: 1448
	 * There is no need to synchronize_rcu() here to protect map elements. /Users/rubber/linux/kernel/bpf/hashtab.c: 1449
	/* some of free_htab_elem() callbacks for elements of this map may /Users/rubber/linux/kernel/bpf/hashtab.c: 1452
	 * not have executed. Wait for them. /Users/rubber/linux/kernel/bpf/hashtab.c: 1453
	/* while experimenting with hash tables with sizes ranging from 10 to /Users/rubber/linux/kernel/bpf/hashtab.c: 1638
	 * 1000, it was observed that a bucket can have upto 5 entries. /Users/rubber/linux/kernel/bpf/hashtab.c: 1639
	/* We cannot do copy_from_user or copy_to_user inside /Users/rubber/linux/kernel/bpf/hashtab.c: 1644
	 * the rcu_read_lock. Allocate enough space here. /Users/rubber/linux/kernel/bpf/hashtab.c: 1645
		/* Note that since bucket_cnt > 0 here, it is implicit /Users/rubber/linux/kernel/bpf/hashtab.c: 1681
		 * that the locked was grabbed, so release it. /Users/rubber/linux/kernel/bpf/hashtab.c: 1682
		/* Note that since bucket_cnt > 0 here, it is implicit /Users/rubber/linux/kernel/bpf/hashtab.c: 1692
		 * that the locked was grabbed, so release it. /Users/rubber/linux/kernel/bpf/hashtab.c: 1693
			/* bpf_lru_push_free() will acquire lru_lock, which /Users/rubber/linux/kernel/bpf/hashtab.c: 1732
			 * may cause deadlock. See comments in function /Users/rubber/linux/kernel/bpf/hashtab.c: 1733
			 * prealloc_lru_pop(). Let us do bpf_lru_push_free() /Users/rubber/linux/kernel/bpf/hashtab.c: 1734
			 * after releasing the bucket lock. /Users/rubber/linux/kernel/bpf/hashtab.c: 1735
	/* If we are not copying data, we can go to next bucket and avoid /Users/rubber/linux/kernel/bpf/hashtab.c: 1758
	 * unlocking the rcu. /Users/rubber/linux/kernel/bpf/hashtab.c: 1759
 non-zero means percpu hash /Users/rubber/linux/kernel/bpf/hashtab.c: 1872
		/* no update/deletion on this bucket, prev_elem should be still valid /Users/rubber/linux/kernel/bpf/hashtab.c: 1895
		 * and we won't skip elements. /Users/rubber/linux/kernel/bpf/hashtab.c: 1896
	/* disable migration so percpu value prepared here will be the /Users/rubber/linux/kernel/bpf/hashtab.c: 2073
	 * same as the one seen by the bpf program with bpf_map_lookup_elem(). /Users/rubber/linux/kernel/bpf/hashtab.c: 2074
	/* per_cpu areas are zero-filled and bpf programs can only /Users/rubber/linux/kernel/bpf/hashtab.c: 2184
	 * access 'value_size' of them, so copying rounded areas /Users/rubber/linux/kernel/bpf/hashtab.c: 2185
	 * will not leak any kernel data /Users/rubber/linux/kernel/bpf/hashtab.c: 2186
	/* We do not mark LRU map element here in order to not mess up /Users/rubber/linux/kernel/bpf/hashtab.c: 2193
	 * eviction heuristics when user space does a map walk. /Users/rubber/linux/kernel/bpf/hashtab.c: 2194
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 1
 * Copyright (C) 2020 Google LLC. /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 4
/* For every LSM hook that allows attachment of BPF programs, declare a nop /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 20
 * function where a BPF program can be attached. /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 21
/* The set of hooks which are called without pagefaults disabled and are allowed /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 129
 * to "sleep" and thus can be used for sleepable BPF programs. /Users/rubber/linux/kernel/bpf/bpf_lsm.c: 130
 SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause) /Users/rubber/linux/kernel/bpf/disasm.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/disasm.c: 2
 * Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/disasm.c: 3
 BPF_NOSPEC, no UAPI */) { /Users/rubber/linux/kernel/bpf/disasm.c: 215
			/* At this point, we already made sure that the second /Users/rubber/linux/kernel/bpf/disasm.c: 242
			 * part of the ldimm64 insn is accessible. /Users/rubber/linux/kernel/bpf/disasm.c: 243
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/map_iter.c: 1
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/reuseport_array.c: 1
 * Copyright (c) 2018 Facebook /Users/rubber/linux/kernel/bpf/reuseport_array.c: 3
		/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 32
		 * Do not move this NULL assignment outside of /Users/rubber/linux/kernel/bpf/reuseport_array.c: 33
		 * sk->sk_callback_lock because there is /Users/rubber/linux/kernel/bpf/reuseport_array.c: 34
		 * a race with reuseport_array_free() /Users/rubber/linux/kernel/bpf/reuseport_array.c: 35
		 * which does not hold the reuseport_lock. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 36
	/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 102
	 * ops->map_*_elem() will not be able to access this /Users/rubber/linux/kernel/bpf/reuseport_array.c: 103
	 * array now. Hence, this function only races with /Users/rubber/linux/kernel/bpf/reuseport_array.c: 104
	 * bpf_sk_reuseport_detach() which was triggered by /Users/rubber/linux/kernel/bpf/reuseport_array.c: 105
	 * close() or disconnect(). /Users/rubber/linux/kernel/bpf/reuseport_array.c: 106
	 * /Users/rubber/linux/kernel/bpf/reuseport_array.c: 107
	 * This function and bpf_sk_reuseport_detach() are /Users/rubber/linux/kernel/bpf/reuseport_array.c: 108
	 * both removing sk from "array".  Who removes it /Users/rubber/linux/kernel/bpf/reuseport_array.c: 109
	 * first does not matter. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 110
	 * /Users/rubber/linux/kernel/bpf/reuseport_array.c: 111
	 * The only concern here is bpf_sk_reuseport_detach() /Users/rubber/linux/kernel/bpf/reuseport_array.c: 112
	 * may access "array" which is being freed here. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 113
	 * bpf_sk_reuseport_detach() access this "array" /Users/rubber/linux/kernel/bpf/reuseport_array.c: 114
	 * through sk->sk_user_data _and_ with sk->sk_callback_lock /Users/rubber/linux/kernel/bpf/reuseport_array.c: 115
	 * held which is enough because this "array" is not freed /Users/rubber/linux/kernel/bpf/reuseport_array.c: 116
	 * until all sk->sk_user_data has stopped referencing this "array". /Users/rubber/linux/kernel/bpf/reuseport_array.c: 117
	 * /Users/rubber/linux/kernel/bpf/reuseport_array.c: 118
	 * Hence, due to the above, taking "reuseport_lock" is not /Users/rubber/linux/kernel/bpf/reuseport_array.c: 119
	 * needed here. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 120
	/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 123
	 * Since reuseport_lock is not taken, sk is accessed under /Users/rubber/linux/kernel/bpf/reuseport_array.c: 124
	 * rcu_read_lock() /Users/rubber/linux/kernel/bpf/reuseport_array.c: 125
			/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 132
			 * No need for WRITE_ONCE(). At this point, /Users/rubber/linux/kernel/bpf/reuseport_array.c: 133
			 * no one is reading it without taking the /Users/rubber/linux/kernel/bpf/reuseport_array.c: 134
			 * sk->sk_callback_lock. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 135
	/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 144
	 * Once reaching here, all sk->sk_user_data is not /Users/rubber/linux/kernel/bpf/reuseport_array.c: 145
	 * referenceing this "array".  "array" can be freed now. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 146
	/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 218
	 * sk must be hashed (i.e. listening in the TCP case or binded /Users/rubber/linux/kernel/bpf/reuseport_array.c: 219
	 * in the UDP case) and /Users/rubber/linux/kernel/bpf/reuseport_array.c: 220
	 * it must also be a SO_REUSEPORT sk (i.e. reuse cannot be NULL). /Users/rubber/linux/kernel/bpf/reuseport_array.c: 221
	 * /Users/rubber/linux/kernel/bpf/reuseport_array.c: 222
	 * Also, sk will be used in bpf helper that is protected by /Users/rubber/linux/kernel/bpf/reuseport_array.c: 223
	 * rcu_read_lock(). /Users/rubber/linux/kernel/bpf/reuseport_array.c: 224
 * Called from syscall only. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 237
 * The "nsk" in the fd refcnt. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 238
 * The "osk" and "reuse" are protected by reuseport_lock. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 239
	/* /Users/rubber/linux/kernel/bpf/reuseport_array.c: 287
	 * Some of the checks only need reuseport_lock /Users/rubber/linux/kernel/bpf/reuseport_array.c: 288
	 * but it is done under sk_callback_lock also /Users/rubber/linux/kernel/bpf/reuseport_array.c: 289
	 * for simplicity reason. /Users/rubber/linux/kernel/bpf/reuseport_array.c: 290
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 1
	/* progs has all the bpf_prog that is populated /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 35
	 * to the func ptr of the kernel's struct /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 36
	 * (in kvalue.data). /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 37
	/* image is a page that has all the trampolines /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 40
	 * that stores the func args before calling the bpf_prog. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 41
	 * A PAGE_SIZE "image" is enough to store all trampoline for /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 42
	 * "progs[]". /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 43
	/* uvalue->data stores the kernel struct /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 46
	 * (e.g. tcp_congestion_ops) that is more useful /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 47
	 * to userspace than the kvalue.  For example, /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 48
	 * the bpf_prog's id is stored instead of the kernel /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 49
	 * address of a func ptr. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 50
	/* kvalue.data stores the actual kernel's struct /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 53
	 * (e.g. tcp_congestion_ops) that will be /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 54
	 * registered to the kernel subsystem. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 55
/* bpf_struct_ops_##_name (e.g. bpf_struct_ops_tcp_congestion_ops) is /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 63
 * the map's value exposed to the userspace and its btf-type-id is /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 64
 * stored at the map->btf_vmlinux_value_type_id. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 65
	/* No lock is needed.  state and refcnt do not need /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 263
	 * to be updated together under atomic context. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 264
		/* If st_ops->init_member does not handle it, /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 407
		 * we will only handle func ptrs and zero-ed members /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 408
		 * here.  Reject everything else. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 409
		/* Pair with smp_load_acquire() during lookup_elem(). /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 470
		 * It ensures the above udata updates (e.g. prog->aux->id) /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 471
		 * can be seen once BPF_STRUCT_OPS_STATE_INUSE is set. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 472
	/* Error during st_ops->reg().  It is very unlikely since /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 478
	 * the above init_member() should have caught it earlier /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 479
	 * before reg().  The only possibility is if there was a race /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 480
	 * in registering the struct_ops (under the same name) to /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 481
	 * a sub-system through different struct_ops's maps. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 482
		/* kvalue stores the /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 586
		 * struct bpf_struct_ops_tcp_congestions_ops /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 587
/* "const void *" because some subsystem is /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 629
 * passing a const (e.g. const struct tcp_congestion_ops *) /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 630
		/* The struct_ops's function may switch to another struct_ops. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 659
		 * /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 660
		 * For example, bpf_tcp_cc_x->init() may switch to /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 661
		 * another tcp_cc_y by calling /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 662
		 * setsockopt(TCP_CONGESTION, "tcp_cc_y"). /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 663
		 * During the switch,  bpf_struct_ops_put(tcp_cc_x) is called /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 664
		 * and its map->refcnt may reach 0 which then free its /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 665
		 * trampoline image while tcp_cc_x is still running. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 666
		 * /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 667
		 * Thus, a rcu grace period is needed here. /Users/rubber/linux/kernel/bpf/bpf_struct_ops.c: 668
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/stackmap.c: 1
/* Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/stackmap.c: 2
			/* /Users/rubber/linux/kernel/bpf/stackmap.c: 164
			 * PREEMPT_RT does not allow to trylock mmap sem in /Users/rubber/linux/kernel/bpf/stackmap.c: 165
			 * interrupt disabled context. Force the fallback code. /Users/rubber/linux/kernel/bpf/stackmap.c: 166
	/* /Users/rubber/linux/kernel/bpf/stackmap.c: 172
	 * We cannot do up_read() when the irq is disabled, because of /Users/rubber/linux/kernel/bpf/stackmap.c: 173
	 * risk to deadlock with rq_lock. To do build_id lookup when the /Users/rubber/linux/kernel/bpf/stackmap.c: 174
	 * irqs are disabled, we need to run up_read() in irq_work. We use /Users/rubber/linux/kernel/bpf/stackmap.c: 175
	 * a percpu variable to do the irq_work. If the irq_work is /Users/rubber/linux/kernel/bpf/stackmap.c: 176
	 * already used by another lookup, we fall back to report ips. /Users/rubber/linux/kernel/bpf/stackmap.c: 177
	 * /Users/rubber/linux/kernel/bpf/stackmap.c: 178
	 * Same fallback is used for kernel stack (!user) on a stackmap /Users/rubber/linux/kernel/bpf/stackmap.c: 179
	 * with build_id. /Users/rubber/linux/kernel/bpf/stackmap.c: 180
		/* The lock will be released once we're out of interrupt /Users/rubber/linux/kernel/bpf/stackmap.c: 212
		 * context. Tell lockdep that we've released it now so /Users/rubber/linux/kernel/bpf/stackmap.c: 213
		 * it doesn't complain that we forgot to release it. /Users/rubber/linux/kernel/bpf/stackmap.c: 214
	/* stack_trace_save_tsk() works on unsigned long array, while /Users/rubber/linux/kernel/bpf/stackmap.c: 237
	 * perf_callchain_entry uses u64 array. For 32-bit systems, it is /Users/rubber/linux/kernel/bpf/stackmap.c: 238
	 * necessary to fix this mismatch. /Users/rubber/linux/kernel/bpf/stackmap.c: 239
	/* get_perf_callchain() guarantees that trace->nr >= init_nr /Users/rubber/linux/kernel/bpf/stackmap.c: 273
	 * and trace-nr <= sysctl_perf_event_max_stack, so trace_nr <= max_depth /Users/rubber/linux/kernel/bpf/stackmap.c: 274
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/map_in_map.c: 1
/* Copyright (c) 2017 Facebook /Users/rubber/linux/kernel/bpf/map_in_map.c: 2
 not used */, /Users/rubber/linux/kernel/bpf/map_in_map.c: 90
	/* ptr->ops->map_free() has to go through one /Users/rubber/linux/kernel/bpf/map_in_map.c: 113
	 * rcu grace period by itself. /Users/rubber/linux/kernel/bpf/map_in_map.c: 114
 * Copyright (C) 2017-2018 Netronome Systems, Inc. /Users/rubber/linux/kernel/bpf/offload.c: 2
 * This software is licensed under the GNU General License Version 2, /Users/rubber/linux/kernel/bpf/offload.c: 4
 * June 1991 as shown in the file COPYING in the top-level directory of this /Users/rubber/linux/kernel/bpf/offload.c: 5
 * source tree. /Users/rubber/linux/kernel/bpf/offload.c: 6
 * THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" /Users/rubber/linux/kernel/bpf/offload.c: 8
 * WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, /Users/rubber/linux/kernel/bpf/offload.c: 9
 * BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS /Users/rubber/linux/kernel/bpf/offload.c: 10
 * FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE /Users/rubber/linux/kernel/bpf/offload.c: 11
 * OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME /Users/rubber/linux/kernel/bpf/offload.c: 12
 * THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. /Users/rubber/linux/kernel/bpf/offload.c: 13
/* Protects offdevs, members of bpf_offload_netdev and offload members /Users/rubber/linux/kernel/bpf/offload.c: 29
 * of all progs. /Users/rubber/linux/kernel/bpf/offload.c: 30
 * RTNL lock cannot be taken when holding this lock. /Users/rubber/linux/kernel/bpf/offload.c: 31
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/syscall.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/syscall.c: 2
 * If we're handed a bigger struct than we know of, ensure all the unknown bits /Users/rubber/linux/kernel/bpf/syscall.c: 68
 * are 0 - i.e. new user-space does not rely on any kernel feature extensions /Users/rubber/linux/kernel/bpf/syscall.c: 69
 * we don't know about yet. /Users/rubber/linux/kernel/bpf/syscall.c: 70
 * There is a ToCToU between this function call and the following /Users/rubber/linux/kernel/bpf/syscall.c: 72
 * copy_from_user() call. However, this is not a concern since this function is /Users/rubber/linux/kernel/bpf/syscall.c: 73
 * meant to be a future-proofing of bits. /Users/rubber/linux/kernel/bpf/syscall.c: 74
	/* Wait for any running BPF programs to complete so that /Users/rubber/linux/kernel/bpf/syscall.c: 165
	 * userspace, when we return to it, knows that all programs /Users/rubber/linux/kernel/bpf/syscall.c: 166
	 * that could be running use the new map value. /Users/rubber/linux/kernel/bpf/syscall.c: 167
/* Please, do not use this function outside from the map creation path /Users/rubber/linux/kernel/bpf/syscall.c: 292
 * (e.g. in map update path) without taking care of setting the active /Users/rubber/linux/kernel/bpf/syscall.c: 293
 * memory cgroup (see at bpf_map_kmalloc_node() for example). /Users/rubber/linux/kernel/bpf/syscall.c: 294
	/* We really just want to fail instead of triggering OOM killer /Users/rubber/linux/kernel/bpf/syscall.c: 298
	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc, /Users/rubber/linux/kernel/bpf/syscall.c: 299
	 * which is used for lower order allocation requests. /Users/rubber/linux/kernel/bpf/syscall.c: 300
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 301
	 * It has been observed that higher order allocation requests done by /Users/rubber/linux/kernel/bpf/syscall.c: 302
	 * vmalloc with __GFP_NORETRY being set might fail due to not trying /Users/rubber/linux/kernel/bpf/syscall.c: 303
	 * to reclaim memory from the page cache, thus we set /Users/rubber/linux/kernel/bpf/syscall.c: 304
	 * __GFP_RETRY_MAYFAIL to avoid such situations. /Users/rubber/linux/kernel/bpf/syscall.c: 305
	/* Some map creation flags are not tied to the map object but /Users/rubber/linux/kernel/bpf/syscall.c: 350
	 * rather to the map fd instead, so they have no meaning upon /Users/rubber/linux/kernel/bpf/syscall.c: 351
	 * map object inspection since multiple file descriptors with /Users/rubber/linux/kernel/bpf/syscall.c: 352
	 * different (access) properties can exist here. Thus, given /Users/rubber/linux/kernel/bpf/syscall.c: 353
	 * this has zero meaning for the map itself, lets clear these /Users/rubber/linux/kernel/bpf/syscall.c: 354
	 * from here. /Users/rubber/linux/kernel/bpf/syscall.c: 355
	/* Offloaded maps are removed from the IDR store when their device /Users/rubber/linux/kernel/bpf/syscall.c: 393
	 * disappears - even if someone holds an fd to them they are unusable, /Users/rubber/linux/kernel/bpf/syscall.c: 394
	 * the memory is gone, all ops will fail; they are simply waiting for /Users/rubber/linux/kernel/bpf/syscall.c: 395
	 * refcnt to drop to be freed. /Users/rubber/linux/kernel/bpf/syscall.c: 396
/* decrement map refcnt and schedule it for freeing via workqueue /Users/rubber/linux/kernel/bpf/syscall.c: 493
 * (unrelying map implementation ops->map_free() might sleep) /Users/rubber/linux/kernel/bpf/syscall.c: 494
	/* Our file permissions may have been overridden by global /Users/rubber/linux/kernel/bpf/syscall.c: 534
	 * map permissions facing syscall side. /Users/rubber/linux/kernel/bpf/syscall.c: 535
/* Provides an approximation of the map's memory footprint. /Users/rubber/linux/kernel/bpf/syscall.c: 543
 * Used only to provide a backward compatibility and display /Users/rubber/linux/kernel/bpf/syscall.c: 544
 * a reasonable "memlock" info. /Users/rubber/linux/kernel/bpf/syscall.c: 545
	/* We need this handler such that alloc_file() enables /Users/rubber/linux/kernel/bpf/syscall.c: 599
	 * f_mode with FMODE_CAN_READ. /Users/rubber/linux/kernel/bpf/syscall.c: 600
	/* We need this handler such that alloc_file() enables /Users/rubber/linux/kernel/bpf/syscall.c: 608
	 * f_mode with FMODE_CAN_WRITE. /Users/rubber/linux/kernel/bpf/syscall.c: 609
		/* map is meant to be read-only, so do not allow mapping as /Users/rubber/linux/kernel/bpf/syscall.c: 656
		 * writable, because it's possible to leak a writable page /Users/rubber/linux/kernel/bpf/syscall.c: 657
		 * reference and allows user-space to still modify it after /Users/rubber/linux/kernel/bpf/syscall.c: 658
		 * freezing, while verifier will assume contents do not change /Users/rubber/linux/kernel/bpf/syscall.c: 659
/* dst and src must have at least "size" number of bytes. /Users/rubber/linux/kernel/bpf/syscall.c: 738
 * Return strlen on success and < 0 on error. /Users/rubber/linux/kernel/bpf/syscall.c: 739
	    /* Even the map's value is a kernel's struct, /Users/rubber/linux/kernel/bpf/syscall.c: 880
	     * the bpf_prog.o must have BTF to begin with /Users/rubber/linux/kernel/bpf/syscall.c: 881
	     * to figure out the corresponding kernel's /Users/rubber/linux/kernel/bpf/syscall.c: 882
	     * counter part.  Thus, attr->btf_fd has /Users/rubber/linux/kernel/bpf/syscall.c: 883
	     * to be valid also. /Users/rubber/linux/kernel/bpf/syscall.c: 884
		/* failed to allocate fd. /Users/rubber/linux/kernel/bpf/syscall.c: 926
		 * bpf_map_put_with_uref() is needed because the above /Users/rubber/linux/kernel/bpf/syscall.c: 927
		 * bpf_map_alloc_id() has published the map /Users/rubber/linux/kernel/bpf/syscall.c: 928
		 * to the userspace and the userspace may /Users/rubber/linux/kernel/bpf/syscall.c: 929
		 * have refcnt-ed it through BPF_MAP_GET_FD_BY_ID. /Users/rubber/linux/kernel/bpf/syscall.c: 930
/* if error is returned, fd is released. /Users/rubber/linux/kernel/bpf/syscall.c: 946
 * On success caller should complete fd access with matching fdput() /Users/rubber/linux/kernel/bpf/syscall.c: 947
	/* cBPF to eBPF migrations are currently not in the idr store. /Users/rubber/linux/kernel/bpf/syscall.c: 1745
	 * Offloaded programs are removed from the store when their device /Users/rubber/linux/kernel/bpf/syscall.c: 1746
	 * disappears - even if someone grabs an fd to them they are unusable, /Users/rubber/linux/kernel/bpf/syscall.c: 1747
	 * simply waiting for refcnt to drop to be freed. /Users/rubber/linux/kernel/bpf/syscall.c: 1748
	/* Only to be used for undoing previous bpf_prog_add() in some /Users/rubber/linux/kernel/bpf/syscall.c: 1946
	 * error path. We still know that another entity in our call /Users/rubber/linux/kernel/bpf/syscall.c: 1947
	 * path holds a reference to the program, thus atomic_sub() can /Users/rubber/linux/kernel/bpf/syscall.c: 1948
	 * be safely used in such cases! /Users/rubber/linux/kernel/bpf/syscall.c: 1949
/* Initially all BPF programs could be loaded w/o specifying /Users/rubber/linux/kernel/bpf/syscall.c: 2022
 * expected_attach_type. Later for some of them specifying expected_attach_type /Users/rubber/linux/kernel/bpf/syscall.c: 2023
 * at load time became required so that program could be validated properly. /Users/rubber/linux/kernel/bpf/syscall.c: 2024
 * Programs of types that are allowed to be loaded both w/ and w/o (for /Users/rubber/linux/kernel/bpf/syscall.c: 2025
 * backward compatibility) expected_attach_type, should have the default attach /Users/rubber/linux/kernel/bpf/syscall.c: 2026
 * type assigned to expected_attach_type for the latter case, so that it can be /Users/rubber/linux/kernel/bpf/syscall.c: 2027
 * validated later at attach time. /Users/rubber/linux/kernel/bpf/syscall.c: 2028
 * bpf_prog_load_fixup_attach_type() sets expected_attach_type in @attr if /Users/rubber/linux/kernel/bpf/syscall.c: 2030
 * prog type requires it but has some attach types that have to be backward /Users/rubber/linux/kernel/bpf/syscall.c: 2031
 * compatible. /Users/rubber/linux/kernel/bpf/syscall.c: 2032
		/* Unfortunately BPF_ATTACH_TYPE_UNSPEC enumeration doesn't /Users/rubber/linux/kernel/bpf/syscall.c: 2038
		 * exist so checking for non-zero is the way to go here. /Users/rubber/linux/kernel/bpf/syscall.c: 2039
	/* attach_prog_fd/attach_btf_obj_fd can specify fd of either bpf_prog /Users/rubber/linux/kernel/bpf/syscall.c: 2250
	 * or btf, we need to check which one it is /Users/rubber/linux/kernel/bpf/syscall.c: 2251
				/* attaching through specifying bpf_prog's BTF /Users/rubber/linux/kernel/bpf/syscall.c: 2261
				 * objects directly might be supported eventually /Users/rubber/linux/kernel/bpf/syscall.c: 2262
	/* Upon success of bpf_prog_alloc_id(), the BPF prog is /Users/rubber/linux/kernel/bpf/syscall.c: 2355
	 * effectively publicly exposed. However, retrieving via /Users/rubber/linux/kernel/bpf/syscall.c: 2356
	 * bpf_prog_get_fd_by_id() will take another reference, /Users/rubber/linux/kernel/bpf/syscall.c: 2357
	 * therefore it cannot be gone underneath us. /Users/rubber/linux/kernel/bpf/syscall.c: 2358
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2359
	 * Only for the time /after/ successful bpf_prog_new_fd() /Users/rubber/linux/kernel/bpf/syscall.c: 2360
	 * and before returning to userspace, we might just hold /Users/rubber/linux/kernel/bpf/syscall.c: 2361
	 * one reference and any parallel close on that fd could /Users/rubber/linux/kernel/bpf/syscall.c: 2362
	 * rip everything out. Hence, below notifications must /Users/rubber/linux/kernel/bpf/syscall.c: 2363
	 * happen before bpf_prog_new_fd(). /Users/rubber/linux/kernel/bpf/syscall.c: 2364
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2365
	 * Also, any failure handling from this point onwards must /Users/rubber/linux/kernel/bpf/syscall.c: 2366
	 * be using bpf_prog_put() given the program is exposed. /Users/rubber/linux/kernel/bpf/syscall.c: 2367
	/* In case we have subprogs, we need to wait for a grace /Users/rubber/linux/kernel/bpf/syscall.c: 2379
	 * period before we can tear down JIT memory since symbols /Users/rubber/linux/kernel/bpf/syscall.c: 2380
	 * are already exposed under kallsyms. /Users/rubber/linux/kernel/bpf/syscall.c: 2381
/* Clean up bpf_link and corresponding anon_inode file and FD. After /Users/rubber/linux/kernel/bpf/syscall.c: 2435
 * anon_inode is created, bpf_link can't be just kfree()'d due to deferred /Users/rubber/linux/kernel/bpf/syscall.c: 2436
 * anon_inode's release() call. This helper marksbpf_link as /Users/rubber/linux/kernel/bpf/syscall.c: 2437
 * defunct, releases anon_inode file and puts reserved FD. bpf_prog's refcnt /Users/rubber/linux/kernel/bpf/syscall.c: 2438
 * is not decremented, it's the responsibility of a calling code that failed /Users/rubber/linux/kernel/bpf/syscall.c: 2439
 * to complete bpf_link initialization. /Users/rubber/linux/kernel/bpf/syscall.c: 2440
/* bpf_link_put can be called from atomic context, but ensures that resources /Users/rubber/linux/kernel/bpf/syscall.c: 2475
 * are freed from process context /Users/rubber/linux/kernel/bpf/syscall.c: 2476
/* Prepare bpf_link to be exposed to user-space by allocating anon_inode file, /Users/rubber/linux/kernel/bpf/syscall.c: 2554
 * reserving unused FD and allocating ID from link_idr. This is to be paired /Users/rubber/linux/kernel/bpf/syscall.c: 2555
 * with bpf_link_settle() to install FD and ID and expose bpf_link to /Users/rubber/linux/kernel/bpf/syscall.c: 2556
 * user-space, if bpf_link is successfully attached. If not, bpf_link and /Users/rubber/linux/kernel/bpf/syscall.c: 2557
 * pre-allocated resources are to be freed with bpf_cleanup() call. All the /Users/rubber/linux/kernel/bpf/syscall.c: 2558
 * transient state is passed around in struct bpf_link_primer. /Users/rubber/linux/kernel/bpf/syscall.c: 2559
 * This is preferred way to create and initialize bpf_link, especially when /Users/rubber/linux/kernel/bpf/syscall.c: 2560
 * there are complicated and expensive operations inbetween creating bpf_link /Users/rubber/linux/kernel/bpf/syscall.c: 2561
 * itself and attaching it to BPF hook. By using bpf_link_prime() and /Users/rubber/linux/kernel/bpf/syscall.c: 2562
 * bpf_link_settle() kernel code using bpf_link doesn't have to perform /Users/rubber/linux/kernel/bpf/syscall.c: 2563
 * expensive (and potentially failing) roll back operations in a rare case /Users/rubber/linux/kernel/bpf/syscall.c: 2564
 * that file, FD, or ID can't be allocated. /Users/rubber/linux/kernel/bpf/syscall.c: 2565
	/* There are a few possible cases here: /Users/rubber/linux/kernel/bpf/syscall.c: 2765
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2766
	 * - if prog->aux->dst_trampoline is set, the program was just loaded /Users/rubber/linux/kernel/bpf/syscall.c: 2767
	 *   and not yet attached to anything, so we can use the values stored /Users/rubber/linux/kernel/bpf/syscall.c: 2768
	 *   in prog->aux /Users/rubber/linux/kernel/bpf/syscall.c: 2769
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2770
	 * - if prog->aux->dst_trampoline is NULL, the program has already been /Users/rubber/linux/kernel/bpf/syscall.c: 2771
         *   attached to a target and its initial target was cleared (below) /Users/rubber/linux/kernel/bpf/syscall.c: 2772
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2773
	 * - if tgt_prog != NULL, the caller specified tgt_prog_fd + /Users/rubber/linux/kernel/bpf/syscall.c: 2774
	 *   target_btf_id using the link_create API. /Users/rubber/linux/kernel/bpf/syscall.c: 2775
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2776
	 * - if tgt_prog == NULL when this function was called using the old /Users/rubber/linux/kernel/bpf/syscall.c: 2777
	 *   raw_tracepoint_open API, and we need a target from prog->aux /Users/rubber/linux/kernel/bpf/syscall.c: 2778
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 2779
	 * - if prog->aux->dst_trampoline and tgt_prog is NULL, the program /Users/rubber/linux/kernel/bpf/syscall.c: 2780
	 *   was detached and is going for re-attachment. /Users/rubber/linux/kernel/bpf/syscall.c: 2781
		/* /Users/rubber/linux/kernel/bpf/syscall.c: 2784
		 * Allow re-attach for TRACING and LSM programs. If it's /Users/rubber/linux/kernel/bpf/syscall.c: 2785
		 * currently linked, bpf_trampoline_link_prog will fail. /Users/rubber/linux/kernel/bpf/syscall.c: 2786
		 * EXT programs need to specify tgt_prog_fd, so they /Users/rubber/linux/kernel/bpf/syscall.c: 2787
		 * re-attach in separate code path. /Users/rubber/linux/kernel/bpf/syscall.c: 2788
		/* If there is no saved target, or the specified target is /Users/rubber/linux/kernel/bpf/syscall.c: 2801
		 * different from the destination specified at load time, we /Users/rubber/linux/kernel/bpf/syscall.c: 2802
		 * need a new trampoline and a check for compatibility /Users/rubber/linux/kernel/bpf/syscall.c: 2803
		/* The caller didn't specify a target, or the target was the /Users/rubber/linux/kernel/bpf/syscall.c: 2818
		 * same as the destination supplied during program load. This /Users/rubber/linux/kernel/bpf/syscall.c: 2819
		 * means we can reuse the trampoline and reference from program /Users/rubber/linux/kernel/bpf/syscall.c: 2820
		 * load time, and there is no need to allocate a new one. This /Users/rubber/linux/kernel/bpf/syscall.c: 2821
		 * can only happen once for any program, as the saved values in /Users/rubber/linux/kernel/bpf/syscall.c: 2822
		 * prog->aux are cleared below. /Users/rubber/linux/kernel/bpf/syscall.c: 2823
	/* Always clear the trampoline and target prog from prog->aux to make /Users/rubber/linux/kernel/bpf/syscall.c: 2843
	 * sure the original attach destination is not kept alive after a /Users/rubber/linux/kernel/bpf/syscall.c: 2844
	 * program is (re-)attached to another target. /Users/rubber/linux/kernel/bpf/syscall.c: 2845
			/* The attach point for this category of programs /Users/rubber/linux/kernel/bpf/syscall.c: 3042
			 * should be specified via btf_id during program load. /Users/rubber/linux/kernel/bpf/syscall.c: 3043
			/* cg-skb progs can be loaded by unpriv user. /Users/rubber/linux/kernel/bpf/syscall.c: 3120
			 * check permissions at attach time. /Users/rubber/linux/kernel/bpf/syscall.c: 3121
	/* /Users/rubber/linux/kernel/bpf/syscall.c: 3583
	 * Ensure info.*_rec_size is the same as kernel expected size /Users/rubber/linux/kernel/bpf/syscall.c: 3584
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 3585
	 * or /Users/rubber/linux/kernel/bpf/syscall.c: 3586
	 * /Users/rubber/linux/kernel/bpf/syscall.c: 3587
	 * Only allow zero *_rec_size if both _rec_size and _cnt are /Users/rubber/linux/kernel/bpf/syscall.c: 3588
	 * zero.  In this case, the kernel will set the expected /Users/rubber/linux/kernel/bpf/syscall.c: 3589
	 * _rec_size back to the info. /Users/rubber/linux/kernel/bpf/syscall.c: 3590
	/* NOTE: the following code is supposed to be skipped for offload. /Users/rubber/linux/kernel/bpf/syscall.c: 3711
	 * bpf_prog_offload_info_fill() is the place to fill similar fields /Users/rubber/linux/kernel/bpf/syscall.c: 3712
	 * for offload. /Users/rubber/linux/kernel/bpf/syscall.c: 3713
			/* for multi-function programs, copy the JITed /Users/rubber/linux/kernel/bpf/syscall.c: 3731
			 * instructions for all the functions /Users/rubber/linux/kernel/bpf/syscall.c: 3732
			/* copy the address of the kernel symbol /Users/rubber/linux/kernel/bpf/syscall.c: 3767
			 * corresponding to each function /Users/rubber/linux/kernel/bpf/syscall.c: 3768
			/* ubuf cannot hold the string with NULL terminator, /Users/rubber/linux/kernel/bpf/syscall.c: 4079
			 * do a partial copy with NULL terminator. /Users/rubber/linux/kernel/bpf/syscall.c: 4080
	/* case BPF_PROG_TEST_RUN: /Users/rubber/linux/kernel/bpf/syscall.c: 4761
	 * is not part of this list to prevent recursive test_run /Users/rubber/linux/kernel/bpf/syscall.c: 4762
	/* When bpf program calls this helper there should not be /Users/rubber/linux/kernel/bpf/syscall.c: 4787
	 * an fdget() without matching completed fdput(). /Users/rubber/linux/kernel/bpf/syscall.c: 4788
	 * This helper is allowed in the following callchain only: /Users/rubber/linux/kernel/bpf/syscall.c: 4789
	 * sys_bpf->prog_test_run->bpf_prog->bpf_sys_close /Users/rubber/linux/kernel/bpf/syscall.c: 4790
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/helpers.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/helpers.c: 2
/* If kernel subsystem is allowing eBPF programs to call this function, /Users/rubber/linux/kernel/bpf/helpers.c: 21
 * inside its own verifier_ops->get_func_proto() callback it should return /Users/rubber/linux/kernel/bpf/helpers.c: 22
 * bpf_map_lookup_elem_proto, so that verifier can properly check the arguments /Users/rubber/linux/kernel/bpf/helpers.c: 23
 * Different map implementations will rely on rcu in map methods /Users/rubber/linux/kernel/bpf/helpers.c: 25
 * lookup/update/delete, therefore eBPF programs must run under rcu lock /Users/rubber/linux/kernel/bpf/helpers.c: 26
 * if program is allowed to access maps, so check rcu_read_lock_held in /Users/rubber/linux/kernel/bpf/helpers.c: 27
 * all three functions. /Users/rubber/linux/kernel/bpf/helpers.c: 28
	/* Verifier guarantees that size > 0. For task->comm exceeding /Users/rubber/linux/kernel/bpf/helpers.c: 227
	 * size, guarantee that buf is %NUL-terminated. Unconditionally /Users/rubber/linux/kernel/bpf/helpers.c: 228
	 * done here to save the size test. /Users/rubber/linux/kernel/bpf/helpers.c: 229
	/* flags argument is not used now, /Users/rubber/linux/kernel/bpf/helpers.c: 409
	 * but provides an ability to extend the API. /Users/rubber/linux/kernel/bpf/helpers.c: 410
	 * verifier checks that its value is correct. /Users/rubber/linux/kernel/bpf/helpers.c: 411
/* Per-cpu temp buffers used by printf-like helpers to store the bprintf binary /Users/rubber/linux/kernel/bpf/helpers.c: 710
 * arguments representation. /Users/rubber/linux/kernel/bpf/helpers.c: 711
 * bpf_bprintf_prepare - Generic pass on format strings for bprintf-like helpers /Users/rubber/linux/kernel/bpf/helpers.c: 750
 * Returns a negative value if fmt is an invalid format string or 0 otherwise. /Users/rubber/linux/kernel/bpf/helpers.c: 752
 * This can be used in two ways: /Users/rubber/linux/kernel/bpf/helpers.c: 754
 * - Format string verification only: when bin_args is NULL /Users/rubber/linux/kernel/bpf/helpers.c: 755
 * - Arguments preparation: in addition to the above verification, it writes in /Users/rubber/linux/kernel/bpf/helpers.c: 756
 *   bin_args a binary representation of arguments usable by bstr_printf where /Users/rubber/linux/kernel/bpf/helpers.c: 757
 *   pointers from BPF have been sanitized. /Users/rubber/linux/kernel/bpf/helpers.c: 758
 * In argument preparation mode, if 0 is returned, safe temporary buffers are /Users/rubber/linux/kernel/bpf/helpers.c: 760
 * allocated and bpf_bprintf_cleanup should be called to free them after use. /Users/rubber/linux/kernel/bpf/helpers.c: 761
		/* The string is zero-terminated so if fmt[i] != 0, we can /Users/rubber/linux/kernel/bpf/helpers.c: 804
		 * always access fmt[i + 1], in the worst case it will be a 0 /Users/rubber/linux/kernel/bpf/helpers.c: 805
			/* hack: bstr_printf expects IP addresses to be /Users/rubber/linux/kernel/bpf/helpers.c: 877
			 * pre-formatted as strings, ironically, the easiest way /Users/rubber/linux/kernel/bpf/helpers.c: 878
			 * to do that is to call snprintf. /Users/rubber/linux/kernel/bpf/helpers.c: 879
	/* ARG_PTR_TO_CONST_STR guarantees that fmt is zero-terminated so we /Users/rubber/linux/kernel/bpf/helpers.c: 993
	 * can safely give an unbounded size. /Users/rubber/linux/kernel/bpf/helpers.c: 994
/* BPF map elements can contain 'struct bpf_timer'. /Users/rubber/linux/kernel/bpf/helpers.c: 1018
 * Such map owns all of its BPF timers. /Users/rubber/linux/kernel/bpf/helpers.c: 1019
 * 'struct bpf_timer' is allocated as part of map element allocation /Users/rubber/linux/kernel/bpf/helpers.c: 1020
 * and it's zero initialized. /Users/rubber/linux/kernel/bpf/helpers.c: 1021
 * That space is used to keep 'struct bpf_timer_kern'. /Users/rubber/linux/kernel/bpf/helpers.c: 1022
 * bpf_timer_init() allocates 'struct bpf_hrtimer', inits hrtimer, and /Users/rubber/linux/kernel/bpf/helpers.c: 1023
 * remembers 'struct bpf_map *' pointer it's part of. /Users/rubber/linux/kernel/bpf/helpers.c: 1024
 * bpf_timer_set_callback() increments prog refcnt and assign bpf callback_fn. /Users/rubber/linux/kernel/bpf/helpers.c: 1025
 * bpf_timer_start() arms the timer. /Users/rubber/linux/kernel/bpf/helpers.c: 1026
 * If user space reference to a map goes to zero at this point /Users/rubber/linux/kernel/bpf/helpers.c: 1027
 * ops->map_release_uref callback is responsible for cancelling the timers, /Users/rubber/linux/kernel/bpf/helpers.c: 1028
 * freeing their memory, and decrementing prog's refcnts. /Users/rubber/linux/kernel/bpf/helpers.c: 1029
 * bpf_timer_cancel() cancels the timer and decrements prog's refcnt. /Users/rubber/linux/kernel/bpf/helpers.c: 1030
 * Inner maps can contain bpf timers as well. ops->map_release_uref is /Users/rubber/linux/kernel/bpf/helpers.c: 1031
 * freeing the timers when inner map is replaced or deleted by user space. /Users/rubber/linux/kernel/bpf/helpers.c: 1032
	/* bpf_spin_lock is used here instead of spinlock_t to make /Users/rubber/linux/kernel/bpf/helpers.c: 1045
	 * sure that it always fits into space resereved by struct bpf_timer /Users/rubber/linux/kernel/bpf/helpers.c: 1046
	 * regardless of LOCKDEP and spinlock debug flags. /Users/rubber/linux/kernel/bpf/helpers.c: 1047
	/* bpf_timer_cb() runs in hrtimer_run_softirq. It doesn't migrate and /Users/rubber/linux/kernel/bpf/helpers.c: 1067
	 * cannot be preempted by another bpf_timer_cb() on the same cpu. /Users/rubber/linux/kernel/bpf/helpers.c: 1068
	 * Remember the timer this callback is servicing to prevent /Users/rubber/linux/kernel/bpf/helpers.c: 1069
	 * deadlock if callback_fn() calls bpf_timer_cancel() or /Users/rubber/linux/kernel/bpf/helpers.c: 1070
	 * bpf_map_delete_elem() on the same timer. /Users/rubber/linux/kernel/bpf/helpers.c: 1071
		/* maps with timers must be either held by user space /Users/rubber/linux/kernel/bpf/helpers.c: 1119
		 * or pinned in bpffs. /Users/rubber/linux/kernel/bpf/helpers.c: 1120
		/* maps with timers must be either held by user space /Users/rubber/linux/kernel/bpf/helpers.c: 1168
		 * or pinned in bpffs. Otherwise timer might still be /Users/rubber/linux/kernel/bpf/helpers.c: 1169
		 * running even when bpf prog is detached and user space /Users/rubber/linux/kernel/bpf/helpers.c: 1170
		 * is gone, since map_release_uref won't ever be called. /Users/rubber/linux/kernel/bpf/helpers.c: 1171
		/* Bump prog refcnt once. Every bpf_timer_set_callback() /Users/rubber/linux/kernel/bpf/helpers.c: 1178
		 * can pick different callback_fn-s within the same prog. /Users/rubber/linux/kernel/bpf/helpers.c: 1179
		/* If bpf callback_fn is trying to bpf_timer_cancel() /Users/rubber/linux/kernel/bpf/helpers.c: 1260
		 * its own timer the hrtimer_cancel() will deadlock /Users/rubber/linux/kernel/bpf/helpers.c: 1261
		 * since it waits for callback_fn to finish /Users/rubber/linux/kernel/bpf/helpers.c: 1262
	/* Cancel the timer and wait for associated callback to finish /Users/rubber/linux/kernel/bpf/helpers.c: 1270
	 * if it was running. /Users/rubber/linux/kernel/bpf/helpers.c: 1271
/* This function is called by map_delete/update_elem for individual element and /Users/rubber/linux/kernel/bpf/helpers.c: 1284
 * by ops->map_release_uref when the user space reference to a map reaches zero. /Users/rubber/linux/kernel/bpf/helpers.c: 1285
	/* The subsequent bpf_timer_start/cancel() helpers won't be able to use /Users/rubber/linux/kernel/bpf/helpers.c: 1302
	 * this timer, since it won't be initialized. /Users/rubber/linux/kernel/bpf/helpers.c: 1303
	/* Cancel the timer and wait for callback to complete if it was running. /Users/rubber/linux/kernel/bpf/helpers.c: 1310
	 * If hrtimer_cancel() can be safely called it's safe to call kfree(t) /Users/rubber/linux/kernel/bpf/helpers.c: 1311
	 * right after for both preallocated and non-preallocated maps. /Users/rubber/linux/kernel/bpf/helpers.c: 1312
	 * The timer->timer = NULL was already done and no code path can /Users/rubber/linux/kernel/bpf/helpers.c: 1313
	 * see address 't' anymore. /Users/rubber/linux/kernel/bpf/helpers.c: 1314
	 * /Users/rubber/linux/kernel/bpf/helpers.c: 1315
	 * Check that bpf_map_delete/update_elem() wasn't called from timer /Users/rubber/linux/kernel/bpf/helpers.c: 1316
	 * callback_fn. In such case don't call hrtimer_cancel() (since it will /Users/rubber/linux/kernel/bpf/helpers.c: 1317
	 * deadlock) and don't call hrtimer_try_to_cancel() (since it will just /Users/rubber/linux/kernel/bpf/helpers.c: 1318
	 * return -1). Though callback_fn is still running on this cpu it's /Users/rubber/linux/kernel/bpf/helpers.c: 1319
	 * safe to do kfree(t) because bpf_timer_cb() read everything it needed /Users/rubber/linux/kernel/bpf/helpers.c: 1320
	 * from 't'. The bpf subprog callback_fn won't be able to access 't', /Users/rubber/linux/kernel/bpf/helpers.c: 1321
	 * since timer->timer = NULL was already done. The timer will be /Users/rubber/linux/kernel/bpf/helpers.c: 1322
	 * effectively cancelled because bpf_timer_cb() will return /Users/rubber/linux/kernel/bpf/helpers.c: 1323
	 * HRTIMER_NORESTART. /Users/rubber/linux/kernel/bpf/helpers.c: 1324
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 1
/* local_storage->lock must be held and selem->local_storage == local_storage. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 84
 * The caller must ensure selem->smap is still valid to be /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 85
 * dereferenced for its smap->elem_size and smap->cache_idx. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 86
	/* All uncharging on the owner must be done first. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 99
	 * The owner may be freed once the last selem is unlinked /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 100
	 * from local_storage. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 101
		/* local_storage is not freed now.  local_storage->lock is /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 115
		 * still held and raw_spin_unlock_bh(&local_storage->lock) /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 116
		 * will be done by the caller. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 117
		 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 118
		 * Although the unlock will be done under /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 119
		 * rcu_read_lock(),  it is more intutivie to /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 120
		 * read if kfree_rcu(local_storage, rcu) is done /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 121
		 * after the raw_spin_unlock_bh(&local_storage->lock). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 122
		 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 123
		 * Hence, a "bool free_local_storage" is returned /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 124
		 * to the caller which then calls the kfree_rcu() /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 125
		 * after unlock. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 126
	/* Always unlink from map before unlinking from local_storage /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 199
	 * because selem will be freed after successfully unlinked from /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 200
	 * the local_storage. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 201
		/* spinlock is needed to avoid racing with the /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 232
		 * parallel delete.  Otherwise, publishing an already /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 233
		 * deleted sdata to the cache will become a use-after-free /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 234
		 * problem in the next bpf_local_storage_lookup(). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 235
	/* Publish storage to the owner. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 289
	 * Instead of using any lock of the kernel object (i.e. owner), /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 290
	 * cmpxchg will work with any kernel object regardless what /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 291
	 * the running context is, bh, irq...etc. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 292
	 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 293
	 * From now on, the owner->storage pointer (e.g. sk->sk_bpf_storage) /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 294
	 * is protected by the storage->lock.  Hence, when freeing /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 295
	 * the owner->storage, the storage->lock must be held before /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 296
	 * setting owner->storage ptr to NULL. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 297
		/* Note that even first_selem was linked to smap's /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 305
		 * bucket->list, first_selem can be freed immediately /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 306
		 * (instead of kfree_rcu) because /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 307
		 * bpf_local_storage_map_free() does a /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 308
		 * synchronize_rcu() before walking the bucket->list. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 309
		 * Hence, no one is accessing selem from the /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 310
		 * bucket->list under rcu_read_lock(). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 311
/* sk cannot be going away because it is linking new elem /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 323
 * to sk->sk_bpf_storage. (i.e. sk->sk_refcnt cannot be 0). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 324
 * Otherwise, it will become a leak (and other memory issues /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 325
 * during map destruction). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 326
		/* Hoping to find an old_sdata to do inline update /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 367
		 * such that it can avoid taking the local_storage->lock /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 368
		 * and changing the lists. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 369
		/* A parallel del is happening and local_storage is going /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 387
		 * away.  It has just been checked before, so very /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 388
		 * unlikely.  Return instead of retry to keep things /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 389
		 * simple. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 390
	/* local_storage->lock is held.  Hence, we are sure /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 408
	 * we can unlink and uncharge the old_sdata successfully /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 409
	 * later.  Hence, instead of charging the new selem now /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 410
	 * and then uncharge the old selem later (which may cause /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 411
	 * a potential but unnecessary charge failure),  avoid taking /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 412
	 * a charge at all here (the "!old_sdata" check) and the /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 413
	 * old_sdata will not be uncharged later during /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 414
	 * bpf_selem_unlink_storage_nolock(). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 415
	/* Note that this map might be concurrently cloned from /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 484
	 * bpf_sk_storage_clone. Wait for any existing bpf_sk_storage_clone /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 485
	 * RCU read section to finish before proceeding. New RCU /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 486
	 * read sections should be prevented via bpf_map_inc_not_zero. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 487
	/* bpf prog and the userspace can no longer access this map /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 491
	 * now.  No new selem (of this map) can be added /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 492
	 * to the owner->storage or to the map bucket's list. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 493
	 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 494
	 * The elem of this map can be cleaned up here /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 495
	 * or when the storage is freed e.g. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 496
	 * by bpf_sk_storage_free() during __sk_destruct(). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 497
	/* While freeing the storage we may still need to access the map. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 521
	 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 522
	 * e.g. when bpf_sk_storage_free() has unlinked selem from the map /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 523
	 * which then made the above while((selem = ...)) loop /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 524
	 * exit immediately. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 525
	 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 526
	 * However, while freeing the storage one still needs to access the /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 527
	 * smap->elem_size to do the uncharging in /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 528
	 * bpf_selem_unlink_storage_nolock(). /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 529
	 * /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 530
	 * Hence, wait another rcu grace period for the storage to be freed. /Users/rubber/linux/kernel/bpf/bpf_local_storage.c: 531
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/bpf_iter.c: 1
/* bpf_seq_read, a customized and simpler version for bpf iterator. /Users/rubber/linux/kernel/bpf/bpf_iter.c: 82
 * no_llseek is assumed for this file. /Users/rubber/linux/kernel/bpf/bpf_iter.c: 83
 * The following are differences from seq_read(): /Users/rubber/linux/kernel/bpf/bpf_iter.c: 84
 *  . fixed buffer size (PAGE_SIZE) /Users/rubber/linux/kernel/bpf/bpf_iter.c: 85
 *  . assuming no_llseek /Users/rubber/linux/kernel/bpf/bpf_iter.c: 86
 *  . stop() may call bpf program, handling potential overflow there /Users/rubber/linux/kernel/bpf/bpf_iter.c: 87
		/* object is skipped, decrease seq_num, so next /Users/rubber/linux/kernel/bpf/bpf_iter.c: 135
		 * valid object can reuse the same seq_num. /Users/rubber/linux/kernel/bpf/bpf_iter.c: 136
/* The argument reg_info will be cached in bpf_iter_target_info. /Users/rubber/linux/kernel/bpf/bpf_iter.c: 281
 * The common practice is to declare target reg_info as /Users/rubber/linux/kernel/bpf/bpf_iter.c: 282
 * a const static variable and passed as an argument to /Users/rubber/linux/kernel/bpf/bpf_iter.c: 283
 * bpf_iter_reg_target(). /Users/rubber/linux/kernel/bpf/bpf_iter.c: 284
	/* bpf program can only return 0 or 1: /Users/rubber/linux/kernel/bpf/bpf_iter.c: 693
	 *  0 : okay /Users/rubber/linux/kernel/bpf/bpf_iter.c: 694
	 *  1 : retry the same object /Users/rubber/linux/kernel/bpf/bpf_iter.c: 695
	 * The bpf_iter_run_prog() return value /Users/rubber/linux/kernel/bpf/bpf_iter.c: 696
	 * will be seq_ops->show() return value. /Users/rubber/linux/kernel/bpf/bpf_iter.c: 697
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/kernel/bpf/core.c: 1
 * Linux Socket Filter - Kernel level socket filtering /Users/rubber/linux/kernel/bpf/core.c: 3
 * Based on the design of the Berkeley Packet Filter. The new /Users/rubber/linux/kernel/bpf/core.c: 5
 * internal format has been designed by PLUMgrid: /Users/rubber/linux/kernel/bpf/core.c: 6
 *	Copyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/core.c: 8
 * Authors: /Users/rubber/linux/kernel/bpf/core.c: 10
 *	Jay Schulist <jschlst@samba.org> /Users/rubber/linux/kernel/bpf/core.c: 12
 *	Alexei Starovoitov <ast@plumgrid.com> /Users/rubber/linux/kernel/bpf/core.c: 13
 *	Daniel Borkmann <dborkman@redhat.com> /Users/rubber/linux/kernel/bpf/core.c: 14
 * Andi Kleen - Fix a few bad bugs and races. /Users/rubber/linux/kernel/bpf/core.c: 16
 * Kris Katterjohn - Added many additional checks in bpf_check_classic() /Users/rubber/linux/kernel/bpf/core.c: 17
/* No hurry in this branch /Users/rubber/linux/kernel/bpf/core.c: 62
 * Exported for the bpf jit load helper. /Users/rubber/linux/kernel/bpf/core.c: 64
/* The jit engine is responsible to provide an array /Users/rubber/linux/kernel/bpf/core.c: 170
 * for insn_off to the jited_off mapping (insn_to_jit_off). /Users/rubber/linux/kernel/bpf/core.c: 171
 * The idx to this array is the insn_off.  Hence, the insn_off /Users/rubber/linux/kernel/bpf/core.c: 173
 * here is relative to the prog itself instead of the main prog. /Users/rubber/linux/kernel/bpf/core.c: 174
 * This array has one entry for each xlated bpf insn. /Users/rubber/linux/kernel/bpf/core.c: 175
 * jited_off is the byte off to the last byte of the jited insn. /Users/rubber/linux/kernel/bpf/core.c: 177
 * Hence, with /Users/rubber/linux/kernel/bpf/core.c: 179
 * insn_start: /Users/rubber/linux/kernel/bpf/core.c: 180
 *      The first bpf insn off of the prog.  The insn off /Users/rubber/linux/kernel/bpf/core.c: 181
 *      here is relative to the main prog. /Users/rubber/linux/kernel/bpf/core.c: 182
 *      e.g. if prog is a subprog, insn_start > 0 /Users/rubber/linux/kernel/bpf/core.c: 183
 * linfo_idx: /Users/rubber/linux/kernel/bpf/core.c: 184
 *      The prog's idx to prog->aux->linfo and jited_linfo /Users/rubber/linux/kernel/bpf/core.c: 185
 * jited_linfo[linfo_idx] = prog->bpf_func /Users/rubber/linux/kernel/bpf/core.c: 187
 * For i > linfo_idx, /Users/rubber/linux/kernel/bpf/core.c: 189
 * jited_linfo[i] = prog->bpf_func + /Users/rubber/linux/kernel/bpf/core.c: 191
 *	insn_to_jit_off[linfo[i].insn_off - insn_start - 1] /Users/rubber/linux/kernel/bpf/core.c: 192
		/* The verifier ensures that linfo[i].insn_off is /Users/rubber/linux/kernel/bpf/core.c: 216
		 * strictly increasing /Users/rubber/linux/kernel/bpf/core.c: 217
		/* We keep fp->aux from fp_old around in the new /Users/rubber/linux/kernel/bpf/core.c: 241
		 * reallocated structure. /Users/rubber/linux/kernel/bpf/core.c: 242
	/* We need to take out the map fd for the digest calculation /Users/rubber/linux/kernel/bpf/core.c: 286
	 * since they are unstable from user space side. /Users/rubber/linux/kernel/bpf/core.c: 287
		/* In the probing pass we still operate on the original, /Users/rubber/linux/kernel/bpf/core.c: 385
		 * unpatched image in order to check overflows before we /Users/rubber/linux/kernel/bpf/core.c: 386
		 * do any other adjustments. Therefore skip the patchlet. /Users/rubber/linux/kernel/bpf/core.c: 387
	/* Reject anything that would potentially let the insn->off /Users/rubber/linux/kernel/bpf/core.c: 458
	 * target overflow when we have excessive program expansions. /Users/rubber/linux/kernel/bpf/core.c: 459
	 * We need to probe here before we do any reallocation where /Users/rubber/linux/kernel/bpf/core.c: 460
	 * we afterwards may not fail anymore. /Users/rubber/linux/kernel/bpf/core.c: 461
	/* Several new instructions need to be inserted. Make room /Users/rubber/linux/kernel/bpf/core.c: 467
	 * for them. Likely, there's no need for a new allocation as /Users/rubber/linux/kernel/bpf/core.c: 468
	 * last page could have large enough tailroom. /Users/rubber/linux/kernel/bpf/core.c: 469
	/* Patching happens in 3 steps: /Users/rubber/linux/kernel/bpf/core.c: 478
	 * /Users/rubber/linux/kernel/bpf/core.c: 479
	 * 1) Move over tail of insnsi from next instruction onwards, /Users/rubber/linux/kernel/bpf/core.c: 480
	 *    so we can patch the single target insn with one or more /Users/rubber/linux/kernel/bpf/core.c: 481
	 *    new ones (patching is always from 1 to n insns, n > 0). /Users/rubber/linux/kernel/bpf/core.c: 482
	 * 2) Inject new instructions at the target location. /Users/rubber/linux/kernel/bpf/core.c: 483
	 * 3) Adjust branch offsets if necessary. /Users/rubber/linux/kernel/bpf/core.c: 484
	/* We are guaranteed to not fail at this point, otherwise /Users/rubber/linux/kernel/bpf/core.c: 492
	 * the ship has sailed to reverse to the original state. An /Users/rubber/linux/kernel/bpf/core.c: 493
	 * overflow cannot happen at this point. /Users/rubber/linux/kernel/bpf/core.c: 494
	/* Branch offsets can't overflow when program is shrinking, no need /Users/rubber/linux/kernel/bpf/core.c: 505
	 * to call bpf_adj_branches(..., true) here /Users/rubber/linux/kernel/bpf/core.c: 506
		     /* name has been null terminated. /Users/rubber/linux/kernel/bpf/core.c: 559
		      * We should need +1 for the '_' preceding /Users/rubber/linux/kernel/bpf/core.c: 560
		      * the name.  However, the null character /Users/rubber/linux/kernel/bpf/core.c: 561
		      * is double counted between the name and the /Users/rubber/linux/kernel/bpf/core.c: 562
		      * sizeof("bpf_prog_") above, so we omit /Users/rubber/linux/kernel/bpf/core.c: 563
		      * the +1 here. /Users/rubber/linux/kernel/bpf/core.c: 564
/* Can be overridden by an arch's JIT compiler if it has a custom, /Users/rubber/linux/kernel/bpf/core.c: 813
 * dedicated BPF backend memory area, or if neither of the two /Users/rubber/linux/kernel/bpf/core.c: 814
 * below apply. /Users/rubber/linux/kernel/bpf/core.c: 815
	/* Most of BPF filters are really small, but if some of them /Users/rubber/linux/kernel/bpf/core.c: 875
	 * fill a page, allow at least 128 extra bytes to insert a /Users/rubber/linux/kernel/bpf/core.c: 876
	 * random section of illegal instructions. /Users/rubber/linux/kernel/bpf/core.c: 877
/* This symbol is only overridden by archs that have different /Users/rubber/linux/kernel/bpf/core.c: 912
 * requirements than the usual eBPF JITs, f.e. when they only /Users/rubber/linux/kernel/bpf/core.c: 913
 * implement cBPF JIT, do not set images read-only, etc. /Users/rubber/linux/kernel/bpf/core.c: 914
		/* Place-holder address till the last pass has collected /Users/rubber/linux/kernel/bpf/core.c: 939
		 * all addresses for JITed subprograms in which case we /Users/rubber/linux/kernel/bpf/core.c: 940
		 * can pick them up from prog->aux. /Users/rubber/linux/kernel/bpf/core.c: 941
		/* Address of a BPF helper call. Since part of the core /Users/rubber/linux/kernel/bpf/core.c: 951
		 * kernel, it's always at a fixed location. __bpf_call_base /Users/rubber/linux/kernel/bpf/core.c: 952
		 * and the helper with imm relative to it are both in core /Users/rubber/linux/kernel/bpf/core.c: 953
		 * kernel. /Users/rubber/linux/kernel/bpf/core.c: 954
	/* Constraints on AX register: /Users/rubber/linux/kernel/bpf/core.c: 975
	 * /Users/rubber/linux/kernel/bpf/core.c: 976
	 * AX register is inaccessible from user space. It is mapped in /Users/rubber/linux/kernel/bpf/core.c: 977
	 * all JITs, and used here for constant blinding rewrites. It is /Users/rubber/linux/kernel/bpf/core.c: 978
	 * typically "stateless" meaning its contents are only valid within /Users/rubber/linux/kernel/bpf/core.c: 979
	 * the executed instruction, but not across several instructions. /Users/rubber/linux/kernel/bpf/core.c: 980
	 * There are a few exceptions however which are further detailed /Users/rubber/linux/kernel/bpf/core.c: 981
	 * below. /Users/rubber/linux/kernel/bpf/core.c: 982
	 * /Users/rubber/linux/kernel/bpf/core.c: 983
	 * Constant blinding is only used by JITs, not in the interpreter. /Users/rubber/linux/kernel/bpf/core.c: 984
	 * The interpreter uses AX in some occasions as a local temporary /Users/rubber/linux/kernel/bpf/core.c: 985
	 * register e.g. in DIV or MOD instructions. /Users/rubber/linux/kernel/bpf/core.c: 986
	 * /Users/rubber/linux/kernel/bpf/core.c: 987
	 * In restricted circumstances, the verifier can also use the AX /Users/rubber/linux/kernel/bpf/core.c: 988
	 * register for rewrites as long as they do not interfere with /Users/rubber/linux/kernel/bpf/core.c: 989
	 * the above cases! /Users/rubber/linux/kernel/bpf/core.c: 990
		/* aux->prog still points to the fp_other one, so /Users/rubber/linux/kernel/bpf/core.c: 1107
		 * when promoting the clone to the real program, /Users/rubber/linux/kernel/bpf/core.c: 1108
		 * this still needs to be adapted. /Users/rubber/linux/kernel/bpf/core.c: 1109
	/* aux was stolen by the other clone, so we cannot free /Users/rubber/linux/kernel/bpf/core.c: 1119
	 * it from this path! It will be freed eventually by the /Users/rubber/linux/kernel/bpf/core.c: 1120
	 * other program on release. /Users/rubber/linux/kernel/bpf/core.c: 1121
	 * /Users/rubber/linux/kernel/bpf/core.c: 1122
	 * At this point, we don't need a deferred release since /Users/rubber/linux/kernel/bpf/core.c: 1123
	 * clone is guaranteed to not be locked. /Users/rubber/linux/kernel/bpf/core.c: 1124
	/* We have to repoint aux->prog to self, as we don't /Users/rubber/linux/kernel/bpf/core.c: 1134
	 * know whether fp here is the clone or the original. /Users/rubber/linux/kernel/bpf/core.c: 1135
		/* We temporarily need to hold the original ld64 insn /Users/rubber/linux/kernel/bpf/core.c: 1160
		 * so that we can still access the first part in the /Users/rubber/linux/kernel/bpf/core.c: 1161
		 * second blinding run. /Users/rubber/linux/kernel/bpf/core.c: 1162
			/* Patching may have repointed aux->prog during /Users/rubber/linux/kernel/bpf/core.c: 1175
			 * realloc from the original one, so we need to /Users/rubber/linux/kernel/bpf/core.c: 1176
			 * fix it up here on error. /Users/rubber/linux/kernel/bpf/core.c: 1177
/* Base function for offset calculation. Needs to go into .text section, /Users/rubber/linux/kernel/bpf/core.c: 1197
 * therefore keeping it non-static as well; will also be used by JITs /Users/rubber/linux/kernel/bpf/core.c: 1198
 * anyway later on, so do not let the compiler omit it. This also needs /Users/rubber/linux/kernel/bpf/core.c: 1199
 * to go into kallsyms for correlation from e.g. bpftool, so naming /Users/rubber/linux/kernel/bpf/core.c: 1200
 * must not change. /Users/rubber/linux/kernel/bpf/core.c: 1201
 32 bit ALU operations. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1211
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1212
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1228
 64 bit ALU operations. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1241
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1242
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1256
 Call instruction. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1269
 Exit instruction. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1271
 32-bit Jump instructions. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1273
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1274
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1286
 Jump instructions. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1298
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1299
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1311
 Store instructions. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1324
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1325
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1332
 Load instructions. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1337
   Register based. */			\ /Users/rubber/linux/kernel/bpf/core.c: 1338
   Immediate based. */		\ /Users/rubber/linux/kernel/bpf/core.c: 1343
 *	___bpf_prog_run - run eBPF program on a given context /Users/rubber/linux/kernel/bpf/core.c: 1375
 *	@regs: is the array of MAX_BPF_EXT_REG eBPF pseudo-registers /Users/rubber/linux/kernel/bpf/core.c: 1376
 *	@insn: is the array of eBPF instructions /Users/rubber/linux/kernel/bpf/core.c: 1377
 * Decode and execute eBPF instructions. /Users/rubber/linux/kernel/bpf/core.c: 1379
 * Return: whatever value is in %BPF_R0 at program exit /Users/rubber/linux/kernel/bpf/core.c: 1381
	/* Explicitly mask the register-based shift amounts with 63 or 31 /Users/rubber/linux/kernel/bpf/core.c: 1410
	 * to avoid undefined behavior. Normally this won't affect the /Users/rubber/linux/kernel/bpf/core.c: 1411
	 * generated code, for example, in case of native 64 bit archs such /Users/rubber/linux/kernel/bpf/core.c: 1412
	 * as x86-64 or arm64, the compiler is optimizing the AND away for /Users/rubber/linux/kernel/bpf/core.c: 1413
	 * the interpreter. In case of JITs, each of the JIT backends compiles /Users/rubber/linux/kernel/bpf/core.c: 1414
	 * the BPF shift operations to machine instructions which produce /Users/rubber/linux/kernel/bpf/core.c: 1415
	 * implementation-defined results in such a case; the resulting /Users/rubber/linux/kernel/bpf/core.c: 1416
	 * contents of the register may be arbitrary, but program behaviour /Users/rubber/linux/kernel/bpf/core.c: 1417
	 * as a whole remains defined. In other words, in case of JIT backends, /Users/rubber/linux/kernel/bpf/core.c: 1418
	 * the AND must /not/ be added to the emitted LSH/RSH/ARSH translation. /Users/rubber/linux/kernel/bpf/core.c: 1419
		/* Function call scratches BPF_R1-BPF_R5 registers, /Users/rubber/linux/kernel/bpf/core.c: 1554
		 * preserves BPF_R6-BPF_R9, and stores return value /Users/rubber/linux/kernel/bpf/core.c: 1555
		 * into BPF_R0. /Users/rubber/linux/kernel/bpf/core.c: 1556
		/* ARG1 at this point is guaranteed to point to CTX from /Users/rubber/linux/kernel/bpf/core.c: 1586
		 * the verifier side due to the fact that the tail call is /Users/rubber/linux/kernel/bpf/core.c: 1587
		 * handled like a helper, that is, bpf_tail_call_proto, /Users/rubber/linux/kernel/bpf/core.c: 1588
		 * where arg1_type is ARG_PTR_TO_CTX. /Users/rubber/linux/kernel/bpf/core.c: 1589
		/* Speculation barrier for mitigating Speculative Store Bypass. /Users/rubber/linux/kernel/bpf/core.c: 1641
		 * In case of arm64, we rely on the firmware mitigation as /Users/rubber/linux/kernel/bpf/core.c: 1642
		 * controlled via the ssbd kernel parameter. Whenever the /Users/rubber/linux/kernel/bpf/core.c: 1643
		 * mitigation is enabled, it works for all of the kernel code /Users/rubber/linux/kernel/bpf/core.c: 1644
		 * with no need to provide any additional instructions here. /Users/rubber/linux/kernel/bpf/core.c: 1645
		 * In case of x86, we use 'lfence' insn for mitigation. We /Users/rubber/linux/kernel/bpf/core.c: 1646
		 * reuse preexisting logic from Spectre v1 mitigation that /Users/rubber/linux/kernel/bpf/core.c: 1647
		 * happens to produce the required code on x86 for v4 as well. /Users/rubber/linux/kernel/bpf/core.c: 1648
		/* If we ever reach this, we have a bug somewhere. Die hard here /Users/rubber/linux/kernel/bpf/core.c: 1736
		 * instead of just returning 0; we could be somewhere in a subprog, /Users/rubber/linux/kernel/bpf/core.c: 1737
		 * so execution could continue otherwise which we do /not/ want. /Users/rubber/linux/kernel/bpf/core.c: 1738
		 * /Users/rubber/linux/kernel/bpf/core.c: 1739
		 * Note, verifier whitelists all opcodes in bpf_opcode_in_insntable(). /Users/rubber/linux/kernel/bpf/core.c: 1740
	/* If this handler ever gets executed, then BPF_JIT_ALWAYS_ON /Users/rubber/linux/kernel/bpf/core.c: 1823
	 * is not working properly, so warn about it! /Users/rubber/linux/kernel/bpf/core.c: 1824
		/* There's no owner yet where we could check for /Users/rubber/linux/kernel/bpf/core.c: 1842
		 * compatibility. /Users/rubber/linux/kernel/bpf/core.c: 1843
 *	bpf_prog_select_runtime - select exec runtime for BPF program /Users/rubber/linux/kernel/bpf/core.c: 1893
 *	@fp: bpf_prog populated with internal BPF program /Users/rubber/linux/kernel/bpf/core.c: 1894
 *	@err: pointer to error variable /Users/rubber/linux/kernel/bpf/core.c: 1895
 * Try to JIT eBPF program, if JIT is not available, use interpreter. /Users/rubber/linux/kernel/bpf/core.c: 1897
 * The BPF program will be executed via bpf_prog_run() function. /Users/rubber/linux/kernel/bpf/core.c: 1898
 * Return: the &fp argument along with &err set to 0 for success or /Users/rubber/linux/kernel/bpf/core.c: 1900
 * a negative errno code on failure /Users/rubber/linux/kernel/bpf/core.c: 1901
	/* In case of BPF to BPF calls, verifier did all the prep /Users/rubber/linux/kernel/bpf/core.c: 1905
	 * work with regards to JITing, etc. /Users/rubber/linux/kernel/bpf/core.c: 1906
	/* eBPF JITs can rewrite the program in case constant /Users/rubber/linux/kernel/bpf/core.c: 1919
	 * blinding is active. However, in case of error during /Users/rubber/linux/kernel/bpf/core.c: 1920
	 * blinding, bpf_int_jit_compile() must always return a /Users/rubber/linux/kernel/bpf/core.c: 1921
	 * valid program, which in this case would simply not /Users/rubber/linux/kernel/bpf/core.c: 1922
	 * be JITed, but falls back to the interpreter. /Users/rubber/linux/kernel/bpf/core.c: 1923
	/* The tail call compatibility check can only be done at /Users/rubber/linux/kernel/bpf/core.c: 1945
	 * this late stage as we need to determine, if we deal /Users/rubber/linux/kernel/bpf/core.c: 1946
	 * with JITed or non JITed program concatenations and not /Users/rubber/linux/kernel/bpf/core.c: 1947
	 * all eBPF JITs might immediately support all features. /Users/rubber/linux/kernel/bpf/core.c: 1948
/* to avoid allocating empty bpf_prog_array for cgroups that /Users/rubber/linux/kernel/bpf/core.c: 1970
 * don't have bpf program attached use one global 'empty_prog_array' /Users/rubber/linux/kernel/bpf/core.c: 1971
 * It will not be modified the caller of bpf_prog_array_alloc() /Users/rubber/linux/kernel/bpf/core.c: 1972
 * (since caller requested prog_cnt == 0) /Users/rubber/linux/kernel/bpf/core.c: 1973
 * that pointer should be 'freed' by bpf_prog_array_free() /Users/rubber/linux/kernel/bpf/core.c: 1974
	/* users of this function are doing: /Users/rubber/linux/kernel/bpf/core.c: 2049
	 * cnt = bpf_prog_array_length(); /Users/rubber/linux/kernel/bpf/core.c: 2050
	 * if (cnt > 0) /Users/rubber/linux/kernel/bpf/core.c: 2051
	 *     bpf_prog_array_copy_to_user(..., cnt); /Users/rubber/linux/kernel/bpf/core.c: 2052
	 * so below kcalloc doesn't need extra cnt > 0 check. /Users/rubber/linux/kernel/bpf/core.c: 2053
 * bpf_prog_array_delete_safe_at() - Replaces the program at the given /Users/rubber/linux/kernel/bpf/core.c: 2081
 *                                   index into the program array with /Users/rubber/linux/kernel/bpf/core.c: 2082
 *                                   a dummy no-op program. /Users/rubber/linux/kernel/bpf/core.c: 2083
 * @array: a bpf_prog_array /Users/rubber/linux/kernel/bpf/core.c: 2084
 * @index: the index of the program to replace /Users/rubber/linux/kernel/bpf/core.c: 2085
 * Skips over dummy programs, by not counting them, when calculating /Users/rubber/linux/kernel/bpf/core.c: 2087
 * the position of the program to replace. /Users/rubber/linux/kernel/bpf/core.c: 2088
 * Return: /Users/rubber/linux/kernel/bpf/core.c: 2090
 * * 0		- Success /Users/rubber/linux/kernel/bpf/core.c: 2091
 * * -EINVAL	- Invalid index value. Must be a non-negative integer. /Users/rubber/linux/kernel/bpf/core.c: 2092
 * * -ENOENT	- Index out of range /Users/rubber/linux/kernel/bpf/core.c: 2093
 * bpf_prog_array_update_at() - Updates the program at the given index /Users/rubber/linux/kernel/bpf/core.c: 2101
 *                              into the program array. /Users/rubber/linux/kernel/bpf/core.c: 2102
 * @array: a bpf_prog_array /Users/rubber/linux/kernel/bpf/core.c: 2103
 * @index: the index of the program to update /Users/rubber/linux/kernel/bpf/core.c: 2104
 * @prog: the program to insert into the array /Users/rubber/linux/kernel/bpf/core.c: 2105
 * Skips over dummy programs, by not counting them, when calculating /Users/rubber/linux/kernel/bpf/core.c: 2107
 * the position of the program to update. /Users/rubber/linux/kernel/bpf/core.c: 2108
 * Return: /Users/rubber/linux/kernel/bpf/core.c: 2110
 * * 0		- Success /Users/rubber/linux/kernel/bpf/core.c: 2111
 * * -EINVAL	- Invalid index value. Must be a non-negative integer. /Users/rubber/linux/kernel/bpf/core.c: 2112
 * * -ENOENT	- Index out of range /Users/rubber/linux/kernel/bpf/core.c: 2113
	/* Figure out how many existing progs we need to carry over to /Users/rubber/linux/kernel/bpf/core.c: 2146
	 * the new array. /Users/rubber/linux/kernel/bpf/core.c: 2147
		/* We can just unlink the subprog poke descriptor table as /Users/rubber/linux/kernel/bpf/core.c: 2288
		 * it was originally linked to the main program and is also /Users/rubber/linux/kernel/bpf/core.c: 2289
		 * released along with it. /Users/rubber/linux/kernel/bpf/core.c: 2290
	/* Should someone ever have the rather unwise idea to use some /Users/rubber/linux/kernel/bpf/core.c: 2325
	 * of the registers passed into this function, then note that /Users/rubber/linux/kernel/bpf/core.c: 2326
	 * this function is called from native eBPF and classic-to-eBPF /Users/rubber/linux/kernel/bpf/core.c: 2327
	 * transformations. Register assignments from both sides are /Users/rubber/linux/kernel/bpf/core.c: 2328
	 * different, f.e. classic always sets fn(ctx, A, X) here. /Users/rubber/linux/kernel/bpf/core.c: 2329
/* Stub for JITs that only support cBPF. eBPF programs are interpreted. /Users/rubber/linux/kernel/bpf/core.c: 2402
 * It is encouraged to implement bpf_int_jit_compile() instead, so that /Users/rubber/linux/kernel/bpf/core.c: 2403
 * eBPF and implicitly also cBPF can get JITed! /Users/rubber/linux/kernel/bpf/core.c: 2404
/* Stub for JITs that support eBPF. All cBPF code gets transformed into /Users/rubber/linux/kernel/bpf/core.c: 2411
 * eBPF by the kernel and is later compiled by bpf_int_jit_compile(). /Users/rubber/linux/kernel/bpf/core.c: 2412
/* Return TRUE if the JIT backend wants verifier to enable sub-register usage /Users/rubber/linux/kernel/bpf/core.c: 2423
 * analysis code and wants explicit zero extension inserted by verifier. /Users/rubber/linux/kernel/bpf/core.c: 2424
 * Otherwise, return FALSE. /Users/rubber/linux/kernel/bpf/core.c: 2425
 * The verifier inserts an explicit zero extension after BPF_CMPXCHGs even if /Users/rubber/linux/kernel/bpf/core.c: 2427
 * you don't override this. JITs that don't want these extra insns can detect /Users/rubber/linux/kernel/bpf/core.c: 2428
 * them using insn_is_zext. /Users/rubber/linux/kernel/bpf/core.c: 2429
/* To execute LD_ABS/LD_IND instructions __bpf_prog_run() may call /Users/rubber/linux/kernel/bpf/core.c: 2441
 * skb_copy_bits(), so provide a weak definition of it for NET-less config. /Users/rubber/linux/kernel/bpf/core.c: 2442
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/inode.c: 1
 * Minimal file system backend for holding eBPF maps and programs, /Users/rubber/linux/kernel/bpf/inode.c: 3
 * used by bpf(2) object pinning. /Users/rubber/linux/kernel/bpf/inode.c: 4
 * Authors: /Users/rubber/linux/kernel/bpf/inode.c: 6
 *	Daniel Borkmann <daniel@iogearbox.net> /Users/rubber/linux/kernel/bpf/inode.c: 8
/* bpffs_map_fops should only implement the basic /Users/rubber/linux/kernel/bpf/inode.c: 306
 * read operation for a BPF map.  The purpose is to /Users/rubber/linux/kernel/bpf/inode.c: 307
 * provide a simple user intuitive way to do /Users/rubber/linux/kernel/bpf/inode.c: 308
 * "cat bpffs/pathto/a-pinned-map". /Users/rubber/linux/kernel/bpf/inode.c: 309
 * Other operations (e.g. write, lookup...) should be realized by /Users/rubber/linux/kernel/bpf/inode.c: 311
 * the userspace tools (e.g. bpftool) through the /Users/rubber/linux/kernel/bpf/inode.c: 312
 * BPF_OBJ_GET_INFO_BY_FD and the map's lookup/update /Users/rubber/linux/kernel/bpf/inode.c: 313
 * interface. /Users/rubber/linux/kernel/bpf/inode.c: 314
	/* Dots in names (e.g. "/sys/fs/bpf/foo.bar") are reserved for future /Users/rubber/linux/kernel/bpf/inode.c: 375
	 * extensions. That allows popoulate_bpffs() create special files. /Users/rubber/linux/kernel/bpf/inode.c: 376
 * Display the mount options in /proc/mounts. /Users/rubber/linux/kernel/bpf/inode.c: 602
		/* We might like to report bad mount options here, but /Users/rubber/linux/kernel/bpf/inode.c: 652
		 * traditionally we've ignored all mount options, so we'd /Users/rubber/linux/kernel/bpf/inode.c: 653
		 * better continue to ignore non-existing options for bpf. /Users/rubber/linux/kernel/bpf/inode.c: 654
	/* If bpf_preload.ko wasn't loaded earlier then load it now. /Users/rubber/linux/kernel/bpf/inode.c: 672
	 * When bpf_preload is built into vmlinux the module's __init /Users/rubber/linux/kernel/bpf/inode.c: 673
	 * function will populate it. /Users/rubber/linux/kernel/bpf/inode.c: 674
	/* And grab the reference, so the module doesn't disappear while the /Users/rubber/linux/kernel/bpf/inode.c: 681
	 * kernel is interacting with the kernel module and its UMD. /Users/rubber/linux/kernel/bpf/inode.c: 682
	/* grab the mutex to make sure the kernel interactions with bpf_preload /Users/rubber/linux/kernel/bpf/inode.c: 706
	 * UMD are serialized /Users/rubber/linux/kernel/bpf/inode.c: 707
			/* do not unlink successfully pinned links even /Users/rubber/linux/kernel/bpf/inode.c: 732
			 * if later link fails to pin /Users/rubber/linux/kernel/bpf/inode.c: 733
 * Set up the filesystem mount context. /Users/rubber/linux/kernel/bpf/inode.c: 790
SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/local_storage.c: 1
	/* per_cpu areas are zero-filled and bpf programs can only /Users/rubber/linux/kernel/bpf/local_storage.c: 197
	 * access 'value_size' of them, so copying rounded areas /Users/rubber/linux/kernel/bpf/local_storage.c: 198
	 * will not leak any kernel data /Users/rubber/linux/kernel/bpf/local_storage.c: 199
	/* the user space will provide round_up(value_size, 8) bytes that /Users/rubber/linux/kernel/bpf/local_storage.c: 229
	 * will be copied into per-cpu area. bpf programs can only access /Users/rubber/linux/kernel/bpf/local_storage.c: 230
	 * value_size of it. During lookup the same extra bytes will be /Users/rubber/linux/kernel/bpf/local_storage.c: 231
	 * returned or zeros which were zero-filled by percpu_alloc, /Users/rubber/linux/kernel/bpf/local_storage.c: 232
	 * so no kernel data leaks possible /Users/rubber/linux/kernel/bpf/local_storage.c: 233
	/* percpu is bound by PCPU_MIN_UNIT_SIZE, non-percu /Users/rubber/linux/kernel/bpf/local_storage.c: 291
	 * is the same as other local storages. /Users/rubber/linux/kernel/bpf/local_storage.c: 292
		/* Key is expected to be of struct bpf_cgroup_storage_key type, /Users/rubber/linux/kernel/bpf/local_storage.c: 366
		 * which is: /Users/rubber/linux/kernel/bpf/local_storage.c: 367
		 * struct bpf_cgroup_storage_key { /Users/rubber/linux/kernel/bpf/local_storage.c: 368
		 *	__u64	cgroup_inode_id; /Users/rubber/linux/kernel/bpf/local_storage.c: 369
		 *	__u32	attach_type; /Users/rubber/linux/kernel/bpf/local_storage.c: 370
		 * }; /Users/rubber/linux/kernel/bpf/local_storage.c: 371
		/* /Users/rubber/linux/kernel/bpf/local_storage.c: 374
		 * Key_type must be a structure with two fields. /Users/rubber/linux/kernel/bpf/local_storage.c: 375
		/* /Users/rubber/linux/kernel/bpf/local_storage.c: 381
		 * The first field must be a 64 bit integer at 0 offset. /Users/rubber/linux/kernel/bpf/local_storage.c: 382
		/* /Users/rubber/linux/kernel/bpf/local_storage.c: 389
		 * The second field must be a 32 bit integer at 64 bit offset. /Users/rubber/linux/kernel/bpf/local_storage.c: 390
		/* /Users/rubber/linux/kernel/bpf/local_storage.c: 400
		 * Key is expected to be u64, which stores the cgroup_inode_id /Users/rubber/linux/kernel/bpf/local_storage.c: 401
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/lpm_trie.c: 1
 * Longest prefix match list implementation /Users/rubber/linux/kernel/bpf/lpm_trie.c: 3
 * Copyright (c) 2016,2017 Daniel Mack /Users/rubber/linux/kernel/bpf/lpm_trie.c: 5
 * Copyright (c) 2016 David Herrmann /Users/rubber/linux/kernel/bpf/lpm_trie.c: 6
/* This trie implements a longest prefix match algorithm that can be used to /Users/rubber/linux/kernel/bpf/lpm_trie.c: 40
 * match IP addresses to a stored set of ranges. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 41
 * Data stored in @data of struct bpf_lpm_key and struct lpm_trie_node is /Users/rubber/linux/kernel/bpf/lpm_trie.c: 43
 * interpreted as big endian, so data[0] stores the most significant byte. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 44
 * Match ranges are internally stored in instances of struct lpm_trie_node /Users/rubber/linux/kernel/bpf/lpm_trie.c: 46
 * which each contain their prefix length as well as two pointers that may /Users/rubber/linux/kernel/bpf/lpm_trie.c: 47
 * lead to more nodes containing more specific matches. Each node also stores /Users/rubber/linux/kernel/bpf/lpm_trie.c: 48
 * a value that is defined by and returned to userspace via the update_elem /Users/rubber/linux/kernel/bpf/lpm_trie.c: 49
 * and lookup functions. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 50
 * For instance, let's start with a trie that was created with a prefix length /Users/rubber/linux/kernel/bpf/lpm_trie.c: 52
 * of 32, so it can be used for IPv4 addresses, and one single element that /Users/rubber/linux/kernel/bpf/lpm_trie.c: 53
 * matches 192.168.0.0/16. The data array would hence contain /Users/rubber/linux/kernel/bpf/lpm_trie.c: 54
 * [0xc0, 0xa8, 0x00, 0x00] in big-endian notation. This documentation will /Users/rubber/linux/kernel/bpf/lpm_trie.c: 55
 * stick to IP-address notation for readability though. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 56
 * As the trie is empty initially, the new node (1) will be places as root /Users/rubber/linux/kernel/bpf/lpm_trie.c: 58
 * node, denoted as (R) in the example below. As there are no other node, both /Users/rubber/linux/kernel/bpf/lpm_trie.c: 59
 * child pointers are %NULL. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 60
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 62
 *              |       (1)  (R) | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 63
 *              | 192.168.0.0/16 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 64
 *              |    value: 1    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 65
 *              |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 66
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 67
 * Next, let's add a new node (2) matching 192.168.0.0/24. As there is already /Users/rubber/linux/kernel/bpf/lpm_trie.c: 69
 * a node with the same data and a smaller prefix (ie, a less specific one), /Users/rubber/linux/kernel/bpf/lpm_trie.c: 70
 * node (2) will become a child of (1). In child index depends on the next bit /Users/rubber/linux/kernel/bpf/lpm_trie.c: 71
 * that is outside of what (1) matches, and that bit is 0, so (2) will be /Users/rubber/linux/kernel/bpf/lpm_trie.c: 72
 * child[0] of (1): /Users/rubber/linux/kernel/bpf/lpm_trie.c: 73
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 75
 *              |       (1)  (R) | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 76
 *              | 192.168.0.0/16 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 77
 *              |    value: 1    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 78
 *              |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 79
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 80
 *                   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 81
 *    +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 82
 *    |       (2)      | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 83
 *    | 192.168.0.0/24 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 84
 *    |    value: 2    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 85
 *    |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 86
 *    +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 87
 * The child[1] slot of (1) could be filled with another node which has bit #17 /Users/rubber/linux/kernel/bpf/lpm_trie.c: 89
 * (the next bit after the ones that (1) matches on) set to 1. For instance, /Users/rubber/linux/kernel/bpf/lpm_trie.c: 90
 * 192.168.128.0/24: /Users/rubber/linux/kernel/bpf/lpm_trie.c: 91
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 93
 *              |       (1)  (R) | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 94
 *              | 192.168.0.0/16 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 95
 *              |    value: 1    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 96
 *              |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 97
 *              +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 98
 *                   |      | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 99
 *    +----------------+  +------------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 100
 *    |       (2)      |  |        (3)       | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 101
 *    | 192.168.0.0/24 |  | 192.168.128.0/24 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 102
 *    |    value: 2    |  |     value: 3     | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 103
 *    |   [0]    [1]   |  |    [0]    [1]    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 104
 *    +----------------+  +------------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 105
 * Let's add another node (4) to the game for 192.168.1.0/24. In order to place /Users/rubber/linux/kernel/bpf/lpm_trie.c: 107
 * it, node (1) is looked at first, and because (4) of the semantics laid out /Users/rubber/linux/kernel/bpf/lpm_trie.c: 108
 * above (bit #17 is 0), it would normally be attached to (1) as child[0]. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 109
 * However, that slot is already allocated, so a new node is needed in between. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 110
 * That node does not have a value attached to it and it will never be /Users/rubber/linux/kernel/bpf/lpm_trie.c: 111
 * returned to users as result of a lookup. It is only there to differentiate /Users/rubber/linux/kernel/bpf/lpm_trie.c: 112
 * the traversal further. It will get a prefix as wide as necessary to /Users/rubber/linux/kernel/bpf/lpm_trie.c: 113
 * distinguish its two children: /Users/rubber/linux/kernel/bpf/lpm_trie.c: 114
 *                      +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 116
 *                      |       (1)  (R) | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 117
 *                      | 192.168.0.0/16 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 118
 *                      |    value: 1    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 119
 *                      |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 120
 *                      +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 121
 *                           |      | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 122
 *            +----------------+  +------------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 123
 *            |       (4)  (I) |  |        (3)       | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 124
 *            | 192.168.0.0/23 |  | 192.168.128.0/24 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 125
 *            |    value: ---  |  |     value: 3     | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 126
 *            |   [0]    [1]   |  |    [0]    [1]    | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 127
 *            +----------------+  +------------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 128
 *                 |      | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 129
 *  +----------------+  +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 130
 *  |       (2)      |  |       (5)      | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 131
 *  | 192.168.0.0/24 |  | 192.168.1.0/24 | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 132
 *  |    value: 2    |  |     value: 5   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 133
 *  |   [0]    [1]   |  |   [0]    [1]   | /Users/rubber/linux/kernel/bpf/lpm_trie.c: 134
 *  +----------------+  +----------------+ /Users/rubber/linux/kernel/bpf/lpm_trie.c: 135
 * 192.168.1.1/32 would be a child of (5) etc. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 137
 * An intermediate node will be turned into a 'real' node on demand. In the /Users/rubber/linux/kernel/bpf/lpm_trie.c: 139
 * example above, (4) would be re-used if 192.168.0.0/23 is added to the trie. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 140
 * A fully populated trie would have a height of 32 nodes, as the trie was /Users/rubber/linux/kernel/bpf/lpm_trie.c: 142
 * created with a prefix length of 32. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 143
 * The lookup starts at the root node. If the current node matches and if there /Users/rubber/linux/kernel/bpf/lpm_trie.c: 145
 * is a child that can be used to become more specific, the trie is traversed /Users/rubber/linux/kernel/bpf/lpm_trie.c: 146
 * downwards. The last node in the traversal that is a non-intermediate one is /Users/rubber/linux/kernel/bpf/lpm_trie.c: 147
 * returned. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 148
 * longest_prefix_match() - determine the longest prefix /Users/rubber/linux/kernel/bpf/lpm_trie.c: 157
 * @trie:	The trie to get internal sizes from /Users/rubber/linux/kernel/bpf/lpm_trie.c: 158
 * @node:	The node to operate on /Users/rubber/linux/kernel/bpf/lpm_trie.c: 159
 * @key:	The key to compare to @node /Users/rubber/linux/kernel/bpf/lpm_trie.c: 160
 * Determine the longest prefix of @node that matches the bits in @key. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 162
	/* data_size >= 16 has very small probability. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 176
	 * We do not use a loop for optimal code generation. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 177
		/* Determine the longest prefix of @node that matches @key. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 240
		 * If it's the maximum possible prefix for this trie, we have /Users/rubber/linux/kernel/bpf/lpm_trie.c: 241
		 * an exact match and can return it directly. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 242
		/* If the number of bits that match is smaller than the prefix /Users/rubber/linux/kernel/bpf/lpm_trie.c: 250
		 * length of @node, bail out and return the node we have seen /Users/rubber/linux/kernel/bpf/lpm_trie.c: 251
		 * last in the traversal (ie, the parent). /Users/rubber/linux/kernel/bpf/lpm_trie.c: 252
		/* Consider this node as return candidate unless it is an /Users/rubber/linux/kernel/bpf/lpm_trie.c: 257
		 * artificially added intermediate one. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 258
		/* If the node match is fully satisfied, let's see if we can /Users/rubber/linux/kernel/bpf/lpm_trie.c: 263
		 * become more specific. Determine the next bit in the key and /Users/rubber/linux/kernel/bpf/lpm_trie.c: 264
		 * traverse down. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 265
	/* Now find a slot to attach the new node. To do that, walk the tree /Users/rubber/linux/kernel/bpf/lpm_trie.c: 342
	 * from the root and match as many bits as possible for each node until /Users/rubber/linux/kernel/bpf/lpm_trie.c: 343
	 * we either find an empty slot or a slot that needs to be replaced by /Users/rubber/linux/kernel/bpf/lpm_trie.c: 344
	 * an intermediate node. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 345
	/* If the slot is empty (a free child pointer or an empty root), /Users/rubber/linux/kernel/bpf/lpm_trie.c: 362
	 * simply assign the @new_node to that slot and be done. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 363
	/* If the slot we picked already exists, replace it with @new_node /Users/rubber/linux/kernel/bpf/lpm_trie.c: 370
	 * which already has the correct data array set. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 371
	/* If the new node matches the prefix completely, it must be inserted /Users/rubber/linux/kernel/bpf/lpm_trie.c: 386
	 * as an ancestor. Simply insert it between @node and *@slot. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 387
	/* Walk the tree looking for an exact key/length match and keeping /Users/rubber/linux/kernel/bpf/lpm_trie.c: 449
	 * track of the path we traverse.  We will need to know the node /Users/rubber/linux/kernel/bpf/lpm_trie.c: 450
	 * we wish to delete, and the slot that points to the node we want /Users/rubber/linux/kernel/bpf/lpm_trie.c: 451
	 * to delete.  We may also need to know the nodes parent and the /Users/rubber/linux/kernel/bpf/lpm_trie.c: 452
	 * slot that contains it. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 453
	/* If the node we are removing has two children, simply mark it /Users/rubber/linux/kernel/bpf/lpm_trie.c: 481
	 * as intermediate and we are done. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 482
	/* If the parent of the node we are about to delete is an intermediate /Users/rubber/linux/kernel/bpf/lpm_trie.c: 490
	 * node, and the deleted node doesn't have any children, we can delete /Users/rubber/linux/kernel/bpf/lpm_trie.c: 491
	 * the intermediate parent as well and promote its other child /Users/rubber/linux/kernel/bpf/lpm_trie.c: 492
	 * up the tree.  Doing this maintains the invariant that all /Users/rubber/linux/kernel/bpf/lpm_trie.c: 493
	 * intermediate nodes have exactly 2 children and that there are no /Users/rubber/linux/kernel/bpf/lpm_trie.c: 494
	 * unnecessary intermediate nodes in the tree. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 495
	/* The node we are removing has either zero or one child. If there /Users/rubber/linux/kernel/bpf/lpm_trie.c: 510
	 * is a child, move it into the removed node's slot then delete /Users/rubber/linux/kernel/bpf/lpm_trie.c: 511
	 * the node.  Otherwise just clear the slot and delete the node. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 512
	/* Always start at the root and walk down to a node that has no /Users/rubber/linux/kernel/bpf/lpm_trie.c: 581
	 * children. Then free that node, nullify its reference in the parent /Users/rubber/linux/kernel/bpf/lpm_trie.c: 582
	 * and start over. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 583
	/* The get_next_key follows postorder. For the 4 node example in /Users/rubber/linux/kernel/bpf/lpm_trie.c: 624
	 * the top of this file, the trie_get_next_key() returns the following /Users/rubber/linux/kernel/bpf/lpm_trie.c: 625
	 * one after another: /Users/rubber/linux/kernel/bpf/lpm_trie.c: 626
	 *   192.168.0.0/24 /Users/rubber/linux/kernel/bpf/lpm_trie.c: 627
	 *   192.168.1.0/24 /Users/rubber/linux/kernel/bpf/lpm_trie.c: 628
	 *   192.168.128.0/24 /Users/rubber/linux/kernel/bpf/lpm_trie.c: 629
	 *   192.168.0.0/16 /Users/rubber/linux/kernel/bpf/lpm_trie.c: 630
	 * /Users/rubber/linux/kernel/bpf/lpm_trie.c: 631
	 * The idea is to return more specific keys before less specific ones. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 632
	/* The node with the exactly-matching key has been found, /Users/rubber/linux/kernel/bpf/lpm_trie.c: 665
	 * find the first node in postorder after the matched node. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 666
	/* Find the leftmost non-intermediate node, all intermediate nodes /Users/rubber/linux/kernel/bpf/lpm_trie.c: 690
	 * have exact two children, so this function will never return NULL. /Users/rubber/linux/kernel/bpf/lpm_trie.c: 691
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/bloom_filter.c: 1
	/* If the size of the values in the bloom filter is u32 aligned, /Users/rubber/linux/kernel/bpf/bloom_filter.c: 18
	 * then it is more performant to use jhash2 as the underlying hash /Users/rubber/linux/kernel/bpf/bloom_filter.c: 19
	 * function, else we use jhash. This tracks the number of u32s /Users/rubber/linux/kernel/bpf/bloom_filter.c: 20
	 * in an u32-aligned value size. If the value size is not u32 aligned, /Users/rubber/linux/kernel/bpf/bloom_filter.c: 21
	 * this will be 0. /Users/rubber/linux/kernel/bpf/bloom_filter.c: 22
	    /* The lower 4 bits of map_extra (0xF) specify the number /Users/rubber/linux/kernel/bpf/bloom_filter.c: 98
	     * of hash functions /Users/rubber/linux/kernel/bpf/bloom_filter.c: 99
	/* For the bloom filter, the optimal bit array size that minimizes the /Users/rubber/linux/kernel/bpf/bloom_filter.c: 109
	 * false positive probability is n * k / ln(2) where n is the number of /Users/rubber/linux/kernel/bpf/bloom_filter.c: 110
	 * expected entries in the bloom filter and k is the number of hash /Users/rubber/linux/kernel/bpf/bloom_filter.c: 111
	 * functions. We use 7 / 5 to approximate 1 / ln(2). /Users/rubber/linux/kernel/bpf/bloom_filter.c: 112
	 * /Users/rubber/linux/kernel/bpf/bloom_filter.c: 113
	 * We round this up to the nearest power of two to enable more efficient /Users/rubber/linux/kernel/bpf/bloom_filter.c: 114
	 * hashing using bitmasks. The bitmask will be the bit array size - 1. /Users/rubber/linux/kernel/bpf/bloom_filter.c: 115
	 * /Users/rubber/linux/kernel/bpf/bloom_filter.c: 116
	 * If this overflows a u32, the bit array size will have 2^32 (4 /Users/rubber/linux/kernel/bpf/bloom_filter.c: 117
	 * GB) bits. /Users/rubber/linux/kernel/bpf/bloom_filter.c: 118
		/* The bit array size is 2^32 bits but to avoid overflowing the /Users/rubber/linux/kernel/bpf/bloom_filter.c: 123
		 * u32, we use U32_MAX, which will round up to the equivalent /Users/rubber/linux/kernel/bpf/bloom_filter.c: 124
		 * number of bytes /Users/rubber/linux/kernel/bpf/bloom_filter.c: 125
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/sysfs_btf.c: 1
 * Provide kernel BTF information for introspection and use by eBPF tools. /Users/rubber/linux/kernel/bpf/sysfs_btf.c: 3
/* Maximum size of ring buffer area is limited by 32-bit page offset within /Users/rubber/linux/kernel/bpf/ringbuf.c: 24
 * record header, counted in pages. Reserve 8 bits for extensibility, and take /Users/rubber/linux/kernel/bpf/ringbuf.c: 25
 * into account few extra pages for consumer/producer pages and /Users/rubber/linux/kernel/bpf/ringbuf.c: 26
 * non-mmap()'able parts. This gives 64GB limit, which seems plenty for single /Users/rubber/linux/kernel/bpf/ringbuf.c: 27
 * ring buffer. /Users/rubber/linux/kernel/bpf/ringbuf.c: 28
	/* Consumer and producer counters are put into separate pages to allow /Users/rubber/linux/kernel/bpf/ringbuf.c: 40
	 * mapping consumer page as r/w, but restrict producer page to r/o. /Users/rubber/linux/kernel/bpf/ringbuf.c: 41
	 * This protects producer position from being modified by user-space /Users/rubber/linux/kernel/bpf/ringbuf.c: 42
	 * application and ruining in-kernel position tracking. /Users/rubber/linux/kernel/bpf/ringbuf.c: 43
	/* Each data page is mapped twice to allow "virtual" /Users/rubber/linux/kernel/bpf/ringbuf.c: 73
	 * continuous read of samples wrapping around the end of ring /Users/rubber/linux/kernel/bpf/ringbuf.c: 74
	 * buffer area: /Users/rubber/linux/kernel/bpf/ringbuf.c: 75
	 * ------------------------------------------------------ /Users/rubber/linux/kernel/bpf/ringbuf.c: 76
	 * | meta pages |  real data pages  |  same data pages  | /Users/rubber/linux/kernel/bpf/ringbuf.c: 77
	 * ------------------------------------------------------ /Users/rubber/linux/kernel/bpf/ringbuf.c: 78
	 * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 | /Users/rubber/linux/kernel/bpf/ringbuf.c: 79
	 * ------------------------------------------------------ /Users/rubber/linux/kernel/bpf/ringbuf.c: 80
	 * |            | TA             DA | TA             DA | /Users/rubber/linux/kernel/bpf/ringbuf.c: 81
	 * ------------------------------------------------------ /Users/rubber/linux/kernel/bpf/ringbuf.c: 82
	 *                               ^^^^^^^ /Users/rubber/linux/kernel/bpf/ringbuf.c: 83
	 *                                  | /Users/rubber/linux/kernel/bpf/ringbuf.c: 84
	 * Here, no need to worry about special handling of wrapped-around /Users/rubber/linux/kernel/bpf/ringbuf.c: 85
	 * data due to double-mapped data pages. This works both in kernel and /Users/rubber/linux/kernel/bpf/ringbuf.c: 86
	 * when mmap()'ed in user-space, simplifying both kernel and /Users/rubber/linux/kernel/bpf/ringbuf.c: 87
	 * user-space implementations significantly. /Users/rubber/linux/kernel/bpf/ringbuf.c: 88
	/* copy pages pointer and nr_pages to local variable, as we are going /Users/rubber/linux/kernel/bpf/ringbuf.c: 183
	 * to unmap rb itself with vunmap() below /Users/rubber/linux/kernel/bpf/ringbuf.c: 184
/* Given pointer to ring buffer record metadata and struct bpf_ringbuf itself, /Users/rubber/linux/kernel/bpf/ringbuf.c: 281
 * calculate offset from record metadata to ring buffer in pages, rounded /Users/rubber/linux/kernel/bpf/ringbuf.c: 282
 * down. This page offset is stored as part of record metadata and allows to /Users/rubber/linux/kernel/bpf/ringbuf.c: 283
 * restore struct bpf_ringbuf * from record pointer. This page offset is /Users/rubber/linux/kernel/bpf/ringbuf.c: 284
 * stored at offset 4 of record metadata header. /Users/rubber/linux/kernel/bpf/ringbuf.c: 285
/* Given pointer to ring buffer record header, restore pointer to struct /Users/rubber/linux/kernel/bpf/ringbuf.c: 293
 * bpf_ringbuf itself by using page offset stored at offset 4 /Users/rubber/linux/kernel/bpf/ringbuf.c: 294
	/* check for out of ringbuf space by ensuring producer position /Users/rubber/linux/kernel/bpf/ringbuf.c: 330
	 * doesn't advance more than (ringbuf_size - 1) ahead /Users/rubber/linux/kernel/bpf/ringbuf.c: 331
	/* if consumer caught up and is waiting for our record, notify about /Users/rubber/linux/kernel/bpf/ringbuf.c: 386
	 * new data availability /Users/rubber/linux/kernel/bpf/ringbuf.c: 387
 discard */); /Users/rubber/linux/kernel/bpf/ringbuf.c: 400
 discard */); /Users/rubber/linux/kernel/bpf/ringbuf.c: 413
 discard */); /Users/rubber/linux/kernel/bpf/ringbuf.c: 439
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/trampoline.c: 1
	/* Keep image as writeable. The alternative is to keep flipping ro/rw /Users/rubber/linux/kernel/bpf/trampoline.c: 39
	 * everytime new program is attached or detached. /Users/rubber/linux/kernel/bpf/trampoline.c: 40
	/* The trampoline image that calls original function is using: /Users/rubber/linux/kernel/bpf/trampoline.c: 248
	 * rcu_read_lock_trace to protect sleepable bpf progs /Users/rubber/linux/kernel/bpf/trampoline.c: 249
	 * rcu_read_lock to protect normal bpf progs /Users/rubber/linux/kernel/bpf/trampoline.c: 250
	 * percpu_ref to protect trampoline itself /Users/rubber/linux/kernel/bpf/trampoline.c: 251
	 * rcu tasks to protect trampoline asm not covered by percpu_ref /Users/rubber/linux/kernel/bpf/trampoline.c: 252
	 * (which are few asm insns before __bpf_tramp_enter and /Users/rubber/linux/kernel/bpf/trampoline.c: 253
	 *  after __bpf_tramp_exit) /Users/rubber/linux/kernel/bpf/trampoline.c: 254
	 * /Users/rubber/linux/kernel/bpf/trampoline.c: 255
	 * The trampoline is unreachable before bpf_tramp_image_put(). /Users/rubber/linux/kernel/bpf/trampoline.c: 256
	 * /Users/rubber/linux/kernel/bpf/trampoline.c: 257
	 * First, patch the trampoline to avoid calling into fexit progs. /Users/rubber/linux/kernel/bpf/trampoline.c: 258
	 * The progs will be freed even if the original function is still /Users/rubber/linux/kernel/bpf/trampoline.c: 259
	 * executing or sleeping. /Users/rubber/linux/kernel/bpf/trampoline.c: 260
	 * In case of CONFIG_PREEMPT=y use call_rcu_tasks() to wait on /Users/rubber/linux/kernel/bpf/trampoline.c: 261
	 * first few asm instructions to execute and call into /Users/rubber/linux/kernel/bpf/trampoline.c: 262
	 * __bpf_tramp_enter->percpu_ref_get. /Users/rubber/linux/kernel/bpf/trampoline.c: 263
	 * Then use percpu_ref_kill to wait for the trampoline and the original /Users/rubber/linux/kernel/bpf/trampoline.c: 264
	 * function to finish. /Users/rubber/linux/kernel/bpf/trampoline.c: 265
	 * Then use call_rcu_tasks() to make sure few asm insns in /Users/rubber/linux/kernel/bpf/trampoline.c: 266
	 * the trampoline epilogue are done as well. /Users/rubber/linux/kernel/bpf/trampoline.c: 267
	 * /Users/rubber/linux/kernel/bpf/trampoline.c: 268
	 * In !PREEMPT case the task that got interrupted in the first asm /Users/rubber/linux/kernel/bpf/trampoline.c: 269
	 * insns won't go through an RCU quiescent state which the /Users/rubber/linux/kernel/bpf/trampoline.c: 270
	 * percpu_ref_kill will be waiting for. Hence the first /Users/rubber/linux/kernel/bpf/trampoline.c: 271
	 * call_rcu_tasks() is not necessary. /Users/rubber/linux/kernel/bpf/trampoline.c: 272
	/* The trampoline without fexit and fmod_ret progs doesn't call original /Users/rubber/linux/kernel/bpf/trampoline.c: 285
	 * function and doesn't use percpu_ref. /Users/rubber/linux/kernel/bpf/trampoline.c: 286
	 * Use call_rcu_tasks_trace() to wait for sleepable progs to finish. /Users/rubber/linux/kernel/bpf/trampoline.c: 287
	 * Then use call_rcu_tasks() to wait for the rest of trampoline asm /Users/rubber/linux/kernel/bpf/trampoline.c: 288
	 * and normal progs. /Users/rubber/linux/kernel/bpf/trampoline.c: 289
			/* The function returns void, we cannot modify its /Users/rubber/linux/kernel/bpf/trampoline.c: 403
			 * return value. /Users/rubber/linux/kernel/bpf/trampoline.c: 404
		/* cannot attach fentry/fexit if extension prog is attached. /Users/rubber/linux/kernel/bpf/trampoline.c: 423
		 * cannot overwrite extension prog either. /Users/rubber/linux/kernel/bpf/trampoline.c: 424
	/* This code will be executed even when the last bpf_tramp_image /Users/rubber/linux/kernel/bpf/trampoline.c: 517
	 * is alive. All progs are detached from the trampoline and the /Users/rubber/linux/kernel/bpf/trampoline.c: 518
	 * trampoline image is patched with jmp into epilogue to skip /Users/rubber/linux/kernel/bpf/trampoline.c: 519
	 * fexit progs. The fentry-only trampoline will be freed via /Users/rubber/linux/kernel/bpf/trampoline.c: 520
	 * multiple rcu callbacks. /Users/rubber/linux/kernel/bpf/trampoline.c: 521
/* The logic is similar to bpf_prog_run(), but with an explicit /Users/rubber/linux/kernel/bpf/trampoline.c: 552
 * rcu_read_lock() and migrate_disable() which are required /Users/rubber/linux/kernel/bpf/trampoline.c: 553
 * for the trampoline. The macro is split into /Users/rubber/linux/kernel/bpf/trampoline.c: 554
 * call __bpf_prog_enter /Users/rubber/linux/kernel/bpf/trampoline.c: 555
 * call prog->bpf_func /Users/rubber/linux/kernel/bpf/trampoline.c: 556
 * call __bpf_prog_exit /Users/rubber/linux/kernel/bpf/trampoline.c: 557
 * __bpf_prog_enter returns: /Users/rubber/linux/kernel/bpf/trampoline.c: 559
 * 0 - skip execution of the bpf prog /Users/rubber/linux/kernel/bpf/trampoline.c: 560
 * 1 - execute bpf prog /Users/rubber/linux/kernel/bpf/trampoline.c: 561
 * [2..MAX_U64] - execute bpf prog and record execution time. /Users/rubber/linux/kernel/bpf/trampoline.c: 562
 *     This is start time. /Users/rubber/linux/kernel/bpf/trampoline.c: 563
	    /* static_key could be enabled in __bpf_prog_enter* /Users/rubber/linux/kernel/bpf/trampoline.c: 583
	     * and disabled in __bpf_prog_exit*. /Users/rubber/linux/kernel/bpf/trampoline.c: 584
	     * And vice versa. /Users/rubber/linux/kernel/bpf/trampoline.c: 585
	     * Hence check that 'start' is valid. /Users/rubber/linux/kernel/bpf/trampoline.c: 586
/* BTF (BPF Type Format) is the meta data format which describes /Users/rubber/linux/kernel/bpf/btf.c: 29
 * the data types of BPF program/map.  Hence, it basically focus /Users/rubber/linux/kernel/bpf/btf.c: 30
 * on the C programming language which the modern BPF is primary /Users/rubber/linux/kernel/bpf/btf.c: 31
 * using. /Users/rubber/linux/kernel/bpf/btf.c: 32
 * ELF Section: /Users/rubber/linux/kernel/bpf/btf.c: 34
 * ~~~~~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 35
 * The BTF data is stored under the ".BTF" ELF section /Users/rubber/linux/kernel/bpf/btf.c: 36
 * struct btf_type: /Users/rubber/linux/kernel/bpf/btf.c: 38
 * ~~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 39
 * Each 'struct btf_type' object describes a C data type. /Users/rubber/linux/kernel/bpf/btf.c: 40
 * Depending on the type it is describing, a 'struct btf_type' /Users/rubber/linux/kernel/bpf/btf.c: 41
 * object may be followed by more data.  F.e. /Users/rubber/linux/kernel/bpf/btf.c: 42
 * To describe an array, 'struct btf_type' is followed by /Users/rubber/linux/kernel/bpf/btf.c: 43
 * 'struct btf_array'. /Users/rubber/linux/kernel/bpf/btf.c: 44
 * 'struct btf_type' and any extra data following it are /Users/rubber/linux/kernel/bpf/btf.c: 46
 * 4 bytes aligned. /Users/rubber/linux/kernel/bpf/btf.c: 47
 * Type section: /Users/rubber/linux/kernel/bpf/btf.c: 49
 * ~~~~~~~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 50
 * The BTF type section contains a list of 'struct btf_type' objects. /Users/rubber/linux/kernel/bpf/btf.c: 51
 * Each one describes a C type.  Recall from the above section /Users/rubber/linux/kernel/bpf/btf.c: 52
 * that a 'struct btf_type' object could be immediately followed by extra /Users/rubber/linux/kernel/bpf/btf.c: 53
 * data in order to describe some particular C types. /Users/rubber/linux/kernel/bpf/btf.c: 54
 * type_id: /Users/rubber/linux/kernel/bpf/btf.c: 56
 * ~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 57
 * Each btf_type object is identified by a type_id.  The type_id /Users/rubber/linux/kernel/bpf/btf.c: 58
 * is implicitly implied by the location of the btf_type object in /Users/rubber/linux/kernel/bpf/btf.c: 59
 * the BTF type section.  The first one has type_id 1.  The second /Users/rubber/linux/kernel/bpf/btf.c: 60
 * one has type_id 2...etc.  Hence, an earlier btf_type has /Users/rubber/linux/kernel/bpf/btf.c: 61
 * a smaller type_id. /Users/rubber/linux/kernel/bpf/btf.c: 62
 * A btf_type object may refer to another btf_type object by using /Users/rubber/linux/kernel/bpf/btf.c: 64
 * type_id (i.e. the "type" in the "struct btf_type"). /Users/rubber/linux/kernel/bpf/btf.c: 65
 * NOTE that we cannot assume any reference-order. /Users/rubber/linux/kernel/bpf/btf.c: 67
 * A btf_type object can refer to an earlier btf_type object /Users/rubber/linux/kernel/bpf/btf.c: 68
 * but it can also refer to a later btf_type object. /Users/rubber/linux/kernel/bpf/btf.c: 69
 * For example, to describe "const void *".  A btf_type /Users/rubber/linux/kernel/bpf/btf.c: 71
 * object describing "const" may refer to another btf_type /Users/rubber/linux/kernel/bpf/btf.c: 72
 * object describing "void *".  This type-reference is done /Users/rubber/linux/kernel/bpf/btf.c: 73
 * by specifying type_id: /Users/rubber/linux/kernel/bpf/btf.c: 74
 * [1] CONST (anon) type_id=2 /Users/rubber/linux/kernel/bpf/btf.c: 76
 * [2] PTR (anon) type_id=0 /Users/rubber/linux/kernel/bpf/btf.c: 77
 * The above is the btf_verifier debug log: /Users/rubber/linux/kernel/bpf/btf.c: 79
 *   - Each line started with "[?]" is a btf_type object /Users/rubber/linux/kernel/bpf/btf.c: 80
 *   - [?] is the type_id of the btf_type object. /Users/rubber/linux/kernel/bpf/btf.c: 81
 *   - CONST/PTR is the BTF_KIND_XXX /Users/rubber/linux/kernel/bpf/btf.c: 82
 *   - "(anon)" is the name of the type.  It just /Users/rubber/linux/kernel/bpf/btf.c: 83
 *     happens that CONST and PTR has no name. /Users/rubber/linux/kernel/bpf/btf.c: 84
 *   - type_id=XXX is the 'u32 type' in btf_type /Users/rubber/linux/kernel/bpf/btf.c: 85
 * NOTE: "void" has type_id 0 /Users/rubber/linux/kernel/bpf/btf.c: 87
 * String section: /Users/rubber/linux/kernel/bpf/btf.c: 89
 * ~~~~~~~~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 90
 * The BTF string section contains the names used by the type section. /Users/rubber/linux/kernel/bpf/btf.c: 91
 * Each string is referred by an "offset" from the beginning of the /Users/rubber/linux/kernel/bpf/btf.c: 92
 * string section. /Users/rubber/linux/kernel/bpf/btf.c: 93
 * Each string is '\0' terminated. /Users/rubber/linux/kernel/bpf/btf.c: 95
 * The first character in the string section must be '\0' /Users/rubber/linux/kernel/bpf/btf.c: 97
 * which is used to mean 'anonymous'. Some btf_type may not /Users/rubber/linux/kernel/bpf/btf.c: 98
 * have a name. /Users/rubber/linux/kernel/bpf/btf.c: 99
/* BTF verification: /Users/rubber/linux/kernel/bpf/btf.c: 102
 * To verify BTF data, two passes are needed. /Users/rubber/linux/kernel/bpf/btf.c: 104
 * Pass #1 /Users/rubber/linux/kernel/bpf/btf.c: 106
 * ~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 107
 * The first pass is to collect all btf_type objects to /Users/rubber/linux/kernel/bpf/btf.c: 108
 * an array: "btf->types". /Users/rubber/linux/kernel/bpf/btf.c: 109
 * Depending on the C type that a btf_type is describing, /Users/rubber/linux/kernel/bpf/btf.c: 111
 * a btf_type may be followed by extra data.  We don't know /Users/rubber/linux/kernel/bpf/btf.c: 112
 * how many btf_type is there, and more importantly we don't /Users/rubber/linux/kernel/bpf/btf.c: 113
 * know where each btf_type is located in the type section. /Users/rubber/linux/kernel/bpf/btf.c: 114
 * Without knowing the location of each type_id, most verifications /Users/rubber/linux/kernel/bpf/btf.c: 116
 * cannot be done.  e.g. an earlier btf_type may refer to a later /Users/rubber/linux/kernel/bpf/btf.c: 117
 * btf_type (recall the "const void *" above), so we cannot /Users/rubber/linux/kernel/bpf/btf.c: 118
 * check this type-reference in the first pass. /Users/rubber/linux/kernel/bpf/btf.c: 119
 * In the first pass, it still does some verifications (e.g. /Users/rubber/linux/kernel/bpf/btf.c: 121
 * checking the name is a valid offset to the string section). /Users/rubber/linux/kernel/bpf/btf.c: 122
 * Pass #2 /Users/rubber/linux/kernel/bpf/btf.c: 124
 * ~~~~~~~ /Users/rubber/linux/kernel/bpf/btf.c: 125
 * The main focus is to resolve a btf_type that is referring /Users/rubber/linux/kernel/bpf/btf.c: 126
 * to another type. /Users/rubber/linux/kernel/bpf/btf.c: 127
 * We have to ensure the referring type: /Users/rubber/linux/kernel/bpf/btf.c: 129
 * 1) does exist in the BTF (i.e. in btf->types[]) /Users/rubber/linux/kernel/bpf/btf.c: 130
 * 2) does not cause a loop: /Users/rubber/linux/kernel/bpf/btf.c: 131
 *	struct A { /Users/rubber/linux/kernel/bpf/btf.c: 132
 *		struct B b; /Users/rubber/linux/kernel/bpf/btf.c: 133
 *	}; /Users/rubber/linux/kernel/bpf/btf.c: 134
 *	struct B { /Users/rubber/linux/kernel/bpf/btf.c: 136
 *		struct A a; /Users/rubber/linux/kernel/bpf/btf.c: 137
 *	}; /Users/rubber/linux/kernel/bpf/btf.c: 138
 * btf_type_needs_resolve() decides if a btf_type needs /Users/rubber/linux/kernel/bpf/btf.c: 140
 * to be resolved. /Users/rubber/linux/kernel/bpf/btf.c: 141
 * The needs_resolve type implements the "resolve()" ops which /Users/rubber/linux/kernel/bpf/btf.c: 143
 * essentially does a DFS and detects backedge. /Users/rubber/linux/kernel/bpf/btf.c: 144
 * During resolve (or DFS), different C types have different /Users/rubber/linux/kernel/bpf/btf.c: 146
 * "RESOLVED" conditions. /Users/rubber/linux/kernel/bpf/btf.c: 147
 * When resolving a BTF_KIND_STRUCT, we need to resolve all its /Users/rubber/linux/kernel/bpf/btf.c: 149
 * members because a member is always referring to another /Users/rubber/linux/kernel/bpf/btf.c: 150
 * type.  A struct's member can be treated as "RESOLVED" if /Users/rubber/linux/kernel/bpf/btf.c: 151
 * it is referring to a BTF_KIND_PTR.  Otherwise, the /Users/rubber/linux/kernel/bpf/btf.c: 152
 * following valid C struct would be rejected: /Users/rubber/linux/kernel/bpf/btf.c: 153
 *	struct A { /Users/rubber/linux/kernel/bpf/btf.c: 155
 *		int m; /Users/rubber/linux/kernel/bpf/btf.c: 156
 *		struct A *a; /Users/rubber/linux/kernel/bpf/btf.c: 157
 *	}; /Users/rubber/linux/kernel/bpf/btf.c: 158
 * When resolving a BTF_KIND_PTR, it needs to keep resolving if /Users/rubber/linux/kernel/bpf/btf.c: 160
 * it is referring to another BTF_KIND_PTR.  Otherwise, we cannot /Users/rubber/linux/kernel/bpf/btf.c: 161
 * detect a pointer loop, e.g.: /Users/rubber/linux/kernel/bpf/btf.c: 162
 * BTF_KIND_CONST -> BTF_KIND_PTR -> BTF_KIND_CONST -> BTF_KIND_PTR + /Users/rubber/linux/kernel/bpf/btf.c: 163
 *                        ^                                         | /Users/rubber/linux/kernel/bpf/btf.c: 164
 *                        +-----------------------------------------+ /Users/rubber/linux/kernel/bpf/btf.c: 165
/* 16MB for 64k structs and each has 16 members and /Users/rubber/linux/kernel/bpf/btf.c: 181
 * a few MB spaces for the string section. /Users/rubber/linux/kernel/bpf/btf.c: 182
 * The hard limit is S32_MAX. /Users/rubber/linux/kernel/bpf/btf.c: 183
	RESOLVE_STRUCT_OR_ARRAY,	/* Resolving for struct/union /Users/rubber/linux/kernel/bpf/btf.c: 243
					 * or array /Users/rubber/linux/kernel/bpf/btf.c: 244
 * This is the maximum size of a base type value (equivalent to a /Users/rubber/linux/kernel/bpf/btf.c: 296
 * 128-bit int); if we are at the end of our safe buffer and have /Users/rubber/linux/kernel/bpf/btf.c: 297
 * less than 16 bytes space we can't be assured of being able /Users/rubber/linux/kernel/bpf/btf.c: 298
 * to copy the next type safely, so in such cases we will initiate /Users/rubber/linux/kernel/bpf/btf.c: 299
 * a new copy. /Users/rubber/linux/kernel/bpf/btf.c: 300
 * Common data to all BTF show operations. Private show functions can add /Users/rubber/linux/kernel/bpf/btf.c: 308
 * their own data to a structure containing a struct btf_show and consult it /Users/rubber/linux/kernel/bpf/btf.c: 309
 * in the show callback.  See btf_type_show() below. /Users/rubber/linux/kernel/bpf/btf.c: 310
 * One challenge with showing nested data is we want to skip 0-valued /Users/rubber/linux/kernel/bpf/btf.c: 312
 * data, but in order to figure out whether a nested object is all zeros /Users/rubber/linux/kernel/bpf/btf.c: 313
 * we need to walk through it.  As a result, we need to make two passes /Users/rubber/linux/kernel/bpf/btf.c: 314
 * when handling structs, unions and arrays; the first path simply looks /Users/rubber/linux/kernel/bpf/btf.c: 315
 * for nonzero data, while the second actually does the display.  The first /Users/rubber/linux/kernel/bpf/btf.c: 316
 * pass is signalled by show->state.depth_check being set, and if we /Users/rubber/linux/kernel/bpf/btf.c: 317
 * encounter a non-zero value we set show->state.depth_to_show to /Users/rubber/linux/kernel/bpf/btf.c: 318
 * the depth at which we encountered it.  When we have completed the /Users/rubber/linux/kernel/bpf/btf.c: 319
 * first pass, we will know if anything needs to be displayed if /Users/rubber/linux/kernel/bpf/btf.c: 320
 * depth_to_show > depth.  See btf_[struct,array]_show() for the /Users/rubber/linux/kernel/bpf/btf.c: 321
 * implementation of this. /Users/rubber/linux/kernel/bpf/btf.c: 322
 * Another problem is we want to ensure the data for display is safe to /Users/rubber/linux/kernel/bpf/btf.c: 324
 * access.  To support this, the anonymous "struct {} obj" tracks the data /Users/rubber/linux/kernel/bpf/btf.c: 325
 * object and our safe copy of it.  We copy portions of the data needed /Users/rubber/linux/kernel/bpf/btf.c: 326
 * to the object "copy" buffer, but because its size is limited to /Users/rubber/linux/kernel/bpf/btf.c: 327
 * BTF_SHOW_OBJ_COPY_LEN bytes, multiple copies may be required as we /Users/rubber/linux/kernel/bpf/btf.c: 328
 * traverse larger objects for display. /Users/rubber/linux/kernel/bpf/btf.c: 329
 * The various data type show functions all start with a call to /Users/rubber/linux/kernel/bpf/btf.c: 331
 * btf_show_start_type() which returns a pointer to the safe copy /Users/rubber/linux/kernel/bpf/btf.c: 332
 * of the data needed (or if BTF_SHOW_UNSAFE is specified, to the /Users/rubber/linux/kernel/bpf/btf.c: 333
 * raw data itself).  btf_show_obj_safe() is responsible for /Users/rubber/linux/kernel/bpf/btf.c: 334
 * using copy_from_kernel_nofault() to update the safe data if necessary /Users/rubber/linux/kernel/bpf/btf.c: 335
 * as we traverse the object's data.  skbuff-like semantics are /Users/rubber/linux/kernel/bpf/btf.c: 336
 * used: /Users/rubber/linux/kernel/bpf/btf.c: 337
 * - obj.head points to the start of the toplevel object for display /Users/rubber/linux/kernel/bpf/btf.c: 339
 * - obj.size is the size of the toplevel object /Users/rubber/linux/kernel/bpf/btf.c: 340
 * - obj.data points to the current point in the original data at /Users/rubber/linux/kernel/bpf/btf.c: 341
 *   which our safe data starts.  obj.data will advance as we copy /Users/rubber/linux/kernel/bpf/btf.c: 342
 *   portions of the data. /Users/rubber/linux/kernel/bpf/btf.c: 343
 * In most cases a single copy will suffice, but larger data structures /Users/rubber/linux/kernel/bpf/btf.c: 345
 * such as "struct task_struct" will require many copies.  The logic in /Users/rubber/linux/kernel/bpf/btf.c: 346
 * btf_show_obj_safe() handles the logic that determines if a new /Users/rubber/linux/kernel/bpf/btf.c: 347
 * copy_from_kernel_nofault() is needed. /Users/rubber/linux/kernel/bpf/btf.c: 348
	/* Some of them is not strictly a C modifier /Users/rubber/linux/kernel/bpf/btf.c: 406
	 * but they are grouped into the same bucket /Users/rubber/linux/kernel/bpf/btf.c: 407
	 * for BTF concern: /Users/rubber/linux/kernel/bpf/btf.c: 408
	 *   A type (t) that refers to another /Users/rubber/linux/kernel/bpf/btf.c: 409
	 *   type through t->type AND its size cannot /Users/rubber/linux/kernel/bpf/btf.c: 410
	 *   be determined without following the t->type. /Users/rubber/linux/kernel/bpf/btf.c: 411
	 * /Users/rubber/linux/kernel/bpf/btf.c: 412
	 * ptr does not fall into this bucket /Users/rubber/linux/kernel/bpf/btf.c: 413
	 * because its size is always sizeof(void *). /Users/rubber/linux/kernel/bpf/btf.c: 414
/* Types that act only as a source, not sink or intermediate /Users/rubber/linux/kernel/bpf/btf.c: 546
 * type when resolving. /Users/rubber/linux/kernel/bpf/btf.c: 547
/* What types need to be resolved? /Users/rubber/linux/kernel/bpf/btf.c: 556
 * btf_type_is_modifier() is an obvious one. /Users/rubber/linux/kernel/bpf/btf.c: 558
 * btf_type_is_struct() because its member refers to /Users/rubber/linux/kernel/bpf/btf.c: 560
 * another type (through member->type). /Users/rubber/linux/kernel/bpf/btf.c: 561
 * btf_type_is_var() because the variable refers to /Users/rubber/linux/kernel/bpf/btf.c: 563
 * another type. btf_type_is_datasec() holds multiple /Users/rubber/linux/kernel/bpf/btf.c: 564
 * btf_type_is_var() types that need resolving. /Users/rubber/linux/kernel/bpf/btf.c: 565
 * btf_type_is_array() because its element (array->type) /Users/rubber/linux/kernel/bpf/btf.c: 567
 * refers to another type.  Array can be thought of a /Users/rubber/linux/kernel/bpf/btf.c: 568
 * special case of struct while array just has the same /Users/rubber/linux/kernel/bpf/btf.c: 569
 * member-type repeated by array->nelems of times. /Users/rubber/linux/kernel/bpf/btf.c: 570
/* Only C-style identifier is permitted. This can be relaxed if /Users/rubber/linux/kernel/bpf/btf.c: 699
 * necessary. /Users/rubber/linux/kernel/bpf/btf.c: 700
 * Regular int is not a bit field and it must be either /Users/rubber/linux/kernel/bpf/btf.c: 740
 * u8/u16/u32/u64 or __int128. /Users/rubber/linux/kernel/bpf/btf.c: 741
 * Check that given struct member is a regular int with expected /Users/rubber/linux/kernel/bpf/btf.c: 763
 * offset and size. /Users/rubber/linux/kernel/bpf/btf.c: 764
		/* if kflag set, int should be a regular int and /Users/rubber/linux/kernel/bpf/btf.c: 785
		 * bit offset should be at byte boundary. /Users/rubber/linux/kernel/bpf/btf.c: 786
 * Populate show->state.name with type name information. /Users/rubber/linux/kernel/bpf/btf.c: 822
 * Format of type name is /Users/rubber/linux/kernel/bpf/btf.c: 823
 * [.member_name = ] (type_name) /Users/rubber/linux/kernel/bpf/btf.c: 825
	/* /Users/rubber/linux/kernel/bpf/btf.c: 847
	 * Don't show type name if we're showing an array member; /Users/rubber/linux/kernel/bpf/btf.c: 848
	 * in that case we show the array type so don't need to repeat /Users/rubber/linux/kernel/bpf/btf.c: 849
	 * ourselves for each member. /Users/rubber/linux/kernel/bpf/btf.c: 850
	/* /Users/rubber/linux/kernel/bpf/btf.c: 862
	 * Start with type_id, as we have resolved the struct btf_type * /Users/rubber/linux/kernel/bpf/btf.c: 863
	 * via btf_modifier_show() past the parent typedef to the child /Users/rubber/linux/kernel/bpf/btf.c: 864
	 * struct, int etc it is defined as.  In such cases, the type_id /Users/rubber/linux/kernel/bpf/btf.c: 865
	 * still represents the starting type while the struct btf_type * /Users/rubber/linux/kernel/bpf/btf.c: 866
	 * in our show->state points at the resolved type of the typedef. /Users/rubber/linux/kernel/bpf/btf.c: 867
	/* /Users/rubber/linux/kernel/bpf/btf.c: 873
	 * The goal here is to build up the right number of pointer and /Users/rubber/linux/kernel/bpf/btf.c: 874
	 * array suffixes while ensuring the type name for a typedef /Users/rubber/linux/kernel/bpf/btf.c: 875
	 * is represented.  Along the way we accumulate a list of /Users/rubber/linux/kernel/bpf/btf.c: 876
	 * BTF kinds we have encountered, since these will inform later /Users/rubber/linux/kernel/bpf/btf.c: 877
	 * display; for example, pointer types will not require an /Users/rubber/linux/kernel/bpf/btf.c: 878
	 * opening "{" for struct, we will just display the pointer value. /Users/rubber/linux/kernel/bpf/btf.c: 879
	 * /Users/rubber/linux/kernel/bpf/btf.c: 880
	 * We also want to accumulate the right number of pointer or array /Users/rubber/linux/kernel/bpf/btf.c: 881
	 * indices in the format string while iterating until we get to /Users/rubber/linux/kernel/bpf/btf.c: 882
	 * the typedef/pointee/array member target type. /Users/rubber/linux/kernel/bpf/btf.c: 883
	 * /Users/rubber/linux/kernel/bpf/btf.c: 884
	 * We start by pointing at the end of pointer and array suffix /Users/rubber/linux/kernel/bpf/btf.c: 885
	 * strings; as we accumulate pointers and arrays we move the pointer /Users/rubber/linux/kernel/bpf/btf.c: 886
	 * or array string backwards so it will show the expected number of /Users/rubber/linux/kernel/bpf/btf.c: 887
	 * '*' or '[]' for the type.  BTF_SHOW_MAX_ITER of nesting of pointers /Users/rubber/linux/kernel/bpf/btf.c: 888
	 * and/or arrays and typedefs are supported as a precaution. /Users/rubber/linux/kernel/bpf/btf.c: 889
	 * /Users/rubber/linux/kernel/bpf/btf.c: 890
	 * We also want to get typedef name while proceeding to resolve /Users/rubber/linux/kernel/bpf/btf.c: 891
	 * type it points to so that we can add parentheses if it is a /Users/rubber/linux/kernel/bpf/btf.c: 892
	 * "typedef struct" etc. /Users/rubber/linux/kernel/bpf/btf.c: 893
/* Macros are used here as btf_show_type_value[s]() prepends and appends /Users/rubber/linux/kernel/bpf/btf.c: 1028
 * format specifiers to the format specifier passed in; these do the work of /Users/rubber/linux/kernel/bpf/btf.c: 1029
 * adding indentation, delimiters etc while the caller simply has to specify /Users/rubber/linux/kernel/bpf/btf.c: 1030
 * the type value(s) in the format specifier + value(s). /Users/rubber/linux/kernel/bpf/btf.c: 1031
 * If object pointed to by @data of @size falls within our safe buffer, return /Users/rubber/linux/kernel/bpf/btf.c: 1071
 * the equivalent pointer to the same safe data.  Assumes /Users/rubber/linux/kernel/bpf/btf.c: 1072
 * copy_from_kernel_nofault() has already happened and our safe buffer is /Users/rubber/linux/kernel/bpf/btf.c: 1073
 * populated. /Users/rubber/linux/kernel/bpf/btf.c: 1074
 * Return a safe-to-access version of data pointed to by @data. /Users/rubber/linux/kernel/bpf/btf.c: 1084
 * We do this by copying the relevant amount of information /Users/rubber/linux/kernel/bpf/btf.c: 1085
 * to the struct btf_show obj.safe buffer using copy_from_kernel_nofault(). /Users/rubber/linux/kernel/bpf/btf.c: 1086
 * If BTF_SHOW_UNSAFE is specified, just return data as-is; no /Users/rubber/linux/kernel/bpf/btf.c: 1088
 * safe copy is needed. /Users/rubber/linux/kernel/bpf/btf.c: 1089
 * Otherwise we need to determine if we have the required amount /Users/rubber/linux/kernel/bpf/btf.c: 1091
 * of data (determined by the @data pointer and the size of the /Users/rubber/linux/kernel/bpf/btf.c: 1092
 * largest base type we can encounter (represented by /Users/rubber/linux/kernel/bpf/btf.c: 1093
 * BTF_SHOW_OBJ_BASE_TYPE_SIZE). Having that much data ensures /Users/rubber/linux/kernel/bpf/btf.c: 1094
 * that we will be able to print some of the current object, /Users/rubber/linux/kernel/bpf/btf.c: 1095
 * and if more is needed a copy will be triggered. /Users/rubber/linux/kernel/bpf/btf.c: 1096
 * Some objects such as structs will not fit into the buffer; /Users/rubber/linux/kernel/bpf/btf.c: 1097
 * in such cases additional copies when we iterate over their /Users/rubber/linux/kernel/bpf/btf.c: 1098
 * members may be needed. /Users/rubber/linux/kernel/bpf/btf.c: 1099
 * btf_show_obj_safe() is used to return a safe buffer for /Users/rubber/linux/kernel/bpf/btf.c: 1101
 * btf_show_start_type(); this ensures that as we recurse into /Users/rubber/linux/kernel/bpf/btf.c: 1102
 * nested types we always have safe data for the given type. /Users/rubber/linux/kernel/bpf/btf.c: 1103
 * This approach is somewhat wasteful; it's possible for example /Users/rubber/linux/kernel/bpf/btf.c: 1104
 * that when iterating over a large union we'll end up copying the /Users/rubber/linux/kernel/bpf/btf.c: 1105
 * same data repeatedly, but the goal is safety not performance. /Users/rubber/linux/kernel/bpf/btf.c: 1106
 * We use stack data as opposed to per-CPU buffers because the /Users/rubber/linux/kernel/bpf/btf.c: 1107
 * iteration over a type can take some time, and preemption handling /Users/rubber/linux/kernel/bpf/btf.c: 1108
 * would greatly complicate use of the safe buffer. /Users/rubber/linux/kernel/bpf/btf.c: 1109
	/* /Users/rubber/linux/kernel/bpf/btf.c: 1128
	 * Is this toplevel object? If so, set total object size and /Users/rubber/linux/kernel/bpf/btf.c: 1129
	 * initialize pointers.  Otherwise check if we still fall within /Users/rubber/linux/kernel/bpf/btf.c: 1130
	 * our safe object data. /Users/rubber/linux/kernel/bpf/btf.c: 1131
		/* /Users/rubber/linux/kernel/bpf/btf.c: 1137
		 * If the size of the current object is > our remaining /Users/rubber/linux/kernel/bpf/btf.c: 1138
		 * safe buffer we _may_ need to do a new copy.  However /Users/rubber/linux/kernel/bpf/btf.c: 1139
		 * consider the case of a nested struct; it's size pushes /Users/rubber/linux/kernel/bpf/btf.c: 1140
		 * us over the safe buffer limit, but showing any individual /Users/rubber/linux/kernel/bpf/btf.c: 1141
		 * struct members does not.  In such cases, we don't need /Users/rubber/linux/kernel/bpf/btf.c: 1142
		 * to initiate a fresh copy yet; however we definitely need /Users/rubber/linux/kernel/bpf/btf.c: 1143
		 * at least BTF_SHOW_OBJ_BASE_TYPE_SIZE bytes left /Users/rubber/linux/kernel/bpf/btf.c: 1144
		 * in our buffer, regardless of the current object size. /Users/rubber/linux/kernel/bpf/btf.c: 1145
		 * The logic here is that as we resolve types we will /Users/rubber/linux/kernel/bpf/btf.c: 1146
		 * hit a base type at some point, and we need to be sure /Users/rubber/linux/kernel/bpf/btf.c: 1147
		 * the next chunk of data is safely available to display /Users/rubber/linux/kernel/bpf/btf.c: 1148
		 * that type info safely.  We cannot rely on the size of /Users/rubber/linux/kernel/bpf/btf.c: 1149
		 * the current object here because it may be much larger /Users/rubber/linux/kernel/bpf/btf.c: 1150
		 * than our current buffer (e.g. task_struct is 8k). /Users/rubber/linux/kernel/bpf/btf.c: 1151
		 * All we want to do here is ensure that we can print the /Users/rubber/linux/kernel/bpf/btf.c: 1152
		 * next basic type, which we can if either /Users/rubber/linux/kernel/bpf/btf.c: 1153
		 * - the current type size is within the safe buffer; or /Users/rubber/linux/kernel/bpf/btf.c: 1154
		 * - at least BTF_SHOW_OBJ_BASE_TYPE_SIZE bytes are left in /Users/rubber/linux/kernel/bpf/btf.c: 1155
		 *   the safe buffer. /Users/rubber/linux/kernel/bpf/btf.c: 1156
	/* /Users/rubber/linux/kernel/bpf/btf.c: 1163
	 * We need a new copy to our safe object, either because we haven't /Users/rubber/linux/kernel/bpf/btf.c: 1164
	 * yet copied and are initializing safe data, or because the data /Users/rubber/linux/kernel/bpf/btf.c: 1165
	 * we want falls outside the boundaries of the safe object. /Users/rubber/linux/kernel/bpf/btf.c: 1166
 * Set the type we are starting to show and return a safe data pointer /Users/rubber/linux/kernel/bpf/btf.c: 1184
 * to be used for showing the associated data. /Users/rubber/linux/kernel/bpf/btf.c: 1185
	/* btf verifier prints all types it is processing via /Users/rubber/linux/kernel/bpf/btf.c: 1321
	 * btf_verifier_log_type(..., fmt = NULL). /Users/rubber/linux/kernel/bpf/btf.c: 1322
	 * Skip those prints for in-kernel BTF verification. /Users/rubber/linux/kernel/bpf/btf.c: 1323
	/* The CHECK_META phase already did a btf dump. /Users/rubber/linux/kernel/bpf/btf.c: 1367
	 * /Users/rubber/linux/kernel/bpf/btf.c: 1368
	 * If member is logged again, it must hit an error in /Users/rubber/linux/kernel/bpf/btf.c: 1369
	 * parsing this member.  It is useful to print out which /Users/rubber/linux/kernel/bpf/btf.c: 1370
	 * struct this member belongs to. /Users/rubber/linux/kernel/bpf/btf.c: 1371
	/* /Users/rubber/linux/kernel/bpf/btf.c: 1517
	 * In map-in-map, calling map_delete_elem() on outer /Users/rubber/linux/kernel/bpf/btf.c: 1518
	 * map will call bpf_map_put on the inner map. /Users/rubber/linux/kernel/bpf/btf.c: 1519
	 * It will then eventually call btf_free_id() /Users/rubber/linux/kernel/bpf/btf.c: 1520
	 * on the inner map.  Some of the map_delete_elem() /Users/rubber/linux/kernel/bpf/btf.c: 1521
	 * implementation may have irq disabled, so /Users/rubber/linux/kernel/bpf/btf.c: 1522
	 * we need to use the _irqsave() version instead /Users/rubber/linux/kernel/bpf/btf.c: 1523
	 * of the _bh() version. /Users/rubber/linux/kernel/bpf/btf.c: 1524
		/* int, enum, void, struct, array, func or func_proto is a sink /Users/rubber/linux/kernel/bpf/btf.c: 1610
		 * for ptr /Users/rubber/linux/kernel/bpf/btf.c: 1611
		/* int, enum, void, ptr, func or func_proto is a sink /Users/rubber/linux/kernel/bpf/btf.c: 1616
		 * for struct and array /Users/rubber/linux/kernel/bpf/btf.c: 1617
/* Resolve the size of a passed-in "type" /Users/rubber/linux/kernel/bpf/btf.c: 1691
 * type: is an array (e.g. u32 array[x][y]) /Users/rubber/linux/kernel/bpf/btf.c: 1693
 * return type: type "u32[x][y]", i.e. BTF_KIND_ARRAY, /Users/rubber/linux/kernel/bpf/btf.c: 1694
 * *type_size: (x * y * sizeof(u32)).  Hence, *type_size always /Users/rubber/linux/kernel/bpf/btf.c: 1695
 *             corresponds to the return type. /Users/rubber/linux/kernel/bpf/btf.c: 1696
 * *elem_type: u32 /Users/rubber/linux/kernel/bpf/btf.c: 1697
 * *elem_id: id of u32 /Users/rubber/linux/kernel/bpf/btf.c: 1698
 * *total_nelems: (x * y).  Hence, individual elem size is /Users/rubber/linux/kernel/bpf/btf.c: 1699
 *                (*type_size / *total_nelems) /Users/rubber/linux/kernel/bpf/btf.c: 1700
 * *type_id: id of type if it's changed within the function, 0 if not /Users/rubber/linux/kernel/bpf/btf.c: 1701
 * type: is not an array (e.g. const struct X) /Users/rubber/linux/kernel/bpf/btf.c: 1703
 * return type: type "struct X" /Users/rubber/linux/kernel/bpf/btf.c: 1704
 * *type_size: sizeof(struct X) /Users/rubber/linux/kernel/bpf/btf.c: 1705
 * *elem_type: same as return type ("struct X") /Users/rubber/linux/kernel/bpf/btf.c: 1706
 * *elem_id: 0 /Users/rubber/linux/kernel/bpf/btf.c: 1707
 * *total_nelems: 1 /Users/rubber/linux/kernel/bpf/btf.c: 1708
 * *type_id: id of type if it's changed within the function, 0 if not /Users/rubber/linux/kernel/bpf/btf.c: 1709
/* Used for ptr, array struct/union and float type members. /Users/rubber/linux/kernel/bpf/btf.c: 1873
 * int, enum and modifier types have their specific callback functions. /Users/rubber/linux/kernel/bpf/btf.c: 1874
	/* bitfield size is 0, so member->offset represents bit offset only. /Users/rubber/linux/kernel/bpf/btf.c: 1887
	 * It is safe to call non kflag check_member variants. /Users/rubber/linux/kernel/bpf/btf.c: 1888
		/* Not a bitfield member, member offset must be at byte /Users/rubber/linux/kernel/bpf/btf.c: 1969
		 * boundary. /Users/rubber/linux/kernel/bpf/btf.c: 1970
	/* /Users/rubber/linux/kernel/bpf/btf.c: 2047
	 * Only one of the encoding bits is allowed and it /Users/rubber/linux/kernel/bpf/btf.c: 2048
	 * should be sufficient for the pretty print purpose (i.e. decoding). /Users/rubber/linux/kernel/bpf/btf.c: 2049
	 * Multiple bits can be allowed later if it is found /Users/rubber/linux/kernel/bpf/btf.c: 2050
	 * to be insufficient. /Users/rubber/linux/kernel/bpf/btf.c: 2051
	/* data points to a __int128 number. /Users/rubber/linux/kernel/bpf/btf.c: 2081
	 * Suppose /Users/rubber/linux/kernel/bpf/btf.c: 2082
	 *     int128_num = *(__int128 *)data; /Users/rubber/linux/kernel/bpf/btf.c: 2083
	 * The below formulas shows what upper_num and lower_num represents: /Users/rubber/linux/kernel/bpf/btf.c: 2084
	 *     upper_num = int128_num >> 64; /Users/rubber/linux/kernel/bpf/btf.c: 2085
	 *     lower_num = int128_num & 0xffffffffFFFFFFFFULL; /Users/rubber/linux/kernel/bpf/btf.c: 2086
	/* /Users/rubber/linux/kernel/bpf/btf.c: 2179
	 * bits_offset is at most 7. /Users/rubber/linux/kernel/bpf/btf.c: 2180
	 * BTF_INT_OFFSET() cannot exceed 128 bits. /Users/rubber/linux/kernel/bpf/btf.c: 2181
	/* typedef type must have a valid name, and other ref types, /Users/rubber/linux/kernel/bpf/btf.c: 2363
	 * volatile, const, restrict, should have a null name. /Users/rubber/linux/kernel/bpf/btf.c: 2364
	/* Figure out the resolved next_type_id with size. /Users/rubber/linux/kernel/bpf/btf.c: 2402
	 * They will be stored in the current modifier's /Users/rubber/linux/kernel/bpf/btf.c: 2403
	 * resolved_ids and resolved_sizes such that it can /Users/rubber/linux/kernel/bpf/btf.c: 2404
	 * save us a few type-following when we use it later (e.g. in /Users/rubber/linux/kernel/bpf/btf.c: 2405
	 * pretty print). /Users/rubber/linux/kernel/bpf/btf.c: 2406
	/* We must resolve to something concrete at this point, no /Users/rubber/linux/kernel/bpf/btf.c: 2458
	 * forward types or similar that would resolve to size of /Users/rubber/linux/kernel/bpf/btf.c: 2459
	 * zero is allowed. /Users/rubber/linux/kernel/bpf/btf.c: 2460
	/* If the modifier was RESOLVED during RESOLVE_STRUCT_OR_ARRAY, /Users/rubber/linux/kernel/bpf/btf.c: 2490
	 * the modifier may have stopped resolving when it was resolved /Users/rubber/linux/kernel/bpf/btf.c: 2491
	 * to a ptr (last-resolved-ptr). /Users/rubber/linux/kernel/bpf/btf.c: 2492
	 * /Users/rubber/linux/kernel/bpf/btf.c: 2493
	 * We now need to continue from the last-resolved-ptr to /Users/rubber/linux/kernel/bpf/btf.c: 2494
	 * ensure the last-resolved-ptr will not referring back to /Users/rubber/linux/kernel/bpf/btf.c: 2495
	 * the currenct ptr (t). /Users/rubber/linux/kernel/bpf/btf.c: 2496
	/* Array elem type and index type cannot be in type void, /Users/rubber/linux/kernel/bpf/btf.c: 2698
	 * so !array->type and !array->index_type are not allowed. /Users/rubber/linux/kernel/bpf/btf.c: 2699
		/* /Users/rubber/linux/kernel/bpf/btf.c: 2810
		 * BTF_INT_CHAR encoding never seems to be set for /Users/rubber/linux/kernel/bpf/btf.c: 2811
		 * char arrays, so if size is 1 and element is /Users/rubber/linux/kernel/bpf/btf.c: 2812
		 * printable as a char, we'll do that. /Users/rubber/linux/kernel/bpf/btf.c: 2813
	/* /Users/rubber/linux/kernel/bpf/btf.c: 2849
	 * First check if any members would be shown (are non-zero). /Users/rubber/linux/kernel/bpf/btf.c: 2850
	 * See comments above "struct btf_show" definition for more /Users/rubber/linux/kernel/bpf/btf.c: 2851
	 * details on how this works at a high-level. /Users/rubber/linux/kernel/bpf/btf.c: 2852
		/* /Users/rubber/linux/kernel/bpf/btf.c: 2868
		 * Reaching here indicates we have recursed and found /Users/rubber/linux/kernel/bpf/btf.c: 2869
		 * non-zero array member(s). /Users/rubber/linux/kernel/bpf/btf.c: 2870
		/* /Users/rubber/linux/kernel/bpf/btf.c: 2968
		 * ">" instead of ">=" because the last member could be /Users/rubber/linux/kernel/bpf/btf.c: 2969
		 * "char a[0];" /Users/rubber/linux/kernel/bpf/btf.c: 2970
	/* Before continue resolving the next_member, /Users/rubber/linux/kernel/bpf/btf.c: 2998
	 * ensure the last member is indeed resolved to a /Users/rubber/linux/kernel/bpf/btf.c: 2999
	 * type with size info. /Users/rubber/linux/kernel/bpf/btf.c: 3000
/* find 'struct bpf_spin_lock' in map value. /Users/rubber/linux/kernel/bpf/btf.c: 3136
 * return >= 0 offset if found /Users/rubber/linux/kernel/bpf/btf.c: 3137
 * and < 0 in case of error /Users/rubber/linux/kernel/bpf/btf.c: 3138
	/* /Users/rubber/linux/kernel/bpf/btf.c: 3207
	 * First check if any members would be shown (are non-zero). /Users/rubber/linux/kernel/bpf/btf.c: 3208
	 * See comments above "struct btf_show" definition for more /Users/rubber/linux/kernel/bpf/btf.c: 3209
	 * details on how this works at a high-level. /Users/rubber/linux/kernel/bpf/btf.c: 3210
		/* /Users/rubber/linux/kernel/bpf/btf.c: 3226
		 * Reaching here indicates we have recursed and found /Users/rubber/linux/kernel/bpf/btf.c: 3227
		 * non-zero child values. /Users/rubber/linux/kernel/bpf/btf.c: 3228
	/* /Users/rubber/linux/kernel/bpf/btf.c: 3484
	 * BTF_KIND_FUNC_PROTO cannot be directly referred by /Users/rubber/linux/kernel/bpf/btf.c: 3485
	 * a struct's member. /Users/rubber/linux/kernel/bpf/btf.c: 3486
	 * /Users/rubber/linux/kernel/bpf/btf.c: 3487
	 * It should be a function pointer instead. /Users/rubber/linux/kernel/bpf/btf.c: 3488
	 * (i.e. struct's member -> BTF_KIND_PTR -> BTF_KIND_FUNC_PROTO) /Users/rubber/linux/kernel/bpf/btf.c: 3489
	 * /Users/rubber/linux/kernel/bpf/btf.c: 3490
	 * Hence, there is no btf_func_check_member(). /Users/rubber/linux/kernel/bpf/btf.c: 3491
	/* Different architectures have different alignment requirements, so /Users/rubber/linux/kernel/bpf/btf.c: 3784
	 * here we check only for the reasonable minimum. This way we ensure /Users/rubber/linux/kernel/bpf/btf.c: 3785
	 * that types after CO-RE can pass the kernel BTF verifier. /Users/rubber/linux/kernel/bpf/btf.c: 3786
		/* user requested verbose verifier output /Users/rubber/linux/kernel/bpf/btf.c: 4455
		 * and supplied buffer to store the verification trace /Users/rubber/linux/kernel/bpf/btf.c: 4456
		/* Only pointer to struct is supported for now. /Users/rubber/linux/kernel/bpf/btf.c: 4574
		 * That means that BPF_PROG_TYPE_TRACEPOINT with BTF /Users/rubber/linux/kernel/bpf/btf.c: 4575
		 * is not supported yet. /Users/rubber/linux/kernel/bpf/btf.c: 4576
		 * BPF_PROG_TYPE_RAW_TRACEPOINT is fine. /Users/rubber/linux/kernel/bpf/btf.c: 4577
	/* ctx_struct is a pointer to prog_ctx_type in vmlinux. /Users/rubber/linux/kernel/bpf/btf.c: 4588
	 * Like 'struct __sk_buff' /Users/rubber/linux/kernel/bpf/btf.c: 4589
	/* only compare that prog's ctx type name is the same as /Users/rubber/linux/kernel/bpf/btf.c: 4601
	 * kernel expects. No need to compare field by field. /Users/rubber/linux/kernel/bpf/btf.c: 4602
	 * It's ok for bpf prog to do: /Users/rubber/linux/kernel/bpf/btf.c: 4603
	 * struct __sk_buff {}; /Users/rubber/linux/kernel/bpf/btf.c: 4604
	 * int socket_filter_bpf_prog(struct __sk_buff *skb) /Users/rubber/linux/kernel/bpf/btf.c: 4605
	 * { // no fields of skb are ever used } /Users/rubber/linux/kernel/bpf/btf.c: 4606
	/* if (t == NULL) Fall back to default BPF prog with /Users/rubber/linux/kernel/bpf/btf.c: 4850
	 * MAX_BPF_FUNC_REG_ARGS u64 arguments. /Users/rubber/linux/kernel/bpf/btf.c: 4851
			/* When LSM programs are attached to void LSM hooks /Users/rubber/linux/kernel/bpf/btf.c: 4870
			 * they use FEXIT trampolines and when attached to /Users/rubber/linux/kernel/bpf/btf.c: 4871
			 * int LSM hooks, they use MODIFY_RETURN trampolines. /Users/rubber/linux/kernel/bpf/btf.c: 4872
			 * /Users/rubber/linux/kernel/bpf/btf.c: 4873
			 * While the LSM programs are BPF_MODIFY_RETURN-like /Users/rubber/linux/kernel/bpf/btf.c: 4874
			 * the check: /Users/rubber/linux/kernel/bpf/btf.c: 4875
			 * /Users/rubber/linux/kernel/bpf/btf.c: 4876
			 *	if (ret_type != 'int') /Users/rubber/linux/kernel/bpf/btf.c: 4877
			 *		return -EINVAL; /Users/rubber/linux/kernel/bpf/btf.c: 4878
			 * /Users/rubber/linux/kernel/bpf/btf.c: 4879
			 * is _not_ done here. This is still safe as LSM hooks /Users/rubber/linux/kernel/bpf/btf.c: 4880
			 * have only void and int return types. /Users/rubber/linux/kernel/bpf/btf.c: 4881
			/* For now the BPF_MODIFY_RETURN can only be attached to /Users/rubber/linux/kernel/bpf/btf.c: 4888
			 * functions that return an int. /Users/rubber/linux/kernel/bpf/btf.c: 4889
		/* This is a pointer to void. /Users/rubber/linux/kernel/bpf/btf.c: 4942
		 * It is the same as scalar from the verifier safety pov. /Users/rubber/linux/kernel/bpf/btf.c: 4943
		 * No further pointer walking is allowed. /Users/rubber/linux/kernel/bpf/btf.c: 4944
		/* If the last element is a variable size array, we may /Users/rubber/linux/kernel/bpf/btf.c: 5033
		 * need to relax the rule. /Users/rubber/linux/kernel/bpf/btf.c: 5034
		/* Only allow structure for now, can be relaxed for /Users/rubber/linux/kernel/bpf/btf.c: 5055
		 * other types later. /Users/rubber/linux/kernel/bpf/btf.c: 5056
			/* off <= moff instead of off == moff because clang /Users/rubber/linux/kernel/bpf/btf.c: 5083
			 * does not generate a BTF member for anonymous /Users/rubber/linux/kernel/bpf/btf.c: 5084
			 * bitfield like the ":16" here: /Users/rubber/linux/kernel/bpf/btf.c: 5085
			 * struct { /Users/rubber/linux/kernel/bpf/btf.c: 5086
			 *	int :16; /Users/rubber/linux/kernel/bpf/btf.c: 5087
			 *	int x:8; /Users/rubber/linux/kernel/bpf/btf.c: 5088
			 * }; /Users/rubber/linux/kernel/bpf/btf.c: 5089
			/* off may be accessing a following member /Users/rubber/linux/kernel/bpf/btf.c: 5095
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5096
			 * or /Users/rubber/linux/kernel/bpf/btf.c: 5097
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5098
			 * Doing partial access at either end of this /Users/rubber/linux/kernel/bpf/btf.c: 5099
			 * bitfield.  Continue on this case also to /Users/rubber/linux/kernel/bpf/btf.c: 5100
			 * treat it as not accessing this bitfield /Users/rubber/linux/kernel/bpf/btf.c: 5101
			 * and eventually error out as field not /Users/rubber/linux/kernel/bpf/btf.c: 5102
			 * found to keep it simple. /Users/rubber/linux/kernel/bpf/btf.c: 5103
			 * It could be relaxed if there was a legit /Users/rubber/linux/kernel/bpf/btf.c: 5104
			 * partial access case later. /Users/rubber/linux/kernel/bpf/btf.c: 5105
			/* __btf_resolve_size() above helps to /Users/rubber/linux/kernel/bpf/btf.c: 5135
			 * linearize a multi-dimensional array. /Users/rubber/linux/kernel/bpf/btf.c: 5136
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5137
			 * The logic here is treating an array /Users/rubber/linux/kernel/bpf/btf.c: 5138
			 * in a struct as the following way: /Users/rubber/linux/kernel/bpf/btf.c: 5139
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5140
			 * struct outer { /Users/rubber/linux/kernel/bpf/btf.c: 5141
			 *	struct inner array[2][2]; /Users/rubber/linux/kernel/bpf/btf.c: 5142
			 * }; /Users/rubber/linux/kernel/bpf/btf.c: 5143
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5144
			 * looks like: /Users/rubber/linux/kernel/bpf/btf.c: 5145
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5146
			 * struct outer { /Users/rubber/linux/kernel/bpf/btf.c: 5147
			 *	struct inner array_elem0; /Users/rubber/linux/kernel/bpf/btf.c: 5148
			 *	struct inner array_elem1; /Users/rubber/linux/kernel/bpf/btf.c: 5149
			 *	struct inner array_elem2; /Users/rubber/linux/kernel/bpf/btf.c: 5150
			 *	struct inner array_elem3; /Users/rubber/linux/kernel/bpf/btf.c: 5151
			 * }; /Users/rubber/linux/kernel/bpf/btf.c: 5152
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5153
			 * When accessing outer->array[1][0], it moves /Users/rubber/linux/kernel/bpf/btf.c: 5154
			 * moff to "array_elem2", set mtype to /Users/rubber/linux/kernel/bpf/btf.c: 5155
			 * "struct inner", and msize also becomes /Users/rubber/linux/kernel/bpf/btf.c: 5156
			 * sizeof(struct inner).  Then most of the /Users/rubber/linux/kernel/bpf/btf.c: 5157
			 * remaining logic will fall through without /Users/rubber/linux/kernel/bpf/btf.c: 5158
			 * caring the current member is an array or /Users/rubber/linux/kernel/bpf/btf.c: 5159
			 * not. /Users/rubber/linux/kernel/bpf/btf.c: 5160
			 * /Users/rubber/linux/kernel/bpf/btf.c: 5161
			 * Unlike mtype/msize/moff, mtrue_end does not /Users/rubber/linux/kernel/bpf/btf.c: 5162
			 * change.  The naming difference ("_true") tells /Users/rubber/linux/kernel/bpf/btf.c: 5163
			 * that it is not always corresponding to /Users/rubber/linux/kernel/bpf/btf.c: 5164
			 * the current mtype/msize/moff. /Users/rubber/linux/kernel/bpf/btf.c: 5165
			 * It is the true end of the current /Users/rubber/linux/kernel/bpf/btf.c: 5166
			 * member (i.e. array in this case).  That /Users/rubber/linux/kernel/bpf/btf.c: 5167
			 * will allow an int array to be accessed like /Users/rubber/linux/kernel/bpf/btf.c: 5168
			 * a scratch space, /Users/rubber/linux/kernel/bpf/btf.c: 5169
			 * i.e. allow access beyond the size of /Users/rubber/linux/kernel/bpf/btf.c: 5170
			 *      the array's element as long as it is /Users/rubber/linux/kernel/bpf/btf.c: 5171
			 *      within the mtrue_end boundary. /Users/rubber/linux/kernel/bpf/btf.c: 5172
		/* the 'off' we're looking for is either equal to start /Users/rubber/linux/kernel/bpf/btf.c: 5186
		 * of this field or inside of this struct /Users/rubber/linux/kernel/bpf/btf.c: 5187
		/* Allow more flexible access within an int as long as /Users/rubber/linux/kernel/bpf/btf.c: 5221
		 * it is within mtrue_end. /Users/rubber/linux/kernel/bpf/btf.c: 5222
		 * Since mtrue_end could be the end of an array, /Users/rubber/linux/kernel/bpf/btf.c: 5223
		 * that also allows using an array of int as a scratch /Users/rubber/linux/kernel/bpf/btf.c: 5224
		 * space. e.g. skb->cb[]. /Users/rubber/linux/kernel/bpf/btf.c: 5225
			/* If we found the pointer or scalar on t+off, /Users/rubber/linux/kernel/bpf/btf.c: 5253
			 * we're done. /Users/rubber/linux/kernel/bpf/btf.c: 5254
			/* We found nested struct, so continue the search /Users/rubber/linux/kernel/bpf/btf.c: 5261
			 * by diving in it. At this point the offset is /Users/rubber/linux/kernel/bpf/btf.c: 5262
			 * aligned with the new type, so set it to 0. /Users/rubber/linux/kernel/bpf/btf.c: 5263
			/* It's either error or unknown return value.. /Users/rubber/linux/kernel/bpf/btf.c: 5269
			 * scream and leave. /Users/rubber/linux/kernel/bpf/btf.c: 5270
/* Check that two BTF types, each specified as an BTF object + id, are exactly /Users/rubber/linux/kernel/bpf/btf.c: 5281
 * the same. Trivial ID check is not enough due to module BTFs, because we can /Users/rubber/linux/kernel/bpf/btf.c: 5282
 * end up with two different module BTFs, but IDs point to the common type in /Users/rubber/linux/kernel/bpf/btf.c: 5283
 * vmlinux BTF. /Users/rubber/linux/kernel/bpf/btf.c: 5284
	/* We found nested struct object. If it matches /Users/rubber/linux/kernel/bpf/btf.c: 5315
	 * the requested ID, we're done. Otherwise let's /Users/rubber/linux/kernel/bpf/btf.c: 5316
	 * continue the search with offset 0 in the new /Users/rubber/linux/kernel/bpf/btf.c: 5317
	 * type. /Users/rubber/linux/kernel/bpf/btf.c: 5318
		/* BTF function prototype doesn't match the verifier types. /Users/rubber/linux/kernel/bpf/btf.c: 5364
		 * Fall back to MAX_BPF_FUNC_REG_ARGS u64 args. /Users/rubber/linux/kernel/bpf/btf.c: 5365
/* Compare BTFs of two functions assuming only scalars and pointers to context. /Users/rubber/linux/kernel/bpf/btf.c: 5416
 * t1 points to BTF_KIND_FUNC in btf1 /Users/rubber/linux/kernel/bpf/btf.c: 5417
 * t2 points to BTF_KIND_FUNC in btf2 /Users/rubber/linux/kernel/bpf/btf.c: 5418
 * Returns: /Users/rubber/linux/kernel/bpf/btf.c: 5419
 * EINVAL - function prototype mismatch /Users/rubber/linux/kernel/bpf/btf.c: 5420
 * EFAULT - verifier bug /Users/rubber/linux/kernel/bpf/btf.c: 5421
 * 0 - 99% match. The last 1% is validated by the verifier. /Users/rubber/linux/kernel/bpf/btf.c: 5422
		/* global functions are validated with scalars and pointers /Users/rubber/linux/kernel/bpf/btf.c: 5490
		 * to context only. And only global functions can be replaced. /Users/rubber/linux/kernel/bpf/btf.c: 5491
		 * Hence type check only those types. /Users/rubber/linux/kernel/bpf/btf.c: 5492
		/* This is an optional check to make program writing easier. /Users/rubber/linux/kernel/bpf/btf.c: 5516
		 * Compare names of structs and report an error to the user. /Users/rubber/linux/kernel/bpf/btf.c: 5517
		 * btf_prepare_func_args() already checked that t2 struct /Users/rubber/linux/kernel/bpf/btf.c: 5518
		 * is a context type. btf_prepare_func_args() will check /Users/rubber/linux/kernel/bpf/btf.c: 5519
		 * later that t1 struct is a context type as well. /Users/rubber/linux/kernel/bpf/btf.c: 5520
		/* These checks were already done by the verifier while loading /Users/rubber/linux/kernel/bpf/btf.c: 5579
		 * struct bpf_func_info or in add_kfunc_call(). /Users/rubber/linux/kernel/bpf/btf.c: 5580
	/* check that BTF function arguments match actual types that the /Users/rubber/linux/kernel/bpf/btf.c: 5601
	 * verifier sees. /Users/rubber/linux/kernel/bpf/btf.c: 5602
			/* If function expects ctx type in BTF check that caller /Users/rubber/linux/kernel/bpf/btf.c: 5665
			 * is passing PTR_TO_CTX. /Users/rubber/linux/kernel/bpf/btf.c: 5666
/* Compare BTF of a function with given bpf_reg_state. /Users/rubber/linux/kernel/bpf/btf.c: 5699
 * Returns: /Users/rubber/linux/kernel/bpf/btf.c: 5700
 * EFAULT - there is a verifier bug. Abort verification. /Users/rubber/linux/kernel/bpf/btf.c: 5701
 * EINVAL - there is a type mismatch or BTF is not available. /Users/rubber/linux/kernel/bpf/btf.c: 5702
 * 0 - BTF matches with what bpf_reg_state expects. /Users/rubber/linux/kernel/bpf/btf.c: 5703
 * Only PTR_TO_CTX and SCALAR_VALUE states are recognized. /Users/rubber/linux/kernel/bpf/btf.c: 5704
	/* Compiler optimizations can remove arguments from static functions /Users/rubber/linux/kernel/bpf/btf.c: 5728
	 * or mismatched type can be passed into a global function. /Users/rubber/linux/kernel/bpf/btf.c: 5729
	 * In such cases mark the function as unreliable from BTF point of view. /Users/rubber/linux/kernel/bpf/btf.c: 5730
/* Convert BTF of a function into bpf_reg_state if possible /Users/rubber/linux/kernel/bpf/btf.c: 5744
 * Returns: /Users/rubber/linux/kernel/bpf/btf.c: 5745
 * EFAULT - there is a verifier bug. Abort verification. /Users/rubber/linux/kernel/bpf/btf.c: 5746
 * EINVAL - cannot convert BTF. /Users/rubber/linux/kernel/bpf/btf.c: 5747
 * 0 - Successfully converted BTF into bpf_reg_state /Users/rubber/linux/kernel/bpf/btf.c: 5748
 * (either PTR_TO_CTX or SCALAR_VALUE). /Users/rubber/linux/kernel/bpf/btf.c: 5749
		/* These checks were already done by the verifier while loading /Users/rubber/linux/kernel/bpf/btf.c: 5777
		 * struct bpf_func_info /Users/rubber/linux/kernel/bpf/btf.c: 5778
	/* Convert BTF function arguments into verifier types. /Users/rubber/linux/kernel/bpf/btf.c: 5819
	 * Only PTR_TO_CTX and SCALAR are supported atm. /Users/rubber/linux/kernel/bpf/btf.c: 5820
	/* /Users/rubber/linux/kernel/bpf/btf.c: 5995
	 * The BTF ID is published to the userspace. /Users/rubber/linux/kernel/bpf/btf.c: 5996
	 * All BTF free must go through call_rcu() from /Users/rubber/linux/kernel/bpf/btf.c: 5997
	 * now on (i.e. free by calling btf_put()). /Users/rubber/linux/kernel/bpf/btf.c: 5998
	/* ret is never zero, since btf_find_by_name_kind returns /Users/rubber/linux/kernel/bpf/btf.c: 6299
	 * positive btf_id or negative error. /Users/rubber/linux/kernel/bpf/btf.c: 6300
			/* linear search could be slow hence unlock/lock /Users/rubber/linux/kernel/bpf/btf.c: 6311
			 * the IDR to avoiding holding it for too long /Users/rubber/linux/kernel/bpf/btf.c: 6312
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 1
 * Copyright (c) 2020 Facebook /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 3
 * Copyright 2020 Google LLC. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 4
	/* Neither the bpf_prog nor the bpf-map's syscall /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 86
	 * could be modifying the local_storage->list now. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 87
	 * Thus, no elem can be added-to or deleted-from the /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 88
	 * local_storage->list by the bpf_prog or by the bpf-map's syscall. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 89
	 * /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 90
	 * It is racing with bpf_local_storage_map_free() alone /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 91
	 * when unlinking elem from the local_storage->list and /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 92
	 * the map's bucket->list. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 93
		/* Always unlink from map before unlinking from /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 98
		 * local_storage. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 99
	/* free_task_storage should always be true as long as /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 109
	 * local_storage->list was non-empty. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 110
	/* We should be in an RCU read side critical section, it should be safe /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 129
	 * to call pid_task. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 130
	/* We should be in an RCU read side critical section, it should be safe /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 163
	 * to call pid_task. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 164
	/* We should be in an RCU read side critical section, it should be safe /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 209
	 * to call pid_task. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 210
	/* This helper must only be called from places where the lifetime of the task /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 269
	 * is guaranteed. Either by being refcounted or by being protected /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 270
	 * by an RCU read-side critical section. /Users/rubber/linux/kernel/bpf/bpf_task_storage.c: 271
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/verifier.c: 1
/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com /Users/rubber/linux/kernel/bpf/verifier.c: 2
 * Copyright (c) 2016 Facebook /Users/rubber/linux/kernel/bpf/verifier.c: 3
 * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io /Users/rubber/linux/kernel/bpf/verifier.c: 4
/* bpf_check() is a static code analyzer that walks eBPF program /Users/rubber/linux/kernel/bpf/verifier.c: 39
 * instruction by instruction and updates register/stack state. /Users/rubber/linux/kernel/bpf/verifier.c: 40
 * All paths of conditional branches are analyzed until 'bpf_exit' insn. /Users/rubber/linux/kernel/bpf/verifier.c: 41
 * The first pass is depth-first-search to check that the program is a DAG. /Users/rubber/linux/kernel/bpf/verifier.c: 43
 * It rejects the following programs: /Users/rubber/linux/kernel/bpf/verifier.c: 44
 * - larger than BPF_MAXINSNS insns /Users/rubber/linux/kernel/bpf/verifier.c: 45
 * - if loop is present (detected via back-edge) /Users/rubber/linux/kernel/bpf/verifier.c: 46
 * - unreachable insns exist (shouldn't be a forest. program = one function) /Users/rubber/linux/kernel/bpf/verifier.c: 47
 * - out of bounds or malformed jumps /Users/rubber/linux/kernel/bpf/verifier.c: 48
 * The second pass is all possible path descent from the 1st insn. /Users/rubber/linux/kernel/bpf/verifier.c: 49
 * Since it's analyzing all paths through the program, the length of the /Users/rubber/linux/kernel/bpf/verifier.c: 50
 * analysis is limited to 64k insn, which may be hit even if total number of /Users/rubber/linux/kernel/bpf/verifier.c: 51
 * insn is less then 4K, but there are too many branches that change stack/regs. /Users/rubber/linux/kernel/bpf/verifier.c: 52
 * Number of 'branches to be analyzed' is limited to 1k /Users/rubber/linux/kernel/bpf/verifier.c: 53
 * On entry to each instruction, each register has a type, and the instruction /Users/rubber/linux/kernel/bpf/verifier.c: 55
 * changes the types of the registers depending on instruction semantics. /Users/rubber/linux/kernel/bpf/verifier.c: 56
 * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is /Users/rubber/linux/kernel/bpf/verifier.c: 57
 * copied to R1. /Users/rubber/linux/kernel/bpf/verifier.c: 58
 * All registers are 64-bit. /Users/rubber/linux/kernel/bpf/verifier.c: 60
 * R0 - return register /Users/rubber/linux/kernel/bpf/verifier.c: 61
 * R1-R5 argument passing registers /Users/rubber/linux/kernel/bpf/verifier.c: 62
 * R6-R9 callee saved registers /Users/rubber/linux/kernel/bpf/verifier.c: 63
 * R10 - frame pointer read-only /Users/rubber/linux/kernel/bpf/verifier.c: 64
 * At the start of BPF program the register R1 contains a pointer to bpf_context /Users/rubber/linux/kernel/bpf/verifier.c: 66
 * and has type PTR_TO_CTX. /Users/rubber/linux/kernel/bpf/verifier.c: 67
 * Verifier tracks arithmetic operations on pointers in case: /Users/rubber/linux/kernel/bpf/verifier.c: 69
 *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10), /Users/rubber/linux/kernel/bpf/verifier.c: 70
 *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20), /Users/rubber/linux/kernel/bpf/verifier.c: 71
 * 1st insn copies R10 (which has FRAME_PTR) type into R1 /Users/rubber/linux/kernel/bpf/verifier.c: 72
 * and 2nd arithmetic instruction is pattern matched to recognize /Users/rubber/linux/kernel/bpf/verifier.c: 73
 * that it wants to construct a pointer to some element within stack. /Users/rubber/linux/kernel/bpf/verifier.c: 74
 * So after 2nd insn, the register R1 has type PTR_TO_STACK /Users/rubber/linux/kernel/bpf/verifier.c: 75
 * (and -20 constant is saved for further stack bounds checking). /Users/rubber/linux/kernel/bpf/verifier.c: 76
 * Meaning that this reg is a pointer to stack plus known immediate constant. /Users/rubber/linux/kernel/bpf/verifier.c: 77
 * Most of the time the registers have SCALAR_VALUE type, which /Users/rubber/linux/kernel/bpf/verifier.c: 79
 * means the register has some value, but it's not a valid pointer. /Users/rubber/linux/kernel/bpf/verifier.c: 80
 * (like pointer plus pointer becomes SCALAR_VALUE type) /Users/rubber/linux/kernel/bpf/verifier.c: 81
 * When verifier sees load or store instructions the type of base register /Users/rubber/linux/kernel/bpf/verifier.c: 83
 * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are /Users/rubber/linux/kernel/bpf/verifier.c: 84
 * four pointer types recognized by check_mem_access() function. /Users/rubber/linux/kernel/bpf/verifier.c: 85
 * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value' /Users/rubber/linux/kernel/bpf/verifier.c: 87
 * and the range of [ptr, ptr + map's value_size) is accessible. /Users/rubber/linux/kernel/bpf/verifier.c: 88
 * registers used to pass values to function calls are checked against /Users/rubber/linux/kernel/bpf/verifier.c: 90
 * function argument constraints. /Users/rubber/linux/kernel/bpf/verifier.c: 91
 * ARG_PTR_TO_MAP_KEY is one of such argument constraints. /Users/rubber/linux/kernel/bpf/verifier.c: 93
 * It means that the register type passed to this function must be /Users/rubber/linux/kernel/bpf/verifier.c: 94
 * PTR_TO_STACK and it will be used inside the function as /Users/rubber/linux/kernel/bpf/verifier.c: 95
 * 'pointer to map element key' /Users/rubber/linux/kernel/bpf/verifier.c: 96
 * For example the argument constraints for bpf_map_lookup_elem(): /Users/rubber/linux/kernel/bpf/verifier.c: 98
 *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL, /Users/rubber/linux/kernel/bpf/verifier.c: 99
 *   .arg1_type = ARG_CONST_MAP_PTR, /Users/rubber/linux/kernel/bpf/verifier.c: 100
 *   .arg2_type = ARG_PTR_TO_MAP_KEY, /Users/rubber/linux/kernel/bpf/verifier.c: 101
 * ret_type says that this function returns 'pointer to map elem value or null' /Users/rubber/linux/kernel/bpf/verifier.c: 103
 * function expects 1st argument to be a const pointer to 'struct bpf_map' and /Users/rubber/linux/kernel/bpf/verifier.c: 104
 * 2nd argument should be a pointer to stack, which will be used inside /Users/rubber/linux/kernel/bpf/verifier.c: 105
 * the helper function as a pointer to map element key. /Users/rubber/linux/kernel/bpf/verifier.c: 106
 * On the kernel side the helper function looks like: /Users/rubber/linux/kernel/bpf/verifier.c: 108
 * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5) /Users/rubber/linux/kernel/bpf/verifier.c: 109
 * { /Users/rubber/linux/kernel/bpf/verifier.c: 110
 *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1; /Users/rubber/linux/kernel/bpf/verifier.c: 111
 *    void *key = (void *) (unsigned long) r2; /Users/rubber/linux/kernel/bpf/verifier.c: 112
 *    void *value; /Users/rubber/linux/kernel/bpf/verifier.c: 113
 *    here kernel can access 'key' and 'map' pointers safely, knowing that /Users/rubber/linux/kernel/bpf/verifier.c: 115
 *    [key, key + map->key_size) bytes are valid and were initialized on /Users/rubber/linux/kernel/bpf/verifier.c: 116
 *    the stack of eBPF program. /Users/rubber/linux/kernel/bpf/verifier.c: 117
 * } /Users/rubber/linux/kernel/bpf/verifier.c: 118
 * Corresponding eBPF program may look like: /Users/rubber/linux/kernel/bpf/verifier.c: 120
 *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR /Users/rubber/linux/kernel/bpf/verifier.c: 121
 *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK /Users/rubber/linux/kernel/bpf/verifier.c: 122
 *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP /Users/rubber/linux/kernel/bpf/verifier.c: 123
 *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem), /Users/rubber/linux/kernel/bpf/verifier.c: 124
 * here verifier looks at prototype of map_lookup_elem() and sees: /Users/rubber/linux/kernel/bpf/verifier.c: 125
 * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok, /Users/rubber/linux/kernel/bpf/verifier.c: 126
 * Now verifier knows that this map has key of R1->map_ptr->key_size bytes /Users/rubber/linux/kernel/bpf/verifier.c: 127
 * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far, /Users/rubber/linux/kernel/bpf/verifier.c: 129
 * Now verifier checks that [R2, R2 + map's key_size) are within stack limits /Users/rubber/linux/kernel/bpf/verifier.c: 130
 * and were initialized prior to this call. /Users/rubber/linux/kernel/bpf/verifier.c: 131
 * If it's ok, then verifier allows this BPF_CALL insn and looks at /Users/rubber/linux/kernel/bpf/verifier.c: 132
 * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets /Users/rubber/linux/kernel/bpf/verifier.c: 133
 * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function /Users/rubber/linux/kernel/bpf/verifier.c: 134
 * returns either pointer to map value or NULL. /Users/rubber/linux/kernel/bpf/verifier.c: 135
 * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off' /Users/rubber/linux/kernel/bpf/verifier.c: 137
 * insn, the register holding that pointer in the true branch changes state to /Users/rubber/linux/kernel/bpf/verifier.c: 138
 * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false /Users/rubber/linux/kernel/bpf/verifier.c: 139
 * branch. See check_cond_jmp_op(). /Users/rubber/linux/kernel/bpf/verifier.c: 140
 * After the call R0 is set to return type of the function and registers R1-R5 /Users/rubber/linux/kernel/bpf/verifier.c: 142
 * are set to NOT_INIT to indicate that they are no longer readable. /Users/rubber/linux/kernel/bpf/verifier.c: 143
 * The following reference types represent a potential reference to a kernel /Users/rubber/linux/kernel/bpf/verifier.c: 145
 * resource which, after first being allocated, must be checked and freed by /Users/rubber/linux/kernel/bpf/verifier.c: 146
 * the BPF program: /Users/rubber/linux/kernel/bpf/verifier.c: 147
 * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET /Users/rubber/linux/kernel/bpf/verifier.c: 148
 * When the verifier sees a helper call return a reference type, it allocates a /Users/rubber/linux/kernel/bpf/verifier.c: 150
 * pointer id for the reference and stores it in the current function state. /Users/rubber/linux/kernel/bpf/verifier.c: 151
 * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into /Users/rubber/linux/kernel/bpf/verifier.c: 152
 * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type /Users/rubber/linux/kernel/bpf/verifier.c: 153
 * passes through a NULL-check conditional. For the branch wherein the state is /Users/rubber/linux/kernel/bpf/verifier.c: 154
 * changed to CONST_IMM, the verifier releases the reference. /Users/rubber/linux/kernel/bpf/verifier.c: 155
 * For each helper function that allocates a reference, such as /Users/rubber/linux/kernel/bpf/verifier.c: 157
 * bpf_sk_lookup_tcp(), there is a corresponding release function, such as /Users/rubber/linux/kernel/bpf/verifier.c: 158
 * bpf_sk_release(). When a reference type passes into the release function, /Users/rubber/linux/kernel/bpf/verifier.c: 159
 * the verifier also releases the reference. If any unchecked or unreleased /Users/rubber/linux/kernel/bpf/verifier.c: 160
 * reference remains at the end of the program, the verifier rejects it. /Users/rubber/linux/kernel/bpf/verifier.c: 161
	/* verifer state is 'st' /Users/rubber/linux/kernel/bpf/verifier.c: 166
	 * before processing instruction 'insn_idx' /Users/rubber/linux/kernel/bpf/verifier.c: 167
	 * and after processing instruction 'prev_insn_idx' /Users/rubber/linux/kernel/bpf/verifier.c: 168
/* log_level controls verbosity level of eBPF verifier. /Users/rubber/linux/kernel/bpf/verifier.c: 321
 * bpf_verifier_log_write() is used to dump the verification trace to the log, /Users/rubber/linux/kernel/bpf/verifier.c: 322
 * so the user can figure out what's wrong with the program /Users/rubber/linux/kernel/bpf/verifier.c: 323
/* Determine whether the function releases some resources allocated by another /Users/rubber/linux/kernel/bpf/verifier.c: 485
 * function call. The first reference type argument will be assumed to be /Users/rubber/linux/kernel/bpf/verifier.c: 486
 * released by release_reference(). /Users/rubber/linux/kernel/bpf/verifier.c: 487
/* The reg state of a pointer or a bounded scalar was saved when /Users/rubber/linux/kernel/bpf/verifier.c: 609
 * it was spilled to the stack. /Users/rubber/linux/kernel/bpf/verifier.c: 610
				/* Typically an immediate SCALAR_VALUE, but /Users/rubber/linux/kernel/bpf/verifier.c: 666
				 * could be a pointer whose offset is too big /Users/rubber/linux/kernel/bpf/verifier.c: 667
				 * for reg->off /Users/rubber/linux/kernel/bpf/verifier.c: 668
/* copy array src of length n * size bytes to dst. dst is reallocated if it's too /Users/rubber/linux/kernel/bpf/verifier.c: 753
 * small to hold src. This is different from krealloc since we don't want to preserve /Users/rubber/linux/kernel/bpf/verifier.c: 754
 * the contents of dst. /Users/rubber/linux/kernel/bpf/verifier.c: 755
 * Leaves dst untouched if src is NULL or length is zero. Returns NULL if memory could /Users/rubber/linux/kernel/bpf/verifier.c: 757
 * not be allocated. /Users/rubber/linux/kernel/bpf/verifier.c: 758
/* resize an array from old_n items to new_n items. the array is reallocated if it's too /Users/rubber/linux/kernel/bpf/verifier.c: 782
 * small to hold new_n items. new items are zeroed out if the array grows. /Users/rubber/linux/kernel/bpf/verifier.c: 783
 * Contrary to krealloc_array, does not free arr if new_n is zero. /Users/rubber/linux/kernel/bpf/verifier.c: 785
/* Acquire a pointer id from the env and update the state->refs to include /Users/rubber/linux/kernel/bpf/verifier.c: 853
 * this new pointer reference. /Users/rubber/linux/kernel/bpf/verifier.c: 854
 * On success, returns a valid pointer id to associate with the register /Users/rubber/linux/kernel/bpf/verifier.c: 855
 * On failure, returns a negative errno. /Users/rubber/linux/kernel/bpf/verifier.c: 856
/* copy verifier state from src to dst growing dst stack space /Users/rubber/linux/kernel/bpf/verifier.c: 923
 * when necessary to accommodate larger src stack /Users/rubber/linux/kernel/bpf/verifier.c: 924
		/* WARN_ON(br > 1) technically makes sense here, /Users/rubber/linux/kernel/bpf/verifier.c: 983
		 * but see comment in push_stack(), hence: /Users/rubber/linux/kernel/bpf/verifier.c: 984
		/* WARN_ON(branches > 2) technically makes sense here, /Users/rubber/linux/kernel/bpf/verifier.c: 1053
		 * but /Users/rubber/linux/kernel/bpf/verifier.c: 1054
		 * 1. speculative states will bump 'branches' for non-branch /Users/rubber/linux/kernel/bpf/verifier.c: 1055
		 * instructions /Users/rubber/linux/kernel/bpf/verifier.c: 1056
		 * 2. is_state_visited() heuristics may decide not to create /Users/rubber/linux/kernel/bpf/verifier.c: 1057
		 * a new state for a sequence of branches and all such current /Users/rubber/linux/kernel/bpf/verifier.c: 1058
		 * and cloned states will be pointing to a single parent state /Users/rubber/linux/kernel/bpf/verifier.c: 1059
		 * which might have large 'branches' count. /Users/rubber/linux/kernel/bpf/verifier.c: 1060
/* Mark the unknown part of a register (variable offset or scalar value) as /Users/rubber/linux/kernel/bpf/verifier.c: 1095
 * known to have the value @imm. /Users/rubber/linux/kernel/bpf/verifier.c: 1096
/* Mark the 'variable offset' part of a register as zero.  This should be /Users/rubber/linux/kernel/bpf/verifier.c: 1115
 * used only on registers holding a pointer type. /Users/rubber/linux/kernel/bpf/verifier.c: 1116
			/* transfer reg's id which is unique for every map_lookup_elem /Users/rubber/linux/kernel/bpf/verifier.c: 1151
			 * as UID of the inner map. /Users/rubber/linux/kernel/bpf/verifier.c: 1152
	/* The register can already have a range from prior markings. /Users/rubber/linux/kernel/bpf/verifier.c: 1207
	 * This is fine as long as it hasn't been advanced from its /Users/rubber/linux/kernel/bpf/verifier.c: 1208
	 * origin. /Users/rubber/linux/kernel/bpf/verifier.c: 1209
	/* Learn sign from signed bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 1284
	 * If we cannot cross the sign boundary, then signed and unsigned bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1285
	 * are the same, so combine.  This works even in the negative case, e.g. /Users/rubber/linux/kernel/bpf/verifier.c: 1286
	 * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff. /Users/rubber/linux/kernel/bpf/verifier.c: 1287
	/* Learn sign from unsigned bounds.  Signed bounds cross the sign /Users/rubber/linux/kernel/bpf/verifier.c: 1296
	 * boundary, so we must be careful. /Users/rubber/linux/kernel/bpf/verifier.c: 1297
		/* Positive.  We can't learn anything from the smin, but smax /Users/rubber/linux/kernel/bpf/verifier.c: 1300
		 * is positive, hence safe. /Users/rubber/linux/kernel/bpf/verifier.c: 1301
		/* Negative.  We can't learn anything from the smax, but smin /Users/rubber/linux/kernel/bpf/verifier.c: 1307
		 * is negative, hence safe. /Users/rubber/linux/kernel/bpf/verifier.c: 1308
	/* Learn sign from signed bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 1318
	 * If we cannot cross the sign boundary, then signed and unsigned bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1319
	 * are the same, so combine.  This works even in the negative case, e.g. /Users/rubber/linux/kernel/bpf/verifier.c: 1320
	 * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff. /Users/rubber/linux/kernel/bpf/verifier.c: 1321
	/* Learn sign from unsigned bounds.  Signed bounds cross the sign /Users/rubber/linux/kernel/bpf/verifier.c: 1330
	 * boundary, so we must be careful. /Users/rubber/linux/kernel/bpf/verifier.c: 1331
		/* Positive.  We can't learn anything from the smin, but smax /Users/rubber/linux/kernel/bpf/verifier.c: 1334
		 * is positive, hence safe. /Users/rubber/linux/kernel/bpf/verifier.c: 1335
		/* Negative.  We can't learn anything from the smax, but smin /Users/rubber/linux/kernel/bpf/verifier.c: 1341
		 * is negative, hence safe. /Users/rubber/linux/kernel/bpf/verifier.c: 1342
	/* Attempt to pull 32-bit signed bounds into 64-bit bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1373
	 * but must be positive otherwise set to worse case bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1374
	 * and refine later from tnum. /Users/rubber/linux/kernel/bpf/verifier.c: 1375
	/* special case when 64-bit register has upper 32-bit register /Users/rubber/linux/kernel/bpf/verifier.c: 1389
	 * zeroed. Typically happens after zext or <<32, >>32 sequence /Users/rubber/linux/kernel/bpf/verifier.c: 1390
	 * allowing us to use 32-bit bounds directly, /Users/rubber/linux/kernel/bpf/verifier.c: 1391
		/* Otherwise the best we can do is push lower 32bit known and /Users/rubber/linux/kernel/bpf/verifier.c: 1396
		 * unknown bits into register (var_off set from jmp logic) /Users/rubber/linux/kernel/bpf/verifier.c: 1397
		 * then learn as much as possible from the 64-bit tnum /Users/rubber/linux/kernel/bpf/verifier.c: 1398
		 * known and unknown bits. The previous smin/smax bounds are /Users/rubber/linux/kernel/bpf/verifier.c: 1399
		 * invalid here because of jmp32 compare so mark them unknown /Users/rubber/linux/kernel/bpf/verifier.c: 1400
		 * so they do not impact tnum bounds calculation. /Users/rubber/linux/kernel/bpf/verifier.c: 1401
	/* Intersecting with the old var_off might have improved our bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1407
	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc), /Users/rubber/linux/kernel/bpf/verifier.c: 1408
	 * then new var_off is (0; 0x7f...fc) which improves our umax. /Users/rubber/linux/kernel/bpf/verifier.c: 1409
	/* Intersecting with the old var_off might have improved our bounds /Users/rubber/linux/kernel/bpf/verifier.c: 1439
	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc), /Users/rubber/linux/kernel/bpf/verifier.c: 1440
	 * then new var_off is (0; 0x7f...fc) which improves our umax. /Users/rubber/linux/kernel/bpf/verifier.c: 1441
	/* /Users/rubber/linux/kernel/bpf/verifier.c: 1452
	 * Clear type, id, off, and union(map_ptr, range) and /Users/rubber/linux/kernel/bpf/verifier.c: 1453
	 * padding between 'type' and union /Users/rubber/linux/kernel/bpf/verifier.c: 1454
	/* Unlike push_stack() do not copy_verifier_state(). /Users/rubber/linux/kernel/bpf/verifier.c: 1567
	 * The caller state doesn't matter. /Users/rubber/linux/kernel/bpf/verifier.c: 1568
	 * This is async callback. It starts in a fresh stack. /Users/rubber/linux/kernel/bpf/verifier.c: 1569
	 * Initialize it similar to do_check_common(). /Users/rubber/linux/kernel/bpf/verifier.c: 1570
 callsite */, /Users/rubber/linux/kernel/bpf/verifier.c: 1577
 frameno within this callchain */, /Users/rubber/linux/kernel/bpf/verifier.c: 1578
 subprog number within this prog */); /Users/rubber/linux/kernel/bpf/verifier.c: 1579
			/* In the future, this can be allowed to increase limit /Users/rubber/linux/kernel/bpf/verifier.c: 1773
			 * of fd index into fd_array, interpreted as u16. /Users/rubber/linux/kernel/bpf/verifier.c: 1774
	/* func_id == 0 is always invalid, but instead of returning an error, be /Users/rubber/linux/kernel/bpf/verifier.c: 1827
	 * conservative and wait until the code elimination pass before returning /Users/rubber/linux/kernel/bpf/verifier.c: 1828
	 * error, so that invalid calls that get pruned out can be in BPF programs /Users/rubber/linux/kernel/bpf/verifier.c: 1829
	 * loaded from userspace.  It is also required that offset be untouched /Users/rubber/linux/kernel/bpf/verifier.c: 1830
	 * for such calls. /Users/rubber/linux/kernel/bpf/verifier.c: 1831
	/* Add a fake 'exit' subprog which could simplify subprog iteration /Users/rubber/linux/kernel/bpf/verifier.c: 1967
	 * logic. 'subprog_cnt' should not be increased. /Users/rubber/linux/kernel/bpf/verifier.c: 1968
			/* to avoid fall-through from one subprog into another /Users/rubber/linux/kernel/bpf/verifier.c: 2010
			 * the last insn of the subprog should be either exit /Users/rubber/linux/kernel/bpf/verifier.c: 2011
			 * or unconditional jump back /Users/rubber/linux/kernel/bpf/verifier.c: 2012
/* Parentage chain of this register (or stack slot) should take care of all /Users/rubber/linux/kernel/bpf/verifier.c: 2028
 * issues like callee-saved registers, stack slot allocation time, etc. /Users/rubber/linux/kernel/bpf/verifier.c: 2029
		/* The first condition is more likely to be true than the /Users/rubber/linux/kernel/bpf/verifier.c: 2048
		 * second, checked it first. /Users/rubber/linux/kernel/bpf/verifier.c: 2049
			/* The parentage chain never changes and /Users/rubber/linux/kernel/bpf/verifier.c: 2053
			 * this parent was already marked as LIVE_READ. /Users/rubber/linux/kernel/bpf/verifier.c: 2054
			 * There is no need to keep walking the chain again and /Users/rubber/linux/kernel/bpf/verifier.c: 2055
			 * keep re-marking all parents as LIVE_READ. /Users/rubber/linux/kernel/bpf/verifier.c: 2056
			 * This case happens when the same register is read /Users/rubber/linux/kernel/bpf/verifier.c: 2057
			 * multiple times without writes into it in-between. /Users/rubber/linux/kernel/bpf/verifier.c: 2058
			 * Also, if parent has the stronger REG_LIVE_READ64 set, /Users/rubber/linux/kernel/bpf/verifier.c: 2059
			 * then no need to set the weak REG_LIVE_READ32. /Users/rubber/linux/kernel/bpf/verifier.c: 2060
/* This function is supposed to be used by the following 32-bit optimization /Users/rubber/linux/kernel/bpf/verifier.c: 2079
 * code only. It returns TRUE if the source or destination register operates /Users/rubber/linux/kernel/bpf/verifier.c: 2080
 * on 64-bit, otherwise return FALSE. /Users/rubber/linux/kernel/bpf/verifier.c: 2081
		/* BPF_EXIT for "main" will reach here. Return TRUE /Users/rubber/linux/kernel/bpf/verifier.c: 2092
		 * conservatively. /Users/rubber/linux/kernel/bpf/verifier.c: 2093
			/* BPF to BPF call will reach here because of marking /Users/rubber/linux/kernel/bpf/verifier.c: 2098
			 * caller saved clobber with DST_OP_NO_MARK for which we /Users/rubber/linux/kernel/bpf/verifier.c: 2099
			 * don't care the register def because they are anyway /Users/rubber/linux/kernel/bpf/verifier.c: 2100
			 * marked as NOT_INIT already. /Users/rubber/linux/kernel/bpf/verifier.c: 2101
			/* Helper call will reach here because of arg type /Users/rubber/linux/kernel/bpf/verifier.c: 2105
			 * check, conservatively return TRUE. /Users/rubber/linux/kernel/bpf/verifier.c: 2106
		/* BPF_STX (including atomic variants) has multiple source /Users/rubber/linux/kernel/bpf/verifier.c: 2131
		 * operands, one of which is a ptr. Check whether the caller is /Users/rubber/linux/kernel/bpf/verifier.c: 2132
		 * asking about it. /Users/rubber/linux/kernel/bpf/verifier.c: 2133
/* Backtrack one insn at a time. If idx is not at the top of recorded /Users/rubber/linux/kernel/bpf/verifier.c: 2277
 * history then previous instruction came from straight line execution. /Users/rubber/linux/kernel/bpf/verifier.c: 2278
/* For given verifier state backtrack_insn() is called from the last insn to /Users/rubber/linux/kernel/bpf/verifier.c: 2310
 * the first insn. Its purpose is to compute a bitmask of registers and /Users/rubber/linux/kernel/bpf/verifier.c: 2311
 * stack slots that needs precision in the parent verifier state. /Users/rubber/linux/kernel/bpf/verifier.c: 2312
				/* dreg = sreg /Users/rubber/linux/kernel/bpf/verifier.c: 2343
				 * dreg needs precision after this insn /Users/rubber/linux/kernel/bpf/verifier.c: 2344
				 * sreg needs precision before this insn /Users/rubber/linux/kernel/bpf/verifier.c: 2345
				/* dreg = K /Users/rubber/linux/kernel/bpf/verifier.c: 2350
				 * dreg needs precision after this insn. /Users/rubber/linux/kernel/bpf/verifier.c: 2351
				 * Corresponding register is already marked /Users/rubber/linux/kernel/bpf/verifier.c: 2352
				 * as precise=true in this verifier state. /Users/rubber/linux/kernel/bpf/verifier.c: 2353
				 * No further markings in parent are necessary /Users/rubber/linux/kernel/bpf/verifier.c: 2354
				/* dreg += sreg /Users/rubber/linux/kernel/bpf/verifier.c: 2360
				 * both dreg and sreg need precision /Users/rubber/linux/kernel/bpf/verifier.c: 2361
				 * before this insn /Users/rubber/linux/kernel/bpf/verifier.c: 2362
			} /* else dreg += K /Users/rubber/linux/kernel/bpf/verifier.c: 2365
			   * dreg still needs precision before this insn /Users/rubber/linux/kernel/bpf/verifier.c: 2366
		/* scalars can only be spilled into stack w/o losing precision. /Users/rubber/linux/kernel/bpf/verifier.c: 2374
		 * Load from any other memory can be zero extended. /Users/rubber/linux/kernel/bpf/verifier.c: 2375
		 * The desire to keep that precision is already indicated /Users/rubber/linux/kernel/bpf/verifier.c: 2376
		 * by 'precise' mark in corresponding register of this state. /Users/rubber/linux/kernel/bpf/verifier.c: 2377
		 * No further tracking necessary. /Users/rubber/linux/kernel/bpf/verifier.c: 2378
		/* dreg = *(u64 *)[fp - off] was a fill from the stack. /Users/rubber/linux/kernel/bpf/verifier.c: 2385
		 * that [fp - off] slot contains scalar that needs to be /Users/rubber/linux/kernel/bpf/verifier.c: 2386
		 * tracked with precision /Users/rubber/linux/kernel/bpf/verifier.c: 2387
			/* stx & st shouldn't be using _scalar_ dst_reg /Users/rubber/linux/kernel/bpf/verifier.c: 2398
			 * to access memory. It means backtracking /Users/rubber/linux/kernel/bpf/verifier.c: 2399
			 * encountered a case of pointer subtraction. /Users/rubber/linux/kernel/bpf/verifier.c: 2400
				/* if backtracing was looking for registers R1-R5 /Users/rubber/linux/kernel/bpf/verifier.c: 2426
				 * they should have been found already. /Users/rubber/linux/kernel/bpf/verifier.c: 2427
		/* It's ld_imm64 or ld_abs or ld_ind. /Users/rubber/linux/kernel/bpf/verifier.c: 2440
		 * For ld_imm64 no further tracking of precision /Users/rubber/linux/kernel/bpf/verifier.c: 2441
		 * into parent is necessary /Users/rubber/linux/kernel/bpf/verifier.c: 2442
/* the scalar precision tracking algorithm: /Users/rubber/linux/kernel/bpf/verifier.c: 2451
 * . at the start all registers have precise=false. /Users/rubber/linux/kernel/bpf/verifier.c: 2452
 * . scalar ranges are tracked as normal through alu and jmp insns. /Users/rubber/linux/kernel/bpf/verifier.c: 2453
 * . once precise value of the scalar register is used in: /Users/rubber/linux/kernel/bpf/verifier.c: 2454
 *   .  ptr + scalar alu /Users/rubber/linux/kernel/bpf/verifier.c: 2455
 *   . if (scalar cond K|scalar) /Users/rubber/linux/kernel/bpf/verifier.c: 2456
 *   .  helper_call(.., scalar, ...) where ARG_CONST is expected /Users/rubber/linux/kernel/bpf/verifier.c: 2457
 *   backtrack through the verifier states and mark all registers and /Users/rubber/linux/kernel/bpf/verifier.c: 2458
 *   stack slots with spilled constants that these scalar regisers /Users/rubber/linux/kernel/bpf/verifier.c: 2459
 *   should be precise. /Users/rubber/linux/kernel/bpf/verifier.c: 2460
 * . during state pruning two registers (or spilled stack slots) /Users/rubber/linux/kernel/bpf/verifier.c: 2461
 *   are equivalent if both are not precise. /Users/rubber/linux/kernel/bpf/verifier.c: 2462
 * Note the verifier cannot simply walk register parentage chain, /Users/rubber/linux/kernel/bpf/verifier.c: 2464
 * since many different registers and stack slots could have been /Users/rubber/linux/kernel/bpf/verifier.c: 2465
 * used to compute single precise scalar. /Users/rubber/linux/kernel/bpf/verifier.c: 2466
 * The approach of starting with precise=true for all registers and then /Users/rubber/linux/kernel/bpf/verifier.c: 2468
 * backtrack to mark a register as not precise when the verifier detects /Users/rubber/linux/kernel/bpf/verifier.c: 2469
 * that program doesn't care about specific value (e.g., when helper /Users/rubber/linux/kernel/bpf/verifier.c: 2470
 * takes register as ARG_ANYTHING parameter) is not safe. /Users/rubber/linux/kernel/bpf/verifier.c: 2471
 * It's ok to walk single parentage chain of the verifier states. /Users/rubber/linux/kernel/bpf/verifier.c: 2473
 * It's possible that this backtracking will go all the way till 1st insn. /Users/rubber/linux/kernel/bpf/verifier.c: 2474
 * All other branches will be explored for needing precision later. /Users/rubber/linux/kernel/bpf/verifier.c: 2475
 * The backtracking needs to deal with cases like: /Users/rubber/linux/kernel/bpf/verifier.c: 2477
 *   R8=map_value(id=0,off=0,ks=4,vs=1952,imm=0) R9_w=map_value(id=0,off=40,ks=4,vs=1952,imm=0) /Users/rubber/linux/kernel/bpf/verifier.c: 2478
 * r9 -= r8 /Users/rubber/linux/kernel/bpf/verifier.c: 2479
 * r5 = r9 /Users/rubber/linux/kernel/bpf/verifier.c: 2480
 * if r5 > 0x79f goto pc+7 /Users/rubber/linux/kernel/bpf/verifier.c: 2481
 *    R5_w=inv(id=0,umax_value=1951,var_off=(0x0; 0x7ff)) /Users/rubber/linux/kernel/bpf/verifier.c: 2482
 * r5 += 1 /Users/rubber/linux/kernel/bpf/verifier.c: 2483
 * ... /Users/rubber/linux/kernel/bpf/verifier.c: 2484
 * call bpf_perf_event_output#25 /Users/rubber/linux/kernel/bpf/verifier.c: 2485
 *   where .arg5_type = ARG_CONST_SIZE_OR_ZERO /Users/rubber/linux/kernel/bpf/verifier.c: 2486
 * and this case: /Users/rubber/linux/kernel/bpf/verifier.c: 2488
 * r6 = 1 /Users/rubber/linux/kernel/bpf/verifier.c: 2489
 * call foo // uses callee's r6 inside to compute r0 /Users/rubber/linux/kernel/bpf/verifier.c: 2490
 * r0 += r6 /Users/rubber/linux/kernel/bpf/verifier.c: 2491
 * if r0 == 0 goto /Users/rubber/linux/kernel/bpf/verifier.c: 2492
 * to track above reg_mask/stack_mask needs to be independent for each frame. /Users/rubber/linux/kernel/bpf/verifier.c: 2494
 * Also if parent's curframe > frame where backtracking started, /Users/rubber/linux/kernel/bpf/verifier.c: 2496
 * the verifier need to mark registers in both frames, otherwise callees /Users/rubber/linux/kernel/bpf/verifier.c: 2497
 * may incorrectly prune callers. This is similar to /Users/rubber/linux/kernel/bpf/verifier.c: 2498
 * commit 7640ead93924 ("bpf: verifier: make sure callees don't prune with caller differences") /Users/rubber/linux/kernel/bpf/verifier.c: 2499
 * For now backtracking falls back into conservative marking. /Users/rubber/linux/kernel/bpf/verifier.c: 2501
	/* big hammer: mark all scalars precise in this path. /Users/rubber/linux/kernel/bpf/verifier.c: 2510
	 * pop_stack may still get !precise scalars. /Users/rubber/linux/kernel/bpf/verifier.c: 2511
				/* Found assignment(s) into tracked register in this state. /Users/rubber/linux/kernel/bpf/verifier.c: 2606
				 * Since this state is already marked, just return. /Users/rubber/linux/kernel/bpf/verifier.c: 2607
				 * Nothing to be tracked further in the parent state. /Users/rubber/linux/kernel/bpf/verifier.c: 2608
				/* This can happen if backtracking reached insn 0 /Users/rubber/linux/kernel/bpf/verifier.c: 2615
				 * and there are still reg_mask or stack_mask /Users/rubber/linux/kernel/bpf/verifier.c: 2616
				 * to backtrack. /Users/rubber/linux/kernel/bpf/verifier.c: 2617
				 * It means the backtracking missed the spot where /Users/rubber/linux/kernel/bpf/verifier.c: 2618
				 * particular register was initialized with a constant. /Users/rubber/linux/kernel/bpf/verifier.c: 2619
				/* the sequence of instructions: /Users/rubber/linux/kernel/bpf/verifier.c: 2647
				 * 2: (bf) r3 = r10 /Users/rubber/linux/kernel/bpf/verifier.c: 2648
				 * 3: (7b) *(u64 *)(r3 -8) = r0 /Users/rubber/linux/kernel/bpf/verifier.c: 2649
				 * 4: (79) r4 = *(u64 *)(r10 -8) /Users/rubber/linux/kernel/bpf/verifier.c: 2650
				 * doesn't contain jmps. It's backtracked /Users/rubber/linux/kernel/bpf/verifier.c: 2651
				 * as a single block. /Users/rubber/linux/kernel/bpf/verifier.c: 2652
				 * During backtracking insn 3 is not recognized as /Users/rubber/linux/kernel/bpf/verifier.c: 2653
				 * stack access, so at the end of backtracking /Users/rubber/linux/kernel/bpf/verifier.c: 2654
				 * stack slot fp-8 is still marked in stack_mask. /Users/rubber/linux/kernel/bpf/verifier.c: 2655
				 * However the parent state may not have accessed /Users/rubber/linux/kernel/bpf/verifier.c: 2656
				 * fp-8 and it's "unallocated" stack space. /Users/rubber/linux/kernel/bpf/verifier.c: 2657
				 * In such case fallback to conservative. /Users/rubber/linux/kernel/bpf/verifier.c: 2658
/* check_stack_{read,write}_fixed_off functions track spill/fill of registers, /Users/rubber/linux/kernel/bpf/verifier.c: 2793
 * stack boundary and alignment are checked in check_mem_access() /Users/rubber/linux/kernel/bpf/verifier.c: 2794
	/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0, /Users/rubber/linux/kernel/bpf/verifier.c: 2810
	 * so it's aligned access and [off, off + size) are within stack limits /Users/rubber/linux/kernel/bpf/verifier.c: 2811
			/* The backtracking logic can only recognize explicit /Users/rubber/linux/kernel/bpf/verifier.c: 2840
			 * stack slot address like [fp - 8]. Other spill of /Users/rubber/linux/kernel/bpf/verifier.c: 2841
			 * scalar via different register has to be conservative. /Users/rubber/linux/kernel/bpf/verifier.c: 2842
			 * Backtrack from here and mark all registers as precise /Users/rubber/linux/kernel/bpf/verifier.c: 2843
			 * that contributed into 'reg' being a constant. /Users/rubber/linux/kernel/bpf/verifier.c: 2844
		/* only mark the slot as written if all 8 bytes were written /Users/rubber/linux/kernel/bpf/verifier.c: 2873
		 * otherwise read propagation may incorrectly stop too soon /Users/rubber/linux/kernel/bpf/verifier.c: 2874
		 * when stack slots are partially written. /Users/rubber/linux/kernel/bpf/verifier.c: 2875
		 * This heuristic means that read propagation will be /Users/rubber/linux/kernel/bpf/verifier.c: 2876
		 * conservative, since it will add reg_live_read marks /Users/rubber/linux/kernel/bpf/verifier.c: 2877
		 * to stack slots all the way to first state when programs /Users/rubber/linux/kernel/bpf/verifier.c: 2878
		 * writes+reads less than 8 bytes /Users/rubber/linux/kernel/bpf/verifier.c: 2879
/* Write the stack: 'stack[ptr_regno + off] = value_regno'. 'ptr_regno' is /Users/rubber/linux/kernel/bpf/verifier.c: 2901
 * known to contain a variable offset. /Users/rubber/linux/kernel/bpf/verifier.c: 2902
 * This function checks whether the write is permitted and conservatively /Users/rubber/linux/kernel/bpf/verifier.c: 2903
 * tracks the effects of the write, considering that each stack slot in the /Users/rubber/linux/kernel/bpf/verifier.c: 2904
 * dynamic range is potentially written to. /Users/rubber/linux/kernel/bpf/verifier.c: 2905
 * 'off' includes 'regno->off'. /Users/rubber/linux/kernel/bpf/verifier.c: 2907
 * 'value_regno' can be -1, meaning that an unknown value is being written to /Users/rubber/linux/kernel/bpf/verifier.c: 2908
 * the stack. /Users/rubber/linux/kernel/bpf/verifier.c: 2909
 * Spilled pointers in range are not marked as written because we don't know /Users/rubber/linux/kernel/bpf/verifier.c: 2911
 * what's going to be actually written. This means that read propagation for /Users/rubber/linux/kernel/bpf/verifier.c: 2912
 * future reads cannot be terminated by this write. /Users/rubber/linux/kernel/bpf/verifier.c: 2913
 * For privileged programs, uninitialized stack slots are considered /Users/rubber/linux/kernel/bpf/verifier.c: 2915
 * initialized by this write (even though we don't know exactly what offsets /Users/rubber/linux/kernel/bpf/verifier.c: 2916
 * are going to be written to). The idea is that we don't want the verifier to /Users/rubber/linux/kernel/bpf/verifier.c: 2917
 * reject future reads that access slots written to through variable offsets. /Users/rubber/linux/kernel/bpf/verifier.c: 2918
	/* set if the fact that we're writing a zero is used to let any /Users/rubber/linux/kernel/bpf/verifier.c: 2931
	 * stack slots remain STACK_ZERO /Users/rubber/linux/kernel/bpf/verifier.c: 2932
			/* Reject the write if there's are spilled pointers in /Users/rubber/linux/kernel/bpf/verifier.c: 2962
			 * range. If we didn't reject here, the ptr status /Users/rubber/linux/kernel/bpf/verifier.c: 2963
			 * would be erased below (even though not all slots are /Users/rubber/linux/kernel/bpf/verifier.c: 2964
			 * actually overwritten), possibly opening the door to /Users/rubber/linux/kernel/bpf/verifier.c: 2965
			 * leaks. /Users/rubber/linux/kernel/bpf/verifier.c: 2966
		/* If the slot is STACK_INVALID, we check whether it's OK to /Users/rubber/linux/kernel/bpf/verifier.c: 2982
		 * pretend that it will be initialized by this write. The slot /Users/rubber/linux/kernel/bpf/verifier.c: 2983
		 * might not actually be written to, and so if we mark it as /Users/rubber/linux/kernel/bpf/verifier.c: 2984
		 * initialized future reads might leak uninitialized memory. /Users/rubber/linux/kernel/bpf/verifier.c: 2985
		 * For privileged programs, we will accept such reads to slots /Users/rubber/linux/kernel/bpf/verifier.c: 2986
		 * that may or may not be written because, if we're reject /Users/rubber/linux/kernel/bpf/verifier.c: 2987
		 * them, the error would be too confusing. /Users/rubber/linux/kernel/bpf/verifier.c: 2988
/* When register 'dst_regno' is assigned some values from stack[min_off, /Users/rubber/linux/kernel/bpf/verifier.c: 3006
 * max_off), we set the register's type according to the types of the /Users/rubber/linux/kernel/bpf/verifier.c: 3007
 * respective stack slots. If all the stack values are known to be zeros, then /Users/rubber/linux/kernel/bpf/verifier.c: 3008
 * so is the destination reg. Otherwise, the register is considered to be /Users/rubber/linux/kernel/bpf/verifier.c: 3009
 * SCALAR. This function does not deal with register filling; the caller must /Users/rubber/linux/kernel/bpf/verifier.c: 3010
 * ensure that all spilled registers in the stack range have been marked as /Users/rubber/linux/kernel/bpf/verifier.c: 3011
 * read. /Users/rubber/linux/kernel/bpf/verifier.c: 3012
		/* any access_size read into register is zero extended, /Users/rubber/linux/kernel/bpf/verifier.c: 3034
		 * so the whole register == const_zero /Users/rubber/linux/kernel/bpf/verifier.c: 3035
		/* backtracking doesn't support STACK_ZERO yet, /Users/rubber/linux/kernel/bpf/verifier.c: 3038
		 * so mark it precise here, so that later /Users/rubber/linux/kernel/bpf/verifier.c: 3039
		 * backtracking can stop here. /Users/rubber/linux/kernel/bpf/verifier.c: 3040
		 * Backtracking may not need this if this register /Users/rubber/linux/kernel/bpf/verifier.c: 3041
		 * doesn't participate in pointer adjustment. /Users/rubber/linux/kernel/bpf/verifier.c: 3042
		 * Forward propagation of precise flag is not /Users/rubber/linux/kernel/bpf/verifier.c: 3043
		 * necessary either. This mark is only to stop /Users/rubber/linux/kernel/bpf/verifier.c: 3044
		 * backtracking. Any register that contributed /Users/rubber/linux/kernel/bpf/verifier.c: 3045
		 * to const 0 was marked precise before spill. /Users/rubber/linux/kernel/bpf/verifier.c: 3046
/* Read the stack at 'off' and put the results into the register indicated by /Users/rubber/linux/kernel/bpf/verifier.c: 3056
 * 'dst_regno'. It handles reg filling if the addressed stack slot is a /Users/rubber/linux/kernel/bpf/verifier.c: 3057
 * spilled reg. /Users/rubber/linux/kernel/bpf/verifier.c: 3058
 * 'dst_regno' can be -1, meaning that the read value is not going to a /Users/rubber/linux/kernel/bpf/verifier.c: 3060
 * register. /Users/rubber/linux/kernel/bpf/verifier.c: 3061
 * The access is assumed to be within the current stack bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 3063
				/* The earlier check_reg_arg() has decided the /Users/rubber/linux/kernel/bpf/verifier.c: 3097
				 * subreg_def for this insn.  Save it first. /Users/rubber/linux/kernel/bpf/verifier.c: 3098
			/* mark reg as written since spilled pointer state likely /Users/rubber/linux/kernel/bpf/verifier.c: 3124
			 * has its liveness marks cleared by is_state_visited() /Users/rubber/linux/kernel/bpf/verifier.c: 3125
			 * which resets stack/reg liveness for state transitions /Users/rubber/linux/kernel/bpf/verifier.c: 3126
			/* If dst_regno==-1, the caller is asking us whether /Users/rubber/linux/kernel/bpf/verifier.c: 3130
			 * it is acceptable to use this value as a SCALAR_VALUE /Users/rubber/linux/kernel/bpf/verifier.c: 3131
			 * (e.g. for XADD). /Users/rubber/linux/kernel/bpf/verifier.c: 3132
			 * We must not allow unprivileged callers to do that /Users/rubber/linux/kernel/bpf/verifier.c: 3133
			 * with spilled pointers. /Users/rubber/linux/kernel/bpf/verifier.c: 3134
/* Read the stack at 'ptr_regno + off' and put the result into the register /Users/rubber/linux/kernel/bpf/verifier.c: 3175
 * 'dst_regno'. /Users/rubber/linux/kernel/bpf/verifier.c: 3176
 * 'off' includes the pointer register's fixed offset(i.e. 'ptr_regno.off'), /Users/rubber/linux/kernel/bpf/verifier.c: 3177
 * but not its variable offset. /Users/rubber/linux/kernel/bpf/verifier.c: 3178
 * 'size' is assumed to be <= reg size and the access is assumed to be aligned. /Users/rubber/linux/kernel/bpf/verifier.c: 3179
 * As opposed to check_stack_read_fixed_off, this function doesn't deal with /Users/rubber/linux/kernel/bpf/verifier.c: 3181
 * filling registers (i.e. reads of spilled register cannot be detected when /Users/rubber/linux/kernel/bpf/verifier.c: 3182
 * the offset is not fixed). We conservatively mark 'dst_regno' as containing /Users/rubber/linux/kernel/bpf/verifier.c: 3183
 * SCALAR_VALUE. That's why we assert that the 'ptr_regno' has a variable /Users/rubber/linux/kernel/bpf/verifier.c: 3184
 * offset; for a fixed offset check_stack_read_fixed_off should be used /Users/rubber/linux/kernel/bpf/verifier.c: 3185
 * instead. /Users/rubber/linux/kernel/bpf/verifier.c: 3186
	/* Note that we pass a NULL meta, so raw access will not be permitted. /Users/rubber/linux/kernel/bpf/verifier.c: 3197
/* check_stack_read dispatches to check_stack_read_fixed_off or /Users/rubber/linux/kernel/bpf/verifier.c: 3210
 * check_stack_read_var_off. /Users/rubber/linux/kernel/bpf/verifier.c: 3211
 * The caller must ensure that the offset falls within the allocated stack /Users/rubber/linux/kernel/bpf/verifier.c: 3213
 * bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 3214
 * 'dst_regno' is a register which will receive the value from the stack. It /Users/rubber/linux/kernel/bpf/verifier.c: 3216
 * can be -1, meaning that the read value is not going to a register. /Users/rubber/linux/kernel/bpf/verifier.c: 3217
	/* The offset is required to be static when reads don't go to a /Users/rubber/linux/kernel/bpf/verifier.c: 3229
	 * register, in order to not leak pointers (see /Users/rubber/linux/kernel/bpf/verifier.c: 3230
	 * check_stack_read_fixed_off). /Users/rubber/linux/kernel/bpf/verifier.c: 3231
	/* Variable offset is prohibited for unprivileged mode for simplicity /Users/rubber/linux/kernel/bpf/verifier.c: 3241
	 * since it requires corresponding support in Spectre masking for stack /Users/rubber/linux/kernel/bpf/verifier.c: 3242
	 * ALU. See also retrieve_ptr_limit(). /Users/rubber/linux/kernel/bpf/verifier.c: 3243
		/* Variable offset stack reads need more conservative handling /Users/rubber/linux/kernel/bpf/verifier.c: 3259
		 * than fixed offset ones. Note that dst_regno >= 0 on this /Users/rubber/linux/kernel/bpf/verifier.c: 3260
		 * branch. /Users/rubber/linux/kernel/bpf/verifier.c: 3261
/* check_stack_write dispatches to check_stack_write_fixed_off or /Users/rubber/linux/kernel/bpf/verifier.c: 3270
 * check_stack_write_var_off. /Users/rubber/linux/kernel/bpf/verifier.c: 3271
 * 'ptr_regno' is the register used as a pointer into the stack. /Users/rubber/linux/kernel/bpf/verifier.c: 3273
 * 'off' includes 'ptr_regno->off', but not its variable offset (if any). /Users/rubber/linux/kernel/bpf/verifier.c: 3274
 * 'value_regno' is the register whose value we're writing to the stack. It can /Users/rubber/linux/kernel/bpf/verifier.c: 3275
 * be -1, meaning that we're not writing from a register. /Users/rubber/linux/kernel/bpf/verifier.c: 3276
 * The caller must ensure that the offset falls within the maximum stack size. /Users/rubber/linux/kernel/bpf/verifier.c: 3278
		/* Variable offset stack reads need more conservative handling /Users/rubber/linux/kernel/bpf/verifier.c: 3293
		 * than fixed offset ones. /Users/rubber/linux/kernel/bpf/verifier.c: 3294
	/* We may have adjusted the register pointing to memory region, so we /Users/rubber/linux/kernel/bpf/verifier.c: 3371
	 * need to try adding each of min_value and max_value to off /Users/rubber/linux/kernel/bpf/verifier.c: 3372
	 * to make sure our theoretical access will be safe. /Users/rubber/linux/kernel/bpf/verifier.c: 3373
	/* The minimum value is only important with signed /Users/rubber/linux/kernel/bpf/verifier.c: 3378
	 * comparisons where we can't assume the floor of a /Users/rubber/linux/kernel/bpf/verifier.c: 3379
	 * value is 0.  If we are using signed variables for our /Users/rubber/linux/kernel/bpf/verifier.c: 3380
	 * index'es we need to make sure that whatever we use /Users/rubber/linux/kernel/bpf/verifier.c: 3381
	 * will have a set floor within our range. /Users/rubber/linux/kernel/bpf/verifier.c: 3382
	/* If we haven't set a max value then we need to bail since we can't be /Users/rubber/linux/kernel/bpf/verifier.c: 3400
	 * sure we won't do bad things. /Users/rubber/linux/kernel/bpf/verifier.c: 3401
	 * If reg->umax_value + off could overflow, treat that as unbounded too. /Users/rubber/linux/kernel/bpf/verifier.c: 3402
		/* if any part of struct bpf_spin_lock can be touched by /Users/rubber/linux/kernel/bpf/verifier.c: 3438
		 * load/store reject this program. /Users/rubber/linux/kernel/bpf/verifier.c: 3439
		 * To check that [x1, x2) overlaps with [y1, y2) /Users/rubber/linux/kernel/bpf/verifier.c: 3440
		 * it is sufficient to check x1 < y2 && y1 < x2. /Users/rubber/linux/kernel/bpf/verifier.c: 3441
	/* We may have added a variable offset to the packet pointer; but any /Users/rubber/linux/kernel/bpf/verifier.c: 3517
	 * reg->range we have comes after that.  We are only checking the fixed /Users/rubber/linux/kernel/bpf/verifier.c: 3518
	 * offset. /Users/rubber/linux/kernel/bpf/verifier.c: 3519
	/* We don't allow negative numbers, because we aren't tracking enough /Users/rubber/linux/kernel/bpf/verifier.c: 3522
	 * detail to prove they're safe. /Users/rubber/linux/kernel/bpf/verifier.c: 3523
	/* __check_mem_access has made sure "off + size - 1" is within u16. /Users/rubber/linux/kernel/bpf/verifier.c: 3539
	 * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff, /Users/rubber/linux/kernel/bpf/verifier.c: 3540
	 * otherwise find_good_pkt_pointers would have refused to set range info /Users/rubber/linux/kernel/bpf/verifier.c: 3541
	 * that __check_mem_access would have rejected this pkt access. /Users/rubber/linux/kernel/bpf/verifier.c: 3542
	 * Therefore, "off + reg->umax_value + size - 1" won't overflow u32. /Users/rubber/linux/kernel/bpf/verifier.c: 3543
		/* A non zero info.ctx_field_size indicates that this field is a /Users/rubber/linux/kernel/bpf/verifier.c: 3564
		 * candidate for later verifier transformation to load the whole /Users/rubber/linux/kernel/bpf/verifier.c: 3565
		 * field and then apply a mask when accessed with a narrower /Users/rubber/linux/kernel/bpf/verifier.c: 3566
		 * access than actual ctx access size. A zero info.ctx_field_size /Users/rubber/linux/kernel/bpf/verifier.c: 3567
		 * will only allow for whole field access and rejects any other /Users/rubber/linux/kernel/bpf/verifier.c: 3568
		 * type of narrower access. /Users/rubber/linux/kernel/bpf/verifier.c: 3569
	/* For platforms that do not have a Kconfig enabling /Users/rubber/linux/kernel/bpf/verifier.c: 3691
	 * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of /Users/rubber/linux/kernel/bpf/verifier.c: 3692
	 * NET_IP_ALIGN is universally set to '2'.  And on platforms /Users/rubber/linux/kernel/bpf/verifier.c: 3693
	 * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get /Users/rubber/linux/kernel/bpf/verifier.c: 3694
	 * to this code only in strict mode where we want to emulate /Users/rubber/linux/kernel/bpf/verifier.c: 3695
	 * the NET_IP_ALIGN==2 checking.  Therefore use an /Users/rubber/linux/kernel/bpf/verifier.c: 3696
	 * unconditional IP align value of '2'. /Users/rubber/linux/kernel/bpf/verifier.c: 3697
		/* Special case, because of NET_IP_ALIGN. Given metadata sits /Users/rubber/linux/kernel/bpf/verifier.c: 3749
		 * right in front, treat it the very same way. /Users/rubber/linux/kernel/bpf/verifier.c: 3750
		/* The stack spill tracking logic in check_stack_write_fixed_off() /Users/rubber/linux/kernel/bpf/verifier.c: 3767
		 * and check_stack_read_fixed_off() relies on stack accesses being /Users/rubber/linux/kernel/bpf/verifier.c: 3768
		 * aligned. /Users/rubber/linux/kernel/bpf/verifier.c: 3769
/* starting from main bpf function walk all instructions of the function /Users/rubber/linux/kernel/bpf/verifier.c: 3806
 * and recursively walk all callees that given function can call. /Users/rubber/linux/kernel/bpf/verifier.c: 3807
 * Ignore jump and exit insns. /Users/rubber/linux/kernel/bpf/verifier.c: 3808
 * Since recursion is prevented by check_cfg() this algorithm /Users/rubber/linux/kernel/bpf/verifier.c: 3809
 * only needs a local stack of MAX_CALL_FRAMES to remember callsites /Users/rubber/linux/kernel/bpf/verifier.c: 3810
	/* protect against potential stack overflow that might happen when /Users/rubber/linux/kernel/bpf/verifier.c: 3823
	 * bpf2bpf calls get combined with tailcalls. Limit the caller's stack /Users/rubber/linux/kernel/bpf/verifier.c: 3824
	 * depth for such case down to 256 so that the worst case scenario /Users/rubber/linux/kernel/bpf/verifier.c: 3825
	 * would result in 8k stack size (32 which is tailcall limit * 256 = /Users/rubber/linux/kernel/bpf/verifier.c: 3826
	 * 8k). /Users/rubber/linux/kernel/bpf/verifier.c: 3827
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 3828
	 * To get the idea what might happen, see an example: /Users/rubber/linux/kernel/bpf/verifier.c: 3829
	 * func1 -> sub rsp, 128 /Users/rubber/linux/kernel/bpf/verifier.c: 3830
	 *  subfunc1 -> sub rsp, 256 /Users/rubber/linux/kernel/bpf/verifier.c: 3831
	 *  tailcall1 -> add rsp, 256 /Users/rubber/linux/kernel/bpf/verifier.c: 3832
	 *   func2 -> sub rsp, 192 (total stack size = 128 + 192 = 320) /Users/rubber/linux/kernel/bpf/verifier.c: 3833
	 *   subfunc2 -> sub rsp, 64 /Users/rubber/linux/kernel/bpf/verifier.c: 3834
	 *   subfunc22 -> sub rsp, 128 /Users/rubber/linux/kernel/bpf/verifier.c: 3835
	 *   tailcall2 -> add rsp, 128 /Users/rubber/linux/kernel/bpf/verifier.c: 3836
	 *    func3 -> sub rsp, 32 (total stack size 128 + 192 + 64 + 32 = 416) /Users/rubber/linux/kernel/bpf/verifier.c: 3837
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 3838
	 * tailcall will unwind the current stack frame but it will not get rid /Users/rubber/linux/kernel/bpf/verifier.c: 3839
	 * of caller's stack as shown on the example above. /Users/rubber/linux/kernel/bpf/verifier.c: 3840
	/* round up to 32-bytes, since this is granularity /Users/rubber/linux/kernel/bpf/verifier.c: 3848
	 * of interpreter stack size /Users/rubber/linux/kernel/bpf/verifier.c: 3849
	/* if tail call got detected across bpf2bpf calls then mark each of the /Users/rubber/linux/kernel/bpf/verifier.c: 3897
	 * currently present subprog frames as tail call reachable subprogs; /Users/rubber/linux/kernel/bpf/verifier.c: 3898
	 * this info will be utilized by JIT so that we will be preserving the /Users/rubber/linux/kernel/bpf/verifier.c: 3899
	 * tail call counter throughout bpf2bpf calls combined with tailcalls /Users/rubber/linux/kernel/bpf/verifier.c: 3900
	/* end of for() loop means the last insn of the 'subprog' /Users/rubber/linux/kernel/bpf/verifier.c: 3908
	 * was reached. Doesn't matter whether it was JA or EXIT /Users/rubber/linux/kernel/bpf/verifier.c: 3909
	/* Access to ctx or passing it to a helper is only allowed in /Users/rubber/linux/kernel/bpf/verifier.c: 3939
	 * its original, unmodified form. /Users/rubber/linux/kernel/bpf/verifier.c: 3940
/* truncate register to smaller size (in bytes) /Users/rubber/linux/kernel/bpf/verifier.c: 4026
 * must be called with size < BPF_REG_SIZE /Users/rubber/linux/kernel/bpf/verifier.c: 4027
	/* If size is smaller than 32bit register the 32bit register /Users/rubber/linux/kernel/bpf/verifier.c: 4048
	 * values are also truncated so we push 64-bit bounds into /Users/rubber/linux/kernel/bpf/verifier.c: 4049
	 * 32-bit bounds. Above were truncated < 32-bits already. /Users/rubber/linux/kernel/bpf/verifier.c: 4050
	/* A map is considered read-only if the following condition are true: /Users/rubber/linux/kernel/bpf/verifier.c: 4059
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 4060
	 * 1) BPF program side cannot change any of the map content. The /Users/rubber/linux/kernel/bpf/verifier.c: 4061
	 *    BPF_F_RDONLY_PROG flag is throughout the lifetime of a map /Users/rubber/linux/kernel/bpf/verifier.c: 4062
	 *    and was set at map creation time. /Users/rubber/linux/kernel/bpf/verifier.c: 4063
	 * 2) The map value(s) have been initialized from user space by a /Users/rubber/linux/kernel/bpf/verifier.c: 4064
	 *    loader and then "frozen", such that no new map update/delete /Users/rubber/linux/kernel/bpf/verifier.c: 4065
	 *    operations from syscall side are possible for the rest of /Users/rubber/linux/kernel/bpf/verifier.c: 4066
	 *    the map's lifetime from that point onwards. /Users/rubber/linux/kernel/bpf/verifier.c: 4067
	 * 3) Any parallel/pending map update/delete operations from syscall /Users/rubber/linux/kernel/bpf/verifier.c: 4068
	 *    side have been completed. Only after that point, it's safe to /Users/rubber/linux/kernel/bpf/verifier.c: 4069
	 *    assume that map value(s) are immutable. /Users/rubber/linux/kernel/bpf/verifier.c: 4070
/* Check that the stack access at the given offset is within bounds. The /Users/rubber/linux/kernel/bpf/verifier.c: 4212
 * maximum valid offset is -1. /Users/rubber/linux/kernel/bpf/verifier.c: 4213
 * The minimum valid offset is -MAX_BPF_STACK for writes, and /Users/rubber/linux/kernel/bpf/verifier.c: 4215
 * -state->allocated_stack for reads. /Users/rubber/linux/kernel/bpf/verifier.c: 4216
/* Check that the stack access at 'regno + off' falls within the maximum stack /Users/rubber/linux/kernel/bpf/verifier.c: 4234
 * bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 4235
 * 'off' includes `regno->offset`, but not its dynamic part (if any). /Users/rubber/linux/kernel/bpf/verifier.c: 4237
/* check whether memory at (regno + off) is accessible for t = (read | write) /Users/rubber/linux/kernel/bpf/verifier.c: 4298
 * if t==write, value_regno is a register which value is stored into memory /Users/rubber/linux/kernel/bpf/verifier.c: 4299
 * if t==read, value_regno is a register which will receive the value from memory /Users/rubber/linux/kernel/bpf/verifier.c: 4300
 * if t==write && value_regno==-1, some unknown value is stored into memory /Users/rubber/linux/kernel/bpf/verifier.c: 4301
 * if t==read && value_regno==-1, don't care what we read from memory /Users/rubber/linux/kernel/bpf/verifier.c: 4302
			/* ctx access returns either a scalar, or a /Users/rubber/linux/kernel/bpf/verifier.c: 4397
			 * PTR_TO_PACKET[_META,_END]. In the latter /Users/rubber/linux/kernel/bpf/verifier.c: 4398
			 * case, we know the offset is zero. /Users/rubber/linux/kernel/bpf/verifier.c: 4399
				/* A load of ctx field could have different /Users/rubber/linux/kernel/bpf/verifier.c: 4408
				 * actual load size with the one encoded in the /Users/rubber/linux/kernel/bpf/verifier.c: 4409
				 * insn. When the dst is PTR, it is for sure not /Users/rubber/linux/kernel/bpf/verifier.c: 4410
				 * a sub-register. /Users/rubber/linux/kernel/bpf/verifier.c: 4411
		/* This instruction accesses a memory location but doesn't /Users/rubber/linux/kernel/bpf/verifier.c: 4585
		 * actually load it into a register. /Users/rubber/linux/kernel/bpf/verifier.c: 4586
/* When register 'regno' is used to read the stack (either directly or through /Users/rubber/linux/kernel/bpf/verifier.c: 4606
 * a helper function) make sure that it's within stack boundary and, depending /Users/rubber/linux/kernel/bpf/verifier.c: 4607
 * on the access type, that all elements of the stack are initialized. /Users/rubber/linux/kernel/bpf/verifier.c: 4608
 * 'off' includes 'regno->off', but not its dynamic part (if any). /Users/rubber/linux/kernel/bpf/verifier.c: 4610
 * All registers that have been spilled on the stack in the slots within the /Users/rubber/linux/kernel/bpf/verifier.c: 4612
 * read offsets are marked as read. /Users/rubber/linux/kernel/bpf/verifier.c: 4613
	/* Some accesses can write anything into the stack, others are /Users/rubber/linux/kernel/bpf/verifier.c: 4625
	 * read-only. /Users/rubber/linux/kernel/bpf/verifier.c: 4626
		/* The bounds checks for writes are more permissive than for /Users/rubber/linux/kernel/bpf/verifier.c: 4636
		 * reads. However, if raw_mode is not set, we'll do extra /Users/rubber/linux/kernel/bpf/verifier.c: 4637
		 * checks below. /Users/rubber/linux/kernel/bpf/verifier.c: 4638
		/* Variable offset is prohibited for unprivileged mode for /Users/rubber/linux/kernel/bpf/verifier.c: 4654
		 * simplicity since it requires corresponding support in /Users/rubber/linux/kernel/bpf/verifier.c: 4655
		 * Spectre masking for stack ALU. /Users/rubber/linux/kernel/bpf/verifier.c: 4656
		 * See also retrieve_ptr_limit(). /Users/rubber/linux/kernel/bpf/verifier.c: 4657
		/* Only initialized buffer on stack is allowed to be accessed /Users/rubber/linux/kernel/bpf/verifier.c: 4667
		 * with variable offset. With uninitialized buffer it's hard to /Users/rubber/linux/kernel/bpf/verifier.c: 4668
		 * guarantee that whole memory is marked as initialized on /Users/rubber/linux/kernel/bpf/verifier.c: 4669
		 * helper return since specific bounds are unknown what may /Users/rubber/linux/kernel/bpf/verifier.c: 4670
		 * cause uninitialized stack leaking. /Users/rubber/linux/kernel/bpf/verifier.c: 4671
		/* reading any byte out of 8-byte 'spill_slot' will cause /Users/rubber/linux/kernel/bpf/verifier.c: 4732
		 * the whole slot to be marked as 'read' /Users/rubber/linux/kernel/bpf/verifier.c: 4733
		/* Assuming that the register contains a value check if the memory /Users/rubber/linux/kernel/bpf/verifier.c: 4804
		 * access is safe. Temporarily save and restore the register's state as /Users/rubber/linux/kernel/bpf/verifier.c: 4805
		 * the conversion shouldn't be visible to a caller. /Users/rubber/linux/kernel/bpf/verifier.c: 4806
/* Implementation details: /Users/rubber/linux/kernel/bpf/verifier.c: 4820
 * bpf_map_lookup returns PTR_TO_MAP_VALUE_OR_NULL /Users/rubber/linux/kernel/bpf/verifier.c: 4821
 * Two bpf_map_lookups (even with the same key) will have different reg->id. /Users/rubber/linux/kernel/bpf/verifier.c: 4822
 * For traditional PTR_TO_MAP_VALUE the verifier clears reg->id after /Users/rubber/linux/kernel/bpf/verifier.c: 4823
 * value_or_null->value transition, since the verifier only cares about /Users/rubber/linux/kernel/bpf/verifier.c: 4824
 * the range of access to valid map value pointer and doesn't care about actual /Users/rubber/linux/kernel/bpf/verifier.c: 4825
 * address of the map element. /Users/rubber/linux/kernel/bpf/verifier.c: 4826
 * For maps with 'struct bpf_spin_lock' inside map value the verifier keeps /Users/rubber/linux/kernel/bpf/verifier.c: 4827
 * reg->id > 0 after value_or_null->value transition. By doing so /Users/rubber/linux/kernel/bpf/verifier.c: 4828
 * two bpf_map_lookups will be considered two different pointers that /Users/rubber/linux/kernel/bpf/verifier.c: 4829
 * point to different bpf_spin_locks. /Users/rubber/linux/kernel/bpf/verifier.c: 4830
 * The verifier allows taking only one bpf_spin_lock at a time to avoid /Users/rubber/linux/kernel/bpf/verifier.c: 4831
 * dead-locks. /Users/rubber/linux/kernel/bpf/verifier.c: 4832
 * Since only one bpf_spin_lock is allowed the checks are simpler than /Users/rubber/linux/kernel/bpf/verifier.c: 4833
 * reg_is_refcounted() logic. The verifier needs to remember only /Users/rubber/linux/kernel/bpf/verifier.c: 4834
 * one spin_lock instead of array of acquired_refs. /Users/rubber/linux/kernel/bpf/verifier.c: 4835
 * cur_state->active_spin_lock remembers which map value element got locked /Users/rubber/linux/kernel/bpf/verifier.c: 4836
 * and clears it after bpf_spin_unlock. /Users/rubber/linux/kernel/bpf/verifier.c: 4837
		/* A NULL register has a SCALAR_VALUE type, so skip /Users/rubber/linux/kernel/bpf/verifier.c: 5218
		 * type checking. /Users/rubber/linux/kernel/bpf/verifier.c: 5219
			/* Use map_uid (which is unique id of inner map) to reject: /Users/rubber/linux/kernel/bpf/verifier.c: 5247
			 * inner_map1 = bpf_map_lookup_elem(outer_map, key1) /Users/rubber/linux/kernel/bpf/verifier.c: 5248
			 * inner_map2 = bpf_map_lookup_elem(outer_map, key2) /Users/rubber/linux/kernel/bpf/verifier.c: 5249
			 * if (inner_map1 && inner_map2) { /Users/rubber/linux/kernel/bpf/verifier.c: 5250
			 *     timer = bpf_map_lookup_elem(inner_map1); /Users/rubber/linux/kernel/bpf/verifier.c: 5251
			 *     if (timer) /Users/rubber/linux/kernel/bpf/verifier.c: 5252
			 *         // mismatch would have been allowed /Users/rubber/linux/kernel/bpf/verifier.c: 5253
			 *         bpf_timer_init(timer, inner_map2); /Users/rubber/linux/kernel/bpf/verifier.c: 5254
			 * } /Users/rubber/linux/kernel/bpf/verifier.c: 5255
			 * /Users/rubber/linux/kernel/bpf/verifier.c: 5256
			 * Comparing map_ptr is enough to distinguish normal and outer maps. /Users/rubber/linux/kernel/bpf/verifier.c: 5257
		/* bpf_map_xxx(..., map_ptr, ..., key) call: /Users/rubber/linux/kernel/bpf/verifier.c: 5270
		 * check that [key, key + map->key_size) are within /Users/rubber/linux/kernel/bpf/verifier.c: 5271
		 * stack limits and initialized /Users/rubber/linux/kernel/bpf/verifier.c: 5272
			/* in function declaration map_ptr must come before /Users/rubber/linux/kernel/bpf/verifier.c: 5275
			 * map_key, so that it's verified and known before /Users/rubber/linux/kernel/bpf/verifier.c: 5276
			 * we have to check map_key here. Otherwise it means /Users/rubber/linux/kernel/bpf/verifier.c: 5277
			 * that kernel subsystem misconfigured verifier /Users/rubber/linux/kernel/bpf/verifier.c: 5278
		/* bpf_map_xxx(..., map_ptr, ..., value) call: /Users/rubber/linux/kernel/bpf/verifier.c: 5290
		 * check [value, value + map->value_size) validity /Users/rubber/linux/kernel/bpf/verifier.c: 5291
		/* The access to this pointer is only checked when we hit the /Users/rubber/linux/kernel/bpf/verifier.c: 5326
		 * next is_mem_size argument below. /Users/rubber/linux/kernel/bpf/verifier.c: 5327
		/* This is used to refine r0 return value bounds for helpers /Users/rubber/linux/kernel/bpf/verifier.c: 5333
		 * that enforce this value as an upper bound on return values. /Users/rubber/linux/kernel/bpf/verifier.c: 5334
		 * See do_refine_retval_range() for helpers that can refine /Users/rubber/linux/kernel/bpf/verifier.c: 5335
		 * the return value. C type of helper is u32 so we pull register /Users/rubber/linux/kernel/bpf/verifier.c: 5336
		 * bound from umax_value however, if negative verifier errors /Users/rubber/linux/kernel/bpf/verifier.c: 5337
		 * out. Only upper bounds can be learned because retval is an /Users/rubber/linux/kernel/bpf/verifier.c: 5338
		 * int type and negative retvals are allowed. /Users/rubber/linux/kernel/bpf/verifier.c: 5339
		/* The register is SCALAR_VALUE; the access check /Users/rubber/linux/kernel/bpf/verifier.c: 5343
		 * happens using its boundaries. /Users/rubber/linux/kernel/bpf/verifier.c: 5344
			/* For unprivileged variable accesses, disable raw /Users/rubber/linux/kernel/bpf/verifier.c: 5347
			 * mode so that the program is required to /Users/rubber/linux/kernel/bpf/verifier.c: 5348
			 * initialize all the memory that the helper could /Users/rubber/linux/kernel/bpf/verifier.c: 5349
			 * just partially fill up. /Users/rubber/linux/kernel/bpf/verifier.c: 5350
	/* It's not possible to get access to a locked struct sock in these /Users/rubber/linux/kernel/bpf/verifier.c: 5443
	 * contexts, so updating is safe. /Users/rubber/linux/kernel/bpf/verifier.c: 5444
	/* Restrict bpf side of cpumap and xskmap, open when use-cases /Users/rubber/linux/kernel/bpf/verifier.c: 5518
	 * appear. /Users/rubber/linux/kernel/bpf/verifier.c: 5519
	/* We only support one arg being in raw mode at the moment, /Users/rubber/linux/kernel/bpf/verifier.c: 5706
	 * which is sufficient for the helper functions we have /Users/rubber/linux/kernel/bpf/verifier.c: 5707
	 * right now. /Users/rubber/linux/kernel/bpf/verifier.c: 5708
	/* bpf_xxx(..., buf, len) call will access 'len' /Users/rubber/linux/kernel/bpf/verifier.c: 5724
	 * bytes from memory 'buf'. Both arg types need /Users/rubber/linux/kernel/bpf/verifier.c: 5725
	 * to be paired, so make sure there's no buggy /Users/rubber/linux/kernel/bpf/verifier.c: 5726
	 * helper function specification. /Users/rubber/linux/kernel/bpf/verifier.c: 5727
	/* A reference acquiring function cannot acquire /Users/rubber/linux/kernel/bpf/verifier.c: 5755
	 * another refcounted ptr. /Users/rubber/linux/kernel/bpf/verifier.c: 5756
	/* We only support one arg being unreferenced at the moment, /Users/rubber/linux/kernel/bpf/verifier.c: 5761
	 * which is sufficient for the helper functions we have right now. /Users/rubber/linux/kernel/bpf/verifier.c: 5762
/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END] /Users/rubber/linux/kernel/bpf/verifier.c: 5790
 * are now invalid, so turn them into unknown SCALAR_VALUE. /Users/rubber/linux/kernel/bpf/verifier.c: 5791
	/* The 'reg' is pkt > pkt_end or pkt >= pkt_end. /Users/rubber/linux/kernel/bpf/verifier.c: 5834
	 * How far beyond pkt_end it goes is unknown. /Users/rubber/linux/kernel/bpf/verifier.c: 5835
	 * if (!range_open) it's the case of pkt >= pkt_end /Users/rubber/linux/kernel/bpf/verifier.c: 5836
	 * if (range_open) it's the case of pkt > pkt_end /Users/rubber/linux/kernel/bpf/verifier.c: 5837
	 * hence this pointer is at least 1 byte bigger than pkt_end /Users/rubber/linux/kernel/bpf/verifier.c: 5838
/* The pointer with the specified id has released its reference to kernel /Users/rubber/linux/kernel/bpf/verifier.c: 5865
 * resources. Identify all copies of the same pointer and clear the reference. /Users/rubber/linux/kernel/bpf/verifier.c: 5866
	/* callee cannot access r0, r6 - r9 for reading and has to write /Users/rubber/linux/kernel/bpf/verifier.c: 5982
	 * into its own stack before reading from it. /Users/rubber/linux/kernel/bpf/verifier.c: 5983
	 * callee can read/write into caller's stack /Users/rubber/linux/kernel/bpf/verifier.c: 5984
 callsite */, /Users/rubber/linux/kernel/bpf/verifier.c: 5988
 frameno within this callchain */, /Users/rubber/linux/kernel/bpf/verifier.c: 5989
 subprog number within this prog */); /Users/rubber/linux/kernel/bpf/verifier.c: 5990
	/* bpf_for_each_map_elem(struct bpf_map *map, void *callback_fn, /Users/rubber/linux/kernel/bpf/verifier.c: 6022
	 *      void *callback_ctx, u64 flags); /Users/rubber/linux/kernel/bpf/verifier.c: 6023
	 * callback_fn(struct bpf_map *map, void *key, void *value, /Users/rubber/linux/kernel/bpf/verifier.c: 6024
	 *      void *callback_ctx); /Users/rubber/linux/kernel/bpf/verifier.c: 6025
	/* copy r1 - r5 args that callee can access.  The copy includes parent /Users/rubber/linux/kernel/bpf/verifier.c: 6051
	 * pointers, which connects us up to the liveness chain /Users/rubber/linux/kernel/bpf/verifier.c: 6052
	/* bpf_timer_set_callback(struct bpf_timer *timer, void *callback_fn); /Users/rubber/linux/kernel/bpf/verifier.c: 6111
	 * callback_fn(struct bpf_map *map, void *key, void *value); /Users/rubber/linux/kernel/bpf/verifier.c: 6112
		/* technically it's ok to return caller's stack pointer /Users/rubber/linux/kernel/bpf/verifier.c: 6143
		 * (or caller's caller's pointer) back to the caller, /Users/rubber/linux/kernel/bpf/verifier.c: 6144
		 * since these pointers are valid. Only current stack /Users/rubber/linux/kernel/bpf/verifier.c: 6145
		 * pointer will be invalid as soon as function exits, /Users/rubber/linux/kernel/bpf/verifier.c: 6146
		 * but let's be conservative /Users/rubber/linux/kernel/bpf/verifier.c: 6147
	/* In case of read-only, some additional restrictions /Users/rubber/linux/kernel/bpf/verifier.c: 6236
	 * need to be applied in order to prevent altering the /Users/rubber/linux/kernel/bpf/verifier.c: 6237
	 * state of the map from program side. /Users/rubber/linux/kernel/bpf/verifier.c: 6238
	/* fmt being ARG_PTR_TO_CONST_STR guarantees that var_off is const /Users/rubber/linux/kernel/bpf/verifier.c: 6324
	 * and map_direct_value_addr is set. /Users/rubber/linux/kernel/bpf/verifier.c: 6325
	/* We are also guaranteed that fmt+fmt_map_off is NULL terminated, we /Users/rubber/linux/kernel/bpf/verifier.c: 6336
	 * can focus on validating the format specifiers. /Users/rubber/linux/kernel/bpf/verifier.c: 6337
	/* Mark slots with STACK_MISC in case of raw mode, stack offset /Users/rubber/linux/kernel/bpf/verifier.c: 6440
	 * is inferred from register state. /Users/rubber/linux/kernel/bpf/verifier.c: 6441
	/* check that flags argument in get_local_storage(map, flags) is 0, /Users/rubber/linux/kernel/bpf/verifier.c: 6467
	 * this is required because get_local_storage() can't return an error. /Users/rubber/linux/kernel/bpf/verifier.c: 6468
		/* remember map_ptr, so that check_map_access() /Users/rubber/linux/kernel/bpf/verifier.c: 6515
		 * can check 'value_size' boundary of memory access /Users/rubber/linux/kernel/bpf/verifier.c: 6516
		 * to map element returned from bpf_map_lookup_elem() /Users/rubber/linux/kernel/bpf/verifier.c: 6517
		/* current BPF helper definitions are only coming from /Users/rubber/linux/kernel/bpf/verifier.c: 6590
		 * built-in code with type IDs from  vmlinux BTF /Users/rubber/linux/kernel/bpf/verifier.c: 6591
/* mark_btf_func_reg_size() is used when the reg size is determined by /Users/rubber/linux/kernel/bpf/verifier.c: 6658
 * the BTF func_proto's return value size and argument. /Users/rubber/linux/kernel/bpf/verifier.c: 6659
		/* Offset 0 is out-of-bounds, but acceptable start for the /Users/rubber/linux/kernel/bpf/verifier.c: 6855
		 * left direction, see BPF_REG_FP. Also, unknown scalar /Users/rubber/linux/kernel/bpf/verifier.c: 6856
		 * offset where we would need to deal with min/max bounds is /Users/rubber/linux/kernel/bpf/verifier.c: 6857
		 * currently prohibited for unprivileged. /Users/rubber/linux/kernel/bpf/verifier.c: 6858
	/* If we arrived here from different branches with different /Users/rubber/linux/kernel/bpf/verifier.c: 6888
	 * state or limits to sanitize, then this won't work. /Users/rubber/linux/kernel/bpf/verifier.c: 6889
	/* We already marked aux for masking from non-speculative /Users/rubber/linux/kernel/bpf/verifier.c: 6966
	 * paths, thus we got here in the first place. We only care /Users/rubber/linux/kernel/bpf/verifier.c: 6967
	 * to explore bad access from here. /Users/rubber/linux/kernel/bpf/verifier.c: 6968
		/* In commit phase we narrow the masking window based on /Users/rubber/linux/kernel/bpf/verifier.c: 6987
		 * the observed pointer move after the simulated operation. /Users/rubber/linux/kernel/bpf/verifier.c: 6988
		/* Limit pruning on unknown scalars to enable deep search for /Users/rubber/linux/kernel/bpf/verifier.c: 6998
		 * potential masking differences from other program paths. /Users/rubber/linux/kernel/bpf/verifier.c: 6999
	/* If we're in commit phase, we're done here given we already /Users/rubber/linux/kernel/bpf/verifier.c: 7009
	 * pushed the truncated dst_reg into the speculative verification /Users/rubber/linux/kernel/bpf/verifier.c: 7010
	 * stack. /Users/rubber/linux/kernel/bpf/verifier.c: 7011
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 7012
	 * Also, when register is a known constant, we rewrite register-based /Users/rubber/linux/kernel/bpf/verifier.c: 7013
	 * operation to immediate-based, and thus do not need masking (and as /Users/rubber/linux/kernel/bpf/verifier.c: 7014
	 * a consequence, do not need to simulate the zero-truncation either). /Users/rubber/linux/kernel/bpf/verifier.c: 7015
	/* Simulate and find potential out-of-bounds access under /Users/rubber/linux/kernel/bpf/verifier.c: 7020
	 * speculative execution from truncation as a result of /Users/rubber/linux/kernel/bpf/verifier.c: 7021
	 * masking when off was not within expected range. If off /Users/rubber/linux/kernel/bpf/verifier.c: 7022
	 * sits in dst, then we temporarily need to move ptr there /Users/rubber/linux/kernel/bpf/verifier.c: 7023
	 * to simulate dst (== 0) +/-= ptr. Needed, for example, /Users/rubber/linux/kernel/bpf/verifier.c: 7024
	 * for cases where we use K-based arithmetic in one direction /Users/rubber/linux/kernel/bpf/verifier.c: 7025
	 * and truncated reg-based in the other in order to explore /Users/rubber/linux/kernel/bpf/verifier.c: 7026
	 * bad access. /Users/rubber/linux/kernel/bpf/verifier.c: 7027
	/* If we simulate paths under speculation, we don't update the /Users/rubber/linux/kernel/bpf/verifier.c: 7044
	 * insn as 'seen' such that when we verify unreachable paths in /Users/rubber/linux/kernel/bpf/verifier.c: 7045
	 * the non-speculative domain, sanitize_dead_code() can still /Users/rubber/linux/kernel/bpf/verifier.c: 7046
	 * rewrite/sanitize them. /Users/rubber/linux/kernel/bpf/verifier.c: 7047
/* check that stack access falls within stack limits and that 'reg' doesn't /Users/rubber/linux/kernel/bpf/verifier.c: 7092
 * have a variable offset. /Users/rubber/linux/kernel/bpf/verifier.c: 7093
 * Variable offset is prohibited for unprivileged mode for simplicity since it /Users/rubber/linux/kernel/bpf/verifier.c: 7095
 * requires corresponding support in Spectre masking for stack ALU.  See also /Users/rubber/linux/kernel/bpf/verifier.c: 7096
 * retrieve_ptr_limit(). /Users/rubber/linux/kernel/bpf/verifier.c: 7097
 * 'off' includes 'reg->off'. /Users/rubber/linux/kernel/bpf/verifier.c: 7100
	/* For unprivileged we require that resulting offset must be in bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7132
	 * in order to be able to sanitize access later on. /Users/rubber/linux/kernel/bpf/verifier.c: 7133
/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off. /Users/rubber/linux/kernel/bpf/verifier.c: 7158
 * Caller should also handle BPF_MOV case separately. /Users/rubber/linux/kernel/bpf/verifier.c: 7159
 * If we return -EACCES, caller may want to try again treating pointer as a /Users/rubber/linux/kernel/bpf/verifier.c: 7160
 * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks. /Users/rubber/linux/kernel/bpf/verifier.c: 7161
		/* Taint dst register if offset had invalid bounds derived from /Users/rubber/linux/kernel/bpf/verifier.c: 7185
		 * e.g. dead branches. /Users/rubber/linux/kernel/bpf/verifier.c: 7186
	/* In case of 'scalar += pointer', dst_reg inherits pointer type and id. /Users/rubber/linux/kernel/bpf/verifier.c: 7230
	 * The id may be overwritten later if we create a new variable offset. /Users/rubber/linux/kernel/bpf/verifier.c: 7231
		/* We can take a fixed offset as long as it doesn't overflow /Users/rubber/linux/kernel/bpf/verifier.c: 7252
		 * the s32 'off' field /Users/rubber/linux/kernel/bpf/verifier.c: 7253
		/* A new variable offset is created.  Note that off_reg->off /Users/rubber/linux/kernel/bpf/verifier.c: 7267
		 * == 0, since it's a scalar. /Users/rubber/linux/kernel/bpf/verifier.c: 7268
		 * dst_reg gets the pointer type and since some positive /Users/rubber/linux/kernel/bpf/verifier.c: 7269
		 * integer value was added to the pointer, give it a new 'id' /Users/rubber/linux/kernel/bpf/verifier.c: 7270
		 * if it's a PTR_TO_PACKET. /Users/rubber/linux/kernel/bpf/verifier.c: 7271
		 * this creates a new 'base' pointer, off_reg (variable) gets /Users/rubber/linux/kernel/bpf/verifier.c: 7272
		 * added into the variable offset, and we copy the fixed offset /Users/rubber/linux/kernel/bpf/verifier.c: 7273
		 * from ptr_reg. /Users/rubber/linux/kernel/bpf/verifier.c: 7274
		/* We don't allow subtraction from FP, because (according to /Users/rubber/linux/kernel/bpf/verifier.c: 7308
		 * test_verifier.c test "invalid fp arithmetic", JITs might not /Users/rubber/linux/kernel/bpf/verifier.c: 7309
		 * be able to deal with it. /Users/rubber/linux/kernel/bpf/verifier.c: 7310
		/* A new variable offset is created.  If the subtrahend is known /Users/rubber/linux/kernel/bpf/verifier.c: 7330
		 * nonnegative, then any reg->range we had before is still good. /Users/rubber/linux/kernel/bpf/verifier.c: 7331
	/* Both values are positive, so we can work with unsigned and /Users/rubber/linux/kernel/bpf/verifier.c: 7514
	 * copy the result to signed (unless it exceeds S32_MAX). /Users/rubber/linux/kernel/bpf/verifier.c: 7515
	/* Both values are positive, so we can work with unsigned and /Users/rubber/linux/kernel/bpf/verifier.c: 7546
	 * copy the result to signed (unless it exceeds S64_MAX). /Users/rubber/linux/kernel/bpf/verifier.c: 7547
	/* We get our minimum from the var_off, since that's inherently /Users/rubber/linux/kernel/bpf/verifier.c: 7580
	 * bitwise.  Our maximum is the minimum of the operands' maxima. /Users/rubber/linux/kernel/bpf/verifier.c: 7581
		/* Lose signed bounds when ANDing negative numbers, /Users/rubber/linux/kernel/bpf/verifier.c: 7586
		 * ain't nobody got time for that. /Users/rubber/linux/kernel/bpf/verifier.c: 7587
		/* ANDing two positives gives a positive, so safe to /Users/rubber/linux/kernel/bpf/verifier.c: 7592
		 * cast result into s64. /Users/rubber/linux/kernel/bpf/verifier.c: 7593
	/* We get our minimum from the var_off, since that's inherently /Users/rubber/linux/kernel/bpf/verifier.c: 7613
	 * bitwise.  Our maximum is the minimum of the operands' maxima. /Users/rubber/linux/kernel/bpf/verifier.c: 7614
		/* Lose signed bounds when ANDing negative numbers, /Users/rubber/linux/kernel/bpf/verifier.c: 7619
		 * ain't nobody got time for that. /Users/rubber/linux/kernel/bpf/verifier.c: 7620
		/* ANDing two positives gives a positive, so safe to /Users/rubber/linux/kernel/bpf/verifier.c: 7625
		 * cast result into s64. /Users/rubber/linux/kernel/bpf/verifier.c: 7626
	/* We get our maximum from the var_off, and our minimum is the /Users/rubber/linux/kernel/bpf/verifier.c: 7649
	 * maximum of the operands' minima /Users/rubber/linux/kernel/bpf/verifier.c: 7650
		/* Lose signed bounds when ORing negative numbers, /Users/rubber/linux/kernel/bpf/verifier.c: 7655
		 * ain't nobody got time for that. /Users/rubber/linux/kernel/bpf/verifier.c: 7656
		/* ORing two positives gives a positive, so safe to /Users/rubber/linux/kernel/bpf/verifier.c: 7661
		 * cast result into s64. /Users/rubber/linux/kernel/bpf/verifier.c: 7662
	/* We get our maximum from the var_off, and our minimum is the /Users/rubber/linux/kernel/bpf/verifier.c: 7682
	 * maximum of the operands' minima /Users/rubber/linux/kernel/bpf/verifier.c: 7683
		/* Lose signed bounds when ORing negative numbers, /Users/rubber/linux/kernel/bpf/verifier.c: 7688
		 * ain't nobody got time for that. /Users/rubber/linux/kernel/bpf/verifier.c: 7689
		/* ORing two positives gives a positive, so safe to /Users/rubber/linux/kernel/bpf/verifier.c: 7694
		 * cast result into s64. /Users/rubber/linux/kernel/bpf/verifier.c: 7695
		/* XORing two positive sign numbers gives a positive, /Users/rubber/linux/kernel/bpf/verifier.c: 7722
		 * so safe to cast u32 result into s32. /Users/rubber/linux/kernel/bpf/verifier.c: 7723
		/* XORing two positive sign numbers gives a positive, /Users/rubber/linux/kernel/bpf/verifier.c: 7751
		 * so safe to cast u64 result into s64. /Users/rubber/linux/kernel/bpf/verifier.c: 7752
	/* We lose all sign bit information (except what we can pick /Users/rubber/linux/kernel/bpf/verifier.c: 7767
	 * up from var_off) /Users/rubber/linux/kernel/bpf/verifier.c: 7768
	/* Not required but being careful mark reg64 bounds as unknown so /Users/rubber/linux/kernel/bpf/verifier.c: 7792
	 * that we are forced to pick them up from tnum and zext later and /Users/rubber/linux/kernel/bpf/verifier.c: 7793
	 * if some path skips this step we are still safe. /Users/rubber/linux/kernel/bpf/verifier.c: 7794
	/* Special case <<32 because it is a common compiler pattern to sign /Users/rubber/linux/kernel/bpf/verifier.c: 7803
	 * extend subreg by doing <<32 s>>32. In this case if 32bit bounds are /Users/rubber/linux/kernel/bpf/verifier.c: 7804
	 * positive we know this shift will also be positive so we can track /Users/rubber/linux/kernel/bpf/verifier.c: 7805
	 * bounds correctly. Otherwise we lose all sign bit information except /Users/rubber/linux/kernel/bpf/verifier.c: 7806
	 * what we can pick up from var_off. Perhaps we can generalize this /Users/rubber/linux/kernel/bpf/verifier.c: 7807
	 * later to shifts of any length. /Users/rubber/linux/kernel/bpf/verifier.c: 7808
	/* BPF_RSH is an unsigned shift.  If the value in dst_reg might /Users/rubber/linux/kernel/bpf/verifier.c: 7852
	 * be negative, then either: /Users/rubber/linux/kernel/bpf/verifier.c: 7853
	 * 1) src_reg might be zero, so the sign bit of the result is /Users/rubber/linux/kernel/bpf/verifier.c: 7854
	 *    unknown, so we lose our signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7855
	 * 2) it's known negative, thus the unsigned bounds capture the /Users/rubber/linux/kernel/bpf/verifier.c: 7856
	 *    signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7857
	 * 3) the signed bounds cross zero, so they tell us nothing /Users/rubber/linux/kernel/bpf/verifier.c: 7858
	 *    about the result /Users/rubber/linux/kernel/bpf/verifier.c: 7859
	 * If the value in dst_reg is known nonnegative, then again the /Users/rubber/linux/kernel/bpf/verifier.c: 7860
	 * unsigned bounds capture the signed bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 7861
	 * Thus, in all cases it suffices to blow away our signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7862
	 * and rely on inferring new ones from the unsigned bounds and /Users/rubber/linux/kernel/bpf/verifier.c: 7863
	 * var_off of the result. /Users/rubber/linux/kernel/bpf/verifier.c: 7864
	/* BPF_RSH is an unsigned shift.  If the value in dst_reg might /Users/rubber/linux/kernel/bpf/verifier.c: 7883
	 * be negative, then either: /Users/rubber/linux/kernel/bpf/verifier.c: 7884
	 * 1) src_reg might be zero, so the sign bit of the result is /Users/rubber/linux/kernel/bpf/verifier.c: 7885
	 *    unknown, so we lose our signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7886
	 * 2) it's known negative, thus the unsigned bounds capture the /Users/rubber/linux/kernel/bpf/verifier.c: 7887
	 *    signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7888
	 * 3) the signed bounds cross zero, so they tell us nothing /Users/rubber/linux/kernel/bpf/verifier.c: 7889
	 *    about the result /Users/rubber/linux/kernel/bpf/verifier.c: 7890
	 * If the value in dst_reg is known nonnegative, then again the /Users/rubber/linux/kernel/bpf/verifier.c: 7891
	 * unsigned bounds capture the signed bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 7892
	 * Thus, in all cases it suffices to blow away our signed bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7893
	 * and rely on inferring new ones from the unsigned bounds and /Users/rubber/linux/kernel/bpf/verifier.c: 7894
	 * var_off of the result. /Users/rubber/linux/kernel/bpf/verifier.c: 7895
	/* Its not easy to operate on alu32 bounds here because it depends /Users/rubber/linux/kernel/bpf/verifier.c: 7903
	 * on bits being shifted in. Take easy way out and mark unbounded /Users/rubber/linux/kernel/bpf/verifier.c: 7904
	 * so we can recalculate later from tnum. /Users/rubber/linux/kernel/bpf/verifier.c: 7905
	/* Upon reaching here, src_known is true and /Users/rubber/linux/kernel/bpf/verifier.c: 7916
	 * umax_val is equal to umin_val. /Users/rubber/linux/kernel/bpf/verifier.c: 7917
	/* blow away the dst_reg umin_value/umax_value and rely on /Users/rubber/linux/kernel/bpf/verifier.c: 7924
	 * dst_reg var_off to refine the result. /Users/rubber/linux/kernel/bpf/verifier.c: 7925
	/* Upon reaching here, src_known is true and umax_val is equal /Users/rubber/linux/kernel/bpf/verifier.c: 7939
	 * to umin_val. /Users/rubber/linux/kernel/bpf/verifier.c: 7940
	/* blow away the dst_reg umin_value/umax_value and rely on /Users/rubber/linux/kernel/bpf/verifier.c: 7947
	 * dst_reg var_off to refine the result. /Users/rubber/linux/kernel/bpf/verifier.c: 7948
	/* Its not easy to operate on alu32 bounds here because it depends /Users/rubber/linux/kernel/bpf/verifier.c: 7953
	 * on bits being shifted in from upper 32-bits. Take easy way out /Users/rubber/linux/kernel/bpf/verifier.c: 7954
	 * and mark unbounded so we can recalculate later from tnum. /Users/rubber/linux/kernel/bpf/verifier.c: 7955
/* WARNING: This function does calculations on 64-bit values, but the actual /Users/rubber/linux/kernel/bpf/verifier.c: 7961
 * execution may occur on 32-bit values. Therefore, things like bitshifts /Users/rubber/linux/kernel/bpf/verifier.c: 7962
 * need extra checks in the 32-bit case. /Users/rubber/linux/kernel/bpf/verifier.c: 7963
			/* Taint dst register if offset had invalid bounds /Users/rubber/linux/kernel/bpf/verifier.c: 7996
			 * derived from e.g. dead branches. /Users/rubber/linux/kernel/bpf/verifier.c: 7997
			/* Taint dst register if offset had invalid bounds /Users/rubber/linux/kernel/bpf/verifier.c: 8007
			 * derived from e.g. dead branches. /Users/rubber/linux/kernel/bpf/verifier.c: 8008
	/* Calculate sign/unsigned bounds and tnum for alu32 and alu64 bit ops. /Users/rubber/linux/kernel/bpf/verifier.c: 8027
	 * There are two classes of instructions: The first class we track both /Users/rubber/linux/kernel/bpf/verifier.c: 8028
	 * alu32 and alu64 sign/unsigned bounds independently this provides the /Users/rubber/linux/kernel/bpf/verifier.c: 8029
	 * greatest amount of precision when alu operations are mixed with jmp32 /Users/rubber/linux/kernel/bpf/verifier.c: 8030
	 * operations. These operations are BPF_ADD, BPF_SUB, BPF_MUL, BPF_ADD, /Users/rubber/linux/kernel/bpf/verifier.c: 8031
	 * and BPF_OR. This is possible because these ops have fairly easy to /Users/rubber/linux/kernel/bpf/verifier.c: 8032
	 * understand and calculate behavior in both 32-bit and 64-bit alu ops. /Users/rubber/linux/kernel/bpf/verifier.c: 8033
	 * See alu32 verifier tests for examples. The second class of /Users/rubber/linux/kernel/bpf/verifier.c: 8034
	 * operations, BPF_LSH, BPF_RSH, and BPF_ARSH, however are not so easy /Users/rubber/linux/kernel/bpf/verifier.c: 8035
	 * with regards to tracking sign/unsigned bounds because the bits may /Users/rubber/linux/kernel/bpf/verifier.c: 8036
	 * cross subreg boundaries in the alu64 case. When this happens we mark /Users/rubber/linux/kernel/bpf/verifier.c: 8037
	 * the reg unbounded in the subreg bound space and use the resulting /Users/rubber/linux/kernel/bpf/verifier.c: 8038
	 * tnum to calculate an approximation of the sign/unsigned bounds. /Users/rubber/linux/kernel/bpf/verifier.c: 8039
			/* Shifts greater than 31 or 63 are undefined. /Users/rubber/linux/kernel/bpf/verifier.c: 8074
			 * This includes shifts by a negative number. /Users/rubber/linux/kernel/bpf/verifier.c: 8075
			/* Shifts greater than 31 or 63 are undefined. /Users/rubber/linux/kernel/bpf/verifier.c: 8087
			 * This includes shifts by a negative number. /Users/rubber/linux/kernel/bpf/verifier.c: 8088
			/* Shifts greater than 31 or 63 are undefined. /Users/rubber/linux/kernel/bpf/verifier.c: 8100
			 * This includes shifts by a negative number. /Users/rubber/linux/kernel/bpf/verifier.c: 8101
/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max /Users/rubber/linux/kernel/bpf/verifier.c: 8126
 * and var_off. /Users/rubber/linux/kernel/bpf/verifier.c: 8127
		/* Make sure ID is cleared otherwise dst_reg min/max could be /Users/rubber/linux/kernel/bpf/verifier.c: 8144
		 * incorrectly propagated into other registers by find_equal_scalars() /Users/rubber/linux/kernel/bpf/verifier.c: 8145
				/* Combining two pointers by any ALU op yields /Users/rubber/linux/kernel/bpf/verifier.c: 8152
				 * an arbitrary scalar. Disallow all math except /Users/rubber/linux/kernel/bpf/verifier.c: 8153
				 * pointer subtraction /Users/rubber/linux/kernel/bpf/verifier.c: 8154
				/* scalar += pointer /Users/rubber/linux/kernel/bpf/verifier.c: 8165
				 * This is legal, but we have to reverse our /Users/rubber/linux/kernel/bpf/verifier.c: 8166
				 * src/dest handling in computing the range /Users/rubber/linux/kernel/bpf/verifier.c: 8167
		/* Pretend the src is a reg with a known value, since we only /Users/rubber/linux/kernel/bpf/verifier.c: 8184
		 * need to be able to read from this state. /Users/rubber/linux/kernel/bpf/verifier.c: 8185
				/* case: R1 = R2 /Users/rubber/linux/kernel/bpf/verifier.c: 8278
				 * copy register state to dest reg /Users/rubber/linux/kernel/bpf/verifier.c: 8279
					/* Assign src and dst registers the same ID /Users/rubber/linux/kernel/bpf/verifier.c: 8282
					 * that will be used by find_equal_scalars() /Users/rubber/linux/kernel/bpf/verifier.c: 8283
					 * to propagate min/max range. /Users/rubber/linux/kernel/bpf/verifier.c: 8284
					/* Make sure ID is cleared otherwise /Users/rubber/linux/kernel/bpf/verifier.c: 8299
					 * dst_reg min/max could be incorrectly /Users/rubber/linux/kernel/bpf/verifier.c: 8300
					 * propagated into src_reg by find_equal_scalars() /Users/rubber/linux/kernel/bpf/verifier.c: 8301
			/* case: R = imm /Users/rubber/linux/kernel/bpf/verifier.c: 8313
			 * remember the value we stored into this reg /Users/rubber/linux/kernel/bpf/verifier.c: 8314
		/* Risk of overflow.  For instance, ptr + (1<<63) may be less /Users/rubber/linux/kernel/bpf/verifier.c: 8418
		 * than pkt_end, but that's because it's also less than pkt. /Users/rubber/linux/kernel/bpf/verifier.c: 8419
	/* Examples for register markings: /Users/rubber/linux/kernel/bpf/verifier.c: 8427
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8428
	 * pkt_data in dst register: /Users/rubber/linux/kernel/bpf/verifier.c: 8429
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8430
	 *   r2 = r3; /Users/rubber/linux/kernel/bpf/verifier.c: 8431
	 *   r2 += 8; /Users/rubber/linux/kernel/bpf/verifier.c: 8432
	 *   if (r2 > pkt_end) goto <handle exception> /Users/rubber/linux/kernel/bpf/verifier.c: 8433
	 *   <access okay> /Users/rubber/linux/kernel/bpf/verifier.c: 8434
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8435
	 *   r2 = r3; /Users/rubber/linux/kernel/bpf/verifier.c: 8436
	 *   r2 += 8; /Users/rubber/linux/kernel/bpf/verifier.c: 8437
	 *   if (r2 < pkt_end) goto <access okay> /Users/rubber/linux/kernel/bpf/verifier.c: 8438
	 *   <handle exception> /Users/rubber/linux/kernel/bpf/verifier.c: 8439
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8440
	 *   Where: /Users/rubber/linux/kernel/bpf/verifier.c: 8441
	 *     r2 == dst_reg, pkt_end == src_reg /Users/rubber/linux/kernel/bpf/verifier.c: 8442
	 *     r2=pkt(id=n,off=8,r=0) /Users/rubber/linux/kernel/bpf/verifier.c: 8443
	 *     r3=pkt(id=n,off=0,r=0) /Users/rubber/linux/kernel/bpf/verifier.c: 8444
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8445
	 * pkt_data in src register: /Users/rubber/linux/kernel/bpf/verifier.c: 8446
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8447
	 *   r2 = r3; /Users/rubber/linux/kernel/bpf/verifier.c: 8448
	 *   r2 += 8; /Users/rubber/linux/kernel/bpf/verifier.c: 8449
	 *   if (pkt_end >= r2) goto <access okay> /Users/rubber/linux/kernel/bpf/verifier.c: 8450
	 *   <handle exception> /Users/rubber/linux/kernel/bpf/verifier.c: 8451
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8452
	 *   r2 = r3; /Users/rubber/linux/kernel/bpf/verifier.c: 8453
	 *   r2 += 8; /Users/rubber/linux/kernel/bpf/verifier.c: 8454
	 *   if (pkt_end <= r2) goto <handle exception> /Users/rubber/linux/kernel/bpf/verifier.c: 8455
	 *   <access okay> /Users/rubber/linux/kernel/bpf/verifier.c: 8456
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8457
	 *   Where: /Users/rubber/linux/kernel/bpf/verifier.c: 8458
	 *     pkt_end == dst_reg, r2 == src_reg /Users/rubber/linux/kernel/bpf/verifier.c: 8459
	 *     r2=pkt(id=n,off=8,r=0) /Users/rubber/linux/kernel/bpf/verifier.c: 8460
	 *     r3=pkt(id=n,off=0,r=0) /Users/rubber/linux/kernel/bpf/verifier.c: 8461
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 8462
	 * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8) /Users/rubber/linux/kernel/bpf/verifier.c: 8463
	 * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8) /Users/rubber/linux/kernel/bpf/verifier.c: 8464
	 * and [r3, r3 + 8-1) respectively is safe to access depending on /Users/rubber/linux/kernel/bpf/verifier.c: 8465
	 * the check. /Users/rubber/linux/kernel/bpf/verifier.c: 8466
	/* If our ids match, then we must have the same max_value.  And we /Users/rubber/linux/kernel/bpf/verifier.c: 8469
	 * don't care about the other reg's fixed offset, since if it's too big /Users/rubber/linux/kernel/bpf/verifier.c: 8470
	 * the range won't allow anything. /Users/rubber/linux/kernel/bpf/verifier.c: 8471
	 * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16. /Users/rubber/linux/kernel/bpf/verifier.c: 8472
/* compute branch direction of the expression "if (reg opcode val) goto target;" /Users/rubber/linux/kernel/bpf/verifier.c: 8625
 * and return: /Users/rubber/linux/kernel/bpf/verifier.c: 8626
 *  1 - branch will be taken and "goto target" will be executed /Users/rubber/linux/kernel/bpf/verifier.c: 8627
 *  0 - branch will not be taken and fall-through to next insn /Users/rubber/linux/kernel/bpf/verifier.c: 8628
 * -1 - unknown. Example: "if (reg < 5)" is unknown when register value /Users/rubber/linux/kernel/bpf/verifier.c: 8629
 *      range [0,10] /Users/rubber/linux/kernel/bpf/verifier.c: 8630
		/* If pointer is valid tests against zero will fail so we can /Users/rubber/linux/kernel/bpf/verifier.c: 8639
		 * use this to direct branch taken. /Users/rubber/linux/kernel/bpf/verifier.c: 8640
/* Adjusts the register min/max values in the case that the dst_reg is the /Users/rubber/linux/kernel/bpf/verifier.c: 8721
 * variable register that we are working on, and src_reg is a constant or we're /Users/rubber/linux/kernel/bpf/verifier.c: 8722
 * simply doing a BPF_K check. /Users/rubber/linux/kernel/bpf/verifier.c: 8723
 * In JEQ/JNE cases we also adjust the var_off values. /Users/rubber/linux/kernel/bpf/verifier.c: 8724
	/* If the dst_reg is a pointer, we can't learn anything about its /Users/rubber/linux/kernel/bpf/verifier.c: 8738
	 * variable offset from the compare (unless src_reg were a pointer into /Users/rubber/linux/kernel/bpf/verifier.c: 8739
	 * the same object, but we don't bother with that. /Users/rubber/linux/kernel/bpf/verifier.c: 8740
	 * Since false_reg and true_reg have the same type by construction, we /Users/rubber/linux/kernel/bpf/verifier.c: 8741
	 * only need to check one of them for pointerness. /Users/rubber/linux/kernel/bpf/verifier.c: 8742
		/* JEQ/JNE comparison doesn't change the register equivalence. /Users/rubber/linux/kernel/bpf/verifier.c: 8754
		 * r1 = r2; /Users/rubber/linux/kernel/bpf/verifier.c: 8755
		 * if (r1 == 42) goto label; /Users/rubber/linux/kernel/bpf/verifier.c: 8756
		 * ... /Users/rubber/linux/kernel/bpf/verifier.c: 8757
		 * label: // here both r1 and r2 are known to be 42. /Users/rubber/linux/kernel/bpf/verifier.c: 8758
		 * /Users/rubber/linux/kernel/bpf/verifier.c: 8759
		 * Hence when marking register as known preserve it's ID. /Users/rubber/linux/kernel/bpf/verifier.c: 8760
/* Same as above, but for the case that dst_reg holds a constant and src_reg is /Users/rubber/linux/kernel/bpf/verifier.c: 8876
 * the variable reg. /Users/rubber/linux/kernel/bpf/verifier.c: 8877
	/* This uses zero as "not present in table"; luckily the zero opcode, /Users/rubber/linux/kernel/bpf/verifier.c: 8885
	 * BPF_JA, can't get here. /Users/rubber/linux/kernel/bpf/verifier.c: 8886
	/* Intersecting with the old var_off might have improved our bounds /Users/rubber/linux/kernel/bpf/verifier.c: 8915
	 * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc), /Users/rubber/linux/kernel/bpf/verifier.c: 8916
	 * then new var_off is (0; 0x7f...fc) which improves our umax. /Users/rubber/linux/kernel/bpf/verifier.c: 8917
		/* Old offset (both fixed and variable parts) should /Users/rubber/linux/kernel/bpf/verifier.c: 8945
		 * have been known-zero, because we don't allow pointer /Users/rubber/linux/kernel/bpf/verifier.c: 8946
		 * arithmetic on pointers that might be NULL. /Users/rubber/linux/kernel/bpf/verifier.c: 8947
			/* We don't need id and ref_obj_id from this point /Users/rubber/linux/kernel/bpf/verifier.c: 8957
			 * onwards anymore, thus we should better reset it, /Users/rubber/linux/kernel/bpf/verifier.c: 8958
			 * so that state pruning has chances to take effect. /Users/rubber/linux/kernel/bpf/verifier.c: 8959
			/* For not-NULL ptr, reg->ref_obj_id will be reset /Users/rubber/linux/kernel/bpf/verifier.c: 8970
			 * in release_reg_references(). /Users/rubber/linux/kernel/bpf/verifier.c: 8971
			 * /Users/rubber/linux/kernel/bpf/verifier.c: 8972
			 * reg->id is still used by spin_lock ptr. Other /Users/rubber/linux/kernel/bpf/verifier.c: 8973
			 * than spin_lock ptr type, reg->id can be reset. /Users/rubber/linux/kernel/bpf/verifier.c: 8974
/* The logic is similar to find_good_pkt_pointers(), both could eventually /Users/rubber/linux/kernel/bpf/verifier.c: 8997
 * be folded together at some point. /Users/rubber/linux/kernel/bpf/verifier.c: 8998
		/* regs[regno] is in the " == NULL" branch. /Users/rubber/linux/kernel/bpf/verifier.c: 9010
		 * No one could have freed the reference state before /Users/rubber/linux/kernel/bpf/verifier.c: 9011
		 * doing the NULL check. /Users/rubber/linux/kernel/bpf/verifier.c: 9012
		/* If we get here with a dst_reg pointer type it is because /Users/rubber/linux/kernel/bpf/verifier.c: 9220
		 * above is_branch_taken() special cased the 0 comparison. /Users/rubber/linux/kernel/bpf/verifier.c: 9221
		/* Only follow the goto, ignore fall-through. If needed, push /Users/rubber/linux/kernel/bpf/verifier.c: 9233
		 * the fall-through branch for simulation under speculative /Users/rubber/linux/kernel/bpf/verifier.c: 9234
		 * execution. /Users/rubber/linux/kernel/bpf/verifier.c: 9235
		/* Only follow the fall-through branch, since that's where the /Users/rubber/linux/kernel/bpf/verifier.c: 9244
		 * program will go. If needed, push the goto branch for /Users/rubber/linux/kernel/bpf/verifier.c: 9245
		 * simulation under speculative execution. /Users/rubber/linux/kernel/bpf/verifier.c: 9246
	/* detect if we are comparing against a constant value so we can adjust /Users/rubber/linux/kernel/bpf/verifier.c: 9262
	 * our min/max values for our dst register. /Users/rubber/linux/kernel/bpf/verifier.c: 9263
	 * this is only legit if both are scalars (or pointers to the same /Users/rubber/linux/kernel/bpf/verifier.c: 9264
	 * object, I suppose, but we don't support that right now), because /Users/rubber/linux/kernel/bpf/verifier.c: 9265
	 * otherwise the different base pointers mean the offsets aren't /Users/rubber/linux/kernel/bpf/verifier.c: 9266
	 * comparable. /Users/rubber/linux/kernel/bpf/verifier.c: 9267
	/* detect if R == 0 where R is returned from bpf_map_lookup_elem(). /Users/rubber/linux/kernel/bpf/verifier.c: 9315
	 * NOTE: these optimizations below are related with pointer comparison /Users/rubber/linux/kernel/bpf/verifier.c: 9316
	 *       which will never be JMP32. /Users/rubber/linux/kernel/bpf/verifier.c: 9317
		/* Mark all identical registers in each branch as either /Users/rubber/linux/kernel/bpf/verifier.c: 9322
		 * safe or unknown depending R == 0 or R != 0 conditional. /Users/rubber/linux/kernel/bpf/verifier.c: 9323
/* verify safety of LD_ABS|LD_IND instructions: /Users/rubber/linux/kernel/bpf/verifier.c: 9444
 * - they can only appear in the programs where ctx == skb /Users/rubber/linux/kernel/bpf/verifier.c: 9445
 * - since they are wrappers of function calls, they scratch R1-R5 registers, /Users/rubber/linux/kernel/bpf/verifier.c: 9446
 *   preserve R6-R9, and store return value into R0 /Users/rubber/linux/kernel/bpf/verifier.c: 9447
 * Implicit input: /Users/rubber/linux/kernel/bpf/verifier.c: 9449
 *   ctx == skb == R6 == CTX /Users/rubber/linux/kernel/bpf/verifier.c: 9450
 * Explicit input: /Users/rubber/linux/kernel/bpf/verifier.c: 9452
 *   SRC == any register /Users/rubber/linux/kernel/bpf/verifier.c: 9453
 *   IMM == 32-bit immediate /Users/rubber/linux/kernel/bpf/verifier.c: 9454
 * Output: /Users/rubber/linux/kernel/bpf/verifier.c: 9456
 *   R0 - 8/16/32-bit skb data converted to cpu endianness /Users/rubber/linux/kernel/bpf/verifier.c: 9457
	/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as /Users/rubber/linux/kernel/bpf/verifier.c: 9488
	 * gen_ld_abs() may terminate the program at runtime, leading to /Users/rubber/linux/kernel/bpf/verifier.c: 9489
	 * reference leak. /Users/rubber/linux/kernel/bpf/verifier.c: 9490
	/* mark destination R0 register as readable, since it contains /Users/rubber/linux/kernel/bpf/verifier.c: 9526
	 * the value fetched from the packet. /Users/rubber/linux/kernel/bpf/verifier.c: 9527
	 * Already marked as written above. /Users/rubber/linux/kernel/bpf/verifier.c: 9528
	/* eBPF calling convention is such that R0 is used /Users/rubber/linux/kernel/bpf/verifier.c: 9554
	 * to return the value from eBPF program. /Users/rubber/linux/kernel/bpf/verifier.c: 9555
	 * Make sure that it's readable at this time /Users/rubber/linux/kernel/bpf/verifier.c: 9556
	 * of bpf_exit, which means that program wrote /Users/rubber/linux/kernel/bpf/verifier.c: 9557
	 * something into it earlier /Users/rubber/linux/kernel/bpf/verifier.c: 9558
		/* freplace program can return anything as its return value /Users/rubber/linux/kernel/bpf/verifier.c: 9644
		 * depends on the to-be-replaced kernel func or bpf program. /Users/rubber/linux/kernel/bpf/verifier.c: 9645
/* non-recursive DFS pseudo code /Users/rubber/linux/kernel/bpf/verifier.c: 9668
 * 1  procedure DFS-iterative(G,v): /Users/rubber/linux/kernel/bpf/verifier.c: 9669
 * 2      label v as discovered /Users/rubber/linux/kernel/bpf/verifier.c: 9670
 * 3      let S be a stack /Users/rubber/linux/kernel/bpf/verifier.c: 9671
 * 4      S.push(v) /Users/rubber/linux/kernel/bpf/verifier.c: 9672
 * 5      while S is not empty /Users/rubber/linux/kernel/bpf/verifier.c: 9673
 * 6            t <- S.pop() /Users/rubber/linux/kernel/bpf/verifier.c: 9674
 * 7            if t is what we're looking for: /Users/rubber/linux/kernel/bpf/verifier.c: 9675
 * 8                return t /Users/rubber/linux/kernel/bpf/verifier.c: 9676
 * 9            for all edges e in G.adjacentEdges(t) do /Users/rubber/linux/kernel/bpf/verifier.c: 9677
 * 10               if edge e is already labelled /Users/rubber/linux/kernel/bpf/verifier.c: 9678
 * 11                   continue with the next edge /Users/rubber/linux/kernel/bpf/verifier.c: 9679
 * 12               w <- G.adjacentVertex(t,e) /Users/rubber/linux/kernel/bpf/verifier.c: 9680
 * 13               if vertex w is not discovered and not explored /Users/rubber/linux/kernel/bpf/verifier.c: 9681
 * 14                   label e as tree-edge /Users/rubber/linux/kernel/bpf/verifier.c: 9682
 * 15                   label w as discovered /Users/rubber/linux/kernel/bpf/verifier.c: 9683
 * 16                   S.push(w) /Users/rubber/linux/kernel/bpf/verifier.c: 9684
 * 17                   continue at 5 /Users/rubber/linux/kernel/bpf/verifier.c: 9685
 * 18               else if vertex w is discovered /Users/rubber/linux/kernel/bpf/verifier.c: 9686
 * 19                   label e as back-edge /Users/rubber/linux/kernel/bpf/verifier.c: 9687
 * 20               else /Users/rubber/linux/kernel/bpf/verifier.c: 9688
 * 21                   // vertex w is explored /Users/rubber/linux/kernel/bpf/verifier.c: 9689
 * 22                   label e as forward- or cross-edge /Users/rubber/linux/kernel/bpf/verifier.c: 9690
 * 23           label t as explored /Users/rubber/linux/kernel/bpf/verifier.c: 9691
 * 24           S.pop() /Users/rubber/linux/kernel/bpf/verifier.c: 9692
 * convention: /Users/rubber/linux/kernel/bpf/verifier.c: 9694
 * 0x10 - discovered /Users/rubber/linux/kernel/bpf/verifier.c: 9695
 * 0x11 - discovered and fall-through edge labelled /Users/rubber/linux/kernel/bpf/verifier.c: 9696
 * 0x12 - discovered and fall-through and branch edges labelled /Users/rubber/linux/kernel/bpf/verifier.c: 9697
 * 0x20 - explored /Users/rubber/linux/kernel/bpf/verifier.c: 9698
/* t, w, e - match pseudo-code above: /Users/rubber/linux/kernel/bpf/verifier.c: 9733
 * t - index of current instruction /Users/rubber/linux/kernel/bpf/verifier.c: 9734
 * w - next instruction /Users/rubber/linux/kernel/bpf/verifier.c: 9735
 * e - edge /Users/rubber/linux/kernel/bpf/verifier.c: 9736
				/* It's ok to allow recursion from CFG point of /Users/rubber/linux/kernel/bpf/verifier.c: 9801
				 * view. __check_func_call() will do the actual /Users/rubber/linux/kernel/bpf/verifier.c: 9802
				 * check. /Users/rubber/linux/kernel/bpf/verifier.c: 9803
/* Visits the instruction at index t and returns one of the following: /Users/rubber/linux/kernel/bpf/verifier.c: 9810
 *  < 0 - an error occurred /Users/rubber/linux/kernel/bpf/verifier.c: 9811
 *  DONE_EXPLORING - the instruction was fully explored /Users/rubber/linux/kernel/bpf/verifier.c: 9812
 *  KEEP_EXPLORING - there is still work to be done before it is fully explored /Users/rubber/linux/kernel/bpf/verifier.c: 9813
			/* Mark this call insn to trigger is_state_visited() check /Users/rubber/linux/kernel/bpf/verifier.c: 9834
			 * before call itself is processed by __check_func_call(). /Users/rubber/linux/kernel/bpf/verifier.c: 9835
			 * Otherwise new async state will be pushed for further /Users/rubber/linux/kernel/bpf/verifier.c: 9836
			 * exploration. /Users/rubber/linux/kernel/bpf/verifier.c: 9837
		/* unconditional jmp is not a good pruning point, /Users/rubber/linux/kernel/bpf/verifier.c: 9853
		 * but it's marked, since backtracking needs /Users/rubber/linux/kernel/bpf/verifier.c: 9854
		 * to record jmp history in is_state_visited(). /Users/rubber/linux/kernel/bpf/verifier.c: 9855
		/* tell verifier to check for equivalent states /Users/rubber/linux/kernel/bpf/verifier.c: 9858
		 * after every call and jump /Users/rubber/linux/kernel/bpf/verifier.c: 9859
/* non-recursive depth-first-search to detect loops in BPF program /Users/rubber/linux/kernel/bpf/verifier.c: 9877
 * loop == back-edge in directed graph /Users/rubber/linux/kernel/bpf/verifier.c: 9878
				/* set the size kernel expects so loader can zero /Users/rubber/linux/kernel/bpf/verifier.c: 10018
				 * out the rest of the record. /Users/rubber/linux/kernel/bpf/verifier.c: 10019
	/* Need to zero it in case the userspace may /Users/rubber/linux/kernel/bpf/verifier.c: 10135
	 * pass in a smaller bpf_line_info object. /Users/rubber/linux/kernel/bpf/verifier.c: 10136
		/* /Users/rubber/linux/kernel/bpf/verifier.c: 10169
		 * Check insn_off to ensure /Users/rubber/linux/kernel/bpf/verifier.c: 10170
		 * 1) strictly increasing AND /Users/rubber/linux/kernel/bpf/verifier.c: 10171
		 * 2) bounded by prog->len /Users/rubber/linux/kernel/bpf/verifier.c: 10172
		 * /Users/rubber/linux/kernel/bpf/verifier.c: 10173
		 * The linfo[0].insn_off == 0 check logically falls into /Users/rubber/linux/kernel/bpf/verifier.c: 10174
		 * the later "missing bpf_line_info for func..." case /Users/rubber/linux/kernel/bpf/verifier.c: 10175
		 * because the first linfo[0].insn_off must be the /Users/rubber/linux/kernel/bpf/verifier.c: 10176
		 * first sub also and the first sub must have /Users/rubber/linux/kernel/bpf/verifier.c: 10177
		 * subprog_info[0].start == 0. /Users/rubber/linux/kernel/bpf/verifier.c: 10178
/* If in the old state two registers had the same id, then they need to have /Users/rubber/linux/kernel/bpf/verifier.c: 10283
 * the same id in the new state as well.  But that id could be different from /Users/rubber/linux/kernel/bpf/verifier.c: 10284
 * the old state, so we need to track the mapping from old to new ids. /Users/rubber/linux/kernel/bpf/verifier.c: 10285
 * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent /Users/rubber/linux/kernel/bpf/verifier.c: 10286
 * regs with old id 5 must also have new id 9 for the new state to be safe.  But /Users/rubber/linux/kernel/bpf/verifier.c: 10287
 * regs with a different old id could still have new id 9, we don't care about /Users/rubber/linux/kernel/bpf/verifier.c: 10288
 * that. /Users/rubber/linux/kernel/bpf/verifier.c: 10289
 * So we look through our idmap to see if this old id has been seen before.  If /Users/rubber/linux/kernel/bpf/verifier.c: 10290
 * so, we require the new id to match; otherwise, we add the id pair to the map. /Users/rubber/linux/kernel/bpf/verifier.c: 10291
			/* since the register is unused, clear its state /Users/rubber/linux/kernel/bpf/verifier.c: 10323
			 * to make further comparison simpler /Users/rubber/linux/kernel/bpf/verifier.c: 10324
/* the parentage chains form a tree. /Users/rubber/linux/kernel/bpf/verifier.c: 10354
 * the verifier states are added to state lists at given insn and /Users/rubber/linux/kernel/bpf/verifier.c: 10355
 * pushed into state stack for future exploration. /Users/rubber/linux/kernel/bpf/verifier.c: 10356
 * when the verifier reaches bpf_exit insn some of the verifer states /Users/rubber/linux/kernel/bpf/verifier.c: 10357
 * stored in the state lists have their final liveness state already, /Users/rubber/linux/kernel/bpf/verifier.c: 10358
 * but a lot of states will get revised from liveness point of view when /Users/rubber/linux/kernel/bpf/verifier.c: 10359
 * the verifier explores other branches. /Users/rubber/linux/kernel/bpf/verifier.c: 10360
 * Example: /Users/rubber/linux/kernel/bpf/verifier.c: 10361
 * 1: r0 = 1 /Users/rubber/linux/kernel/bpf/verifier.c: 10362
 * 2: if r1 == 100 goto pc+1 /Users/rubber/linux/kernel/bpf/verifier.c: 10363
 * 3: r0 = 2 /Users/rubber/linux/kernel/bpf/verifier.c: 10364
 * 4: exit /Users/rubber/linux/kernel/bpf/verifier.c: 10365
 * when the verifier reaches exit insn the register r0 in the state list of /Users/rubber/linux/kernel/bpf/verifier.c: 10366
 * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch /Users/rubber/linux/kernel/bpf/verifier.c: 10367
 * of insn 2 and goes exploring further. At the insn 4 it will walk the /Users/rubber/linux/kernel/bpf/verifier.c: 10368
 * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ. /Users/rubber/linux/kernel/bpf/verifier.c: 10369
 * Since the verifier pushes the branch states as it sees them while exploring /Users/rubber/linux/kernel/bpf/verifier.c: 10371
 * the program the condition of walking the branch instruction for the second /Users/rubber/linux/kernel/bpf/verifier.c: 10372
 * time means that all states below this branch were already explored and /Users/rubber/linux/kernel/bpf/verifier.c: 10373
 * their final liveness marks are already propagated. /Users/rubber/linux/kernel/bpf/verifier.c: 10374
 * Hence when the verifier completes the search of state list in is_state_visited() /Users/rubber/linux/kernel/bpf/verifier.c: 10375
 * we can call this clean_live_states() function to mark all liveness states /Users/rubber/linux/kernel/bpf/verifier.c: 10376
 * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state' /Users/rubber/linux/kernel/bpf/verifier.c: 10377
 * will not be used. /Users/rubber/linux/kernel/bpf/verifier.c: 10378
 * This function also clears the registers and stack for states that !READ /Users/rubber/linux/kernel/bpf/verifier.c: 10379
 * to simplify state merging. /Users/rubber/linux/kernel/bpf/verifier.c: 10380
 * Important note here that walking the same branch instruction in the callee /Users/rubber/linux/kernel/bpf/verifier.c: 10382
 * doesn't meant that the states are DONE. The verifier has to compare /Users/rubber/linux/kernel/bpf/verifier.c: 10383
 * the callsites /Users/rubber/linux/kernel/bpf/verifier.c: 10384
		/* two stack pointers are equal only if they're pointing to /Users/rubber/linux/kernel/bpf/verifier.c: 10421
		 * the same stack frame, since fp-8 in foo != fp-8 in bar /Users/rubber/linux/kernel/bpf/verifier.c: 10422
			/* We're trying to use a pointer in place of a scalar. /Users/rubber/linux/kernel/bpf/verifier.c: 10445
			 * Even if the scalar was unbounded, this could lead to /Users/rubber/linux/kernel/bpf/verifier.c: 10446
			 * pointer leaks because scalars are allowed to leak /Users/rubber/linux/kernel/bpf/verifier.c: 10447
			 * while pointers are not. We could make this safe in /Users/rubber/linux/kernel/bpf/verifier.c: 10448
			 * special cases if root is calling us, but it's /Users/rubber/linux/kernel/bpf/verifier.c: 10449
			 * probably not worth the hassle. /Users/rubber/linux/kernel/bpf/verifier.c: 10450
		/* If the new min/max/var_off satisfy the old ones and /Users/rubber/linux/kernel/bpf/verifier.c: 10456
		 * everything else matches, we are OK. /Users/rubber/linux/kernel/bpf/verifier.c: 10457
		 * 'id' is not compared, since it's only used for maps with /Users/rubber/linux/kernel/bpf/verifier.c: 10458
		 * bpf_spin_lock inside map element and in such cases if /Users/rubber/linux/kernel/bpf/verifier.c: 10459
		 * the rest of the prog is valid for one map element then /Users/rubber/linux/kernel/bpf/verifier.c: 10460
		 * it's valid for all map elements regardless of the key /Users/rubber/linux/kernel/bpf/verifier.c: 10461
		 * used in bpf_map_lookup() /Users/rubber/linux/kernel/bpf/verifier.c: 10462
		/* a PTR_TO_MAP_VALUE could be safe to use as a /Users/rubber/linux/kernel/bpf/verifier.c: 10468
		 * PTR_TO_MAP_VALUE_OR_NULL into the same map. /Users/rubber/linux/kernel/bpf/verifier.c: 10469
		 * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL- /Users/rubber/linux/kernel/bpf/verifier.c: 10470
		 * checked, doing so could have affected others with the same /Users/rubber/linux/kernel/bpf/verifier.c: 10471
		 * id, and we can't check for that because we lost the id when /Users/rubber/linux/kernel/bpf/verifier.c: 10472
		 * we converted to a PTR_TO_MAP_VALUE. /Users/rubber/linux/kernel/bpf/verifier.c: 10473
		/* We must have at least as much range as the old ptr /Users/rubber/linux/kernel/bpf/verifier.c: 10485
		 * did, so that any accesses which were safe before are /Users/rubber/linux/kernel/bpf/verifier.c: 10486
		 * still safe.  This is true even if old range < old off, /Users/rubber/linux/kernel/bpf/verifier.c: 10487
		 * since someone could have accessed through (ptr - k), or /Users/rubber/linux/kernel/bpf/verifier.c: 10488
		 * even done ptr -= k in a register, to get a safe access. /Users/rubber/linux/kernel/bpf/verifier.c: 10489
		/* If the offsets don't match, we can't trust our alignment; /Users/rubber/linux/kernel/bpf/verifier.c: 10493
		 * nor can we be sure that we won't fall out of range. /Users/rubber/linux/kernel/bpf/verifier.c: 10494
		/* Only valid matches are exact, which memcmp() above /Users/rubber/linux/kernel/bpf/verifier.c: 10515
		 * would have accepted /Users/rubber/linux/kernel/bpf/verifier.c: 10516
	/* walk slots of the explored stack and ignore any additional /Users/rubber/linux/kernel/bpf/verifier.c: 10533
	 * slots in the current stack, since explored(safe) state /Users/rubber/linux/kernel/bpf/verifier.c: 10534
	 * didn't use them /Users/rubber/linux/kernel/bpf/verifier.c: 10535
		/* explored stack has more populated slots than current stack /Users/rubber/linux/kernel/bpf/verifier.c: 10549
		 * and these slots were used /Users/rubber/linux/kernel/bpf/verifier.c: 10550
		/* if old state was safe with misc data in the stack /Users/rubber/linux/kernel/bpf/verifier.c: 10555
		 * it will be safe with zero-initialized stack. /Users/rubber/linux/kernel/bpf/verifier.c: 10556
		 * The opposite is not true /Users/rubber/linux/kernel/bpf/verifier.c: 10557
			/* Ex: old explored (safe) state has STACK_SPILL in /Users/rubber/linux/kernel/bpf/verifier.c: 10564
			 * this stack slot, but current has STACK_MISC -> /Users/rubber/linux/kernel/bpf/verifier.c: 10565
			 * this verifier states are not equivalent, /Users/rubber/linux/kernel/bpf/verifier.c: 10566
			 * return false to continue verification of this path /Users/rubber/linux/kernel/bpf/verifier.c: 10567
			/* when explored and current stack slot are both storing /Users/rubber/linux/kernel/bpf/verifier.c: 10576
			 * spilled registers, check that stored pointers types /Users/rubber/linux/kernel/bpf/verifier.c: 10577
			 * are the same as well. /Users/rubber/linux/kernel/bpf/verifier.c: 10578
			 * Ex: explored safe path could have stored /Users/rubber/linux/kernel/bpf/verifier.c: 10579
			 * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8} /Users/rubber/linux/kernel/bpf/verifier.c: 10580
			 * but current path has stored: /Users/rubber/linux/kernel/bpf/verifier.c: 10581
			 * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16} /Users/rubber/linux/kernel/bpf/verifier.c: 10582
			 * such verifier states are not equivalent. /Users/rubber/linux/kernel/bpf/verifier.c: 10583
			 * return false to continue verification of this path /Users/rubber/linux/kernel/bpf/verifier.c: 10584
/* compare two verifier states /Users/rubber/linux/kernel/bpf/verifier.c: 10599
 * all states stored in state_list are known to be valid, since /Users/rubber/linux/kernel/bpf/verifier.c: 10601
 * verifier reached 'bpf_exit' instruction through them /Users/rubber/linux/kernel/bpf/verifier.c: 10602
 * this function is called when verifier exploring different branches of /Users/rubber/linux/kernel/bpf/verifier.c: 10604
 * execution popped from the state stack. If it sees an old state that has /Users/rubber/linux/kernel/bpf/verifier.c: 10605
 * more strict register state and more strict stack state then this execution /Users/rubber/linux/kernel/bpf/verifier.c: 10606
 * branch doesn't need to be explored further, since verifier already /Users/rubber/linux/kernel/bpf/verifier.c: 10607
 * concluded that more strict state leads to valid finish. /Users/rubber/linux/kernel/bpf/verifier.c: 10608
 * Therefore two states are equivalent if register state is more conservative /Users/rubber/linux/kernel/bpf/verifier.c: 10610
 * and explored stack state is more conservative than the current one. /Users/rubber/linux/kernel/bpf/verifier.c: 10611
 * Example: /Users/rubber/linux/kernel/bpf/verifier.c: 10612
 *       explored                   current /Users/rubber/linux/kernel/bpf/verifier.c: 10613
 * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC) /Users/rubber/linux/kernel/bpf/verifier.c: 10614
 * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC) /Users/rubber/linux/kernel/bpf/verifier.c: 10615
 * In other words if current stack state (one being explored) has more /Users/rubber/linux/kernel/bpf/verifier.c: 10617
 * valid slots than old one that already passed validation, it means /Users/rubber/linux/kernel/bpf/verifier.c: 10618
 * the verifier can stop exploring and conclude that current state is valid too /Users/rubber/linux/kernel/bpf/verifier.c: 10619
 * Similarly with registers. If explored state has register type as invalid /Users/rubber/linux/kernel/bpf/verifier.c: 10621
 * whereas register type in current state is meaningful, it means that /Users/rubber/linux/kernel/bpf/verifier.c: 10622
 * the current state will reach 'bpf_exit' instruction safely /Users/rubber/linux/kernel/bpf/verifier.c: 10623
	/* Verification state from speculative execution simulation /Users/rubber/linux/kernel/bpf/verifier.c: 10654
	 * must never prune a non-speculative execution one. /Users/rubber/linux/kernel/bpf/verifier.c: 10655
	/* for states to be equal callsites have to be the same /Users/rubber/linux/kernel/bpf/verifier.c: 10663
	 * and all frame states need to be equivalent /Users/rubber/linux/kernel/bpf/verifier.c: 10664
/* Return 0 if no propagation happened. Return negative error code if error /Users/rubber/linux/kernel/bpf/verifier.c: 10675
 * happened. Otherwise, return the propagated bit. /Users/rubber/linux/kernel/bpf/verifier.c: 10676
	/* When comes here, read flags of PARENT_REG or REG could be any of /Users/rubber/linux/kernel/bpf/verifier.c: 10686
	 * REG_LIVE_READ64, REG_LIVE_READ32, REG_LIVE_NONE. There is no need /Users/rubber/linux/kernel/bpf/verifier.c: 10687
	 * of propagation if PARENT_REG has strongest REG_LIVE_READ64. /Users/rubber/linux/kernel/bpf/verifier.c: 10688
/* A write screens off any subsequent reads; but write marks come from the /Users/rubber/linux/kernel/bpf/verifier.c: 10704
 * straight-line code between a state and its parent.  When we arrive at an /Users/rubber/linux/kernel/bpf/verifier.c: 10705
 * equivalent state (jump target or such) we didn't arrive by the straight-line /Users/rubber/linux/kernel/bpf/verifier.c: 10706
 * code, so read marks in the state must propagate to the parent regardless /Users/rubber/linux/kernel/bpf/verifier.c: 10707
 * of the state's write marks. That's what 'parent == state->parent' comparison /Users/rubber/linux/kernel/bpf/verifier.c: 10708
 * in mark_reg_read() is for. /Users/rubber/linux/kernel/bpf/verifier.c: 10709
/* find precise scalars in the previous equivalent state and /Users/rubber/linux/kernel/bpf/verifier.c: 10755
 * propagate them into the current state /Users/rubber/linux/kernel/bpf/verifier.c: 10756
		/* this 'insn_idx' instruction wasn't marked, so we will not /Users/rubber/linux/kernel/bpf/verifier.c: 10824
		 * be doing state search here /Users/rubber/linux/kernel/bpf/verifier.c: 10825
	/* bpf progs typically have pruning point every 4 instructions /Users/rubber/linux/kernel/bpf/verifier.c: 10829
	 * http://vger.kernel.org/bpfconf2019.html#session-1 /Users/rubber/linux/kernel/bpf/verifier.c: 10830
	 * Do not add new state for future pruning if the verifier hasn't seen /Users/rubber/linux/kernel/bpf/verifier.c: 10831
	 * at least 2 jumps and at least 8 instructions. /Users/rubber/linux/kernel/bpf/verifier.c: 10832
	 * This heuristics helps decrease 'total_states' and 'peak_states' metric. /Users/rubber/linux/kernel/bpf/verifier.c: 10833
	 * In tests that amounts to up to 50% reduction into total verifier /Users/rubber/linux/kernel/bpf/verifier.c: 10834
	 * memory consumption and 20% verifier time speedup. /Users/rubber/linux/kernel/bpf/verifier.c: 10835
				/* Different async_entry_cnt means that the verifier is /Users/rubber/linux/kernel/bpf/verifier.c: 10856
				 * processing another entry into async callback. /Users/rubber/linux/kernel/bpf/verifier.c: 10857
				 * Seeing the same state is not an indication of infinite /Users/rubber/linux/kernel/bpf/verifier.c: 10858
				 * loop or infinite recursion. /Users/rubber/linux/kernel/bpf/verifier.c: 10859
				 * But finding the same state doesn't mean that it's safe /Users/rubber/linux/kernel/bpf/verifier.c: 10860
				 * to stop processing the current state. The previous state /Users/rubber/linux/kernel/bpf/verifier.c: 10861
				 * hasn't yet reached bpf_exit, since state.branches > 0. /Users/rubber/linux/kernel/bpf/verifier.c: 10862
				 * Checking in_async_callback_fn alone is not enough either. /Users/rubber/linux/kernel/bpf/verifier.c: 10863
				 * Since the verifier still needs to catch infinite loops /Users/rubber/linux/kernel/bpf/verifier.c: 10864
				 * inside async callbacks. /Users/rubber/linux/kernel/bpf/verifier.c: 10865
			/* if the verifier is processing a loop, avoid adding new state /Users/rubber/linux/kernel/bpf/verifier.c: 10873
			 * too often, since different loop iterations have distinct /Users/rubber/linux/kernel/bpf/verifier.c: 10874
			 * states and may not help future pruning. /Users/rubber/linux/kernel/bpf/verifier.c: 10875
			 * This threshold shouldn't be too low to make sure that /Users/rubber/linux/kernel/bpf/verifier.c: 10876
			 * a loop with large bound will be rejected quickly. /Users/rubber/linux/kernel/bpf/verifier.c: 10877
			 * The most abusive loop will be: /Users/rubber/linux/kernel/bpf/verifier.c: 10878
			 * r1 += 1 /Users/rubber/linux/kernel/bpf/verifier.c: 10879
			 * if r1 < 1000000 goto pc-2 /Users/rubber/linux/kernel/bpf/verifier.c: 10880
			 * 1M insn_procssed limit / 100 == 10k peak states. /Users/rubber/linux/kernel/bpf/verifier.c: 10881
			 * This threshold shouldn't be too high either, since states /Users/rubber/linux/kernel/bpf/verifier.c: 10882
			 * at the end of the loop are likely to be useful in pruning. /Users/rubber/linux/kernel/bpf/verifier.c: 10883
			/* reached equivalent register/stack state, /Users/rubber/linux/kernel/bpf/verifier.c: 10892
			 * prune the search. /Users/rubber/linux/kernel/bpf/verifier.c: 10893
			 * Registers read by the continuation are read by us. /Users/rubber/linux/kernel/bpf/verifier.c: 10894
			 * If we have any write marks in env->cur_state, they /Users/rubber/linux/kernel/bpf/verifier.c: 10895
			 * will prevent corresponding reads in the continuation /Users/rubber/linux/kernel/bpf/verifier.c: 10896
			 * from reaching our parent (an explored_state).  Our /Users/rubber/linux/kernel/bpf/verifier.c: 10897
			 * own state will get the read marks recorded, but /Users/rubber/linux/kernel/bpf/verifier.c: 10898
			 * they'll be immediately forgotten as we're pruning /Users/rubber/linux/kernel/bpf/verifier.c: 10899
			 * this state and will pop a new one. /Users/rubber/linux/kernel/bpf/verifier.c: 10900
			/* if previous state reached the exit with precision and /Users/rubber/linux/kernel/bpf/verifier.c: 10904
			 * current state is equivalent to it (except precsion marks) /Users/rubber/linux/kernel/bpf/verifier.c: 10905
			 * the precision needs to be propagated back in /Users/rubber/linux/kernel/bpf/verifier.c: 10906
			 * the current state. /Users/rubber/linux/kernel/bpf/verifier.c: 10907
		/* when new state is not going to be added do not increase miss count. /Users/rubber/linux/kernel/bpf/verifier.c: 10916
		 * Otherwise several loop iterations will remove the state /Users/rubber/linux/kernel/bpf/verifier.c: 10917
		 * recorded earlier. The goal of these heuristics is to have /Users/rubber/linux/kernel/bpf/verifier.c: 10918
		 * states from some iterations of the loop (some in the beginning /Users/rubber/linux/kernel/bpf/verifier.c: 10919
		 * and some at the end) to help pruning. /Users/rubber/linux/kernel/bpf/verifier.c: 10920
		/* heuristic to determine whether this state is beneficial /Users/rubber/linux/kernel/bpf/verifier.c: 10924
		 * to keep checking from state equivalence point of view. /Users/rubber/linux/kernel/bpf/verifier.c: 10925
		 * Higher numbers increase max_states_per_insn and verification time, /Users/rubber/linux/kernel/bpf/verifier.c: 10926
		 * but do not meaningfully decrease insn_processed. /Users/rubber/linux/kernel/bpf/verifier.c: 10927
			/* the state is unlikely to be useful. Remove it to /Users/rubber/linux/kernel/bpf/verifier.c: 10930
			 * speed up verification /Users/rubber/linux/kernel/bpf/verifier.c: 10931
				/* cannot free this state, since parentage chain may /Users/rubber/linux/kernel/bpf/verifier.c: 10944
				 * walk it later. Add it for free_list instead to /Users/rubber/linux/kernel/bpf/verifier.c: 10945
				 * be freed at the end of verification /Users/rubber/linux/kernel/bpf/verifier.c: 10946
	/* There were no equivalent states, remember the current one. /Users/rubber/linux/kernel/bpf/verifier.c: 10968
	 * Technically the current state is not proven to be safe yet, /Users/rubber/linux/kernel/bpf/verifier.c: 10969
	 * but it will either reach outer most bpf_exit (which means it's safe) /Users/rubber/linux/kernel/bpf/verifier.c: 10970
	 * or it will be rejected. When there are no loops the verifier won't be /Users/rubber/linux/kernel/bpf/verifier.c: 10971
	 * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx) /Users/rubber/linux/kernel/bpf/verifier.c: 10972
	 * again on the way to bpf_exit. /Users/rubber/linux/kernel/bpf/verifier.c: 10973
	 * When looping the sl->state.branches will be > 0 and this state /Users/rubber/linux/kernel/bpf/verifier.c: 10974
	 * will not be considered for equivalence until branches == 0. /Users/rubber/linux/kernel/bpf/verifier.c: 10975
	/* connect new state to parentage chain. Current frame needs all /Users/rubber/linux/kernel/bpf/verifier.c: 11002
	 * registers connected. Only r6 - r9 of the callers are alive (pushed /Users/rubber/linux/kernel/bpf/verifier.c: 11003
	 * to the stack implicitly by JITs) so in callers' frames connect just /Users/rubber/linux/kernel/bpf/verifier.c: 11004
	 * r6 - r9 as an optimization. Callers will have r1 - r5 connected to /Users/rubber/linux/kernel/bpf/verifier.c: 11005
	 * the state of the call instruction (with WRITTEN set), and r0 comes /Users/rubber/linux/kernel/bpf/verifier.c: 11006
	 * from callee with its full parentage chain, anyway. /Users/rubber/linux/kernel/bpf/verifier.c: 11007
	/* clear write marks in current state: the writes we did are not writes /Users/rubber/linux/kernel/bpf/verifier.c: 11009
	 * our child did, so they don't screen off its reads from us. /Users/rubber/linux/kernel/bpf/verifier.c: 11010
	 * (There are no read marks in current state, because reads always mark /Users/rubber/linux/kernel/bpf/verifier.c: 11011
	 * their parent and current state never has children yet.  Only /Users/rubber/linux/kernel/bpf/verifier.c: 11012
	 * explored_states can get read marks.) /Users/rubber/linux/kernel/bpf/verifier.c: 11013
/* If an instruction was previously used with particular pointer types, then we /Users/rubber/linux/kernel/bpf/verifier.c: 11056
 * need to be careful to avoid cases such as the below, where it may be ok /Users/rubber/linux/kernel/bpf/verifier.c: 11057
 * for one branch accessing the pointer, but not ok for the other branch: /Users/rubber/linux/kernel/bpf/verifier.c: 11058
 * R1 = sock_ptr /Users/rubber/linux/kernel/bpf/verifier.c: 11060
 * goto X; /Users/rubber/linux/kernel/bpf/verifier.c: 11061
 * ... /Users/rubber/linux/kernel/bpf/verifier.c: 11062
 * R1 = some_other_valid_ptr; /Users/rubber/linux/kernel/bpf/verifier.c: 11063
 * goto X; /Users/rubber/linux/kernel/bpf/verifier.c: 11064
 * ... /Users/rubber/linux/kernel/bpf/verifier.c: 11065
 * R2 = *(u32 *)(R1 + 0); /Users/rubber/linux/kernel/bpf/verifier.c: 11066
			/* check that memory (src_reg + off) is readable, /Users/rubber/linux/kernel/bpf/verifier.c: 11186
			 * the state of dst_reg will be updated by this func /Users/rubber/linux/kernel/bpf/verifier.c: 11187
				/* saw a valid insn /Users/rubber/linux/kernel/bpf/verifier.c: 11198
				 * dst_reg = *(u32 *)(src_reg + off) /Users/rubber/linux/kernel/bpf/verifier.c: 11199
				 * save type to validate intersecting paths /Users/rubber/linux/kernel/bpf/verifier.c: 11200
				/* ABuser program is trying to use the same insn /Users/rubber/linux/kernel/bpf/verifier.c: 11205
				 * dst_reg = *(u32*) (src_reg + off) /Users/rubber/linux/kernel/bpf/verifier.c: 11206
				 * with different pointer types: /Users/rubber/linux/kernel/bpf/verifier.c: 11207
				 * src_reg == ctx in one branch and /Users/rubber/linux/kernel/bpf/verifier.c: 11208
				 * src_reg == stack|map in some other branch. /Users/rubber/linux/kernel/bpf/verifier.c: 11209
				 * Reject it. /Users/rubber/linux/kernel/bpf/verifier.c: 11210
	/* /Users/rubber/linux/kernel/bpf/verifier.c: 11412
	 * Both vmlinux and module each have their own ".data..percpu" /Users/rubber/linux/kernel/bpf/verifier.c: 11413
	 * DATASECs in BTF. So for module's case, we need to skip vmlinux BTF /Users/rubber/linux/kernel/bpf/verifier.c: 11414
	 * types to look at only module's own BTF types. /Users/rubber/linux/kernel/bpf/verifier.c: 11415
	/* /Users/rubber/linux/kernel/bpf/verifier.c: 11603
	 * Validate that trace type programs use preallocated hash maps. /Users/rubber/linux/kernel/bpf/verifier.c: 11604
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 11605
	 * For programs attached to PERF events this is mandatory as the /Users/rubber/linux/kernel/bpf/verifier.c: 11606
	 * perf NMI can hit any arbitrary code sequence. /Users/rubber/linux/kernel/bpf/verifier.c: 11607
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 11608
	 * All other trace types using preallocated hash maps are unsafe as /Users/rubber/linux/kernel/bpf/verifier.c: 11609
	 * well because tracepoint or kprobes can be inside locked regions /Users/rubber/linux/kernel/bpf/verifier.c: 11610
	 * of the memory allocator or at a place where a recursion into the /Users/rubber/linux/kernel/bpf/verifier.c: 11611
	 * memory allocator would see inconsistent state. /Users/rubber/linux/kernel/bpf/verifier.c: 11612
	 * /Users/rubber/linux/kernel/bpf/verifier.c: 11613
	 * On RT enabled kernels run-time allocation of all trace type /Users/rubber/linux/kernel/bpf/verifier.c: 11614
	 * programs is strictly prohibited due to lock type constraints. On /Users/rubber/linux/kernel/bpf/verifier.c: 11615
	 * !RT kernels it is allowed for backwards compatibility reasons for /Users/rubber/linux/kernel/bpf/verifier.c: 11616
	 * now, but warnings are emitted so developers are made aware of /Users/rubber/linux/kernel/bpf/verifier.c: 11617
	 * the unsafety and can fix their programs before this is enforced. /Users/rubber/linux/kernel/bpf/verifier.c: 11618
/* find and rewrite pseudo imm in ld_imm64 instructions: /Users/rubber/linux/kernel/bpf/verifier.c: 11701
 * 1. if it accesses map FD, replace it with actual map pointer. /Users/rubber/linux/kernel/bpf/verifier.c: 11703
 * 2. if it accesses btf_id of a VAR, replace it with pointer to the var. /Users/rubber/linux/kernel/bpf/verifier.c: 11704
 * NOTE: btf_vmlinux is required for converting pseudo btf_id. /Users/rubber/linux/kernel/bpf/verifier.c: 11706
			/* In final convert_pseudo_ld_imm64() step, this is /Users/rubber/linux/kernel/bpf/verifier.c: 11757
			 * converted into regular 64-bit imm load insn. /Users/rubber/linux/kernel/bpf/verifier.c: 11758
			/* hold the map. If the program is rejected by verifier, /Users/rubber/linux/kernel/bpf/verifier.c: 11853
			 * the map will be released by release_maps() or it /Users/rubber/linux/kernel/bpf/verifier.c: 11854
			 * will be used by the valid program until it's unloaded /Users/rubber/linux/kernel/bpf/verifier.c: 11855
			 * and all maps are released in free_used_maps() /Users/rubber/linux/kernel/bpf/verifier.c: 11856
	/* now all pseudo BPF_LD_IMM64 instructions load valid /Users/rubber/linux/kernel/bpf/verifier.c: 11884
	 * 'struct bpf_map *' into a register instead of user map_fd. /Users/rubber/linux/kernel/bpf/verifier.c: 11885
	 * These pointers will be used later by verifier to validate map access. /Users/rubber/linux/kernel/bpf/verifier.c: 11886
/* single env->prog->insni[off] instruction was replaced with the range /Users/rubber/linux/kernel/bpf/verifier.c: 11921
 * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying /Users/rubber/linux/kernel/bpf/verifier.c: 11922
 * [0, off) and [off, end) to new locations, so the patched range stays zero /Users/rubber/linux/kernel/bpf/verifier.c: 11923
	/* aux info at OFF always needs adjustment, no matter fast path /Users/rubber/linux/kernel/bpf/verifier.c: 11935
	 * (cnt == 1) is taken or not. There is no guarantee INSN at OFF is the /Users/rubber/linux/kernel/bpf/verifier.c: 11936
	 * original insn at old prog. /Users/rubber/linux/kernel/bpf/verifier.c: 11937
	/* if j doesn't start exactly at off + cnt, we are just removing /Users/rubber/linux/kernel/bpf/verifier.c: 12026
	 * the front of previous prog /Users/rubber/linux/kernel/bpf/verifier.c: 12027
			/* func_info->insn_off is set after all code rewrites, /Users/rubber/linux/kernel/bpf/verifier.c: 12052
			 * in adjust_btf_func() - no need to adjust /Users/rubber/linux/kernel/bpf/verifier.c: 12053
	/* First live insn doesn't match first live linfo, it needs to "inherit" /Users/rubber/linux/kernel/bpf/verifier.c: 12095
	 * last removed linfo.  prog is already modified, so prog->len == off /Users/rubber/linux/kernel/bpf/verifier.c: 12096
	 * means no live instructions after (tail of the program was removed). /Users/rubber/linux/kernel/bpf/verifier.c: 12097
			/* program may have started in the removed region but /Users/rubber/linux/kernel/bpf/verifier.c: 12121
			 * may not be fully removed /Users/rubber/linux/kernel/bpf/verifier.c: 12122
/* The verifier does more data flow analysis than llvm and will not /Users/rubber/linux/kernel/bpf/verifier.c: 12160
 * explore branches that are dead at run time. Malicious programs can /Users/rubber/linux/kernel/bpf/verifier.c: 12161
 * have dead code too. Therefore replace all dead at-run-time code /Users/rubber/linux/kernel/bpf/verifier.c: 12162
 * with 'ja -1'. /Users/rubber/linux/kernel/bpf/verifier.c: 12163
 * Just nops are not optimal, e.g. if they would sit at the end of the /Users/rubber/linux/kernel/bpf/verifier.c: 12165
 * program and through another bug we would manage to jump there, then /Users/rubber/linux/kernel/bpf/verifier.c: 12166
 * we'd execute beyond program memory otherwise. Returning exception /Users/rubber/linux/kernel/bpf/verifier.c: 12167
 * code also wouldn't work since we can have subprogs where the dead /Users/rubber/linux/kernel/bpf/verifier.c: 12168
 * code could be located. /Users/rubber/linux/kernel/bpf/verifier.c: 12169
			/* NOTE: arg "reg" (the fourth one) is only used for /Users/rubber/linux/kernel/bpf/verifier.c: 12306
			 *       BPF_STX + SRC_OP, so it is safe to pass NULL /Users/rubber/linux/kernel/bpf/verifier.c: 12307
			 *       here. /Users/rubber/linux/kernel/bpf/verifier.c: 12308
		/* Add in an zero-extend instruction if a) the JIT has requested /Users/rubber/linux/kernel/bpf/verifier.c: 12331
		 * it or b) it's a CMPXCHG. /Users/rubber/linux/kernel/bpf/verifier.c: 12332
		 * /Users/rubber/linux/kernel/bpf/verifier.c: 12333
		 * The latter is because: BPF_CMPXCHG always loads a value into /Users/rubber/linux/kernel/bpf/verifier.c: 12334
		 * R0, therefore always zero-extends. However some archs' /Users/rubber/linux/kernel/bpf/verifier.c: 12335
		 * equivalent instruction only does this load when the /Users/rubber/linux/kernel/bpf/verifier.c: 12336
		 * comparison is successful. This detail of CMPXCHG is /Users/rubber/linux/kernel/bpf/verifier.c: 12337
		 * orthogonal to the general zero-extension behaviour of the /Users/rubber/linux/kernel/bpf/verifier.c: 12338
		 * CPU, so it's treated independently of bpf_jit_needs_zext. /Users/rubber/linux/kernel/bpf/verifier.c: 12339
/* convert load instructions that access fields of a context type into a /Users/rubber/linux/kernel/bpf/verifier.c: 12367
 * sequence of instructions that access fields of the underlying structure: /Users/rubber/linux/kernel/bpf/verifier.c: 12368
 *     struct __sk_buff    -> struct sk_buff /Users/rubber/linux/kernel/bpf/verifier.c: 12369
 *     struct bpf_sock_ops -> struct sock /Users/rubber/linux/kernel/bpf/verifier.c: 12370
		/* If the read access is a narrower load of the field, /Users/rubber/linux/kernel/bpf/verifier.c: 12486
		 * convert to a 4/8-byte load, to minimum program type specific /Users/rubber/linux/kernel/bpf/verifier.c: 12487
		 * convert_ctx_access changes. If conversion is successful, /Users/rubber/linux/kernel/bpf/verifier.c: 12488
		 * we will apply proper mask to the result. /Users/rubber/linux/kernel/bpf/verifier.c: 12489
		/* Upon error here we cannot fall back to interpreter but /Users/rubber/linux/kernel/bpf/verifier.c: 12575
		 * need a hard reject of the program. Thus -EFAULT is /Users/rubber/linux/kernel/bpf/verifier.c: 12576
		 * propagated in any case. /Users/rubber/linux/kernel/bpf/verifier.c: 12577
		/* temporarily remember subprog id inside insn instead of /Users/rubber/linux/kernel/bpf/verifier.c: 12585
		 * aux_data, since next loop will split up all insns into funcs /Users/rubber/linux/kernel/bpf/verifier.c: 12586
		/* remember original imm in case JIT fails and fallback /Users/rubber/linux/kernel/bpf/verifier.c: 12589
		 * to interpreter will be needed /Users/rubber/linux/kernel/bpf/verifier.c: 12590
			/* jit (e.g. x86_64) may emit fewer instructions /Users/rubber/linux/kernel/bpf/verifier.c: 12596
			 * if it learns a u32 imm is the same as a u64 imm. /Users/rubber/linux/kernel/bpf/verifier.c: 12597
			 * Force a non zero here. /Users/rubber/linux/kernel/bpf/verifier.c: 12598
		/* bpf_prog_run() doesn't call subprogs directly, /Users/rubber/linux/kernel/bpf/verifier.c: 12617
		 * hence main prog stats include the runtime of subprogs. /Users/rubber/linux/kernel/bpf/verifier.c: 12618
		 * subprogs don't have IDs and not reachable via prog_get_next_id /Users/rubber/linux/kernel/bpf/verifier.c: 12619
		 * func[i]->stats will never be accessed and stays NULL /Users/rubber/linux/kernel/bpf/verifier.c: 12620
		/* Use bpf_prog_F_tag to indicate functions in stack traces. /Users/rubber/linux/kernel/bpf/verifier.c: 12648
		 * Long term would need debug info to populate names /Users/rubber/linux/kernel/bpf/verifier.c: 12649
	/* at this point all bpf functions were successfully JITed /Users/rubber/linux/kernel/bpf/verifier.c: 12677
	 * now populate all bpf_calls with correct addresses and /Users/rubber/linux/kernel/bpf/verifier.c: 12678
	 * run last pass of JIT /Users/rubber/linux/kernel/bpf/verifier.c: 12679
		/* we use the aux data to keep a list of the start addresses /Users/rubber/linux/kernel/bpf/verifier.c: 12696
		 * of the JITed images for each function in the program /Users/rubber/linux/kernel/bpf/verifier.c: 12697
		 * /Users/rubber/linux/kernel/bpf/verifier.c: 12698
		 * for some architectures, such as powerpc64, the imm field /Users/rubber/linux/kernel/bpf/verifier.c: 12699
		 * might not be large enough to hold the offset of the start /Users/rubber/linux/kernel/bpf/verifier.c: 12700
		 * address of the callee's JITed image from __bpf_call_base /Users/rubber/linux/kernel/bpf/verifier.c: 12701
		 * /Users/rubber/linux/kernel/bpf/verifier.c: 12702
		 * in such cases, we can lookup the start address of a callee /Users/rubber/linux/kernel/bpf/verifier.c: 12703
		 * by using its subprog id, available from the off field of /Users/rubber/linux/kernel/bpf/verifier.c: 12704
		 * the call instruction, as an index for this list /Users/rubber/linux/kernel/bpf/verifier.c: 12705
	/* finally lock prog and jit images for all functions and /Users/rubber/linux/kernel/bpf/verifier.c: 12721
	 * populate kallsysm /Users/rubber/linux/kernel/bpf/verifier.c: 12722
	/* Last step: make now unused interpreter insns from main /Users/rubber/linux/kernel/bpf/verifier.c: 12729
	 * prog consistent for later dump requests, so they can /Users/rubber/linux/kernel/bpf/verifier.c: 12730
	 * later look the same as if they were interpreted only. /Users/rubber/linux/kernel/bpf/verifier.c: 12731
	/* We failed JIT'ing, so at this point we need to unregister poke /Users/rubber/linux/kernel/bpf/verifier.c: 12754
	 * descriptors from subprogs, so that kernel is not attempting to /Users/rubber/linux/kernel/bpf/verifier.c: 12755
	 * patch it anymore as we're freeing the subprog JIT memory. /Users/rubber/linux/kernel/bpf/verifier.c: 12756
	/* At this point we're guaranteed that poke descriptors are not /Users/rubber/linux/kernel/bpf/verifier.c: 12762
	 * live anymore. We can just unlink its descriptor table as it's /Users/rubber/linux/kernel/bpf/verifier.c: 12763
	 * released with the main prog. /Users/rubber/linux/kernel/bpf/verifier.c: 12764
		/* When JIT fails the progs with bpf2bpf calls and tail_calls /Users/rubber/linux/kernel/bpf/verifier.c: 12810
		 * have to be rejected, since interpreter doesn't support them yet. /Users/rubber/linux/kernel/bpf/verifier.c: 12811
			/* When JIT fails the progs with callback calls /Users/rubber/linux/kernel/bpf/verifier.c: 12818
			 * have to be rejected, since interpreter doesn't support them yet. /Users/rubber/linux/kernel/bpf/verifier.c: 12819
	/* insn->imm has the btf func_id. Replace it with /Users/rubber/linux/kernel/bpf/verifier.c: 12847
	 * an address (relative to __bpf_base_call). /Users/rubber/linux/kernel/bpf/verifier.c: 12848
/* Do various post-verification rewrites in a single program pass. /Users/rubber/linux/kernel/bpf/verifier.c: 12862
 * These rewrites simplify JIT and interpreter implementations. /Users/rubber/linux/kernel/bpf/verifier.c: 12863
			/* If we tail call into other programs, we /Users/rubber/linux/kernel/bpf/verifier.c: 13013
			 * cannot make any assumptions since they can /Users/rubber/linux/kernel/bpf/verifier.c: 13014
			 * be replaced dynamically during runtime in /Users/rubber/linux/kernel/bpf/verifier.c: 13015
			 * the program array. /Users/rubber/linux/kernel/bpf/verifier.c: 13016
			/* mark bpf_tail_call as different opcode to avoid /Users/rubber/linux/kernel/bpf/verifier.c: 13023
			 * conditional branch in the interpreter for every normal /Users/rubber/linux/kernel/bpf/verifier.c: 13024
			 * call and to prevent accidental JITing by JIT compiler /Users/rubber/linux/kernel/bpf/verifier.c: 13025
			 * that doesn't support bpf_tail_call yet /Users/rubber/linux/kernel/bpf/verifier.c: 13026
			/* instead of changing every JIT dealing with tail_call /Users/rubber/linux/kernel/bpf/verifier.c: 13057
			 * emit two extra insns: /Users/rubber/linux/kernel/bpf/verifier.c: 13058
			 * if (index >= max_entries) goto out; /Users/rubber/linux/kernel/bpf/verifier.c: 13059
			 * index &= array->index_mask; /Users/rubber/linux/kernel/bpf/verifier.c: 13060
			 * to avoid out-of-bounds cpu speculation /Users/rubber/linux/kernel/bpf/verifier.c: 13061
			/* The verifier will process callback_fn as many times as necessary /Users/rubber/linux/kernel/bpf/verifier.c: 13088
			 * with different maps and the register states prepared by /Users/rubber/linux/kernel/bpf/verifier.c: 13089
			 * set_timer_callback_state will be accurate. /Users/rubber/linux/kernel/bpf/verifier.c: 13090
			 * /Users/rubber/linux/kernel/bpf/verifier.c: 13091
			 * The following use case is valid: /Users/rubber/linux/kernel/bpf/verifier.c: 13092
			 *   map1 is shared by prog1, prog2, prog3. /Users/rubber/linux/kernel/bpf/verifier.c: 13093
			 *   prog1 calls bpf_timer_init for some map1 elements /Users/rubber/linux/kernel/bpf/verifier.c: 13094
			 *   prog2 calls bpf_timer_set_callback for some map1 elements. /Users/rubber/linux/kernel/bpf/verifier.c: 13095
			 *     Those that were not bpf_timer_init-ed will return -EINVAL. /Users/rubber/linux/kernel/bpf/verifier.c: 13096
			 *   prog3 calls bpf_timer_start for some map1 elements. /Users/rubber/linux/kernel/bpf/verifier.c: 13097
			 *     Those that were not both bpf_timer_init-ed and /Users/rubber/linux/kernel/bpf/verifier.c: 13098
			 *     bpf_timer_set_callback-ed will return -EINVAL. /Users/rubber/linux/kernel/bpf/verifier.c: 13099
		/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup /Users/rubber/linux/kernel/bpf/verifier.c: 13120
		 * and other inlining handlers are currently limited to 64 bit /Users/rubber/linux/kernel/bpf/verifier.c: 13121
		 * only. /Users/rubber/linux/kernel/bpf/verifier.c: 13122
		/* all functions that have prototype and verifier allowed /Users/rubber/linux/kernel/bpf/verifier.c: 13255
		 * programs to call them, must be real in-kernel functions /Users/rubber/linux/kernel/bpf/verifier.c: 13256
 callsite */, /Users/rubber/linux/kernel/bpf/verifier.c: 13342
 frameno */, /Users/rubber/linux/kernel/bpf/verifier.c: 13343
			/* unlikely verifier bug. abort. /Users/rubber/linux/kernel/bpf/verifier.c: 13370
			 * ret == 0 and ret < 0 are sadly acceptable for /Users/rubber/linux/kernel/bpf/verifier.c: 13371
			 * main() function due to backward compatibility. /Users/rubber/linux/kernel/bpf/verifier.c: 13372
			 * Like socket filter program may be written as: /Users/rubber/linux/kernel/bpf/verifier.c: 13373
			 * int bpf_prog(struct pt_regs *ctx) /Users/rubber/linux/kernel/bpf/verifier.c: 13374
			 * and never dereference that ctx in the program. /Users/rubber/linux/kernel/bpf/verifier.c: 13375
			 * 'struct pt_regs' is a type mismatch for socket /Users/rubber/linux/kernel/bpf/verifier.c: 13376
			 * filter that should be using 'struct __sk_buff'. /Users/rubber/linux/kernel/bpf/verifier.c: 13377
	/* check for NULL is necessary, since cur_state can be freed inside /Users/rubber/linux/kernel/bpf/verifier.c: 13384
	 * do_check() under memory pressure. /Users/rubber/linux/kernel/bpf/verifier.c: 13385
/* Verify all global functions in a BPF program one by one based on their BTF. /Users/rubber/linux/kernel/bpf/verifier.c: 13398
 * All global functions must pass verification. Otherwise the whole program is rejected. /Users/rubber/linux/kernel/bpf/verifier.c: 13399
 * Consider: /Users/rubber/linux/kernel/bpf/verifier.c: 13400
 * int bar(int); /Users/rubber/linux/kernel/bpf/verifier.c: 13401
 * int foo(int f) /Users/rubber/linux/kernel/bpf/verifier.c: 13402
 * { /Users/rubber/linux/kernel/bpf/verifier.c: 13403
 *    return bar(f); /Users/rubber/linux/kernel/bpf/verifier.c: 13404
 * } /Users/rubber/linux/kernel/bpf/verifier.c: 13405
 * int bar(int b) /Users/rubber/linux/kernel/bpf/verifier.c: 13406
 * { /Users/rubber/linux/kernel/bpf/verifier.c: 13407
 *    ... /Users/rubber/linux/kernel/bpf/verifier.c: 13408
 * } /Users/rubber/linux/kernel/bpf/verifier.c: 13409
 * foo() will be verified first for R1=any_scalar_value. During verification it /Users/rubber/linux/kernel/bpf/verifier.c: 13410
 * will be assumed that bar() already verified successfully and call to bar() /Users/rubber/linux/kernel/bpf/verifier.c: 13411
 * from foo() will be checked for type match only. Later bar() will be verified /Users/rubber/linux/kernel/bpf/verifier.c: 13412
 * independently to check that it's safe for R1=any_scalar_value. /Users/rubber/linux/kernel/bpf/verifier.c: 13413
/* list of non-sleepable functions that are otherwise on /Users/rubber/linux/kernel/bpf/verifier.c: 13543
 * ALLOW_ERROR_INJECTION list /Users/rubber/linux/kernel/bpf/verifier.c: 13544
/* Three functions below can be called from sleepable and non-sleepable context. /Users/rubber/linux/kernel/bpf/verifier.c: 13547
 * Assume non-sleepable from bpf safety point of view. /Users/rubber/linux/kernel/bpf/verifier.c: 13548
			/* Cannot fentry/fexit another fentry/fexit program. /Users/rubber/linux/kernel/bpf/verifier.c: 13625
			 * Cannot attach program extension to another extension. /Users/rubber/linux/kernel/bpf/verifier.c: 13626
			 * It's ok to attach fentry/fexit to extension program. /Users/rubber/linux/kernel/bpf/verifier.c: 13627
			/* Program extensions can extend all program types /Users/rubber/linux/kernel/bpf/verifier.c: 13636
			 * except fentry/fexit. The reason is the following. /Users/rubber/linux/kernel/bpf/verifier.c: 13637
			 * The fentry/fexit programs are used for performance /Users/rubber/linux/kernel/bpf/verifier.c: 13638
			 * analysis, stats and can be attached to any program /Users/rubber/linux/kernel/bpf/verifier.c: 13639
			 * type except themselves. When extension program is /Users/rubber/linux/kernel/bpf/verifier.c: 13640
			 * replacing XDP function it is necessary to allow /Users/rubber/linux/kernel/bpf/verifier.c: 13641
			 * performance analysis of all functions. Both original /Users/rubber/linux/kernel/bpf/verifier.c: 13642
			 * XDP program and its program extension. Hence /Users/rubber/linux/kernel/bpf/verifier.c: 13643
			 * attaching fentry/fexit to BPF_PROG_TYPE_EXT is /Users/rubber/linux/kernel/bpf/verifier.c: 13644
			 * allowed. If extending of fentry/fexit was allowed it /Users/rubber/linux/kernel/bpf/verifier.c: 13645
			 * would be possible to create long call chain /Users/rubber/linux/kernel/bpf/verifier.c: 13646
			 * fentry->extension->fentry->extension beyond /Users/rubber/linux/kernel/bpf/verifier.c: 13647
			 * reasonable stack size. Hence extending fentry is not /Users/rubber/linux/kernel/bpf/verifier.c: 13648
			 * allowed. /Users/rubber/linux/kernel/bpf/verifier.c: 13649
				/* fentry/fexit/fmod_ret progs can be sleepable only if they are /Users/rubber/linux/kernel/bpf/verifier.c: 13753
				 * attached to ALLOW_ERROR_INJECTION and are not in denylist. /Users/rubber/linux/kernel/bpf/verifier.c: 13754
				/* LSM progs check that they are attached to bpf_lsm_*() funcs. /Users/rubber/linux/kernel/bpf/verifier.c: 13761
				 * Only some of them are sleepable. /Users/rubber/linux/kernel/bpf/verifier.c: 13762
		/* to make freplace equivalent to their targets, they need to /Users/rubber/linux/kernel/bpf/verifier.c: 13842
		 * inherit env->ops and expected_attach_type for the rest of the /Users/rubber/linux/kernel/bpf/verifier.c: 13843
		 * verification /Users/rubber/linux/kernel/bpf/verifier.c: 13844
	/* 'struct bpf_verifier_env' can be global, but since it's not small, /Users/rubber/linux/kernel/bpf/verifier.c: 13909
	 * allocate/free it every time bpf_check() is called /Users/rubber/linux/kernel/bpf/verifier.c: 13910
		/* user requested verbose verifier output /Users/rubber/linux/kernel/bpf/verifier.c: 13937
		 * and supplied buffer to store the verification trace /Users/rubber/linux/kernel/bpf/verifier.c: 13938
	/* do 32-bit optimization after insn patching has done so those patched /Users/rubber/linux/kernel/bpf/verifier.c: 14043
	 * insns could be handled correctly. /Users/rubber/linux/kernel/bpf/verifier.c: 14044
		/* program is valid. Convert pseudo bpf_ld_imm64 into generic /Users/rubber/linux/kernel/bpf/verifier.c: 14099
		 * bpf_ld_imm64 instructions /Users/rubber/linux/kernel/bpf/verifier.c: 14100
		/* if we didn't copy map pointers into bpf_prog_info, release /Users/rubber/linux/kernel/bpf/verifier.c: 14109
		 * them now. Otherwise free_used_maps() will release them. /Users/rubber/linux/kernel/bpf/verifier.c: 14110
	/* extension progs temporarily inherit the attach_type of their targets /Users/rubber/linux/kernel/bpf/verifier.c: 14116
	   for verification purposes, so set it back to zero before returning /Users/rubber/linux/kernel/bpf/verifier.c: 14117
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/task_iter.c: 1
	/* The first field must be struct bpf_iter_seq_task_common. /Users/rubber/linux/kernel/bpf/task_iter.c: 17
	 * this is assumed by {init, fini}_seq_pidns() callback functions. /Users/rubber/linux/kernel/bpf/task_iter.c: 18
	/* The first field must be struct bpf_iter_seq_task_common. /Users/rubber/linux/kernel/bpf/task_iter.c: 128
	 * this is assumed by {init, fini}_seq_pidns() callback functions. /Users/rubber/linux/kernel/bpf/task_iter.c: 129
	/* If this function returns a non-NULL file object, /Users/rubber/linux/kernel/bpf/task_iter.c: 145
	 * it held a reference to the task/file. /Users/rubber/linux/kernel/bpf/task_iter.c: 146
	 * Otherwise, it does not hold any reference. /Users/rubber/linux/kernel/bpf/task_iter.c: 147
	/* The first field must be struct bpf_iter_seq_task_common. /Users/rubber/linux/kernel/bpf/task_iter.c: 290
	 * this is assumed by {init, fini}_seq_pidns() callback functions. /Users/rubber/linux/kernel/bpf/task_iter.c: 291
	/* If this function returns a non-NULL vma, it holds a reference to /Users/rubber/linux/kernel/bpf/task_iter.c: 316
	 * the task_struct, and holds read lock on vma->mm->mmap_lock. /Users/rubber/linux/kernel/bpf/task_iter.c: 317
	 * If this function returns NULL, it does not hold any reference or /Users/rubber/linux/kernel/bpf/task_iter.c: 318
	 * lock. /Users/rubber/linux/kernel/bpf/task_iter.c: 319
		/* In case of lock contention, drop mmap_lock to unblock /Users/rubber/linux/kernel/bpf/task_iter.c: 324
		 * the writer. /Users/rubber/linux/kernel/bpf/task_iter.c: 325
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 326
		 * After relock, call find(mm, prev_vm_end - 1) to find /Users/rubber/linux/kernel/bpf/task_iter.c: 327
		 * new vma to process. /Users/rubber/linux/kernel/bpf/task_iter.c: 328
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 329
		 *   +------+------+-----------+ /Users/rubber/linux/kernel/bpf/task_iter.c: 330
		 *   | VMA1 | VMA2 | VMA3      | /Users/rubber/linux/kernel/bpf/task_iter.c: 331
		 *   +------+------+-----------+ /Users/rubber/linux/kernel/bpf/task_iter.c: 332
		 *   |      |      |           | /Users/rubber/linux/kernel/bpf/task_iter.c: 333
		 *  4k     8k     16k         400k /Users/rubber/linux/kernel/bpf/task_iter.c: 334
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 335
		 * For example, curr_vma == VMA2. Before unlock, we set /Users/rubber/linux/kernel/bpf/task_iter.c: 336
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 337
		 *    prev_vm_start = 8k /Users/rubber/linux/kernel/bpf/task_iter.c: 338
		 *    prev_vm_end   = 16k /Users/rubber/linux/kernel/bpf/task_iter.c: 339
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 340
		 * There are a few cases: /Users/rubber/linux/kernel/bpf/task_iter.c: 341
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 342
		 * 1) VMA2 is freed, but VMA3 exists. /Users/rubber/linux/kernel/bpf/task_iter.c: 343
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 344
		 *    find_vma() will return VMA3, just process VMA3. /Users/rubber/linux/kernel/bpf/task_iter.c: 345
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 346
		 * 2) VMA2 still exists. /Users/rubber/linux/kernel/bpf/task_iter.c: 347
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 348
		 *    find_vma() will return VMA2, process VMA2->next. /Users/rubber/linux/kernel/bpf/task_iter.c: 349
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 350
		 * 3) no more vma in this mm. /Users/rubber/linux/kernel/bpf/task_iter.c: 351
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 352
		 *    Process the next task. /Users/rubber/linux/kernel/bpf/task_iter.c: 353
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 354
		 * 4) find_vma() returns a different vma, VMA2'. /Users/rubber/linux/kernel/bpf/task_iter.c: 355
		 * /Users/rubber/linux/kernel/bpf/task_iter.c: 356
		 *    4.1) If VMA2 covers same range as VMA2', skip VMA2', /Users/rubber/linux/kernel/bpf/task_iter.c: 357
		 *         because we already covered the range; /Users/rubber/linux/kernel/bpf/task_iter.c: 358
		 *    4.2) VMA2 and VMA2' covers different ranges, process /Users/rubber/linux/kernel/bpf/task_iter.c: 359
		 *         VMA2'. /Users/rubber/linux/kernel/bpf/task_iter.c: 360
			/* Found the same tid, which means the user space /Users/rubber/linux/kernel/bpf/task_iter.c: 385
			 * finished data in previous buffer and read more. /Users/rubber/linux/kernel/bpf/task_iter.c: 386
			 * We dropped mmap_lock before returning to user /Users/rubber/linux/kernel/bpf/task_iter.c: 387
			 * space, so it is necessary to use find_vma() to /Users/rubber/linux/kernel/bpf/task_iter.c: 388
			 * find the next vma to process. /Users/rubber/linux/kernel/bpf/task_iter.c: 389
		/* We dropped mmap_lock so it is necessary to use find_vma /Users/rubber/linux/kernel/bpf/task_iter.c: 409
		 * to find the next vma. This is similar to the  mechanism /Users/rubber/linux/kernel/bpf/task_iter.c: 410
		 * in show_smaps_rollup(). /Users/rubber/linux/kernel/bpf/task_iter.c: 411
		/* info->vma has not been seen by the BPF program. If the /Users/rubber/linux/kernel/bpf/task_iter.c: 505
		 * user space reads more, task_vma_seq_get_next should /Users/rubber/linux/kernel/bpf/task_iter.c: 506
		 * return this vma again. Set prev_vm_start to ~0UL, /Users/rubber/linux/kernel/bpf/task_iter.c: 507
		 * so that we don't skip the vma returned by the next /Users/rubber/linux/kernel/bpf/task_iter.c: 508
		 * find_vma() (case task_vma_iter_find_vma in /Users/rubber/linux/kernel/bpf/task_iter.c: 509
		 * task_vma_seq_get_next()). /Users/rubber/linux/kernel/bpf/task_iter.c: 510
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/cgroup.c: 1
 * Functions to manage eBPF programs attached to cgroups /Users/rubber/linux/kernel/bpf/cgroup.c: 3
 * Copyright (c) 2016 Daniel Mack /Users/rubber/linux/kernel/bpf/cgroup.c: 5
/* Called when bpf_cgroup_link is auto-detached from dying cgroup. /Users/rubber/linux/kernel/bpf/cgroup.c: 92
 * It drops cgroup and bpf_prog refcounts, and marks bpf_link as defunct. It /Users/rubber/linux/kernel/bpf/cgroup.c: 93
 * doesn't free link memory, which will eventually be done by bpf_link's /Users/rubber/linux/kernel/bpf/cgroup.c: 94
 * release() callback, when its last FD is closed. /Users/rubber/linux/kernel/bpf/cgroup.c: 95
 * cgroup_bpf_release() - put references of all bpf programs and /Users/rubber/linux/kernel/bpf/cgroup.c: 104
 *                        release all cgroup bpf data /Users/rubber/linux/kernel/bpf/cgroup.c: 105
 * @work: work structure embedded into the cgroup to modify /Users/rubber/linux/kernel/bpf/cgroup.c: 106
 * cgroup_bpf_release_fn() - callback used to schedule releasing /Users/rubber/linux/kernel/bpf/cgroup.c: 154
 *                           of bpf cgroup data /Users/rubber/linux/kernel/bpf/cgroup.c: 155
 * @ref: percpu ref counter structure /Users/rubber/linux/kernel/bpf/cgroup.c: 156
/* Get underlying bpf_prog of bpf_prog_list entry, regardless if it's through /Users/rubber/linux/kernel/bpf/cgroup.c: 166
 * link or direct prog. /Users/rubber/linux/kernel/bpf/cgroup.c: 167
/* count number of elements in the list. /Users/rubber/linux/kernel/bpf/cgroup.c: 178
 * it's slow but the list cannot be long /Users/rubber/linux/kernel/bpf/cgroup.c: 179
/* if parent has non-overridable prog attached, /Users/rubber/linux/kernel/bpf/cgroup.c: 194
 * disallow attaching new programs to the descendent cgroup. /Users/rubber/linux/kernel/bpf/cgroup.c: 195
 * if parent has overridable or multi-prog, allow attaching /Users/rubber/linux/kernel/bpf/cgroup.c: 196
/* compute a chain of effective programs for a given cgroup: /Users/rubber/linux/kernel/bpf/cgroup.c: 221
 * start from the list of programs in this cgroup and add /Users/rubber/linux/kernel/bpf/cgroup.c: 222
 * all parent programs. /Users/rubber/linux/kernel/bpf/cgroup.c: 223
 * Note that parent's F_ALLOW_OVERRIDE-type program is yielding /Users/rubber/linux/kernel/bpf/cgroup.c: 224
 * to programs in this cgroup /Users/rubber/linux/kernel/bpf/cgroup.c: 225
	/* free prog array after grace period, since __cgroup_bpf_run_*() /Users/rubber/linux/kernel/bpf/cgroup.c: 277
	 * might be still walking the array /Users/rubber/linux/kernel/bpf/cgroup.c: 278
 * cgroup_bpf_inherit() - inherit effective programs from parent /Users/rubber/linux/kernel/bpf/cgroup.c: 284
 * @cgrp: the cgroup to modify /Users/rubber/linux/kernel/bpf/cgroup.c: 285
/* has to use marco instead of const int, since compiler thinks /Users/rubber/linux/kernel/bpf/cgroup.c: 289
 * that array below is variable length /Users/rubber/linux/kernel/bpf/cgroup.c: 290
	/* oom while computing effective. Free all computed effective arrays /Users/rubber/linux/kernel/bpf/cgroup.c: 367
	 * since they were not activated /Users/rubber/linux/kernel/bpf/cgroup.c: 368
 * __cgroup_bpf_attach() - Attach the program or the link to a cgroup, and /Users/rubber/linux/kernel/bpf/cgroup.c: 421
 *                         propagate the change to descendants /Users/rubber/linux/kernel/bpf/cgroup.c: 422
 * @cgrp: The cgroup which descendants to traverse /Users/rubber/linux/kernel/bpf/cgroup.c: 423
 * @prog: A program to attach /Users/rubber/linux/kernel/bpf/cgroup.c: 424
 * @link: A link to attach /Users/rubber/linux/kernel/bpf/cgroup.c: 425
 * @replace_prog: Previously attached program to replace if BPF_F_REPLACE is set /Users/rubber/linux/kernel/bpf/cgroup.c: 426
 * @type: Type of attach operation /Users/rubber/linux/kernel/bpf/cgroup.c: 427
 * @flags: Option flags /Users/rubber/linux/kernel/bpf/cgroup.c: 428
 * Exactly one of @prog or @link can be non-null. /Users/rubber/linux/kernel/bpf/cgroup.c: 430
 * Must be called with cgroup_mutex held. /Users/rubber/linux/kernel/bpf/cgroup.c: 431
		/* Disallow attaching non-overridable on top /Users/rubber/linux/kernel/bpf/cgroup.c: 468
		 * of existing overridable in this cgroup. /Users/rubber/linux/kernel/bpf/cgroup.c: 469
		 * Disallow attaching multi-prog if overridable or none /Users/rubber/linux/kernel/bpf/cgroup.c: 470
/* Swap updated BPF program for given link in effective program arrays across /Users/rubber/linux/kernel/bpf/cgroup.c: 540
 * all descendant cgroups. This function is guaranteed to succeed. /Users/rubber/linux/kernel/bpf/cgroup.c: 541
 * __cgroup_bpf_replace() - Replace link's program and propagate the change /Users/rubber/linux/kernel/bpf/cgroup.c: 586
 *                          to descendants /Users/rubber/linux/kernel/bpf/cgroup.c: 587
 * @cgrp: The cgroup which descendants to traverse /Users/rubber/linux/kernel/bpf/cgroup.c: 588
 * @link: A link for which to replace BPF program /Users/rubber/linux/kernel/bpf/cgroup.c: 589
 * @type: Type of attach operation /Users/rubber/linux/kernel/bpf/cgroup.c: 590
 * Must be called with cgroup_mutex held. /Users/rubber/linux/kernel/bpf/cgroup.c: 592
		/* to maintain backward compatibility NONE and OVERRIDE cgroups /Users/rubber/linux/kernel/bpf/cgroup.c: 664
		 * allow detaching with invalid FD (prog==NULL) in legacy mode /Users/rubber/linux/kernel/bpf/cgroup.c: 665
		/* to detach MULTI prog the user has to specify valid FD /Users/rubber/linux/kernel/bpf/cgroup.c: 671
		 * of the program or link to be detached /Users/rubber/linux/kernel/bpf/cgroup.c: 672
 * __cgroup_bpf_detach() - Detach the program or link from a cgroup, and /Users/rubber/linux/kernel/bpf/cgroup.c: 685
 *                         propagate the change to descendants /Users/rubber/linux/kernel/bpf/cgroup.c: 686
 * @cgrp: The cgroup which descendants to traverse /Users/rubber/linux/kernel/bpf/cgroup.c: 687
 * @prog: A program to detach or NULL /Users/rubber/linux/kernel/bpf/cgroup.c: 688
 * @link: A link to detach or NULL /Users/rubber/linux/kernel/bpf/cgroup.c: 689
 * @type: Type of detach operation /Users/rubber/linux/kernel/bpf/cgroup.c: 690
 * At most one of @prog or @link can be non-NULL. /Users/rubber/linux/kernel/bpf/cgroup.c: 692
 * Must be called with cgroup_mutex held. /Users/rubber/linux/kernel/bpf/cgroup.c: 693
	/* link might have been auto-detached by dying cgroup already, /Users/rubber/linux/kernel/bpf/cgroup.c: 885
	 * in that case our work is done here /Users/rubber/linux/kernel/bpf/cgroup.c: 886
 * __cgroup_bpf_run_filter_skb() - Run a program for packet filtering /Users/rubber/linux/kernel/bpf/cgroup.c: 1031
 * @sk: The socket sending or receiving traffic /Users/rubber/linux/kernel/bpf/cgroup.c: 1032
 * @skb: The skb that is being sent or received /Users/rubber/linux/kernel/bpf/cgroup.c: 1033
 * @type: The type of program to be exectuted /Users/rubber/linux/kernel/bpf/cgroup.c: 1034
 * If no socket is passed, or the socket is not of type INET or INET6, /Users/rubber/linux/kernel/bpf/cgroup.c: 1036
 * this function does nothing and returns 0. /Users/rubber/linux/kernel/bpf/cgroup.c: 1037
 * The program type passed in via @type must be suitable for network /Users/rubber/linux/kernel/bpf/cgroup.c: 1039
 * filtering. No further check is performed to assert that. /Users/rubber/linux/kernel/bpf/cgroup.c: 1040
 * For egress packets, this function can return: /Users/rubber/linux/kernel/bpf/cgroup.c: 1042
 *   NET_XMIT_SUCCESS    (0)	- continue with packet output /Users/rubber/linux/kernel/bpf/cgroup.c: 1043
 *   NET_XMIT_DROP       (1)	- drop packet and notify TCP to call cwr /Users/rubber/linux/kernel/bpf/cgroup.c: 1044
 *   NET_XMIT_CN         (2)	- continue with packet output and notify TCP /Users/rubber/linux/kernel/bpf/cgroup.c: 1045
 *				  to call cwr /Users/rubber/linux/kernel/bpf/cgroup.c: 1046
 *   -EPERM			- drop packet /Users/rubber/linux/kernel/bpf/cgroup.c: 1047
 * For ingress packets, this function will return -EPERM if any /Users/rubber/linux/kernel/bpf/cgroup.c: 1049
 * attached program was found and if it returned != 1 during execution. /Users/rubber/linux/kernel/bpf/cgroup.c: 1050
 * Otherwise 0 is returned. /Users/rubber/linux/kernel/bpf/cgroup.c: 1051
 * __cgroup_bpf_run_filter_sk() - Run a program on a sock /Users/rubber/linux/kernel/bpf/cgroup.c: 1094
 * @sk: sock structure to manipulate /Users/rubber/linux/kernel/bpf/cgroup.c: 1095
 * @type: The type of program to be exectuted /Users/rubber/linux/kernel/bpf/cgroup.c: 1096
 * socket is passed is expected to be of type INET or INET6. /Users/rubber/linux/kernel/bpf/cgroup.c: 1098
 * The program type passed in via @type must be suitable for sock /Users/rubber/linux/kernel/bpf/cgroup.c: 1100
 * filtering. No further check is performed to assert that. /Users/rubber/linux/kernel/bpf/cgroup.c: 1101
 * This function will return %-EPERM if any if an attached program was found /Users/rubber/linux/kernel/bpf/cgroup.c: 1103
 * and if it returned != 1 during execution. In all other cases, 0 is returned. /Users/rubber/linux/kernel/bpf/cgroup.c: 1104
 * __cgroup_bpf_run_filter_sock_addr() - Run a program on a sock and /Users/rubber/linux/kernel/bpf/cgroup.c: 1118
 *                                       provided by user sockaddr /Users/rubber/linux/kernel/bpf/cgroup.c: 1119
 * @sk: sock struct that will use sockaddr /Users/rubber/linux/kernel/bpf/cgroup.c: 1120
 * @uaddr: sockaddr struct provided by user /Users/rubber/linux/kernel/bpf/cgroup.c: 1121
 * @type: The type of program to be exectuted /Users/rubber/linux/kernel/bpf/cgroup.c: 1122
 * @t_ctx: Pointer to attach type specific context /Users/rubber/linux/kernel/bpf/cgroup.c: 1123
 * @flags: Pointer to u32 which contains higher bits of BPF program /Users/rubber/linux/kernel/bpf/cgroup.c: 1124
 *         return value (OR'ed together). /Users/rubber/linux/kernel/bpf/cgroup.c: 1125
 * socket is expected to be of type INET or INET6. /Users/rubber/linux/kernel/bpf/cgroup.c: 1127
 * This function will return %-EPERM if an attached program is found and /Users/rubber/linux/kernel/bpf/cgroup.c: 1129
 * returned value != 1 during execution. In all other cases, 0 is returned. /Users/rubber/linux/kernel/bpf/cgroup.c: 1130
	/* Check socket family since not all sockets represent network /Users/rubber/linux/kernel/bpf/cgroup.c: 1147
	 * endpoint (e.g. AF_UNIX). /Users/rubber/linux/kernel/bpf/cgroup.c: 1148
 * __cgroup_bpf_run_filter_sock_ops() - Run a program on a sock /Users/rubber/linux/kernel/bpf/cgroup.c: 1167
 * @sk: socket to get cgroup from /Users/rubber/linux/kernel/bpf/cgroup.c: 1168
 * @sock_ops: bpf_sock_ops_kern struct to pass to program. Contains /Users/rubber/linux/kernel/bpf/cgroup.c: 1169
 * sk with connection information (IP addresses, etc.) May not contain /Users/rubber/linux/kernel/bpf/cgroup.c: 1170
 * cgroup info if it is a req sock. /Users/rubber/linux/kernel/bpf/cgroup.c: 1171
 * @type: The type of program to be exectuted /Users/rubber/linux/kernel/bpf/cgroup.c: 1172
 * socket passed is expected to be of type INET or INET6. /Users/rubber/linux/kernel/bpf/cgroup.c: 1174
 * The program type passed in via @type must be suitable for sock_ops /Users/rubber/linux/kernel/bpf/cgroup.c: 1176
 * filtering. No further check is performed to assert that. /Users/rubber/linux/kernel/bpf/cgroup.c: 1177
 * This function will return %-EPERM if any if an attached program was found /Users/rubber/linux/kernel/bpf/cgroup.c: 1179
 * and if it returned != 1 during execution. In all other cases, 0 is returned. /Users/rubber/linux/kernel/bpf/cgroup.c: 1180
 * __cgroup_bpf_run_filter_sysctl - Run a program on sysctl /Users/rubber/linux/kernel/bpf/cgroup.c: 1277
 * @head: sysctl table header /Users/rubber/linux/kernel/bpf/cgroup.c: 1279
 * @table: sysctl table /Users/rubber/linux/kernel/bpf/cgroup.c: 1280
 * @write: sysctl is being read (= 0) or written (= 1) /Users/rubber/linux/kernel/bpf/cgroup.c: 1281
 * @buf: pointer to buffer (in and out) /Users/rubber/linux/kernel/bpf/cgroup.c: 1282
 * @pcount: value-result argument: value is size of buffer pointed to by @buf, /Users/rubber/linux/kernel/bpf/cgroup.c: 1283
 *	result is size of @new_buf if program set new value, initial value /Users/rubber/linux/kernel/bpf/cgroup.c: 1284
 *	otherwise /Users/rubber/linux/kernel/bpf/cgroup.c: 1285
 * @ppos: value-result argument: value is position at which read from or write /Users/rubber/linux/kernel/bpf/cgroup.c: 1286
 *	to sysctl is happening, result is new position if program overrode it, /Users/rubber/linux/kernel/bpf/cgroup.c: 1287
 *	initial value otherwise /Users/rubber/linux/kernel/bpf/cgroup.c: 1288
 * @type: type of program to be executed /Users/rubber/linux/kernel/bpf/cgroup.c: 1289
 * Program is run when sysctl is being accessed, either read or written, and /Users/rubber/linux/kernel/bpf/cgroup.c: 1291
 * can allow or deny such access. /Users/rubber/linux/kernel/bpf/cgroup.c: 1292
 * This function will return %-EPERM if an attached program is found and /Users/rubber/linux/kernel/bpf/cgroup.c: 1294
 * returned value != 1 during execution. In all other cases 0 is returned. /Users/rubber/linux/kernel/bpf/cgroup.c: 1295
		/* BPF program should be able to override new value with a /Users/rubber/linux/kernel/bpf/cgroup.c: 1325
		 * buffer bigger than provided by user. /Users/rubber/linux/kernel/bpf/cgroup.c: 1326
		/* We don't expose optvals that are greater than PAGE_SIZE /Users/rubber/linux/kernel/bpf/cgroup.c: 1378
		 * to the BPF program. /Users/rubber/linux/kernel/bpf/cgroup.c: 1379
		/* When the optval fits into BPF_SOCKOPT_KERN_BUF_SIZE /Users/rubber/linux/kernel/bpf/cgroup.c: 1385
		 * bytes avoid the cost of kzalloc. /Users/rubber/linux/kernel/bpf/cgroup.c: 1386
	/* Opportunistic check to see whether we have any BPF program /Users/rubber/linux/kernel/bpf/cgroup.c: 1429
	 * attached to the hook so we don't waste time allocating /Users/rubber/linux/kernel/bpf/cgroup.c: 1430
	 * memory and locking the socket. /Users/rubber/linux/kernel/bpf/cgroup.c: 1431
	/* Allocate a bit more than the initial user buffer for /Users/rubber/linux/kernel/bpf/cgroup.c: 1436
	 * BPF program. The canonical use case is overriding /Users/rubber/linux/kernel/bpf/cgroup.c: 1437
	 * TCP_CONGESTION(nv) to TCP_CONGESTION(cubic). /Users/rubber/linux/kernel/bpf/cgroup.c: 1438
		/* optlen == 0 from BPF indicates that we should /Users/rubber/linux/kernel/bpf/cgroup.c: 1477
		 * use original userspace data. /Users/rubber/linux/kernel/bpf/cgroup.c: 1478
			/* We've used bpf_sockopt_kern->buf as an intermediary /Users/rubber/linux/kernel/bpf/cgroup.c: 1482
			 * storage, but the BPF program indicates that we need /Users/rubber/linux/kernel/bpf/cgroup.c: 1483
			 * to pass this data to the kernel setsockopt handler. /Users/rubber/linux/kernel/bpf/cgroup.c: 1484
			 * No way to export on-stack buf, have to allocate a /Users/rubber/linux/kernel/bpf/cgroup.c: 1485
			 * new buffer. /Users/rubber/linux/kernel/bpf/cgroup.c: 1486
	/* Opportunistic check to see whether we have any BPF program /Users/rubber/linux/kernel/bpf/cgroup.c: 1525
	 * attached to the hook so we don't waste time allocating /Users/rubber/linux/kernel/bpf/cgroup.c: 1526
	 * memory and locking the socket. /Users/rubber/linux/kernel/bpf/cgroup.c: 1527
		/* If kernel getsockopt finished successfully, /Users/rubber/linux/kernel/bpf/cgroup.c: 1539
		 * copy whatever was returned to the user back /Users/rubber/linux/kernel/bpf/cgroup.c: 1540
		 * into our temporary buffer. Set optlen to the /Users/rubber/linux/kernel/bpf/cgroup.c: 1541
		 * one that kernel returned as well to let /Users/rubber/linux/kernel/bpf/cgroup.c: 1542
		 * BPF programs inspect the value. /Users/rubber/linux/kernel/bpf/cgroup.c: 1543
	/* BPF programs only allowed to set retval to 0, not some /Users/rubber/linux/kernel/bpf/cgroup.c: 1578
	 * arbitrary value. /Users/rubber/linux/kernel/bpf/cgroup.c: 1579
	/* Note that __cgroup_bpf_run_filter_getsockopt doesn't copy /Users/rubber/linux/kernel/bpf/cgroup.c: 1617
	 * user data back into BPF buffer when reval != 0. This is /Users/rubber/linux/kernel/bpf/cgroup.c: 1618
	 * done as an optimization to avoid extra copy, assuming /Users/rubber/linux/kernel/bpf/cgroup.c: 1619
	 * kernel won't populate the data in case of an error. /Users/rubber/linux/kernel/bpf/cgroup.c: 1620
	 * Here we always pass the data and memset() should /Users/rubber/linux/kernel/bpf/cgroup.c: 1621
	 * be called if that data shouldn't be "exported". /Users/rubber/linux/kernel/bpf/cgroup.c: 1622
	/* BPF programs only allowed to set retval to 0, not some /Users/rubber/linux/kernel/bpf/cgroup.c: 1633
	 * arbitrary value. /Users/rubber/linux/kernel/bpf/cgroup.c: 1634
	/* BPF programs can shrink the buffer, export the modifications. /Users/rubber/linux/kernel/bpf/cgroup.c: 1639
		/* ppos is a pointer so it should be accessed via indirect /Users/rubber/linux/kernel/bpf/cgroup.c: 1864
		 * loads and stores. Also for stores additional temporary /Users/rubber/linux/kernel/bpf/cgroup.c: 1865
		 * register is used since neither src_reg nor dst_reg can be /Users/rubber/linux/kernel/bpf/cgroup.c: 1866
		 * overridden. /Users/rubber/linux/kernel/bpf/cgroup.c: 1867
	/* Nothing to do for sockopt argument. The data is kzalloc'ated. /Users/rubber/linux/kernel/bpf/cgroup.c: 2080
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/bpf/cpumap.c: 1
/* bpf/cpumap.c /Users/rubber/linux/kernel/bpf/cpumap.c: 2
 * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc. /Users/rubber/linux/kernel/bpf/cpumap.c: 4
/* The 'cpumap' is primarily used as a backend map for XDP BPF helper /Users/rubber/linux/kernel/bpf/cpumap.c: 7
 * call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'. /Users/rubber/linux/kernel/bpf/cpumap.c: 8
 * Unlike devmap which redirects XDP frames out another NIC device, /Users/rubber/linux/kernel/bpf/cpumap.c: 10
 * this map type redirects raw XDP frames to another CPU.  The remote /Users/rubber/linux/kernel/bpf/cpumap.c: 11
 * CPU will do SKB-allocation and call the normal network stack. /Users/rubber/linux/kernel/bpf/cpumap.c: 12
 * This is a scalability and isolation mechanism, that allow /Users/rubber/linux/kernel/bpf/cpumap.c: 14
 * separating the early driver network XDP layer, from the rest of the /Users/rubber/linux/kernel/bpf/cpumap.c: 15
 * netstack, and assigning dedicated CPUs for this stage.  This /Users/rubber/linux/kernel/bpf/cpumap.c: 16
 * basically allows for 10G wirespeed pre-filtering via bpf. /Users/rubber/linux/kernel/bpf/cpumap.c: 17
/* General idea: XDP packets getting XDP redirected to another CPU, /Users/rubber/linux/kernel/bpf/cpumap.c: 34
 * will maximum be stored/queued for one driver ->poll() call.  It is /Users/rubber/linux/kernel/bpf/cpumap.c: 35
 * guaranteed that queueing the frame and the flush operation happen on /Users/rubber/linux/kernel/bpf/cpumap.c: 36
 * same CPU.  Thus, cpu_map_flush operation can deduct via this_cpu_ptr() /Users/rubber/linux/kernel/bpf/cpumap.c: 37
 * which queue in bpf_cpu_map_entry contains packets. /Users/rubber/linux/kernel/bpf/cpumap.c: 38
	/* Wait for flush in __cpu_map_entry_free(), via full RCU barrier, /Users/rubber/linux/kernel/bpf/cpumap.c: 136
	 * as it waits until all in-flight call_rcu() callbacks complete. /Users/rubber/linux/kernel/bpf/cpumap.c: 137
	/* The tear-down procedure should have made sure that queue is /Users/rubber/linux/kernel/bpf/cpumap.c: 147
	 * empty.  See __cpu_map_entry_replace() and work-queue /Users/rubber/linux/kernel/bpf/cpumap.c: 148
	 * invoked cpu_map_kthread_stop(). Catch any broken behaviour /Users/rubber/linux/kernel/bpf/cpumap.c: 149
	 * gracefully and warn once. /Users/rubber/linux/kernel/bpf/cpumap.c: 150
	/* When kthread gives stop order, then rcpu have been disconnected /Users/rubber/linux/kernel/bpf/cpumap.c: 304
	 * from map, thus no new packets can enter. Remaining in-flight /Users/rubber/linux/kernel/bpf/cpumap.c: 305
	 * per CPU stored packets are flushed to this queue.  Wait honoring /Users/rubber/linux/kernel/bpf/cpumap.c: 306
	 * kthread_stop signal until queue is empty. /Users/rubber/linux/kernel/bpf/cpumap.c: 307
		/* /Users/rubber/linux/kernel/bpf/cpumap.c: 332
		 * The bpf_cpu_map_entry is single consumer, with this /Users/rubber/linux/kernel/bpf/cpumap.c: 333
		 * kthread CPU pinned. Lockless access to ptr_ring /Users/rubber/linux/kernel/bpf/cpumap.c: 334
		 * consume side valid as no-resize allowed of queue. /Users/rubber/linux/kernel/bpf/cpumap.c: 335
			/* Bring struct page memory area to curr CPU. Read by /Users/rubber/linux/kernel/bpf/cpumap.c: 354
			 * build_skb_around via page_is_pfmemalloc(), and when /Users/rubber/linux/kernel/bpf/cpumap.c: 355
			 * freed written by page_frag_free call. /Users/rubber/linux/kernel/bpf/cpumap.c: 356
	/* This cpu_map_entry have been disconnected from map and one /Users/rubber/linux/kernel/bpf/cpumap.c: 497
	 * RCU grace-period have elapsed.  Thus, XDP cannot queue any /Users/rubber/linux/kernel/bpf/cpumap.c: 498
	 * new packets and cannot change/set flush_needed that can /Users/rubber/linux/kernel/bpf/cpumap.c: 499
	 * find this entry. /Users/rubber/linux/kernel/bpf/cpumap.c: 500
/* After xchg pointer to bpf_cpu_map_entry, use the call_rcu() to /Users/rubber/linux/kernel/bpf/cpumap.c: 509
 * ensure any driver rcu critical sections have completed, but this /Users/rubber/linux/kernel/bpf/cpumap.c: 510
 * does not guarantee a flush has happened yet. Because driver side /Users/rubber/linux/kernel/bpf/cpumap.c: 511
 * rcu_read_lock/unlock only protects the running XDP program.  The /Users/rubber/linux/kernel/bpf/cpumap.c: 512
 * atomic xchg and NULL-ptr check in __cpu_map_flush() makes sure a /Users/rubber/linux/kernel/bpf/cpumap.c: 513
 * pending flush op doesn't fail. /Users/rubber/linux/kernel/bpf/cpumap.c: 514
 * The bpf_cpu_map_entry is still used by the kthread, and there can /Users/rubber/linux/kernel/bpf/cpumap.c: 516
 * still be pending packets (in queue and percpu bulkq).  A refcnt /Users/rubber/linux/kernel/bpf/cpumap.c: 517
 * makes sure to last user (kthread_stop vs. call_rcu) free memory /Users/rubber/linux/kernel/bpf/cpumap.c: 518
 * resources. /Users/rubber/linux/kernel/bpf/cpumap.c: 519
 * The rcu callback __cpu_map_entry_free flush remaining packets in /Users/rubber/linux/kernel/bpf/cpumap.c: 521
 * percpu bulkq to queue.  Due to caller map_delete_elem() disable /Users/rubber/linux/kernel/bpf/cpumap.c: 522
 * preemption, cannot call kthread_stop() to make sure queue is empty. /Users/rubber/linux/kernel/bpf/cpumap.c: 523
 * Instead a work_queue is started for stopping kthread, /Users/rubber/linux/kernel/bpf/cpumap.c: 524
 * cpu_map_kthread_stop, which waits for an RCU grace period before /Users/rubber/linux/kernel/bpf/cpumap.c: 525
 * stopping kthread, emptying the queue. /Users/rubber/linux/kernel/bpf/cpumap.c: 526
	/* At this point bpf_prog->aux->refcnt == 0 and this map->refcnt == 0, /Users/rubber/linux/kernel/bpf/cpumap.c: 598
	 * so the bpf programs (can be more than one that used this map) were /Users/rubber/linux/kernel/bpf/cpumap.c: 599
	 * disconnected from events. Wait for outstanding critical sections in /Users/rubber/linux/kernel/bpf/cpumap.c: 600
	 * these programs to complete. The rcu critical section only guarantees /Users/rubber/linux/kernel/bpf/cpumap.c: 601
	 * no further "XDP/bpf-side" reads against bpf_cpu_map->cpu_map. /Users/rubber/linux/kernel/bpf/cpumap.c: 602
	 * It does __not__ ensure pending flush operations (if any) are /Users/rubber/linux/kernel/bpf/cpumap.c: 603
	 * complete. /Users/rubber/linux/kernel/bpf/cpumap.c: 604
	/* For cpu_map the remote CPUs can still be using the entries /Users/rubber/linux/kernel/bpf/cpumap.c: 609
	 * (struct bpf_cpu_map_entry). /Users/rubber/linux/kernel/bpf/cpumap.c: 610
/* Elements are kept alive by RCU; either by rcu_read_lock() (from syscall) or /Users/rubber/linux/kernel/bpf/cpumap.c: 626
 * by local_bh_disable() (from XDP calls inside NAPI). The /Users/rubber/linux/kernel/bpf/cpumap.c: 627
 * rcu_read_lock_bh_held() below makes lockdep accept both. /Users/rubber/linux/kernel/bpf/cpumap.c: 628
/* Runs under RCU-read-side, plus in softirq under NAPI protection. /Users/rubber/linux/kernel/bpf/cpumap.c: 723
 * Thus, safe percpu variable access. /Users/rubber/linux/kernel/bpf/cpumap.c: 724
	/* Notice, xdp_buff/page MUST be queued here, long enough for /Users/rubber/linux/kernel/bpf/cpumap.c: 734
	 * driver to code invoking us to finished, due to driver /Users/rubber/linux/kernel/bpf/cpumap.c: 735
	 * (e.g. ixgbe) recycle tricks based on page-refcnt. /Users/rubber/linux/kernel/bpf/cpumap.c: 736
	 * /Users/rubber/linux/kernel/bpf/cpumap.c: 737
	 * Thus, incoming xdp_frame is always queued here (else we race /Users/rubber/linux/kernel/bpf/cpumap.c: 738
	 * with another CPU on page-refcnt and remaining driver code). /Users/rubber/linux/kernel/bpf/cpumap.c: 739
	 * Queue time is very short, as driver will invoke flush /Users/rubber/linux/kernel/bpf/cpumap.c: 740
	 * operation, when completing napi->poll call. /Users/rubber/linux/kernel/bpf/cpumap.c: 741
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/preload/bpf_preload_kern.c: 1
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 1
		/* iterators.skel.h is little endian. /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 62
		 * libbpf doesn't support automatic little->big conversion /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 63
		 * of BPF bytecode yet. /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 64
		 * The program load will fail in such case. /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 65
	/* The kernel will proceed with pinnging the links in bpffs. /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 82
	 * UMD will wait on read from pipe. /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.c: 83
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/kernel/bpf/preload/iterators/iterators.bpf.c: 1
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/debug/debug_core.c: 1
 * Kernel Debug Core /Users/rubber/linux/kernel/debug/debug_core.c: 3
 * Maintainer: Jason Wessel <jason.wessel@windriver.com> /Users/rubber/linux/kernel/debug/debug_core.c: 5
 * Copyright (C) 2000-2001 VERITAS Software Corporation. /Users/rubber/linux/kernel/debug/debug_core.c: 7
 * Copyright (C) 2002-2004 Timesys Corporation /Users/rubber/linux/kernel/debug/debug_core.c: 8
 * Copyright (C) 2003-2004 Amit S. Kale <amitkale@linsyssoft.com> /Users/rubber/linux/kernel/debug/debug_core.c: 9
 * Copyright (C) 2004 Pavel Machek <pavel@ucw.cz> /Users/rubber/linux/kernel/debug/debug_core.c: 10
 * Copyright (C) 2004-2006 Tom Rini <trini@kernel.crashing.org> /Users/rubber/linux/kernel/debug/debug_core.c: 11
 * Copyright (C) 2004-2006 LinSysSoft Technologies Pvt. Ltd. /Users/rubber/linux/kernel/debug/debug_core.c: 12
 * Copyright (C) 2005-2009 Wind River Systems, Inc. /Users/rubber/linux/kernel/debug/debug_core.c: 13
 * Copyright (C) 2007 MontaVista Software, Inc. /Users/rubber/linux/kernel/debug/debug_core.c: 14
 * Copyright (C) 2008 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/debug/debug_core.c: 15
 * Contributors at various stages not listed above: /Users/rubber/linux/kernel/debug/debug_core.c: 17
 *  Jason Wessel ( jason.wessel@windriver.com ) /Users/rubber/linux/kernel/debug/debug_core.c: 18
 *  George Anzinger <george@mvista.com> /Users/rubber/linux/kernel/debug/debug_core.c: 19
 *  Anurekh Saxena (anurekh.saxena@timesys.com) /Users/rubber/linux/kernel/debug/debug_core.c: 20
 *  Lake Stevens Instrument Division (Glenn Engel) /Users/rubber/linux/kernel/debug/debug_core.c: 21
 *  Jim Kingdon, Cygnus Support. /Users/rubber/linux/kernel/debug/debug_core.c: 22
 * Original KGDB stub: David Grothe <dave@gcom.com>, /Users/rubber/linux/kernel/debug/debug_core.c: 24
 * Tigran Aivazian <tigran@sco.com> /Users/rubber/linux/kernel/debug/debug_core.c: 25
 * Holds information about breakpoints in a kernel. These breakpoints are /Users/rubber/linux/kernel/debug/debug_core.c: 98
 * added and removed by gdb. /Users/rubber/linux/kernel/debug/debug_core.c: 99
 * The CPU# of the active CPU, or -1 if none: /Users/rubber/linux/kernel/debug/debug_core.c: 106
 * We use NR_CPUs not PERCPU, in case kgdb is used to debug early /Users/rubber/linux/kernel/debug/debug_core.c: 114
 * bootup code (which might not have percpu set up yet): /Users/rubber/linux/kernel/debug/debug_core.c: 115
 * If you are debugging a problem where roundup (the collection of /Users/rubber/linux/kernel/debug/debug_core.c: 131
 * all other CPUs) is a problem [this should be extremely rare], /Users/rubber/linux/kernel/debug/debug_core.c: 132
 * then use the nokgdbroundup option to avoid roundup. In that case /Users/rubber/linux/kernel/debug/debug_core.c: 133
 * the other CPUs might interfere with your debugging context, so /Users/rubber/linux/kernel/debug/debug_core.c: 134
 * use this with care: /Users/rubber/linux/kernel/debug/debug_core.c: 135
 * Finally, some KGDB code :-) /Users/rubber/linux/kernel/debug/debug_core.c: 149
 * Weak aliases for breakpoint management, /Users/rubber/linux/kernel/debug/debug_core.c: 153
 * can be overridden by architectures when needed: /Users/rubber/linux/kernel/debug/debug_core.c: 154
	/* Validate setting the breakpoint and then removing it.  If the /Users/rubber/linux/kernel/debug/debug_core.c: 185
	 * remove fails, the kernel needs to emit a bad message because we /Users/rubber/linux/kernel/debug/debug_core.c: 186
	 * are deep trouble not being able to put things back the way we /Users/rubber/linux/kernel/debug/debug_core.c: 187
	 * found them. /Users/rubber/linux/kernel/debug/debug_core.c: 188
 * Default (weak) implementation for kgdb_roundup_cpus /Users/rubber/linux/kernel/debug/debug_core.c: 221
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 226
	 * NOTE: get_irq_regs() is supposed to get the registers from /Users/rubber/linux/kernel/debug/debug_core.c: 227
	 * before the IPI interrupt happened and so is supposed to /Users/rubber/linux/kernel/debug/debug_core.c: 228
	 * show where the processor was.  In some situations it's /Users/rubber/linux/kernel/debug/debug_core.c: 229
	 * possible we might be called without an IPI, so it might be /Users/rubber/linux/kernel/debug/debug_core.c: 230
	 * safer to figure out how to make kgdb_breakpoint() work /Users/rubber/linux/kernel/debug/debug_core.c: 231
	 * properly here. /Users/rubber/linux/kernel/debug/debug_core.c: 232
		/* /Users/rubber/linux/kernel/debug/debug_core.c: 255
		 * If it didn't round up last time, don't try again /Users/rubber/linux/kernel/debug/debug_core.c: 256
		 * since smp_call_function_single_async() will block. /Users/rubber/linux/kernel/debug/debug_core.c: 257
		 * /Users/rubber/linux/kernel/debug/debug_core.c: 258
		 * If rounding_up is false then we know that the /Users/rubber/linux/kernel/debug/debug_core.c: 259
		 * previous call must have at least started and that /Users/rubber/linux/kernel/debug/debug_core.c: 260
		 * means smp_call_function_single_async() won't block. /Users/rubber/linux/kernel/debug/debug_core.c: 261
 * Some architectures need cache flushes when we set/clear a /Users/rubber/linux/kernel/debug/debug_core.c: 277
 * breakpoint: /Users/rubber/linux/kernel/debug/debug_core.c: 278
 * SW breakpoint management: /Users/rubber/linux/kernel/debug/debug_core.c: 302
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 480
	 * In general, architectures don't support dumping the stack of a /Users/rubber/linux/kernel/debug/debug_core.c: 481
	 * "running" process that's not the current one.  From the point of /Users/rubber/linux/kernel/debug/debug_core.c: 482
	 * view of the Linux, kernel processes that are looping in the kgdb /Users/rubber/linux/kernel/debug/debug_core.c: 483
	 * slave loop are still "running".  There's also no API (that actually /Users/rubber/linux/kernel/debug/debug_core.c: 484
	 * works across all architectures) that can do a stack crawl based /Users/rubber/linux/kernel/debug/debug_core.c: 485
	 * on registers passed as a parameter. /Users/rubber/linux/kernel/debug/debug_core.c: 486
	 * /Users/rubber/linux/kernel/debug/debug_core.c: 487
	 * Solve this conundrum by asking slave CPUs to do the backtrace /Users/rubber/linux/kernel/debug/debug_core.c: 488
	 * themselves. /Users/rubber/linux/kernel/debug/debug_core.c: 489
 * Return true if there is a valid kgdb I/O module.  Also if no /Users/rubber/linux/kernel/debug/debug_core.c: 498
 * debugger is attached a message can be printed to the console about /Users/rubber/linux/kernel/debug/debug_core.c: 499
 * waiting for the debugger to attach. /Users/rubber/linux/kernel/debug/debug_core.c: 500
 * The print_wait argument is only to be true when called from inside /Users/rubber/linux/kernel/debug/debug_core.c: 502
 * the core kgdb_handle_exception, because it will wait for the /Users/rubber/linux/kernel/debug/debug_core.c: 503
 * debugger to attach. /Users/rubber/linux/kernel/debug/debug_core.c: 504
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 538
	 * If the break point removed ok at the place exception /Users/rubber/linux/kernel/debug/debug_core.c: 539
	 * occurred, try to recover and print a warning to the end /Users/rubber/linux/kernel/debug/debug_core.c: 540
	 * user because the user planted a breakpoint in a place that /Users/rubber/linux/kernel/debug/debug_core.c: 541
	 * KGDB needs in order to function. /Users/rubber/linux/kernel/debug/debug_core.c: 542
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 606
	 * Interrupts will be restored by the 'trap return' code, except when /Users/rubber/linux/kernel/debug/debug_core.c: 607
	 * single stepping. /Users/rubber/linux/kernel/debug/debug_core.c: 608
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 627
	 * CPU will loop if it is a slave or request to become a kgdb /Users/rubber/linux/kernel/debug/debug_core.c: 628
	 * master cpu and acquire the kgdb_active lock: /Users/rubber/linux/kernel/debug/debug_core.c: 629
			/* Return to normal operation by executing any /Users/rubber/linux/kernel/debug/debug_core.c: 649
			 * hw breakpoint fixup. /Users/rubber/linux/kernel/debug/debug_core.c: 650
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 671
	 * For single stepping, try to only enter on the processor /Users/rubber/linux/kernel/debug/debug_core.c: 672
	 * that was single stepping.  To guard against a deadlock, the /Users/rubber/linux/kernel/debug/debug_core.c: 673
	 * kernel will only try for the value of sstep_tries before /Users/rubber/linux/kernel/debug/debug_core.c: 674
	 * giving up and continuing on. /Users/rubber/linux/kernel/debug/debug_core.c: 675
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 694
	 * Don't enter if we have hit a removed breakpoint. /Users/rubber/linux/kernel/debug/debug_core.c: 695
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 706
	 * Get the passive CPU lock which will hold all the non-primary /Users/rubber/linux/kernel/debug/debug_core.c: 707
	 * CPU in a spin state while the debugger is active /Users/rubber/linux/kernel/debug/debug_core.c: 708
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 723
	 * Wait for the other CPUs to be notified and be waiting for us: /Users/rubber/linux/kernel/debug/debug_core.c: 724
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 734
	 * At this point the primary processor is completely /Users/rubber/linux/kernel/debug/debug_core.c: 735
	 * in the debugger and all secondary CPUs are quiescent /Users/rubber/linux/kernel/debug/debug_core.c: 736
 * kgdb_handle_exception() - main entry point from a kernel exception /Users/rubber/linux/kernel/debug/debug_core.c: 817
 * Locking hierarchy: /Users/rubber/linux/kernel/debug/debug_core.c: 819
 *	interface locks, if any (begin_session) /Users/rubber/linux/kernel/debug/debug_core.c: 820
 *	kgdb lock (kgdb_active) /Users/rubber/linux/kernel/debug/debug_core.c: 821
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 832
	 * Avoid entering the debugger if we were triggered due to an oops /Users/rubber/linux/kernel/debug/debug_core.c: 833
	 * but panic_timeout indicates the system should automatically /Users/rubber/linux/kernel/debug/debug_core.c: 834
	 * reboot on panic. We don't want to get stuck waiting for input /Users/rubber/linux/kernel/debug/debug_core.c: 835
	 * on such systems, especially if its "just" an oops. /Users/rubber/linux/kernel/debug/debug_core.c: 836
 * GDB places a breakpoint at this function to know dynamically loaded objects. /Users/rubber/linux/kernel/debug/debug_core.c: 862
	/* If we're debugging, or KGDB has not connected, don't try /Users/rubber/linux/kernel/debug/debug_core.c: 927
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 989
	 * We don't want to get stuck waiting for input from user if /Users/rubber/linux/kernel/debug/debug_core.c: 990
	 * "panic_timeout" indicates the system should automatically /Users/rubber/linux/kernel/debug/debug_core.c: 991
	 * reboot on panic. /Users/rubber/linux/kernel/debug/debug_core.c: 992
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 1029
	 * Take the following action on reboot notify depending on value: /Users/rubber/linux/kernel/debug/debug_core.c: 1030
	 *    1 == Enter debugger /Users/rubber/linux/kernel/debug/debug_core.c: 1031
	 *    0 == [the default] detach debug client /Users/rubber/linux/kernel/debug/debug_core.c: 1032
	 *   -1 == Do nothing... and use this until the board resets /Users/rubber/linux/kernel/debug/debug_core.c: 1033
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 1075
	 * When this routine is called KGDB should unregister from /Users/rubber/linux/kernel/debug/debug_core.c: 1076
	 * handlers and clean up, making sure it is not handling any /Users/rubber/linux/kernel/debug/debug_core.c: 1077
	 * break exceptions at the time. /Users/rubber/linux/kernel/debug/debug_core.c: 1078
 *	kgdb_register_io_module - register KGDB IO module /Users/rubber/linux/kernel/debug/debug_core.c: 1096
 *	@new_dbg_io_ops: the io ops vector /Users/rubber/linux/kernel/debug/debug_core.c: 1097
 *	Register it with the KGDB core. /Users/rubber/linux/kernel/debug/debug_core.c: 1099
 *	kgdb_unregister_io_module - unregister KGDB IO module /Users/rubber/linux/kernel/debug/debug_core.c: 1152
 *	@old_dbg_io_ops: the io ops vector /Users/rubber/linux/kernel/debug/debug_core.c: 1153
 *	Unregister it with the KGDB core. /Users/rubber/linux/kernel/debug/debug_core.c: 1155
	/* /Users/rubber/linux/kernel/debug/debug_core.c: 1161
	 * KGDB is no longer able to communicate out, so /Users/rubber/linux/kernel/debug/debug_core.c: 1162
	 * unregister our callbacks and reset state. /Users/rubber/linux/kernel/debug/debug_core.c: 1163
 * kgdb_breakpoint - generate breakpoint exception /Users/rubber/linux/kernel/debug/debug_core.c: 1195
 * This function will generate a breakpoint exception.  It is used at the /Users/rubber/linux/kernel/debug/debug_core.c: 1197
 * beginning of a program to sync up with a debugger and can be used /Users/rubber/linux/kernel/debug/debug_core.c: 1198
 * otherwise as a quick means to stop program execution and "break" into /Users/rubber/linux/kernel/debug/debug_core.c: 1199
 * the debugger. /Users/rubber/linux/kernel/debug/debug_core.c: 1200
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/kernel/debug/gdbstub.c: 1
 * Kernel Debug Core /Users/rubber/linux/kernel/debug/gdbstub.c: 3
 * Maintainer: Jason Wessel <jason.wessel@windriver.com> /Users/rubber/linux/kernel/debug/gdbstub.c: 5
 * Copyright (C) 2000-2001 VERITAS Software Corporation. /Users/rubber/linux/kernel/debug/gdbstub.c: 7
 * Copyright (C) 2002-2004 Timesys Corporation /Users/rubber/linux/kernel/debug/gdbstub.c: 8
 * Copyright (C) 2003-2004 Amit S. Kale <amitkale@linsyssoft.com> /Users/rubber/linux/kernel/debug/gdbstub.c: 9
 * Copyright (C) 2004 Pavel Machek <pavel@ucw.cz> /Users/rubber/linux/kernel/debug/gdbstub.c: 10
 * Copyright (C) 2004-2006 Tom Rini <trini@kernel.crashing.org> /Users/rubber/linux/kernel/debug/gdbstub.c: 11
 * Copyright (C) 2004-2006 LinSysSoft Technologies Pvt. Ltd. /Users/rubber/linux/kernel/debug/gdbstub.c: 12
 * Copyright (C) 2005-2009 Wind River Systems, Inc. /Users/rubber/linux/kernel/debug/gdbstub.c: 13
 * Copyright (C) 2007 MontaVista Software, Inc. /Users/rubber/linux/kernel/debug/gdbstub.c: 14
 * Copyright (C) 2008 Red Hat, Inc., Ingo Molnar <mingo@redhat.com> /Users/rubber/linux/kernel/debug/gdbstub.c: 15
 * Contributors at various stages not listed above: /Users/rubber/linux/kernel/debug/gdbstub.c: 17
 *  Jason Wessel ( jason.wessel@windriver.com ) /Users/rubber/linux/kernel/debug/gdbstub.c: 18
 *  George Anzinger <george@mvista.com> /Users/rubber/linux/kernel/debug/gdbstub.c: 19
 *  Anurekh Saxena (anurekh.saxena@timesys.com) /Users/rubber/linux/kernel/debug/gdbstub.c: 20
 *  Lake Stevens Instrument Division (Glenn Engel) /Users/rubber/linux/kernel/debug/gdbstub.c: 21
 *  Jim Kingdon, Cygnus Support. /Users/rubber/linux/kernel/debug/gdbstub.c: 22
 * Original KGDB stub: David Grothe <dave@gcom.com>, /Users/rubber/linux/kernel/debug/gdbstub.c: 24
 * Tigran Aivazian <tigran@sco.com> /Users/rubber/linux/kernel/debug/gdbstub.c: 25
 * GDB remote protocol parser: /Users/rubber/linux/kernel/debug/gdbstub.c: 53
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 96
		 * Spin and wait around for the start character, ignore all /Users/rubber/linux/kernel/debug/gdbstub.c: 97
		 * other characters: /Users/rubber/linux/kernel/debug/gdbstub.c: 98
 nothing */; /Users/rubber/linux/kernel/debug/gdbstub.c: 101
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 109
		 * now, read until a # or end of buffer is found: /Users/rubber/linux/kernel/debug/gdbstub.c: 110
 * Send the packet in buffer. /Users/rubber/linux/kernel/debug/gdbstub.c: 139
 * Check for gdb connection if asked for. /Users/rubber/linux/kernel/debug/gdbstub.c: 140
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 148
	 * $<packet info>#<checksum>. /Users/rubber/linux/kernel/debug/gdbstub.c: 149
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 178
		 * If we get the start of another packet, this means /Users/rubber/linux/kernel/debug/gdbstub.c: 179
		 * that GDB is attempting to reconnect.  We will NAK /Users/rubber/linux/kernel/debug/gdbstub.c: 180
		 * the packet being sent, and stop trying to send this /Users/rubber/linux/kernel/debug/gdbstub.c: 181
		 * packet. /Users/rubber/linux/kernel/debug/gdbstub.c: 182
 * Convert the memory pointed to by mem into hex, placing result in /Users/rubber/linux/kernel/debug/gdbstub.c: 232
 * buf.  Return a pointer to the last char put in buf (null). May /Users/rubber/linux/kernel/debug/gdbstub.c: 233
 * return an error. /Users/rubber/linux/kernel/debug/gdbstub.c: 234
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 241
	 * We use the upper half of buf as an intermediate buffer for the /Users/rubber/linux/kernel/debug/gdbstub.c: 242
	 * raw memory copy.  Hex conversion will work against this one. /Users/rubber/linux/kernel/debug/gdbstub.c: 243
 * Convert the hex array pointed to by buf into binary to be placed in /Users/rubber/linux/kernel/debug/gdbstub.c: 261
 * mem.  Return a pointer to the character AFTER the last byte /Users/rubber/linux/kernel/debug/gdbstub.c: 262
 * written.  May return an error. /Users/rubber/linux/kernel/debug/gdbstub.c: 263
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 270
	 * We use the upper half of buf as an intermediate buffer for the /Users/rubber/linux/kernel/debug/gdbstub.c: 271
	 * raw memory that is converted from hex. /Users/rubber/linux/kernel/debug/gdbstub.c: 272
 * While we find nice hex chars, build a long_val. /Users/rubber/linux/kernel/debug/gdbstub.c: 287
 * Return number of chars processed. /Users/rubber/linux/kernel/debug/gdbstub.c: 288
 * Copy the binary array pointed to by buf into mem.  Fix $, #, and /Users/rubber/linux/kernel/debug/gdbstub.c: 319
 * 0x7d escaped with 0x7d. Return -EFAULT on failure or 0 on success. /Users/rubber/linux/kernel/debug/gdbstub.c: 320
 * The input buf is overwritten with the result to write to mem. /Users/rubber/linux/kernel/debug/gdbstub.c: 321
 * Thread ID accessors. We represent a flat TID space to GDB, where /Users/rubber/linux/kernel/debug/gdbstub.c: 398
 * the per CPU idle threads (which under Linux all have PID 0) are /Users/rubber/linux/kernel/debug/gdbstub.c: 399
 * remapped to negative TIDs. /Users/rubber/linux/kernel/debug/gdbstub.c: 400
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 432
	 * Non-positive TIDs are remapped to the cpu shadow information /Users/rubber/linux/kernel/debug/gdbstub.c: 433
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 449
	 * find_task_by_pid_ns() does not take the tasklist lock anymore /Users/rubber/linux/kernel/debug/gdbstub.c: 450
	 * but is nicely RCU locked - hence is a pretty resilient /Users/rubber/linux/kernel/debug/gdbstub.c: 451
	 * thing to use: /Users/rubber/linux/kernel/debug/gdbstub.c: 452
 * Remap normal tasks to their real PID, /Users/rubber/linux/kernel/debug/gdbstub.c: 459
 * CPU shadow threads are mapped to -CPU - 2 /Users/rubber/linux/kernel/debug/gdbstub.c: 460
 * All the functions that start with gdb_cmd are the various /Users/rubber/linux/kernel/debug/gdbstub.c: 471
 * operations to implement the handlers for the gdbserial protocol /Users/rubber/linux/kernel/debug/gdbstub.c: 472
 * where KGDB is communicating with an external debugger /Users/rubber/linux/kernel/debug/gdbstub.c: 473
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 479
	 * We know that this packet is only sent /Users/rubber/linux/kernel/debug/gdbstub.c: 480
	 * during initial connect.  So to be safe, /Users/rubber/linux/kernel/debug/gdbstub.c: 481
	 * we clear out our breakpoints now in case /Users/rubber/linux/kernel/debug/gdbstub.c: 482
	 * GDB is reconnecting. /Users/rubber/linux/kernel/debug/gdbstub.c: 483
			/* /Users/rubber/linux/kernel/debug/gdbstub.c: 504
			 * Try to find the task on some other /Users/rubber/linux/kernel/debug/gdbstub.c: 505
			 * or possibly this node if we do not /Users/rubber/linux/kernel/debug/gdbstub.c: 506
			 * find the matching task then we try /Users/rubber/linux/kernel/debug/gdbstub.c: 507
			 * to approximate the results. /Users/rubber/linux/kernel/debug/gdbstub.c: 508
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 515
	 * All threads that don't have debuggerinfo should be /Users/rubber/linux/kernel/debug/gdbstub.c: 516
	 * in schedule() sleeping, since all other CPUs /Users/rubber/linux/kernel/debug/gdbstub.c: 517
	 * are in kgdb_wait, and thus have debuggerinfo. /Users/rubber/linux/kernel/debug/gdbstub.c: 518
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 523
		 * Pull stuff saved during switch_to; nothing /Users/rubber/linux/kernel/debug/gdbstub.c: 524
		 * else is accessible (or even particularly /Users/rubber/linux/kernel/debug/gdbstub.c: 525
		 * relevant). /Users/rubber/linux/kernel/debug/gdbstub.c: 526
		 * /Users/rubber/linux/kernel/debug/gdbstub.c: 527
		 * This should be enough for a stack trace. /Users/rubber/linux/kernel/debug/gdbstub.c: 528
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 664
		 * Assume the kill case, with no exit code checking, /Users/rubber/linux/kernel/debug/gdbstub.c: 665
		 * trying to force detach the debugger: /Users/rubber/linux/kernel/debug/gdbstub.c: 666
		/* /Users/rubber/linux/kernel/debug/gdbstub.c: 682
		 * Execution should not return from /Users/rubber/linux/kernel/debug/gdbstub.c: 683
		 * machine_emergency_restart() /Users/rubber/linux/kernel/debug/gdbstub.c: 684
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 862
	 * Since GDB-5.3, it's been drafted that '0' is a software /Users/rubber/linux/kernel/debug/gdbstub.c: 863
	 * breakpoint, '1' is a hardware breakpoint, so let's do that. /Users/rubber/linux/kernel/debug/gdbstub.c: 864
	/* /Users/rubber/linux/kernel/debug/gdbstub.c: 882
	 * Test if this is a hardware breakpoint, and /Users/rubber/linux/kernel/debug/gdbstub.c: 883
	 * if we support it: /Users/rubber/linux/kernel/debug/gdbstub.c: 884
	/* C09 == pass exception /Users/rubber/linux/kernel/debug/gdbstub.c: 924
	 * C15 == detach kgdb, pass exception /Users/rubber/linux/kernel/debug/gdbstub.c: 925
 * This function performs all gdbserial command processing /Users/rubber/linux/kernel/debug/gdbstub.c: 952
			/* kill or detach. KGDB should treat this like a /Users/rubber/linux/kernel/debug/gdbstub.c: 1015
			 * continue. /Users/rubber/linux/kernel/debug/gdbstub.c: 1016
			/* /Users/rubber/linux/kernel/debug/gdbstub.c: 1070
			 * Leave cmd processing on error, detach, /Users/rubber/linux/kernel/debug/gdbstub.c: 1071
			 * kill, continue, or single step. /Users/rubber/linux/kernel/debug/gdbstub.c: 1072
 * gdbstub_exit - Send an exit message to GDB /Users/rubber/linux/kernel/debug/gdbstub.c: 1121
 * @status: The exit code to report. /Users/rubber/linux/kernel/debug/gdbstub.c: 1122
 * Created by: Jason Wessel <jason.wessel@windriver.com> /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 2
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 4
 * This file is licensed under the terms of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 6
 * License version 2. This program is licensed "as is" without any /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 7
 * warranty of any kind, whether express or implied. /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 8
 * KDB interface to KGDB internals /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 20
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 94
	 * SSBPT is set when the kernel debugger must single step a /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 95
	 * task in order to re-establish an instruction breakpoint /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 96
	 * which uses the instruction replacement mechanism.  It is /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 97
	 * cleared by any action that removes the need to single-step /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 98
	 * the breakpoint. /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 99
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 137
	 * Upon exit from the kdb main loop setup break points and restart /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 138
	 * the system based on the requested continue state /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 139
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 163
		 * Force clear the single step bit because kdb emulates this /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 164
		 * differently vs the gdbstub /Users/rubber/linux/kernel/debug/kdb/kdb_debugger.c: 165
 * Kernel Debugger Architecture Dependent Console I/O handler /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 4
 * License. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 5
 * Copyright (c) 1999-2006 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 7
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 8
 * Check if the keyboard controller has a keypress for us. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 31
 * Some parts (Enter Release, LED change) are still blocking polled here, /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 32
 * but hopefully they are all short. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 33
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 53
	 * Fetch the scancode /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 54
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 59
	 * Ignore mouse events. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 60
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 65
	 * Ignore release, trigger on make /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 66
	 * (except for shift keys, where we want to /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 67
	 *  keep the shift state so long as the key is /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 68
	 *  held down). /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 69
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 73
		 * Next key may use shift table /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 74
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 84
		 * Left ctrl key /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 85
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 102
	 * Translate scancode /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 103
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 107
		 * Toggle caps lock /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 108
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 119
		 * Backspace /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 120
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 148
	 * For Japanese 86/106 keyboards /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 149
	 * 	See comment in drivers/char/pc_keyb.c. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 150
	 * 	- Masahiro Adegawa /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 151
 * Best effort cleanup of ENTER break codes on leaving KDB. Called on /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 195
 * exiting KDB, when we know we processed an ENTER or KP ENTER scan /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 196
 * code. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 197
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 203
	 * Nothing to clean up, since either /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 204
	 * ENTER was never pressed, or has already /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 205
	 * gotten cleaned up. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 206
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 212
	 * Enter key. Need to absorb the break code here, lest it gets /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 213
	 * leaked out if we exit KDB as the result of processing 'g'. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 214
	 * /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 215
	 * This has several interesting implications: /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 216
	 * + Need to handle KP ENTER, which has break code 0xe0 0x9c. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 217
	 * + Need to handle repeat ENTER and repeat KP ENTER. Repeats /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 218
	 *   only get a break code at the end of the repeated /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 219
	 *   sequence. This means we can't propagate the repeated key /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 220
	 *   press, and must swallow it away. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 221
	 * + Need to handle possible PS/2 mouse input. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 222
	 * + Need to handle mashed keys. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 223
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 230
		 * Fetch the scancode. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 231
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 236
		 * Skip mouse input. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 237
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 242
		 * If we see 0xe0, this is either a break code for KP /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 243
		 * ENTER, or a repeat make for KP ENTER. Either way, /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 244
		 * since the second byte is equivalent to an ENTER, /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 245
		 * skip the 0xe0 and try again. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 246
		 * /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 247
		 * If we see 0x1c, this must be a repeat ENTER or KP /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 248
		 * ENTER (and we swallowed 0xe0 before). Try again. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 249
		 * /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 250
		 * We can also see make and break codes for other keys /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 251
		 * mashed before or after pressing ENTER. Thus, if we /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 252
		 * see anything other than 0x9c, we have to try again. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 253
		 * /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 254
		 * Note, if you held some key as ENTER was depressed, /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 255
		 * that break code would get leaked out. /Users/rubber/linux/kernel/debug/kdb/kdb_keyboard.c: 256
 * Kernel Debugger Architecture Independent Console I/O handler /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 4
 * License.  See the file "COPYING" in the main directory of this archive /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 5
 * for more details. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 6
 * Copyright (c) 1999-2006 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 8
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 9
 * kdb_handle_escape() - validity check on an accumulated escape sequence. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 53
 * @buf:	Accumulated escape characters to be examined. Note that buf /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 54
 *		is not a string, it is an array of characters and need not be /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 55
 *		nil terminated. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 56
 * @sz:		Number of accumulated escape characters. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 57
 * Return: -1 if the escape sequence is unwanted, 0 if it is incomplete, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 59
 * otherwise it returns a mapped key value to pass to the upper layers. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 60
 * kdb_getchar() - Read a single character from a kdb console (or consoles). /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 112
 * Other than polling the various consoles that are currently enabled, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 114
 * most of the work done in this function is dealing with escape sequences. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 115
 * An escape key could be the start of a vt100 control sequence such as \e[D /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 117
 * (left arrow) or it could be a character in its own right.  The standard /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 118
 * method for detecting the difference is to wait for 2 seconds to see if there /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 119
 * are any other characters.  kdb is complicated by the lack of a timer service /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 120
 * (interrupts are off), by multiple input sources. Escape sequence processing /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 121
 * has to be done as states in the polling loop. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 122
 * Return: The key pressed or a control code derived from an escape sequence. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 124
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 153
		 * When the first character is received (or we get a change /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 154
		 * input source) we set ourselves up to handle an escape /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 155
		 * sequences (just in case). /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 156
 * kdb_read /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 176
 *	This function reads a string of characters, terminated by /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 178
 *	a newline, or by reaching the end of the supplied buffer, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 179
 *	from the current kernel debugger console device. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 180
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 181
 *	buffer	- Address of character buffer to receive input characters. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 182
 *	bufsize - size, in bytes, of the character buffer /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 183
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 184
 *	Returns a pointer to the buffer containing the received /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 185
 *	character string.  This string will be terminated by a /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 186
 *	newline character. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 187
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 188
 *	No locks are required to be held upon entry to this /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 189
 *	function.  It is not reentrant - it relies on the fact /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 190
 *	that while kdb is running on only one "master debug" cpu. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 191
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 192
 *	The buffer size must be >= 2. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 193
	char *bufend = buffer+bufsize-2;	/* Reserve space for newline /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 199
				/* The kgdb transition check will hide /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 385
				 * printed characters if we think that /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 386
				 * kgdb is connecting, until the check /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 387
 * kdb_getstr /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 418
 *	Print the prompt string and read a command from the /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 420
 *	input device. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 421
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 423
 *	buffer	Address of buffer to receive command /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 424
 *	bufsize Size of buffer in bytes /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 425
 *	prompt	Pointer to string to use as prompt string /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 426
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 427
 *	Pointer to command buffer. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 428
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 429
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 430
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 431
 *	For SMP kernels, the processor number will be /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 432
 *	substituted for %d, %x or %o in the prompt. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 433
 * kdb_input_flush /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 446
 *	Get rid of any buffered console input. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 448
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 450
 *	none /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 451
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 452
 *	nothing /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 453
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 454
 *	none /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 455
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 456
 *	Call this function whenever you want to flush input.  If there is any /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 457
 *	outstanding input, it ignores all characters until there has been no /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 458
 *	data for approximately 1ms. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 459
 * kdb_printf /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 484
 *	Print a string to the output device(s). /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 486
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 488
 *	printf-like format and optional args. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 489
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 490
 *	0 /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 491
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 492
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 493
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 494
 *	use 'kdbcons->write()' to avoid polluting 'log_buf' with /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 495
 *	kdb output. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 496
 *  If the user is doing a cmd args | grep srch /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 498
 *  then kdb_grepping_flag is set. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 499
 *  In that case we need to accumulate full lines (ending in \n) before /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 500
 *  searching for the pattern. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 501
 * search arg1 to see if it contains arg2 /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 510
 * (kdmain.c provides flags for ^pat and pat$) /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 511
 * return 1 for found, 0 for not found /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 513
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 567
		 * Set oops_in_progress to encourage the console drivers to /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 568
		 * disregard their internal spin locks: in the current calling /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 569
		 * context the risk of deadlock is a bigger problem than risks /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 570
		 * due to re-entering the console driver. We operate directly on /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 571
		 * oops_in_progress rather than using bust_spinlocks() because /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 572
		 * the calls bust_spinlocks() makes on exit are not appropriate /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 573
		 * for this calling context. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 574
	/* Serialize kdb_printf if multiple cpus try to write at once. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 596
	 * But if any cpu goes recursive in kdb, just print the output, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 597
	 * even if it is interleaved with any other text. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 598
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 629
	 * If kdb_parse() found that the command was cmd xxx | grep yyy /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 630
	 * then kdb_grepping_flag is set, and kdb_grep_string contains yyy /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 631
	 * /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 632
	 * Accumulate the print data up to a newline before searching it. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 633
	 * (vsnprintf does null-terminate the string that it generates) /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 634
			/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 641
			 * Special cases that don't end with newlines /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 642
			 * but should be written without one: /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 643
			 *   The "[nn]kdb> " prompt should /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 644
			 *   appear at the front of the buffer. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 645
			 * /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 646
			 *   The "[nn]more " prompt should also be /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 647
			 *     (MOREPROMPT -> moreprompt) /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 648
			 *   written *   but we print that ourselves, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 649
			 *   we set the suspend_grep flag to make /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 650
			 *   it unconditional. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 651
			 * /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 652
				/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 655
				 * these should occur after a newline, /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 656
				 * so they will be at the front of the /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 657
				 * buffer /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 658
					/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 663
					 * We're about to start a new /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 664
					 * command, so we can go back /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 665
					 * to normal mode. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 666
			/* no newline; don't search/write the buffer /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 672
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 680
		 * The newline is present; print through it or discard /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 681
		 * it, depending on the results of the search. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 682
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 689
		 * We now have a newline at the end of the string /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 690
		 * Only continue with this output if it contains the /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 691
		 * search string. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 692
			/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 696
			 * At this point the complete line at the start /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 697
			 * of kdb_buffer can be discarded, as it does /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 698
			 * not contain what the user is looking for. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 699
			 * Shift the buffer left. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 700
			/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 710
			 * This was a interactive search (using '/' at more /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 711
			 * prompt) and it has completed. Replace the \0 with /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 712
			 * its original value to ensure multi-line strings /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 713
			 * are handled properly, and return to normal mode. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 714
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 719
		 * at this point the string is a full line and /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 720
		 * should be printed, up to the null. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 721
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 726
	 * Write to all consoles. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 727
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 746
		 * Check printed string to decide how to bump the /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 747
		 * kdb_nextline to control when the more prompt should /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 748
		 * show up. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 749
		/* Watch out for recursion here.  Any routine that calls /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 770
		 * kdb_printf will come back through here.  And kdb_read /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 771
		 * uses kdb_printf to echo on serial consoles ... /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 772
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 776
		 * Pause until cr. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 777
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 835
	 * For grep searches, shift the printed string left. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 836
	 *  replaced_byte contains the character that was overwritten with /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 837
	 *  the terminating null, and cphold points to the null. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 838
	 * Then adjust the notion of available space in the buffer. /Users/rubber/linux/kernel/debug/kdb/kdb_io.c: 839
 * Kernel Debugger Architecture Independent Breakpoint Handler /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 4
 * License.  See the file "COPYING" in the main directory of this archive /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 5
 * for more details. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 6
 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 8
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 9
 * Table of kdb_breakpoints /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 23
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 113
	 * Setup single step /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 114
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 118
	 * Reset delay attribute /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 119
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 128
	 * Install the breakpoint, if it is not already installed. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 129
 * kdb_bp_install /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 167
 *	Install kdb_breakpoints prior to returning from the /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 169
 *	kernel debugger.  This allows the kdb_breakpoints to be set /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 170
 *	upon functions that are used internally by kdb, such as /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 171
 *	printk().  This function is only called once per kdb session. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 172
 * kdb_bp_remove /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 191
 *	Remove kdb_breakpoints upon entry to the kernel debugger. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 193
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 195
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 196
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 197
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 198
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 199
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 200
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 201
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 202
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 203
 * kdb_printbp /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 223
 *	Internal function to format and print a breakpoint entry. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 225
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 227
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 228
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 229
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 230
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 231
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 232
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 233
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 234
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 235
 * kdb_bp /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 256
 *	Handle the bp commands. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 258
 *	[bp|bph] <addr-expression> [DATAR|DATAW] /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 260
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 262
 *	argc	Count of arguments in argv /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 263
 *	argv	Space delimited command line arguments /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 264
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 265
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 266
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 267
 *	Zero for success, a kdb diagnostic if failure. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 268
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 269
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 270
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 271
 *	bp	Set breakpoint on all cpus.  Only use hardware assist if need. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 273
 *	bph	Set breakpoint on all cpus.  Force hardware register /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 274
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 288
		 * Display breakpoint table /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 289
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 309
	 * This check is redundant (since the breakpoint machinery should /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 310
	 * be doing the same check during kdb_bp_install) but gives the /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 311
	 * user immediate feedback. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 312
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 318
	 * Find an empty bp structure to allocate /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 319
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 338
	 * Check for clashing breakpoints. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 339
	 * /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 340
	 * Note, in this design we can't have hardware breakpoints /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 341
	 * enabled for both read and write on the same address. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 342
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 356
	 * Actually allocate the breakpoint found earlier /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 357
 * kdb_bc /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 368
 *	Handles the 'bc', 'be', and 'bd' commands /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 370
 *	[bd|bc|be] <breakpoint-number> /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 372
 *	[bd|bc|be] * /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 373
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 375
 *	argc	Count of arguments in argv /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 376
 *	argv	Space delimited command line arguments /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 377
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 378
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 379
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 380
 *	Zero for success, a kdb diagnostic for failure /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 381
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 382
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 383
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 384
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 419
		 * For addresses less than the maximum breakpoint number, /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 420
		 * assume that the breakpoint number is desired. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 421
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 438
	 * Now operate on the set of breakpoints matching the input /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 439
	 * criteria (either '*' for all, or an individual breakpoint). /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 440
 * kdb_ss /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 493
 *	Process the 'ss' (Single Step) command. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 495
 *	ss /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 497
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 499
 *	argc	Argument count /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 500
 *	argv	Argument vector /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 501
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 502
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 503
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 504
 *	KDB_CMD_SS for success, a kdb error if failure. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 505
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 506
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 507
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 508
 *	Set the arch specific option to trigger a debug trap after the next /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 510
 *	instruction. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 511
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 518
	 * Set trace flag and go. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 519
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 580
	 * First time initialization. /Users/rubber/linux/kernel/debug/kdb/kdb_bp.c: 581
 * Kernel Debugger Architecture Independent Stack Traceback /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 4
 * License.  See the file "COPYING" in the main directory of this archive /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 5
 * for more details. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 6
 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 8
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 9
 * kdb_bt /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 40
 *	This function implements the 'bt' command.  Print a stack /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 42
 *	traceback. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 43
 *	bt [<address-expression>]	(addr-exp is for alternate stacks) /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 45
 *	btp <pid>			Kernel stack for <pid> /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 46
 *	btt <address-expression>	Kernel stack for task structure at /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 47
 *					<address-expression> /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 48
 *	bta [state_chars>|A]		All useful processes, optionally /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 49
 *					filtered by state /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 50
 *	btc [<cpu>]			The current process on one cpu, /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 51
 *					default is all cpus /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 52
 *	bt <address-expression> refers to a address on the stack, that location /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 54
 *	is assumed to contain a return address. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 55
 *	btt <address-expression> refers to the address of a struct task. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 57
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 59
 *	argc	argument count /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 60
 *	argv	argument vector /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 61
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 62
 *	None. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 63
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 64
 *	zero for success, a kdb diagnostic if error /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 65
 * Locking: /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 66
 *	none. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 67
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 68
 *	Backtrack works best when the code uses frame pointers.  But even /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 69
 *	without frame pointers we should get a reasonable trace. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 70
 *	mds comes in handy when examining the stack to do a manual traceback or /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 72
 *	to get a starting point for bt <address-expression>. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 73
			/* /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 192
			 * Recursive use of kdb_parse, do not use argv after /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 193
			 * this point. /Users/rubber/linux/kernel/debug/kdb/kdb_bt.c: 194
 * Kernel Debugger Architecture Independent Main Code /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 4
 * License.  See the file "COPYING" in the main directory of this archive /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 5
 * for more details. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 6
 * Copyright (C) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 8
 * Copyright (C) 2000 Stephane Eranian <eranian@hpl.hp.com> /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 9
 * Xscale (R) modifications copyright (C) 2003 Intel Corporation. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 10
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 11
 * Kernel debugger state flags /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 63
 * kdb_lock protects updates to kdb_initial_cpu.  Used to /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 68
 * single thread processors through the kernel debugger. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 69
 * Initial environment.   This is all kept static and local to /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 133
 * this file.   We don't want to rely on the memory allocation /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 134
 * mechanisms in the kernel, so we use a very limited allocate-only /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 135
 * heap for new and altered environment variables.  The entire /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 136
 * environment is limited to a fixed number of entries (add more /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 137
 * to __env[] if required) and a fixed amount of heap (add more to /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 138
 * KDB_ENVBUFSIZE if required). /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 139
 * Check whether the flags of the current command and the permissions /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 169
 * of the kdb console has allow a command to be run. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 170
 * kdbgetenv - This function will return the character string value of /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 189
 *	an environment variable. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 190
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 191
 *	match	A character string representing an environment variable. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 192
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 193
 *	NULL	No environment variable matches 'match' /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 194
 *	char*	Pointer to string value of environment variable. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 195
 * kdballocenv - This function is used to allocate bytes for /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 220
 *	environment entries. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 221
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 222
 *	match	A character string representing a numeric value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 223
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 224
 *	*value  the unsigned long representation of the env variable 'match' /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 225
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 226
 *	Zero on success, a kdb diagnostic on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 227
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 228
 *	We use a static environment buffer (envbuffer) to hold the values /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 229
 *	of dynamically generated environment variables (see kdb_set).  Buffer /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 230
 *	space once allocated is never free'd, so over time, the amount of space /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 231
 *	(currently 512 bytes) will be exhausted if env variables are changed /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 232
 *	frequently. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 233
 * kdbgetulenv - This function will return the value of an unsigned /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 250
 *	long-valued environment variable. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 251
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 252
 *	match	A character string representing a numeric value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 253
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 254
 *	*value  the unsigned long representation of the env variable 'match' /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 255
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 256
 *	Zero on success, a kdb diagnostic on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 257
 * kdbgetintenv - This function will return the value of an /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 275
 *	integer-valued environment variable. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 276
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 277
 *	match	A character string representing an integer-valued env variable /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 278
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 279
 *	*value  the integer representation of the environment variable 'match' /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 280
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 281
 *	Zero on success, a kdb diagnostic on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 282
 * kdb_setenv() - Alter an existing environment variable or create a new one. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 296
 * @var: Name of the variable /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 297
 * @val: Value of the variable /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 298
 * Return: Zero on success, a kdb diagnostic on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 300
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 326
	 * Wasn't existing variable.  Fit into slot. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 327
 * kdb_printenv() - Display the current environment variables. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 340
 * kdbgetularg - This function will convert a numeric string into an /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 353
 *	unsigned long value. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 354
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 355
 *	arg	A character string representing a numeric value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 356
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 357
 *	*value  the unsigned long representation of arg. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 358
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 359
 *	Zero on success, a kdb diagnostic on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 360
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 370
		 * Also try base 16, for us folks too lazy to type the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 371
		 * leading 0x... /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 372
 * kdb_set - This function implements the 'set' command.  Alter an /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 404
 *	existing environment variable or create a new one. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 405
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 409
	 * we can be invoked two ways: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 410
	 *   set var=value    argv[1]="var", argv[2]="value" /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 411
	 *   set var = value  argv[1]="var", argv[2]="=", argv[3]="value" /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 412
	 * - if the latter, shift 'em down. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 413
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 423
	 * Censor sensitive variables /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 424
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 430
	 * Check for internal variables /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 431
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 449
	 * Tokenizer squashed the '=' sign.  argv[1] is variable /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 450
	 * name, argv[2] = value. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 451
 * kdbgetaddrarg - This function is responsible for parsing an /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 467
 *	address-expression and returning the value of the expression, /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 468
 *	symbol name, and offset to the caller. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 469
 *	The argument may consist of a numeric value (decimal or /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 471
 *	hexadecimal), a symbol name, a register name (preceded by the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 472
 *	percent sign), an environment variable with a numeric value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 473
 *	(preceded by a dollar sign) or a simple arithmetic expression /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 474
 *	consisting of a symbol name, +/-, and a numeric constant value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 475
 *	(offset). /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 476
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 477
 *	argc	- count of arguments in argv /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 478
 *	argv	- argument vector /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 479
 *	*nextarg - index to next unparsed argument in argv[] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 480
 *	regs	- Register state at time of KDB entry /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 481
 * Outputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 482
 *	*value	- receives the value of the address-expression /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 483
 *	*offset - receives the offset specified, if any /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 484
 *	*name   - receives the symbol name, if any /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 485
 *	*nextarg - index to next unparsed argument in argv[] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 486
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 487
 *	zero is returned on success, a kdb diagnostic code is /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 488
 *      returned on error. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 489
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 505
	 * If the enable flags prohibit both arbitrary memory access /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 506
	 * and flow control then there are no reasonable grounds to /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 507
	 * provide symbol lookup. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 508
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 514
	 * Process arguments which follow the following syntax: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 515
	 * /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 516
	 *  symbol | numeric-address [+/- numeric-offset] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 517
	 *  %register /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 518
	 *  $environment-variable /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 519
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 527
	 * If there is no whitespace between the symbol /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 528
	 * or address and the '+' or '-' symbols, we /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 529
	 * remember the character and replace it with a /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 530
	 * null so the symbol/value can be properly parsed /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 531
		/* Implement register values with % at a later time as it is /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 547
		 * arch optional. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 548
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 578
	 * check for +/- and offset /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 579
			/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 585
			 * Not our argument.  Return. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 586
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 596
	 * Now there must be an offset! /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 597
 * kdb_defcmd, kdb_defcmd2 - This function implements the 'defcmd' /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 645
 *	command which defines one command as a set of other commands, /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 646
 *	terminated by endefcmd.  kdb_defcmd processes the initial /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 647
 *	'defcmd' command, kdb_defcmd2 is invoked from kdb_parse for /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 648
 *	the following commands until 'endefcmd'. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 649
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 650
 *	argc	argument count /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 651
 *	argv	argument vector /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 652
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 653
 *	zero for success, a kdb diagnostic if error /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 654
 * kdb_exec_defcmd - Execute the set of commands associated with this /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 773
 *	defcmd name. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 774
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 775
 *	argc	argument count /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 776
 *	argv	argument vector /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 777
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 778
 *	zero for success, a kdb diagnostic if error /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 779
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 802
		 * Recursive use of kdb_parse, do not use argv after this point. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 803
#define CMD_BUFLEN		200	/* kdb_printf: max printline /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 816
 * The "str" argument may point to something like  | grep xyz /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 824
		/* allow it be "x y z" by removing the "'s - there must /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 854
 * kdb_parse - Parse the command line, search the command table for a /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 888
 *	matching command and invoke the command function.  This /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 889
 *	function may be called recursively, if it is, the second call /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 890
 *	will overwrite argv and cbuf.  It is the caller's /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 891
 *	responsibility to save their argv if they recursively call /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 892
 *	kdb_parse(). /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 893
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 894
 *      cmdstr	The input command line to be parsed. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 895
 *	regs	The registers at the time kdb was entered. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 896
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 897
 *	Zero for success, a kdb diagnostic if failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 898
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 899
 *	Limited to 20 tokens. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 900
 *	Real rudimentary tokenization. Basically only whitespace /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 902
 *	is considered a token delimiter (but special consideration /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 903
 *	is taken of the '=' sign as used by the 'set' command). /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 904
 *	The algorithm used to tokenize the input string relies on /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 906
 *	there being at least one whitespace (or otherwise useless) /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 907
 *	character between tokens as the character immediately following /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 908
 *	the token is altered in-place to a null-byte to terminate the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 909
 *	token string. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 910
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 925
	 * First tokenize the command string. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 926
		/* Previous command was interrupted, newline must not /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 931
			/* Copy to next unquoted and unescaped /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 967
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1014
		 * If this command is allowed to be abbreviated, /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1015
		 * check to see if this is it. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1016
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1026
	 * If we don't find a command by this name, see if the first /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1027
	 * few characters of this match any of the known commands. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1028
	 * e.g., md1c20 should match md. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1029
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1059
	 * If the input with which we were presented does not /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1060
	 * map to an existing command, attempt to parse it as an /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1061
	 * address argument and display the result.   Useful for /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1062
	 * obtaining the address of a variable, or the nearest symbol /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1063
	 * to an address contained in a register. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1064
 * kdb_reboot - This function implements the 'reboot' command.  Reboot /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1110
 *	the system immediately, or loop for ever on failure. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1111
 * kdb_local - The main code for kdb.  This routine is invoked on a /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1156
 *	specific processor, it is not global.  The main kdb() routine /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1157
 *	ensures that only one processor at a time is in this routine. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1158
 *	This code is called with the real reason code on the first /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1159
 *	entry to a kdb session, thereafter it is called with reason /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1160
 *	SWITCH, even if the user goes back to the original cpu. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1161
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1162
 *	reason		The reason KDB was invoked /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1163
 *	error		The hardware-defined error code /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1164
 *	regs		The exception frame at time of fault/breakpoint. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1165
 *	db_result	Result code from the break or debug point. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1166
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1167
 *	0	KDB was invoked for an event which it wasn't responsible /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1168
 *	1	KDB handled the event for which it was invoked. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1169
 *	KDB_CMD_GO	User typed 'go'. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1170
 *	KDB_CMD_CPU	User switched to another cpu. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1171
 *	KDB_CMD_SS	Single step. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1172
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1197
		 * If re-entering kdb after a single step /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1198
		 * command, don't print the message. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1199
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1258
		 * Determine if this breakpoint is one that we /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1259
		 * are interested in. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1260
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1280
		 * Initialize pager context. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1281
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1300
		 * Fetch command from keyboard /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1301
 * kdb_print_state - Print the state data for the current processor /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1348
 *	for debugging. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1349
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1350
 *	text		Identifies the debug point /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1351
 *	value		Any integer value to be printed, e.g. reason code. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1352
 * kdb_main_loop - After initial setup and assignment of the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1362
 *	controlling cpu, all cpus are in this loop.  One cpu is in /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1363
 *	control and will issue the kdb prompt, the others will spin /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1364
 *	until 'go' or cpu switch. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1365
 *	To get a consistent view of the kernel stacks for all /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1367
 *	processes, this routine is invoked from the main kdb code via /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1368
 *	an architecture specific routine.  kdba_main_loop is /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1369
 *	responsible for making the kernel stacks consistent for all /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1370
 *	processes, there should be no difference between a blocked /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1371
 *	process and a running process as far as kdb is concerned. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1372
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1373
 *	reason		The reason KDB was invoked /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1374
 *	error		The hardware-defined error code /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1375
 *	reason2		kdb's current reason code. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1376
 *			Initially error but can change /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1377
 *			according to kdb state. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1378
 *	db_result	Result code from break or debug point. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1379
 *	regs		The exception frame at time of fault/breakpoint. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1380
 *			should always be valid. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1381
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1382
 *	0	KDB was invoked for an event which it wasn't responsible /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1383
 *	1	KDB handled the event for which it was invoked. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1384
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1392
		 * All processors except the one that is in control /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1393
		 * will spin here. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1394
			/* state KDB is turned off by kdb_cpu to see if the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1398
			 * other cpus are still live, each cpu in this loop /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1399
			 * turns it back on. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1400
 * kdb_mdr - This function implements the guts of the 'mdr', memory /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1444
 * read command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1445
 *	mdr  <addr arg>,<byte count> /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1446
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1447
 *	addr	Start address /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1448
 *	count	Number of bytes /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1449
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1450
 *	Always 0.  Any errors are detected and printed by kdb_getarea. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1451
 * kdb_md - This function implements the 'md', 'md1', 'md2', 'md4', /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1467
 *	'md8' 'mdr' and 'mds' commands. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1468
 *	md|mds  [<addr arg> [<line count> [<radix>]]] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1470
 *	mdWcN	[<addr arg> [<line count> [<radix>]]] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1471
 *		where W = is the width (1, 2, 4 or 8) and N is the count. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1472
 *		for eg., md1c20 reads 20 bytes, 1 at a time. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1473
 *	mdr  <addr arg>,<byte count> /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1474
 to make REPEAT happy /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1642
		/* Do not save these changes as last_*, they are temporary mds /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1687
		 * overrides. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1688
 * kdb_mm - This function implements the 'mm' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1734
 *	mm address-expression new-value /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1735
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1736
 *	mm works on machine words, mmW works on bytes. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1737
 * kdb_go - This function implements the 'go' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1779
 *	go [address-expression] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1780
 * kdb_rd - This function implements the 'rd' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1825
 * kdb_rm - This function implements the 'rm' (register modify)  command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1892
 *	rm register-name new-contents /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1893
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1894
 *	Allows register modification with the same restrictions as gdb /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1895
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1910
	 * Allow presence or absence of leading '%' symbol. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1911
 * kdb_sr - This function implements the 'sr' (SYSRQ key) command /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1960
 *	which interfaces to the soi-disant MAGIC SYSRQ functionality. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1961
 *		sr <magic-sysrq-code> /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1962
 * kdb_ef - This function implements the 'regs' (display exception /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1981
 *	frame) command.  This command takes an address and expects to /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1982
 *	find an exception frame at that address, formats and prints /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1983
 *	it. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1984
 *		regs address-expression /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1985
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1986
 *	Not done yet. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 1987
 * kdb_lsmod - This function implements the 'lsmod' command.  Lists /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2009
 *	currently loaded kernel modules. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2010
 *	Mostly taken from userland lsmod. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2011
 * kdb_env - This function implements the 'env' command.  Display the /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2056
 *	current environment variables. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2057
 * kdb_dmesg - This function implements the 'dmesg' command to display /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2073
 *	the contents of the syslog buffer. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2074
 *		dmesg [lines] [adjust] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2075
 * kdb_cpu - This function implements the 'cpu' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2187
 *	cpu	[<cpunum>] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2188
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2189
 *	KDB_CMD_CPU for success, a kdb diagnostic if error /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2190
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2254
	 * Validate cpunum /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2255
	/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2262
	 * Switch to other cpu /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2263
/* The user may not realize that ps/bta with no parameters does not print idle /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2268
 * or sleeping system daemon processes, so tell them how many were suppressed. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2269
 * kdb_ps - This function implements the 'ps' command which shows a /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2327
 *	    list of the active processes. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2328
 * ps [<state_chars>]   Show processes, optionally selecting only those whose /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2330
 *                      state character is found in <state_chars>. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2331
 * kdb_pid - This function implements the 'pid' command which switches /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2366
 *	the currently active process. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2367
 *		pid [<pid> | R] /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2368
 * kdb_help - This function implements the 'help' and '?' commands. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2408
 * kdb_kill - This function implements the 'kill' commands. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2432
 * Most of this code has been lifted from kernel/timer.c::sys_sysinfo(). /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2472
 * I cannot call that code directly from kdb, it has an unconditional /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2473
 * cli()/sti() and calls routines that take locks which can stop the debugger. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2474
 * kdb_summary - This function implements the 'summary' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2492
 * kdb_per_cpu - This function implements the 'per_cpu' command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2534
	/* Most architectures use __per_cpu_offset[cpu], some use /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2569
	 * __per_cpu_offset(cpu), smp has no __per_cpu_offset. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2570
 * display help for the use of cmd | grep pattern /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2604
 * kdb_register() - This function is used to register a kernel debugger /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2623
 *                  command. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2624
 * @cmd: pointer to kdb command /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2625
 * Note that it's the job of the caller to keep the memory for the cmd /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2627
 * allocated until unregister is called. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2628
 * kdb_register_table() - This function is used to register a kdb command /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2648
 *                        table. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2649
 * @kp: pointer to kdb command table /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2650
 * @len: length of kdb command table /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2651
 * kdb_unregister() - This function is used to unregister a kernel debugger /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2662
 *                    command. It is generally called when a module which /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2663
 *                    implements kdb command is unloaded. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2664
 * @cmd: pointer to kdb command /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2665
		/* /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2845
		 * Macros are always safe because when executed each /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2846
		 * internal command re-enters kdb_parse() and is safety /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2847
		 * checked individually. /Users/rubber/linux/kernel/debug/kdb/kdb_main.c: 2848
 * Kernel Debugger Architecture Independent Support Functions /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 2
 * This file is subject to the terms and conditions of the GNU General Public /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 4
 * License.  See the file "COPYING" in the main directory of this archive /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 5
 * for more details. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 6
 * Copyright (c) 1999-2004 Silicon Graphics, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 8
 * Copyright (c) 2009 Wind River Systems, Inc.  All Rights Reserved. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 9
 * 03/02/13    added new 2.5 kallsyms <xavier.bru@bull.net> /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 10
 * kdbgetsymval - Return the address of the given symbol. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 31
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 33
 *	symname	Character string containing symbol name /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 34
 *      symtab  Structure to receive results /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 35
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 36
 *	0	Symbol not found, symtab zero filled /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 37
 *	1	Symbol mapped to module/symbol/section, data in symtab /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 38
 * kdbnearsym() - Return the name of the symbol with the nearest address /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 56
 *                less than @addr. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 57
 * @addr: Address to check for near symbol /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 58
 * @symtab: Structure to receive results /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 59
 * WARNING: This function may return a pointer to a single statically /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 61
 * allocated buffer (namebuf). kdb's unusual calling context (single /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 62
 * threaded, all other CPUs halted) provides us sufficient locking for /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 63
 * this to be safe. The only constraint imposed by the static buffer is /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 64
 * that the caller must consume any previous reply prior to another call /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 65
 * to lookup a new symbol. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 66
 * Note that, strictly speaking, some architectures may re-enter the kdb /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 68
 * trap if the system turns out to be very badly damaged and this breaks /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 69
 * the single-threaded assumption above. In these circumstances successful /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 70
 * continuation and exit from the inner trap is unlikely to work and any /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 71
 * user attempting this receives a prominent warning before being allowed /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 72
 * to progress. In these circumstances we remain memory safe because /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 73
 * namebuf[KSYM_NAME_LEN-1] will never change from '\0' although we do /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 74
 * tolerate the possibility of garbled symbol display from the outer kdb /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 75
 * trap. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 76
 * Return: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 78
 * * 0 - No sections contain this address, symtab zero filled /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 79
 * * 1 - Address mapped to module/symbol/section, data in symtab /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 80
 * kallsyms_symbol_complete /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 116
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 118
 *	prefix_name	prefix of a symbol name to lookup /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 119
 *	max_len		maximum length that can be returned /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 120
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 121
 *	Number of symbols which match the given prefix. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 122
 * Notes: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 123
 *	prefix_name is changed to contain the longest unique prefix that /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 124
 *	starts with this prefix (tab completion). /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 125
 * kallsyms_symbol_next /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 160
 * Parameters: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 162
 *	prefix_name	prefix of a symbol name to lookup /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 163
 *	flag	0 means search from the head, 1 means continue search. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 164
 *	buf_size	maximum length that can be written to prefix_name /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 165
 *			buffer /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 166
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 167
 *	1 if a symbol matches the given prefix. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 168
 *	0 if no string found /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 169
 * kdb_symbol_print - Standard method for printing a symbol name and offset. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 188
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 189
 *	addr	Address to be printed. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 190
 *	symtab	Address of symbol data, if NULL this routine does its /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 191
 *		own lookup. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 192
 *	punc	Punctuation for string, bit field. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 193
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 194
 *	The string and its punctuation is only printed if the address /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 195
 *	is inside the kernel, except that the value is always printed /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 196
 *	when requested. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 197
 * kdb_strdup - kdb equivalent of strdup, for disasm code. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 238
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 239
 *	str	The string to duplicate. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 240
 *	type	Flags to kmalloc for the new string. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 241
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 242
 *	Address of the new string, NULL if storage could not be allocated. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 243
 * Remarks: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 244
 *	This is not in lib/string.c because it uses kmalloc which is not /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 245
 *	available when string.o is used in boot loaders. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 246
 * kdb_getarea_size - Read an area of data.  The kdb equivalent of /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 258
 *	copy_from_user, with kdb messages for invalid addresses. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 259
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 260
 *	res	Pointer to the area to receive the result. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 261
 *	addr	Address of the area to copy. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 262
 *	size	Size of the area. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 263
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 264
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 265
 * kdb_putarea_size - Write an area of data.  The kdb equivalent of /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 283
 *	copy_to_user, with kdb messages for invalid addresses. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 284
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 285
 *	addr	Address of the area to write to. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 286
 *	res	Pointer to the area holding the data. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 287
 *	size	Size of the area. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 288
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 289
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 290
 * kdb_getphys - Read data from a physical address. Validate the /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 308
 * 	address is in range, use kmap_atomic() to get data /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 309
 * 	similar to kdb_getarea() - but for phys addresses /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 310
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 311
 * 	res	Pointer to the word to receive the result /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 312
 * 	addr	Physical address of the area to copy /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 313
 * 	size	Size of the area /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 314
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 315
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 316
 * kdb_getphysword /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 336
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 337
 *	word	Pointer to the word to receive the result. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 338
 *	addr	Address of the area to copy. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 339
 *	size	Size of the area. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 340
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 341
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 342
 * kdb_getword - Read a binary value.  Unlike kdb_getarea, this treats /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 385
 *	data as numbers. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 386
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 387
 *	word	Pointer to the word to receive the result. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 388
 *	addr	Address of the area to copy. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 389
 *	size	Size of the area. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 390
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 391
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 392
 * kdb_putword - Write a binary value.  Unlike kdb_putarea, this /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 434
 *	treats data as numbers. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 435
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 436
 *	addr	Address of the area to write to.. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 437
 *	word	The value to set. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 438
 *	size	Size of the area. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 439
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 440
 *	0 for success, < 0 for error. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 441
 * kdb_task_state_char - Return the character that represents the task state. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 480
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 481
 *	p	struct task for the process /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 482
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 483
 *	One character to represent the task state. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 484
		/* Idle task.  Is it really idle, apart from the kdb /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 499
 * kdb_task_state - Return true if a process has the desired state /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 513
 *	given by the mask. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 514
 * Inputs: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 515
 *	p	struct task for the process /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 516
 *	mask	set of characters used to select processes; both NULL /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 517
 *	        and the empty string mean adopt a default filter, which /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 518
 *	        is to suppress sleeping system daemons and the idle tasks /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 519
 * Returns: /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 520
 *	True if the process matches at least one criteria defined by the mask. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 521
	/* If there is no mask, then we will filter code that runs when the /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 527
	 * scheduler is idling and any system daemons that are currently /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 528
	 * sleeping. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 529
/* Maintain a small stack of kdb_flags to allow recursion without disturbing /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 541
 * the global kdb state. /Users/rubber/linux/kernel/debug/kdb/kdb_support.c: 542
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/disk-events.c: 1
 * Disk events - monitor disk events like media change and eject request. /Users/rubber/linux/block/disk-events.c: 3
	/* /Users/rubber/linux/block/disk-events.c: 46
	 * If device-specific poll interval is set, always use it.  If /Users/rubber/linux/block/disk-events.c: 47
	 * the default is being used, poll if the POLL flag is set. /Users/rubber/linux/block/disk-events.c: 48
 * disk_block_events - block and flush disk event checking /Users/rubber/linux/block/disk-events.c: 59
 * @disk: disk to block events for /Users/rubber/linux/block/disk-events.c: 60
 * On return from this function, it is guaranteed that event checking /Users/rubber/linux/block/disk-events.c: 62
 * isn't in progress and won't happen until unblocked by /Users/rubber/linux/block/disk-events.c: 63
 * disk_unblock_events().  Events blocking is counted and the actual /Users/rubber/linux/block/disk-events.c: 64
 * unblocking happens after the matching number of unblocks are done. /Users/rubber/linux/block/disk-events.c: 65
 * Note that this intentionally does not block event checking from /Users/rubber/linux/block/disk-events.c: 67
 * disk_clear_events(). /Users/rubber/linux/block/disk-events.c: 68
 * CONTEXT: /Users/rubber/linux/block/disk-events.c: 70
 * Might sleep. /Users/rubber/linux/block/disk-events.c: 71
	/* /Users/rubber/linux/block/disk-events.c: 82
	 * Outer mutex ensures that the first blocker completes canceling /Users/rubber/linux/block/disk-events.c: 83
	 * the event work before further blockers are allowed to finish. /Users/rubber/linux/block/disk-events.c: 84
 * disk_unblock_events - unblock disk event checking /Users/rubber/linux/block/disk-events.c: 124
 * @disk: disk to unblock events for /Users/rubber/linux/block/disk-events.c: 125
 * Undo disk_block_events().  When the block count reaches zero, it /Users/rubber/linux/block/disk-events.c: 127
 * starts events polling if configured. /Users/rubber/linux/block/disk-events.c: 128
 * CONTEXT: /Users/rubber/linux/block/disk-events.c: 130
 * Don't care.  Safe to call from irq context. /Users/rubber/linux/block/disk-events.c: 131
 * disk_flush_events - schedule immediate event checking and flushing /Users/rubber/linux/block/disk-events.c: 140
 * @disk: disk to check and flush events for /Users/rubber/linux/block/disk-events.c: 141
 * @mask: events to flush /Users/rubber/linux/block/disk-events.c: 142
 * Schedule immediate event checking on @disk if not blocked.  Events in /Users/rubber/linux/block/disk-events.c: 144
 * @mask are scheduled to be cleared from the driver.  Note that this /Users/rubber/linux/block/disk-events.c: 145
 * doesn't clear the events from @disk->ev. /Users/rubber/linux/block/disk-events.c: 146
 * CONTEXT: /Users/rubber/linux/block/disk-events.c: 148
 * If @mask is non-zero must be called with disk->open_mutex held. /Users/rubber/linux/block/disk-events.c: 149
 * Tell userland about new events.  Only the events listed in @disk->events are /Users/rubber/linux/block/disk-events.c: 167
 * reported, and only if DISK_EVENT_FLAG_UEVENT is set.  Otherwise, events are /Users/rubber/linux/block/disk-events.c: 168
 * processed internally but never get reported to userland. /Users/rubber/linux/block/disk-events.c: 169
 * disk_clear_events - synchronously check, clear and return pending events /Users/rubber/linux/block/disk-events.c: 217
 * @disk: disk to fetch and clear events from /Users/rubber/linux/block/disk-events.c: 218
 * @mask: mask of events to be fetched and cleared /Users/rubber/linux/block/disk-events.c: 219
 * Disk events are synchronously checked and pending events in @mask /Users/rubber/linux/block/disk-events.c: 221
 * are cleared and returned.  This ignores the block count. /Users/rubber/linux/block/disk-events.c: 222
 * CONTEXT: /Users/rubber/linux/block/disk-events.c: 224
 * Might sleep. /Users/rubber/linux/block/disk-events.c: 225
	/* /Users/rubber/linux/block/disk-events.c: 238
	 * store the union of mask and ev->clearing on the stack so that the /Users/rubber/linux/block/disk-events.c: 239
	 * race with disk_flush_events does not cause ambiguity (ev->clearing /Users/rubber/linux/block/disk-events.c: 240
	 * can still be modified even if events are blocked). /Users/rubber/linux/block/disk-events.c: 241
	/* /Users/rubber/linux/block/disk-events.c: 249
	 * if ev->clearing is not 0, the disk_flush_events got called in the /Users/rubber/linux/block/disk-events.c: 250
	 * middle of this function, so we want to run the workfn without delay. /Users/rubber/linux/block/disk-events.c: 251
 * bdev_check_media_change - check if a removable media has been changed /Users/rubber/linux/block/disk-events.c: 266
 * @bdev: block device to check /Users/rubber/linux/block/disk-events.c: 267
 * Check whether a removable media has been changed, and attempt to free all /Users/rubber/linux/block/disk-events.c: 269
 * dentries and inodes and invalidates all block device page cache entries in /Users/rubber/linux/block/disk-events.c: 270
 * that case. /Users/rubber/linux/block/disk-events.c: 271
 * Returns %true if the block device changed, or %false if not. /Users/rubber/linux/block/disk-events.c: 273
 * disk_force_media_change - force a media change event /Users/rubber/linux/block/disk-events.c: 293
 * @disk: the disk which will raise the event /Users/rubber/linux/block/disk-events.c: 294
 * @events: the events to raise /Users/rubber/linux/block/disk-events.c: 295
 * Generate uevents for the disk. If DISK_EVENT_MEDIA_CHANGE is present, /Users/rubber/linux/block/disk-events.c: 297
 * attempt to free all dentries and inodes and invalidates all block /Users/rubber/linux/block/disk-events.c: 298
 * device page cache entries in that case. /Users/rubber/linux/block/disk-events.c: 299
 * Returns %true if DISK_EVENT_MEDIA_CHANGE was raised, or %false if not. /Users/rubber/linux/block/disk-events.c: 301
 * Separate this part out so that a different pointer for clearing_ptr can be /Users/rubber/linux/block/disk-events.c: 319
 * passed in for disk_clear_events. /Users/rubber/linux/block/disk-events.c: 320
 * A disk events enabled device has the following sysfs nodes under /Users/rubber/linux/block/disk-events.c: 331
 * its /sys/block/X/ directory. /Users/rubber/linux/block/disk-events.c: 332
 * events		: list of all supported events /Users/rubber/linux/block/disk-events.c: 334
 * events_async		: list of events which can be detected w/o polling /Users/rubber/linux/block/disk-events.c: 335
 *			  (always empty, only for backwards compatibility) /Users/rubber/linux/block/disk-events.c: 336
 * events_poll_msecs	: polling interval, 0: disable, -1: system default /Users/rubber/linux/block/disk-events.c: 337
 * The default polling interval can be specified by the kernel /Users/rubber/linux/block/disk-events.c: 411
 * parameter block.events_dfl_poll_msecs which defaults to 0 /Users/rubber/linux/block/disk-events.c: 412
 * (disable).  This can also be modified runtime by writing to /Users/rubber/linux/block/disk-events.c: 413
 * /sys/module/block/parameters/events_dfl_poll_msecs. /Users/rubber/linux/block/disk-events.c: 414
 * disk_{alloc|add|del|release}_events - initialize and destroy disk_events. /Users/rubber/linux/block/disk-events.c: 445
	/* /Users/rubber/linux/block/disk-events.c: 481
	 * Block count is initialized to 1 and the following initial /Users/rubber/linux/block/disk-events.c: 482
	 * unblock kicks it into action. /Users/rubber/linux/block/disk-events.c: 483
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/t10-pi.c: 1
 * t10_pi.c - Functions for generating and verifying T10 Protection /Users/rubber/linux/block/t10-pi.c: 3
 *	      Information. /Users/rubber/linux/block/t10-pi.c: 4
 * Type 1 and Type 2 protection use the same format: 16 bit guard tag, /Users/rubber/linux/block/t10-pi.c: 26
 * 16 bit app tag, 32 bit reference tag. Type 3 does not define the ref /Users/rubber/linux/block/t10-pi.c: 27
 * tag. /Users/rubber/linux/block/t10-pi.c: 28
 * t10_pi_type1_prepare - prepare PI prior submitting request to device /Users/rubber/linux/block/t10-pi.c: 124
 * @rq:              request with PI that should be prepared /Users/rubber/linux/block/t10-pi.c: 125
 * For Type 1/Type 2, the virtual start sector is the one that was /Users/rubber/linux/block/t10-pi.c: 127
 * originally submitted by the block layer for the ref_tag usage. Due to /Users/rubber/linux/block/t10-pi.c: 128
 * partitioning, MD/DM cloning, etc. the actual physical start sector is /Users/rubber/linux/block/t10-pi.c: 129
 * likely to be different. Remap protection information to match the /Users/rubber/linux/block/t10-pi.c: 130
 * physical LBA. /Users/rubber/linux/block/t10-pi.c: 131
 * t10_pi_type1_complete - prepare PI prior returning request to the blk layer /Users/rubber/linux/block/t10-pi.c: 171
 * @rq:              request with PI that should be prepared /Users/rubber/linux/block/t10-pi.c: 172
 * @nr_bytes:        total bytes to prepare /Users/rubber/linux/block/t10-pi.c: 173
 * For Type 1/Type 2, the virtual start sector is the one that was /Users/rubber/linux/block/t10-pi.c: 175
 * originally submitted by the block layer for the ref_tag usage. Due to /Users/rubber/linux/block/t10-pi.c: 176
 * partitioning, MD/DM cloning, etc. the actual physical start sector is /Users/rubber/linux/block/t10-pi.c: 177
 * likely to be different. Since the physical start sector was submitted /Users/rubber/linux/block/t10-pi.c: 178
 * to the device, we should remap it back to virtual values expected by the /Users/rubber/linux/block/t10-pi.c: 179
 * block layer. /Users/rubber/linux/block/t10-pi.c: 180
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-crypto-fallback.c: 1
 * Copyright 2019 Google LLC /Users/rubber/linux/block/blk-crypto-fallback.c: 3
 * Refer to Documentation/block/inline-encryption.rst for detailed explanation. /Users/rubber/linux/block/blk-crypto-fallback.c: 7
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 42
	 * Copy of the bvec_iter when this bio was submitted. /Users/rubber/linux/block/blk-crypto-fallback.c: 43
	 * We only want to en/decrypt the part of the bio as described by the /Users/rubber/linux/block/blk-crypto-fallback.c: 44
	 * bvec_iter upon submission because bio might be split before being /Users/rubber/linux/block/blk-crypto-fallback.c: 45
	 * resubmitted /Users/rubber/linux/block/blk-crypto-fallback.c: 46
 * Allocating a crypto tfm during I/O can deadlock, so we have to preallocate /Users/rubber/linux/block/blk-crypto-fallback.c: 65
 * all of a mode's tfms when that mode starts being used. Since each mode may /Users/rubber/linux/block/blk-crypto-fallback.c: 66
 * need all the keyslots at some point, each mode needs its own tfm for each /Users/rubber/linux/block/blk-crypto-fallback.c: 67
 * keyslot; thus, a keyslot may contain tfms for multiple modes.  However, to /Users/rubber/linux/block/blk-crypto-fallback.c: 68
 * match the behavior of real inline encryption hardware (which only supports a /Users/rubber/linux/block/blk-crypto-fallback.c: 69
 * single encryption context per keyslot), we only allow one tfm per keyslot to /Users/rubber/linux/block/blk-crypto-fallback.c: 70
 * be used at a time - the rest of the unused tfms have their keys cleared. /Users/rubber/linux/block/blk-crypto-fallback.c: 71
 * This is the key we set when evicting a keyslot. This *should* be the all 0's /Users/rubber/linux/block/blk-crypto-fallback.c: 87
 * key, but AES-XTS rejects that key, so we use some random bytes instead. /Users/rubber/linux/block/blk-crypto-fallback.c: 88
 * The crypto API fallback's encryption routine. /Users/rubber/linux/block/blk-crypto-fallback.c: 255
 * Allocate a bounce bio for encryption, encrypt the input bio using crypto API, /Users/rubber/linux/block/blk-crypto-fallback.c: 256
 * and replace *bio_ptr with the bounce bio. May split input bio if it's too /Users/rubber/linux/block/blk-crypto-fallback.c: 257
 * large. Returns true on success. Returns false and sets bio->bi_status on /Users/rubber/linux/block/blk-crypto-fallback.c: 258
 * error. /Users/rubber/linux/block/blk-crypto-fallback.c: 259
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 291
	 * Get a blk-crypto-fallback keyslot that contains a crypto_skcipher for /Users/rubber/linux/block/blk-crypto-fallback.c: 292
	 * this bio's algorithm and key. /Users/rubber/linux/block/blk-crypto-fallback.c: 293
 * The crypto API fallback's main decryption routine. /Users/rubber/linux/block/blk-crypto-fallback.c: 373
 * Decrypts input bio in place, and calls bio_endio on the bio. /Users/rubber/linux/block/blk-crypto-fallback.c: 374
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 394
	 * Get a blk-crypto-fallback keyslot that contains a crypto_skcipher for /Users/rubber/linux/block/blk-crypto-fallback.c: 395
	 * this bio's algorithm and key. /Users/rubber/linux/block/blk-crypto-fallback.c: 396
 * blk_crypto_fallback_decrypt_endio - queue bio for fallback decryption /Users/rubber/linux/block/blk-crypto-fallback.c: 444
 * @bio: the bio to queue /Users/rubber/linux/block/blk-crypto-fallback.c: 446
 * Restore bi_private and bi_end_io, and queue the bio for decryption into a /Users/rubber/linux/block/blk-crypto-fallback.c: 448
 * workqueue, since this function will be called from an atomic context. /Users/rubber/linux/block/blk-crypto-fallback.c: 449
 * blk_crypto_fallback_bio_prep - Prepare a bio to use fallback en/decryption /Users/rubber/linux/block/blk-crypto-fallback.c: 471
 * @bio_ptr: pointer to the bio to prepare /Users/rubber/linux/block/blk-crypto-fallback.c: 473
 * If bio is doing a WRITE operation, this splits the bio into two parts if it's /Users/rubber/linux/block/blk-crypto-fallback.c: 475
 * too big (see blk_crypto_fallback_split_bio_if_needed()). It then allocates a /Users/rubber/linux/block/blk-crypto-fallback.c: 476
 * bounce bio for the first part, encrypts it, and updates bio_ptr to point to /Users/rubber/linux/block/blk-crypto-fallback.c: 477
 * the bounce bio. /Users/rubber/linux/block/blk-crypto-fallback.c: 478
 * For a READ operation, we mark the bio for decryption by using bi_private and /Users/rubber/linux/block/blk-crypto-fallback.c: 480
 * bi_end_io. /Users/rubber/linux/block/blk-crypto-fallback.c: 481
 * In either case, this function will make the bio look like a regular bio (i.e. /Users/rubber/linux/block/blk-crypto-fallback.c: 483
 * as if no encryption context was ever specified) for the purposes of the rest /Users/rubber/linux/block/blk-crypto-fallback.c: 484
 * of the stack except for blk-integrity (blk-integrity and blk-crypto are not /Users/rubber/linux/block/blk-crypto-fallback.c: 485
 * currently supported together). /Users/rubber/linux/block/blk-crypto-fallback.c: 486
 * Return: true on success. Sets bio->bi_status and returns false on error. /Users/rubber/linux/block/blk-crypto-fallback.c: 488
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 511
	 * bio READ case: Set up a f_ctx in the bio's bi_private and set the /Users/rubber/linux/block/blk-crypto-fallback.c: 512
	 * bi_end_io appropriately to trigger decryption when the bio is ended. /Users/rubber/linux/block/blk-crypto-fallback.c: 513
 * Prepare blk-crypto-fallback for the specified crypto mode. /Users/rubber/linux/block/blk-crypto-fallback.c: 608
 * Returns -ENOPKG if the needed crypto API support is missing. /Users/rubber/linux/block/blk-crypto-fallback.c: 609
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 618
	 * Fast path /Users/rubber/linux/block/blk-crypto-fallback.c: 619
	 * Ensure that updates to blk_crypto_keyslots[i].tfms[mode_num] /Users/rubber/linux/block/blk-crypto-fallback.c: 620
	 * for each i are visible before we try to access them. /Users/rubber/linux/block/blk-crypto-fallback.c: 621
	/* /Users/rubber/linux/block/blk-crypto-fallback.c: 652
	 * Ensure that updates to blk_crypto_keyslots[i].tfms[mode_num] /Users/rubber/linux/block/blk-crypto-fallback.c: 653
	 * for each i are visible before we set tfms_inited[mode_num]. /Users/rubber/linux/block/blk-crypto-fallback.c: 654
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-merge.c: 1
 * Functions related to segment and merge handling /Users/rubber/linux/block/blk-merge.c: 3
	/* /Users/rubber/linux/block/blk-merge.c: 41
	 * iter.bi_bvec_done records actual length of the last bvec /Users/rubber/linux/block/blk-merge.c: 42
	 * if this bio ends in the middle of one io vector /Users/rubber/linux/block/blk-merge.c: 43
	/* /Users/rubber/linux/block/blk-merge.c: 57
	 * Don't merge if the 1st bio starts with non-zero offset, otherwise it /Users/rubber/linux/block/blk-merge.c: 58
	 * is quite difficult to respect the sg gap limit.  We work hard to /Users/rubber/linux/block/blk-merge.c: 59
	 * merge a huge number of small single bios in case of mkfs. /Users/rubber/linux/block/blk-merge.c: 60
	/* /Users/rubber/linux/block/blk-merge.c: 69
	 * We don't need to worry about the situation that the merged segment /Users/rubber/linux/block/blk-merge.c: 70
	 * ends in unaligned virt boundary: /Users/rubber/linux/block/blk-merge.c: 71
	 * /Users/rubber/linux/block/blk-merge.c: 72
	 * - if 'pb' ends aligned, the merged segment ends aligned /Users/rubber/linux/block/blk-merge.c: 73
	 * - if 'pb' ends unaligned, the next bio must include /Users/rubber/linux/block/blk-merge.c: 74
	 *   one single bvec of 'nb', otherwise the 'nb' can't /Users/rubber/linux/block/blk-merge.c: 75
	 *   merge with 'pb' /Users/rubber/linux/block/blk-merge.c: 76
	/* /Users/rubber/linux/block/blk-merge.c: 124
	 * If the next starting sector would be misaligned, stop the discard at /Users/rubber/linux/block/blk-merge.c: 125
	 * the previous aligned sector. /Users/rubber/linux/block/blk-merge.c: 126
 * Return the maximum number of sectors from the start of a bio that may be /Users/rubber/linux/block/blk-merge.c: 170
 * submitted as a single request to a block device. If enough sectors remain, /Users/rubber/linux/block/blk-merge.c: 171
 * align the end to the physical block size. Otherwise align the end to the /Users/rubber/linux/block/blk-merge.c: 172
 * logical block size. This approach minimizes the number of non-aligned /Users/rubber/linux/block/blk-merge.c: 173
 * requests that are submitted to a block device if the start of a bio is not /Users/rubber/linux/block/blk-merge.c: 174
 * aligned to a physical block boundary. /Users/rubber/linux/block/blk-merge.c: 175
	/* /Users/rubber/linux/block/blk-merge.c: 202
	 * overflow may be triggered in case of zero page physical address /Users/rubber/linux/block/blk-merge.c: 203
	 * on 32bit arch, use queue's max segment size when that happens. /Users/rubber/linux/block/blk-merge.c: 204
 * bvec_split_segs - verify whether or not a bvec should be split in the middle /Users/rubber/linux/block/blk-merge.c: 211
 * @q:        [in] request queue associated with the bio associated with @bv /Users/rubber/linux/block/blk-merge.c: 212
 * @bv:       [in] bvec to examine /Users/rubber/linux/block/blk-merge.c: 213
 * @nsegs:    [in,out] Number of segments in the bio being built. Incremented /Users/rubber/linux/block/blk-merge.c: 214
 *            by the number of segments from @bv that may be appended to that /Users/rubber/linux/block/blk-merge.c: 215
 *            bio without exceeding @max_segs /Users/rubber/linux/block/blk-merge.c: 216
 * @sectors:  [in,out] Number of sectors in the bio being built. Incremented /Users/rubber/linux/block/blk-merge.c: 217
 *            by the number of sectors from @bv that may be appended to that /Users/rubber/linux/block/blk-merge.c: 218
 *            bio without exceeding @max_sectors /Users/rubber/linux/block/blk-merge.c: 219
 * @max_segs: [in] upper bound for *@nsegs /Users/rubber/linux/block/blk-merge.c: 220
 * @max_sectors: [in] upper bound for *@sectors /Users/rubber/linux/block/blk-merge.c: 221
 * When splitting a bio, it can happen that a bvec is encountered that is too /Users/rubber/linux/block/blk-merge.c: 223
 * big to fit in a single segment and hence that it has to be split in the /Users/rubber/linux/block/blk-merge.c: 224
 * middle. This function verifies whether or not that should happen. The value /Users/rubber/linux/block/blk-merge.c: 225
 * %true is returned if and only if appending the entire @bv to a bio with /Users/rubber/linux/block/blk-merge.c: 226
 * *@nsegs segments and *@sectors sectors would make that bio unacceptable for /Users/rubber/linux/block/blk-merge.c: 227
 * the block driver. /Users/rubber/linux/block/blk-merge.c: 228
 * blk_bio_segment_split - split a bio in two bios /Users/rubber/linux/block/blk-merge.c: 260
 * @q:    [in] request queue pointer /Users/rubber/linux/block/blk-merge.c: 261
 * @bio:  [in] bio to be split /Users/rubber/linux/block/blk-merge.c: 262
 * @bs:	  [in] bio set to allocate the clone from /Users/rubber/linux/block/blk-merge.c: 263
 * @segs: [out] number of segments in the bio with the first half of the sectors /Users/rubber/linux/block/blk-merge.c: 264
 * Clone @bio, update the bi_iter of the clone to represent the first sectors /Users/rubber/linux/block/blk-merge.c: 266
 * of @bio and update @bio->bi_iter to represent the remaining sectors. The /Users/rubber/linux/block/blk-merge.c: 267
 * following is guaranteed for the cloned bio: /Users/rubber/linux/block/blk-merge.c: 268
 * - That it has at most get_max_io_size(@q, @bio) sectors. /Users/rubber/linux/block/blk-merge.c: 269
 * - That it has at most queue_max_segments(@q) segments. /Users/rubber/linux/block/blk-merge.c: 270
 * Except for discard requests the cloned bio will point at the bi_io_vec of /Users/rubber/linux/block/blk-merge.c: 272
 * the original bio. It is the responsibility of the caller to ensure that the /Users/rubber/linux/block/blk-merge.c: 273
 * original bio is not freed before the cloned bio. The caller is also /Users/rubber/linux/block/blk-merge.c: 274
 * responsible for ensuring that @bs is only destroyed after processing of the /Users/rubber/linux/block/blk-merge.c: 275
 * split bio has finished. /Users/rubber/linux/block/blk-merge.c: 276
		/* /Users/rubber/linux/block/blk-merge.c: 290
		 * If the queue doesn't support SG gaps and adding this /Users/rubber/linux/block/blk-merge.c: 291
		 * offset would create a gap, disallow it. /Users/rubber/linux/block/blk-merge.c: 292
	/* /Users/rubber/linux/block/blk-merge.c: 316
	 * Bio splitting may cause subtle trouble such as hang when doing sync /Users/rubber/linux/block/blk-merge.c: 317
	 * iopoll in direct IO routine. Given performance gain of iopoll for /Users/rubber/linux/block/blk-merge.c: 318
	 * big IO can be trival, disable iopoll when split needed. /Users/rubber/linux/block/blk-merge.c: 319
 * __blk_queue_split - split a bio and submit the second half /Users/rubber/linux/block/blk-merge.c: 326
 * @q:       [in] request_queue new bio is being queued at /Users/rubber/linux/block/blk-merge.c: 327
 * @bio:     [in, out] bio to be split /Users/rubber/linux/block/blk-merge.c: 328
 * @nr_segs: [out] number of segments in the first bio /Users/rubber/linux/block/blk-merge.c: 329
 * Split a bio into two bios, chain the two bios, submit the second half and /Users/rubber/linux/block/blk-merge.c: 331
 * store a pointer to the first half in *@bio. If the second bio is still too /Users/rubber/linux/block/blk-merge.c: 332
 * big it will be split by a recursive call to this function. Since this /Users/rubber/linux/block/blk-merge.c: 333
 * function may allocate a new bio from q->bio_split, it is the responsibility /Users/rubber/linux/block/blk-merge.c: 334
 * of the caller to ensure that q->bio_split is only released after processing /Users/rubber/linux/block/blk-merge.c: 335
 * of the split bio has finished. /Users/rubber/linux/block/blk-merge.c: 336
 * blk_queue_split - split a bio and submit the second half /Users/rubber/linux/block/blk-merge.c: 375
 * @bio: [in, out] bio to be split /Users/rubber/linux/block/blk-merge.c: 376
 * Split a bio into two bios, chains the two bios, submit the second half and /Users/rubber/linux/block/blk-merge.c: 378
 * store a pointer to the first half in *@bio. Since this function may allocate /Users/rubber/linux/block/blk-merge.c: 379
 * a new bio from q->bio_split, it is the responsibility of the caller to ensure /Users/rubber/linux/block/blk-merge.c: 380
 * that q->bio_split is only released after processing of the split bio has /Users/rubber/linux/block/blk-merge.c: 381
 * finished. /Users/rubber/linux/block/blk-merge.c: 382
	/* /Users/rubber/linux/block/blk-merge.c: 433
	 * If the driver previously mapped a shorter list, we could see a /Users/rubber/linux/block/blk-merge.c: 434
	 * termination bit prematurely unless it fully inits the sg table /Users/rubber/linux/block/blk-merge.c: 435
	 * on each mapping. We KNOW that there must be more entries here /Users/rubber/linux/block/blk-merge.c: 436
	 * or the driver would be buggy, so force clear the termination bit /Users/rubber/linux/block/blk-merge.c: 437
	 * to avoid doing a full sg_init_table() in drivers for each command. /Users/rubber/linux/block/blk-merge.c: 438
		/* /Users/rubber/linux/block/blk-merge.c: 457
		 * Unfortunately a fair number of drivers barf on scatterlists /Users/rubber/linux/block/blk-merge.c: 458
		 * that have an offset larger than PAGE_SIZE, despite other /Users/rubber/linux/block/blk-merge.c: 459
		 * subsystems dealing with that invariant just fine.  For now /Users/rubber/linux/block/blk-merge.c: 460
		 * stick to the legacy format where we never present those from /Users/rubber/linux/block/blk-merge.c: 461
		 * the block layer, but the code below should be removed once /Users/rubber/linux/block/blk-merge.c: 462
		 * these offenders (mostly MMC/SD drivers) are fixed. /Users/rubber/linux/block/blk-merge.c: 463
			/* /Users/rubber/linux/block/blk-merge.c: 520
			 * Only try to merge bvecs from two bios given we /Users/rubber/linux/block/blk-merge.c: 521
			 * have done bio internal merge when adding pages /Users/rubber/linux/block/blk-merge.c: 522
			 * to bio /Users/rubber/linux/block/blk-merge.c: 523
 * map a request to scatterlist, return number of sg entries setup. Caller /Users/rubber/linux/block/blk-merge.c: 546
 * must make sure sg can hold rq->nr_phys_segments entries /Users/rubber/linux/block/blk-merge.c: 547
	/* /Users/rubber/linux/block/blk-merge.c: 564
	 * Something must have been wrong if the figured number of /Users/rubber/linux/block/blk-merge.c: 565
	 * segment is bigger than number of req's physical segments /Users/rubber/linux/block/blk-merge.c: 566
	/* /Users/rubber/linux/block/blk-merge.c: 611
	 * This will form the start of a new hw segment.  Bump both /Users/rubber/linux/block/blk-merge.c: 612
	 * counters. /Users/rubber/linux/block/blk-merge.c: 613
	/* /Users/rubber/linux/block/blk-merge.c: 686
	 * Will it become too large? /Users/rubber/linux/block/blk-merge.c: 687
 * blk_rq_set_mixed_merge - mark a request as mixed merge /Users/rubber/linux/block/blk-merge.c: 709
 * @rq: request to mark as mixed merge /Users/rubber/linux/block/blk-merge.c: 710
 * Description: /Users/rubber/linux/block/blk-merge.c: 712
 *     @rq is about to be mixed merged.  Make sure the attributes /Users/rubber/linux/block/blk-merge.c: 713
 *     which can be mixed are set in each bio and mark @rq as mixed /Users/rubber/linux/block/blk-merge.c: 714
 *     merged. /Users/rubber/linux/block/blk-merge.c: 715
	/* /Users/rubber/linux/block/blk-merge.c: 725
	 * @rq will no longer represent mixable attributes for all the /Users/rubber/linux/block/blk-merge.c: 726
	 * contained bios.  It will just track those of the first one. /Users/rubber/linux/block/blk-merge.c: 727
	 * Distributes the attributs to each bio. /Users/rubber/linux/block/blk-merge.c: 728
 * For non-mq, this has to be called with the request spinlock acquired. /Users/rubber/linux/block/blk-merge.c: 766
 * For mq with scheduling, the appropriate queue wide lock should be held. /Users/rubber/linux/block/blk-merge.c: 767
	/* /Users/rubber/linux/block/blk-merge.c: 786
	 * Don't allow merge of different write hints, or for a hint with /Users/rubber/linux/block/blk-merge.c: 787
	 * non-hint IO. /Users/rubber/linux/block/blk-merge.c: 788
	/* /Users/rubber/linux/block/blk-merge.c: 796
	 * If we are allowed to merge, then append bio list /Users/rubber/linux/block/blk-merge.c: 797
	 * from next to rq and release next. merge_requests_fn /Users/rubber/linux/block/blk-merge.c: 798
	 * will have updated segment counts, update sector /Users/rubber/linux/block/blk-merge.c: 799
	 * counts here. Handle DISCARDs separately, as they /Users/rubber/linux/block/blk-merge.c: 800
	 * have separate settings. /Users/rubber/linux/block/blk-merge.c: 801
	/* /Users/rubber/linux/block/blk-merge.c: 817
	 * If failfast settings disagree or any of the two is already /Users/rubber/linux/block/blk-merge.c: 818
	 * a mixed merge, mark both as mixed before proceeding.  This /Users/rubber/linux/block/blk-merge.c: 819
	 * makes sure that all involved bios have mixable attributes /Users/rubber/linux/block/blk-merge.c: 820
	 * set properly. /Users/rubber/linux/block/blk-merge.c: 821
	/* /Users/rubber/linux/block/blk-merge.c: 830
	 * At this point we have either done a back merge or front merge. We /Users/rubber/linux/block/blk-merge.c: 831
	 * need the smaller start_time_ns of the merged requests to be the /Users/rubber/linux/block/blk-merge.c: 832
	 * current request for accounting purposes. /Users/rubber/linux/block/blk-merge.c: 833
	/* /Users/rubber/linux/block/blk-merge.c: 846
	 * 'next' is going away, so update stats accordingly /Users/rubber/linux/block/blk-merge.c: 847
	/* /Users/rubber/linux/block/blk-merge.c: 853
	 * ownership of bio passed from next to req, return 'next' for /Users/rubber/linux/block/blk-merge.c: 854
	 * the caller to free /Users/rubber/linux/block/blk-merge.c: 855
 * Try to merge 'next' into 'rq'. Return true if the merge happened, false /Users/rubber/linux/block/blk-merge.c: 884
 * otherwise. The caller is responsible for freeing 'next' if the merge /Users/rubber/linux/block/blk-merge.c: 885
 * happened. /Users/rubber/linux/block/blk-merge.c: 886
	/* /Users/rubber/linux/block/blk-merge.c: 923
	 * Don't allow merge of different write hints, or for a hint with /Users/rubber/linux/block/blk-merge.c: 924
	 * non-hint IO. /Users/rubber/linux/block/blk-merge.c: 925
 * blk_attempt_plug_merge - try to merge with %current's plugged list /Users/rubber/linux/block/blk-merge.c: 1066
 * @q: request_queue new bio is being queued at /Users/rubber/linux/block/blk-merge.c: 1067
 * @bio: new bio being queued /Users/rubber/linux/block/blk-merge.c: 1068
 * @nr_segs: number of segments in @bio /Users/rubber/linux/block/blk-merge.c: 1069
 * @same_queue_rq: output value, will be true if there's an existing request /Users/rubber/linux/block/blk-merge.c: 1070
 * from the passed in @q already in the plug list /Users/rubber/linux/block/blk-merge.c: 1071
 * Determine whether @bio being queued on @q can be merged with the previous /Users/rubber/linux/block/blk-merge.c: 1073
 * request on %current's plugged list.  Returns %true if merge was successful, /Users/rubber/linux/block/blk-merge.c: 1074
 * otherwise %false. /Users/rubber/linux/block/blk-merge.c: 1075
 * Plugging coalesces IOs from the same issuer for the same purpose without /Users/rubber/linux/block/blk-merge.c: 1077
 * going through @q->queue_lock.  As such it's more of an issuing mechanism /Users/rubber/linux/block/blk-merge.c: 1078
 * than scheduling, and the request, while may have elvpriv data, is not /Users/rubber/linux/block/blk-merge.c: 1079
 * added on the elevator at this point.  In addition, we don't have /Users/rubber/linux/block/blk-merge.c: 1080
 * reliable access to the elevator outside queue lock.  Only check basic /Users/rubber/linux/block/blk-merge.c: 1081
 * merging parameters without querying the elevator. /Users/rubber/linux/block/blk-merge.c: 1082
 * Caller must ensure !blk_queue_nomerges(q) beforehand. /Users/rubber/linux/block/blk-merge.c: 1084
		/* /Users/rubber/linux/block/blk-merge.c: 1099
		 * Only blk-mq multiple hardware queues case checks the rq in /Users/rubber/linux/block/blk-merge.c: 1100
		 * the same queue, there should be only one such rq in a queue /Users/rubber/linux/block/blk-merge.c: 1101
 * Iterate list of requests and see if we can merge this bio with any /Users/rubber/linux/block/blk-merge.c: 1113
 * of them. /Users/rubber/linux/block/blk-merge.c: 1114
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-ioprio.c: 1
 * Block rq-qos policy for assigning an I/O priority class to requests. /Users/rubber/linux/block/blk-ioprio.c: 3
 * Using an rq-qos policy for assigning I/O priority class has two advantages /Users/rubber/linux/block/blk-ioprio.c: 5
 * over using the ioprio_set() system call: /Users/rubber/linux/block/blk-ioprio.c: 6
 * - This policy is cgroup based so it has all the advantages of cgroups. /Users/rubber/linux/block/blk-ioprio.c: 8
 * - While ioprio_set() does not affect page cache writeback I/O, this rq-qos /Users/rubber/linux/block/blk-ioprio.c: 9
 *   controller affects page cache writeback I/O for filesystems that support /Users/rubber/linux/block/blk-ioprio.c: 10
 *   assiociating a cgroup with writeback I/O. See also /Users/rubber/linux/block/blk-ioprio.c: 11
 *   Documentation/admin-guide/cgroup-v2.rst. /Users/rubber/linux/block/blk-ioprio.c: 12
 * enum prio_policy - I/O priority class policy. /Users/rubber/linux/block/blk-ioprio.c: 24
 * @POLICY_NO_CHANGE: (default) do not modify the I/O priority class. /Users/rubber/linux/block/blk-ioprio.c: 25
 * @POLICY_NONE_TO_RT: modify IOPRIO_CLASS_NONE into IOPRIO_CLASS_RT. /Users/rubber/linux/block/blk-ioprio.c: 26
 * @POLICY_RESTRICT_TO_BE: modify IOPRIO_CLASS_NONE and IOPRIO_CLASS_RT into /Users/rubber/linux/block/blk-ioprio.c: 27
 *		IOPRIO_CLASS_BE. /Users/rubber/linux/block/blk-ioprio.c: 28
 * @POLICY_ALL_TO_IDLE: change the I/O priority class into IOPRIO_CLASS_IDLE. /Users/rubber/linux/block/blk-ioprio.c: 29
 * See also <linux/ioprio.h>. /Users/rubber/linux/block/blk-ioprio.c: 31
 * struct ioprio_blkg - Per (cgroup, request queue) data. /Users/rubber/linux/block/blk-ioprio.c: 50
 * @pd: blkg_policy_data structure. /Users/rubber/linux/block/blk-ioprio.c: 51
 * struct ioprio_blkcg - Per cgroup data. /Users/rubber/linux/block/blk-ioprio.c: 58
 * @cpd: blkcg_policy_data structure. /Users/rubber/linux/block/blk-ioprio.c: 59
 * @prio_policy: One of the IOPRIO_CLASS_* values. See also <linux/ioprio.h>. /Users/rubber/linux/block/blk-ioprio.c: 60
	/* /Users/rubber/linux/block/blk-ioprio.c: 194
	 * Except for IOPRIO_CLASS_NONE, higher I/O priority numbers /Users/rubber/linux/block/blk-ioprio.c: 195
	 * correspond to a lower priority. Hence, the max_t() below selects /Users/rubber/linux/block/blk-ioprio.c: 196
	 * the lower priority of bi_ioprio and the cgroup I/O priority class. /Users/rubber/linux/block/blk-ioprio.c: 197
	 * If the cgroup policy has been set to POLICY_NO_CHANGE == 0, the /Users/rubber/linux/block/blk-ioprio.c: 198
	 * bio I/O priority is not modified. If the bio I/O priority equals /Users/rubber/linux/block/blk-ioprio.c: 199
	 * IOPRIO_CLASS_NONE, the cgroup I/O priority is assigned to the bio. /Users/rubber/linux/block/blk-ioprio.c: 200
	/* /Users/rubber/linux/block/blk-ioprio.c: 241
	 * Registering the rq-qos policy after activating the blk-cgroup /Users/rubber/linux/block/blk-ioprio.c: 242
	 * policy guarantees that ioprio_blkcg_from_bio(bio) != NULL in the /Users/rubber/linux/block/blk-ioprio.c: 243
	 * rq-qos callbacks. /Users/rubber/linux/block/blk-ioprio.c: 244
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-crypto-profile.c: 1
 * Copyright 2019 Google LLC /Users/rubber/linux/block/blk-crypto-profile.c: 3
 * DOC: blk-crypto profiles /Users/rubber/linux/block/blk-crypto-profile.c: 7
 * 'struct blk_crypto_profile' contains all generic inline encryption-related /Users/rubber/linux/block/blk-crypto-profile.c: 9
 * state for a particular inline encryption device.  blk_crypto_profile serves /Users/rubber/linux/block/blk-crypto-profile.c: 10
 * as the way that drivers for inline encryption hardware expose their crypto /Users/rubber/linux/block/blk-crypto-profile.c: 11
 * capabilities and certain functions (e.g., functions to program and evict /Users/rubber/linux/block/blk-crypto-profile.c: 12
 * keys) to upper layers.  Device drivers that want to support inline encryption /Users/rubber/linux/block/blk-crypto-profile.c: 13
 * construct a crypto profile, then associate it with the disk's request_queue. /Users/rubber/linux/block/blk-crypto-profile.c: 14
 * If the device has keyslots, then its blk_crypto_profile also handles managing /Users/rubber/linux/block/blk-crypto-profile.c: 16
 * these keyslots in a device-independent way, using the driver-provided /Users/rubber/linux/block/blk-crypto-profile.c: 17
 * functions to program and evict keys as needed.  This includes keeping track /Users/rubber/linux/block/blk-crypto-profile.c: 18
 * of which key and how many I/O requests are using each keyslot, getting /Users/rubber/linux/block/blk-crypto-profile.c: 19
 * keyslots for I/O requests, and handling key eviction requests. /Users/rubber/linux/block/blk-crypto-profile.c: 20
 * For more information, see Documentation/block/inline-encryption.rst. /Users/rubber/linux/block/blk-crypto-profile.c: 22
	/* /Users/rubber/linux/block/blk-crypto-profile.c: 46
	 * Calling into the driver requires profile->lock held and the device /Users/rubber/linux/block/blk-crypto-profile.c: 47
	 * resumed.  But we must resume the device first, since that can acquire /Users/rubber/linux/block/blk-crypto-profile.c: 48
	 * and release profile->lock via blk_crypto_reprogram_all_keys(). /Users/rubber/linux/block/blk-crypto-profile.c: 49
 * blk_crypto_profile_init() - Initialize a blk_crypto_profile /Users/rubber/linux/block/blk-crypto-profile.c: 64
 * @profile: the blk_crypto_profile to initialize /Users/rubber/linux/block/blk-crypto-profile.c: 65
 * @num_slots: the number of keyslots /Users/rubber/linux/block/blk-crypto-profile.c: 66
 * Storage drivers must call this when starting to set up a blk_crypto_profile, /Users/rubber/linux/block/blk-crypto-profile.c: 68
 * before filling in additional fields. /Users/rubber/linux/block/blk-crypto-profile.c: 69
 * Return: 0 on success, or else a negative error code. /Users/rubber/linux/block/blk-crypto-profile.c: 71
	/* /Users/rubber/linux/block/blk-crypto-profile.c: 107
	 * hash_ptr() assumes bits != 0, so ensure the hash table has at least 2 /Users/rubber/linux/block/blk-crypto-profile.c: 108
	 * buckets.  This only makes a difference when there is only 1 keyslot. /Users/rubber/linux/block/blk-crypto-profile.c: 109
 * devm_blk_crypto_profile_init() - Resource-managed blk_crypto_profile_init() /Users/rubber/linux/block/blk-crypto-profile.c: 137
 * @dev: the device which owns the blk_crypto_profile /Users/rubber/linux/block/blk-crypto-profile.c: 138
 * @profile: the blk_crypto_profile to initialize /Users/rubber/linux/block/blk-crypto-profile.c: 139
 * @num_slots: the number of keyslots /Users/rubber/linux/block/blk-crypto-profile.c: 140
 * Like blk_crypto_profile_init(), but causes blk_crypto_profile_destroy() to be /Users/rubber/linux/block/blk-crypto-profile.c: 142
 * called automatically on driver detach. /Users/rubber/linux/block/blk-crypto-profile.c: 143
 * Return: 0 on success, or else a negative error code. /Users/rubber/linux/block/blk-crypto-profile.c: 145
 * blk_crypto_keyslot_index() - Get the index of a keyslot /Users/rubber/linux/block/blk-crypto-profile.c: 213
 * @slot: a keyslot that blk_crypto_get_keyslot() returned /Users/rubber/linux/block/blk-crypto-profile.c: 214
 * Return: the 0-based index of the keyslot within the device's keyslots. /Users/rubber/linux/block/blk-crypto-profile.c: 216
 * blk_crypto_get_keyslot() - Get a keyslot for a key, if needed. /Users/rubber/linux/block/blk-crypto-profile.c: 225
 * @profile: the crypto profile of the device the key will be used on /Users/rubber/linux/block/blk-crypto-profile.c: 226
 * @key: the key that will be used /Users/rubber/linux/block/blk-crypto-profile.c: 227
 * @slot_ptr: If a keyslot is allocated, an opaque pointer to the keyslot struct /Users/rubber/linux/block/blk-crypto-profile.c: 228
 *	      will be stored here; otherwise NULL will be stored here. /Users/rubber/linux/block/blk-crypto-profile.c: 229
 * If the device has keyslots, this gets a keyslot that's been programmed with /Users/rubber/linux/block/blk-crypto-profile.c: 231
 * the specified key.  If the key is already in a slot, this reuses it; /Users/rubber/linux/block/blk-crypto-profile.c: 232
 * otherwise this waits for a slot to become idle and programs the key into it. /Users/rubber/linux/block/blk-crypto-profile.c: 233
 * This must be paired with a call to blk_crypto_put_keyslot(). /Users/rubber/linux/block/blk-crypto-profile.c: 235
 * Context: Process context. Takes and releases profile->lock. /Users/rubber/linux/block/blk-crypto-profile.c: 237
 * Return: BLK_STS_OK on success, meaning that either a keyslot was allocated or /Users/rubber/linux/block/blk-crypto-profile.c: 238
 *	   one wasn't needed; or a blk_status_t error on failure. /Users/rubber/linux/block/blk-crypto-profile.c: 239
	/* /Users/rubber/linux/block/blk-crypto-profile.c: 251
	 * If the device has no concept of "keyslots", then there is no need to /Users/rubber/linux/block/blk-crypto-profile.c: 252
	 * get one. /Users/rubber/linux/block/blk-crypto-profile.c: 253
		/* /Users/rubber/linux/block/blk-crypto-profile.c: 272
		 * If we're here, that means there wasn't a slot that was /Users/rubber/linux/block/blk-crypto-profile.c: 273
		 * already programmed with the key. So try to program it. /Users/rubber/linux/block/blk-crypto-profile.c: 274
 * blk_crypto_put_keyslot() - Release a reference to a keyslot /Users/rubber/linux/block/blk-crypto-profile.c: 313
 * @slot: The keyslot to release the reference of (may be NULL). /Users/rubber/linux/block/blk-crypto-profile.c: 314
 * Context: Any context. /Users/rubber/linux/block/blk-crypto-profile.c: 316
 * __blk_crypto_cfg_supported() - Check whether the given crypto profile /Users/rubber/linux/block/blk-crypto-profile.c: 337
 *				  supports the given crypto configuration. /Users/rubber/linux/block/blk-crypto-profile.c: 338
 * @profile: the crypto profile to check /Users/rubber/linux/block/blk-crypto-profile.c: 339
 * @cfg: the crypto configuration to check for /Users/rubber/linux/block/blk-crypto-profile.c: 340
 * Return: %true if @profile supports the given @cfg. /Users/rubber/linux/block/blk-crypto-profile.c: 342
 * __blk_crypto_evict_key() - Evict a key from a device. /Users/rubber/linux/block/blk-crypto-profile.c: 357
 * @profile: the crypto profile of the device /Users/rubber/linux/block/blk-crypto-profile.c: 358
 * @key: the key to evict.  It must not still be used in any I/O. /Users/rubber/linux/block/blk-crypto-profile.c: 359
 * If the device has keyslots, this finds the keyslot (if any) that contains the /Users/rubber/linux/block/blk-crypto-profile.c: 361
 * specified key and calls the driver's keyslot_evict function to evict it. /Users/rubber/linux/block/blk-crypto-profile.c: 362
 * Otherwise, this just calls the driver's keyslot_evict function if it is /Users/rubber/linux/block/blk-crypto-profile.c: 364
 * implemented, passing just the key (without any particular keyslot).  This /Users/rubber/linux/block/blk-crypto-profile.c: 365
 * allows layered devices to evict the key from their underlying devices. /Users/rubber/linux/block/blk-crypto-profile.c: 366
 * Context: Process context. Takes and releases profile->lock. /Users/rubber/linux/block/blk-crypto-profile.c: 368
 * Return: 0 on success or if there's no keyslot with the specified key, -EBUSY /Users/rubber/linux/block/blk-crypto-profile.c: 369
 *	   if the keyslot is still in use, or another -errno value on other /Users/rubber/linux/block/blk-crypto-profile.c: 370
 *	   error. /Users/rubber/linux/block/blk-crypto-profile.c: 371
 * blk_crypto_reprogram_all_keys() - Re-program all keyslots. /Users/rubber/linux/block/blk-crypto-profile.c: 412
 * @profile: The crypto profile /Users/rubber/linux/block/blk-crypto-profile.c: 413
 * Re-program all keyslots that are supposed to have a key programmed.  This is /Users/rubber/linux/block/blk-crypto-profile.c: 415
 * intended only for use by drivers for hardware that loses its keys on reset. /Users/rubber/linux/block/blk-crypto-profile.c: 416
 * Context: Process context. Takes and releases profile->lock. /Users/rubber/linux/block/blk-crypto-profile.c: 418
 * blk_crypto_intersect_capabilities() - restrict supported crypto capabilities /Users/rubber/linux/block/blk-crypto-profile.c: 472
 *					 by child device /Users/rubber/linux/block/blk-crypto-profile.c: 473
 * @parent: the crypto profile for the parent device /Users/rubber/linux/block/blk-crypto-profile.c: 474
 * @child: the crypto profile for the child device, or NULL /Users/rubber/linux/block/blk-crypto-profile.c: 475
 * This clears all crypto capabilities in @parent that aren't set in @child.  If /Users/rubber/linux/block/blk-crypto-profile.c: 477
 * @child is NULL, then this clears all parent capabilities. /Users/rubber/linux/block/blk-crypto-profile.c: 478
 * Only use this when setting up the crypto profile for a layered device, before /Users/rubber/linux/block/blk-crypto-profile.c: 480
 * it's been exposed yet. /Users/rubber/linux/block/blk-crypto-profile.c: 481
 * blk_crypto_has_capabilities() - Check whether @target supports at least all /Users/rubber/linux/block/blk-crypto-profile.c: 503
 *				   the crypto capabilities that @reference does. /Users/rubber/linux/block/blk-crypto-profile.c: 504
 * @target: the target profile /Users/rubber/linux/block/blk-crypto-profile.c: 505
 * @reference: the reference profile /Users/rubber/linux/block/blk-crypto-profile.c: 506
 * Return: %true if @target supports all the crypto capabilities of @reference. /Users/rubber/linux/block/blk-crypto-profile.c: 508
 * blk_crypto_update_capabilities() - Update the capabilities of a crypto /Users/rubber/linux/block/blk-crypto-profile.c: 535
 *				      profile to match those of another crypto /Users/rubber/linux/block/blk-crypto-profile.c: 536
 *				      profile. /Users/rubber/linux/block/blk-crypto-profile.c: 537
 * @dst: The crypto profile whose capabilities to update. /Users/rubber/linux/block/blk-crypto-profile.c: 538
 * @src: The crypto profile whose capabilities this function will update @dst's /Users/rubber/linux/block/blk-crypto-profile.c: 539
 *	 capabilities to. /Users/rubber/linux/block/blk-crypto-profile.c: 540
 * Blk-crypto requires that crypto capabilities that were /Users/rubber/linux/block/blk-crypto-profile.c: 542
 * advertised when a bio was created continue to be supported by the /Users/rubber/linux/block/blk-crypto-profile.c: 543
 * device until that bio is ended. This is turn means that a device cannot /Users/rubber/linux/block/blk-crypto-profile.c: 544
 * shrink its advertised crypto capabilities without any explicit /Users/rubber/linux/block/blk-crypto-profile.c: 545
 * synchronization with upper layers. So if there's no such explicit /Users/rubber/linux/block/blk-crypto-profile.c: 546
 * synchronization, @src must support all the crypto capabilities that /Users/rubber/linux/block/blk-crypto-profile.c: 547
 * @dst does (i.e. we need blk_crypto_has_capabilities(@src, @dst)). /Users/rubber/linux/block/blk-crypto-profile.c: 548
 * Note also that as long as the crypto capabilities are being expanded, the /Users/rubber/linux/block/blk-crypto-profile.c: 550
 * order of updates becoming visible is not important because it's alright /Users/rubber/linux/block/blk-crypto-profile.c: 551
 * for blk-crypto to see stale values - they only cause blk-crypto to /Users/rubber/linux/block/blk-crypto-profile.c: 552
 * believe that a crypto capability isn't supported when it actually is (which /Users/rubber/linux/block/blk-crypto-profile.c: 553
 * might result in blk-crypto-fallback being used if available, or the bio being /Users/rubber/linux/block/blk-crypto-profile.c: 554
 * failed). /Users/rubber/linux/block/blk-crypto-profile.c: 555
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-map.c: 1
 * Functions related to mapping data to requests /Users/rubber/linux/block/blk-map.c: 3
 * bio_copy_from_iter - copy all pages from iov_iter to bio /Users/rubber/linux/block/blk-map.c: 39
 * @bio: The &struct bio which describes the I/O as destination /Users/rubber/linux/block/blk-map.c: 40
 * @iter: iov_iter as source /Users/rubber/linux/block/blk-map.c: 41
 * Copy all pages from iov_iter to bio. /Users/rubber/linux/block/blk-map.c: 43
 * Returns 0 on success, or error on failure. /Users/rubber/linux/block/blk-map.c: 44
 * bio_copy_to_iter - copy all pages from bio to iov_iter /Users/rubber/linux/block/blk-map.c: 70
 * @bio: The &struct bio which describes the I/O as source /Users/rubber/linux/block/blk-map.c: 71
 * @iter: iov_iter as destination /Users/rubber/linux/block/blk-map.c: 72
 * Copy all pages from bio to iov_iter. /Users/rubber/linux/block/blk-map.c: 74
 * Returns 0 on success, or error on failure. /Users/rubber/linux/block/blk-map.c: 75
 *	bio_uncopy_user	-	finish previously mapped bio /Users/rubber/linux/block/blk-map.c: 101
 *	@bio: bio being terminated /Users/rubber/linux/block/blk-map.c: 102
 *	Free pages allocated from bio_copy_user_iov() and write back data /Users/rubber/linux/block/blk-map.c: 104
 *	to user space in case of a read. /Users/rubber/linux/block/blk-map.c: 105
		/* /Users/rubber/linux/block/blk-map.c: 113
		 * if we're in a workqueue, the request is orphaned, so /Users/rubber/linux/block/blk-map.c: 114
		 * don't copy into a random user address space, just free /Users/rubber/linux/block/blk-map.c: 115
		 * and return -EINTR so user space doesn't expect any data. /Users/rubber/linux/block/blk-map.c: 116
	/* /Users/rubber/linux/block/blk-map.c: 144
	 * We need to do a deep copy of the iov_iter including the iovecs. /Users/rubber/linux/block/blk-map.c: 145
	 * The caller provided iov might point to an on-stack or otherwise /Users/rubber/linux/block/blk-map.c: 146
	 * shortlived one. /Users/rubber/linux/block/blk-map.c: 147
	/* /Users/rubber/linux/block/blk-map.c: 203
	 * success /Users/rubber/linux/block/blk-map.c: 204
		/* /Users/rubber/linux/block/blk-map.c: 288
		 * release the pages we didn't map into the bio, if any /Users/rubber/linux/block/blk-map.c: 289
 *	bio_map_kern	-	map kernel address into bio /Users/rubber/linux/block/blk-map.c: 330
 *	@q: the struct request_queue for the bio /Users/rubber/linux/block/blk-map.c: 331
 *	@data: pointer to buffer to map /Users/rubber/linux/block/blk-map.c: 332
 *	@len: length in bytes /Users/rubber/linux/block/blk-map.c: 333
 *	@gfp_mask: allocation flags for bio allocation /Users/rubber/linux/block/blk-map.c: 334
 *	Map the kernel address into a bio suitable for io to a block /Users/rubber/linux/block/blk-map.c: 336
 *	device. Returns an error pointer in case of error. /Users/rubber/linux/block/blk-map.c: 337
 *	bio_copy_kern	-	copy kernel address into bio /Users/rubber/linux/block/blk-map.c: 411
 *	@q: the struct request_queue for the bio /Users/rubber/linux/block/blk-map.c: 412
 *	@data: pointer to buffer to copy /Users/rubber/linux/block/blk-map.c: 413
 *	@len: length in bytes /Users/rubber/linux/block/blk-map.c: 414
 *	@gfp_mask: allocation flags for bio and page allocation /Users/rubber/linux/block/blk-map.c: 415
 *	@reading: data direction is READ /Users/rubber/linux/block/blk-map.c: 416
 *	copy the kernel address into a bio suitable for io to a block /Users/rubber/linux/block/blk-map.c: 418
 *	device. Returns an error pointer in case of error. /Users/rubber/linux/block/blk-map.c: 419
	/* /Users/rubber/linux/block/blk-map.c: 431
	 * Overflow, abort /Users/rubber/linux/block/blk-map.c: 432
 * Append a bio to a passthrough request.  Only works if the bio can be merged /Users/rubber/linux/block/blk-map.c: 479
 * into the request based on the driver constraints. /Users/rubber/linux/block/blk-map.c: 480
 * blk_rq_map_user_iov - map user data to a request, for passthrough requests /Users/rubber/linux/block/blk-map.c: 507
 * @q:		request queue where request should be inserted /Users/rubber/linux/block/blk-map.c: 508
 * @rq:		request to map data to /Users/rubber/linux/block/blk-map.c: 509
 * @map_data:   pointer to the rq_map_data holding pages (if necessary) /Users/rubber/linux/block/blk-map.c: 510
 * @iter:	iovec iterator /Users/rubber/linux/block/blk-map.c: 511
 * @gfp_mask:	memory allocation flags /Users/rubber/linux/block/blk-map.c: 512
 * Description: /Users/rubber/linux/block/blk-map.c: 514
 *    Data will be mapped directly for zero copy I/O, if possible. Otherwise /Users/rubber/linux/block/blk-map.c: 515
 *    a kernel bounce buffer is used. /Users/rubber/linux/block/blk-map.c: 516
 *    A matching blk_rq_unmap_user() must be issued at the end of I/O, while /Users/rubber/linux/block/blk-map.c: 518
 *    still in process context. /Users/rubber/linux/block/blk-map.c: 519
 * blk_rq_unmap_user - unmap a request with user data /Users/rubber/linux/block/blk-map.c: 581
 * @bio:	       start of bio list /Users/rubber/linux/block/blk-map.c: 582
 * Description: /Users/rubber/linux/block/blk-map.c: 584
 *    Unmap a rq previously mapped by blk_rq_map_user(). The caller must /Users/rubber/linux/block/blk-map.c: 585
 *    supply the original rq->bio from the blk_rq_map_user() return, since /Users/rubber/linux/block/blk-map.c: 586
 *    the I/O completion may have changed rq->bio. /Users/rubber/linux/block/blk-map.c: 587
 * blk_rq_map_kern - map kernel data to a request, for passthrough requests /Users/rubber/linux/block/blk-map.c: 613
 * @q:		request queue where request should be inserted /Users/rubber/linux/block/blk-map.c: 614
 * @rq:		request to fill /Users/rubber/linux/block/blk-map.c: 615
 * @kbuf:	the kernel buffer /Users/rubber/linux/block/blk-map.c: 616
 * @len:	length of user data /Users/rubber/linux/block/blk-map.c: 617
 * @gfp_mask:	memory allocation flags /Users/rubber/linux/block/blk-map.c: 618
 * Description: /Users/rubber/linux/block/blk-map.c: 620
 *    Data will be mapped directly if possible. Otherwise a bounce /Users/rubber/linux/block/blk-map.c: 621
 *    buffer is used. Can be called multiple times to append multiple /Users/rubber/linux/block/blk-map.c: 622
 *    buffers. /Users/rubber/linux/block/blk-map.c: 623
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-virtio.c: 1
 * Copyright (c) 2016 Christoph Hellwig. /Users/rubber/linux/block/blk-mq-virtio.c: 3
 * blk_mq_virtio_map_queues - provide a default queue mapping for virtio device /Users/rubber/linux/block/blk-mq-virtio.c: 13
 * @qmap:	CPU to hardware queue map. /Users/rubber/linux/block/blk-mq-virtio.c: 14
 * @vdev:	virtio device to provide a mapping for. /Users/rubber/linux/block/blk-mq-virtio.c: 15
 * @first_vec:	first interrupt vectors to use for queues (usually 0) /Users/rubber/linux/block/blk-mq-virtio.c: 16
 * This function assumes the virtio device @vdev has at least as many available /Users/rubber/linux/block/blk-mq-virtio.c: 18
 * interrupt vectors as @set has queues.  It will then query the vector /Users/rubber/linux/block/blk-mq-virtio.c: 19
 * corresponding to each queue for it's affinity mask and built queue mapping /Users/rubber/linux/block/blk-mq-virtio.c: 20
 * that maps a queue to the CPUs that have irq affinity for the corresponding /Users/rubber/linux/block/blk-mq-virtio.c: 21
 * vector. /Users/rubber/linux/block/blk-mq-virtio.c: 22
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-wbt.c: 1
 * buffered writeback throttling. loosely based on CoDel. We can't drop /Users/rubber/linux/block/blk-wbt.c: 3
 * packets for IO scheduling, so the logic is something like this: /Users/rubber/linux/block/blk-wbt.c: 4
 * - Monitor latencies in a defined window of time. /Users/rubber/linux/block/blk-wbt.c: 6
 * - If the minimum latency in the above window exceeds some target, increment /Users/rubber/linux/block/blk-wbt.c: 7
 *   scaling step and scale down queue depth by a factor of 2x. The monitoring /Users/rubber/linux/block/blk-wbt.c: 8
 *   window is then shrunk to 100 / sqrt(scaling step + 1). /Users/rubber/linux/block/blk-wbt.c: 9
 * - For any window where we don't have solid data on what the latencies /Users/rubber/linux/block/blk-wbt.c: 10
 *   look like, retain status quo. /Users/rubber/linux/block/blk-wbt.c: 11
 * - If latencies look good, decrement scaling step. /Users/rubber/linux/block/blk-wbt.c: 12
 * - If we're only doing writes, allow the scaling step to go negative. This /Users/rubber/linux/block/blk-wbt.c: 13
 *   will temporarily boost write performance, snapping back to a stable /Users/rubber/linux/block/blk-wbt.c: 14
 *   scaling step of 0 if reads show up or the heavy writers finish. Unlike /Users/rubber/linux/block/blk-wbt.c: 15
 *   positive scaling steps where we shrink the monitoring window, a negative /Users/rubber/linux/block/blk-wbt.c: 16
 *   scaling step retains the default step==0 window size. /Users/rubber/linux/block/blk-wbt.c: 17
 * Copyright (C) 2016 Jens Axboe /Users/rubber/linux/block/blk-wbt.c: 19
	/* /Users/rubber/linux/block/blk-wbt.c: 55
	 * Default setting, we'll scale up (to 75% of QD max) or down (min 1) /Users/rubber/linux/block/blk-wbt.c: 56
	 * from here depending on device stats /Users/rubber/linux/block/blk-wbt.c: 57
	/* /Users/rubber/linux/block/blk-wbt.c: 61
	 * 100msec window /Users/rubber/linux/block/blk-wbt.c: 62
	/* /Users/rubber/linux/block/blk-wbt.c: 66
	 * Disregard stats, if we don't meet this minimum /Users/rubber/linux/block/blk-wbt.c: 67
	/* /Users/rubber/linux/block/blk-wbt.c: 71
	 * If we have this number of consecutive windows with not enough /Users/rubber/linux/block/blk-wbt.c: 72
	 * information to scale up or down, scale up. /Users/rubber/linux/block/blk-wbt.c: 73
 * If a task was rate throttled in balance_dirty_pages() within the last /Users/rubber/linux/block/blk-wbt.c: 95
 * second or so, use that to indicate a higher cleaning rate. /Users/rubber/linux/block/blk-wbt.c: 96
	/* /Users/rubber/linux/block/blk-wbt.c: 135
	 * wbt got disabled with IO in flight. Wake up any potential /Users/rubber/linux/block/blk-wbt.c: 136
	 * waiters, we don't have to do more than that. /Users/rubber/linux/block/blk-wbt.c: 137
	/* /Users/rubber/linux/block/blk-wbt.c: 144
	 * For discards, our limit is always the background. For writes, if /Users/rubber/linux/block/blk-wbt.c: 145
	 * the device does write back caching, drop further down before we /Users/rubber/linux/block/blk-wbt.c: 146
	 * wake people up. /Users/rubber/linux/block/blk-wbt.c: 147
	/* /Users/rubber/linux/block/blk-wbt.c: 156
	 * Don't wake anyone up if we are above the normal limit. /Users/rubber/linux/block/blk-wbt.c: 157
 * Called on completion of a request. Note that it's also called when /Users/rubber/linux/block/blk-wbt.c: 183
 * a request is merged, when the request gets freed. /Users/rubber/linux/block/blk-wbt.c: 184
	/* /Users/rubber/linux/block/blk-wbt.c: 207
	 * We need at least one read sample, and a minimum of /Users/rubber/linux/block/blk-wbt.c: 208
	 * RWB_MIN_WRITE_SAMPLES. We require some write samples to know /Users/rubber/linux/block/blk-wbt.c: 209
	 * that it's writes impacting us, and not just some sole read on /Users/rubber/linux/block/blk-wbt.c: 210
	 * a device that is in a lower power state. /Users/rubber/linux/block/blk-wbt.c: 211
	/* /Users/rubber/linux/block/blk-wbt.c: 241
	 * If our stored sync issue exceeds the window size, or it /Users/rubber/linux/block/blk-wbt.c: 242
	 * exceeds our min target AND we haven't logged any entries, /Users/rubber/linux/block/blk-wbt.c: 243
	 * flag the latency as exceeded. wbt works off completion latencies, /Users/rubber/linux/block/blk-wbt.c: 244
	 * but for a flooded device, a single sync IO can take a long time /Users/rubber/linux/block/blk-wbt.c: 245
	 * to complete after being issued. If this time exceeds our /Users/rubber/linux/block/blk-wbt.c: 246
	 * monitoring window AND we didn't see any other completions in that /Users/rubber/linux/block/blk-wbt.c: 247
	 * window, then count that sync IO as a violation of the latency. /Users/rubber/linux/block/blk-wbt.c: 248
	/* /Users/rubber/linux/block/blk-wbt.c: 257
	 * No read/write mix, if stat isn't valid /Users/rubber/linux/block/blk-wbt.c: 258
		/* /Users/rubber/linux/block/blk-wbt.c: 261
		 * If we had writes in this stat window and the window is /Users/rubber/linux/block/blk-wbt.c: 262
		 * current, we're only doing writes. If a task recently /Users/rubber/linux/block/blk-wbt.c: 263
		 * waited or still has writes in flights, consider us doing /Users/rubber/linux/block/blk-wbt.c: 264
		 * just writes as well. /Users/rubber/linux/block/blk-wbt.c: 265
	/* /Users/rubber/linux/block/blk-wbt.c: 273
	 * If the 'min' latency exceeds our target, step down. /Users/rubber/linux/block/blk-wbt.c: 274
		/* /Users/rubber/linux/block/blk-wbt.c: 334
		 * We should speed this up, using some variant of a fast /Users/rubber/linux/block/blk-wbt.c: 335
		 * integer inverse square root calculation. Since we only do /Users/rubber/linux/block/blk-wbt.c: 336
		 * this for every window expiration, it's not a huge deal, /Users/rubber/linux/block/blk-wbt.c: 337
		 * though. /Users/rubber/linux/block/blk-wbt.c: 338
		/* /Users/rubber/linux/block/blk-wbt.c: 343
		 * For step < 0, we don't want to increase/decrease the /Users/rubber/linux/block/blk-wbt.c: 344
		 * window size. /Users/rubber/linux/block/blk-wbt.c: 345
	/* /Users/rubber/linux/block/blk-wbt.c: 368
	 * If we exceeded the latency target, step down. If we did not, /Users/rubber/linux/block/blk-wbt.c: 369
	 * step one level up. If we don't know enough to say either exceeded /Users/rubber/linux/block/blk-wbt.c: 370
	 * or ok, then don't do anything. /Users/rubber/linux/block/blk-wbt.c: 371
		/* /Users/rubber/linux/block/blk-wbt.c: 381
		 * We started a the center step, but don't have a valid /Users/rubber/linux/block/blk-wbt.c: 382
		 * read/write sample, but we do have writes going on. /Users/rubber/linux/block/blk-wbt.c: 383
		 * Allow step to go negative, to increase write perf. /Users/rubber/linux/block/blk-wbt.c: 384
		/* /Users/rubber/linux/block/blk-wbt.c: 391
		 * We get here when previously scaled reduced depth, and we /Users/rubber/linux/block/blk-wbt.c: 392
		 * currently don't have a valid read/write sample. For that /Users/rubber/linux/block/blk-wbt.c: 393
		 * case, slowly return to center state (step == 0). /Users/rubber/linux/block/blk-wbt.c: 394
	/* /Users/rubber/linux/block/blk-wbt.c: 405
	 * Re-arm timer, if we have IO in flight /Users/rubber/linux/block/blk-wbt.c: 406
	/* /Users/rubber/linux/block/blk-wbt.c: 458
	 * If we got disabled, just return UINT_MAX. This ensures that /Users/rubber/linux/block/blk-wbt.c: 459
	 * we'll properly inc a new IO, and dec+wakeup at the end. /Users/rubber/linux/block/blk-wbt.c: 460
	/* /Users/rubber/linux/block/blk-wbt.c: 468
	 * At this point we know it's a buffered write. If this is /Users/rubber/linux/block/blk-wbt.c: 469
	 * kswapd trying to free memory, or REQ_SYNC is set, then /Users/rubber/linux/block/blk-wbt.c: 470
	 * it's WB_SYNC_ALL writeback, and we'll use the max limit for /Users/rubber/linux/block/blk-wbt.c: 471
	 * that. If the write is marked as a background write, then use /Users/rubber/linux/block/blk-wbt.c: 472
	 * the idle limit, or go to normal if we haven't had competing /Users/rubber/linux/block/blk-wbt.c: 473
	 * IO for a bit. /Users/rubber/linux/block/blk-wbt.c: 474
		/* /Users/rubber/linux/block/blk-wbt.c: 479
		 * If less than 100ms since we completed unrelated IO, /Users/rubber/linux/block/blk-wbt.c: 480
		 * limit us to half the depth for background writeback. /Users/rubber/linux/block/blk-wbt.c: 481
 * Block if we will exceed our limit, or if we are currently waiting for /Users/rubber/linux/block/blk-wbt.c: 509
 * the timer to kick off queuing again. /Users/rubber/linux/block/blk-wbt.c: 510
		/* /Users/rubber/linux/block/blk-wbt.c: 529
		 * Don't throttle WRITE_ODIRECT /Users/rubber/linux/block/blk-wbt.c: 530
 * May sleep, if we have exceeded the writeback limits. Caller can pass /Users/rubber/linux/block/blk-wbt.c: 570
 * in an irq held spinlock, if it holds one when calling this function. /Users/rubber/linux/block/blk-wbt.c: 571
 * If we do sleep, we'll release and re-grab it. /Users/rubber/linux/block/blk-wbt.c: 572
	/* /Users/rubber/linux/block/blk-wbt.c: 605
	 * Track sync issue, in case it takes a long time to complete. Allows us /Users/rubber/linux/block/blk-wbt.c: 606
	 * to react quicker, if a sync IO takes a long time to complete. Note /Users/rubber/linux/block/blk-wbt.c: 607
	 * that this is just a hint. The request can go away when it completes, /Users/rubber/linux/block/blk-wbt.c: 608
	 * so it's important we never dereference it. We only use the address to /Users/rubber/linux/block/blk-wbt.c: 609
	 * compare with, which is why we store the sync_issue time locally. /Users/rubber/linux/block/blk-wbt.c: 610
 * Enable wbt if defaults are configured that way /Users/rubber/linux/block/blk-wbt.c: 637
	/* /Users/rubber/linux/block/blk-wbt.c: 661
	 * We default to 2msec for non-rotational storage, and 75msec /Users/rubber/linux/block/blk-wbt.c: 662
	 * for rotational storage. /Users/rubber/linux/block/blk-wbt.c: 663
 * Disable wbt, if enabled by default. /Users/rubber/linux/block/blk-wbt.c: 701
	/* /Users/rubber/linux/block/blk-wbt.c: 846
	 * Assign rwb and add the stats callback. /Users/rubber/linux/block/blk-wbt.c: 847
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/ioprio.c: 1
 * fs/ioprio.c /Users/rubber/linux/block/ioprio.c: 3
 * Copyright (C) 2004 Jens Axboe <axboe@kernel.dk> /Users/rubber/linux/block/ioprio.c: 5
 * Helper functions for setting/querying io priorities of processes. The /Users/rubber/linux/block/ioprio.c: 7
 * system calls closely mimmick getpriority/setpriority, see the man page for /Users/rubber/linux/block/ioprio.c: 8
 * those. The prio argument is a composite of prio class and prio data, where /Users/rubber/linux/block/ioprio.c: 9
 * the data argument has meaning within that class. The standard scheduling /Users/rubber/linux/block/ioprio.c: 10
 * classes have 8 distinct prio levels, with 0 being the highest prio and 7 /Users/rubber/linux/block/ioprio.c: 11
 * being the lowest. /Users/rubber/linux/block/ioprio.c: 12
 * IOW, setting BE scheduling class with prio 2 is done ala: /Users/rubber/linux/block/ioprio.c: 14
 * unsigned int prio = (IOPRIO_CLASS_BE << IOPRIO_CLASS_SHIFT) | 2; /Users/rubber/linux/block/ioprio.c: 16
 * ioprio_set(PRIO_PROCESS, pid, prio); /Users/rubber/linux/block/ioprio.c: 18
 * See also Documentation/block/ioprio.rst /Users/rubber/linux/block/ioprio.c: 20
			/* /Users/rubber/linux/block/ioprio.c: 72
			 * Originally this only checked for CAP_SYS_ADMIN, /Users/rubber/linux/block/ioprio.c: 73
			 * which was implicitly allowed for pid 0 by security /Users/rubber/linux/block/ioprio.c: 74
			 * modules such as SELinux. Make sure we check /Users/rubber/linux/block/ioprio.c: 75
			 * CAP_SYS_ADMIN first to avoid a denial/avc for /Users/rubber/linux/block/ioprio.c: 76
			 * possibly missing CAP_SYS_NICE permission. /Users/rubber/linux/block/ioprio.c: 77
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/bio-integrity.c: 1
 * bio-integrity.c - bio data integrity extensions /Users/rubber/linux/block/bio-integrity.c: 3
 * Copyright (C) 2007, 2008, 2009 Oracle Corporation /Users/rubber/linux/block/bio-integrity.c: 5
 * Written by: Martin K. Petersen <martin.petersen@oracle.com> /Users/rubber/linux/block/bio-integrity.c: 6
 * bio_integrity_alloc - Allocate integrity payload and attach it to bio /Users/rubber/linux/block/bio-integrity.c: 39
 * @bio:	bio to attach integrity metadata to /Users/rubber/linux/block/bio-integrity.c: 40
 * @gfp_mask:	Memory allocation mask /Users/rubber/linux/block/bio-integrity.c: 41
 * @nr_vecs:	Number of integrity metadata scatter-gather elements /Users/rubber/linux/block/bio-integrity.c: 42
 * Description: This function prepares a bio for attaching integrity /Users/rubber/linux/block/bio-integrity.c: 44
 * metadata.  nr_vecs specifies the maximum number of pages containing /Users/rubber/linux/block/bio-integrity.c: 45
 * integrity metadata that can be attached. /Users/rubber/linux/block/bio-integrity.c: 46
 * bio_integrity_free - Free bio integrity payload /Users/rubber/linux/block/bio-integrity.c: 95
 * @bio:	bio containing bip to be freed /Users/rubber/linux/block/bio-integrity.c: 96
 * Description: Used to free the integrity portion of a bio. Usually /Users/rubber/linux/block/bio-integrity.c: 98
 * called from bio_free(). /Users/rubber/linux/block/bio-integrity.c: 99
 * bio_integrity_add_page - Attach integrity metadata /Users/rubber/linux/block/bio-integrity.c: 115
 * @bio:	bio to update /Users/rubber/linux/block/bio-integrity.c: 116
 * @page:	page containing integrity metadata /Users/rubber/linux/block/bio-integrity.c: 117
 * @len:	number of bytes of integrity metadata in page /Users/rubber/linux/block/bio-integrity.c: 118
 * @offset:	start offset within page /Users/rubber/linux/block/bio-integrity.c: 119
 * Description: Attach a page containing integrity metadata to bio. /Users/rubber/linux/block/bio-integrity.c: 121
 * bio_integrity_process - Process integrity metadata for a bio /Users/rubber/linux/block/bio-integrity.c: 151
 * @bio:	bio to generate/verify integrity metadata for /Users/rubber/linux/block/bio-integrity.c: 152
 * @proc_iter:  iterator to process /Users/rubber/linux/block/bio-integrity.c: 153
 * @proc_fn:	Pointer to the relevant processing function /Users/rubber/linux/block/bio-integrity.c: 154
 * bio_integrity_prep - Prepare bio for integrity I/O /Users/rubber/linux/block/bio-integrity.c: 187
 * @bio:	bio to prepare /Users/rubber/linux/block/bio-integrity.c: 188
 * Description:  Checks if the bio already has an integrity payload attached. /Users/rubber/linux/block/bio-integrity.c: 190
 * If it does, the payload has been generated by another kernel subsystem, /Users/rubber/linux/block/bio-integrity.c: 191
 * and we just pass it through. Otherwise allocates integrity payload. /Users/rubber/linux/block/bio-integrity.c: 192
 * The bio must have data direction, target device and start sector set priot /Users/rubber/linux/block/bio-integrity.c: 193
 * to calling.  In the WRITE case, integrity metadata will be generated using /Users/rubber/linux/block/bio-integrity.c: 194
 * the block device's integrity function.  In the READ case, the buffer /Users/rubber/linux/block/bio-integrity.c: 195
 * will be prepared for DMA and a suitable end_io handler set up. /Users/rubber/linux/block/bio-integrity.c: 196
 * bio_integrity_verify_fn - Integrity I/O completion worker /Users/rubber/linux/block/bio-integrity.c: 309
 * @work:	Work struct stored in bio to be verified /Users/rubber/linux/block/bio-integrity.c: 310
 * Description: This workqueue function is called to complete a READ /Users/rubber/linux/block/bio-integrity.c: 312
 * request.  The function verifies the transferred integrity metadata /Users/rubber/linux/block/bio-integrity.c: 313
 * and then calls the original bio end_io function. /Users/rubber/linux/block/bio-integrity.c: 314
	/* /Users/rubber/linux/block/bio-integrity.c: 323
	 * At the moment verify is called bio's iterator was advanced /Users/rubber/linux/block/bio-integrity.c: 324
	 * during split and completion, we need to rewind iterator to /Users/rubber/linux/block/bio-integrity.c: 325
	 * it's original position. /Users/rubber/linux/block/bio-integrity.c: 326
 * __bio_integrity_endio - Integrity I/O completion function /Users/rubber/linux/block/bio-integrity.c: 335
 * @bio:	Protected bio /Users/rubber/linux/block/bio-integrity.c: 336
 * Description: Completion for integrity I/O /Users/rubber/linux/block/bio-integrity.c: 338
 * Normally I/O completion is done in interrupt context.  However, /Users/rubber/linux/block/bio-integrity.c: 340
 * verifying I/O integrity is a time-consuming task which must be run /Users/rubber/linux/block/bio-integrity.c: 341
 * in process context.	This function postpones completion /Users/rubber/linux/block/bio-integrity.c: 342
 * accordingly. /Users/rubber/linux/block/bio-integrity.c: 343
 * bio_integrity_advance - Advance integrity vector /Users/rubber/linux/block/bio-integrity.c: 362
 * @bio:	bio whose integrity vector to update /Users/rubber/linux/block/bio-integrity.c: 363
 * @bytes_done:	number of data bytes that have been completed /Users/rubber/linux/block/bio-integrity.c: 364
 * Description: This function calculates how many integrity bytes the /Users/rubber/linux/block/bio-integrity.c: 366
 * number of completed data bytes correspond to and advances the /Users/rubber/linux/block/bio-integrity.c: 367
 * integrity vector accordingly. /Users/rubber/linux/block/bio-integrity.c: 368
 * bio_integrity_trim - Trim integrity vector /Users/rubber/linux/block/bio-integrity.c: 381
 * @bio:	bio whose integrity vector to update /Users/rubber/linux/block/bio-integrity.c: 382
 * Description: Used to trim the integrity vector in a cloned bio. /Users/rubber/linux/block/bio-integrity.c: 384
 * bio_integrity_clone - Callback for cloning bios with integrity metadata /Users/rubber/linux/block/bio-integrity.c: 396
 * @bio:	New bio /Users/rubber/linux/block/bio-integrity.c: 397
 * @bio_src:	Original bio /Users/rubber/linux/block/bio-integrity.c: 398
 * @gfp_mask:	Memory allocation mask /Users/rubber/linux/block/bio-integrity.c: 399
 * Description:	Called to allocate a bip when cloning a bio /Users/rubber/linux/block/bio-integrity.c: 401
	/* /Users/rubber/linux/block/bio-integrity.c: 451
	 * kintegrityd won't block much but may burn a lot of CPU cycles. /Users/rubber/linux/block/bio-integrity.c: 452
	 * Make it highpri CPU intensive wq with max concurrency of 1. /Users/rubber/linux/block/bio-integrity.c: 453
/* SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-iocost.c: 1
 * IO cost model based controller. /Users/rubber/linux/block/blk-iocost.c: 3
 * Copyright (C) 2019 Tejun Heo <tj@kernel.org> /Users/rubber/linux/block/blk-iocost.c: 5
 * Copyright (C) 2019 Andy Newell <newella@fb.com> /Users/rubber/linux/block/blk-iocost.c: 6
 * Copyright (C) 2019 Facebook /Users/rubber/linux/block/blk-iocost.c: 7
 * One challenge of controlling IO resources is the lack of trivially /Users/rubber/linux/block/blk-iocost.c: 9
 * observable cost metric.  This is distinguished from CPU and memory where /Users/rubber/linux/block/blk-iocost.c: 10
 * wallclock time and the number of bytes can serve as accurate enough /Users/rubber/linux/block/blk-iocost.c: 11
 * approximations. /Users/rubber/linux/block/blk-iocost.c: 12
 * Bandwidth and iops are the most commonly used metrics for IO devices but /Users/rubber/linux/block/blk-iocost.c: 14
 * depending on the type and specifics of the device, different IO patterns /Users/rubber/linux/block/blk-iocost.c: 15
 * easily lead to multiple orders of magnitude variations rendering them /Users/rubber/linux/block/blk-iocost.c: 16
 * useless for the purpose of IO capacity distribution.  While on-device /Users/rubber/linux/block/blk-iocost.c: 17
 * time, with a lot of clutches, could serve as a useful approximation for /Users/rubber/linux/block/blk-iocost.c: 18
 * non-queued rotational devices, this is no longer viable with modern /Users/rubber/linux/block/blk-iocost.c: 19
 * devices, even the rotational ones. /Users/rubber/linux/block/blk-iocost.c: 20
 * While there is no cost metric we can trivially observe, it isn't a /Users/rubber/linux/block/blk-iocost.c: 22
 * complete mystery.  For example, on a rotational device, seek cost /Users/rubber/linux/block/blk-iocost.c: 23
 * dominates while a contiguous transfer contributes a smaller amount /Users/rubber/linux/block/blk-iocost.c: 24
 * proportional to the size.  If we can characterize at least the relative /Users/rubber/linux/block/blk-iocost.c: 25
 * costs of these different types of IOs, it should be possible to /Users/rubber/linux/block/blk-iocost.c: 26
 * implement a reasonable work-conserving proportional IO resource /Users/rubber/linux/block/blk-iocost.c: 27
 * distribution. /Users/rubber/linux/block/blk-iocost.c: 28
 * 1. IO Cost Model /Users/rubber/linux/block/blk-iocost.c: 30
 * IO cost model estimates the cost of an IO given its basic parameters and /Users/rubber/linux/block/blk-iocost.c: 32
 * history (e.g. the end sector of the last IO).  The cost is measured in /Users/rubber/linux/block/blk-iocost.c: 33
 * device time.  If a given IO is estimated to cost 10ms, the device should /Users/rubber/linux/block/blk-iocost.c: 34
 * be able to process ~100 of those IOs in a second. /Users/rubber/linux/block/blk-iocost.c: 35
 * Currently, there's only one builtin cost model - linear.  Each IO is /Users/rubber/linux/block/blk-iocost.c: 37
 * classified as sequential or random and given a base cost accordingly. /Users/rubber/linux/block/blk-iocost.c: 38
 * On top of that, a size cost proportional to the length of the IO is /Users/rubber/linux/block/blk-iocost.c: 39
 * added.  While simple, this model captures the operational /Users/rubber/linux/block/blk-iocost.c: 40
 * characteristics of a wide varienty of devices well enough.  Default /Users/rubber/linux/block/blk-iocost.c: 41
 * parameters for several different classes of devices are provided and the /Users/rubber/linux/block/blk-iocost.c: 42
 * parameters can be configured from userspace via /Users/rubber/linux/block/blk-iocost.c: 43
 * /sys/fs/cgroup/io.cost.model. /Users/rubber/linux/block/blk-iocost.c: 44
 * If needed, tools/cgroup/iocost_coef_gen.py can be used to generate /Users/rubber/linux/block/blk-iocost.c: 46
 * device-specific coefficients. /Users/rubber/linux/block/blk-iocost.c: 47
 * 2. Control Strategy /Users/rubber/linux/block/blk-iocost.c: 49
 * The device virtual time (vtime) is used as the primary control metric. /Users/rubber/linux/block/blk-iocost.c: 51
 * The control strategy is composed of the following three parts. /Users/rubber/linux/block/blk-iocost.c: 52
 * 2-1. Vtime Distribution /Users/rubber/linux/block/blk-iocost.c: 54
 * When a cgroup becomes active in terms of IOs, its hierarchical share is /Users/rubber/linux/block/blk-iocost.c: 56
 * calculated.  Please consider the following hierarchy where the numbers /Users/rubber/linux/block/blk-iocost.c: 57
 * inside parentheses denote the configured weights. /Users/rubber/linux/block/blk-iocost.c: 58
 *           root /Users/rubber/linux/block/blk-iocost.c: 60
 *         /       \ /Users/rubber/linux/block/blk-iocost.c: 61
 *      A (w:100)  B (w:300) /Users/rubber/linux/block/blk-iocost.c: 62
 *      /       \ /Users/rubber/linux/block/blk-iocost.c: 63
 *  A0 (w:100)  A1 (w:100) /Users/rubber/linux/block/blk-iocost.c: 64
 * If B is idle and only A0 and A1 are actively issuing IOs, as the two are /Users/rubber/linux/block/blk-iocost.c: 66
 * of equal weight, each gets 50% share.  If then B starts issuing IOs, B /Users/rubber/linux/block/blk-iocost.c: 67
 * gets 300/(100+300) or 75% share, and A0 and A1 equally splits the rest, /Users/rubber/linux/block/blk-iocost.c: 68
 * 12.5% each.  The distribution mechanism only cares about these flattened /Users/rubber/linux/block/blk-iocost.c: 69
 * shares.  They're called hweights (hierarchical weights) and always add /Users/rubber/linux/block/blk-iocost.c: 70
 * upto 1 (WEIGHT_ONE). /Users/rubber/linux/block/blk-iocost.c: 71
 * A given cgroup's vtime runs slower in inverse proportion to its hweight. /Users/rubber/linux/block/blk-iocost.c: 73
 * For example, with 12.5% weight, A0's time runs 8 times slower (100/12.5) /Users/rubber/linux/block/blk-iocost.c: 74
 * against the device vtime - an IO which takes 10ms on the underlying /Users/rubber/linux/block/blk-iocost.c: 75
 * device is considered to take 80ms on A0. /Users/rubber/linux/block/blk-iocost.c: 76
 * This constitutes the basis of IO capacity distribution.  Each cgroup's /Users/rubber/linux/block/blk-iocost.c: 78
 * vtime is running at a rate determined by its hweight.  A cgroup tracks /Users/rubber/linux/block/blk-iocost.c: 79
 * the vtime consumed by past IOs and can issue a new IO if doing so /Users/rubber/linux/block/blk-iocost.c: 80
 * wouldn't outrun the current device vtime.  Otherwise, the IO is /Users/rubber/linux/block/blk-iocost.c: 81
 * suspended until the vtime has progressed enough to cover it. /Users/rubber/linux/block/blk-iocost.c: 82
 * 2-2. Vrate Adjustment /Users/rubber/linux/block/blk-iocost.c: 84
 * It's unrealistic to expect the cost model to be perfect.  There are too /Users/rubber/linux/block/blk-iocost.c: 86
 * many devices and even on the same device the overall performance /Users/rubber/linux/block/blk-iocost.c: 87
 * fluctuates depending on numerous factors such as IO mixture and device /Users/rubber/linux/block/blk-iocost.c: 88
 * internal garbage collection.  The controller needs to adapt dynamically. /Users/rubber/linux/block/blk-iocost.c: 89
 * This is achieved by adjusting the overall IO rate according to how busy /Users/rubber/linux/block/blk-iocost.c: 91
 * the device is.  If the device becomes overloaded, we're sending down too /Users/rubber/linux/block/blk-iocost.c: 92
 * many IOs and should generally slow down.  If there are waiting issuers /Users/rubber/linux/block/blk-iocost.c: 93
 * but the device isn't saturated, we're issuing too few and should /Users/rubber/linux/block/blk-iocost.c: 94
 * generally speed up. /Users/rubber/linux/block/blk-iocost.c: 95
 * To slow down, we lower the vrate - the rate at which the device vtime /Users/rubber/linux/block/blk-iocost.c: 97
 * passes compared to the wall clock.  For example, if the vtime is running /Users/rubber/linux/block/blk-iocost.c: 98
 * at the vrate of 75%, all cgroups added up would only be able to issue /Users/rubber/linux/block/blk-iocost.c: 99
 * 750ms worth of IOs per second, and vice-versa for speeding up. /Users/rubber/linux/block/blk-iocost.c: 100
 * Device business is determined using two criteria - rq wait and /Users/rubber/linux/block/blk-iocost.c: 102
 * completion latencies. /Users/rubber/linux/block/blk-iocost.c: 103
 * When a device gets saturated, the on-device and then the request queues /Users/rubber/linux/block/blk-iocost.c: 105
 * fill up and a bio which is ready to be issued has to wait for a request /Users/rubber/linux/block/blk-iocost.c: 106
 * to become available.  When this delay becomes noticeable, it's a clear /Users/rubber/linux/block/blk-iocost.c: 107
 * indication that the device is saturated and we lower the vrate.  This /Users/rubber/linux/block/blk-iocost.c: 108
 * saturation signal is fairly conservative as it only triggers when both /Users/rubber/linux/block/blk-iocost.c: 109
 * hardware and software queues are filled up, and is used as the default /Users/rubber/linux/block/blk-iocost.c: 110
 * busy signal. /Users/rubber/linux/block/blk-iocost.c: 111
 * As devices can have deep queues and be unfair in how the queued commands /Users/rubber/linux/block/blk-iocost.c: 113
 * are executed, soley depending on rq wait may not result in satisfactory /Users/rubber/linux/block/blk-iocost.c: 114
 * control quality.  For a better control quality, completion latency QoS /Users/rubber/linux/block/blk-iocost.c: 115
 * parameters can be configured so that the device is considered saturated /Users/rubber/linux/block/blk-iocost.c: 116
 * if N'th percentile completion latency rises above the set point. /Users/rubber/linux/block/blk-iocost.c: 117
 * The completion latency requirements are a function of both the /Users/rubber/linux/block/blk-iocost.c: 119
 * underlying device characteristics and the desired IO latency quality of /Users/rubber/linux/block/blk-iocost.c: 120
 * service.  There is an inherent trade-off - the tighter the latency QoS, /Users/rubber/linux/block/blk-iocost.c: 121
 * the higher the bandwidth lossage.  Latency QoS is disabled by default /Users/rubber/linux/block/blk-iocost.c: 122
 * and can be set through /sys/fs/cgroup/io.cost.qos. /Users/rubber/linux/block/blk-iocost.c: 123
 * 2-3. Work Conservation /Users/rubber/linux/block/blk-iocost.c: 125
 * Imagine two cgroups A and B with equal weights.  A is issuing a small IO /Users/rubber/linux/block/blk-iocost.c: 127
 * periodically while B is sending out enough parallel IOs to saturate the /Users/rubber/linux/block/blk-iocost.c: 128
 * device on its own.  Let's say A's usage amounts to 100ms worth of IO /Users/rubber/linux/block/blk-iocost.c: 129
 * cost per second, i.e., 10% of the device capacity.  The naive /Users/rubber/linux/block/blk-iocost.c: 130
 * distribution of half and half would lead to 60% utilization of the /Users/rubber/linux/block/blk-iocost.c: 131
 * device, a significant reduction in the total amount of work done /Users/rubber/linux/block/blk-iocost.c: 132
 * compared to free-for-all competition.  This is too high a cost to pay /Users/rubber/linux/block/blk-iocost.c: 133
 * for IO control. /Users/rubber/linux/block/blk-iocost.c: 134
 * To conserve the total amount of work done, we keep track of how much /Users/rubber/linux/block/blk-iocost.c: 136
 * each active cgroup is actually using and yield part of its weight if /Users/rubber/linux/block/blk-iocost.c: 137
 * there are other cgroups which can make use of it.  In the above case, /Users/rubber/linux/block/blk-iocost.c: 138
 * A's weight will be lowered so that it hovers above the actual usage and /Users/rubber/linux/block/blk-iocost.c: 139
 * B would be able to use the rest. /Users/rubber/linux/block/blk-iocost.c: 140
 * As we don't want to penalize a cgroup for donating its weight, the /Users/rubber/linux/block/blk-iocost.c: 142
 * surplus weight adjustment factors in a margin and has an immediate /Users/rubber/linux/block/blk-iocost.c: 143
 * snapback mechanism in case the cgroup needs more IO vtime for itself. /Users/rubber/linux/block/blk-iocost.c: 144
 * Note that adjusting down surplus weights has the same effects as /Users/rubber/linux/block/blk-iocost.c: 146
 * accelerating vtime for other cgroups and work conservation can also be /Users/rubber/linux/block/blk-iocost.c: 147
 * implemented by adjusting vrate dynamically.  However, squaring who can /Users/rubber/linux/block/blk-iocost.c: 148
 * donate and should take back how much requires hweight propagations /Users/rubber/linux/block/blk-iocost.c: 149
 * anyway making it easier to implement and understand as a separate /Users/rubber/linux/block/blk-iocost.c: 150
 * mechanism. /Users/rubber/linux/block/blk-iocost.c: 151
 * 3. Monitoring /Users/rubber/linux/block/blk-iocost.c: 153
 * Instead of debugfs or other clumsy monitoring mechanisms, this /Users/rubber/linux/block/blk-iocost.c: 155
 * controller uses a drgn based monitoring script - /Users/rubber/linux/block/blk-iocost.c: 156
 * tools/cgroup/iocost_monitor.py.  For details on drgn, please see /Users/rubber/linux/block/blk-iocost.c: 157
 * https://github.com/osandov/drgn.  The output looks like the following. /Users/rubber/linux/block/blk-iocost.c: 158
 *  sdb RUN   per=300ms cur_per=234.218:v203.695 busy= +1 vrate= 62.12% /Users/rubber/linux/block/blk-iocost.c: 160
 *                 active      weight      hweight% inflt% dbt  delay usages% /Users/rubber/linux/block/blk-iocost.c: 161
 *  test/a              *    50/   50  33.33/ 33.33  27.65   2  0*041 033:033:033 /Users/rubber/linux/block/blk-iocost.c: 162
 *  test/b              *   100/  100  66.67/ 66.67  17.56   0  0*000 066:079:077 /Users/rubber/linux/block/blk-iocost.c: 163
 * - per	: Timer period /Users/rubber/linux/block/blk-iocost.c: 165
 * - cur_per	: Internal wall and device vtime clock /Users/rubber/linux/block/blk-iocost.c: 166
 * - vrate	: Device virtual time rate against wall clock /Users/rubber/linux/block/blk-iocost.c: 167
 * - weight	: Surplus-adjusted and configured weights /Users/rubber/linux/block/blk-iocost.c: 168
 * - hweight	: Surplus-adjusted and configured hierarchical weights /Users/rubber/linux/block/blk-iocost.c: 169
 * - inflt	: The percentage of in-flight IO cost at the end of last period /Users/rubber/linux/block/blk-iocost.c: 170
 * - del_ms	: Deferred issuer delay induction level and duration /Users/rubber/linux/block/blk-iocost.c: 171
 * - usages	: Usage history /Users/rubber/linux/block/blk-iocost.c: 172
	/* /Users/rubber/linux/block/blk-iocost.c: 219
	 * iocg->vtime is targeted at 50% behind the device vtime, which /Users/rubber/linux/block/blk-iocost.c: 220
	 * serves as its IO credit buffer.  Surplus weight adjustment is /Users/rubber/linux/block/blk-iocost.c: 221
	 * immediately canceled if the vtime margin runs below 10%. /Users/rubber/linux/block/blk-iocost.c: 222
	/* /Users/rubber/linux/block/blk-iocost.c: 236
	 * As vtime is used to calculate the cost of each IO, it needs to /Users/rubber/linux/block/blk-iocost.c: 237
	 * be fairly high precision.  For example, it should be able to /Users/rubber/linux/block/blk-iocost.c: 238
	 * represent the cost of a single page worth of discard with /Users/rubber/linux/block/blk-iocost.c: 239
	 * suffificient accuracy.  At the same time, it should be able to /Users/rubber/linux/block/blk-iocost.c: 240
	 * represent reasonably long enough durations to be useful and /Users/rubber/linux/block/blk-iocost.c: 241
	 * convenient during operation. /Users/rubber/linux/block/blk-iocost.c: 242
	 * /Users/rubber/linux/block/blk-iocost.c: 243
	 * 1s worth of vtime is 2^37.  This gives us both sub-nanosecond /Users/rubber/linux/block/blk-iocost.c: 244
	 * granularity and days of wrap-around time even at extreme vrates. /Users/rubber/linux/block/blk-iocost.c: 245
	/* /Users/rubber/linux/block/blk-iocost.c: 265
	 * The effect of delay is indirect and non-linear and a huge amount of /Users/rubber/linux/block/blk-iocost.c: 266
	 * future debt can accumulate abruptly while unthrottled. Linearly scale /Users/rubber/linux/block/blk-iocost.c: 267
	 * up delay as debt is going up and then let it decay exponentially. /Users/rubber/linux/block/blk-iocost.c: 268
	 * This gives us quick ramp ups while delay is accumulating and long /Users/rubber/linux/block/blk-iocost.c: 269
	 * tails which can help reducing the frequency of debt explosions on /Users/rubber/linux/block/blk-iocost.c: 270
	 * unthrottle. The parameters are experimentally determined. /Users/rubber/linux/block/blk-iocost.c: 271
	 * /Users/rubber/linux/block/blk-iocost.c: 272
	 * The delay mechanism provides adequate protection and behavior in many /Users/rubber/linux/block/blk-iocost.c: 273
	 * cases. However, this is far from ideal and falls shorts on both /Users/rubber/linux/block/blk-iocost.c: 274
	 * fronts. The debtors are often throttled too harshly costing a /Users/rubber/linux/block/blk-iocost.c: 275
	 * significant level of fairness and possibly total work while the /Users/rubber/linux/block/blk-iocost.c: 276
	 * protection against their impacts on the system can be choppy and /Users/rubber/linux/block/blk-iocost.c: 277
	 * unreliable. /Users/rubber/linux/block/blk-iocost.c: 278
	 * /Users/rubber/linux/block/blk-iocost.c: 279
	 * The shortcoming primarily stems from the fact that, unlike for page /Users/rubber/linux/block/blk-iocost.c: 280
	 * cache, the kernel doesn't have well-defined back-pressure propagation /Users/rubber/linux/block/blk-iocost.c: 281
	 * mechanism and policies for anonymous memory. Fully addressing this /Users/rubber/linux/block/blk-iocost.c: 282
	 * issue will likely require substantial improvements in the area. /Users/rubber/linux/block/blk-iocost.c: 283
	/* /Users/rubber/linux/block/blk-iocost.c: 300
	 * Count IO size in 4k pages.  The 12bit shift helps keeping /Users/rubber/linux/block/blk-iocost.c: 301
	 * size-proportional components of cost calculation in closer /Users/rubber/linux/block/blk-iocost.c: 302
	 * numbers of digits to per-IO cost components. /Users/rubber/linux/block/blk-iocost.c: 303
	/* /Users/rubber/linux/block/blk-iocost.c: 462
	 * A iocg can get its weight from two sources - an explicit /Users/rubber/linux/block/blk-iocost.c: 463
	 * per-device-cgroup configuration or the default weight of the /Users/rubber/linux/block/blk-iocost.c: 464
	 * cgroup.  `cfg_weight` is the explicit per-device-cgroup /Users/rubber/linux/block/blk-iocost.c: 465
	 * configuration.  `weight` is the effective considering both /Users/rubber/linux/block/blk-iocost.c: 466
	 * sources. /Users/rubber/linux/block/blk-iocost.c: 467
	 * /Users/rubber/linux/block/blk-iocost.c: 468
	 * When an idle cgroup becomes active its `active` goes from 0 to /Users/rubber/linux/block/blk-iocost.c: 469
	 * `weight`.  `inuse` is the surplus adjusted active weight. /Users/rubber/linux/block/blk-iocost.c: 470
	 * `active` and `inuse` are used to calculate `hweight_active` and /Users/rubber/linux/block/blk-iocost.c: 471
	 * `hweight_inuse`. /Users/rubber/linux/block/blk-iocost.c: 472
	 * /Users/rubber/linux/block/blk-iocost.c: 473
	 * `last_inuse` remembers `inuse` while an iocg is idle to persist /Users/rubber/linux/block/blk-iocost.c: 474
	 * surplus adjustments. /Users/rubber/linux/block/blk-iocost.c: 475
	 * /Users/rubber/linux/block/blk-iocost.c: 476
	 * `inuse` may be adjusted dynamically during period. `saved_*` are used /Users/rubber/linux/block/blk-iocost.c: 477
	 * to determine and track adjustments. /Users/rubber/linux/block/blk-iocost.c: 478
	/* /Users/rubber/linux/block/blk-iocost.c: 490
	 * `vtime` is this iocg's vtime cursor which progresses as IOs are /Users/rubber/linux/block/blk-iocost.c: 491
	 * issued.  If lagging behind device vtime, the delta represents /Users/rubber/linux/block/blk-iocost.c: 492
	 * the currently available IO budget.  If running ahead, the /Users/rubber/linux/block/blk-iocost.c: 493
	 * overage. /Users/rubber/linux/block/blk-iocost.c: 494
	 * /Users/rubber/linux/block/blk-iocost.c: 495
	 * `vtime_done` is the same but progressed on completion rather /Users/rubber/linux/block/blk-iocost.c: 496
	 * than issue.  The delta behind `vtime` represents the cost of /Users/rubber/linux/block/blk-iocost.c: 497
	 * currently in-flight IOs. /Users/rubber/linux/block/blk-iocost.c: 498
	/* /Users/rubber/linux/block/blk-iocost.c: 508
	 * The period this iocg was last active in.  Used for deactivation /Users/rubber/linux/block/blk-iocost.c: 509
	 * and invalidating `vtime`. /Users/rubber/linux/block/blk-iocost.c: 510
 * vrate adjust percentages indexed by ioc->busy_level.  We adjust up on /Users/rubber/linux/block/blk-iocost.c: 646
 * vtime credit shortage and down on device saturation. /Users/rubber/linux/block/blk-iocost.c: 647
 * Scale @abs_cost to the inverse of @hw_inuse.  The lower the hierarchical /Users/rubber/linux/block/blk-iocost.c: 703
 * weight, the more expensive each IO.  Must round up. /Users/rubber/linux/block/blk-iocost.c: 704
 * The inverse of abs_cost_to_cost().  Must round up. /Users/rubber/linux/block/blk-iocost.c: 712
	/* /Users/rubber/linux/block/blk-iocost.c: 782
	 * We want the period to be long enough to contain a healthy number /Users/rubber/linux/block/blk-iocost.c: 783
	 * of IOs while short enough for granular control.  Define it as a /Users/rubber/linux/block/blk-iocost.c: 784
	 * multiple of the latency target.  Ideally, the multiplier should /Users/rubber/linux/block/blk-iocost.c: 785
	 * be scaled according to the percentile so that it would nominally /Users/rubber/linux/block/blk-iocost.c: 786
	 * contain a certain number of requests.  Let's be simpler and /Users/rubber/linux/block/blk-iocost.c: 787
	 * scale it linearly so that it's 2x >= pct(90) and 10x at pct(50). /Users/rubber/linux/block/blk-iocost.c: 788
 * Take the followings as input /Users/rubber/linux/block/blk-iocost.c: 854
 *  @bps	maximum sequential throughput /Users/rubber/linux/block/blk-iocost.c: 856
 *  @seqiops	maximum sequential 4k iops /Users/rubber/linux/block/blk-iocost.c: 857
 *  @randiops	maximum random 4k iops /Users/rubber/linux/block/blk-iocost.c: 858
 * and calculate the linear model cost coefficients. /Users/rubber/linux/block/blk-iocost.c: 860
 *  *@page	per-page cost		1s / (@bps / 4096) /Users/rubber/linux/block/blk-iocost.c: 862
 *  *@seqio	base cost of a seq IO	max((1s / @seqiops) - *@page, 0) /Users/rubber/linux/block/blk-iocost.c: 863
 *  @randiops	base cost of a rand IO	max((1s / @randiops) - *@page, 0) /Users/rubber/linux/block/blk-iocost.c: 864
 * When an iocg accumulates too much vtime or gets deactivated, we throw away /Users/rubber/linux/block/blk-iocost.c: 938
 * some vtime, which lowers the overall device utilization. As the exact amount /Users/rubber/linux/block/blk-iocost.c: 939
 * which is being thrown away is known, we can compensate by accelerating the /Users/rubber/linux/block/blk-iocost.c: 940
 * vrate accordingly so that the extra vtime generated in the current period /Users/rubber/linux/block/blk-iocost.c: 941
 * matches what got lost. /Users/rubber/linux/block/blk-iocost.c: 942
	/* /Users/rubber/linux/block/blk-iocost.c: 956
	 * Calculate how much vrate should be adjusted to offset the error. /Users/rubber/linux/block/blk-iocost.c: 957
	 * Limit the amount of adjustment and deduct the adjusted amount from /Users/rubber/linux/block/blk-iocost.c: 958
	 * the error. /Users/rubber/linux/block/blk-iocost.c: 959
	/* /Users/rubber/linux/block/blk-iocost.c: 990
	 * If vrate is out of bounds, apply clamp gradually as the /Users/rubber/linux/block/blk-iocost.c: 991
	 * bounds can change abruptly.  Otherwise, apply busy_level /Users/rubber/linux/block/blk-iocost.c: 992
	 * based adjustment. /Users/rubber/linux/block/blk-iocost.c: 993
	/* /Users/rubber/linux/block/blk-iocost.c: 1031
	 * The current vtime is /Users/rubber/linux/block/blk-iocost.c: 1032
	 * /Users/rubber/linux/block/blk-iocost.c: 1033
	 *   vtime at period start + (wallclock time since the start) * vrate /Users/rubber/linux/block/blk-iocost.c: 1034
	 * /Users/rubber/linux/block/blk-iocost.c: 1035
	 * As a consistent snapshot of `period_at_vtime` and `period_at` is /Users/rubber/linux/block/blk-iocost.c: 1036
	 * needed, they're seqcount protected. /Users/rubber/linux/block/blk-iocost.c: 1037
 * Update @iocg's `active` and `inuse` to @active and @inuse, update level /Users/rubber/linux/block/blk-iocost.c: 1060
 * weight sums and propagate upwards accordingly. If @save, the current margin /Users/rubber/linux/block/blk-iocost.c: 1061
 * is saved to be used as reference for later inuse in-period adjustments. /Users/rubber/linux/block/blk-iocost.c: 1062
	/* /Users/rubber/linux/block/blk-iocost.c: 1072
	 * For an active leaf node, its inuse shouldn't be zero or exceed /Users/rubber/linux/block/blk-iocost.c: 1073
	 * @active. An active internal node's inuse is solely determined by the /Users/rubber/linux/block/blk-iocost.c: 1074
	 * inuse to active ratio of its children regardless of @inuse. /Users/rubber/linux/block/blk-iocost.c: 1075
		/* /Users/rubber/linux/block/blk-iocost.c: 1103
		 * The delta between inuse and active sums indicates that /Users/rubber/linux/block/blk-iocost.c: 1104
		 * much of weight is being given away.  Parent's inuse /Users/rubber/linux/block/blk-iocost.c: 1105
		 * and active should reflect the ratio. /Users/rubber/linux/block/blk-iocost.c: 1106
	/* /Users/rubber/linux/block/blk-iocost.c: 1158
	 * Paired with wmb in commit_weights(). If we saw the updated /Users/rubber/linux/block/blk-iocost.c: 1159
	 * hweight_gen, all the weight updates from __propagate_weights() are /Users/rubber/linux/block/blk-iocost.c: 1160
	 * visible too. /Users/rubber/linux/block/blk-iocost.c: 1161
	 * /Users/rubber/linux/block/blk-iocost.c: 1162
	 * We can race with weight updates during calculation and get it /Users/rubber/linux/block/blk-iocost.c: 1163
	 * wrong.  However, hweight_gen would have changed and a future /Users/rubber/linux/block/blk-iocost.c: 1164
	 * reader will recalculate and we're guaranteed to discard the /Users/rubber/linux/block/blk-iocost.c: 1165
	 * wrong result soon. /Users/rubber/linux/block/blk-iocost.c: 1166
 * Calculate the hweight_inuse @iocg would get with max @inuse assuming all the /Users/rubber/linux/block/blk-iocost.c: 1201
 * other weights stay unchanged. /Users/rubber/linux/block/blk-iocost.c: 1202
	/* /Users/rubber/linux/block/blk-iocost.c: 1248
	 * If seem to be already active, just update the stamp to tell the /Users/rubber/linux/block/blk-iocost.c: 1249
	 * timer that we're still active.  We don't mind occassional races. /Users/rubber/linux/block/blk-iocost.c: 1250
	/* /Users/rubber/linux/block/blk-iocost.c: 1283
	 * Always start with the target budget. On deactivation, we throw away /Users/rubber/linux/block/blk-iocost.c: 1284
	 * anything above it. /Users/rubber/linux/block/blk-iocost.c: 1285
	/* /Users/rubber/linux/block/blk-iocost.c: 1294
	 * Activate, propagate weight and start period timer if not /Users/rubber/linux/block/blk-iocost.c: 1295
	 * running.  Reset hweight_gen to avoid accidental match from /Users/rubber/linux/block/blk-iocost.c: 1296
	 * wrapping. /Users/rubber/linux/block/blk-iocost.c: 1297
	/* /Users/rubber/linux/block/blk-iocost.c: 1392
	 * Once in debt, debt handling owns inuse. @iocg stays at the minimum /Users/rubber/linux/block/blk-iocost.c: 1393
	 * inuse donating all of it share to others until its debt is paid off. /Users/rubber/linux/block/blk-iocost.c: 1394
	/* /Users/rubber/linux/block/blk-iocost.c: 1445
	 * autoremove_wake_function() removes the wait entry only when it /Users/rubber/linux/block/blk-iocost.c: 1446
	 * actually changed the task state. We want the wait always removed. /Users/rubber/linux/block/blk-iocost.c: 1447
	 * Remove explicitly and use default_wake_function(). Note that the /Users/rubber/linux/block/blk-iocost.c: 1448
	 * order of operations is important as finish_wait() tests whether /Users/rubber/linux/block/blk-iocost.c: 1449
	 * @wq_entry is removed without grabbing the lock. /Users/rubber/linux/block/blk-iocost.c: 1450
 * Calculate the accumulated budget, pay debt if @pay_debt and wake up waiters /Users/rubber/linux/block/blk-iocost.c: 1458
 * accordingly. When @pay_debt is %true, the caller must be holding ioc->lock in /Users/rubber/linux/block/blk-iocost.c: 1459
 * addition to iocg->waitq.lock. /Users/rubber/linux/block/blk-iocost.c: 1460
	/* /Users/rubber/linux/block/blk-iocost.c: 1493
	 * Debt can still be outstanding if we haven't paid all yet or the /Users/rubber/linux/block/blk-iocost.c: 1494
	 * caller raced and called without @pay_debt. Shouldn't wake up waiters /Users/rubber/linux/block/blk-iocost.c: 1495
	 * under debt. Make sure @vbudget reflects the outstanding amount and is /Users/rubber/linux/block/blk-iocost.c: 1496
	 * not positive. /Users/rubber/linux/block/blk-iocost.c: 1497
	/* /Users/rubber/linux/block/blk-iocost.c: 1504
	 * Wake up the ones which are due and see how much vtime we'll need for /Users/rubber/linux/block/blk-iocost.c: 1505
	 * the next one. As paying off debt restores hw_inuse, it must be read /Users/rubber/linux/block/blk-iocost.c: 1506
	 * after the above debt payment. /Users/rubber/linux/block/blk-iocost.c: 1507
 * Call this function on the target leaf @iocg's to build pre-order traversal /Users/rubber/linux/block/blk-iocost.c: 1618
 * list of all the ancestors in @inner_walk. The inner nodes are linked through /Users/rubber/linux/block/blk-iocost.c: 1619
 * ->walk_list and the caller is responsible for dissolving the list after use. /Users/rubber/linux/block/blk-iocost.c: 1620
 * Determine what @iocg's hweight_inuse should be after donating unused /Users/rubber/linux/block/blk-iocost.c: 1714
 * capacity. @hwm is the upper bound and used to signal no donation. This /Users/rubber/linux/block/blk-iocost.c: 1715
 * function also throws away @iocg's excess budget. /Users/rubber/linux/block/blk-iocost.c: 1716
	/* /Users/rubber/linux/block/blk-iocost.c: 1743
	 * Let's say the distance between iocg's and device's vtimes as a /Users/rubber/linux/block/blk-iocost.c: 1744
	 * fraction of period duration is delta. Assuming that the iocg will /Users/rubber/linux/block/blk-iocost.c: 1745
	 * consume the usage determined above, we want to determine new_hwi so /Users/rubber/linux/block/blk-iocost.c: 1746
	 * that delta equals MARGIN_TARGET at the end of the next period. /Users/rubber/linux/block/blk-iocost.c: 1747
	 * /Users/rubber/linux/block/blk-iocost.c: 1748
	 * We need to execute usage worth of IOs while spending the sum of the /Users/rubber/linux/block/blk-iocost.c: 1749
	 * new budget (1 - MARGIN_TARGET) and the leftover from the last period /Users/rubber/linux/block/blk-iocost.c: 1750
	 * (delta): /Users/rubber/linux/block/blk-iocost.c: 1751
	 * /Users/rubber/linux/block/blk-iocost.c: 1752
	 *   usage = (1 - MARGIN_TARGET + delta) * new_hwi /Users/rubber/linux/block/blk-iocost.c: 1753
	 * /Users/rubber/linux/block/blk-iocost.c: 1754
	 * Therefore, the new_hwi is: /Users/rubber/linux/block/blk-iocost.c: 1755
	 * /Users/rubber/linux/block/blk-iocost.c: 1756
	 *   new_hwi = usage / (1 - MARGIN_TARGET + delta) /Users/rubber/linux/block/blk-iocost.c: 1757
 * For work-conservation, an iocg which isn't using all of its share should /Users/rubber/linux/block/blk-iocost.c: 1768
 * donate the leftover to other iocgs. There are two ways to achieve this - 1. /Users/rubber/linux/block/blk-iocost.c: 1769
 * bumping up vrate accordingly 2. lowering the donating iocg's inuse weight. /Users/rubber/linux/block/blk-iocost.c: 1770
 * #1 is mathematically simpler but has the drawback of requiring synchronous /Users/rubber/linux/block/blk-iocost.c: 1772
 * global hweight_inuse updates when idle iocg's get activated or inuse weights /Users/rubber/linux/block/blk-iocost.c: 1773
 * change due to donation snapbacks as it has the possibility of grossly /Users/rubber/linux/block/blk-iocost.c: 1774
 * overshooting what's allowed by the model and vrate. /Users/rubber/linux/block/blk-iocost.c: 1775
 * #2 is inherently safe with local operations. The donating iocg can easily /Users/rubber/linux/block/blk-iocost.c: 1777
 * snap back to higher weights when needed without worrying about impacts on /Users/rubber/linux/block/blk-iocost.c: 1778
 * other nodes as the impacts will be inherently correct. This also makes idle /Users/rubber/linux/block/blk-iocost.c: 1779
 * iocg activations safe. The only effect activations have is decreasing /Users/rubber/linux/block/blk-iocost.c: 1780
 * hweight_inuse of others, the right solution to which is for those iocgs to /Users/rubber/linux/block/blk-iocost.c: 1781
 * snap back to higher weights. /Users/rubber/linux/block/blk-iocost.c: 1782
 * So, we go with #2. The challenge is calculating how each donating iocg's /Users/rubber/linux/block/blk-iocost.c: 1784
 * inuse should be adjusted to achieve the target donation amounts. This is done /Users/rubber/linux/block/blk-iocost.c: 1785
 * using Andy's method described in the following pdf. /Users/rubber/linux/block/blk-iocost.c: 1786
 *   https://drive.google.com/file/d/1PsJwxPFtjUnwOY1QJ5AeICCcsL7BM3bo /Users/rubber/linux/block/blk-iocost.c: 1788
 * Given the weights and target after-donation hweight_inuse values, Andy's /Users/rubber/linux/block/blk-iocost.c: 1790
 * method determines how the proportional distribution should look like at each /Users/rubber/linux/block/blk-iocost.c: 1791
 * sibling level to maintain the relative relationship between all non-donating /Users/rubber/linux/block/blk-iocost.c: 1792
 * pairs. To roughly summarize, it divides the tree into donating and /Users/rubber/linux/block/blk-iocost.c: 1793
 * non-donating parts, calculates global donation rate which is used to /Users/rubber/linux/block/blk-iocost.c: 1794
 * determine the target hweight_inuse for each node, and then derives per-level /Users/rubber/linux/block/blk-iocost.c: 1795
 * proportions. /Users/rubber/linux/block/blk-iocost.c: 1796
 * The following pdf shows that global distribution calculated this way can be /Users/rubber/linux/block/blk-iocost.c: 1798
 * achieved by scaling inuse weights of donating leaves and propagating the /Users/rubber/linux/block/blk-iocost.c: 1799
 * adjustments upwards proportionally. /Users/rubber/linux/block/blk-iocost.c: 1800
 *   https://drive.google.com/file/d/1vONz1-fzVO7oY5DXXsLjSxEtYYQbOvsE /Users/rubber/linux/block/blk-iocost.c: 1802
 * Combining the above two, we can determine how each leaf iocg's inuse should /Users/rubber/linux/block/blk-iocost.c: 1804
 * be adjusted to achieve the target donation. /Users/rubber/linux/block/blk-iocost.c: 1805
 *   https://drive.google.com/file/d/1WcrltBOSPN0qXVdBgnKm4mdp9FhuEFQN /Users/rubber/linux/block/blk-iocost.c: 1807
 * The inline comments use symbols from the last pdf. /Users/rubber/linux/block/blk-iocost.c: 1809
 *   b is the sum of the absolute budgets in the subtree. 1 for the root node. /Users/rubber/linux/block/blk-iocost.c: 1811
 *   f is the sum of the absolute budgets of non-donating nodes in the subtree. /Users/rubber/linux/block/blk-iocost.c: 1812
 *   t is the sum of the absolute budgets of donating nodes in the subtree. /Users/rubber/linux/block/blk-iocost.c: 1813
 *   w is the weight of the node. w = w_f + w_t /Users/rubber/linux/block/blk-iocost.c: 1814
 *   w_f is the non-donating portion of w. w_f = w * f / b /Users/rubber/linux/block/blk-iocost.c: 1815
 *   w_b is the donating portion of w. w_t = w * t / b /Users/rubber/linux/block/blk-iocost.c: 1816
 *   s is the sum of all sibling weights. s = Sum(w) for siblings /Users/rubber/linux/block/blk-iocost.c: 1817
 *   s_f and s_t are the non-donating and donating portions of s. /Users/rubber/linux/block/blk-iocost.c: 1818
 * Subscript p denotes the parent's counterpart and ' the adjusted value - e.g. /Users/rubber/linux/block/blk-iocost.c: 1820
 * w_pt is the donating portion of the parent's weight and w'_pt the same value /Users/rubber/linux/block/blk-iocost.c: 1821
 * after adjustments. Subscript r denotes the root node's values. /Users/rubber/linux/block/blk-iocost.c: 1822
	/* /Users/rubber/linux/block/blk-iocost.c: 1831
	 * It's pretty unlikely but possible for the total sum of /Users/rubber/linux/block/blk-iocost.c: 1832
	 * hweight_after_donation's to be higher than WEIGHT_ONE, which will /Users/rubber/linux/block/blk-iocost.c: 1833
	 * confuse the following calculations. If such condition is detected, /Users/rubber/linux/block/blk-iocost.c: 1834
	 * scale down everyone over its full share equally to keep the sum below /Users/rubber/linux/block/blk-iocost.c: 1835
	 * WEIGHT_ONE. /Users/rubber/linux/block/blk-iocost.c: 1836
		/* /Users/rubber/linux/block/blk-iocost.c: 1853
		 * The delta should be deducted from the over_sum, calculate /Users/rubber/linux/block/blk-iocost.c: 1854
		 * target over_sum value. /Users/rubber/linux/block/blk-iocost.c: 1855
	/* /Users/rubber/linux/block/blk-iocost.c: 1872
	 * Build pre-order inner node walk list and prepare for donation /Users/rubber/linux/block/blk-iocost.c: 1873
	 * adjustment calculations. /Users/rubber/linux/block/blk-iocost.c: 1874
	/* /Users/rubber/linux/block/blk-iocost.c: 1889
	 * Propagate the donating budget (b_t) and after donation budget (b'_t) /Users/rubber/linux/block/blk-iocost.c: 1890
	 * up the hierarchy. /Users/rubber/linux/block/blk-iocost.c: 1891
	/* /Users/rubber/linux/block/blk-iocost.c: 1909
	 * Calculate inner hwa's (b) and make sure the donation values are /Users/rubber/linux/block/blk-iocost.c: 1910
	 * within the accepted ranges as we're doing low res calculations with /Users/rubber/linux/block/blk-iocost.c: 1911
	 * roundups. /Users/rubber/linux/block/blk-iocost.c: 1912
	/* /Users/rubber/linux/block/blk-iocost.c: 1939
	 * Calculate the global donation rate (gamma) - the rate to adjust /Users/rubber/linux/block/blk-iocost.c: 1940
	 * non-donating budgets by. /Users/rubber/linux/block/blk-iocost.c: 1941
	 * /Users/rubber/linux/block/blk-iocost.c: 1942
	 * No need to use 64bit multiplication here as the first operand is /Users/rubber/linux/block/blk-iocost.c: 1943
	 * guaranteed to be smaller than WEIGHT_ONE (1<<16). /Users/rubber/linux/block/blk-iocost.c: 1944
	 * /Users/rubber/linux/block/blk-iocost.c: 1945
	 * We know that there are beneficiary nodes and the sum of the donating /Users/rubber/linux/block/blk-iocost.c: 1946
	 * hweights can't be whole; however, due to the round-ups during hweight /Users/rubber/linux/block/blk-iocost.c: 1947
	 * calculations, root_iocg->hweight_donating might still end up equal to /Users/rubber/linux/block/blk-iocost.c: 1948
	 * or greater than whole. Limit the range when calculating the divider. /Users/rubber/linux/block/blk-iocost.c: 1949
	 * /Users/rubber/linux/block/blk-iocost.c: 1950
	 * gamma = (1 - t_r') / (1 - t_r) /Users/rubber/linux/block/blk-iocost.c: 1951
	/* /Users/rubber/linux/block/blk-iocost.c: 1957
	 * Calculate adjusted hwi, child_adjusted_sum and inuse for the inner /Users/rubber/linux/block/blk-iocost.c: 1958
	 * nodes. /Users/rubber/linux/block/blk-iocost.c: 1959
	/* /Users/rubber/linux/block/blk-iocost.c: 2001
	 * All inner nodes now have ->hweight_inuse and ->child_adjusted_sum and /Users/rubber/linux/block/blk-iocost.c: 2002
	 * we can finally determine leaf adjustments. /Users/rubber/linux/block/blk-iocost.c: 2003
		/* /Users/rubber/linux/block/blk-iocost.c: 2009
		 * In-debt iocgs participated in the donation calculation with /Users/rubber/linux/block/blk-iocost.c: 2010
		 * the minimum target hweight_inuse. Configuring inuse /Users/rubber/linux/block/blk-iocost.c: 2011
		 * accordingly would work fine but debt handling expects /Users/rubber/linux/block/blk-iocost.c: 2012
		 * @iocg->inuse stay at the minimum and we don't wanna /Users/rubber/linux/block/blk-iocost.c: 2013
		 * interfere. /Users/rubber/linux/block/blk-iocost.c: 2014
 * A low weight iocg can amass a large amount of debt, for example, when /Users/rubber/linux/block/blk-iocost.c: 2040
 * anonymous memory gets reclaimed aggressively. If the system has a lot of /Users/rubber/linux/block/blk-iocost.c: 2041
 * memory paired with a slow IO device, the debt can span multiple seconds or /Users/rubber/linux/block/blk-iocost.c: 2042
 * more. If there are no other subsequent IO issuers, the in-debt iocg may end /Users/rubber/linux/block/blk-iocost.c: 2043
 * up blocked paying its debt while the IO device is idle. /Users/rubber/linux/block/blk-iocost.c: 2044
 * The following protects against such cases. If the device has been /Users/rubber/linux/block/blk-iocost.c: 2046
 * sufficiently idle for a while, the debts are halved and delays are /Users/rubber/linux/block/blk-iocost.c: 2047
 * recalculated. /Users/rubber/linux/block/blk-iocost.c: 2048
	/* /Users/rubber/linux/block/blk-iocost.c: 2064
	 * Debtors can pass through a lot of writes choking the device and we /Users/rubber/linux/block/blk-iocost.c: 2065
	 * don't want to be forgiving debts while the device is struggling from /Users/rubber/linux/block/blk-iocost.c: 2066
	 * write bursts. If we're missing latency targets, consider the device /Users/rubber/linux/block/blk-iocost.c: 2067
	 * fully utilized. /Users/rubber/linux/block/blk-iocost.c: 2068
	/* /Users/rubber/linux/block/blk-iocost.c: 2077
	 * At least DFGV_PERIOD has passed since the last period. Calculate the /Users/rubber/linux/block/blk-iocost.c: 2078
	 * average usage and reset the period counters. /Users/rubber/linux/block/blk-iocost.c: 2079
	/* /Users/rubber/linux/block/blk-iocost.c: 2093
	 * Usage is lower than threshold. Let's forgive some debts. Debt /Users/rubber/linux/block/blk-iocost.c: 2094
	 * forgiveness runs off of the usual ioc timer but its period usually /Users/rubber/linux/block/blk-iocost.c: 2095
	 * doesn't match ioc's. Compensate the difference by performing the /Users/rubber/linux/block/blk-iocost.c: 2096
	 * reduction as many times as would fit in the duration since the last /Users/rubber/linux/block/blk-iocost.c: 2097
	 * run and carrying over the left-over duration in @ioc->dfgv_period_rem /Users/rubber/linux/block/blk-iocost.c: 2098
	 * - if ioc period is 75% of DFGV_PERIOD, one out of three consecutive /Users/rubber/linux/block/blk-iocost.c: 2099
	 * reductions is doubled. /Users/rubber/linux/block/blk-iocost.c: 2100
 * Check the active iocgs' state to avoid oversleeping and deactive /Users/rubber/linux/block/blk-iocost.c: 2132
 * idle iocgs. /Users/rubber/linux/block/blk-iocost.c: 2133
 * Since waiters determine the sleep durations based on the vrate /Users/rubber/linux/block/blk-iocost.c: 2135
 * they saw at the time of sleep, if vrate has increased, some /Users/rubber/linux/block/blk-iocost.c: 2136
 * waiters could be sleeping for too long. Wake up tardy waiters /Users/rubber/linux/block/blk-iocost.c: 2137
 * which should have woken up in the last period and expire idle /Users/rubber/linux/block/blk-iocost.c: 2138
 * iocgs. /Users/rubber/linux/block/blk-iocost.c: 2139
			/* /Users/rubber/linux/block/blk-iocost.c: 2180
			 * @iocg has been inactive for a full duration and will /Users/rubber/linux/block/blk-iocost.c: 2181
			 * have a high budget. Account anything above target as /Users/rubber/linux/block/blk-iocost.c: 2182
			 * error and throw away. On reactivation, it'll start /Users/rubber/linux/block/blk-iocost.c: 2183
			 * with the target budget. /Users/rubber/linux/block/blk-iocost.c: 2184
	/* /Users/rubber/linux/block/blk-iocost.c: 2239
	 * Wait and indebt stat are flushed above and the donation calculation /Users/rubber/linux/block/blk-iocost.c: 2240
	 * below needs updated usage stat. Let's bring stat up-to-date. /Users/rubber/linux/block/blk-iocost.c: 2241
		/* /Users/rubber/linux/block/blk-iocost.c: 2250
		 * Collect unused and wind vtime closer to vnow to prevent /Users/rubber/linux/block/blk-iocost.c: 2251
		 * iocgs from accumulating a large amount of budget. /Users/rubber/linux/block/blk-iocost.c: 2252
		/* /Users/rubber/linux/block/blk-iocost.c: 2258
		 * Latency QoS detection doesn't account for IOs which are /Users/rubber/linux/block/blk-iocost.c: 2259
		 * in-flight for longer than a period.  Detect them by /Users/rubber/linux/block/blk-iocost.c: 2260
		 * comparing vdone against period start.  If lagging behind /Users/rubber/linux/block/blk-iocost.c: 2261
		 * IOs from past periods, don't increase vrate. /Users/rubber/linux/block/blk-iocost.c: 2262
		/* /Users/rubber/linux/block/blk-iocost.c: 2272
		 * Determine absolute usage factoring in in-flight IOs to avoid /Users/rubber/linux/block/blk-iocost.c: 2273
		 * high-latency completions appearing as idle. /Users/rubber/linux/block/blk-iocost.c: 2274
			/* /Users/rubber/linux/block/blk-iocost.c: 2306
			 * Already donating or accumulated enough to start. /Users/rubber/linux/block/blk-iocost.c: 2307
			 * Determine the donation amount. /Users/rubber/linux/block/blk-iocost.c: 2308
	/* /Users/rubber/linux/block/blk-iocost.c: 2342
	 * If q is getting clogged or we're missing too much, we're issuing /Users/rubber/linux/block/blk-iocost.c: 2343
	 * too much IO and should lower vtime rate.  If we're not missing /Users/rubber/linux/block/blk-iocost.c: 2344
	 * and experiencing shortages but not surpluses, we're too stingy /Users/rubber/linux/block/blk-iocost.c: 2345
	 * and should increase vtime rate. /Users/rubber/linux/block/blk-iocost.c: 2346
			/* /Users/rubber/linux/block/blk-iocost.c: 2360
			 * We're throttling while the device has spare /Users/rubber/linux/block/blk-iocost.c: 2361
			 * capacity.  If vrate was being slowed down, stop. /Users/rubber/linux/block/blk-iocost.c: 2362
			/* /Users/rubber/linux/block/blk-iocost.c: 2366
			 * If there are IOs spanning multiple periods, wait /Users/rubber/linux/block/blk-iocost.c: 2367
			 * them out before pushing the device harder. /Users/rubber/linux/block/blk-iocost.c: 2368
			/* /Users/rubber/linux/block/blk-iocost.c: 2373
			 * Nobody is being throttled and the users aren't /Users/rubber/linux/block/blk-iocost.c: 2374
			 * issuing enough IOs to saturate the device.  We /Users/rubber/linux/block/blk-iocost.c: 2375
			 * simply don't know how close the device is to /Users/rubber/linux/block/blk-iocost.c: 2376
			 * saturation.  Coast. /Users/rubber/linux/block/blk-iocost.c: 2377
	/* /Users/rubber/linux/block/blk-iocost.c: 2395
	 * This period is done.  Move onto the next one.  If nothing's /Users/rubber/linux/block/blk-iocost.c: 2396
	 * going on with the device, stop the timer. /Users/rubber/linux/block/blk-iocost.c: 2397
	/* /Users/rubber/linux/block/blk-iocost.c: 2435
	 * We only increase inuse during period and do so if the margin has /Users/rubber/linux/block/blk-iocost.c: 2436
	 * deteriorated since the previous adjustment. /Users/rubber/linux/block/blk-iocost.c: 2437
	/* /Users/rubber/linux/block/blk-iocost.c: 2451
	 * Bump up inuse till @abs_cost fits in the existing budget. /Users/rubber/linux/block/blk-iocost.c: 2452
	 * adj_step must be determined after acquiring ioc->lock - we might /Users/rubber/linux/block/blk-iocost.c: 2453
	 * have raced and lost to another thread for activation and could /Users/rubber/linux/block/blk-iocost.c: 2454
	 * be reading 0 iocg->active before ioc->lock which will lead to /Users/rubber/linux/block/blk-iocost.c: 2455
	 * infinite loop. /Users/rubber/linux/block/blk-iocost.c: 2456
	/* /Users/rubber/linux/block/blk-iocost.c: 2577
	 * If no one's waiting and within budget, issue right away.  The /Users/rubber/linux/block/blk-iocost.c: 2578
	 * tests are racy but the races aren't systemic - we only miss once /Users/rubber/linux/block/blk-iocost.c: 2579
	 * in a while which is fine. /Users/rubber/linux/block/blk-iocost.c: 2580
	/* /Users/rubber/linux/block/blk-iocost.c: 2588
	 * We're over budget. This can be handled in two ways. IOs which may /Users/rubber/linux/block/blk-iocost.c: 2589
	 * cause priority inversions are punted to @ioc->aux_iocg and charged as /Users/rubber/linux/block/blk-iocost.c: 2590
	 * debt. Otherwise, the issuer is blocked on @iocg->waitq. Debt handling /Users/rubber/linux/block/blk-iocost.c: 2591
	 * requires @ioc->lock, waitq handling @iocg->waitq.lock. Determine /Users/rubber/linux/block/blk-iocost.c: 2592
	 * whether debt handling is needed and acquire locks accordingly. /Users/rubber/linux/block/blk-iocost.c: 2593
	/* /Users/rubber/linux/block/blk-iocost.c: 2600
	 * @iocg must stay activated for debt and waitq handling. Deactivation /Users/rubber/linux/block/blk-iocost.c: 2601
	 * is synchronized against both ioc->lock and waitq.lock and we won't /Users/rubber/linux/block/blk-iocost.c: 2602
	 * get deactivated as long as we're waiting or has debt, so we're good /Users/rubber/linux/block/blk-iocost.c: 2603
	 * if we're activated here. In the unlikely cases that we aren't, just /Users/rubber/linux/block/blk-iocost.c: 2604
	 * issue the IO. /Users/rubber/linux/block/blk-iocost.c: 2605
	/* /Users/rubber/linux/block/blk-iocost.c: 2613
	 * We're over budget. If @bio has to be issued regardless, remember /Users/rubber/linux/block/blk-iocost.c: 2614
	 * the abs_cost instead of advancing vtime. iocg_kick_waitq() will pay /Users/rubber/linux/block/blk-iocost.c: 2615
	 * off the debt before waking more IOs. /Users/rubber/linux/block/blk-iocost.c: 2616
	 * /Users/rubber/linux/block/blk-iocost.c: 2617
	 * This way, the debt is continuously paid off each period with the /Users/rubber/linux/block/blk-iocost.c: 2618
	 * actual budget available to the cgroup. If we just wound vtime, we /Users/rubber/linux/block/blk-iocost.c: 2619
	 * would incorrectly use the current hw_inuse for the entire amount /Users/rubber/linux/block/blk-iocost.c: 2620
	 * which, for example, can lead to the cgroup staying blocked for a /Users/rubber/linux/block/blk-iocost.c: 2621
	 * long time even with substantially raised hw_inuse. /Users/rubber/linux/block/blk-iocost.c: 2622
	 * /Users/rubber/linux/block/blk-iocost.c: 2623
	 * An iocg with vdebt should stay online so that the timer can keep /Users/rubber/linux/block/blk-iocost.c: 2624
	 * deducting its vdebt and [de]activate use_delay mechanism /Users/rubber/linux/block/blk-iocost.c: 2625
	 * accordingly. We don't want to race against the timer trying to /Users/rubber/linux/block/blk-iocost.c: 2626
	 * clear them and leave @iocg inactive w/ dangling use_delay heavily /Users/rubber/linux/block/blk-iocost.c: 2627
	 * penalizing the cgroup and its descendants. /Users/rubber/linux/block/blk-iocost.c: 2628
	/* /Users/rubber/linux/block/blk-iocost.c: 2650
	 * Append self to the waitq and schedule the wakeup timer if we're /Users/rubber/linux/block/blk-iocost.c: 2651
	 * the first waiter.  The timer duration is calculated based on the /Users/rubber/linux/block/blk-iocost.c: 2652
	 * current vrate.  vtime and hweight changes can make it too short /Users/rubber/linux/block/blk-iocost.c: 2653
	 * or too long.  Each wait entry records the absolute cost it's /Users/rubber/linux/block/blk-iocost.c: 2654
	 * waiting for to allow re-evaluation using a custom wait entry. /Users/rubber/linux/block/blk-iocost.c: 2655
	 * /Users/rubber/linux/block/blk-iocost.c: 2656
	 * If too short, the timer simply reschedules itself.  If too long, /Users/rubber/linux/block/blk-iocost.c: 2657
	 * the period timer will notice and trigger wakeups. /Users/rubber/linux/block/blk-iocost.c: 2658
	 * /Users/rubber/linux/block/blk-iocost.c: 2659
	 * All waiters are on iocg->waitq and the wait states are /Users/rubber/linux/block/blk-iocost.c: 2660
	 * synchronized using waitq.lock. /Users/rubber/linux/block/blk-iocost.c: 2661
	/* /Users/rubber/linux/block/blk-iocost.c: 2713
	 * Charge if there's enough vtime budget and the existing request has /Users/rubber/linux/block/blk-iocost.c: 2714
	 * cost assigned. /Users/rubber/linux/block/blk-iocost.c: 2715
	/* /Users/rubber/linux/block/blk-iocost.c: 2723
	 * Otherwise, account it as debt if @iocg is online, which it should /Users/rubber/linux/block/blk-iocost.c: 2724
	 * be for the vast majority of cases. See debt handling in /Users/rubber/linux/block/blk-iocost.c: 2725
	 * ioc_rqos_throttle() for details. /Users/rubber/linux/block/blk-iocost.c: 2726
	/* /Users/rubber/linux/block/blk-iocost.c: 2873
	 * rqos must be added before activation to allow iocg_pd_init() to /Users/rubber/linux/block/blk-iocost.c: 2874
	 * lookup the ioc from q. This means that the rqos methods may get /Users/rubber/linux/block/blk-iocost.c: 2875
	 * called before policy activation completion, can't assume that the /Users/rubber/linux/block/blk-iocost.c: 2876
	 * target bio has an iocg associated and need to test for NULL iocg. /Users/rubber/linux/block/blk-iocost.c: 2877
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/kyber-iosched.c: 1
 * The Kyber I/O scheduler. Controls latency by throttling queue depths using /Users/rubber/linux/block/kyber-iosched.c: 3
 * scalable techniques. /Users/rubber/linux/block/kyber-iosched.c: 4
 * Copyright (C) 2017 Facebook /Users/rubber/linux/block/kyber-iosched.c: 6
 * Scheduling domains: the device is divided into multiple domains based on the /Users/rubber/linux/block/kyber-iosched.c: 28
 * request type. /Users/rubber/linux/block/kyber-iosched.c: 29
	/* /Users/rubber/linux/block/kyber-iosched.c: 47
	 * In order to prevent starvation of synchronous requests by a flood of /Users/rubber/linux/block/kyber-iosched.c: 48
	 * asynchronous requests, we reserve 25% of requests for synchronous /Users/rubber/linux/block/kyber-iosched.c: 49
	 * operations. /Users/rubber/linux/block/kyber-iosched.c: 50
 * Maximum device-wide depth for each scheduling domain. /Users/rubber/linux/block/kyber-iosched.c: 56
 * Even for fast devices with lots of tags like NVMe, you can saturate the /Users/rubber/linux/block/kyber-iosched.c: 58
 * device with only a fraction of the maximum possible queue depth. So, we cap /Users/rubber/linux/block/kyber-iosched.c: 59
 * these to a reasonable value. /Users/rubber/linux/block/kyber-iosched.c: 60
 * Default latency targets for each scheduling domain. /Users/rubber/linux/block/kyber-iosched.c: 70
 * Batch size (number of requests we'll dispatch in a row) for each scheduling /Users/rubber/linux/block/kyber-iosched.c: 79
 * domain. /Users/rubber/linux/block/kyber-iosched.c: 80
 * Requests latencies are recorded in a histogram with buckets defined relative /Users/rubber/linux/block/kyber-iosched.c: 90
 * to the target latency: /Users/rubber/linux/block/kyber-iosched.c: 91
 * <= 1/4 * target latency /Users/rubber/linux/block/kyber-iosched.c: 93
 * <= 1/2 * target latency /Users/rubber/linux/block/kyber-iosched.c: 94
 * <= 3/4 * target latency /Users/rubber/linux/block/kyber-iosched.c: 95
 * <= target latency /Users/rubber/linux/block/kyber-iosched.c: 96
 * <= 1 1/4 * target latency /Users/rubber/linux/block/kyber-iosched.c: 97
 * <= 1 1/2 * target latency /Users/rubber/linux/block/kyber-iosched.c: 98
 * <= 1 3/4 * target latency /Users/rubber/linux/block/kyber-iosched.c: 99
 * > 1 3/4 * target latency /Users/rubber/linux/block/kyber-iosched.c: 100
	/* /Users/rubber/linux/block/kyber-iosched.c: 103
	 * The width of the latency histogram buckets is /Users/rubber/linux/block/kyber-iosched.c: 104
	 * 1 / (1 << KYBER_LATENCY_SHIFT) * target latency. /Users/rubber/linux/block/kyber-iosched.c: 105
	/* /Users/rubber/linux/block/kyber-iosched.c: 108
	 * The first (1 << KYBER_LATENCY_SHIFT) buckets are <= target latency, /Users/rubber/linux/block/kyber-iosched.c: 109
	 * thus, "good". /Users/rubber/linux/block/kyber-iosched.c: 110
 * We measure both the total latency and the I/O latency (i.e., latency after /Users/rubber/linux/block/kyber-iosched.c: 118
 * submitting to the device). /Users/rubber/linux/block/kyber-iosched.c: 119
 * Per-cpu latency histograms: total latency and I/O latency for each scheduling /Users/rubber/linux/block/kyber-iosched.c: 132
 * domain except for KYBER_OTHER. /Users/rubber/linux/block/kyber-iosched.c: 133
 * There is a same mapping between ctx & hctx and kcq & khd, /Users/rubber/linux/block/kyber-iosched.c: 140
 * we use request->mq_ctx->index_hw to index the kcq in khd. /Users/rubber/linux/block/kyber-iosched.c: 141
	/* /Users/rubber/linux/block/kyber-iosched.c: 144
	 * Used to ensure operations on rq_list and kcq_map to be an atmoic one. /Users/rubber/linux/block/kyber-iosched.c: 145
	 * Also protect the rqs on rq_list when merge. /Users/rubber/linux/block/kyber-iosched.c: 146
	/* /Users/rubber/linux/block/kyber-iosched.c: 156
	 * Each scheduling domain has a limited number of in-flight requests /Users/rubber/linux/block/kyber-iosched.c: 157
	 * device-wide, limited by these tokens. /Users/rubber/linux/block/kyber-iosched.c: 158
	/* /Users/rubber/linux/block/kyber-iosched.c: 162
	 * Async request percentage, converted to per-word depth for /Users/rubber/linux/block/kyber-iosched.c: 163
	 * sbitmap_get_shallow(). /Users/rubber/linux/block/kyber-iosched.c: 164
 * Calculate the histogram bucket with the given percentile rank, or -1 if there /Users/rubber/linux/block/kyber-iosched.c: 225
 * aren't enough samples yet. /Users/rubber/linux/block/kyber-iosched.c: 226
	/* /Users/rubber/linux/block/kyber-iosched.c: 241
	 * We do the calculation once we have 500 samples or one second passes /Users/rubber/linux/block/kyber-iosched.c: 242
	 * since the first sample was recorded, whichever comes first. /Users/rubber/linux/block/kyber-iosched.c: 243
	/* /Users/rubber/linux/block/kyber-iosched.c: 299
	 * Check if any domains have a high I/O latency, which might indicate /Users/rubber/linux/block/kyber-iosched.c: 300
	 * congestion in the device. Note that we use the p90; we don't want to /Users/rubber/linux/block/kyber-iosched.c: 301
	 * be too sensitive to outliers here. /Users/rubber/linux/block/kyber-iosched.c: 302
	/* /Users/rubber/linux/block/kyber-iosched.c: 313
	 * Adjust the scheduling domain depths. If we determined that there was /Users/rubber/linux/block/kyber-iosched.c: 314
	 * congestion, we throttle all domains with good latencies. Either way, /Users/rubber/linux/block/kyber-iosched.c: 315
	 * we ease up on throttling domains with bad latencies. /Users/rubber/linux/block/kyber-iosched.c: 316
		/* /Users/rubber/linux/block/kyber-iosched.c: 324
		 * This is kind of subtle: different domains will not /Users/rubber/linux/block/kyber-iosched.c: 325
		 * necessarily have enough samples to calculate the latency /Users/rubber/linux/block/kyber-iosched.c: 326
		 * percentiles during the same window, so we have to remember /Users/rubber/linux/block/kyber-iosched.c: 327
		 * the p99 for the next time we observe congestion; once we do, /Users/rubber/linux/block/kyber-iosched.c: 328
		 * we don't want to throttle again until we get more data, so we /Users/rubber/linux/block/kyber-iosched.c: 329
		 * reset it to -1. /Users/rubber/linux/block/kyber-iosched.c: 330
		/* /Users/rubber/linux/block/kyber-iosched.c: 342
		 * If this domain has bad latency, throttle less. Otherwise, /Users/rubber/linux/block/kyber-iosched.c: 343
		 * throttle more iff we determined that there is congestion. /Users/rubber/linux/block/kyber-iosched.c: 344
		 * /Users/rubber/linux/block/kyber-iosched.c: 345
		 * The new depth is scaled linearly with the p99 latency vs the /Users/rubber/linux/block/kyber-iosched.c: 346
		 * latency target. E.g., if the p99 is 3/4 of the target, then /Users/rubber/linux/block/kyber-iosched.c: 347
		 * we throttle down to 3/4 of the current depth, and if the p99 /Users/rubber/linux/block/kyber-iosched.c: 348
		 * is 2x the target, then we double the depth. /Users/rubber/linux/block/kyber-iosched.c: 349
	/* /Users/rubber/linux/block/kyber-iosched.c: 555
	 * We use the scheduler tags as per-hardware queue queueing tokens. /Users/rubber/linux/block/kyber-iosched.c: 556
	 * Async requests can be limited at this stage. /Users/rubber/linux/block/kyber-iosched.c: 557
	/* /Users/rubber/linux/block/kyber-iosched.c: 716
	 * If we failed to get a domain token, make sure the hardware queue is /Users/rubber/linux/block/kyber-iosched.c: 717
	 * run when one becomes available. Note that this is serialized on /Users/rubber/linux/block/kyber-iosched.c: 718
	 * khd->lock, but we still need to be careful about the waker. /Users/rubber/linux/block/kyber-iosched.c: 719
		/* /Users/rubber/linux/block/kyber-iosched.c: 727
		 * Try again in case a token was freed before we got on the wait /Users/rubber/linux/block/kyber-iosched.c: 728
		 * queue. /Users/rubber/linux/block/kyber-iosched.c: 729
	/* /Users/rubber/linux/block/kyber-iosched.c: 734
	 * If we got a token while we were on the wait queue, remove ourselves /Users/rubber/linux/block/kyber-iosched.c: 735
	 * from the wait queue to ensure that all wake ups make forward /Users/rubber/linux/block/kyber-iosched.c: 736
	 * progress. It's possible that the waker already deleted the entry /Users/rubber/linux/block/kyber-iosched.c: 737
	 * between the !list_empty_careful() check and us grabbing the lock, but /Users/rubber/linux/block/kyber-iosched.c: 738
	 * list_del_init() is okay with that. /Users/rubber/linux/block/kyber-iosched.c: 739
	/* /Users/rubber/linux/block/kyber-iosched.c: 762
	 * If we already have a flushed request, then we just need to get a /Users/rubber/linux/block/kyber-iosched.c: 763
	 * token for it. Otherwise, if there are pending requests in the kcqs, /Users/rubber/linux/block/kyber-iosched.c: 764
	 * flush the kcqs, but only if we can get a token. If not, we should /Users/rubber/linux/block/kyber-iosched.c: 765
	 * leave the requests in the kcqs so that they can be merged. Note that /Users/rubber/linux/block/kyber-iosched.c: 766
	 * khd->lock serializes the flushes, so if we observed any bit set in /Users/rubber/linux/block/kyber-iosched.c: 767
	 * the kcq_map, we will always get a request. /Users/rubber/linux/block/kyber-iosched.c: 768
	/* /Users/rubber/linux/block/kyber-iosched.c: 810
	 * First, if we are still entitled to batch, try to dispatch a request /Users/rubber/linux/block/kyber-iosched.c: 811
	 * from the batch. /Users/rubber/linux/block/kyber-iosched.c: 812
	/* /Users/rubber/linux/block/kyber-iosched.c: 820
	 * Either, /Users/rubber/linux/block/kyber-iosched.c: 821
	 * 1. We were no longer entitled to a batch. /Users/rubber/linux/block/kyber-iosched.c: 822
	 * 2. The domain we were batching didn't have any requests. /Users/rubber/linux/block/kyber-iosched.c: 823
	 * 3. The domain we were batching was out of tokens. /Users/rubber/linux/block/kyber-iosched.c: 824
	 * /Users/rubber/linux/block/kyber-iosched.c: 825
	 * Start another batch. Note that this wraps back around to the original /Users/rubber/linux/block/kyber-iosched.c: 826
	 * domain if no other domains have requests or tokens. /Users/rubber/linux/block/kyber-iosched.c: 827
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq.c: 1
 * Block multiqueue core code /Users/rubber/linux/block/blk-mq.c: 3
 * Copyright (C) 2013-2014 Jens Axboe /Users/rubber/linux/block/blk-mq.c: 5
 * Copyright (C) 2013-2014 Christoph Hellwig /Users/rubber/linux/block/blk-mq.c: 6
 * Check if any of the ctx, dispatch list or elevator /Users/rubber/linux/block/blk-mq.c: 94
 * have pending work in this hardware queue. /Users/rubber/linux/block/blk-mq.c: 95
 * Mark this ctx as having pending work in this hardware queue /Users/rubber/linux/block/blk-mq.c: 105
 * Guarantee no request is in use, so we can change any data structure of /Users/rubber/linux/block/blk-mq.c: 192
 * the queue afterward. /Users/rubber/linux/block/blk-mq.c: 193
	/* /Users/rubber/linux/block/blk-mq.c: 197
	 * In the !blk_mq case we are only calling this to kill the /Users/rubber/linux/block/blk-mq.c: 198
	 * q_usage_counter, otherwise this increases the freeze depth /Users/rubber/linux/block/blk-mq.c: 199
	 * and waits for it to return to zero.  For this reason there is /Users/rubber/linux/block/blk-mq.c: 200
	 * no blk_unfreeze_queue(), and blk_freeze_queue() is not /Users/rubber/linux/block/blk-mq.c: 201
	 * exported to drivers as the only user for unfreeze is blk_mq. /Users/rubber/linux/block/blk-mq.c: 202
	/* /Users/rubber/linux/block/blk-mq.c: 210
	 * ...just an alias to keep freeze and unfreeze actions balanced /Users/rubber/linux/block/blk-mq.c: 211
	 * in the blk_mq_* namespace /Users/rubber/linux/block/blk-mq.c: 212
 * FIXME: replace the scsi_internal_device_*block_nowait() calls in the /Users/rubber/linux/block/blk-mq.c: 239
 * mpt3sas driver such that this function can be removed. /Users/rubber/linux/block/blk-mq.c: 240
 * blk_mq_wait_quiesce_done() - wait until in-progress quiesce is done /Users/rubber/linux/block/blk-mq.c: 254
 * @q: request queue. /Users/rubber/linux/block/blk-mq.c: 255
 * Note: it is driver's responsibility for making sure that quiesce has /Users/rubber/linux/block/blk-mq.c: 257
 * been started. /Users/rubber/linux/block/blk-mq.c: 258
 * blk_mq_quiesce_queue() - wait until all ongoing dispatches have finished /Users/rubber/linux/block/blk-mq.c: 278
 * @q: request queue. /Users/rubber/linux/block/blk-mq.c: 279
 * Note: this function does not prevent that the struct request end_io() /Users/rubber/linux/block/blk-mq.c: 281
 * callback function is invoked. Once this function is returned, we make /Users/rubber/linux/block/blk-mq.c: 282
 * sure no dispatch can happen until the queue is unquiesced via /Users/rubber/linux/block/blk-mq.c: 283
 * blk_mq_unquiesce_queue(). /Users/rubber/linux/block/blk-mq.c: 284
 * blk_mq_unquiesce_queue() - counterpart of blk_mq_quiesce_queue() /Users/rubber/linux/block/blk-mq.c: 294
 * @q: request queue. /Users/rubber/linux/block/blk-mq.c: 295
 * This function recovers queue into the state before quiescing /Users/rubber/linux/block/blk-mq.c: 297
 * which is done by blk_mq_quiesce_queue. /Users/rubber/linux/block/blk-mq.c: 298
		/* /Users/rubber/linux/block/blk-mq.c: 453
		 * Flush/passthrough requests are special and go directly to the /Users/rubber/linux/block/blk-mq.c: 454
		 * dispatch list. Don't include reserved tags in the /Users/rubber/linux/block/blk-mq.c: 455
		 * limiting, as it isn't useful. /Users/rubber/linux/block/blk-mq.c: 456
	/* /Users/rubber/linux/block/blk-mq.c: 471
	 * Try batched alloc if we want more than 1 tag. /Users/rubber/linux/block/blk-mq.c: 472
	/* /Users/rubber/linux/block/blk-mq.c: 481
	 * Waiting allocations only fail because of an inactive hctx.  In that /Users/rubber/linux/block/blk-mq.c: 482
	 * case just retry the hctx assignment and tag allocation as CPU hotplug /Users/rubber/linux/block/blk-mq.c: 483
	 * should have migrated us to an online CPU by now. /Users/rubber/linux/block/blk-mq.c: 484
		/* /Users/rubber/linux/block/blk-mq.c: 490
		 * Give up the CPU and sleep for a random short time to /Users/rubber/linux/block/blk-mq.c: 491
		 * ensure that thread using a realtime scheduling class /Users/rubber/linux/block/blk-mq.c: 492
		 * are migrated off the CPU, and thus off the hctx that /Users/rubber/linux/block/blk-mq.c: 493
		 * is going away. /Users/rubber/linux/block/blk-mq.c: 494
	/* /Users/rubber/linux/block/blk-mq.c: 551
	 * If the tag allocator sleeps we could get an allocation for a /Users/rubber/linux/block/blk-mq.c: 552
	 * different hardware context.  No need to complicate the low level /Users/rubber/linux/block/blk-mq.c: 553
	 * allocator for this for the rare use case of a command tied to /Users/rubber/linux/block/blk-mq.c: 554
	 * a specific queue. /Users/rubber/linux/block/blk-mq.c: 555
	/* /Users/rubber/linux/block/blk-mq.c: 567
	 * Check if the hardware context is actually mapped to anything. /Users/rubber/linux/block/blk-mq.c: 568
	 * If not tell the caller that it should skip this queue. /Users/rubber/linux/block/blk-mq.c: 569
		/* /Users/rubber/linux/block/blk-mq.c: 658
		 * Partial zone append completions cannot be supported as the /Users/rubber/linux/block/blk-mq.c: 659
		 * BIO fragments may end up not being written sequentially. /Users/rubber/linux/block/blk-mq.c: 660
 * blk_update_request - Complete multiple bytes without completing the request /Users/rubber/linux/block/blk-mq.c: 689
 * @req:      the request being processed /Users/rubber/linux/block/blk-mq.c: 690
 * @error:    block status code /Users/rubber/linux/block/blk-mq.c: 691
 * @nr_bytes: number of bytes to complete for @req /Users/rubber/linux/block/blk-mq.c: 692
 * Description: /Users/rubber/linux/block/blk-mq.c: 694
 *     Ends I/O on a number of bytes attached to @req, but doesn't complete /Users/rubber/linux/block/blk-mq.c: 695
 *     the request structure even if @req doesn't have leftover. /Users/rubber/linux/block/blk-mq.c: 696
 *     If @req has leftover, sets it up for the next range of segments. /Users/rubber/linux/block/blk-mq.c: 697
 *     Passing the result of blk_rq_bytes() as @nr_bytes guarantees /Users/rubber/linux/block/blk-mq.c: 699
 *     %false return from this function. /Users/rubber/linux/block/blk-mq.c: 700
 * Note: /Users/rubber/linux/block/blk-mq.c: 702
 *	The RQF_SPECIAL_PAYLOAD flag is ignored on purpose in this function /Users/rubber/linux/block/blk-mq.c: 703
 *      except in the consistency check at the end of this function. /Users/rubber/linux/block/blk-mq.c: 704
 * Return: /Users/rubber/linux/block/blk-mq.c: 706
 *     %false - this request doesn't have any more data /Users/rubber/linux/block/blk-mq.c: 707
 *     %true  - this request has more data /Users/rubber/linux/block/blk-mq.c: 708
	/* /Users/rubber/linux/block/blk-mq.c: 751
	 * completely done /Users/rubber/linux/block/blk-mq.c: 752
		/* /Users/rubber/linux/block/blk-mq.c: 755
		 * Reset counters so that the request stacking driver /Users/rubber/linux/block/blk-mq.c: 756
		 * can find how many bytes remain in the request /Users/rubber/linux/block/blk-mq.c: 757
		 * later. /Users/rubber/linux/block/blk-mq.c: 758
		/* /Users/rubber/linux/block/blk-mq.c: 777
		 * If total number of sectors is less than the first segment /Users/rubber/linux/block/blk-mq.c: 778
		 * size, something has gone terribly wrong. /Users/rubber/linux/block/blk-mq.c: 779
	/* /Users/rubber/linux/block/blk-mq.c: 834
	 * All requests should have been marked as RQF_MQ_INFLIGHT, so /Users/rubber/linux/block/blk-mq.c: 835
	 * update hctx->nr_active in batch /Users/rubber/linux/block/blk-mq.c: 836
	/* /Users/rubber/linux/block/blk-mq.c: 918
	 * With force threaded interrupts enabled, raising softirq from an SMP /Users/rubber/linux/block/blk-mq.c: 919
	 * function call will always result in waking the ksoftirqd thread. /Users/rubber/linux/block/blk-mq.c: 920
	 * This is probably worse than completing the request on a different /Users/rubber/linux/block/blk-mq.c: 921
	 * cache domain. /Users/rubber/linux/block/blk-mq.c: 922
	/* /Users/rubber/linux/block/blk-mq.c: 965
	 * For a polled request, always complete locallly, it's pointless /Users/rubber/linux/block/blk-mq.c: 966
	 * to redirect the completion. /Users/rubber/linux/block/blk-mq.c: 967
 * blk_mq_complete_request - end I/O on a request /Users/rubber/linux/block/blk-mq.c: 986
 * @rq:		the request being processed /Users/rubber/linux/block/blk-mq.c: 987
 * Description: /Users/rubber/linux/block/blk-mq.c: 989
 *	Complete a request by scheduling the ->complete_rq operation. /Users/rubber/linux/block/blk-mq.c: 990
 * blk_mq_start_request - Start processing a request /Users/rubber/linux/block/blk-mq.c: 1020
 * @rq: Pointer to request to be started /Users/rubber/linux/block/blk-mq.c: 1021
 * Function used by device drivers to notify the block layer that a request /Users/rubber/linux/block/blk-mq.c: 1023
 * is going to be processed now, so blk layer can do proper initializations /Users/rubber/linux/block/blk-mq.c: 1024
 * such as starting the timeout timer. /Users/rubber/linux/block/blk-mq.c: 1025
		/* /Users/rubber/linux/block/blk-mq.c: 1104
		 * If RQF_DONTPREP, rq has contained some driver specific /Users/rubber/linux/block/blk-mq.c: 1105
		 * data, so insert it to hctx dispatch list to avoid any /Users/rubber/linux/block/blk-mq.c: 1106
		 * merge. /Users/rubber/linux/block/blk-mq.c: 1107
	/* /Users/rubber/linux/block/blk-mq.c: 1130
	 * We abuse this flag that is otherwise used by the I/O scheduler to /Users/rubber/linux/block/blk-mq.c: 1131
	 * request head insertion from the workqueue. /Users/rubber/linux/block/blk-mq.c: 1132
	/* /Users/rubber/linux/block/blk-mq.c: 1166
	 * If we find a request that isn't idle and the queue matches, /Users/rubber/linux/block/blk-mq.c: 1167
	 * we know the queue is busy. Return false to stop the iteration. /Users/rubber/linux/block/blk-mq.c: 1168
	/* /Users/rubber/linux/block/blk-mq.c: 1237
	 * blk_mq_queue_tag_busy_iter() has locked the request, so it cannot /Users/rubber/linux/block/blk-mq.c: 1238
	 * be reallocated underneath the timeout handler's processing, then /Users/rubber/linux/block/blk-mq.c: 1239
	 * the expire check is reliable. If the request is not expired, then /Users/rubber/linux/block/blk-mq.c: 1240
	 * it was completed and reallocated as a new request after returning /Users/rubber/linux/block/blk-mq.c: 1241
	 * from blk_mq_check_expired(). /Users/rubber/linux/block/blk-mq.c: 1242
	/* A deadlock might occur if a request is stuck requiring a /Users/rubber/linux/block/blk-mq.c: 1257
	 * timeout at the same time a queue freeze is waiting /Users/rubber/linux/block/blk-mq.c: 1258
	 * completion, since the timeout code would not be able to /Users/rubber/linux/block/blk-mq.c: 1259
	 * acquire the queue reference here. /Users/rubber/linux/block/blk-mq.c: 1260
	 * /Users/rubber/linux/block/blk-mq.c: 1261
	 * That's why we don't use blk_queue_enter here; instead, we use /Users/rubber/linux/block/blk-mq.c: 1262
	 * percpu_ref_tryget directly, because we need to be able to /Users/rubber/linux/block/blk-mq.c: 1263
	 * obtain a reference even in the short window between the queue /Users/rubber/linux/block/blk-mq.c: 1264
	 * starting to freeze, by dropping the first reference in /Users/rubber/linux/block/blk-mq.c: 1265
	 * blk_freeze_queue_start, and the moment the last request is /Users/rubber/linux/block/blk-mq.c: 1266
	 * consumed, marked by the instant q_usage_counter reaches /Users/rubber/linux/block/blk-mq.c: 1267
	 * zero. /Users/rubber/linux/block/blk-mq.c: 1268
		/* /Users/rubber/linux/block/blk-mq.c: 1278
		 * Request timeouts are handled as a forward rolling timer. If /Users/rubber/linux/block/blk-mq.c: 1279
		 * we end up here it means that no requests are pending and /Users/rubber/linux/block/blk-mq.c: 1280
		 * also that no request has been pending for a while. Mark /Users/rubber/linux/block/blk-mq.c: 1281
		 * each hctx as idle. /Users/rubber/linux/block/blk-mq.c: 1282
 * Process software queues that have been marked busy, splicing them /Users/rubber/linux/block/blk-mq.c: 1313
 * to the for-dispatch /Users/rubber/linux/block/blk-mq.c: 1314
 * Mark us waiting for a tag. For shared tags, this involves hooking us into /Users/rubber/linux/block/blk-mq.c: 1427
 * the tag wakeups. For non-shared tags, we can simply mark us needing a /Users/rubber/linux/block/blk-mq.c: 1428
 * restart. For both cases, take care to check the condition again after /Users/rubber/linux/block/blk-mq.c: 1429
 * marking us as waiting. /Users/rubber/linux/block/blk-mq.c: 1430
		/* /Users/rubber/linux/block/blk-mq.c: 1443
		 * It's possible that a tag was freed in the window between the /Users/rubber/linux/block/blk-mq.c: 1444
		 * allocation failure and adding the hardware queue to the wait /Users/rubber/linux/block/blk-mq.c: 1445
		 * queue. /Users/rubber/linux/block/blk-mq.c: 1446
		 * /Users/rubber/linux/block/blk-mq.c: 1447
		 * Don't clear RESTART here, someone else could have set it. /Users/rubber/linux/block/blk-mq.c: 1448
		 * At most this will cost an extra queue run. /Users/rubber/linux/block/blk-mq.c: 1449
	/* /Users/rubber/linux/block/blk-mq.c: 1472
	 * It's possible that a tag was freed in the window between the /Users/rubber/linux/block/blk-mq.c: 1473
	 * allocation failure and adding the hardware queue to the wait /Users/rubber/linux/block/blk-mq.c: 1474
	 * queue. /Users/rubber/linux/block/blk-mq.c: 1475
	/* /Users/rubber/linux/block/blk-mq.c: 1484
	 * We got a tag, remove ourselves from the wait queue to ensure /Users/rubber/linux/block/blk-mq.c: 1485
	 * someone else gets the wakeup. /Users/rubber/linux/block/blk-mq.c: 1486
 * Update dispatch busy with the Exponential Weighted Moving Average(EWMA): /Users/rubber/linux/block/blk-mq.c: 1499
 * - EWMA is one simple way to compute running average value /Users/rubber/linux/block/blk-mq.c: 1500
 * - weight(7/8 and 1/8) is applied so that it can decrease exponentially /Users/rubber/linux/block/blk-mq.c: 1501
 * - take 4 as factor for avoiding to get too small(0) result, and this /Users/rubber/linux/block/blk-mq.c: 1502
 *   factor doesn't matter because EWMA decreases exponentially /Users/rubber/linux/block/blk-mq.c: 1503
	/* /Users/rubber/linux/block/blk-mq.c: 1530
	 * If an I/O scheduler has been configured and we got a driver tag for /Users/rubber/linux/block/blk-mq.c: 1531
	 * the next request already, free it. /Users/rubber/linux/block/blk-mq.c: 1532
	/* /Users/rubber/linux/block/blk-mq.c: 1544
	 * If we end up here it is because we cannot dispatch a request to a /Users/rubber/linux/block/blk-mq.c: 1545
	 * specific zone due to LLD level zone-write locking or other zone /Users/rubber/linux/block/blk-mq.c: 1546
	 * related resource not being available. In this case, set the request /Users/rubber/linux/block/blk-mq.c: 1547
	 * aside in zone_list for retrying it later. /Users/rubber/linux/block/blk-mq.c: 1548
		/* /Users/rubber/linux/block/blk-mq.c: 1576
		 * The initial allocation attempt failed, so we need to /Users/rubber/linux/block/blk-mq.c: 1577
		 * rerun the hardware queue when a tag is freed. The /Users/rubber/linux/block/blk-mq.c: 1578
		 * waitqueue takes care of that. If the queue is run /Users/rubber/linux/block/blk-mq.c: 1579
		 * before we add this entry back on the dispatch list, /Users/rubber/linux/block/blk-mq.c: 1580
		 * we'll re-run it below. /Users/rubber/linux/block/blk-mq.c: 1581
			/* /Users/rubber/linux/block/blk-mq.c: 1584
			 * All budgets not got from this function will be put /Users/rubber/linux/block/blk-mq.c: 1585
			 * together during handling partial dispatch /Users/rubber/linux/block/blk-mq.c: 1586
 * Returns true if we did some work AND can potentially do more. /Users/rubber/linux/block/blk-mq.c: 1612
	/* /Users/rubber/linux/block/blk-mq.c: 1628
	 * Now process all the entries, sending them to the driver. /Users/rubber/linux/block/blk-mq.c: 1629
		/* /Users/rubber/linux/block/blk-mq.c: 1646
		 * Flag last if we have no more requests, or if we have more /Users/rubber/linux/block/blk-mq.c: 1647
		 * but can't assign a driver tag to it. /Users/rubber/linux/block/blk-mq.c: 1648
		/* /Users/rubber/linux/block/blk-mq.c: 1657
		 * once the request is queued to lld, no need to cover the /Users/rubber/linux/block/blk-mq.c: 1658
		 * budget any more /Users/rubber/linux/block/blk-mq.c: 1659
			/* /Users/rubber/linux/block/blk-mq.c: 1675
			 * Move the request to zone_list and keep going through /Users/rubber/linux/block/blk-mq.c: 1676
			 * the dispatch list to find more requests the drive can /Users/rubber/linux/block/blk-mq.c: 1677
			 * accept. /Users/rubber/linux/block/blk-mq.c: 1678
	/* If we didn't flush the entire list, we could have told the driver /Users/rubber/linux/block/blk-mq.c: 1692
	 * there was more coming, but that turned out to be a lie. /Users/rubber/linux/block/blk-mq.c: 1693
	/* /Users/rubber/linux/block/blk-mq.c: 1697
	 * Any items that need requeuing? Stuff them into hctx->dispatch, /Users/rubber/linux/block/blk-mq.c: 1698
	 * that is where we will continue on next queue run. /Users/rubber/linux/block/blk-mq.c: 1699
		/* /Users/rubber/linux/block/blk-mq.c: 1714
		 * Order adding requests to hctx->dispatch and checking /Users/rubber/linux/block/blk-mq.c: 1715
		 * SCHED_RESTART flag. The pair of this smp_mb() is the one /Users/rubber/linux/block/blk-mq.c: 1716
		 * in blk_mq_sched_restart(). Avoid restart code path to /Users/rubber/linux/block/blk-mq.c: 1717
		 * miss the new added requests to hctx->dispatch, meantime /Users/rubber/linux/block/blk-mq.c: 1718
		 * SCHED_RESTART is observed here. /Users/rubber/linux/block/blk-mq.c: 1719
		/* /Users/rubber/linux/block/blk-mq.c: 1723
		 * If SCHED_RESTART was set by the caller of this function and /Users/rubber/linux/block/blk-mq.c: 1724
		 * it is no longer set that means that it was cleared by another /Users/rubber/linux/block/blk-mq.c: 1725
		 * thread and hence that a queue rerun is needed. /Users/rubber/linux/block/blk-mq.c: 1726
		 * /Users/rubber/linux/block/blk-mq.c: 1727
		 * If 'no_tag' is set, that means that we failed getting /Users/rubber/linux/block/blk-mq.c: 1728
		 * a driver tag with an I/O scheduler attached. If our dispatch /Users/rubber/linux/block/blk-mq.c: 1729
		 * waitqueue is no longer active, ensure that we run the queue /Users/rubber/linux/block/blk-mq.c: 1730
		 * AFTER adding our entries back to the list. /Users/rubber/linux/block/blk-mq.c: 1731
		 * /Users/rubber/linux/block/blk-mq.c: 1732
		 * If no I/O scheduler has been configured it is possible that /Users/rubber/linux/block/blk-mq.c: 1733
		 * the hardware queue got stopped and restarted before requests /Users/rubber/linux/block/blk-mq.c: 1734
		 * were pushed back onto the dispatch list. Rerun the queue to /Users/rubber/linux/block/blk-mq.c: 1735
		 * avoid starvation. Notes: /Users/rubber/linux/block/blk-mq.c: 1736
		 * - blk_mq_run_hw_queue() checks whether or not a queue has /Users/rubber/linux/block/blk-mq.c: 1737
		 *   been stopped before rerunning a queue. /Users/rubber/linux/block/blk-mq.c: 1738
		 * - Some but not all block drivers stop a queue before /Users/rubber/linux/block/blk-mq.c: 1739
		 *   returning BLK_STS_RESOURCE. Two exceptions are scsi-mq /Users/rubber/linux/block/blk-mq.c: 1740
		 *   and dm-rq. /Users/rubber/linux/block/blk-mq.c: 1741
		 * /Users/rubber/linux/block/blk-mq.c: 1742
		 * If driver returns BLK_STS_RESOURCE and SCHED_RESTART /Users/rubber/linux/block/blk-mq.c: 1743
		 * bit is set, run queue after a delay to avoid IO stalls /Users/rubber/linux/block/blk-mq.c: 1744
		 * that could otherwise occur if the queue is idle.  We'll do /Users/rubber/linux/block/blk-mq.c: 1745
		 * similar if we couldn't get budget or couldn't lock a zone /Users/rubber/linux/block/blk-mq.c: 1746
		 * and SCHED_RESTART is set. /Users/rubber/linux/block/blk-mq.c: 1747
 * __blk_mq_run_hw_queue - Run a hardware queue. /Users/rubber/linux/block/blk-mq.c: 1767
 * @hctx: Pointer to the hardware queue to run. /Users/rubber/linux/block/blk-mq.c: 1768
 * Send pending requests to the hardware. /Users/rubber/linux/block/blk-mq.c: 1770
	/* /Users/rubber/linux/block/blk-mq.c: 1776
	 * We can't run the queue inline with ints disabled. Ensure that /Users/rubber/linux/block/blk-mq.c: 1777
	 * we catch bad users of this early. /Users/rubber/linux/block/blk-mq.c: 1778
 * It'd be great if the workqueue API had a way to pass /Users/rubber/linux/block/blk-mq.c: 1799
 * in a mask and had some smarts for more clever placement. /Users/rubber/linux/block/blk-mq.c: 1800
 * For now we just round-robin here, switching for every /Users/rubber/linux/block/blk-mq.c: 1801
 * BLK_MQ_CPU_WORK_BATCH queued items. /Users/rubber/linux/block/blk-mq.c: 1802
	/* /Users/rubber/linux/block/blk-mq.c: 1821
	 * Do unbound schedule if we can't find a online CPU for this hctx, /Users/rubber/linux/block/blk-mq.c: 1822
	 * and it should only happen in the path of handling CPU DEAD. /Users/rubber/linux/block/blk-mq.c: 1823
		/* /Users/rubber/linux/block/blk-mq.c: 1831
		 * Make sure to re-select CPU next time once after CPUs /Users/rubber/linux/block/blk-mq.c: 1832
		 * in hctx->cpumask become online again. /Users/rubber/linux/block/blk-mq.c: 1833
 * __blk_mq_delay_run_hw_queue - Run (or schedule to run) a hardware queue. /Users/rubber/linux/block/blk-mq.c: 1845
 * @hctx: Pointer to the hardware queue to run. /Users/rubber/linux/block/blk-mq.c: 1846
 * @async: If we want to run the queue asynchronously. /Users/rubber/linux/block/blk-mq.c: 1847
 * @msecs: Milliseconds of delay to wait before running the queue. /Users/rubber/linux/block/blk-mq.c: 1848
 * If !@async, try to run the queue now. Else, run the queue asynchronously and /Users/rubber/linux/block/blk-mq.c: 1850
 * with a delay of @msecs. /Users/rubber/linux/block/blk-mq.c: 1851
 * blk_mq_delay_run_hw_queue - Run a hardware queue asynchronously. /Users/rubber/linux/block/blk-mq.c: 1875
 * @hctx: Pointer to the hardware queue to run. /Users/rubber/linux/block/blk-mq.c: 1876
 * @msecs: Milliseconds of delay to wait before running the queue. /Users/rubber/linux/block/blk-mq.c: 1877
 * Run a hardware queue asynchronously with a delay of @msecs. /Users/rubber/linux/block/blk-mq.c: 1879
 * blk_mq_run_hw_queue - Start to run a hardware queue. /Users/rubber/linux/block/blk-mq.c: 1888
 * @hctx: Pointer to the hardware queue to run. /Users/rubber/linux/block/blk-mq.c: 1889
 * @async: If we want to run the queue asynchronously. /Users/rubber/linux/block/blk-mq.c: 1890
 * Check if the request queue is not in a quiesced state and if there are /Users/rubber/linux/block/blk-mq.c: 1892
 * pending requests to be sent. If this is true, run the queue to send requests /Users/rubber/linux/block/blk-mq.c: 1893
 * to hardware. /Users/rubber/linux/block/blk-mq.c: 1894
	/* /Users/rubber/linux/block/blk-mq.c: 1901
	 * When queue is quiesced, we may be switching io scheduler, or /Users/rubber/linux/block/blk-mq.c: 1902
	 * updating nr_hw_queues, or other things, and we can't run queue /Users/rubber/linux/block/blk-mq.c: 1903
	 * any more, even __blk_mq_hctx_has_pending() can't be called safely. /Users/rubber/linux/block/blk-mq.c: 1904
	 * /Users/rubber/linux/block/blk-mq.c: 1905
	 * And queue will be rerun in blk_mq_unquiesce_queue() if it is /Users/rubber/linux/block/blk-mq.c: 1906
	 * quiesced. /Users/rubber/linux/block/blk-mq.c: 1907
 * Is the request queue handled by an IO scheduler that does not respect /Users/rubber/linux/block/blk-mq.c: 1920
 * hardware queues when dispatching? /Users/rubber/linux/block/blk-mq.c: 1921
 * Return prefered queue to dispatch from (if any) for non-mq aware IO /Users/rubber/linux/block/blk-mq.c: 1934
 * scheduler. /Users/rubber/linux/block/blk-mq.c: 1935
	/* /Users/rubber/linux/block/blk-mq.c: 1941
	 * If the IO scheduler does not respect hardware queues when /Users/rubber/linux/block/blk-mq.c: 1942
	 * dispatching, we just don't bother with multiple HW queues and /Users/rubber/linux/block/blk-mq.c: 1943
	 * dispatch from hctx for the current CPU since running multiple queues /Users/rubber/linux/block/blk-mq.c: 1944
	 * just causes lock contention inside the scheduler and pointless cache /Users/rubber/linux/block/blk-mq.c: 1945
	 * bouncing. /Users/rubber/linux/block/blk-mq.c: 1946
 * blk_mq_run_hw_queues - Run all hardware queues in a request queue. /Users/rubber/linux/block/blk-mq.c: 1956
 * @q: Pointer to the request queue to run. /Users/rubber/linux/block/blk-mq.c: 1957
 * @async: If we want to run the queue asynchronously. /Users/rubber/linux/block/blk-mq.c: 1958
		/* /Users/rubber/linux/block/blk-mq.c: 1971
		 * Dispatch from this hctx either if there's no hctx preferred /Users/rubber/linux/block/blk-mq.c: 1972
		 * by IO scheduler or if it has requests that bypass the /Users/rubber/linux/block/blk-mq.c: 1973
		 * scheduler. /Users/rubber/linux/block/blk-mq.c: 1974
 * blk_mq_delay_run_hw_queues - Run all hardware queues asynchronously. /Users/rubber/linux/block/blk-mq.c: 1984
 * @q: Pointer to the request queue to run. /Users/rubber/linux/block/blk-mq.c: 1985
 * @msecs: Milliseconds of delay to wait before running the queues. /Users/rubber/linux/block/blk-mq.c: 1986
		/* /Users/rubber/linux/block/blk-mq.c: 1999
		 * Dispatch from this hctx either if there's no hctx preferred /Users/rubber/linux/block/blk-mq.c: 2000
		 * by IO scheduler or if it has requests that bypass the /Users/rubber/linux/block/blk-mq.c: 2001
		 * scheduler. /Users/rubber/linux/block/blk-mq.c: 2002
 * blk_mq_queue_stopped() - check whether one or more hctxs have been stopped /Users/rubber/linux/block/blk-mq.c: 2012
 * @q: request queue. /Users/rubber/linux/block/blk-mq.c: 2013
 * The caller is responsible for serializing this function against /Users/rubber/linux/block/blk-mq.c: 2015
 * blk_mq_{start,stop}_hw_queue(). /Users/rubber/linux/block/blk-mq.c: 2016
 * This function is often used for pausing .queue_rq() by driver when /Users/rubber/linux/block/blk-mq.c: 2032
 * there isn't enough resource or some conditions aren't satisfied, and /Users/rubber/linux/block/blk-mq.c: 2033
 * BLK_STS_RESOURCE is usually returned. /Users/rubber/linux/block/blk-mq.c: 2034
 * We do not guarantee that dispatch can be drained or blocked /Users/rubber/linux/block/blk-mq.c: 2036
 * after blk_mq_stop_hw_queue() returns. Please use /Users/rubber/linux/block/blk-mq.c: 2037
 * blk_mq_quiesce_queue() for that requirement. /Users/rubber/linux/block/blk-mq.c: 2038
 * This function is often used for pausing .queue_rq() by driver when /Users/rubber/linux/block/blk-mq.c: 2049
 * there isn't enough resource or some conditions aren't satisfied, and /Users/rubber/linux/block/blk-mq.c: 2050
 * BLK_STS_RESOURCE is usually returned. /Users/rubber/linux/block/blk-mq.c: 2051
 * We do not guarantee that dispatch can be drained or blocked /Users/rubber/linux/block/blk-mq.c: 2053
 * after blk_mq_stop_hw_queues() returns. Please use /Users/rubber/linux/block/blk-mq.c: 2054
 * blk_mq_quiesce_queue() for that requirement. /Users/rubber/linux/block/blk-mq.c: 2055
	/* /Users/rubber/linux/block/blk-mq.c: 2111
	 * If we are stopped, don't run the queue. /Users/rubber/linux/block/blk-mq.c: 2112
 * blk_mq_request_bypass_insert - Insert a request at dispatch list. /Users/rubber/linux/block/blk-mq.c: 2149
 * @rq: Pointer to request to be inserted. /Users/rubber/linux/block/blk-mq.c: 2150
 * @at_head: true if the request should be inserted at the head of the list. /Users/rubber/linux/block/blk-mq.c: 2151
 * @run_queue: If we should run the hardware queue after inserting the request. /Users/rubber/linux/block/blk-mq.c: 2152
 * Should only be used carefully, when the caller knows we want to /Users/rubber/linux/block/blk-mq.c: 2154
 * bypass a potential IO scheduler on the target device. /Users/rubber/linux/block/blk-mq.c: 2155
	/* /Users/rubber/linux/block/blk-mq.c: 2180
	 * preemption doesn't flush plug list, so it's possible ctx->cpu is /Users/rubber/linux/block/blk-mq.c: 2181
	 * offline now /Users/rubber/linux/block/blk-mq.c: 2182
	/* /Users/rubber/linux/block/blk-mq.c: 2239
	 * If we didn't flush the entire list, we could have told the driver /Users/rubber/linux/block/blk-mq.c: 2240
	 * there was more coming, but that turned out to be a lie. /Users/rubber/linux/block/blk-mq.c: 2241
	/* /Users/rubber/linux/block/blk-mq.c: 2326
	 * For OK queue, we are done. For error, caller may kill it. /Users/rubber/linux/block/blk-mq.c: 2327
	 * Any other error (busy), just add it to our list as we /Users/rubber/linux/block/blk-mq.c: 2328
	 * previously would have done. /Users/rubber/linux/block/blk-mq.c: 2329
	/* /Users/rubber/linux/block/blk-mq.c: 2357
	 * RCU or SRCU read lock is needed before checking quiesced flag. /Users/rubber/linux/block/blk-mq.c: 2358
	 * /Users/rubber/linux/block/blk-mq.c: 2359
	 * When queue is stopped or quiesced, ignore 'bypass_insert' from /Users/rubber/linux/block/blk-mq.c: 2360
	 * blk_mq_request_issue_directly(), and return BLK_STS_OK to caller, /Users/rubber/linux/block/blk-mq.c: 2361
	 * and avoid driver to try to dispatch again. /Users/rubber/linux/block/blk-mq.c: 2362
 * blk_mq_try_issue_directly - Try to send a request directly to device driver. /Users/rubber/linux/block/blk-mq.c: 2395
 * @hctx: Pointer of the associated hardware queue. /Users/rubber/linux/block/blk-mq.c: 2396
 * @rq: Pointer to request to be sent. /Users/rubber/linux/block/blk-mq.c: 2397
 * If the device has enough resources to accept a new request now, send the /Users/rubber/linux/block/blk-mq.c: 2399
 * request directly to device driver. Else, insert at hctx->dispatch queue, so /Users/rubber/linux/block/blk-mq.c: 2400
 * we can try send it another time in the future. Requests inserted at this /Users/rubber/linux/block/blk-mq.c: 2401
 * queue have higher priority. /Users/rubber/linux/block/blk-mq.c: 2402
	/* /Users/rubber/linux/block/blk-mq.c: 2462
	 * If we didn't flush the entire list, we could have told /Users/rubber/linux/block/blk-mq.c: 2463
	 * the driver there was more coming, but that turned out to /Users/rubber/linux/block/blk-mq.c: 2464
	 * be a lie. /Users/rubber/linux/block/blk-mq.c: 2465
 * Allow 2x BLK_MAX_REQUEST_COUNT requests on plug queue for multiple /Users/rubber/linux/block/blk-mq.c: 2488
 * queues. This is important for md arrays to benefit from merging /Users/rubber/linux/block/blk-mq.c: 2489
 * requests. /Users/rubber/linux/block/blk-mq.c: 2490
 * blk_mq_submit_bio - Create and send a request to block device. /Users/rubber/linux/block/blk-mq.c: 2600
 * @bio: Bio pointer. /Users/rubber/linux/block/blk-mq.c: 2601
 * Builds up a request structure from @q and @bio and send to the device. The /Users/rubber/linux/block/blk-mq.c: 2603
 * request may not be queued directly to hardware if: /Users/rubber/linux/block/blk-mq.c: 2604
 * * This request can be merged with another one /Users/rubber/linux/block/blk-mq.c: 2605
 * * We want to place request at plug queue for possible future merging /Users/rubber/linux/block/blk-mq.c: 2606
 * * There is an IO scheduler active at this queue /Users/rubber/linux/block/blk-mq.c: 2607
 * It will not queue the request if there is an error with the bio, or at the /Users/rubber/linux/block/blk-mq.c: 2609
 * request creation. /Users/rubber/linux/block/blk-mq.c: 2610
		/* /Users/rubber/linux/block/blk-mq.c: 2659
		 * Use plugging if we have a ->commit_rqs() hook as well, as /Users/rubber/linux/block/blk-mq.c: 2660
		 * we know the driver uses bd->last in a smart fashion. /Users/rubber/linux/block/blk-mq.c: 2661
		 * /Users/rubber/linux/block/blk-mq.c: 2662
		 * Use normal plugging if this disk is slow HDD, as sequential /Users/rubber/linux/block/blk-mq.c: 2663
		 * IO may benefit a lot from plug merging. /Users/rubber/linux/block/blk-mq.c: 2664
		/* /Users/rubber/linux/block/blk-mq.c: 2689
		 * We do limited plugging. If the bio can be merged, do that. /Users/rubber/linux/block/blk-mq.c: 2690
		 * Otherwise the existing request in the plug list will be /Users/rubber/linux/block/blk-mq.c: 2691
		 * issued. So the plug list will have one request at most /Users/rubber/linux/block/blk-mq.c: 2692
		 * The plug list might get flushed before this. If that happens, /Users/rubber/linux/block/blk-mq.c: 2693
		 * the plug list is empty, and same_queue_rq is invalid. /Users/rubber/linux/block/blk-mq.c: 2694
		/* /Users/rubber/linux/block/blk-mq.c: 2709
		 * There is no scheduler and we can try to send directly /Users/rubber/linux/block/blk-mq.c: 2710
		 * to the hardware. /Users/rubber/linux/block/blk-mq.c: 2711
	/* /Users/rubber/linux/block/blk-mq.c: 2752
	 * Wait until all pending iteration is done. /Users/rubber/linux/block/blk-mq.c: 2753
	 * /Users/rubber/linux/block/blk-mq.c: 2754
	 * Request reference is cleared and it is guaranteed to be observed /Users/rubber/linux/block/blk-mq.c: 2755
	 * after the ->lock is released. /Users/rubber/linux/block/blk-mq.c: 2756
		/* /Users/rubber/linux/block/blk-mq.c: 2791
		 * Remove kmemleak object previously allocated in /Users/rubber/linux/block/blk-mq.c: 2792
		 * blk_mq_alloc_rqs(). /Users/rubber/linux/block/blk-mq.c: 2793
	/* /Users/rubber/linux/block/blk-mq.c: 2876
	 * rq_size is the size of the request plus driver payload, rounded /Users/rubber/linux/block/blk-mq.c: 2877
	 * to the cacheline size /Users/rubber/linux/block/blk-mq.c: 2878
		/* /Users/rubber/linux/block/blk-mq.c: 2912
		 * Allow kmemleak to scan these pages as they contain pointers /Users/rubber/linux/block/blk-mq.c: 2913
		 * to additional allocations like via ops->init_request(). /Users/rubber/linux/block/blk-mq.c: 2914
	/* /Users/rubber/linux/block/blk-mq.c: 2986
	 * Prevent new request from being allocated on the current hctx. /Users/rubber/linux/block/blk-mq.c: 2987
	 * /Users/rubber/linux/block/blk-mq.c: 2988
	 * The smp_mb__after_atomic() Pairs with the implied barrier in /Users/rubber/linux/block/blk-mq.c: 2989
	 * test_and_set_bit_lock in sbitmap_get().  Ensures the inactive flag is /Users/rubber/linux/block/blk-mq.c: 2990
	 * seen once we return from the tag allocator. /Users/rubber/linux/block/blk-mq.c: 2991
	/* /Users/rubber/linux/block/blk-mq.c: 2996
	 * Try to grab a reference to the queue and wait for any outstanding /Users/rubber/linux/block/blk-mq.c: 2997
	 * requests.  If we could not grab a reference the queue has been /Users/rubber/linux/block/blk-mq.c: 2998
	 * frozen and there are no requests. /Users/rubber/linux/block/blk-mq.c: 2999
 * 'cpu' is going away. splice any existing rq_list entries from this /Users/rubber/linux/block/blk-mq.c: 3021
 * software queue to the hw queue dispatch list, and ensure that it /Users/rubber/linux/block/blk-mq.c: 3022
 * gets run. /Users/rubber/linux/block/blk-mq.c: 3023
 * Before freeing hw queue, clearing the flush request reference in /Users/rubber/linux/block/blk-mq.c: 3067
 * tags->rqs[] for avoiding potential UAF. /Users/rubber/linux/block/blk-mq.c: 3068
	/* /Users/rubber/linux/block/blk-mq.c: 3085
	 * Wait until all pending iteration is done. /Users/rubber/linux/block/blk-mq.c: 3086
	 * /Users/rubber/linux/block/blk-mq.c: 3087
	 * Request reference is cleared and it is guaranteed to be observed /Users/rubber/linux/block/blk-mq.c: 3088
	 * after the ->lock is released. /Users/rubber/linux/block/blk-mq.c: 3089
	/* /Users/rubber/linux/block/blk-mq.c: 3205
	 * Allocate space for all possible cpus to avoid allocation at /Users/rubber/linux/block/blk-mq.c: 3206
	 * runtime /Users/rubber/linux/block/blk-mq.c: 3207
		/* /Users/rubber/linux/block/blk-mq.c: 3263
		 * Set local node, IFF we have more than one hw queue. If /Users/rubber/linux/block/blk-mq.c: 3264
		 * not, we remain on the home node of the device /Users/rubber/linux/block/blk-mq.c: 3265
	/* /Users/rubber/linux/block/blk-mq.c: 3342
	 * Map software to hardware queues. /Users/rubber/linux/block/blk-mq.c: 3343
	 * /Users/rubber/linux/block/blk-mq.c: 3344
	 * If the cpu isn't present, the cpu is mapped to first hctx. /Users/rubber/linux/block/blk-mq.c: 3345
				/* /Users/rubber/linux/block/blk-mq.c: 3360
				 * If tags initialization fail for some hctx, /Users/rubber/linux/block/blk-mq.c: 3361
				 * that hctx won't be brought online.  In this /Users/rubber/linux/block/blk-mq.c: 3362
				 * case, remap the current ctx to hctx[0] which /Users/rubber/linux/block/blk-mq.c: 3363
				 * is guaranteed to always have tags allocated /Users/rubber/linux/block/blk-mq.c: 3364
			/* /Users/rubber/linux/block/blk-mq.c: 3371
			 * If the CPU is already set in the mask, then we've /Users/rubber/linux/block/blk-mq.c: 3372
			 * mapped this one already. This can happen if /Users/rubber/linux/block/blk-mq.c: 3373
			 * devices share queues across queue maps. /Users/rubber/linux/block/blk-mq.c: 3374
			/* /Users/rubber/linux/block/blk-mq.c: 3384
			 * If the nr_ctx type overflows, we have exceeded the /Users/rubber/linux/block/blk-mq.c: 3385
			 * amount of sw queues we can support. /Users/rubber/linux/block/blk-mq.c: 3386
		/* /Users/rubber/linux/block/blk-mq.c: 3397
		 * If no software queues are mapped to this hardware queue, /Users/rubber/linux/block/blk-mq.c: 3398
		 * disable it and free the request entries. /Users/rubber/linux/block/blk-mq.c: 3399
			/* Never unmap queue 0.  We need it as a /Users/rubber/linux/block/blk-mq.c: 3402
			 * fallback in case of a new remap fails /Users/rubber/linux/block/blk-mq.c: 3403
			 * allocation /Users/rubber/linux/block/blk-mq.c: 3404
		/* /Users/rubber/linux/block/blk-mq.c: 3416
		 * Set the map size to the number of mapped software queues. /Users/rubber/linux/block/blk-mq.c: 3417
		 * This is more accurate and more efficient than looping /Users/rubber/linux/block/blk-mq.c: 3418
		 * over all possibly mapped software queues. /Users/rubber/linux/block/blk-mq.c: 3419
		/* /Users/rubber/linux/block/blk-mq.c: 3423
		 * Initialize batch roundrobin counts /Users/rubber/linux/block/blk-mq.c: 3424
 * Caller needs to ensure that we're either frozen/quiesced, or that /Users/rubber/linux/block/blk-mq.c: 3432
 * the queue isn't live yet. /Users/rubber/linux/block/blk-mq.c: 3433
	/* /Users/rubber/linux/block/blk-mq.c: 3485
	 * Check to see if we're transitioning to shared (from 1 to 2 queues). /Users/rubber/linux/block/blk-mq.c: 3486
 * It is the actual release handler for mq, but we do it from /Users/rubber/linux/block/blk-mq.c: 3530
 * request queue's release handler for avoiding use-after-free /Users/rubber/linux/block/blk-mq.c: 3531
 * and headache because q->mq_kobj shouldn't have been introduced, /Users/rubber/linux/block/blk-mq.c: 3532
 * but we can't group ctx/kctx kobj without it. /Users/rubber/linux/block/blk-mq.c: 3533
	/* /Users/rubber/linux/block/blk-mq.c: 3551
	 * release .mq_kobj and sw queue's kobject now because /Users/rubber/linux/block/blk-mq.c: 3552
	 * both share lifetime with request queue. /Users/rubber/linux/block/blk-mq.c: 3553
		/* /Users/rubber/linux/block/blk-mq.c: 3664
		 * If the hw queue has been mapped to another numa node, /Users/rubber/linux/block/blk-mq.c: 3665
		 * we need to realloc the hctx. If allocation fails, fallback /Users/rubber/linux/block/blk-mq.c: 3666
		 * to use the previous one. /Users/rubber/linux/block/blk-mq.c: 3667
	/* /Users/rubber/linux/block/blk-mq.c: 3686
	 * Increasing nr_hw_queues fails. Free the newly allocated /Users/rubber/linux/block/blk-mq.c: 3687
	 * hctxs and keep the previous q->nr_hw_queues. /Users/rubber/linux/block/blk-mq.c: 3688
	/* /Users/rubber/linux/block/blk-mq.c: 3751
	 * Default to classic polling /Users/rubber/linux/block/blk-mq.c: 3752
 * Allocate the request maps associated with this tag_set. Note that this /Users/rubber/linux/block/blk-mq.c: 3818
 * may reduce the depth asked for, if memory is tight. set->queue_depth /Users/rubber/linux/block/blk-mq.c: 3819
 * will be updated to reflect the allocated depth. /Users/rubber/linux/block/blk-mq.c: 3820
	/* /Users/rubber/linux/block/blk-mq.c: 3854
	 * blk_mq_map_queues() and multiple .map_queues() implementations /Users/rubber/linux/block/blk-mq.c: 3855
	 * expect that set->map[HCTX_TYPE_DEFAULT].nr_queues is set to the /Users/rubber/linux/block/blk-mq.c: 3856
	 * number of hardware queues. /Users/rubber/linux/block/blk-mq.c: 3857
		/* /Users/rubber/linux/block/blk-mq.c: 3865
		 * transport .map_queues is usually done in the following /Users/rubber/linux/block/blk-mq.c: 3866
		 * way: /Users/rubber/linux/block/blk-mq.c: 3867
		 * /Users/rubber/linux/block/blk-mq.c: 3868
		 * for (queue = 0; queue < set->nr_hw_queues; queue++) { /Users/rubber/linux/block/blk-mq.c: 3869
		 * 	mask = get_cpu_mask(queue) /Users/rubber/linux/block/blk-mq.c: 3870
		 * 	for_each_cpu(cpu, mask) /Users/rubber/linux/block/blk-mq.c: 3871
		 * 		set->map[x].mq_map[cpu] = queue; /Users/rubber/linux/block/blk-mq.c: 3872
		 * } /Users/rubber/linux/block/blk-mq.c: 3873
		 * /Users/rubber/linux/block/blk-mq.c: 3874
		 * When we need to remap, the table has to be cleared for /Users/rubber/linux/block/blk-mq.c: 3875
		 * killing stale mapping since one CPU may not be mapped /Users/rubber/linux/block/blk-mq.c: 3876
		 * to any hw queue. /Users/rubber/linux/block/blk-mq.c: 3877
 * Alloc a tag set to be associated with one or more request queues. /Users/rubber/linux/block/blk-mq.c: 3919
 * May fail with EINVAL for various error conditions. May adjust the /Users/rubber/linux/block/blk-mq.c: 3920
 * requested depth down, if it's too large. In that case, the set /Users/rubber/linux/block/blk-mq.c: 3921
 * value will be stored in set->queue_depth. /Users/rubber/linux/block/blk-mq.c: 3922
	/* /Users/rubber/linux/block/blk-mq.c: 3954
	 * If a crashdump is active, then we are potentially in a very /Users/rubber/linux/block/blk-mq.c: 3955
	 * memory constrained environment. Limit us to 1 queue and /Users/rubber/linux/block/blk-mq.c: 3956
	 * 64 tags to prevent using too much memory. /Users/rubber/linux/block/blk-mq.c: 3957
	/* /Users/rubber/linux/block/blk-mq.c: 3964
	 * There is no use for more h/w queues than cpus if we just have /Users/rubber/linux/block/blk-mq.c: 3965
	 * a single map /Users/rubber/linux/block/blk-mq.c: 3966
		/* /Users/rubber/linux/block/blk-mq.c: 4065
		 * If we're using an MQ scheduler, just update the scheduler /Users/rubber/linux/block/blk-mq.c: 4066
		 * queue depth. This is similar to what the old code would do. /Users/rubber/linux/block/blk-mq.c: 4067
 * request_queue and elevator_type pair. /Users/rubber/linux/block/blk-mq.c: 4098
 * It is just used by __blk_mq_update_nr_hw_queues to cache /Users/rubber/linux/block/blk-mq.c: 4099
 * the elevator_type associated with a request_queue. /Users/rubber/linux/block/blk-mq.c: 4100
 * Cache the elevator_type in qe pair list and switch the /Users/rubber/linux/block/blk-mq.c: 4109
 * io scheduler to 'none' /Users/rubber/linux/block/blk-mq.c: 4110
	/* /Users/rubber/linux/block/blk-mq.c: 4130
	 * After elevator_switch_mq, the previous elevator_queue will be /Users/rubber/linux/block/blk-mq.c: 4131
	 * released by elevator_release. The reference of the io scheduler /Users/rubber/linux/block/blk-mq.c: 4132
	 * module get by elevator_get will also be put. So we need to get /Users/rubber/linux/block/blk-mq.c: 4133
	 * a reference of the io scheduler module here to prevent it to be /Users/rubber/linux/block/blk-mq.c: 4134
	 * removed. /Users/rubber/linux/block/blk-mq.c: 4135
	/* /Users/rubber/linux/block/blk-mq.c: 4185
	 * Switch IO scheduler to 'none', cleaning up the data associated /Users/rubber/linux/block/blk-mq.c: 4186
	 * with the previous scheduler. We will switch back once we are done /Users/rubber/linux/block/blk-mq.c: 4187
	 * updating the new sw to hw queue mappings. /Users/rubber/linux/block/blk-mq.c: 4188
	/* /Users/rubber/linux/block/blk-mq.c: 4258
	 * We don't arm the callback if polling stats are not enabled or the /Users/rubber/linux/block/blk-mq.c: 4259
	 * callback is already active. /Users/rubber/linux/block/blk-mq.c: 4260
	/* /Users/rubber/linux/block/blk-mq.c: 4286
	 * If stats collection isn't on, don't sleep but turn it on for /Users/rubber/linux/block/blk-mq.c: 4287
	 * future users /Users/rubber/linux/block/blk-mq.c: 4288
	/* /Users/rubber/linux/block/blk-mq.c: 4293
	 * As an optimistic guess, use half of the mean service time /Users/rubber/linux/block/blk-mq.c: 4294
	 * for this type of request. We can (and should) make this smarter. /Users/rubber/linux/block/blk-mq.c: 4295
	 * For instance, if the completion latencies are tight, we can /Users/rubber/linux/block/blk-mq.c: 4296
	 * get closer than just half the mean. This is especially /Users/rubber/linux/block/blk-mq.c: 4297
	 * important on devices where the completion latencies are longer /Users/rubber/linux/block/blk-mq.c: 4298
	 * than ~10 usec. We do use the stats for the relevant IO size /Users/rubber/linux/block/blk-mq.c: 4299
	 * if available which does lead to better estimates. /Users/rubber/linux/block/blk-mq.c: 4300
	/* /Users/rubber/linux/block/blk-mq.c: 4321
	 * If a request has completed on queue that uses an I/O scheduler, we /Users/rubber/linux/block/blk-mq.c: 4322
	 * won't get back a request from blk_qc_to_rq. /Users/rubber/linux/block/blk-mq.c: 4323
	/* /Users/rubber/linux/block/blk-mq.c: 4328
	 * If we get here, hybrid polling is enabled. Hence poll_nsec can be: /Users/rubber/linux/block/blk-mq.c: 4329
	 * /Users/rubber/linux/block/blk-mq.c: 4330
	 *  0:	use half of prev avg /Users/rubber/linux/block/blk-mq.c: 4331
	 * >0:	use this specific value /Users/rubber/linux/block/blk-mq.c: 4332
	/* /Users/rubber/linux/block/blk-mq.c: 4344
	 * This will be replaced with the stats tracking code, using /Users/rubber/linux/block/blk-mq.c: 4345
	 * 'avg_completion_time / 2' as the pre-sleep target. /Users/rubber/linux/block/blk-mq.c: 4346
	/* /Users/rubber/linux/block/blk-mq.c: 4368
	 * If we sleep, have the caller restart the poll loop to reset the /Users/rubber/linux/block/blk-mq.c: 4369
	 * state.  Like for the other success return cases, the caller is /Users/rubber/linux/block/blk-mq.c: 4370
	 * responsible for checking if the IO completed.  If the IO isn't /Users/rubber/linux/block/blk-mq.c: 4371
	 * complete, we'll get called again and will go straight to the busy /Users/rubber/linux/block/blk-mq.c: 4372
	 * poll loop. /Users/rubber/linux/block/blk-mq.c: 4373
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-stat.c: 1
 * Block stat tracking code /Users/rubber/linux/block/blk-stat.c: 3
 * Copyright (C) 2016 Jens Axboe /Users/rubber/linux/block/blk-stat.c: 5
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-pm.c: 1
 * blk_pm_runtime_init - Block layer runtime PM initialization routine /Users/rubber/linux/block/blk-pm.c: 11
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 12
 * @dev: the device the queue belongs to /Users/rubber/linux/block/blk-pm.c: 13
 * Description: /Users/rubber/linux/block/blk-pm.c: 15
 *    Initialize runtime-PM-related fields for @q and start auto suspend for /Users/rubber/linux/block/blk-pm.c: 16
 *    @dev. Drivers that want to take advantage of request-based runtime PM /Users/rubber/linux/block/blk-pm.c: 17
 *    should call this function after @dev has been initialized, and its /Users/rubber/linux/block/blk-pm.c: 18
 *    request queue @q has been allocated, and runtime PM for it can not happen /Users/rubber/linux/block/blk-pm.c: 19
 *    yet(either due to disabled/forbidden or its usage_count > 0). In most /Users/rubber/linux/block/blk-pm.c: 20
 *    cases, driver should call this function before any I/O has taken place. /Users/rubber/linux/block/blk-pm.c: 21
 *    This function takes care of setting up using auto suspend for the device, /Users/rubber/linux/block/blk-pm.c: 23
 *    the autosuspend delay is set to -1 to make runtime suspend impossible /Users/rubber/linux/block/blk-pm.c: 24
 *    until an updated value is either set by user or by driver. Drivers do /Users/rubber/linux/block/blk-pm.c: 25
 *    not need to touch other autosuspend settings. /Users/rubber/linux/block/blk-pm.c: 26
 *    The block layer runtime PM is request based, so only works for drivers /Users/rubber/linux/block/blk-pm.c: 28
 *    that use request as their IO unit instead of those directly use bio's. /Users/rubber/linux/block/blk-pm.c: 29
 * blk_pre_runtime_suspend - Pre runtime suspend check /Users/rubber/linux/block/blk-pm.c: 41
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 42
 * Description: /Users/rubber/linux/block/blk-pm.c: 44
 *    This function will check if runtime suspend is allowed for the device /Users/rubber/linux/block/blk-pm.c: 45
 *    by examining if there are any requests pending in the queue. If there /Users/rubber/linux/block/blk-pm.c: 46
 *    are requests pending, the device can not be runtime suspended; otherwise, /Users/rubber/linux/block/blk-pm.c: 47
 *    the queue's status will be updated to SUSPENDING and the driver can /Users/rubber/linux/block/blk-pm.c: 48
 *    proceed to suspend the device. /Users/rubber/linux/block/blk-pm.c: 49
 *    For the not allowed case, we mark last busy for the device so that /Users/rubber/linux/block/blk-pm.c: 51
 *    runtime PM core will try to autosuspend it some time later. /Users/rubber/linux/block/blk-pm.c: 52
 *    This function should be called near the start of the device's /Users/rubber/linux/block/blk-pm.c: 54
 *    runtime_suspend callback. /Users/rubber/linux/block/blk-pm.c: 55
 * Return: /Users/rubber/linux/block/blk-pm.c: 57
 *    0		- OK to runtime suspend the device /Users/rubber/linux/block/blk-pm.c: 58
 *    -EBUSY	- Device should not be runtime suspended /Users/rubber/linux/block/blk-pm.c: 59
	/* /Users/rubber/linux/block/blk-pm.c: 74
	 * Increase the pm_only counter before checking whether any /Users/rubber/linux/block/blk-pm.c: 75
	 * non-PM blk_queue_enter() calls are in progress to avoid that any /Users/rubber/linux/block/blk-pm.c: 76
	 * new non-PM blk_queue_enter() calls succeed before the pm_only /Users/rubber/linux/block/blk-pm.c: 77
	 * counter is decreased again. /Users/rubber/linux/block/blk-pm.c: 78
	/* /Users/rubber/linux/block/blk-pm.c: 84
	 * Wait until atomic mode has been reached. Since that /Users/rubber/linux/block/blk-pm.c: 85
	 * involves calling call_rcu(), it is guaranteed that later /Users/rubber/linux/block/blk-pm.c: 86
	 * blk_queue_enter() calls see the pm-only state. See also /Users/rubber/linux/block/blk-pm.c: 87
	 * http://lwn.net/Articles/573497/. /Users/rubber/linux/block/blk-pm.c: 88
 * blk_post_runtime_suspend - Post runtime suspend processing /Users/rubber/linux/block/blk-pm.c: 110
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 111
 * @err: return value of the device's runtime_suspend function /Users/rubber/linux/block/blk-pm.c: 112
 * Description: /Users/rubber/linux/block/blk-pm.c: 114
 *    Update the queue's runtime status according to the return value of the /Users/rubber/linux/block/blk-pm.c: 115
 *    device's runtime suspend function and mark last busy for the device so /Users/rubber/linux/block/blk-pm.c: 116
 *    that PM core will try to auto suspend the device at a later time. /Users/rubber/linux/block/blk-pm.c: 117
 *    This function should be called near the end of the device's /Users/rubber/linux/block/blk-pm.c: 119
 *    runtime_suspend callback. /Users/rubber/linux/block/blk-pm.c: 120
 * blk_pre_runtime_resume - Pre runtime resume processing /Users/rubber/linux/block/blk-pm.c: 142
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 143
 * Description: /Users/rubber/linux/block/blk-pm.c: 145
 *    Update the queue's runtime status to RESUMING in preparation for the /Users/rubber/linux/block/blk-pm.c: 146
 *    runtime resume of the device. /Users/rubber/linux/block/blk-pm.c: 147
 *    This function should be called near the start of the device's /Users/rubber/linux/block/blk-pm.c: 149
 *    runtime_resume callback. /Users/rubber/linux/block/blk-pm.c: 150
 * blk_post_runtime_resume - Post runtime resume processing /Users/rubber/linux/block/blk-pm.c: 164
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 165
 * @err: return value of the device's runtime_resume function /Users/rubber/linux/block/blk-pm.c: 166
 * Description: /Users/rubber/linux/block/blk-pm.c: 168
 *    Update the queue's runtime status according to the return value of the /Users/rubber/linux/block/blk-pm.c: 169
 *    device's runtime_resume function. If the resume was successful, call /Users/rubber/linux/block/blk-pm.c: 170
 *    blk_set_runtime_active() to do the real work of restarting the queue. /Users/rubber/linux/block/blk-pm.c: 171
 *    This function should be called near the end of the device's /Users/rubber/linux/block/blk-pm.c: 173
 *    runtime_resume callback. /Users/rubber/linux/block/blk-pm.c: 174
 * blk_set_runtime_active - Force runtime status of the queue to be active /Users/rubber/linux/block/blk-pm.c: 191
 * @q: the queue of the device /Users/rubber/linux/block/blk-pm.c: 192
 * If the device is left runtime suspended during system suspend the resume /Users/rubber/linux/block/blk-pm.c: 194
 * hook typically resumes the device and corrects runtime status /Users/rubber/linux/block/blk-pm.c: 195
 * accordingly. However, that does not affect the queue runtime PM status /Users/rubber/linux/block/blk-pm.c: 196
 * which is still "suspended". This prevents processing requests from the /Users/rubber/linux/block/blk-pm.c: 197
 * queue. /Users/rubber/linux/block/blk-pm.c: 198
 * This function can be used in driver's resume hook to correct queue /Users/rubber/linux/block/blk-pm.c: 200
 * runtime PM status and re-enable peeking requests from the queue. It /Users/rubber/linux/block/blk-pm.c: 201
 * should be called before first request is added to the queue. /Users/rubber/linux/block/blk-pm.c: 202
 * This function is also called by blk_post_runtime_resume() for successful /Users/rubber/linux/block/blk-pm.c: 204
 * runtime resumes.  It does everything necessary to restart the queue. /Users/rubber/linux/block/blk-pm.c: 205
/* SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-cgroup-rwstat.c: 1
 * Legacy blkg rwstat helpers enabled by CONFIG_BLK_CGROUP_RWSTAT. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 3
 * Do not use in new code. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 4
 * __blkg_prfill_rwstat - prfill helper for a blkg_rwstat /Users/rubber/linux/block/blk-cgroup-rwstat.c: 35
 * @sf: seq_file to print to /Users/rubber/linux/block/blk-cgroup-rwstat.c: 36
 * @pd: policy private data of interest /Users/rubber/linux/block/blk-cgroup-rwstat.c: 37
 * @rwstat: rwstat to print /Users/rubber/linux/block/blk-cgroup-rwstat.c: 38
 * Print @rwstat to @sf for the device assocaited with @pd. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 40
 * blkg_prfill_rwstat - prfill callback for blkg_rwstat /Users/rubber/linux/block/blk-cgroup-rwstat.c: 72
 * @sf: seq_file to print to /Users/rubber/linux/block/blk-cgroup-rwstat.c: 73
 * @pd: policy private data of interest /Users/rubber/linux/block/blk-cgroup-rwstat.c: 74
 * @off: offset to the blkg_rwstat in @pd /Users/rubber/linux/block/blk-cgroup-rwstat.c: 75
 * prfill callback for printing a blkg_rwstat. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 77
 * blkg_rwstat_recursive_sum - collect hierarchical blkg_rwstat /Users/rubber/linux/block/blk-cgroup-rwstat.c: 90
 * @blkg: blkg of interest /Users/rubber/linux/block/blk-cgroup-rwstat.c: 91
 * @pol: blkcg_policy which contains the blkg_rwstat /Users/rubber/linux/block/blk-cgroup-rwstat.c: 92
 * @off: offset to the blkg_rwstat in blkg_policy_data or @blkg /Users/rubber/linux/block/blk-cgroup-rwstat.c: 93
 * @sum: blkg_rwstat_sample structure containing the results /Users/rubber/linux/block/blk-cgroup-rwstat.c: 94
 * Collect the blkg_rwstat specified by @blkg, @pol and @off and all its /Users/rubber/linux/block/blk-cgroup-rwstat.c: 96
 * online descendants and their aux counts.  The caller must be holding the /Users/rubber/linux/block/blk-cgroup-rwstat.c: 97
 * queue lock for online tests. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 98
 * If @pol is NULL, blkg_rwstat is at @off bytes into @blkg; otherwise, it /Users/rubber/linux/block/blk-cgroup-rwstat.c: 100
 * is at @off bytes into @blkg's blkg_policy_data of the policy. /Users/rubber/linux/block/blk-cgroup-rwstat.c: 101
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/bfq-wf2q.c: 1
 * Hierarchical Budget Worst-case Fair Weighted Fair Queueing /Users/rubber/linux/block/bfq-wf2q.c: 3
 * (B-WF2Q+): hierarchical scheduling algorithm by which the BFQ I/O /Users/rubber/linux/block/bfq-wf2q.c: 4
 * scheduler schedules generic entities. The latter can represent /Users/rubber/linux/block/bfq-wf2q.c: 5
 * either single bfq queues (associated with processes) or groups of /Users/rubber/linux/block/bfq-wf2q.c: 6
 * bfq queues (associated with cgroups). /Users/rubber/linux/block/bfq-wf2q.c: 7
 * bfq_gt - compare two timestamps. /Users/rubber/linux/block/bfq-wf2q.c: 12
 * @a: first ts. /Users/rubber/linux/block/bfq-wf2q.c: 13
 * @b: second ts. /Users/rubber/linux/block/bfq-wf2q.c: 14
 * Return @a > @b, dealing with wrapping correctly. /Users/rubber/linux/block/bfq-wf2q.c: 16
 * bfq_update_next_in_service - update sd->next_in_service /Users/rubber/linux/block/bfq-wf2q.c: 50
 * @sd: sched_data for which to perform the update. /Users/rubber/linux/block/bfq-wf2q.c: 51
 * @new_entity: if not NULL, pointer to the entity whose activation, /Users/rubber/linux/block/bfq-wf2q.c: 52
 *		requeueing or repositioning triggered the invocation of /Users/rubber/linux/block/bfq-wf2q.c: 53
 *		this function. /Users/rubber/linux/block/bfq-wf2q.c: 54
 * @expiration: id true, this function is being invoked after the /Users/rubber/linux/block/bfq-wf2q.c: 55
 *             expiration of the in-service entity /Users/rubber/linux/block/bfq-wf2q.c: 56
 * This function is called to update sd->next_in_service, which, in /Users/rubber/linux/block/bfq-wf2q.c: 58
 * its turn, may change as a consequence of the insertion or /Users/rubber/linux/block/bfq-wf2q.c: 59
 * extraction of an entity into/from one of the active trees of /Users/rubber/linux/block/bfq-wf2q.c: 60
 * sd. These insertions/extractions occur as a consequence of /Users/rubber/linux/block/bfq-wf2q.c: 61
 * activations/deactivations of entities, with some activations being /Users/rubber/linux/block/bfq-wf2q.c: 62
 * 'true' activations, and other activations being requeueings (i.e., /Users/rubber/linux/block/bfq-wf2q.c: 63
 * implementing the second, requeueing phase of the mechanism used to /Users/rubber/linux/block/bfq-wf2q.c: 64
 * reposition an entity in its active tree; see comments on /Users/rubber/linux/block/bfq-wf2q.c: 65
 * __bfq_activate_entity and __bfq_requeue_entity for details). In /Users/rubber/linux/block/bfq-wf2q.c: 66
 * both the last two activation sub-cases, new_entity points to the /Users/rubber/linux/block/bfq-wf2q.c: 67
 * just activated or requeued entity. /Users/rubber/linux/block/bfq-wf2q.c: 68
 * Returns true if sd->next_in_service changes in such a way that /Users/rubber/linux/block/bfq-wf2q.c: 70
 * entity->parent may become the next_in_service for its parent /Users/rubber/linux/block/bfq-wf2q.c: 71
 * entity. /Users/rubber/linux/block/bfq-wf2q.c: 72
	/* /Users/rubber/linux/block/bfq-wf2q.c: 82
	 * If this update is triggered by the activation, requeueing /Users/rubber/linux/block/bfq-wf2q.c: 83
	 * or repositioning of an entity that does not coincide with /Users/rubber/linux/block/bfq-wf2q.c: 84
	 * sd->next_in_service, then a full lookup in the active tree /Users/rubber/linux/block/bfq-wf2q.c: 85
	 * can be avoided. In fact, it is enough to check whether the /Users/rubber/linux/block/bfq-wf2q.c: 86
	 * just-modified entity has the same priority as /Users/rubber/linux/block/bfq-wf2q.c: 87
	 * sd->next_in_service, is eligible and has a lower virtual /Users/rubber/linux/block/bfq-wf2q.c: 88
	 * finish time than sd->next_in_service. If this compound /Users/rubber/linux/block/bfq-wf2q.c: 89
	 * condition holds, then the new entity becomes the new /Users/rubber/linux/block/bfq-wf2q.c: 90
	 * next_in_service. Otherwise no change is needed. /Users/rubber/linux/block/bfq-wf2q.c: 91
		/* /Users/rubber/linux/block/bfq-wf2q.c: 94
		 * Flag used to decide whether to replace /Users/rubber/linux/block/bfq-wf2q.c: 95
		 * sd->next_in_service with new_entity. Tentatively /Users/rubber/linux/block/bfq-wf2q.c: 96
		 * set to true, and left as true if /Users/rubber/linux/block/bfq-wf2q.c: 97
		 * sd->next_in_service is NULL. /Users/rubber/linux/block/bfq-wf2q.c: 98
		/* /Users/rubber/linux/block/bfq-wf2q.c: 102
		 * If there is already a next_in_service candidate /Users/rubber/linux/block/bfq-wf2q.c: 103
		 * entity, then compare timestamps to decide whether /Users/rubber/linux/block/bfq-wf2q.c: 104
		 * to replace sd->service_tree with new_entity. /Users/rubber/linux/block/bfq-wf2q.c: 105
 * Returns true if this budget changes may let next_in_service->parent /Users/rubber/linux/block/bfq-wf2q.c: 156
 * become the next_in_service entity for its parent entity. /Users/rubber/linux/block/bfq-wf2q.c: 157
	/* /Users/rubber/linux/block/bfq-wf2q.c: 169
	 * bfq_group's my_entity field is not NULL only if the group /Users/rubber/linux/block/bfq-wf2q.c: 170
	 * is not the root group. We must not touch the root entity /Users/rubber/linux/block/bfq-wf2q.c: 171
	 * as it must never become an in-service entity. /Users/rubber/linux/block/bfq-wf2q.c: 172
 * This function tells whether entity stops being a candidate for next /Users/rubber/linux/block/bfq-wf2q.c: 185
 * service, according to the restrictive definition of the field /Users/rubber/linux/block/bfq-wf2q.c: 186
 * next_in_service. In particular, this function is invoked for an /Users/rubber/linux/block/bfq-wf2q.c: 187
 * entity that is about to be set in service. /Users/rubber/linux/block/bfq-wf2q.c: 188
 * If entity is a queue, then the entity is no longer a candidate for /Users/rubber/linux/block/bfq-wf2q.c: 190
 * next service according to the that definition, because entity is /Users/rubber/linux/block/bfq-wf2q.c: 191
 * about to become the in-service queue. This function then returns /Users/rubber/linux/block/bfq-wf2q.c: 192
 * true if entity is a queue. /Users/rubber/linux/block/bfq-wf2q.c: 193
 * In contrast, entity could still be a candidate for next service if /Users/rubber/linux/block/bfq-wf2q.c: 195
 * it is not a queue, and has more than one active child. In fact, /Users/rubber/linux/block/bfq-wf2q.c: 196
 * even if one of its children is about to be set in service, other /Users/rubber/linux/block/bfq-wf2q.c: 197
 * active children may still be the next to serve, for the parent /Users/rubber/linux/block/bfq-wf2q.c: 198
 * entity, even according to the above definition. As a consequence, a /Users/rubber/linux/block/bfq-wf2q.c: 199
 * non-queue entity is not a candidate for next-service only if it has /Users/rubber/linux/block/bfq-wf2q.c: 200
 * only one active child. And only if this condition holds, then this /Users/rubber/linux/block/bfq-wf2q.c: 201
 * function returns true for a non-queue entity. /Users/rubber/linux/block/bfq-wf2q.c: 202
	/* /Users/rubber/linux/block/bfq-wf2q.c: 213
	 * The field active_entities does not always contain the /Users/rubber/linux/block/bfq-wf2q.c: 214
	 * actual number of active children entities: it happens to /Users/rubber/linux/block/bfq-wf2q.c: 215
	 * not account for the in-service entity in case the latter is /Users/rubber/linux/block/bfq-wf2q.c: 216
	 * removed from its active tree (which may get done after /Users/rubber/linux/block/bfq-wf2q.c: 217
	 * invoking the function bfq_no_longer_next_in_service in /Users/rubber/linux/block/bfq-wf2q.c: 218
	 * bfq_get_next_queue). Fortunately, here, i.e., while /Users/rubber/linux/block/bfq-wf2q.c: 219
	 * bfq_no_longer_next_in_service is not yet completed in /Users/rubber/linux/block/bfq-wf2q.c: 220
	 * bfq_get_next_queue, bfq_active_extract has not yet been /Users/rubber/linux/block/bfq-wf2q.c: 221
	 * invoked, and thus active_entities still coincides with the /Users/rubber/linux/block/bfq-wf2q.c: 222
	 * actual number of active entities. /Users/rubber/linux/block/bfq-wf2q.c: 223
 * Shift for timestamp calculations.  This actually limits the maximum /Users/rubber/linux/block/bfq-wf2q.c: 251
 * service allowed in one timestamp delta (small shift values increase it), /Users/rubber/linux/block/bfq-wf2q.c: 252
 * the maximum total weight that can be used for the queues in the system /Users/rubber/linux/block/bfq-wf2q.c: 253
 * (big shift values increase it), and the period of virtual time /Users/rubber/linux/block/bfq-wf2q.c: 254
 * wraparounds. /Users/rubber/linux/block/bfq-wf2q.c: 255
 * bfq_delta - map service into the virtual time domain. /Users/rubber/linux/block/bfq-wf2q.c: 271
 * @service: amount of service. /Users/rubber/linux/block/bfq-wf2q.c: 272
 * @weight: scale factor (weight of an entity or weight sum). /Users/rubber/linux/block/bfq-wf2q.c: 273
 * bfq_calc_finish - assign the finish time to an entity. /Users/rubber/linux/block/bfq-wf2q.c: 281
 * @entity: the entity to act upon. /Users/rubber/linux/block/bfq-wf2q.c: 282
 * @service: the service to be charged to the entity. /Users/rubber/linux/block/bfq-wf2q.c: 283
 * bfq_entity_of - get an entity from a node. /Users/rubber/linux/block/bfq-wf2q.c: 304
 * @node: the node field of the entity. /Users/rubber/linux/block/bfq-wf2q.c: 305
 * Convert a node pointer to the relative entity.  This is used only /Users/rubber/linux/block/bfq-wf2q.c: 307
 * to simplify the logic of some functions and not as the generic /Users/rubber/linux/block/bfq-wf2q.c: 308
 * conversion mechanism because, e.g., in the tree walking functions, /Users/rubber/linux/block/bfq-wf2q.c: 309
 * the check for a %NULL value would be redundant. /Users/rubber/linux/block/bfq-wf2q.c: 310
 * bfq_extract - remove an entity from a tree. /Users/rubber/linux/block/bfq-wf2q.c: 323
 * @root: the tree root. /Users/rubber/linux/block/bfq-wf2q.c: 324
 * @entity: the entity to remove. /Users/rubber/linux/block/bfq-wf2q.c: 325
 * bfq_idle_extract - extract an entity from the idle tree. /Users/rubber/linux/block/bfq-wf2q.c: 334
 * @st: the service tree of the owning @entity. /Users/rubber/linux/block/bfq-wf2q.c: 335
 * @entity: the entity being removed. /Users/rubber/linux/block/bfq-wf2q.c: 336
 * bfq_insert - generic tree insertion. /Users/rubber/linux/block/bfq-wf2q.c: 361
 * @root: tree root. /Users/rubber/linux/block/bfq-wf2q.c: 362
 * @entity: entity to insert. /Users/rubber/linux/block/bfq-wf2q.c: 363
 * This is used for the idle and the active tree, since they are both /Users/rubber/linux/block/bfq-wf2q.c: 365
 * ordered by finish time. /Users/rubber/linux/block/bfq-wf2q.c: 366
 * bfq_update_min - update the min_start field of a entity. /Users/rubber/linux/block/bfq-wf2q.c: 391
 * @entity: the entity to update. /Users/rubber/linux/block/bfq-wf2q.c: 392
 * @node: one of its children. /Users/rubber/linux/block/bfq-wf2q.c: 393
 * This function is called when @entity may store an invalid value for /Users/rubber/linux/block/bfq-wf2q.c: 395
 * min_start due to updates to the active tree.  The function  assumes /Users/rubber/linux/block/bfq-wf2q.c: 396
 * that the subtree rooted at @node (which may be its left or its right /Users/rubber/linux/block/bfq-wf2q.c: 397
 * child) has a valid min_start value. /Users/rubber/linux/block/bfq-wf2q.c: 398
 * bfq_update_active_node - recalculate min_start. /Users/rubber/linux/block/bfq-wf2q.c: 412
 * @node: the node to update. /Users/rubber/linux/block/bfq-wf2q.c: 413
 * @node may have changed position or one of its children may have moved, /Users/rubber/linux/block/bfq-wf2q.c: 415
 * this function updates its min_start value.  The left and right subtrees /Users/rubber/linux/block/bfq-wf2q.c: 416
 * are assumed to hold a correct min_start value. /Users/rubber/linux/block/bfq-wf2q.c: 417
 * bfq_update_active_tree - update min_start for the whole active tree. /Users/rubber/linux/block/bfq-wf2q.c: 429
 * @node: the starting node. /Users/rubber/linux/block/bfq-wf2q.c: 430
 * @node must be the deepest modified node after an update.  This function /Users/rubber/linux/block/bfq-wf2q.c: 432
 * updates its min_start using the values held by its children, assuming /Users/rubber/linux/block/bfq-wf2q.c: 433
 * that they did not change, and then updates all the nodes that may have /Users/rubber/linux/block/bfq-wf2q.c: 434
 * changed in the path to the root.  The only nodes that may have changed /Users/rubber/linux/block/bfq-wf2q.c: 435
 * are the ones in the path or their siblings. /Users/rubber/linux/block/bfq-wf2q.c: 436
 * bfq_active_insert - insert an entity in the active tree of its /Users/rubber/linux/block/bfq-wf2q.c: 459
 *                     group/device. /Users/rubber/linux/block/bfq-wf2q.c: 460
 * @st: the service tree of the entity. /Users/rubber/linux/block/bfq-wf2q.c: 461
 * @entity: the entity being inserted. /Users/rubber/linux/block/bfq-wf2q.c: 462
 * The active tree is ordered by finish time, but an extra key is kept /Users/rubber/linux/block/bfq-wf2q.c: 464
 * per each node, containing the minimum value for the start times of /Users/rubber/linux/block/bfq-wf2q.c: 465
 * its children (and the node itself), so it's possible to search for /Users/rubber/linux/block/bfq-wf2q.c: 466
 * the eligible node with the lowest finish time in logarithmic time. /Users/rubber/linux/block/bfq-wf2q.c: 467
 * bfq_ioprio_to_weight - calc a weight from an ioprio. /Users/rubber/linux/block/bfq-wf2q.c: 503
 * @ioprio: the ioprio value to convert. /Users/rubber/linux/block/bfq-wf2q.c: 504
 * bfq_weight_to_ioprio - calc an ioprio from a weight. /Users/rubber/linux/block/bfq-wf2q.c: 512
 * @weight: the weight value to convert. /Users/rubber/linux/block/bfq-wf2q.c: 513
 * To preserve as much as possible the old only-ioprio user interface, /Users/rubber/linux/block/bfq-wf2q.c: 515
 * 0 is used as an escape ioprio value for weights (numerically) equal or /Users/rubber/linux/block/bfq-wf2q.c: 516
 * larger than IOPRIO_NR_LEVELS * BFQ_WEIGHT_CONVERSION_COEFF. /Users/rubber/linux/block/bfq-wf2q.c: 517
 * bfq_find_deepest - find the deepest node that an extraction can modify. /Users/rubber/linux/block/bfq-wf2q.c: 537
 * @node: the node being removed. /Users/rubber/linux/block/bfq-wf2q.c: 538
 * Do the first step of an extraction in an rb tree, looking for the /Users/rubber/linux/block/bfq-wf2q.c: 540
 * node that will replace @node, and returning the deepest node that /Users/rubber/linux/block/bfq-wf2q.c: 541
 * the following modifications to the tree can touch.  If @node is the /Users/rubber/linux/block/bfq-wf2q.c: 542
 * last node in the tree return %NULL. /Users/rubber/linux/block/bfq-wf2q.c: 543
 * bfq_active_extract - remove an entity from the active tree. /Users/rubber/linux/block/bfq-wf2q.c: 567
 * @st: the service_tree containing the tree. /Users/rubber/linux/block/bfq-wf2q.c: 568
 * @entity: the entity being removed. /Users/rubber/linux/block/bfq-wf2q.c: 569
 * bfq_idle_insert - insert an entity into the idle tree. /Users/rubber/linux/block/bfq-wf2q.c: 602
 * @st: the service tree containing the tree. /Users/rubber/linux/block/bfq-wf2q.c: 603
 * @entity: the entity to insert. /Users/rubber/linux/block/bfq-wf2q.c: 604
 * bfq_forget_entity - do not consider entity any longer for scheduling /Users/rubber/linux/block/bfq-wf2q.c: 625
 * @st: the service tree. /Users/rubber/linux/block/bfq-wf2q.c: 626
 * @entity: the entity being removed. /Users/rubber/linux/block/bfq-wf2q.c: 627
 * @is_in_service: true if entity is currently the in-service entity. /Users/rubber/linux/block/bfq-wf2q.c: 628
 * Forget everything about @entity. In addition, if entity represents /Users/rubber/linux/block/bfq-wf2q.c: 630
 * a queue, and the latter is not in service, then release the service /Users/rubber/linux/block/bfq-wf2q.c: 631
 * reference to the queue (the one taken through bfq_get_entity). In /Users/rubber/linux/block/bfq-wf2q.c: 632
 * fact, in this case, there is really no more service reference to /Users/rubber/linux/block/bfq-wf2q.c: 633
 * the queue, as the latter is also outside any service tree. If, /Users/rubber/linux/block/bfq-wf2q.c: 634
 * instead, the queue is in service, then __bfq_bfqd_reset_in_service /Users/rubber/linux/block/bfq-wf2q.c: 635
 * will take care of putting the reference when the queue finally /Users/rubber/linux/block/bfq-wf2q.c: 636
 * stops being served. /Users/rubber/linux/block/bfq-wf2q.c: 637
 * bfq_put_idle_entity - release the idle tree ref of an entity. /Users/rubber/linux/block/bfq-wf2q.c: 652
 * @st: service tree for the entity. /Users/rubber/linux/block/bfq-wf2q.c: 653
 * @entity: the entity being released. /Users/rubber/linux/block/bfq-wf2q.c: 654
 * bfq_forget_idle - update the idle tree if necessary. /Users/rubber/linux/block/bfq-wf2q.c: 664
 * @st: the service tree to act upon. /Users/rubber/linux/block/bfq-wf2q.c: 665
 * To preserve the global O(log N) complexity we only remove one entry here; /Users/rubber/linux/block/bfq-wf2q.c: 667
 * as the idle tree will not grow indefinitely this can be done safely. /Users/rubber/linux/block/bfq-wf2q.c: 668
		/* /Users/rubber/linux/block/bfq-wf2q.c: 677
		 * Forget the whole idle tree, increasing the vtime past /Users/rubber/linux/block/bfq-wf2q.c: 678
		 * the last finish time of idle entities. /Users/rubber/linux/block/bfq-wf2q.c: 679
 * Update weight and priority of entity. If update_class_too is true, /Users/rubber/linux/block/bfq-wf2q.c: 697
 * then update the ioprio_class of entity too. /Users/rubber/linux/block/bfq-wf2q.c: 698
 * The reason why the update of ioprio_class is controlled through the /Users/rubber/linux/block/bfq-wf2q.c: 700
 * last parameter is as follows. Changing the ioprio class of an /Users/rubber/linux/block/bfq-wf2q.c: 701
 * entity implies changing the destination service trees for that /Users/rubber/linux/block/bfq-wf2q.c: 702
 * entity. If such a change occurred when the entity is already on one /Users/rubber/linux/block/bfq-wf2q.c: 703
 * of the service trees for its previous class, then the state of the /Users/rubber/linux/block/bfq-wf2q.c: 704
 * entity would become more complex: none of the new possible service /Users/rubber/linux/block/bfq-wf2q.c: 705
 * trees for the entity, according to bfq_entity_service_tree(), would /Users/rubber/linux/block/bfq-wf2q.c: 706
 * match any of the possible service trees on which the entity /Users/rubber/linux/block/bfq-wf2q.c: 707
 * is. Complex operations involving these trees, such as entity /Users/rubber/linux/block/bfq-wf2q.c: 708
 * activations and deactivations, should take into account this /Users/rubber/linux/block/bfq-wf2q.c: 709
 * additional complexity.  To avoid this issue, this function is /Users/rubber/linux/block/bfq-wf2q.c: 710
 * invoked with update_class_too unset in the points in the code where /Users/rubber/linux/block/bfq-wf2q.c: 711
 * entity may happen to be on some tree. /Users/rubber/linux/block/bfq-wf2q.c: 712
		/* /Users/rubber/linux/block/bfq-wf2q.c: 764
		 * Reset prio_changed only if the ioprio_class change /Users/rubber/linux/block/bfq-wf2q.c: 765
		 * is not pending any longer. /Users/rubber/linux/block/bfq-wf2q.c: 766
		/* /Users/rubber/linux/block/bfq-wf2q.c: 771
		 * NOTE: here we may be changing the weight too early, /Users/rubber/linux/block/bfq-wf2q.c: 772
		 * this will cause unfairness.  The correct approach /Users/rubber/linux/block/bfq-wf2q.c: 773
		 * would have required additional complexity to defer /Users/rubber/linux/block/bfq-wf2q.c: 774
		 * weight changes to the proper time instants (i.e., /Users/rubber/linux/block/bfq-wf2q.c: 775
		 * when entity->finish <= old_st->vtime). /Users/rubber/linux/block/bfq-wf2q.c: 776
		/* /Users/rubber/linux/block/bfq-wf2q.c: 783
		 * If the weight of the entity changes, and the entity is a /Users/rubber/linux/block/bfq-wf2q.c: 784
		 * queue, remove the entity from its old weight counter (if /Users/rubber/linux/block/bfq-wf2q.c: 785
		 * there is a counter associated with the entity). /Users/rubber/linux/block/bfq-wf2q.c: 786
		/* /Users/rubber/linux/block/bfq-wf2q.c: 793
		 * Add the entity, if it is not a weight-raised queue, /Users/rubber/linux/block/bfq-wf2q.c: 794
		 * to the counter associated with its new weight. /Users/rubber/linux/block/bfq-wf2q.c: 795
 * bfq_bfqq_served - update the scheduler status after selection for /Users/rubber/linux/block/bfq-wf2q.c: 812
 *                   service. /Users/rubber/linux/block/bfq-wf2q.c: 813
 * @bfqq: the queue being served. /Users/rubber/linux/block/bfq-wf2q.c: 814
 * @served: bytes to transfer. /Users/rubber/linux/block/bfq-wf2q.c: 815
 * NOTE: this can be optimized, as the timestamps of upper level entities /Users/rubber/linux/block/bfq-wf2q.c: 817
 * are synchronized every time a new bfqq is selected for service.  By now, /Users/rubber/linux/block/bfq-wf2q.c: 818
 * we keep it to better check consistency. /Users/rubber/linux/block/bfq-wf2q.c: 819
 * bfq_bfqq_charge_time - charge an amount of service equivalent to the length /Users/rubber/linux/block/bfq-wf2q.c: 845
 *			  of the time interval during which bfqq has been in /Users/rubber/linux/block/bfq-wf2q.c: 846
 *			  service. /Users/rubber/linux/block/bfq-wf2q.c: 847
 * @bfqd: the device /Users/rubber/linux/block/bfq-wf2q.c: 848
 * @bfqq: the queue that needs a service update. /Users/rubber/linux/block/bfq-wf2q.c: 849
 * @time_ms: the amount of time during which the queue has received service /Users/rubber/linux/block/bfq-wf2q.c: 850
 * If a queue does not consume its budget fast enough, then providing /Users/rubber/linux/block/bfq-wf2q.c: 852
 * the queue with service fairness may impair throughput, more or less /Users/rubber/linux/block/bfq-wf2q.c: 853
 * severely. For this reason, queues that consume their budget slowly /Users/rubber/linux/block/bfq-wf2q.c: 854
 * are provided with time fairness instead of service fairness. This /Users/rubber/linux/block/bfq-wf2q.c: 855
 * goal is achieved through the BFQ scheduling engine, even if such an /Users/rubber/linux/block/bfq-wf2q.c: 856
 * engine works in the service, and not in the time domain. The trick /Users/rubber/linux/block/bfq-wf2q.c: 857
 * is charging these queues with an inflated amount of service, equal /Users/rubber/linux/block/bfq-wf2q.c: 858
 * to the amount of service that they would have received during their /Users/rubber/linux/block/bfq-wf2q.c: 859
 * service slot if they had been fast, i.e., if their requests had /Users/rubber/linux/block/bfq-wf2q.c: 860
 * been dispatched at a rate equal to the estimated peak rate. /Users/rubber/linux/block/bfq-wf2q.c: 861
 * It is worth noting that time fairness can cause important /Users/rubber/linux/block/bfq-wf2q.c: 863
 * distortions in terms of bandwidth distribution, on devices with /Users/rubber/linux/block/bfq-wf2q.c: 864
 * internal queueing. The reason is that I/O requests dispatched /Users/rubber/linux/block/bfq-wf2q.c: 865
 * during the service slot of a queue may be served after that service /Users/rubber/linux/block/bfq-wf2q.c: 866
 * slot is finished, and may have a total processing time loosely /Users/rubber/linux/block/bfq-wf2q.c: 867
 * correlated with the duration of the service slot. This is /Users/rubber/linux/block/bfq-wf2q.c: 868
 * especially true for short service slots. /Users/rubber/linux/block/bfq-wf2q.c: 869
	/* /Users/rubber/linux/block/bfq-wf2q.c: 895
	 * When this function is invoked, entity is not in any service /Users/rubber/linux/block/bfq-wf2q.c: 896
	 * tree, then it is safe to invoke next function with the last /Users/rubber/linux/block/bfq-wf2q.c: 897
	 * parameter set (see the comments on the function). /Users/rubber/linux/block/bfq-wf2q.c: 898
	/* /Users/rubber/linux/block/bfq-wf2q.c: 903
	 * If some queues enjoy backshifting for a while, then their /Users/rubber/linux/block/bfq-wf2q.c: 904
	 * (virtual) finish timestamps may happen to become lower and /Users/rubber/linux/block/bfq-wf2q.c: 905
	 * lower than the system virtual time.	In particular, if /Users/rubber/linux/block/bfq-wf2q.c: 906
	 * these queues often happen to be idle for short time /Users/rubber/linux/block/bfq-wf2q.c: 907
	 * periods, and during such time periods other queues with /Users/rubber/linux/block/bfq-wf2q.c: 908
	 * higher timestamps happen to be busy, then the backshifted /Users/rubber/linux/block/bfq-wf2q.c: 909
	 * timestamps of the former queues can become much lower than /Users/rubber/linux/block/bfq-wf2q.c: 910
	 * the system virtual time. In fact, to serve the queues with /Users/rubber/linux/block/bfq-wf2q.c: 911
	 * higher timestamps while the ones with lower timestamps are /Users/rubber/linux/block/bfq-wf2q.c: 912
	 * idle, the system virtual time may be pushed-up to much /Users/rubber/linux/block/bfq-wf2q.c: 913
	 * higher values than the finish timestamps of the idle /Users/rubber/linux/block/bfq-wf2q.c: 914
	 * queues. As a consequence, the finish timestamps of all new /Users/rubber/linux/block/bfq-wf2q.c: 915
	 * or newly activated queues may end up being much larger than /Users/rubber/linux/block/bfq-wf2q.c: 916
	 * those of lucky queues with backshifted timestamps. The /Users/rubber/linux/block/bfq-wf2q.c: 917
	 * latter queues may then monopolize the device for a lot of /Users/rubber/linux/block/bfq-wf2q.c: 918
	 * time. This would simply break service guarantees. /Users/rubber/linux/block/bfq-wf2q.c: 919
	 * /Users/rubber/linux/block/bfq-wf2q.c: 920
	 * To reduce this problem, push up a little bit the /Users/rubber/linux/block/bfq-wf2q.c: 921
	 * backshifted timestamps of the queue associated with this /Users/rubber/linux/block/bfq-wf2q.c: 922
	 * entity (only a queue can happen to have the backshifted /Users/rubber/linux/block/bfq-wf2q.c: 923
	 * flag set): just enough to let the finish timestamp of the /Users/rubber/linux/block/bfq-wf2q.c: 924
	 * queue be equal to the current value of the system virtual /Users/rubber/linux/block/bfq-wf2q.c: 925
	 * time. This may introduce a little unfairness among queues /Users/rubber/linux/block/bfq-wf2q.c: 926
	 * with backshifted timestamps, but it does not break /Users/rubber/linux/block/bfq-wf2q.c: 927
	 * worst-case fairness guarantees. /Users/rubber/linux/block/bfq-wf2q.c: 928
	 * /Users/rubber/linux/block/bfq-wf2q.c: 929
	 * As a special case, if bfqq is weight-raised, push up /Users/rubber/linux/block/bfq-wf2q.c: 930
	 * timestamps much less, to keep very low the probability that /Users/rubber/linux/block/bfq-wf2q.c: 931
	 * this push up causes the backshifted finish timestamps of /Users/rubber/linux/block/bfq-wf2q.c: 932
	 * weight-raised queues to become higher than the backshifted /Users/rubber/linux/block/bfq-wf2q.c: 933
	 * finish timestamps of non weight-raised queues. /Users/rubber/linux/block/bfq-wf2q.c: 934
 * __bfq_activate_entity - handle activation of entity. /Users/rubber/linux/block/bfq-wf2q.c: 950
 * @entity: the entity being activated. /Users/rubber/linux/block/bfq-wf2q.c: 951
 * @non_blocking_wait_rq: true if entity was waiting for a request /Users/rubber/linux/block/bfq-wf2q.c: 952
 * Called for a 'true' activation, i.e., if entity is not active and /Users/rubber/linux/block/bfq-wf2q.c: 954
 * one of its children receives a new request. /Users/rubber/linux/block/bfq-wf2q.c: 955
 * Basically, this function updates the timestamps of entity and /Users/rubber/linux/block/bfq-wf2q.c: 957
 * inserts entity into its active tree, after possibly extracting it /Users/rubber/linux/block/bfq-wf2q.c: 958
 * from its idle tree. /Users/rubber/linux/block/bfq-wf2q.c: 959
		/* /Users/rubber/linux/block/bfq-wf2q.c: 976
		 * Must be on the idle tree, bfq_idle_extract() will /Users/rubber/linux/block/bfq-wf2q.c: 977
		 * check for that. /Users/rubber/linux/block/bfq-wf2q.c: 978
		/* /Users/rubber/linux/block/bfq-wf2q.c: 984
		 * The finish time of the entity may be invalid, and /Users/rubber/linux/block/bfq-wf2q.c: 985
		 * it is in the past for sure, otherwise the queue /Users/rubber/linux/block/bfq-wf2q.c: 986
		 * would have been on the idle tree. /Users/rubber/linux/block/bfq-wf2q.c: 987
		/* /Users/rubber/linux/block/bfq-wf2q.c: 991
		 * entity is about to be inserted into a service tree, /Users/rubber/linux/block/bfq-wf2q.c: 992
		 * and then set in service: get a reference to make /Users/rubber/linux/block/bfq-wf2q.c: 993
		 * sure entity does not disappear until it is no /Users/rubber/linux/block/bfq-wf2q.c: 994
		 * longer in service or scheduled for service. /Users/rubber/linux/block/bfq-wf2q.c: 995
 * __bfq_requeue_entity - handle requeueing or repositioning of an entity. /Users/rubber/linux/block/bfq-wf2q.c: 1019
 * @entity: the entity being requeued or repositioned. /Users/rubber/linux/block/bfq-wf2q.c: 1020
 * Requeueing is needed if this entity stops being served, which /Users/rubber/linux/block/bfq-wf2q.c: 1022
 * happens if a leaf descendant entity has expired. On the other hand, /Users/rubber/linux/block/bfq-wf2q.c: 1023
 * repositioning is needed if the next_inservice_entity for the child /Users/rubber/linux/block/bfq-wf2q.c: 1024
 * entity has changed. See the comments inside the function for /Users/rubber/linux/block/bfq-wf2q.c: 1025
 * details. /Users/rubber/linux/block/bfq-wf2q.c: 1026
 * Basically, this function: 1) removes entity from its active tree if /Users/rubber/linux/block/bfq-wf2q.c: 1028
 * present there, 2) updates the timestamps of entity and 3) inserts /Users/rubber/linux/block/bfq-wf2q.c: 1029
 * entity back into its active tree (in the new, right position for /Users/rubber/linux/block/bfq-wf2q.c: 1030
 * the new values of the timestamps). /Users/rubber/linux/block/bfq-wf2q.c: 1031
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1039
		 * We are requeueing the current in-service entity, /Users/rubber/linux/block/bfq-wf2q.c: 1040
		 * which may have to be done for one of the following /Users/rubber/linux/block/bfq-wf2q.c: 1041
		 * reasons: /Users/rubber/linux/block/bfq-wf2q.c: 1042
		 * - entity represents the in-service queue, and the /Users/rubber/linux/block/bfq-wf2q.c: 1043
		 *   in-service queue is being requeued after an /Users/rubber/linux/block/bfq-wf2q.c: 1044
		 *   expiration; /Users/rubber/linux/block/bfq-wf2q.c: 1045
		 * - entity represents a group, and its budget has /Users/rubber/linux/block/bfq-wf2q.c: 1046
		 *   changed because one of its child entities has /Users/rubber/linux/block/bfq-wf2q.c: 1047
		 *   just been either activated or requeued for some /Users/rubber/linux/block/bfq-wf2q.c: 1048
		 *   reason; the timestamps of the entity need then to /Users/rubber/linux/block/bfq-wf2q.c: 1049
		 *   be updated, and the entity needs to be enqueued /Users/rubber/linux/block/bfq-wf2q.c: 1050
		 *   or repositioned accordingly. /Users/rubber/linux/block/bfq-wf2q.c: 1051
		 * /Users/rubber/linux/block/bfq-wf2q.c: 1052
		 * In particular, before requeueing, the start time of /Users/rubber/linux/block/bfq-wf2q.c: 1053
		 * the entity must be moved forward to account for the /Users/rubber/linux/block/bfq-wf2q.c: 1054
		 * service that the entity has received while in /Users/rubber/linux/block/bfq-wf2q.c: 1055
		 * service. This is done by the next instructions. The /Users/rubber/linux/block/bfq-wf2q.c: 1056
		 * finish time will then be updated according to this /Users/rubber/linux/block/bfq-wf2q.c: 1057
		 * new value of the start time, and to the budget of /Users/rubber/linux/block/bfq-wf2q.c: 1058
		 * the entity. /Users/rubber/linux/block/bfq-wf2q.c: 1059
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1063
		 * In addition, if the entity had more than one child /Users/rubber/linux/block/bfq-wf2q.c: 1064
		 * when set in service, then it was not extracted from /Users/rubber/linux/block/bfq-wf2q.c: 1065
		 * the active tree. This implies that the position of /Users/rubber/linux/block/bfq-wf2q.c: 1066
		 * the entity in the active tree may need to be /Users/rubber/linux/block/bfq-wf2q.c: 1067
		 * changed now, because we have just updated the start /Users/rubber/linux/block/bfq-wf2q.c: 1068
		 * time of the entity, and we will update its finish /Users/rubber/linux/block/bfq-wf2q.c: 1069
		 * time in a moment (the requeueing is then, more /Users/rubber/linux/block/bfq-wf2q.c: 1070
		 * precisely, a repositioning in this case). To /Users/rubber/linux/block/bfq-wf2q.c: 1071
		 * implement this repositioning, we: 1) dequeue the /Users/rubber/linux/block/bfq-wf2q.c: 1072
		 * entity here, 2) update the finish time and requeue /Users/rubber/linux/block/bfq-wf2q.c: 1073
		 * the entity according to the new timestamps below. /Users/rubber/linux/block/bfq-wf2q.c: 1074
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1079
		 * In this case, this function gets called only if the /Users/rubber/linux/block/bfq-wf2q.c: 1080
		 * next_in_service entity below this entity has /Users/rubber/linux/block/bfq-wf2q.c: 1081
		 * changed, and this change has caused the budget of /Users/rubber/linux/block/bfq-wf2q.c: 1082
		 * this entity to change, which, finally implies that /Users/rubber/linux/block/bfq-wf2q.c: 1083
		 * the finish time of this entity must be /Users/rubber/linux/block/bfq-wf2q.c: 1084
		 * updated. Such an update may cause the scheduling, /Users/rubber/linux/block/bfq-wf2q.c: 1085
		 * i.e., the position in the active tree, of this /Users/rubber/linux/block/bfq-wf2q.c: 1086
		 * entity to change. We handle this change by: 1) /Users/rubber/linux/block/bfq-wf2q.c: 1087
		 * dequeueing the entity here, 2) updating the finish /Users/rubber/linux/block/bfq-wf2q.c: 1088
		 * time and requeueing the entity according to the new /Users/rubber/linux/block/bfq-wf2q.c: 1089
		 * timestamps below. This is the same approach as the /Users/rubber/linux/block/bfq-wf2q.c: 1090
		 * non-extracted-entity sub-case above. /Users/rubber/linux/block/bfq-wf2q.c: 1091
		 /* /Users/rubber/linux/block/bfq-wf2q.c: 1106
		  * in service or already queued on the active tree, /Users/rubber/linux/block/bfq-wf2q.c: 1107
		  * requeue or reposition /Users/rubber/linux/block/bfq-wf2q.c: 1108
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1112
		 * Not in service and not queued on its active tree: /Users/rubber/linux/block/bfq-wf2q.c: 1113
		 * the activity is idle and this is a true activation. /Users/rubber/linux/block/bfq-wf2q.c: 1114
 * bfq_activate_requeue_entity - activate or requeue an entity representing a /Users/rubber/linux/block/bfq-wf2q.c: 1121
 *				 bfq_queue, and activate, requeue or reposition /Users/rubber/linux/block/bfq-wf2q.c: 1122
 *				 all ancestors for which such an update becomes /Users/rubber/linux/block/bfq-wf2q.c: 1123
 *				 necessary. /Users/rubber/linux/block/bfq-wf2q.c: 1124
 * @entity: the entity to activate. /Users/rubber/linux/block/bfq-wf2q.c: 1125
 * @non_blocking_wait_rq: true if this entity was waiting for a request /Users/rubber/linux/block/bfq-wf2q.c: 1126
 * @requeue: true if this is a requeue, which implies that bfqq is /Users/rubber/linux/block/bfq-wf2q.c: 1127
 *	     being expired; thus ALL its ancestors stop being served and must /Users/rubber/linux/block/bfq-wf2q.c: 1128
 *	     therefore be requeued /Users/rubber/linux/block/bfq-wf2q.c: 1129
 * @expiration: true if this function is being invoked in the expiration path /Users/rubber/linux/block/bfq-wf2q.c: 1130
 *             of the in-service queue /Users/rubber/linux/block/bfq-wf2q.c: 1131
 * __bfq_deactivate_entity - update sched_data and service trees for /Users/rubber/linux/block/bfq-wf2q.c: 1150
 * entity, so as to represent entity as inactive /Users/rubber/linux/block/bfq-wf2q.c: 1151
 * @entity: the entity being deactivated. /Users/rubber/linux/block/bfq-wf2q.c: 1152
 * @ins_into_idle_tree: if false, the entity will not be put into the /Users/rubber/linux/block/bfq-wf2q.c: 1153
 *			idle tree. /Users/rubber/linux/block/bfq-wf2q.c: 1154
 * If necessary and allowed, puts entity into the idle tree. NOTE: /Users/rubber/linux/block/bfq-wf2q.c: 1156
 * entity may be on no tree if in service. /Users/rubber/linux/block/bfq-wf2q.c: 1157
	if (!entity->on_st_or_in_serv) /* /Users/rubber/linux/block/bfq-wf2q.c: 1165
					* entity never activated, or /Users/rubber/linux/block/bfq-wf2q.c: 1166
					* already inactive /Users/rubber/linux/block/bfq-wf2q.c: 1167
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1171
	 * If we get here, then entity is active, which implies that /Users/rubber/linux/block/bfq-wf2q.c: 1172
	 * bfq_group_set_parent has already been invoked for the group /Users/rubber/linux/block/bfq-wf2q.c: 1173
	 * represented by entity. Therefore, the field /Users/rubber/linux/block/bfq-wf2q.c: 1174
	 * entity->sched_data has been set, and we can safely use it. /Users/rubber/linux/block/bfq-wf2q.c: 1175
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1185
		 * Non in-service entity: nobody will take care of /Users/rubber/linux/block/bfq-wf2q.c: 1186
		 * resetting its service counter on expiration. Do it /Users/rubber/linux/block/bfq-wf2q.c: 1187
		 * now. /Users/rubber/linux/block/bfq-wf2q.c: 1188
 * bfq_deactivate_entity - deactivate an entity representing a bfq_queue. /Users/rubber/linux/block/bfq-wf2q.c: 1206
 * @entity: the entity to deactivate. /Users/rubber/linux/block/bfq-wf2q.c: 1207
 * @ins_into_idle_tree: true if the entity can be put into the idle tree /Users/rubber/linux/block/bfq-wf2q.c: 1208
 * @expiration: true if this function is being invoked in the expiration path /Users/rubber/linux/block/bfq-wf2q.c: 1209
 *             of the in-service queue /Users/rubber/linux/block/bfq-wf2q.c: 1210
			/* /Users/rubber/linux/block/bfq-wf2q.c: 1223
			 * entity is not in any tree any more, so /Users/rubber/linux/block/bfq-wf2q.c: 1224
			 * this deactivation is a no-op, and there is /Users/rubber/linux/block/bfq-wf2q.c: 1225
			 * nothing to change for upper-level entities /Users/rubber/linux/block/bfq-wf2q.c: 1226
			 * (in case of expiration, this can never /Users/rubber/linux/block/bfq-wf2q.c: 1227
			 * happen). /Users/rubber/linux/block/bfq-wf2q.c: 1228
			/* /Users/rubber/linux/block/bfq-wf2q.c: 1234
			 * entity was the next_in_service entity, /Users/rubber/linux/block/bfq-wf2q.c: 1235
			 * then, since entity has just been /Users/rubber/linux/block/bfq-wf2q.c: 1236
			 * deactivated, a new one must be found. /Users/rubber/linux/block/bfq-wf2q.c: 1237
			/* /Users/rubber/linux/block/bfq-wf2q.c: 1242
			 * The parent entity is still active, because /Users/rubber/linux/block/bfq-wf2q.c: 1243
			 * either next_in_service or in_service_entity /Users/rubber/linux/block/bfq-wf2q.c: 1244
			 * is not NULL. So, no further upwards /Users/rubber/linux/block/bfq-wf2q.c: 1245
			 * deactivation must be performed.  Yet, /Users/rubber/linux/block/bfq-wf2q.c: 1246
			 * next_in_service has changed.	Then the /Users/rubber/linux/block/bfq-wf2q.c: 1247
			 * schedule does need to be updated upwards. /Users/rubber/linux/block/bfq-wf2q.c: 1248
			 * /Users/rubber/linux/block/bfq-wf2q.c: 1249
			 * NOTE If in_service_entity is not NULL, then /Users/rubber/linux/block/bfq-wf2q.c: 1250
			 * next_in_service may happen to be NULL, /Users/rubber/linux/block/bfq-wf2q.c: 1251
			 * although the parent entity is evidently /Users/rubber/linux/block/bfq-wf2q.c: 1252
			 * active. This happens if 1) the entity /Users/rubber/linux/block/bfq-wf2q.c: 1253
			 * pointed by in_service_entity is the only /Users/rubber/linux/block/bfq-wf2q.c: 1254
			 * active entity in the parent entity, and 2) /Users/rubber/linux/block/bfq-wf2q.c: 1255
			 * according to the definition of /Users/rubber/linux/block/bfq-wf2q.c: 1256
			 * next_in_service, the in_service_entity /Users/rubber/linux/block/bfq-wf2q.c: 1257
			 * cannot be considered as /Users/rubber/linux/block/bfq-wf2q.c: 1258
			 * next_in_service. See the comments on the /Users/rubber/linux/block/bfq-wf2q.c: 1259
			 * definition of next_in_service for details. /Users/rubber/linux/block/bfq-wf2q.c: 1260
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1265
		 * If we get here, then the parent is no more /Users/rubber/linux/block/bfq-wf2q.c: 1266
		 * backlogged and we need to propagate the /Users/rubber/linux/block/bfq-wf2q.c: 1267
		 * deactivation upwards. Thus let the loop go on. /Users/rubber/linux/block/bfq-wf2q.c: 1268
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1271
		 * Also let parent be queued into the idle tree on /Users/rubber/linux/block/bfq-wf2q.c: 1272
		 * deactivation, to preserve service guarantees, and /Users/rubber/linux/block/bfq-wf2q.c: 1273
		 * assuming that who invoked this function does not /Users/rubber/linux/block/bfq-wf2q.c: 1274
		 * need parent entities too to be removed completely. /Users/rubber/linux/block/bfq-wf2q.c: 1275
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1280
	 * If the deactivation loop is fully executed, then there are /Users/rubber/linux/block/bfq-wf2q.c: 1281
	 * no more entities to touch and next loop is not executed at /Users/rubber/linux/block/bfq-wf2q.c: 1282
	 * all. Otherwise, requeue remaining entities if they are /Users/rubber/linux/block/bfq-wf2q.c: 1283
	 * about to stop receiving service, or reposition them if this /Users/rubber/linux/block/bfq-wf2q.c: 1284
	 * is not the case. /Users/rubber/linux/block/bfq-wf2q.c: 1285
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1289
		 * Invoke __bfq_requeue_entity on entity, even if /Users/rubber/linux/block/bfq-wf2q.c: 1290
		 * already active, to requeue/reposition it in the /Users/rubber/linux/block/bfq-wf2q.c: 1291
		 * active tree (because sd->next_in_service has /Users/rubber/linux/block/bfq-wf2q.c: 1292
		 * changed) /Users/rubber/linux/block/bfq-wf2q.c: 1293
			/* /Users/rubber/linux/block/bfq-wf2q.c: 1300
			 * next_in_service unchanged or not causing /Users/rubber/linux/block/bfq-wf2q.c: 1301
			 * any change in entity->parent->sd, and no /Users/rubber/linux/block/bfq-wf2q.c: 1302
			 * requeueing needed for expiration: stop /Users/rubber/linux/block/bfq-wf2q.c: 1303
			 * here. /Users/rubber/linux/block/bfq-wf2q.c: 1304
 * bfq_calc_vtime_jump - compute the value to which the vtime should jump, /Users/rubber/linux/block/bfq-wf2q.c: 1311
 *                       if needed, to have at least one entity eligible. /Users/rubber/linux/block/bfq-wf2q.c: 1312
 * @st: the service tree to act upon. /Users/rubber/linux/block/bfq-wf2q.c: 1313
 * Assumes that st is not empty. /Users/rubber/linux/block/bfq-wf2q.c: 1315
 * bfq_first_active_entity - find the eligible entity with /Users/rubber/linux/block/bfq-wf2q.c: 1336
 *                           the smallest finish time /Users/rubber/linux/block/bfq-wf2q.c: 1337
 * @st: the service tree to select from. /Users/rubber/linux/block/bfq-wf2q.c: 1338
 * @vtime: the system virtual to use as a reference for eligibility /Users/rubber/linux/block/bfq-wf2q.c: 1339
 * This function searches the first schedulable entity, starting from the /Users/rubber/linux/block/bfq-wf2q.c: 1341
 * root of the tree and going on the left every time on this side there is /Users/rubber/linux/block/bfq-wf2q.c: 1342
 * a subtree with at least one eligible (start <= vtime) entity. The path on /Users/rubber/linux/block/bfq-wf2q.c: 1343
 * the right is followed only if a) the left subtree contains no eligible /Users/rubber/linux/block/bfq-wf2q.c: 1344
 * entities and b) no eligible entity has been found yet. /Users/rubber/linux/block/bfq-wf2q.c: 1345
 * __bfq_lookup_next_entity - return the first eligible entity in @st. /Users/rubber/linux/block/bfq-wf2q.c: 1376
 * @st: the service tree. /Users/rubber/linux/block/bfq-wf2q.c: 1377
 * If there is no in-service entity for the sched_data st belongs to, /Users/rubber/linux/block/bfq-wf2q.c: 1379
 * then return the entity that will be set in service if: /Users/rubber/linux/block/bfq-wf2q.c: 1380
 * 1) the parent entity this st belongs to is set in service; /Users/rubber/linux/block/bfq-wf2q.c: 1381
 * 2) no entity belonging to such parent entity undergoes a state change /Users/rubber/linux/block/bfq-wf2q.c: 1382
 * that would influence the timestamps of the entity (e.g., becomes idle, /Users/rubber/linux/block/bfq-wf2q.c: 1383
 * becomes backlogged, changes its budget, ...). /Users/rubber/linux/block/bfq-wf2q.c: 1384
 * In this first case, update the virtual time in @st too (see the /Users/rubber/linux/block/bfq-wf2q.c: 1386
 * comments on this update inside the function). /Users/rubber/linux/block/bfq-wf2q.c: 1387
 * In contrast, if there is an in-service entity, then return the /Users/rubber/linux/block/bfq-wf2q.c: 1389
 * entity that would be set in service if not only the above /Users/rubber/linux/block/bfq-wf2q.c: 1390
 * conditions, but also the next one held true: the currently /Users/rubber/linux/block/bfq-wf2q.c: 1391
 * in-service entity, on expiration, /Users/rubber/linux/block/bfq-wf2q.c: 1392
 * 1) gets a finish time equal to the current one, or /Users/rubber/linux/block/bfq-wf2q.c: 1393
 * 2) is not eligible any more, or /Users/rubber/linux/block/bfq-wf2q.c: 1394
 * 3) is idle. /Users/rubber/linux/block/bfq-wf2q.c: 1395
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1406
	 * Get the value of the system virtual time for which at /Users/rubber/linux/block/bfq-wf2q.c: 1407
	 * least one entity is eligible. /Users/rubber/linux/block/bfq-wf2q.c: 1408
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1412
	 * If there is no in-service entity for the sched_data this /Users/rubber/linux/block/bfq-wf2q.c: 1413
	 * active tree belongs to, then push the system virtual time /Users/rubber/linux/block/bfq-wf2q.c: 1414
	 * up to the value that guarantees that at least one entity is /Users/rubber/linux/block/bfq-wf2q.c: 1415
	 * eligible. If, instead, there is an in-service entity, then /Users/rubber/linux/block/bfq-wf2q.c: 1416
	 * do not make any such update, because there is already an /Users/rubber/linux/block/bfq-wf2q.c: 1417
	 * eligible entity, namely the in-service one (even if the /Users/rubber/linux/block/bfq-wf2q.c: 1418
	 * entity is not on st, because it was extracted when set in /Users/rubber/linux/block/bfq-wf2q.c: 1419
	 * service). /Users/rubber/linux/block/bfq-wf2q.c: 1420
 * bfq_lookup_next_entity - return the first eligible entity in @sd. /Users/rubber/linux/block/bfq-wf2q.c: 1431
 * @sd: the sched_data. /Users/rubber/linux/block/bfq-wf2q.c: 1432
 * @expiration: true if we are on the expiration path of the in-service queue /Users/rubber/linux/block/bfq-wf2q.c: 1433
 * This function is invoked when there has been a change in the trees /Users/rubber/linux/block/bfq-wf2q.c: 1435
 * for sd, and we need to know what is the new next entity to serve /Users/rubber/linux/block/bfq-wf2q.c: 1436
 * after this change. /Users/rubber/linux/block/bfq-wf2q.c: 1437
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1447
	 * Choose from idle class, if needed to guarantee a minimum /Users/rubber/linux/block/bfq-wf2q.c: 1448
	 * bandwidth to this class (and if there is some active entity /Users/rubber/linux/block/bfq-wf2q.c: 1449
	 * in idle class). This should also mitigate /Users/rubber/linux/block/bfq-wf2q.c: 1450
	 * priority-inversion problems in case a low priority task is /Users/rubber/linux/block/bfq-wf2q.c: 1451
	 * holding file system resources. /Users/rubber/linux/block/bfq-wf2q.c: 1452
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1462
	 * Find the next entity to serve for the highest-priority /Users/rubber/linux/block/bfq-wf2q.c: 1463
	 * class, unless the idle class needs to be served. /Users/rubber/linux/block/bfq-wf2q.c: 1464
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1467
		 * If expiration is true, then bfq_lookup_next_entity /Users/rubber/linux/block/bfq-wf2q.c: 1468
		 * is being invoked as a part of the expiration path /Users/rubber/linux/block/bfq-wf2q.c: 1469
		 * of the in-service queue. In this case, even if /Users/rubber/linux/block/bfq-wf2q.c: 1470
		 * sd->in_service_entity is not NULL, /Users/rubber/linux/block/bfq-wf2q.c: 1471
		 * sd->in_service_entity at this point is actually not /Users/rubber/linux/block/bfq-wf2q.c: 1472
		 * in service any more, and, if needed, has already /Users/rubber/linux/block/bfq-wf2q.c: 1473
		 * been properly queued or requeued into the right /Users/rubber/linux/block/bfq-wf2q.c: 1474
		 * tree. The reason why sd->in_service_entity is still /Users/rubber/linux/block/bfq-wf2q.c: 1475
		 * not NULL here, even if expiration is true, is that /Users/rubber/linux/block/bfq-wf2q.c: 1476
		 * sd->in_service_entity is reset as a last step in the /Users/rubber/linux/block/bfq-wf2q.c: 1477
		 * expiration path. So, if expiration is true, tell /Users/rubber/linux/block/bfq-wf2q.c: 1478
		 * __bfq_lookup_next_entity that there is no /Users/rubber/linux/block/bfq-wf2q.c: 1479
		 * sd->in_service_entity. /Users/rubber/linux/block/bfq-wf2q.c: 1480
 * Get next queue for service. /Users/rubber/linux/block/bfq-wf2q.c: 1504
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1515
	 * Traverse the path from the root to the leaf entity to /Users/rubber/linux/block/bfq-wf2q.c: 1516
	 * serve. Set in service all the entities visited along the /Users/rubber/linux/block/bfq-wf2q.c: 1517
	 * way. /Users/rubber/linux/block/bfq-wf2q.c: 1518
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1522
		 * WARNING. We are about to set the in-service entity /Users/rubber/linux/block/bfq-wf2q.c: 1523
		 * to sd->next_in_service, i.e., to the (cached) value /Users/rubber/linux/block/bfq-wf2q.c: 1524
		 * returned by bfq_lookup_next_entity(sd) the last /Users/rubber/linux/block/bfq-wf2q.c: 1525
		 * time it was invoked, i.e., the last time when the /Users/rubber/linux/block/bfq-wf2q.c: 1526
		 * service order in sd changed as a consequence of the /Users/rubber/linux/block/bfq-wf2q.c: 1527
		 * activation or deactivation of an entity. In this /Users/rubber/linux/block/bfq-wf2q.c: 1528
		 * respect, if we execute bfq_lookup_next_entity(sd) /Users/rubber/linux/block/bfq-wf2q.c: 1529
		 * in this very moment, it may, although with low /Users/rubber/linux/block/bfq-wf2q.c: 1530
		 * probability, yield a different entity than that /Users/rubber/linux/block/bfq-wf2q.c: 1531
		 * pointed to by sd->next_in_service. This rare event /Users/rubber/linux/block/bfq-wf2q.c: 1532
		 * happens in case there was no CLASS_IDLE entity to /Users/rubber/linux/block/bfq-wf2q.c: 1533
		 * serve for sd when bfq_lookup_next_entity(sd) was /Users/rubber/linux/block/bfq-wf2q.c: 1534
		 * invoked for the last time, while there is now one /Users/rubber/linux/block/bfq-wf2q.c: 1535
		 * such entity. /Users/rubber/linux/block/bfq-wf2q.c: 1536
		 * /Users/rubber/linux/block/bfq-wf2q.c: 1537
		 * If the above event happens, then the scheduling of /Users/rubber/linux/block/bfq-wf2q.c: 1538
		 * such entity in CLASS_IDLE is postponed until the /Users/rubber/linux/block/bfq-wf2q.c: 1539
		 * service of the sd->next_in_service entity /Users/rubber/linux/block/bfq-wf2q.c: 1540
		 * finishes. In fact, when the latter is expired, /Users/rubber/linux/block/bfq-wf2q.c: 1541
		 * bfq_lookup_next_entity(sd) gets called again, /Users/rubber/linux/block/bfq-wf2q.c: 1542
		 * exactly to update sd->next_in_service. /Users/rubber/linux/block/bfq-wf2q.c: 1543
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1550
		 * If entity is no longer a candidate for next /Users/rubber/linux/block/bfq-wf2q.c: 1551
		 * service, then it must be extracted from its active /Users/rubber/linux/block/bfq-wf2q.c: 1552
		 * tree, so as to make sure that it won't be /Users/rubber/linux/block/bfq-wf2q.c: 1553
		 * considered when computing next_in_service. See the /Users/rubber/linux/block/bfq-wf2q.c: 1554
		 * comments on the function /Users/rubber/linux/block/bfq-wf2q.c: 1555
		 * bfq_no_longer_next_in_service() for details. /Users/rubber/linux/block/bfq-wf2q.c: 1556
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1562
		 * Even if entity is not to be extracted according to /Users/rubber/linux/block/bfq-wf2q.c: 1563
		 * the above check, a descendant entity may get /Users/rubber/linux/block/bfq-wf2q.c: 1564
		 * extracted in one of the next iterations of this /Users/rubber/linux/block/bfq-wf2q.c: 1565
		 * loop. Such an event could cause a change in /Users/rubber/linux/block/bfq-wf2q.c: 1566
		 * next_in_service for the level of the descendant /Users/rubber/linux/block/bfq-wf2q.c: 1567
		 * entity, and thus possibly back to this level. /Users/rubber/linux/block/bfq-wf2q.c: 1568
		 * /Users/rubber/linux/block/bfq-wf2q.c: 1569
		 * However, we cannot perform the resulting needed /Users/rubber/linux/block/bfq-wf2q.c: 1570
		 * update of next_in_service for this level before the /Users/rubber/linux/block/bfq-wf2q.c: 1571
		 * end of the whole loop, because, to know which is /Users/rubber/linux/block/bfq-wf2q.c: 1572
		 * the correct next-to-serve candidate entity for each /Users/rubber/linux/block/bfq-wf2q.c: 1573
		 * level, we need first to find the leaf entity to set /Users/rubber/linux/block/bfq-wf2q.c: 1574
		 * in service. In fact, only after we know which is /Users/rubber/linux/block/bfq-wf2q.c: 1575
		 * the next-to-serve leaf entity, we can discover /Users/rubber/linux/block/bfq-wf2q.c: 1576
		 * whether the parent entity of the leaf entity /Users/rubber/linux/block/bfq-wf2q.c: 1577
		 * becomes the next-to-serve, and so on. /Users/rubber/linux/block/bfq-wf2q.c: 1578
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1584
	 * We can finally update all next-to-serve entities along the /Users/rubber/linux/block/bfq-wf2q.c: 1585
	 * path from the leaf entity just set in service to the root. /Users/rubber/linux/block/bfq-wf2q.c: 1586
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1609
	 * When this function is called, all in-service entities have /Users/rubber/linux/block/bfq-wf2q.c: 1610
	 * been properly deactivated or requeued, so we can safely /Users/rubber/linux/block/bfq-wf2q.c: 1611
	 * execute the final step: reset in_service_entity along the /Users/rubber/linux/block/bfq-wf2q.c: 1612
	 * path from entity to the root. /Users/rubber/linux/block/bfq-wf2q.c: 1613
	/* /Users/rubber/linux/block/bfq-wf2q.c: 1618
	 * in_serv_entity is no longer in service, so, if it is in no /Users/rubber/linux/block/bfq-wf2q.c: 1619
	 * service tree either, then release the service reference to /Users/rubber/linux/block/bfq-wf2q.c: 1620
	 * the queue it represents (taken with bfq_get_entity). /Users/rubber/linux/block/bfq-wf2q.c: 1621
		/* /Users/rubber/linux/block/bfq-wf2q.c: 1624
		 * If no process is referencing in_serv_bfqq any /Users/rubber/linux/block/bfq-wf2q.c: 1625
		 * longer, then the service reference may be the only /Users/rubber/linux/block/bfq-wf2q.c: 1626
		 * reference to the queue. If this is the case, then /Users/rubber/linux/block/bfq-wf2q.c: 1627
		 * bfqq gets freed here. /Users/rubber/linux/block/bfq-wf2q.c: 1628
 * Called when the bfqq no longer has requests pending, remove it from /Users/rubber/linux/block/bfq-wf2q.c: 1666
 * the service tree. As a special case, it can be invoked during an /Users/rubber/linux/block/bfq-wf2q.c: 1667
 * expiration. /Users/rubber/linux/block/bfq-wf2q.c: 1668
 * Called when an inactive queue receives a new request. /Users/rubber/linux/block/bfq-wf2q.c: 1691
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/block/holder.c: 1
 * bd_link_disk_holder - create symlinks between holding disk and slave bdev /Users/rubber/linux/block/holder.c: 46
 * @bdev: the claimed slave bdev /Users/rubber/linux/block/holder.c: 47
 * @disk: the holding disk /Users/rubber/linux/block/holder.c: 48
 * DON'T USE THIS UNLESS YOU'RE ALREADY USING IT. /Users/rubber/linux/block/holder.c: 50
 * This functions creates the following sysfs symlinks. /Users/rubber/linux/block/holder.c: 52
 * - from "slaves" directory of the holder @disk to the claimed @bdev /Users/rubber/linux/block/holder.c: 54
 * - from "holders" directory of the @bdev to the holder @disk /Users/rubber/linux/block/holder.c: 55
 * For example, if /dev/dm-0 maps to /dev/sda and disk for dm-0 is /Users/rubber/linux/block/holder.c: 57
 * passed to bd_link_disk_holder(), then: /Users/rubber/linux/block/holder.c: 58
 *   /sys/block/dm-0/slaves/sda --> /sys/block/sda /Users/rubber/linux/block/holder.c: 60
 *   /sys/block/sda/holders/dm-0 --> /sys/block/dm-0 /Users/rubber/linux/block/holder.c: 61
 * The caller must have claimed @bdev before calling this function and /Users/rubber/linux/block/holder.c: 63
 * ensure that both @bdev and @disk are valid during the creation and /Users/rubber/linux/block/holder.c: 64
 * lifetime of these symlinks. /Users/rubber/linux/block/holder.c: 65
 * CONTEXT: /Users/rubber/linux/block/holder.c: 67
 * Might sleep. /Users/rubber/linux/block/holder.c: 68
 * RETURNS: /Users/rubber/linux/block/holder.c: 70
 * 0 on success, -errno on failure. /Users/rubber/linux/block/holder.c: 71
	/* /Users/rubber/linux/block/holder.c: 110
	 * del_gendisk drops the initial reference to bd_holder_dir, so we need /Users/rubber/linux/block/holder.c: 111
	 * to keep our own here to allow for cleanup past that point. /Users/rubber/linux/block/holder.c: 112
 * bd_unlink_disk_holder - destroy symlinks created by bd_link_disk_holder() /Users/rubber/linux/block/holder.c: 130
 * @bdev: the calimed slave bdev /Users/rubber/linux/block/holder.c: 131
 * @disk: the holding disk /Users/rubber/linux/block/holder.c: 132
 * DON'T USE THIS UNLESS YOU'RE ALREADY USING IT. /Users/rubber/linux/block/holder.c: 134
 * CONTEXT: /Users/rubber/linux/block/holder.c: 136
 * Might sleep. /Users/rubber/linux/block/holder.c: 137
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-debugfs.c: 1
 * Copyright (C) 2017 Facebook /Users/rubber/linux/block/blk-mq-debugfs.c: 3
	/* /Users/rubber/linux/block/blk-mq-debugfs.c: 152
	 * The "state" attribute is removed after blk_cleanup_queue() has called /Users/rubber/linux/block/blk-mq-debugfs.c: 153
	 * blk_mq_free_queue(). Return if QUEUE_FLAG_DEAD has been set to avoid /Users/rubber/linux/block/blk-mq-debugfs.c: 154
	 * triggering a use-after-free. /Users/rubber/linux/block/blk-mq-debugfs.c: 155
 * Note: the state of a request may change while this function is in progress, /Users/rubber/linux/block/blk-mq-debugfs.c: 399
 * e.g. due to a concurrent blk_mq_finish_request() call. Returns true to /Users/rubber/linux/block/blk-mq-debugfs.c: 400
 * keep iterating requests. /Users/rubber/linux/block/blk-mq-debugfs.c: 401
	/* /Users/rubber/linux/block/blk-mq-debugfs.c: 617
	 * Attributes that only implement .seq_ops are read-only and 'attr' is /Users/rubber/linux/block/blk-mq-debugfs.c: 618
	 * the same with 'data' in this case. /Users/rubber/linux/block/blk-mq-debugfs.c: 619
	/* /Users/rubber/linux/block/blk-mq-debugfs.c: 711
	 * blk_mq_init_sched() attempted to do this already, but q->debugfs_dir /Users/rubber/linux/block/blk-mq-debugfs.c: 712
	 * didn't exist yet (because we don't know what to name the directory /Users/rubber/linux/block/blk-mq-debugfs.c: 713
	 * until the queue is registered to a gendisk). /Users/rubber/linux/block/blk-mq-debugfs.c: 714
	/* /Users/rubber/linux/block/blk-mq-debugfs.c: 799
	 * If the parent directory has not been created yet, return, we will be /Users/rubber/linux/block/blk-mq-debugfs.c: 800
	 * called again later on and the directory/files will be created then. /Users/rubber/linux/block/blk-mq-debugfs.c: 801
	/* /Users/rubber/linux/block/blk-mq-debugfs.c: 870
	 * If the parent debugfs directory has not been created yet, return; /Users/rubber/linux/block/blk-mq-debugfs.c: 871
	 * We will be called again later on with appropriate parent debugfs /Users/rubber/linux/block/blk-mq-debugfs.c: 872
	 * directory from blk_register_queue() /Users/rubber/linux/block/blk-mq-debugfs.c: 873
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-timeout.c: 1
 * Functions related to generic timeout handling of requests. /Users/rubber/linux/block/blk-timeout.c: 3
 * blk_abort_request - Request recovery for the specified command /Users/rubber/linux/block/blk-timeout.c: 71
 * @req:	pointer to the request of interest /Users/rubber/linux/block/blk-timeout.c: 72
 * This function requests that the block layer start recovery for the /Users/rubber/linux/block/blk-timeout.c: 74
 * request by deleting the timer and calling the q's timeout function. /Users/rubber/linux/block/blk-timeout.c: 75
 * LLDDs who implement their own error recovery MAY ignore the timeout /Users/rubber/linux/block/blk-timeout.c: 76
 * event if they generated blk_abort_request. /Users/rubber/linux/block/blk-timeout.c: 77
	/* /Users/rubber/linux/block/blk-timeout.c: 81
	 * All we need to ensure is that timeout scan takes place /Users/rubber/linux/block/blk-timeout.c: 82
	 * immediately and that scan sees the new timeout value. /Users/rubber/linux/block/blk-timeout.c: 83
	 * No need for fancy synchronizations. /Users/rubber/linux/block/blk-timeout.c: 84
 * Just a rough estimate, we don't care about specific values for timeouts. /Users/rubber/linux/block/blk-timeout.c: 102
 * blk_add_timer - Start timeout timer for a single request /Users/rubber/linux/block/blk-timeout.c: 121
 * @req:	request that is about to start running. /Users/rubber/linux/block/blk-timeout.c: 122
 * Notes: /Users/rubber/linux/block/blk-timeout.c: 124
 *    Each request has its own timer, and as it is added to the queue, we /Users/rubber/linux/block/blk-timeout.c: 125
 *    set up the timer. When the request completes, we cancel the timer. /Users/rubber/linux/block/blk-timeout.c: 126
	/* /Users/rubber/linux/block/blk-timeout.c: 133
	 * Some LLDs, like scsi, peek at the timeout to prevent a /Users/rubber/linux/block/blk-timeout.c: 134
	 * command from being retried forever. /Users/rubber/linux/block/blk-timeout.c: 135
	/* /Users/rubber/linux/block/blk-timeout.c: 145
	 * If the timer isn't already pending or this timeout is earlier /Users/rubber/linux/block/blk-timeout.c: 146
	 * than an existing one, modify the timer. Round up to next nearest /Users/rubber/linux/block/blk-timeout.c: 147
	 * second. /Users/rubber/linux/block/blk-timeout.c: 148
		/* /Users/rubber/linux/block/blk-timeout.c: 156
		 * Due to added timer slack to group timers, the timer /Users/rubber/linux/block/blk-timeout.c: 157
		 * will often be a little in front of what we asked for. /Users/rubber/linux/block/blk-timeout.c: 158
		 * So apply some tolerance here too, otherwise we keep /Users/rubber/linux/block/blk-timeout.c: 159
		 * modifying the timer because expires for value X /Users/rubber/linux/block/blk-timeout.c: 160
		 * will be X + something. /Users/rubber/linux/block/blk-timeout.c: 161
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/genhd.c: 1
 *  gendisk handling /Users/rubber/linux/block/genhd.c: 3
 * Portions Copyright (C) 2020 Christoph Hellwig /Users/rubber/linux/block/genhd.c: 5
 * Unique, monotonically increasing sequential number associated with block /Users/rubber/linux/block/genhd.c: 35
 * devices instances (i.e. incremented each time a device is attached). /Users/rubber/linux/block/genhd.c: 36
 * Associating uevents with block devices in userspace is difficult and racy: /Users/rubber/linux/block/genhd.c: 37
 * the uevent netlink socket is lossy, and on slow and overloaded systems has /Users/rubber/linux/block/genhd.c: 38
 * a very high latency. /Users/rubber/linux/block/genhd.c: 39
 * Block devices do not have exclusive owners in userspace, any process can set /Users/rubber/linux/block/genhd.c: 40
 * one up (e.g. loop devices). Moreover, device names can be reused (e.g. loop0 /Users/rubber/linux/block/genhd.c: 41
 * can be reused again and again). /Users/rubber/linux/block/genhd.c: 42
 * A userspace process setting up a block device and watching for its events /Users/rubber/linux/block/genhd.c: 43
 * cannot thus reliably tell whether an event relates to the device it just set /Users/rubber/linux/block/genhd.c: 44
 * up or another earlier instance with the same name. /Users/rubber/linux/block/genhd.c: 45
 * This sequential number allows userspace processes to solve this problem, and /Users/rubber/linux/block/genhd.c: 46
 * uniquely associate an uevent to the lifetime to a device. /Users/rubber/linux/block/genhd.c: 47
 * Set disk capacity and notify if the size is not currently zero and will not /Users/rubber/linux/block/genhd.c: 67
 * be set to zero.  Returns true if a uevent was sent, otherwise false. /Users/rubber/linux/block/genhd.c: 68
	/* /Users/rubber/linux/block/genhd.c: 77
	 * Only print a message and send a uevent if the gendisk is user visible /Users/rubber/linux/block/genhd.c: 78
	 * and alive.  This avoids spamming the log and udev when setting the /Users/rubber/linux/block/genhd.c: 79
	 * initial capacity during probing. /Users/rubber/linux/block/genhd.c: 80
	/* /Users/rubber/linux/block/genhd.c: 90
	 * Historically we did not send a uevent for changes to/from an empty /Users/rubber/linux/block/genhd.c: 91
	 * device. /Users/rubber/linux/block/genhd.c: 92
 * Format the device name of the indicated block device into the supplied buffer /Users/rubber/linux/block/genhd.c: 102
 * and return a pointer to that same buffer for convenience. /Users/rubber/linux/block/genhd.c: 103
 * Note: do not use this in new code, use the %pg specifier to sprintf and /Users/rubber/linux/block/genhd.c: 105
 * printk insted. /Users/rubber/linux/block/genhd.c: 106
 * Can be deleted altogether. Later. /Users/rubber/linux/block/genhd.c: 178
 * __register_blkdev - register a new block device /Users/rubber/linux/block/genhd.c: 211
 * @major: the requested major device number [1..BLKDEV_MAJOR_MAX-1]. If /Users/rubber/linux/block/genhd.c: 213
 *         @major = 0, try to allocate any unused major number. /Users/rubber/linux/block/genhd.c: 214
 * @name: the name of the new block device as a zero terminated string /Users/rubber/linux/block/genhd.c: 215
 * @probe: pre-devtmpfs / pre-udev callback used to create disks when their /Users/rubber/linux/block/genhd.c: 216
 *	   pre-created device node is accessed. When a probe call uses /Users/rubber/linux/block/genhd.c: 217
 *	   add_disk() and it fails the driver must cleanup resources. This /Users/rubber/linux/block/genhd.c: 218
 *	   interface may soon be removed. /Users/rubber/linux/block/genhd.c: 219
 * The @name must be unique within the system. /Users/rubber/linux/block/genhd.c: 221
 * The return value depends on the @major input parameter: /Users/rubber/linux/block/genhd.c: 223
 *  - if a major device number was requested in range [1..BLKDEV_MAJOR_MAX-1] /Users/rubber/linux/block/genhd.c: 225
 *    then the function returns zero on success, or a negative error code /Users/rubber/linux/block/genhd.c: 226
 *  - if any unused major number was requested with @major = 0 parameter /Users/rubber/linux/block/genhd.c: 227
 *    then the return value is the allocated major number in range /Users/rubber/linux/block/genhd.c: 228
 *    [1..BLKDEV_MAJOR_MAX-1] or a negative error code otherwise /Users/rubber/linux/block/genhd.c: 229
 * See Documentation/admin-guide/devices.txt for the list of allocated /Users/rubber/linux/block/genhd.c: 231
 * major numbers. /Users/rubber/linux/block/genhd.c: 232
 * Use register_blkdev instead for any new code. /Users/rubber/linux/block/genhd.c: 234
 * device_add_disk - add disk information to kernel list /Users/rubber/linux/block/genhd.c: 389
 * @parent: parent device for the disk /Users/rubber/linux/block/genhd.c: 390
 * @disk: per-device partitioning information /Users/rubber/linux/block/genhd.c: 391
 * @groups: Additional per-device sysfs groups /Users/rubber/linux/block/genhd.c: 392
 * This function registers the partitioning information in @disk /Users/rubber/linux/block/genhd.c: 394
 * with the kernel. /Users/rubber/linux/block/genhd.c: 395
	/* /Users/rubber/linux/block/genhd.c: 404
	 * The disk queue should now be all set with enough information about /Users/rubber/linux/block/genhd.c: 405
	 * the device for the elevator code to pick an adequate default /Users/rubber/linux/block/genhd.c: 406
	 * elevator if one is needed, that is, for devices requesting queue /Users/rubber/linux/block/genhd.c: 407
	 * registration. /Users/rubber/linux/block/genhd.c: 408
	/* /Users/rubber/linux/block/genhd.c: 412
	 * If the driver provides an explicit major number it also must provide /Users/rubber/linux/block/genhd.c: 413
	 * the number of minors numbers supported, and those will be used to /Users/rubber/linux/block/genhd.c: 414
	 * setup the gendisk. /Users/rubber/linux/block/genhd.c: 415
	 * Otherwise just allocate the device numbers for both the whole device /Users/rubber/linux/block/genhd.c: 416
	 * and all partitions from the extended dev_t space. /Users/rubber/linux/block/genhd.c: 417
	/* /Users/rubber/linux/block/genhd.c: 462
	 * avoid probable deadlock caused by allocating memory with /Users/rubber/linux/block/genhd.c: 463
	 * GFP_KERNEL in runtime_resume callback of its all ancestor /Users/rubber/linux/block/genhd.c: 464
	 * devices /Users/rubber/linux/block/genhd.c: 465
		/* /Users/rubber/linux/block/genhd.c: 494
		 * Don't let hidden disks show up in /proc/partitions, /Users/rubber/linux/block/genhd.c: 495
		 * and don't bother scanning for partitions either. /Users/rubber/linux/block/genhd.c: 496
		/* /Users/rubber/linux/block/genhd.c: 514
		 * Announce the disk and partitions after all partitions are /Users/rubber/linux/block/genhd.c: 515
		 * created. (for hidden disks uevents remain suppressed forever) /Users/rubber/linux/block/genhd.c: 516
 * del_gendisk - remove the gendisk /Users/rubber/linux/block/genhd.c: 552
 * @disk: the struct gendisk to remove /Users/rubber/linux/block/genhd.c: 553
 * Removes the gendisk and all its associated resources. This deletes the /Users/rubber/linux/block/genhd.c: 555
 * partitions associated with the gendisk, and unregisters the associated /Users/rubber/linux/block/genhd.c: 556
 * request_queue. /Users/rubber/linux/block/genhd.c: 557
 * This is the counter to the respective __device_add_disk() call. /Users/rubber/linux/block/genhd.c: 559
 * The final removal of the struct gendisk happens when its refcount reaches 0 /Users/rubber/linux/block/genhd.c: 561
 * with put_disk(), which should be called after del_gendisk(), if /Users/rubber/linux/block/genhd.c: 562
 * __device_add_disk() was used. /Users/rubber/linux/block/genhd.c: 563
 * Drivers exist which depend on the release of the gendisk to be synchronous, /Users/rubber/linux/block/genhd.c: 565
 * it should not be deferred. /Users/rubber/linux/block/genhd.c: 566
 * Context: can sleep /Users/rubber/linux/block/genhd.c: 568
	/* /Users/rubber/linux/block/genhd.c: 590
	 * Fail any new I/O. /Users/rubber/linux/block/genhd.c: 591
	/* /Users/rubber/linux/block/genhd.c: 596
	 * Prevent new I/O from crossing bio_queue_enter(). /Users/rubber/linux/block/genhd.c: 597
		/* /Users/rubber/linux/block/genhd.c: 604
		 * Unregister bdi before releasing device numbers (as they can /Users/rubber/linux/block/genhd.c: 605
		 * get reused and we'd get clashes in sysfs). /Users/rubber/linux/block/genhd.c: 606
	/* /Users/rubber/linux/block/genhd.c: 628
	 * Allow using passthrough request again after the queue is torn down. /Users/rubber/linux/block/genhd.c: 629
 * invalidate_disk - invalidate the disk /Users/rubber/linux/block/genhd.c: 638
 * @disk: the struct gendisk to invalidate /Users/rubber/linux/block/genhd.c: 639
 * A helper to invalidates the disk. It will clean the disk's associated /Users/rubber/linux/block/genhd.c: 641
 * buffer/page caches and reset its internal states so that the disk /Users/rubber/linux/block/genhd.c: 642
 * can be reused by the drivers. /Users/rubber/linux/block/genhd.c: 643
 * Context: can sleep /Users/rubber/linux/block/genhd.c: 645
 * print a full list of all partitions - intended for places where the root /Users/rubber/linux/block/genhd.c: 703
 * filesystem can't be mounted and thus to give the victim some idea of what /Users/rubber/linux/block/genhd.c: 704
 * went wrong /Users/rubber/linux/block/genhd.c: 705
		/* /Users/rubber/linux/block/genhd.c: 719
		 * Don't show empty devices or things that have been /Users/rubber/linux/block/genhd.c: 720
		 * suppressed /Users/rubber/linux/block/genhd.c: 721
		/* /Users/rubber/linux/block/genhd.c: 727
		 * Note, unlike /proc/partitions, I am showing the numbers in /Users/rubber/linux/block/genhd.c: 728
		 * hex - the same format as the root= option takes. /Users/rubber/linux/block/genhd.c: 729
 * disk_release - releases all allocated resources of the gendisk /Users/rubber/linux/block/genhd.c: 1094
 * @dev: the device representing this disk /Users/rubber/linux/block/genhd.c: 1095
 * This function releases all allocated resources of the gendisk. /Users/rubber/linux/block/genhd.c: 1097
 * Drivers which used __device_add_disk() have a gendisk with a request_queue /Users/rubber/linux/block/genhd.c: 1099
 * assigned. Since the request_queue sits on top of the gendisk for these /Users/rubber/linux/block/genhd.c: 1100
 * drivers we also call blk_put_queue() for them, and we expect the /Users/rubber/linux/block/genhd.c: 1101
 * request_queue refcount to reach 0 at this point, and so the request_queue /Users/rubber/linux/block/genhd.c: 1102
 * will also be freed prior to the disk. /Users/rubber/linux/block/genhd.c: 1103
 * Context: can sleep /Users/rubber/linux/block/genhd.c: 1105
 * aggregate disk stat collector.  Uses the same stats that the sysfs /Users/rubber/linux/block/genhd.c: 1155
 * entries do, above, but makes them available through one seq_file. /Users/rubber/linux/block/genhd.c: 1156
 * The output looks suspiciously like /proc/partitions with a bunch of /Users/rubber/linux/block/genhd.c: 1158
 * extra fields. /Users/rubber/linux/block/genhd.c: 1159
	/* /Users/rubber/linux/block/genhd.c: 1169
	if (&disk_to_dev(gp)->kobj.entry == block_class.devices.next) /Users/rubber/linux/block/genhd.c: 1170
		seq_puts(seqf,	"major minor name" /Users/rubber/linux/block/genhd.c: 1171
				"     rio rmerge rsect ruse wio wmerge " /Users/rubber/linux/block/genhd.c: 1172
				"wsect wuse running use aveq" /Users/rubber/linux/block/genhd.c: 1173
				"\n\n"); /Users/rubber/linux/block/genhd.c: 1174
			/* We need to return the right devno, even /Users/rubber/linux/block/genhd.c: 1271
			 * if the partition doesn't exist yet. /Users/rubber/linux/block/genhd.c: 1272
 * put_disk - decrements the gendisk refcount /Users/rubber/linux/block/genhd.c: 1360
 * @disk: the struct gendisk to decrement the refcount for /Users/rubber/linux/block/genhd.c: 1361
 * This decrements the refcount for the struct gendisk. When this reaches 0 /Users/rubber/linux/block/genhd.c: 1363
 * we'll have disk_release() called. /Users/rubber/linux/block/genhd.c: 1364
 * Context: Any context, but the last reference must not be dropped from /Users/rubber/linux/block/genhd.c: 1366
 *          atomic context. /Users/rubber/linux/block/genhd.c: 1367
 * blk_cleanup_disk - shutdown a gendisk allocated by blk_alloc_disk /Users/rubber/linux/block/genhd.c: 1377
 * @disk: gendisk to shutdown /Users/rubber/linux/block/genhd.c: 1378
 * Mark the queue hanging off @disk DYING, drain all pending requests, then mark /Users/rubber/linux/block/genhd.c: 1380
 * the queue DEAD, destroy and put it and the gendisk structure. /Users/rubber/linux/block/genhd.c: 1381
 * Context: can sleep /Users/rubber/linux/block/genhd.c: 1383
 * set_disk_ro - set a gendisk read-only /Users/rubber/linux/block/genhd.c: 1403
 * @disk:	gendisk to operate on /Users/rubber/linux/block/genhd.c: 1404
 * @read_only:	%true to set the disk read-only, %false set the disk read/write /Users/rubber/linux/block/genhd.c: 1405
 * This function is used to indicate whether a given disk device should have its /Users/rubber/linux/block/genhd.c: 1407
 * read-only flag set. set_disk_ro() is typically used by device drivers to /Users/rubber/linux/block/genhd.c: 1408
 * indicate whether the underlying physical device is write-protected. /Users/rubber/linux/block/genhd.c: 1409
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-settings.c: 1
 * Functions related to setting various queue properties from drivers /Users/rubber/linux/block/blk-settings.c: 3
 * blk_set_default_limits - reset limits to default values /Users/rubber/linux/block/blk-settings.c: 28
 * @lim:  the queue_limits structure to reset /Users/rubber/linux/block/blk-settings.c: 29
 * Description: /Users/rubber/linux/block/blk-settings.c: 31
 *   Returns a queue_limit struct to its default state. /Users/rubber/linux/block/blk-settings.c: 32
 * blk_set_stacking_limits - set default limits for stacking devices /Users/rubber/linux/block/blk-settings.c: 64
 * @lim:  the queue_limits structure to reset /Users/rubber/linux/block/blk-settings.c: 65
 * Description: /Users/rubber/linux/block/blk-settings.c: 67
 *   Returns a queue_limit struct to its default state. Should be used /Users/rubber/linux/block/blk-settings.c: 68
 *   by stacking drivers like DM that have no internal limits. /Users/rubber/linux/block/blk-settings.c: 69
 * blk_queue_bounce_limit - set bounce buffer limit for queue /Users/rubber/linux/block/blk-settings.c: 89
 * @q: the request queue for the device /Users/rubber/linux/block/blk-settings.c: 90
 * @bounce: bounce limit to enforce /Users/rubber/linux/block/blk-settings.c: 91
 * Description: /Users/rubber/linux/block/blk-settings.c: 93
 *    Force bouncing for ISA DMA ranges or highmem. /Users/rubber/linux/block/blk-settings.c: 94
 *    DEPRECATED, don't use in new code. /Users/rubber/linux/block/blk-settings.c: 96
 * blk_queue_max_hw_sectors - set max sectors for a request for this queue /Users/rubber/linux/block/blk-settings.c: 105
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 106
 * @max_hw_sectors:  max hardware sectors in the usual 512b unit /Users/rubber/linux/block/blk-settings.c: 107
 * Description: /Users/rubber/linux/block/blk-settings.c: 109
 *    Enables a low level driver to set a hard upper limit, /Users/rubber/linux/block/blk-settings.c: 110
 *    max_hw_sectors, on the size of requests.  max_hw_sectors is set by /Users/rubber/linux/block/blk-settings.c: 111
 *    the device driver based upon the capabilities of the I/O /Users/rubber/linux/block/blk-settings.c: 112
 *    controller. /Users/rubber/linux/block/blk-settings.c: 113
 *    max_dev_sectors is a hard limit imposed by the storage device for /Users/rubber/linux/block/blk-settings.c: 115
 *    READ/WRITE requests. It is set by the disk driver. /Users/rubber/linux/block/blk-settings.c: 116
 *    max_sectors is a soft limit imposed by the block layer for /Users/rubber/linux/block/blk-settings.c: 118
 *    filesystem type requests.  This value can be overridden on a /Users/rubber/linux/block/blk-settings.c: 119
 *    per-device basis in /sys/block/<device>/queue/max_sectors_kb. /Users/rubber/linux/block/blk-settings.c: 120
 *    The soft limit can not exceed max_hw_sectors. /Users/rubber/linux/block/blk-settings.c: 121
 * blk_queue_chunk_sectors - set size of the chunk for this queue /Users/rubber/linux/block/blk-settings.c: 151
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 152
 * @chunk_sectors:  chunk sectors in the usual 512b unit /Users/rubber/linux/block/blk-settings.c: 153
 * Description: /Users/rubber/linux/block/blk-settings.c: 155
 *    If a driver doesn't want IOs to cross a given chunk size, it can set /Users/rubber/linux/block/blk-settings.c: 156
 *    this limit and prevent merging across chunks. Note that the block layer /Users/rubber/linux/block/blk-settings.c: 157
 *    must accept a page worth of data at any offset. So if the crossing of /Users/rubber/linux/block/blk-settings.c: 158
 *    chunks is a hard limitation in the driver, it must still be prepared /Users/rubber/linux/block/blk-settings.c: 159
 *    to split single page bios. /Users/rubber/linux/block/blk-settings.c: 160
 * blk_queue_max_discard_sectors - set max sectors for a single discard /Users/rubber/linux/block/blk-settings.c: 169
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 170
 * @max_discard_sectors: maximum number of sectors to discard /Users/rubber/linux/block/blk-settings.c: 171
 * blk_queue_max_write_same_sectors - set max sectors for a single write same /Users/rubber/linux/block/blk-settings.c: 182
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 183
 * @max_write_same_sectors: maximum number of sectors to write per command /Users/rubber/linux/block/blk-settings.c: 184
 * blk_queue_max_write_zeroes_sectors - set max sectors for a single /Users/rubber/linux/block/blk-settings.c: 194
 *                                      write zeroes /Users/rubber/linux/block/blk-settings.c: 195
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 196
 * @max_write_zeroes_sectors: maximum number of sectors to write per command /Users/rubber/linux/block/blk-settings.c: 197
 * blk_queue_max_zone_append_sectors - set max sectors for a single zone append /Users/rubber/linux/block/blk-settings.c: 207
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 208
 * @max_zone_append_sectors: maximum number of sectors to write per command /Users/rubber/linux/block/blk-settings.c: 209
	/* /Users/rubber/linux/block/blk-settings.c: 222
	 * Signal eventual driver bugs resulting in the max_zone_append sectors limit /Users/rubber/linux/block/blk-settings.c: 223
	 * being 0 due to a 0 argument, the chunk_sectors limit (zone size) not set, /Users/rubber/linux/block/blk-settings.c: 224
	 * or the max_hw_sectors limit not set. /Users/rubber/linux/block/blk-settings.c: 225
 * blk_queue_max_segments - set max hw segments for a request for this queue /Users/rubber/linux/block/blk-settings.c: 234
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 235
 * @max_segments:  max number of segments /Users/rubber/linux/block/blk-settings.c: 236
 * Description: /Users/rubber/linux/block/blk-settings.c: 238
 *    Enables a low level driver to set an upper limit on the number of /Users/rubber/linux/block/blk-settings.c: 239
 *    hw data segments in a request. /Users/rubber/linux/block/blk-settings.c: 240
 * blk_queue_max_discard_segments - set max segments for discard requests /Users/rubber/linux/block/blk-settings.c: 255
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 256
 * @max_segments:  max number of segments /Users/rubber/linux/block/blk-settings.c: 257
 * Description: /Users/rubber/linux/block/blk-settings.c: 259
 *    Enables a low level driver to set an upper limit on the number of /Users/rubber/linux/block/blk-settings.c: 260
 *    segments in a discard request. /Users/rubber/linux/block/blk-settings.c: 261
 * blk_queue_max_segment_size - set max segment size for blk_rq_map_sg /Users/rubber/linux/block/blk-settings.c: 271
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 272
 * @max_size:  max size of segment in bytes /Users/rubber/linux/block/blk-settings.c: 273
 * Description: /Users/rubber/linux/block/blk-settings.c: 275
 *    Enables a low level driver to set an upper limit on the size of a /Users/rubber/linux/block/blk-settings.c: 276
 *    coalesced segment /Users/rubber/linux/block/blk-settings.c: 277
 * blk_queue_logical_block_size - set logical block size for the queue /Users/rubber/linux/block/blk-settings.c: 295
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 296
 * @size:  the logical block size, in bytes /Users/rubber/linux/block/blk-settings.c: 297
 * Description: /Users/rubber/linux/block/blk-settings.c: 299
 *   This should be set to the lowest possible block size that the /Users/rubber/linux/block/blk-settings.c: 300
 *   storage device can address.  The default of 512 covers most /Users/rubber/linux/block/blk-settings.c: 301
 *   hardware. /Users/rubber/linux/block/blk-settings.c: 302
 * blk_queue_physical_block_size - set physical block size for the queue /Users/rubber/linux/block/blk-settings.c: 324
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 325
 * @size:  the physical block size, in bytes /Users/rubber/linux/block/blk-settings.c: 326
 * Description: /Users/rubber/linux/block/blk-settings.c: 328
 *   This should be set to the lowest possible sector size that the /Users/rubber/linux/block/blk-settings.c: 329
 *   hardware can operate on without reverting to read-modify-write /Users/rubber/linux/block/blk-settings.c: 330
 *   operations. /Users/rubber/linux/block/blk-settings.c: 331
 * blk_queue_zone_write_granularity - set zone write granularity for the queue /Users/rubber/linux/block/blk-settings.c: 346
 * @q:  the request queue for the zoned device /Users/rubber/linux/block/blk-settings.c: 347
 * @size:  the zone write granularity size, in bytes /Users/rubber/linux/block/blk-settings.c: 348
 * Description: /Users/rubber/linux/block/blk-settings.c: 350
 *   This should be set to the lowest possible size allowing to write in /Users/rubber/linux/block/blk-settings.c: 351
 *   sequential zones of a zoned block device. /Users/rubber/linux/block/blk-settings.c: 352
 * blk_queue_alignment_offset - set physical block alignment offset /Users/rubber/linux/block/blk-settings.c: 368
 * @q:	the request queue for the device /Users/rubber/linux/block/blk-settings.c: 369
 * @offset: alignment offset in bytes /Users/rubber/linux/block/blk-settings.c: 370
 * Description: /Users/rubber/linux/block/blk-settings.c: 372
 *   Some devices are naturally misaligned to compensate for things like /Users/rubber/linux/block/blk-settings.c: 373
 *   the legacy DOS partition table 63-sector offset.  Low-level drivers /Users/rubber/linux/block/blk-settings.c: 374
 *   should call this function for devices whose first sector is not /Users/rubber/linux/block/blk-settings.c: 375
 *   naturally aligned. /Users/rubber/linux/block/blk-settings.c: 376
	/* /Users/rubber/linux/block/blk-settings.c: 390
	 * For read-ahead of large files to be effective, we need to read ahead /Users/rubber/linux/block/blk-settings.c: 391
	 * at least twice the optimal I/O size. /Users/rubber/linux/block/blk-settings.c: 392
 * blk_limits_io_min - set minimum request size for a device /Users/rubber/linux/block/blk-settings.c: 401
 * @limits: the queue limits /Users/rubber/linux/block/blk-settings.c: 402
 * @min:  smallest I/O size in bytes /Users/rubber/linux/block/blk-settings.c: 403
 * Description: /Users/rubber/linux/block/blk-settings.c: 405
 *   Some devices have an internal block size bigger than the reported /Users/rubber/linux/block/blk-settings.c: 406
 *   hardware sector size.  This function can be used to signal the /Users/rubber/linux/block/blk-settings.c: 407
 *   smallest I/O the device can perform without incurring a performance /Users/rubber/linux/block/blk-settings.c: 408
 *   penalty. /Users/rubber/linux/block/blk-settings.c: 409
 * blk_queue_io_min - set minimum request size for the queue /Users/rubber/linux/block/blk-settings.c: 424
 * @q:	the request queue for the device /Users/rubber/linux/block/blk-settings.c: 425
 * @min:  smallest I/O size in bytes /Users/rubber/linux/block/blk-settings.c: 426
 * Description: /Users/rubber/linux/block/blk-settings.c: 428
 *   Storage devices may report a granularity or preferred minimum I/O /Users/rubber/linux/block/blk-settings.c: 429
 *   size which is the smallest request the device can perform without /Users/rubber/linux/block/blk-settings.c: 430
 *   incurring a performance penalty.  For disk drives this is often the /Users/rubber/linux/block/blk-settings.c: 431
 *   physical block size.  For RAID arrays it is often the stripe chunk /Users/rubber/linux/block/blk-settings.c: 432
 *   size.  A properly aligned multiple of minimum_io_size is the /Users/rubber/linux/block/blk-settings.c: 433
 *   preferred request size for workloads where a high number of I/O /Users/rubber/linux/block/blk-settings.c: 434
 *   operations is desired. /Users/rubber/linux/block/blk-settings.c: 435
 * blk_limits_io_opt - set optimal request size for a device /Users/rubber/linux/block/blk-settings.c: 444
 * @limits: the queue limits /Users/rubber/linux/block/blk-settings.c: 445
 * @opt:  smallest I/O size in bytes /Users/rubber/linux/block/blk-settings.c: 446
 * Description: /Users/rubber/linux/block/blk-settings.c: 448
 *   Storage devices may report an optimal I/O size, which is the /Users/rubber/linux/block/blk-settings.c: 449
 *   device's preferred unit for sustained I/O.  This is rarely reported /Users/rubber/linux/block/blk-settings.c: 450
 *   for disk drives.  For RAID arrays it is usually the stripe width or /Users/rubber/linux/block/blk-settings.c: 451
 *   the internal track size.  A properly aligned multiple of /Users/rubber/linux/block/blk-settings.c: 452
 *   optimal_io_size is the preferred request size for workloads where /Users/rubber/linux/block/blk-settings.c: 453
 *   sustained throughput is desired. /Users/rubber/linux/block/blk-settings.c: 454
 * blk_queue_io_opt - set optimal request size for the queue /Users/rubber/linux/block/blk-settings.c: 463
 * @q:	the request queue for the device /Users/rubber/linux/block/blk-settings.c: 464
 * @opt:  optimal request size in bytes /Users/rubber/linux/block/blk-settings.c: 465
 * Description: /Users/rubber/linux/block/blk-settings.c: 467
 *   Storage devices may report an optimal I/O size, which is the /Users/rubber/linux/block/blk-settings.c: 468
 *   device's preferred unit for sustained I/O.  This is rarely reported /Users/rubber/linux/block/blk-settings.c: 469
 *   for disk drives.  For RAID arrays it is usually the stripe width or /Users/rubber/linux/block/blk-settings.c: 470
 *   the internal track size.  A properly aligned multiple of /Users/rubber/linux/block/blk-settings.c: 471
 *   optimal_io_size is the preferred request size for workloads where /Users/rubber/linux/block/blk-settings.c: 472
 *   sustained throughput is desired. /Users/rubber/linux/block/blk-settings.c: 473
 * blk_stack_limits - adjust queue_limits for stacked devices /Users/rubber/linux/block/blk-settings.c: 494
 * @t:	the stacking driver limits (top device) /Users/rubber/linux/block/blk-settings.c: 495
 * @b:  the underlying queue limits (bottom, component device) /Users/rubber/linux/block/blk-settings.c: 496
 * @start:  first data sector within component device /Users/rubber/linux/block/blk-settings.c: 497
 * Description: /Users/rubber/linux/block/blk-settings.c: 499
 *    This function is used by stacking drivers like MD and DM to ensure /Users/rubber/linux/block/blk-settings.c: 500
 *    that all component devices have compatible block sizes and /Users/rubber/linux/block/blk-settings.c: 501
 *    alignments.  The stacking driver must provide a queue_limits /Users/rubber/linux/block/blk-settings.c: 502
 *    struct (top) and then iteratively call the stacking function for /Users/rubber/linux/block/blk-settings.c: 503
 *    all component (bottom) devices.  The stacking function will /Users/rubber/linux/block/blk-settings.c: 504
 *    attempt to combine the values and ensure proper alignment. /Users/rubber/linux/block/blk-settings.c: 505
 *    Returns 0 if the top and bottom queue_limits are compatible.  The /Users/rubber/linux/block/blk-settings.c: 507
 *    top device's block sizes and alignment offsets may be adjusted to /Users/rubber/linux/block/blk-settings.c: 508
 *    ensure alignment with the bottom device. If no compatible sizes /Users/rubber/linux/block/blk-settings.c: 509
 *    and alignments exist, -1 is returned and the resulting top /Users/rubber/linux/block/blk-settings.c: 510
 *    queue_limits will have the misaligned flag set to indicate that /Users/rubber/linux/block/blk-settings.c: 511
 *    the alignment_offset is undefined. /Users/rubber/linux/block/blk-settings.c: 512
	/* Bottom device has different alignment.  Check that it is /Users/rubber/linux/block/blk-settings.c: 548
	 * compatible with the current top alignment. /Users/rubber/linux/block/blk-settings.c: 549
 * disk_stack_limits - adjust queue limits for stacked drivers /Users/rubber/linux/block/blk-settings.c: 655
 * @disk:  MD/DM gendisk (top) /Users/rubber/linux/block/blk-settings.c: 656
 * @bdev:  the underlying block device (bottom) /Users/rubber/linux/block/blk-settings.c: 657
 * @offset:  offset to beginning of data within component device /Users/rubber/linux/block/blk-settings.c: 658
 * Description: /Users/rubber/linux/block/blk-settings.c: 660
 *    Merges the limits for a top level gendisk and a bottom level /Users/rubber/linux/block/blk-settings.c: 661
 *    block_device. /Users/rubber/linux/block/blk-settings.c: 662
 * blk_queue_update_dma_pad - update pad mask /Users/rubber/linux/block/blk-settings.c: 679
 * @q:     the request queue for the device /Users/rubber/linux/block/blk-settings.c: 680
 * @mask:  pad mask /Users/rubber/linux/block/blk-settings.c: 681
 * Update dma pad mask. /Users/rubber/linux/block/blk-settings.c: 683
 * Appending pad buffer to a request modifies the last entry of a /Users/rubber/linux/block/blk-settings.c: 685
 * scatter list such that it includes the pad buffer. /Users/rubber/linux/block/blk-settings.c: 686
 * blk_queue_segment_boundary - set boundary rules for segment merging /Users/rubber/linux/block/blk-settings.c: 696
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 697
 * @mask:  the memory boundary mask /Users/rubber/linux/block/blk-settings.c: 698
 * blk_queue_virt_boundary - set boundary rules for bio merging /Users/rubber/linux/block/blk-settings.c: 713
 * @q:  the request queue for the device /Users/rubber/linux/block/blk-settings.c: 714
 * @mask:  the memory boundary mask /Users/rubber/linux/block/blk-settings.c: 715
	/* /Users/rubber/linux/block/blk-settings.c: 721
	 * Devices that require a virtual boundary do not support scatter/gather /Users/rubber/linux/block/blk-settings.c: 722
	 * I/O natively, but instead require a descriptor list entry for each /Users/rubber/linux/block/blk-settings.c: 723
	 * page (which might not be idential to the Linux PAGE_SIZE).  Because /Users/rubber/linux/block/blk-settings.c: 724
	 * of that they are not limited by our notion of "segment size". /Users/rubber/linux/block/blk-settings.c: 725
 * blk_queue_dma_alignment - set dma length and memory alignment /Users/rubber/linux/block/blk-settings.c: 733
 * @q:     the request queue for the device /Users/rubber/linux/block/blk-settings.c: 734
 * @mask:  alignment mask /Users/rubber/linux/block/blk-settings.c: 735
 * description: /Users/rubber/linux/block/blk-settings.c: 737
 *    set required memory and length alignment for direct dma transactions. /Users/rubber/linux/block/blk-settings.c: 738
 *    this is used when building direct io requests for the queue. /Users/rubber/linux/block/blk-settings.c: 739
 * blk_queue_update_dma_alignment - update dma length and memory alignment /Users/rubber/linux/block/blk-settings.c: 749
 * @q:     the request queue for the device /Users/rubber/linux/block/blk-settings.c: 750
 * @mask:  alignment mask /Users/rubber/linux/block/blk-settings.c: 751
 * description: /Users/rubber/linux/block/blk-settings.c: 753
 *    update required memory and length alignment for direct dma transactions. /Users/rubber/linux/block/blk-settings.c: 754
 *    If the requested alignment is larger than the current alignment, then /Users/rubber/linux/block/blk-settings.c: 755
 *    the current queue alignment is updated to the new value, otherwise it /Users/rubber/linux/block/blk-settings.c: 756
 *    is left alone.  The design of this is to allow multiple objects /Users/rubber/linux/block/blk-settings.c: 757
 *    (driver, device, transport etc) to set their respective /Users/rubber/linux/block/blk-settings.c: 758
 *    alignments without having them interfere. /Users/rubber/linux/block/blk-settings.c: 759
 * blk_set_queue_depth - tell the block layer about the device queue depth /Users/rubber/linux/block/blk-settings.c: 772
 * @q:		the request queue for the device /Users/rubber/linux/block/blk-settings.c: 773
 * @depth:		queue depth /Users/rubber/linux/block/blk-settings.c: 774
 * blk_queue_write_cache - configure queue's write cache /Users/rubber/linux/block/blk-settings.c: 785
 * @q:		the request queue for the device /Users/rubber/linux/block/blk-settings.c: 786
 * @wc:		write back cache on or off /Users/rubber/linux/block/blk-settings.c: 787
 * @fua:	device supports FUA writes, if true /Users/rubber/linux/block/blk-settings.c: 788
 * Tell the block layer about the write cache of @q. /Users/rubber/linux/block/blk-settings.c: 790
 * blk_queue_required_elevator_features - Set a queue required elevator features /Users/rubber/linux/block/blk-settings.c: 808
 * @q:		the request queue for the target device /Users/rubber/linux/block/blk-settings.c: 809
 * @features:	Required elevator features OR'ed together /Users/rubber/linux/block/blk-settings.c: 810
 * Tell the block layer that for the device controlled through @q, only the /Users/rubber/linux/block/blk-settings.c: 812
 * only elevators that can be used are those that implement at least the set of /Users/rubber/linux/block/blk-settings.c: 813
 * features specified by @features. /Users/rubber/linux/block/blk-settings.c: 814
 * blk_queue_can_use_dma_map_merging - configure queue for merging segments. /Users/rubber/linux/block/blk-settings.c: 824
 * @q:		the request queue for the device /Users/rubber/linux/block/blk-settings.c: 825
 * @dev:	the device pointer for dma /Users/rubber/linux/block/blk-settings.c: 826
 * Tell the block layer about merging the segments by dma map of @q. /Users/rubber/linux/block/blk-settings.c: 828
 * blk_queue_set_zoned - configure a disk queue zoned model. /Users/rubber/linux/block/blk-settings.c: 864
 * @disk:	the gendisk of the queue to configure /Users/rubber/linux/block/blk-settings.c: 865
 * @model:	the zoned model to set /Users/rubber/linux/block/blk-settings.c: 866
 * Set the zoned model of the request queue of @disk according to @model. /Users/rubber/linux/block/blk-settings.c: 868
 * When @model is BLK_ZONED_HM (host managed), this should be called only /Users/rubber/linux/block/blk-settings.c: 869
 * if zoned block device support is enabled (CONFIG_BLK_DEV_ZONED option). /Users/rubber/linux/block/blk-settings.c: 870
 * If @model specifies BLK_ZONED_HA (host aware), the effective model used /Users/rubber/linux/block/blk-settings.c: 871
 * depends on CONFIG_BLK_DEV_ZONED settings and on the existence of partitions /Users/rubber/linux/block/blk-settings.c: 872
 * on the disk. /Users/rubber/linux/block/blk-settings.c: 873
		/* /Users/rubber/linux/block/blk-settings.c: 881
		 * Host managed devices are supported only if /Users/rubber/linux/block/blk-settings.c: 882
		 * CONFIG_BLK_DEV_ZONED is enabled. /Users/rubber/linux/block/blk-settings.c: 883
		/* /Users/rubber/linux/block/blk-settings.c: 888
		 * Host aware devices can be treated either as regular block /Users/rubber/linux/block/blk-settings.c: 889
		 * devices (similar to drive managed devices) or as zoned block /Users/rubber/linux/block/blk-settings.c: 890
		 * devices to take advantage of the zone command set, similarly /Users/rubber/linux/block/blk-settings.c: 891
		 * to host managed devices. We try the latter if there are no /Users/rubber/linux/block/blk-settings.c: 892
		 * partitions and zoned block device support is enabled, else /Users/rubber/linux/block/blk-settings.c: 893
		 * we do nothing special as far as the block layer is concerned. /Users/rubber/linux/block/blk-settings.c: 894
		/* /Users/rubber/linux/block/blk-settings.c: 909
		 * Set the zone write granularity to the device logical block /Users/rubber/linux/block/blk-settings.c: 910
		 * size by default. The driver can change this value if needed. /Users/rubber/linux/block/blk-settings.c: 911
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/bfq-cgroup.c: 1
 * cgroups support for the BFQ I/O scheduler. /Users/rubber/linux/block/bfq-cgroup.c: 3
 * bfq_stat_add - add a value to a bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 37
 * @stat: target bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 38
 * @val: value to add /Users/rubber/linux/block/bfq-cgroup.c: 39
 * Add @val to @stat.  The caller must ensure that IRQ on the same CPU /Users/rubber/linux/block/bfq-cgroup.c: 41
 * don't re-enter this function for the same counter. /Users/rubber/linux/block/bfq-cgroup.c: 42
 * bfq_stat_read - read the current value of a bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 50
 * @stat: bfq_stat to read /Users/rubber/linux/block/bfq-cgroup.c: 51
 * bfq_stat_reset - reset a bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 59
 * @stat: bfq_stat to reset /Users/rubber/linux/block/bfq-cgroup.c: 60
 * bfq_stat_add_aux - add a bfq_stat into another's aux count /Users/rubber/linux/block/bfq-cgroup.c: 69
 * @to: the destination bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 70
 * @from: the source /Users/rubber/linux/block/bfq-cgroup.c: 71
 * Add @from's count including the aux one to @to's aux count. /Users/rubber/linux/block/bfq-cgroup.c: 73
 * blkg_prfill_stat - prfill callback for bfq_stat /Users/rubber/linux/block/bfq-cgroup.c: 83
 * @sf: seq_file to print to /Users/rubber/linux/block/bfq-cgroup.c: 84
 * @pd: policy private data of interest /Users/rubber/linux/block/bfq-cgroup.c: 85
 * @off: offset to the bfq_stat in @pd /Users/rubber/linux/block/bfq-cgroup.c: 86
 * prfill callback for printing a bfq_stat. /Users/rubber/linux/block/bfq-cgroup.c: 88
	/* /Users/rubber/linux/block/bfq-cgroup.c: 178
	 * group is already marked empty. This can happen if bfqq got new /Users/rubber/linux/block/bfq-cgroup.c: 179
	 * request in parent group and moved to this group while being added /Users/rubber/linux/block/bfq-cgroup.c: 180
	 * to service tree. Just ignore the event and move on. /Users/rubber/linux/block/bfq-cgroup.c: 181
 * blk-cgroup policy-related handlers /Users/rubber/linux/block/bfq-cgroup.c: 274
 * The following functions help in converting between blk-cgroup /Users/rubber/linux/block/bfq-cgroup.c: 275
 * internal structures and BFQ-specific structures. /Users/rubber/linux/block/bfq-cgroup.c: 276
 * bfq_group handlers /Users/rubber/linux/block/bfq-cgroup.c: 295
 * The following functions help in navigating the bfq_group hierarchy /Users/rubber/linux/block/bfq-cgroup.c: 296
 * by allowing to find the parent of a bfq_group or the bfq_group /Users/rubber/linux/block/bfq-cgroup.c: 297
 * associated to a bfq_queue. /Users/rubber/linux/block/bfq-cgroup.c: 298
 * The following two functions handle get and put of a bfq_group by /Users/rubber/linux/block/bfq-cgroup.c: 318
 * wrapping the related blk-cgroup hooks. /Users/rubber/linux/block/bfq-cgroup.c: 319
 * Transfer @bfqg's stats to its parent's aux counts so that the ancestors' /Users/rubber/linux/block/bfq-cgroup.c: 402
 * recursive stats can still account for the amount used by this bfqg after /Users/rubber/linux/block/bfq-cgroup.c: 403
 * it's gone. /Users/rubber/linux/block/bfq-cgroup.c: 404
		/* /Users/rubber/linux/block/bfq-cgroup.c: 433
		 * Make sure that bfqg and its associated blkg do not /Users/rubber/linux/block/bfq-cgroup.c: 434
		 * disappear before entity. /Users/rubber/linux/block/bfq-cgroup.c: 435
	bfqg->my_entity = entity; /* /Users/rubber/linux/block/bfq-cgroup.c: 554
				   * the root_group's will be set to NULL /Users/rubber/linux/block/bfq-cgroup.c: 555
				   * in bfq_init_queue() /Users/rubber/linux/block/bfq-cgroup.c: 556
	/* /Users/rubber/linux/block/bfq-cgroup.c: 610
	 * Update chain of bfq_groups as we might be handling a leaf group /Users/rubber/linux/block/bfq-cgroup.c: 611
	 * which, along with some of its relatives, has not been hooked yet /Users/rubber/linux/block/bfq-cgroup.c: 612
	 * to the private hierarchy of BFQ. /Users/rubber/linux/block/bfq-cgroup.c: 613
 * bfq_bfqq_move - migrate @bfqq to @bfqg. /Users/rubber/linux/block/bfq-cgroup.c: 631
 * @bfqd: queue descriptor. /Users/rubber/linux/block/bfq-cgroup.c: 632
 * @bfqq: the queue to move. /Users/rubber/linux/block/bfq-cgroup.c: 633
 * @bfqg: the group to move to. /Users/rubber/linux/block/bfq-cgroup.c: 634
 * Move @bfqq to @bfqg, deactivating it from its old group and reactivating /Users/rubber/linux/block/bfq-cgroup.c: 636
 * it on the new one.  Avoid putting the entity on the old group idle tree. /Users/rubber/linux/block/bfq-cgroup.c: 637
 * Must be called under the scheduler lock, to make sure that the blkg /Users/rubber/linux/block/bfq-cgroup.c: 639
 * owning @bfqg does not disappear (see comments in /Users/rubber/linux/block/bfq-cgroup.c: 640
 * bfq_bic_update_cgroup on guaranteeing the consistency of blkg /Users/rubber/linux/block/bfq-cgroup.c: 641
 * objects). /Users/rubber/linux/block/bfq-cgroup.c: 642
	/* /Users/rubber/linux/block/bfq-cgroup.c: 649
	 * Get extra reference to prevent bfqq from being freed in /Users/rubber/linux/block/bfq-cgroup.c: 650
	 * next possible expire or deactivate. /Users/rubber/linux/block/bfq-cgroup.c: 651
	/* If bfqq is empty, then bfq_bfqq_expire also invokes /Users/rubber/linux/block/bfq-cgroup.c: 655
	 * bfq_del_bfqq_busy, thereby removing bfqq and its entity /Users/rubber/linux/block/bfq-cgroup.c: 656
	 * from data structures related to current group. Otherwise we /Users/rubber/linux/block/bfq-cgroup.c: 657
	 * need to remove bfqq explicitly with bfq_deactivate_bfqq, as /Users/rubber/linux/block/bfq-cgroup.c: 658
	 * we do below. /Users/rubber/linux/block/bfq-cgroup.c: 659
 * __bfq_bic_change_cgroup - move @bic to @cgroup. /Users/rubber/linux/block/bfq-cgroup.c: 695
 * @bfqd: the queue descriptor. /Users/rubber/linux/block/bfq-cgroup.c: 696
 * @bic: the bic to move. /Users/rubber/linux/block/bfq-cgroup.c: 697
 * @blkcg: the blk-cgroup to move to. /Users/rubber/linux/block/bfq-cgroup.c: 698
 * Move bic to blkcg, assuming that bfqd->lock is held; which makes /Users/rubber/linux/block/bfq-cgroup.c: 700
 * sure that the reference to cgroup is valid across the call (see /Users/rubber/linux/block/bfq-cgroup.c: 701
 * comments in bfq_bic_update_cgroup on this issue) /Users/rubber/linux/block/bfq-cgroup.c: 702
 * NOTE: an alternative approach might have been to store the current /Users/rubber/linux/block/bfq-cgroup.c: 704
 * cgroup in bfqq and getting a reference to it, reducing the lookup /Users/rubber/linux/block/bfq-cgroup.c: 705
 * time here, at the price of slightly more complex code. /Users/rubber/linux/block/bfq-cgroup.c: 706
	/* /Users/rubber/linux/block/bfq-cgroup.c: 749
	 * Check whether blkcg has changed.  The condition may trigger /Users/rubber/linux/block/bfq-cgroup.c: 750
	 * spuriously on a newly created cic but there's no harm. /Users/rubber/linux/block/bfq-cgroup.c: 751
	/* /Users/rubber/linux/block/bfq-cgroup.c: 757
	 * Update blkg_path for bfq_log_* functions. We cache this /Users/rubber/linux/block/bfq-cgroup.c: 758
	 * path, and update it here, for the following /Users/rubber/linux/block/bfq-cgroup.c: 759
	 * reasons. Operations on blkg objects in blk-cgroup are /Users/rubber/linux/block/bfq-cgroup.c: 760
	 * protected with the request_queue lock, and not with the /Users/rubber/linux/block/bfq-cgroup.c: 761
	 * lock that protects the instances of this scheduler /Users/rubber/linux/block/bfq-cgroup.c: 762
	 * (bfqd->lock). This exposes BFQ to the following sort of /Users/rubber/linux/block/bfq-cgroup.c: 763
	 * race. /Users/rubber/linux/block/bfq-cgroup.c: 764
	 * /Users/rubber/linux/block/bfq-cgroup.c: 765
	 * The blkg_lookup performed in bfq_get_queue, protected /Users/rubber/linux/block/bfq-cgroup.c: 766
	 * through rcu, may happen to return the address of a copy of /Users/rubber/linux/block/bfq-cgroup.c: 767
	 * the original blkg. If this is the case, then the /Users/rubber/linux/block/bfq-cgroup.c: 768
	 * bfqg_and_blkg_get performed in bfq_get_queue, to pin down /Users/rubber/linux/block/bfq-cgroup.c: 769
	 * the blkg, is useless: it does not prevent blk-cgroup code /Users/rubber/linux/block/bfq-cgroup.c: 770
	 * from destroying both the original blkg and all objects /Users/rubber/linux/block/bfq-cgroup.c: 771
	 * directly or indirectly referred by the copy of the /Users/rubber/linux/block/bfq-cgroup.c: 772
	 * blkg. /Users/rubber/linux/block/bfq-cgroup.c: 773
	 * /Users/rubber/linux/block/bfq-cgroup.c: 774
	 * On the bright side, destroy operations on a blkg invoke, as /Users/rubber/linux/block/bfq-cgroup.c: 775
	 * a first step, hooks of the scheduler associated with the /Users/rubber/linux/block/bfq-cgroup.c: 776
	 * blkg. And these hooks are executed with bfqd->lock held for /Users/rubber/linux/block/bfq-cgroup.c: 777
	 * BFQ. As a consequence, for any blkg associated with the /Users/rubber/linux/block/bfq-cgroup.c: 778
	 * request queue this instance of the scheduler is attached /Users/rubber/linux/block/bfq-cgroup.c: 779
	 * to, we are guaranteed that such a blkg is not destroyed, and /Users/rubber/linux/block/bfq-cgroup.c: 780
	 * that all the pointers it contains are consistent, while we /Users/rubber/linux/block/bfq-cgroup.c: 781
	 * are holding bfqd->lock. A blkg_lookup performed with /Users/rubber/linux/block/bfq-cgroup.c: 782
	 * bfqd->lock held then returns a fully consistent blkg, which /Users/rubber/linux/block/bfq-cgroup.c: 783
	 * remains consistent until this lock is held. /Users/rubber/linux/block/bfq-cgroup.c: 784
	 * /Users/rubber/linux/block/bfq-cgroup.c: 785
	 * Thanks to the last fact, and to the fact that: (1) bfqg has /Users/rubber/linux/block/bfq-cgroup.c: 786
	 * been obtained through a blkg_lookup in the above /Users/rubber/linux/block/bfq-cgroup.c: 787
	 * assignment, and (2) bfqd->lock is being held, here we can /Users/rubber/linux/block/bfq-cgroup.c: 788
	 * safely use the policy data for the involved blkg (i.e., the /Users/rubber/linux/block/bfq-cgroup.c: 789
	 * field bfqg->pd) to get to the blkg associated with bfqg, /Users/rubber/linux/block/bfq-cgroup.c: 790
	 * and then we can safely use any field of blkg. After we /Users/rubber/linux/block/bfq-cgroup.c: 791
	 * release bfqd->lock, even just getting blkg through this /Users/rubber/linux/block/bfq-cgroup.c: 792
	 * bfqg may cause dangling references to be traversed, as /Users/rubber/linux/block/bfq-cgroup.c: 793
	 * bfqg->pd may not exist any more. /Users/rubber/linux/block/bfq-cgroup.c: 794
	 * /Users/rubber/linux/block/bfq-cgroup.c: 795
	 * In view of the above facts, here we cache, in the bfqg, any /Users/rubber/linux/block/bfq-cgroup.c: 796
	 * blkg data we may need for this bic, and for its associated /Users/rubber/linux/block/bfq-cgroup.c: 797
	 * bfq_queue. As of now, we need to cache only the path of the /Users/rubber/linux/block/bfq-cgroup.c: 798
	 * blkg, which is used in the bfq_log_* functions. /Users/rubber/linux/block/bfq-cgroup.c: 799
	 * /Users/rubber/linux/block/bfq-cgroup.c: 800
	 * Finally, note that bfqg itself needs to be protected from /Users/rubber/linux/block/bfq-cgroup.c: 801
	 * destruction on the blkg_free of the original blkg (which /Users/rubber/linux/block/bfq-cgroup.c: 802
	 * invokes bfq_pd_free). We use an additional private /Users/rubber/linux/block/bfq-cgroup.c: 803
	 * refcounter for bfqg, to let it disappear only after no /Users/rubber/linux/block/bfq-cgroup.c: 804
	 * bfq_queue refers to it any longer. /Users/rubber/linux/block/bfq-cgroup.c: 805
 * bfq_flush_idle_tree - deactivate any entity on the idle tree of @st. /Users/rubber/linux/block/bfq-cgroup.c: 814
 * @st: the service tree being flushed. /Users/rubber/linux/block/bfq-cgroup.c: 815
 * bfq_reparent_leaf_entity - move leaf entity to the root_group. /Users/rubber/linux/block/bfq-cgroup.c: 826
 * @bfqd: the device data structure with the root group. /Users/rubber/linux/block/bfq-cgroup.c: 827
 * @entity: the entity to move, if entity is a leaf; or the parent entity /Users/rubber/linux/block/bfq-cgroup.c: 828
 *	    of an active leaf entity to move, if entity is not a leaf. /Users/rubber/linux/block/bfq-cgroup.c: 829
 * bfq_reparent_active_queues - move to the root group all active queues. /Users/rubber/linux/block/bfq-cgroup.c: 855
 * @bfqd: the device data structure with the root group. /Users/rubber/linux/block/bfq-cgroup.c: 856
 * @bfqg: the group to move from. /Users/rubber/linux/block/bfq-cgroup.c: 857
 * @st: the service tree to start the search from. /Users/rubber/linux/block/bfq-cgroup.c: 858
 * bfq_pd_offline - deactivate the entity associated with @pd, /Users/rubber/linux/block/bfq-cgroup.c: 878
 *		    and reparent its children entities. /Users/rubber/linux/block/bfq-cgroup.c: 879
 * @pd: descriptor of the policy going offline. /Users/rubber/linux/block/bfq-cgroup.c: 880
 * blkio already grabs the queue_lock for us, so no need to use /Users/rubber/linux/block/bfq-cgroup.c: 882
 * RCU-based magic /Users/rubber/linux/block/bfq-cgroup.c: 883
	/* /Users/rubber/linux/block/bfq-cgroup.c: 899
	 * Empty all service_trees belonging to this group before /Users/rubber/linux/block/bfq-cgroup.c: 900
	 * deactivating the group itself. /Users/rubber/linux/block/bfq-cgroup.c: 901
		/* /Users/rubber/linux/block/bfq-cgroup.c: 906
		 * It may happen that some queues are still active /Users/rubber/linux/block/bfq-cgroup.c: 907
		 * (busy) upon group destruction (if the corresponding /Users/rubber/linux/block/bfq-cgroup.c: 908
		 * processes have been forced to terminate). We move /Users/rubber/linux/block/bfq-cgroup.c: 909
		 * all the leaf entities corresponding to these queues /Users/rubber/linux/block/bfq-cgroup.c: 910
		 * to the root_group. /Users/rubber/linux/block/bfq-cgroup.c: 911
		 * Also, it may happen that the group has an entity /Users/rubber/linux/block/bfq-cgroup.c: 912
		 * in service, which is disconnected from the active /Users/rubber/linux/block/bfq-cgroup.c: 913
		 * tree: it must be moved, too. /Users/rubber/linux/block/bfq-cgroup.c: 914
		 * There is no need to put the sync queues, as the /Users/rubber/linux/block/bfq-cgroup.c: 915
		 * scheduler has taken no reference. /Users/rubber/linux/block/bfq-cgroup.c: 916
		/* /Users/rubber/linux/block/bfq-cgroup.c: 920
		 * The idle tree may still contain bfq_queues /Users/rubber/linux/block/bfq-cgroup.c: 921
		 * belonging to exited task because they never /Users/rubber/linux/block/bfq-cgroup.c: 922
		 * migrated to a different cgroup from the one being /Users/rubber/linux/block/bfq-cgroup.c: 923
		 * destroyed now. In addition, even /Users/rubber/linux/block/bfq-cgroup.c: 924
		 * bfq_reparent_active_queues() may happen to add some /Users/rubber/linux/block/bfq-cgroup.c: 925
		 * entities to the idle tree. It happens if, in some /Users/rubber/linux/block/bfq-cgroup.c: 926
		 * of the calls to bfq_bfqq_move() performed by /Users/rubber/linux/block/bfq-cgroup.c: 927
		 * bfq_reparent_active_queues(), the queue to move is /Users/rubber/linux/block/bfq-cgroup.c: 928
		 * empty and gets expired. /Users/rubber/linux/block/bfq-cgroup.c: 929
	/* /Users/rubber/linux/block/bfq-cgroup.c: 940
	 * @blkg is going offline and will be ignored by /Users/rubber/linux/block/bfq-cgroup.c: 941
	 * blkg_[rw]stat_recursive_sum().  Transfer stats to the parent so /Users/rubber/linux/block/bfq-cgroup.c: 942
	 * that they don't get lost.  If IOs complete after this point, the /Users/rubber/linux/block/bfq-cgroup.c: 943
	 * stats for them will be lost.  Oh well... /Users/rubber/linux/block/bfq-cgroup.c: 944
	/* /Users/rubber/linux/block/bfq-cgroup.c: 1001
	 * Setting the prio_changed flag of the entity /Users/rubber/linux/block/bfq-cgroup.c: 1002
	 * to 1 with new_weight == weight would re-set /Users/rubber/linux/block/bfq-cgroup.c: 1003
	 * the value of the weight to its ioprio mapping. /Users/rubber/linux/block/bfq-cgroup.c: 1004
	 * Set the flag only if necessary. /Users/rubber/linux/block/bfq-cgroup.c: 1005
		/* /Users/rubber/linux/block/bfq-cgroup.c: 1009
		 * Make sure that the above new value has been /Users/rubber/linux/block/bfq-cgroup.c: 1010
		 * stored in bfqg->entity.new_weight before /Users/rubber/linux/block/bfq-cgroup.c: 1011
		 * setting the prio_changed flag. In fact, /Users/rubber/linux/block/bfq-cgroup.c: 1012
		 * this flag may be read asynchronously (in /Users/rubber/linux/block/bfq-cgroup.c: 1013
		 * critical sections protected by a different /Users/rubber/linux/block/bfq-cgroup.c: 1014
		 * lock than that held here), and finding this /Users/rubber/linux/block/bfq-cgroup.c: 1015
		 * flag set may cause the execution of the code /Users/rubber/linux/block/bfq-cgroup.c: 1016
		 * for updating parameters whose value may /Users/rubber/linux/block/bfq-cgroup.c: 1017
		 * depend also on bfqg->entity.new_weight (in /Users/rubber/linux/block/bfq-cgroup.c: 1018
		 * __bfq_entity_update_weight_prio). /Users/rubber/linux/block/bfq-cgroup.c: 1019
		 * This barrier makes sure that the new value /Users/rubber/linux/block/bfq-cgroup.c: 1020
		 * of bfqg->entity.new_weight is correctly /Users/rubber/linux/block/bfq-cgroup.c: 1021
		 * seen in that code. /Users/rubber/linux/block/bfq-cgroup.c: 1022
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-zoned.c: 1
 * Zoned block device handling /Users/rubber/linux/block/blk-zoned.c: 3
 * Copyright (c) 2015, Hannes Reinecke /Users/rubber/linux/block/blk-zoned.c: 5
 * Copyright (c) 2015, SUSE Linux GmbH /Users/rubber/linux/block/blk-zoned.c: 6
 * Copyright (c) 2016, Damien Le Moal /Users/rubber/linux/block/blk-zoned.c: 8
 * Copyright (c) 2016, Western Digital /Users/rubber/linux/block/blk-zoned.c: 9
 * blk_zone_cond_str - Return string XXX in BLK_ZONE_COND_XXX. /Users/rubber/linux/block/blk-zoned.c: 37
 * @zone_cond: BLK_ZONE_COND_XXX. /Users/rubber/linux/block/blk-zoned.c: 38
 * Description: Centralize block layer function to convert BLK_ZONE_COND_XXX /Users/rubber/linux/block/blk-zoned.c: 40
 * into string format. Useful in the debugging and tracing zone conditions. For /Users/rubber/linux/block/blk-zoned.c: 41
 * invalid BLK_ZONE_COND_XXX it returns string "UNKNOWN". /Users/rubber/linux/block/blk-zoned.c: 42
 * Return true if a request is a write requests that needs zone write locking. /Users/rubber/linux/block/blk-zoned.c: 56
 * blkdev_nr_zones - Get number of zones /Users/rubber/linux/block/blk-zoned.c: 112
 * @disk:	Target gendisk /Users/rubber/linux/block/blk-zoned.c: 113
 * Return the total number of zones of a zoned block device.  For a block /Users/rubber/linux/block/blk-zoned.c: 115
 * device without zone capabilities, the number of zones is always 0. /Users/rubber/linux/block/blk-zoned.c: 116
 * blkdev_report_zones - Get zones information /Users/rubber/linux/block/blk-zoned.c: 129
 * @bdev:	Target block device /Users/rubber/linux/block/blk-zoned.c: 130
 * @sector:	Sector from which to report zones /Users/rubber/linux/block/blk-zoned.c: 131
 * @nr_zones:	Maximum number of zones to report /Users/rubber/linux/block/blk-zoned.c: 132
 * @cb:		Callback function called for each reported zone /Users/rubber/linux/block/blk-zoned.c: 133
 * @data:	Private data for the callback /Users/rubber/linux/block/blk-zoned.c: 134
 * Description: /Users/rubber/linux/block/blk-zoned.c: 136
 *    Get zone information starting from the zone containing @sector for at most /Users/rubber/linux/block/blk-zoned.c: 137
 *    @nr_zones, and call @cb for each zone reported by the device. /Users/rubber/linux/block/blk-zoned.c: 138
 *    To report all zones in a device starting from @sector, the BLK_ALL_ZONES /Users/rubber/linux/block/blk-zoned.c: 139
 *    constant can be passed to @nr_zones. /Users/rubber/linux/block/blk-zoned.c: 140
 *    Returns the number of zones reported by the device, or a negative errno /Users/rubber/linux/block/blk-zoned.c: 141
 *    value in case of failure. /Users/rubber/linux/block/blk-zoned.c: 142
 *    Note: The caller must use memalloc_noXX_save/restore() calls to control /Users/rubber/linux/block/blk-zoned.c: 144
 *    memory allocations done within this function. /Users/rubber/linux/block/blk-zoned.c: 145
	/* /Users/rubber/linux/block/blk-zoned.c: 174
	 * For an all-zones reset, ignore conventional, empty, read-only /Users/rubber/linux/block/blk-zoned.c: 175
	 * and offline zones. /Users/rubber/linux/block/blk-zoned.c: 176
 * blkdev_zone_mgmt - Execute a zone management operation on a range of zones /Users/rubber/linux/block/blk-zoned.c: 250
 * @bdev:	Target block device /Users/rubber/linux/block/blk-zoned.c: 251
 * @op:		Operation to be performed on the zones /Users/rubber/linux/block/blk-zoned.c: 252
 * @sector:	Start sector of the first zone to operate on /Users/rubber/linux/block/blk-zoned.c: 253
 * @nr_sectors:	Number of sectors, should be at least the length of one zone and /Users/rubber/linux/block/blk-zoned.c: 254
 *		must be zone size aligned. /Users/rubber/linux/block/blk-zoned.c: 255
 * @gfp_mask:	Memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-zoned.c: 256
 * Description: /Users/rubber/linux/block/blk-zoned.c: 258
 *    Perform the specified operation on the range of zones specified by /Users/rubber/linux/block/blk-zoned.c: 259
 *    @sector..@sector+@nr_sectors. Specifying the entire disk sector range /Users/rubber/linux/block/blk-zoned.c: 260
 *    is valid, but the specified range should not contain conventional zones. /Users/rubber/linux/block/blk-zoned.c: 261
 *    The operation to execute on each zone can be a zone reset, open, close /Users/rubber/linux/block/blk-zoned.c: 262
 *    or finish request. /Users/rubber/linux/block/blk-zoned.c: 263
	/* /Users/rubber/linux/block/blk-zoned.c: 296
	 * In the case of a zone reset operation over all zones, /Users/rubber/linux/block/blk-zoned.c: 297
	 * REQ_OP_ZONE_RESET_ALL can be used with devices supporting this /Users/rubber/linux/block/blk-zoned.c: 298
	 * command. For other devices, we emulate this command behavior by /Users/rubber/linux/block/blk-zoned.c: 299
	 * identifying the zones needing a reset. /Users/rubber/linux/block/blk-zoned.c: 300
 * BLKREPORTZONE ioctl processing. /Users/rubber/linux/block/blk-zoned.c: 341
 * Called from blkdev_ioctl. /Users/rubber/linux/block/blk-zoned.c: 342
 * BLKRESETZONE, BLKOPENZONE, BLKCLOSEZONE and BLKFINISHZONE ioctl processing. /Users/rubber/linux/block/blk-zoned.c: 399
 * Called from blkdev_ioctl. /Users/rubber/linux/block/blk-zoned.c: 400
 * Helper function to check the validity of zones of a zoned block device. /Users/rubber/linux/block/blk-zoned.c: 478
	/* /Users/rubber/linux/block/blk-zoned.c: 488
	 * All zones must have the same size, with the exception on an eventual /Users/rubber/linux/block/blk-zoned.c: 489
	 * smaller last zone. /Users/rubber/linux/block/blk-zoned.c: 490
 * blk_revalidate_disk_zones - (re)allocate and initialize zone bitmaps /Users/rubber/linux/block/blk-zoned.c: 553
 * @disk:	Target disk /Users/rubber/linux/block/blk-zoned.c: 554
 * @update_driver_data:	Callback to update driver data on the frozen disk /Users/rubber/linux/block/blk-zoned.c: 555
 * Helper function for low-level device drivers to (re) allocate and initialize /Users/rubber/linux/block/blk-zoned.c: 557
 * a disk request queue zone bitmaps. This functions should normally be called /Users/rubber/linux/block/blk-zoned.c: 558
 * within the disk ->revalidate method for blk-mq based drivers.  For BIO based /Users/rubber/linux/block/blk-zoned.c: 559
 * drivers only q->nr_zones needs to be updated so that the sysfs exposed value /Users/rubber/linux/block/blk-zoned.c: 560
 * is correct. /Users/rubber/linux/block/blk-zoned.c: 561
 * If the @update_driver_data callback function is not NULL, the callback is /Users/rubber/linux/block/blk-zoned.c: 562
 * executed with the device request queue frozen after all zones have been /Users/rubber/linux/block/blk-zoned.c: 563
 * checked. /Users/rubber/linux/block/blk-zoned.c: 564
	/* /Users/rubber/linux/block/blk-zoned.c: 584
	 * Ensure that all memory allocations in this context are done as if /Users/rubber/linux/block/blk-zoned.c: 585
	 * GFP_NOIO was specified. /Users/rubber/linux/block/blk-zoned.c: 586
	/* /Users/rubber/linux/block/blk-zoned.c: 597
	 * If zones where reported, make sure that the entire disk capacity /Users/rubber/linux/block/blk-zoned.c: 598
	 * has been checked. /Users/rubber/linux/block/blk-zoned.c: 599
	/* /Users/rubber/linux/block/blk-zoned.c: 607
	 * Install the new bitmaps and update nr_zones only once the queue is /Users/rubber/linux/block/blk-zoned.c: 608
	 * stopped and all I/Os are completed (i.e. a scheduler is not /Users/rubber/linux/block/blk-zoned.c: 609
	 * referencing the bitmaps). /Users/rubber/linux/block/blk-zoned.c: 610
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-core.c: 1
 * Copyright (C) 1991, 1992 Linus Torvalds /Users/rubber/linux/block/blk-core.c: 3
 * Copyright (C) 1994,      Karl Keyte: Added support for disk statistics /Users/rubber/linux/block/blk-core.c: 4
 * Elevator latency, (C) 2000  Andrea Arcangeli <andrea@suse.de> SuSE /Users/rubber/linux/block/blk-core.c: 5
 * Queue request tables / lock, selectable elevator, Jens Axboe <axboe@suse.de> /Users/rubber/linux/block/blk-core.c: 6
 * kernel-doc documentation started by NeilBrown <neilb@cse.unsw.edu.au> /Users/rubber/linux/block/blk-core.c: 7
 *	-  July2000 /Users/rubber/linux/block/blk-core.c: 8
 * bio rewrite, highmem i/o, etc, Jens Axboe <axboe@suse.de> - may 2001 /Users/rubber/linux/block/blk-core.c: 9
 * This handles all read/write requests to block devices /Users/rubber/linux/block/blk-core.c: 13
 * For queue allocation /Users/rubber/linux/block/blk-core.c: 67
 * Controlling structure to kblockd /Users/rubber/linux/block/blk-core.c: 72
 * blk_queue_flag_set - atomically set a queue flag /Users/rubber/linux/block/blk-core.c: 77
 * @flag: flag to be set /Users/rubber/linux/block/blk-core.c: 78
 * @q: request queue /Users/rubber/linux/block/blk-core.c: 79
 * blk_queue_flag_clear - atomically clear a queue flag /Users/rubber/linux/block/blk-core.c: 88
 * @flag: flag to be cleared /Users/rubber/linux/block/blk-core.c: 89
 * @q: request queue /Users/rubber/linux/block/blk-core.c: 90
 * blk_queue_flag_test_and_set - atomically test and set a queue flag /Users/rubber/linux/block/blk-core.c: 99
 * @flag: flag to be set /Users/rubber/linux/block/blk-core.c: 100
 * @q: request queue /Users/rubber/linux/block/blk-core.c: 101
 * Returns the previous value of @flag - 0 if the flag was not set and 1 if /Users/rubber/linux/block/blk-core.c: 103
 * the flag was already set. /Users/rubber/linux/block/blk-core.c: 104
 * blk_op_str - Return string XXX in the REQ_OP_XXX. /Users/rubber/linux/block/blk-core.c: 150
 * @op: REQ_OP_XXX. /Users/rubber/linux/block/blk-core.c: 151
 * Description: Centralize block layer function to convert REQ_OP_XXX into /Users/rubber/linux/block/blk-core.c: 153
 * string format. Useful in the debugging and tracing bio or request. For /Users/rubber/linux/block/blk-core.c: 154
 * invalid REQ_OP_XXX it returns string "UNKNOWN". /Users/rubber/linux/block/blk-core.c: 155
 * blk_sync_queue - cancel any pending callbacks on a queue /Users/rubber/linux/block/blk-core.c: 252
 * @q: the queue /Users/rubber/linux/block/blk-core.c: 253
 * Description: /Users/rubber/linux/block/blk-core.c: 255
 *     The block layer may perform asynchronous callback activity /Users/rubber/linux/block/blk-core.c: 256
 *     on a queue, such as calling the unplug function after a timeout. /Users/rubber/linux/block/blk-core.c: 257
 *     A block device may call blk_sync_queue to ensure that any /Users/rubber/linux/block/blk-core.c: 258
 *     such activity is cancelled, thus allowing it to release resources /Users/rubber/linux/block/blk-core.c: 259
 *     that the callbacks might use. The caller must already have made sure /Users/rubber/linux/block/blk-core.c: 260
 *     that its ->submit_bio will not re-add plugging prior to calling /Users/rubber/linux/block/blk-core.c: 261
 *     this function. /Users/rubber/linux/block/blk-core.c: 262
 *     This function does not cancel any asynchronous activity arising /Users/rubber/linux/block/blk-core.c: 264
 *     out of elevator or throttling code. That would require elevator_exit() /Users/rubber/linux/block/blk-core.c: 265
 *     and blkcg_exit_queue() to be called with queue lock initialized. /Users/rubber/linux/block/blk-core.c: 266
 * blk_set_pm_only - increment pm_only counter /Users/rubber/linux/block/blk-core.c: 277
 * @q: request queue pointer /Users/rubber/linux/block/blk-core.c: 278
 * blk_put_queue - decrement the request_queue refcount /Users/rubber/linux/block/blk-core.c: 298
 * @q: the request_queue structure to decrement the refcount for /Users/rubber/linux/block/blk-core.c: 299
 * Decrements the refcount of the request_queue kobject. When this reaches 0 /Users/rubber/linux/block/blk-core.c: 301
 * we'll have blk_release_queue() called. /Users/rubber/linux/block/blk-core.c: 302
 * Context: Any context, but the last reference must not be dropped from /Users/rubber/linux/block/blk-core.c: 304
 *          atomic context. /Users/rubber/linux/block/blk-core.c: 305
	/* /Users/rubber/linux/block/blk-core.c: 315
	 * When queue DYING flag is set, we need to block new req /Users/rubber/linux/block/blk-core.c: 316
	 * entering queue, so we call blk_freeze_queue_start() to /Users/rubber/linux/block/blk-core.c: 317
	 * prevent I/O from crossing blk_queue_enter(). /Users/rubber/linux/block/blk-core.c: 318
 * blk_cleanup_queue - shutdown a request queue /Users/rubber/linux/block/blk-core.c: 335
 * @q: request queue to shutdown /Users/rubber/linux/block/blk-core.c: 336
 * Mark @q DYING, drain all pending requests, mark @q DEAD, destroy and /Users/rubber/linux/block/blk-core.c: 338
 * put it.  All future requests will be failed immediately with -ENODEV. /Users/rubber/linux/block/blk-core.c: 339
 * Context: can sleep /Users/rubber/linux/block/blk-core.c: 341
	/* /Users/rubber/linux/block/blk-core.c: 356
	 * Drain all requests queued before DYING marking. Set DEAD flag to /Users/rubber/linux/block/blk-core.c: 357
	 * prevent that blk_mq_run_hw_queues() accesses the hardware queues /Users/rubber/linux/block/blk-core.c: 358
	 * after draining finished. /Users/rubber/linux/block/blk-core.c: 359
	/* /Users/rubber/linux/block/blk-core.c: 371
	 * In theory, request pool of sched_tags belongs to request queue. /Users/rubber/linux/block/blk-core.c: 372
	 * However, the current implementation requires tag_set for freeing /Users/rubber/linux/block/blk-core.c: 373
	 * requests, so free the pool now. /Users/rubber/linux/block/blk-core.c: 374
	 * /Users/rubber/linux/block/blk-core.c: 375
	 * Queue has become frozen, there can't be any in-queue requests, so /Users/rubber/linux/block/blk-core.c: 376
	 * it is safe to free requests now. /Users/rubber/linux/block/blk-core.c: 377
 * blk_queue_enter() - try to increase q->q_usage_counter /Users/rubber/linux/block/blk-core.c: 392
 * @q: request queue pointer /Users/rubber/linux/block/blk-core.c: 393
 * @flags: BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PM /Users/rubber/linux/block/blk-core.c: 394
		/* /Users/rubber/linux/block/blk-core.c: 404
		 * read pair of barrier in blk_freeze_queue_start(), we need to /Users/rubber/linux/block/blk-core.c: 405
		 * order reading __PERCPU_REF_DEAD flag of .q_usage_counter and /Users/rubber/linux/block/blk-core.c: 406
		 * reading .mq_freeze_depth or queue dying flag, otherwise the /Users/rubber/linux/block/blk-core.c: 407
		 * following wait may never return if the two reads are /Users/rubber/linux/block/blk-core.c: 408
		 * reordered. /Users/rubber/linux/block/blk-core.c: 409
		/* /Users/rubber/linux/block/blk-core.c: 435
		 * read pair of barrier in blk_freeze_queue_start(), we need to /Users/rubber/linux/block/blk-core.c: 436
		 * order reading __PERCPU_REF_DEAD flag of .q_usage_counter and /Users/rubber/linux/block/blk-core.c: 437
		 * reading .mq_freeze_depth or queue dying flag, otherwise the /Users/rubber/linux/block/blk-core.c: 438
		 * following wait may never return if the two reads are /Users/rubber/linux/block/blk-core.c: 439
		 * reordered. /Users/rubber/linux/block/blk-core.c: 440
	/* /Users/rubber/linux/block/blk-core.c: 526
	 * Init percpu_ref in atomic mode so that it's faster to shutdown. /Users/rubber/linux/block/blk-core.c: 527
	 * See blk_register_queue() for details. /Users/rubber/linux/block/blk-core.c: 528
 * blk_get_queue - increment the request_queue refcount /Users/rubber/linux/block/blk-core.c: 558
 * @q: the request_queue structure to increment the refcount for /Users/rubber/linux/block/blk-core.c: 559
 * Increment the refcount of the request_queue kobject. /Users/rubber/linux/block/blk-core.c: 561
 * Context: Any context. /Users/rubber/linux/block/blk-core.c: 563
 * Check whether this bio extends beyond the end of the device or partition. /Users/rubber/linux/block/blk-core.c: 649
 * This may well happen - the kernel calls bread() without checking the size of /Users/rubber/linux/block/blk-core.c: 650
 * the device, e.g., when mounting a file system. /Users/rubber/linux/block/blk-core.c: 651
 * Remap block n of partition p to block n+start(p) of the disk. /Users/rubber/linux/block/blk-core.c: 668
 * Check write append to a zoned block device. /Users/rubber/linux/block/blk-core.c: 687
	/* /Users/rubber/linux/block/blk-core.c: 704
	 * Not allowed to cross zone boundaries. Otherwise, the BIO will be /Users/rubber/linux/block/blk-core.c: 705
	 * split and could result in non-contiguous sectors being written in /Users/rubber/linux/block/blk-core.c: 706
	 * different zones. /Users/rubber/linux/block/blk-core.c: 707
	/* /Users/rubber/linux/block/blk-core.c: 734
	 * For a REQ_NOWAIT based request, return -EOPNOTSUPP /Users/rubber/linux/block/blk-core.c: 735
	 * if queue does not support NOWAIT. /Users/rubber/linux/block/blk-core.c: 736
	/* /Users/rubber/linux/block/blk-core.c: 752
	 * Filter flush bio's early so that bio based drivers without flush /Users/rubber/linux/block/blk-core.c: 753
	 * support don't have to worry about them. /Users/rubber/linux/block/blk-core.c: 754
	/* /Users/rubber/linux/block/blk-core.c: 805
	 * Various block parts want %current->io_context, so allocate it up /Users/rubber/linux/block/blk-core.c: 806
	 * front rather than dealing with lots of pain to allocate it only /Users/rubber/linux/block/blk-core.c: 807
	 * where needed. This may fail and the block layer knows how to live /Users/rubber/linux/block/blk-core.c: 808
	 * with it. /Users/rubber/linux/block/blk-core.c: 809
		/* Now that enqueuing has been traced, we need to trace /Users/rubber/linux/block/blk-core.c: 822
		 * completion as well. /Users/rubber/linux/block/blk-core.c: 823
 * The loop in this function may be a bit non-obvious, and so deserves some /Users/rubber/linux/block/blk-core.c: 857
 * explanation: /Users/rubber/linux/block/blk-core.c: 858
 *  - Before entering the loop, bio->bi_next is NULL (as all callers ensure /Users/rubber/linux/block/blk-core.c: 860
 *    that), so we have a list with a single bio. /Users/rubber/linux/block/blk-core.c: 861
 *  - We pretend that we have just taken it off a longer list, so we assign /Users/rubber/linux/block/blk-core.c: 862
 *    bio_list to a pointer to the bio_list_on_stack, thus initialising the /Users/rubber/linux/block/blk-core.c: 863
 *    bio_list of new bios to be added.  ->submit_bio() may indeed add some more /Users/rubber/linux/block/blk-core.c: 864
 *    bios through a recursive call to submit_bio_noacct.  If it did, we find a /Users/rubber/linux/block/blk-core.c: 865
 *    non-NULL value in bio_list and re-enter the loop from the top. /Users/rubber/linux/block/blk-core.c: 866
 *  - In this case we really did just take the bio of the top of the list (no /Users/rubber/linux/block/blk-core.c: 867
 *    pretending) and so remove it from bio_list, and call into ->submit_bio() /Users/rubber/linux/block/blk-core.c: 868
 *    again. /Users/rubber/linux/block/blk-core.c: 869
 * bio_list_on_stack[0] contains bios submitted by the current ->submit_bio. /Users/rubber/linux/block/blk-core.c: 871
 * bio_list_on_stack[1] contains bios that were submitted before the current /Users/rubber/linux/block/blk-core.c: 872
 *	->submit_bio_bio, but that haven't been processed yet. /Users/rubber/linux/block/blk-core.c: 873
		/* /Users/rubber/linux/block/blk-core.c: 888
		 * Create a fresh bio_list for all subordinate requests. /Users/rubber/linux/block/blk-core.c: 889
		/* /Users/rubber/linux/block/blk-core.c: 896
		 * Sort new bios into those for a lower level and those for the /Users/rubber/linux/block/blk-core.c: 897
		 * same level. /Users/rubber/linux/block/blk-core.c: 898
		/* /Users/rubber/linux/block/blk-core.c: 908
		 * Now assemble so we handle the lowest level first. /Users/rubber/linux/block/blk-core.c: 909
 * submit_bio_noacct - re-submit a bio to the block device layer for I/O /Users/rubber/linux/block/blk-core.c: 933
 * @bio:  The bio describing the location in memory and on the device. /Users/rubber/linux/block/blk-core.c: 934
 * This is a version of submit_bio() that shall only be used for I/O that is /Users/rubber/linux/block/blk-core.c: 936
 * resubmitted to lower level drivers by stacking block drivers.  All file /Users/rubber/linux/block/blk-core.c: 937
 * systems and other upper level users of the block layer should use /Users/rubber/linux/block/blk-core.c: 938
 * submit_bio() instead. /Users/rubber/linux/block/blk-core.c: 939
	/* /Users/rubber/linux/block/blk-core.c: 943
	 * We only want one ->submit_bio to be active at a time, else stack /Users/rubber/linux/block/blk-core.c: 944
	 * usage with stacked devices could be a problem.  Use current->bio_list /Users/rubber/linux/block/blk-core.c: 945
	 * to collect a list of requests submited by a ->submit_bio method while /Users/rubber/linux/block/blk-core.c: 946
	 * it is active, and then process them after it returned. /Users/rubber/linux/block/blk-core.c: 947
 * submit_bio - submit a bio to the block device layer for I/O /Users/rubber/linux/block/blk-core.c: 959
 * @bio: The &struct bio which describes the I/O /Users/rubber/linux/block/blk-core.c: 960
 * submit_bio() is used to submit I/O requests to block devices.  It is passed a /Users/rubber/linux/block/blk-core.c: 962
 * fully set up &struct bio that describes the I/O that needs to be done.  The /Users/rubber/linux/block/blk-core.c: 963
 * bio will be send to the device described by the bi_bdev field. /Users/rubber/linux/block/blk-core.c: 964
 * The success/failure status of the request, along with notification of /Users/rubber/linux/block/blk-core.c: 966
 * completion, is delivered asynchronously through the ->bi_end_io() callback /Users/rubber/linux/block/blk-core.c: 967
 * in @bio.  The bio must NOT be touched by thecaller until ->bi_end_io() has /Users/rubber/linux/block/blk-core.c: 968
 * been called. /Users/rubber/linux/block/blk-core.c: 969
	/* /Users/rubber/linux/block/blk-core.c: 976
	 * If it's a regular read/write or a barrier with data attached, /Users/rubber/linux/block/blk-core.c: 977
	 * go through the normal accounting stuff before submission. /Users/rubber/linux/block/blk-core.c: 978
	/* /Users/rubber/linux/block/blk-core.c: 997
	 * If we're reading data that is part of the userspace workingset, count /Users/rubber/linux/block/blk-core.c: 998
	 * submission time as memory stall.  When the device is congested, or /Users/rubber/linux/block/blk-core.c: 999
	 * the submitting cgroup IO-throttled, submission can be a significant /Users/rubber/linux/block/blk-core.c: 1000
	 * part of overall IO time. /Users/rubber/linux/block/blk-core.c: 1001
 * bio_poll - poll for BIO completions /Users/rubber/linux/block/blk-core.c: 1018
 * @bio: bio to poll for /Users/rubber/linux/block/blk-core.c: 1019
 * @iob: batches of IO /Users/rubber/linux/block/blk-core.c: 1020
 * @flags: BLK_POLL_* flags that control the behavior /Users/rubber/linux/block/blk-core.c: 1021
 * Poll for completions on queue associated with the bio. Returns number of /Users/rubber/linux/block/blk-core.c: 1023
 * completed entries found. /Users/rubber/linux/block/blk-core.c: 1024
 * Note: the caller must either be the context that submitted @bio, or /Users/rubber/linux/block/blk-core.c: 1026
 * be in a RCU critical section to prevent freeing of @bio. /Users/rubber/linux/block/blk-core.c: 1027
 * Helper to implement file_operations.iopoll.  Requires the bio to be stored /Users/rubber/linux/block/blk-core.c: 1054
 * in iocb->private, and cleared before freeing the bio. /Users/rubber/linux/block/blk-core.c: 1055
	/* /Users/rubber/linux/block/blk-core.c: 1063
	 * Note: the bio cache only uses SLAB_TYPESAFE_BY_RCU, so bio can /Users/rubber/linux/block/blk-core.c: 1064
	 * point to a freshly allocated bio at this point.  If that happens /Users/rubber/linux/block/blk-core.c: 1065
	 * we have a few cases to consider: /Users/rubber/linux/block/blk-core.c: 1066
	 * /Users/rubber/linux/block/blk-core.c: 1067
	 *  1) the bio is beeing initialized and bi_bdev is NULL.  We can just /Users/rubber/linux/block/blk-core.c: 1068
	 *     simply nothing in this case /Users/rubber/linux/block/blk-core.c: 1069
	 *  2) the bio points to a not poll enabled device.  bio_poll will catch /Users/rubber/linux/block/blk-core.c: 1070
	 *     this and return 0 /Users/rubber/linux/block/blk-core.c: 1071
	 *  3) the bio points to a poll capable device, including but not /Users/rubber/linux/block/blk-core.c: 1072
	 *     limited to the one that the original bio pointed to.  In this /Users/rubber/linux/block/blk-core.c: 1073
	 *     case we will call into the actual poll method and poll for I/O, /Users/rubber/linux/block/blk-core.c: 1074
	 *     even if we don't need to, but it won't cause harm either. /Users/rubber/linux/block/blk-core.c: 1075
	 * /Users/rubber/linux/block/blk-core.c: 1076
	 * For cases 2) and 3) above the RCU grace period ensures that bi_bdev /Users/rubber/linux/block/blk-core.c: 1077
	 * is still allocated. Because partitions hold a reference to the whole /Users/rubber/linux/block/blk-core.c: 1078
	 * device bdev and thus disk, the disk is also still valid.  Grabbing /Users/rubber/linux/block/blk-core.c: 1079
	 * a reference to the queue in bio_poll() ensures the hctxs and requests /Users/rubber/linux/block/blk-core.c: 1080
	 * are still valid as well. /Users/rubber/linux/block/blk-core.c: 1081
 * blk_cloned_rq_check_limits - Helper function to check a cloned request /Users/rubber/linux/block/blk-core.c: 1094
 *                              for the new queue limits /Users/rubber/linux/block/blk-core.c: 1095
 * @q:  the queue /Users/rubber/linux/block/blk-core.c: 1096
 * @rq: the request being checked /Users/rubber/linux/block/blk-core.c: 1097
 * Description: /Users/rubber/linux/block/blk-core.c: 1099
 *    @rq may have been made based on weaker limitations of upper-level queues /Users/rubber/linux/block/blk-core.c: 1100
 *    in request stacking drivers, and it may violate the limitation of @q. /Users/rubber/linux/block/blk-core.c: 1101
 *    Since the block layer and the underlying device driver trust @rq /Users/rubber/linux/block/blk-core.c: 1102
 *    after it is inserted to @q, it should be checked against @q before /Users/rubber/linux/block/blk-core.c: 1103
 *    the insertion using this generic function. /Users/rubber/linux/block/blk-core.c: 1104
 *    Request stacking drivers like request-based dm may change the queue /Users/rubber/linux/block/blk-core.c: 1106
 *    limits when retrying requests on other queues. Those requests need /Users/rubber/linux/block/blk-core.c: 1107
 *    to be checked against the new queue limits again during dispatch. /Users/rubber/linux/block/blk-core.c: 1108
		/* /Users/rubber/linux/block/blk-core.c: 1116
		 * SCSI device does not have a good way to return if /Users/rubber/linux/block/blk-core.c: 1117
		 * Write Same/Zero is actually supported. If a device rejects /Users/rubber/linux/block/blk-core.c: 1118
		 * a non-read/write command (discard, write same,etc.) the /Users/rubber/linux/block/blk-core.c: 1119
		 * low-level device driver will set the relevant queue limit to /Users/rubber/linux/block/blk-core.c: 1120
		 * 0 to prevent blk-lib from issuing more of the offending /Users/rubber/linux/block/blk-core.c: 1121
		 * operations. Commands queued prior to the queue limit being /Users/rubber/linux/block/blk-core.c: 1122
		 * reset need to be completed with BLK_STS_NOTSUPP to avoid I/O /Users/rubber/linux/block/blk-core.c: 1123
		 * errors being propagated to upper layers. /Users/rubber/linux/block/blk-core.c: 1124
	/* /Users/rubber/linux/block/blk-core.c: 1134
	 * The queue settings related to segment counting may differ from the /Users/rubber/linux/block/blk-core.c: 1135
	 * original queue. /Users/rubber/linux/block/blk-core.c: 1136
 * blk_insert_cloned_request - Helper for stacking drivers to submit a request /Users/rubber/linux/block/blk-core.c: 1149
 * @q:  the queue to submit the request /Users/rubber/linux/block/blk-core.c: 1150
 * @rq: the request being queued /Users/rubber/linux/block/blk-core.c: 1151
	/* /Users/rubber/linux/block/blk-core.c: 1170
	 * Since we have a scheduler attached on the top device, /Users/rubber/linux/block/blk-core.c: 1171
	 * bypass a potential scheduler on the bottom device for /Users/rubber/linux/block/blk-core.c: 1172
	 * insert. /Users/rubber/linux/block/blk-core.c: 1173
 * blk_rq_err_bytes - determine number of bytes till the next failure boundary /Users/rubber/linux/block/blk-core.c: 1180
 * @rq: request to examine /Users/rubber/linux/block/blk-core.c: 1181
 * Description: /Users/rubber/linux/block/blk-core.c: 1183
 *     A request could be merge of IOs which require different failure /Users/rubber/linux/block/blk-core.c: 1184
 *     handling.  This function determines the number of bytes which /Users/rubber/linux/block/blk-core.c: 1185
 *     can be failed from the beginning of the request without /Users/rubber/linux/block/blk-core.c: 1186
 *     crossing into area which need to be retried further. /Users/rubber/linux/block/blk-core.c: 1187
 * Return: /Users/rubber/linux/block/blk-core.c: 1189
 *     The number of bytes to fail. /Users/rubber/linux/block/blk-core.c: 1190
	/* /Users/rubber/linux/block/blk-core.c: 1201
	 * Currently the only 'mixing' which can happen is between /Users/rubber/linux/block/blk-core.c: 1202
	 * different fastfail types.  We can safely fail portions /Users/rubber/linux/block/blk-core.c: 1203
	 * which have all the failfast bits that the first one has - /Users/rubber/linux/block/blk-core.c: 1204
	 * the ones which are at least as eager to fail as the first /Users/rubber/linux/block/blk-core.c: 1205
	 * one. /Users/rubber/linux/block/blk-core.c: 1206
 * bio_start_io_acct - start I/O accounting for bio based drivers /Users/rubber/linux/block/blk-core.c: 1277
 * @bio:	bio to start account for /Users/rubber/linux/block/blk-core.c: 1278
 * Returns the start time that should be passed back to bio_end_io_acct(). /Users/rubber/linux/block/blk-core.c: 1280
 * Steal bios from a request and add them to a bio list. /Users/rubber/linux/block/blk-core.c: 1324
 * The request must not have been partially completed before. /Users/rubber/linux/block/blk-core.c: 1325
 * rq_flush_dcache_pages - Helper function to flush all pages in a request /Users/rubber/linux/block/blk-core.c: 1346
 * @rq: the request to be flushed /Users/rubber/linux/block/blk-core.c: 1347
 * Description: /Users/rubber/linux/block/blk-core.c: 1349
 *     Flush all pages in @rq. /Users/rubber/linux/block/blk-core.c: 1350
 * blk_lld_busy - Check if underlying low-level drivers of a device are busy /Users/rubber/linux/block/blk-core.c: 1364
 * @q : the queue of the device being checked /Users/rubber/linux/block/blk-core.c: 1365
 * Description: /Users/rubber/linux/block/blk-core.c: 1367
 *    Check if underlying low-level drivers of a device are busy. /Users/rubber/linux/block/blk-core.c: 1368
 *    If the drivers want to export their busy state, they must set own /Users/rubber/linux/block/blk-core.c: 1369
 *    exporting function using blk_queue_lld_busy() first. /Users/rubber/linux/block/blk-core.c: 1370
 *    Basically, this function is used only by request stacking drivers /Users/rubber/linux/block/blk-core.c: 1372
 *    to stop dispatching requests to underlying devices when underlying /Users/rubber/linux/block/blk-core.c: 1373
 *    devices are busy.  This behavior helps more I/O merging on the queue /Users/rubber/linux/block/blk-core.c: 1374
 *    of the request stacking driver and prevents I/O throughput regression /Users/rubber/linux/block/blk-core.c: 1375
 *    on burst I/O load. /Users/rubber/linux/block/blk-core.c: 1376
 * Return: /Users/rubber/linux/block/blk-core.c: 1378
 *    0 - Not busy (The request stacking driver should dispatch request) /Users/rubber/linux/block/blk-core.c: 1379
 *    1 - Busy (The request stacking driver should stop dispatching request) /Users/rubber/linux/block/blk-core.c: 1380
 * blk_rq_unprep_clone - Helper function to free all bios in a cloned request /Users/rubber/linux/block/blk-core.c: 1392
 * @rq: the clone request to be cleaned up /Users/rubber/linux/block/blk-core.c: 1393
 * Description: /Users/rubber/linux/block/blk-core.c: 1395
 *     Free all bios in @rq for a cloned request. /Users/rubber/linux/block/blk-core.c: 1396
 * blk_rq_prep_clone - Helper function to setup clone request /Users/rubber/linux/block/blk-core.c: 1411
 * @rq: the request to be setup /Users/rubber/linux/block/blk-core.c: 1412
 * @rq_src: original request to be cloned /Users/rubber/linux/block/blk-core.c: 1413
 * @bs: bio_set that bios for clone are allocated from /Users/rubber/linux/block/blk-core.c: 1414
 * @gfp_mask: memory allocation mask for bio /Users/rubber/linux/block/blk-core.c: 1415
 * @bio_ctr: setup function to be called for each clone bio. /Users/rubber/linux/block/blk-core.c: 1416
 *           Returns %0 for success, non %0 for failure. /Users/rubber/linux/block/blk-core.c: 1417
 * @data: private data to be passed to @bio_ctr /Users/rubber/linux/block/blk-core.c: 1418
 * Description: /Users/rubber/linux/block/blk-core.c: 1420
 *     Clones bios in @rq_src to @rq, and copies attributes of @rq_src to @rq. /Users/rubber/linux/block/blk-core.c: 1421
 *     Also, pages which the original bios are pointing to are not copied /Users/rubber/linux/block/blk-core.c: 1422
 *     and the cloned bios just point same pages. /Users/rubber/linux/block/blk-core.c: 1423
 *     So cloned bios must be completed before original bios, which means /Users/rubber/linux/block/blk-core.c: 1424
 *     the caller must complete @rq before @rq_src. /Users/rubber/linux/block/blk-core.c: 1425
	/* /Users/rubber/linux/block/blk-core.c: 1495
	 * If this is a nested plug, don't actually assign it. /Users/rubber/linux/block/blk-core.c: 1496
	/* /Users/rubber/linux/block/blk-core.c: 1510
	 * Store ordering should not be needed here, since a potential /Users/rubber/linux/block/blk-core.c: 1511
	 * preempt will imply a full memory barrier /Users/rubber/linux/block/blk-core.c: 1512
 * blk_start_plug - initialize blk_plug and track it inside the task_struct /Users/rubber/linux/block/blk-core.c: 1518
 * @plug:	The &struct blk_plug that needs to be initialized /Users/rubber/linux/block/blk-core.c: 1519
 * Description: /Users/rubber/linux/block/blk-core.c: 1521
 *   blk_start_plug() indicates to the block layer an intent by the caller /Users/rubber/linux/block/blk-core.c: 1522
 *   to submit multiple I/O requests in a batch.  The block layer may use /Users/rubber/linux/block/blk-core.c: 1523
 *   this hint to defer submitting I/Os from the caller until blk_finish_plug() /Users/rubber/linux/block/blk-core.c: 1524
 *   is called.  However, the block layer may choose to submit requests /Users/rubber/linux/block/blk-core.c: 1525
 *   before a call to blk_finish_plug() if the number of queued I/Os /Users/rubber/linux/block/blk-core.c: 1526
 *   exceeds %BLK_MAX_REQUEST_COUNT, or if the size of the I/O is larger than /Users/rubber/linux/block/blk-core.c: 1527
 *   %BLK_PLUG_FLUSH_SIZE.  The queued I/Os may also be submitted early if /Users/rubber/linux/block/blk-core.c: 1528
 *   the task schedules (see below). /Users/rubber/linux/block/blk-core.c: 1529
 *   Tracking blk_plug inside the task_struct will help with auto-flushing the /Users/rubber/linux/block/blk-core.c: 1531
 *   pending I/O should the task end up blocking between blk_start_plug() and /Users/rubber/linux/block/blk-core.c: 1532
 *   blk_finish_plug(). This is important from a performance perspective, but /Users/rubber/linux/block/blk-core.c: 1533
 *   also ensures that we don't deadlock. For instance, if the task is blocking /Users/rubber/linux/block/blk-core.c: 1534
 *   for a memory allocation, memory reclaim could end up wanting to free a /Users/rubber/linux/block/blk-core.c: 1535
 *   page belonging to that request that is currently residing in our private /Users/rubber/linux/block/blk-core.c: 1536
 *   plug. By flushing the pending I/O when the process goes to sleep, we avoid /Users/rubber/linux/block/blk-core.c: 1537
 *   this kind of deadlock. /Users/rubber/linux/block/blk-core.c: 1538
	/* /Users/rubber/linux/block/blk-core.c: 1594
	 * Unconditionally flush out cached requests, even if the unplug /Users/rubber/linux/block/blk-core.c: 1595
	 * event came from schedule. Since we know hold references to the /Users/rubber/linux/block/blk-core.c: 1596
	 * queue for cached requests, we don't want a blocked task holding /Users/rubber/linux/block/blk-core.c: 1597
	 * up a queue freeze/quiesce event. /Users/rubber/linux/block/blk-core.c: 1598
 * blk_finish_plug - mark the end of a batch of submitted I/O /Users/rubber/linux/block/blk-core.c: 1605
 * @plug:	The &struct blk_plug passed to blk_start_plug() /Users/rubber/linux/block/blk-core.c: 1606
 * Description: /Users/rubber/linux/block/blk-core.c: 1608
 * Indicate that a batch of I/O submissions is complete.  This function /Users/rubber/linux/block/blk-core.c: 1609
 * must be paired with an initial call to blk_start_plug().  The intent /Users/rubber/linux/block/blk-core.c: 1610
 * is to allow the block layer to optimize I/O submission.  See the /Users/rubber/linux/block/blk-core.c: 1611
 * documentation for blk_start_plug() for more information. /Users/rubber/linux/block/blk-core.c: 1612
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-rq-qos.c: 1
 * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded, /Users/rubber/linux/block/blk-rq-qos.c: 6
 * false if 'v' + 1 would be bigger than 'below'. /Users/rubber/linux/block/blk-rq-qos.c: 7
 * Return true, if we can't increase the depth further by scaling /Users/rubber/linux/block/blk-rq-qos.c: 114
	/* /Users/rubber/linux/block/blk-rq-qos.c: 121
	 * For QD=1 devices, this is a special case. It's important for those /Users/rubber/linux/block/blk-rq-qos.c: 122
	 * to have one request ready when one completes, so force a depth of /Users/rubber/linux/block/blk-rq-qos.c: 123
	 * 2 for those devices. On the backend, it'll be a depth of 1 anyway, /Users/rubber/linux/block/blk-rq-qos.c: 124
	 * since the device can't have more than that in flight. If we're /Users/rubber/linux/block/blk-rq-qos.c: 125
	 * scaling down, then keep a setting of 1/1/1. /Users/rubber/linux/block/blk-rq-qos.c: 126
		/* /Users/rubber/linux/block/blk-rq-qos.c: 136
		 * scale_step == 0 is our default state. If we have suffered /Users/rubber/linux/block/blk-rq-qos.c: 137
		 * latency spikes, step will be > 0, and we shrink the /Users/rubber/linux/block/blk-rq-qos.c: 138
		 * allowed write depths. If step is < 0, we're only doing /Users/rubber/linux/block/blk-rq-qos.c: 139
		 * writes, and we allow a temporarily higher depth to /Users/rubber/linux/block/blk-rq-qos.c: 140
		 * increase performance. /Users/rubber/linux/block/blk-rq-qos.c: 141
	/* /Users/rubber/linux/block/blk-rq-qos.c: 166
	 * Hit max in previous round, stop here /Users/rubber/linux/block/blk-rq-qos.c: 167
 * Scale rwb down. If 'hard_throttle' is set, do it quicker, since we /Users/rubber/linux/block/blk-rq-qos.c: 179
 * had a latency violation. Returns true on success and returns false if /Users/rubber/linux/block/blk-rq-qos.c: 180
 * scaling down wasn't possible. /Users/rubber/linux/block/blk-rq-qos.c: 181
	/* /Users/rubber/linux/block/blk-rq-qos.c: 185
	 * Stop scaling down when we've hit the limit. This also prevents /Users/rubber/linux/block/blk-rq-qos.c: 186
	 * ->scale_step from going to crazy values, if the device can't /Users/rubber/linux/block/blk-rq-qos.c: 187
	 * keep up. /Users/rubber/linux/block/blk-rq-qos.c: 188
	/* /Users/rubber/linux/block/blk-rq-qos.c: 219
	 * If we fail to get a budget, return -1 to interrupt the wake up loop /Users/rubber/linux/block/blk-rq-qos.c: 220
	 * in __wake_up_common. /Users/rubber/linux/block/blk-rq-qos.c: 221
 * rq_qos_wait - throttle on a rqw if we need to /Users/rubber/linux/block/blk-rq-qos.c: 234
 * @rqw: rqw to throttle on /Users/rubber/linux/block/blk-rq-qos.c: 235
 * @private_data: caller provided specific data /Users/rubber/linux/block/blk-rq-qos.c: 236
 * @acquire_inflight_cb: inc the rqw->inflight counter if we can /Users/rubber/linux/block/blk-rq-qos.c: 237
 * @cleanup_cb: the callback to cleanup in case we race with a waker /Users/rubber/linux/block/blk-rq-qos.c: 238
 * This provides a uniform place for the rq_qos users to do their throttling. /Users/rubber/linux/block/blk-rq-qos.c: 240
 * Since you can end up with a lot of things sleeping at once, this manages the /Users/rubber/linux/block/blk-rq-qos.c: 241
 * waking up based on the resources available.  The acquire_inflight_cb should /Users/rubber/linux/block/blk-rq-qos.c: 242
 * inc the rqw->inflight if we have the ability to do so, or return false if not /Users/rubber/linux/block/blk-rq-qos.c: 243
 * and then we will sleep until the room becomes available. /Users/rubber/linux/block/blk-rq-qos.c: 244
 * cleanup_cb is in case that we race with a waker and need to cleanup the /Users/rubber/linux/block/blk-rq-qos.c: 246
 * inflight count accordingly. /Users/rubber/linux/block/blk-rq-qos.c: 247
			/* /Users/rubber/linux/block/blk-rq-qos.c: 278
			 * We raced with wbt_wake_function() getting a token, /Users/rubber/linux/block/blk-rq-qos.c: 279
			 * which means we now have two. Put our local token /Users/rubber/linux/block/blk-rq-qos.c: 280
			 * and wake anyone else potentially waiting for one. /Users/rubber/linux/block/blk-rq-qos.c: 281
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-integrity.c: 1
 * blk-integrity.c - Block layer data integrity extensions /Users/rubber/linux/block/blk-integrity.c: 3
 * Copyright (C) 2007, 2008 Oracle Corporation /Users/rubber/linux/block/blk-integrity.c: 5
 * Written by: Martin K. Petersen <martin.petersen@oracle.com> /Users/rubber/linux/block/blk-integrity.c: 6
 * blk_rq_count_integrity_sg - Count number of integrity scatterlist elements /Users/rubber/linux/block/blk-integrity.c: 20
 * @q:		request queue /Users/rubber/linux/block/blk-integrity.c: 21
 * @bio:	bio with integrity metadata attached /Users/rubber/linux/block/blk-integrity.c: 22
 * Description: Returns the number of elements required in a /Users/rubber/linux/block/blk-integrity.c: 24
 * scatterlist corresponding to the integrity metadata in a bio. /Users/rubber/linux/block/blk-integrity.c: 25
 * blk_rq_map_integrity_sg - Map integrity metadata into a scatterlist /Users/rubber/linux/block/blk-integrity.c: 59
 * @q:		request queue /Users/rubber/linux/block/blk-integrity.c: 60
 * @bio:	bio with integrity metadata attached /Users/rubber/linux/block/blk-integrity.c: 61
 * @sglist:	target scatterlist /Users/rubber/linux/block/blk-integrity.c: 62
 * Description: Map the integrity vectors in request into a /Users/rubber/linux/block/blk-integrity.c: 64
 * scatterlist.  The scatterlist must be big enough to hold all /Users/rubber/linux/block/blk-integrity.c: 65
 * elements.  I.e. sized using blk_rq_count_integrity_sg(). /Users/rubber/linux/block/blk-integrity.c: 66
 * blk_integrity_compare - Compare integrity profile of two disks /Users/rubber/linux/block/blk-integrity.c: 111
 * @gd1:	Disk to compare /Users/rubber/linux/block/blk-integrity.c: 112
 * @gd2:	Disk to compare /Users/rubber/linux/block/blk-integrity.c: 113
 * Description: Meta-devices like DM and MD need to verify that all /Users/rubber/linux/block/blk-integrity.c: 115
 * sub-devices use the same integrity format before advertising to /Users/rubber/linux/block/blk-integrity.c: 116
 * upper layers that they can send/receive integrity metadata.  This /Users/rubber/linux/block/blk-integrity.c: 117
 * function can be used to check whether two gendisk devices have /Users/rubber/linux/block/blk-integrity.c: 118
 * compatible integrity formats. /Users/rubber/linux/block/blk-integrity.c: 119
 * blk_integrity_register - Register a gendisk as being integrity-capable /Users/rubber/linux/block/blk-integrity.c: 387
 * @disk:	struct gendisk pointer to make integrity-aware /Users/rubber/linux/block/blk-integrity.c: 388
 * @template:	block integrity profile to register /Users/rubber/linux/block/blk-integrity.c: 389
 * Description: When a device needs to advertise itself as being able to /Users/rubber/linux/block/blk-integrity.c: 391
 * send/receive integrity metadata it must use this function to register /Users/rubber/linux/block/blk-integrity.c: 392
 * the capability with the block layer. The template is a blk_integrity /Users/rubber/linux/block/blk-integrity.c: 393
 * struct with values appropriate for the underlying hardware. See /Users/rubber/linux/block/blk-integrity.c: 394
 * Documentation/block/data-integrity.rst. /Users/rubber/linux/block/blk-integrity.c: 395
 * blk_integrity_unregister - Unregister block integrity profile /Users/rubber/linux/block/blk-integrity.c: 421
 * @disk:	disk whose integrity profile to unregister /Users/rubber/linux/block/blk-integrity.c: 422
 * Description: This function unregisters the integrity capability from /Users/rubber/linux/block/blk-integrity.c: 424
 * a block device. /Users/rubber/linux/block/blk-integrity.c: 425
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-tag.c: 1
 * Tag allocation using scalable bitmaps. Uses active queue tracking to support /Users/rubber/linux/block/blk-mq-tag.c: 3
 * fairer distribution of tags between multiple submitters when a shared tag map /Users/rubber/linux/block/blk-mq-tag.c: 4
 * is used. /Users/rubber/linux/block/blk-mq-tag.c: 5
 * Copyright (C) 2013-2014 Jens Axboe /Users/rubber/linux/block/blk-mq-tag.c: 7
 * If a previously inactive queue goes active, bump the active user count. /Users/rubber/linux/block/blk-mq-tag.c: 20
 * We need to do this before try to allocate driver tag, then even if fail /Users/rubber/linux/block/blk-mq-tag.c: 21
 * to get tag when first time, the other shared-tag users could reserve /Users/rubber/linux/block/blk-mq-tag.c: 22
 * budget for it. /Users/rubber/linux/block/blk-mq-tag.c: 23
 * Wakeup all potentially sleeping on tags /Users/rubber/linux/block/blk-mq-tag.c: 43
 * If a previously busy queue goes inactive, potential waiters could now /Users/rubber/linux/block/blk-mq-tag.c: 53
 * be allowed to queue. Wake them up and check. /Users/rubber/linux/block/blk-mq-tag.c: 54
		/* /Users/rubber/linux/block/blk-mq-tag.c: 136
		 * We're out of tags on this hardware queue, kick any /Users/rubber/linux/block/blk-mq-tag.c: 137
		 * pending IO submits before going to sleep waiting for /Users/rubber/linux/block/blk-mq-tag.c: 138
		 * some to complete. /Users/rubber/linux/block/blk-mq-tag.c: 139
		/* /Users/rubber/linux/block/blk-mq-tag.c: 143
		 * Retry tag allocation after running the hardware queue, /Users/rubber/linux/block/blk-mq-tag.c: 144
		 * as running the queue may also have found completions. /Users/rubber/linux/block/blk-mq-tag.c: 145
		/* /Users/rubber/linux/block/blk-mq-tag.c: 171
		 * If destination hw queue is changed, fake wake up on /Users/rubber/linux/block/blk-mq-tag.c: 172
		 * previous queue for compensating the wake up miss, so /Users/rubber/linux/block/blk-mq-tag.c: 173
		 * other allocations on previous queue won't be starved. /Users/rubber/linux/block/blk-mq-tag.c: 174
	/* /Users/rubber/linux/block/blk-mq-tag.c: 185
	 * Give up this allocation if the hctx is inactive.  The caller will /Users/rubber/linux/block/blk-mq-tag.c: 186
	 * retry on an active hctx. /Users/rubber/linux/block/blk-mq-tag.c: 187
	/* /Users/rubber/linux/block/blk-mq-tag.c: 248
	 * We can hit rq == NULL here, because the tagging functions /Users/rubber/linux/block/blk-mq-tag.c: 249
	 * test and set the bit before assigning ->rqs[]. /Users/rubber/linux/block/blk-mq-tag.c: 250
 * bt_for_each - iterate over the requests associated with a hardware queue /Users/rubber/linux/block/blk-mq-tag.c: 263
 * @hctx:	Hardware queue to examine. /Users/rubber/linux/block/blk-mq-tag.c: 264
 * @bt:		sbitmap to examine. This is either the breserved_tags member /Users/rubber/linux/block/blk-mq-tag.c: 265
 *		or the bitmap_tags member of struct blk_mq_tags. /Users/rubber/linux/block/blk-mq-tag.c: 266
 * @fn:		Pointer to the function that will be called for each request /Users/rubber/linux/block/blk-mq-tag.c: 267
 *		associated with @hctx that has been assigned a driver tag. /Users/rubber/linux/block/blk-mq-tag.c: 268
 *		@fn will be called as follows: @fn(@hctx, rq, @data, @reserved) /Users/rubber/linux/block/blk-mq-tag.c: 269
 *		where rq is a pointer to a request. Return true to continue /Users/rubber/linux/block/blk-mq-tag.c: 270
 *		iterating tags, false to stop. /Users/rubber/linux/block/blk-mq-tag.c: 271
 * @data:	Will be passed as third argument to @fn. /Users/rubber/linux/block/blk-mq-tag.c: 272
 * @reserved:	Indicates whether @bt is the breserved_tags member or the /Users/rubber/linux/block/blk-mq-tag.c: 273
 *		bitmap_tags member of struct blk_mq_tags. /Users/rubber/linux/block/blk-mq-tag.c: 274
	/* /Users/rubber/linux/block/blk-mq-tag.c: 312
	 * We can hit rq == NULL here, because the tagging functions /Users/rubber/linux/block/blk-mq-tag.c: 313
	 * test and set the bit before assigning ->rqs[]. /Users/rubber/linux/block/blk-mq-tag.c: 314
 * bt_tags_for_each - iterate over the requests in a tag map /Users/rubber/linux/block/blk-mq-tag.c: 332
 * @tags:	Tag map to iterate over. /Users/rubber/linux/block/blk-mq-tag.c: 333
 * @bt:		sbitmap to examine. This is either the breserved_tags member /Users/rubber/linux/block/blk-mq-tag.c: 334
 *		or the bitmap_tags member of struct blk_mq_tags. /Users/rubber/linux/block/blk-mq-tag.c: 335
 * @fn:		Pointer to the function that will be called for each started /Users/rubber/linux/block/blk-mq-tag.c: 336
 *		request. @fn will be called as follows: @fn(rq, @data, /Users/rubber/linux/block/blk-mq-tag.c: 337
 *		@reserved) where rq is a pointer to a request. Return true /Users/rubber/linux/block/blk-mq-tag.c: 338
 *		to continue iterating tags, false to stop. /Users/rubber/linux/block/blk-mq-tag.c: 339
 * @data:	Will be passed as second argument to @fn. /Users/rubber/linux/block/blk-mq-tag.c: 340
 * @flags:	BT_TAG_ITER_* /Users/rubber/linux/block/blk-mq-tag.c: 341
 * blk_mq_all_tag_iter - iterate over all requests in a tag map /Users/rubber/linux/block/blk-mq-tag.c: 369
 * @tags:	Tag map to iterate over. /Users/rubber/linux/block/blk-mq-tag.c: 370
 * @fn:		Pointer to the function that will be called for each /Users/rubber/linux/block/blk-mq-tag.c: 371
 *		request. @fn will be called as follows: @fn(rq, @priv, /Users/rubber/linux/block/blk-mq-tag.c: 372
 *		reserved) where rq is a pointer to a request. 'reserved' /Users/rubber/linux/block/blk-mq-tag.c: 373
 *		indicates whether or not @rq is a reserved request. Return /Users/rubber/linux/block/blk-mq-tag.c: 374
 *		true to continue iterating tags, false to stop. /Users/rubber/linux/block/blk-mq-tag.c: 375
 * @priv:	Will be passed as second argument to @fn. /Users/rubber/linux/block/blk-mq-tag.c: 376
 * Caller has to pass the tag map from which requests are allocated. /Users/rubber/linux/block/blk-mq-tag.c: 378
 * blk_mq_tagset_busy_iter - iterate over all started requests in a tag set /Users/rubber/linux/block/blk-mq-tag.c: 387
 * @tagset:	Tag set to iterate over. /Users/rubber/linux/block/blk-mq-tag.c: 388
 * @fn:		Pointer to the function that will be called for each started /Users/rubber/linux/block/blk-mq-tag.c: 389
 *		request. @fn will be called as follows: @fn(rq, @priv, /Users/rubber/linux/block/blk-mq-tag.c: 390
 *		reserved) where rq is a pointer to a request. 'reserved' /Users/rubber/linux/block/blk-mq-tag.c: 391
 *		indicates whether or not @rq is a reserved request. Return /Users/rubber/linux/block/blk-mq-tag.c: 392
 *		true to continue iterating tags, false to stop. /Users/rubber/linux/block/blk-mq-tag.c: 393
 * @priv:	Will be passed as second argument to @fn. /Users/rubber/linux/block/blk-mq-tag.c: 394
 * We grab one request reference before calling @fn and release it after /Users/rubber/linux/block/blk-mq-tag.c: 396
 * @fn returns. /Users/rubber/linux/block/blk-mq-tag.c: 397
 * blk_mq_tagset_wait_completed_request - Wait until all scheduled request /Users/rubber/linux/block/blk-mq-tag.c: 426
 * completions have finished. /Users/rubber/linux/block/blk-mq-tag.c: 427
 * @tagset:	Tag set to drain completed request /Users/rubber/linux/block/blk-mq-tag.c: 428
 * Note: This function has to be run after all IO queues are shutdown /Users/rubber/linux/block/blk-mq-tag.c: 430
 * blk_mq_queue_tag_busy_iter - iterate over all requests with a driver tag /Users/rubber/linux/block/blk-mq-tag.c: 447
 * @q:		Request queue to examine. /Users/rubber/linux/block/blk-mq-tag.c: 448
 * @fn:		Pointer to the function that will be called for each request /Users/rubber/linux/block/blk-mq-tag.c: 449
 *		on @q. @fn will be called as follows: @fn(hctx, rq, @priv, /Users/rubber/linux/block/blk-mq-tag.c: 450
 *		reserved) where rq is a pointer to a request and hctx points /Users/rubber/linux/block/blk-mq-tag.c: 451
 *		to the hardware queue associated with the request. 'reserved' /Users/rubber/linux/block/blk-mq-tag.c: 452
 *		indicates whether or not @rq is a reserved request. /Users/rubber/linux/block/blk-mq-tag.c: 453
 * @priv:	Will be passed as third argument to @fn. /Users/rubber/linux/block/blk-mq-tag.c: 454
 * Note: if @q->tag_set is shared with other request queues then @fn will be /Users/rubber/linux/block/blk-mq-tag.c: 456
 * called for all requests on all queues that share that tag set and not only /Users/rubber/linux/block/blk-mq-tag.c: 457
 * for requests associated with @q. /Users/rubber/linux/block/blk-mq-tag.c: 458
	/* /Users/rubber/linux/block/blk-mq-tag.c: 466
	 * __blk_mq_update_nr_hw_queues() updates nr_hw_queues and queue_hw_ctx /Users/rubber/linux/block/blk-mq-tag.c: 467
	 * while the queue is frozen. So we can use q_usage_counter to avoid /Users/rubber/linux/block/blk-mq-tag.c: 468
	 * racing with it. /Users/rubber/linux/block/blk-mq-tag.c: 469
		/* /Users/rubber/linux/block/blk-mq-tag.c: 477
		 * If no software queues are currently mapped to this /Users/rubber/linux/block/blk-mq-tag.c: 478
		 * hardware queue, there's nothing to check /Users/rubber/linux/block/blk-mq-tag.c: 479
	/* /Users/rubber/linux/block/blk-mq-tag.c: 562
	 * If we are allowed to grow beyond the original size, allocate /Users/rubber/linux/block/blk-mq-tag.c: 563
	 * a new set of tags before freeing the old one. /Users/rubber/linux/block/blk-mq-tag.c: 564
		/* /Users/rubber/linux/block/blk-mq-tag.c: 573
		 * We need some sort of upper limit, set it high enough that /Users/rubber/linux/block/blk-mq-tag.c: 574
		 * no valid use cases should require more. /Users/rubber/linux/block/blk-mq-tag.c: 575
		/* /Users/rubber/linux/block/blk-mq-tag.c: 580
		 * Only the sbitmap needs resizing since we allocated the max /Users/rubber/linux/block/blk-mq-tag.c: 581
		 * initially. /Users/rubber/linux/block/blk-mq-tag.c: 582
		/* /Users/rubber/linux/block/blk-mq-tag.c: 594
		 * Don't need (or can't) update reserved tags here, they /Users/rubber/linux/block/blk-mq-tag.c: 595
		 * remain static and should never need resizing. /Users/rubber/linux/block/blk-mq-tag.c: 596
 * blk_mq_unique_tag() - return a tag that is unique queue-wide /Users/rubber/linux/block/blk-mq-tag.c: 619
 * @rq: request for which to compute a unique tag /Users/rubber/linux/block/blk-mq-tag.c: 620
 * The tag field in struct request is unique per hardware queue but not over /Users/rubber/linux/block/blk-mq-tag.c: 622
 * all hardware queues. Hence this function that returns a tag with the /Users/rubber/linux/block/blk-mq-tag.c: 623
 * hardware context index in the upper bits and the per hardware queue tag in /Users/rubber/linux/block/blk-mq-tag.c: 624
 * the lower bits. /Users/rubber/linux/block/blk-mq-tag.c: 625
 * Note: When called for a request that is queued on a non-multiqueue request /Users/rubber/linux/block/blk-mq-tag.c: 627
 * queue, the hardware context index is set to zero. /Users/rubber/linux/block/blk-mq-tag.c: 628
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-iolatency.c: 1
 * Block rq-qos base io controller /Users/rubber/linux/block/blk-iolatency.c: 3
 * This works similar to wbt with a few exceptions /Users/rubber/linux/block/blk-iolatency.c: 5
 * - It's bio based, so the latency covers the whole block layer in addition to /Users/rubber/linux/block/blk-iolatency.c: 7
 *   the actual io. /Users/rubber/linux/block/blk-iolatency.c: 8
 * - We will throttle all IO that comes in here if we need to. /Users/rubber/linux/block/blk-iolatency.c: 9
 * - We use the mean latency over the 100ms window.  This is because writes can /Users/rubber/linux/block/blk-iolatency.c: 10
 *   be particularly fast, which could give us a false sense of the impact of /Users/rubber/linux/block/blk-iolatency.c: 11
 *   other workloads on our protected workload. /Users/rubber/linux/block/blk-iolatency.c: 12
 * - By default there's no throttling, we set the queue_depth to UINT_MAX so /Users/rubber/linux/block/blk-iolatency.c: 13
 *   that we can have as many outstanding bio's as we're allowed to.  Only at /Users/rubber/linux/block/blk-iolatency.c: 14
 *   throttle time do we pay attention to the actual queue depth. /Users/rubber/linux/block/blk-iolatency.c: 15
 * The hierarchy works like the cpu controller does, we track the latency at /Users/rubber/linux/block/blk-iolatency.c: 17
 * every configured node, and each configured node has it's own independent /Users/rubber/linux/block/blk-iolatency.c: 18
 * queue depth.  This means that we only care about our latency targets at the /Users/rubber/linux/block/blk-iolatency.c: 19
 * peer level.  Some group at the bottom of the hierarchy isn't going to affect /Users/rubber/linux/block/blk-iolatency.c: 20
 * a group at the end of some other path if we're only configred at leaf level. /Users/rubber/linux/block/blk-iolatency.c: 21
 * Consider the following /Users/rubber/linux/block/blk-iolatency.c: 23
 *                   root blkg /Users/rubber/linux/block/blk-iolatency.c: 25
 *             /                     \ /Users/rubber/linux/block/blk-iolatency.c: 26
 *        fast (target=5ms)     slow (target=10ms) /Users/rubber/linux/block/blk-iolatency.c: 27
 *         /     \                  /        \ /Users/rubber/linux/block/blk-iolatency.c: 28
 *       a        b          normal(15ms)   unloved /Users/rubber/linux/block/blk-iolatency.c: 29
 * "a" and "b" have no target, but their combined io under "fast" cannot exceed /Users/rubber/linux/block/blk-iolatency.c: 31
 * an average latency of 5ms.  If it does then we will throttle the "slow" /Users/rubber/linux/block/blk-iolatency.c: 32
 * group.  In the case of "normal", if it exceeds its 15ms target, we will /Users/rubber/linux/block/blk-iolatency.c: 33
 * throttle "unloved", but nobody else. /Users/rubber/linux/block/blk-iolatency.c: 34
 * In this example "fast", "slow", and "normal" will be the only groups actually /Users/rubber/linux/block/blk-iolatency.c: 36
 * accounting their io latencies.  We have to walk up the heirarchy to the root /Users/rubber/linux/block/blk-iolatency.c: 37
 * on every submit and complete so we can do the appropriate stat recording and /Users/rubber/linux/block/blk-iolatency.c: 38
 * adjust the queue depth of ourselves if needed. /Users/rubber/linux/block/blk-iolatency.c: 39
 * There are 2 ways we throttle IO. /Users/rubber/linux/block/blk-iolatency.c: 41
 * 1) Queue depth throttling.  As we throttle down we will adjust the maximum /Users/rubber/linux/block/blk-iolatency.c: 43
 * number of IO's we're allowed to have in flight.  This starts at (u64)-1 down /Users/rubber/linux/block/blk-iolatency.c: 44
 * to 1.  If the group is only ever submitting IO for itself then this is the /Users/rubber/linux/block/blk-iolatency.c: 45
 * only way we throttle. /Users/rubber/linux/block/blk-iolatency.c: 46
 * 2) Induced delay throttling.  This is for the case that a group is generating /Users/rubber/linux/block/blk-iolatency.c: 48
 * IO that has to be issued by the root cg to avoid priority inversion. So think /Users/rubber/linux/block/blk-iolatency.c: 49
 * REQ_META or REQ_SWAP.  If we are already at qd == 1 and we're getting a lot /Users/rubber/linux/block/blk-iolatency.c: 50
 * of work done for us on behalf of the root cg and are being asked to scale /Users/rubber/linux/block/blk-iolatency.c: 51
 * down more then we induce a latency at userspace return.  We accumulate the /Users/rubber/linux/block/blk-iolatency.c: 52
 * total amount of time we need to be punished by doing /Users/rubber/linux/block/blk-iolatency.c: 53
 * total_time += min_lat_nsec - actual_io_completion /Users/rubber/linux/block/blk-iolatency.c: 55
 * and then at throttle time will do /Users/rubber/linux/block/blk-iolatency.c: 57
 * throttle_time = min(total_time, NSEC_PER_SEC) /Users/rubber/linux/block/blk-iolatency.c: 59
 * This induced delay will throttle back the activity that is generating the /Users/rubber/linux/block/blk-iolatency.c: 61
 * root cg issued io's, wethere that's some metadata intensive operation or the /Users/rubber/linux/block/blk-iolatency.c: 62
 * group is using so much memory that it is pushing us into swap. /Users/rubber/linux/block/blk-iolatency.c: 63
 * Copyright (C) 2018 Josef Bacik /Users/rubber/linux/block/blk-iolatency.c: 65
 * These are the constants used to fake the fixed-point moving average /Users/rubber/linux/block/blk-iolatency.c: 159
 * calculation just like load average.  The call to calc_load() folds /Users/rubber/linux/block/blk-iolatency.c: 160
 * (FIXED_1 (2048) - exp_factor) * new_sample into lat_avg.  The sampling /Users/rubber/linux/block/blk-iolatency.c: 161
 * window size is bucketed to try to approximately calculate average /Users/rubber/linux/block/blk-iolatency.c: 162
 * latency such that 1/exp (decay rate) is [1 min, 2.5 min) when windows /Users/rubber/linux/block/blk-iolatency.c: 163
 * elapse immediately.  Note, windows only elapse with IO activity.  Idle /Users/rubber/linux/block/blk-iolatency.c: 164
 * periods extend the most recent window. /Users/rubber/linux/block/blk-iolatency.c: 165
 exp(1/600) - 600 samples /Users/rubber/linux/block/blk-iolatency.c: 171
 exp(1/240) - 240 samples /Users/rubber/linux/block/blk-iolatency.c: 172
 exp(1/120) - 120 samples /Users/rubber/linux/block/blk-iolatency.c: 173
 exp(1/80)  - 80 samples /Users/rubber/linux/block/blk-iolatency.c: 174
 exp(1/60)  - 60 samples /Users/rubber/linux/block/blk-iolatency.c: 175
	/* /Users/rubber/linux/block/blk-iolatency.c: 254
	 * calc_load() takes in a number stored in fixed point representation. /Users/rubber/linux/block/blk-iolatency.c: 255
	 * Because we are using this for IO time in ns, the values stored /Users/rubber/linux/block/blk-iolatency.c: 256
	 * are significantly larger than the FIXED_1 denominator (2048). /Users/rubber/linux/block/blk-iolatency.c: 257
	 * Therefore, rounding errors in the calculation are negligible and /Users/rubber/linux/block/blk-iolatency.c: 258
	 * can be ignored. /Users/rubber/linux/block/blk-iolatency.c: 259
	/* /Users/rubber/linux/block/blk-iolatency.c: 292
	 * To avoid priority inversions we want to just take a slot if we are /Users/rubber/linux/block/blk-iolatency.c: 293
	 * issuing as root.  If we're being killed off there's no point in /Users/rubber/linux/block/blk-iolatency.c: 294
	 * delaying things, we may have been killed by OOM so throttling may /Users/rubber/linux/block/blk-iolatency.c: 295
	 * make recovery take even longer, so just let the IO's through so the /Users/rubber/linux/block/blk-iolatency.c: 296
	 * task can go away. /Users/rubber/linux/block/blk-iolatency.c: 297
 * We scale the qd down faster than we scale up, so we need to use this helper /Users/rubber/linux/block/blk-iolatency.c: 316
 * to adjust the scale_cookie accordingly so we don't prematurely get /Users/rubber/linux/block/blk-iolatency.c: 317
 * scale_cookie at DEFAULT_SCALE_COOKIE and unthrottle too much. /Users/rubber/linux/block/blk-iolatency.c: 318
 * Each group has their own local copy of the last scale cookie they saw, so if /Users/rubber/linux/block/blk-iolatency.c: 320
 * the global scale cookie goes up or down they know which way they need to go /Users/rubber/linux/block/blk-iolatency.c: 321
 * based on their last knowledge of it. /Users/rubber/linux/block/blk-iolatency.c: 322
		/* /Users/rubber/linux/block/blk-iolatency.c: 346
		 * We don't want to dig a hole so deep that it takes us hours to /Users/rubber/linux/block/blk-iolatency.c: 347
		 * dig out of it.  Just enough that we don't throttle/unthrottle /Users/rubber/linux/block/blk-iolatency.c: 348
		 * with jagged workloads but can still unthrottle once pressure /Users/rubber/linux/block/blk-iolatency.c: 349
		 * has sufficiently dissipated. /Users/rubber/linux/block/blk-iolatency.c: 350
 * Change the queue depth of the iolatency_grp.  We add/subtract 1/16th of the /Users/rubber/linux/block/blk-iolatency.c: 362
 * queue depth at a time so we don't get wild swings and hopefully dial in to /Users/rubber/linux/block/blk-iolatency.c: 363
 * fairer distribution of the overall queue depth. /Users/rubber/linux/block/blk-iolatency.c: 364
		/* /Users/rubber/linux/block/blk-iolatency.c: 432
		 * Sometimes high priority groups are their own worst enemy, so /Users/rubber/linux/block/blk-iolatency.c: 433
		 * instead of taking it out on some poor other group that did 5% /Users/rubber/linux/block/blk-iolatency.c: 434
		 * or less of the IO's for the last summation just skip this /Users/rubber/linux/block/blk-iolatency.c: 435
		 * scale down event. /Users/rubber/linux/block/blk-iolatency.c: 436
	/* /Users/rubber/linux/block/blk-iolatency.c: 493
	 * Have to do this so we are truncated to the correct time that our /Users/rubber/linux/block/blk-iolatency.c: 494
	 * issue is truncated to. /Users/rubber/linux/block/blk-iolatency.c: 495
	/* /Users/rubber/linux/block/blk-iolatency.c: 504
	 * We don't want to count issue_as_root bio's in the cgroups latency /Users/rubber/linux/block/blk-iolatency.c: 505
	 * statistics as it could skew the numbers downwards. /Users/rubber/linux/block/blk-iolatency.c: 506
		/* /Users/rubber/linux/block/blk-iolatency.c: 623
		 * If bi_status is BLK_STS_AGAIN, the bio wasn't actually /Users/rubber/linux/block/blk-iolatency.c: 624
		 * submitted, so do not account for it. /Users/rubber/linux/block/blk-iolatency.c: 625
		/* /Users/rubber/linux/block/blk-iolatency.c: 673
		 * We could be exiting, don't access the pd unless we have a /Users/rubber/linux/block/blk-iolatency.c: 674
		 * ref on the blkg. /Users/rubber/linux/block/blk-iolatency.c: 675
		/* /Users/rubber/linux/block/blk-iolatency.c: 694
		 * We scaled down but don't have a scale_grp, scale up and carry /Users/rubber/linux/block/blk-iolatency.c: 695
		 * on. /Users/rubber/linux/block/blk-iolatency.c: 696
		/* /Users/rubber/linux/block/blk-iolatency.c: 703
		 * It's been 5 seconds since our last scale event, clear the /Users/rubber/linux/block/blk-iolatency.c: 704
		 * scale grp in case the group that needed the scale down isn't /Users/rubber/linux/block/blk-iolatency.c: 705
		 * doing any IO currently. /Users/rubber/linux/block/blk-iolatency.c: 706
 * return 1 for enabling iolatency, return -1 for disabling iolatency, otherwise /Users/rubber/linux/block/blk-iolatency.c: 749
 * return 0. /Users/rubber/linux/block/blk-iolatency.c: 750
	/* /Users/rubber/linux/block/blk-iolatency.c: 991
	 * We init things in list order, so the pd for the parent may not be /Users/rubber/linux/block/blk-iolatency.c: 992
	 * init'ed yet for whatever reason. /Users/rubber/linux/block/blk-iolatency.c: 993
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-throttle.c: 1
 * Interface for controlling IO bandwidth on a request queue /Users/rubber/linux/block/blk-throttle.c: 3
 * Copyright (C) 2010 Vivek Goyal <vgoyal@redhat.com> /Users/rubber/linux/block/blk-throttle.c: 5
 * For HD, very small latency comes from sequential IO. Such IO is helpless to /Users/rubber/linux/block/blk-throttle.c: 36
 * help determine if its IO is impacted by others, hence we ignore the IO /Users/rubber/linux/block/blk-throttle.c: 37
 * sq_to_tg - return the throl_grp the specified service queue belongs to /Users/rubber/linux/block/blk-throttle.c: 103
 * @sq: the throtl_service_queue of interest /Users/rubber/linux/block/blk-throttle.c: 104
 * Return the throtl_grp @sq belongs to.  If @sq is the top-level one /Users/rubber/linux/block/blk-throttle.c: 106
 * embedded in throtl_data, %NULL is returned. /Users/rubber/linux/block/blk-throttle.c: 107
 * sq_to_td - return throtl_data the specified service queue belongs to /Users/rubber/linux/block/blk-throttle.c: 118
 * @sq: the throtl_service_queue of interest /Users/rubber/linux/block/blk-throttle.c: 119
 * A service_queue can be embedded in either a throtl_grp or throtl_data. /Users/rubber/linux/block/blk-throttle.c: 121
 * Determine the associated throtl_data accordingly and return it. /Users/rubber/linux/block/blk-throttle.c: 122
 * cgroup's limit in LIMIT_MAX is scaled if low limit is set. This scale is to /Users/rubber/linux/block/blk-throttle.c: 135
 * make the IO dispatch more smooth. /Users/rubber/linux/block/blk-throttle.c: 136
 * Scale up: linearly scale up according to lapsed time since upgrade. For /Users/rubber/linux/block/blk-throttle.c: 137
 *           every throtl_slice, the limit scales up 1/2 .low limit till the /Users/rubber/linux/block/blk-throttle.c: 138
 *           limit hits .max limit /Users/rubber/linux/block/blk-throttle.c: 139
 * Scale down: exponentially scale down if a cgroup doesn't hit its .low limit /Users/rubber/linux/block/blk-throttle.c: 140
 * throtl_log - log debug message via blktrace /Users/rubber/linux/block/blk-throttle.c: 218
 * @sq: the service_queue being reported /Users/rubber/linux/block/blk-throttle.c: 219
 * @fmt: printf format string /Users/rubber/linux/block/blk-throttle.c: 220
 * @args: printf args /Users/rubber/linux/block/blk-throttle.c: 221
 * The messages are prefixed with "throtl BLKG_NAME" if @sq belongs to a /Users/rubber/linux/block/blk-throttle.c: 223
 * throtl_grp; otherwise, just "throtl". /Users/rubber/linux/block/blk-throttle.c: 224
 * throtl_qnode_add_bio - add a bio to a throtl_qnode and activate it /Users/rubber/linux/block/blk-throttle.c: 257
 * @bio: bio being added /Users/rubber/linux/block/blk-throttle.c: 258
 * @qn: qnode to add bio to /Users/rubber/linux/block/blk-throttle.c: 259
 * @queued: the service_queue->queued[] list @qn belongs to /Users/rubber/linux/block/blk-throttle.c: 260
 * Add @bio to @qn and put @qn on @queued if it's not already on. /Users/rubber/linux/block/blk-throttle.c: 262
 * @qn->tg's reference count is bumped when @qn is activated.  See the /Users/rubber/linux/block/blk-throttle.c: 263
 * comment on top of throtl_qnode definition for details. /Users/rubber/linux/block/blk-throttle.c: 264
 * throtl_peek_queued - peek the first bio on a qnode list /Users/rubber/linux/block/blk-throttle.c: 277
 * @queued: the qnode list to peek /Users/rubber/linux/block/blk-throttle.c: 278
 * throtl_pop_queued - pop the first bio form a qnode list /Users/rubber/linux/block/blk-throttle.c: 295
 * @queued: the qnode list to pop a bio from /Users/rubber/linux/block/blk-throttle.c: 296
 * @tg_to_put: optional out argument for throtl_grp to put /Users/rubber/linux/block/blk-throttle.c: 297
 * Pop the first bio from the qnode list @queued.  After popping, the first /Users/rubber/linux/block/blk-throttle.c: 299
 * qnode is removed from @queued if empty or moved to the end of @queued so /Users/rubber/linux/block/blk-throttle.c: 300
 * that the popping order is round-robin. /Users/rubber/linux/block/blk-throttle.c: 301
 * When the first qnode is removed, its associated throtl_grp should be put /Users/rubber/linux/block/blk-throttle.c: 303
 * too.  If @tg_to_put is NULL, this function automatically puts it; /Users/rubber/linux/block/blk-throttle.c: 304
 * otherwise, *@tg_to_put is set to the throtl_grp to put and the caller is /Users/rubber/linux/block/blk-throttle.c: 305
 * responsible for putting it. /Users/rubber/linux/block/blk-throttle.c: 306
	/* /Users/rubber/linux/block/blk-throttle.c: 399
	 * If on the default hierarchy, we switch to properly hierarchical /Users/rubber/linux/block/blk-throttle.c: 400
	 * behavior where limits on a given throtl_grp are applied to the /Users/rubber/linux/block/blk-throttle.c: 401
	 * whole subtree rather than just the group itself.  e.g. If 16M /Users/rubber/linux/block/blk-throttle.c: 402
	 * read_bps limit is set on the root group, the whole system can't /Users/rubber/linux/block/blk-throttle.c: 403
	 * exceed 16M for the device. /Users/rubber/linux/block/blk-throttle.c: 404
	 * /Users/rubber/linux/block/blk-throttle.c: 405
	 * If not on the default hierarchy, the broken flat hierarchy /Users/rubber/linux/block/blk-throttle.c: 406
	 * behavior is retained where all throtl_grps are treated as if /Users/rubber/linux/block/blk-throttle.c: 407
	 * they're all separate root groups right below throtl_data. /Users/rubber/linux/block/blk-throttle.c: 408
	 * Limits of a group don't interact with limits of other groups /Users/rubber/linux/block/blk-throttle.c: 409
	 * regardless of the position of the group in the hierarchy. /Users/rubber/linux/block/blk-throttle.c: 410
 * Set has_rules[] if @tg or any of its parents have limits configured. /Users/rubber/linux/block/blk-throttle.c: 419
 * This doesn't require walking up to the top of the hierarchy as the /Users/rubber/linux/block/blk-throttle.c: 420
 * parent's has_rules[] is guaranteed to be correct. /Users/rubber/linux/block/blk-throttle.c: 421
	/* /Users/rubber/linux/block/blk-throttle.c: 439
	 * We don't want new groups to escape the limits of its ancestors. /Users/rubber/linux/block/blk-throttle.c: 440
	 * Update has_rules[] after a new group is brought online. /Users/rubber/linux/block/blk-throttle.c: 441
	/* /Users/rubber/linux/block/blk-throttle.c: 579
	 * Since we are adjusting the throttle limit dynamically, the sleep /Users/rubber/linux/block/blk-throttle.c: 580
	 * time calculated according to previous limit might be invalid. It's /Users/rubber/linux/block/blk-throttle.c: 581
	 * possible the cgroup sleep time is very long and no other cgroups /Users/rubber/linux/block/blk-throttle.c: 582
	 * have IO running so notify the limit changes. Make sure the cgroup /Users/rubber/linux/block/blk-throttle.c: 583
	 * doesn't sleep too long to avoid the missed notification. /Users/rubber/linux/block/blk-throttle.c: 584
 * throtl_schedule_next_dispatch - schedule the next dispatch cycle /Users/rubber/linux/block/blk-throttle.c: 594
 * @sq: the service_queue to schedule dispatch for /Users/rubber/linux/block/blk-throttle.c: 595
 * @force: force scheduling /Users/rubber/linux/block/blk-throttle.c: 596
 * Arm @sq->pending_timer so that the next dispatch cycle starts on the /Users/rubber/linux/block/blk-throttle.c: 598
 * dispatch time of the first pending child.  Returns %true if either timer /Users/rubber/linux/block/blk-throttle.c: 599
 * is armed or there's no pending child left.  %false if the current /Users/rubber/linux/block/blk-throttle.c: 600
 * dispatch window is still open and the caller should continue /Users/rubber/linux/block/blk-throttle.c: 601
 * dispatching. /Users/rubber/linux/block/blk-throttle.c: 602
 * If @force is %true, the dispatch timer is always scheduled and this /Users/rubber/linux/block/blk-throttle.c: 604
 * function is guaranteed to return %true.  This is to be used when the /Users/rubber/linux/block/blk-throttle.c: 605
 * caller can't dispatch itself and needs to invoke pending_timer /Users/rubber/linux/block/blk-throttle.c: 606
 * unconditionally.  Note that forced scheduling is likely to induce short /Users/rubber/linux/block/blk-throttle.c: 607
 * delay before dispatch starts even if @sq->first_pending_disptime is not /Users/rubber/linux/block/blk-throttle.c: 608
 * in the future and thus shouldn't be used in hot paths. /Users/rubber/linux/block/blk-throttle.c: 609
	/* /Users/rubber/linux/block/blk-throttle.c: 638
	 * Previous slice has expired. We must have trimmed it after last /Users/rubber/linux/block/blk-throttle.c: 639
	 * bio dispatch. That means since start of last slice, we never used /Users/rubber/linux/block/blk-throttle.c: 640
	 * that bandwidth. Do try to make use of that bandwidth while giving /Users/rubber/linux/block/blk-throttle.c: 641
	 * credit. /Users/rubber/linux/block/blk-throttle.c: 642
	/* /Users/rubber/linux/block/blk-throttle.c: 702
	 * If bps are unlimited (-1), then time slice don't get /Users/rubber/linux/block/blk-throttle.c: 703
	 * renewed. Don't try to trim the slice if slice is used. A new /Users/rubber/linux/block/blk-throttle.c: 704
	 * slice will start when appropriate. /Users/rubber/linux/block/blk-throttle.c: 705
	/* /Users/rubber/linux/block/blk-throttle.c: 710
	 * A bio has been dispatched. Also adjust slice_end. It might happen /Users/rubber/linux/block/blk-throttle.c: 711
	 * that initially cgroup limit was very low resulting in high /Users/rubber/linux/block/blk-throttle.c: 712
	 * slice_end, but later limit was bumped up and bio was dispatched /Users/rubber/linux/block/blk-throttle.c: 713
	 * sooner, then we need to reduce slice_end. A high bogus slice_end /Users/rubber/linux/block/blk-throttle.c: 714
	 * is bad because it does not allow new slice to start. /Users/rubber/linux/block/blk-throttle.c: 715
	/* /Users/rubber/linux/block/blk-throttle.c: 773
	 * jiffy_elapsed_rnd should not be a big value as minimum iops can be /Users/rubber/linux/block/blk-throttle.c: 774
	 * 1 then at max jiffy elapsed should be equivalent of 1 second as we /Users/rubber/linux/block/blk-throttle.c: 775
	 * will allow dispatch after 1 second and after that slice should /Users/rubber/linux/block/blk-throttle.c: 776
	 * have been trimmed. /Users/rubber/linux/block/blk-throttle.c: 777
	/* /Users/rubber/linux/block/blk-throttle.c: 841
	 * This wait time is without taking into consideration the rounding /Users/rubber/linux/block/blk-throttle.c: 842
	 * up we did. Add that time also. /Users/rubber/linux/block/blk-throttle.c: 843
 * Returns whether one can dispatch a bio or not. Also returns approx number /Users/rubber/linux/block/blk-throttle.c: 852
 * of jiffies to wait before this bio is with-in IO rate and can be dispatched /Users/rubber/linux/block/blk-throttle.c: 853
	/* /Users/rubber/linux/block/blk-throttle.c: 863
 	 * Currently whole state machine of group depends on first bio /Users/rubber/linux/block/blk-throttle.c: 864
	 * queued in the group bio list. So one should not be calling /Users/rubber/linux/block/blk-throttle.c: 865
	 * this function with a different bio if there are other bios /Users/rubber/linux/block/blk-throttle.c: 866
	 * queued. /Users/rubber/linux/block/blk-throttle.c: 867
	/* /Users/rubber/linux/block/blk-throttle.c: 879
	 * If previous slice expired, start a new one otherwise renew/extend /Users/rubber/linux/block/blk-throttle.c: 880
	 * existing slice to make sure it is at least throtl_slice interval /Users/rubber/linux/block/blk-throttle.c: 881
	 * long since now. New slice is started only for empty throttle group. /Users/rubber/linux/block/blk-throttle.c: 882
	 * If there is queued bio, that means there should be an active /Users/rubber/linux/block/blk-throttle.c: 883
	 * slice and it should be extended instead. /Users/rubber/linux/block/blk-throttle.c: 884
	/* /Users/rubber/linux/block/blk-throttle.c: 927
	 * BIO_THROTTLED is used to prevent the same bio to be throttled /Users/rubber/linux/block/blk-throttle.c: 928
	 * more than once as a throttled bio will go through blk-throtl the /Users/rubber/linux/block/blk-throttle.c: 929
	 * second time when it eventually gets issued.  Set it when a bio /Users/rubber/linux/block/blk-throttle.c: 930
	 * is being charged to a tg. /Users/rubber/linux/block/blk-throttle.c: 931
 * throtl_add_bio_tg - add a bio to the specified throtl_grp /Users/rubber/linux/block/blk-throttle.c: 938
 * @bio: bio to add /Users/rubber/linux/block/blk-throttle.c: 939
 * @qn: qnode to use /Users/rubber/linux/block/blk-throttle.c: 940
 * @tg: the target throtl_grp /Users/rubber/linux/block/blk-throttle.c: 941
 * Add @bio to @tg's service_queue using @qn.  If @qn is not specified, /Users/rubber/linux/block/blk-throttle.c: 943
 * tg->qnode_on_self[] is used. /Users/rubber/linux/block/blk-throttle.c: 944
	/* /Users/rubber/linux/block/blk-throttle.c: 955
	 * If @tg doesn't currently have any bios queued in the same /Users/rubber/linux/block/blk-throttle.c: 956
	 * direction, queueing @bio can change when @tg should be /Users/rubber/linux/block/blk-throttle.c: 957
	 * dispatched.  Mark that @tg was empty.  This is automatically /Users/rubber/linux/block/blk-throttle.c: 958
	 * cleared on the next tg_update_disptime(). /Users/rubber/linux/block/blk-throttle.c: 959
	/* /Users/rubber/linux/block/blk-throttle.c: 1014
	 * @bio is being transferred from @tg to @parent_sq.  Popping a bio /Users/rubber/linux/block/blk-throttle.c: 1015
	 * from @tg may put its reference and @parent_sq might end up /Users/rubber/linux/block/blk-throttle.c: 1016
	 * getting released prematurely.  Remember the tg to put and put it /Users/rubber/linux/block/blk-throttle.c: 1017
	 * after @bio is transferred to @parent_sq. /Users/rubber/linux/block/blk-throttle.c: 1018
	/* /Users/rubber/linux/block/blk-throttle.c: 1025
	 * If our parent is another tg, we just need to transfer @bio to /Users/rubber/linux/block/blk-throttle.c: 1026
	 * the parent using throtl_add_bio_tg().  If our parent is /Users/rubber/linux/block/blk-throttle.c: 1027
	 * @td->service_queue, @bio is ready to be issued.  Put it on its /Users/rubber/linux/block/blk-throttle.c: 1028
	 * bio_lists[] and decrease total number queued.  The caller is /Users/rubber/linux/block/blk-throttle.c: 1029
	 * responsible for issuing these bios. /Users/rubber/linux/block/blk-throttle.c: 1030
 * throtl_pending_timer_fn - timer function for service_queue->pending_timer /Users/rubber/linux/block/blk-throttle.c: 1117
 * @t: the pending_timer member of the throtl_service_queue being serviced /Users/rubber/linux/block/blk-throttle.c: 1118
 * This timer is armed when a child throtl_grp with active bio's become /Users/rubber/linux/block/blk-throttle.c: 1120
 * pending and queued on the service_queue's pending_tree and expires when /Users/rubber/linux/block/blk-throttle.c: 1121
 * the first child throtl_grp should be dispatched.  This function /Users/rubber/linux/block/blk-throttle.c: 1122
 * dispatches bio's from the children throtl_grps to the parent /Users/rubber/linux/block/blk-throttle.c: 1123
 * service_queue. /Users/rubber/linux/block/blk-throttle.c: 1124
 * If the parent's parent is another throtl_grp, dispatching is propagated /Users/rubber/linux/block/blk-throttle.c: 1126
 * by either arming its pending_timer or repeating dispatch directly.  If /Users/rubber/linux/block/blk-throttle.c: 1127
 * the top-level service_tree is reached, throtl_data->dispatch_work is /Users/rubber/linux/block/blk-throttle.c: 1128
 * kicked so that the ready bio's are issued. /Users/rubber/linux/block/blk-throttle.c: 1129
 * blk_throtl_dispatch_work_fn - work function for throtl_data->dispatch_work /Users/rubber/linux/block/blk-throttle.c: 1192
 * @work: work item being executed /Users/rubber/linux/block/blk-throttle.c: 1193
 * This function is queued for execution when bios reach the bio_lists[] /Users/rubber/linux/block/blk-throttle.c: 1195
 * of throtl_data->service_queue.  Those bios are ready and issued by this /Users/rubber/linux/block/blk-throttle.c: 1196
 * function. /Users/rubber/linux/block/blk-throttle.c: 1197
	/* /Users/rubber/linux/block/blk-throttle.c: 1273
	 * Update has_rules[] flags for the updated tg's subtree.  A tg is /Users/rubber/linux/block/blk-throttle.c: 1274
	 * considered to have rules if either the tg itself or any of its /Users/rubber/linux/block/blk-throttle.c: 1275
	 * ancestors has rules.  This identifies groups without any /Users/rubber/linux/block/blk-throttle.c: 1276
	 * restrictions in the whole hierarchy and allows them to bypass /Users/rubber/linux/block/blk-throttle.c: 1277
	 * blk-throttle. /Users/rubber/linux/block/blk-throttle.c: 1278
		/* /Users/rubber/linux/block/blk-throttle.c: 1291
		 * make sure all children has lower idle time threshold and /Users/rubber/linux/block/blk-throttle.c: 1292
		 * higher latency target /Users/rubber/linux/block/blk-throttle.c: 1293
	/* /Users/rubber/linux/block/blk-throttle.c: 1301
	 * We're already holding queue_lock and know @tg is valid.  Let's /Users/rubber/linux/block/blk-throttle.c: 1302
	 * apply the new config directly. /Users/rubber/linux/block/blk-throttle.c: 1303
	 * /Users/rubber/linux/block/blk-throttle.c: 1304
	 * Restart the slices for both READ and WRITES. It might happen /Users/rubber/linux/block/blk-throttle.c: 1305
	 * that a group's limit are dropped suddenly and we don't want to /Users/rubber/linux/block/blk-throttle.c: 1306
	 * account recently dispatched IO with new low rate. /Users/rubber/linux/block/blk-throttle.c: 1307
		/* /Users/rubber/linux/block/blk-throttle.c: 1685
		 * The parent doesn't have low limit, it always reaches low /Users/rubber/linux/block/blk-throttle.c: 1686
		 * limit. Its overflow time is useless for children /Users/rubber/linux/block/blk-throttle.c: 1687
	/* /Users/rubber/linux/block/blk-throttle.c: 1702
	 * cgroup is idle if: /Users/rubber/linux/block/blk-throttle.c: 1703
	 * - single idle is too long, longer than a fixed value (in case user /Users/rubber/linux/block/blk-throttle.c: 1704
	 *   configure a too big threshold) or 4 times of idletime threshold /Users/rubber/linux/block/blk-throttle.c: 1705
	 * - average think time is more than threshold /Users/rubber/linux/block/blk-throttle.c: 1706
	 * - IO latency is largely below threshold /Users/rubber/linux/block/blk-throttle.c: 1707
	/* /Users/rubber/linux/block/blk-throttle.c: 1731
	 * if cgroup reaches low limit (if low limit is 0, the cgroup always /Users/rubber/linux/block/blk-throttle.c: 1732
	 * reaches), it's ok to upgrade to next limit /Users/rubber/linux/block/blk-throttle.c: 1733
	/* /Users/rubber/linux/block/blk-throttle.c: 1857
	 * If cgroup is below low limit, consider downgrade and throttle other /Users/rubber/linux/block/blk-throttle.c: 1858
	 * cgroups /Users/rubber/linux/block/blk-throttle.c: 1859
	/* /Users/rubber/linux/block/blk-throttle.c: 1932
	 * If cgroup is below low limit, consider downgrade and throttle other /Users/rubber/linux/block/blk-throttle.c: 1933
	 * cgroups /Users/rubber/linux/block/blk-throttle.c: 1934
		/* /Users/rubber/linux/block/blk-throttle.c: 2114
		 * We need to trim slice even when bios are not being queued /Users/rubber/linux/block/blk-throttle.c: 2115
		 * otherwise it might happen that a bio is not queued for /Users/rubber/linux/block/blk-throttle.c: 2116
		 * a long time and slice keeps on extending and trim is not /Users/rubber/linux/block/blk-throttle.c: 2117
		 * called for a long time. Now if limits are reduced suddenly /Users/rubber/linux/block/blk-throttle.c: 2118
		 * we take into account all the IO dispatched so far at new /Users/rubber/linux/block/blk-throttle.c: 2119
		 * low rate and * newly queued IO gets a really long dispatch /Users/rubber/linux/block/blk-throttle.c: 2120
		 * time. /Users/rubber/linux/block/blk-throttle.c: 2121
		 * /Users/rubber/linux/block/blk-throttle.c: 2122
		 * So keep on trimming slice even if bio is not queued. /Users/rubber/linux/block/blk-throttle.c: 2123
		/* /Users/rubber/linux/block/blk-throttle.c: 2127
		 * @bio passed through this layer without being throttled. /Users/rubber/linux/block/blk-throttle.c: 2128
		 * Climb up the ladder.  If we're already at the top, it /Users/rubber/linux/block/blk-throttle.c: 2129
		 * can be executed directly. /Users/rubber/linux/block/blk-throttle.c: 2130
	/* /Users/rubber/linux/block/blk-throttle.c: 2153
	 * Update @tg's dispatch time and force schedule dispatch if @tg /Users/rubber/linux/block/blk-throttle.c: 2154
	 * was empty before @bio.  The forced scheduling isn't likely to /Users/rubber/linux/block/blk-throttle.c: 2155
	 * cause undue delay as @bio is likely to be dispatched directly if /Users/rubber/linux/block/blk-throttle.c: 2156
	 * its @tg's disptime is not in the future. /Users/rubber/linux/block/blk-throttle.c: 2157
		/* /Users/rubber/linux/block/blk-throttle.c: 2245
		 * Not race free, could get wrong count, which means cgroups /Users/rubber/linux/block/blk-throttle.c: 2246
		 * will be throttled /Users/rubber/linux/block/blk-throttle.c: 2247
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-pci.c: 1
 * Copyright (c) 2016 Christoph Hellwig. /Users/rubber/linux/block/blk-mq-pci.c: 3
 * blk_mq_pci_map_queues - provide a default queue mapping for PCI device /Users/rubber/linux/block/blk-mq-pci.c: 15
 * @qmap:	CPU to hardware queue map. /Users/rubber/linux/block/blk-mq-pci.c: 16
 * @pdev:	PCI device associated with @set. /Users/rubber/linux/block/blk-mq-pci.c: 17
 * @offset:	Offset to use for the pci irq vector /Users/rubber/linux/block/blk-mq-pci.c: 18
 * This function assumes the PCI device @pdev has at least as many available /Users/rubber/linux/block/blk-mq-pci.c: 20
 * interrupt vectors as @set has queues.  It will then query the vector /Users/rubber/linux/block/blk-mq-pci.c: 21
 * corresponding to each queue for it's affinity mask and built queue mapping /Users/rubber/linux/block/blk-mq-pci.c: 22
 * that maps a queue to the CPUs that have irq affinity for the corresponding /Users/rubber/linux/block/blk-mq-pci.c: 23
 * vector. /Users/rubber/linux/block/blk-mq-pci.c: 24
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/ioctl.c: 1
	/* /Users/rubber/linux/block/ioctl.c: 96
	 * Reopen the device to revalidate the driver state and force a /Users/rubber/linux/block/ioctl.c: 97
	 * partition rescan. /Users/rubber/linux/block/ioctl.c: 98
 * This is the equivalent of compat_ptr_ioctl(), to be used by block /Users/rubber/linux/block/ioctl.c: 237
 * drivers that implement only commands that are completely compatible /Users/rubber/linux/block/ioctl.c: 238
 * between 32-bit and 64-bit user space /Users/rubber/linux/block/ioctl.c: 239
	/* /Users/rubber/linux/block/ioctl.c: 386
	 * We need to set the startsect first, the driver may /Users/rubber/linux/block/ioctl.c: 387
	 * want to override it. /Users/rubber/linux/block/ioctl.c: 388
	/* /Users/rubber/linux/block/ioctl.c: 421
	 * We need to set the startsect first, the driver may /Users/rubber/linux/block/ioctl.c: 422
	 * want to override it. /Users/rubber/linux/block/ioctl.c: 423
 * Common commands that are handled the same way on native and compat /Users/rubber/linux/block/ioctl.c: 464
 * user space. Note the separate arg/argp parameters that are needed /Users/rubber/linux/block/ioctl.c: 465
 * to deal with the compat_ptr() conversion. /Users/rubber/linux/block/ioctl.c: 466
 * Always keep this in sync with compat_blkdev_ioctl() /Users/rubber/linux/block/ioctl.c: 548
 * to handle all incompatible commands in both functions. /Users/rubber/linux/block/ioctl.c: 549
 * New commands must be compatible and go into blkdev_common_ioctl /Users/rubber/linux/block/ioctl.c: 551
	/* /Users/rubber/linux/block/ioctl.c: 560
	 * O_NDELAY can be altered using fcntl(.., F_SETFL, ..), so we have /Users/rubber/linux/block/ioctl.c: 561
	 * to updated it before every ioctl. /Users/rubber/linux/block/ioctl.c: 562
/* Most of the generic ioctls are handled in the normal fallback path. /Users/rubber/linux/block/ioctl.c: 618
   This assumes the blkdev's low level compat_ioctl always returns /Users/rubber/linux/block/ioctl.c: 619
	/* /Users/rubber/linux/block/ioctl.c: 629
	 * O_NDELAY can be altered using fcntl(.., F_SETFL, ..), so we have /Users/rubber/linux/block/ioctl.c: 630
	 * to updated it before every ioctl. /Users/rubber/linux/block/ioctl.c: 631
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-ioc.c: 1
 * Functions related to io context handling /Users/rubber/linux/block/blk-ioc.c: 3
 * For io context allocations /Users/rubber/linux/block/blk-ioc.c: 16
 * get_io_context - increment reference count to io_context /Users/rubber/linux/block/blk-ioc.c: 21
 * @ioc: io_context to get /Users/rubber/linux/block/blk-ioc.c: 22
 * Increment reference count to @ioc. /Users/rubber/linux/block/blk-ioc.c: 24
 * Exit an icq. Called with ioc locked for blk-mq, and with both ioc /Users/rubber/linux/block/blk-ioc.c: 40
 * and queue locked for legacy. /Users/rubber/linux/block/blk-ioc.c: 41
 * Release an icq. Called with ioc locked for blk-mq, and with both ioc /Users/rubber/linux/block/blk-ioc.c: 57
 * and queue locked for legacy. /Users/rubber/linux/block/blk-ioc.c: 58
	/* /Users/rubber/linux/block/blk-ioc.c: 72
	 * Both setting lookup hint to and clearing it from @icq are done /Users/rubber/linux/block/blk-ioc.c: 73
	 * under queue_lock.  If it's not pointing to @icq now, it never /Users/rubber/linux/block/blk-ioc.c: 74
	 * will.  Hint assignment itself can race safely. /Users/rubber/linux/block/blk-ioc.c: 75
	/* /Users/rubber/linux/block/blk-ioc.c: 82
	 * @icq->q might have gone away by the time RCU callback runs /Users/rubber/linux/block/blk-ioc.c: 83
	 * making it impossible to determine icq_cache.  Record it in @icq. /Users/rubber/linux/block/blk-ioc.c: 84
 * Slow path for ioc release in put_io_context().  Performs double-lock /Users/rubber/linux/block/blk-ioc.c: 92
 * dancing to unlink all icq's and then frees ioc. /Users/rubber/linux/block/blk-ioc.c: 93
			/* /Users/rubber/linux/block/blk-ioc.c: 118
			 * The icq may have been destroyed when the ioc lock /Users/rubber/linux/block/blk-ioc.c: 119
			 * was released. /Users/rubber/linux/block/blk-ioc.c: 120
 * put_io_context - put a reference of io_context /Users/rubber/linux/block/blk-ioc.c: 136
 * @ioc: io_context to put /Users/rubber/linux/block/blk-ioc.c: 137
 * Decrement reference count of @ioc and release it if the count reaches /Users/rubber/linux/block/blk-ioc.c: 139
 * zero. /Users/rubber/linux/block/blk-ioc.c: 140
	/* /Users/rubber/linux/block/blk-ioc.c: 152
	 * Releasing ioc requires reverse order double locking and we may /Users/rubber/linux/block/blk-ioc.c: 153
	 * already be holding a queue_lock.  Do it asynchronously from wq. /Users/rubber/linux/block/blk-ioc.c: 154
 * put_io_context_active - put active reference on ioc /Users/rubber/linux/block/blk-ioc.c: 171
 * @ioc: ioc of interest /Users/rubber/linux/block/blk-ioc.c: 172
 * Undo get_io_context_active().  If active reference reaches zero after /Users/rubber/linux/block/blk-ioc.c: 174
 * put, @ioc can never issue further IOs and ioscheds are notified. /Users/rubber/linux/block/blk-ioc.c: 175
 * ioc_clear_queue - break any ioc association with the specified queue /Users/rubber/linux/block/blk-ioc.c: 234
 * @q: request_queue being cleared /Users/rubber/linux/block/blk-ioc.c: 235
 * Walk @q->icq_list and exit all io_cq's. /Users/rubber/linux/block/blk-ioc.c: 237
	/* /Users/rubber/linux/block/blk-ioc.c: 269
	 * Try to install.  ioc shouldn't be installed if someone else /Users/rubber/linux/block/blk-ioc.c: 270
	 * already did or @task, which isn't %current, is exiting.  Note /Users/rubber/linux/block/blk-ioc.c: 271
	 * that we need to allow ioc creation on exiting %current as exit /Users/rubber/linux/block/blk-ioc.c: 272
	 * path may issue IOs from e.g. exit_files().  The exit path is /Users/rubber/linux/block/blk-ioc.c: 273
	 * responsible for not issuing IO after exit_io_context(). /Users/rubber/linux/block/blk-ioc.c: 274
 * get_task_io_context - get io_context of a task /Users/rubber/linux/block/blk-ioc.c: 291
 * @task: task of interest /Users/rubber/linux/block/blk-ioc.c: 292
 * @gfp_flags: allocation flags, used if allocation is necessary /Users/rubber/linux/block/blk-ioc.c: 293
 * @node: allocation node, used if allocation is necessary /Users/rubber/linux/block/blk-ioc.c: 294
 * Return io_context of @task.  If it doesn't exist, it is created with /Users/rubber/linux/block/blk-ioc.c: 296
 * @gfp_flags and @node.  The returned io_context has its reference count /Users/rubber/linux/block/blk-ioc.c: 297
 * incremented. /Users/rubber/linux/block/blk-ioc.c: 298
 * This function always goes through task_lock() and it's better to use /Users/rubber/linux/block/blk-ioc.c: 300
 * %current->io_context + get_io_context() for %current. /Users/rubber/linux/block/blk-ioc.c: 301
 * ioc_lookup_icq - lookup io_cq from ioc /Users/rubber/linux/block/blk-ioc.c: 325
 * @ioc: the associated io_context /Users/rubber/linux/block/blk-ioc.c: 326
 * @q: the associated request_queue /Users/rubber/linux/block/blk-ioc.c: 327
 * Look up io_cq associated with @ioc - @q pair from @ioc.  Must be called /Users/rubber/linux/block/blk-ioc.c: 329
 * with @q->queue_lock held. /Users/rubber/linux/block/blk-ioc.c: 330
	/* /Users/rubber/linux/block/blk-ioc.c: 338
	 * icq's are indexed from @ioc using radix tree and hint pointer, /Users/rubber/linux/block/blk-ioc.c: 339
	 * both of which are protected with RCU.  All removals are done /Users/rubber/linux/block/blk-ioc.c: 340
	 * holding both q and ioc locks, and we're holding q lock - if we /Users/rubber/linux/block/blk-ioc.c: 341
	 * find a icq which points to us, it's guaranteed to be valid. /Users/rubber/linux/block/blk-ioc.c: 342
 * ioc_create_icq - create and link io_cq /Users/rubber/linux/block/blk-ioc.c: 361
 * @ioc: io_context of interest /Users/rubber/linux/block/blk-ioc.c: 362
 * @q: request_queue of interest /Users/rubber/linux/block/blk-ioc.c: 363
 * @gfp_mask: allocation mask /Users/rubber/linux/block/blk-ioc.c: 364
 * Make sure io_cq linking @ioc and @q exists.  If icq doesn't exist, they /Users/rubber/linux/block/blk-ioc.c: 366
 * will be created using @gfp_mask. /Users/rubber/linux/block/blk-ioc.c: 367
 * The caller is responsible for ensuring @ioc won't go away and @q is /Users/rubber/linux/block/blk-ioc.c: 369
 * alive and will stay alive until this function returns. /Users/rubber/linux/block/blk-ioc.c: 370
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-debugfs-zoned.c: 1
 * Copyright (C) 2017 Western Digital Corporation or its affiliates. /Users/rubber/linux/block/blk-mq-debugfs-zoned.c: 3
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/bsg-lib.c: 1
 *  BSG helper library /Users/rubber/linux/block/bsg-lib.c: 3
 *  Copyright (C) 2008   James Smart, Emulex Corporation /Users/rubber/linux/block/bsg-lib.c: 5
 *  Copyright (C) 2011   Red Hat, Inc.  All rights reserved. /Users/rubber/linux/block/bsg-lib.c: 6
 *  Copyright (C) 2011   Mike Christie /Users/rubber/linux/block/bsg-lib.c: 7
	/* /Users/rubber/linux/block/bsg-lib.c: 97
	 * The assignments below don't make much sense, but are kept for /Users/rubber/linux/block/bsg-lib.c: 98
	 * bug by bug backwards compatibility: /Users/rubber/linux/block/bsg-lib.c: 99
 * bsg_teardown_job - routine to teardown a bsg job /Users/rubber/linux/block/bsg-lib.c: 153
 * @kref: kref inside bsg_job that is to be torn down /Users/rubber/linux/block/bsg-lib.c: 154
 * bsg_job_done - completion routine for bsg requests /Users/rubber/linux/block/bsg-lib.c: 182
 * @job: bsg_job that is complete /Users/rubber/linux/block/bsg-lib.c: 183
 * @result: job reply result /Users/rubber/linux/block/bsg-lib.c: 184
 * @reply_payload_rcv_len: length of payload recvd /Users/rubber/linux/block/bsg-lib.c: 185
 * The LLD should call this when the bsg job has completed. /Users/rubber/linux/block/bsg-lib.c: 187
 * bsg_complete - softirq done routine for destroying the bsg requests /Users/rubber/linux/block/bsg-lib.c: 202
 * @rq: BSG request that holds the job to be destroyed /Users/rubber/linux/block/bsg-lib.c: 203
 * bsg_prepare_job - create the bsg_job structure for the bsg request /Users/rubber/linux/block/bsg-lib.c: 228
 * @dev: device that is being sent the bsg request /Users/rubber/linux/block/bsg-lib.c: 229
 * @req: BSG request that needs a job structure /Users/rubber/linux/block/bsg-lib.c: 230
 * bsg_queue_rq - generic handler for bsg requests /Users/rubber/linux/block/bsg-lib.c: 263
 * @hctx: hardware queue /Users/rubber/linux/block/bsg-lib.c: 264
 * @bd: queue data /Users/rubber/linux/block/bsg-lib.c: 265
 * On error the create_bsg_job function should return a -Exyz error value /Users/rubber/linux/block/bsg-lib.c: 267
 * that will be set to ->result. /Users/rubber/linux/block/bsg-lib.c: 268
 * Drivers/subsys should pass this to the queue init function. /Users/rubber/linux/block/bsg-lib.c: 270
 * bsg_setup_queue - Create and add the bsg hooks so we can receive requests /Users/rubber/linux/block/bsg-lib.c: 353
 * @dev: device to attach bsg device to /Users/rubber/linux/block/bsg-lib.c: 354
 * @name: device to give bsg device /Users/rubber/linux/block/bsg-lib.c: 355
 * @job_fn: bsg job handler /Users/rubber/linux/block/bsg-lib.c: 356
 * @timeout: timeout handler function pointer /Users/rubber/linux/block/bsg-lib.c: 357
 * @dd_job_size: size of LLD data needed for each job /Users/rubber/linux/block/bsg-lib.c: 358
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/bio.c: 1
 * Copyright (C) 2001 Jens Axboe <axboe@kernel.dk> /Users/rubber/linux/block/bio.c: 3
 * fs_bio_set is the bio_set containing bio and iovec memory pools used by /Users/rubber/linux/block/bio.c: 63
 * IO code that does not need private memory pools. /Users/rubber/linux/block/bio.c: 64
 * Our slab pool management /Users/rubber/linux/block/bio.c: 70
 * Make the first allocation restricted and don't dump info on allocation /Users/rubber/linux/block/bio.c: 169
 * failures, since we'll fall back to the mempool in case of failure. /Users/rubber/linux/block/bio.c: 170
	/* /Users/rubber/linux/block/bio.c: 186
	 * Upgrade the nr_vecs request to take full advantage of the allocation. /Users/rubber/linux/block/bio.c: 187
	 * We also rely on this in the bvec_free path. /Users/rubber/linux/block/bio.c: 188
	/* /Users/rubber/linux/block/bio.c: 192
	 * Try a slab allocation first for all smaller allocations.  If that /Users/rubber/linux/block/bio.c: 193
	 * fails and __GFP_DIRECT_RECLAIM is set retry with the mempool. /Users/rubber/linux/block/bio.c: 194
	 * The mempool is sized to handle up to BIO_MAX_VECS entries. /Users/rubber/linux/block/bio.c: 195
		/* /Users/rubber/linux/block/bio.c: 234
		 * If we have front padding, adjust the bio pointer before freeing /Users/rubber/linux/block/bio.c: 235
 * Users of this function have their own bio allocation. Subsequently, /Users/rubber/linux/block/bio.c: 248
 * they must remember to pair any call to bio_init() with bio_uninit() /Users/rubber/linux/block/bio.c: 249
 * when IO has completed, or when the bio is released. /Users/rubber/linux/block/bio.c: 250
 * bio_reset - reinitialize a bio /Users/rubber/linux/block/bio.c: 294
 * @bio:	bio to reset /Users/rubber/linux/block/bio.c: 295
 * Description: /Users/rubber/linux/block/bio.c: 297
 *   After calling bio_reset(), @bio will be in the same state as a freshly /Users/rubber/linux/block/bio.c: 298
 *   allocated bio returned bio bio_alloc_bioset() - the only fields that are /Users/rubber/linux/block/bio.c: 299
 *   preserved are the ones that are initialized by bio_alloc_bioset(). See /Users/rubber/linux/block/bio.c: 300
 *   comment in struct bio. /Users/rubber/linux/block/bio.c: 301
 * bio_chain - chain bio completions /Users/rubber/linux/block/bio.c: 327
 * @bio: the target bio /Users/rubber/linux/block/bio.c: 328
 * @parent: the parent bio of @bio /Users/rubber/linux/block/bio.c: 329
 * The caller won't have a bi_end_io called when @bio completes - instead, /Users/rubber/linux/block/bio.c: 331
 * @parent's bi_end_io won't be called until both @parent and @bio have /Users/rubber/linux/block/bio.c: 332
 * completed; the chained bio will also be freed when it completes. /Users/rubber/linux/block/bio.c: 333
 * The caller must not set bi_private or bi_end_io in @bio. /Users/rubber/linux/block/bio.c: 335
	/* /Users/rubber/linux/block/bio.c: 371
	 * In order to guarantee forward progress we must punt only bios that /Users/rubber/linux/block/bio.c: 372
	 * were allocated from this bio_set; otherwise, if there was a bio on /Users/rubber/linux/block/bio.c: 373
	 * there for a stacking driver higher up in the stack, processing it /Users/rubber/linux/block/bio.c: 374
	 * could require allocating bios from this bio_set, and doing that from /Users/rubber/linux/block/bio.c: 375
	 * our own rescuer would be bad. /Users/rubber/linux/block/bio.c: 376
	 * /Users/rubber/linux/block/bio.c: 377
	 * Since bio lists are singly linked, pop them all instead of trying to /Users/rubber/linux/block/bio.c: 378
	 * remove from the middle of the list: /Users/rubber/linux/block/bio.c: 379
 * bio_alloc_bioset - allocate a bio for I/O /Users/rubber/linux/block/bio.c: 402
 * @gfp_mask:   the GFP_* mask given to the slab allocator /Users/rubber/linux/block/bio.c: 403
 * @nr_iovecs:	number of iovecs to pre-allocate /Users/rubber/linux/block/bio.c: 404
 * @bs:		the bio_set to allocate from. /Users/rubber/linux/block/bio.c: 405
 * Allocate a bio from the mempools in @bs. /Users/rubber/linux/block/bio.c: 407
 * If %__GFP_DIRECT_RECLAIM is set then bio_alloc will always be able to /Users/rubber/linux/block/bio.c: 409
 * allocate a bio.  This is due to the mempool guarantees.  To make this work, /Users/rubber/linux/block/bio.c: 410
 * callers must never allocate more than 1 bio at a time from the general pool. /Users/rubber/linux/block/bio.c: 411
 * Callers that need to allocate more than 1 bio must always submit the /Users/rubber/linux/block/bio.c: 412
 * previously allocated bio for IO before attempting to allocate a new one. /Users/rubber/linux/block/bio.c: 413
 * Failure to do so can cause deadlocks under memory pressure. /Users/rubber/linux/block/bio.c: 414
 * Note that when running under submit_bio_noacct() (i.e. any block driver), /Users/rubber/linux/block/bio.c: 416
 * bios are not submitted until after you return - see the code in /Users/rubber/linux/block/bio.c: 417
 * submit_bio_noacct() that converts recursion into iteration, to prevent /Users/rubber/linux/block/bio.c: 418
 * stack overflows. /Users/rubber/linux/block/bio.c: 419
 * This would normally mean allocating multiple bios under submit_bio_noacct() /Users/rubber/linux/block/bio.c: 421
 * would be susceptible to deadlocks, but we have /Users/rubber/linux/block/bio.c: 422
 * deadlock avoidance code that resubmits any blocked bios from a rescuer /Users/rubber/linux/block/bio.c: 423
 * thread. /Users/rubber/linux/block/bio.c: 424
 * However, we do not guarantee forward progress for allocations from other /Users/rubber/linux/block/bio.c: 426
 * mempools. Doing multiple allocations from the same mempool under /Users/rubber/linux/block/bio.c: 427
 * submit_bio_noacct() should be avoided - instead, use bio_set's front_pad /Users/rubber/linux/block/bio.c: 428
 * for per bio allocations. /Users/rubber/linux/block/bio.c: 429
 * Returns: Pointer to new bio on success, NULL on failure. /Users/rubber/linux/block/bio.c: 431
	/* /Users/rubber/linux/block/bio.c: 444
	 * submit_bio_noacct() converts recursion to iteration; this means if /Users/rubber/linux/block/bio.c: 445
	 * we're running beneath it, any bios we allocate and submit will not be /Users/rubber/linux/block/bio.c: 446
	 * submitted (and thus freed) until after we return. /Users/rubber/linux/block/bio.c: 447
	 * /Users/rubber/linux/block/bio.c: 448
	 * This exposes us to a potential deadlock if we allocate multiple bios /Users/rubber/linux/block/bio.c: 449
	 * from the same bio_set() while running underneath submit_bio_noacct(). /Users/rubber/linux/block/bio.c: 450
	 * If we were to allocate multiple bios (say a stacking block driver /Users/rubber/linux/block/bio.c: 451
	 * that was splitting bios), we would deadlock if we exhausted the /Users/rubber/linux/block/bio.c: 452
	 * mempool's reserve. /Users/rubber/linux/block/bio.c: 453
	 * /Users/rubber/linux/block/bio.c: 454
	 * We solve this, and guarantee forward progress, with a rescuer /Users/rubber/linux/block/bio.c: 455
	 * workqueue per bio_set. If we go to allocate and there are bios on /Users/rubber/linux/block/bio.c: 456
	 * current->bio_list, we first try the allocation without /Users/rubber/linux/block/bio.c: 457
	 * __GFP_DIRECT_RECLAIM; if that fails, we punt those bios we would be /Users/rubber/linux/block/bio.c: 458
	 * blocking to the rescuer workqueue before we retry with the original /Users/rubber/linux/block/bio.c: 459
	 * gfp_flags. /Users/rubber/linux/block/bio.c: 460
 * bio_kmalloc - kmalloc a bio for I/O /Users/rubber/linux/block/bio.c: 507
 * @gfp_mask:   the GFP_* mask given to the slab allocator /Users/rubber/linux/block/bio.c: 508
 * @nr_iovecs:	number of iovecs to pre-allocate /Users/rubber/linux/block/bio.c: 509
 * Use kmalloc to allocate and initialize a bio. /Users/rubber/linux/block/bio.c: 511
 * Returns: Pointer to new bio on success, NULL on failure. /Users/rubber/linux/block/bio.c: 513
 * bio_truncate - truncate the bio to small size of @new_size /Users/rubber/linux/block/bio.c: 542
 * @bio:	the bio to be truncated /Users/rubber/linux/block/bio.c: 543
 * @new_size:	new size for truncating the bio /Users/rubber/linux/block/bio.c: 544
 * Description: /Users/rubber/linux/block/bio.c: 546
 *   Truncate the bio to new size of @new_size. If bio_op(bio) is /Users/rubber/linux/block/bio.c: 547
 *   REQ_OP_READ, zero the truncated part. This function should only /Users/rubber/linux/block/bio.c: 548
 *   be used for handling corner cases, such as bio eod. /Users/rubber/linux/block/bio.c: 549
	/* /Users/rubber/linux/block/bio.c: 579
	 * Don't touch bvec table here and make it really immutable, since /Users/rubber/linux/block/bio.c: 580
	 * fs bio user has to retrieve all pages via bio_for_each_segment_all /Users/rubber/linux/block/bio.c: 581
	 * in its .end_bio() callback. /Users/rubber/linux/block/bio.c: 582
	 * /Users/rubber/linux/block/bio.c: 583
	 * It is enough to truncate bio by updating .bi_size since we can make /Users/rubber/linux/block/bio.c: 584
	 * correct bvec with the updated .bi_size for drivers. /Users/rubber/linux/block/bio.c: 585
 * guard_bio_eod - truncate a BIO to fit the block device /Users/rubber/linux/block/bio.c: 591
 * @bio:	bio to truncate /Users/rubber/linux/block/bio.c: 592
 * This allows us to do IO even on the odd last sectors of a device, even if the /Users/rubber/linux/block/bio.c: 594
 * block size is some multiple of the physical sector size. /Users/rubber/linux/block/bio.c: 595
 * We'll just truncate the bio to the size of the device, and clear the end of /Users/rubber/linux/block/bio.c: 597
 * the buffer head manually.  Truly out-of-range accesses will turn into actual /Users/rubber/linux/block/bio.c: 598
 * I/O errors, this only handles the "we need to be able to do I/O at the final /Users/rubber/linux/block/bio.c: 599
 * sector" case. /Users/rubber/linux/block/bio.c: 600
	/* /Users/rubber/linux/block/bio.c: 609
	 * If the *whole* IO is past the end of the device, /Users/rubber/linux/block/bio.c: 610
	 * let it through, and the IO layer will turn it into /Users/rubber/linux/block/bio.c: 611
	 * an EIO. /Users/rubber/linux/block/bio.c: 612
 * bio_put - release a reference to a bio /Users/rubber/linux/block/bio.c: 672
 * @bio:   bio to release reference to /Users/rubber/linux/block/bio.c: 673
 * Description: /Users/rubber/linux/block/bio.c: 675
 *   Put a reference to a &struct bio, either one you have gotten with /Users/rubber/linux/block/bio.c: 676
 *   bio_alloc, bio_get or bio_clone_*. The last put of a bio will free it. /Users/rubber/linux/block/bio.c: 677
 * 	__bio_clone_fast - clone a bio that shares the original bio's biovec /Users/rubber/linux/block/bio.c: 703
 * 	@bio: destination bio /Users/rubber/linux/block/bio.c: 704
 * 	@bio_src: bio to clone /Users/rubber/linux/block/bio.c: 705
 *	Clone a &bio. Caller will own the returned bio, but not /Users/rubber/linux/block/bio.c: 707
 *	the actual data it points to. Reference count of returned /Users/rubber/linux/block/bio.c: 708
 * 	bio will be one. /Users/rubber/linux/block/bio.c: 709
 * 	Caller must ensure that @bio_src is not freed before @bio. /Users/rubber/linux/block/bio.c: 711
	/* /Users/rubber/linux/block/bio.c: 717
	 * most users will be overriding ->bi_bdev with a new target, /Users/rubber/linux/block/bio.c: 718
	 * so we don't set nor calculate new physical/hw segment counts here /Users/rubber/linux/block/bio.c: 719
 *	bio_clone_fast - clone a bio that shares the original bio's biovec /Users/rubber/linux/block/bio.c: 739
 *	@bio: bio to clone /Users/rubber/linux/block/bio.c: 740
 *	@gfp_mask: allocation priority /Users/rubber/linux/block/bio.c: 741
 *	@bs: bio_set to allocate from /Users/rubber/linux/block/bio.c: 742
 * 	Like __bio_clone_fast, only also allocates the returned bio /Users/rubber/linux/block/bio.c: 744
 * bio_full - check if the bio is full /Users/rubber/linux/block/bio.c: 778
 * @bio:	bio to check /Users/rubber/linux/block/bio.c: 779
 * @len:	length of one segment to be added /Users/rubber/linux/block/bio.c: 780
 * Return true if @bio is full and one segment with @len bytes can't be /Users/rubber/linux/block/bio.c: 782
 * added to the bio, otherwise return false /Users/rubber/linux/block/bio.c: 783
 * __bio_try_merge_page - try appending data to an existing bvec. /Users/rubber/linux/block/bio.c: 814
 * @bio: destination bio /Users/rubber/linux/block/bio.c: 815
 * @page: start page to add /Users/rubber/linux/block/bio.c: 816
 * @len: length of the data to add /Users/rubber/linux/block/bio.c: 817
 * @off: offset of the data relative to @page /Users/rubber/linux/block/bio.c: 818
 * @same_page: return if the segment has been merged inside the same page /Users/rubber/linux/block/bio.c: 819
 * Try to add the data at @page + @off to the last bvec of @bio.  This is a /Users/rubber/linux/block/bio.c: 821
 * useful optimisation for file systems with a block size smaller than the /Users/rubber/linux/block/bio.c: 822
 * page size. /Users/rubber/linux/block/bio.c: 823
 * Warn if (@len, @off) crosses pages in case that @same_page is true. /Users/rubber/linux/block/bio.c: 825
 * Return %true on success or %false on failure. /Users/rubber/linux/block/bio.c: 827
 * Try to merge a page into a segment, while obeying the hardware segment /Users/rubber/linux/block/bio.c: 852
 * size limit.  This is not for normal read/write bios, but for passthrough /Users/rubber/linux/block/bio.c: 853
 * or Zone Append operations that we can't split. /Users/rubber/linux/block/bio.c: 854
 * bio_add_hw_page - attempt to add a page to a bio with hw constraints /Users/rubber/linux/block/bio.c: 873
 * @q: the target queue /Users/rubber/linux/block/bio.c: 874
 * @bio: destination bio /Users/rubber/linux/block/bio.c: 875
 * @page: page to add /Users/rubber/linux/block/bio.c: 876
 * @len: vec entry length /Users/rubber/linux/block/bio.c: 877
 * @offset: vec entry offset /Users/rubber/linux/block/bio.c: 878
 * @max_sectors: maximum number of sectors that can be added /Users/rubber/linux/block/bio.c: 879
 * @same_page: return if the segment has been merged inside the same page /Users/rubber/linux/block/bio.c: 880
 * Add a page to a bio while respecting the hardware max_sectors, max_segment /Users/rubber/linux/block/bio.c: 882
 * and gap limitations. /Users/rubber/linux/block/bio.c: 883
		/* /Users/rubber/linux/block/bio.c: 901
		 * If the queue doesn't support SG gaps and adding this segment /Users/rubber/linux/block/bio.c: 902
		 * would create a gap, disallow it. /Users/rubber/linux/block/bio.c: 903
 * bio_add_pc_page	- attempt to add page to passthrough bio /Users/rubber/linux/block/bio.c: 926
 * @q: the target queue /Users/rubber/linux/block/bio.c: 927
 * @bio: destination bio /Users/rubber/linux/block/bio.c: 928
 * @page: page to add /Users/rubber/linux/block/bio.c: 929
 * @len: vec entry length /Users/rubber/linux/block/bio.c: 930
 * @offset: vec entry offset /Users/rubber/linux/block/bio.c: 931
 * Attempt to add a page to the bio_vec maplist. This can fail for a /Users/rubber/linux/block/bio.c: 933
 * number of reasons, such as the bio being full or target block device /Users/rubber/linux/block/bio.c: 934
 * limitations. The target block device must allow bio's up to PAGE_SIZE, /Users/rubber/linux/block/bio.c: 935
 * so it is always possible to add a single page to an empty bio. /Users/rubber/linux/block/bio.c: 936
 * This should only be used by passthrough bios. /Users/rubber/linux/block/bio.c: 938
 * bio_add_zone_append_page - attempt to add page to zone-append bio /Users/rubber/linux/block/bio.c: 950
 * @bio: destination bio /Users/rubber/linux/block/bio.c: 951
 * @page: page to add /Users/rubber/linux/block/bio.c: 952
 * @len: vec entry length /Users/rubber/linux/block/bio.c: 953
 * @offset: vec entry offset /Users/rubber/linux/block/bio.c: 954
 * Attempt to add a page to the bio_vec maplist of a bio that will be submitted /Users/rubber/linux/block/bio.c: 956
 * for a zone-append request. This can fail for a number of reasons, such as the /Users/rubber/linux/block/bio.c: 957
 * bio being full or the target block device is not a zoned block device or /Users/rubber/linux/block/bio.c: 958
 * other limitations of the target block device. The target block device must /Users/rubber/linux/block/bio.c: 959
 * allow bio's up to PAGE_SIZE, so it is always possible to add a single page /Users/rubber/linux/block/bio.c: 960
 * to an empty bio. /Users/rubber/linux/block/bio.c: 961
 * Returns: number of bytes added to the bio, or 0 in case of a failure. /Users/rubber/linux/block/bio.c: 963
 * __bio_add_page - add page(s) to a bio in a new segment /Users/rubber/linux/block/bio.c: 983
 * @bio: destination bio /Users/rubber/linux/block/bio.c: 984
 * @page: start page to add /Users/rubber/linux/block/bio.c: 985
 * @len: length of the data to add, may cross pages /Users/rubber/linux/block/bio.c: 986
 * @off: offset of the data relative to @page, may cross pages /Users/rubber/linux/block/bio.c: 987
 * Add the data at @page + @off to @bio as a new bvec.  The caller must ensure /Users/rubber/linux/block/bio.c: 989
 * that @bio has space for another bvec. /Users/rubber/linux/block/bio.c: 990
 *	bio_add_page	-	attempt to add page(s) to bio /Users/rubber/linux/block/bio.c: 1013
 *	@bio: destination bio /Users/rubber/linux/block/bio.c: 1014
 *	@page: start page to add /Users/rubber/linux/block/bio.c: 1015
 *	@len: vec entry length, may cross pages /Users/rubber/linux/block/bio.c: 1016
 *	@offset: vec entry offset relative to @page, may cross pages /Users/rubber/linux/block/bio.c: 1017
 *	Attempt to add page(s) to the bio_vec maplist. This will only fail /Users/rubber/linux/block/bio.c: 1019
 *	if either bio->bi_vcnt == bio->bi_max_vecs or it's a cloned bio. /Users/rubber/linux/block/bio.c: 1020
 * __bio_iov_iter_get_pages - pin user or kernel pages and add them to a bio /Users/rubber/linux/block/bio.c: 1081
 * @bio: bio to add pages to /Users/rubber/linux/block/bio.c: 1082
 * @iter: iov iterator describing the region to be mapped /Users/rubber/linux/block/bio.c: 1083
 * Pins pages from *iter and appends them to @bio's bvec array. The /Users/rubber/linux/block/bio.c: 1085
 * pages will have to be released using put_page() when done. /Users/rubber/linux/block/bio.c: 1086
 * For multi-segment *iter, this function only adds pages from the /Users/rubber/linux/block/bio.c: 1087
 * next non-empty segment of the iov iterator. /Users/rubber/linux/block/bio.c: 1088
	/* /Users/rubber/linux/block/bio.c: 1101
	 * Move page array up in the allocated memory for the bio vecs as far as /Users/rubber/linux/block/bio.c: 1102
	 * possible so that we can start filling biovecs from the beginning /Users/rubber/linux/block/bio.c: 1103
	 * without overwriting the temporary page array. /Users/rubber/linux/block/bio.c: 1104
	/* /Users/rubber/linux/block/bio.c: 1151
	 * Move page array up in the allocated memory for the bio vecs as far as /Users/rubber/linux/block/bio.c: 1152
	 * possible so that we can start filling biovecs from the beginning /Users/rubber/linux/block/bio.c: 1153
	 * without overwriting the temporary page array. /Users/rubber/linux/block/bio.c: 1154
 * bio_iov_iter_get_pages - add user or kernel pages to a bio /Users/rubber/linux/block/bio.c: 1184
 * @bio: bio to add pages to /Users/rubber/linux/block/bio.c: 1185
 * @iter: iov iterator describing the region to be added /Users/rubber/linux/block/bio.c: 1186
 * This takes either an iterator pointing to user memory, or one pointing to /Users/rubber/linux/block/bio.c: 1188
 * kernel pages (BVEC iterator). If we're adding user pages, we pin them and /Users/rubber/linux/block/bio.c: 1189
 * map them into the kernel. On IO completion, the caller should put those /Users/rubber/linux/block/bio.c: 1190
 * pages. For bvec based iterators bio_iov_iter_get_pages() uses the provided /Users/rubber/linux/block/bio.c: 1191
 * bvecs rather than copying them. Hence anyone issuing kiocb based IO needs /Users/rubber/linux/block/bio.c: 1192
 * to ensure the bvecs and pages stay referenced until the submitted I/O is /Users/rubber/linux/block/bio.c: 1193
 * completed by a call to ->ki_complete() or returns with an error other than /Users/rubber/linux/block/bio.c: 1194
 * -EIOCBQUEUED. The caller needs to check if the bio is flagged BIO_NO_PAGE_REF /Users/rubber/linux/block/bio.c: 1195
 * on IO completion. If it isn't, then pages should be released. /Users/rubber/linux/block/bio.c: 1196
 * The function tries, but does not guarantee, to pin as many pages as /Users/rubber/linux/block/bio.c: 1198
 * fit into the bio, or are requested in @iter, whatever is smaller. If /Users/rubber/linux/block/bio.c: 1199
 * MM encounters an error pinning the requested pages, it stops. Error /Users/rubber/linux/block/bio.c: 1200
 * is returned only if 0 pages could be pinned. /Users/rubber/linux/block/bio.c: 1201
 * It's intended for direct IO, so doesn't do PSI tracking, the caller is /Users/rubber/linux/block/bio.c: 1203
 * responsible for setting BIO_WORKINGSET if necessary. /Users/rubber/linux/block/bio.c: 1204
 * submit_bio_wait - submit a bio, and wait until it completes /Users/rubber/linux/block/bio.c: 1235
 * @bio: The &struct bio which describes the I/O /Users/rubber/linux/block/bio.c: 1236
 * Simple wrapper around submit_bio(). Returns 0 on success, or the error from /Users/rubber/linux/block/bio.c: 1238
 * bio_endio() on failure. /Users/rubber/linux/block/bio.c: 1239
 * WARNING: Unlike to how submit_bio() is usually used, this function does not /Users/rubber/linux/block/bio.c: 1241
 * result in bio reference to be consumed. The caller must drop the reference /Users/rubber/linux/block/bio.c: 1242
 * on his own. /Users/rubber/linux/block/bio.c: 1243
 * bio_copy_data - copy contents of data buffers from one bio to another /Users/rubber/linux/block/bio.c: 1299
 * @src: source bio /Users/rubber/linux/block/bio.c: 1300
 * @dst: destination bio /Users/rubber/linux/block/bio.c: 1301
 * Stops when it reaches the end of either @src or @dst - that is, copies /Users/rubber/linux/block/bio.c: 1303
 * min(src->bi_size, dst->bi_size) bytes (or the equivalent for lists of bios). /Users/rubber/linux/block/bio.c: 1304
 * bio_set_pages_dirty() and bio_check_pages_dirty() are support functions /Users/rubber/linux/block/bio.c: 1326
 * for performing direct-IO in BIOs. /Users/rubber/linux/block/bio.c: 1327
 * The problem is that we cannot run set_page_dirty() from interrupt context /Users/rubber/linux/block/bio.c: 1329
 * because the required locks are not interrupt-safe.  So what we can do is to /Users/rubber/linux/block/bio.c: 1330
 * mark the pages dirty _before_ performing IO.  And in interrupt context, /Users/rubber/linux/block/bio.c: 1331
 * check that the pages are still dirty.   If so, fine.  If not, redirty them /Users/rubber/linux/block/bio.c: 1332
 * in process context. /Users/rubber/linux/block/bio.c: 1333
 * We special-case compound pages here: normally this means reads into hugetlb /Users/rubber/linux/block/bio.c: 1335
 * pages.  The logic in here doesn't really work right for compound pages /Users/rubber/linux/block/bio.c: 1336
 * because the VM does not uniformly chase down the head page in all cases. /Users/rubber/linux/block/bio.c: 1337
 * But dirtiness of compound pages is pretty meaningless anyway: the VM doesn't /Users/rubber/linux/block/bio.c: 1338
 * handle them at all.  So we skip compound pages here at an early stage. /Users/rubber/linux/block/bio.c: 1339
 * Note that this code is very hard to test under normal circumstances because /Users/rubber/linux/block/bio.c: 1341
 * direct-io pins the pages with get_user_pages().  This makes /Users/rubber/linux/block/bio.c: 1342
 * is_page_cache_freeable return false, and the VM will not clean the pages. /Users/rubber/linux/block/bio.c: 1343
 * But other code (eg, flusher threads) could clean the pages if they are mapped /Users/rubber/linux/block/bio.c: 1344
 * pagecache. /Users/rubber/linux/block/bio.c: 1345
 * Simply disabling the call to bio_set_pages_dirty() is a good way to test the /Users/rubber/linux/block/bio.c: 1347
 * deferred bio dirtying paths. /Users/rubber/linux/block/bio.c: 1348
 * bio_set_pages_dirty() will mark all the bio's pages as dirty. /Users/rubber/linux/block/bio.c: 1352
 * bio_check_pages_dirty() will check that all the BIO's pages are still dirty. /Users/rubber/linux/block/bio.c: 1366
 * If they are, then fine.  If, however, some pages are clean then they must /Users/rubber/linux/block/bio.c: 1367
 * have been written out during the direct-IO read.  So we take another ref on /Users/rubber/linux/block/bio.c: 1368
 * the BIO and re-dirty the pages in process context. /Users/rubber/linux/block/bio.c: 1369
 * It is expected that bio_check_pages_dirty() will wholly own the BIO from /Users/rubber/linux/block/bio.c: 1371
 * here on.  It will run one put_page() against each page and will run one /Users/rubber/linux/block/bio.c: 1372
 * bio_put() against the BIO. /Users/rubber/linux/block/bio.c: 1373
 * This runs in process context /Users/rubber/linux/block/bio.c: 1383
	/* /Users/rubber/linux/block/bio.c: 1426
	 * If we're not chaining, then ->__bi_remaining is always 1 and /Users/rubber/linux/block/bio.c: 1427
	 * we always end io on the first invocation. /Users/rubber/linux/block/bio.c: 1428
 * bio_endio - end I/O on a bio /Users/rubber/linux/block/bio.c: 1444
 * @bio:	bio /Users/rubber/linux/block/bio.c: 1445
 * Description: /Users/rubber/linux/block/bio.c: 1447
 *   bio_endio() will end I/O on the whole bio. bio_endio() is the preferred /Users/rubber/linux/block/bio.c: 1448
 *   way to end I/O on a bio. No one should call bi_end_io() directly on a /Users/rubber/linux/block/bio.c: 1449
 *   bio unless they own it and thus know that it has an end_io function. /Users/rubber/linux/block/bio.c: 1450
 *   bio_endio() can be called several times on a bio that has been chained /Users/rubber/linux/block/bio.c: 1452
 *   using bio_chain().  The ->bi_end_io() function will only be called the /Users/rubber/linux/block/bio.c: 1453
 *   last time. /Users/rubber/linux/block/bio.c: 1454
	/* /Users/rubber/linux/block/bio.c: 1472
	 * Need to have a real endio function for chained bios, otherwise /Users/rubber/linux/block/bio.c: 1473
	 * various corner cases will break (like stacking block devices that /Users/rubber/linux/block/bio.c: 1474
	 * save/restore bi_end_io) - however, we want to avoid unbounded /Users/rubber/linux/block/bio.c: 1475
	 * recursion and blowing the stack. Tail call optimization would /Users/rubber/linux/block/bio.c: 1476
	 * handle this, but compiling with frame pointers also disables /Users/rubber/linux/block/bio.c: 1477
	 * gcc's sibling call optimization. /Users/rubber/linux/block/bio.c: 1478
 * bio_split - split a bio /Users/rubber/linux/block/bio.c: 1494
 * @bio:	bio to split /Users/rubber/linux/block/bio.c: 1495
 * @sectors:	number of sectors to split from the front of @bio /Users/rubber/linux/block/bio.c: 1496
 * @gfp:	gfp mask /Users/rubber/linux/block/bio.c: 1497
 * @bs:		bio set to allocate from /Users/rubber/linux/block/bio.c: 1498
 * Allocates and returns a new bio which represents @sectors from the start of /Users/rubber/linux/block/bio.c: 1500
 * @bio, and updates @bio to represent the remaining sectors. /Users/rubber/linux/block/bio.c: 1501
 * Unless this is a discard request the newly allocated bio will point /Users/rubber/linux/block/bio.c: 1503
 * to @bio's bi_io_vec. It is the caller's responsibility to ensure that /Users/rubber/linux/block/bio.c: 1504
 * neither @bio nor @bs are freed before the split bio. /Users/rubber/linux/block/bio.c: 1505
 * bio_trim - trim a bio /Users/rubber/linux/block/bio.c: 1538
 * @bio:	bio to trim /Users/rubber/linux/block/bio.c: 1539
 * @offset:	number of sectors to trim from the front of @bio /Users/rubber/linux/block/bio.c: 1540
 * @size:	size we want to trim @bio to, in sectors /Users/rubber/linux/block/bio.c: 1541
 * This function is typically used for bios that are cloned and submitted /Users/rubber/linux/block/bio.c: 1543
 * to the underlying device in parts. /Users/rubber/linux/block/bio.c: 1544
 * create memory pools for biovec's in a bio_set. /Users/rubber/linux/block/bio.c: 1565
 * use the global biovec slabs created for general use. /Users/rubber/linux/block/bio.c: 1566
 * bioset_exit - exit a bioset initialized with bioset_init() /Users/rubber/linux/block/bio.c: 1576
 * May be called on a zeroed but uninitialized bioset (i.e. allocated with /Users/rubber/linux/block/bio.c: 1578
 * kzalloc()). /Users/rubber/linux/block/bio.c: 1579
 * bioset_init - Initialize a bio_set /Users/rubber/linux/block/bio.c: 1599
 * @bs:		pool to initialize /Users/rubber/linux/block/bio.c: 1600
 * @pool_size:	Number of bio and bio_vecs to cache in the mempool /Users/rubber/linux/block/bio.c: 1601
 * @front_pad:	Number of bytes to allocate in front of the returned bio /Users/rubber/linux/block/bio.c: 1602
 * @flags:	Flags to modify behavior, currently %BIOSET_NEED_BVECS /Users/rubber/linux/block/bio.c: 1603
 *              and %BIOSET_NEED_RESCUER /Users/rubber/linux/block/bio.c: 1604
 * Description: /Users/rubber/linux/block/bio.c: 1606
 *    Set up a bio_set to be used with @bio_alloc_bioset. Allows the caller /Users/rubber/linux/block/bio.c: 1607
 *    to ask for a number of bytes to be allocated in front of the bio. /Users/rubber/linux/block/bio.c: 1608
 *    Front pad allocation is useful for embedding the bio inside /Users/rubber/linux/block/bio.c: 1609
 *    another structure, to avoid allocating extra data to go with the bio. /Users/rubber/linux/block/bio.c: 1610
 *    Note that the bio must be embedded at the END of that structure always, /Users/rubber/linux/block/bio.c: 1611
 *    or things will break badly. /Users/rubber/linux/block/bio.c: 1612
 *    If %BIOSET_NEED_BVECS is set in @flags, a separate pool will be allocated /Users/rubber/linux/block/bio.c: 1613
 *    for allocating iovecs.  This pool is not needed e.g. for bio_clone_fast(). /Users/rubber/linux/block/bio.c: 1614
 *    If %BIOSET_NEED_RESCUER is set, a workqueue is created which can be used to /Users/rubber/linux/block/bio.c: 1615
 *    dispatch queued requests when the mempool runs out of space. /Users/rubber/linux/block/bio.c: 1616
 * Initialize and setup a new bio_set, based on the settings from /Users/rubber/linux/block/bio.c: 1666
 * another bio_set. /Users/rubber/linux/block/bio.c: 1667
 * bio_alloc_kiocb - Allocate a bio from bio_set based on kiocb /Users/rubber/linux/block/bio.c: 1684
 * @kiocb:	kiocb describing the IO /Users/rubber/linux/block/bio.c: 1685
 * @nr_vecs:	number of iovecs to pre-allocate /Users/rubber/linux/block/bio.c: 1686
 * @bs:		bio_set to allocate from /Users/rubber/linux/block/bio.c: 1687
 * Description: /Users/rubber/linux/block/bio.c: 1689
 *    Like @bio_alloc_bioset, but pass in the kiocb. The kiocb is only /Users/rubber/linux/block/bio.c: 1690
 *    used to check if we should dip into the per-cpu bio_set allocation /Users/rubber/linux/block/bio.c: 1691
 *    cache. The allocation uses GFP_KERNEL internally. On return, the /Users/rubber/linux/block/bio.c: 1692
 *    bio is marked BIO_PERCPU_CACHEABLE, and the final put of the bio /Users/rubber/linux/block/bio.c: 1693
 *    MUST be done from process context, not hard/soft IRQ. /Users/rubber/linux/block/bio.c: 1694
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-lib.c: 1
 * Functions related to generic helpers functions /Users/rubber/linux/block/blk-lib.c: 3
		/* /Users/rubber/linux/block/blk-lib.c: 78
		 * Check whether the discard bio starts at a discard_granularity /Users/rubber/linux/block/blk-lib.c: 79
		 * aligned LBA, /Users/rubber/linux/block/blk-lib.c: 80
		 * - If no: set (granularity_aligned_lba - sector_mapped) to /Users/rubber/linux/block/blk-lib.c: 81
		 *   bi_size of the first split bio, then the second bio will /Users/rubber/linux/block/blk-lib.c: 82
		 *   start at a discard_granularity aligned LBA on the device. /Users/rubber/linux/block/blk-lib.c: 83
		 * - If yes: use bio_aligned_discard_max_sectors() as the max /Users/rubber/linux/block/blk-lib.c: 84
		 *   possible bi_size of the first split bio. Then when this bio /Users/rubber/linux/block/blk-lib.c: 85
		 *   is split in device drive, the split ones are very probably /Users/rubber/linux/block/blk-lib.c: 86
		 *   to be aligned to discard_granularity of the device's queue. /Users/rubber/linux/block/blk-lib.c: 87
		/* /Users/rubber/linux/block/blk-lib.c: 107
		 * We can loop for a long time in here, if someone does /Users/rubber/linux/block/blk-lib.c: 108
		 * full device discards (like mkfs). Be nice and allow /Users/rubber/linux/block/blk-lib.c: 109
		 * us to schedule out to avoid softlocking if preempt /Users/rubber/linux/block/blk-lib.c: 110
		 * is disabled. /Users/rubber/linux/block/blk-lib.c: 111
 * blkdev_issue_discard - queue a discard /Users/rubber/linux/block/blk-lib.c: 122
 * @bdev:	blockdev to issue discard for /Users/rubber/linux/block/blk-lib.c: 123
 * @sector:	start sector /Users/rubber/linux/block/blk-lib.c: 124
 * @nr_sects:	number of sectors to discard /Users/rubber/linux/block/blk-lib.c: 125
 * @gfp_mask:	memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-lib.c: 126
 * @flags:	BLKDEV_DISCARD_* flags to control behaviour /Users/rubber/linux/block/blk-lib.c: 127
 * Description: /Users/rubber/linux/block/blk-lib.c: 129
 *    Issue a discard request for the sectors in question. /Users/rubber/linux/block/blk-lib.c: 130
 * __blkdev_issue_write_same - generate number of bios with same page /Users/rubber/linux/block/blk-lib.c: 155
 * @bdev:	target blockdev /Users/rubber/linux/block/blk-lib.c: 156
 * @sector:	start sector /Users/rubber/linux/block/blk-lib.c: 157
 * @nr_sects:	number of sectors to write /Users/rubber/linux/block/blk-lib.c: 158
 * @gfp_mask:	memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-lib.c: 159
 * @page:	page containing data to write /Users/rubber/linux/block/blk-lib.c: 160
 * @biop:	pointer to anchor bio /Users/rubber/linux/block/blk-lib.c: 161
 * Description: /Users/rubber/linux/block/blk-lib.c: 163
 *  Generate and issue number of bios(REQ_OP_WRITE_SAME) with same page. /Users/rubber/linux/block/blk-lib.c: 164
 * blkdev_issue_write_same - queue a write same operation /Users/rubber/linux/block/blk-lib.c: 217
 * @bdev:	target blockdev /Users/rubber/linux/block/blk-lib.c: 218
 * @sector:	start sector /Users/rubber/linux/block/blk-lib.c: 219
 * @nr_sects:	number of sectors to write /Users/rubber/linux/block/blk-lib.c: 220
 * @gfp_mask:	memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-lib.c: 221
 * @page:	page containing data /Users/rubber/linux/block/blk-lib.c: 222
 * Description: /Users/rubber/linux/block/blk-lib.c: 224
 *    Issue a write same request for the sectors in question. /Users/rubber/linux/block/blk-lib.c: 225
 * Convert a number of 512B sectors to a number of pages. /Users/rubber/linux/block/blk-lib.c: 291
 * The result is limited to a number of pages that can fit into a BIO. /Users/rubber/linux/block/blk-lib.c: 292
 * Also make sure that the result is always at least 1 (page) for the cases /Users/rubber/linux/block/blk-lib.c: 293
 * where nr_sects is lower than the number of sectors in a page. /Users/rubber/linux/block/blk-lib.c: 294
 * __blkdev_issue_zeroout - generate number of zero filed write bios /Users/rubber/linux/block/blk-lib.c: 341
 * @bdev:	blockdev to issue /Users/rubber/linux/block/blk-lib.c: 342
 * @sector:	start sector /Users/rubber/linux/block/blk-lib.c: 343
 * @nr_sects:	number of sectors to write /Users/rubber/linux/block/blk-lib.c: 344
 * @gfp_mask:	memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-lib.c: 345
 * @biop:	pointer to anchor bio /Users/rubber/linux/block/blk-lib.c: 346
 * @flags:	controls detailed behavior /Users/rubber/linux/block/blk-lib.c: 347
 * Description: /Users/rubber/linux/block/blk-lib.c: 349
 *  Zero-fill a block range, either using hardware offload or by explicitly /Users/rubber/linux/block/blk-lib.c: 350
 *  writing zeroes to the device. /Users/rubber/linux/block/blk-lib.c: 351
 *  If a device is using logical block provisioning, the underlying space will /Users/rubber/linux/block/blk-lib.c: 353
 *  not be released if %flags contains BLKDEV_ZERO_NOUNMAP. /Users/rubber/linux/block/blk-lib.c: 354
 *  If %flags contains BLKDEV_ZERO_NOFALLBACK, the function will return /Users/rubber/linux/block/blk-lib.c: 356
 *  -EOPNOTSUPP if no explicit hardware offload for zeroing is provided. /Users/rubber/linux/block/blk-lib.c: 357
 * blkdev_issue_zeroout - zero-fill a block range /Users/rubber/linux/block/blk-lib.c: 381
 * @bdev:	blockdev to write /Users/rubber/linux/block/blk-lib.c: 382
 * @sector:	start sector /Users/rubber/linux/block/blk-lib.c: 383
 * @nr_sects:	number of sectors to write /Users/rubber/linux/block/blk-lib.c: 384
 * @gfp_mask:	memory allocation flags (for bio_alloc) /Users/rubber/linux/block/blk-lib.c: 385
 * @flags:	controls detailed behavior /Users/rubber/linux/block/blk-lib.c: 386
 * Description: /Users/rubber/linux/block/blk-lib.c: 388
 *  Zero-fill a block range, either using hardware offload or by explicitly /Users/rubber/linux/block/blk-lib.c: 389
 *  writing zeroes to the device.  See __blkdev_issue_zeroout() for the /Users/rubber/linux/block/blk-lib.c: 390
 *  valid values for %flags. /Users/rubber/linux/block/blk-lib.c: 391
			/* /Users/rubber/linux/block/blk-lib.c: 430
			 * Zeroing offload support was indicated, but the /Users/rubber/linux/block/blk-lib.c: 431
			 * device reported ILLEGAL REQUEST (for some devices /Users/rubber/linux/block/blk-lib.c: 432
			 * there is no non-destructive way to verify whether /Users/rubber/linux/block/blk-lib.c: 433
			 * WRITE ZEROES is actually supported). /Users/rubber/linux/block/blk-lib.c: 434
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-sysfs.c: 1
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/badblocks.c: 1
 * Bad block management /Users/rubber/linux/block/badblocks.c: 3
 * - Heavily based on MD badblocks code from Neil Brown /Users/rubber/linux/block/badblocks.c: 5
 * Copyright (c) 2015, Intel Corporation. /Users/rubber/linux/block/badblocks.c: 7
 * badblocks_check() - check a given range for bad sectors /Users/rubber/linux/block/badblocks.c: 20
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 21
 * @s:		sector (start) at which to check for badblocks /Users/rubber/linux/block/badblocks.c: 22
 * @sectors:	number of sectors to check for badblocks /Users/rubber/linux/block/badblocks.c: 23
 * @first_bad:	pointer to store location of the first badblock /Users/rubber/linux/block/badblocks.c: 24
 * @bad_sectors: pointer to store number of badblocks after @first_bad /Users/rubber/linux/block/badblocks.c: 25
 * We can record which blocks on each device are 'bad' and so just /Users/rubber/linux/block/badblocks.c: 27
 * fail those blocks, or that stripe, rather than the whole device. /Users/rubber/linux/block/badblocks.c: 28
 * Entries in the bad-block table are 64bits wide.  This comprises: /Users/rubber/linux/block/badblocks.c: 29
 * Length of bad-range, in sectors: 0-511 for lengths 1-512 /Users/rubber/linux/block/badblocks.c: 30
 * Start of bad-range, sector offset, 54 bits (allows 8 exbibytes) /Users/rubber/linux/block/badblocks.c: 31
 *  A 'shift' can be set so that larger blocks are tracked and /Users/rubber/linux/block/badblocks.c: 32
 *  consequently larger devices can be covered. /Users/rubber/linux/block/badblocks.c: 33
 * 'Acknowledged' flag - 1 bit. - the most significant bit. /Users/rubber/linux/block/badblocks.c: 34
 * Locking of the bad-block table uses a seqlock so badblocks_check /Users/rubber/linux/block/badblocks.c: 36
 * might need to retry if it is very unlucky. /Users/rubber/linux/block/badblocks.c: 37
 * We will sometimes want to check for bad blocks in a bi_end_io function, /Users/rubber/linux/block/badblocks.c: 38
 * so we use the write_seqlock_irq variant. /Users/rubber/linux/block/badblocks.c: 39
 * When looking for a bad block we specify a range and want to /Users/rubber/linux/block/badblocks.c: 41
 * know if any block in the range is bad.  So we binary-search /Users/rubber/linux/block/badblocks.c: 42
 * to the last range that starts at-or-before the given endpoint, /Users/rubber/linux/block/badblocks.c: 43
 * (or "before the sector after the target range") /Users/rubber/linux/block/badblocks.c: 44
 * then see if it ends after the given start. /Users/rubber/linux/block/badblocks.c: 45
 * Return: /Users/rubber/linux/block/badblocks.c: 47
 *  0: there are no known bad blocks in the range /Users/rubber/linux/block/badblocks.c: 48
 *  1: there are known bad block which are all acknowledged /Users/rubber/linux/block/badblocks.c: 49
 * -1: there are bad blocks which have not yet been acknowledged in metadata. /Users/rubber/linux/block/badblocks.c: 50
 * plus the start/length of the first bad section we overlap. /Users/rubber/linux/block/badblocks.c: 51
	/* Binary search between lo and hi for 'target' /Users/rubber/linux/block/badblocks.c: 78
	 * i.e. for the last range that starts before 'target' /Users/rubber/linux/block/badblocks.c: 79
	/* INVARIANT: ranges before 'lo' and at-or-after 'hi' /Users/rubber/linux/block/badblocks.c: 81
	 * are known not to be the last range before target. /Users/rubber/linux/block/badblocks.c: 82
	 * VARIANT: hi-lo is the number of possible /Users/rubber/linux/block/badblocks.c: 83
	 * ranges, and decreases until it reaches 1 /Users/rubber/linux/block/badblocks.c: 84
			/* This could still be the one, earlier ranges /Users/rubber/linux/block/badblocks.c: 91
			 * could not. /Users/rubber/linux/block/badblocks.c: 92
		/* need to check all range that end after 's' to see if /Users/rubber/linux/block/badblocks.c: 101
		 * any are unacknowledged. /Users/rubber/linux/block/badblocks.c: 102
				/* starts before the end, and finishes after /Users/rubber/linux/block/badblocks.c: 107
				 * the start, so they must overlap /Users/rubber/linux/block/badblocks.c: 108
 * badblocks_set() - Add a range of bad blocks to the table. /Users/rubber/linux/block/badblocks.c: 149
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 150
 * @s:		first sector to mark as bad /Users/rubber/linux/block/badblocks.c: 151
 * @sectors:	number of sectors to mark as bad /Users/rubber/linux/block/badblocks.c: 152
 * @acknowledged: weather to mark the bad sectors as acknowledged /Users/rubber/linux/block/badblocks.c: 153
 * This might extend the table, or might contract it if two adjacent ranges /Users/rubber/linux/block/badblocks.c: 155
 * can be merged. We binary-search to find the 'insertion' point, then /Users/rubber/linux/block/badblocks.c: 156
 * decide how best to handle it. /Users/rubber/linux/block/badblocks.c: 157
 * Return: /Users/rubber/linux/block/badblocks.c: 159
 *  0: success /Users/rubber/linux/block/badblocks.c: 160
 *  1: failed to set badblocks (out of space) /Users/rubber/linux/block/badblocks.c: 161
		/* we found a range that might merge with the start /Users/rubber/linux/block/badblocks.c: 204
		 * of our new range /Users/rubber/linux/block/badblocks.c: 205
				/* does not all fit in one range, /Users/rubber/linux/block/badblocks.c: 225
				 * make p[lo] maximal /Users/rubber/linux/block/badblocks.c: 226
		/* 'hi' points to the first range that starts after 's'. /Users/rubber/linux/block/badblocks.c: 236
		 * Maybe we can merge with the start of that range /Users/rubber/linux/block/badblocks.c: 237
		/* didn't merge (it all). /Users/rubber/linux/block/badblocks.c: 284
		 * Need to add a range just before 'hi' /Users/rubber/linux/block/badblocks.c: 285
 * badblocks_clear() - Remove a range of bad blocks to the table. /Users/rubber/linux/block/badblocks.c: 318
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 319
 * @s:		first sector to mark as bad /Users/rubber/linux/block/badblocks.c: 320
 * @sectors:	number of sectors to mark as bad /Users/rubber/linux/block/badblocks.c: 321
 * This may involve extending the table if we spilt a region, /Users/rubber/linux/block/badblocks.c: 323
 * but it must not fail.  So if the table becomes full, we just /Users/rubber/linux/block/badblocks.c: 324
 * drop the remove request. /Users/rubber/linux/block/badblocks.c: 325
 * Return: /Users/rubber/linux/block/badblocks.c: 327
 *  0: success /Users/rubber/linux/block/badblocks.c: 328
 *  1: failed to clear badblocks /Users/rubber/linux/block/badblocks.c: 329
		/* When clearing we round the start up and the end down. /Users/rubber/linux/block/badblocks.c: 339
		 * This should not matter as the shift should align with /Users/rubber/linux/block/badblocks.c: 340
		 * the block size and no rounding should ever be needed. /Users/rubber/linux/block/badblocks.c: 341
		 * However it is better the think a block is bad when it /Users/rubber/linux/block/badblocks.c: 342
		 * isn't than to think a block is not bad when it is. /Users/rubber/linux/block/badblocks.c: 343
		/* p[lo] is the last range that could overlap the /Users/rubber/linux/block/badblocks.c: 367
		 * current range.  Earlier ranges could also overlap, /Users/rubber/linux/block/badblocks.c: 368
		 * but only this one can overlap the end of the range. /Users/rubber/linux/block/badblocks.c: 369
		/* 'lo' is strictly before, 'hi' is strictly after, /Users/rubber/linux/block/badblocks.c: 409
		 * anything between needs to be discarded /Users/rubber/linux/block/badblocks.c: 410
 * ack_all_badblocks() - Acknowledge all bad blocks in a list. /Users/rubber/linux/block/badblocks.c: 427
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 428
 * This only succeeds if ->changed is clear.  It is used by /Users/rubber/linux/block/badblocks.c: 430
 * in-kernel metadata updates /Users/rubber/linux/block/badblocks.c: 431
 * badblocks_show() - sysfs access to bad-blocks list /Users/rubber/linux/block/badblocks.c: 459
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 460
 * @page:	buffer received from sysfs /Users/rubber/linux/block/badblocks.c: 461
 * @unack:	weather to show unacknowledged badblocks /Users/rubber/linux/block/badblocks.c: 462
 * Return: /Users/rubber/linux/block/badblocks.c: 464
 *  Length of returned data /Users/rubber/linux/block/badblocks.c: 465
 * badblocks_store() - sysfs access to bad-blocks list /Users/rubber/linux/block/badblocks.c: 508
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 509
 * @page:	buffer received from sysfs /Users/rubber/linux/block/badblocks.c: 510
 * @len:	length of data received from sysfs /Users/rubber/linux/block/badblocks.c: 511
 * @unack:	weather to show unacknowledged badblocks /Users/rubber/linux/block/badblocks.c: 512
 * Return: /Users/rubber/linux/block/badblocks.c: 514
 *  Length of the buffer processed or -ve error. /Users/rubber/linux/block/badblocks.c: 515
 * badblocks_init() - initialize the badblocks structure /Users/rubber/linux/block/badblocks.c: 567
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 568
 * @enable:	weather to enable badblocks accounting /Users/rubber/linux/block/badblocks.c: 569
 * Return: /Users/rubber/linux/block/badblocks.c: 571
 *  0: success /Users/rubber/linux/block/badblocks.c: 572
 *  -ve errno: on error /Users/rubber/linux/block/badblocks.c: 573
 * badblocks_exit() - free the badblocks structure /Users/rubber/linux/block/badblocks.c: 590
 * @bb:		the badblocks structure that holds all badblock information /Users/rubber/linux/block/badblocks.c: 591
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-sched.c: 1
 * blk-mq scheduling framework /Users/rubber/linux/block/blk-mq-sched.c: 3
 * Copyright (C) 2016 Jens Axboe /Users/rubber/linux/block/blk-mq-sched.c: 5
	/* /Users/rubber/linux/block/blk-mq-sched.c: 27
	 * May not have an IO context if it's a passthrough request /Users/rubber/linux/block/blk-mq-sched.c: 28
 * Mark a hardware queue as needing a restart. For shared queues, maintain /Users/rubber/linux/block/blk-mq-sched.c: 48
 * a count of how many hardware queues are marked for restart. /Users/rubber/linux/block/blk-mq-sched.c: 49
	/* /Users/rubber/linux/block/blk-mq-sched.c: 64
	 * Order clearing SCHED_RESTART and list_empty_careful(&hctx->dispatch) /Users/rubber/linux/block/blk-mq-sched.c: 65
	 * in blk_mq_run_hw_queue(). Its pair is the barrier in /Users/rubber/linux/block/blk-mq-sched.c: 66
	 * blk_mq_dispatch_rq_list(). So dispatch code won't see SCHED_RESTART, /Users/rubber/linux/block/blk-mq-sched.c: 67
	 * meantime new request added to hctx->dispatch is missed to check in /Users/rubber/linux/block/blk-mq-sched.c: 68
	 * blk_mq_run_hw_queue(). /Users/rubber/linux/block/blk-mq-sched.c: 69
 * Only SCSI implements .get_budget and .put_budget, and SCSI restarts /Users/rubber/linux/block/blk-mq-sched.c: 109
 * its queue by itself in its completion handler, so we don't need to /Users/rubber/linux/block/blk-mq-sched.c: 110
 * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE. /Users/rubber/linux/block/blk-mq-sched.c: 111
 * Returns -EAGAIN if hctx->dispatch was found non-empty and run_work has to /Users/rubber/linux/block/blk-mq-sched.c: 113
 * be run again.  This is necessary to avoid starving flushes. /Users/rubber/linux/block/blk-mq-sched.c: 114
			/* /Users/rubber/linux/block/blk-mq-sched.c: 150
			 * We're releasing without dispatching. Holding the /Users/rubber/linux/block/blk-mq-sched.c: 151
			 * budget could have blocked any "hctx"s with the /Users/rubber/linux/block/blk-mq-sched.c: 152
			 * same queue and if we didn't dispatch then there's /Users/rubber/linux/block/blk-mq-sched.c: 153
			 * no guarantee anyone will kick the queue.  Kick it /Users/rubber/linux/block/blk-mq-sched.c: 154
			 * ourselves. /Users/rubber/linux/block/blk-mq-sched.c: 155
		/* /Users/rubber/linux/block/blk-mq-sched.c: 163
		 * Now this rq owns the budget which has to be released /Users/rubber/linux/block/blk-mq-sched.c: 164
		 * if this rq won't be queued to driver via .queue_rq() /Users/rubber/linux/block/blk-mq-sched.c: 165
		 * in blk_mq_dispatch_rq_list(). /Users/rubber/linux/block/blk-mq-sched.c: 166
		/* /Users/rubber/linux/block/blk-mq-sched.c: 173
		 * If we cannot get tag for the request, stop dequeueing /Users/rubber/linux/block/blk-mq-sched.c: 174
		 * requests from the IO scheduler. We are unlikely to be able /Users/rubber/linux/block/blk-mq-sched.c: 175
		 * to submit them anyway and it creates false impression for /Users/rubber/linux/block/blk-mq-sched.c: 176
		 * scheduling heuristics that the device can take more IO. /Users/rubber/linux/block/blk-mq-sched.c: 177
		/* /Users/rubber/linux/block/blk-mq-sched.c: 187
		 * Requests from different hctx may be dequeued from some /Users/rubber/linux/block/blk-mq-sched.c: 188
		 * schedulers, such as bfq and deadline. /Users/rubber/linux/block/blk-mq-sched.c: 189
		 * /Users/rubber/linux/block/blk-mq-sched.c: 190
		 * Sort the requests in the list according to their hctx, /Users/rubber/linux/block/blk-mq-sched.c: 191
		 * dispatch batching requests from same hctx at a time. /Users/rubber/linux/block/blk-mq-sched.c: 192
 * Only SCSI implements .get_budget and .put_budget, and SCSI restarts /Users/rubber/linux/block/blk-mq-sched.c: 230
 * its queue by itself in its completion handler, so we don't need to /Users/rubber/linux/block/blk-mq-sched.c: 231
 * restart queue if .get_budget() returns BLK_STS_NO_RESOURCE. /Users/rubber/linux/block/blk-mq-sched.c: 232
 * Returns -EAGAIN if hctx->dispatch was found non-empty and run_work has to /Users/rubber/linux/block/blk-mq-sched.c: 234
 * be run again.  This is necessary to avoid starving flushes. /Users/rubber/linux/block/blk-mq-sched.c: 235
			/* /Users/rubber/linux/block/blk-mq-sched.c: 263
			 * We're releasing without dispatching. Holding the /Users/rubber/linux/block/blk-mq-sched.c: 264
			 * budget could have blocked any "hctx"s with the /Users/rubber/linux/block/blk-mq-sched.c: 265
			 * same queue and if we didn't dispatch then there's /Users/rubber/linux/block/blk-mq-sched.c: 266
			 * no guarantee anyone will kick the queue.  Kick it /Users/rubber/linux/block/blk-mq-sched.c: 267
			 * ourselves. /Users/rubber/linux/block/blk-mq-sched.c: 268
		/* /Users/rubber/linux/block/blk-mq-sched.c: 276
		 * Now this rq owns the budget which has to be released /Users/rubber/linux/block/blk-mq-sched.c: 277
		 * if this rq won't be queued to driver via .queue_rq() /Users/rubber/linux/block/blk-mq-sched.c: 278
		 * in blk_mq_dispatch_rq_list(). /Users/rubber/linux/block/blk-mq-sched.c: 279
	/* /Users/rubber/linux/block/blk-mq-sched.c: 299
	 * If we have previous entries on our dispatch list, grab them first for /Users/rubber/linux/block/blk-mq-sched.c: 300
	 * more fair dispatch. /Users/rubber/linux/block/blk-mq-sched.c: 301
	/* /Users/rubber/linux/block/blk-mq-sched.c: 310
	 * Only ask the scheduler for requests, if we didn't have residual /Users/rubber/linux/block/blk-mq-sched.c: 311
	 * requests from the dispatch list. This is to avoid the case where /Users/rubber/linux/block/blk-mq-sched.c: 312
	 * we only ever dispatch a fraction of the requests available because /Users/rubber/linux/block/blk-mq-sched.c: 313
	 * of low device queue depth. Once we pull requests out of the IO /Users/rubber/linux/block/blk-mq-sched.c: 314
	 * scheduler, we can no longer merge or sort them. So it's best to /Users/rubber/linux/block/blk-mq-sched.c: 315
	 * leave them there for as long as we can. Mark the hw queue as /Users/rubber/linux/block/blk-mq-sched.c: 316
	 * needing a restart in that case. /Users/rubber/linux/block/blk-mq-sched.c: 317
	 * /Users/rubber/linux/block/blk-mq-sched.c: 318
	 * We want to dispatch from the scheduler if there was nothing /Users/rubber/linux/block/blk-mq-sched.c: 319
	 * on the dispatch list or we were able to dispatch from the /Users/rubber/linux/block/blk-mq-sched.c: 320
	 * dispatch list. /Users/rubber/linux/block/blk-mq-sched.c: 321
	/* /Users/rubber/linux/block/blk-mq-sched.c: 354
	 * A return of -EAGAIN is an indication that hctx->dispatch is not /Users/rubber/linux/block/blk-mq-sched.c: 355
	 * empty and we must run again in order to avoid starving flushes. /Users/rubber/linux/block/blk-mq-sched.c: 356
	/* /Users/rubber/linux/block/blk-mq-sched.c: 387
	 * Reverse check our software queue for entries that we could /Users/rubber/linux/block/blk-mq-sched.c: 388
	 * potentially merge with. Currently includes a hand-wavy stop /Users/rubber/linux/block/blk-mq-sched.c: 389
	 * count of 8, to not spend too much time checking for merges. /Users/rubber/linux/block/blk-mq-sched.c: 390
	/* /Users/rubber/linux/block/blk-mq-sched.c: 410
	 * dispatch flush and passthrough rq directly /Users/rubber/linux/block/blk-mq-sched.c: 411
	 * /Users/rubber/linux/block/blk-mq-sched.c: 412
	 * passthrough request has to be added to hctx->dispatch directly. /Users/rubber/linux/block/blk-mq-sched.c: 413
	 * For some reason, device may be in one situation which can't /Users/rubber/linux/block/blk-mq-sched.c: 414
	 * handle FS request, so STS_RESOURCE is always returned and the /Users/rubber/linux/block/blk-mq-sched.c: 415
	 * FS request will be added to hctx->dispatch. However passthrough /Users/rubber/linux/block/blk-mq-sched.c: 416
	 * request may be required at that time for fixing the problem. If /Users/rubber/linux/block/blk-mq-sched.c: 417
	 * passthrough request is added to scheduler queue, there isn't any /Users/rubber/linux/block/blk-mq-sched.c: 418
	 * chance to dispatch it given we prioritize requests in hctx->dispatch. /Users/rubber/linux/block/blk-mq-sched.c: 419
		/* /Users/rubber/linux/block/blk-mq-sched.c: 438
		 * Firstly normal IO request is inserted to scheduler queue or /Users/rubber/linux/block/blk-mq-sched.c: 439
		 * sw queue, meantime we add flush request to dispatch queue( /Users/rubber/linux/block/blk-mq-sched.c: 440
		 * hctx->dispatch) directly and there is at most one in-flight /Users/rubber/linux/block/blk-mq-sched.c: 441
		 * flush request for each hw queue, so it doesn't matter to add /Users/rubber/linux/block/blk-mq-sched.c: 442
		 * flush request to tail or front of the dispatch queue. /Users/rubber/linux/block/blk-mq-sched.c: 443
		 * /Users/rubber/linux/block/blk-mq-sched.c: 444
		 * Secondly in case of NCQ, flush request belongs to non-NCQ /Users/rubber/linux/block/blk-mq-sched.c: 445
		 * command, and queueing it will fail when there is any /Users/rubber/linux/block/blk-mq-sched.c: 446
		 * in-flight normal IO request(NCQ command). When adding flush /Users/rubber/linux/block/blk-mq-sched.c: 447
		 * rq to the front of hctx->dispatch, it is easier to introduce /Users/rubber/linux/block/blk-mq-sched.c: 448
		 * extra time to flush rq's latency because of S_SCHED_RESTART /Users/rubber/linux/block/blk-mq-sched.c: 449
		 * compared with adding to the tail of dispatch queue, then /Users/rubber/linux/block/blk-mq-sched.c: 450
		 * chance of flush merge is increased, and less flush requests /Users/rubber/linux/block/blk-mq-sched.c: 451
		 * will be issued to controller. It is observed that ~10% time /Users/rubber/linux/block/blk-mq-sched.c: 452
		 * is saved in blktests block/004 on disk attached to AHCI/NCQ /Users/rubber/linux/block/blk-mq-sched.c: 453
		 * drive when adding flush rq to the front of hctx->dispatch. /Users/rubber/linux/block/blk-mq-sched.c: 454
		 * /Users/rubber/linux/block/blk-mq-sched.c: 455
		 * Simply queue flush rq to the front of hctx->dispatch so that /Users/rubber/linux/block/blk-mq-sched.c: 456
		 * intensive flush workloads can benefit in case of NCQ HW. /Users/rubber/linux/block/blk-mq-sched.c: 457
	/* /Users/rubber/linux/block/blk-mq-sched.c: 487
	 * blk_mq_sched_insert_requests() is called from flush plug /Users/rubber/linux/block/blk-mq-sched.c: 488
	 * context only, and hold one usage counter to prevent queue /Users/rubber/linux/block/blk-mq-sched.c: 489
	 * from being released. /Users/rubber/linux/block/blk-mq-sched.c: 490
		/* /Users/rubber/linux/block/blk-mq-sched.c: 498
		 * try to issue requests directly if the hw queue isn't /Users/rubber/linux/block/blk-mq-sched.c: 499
		 * busy in case of 'none' scheduler, and this way may save /Users/rubber/linux/block/blk-mq-sched.c: 500
		 * us one extra enqueue & dequeue to sw queue. /Users/rubber/linux/block/blk-mq-sched.c: 501
	/* /Users/rubber/linux/block/blk-mq-sched.c: 561
	 * Set initial depth at max so that we don't need to reallocate for /Users/rubber/linux/block/blk-mq-sched.c: 562
	 * updating nr_requests. /Users/rubber/linux/block/blk-mq-sched.c: 563
	/* /Users/rubber/linux/block/blk-mq-sched.c: 589
	 * Default to double of smaller one between hw queue_depth and 128, /Users/rubber/linux/block/blk-mq-sched.c: 590
	 * since we don't split into sync/async like the old code did. /Users/rubber/linux/block/blk-mq-sched.c: 591
	 * Additionally, this is a per-hw queue depth. /Users/rubber/linux/block/blk-mq-sched.c: 592
 * called in either blk_queue_cleanup or elevator_switch, tagset /Users/rubber/linux/block/blk-mq-sched.c: 640
 * is required for freeing requests /Users/rubber/linux/block/blk-mq-sched.c: 641
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-exec.c: 1
 * Functions related to setting various queue properties from drivers /Users/rubber/linux/block/blk-exec.c: 3
 * blk_end_sync_rq - executes a completion event on a request /Users/rubber/linux/block/blk-exec.c: 16
 * @rq: request to complete /Users/rubber/linux/block/blk-exec.c: 17
 * @error: end I/O status of the request /Users/rubber/linux/block/blk-exec.c: 18
	/* /Users/rubber/linux/block/blk-exec.c: 26
	 * complete last, if this is a stack request the process (and thus /Users/rubber/linux/block/blk-exec.c: 27
	 * the rq pointer) could be invalid right after this complete() /Users/rubber/linux/block/blk-exec.c: 28
 * blk_execute_rq_nowait - insert a request to I/O scheduler for execution /Users/rubber/linux/block/blk-exec.c: 34
 * @bd_disk:	matching gendisk /Users/rubber/linux/block/blk-exec.c: 35
 * @rq:		request to insert /Users/rubber/linux/block/blk-exec.c: 36
 * @at_head:    insert request at head or tail of queue /Users/rubber/linux/block/blk-exec.c: 37
 * @done:	I/O completion handler /Users/rubber/linux/block/blk-exec.c: 38
 * Description: /Users/rubber/linux/block/blk-exec.c: 40
 *    Insert a fully prepared request at the back of the I/O scheduler queue /Users/rubber/linux/block/blk-exec.c: 41
 *    for execution.  Don't wait for completion. /Users/rubber/linux/block/blk-exec.c: 42
 * Note: /Users/rubber/linux/block/blk-exec.c: 44
 *    This function will invoke @done directly if the queue is dead. /Users/rubber/linux/block/blk-exec.c: 45
	/* /Users/rubber/linux/block/blk-exec.c: 58
	 * don't check dying flag for MQ because the request won't /Users/rubber/linux/block/blk-exec.c: 59
	 * be reused after dying flag is set /Users/rubber/linux/block/blk-exec.c: 60
 * blk_execute_rq - insert a request into queue for execution /Users/rubber/linux/block/blk-exec.c: 86
 * @bd_disk:	matching gendisk /Users/rubber/linux/block/blk-exec.c: 87
 * @rq:		request to insert /Users/rubber/linux/block/blk-exec.c: 88
 * @at_head:    insert request at head or tail of queue /Users/rubber/linux/block/blk-exec.c: 89
 * Description: /Users/rubber/linux/block/blk-exec.c: 91
 *    Insert a fully prepared request at the back of the I/O scheduler queue /Users/rubber/linux/block/blk-exec.c: 92
 *    for execution and wait for completion. /Users/rubber/linux/block/blk-exec.c: 93
 * Return: The blk_status_t result provided to blk_mq_end_request(). /Users/rubber/linux/block/blk-exec.c: 94
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/sed-opal.c: 1
 * Copyright © 2016 Intel Corporation /Users/rubber/linux/block/sed-opal.c: 3
 * Authors: /Users/rubber/linux/block/sed-opal.c: 5
 *    Scott  Bauer      <scott.bauer@intel.com> /Users/rubber/linux/block/sed-opal.c: 6
 *    Rafael Antognolli <rafael.antognolli@intel.com> /Users/rubber/linux/block/sed-opal.c: 7
 * On the parsed response, we don't store again the toks that are already /Users/rubber/linux/block/sed-opal.c: 47
 * stored in the response buffer. Instead, for each token, we just store a /Users/rubber/linux/block/sed-opal.c: 48
 * pointer to the position in the buffer where the token starts, and the size /Users/rubber/linux/block/sed-opal.c: 49
 * of the token in bytes. /Users/rubber/linux/block/sed-opal.c: 50
 * From the response header it's not possible to know how many tokens there are /Users/rubber/linux/block/sed-opal.c: 64
 * on the payload. So we hardcode that the maximum will be MAX_TOKS, and later /Users/rubber/linux/block/sed-opal.c: 65
 * if we start dealing with messages that have more than that, we can increase /Users/rubber/linux/block/sed-opal.c: 66
 * this number. This is done to avoid having to make two passes through the /Users/rubber/linux/block/sed-opal.c: 67
 * response, the first one counting how many tokens we have and the second one /Users/rubber/linux/block/sed-opal.c: 68
 * actually storing the positions. /Users/rubber/linux/block/sed-opal.c: 69
 * TCG Storage SSC Methods. /Users/rubber/linux/block/sed-opal.c: 175
 * Derived from: TCG_Storage_Architecture_Core_Spec_v2.01_r1.00 /Users/rubber/linux/block/sed-opal.c: 176
 * Section: 6.3 Assigned UIDs /Users/rubber/linux/block/sed-opal.c: 177
 * Derived from: /Users/rubber/linux/block/sed-opal.c: 224
 * TCG_Storage_Architecture_Core_Spec_v2.01_r1.00 /Users/rubber/linux/block/sed-opal.c: 225
 * Section: 5.1.5 Method Status Codes /Users/rubber/linux/block/sed-opal.c: 226
	/* /Users/rubber/linux/block/sed-opal.c: 415
	 * For each OPAL command the first step in steps starts some sort of /Users/rubber/linux/block/sed-opal.c: 416
	 * session. If an error occurred in the initial discovery0 or if an /Users/rubber/linux/block/sed-opal.c: 417
	 * error occurred in the first step (and thus stopping the loop with /Users/rubber/linux/block/sed-opal.c: 418
	 * state == 0) then there was an error before or during the attempt to /Users/rubber/linux/block/sed-opal.c: 419
	 * start a session. Therefore we shouldn't attempt to terminate a /Users/rubber/linux/block/sed-opal.c: 420
	 * session, as one has not yet been created. /Users/rubber/linux/block/sed-opal.c: 421
	/* /Users/rubber/linux/block/sed-opal.c: 691
	 * Close the parameter list opened from cmd_start. /Users/rubber/linux/block/sed-opal.c: 692
	 * The number of bytes added must be equal to /Users/rubber/linux/block/sed-opal.c: 693
	 * CMD_FINALIZE_BYTES_NEEDED. /Users/rubber/linux/block/sed-opal.c: 694
	/* /Users/rubber/linux/block/sed-opal.c: 1037
	 * Every method call is followed by its parameters enclosed within /Users/rubber/linux/block/sed-opal.c: 1038
	 * OPAL_STARTLIST and OPAL_ENDLIST tokens. We automatically open the /Users/rubber/linux/block/sed-opal.c: 1039
	 * parameter list here and close it later in cmd_finalize. /Users/rubber/linux/block/sed-opal.c: 1040
 * request @column from table @table on device @dev. On success, the column /Users/rubber/linux/block/sed-opal.c: 1109
 * data will be available in dev->resp->tok[4] /Users/rubber/linux/block/sed-opal.c: 1110
 * see TCG SAS 5.3.2.3 for a description of the available columns /Users/rubber/linux/block/sed-opal.c: 1140
 * the result is provided in dev->resp->tok[4] /Users/rubber/linux/block/sed-opal.c: 1142
	/* sed-opal UIDs can be split in two halves: /Users/rubber/linux/block/sed-opal.c: 1150
	 *  first:  actual table index /Users/rubber/linux/block/sed-opal.c: 1151
	 *  second: relative index in the table /Users/rubber/linux/block/sed-opal.c: 1152
	 * so we have to get the first half of the OPAL_TABLE_TABLE and use the /Users/rubber/linux/block/sed-opal.c: 1153
	 * first part of the target table as relative index into that table /Users/rubber/linux/block/sed-opal.c: 1154
		/* /Users/rubber/linux/block/sed-opal.c: 1260
		 * The bytestring header is either 1 or 2 bytes, so assume 2. /Users/rubber/linux/block/sed-opal.c: 1261
		 * There also needs to be enough space to accommodate the /Users/rubber/linux/block/sed-opal.c: 1262
		 * trailing OPAL_ENDNAME (1 byte) and tokens added by /Users/rubber/linux/block/sed-opal.c: 1263
		 * cmd_finalize. /Users/rubber/linux/block/sed-opal.c: 1264
 * IO_BUFFER_LENGTH = 2048 /Users/rubber/linux/block/sed-opal.c: 1996
 * sizeof(header) = 56 /Users/rubber/linux/block/sed-opal.c: 1997
 * No. of Token Bytes in the Response = 11 /Users/rubber/linux/block/sed-opal.c: 1998
 * MAX size of data that can be carried in response buffer /Users/rubber/linux/block/sed-opal.c: 1999
 * at a time is : 2048 - (56 + 11) = 1981 = 0x7BD. /Users/rubber/linux/block/sed-opal.c: 2000
		add_token_u64(&err, dev, offset + off + len); /* end row value /Users/rubber/linux/block/sed-opal.c: 2041
	/* /Users/rubber/linux/block/sed-opal.c: 2354
	 * If we successfully reverted lets clean /Users/rubber/linux/block/sed-opal.c: 2355
	 * any saved locking ranges. /Users/rubber/linux/block/sed-opal.c: 2356
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/block/fops.c: 1
 * Copyright (C) 1991, 1992  Linus Torvalds /Users/rubber/linux/block/fops.c: 3
 * Copyright (C) 2001  Andrea Arcangeli <andrea@suse.de> SuSE /Users/rubber/linux/block/fops.c: 4
 * Copyright (C) 2016 - 2020 Christoph Hellwig /Users/rubber/linux/block/fops.c: 5
	/* /Users/rubber/linux/block/fops.c: 203
	 * Grab an extra reference to ensure the dio structure which is embedded /Users/rubber/linux/block/fops.c: 204
	 * into the first bio stays around. /Users/rubber/linux/block/fops.c: 205
		/* /Users/rubber/linux/block/fops.c: 333
		 * Users don't rely on the iterator being in any particular /Users/rubber/linux/block/fops.c: 334
		 * state for async I/O returning -EIOCBQUEUED, hence we can /Users/rubber/linux/block/fops.c: 335
		 * avoid expensive iov_iter_advance(). Bypass /Users/rubber/linux/block/fops.c: 336
		 * bio_iov_iter_get_pages() and set the bvec directly. /Users/rubber/linux/block/fops.c: 337
 * for a block special file file_inode(file)->i_size is zero /Users/rubber/linux/block/fops.c: 445
 * so we compute the size by hand (just as in block_read/write above) /Users/rubber/linux/block/fops.c: 446
	/* /Users/rubber/linux/block/fops.c: 469
	 * There is no need to serialise calls to blkdev_issue_flush with /Users/rubber/linux/block/fops.c: 470
	 * i_mutex and doing so causes performance issues with concurrent /Users/rubber/linux/block/fops.c: 471
	 * O_SYNC writers to a block device. /Users/rubber/linux/block/fops.c: 472
	/* /Users/rubber/linux/block/fops.c: 485
	 * Preserve backwards compatibility and allow large file access /Users/rubber/linux/block/fops.c: 486
	 * even if userspace doesn't ask for it explicitly. Some mkfs /Users/rubber/linux/block/fops.c: 487
	 * binary needs it. We might want to drop this workaround /Users/rubber/linux/block/fops.c: 488
	 * during an unstable branch. /Users/rubber/linux/block/fops.c: 489
 * Write data to the block device.  Only intended for the block device itself /Users/rubber/linux/block/fops.c: 520
 * and the raw driver which basically is a fake block device. /Users/rubber/linux/block/fops.c: 521
 * Does not take i_mutex for the write and thus is not for general purpose /Users/rubber/linux/block/fops.c: 523
 * use. /Users/rubber/linux/block/fops.c: 524
	/* /Users/rubber/linux/block/fops.c: 619
	 * Don't allow IO that isn't aligned to logical block size. /Users/rubber/linux/block/fops.c: 620
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-sysfs.c: 1
 * Functions related to sysfs handling /Users/rubber/linux/block/blk-sysfs.c: 3
	/* /Users/rubber/linux/block/blk-sysfs.c: 499
	 * Ensure that the queue is idled, in case the latency update /Users/rubber/linux/block/blk-sysfs.c: 500
	 * ends up either enabling or disabling wbt completely. We can't /Users/rubber/linux/block/blk-sysfs.c: 501
	 * have IO inflight if that happens. /Users/rubber/linux/block/blk-sysfs.c: 502
	/* /Users/rubber/linux/block/blk-sysfs.c: 743
	 * Since the I/O scheduler exit code may access cgroup information, /Users/rubber/linux/block/blk-sysfs.c: 744
	 * perform I/O scheduler exit before disassociating from the block /Users/rubber/linux/block/blk-sysfs.c: 745
	 * cgroup controller. /Users/rubber/linux/block/blk-sysfs.c: 746
	/* /Users/rubber/linux/block/blk-sysfs.c: 753
	 * Remove all references to @q from the block cgroup controller before /Users/rubber/linux/block/blk-sysfs.c: 754
	 * restoring @q->queue_lock to avoid that restoring this pointer causes /Users/rubber/linux/block/blk-sysfs.c: 755
	 * e.g. blkcg_print_blkgs() to crash. /Users/rubber/linux/block/blk-sysfs.c: 756
 * blk_release_queue - releases all allocated resources of the request_queue /Users/rubber/linux/block/blk-sysfs.c: 762
 * @kobj: pointer to a kobject, whose container is a request_queue /Users/rubber/linux/block/blk-sysfs.c: 763
 * This function releases all allocated resources of the request queue. /Users/rubber/linux/block/blk-sysfs.c: 765
 * The struct request_queue refcount is incremented with blk_get_queue() and /Users/rubber/linux/block/blk-sysfs.c: 767
 * decremented with blk_put_queue(). Once the refcount reaches 0 this function /Users/rubber/linux/block/blk-sysfs.c: 768
 * is called. /Users/rubber/linux/block/blk-sysfs.c: 769
 * For drivers that have a request_queue on a gendisk and added with /Users/rubber/linux/block/blk-sysfs.c: 771
 * __device_add_disk() the refcount to request_queue will reach 0 with /Users/rubber/linux/block/blk-sysfs.c: 772
 * the last put_disk() called by the driver. For drivers which don't use /Users/rubber/linux/block/blk-sysfs.c: 773
 * __device_add_disk() this happens with blk_cleanup_queue(). /Users/rubber/linux/block/blk-sysfs.c: 774
 * Drivers exist which depend on the release of the request_queue to be /Users/rubber/linux/block/blk-sysfs.c: 776
 * synchronous, it should not be deferred. /Users/rubber/linux/block/blk-sysfs.c: 777
 * Context: can sleep /Users/rubber/linux/block/blk-sysfs.c: 779
 * blk_register_queue - register a block layer queue with sysfs /Users/rubber/linux/block/blk-sysfs.c: 826
 * @disk: Disk of which the request queue should be registered with sysfs. /Users/rubber/linux/block/blk-sysfs.c: 827
	/* /Users/rubber/linux/block/blk-sysfs.c: 891
	 * SCSI probing may synchronously create and destroy a lot of /Users/rubber/linux/block/blk-sysfs.c: 892
	 * request_queues for non-existent devices.  Shutting down a fully /Users/rubber/linux/block/blk-sysfs.c: 893
	 * functional queue takes measureable wallclock time as RCU grace /Users/rubber/linux/block/blk-sysfs.c: 894
	 * periods are involved.  To avoid excessive latency in these /Users/rubber/linux/block/blk-sysfs.c: 895
	 * cases, a request_queue starts out in a degraded mode which is /Users/rubber/linux/block/blk-sysfs.c: 896
	 * faster to shut down and is made fully functional here as /Users/rubber/linux/block/blk-sysfs.c: 897
	 * request_queues for non-existent devices never get registered. /Users/rubber/linux/block/blk-sysfs.c: 898
 * blk_unregister_queue - counterpart of blk_register_queue() /Users/rubber/linux/block/blk-sysfs.c: 919
 * @disk: Disk of which the request queue should be unregistered from sysfs. /Users/rubber/linux/block/blk-sysfs.c: 920
 * Note: the caller is responsible for guaranteeing that this function is called /Users/rubber/linux/block/blk-sysfs.c: 922
 * after blk_register_queue() has finished. /Users/rubber/linux/block/blk-sysfs.c: 923
	/* /Users/rubber/linux/block/blk-sysfs.c: 936
	 * Since sysfs_remove_dir() prevents adding new directory entries /Users/rubber/linux/block/blk-sysfs.c: 937
	 * before removal of existing entries starts, protect against /Users/rubber/linux/block/blk-sysfs.c: 938
	 * concurrent elv_iosched_store() calls. /Users/rubber/linux/block/blk-sysfs.c: 939
	/* /Users/rubber/linux/block/blk-sysfs.c: 946
	 * Remove the sysfs attributes before unregistering the queue data /Users/rubber/linux/block/blk-sysfs.c: 947
	 * structures that can be modified through sysfs. /Users/rubber/linux/block/blk-sysfs.c: 948
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/mq-deadline.c: 1
 *  MQ Deadline i/o scheduler - adaptation of the legacy deadline scheduler, /Users/rubber/linux/block/mq-deadline.c: 3
 *  for the blk-mq scheduling framework /Users/rubber/linux/block/mq-deadline.c: 4
 *  Copyright (C) 2016 Jens Axboe <axboe@kernel.dk> /Users/rubber/linux/block/mq-deadline.c: 6
 * See Documentation/block/deadline-iosched.rst /Users/rubber/linux/block/mq-deadline.c: 30
 * Time after which to dispatch lower priority requests even if higher /Users/rubber/linux/block/mq-deadline.c: 35
 * priority requests are pending. /Users/rubber/linux/block/mq-deadline.c: 36
static const int fifo_batch = 16;       /* # of sequential requests treated as one /Users/rubber/linux/block/mq-deadline.c: 40
 * I/O statistics per I/O priority. It is fine if these counters overflow. /Users/rubber/linux/block/mq-deadline.c: 60
 * What matters is that these counters are at least as wide as /Users/rubber/linux/block/mq-deadline.c: 61
 * log2(max_outstanding_requests). /Users/rubber/linux/block/mq-deadline.c: 62
 * Deadline scheduler data per I/O priority (enum dd_prio). Requests are /Users/rubber/linux/block/mq-deadline.c: 72
 * present on both sort_list[] and fifo_list[]. /Users/rubber/linux/block/mq-deadline.c: 73
	/* /Users/rubber/linux/block/mq-deadline.c: 85
	 * run time data /Users/rubber/linux/block/mq-deadline.c: 86
	/* /Users/rubber/linux/block/mq-deadline.c: 96
	 * settings that change how the i/o scheduler behaves /Users/rubber/linux/block/mq-deadline.c: 97
 * Returns the I/O priority class (IOPRIO_CLASS_*) that has been assigned to a /Users/rubber/linux/block/mq-deadline.c: 125
 * request. /Users/rubber/linux/block/mq-deadline.c: 126
 * get the request after `rq' in sector-sorted order /Users/rubber/linux/block/mq-deadline.c: 134
 * remove rq from rbtree and fifo. /Users/rubber/linux/block/mq-deadline.c: 167
	/* /Users/rubber/linux/block/mq-deadline.c: 175
	 * We might not be on the rbtree, if we are doing an insert merge /Users/rubber/linux/block/mq-deadline.c: 176
	/* /Users/rubber/linux/block/mq-deadline.c: 194
	 * if the merge was a front merge, we need to reposition request /Users/rubber/linux/block/mq-deadline.c: 195
 * Callback function that is invoked after @next has been merged into @req. /Users/rubber/linux/block/mq-deadline.c: 204
	/* /Users/rubber/linux/block/mq-deadline.c: 217
	 * if next expires before rq, assign its expire time to rq /Users/rubber/linux/block/mq-deadline.c: 218
	 * and move into next position (next will be deleted) in fifo /Users/rubber/linux/block/mq-deadline.c: 219
	/* /Users/rubber/linux/block/mq-deadline.c: 229
	 * kill knowledge of next, this one is a goner /Users/rubber/linux/block/mq-deadline.c: 230
 * move an entry to dispatch queue /Users/rubber/linux/block/mq-deadline.c: 236
	/* /Users/rubber/linux/block/mq-deadline.c: 246
	 * take it off the sort and fifo list /Users/rubber/linux/block/mq-deadline.c: 247
 * deadline_check_fifo returns 0 if there are no expired requests on the fifo, /Users/rubber/linux/block/mq-deadline.c: 263
 * 1 otherwise. Requires !list_empty(&dd->fifo_list[data_dir]) /Users/rubber/linux/block/mq-deadline.c: 264
	/* /Users/rubber/linux/block/mq-deadline.c: 271
	 * rq is expired! /Users/rubber/linux/block/mq-deadline.c: 272
 * For the specified data direction, return the next request to /Users/rubber/linux/block/mq-deadline.c: 281
 * dispatch using arrival ordered lists. /Users/rubber/linux/block/mq-deadline.c: 282
	/* /Users/rubber/linux/block/mq-deadline.c: 298
	 * Look for a write request that can be dispatched, that is one with /Users/rubber/linux/block/mq-deadline.c: 299
	 * an unlocked target zone. /Users/rubber/linux/block/mq-deadline.c: 300
 * For the specified data direction, return the next request to /Users/rubber/linux/block/mq-deadline.c: 315
 * dispatch using sector position sorted lists. /Users/rubber/linux/block/mq-deadline.c: 316
	/* /Users/rubber/linux/block/mq-deadline.c: 332
	 * Look for a write request that can be dispatched, that is one with /Users/rubber/linux/block/mq-deadline.c: 333
	 * an unlocked target zone. /Users/rubber/linux/block/mq-deadline.c: 334
 * Returns true if and only if @rq started after @latest_start where /Users/rubber/linux/block/mq-deadline.c: 348
 * @latest_start is in jiffies. /Users/rubber/linux/block/mq-deadline.c: 349
 * deadline_dispatch_requests selects the best request according to /Users/rubber/linux/block/mq-deadline.c: 362
 * read/write expire, fifo_batch, etc and with a start time <= @latest_start. /Users/rubber/linux/block/mq-deadline.c: 363
	/* /Users/rubber/linux/block/mq-deadline.c: 385
	 * batches are currently reads XOR writes /Users/rubber/linux/block/mq-deadline.c: 386
	/* /Users/rubber/linux/block/mq-deadline.c: 393
	 * at this point we are not running a batch. select the appropriate /Users/rubber/linux/block/mq-deadline.c: 394
	 * data direction (read / write) /Users/rubber/linux/block/mq-deadline.c: 395
	/* /Users/rubber/linux/block/mq-deadline.c: 410
	 * there are either no reads or writes have been starved /Users/rubber/linux/block/mq-deadline.c: 411
	/* /Users/rubber/linux/block/mq-deadline.c: 428
	 * we are not running a batch, find best request for selected data_dir /Users/rubber/linux/block/mq-deadline.c: 429
		/* /Users/rubber/linux/block/mq-deadline.c: 433
		 * A deadline has expired, the last request was in the other /Users/rubber/linux/block/mq-deadline.c: 434
		 * direction, or we have run out of higher-sectored requests. /Users/rubber/linux/block/mq-deadline.c: 435
		 * Start again from the request with the earliest expiry time. /Users/rubber/linux/block/mq-deadline.c: 436
		/* /Users/rubber/linux/block/mq-deadline.c: 440
		 * The last req was the same dir and we have a next request in /Users/rubber/linux/block/mq-deadline.c: 441
		 * sort order. No expired requests so continue on from here. /Users/rubber/linux/block/mq-deadline.c: 442
	/* /Users/rubber/linux/block/mq-deadline.c: 447
	 * For a zoned block device, if we only have writes queued and none of /Users/rubber/linux/block/mq-deadline.c: 448
	 * them can be dispatched, rq will be NULL. /Users/rubber/linux/block/mq-deadline.c: 449
	/* /Users/rubber/linux/block/mq-deadline.c: 461
	 * rq is the selected appropriate request. /Users/rubber/linux/block/mq-deadline.c: 462
	/* /Users/rubber/linux/block/mq-deadline.c: 470
	 * If the request needs its target zone locked, do it. /Users/rubber/linux/block/mq-deadline.c: 471
 * Check whether there are any requests with priority other than DD_RT_PRIO /Users/rubber/linux/block/mq-deadline.c: 479
 * that were inserted more than prio_aging_expire jiffies ago. /Users/rubber/linux/block/mq-deadline.c: 480
 * Called from blk_mq_run_hw_queue() -> __blk_mq_sched_dispatch_requests(). /Users/rubber/linux/block/mq-deadline.c: 507
 * One confusing aspect here is that we get called for a specific /Users/rubber/linux/block/mq-deadline.c: 509
 * hardware queue, but we may return a request that is for a /Users/rubber/linux/block/mq-deadline.c: 510
 * different hardware queue. This is because mq-deadline has shared /Users/rubber/linux/block/mq-deadline.c: 511
 * state for all hardware queues, in terms of sorting, FIFOs, etc. /Users/rubber/linux/block/mq-deadline.c: 512
	/* /Users/rubber/linux/block/mq-deadline.c: 526
	 * Next, dispatch requests in priority order. Ignore lower priority /Users/rubber/linux/block/mq-deadline.c: 527
	 * requests if any higher priority requests are pending. /Users/rubber/linux/block/mq-deadline.c: 528
 * Called by __blk_mq_alloc_request(). The shallow_depth value set by this /Users/rubber/linux/block/mq-deadline.c: 543
 * function is used by __blk_mq_get_tag(). /Users/rubber/linux/block/mq-deadline.c: 544
	/* /Users/rubber/linux/block/mq-deadline.c: 554
	 * Throttle asynchronous requests and writes such that these requests /Users/rubber/linux/block/mq-deadline.c: 555
	 * do not block the allocation of synchronous requests. /Users/rubber/linux/block/mq-deadline.c: 556
 * initialize elevator private data (deadline_data). /Users/rubber/linux/block/mq-deadline.c: 607
 * Try to merge @bio into an existing request. If @bio has been merged into /Users/rubber/linux/block/mq-deadline.c: 654
 * an existing request, store the pointer to that request into *@rq. /Users/rubber/linux/block/mq-deadline.c: 655
 * Attempt to merge a bio into an existing request. This function is called /Users/rubber/linux/block/mq-deadline.c: 686
 * before @bio is associated with a request. /Users/rubber/linux/block/mq-deadline.c: 687
 * add rq to rbtree and fifo /Users/rubber/linux/block/mq-deadline.c: 707
	/* /Users/rubber/linux/block/mq-deadline.c: 723
	 * This may be a requeue of a write request that has locked its /Users/rubber/linux/block/mq-deadline.c: 724
	 * target zone. If it is the case, this releases the zone lock. /Users/rubber/linux/block/mq-deadline.c: 725
		/* /Users/rubber/linux/block/mq-deadline.c: 754
		 * set expire time and add to fifo list /Users/rubber/linux/block/mq-deadline.c: 755
 * Called from blk_mq_sched_insert_request() or blk_mq_sched_insert_requests(). /Users/rubber/linux/block/mq-deadline.c: 763
 * Callback from inside blk_mq_free_request(). /Users/rubber/linux/block/mq-deadline.c: 789
 * For zoned block devices, write unlock the target zone of /Users/rubber/linux/block/mq-deadline.c: 791
 * completed write requests. Do this while holding the zone lock /Users/rubber/linux/block/mq-deadline.c: 792
 * spinlock so that the zone is never unlocked while deadline_fifo_request() /Users/rubber/linux/block/mq-deadline.c: 793
 * or deadline_next_request() are executing. This function is called for /Users/rubber/linux/block/mq-deadline.c: 794
 * all requests, whether or not these requests complete successfully. /Users/rubber/linux/block/mq-deadline.c: 795
 * For a zoned block device, __dd_dispatch_request() may have stopped /Users/rubber/linux/block/mq-deadline.c: 797
 * dispatching requests if all the queued requests are write requests directed /Users/rubber/linux/block/mq-deadline.c: 798
 * at zones that are already locked due to on-going write requests. To ensure /Users/rubber/linux/block/mq-deadline.c: 799
 * write request dispatch progress in this case, mark the queue as needing a /Users/rubber/linux/block/mq-deadline.c: 800
 * restart to ensure that the queue is run again after completion of the /Users/rubber/linux/block/mq-deadline.c: 801
 * request and zones being unlocked. /Users/rubber/linux/block/mq-deadline.c: 802
	/* /Users/rubber/linux/block/mq-deadline.c: 812
	 * The block layer core may call dd_finish_request() without having /Users/rubber/linux/block/mq-deadline.c: 813
	 * called dd_insert_requests(). Skip requests that bypassed I/O /Users/rubber/linux/block/mq-deadline.c: 814
	 * scheduling. See also blk_mq_request_bypass_insert(). /Users/rubber/linux/block/mq-deadline.c: 815
 * sysfs parts below /Users/rubber/linux/block/mq-deadline.c: 853
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-ia-ranges.c: 1
 *  Block device concurrent positioning ranges. /Users/rubber/linux/block/blk-ia-ranges.c: 3
 *  Copyright (C) 2021 Western Digital Corporation or its Affiliates. /Users/rubber/linux/block/blk-ia-ranges.c: 5
 * Independent access range entries are not freed individually, but alltogether /Users/rubber/linux/block/blk-ia-ranges.c: 71
 * with struct blk_independent_access_ranges and its array of ranges. Since /Users/rubber/linux/block/blk-ia-ranges.c: 72
 * kobject_add() takes a reference on the parent kobject contained in /Users/rubber/linux/block/blk-ia-ranges.c: 73
 * struct blk_independent_access_ranges, the array of independent access range /Users/rubber/linux/block/blk-ia-ranges.c: 74
 * entries cannot be freed until kobject_del() is called for all entries. /Users/rubber/linux/block/blk-ia-ranges.c: 75
 * So we do not need to do anything here, but still need this no-op release /Users/rubber/linux/block/blk-ia-ranges.c: 76
 * operation to avoid complaints from the kobject code. /Users/rubber/linux/block/blk-ia-ranges.c: 77
 * This will be executed only after all independent access range entries are /Users/rubber/linux/block/blk-ia-ranges.c: 90
 * removed with kobject_del(), at which point, it is safe to free everything, /Users/rubber/linux/block/blk-ia-ranges.c: 91
 * including the array of ranges. /Users/rubber/linux/block/blk-ia-ranges.c: 92
 * disk_register_independent_access_ranges - register with sysfs a set of /Users/rubber/linux/block/blk-ia-ranges.c: 107
 *		independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 108
 * @disk:	Target disk /Users/rubber/linux/block/blk-ia-ranges.c: 109
 * @new_iars:	New set of independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 110
 * Register with sysfs a set of independent access ranges for @disk. /Users/rubber/linux/block/blk-ia-ranges.c: 112
 * If @new_iars is not NULL, this set of ranges is registered and the old set /Users/rubber/linux/block/blk-ia-ranges.c: 113
 * specified by q->ia_ranges is unregistered. Otherwise, q->ia_ranges is /Users/rubber/linux/block/blk-ia-ranges.c: 114
 * registered if it is not already. /Users/rubber/linux/block/blk-ia-ranges.c: 115
	/* /Users/rubber/linux/block/blk-ia-ranges.c: 138
	 * At this point, iars is the new set of sector access ranges that needs /Users/rubber/linux/block/blk-ia-ranges.c: 139
	 * to be registered with sysfs. /Users/rubber/linux/block/blk-ia-ranges.c: 140
	/* /Users/rubber/linux/block/blk-ia-ranges.c: 219
	 * While sorting the ranges in increasing LBA order, check that the /Users/rubber/linux/block/blk-ia-ranges.c: 220
	 * ranges do not overlap, that there are no sector holes and that all /Users/rubber/linux/block/blk-ia-ranges.c: 221
	 * sectors belong to one range. /Users/rubber/linux/block/blk-ia-ranges.c: 222
 * disk_alloc_independent_access_ranges - Allocate an independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 270
 *                                        data structure /Users/rubber/linux/block/blk-ia-ranges.c: 271
 * @disk:		target disk /Users/rubber/linux/block/blk-ia-ranges.c: 272
 * @nr_ia_ranges:	Number of independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 273
 * Allocate a struct blk_independent_access_ranges structure with @nr_ia_ranges /Users/rubber/linux/block/blk-ia-ranges.c: 275
 * access range descriptors. /Users/rubber/linux/block/blk-ia-ranges.c: 276
 * disk_set_independent_access_ranges - Set a disk independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 292
 * @disk:	target disk /Users/rubber/linux/block/blk-ia-ranges.c: 293
 * @iars:	independent access ranges structure /Users/rubber/linux/block/blk-ia-ranges.c: 294
 * Set the independent access ranges information of the request queue /Users/rubber/linux/block/blk-ia-ranges.c: 296
 * of @disk to @iars. If @iars is NULL and the independent access ranges /Users/rubber/linux/block/blk-ia-ranges.c: 297
 * structure already set is cleared. If there are no differences between /Users/rubber/linux/block/blk-ia-ranges.c: 298
 * @iars and the independent access ranges structure already set, @iars /Users/rubber/linux/block/blk-ia-ranges.c: 299
 * is freed. /Users/rubber/linux/block/blk-ia-ranges.c: 300
	/* /Users/rubber/linux/block/blk-ia-ranges.c: 328
	 * This may be called for a registered queue. E.g. during a device /Users/rubber/linux/block/blk-ia-ranges.c: 329
	 * revalidation. If that is the case, we need to unregister the old /Users/rubber/linux/block/blk-ia-ranges.c: 330
	 * set of independent access ranges and register the new set. If the /Users/rubber/linux/block/blk-ia-ranges.c: 331
	 * queue is not registered, registration of the device request queue /Users/rubber/linux/block/blk-ia-ranges.c: 332
	 * will register the independent access ranges, so only swap in the /Users/rubber/linux/block/blk-ia-ranges.c: 333
	 * new set and free the old one. /Users/rubber/linux/block/blk-ia-ranges.c: 334
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-flush.c: 1
 * Functions to sequence PREFLUSH and FUA writes. /Users/rubber/linux/block/blk-flush.c: 3
 * Copyright (C) 2011		Max Planck Institute for Gravitational Physics /Users/rubber/linux/block/blk-flush.c: 5
 * Copyright (C) 2011		Tejun Heo <tj@kernel.org> /Users/rubber/linux/block/blk-flush.c: 6
 * REQ_{PREFLUSH|FUA} requests are decomposed to sequences consisted of three /Users/rubber/linux/block/blk-flush.c: 8
 * optional steps - PREFLUSH, DATA and POSTFLUSH - according to the request /Users/rubber/linux/block/blk-flush.c: 9
 * properties and hardware capability. /Users/rubber/linux/block/blk-flush.c: 10
 * If a request doesn't have data, only REQ_PREFLUSH makes sense, which /Users/rubber/linux/block/blk-flush.c: 12
 * indicates a simple flush request.  If there is data, REQ_PREFLUSH indicates /Users/rubber/linux/block/blk-flush.c: 13
 * that the device cache should be flushed before the data is executed, and /Users/rubber/linux/block/blk-flush.c: 14
 * REQ_FUA means that the data must be on non-volatile media on request /Users/rubber/linux/block/blk-flush.c: 15
 * completion. /Users/rubber/linux/block/blk-flush.c: 16
 * If the device doesn't have writeback cache, PREFLUSH and FUA don't make any /Users/rubber/linux/block/blk-flush.c: 18
 * difference.  The requests are either completed immediately if there's no data /Users/rubber/linux/block/blk-flush.c: 19
 * or executed as normal requests otherwise. /Users/rubber/linux/block/blk-flush.c: 20
 * If the device has writeback cache and supports FUA, REQ_PREFLUSH is /Users/rubber/linux/block/blk-flush.c: 22
 * translated to PREFLUSH but REQ_FUA is passed down directly with DATA. /Users/rubber/linux/block/blk-flush.c: 23
 * If the device has writeback cache and doesn't support FUA, REQ_PREFLUSH /Users/rubber/linux/block/blk-flush.c: 25
 * is translated to PREFLUSH and REQ_FUA to POSTFLUSH. /Users/rubber/linux/block/blk-flush.c: 26
 * The actual execution of flush is double buffered.  Whenever a request /Users/rubber/linux/block/blk-flush.c: 28
 * needs to execute PRE or POSTFLUSH, it queues at /Users/rubber/linux/block/blk-flush.c: 29
 * fq->flush_queue[fq->flush_pending_idx].  Once certain criteria are met, a /Users/rubber/linux/block/blk-flush.c: 30
 * REQ_OP_FLUSH is issued and the pending_idx is toggled.  When the flush /Users/rubber/linux/block/blk-flush.c: 31
 * completes, all the requests which were pending are proceeded to the next /Users/rubber/linux/block/blk-flush.c: 32
 * step.  This allows arbitrary merging of different types of PREFLUSH/FUA /Users/rubber/linux/block/blk-flush.c: 33
 * requests. /Users/rubber/linux/block/blk-flush.c: 34
 * Currently, the following conditions are used to determine when to issue /Users/rubber/linux/block/blk-flush.c: 36
 * flush. /Users/rubber/linux/block/blk-flush.c: 37
 * C1. At any given time, only one flush shall be in progress.  This makes /Users/rubber/linux/block/blk-flush.c: 39
 *     double buffering sufficient. /Users/rubber/linux/block/blk-flush.c: 40
 * C2. Flush is deferred if any request is executing DATA of its sequence. /Users/rubber/linux/block/blk-flush.c: 42
 *     This avoids issuing separate POSTFLUSHes for requests which shared /Users/rubber/linux/block/blk-flush.c: 43
 *     PREFLUSH. /Users/rubber/linux/block/blk-flush.c: 44
 * C3. The second condition is ignored if there is a request which has /Users/rubber/linux/block/blk-flush.c: 46
 *     waited longer than FLUSH_PENDING_TIMEOUT.  This is to avoid /Users/rubber/linux/block/blk-flush.c: 47
 *     starvation in the unlikely case where there are continuous stream of /Users/rubber/linux/block/blk-flush.c: 48
 *     FUA (without PREFLUSH) requests. /Users/rubber/linux/block/blk-flush.c: 49
 * For devices which support FUA, it isn't clear whether C2 (and thus C3) /Users/rubber/linux/block/blk-flush.c: 51
 * is beneficial. /Users/rubber/linux/block/blk-flush.c: 52
 * Note that a sequenced PREFLUSH/FUA request with DATA is completed twice. /Users/rubber/linux/block/blk-flush.c: 54
 * Once while executing DATA and again after the whole sequence is /Users/rubber/linux/block/blk-flush.c: 55
 * complete.  The first completion updates the contained bio but doesn't /Users/rubber/linux/block/blk-flush.c: 56
 * finish it so that the bio submitter is notified only after the whole /Users/rubber/linux/block/blk-flush.c: 57
 * sequence is complete.  This is implemented by testing RQF_FLUSH_SEQ in /Users/rubber/linux/block/blk-flush.c: 58
 * req_bio_endio(). /Users/rubber/linux/block/blk-flush.c: 59
 * The above peculiarity requires that each PREFLUSH/FUA request has only one /Users/rubber/linux/block/blk-flush.c: 61
 * bio attached to it, which is guaranteed as they aren't allowed to be /Users/rubber/linux/block/blk-flush.c: 62
 * merged in the usual way. /Users/rubber/linux/block/blk-flush.c: 63
	/* /Users/rubber/linux/block/blk-flush.c: 88
	 * If flush has been pending longer than the following timeout, /Users/rubber/linux/block/blk-flush.c: 89
	 * it's issued even if flush_data requests are still in flight. /Users/rubber/linux/block/blk-flush.c: 90
	/* /Users/rubber/linux/block/blk-flush.c: 122
	 * After flush data completion, @rq->bio is %NULL but we need to /Users/rubber/linux/block/blk-flush.c: 123
	 * complete the bio again.  @rq->biotail is guaranteed to equal the /Users/rubber/linux/block/blk-flush.c: 124
	 * original @rq->bio.  Restore it. /Users/rubber/linux/block/blk-flush.c: 125
 * blk_flush_complete_seq - complete flush sequence /Users/rubber/linux/block/blk-flush.c: 151
 * @rq: PREFLUSH/FUA request being sequenced /Users/rubber/linux/block/blk-flush.c: 152
 * @fq: flush queue /Users/rubber/linux/block/blk-flush.c: 153
 * @seq: sequences to complete (mask of %REQ_FSEQ_*, can be zero) /Users/rubber/linux/block/blk-flush.c: 154
 * @error: whether an error occurred /Users/rubber/linux/block/blk-flush.c: 155
 * @rq just completed @seq part of its flush sequence, record the /Users/rubber/linux/block/blk-flush.c: 157
 * completion and trigger the next step. /Users/rubber/linux/block/blk-flush.c: 158
 * CONTEXT: /Users/rubber/linux/block/blk-flush.c: 160
 * spin_lock_irq(fq->mq_flush_lock) /Users/rubber/linux/block/blk-flush.c: 161
		/* /Users/rubber/linux/block/blk-flush.c: 195
		 * @rq was previously adjusted by blk_insert_flush() for /Users/rubber/linux/block/blk-flush.c: 196
		 * flush sequencing and may already have gone through the /Users/rubber/linux/block/blk-flush.c: 197
		 * flush data request completion path.  Restore @rq for /Users/rubber/linux/block/blk-flush.c: 198
		 * normal completion and end it. /Users/rubber/linux/block/blk-flush.c: 199
	/* /Users/rubber/linux/block/blk-flush.c: 232
	 * Flush request has to be marked as IDLE when it is really ended /Users/rubber/linux/block/blk-flush.c: 233
	 * because its .end_io() is called from timeout code path too for /Users/rubber/linux/block/blk-flush.c: 234
	 * avoiding use-after-free. /Users/rubber/linux/block/blk-flush.c: 235
 * blk_kick_flush - consider issuing flush request /Users/rubber/linux/block/blk-flush.c: 271
 * @q: request_queue being kicked /Users/rubber/linux/block/blk-flush.c: 272
 * @fq: flush queue /Users/rubber/linux/block/blk-flush.c: 273
 * @flags: cmd_flags of the original request /Users/rubber/linux/block/blk-flush.c: 274
 * Flush related states of @q have changed, consider issuing flush request. /Users/rubber/linux/block/blk-flush.c: 276
 * Please read the comment at the top of this file for more info. /Users/rubber/linux/block/blk-flush.c: 277
 * CONTEXT: /Users/rubber/linux/block/blk-flush.c: 279
 * spin_lock_irq(fq->mq_flush_lock) /Users/rubber/linux/block/blk-flush.c: 280
	/* /Users/rubber/linux/block/blk-flush.c: 301
	 * Issue flush and toggle pending_idx.  This makes pending_idx /Users/rubber/linux/block/blk-flush.c: 302
	 * different from running_idx, which means flush is in flight. /Users/rubber/linux/block/blk-flush.c: 303
	/* /Users/rubber/linux/block/blk-flush.c: 309
	 * In case of none scheduler, borrow tag from the first request /Users/rubber/linux/block/blk-flush.c: 310
	 * since they can't be in flight at the same time. And acquire /Users/rubber/linux/block/blk-flush.c: 311
	 * the tag's ownership for flush req. /Users/rubber/linux/block/blk-flush.c: 312
	 * /Users/rubber/linux/block/blk-flush.c: 313
	 * In case of IO scheduler, flush rq need to borrow scheduler tag /Users/rubber/linux/block/blk-flush.c: 314
	 * just for cheating put/get driver tag. /Users/rubber/linux/block/blk-flush.c: 315
		/* /Users/rubber/linux/block/blk-flush.c: 323
		 * We borrow data request's driver tag, so have to mark /Users/rubber/linux/block/blk-flush.c: 324
		 * this flush request as INFLIGHT for avoiding double /Users/rubber/linux/block/blk-flush.c: 325
		 * account of this driver tag /Users/rubber/linux/block/blk-flush.c: 326
	/* /Users/rubber/linux/block/blk-flush.c: 337
	 * Order WRITE ->end_io and WRITE rq->ref, and its pair is the one /Users/rubber/linux/block/blk-flush.c: 338
	 * implied in refcount_inc_not_zero() called from /Users/rubber/linux/block/blk-flush.c: 339
	 * blk_mq_find_and_get_req(), which orders WRITE/READ flush_rq->ref /Users/rubber/linux/block/blk-flush.c: 340
	 * and READ flush_rq->end_io /Users/rubber/linux/block/blk-flush.c: 341
	/* /Users/rubber/linux/block/blk-flush.c: 362
	 * After populating an empty queue, kick it to avoid stall.  Read /Users/rubber/linux/block/blk-flush.c: 363
	 * the comment in flush_end_io(). /Users/rubber/linux/block/blk-flush.c: 364
 * blk_insert_flush - insert a new PREFLUSH/FUA request /Users/rubber/linux/block/blk-flush.c: 374
 * @rq: request to insert /Users/rubber/linux/block/blk-flush.c: 375
 * To be called from __elv_add_request() for %ELEVATOR_INSERT_FLUSH insertions. /Users/rubber/linux/block/blk-flush.c: 377
 * or __blk_mq_run_hw_queue() to dispatch request. /Users/rubber/linux/block/blk-flush.c: 378
 * @rq is being submitted.  Analyze what needs to be done and put it on the /Users/rubber/linux/block/blk-flush.c: 379
 * right queue. /Users/rubber/linux/block/blk-flush.c: 380
	/* /Users/rubber/linux/block/blk-flush.c: 389
	 * @policy now records what operations need to be done.  Adjust /Users/rubber/linux/block/blk-flush.c: 390
	 * REQ_PREFLUSH and FUA for the driver. /Users/rubber/linux/block/blk-flush.c: 391
	/* /Users/rubber/linux/block/blk-flush.c: 397
	 * REQ_PREFLUSH|REQ_FUA implies REQ_SYNC, so if we clear any /Users/rubber/linux/block/blk-flush.c: 398
	 * of those flags, we have to set REQ_SYNC to avoid skewing /Users/rubber/linux/block/blk-flush.c: 399
	 * the request accounting. /Users/rubber/linux/block/blk-flush.c: 400
	/* /Users/rubber/linux/block/blk-flush.c: 404
	 * An empty flush handed down from a stacking driver may /Users/rubber/linux/block/blk-flush.c: 405
	 * translate into nothing if the underlying device does not /Users/rubber/linux/block/blk-flush.c: 406
	 * advertise a write-back cache.  In this case, simply /Users/rubber/linux/block/blk-flush.c: 407
	 * complete the request. /Users/rubber/linux/block/blk-flush.c: 408
	/* /Users/rubber/linux/block/blk-flush.c: 417
	 * If there's data but flush is not necessary, the request can be /Users/rubber/linux/block/blk-flush.c: 418
	 * processed directly without going through flush machinery.  Queue /Users/rubber/linux/block/blk-flush.c: 419
	 * for normal execution. /Users/rubber/linux/block/blk-flush.c: 420
	/* /Users/rubber/linux/block/blk-flush.c: 428
	 * @rq should go through flush machinery.  Mark it part of flush /Users/rubber/linux/block/blk-flush.c: 429
	 * sequence and submit for further processing. /Users/rubber/linux/block/blk-flush.c: 430
 * blkdev_issue_flush - queue a flush /Users/rubber/linux/block/blk-flush.c: 445
 * @bdev:	blockdev to issue flush for /Users/rubber/linux/block/blk-flush.c: 446
 * Description: /Users/rubber/linux/block/blk-flush.c: 448
 *    Issue a flush for the block device in question. /Users/rubber/linux/block/blk-flush.c: 449
 * Allow driver to set its own lock class to fq->mq_flush_lock for /Users/rubber/linux/block/blk-flush.c: 502
 * avoiding lockdep complaint. /Users/rubber/linux/block/blk-flush.c: 503
 * flush_end_io() may be called recursively from some driver, such as /Users/rubber/linux/block/blk-flush.c: 505
 * nvme-loop, so lockdep may complain 'possible recursive locking' because /Users/rubber/linux/block/blk-flush.c: 506
 * all 'struct blk_flush_queue' instance share same mq_flush_lock lock class /Users/rubber/linux/block/blk-flush.c: 507
 * key. We need to assign different lock class for these driver's /Users/rubber/linux/block/blk-flush.c: 508
 * fq->mq_flush_lock for avoiding the lockdep warning. /Users/rubber/linux/block/blk-flush.c: 509
 * Use dynamically allocated lock class key for each 'blk_flush_queue' /Users/rubber/linux/block/blk-flush.c: 511
 * instance is over-kill, and more worse it introduces horrible boot delay /Users/rubber/linux/block/blk-flush.c: 512
 * issue because synchronize_rcu() is implied in lockdep_unregister_key which /Users/rubber/linux/block/blk-flush.c: 513
 * is called for each hctx release. SCSI probing may synchronously create and /Users/rubber/linux/block/blk-flush.c: 514
 * destroy lots of MQ request_queues for non-existent devices, and some robot /Users/rubber/linux/block/blk-flush.c: 515
 * test kernel always enable lockdep option. It is observed that more than half /Users/rubber/linux/block/blk-flush.c: 516
 * an hour is taken during SCSI MQ probe with per-fq lock class. /Users/rubber/linux/block/blk-flush.c: 517
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/bounce.c: 1
/* bounce buffer handling for block devices /Users/rubber/linux/block/bounce.c: 2
 * - Split from highmem.c /Users/rubber/linux/block/bounce.c: 4
 * Simple bounce buffer support for highmem pages. Depending on the /Users/rubber/linux/block/bounce.c: 72
 * queue gfp mask set, *to may or may not be a highmem page. kmap it /Users/rubber/linux/block/bounce.c: 73
 * always, it will do the Right Thing /Users/rubber/linux/block/bounce.c: 74
	/* /Users/rubber/linux/block/bounce.c: 80
	 * The bio of @from is created by bounce, so we can iterate /Users/rubber/linux/block/bounce.c: 81
	 * its bvec from start to end, but the @from->bi_iter can't be /Users/rubber/linux/block/bounce.c: 82
	 * trusted because it might be changed by splitting. /Users/rubber/linux/block/bounce.c: 83
			/* /Users/rubber/linux/block/bounce.c: 90
			 * fromvec->bv_offset and fromvec->bv_len might have /Users/rubber/linux/block/bounce.c: 91
			 * been modified by the block layer, so use the original /Users/rubber/linux/block/bounce.c: 92
			 * copy, bounce_copy_vec already uses tovec->bv_len /Users/rubber/linux/block/bounce.c: 93
	/* /Users/rubber/linux/block/bounce.c: 109
	 * free up bounce indirect pages used /Users/rubber/linux/block/bounce.c: 110
	/* /Users/rubber/linux/block/bounce.c: 147
	 * Pre immutable biovecs, __bio_clone() used to just do a memcpy from /Users/rubber/linux/block/bounce.c: 148
	 * bio_src->bi_io_vec to bio->bi_io_vec. /Users/rubber/linux/block/bounce.c: 149
	 * /Users/rubber/linux/block/bounce.c: 150
	 * We can't do that anymore, because: /Users/rubber/linux/block/bounce.c: 151
	 * /Users/rubber/linux/block/bounce.c: 152
	 *  - The point of cloning the biovec is to produce a bio with a biovec /Users/rubber/linux/block/bounce.c: 153
	 *    the caller can modify: bi_idx and bi_bvec_done should be 0. /Users/rubber/linux/block/bounce.c: 154
	 * /Users/rubber/linux/block/bounce.c: 155
	 *  - The original bio could've had more than BIO_MAX_VECS biovecs; if /Users/rubber/linux/block/bounce.c: 156
	 *    we tried to clone the whole thing bio_alloc_bioset() would fail. /Users/rubber/linux/block/bounce.c: 157
	 *    But the clone should succeed as long as the number of biovecs we /Users/rubber/linux/block/bounce.c: 158
	 *    actually need to allocate is fewer than BIO_MAX_VECS. /Users/rubber/linux/block/bounce.c: 159
	 * /Users/rubber/linux/block/bounce.c: 160
	 *  - Lastly, bi_vcnt should not be looked at or relied upon by code /Users/rubber/linux/block/bounce.c: 161
	 *    that does not own the bio - reason being drivers don't use it for /Users/rubber/linux/block/bounce.c: 162
	 *    iterating over the biovec anymore, so expecting it to be kept up /Users/rubber/linux/block/bounce.c: 163
	 *    to date (i.e. for clones that share the parent biovec) is just /Users/rubber/linux/block/bounce.c: 164
	 *    asking for trouble and would force extra work on /Users/rubber/linux/block/bounce.c: 165
	 *    __bio_clone_fast() anyways. /Users/rubber/linux/block/bounce.c: 166
	/* /Users/rubber/linux/block/bounce.c: 237
	 * Bvec table can't be updated by bio_for_each_segment_all(), /Users/rubber/linux/block/bounce.c: 238
	 * so retrieve bvec from the table directly. This way is safe /Users/rubber/linux/block/bounce.c: 239
	 * because the 'bio' is single-page bvec. /Users/rubber/linux/block/bounce.c: 240
 SPDX-License-Identifier: GPL-2.0-only /Users/rubber/linux/block/bdev.c: 1
 *  Copyright (C) 1991, 1992  Linus Torvalds /Users/rubber/linux/block/bdev.c: 3
 *  Copyright (C) 2001  Andrea Arcangeli <andrea@suse.de> SuSE /Users/rubber/linux/block/bdev.c: 4
 *  Copyright (C) 2016 - 2020 Christoph Hellwig /Users/rubber/linux/block/bdev.c: 5
	/* 99% of the time, we don't need to flush the cleancache on the bdev. /Users/rubber/linux/block/bdev.c: 91
	 * But, for the strange corners, lets be cautious /Users/rubber/linux/block/bdev.c: 92
 * Drop all buffers & page cache for given bdev range. This function bails /Users/rubber/linux/block/bdev.c: 99
 * with error if bdev has other exclusive owner (such as filesystem). /Users/rubber/linux/block/bdev.c: 100
	/* /Users/rubber/linux/block/bdev.c: 105
	 * If we don't hold exclusive handle for the device, upgrade to it /Users/rubber/linux/block/bdev.c: 106
	 * while we discard the buffer cache to avoid discarding buffers /Users/rubber/linux/block/bdev.c: 107
	 * under live filesystem. /Users/rubber/linux/block/bdev.c: 108
	/* /Users/rubber/linux/block/bdev.c: 122
	 * Someone else has handle exclusively open. Try invalidating instead. /Users/rubber/linux/block/bdev.c: 123
	 * The 'end' argument is inclusive so the rounding is safe. /Users/rubber/linux/block/bdev.c: 124
	/* If we get here, we know size is power of two /Users/rubber/linux/block/bdev.c: 169
 * Write out and wait upon all the dirty data associated with a block /Users/rubber/linux/block/bdev.c: 197
 * device via its mapping.  Does not take the superblock lock. /Users/rubber/linux/block/bdev.c: 198
 * Write out and wait upon all dirty data associated with this /Users/rubber/linux/block/bdev.c: 209
 * device.   Filesystem data as well as the underlying block /Users/rubber/linux/block/bdev.c: 210
 * device.  Takes the superblock lock. /Users/rubber/linux/block/bdev.c: 211
 * freeze_bdev  --  lock a filesystem and force it into a consistent state /Users/rubber/linux/block/bdev.c: 226
 * @bdev:	blockdevice to lock /Users/rubber/linux/block/bdev.c: 227
 * If a superblock is found on this device, we take the s_umount semaphore /Users/rubber/linux/block/bdev.c: 229
 * on it to make sure nobody unmounts until the snapshot creation is done. /Users/rubber/linux/block/bdev.c: 230
 * The reference counter (bd_fsfreeze_count) guarantees that only the last /Users/rubber/linux/block/bdev.c: 231
 * unfreeze process can unfreeze the frozen filesystem actually when multiple /Users/rubber/linux/block/bdev.c: 232
 * freeze requests arrive simultaneously. It counts up in freeze_bdev() and /Users/rubber/linux/block/bdev.c: 233
 * count down in thaw_bdev(). When it becomes 0, thaw_bdev() will unfreeze /Users/rubber/linux/block/bdev.c: 234
 * actually. /Users/rubber/linux/block/bdev.c: 235
 * thaw_bdev  -- unlock filesystem /Users/rubber/linux/block/bdev.c: 270
 * @bdev:	blockdevice to unlock /Users/rubber/linux/block/bdev.c: 271
 * Unlocks the filesystem and marks it writeable again after freeze_bdev(). /Users/rubber/linux/block/bdev.c: 273
 * bdev_read_page() - Start reading a page from a block device /Users/rubber/linux/block/bdev.c: 307
 * @bdev: The device to read the page from /Users/rubber/linux/block/bdev.c: 308
 * @sector: The offset on the device to read the page to (need not be aligned) /Users/rubber/linux/block/bdev.c: 309
 * @page: The page to read /Users/rubber/linux/block/bdev.c: 310
 * On entry, the page should be locked.  It will be unlocked when the page /Users/rubber/linux/block/bdev.c: 312
 * has been read.  If the block driver implements rw_page synchronously, /Users/rubber/linux/block/bdev.c: 313
 * that will be true on exit from this function, but it need not be. /Users/rubber/linux/block/bdev.c: 314
 * Errors returned by this function are usually "soft", eg out of memory, or /Users/rubber/linux/block/bdev.c: 316
 * queue full; callers should try a different route to read this page rather /Users/rubber/linux/block/bdev.c: 317
 * than propagate an error back up the stack. /Users/rubber/linux/block/bdev.c: 318
 * Return: negative errno if an error occurs, 0 if submission was successful. /Users/rubber/linux/block/bdev.c: 320
 * bdev_write_page() - Start writing a page to a block device /Users/rubber/linux/block/bdev.c: 341
 * @bdev: The device to write the page to /Users/rubber/linux/block/bdev.c: 342
 * @sector: The offset on the device to write the page to (need not be aligned) /Users/rubber/linux/block/bdev.c: 343
 * @page: The page to write /Users/rubber/linux/block/bdev.c: 344
 * @wbc: The writeback_control for the write /Users/rubber/linux/block/bdev.c: 345
 * On entry, the page should be locked and not currently under writeback. /Users/rubber/linux/block/bdev.c: 347
 * On exit, if the write started successfully, the page will be unlocked and /Users/rubber/linux/block/bdev.c: 348
 * under writeback.  If the write failed already (eg the driver failed to /Users/rubber/linux/block/bdev.c: 349
 * queue the page to the device), the page will still be locked.  If the /Users/rubber/linux/block/bdev.c: 350
 * caller is a ->writepage implementation, it will need to unlock the page. /Users/rubber/linux/block/bdev.c: 351
 * Errors returned by this function are usually "soft", eg out of memory, or /Users/rubber/linux/block/bdev.c: 353
 * queue full; callers should try a different route to write this page rather /Users/rubber/linux/block/bdev.c: 354
 * than propagate an error back up the stack. /Users/rubber/linux/block/bdev.c: 355
 * Return: negative errno if an error occurs, 0 if submission was successful. /Users/rubber/linux/block/bdev.c: 357
 * pseudo-fs /Users/rubber/linux/block/bdev.c: 385
 * bd_may_claim - test whether a block device can be claimed /Users/rubber/linux/block/bdev.c: 529
 * @bdev: block device of interest /Users/rubber/linux/block/bdev.c: 530
 * @whole: whole block device containing @bdev, may equal @bdev /Users/rubber/linux/block/bdev.c: 531
 * @holder: holder trying to claim @bdev /Users/rubber/linux/block/bdev.c: 532
 * Test whether @bdev can be claimed by @holder. /Users/rubber/linux/block/bdev.c: 534
 * CONTEXT: /Users/rubber/linux/block/bdev.c: 536
 * spin_lock(&bdev_lock). /Users/rubber/linux/block/bdev.c: 537
 * RETURNS: /Users/rubber/linux/block/bdev.c: 539
 * %true if @bdev can be claimed, %false otherwise. /Users/rubber/linux/block/bdev.c: 540
 * bd_prepare_to_claim - claim a block device /Users/rubber/linux/block/bdev.c: 561
 * @bdev: block device of interest /Users/rubber/linux/block/bdev.c: 562
 * @holder: holder trying to claim @bdev /Users/rubber/linux/block/bdev.c: 563
 * Claim @bdev.  This function fails if @bdev is already claimed by another /Users/rubber/linux/block/bdev.c: 565
 * holder and waits if another claiming is in progress. return, the caller /Users/rubber/linux/block/bdev.c: 566
 * has ownership of bd_claiming and bd_holder[s]. /Users/rubber/linux/block/bdev.c: 567
 * RETURNS: /Users/rubber/linux/block/bdev.c: 569
 * 0 if @bdev can be claimed, -EBUSY otherwise. /Users/rubber/linux/block/bdev.c: 570
 * bd_finish_claiming - finish claiming of a block device /Users/rubber/linux/block/bdev.c: 615
 * @bdev: block device of interest /Users/rubber/linux/block/bdev.c: 616
 * @holder: holder that has claimed @bdev /Users/rubber/linux/block/bdev.c: 617
 * Finish exclusive open of a block device. Mark the device as exlusively /Users/rubber/linux/block/bdev.c: 619
 * open by the holder and wake up all waiters for exclusive open to finish. /Users/rubber/linux/block/bdev.c: 620
	/* /Users/rubber/linux/block/bdev.c: 628
	 * Note that for a whole device bd_holders will be incremented twice, /Users/rubber/linux/block/bdev.c: 629
	 * and bd_holder will be set to bd_may_claim before being set to holder /Users/rubber/linux/block/bdev.c: 630
 * bd_abort_claiming - abort claiming of a block device /Users/rubber/linux/block/bdev.c: 641
 * @bdev: block device of interest /Users/rubber/linux/block/bdev.c: 642
 * @holder: holder that has claimed @bdev /Users/rubber/linux/block/bdev.c: 643
 * Abort claiming of a block device when the exclusive open failed. This can be /Users/rubber/linux/block/bdev.c: 645
 * also used when exclusive open is not actually desired and we just needed /Users/rubber/linux/block/bdev.c: 646
 * to block other exclusive openers for a while. /Users/rubber/linux/block/bdev.c: 647
 * blkdev_get_by_dev - open a block device by device number /Users/rubber/linux/block/bdev.c: 770
 * @dev: device number of block device to open /Users/rubber/linux/block/bdev.c: 771
 * @mode: FMODE_* mask /Users/rubber/linux/block/bdev.c: 772
 * @holder: exclusive holder identifier /Users/rubber/linux/block/bdev.c: 773
 * Open the block device described by device number @dev. If @mode includes /Users/rubber/linux/block/bdev.c: 775
 * %FMODE_EXCL, the block device is opened with exclusive access.  Specifying /Users/rubber/linux/block/bdev.c: 776
 * %FMODE_EXCL with a %NULL @holder is invalid.  Exclusive opens may nest for /Users/rubber/linux/block/bdev.c: 777
 * the same @holder. /Users/rubber/linux/block/bdev.c: 778
 * Use this interface ONLY if you really do not have anything better - i.e. when /Users/rubber/linux/block/bdev.c: 780
 * you are behind a truly sucky interface and all you are given is a device /Users/rubber/linux/block/bdev.c: 781
 * number.  Everything else should use blkdev_get_by_path(). /Users/rubber/linux/block/bdev.c: 782
 * CONTEXT: /Users/rubber/linux/block/bdev.c: 784
 * Might sleep. /Users/rubber/linux/block/bdev.c: 785
 * RETURNS: /Users/rubber/linux/block/bdev.c: 787
 * Reference to the block_device on success, ERR_PTR(-errno) on failure. /Users/rubber/linux/block/bdev.c: 788
		/* /Users/rubber/linux/block/bdev.c: 832
		 * Block event polling for write claims if requested.  Any write /Users/rubber/linux/block/bdev.c: 833
		 * holder makes the write_holder state stick until all are /Users/rubber/linux/block/bdev.c: 834
		 * released.  This is good enough and tracking individual /Users/rubber/linux/block/bdev.c: 835
		 * writeable reference is too fragile given the way @mode is /Users/rubber/linux/block/bdev.c: 836
		 * used in blkdev_get/put(). /Users/rubber/linux/block/bdev.c: 837
 * blkdev_get_by_path - open a block device by name /Users/rubber/linux/block/bdev.c: 864
 * @path: path to the block device to open /Users/rubber/linux/block/bdev.c: 865
 * @mode: FMODE_* mask /Users/rubber/linux/block/bdev.c: 866
 * @holder: exclusive holder identifier /Users/rubber/linux/block/bdev.c: 867
 * Open the block device described by the device file at @path.  If @mode /Users/rubber/linux/block/bdev.c: 869
 * includes %FMODE_EXCL, the block device is opened with exclusive access. /Users/rubber/linux/block/bdev.c: 870
 * Specifying %FMODE_EXCL with a %NULL @holder is invalid.  Exclusive opens may /Users/rubber/linux/block/bdev.c: 871
 * nest for the same @holder. /Users/rubber/linux/block/bdev.c: 872
 * CONTEXT: /Users/rubber/linux/block/bdev.c: 874
 * Might sleep. /Users/rubber/linux/block/bdev.c: 875
 * RETURNS: /Users/rubber/linux/block/bdev.c: 877
 * Reference to the block_device on success, ERR_PTR(-errno) on failure. /Users/rubber/linux/block/bdev.c: 878
	/* /Users/rubber/linux/block/bdev.c: 905
	 * Sync early if it looks like we're the last one.  If someone else /Users/rubber/linux/block/bdev.c: 906
	 * opens the block device between now and the decrement of bd_openers /Users/rubber/linux/block/bdev.c: 907
	 * then we did a sync that we didn't need to, but that's not the end /Users/rubber/linux/block/bdev.c: 908
	 * of the world and we want to avoid long (could be several minute) /Users/rubber/linux/block/bdev.c: 909
	 * syncs while holding the mutex. /Users/rubber/linux/block/bdev.c: 910
		/* /Users/rubber/linux/block/bdev.c: 920
		 * Release a claim on the device.  The holder fields /Users/rubber/linux/block/bdev.c: 921
		 * are protected with bdev_lock.  open_mutex is to /Users/rubber/linux/block/bdev.c: 922
		 * synchronize disk_holder unlinking. /Users/rubber/linux/block/bdev.c: 923
		/* /Users/rubber/linux/block/bdev.c: 937
		 * If this was the last claim, remove holder link and /Users/rubber/linux/block/bdev.c: 938
		 * unblock evpoll if it was a write holder. /Users/rubber/linux/block/bdev.c: 939
	/* /Users/rubber/linux/block/bdev.c: 947
	 * Trigger event checking and tell drivers to flush MEDIA_CHANGE /Users/rubber/linux/block/bdev.c: 948
	 * event.  This is to ensure detection of media removal commanded /Users/rubber/linux/block/bdev.c: 949
	 * from userland - e.g. eject(1). /Users/rubber/linux/block/bdev.c: 950
 * lookup_bdev  - lookup a struct block_device by name /Users/rubber/linux/block/bdev.c: 966
 * @pathname:	special file representing the block device /Users/rubber/linux/block/bdev.c: 967
 * @dev:	return value of the block device's dev_t /Users/rubber/linux/block/bdev.c: 968
 * Lookup the block device's dev_t at @pathname in the current /Users/rubber/linux/block/bdev.c: 970
 * namespace if possible and return it by @dev. /Users/rubber/linux/block/bdev.c: 971
 * RETURNS: /Users/rubber/linux/block/bdev.c: 973
 * 0 if succeeded, errno otherwise. /Users/rubber/linux/block/bdev.c: 974
		/* /Users/rubber/linux/block/bdev.c: 1011
		 * no need to lock the super, get_super holds the /Users/rubber/linux/block/bdev.c: 1012
		 * read mutex so the filesystem cannot go away /Users/rubber/linux/block/bdev.c: 1013
		 * under us (->put_super runs with the write lock /Users/rubber/linux/block/bdev.c: 1014
		 * hold). /Users/rubber/linux/block/bdev.c: 1015
		/* /Users/rubber/linux/block/bdev.c: 1044
		 * We hold a reference to 'inode' so it couldn't have been /Users/rubber/linux/block/bdev.c: 1045
		 * removed from s_inodes list while we dropped the /Users/rubber/linux/block/bdev.c: 1046
		 * s_inode_list_lock  We cannot iput the inode now as we can /Users/rubber/linux/block/bdev.c: 1047
		 * be holding the last reference and we cannot iput it under /Users/rubber/linux/block/bdev.c: 1048
		 * s_inode_list_lock. So we keep the reference and iput it /Users/rubber/linux/block/bdev.c: 1049
		 * later. /Users/rubber/linux/block/bdev.c: 1050
			/* /Users/rubber/linux/block/bdev.c: 1060
			 * We keep the error status of individual mapping so /Users/rubber/linux/block/bdev.c: 1061
			 * that applications can catch the writeback error using /Users/rubber/linux/block/bdev.c: 1062
			 * fsync(2). See filemap_fdatawait_keep_errors() for /Users/rubber/linux/block/bdev.c: 1063
			 * details. /Users/rubber/linux/block/bdev.c: 1064
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/elevator.c: 1
 *  Block device elevator/IO-scheduler. /Users/rubber/linux/block/elevator.c: 3
 *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE /Users/rubber/linux/block/elevator.c: 5
 * 30042000 Jens Axboe <axboe@kernel.dk> : /Users/rubber/linux/block/elevator.c: 7
 * Split the elevator a bit so that it is possible to choose a different /Users/rubber/linux/block/elevator.c: 9
 * one or even write a new "plug in". There are three pieces: /Users/rubber/linux/block/elevator.c: 10
 * - elevator_fn, inserts a new request in the queue list /Users/rubber/linux/block/elevator.c: 11
 * - elevator_merge_fn, decides whether a new buffer can be merged with /Users/rubber/linux/block/elevator.c: 12
 *   an existing request /Users/rubber/linux/block/elevator.c: 13
 * - elevator_dequeue_fn, called when a request is taken off the active list /Users/rubber/linux/block/elevator.c: 14
 * 20082000 Dave Jones <davej@suse.de> : /Users/rubber/linux/block/elevator.c: 16
 * Removed tests for max-bomb-segments, which was breaking elvtune /Users/rubber/linux/block/elevator.c: 17
 *  when run without -bN /Users/rubber/linux/block/elevator.c: 18
 * Jens: /Users/rubber/linux/block/elevator.c: 20
 * - Rework again to work with bio instead of buffer_heads /Users/rubber/linux/block/elevator.c: 21
 * - loose bi_dev comparisons, partition handling is right now /Users/rubber/linux/block/elevator.c: 22
 * - completely modularize elevator setup and teardown /Users/rubber/linux/block/elevator.c: 23
 * Merge hash stuff. /Users/rubber/linux/block/elevator.c: 52
 * Query io scheduler to see if the current process issuing bio may be /Users/rubber/linux/block/elevator.c: 57
 * merged with rq. /Users/rubber/linux/block/elevator.c: 58
 * can we safely merge with this request? /Users/rubber/linux/block/elevator.c: 72
 * elevator_match - Test an elevator name and features /Users/rubber/linux/block/elevator.c: 93
 * @e: Scheduler to test /Users/rubber/linux/block/elevator.c: 94
 * @name: Elevator name to test /Users/rubber/linux/block/elevator.c: 95
 * @required_features: Features that the elevator must provide /Users/rubber/linux/block/elevator.c: 96
 * Return true if the elevator @e name matches @name and if @e provides all /Users/rubber/linux/block/elevator.c: 98
 * the features specified by @required_features. /Users/rubber/linux/block/elevator.c: 99
 * elevator_find - Find an elevator /Users/rubber/linux/block/elevator.c: 115
 * @name: Name of the elevator to find /Users/rubber/linux/block/elevator.c: 116
 * @required_features: Features that the elevator must provide /Users/rubber/linux/block/elevator.c: 117
 * Return the first registered scheduler with name @name and supporting the /Users/rubber/linux/block/elevator.c: 119
 * features @required_features and NULL otherwise. /Users/rubber/linux/block/elevator.c: 120
 * RB-tree support functions for inserting/lookup/removal of requests /Users/rubber/linux/block/elevator.c: 251
 * in a sorted RB tree. /Users/rubber/linux/block/elevator.c: 252
	/* /Users/rubber/linux/block/elevator.c: 309
	 * Levels of merges: /Users/rubber/linux/block/elevator.c: 310
	 * 	nomerges:  No merges at all attempted /Users/rubber/linux/block/elevator.c: 311
	 * 	noxmerges: Only simple one-hit cache try /Users/rubber/linux/block/elevator.c: 312
	 * 	merges:	   All merge tries attempted /Users/rubber/linux/block/elevator.c: 313
	/* /Users/rubber/linux/block/elevator.c: 318
	 * First try one-hit cache. /Users/rubber/linux/block/elevator.c: 319
	/* /Users/rubber/linux/block/elevator.c: 333
	 * See if our hash lookup can find a potential backmerge. /Users/rubber/linux/block/elevator.c: 334
 * Attempt to do an insertion back merge. Only check for the case where /Users/rubber/linux/block/elevator.c: 352
 * we can append 'rq' to an existing request, so we can throw 'rq' away /Users/rubber/linux/block/elevator.c: 353
 * afterwards. /Users/rubber/linux/block/elevator.c: 354
 * Returns true if we merged, false otherwise. 'free' will contain all /Users/rubber/linux/block/elevator.c: 356
 * requests that need to be freed. /Users/rubber/linux/block/elevator.c: 357
	/* /Users/rubber/linux/block/elevator.c: 368
	 * First try one-hit cache. /Users/rubber/linux/block/elevator.c: 369
	/* /Users/rubber/linux/block/elevator.c: 380
	 * See if our hash lookup can find a potential backmerge. /Users/rubber/linux/block/elevator.c: 381
	/* /Users/rubber/linux/block/elevator.c: 574
	 * Destroy icq_cache if it exists.  icq's are RCU managed.  Make /Users/rubber/linux/block/elevator.c: 575
	 * sure all RCU operations are complete before proceeding. /Users/rubber/linux/block/elevator.c: 576
 * For single queue devices, default to using mq-deadline. If we have multiple /Users/rubber/linux/block/elevator.c: 631
 * queues or mq-deadline is not available, default to "none". /Users/rubber/linux/block/elevator.c: 632
 * Get the first elevator providing the features required by the request queue. /Users/rubber/linux/block/elevator.c: 647
 * Default to "none" if no matching elevator is found. /Users/rubber/linux/block/elevator.c: 648
 * For a device queue that has no required features, use the default elevator /Users/rubber/linux/block/elevator.c: 672
 * settings. Otherwise, use the first elevator available matching the required /Users/rubber/linux/block/elevator.c: 673
 * features. If no suitable elevator is find or if the chosen elevator /Users/rubber/linux/block/elevator.c: 674
 * initialization fails, fall back to the "none" elevator (no elevator). /Users/rubber/linux/block/elevator.c: 675
	/* /Users/rubber/linux/block/elevator.c: 697
	 * We are called before adding disk, when there isn't any FS I/O, /Users/rubber/linux/block/elevator.c: 698
	 * so freezing queue plus canceling dispatch work is enough to /Users/rubber/linux/block/elevator.c: 699
	 * drain any dispatch activities originated from passthrough /Users/rubber/linux/block/elevator.c: 700
	 * requests, then no need to quiesce queue which may add long boot /Users/rubber/linux/block/elevator.c: 701
	 * latency, especially when lots of disks are involved. /Users/rubber/linux/block/elevator.c: 702
 * switch to new_e io scheduler. be careful not to introduce deadlocks - /Users/rubber/linux/block/elevator.c: 719
 * we don't free the old io scheduler, before we have allocated what we /Users/rubber/linux/block/elevator.c: 720
 * need for the new one. this way we have a chance of going back to the old /Users/rubber/linux/block/elevator.c: 721
 * one, if the new one fails init for some reason. /Users/rubber/linux/block/elevator.c: 722
 * Switch this queue to the given IO scheduler. /Users/rubber/linux/block/elevator.c: 742
	/* /Users/rubber/linux/block/elevator.c: 753
	 * Special case for mq, turn off scheduling /Users/rubber/linux/block/elevator.c: 754
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-crypto.c: 1
 * Copyright 2019 Google LLC /Users/rubber/linux/block/blk-crypto.c: 3
 * Refer to Documentation/block/inline-encryption.rst for detailed explanation. /Users/rubber/linux/block/blk-crypto.c: 7
 * This number needs to be at least (the number of threads doing IO /Users/rubber/linux/block/blk-crypto.c: 39
 * concurrently) * (maximum recursive depth of a bio), so that we don't /Users/rubber/linux/block/blk-crypto.c: 40
 * deadlock on crypt_ctx allocations. The default is chosen to be the same /Users/rubber/linux/block/blk-crypto.c: 41
 * as the default number of post read contexts in both EXT4 and F2FS. /Users/rubber/linux/block/blk-crypto.c: 42
	/* /Users/rubber/linux/block/blk-crypto.c: 86
	 * The caller must use a gfp_mask that contains __GFP_DIRECT_RECLAIM so /Users/rubber/linux/block/blk-crypto.c: 87
	 * that the mempool_alloc() can't fail. /Users/rubber/linux/block/blk-crypto.c: 88
		/* /Users/rubber/linux/block/blk-crypto.c: 124
		 * If the addition in this limb overflowed, then we need to /Users/rubber/linux/block/blk-crypto.c: 125
		 * carry 1 into the next limb. Else the carry is 0. /Users/rubber/linux/block/blk-crypto.c: 126
 * Returns true if @bc->bc_dun plus @bytes converted to data units is equal to /Users/rubber/linux/block/blk-crypto.c: 144
 * @next_dun, treating the DUNs as multi-limb integers. /Users/rubber/linux/block/blk-crypto.c: 145
		/* /Users/rubber/linux/block/blk-crypto.c: 157
		 * If the addition in this limb overflowed, then we need to /Users/rubber/linux/block/blk-crypto.c: 158
		 * carry 1 into the next limb. Else the carry is 0. /Users/rubber/linux/block/blk-crypto.c: 159
 * Checks that two bio crypt contexts are compatible - i.e. that /Users/rubber/linux/block/blk-crypto.c: 172
 * they are mergeable except for data_unit_num continuity. /Users/rubber/linux/block/blk-crypto.c: 173
 * Checks that two bio crypt contexts are compatible, and also /Users/rubber/linux/block/blk-crypto.c: 190
 * that their data_unit_nums are continuous (and can hence be merged) /Users/rubber/linux/block/blk-crypto.c: 191
 * in the order @bc1 followed by @bc2. /Users/rubber/linux/block/blk-crypto.c: 192
 * __blk_crypto_free_request - Uninitialize the crypto fields of a request. /Users/rubber/linux/block/blk-crypto.c: 227
 * @rq: The request whose crypto fields to uninitialize. /Users/rubber/linux/block/blk-crypto.c: 229
 * Completely uninitializes the crypto fields of a request. If a keyslot has /Users/rubber/linux/block/blk-crypto.c: 231
 * been programmed into some inline encryption hardware, that keyslot is /Users/rubber/linux/block/blk-crypto.c: 232
 * released. The rq->crypt_ctx is also freed. /Users/rubber/linux/block/blk-crypto.c: 233
 * __blk_crypto_bio_prep - Prepare bio for inline encryption /Users/rubber/linux/block/blk-crypto.c: 243
 * @bio_ptr: pointer to original bio pointer /Users/rubber/linux/block/blk-crypto.c: 245
 * If the bio crypt context provided for the bio is supported by the underlying /Users/rubber/linux/block/blk-crypto.c: 247
 * device's inline encryption hardware, do nothing. /Users/rubber/linux/block/blk-crypto.c: 248
 * Otherwise, try to perform en/decryption for this bio by falling back to the /Users/rubber/linux/block/blk-crypto.c: 250
 * kernel crypto API. When the crypto API fallback is used for encryption, /Users/rubber/linux/block/blk-crypto.c: 251
 * blk-crypto may choose to split the bio into 2 - the first one that will /Users/rubber/linux/block/blk-crypto.c: 252
 * continue to be processed and the second one that will be resubmitted via /Users/rubber/linux/block/blk-crypto.c: 253
 * submit_bio_noacct. A bounce bio will be allocated to encrypt the contents /Users/rubber/linux/block/blk-crypto.c: 254
 * of the aforementioned "first one", and *bio_ptr will be updated to this /Users/rubber/linux/block/blk-crypto.c: 255
 * bounce bio. /Users/rubber/linux/block/blk-crypto.c: 256
 * Caller must ensure bio has bio_crypt_ctx. /Users/rubber/linux/block/blk-crypto.c: 258
 * Return: true on success; false on error (and bio->bi_status will be set /Users/rubber/linux/block/blk-crypto.c: 260
 *	   appropriately, and bio_endio() will have been called so bio /Users/rubber/linux/block/blk-crypto.c: 261
 *	   submission should abort). /Users/rubber/linux/block/blk-crypto.c: 262
	/* /Users/rubber/linux/block/blk-crypto.c: 281
	 * Success if device supports the encryption context, or if we succeeded /Users/rubber/linux/block/blk-crypto.c: 282
	 * in falling back to the crypto API. /Users/rubber/linux/block/blk-crypto.c: 283
 * blk_crypto_init_key() - Prepare a key for use with blk-crypto /Users/rubber/linux/block/blk-crypto.c: 309
 * @blk_key: Pointer to the blk_crypto_key to initialize. /Users/rubber/linux/block/blk-crypto.c: 310
 * @raw_key: Pointer to the raw key. Must be the correct length for the chosen /Users/rubber/linux/block/blk-crypto.c: 311
 *	     @crypto_mode; see blk_crypto_modes[]. /Users/rubber/linux/block/blk-crypto.c: 312
 * @crypto_mode: identifier for the encryption algorithm to use /Users/rubber/linux/block/blk-crypto.c: 313
 * @dun_bytes: number of bytes that will be used to specify the DUN when this /Users/rubber/linux/block/blk-crypto.c: 314
 *	       key is used /Users/rubber/linux/block/blk-crypto.c: 315
 * @data_unit_size: the data unit size to use for en/decryption /Users/rubber/linux/block/blk-crypto.c: 316
 * Return: 0 on success, -errno on failure.  The caller is responsible for /Users/rubber/linux/block/blk-crypto.c: 318
 *	   zeroizing both blk_key and raw_key when done with them. /Users/rubber/linux/block/blk-crypto.c: 319
 * Check if bios with @cfg can be en/decrypted by blk-crypto (i.e. either the /Users/rubber/linux/block/blk-crypto.c: 354
 * request queue it's submitted to supports inline crypto, or the /Users/rubber/linux/block/blk-crypto.c: 355
 * blk-crypto-fallback is enabled and supports the cfg). /Users/rubber/linux/block/blk-crypto.c: 356
 * blk_crypto_start_using_key() - Start using a blk_crypto_key on a device /Users/rubber/linux/block/blk-crypto.c: 366
 * @key: A key to use on the device /Users/rubber/linux/block/blk-crypto.c: 367
 * @q: the request queue for the device /Users/rubber/linux/block/blk-crypto.c: 368
 * Upper layers must call this function to ensure that either the hardware /Users/rubber/linux/block/blk-crypto.c: 370
 * supports the key's crypto settings, or the crypto API fallback has transforms /Users/rubber/linux/block/blk-crypto.c: 371
 * for the needed mode allocated and ready to go. This function may allocate /Users/rubber/linux/block/blk-crypto.c: 372
 * an skcipher, and *should not* be called from the data path, since that might /Users/rubber/linux/block/blk-crypto.c: 373
 * cause a deadlock /Users/rubber/linux/block/blk-crypto.c: 374
 * Return: 0 on success; -ENOPKG if the hardware doesn't support the key and /Users/rubber/linux/block/blk-crypto.c: 376
 *	   blk-crypto-fallback is either disabled or the needed algorithm /Users/rubber/linux/block/blk-crypto.c: 377
 *	   is disabled in the crypto API; or another -errno code. /Users/rubber/linux/block/blk-crypto.c: 378
 * blk_crypto_evict_key() - Evict a key from any inline encryption hardware /Users/rubber/linux/block/blk-crypto.c: 389
 *			    it may have been programmed into /Users/rubber/linux/block/blk-crypto.c: 390
 * @q: The request queue who's associated inline encryption hardware this key /Users/rubber/linux/block/blk-crypto.c: 391
 *     might have been programmed into /Users/rubber/linux/block/blk-crypto.c: 392
 * @key: The key to evict /Users/rubber/linux/block/blk-crypto.c: 393
 * Upper layers (filesystems) must call this function to ensure that a key is /Users/rubber/linux/block/blk-crypto.c: 395
 * evicted from any hardware that it might have been programmed into.  The key /Users/rubber/linux/block/blk-crypto.c: 396
 * must not be in use by any in-flight IO when this function is called. /Users/rubber/linux/block/blk-crypto.c: 397
 * Return: 0 on success or if the key wasn't in any keyslot; -errno on error. /Users/rubber/linux/block/blk-crypto.c: 399
	/* /Users/rubber/linux/block/blk-crypto.c: 407
	 * If the request_queue didn't support the key, then blk-crypto-fallback /Users/rubber/linux/block/blk-crypto.c: 408
	 * may have been used, so try to evict the key from blk-crypto-fallback. /Users/rubber/linux/block/blk-crypto.c: 409
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-rdma.c: 1
 * Copyright (c) 2017 Sagi Grimberg. /Users/rubber/linux/block/blk-mq-rdma.c: 3
 * blk_mq_rdma_map_queues - provide a default queue mapping for rdma device /Users/rubber/linux/block/blk-mq-rdma.c: 10
 * @map:	CPU to hardware queue map. /Users/rubber/linux/block/blk-mq-rdma.c: 11
 * @dev:	rdma device to provide a mapping for. /Users/rubber/linux/block/blk-mq-rdma.c: 12
 * @first_vec:	first interrupt vectors to use for queues (usually 0) /Users/rubber/linux/block/blk-mq-rdma.c: 13
 * This function assumes the rdma device @dev has at least as many available /Users/rubber/linux/block/blk-mq-rdma.c: 15
 * interrupt vetors as @set has queues.  It will then query it's affinity mask /Users/rubber/linux/block/blk-mq-rdma.c: 16
 * and built queue mapping that maps a queue to the CPUs that have irq affinity /Users/rubber/linux/block/blk-mq-rdma.c: 17
 * for the corresponding vector. /Users/rubber/linux/block/blk-mq-rdma.c: 18
 * In case either the driver passed a @dev with less vectors than /Users/rubber/linux/block/blk-mq-rdma.c: 20
 * @set->nr_hw_queues, or @dev does not provide an affinity mask for a /Users/rubber/linux/block/blk-mq-rdma.c: 21
 * vector, we fallback to the naive mapping. /Users/rubber/linux/block/blk-mq-rdma.c: 22
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-mq-cpumap.c: 1
 * CPU <-> hardware queue mapping helpers /Users/rubber/linux/block/blk-mq-cpumap.c: 3
 * Copyright (C) 2013-2014 Jens Axboe /Users/rubber/linux/block/blk-mq-cpumap.c: 5
	/* /Users/rubber/linux/block/blk-mq-cpumap.c: 44
	 * Spread queues among present CPUs first for minimizing /Users/rubber/linux/block/blk-mq-cpumap.c: 45
	 * count of dead queues which are mapped by all un-present CPUs /Users/rubber/linux/block/blk-mq-cpumap.c: 46
		/* /Users/rubber/linux/block/blk-mq-cpumap.c: 57
		 * First do sequential mapping between CPUs and queues. /Users/rubber/linux/block/blk-mq-cpumap.c: 58
		 * In case we still have CPUs to map, and we have some number of /Users/rubber/linux/block/blk-mq-cpumap.c: 59
		 * threads per cores then map sibling threads to the same queue /Users/rubber/linux/block/blk-mq-cpumap.c: 60
		 * for performance optimizations. /Users/rubber/linux/block/blk-mq-cpumap.c: 61
 * blk_mq_hw_queue_to_node - Look up the memory node for a hardware queue index /Users/rubber/linux/block/blk-mq-cpumap.c: 79
 * @qmap: CPU to hardware queue map. /Users/rubber/linux/block/blk-mq-cpumap.c: 80
 * @index: hardware queue index. /Users/rubber/linux/block/blk-mq-cpumap.c: 81
 * We have no quick way of doing reverse lookups. This is only used at /Users/rubber/linux/block/blk-mq-cpumap.c: 83
 * queue init time, so runtime isn't important. /Users/rubber/linux/block/blk-mq-cpumap.c: 84
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/blk-cgroup.c: 1
 * Common Block IO controller cgroup interface /Users/rubber/linux/block/blk-cgroup.c: 3
 * Based on ideas and code from CFQ, CFS and BFQ: /Users/rubber/linux/block/blk-cgroup.c: 5
 * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk> /Users/rubber/linux/block/blk-cgroup.c: 6
 * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it> /Users/rubber/linux/block/blk-cgroup.c: 8
 *		      Paolo Valente <paolo.valente@unimore.it> /Users/rubber/linux/block/blk-cgroup.c: 9
 * Copyright (C) 2009 Vivek Goyal <vgoyal@redhat.com> /Users/rubber/linux/block/blk-cgroup.c: 11
 * 	              Nauman Rafique <nauman@google.com> /Users/rubber/linux/block/blk-cgroup.c: 12
 * For policy-specific per-blkcg data: /Users/rubber/linux/block/blk-cgroup.c: 14
 * Copyright (C) 2015 Paolo Valente <paolo.valente@unimore.it> /Users/rubber/linux/block/blk-cgroup.c: 15
 *                    Arianna Avanzini <avanzini.arianna@gmail.com> /Users/rubber/linux/block/blk-cgroup.c: 16
 * blkcg_pol_mutex protects blkcg_policy[] and policy [de]activation. /Users/rubber/linux/block/blk-cgroup.c: 38
 * blkcg_pol_register_mutex nests outside of it and synchronizes entire /Users/rubber/linux/block/blk-cgroup.c: 39
 * policy [un]register operations including cgroup file additions / /Users/rubber/linux/block/blk-cgroup.c: 40
 * removals.  Putting cgroup file registration outside blkcg_pol_mutex /Users/rubber/linux/block/blk-cgroup.c: 41
 * allows grabbing it from cgroup callbacks. /Users/rubber/linux/block/blk-cgroup.c: 42
 * blkg_free - free a blkg /Users/rubber/linux/block/blk-cgroup.c: 69
 * @blkg: blkg to free /Users/rubber/linux/block/blk-cgroup.c: 70
 * Free @blkg which may be partially allocated. /Users/rubber/linux/block/blk-cgroup.c: 72
 * A group is RCU protected, but having an rcu lock does not mean that one /Users/rubber/linux/block/blk-cgroup.c: 104
 * can access all the fields of blkg and assume these are valid.  For /Users/rubber/linux/block/blk-cgroup.c: 105
 * example, don't try to follow throtl_data and request queue links. /Users/rubber/linux/block/blk-cgroup.c: 106
 * Having a reference to blkg under an rcu allows accesses to only values /Users/rubber/linux/block/blk-cgroup.c: 108
 * local to groups like group stats and group rate limits. /Users/rubber/linux/block/blk-cgroup.c: 109
 * blkg_alloc - allocate a blkg /Users/rubber/linux/block/blk-cgroup.c: 145
 * @blkcg: block cgroup the new blkg is associated with /Users/rubber/linux/block/blk-cgroup.c: 146
 * @q: request_queue the new blkg is associated with /Users/rubber/linux/block/blk-cgroup.c: 147
 * @gfp_mask: allocation mask to use /Users/rubber/linux/block/blk-cgroup.c: 148
 * Allocate a new blkg assocating @blkcg and @q. /Users/rubber/linux/block/blk-cgroup.c: 150
	/* /Users/rubber/linux/block/blk-cgroup.c: 210
	 * Hint didn't match.  Look up from the radix tree.  Note that the /Users/rubber/linux/block/blk-cgroup.c: 211
	 * hint can only be updated under queue_lock as otherwise @blkg /Users/rubber/linux/block/blk-cgroup.c: 212
	 * could have already been removed from blkg_tree.  The caller is /Users/rubber/linux/block/blk-cgroup.c: 213
	 * responsible for grabbing queue_lock if @update_hint. /Users/rubber/linux/block/blk-cgroup.c: 214
 * If @new_blkg is %NULL, this function tries to allocate a new one as /Users/rubber/linux/block/blk-cgroup.c: 230
 * necessary using %GFP_NOWAIT.  @new_blkg is always consumed on return. /Users/rubber/linux/block/blk-cgroup.c: 231
 * blkg_lookup_create - lookup blkg, try to create one if not there /Users/rubber/linux/block/blk-cgroup.c: 315
 * @blkcg: blkcg of interest /Users/rubber/linux/block/blk-cgroup.c: 316
 * @q: request_queue of interest /Users/rubber/linux/block/blk-cgroup.c: 317
 * Lookup blkg for the @blkcg - @q pair.  If it doesn't exist, try to /Users/rubber/linux/block/blk-cgroup.c: 319
 * create one.  blkg creation is performed recursively from blkcg_root such /Users/rubber/linux/block/blk-cgroup.c: 320
 * that all non-root blkg's have access to the parent blkg.  This function /Users/rubber/linux/block/blk-cgroup.c: 321
 * should be called under RCU read lock and takes @q->queue_lock. /Users/rubber/linux/block/blk-cgroup.c: 322
 * Returns the blkg or the closest blkg if blkg_create() fails as it walks /Users/rubber/linux/block/blk-cgroup.c: 324
 * down from root. /Users/rubber/linux/block/blk-cgroup.c: 325
	/* /Users/rubber/linux/block/blk-cgroup.c: 344
	 * Create blkgs walking down from blkcg_root to @blkcg, so that all /Users/rubber/linux/block/blk-cgroup.c: 345
	 * non-root blkgs have access to their parents.  Returns the closest /Users/rubber/linux/block/blk-cgroup.c: 346
	 * blkg to the intended blkg should blkg_create() fail. /Users/rubber/linux/block/blk-cgroup.c: 347
	/* /Users/rubber/linux/block/blk-cgroup.c: 404
	 * Both setting lookup hint to and clearing it from @blkg are done /Users/rubber/linux/block/blk-cgroup.c: 405
	 * under queue_lock.  If it's not pointing to @blkg now, it never /Users/rubber/linux/block/blk-cgroup.c: 406
	 * will.  Hint assignment itself can race safely. /Users/rubber/linux/block/blk-cgroup.c: 407
	/* /Users/rubber/linux/block/blk-cgroup.c: 412
	 * Put the reference taken at the time of creation so that when all /Users/rubber/linux/block/blk-cgroup.c: 413
	 * queues are gone, group can be destroyed. /Users/rubber/linux/block/blk-cgroup.c: 414
 * blkg_destroy_all - destroy all blkgs associated with a request_queue /Users/rubber/linux/block/blk-cgroup.c: 420
 * @q: request_queue of interest /Users/rubber/linux/block/blk-cgroup.c: 421
 * Destroy all blkgs associated with @q. /Users/rubber/linux/block/blk-cgroup.c: 423
		/* /Users/rubber/linux/block/blk-cgroup.c: 439
		 * in order to avoid holding the spin lock for too long, release /Users/rubber/linux/block/blk-cgroup.c: 440
		 * it when a batch of blkgs are destroyed. /Users/rubber/linux/block/blk-cgroup.c: 441
	/* /Users/rubber/linux/block/blk-cgroup.c: 465
	 * Note that stat reset is racy - it doesn't synchronize against /Users/rubber/linux/block/blk-cgroup.c: 466
	 * stat updates.  This is a debug feature which shouldn't exist /Users/rubber/linux/block/blk-cgroup.c: 467
	 * anyway.  If you get hit by a race, retry. /Users/rubber/linux/block/blk-cgroup.c: 468
 * blkcg_print_blkgs - helper for printing per-blkg data /Users/rubber/linux/block/blk-cgroup.c: 499
 * @sf: seq_file to print to /Users/rubber/linux/block/blk-cgroup.c: 500
 * @blkcg: blkcg of interest /Users/rubber/linux/block/blk-cgroup.c: 501
 * @prfill: fill function to print out a blkg /Users/rubber/linux/block/blk-cgroup.c: 502
 * @pol: policy in question /Users/rubber/linux/block/blk-cgroup.c: 503
 * @data: data to be passed to @prfill /Users/rubber/linux/block/blk-cgroup.c: 504
 * @show_total: to print out sum of prfill return values or not /Users/rubber/linux/block/blk-cgroup.c: 505
 * This function invokes @prfill on each blkg of @blkcg if pd for the /Users/rubber/linux/block/blk-cgroup.c: 507
 * policy specified by @pol exists.  @prfill is invoked with @sf, the /Users/rubber/linux/block/blk-cgroup.c: 508
 * policy data and @data and the matching queue lock held.  If @show_total /Users/rubber/linux/block/blk-cgroup.c: 509
 * is %true, the sum of the return values from @prfill is printed with /Users/rubber/linux/block/blk-cgroup.c: 510
 * "Total" label at the end. /Users/rubber/linux/block/blk-cgroup.c: 511
 * This is to be used to construct print functions for /Users/rubber/linux/block/blk-cgroup.c: 513
 * cftype->read_seq_string method. /Users/rubber/linux/block/blk-cgroup.c: 514
 * __blkg_prfill_u64 - prfill helper for a single u64 value /Users/rubber/linux/block/blk-cgroup.c: 540
 * @sf: seq_file to print to /Users/rubber/linux/block/blk-cgroup.c: 541
 * @pd: policy private data of interest /Users/rubber/linux/block/blk-cgroup.c: 542
 * @v: value to print /Users/rubber/linux/block/blk-cgroup.c: 543
 * Print @v to @sf for the device assocaited with @pd. /Users/rubber/linux/block/blk-cgroup.c: 545
 update_hint */); /Users/rubber/linux/block/blk-cgroup.c: 569
 * blkcg_conf_open_bdev - parse and open bdev for per-blkg config update /Users/rubber/linux/block/blk-cgroup.c: 573
 * @inputp: input string pointer /Users/rubber/linux/block/blk-cgroup.c: 574
 * Parse the device node prefix part, MAJ:MIN, of per-blkg config update /Users/rubber/linux/block/blk-cgroup.c: 576
 * from @input and get and return the matching bdev.  *@inputp is /Users/rubber/linux/block/blk-cgroup.c: 577
 * updated to point past the device node prefix.  Returns an ERR_PTR() /Users/rubber/linux/block/blk-cgroup.c: 578
 * value on error. /Users/rubber/linux/block/blk-cgroup.c: 579
 * Use this function iff blkg_conf_prep() can't be used for some reason. /Users/rubber/linux/block/blk-cgroup.c: 581
 * blkg_conf_prep - parse and prepare for per-blkg config update /Users/rubber/linux/block/blk-cgroup.c: 611
 * @blkcg: target block cgroup /Users/rubber/linux/block/blk-cgroup.c: 612
 * @pol: target policy /Users/rubber/linux/block/blk-cgroup.c: 613
 * @input: input string /Users/rubber/linux/block/blk-cgroup.c: 614
 * @ctx: blkg_conf_ctx to be filled /Users/rubber/linux/block/blk-cgroup.c: 615
 * Parse per-blkg config update from @input and initialize @ctx with the /Users/rubber/linux/block/blk-cgroup.c: 617
 * result.  @ctx->blkg points to the blkg to be updated and @ctx->body the /Users/rubber/linux/block/blk-cgroup.c: 618
 * part of @input following MAJ:MIN.  This function returns with RCU read /Users/rubber/linux/block/blk-cgroup.c: 619
 * lock and queue lock held and must be paired with blkg_conf_finish(). /Users/rubber/linux/block/blk-cgroup.c: 620
	/* /Users/rubber/linux/block/blk-cgroup.c: 637
	 * blkcg_deactivate_policy() requires queue to be frozen, we can grab /Users/rubber/linux/block/blk-cgroup.c: 638
	 * q_usage_counter to prevent concurrent with blkcg_deactivate_policy(). /Users/rubber/linux/block/blk-cgroup.c: 639
	/* /Users/rubber/linux/block/blk-cgroup.c: 657
	 * Create blkgs walking down from blkcg_root to @blkcg, so that all /Users/rubber/linux/block/blk-cgroup.c: 658
	 * non-root blkgs have access to their parents. /Users/rubber/linux/block/blk-cgroup.c: 659
	/* /Users/rubber/linux/block/blk-cgroup.c: 729
	 * If queue was bypassing, we should retry.  Do so after a /Users/rubber/linux/block/blk-cgroup.c: 730
	 * short msleep().  It isn't strictly necessary but queue /Users/rubber/linux/block/blk-cgroup.c: 731
	 * can be bypassing for some time and it's always nice to /Users/rubber/linux/block/blk-cgroup.c: 732
	 * avoid busy looping. /Users/rubber/linux/block/blk-cgroup.c: 733
 * blkg_conf_finish - finish up per-blkg config update /Users/rubber/linux/block/blk-cgroup.c: 744
 * @ctx: blkg_conf_ctx intiailized by blkg_conf_prep() /Users/rubber/linux/block/blk-cgroup.c: 745
 * Finish up after per-blkg config update.  This function must be paired /Users/rubber/linux/block/blk-cgroup.c: 747
 * with blkg_conf_prep(). /Users/rubber/linux/block/blk-cgroup.c: 748
 * We source root cgroup stats from the system-wide stats to avoid /Users/rubber/linux/block/blk-cgroup.c: 836
 * tracking the same information twice and incurring overhead when no /Users/rubber/linux/block/blk-cgroup.c: 837
 * cgroups are defined. For that reason, cgroup_rstat_flush in /Users/rubber/linux/block/blk-cgroup.c: 838
 * blkcg_print_stat does not actually fill out the iostat in the root /Users/rubber/linux/block/blk-cgroup.c: 839
 * cgroup's blkcg_gq. /Users/rubber/linux/block/blk-cgroup.c: 840
 * However, we would like to re-use the printing code between the root and /Users/rubber/linux/block/blk-cgroup.c: 842
 * non-root cgroups to the extent possible. For that reason, we simulate /Users/rubber/linux/block/blk-cgroup.c: 843
 * flushing the root cgroup's stats by explicitly filling in the iostat /Users/rubber/linux/block/blk-cgroup.c: 844
 * with disk level statistics. /Users/rubber/linux/block/blk-cgroup.c: 845
 convert sectors to bytes /Users/rubber/linux/block/blk-cgroup.c: 872
 * blkcg destruction is a three-stage process. /Users/rubber/linux/block/blk-cgroup.c: 981
 * 1. Destruction starts.  The blkcg_css_offline() callback is invoked /Users/rubber/linux/block/blk-cgroup.c: 983
 *    which offlines writeback.  Here we tie the next stage of blkg destruction /Users/rubber/linux/block/blk-cgroup.c: 984
 *    to the completion of writeback associated with the blkcg.  This lets us /Users/rubber/linux/block/blk-cgroup.c: 985
 *    avoid punting potentially large amounts of outstanding writeback to root /Users/rubber/linux/block/blk-cgroup.c: 986
 *    while maintaining any ongoing policies.  The next stage is triggered when /Users/rubber/linux/block/blk-cgroup.c: 987
 *    the nr_cgwbs count goes to zero. /Users/rubber/linux/block/blk-cgroup.c: 988
 * 2. When the nr_cgwbs count goes to zero, blkcg_destroy_blkgs() is called /Users/rubber/linux/block/blk-cgroup.c: 990
 *    and handles the destruction of blkgs.  Here the css reference held by /Users/rubber/linux/block/blk-cgroup.c: 991
 *    the blkg is put back eventually allowing blkcg_css_free() to be called. /Users/rubber/linux/block/blk-cgroup.c: 992
 *    This work may occur in cgwb_release_workfn() on the cgwb_release /Users/rubber/linux/block/blk-cgroup.c: 993
 *    workqueue.  Any submitted ios that fail to get the blkg ref will be /Users/rubber/linux/block/blk-cgroup.c: 994
 *    punted to the root_blkg. /Users/rubber/linux/block/blk-cgroup.c: 995
 * 3. Once the blkcg ref count goes to zero, blkcg_css_free() is called. /Users/rubber/linux/block/blk-cgroup.c: 997
 *    This finally frees the blkcg. /Users/rubber/linux/block/blk-cgroup.c: 998
 * blkcg_css_offline - cgroup css_offline callback /Users/rubber/linux/block/blk-cgroup.c: 1002
 * @css: css of interest /Users/rubber/linux/block/blk-cgroup.c: 1003
 * This function is called when @css is about to go away.  Here the cgwbs are /Users/rubber/linux/block/blk-cgroup.c: 1005
 * offlined first and only once writeback associated with the blkcg has /Users/rubber/linux/block/blk-cgroup.c: 1006
 * finished do we start step 2 (see above). /Users/rubber/linux/block/blk-cgroup.c: 1007
 * blkcg_destroy_blkgs - responsible for shooting down blkgs /Users/rubber/linux/block/blk-cgroup.c: 1021
 * @blkcg: blkcg of interest /Users/rubber/linux/block/blk-cgroup.c: 1022
 * blkgs should be removed while holding both q and blkcg locks.  As blkcg lock /Users/rubber/linux/block/blk-cgroup.c: 1024
 * is nested inside q lock, this function performs reverse double lock dancing. /Users/rubber/linux/block/blk-cgroup.c: 1025
 * Destroying the blkgs releases the reference held on the blkcg's css allowing /Users/rubber/linux/block/blk-cgroup.c: 1026
 * blkcg_css_free to eventually be called. /Users/rubber/linux/block/blk-cgroup.c: 1027
 * This is the blkcg counterpart of ioc_release_fn(). /Users/rubber/linux/block/blk-cgroup.c: 1029
			/* /Users/rubber/linux/block/blk-cgroup.c: 1043
			 * Given that the system can accumulate a huge number /Users/rubber/linux/block/blk-cgroup.c: 1044
			 * of blkgs in pathological cases, check to see if we /Users/rubber/linux/block/blk-cgroup.c: 1045
			 * need to rescheduling to avoid softlockup. /Users/rubber/linux/block/blk-cgroup.c: 1046
		/* /Users/rubber/linux/block/blk-cgroup.c: 1102
		 * If the policy hasn't been attached yet, wait for it /Users/rubber/linux/block/blk-cgroup.c: 1103
		 * to be attached before doing anything else. Otherwise, /Users/rubber/linux/block/blk-cgroup.c: 1104
		 * check if the policy requires any specific per-cgroup /Users/rubber/linux/block/blk-cgroup.c: 1105
		 * data: if it does, allocate and initialize it. /Users/rubber/linux/block/blk-cgroup.c: 1106
	/* /Users/rubber/linux/block/blk-cgroup.c: 1152
	 * blkcg_pin_online() is used to delay blkcg offline so that blkgs /Users/rubber/linux/block/blk-cgroup.c: 1153
	 * don't go offline while cgwbs are still active on them.  Pin the /Users/rubber/linux/block/blk-cgroup.c: 1154
	 * parent so that offline always happens towards the root. /Users/rubber/linux/block/blk-cgroup.c: 1155
 * blkcg_init_queue - initialize blkcg part of request queue /Users/rubber/linux/block/blk-cgroup.c: 1163
 * @q: request_queue to initialize /Users/rubber/linux/block/blk-cgroup.c: 1164
 * Called from blk_alloc_queue(). Responsible for initializing blkcg /Users/rubber/linux/block/blk-cgroup.c: 1166
 * part of new request_queue @q. /Users/rubber/linux/block/blk-cgroup.c: 1167
 * RETURNS: /Users/rubber/linux/block/blk-cgroup.c: 1169
 * 0 on success, -errno on failure. /Users/rubber/linux/block/blk-cgroup.c: 1170
 * blkcg_exit_queue - exit and release blkcg part of request_queue /Users/rubber/linux/block/blk-cgroup.c: 1225
 * @q: request_queue being released /Users/rubber/linux/block/blk-cgroup.c: 1226
 * Called from blk_exit_queue().  Responsible for exiting blkcg part. /Users/rubber/linux/block/blk-cgroup.c: 1228
	/* /Users/rubber/linux/block/blk-cgroup.c: 1275
	 * This ensures that, if available, memcg is automatically enabled /Users/rubber/linux/block/blk-cgroup.c: 1276
	 * together on the default hierarchy so that the owner cgroup can /Users/rubber/linux/block/blk-cgroup.c: 1277
	 * be retrieved from writeback pages. /Users/rubber/linux/block/blk-cgroup.c: 1278
 * blkcg_activate_policy - activate a blkcg policy on a request_queue /Users/rubber/linux/block/blk-cgroup.c: 1286
 * @q: request_queue of interest /Users/rubber/linux/block/blk-cgroup.c: 1287
 * @pol: blkcg policy to activate /Users/rubber/linux/block/blk-cgroup.c: 1288
 * Activate @pol on @q.  Requires %GFP_KERNEL context.  @q goes through /Users/rubber/linux/block/blk-cgroup.c: 1290
 * bypass mode to populate its blkgs with policy_data for @pol. /Users/rubber/linux/block/blk-cgroup.c: 1291
 * Activation happens with @q bypassed, so nobody would be accessing blkgs /Users/rubber/linux/block/blk-cgroup.c: 1293
 * from IO path.  Update of each blkg is protected by both queue and blkcg /Users/rubber/linux/block/blk-cgroup.c: 1294
 * locks so that holding either lock and testing blkcg_policy_enabled() is /Users/rubber/linux/block/blk-cgroup.c: 1295
 * always enough for dereferencing policy data. /Users/rubber/linux/block/blk-cgroup.c: 1296
 * The caller is responsible for synchronizing [de]activations and policy /Users/rubber/linux/block/blk-cgroup.c: 1298
 * [un]registerations.  Returns 0 on success, -errno on failure. /Users/rubber/linux/block/blk-cgroup.c: 1299
			/* /Users/rubber/linux/block/blk-cgroup.c: 1333
			 * GFP_NOWAIT failed.  Free the existing one and /Users/rubber/linux/block/blk-cgroup.c: 1334
			 * prealloc for @blkg w/ GFP_KERNEL. /Users/rubber/linux/block/blk-cgroup.c: 1335
 * blkcg_deactivate_policy - deactivate a blkcg policy on a request_queue /Users/rubber/linux/block/blk-cgroup.c: 1397
 * @q: request_queue of interest /Users/rubber/linux/block/blk-cgroup.c: 1398
 * @pol: blkcg policy to deactivate /Users/rubber/linux/block/blk-cgroup.c: 1399
 * Deactivate @pol on @q.  Follows the same synchronization rules as /Users/rubber/linux/block/blk-cgroup.c: 1401
 * blkcg_activate_policy(). /Users/rubber/linux/block/blk-cgroup.c: 1402
 * blkcg_policy_register - register a blkcg policy /Users/rubber/linux/block/blk-cgroup.c: 1440
 * @pol: blkcg policy to register /Users/rubber/linux/block/blk-cgroup.c: 1441
 * Register @pol with blkcg core.  Might sleep and @pol may be modified on /Users/rubber/linux/block/blk-cgroup.c: 1443
 * successful registration.  Returns 0 on success and -errno on failure. /Users/rubber/linux/block/blk-cgroup.c: 1444
 * blkcg_policy_unregister - unregister a blkcg policy /Users/rubber/linux/block/blk-cgroup.c: 1520
 * @pol: blkcg policy to unregister /Users/rubber/linux/block/blk-cgroup.c: 1521
 * Undo blkcg_policy_register(@pol).  Might sleep. /Users/rubber/linux/block/blk-cgroup.c: 1523
 * Scale the accumulated delay based on how long it has been since we updated /Users/rubber/linux/block/blk-cgroup.c: 1579
 * the delay.  We only call this when we are adding delay, in case it's been a /Users/rubber/linux/block/blk-cgroup.c: 1580
 * while since we added delay, and when we are checking to see if we need to /Users/rubber/linux/block/blk-cgroup.c: 1581
 * delay a task, to account for any delays that may have occurred. /Users/rubber/linux/block/blk-cgroup.c: 1582
	/* /Users/rubber/linux/block/blk-cgroup.c: 1592
	 * We only want to scale down every second.  The idea here is that we /Users/rubber/linux/block/blk-cgroup.c: 1593
	 * want to delay people for min(delay_nsec, NSEC_PER_SEC) in a certain /Users/rubber/linux/block/blk-cgroup.c: 1594
	 * time window.  We only want to throttle tasks for recent delay that /Users/rubber/linux/block/blk-cgroup.c: 1595
	 * has occurred, in 1 second time windows since that's the maximum /Users/rubber/linux/block/blk-cgroup.c: 1596
	 * things can be throttled.  We save the current delay window in /Users/rubber/linux/block/blk-cgroup.c: 1597
	 * blkg->last_delay so we know what amount is still left to be charged /Users/rubber/linux/block/blk-cgroup.c: 1598
	 * to the blkg from this point onward.  blkg->last_use keeps track of /Users/rubber/linux/block/blk-cgroup.c: 1599
	 * the use_delay counter.  The idea is if we're unthrottling the blkg we /Users/rubber/linux/block/blk-cgroup.c: 1600
	 * are ok with whatever is happening now, and we can take away more of /Users/rubber/linux/block/blk-cgroup.c: 1601
	 * the accumulated delay as we've already throttled enough that /Users/rubber/linux/block/blk-cgroup.c: 1602
	 * everybody is happy with their IO latencies. /Users/rubber/linux/block/blk-cgroup.c: 1603
		/* /Users/rubber/linux/block/blk-cgroup.c: 1611
		 * We've been unthrottled, subtract a larger chunk of our /Users/rubber/linux/block/blk-cgroup.c: 1612
		 * accumulated delay. /Users/rubber/linux/block/blk-cgroup.c: 1613
		/* /Users/rubber/linux/block/blk-cgroup.c: 1618
		 * This shouldn't happen, but handle it anyway.  Our delay_nsec /Users/rubber/linux/block/blk-cgroup.c: 1619
		 * should only ever be growing except here where we subtract out /Users/rubber/linux/block/blk-cgroup.c: 1620
		 * min(last_delay, 1 second), but lord knows bugs happen and I'd /Users/rubber/linux/block/blk-cgroup.c: 1621
		 * rather not end up with negative numbers. /Users/rubber/linux/block/blk-cgroup.c: 1622
 * This is called when we want to actually walk up the hierarchy and check to /Users/rubber/linux/block/blk-cgroup.c: 1636
 * see if we need to throttle, and then actually throttle if there is some /Users/rubber/linux/block/blk-cgroup.c: 1637
 * accumulated delay.  This should only be called upon return to user space so /Users/rubber/linux/block/blk-cgroup.c: 1638
 * we're not holding some lock that would induce a priority inversion. /Users/rubber/linux/block/blk-cgroup.c: 1639
	/* /Users/rubber/linux/block/blk-cgroup.c: 1669
	 * Let's not sleep for all eternity if we've amassed a huge delay. /Users/rubber/linux/block/blk-cgroup.c: 1670
	 * Swapping or metadata IO can accumulate 10's of seconds worth of /Users/rubber/linux/block/blk-cgroup.c: 1671
	 * delay, and we want userspace to be able to do _something_ so cap the /Users/rubber/linux/block/blk-cgroup.c: 1672
	 * delays at 0.25s. If there's 10's of seconds worth of delay then the /Users/rubber/linux/block/blk-cgroup.c: 1673
	 * tasks will be delayed for 0.25 second for every syscall. If /Users/rubber/linux/block/blk-cgroup.c: 1674
	 * blkcg_set_delay() was used as indicated by negative use_delay, the /Users/rubber/linux/block/blk-cgroup.c: 1675
	 * caller is responsible for regulating the range. /Users/rubber/linux/block/blk-cgroup.c: 1676
 * blkcg_maybe_throttle_current - throttle the current task if it has been marked /Users/rubber/linux/block/blk-cgroup.c: 1698
 * This is only called if we've been marked with set_notify_resume().  Obviously /Users/rubber/linux/block/blk-cgroup.c: 1700
 * we can be set_notify_resume() for reasons other than blkcg throttling, so we /Users/rubber/linux/block/blk-cgroup.c: 1701
 * check to see if current->throttle_queue is set and if not this doesn't do /Users/rubber/linux/block/blk-cgroup.c: 1702
 * anything.  This should only ever be called by the resume code, it's not meant /Users/rubber/linux/block/blk-cgroup.c: 1703
 * to be called by people willy-nilly as it will actually do the work to /Users/rubber/linux/block/blk-cgroup.c: 1704
 * throttle the task if it is setup for throttling. /Users/rubber/linux/block/blk-cgroup.c: 1705
 * blkcg_schedule_throttle - this task needs to check for throttling /Users/rubber/linux/block/blk-cgroup.c: 1747
 * @q: the request queue IO was submitted on /Users/rubber/linux/block/blk-cgroup.c: 1748
 * @use_memdelay: do we charge this to memory delay for PSI /Users/rubber/linux/block/blk-cgroup.c: 1749
 * This is called by the IO controller when we know there's delay accumulated /Users/rubber/linux/block/blk-cgroup.c: 1751
 * for the blkg for this task.  We do not pass the blkg because there are places /Users/rubber/linux/block/blk-cgroup.c: 1752
 * we call this that may not have that information, the swapping code for /Users/rubber/linux/block/blk-cgroup.c: 1753
 * instance will only have a request_queue at that point.  This set's the /Users/rubber/linux/block/blk-cgroup.c: 1754
 * notify_resume for the task to check and see if it requires throttling before /Users/rubber/linux/block/blk-cgroup.c: 1755
 * returning to user space. /Users/rubber/linux/block/blk-cgroup.c: 1756
 * We will only schedule once per syscall.  You can call this over and over /Users/rubber/linux/block/blk-cgroup.c: 1758
 * again and it will only do the check once upon return to user space, and only /Users/rubber/linux/block/blk-cgroup.c: 1759
 * throttle once.  If the task needs to be throttled again it'll need to be /Users/rubber/linux/block/blk-cgroup.c: 1760
 * re-set at the next time we see the task. /Users/rubber/linux/block/blk-cgroup.c: 1761
 * blkcg_add_delay - add delay to this blkg /Users/rubber/linux/block/blk-cgroup.c: 1783
 * @blkg: blkg of interest /Users/rubber/linux/block/blk-cgroup.c: 1784
 * @now: the current time in nanoseconds /Users/rubber/linux/block/blk-cgroup.c: 1785
 * @delta: how many nanoseconds of delay to add /Users/rubber/linux/block/blk-cgroup.c: 1786
 * Charge @delta to the blkg's current delay accumulation.  This is used to /Users/rubber/linux/block/blk-cgroup.c: 1788
 * throttle tasks if an IO controller thinks we need more throttling. /Users/rubber/linux/block/blk-cgroup.c: 1789
 * blkg_tryget_closest - try and get a blkg ref on the closet blkg /Users/rubber/linux/block/blk-cgroup.c: 1800
 * @bio: target bio /Users/rubber/linux/block/blk-cgroup.c: 1801
 * @css: target css /Users/rubber/linux/block/blk-cgroup.c: 1802
 * As the failure mode here is to walk up the blkg tree, this ensure that the /Users/rubber/linux/block/blk-cgroup.c: 1804
 * blkg->parent pointers are always valid.  This returns the blkg that it ended /Users/rubber/linux/block/blk-cgroup.c: 1805
 * up taking a reference on or %NULL if no reference was taken. /Users/rubber/linux/block/blk-cgroup.c: 1806
 * bio_associate_blkg_from_css - associate a bio with a specified css /Users/rubber/linux/block/blk-cgroup.c: 1829
 * @bio: target bio /Users/rubber/linux/block/blk-cgroup.c: 1830
 * @css: target css /Users/rubber/linux/block/blk-cgroup.c: 1831
 * Associate @bio with the blkg found by combining the css's blkg and the /Users/rubber/linux/block/blk-cgroup.c: 1833
 * request_queue of the @bio.  An association failure is handled by walking up /Users/rubber/linux/block/blk-cgroup.c: 1834
 * the blkg tree.  Therefore, the blkg associated can be anything between @blkg /Users/rubber/linux/block/blk-cgroup.c: 1835
 * and q->root_blkg.  This situation only happens when a cgroup is dying and /Users/rubber/linux/block/blk-cgroup.c: 1836
 * then the remaining bios will spill to the closest alive blkg. /Users/rubber/linux/block/blk-cgroup.c: 1837
 * A reference will be taken on the blkg and will be released when @bio is /Users/rubber/linux/block/blk-cgroup.c: 1839
 * freed. /Users/rubber/linux/block/blk-cgroup.c: 1840
 * bio_associate_blkg - associate a bio with a blkg /Users/rubber/linux/block/blk-cgroup.c: 1858
 * @bio: target bio /Users/rubber/linux/block/blk-cgroup.c: 1859
 * Associate @bio with the blkg found from the bio's css and request_queue. /Users/rubber/linux/block/blk-cgroup.c: 1861
 * If one is not found, bio_lookup_blkg() creates the blkg.  If a blkg is /Users/rubber/linux/block/blk-cgroup.c: 1862
 * already associated, the css is reused and association redone as the /Users/rubber/linux/block/blk-cgroup.c: 1863
 * request_queue may have changed. /Users/rubber/linux/block/blk-cgroup.c: 1864
 * bio_clone_blkg_association - clone blkg association from src to dst bio /Users/rubber/linux/block/blk-cgroup.c: 1884
 * @dst: destination bio /Users/rubber/linux/block/blk-cgroup.c: 1885
 * @src: source bio /Users/rubber/linux/block/blk-cgroup.c: 1886
	/* /Users/rubber/linux/block/blk-cgroup.c: 1918
	 * If the bio is flagged with BIO_CGROUP_ACCT it means this is a split /Users/rubber/linux/block/blk-cgroup.c: 1919
	 * bio and we would have already accounted for the size of the bio. /Users/rubber/linux/block/blk-cgroup.c: 1920
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/bsg.c: 1
 * bsg.c - block layer implementation of the sg v4 interface /Users/rubber/linux/block/bsg.c: 3
	/* /Users/rubber/linux/block/bsg.c: 111
	 * Our own ioctls /Users/rubber/linux/block/bsg.c: 112
	/* /Users/rubber/linux/block/bsg.c: 119
	 * SCSI/sg ioctls /Users/rubber/linux/block/bsg.c: 120
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/bfq-iosched.c: 1
 * Budget Fair Queueing (BFQ) I/O scheduler. /Users/rubber/linux/block/bfq-iosched.c: 3
 * Based on ideas and code from CFQ: /Users/rubber/linux/block/bfq-iosched.c: 5
 * Copyright (C) 2003 Jens Axboe <axboe@kernel.dk> /Users/rubber/linux/block/bfq-iosched.c: 6
 * Copyright (C) 2008 Fabio Checconi <fabio@gandalf.sssup.it> /Users/rubber/linux/block/bfq-iosched.c: 8
 *		      Paolo Valente <paolo.valente@unimore.it> /Users/rubber/linux/block/bfq-iosched.c: 9
 * Copyright (C) 2010 Paolo Valente <paolo.valente@unimore.it> /Users/rubber/linux/block/bfq-iosched.c: 11
 *                    Arianna Avanzini <avanzini@google.com> /Users/rubber/linux/block/bfq-iosched.c: 12
 * Copyright (C) 2017 Paolo Valente <paolo.valente@linaro.org> /Users/rubber/linux/block/bfq-iosched.c: 14
 * BFQ is a proportional-share I/O scheduler, with some extra /Users/rubber/linux/block/bfq-iosched.c: 16
 * low-latency capabilities. BFQ also supports full hierarchical /Users/rubber/linux/block/bfq-iosched.c: 17
 * scheduling through cgroups. Next paragraphs provide an introduction /Users/rubber/linux/block/bfq-iosched.c: 18
 * on BFQ inner workings. Details on BFQ benefits, usage and /Users/rubber/linux/block/bfq-iosched.c: 19
 * limitations can be found in Documentation/block/bfq-iosched.rst. /Users/rubber/linux/block/bfq-iosched.c: 20
 * BFQ is a proportional-share storage-I/O scheduling algorithm based /Users/rubber/linux/block/bfq-iosched.c: 22
 * on the slice-by-slice service scheme of CFQ. But BFQ assigns /Users/rubber/linux/block/bfq-iosched.c: 23
 * budgets, measured in number of sectors, to processes instead of /Users/rubber/linux/block/bfq-iosched.c: 24
 * time slices. The device is not granted to the in-service process /Users/rubber/linux/block/bfq-iosched.c: 25
 * for a given time slice, but until it has exhausted its assigned /Users/rubber/linux/block/bfq-iosched.c: 26
 * budget. This change from the time to the service domain enables BFQ /Users/rubber/linux/block/bfq-iosched.c: 27
 * to distribute the device throughput among processes as desired, /Users/rubber/linux/block/bfq-iosched.c: 28
 * without any distortion due to throughput fluctuations, or to device /Users/rubber/linux/block/bfq-iosched.c: 29
 * internal queueing. BFQ uses an ad hoc internal scheduler, called /Users/rubber/linux/block/bfq-iosched.c: 30
 * B-WF2Q+, to schedule processes according to their budgets. More /Users/rubber/linux/block/bfq-iosched.c: 31
 * precisely, BFQ schedules queues associated with processes. Each /Users/rubber/linux/block/bfq-iosched.c: 32
 * process/queue is assigned a user-configurable weight, and B-WF2Q+ /Users/rubber/linux/block/bfq-iosched.c: 33
 * guarantees that each queue receives a fraction of the throughput /Users/rubber/linux/block/bfq-iosched.c: 34
 * proportional to its weight. Thanks to the accurate policy of /Users/rubber/linux/block/bfq-iosched.c: 35
 * B-WF2Q+, BFQ can afford to assign high budgets to I/O-bound /Users/rubber/linux/block/bfq-iosched.c: 36
 * processes issuing sequential requests (to boost the throughput), /Users/rubber/linux/block/bfq-iosched.c: 37
 * and yet guarantee a low latency to interactive and soft real-time /Users/rubber/linux/block/bfq-iosched.c: 38
 * applications. /Users/rubber/linux/block/bfq-iosched.c: 39
 * In particular, to provide these low-latency guarantees, BFQ /Users/rubber/linux/block/bfq-iosched.c: 41
 * explicitly privileges the I/O of two classes of time-sensitive /Users/rubber/linux/block/bfq-iosched.c: 42
 * applications: interactive and soft real-time. In more detail, BFQ /Users/rubber/linux/block/bfq-iosched.c: 43
 * behaves this way if the low_latency parameter is set (default /Users/rubber/linux/block/bfq-iosched.c: 44
 * configuration). This feature enables BFQ to provide applications in /Users/rubber/linux/block/bfq-iosched.c: 45
 * these classes with a very low latency. /Users/rubber/linux/block/bfq-iosched.c: 46
 * To implement this feature, BFQ constantly tries to detect whether /Users/rubber/linux/block/bfq-iosched.c: 48
 * the I/O requests in a bfq_queue come from an interactive or a soft /Users/rubber/linux/block/bfq-iosched.c: 49
 * real-time application. For brevity, in these cases, the queue is /Users/rubber/linux/block/bfq-iosched.c: 50
 * said to be interactive or soft real-time. In both cases, BFQ /Users/rubber/linux/block/bfq-iosched.c: 51
 * privileges the service of the queue, over that of non-interactive /Users/rubber/linux/block/bfq-iosched.c: 52
 * and non-soft-real-time queues. This privileging is performed, /Users/rubber/linux/block/bfq-iosched.c: 53
 * mainly, by raising the weight of the queue. So, for brevity, we /Users/rubber/linux/block/bfq-iosched.c: 54
 * call just weight-raising periods the time periods during which a /Users/rubber/linux/block/bfq-iosched.c: 55
 * queue is privileged, because deemed interactive or soft real-time. /Users/rubber/linux/block/bfq-iosched.c: 56
 * The detection of soft real-time queues/applications is described in /Users/rubber/linux/block/bfq-iosched.c: 58
 * detail in the comments on the function /Users/rubber/linux/block/bfq-iosched.c: 59
 * bfq_bfqq_softrt_next_start. On the other hand, the detection of an /Users/rubber/linux/block/bfq-iosched.c: 60
 * interactive queue works as follows: a queue is deemed interactive /Users/rubber/linux/block/bfq-iosched.c: 61
 * if it is constantly non empty only for a limited time interval, /Users/rubber/linux/block/bfq-iosched.c: 62
 * after which it does become empty. The queue may be deemed /Users/rubber/linux/block/bfq-iosched.c: 63
 * interactive again (for a limited time), if it restarts being /Users/rubber/linux/block/bfq-iosched.c: 64
 * constantly non empty, provided that this happens only after the /Users/rubber/linux/block/bfq-iosched.c: 65
 * queue has remained empty for a given minimum idle time. /Users/rubber/linux/block/bfq-iosched.c: 66
 * By default, BFQ computes automatically the above maximum time /Users/rubber/linux/block/bfq-iosched.c: 68
 * interval, i.e., the time interval after which a constantly /Users/rubber/linux/block/bfq-iosched.c: 69
 * non-empty queue stops being deemed interactive. Since a queue is /Users/rubber/linux/block/bfq-iosched.c: 70
 * weight-raised while it is deemed interactive, this maximum time /Users/rubber/linux/block/bfq-iosched.c: 71
 * interval happens to coincide with the (maximum) duration of the /Users/rubber/linux/block/bfq-iosched.c: 72
 * weight-raising for interactive queues. /Users/rubber/linux/block/bfq-iosched.c: 73
 * Finally, BFQ also features additional heuristics for /Users/rubber/linux/block/bfq-iosched.c: 75
 * preserving both a low latency and a high throughput on NCQ-capable, /Users/rubber/linux/block/bfq-iosched.c: 76
 * rotational or flash-based devices, and to get the job done quickly /Users/rubber/linux/block/bfq-iosched.c: 77
 * for applications consisting in many I/O-bound processes. /Users/rubber/linux/block/bfq-iosched.c: 78
 * NOTE: if the main or only goal, with a given device, is to achieve /Users/rubber/linux/block/bfq-iosched.c: 80
 * the maximum-possible throughput at all times, then do switch off /Users/rubber/linux/block/bfq-iosched.c: 81
 * all low-latency heuristics for that device, by setting low_latency /Users/rubber/linux/block/bfq-iosched.c: 82
 * to 0. /Users/rubber/linux/block/bfq-iosched.c: 83
 * BFQ is described in [1], where also a reference to the initial, /Users/rubber/linux/block/bfq-iosched.c: 85
 * more theoretical paper on BFQ can be found. The interested reader /Users/rubber/linux/block/bfq-iosched.c: 86
 * can find in the latter paper full details on the main algorithm, as /Users/rubber/linux/block/bfq-iosched.c: 87
 * well as formulas of the guarantees and formal proofs of all the /Users/rubber/linux/block/bfq-iosched.c: 88
 * properties.  With respect to the version of BFQ presented in these /Users/rubber/linux/block/bfq-iosched.c: 89
 * papers, this implementation adds a few more heuristics, such as the /Users/rubber/linux/block/bfq-iosched.c: 90
 * ones that guarantee a low latency to interactive and soft real-time /Users/rubber/linux/block/bfq-iosched.c: 91
 * applications, and a hierarchical extension based on H-WF2Q+. /Users/rubber/linux/block/bfq-iosched.c: 92
 * B-WF2Q+ is based on WF2Q+, which is described in [2], together with /Users/rubber/linux/block/bfq-iosched.c: 94
 * H-WF2Q+, while the augmented tree used here to implement B-WF2Q+ /Users/rubber/linux/block/bfq-iosched.c: 95
 * with O(log N) complexity derives from the one introduced with EEVDF /Users/rubber/linux/block/bfq-iosched.c: 96
 * in [3]. /Users/rubber/linux/block/bfq-iosched.c: 97
 * [1] P. Valente, A. Avanzini, "Evolution of the BFQ Storage I/O /Users/rubber/linux/block/bfq-iosched.c: 99
 *     Scheduler", Proceedings of the First Workshop on Mobile System /Users/rubber/linux/block/bfq-iosched.c: 100
 *     Technologies (MST-2015), May 2015. /Users/rubber/linux/block/bfq-iosched.c: 101
 *     http://algogroup.unimore.it/people/paolo/disk_sched/mst-2015.pdf /Users/rubber/linux/block/bfq-iosched.c: 102
 * [2] Jon C.R. Bennett and H. Zhang, "Hierarchical Packet Fair Queueing /Users/rubber/linux/block/bfq-iosched.c: 104
 *     Algorithms", IEEE/ACM Transactions on Networking, 5(5):675-689, /Users/rubber/linux/block/bfq-iosched.c: 105
 *     Oct 1997. /Users/rubber/linux/block/bfq-iosched.c: 106
 * http://www.cs.cmu.edu/~hzhang/papers/TON-97-Oct.ps.gz /Users/rubber/linux/block/bfq-iosched.c: 108
 * [3] I. Stoica and H. Abdel-Wahab, "Earliest Eligible Virtual Deadline /Users/rubber/linux/block/bfq-iosched.c: 110
 *     First: A Flexible and Accurate Mechanism for Proportional Share /Users/rubber/linux/block/bfq-iosched.c: 111
 *     Resource Allocation", technical report. /Users/rubber/linux/block/bfq-iosched.c: 112
 * http://www.cs.berkeley.edu/~istoica/papers/eevdf-tr-95.pdf /Users/rubber/linux/block/bfq-iosched.c: 114
 * When a sync request is dispatched, the queue that contains that /Users/rubber/linux/block/bfq-iosched.c: 184
 * request, and all the ancestor entities of that queue, are charged /Users/rubber/linux/block/bfq-iosched.c: 185
 * with the number of sectors of the request. In contrast, if the /Users/rubber/linux/block/bfq-iosched.c: 186
 * request is async, then the queue and its ancestor entities are /Users/rubber/linux/block/bfq-iosched.c: 187
 * charged with the number of sectors of the request, multiplied by /Users/rubber/linux/block/bfq-iosched.c: 188
 * the factor below. This throttles the bandwidth for async I/O, /Users/rubber/linux/block/bfq-iosched.c: 189
 * w.r.t. to sync I/O, and it is done to counter the tendency of async /Users/rubber/linux/block/bfq-iosched.c: 190
 * writes to steal I/O throughput to reads. /Users/rubber/linux/block/bfq-iosched.c: 191
 * The current value of this parameter is the result of a tuning with /Users/rubber/linux/block/bfq-iosched.c: 193
 * several hardware and software configurations. We tried to find the /Users/rubber/linux/block/bfq-iosched.c: 194
 * lowest value for which writes do not cause noticeable problems to /Users/rubber/linux/block/bfq-iosched.c: 195
 * reads. In fact, the lower this parameter, the stabler I/O control, /Users/rubber/linux/block/bfq-iosched.c: 196
 * in the following respect.  The lower this parameter is, the less /Users/rubber/linux/block/bfq-iosched.c: 197
 * the bandwidth enjoyed by a group decreases /Users/rubber/linux/block/bfq-iosched.c: 198
 * - when the group does writes, w.r.t. to when it does reads; /Users/rubber/linux/block/bfq-iosched.c: 199
 * - when other groups do reads, w.r.t. to when they do writes. /Users/rubber/linux/block/bfq-iosched.c: 200
 * Time limit for merging (see comments in bfq_setup_cooperator). Set /Users/rubber/linux/block/bfq-iosched.c: 208
 * to the slowest value that, in our tests, proved to be effective in /Users/rubber/linux/block/bfq-iosched.c: 209
 * removing false positives, while not causing true positives to miss /Users/rubber/linux/block/bfq-iosched.c: 210
 * queue merging. /Users/rubber/linux/block/bfq-iosched.c: 211
 * As can be deduced from the low time limit below, queue merging, if /Users/rubber/linux/block/bfq-iosched.c: 213
 * successful, happens at the very beginning of the I/O of the involved /Users/rubber/linux/block/bfq-iosched.c: 214
 * cooperating processes, as a consequence of the arrival of the very /Users/rubber/linux/block/bfq-iosched.c: 215
 * first requests from each cooperator.  After that, there is very /Users/rubber/linux/block/bfq-iosched.c: 216
 * little chance to find cooperators. /Users/rubber/linux/block/bfq-iosched.c: 217
 * Sync random I/O is likely to be confused with soft real-time I/O, /Users/rubber/linux/block/bfq-iosched.c: 240
 * because it is characterized by limited throughput and apparently /Users/rubber/linux/block/bfq-iosched.c: 241
 * isochronous arrival pattern. To avoid false positives, queues /Users/rubber/linux/block/bfq-iosched.c: 242
 * containing only random (seeky) I/O are prevented from being tagged /Users/rubber/linux/block/bfq-iosched.c: 243
 * as soft real-time. /Users/rubber/linux/block/bfq-iosched.c: 244
 * Shift used for peak-rate fixed precision calculations. /Users/rubber/linux/block/bfq-iosched.c: 256
 * With /Users/rubber/linux/block/bfq-iosched.c: 257
 * - the current shift: 16 positions /Users/rubber/linux/block/bfq-iosched.c: 258
 * - the current type used to store rate: u32 /Users/rubber/linux/block/bfq-iosched.c: 259
 * - the current unit of measure for rate: [sectors/usec], or, more precisely, /Users/rubber/linux/block/bfq-iosched.c: 260
 *   [(sectors/usec) / 2^BFQ_RATE_SHIFT] to take into account the shift, /Users/rubber/linux/block/bfq-iosched.c: 261
 * the range of rates that can be stored is /Users/rubber/linux/block/bfq-iosched.c: 262
 * [1 / 2^BFQ_RATE_SHIFT, 2^(32 - BFQ_RATE_SHIFT)] sectors/usec = /Users/rubber/linux/block/bfq-iosched.c: 263
 * [1 / 2^16, 2^16] sectors/usec = [15e-6, 65536] sectors/usec = /Users/rubber/linux/block/bfq-iosched.c: 264
 * [15, 65G] sectors/sec /Users/rubber/linux/block/bfq-iosched.c: 265
 * Which, assuming a sector size of 512B, corresponds to a range of /Users/rubber/linux/block/bfq-iosched.c: 266
 * [7.5K, 33T] B/sec /Users/rubber/linux/block/bfq-iosched.c: 267
 * When configured for computing the duration of the weight-raising /Users/rubber/linux/block/bfq-iosched.c: 272
 * for interactive queues automatically (see the comments at the /Users/rubber/linux/block/bfq-iosched.c: 273
 * beginning of this file), BFQ does it using the following formula: /Users/rubber/linux/block/bfq-iosched.c: 274
 * duration = (ref_rate / r) * ref_wr_duration, /Users/rubber/linux/block/bfq-iosched.c: 275
 * where r is the peak rate of the device, and ref_rate and /Users/rubber/linux/block/bfq-iosched.c: 276
 * ref_wr_duration are two reference parameters.  In particular, /Users/rubber/linux/block/bfq-iosched.c: 277
 * ref_rate is the peak rate of the reference storage device (see /Users/rubber/linux/block/bfq-iosched.c: 278
 * below), and ref_wr_duration is about the maximum time needed, with /Users/rubber/linux/block/bfq-iosched.c: 279
 * BFQ and while reading two files in parallel, to load typical large /Users/rubber/linux/block/bfq-iosched.c: 280
 * applications on the reference device (see the comments on /Users/rubber/linux/block/bfq-iosched.c: 281
 * max_service_from_wr below, for more details on how ref_wr_duration /Users/rubber/linux/block/bfq-iosched.c: 282
 * is obtained).  In practice, the slower/faster the device at hand /Users/rubber/linux/block/bfq-iosched.c: 283
 * is, the more/less it takes to load applications with respect to the /Users/rubber/linux/block/bfq-iosched.c: 284
 * reference device.  Accordingly, the longer/shorter BFQ grants /Users/rubber/linux/block/bfq-iosched.c: 285
 * weight raising to interactive applications. /Users/rubber/linux/block/bfq-iosched.c: 286
 * BFQ uses two different reference pairs (ref_rate, ref_wr_duration), /Users/rubber/linux/block/bfq-iosched.c: 288
 * depending on whether the device is rotational or non-rotational. /Users/rubber/linux/block/bfq-iosched.c: 289
 * In the following definitions, ref_rate[0] and ref_wr_duration[0] /Users/rubber/linux/block/bfq-iosched.c: 291
 * are the reference values for a rotational device, whereas /Users/rubber/linux/block/bfq-iosched.c: 292
 * ref_rate[1] and ref_wr_duration[1] are the reference values for a /Users/rubber/linux/block/bfq-iosched.c: 293
 * non-rotational device. The reference rates are not the actual peak /Users/rubber/linux/block/bfq-iosched.c: 294
 * rates of the devices used as a reference, but slightly lower /Users/rubber/linux/block/bfq-iosched.c: 295
 * values. The reason for using slightly lower values is that the /Users/rubber/linux/block/bfq-iosched.c: 296
 * peak-rate estimator tends to yield slightly lower values than the /Users/rubber/linux/block/bfq-iosched.c: 297
 * actual peak rate (it can yield the actual peak rate only if there /Users/rubber/linux/block/bfq-iosched.c: 298
 * is only one process doing I/O, and the process does sequential /Users/rubber/linux/block/bfq-iosched.c: 299
 * I/O). /Users/rubber/linux/block/bfq-iosched.c: 300
 * The reference peak rates are measured in sectors/usec, left-shifted /Users/rubber/linux/block/bfq-iosched.c: 302
 * by BFQ_RATE_SHIFT. /Users/rubber/linux/block/bfq-iosched.c: 303
 * To improve readability, a conversion function is used to initialize /Users/rubber/linux/block/bfq-iosched.c: 307
 * the following array, which entails that the array can be /Users/rubber/linux/block/bfq-iosched.c: 308
 * initialized only in a function. /Users/rubber/linux/block/bfq-iosched.c: 309
 * BFQ uses the above-detailed, time-based weight-raising mechanism to /Users/rubber/linux/block/bfq-iosched.c: 314
 * privilege interactive tasks. This mechanism is vulnerable to the /Users/rubber/linux/block/bfq-iosched.c: 315
 * following false positives: I/O-bound applications that will go on /Users/rubber/linux/block/bfq-iosched.c: 316
 * doing I/O for much longer than the duration of weight /Users/rubber/linux/block/bfq-iosched.c: 317
 * raising. These applications have basically no benefit from being /Users/rubber/linux/block/bfq-iosched.c: 318
 * weight-raised at the beginning of their I/O. On the opposite end, /Users/rubber/linux/block/bfq-iosched.c: 319
 * while being weight-raised, these applications /Users/rubber/linux/block/bfq-iosched.c: 320
 * a) unjustly steal throughput to applications that may actually need /Users/rubber/linux/block/bfq-iosched.c: 321
 * low latency; /Users/rubber/linux/block/bfq-iosched.c: 322
 * b) make BFQ uselessly perform device idling; device idling results /Users/rubber/linux/block/bfq-iosched.c: 323
 * in loss of device throughput with most flash-based storage, and may /Users/rubber/linux/block/bfq-iosched.c: 324
 * increase latencies when used purposelessly. /Users/rubber/linux/block/bfq-iosched.c: 325
 * BFQ tries to reduce these problems, by adopting the following /Users/rubber/linux/block/bfq-iosched.c: 327
 * countermeasure. To introduce this countermeasure, we need first to /Users/rubber/linux/block/bfq-iosched.c: 328
 * finish explaining how the duration of weight-raising for /Users/rubber/linux/block/bfq-iosched.c: 329
 * interactive tasks is computed. /Users/rubber/linux/block/bfq-iosched.c: 330
 * For a bfq_queue deemed as interactive, the duration of weight /Users/rubber/linux/block/bfq-iosched.c: 332
 * raising is dynamically adjusted, as a function of the estimated /Users/rubber/linux/block/bfq-iosched.c: 333
 * peak rate of the device, so as to be equal to the time needed to /Users/rubber/linux/block/bfq-iosched.c: 334
 * execute the 'largest' interactive task we benchmarked so far. By /Users/rubber/linux/block/bfq-iosched.c: 335
 * largest task, we mean the task for which each involved process has /Users/rubber/linux/block/bfq-iosched.c: 336
 * to do more I/O than for any of the other tasks we benchmarked. This /Users/rubber/linux/block/bfq-iosched.c: 337
 * reference interactive task is the start-up of LibreOffice Writer, /Users/rubber/linux/block/bfq-iosched.c: 338
 * and in this task each process/bfq_queue needs to have at most ~110K /Users/rubber/linux/block/bfq-iosched.c: 339
 * sectors transferred. /Users/rubber/linux/block/bfq-iosched.c: 340
 * This last piece of information enables BFQ to reduce the actual /Users/rubber/linux/block/bfq-iosched.c: 342
 * duration of weight-raising for at least one class of I/O-bound /Users/rubber/linux/block/bfq-iosched.c: 343
 * applications: those doing sequential or quasi-sequential I/O. An /Users/rubber/linux/block/bfq-iosched.c: 344
 * example is file copy. In fact, once started, the main I/O-bound /Users/rubber/linux/block/bfq-iosched.c: 345
 * processes of these applications usually consume the above 110K /Users/rubber/linux/block/bfq-iosched.c: 346
 * sectors in much less time than the processes of an application that /Users/rubber/linux/block/bfq-iosched.c: 347
 * is starting, because these I/O-bound processes will greedily devote /Users/rubber/linux/block/bfq-iosched.c: 348
 * almost all their CPU cycles only to their target, /Users/rubber/linux/block/bfq-iosched.c: 349
 * throughput-friendly I/O operations. This is even more true if BFQ /Users/rubber/linux/block/bfq-iosched.c: 350
 * happens to be underestimating the device peak rate, and thus /Users/rubber/linux/block/bfq-iosched.c: 351
 * overestimating the duration of weight raising. But, according to /Users/rubber/linux/block/bfq-iosched.c: 352
 * our measurements, once transferred 110K sectors, these processes /Users/rubber/linux/block/bfq-iosched.c: 353
 * have no right to be weight-raised any longer. /Users/rubber/linux/block/bfq-iosched.c: 354
 * Basing on the last consideration, BFQ ends weight-raising for a /Users/rubber/linux/block/bfq-iosched.c: 356
 * bfq_queue if the latter happens to have received an amount of /Users/rubber/linux/block/bfq-iosched.c: 357
 * service at least equal to the following constant. The constant is /Users/rubber/linux/block/bfq-iosched.c: 358
 * set to slightly more than 110K, to have a minimum safety margin. /Users/rubber/linux/block/bfq-iosched.c: 359
 * This early ending of weight-raising reduces the amount of time /Users/rubber/linux/block/bfq-iosched.c: 361
 * during which interactive false positives cause the two problems /Users/rubber/linux/block/bfq-iosched.c: 362
 * described at the beginning of these comments. /Users/rubber/linux/block/bfq-iosched.c: 363
 * Maximum time between the creation of two queues, for stable merge /Users/rubber/linux/block/bfq-iosched.c: 368
 * to be activated (in ms) /Users/rubber/linux/block/bfq-iosched.c: 369
 * Minimum time to be waited before evaluating delayed stable merge (in ms) /Users/rubber/linux/block/bfq-iosched.c: 373
	/* /Users/rubber/linux/block/bfq-iosched.c: 389
	 * If bfqq != NULL, then a non-stable queue merge between /Users/rubber/linux/block/bfq-iosched.c: 390
	 * bic->bfqq and bfqq is happening here. This causes troubles /Users/rubber/linux/block/bfq-iosched.c: 391
	 * in the following case: bic->bfqq has also been scheduled /Users/rubber/linux/block/bfq-iosched.c: 392
	 * for a possible stable merge with bic->stable_merge_bfqq, /Users/rubber/linux/block/bfq-iosched.c: 393
	 * and bic->stable_merge_bfqq == bfqq happens to /Users/rubber/linux/block/bfq-iosched.c: 394
	 * hold. Troubles occur because bfqq may then undergo a split, /Users/rubber/linux/block/bfq-iosched.c: 395
	 * thereby becoming eligible for a stable merge. Yet, if /Users/rubber/linux/block/bfq-iosched.c: 396
	 * bic->stable_merge_bfqq points exactly to bfqq, then bfqq /Users/rubber/linux/block/bfq-iosched.c: 397
	 * would be stably merged with itself. To avoid this anomaly, /Users/rubber/linux/block/bfq-iosched.c: 398
	 * we cancel the stable merge if /Users/rubber/linux/block/bfq-iosched.c: 399
	 * bic->stable_merge_bfqq == bfqq. /Users/rubber/linux/block/bfq-iosched.c: 400
		/* /Users/rubber/linux/block/bfq-iosched.c: 405
		 * Actually, these same instructions are executed also /Users/rubber/linux/block/bfq-iosched.c: 406
		 * in bfq_setup_cooperator, in case of abort or actual /Users/rubber/linux/block/bfq-iosched.c: 407
		 * execution of a stable merge. We could avoid /Users/rubber/linux/block/bfq-iosched.c: 408
		 * repeating these instructions there too, but if we /Users/rubber/linux/block/bfq-iosched.c: 409
		 * did so, we would nest even more complexity in this /Users/rubber/linux/block/bfq-iosched.c: 410
		 * function. /Users/rubber/linux/block/bfq-iosched.c: 411
 * icq_to_bic - convert iocontext queue structure to bfq_io_cq. /Users/rubber/linux/block/bfq-iosched.c: 425
 * @icq: the iocontext queue. /Users/rubber/linux/block/bfq-iosched.c: 426
 * bfq_bic_lookup - search into @ioc a bic associated to @bfqd. /Users/rubber/linux/block/bfq-iosched.c: 435
 * @bfqd: the lookup key. /Users/rubber/linux/block/bfq-iosched.c: 436
 * @ioc: the io_context of the process doing I/O. /Users/rubber/linux/block/bfq-iosched.c: 437
 * @q: the request queue. /Users/rubber/linux/block/bfq-iosched.c: 438
 * Scheduler run of queue, if there are requests pending and no one in the /Users/rubber/linux/block/bfq-iosched.c: 459
 * driver that will restart queueing. /Users/rubber/linux/block/bfq-iosched.c: 460
 * Lifted from AS - choose which of rq1 and rq2 that is best served now. /Users/rubber/linux/block/bfq-iosched.c: 475
 * We choose the request that is closer to the head right now.  Distance /Users/rubber/linux/block/bfq-iosched.c: 476
 * behind the head is penalized and only allowed to a certain extent. /Users/rubber/linux/block/bfq-iosched.c: 477
	/* /Users/rubber/linux/block/bfq-iosched.c: 507
	 * By definition, 1KiB is 2 sectors. /Users/rubber/linux/block/bfq-iosched.c: 508
	/* /Users/rubber/linux/block/bfq-iosched.c: 512
	 * Strict one way elevator _except_ in the case where we allow /Users/rubber/linux/block/bfq-iosched.c: 513
	 * short backward seeks which are biased as twice the cost of a /Users/rubber/linux/block/bfq-iosched.c: 514
	 * similar forward seek. /Users/rubber/linux/block/bfq-iosched.c: 515
	/* /Users/rubber/linux/block/bfq-iosched.c: 533
	 * By doing switch() on the bit mask "wrap" we avoid having to /Users/rubber/linux/block/bfq-iosched.c: 534
	 * check two variables for all permutations: --> faster! /Users/rubber/linux/block/bfq-iosched.c: 535
		/* /Users/rubber/linux/block/bfq-iosched.c: 555
		 * Since both rqs are wrapped, /Users/rubber/linux/block/bfq-iosched.c: 556
		 * start with the one that's further behind head /Users/rubber/linux/block/bfq-iosched.c: 557
		 * (--> only *one* back seek required), /Users/rubber/linux/block/bfq-iosched.c: 558
		 * since back seek takes more time than forward. /Users/rubber/linux/block/bfq-iosched.c: 559
 * Async I/O can easily starve sync I/O (both sync reads and sync /Users/rubber/linux/block/bfq-iosched.c: 569
 * writes), by consuming all tags. Similarly, storms of sync writes, /Users/rubber/linux/block/bfq-iosched.c: 570
 * such as those that sync(2) may trigger, can starve sync reads. /Users/rubber/linux/block/bfq-iosched.c: 571
 * Limit depths of async I/O and sync writes so as to counter both /Users/rubber/linux/block/bfq-iosched.c: 572
 * problems. /Users/rubber/linux/block/bfq-iosched.c: 573
		/* /Users/rubber/linux/block/bfq-iosched.c: 606
		 * Sort strictly based on sector. Smallest to the left, /Users/rubber/linux/block/bfq-iosched.c: 607
		 * largest to the right. /Users/rubber/linux/block/bfq-iosched.c: 608
 * The following function is not marked as __cold because it is /Users/rubber/linux/block/bfq-iosched.c: 639
 * actually cold, but for the same performance goal described in the /Users/rubber/linux/block/bfq-iosched.c: 640
 * comments on the likely() at the beginning of /Users/rubber/linux/block/bfq-iosched.c: 641
 * bfq_setup_cooperator(). Unexpectedly, to reach an even lower /Users/rubber/linux/block/bfq-iosched.c: 642
 * execution time for the case where this function is not invoked, we /Users/rubber/linux/block/bfq-iosched.c: 643
 * had to add an unlikely() in each involved if(). /Users/rubber/linux/block/bfq-iosched.c: 644
	/* /Users/rubber/linux/block/bfq-iosched.c: 661
	 * bfqq cannot be merged any longer (see comments in /Users/rubber/linux/block/bfq-iosched.c: 662
	 * bfq_setup_cooperator): no point in adding bfqq into the /Users/rubber/linux/block/bfq-iosched.c: 663
	 * position tree. /Users/rubber/linux/block/bfq-iosched.c: 664
 * The following function returns false either if every active queue /Users/rubber/linux/block/bfq-iosched.c: 685
 * must receive the same share of the throughput (symmetric scenario), /Users/rubber/linux/block/bfq-iosched.c: 686
 * or, as a special case, if bfqq must receive a share of the /Users/rubber/linux/block/bfq-iosched.c: 687
 * throughput lower than or equal to the share that every other active /Users/rubber/linux/block/bfq-iosched.c: 688
 * queue must receive.  If bfqq does sync I/O, then these are the only /Users/rubber/linux/block/bfq-iosched.c: 689
 * two cases where bfqq happens to be guaranteed its share of the /Users/rubber/linux/block/bfq-iosched.c: 690
 * throughput even if I/O dispatching is not plugged when bfqq remains /Users/rubber/linux/block/bfq-iosched.c: 691
 * temporarily empty (for more details, see the comments in the /Users/rubber/linux/block/bfq-iosched.c: 692
 * function bfq_better_to_idle()). For this reason, the return value /Users/rubber/linux/block/bfq-iosched.c: 693
 * of this function is used to check whether I/O-dispatch plugging can /Users/rubber/linux/block/bfq-iosched.c: 694
 * be avoided. /Users/rubber/linux/block/bfq-iosched.c: 695
 * The above first case (symmetric scenario) occurs when: /Users/rubber/linux/block/bfq-iosched.c: 697
 * 1) all active queues have the same weight, /Users/rubber/linux/block/bfq-iosched.c: 698
 * 2) all active queues belong to the same I/O-priority class, /Users/rubber/linux/block/bfq-iosched.c: 699
 * 3) all active groups at the same level in the groups tree have the same /Users/rubber/linux/block/bfq-iosched.c: 700
 *    weight, /Users/rubber/linux/block/bfq-iosched.c: 701
 * 4) all active groups at the same level in the groups tree have the same /Users/rubber/linux/block/bfq-iosched.c: 702
 *    number of children. /Users/rubber/linux/block/bfq-iosched.c: 703
 * Unfortunately, keeping the necessary state for evaluating exactly /Users/rubber/linux/block/bfq-iosched.c: 705
 * the last two symmetry sub-conditions above would be quite complex /Users/rubber/linux/block/bfq-iosched.c: 706
 * and time consuming. Therefore this function evaluates, instead, /Users/rubber/linux/block/bfq-iosched.c: 707
 * only the following stronger three sub-conditions, for which it is /Users/rubber/linux/block/bfq-iosched.c: 708
 * much easier to maintain the needed state: /Users/rubber/linux/block/bfq-iosched.c: 709
 * 1) all active queues have the same weight, /Users/rubber/linux/block/bfq-iosched.c: 710
 * 2) all active queues belong to the same I/O-priority class, /Users/rubber/linux/block/bfq-iosched.c: 711
 * 3) there are no active groups. /Users/rubber/linux/block/bfq-iosched.c: 712
 * In particular, the last condition is always true if hierarchical /Users/rubber/linux/block/bfq-iosched.c: 713
 * support or the cgroups interface are not enabled, thus no state /Users/rubber/linux/block/bfq-iosched.c: 714
 * needs to be maintained in this case. /Users/rubber/linux/block/bfq-iosched.c: 715
	/* /Users/rubber/linux/block/bfq-iosched.c: 728
	 * For queue weights to differ, queue_weights_tree must contain /Users/rubber/linux/block/bfq-iosched.c: 729
	 * at least two nodes. /Users/rubber/linux/block/bfq-iosched.c: 730
 * If the weight-counter tree passed as input contains no counter for /Users/rubber/linux/block/bfq-iosched.c: 750
 * the weight of the input queue, then add that counter; otherwise just /Users/rubber/linux/block/bfq-iosched.c: 751
 * increment the existing counter. /Users/rubber/linux/block/bfq-iosched.c: 752
 * Note that weight-counter trees contain few nodes in mostly symmetric /Users/rubber/linux/block/bfq-iosched.c: 754
 * scenarios. For example, if all queues have the same weight, then the /Users/rubber/linux/block/bfq-iosched.c: 755
 * weight-counter tree for the queues may contain at most one node. /Users/rubber/linux/block/bfq-iosched.c: 756
 * This holds even if low_latency is on, because weight-raised queues /Users/rubber/linux/block/bfq-iosched.c: 757
 * are not inserted in the tree. /Users/rubber/linux/block/bfq-iosched.c: 758
 * In most scenarios, the rate at which nodes are created/destroyed /Users/rubber/linux/block/bfq-iosched.c: 759
 * should be low too. /Users/rubber/linux/block/bfq-iosched.c: 760
	/* /Users/rubber/linux/block/bfq-iosched.c: 769
	 * Do not insert if the queue is already associated with a /Users/rubber/linux/block/bfq-iosched.c: 770
	 * counter, which happens if: /Users/rubber/linux/block/bfq-iosched.c: 771
	 *   1) a request arrival has caused the queue to become both /Users/rubber/linux/block/bfq-iosched.c: 772
	 *      non-weight-raised, and hence change its weight, and /Users/rubber/linux/block/bfq-iosched.c: 773
	 *      backlogged; in this respect, each of the two events /Users/rubber/linux/block/bfq-iosched.c: 774
	 *      causes an invocation of this function, /Users/rubber/linux/block/bfq-iosched.c: 775
	 *   2) this is the invocation of this function caused by the /Users/rubber/linux/block/bfq-iosched.c: 776
	 *      second event. This second invocation is actually useless, /Users/rubber/linux/block/bfq-iosched.c: 777
	 *      and we handle this fact by exiting immediately. More /Users/rubber/linux/block/bfq-iosched.c: 778
	 *      efficient or clearer solutions might possibly be adopted. /Users/rubber/linux/block/bfq-iosched.c: 779
	/* /Users/rubber/linux/block/bfq-iosched.c: 805
	 * In the unlucky event of an allocation failure, we just /Users/rubber/linux/block/bfq-iosched.c: 806
	 * exit. This will cause the weight of queue to not be /Users/rubber/linux/block/bfq-iosched.c: 807
	 * considered in bfq_asymmetric_scenario, which, in its turn, /Users/rubber/linux/block/bfq-iosched.c: 808
	 * causes the scenario to be deemed wrongly symmetric in case /Users/rubber/linux/block/bfq-iosched.c: 809
	 * bfqq's weight would have been the only weight making the /Users/rubber/linux/block/bfq-iosched.c: 810
	 * scenario asymmetric.  On the bright side, no unbalance will /Users/rubber/linux/block/bfq-iosched.c: 811
	 * however occur when bfqq becomes inactive again (the /Users/rubber/linux/block/bfq-iosched.c: 812
	 * invocation of this function is triggered by an activation /Users/rubber/linux/block/bfq-iosched.c: 813
	 * of queue).  In fact, bfq_weights_tree_remove does nothing /Users/rubber/linux/block/bfq-iosched.c: 814
	 * if !bfqq->weight_counter. /Users/rubber/linux/block/bfq-iosched.c: 815
 * Decrement the weight counter associated with the queue, and, if the /Users/rubber/linux/block/bfq-iosched.c: 831
 * counter reaches 0, remove the counter from the tree. /Users/rubber/linux/block/bfq-iosched.c: 832
 * See the comments to the function bfq_weights_tree_add() for considerations /Users/rubber/linux/block/bfq-iosched.c: 833
 * about overhead. /Users/rubber/linux/block/bfq-iosched.c: 834
 * Invoke __bfq_weights_tree_remove on bfqq and decrement the number /Users/rubber/linux/block/bfq-iosched.c: 856
 * of active groups for each queue's inactive parent entity. /Users/rubber/linux/block/bfq-iosched.c: 857
			/* /Users/rubber/linux/block/bfq-iosched.c: 868
			 * entity is still active, because either /Users/rubber/linux/block/bfq-iosched.c: 869
			 * next_in_service or in_service_entity is not /Users/rubber/linux/block/bfq-iosched.c: 870
			 * NULL (see the comments on the definition of /Users/rubber/linux/block/bfq-iosched.c: 871
			 * next_in_service for details on why /Users/rubber/linux/block/bfq-iosched.c: 872
			 * in_service_entity must be checked too). /Users/rubber/linux/block/bfq-iosched.c: 873
			 * /Users/rubber/linux/block/bfq-iosched.c: 874
			 * As a consequence, its parent entities are /Users/rubber/linux/block/bfq-iosched.c: 875
			 * active as well, and thus this loop must /Users/rubber/linux/block/bfq-iosched.c: 876
			 * stop here. /Users/rubber/linux/block/bfq-iosched.c: 877
		/* /Users/rubber/linux/block/bfq-iosched.c: 882
		 * The decrement of num_groups_with_pending_reqs is /Users/rubber/linux/block/bfq-iosched.c: 883
		 * not performed immediately upon the deactivation of /Users/rubber/linux/block/bfq-iosched.c: 884
		 * entity, but it is delayed to when it also happens /Users/rubber/linux/block/bfq-iosched.c: 885
		 * that the first leaf descendant bfqq of entity gets /Users/rubber/linux/block/bfq-iosched.c: 886
		 * all its pending requests completed. The following /Users/rubber/linux/block/bfq-iosched.c: 887
		 * instructions perform this delayed decrement, if /Users/rubber/linux/block/bfq-iosched.c: 888
		 * needed. See the comments on /Users/rubber/linux/block/bfq-iosched.c: 889
		 * num_groups_with_pending_reqs for details. /Users/rubber/linux/block/bfq-iosched.c: 890
	/* /Users/rubber/linux/block/bfq-iosched.c: 898
	 * Next function is invoked last, because it causes bfqq to be /Users/rubber/linux/block/bfq-iosched.c: 899
	 * freed if the following holds: bfqq is not in service and /Users/rubber/linux/block/bfq-iosched.c: 900
	 * has no dispatched request. DO NOT use bfqq after the next /Users/rubber/linux/block/bfq-iosched.c: 901
	 * function invocation. /Users/rubber/linux/block/bfq-iosched.c: 902
 * Return expired entry, or NULL to just start from scratch in rbtree. /Users/rubber/linux/block/bfq-iosched.c: 909
 * bfq_updated_next_req - update the queue after a new next_rq selection. /Users/rubber/linux/block/bfq-iosched.c: 969
 * @bfqd: the device data the queue belongs to. /Users/rubber/linux/block/bfq-iosched.c: 970
 * @bfqq: the queue to update. /Users/rubber/linux/block/bfq-iosched.c: 971
 * If the first request of a queue changes we make sure that the queue /Users/rubber/linux/block/bfq-iosched.c: 973
 * has enough budget to serve at least its first request (if the /Users/rubber/linux/block/bfq-iosched.c: 974
 * request has grown).  We do this because if the queue has not enough /Users/rubber/linux/block/bfq-iosched.c: 975
 * budget for its first request, it has to go through two dispatch /Users/rubber/linux/block/bfq-iosched.c: 976
 * rounds to actually get it dispatched. /Users/rubber/linux/block/bfq-iosched.c: 977
		/* /Users/rubber/linux/block/bfq-iosched.c: 990
		 * In order not to break guarantees, budgets cannot be /Users/rubber/linux/block/bfq-iosched.c: 991
		 * changed after an entity has been selected. /Users/rubber/linux/block/bfq-iosched.c: 992
	/* /Users/rubber/linux/block/bfq-iosched.c: 1018
	 * Limit duration between 3 and 25 seconds. The upper limit /Users/rubber/linux/block/bfq-iosched.c: 1019
	 * has been conservatively set after the following worst case: /Users/rubber/linux/block/bfq-iosched.c: 1020
	 * on a QEMU/KVM virtual machine /Users/rubber/linux/block/bfq-iosched.c: 1021
	 * - running in a slow PC /Users/rubber/linux/block/bfq-iosched.c: 1022
	 * - with a virtual disk stacked on a slow low-end 5400rpm HDD /Users/rubber/linux/block/bfq-iosched.c: 1023
	 * - serving a heavy I/O workload, such as the sequential reading /Users/rubber/linux/block/bfq-iosched.c: 1024
	 *   of several files /Users/rubber/linux/block/bfq-iosched.c: 1025
	 * mplayer took 23 seconds to start, if constantly weight-raised. /Users/rubber/linux/block/bfq-iosched.c: 1026
	 * /Users/rubber/linux/block/bfq-iosched.c: 1027
	 * As for higher values than that accommodating the above bad /Users/rubber/linux/block/bfq-iosched.c: 1028
	 * scenario, tests show that higher values would often yield /Users/rubber/linux/block/bfq-iosched.c: 1029
	 * the opposite of the desired result, i.e., would worsen /Users/rubber/linux/block/bfq-iosched.c: 1030
	 * responsiveness by allowing non-interactive applications to /Users/rubber/linux/block/bfq-iosched.c: 1031
	 * preserve weight raising for too long. /Users/rubber/linux/block/bfq-iosched.c: 1032
	 * /Users/rubber/linux/block/bfq-iosched.c: 1033
	 * On the other end, lower values than 3 seconds make it /Users/rubber/linux/block/bfq-iosched.c: 1034
	 * difficult for most interactive tasks to complete their jobs /Users/rubber/linux/block/bfq-iosched.c: 1035
	 * before weight-raising finishes. /Users/rubber/linux/block/bfq-iosched.c: 1036
	/* /Users/rubber/linux/block/bfq-iosched.c: 1075
	 * Restore weight coefficient only if low_latency is on /Users/rubber/linux/block/bfq-iosched.c: 1076
	/* /Users/rubber/linux/block/bfq-iosched.c: 1129
	 * Start the creation of a new burst list only if there is no /Users/rubber/linux/block/bfq-iosched.c: 1130
	 * active queue. See comments on the conditional invocation of /Users/rubber/linux/block/bfq-iosched.c: 1131
	 * bfq_handle_burst(). /Users/rubber/linux/block/bfq-iosched.c: 1132
		/* /Users/rubber/linux/block/bfq-iosched.c: 1153
		 * Enough queues have been activated shortly after each /Users/rubber/linux/block/bfq-iosched.c: 1154
		 * other to consider this burst as large. /Users/rubber/linux/block/bfq-iosched.c: 1155
		/* /Users/rubber/linux/block/bfq-iosched.c: 1159
		 * We can now mark all queues in the burst list as /Users/rubber/linux/block/bfq-iosched.c: 1160
		 * belonging to a large burst. /Users/rubber/linux/block/bfq-iosched.c: 1161
		/* /Users/rubber/linux/block/bfq-iosched.c: 1168
		 * From now on, and until the current burst finishes, any /Users/rubber/linux/block/bfq-iosched.c: 1169
		 * new queue being activated shortly after the last queue /Users/rubber/linux/block/bfq-iosched.c: 1170
		 * was inserted in the burst can be immediately marked as /Users/rubber/linux/block/bfq-iosched.c: 1171
		 * belonging to a large burst. So the burst list is not /Users/rubber/linux/block/bfq-iosched.c: 1172
		 * needed any more. Remove it. /Users/rubber/linux/block/bfq-iosched.c: 1173
	} else /* /Users/rubber/linux/block/bfq-iosched.c: 1178
		* Burst not yet large: add bfqq to the burst list. Do /Users/rubber/linux/block/bfq-iosched.c: 1179
		* not increment the ref counter for bfqq, because bfqq /Users/rubber/linux/block/bfq-iosched.c: 1180
		* is removed from the burst list before freeing bfqq /Users/rubber/linux/block/bfq-iosched.c: 1181
		* in put_queue. /Users/rubber/linux/block/bfq-iosched.c: 1182
 * If many queues belonging to the same group happen to be created /Users/rubber/linux/block/bfq-iosched.c: 1188
 * shortly after each other, then the processes associated with these /Users/rubber/linux/block/bfq-iosched.c: 1189
 * queues have typically a common goal. In particular, bursts of queue /Users/rubber/linux/block/bfq-iosched.c: 1190
 * creations are usually caused by services or applications that spawn /Users/rubber/linux/block/bfq-iosched.c: 1191
 * many parallel threads/processes. Examples are systemd during boot, /Users/rubber/linux/block/bfq-iosched.c: 1192
 * or git grep. To help these processes get their job done as soon as /Users/rubber/linux/block/bfq-iosched.c: 1193
 * possible, it is usually better to not grant either weight-raising /Users/rubber/linux/block/bfq-iosched.c: 1194
 * or device idling to their queues, unless these queues must be /Users/rubber/linux/block/bfq-iosched.c: 1195
 * protected from the I/O flowing through other active queues. /Users/rubber/linux/block/bfq-iosched.c: 1196
 * In this comment we describe, firstly, the reasons why this fact /Users/rubber/linux/block/bfq-iosched.c: 1198
 * holds, and, secondly, the next function, which implements the main /Users/rubber/linux/block/bfq-iosched.c: 1199
 * steps needed to properly mark these queues so that they can then be /Users/rubber/linux/block/bfq-iosched.c: 1200
 * treated in a different way. /Users/rubber/linux/block/bfq-iosched.c: 1201
 * The above services or applications benefit mostly from a high /Users/rubber/linux/block/bfq-iosched.c: 1203
 * throughput: the quicker the requests of the activated queues are /Users/rubber/linux/block/bfq-iosched.c: 1204
 * cumulatively served, the sooner the target job of these queues gets /Users/rubber/linux/block/bfq-iosched.c: 1205
 * completed. As a consequence, weight-raising any of these queues, /Users/rubber/linux/block/bfq-iosched.c: 1206
 * which also implies idling the device for it, is almost always /Users/rubber/linux/block/bfq-iosched.c: 1207
 * counterproductive, unless there are other active queues to isolate /Users/rubber/linux/block/bfq-iosched.c: 1208
 * these new queues from. If there no other active queues, then /Users/rubber/linux/block/bfq-iosched.c: 1209
 * weight-raising these new queues just lowers throughput in most /Users/rubber/linux/block/bfq-iosched.c: 1210
 * cases. /Users/rubber/linux/block/bfq-iosched.c: 1211
 * On the other hand, a burst of queue creations may be caused also by /Users/rubber/linux/block/bfq-iosched.c: 1213
 * the start of an application that does not consist of a lot of /Users/rubber/linux/block/bfq-iosched.c: 1214
 * parallel I/O-bound threads. In fact, with a complex application, /Users/rubber/linux/block/bfq-iosched.c: 1215
 * several short processes may need to be executed to start-up the /Users/rubber/linux/block/bfq-iosched.c: 1216
 * application. In this respect, to start an application as quickly as /Users/rubber/linux/block/bfq-iosched.c: 1217
 * possible, the best thing to do is in any case to privilege the I/O /Users/rubber/linux/block/bfq-iosched.c: 1218
 * related to the application with respect to all other /Users/rubber/linux/block/bfq-iosched.c: 1219
 * I/O. Therefore, the best strategy to start as quickly as possible /Users/rubber/linux/block/bfq-iosched.c: 1220
 * an application that causes a burst of queue creations is to /Users/rubber/linux/block/bfq-iosched.c: 1221
 * weight-raise all the queues created during the burst. This is the /Users/rubber/linux/block/bfq-iosched.c: 1222
 * exact opposite of the best strategy for the other type of bursts. /Users/rubber/linux/block/bfq-iosched.c: 1223
 * In the end, to take the best action for each of the two cases, the /Users/rubber/linux/block/bfq-iosched.c: 1225
 * two types of bursts need to be distinguished. Fortunately, this /Users/rubber/linux/block/bfq-iosched.c: 1226
 * seems relatively easy, by looking at the sizes of the bursts. In /Users/rubber/linux/block/bfq-iosched.c: 1227
 * particular, we found a threshold such that only bursts with a /Users/rubber/linux/block/bfq-iosched.c: 1228
 * larger size than that threshold are apparently caused by /Users/rubber/linux/block/bfq-iosched.c: 1229
 * services or commands such as systemd or git grep. For brevity, /Users/rubber/linux/block/bfq-iosched.c: 1230
 * hereafter we call just 'large' these bursts. BFQ *does not* /Users/rubber/linux/block/bfq-iosched.c: 1231
 * weight-raise queues whose creation occurs in a large burst. In /Users/rubber/linux/block/bfq-iosched.c: 1232
 * addition, for each of these queues BFQ performs or does not perform /Users/rubber/linux/block/bfq-iosched.c: 1233
 * idling depending on which choice boosts the throughput more. The /Users/rubber/linux/block/bfq-iosched.c: 1234
 * exact choice depends on the device and request pattern at /Users/rubber/linux/block/bfq-iosched.c: 1235
 * hand. /Users/rubber/linux/block/bfq-iosched.c: 1236
 * Unfortunately, false positives may occur while an interactive task /Users/rubber/linux/block/bfq-iosched.c: 1238
 * is starting (e.g., an application is being started). The /Users/rubber/linux/block/bfq-iosched.c: 1239
 * consequence is that the queues associated with the task do not /Users/rubber/linux/block/bfq-iosched.c: 1240
 * enjoy weight raising as expected. Fortunately these false positives /Users/rubber/linux/block/bfq-iosched.c: 1241
 * are very rare. They typically occur if some service happens to /Users/rubber/linux/block/bfq-iosched.c: 1242
 * start doing I/O exactly when the interactive task starts. /Users/rubber/linux/block/bfq-iosched.c: 1243
 * Turning back to the next function, it is invoked only if there are /Users/rubber/linux/block/bfq-iosched.c: 1245
 * no active queues (apart from active queues that would belong to the /Users/rubber/linux/block/bfq-iosched.c: 1246
 * same, possible burst bfqq would belong to), and it implements all /Users/rubber/linux/block/bfq-iosched.c: 1247
 * the steps needed to detect the occurrence of a large burst and to /Users/rubber/linux/block/bfq-iosched.c: 1248
 * properly mark all the queues belonging to it (so that they can then /Users/rubber/linux/block/bfq-iosched.c: 1249
 * be treated in a different way). This goal is achieved by /Users/rubber/linux/block/bfq-iosched.c: 1250
 * maintaining a "burst list" that holds, temporarily, the queues that /Users/rubber/linux/block/bfq-iosched.c: 1251
 * belong to the burst in progress. The list is then used to mark /Users/rubber/linux/block/bfq-iosched.c: 1252
 * these queues as belonging to a large burst if the burst does become /Users/rubber/linux/block/bfq-iosched.c: 1253
 * large. The main steps are the following. /Users/rubber/linux/block/bfq-iosched.c: 1254
 * . when the very first queue is created, the queue is inserted into the /Users/rubber/linux/block/bfq-iosched.c: 1256
 *   list (as it could be the first queue in a possible burst) /Users/rubber/linux/block/bfq-iosched.c: 1257
 * . if the current burst has not yet become large, and a queue Q that does /Users/rubber/linux/block/bfq-iosched.c: 1259
 *   not yet belong to the burst is activated shortly after the last time /Users/rubber/linux/block/bfq-iosched.c: 1260
 *   at which a new queue entered the burst list, then the function appends /Users/rubber/linux/block/bfq-iosched.c: 1261
 *   Q to the burst list /Users/rubber/linux/block/bfq-iosched.c: 1262
 * . if, as a consequence of the previous step, the burst size reaches /Users/rubber/linux/block/bfq-iosched.c: 1264
 *   the large-burst threshold, then /Users/rubber/linux/block/bfq-iosched.c: 1265
 *     . all the queues in the burst list are marked as belonging to a /Users/rubber/linux/block/bfq-iosched.c: 1267
 *       large burst /Users/rubber/linux/block/bfq-iosched.c: 1268
 *     . the burst list is deleted; in fact, the burst list already served /Users/rubber/linux/block/bfq-iosched.c: 1270
 *       its purpose (keeping temporarily track of the queues in a burst, /Users/rubber/linux/block/bfq-iosched.c: 1271
 *       so as to be able to mark them as belonging to a large burst in the /Users/rubber/linux/block/bfq-iosched.c: 1272
 *       previous sub-step), and now is not needed any more /Users/rubber/linux/block/bfq-iosched.c: 1273
 *     . the device enters a large-burst mode /Users/rubber/linux/block/bfq-iosched.c: 1275
 * . if a queue Q that does not belong to the burst is created while /Users/rubber/linux/block/bfq-iosched.c: 1277
 *   the device is in large-burst mode and shortly after the last time /Users/rubber/linux/block/bfq-iosched.c: 1278
 *   at which a queue either entered the burst list or was marked as /Users/rubber/linux/block/bfq-iosched.c: 1279
 *   belonging to the current large burst, then Q is immediately marked /Users/rubber/linux/block/bfq-iosched.c: 1280
 *   as belonging to a large burst. /Users/rubber/linux/block/bfq-iosched.c: 1281
 * . if a queue Q that does not belong to the burst is created a while /Users/rubber/linux/block/bfq-iosched.c: 1283
 *   later, i.e., not shortly after, than the last time at which a queue /Users/rubber/linux/block/bfq-iosched.c: 1284
 *   either entered the burst list or was marked as belonging to the /Users/rubber/linux/block/bfq-iosched.c: 1285
 *   current large burst, then the current burst is deemed as finished and: /Users/rubber/linux/block/bfq-iosched.c: 1286
 *        . the large-burst mode is reset if set /Users/rubber/linux/block/bfq-iosched.c: 1288
 *        . the burst list is emptied /Users/rubber/linux/block/bfq-iosched.c: 1290
 *        . Q is inserted in the burst list, as Q may be the first queue /Users/rubber/linux/block/bfq-iosched.c: 1292
 *          in a possible new burst (then the burst list contains just Q /Users/rubber/linux/block/bfq-iosched.c: 1293
 *          after this step). /Users/rubber/linux/block/bfq-iosched.c: 1294
	/* /Users/rubber/linux/block/bfq-iosched.c: 1298
	 * If bfqq is already in the burst list or is part of a large /Users/rubber/linux/block/bfq-iosched.c: 1299
	 * burst, or finally has just been split, then there is /Users/rubber/linux/block/bfq-iosched.c: 1300
	 * nothing else to do. /Users/rubber/linux/block/bfq-iosched.c: 1301
	/* /Users/rubber/linux/block/bfq-iosched.c: 1309
	 * If bfqq's creation happens late enough, or bfqq belongs to /Users/rubber/linux/block/bfq-iosched.c: 1310
	 * a different group than the burst group, then the current /Users/rubber/linux/block/bfq-iosched.c: 1311
	 * burst is finished, and related data structures must be /Users/rubber/linux/block/bfq-iosched.c: 1312
	 * reset. /Users/rubber/linux/block/bfq-iosched.c: 1313
	 * /Users/rubber/linux/block/bfq-iosched.c: 1314
	 * In this respect, consider the special case where bfqq is /Users/rubber/linux/block/bfq-iosched.c: 1315
	 * the very first queue created after BFQ is selected for this /Users/rubber/linux/block/bfq-iosched.c: 1316
	 * device. In this case, last_ins_in_burst and /Users/rubber/linux/block/bfq-iosched.c: 1317
	 * burst_parent_entity are not yet significant when we get /Users/rubber/linux/block/bfq-iosched.c: 1318
	 * here. But it is easy to verify that, whether or not the /Users/rubber/linux/block/bfq-iosched.c: 1319
	 * following condition is true, bfqq will end up being /Users/rubber/linux/block/bfq-iosched.c: 1320
	 * inserted into the burst list. In particular the list will /Users/rubber/linux/block/bfq-iosched.c: 1321
	 * happen to contain only bfqq. And this is exactly what has /Users/rubber/linux/block/bfq-iosched.c: 1322
	 * to happen, as bfqq may be the first queue of the first /Users/rubber/linux/block/bfq-iosched.c: 1323
	 * burst. /Users/rubber/linux/block/bfq-iosched.c: 1324
	/* /Users/rubber/linux/block/bfq-iosched.c: 1334
	 * If we get here, then bfqq is being activated shortly after the /Users/rubber/linux/block/bfq-iosched.c: 1335
	 * last queue. So, if the current burst is also large, we can mark /Users/rubber/linux/block/bfq-iosched.c: 1336
	 * bfqq as belonging to this large burst immediately. /Users/rubber/linux/block/bfq-iosched.c: 1337
	/* /Users/rubber/linux/block/bfq-iosched.c: 1344
	 * If we get here, then a large-burst state has not yet been /Users/rubber/linux/block/bfq-iosched.c: 1345
	 * reached, but bfqq is being activated shortly after the last /Users/rubber/linux/block/bfq-iosched.c: 1346
	 * queue. Then we add bfqq to the burst. /Users/rubber/linux/block/bfq-iosched.c: 1347
	/* /Users/rubber/linux/block/bfq-iosched.c: 1351
	 * At this point, bfqq either has been added to the current /Users/rubber/linux/block/bfq-iosched.c: 1352
	 * burst or has caused the current burst to terminate and a /Users/rubber/linux/block/bfq-iosched.c: 1353
	 * possible new burst to start. In particular, in the second /Users/rubber/linux/block/bfq-iosched.c: 1354
	 * case, bfqq has become the first queue in the possible new /Users/rubber/linux/block/bfq-iosched.c: 1355
	 * burst.  In both cases last_ins_in_burst needs to be moved /Users/rubber/linux/block/bfq-iosched.c: 1356
	 * forward. /Users/rubber/linux/block/bfq-iosched.c: 1357
 * If enough samples have been computed, return the current max budget /Users/rubber/linux/block/bfq-iosched.c: 1370
 * stored in bfqd, which is dynamically updated according to the /Users/rubber/linux/block/bfq-iosched.c: 1371
 * estimated disk peak rate; otherwise return the default max budget /Users/rubber/linux/block/bfq-iosched.c: 1372
 * Return min budget, which is a fraction of the current or default /Users/rubber/linux/block/bfq-iosched.c: 1383
 * max budget (trying with 1/32) /Users/rubber/linux/block/bfq-iosched.c: 1384
 * The next function, invoked after the input queue bfqq switches from /Users/rubber/linux/block/bfq-iosched.c: 1395
 * idle to busy, updates the budget of bfqq. The function also tells /Users/rubber/linux/block/bfq-iosched.c: 1396
 * whether the in-service queue should be expired, by returning /Users/rubber/linux/block/bfq-iosched.c: 1397
 * true. The purpose of expiring the in-service queue is to give bfqq /Users/rubber/linux/block/bfq-iosched.c: 1398
 * the chance to possibly preempt the in-service queue, and the reason /Users/rubber/linux/block/bfq-iosched.c: 1399
 * for preempting the in-service queue is to achieve one of the two /Users/rubber/linux/block/bfq-iosched.c: 1400
 * goals below. /Users/rubber/linux/block/bfq-iosched.c: 1401
 * 1. Guarantee to bfqq its reserved bandwidth even if bfqq has /Users/rubber/linux/block/bfq-iosched.c: 1403
 * expired because it has remained idle. In particular, bfqq may have /Users/rubber/linux/block/bfq-iosched.c: 1404
 * expired for one of the following two reasons: /Users/rubber/linux/block/bfq-iosched.c: 1405
 * - BFQQE_NO_MORE_REQUESTS bfqq did not enjoy any device idling /Users/rubber/linux/block/bfq-iosched.c: 1407
 *   and did not make it to issue a new request before its last /Users/rubber/linux/block/bfq-iosched.c: 1408
 *   request was served; /Users/rubber/linux/block/bfq-iosched.c: 1409
 * - BFQQE_TOO_IDLE bfqq did enjoy device idling, but did not issue /Users/rubber/linux/block/bfq-iosched.c: 1411
 *   a new request before the expiration of the idling-time. /Users/rubber/linux/block/bfq-iosched.c: 1412
 * Even if bfqq has expired for one of the above reasons, the process /Users/rubber/linux/block/bfq-iosched.c: 1414
 * associated with the queue may be however issuing requests greedily, /Users/rubber/linux/block/bfq-iosched.c: 1415
 * and thus be sensitive to the bandwidth it receives (bfqq may have /Users/rubber/linux/block/bfq-iosched.c: 1416
 * remained idle for other reasons: CPU high load, bfqq not enjoying /Users/rubber/linux/block/bfq-iosched.c: 1417
 * idling, I/O throttling somewhere in the path from the process to /Users/rubber/linux/block/bfq-iosched.c: 1418
 * the I/O scheduler, ...). But if, after every expiration for one of /Users/rubber/linux/block/bfq-iosched.c: 1419
 * the above two reasons, bfqq has to wait for the service of at least /Users/rubber/linux/block/bfq-iosched.c: 1420
 * one full budget of another queue before being served again, then /Users/rubber/linux/block/bfq-iosched.c: 1421
 * bfqq is likely to get a much lower bandwidth or resource time than /Users/rubber/linux/block/bfq-iosched.c: 1422
 * its reserved ones. To address this issue, two countermeasures need /Users/rubber/linux/block/bfq-iosched.c: 1423
 * to be taken. /Users/rubber/linux/block/bfq-iosched.c: 1424
 * First, the budget and the timestamps of bfqq need to be updated in /Users/rubber/linux/block/bfq-iosched.c: 1426
 * a special way on bfqq reactivation: they need to be updated as if /Users/rubber/linux/block/bfq-iosched.c: 1427
 * bfqq did not remain idle and did not expire. In fact, if they are /Users/rubber/linux/block/bfq-iosched.c: 1428
 * computed as if bfqq expired and remained idle until reactivation, /Users/rubber/linux/block/bfq-iosched.c: 1429
 * then the process associated with bfqq is treated as if, instead of /Users/rubber/linux/block/bfq-iosched.c: 1430
 * being greedy, it stopped issuing requests when bfqq remained idle, /Users/rubber/linux/block/bfq-iosched.c: 1431
 * and restarts issuing requests only on this reactivation. In other /Users/rubber/linux/block/bfq-iosched.c: 1432
 * words, the scheduler does not help the process recover the "service /Users/rubber/linux/block/bfq-iosched.c: 1433
 * hole" between bfqq expiration and reactivation. As a consequence, /Users/rubber/linux/block/bfq-iosched.c: 1434
 * the process receives a lower bandwidth than its reserved one. In /Users/rubber/linux/block/bfq-iosched.c: 1435
 * contrast, to recover this hole, the budget must be updated as if /Users/rubber/linux/block/bfq-iosched.c: 1436
 * bfqq was not expired at all before this reactivation, i.e., it must /Users/rubber/linux/block/bfq-iosched.c: 1437
 * be set to the value of the remaining budget when bfqq was /Users/rubber/linux/block/bfq-iosched.c: 1438
 * expired. Along the same line, timestamps need to be assigned the /Users/rubber/linux/block/bfq-iosched.c: 1439
 * value they had the last time bfqq was selected for service, i.e., /Users/rubber/linux/block/bfq-iosched.c: 1440
 * before last expiration. Thus timestamps need to be back-shifted /Users/rubber/linux/block/bfq-iosched.c: 1441
 * with respect to their normal computation (see [1] for more details /Users/rubber/linux/block/bfq-iosched.c: 1442
 * on this tricky aspect). /Users/rubber/linux/block/bfq-iosched.c: 1443
 * Secondly, to allow the process to recover the hole, the in-service /Users/rubber/linux/block/bfq-iosched.c: 1445
 * queue must be expired too, to give bfqq the chance to preempt it /Users/rubber/linux/block/bfq-iosched.c: 1446
 * immediately. In fact, if bfqq has to wait for a full budget of the /Users/rubber/linux/block/bfq-iosched.c: 1447
 * in-service queue to be completed, then it may become impossible to /Users/rubber/linux/block/bfq-iosched.c: 1448
 * let the process recover the hole, even if the back-shifted /Users/rubber/linux/block/bfq-iosched.c: 1449
 * timestamps of bfqq are lower than those of the in-service queue. If /Users/rubber/linux/block/bfq-iosched.c: 1450
 * this happens for most or all of the holes, then the process may not /Users/rubber/linux/block/bfq-iosched.c: 1451
 * receive its reserved bandwidth. In this respect, it is worth noting /Users/rubber/linux/block/bfq-iosched.c: 1452
 * that, being the service of outstanding requests unpreemptible, a /Users/rubber/linux/block/bfq-iosched.c: 1453
 * little fraction of the holes may however be unrecoverable, thereby /Users/rubber/linux/block/bfq-iosched.c: 1454
 * causing a little loss of bandwidth. /Users/rubber/linux/block/bfq-iosched.c: 1455
 * The last important point is detecting whether bfqq does need this /Users/rubber/linux/block/bfq-iosched.c: 1457
 * bandwidth recovery. In this respect, the next function deems the /Users/rubber/linux/block/bfq-iosched.c: 1458
 * process associated with bfqq greedy, and thus allows it to recover /Users/rubber/linux/block/bfq-iosched.c: 1459
 * the hole, if: 1) the process is waiting for the arrival of a new /Users/rubber/linux/block/bfq-iosched.c: 1460
 * request (which implies that bfqq expired for one of the above two /Users/rubber/linux/block/bfq-iosched.c: 1461
 * reasons), and 2) such a request has arrived soon. The first /Users/rubber/linux/block/bfq-iosched.c: 1462
 * condition is controlled through the flag non_blocking_wait_rq, /Users/rubber/linux/block/bfq-iosched.c: 1463
 * while the second through the flag arrived_in_time. If both /Users/rubber/linux/block/bfq-iosched.c: 1464
 * conditions hold, then the function computes the budget in the /Users/rubber/linux/block/bfq-iosched.c: 1465
 * above-described special way, and signals that the in-service queue /Users/rubber/linux/block/bfq-iosched.c: 1466
 * should be expired. Timestamp back-shifting is done later in /Users/rubber/linux/block/bfq-iosched.c: 1467
 * __bfq_activate_entity. /Users/rubber/linux/block/bfq-iosched.c: 1468
 * 2. Reduce latency. Even if timestamps are not backshifted to let /Users/rubber/linux/block/bfq-iosched.c: 1470
 * the process associated with bfqq recover a service hole, bfqq may /Users/rubber/linux/block/bfq-iosched.c: 1471
 * however happen to have, after being (re)activated, a lower finish /Users/rubber/linux/block/bfq-iosched.c: 1472
 * timestamp than the in-service queue.	 That is, the next budget of /Users/rubber/linux/block/bfq-iosched.c: 1473
 * bfqq may have to be completed before the one of the in-service /Users/rubber/linux/block/bfq-iosched.c: 1474
 * queue. If this is the case, then preempting the in-service queue /Users/rubber/linux/block/bfq-iosched.c: 1475
 * allows this goal to be achieved, apart from the unpreemptible, /Users/rubber/linux/block/bfq-iosched.c: 1476
 * outstanding requests mentioned above. /Users/rubber/linux/block/bfq-iosched.c: 1477
 * Unfortunately, regardless of which of the above two goals one wants /Users/rubber/linux/block/bfq-iosched.c: 1479
 * to achieve, service trees need first to be updated to know whether /Users/rubber/linux/block/bfq-iosched.c: 1480
 * the in-service queue must be preempted. To have service trees /Users/rubber/linux/block/bfq-iosched.c: 1481
 * correctly updated, the in-service queue must be expired and /Users/rubber/linux/block/bfq-iosched.c: 1482
 * rescheduled, and bfqq must be scheduled too. This is one of the /Users/rubber/linux/block/bfq-iosched.c: 1483
 * most costly operations (in future versions, the scheduling /Users/rubber/linux/block/bfq-iosched.c: 1484
 * mechanism may be re-designed in such a way to make it possible to /Users/rubber/linux/block/bfq-iosched.c: 1485
 * know whether preemption is needed without needing to update service /Users/rubber/linux/block/bfq-iosched.c: 1486
 * trees). In addition, queue preemptions almost always cause random /Users/rubber/linux/block/bfq-iosched.c: 1487
 * I/O, which may in turn cause loss of throughput. Finally, there may /Users/rubber/linux/block/bfq-iosched.c: 1488
 * even be no in-service queue when the next function is invoked (so, /Users/rubber/linux/block/bfq-iosched.c: 1489
 * no queue to compare timestamps with). Because of these facts, the /Users/rubber/linux/block/bfq-iosched.c: 1490
 * next function adopts the following simple scheme to avoid costly /Users/rubber/linux/block/bfq-iosched.c: 1491
 * operations, too frequent preemptions and too many dependencies on /Users/rubber/linux/block/bfq-iosched.c: 1492
 * the state of the scheduler: it requests the expiration of the /Users/rubber/linux/block/bfq-iosched.c: 1493
 * in-service queue (unconditionally) only for queues that need to /Users/rubber/linux/block/bfq-iosched.c: 1494
 * recover a hole. Then it delegates to other parts of the code the /Users/rubber/linux/block/bfq-iosched.c: 1495
 * responsibility of handling the above case 2. /Users/rubber/linux/block/bfq-iosched.c: 1496
	/* /Users/rubber/linux/block/bfq-iosched.c: 1504
	 * In the next compound condition, we check also whether there /Users/rubber/linux/block/bfq-iosched.c: 1505
	 * is some budget left, because otherwise there is no point in /Users/rubber/linux/block/bfq-iosched.c: 1506
	 * trying to go on serving bfqq with this same budget: bfqq /Users/rubber/linux/block/bfq-iosched.c: 1507
	 * would be expired immediately after being selected for /Users/rubber/linux/block/bfq-iosched.c: 1508
	 * service. This would only cause useless overhead. /Users/rubber/linux/block/bfq-iosched.c: 1509
		/* /Users/rubber/linux/block/bfq-iosched.c: 1513
		 * We do not clear the flag non_blocking_wait_rq here, as /Users/rubber/linux/block/bfq-iosched.c: 1514
		 * the latter is used in bfq_activate_bfqq to signal /Users/rubber/linux/block/bfq-iosched.c: 1515
		 * that timestamps need to be back-shifted (and is /Users/rubber/linux/block/bfq-iosched.c: 1516
		 * cleared right after). /Users/rubber/linux/block/bfq-iosched.c: 1517
		/* /Users/rubber/linux/block/bfq-iosched.c: 1520
		 * In next assignment we rely on that either /Users/rubber/linux/block/bfq-iosched.c: 1521
		 * entity->service or entity->budget are not updated /Users/rubber/linux/block/bfq-iosched.c: 1522
		 * on expiration if bfqq is empty (see /Users/rubber/linux/block/bfq-iosched.c: 1523
		 * __bfq_bfqq_recalc_budget). Thus both quantities /Users/rubber/linux/block/bfq-iosched.c: 1524
		 * remain unchanged after such an expiration, and the /Users/rubber/linux/block/bfq-iosched.c: 1525
		 * following statement therefore assigns to /Users/rubber/linux/block/bfq-iosched.c: 1526
		 * entity->budget the remaining budget on such an /Users/rubber/linux/block/bfq-iosched.c: 1527
		 * expiration. /Users/rubber/linux/block/bfq-iosched.c: 1528
		/* /Users/rubber/linux/block/bfq-iosched.c: 1534
		 * At this point, we have used entity->service to get /Users/rubber/linux/block/bfq-iosched.c: 1535
		 * the budget left (needed for updating /Users/rubber/linux/block/bfq-iosched.c: 1536
		 * entity->budget). Thus we finally can, and have to, /Users/rubber/linux/block/bfq-iosched.c: 1537
		 * reset entity->service. The latter must be reset /Users/rubber/linux/block/bfq-iosched.c: 1538
		 * because bfqq would otherwise be charged again for /Users/rubber/linux/block/bfq-iosched.c: 1539
		 * the service it has received during its previous /Users/rubber/linux/block/bfq-iosched.c: 1540
		 * service slot(s). /Users/rubber/linux/block/bfq-iosched.c: 1541
	/* /Users/rubber/linux/block/bfq-iosched.c: 1548
	 * We can finally complete expiration, by setting service to 0. /Users/rubber/linux/block/bfq-iosched.c: 1549
 * Return the farthest past time instant according to jiffies /Users/rubber/linux/block/bfq-iosched.c: 1559
 * macros. /Users/rubber/linux/block/bfq-iosched.c: 1560
			/* /Users/rubber/linux/block/bfq-iosched.c: 1582
			 * No interactive weight raising in progress /Users/rubber/linux/block/bfq-iosched.c: 1583
			 * here: assign minus infinity to /Users/rubber/linux/block/bfq-iosched.c: 1584
			 * wr_start_at_switch_to_srt, to make sure /Users/rubber/linux/block/bfq-iosched.c: 1585
			 * that, at the end of the soft-real-time /Users/rubber/linux/block/bfq-iosched.c: 1586
			 * weight raising periods that is starting /Users/rubber/linux/block/bfq-iosched.c: 1587
			 * now, no interactive weight-raising period /Users/rubber/linux/block/bfq-iosched.c: 1588
			 * may be wrongly considered as still in /Users/rubber/linux/block/bfq-iosched.c: 1589
			 * progress (and thus actually started by /Users/rubber/linux/block/bfq-iosched.c: 1590
			 * mistake). /Users/rubber/linux/block/bfq-iosched.c: 1591
		/* /Users/rubber/linux/block/bfq-iosched.c: 1601
		 * If needed, further reduce budget to make sure it is /Users/rubber/linux/block/bfq-iosched.c: 1602
		 * close to bfqq's backlog, so as to reduce the /Users/rubber/linux/block/bfq-iosched.c: 1603
		 * scheduling-error component due to a too large /Users/rubber/linux/block/bfq-iosched.c: 1604
		 * budget. Do not care about throughput consequences, /Users/rubber/linux/block/bfq-iosched.c: 1605
		 * but only about latency. Finally, do not assign a /Users/rubber/linux/block/bfq-iosched.c: 1606
		 * too small budget either, to avoid increasing /Users/rubber/linux/block/bfq-iosched.c: 1607
		 * latency by causing too frequent expirations. /Users/rubber/linux/block/bfq-iosched.c: 1608
			/* /Users/rubber/linux/block/bfq-iosched.c: 1620
			 * The application is now or still meeting the /Users/rubber/linux/block/bfq-iosched.c: 1621
			 * requirements for being deemed soft rt.  We /Users/rubber/linux/block/bfq-iosched.c: 1622
			 * can then correctly and safely (re)charge /Users/rubber/linux/block/bfq-iosched.c: 1623
			 * the weight-raising duration for the /Users/rubber/linux/block/bfq-iosched.c: 1624
			 * application with the weight-raising /Users/rubber/linux/block/bfq-iosched.c: 1625
			 * duration for soft rt applications. /Users/rubber/linux/block/bfq-iosched.c: 1626
			 * /Users/rubber/linux/block/bfq-iosched.c: 1627
			 * In particular, doing this recharge now, i.e., /Users/rubber/linux/block/bfq-iosched.c: 1628
			 * before the weight-raising period for the /Users/rubber/linux/block/bfq-iosched.c: 1629
			 * application finishes, reduces the probability /Users/rubber/linux/block/bfq-iosched.c: 1630
			 * of the following negative scenario: /Users/rubber/linux/block/bfq-iosched.c: 1631
			 * 1) the weight of a soft rt application is /Users/rubber/linux/block/bfq-iosched.c: 1632
			 *    raised at startup (as for any newly /Users/rubber/linux/block/bfq-iosched.c: 1633
			 *    created application), /Users/rubber/linux/block/bfq-iosched.c: 1634
			 * 2) since the application is not interactive, /Users/rubber/linux/block/bfq-iosched.c: 1635
			 *    at a certain time weight-raising is /Users/rubber/linux/block/bfq-iosched.c: 1636
			 *    stopped for the application, /Users/rubber/linux/block/bfq-iosched.c: 1637
			 * 3) at that time the application happens to /Users/rubber/linux/block/bfq-iosched.c: 1638
			 *    still have pending requests, and hence /Users/rubber/linux/block/bfq-iosched.c: 1639
			 *    is destined to not have a chance to be /Users/rubber/linux/block/bfq-iosched.c: 1640
			 *    deemed soft rt before these requests are /Users/rubber/linux/block/bfq-iosched.c: 1641
			 *    completed (see the comments to the /Users/rubber/linux/block/bfq-iosched.c: 1642
			 *    function bfq_bfqq_softrt_next_start() /Users/rubber/linux/block/bfq-iosched.c: 1643
			 *    for details on soft rt detection), /Users/rubber/linux/block/bfq-iosched.c: 1644
			 * 4) these pending requests experience a high /Users/rubber/linux/block/bfq-iosched.c: 1645
			 *    latency because the application is not /Users/rubber/linux/block/bfq-iosched.c: 1646
			 *    weight-raised while they are pending. /Users/rubber/linux/block/bfq-iosched.c: 1647
 * Return true if bfqq is in a higher priority class, or has a higher /Users/rubber/linux/block/bfq-iosched.c: 1675
 * weight than the in-service queue. /Users/rubber/linux/block/bfq-iosched.c: 1676
		/* /Users/rubber/linux/block/bfq-iosched.c: 1714
		 * See the comments on /Users/rubber/linux/block/bfq-iosched.c: 1715
		 * bfq_bfqq_update_budg_for_activation for /Users/rubber/linux/block/bfq-iosched.c: 1716
		 * details on the usage of the next variable. /Users/rubber/linux/block/bfq-iosched.c: 1717
	/* /Users/rubber/linux/block/bfq-iosched.c: 1724
	 * bfqq deserves to be weight-raised if: /Users/rubber/linux/block/bfq-iosched.c: 1725
	 * - it is sync, /Users/rubber/linux/block/bfq-iosched.c: 1726
	 * - it does not belong to a large burst, /Users/rubber/linux/block/bfq-iosched.c: 1727
	 * - it has been idle for enough time or is soft real-time, /Users/rubber/linux/block/bfq-iosched.c: 1728
	 * - is linked to a bfq_io_cq (it is not shared in any sense), /Users/rubber/linux/block/bfq-iosched.c: 1729
	 * - has a default weight (otherwise we assume the user wanted /Users/rubber/linux/block/bfq-iosched.c: 1730
	 *   to control its weight explicitly) /Users/rubber/linux/block/bfq-iosched.c: 1731
	/* /Users/rubber/linux/block/bfq-iosched.c: 1742
	 * Merged bfq_queues are kept out of weight-raising /Users/rubber/linux/block/bfq-iosched.c: 1743
	 * (low-latency) mechanisms. The reason is that these queues /Users/rubber/linux/block/bfq-iosched.c: 1744
	 * are usually created for non-interactive and /Users/rubber/linux/block/bfq-iosched.c: 1745
	 * non-soft-real-time tasks. Yet this is not the case for /Users/rubber/linux/block/bfq-iosched.c: 1746
	 * stably-merged queues. These queues are merged just because /Users/rubber/linux/block/bfq-iosched.c: 1747
	 * they are created shortly after each other. So they may /Users/rubber/linux/block/bfq-iosched.c: 1748
	 * easily serve the I/O of an interactive or soft-real time /Users/rubber/linux/block/bfq-iosched.c: 1749
	 * application, if the application happens to spawn multiple /Users/rubber/linux/block/bfq-iosched.c: 1750
	 * processes. So let also stably-merged queued enjoy weight /Users/rubber/linux/block/bfq-iosched.c: 1751
	 * raising. /Users/rubber/linux/block/bfq-iosched.c: 1752
	/* /Users/rubber/linux/block/bfq-iosched.c: 1760
	 * Using the last flag, update budget and check whether bfqq /Users/rubber/linux/block/bfq-iosched.c: 1761
	 * may want to preempt the in-service queue. /Users/rubber/linux/block/bfq-iosched.c: 1762
	/* /Users/rubber/linux/block/bfq-iosched.c: 1768
	 * If bfqq happened to be activated in a burst, but has been /Users/rubber/linux/block/bfq-iosched.c: 1769
	 * idle for much more than an interactive queue, then we /Users/rubber/linux/block/bfq-iosched.c: 1770
	 * assume that, in the overall I/O initiated in the burst, the /Users/rubber/linux/block/bfq-iosched.c: 1771
	 * I/O associated with bfqq is finished. So bfqq does not need /Users/rubber/linux/block/bfq-iosched.c: 1772
	 * to be treated as a queue belonging to a burst /Users/rubber/linux/block/bfq-iosched.c: 1773
	 * anymore. Accordingly, we reset bfqq's in_large_burst flag /Users/rubber/linux/block/bfq-iosched.c: 1774
	 * if set, and remove bfqq from the burst list if it's /Users/rubber/linux/block/bfq-iosched.c: 1775
	 * there. We do not decrement burst_size, because the fact /Users/rubber/linux/block/bfq-iosched.c: 1776
	 * that bfqq does not need to belong to the burst list any /Users/rubber/linux/block/bfq-iosched.c: 1777
	 * more does not invalidate the fact that bfqq was created in /Users/rubber/linux/block/bfq-iosched.c: 1778
	 * a burst. /Users/rubber/linux/block/bfq-iosched.c: 1779
	/* /Users/rubber/linux/block/bfq-iosched.c: 1818
	 * Expire in-service queue if preemption may be needed for /Users/rubber/linux/block/bfq-iosched.c: 1819
	 * guarantees or throughput. As for guarantees, we care /Users/rubber/linux/block/bfq-iosched.c: 1820
	 * explicitly about two cases. The first is that bfqq has to /Users/rubber/linux/block/bfq-iosched.c: 1821
	 * recover a service hole, as explained in the comments on /Users/rubber/linux/block/bfq-iosched.c: 1822
	 * bfq_bfqq_update_budg_for_activation(), i.e., that /Users/rubber/linux/block/bfq-iosched.c: 1823
	 * bfqq_wants_to_preempt is true. However, if bfqq does not /Users/rubber/linux/block/bfq-iosched.c: 1824
	 * carry time-critical I/O, then bfqq's bandwidth is less /Users/rubber/linux/block/bfq-iosched.c: 1825
	 * important than that of queues that carry time-critical I/O. /Users/rubber/linux/block/bfq-iosched.c: 1826
	 * So, as a further constraint, we consider this case only if /Users/rubber/linux/block/bfq-iosched.c: 1827
	 * bfqq is at least as weight-raised, i.e., at least as time /Users/rubber/linux/block/bfq-iosched.c: 1828
	 * critical, as the in-service queue. /Users/rubber/linux/block/bfq-iosched.c: 1829
	 * /Users/rubber/linux/block/bfq-iosched.c: 1830
	 * The second case is that bfqq is in a higher priority class, /Users/rubber/linux/block/bfq-iosched.c: 1831
	 * or has a higher weight than the in-service queue. If this /Users/rubber/linux/block/bfq-iosched.c: 1832
	 * condition does not hold, we don't care because, even if /Users/rubber/linux/block/bfq-iosched.c: 1833
	 * bfqq does not start to be served immediately, the resulting /Users/rubber/linux/block/bfq-iosched.c: 1834
	 * delay for bfqq's I/O is however lower or much lower than /Users/rubber/linux/block/bfq-iosched.c: 1835
	 * the ideal completion time to be guaranteed to bfqq's I/O. /Users/rubber/linux/block/bfq-iosched.c: 1836
	 * /Users/rubber/linux/block/bfq-iosched.c: 1837
	 * In both cases, preemption is needed only if, according to /Users/rubber/linux/block/bfq-iosched.c: 1838
	 * the timestamps of both bfqq and of the in-service queue, /Users/rubber/linux/block/bfq-iosched.c: 1839
	 * bfqq actually is the next queue to serve. So, to reduce /Users/rubber/linux/block/bfq-iosched.c: 1840
	 * useless preemptions, the return value of /Users/rubber/linux/block/bfq-iosched.c: 1841
	 * next_queue_may_preempt() is considered in the next compound /Users/rubber/linux/block/bfq-iosched.c: 1842
	 * condition too. Yet next_queue_may_preempt() just checks a /Users/rubber/linux/block/bfq-iosched.c: 1843
	 * simple, necessary condition for bfqq to be the next queue /Users/rubber/linux/block/bfq-iosched.c: 1844
	 * to serve. In fact, to evaluate a sufficient condition, the /Users/rubber/linux/block/bfq-iosched.c: 1845
	 * timestamps of the in-service queue would need to be /Users/rubber/linux/block/bfq-iosched.c: 1846
	 * updated, and this operation is quite costly (see the /Users/rubber/linux/block/bfq-iosched.c: 1847
	 * comments on bfq_bfqq_update_budg_for_activation()). /Users/rubber/linux/block/bfq-iosched.c: 1848
	 * /Users/rubber/linux/block/bfq-iosched.c: 1849
	 * As for throughput, we ask bfq_better_to_idle() whether we /Users/rubber/linux/block/bfq-iosched.c: 1850
	 * still need to plug I/O dispatching. If bfq_better_to_idle() /Users/rubber/linux/block/bfq-iosched.c: 1851
	 * says no, then plugging is not needed any longer, either to /Users/rubber/linux/block/bfq-iosched.c: 1852
	 * boost throughput or to perserve service guarantees. Then /Users/rubber/linux/block/bfq-iosched.c: 1853
	 * the best option is to stop plugging I/O, as not doing so /Users/rubber/linux/block/bfq-iosched.c: 1854
	 * would certainly lower throughput. We may end up in this /Users/rubber/linux/block/bfq-iosched.c: 1855
	 * case if: (1) upon a dispatch attempt, we detected that it /Users/rubber/linux/block/bfq-iosched.c: 1856
	 * was better to plug I/O dispatch, and to wait for a new /Users/rubber/linux/block/bfq-iosched.c: 1857
	 * request to arrive for the currently in-service queue, but /Users/rubber/linux/block/bfq-iosched.c: 1858
	 * (2) this switch of bfqq to busy changes the scenario. /Users/rubber/linux/block/bfq-iosched.c: 1859
	/* /Users/rubber/linux/block/bfq-iosched.c: 1877
	 * Reset pointer in case we are waiting for /Users/rubber/linux/block/bfq-iosched.c: 1878
	 * some request completion. /Users/rubber/linux/block/bfq-iosched.c: 1879
	/* /Users/rubber/linux/block/bfq-iosched.c: 1883
	 * If bfqq has a short think time, then start by setting the /Users/rubber/linux/block/bfq-iosched.c: 1884
	 * inject limit to 0 prudentially, because the service time of /Users/rubber/linux/block/bfq-iosched.c: 1885
	 * an injected I/O request may be higher than the think time /Users/rubber/linux/block/bfq-iosched.c: 1886
	 * of bfqq, and therefore, if one request was injected when /Users/rubber/linux/block/bfq-iosched.c: 1887
	 * bfqq remains empty, this injected request might delay the /Users/rubber/linux/block/bfq-iosched.c: 1888
	 * service of the next I/O request for bfqq significantly. In /Users/rubber/linux/block/bfq-iosched.c: 1889
	 * case bfqq can actually tolerate some injection, then the /Users/rubber/linux/block/bfq-iosched.c: 1890
	 * adaptive update will however raise the limit soon. This /Users/rubber/linux/block/bfq-iosched.c: 1891
	 * lucky circumstance holds exactly because bfqq has a short /Users/rubber/linux/block/bfq-iosched.c: 1892
	 * think time, and thus, after remaining empty, is likely to /Users/rubber/linux/block/bfq-iosched.c: 1893
	 * get new I/O enqueued---and then completed---before being /Users/rubber/linux/block/bfq-iosched.c: 1894
	 * expired. This is the very pattern that gives the /Users/rubber/linux/block/bfq-iosched.c: 1895
	 * limit-update algorithm the chance to measure the effect of /Users/rubber/linux/block/bfq-iosched.c: 1896
	 * injection on request service times, and then to update the /Users/rubber/linux/block/bfq-iosched.c: 1897
	 * limit accordingly. /Users/rubber/linux/block/bfq-iosched.c: 1898
	 * /Users/rubber/linux/block/bfq-iosched.c: 1899
	 * However, in the following special case, the inject limit is /Users/rubber/linux/block/bfq-iosched.c: 1900
	 * left to 1 even if the think time is short: bfqq's I/O is /Users/rubber/linux/block/bfq-iosched.c: 1901
	 * synchronized with that of some other queue, i.e., bfqq may /Users/rubber/linux/block/bfq-iosched.c: 1902
	 * receive new I/O only after the I/O of the other queue is /Users/rubber/linux/block/bfq-iosched.c: 1903
	 * completed. Keeping the inject limit to 1 allows the /Users/rubber/linux/block/bfq-iosched.c: 1904
	 * blocking I/O to be served while bfqq is in service. And /Users/rubber/linux/block/bfq-iosched.c: 1905
	 * this is very convenient both for bfqq and for overall /Users/rubber/linux/block/bfq-iosched.c: 1906
	 * throughput, as explained in detail in the comments in /Users/rubber/linux/block/bfq-iosched.c: 1907
	 * bfq_update_has_short_ttime(). /Users/rubber/linux/block/bfq-iosched.c: 1908
	 * /Users/rubber/linux/block/bfq-iosched.c: 1909
	 * On the opposite end, if bfqq has a long think time, then /Users/rubber/linux/block/bfq-iosched.c: 1910
	 * start directly by 1, because: /Users/rubber/linux/block/bfq-iosched.c: 1911
	 * a) on the bright side, keeping at most one request in /Users/rubber/linux/block/bfq-iosched.c: 1912
	 * service in the drive is unlikely to cause any harm to the /Users/rubber/linux/block/bfq-iosched.c: 1913
	 * latency of bfqq's requests, as the service time of a single /Users/rubber/linux/block/bfq-iosched.c: 1914
	 * request is likely to be lower than the think time of bfqq; /Users/rubber/linux/block/bfq-iosched.c: 1915
	 * b) on the downside, after becoming empty, bfqq is likely to /Users/rubber/linux/block/bfq-iosched.c: 1916
	 * expire before getting its next request. With this request /Users/rubber/linux/block/bfq-iosched.c: 1917
	 * arrival pattern, it is very hard to sample total service /Users/rubber/linux/block/bfq-iosched.c: 1918
	 * times and update the inject limit accordingly (see comments /Users/rubber/linux/block/bfq-iosched.c: 1919
	 * on bfq_update_inject_limit()). So the limit is likely to be /Users/rubber/linux/block/bfq-iosched.c: 1920
	 * never, or at least seldom, updated.  As a consequence, by /Users/rubber/linux/block/bfq-iosched.c: 1921
	 * setting the limit to 1, we avoid that no injection ever /Users/rubber/linux/block/bfq-iosched.c: 1922
	 * occurs with bfqq. On the downside, this proactive step /Users/rubber/linux/block/bfq-iosched.c: 1923
	 * further reduces chances to actually compute the baseline /Users/rubber/linux/block/bfq-iosched.c: 1924
	 * total service time. Thus it reduces chances to execute the /Users/rubber/linux/block/bfq-iosched.c: 1925
	 * limit-update algorithm and possibly raise the limit to more /Users/rubber/linux/block/bfq-iosched.c: 1926
	 * than 1. /Users/rubber/linux/block/bfq-iosched.c: 1927
	/* /Users/rubber/linux/block/bfq-iosched.c: 1948
	 * Must be busy for at least about 80% of the time to be /Users/rubber/linux/block/bfq-iosched.c: 1949
	 * considered I/O bound. /Users/rubber/linux/block/bfq-iosched.c: 1950
	/* /Users/rubber/linux/block/bfq-iosched.c: 1957
	 * Keep an observation window of at most 200 ms in the past /Users/rubber/linux/block/bfq-iosched.c: 1958
	 * from now. /Users/rubber/linux/block/bfq-iosched.c: 1959
 * Detect whether bfqq's I/O seems synchronized with that of some /Users/rubber/linux/block/bfq-iosched.c: 1968
 * other queue, i.e., whether bfqq, after remaining empty, happens to /Users/rubber/linux/block/bfq-iosched.c: 1969
 * receive new I/O only right after some I/O request of the other /Users/rubber/linux/block/bfq-iosched.c: 1970
 * queue has been completed. We call waker queue the other queue, and /Users/rubber/linux/block/bfq-iosched.c: 1971
 * we assume, for simplicity, that bfqq may have at most one waker /Users/rubber/linux/block/bfq-iosched.c: 1972
 * queue. /Users/rubber/linux/block/bfq-iosched.c: 1973
 * A remarkable throughput boost can be reached by unconditionally /Users/rubber/linux/block/bfq-iosched.c: 1975
 * injecting the I/O of the waker queue, every time a new /Users/rubber/linux/block/bfq-iosched.c: 1976
 * bfq_dispatch_request happens to be invoked while I/O is being /Users/rubber/linux/block/bfq-iosched.c: 1977
 * plugged for bfqq.  In addition to boosting throughput, this /Users/rubber/linux/block/bfq-iosched.c: 1978
 * unblocks bfqq's I/O, thereby improving bandwidth and latency for /Users/rubber/linux/block/bfq-iosched.c: 1979
 * bfqq. Note that these same results may be achieved with the general /Users/rubber/linux/block/bfq-iosched.c: 1980
 * injection mechanism, but less effectively. For details on this /Users/rubber/linux/block/bfq-iosched.c: 1981
 * aspect, see the comments on the choice of the queue for injection /Users/rubber/linux/block/bfq-iosched.c: 1982
 * in bfq_select_queue(). /Users/rubber/linux/block/bfq-iosched.c: 1983
 * Turning back to the detection of a waker queue, a queue Q is deemed /Users/rubber/linux/block/bfq-iosched.c: 1985
 * as a waker queue for bfqq if, for three consecutive times, bfqq /Users/rubber/linux/block/bfq-iosched.c: 1986
 * happens to become non empty right after a request of Q has been /Users/rubber/linux/block/bfq-iosched.c: 1987
 * completed. In this respect, even if bfqq is empty, we do not check /Users/rubber/linux/block/bfq-iosched.c: 1988
 * for a waker if it still has some in-flight I/O. In fact, in this /Users/rubber/linux/block/bfq-iosched.c: 1989
 * case bfqq is actually still being served by the drive, and may /Users/rubber/linux/block/bfq-iosched.c: 1990
 * receive new I/O on the completion of some of the in-flight /Users/rubber/linux/block/bfq-iosched.c: 1991
 * requests. In particular, on the first time, Q is tentatively set as /Users/rubber/linux/block/bfq-iosched.c: 1992
 * a candidate waker queue, while on the third consecutive time that Q /Users/rubber/linux/block/bfq-iosched.c: 1993
 * is detected, the field waker_bfqq is set to Q, to confirm that Q is /Users/rubber/linux/block/bfq-iosched.c: 1994
 * a waker queue for bfqq. These detection steps are performed only if /Users/rubber/linux/block/bfq-iosched.c: 1995
 * bfqq has a long think time, so as to make it more likely that /Users/rubber/linux/block/bfq-iosched.c: 1996
 * bfqq's I/O is actually being blocked by a synchronization. This /Users/rubber/linux/block/bfq-iosched.c: 1997
 * last filter, plus the above three-times requirement, make false /Users/rubber/linux/block/bfq-iosched.c: 1998
 * positives less likely. /Users/rubber/linux/block/bfq-iosched.c: 1999
 * NOTE /Users/rubber/linux/block/bfq-iosched.c: 2001
 * The sooner a waker queue is detected, the sooner throughput can be /Users/rubber/linux/block/bfq-iosched.c: 2003
 * boosted by injecting I/O from the waker queue. Fortunately, /Users/rubber/linux/block/bfq-iosched.c: 2004
 * detection is likely to be actually fast, for the following /Users/rubber/linux/block/bfq-iosched.c: 2005
 * reasons. While blocked by synchronization, bfqq has a long think /Users/rubber/linux/block/bfq-iosched.c: 2006
 * time. This implies that bfqq's inject limit is at least equal to 1 /Users/rubber/linux/block/bfq-iosched.c: 2007
 * (see the comments in bfq_update_inject_limit()). So, thanks to /Users/rubber/linux/block/bfq-iosched.c: 2008
 * injection, the waker queue is likely to be served during the very /Users/rubber/linux/block/bfq-iosched.c: 2009
 * first I/O-plugging time interval for bfqq. This triggers the first /Users/rubber/linux/block/bfq-iosched.c: 2010
 * step of the detection mechanism. Thanks again to injection, the /Users/rubber/linux/block/bfq-iosched.c: 2011
 * candidate waker queue is then likely to be confirmed no later than /Users/rubber/linux/block/bfq-iosched.c: 2012
 * during the next I/O-plugging interval for bfqq. /Users/rubber/linux/block/bfq-iosched.c: 2013
 * ISSUE /Users/rubber/linux/block/bfq-iosched.c: 2015
 * On queue merging all waker information is lost. /Users/rubber/linux/block/bfq-iosched.c: 2017
		/* /Users/rubber/linux/block/bfq-iosched.c: 2032
		 * First synchronization detected with a /Users/rubber/linux/block/bfq-iosched.c: 2033
		 * candidate waker queue, or with a different /Users/rubber/linux/block/bfq-iosched.c: 2034
		 * candidate waker queue from the current one. /Users/rubber/linux/block/bfq-iosched.c: 2035
		/* /Users/rubber/linux/block/bfq-iosched.c: 2047
		 * If the waker queue disappears, then /Users/rubber/linux/block/bfq-iosched.c: 2048
		 * bfqq->waker_bfqq must be reset. To /Users/rubber/linux/block/bfq-iosched.c: 2049
		 * this goal, we maintain in each /Users/rubber/linux/block/bfq-iosched.c: 2050
		 * waker queue a list, woken_list, of /Users/rubber/linux/block/bfq-iosched.c: 2051
		 * all the queues that reference the /Users/rubber/linux/block/bfq-iosched.c: 2052
		 * waker queue through their /Users/rubber/linux/block/bfq-iosched.c: 2053
		 * waker_bfqq pointer. When the waker /Users/rubber/linux/block/bfq-iosched.c: 2054
		 * queue exits, the waker_bfqq pointer /Users/rubber/linux/block/bfq-iosched.c: 2055
		 * of all the queues in the woken_list /Users/rubber/linux/block/bfq-iosched.c: 2056
		 * is reset. /Users/rubber/linux/block/bfq-iosched.c: 2057
		 * /Users/rubber/linux/block/bfq-iosched.c: 2058
		 * In addition, if bfqq is already in /Users/rubber/linux/block/bfq-iosched.c: 2059
		 * the woken_list of a waker queue, /Users/rubber/linux/block/bfq-iosched.c: 2060
		 * then, before being inserted into /Users/rubber/linux/block/bfq-iosched.c: 2061
		 * the woken_list of a new waker /Users/rubber/linux/block/bfq-iosched.c: 2062
		 * queue, bfqq must be removed from /Users/rubber/linux/block/bfq-iosched.c: 2063
		 * the woken_list of the old waker /Users/rubber/linux/block/bfq-iosched.c: 2064
		 * queue. /Users/rubber/linux/block/bfq-iosched.c: 2065
		/* /Users/rubber/linux/block/bfq-iosched.c: 2090
		 * Periodically reset inject limit, to make sure that /Users/rubber/linux/block/bfq-iosched.c: 2091
		 * the latter eventually drops in case workload /Users/rubber/linux/block/bfq-iosched.c: 2092
		 * changes, see step (3) in the comments on /Users/rubber/linux/block/bfq-iosched.c: 2093
		 * bfq_update_inject_limit(). /Users/rubber/linux/block/bfq-iosched.c: 2094
		/* /Users/rubber/linux/block/bfq-iosched.c: 2100
		 * The following conditions must hold to setup a new /Users/rubber/linux/block/bfq-iosched.c: 2101
		 * sampling of total service time, and then a new /Users/rubber/linux/block/bfq-iosched.c: 2102
		 * update of the inject limit: /Users/rubber/linux/block/bfq-iosched.c: 2103
		 * - bfqq is in service, because the total service /Users/rubber/linux/block/bfq-iosched.c: 2104
		 *   time is evaluated only for the I/O requests of /Users/rubber/linux/block/bfq-iosched.c: 2105
		 *   the queues in service; /Users/rubber/linux/block/bfq-iosched.c: 2106
		 * - this is the right occasion to compute or to /Users/rubber/linux/block/bfq-iosched.c: 2107
		 *   lower the baseline total service time, because /Users/rubber/linux/block/bfq-iosched.c: 2108
		 *   there are actually no requests in the drive, /Users/rubber/linux/block/bfq-iosched.c: 2109
		 *   or /Users/rubber/linux/block/bfq-iosched.c: 2110
		 *   the baseline total service time is available, and /Users/rubber/linux/block/bfq-iosched.c: 2111
		 *   this is the right occasion to compute the other /Users/rubber/linux/block/bfq-iosched.c: 2112
		 *   quantity needed to update the inject limit, i.e., /Users/rubber/linux/block/bfq-iosched.c: 2113
		 *   the total service time caused by the amount of /Users/rubber/linux/block/bfq-iosched.c: 2114
		 *   injection allowed by the current value of the /Users/rubber/linux/block/bfq-iosched.c: 2115
		 *   limit. It is the right occasion because injection /Users/rubber/linux/block/bfq-iosched.c: 2116
		 *   has actually been performed during the service /Users/rubber/linux/block/bfq-iosched.c: 2117
		 *   hole, and there are still in-flight requests, /Users/rubber/linux/block/bfq-iosched.c: 2118
		 *   which are very likely to be exactly the injected /Users/rubber/linux/block/bfq-iosched.c: 2119
		 *   requests, or part of them; /Users/rubber/linux/block/bfq-iosched.c: 2120
		 * - the minimum interval for sampling the total /Users/rubber/linux/block/bfq-iosched.c: 2121
		 *   service time and updating the inject limit has /Users/rubber/linux/block/bfq-iosched.c: 2122
		 *   elapsed. /Users/rubber/linux/block/bfq-iosched.c: 2123
			/* /Users/rubber/linux/block/bfq-iosched.c: 2132
			 * Start the state machine for measuring the /Users/rubber/linux/block/bfq-iosched.c: 2133
			 * total service time of rq: setting /Users/rubber/linux/block/bfq-iosched.c: 2134
			 * wait_dispatch will cause bfqd->waited_rq to /Users/rubber/linux/block/bfq-iosched.c: 2135
			 * be set when rq will be dispatched. /Users/rubber/linux/block/bfq-iosched.c: 2136
			/* /Users/rubber/linux/block/bfq-iosched.c: 2139
			 * If there is no I/O in service in the drive, /Users/rubber/linux/block/bfq-iosched.c: 2140
			 * then possible injection occurred before the /Users/rubber/linux/block/bfq-iosched.c: 2141
			 * arrival of rq will not affect the total /Users/rubber/linux/block/bfq-iosched.c: 2142
			 * service time of rq. So the injection limit /Users/rubber/linux/block/bfq-iosched.c: 2143
			 * must not be updated as a function of such /Users/rubber/linux/block/bfq-iosched.c: 2144
			 * total service time, unless new injection /Users/rubber/linux/block/bfq-iosched.c: 2145
			 * occurs before rq is completed. To have the /Users/rubber/linux/block/bfq-iosched.c: 2146
			 * injection limit updated only in the latter /Users/rubber/linux/block/bfq-iosched.c: 2147
			 * case, reset rqs_injected here (rqs_injected /Users/rubber/linux/block/bfq-iosched.c: 2148
			 * will be set in case injection is performed /Users/rubber/linux/block/bfq-iosched.c: 2149
			 * on bfqq before rq is completed). /Users/rubber/linux/block/bfq-iosched.c: 2150
	/* /Users/rubber/linux/block/bfq-iosched.c: 2162
	 * Check if this request is a better next-serve candidate. /Users/rubber/linux/block/bfq-iosched.c: 2163
	/* /Users/rubber/linux/block/bfq-iosched.c: 2169
	 * Adjust priority tree position, if next_rq changes. /Users/rubber/linux/block/bfq-iosched.c: 2170
	 * See comments on bfq_pos_tree_add_move() for the unlikely(). /Users/rubber/linux/block/bfq-iosched.c: 2171
	/* /Users/rubber/linux/block/bfq-iosched.c: 2194
	 * Assign jiffies to last_wr_start_finish in the following /Users/rubber/linux/block/bfq-iosched.c: 2195
	 * cases: /Users/rubber/linux/block/bfq-iosched.c: 2196
	 * /Users/rubber/linux/block/bfq-iosched.c: 2197
	 * . if bfqq is not going to be weight-raised, because, for /Users/rubber/linux/block/bfq-iosched.c: 2198
	 *   non weight-raised queues, last_wr_start_finish stores the /Users/rubber/linux/block/bfq-iosched.c: 2199
	 *   arrival time of the last request; as of now, this piece /Users/rubber/linux/block/bfq-iosched.c: 2200
	 *   of information is used only for deciding whether to /Users/rubber/linux/block/bfq-iosched.c: 2201
	 *   weight-raise async queues /Users/rubber/linux/block/bfq-iosched.c: 2202
	 * /Users/rubber/linux/block/bfq-iosched.c: 2203
	 * . if bfqq is not weight-raised, because, if bfqq is now /Users/rubber/linux/block/bfq-iosched.c: 2204
	 *   switching to weight-raised, then last_wr_start_finish /Users/rubber/linux/block/bfq-iosched.c: 2205
	 *   stores the time when weight-raising starts /Users/rubber/linux/block/bfq-iosched.c: 2206
	 * /Users/rubber/linux/block/bfq-iosched.c: 2207
	 * . if bfqq is interactive, because, regardless of whether /Users/rubber/linux/block/bfq-iosched.c: 2208
	 *   bfqq is currently weight-raised, the weight-raising /Users/rubber/linux/block/bfq-iosched.c: 2209
	 *   period must start or restart (this case is considered /Users/rubber/linux/block/bfq-iosched.c: 2210
	 *   separately because it is not detected by the above /Users/rubber/linux/block/bfq-iosched.c: 2211
	 *   conditions, if bfqq is already weight-raised) /Users/rubber/linux/block/bfq-iosched.c: 2212
	 * /Users/rubber/linux/block/bfq-iosched.c: 2213
	 * last_wr_start_finish has to be updated also if bfqq is soft /Users/rubber/linux/block/bfq-iosched.c: 2214
	 * real-time, because the weight-raising period is constantly /Users/rubber/linux/block/bfq-iosched.c: 2215
	 * restarted on idle-to-busy transitions for these queues, but /Users/rubber/linux/block/bfq-iosched.c: 2216
	 * this is already done in bfq_bfqq_handle_idle_busy_switch if /Users/rubber/linux/block/bfq-iosched.c: 2217
	 * needed. /Users/rubber/linux/block/bfq-iosched.c: 2218
			/* /Users/rubber/linux/block/bfq-iosched.c: 2289
			 * bfqq emptied. In normal operation, when /Users/rubber/linux/block/bfq-iosched.c: 2290
			 * bfqq is empty, bfqq->entity.service and /Users/rubber/linux/block/bfq-iosched.c: 2291
			 * bfqq->entity.budget must contain, /Users/rubber/linux/block/bfq-iosched.c: 2292
			 * respectively, the service received and the /Users/rubber/linux/block/bfq-iosched.c: 2293
			 * budget used last time bfqq emptied. These /Users/rubber/linux/block/bfq-iosched.c: 2294
			 * facts do not hold in this case, as at least /Users/rubber/linux/block/bfq-iosched.c: 2295
			 * this last removal occurred while bfqq is /Users/rubber/linux/block/bfq-iosched.c: 2296
			 * not in service. To avoid inconsistencies, /Users/rubber/linux/block/bfq-iosched.c: 2297
			 * reset both bfqq->entity.service and /Users/rubber/linux/block/bfq-iosched.c: 2298
			 * bfqq->entity.budget, if bfqq has still a /Users/rubber/linux/block/bfq-iosched.c: 2299
			 * process that may issue I/O requests to it. /Users/rubber/linux/block/bfq-iosched.c: 2300
		/* /Users/rubber/linux/block/bfq-iosched.c: 2305
		 * Remove queue from request-position tree as it is empty. /Users/rubber/linux/block/bfq-iosched.c: 2306
	/* /Users/rubber/linux/block/bfq-iosched.c: 2328
	 * bfq_bic_lookup grabs the queue_lock: invoke it now and /Users/rubber/linux/block/bfq-iosched.c: 2329
	 * store its return value for later use, to avoid nesting /Users/rubber/linux/block/bfq-iosched.c: 2330
	 * queue_lock inside the bfqd->lock. We assume that the bic /Users/rubber/linux/block/bfq-iosched.c: 2331
	 * returned by bfq_bic_lookup does not go away before /Users/rubber/linux/block/bfq-iosched.c: 2332
	 * bfqd->lock is taken. /Users/rubber/linux/block/bfq-iosched.c: 2333
		/* /Users/rubber/linux/block/bfq-iosched.c: 2401
		 * If next_rq changes, update both the queue's budget to /Users/rubber/linux/block/bfq-iosched.c: 2402
		 * fit the new request and the queue's position in its /Users/rubber/linux/block/bfq-iosched.c: 2403
		 * rq_pos_tree. /Users/rubber/linux/block/bfq-iosched.c: 2404
			/* /Users/rubber/linux/block/bfq-iosched.c: 2408
			 * See comments on bfq_pos_tree_add_move() for /Users/rubber/linux/block/bfq-iosched.c: 2409
			 * the unlikely(). /Users/rubber/linux/block/bfq-iosched.c: 2410
 * This function is called to notify the scheduler that the requests /Users/rubber/linux/block/bfq-iosched.c: 2419
 * rq and 'next' have been merged, with 'next' going away.  BFQ /Users/rubber/linux/block/bfq-iosched.c: 2420
 * exploits this hook to address the following issue: if 'next' has a /Users/rubber/linux/block/bfq-iosched.c: 2421
 * fifo_time lower that rq, then the fifo_time of rq must be set to /Users/rubber/linux/block/bfq-iosched.c: 2422
 * the value of 'next', to not forget the greater age of 'next'. /Users/rubber/linux/block/bfq-iosched.c: 2423
 * NOTE: in this function we assume that rq is in a bfq_queue, basing /Users/rubber/linux/block/bfq-iosched.c: 2425
 * on that rq is picked from the hash table q->elevator->hash, which, /Users/rubber/linux/block/bfq-iosched.c: 2426
 * in its turn, is filled only with I/O requests present in /Users/rubber/linux/block/bfq-iosched.c: 2427
 * bfq_queues, while BFQ is in use for the request queue q. In fact, /Users/rubber/linux/block/bfq-iosched.c: 2428
 * the function that fills this hash table (elv_rqhash_add) is called /Users/rubber/linux/block/bfq-iosched.c: 2429
 * only by bfq_insert_request. /Users/rubber/linux/block/bfq-iosched.c: 2430
	/* /Users/rubber/linux/block/bfq-iosched.c: 2441
	 * If next and rq belong to the same bfq_queue and next is older /Users/rubber/linux/block/bfq-iosched.c: 2442
	 * than rq, then reposition rq in the fifo (by substituting next /Users/rubber/linux/block/bfq-iosched.c: 2443
	 * with rq). Otherwise, if next and rq belong to different /Users/rubber/linux/block/bfq-iosched.c: 2444
	 * bfq_queues, never reposition rq: in fact, we would have to /Users/rubber/linux/block/bfq-iosched.c: 2445
	 * reposition it with respect to next's position in its own fifo, /Users/rubber/linux/block/bfq-iosched.c: 2446
	 * which would most certainly be too expensive with respect to /Users/rubber/linux/block/bfq-iosched.c: 2447
	 * the benefits. /Users/rubber/linux/block/bfq-iosched.c: 2448
	/* /Users/rubber/linux/block/bfq-iosched.c: 2475
	 * If bfqq has been enjoying interactive weight-raising, then /Users/rubber/linux/block/bfq-iosched.c: 2476
	 * reset soft_rt_next_start. We do it for the following /Users/rubber/linux/block/bfq-iosched.c: 2477
	 * reason. bfqq may have been conveying the I/O needed to load /Users/rubber/linux/block/bfq-iosched.c: 2478
	 * a soft real-time application. Such an application actually /Users/rubber/linux/block/bfq-iosched.c: 2479
	 * exhibits a soft real-time I/O pattern after it finishes /Users/rubber/linux/block/bfq-iosched.c: 2480
	 * loading, and finally starts doing its job. But, if bfqq has /Users/rubber/linux/block/bfq-iosched.c: 2481
	 * been receiving a lot of bandwidth so far (likely to happen /Users/rubber/linux/block/bfq-iosched.c: 2482
	 * on a fast device), then soft_rt_next_start now contains a /Users/rubber/linux/block/bfq-iosched.c: 2483
	 * high value that. So, without this reset, bfqq would be /Users/rubber/linux/block/bfq-iosched.c: 2484
	 * prevented from being possibly considered as soft_rt for a /Users/rubber/linux/block/bfq-iosched.c: 2485
	 * very long time. /Users/rubber/linux/block/bfq-iosched.c: 2486
	/* /Users/rubber/linux/block/bfq-iosched.c: 2498
	 * Trigger a weight change on the next invocation of /Users/rubber/linux/block/bfq-iosched.c: 2499
	 * __bfq_entity_update_weight_prio. /Users/rubber/linux/block/bfq-iosched.c: 2500
	/* /Users/rubber/linux/block/bfq-iosched.c: 2559
	 * First, if we find a request starting at the end of the last /Users/rubber/linux/block/bfq-iosched.c: 2560
	 * request, choose it. /Users/rubber/linux/block/bfq-iosched.c: 2561
	/* /Users/rubber/linux/block/bfq-iosched.c: 2567
	 * If the exact sector wasn't found, the parent of the NULL leaf /Users/rubber/linux/block/bfq-iosched.c: 2568
	 * will contain the closest sector (rq_pos_tree sorted by /Users/rubber/linux/block/bfq-iosched.c: 2569
	 * next_request position). /Users/rubber/linux/block/bfq-iosched.c: 2570
	/* /Users/rubber/linux/block/bfq-iosched.c: 2596
	 * We shall notice if some of the queues are cooperating, /Users/rubber/linux/block/bfq-iosched.c: 2597
	 * e.g., working closely on the same area of the device. In /Users/rubber/linux/block/bfq-iosched.c: 2598
	 * that case, we can group them together and: 1) don't waste /Users/rubber/linux/block/bfq-iosched.c: 2599
	 * time idling, and 2) serve the union of their requests in /Users/rubber/linux/block/bfq-iosched.c: 2600
	 * the best possible order for throughput. /Users/rubber/linux/block/bfq-iosched.c: 2601
	/* /Users/rubber/linux/block/bfq-iosched.c: 2616
	 * If there are no process references on the new_bfqq, then it is /Users/rubber/linux/block/bfq-iosched.c: 2617
	 * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain /Users/rubber/linux/block/bfq-iosched.c: 2618
	 * may have dropped their last reference (not just their last process /Users/rubber/linux/block/bfq-iosched.c: 2619
	 * reference). /Users/rubber/linux/block/bfq-iosched.c: 2620
	/* /Users/rubber/linux/block/bfq-iosched.c: 2634
	 * If the process for the bfqq has gone away, there is no /Users/rubber/linux/block/bfq-iosched.c: 2635
	 * sense in merging the queues. /Users/rubber/linux/block/bfq-iosched.c: 2636
	/* /Users/rubber/linux/block/bfq-iosched.c: 2644
	 * Merging is just a redirection: the requests of the process /Users/rubber/linux/block/bfq-iosched.c: 2645
	 * owning one of the two queues are redirected to the other queue. /Users/rubber/linux/block/bfq-iosched.c: 2646
	 * The latter queue, in its turn, is set as shared if this is the /Users/rubber/linux/block/bfq-iosched.c: 2647
	 * first time that the requests of some process are redirected to /Users/rubber/linux/block/bfq-iosched.c: 2648
	 * it. /Users/rubber/linux/block/bfq-iosched.c: 2649
	 * /Users/rubber/linux/block/bfq-iosched.c: 2650
	 * We redirect bfqq to new_bfqq and not the opposite, because /Users/rubber/linux/block/bfq-iosched.c: 2651
	 * we are in the context of the process owning bfqq, thus we /Users/rubber/linux/block/bfq-iosched.c: 2652
	 * have the io_cq of this process. So we can immediately /Users/rubber/linux/block/bfq-iosched.c: 2653
	 * configure this io_cq to redirect the requests of the /Users/rubber/linux/block/bfq-iosched.c: 2654
	 * process to new_bfqq. In contrast, the io_cq of new_bfqq is /Users/rubber/linux/block/bfq-iosched.c: 2655
	 * not available any more (new_bfqq->bic == NULL). /Users/rubber/linux/block/bfq-iosched.c: 2656
	 * /Users/rubber/linux/block/bfq-iosched.c: 2657
	 * Anyway, even in case new_bfqq coincides with the in-service /Users/rubber/linux/block/bfq-iosched.c: 2658
	 * queue, redirecting requests the in-service queue is the /Users/rubber/linux/block/bfq-iosched.c: 2659
	 * best option, as we feed the in-service queue with new /Users/rubber/linux/block/bfq-iosched.c: 2660
	 * requests close to the last request served and, by doing so, /Users/rubber/linux/block/bfq-iosched.c: 2661
	 * are likely to increase the throughput. /Users/rubber/linux/block/bfq-iosched.c: 2662
	/* /Users/rubber/linux/block/bfq-iosched.c: 2679
	 * If either of the queues has already been detected as seeky, /Users/rubber/linux/block/bfq-iosched.c: 2680
	 * then merging it with the other queue is unlikely to lead to /Users/rubber/linux/block/bfq-iosched.c: 2681
	 * sequential I/O. /Users/rubber/linux/block/bfq-iosched.c: 2682
	/* /Users/rubber/linux/block/bfq-iosched.c: 2687
	 * Interleaved I/O is known to be done by (some) applications /Users/rubber/linux/block/bfq-iosched.c: 2688
	 * only for reads, so it does not make sense to merge async /Users/rubber/linux/block/bfq-iosched.c: 2689
	 * queues. /Users/rubber/linux/block/bfq-iosched.c: 2690
 * Attempt to schedule a merge of bfqq with the currently in-service /Users/rubber/linux/block/bfq-iosched.c: 2702
 * queue or with a close queue among the scheduled queues.  Return /Users/rubber/linux/block/bfq-iosched.c: 2703
 * NULL if no merge was scheduled, a pointer to the shared bfq_queue /Users/rubber/linux/block/bfq-iosched.c: 2704
 * structure otherwise. /Users/rubber/linux/block/bfq-iosched.c: 2705
 * The OOM queue is not allowed to participate to cooperation: in fact, since /Users/rubber/linux/block/bfq-iosched.c: 2707
 * the requests temporarily redirected to the OOM queue could be redirected /Users/rubber/linux/block/bfq-iosched.c: 2708
 * again to dedicated queues at any time, the state needed to correctly /Users/rubber/linux/block/bfq-iosched.c: 2709
 * handle merging with the OOM queue would be quite complex and expensive /Users/rubber/linux/block/bfq-iosched.c: 2710
 * to maintain. Besides, in such a critical condition as an out of memory, /Users/rubber/linux/block/bfq-iosched.c: 2711
 * the benefits of queue merging may be little relevant, or even negligible. /Users/rubber/linux/block/bfq-iosched.c: 2712
 * WARNING: queue merging may impair fairness among non-weight raised /Users/rubber/linux/block/bfq-iosched.c: 2714
 * queues, for at least two reasons: 1) the original weight of a /Users/rubber/linux/block/bfq-iosched.c: 2715
 * merged queue may change during the merged state, 2) even being the /Users/rubber/linux/block/bfq-iosched.c: 2716
 * weight the same, a merged queue may be bloated with many more /Users/rubber/linux/block/bfq-iosched.c: 2717
 * requests than the ones produced by its originally-associated /Users/rubber/linux/block/bfq-iosched.c: 2718
 * process. /Users/rubber/linux/block/bfq-iosched.c: 2719
	/* /Users/rubber/linux/block/bfq-iosched.c: 2727
	 * Check delayed stable merge for rotational or non-queueing /Users/rubber/linux/block/bfq-iosched.c: 2728
	 * devs. For this branch to be executed, bfqq must not be /Users/rubber/linux/block/bfq-iosched.c: 2729
	 * currently merged with some other queue (i.e., bfqq->bic /Users/rubber/linux/block/bfq-iosched.c: 2730
	 * must be non null). If we considered also merged queues, /Users/rubber/linux/block/bfq-iosched.c: 2731
	 * then we should also check whether bfqq has already been /Users/rubber/linux/block/bfq-iosched.c: 2732
	 * merged with bic->stable_merge_bfqq. But this would be /Users/rubber/linux/block/bfq-iosched.c: 2733
	 * costly and complicated. /Users/rubber/linux/block/bfq-iosched.c: 2734
		/* /Users/rubber/linux/block/bfq-iosched.c: 2737
		 * Make sure also that bfqq is sync, because /Users/rubber/linux/block/bfq-iosched.c: 2738
		 * bic->stable_merge_bfqq may point to some queue (for /Users/rubber/linux/block/bfq-iosched.c: 2739
		 * stable merging) also if bic is associated with a /Users/rubber/linux/block/bfq-iosched.c: 2740
		 * sync queue, but this bfqq is async /Users/rubber/linux/block/bfq-iosched.c: 2741
	/* /Users/rubber/linux/block/bfq-iosched.c: 2774
	 * Do not perform queue merging if the device is non /Users/rubber/linux/block/bfq-iosched.c: 2775
	 * rotational and performs internal queueing. In fact, such a /Users/rubber/linux/block/bfq-iosched.c: 2776
	 * device reaches a high speed through internal parallelism /Users/rubber/linux/block/bfq-iosched.c: 2777
	 * and pipelining. This means that, to reach a high /Users/rubber/linux/block/bfq-iosched.c: 2778
	 * throughput, it must have many requests enqueued at the same /Users/rubber/linux/block/bfq-iosched.c: 2779
	 * time. But, in this configuration, the internal scheduling /Users/rubber/linux/block/bfq-iosched.c: 2780
	 * algorithm of the device does exactly the job of queue /Users/rubber/linux/block/bfq-iosched.c: 2781
	 * merging: it reorders requests so as to obtain as much as /Users/rubber/linux/block/bfq-iosched.c: 2782
	 * possible a sequential I/O pattern. As a consequence, with /Users/rubber/linux/block/bfq-iosched.c: 2783
	 * the workload generated by processes doing interleaved I/O, /Users/rubber/linux/block/bfq-iosched.c: 2784
	 * the throughput reached by the device is likely to be the /Users/rubber/linux/block/bfq-iosched.c: 2785
	 * same, with and without queue merging. /Users/rubber/linux/block/bfq-iosched.c: 2786
	 * /Users/rubber/linux/block/bfq-iosched.c: 2787
	 * Disabling merging also provides a remarkable benefit in /Users/rubber/linux/block/bfq-iosched.c: 2788
	 * terms of throughput. Merging tends to make many workloads /Users/rubber/linux/block/bfq-iosched.c: 2789
	 * artificially more uneven, because of shared queues /Users/rubber/linux/block/bfq-iosched.c: 2790
	 * remaining non empty for incomparably more time than /Users/rubber/linux/block/bfq-iosched.c: 2791
	 * non-merged queues. This may accentuate workload /Users/rubber/linux/block/bfq-iosched.c: 2792
	 * asymmetries. For example, if one of the queues in a set of /Users/rubber/linux/block/bfq-iosched.c: 2793
	 * merged queues has a higher weight than a normal queue, then /Users/rubber/linux/block/bfq-iosched.c: 2794
	 * the shared queue may inherit such a high weight and, by /Users/rubber/linux/block/bfq-iosched.c: 2795
	 * staying almost always active, may force BFQ to perform I/O /Users/rubber/linux/block/bfq-iosched.c: 2796
	 * plugging most of the time. This evidently makes it harder /Users/rubber/linux/block/bfq-iosched.c: 2797
	 * for BFQ to let the device reach a high throughput. /Users/rubber/linux/block/bfq-iosched.c: 2798
	 * /Users/rubber/linux/block/bfq-iosched.c: 2799
	 * Finally, the likely() macro below is not used because one /Users/rubber/linux/block/bfq-iosched.c: 2800
	 * of the two branches is more likely than the other, but to /Users/rubber/linux/block/bfq-iosched.c: 2801
	 * have the code path after the following if() executed as /Users/rubber/linux/block/bfq-iosched.c: 2802
	 * fast as possible for the case of a non rotational device /Users/rubber/linux/block/bfq-iosched.c: 2803
	 * with queueing. We want it because this is the fastest kind /Users/rubber/linux/block/bfq-iosched.c: 2804
	 * of device. On the opposite end, the likely() may lengthen /Users/rubber/linux/block/bfq-iosched.c: 2805
	 * the execution time of BFQ for the case of slower devices /Users/rubber/linux/block/bfq-iosched.c: 2806
	 * (rotational or at least without queueing). But in this case /Users/rubber/linux/block/bfq-iosched.c: 2807
	 * the execution time of BFQ matters very little, if not at /Users/rubber/linux/block/bfq-iosched.c: 2808
	 * all. /Users/rubber/linux/block/bfq-iosched.c: 2809
	/* /Users/rubber/linux/block/bfq-iosched.c: 2814
	 * Prevent bfqq from being merged if it has been created too /Users/rubber/linux/block/bfq-iosched.c: 2815
	 * long ago. The idea is that true cooperating processes, and /Users/rubber/linux/block/bfq-iosched.c: 2816
	 * thus their associated bfq_queues, are supposed to be /Users/rubber/linux/block/bfq-iosched.c: 2817
	 * created shortly after each other. This is the case, e.g., /Users/rubber/linux/block/bfq-iosched.c: 2818
	 * for KVM/QEMU and dump I/O threads. Basing on this /Users/rubber/linux/block/bfq-iosched.c: 2819
	 * assumption, the following filtering greatly reduces the /Users/rubber/linux/block/bfq-iosched.c: 2820
	 * probability that two non-cooperating processes, which just /Users/rubber/linux/block/bfq-iosched.c: 2821
	 * happen to do close I/O for some short time interval, have /Users/rubber/linux/block/bfq-iosched.c: 2822
	 * their queues merged by mistake. /Users/rubber/linux/block/bfq-iosched.c: 2823
	/* /Users/rubber/linux/block/bfq-iosched.c: 2850
	 * Check whether there is a cooperator among currently scheduled /Users/rubber/linux/block/bfq-iosched.c: 2851
	 * queues. The only thing we need is that the bio/request is not /Users/rubber/linux/block/bfq-iosched.c: 2852
	 * NULL, as we need it to establish whether a cooperator exists. /Users/rubber/linux/block/bfq-iosched.c: 2853
	/* /Users/rubber/linux/block/bfq-iosched.c: 2869
	 * If !bfqq->bic, the queue is already shared or its requests /Users/rubber/linux/block/bfq-iosched.c: 2870
	 * have already been redirected to a shared queue; both idle window /Users/rubber/linux/block/bfq-iosched.c: 2871
	 * and weight raising state have already been saved. Do nothing. /Users/rubber/linux/block/bfq-iosched.c: 2872
		/* /Users/rubber/linux/block/bfq-iosched.c: 2892
		 * bfqq being merged right after being created: bfqq /Users/rubber/linux/block/bfq-iosched.c: 2893
		 * would have deserved interactive weight raising, but /Users/rubber/linux/block/bfq-iosched.c: 2894
		 * did not make it to be set in a weight-raised state, /Users/rubber/linux/block/bfq-iosched.c: 2895
		 * because of this early merge.	Store directly the /Users/rubber/linux/block/bfq-iosched.c: 2896
		 * weight-raising state that would have been assigned /Users/rubber/linux/block/bfq-iosched.c: 2897
		 * to bfqq, so that to avoid that bfqq unjustly fails /Users/rubber/linux/block/bfq-iosched.c: 2898
		 * to enjoy weight raising if split soon. /Users/rubber/linux/block/bfq-iosched.c: 2899
	/* /Users/rubber/linux/block/bfq-iosched.c: 2928
	 * To prevent bfqq's service guarantees from being violated, /Users/rubber/linux/block/bfq-iosched.c: 2929
	 * bfqq may be left busy, i.e., queued for service, even if /Users/rubber/linux/block/bfq-iosched.c: 2930
	 * empty (see comments in __bfq_bfqq_expire() for /Users/rubber/linux/block/bfq-iosched.c: 2931
	 * details). But, if no process will send requests to bfqq any /Users/rubber/linux/block/bfq-iosched.c: 2932
	 * longer, then there is no point in keeping bfqq queued for /Users/rubber/linux/block/bfq-iosched.c: 2933
	 * service. In addition, keeping bfqq queued for service, but /Users/rubber/linux/block/bfq-iosched.c: 2934
	 * with no process ref any longer, may have caused bfqq to be /Users/rubber/linux/block/bfq-iosched.c: 2935
	 * freed when dequeued from service. But this is assumed to /Users/rubber/linux/block/bfq-iosched.c: 2936
	 * never happen. /Users/rubber/linux/block/bfq-iosched.c: 2937
	/* /Users/rubber/linux/block/bfq-iosched.c: 2961
	 * The processes associated with bfqq are cooperators of the /Users/rubber/linux/block/bfq-iosched.c: 2962
	 * processes associated with new_bfqq. So, if bfqq has a /Users/rubber/linux/block/bfq-iosched.c: 2963
	 * waker, then assume that all these processes will be happy /Users/rubber/linux/block/bfq-iosched.c: 2964
	 * to let bfqq's waker freely inject I/O when they have no /Users/rubber/linux/block/bfq-iosched.c: 2965
	 * I/O. /Users/rubber/linux/block/bfq-iosched.c: 2966
		/* /Users/rubber/linux/block/bfq-iosched.c: 2973
		 * If the waker queue disappears, then /Users/rubber/linux/block/bfq-iosched.c: 2974
		 * new_bfqq->waker_bfqq must be reset. So insert /Users/rubber/linux/block/bfq-iosched.c: 2975
		 * new_bfqq into the woken_list of the waker. See /Users/rubber/linux/block/bfq-iosched.c: 2976
		 * bfq_check_waker for details. /Users/rubber/linux/block/bfq-iosched.c: 2977
	/* /Users/rubber/linux/block/bfq-iosched.c: 2984
	 * If bfqq is weight-raised, then let new_bfqq inherit /Users/rubber/linux/block/bfq-iosched.c: 2985
	 * weight-raising. To reduce false positives, neglect the case /Users/rubber/linux/block/bfq-iosched.c: 2986
	 * where bfqq has just been created, but has not yet made it /Users/rubber/linux/block/bfq-iosched.c: 2987
	 * to be weight-raised (which may happen because EQM may merge /Users/rubber/linux/block/bfq-iosched.c: 2988
	 * bfqq even before bfq_add_request is executed for the first /Users/rubber/linux/block/bfq-iosched.c: 2989
	 * time for bfqq). Handling this case would however be very /Users/rubber/linux/block/bfq-iosched.c: 2990
	 * easy, thanks to the flag just_created. /Users/rubber/linux/block/bfq-iosched.c: 2991
	/* /Users/rubber/linux/block/bfq-iosched.c: 3014
	 * Merge queues (that is, let bic redirect its requests to new_bfqq) /Users/rubber/linux/block/bfq-iosched.c: 3015
	/* /Users/rubber/linux/block/bfq-iosched.c: 3019
	 * new_bfqq now belongs to at least two bics (it is a shared queue): /Users/rubber/linux/block/bfq-iosched.c: 3020
	 * set new_bfqq->bic to NULL. bfqq either: /Users/rubber/linux/block/bfq-iosched.c: 3021
	 * - does not belong to any bic any more, and hence bfqq->bic must /Users/rubber/linux/block/bfq-iosched.c: 3022
	 *   be set to NULL, or /Users/rubber/linux/block/bfq-iosched.c: 3023
	 * - is a queue whose owning bics have already been redirected to a /Users/rubber/linux/block/bfq-iosched.c: 3024
	 *   different queue, hence the queue is destined to not belong to /Users/rubber/linux/block/bfq-iosched.c: 3025
	 *   any bic soon and bfqq->bic is already NULL (therefore the next /Users/rubber/linux/block/bfq-iosched.c: 3026
	 *   assignment causes no harm). /Users/rubber/linux/block/bfq-iosched.c: 3027
	/* /Users/rubber/linux/block/bfq-iosched.c: 3030
	 * If the queue is shared, the pid is the pid of one of the associated /Users/rubber/linux/block/bfq-iosched.c: 3031
	 * processes. Which pid depends on the exact sequence of merge events /Users/rubber/linux/block/bfq-iosched.c: 3032
	 * the queue underwent. So printing such a pid is useless and confusing /Users/rubber/linux/block/bfq-iosched.c: 3033
	 * because it reports a random pid between those of the associated /Users/rubber/linux/block/bfq-iosched.c: 3034
	 * processes. /Users/rubber/linux/block/bfq-iosched.c: 3035
	 * We mark such a queue with a pid -1, and then print SHARED instead of /Users/rubber/linux/block/bfq-iosched.c: 3036
	 * a pid in logging messages. /Users/rubber/linux/block/bfq-iosched.c: 3037
	/* /Users/rubber/linux/block/bfq-iosched.c: 3054
	 * Disallow merge of a sync bio into an async request. /Users/rubber/linux/block/bfq-iosched.c: 3055
	/* /Users/rubber/linux/block/bfq-iosched.c: 3060
	 * Lookup the bfqq that this bio will be queued with. Allow /Users/rubber/linux/block/bfq-iosched.c: 3061
	 * merge only if rq is queued there. /Users/rubber/linux/block/bfq-iosched.c: 3062
	/* /Users/rubber/linux/block/bfq-iosched.c: 3067
	 * We take advantage of this function to perform an early merge /Users/rubber/linux/block/bfq-iosched.c: 3068
	 * of the queues of possible cooperating processes. /Users/rubber/linux/block/bfq-iosched.c: 3069
		/* /Users/rubber/linux/block/bfq-iosched.c: 3073
		 * bic still points to bfqq, then it has not yet been /Users/rubber/linux/block/bfq-iosched.c: 3074
		 * redirected to some other bfq_queue, and a queue /Users/rubber/linux/block/bfq-iosched.c: 3075
		 * merge between bfqq and new_bfqq can be safely /Users/rubber/linux/block/bfq-iosched.c: 3076
		 * fulfilled, i.e., bic can be redirected to new_bfqq /Users/rubber/linux/block/bfq-iosched.c: 3077
		 * and bfqq can be put. /Users/rubber/linux/block/bfq-iosched.c: 3078
		/* /Users/rubber/linux/block/bfq-iosched.c: 3082
		 * If we get here, bio will be queued into new_queue, /Users/rubber/linux/block/bfq-iosched.c: 3083
		 * so use new_bfqq to decide whether bio and rq can be /Users/rubber/linux/block/bfq-iosched.c: 3084
		 * merged. /Users/rubber/linux/block/bfq-iosched.c: 3085
		/* /Users/rubber/linux/block/bfq-iosched.c: 3089
		 * Change also bqfd->bio_bfqq, as /Users/rubber/linux/block/bfq-iosched.c: 3090
		 * bfqd->bio_bic now points to new_bfqq, and /Users/rubber/linux/block/bfq-iosched.c: 3091
		 * this function may be invoked again (and then may /Users/rubber/linux/block/bfq-iosched.c: 3092
		 * use again bqfd->bio_bfqq). /Users/rubber/linux/block/bfq-iosched.c: 3093
 * Set the maximum time for the in-service queue to consume its /Users/rubber/linux/block/bfq-iosched.c: 3102
 * budget. This prevents seeky processes from lowering the throughput. /Users/rubber/linux/block/bfq-iosched.c: 3103
 * In practice, a time-slice service scheme is used with seeky /Users/rubber/linux/block/bfq-iosched.c: 3104
 * processes. /Users/rubber/linux/block/bfq-iosched.c: 3105
			/* /Users/rubber/linux/block/bfq-iosched.c: 3135
			 * For soft real-time queues, move the start /Users/rubber/linux/block/bfq-iosched.c: 3136
			 * of the weight-raising period forward by the /Users/rubber/linux/block/bfq-iosched.c: 3137
			 * time the queue has not received any /Users/rubber/linux/block/bfq-iosched.c: 3138
			 * service. Otherwise, a relatively long /Users/rubber/linux/block/bfq-iosched.c: 3139
			 * service delay is likely to cause the /Users/rubber/linux/block/bfq-iosched.c: 3140
			 * weight-raising period of the queue to end, /Users/rubber/linux/block/bfq-iosched.c: 3141
			 * because of the short duration of the /Users/rubber/linux/block/bfq-iosched.c: 3142
			 * weight-raising period of a soft real-time /Users/rubber/linux/block/bfq-iosched.c: 3143
			 * queue.  It is worth noting that this move /Users/rubber/linux/block/bfq-iosched.c: 3144
			 * is not so dangerous for the other queues, /Users/rubber/linux/block/bfq-iosched.c: 3145
			 * because soft real-time queues are not /Users/rubber/linux/block/bfq-iosched.c: 3146
			 * greedy. /Users/rubber/linux/block/bfq-iosched.c: 3147
			 * /Users/rubber/linux/block/bfq-iosched.c: 3148
			 * To not add a further variable, we use the /Users/rubber/linux/block/bfq-iosched.c: 3149
			 * overloaded field budget_timeout to /Users/rubber/linux/block/bfq-iosched.c: 3150
			 * determine for how long the queue has not /Users/rubber/linux/block/bfq-iosched.c: 3151
			 * received service, i.e., how much time has /Users/rubber/linux/block/bfq-iosched.c: 3152
			 * elapsed since the queue expired. However, /Users/rubber/linux/block/bfq-iosched.c: 3153
			 * this is a little imprecise, because /Users/rubber/linux/block/bfq-iosched.c: 3154
			 * budget_timeout is set to jiffies if bfqq /Users/rubber/linux/block/bfq-iosched.c: 3155
			 * not only expires, but also remains with no /Users/rubber/linux/block/bfq-iosched.c: 3156
			 * request. /Users/rubber/linux/block/bfq-iosched.c: 3157
 * Get and set a new queue for service. /Users/rubber/linux/block/bfq-iosched.c: 3178
	/* /Users/rubber/linux/block/bfq-iosched.c: 3195
	 * We don't want to idle for seeks, but we do want to allow /Users/rubber/linux/block/bfq-iosched.c: 3196
	 * fair distribution of slice time for a process doing back-to-back /Users/rubber/linux/block/bfq-iosched.c: 3197
	 * seeks. So allow a little bit of time for him to submit a new rq. /Users/rubber/linux/block/bfq-iosched.c: 3198
	/* /Users/rubber/linux/block/bfq-iosched.c: 3201
	 * Unless the queue is being weight-raised or the scenario is /Users/rubber/linux/block/bfq-iosched.c: 3202
	 * asymmetric, grant only minimum idle time if the queue /Users/rubber/linux/block/bfq-iosched.c: 3203
	 * is seeky. A long idling is preserved for a weight-raised /Users/rubber/linux/block/bfq-iosched.c: 3204
	 * queue, or, more in general, in an asymmetric scenario, /Users/rubber/linux/block/bfq-iosched.c: 3205
	 * because a long idling is needed for guaranteeing to a queue /Users/rubber/linux/block/bfq-iosched.c: 3206
	 * its reserved share of the throughput (in particular, it is /Users/rubber/linux/block/bfq-iosched.c: 3207
	 * needed if the queue has a higher weight than some other /Users/rubber/linux/block/bfq-iosched.c: 3208
	 * queue). /Users/rubber/linux/block/bfq-iosched.c: 3209
 * In autotuning mode, max_budget is dynamically recomputed as the /Users/rubber/linux/block/bfq-iosched.c: 3226
 * amount of sectors transferred in timeout at the estimated peak /Users/rubber/linux/block/bfq-iosched.c: 3227
 * rate. This enables BFQ to utilize a full timeslice with a full /Users/rubber/linux/block/bfq-iosched.c: 3228
 * budget, even if the in-service queue is served at peak rate. And /Users/rubber/linux/block/bfq-iosched.c: 3229
 * this maximises throughput with sequential workloads. /Users/rubber/linux/block/bfq-iosched.c: 3230
 * Update parameters related to throughput and responsiveness, as a /Users/rubber/linux/block/bfq-iosched.c: 3239
 * function of the estimated peak rate. See comments on /Users/rubber/linux/block/bfq-iosched.c: 3240
 * bfq_calc_max_budget(), and on the ref_wr_duration array. /Users/rubber/linux/block/bfq-iosched.c: 3241
	/* /Users/rubber/linux/block/bfq-iosched.c: 3274
	 * For the convergence property to hold (see comments on /Users/rubber/linux/block/bfq-iosched.c: 3275
	 * bfq_update_peak_rate()) and for the assessment to be /Users/rubber/linux/block/bfq-iosched.c: 3276
	 * reliable, a minimum number of samples must be present, and /Users/rubber/linux/block/bfq-iosched.c: 3277
	 * a minimum amount of time must have elapsed. If not so, do /Users/rubber/linux/block/bfq-iosched.c: 3278
	 * not compute new rate. Just reset parameters, to get ready /Users/rubber/linux/block/bfq-iosched.c: 3279
	 * for a new evaluation attempt. /Users/rubber/linux/block/bfq-iosched.c: 3280
	/* /Users/rubber/linux/block/bfq-iosched.c: 3286
	 * If a new request completion has occurred after last /Users/rubber/linux/block/bfq-iosched.c: 3287
	 * dispatch, then, to approximate the rate at which requests /Users/rubber/linux/block/bfq-iosched.c: 3288
	 * have been served by the device, it is more precise to /Users/rubber/linux/block/bfq-iosched.c: 3289
	 * extend the observation interval to the last completion. /Users/rubber/linux/block/bfq-iosched.c: 3290
	/* /Users/rubber/linux/block/bfq-iosched.c: 3296
	 * Rate computed in sects/usec, and not sects/nsec, for /Users/rubber/linux/block/bfq-iosched.c: 3297
	 * precision issues. /Users/rubber/linux/block/bfq-iosched.c: 3298
	/* /Users/rubber/linux/block/bfq-iosched.c: 3303
	 * Peak rate not updated if: /Users/rubber/linux/block/bfq-iosched.c: 3304
	 * - the percentage of sequential dispatches is below 3/4 of the /Users/rubber/linux/block/bfq-iosched.c: 3305
	 *   total, and rate is below the current estimated peak rate /Users/rubber/linux/block/bfq-iosched.c: 3306
	 * - rate is unreasonably high (> 20M sectors/sec) /Users/rubber/linux/block/bfq-iosched.c: 3307
	/* /Users/rubber/linux/block/bfq-iosched.c: 3314
	 * We have to update the peak rate, at last! To this purpose, /Users/rubber/linux/block/bfq-iosched.c: 3315
	 * we use a low-pass filter. We compute the smoothing constant /Users/rubber/linux/block/bfq-iosched.c: 3316
	 * of the filter as a function of the 'weight' of the new /Users/rubber/linux/block/bfq-iosched.c: 3317
	 * measured rate. /Users/rubber/linux/block/bfq-iosched.c: 3318
	 * /Users/rubber/linux/block/bfq-iosched.c: 3319
	 * As can be seen in next formulas, we define this weight as a /Users/rubber/linux/block/bfq-iosched.c: 3320
	 * quantity proportional to how sequential the workload is, /Users/rubber/linux/block/bfq-iosched.c: 3321
	 * and to how long the observation time interval is. /Users/rubber/linux/block/bfq-iosched.c: 3322
	 * /Users/rubber/linux/block/bfq-iosched.c: 3323
	 * The weight runs from 0 to 8. The maximum value of the /Users/rubber/linux/block/bfq-iosched.c: 3324
	 * weight, 8, yields the minimum value for the smoothing /Users/rubber/linux/block/bfq-iosched.c: 3325
	 * constant. At this minimum value for the smoothing constant, /Users/rubber/linux/block/bfq-iosched.c: 3326
	 * the measured rate contributes for half of the next value of /Users/rubber/linux/block/bfq-iosched.c: 3327
	 * the estimated peak rate. /Users/rubber/linux/block/bfq-iosched.c: 3328
	 * /Users/rubber/linux/block/bfq-iosched.c: 3329
	 * So, the first step is to compute the weight as a function /Users/rubber/linux/block/bfq-iosched.c: 3330
	 * of how sequential the workload is. Note that the weight /Users/rubber/linux/block/bfq-iosched.c: 3331
	 * cannot reach 9, because bfqd->sequential_samples cannot /Users/rubber/linux/block/bfq-iosched.c: 3332
	 * become equal to bfqd->peak_rate_samples, which, in its /Users/rubber/linux/block/bfq-iosched.c: 3333
	 * turn, holds true because bfqd->sequential_samples is not /Users/rubber/linux/block/bfq-iosched.c: 3334
	 * incremented for the first sample. /Users/rubber/linux/block/bfq-iosched.c: 3335
	/* /Users/rubber/linux/block/bfq-iosched.c: 3339
	 * Second step: further refine the weight as a function of the /Users/rubber/linux/block/bfq-iosched.c: 3340
	 * duration of the observation interval. /Users/rubber/linux/block/bfq-iosched.c: 3341
	/* /Users/rubber/linux/block/bfq-iosched.c: 3347
	 * Divisor ranging from 10, for minimum weight, to 2, for /Users/rubber/linux/block/bfq-iosched.c: 3348
	 * maximum weight. /Users/rubber/linux/block/bfq-iosched.c: 3349
	/* /Users/rubber/linux/block/bfq-iosched.c: 3353
	 * Finally, update peak rate: /Users/rubber/linux/block/bfq-iosched.c: 3354
	 * /Users/rubber/linux/block/bfq-iosched.c: 3355
	 * peak_rate = peak_rate * (divisor-1) / divisor  +  rate / divisor /Users/rubber/linux/block/bfq-iosched.c: 3356
	/* /Users/rubber/linux/block/bfq-iosched.c: 3364
	 * For a very slow device, bfqd->peak_rate can reach 0 (see /Users/rubber/linux/block/bfq-iosched.c: 3365
	 * the minimum representable values reported in the comments /Users/rubber/linux/block/bfq-iosched.c: 3366
	 * on BFQ_RATE_SHIFT). Push to 1 if this happens, to avoid /Users/rubber/linux/block/bfq-iosched.c: 3367
	 * divisions by zero where bfqd->peak_rate is used as a /Users/rubber/linux/block/bfq-iosched.c: 3368
	 * divisor. /Users/rubber/linux/block/bfq-iosched.c: 3369
 * Update the read/write peak rate (the main quantity used for /Users/rubber/linux/block/bfq-iosched.c: 3380
 * auto-tuning, see update_thr_responsiveness_params()). /Users/rubber/linux/block/bfq-iosched.c: 3381
 * It is not trivial to estimate the peak rate (correctly): because of /Users/rubber/linux/block/bfq-iosched.c: 3383
 * the presence of sw and hw queues between the scheduler and the /Users/rubber/linux/block/bfq-iosched.c: 3384
 * device components that finally serve I/O requests, it is hard to /Users/rubber/linux/block/bfq-iosched.c: 3385
 * say exactly when a given dispatched request is served inside the /Users/rubber/linux/block/bfq-iosched.c: 3386
 * device, and for how long. As a consequence, it is hard to know /Users/rubber/linux/block/bfq-iosched.c: 3387
 * precisely at what rate a given set of requests is actually served /Users/rubber/linux/block/bfq-iosched.c: 3388
 * by the device. /Users/rubber/linux/block/bfq-iosched.c: 3389
 * On the opposite end, the dispatch time of any request is trivially /Users/rubber/linux/block/bfq-iosched.c: 3391
 * available, and, from this piece of information, the "dispatch rate" /Users/rubber/linux/block/bfq-iosched.c: 3392
 * of requests can be immediately computed. So, the idea in the next /Users/rubber/linux/block/bfq-iosched.c: 3393
 * function is to use what is known, namely request dispatch times /Users/rubber/linux/block/bfq-iosched.c: 3394
 * (plus, when useful, request completion times), to estimate what is /Users/rubber/linux/block/bfq-iosched.c: 3395
 * unknown, namely in-device request service rate. /Users/rubber/linux/block/bfq-iosched.c: 3396
 * The main issue is that, because of the above facts, the rate at /Users/rubber/linux/block/bfq-iosched.c: 3398
 * which a certain set of requests is dispatched over a certain time /Users/rubber/linux/block/bfq-iosched.c: 3399
 * interval can vary greatly with respect to the rate at which the /Users/rubber/linux/block/bfq-iosched.c: 3400
 * same requests are then served. But, since the size of any /Users/rubber/linux/block/bfq-iosched.c: 3401
 * intermediate queue is limited, and the service scheme is lossless /Users/rubber/linux/block/bfq-iosched.c: 3402
 * (no request is silently dropped), the following obvious convergence /Users/rubber/linux/block/bfq-iosched.c: 3403
 * property holds: the number of requests dispatched MUST become /Users/rubber/linux/block/bfq-iosched.c: 3404
 * closer and closer to the number of requests completed as the /Users/rubber/linux/block/bfq-iosched.c: 3405
 * observation interval grows. This is the key property used in /Users/rubber/linux/block/bfq-iosched.c: 3406
 * the next function to estimate the peak service rate as a function /Users/rubber/linux/block/bfq-iosched.c: 3407
 * of the observed dispatch rate. The function assumes to be invoked /Users/rubber/linux/block/bfq-iosched.c: 3408
 * on every request dispatch. /Users/rubber/linux/block/bfq-iosched.c: 3409
	/* /Users/rubber/linux/block/bfq-iosched.c: 3422
	 * Device idle for very long: the observation interval lasting /Users/rubber/linux/block/bfq-iosched.c: 3423
	 * up to this dispatch cannot be a valid observation interval /Users/rubber/linux/block/bfq-iosched.c: 3424
	 * for computing a new peak rate (similarly to the late- /Users/rubber/linux/block/bfq-iosched.c: 3425
	 * completion event in bfq_completed_request()). Go to /Users/rubber/linux/block/bfq-iosched.c: 3426
	 * update_rate_and_reset to have the following three steps /Users/rubber/linux/block/bfq-iosched.c: 3427
	 * taken: /Users/rubber/linux/block/bfq-iosched.c: 3428
	 * - close the observation interval at the last (previous) /Users/rubber/linux/block/bfq-iosched.c: 3429
	 *   request dispatch or completion /Users/rubber/linux/block/bfq-iosched.c: 3430
	 * - compute rate, if possible, for that observation interval /Users/rubber/linux/block/bfq-iosched.c: 3431
	 * - start a new observation interval with this dispatch /Users/rubber/linux/block/bfq-iosched.c: 3432
 * Remove request from internal lists. /Users/rubber/linux/block/bfq-iosched.c: 3471
	/* /Users/rubber/linux/block/bfq-iosched.c: 3477
	 * For consistency, the next instruction should have been /Users/rubber/linux/block/bfq-iosched.c: 3478
	 * executed after removing the request from the queue and /Users/rubber/linux/block/bfq-iosched.c: 3479
	 * dispatching it.  We execute instead this instruction before /Users/rubber/linux/block/bfq-iosched.c: 3480
	 * bfq_remove_request() (and hence introduce a temporary /Users/rubber/linux/block/bfq-iosched.c: 3481
	 * inconsistency), for efficiency.  In fact, should this /Users/rubber/linux/block/bfq-iosched.c: 3482
	 * dispatch occur for a non in-service bfqq, this anticipated /Users/rubber/linux/block/bfq-iosched.c: 3483
	 * increment prevents two counters related to bfqq->dispatched /Users/rubber/linux/block/bfq-iosched.c: 3484
	 * from risking to be, first, uselessly decremented, and then /Users/rubber/linux/block/bfq-iosched.c: 3485
	 * incremented again when the (new) value of bfqq->dispatched /Users/rubber/linux/block/bfq-iosched.c: 3486
	 * happens to be taken into account. /Users/rubber/linux/block/bfq-iosched.c: 3487
 * There is a case where idling does not have to be performed for /Users/rubber/linux/block/bfq-iosched.c: 3496
 * throughput concerns, but to preserve the throughput share of /Users/rubber/linux/block/bfq-iosched.c: 3497
 * the process associated with bfqq. /Users/rubber/linux/block/bfq-iosched.c: 3498
 * To introduce this case, we can note that allowing the drive /Users/rubber/linux/block/bfq-iosched.c: 3500
 * to enqueue more than one request at a time, and hence /Users/rubber/linux/block/bfq-iosched.c: 3501
 * delegating de facto final scheduling decisions to the /Users/rubber/linux/block/bfq-iosched.c: 3502
 * drive's internal scheduler, entails loss of control on the /Users/rubber/linux/block/bfq-iosched.c: 3503
 * actual request service order. In particular, the critical /Users/rubber/linux/block/bfq-iosched.c: 3504
 * situation is when requests from different processes happen /Users/rubber/linux/block/bfq-iosched.c: 3505
 * to be present, at the same time, in the internal queue(s) /Users/rubber/linux/block/bfq-iosched.c: 3506
 * of the drive. In such a situation, the drive, by deciding /Users/rubber/linux/block/bfq-iosched.c: 3507
 * the service order of the internally-queued requests, does /Users/rubber/linux/block/bfq-iosched.c: 3508
 * determine also the actual throughput distribution among /Users/rubber/linux/block/bfq-iosched.c: 3509
 * these processes. But the drive typically has no notion or /Users/rubber/linux/block/bfq-iosched.c: 3510
 * concern about per-process throughput distribution, and /Users/rubber/linux/block/bfq-iosched.c: 3511
 * makes its decisions only on a per-request basis. Therefore, /Users/rubber/linux/block/bfq-iosched.c: 3512
 * the service distribution enforced by the drive's internal /Users/rubber/linux/block/bfq-iosched.c: 3513
 * scheduler is likely to coincide with the desired throughput /Users/rubber/linux/block/bfq-iosched.c: 3514
 * distribution only in a completely symmetric, or favorably /Users/rubber/linux/block/bfq-iosched.c: 3515
 * skewed scenario where: /Users/rubber/linux/block/bfq-iosched.c: 3516
 * (i-a) each of these processes must get the same throughput as /Users/rubber/linux/block/bfq-iosched.c: 3517
 *	 the others, /Users/rubber/linux/block/bfq-iosched.c: 3518
 * (i-b) in case (i-a) does not hold, it holds that the process /Users/rubber/linux/block/bfq-iosched.c: 3519
 *       associated with bfqq must receive a lower or equal /Users/rubber/linux/block/bfq-iosched.c: 3520
 *	 throughput than any of the other processes; /Users/rubber/linux/block/bfq-iosched.c: 3521
 * (ii)  the I/O of each process has the same properties, in /Users/rubber/linux/block/bfq-iosched.c: 3522
 *       terms of locality (sequential or random), direction /Users/rubber/linux/block/bfq-iosched.c: 3523
 *       (reads or writes), request sizes, greediness /Users/rubber/linux/block/bfq-iosched.c: 3524
 *       (from I/O-bound to sporadic), and so on; /Users/rubber/linux/block/bfq-iosched.c: 3525
 * In fact, in such a scenario, the drive tends to treat the requests /Users/rubber/linux/block/bfq-iosched.c: 3527
 * of each process in about the same way as the requests of the /Users/rubber/linux/block/bfq-iosched.c: 3528
 * others, and thus to provide each of these processes with about the /Users/rubber/linux/block/bfq-iosched.c: 3529
 * same throughput.  This is exactly the desired throughput /Users/rubber/linux/block/bfq-iosched.c: 3530
 * distribution if (i-a) holds, or, if (i-b) holds instead, this is an /Users/rubber/linux/block/bfq-iosched.c: 3531
 * even more convenient distribution for (the process associated with) /Users/rubber/linux/block/bfq-iosched.c: 3532
 * bfqq. /Users/rubber/linux/block/bfq-iosched.c: 3533
 * In contrast, in any asymmetric or unfavorable scenario, device /Users/rubber/linux/block/bfq-iosched.c: 3535
 * idling (I/O-dispatch plugging) is certainly needed to guarantee /Users/rubber/linux/block/bfq-iosched.c: 3536
 * that bfqq receives its assigned fraction of the device throughput /Users/rubber/linux/block/bfq-iosched.c: 3537
 * (see [1] for details). /Users/rubber/linux/block/bfq-iosched.c: 3538
 * The problem is that idling may significantly reduce throughput with /Users/rubber/linux/block/bfq-iosched.c: 3540
 * certain combinations of types of I/O and devices. An important /Users/rubber/linux/block/bfq-iosched.c: 3541
 * example is sync random I/O on flash storage with command /Users/rubber/linux/block/bfq-iosched.c: 3542
 * queueing. So, unless bfqq falls in cases where idling also boosts /Users/rubber/linux/block/bfq-iosched.c: 3543
 * throughput, it is important to check conditions (i-a), i(-b) and /Users/rubber/linux/block/bfq-iosched.c: 3544
 * (ii) accurately, so as to avoid idling when not strictly needed for /Users/rubber/linux/block/bfq-iosched.c: 3545
 * service guarantees. /Users/rubber/linux/block/bfq-iosched.c: 3546
 * Unfortunately, it is extremely difficult to thoroughly check /Users/rubber/linux/block/bfq-iosched.c: 3548
 * condition (ii). And, in case there are active groups, it becomes /Users/rubber/linux/block/bfq-iosched.c: 3549
 * very difficult to check conditions (i-a) and (i-b) too.  In fact, /Users/rubber/linux/block/bfq-iosched.c: 3550
 * if there are active groups, then, for conditions (i-a) or (i-b) to /Users/rubber/linux/block/bfq-iosched.c: 3551
 * become false 'indirectly', it is enough that an active group /Users/rubber/linux/block/bfq-iosched.c: 3552
 * contains more active processes or sub-groups than some other active /Users/rubber/linux/block/bfq-iosched.c: 3553
 * group. More precisely, for conditions (i-a) or (i-b) to become /Users/rubber/linux/block/bfq-iosched.c: 3554
 * false because of such a group, it is not even necessary that the /Users/rubber/linux/block/bfq-iosched.c: 3555
 * group is (still) active: it is sufficient that, even if the group /Users/rubber/linux/block/bfq-iosched.c: 3556
 * has become inactive, some of its descendant processes still have /Users/rubber/linux/block/bfq-iosched.c: 3557
 * some request already dispatched but still waiting for /Users/rubber/linux/block/bfq-iosched.c: 3558
 * completion. In fact, requests have still to be guaranteed their /Users/rubber/linux/block/bfq-iosched.c: 3559
 * share of the throughput even after being dispatched. In this /Users/rubber/linux/block/bfq-iosched.c: 3560
 * respect, it is easy to show that, if a group frequently becomes /Users/rubber/linux/block/bfq-iosched.c: 3561
 * inactive while still having in-flight requests, and if, when this /Users/rubber/linux/block/bfq-iosched.c: 3562
 * happens, the group is not considered in the calculation of whether /Users/rubber/linux/block/bfq-iosched.c: 3563
 * the scenario is asymmetric, then the group may fail to be /Users/rubber/linux/block/bfq-iosched.c: 3564
 * guaranteed its fair share of the throughput (basically because /Users/rubber/linux/block/bfq-iosched.c: 3565
 * idling may not be performed for the descendant processes of the /Users/rubber/linux/block/bfq-iosched.c: 3566
 * group, but it had to be).  We address this issue with the following /Users/rubber/linux/block/bfq-iosched.c: 3567
 * bi-modal behavior, implemented in the function /Users/rubber/linux/block/bfq-iosched.c: 3568
 * bfq_asymmetric_scenario(). /Users/rubber/linux/block/bfq-iosched.c: 3569
 * If there are groups with requests waiting for completion /Users/rubber/linux/block/bfq-iosched.c: 3571
 * (as commented above, some of these groups may even be /Users/rubber/linux/block/bfq-iosched.c: 3572
 * already inactive), then the scenario is tagged as /Users/rubber/linux/block/bfq-iosched.c: 3573
 * asymmetric, conservatively, without checking any of the /Users/rubber/linux/block/bfq-iosched.c: 3574
 * conditions (i-a), (i-b) or (ii). So the device is idled for bfqq. /Users/rubber/linux/block/bfq-iosched.c: 3575
 * This behavior matches also the fact that groups are created /Users/rubber/linux/block/bfq-iosched.c: 3576
 * exactly if controlling I/O is a primary concern (to /Users/rubber/linux/block/bfq-iosched.c: 3577
 * preserve bandwidth and latency guarantees). /Users/rubber/linux/block/bfq-iosched.c: 3578
 * On the opposite end, if there are no groups with requests waiting /Users/rubber/linux/block/bfq-iosched.c: 3580
 * for completion, then only conditions (i-a) and (i-b) are actually /Users/rubber/linux/block/bfq-iosched.c: 3581
 * controlled, i.e., provided that conditions (i-a) or (i-b) holds, /Users/rubber/linux/block/bfq-iosched.c: 3582
 * idling is not performed, regardless of whether condition (ii) /Users/rubber/linux/block/bfq-iosched.c: 3583
 * holds.  In other words, only if conditions (i-a) and (i-b) do not /Users/rubber/linux/block/bfq-iosched.c: 3584
 * hold, then idling is allowed, and the device tends to be prevented /Users/rubber/linux/block/bfq-iosched.c: 3585
 * from queueing many requests, possibly of several processes. Since /Users/rubber/linux/block/bfq-iosched.c: 3586
 * there are no groups with requests waiting for completion, then, to /Users/rubber/linux/block/bfq-iosched.c: 3587
 * control conditions (i-a) and (i-b) it is enough to check just /Users/rubber/linux/block/bfq-iosched.c: 3588
 * whether all the queues with requests waiting for completion also /Users/rubber/linux/block/bfq-iosched.c: 3589
 * have the same weight. /Users/rubber/linux/block/bfq-iosched.c: 3590
 * Not checking condition (ii) evidently exposes bfqq to the /Users/rubber/linux/block/bfq-iosched.c: 3592
 * risk of getting less throughput than its fair share. /Users/rubber/linux/block/bfq-iosched.c: 3593
 * However, for queues with the same weight, a further /Users/rubber/linux/block/bfq-iosched.c: 3594
 * mechanism, preemption, mitigates or even eliminates this /Users/rubber/linux/block/bfq-iosched.c: 3595
 * problem. And it does so without consequences on overall /Users/rubber/linux/block/bfq-iosched.c: 3596
 * throughput. This mechanism and its benefits are explained /Users/rubber/linux/block/bfq-iosched.c: 3597
 * in the next three paragraphs. /Users/rubber/linux/block/bfq-iosched.c: 3598
 * Even if a queue, say Q, is expired when it remains idle, Q /Users/rubber/linux/block/bfq-iosched.c: 3600
 * can still preempt the new in-service queue if the next /Users/rubber/linux/block/bfq-iosched.c: 3601
 * request of Q arrives soon (see the comments on /Users/rubber/linux/block/bfq-iosched.c: 3602
 * bfq_bfqq_update_budg_for_activation). If all queues and /Users/rubber/linux/block/bfq-iosched.c: 3603
 * groups have the same weight, this form of preemption, /Users/rubber/linux/block/bfq-iosched.c: 3604
 * combined with the hole-recovery heuristic described in the /Users/rubber/linux/block/bfq-iosched.c: 3605
 * comments on function bfq_bfqq_update_budg_for_activation, /Users/rubber/linux/block/bfq-iosched.c: 3606
 * are enough to preserve a correct bandwidth distribution in /Users/rubber/linux/block/bfq-iosched.c: 3607
 * the mid term, even without idling. In fact, even if not /Users/rubber/linux/block/bfq-iosched.c: 3608
 * idling allows the internal queues of the device to contain /Users/rubber/linux/block/bfq-iosched.c: 3609
 * many requests, and thus to reorder requests, we can rather /Users/rubber/linux/block/bfq-iosched.c: 3610
 * safely assume that the internal scheduler still preserves a /Users/rubber/linux/block/bfq-iosched.c: 3611
 * minimum of mid-term fairness. /Users/rubber/linux/block/bfq-iosched.c: 3612
 * More precisely, this preemption-based, idleless approach /Users/rubber/linux/block/bfq-iosched.c: 3614
 * provides fairness in terms of IOPS, and not sectors per /Users/rubber/linux/block/bfq-iosched.c: 3615
 * second. This can be seen with a simple example. Suppose /Users/rubber/linux/block/bfq-iosched.c: 3616
 * that there are two queues with the same weight, but that /Users/rubber/linux/block/bfq-iosched.c: 3617
 * the first queue receives requests of 8 sectors, while the /Users/rubber/linux/block/bfq-iosched.c: 3618
 * second queue receives requests of 1024 sectors. In /Users/rubber/linux/block/bfq-iosched.c: 3619
 * addition, suppose that each of the two queues contains at /Users/rubber/linux/block/bfq-iosched.c: 3620
 * most one request at a time, which implies that each queue /Users/rubber/linux/block/bfq-iosched.c: 3621
 * always remains idle after it is served. Finally, after /Users/rubber/linux/block/bfq-iosched.c: 3622
 * remaining idle, each queue receives very quickly a new /Users/rubber/linux/block/bfq-iosched.c: 3623
 * request. It follows that the two queues are served /Users/rubber/linux/block/bfq-iosched.c: 3624
 * alternatively, preempting each other if needed. This /Users/rubber/linux/block/bfq-iosched.c: 3625
 * implies that, although both queues have the same weight, /Users/rubber/linux/block/bfq-iosched.c: 3626
 * the queue with large requests receives a service that is /Users/rubber/linux/block/bfq-iosched.c: 3627
 * 1024/8 times as high as the service received by the other /Users/rubber/linux/block/bfq-iosched.c: 3628
 * queue. /Users/rubber/linux/block/bfq-iosched.c: 3629
 * The motivation for using preemption instead of idling (for /Users/rubber/linux/block/bfq-iosched.c: 3631
 * queues with the same weight) is that, by not idling, /Users/rubber/linux/block/bfq-iosched.c: 3632
 * service guarantees are preserved (completely or at least in /Users/rubber/linux/block/bfq-iosched.c: 3633
 * part) without minimally sacrificing throughput. And, if /Users/rubber/linux/block/bfq-iosched.c: 3634
 * there is no active group, then the primary expectation for /Users/rubber/linux/block/bfq-iosched.c: 3635
 * this device is probably a high throughput. /Users/rubber/linux/block/bfq-iosched.c: 3636
 * We are now left only with explaining the two sub-conditions in the /Users/rubber/linux/block/bfq-iosched.c: 3638
 * additional compound condition that is checked below for deciding /Users/rubber/linux/block/bfq-iosched.c: 3639
 * whether the scenario is asymmetric. To explain the first /Users/rubber/linux/block/bfq-iosched.c: 3640
 * sub-condition, we need to add that the function /Users/rubber/linux/block/bfq-iosched.c: 3641
 * bfq_asymmetric_scenario checks the weights of only /Users/rubber/linux/block/bfq-iosched.c: 3642
 * non-weight-raised queues, for efficiency reasons (see comments on /Users/rubber/linux/block/bfq-iosched.c: 3643
 * bfq_weights_tree_add()). Then the fact that bfqq is weight-raised /Users/rubber/linux/block/bfq-iosched.c: 3644
 * is checked explicitly here. More precisely, the compound condition /Users/rubber/linux/block/bfq-iosched.c: 3645
 * below takes into account also the fact that, even if bfqq is being /Users/rubber/linux/block/bfq-iosched.c: 3646
 * weight-raised, the scenario is still symmetric if all queues with /Users/rubber/linux/block/bfq-iosched.c: 3647
 * requests waiting for completion happen to be /Users/rubber/linux/block/bfq-iosched.c: 3648
 * weight-raised. Actually, we should be even more precise here, and /Users/rubber/linux/block/bfq-iosched.c: 3649
 * differentiate between interactive weight raising and soft real-time /Users/rubber/linux/block/bfq-iosched.c: 3650
 * weight raising. /Users/rubber/linux/block/bfq-iosched.c: 3651
 * The second sub-condition checked in the compound condition is /Users/rubber/linux/block/bfq-iosched.c: 3653
 * whether there is a fair amount of already in-flight I/O not /Users/rubber/linux/block/bfq-iosched.c: 3654
 * belonging to bfqq. If so, I/O dispatching is to be plugged, for the /Users/rubber/linux/block/bfq-iosched.c: 3655
 * following reason. The drive may decide to serve in-flight /Users/rubber/linux/block/bfq-iosched.c: 3656
 * non-bfqq's I/O requests before bfqq's ones, thereby delaying the /Users/rubber/linux/block/bfq-iosched.c: 3657
 * arrival of new I/O requests for bfqq (recall that bfqq is sync). If /Users/rubber/linux/block/bfq-iosched.c: 3658
 * I/O-dispatching is not plugged, then, while bfqq remains empty, a /Users/rubber/linux/block/bfq-iosched.c: 3659
 * basically uncontrolled amount of I/O from other queues may be /Users/rubber/linux/block/bfq-iosched.c: 3660
 * dispatched too, possibly causing the service of bfqq's I/O to be /Users/rubber/linux/block/bfq-iosched.c: 3661
 * delayed even longer in the drive. This problem gets more and more /Users/rubber/linux/block/bfq-iosched.c: 3662
 * serious as the speed and the queue depth of the drive grow, /Users/rubber/linux/block/bfq-iosched.c: 3663
 * because, as these two quantities grow, the probability to find no /Users/rubber/linux/block/bfq-iosched.c: 3664
 * queue busy but many requests in flight grows too. By contrast, /Users/rubber/linux/block/bfq-iosched.c: 3665
 * plugging I/O dispatching minimizes the delay induced by already /Users/rubber/linux/block/bfq-iosched.c: 3666
 * in-flight I/O, and enables bfqq to recover the bandwidth it may /Users/rubber/linux/block/bfq-iosched.c: 3667
 * lose because of this delay. /Users/rubber/linux/block/bfq-iosched.c: 3668
 * As a side note, it is worth considering that the above /Users/rubber/linux/block/bfq-iosched.c: 3670
 * device-idling countermeasures may however fail in the following /Users/rubber/linux/block/bfq-iosched.c: 3671
 * unlucky scenario: if I/O-dispatch plugging is (correctly) disabled /Users/rubber/linux/block/bfq-iosched.c: 3672
 * in a time period during which all symmetry sub-conditions hold, and /Users/rubber/linux/block/bfq-iosched.c: 3673
 * therefore the device is allowed to enqueue many requests, but at /Users/rubber/linux/block/bfq-iosched.c: 3674
 * some later point in time some sub-condition stops to hold, then it /Users/rubber/linux/block/bfq-iosched.c: 3675
 * may become impossible to make requests be served in the desired /Users/rubber/linux/block/bfq-iosched.c: 3676
 * order until all the requests already queued in the device have been /Users/rubber/linux/block/bfq-iosched.c: 3677
 * served. The last sub-condition commented above somewhat mitigates /Users/rubber/linux/block/bfq-iosched.c: 3678
 * this problem for weight-raised queues. /Users/rubber/linux/block/bfq-iosched.c: 3679
 * However, as an additional mitigation for this problem, we preserve /Users/rubber/linux/block/bfq-iosched.c: 3681
 * plugging for a special symmetric case that may suddenly turn into /Users/rubber/linux/block/bfq-iosched.c: 3682
 * asymmetric: the case where only bfqq is busy. In this case, not /Users/rubber/linux/block/bfq-iosched.c: 3683
 * expiring bfqq does not cause any harm to any other queues in terms /Users/rubber/linux/block/bfq-iosched.c: 3684
 * of service guarantees. In contrast, it avoids the following unlucky /Users/rubber/linux/block/bfq-iosched.c: 3685
 * sequence of events: (1) bfqq is expired, (2) a new queue with a /Users/rubber/linux/block/bfq-iosched.c: 3686
 * lower weight than bfqq becomes busy (or more queues), (3) the new /Users/rubber/linux/block/bfq-iosched.c: 3687
 * queue is served until a new request arrives for bfqq, (4) when bfqq /Users/rubber/linux/block/bfq-iosched.c: 3688
 * is finally served, there are so many requests of the new queue in /Users/rubber/linux/block/bfq-iosched.c: 3689
 * the drive that the pending requests for bfqq take a lot of time to /Users/rubber/linux/block/bfq-iosched.c: 3690
 * be served. In particular, event (2) may case even already /Users/rubber/linux/block/bfq-iosched.c: 3691
 * dispatched requests of bfqq to be delayed, inside the drive. So, to /Users/rubber/linux/block/bfq-iosched.c: 3692
 * avoid this series of events, the scenario is preventively declared /Users/rubber/linux/block/bfq-iosched.c: 3693
 * as asymmetric also if bfqq is the only busy queues /Users/rubber/linux/block/bfq-iosched.c: 3694
	/* /Users/rubber/linux/block/bfq-iosched.c: 3717
	 * If this bfqq is shared between multiple processes, check /Users/rubber/linux/block/bfq-iosched.c: 3718
	 * to make sure that those processes are still issuing I/Os /Users/rubber/linux/block/bfq-iosched.c: 3719
	 * within the mean seek distance. If not, it may be time to /Users/rubber/linux/block/bfq-iosched.c: 3720
	 * break the queues apart again. /Users/rubber/linux/block/bfq-iosched.c: 3721
	/* /Users/rubber/linux/block/bfq-iosched.c: 3726
	 * Consider queues with a higher finish virtual time than /Users/rubber/linux/block/bfq-iosched.c: 3727
	 * bfqq. If idling_needed_for_service_guarantees(bfqq) returns /Users/rubber/linux/block/bfq-iosched.c: 3728
	 * true, then bfqq's bandwidth would be violated if an /Users/rubber/linux/block/bfq-iosched.c: 3729
	 * uncontrolled amount of I/O from these queues were /Users/rubber/linux/block/bfq-iosched.c: 3730
	 * dispatched while bfqq is waiting for its new I/O to /Users/rubber/linux/block/bfq-iosched.c: 3731
	 * arrive. This is exactly what may happen if this is a forced /Users/rubber/linux/block/bfq-iosched.c: 3732
	 * expiration caused by a preemption attempt, and if bfqq is /Users/rubber/linux/block/bfq-iosched.c: 3733
	 * not re-scheduled. To prevent this from happening, re-queue /Users/rubber/linux/block/bfq-iosched.c: 3734
	 * bfqq if it needs I/O-dispatch plugging, even if it is /Users/rubber/linux/block/bfq-iosched.c: 3735
	 * empty. By doing so, bfqq is granted to be served before the /Users/rubber/linux/block/bfq-iosched.c: 3736
	 * above queues (provided that bfqq is of course eligible). /Users/rubber/linux/block/bfq-iosched.c: 3737
			/* /Users/rubber/linux/block/bfq-iosched.c: 3743
			 * Overloading budget_timeout field to store /Users/rubber/linux/block/bfq-iosched.c: 3744
			 * the time at which the queue remains with no /Users/rubber/linux/block/bfq-iosched.c: 3745
			 * backlog and no outstanding request; used by /Users/rubber/linux/block/bfq-iosched.c: 3746
			 * the weight-raising mechanism. /Users/rubber/linux/block/bfq-iosched.c: 3747
		/* /Users/rubber/linux/block/bfq-iosched.c: 3754
		 * Resort priority tree of potential close cooperators. /Users/rubber/linux/block/bfq-iosched.c: 3755
		 * See comments on bfq_pos_tree_add_move() for the unlikely(). /Users/rubber/linux/block/bfq-iosched.c: 3756
	/* /Users/rubber/linux/block/bfq-iosched.c: 3763
	 * All in-service entities must have been properly deactivated /Users/rubber/linux/block/bfq-iosched.c: 3764
	 * or requeued before executing the next function, which /Users/rubber/linux/block/bfq-iosched.c: 3765
	 * resets all in-service entities as no more in service. This /Users/rubber/linux/block/bfq-iosched.c: 3766
	 * may cause bfqq to be freed. If this happens, the next /Users/rubber/linux/block/bfq-iosched.c: 3767
	 * function returns true. /Users/rubber/linux/block/bfq-iosched.c: 3768
 * __bfq_bfqq_recalc_budget - try to adapt the budget to the @bfqq behavior. /Users/rubber/linux/block/bfq-iosched.c: 3774
 * @bfqd: device data. /Users/rubber/linux/block/bfq-iosched.c: 3775
 * @bfqq: queue to update. /Users/rubber/linux/block/bfq-iosched.c: 3776
 * @reason: reason for expiration. /Users/rubber/linux/block/bfq-iosched.c: 3777
 * Handle the feedback on @bfqq budget at queue expiration. /Users/rubber/linux/block/bfq-iosched.c: 3779
 * See the body for detailed comments. /Users/rubber/linux/block/bfq-iosched.c: 3780
	else /* /Users/rubber/linux/block/bfq-iosched.c: 3793
	      * Use a constant, low budget for weight-raised queues, /Users/rubber/linux/block/bfq-iosched.c: 3794
	      * to help achieve a low latency. Keep it slightly higher /Users/rubber/linux/block/bfq-iosched.c: 3795
	      * than the minimum possible budget, to cause a little /Users/rubber/linux/block/bfq-iosched.c: 3796
	      * bit fewer expirations. /Users/rubber/linux/block/bfq-iosched.c: 3797
		/* /Users/rubber/linux/block/bfq-iosched.c: 3810
		 * Caveat: in all the following cases we trade latency /Users/rubber/linux/block/bfq-iosched.c: 3811
		 * for throughput. /Users/rubber/linux/block/bfq-iosched.c: 3812
			/* /Users/rubber/linux/block/bfq-iosched.c: 3815
			 * This is the only case where we may reduce /Users/rubber/linux/block/bfq-iosched.c: 3816
			 * the budget: if there is no request of the /Users/rubber/linux/block/bfq-iosched.c: 3817
			 * process still waiting for completion, then /Users/rubber/linux/block/bfq-iosched.c: 3818
			 * we assume (tentatively) that the timer has /Users/rubber/linux/block/bfq-iosched.c: 3819
			 * expired because the batch of requests of /Users/rubber/linux/block/bfq-iosched.c: 3820
			 * the process could have been served with a /Users/rubber/linux/block/bfq-iosched.c: 3821
			 * smaller budget.  Hence, betting that /Users/rubber/linux/block/bfq-iosched.c: 3822
			 * process will behave in the same way when it /Users/rubber/linux/block/bfq-iosched.c: 3823
			 * becomes backlogged again, we reduce its /Users/rubber/linux/block/bfq-iosched.c: 3824
			 * next budget.  As long as we guess right, /Users/rubber/linux/block/bfq-iosched.c: 3825
			 * this budget cut reduces the latency /Users/rubber/linux/block/bfq-iosched.c: 3826
			 * experienced by the process. /Users/rubber/linux/block/bfq-iosched.c: 3827
			 * /Users/rubber/linux/block/bfq-iosched.c: 3828
			 * However, if there are still outstanding /Users/rubber/linux/block/bfq-iosched.c: 3829
			 * requests, then the process may have not yet /Users/rubber/linux/block/bfq-iosched.c: 3830
			 * issued its next request just because it is /Users/rubber/linux/block/bfq-iosched.c: 3831
			 * still waiting for the completion of some of /Users/rubber/linux/block/bfq-iosched.c: 3832
			 * the still outstanding ones.  So in this /Users/rubber/linux/block/bfq-iosched.c: 3833
			 * subcase we do not reduce its budget, on the /Users/rubber/linux/block/bfq-iosched.c: 3834
			 * contrary we increase it to possibly boost /Users/rubber/linux/block/bfq-iosched.c: 3835
			 * the throughput, as discussed in the /Users/rubber/linux/block/bfq-iosched.c: 3836
			 * comments to the BUDGET_TIMEOUT case. /Users/rubber/linux/block/bfq-iosched.c: 3837
			/* /Users/rubber/linux/block/bfq-iosched.c: 3849
			 * We double the budget here because it gives /Users/rubber/linux/block/bfq-iosched.c: 3850
			 * the chance to boost the throughput if this /Users/rubber/linux/block/bfq-iosched.c: 3851
			 * is not a seeky process (and has bumped into /Users/rubber/linux/block/bfq-iosched.c: 3852
			 * this timeout because of, e.g., ZBR). /Users/rubber/linux/block/bfq-iosched.c: 3853
			/* /Users/rubber/linux/block/bfq-iosched.c: 3858
			 * The process still has backlog, and did not /Users/rubber/linux/block/bfq-iosched.c: 3859
			 * let either the budget timeout or the disk /Users/rubber/linux/block/bfq-iosched.c: 3860
			 * idling timeout expire. Hence it is not /Users/rubber/linux/block/bfq-iosched.c: 3861
			 * seeky, has a short thinktime and may be /Users/rubber/linux/block/bfq-iosched.c: 3862
			 * happy with a higher budget too. So /Users/rubber/linux/block/bfq-iosched.c: 3863
			 * definitely increase the budget of this good /Users/rubber/linux/block/bfq-iosched.c: 3864
			 * candidate to boost the disk throughput. /Users/rubber/linux/block/bfq-iosched.c: 3865
			/* /Users/rubber/linux/block/bfq-iosched.c: 3870
			 * For queues that expire for this reason, it /Users/rubber/linux/block/bfq-iosched.c: 3871
			 * is particularly important to keep the /Users/rubber/linux/block/bfq-iosched.c: 3872
			 * budget close to the actual service they /Users/rubber/linux/block/bfq-iosched.c: 3873
			 * need. Doing so reduces the timestamp /Users/rubber/linux/block/bfq-iosched.c: 3874
			 * misalignment problem described in the /Users/rubber/linux/block/bfq-iosched.c: 3875
			 * comments in the body of /Users/rubber/linux/block/bfq-iosched.c: 3876
			 * __bfq_activate_entity. In fact, suppose /Users/rubber/linux/block/bfq-iosched.c: 3877
			 * that a queue systematically expires for /Users/rubber/linux/block/bfq-iosched.c: 3878
			 * BFQQE_NO_MORE_REQUESTS and presents a /Users/rubber/linux/block/bfq-iosched.c: 3879
			 * new request in time to enjoy timestamp /Users/rubber/linux/block/bfq-iosched.c: 3880
			 * back-shifting. The larger the budget of the /Users/rubber/linux/block/bfq-iosched.c: 3881
			 * queue is with respect to the service the /Users/rubber/linux/block/bfq-iosched.c: 3882
			 * queue actually requests in each service /Users/rubber/linux/block/bfq-iosched.c: 3883
			 * slot, the more times the queue can be /Users/rubber/linux/block/bfq-iosched.c: 3884
			 * reactivated with the same virtual finish /Users/rubber/linux/block/bfq-iosched.c: 3885
			 * time. It follows that, even if this finish /Users/rubber/linux/block/bfq-iosched.c: 3886
			 * time is pushed to the system virtual time /Users/rubber/linux/block/bfq-iosched.c: 3887
			 * to reduce the consequent timestamp /Users/rubber/linux/block/bfq-iosched.c: 3888
			 * misalignment, the queue unjustly enjoys for /Users/rubber/linux/block/bfq-iosched.c: 3889
			 * many re-activations a lower finish time /Users/rubber/linux/block/bfq-iosched.c: 3890
			 * than all newly activated queues. /Users/rubber/linux/block/bfq-iosched.c: 3891
			 * /Users/rubber/linux/block/bfq-iosched.c: 3892
			 * The service needed by bfqq is measured /Users/rubber/linux/block/bfq-iosched.c: 3893
			 * quite precisely by bfqq->entity.service. /Users/rubber/linux/block/bfq-iosched.c: 3894
			 * Since bfqq does not enjoy device idling, /Users/rubber/linux/block/bfq-iosched.c: 3895
			 * bfqq->entity.service is equal to the number /Users/rubber/linux/block/bfq-iosched.c: 3896
			 * of sectors that the process associated with /Users/rubber/linux/block/bfq-iosched.c: 3897
			 * bfqq requested to read/write before waiting /Users/rubber/linux/block/bfq-iosched.c: 3898
			 * for request completions, or blocking for /Users/rubber/linux/block/bfq-iosched.c: 3899
			 * other reasons. /Users/rubber/linux/block/bfq-iosched.c: 3900
		/* /Users/rubber/linux/block/bfq-iosched.c: 3908
		 * Async queues get always the maximum possible /Users/rubber/linux/block/bfq-iosched.c: 3909
		 * budget, as for them we do not care about latency /Users/rubber/linux/block/bfq-iosched.c: 3910
		 * (in addition, their ability to dispatch is limited /Users/rubber/linux/block/bfq-iosched.c: 3911
		 * by the charging factor). /Users/rubber/linux/block/bfq-iosched.c: 3912
	/* /Users/rubber/linux/block/bfq-iosched.c: 3923
	 * If there is still backlog, then assign a new budget, making /Users/rubber/linux/block/bfq-iosched.c: 3924
	 * sure that it is large enough for the next request.  Since /Users/rubber/linux/block/bfq-iosched.c: 3925
	 * the finish time of bfqq must be kept in sync with the /Users/rubber/linux/block/bfq-iosched.c: 3926
	 * budget, be sure to call __bfq_bfqq_expire() *after* this /Users/rubber/linux/block/bfq-iosched.c: 3927
	 * update. /Users/rubber/linux/block/bfq-iosched.c: 3928
	 * /Users/rubber/linux/block/bfq-iosched.c: 3929
	 * If there is no backlog, then no need to update the budget; /Users/rubber/linux/block/bfq-iosched.c: 3930
	 * it will be updated on the arrival of a new request. /Users/rubber/linux/block/bfq-iosched.c: 3931
 * Return true if the process associated with bfqq is "slow". The slow /Users/rubber/linux/block/bfq-iosched.c: 3944
 * flag is used, in addition to the budget timeout, to reduce the /Users/rubber/linux/block/bfq-iosched.c: 3945
 * amount of service provided to seeky processes, and thus reduce /Users/rubber/linux/block/bfq-iosched.c: 3946
 * their chances to lower the throughput. More details in the comments /Users/rubber/linux/block/bfq-iosched.c: 3947
 * on the function bfq_bfqq_expire(). /Users/rubber/linux/block/bfq-iosched.c: 3948
 * An important observation is in order: as discussed in the comments /Users/rubber/linux/block/bfq-iosched.c: 3950
 * on the function bfq_update_peak_rate(), with devices with internal /Users/rubber/linux/block/bfq-iosched.c: 3951
 * queues, it is hard if ever possible to know when and for how long /Users/rubber/linux/block/bfq-iosched.c: 3952
 * an I/O request is processed by the device (apart from the trivial /Users/rubber/linux/block/bfq-iosched.c: 3953
 * I/O pattern where a new request is dispatched only after the /Users/rubber/linux/block/bfq-iosched.c: 3954
 * previous one has been completed). This makes it hard to evaluate /Users/rubber/linux/block/bfq-iosched.c: 3955
 * the real rate at which the I/O requests of each bfq_queue are /Users/rubber/linux/block/bfq-iosched.c: 3956
 * served.  In fact, for an I/O scheduler like BFQ, serving a /Users/rubber/linux/block/bfq-iosched.c: 3957
 * bfq_queue means just dispatching its requests during its service /Users/rubber/linux/block/bfq-iosched.c: 3958
 * slot (i.e., until the budget of the queue is exhausted, or the /Users/rubber/linux/block/bfq-iosched.c: 3959
 * queue remains idle, or, finally, a timeout fires). But, during the /Users/rubber/linux/block/bfq-iosched.c: 3960
 * service slot of a bfq_queue, around 100 ms at most, the device may /Users/rubber/linux/block/bfq-iosched.c: 3961
 * be even still processing requests of bfq_queues served in previous /Users/rubber/linux/block/bfq-iosched.c: 3962
 * service slots. On the opposite end, the requests of the in-service /Users/rubber/linux/block/bfq-iosched.c: 3963
 * bfq_queue may be completed after the service slot of the queue /Users/rubber/linux/block/bfq-iosched.c: 3964
 * finishes. /Users/rubber/linux/block/bfq-iosched.c: 3965
 * Anyway, unless more sophisticated solutions are used /Users/rubber/linux/block/bfq-iosched.c: 3967
 * (where possible), the sum of the sizes of the requests dispatched /Users/rubber/linux/block/bfq-iosched.c: 3968
 * during the service slot of a bfq_queue is probably the only /Users/rubber/linux/block/bfq-iosched.c: 3969
 * approximation available for the service received by the bfq_queue /Users/rubber/linux/block/bfq-iosched.c: 3970
 * during its service slot. And this sum is the quantity used in this /Users/rubber/linux/block/bfq-iosched.c: 3971
 * function to evaluate the I/O speed of a process. /Users/rubber/linux/block/bfq-iosched.c: 3972
			 /* /Users/rubber/linux/block/bfq-iosched.c: 3995
			  * give same worst-case guarantees as idling /Users/rubber/linux/block/bfq-iosched.c: 3996
			  * for seeky /Users/rubber/linux/block/bfq-iosched.c: 3997
	/* /Users/rubber/linux/block/bfq-iosched.c: 4008
	 * Use only long (> 20ms) intervals to filter out excessive /Users/rubber/linux/block/bfq-iosched.c: 4009
	 * spikes in service rate estimation. /Users/rubber/linux/block/bfq-iosched.c: 4010
		/* /Users/rubber/linux/block/bfq-iosched.c: 4013
		 * Caveat for rotational devices: processes doing I/O /Users/rubber/linux/block/bfq-iosched.c: 4014
		 * in the slower disk zones tend to be slow(er) even /Users/rubber/linux/block/bfq-iosched.c: 4015
		 * if not seeky. In this respect, the estimated peak /Users/rubber/linux/block/bfq-iosched.c: 4016
		 * rate is likely to be an average over the disk /Users/rubber/linux/block/bfq-iosched.c: 4017
		 * surface. Accordingly, to not be too harsh with /Users/rubber/linux/block/bfq-iosched.c: 4018
		 * unlucky processes, a process is deemed slow only if /Users/rubber/linux/block/bfq-iosched.c: 4019
		 * its rate has been lower than half of the estimated /Users/rubber/linux/block/bfq-iosched.c: 4020
		 * peak rate. /Users/rubber/linux/block/bfq-iosched.c: 4021
 * To be deemed as soft real-time, an application must meet two /Users/rubber/linux/block/bfq-iosched.c: 4032
 * requirements. First, the application must not require an average /Users/rubber/linux/block/bfq-iosched.c: 4033
 * bandwidth higher than the approximate bandwidth required to playback or /Users/rubber/linux/block/bfq-iosched.c: 4034
 * record a compressed high-definition video. /Users/rubber/linux/block/bfq-iosched.c: 4035
 * The next function is invoked on the completion of the last request of a /Users/rubber/linux/block/bfq-iosched.c: 4036
 * batch, to compute the next-start time instant, soft_rt_next_start, such /Users/rubber/linux/block/bfq-iosched.c: 4037
 * that, if the next request of the application does not arrive before /Users/rubber/linux/block/bfq-iosched.c: 4038
 * soft_rt_next_start, then the above requirement on the bandwidth is met. /Users/rubber/linux/block/bfq-iosched.c: 4039
 * The second requirement is that the request pattern of the application is /Users/rubber/linux/block/bfq-iosched.c: 4041
 * isochronous, i.e., that, after issuing a request or a batch of requests, /Users/rubber/linux/block/bfq-iosched.c: 4042
 * the application stops issuing new requests until all its pending requests /Users/rubber/linux/block/bfq-iosched.c: 4043
 * have been completed. After that, the application may issue a new batch, /Users/rubber/linux/block/bfq-iosched.c: 4044
 * and so on. /Users/rubber/linux/block/bfq-iosched.c: 4045
 * For this reason the next function is invoked to compute /Users/rubber/linux/block/bfq-iosched.c: 4046
 * soft_rt_next_start only for applications that meet this requirement, /Users/rubber/linux/block/bfq-iosched.c: 4047
 * whereas soft_rt_next_start is set to infinity for applications that do /Users/rubber/linux/block/bfq-iosched.c: 4048
 * not. /Users/rubber/linux/block/bfq-iosched.c: 4049
 * Unfortunately, even a greedy (i.e., I/O-bound) application may /Users/rubber/linux/block/bfq-iosched.c: 4051
 * happen to meet, occasionally or systematically, both the above /Users/rubber/linux/block/bfq-iosched.c: 4052
 * bandwidth and isochrony requirements. This may happen at least in /Users/rubber/linux/block/bfq-iosched.c: 4053
 * the following circumstances. First, if the CPU load is high. The /Users/rubber/linux/block/bfq-iosched.c: 4054
 * application may stop issuing requests while the CPUs are busy /Users/rubber/linux/block/bfq-iosched.c: 4055
 * serving other processes, then restart, then stop again for a while, /Users/rubber/linux/block/bfq-iosched.c: 4056
 * and so on. The other circumstances are related to the storage /Users/rubber/linux/block/bfq-iosched.c: 4057
 * device: the storage device is highly loaded or reaches a low-enough /Users/rubber/linux/block/bfq-iosched.c: 4058
 * throughput with the I/O of the application (e.g., because the I/O /Users/rubber/linux/block/bfq-iosched.c: 4059
 * is random and/or the device is slow). In all these cases, the /Users/rubber/linux/block/bfq-iosched.c: 4060
 * I/O of the application may be simply slowed down enough to meet /Users/rubber/linux/block/bfq-iosched.c: 4061
 * the bandwidth and isochrony requirements. To reduce the probability /Users/rubber/linux/block/bfq-iosched.c: 4062
 * that greedy applications are deemed as soft real-time in these /Users/rubber/linux/block/bfq-iosched.c: 4063
 * corner cases, a further rule is used in the computation of /Users/rubber/linux/block/bfq-iosched.c: 4064
 * soft_rt_next_start: the return value of this function is forced to /Users/rubber/linux/block/bfq-iosched.c: 4065
 * be higher than the maximum between the following two quantities. /Users/rubber/linux/block/bfq-iosched.c: 4066
 * (a) Current time plus: (1) the maximum time for which the arrival /Users/rubber/linux/block/bfq-iosched.c: 4068
 *     of a request is waited for when a sync queue becomes idle, /Users/rubber/linux/block/bfq-iosched.c: 4069
 *     namely bfqd->bfq_slice_idle, and (2) a few extra jiffies. We /Users/rubber/linux/block/bfq-iosched.c: 4070
 *     postpone for a moment the reason for adding a few extra /Users/rubber/linux/block/bfq-iosched.c: 4071
 *     jiffies; we get back to it after next item (b).  Lower-bounding /Users/rubber/linux/block/bfq-iosched.c: 4072
 *     the return value of this function with the current time plus /Users/rubber/linux/block/bfq-iosched.c: 4073
 *     bfqd->bfq_slice_idle tends to filter out greedy applications, /Users/rubber/linux/block/bfq-iosched.c: 4074
 *     because the latter issue their next request as soon as possible /Users/rubber/linux/block/bfq-iosched.c: 4075
 *     after the last one has been completed. In contrast, a soft /Users/rubber/linux/block/bfq-iosched.c: 4076
 *     real-time application spends some time processing data, after a /Users/rubber/linux/block/bfq-iosched.c: 4077
 *     batch of its requests has been completed. /Users/rubber/linux/block/bfq-iosched.c: 4078
 * (b) Current value of bfqq->soft_rt_next_start. As pointed out /Users/rubber/linux/block/bfq-iosched.c: 4080
 *     above, greedy applications may happen to meet both the /Users/rubber/linux/block/bfq-iosched.c: 4081
 *     bandwidth and isochrony requirements under heavy CPU or /Users/rubber/linux/block/bfq-iosched.c: 4082
 *     storage-device load. In more detail, in these scenarios, these /Users/rubber/linux/block/bfq-iosched.c: 4083
 *     applications happen, only for limited time periods, to do I/O /Users/rubber/linux/block/bfq-iosched.c: 4084
 *     slowly enough to meet all the requirements described so far, /Users/rubber/linux/block/bfq-iosched.c: 4085
 *     including the filtering in above item (a). These slow-speed /Users/rubber/linux/block/bfq-iosched.c: 4086
 *     time intervals are usually interspersed between other time /Users/rubber/linux/block/bfq-iosched.c: 4087
 *     intervals during which these applications do I/O at a very high /Users/rubber/linux/block/bfq-iosched.c: 4088
 *     speed. Fortunately, exactly because of the high speed of the /Users/rubber/linux/block/bfq-iosched.c: 4089
 *     I/O in the high-speed intervals, the values returned by this /Users/rubber/linux/block/bfq-iosched.c: 4090
 *     function happen to be so high, near the end of any such /Users/rubber/linux/block/bfq-iosched.c: 4091
 *     high-speed interval, to be likely to fall *after* the end of /Users/rubber/linux/block/bfq-iosched.c: 4092
 *     the low-speed time interval that follows. These high values are /Users/rubber/linux/block/bfq-iosched.c: 4093
 *     stored in bfqq->soft_rt_next_start after each invocation of /Users/rubber/linux/block/bfq-iosched.c: 4094
 *     this function. As a consequence, if the last value of /Users/rubber/linux/block/bfq-iosched.c: 4095
 *     bfqq->soft_rt_next_start is constantly used to lower-bound the /Users/rubber/linux/block/bfq-iosched.c: 4096
 *     next value that this function may return, then, from the very /Users/rubber/linux/block/bfq-iosched.c: 4097
 *     beginning of a low-speed interval, bfqq->soft_rt_next_start is /Users/rubber/linux/block/bfq-iosched.c: 4098
 *     likely to be constantly kept so high that any I/O request /Users/rubber/linux/block/bfq-iosched.c: 4099
 *     issued during the low-speed interval is considered as arriving /Users/rubber/linux/block/bfq-iosched.c: 4100
 *     to soon for the application to be deemed as soft /Users/rubber/linux/block/bfq-iosched.c: 4101
 *     real-time. Then, in the high-speed interval that follows, the /Users/rubber/linux/block/bfq-iosched.c: 4102
 *     application will not be deemed as soft real-time, just because /Users/rubber/linux/block/bfq-iosched.c: 4103
 *     it will do I/O at a high speed. And so on. /Users/rubber/linux/block/bfq-iosched.c: 4104
 * Getting back to the filtering in item (a), in the following two /Users/rubber/linux/block/bfq-iosched.c: 4106
 * cases this filtering might be easily passed by a greedy /Users/rubber/linux/block/bfq-iosched.c: 4107
 * application, if the reference quantity was just /Users/rubber/linux/block/bfq-iosched.c: 4108
 * bfqd->bfq_slice_idle: /Users/rubber/linux/block/bfq-iosched.c: 4109
 * 1) HZ is so low that the duration of a jiffy is comparable to or /Users/rubber/linux/block/bfq-iosched.c: 4110
 *    higher than bfqd->bfq_slice_idle. This happens, e.g., on slow /Users/rubber/linux/block/bfq-iosched.c: 4111
 *    devices with HZ=100. The time granularity may be so coarse /Users/rubber/linux/block/bfq-iosched.c: 4112
 *    that the approximation, in jiffies, of bfqd->bfq_slice_idle /Users/rubber/linux/block/bfq-iosched.c: 4113
 *    is rather lower than the exact value. /Users/rubber/linux/block/bfq-iosched.c: 4114
 * 2) jiffies, instead of increasing at a constant rate, may stop increasing /Users/rubber/linux/block/bfq-iosched.c: 4115
 *    for a while, then suddenly 'jump' by several units to recover the lost /Users/rubber/linux/block/bfq-iosched.c: 4116
 *    increments. This seems to happen, e.g., inside virtual machines. /Users/rubber/linux/block/bfq-iosched.c: 4117
 * To address this issue, in the filtering in (a) we do not use as a /Users/rubber/linux/block/bfq-iosched.c: 4118
 * reference time interval just bfqd->bfq_slice_idle, but /Users/rubber/linux/block/bfq-iosched.c: 4119
 * bfqd->bfq_slice_idle plus a few jiffies. In particular, we add the /Users/rubber/linux/block/bfq-iosched.c: 4120
 * minimum number of jiffies for which the filter seems to be quite /Users/rubber/linux/block/bfq-iosched.c: 4121
 * precise also in embedded systems and KVM/QEMU virtual machines. /Users/rubber/linux/block/bfq-iosched.c: 4122
 * bfq_bfqq_expire - expire a queue. /Users/rubber/linux/block/bfq-iosched.c: 4135
 * @bfqd: device owning the queue. /Users/rubber/linux/block/bfq-iosched.c: 4136
 * @bfqq: the queue to expire. /Users/rubber/linux/block/bfq-iosched.c: 4137
 * @compensate: if true, compensate for the time spent idling. /Users/rubber/linux/block/bfq-iosched.c: 4138
 * @reason: the reason causing the expiration. /Users/rubber/linux/block/bfq-iosched.c: 4139
 * If the process associated with bfqq does slow I/O (e.g., because it /Users/rubber/linux/block/bfq-iosched.c: 4141
 * issues random requests), we charge bfqq with the time it has been /Users/rubber/linux/block/bfq-iosched.c: 4142
 * in service instead of the service it has received (see /Users/rubber/linux/block/bfq-iosched.c: 4143
 * bfq_bfqq_charge_time for details on how this goal is achieved). As /Users/rubber/linux/block/bfq-iosched.c: 4144
 * a consequence, bfqq will typically get higher timestamps upon /Users/rubber/linux/block/bfq-iosched.c: 4145
 * reactivation, and hence it will be rescheduled as if it had /Users/rubber/linux/block/bfq-iosched.c: 4146
 * received more service than what it has actually received. In the /Users/rubber/linux/block/bfq-iosched.c: 4147
 * end, bfqq receives less service in proportion to how slowly its /Users/rubber/linux/block/bfq-iosched.c: 4148
 * associated process consumes its budgets (and hence how seriously it /Users/rubber/linux/block/bfq-iosched.c: 4149
 * tends to lower the throughput). In addition, this time-charging /Users/rubber/linux/block/bfq-iosched.c: 4150
 * strategy guarantees time fairness among slow processes. In /Users/rubber/linux/block/bfq-iosched.c: 4151
 * contrast, if the process associated with bfqq is not slow, we /Users/rubber/linux/block/bfq-iosched.c: 4152
 * charge bfqq exactly with the service it has received. /Users/rubber/linux/block/bfq-iosched.c: 4153
 * Charging time to the first type of queues and the exact service to /Users/rubber/linux/block/bfq-iosched.c: 4155
 * the other has the effect of using the WF2Q+ policy to schedule the /Users/rubber/linux/block/bfq-iosched.c: 4156
 * former on a timeslice basis, without violating service domain /Users/rubber/linux/block/bfq-iosched.c: 4157
 * guarantees among the latter. /Users/rubber/linux/block/bfq-iosched.c: 4158
	/* /Users/rubber/linux/block/bfq-iosched.c: 4169
	 * Check whether the process is slow (see bfq_bfqq_is_slow). /Users/rubber/linux/block/bfq-iosched.c: 4170
	/* /Users/rubber/linux/block/bfq-iosched.c: 4174
	 * As above explained, charge slow (typically seeky) and /Users/rubber/linux/block/bfq-iosched.c: 4175
	 * timed-out queues with the time and not the service /Users/rubber/linux/block/bfq-iosched.c: 4176
	 * received, to favor sequential workloads. /Users/rubber/linux/block/bfq-iosched.c: 4177
	 * /Users/rubber/linux/block/bfq-iosched.c: 4178
	 * Processes doing I/O in the slower disk zones will tend to /Users/rubber/linux/block/bfq-iosched.c: 4179
	 * be slow(er) even if not seeky. Therefore, since the /Users/rubber/linux/block/bfq-iosched.c: 4180
	 * estimated peak rate is actually an average over the disk /Users/rubber/linux/block/bfq-iosched.c: 4181
	 * surface, these processes may timeout just for bad luck. To /Users/rubber/linux/block/bfq-iosched.c: 4182
	 * avoid punishing them, do not charge time to processes that /Users/rubber/linux/block/bfq-iosched.c: 4183
	 * succeeded in consuming at least 2/3 of their budget. This /Users/rubber/linux/block/bfq-iosched.c: 4184
	 * allows BFQ to preserve enough elasticity to still perform /Users/rubber/linux/block/bfq-iosched.c: 4185
	 * bandwidth, and not time, distribution with little unlucky /Users/rubber/linux/block/bfq-iosched.c: 4186
	 * or quasi-sequential processes. /Users/rubber/linux/block/bfq-iosched.c: 4187
		/* /Users/rubber/linux/block/bfq-iosched.c: 4200
		 * If we get here, and there are no outstanding /Users/rubber/linux/block/bfq-iosched.c: 4201
		 * requests, then the request pattern is isochronous /Users/rubber/linux/block/bfq-iosched.c: 4202
		 * (see the comments on the function /Users/rubber/linux/block/bfq-iosched.c: 4203
		 * bfq_bfqq_softrt_next_start()). Therefore we can /Users/rubber/linux/block/bfq-iosched.c: 4204
		 * compute soft_rt_next_start. /Users/rubber/linux/block/bfq-iosched.c: 4205
		 * /Users/rubber/linux/block/bfq-iosched.c: 4206
		 * If, instead, the queue still has outstanding /Users/rubber/linux/block/bfq-iosched.c: 4207
		 * requests, then we have to wait for the completion /Users/rubber/linux/block/bfq-iosched.c: 4208
		 * of all the outstanding requests to discover whether /Users/rubber/linux/block/bfq-iosched.c: 4209
		 * the request pattern is actually isochronous. /Users/rubber/linux/block/bfq-iosched.c: 4210
			/* /Users/rubber/linux/block/bfq-iosched.c: 4216
			 * Schedule an update of soft_rt_next_start to when /Users/rubber/linux/block/bfq-iosched.c: 4217
			 * the task may be discovered to be isochronous. /Users/rubber/linux/block/bfq-iosched.c: 4218
	/* /Users/rubber/linux/block/bfq-iosched.c: 4228
	 * bfqq expired, so no total service time needs to be computed /Users/rubber/linux/block/bfq-iosched.c: 4229
	 * any longer: reset state machine for measuring total service /Users/rubber/linux/block/bfq-iosched.c: 4230
	 * times. /Users/rubber/linux/block/bfq-iosched.c: 4231
	/* /Users/rubber/linux/block/bfq-iosched.c: 4236
	 * Increase, decrease or leave budget unchanged according to /Users/rubber/linux/block/bfq-iosched.c: 4237
	 * reason. /Users/rubber/linux/block/bfq-iosched.c: 4238
		/* /Users/rubber/linux/block/bfq-iosched.c: 4250
		 * Not setting service to 0, because, if the next rq /Users/rubber/linux/block/bfq-iosched.c: 4251
		 * arrives in time, the queue will go on receiving /Users/rubber/linux/block/bfq-iosched.c: 4252
		 * service with this same budget (as if it never expired) /Users/rubber/linux/block/bfq-iosched.c: 4253
	/* /Users/rubber/linux/block/bfq-iosched.c: 4258
	 * Reset the received-service counter for every parent entity. /Users/rubber/linux/block/bfq-iosched.c: 4259
	 * Differently from what happens with bfqq->entity.service, /Users/rubber/linux/block/bfq-iosched.c: 4260
	 * the resetting of this counter never needs to be postponed /Users/rubber/linux/block/bfq-iosched.c: 4261
	 * for parent entities. In fact, in case bfqq may have a /Users/rubber/linux/block/bfq-iosched.c: 4262
	 * chance to go on being served using the last, partially /Users/rubber/linux/block/bfq-iosched.c: 4263
	 * consumed budget, bfqq->entity.service needs to be kept, /Users/rubber/linux/block/bfq-iosched.c: 4264
	 * because if bfqq then actually goes on being served using /Users/rubber/linux/block/bfq-iosched.c: 4265
	 * the same budget, the last value of bfqq->entity.service is /Users/rubber/linux/block/bfq-iosched.c: 4266
	 * needed to properly decrement bfqq->entity.budget by the /Users/rubber/linux/block/bfq-iosched.c: 4267
	 * portion already consumed. In contrast, it is not necessary /Users/rubber/linux/block/bfq-iosched.c: 4268
	 * to keep entity->service for parent entities too, because /Users/rubber/linux/block/bfq-iosched.c: 4269
	 * the bubble up of the new value of bfqq->entity.budget will /Users/rubber/linux/block/bfq-iosched.c: 4270
	 * make sure that the budgets of parent entities are correct, /Users/rubber/linux/block/bfq-iosched.c: 4271
	 * even in case bfqq and thus parent entities go on receiving /Users/rubber/linux/block/bfq-iosched.c: 4272
	 * service with the same budget. /Users/rubber/linux/block/bfq-iosched.c: 4273
 * Budget timeout is not implemented through a dedicated timer, but /Users/rubber/linux/block/bfq-iosched.c: 4281
 * just checked on request arrivals and completions, as well as on /Users/rubber/linux/block/bfq-iosched.c: 4282
 * idle timer expirations. /Users/rubber/linux/block/bfq-iosched.c: 4283
 * If we expire a queue that is actively waiting (i.e., with the /Users/rubber/linux/block/bfq-iosched.c: 4291
 * device idled) for the arrival of a new request, then we may incur /Users/rubber/linux/block/bfq-iosched.c: 4292
 * the timestamp misalignment problem described in the body of the /Users/rubber/linux/block/bfq-iosched.c: 4293
 * function __bfq_activate_entity. Hence we return true only if this /Users/rubber/linux/block/bfq-iosched.c: 4294
 * condition does not hold, or if the queue is slow enough to deserve /Users/rubber/linux/block/bfq-iosched.c: 4295
 * only to be kicked off for preserving a high throughput. /Users/rubber/linux/block/bfq-iosched.c: 4296
	/* /Users/rubber/linux/block/bfq-iosched.c: 4327
	 * The next variable takes into account the cases where idling /Users/rubber/linux/block/bfq-iosched.c: 4328
	 * boosts the throughput. /Users/rubber/linux/block/bfq-iosched.c: 4329
	 * /Users/rubber/linux/block/bfq-iosched.c: 4330
	 * The value of the variable is computed considering, first, that /Users/rubber/linux/block/bfq-iosched.c: 4331
	 * idling is virtually always beneficial for the throughput if: /Users/rubber/linux/block/bfq-iosched.c: 4332
	 * (a) the device is not NCQ-capable and rotational, or /Users/rubber/linux/block/bfq-iosched.c: 4333
	 * (b) regardless of the presence of NCQ, the device is rotational and /Users/rubber/linux/block/bfq-iosched.c: 4334
	 *     the request pattern for bfqq is I/O-bound and sequential, or /Users/rubber/linux/block/bfq-iosched.c: 4335
	 * (c) regardless of whether it is rotational, the device is /Users/rubber/linux/block/bfq-iosched.c: 4336
	 *     not NCQ-capable and the request pattern for bfqq is /Users/rubber/linux/block/bfq-iosched.c: 4337
	 *     I/O-bound and sequential. /Users/rubber/linux/block/bfq-iosched.c: 4338
	 * /Users/rubber/linux/block/bfq-iosched.c: 4339
	 * Secondly, and in contrast to the above item (b), idling an /Users/rubber/linux/block/bfq-iosched.c: 4340
	 * NCQ-capable flash-based device would not boost the /Users/rubber/linux/block/bfq-iosched.c: 4341
	 * throughput even with sequential I/O; rather it would lower /Users/rubber/linux/block/bfq-iosched.c: 4342
	 * the throughput in proportion to how fast the device /Users/rubber/linux/block/bfq-iosched.c: 4343
	 * is. Accordingly, the next variable is true if any of the /Users/rubber/linux/block/bfq-iosched.c: 4344
	 * above conditions (a), (b) or (c) is true, and, in /Users/rubber/linux/block/bfq-iosched.c: 4345
	 * particular, happens to be false if bfqd is an NCQ-capable /Users/rubber/linux/block/bfq-iosched.c: 4346
	 * flash-based device. /Users/rubber/linux/block/bfq-iosched.c: 4347
	/* /Users/rubber/linux/block/bfq-iosched.c: 4353
	 * The return value of this function is equal to that of /Users/rubber/linux/block/bfq-iosched.c: 4354
	 * idling_boosts_thr, unless a special case holds. In this /Users/rubber/linux/block/bfq-iosched.c: 4355
	 * special case, described below, idling may cause problems to /Users/rubber/linux/block/bfq-iosched.c: 4356
	 * weight-raised queues. /Users/rubber/linux/block/bfq-iosched.c: 4357
	 * /Users/rubber/linux/block/bfq-iosched.c: 4358
	 * When the request pool is saturated (e.g., in the presence /Users/rubber/linux/block/bfq-iosched.c: 4359
	 * of write hogs), if the processes associated with /Users/rubber/linux/block/bfq-iosched.c: 4360
	 * non-weight-raised queues ask for requests at a lower rate, /Users/rubber/linux/block/bfq-iosched.c: 4361
	 * then processes associated with weight-raised queues have a /Users/rubber/linux/block/bfq-iosched.c: 4362
	 * higher probability to get a request from the pool /Users/rubber/linux/block/bfq-iosched.c: 4363
	 * immediately (or at least soon) when they need one. Thus /Users/rubber/linux/block/bfq-iosched.c: 4364
	 * they have a higher probability to actually get a fraction /Users/rubber/linux/block/bfq-iosched.c: 4365
	 * of the device throughput proportional to their high /Users/rubber/linux/block/bfq-iosched.c: 4366
	 * weight. This is especially true with NCQ-capable drives, /Users/rubber/linux/block/bfq-iosched.c: 4367
	 * which enqueue several requests in advance, and further /Users/rubber/linux/block/bfq-iosched.c: 4368
	 * reorder internally-queued requests. /Users/rubber/linux/block/bfq-iosched.c: 4369
	 * /Users/rubber/linux/block/bfq-iosched.c: 4370
	 * For this reason, we force to false the return value if /Users/rubber/linux/block/bfq-iosched.c: 4371
	 * there are weight-raised busy queues. In this case, and if /Users/rubber/linux/block/bfq-iosched.c: 4372
	 * bfqq is not weight-raised, this guarantees that the device /Users/rubber/linux/block/bfq-iosched.c: 4373
	 * is not idled for bfqq (if, instead, bfqq is weight-raised, /Users/rubber/linux/block/bfq-iosched.c: 4374
	 * then idling will be guaranteed by another variable, see /Users/rubber/linux/block/bfq-iosched.c: 4375
	 * below). Combined with the timestamping rules of BFQ (see /Users/rubber/linux/block/bfq-iosched.c: 4376
	 * [1] for details), this behavior causes bfqq, and hence any /Users/rubber/linux/block/bfq-iosched.c: 4377
	 * sync non-weight-raised queue, to get a lower number of /Users/rubber/linux/block/bfq-iosched.c: 4378
	 * requests served, and thus to ask for a lower number of /Users/rubber/linux/block/bfq-iosched.c: 4379
	 * requests from the request pool, before the busy /Users/rubber/linux/block/bfq-iosched.c: 4380
	 * weight-raised queues get served again. This often mitigates /Users/rubber/linux/block/bfq-iosched.c: 4381
	 * starvation problems in the presence of heavy write /Users/rubber/linux/block/bfq-iosched.c: 4382
	 * workloads and NCQ, thereby guaranteeing a higher /Users/rubber/linux/block/bfq-iosched.c: 4383
	 * application and system responsiveness in these hostile /Users/rubber/linux/block/bfq-iosched.c: 4384
	 * scenarios. /Users/rubber/linux/block/bfq-iosched.c: 4385
 * For a queue that becomes empty, device idling is allowed only if /Users/rubber/linux/block/bfq-iosched.c: 4392
 * this function returns true for that queue. As a consequence, since /Users/rubber/linux/block/bfq-iosched.c: 4393
 * device idling plays a critical role for both throughput boosting /Users/rubber/linux/block/bfq-iosched.c: 4394
 * and service guarantees, the return value of this function plays a /Users/rubber/linux/block/bfq-iosched.c: 4395
 * critical role as well. /Users/rubber/linux/block/bfq-iosched.c: 4396
 * In a nutshell, this function returns true only if idling is /Users/rubber/linux/block/bfq-iosched.c: 4398
 * beneficial for throughput or, even if detrimental for throughput, /Users/rubber/linux/block/bfq-iosched.c: 4399
 * idling is however necessary to preserve service guarantees (low /Users/rubber/linux/block/bfq-iosched.c: 4400
 * latency, desired throughput distribution, ...). In particular, on /Users/rubber/linux/block/bfq-iosched.c: 4401
 * NCQ-capable devices, this function tries to return false, so as to /Users/rubber/linux/block/bfq-iosched.c: 4402
 * help keep the drives' internal queues full, whenever this helps the /Users/rubber/linux/block/bfq-iosched.c: 4403
 * device boost the throughput without causing any service-guarantee /Users/rubber/linux/block/bfq-iosched.c: 4404
 * issue. /Users/rubber/linux/block/bfq-iosched.c: 4405
 * Most of the issues taken into account to get the return value of /Users/rubber/linux/block/bfq-iosched.c: 4407
 * this function are not trivial. We discuss these issues in the two /Users/rubber/linux/block/bfq-iosched.c: 4408
 * functions providing the main pieces of information needed by this /Users/rubber/linux/block/bfq-iosched.c: 4409
 * function. /Users/rubber/linux/block/bfq-iosched.c: 4410
	/* /Users/rubber/linux/block/bfq-iosched.c: 4424
	 * Idling is performed only if slice_idle > 0. In addition, we /Users/rubber/linux/block/bfq-iosched.c: 4425
	 * do not idle if /Users/rubber/linux/block/bfq-iosched.c: 4426
	 * (a) bfqq is async /Users/rubber/linux/block/bfq-iosched.c: 4427
	 * (b) bfqq is in the idle io prio class: in this case we do /Users/rubber/linux/block/bfq-iosched.c: 4428
	 * not idle because we want to minimize the bandwidth that /Users/rubber/linux/block/bfq-iosched.c: 4429
	 * queues in this class can steal to higher-priority queues /Users/rubber/linux/block/bfq-iosched.c: 4430
	/* /Users/rubber/linux/block/bfq-iosched.c: 4442
	 * We have now the two components we need to compute the /Users/rubber/linux/block/bfq-iosched.c: 4443
	 * return value of the function, which is true only if idling /Users/rubber/linux/block/bfq-iosched.c: 4444
	 * either boosts the throughput (without issues), or is /Users/rubber/linux/block/bfq-iosched.c: 4445
	 * necessary to preserve service guarantees. /Users/rubber/linux/block/bfq-iosched.c: 4446
 * If the in-service queue is empty but the function bfq_better_to_idle /Users/rubber/linux/block/bfq-iosched.c: 4453
 * returns true, then: /Users/rubber/linux/block/bfq-iosched.c: 4454
 * 1) the queue must remain in service and cannot be expired, and /Users/rubber/linux/block/bfq-iosched.c: 4455
 * 2) the device must be idled to wait for the possible arrival of a new /Users/rubber/linux/block/bfq-iosched.c: 4456
 *    request for the queue. /Users/rubber/linux/block/bfq-iosched.c: 4457
 * See the comments on the function bfq_better_to_idle for the reasons /Users/rubber/linux/block/bfq-iosched.c: 4458
 * why performing device idling is the best choice to boost the throughput /Users/rubber/linux/block/bfq-iosched.c: 4459
 * and preserve service guarantees when bfq_better_to_idle itself /Users/rubber/linux/block/bfq-iosched.c: 4460
 * returns true. /Users/rubber/linux/block/bfq-iosched.c: 4461
 * This function chooses the queue from which to pick the next extra /Users/rubber/linux/block/bfq-iosched.c: 4469
 * I/O request to inject, if it finds a compatible queue. See the /Users/rubber/linux/block/bfq-iosched.c: 4470
 * comments on bfq_update_inject_limit() for details on the injection /Users/rubber/linux/block/bfq-iosched.c: 4471
 * mechanism, and for the definitions of the quantities mentioned /Users/rubber/linux/block/bfq-iosched.c: 4472
 * below. /Users/rubber/linux/block/bfq-iosched.c: 4473
	/* /Users/rubber/linux/block/bfq-iosched.c: 4480
	 * If /Users/rubber/linux/block/bfq-iosched.c: 4481
	 * - bfqq is not weight-raised and therefore does not carry /Users/rubber/linux/block/bfq-iosched.c: 4482
	 *   time-critical I/O, /Users/rubber/linux/block/bfq-iosched.c: 4483
	 * or /Users/rubber/linux/block/bfq-iosched.c: 4484
	 * - regardless of whether bfqq is weight-raised, bfqq has /Users/rubber/linux/block/bfq-iosched.c: 4485
	 *   however a long think time, during which it can absorb the /Users/rubber/linux/block/bfq-iosched.c: 4486
	 *   effect of an appropriate number of extra I/O requests /Users/rubber/linux/block/bfq-iosched.c: 4487
	 *   from other queues (see bfq_update_inject_limit for /Users/rubber/linux/block/bfq-iosched.c: 4488
	 *   details on the computation of this number); /Users/rubber/linux/block/bfq-iosched.c: 4489
	 * then injection can be performed without restrictions. /Users/rubber/linux/block/bfq-iosched.c: 4490
	/* /Users/rubber/linux/block/bfq-iosched.c: 4495
	 * If /Users/rubber/linux/block/bfq-iosched.c: 4496
	 * - the baseline total service time could not be sampled yet, /Users/rubber/linux/block/bfq-iosched.c: 4497
	 *   so the inject limit happens to be still 0, and /Users/rubber/linux/block/bfq-iosched.c: 4498
	 * - a lot of time has elapsed since the plugging of I/O /Users/rubber/linux/block/bfq-iosched.c: 4499
	 *   dispatching started, so drive speed is being wasted /Users/rubber/linux/block/bfq-iosched.c: 4500
	 *   significantly; /Users/rubber/linux/block/bfq-iosched.c: 4501
	 * then temporarily raise inject limit to one request. /Users/rubber/linux/block/bfq-iosched.c: 4502
	/* /Users/rubber/linux/block/bfq-iosched.c: 4514
	 * Linear search of the source queue for injection; but, with /Users/rubber/linux/block/bfq-iosched.c: 4515
	 * a high probability, very few steps are needed to find a /Users/rubber/linux/block/bfq-iosched.c: 4516
	 * candidate queue, i.e., a queue with enough budget left for /Users/rubber/linux/block/bfq-iosched.c: 4517
	 * its next request. In fact: /Users/rubber/linux/block/bfq-iosched.c: 4518
	 * - BFQ dynamically updates the budget of every queue so as /Users/rubber/linux/block/bfq-iosched.c: 4519
	 *   to accommodate the expected backlog of the queue; /Users/rubber/linux/block/bfq-iosched.c: 4520
	 * - if a queue gets all its requests dispatched as injected /Users/rubber/linux/block/bfq-iosched.c: 4521
	 *   service, then the queue is removed from the active list /Users/rubber/linux/block/bfq-iosched.c: 4522
	 *   (and re-added only if it gets new requests, but then it /Users/rubber/linux/block/bfq-iosched.c: 4523
	 *   is assigned again enough budget for its new backlog). /Users/rubber/linux/block/bfq-iosched.c: 4524
			/* /Users/rubber/linux/block/bfq-iosched.c: 4531
			 * Allow for only one large in-flight request /Users/rubber/linux/block/bfq-iosched.c: 4532
			 * on non-rotational devices, for the /Users/rubber/linux/block/bfq-iosched.c: 4533
			 * following reason. On non-rotationl drives, /Users/rubber/linux/block/bfq-iosched.c: 4534
			 * large requests take much longer than /Users/rubber/linux/block/bfq-iosched.c: 4535
			 * smaller requests to be served. In addition, /Users/rubber/linux/block/bfq-iosched.c: 4536
			 * the drive prefers to serve large requests /Users/rubber/linux/block/bfq-iosched.c: 4537
			 * w.r.t. to small ones, if it can choose. So, /Users/rubber/linux/block/bfq-iosched.c: 4538
			 * having more than one large requests queued /Users/rubber/linux/block/bfq-iosched.c: 4539
			 * in the drive may easily make the next first /Users/rubber/linux/block/bfq-iosched.c: 4540
			 * request of the in-service queue wait for so /Users/rubber/linux/block/bfq-iosched.c: 4541
			 * long to break bfqq's service guarantees. On /Users/rubber/linux/block/bfq-iosched.c: 4542
			 * the bright side, large requests let the /Users/rubber/linux/block/bfq-iosched.c: 4543
			 * drive reach a very high throughput, even if /Users/rubber/linux/block/bfq-iosched.c: 4544
			 * there is only one in-flight large request /Users/rubber/linux/block/bfq-iosched.c: 4545
			 * at a time. /Users/rubber/linux/block/bfq-iosched.c: 4546
 * Select a queue for service.  If we have a current queue in service, /Users/rubber/linux/block/bfq-iosched.c: 4565
 * check whether to continue servicing it, or retrieve and set a new one. /Users/rubber/linux/block/bfq-iosched.c: 4566
	/* /Users/rubber/linux/block/bfq-iosched.c: 4580
	 * Do not expire bfqq for budget timeout if bfqq may be about /Users/rubber/linux/block/bfq-iosched.c: 4581
	 * to enjoy device idling. The reason why, in this case, we /Users/rubber/linux/block/bfq-iosched.c: 4582
	 * prevent bfqq from expiring is the same as in the comments /Users/rubber/linux/block/bfq-iosched.c: 4583
	 * on the case where bfq_bfqq_must_idle() returns true, in /Users/rubber/linux/block/bfq-iosched.c: 4584
	 * bfq_completed_request(). /Users/rubber/linux/block/bfq-iosched.c: 4585
	/* /Users/rubber/linux/block/bfq-iosched.c: 4592
	 * This loop is rarely executed more than once. Even when it /Users/rubber/linux/block/bfq-iosched.c: 4593
	 * happens, it is much more convenient to re-execute this loop /Users/rubber/linux/block/bfq-iosched.c: 4594
	 * than to return NULL and trigger a new dispatch to get a /Users/rubber/linux/block/bfq-iosched.c: 4595
	 * request served. /Users/rubber/linux/block/bfq-iosched.c: 4596
	/* /Users/rubber/linux/block/bfq-iosched.c: 4599
	 * If bfqq has requests queued and it has enough budget left to /Users/rubber/linux/block/bfq-iosched.c: 4600
	 * serve them, keep the queue, otherwise expire it. /Users/rubber/linux/block/bfq-iosched.c: 4601
			/* /Users/rubber/linux/block/bfq-iosched.c: 4606
			 * Expire the queue for budget exhaustion, /Users/rubber/linux/block/bfq-iosched.c: 4607
			 * which makes sure that the next budget is /Users/rubber/linux/block/bfq-iosched.c: 4608
			 * enough to serve the next request, even if /Users/rubber/linux/block/bfq-iosched.c: 4609
			 * it comes from the fifo expired path. /Users/rubber/linux/block/bfq-iosched.c: 4610
			/* /Users/rubber/linux/block/bfq-iosched.c: 4615
			 * The idle timer may be pending because we may /Users/rubber/linux/block/bfq-iosched.c: 4616
			 * not disable disk idling even when a new request /Users/rubber/linux/block/bfq-iosched.c: 4617
			 * arrives. /Users/rubber/linux/block/bfq-iosched.c: 4618
				/* /Users/rubber/linux/block/bfq-iosched.c: 4621
				 * If we get here: 1) at least a new request /Users/rubber/linux/block/bfq-iosched.c: 4622
				 * has arrived but we have not disabled the /Users/rubber/linux/block/bfq-iosched.c: 4623
				 * timer because the request was too small, /Users/rubber/linux/block/bfq-iosched.c: 4624
				 * 2) then the block layer has unplugged /Users/rubber/linux/block/bfq-iosched.c: 4625
				 * the device, causing the dispatch to be /Users/rubber/linux/block/bfq-iosched.c: 4626
				 * invoked. /Users/rubber/linux/block/bfq-iosched.c: 4627
				 * /Users/rubber/linux/block/bfq-iosched.c: 4628
				 * Since the device is unplugged, now the /Users/rubber/linux/block/bfq-iosched.c: 4629
				 * requests are probably large enough to /Users/rubber/linux/block/bfq-iosched.c: 4630
				 * provide a reasonable throughput. /Users/rubber/linux/block/bfq-iosched.c: 4631
				 * So we disable idling. /Users/rubber/linux/block/bfq-iosched.c: 4632
	/* /Users/rubber/linux/block/bfq-iosched.c: 4641
	 * No requests pending. However, if the in-service queue is idling /Users/rubber/linux/block/bfq-iosched.c: 4642
	 * for a new request, or has requests waiting for a completion and /Users/rubber/linux/block/bfq-iosched.c: 4643
	 * may idle after their completion, then keep it anyway. /Users/rubber/linux/block/bfq-iosched.c: 4644
	 * /Users/rubber/linux/block/bfq-iosched.c: 4645
	 * Yet, inject service from other queues if it boosts /Users/rubber/linux/block/bfq-iosched.c: 4646
	 * throughput and is possible. /Users/rubber/linux/block/bfq-iosched.c: 4647
		/* /Users/rubber/linux/block/bfq-iosched.c: 4663
		 * The next four mutually-exclusive ifs decide /Users/rubber/linux/block/bfq-iosched.c: 4664
		 * whether to try injection, and choose the queue to /Users/rubber/linux/block/bfq-iosched.c: 4665
		 * pick an I/O request from. /Users/rubber/linux/block/bfq-iosched.c: 4666
		 * /Users/rubber/linux/block/bfq-iosched.c: 4667
		 * The first if checks whether the process associated /Users/rubber/linux/block/bfq-iosched.c: 4668
		 * with bfqq has also async I/O pending. If so, it /Users/rubber/linux/block/bfq-iosched.c: 4669
		 * injects such I/O unconditionally. Injecting async /Users/rubber/linux/block/bfq-iosched.c: 4670
		 * I/O from the same process can cause no harm to the /Users/rubber/linux/block/bfq-iosched.c: 4671
		 * process. On the contrary, it can only increase /Users/rubber/linux/block/bfq-iosched.c: 4672
		 * bandwidth and reduce latency for the process. /Users/rubber/linux/block/bfq-iosched.c: 4673
		 * /Users/rubber/linux/block/bfq-iosched.c: 4674
		 * The second if checks whether there happens to be a /Users/rubber/linux/block/bfq-iosched.c: 4675
		 * non-empty waker queue for bfqq, i.e., a queue whose /Users/rubber/linux/block/bfq-iosched.c: 4676
		 * I/O needs to be completed for bfqq to receive new /Users/rubber/linux/block/bfq-iosched.c: 4677
		 * I/O. This happens, e.g., if bfqq is associated with /Users/rubber/linux/block/bfq-iosched.c: 4678
		 * a process that does some sync. A sync generates /Users/rubber/linux/block/bfq-iosched.c: 4679
		 * extra blocking I/O, which must be completed before /Users/rubber/linux/block/bfq-iosched.c: 4680
		 * the process associated with bfqq can go on with its /Users/rubber/linux/block/bfq-iosched.c: 4681
		 * I/O. If the I/O of the waker queue is not served, /Users/rubber/linux/block/bfq-iosched.c: 4682
		 * then bfqq remains empty, and no I/O is dispatched, /Users/rubber/linux/block/bfq-iosched.c: 4683
		 * until the idle timeout fires for bfqq. This is /Users/rubber/linux/block/bfq-iosched.c: 4684
		 * likely to result in lower bandwidth and higher /Users/rubber/linux/block/bfq-iosched.c: 4685
		 * latencies for bfqq, and in a severe loss of total /Users/rubber/linux/block/bfq-iosched.c: 4686
		 * throughput. The best action to take is therefore to /Users/rubber/linux/block/bfq-iosched.c: 4687
		 * serve the waker queue as soon as possible. So do it /Users/rubber/linux/block/bfq-iosched.c: 4688
		 * (without relying on the third alternative below for /Users/rubber/linux/block/bfq-iosched.c: 4689
		 * eventually serving waker_bfqq's I/O; see the last /Users/rubber/linux/block/bfq-iosched.c: 4690
		 * paragraph for further details). This systematic /Users/rubber/linux/block/bfq-iosched.c: 4691
		 * injection of I/O from the waker queue does not /Users/rubber/linux/block/bfq-iosched.c: 4692
		 * cause any delay to bfqq's I/O. On the contrary, /Users/rubber/linux/block/bfq-iosched.c: 4693
		 * next bfqq's I/O is brought forward dramatically, /Users/rubber/linux/block/bfq-iosched.c: 4694
		 * for it is not blocked for milliseconds. /Users/rubber/linux/block/bfq-iosched.c: 4695
		 * /Users/rubber/linux/block/bfq-iosched.c: 4696
		 * The third if checks whether there is a queue woken /Users/rubber/linux/block/bfq-iosched.c: 4697
		 * by bfqq, and currently with pending I/O. Such a /Users/rubber/linux/block/bfq-iosched.c: 4698
		 * woken queue does not steal bandwidth from bfqq, /Users/rubber/linux/block/bfq-iosched.c: 4699
		 * because it remains soon without I/O if bfqq is not /Users/rubber/linux/block/bfq-iosched.c: 4700
		 * served. So there is virtually no risk of loss of /Users/rubber/linux/block/bfq-iosched.c: 4701
		 * bandwidth for bfqq if this woken queue has I/O /Users/rubber/linux/block/bfq-iosched.c: 4702
		 * dispatched while bfqq is waiting for new I/O. /Users/rubber/linux/block/bfq-iosched.c: 4703
		 * /Users/rubber/linux/block/bfq-iosched.c: 4704
		 * The fourth if checks whether bfqq is a queue for /Users/rubber/linux/block/bfq-iosched.c: 4705
		 * which it is better to avoid injection. It is so if /Users/rubber/linux/block/bfq-iosched.c: 4706
		 * bfqq delivers more throughput when served without /Users/rubber/linux/block/bfq-iosched.c: 4707
		 * any further I/O from other queues in the middle, or /Users/rubber/linux/block/bfq-iosched.c: 4708
		 * if the service times of bfqq's I/O requests both /Users/rubber/linux/block/bfq-iosched.c: 4709
		 * count more than overall throughput, and may be /Users/rubber/linux/block/bfq-iosched.c: 4710
		 * easily increased by injection (this happens if bfqq /Users/rubber/linux/block/bfq-iosched.c: 4711
		 * has a short think time). If none of these /Users/rubber/linux/block/bfq-iosched.c: 4712
		 * conditions holds, then a candidate queue for /Users/rubber/linux/block/bfq-iosched.c: 4713
		 * injection is looked for through /Users/rubber/linux/block/bfq-iosched.c: 4714
		 * bfq_choose_bfqq_for_injection(). Note that the /Users/rubber/linux/block/bfq-iosched.c: 4715
		 * latter may return NULL (for example if the inject /Users/rubber/linux/block/bfq-iosched.c: 4716
		 * limit for bfqq is currently 0). /Users/rubber/linux/block/bfq-iosched.c: 4717
		 * /Users/rubber/linux/block/bfq-iosched.c: 4718
		 * NOTE: motivation for the second alternative /Users/rubber/linux/block/bfq-iosched.c: 4719
		 * /Users/rubber/linux/block/bfq-iosched.c: 4720
		 * Thanks to the way the inject limit is updated in /Users/rubber/linux/block/bfq-iosched.c: 4721
		 * bfq_update_has_short_ttime(), it is rather likely /Users/rubber/linux/block/bfq-iosched.c: 4722
		 * that, if I/O is being plugged for bfqq and the /Users/rubber/linux/block/bfq-iosched.c: 4723
		 * waker queue has pending I/O requests that are /Users/rubber/linux/block/bfq-iosched.c: 4724
		 * blocking bfqq's I/O, then the fourth alternative /Users/rubber/linux/block/bfq-iosched.c: 4725
		 * above lets the waker queue get served before the /Users/rubber/linux/block/bfq-iosched.c: 4726
		 * I/O-plugging timeout fires. So one may deem the /Users/rubber/linux/block/bfq-iosched.c: 4727
		 * second alternative superfluous. It is not, because /Users/rubber/linux/block/bfq-iosched.c: 4728
		 * the fourth alternative may be way less effective in /Users/rubber/linux/block/bfq-iosched.c: 4729
		 * case of a synchronization. For two main /Users/rubber/linux/block/bfq-iosched.c: 4730
		 * reasons. First, throughput may be low because the /Users/rubber/linux/block/bfq-iosched.c: 4731
		 * inject limit may be too low to guarantee the same /Users/rubber/linux/block/bfq-iosched.c: 4732
		 * amount of injected I/O, from the waker queue or /Users/rubber/linux/block/bfq-iosched.c: 4733
		 * other queues, that the second alternative /Users/rubber/linux/block/bfq-iosched.c: 4734
		 * guarantees (the second alternative unconditionally /Users/rubber/linux/block/bfq-iosched.c: 4735
		 * injects a pending I/O request of the waker queue /Users/rubber/linux/block/bfq-iosched.c: 4736
		 * for each bfq_dispatch_request()). Second, with the /Users/rubber/linux/block/bfq-iosched.c: 4737
		 * fourth alternative, the duration of the plugging, /Users/rubber/linux/block/bfq-iosched.c: 4738
		 * i.e., the time before bfqq finally receives new I/O, /Users/rubber/linux/block/bfq-iosched.c: 4739
		 * may not be minimized, because the waker queue may /Users/rubber/linux/block/bfq-iosched.c: 4740
		 * happen to be served only after other queues. /Users/rubber/linux/block/bfq-iosched.c: 4741
		/* /Users/rubber/linux/block/bfq-iosched.c: 4807
		 * If the queue was activated in a burst, or too much /Users/rubber/linux/block/bfq-iosched.c: 4808
		 * time has elapsed from the beginning of this /Users/rubber/linux/block/bfq-iosched.c: 4809
		 * weight-raising period, then end weight raising. /Users/rubber/linux/block/bfq-iosched.c: 4810
				/* /Users/rubber/linux/block/bfq-iosched.c: 4819
				 * Either in interactive weight /Users/rubber/linux/block/bfq-iosched.c: 4820
				 * raising, or in soft_rt weight /Users/rubber/linux/block/bfq-iosched.c: 4821
				 * raising with the /Users/rubber/linux/block/bfq-iosched.c: 4822
				 * interactive-weight-raising period /Users/rubber/linux/block/bfq-iosched.c: 4823
				 * elapsed (so no switch back to /Users/rubber/linux/block/bfq-iosched.c: 4824
				 * interactive weight raising). /Users/rubber/linux/block/bfq-iosched.c: 4825
			} else { /* /Users/rubber/linux/block/bfq-iosched.c: 4828
				  * soft_rt finishing while still in /Users/rubber/linux/block/bfq-iosched.c: 4829
				  * interactive period, switch back to /Users/rubber/linux/block/bfq-iosched.c: 4830
				  * interactive weight raising /Users/rubber/linux/block/bfq-iosched.c: 4831
	/* /Users/rubber/linux/block/bfq-iosched.c: 4844
	 * To improve latency (for this or other queues), immediately /Users/rubber/linux/block/bfq-iosched.c: 4845
	 * update weight both if it must be raised and if it must be /Users/rubber/linux/block/bfq-iosched.c: 4846
	 * lowered. Since, entity may be on some active tree here, and /Users/rubber/linux/block/bfq-iosched.c: 4847
	 * might have a pending change of its ioprio class, invoke /Users/rubber/linux/block/bfq-iosched.c: 4848
	 * next function with the last parameter unset (see the /Users/rubber/linux/block/bfq-iosched.c: 4849
	 * comments on the function). /Users/rubber/linux/block/bfq-iosched.c: 4850
 * Dispatch next request from bfqq. /Users/rubber/linux/block/bfq-iosched.c: 4858
	/* /Users/rubber/linux/block/bfq-iosched.c: 4880
	 * If weight raising has to terminate for bfqq, then next /Users/rubber/linux/block/bfq-iosched.c: 4881
	 * function causes an immediate update of bfqq's weight, /Users/rubber/linux/block/bfq-iosched.c: 4882
	 * without waiting for next activation. As a consequence, on /Users/rubber/linux/block/bfq-iosched.c: 4883
	 * expiration, bfqq will be timestamped as if has never been /Users/rubber/linux/block/bfq-iosched.c: 4884
	 * weight-raised during this service slot, even if it has /Users/rubber/linux/block/bfq-iosched.c: 4885
	 * received part or even most of the service as a /Users/rubber/linux/block/bfq-iosched.c: 4886
	 * weight-raised queue. This inflates bfqq's timestamps, which /Users/rubber/linux/block/bfq-iosched.c: 4887
	 * is beneficial, as bfqq is then more willing to leave the /Users/rubber/linux/block/bfq-iosched.c: 4888
	 * device immediately to possible other weight-raised queues. /Users/rubber/linux/block/bfq-iosched.c: 4889
	/* /Users/rubber/linux/block/bfq-iosched.c: 4893
	 * Expire bfqq, pretending that its budget expired, if bfqq /Users/rubber/linux/block/bfq-iosched.c: 4894
	 * belongs to CLASS_IDLE and other queues are waiting for /Users/rubber/linux/block/bfq-iosched.c: 4895
	 * service. /Users/rubber/linux/block/bfq-iosched.c: 4896
	/* /Users/rubber/linux/block/bfq-iosched.c: 4911
	 * Avoiding lock: a race on bfqd->busy_queues should cause at /Users/rubber/linux/block/bfq-iosched.c: 4912
	 * most a call to dispatch for nothing /Users/rubber/linux/block/bfq-iosched.c: 4913
			/* /Users/rubber/linux/block/bfq-iosched.c: 4933
			 * Increment counters here, because this /Users/rubber/linux/block/bfq-iosched.c: 4934
			 * dispatch does not follow the standard /Users/rubber/linux/block/bfq-iosched.c: 4935
			 * dispatch flow (where counters are /Users/rubber/linux/block/bfq-iosched.c: 4936
			 * incremented) /Users/rubber/linux/block/bfq-iosched.c: 4937
		/* /Users/rubber/linux/block/bfq-iosched.c: 4944
		 * We exploit the bfq_finish_requeue_request hook to /Users/rubber/linux/block/bfq-iosched.c: 4945
		 * decrement rq_in_driver, but /Users/rubber/linux/block/bfq-iosched.c: 4946
		 * bfq_finish_requeue_request will not be invoked on /Users/rubber/linux/block/bfq-iosched.c: 4947
		 * this request. So, to avoid unbalance, just start /Users/rubber/linux/block/bfq-iosched.c: 4948
		 * this request, without incrementing rq_in_driver. As /Users/rubber/linux/block/bfq-iosched.c: 4949
		 * a negative consequence, rq_in_driver is deceptively /Users/rubber/linux/block/bfq-iosched.c: 4950
		 * lower than it should be while this request is in /Users/rubber/linux/block/bfq-iosched.c: 4951
		 * service. This may cause bfq_schedule_dispatch to be /Users/rubber/linux/block/bfq-iosched.c: 4952
		 * invoked uselessly. /Users/rubber/linux/block/bfq-iosched.c: 4953
		 * /Users/rubber/linux/block/bfq-iosched.c: 4954
		 * As for implementing an exact solution, the /Users/rubber/linux/block/bfq-iosched.c: 4955
		 * bfq_finish_requeue_request hook, if defined, is /Users/rubber/linux/block/bfq-iosched.c: 4956
		 * probably invoked also on this request. So, by /Users/rubber/linux/block/bfq-iosched.c: 4957
		 * exploiting this hook, we could 1) increment /Users/rubber/linux/block/bfq-iosched.c: 4958
		 * rq_in_driver here, and 2) decrement it in /Users/rubber/linux/block/bfq-iosched.c: 4959
		 * bfq_finish_requeue_request. Such a solution would /Users/rubber/linux/block/bfq-iosched.c: 4960
		 * let the value of the counter be always accurate, /Users/rubber/linux/block/bfq-iosched.c: 4961
		 * but it would entail using an extra interface /Users/rubber/linux/block/bfq-iosched.c: 4962
		 * function. This cost seems higher than the benefit, /Users/rubber/linux/block/bfq-iosched.c: 4963
		 * being the frequency of non-elevator-private /Users/rubber/linux/block/bfq-iosched.c: 4964
		 * requests very low. /Users/rubber/linux/block/bfq-iosched.c: 4965
	/* /Users/rubber/linux/block/bfq-iosched.c: 4976
	 * Force device to serve one request at a time if /Users/rubber/linux/block/bfq-iosched.c: 4977
	 * strict_guarantees is true. Forcing this service scheme is /Users/rubber/linux/block/bfq-iosched.c: 4978
	 * currently the ONLY way to guarantee that the request /Users/rubber/linux/block/bfq-iosched.c: 4979
	 * service order enforced by the scheduler is respected by a /Users/rubber/linux/block/bfq-iosched.c: 4980
	 * queueing device. Otherwise the device is free even to make /Users/rubber/linux/block/bfq-iosched.c: 4981
	 * some unlucky request wait for as long as the device /Users/rubber/linux/block/bfq-iosched.c: 4982
	 * wishes. /Users/rubber/linux/block/bfq-iosched.c: 4983
	 * /Users/rubber/linux/block/bfq-iosched.c: 4984
	 * Of course, serving one request at a time may cause loss of /Users/rubber/linux/block/bfq-iosched.c: 4985
	 * throughput. /Users/rubber/linux/block/bfq-iosched.c: 4986
	/* /Users/rubber/linux/block/bfq-iosched.c: 5018
	 * rq and bfqq are guaranteed to exist until this function /Users/rubber/linux/block/bfq-iosched.c: 5019
	 * ends, for the following reasons. First, rq can be /Users/rubber/linux/block/bfq-iosched.c: 5020
	 * dispatched to the device, and then can be completed and /Users/rubber/linux/block/bfq-iosched.c: 5021
	 * freed, only after this function ends. Second, rq cannot be /Users/rubber/linux/block/bfq-iosched.c: 5022
	 * merged (and thus freed because of a merge) any longer, /Users/rubber/linux/block/bfq-iosched.c: 5023
	 * because it has already started. Thus rq cannot be freed /Users/rubber/linux/block/bfq-iosched.c: 5024
	 * before this function ends, and, since rq has a reference to /Users/rubber/linux/block/bfq-iosched.c: 5025
	 * bfqq, the same guarantee holds for bfqq too. /Users/rubber/linux/block/bfq-iosched.c: 5026
	 * /Users/rubber/linux/block/bfq-iosched.c: 5027
	 * In addition, the following queue lock guarantees that /Users/rubber/linux/block/bfq-iosched.c: 5028
	 * bfqq_group(bfqq) exists as well. /Users/rubber/linux/block/bfq-iosched.c: 5029
		/* /Users/rubber/linux/block/bfq-iosched.c: 5033
		 * Since the idle timer has been disabled, /Users/rubber/linux/block/bfq-iosched.c: 5034
		 * in_serv_queue contained some request when /Users/rubber/linux/block/bfq-iosched.c: 5035
		 * __bfq_dispatch_request was invoked above, which /Users/rubber/linux/block/bfq-iosched.c: 5036
		 * implies that rq was picked exactly from /Users/rubber/linux/block/bfq-iosched.c: 5037
		 * in_serv_queue. Thus in_serv_queue == bfqq, and is /Users/rubber/linux/block/bfq-iosched.c: 5038
		 * therefore guaranteed to exist because of the above /Users/rubber/linux/block/bfq-iosched.c: 5039
		 * arguments. /Users/rubber/linux/block/bfq-iosched.c: 5040
 * Task holds one reference to the queue, dropped when task exits.  Each rq /Users/rubber/linux/block/bfq-iosched.c: 5085
 * in-flight on this queue also holds a reference, dropped when rq is freed. /Users/rubber/linux/block/bfq-iosched.c: 5086
 * Scheduler lock must be held here. Recall not to use bfqq after calling /Users/rubber/linux/block/bfq-iosched.c: 5088
 * this function on it. /Users/rubber/linux/block/bfq-iosched.c: 5089
		/* /Users/rubber/linux/block/bfq-iosched.c: 5107
		 * Decrement also burst size after the removal, if the /Users/rubber/linux/block/bfq-iosched.c: 5108
		 * process associated with bfqq is exiting, and thus /Users/rubber/linux/block/bfq-iosched.c: 5109
		 * does not contribute to the burst any longer. This /Users/rubber/linux/block/bfq-iosched.c: 5110
		 * decrement helps filter out false positives of large /Users/rubber/linux/block/bfq-iosched.c: 5111
		 * bursts, when some short-lived process (often due to /Users/rubber/linux/block/bfq-iosched.c: 5112
		 * the execution of commands by some service) happens /Users/rubber/linux/block/bfq-iosched.c: 5113
		 * to start and exit while a complex application is /Users/rubber/linux/block/bfq-iosched.c: 5114
		 * starting, and thus spawning several processes that /Users/rubber/linux/block/bfq-iosched.c: 5115
		 * do I/O (and that *must not* be treated as a large /Users/rubber/linux/block/bfq-iosched.c: 5116
		 * burst, see comments on bfq_handle_burst). /Users/rubber/linux/block/bfq-iosched.c: 5117
		 * /Users/rubber/linux/block/bfq-iosched.c: 5118
		 * In particular, the decrement is performed only if: /Users/rubber/linux/block/bfq-iosched.c: 5119
		 * 1) bfqq is not a merged queue, because, if it is, /Users/rubber/linux/block/bfq-iosched.c: 5120
		 * then this free of bfqq is not triggered by the exit /Users/rubber/linux/block/bfq-iosched.c: 5121
		 * of the process bfqq is associated with, but exactly /Users/rubber/linux/block/bfq-iosched.c: 5122
		 * by the fact that bfqq has just been merged. /Users/rubber/linux/block/bfq-iosched.c: 5123
		 * 2) burst_size is greater than 0, to handle /Users/rubber/linux/block/bfq-iosched.c: 5124
		 * unbalanced decrements. Unbalanced decrements may /Users/rubber/linux/block/bfq-iosched.c: 5125
		 * happen in te following case: bfqq is inserted into /Users/rubber/linux/block/bfq-iosched.c: 5126
		 * the current burst list--without incrementing /Users/rubber/linux/block/bfq-iosched.c: 5127
		 * bust_size--because of a split, but the current /Users/rubber/linux/block/bfq-iosched.c: 5128
		 * burst list is not the burst list bfqq belonged to /Users/rubber/linux/block/bfq-iosched.c: 5129
		 * (see comments on the case of a split in /Users/rubber/linux/block/bfq-iosched.c: 5130
		 * bfq_set_request). /Users/rubber/linux/block/bfq-iosched.c: 5131
	/* /Users/rubber/linux/block/bfq-iosched.c: 5137
	 * bfqq does not exist any longer, so it cannot be woken by /Users/rubber/linux/block/bfq-iosched.c: 5138
	 * any other queue, and cannot wake any other queue. Then bfqq /Users/rubber/linux/block/bfq-iosched.c: 5139
	 * must be removed from the woken list of its possible waker /Users/rubber/linux/block/bfq-iosched.c: 5140
	 * queue, and all queues in the woken list of bfqq must stop /Users/rubber/linux/block/bfq-iosched.c: 5141
	 * having a waker queue. Strictly speaking, these updates /Users/rubber/linux/block/bfq-iosched.c: 5142
	 * should be performed when bfqq remains with no I/O source /Users/rubber/linux/block/bfq-iosched.c: 5143
	 * attached to it, which happens before bfqq gets freed. In /Users/rubber/linux/block/bfq-iosched.c: 5144
	 * particular, this happens when the last process associated /Users/rubber/linux/block/bfq-iosched.c: 5145
	 * with bfqq exits or gets associated with a different /Users/rubber/linux/block/bfq-iosched.c: 5146
	 * queue. However, both events lead to bfqq being freed soon, /Users/rubber/linux/block/bfq-iosched.c: 5147
	 * and dangling references would come out only after bfqq gets /Users/rubber/linux/block/bfq-iosched.c: 5148
	 * freed. So these updates are done here, as a simple and safe /Users/rubber/linux/block/bfq-iosched.c: 5149
	 * way to handle all cases. /Users/rubber/linux/block/bfq-iosched.c: 5150
	/* /Users/rubber/linux/block/bfq-iosched.c: 5180
	 * If this queue was scheduled to merge with another queue, be /Users/rubber/linux/block/bfq-iosched.c: 5181
	 * sure to drop the reference taken on that queue (and others in /Users/rubber/linux/block/bfq-iosched.c: 5182
	 * the merge chain). See bfq_setup_merge and bfq_merge_bfqqs. /Users/rubber/linux/block/bfq-iosched.c: 5183
		/* /Users/rubber/linux/block/bfq-iosched.c: 5235
		 * bfqd is NULL if scheduler already exited, and in /Users/rubber/linux/block/bfq-iosched.c: 5236
		 * that case this is the last time bfqq is accessed. /Users/rubber/linux/block/bfq-iosched.c: 5237
 * Update the entity prio values; note that the new values will not /Users/rubber/linux/block/bfq-iosched.c: 5255
 * be used until the next (re)activation. /Users/rubber/linux/block/bfq-iosched.c: 5256
		/* /Users/rubber/linux/block/bfq-iosched.c: 5276
		 * No prio set, inherit CPU scheduling settings. /Users/rubber/linux/block/bfq-iosched.c: 5277
	/* /Users/rubber/linux/block/bfq-iosched.c: 5319
	 * This condition may trigger on a newly created bic, be sure to /Users/rubber/linux/block/bfq-iosched.c: 5320
	 * drop the lock before returning. /Users/rubber/linux/block/bfq-iosched.c: 5321
		/* /Users/rubber/linux/block/bfq-iosched.c: 5358
		 * No need to mark as has_short_ttime if in /Users/rubber/linux/block/bfq-iosched.c: 5359
		 * idle_class, because no device idling is performed /Users/rubber/linux/block/bfq-iosched.c: 5360
		 * for queues in idle class /Users/rubber/linux/block/bfq-iosched.c: 5361
	/* /Users/rubber/linux/block/bfq-iosched.c: 5391
	 * To not forget the possibly high bandwidth consumed by a /Users/rubber/linux/block/bfq-iosched.c: 5392
	 * process/queue in the recent past, /Users/rubber/linux/block/bfq-iosched.c: 5393
	 * bfq_bfqq_softrt_next_start() returns a value at least equal /Users/rubber/linux/block/bfq-iosched.c: 5394
	 * to the current value of bfqq->soft_rt_next_start (see /Users/rubber/linux/block/bfq-iosched.c: 5395
	 * comments on bfq_bfqq_softrt_next_start).  Set /Users/rubber/linux/block/bfq-iosched.c: 5396
	 * soft_rt_next_start to now, to mean that bfqq has consumed /Users/rubber/linux/block/bfq-iosched.c: 5397
	 * no bandwidth so far. /Users/rubber/linux/block/bfq-iosched.c: 5398
	/* /Users/rubber/linux/block/bfq-iosched.c: 5440
	 * Reusing merge functions. This implies that /Users/rubber/linux/block/bfq-iosched.c: 5441
	 * bfqq->bic must be set too, for /Users/rubber/linux/block/bfq-iosched.c: 5442
	 * bfq_merge_bfqqs to correctly save bfqq's /Users/rubber/linux/block/bfq-iosched.c: 5443
	 * state before killing it. /Users/rubber/linux/block/bfq-iosched.c: 5444
 * Many throughput-sensitive workloads are made of several parallel /Users/rubber/linux/block/bfq-iosched.c: 5453
 * I/O flows, with all flows generated by the same application, or /Users/rubber/linux/block/bfq-iosched.c: 5454
 * more generically by the same task (e.g., system boot). The most /Users/rubber/linux/block/bfq-iosched.c: 5455
 * counterproductive action with these workloads is plugging I/O /Users/rubber/linux/block/bfq-iosched.c: 5456
 * dispatch when one of the bfq_queues associated with these flows /Users/rubber/linux/block/bfq-iosched.c: 5457
 * remains temporarily empty. /Users/rubber/linux/block/bfq-iosched.c: 5458
 * To avoid this plugging, BFQ has been using a burst-handling /Users/rubber/linux/block/bfq-iosched.c: 5460
 * mechanism for years now. This mechanism has proven effective for /Users/rubber/linux/block/bfq-iosched.c: 5461
 * throughput, and not detrimental for service guarantees. The /Users/rubber/linux/block/bfq-iosched.c: 5462
 * following function pushes this mechanism a little bit further, /Users/rubber/linux/block/bfq-iosched.c: 5463
 * basing on the following two facts. /Users/rubber/linux/block/bfq-iosched.c: 5464
 * First, all the I/O flows of a the same application or task /Users/rubber/linux/block/bfq-iosched.c: 5466
 * contribute to the execution/completion of that common application /Users/rubber/linux/block/bfq-iosched.c: 5467
 * or task. So the performance figures that matter are total /Users/rubber/linux/block/bfq-iosched.c: 5468
 * throughput of the flows and task-wide I/O latency.  In particular, /Users/rubber/linux/block/bfq-iosched.c: 5469
 * these flows do not need to be protected from each other, in terms /Users/rubber/linux/block/bfq-iosched.c: 5470
 * of individual bandwidth or latency. /Users/rubber/linux/block/bfq-iosched.c: 5471
 * Second, the above fact holds regardless of the number of flows. /Users/rubber/linux/block/bfq-iosched.c: 5473
 * Putting these two facts together, this commits merges stably the /Users/rubber/linux/block/bfq-iosched.c: 5475
 * bfq_queues associated with these I/O flows, i.e., with the /Users/rubber/linux/block/bfq-iosched.c: 5476
 * processes that generate these IO/ flows, regardless of how many the /Users/rubber/linux/block/bfq-iosched.c: 5477
 * involved processes are. /Users/rubber/linux/block/bfq-iosched.c: 5478
 * To decide whether a set of bfq_queues is actually associated with /Users/rubber/linux/block/bfq-iosched.c: 5480
 * the I/O flows of a common application or task, and to merge these /Users/rubber/linux/block/bfq-iosched.c: 5481
 * queues stably, this function operates as follows: given a bfq_queue, /Users/rubber/linux/block/bfq-iosched.c: 5482
 * say Q2, currently being created, and the last bfq_queue, say Q1, /Users/rubber/linux/block/bfq-iosched.c: 5483
 * created before Q2, Q2 is merged stably with Q1 if /Users/rubber/linux/block/bfq-iosched.c: 5484
 * - very little time has elapsed since when Q1 was created /Users/rubber/linux/block/bfq-iosched.c: 5485
 * - Q2 has the same ioprio as Q1 /Users/rubber/linux/block/bfq-iosched.c: 5486
 * - Q2 belongs to the same group as Q1 /Users/rubber/linux/block/bfq-iosched.c: 5487
 * Merging bfq_queues also reduces scheduling overhead. A fio test /Users/rubber/linux/block/bfq-iosched.c: 5489
 * with ten random readers on /dev/nullb shows a throughput boost of /Users/rubber/linux/block/bfq-iosched.c: 5490
 * 40%, with a quadcore. Since BFQ's execution time amounts to ~50% of /Users/rubber/linux/block/bfq-iosched.c: 5491
 * the total per-request processing time, the above throughput boost /Users/rubber/linux/block/bfq-iosched.c: 5492
 * implies that BFQ's overhead is reduced by more than 50%. /Users/rubber/linux/block/bfq-iosched.c: 5493
 * This new mechanism most certainly obsoletes the current /Users/rubber/linux/block/bfq-iosched.c: 5495
 * burst-handling heuristics. We keep those heuristics for the moment. /Users/rubber/linux/block/bfq-iosched.c: 5496
	/* /Users/rubber/linux/block/bfq-iosched.c: 5508
	 * If last_bfqq_created has not been set yet, then init it. If /Users/rubber/linux/block/bfq-iosched.c: 5509
	 * it has been set already, but too long ago, then move it /Users/rubber/linux/block/bfq-iosched.c: 5510
	 * forward to bfqq. Finally, move also if bfqq belongs to a /Users/rubber/linux/block/bfq-iosched.c: 5511
	 * different group than last_bfqq_created, or if bfqq has a /Users/rubber/linux/block/bfq-iosched.c: 5512
	 * different ioprio or ioprio_class. If none of these /Users/rubber/linux/block/bfq-iosched.c: 5513
	 * conditions holds true, then try an early stable merge or /Users/rubber/linux/block/bfq-iosched.c: 5514
	 * schedule a delayed stable merge. /Users/rubber/linux/block/bfq-iosched.c: 5515
	 * /Users/rubber/linux/block/bfq-iosched.c: 5516
	 * A delayed merge is scheduled (instead of performing an /Users/rubber/linux/block/bfq-iosched.c: 5517
	 * early merge), in case bfqq might soon prove to be more /Users/rubber/linux/block/bfq-iosched.c: 5518
	 * throughput-beneficial if not merged. Currently this is /Users/rubber/linux/block/bfq-iosched.c: 5519
	 * possible only if bfqd is rotational with no queueing. For /Users/rubber/linux/block/bfq-iosched.c: 5520
	 * such a drive, not merging bfqq is better for throughput if /Users/rubber/linux/block/bfq-iosched.c: 5521
	 * bfqq happens to contain sequential I/O. So, we wait a /Users/rubber/linux/block/bfq-iosched.c: 5522
	 * little bit for enough I/O to flow through bfqq. After that, /Users/rubber/linux/block/bfq-iosched.c: 5523
	 * if such an I/O is sequential, then the merge is /Users/rubber/linux/block/bfq-iosched.c: 5524
	 * canceled. Otherwise the merge is finally performed. /Users/rubber/linux/block/bfq-iosched.c: 5525
			/* /Users/rubber/linux/block/bfq-iosched.c: 5539
			 * With this type of drive, leaving /Users/rubber/linux/block/bfq-iosched.c: 5540
			 * bfqq alone may provide no /Users/rubber/linux/block/bfq-iosched.c: 5541
			 * throughput benefits compared with /Users/rubber/linux/block/bfq-iosched.c: 5542
			 * merging bfqq. So merge bfqq now. /Users/rubber/linux/block/bfq-iosched.c: 5543
			/* /Users/rubber/linux/block/bfq-iosched.c: 5549
			 * get reference on last_bfqq_created, /Users/rubber/linux/block/bfq-iosched.c: 5550
			 * to prevent it from being freed, /Users/rubber/linux/block/bfq-iosched.c: 5551
			 * until we decide whether to merge /Users/rubber/linux/block/bfq-iosched.c: 5552
			/* /Users/rubber/linux/block/bfq-iosched.c: 5555
			 * need to keep track of stable refs, to /Users/rubber/linux/block/bfq-iosched.c: 5556
			 * compute process refs correctly /Users/rubber/linux/block/bfq-iosched.c: 5557
			/* /Users/rubber/linux/block/bfq-iosched.c: 5560
			 * Record the bfqq to merge to. /Users/rubber/linux/block/bfq-iosched.c: 5561
	/* /Users/rubber/linux/block/bfq-iosched.c: 5613
	 * Pin the queue now that it's allocated, scheduler exit will /Users/rubber/linux/block/bfq-iosched.c: 5614
	 * prune it. /Users/rubber/linux/block/bfq-iosched.c: 5615
		bfqq->ref++; /* /Users/rubber/linux/block/bfq-iosched.c: 5618
			      * Extra group reference, w.r.t. sync /Users/rubber/linux/block/bfq-iosched.c: 5619
			      * queue. This extra reference is removed /Users/rubber/linux/block/bfq-iosched.c: 5620
			      * only if bfqq->bfqg disappears, to /Users/rubber/linux/block/bfq-iosched.c: 5621
			      * guarantee that this queue is not freed /Users/rubber/linux/block/bfq-iosched.c: 5622
			      * until its group goes away. /Users/rubber/linux/block/bfq-iosched.c: 5623
	/* /Users/rubber/linux/block/bfq-iosched.c: 5646
	 * We are really interested in how long it takes for the queue to /Users/rubber/linux/block/bfq-iosched.c: 5647
	 * become busy when there is no outstanding IO for this queue. So /Users/rubber/linux/block/bfq-iosched.c: 5648
	 * ignore cases when the bfq queue has already IO queued. /Users/rubber/linux/block/bfq-iosched.c: 5649
			/* /Users/rubber/linux/block/bfq-iosched.c: 5674
			 * In soft_rt weight raising with the /Users/rubber/linux/block/bfq-iosched.c: 5675
			 * interactive-weight-raising period /Users/rubber/linux/block/bfq-iosched.c: 5676
			 * elapsed (so no switch back to /Users/rubber/linux/block/bfq-iosched.c: 5677
			 * interactive weight raising). /Users/rubber/linux/block/bfq-iosched.c: 5678
		} else { /* /Users/rubber/linux/block/bfq-iosched.c: 5681
			  * stopping soft_rt weight raising /Users/rubber/linux/block/bfq-iosched.c: 5682
			  * while still in interactive period, /Users/rubber/linux/block/bfq-iosched.c: 5683
			  * switch back to interactive weight /Users/rubber/linux/block/bfq-iosched.c: 5684
			  * raising /Users/rubber/linux/block/bfq-iosched.c: 5685
	/* /Users/rubber/linux/block/bfq-iosched.c: 5699
	 * No need to update has_short_ttime if bfqq is async or in /Users/rubber/linux/block/bfq-iosched.c: 5700
	 * idle io prio class, or if bfq_slice_idle is zero, because /Users/rubber/linux/block/bfq-iosched.c: 5701
	 * no device idling is performed for bfqq in this case. /Users/rubber/linux/block/bfq-iosched.c: 5702
	/* Think time is infinite if no process is linked to /Users/rubber/linux/block/bfq-iosched.c: 5713
	 * bfqq. Otherwise check average think time to decide whether /Users/rubber/linux/block/bfq-iosched.c: 5714
	 * to mark as has_short_ttime. To this goal, compare average /Users/rubber/linux/block/bfq-iosched.c: 5715
	 * think time with half the I/O-plugging timeout. /Users/rubber/linux/block/bfq-iosched.c: 5716
	/* /Users/rubber/linux/block/bfq-iosched.c: 5730
	 * Until the base value for the total service time gets /Users/rubber/linux/block/bfq-iosched.c: 5731
	 * finally computed for bfqq, the inject limit does depend on /Users/rubber/linux/block/bfq-iosched.c: 5732
	 * the think-time state (short|long). In particular, the limit /Users/rubber/linux/block/bfq-iosched.c: 5733
	 * is 0 or 1 if the think time is deemed, respectively, as /Users/rubber/linux/block/bfq-iosched.c: 5734
	 * short or long (details in the comments in /Users/rubber/linux/block/bfq-iosched.c: 5735
	 * bfq_update_inject_limit()). Accordingly, the next /Users/rubber/linux/block/bfq-iosched.c: 5736
	 * instructions reset the inject limit if the think-time state /Users/rubber/linux/block/bfq-iosched.c: 5737
	 * has changed and the above base value is still to be /Users/rubber/linux/block/bfq-iosched.c: 5738
	 * computed. /Users/rubber/linux/block/bfq-iosched.c: 5739
	 * /Users/rubber/linux/block/bfq-iosched.c: 5740
	 * However, the reset is performed only if more than 100 ms /Users/rubber/linux/block/bfq-iosched.c: 5741
	 * have elapsed since the last update of the inject limit, or /Users/rubber/linux/block/bfq-iosched.c: 5742
	 * (inclusive) if the change is from short to long think /Users/rubber/linux/block/bfq-iosched.c: 5743
	 * time. The reason for this waiting is as follows. /Users/rubber/linux/block/bfq-iosched.c: 5744
	 * /Users/rubber/linux/block/bfq-iosched.c: 5745
	 * bfqq may have a long think time because of a /Users/rubber/linux/block/bfq-iosched.c: 5746
	 * synchronization with some other queue, i.e., because the /Users/rubber/linux/block/bfq-iosched.c: 5747
	 * I/O of some other queue may need to be completed for bfqq /Users/rubber/linux/block/bfq-iosched.c: 5748
	 * to receive new I/O. Details in the comments on the choice /Users/rubber/linux/block/bfq-iosched.c: 5749
	 * of the queue for injection in bfq_select_queue(). /Users/rubber/linux/block/bfq-iosched.c: 5750
	 * /Users/rubber/linux/block/bfq-iosched.c: 5751
	 * As stressed in those comments, if such a synchronization is /Users/rubber/linux/block/bfq-iosched.c: 5752
	 * actually in place, then, without injection on bfqq, the /Users/rubber/linux/block/bfq-iosched.c: 5753
	 * blocking I/O cannot happen to served while bfqq is in /Users/rubber/linux/block/bfq-iosched.c: 5754
	 * service. As a consequence, if bfqq is granted /Users/rubber/linux/block/bfq-iosched.c: 5755
	 * I/O-dispatch-plugging, then bfqq remains empty, and no I/O /Users/rubber/linux/block/bfq-iosched.c: 5756
	 * is dispatched, until the idle timeout fires. This is likely /Users/rubber/linux/block/bfq-iosched.c: 5757
	 * to result in lower bandwidth and higher latencies for bfqq, /Users/rubber/linux/block/bfq-iosched.c: 5758
	 * and in a severe loss of total throughput. /Users/rubber/linux/block/bfq-iosched.c: 5759
	 * /Users/rubber/linux/block/bfq-iosched.c: 5760
	 * On the opposite end, a non-zero inject limit may allow the /Users/rubber/linux/block/bfq-iosched.c: 5761
	 * I/O that blocks bfqq to be executed soon, and therefore /Users/rubber/linux/block/bfq-iosched.c: 5762
	 * bfqq to receive new I/O soon. /Users/rubber/linux/block/bfq-iosched.c: 5763
	 * /Users/rubber/linux/block/bfq-iosched.c: 5764
	 * But, if the blocking gets actually eliminated, then the /Users/rubber/linux/block/bfq-iosched.c: 5765
	 * next think-time sample for bfqq may be very low. This in /Users/rubber/linux/block/bfq-iosched.c: 5766
	 * turn may cause bfqq's think time to be deemed /Users/rubber/linux/block/bfq-iosched.c: 5767
	 * short. Without the 100 ms barrier, this new state change /Users/rubber/linux/block/bfq-iosched.c: 5768
	 * would cause the body of the next if to be executed /Users/rubber/linux/block/bfq-iosched.c: 5769
	 * immediately. But this would set to 0 the inject /Users/rubber/linux/block/bfq-iosched.c: 5770
	 * limit. Without injection, the blocking I/O would cause the /Users/rubber/linux/block/bfq-iosched.c: 5771
	 * think time of bfqq to become long again, and therefore the /Users/rubber/linux/block/bfq-iosched.c: 5772
	 * inject limit to be raised again, and so on. The only effect /Users/rubber/linux/block/bfq-iosched.c: 5773
	 * of such a steady oscillation between the two think-time /Users/rubber/linux/block/bfq-iosched.c: 5774
	 * states would be to prevent effective injection on bfqq. /Users/rubber/linux/block/bfq-iosched.c: 5775
	 * /Users/rubber/linux/block/bfq-iosched.c: 5776
	 * In contrast, if the inject limit is not reset during such a /Users/rubber/linux/block/bfq-iosched.c: 5777
	 * long time interval as 100 ms, then the number of short /Users/rubber/linux/block/bfq-iosched.c: 5778
	 * think time samples can grow significantly before the reset /Users/rubber/linux/block/bfq-iosched.c: 5779
	 * is performed. As a consequence, the think time state can /Users/rubber/linux/block/bfq-iosched.c: 5780
	 * become stable before the reset. Therefore there will be no /Users/rubber/linux/block/bfq-iosched.c: 5781
	 * state change when the 100 ms elapse, and no reset of the /Users/rubber/linux/block/bfq-iosched.c: 5782
	 * inject limit. The inject limit remains steadily equal to 1 /Users/rubber/linux/block/bfq-iosched.c: 5783
	 * both during and after the 100 ms. So injection can be /Users/rubber/linux/block/bfq-iosched.c: 5784
	 * performed at all times, and throughput gets boosted. /Users/rubber/linux/block/bfq-iosched.c: 5785
	 * /Users/rubber/linux/block/bfq-iosched.c: 5786
	 * An inject limit equal to 1 is however in conflict, in /Users/rubber/linux/block/bfq-iosched.c: 5787
	 * general, with the fact that the think time of bfqq is /Users/rubber/linux/block/bfq-iosched.c: 5788
	 * short, because injection may be likely to delay bfqq's I/O /Users/rubber/linux/block/bfq-iosched.c: 5789
	 * (as explained in the comments in /Users/rubber/linux/block/bfq-iosched.c: 5790
	 * bfq_update_inject_limit()). But this does not happen in /Users/rubber/linux/block/bfq-iosched.c: 5791
	 * this special case, because bfqq's low think time is due to /Users/rubber/linux/block/bfq-iosched.c: 5792
	 * an effective handling of a synchronization, through /Users/rubber/linux/block/bfq-iosched.c: 5793
	 * injection. In this special case, bfqq's I/O does not get /Users/rubber/linux/block/bfq-iosched.c: 5794
	 * delayed by injection; on the contrary, bfqq's I/O is /Users/rubber/linux/block/bfq-iosched.c: 5795
	 * brought forward, because it is not blocked for /Users/rubber/linux/block/bfq-iosched.c: 5796
	 * milliseconds. /Users/rubber/linux/block/bfq-iosched.c: 5797
	 * /Users/rubber/linux/block/bfq-iosched.c: 5798
	 * In addition, serving the blocking I/O much sooner, and much /Users/rubber/linux/block/bfq-iosched.c: 5799
	 * more frequently than once per I/O-plugging timeout, makes /Users/rubber/linux/block/bfq-iosched.c: 5800
	 * it much quicker to detect a waker queue (the concept of /Users/rubber/linux/block/bfq-iosched.c: 5801
	 * waker queue is defined in the comments in /Users/rubber/linux/block/bfq-iosched.c: 5802
	 * bfq_add_request()). This makes it possible to start sooner /Users/rubber/linux/block/bfq-iosched.c: 5803
	 * to boost throughput more effectively, by injecting the I/O /Users/rubber/linux/block/bfq-iosched.c: 5804
	 * of the waker queue unconditionally on every /Users/rubber/linux/block/bfq-iosched.c: 5805
	 * bfq_dispatch_request(). /Users/rubber/linux/block/bfq-iosched.c: 5806
	 * /Users/rubber/linux/block/bfq-iosched.c: 5807
	 * One last, important benefit of not resetting the inject /Users/rubber/linux/block/bfq-iosched.c: 5808
	 * limit before 100 ms is that, during this time interval, the /Users/rubber/linux/block/bfq-iosched.c: 5809
	 * base value for the total service time is likely to get /Users/rubber/linux/block/bfq-iosched.c: 5810
	 * finally computed for bfqq, freeing the inject limit from /Users/rubber/linux/block/bfq-iosched.c: 5811
	 * its relation with the think time. /Users/rubber/linux/block/bfq-iosched.c: 5812
 * Called when a new fs request (rq) is added to bfqq.  Check if there's /Users/rubber/linux/block/bfq-iosched.c: 5822
 * something we should do about it. /Users/rubber/linux/block/bfq-iosched.c: 5823
		/* /Users/rubber/linux/block/bfq-iosched.c: 5838
		 * There is just this request queued: if /Users/rubber/linux/block/bfq-iosched.c: 5839
		 * - the request is small, and /Users/rubber/linux/block/bfq-iosched.c: 5840
		 * - we are idling to boost throughput, and /Users/rubber/linux/block/bfq-iosched.c: 5841
		 * - the queue is not to be expired, /Users/rubber/linux/block/bfq-iosched.c: 5842
		 * then just exit. /Users/rubber/linux/block/bfq-iosched.c: 5843
		 * /Users/rubber/linux/block/bfq-iosched.c: 5844
		 * In this way, if the device is being idled to wait /Users/rubber/linux/block/bfq-iosched.c: 5845
		 * for a new request from the in-service queue, we /Users/rubber/linux/block/bfq-iosched.c: 5846
		 * avoid unplugging the device and committing the /Users/rubber/linux/block/bfq-iosched.c: 5847
		 * device to serve just a small request. In contrast /Users/rubber/linux/block/bfq-iosched.c: 5848
		 * we wait for the block layer to decide when to /Users/rubber/linux/block/bfq-iosched.c: 5849
		 * unplug the device: hopefully, new requests will be /Users/rubber/linux/block/bfq-iosched.c: 5850
		 * merged to this one quickly, then the device will be /Users/rubber/linux/block/bfq-iosched.c: 5851
		 * unplugged and larger requests will be dispatched. /Users/rubber/linux/block/bfq-iosched.c: 5852
		/* /Users/rubber/linux/block/bfq-iosched.c: 5858
		 * A large enough request arrived, or idling is being /Users/rubber/linux/block/bfq-iosched.c: 5859
		 * performed to preserve service guarantees, or /Users/rubber/linux/block/bfq-iosched.c: 5860
		 * finally the queue is to be expired: in all these /Users/rubber/linux/block/bfq-iosched.c: 5861
		 * cases disk idling is to be stopped, so clear /Users/rubber/linux/block/bfq-iosched.c: 5862
		 * wait_request flag and reset timer. /Users/rubber/linux/block/bfq-iosched.c: 5863
		/* /Users/rubber/linux/block/bfq-iosched.c: 5868
		 * The queue is not empty, because a new request just /Users/rubber/linux/block/bfq-iosched.c: 5869
		 * arrived. Hence we can safely expire the queue, in /Users/rubber/linux/block/bfq-iosched.c: 5870
		 * case of budget timeout, without risking that the /Users/rubber/linux/block/bfq-iosched.c: 5871
		 * timestamps of the queue are not updated correctly. /Users/rubber/linux/block/bfq-iosched.c: 5872
		 * See [1] for more details. /Users/rubber/linux/block/bfq-iosched.c: 5873
		/* /Users/rubber/linux/block/bfq-iosched.c: 5890
		 * Release the request's reference to the old bfqq /Users/rubber/linux/block/bfq-iosched.c: 5891
		 * and make sure one is taken to the shared queue. /Users/rubber/linux/block/bfq-iosched.c: 5892
		/* /Users/rubber/linux/block/bfq-iosched.c: 5897
		 * If the bic associated with the process /Users/rubber/linux/block/bfq-iosched.c: 5898
		 * issuing this request still points to bfqq /Users/rubber/linux/block/bfq-iosched.c: 5899
		 * (and thus has not been already redirected /Users/rubber/linux/block/bfq-iosched.c: 5900
		 * to new_bfqq or even some other bfq_queue), /Users/rubber/linux/block/bfq-iosched.c: 5901
		 * then complete the merge and redirect it to /Users/rubber/linux/block/bfq-iosched.c: 5902
		 * new_bfqq. /Users/rubber/linux/block/bfq-iosched.c: 5903
		/* /Users/rubber/linux/block/bfq-iosched.c: 5910
		 * rq is about to be enqueued into new_bfqq, /Users/rubber/linux/block/bfq-iosched.c: 5911
		 * release rq reference on bfqq /Users/rubber/linux/block/bfq-iosched.c: 5912
	/* /Users/rubber/linux/block/bfq-iosched.c: 5944
	 * bfqq still exists, because it can disappear only after /Users/rubber/linux/block/bfq-iosched.c: 5945
	 * either it is merged with another queue, or the process it /Users/rubber/linux/block/bfq-iosched.c: 5946
	 * is associated with exits. But both actions must be taken by /Users/rubber/linux/block/bfq-iosched.c: 5947
	 * the same process currently executing this flow of /Users/rubber/linux/block/bfq-iosched.c: 5948
	 * instructions. /Users/rubber/linux/block/bfq-iosched.c: 5949
	 * /Users/rubber/linux/block/bfq-iosched.c: 5950
	 * In addition, the following queue lock guarantees that /Users/rubber/linux/block/bfq-iosched.c: 5951
	 * bfqq_group(bfqq) exists as well. /Users/rubber/linux/block/bfq-iosched.c: 5952
	/* /Users/rubber/linux/block/bfq-iosched.c: 5995
	 * Reqs with at_head or passthrough flags set are to be put /Users/rubber/linux/block/bfq-iosched.c: 5996
	 * directly into dispatch list. Additional case for putting rq /Users/rubber/linux/block/bfq-iosched.c: 5997
	 * directly into the dispatch queue: the only active /Users/rubber/linux/block/bfq-iosched.c: 5998
	 * bfq_queues are bfqq and either its waker bfq_queue or one /Users/rubber/linux/block/bfq-iosched.c: 5999
	 * of its woken bfq_queues. The rationale behind this /Users/rubber/linux/block/bfq-iosched.c: 6000
	 * additional condition is as follows: /Users/rubber/linux/block/bfq-iosched.c: 6001
	 * - consider a bfq_queue, say Q1, detected as a waker of /Users/rubber/linux/block/bfq-iosched.c: 6002
	 *   another bfq_queue, say Q2 /Users/rubber/linux/block/bfq-iosched.c: 6003
	 * - by definition of a waker, Q1 blocks the I/O of Q2, i.e., /Users/rubber/linux/block/bfq-iosched.c: 6004
	 *   some I/O of Q1 needs to be completed for new I/O of Q2 /Users/rubber/linux/block/bfq-iosched.c: 6005
	 *   to arrive.  A notable example of waker is journald /Users/rubber/linux/block/bfq-iosched.c: 6006
	 * - so, Q1 and Q2 are in any respect the queues of two /Users/rubber/linux/block/bfq-iosched.c: 6007
	 *   cooperating processes (or of two cooperating sets of /Users/rubber/linux/block/bfq-iosched.c: 6008
	 *   processes): the goal of Q1's I/O is doing what needs to /Users/rubber/linux/block/bfq-iosched.c: 6009
	 *   be done so that new Q2's I/O can finally be /Users/rubber/linux/block/bfq-iosched.c: 6010
	 *   issued. Therefore, if the service of Q1's I/O is delayed, /Users/rubber/linux/block/bfq-iosched.c: 6011
	 *   then Q2's I/O is delayed too.  Conversely, if Q2's I/O is /Users/rubber/linux/block/bfq-iosched.c: 6012
	 *   delayed, the goal of Q1's I/O is hindered. /Users/rubber/linux/block/bfq-iosched.c: 6013
	 * - as a consequence, if some I/O of Q1/Q2 arrives while /Users/rubber/linux/block/bfq-iosched.c: 6014
	 *   Q2/Q1 is the only queue in service, there is absolutely /Users/rubber/linux/block/bfq-iosched.c: 6015
	 *   no point in delaying the service of such an I/O. The /Users/rubber/linux/block/bfq-iosched.c: 6016
	 *   only possible result is a throughput loss /Users/rubber/linux/block/bfq-iosched.c: 6017
	 * - so, when the above condition holds, the best option is to /Users/rubber/linux/block/bfq-iosched.c: 6018
	 *   have the new I/O dispatched as soon as possible /Users/rubber/linux/block/bfq-iosched.c: 6019
	 * - the most effective and efficient way to attain the above /Users/rubber/linux/block/bfq-iosched.c: 6020
	 *   goal is to put the new I/O directly in the dispatch /Users/rubber/linux/block/bfq-iosched.c: 6021
	 *   list /Users/rubber/linux/block/bfq-iosched.c: 6022
	 * - as an additional restriction, Q1 and Q2 must be the only /Users/rubber/linux/block/bfq-iosched.c: 6023
	 *   busy queues for this commit to put the I/O of Q2/Q1 in /Users/rubber/linux/block/bfq-iosched.c: 6024
	 *   the dispatch list.  This is necessary, because, if also /Users/rubber/linux/block/bfq-iosched.c: 6025
	 *   other queues are waiting for service, then putting new /Users/rubber/linux/block/bfq-iosched.c: 6026
	 *   I/O directly in the dispatch list may evidently cause a /Users/rubber/linux/block/bfq-iosched.c: 6027
	 *   violation of service guarantees for the other queues /Users/rubber/linux/block/bfq-iosched.c: 6028
		/* /Users/rubber/linux/block/bfq-iosched.c: 6042
		 * Update bfqq, because, if a queue merge has occurred /Users/rubber/linux/block/bfq-iosched.c: 6043
		 * in __bfq_insert_request, then rq has been /Users/rubber/linux/block/bfq-iosched.c: 6044
		 * redirected into a new queue. /Users/rubber/linux/block/bfq-iosched.c: 6045
	/* /Users/rubber/linux/block/bfq-iosched.c: 6056
	 * Cache cmd_flags before releasing scheduler lock, because rq /Users/rubber/linux/block/bfq-iosched.c: 6057
	 * may disappear afterwards (for example, because of a request /Users/rubber/linux/block/bfq-iosched.c: 6058
	 * merge). /Users/rubber/linux/block/bfq-iosched.c: 6059
	/* /Users/rubber/linux/block/bfq-iosched.c: 6091
	 * This sample is valid if the number of outstanding requests /Users/rubber/linux/block/bfq-iosched.c: 6092
	 * is large enough to allow a queueing behavior.  Note that the /Users/rubber/linux/block/bfq-iosched.c: 6093
	 * sum is not exact, as it's not taking into account deactivated /Users/rubber/linux/block/bfq-iosched.c: 6094
	 * requests. /Users/rubber/linux/block/bfq-iosched.c: 6095
	/* /Users/rubber/linux/block/bfq-iosched.c: 6100
	 * If active queue hasn't enough requests and can idle, bfq might not /Users/rubber/linux/block/bfq-iosched.c: 6101
	 * dispatch sufficient requests to hardware. Don't zero hw_tag in this /Users/rubber/linux/block/bfq-iosched.c: 6102
	 * case /Users/rubber/linux/block/bfq-iosched.c: 6103
		/* /Users/rubber/linux/block/bfq-iosched.c: 6133
		 * Set budget_timeout (which we overload to store the /Users/rubber/linux/block/bfq-iosched.c: 6134
		 * time at which the queue remains with no backlog and /Users/rubber/linux/block/bfq-iosched.c: 6135
		 * no outstanding request; used by the weight-raising /Users/rubber/linux/block/bfq-iosched.c: 6136
		 * mechanism). /Users/rubber/linux/block/bfq-iosched.c: 6137
	/* /Users/rubber/linux/block/bfq-iosched.c: 6148
	 * Using us instead of ns, to get a reasonable precision in /Users/rubber/linux/block/bfq-iosched.c: 6149
	 * computing rate in next check. /Users/rubber/linux/block/bfq-iosched.c: 6150
	/* /Users/rubber/linux/block/bfq-iosched.c: 6154
	 * If the request took rather long to complete, and, according /Users/rubber/linux/block/bfq-iosched.c: 6155
	 * to the maximum request size recorded, this completion latency /Users/rubber/linux/block/bfq-iosched.c: 6156
	 * implies that the request was certainly served at a very low /Users/rubber/linux/block/bfq-iosched.c: 6157
	 * rate (less than 1M sectors/sec), then the whole observation /Users/rubber/linux/block/bfq-iosched.c: 6158
	 * interval that lasts up to this time instant cannot be a /Users/rubber/linux/block/bfq-iosched.c: 6159
	 * valid time interval for computing a new peak rate.  Invoke /Users/rubber/linux/block/bfq-iosched.c: 6160
	 * bfq_update_rate_reset to have the following three steps /Users/rubber/linux/block/bfq-iosched.c: 6161
	 * taken: /Users/rubber/linux/block/bfq-iosched.c: 6162
	 * - close the observation interval at the last (previous) /Users/rubber/linux/block/bfq-iosched.c: 6163
	 *   request dispatch or completion /Users/rubber/linux/block/bfq-iosched.c: 6164
	 * - compute rate, if possible, for that observation interval /Users/rubber/linux/block/bfq-iosched.c: 6165
	 * - reset to zero samples, which will trigger a proper /Users/rubber/linux/block/bfq-iosched.c: 6166
	 *   re-initialization of the observation interval on next /Users/rubber/linux/block/bfq-iosched.c: 6167
	 *   dispatch /Users/rubber/linux/block/bfq-iosched.c: 6168
	/* /Users/rubber/linux/block/bfq-iosched.c: 6175
	 * Shared queues are likely to receive I/O at a high /Users/rubber/linux/block/bfq-iosched.c: 6176
	 * rate. This may deceptively let them be considered as wakers /Users/rubber/linux/block/bfq-iosched.c: 6177
	 * of other queues. But a false waker will unjustly steal /Users/rubber/linux/block/bfq-iosched.c: 6178
	 * bandwidth to its supposedly woken queue. So considering /Users/rubber/linux/block/bfq-iosched.c: 6179
	 * also shared queues in the waking mechanism may cause more /Users/rubber/linux/block/bfq-iosched.c: 6180
	 * control troubles than throughput benefits. Then reset /Users/rubber/linux/block/bfq-iosched.c: 6181
	 * last_completed_rq_bfqq if bfqq is a shared queue. /Users/rubber/linux/block/bfq-iosched.c: 6182
	/* /Users/rubber/linux/block/bfq-iosched.c: 6189
	 * If we are waiting to discover whether the request pattern /Users/rubber/linux/block/bfq-iosched.c: 6190
	 * of the task associated with the queue is actually /Users/rubber/linux/block/bfq-iosched.c: 6191
	 * isochronous, and both requisites for this condition to hold /Users/rubber/linux/block/bfq-iosched.c: 6192
	 * are now satisfied, then compute soft_rt_next_start (see the /Users/rubber/linux/block/bfq-iosched.c: 6193
	 * comments on the function bfq_bfqq_softrt_next_start()). We /Users/rubber/linux/block/bfq-iosched.c: 6194
	 * do not compute soft_rt_next_start if bfqq is in interactive /Users/rubber/linux/block/bfq-iosched.c: 6195
	 * weight raising (see the comments in bfq_bfqq_expire() for /Users/rubber/linux/block/bfq-iosched.c: 6196
	 * an explanation). We schedule this delayed update when bfqq /Users/rubber/linux/block/bfq-iosched.c: 6197
	 * expires, if it still has in-flight requests. /Users/rubber/linux/block/bfq-iosched.c: 6198
	/* /Users/rubber/linux/block/bfq-iosched.c: 6206
	 * If this is the in-service queue, check if it needs to be expired, /Users/rubber/linux/block/bfq-iosched.c: 6207
	 * or if we want to idle in case it has no pending requests. /Users/rubber/linux/block/bfq-iosched.c: 6208
			/* /Users/rubber/linux/block/bfq-iosched.c: 6214
			 * If we get here, we do not expire bfqq, even /Users/rubber/linux/block/bfq-iosched.c: 6215
			 * if bfqq was in budget timeout or had no /Users/rubber/linux/block/bfq-iosched.c: 6216
			 * more requests (as controlled in the next /Users/rubber/linux/block/bfq-iosched.c: 6217
			 * conditional instructions). The reason for /Users/rubber/linux/block/bfq-iosched.c: 6218
			 * not expiring bfqq is as follows. /Users/rubber/linux/block/bfq-iosched.c: 6219
			 * /Users/rubber/linux/block/bfq-iosched.c: 6220
			 * Here bfqq->dispatched > 0 holds, but /Users/rubber/linux/block/bfq-iosched.c: 6221
			 * bfq_bfqq_must_idle() returned true. This /Users/rubber/linux/block/bfq-iosched.c: 6222
			 * implies that, even if no request arrives /Users/rubber/linux/block/bfq-iosched.c: 6223
			 * for bfqq before bfqq->dispatched reaches 0, /Users/rubber/linux/block/bfq-iosched.c: 6224
			 * bfqq will, however, not be expired on the /Users/rubber/linux/block/bfq-iosched.c: 6225
			 * completion event that causes bfqq->dispatch /Users/rubber/linux/block/bfq-iosched.c: 6226
			 * to reach zero. In contrast, on this event, /Users/rubber/linux/block/bfq-iosched.c: 6227
			 * bfqq will start enjoying device idling /Users/rubber/linux/block/bfq-iosched.c: 6228
			 * (I/O-dispatch plugging). /Users/rubber/linux/block/bfq-iosched.c: 6229
			 * /Users/rubber/linux/block/bfq-iosched.c: 6230
			 * But, if we expired bfqq here, bfqq would /Users/rubber/linux/block/bfq-iosched.c: 6231
			 * not have the chance to enjoy device idling /Users/rubber/linux/block/bfq-iosched.c: 6232
			 * when bfqq->dispatched finally reaches /Users/rubber/linux/block/bfq-iosched.c: 6233
			 * zero. This would expose bfqq to violation /Users/rubber/linux/block/bfq-iosched.c: 6234
			 * of its reserved service guarantees. /Users/rubber/linux/block/bfq-iosched.c: 6235
 * The processes associated with bfqq may happen to generate their /Users/rubber/linux/block/bfq-iosched.c: 6260
 * cumulative I/O at a lower rate than the rate at which the device /Users/rubber/linux/block/bfq-iosched.c: 6261
 * could serve the same I/O. This is rather probable, e.g., if only /Users/rubber/linux/block/bfq-iosched.c: 6262
 * one process is associated with bfqq and the device is an SSD. It /Users/rubber/linux/block/bfq-iosched.c: 6263
 * results in bfqq becoming often empty while in service. In this /Users/rubber/linux/block/bfq-iosched.c: 6264
 * respect, if BFQ is allowed to switch to another queue when bfqq /Users/rubber/linux/block/bfq-iosched.c: 6265
 * remains empty, then the device goes on being fed with I/O requests, /Users/rubber/linux/block/bfq-iosched.c: 6266
 * and the throughput is not affected. In contrast, if BFQ is not /Users/rubber/linux/block/bfq-iosched.c: 6267
 * allowed to switch to another queue---because bfqq is sync and /Users/rubber/linux/block/bfq-iosched.c: 6268
 * I/O-dispatch needs to be plugged while bfqq is temporarily /Users/rubber/linux/block/bfq-iosched.c: 6269
 * empty---then, during the service of bfqq, there will be frequent /Users/rubber/linux/block/bfq-iosched.c: 6270
 * "service holes", i.e., time intervals during which bfqq gets empty /Users/rubber/linux/block/bfq-iosched.c: 6271
 * and the device can only consume the I/O already queued in its /Users/rubber/linux/block/bfq-iosched.c: 6272
 * hardware queues. During service holes, the device may even get to /Users/rubber/linux/block/bfq-iosched.c: 6273
 * remaining idle. In the end, during the service of bfqq, the device /Users/rubber/linux/block/bfq-iosched.c: 6274
 * is driven at a lower speed than the one it can reach with the kind /Users/rubber/linux/block/bfq-iosched.c: 6275
 * of I/O flowing through bfqq. /Users/rubber/linux/block/bfq-iosched.c: 6276
 * To counter this loss of throughput, BFQ implements a "request /Users/rubber/linux/block/bfq-iosched.c: 6278
 * injection mechanism", which tries to fill the above service holes /Users/rubber/linux/block/bfq-iosched.c: 6279
 * with I/O requests taken from other queues. The hard part in this /Users/rubber/linux/block/bfq-iosched.c: 6280
 * mechanism is finding the right amount of I/O to inject, so as to /Users/rubber/linux/block/bfq-iosched.c: 6281
 * both boost throughput and not break bfqq's bandwidth and latency /Users/rubber/linux/block/bfq-iosched.c: 6282
 * guarantees. In this respect, the mechanism maintains a per-queue /Users/rubber/linux/block/bfq-iosched.c: 6283
 * inject limit, computed as below. While bfqq is empty, the injection /Users/rubber/linux/block/bfq-iosched.c: 6284
 * mechanism dispatches extra I/O requests only until the total number /Users/rubber/linux/block/bfq-iosched.c: 6285
 * of I/O requests in flight---i.e., already dispatched but not yet /Users/rubber/linux/block/bfq-iosched.c: 6286
 * completed---remains lower than this limit. /Users/rubber/linux/block/bfq-iosched.c: 6287
 * A first definition comes in handy to introduce the algorithm by /Users/rubber/linux/block/bfq-iosched.c: 6289
 * which the inject limit is computed.  We define as first request for /Users/rubber/linux/block/bfq-iosched.c: 6290
 * bfqq, an I/O request for bfqq that arrives while bfqq is in /Users/rubber/linux/block/bfq-iosched.c: 6291
 * service, and causes bfqq to switch from empty to non-empty. The /Users/rubber/linux/block/bfq-iosched.c: 6292
 * algorithm updates the limit as a function of the effect of /Users/rubber/linux/block/bfq-iosched.c: 6293
 * injection on the service times of only the first requests of /Users/rubber/linux/block/bfq-iosched.c: 6294
 * bfqq. The reason for this restriction is that these are the /Users/rubber/linux/block/bfq-iosched.c: 6295
 * requests whose service time is affected most, because they are the /Users/rubber/linux/block/bfq-iosched.c: 6296
 * first to arrive after injection possibly occurred. /Users/rubber/linux/block/bfq-iosched.c: 6297
 * To evaluate the effect of injection, the algorithm measures the /Users/rubber/linux/block/bfq-iosched.c: 6299
 * "total service time" of first requests. We define as total service /Users/rubber/linux/block/bfq-iosched.c: 6300
 * time of an I/O request, the time that elapses since when the /Users/rubber/linux/block/bfq-iosched.c: 6301
 * request is enqueued into bfqq, to when it is completed. This /Users/rubber/linux/block/bfq-iosched.c: 6302
 * quantity allows the whole effect of injection to be measured. It is /Users/rubber/linux/block/bfq-iosched.c: 6303
 * easy to see why. Suppose that some requests of other queues are /Users/rubber/linux/block/bfq-iosched.c: 6304
 * actually injected while bfqq is empty, and that a new request R /Users/rubber/linux/block/bfq-iosched.c: 6305
 * then arrives for bfqq. If the device does start to serve all or /Users/rubber/linux/block/bfq-iosched.c: 6306
 * part of the injected requests during the service hole, then, /Users/rubber/linux/block/bfq-iosched.c: 6307
 * because of this extra service, it may delay the next invocation of /Users/rubber/linux/block/bfq-iosched.c: 6308
 * the dispatch hook of BFQ. Then, even after R gets eventually /Users/rubber/linux/block/bfq-iosched.c: 6309
 * dispatched, the device may delay the actual service of R if it is /Users/rubber/linux/block/bfq-iosched.c: 6310
 * still busy serving the extra requests, or if it decides to serve, /Users/rubber/linux/block/bfq-iosched.c: 6311
 * before R, some extra request still present in its queues. As a /Users/rubber/linux/block/bfq-iosched.c: 6312
 * conclusion, the cumulative extra delay caused by injection can be /Users/rubber/linux/block/bfq-iosched.c: 6313
 * easily evaluated by just comparing the total service time of first /Users/rubber/linux/block/bfq-iosched.c: 6314
 * requests with and without injection. /Users/rubber/linux/block/bfq-iosched.c: 6315
 * The limit-update algorithm works as follows. On the arrival of a /Users/rubber/linux/block/bfq-iosched.c: 6317
 * first request of bfqq, the algorithm measures the total time of the /Users/rubber/linux/block/bfq-iosched.c: 6318
 * request only if one of the three cases below holds, and, for each /Users/rubber/linux/block/bfq-iosched.c: 6319
 * case, it updates the limit as described below: /Users/rubber/linux/block/bfq-iosched.c: 6320
 * (1) If there is no in-flight request. This gives a baseline for the /Users/rubber/linux/block/bfq-iosched.c: 6322
 *     total service time of the requests of bfqq. If the baseline has /Users/rubber/linux/block/bfq-iosched.c: 6323
 *     not been computed yet, then, after computing it, the limit is /Users/rubber/linux/block/bfq-iosched.c: 6324
 *     set to 1, to start boosting throughput, and to prepare the /Users/rubber/linux/block/bfq-iosched.c: 6325
 *     ground for the next case. If the baseline has already been /Users/rubber/linux/block/bfq-iosched.c: 6326
 *     computed, then it is updated, in case it results to be lower /Users/rubber/linux/block/bfq-iosched.c: 6327
 *     than the previous value. /Users/rubber/linux/block/bfq-iosched.c: 6328
 * (2) If the limit is higher than 0 and there are in-flight /Users/rubber/linux/block/bfq-iosched.c: 6330
 *     requests. By comparing the total service time in this case with /Users/rubber/linux/block/bfq-iosched.c: 6331
 *     the above baseline, it is possible to know at which extent the /Users/rubber/linux/block/bfq-iosched.c: 6332
 *     current value of the limit is inflating the total service /Users/rubber/linux/block/bfq-iosched.c: 6333
 *     time. If the inflation is below a certain threshold, then bfqq /Users/rubber/linux/block/bfq-iosched.c: 6334
 *     is assumed to be suffering from no perceivable loss of its /Users/rubber/linux/block/bfq-iosched.c: 6335
 *     service guarantees, and the limit is even tentatively /Users/rubber/linux/block/bfq-iosched.c: 6336
 *     increased. If the inflation is above the threshold, then the /Users/rubber/linux/block/bfq-iosched.c: 6337
 *     limit is decreased. Due to the lack of any hysteresis, this /Users/rubber/linux/block/bfq-iosched.c: 6338
 *     logic makes the limit oscillate even in steady workload /Users/rubber/linux/block/bfq-iosched.c: 6339
 *     conditions. Yet we opted for it, because it is fast in reaching /Users/rubber/linux/block/bfq-iosched.c: 6340
 *     the best value for the limit, as a function of the current I/O /Users/rubber/linux/block/bfq-iosched.c: 6341
 *     workload. To reduce oscillations, this step is disabled for a /Users/rubber/linux/block/bfq-iosched.c: 6342
 *     short time interval after the limit happens to be decreased. /Users/rubber/linux/block/bfq-iosched.c: 6343
 * (3) Periodically, after resetting the limit, to make sure that the /Users/rubber/linux/block/bfq-iosched.c: 6345
 *     limit eventually drops in case the workload changes. This is /Users/rubber/linux/block/bfq-iosched.c: 6346
 *     needed because, after the limit has gone safely up for a /Users/rubber/linux/block/bfq-iosched.c: 6347
 *     certain workload, it is impossible to guess whether the /Users/rubber/linux/block/bfq-iosched.c: 6348
 *     baseline total service time may have changed, without measuring /Users/rubber/linux/block/bfq-iosched.c: 6349
 *     it again without injection. A more effective version of this /Users/rubber/linux/block/bfq-iosched.c: 6350
 *     step might be to just sample the baseline, by interrupting /Users/rubber/linux/block/bfq-iosched.c: 6351
 *     injection only once, and then to reset/lower the limit only if /Users/rubber/linux/block/bfq-iosched.c: 6352
 *     the total service time with the current limit does happen to be /Users/rubber/linux/block/bfq-iosched.c: 6353
 *     too large. /Users/rubber/linux/block/bfq-iosched.c: 6354
 * More details on each step are provided in the comments on the /Users/rubber/linux/block/bfq-iosched.c: 6356
 * pieces of code that implement these steps: the branch handling the /Users/rubber/linux/block/bfq-iosched.c: 6357
 * transition from empty to non empty in bfq_add_request(), the branch /Users/rubber/linux/block/bfq-iosched.c: 6358
 * handling injection in bfq_select_queue(), and the function /Users/rubber/linux/block/bfq-iosched.c: 6359
 * bfq_choose_bfqq_for_injection(). These comments also explain some /Users/rubber/linux/block/bfq-iosched.c: 6360
 * exceptions, made by the injection mechanism in some special cases. /Users/rubber/linux/block/bfq-iosched.c: 6361
	/* /Users/rubber/linux/block/bfq-iosched.c: 6380
	 * Either we still have to compute the base value for the /Users/rubber/linux/block/bfq-iosched.c: 6381
	 * total service time, and there seem to be the right /Users/rubber/linux/block/bfq-iosched.c: 6382
	 * conditions to do it, or we can lower the last base value /Users/rubber/linux/block/bfq-iosched.c: 6383
	 * computed. /Users/rubber/linux/block/bfq-iosched.c: 6384
	 * /Users/rubber/linux/block/bfq-iosched.c: 6385
	 * NOTE: (bfqd->rq_in_driver == 1) means that there is no I/O /Users/rubber/linux/block/bfq-iosched.c: 6386
	 * request in flight, because this function is in the code /Users/rubber/linux/block/bfq-iosched.c: 6387
	 * path that handles the completion of a request of bfqq, and, /Users/rubber/linux/block/bfq-iosched.c: 6388
	 * in particular, this function is executed before /Users/rubber/linux/block/bfq-iosched.c: 6389
	 * bfqd->rq_in_driver is decremented in such a code path. /Users/rubber/linux/block/bfq-iosched.c: 6390
			/* /Users/rubber/linux/block/bfq-iosched.c: 6395
			 * Now we certainly have a base value: make sure we /Users/rubber/linux/block/bfq-iosched.c: 6396
			 * start trying injection. /Users/rubber/linux/block/bfq-iosched.c: 6397
		/* /Users/rubber/linux/block/bfq-iosched.c: 6403
		 * No I/O injected and no request still in service in /Users/rubber/linux/block/bfq-iosched.c: 6404
		 * the drive: these are the exact conditions for /Users/rubber/linux/block/bfq-iosched.c: 6405
		 * computing the base value of the total service time /Users/rubber/linux/block/bfq-iosched.c: 6406
		 * for bfqq. So let's update this value, because it is /Users/rubber/linux/block/bfq-iosched.c: 6407
		 * rather variable. For example, it varies if the size /Users/rubber/linux/block/bfq-iosched.c: 6408
		 * or the spatial locality of the I/O requests in bfqq /Users/rubber/linux/block/bfq-iosched.c: 6409
		 * change. /Users/rubber/linux/block/bfq-iosched.c: 6410
 * Handle either a requeue or a finish for rq. The things to do are /Users/rubber/linux/block/bfq-iosched.c: 6421
 * the same in both cases: all references to rq are to be dropped. In /Users/rubber/linux/block/bfq-iosched.c: 6422
 * particular, rq is considered completed from the point of view of /Users/rubber/linux/block/bfq-iosched.c: 6423
 * the scheduler. /Users/rubber/linux/block/bfq-iosched.c: 6424
	/* /Users/rubber/linux/block/bfq-iosched.c: 6432
	 * rq either is not associated with any icq, or is an already /Users/rubber/linux/block/bfq-iosched.c: 6433
	 * requeued request that has not (yet) been re-inserted into /Users/rubber/linux/block/bfq-iosched.c: 6434
	 * a bfq_queue. /Users/rubber/linux/block/bfq-iosched.c: 6435
	/* /Users/rubber/linux/block/bfq-iosched.c: 6458
	 * Reset private fields. In case of a requeue, this allows /Users/rubber/linux/block/bfq-iosched.c: 6459
	 * this function to correctly do nothing if it is spuriously /Users/rubber/linux/block/bfq-iosched.c: 6460
	 * invoked again on this same request (see the check at the /Users/rubber/linux/block/bfq-iosched.c: 6461
	 * beginning of the function). Probably, a better general /Users/rubber/linux/block/bfq-iosched.c: 6462
	 * design would be to prevent blk-mq from invoking the requeue /Users/rubber/linux/block/bfq-iosched.c: 6463
	 * or finish hooks of an elevator, for a request that is not /Users/rubber/linux/block/bfq-iosched.c: 6464
	 * referred by that elevator. /Users/rubber/linux/block/bfq-iosched.c: 6465
	 * /Users/rubber/linux/block/bfq-iosched.c: 6466
	 * Resetting the following fields would break the /Users/rubber/linux/block/bfq-iosched.c: 6467
	 * request-insertion logic if rq is re-inserted into a bfq /Users/rubber/linux/block/bfq-iosched.c: 6468
	 * internal queue, without a re-preparation. Here we assume /Users/rubber/linux/block/bfq-iosched.c: 6469
	 * that re-insertions of requeued requests, without /Users/rubber/linux/block/bfq-iosched.c: 6470
	 * re-preparation, can happen only for pass_through or at_head /Users/rubber/linux/block/bfq-iosched.c: 6471
	 * requests (which are not re-inserted into bfq internal /Users/rubber/linux/block/bfq-iosched.c: 6472
	 * queues). /Users/rubber/linux/block/bfq-iosched.c: 6473
 * Removes the association between the current task and bfqq, assuming /Users/rubber/linux/block/bfq-iosched.c: 6480
 * that bic points to the bfq iocontext of the task. /Users/rubber/linux/block/bfq-iosched.c: 6481
 * Returns NULL if a new bfqq should be allocated, or the old bfqq if this /Users/rubber/linux/block/bfq-iosched.c: 6482
 * was the last process referring to that bfqq. /Users/rubber/linux/block/bfq-iosched.c: 6483
				/* /Users/rubber/linux/block/bfq-iosched.c: 6531
				 * If bfqq was in the current /Users/rubber/linux/block/bfq-iosched.c: 6532
				 * burst list before being /Users/rubber/linux/block/bfq-iosched.c: 6533
				 * merged, then we have to add /Users/rubber/linux/block/bfq-iosched.c: 6534
				 * it back. And we do not need /Users/rubber/linux/block/bfq-iosched.c: 6535
				 * to increase burst_size, as /Users/rubber/linux/block/bfq-iosched.c: 6536
				 * we did not decrement /Users/rubber/linux/block/bfq-iosched.c: 6537
				 * burst_size when we removed /Users/rubber/linux/block/bfq-iosched.c: 6538
				 * bfqq from the burst list as /Users/rubber/linux/block/bfq-iosched.c: 6539
				 * a consequence of a merge /Users/rubber/linux/block/bfq-iosched.c: 6540
				 * (see comments in /Users/rubber/linux/block/bfq-iosched.c: 6541
				 * bfq_put_queue). In this /Users/rubber/linux/block/bfq-iosched.c: 6542
				 * respect, it would be rather /Users/rubber/linux/block/bfq-iosched.c: 6543
				 * costly to know whether the /Users/rubber/linux/block/bfq-iosched.c: 6544
				 * current burst list is still /Users/rubber/linux/block/bfq-iosched.c: 6545
				 * the same burst list from /Users/rubber/linux/block/bfq-iosched.c: 6546
				 * which bfqq was removed on /Users/rubber/linux/block/bfq-iosched.c: 6547
				 * the merge. To avoid this /Users/rubber/linux/block/bfq-iosched.c: 6548
				 * cost, if bfqq was in a /Users/rubber/linux/block/bfq-iosched.c: 6549
				 * burst list, then we add /Users/rubber/linux/block/bfq-iosched.c: 6550
				 * bfqq to the current burst /Users/rubber/linux/block/bfq-iosched.c: 6551
				 * list without any further /Users/rubber/linux/block/bfq-iosched.c: 6552
				 * check. This can cause /Users/rubber/linux/block/bfq-iosched.c: 6553
				 * inappropriate insertions, /Users/rubber/linux/block/bfq-iosched.c: 6554
				 * but rarely enough to not /Users/rubber/linux/block/bfq-iosched.c: 6555
				 * harm the detection of large /Users/rubber/linux/block/bfq-iosched.c: 6556
				 * bursts significantly. /Users/rubber/linux/block/bfq-iosched.c: 6557
 * Only reset private fields. The actual request preparation will be /Users/rubber/linux/block/bfq-iosched.c: 6569
 * performed by bfq_init_rq, when rq is either inserted or merged. See /Users/rubber/linux/block/bfq-iosched.c: 6570
 * comments on bfq_init_rq for the reason behind this delayed /Users/rubber/linux/block/bfq-iosched.c: 6571
 * preparation. /Users/rubber/linux/block/bfq-iosched.c: 6572
	/* /Users/rubber/linux/block/bfq-iosched.c: 6576
	 * Regardless of whether we have an icq attached, we have to /Users/rubber/linux/block/bfq-iosched.c: 6577
	 * clear the scheduler pointers, as they might point to /Users/rubber/linux/block/bfq-iosched.c: 6578
	 * previously allocated bic/bfqq structs. /Users/rubber/linux/block/bfq-iosched.c: 6579
 * If needed, init rq, allocate bfq data structures associated with /Users/rubber/linux/block/bfq-iosched.c: 6585
 * rq, and increment reference counters in the destination bfq_queue /Users/rubber/linux/block/bfq-iosched.c: 6586
 * for rq. Return the destination bfq_queue for rq, or NULL is rq is /Users/rubber/linux/block/bfq-iosched.c: 6587
 * not associated with any bfq_queue. /Users/rubber/linux/block/bfq-iosched.c: 6588
 * This function is invoked by the functions that perform rq insertion /Users/rubber/linux/block/bfq-iosched.c: 6590
 * or merging. One may have expected the above preparation operations /Users/rubber/linux/block/bfq-iosched.c: 6591
 * to be performed in bfq_prepare_request, and not delayed to when rq /Users/rubber/linux/block/bfq-iosched.c: 6592
 * is inserted or merged. The rationale behind this delayed /Users/rubber/linux/block/bfq-iosched.c: 6593
 * preparation is that, after the prepare_request hook is invoked for /Users/rubber/linux/block/bfq-iosched.c: 6594
 * rq, rq may still be transformed into a request with no icq, i.e., a /Users/rubber/linux/block/bfq-iosched.c: 6595
 * request not associated with any queue. No bfq hook is invoked to /Users/rubber/linux/block/bfq-iosched.c: 6596
 * signal this transformation. As a consequence, should these /Users/rubber/linux/block/bfq-iosched.c: 6597
 * preparation operations be performed when the prepare_request hook /Users/rubber/linux/block/bfq-iosched.c: 6598
 * is invoked, and should rq be transformed one moment later, bfq /Users/rubber/linux/block/bfq-iosched.c: 6599
 * would end up in an inconsistent state, because it would have /Users/rubber/linux/block/bfq-iosched.c: 6600
 * incremented some queue counters for an rq destined to /Users/rubber/linux/block/bfq-iosched.c: 6601
 * transformation, without any chance to correctly lower these /Users/rubber/linux/block/bfq-iosched.c: 6602
 * counters back. In contrast, no transformation can still happen for /Users/rubber/linux/block/bfq-iosched.c: 6603
 * rq after rq has been inserted or merged. So, it is safe to execute /Users/rubber/linux/block/bfq-iosched.c: 6604
 * these preparation operations when rq is finally inserted or merged. /Users/rubber/linux/block/bfq-iosched.c: 6605
	/* /Users/rubber/linux/block/bfq-iosched.c: 6621
	 * Assuming that elv.priv[1] is set only if everything is set /Users/rubber/linux/block/bfq-iosched.c: 6622
	 * for this rq. This holds true, because this function is /Users/rubber/linux/block/bfq-iosched.c: 6623
	 * invoked only for insertion or merging, and, after such /Users/rubber/linux/block/bfq-iosched.c: 6624
	 * events, a request cannot be manipulated any longer before /Users/rubber/linux/block/bfq-iosched.c: 6625
	 * being removed from bfq. /Users/rubber/linux/block/bfq-iosched.c: 6626
				/* /Users/rubber/linux/block/bfq-iosched.c: 6660
				 * If the waker queue disappears, then /Users/rubber/linux/block/bfq-iosched.c: 6661
				 * new_bfqq->waker_bfqq must be /Users/rubber/linux/block/bfq-iosched.c: 6662
				 * reset. So insert new_bfqq into the /Users/rubber/linux/block/bfq-iosched.c: 6663
				 * woken_list of the waker. See /Users/rubber/linux/block/bfq-iosched.c: 6664
				 * bfq_check_waker for details. /Users/rubber/linux/block/bfq-iosched.c: 6665
	/* /Users/rubber/linux/block/bfq-iosched.c: 6683
	 * If a bfq_queue has only one process reference, it is owned /Users/rubber/linux/block/bfq-iosched.c: 6684
	 * by only this bic: we can then set bfqq->bic = bic. in /Users/rubber/linux/block/bfq-iosched.c: 6685
	 * addition, if the queue has also just been split, we have to /Users/rubber/linux/block/bfq-iosched.c: 6686
	 * resume its state. /Users/rubber/linux/block/bfq-iosched.c: 6687
			/* /Users/rubber/linux/block/bfq-iosched.c: 6692
			 * The queue has just been split from a shared /Users/rubber/linux/block/bfq-iosched.c: 6693
			 * queue: restore the idle window and the /Users/rubber/linux/block/bfq-iosched.c: 6694
			 * possible weight raising period. /Users/rubber/linux/block/bfq-iosched.c: 6695
	/* /Users/rubber/linux/block/bfq-iosched.c: 6702
	 * Consider bfqq as possibly belonging to a burst of newly /Users/rubber/linux/block/bfq-iosched.c: 6703
	 * created queues only if: /Users/rubber/linux/block/bfq-iosched.c: 6704
	 * 1) A burst is actually happening (bfqd->burst_size > 0) /Users/rubber/linux/block/bfq-iosched.c: 6705
	 * or /Users/rubber/linux/block/bfq-iosched.c: 6706
	 * 2) There is no other active queue. In fact, if, in /Users/rubber/linux/block/bfq-iosched.c: 6707
	 *    contrast, there are active queues not belonging to the /Users/rubber/linux/block/bfq-iosched.c: 6708
	 *    possible burst bfqq may belong to, then there is no gain /Users/rubber/linux/block/bfq-iosched.c: 6709
	 *    in considering bfqq as belonging to a burst, and /Users/rubber/linux/block/bfq-iosched.c: 6710
	 *    therefore in not weight-raising bfqq. See comments on /Users/rubber/linux/block/bfq-iosched.c: 6711
	 *    bfq_handle_burst(). /Users/rubber/linux/block/bfq-iosched.c: 6712
	 * /Users/rubber/linux/block/bfq-iosched.c: 6713
	 * This filtering also helps eliminating false positives, /Users/rubber/linux/block/bfq-iosched.c: 6714
	 * occurring when bfqq does not belong to an actual large /Users/rubber/linux/block/bfq-iosched.c: 6715
	 * burst, but some background task (e.g., a service) happens /Users/rubber/linux/block/bfq-iosched.c: 6716
	 * to trigger the creation of new queues very close to when /Users/rubber/linux/block/bfq-iosched.c: 6717
	 * bfqq and its possible companion queues are created. See /Users/rubber/linux/block/bfq-iosched.c: 6718
	 * comments on bfq_handle_burst() for further details also on /Users/rubber/linux/block/bfq-iosched.c: 6719
	 * this issue. /Users/rubber/linux/block/bfq-iosched.c: 6720
	/* /Users/rubber/linux/block/bfq-iosched.c: 6738
	 * Considering that bfqq may be in race, we should firstly check /Users/rubber/linux/block/bfq-iosched.c: 6739
	 * whether bfqq is in service before doing something on it. If /Users/rubber/linux/block/bfq-iosched.c: 6740
	 * the bfqq in race is not in service, it has already been expired /Users/rubber/linux/block/bfq-iosched.c: 6741
	 * through __bfq_bfqq_expire func and its wait_request flags has /Users/rubber/linux/block/bfq-iosched.c: 6742
	 * been cleared in __bfq_bfqd_reset_in_service func. /Users/rubber/linux/block/bfq-iosched.c: 6743
		/* /Users/rubber/linux/block/bfq-iosched.c: 6753
		 * Also here the queue can be safely expired /Users/rubber/linux/block/bfq-iosched.c: 6754
		 * for budget timeout without wasting /Users/rubber/linux/block/bfq-iosched.c: 6755
		 * guarantees /Users/rubber/linux/block/bfq-iosched.c: 6756
		/* /Users/rubber/linux/block/bfq-iosched.c: 6760
		 * The queue may not be empty upon timer expiration, /Users/rubber/linux/block/bfq-iosched.c: 6761
		 * because we may not disable the timer when the /Users/rubber/linux/block/bfq-iosched.c: 6762
		 * first request of the in-service queue arrives /Users/rubber/linux/block/bfq-iosched.c: 6763
		 * during disk idling. /Users/rubber/linux/block/bfq-iosched.c: 6764
 * Handler of the expiration of the timer running if the in-service queue /Users/rubber/linux/block/bfq-iosched.c: 6778
 * is idling inside its time slice. /Users/rubber/linux/block/bfq-iosched.c: 6779
	/* /Users/rubber/linux/block/bfq-iosched.c: 6787
	 * Theoretical race here: the in-service queue can be NULL or /Users/rubber/linux/block/bfq-iosched.c: 6788
	 * different from the queue that was idling if a new request /Users/rubber/linux/block/bfq-iosched.c: 6789
	 * arrives for the current queue and there is a full dispatch /Users/rubber/linux/block/bfq-iosched.c: 6790
	 * cycle that changes the in-service queue.  This can hardly /Users/rubber/linux/block/bfq-iosched.c: 6791
	 * happen, but in the worst case we just expire a queue too /Users/rubber/linux/block/bfq-iosched.c: 6792
	 * early. /Users/rubber/linux/block/bfq-iosched.c: 6793
 * Release all the bfqg references to its async queues.  If we are /Users/rubber/linux/block/bfq-iosched.c: 6818
 * deallocating the group these queues may still contain requests, so /Users/rubber/linux/block/bfq-iosched.c: 6819
 * we reparent them to the root cgroup (i.e., the only one that will /Users/rubber/linux/block/bfq-iosched.c: 6820
 * exist for sure until all the requests on a device are gone). /Users/rubber/linux/block/bfq-iosched.c: 6821
 * See the comments on bfq_limit_depth for the purpose of /Users/rubber/linux/block/bfq-iosched.c: 6835
 * the depths set in the function. Return minimum shallow depth we'll use. /Users/rubber/linux/block/bfq-iosched.c: 6836
	/* /Users/rubber/linux/block/bfq-iosched.c: 6843
	 * In-word depths if no bfq_queue is being weight-raised: /Users/rubber/linux/block/bfq-iosched.c: 6844
	 * leaving 25% of tags only for sync reads. /Users/rubber/linux/block/bfq-iosched.c: 6845
	 * /Users/rubber/linux/block/bfq-iosched.c: 6846
	 * In next formulas, right-shift the value /Users/rubber/linux/block/bfq-iosched.c: 6847
	 * (1U<<bt->sb.shift), instead of computing directly /Users/rubber/linux/block/bfq-iosched.c: 6848
	 * (1U<<(bt->sb.shift - something)), to be robust against /Users/rubber/linux/block/bfq-iosched.c: 6849
	 * any possible value of bt->sb.shift, without having to /Users/rubber/linux/block/bfq-iosched.c: 6850
	 * limit 'something'. /Users/rubber/linux/block/bfq-iosched.c: 6851
	/* /Users/rubber/linux/block/bfq-iosched.c: 6855
	 * no more than 75% of tags for sync writes (25% extra tags /Users/rubber/linux/block/bfq-iosched.c: 6856
	 * w.r.t. async I/O, to prevent async I/O from starving sync /Users/rubber/linux/block/bfq-iosched.c: 6857
	 * writes) /Users/rubber/linux/block/bfq-iosched.c: 6858
	/* /Users/rubber/linux/block/bfq-iosched.c: 6862
	 * In-word depths in case some bfq_queue is being weight- /Users/rubber/linux/block/bfq-iosched.c: 6863
	 * raised: leaving ~63% of tags for sync reads. This is the /Users/rubber/linux/block/bfq-iosched.c: 6864
	 * highest percentage for which, in our tests, application /Users/rubber/linux/block/bfq-iosched.c: 6865
	 * start-up times didn't suffer from any regression due to tag /Users/rubber/linux/block/bfq-iosched.c: 6866
	 * shortage. /Users/rubber/linux/block/bfq-iosched.c: 6867
	/* /Users/rubber/linux/block/bfq-iosched.c: 6962
	 * Our fallback bfqq if bfq_find_alloc_queue() runs into OOM issues. /Users/rubber/linux/block/bfq-iosched.c: 6963
	 * Grab a permanent reference to it, so that the normal code flow /Users/rubber/linux/block/bfq-iosched.c: 6964
	 * will not attempt to free it. /Users/rubber/linux/block/bfq-iosched.c: 6965
	/* /Users/rubber/linux/block/bfq-iosched.c: 6977
	 * Trigger weight initialization, according to ioprio, at the /Users/rubber/linux/block/bfq-iosched.c: 6978
	 * oom_bfqq's first activation. The oom_bfqq's ioprio and ioprio /Users/rubber/linux/block/bfq-iosched.c: 6979
	 * class won't be changed any more. /Users/rubber/linux/block/bfq-iosched.c: 6980
	/* /Users/rubber/linux/block/bfq-iosched.c: 7016
	 * Trade-off between responsiveness and fairness. /Users/rubber/linux/block/bfq-iosched.c: 7017
	bfqd->bfq_wr_max_softrt_rate = 7000; /* /Users/rubber/linux/block/bfq-iosched.c: 7024
					      * Approximate rate required /Users/rubber/linux/block/bfq-iosched.c: 7025
					      * to playback or record a /Users/rubber/linux/block/bfq-iosched.c: 7026
					      * high-definition compressed /Users/rubber/linux/block/bfq-iosched.c: 7027
					      * video. /Users/rubber/linux/block/bfq-iosched.c: 7028
	/* /Users/rubber/linux/block/bfq-iosched.c: 7032
	 * Begin by assuming, optimistically, that the device peak /Users/rubber/linux/block/bfq-iosched.c: 7033
	 * rate is equal to 2/3 of the highest reference rate. /Users/rubber/linux/block/bfq-iosched.c: 7034
	/* /Users/rubber/linux/block/bfq-iosched.c: 7042
	 * The invocation of the next bfq_create_group_hierarchy /Users/rubber/linux/block/bfq-iosched.c: 7043
	 * function is the head of a chain of function calls /Users/rubber/linux/block/bfq-iosched.c: 7044
	 * (bfq_create_group_hierarchy->blkcg_activate_policy-> /Users/rubber/linux/block/bfq-iosched.c: 7045
	 * blk_mq_freeze_queue) that may lead to the invocation of the /Users/rubber/linux/block/bfq-iosched.c: 7046
	 * has_work hook function. For this reason, /Users/rubber/linux/block/bfq-iosched.c: 7047
	 * bfq_create_group_hierarchy is invoked only after all /Users/rubber/linux/block/bfq-iosched.c: 7048
	 * scheduler data has been initialized, apart from the fields /Users/rubber/linux/block/bfq-iosched.c: 7049
	 * that can be initialized only after invoking /Users/rubber/linux/block/bfq-iosched.c: 7050
	 * bfq_create_group_hierarchy. This, in particular, enables /Users/rubber/linux/block/bfq-iosched.c: 7051
	 * has_work to correctly return false. Of course, to avoid /Users/rubber/linux/block/bfq-iosched.c: 7052
	 * other inconsistencies, the blk-mq stack must then refrain /Users/rubber/linux/block/bfq-iosched.c: 7053
	 * from invoking further scheduler hooks before this init /Users/rubber/linux/block/bfq-iosched.c: 7054
	 * function is finished. /Users/rubber/linux/block/bfq-iosched.c: 7055
 * Leaving this name to preserve name compatibility with cfq /Users/rubber/linux/block/bfq-iosched.c: 7213
 * parameters, but this timeout is used for both sync and async. /Users/rubber/linux/block/bfq-iosched.c: 7214
	/* /Users/rubber/linux/block/bfq-iosched.c: 7343
	 * Times to load large popular applications for the typical /Users/rubber/linux/block/bfq-iosched.c: 7344
	 * systems installed on the reference devices (see the /Users/rubber/linux/block/bfq-iosched.c: 7345
	 * comments before the definition of the next /Users/rubber/linux/block/bfq-iosched.c: 7346
	 * array). Actually, we use slightly lower values, as the /Users/rubber/linux/block/bfq-iosched.c: 7347
	 * estimated peak rate tends to be smaller than the actual /Users/rubber/linux/block/bfq-iosched.c: 7348
	 * peak rate.  The reason for this last fact is that estimates /Users/rubber/linux/block/bfq-iosched.c: 7349
	 * are computed over much shorter time intervals than the long /Users/rubber/linux/block/bfq-iosched.c: 7350
	 * intervals typically used for benchmarking. Why? First, to /Users/rubber/linux/block/bfq-iosched.c: 7351
	 * adapt more quickly to variations. Second, because an I/O /Users/rubber/linux/block/bfq-iosched.c: 7352
	 * scheduler cannot rely on a peak-rate-evaluation workload to /Users/rubber/linux/block/bfq-iosched.c: 7353
	 * be run for a long time. /Users/rubber/linux/block/bfq-iosched.c: 7354
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/karma.c: 1
 *  fs/partitions/karma.c /Users/rubber/linux/block/partitions/karma.c: 3
 *  Rio Karma partition info. /Users/rubber/linux/block/partitions/karma.c: 4
 *  Copyright (C) 2006 Bob Copeland (me@bobcopeland.com) /Users/rubber/linux/block/partitions/karma.c: 6
 *  based on osf.c /Users/rubber/linux/block/partitions/karma.c: 7
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/aix.c: 1
 *  fs/partitions/aix.c /Users/rubber/linux/block/partitions/aix.c: 3
 *  Copyright (C) 2012-2013 Philippe De Muyter <phdm@macqel.be> /Users/rubber/linux/block/partitions/aix.c: 5
 * read_lba(): Read bytes from disk, starting at given LBA /Users/rubber/linux/block/partitions/aix.c: 70
 * @state /Users/rubber/linux/block/partitions/aix.c: 71
 * @lba /Users/rubber/linux/block/partitions/aix.c: 72
 * @buffer /Users/rubber/linux/block/partitions/aix.c: 73
 * @count /Users/rubber/linux/block/partitions/aix.c: 74
 * Description:  Reads @count bytes from @state->disk into @buffer. /Users/rubber/linux/block/partitions/aix.c: 76
 * Returns number of bytes read on success, 0 on error. /Users/rubber/linux/block/partitions/aix.c: 77
 * alloc_pvd(): reads physical volume descriptor /Users/rubber/linux/block/partitions/aix.c: 105
 * @state /Users/rubber/linux/block/partitions/aix.c: 106
 * @lba /Users/rubber/linux/block/partitions/aix.c: 107
 * Description: Returns pvd on success,  NULL on error. /Users/rubber/linux/block/partitions/aix.c: 109
 * Allocates space for pvd and fill it with disk blocks at @lba /Users/rubber/linux/block/partitions/aix.c: 110
 * Notes: remember to free pvd when you're done! /Users/rubber/linux/block/partitions/aix.c: 111
 * alloc_lvn(): reads logical volume names /Users/rubber/linux/block/partitions/aix.c: 130
 * @state /Users/rubber/linux/block/partitions/aix.c: 131
 * @lba /Users/rubber/linux/block/partitions/aix.c: 132
 * Description: Returns lvn on success,  NULL on error. /Users/rubber/linux/block/partitions/aix.c: 134
 * Allocates space for lvn and fill it with disk blocks at @lba /Users/rubber/linux/block/partitions/aix.c: 135
 * Notes: remember to free lvn when you're done! /Users/rubber/linux/block/partitions/aix.c: 136
 null char /Users/rubber/linux/block/partitions/aix.c: 270
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/mac.c: 1
 *  fs/partitions/mac.c /Users/rubber/linux/block/partitions/mac.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/mac.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/mac.c: 6
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/mac.c: 7
 * Code to understand MacOS partition tables. /Users/rubber/linux/block/partitions/mac.c: 20
		/* /Users/rubber/linux/block/partitions/mac.c: 92
		 * If this is the first bootable partition, tell the /Users/rubber/linux/block/partitions/mac.c: 93
		 * setup code, in case it wants to make this the root. /Users/rubber/linux/block/partitions/mac.c: 94
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/sgi.c: 1
 *  fs/partitions/sgi.c /Users/rubber/linux/block/partitions/sgi.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/sgi.c: 5
		/*printk("Dev %s SGI disklabel: bad magic %08x\n", /Users/rubber/linux/block/partitions/sgi.c: 53
	/* All SGI disk labels have 16 partitions, disks under Linux only /Users/rubber/linux/block/partitions/sgi.c: 69
	 * have 15 minor's.  Luckily there are always a few zero length /Users/rubber/linux/block/partitions/sgi.c: 70
	 * partitions which we don't care about so we never overflow the /Users/rubber/linux/block/partitions/sgi.c: 71
	 * current_minor. /Users/rubber/linux/block/partitions/sgi.c: 72
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/sun.c: 1
 *  fs/partitions/sun.c /Users/rubber/linux/block/partitions/sun.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/sun.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/sun.c: 7
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/sun.c: 8
/*		printk(KERN_INFO "Dev %s Sun disklabel: bad magic %04x\n", /Users/rubber/linux/block/partitions/sun.c: 77
	/* /Users/rubber/linux/block/partitions/sun.c: 101
	 * So that old Linux-Sun partitions continue to work, /Users/rubber/linux/block/partitions/sun.c: 102
	 * alow the VTOC to be used under the additional condition ... /Users/rubber/linux/block/partitions/sun.c: 103
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/ultrix.c: 1
 *  fs/partitions/ultrix.c /Users/rubber/linux/block/partitions/ultrix.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/ultrix.c: 5
 *  Re-organised Jul 1999 Russell King /Users/rubber/linux/block/partitions/ultrix.c: 7
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/amiga.c: 1
 *  fs/partitions/amiga.c /Users/rubber/linux/block/partitions/amiga.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/amiga.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/amiga.c: 7
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/amiga.c: 8
		/* Try again with 0xdc..0xdf zeroed, Windows might have /Users/rubber/linux/block/partitions/amiga.c: 54
		 * trashed it. /Users/rubber/linux/block/partitions/amiga.c: 55
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/partitions/ldm.c: 1
 * ldm - Support for Windows Logical Disk Manager (Dynamic Disks) /Users/rubber/linux/block/partitions/ldm.c: 3
 * Copyright (C) 2001,2002 Richard Russon <ldm@flatcap.org> /Users/rubber/linux/block/partitions/ldm.c: 5
 * Copyright (c) 2001-2012 Anton Altaparmakov /Users/rubber/linux/block/partitions/ldm.c: 6
 * Copyright (C) 2001,2002 Jakob Kemi <jakob.kemi@telia.com> /Users/rubber/linux/block/partitions/ldm.c: 7
 * Documentation is available at http://www.linux-ntfs.org/doku.php?id=downloads  /Users/rubber/linux/block/partitions/ldm.c: 9
 * ldm_debug/info/error/crit - Output an error message /Users/rubber/linux/block/partitions/ldm.c: 23
 * @f:    A printf format string containing the message /Users/rubber/linux/block/partitions/ldm.c: 24
 * @...:  Variables to substitute into @f /Users/rubber/linux/block/partitions/ldm.c: 25
 * ldm_debug() writes a DEBUG level message to the syslog but only if the /Users/rubber/linux/block/partitions/ldm.c: 27
 * driver was compiled with debug enabled. Otherwise, the call turns into a NOP. /Users/rubber/linux/block/partitions/ldm.c: 28
 * ldm_parse_privhead - Read the LDM Database PRIVHEAD structure /Users/rubber/linux/block/partitions/ldm.c: 57
 * @data:  Raw database PRIVHEAD structure loaded from the device /Users/rubber/linux/block/partitions/ldm.c: 58
 * @ph:    In-memory privhead structure in which to return parsed information /Users/rubber/linux/block/partitions/ldm.c: 59
 * This parses the LDM database PRIVHEAD structure supplied in @data and /Users/rubber/linux/block/partitions/ldm.c: 61
 * sets up the in-memory privhead structure @ph with the obtained information. /Users/rubber/linux/block/partitions/ldm.c: 62
 * Return:  'true'   @ph contains the PRIVHEAD data /Users/rubber/linux/block/partitions/ldm.c: 64
 *          'false'  @ph contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 65
 * ldm_parse_tocblock - Read the LDM Database TOCBLOCK structure /Users/rubber/linux/block/partitions/ldm.c: 113
 * @data:  Raw database TOCBLOCK structure loaded from the device /Users/rubber/linux/block/partitions/ldm.c: 114
 * @toc:   In-memory toc structure in which to return parsed information /Users/rubber/linux/block/partitions/ldm.c: 115
 * This parses the LDM Database TOCBLOCK (table of contents) structure supplied /Users/rubber/linux/block/partitions/ldm.c: 117
 * in @data and sets up the in-memory tocblock structure @toc with the obtained /Users/rubber/linux/block/partitions/ldm.c: 118
 * information. /Users/rubber/linux/block/partitions/ldm.c: 119
 * N.B.  The *_start and *_size values returned in @toc are not range-checked. /Users/rubber/linux/block/partitions/ldm.c: 121
 * Return:  'true'   @toc contains the TOCBLOCK data /Users/rubber/linux/block/partitions/ldm.c: 123
 *          'false'  @toc contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 124
 * ldm_parse_vmdb - Read the LDM Database VMDB structure /Users/rubber/linux/block/partitions/ldm.c: 160
 * @data:  Raw database VMDB structure loaded from the device /Users/rubber/linux/block/partitions/ldm.c: 161
 * @vm:    In-memory vmdb structure in which to return parsed information /Users/rubber/linux/block/partitions/ldm.c: 162
 * This parses the LDM Database VMDB structure supplied in @data and sets up /Users/rubber/linux/block/partitions/ldm.c: 164
 * the in-memory vmdb structure @vm with the obtained information. /Users/rubber/linux/block/partitions/ldm.c: 165
 * N.B.  The *_start, *_size and *_seq values will be range-checked later. /Users/rubber/linux/block/partitions/ldm.c: 167
 * Return:  'true'   @vm contains VMDB info /Users/rubber/linux/block/partitions/ldm.c: 169
 *          'false'  @vm contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 170
 * ldm_compare_privheads - Compare two privhead objects /Users/rubber/linux/block/partitions/ldm.c: 203
 * @ph1:  First privhead /Users/rubber/linux/block/partitions/ldm.c: 204
 * @ph2:  Second privhead /Users/rubber/linux/block/partitions/ldm.c: 205
 * This compares the two privhead structures @ph1 and @ph2. /Users/rubber/linux/block/partitions/ldm.c: 207
 * Return:  'true'   Identical /Users/rubber/linux/block/partitions/ldm.c: 209
 *          'false'  Different /Users/rubber/linux/block/partitions/ldm.c: 210
 * ldm_compare_tocblocks - Compare two tocblock objects /Users/rubber/linux/block/partitions/ldm.c: 227
 * @toc1:  First toc /Users/rubber/linux/block/partitions/ldm.c: 228
 * @toc2:  Second toc /Users/rubber/linux/block/partitions/ldm.c: 229
 * This compares the two tocblock structures @toc1 and @toc2. /Users/rubber/linux/block/partitions/ldm.c: 231
 * Return:  'true'   Identical /Users/rubber/linux/block/partitions/ldm.c: 233
 *          'false'  Different /Users/rubber/linux/block/partitions/ldm.c: 234
 * ldm_validate_privheads - Compare the primary privhead with its backups /Users/rubber/linux/block/partitions/ldm.c: 252
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 253
 * @ph1:   Memory struct to fill with ph contents /Users/rubber/linux/block/partitions/ldm.c: 254
 * Read and compare all three privheads from disk. /Users/rubber/linux/block/partitions/ldm.c: 256
 * The privheads on disk show the size and location of the main disk area and /Users/rubber/linux/block/partitions/ldm.c: 258
 * the configuration area (the database).  The values are range-checked against /Users/rubber/linux/block/partitions/ldm.c: 259
 * @hd, which contains the real size of the disk. /Users/rubber/linux/block/partitions/ldm.c: 260
 * Return:  'true'   Success /Users/rubber/linux/block/partitions/ldm.c: 262
 *          'false'  Error /Users/rubber/linux/block/partitions/ldm.c: 263
	/* FIXME ignore this for now /Users/rubber/linux/block/partitions/ldm.c: 326
	if (!ldm_compare_privheads (ph[0], ph[2])) { /Users/rubber/linux/block/partitions/ldm.c: 327
		ldm_crit ("Primary and backup PRIVHEADs don't match."); /Users/rubber/linux/block/partitions/ldm.c: 328
		goto out; /Users/rubber/linux/block/partitions/ldm.c: 329
 * ldm_validate_tocblocks - Validate the table of contents and its backups /Users/rubber/linux/block/partitions/ldm.c: 340
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 341
 * @base:  Offset, into @state->disk, of the database /Users/rubber/linux/block/partitions/ldm.c: 342
 * @ldb:   Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 343
 * Find and compare the four tables of contents of the LDM Database stored on /Users/rubber/linux/block/partitions/ldm.c: 345
 * @state->disk and return the parsed information into @toc1. /Users/rubber/linux/block/partitions/ldm.c: 346
 * The offsets and sizes of the configs are range-checked against a privhead. /Users/rubber/linux/block/partitions/ldm.c: 348
 * Return:  'true'   @toc1 contains validated TOCBLOCK info /Users/rubber/linux/block/partitions/ldm.c: 350
 *          'false'  @toc1 contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 351
	/* /Users/rubber/linux/block/partitions/ldm.c: 374
	 * Try to read and parse all four TOCBLOCKs. /Users/rubber/linux/block/partitions/ldm.c: 375
	 * /Users/rubber/linux/block/partitions/ldm.c: 376
	 * Windows Vista LDM v2.12 does not always have all four TOCBLOCKs so /Users/rubber/linux/block/partitions/ldm.c: 377
	 * skip any that fail as long as we get at least one valid TOCBLOCK. /Users/rubber/linux/block/partitions/ldm.c: 378
 * ldm_validate_vmdb - Read the VMDB and validate it /Users/rubber/linux/block/partitions/ldm.c: 416
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 417
 * @base:  Offset, into @bdev, of the database /Users/rubber/linux/block/partitions/ldm.c: 418
 * @ldb:   Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 419
 * Find the vmdb of the LDM Database stored on @bdev and return the parsed /Users/rubber/linux/block/partitions/ldm.c: 421
 * information in @ldb. /Users/rubber/linux/block/partitions/ldm.c: 422
 * Return:  'true'   @ldb contains validated VBDB info /Users/rubber/linux/block/partitions/ldm.c: 424
 *          'false'  @ldb contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 425
	/* /Users/rubber/linux/block/partitions/ldm.c: 459
	 * The last_vblkd_seq can be before the end of the vmdb, just make sure /Users/rubber/linux/block/partitions/ldm.c: 460
	 * it is not out of bounds. /Users/rubber/linux/block/partitions/ldm.c: 461
 * ldm_validate_partition_table - Determine whether bdev might be a dynamic disk /Users/rubber/linux/block/partitions/ldm.c: 477
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 478
 * This function provides a weak test to decide whether the device is a dynamic /Users/rubber/linux/block/partitions/ldm.c: 480
 * disk or not.  It looks for an MS-DOS-style partition table containing at /Users/rubber/linux/block/partitions/ldm.c: 481
 * least one partition of type 0x42 (formerly SFS, now used by Windows for /Users/rubber/linux/block/partitions/ldm.c: 482
 * dynamic disks). /Users/rubber/linux/block/partitions/ldm.c: 483
 * N.B.  The only possible error can come from the read_part_sector and that is /Users/rubber/linux/block/partitions/ldm.c: 485
 *       only likely to happen if the underlying device is strange.  If that IS /Users/rubber/linux/block/partitions/ldm.c: 486
 *       the case we should return zero to let someone else try. /Users/rubber/linux/block/partitions/ldm.c: 487
 * Return:  'true'   @state->disk is a dynamic disk /Users/rubber/linux/block/partitions/ldm.c: 489
 *          'false'  @state->disk is not a dynamic disk, or an error occurred /Users/rubber/linux/block/partitions/ldm.c: 490
 * ldm_get_disk_objid - Search a linked list of vblk's for a given Disk Id /Users/rubber/linux/block/partitions/ldm.c: 527
 * @ldb:  Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 528
 * The LDM Database contains a list of all partitions on all dynamic disks. /Users/rubber/linux/block/partitions/ldm.c: 530
 * The primary PRIVHEAD, at the beginning of the physical disk, tells us /Users/rubber/linux/block/partitions/ldm.c: 531
 * the GUID of this disk.  This function searches for the GUID in a linked /Users/rubber/linux/block/partitions/ldm.c: 532
 * list of vblk's. /Users/rubber/linux/block/partitions/ldm.c: 533
 * Return:  Pointer, A matching vblk was found /Users/rubber/linux/block/partitions/ldm.c: 535
 *          NULL,    No match, or an error /Users/rubber/linux/block/partitions/ldm.c: 536
 * ldm_create_data_partitions - Create data partitions for this device /Users/rubber/linux/block/partitions/ldm.c: 554
 * @pp:   List of the partitions parsed so far /Users/rubber/linux/block/partitions/ldm.c: 555
 * @ldb:  Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 556
 * The database contains ALL the partitions for ALL disk groups, so we need to /Users/rubber/linux/block/partitions/ldm.c: 558
 * filter out this specific disk. Using the disk's object id, we can find all /Users/rubber/linux/block/partitions/ldm.c: 559
 * the partitions in the database that belong to this disk. /Users/rubber/linux/block/partitions/ldm.c: 560
 * Add each partition in our database, to the parsed_partitions structure. /Users/rubber/linux/block/partitions/ldm.c: 562
 * N.B.  This function creates the partitions in the order it finds partition /Users/rubber/linux/block/partitions/ldm.c: 564
 *       objects in the linked list. /Users/rubber/linux/block/partitions/ldm.c: 565
 * Return:  'true'   Partition created /Users/rubber/linux/block/partitions/ldm.c: 567
 *          'false'  Error, probably a range checking problem /Users/rubber/linux/block/partitions/ldm.c: 568
 * ldm_relative - Calculate the next relative offset /Users/rubber/linux/block/partitions/ldm.c: 608
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 609
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 610
 * @base:    Size of the previous fixed width fields /Users/rubber/linux/block/partitions/ldm.c: 611
 * @offset:  Cumulative size of the previous variable-width fields /Users/rubber/linux/block/partitions/ldm.c: 612
 * Because many of the VBLK fields are variable-width, it's necessary /Users/rubber/linux/block/partitions/ldm.c: 614
 * to calculate each offset based on the previous one and the length /Users/rubber/linux/block/partitions/ldm.c: 615
 * of the field it pointed to. /Users/rubber/linux/block/partitions/ldm.c: 616
 * Return:  -1 Error, the calculated offset exceeded the size of the buffer /Users/rubber/linux/block/partitions/ldm.c: 618
 *           n OK, a range-checked offset into buffer /Users/rubber/linux/block/partitions/ldm.c: 619
 * ldm_get_vnum - Convert a variable-width, big endian number, into cpu order /Users/rubber/linux/block/partitions/ldm.c: 643
 * @block:  Pointer to the variable-width number to convert /Users/rubber/linux/block/partitions/ldm.c: 644
 * Large numbers in the LDM Database are often stored in a packed format.  Each /Users/rubber/linux/block/partitions/ldm.c: 646
 * number is prefixed by a one byte width marker.  All numbers in the database /Users/rubber/linux/block/partitions/ldm.c: 647
 * are stored in big-endian byte order.  This function reads one of these /Users/rubber/linux/block/partitions/ldm.c: 648
 * numbers and returns the result /Users/rubber/linux/block/partitions/ldm.c: 649
 * N.B.  This function DOES NOT perform any range checking, though the most /Users/rubber/linux/block/partitions/ldm.c: 651
 *       it will read is eight bytes. /Users/rubber/linux/block/partitions/ldm.c: 652
 * Return:  n A number /Users/rubber/linux/block/partitions/ldm.c: 654
 *          0 Zero, or an error occurred /Users/rubber/linux/block/partitions/ldm.c: 655
 * ldm_get_vstr - Read a length-prefixed string into a buffer /Users/rubber/linux/block/partitions/ldm.c: 676
 * @block:   Pointer to the length marker /Users/rubber/linux/block/partitions/ldm.c: 677
 * @buffer:  Location to copy string to /Users/rubber/linux/block/partitions/ldm.c: 678
 * @buflen:  Size of the output buffer /Users/rubber/linux/block/partitions/ldm.c: 679
 * Many of the strings in the LDM Database are not NULL terminated.  Instead /Users/rubber/linux/block/partitions/ldm.c: 681
 * they are prefixed by a one byte length marker.  This function copies one of /Users/rubber/linux/block/partitions/ldm.c: 682
 * these strings into a buffer. /Users/rubber/linux/block/partitions/ldm.c: 683
 * N.B.  This function DOES NOT perform any range checking on the input. /Users/rubber/linux/block/partitions/ldm.c: 685
 *       If the buffer is too small, the output will be truncated. /Users/rubber/linux/block/partitions/ldm.c: 686
 * Return:  0, Error and @buffer contents are undefined /Users/rubber/linux/block/partitions/ldm.c: 688
 *          n, String length in characters (excluding NULL) /Users/rubber/linux/block/partitions/ldm.c: 689
 *          buflen-1, String was truncated. /Users/rubber/linux/block/partitions/ldm.c: 690
 * ldm_parse_cmp3 - Read a raw VBLK Component object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 710
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 711
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 712
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 713
 * Read a raw VBLK Component object (version 3) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 715
 * Return:  'true'   @vb contains a Component VBLK /Users/rubber/linux/block/partitions/ldm.c: 717
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 718
 * ldm_parse_dgr3 - Read a raw VBLK Disk Group object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 761
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 762
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 763
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 764
 * Read a raw VBLK Disk Group object (version 3) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 766
 * Return:  'true'   @vb contains a Disk Group VBLK /Users/rubber/linux/block/partitions/ldm.c: 768
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 769
 * ldm_parse_dgr4 - Read a raw VBLK Disk Group object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 805
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 806
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 807
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 808
 * Read a raw VBLK Disk Group object (version 4) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 810
 * Return:  'true'   @vb contains a Disk Group VBLK /Users/rubber/linux/block/partitions/ldm.c: 812
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 813
 * ldm_parse_dsk3 - Read a raw VBLK Disk object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 846
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 847
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 848
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 849
 * Read a raw VBLK Disk object (version 3) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 851
 * Return:  'true'   @vb contains a Disk VBLK /Users/rubber/linux/block/partitions/ldm.c: 853
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 854
 * ldm_parse_dsk4 - Read a raw VBLK Disk object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 885
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 886
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 887
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 888
 * Read a raw VBLK Disk object (version 4) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 890
 * Return:  'true'   @vb contains a Disk VBLK /Users/rubber/linux/block/partitions/ldm.c: 892
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 893
 * ldm_parse_prt3 - Read a raw VBLK Partition object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 918
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 919
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 920
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 921
 * Read a raw VBLK Partition object (version 3) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 923
 * Return:  'true'   @vb contains a Partition VBLK /Users/rubber/linux/block/partitions/ldm.c: 925
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 926
 * ldm_parse_vol5 - Read a raw VBLK Volume object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 994
 * @buffer:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 995
 * @buflen:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 996
 * @vb:      In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 997
 * Read a raw VBLK Volume object (version 5) into a vblk structure. /Users/rubber/linux/block/partitions/ldm.c: 999
 * Return:  'true'   @vb contains a Volume VBLK /Users/rubber/linux/block/partitions/ldm.c: 1001
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 1002
 * ldm_parse_vblk - Read a raw VBLK object into a vblk structure /Users/rubber/linux/block/partitions/ldm.c: 1101
 * @buf:  Block of data being worked on /Users/rubber/linux/block/partitions/ldm.c: 1102
 * @len:  Size of the block of data /Users/rubber/linux/block/partitions/ldm.c: 1103
 * @vb:   In-memory vblk in which to return information /Users/rubber/linux/block/partitions/ldm.c: 1104
 * Read a raw VBLK object into a vblk structure.  This function just reads the /Users/rubber/linux/block/partitions/ldm.c: 1106
 * information common to all VBLK types, then delegates the rest of the work to /Users/rubber/linux/block/partitions/ldm.c: 1107
 * helper functions: ldm_parse_*. /Users/rubber/linux/block/partitions/ldm.c: 1108
 * Return:  'true'   @vb contains a VBLK /Users/rubber/linux/block/partitions/ldm.c: 1110
 *          'false'  @vb contents are not defined /Users/rubber/linux/block/partitions/ldm.c: 1111
 * ldm_ldmdb_add - Adds a raw VBLK entry to the ldmdb database /Users/rubber/linux/block/partitions/ldm.c: 1153
 * @data:  Raw VBLK to add to the database /Users/rubber/linux/block/partitions/ldm.c: 1154
 * @len:   Size of the raw VBLK /Users/rubber/linux/block/partitions/ldm.c: 1155
 * @ldb:   Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 1156
 * The VBLKs are sorted into categories.  Partitions are also sorted by offset. /Users/rubber/linux/block/partitions/ldm.c: 1158
 * N.B.  This function does not check the validity of the VBLKs. /Users/rubber/linux/block/partitions/ldm.c: 1160
 * Return:  'true'   The VBLK was added /Users/rubber/linux/block/partitions/ldm.c: 1162
 *          'false'  An error occurred /Users/rubber/linux/block/partitions/ldm.c: 1163
 * ldm_frag_add - Add a VBLK fragment to a list /Users/rubber/linux/block/partitions/ldm.c: 1216
 * @data:   Raw fragment to be added to the list /Users/rubber/linux/block/partitions/ldm.c: 1217
 * @size:   Size of the raw fragment /Users/rubber/linux/block/partitions/ldm.c: 1218
 * @frags:  Linked list of VBLK fragments /Users/rubber/linux/block/partitions/ldm.c: 1219
 * Fragmented VBLKs may not be consecutive in the database, so they are placed /Users/rubber/linux/block/partitions/ldm.c: 1221
 * in a list so they can be pieced together later. /Users/rubber/linux/block/partitions/ldm.c: 1222
 * Return:  'true'   Success, the VBLK was added to the list /Users/rubber/linux/block/partitions/ldm.c: 1224
 *          'false'  Error, a problem occurred /Users/rubber/linux/block/partitions/ldm.c: 1225
 * ldm_frag_free - Free a linked list of VBLK fragments /Users/rubber/linux/block/partitions/ldm.c: 1290
 * @list:  Linked list of fragments /Users/rubber/linux/block/partitions/ldm.c: 1291
 * Free a linked list of VBLK fragments /Users/rubber/linux/block/partitions/ldm.c: 1293
 * Return:  none /Users/rubber/linux/block/partitions/ldm.c: 1295
 * ldm_frag_commit - Validate fragmented VBLKs and add them to the database /Users/rubber/linux/block/partitions/ldm.c: 1308
 * @frags:  Linked list of VBLK fragments /Users/rubber/linux/block/partitions/ldm.c: 1309
 * @ldb:    Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 1310
 * Now that all the fragmented VBLKs have been collected, they must be added to /Users/rubber/linux/block/partitions/ldm.c: 1312
 * the database for later use. /Users/rubber/linux/block/partitions/ldm.c: 1313
 * Return:  'true'   All the fragments we added successfully /Users/rubber/linux/block/partitions/ldm.c: 1315
 *          'false'  One or more of the fragments we invalid /Users/rubber/linux/block/partitions/ldm.c: 1316
 * ldm_get_vblks - Read the on-disk database of VBLKs into memory /Users/rubber/linux/block/partitions/ldm.c: 1341
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 1342
 * @base:  Offset, into @state->disk, of the database /Users/rubber/linux/block/partitions/ldm.c: 1343
 * @ldb:   Cache of the database structures /Users/rubber/linux/block/partitions/ldm.c: 1344
 * To use the information from the VBLKs, they need to be read from the disk, /Users/rubber/linux/block/partitions/ldm.c: 1346
 * unpacked and validated.  We cache them in @ldb according to their type. /Users/rubber/linux/block/partitions/ldm.c: 1347
 * Return:  'true'   All the VBLKs were read successfully /Users/rubber/linux/block/partitions/ldm.c: 1349
 *          'false'  An error occurred /Users/rubber/linux/block/partitions/ldm.c: 1350
 * ldm_free_vblks - Free a linked list of vblk's /Users/rubber/linux/block/partitions/ldm.c: 1405
 * @lh:  Head of a linked list of struct vblk /Users/rubber/linux/block/partitions/ldm.c: 1406
 * Free a list of vblk's and free the memory used to maintain the list. /Users/rubber/linux/block/partitions/ldm.c: 1408
 * Return:  none /Users/rubber/linux/block/partitions/ldm.c: 1410
 * ldm_partition - Find out whether a device is a dynamic disk and handle it /Users/rubber/linux/block/partitions/ldm.c: 1424
 * @state: Partition check state including device holding the LDM Database /Users/rubber/linux/block/partitions/ldm.c: 1425
 * This determines whether the device @bdev is a dynamic disk and if so creates /Users/rubber/linux/block/partitions/ldm.c: 1427
 * the partitions necessary in the gendisk structure pointed to by @hd. /Users/rubber/linux/block/partitions/ldm.c: 1428
 * We create a dummy device 1, which contains the LDM database, and then create /Users/rubber/linux/block/partitions/ldm.c: 1430
 * each partition described by the LDM database in sequence as devices 2+. For /Users/rubber/linux/block/partitions/ldm.c: 1431
 * example, if the device is hda, we would have: hda1: LDM database, hda2, hda3, /Users/rubber/linux/block/partitions/ldm.c: 1432
 * and so on: the actual data containing partitions. /Users/rubber/linux/block/partitions/ldm.c: 1433
 * Return:  1 Success, @state->disk is a dynamic disk and we handled it /Users/rubber/linux/block/partitions/ldm.c: 1435
 *          0 Success, @state->disk is not a dynamic disk /Users/rubber/linux/block/partitions/ldm.c: 1436
 *         -1 An error occurred before enough information had been read /Users/rubber/linux/block/partitions/ldm.c: 1437
 *            Or @state->disk is a dynamic disk, but it may be corrupted /Users/rubber/linux/block/partitions/ldm.c: 1438
 SPDX-License-Identifier: GPL-2.0-or-later /Users/rubber/linux/block/partitions/efi.c: 1
 * EFI GUID Partition Table handling /Users/rubber/linux/block/partitions/efi.c: 3
 * http://www.uefi.org/specs/ /Users/rubber/linux/block/partitions/efi.c: 5
 * http://www.intel.com/technology/efi/ /Users/rubber/linux/block/partitions/efi.c: 6
 * efi.[ch] by Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 8
 *   Copyright 2000,2001,2002,2004 Dell Inc. /Users/rubber/linux/block/partitions/efi.c: 9
 * TODO: /Users/rubber/linux/block/partitions/efi.c: 11
 * Changelog: /Users/rubber/linux/block/partitions/efi.c: 13
 * Mon August 5th, 2013 Davidlohr Bueso <davidlohr@hp.com> /Users/rubber/linux/block/partitions/efi.c: 14
 * - detect hybrid MBRs, tighter pMBR checking & cleanups. /Users/rubber/linux/block/partitions/efi.c: 15
 * Mon Nov 09 2004 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 17
 * - test for valid PMBR and valid PGPT before ever reading /Users/rubber/linux/block/partitions/efi.c: 18
 *   AGPT, allow override with 'gpt' kernel command line option. /Users/rubber/linux/block/partitions/efi.c: 19
 * - check for first/last_usable_lba outside of size of disk /Users/rubber/linux/block/partitions/efi.c: 20
 * Tue  Mar 26 2002 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 22
 * - Ported to 2.5.7-pre1 and 2.5.7-dj2 /Users/rubber/linux/block/partitions/efi.c: 23
 * - Applied patch to avoid fault in alternate header handling /Users/rubber/linux/block/partitions/efi.c: 24
 * - cleaned up find_valid_gpt /Users/rubber/linux/block/partitions/efi.c: 25
 * - On-disk structure and copy in memory is *always* LE now -  /Users/rubber/linux/block/partitions/efi.c: 26
 *   swab fields as needed /Users/rubber/linux/block/partitions/efi.c: 27
 * - remove print_gpt_header() /Users/rubber/linux/block/partitions/efi.c: 28
 * - only use first max_p partition entries, to keep the kernel minor number /Users/rubber/linux/block/partitions/efi.c: 29
 *   and partition numbers tied. /Users/rubber/linux/block/partitions/efi.c: 30
 * Mon  Feb 04 2002 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 32
 * - Removed __PRIPTR_PREFIX - not being used /Users/rubber/linux/block/partitions/efi.c: 33
 * Mon  Jan 14 2002 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 35
 * - Ported to 2.5.2-pre11 + library crc32 patch Linus applied /Users/rubber/linux/block/partitions/efi.c: 36
 * Thu Dec 6 2001 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 38
 * - Added compare_gpts(). /Users/rubber/linux/block/partitions/efi.c: 39
 * - moved le_efi_guid_to_cpus() back into this file.  GPT is the only /Users/rubber/linux/block/partitions/efi.c: 40
 *   thing that keeps EFI GUIDs on disk. /Users/rubber/linux/block/partitions/efi.c: 41
 * - Changed gpt structure names and members to be simpler and more Linux-like. /Users/rubber/linux/block/partitions/efi.c: 42
 * Wed Oct 17 2001 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 44
 * - Removed CONFIG_DEVFS_VOLUMES_UUID code entirely per Martin Wilck /Users/rubber/linux/block/partitions/efi.c: 45
 * Wed Oct 10 2001 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 47
 * - Changed function comments to DocBook style per Andreas Dilger suggestion. /Users/rubber/linux/block/partitions/efi.c: 48
 * Mon Oct 08 2001 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 50
 * - Change read_lba() to use the page cache per Al Viro's work. /Users/rubber/linux/block/partitions/efi.c: 51
 * - print u64s properly on all architectures /Users/rubber/linux/block/partitions/efi.c: 52
 * - fixed debug_printk(), now Dprintk() /Users/rubber/linux/block/partitions/efi.c: 53
 * Mon Oct 01 2001 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 55
 * - Style cleanups /Users/rubber/linux/block/partitions/efi.c: 56
 * - made most functions static /Users/rubber/linux/block/partitions/efi.c: 57
 * - Endianness addition /Users/rubber/linux/block/partitions/efi.c: 58
 * - remove test for second alternate header, as it's not per spec, /Users/rubber/linux/block/partitions/efi.c: 59
 *   and is unnecessary.  There's now a method to read/write the last /Users/rubber/linux/block/partitions/efi.c: 60
 *   sector of an odd-sized disk from user space.  No tools have ever /Users/rubber/linux/block/partitions/efi.c: 61
 *   been released which used this code, so it's effectively dead. /Users/rubber/linux/block/partitions/efi.c: 62
 * - Per Asit Mallick of Intel, added a test for a valid PMBR. /Users/rubber/linux/block/partitions/efi.c: 63
 * - Added kernel command line option 'gpt' to override valid PMBR test. /Users/rubber/linux/block/partitions/efi.c: 64
 * Wed Jun  6 2001 Martin Wilck <Martin.Wilck@Fujitsu-Siemens.com> /Users/rubber/linux/block/partitions/efi.c: 66
 * - added devfs volume UUID support (/dev/volumes/uuids) for /Users/rubber/linux/block/partitions/efi.c: 67
 *   mounting file systems by the partition GUID.  /Users/rubber/linux/block/partitions/efi.c: 68
 * Tue Dec  5 2000 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 70
 * - Moved crc32() to linux/lib, added efi_crc32(). /Users/rubber/linux/block/partitions/efi.c: 71
 * Thu Nov 30 2000 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 73
 * - Replaced Intel's CRC32 function with an equivalent /Users/rubber/linux/block/partitions/efi.c: 74
 *   non-license-restricted version. /Users/rubber/linux/block/partitions/efi.c: 75
 * Wed Oct 25 2000 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 77
 * - Fixed the last_lba() call to return the proper last block /Users/rubber/linux/block/partitions/efi.c: 78
 * Thu Oct 12 2000 Matt Domsch <Matt_Domsch@dell.com> /Users/rubber/linux/block/partitions/efi.c: 80
 * - Thanks to Andries Brouwer for his debugging assistance. /Users/rubber/linux/block/partitions/efi.c: 81
 * - Code works, detects all the partitions. /Users/rubber/linux/block/partitions/efi.c: 82
/* This allows a kernel command line option 'gpt' to override /Users/rubber/linux/block/partitions/efi.c: 93
 * the test for invalid PMBR.  Not __initdata because reloading /Users/rubber/linux/block/partitions/efi.c: 94
 * the partition tables happens after init too. /Users/rubber/linux/block/partitions/efi.c: 95
 * efi_crc32() - EFI version of crc32 function /Users/rubber/linux/block/partitions/efi.c: 108
 * @buf: buffer to calculate crc32 of /Users/rubber/linux/block/partitions/efi.c: 109
 * @len: length of buf /Users/rubber/linux/block/partitions/efi.c: 110
 * Description: Returns EFI-style CRC32 value for @buf /Users/rubber/linux/block/partitions/efi.c: 112
 * This function uses the little endian Ethernet polynomial /Users/rubber/linux/block/partitions/efi.c: 114
 * but seeds the function with ~0, and xor's with ~0 at the end. /Users/rubber/linux/block/partitions/efi.c: 115
 * Note, the EFI Specification, v1.02, has a reference to /Users/rubber/linux/block/partitions/efi.c: 116
 * Dr. Dobbs Journal, May 1994 (actually it's in May 1992). /Users/rubber/linux/block/partitions/efi.c: 117
 * last_lba(): return number of last logical block of device /Users/rubber/linux/block/partitions/efi.c: 126
 * @disk: block device /Users/rubber/linux/block/partitions/efi.c: 127
 * Description: Returns last LBA value on success, 0 on error. /Users/rubber/linux/block/partitions/efi.c: 129
 * This is stored (by sd and ide-geometry) in /Users/rubber/linux/block/partitions/efi.c: 130
 *  the part[0] entry for this disk, and is the number of /Users/rubber/linux/block/partitions/efi.c: 131
 *  physical sectors available on the disk. /Users/rubber/linux/block/partitions/efi.c: 132
 * is_pmbr_valid(): test Protective MBR for validity /Users/rubber/linux/block/partitions/efi.c: 155
 * @mbr: pointer to a legacy mbr structure /Users/rubber/linux/block/partitions/efi.c: 156
 * @total_sectors: amount of sectors in the device /Users/rubber/linux/block/partitions/efi.c: 157
 * Description: Checks for a valid protective or hybrid /Users/rubber/linux/block/partitions/efi.c: 159
 * master boot record (MBR). The validity of a pMBR depends /Users/rubber/linux/block/partitions/efi.c: 160
 * on all of the following properties: /Users/rubber/linux/block/partitions/efi.c: 161
 *  1) MSDOS signature is in the last two bytes of the MBR /Users/rubber/linux/block/partitions/efi.c: 162
 *  2) One partition of type 0xEE is found /Users/rubber/linux/block/partitions/efi.c: 163
 * In addition, a hybrid MBR will have up to three additional /Users/rubber/linux/block/partitions/efi.c: 165
 * primary partitions, which point to the same space that's /Users/rubber/linux/block/partitions/efi.c: 166
 * marked out by up to three GPT partitions. /Users/rubber/linux/block/partitions/efi.c: 167
 * Returns 0 upon invalid MBR, or GPT_MBR_PROTECTIVE or /Users/rubber/linux/block/partitions/efi.c: 169
 * GPT_MBR_HYBRID depending on the device layout. /Users/rubber/linux/block/partitions/efi.c: 170
			/* /Users/rubber/linux/block/partitions/efi.c: 184
			 * Ok, we at least know that there's a protective MBR, /Users/rubber/linux/block/partitions/efi.c: 185
			 * now check if there are other partition types for /Users/rubber/linux/block/partitions/efi.c: 186
			 * hybrid MBR. /Users/rubber/linux/block/partitions/efi.c: 187
	/* /Users/rubber/linux/block/partitions/efi.c: 202
	 * Protective MBRs take up the lesser of the whole disk /Users/rubber/linux/block/partitions/efi.c: 203
	 * or 2 TiB (32bit LBA), ignoring the rest of the disk. /Users/rubber/linux/block/partitions/efi.c: 204
	 * Some partitioning programs, nonetheless, choose to set /Users/rubber/linux/block/partitions/efi.c: 205
	 * the size to the maximum 32-bit limitation, disregarding /Users/rubber/linux/block/partitions/efi.c: 206
	 * the disk size. /Users/rubber/linux/block/partitions/efi.c: 207
	 * /Users/rubber/linux/block/partitions/efi.c: 208
	 * Hybrid MBRs do not necessarily comply with this. /Users/rubber/linux/block/partitions/efi.c: 209
	 * /Users/rubber/linux/block/partitions/efi.c: 210
	 * Consider a bad value here to be a warning to support dd'ing /Users/rubber/linux/block/partitions/efi.c: 211
	 * an image from a smaller disk to a larger disk. /Users/rubber/linux/block/partitions/efi.c: 212
 * read_lba(): Read bytes from disk, starting at given LBA /Users/rubber/linux/block/partitions/efi.c: 226
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 227
 * @lba: the Logical Block Address of the partition table /Users/rubber/linux/block/partitions/efi.c: 228
 * @buffer: destination buffer /Users/rubber/linux/block/partitions/efi.c: 229
 * @count: bytes to read /Users/rubber/linux/block/partitions/efi.c: 230
 * Description: Reads @count bytes from @state->disk into @buffer. /Users/rubber/linux/block/partitions/efi.c: 232
 * Returns number of bytes read on success, 0 on error. /Users/rubber/linux/block/partitions/efi.c: 233
 * alloc_read_gpt_entries(): reads partition entries from disk /Users/rubber/linux/block/partitions/efi.c: 263
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 264
 * @gpt: GPT header /Users/rubber/linux/block/partitions/efi.c: 265
 * Description: Returns ptes on success,  NULL on error. /Users/rubber/linux/block/partitions/efi.c: 267
 * Allocates space for PTEs based on information found in @gpt. /Users/rubber/linux/block/partitions/efi.c: 268
 * Notes: remember to free pte when you're done! /Users/rubber/linux/block/partitions/efi.c: 269
 * alloc_read_gpt_header(): Allocates GPT header, reads into it from disk /Users/rubber/linux/block/partitions/efi.c: 298
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 299
 * @lba: the Logical Block Address of the partition table /Users/rubber/linux/block/partitions/efi.c: 300
 * Description: returns GPT header on success, NULL on error.   Allocates /Users/rubber/linux/block/partitions/efi.c: 302
 * and fills a GPT header starting at @ from @state->disk. /Users/rubber/linux/block/partitions/efi.c: 303
 * Note: remember to free gpt when finished with it. /Users/rubber/linux/block/partitions/efi.c: 304
 * is_gpt_valid() - tests one GPT header and PTEs for validity /Users/rubber/linux/block/partitions/efi.c: 326
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 327
 * @lba: logical block address of the GPT header to test /Users/rubber/linux/block/partitions/efi.c: 328
 * @gpt: GPT header ptr, filled on return. /Users/rubber/linux/block/partitions/efi.c: 329
 * @ptes: PTEs ptr, filled on return. /Users/rubber/linux/block/partitions/efi.c: 330
 * Description: returns 1 if valid,  0 on error. /Users/rubber/linux/block/partitions/efi.c: 332
 * If valid, returns pointers to newly allocated GPT header and PTEs. /Users/rubber/linux/block/partitions/efi.c: 333
	/* Check that the my_lba entry points to the LBA that contains /Users/rubber/linux/block/partitions/efi.c: 384
	/* Check the first_usable_lba and last_usable_lba are /Users/rubber/linux/block/partitions/efi.c: 393
	 * within the disk. /Users/rubber/linux/block/partitions/efi.c: 394
 * is_pte_valid() - tests one PTE for validity /Users/rubber/linux/block/partitions/efi.c: 454
 * @pte:pte to check /Users/rubber/linux/block/partitions/efi.c: 455
 * @lastlba: last lba of the disk /Users/rubber/linux/block/partitions/efi.c: 456
 * Description: returns 1 if valid,  0 on error. /Users/rubber/linux/block/partitions/efi.c: 458
 * compare_gpts() - Search disk for valid GPT headers and PTEs /Users/rubber/linux/block/partitions/efi.c: 471
 * @pgpt: primary GPT header /Users/rubber/linux/block/partitions/efi.c: 472
 * @agpt: alternate GPT header /Users/rubber/linux/block/partitions/efi.c: 473
 * @lastlba: last LBA number /Users/rubber/linux/block/partitions/efi.c: 474
 * Description: Returns nothing.  Sanity checks pgpt and agpt fields /Users/rubber/linux/block/partitions/efi.c: 476
 * and prints warnings on discrepancies. /Users/rubber/linux/block/partitions/efi.c: 477
 * find_valid_gpt() - Search disk for valid GPT headers and PTEs /Users/rubber/linux/block/partitions/efi.c: 566
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 567
 * @gpt: GPT header ptr, filled on return. /Users/rubber/linux/block/partitions/efi.c: 568
 * @ptes: PTEs ptr, filled on return. /Users/rubber/linux/block/partitions/efi.c: 569
 * Description: Returns 1 if valid, 0 on error. /Users/rubber/linux/block/partitions/efi.c: 571
 * If valid, returns pointers to newly allocated GPT header and PTEs. /Users/rubber/linux/block/partitions/efi.c: 572
 * Validity depends on PMBR being valid (or being overridden by the /Users/rubber/linux/block/partitions/efi.c: 573
 * 'gpt' kernel command line option) and finding either the Primary /Users/rubber/linux/block/partitions/efi.c: 574
 * GPT header and PTEs valid, or the Alternate GPT header and PTEs /Users/rubber/linux/block/partitions/efi.c: 575
 * valid.  If the Primary GPT header is not valid, the Alternate GPT header /Users/rubber/linux/block/partitions/efi.c: 576
 * is not checked unless the 'gpt' kernel command line option is passed. /Users/rubber/linux/block/partitions/efi.c: 577
 * This protects against devices which misreport their size, and forces /Users/rubber/linux/block/partitions/efi.c: 578
 * the user to decide to use the Alternate GPT. /Users/rubber/linux/block/partitions/efi.c: 579
 * utf16_le_to_7bit(): Naively converts a UTF-16LE string to 7-bit ASCII characters /Users/rubber/linux/block/partitions/efi.c: 670
 * @in: input UTF-16LE string /Users/rubber/linux/block/partitions/efi.c: 671
 * @size: size of the input string /Users/rubber/linux/block/partitions/efi.c: 672
 * @out: output string ptr, should be capable to store @size+1 characters /Users/rubber/linux/block/partitions/efi.c: 673
 * Description: Converts @size UTF16-LE symbols from @in string to 7-bit /Users/rubber/linux/block/partitions/efi.c: 675
 * ASCII characters and stores them to @out. Adds trailing zero to @out array. /Users/rubber/linux/block/partitions/efi.c: 676
 * efi_partition - scan for GPT partitions /Users/rubber/linux/block/partitions/efi.c: 695
 * @state: disk parsed partitions /Users/rubber/linux/block/partitions/efi.c: 696
 * Description: called from check.c, if the disk contains GPT /Users/rubber/linux/block/partitions/efi.c: 698
 * partitions, sets up partition entries in the kernel. /Users/rubber/linux/block/partitions/efi.c: 699
 * If the first block on the disk is a legacy MBR, /Users/rubber/linux/block/partitions/efi.c: 701
 * it will get handled by msdos_partition(). /Users/rubber/linux/block/partitions/efi.c: 702
 * If it's a Protective MBR, we'll handle it here. /Users/rubber/linux/block/partitions/efi.c: 703
 * We do not create a Linux partition for GPT, but /Users/rubber/linux/block/partitions/efi.c: 705
 * only for the actual data partitions. /Users/rubber/linux/block/partitions/efi.c: 706
 * Returns: /Users/rubber/linux/block/partitions/efi.c: 707
 * -1 if unable to read the partition table /Users/rubber/linux/block/partitions/efi.c: 708
 *  0 if this isn't our partition table /Users/rubber/linux/block/partitions/efi.c: 709
 *  1 if successful /Users/rubber/linux/block/partitions/efi.c: 710
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/atari.c: 1
 *  fs/partitions/atari.c /Users/rubber/linux/block/partitions/atari.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/atari.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/atari.c: 7
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/atari.c: 8
/* ++guenther: this should be settable by the user ("make config")?. /Users/rubber/linux/block/partitions/atari.c: 15
/* check if a partition entry looks valid -- Atari format is assumed if at /Users/rubber/linux/block/partitions/atari.c: 19
	/* /Users/rubber/linux/block/partitions/atari.c: 46
	 * ATARI partition scheme supports 512 lba only.  If this is not /Users/rubber/linux/block/partitions/atari.c: 47
	 * the case, bail early to avoid miscalculating hd_size. /Users/rubber/linux/block/partitions/atari.c: 48
		/* /Users/rubber/linux/block/partitions/atari.c: 63
		 * if there's no valid primary partition, assume that no Atari /Users/rubber/linux/block/partitions/atari.c: 64
		 * format partition table (there's no reliable magic or the like /Users/rubber/linux/block/partitions/atari.c: 65
	         * :-() /Users/rubber/linux/block/partitions/atari.c: 66
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/ibm.c: 1
 * Author(s)......: Holger Smolinski <Holger.Smolinski@de.ibm.com> /Users/rubber/linux/block/partitions/ibm.c: 3
 *                  Volker Sameske <sameske@de.ibm.com> /Users/rubber/linux/block/partitions/ibm.c: 4
 * Bugreports.to..: <Linux390@de.ibm.com> /Users/rubber/linux/block/partitions/ibm.c: 5
 * Copyright IBM Corp. 1999, 2012 /Users/rubber/linux/block/partitions/ibm.c: 6
 * compute the block number from a /Users/rubber/linux/block/partitions/ibm.c: 28
 * cyl-cyl-head-head structure /Users/rubber/linux/block/partitions/ibm.c: 29
 * compute the block number from a /Users/rubber/linux/block/partitions/ibm.c: 46
 * cyl-cyl-head-head-block structure /Users/rubber/linux/block/partitions/ibm.c: 47
	/* There a three places where we may find a valid label: /Users/rubber/linux/block/partitions/ibm.c: 80
	 * - on an ECKD disk it's block 2 /Users/rubber/linux/block/partitions/ibm.c: 81
	 * - on an FBA disk it's block 1 /Users/rubber/linux/block/partitions/ibm.c: 82
	 * - on an CMS formatted FBA disk it is sector 1, even if the block size /Users/rubber/linux/block/partitions/ibm.c: 83
	 *   is larger than 512 bytes (possible if the DIAG discipline is used) /Users/rubber/linux/block/partitions/ibm.c: 84
	 * If we have a valid info structure, then we know exactly which case we /Users/rubber/linux/block/partitions/ibm.c: 85
	 * have, otherwise we just search through all possebilities. /Users/rubber/linux/block/partitions/ibm.c: 86
	/* /Users/rubber/linux/block/partitions/ibm.c: 150
	 * get start of VTOC from the disk label and then search for format1 /Users/rubber/linux/block/partitions/ibm.c: 151
	 * and format8 labels /Users/rubber/linux/block/partitions/ibm.c: 152
		/* /Users/rubber/linux/block/partitions/ibm.c: 214
		 * Formated w/o large volume support. If the sanity check /Users/rubber/linux/block/partitions/ibm.c: 215
		 * 'size based on geo == size based on nr_sectors' is true, then /Users/rubber/linux/block/partitions/ibm.c: 216
		 * we can safely assume that we know the formatted size of /Users/rubber/linux/block/partitions/ibm.c: 217
		 * the disk, otherwise we need additional information /Users/rubber/linux/block/partitions/ibm.c: 218
		 * that we can only get from a real DASD device. /Users/rubber/linux/block/partitions/ibm.c: 219
	/* /Users/rubber/linux/block/partitions/ibm.c: 253
	 * VM style CMS1 labeled disk /Users/rubber/linux/block/partitions/ibm.c: 254
		/* /Users/rubber/linux/block/partitions/ibm.c: 267
		 * Special case for FBA devices: /Users/rubber/linux/block/partitions/ibm.c: 268
		 * If an FBA device is CMS formatted with blocksize > 512 byte /Users/rubber/linux/block/partitions/ibm.c: 269
		 * and the DIAG discipline is used, then the CMS label is found /Users/rubber/linux/block/partitions/ibm.c: 270
		 * in sector 1 instead of block 1. However, the partition is /Users/rubber/linux/block/partitions/ibm.c: 271
		 * still supposed to start in block 2. /Users/rubber/linux/block/partitions/ibm.c: 272
 * This is the main function, called by check.c /Users/rubber/linux/block/partitions/ibm.c: 288
		/* /Users/rubber/linux/block/partitions/ibm.c: 347
		 * ugly but needed for backward compatibility: /Users/rubber/linux/block/partitions/ibm.c: 348
		 * If the block device is a DASD (i.e. BIODASDINFO2 works), /Users/rubber/linux/block/partitions/ibm.c: 349
		 * then we claim it in any case, even though it has no valid /Users/rubber/linux/block/partitions/ibm.c: 350
		 * label. If it has the LDL format, then we simply define a /Users/rubber/linux/block/partitions/ibm.c: 351
		 * partition as if it had an LNX1 label. /Users/rubber/linux/block/partitions/ibm.c: 352
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/core.c: 1
 * Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/core.c: 3
 * Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/core.c: 4
 * Copyright (C) 2020 Christoph Hellwig /Users/rubber/linux/block/partitions/core.c: 5
	/* /Users/rubber/linux/block/partitions/core.c: 18
	 * Probe partition formats with tables at disk address 0 /Users/rubber/linux/block/partitions/core.c: 19
	 * that also have an ADFS boot block at 0xdc0. /Users/rubber/linux/block/partitions/core.c: 20
	/* /Users/rubber/linux/block/partitions/core.c: 32
	 * Now move on to formats that only have partition info at /Users/rubber/linux/block/partitions/core.c: 33
	 * disk address 0xdc0.  Since these may also have stale /Users/rubber/linux/block/partitions/core.c: 34
	 * PC/BIOS partition tables, they need to come before /Users/rubber/linux/block/partitions/core.c: 35
	 * the msdos entry. /Users/rubber/linux/block/partitions/core.c: 36
			/* /Users/rubber/linux/block/partitions/core.c: 151
			 * We have hit an I/O error which we don't report now. /Users/rubber/linux/block/partitions/core.c: 152
			 * But record it, and let the others do their job. /Users/rubber/linux/block/partitions/core.c: 153
	/* /Users/rubber/linux/block/partitions/core.c: 168
	 * The partition is unrecognized. So report I/O errors if there were any /Users/rubber/linux/block/partitions/core.c: 169
	/* /Users/rubber/linux/block/partitions/core.c: 296
	 * Remove the block device from the inode hash, so that it cannot be /Users/rubber/linux/block/partitions/core.c: 297
	 * looked up any more even when openers still hold references. /Users/rubber/linux/block/partitions/core.c: 298
 * Must be called either with open_mutex held, before a disk can be opened or /Users/rubber/linux/block/partitions/core.c: 313
 * after all disk users are gone. /Users/rubber/linux/block/partitions/core.c: 314
	/* /Users/rubber/linux/block/partitions/core.c: 332
	 * Partitions are not supported on zoned block devices that are used as /Users/rubber/linux/block/partitions/core.c: 333
	 * such. /Users/rubber/linux/block/partitions/core.c: 334
		/* /Users/rubber/linux/block/partitions/core.c: 582
		 * We can not ignore partitions of broken tables created by for /Users/rubber/linux/block/partitions/core.c: 583
		 * example camera firmware, but we limit them to the end of the /Users/rubber/linux/block/partitions/core.c: 584
		 * disk to avoid creating invalid block devices. /Users/rubber/linux/block/partitions/core.c: 585
		/* /Users/rubber/linux/block/partitions/core.c: 617
		 * I/O error reading the partition table.  If we tried to read /Users/rubber/linux/block/partitions/core.c: 618
		 * beyond EOD, retry after unlocking the native capacity. /Users/rubber/linux/block/partitions/core.c: 619
	/* /Users/rubber/linux/block/partitions/core.c: 630
	 * Partitions are not supported on host managed zoned block devices. /Users/rubber/linux/block/partitions/core.c: 631
	/* /Users/rubber/linux/block/partitions/core.c: 640
	 * If we read beyond EOD, try unlocking native capacity even if the /Users/rubber/linux/block/partitions/core.c: 641
	 * partition table was successfully read as we could be missing some /Users/rubber/linux/block/partitions/core.c: 642
	 * partitions. /Users/rubber/linux/block/partitions/core.c: 643
	/* /Users/rubber/linux/block/partitions/core.c: 684
	 * Historically we only set the capacity to zero for devices that /Users/rubber/linux/block/partitions/core.c: 685
	 * support partitions (independ of actually having partitions created). /Users/rubber/linux/block/partitions/core.c: 686
	 * Doing that is rather inconsistent, but changing it broke legacy /Users/rubber/linux/block/partitions/core.c: 687
	 * udisks polling for legacy ide-cdrom devices.  Use the crude check /Users/rubber/linux/block/partitions/core.c: 688
	 * below to get the sane behavior for most device while not breaking /Users/rubber/linux/block/partitions/core.c: 689
	 * userspace for this particular setup. /Users/rubber/linux/block/partitions/core.c: 690
		/* /Users/rubber/linux/block/partitions/core.c: 703
		 * Tell userspace that the media / partition table may have /Users/rubber/linux/block/partitions/core.c: 704
		 * changed. /Users/rubber/linux/block/partitions/core.c: 705
 * Only exported for loop and dasd for historic reasons.  Don't use in new /Users/rubber/linux/block/partitions/core.c: 713
 * code! /Users/rubber/linux/block/partitions/core.c: 714
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/msdos.c: 1
 *  fs/partitions/msdos.c /Users/rubber/linux/block/partitions/msdos.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/msdos.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/msdos.c: 6
 *  Thanks to Branko Lankester, lankeste@fwi.uva.nl, who found a bug /Users/rubber/linux/block/partitions/msdos.c: 8
 *  in the early extended-partition checks and added DM partitions /Users/rubber/linux/block/partitions/msdos.c: 9
 *  Support for DiskManager v6.0x added by Mark Lord, /Users/rubber/linux/block/partitions/msdos.c: 11
 *  with information provided by OnTrack.  This now works for linux fdisk /Users/rubber/linux/block/partitions/msdos.c: 12
 *  and LILO, as well as loadlin and bootln.  Note that disks other than /Users/rubber/linux/block/partitions/msdos.c: 13
 *  /dev/hda *must* have a "DOS" type 0x51 partition in the first slot (hda1). /Users/rubber/linux/block/partitions/msdos.c: 14
 *  More flexible handling of extended partitions - aeb, 950831 /Users/rubber/linux/block/partitions/msdos.c: 16
 *  Check partition table on IDE disks for common CHS translations /Users/rubber/linux/block/partitions/msdos.c: 18
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/msdos.c: 20
 *  BSD disklabel support by Yossi Gottlieb <yogo@math.tau.ac.il> /Users/rubber/linux/block/partitions/msdos.c: 22
 *  updated by Marc Espie <Marc.Espie@openbsd.org> /Users/rubber/linux/block/partitions/msdos.c: 23
 *  Unixware slices support by Andrzej Krzysztofowicz <ankry@mif.pg.gda.pl> /Users/rubber/linux/block/partitions/msdos.c: 25
 *  and Krzysztof G. Baranowski <kgb@knm.org.pl> /Users/rubber/linux/block/partitions/msdos.c: 26
 * Many architectures don't like unaligned accesses, while /Users/rubber/linux/block/partitions/msdos.c: 35
 * the nr_sects and start_sect partition table entries are /Users/rubber/linux/block/partitions/msdos.c: 36
 * at a 2 (mod 4) address. /Users/rubber/linux/block/partitions/msdos.c: 37
	/* /Users/rubber/linux/block/partitions/msdos.c: 85
	 * Assume the partition table is valid if Linux partitions exists. /Users/rubber/linux/block/partitions/msdos.c: 86
	 * Note that old Solaris/x86 partitions use the same indicator as /Users/rubber/linux/block/partitions/msdos.c: 87
	 * Linux swap partitions, so we consider that a Linux partition as /Users/rubber/linux/block/partitions/msdos.c: 88
	 * well. /Users/rubber/linux/block/partitions/msdos.c: 89
 * Create devices for each logical partition in an extended partition. /Users/rubber/linux/block/partitions/msdos.c: 120
 * The logical partitions form a linked list, with each entry being /Users/rubber/linux/block/partitions/msdos.c: 121
 * a partition table with two entries.  The first entry /Users/rubber/linux/block/partitions/msdos.c: 122
 * is the real data partition (with a start relative to the partition /Users/rubber/linux/block/partitions/msdos.c: 123
 * table start).  The second is a pointer to the next logical partition /Users/rubber/linux/block/partitions/msdos.c: 124
 * (with a start relative to the entire extended partition). /Users/rubber/linux/block/partitions/msdos.c: 125
 * We do not create a Linux partition for the partition tables, but /Users/rubber/linux/block/partitions/msdos.c: 126
 * only for the actual data partitions. /Users/rubber/linux/block/partitions/msdos.c: 127
	int loopct = 0;		/* number of links followed /Users/rubber/linux/block/partitions/msdos.c: 139
		/* /Users/rubber/linux/block/partitions/msdos.c: 161
		 * Usually, the first entry is the real data partition, /Users/rubber/linux/block/partitions/msdos.c: 162
		 * the 2nd entry is the next extended partition, or empty, /Users/rubber/linux/block/partitions/msdos.c: 163
		 * and the 3rd and 4th entries are unused. /Users/rubber/linux/block/partitions/msdos.c: 164
		 * However, DRDOS sometimes has the extended partition as /Users/rubber/linux/block/partitions/msdos.c: 165
		 * the first entry (when the data partition is empty), /Users/rubber/linux/block/partitions/msdos.c: 166
		 * and OS/2 seems to use all four entries. /Users/rubber/linux/block/partitions/msdos.c: 167
		/* /Users/rubber/linux/block/partitions/msdos.c: 170
		 * First process the data partition(s) /Users/rubber/linux/block/partitions/msdos.c: 171
			/* Check the 3rd and 4th entries - /Users/rubber/linux/block/partitions/msdos.c: 179
		/* /Users/rubber/linux/block/partitions/msdos.c: 201
		 * Next, process the (first) extended partition, if present. /Users/rubber/linux/block/partitions/msdos.c: 202
		 * (So far, there seems to be no reason to make /Users/rubber/linux/block/partitions/msdos.c: 203
		 *  parse_extended()  recursive and allow a tree /Users/rubber/linux/block/partitions/msdos.c: 204
		 *  of extended partitions.) /Users/rubber/linux/block/partitions/msdos.c: 205
		 * It should be a link to the next logical partition. /Users/rubber/linux/block/partitions/msdos.c: 206
/* james@bpgc.com: Solaris has a nasty indicator: 0x82 which also /Users/rubber/linux/block/partitions/msdos.c: 247
		/* solaris partitions are relative to current MS-DOS /Users/rubber/linux/block/partitions/msdos.c: 291
 * Create devices for BSD partitions listed in a disklabel, under a /Users/rubber/linux/block/partitions/msdos.c: 352
 * dos-like partition. See parse_extended() for more information. /Users/rubber/linux/block/partitions/msdos.c: 353
 * Create devices for Unixware partitions listed in a disklabel, under a /Users/rubber/linux/block/partitions/msdos.c: 480
 * dos-like partition. See parse_extended() for more information. /Users/rubber/linux/block/partitions/msdos.c: 481
 * Minix 2.0.0/2.0.2 subpartition support. /Users/rubber/linux/block/partitions/msdos.c: 525
 * Anand Krishnamurthy <anandk@wiproge.med.ge.com> /Users/rubber/linux/block/partitions/msdos.c: 526
 * Rajeev V. Pillai    <rajeevvp@yahoo.com> /Users/rubber/linux/block/partitions/msdos.c: 527
	/* The first sector of a Minix partition can have either /Users/rubber/linux/block/partitions/msdos.c: 544
	 * a secondary MBR describing its subpartitions, or /Users/rubber/linux/block/partitions/msdos.c: 545
	/* /Users/rubber/linux/block/partitions/msdos.c: 596
	 * Note order! (some AIX disks, e.g. unbootable kind, /Users/rubber/linux/block/partitions/msdos.c: 597
	 * have no MSDOS 55aa) /Users/rubber/linux/block/partitions/msdos.c: 598
	/* /Users/rubber/linux/block/partitions/msdos.c: 615
	 * Now that the 55aa signature is present, this is probably /Users/rubber/linux/block/partitions/msdos.c: 616
	 * either the boot sector of a FAT filesystem or a DOS-type /Users/rubber/linux/block/partitions/msdos.c: 617
	 * partition table. Reject this in case the boot indicator /Users/rubber/linux/block/partitions/msdos.c: 618
	 * is not 0 or 0x80. /Users/rubber/linux/block/partitions/msdos.c: 619
			/* /Users/rubber/linux/block/partitions/msdos.c: 624
			 * Even without a valid boot indicator value /Users/rubber/linux/block/partitions/msdos.c: 625
			 * its still possible this is valid FAT filesystem /Users/rubber/linux/block/partitions/msdos.c: 626
			 * without a partition table. /Users/rubber/linux/block/partitions/msdos.c: 627
	/* /Users/rubber/linux/block/partitions/msdos.c: 656
	 * Look for partitions in two passes: /Users/rubber/linux/block/partitions/msdos.c: 657
	 * First find the primary and DOS-type extended partitions. /Users/rubber/linux/block/partitions/msdos.c: 658
	 * On the second pass look inside *BSD, Unixware and Solaris partitions. /Users/rubber/linux/block/partitions/msdos.c: 659
			/* /Users/rubber/linux/block/partitions/msdos.c: 670
			 * prevent someone doing mkfs or mkswap on an /Users/rubber/linux/block/partitions/msdos.c: 671
			 * extended partition, but leave room for LILO /Users/rubber/linux/block/partitions/msdos.c: 672
			 * FIXME: this uses one logical sector for > 512b /Users/rubber/linux/block/partitions/msdos.c: 673
			 * sector, although it may not be enough/proper. /Users/rubber/linux/block/partitions/msdos.c: 674
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/sysv68.c: 1
 *  fs/partitions/sysv68.c /Users/rubber/linux/block/partitions/sysv68.c: 3
 *  Copyright (C) 2007 Philippe De Muyter <phdm@macqel.be> /Users/rubber/linux/block/partitions/sysv68.c: 5
 *	Volume ID structure: on first 256-bytes sector of disk /Users/rubber/linux/block/partitions/sysv68.c: 11
 *	config block: second 256-bytes sector on disk /Users/rubber/linux/block/partitions/sysv68.c: 20
 *	combined volumeid and dkconfig block /Users/rubber/linux/block/partitions/sysv68.c: 31
 *	Slice Table Structure /Users/rubber/linux/block/partitions/sysv68.c: 40
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/cmdline.c: 1
 * Copyright (C) 2013 HUAWEI /Users/rubber/linux/block/partitions/cmdline.c: 3
 * Author: Cai Zhiyong <caizhiyong@huawei.com> /Users/rubber/linux/block/partitions/cmdline.c: 4
 * Read block device partition table from the command line. /Users/rubber/linux/block/partitions/cmdline.c: 6
 * Typically used for fixed block (eMMC) embedded devices. /Users/rubber/linux/block/partitions/cmdline.c: 7
 * It has no MBR, so saves storage space. Bootloader can be easily accessed /Users/rubber/linux/block/partitions/cmdline.c: 8
 * by absolute address of data on the block device. /Users/rubber/linux/block/partitions/cmdline.c: 9
 * Users can easily change the partition. /Users/rubber/linux/block/partitions/cmdline.c: 10
 * The format for the command line is just like mtdparts. /Users/rubber/linux/block/partitions/cmdline.c: 12
 * For further information, see "Documentation/block/cmdline-partition.rst" /Users/rubber/linux/block/partitions/cmdline.c: 14
 * Purpose: allocate cmdline partitions. /Users/rubber/linux/block/partitions/cmdline.c: 374
 * Returns: /Users/rubber/linux/block/partitions/cmdline.c: 375
 * -1 if unable to read the partition table /Users/rubber/linux/block/partitions/cmdline.c: 376
 *  0 if this isn't our partition table /Users/rubber/linux/block/partitions/cmdline.c: 377
 *  1 if successful /Users/rubber/linux/block/partitions/cmdline.c: 378
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/osf.c: 1
 *  fs/partitions/osf.c /Users/rubber/linux/block/partitions/osf.c: 3
 *  Code extracted from drivers/block/genhd.c /Users/rubber/linux/block/partitions/osf.c: 5
 *  Copyright (C) 1991-1998  Linus Torvalds /Users/rubber/linux/block/partitions/osf.c: 7
 *  Re-organised Feb 1998 Russell King /Users/rubber/linux/block/partitions/osf.c: 8
 SPDX-License-Identifier: GPL-2.0 /Users/rubber/linux/block/partitions/acorn.c: 1
 *  Copyright (c) 1996-2000 Russell King. /Users/rubber/linux/block/partitions/acorn.c: 3
 *  Scan ADFS partitions on hard disk drives.  Unfortunately, there /Users/rubber/linux/block/partitions/acorn.c: 5
 *  isn't a standard for partitioning drives on Acorn machines, so /Users/rubber/linux/block/partitions/acorn.c: 6
 *  every single manufacturer of SCSI and IDE cards created their own /Users/rubber/linux/block/partitions/acorn.c: 7
 *  method. /Users/rubber/linux/block/partitions/acorn.c: 8
 * Partition types. (Oh for reusability) /Users/rubber/linux/block/partitions/acorn.c: 16
	/* /Users/rubber/linux/block/partitions/acorn.c: 169
	 * Try Cumana style partitions - sector 6 contains ADFS boot block /Users/rubber/linux/block/partitions/acorn.c: 170
	 * with pointer to next 'drive'. /Users/rubber/linux/block/partitions/acorn.c: 171
	 * /Users/rubber/linux/block/partitions/acorn.c: 172
	 * There are unknowns in this code - is the 'cylinder number' of the /Users/rubber/linux/block/partitions/acorn.c: 173
	 * next partition relative to the start of this one - I'm assuming /Users/rubber/linux/block/partitions/acorn.c: 174
	 * it is. /Users/rubber/linux/block/partitions/acorn.c: 175
	 * /Users/rubber/linux/block/partitions/acorn.c: 176
	 * Also, which ID did Cumana use? /Users/rubber/linux/block/partitions/acorn.c: 177
	 * /Users/rubber/linux/block/partitions/acorn.c: 178
	 * This is totally unfinished, and will require more work to get it /Users/rubber/linux/block/partitions/acorn.c: 179
	 * going. Hence it is totally untested. /Users/rubber/linux/block/partitions/acorn.c: 180
 * Purpose: allocate ADFS partitions. /Users/rubber/linux/block/partitions/acorn.c: 239
 * Params : hd		- pointer to gendisk structure to store partition info. /Users/rubber/linux/block/partitions/acorn.c: 241
 *	    dev		- device number to access. /Users/rubber/linux/block/partitions/acorn.c: 242
 * Returns: -1 on error, 0 for no ADFS boot sector, 1 for ok. /Users/rubber/linux/block/partitions/acorn.c: 244
 * Alloc  : hda  = whole drive /Users/rubber/linux/block/partitions/acorn.c: 246
 *	    hda1 = ADFS partition on first drive. /Users/rubber/linux/block/partitions/acorn.c: 247
 *	    hda2 = non-ADFS partition. /Users/rubber/linux/block/partitions/acorn.c: 248
	/* /Users/rubber/linux/block/partitions/acorn.c: 275
	 * Work out start of non-adfs partition. /Users/rubber/linux/block/partitions/acorn.c: 276
 * Check for a valid ICS partition using the checksum. /Users/rubber/linux/block/partitions/acorn.c: 325
 * Purpose: allocate ICS partitions. /Users/rubber/linux/block/partitions/acorn.c: 341
 * Params : hd		- pointer to gendisk structure to store partition info. /Users/rubber/linux/block/partitions/acorn.c: 342
 *	    dev		- device number to access. /Users/rubber/linux/block/partitions/acorn.c: 343
 * Returns: -1 on error, 0 for no ICS table, 1 for partitions ok. /Users/rubber/linux/block/partitions/acorn.c: 344
 * Alloc  : hda  = whole drive /Users/rubber/linux/block/partitions/acorn.c: 345
 *	    hda1 = ADFS partition 0 on first drive. /Users/rubber/linux/block/partitions/acorn.c: 346
 *	    hda2 = ADFS partition 1 on first drive. /Users/rubber/linux/block/partitions/acorn.c: 347
 *		..etc.. /Users/rubber/linux/block/partitions/acorn.c: 348
	/* /Users/rubber/linux/block/partitions/acorn.c: 357
	 * Try ICS style partitions - sector 0 contains partition info. /Users/rubber/linux/block/partitions/acorn.c: 358
		/* /Users/rubber/linux/block/partitions/acorn.c: 378
		 * Negative sizes tell the RISC OS ICS driver to ignore /Users/rubber/linux/block/partitions/acorn.c: 379
		 * this partition - in effect it says that this does not /Users/rubber/linux/block/partitions/acorn.c: 380
		 * contain an ADFS filesystem. /Users/rubber/linux/block/partitions/acorn.c: 381
			/* /Users/rubber/linux/block/partitions/acorn.c: 386
			 * Our own extension - We use the first sector /Users/rubber/linux/block/partitions/acorn.c: 387
			 * of the partition to identify what type this /Users/rubber/linux/block/partitions/acorn.c: 388
			 * partition is.  We must not make this visible /Users/rubber/linux/block/partitions/acorn.c: 389
			 * to the filesystem. /Users/rubber/linux/block/partitions/acorn.c: 390
	/* /Users/rubber/linux/block/partitions/acorn.c: 423
	 * If it looks like a PC/BIOS partition, then it /Users/rubber/linux/block/partitions/acorn.c: 424
	 * probably isn't PowerTec. /Users/rubber/linux/block/partitions/acorn.c: 425
 * Purpose: allocate ICS partitions. /Users/rubber/linux/block/partitions/acorn.c: 437
 * Params : hd		- pointer to gendisk structure to store partition info. /Users/rubber/linux/block/partitions/acorn.c: 438
 *	    dev		- device number to access. /Users/rubber/linux/block/partitions/acorn.c: 439
 * Returns: -1 on error, 0 for no ICS table, 1 for partitions ok. /Users/rubber/linux/block/partitions/acorn.c: 440
 * Alloc  : hda  = whole drive /Users/rubber/linux/block/partitions/acorn.c: 441
 *	    hda1 = ADFS partition 0 on first drive. /Users/rubber/linux/block/partitions/acorn.c: 442
 *	    hda2 = ADFS partition 1 on first drive. /Users/rubber/linux/block/partitions/acorn.c: 443
 *		..etc.. /Users/rubber/linux/block/partitions/acorn.c: 444
 * Guess who created this format? /Users/rubber/linux/block/partitions/acorn.c: 490
 * EESOX SCSI partition format. /Users/rubber/linux/block/partitions/acorn.c: 498
 * This is a goddamned awful partition format.  We don't seem to store /Users/rubber/linux/block/partitions/acorn.c: 500
 * the size of the partition in this table, only the start addresses. /Users/rubber/linux/block/partitions/acorn.c: 501
 * There are two possibilities where the size comes from: /Users/rubber/linux/block/partitions/acorn.c: 503
 *  1. The individual ADFS boot block entries that are placed on the disk. /Users/rubber/linux/block/partitions/acorn.c: 504
 *  2. The start address of the next entry. /Users/rubber/linux/block/partitions/acorn.c: 505
	/* /Users/rubber/linux/block/partitions/acorn.c: 520
	 * "Decrypt" the partition table.  God knows why... /Users/rubber/linux/block/partitions/acorn.c: 521
