 SPDX-License-Identifier: GPL-2.0-only

/*

 * I2C bus interface to Cirrus Logic Madera codecs

 *

 * Copyright (C) 2015-2018 Cirrus Logic

 it's polite to say which codec isn't built into the kernel */

 SPDX-License-Identifier: GPL-2.0+



 Copyright (c) 2011-2014 Samsung Electronics Co., Ltd

              http:
	/*

	 * The rtc-s5m driver requests S2MPS14_IRQ_RTCA0 also for S2MPS11

	 * so the interrupt number must be consistent.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2012 ARM Limited

 The sysreg block is just a random collection of various functions... */

	/*

	 * Duplicated SYS_MCI pseudo-GPIO controller for compatibility with

	 * older trees using sysreg node for MMC control lines.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DA9150 Core MFD Driver

 *

 * Copyright (c) 2014 Dialog Semiconductor

 *

 * Author: Adam Thomson <Adam.Thomson.Opensource@diasemi.com>

 Raw device access, used for QIF */

	/*

	 * Read is split into two transfers as device expects STOP/START rather

	 * than repeated start to carry out this kind of access.

 Write address */

 Read data */

 Write address & data */

 Setup secondary I2C interface for QIF access */

 Make sure we have a wakup source for the device */

 Set device to DISABLED mode */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Atmel SMC (Static Memory Controller) helper functions.

 *

 * Copyright (C) 2017 Atmel

 * Copyright (C) 2017 Free Electrons

 *

 * Author: Boris Brezillon <boris.brezillon@free-electrons.com>

/**

 * atmel_smc_cs_conf_init - initialize a SMC CS conf

 * @conf: the SMC CS conf to initialize

 *

 * Set all fields to 0 so that one can start defining a new config.

/**

 * atmel_smc_cs_encode_ncycles - encode a number of MCK clk cycles in the

 *				 format expected by the SMC engine

 * @ncycles: number of MCK clk cycles

 * @msbpos: position of the MSB part of the timing field

 * @msbwidth: width of the MSB part of the timing field

 * @msbfactor: factor applied to the MSB

 * @encodedval: param used to store the encoding result

 *

 * This function encodes the @ncycles value as described in the datasheet

 * (section "SMC Setup/Pulse/Cycle/Timings Register"). This is a generic

 * helper which called with different parameter depending on the encoding

 * scheme.

 *

 * If the @ncycles value is too big to be encoded, -ERANGE is returned and

 * the encodedval is contains the maximum val. Otherwise, 0 is returned.

	/*

	 * Let's just put the maximum we can if the requested setting does

	 * not fit in the register field.

	 * We still return -ERANGE in case the caller cares.

/**

 * atmel_smc_cs_conf_set_timing - set the SMC CS conf Txx parameter to a

 *				  specific value

 * @conf: SMC CS conf descriptor

 * @shift: the position of the Txx field in the TIMINGS register

 * @ncycles: value (expressed in MCK clk cycles) to assign to this Txx

 *	     parameter

 *

 * This function encodes the @ncycles value as described in the datasheet

 * (section "SMC Timings Register"), and then stores the result in the

 * @conf->timings field at @shift position.

 *

 * Returns -EINVAL if shift is invalid, -ERANGE if ncycles does not fit in

 * the field, and 0 otherwise.

	/*

	 * The formula described in atmel datasheets (section "HSMC Timings

	 * Register"):

	 *

	 * ncycles = (Txx[3] * 64) + Txx[2:0]

/**

 * atmel_smc_cs_conf_set_setup - set the SMC CS conf xx_SETUP parameter to a

 *				 specific value

 * @conf: SMC CS conf descriptor

 * @shift: the position of the xx_SETUP field in the SETUP register

 * @ncycles: value (expressed in MCK clk cycles) to assign to this xx_SETUP

 *	     parameter

 *

 * This function encodes the @ncycles value as described in the datasheet

 * (section "SMC Setup Register"), and then stores the result in the

 * @conf->setup field at @shift position.

 *

 * Returns -EINVAL if @shift is invalid, -ERANGE if @ncycles does not fit in

 * the field, and 0 otherwise.

	/*

	 * The formula described in atmel datasheets (section "SMC Setup

	 * Register"):

	 *

	 * ncycles = (128 * xx_SETUP[5]) + xx_SETUP[4:0]

/**

 * atmel_smc_cs_conf_set_pulse - set the SMC CS conf xx_PULSE parameter to a

 *				 specific value

 * @conf: SMC CS conf descriptor

 * @shift: the position of the xx_PULSE field in the PULSE register

 * @ncycles: value (expressed in MCK clk cycles) to assign to this xx_PULSE

 *	     parameter

 *

 * This function encodes the @ncycles value as described in the datasheet

 * (section "SMC Pulse Register"), and then stores the result in the

 * @conf->setup field at @shift position.

 *

 * Returns -EINVAL if @shift is invalid, -ERANGE if @ncycles does not fit in

 * the field, and 0 otherwise.

	/*

	 * The formula described in atmel datasheets (section "SMC Pulse

	 * Register"):

	 *

	 * ncycles = (256 * xx_PULSE[6]) + xx_PULSE[5:0]

/**

 * atmel_smc_cs_conf_set_cycle - set the SMC CS conf xx_CYCLE parameter to a

 *				 specific value

 * @conf: SMC CS conf descriptor

 * @shift: the position of the xx_CYCLE field in the CYCLE register

 * @ncycles: value (expressed in MCK clk cycles) to assign to this xx_CYCLE

 *	     parameter

 *

 * This function encodes the @ncycles value as described in the datasheet

 * (section "SMC Cycle Register"), and then stores the result in the

 * @conf->setup field at @shift position.

 *

 * Returns -EINVAL if @shift is invalid, -ERANGE if @ncycles does not fit in

 * the field, and 0 otherwise.

	/*

	 * The formula described in atmel datasheets (section "SMC Cycle

	 * Register"):

	 *

	 * ncycles = (xx_CYCLE[8:7] * 256) + xx_CYCLE[6:0]

/**

 * atmel_smc_cs_conf_apply - apply an SMC CS conf

 * @regmap: the SMC regmap

 * @cs: the CS id

 * @conf: the SMC CS conf to apply

 *

 * Applies an SMC CS configuration.

 * Only valid on at91sam9/avr32 SoCs.

/**

 * atmel_hsmc_cs_conf_apply - apply an SMC CS conf

 * @regmap: the HSMC regmap

 * @cs: the CS id

 * @layout: the layout of registers

 * @conf: the SMC CS conf to apply

 *

 * Applies an SMC CS configuration.

 * Only valid on post-sama5 SoCs.

/**

 * atmel_smc_cs_conf_get - retrieve the current SMC CS conf

 * @regmap: the SMC regmap

 * @cs: the CS id

 * @conf: the SMC CS conf object to store the current conf

 *

 * Retrieve the SMC CS configuration.

 * Only valid on at91sam9/avr32 SoCs.

/**

 * atmel_hsmc_cs_conf_get - retrieve the current SMC CS conf

 * @regmap: the HSMC regmap

 * @cs: the CS id

 * @layout: the layout of registers

 * @conf: the SMC CS conf object to store the current conf

 *

 * Retrieve the SMC CS configuration.

 * Only valid on post-sama5 SoCs.

 sentinel */ },

/**

 * atmel_hsmc_get_reg_layout - retrieve the layout of HSMC registers

 * @np: the HSMC regmap

 *

 * Retrieve the layout of HSMC registers.

 *

 * Returns NULL in case of SMC, a struct atmel_hsmc_reg_layout pointer

 * in HSMC case, otherwise ERR_PTR(-EINVAL).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I2C driver for Marvell 88PM860x

 *

 * Copyright (C) 2009 Marvell International Ltd.

 *

 * Author: Haojian Zhuang <haojian.zhuang@marvell.com>

 command */

 if data needs to read back, num should be 2 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * twl6030-irq.c - TWL6030 irq support

 *

 * Copyright (C) 2005-2009 Texas Instruments, Inc.

 *

 * Modifications to defer interrupt handling to a kernel thread:

 * Copyright (C) 2006 MontaVista Software, Inc.

 *

 * Based on tlv320aic23.c:

 * Copyright (c) by Kai Svahn <kai.svahn@nokia.com>

 *

 * Code cleanup and modifications to IRQ handler.

 * by syed khasim <x0khasim@ti.com>

 *

 * TWL6030 specific code and IRQ handling changes by

 * Jagadeesh Bhaskar Pakaravoor <j-pakaravoor@ti.com>

 * Balaji T K <balajitk@ti.com>

/*

 * TWL6030 (unlike its predecessors, which had two level interrupt handling)

 * three interrupt registers INT_STS_A, INT_STS_B and INT_STS_C.

 * It exposes status bits saying who has raised an interrupt. There are

 * three mask registers that corresponds to these status registers, that

 * enables/disables these interrupts.

 *

 * We set up IRQs starting at a platform-specified base. An interrupt map table,

 * specifies mapping between interrupt number and the associated module.

 Bit 0	PWRON			*/

 Bit 1	RPWRON			*/

 Bit 2	BAT_VLOW		*/

 Bit 3	RTC_ALARM		*/

 Bit 4	RTC_PERIOD		*/

 Bit 5	HOT_DIE			*/

 Bit 6	VXXX_SHORT		*/

 Bit 7	VMMC_SHORT		*/

 Bit 8	VUSIM_SHORT		*/

 Bit 9	BAT			*/

 Bit 10	SIM			*/

 Bit 11	MMC			*/

 Bit 12	Reserved		*/

 Bit 13	GPADC_RT_EOC		*/

 Bit 14	GPADC_SW_EOC		*/

 Bit 15	CC_AUTOCAL		*/

 Bit 16	ID_WKUP			*/

 Bit 17	VBUS_WKUP		*/

 Bit 18	ID			*/

 Bit 19	VBUS			*/

 Bit 20	CHRG_CTRL		*/

 Bit 21	EXT_CHRG	*/

 Bit 22	INT_CHRG	*/

 Bit 23	Reserved		*/

 Bit 0	PWRON			*/

 Bit 1	RPWRON			*/

 Bit 2	SYS_VLOW		*/

 Bit 3	RTC_ALARM		*/

 Bit 4	RTC_PERIOD		*/

 Bit 5	HOT_DIE			*/

 Bit 6	VXXX_SHORT		*/

 Bit 7	SPDURATION		*/

 Bit 8	WATCHDOG		*/

 Bit 9	BAT			*/

 Bit 10	SIM			*/

 Bit 11	MMC			*/

 Bit 12	GPADC_RT_EOC		*/

 Bit 13	GPADC_SW_EOC		*/

 Bit 14	CC_EOC			*/

 Bit 15	CC_AUTOCAL		*/

 Bit 16	ID_WKUP			*/

 Bit 17	VBUS_WKUP		*/

 Bit 18	ID			*/

 Bit 19	VBUS			*/

 Bit 20	CHRG_CTRL		*/

 Bit 21	EXT_CHRG	*/

 Bit 22	INT_CHRG	*/

 Bit 23	Reserved		*/

----------------------------------------------------------------------*/

/*

* Threaded irq handler for the twl6030 interrupt.

* We query the interrupt controller in the twl6030 to determine

* which module is generating the interrupt request and call

* handle_nested_irq for that module.

 sts.int_sts converted to CPU endianness */

 read INT_STS_A, B and C in one shot using a burst read */

 Only 24 bits are valid*/

	/*

	 * Since VBUS status bit is not reliable for VBUS disconnect

	 * use CHARGER VBUS detection status bit instead.

	/*

	 * NOTE:

	 * Simulation confirms that documentation is wrong w.r.t the

	 * interrupt status clear operation. A single *byte* write to

	 * any one of STS_A to STS_C register results in all three

	 * STS registers being reset. Since it does not matter which

	 * value is written, all three registers are cleared on a

	 * single byte write, so we just use 0x0 to clear.

----------------------------------------------------------------------*/

 unmask INT_MSK_A/B/C */

 mask INT_MSK_A/B/C */

 Unmasking the Card detect Interrupt line for MMC1 from Phoenix */

	/*

	 * Initially Configuring MMC_CTRL for receiving interrupts &

	 * Card status on TWL6030 for MMC1

 Configuring PullUp-PullDown register */

		/* TWL6030 provide's Card detect support for

		 * only MMC1 controller.

	/*

	 * BIT0 of MMC_CTRL on TWL6030 provides card status for MMC1

	 * 0 - Card not present ,1 - Card present

 mask all int lines */

 mask all int sts */

 clear INT_STS_A,B,C */

	/*

	 * install an irq handler for each of the modules;

	 * clone dummy irq_chip since PIH can't *do* anything

 install an irq handler to demultiplex the TWL6030 interrupt */

		/*

		 * TODO: IRQ domain and allocated nested IRQ descriptors

		 * should be freed somehow here. Now It can't be done, because

		 * child devices will not be deleted during removing of

		 * TWL Core driver and they will still contain allocated

		 * virt IRQs in their Resources tables.

		 * The same prevents us from using devm_request_threaded_irq()

		 * in this module.

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for Khadas System control Microcontroller

 *

 * Copyright (C) 2020 BayLibre SAS

 *

 * Author(s): Neil Armstrong <narmstrong@baylibre.com>

 VIM1/2 Rev13+ and VIM3 only */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * wm5102-tables.c  --  WM5102 data tables

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 We use a function so we can use ARRAY_SIZE() */

 R8     - Ctrl IF SPI CFG 1 */

 R9     - Ctrl IF I2C1 CFG 1 */

 R32    - Tone Generator 1 */

 R33    - Tone Generator 2 */

 R34    - Tone Generator 3 */

 R35    - Tone Generator 4 */

 R36    - Tone Generator 5 */

 R48    - PWM Drive 1 */

 R49    - PWM Drive 2 */

 R50    - PWM Drive 3 */

 R64    - Wake control */

 R65    - Sequence control */

 R97    - Sample Rate Sequence Select 1 */

 R98    - Sample Rate Sequence Select 2 */

 R99    - Sample Rate Sequence Select 3 */

 R100   - Sample Rate Sequence Select 4 */

 R102   - Always On Triggers Sequence Select 1 */

 R103   - Always On Triggers Sequence Select 2 */

 R104   - Always On Triggers Sequence Select 3 */

 R105   - Always On Triggers Sequence Select 4 */

 R106   - Always On Triggers Sequence Select 5 */

 R107   - Always On Triggers Sequence Select 6 */

 R112   - Comfort Noise Generator */

 R144   - Haptics Control 1 */

 R145   - Haptics Control 2 */

 R146   - Haptics phase 1 intensity */

 R147   - Haptics phase 1 duration */

 R148   - Haptics phase 2 intensity */

 R149   - Haptics phase 2 duration */

 R150   - Haptics phase 3 intensity */

 R151   - Haptics phase 3 duration */

 R256   - Clock 32k 1 */

 R257   - System Clock 1 */

 R258   - Sample rate 1 */

 R259   - Sample rate 2 */

 R260   - Sample rate 3 */

 R274   - Async clock 1 */

 R275   - Async sample rate 1 */

 R276   - Async sample rate 2 */

 R329   - Output system clock */

 R330   - Output async clock */

 R338   - Rate Estimator 1 */

 R339   - Rate Estimator 2 */

 R340   - Rate Estimator 3 */

 R341   - Rate Estimator 4 */

 R342   - Rate Estimator 5 */

 R353   - Dynamic Frequency Scaling 1 */

 R369   - FLL1 Control 1 */

 R370   - FLL1 Control 2 */

 R371   - FLL1 Control 3 */

 R372   - FLL1 Control 4 */

 R373   - FLL1 Control 5 */

 R374   - FLL1 Control 6 */

 R377   - FLL1 Control 7 */

 R385   - FLL1 Synchroniser 1 */

 R386   - FLL1 Synchroniser 2 */

 R387   - FLL1 Synchroniser 3 */

 R388   - FLL1 Synchroniser 4 */

 R389   - FLL1 Synchroniser 5 */

 R390   - FLL1 Synchroniser 6 */

 R391   - FLL1 Synchroniser 7 */

 R393   - FLL1 Spread Spectrum */

 R394   - FLL1 GPIO Clock */

 R401   - FLL2 Control 1 */

 R402   - FLL2 Control 2 */

 R403   - FLL2 Control 3 */

 R404   - FLL2 Control 4 */

 R405   - FLL2 Control 5 */

 R406   - FLL2 Control 6 */

 R409   - FLL2 Control 7 */

 R417   - FLL2 Synchroniser 1 */

 R418   - FLL2 Synchroniser 2 */

 R419   - FLL2 Synchroniser 3 */

 R420   - FLL2 Synchroniser 4 */

 R421   - FLL2 Synchroniser 5 */

 R422   - FLL2 Synchroniser 6 */

 R423   - FLL2 Synchroniser 7 */

 R425   - FLL2 Spread Spectrum */

 R426   - FLL2 GPIO Clock */

 R512   - Mic Charge Pump 1 */

 R528   - LDO1 Control 1 */

 R530   - LDO1 Control 2 */

 R531   - LDO2 Control 1 */

 R536   - Mic Bias Ctrl 1 */

 R537   - Mic Bias Ctrl 2 */

 R538   - Mic Bias Ctrl 3 */

 R659   - Accessory Detect Mode 1 */

 R667   - Headphone Detect 1 */

 R674   - Micd clamp control */

 R675   - Mic Detect 1 */

 R676   - Mic Detect 2 */

 R678   - Mic Detect Level 1 */

 R679   - Mic Detect Level 2 */

 R680   - Mic Detect Level 3 */

 R681   - Mic Detect Level 4 */

 R707   - Mic noise mix control 1 */

 R715   - Isolation control */

 R723   - Jack detect analogue */

 R768   - Input Enables */

 R776   - Input Rate */

 R777   - Input Volume Ramp */

 R784   - IN1L Control */

 R785   - ADC Digital Volume 1L */

 R786   - DMIC1L Control */

 R788   - IN1R Control */

 R789   - ADC Digital Volume 1R */

 R790   - DMIC1R Control */

 R792   - IN2L Control */

 R793   - ADC Digital Volume 2L */

 R794   - DMIC2L Control */

 R796   - IN2R Control */

 R797   - ADC Digital Volume 2R */

 R798   - DMIC2R Control */

 R800   - IN3L Control */

 R801   - ADC Digital Volume 3L */

 R802   - DMIC3L Control */

 R804   - IN3R Control */

 R805   - ADC Digital Volume 3R */

 R806   - DMIC3R Control */

 R1024  - Output Enables 1 */

 R1032  - Output Rate 1 */

 R1033  - Output Volume Ramp */

 R1040  - Output Path Config 1L */

 R1041  - DAC Digital Volume 1L */

 R1042  - DAC Volume Limit 1L */

 R1043  - Noise Gate Select 1L */

 R1044  - Output Path Config 1R */

 R1045  - DAC Digital Volume 1R */

 R1046  - DAC Volume Limit 1R */

 R1047  - Noise Gate Select 1R */

 R1048  - Output Path Config 2L */

 R1049  - DAC Digital Volume 2L */

 R1050  - DAC Volume Limit 2L */

 R1051  - Noise Gate Select 2L */

 R1052  - Output Path Config 2R */

 R1053  - DAC Digital Volume 2R */

 R1054  - DAC Volume Limit 2R */

 R1055  - Noise Gate Select 2R */

 R1056  - Output Path Config 3L */

 R1057  - DAC Digital Volume 3L */

 R1058  - DAC Volume Limit 3L */

 R1059  - Noise Gate Select 3L */

 R1064  - Output Path Config 4L */

 R1065  - DAC Digital Volume 4L */

 R1066  - Out Volume 4L */

 R1067  - Noise Gate Select 4L */

 R1069  - DAC Digital Volume 4R */

 R1070  - Out Volume 4R */

 R1071  - Noise Gate Select 4R */

 R1072  - Output Path Config 5L */

 R1073  - DAC Digital Volume 5L */

 R1074  - DAC Volume Limit 5L */

 R1075  - Noise Gate Select 5L */

 R1077  - DAC Digital Volume 5R */

 R1078  - DAC Volume Limit 5R */

 R1079  - Noise Gate Select 5R */

 R1088  - DRE Enable */

 R1090  - DRE Control 2 */

 R1090  - DRE Control 3 */

 R1104  - DAC AEC Control 1 */

 R1112  - Noise Gate Control */

 R1168  - PDM SPK1 CTRL 1 */

 R1169  - PDM SPK1 CTRL 2 */

 R1280  - AIF1 BCLK Ctrl */

 R1281  - AIF1 Tx Pin Ctrl */

 R1282  - AIF1 Rx Pin Ctrl */

 R1283  - AIF1 Rate Ctrl */

 R1284  - AIF1 Format */

 R1285  - AIF1 Tx BCLK Rate */

 R1286  - AIF1 Rx BCLK Rate */

 R1287  - AIF1 Frame Ctrl 1 */

 R1288  - AIF1 Frame Ctrl 2 */

 R1289  - AIF1 Frame Ctrl 3 */

 R1290  - AIF1 Frame Ctrl 4 */

 R1291  - AIF1 Frame Ctrl 5 */

 R1292  - AIF1 Frame Ctrl 6 */

 R1293  - AIF1 Frame Ctrl 7 */

 R1294  - AIF1 Frame Ctrl 8 */

 R1295  - AIF1 Frame Ctrl 9 */

 R1296  - AIF1 Frame Ctrl 10 */

 R1297  - AIF1 Frame Ctrl 11 */

 R1298  - AIF1 Frame Ctrl 12 */

 R1299  - AIF1 Frame Ctrl 13 */

 R1300  - AIF1 Frame Ctrl 14 */

 R1301  - AIF1 Frame Ctrl 15 */

 R1302  - AIF1 Frame Ctrl 16 */

 R1303  - AIF1 Frame Ctrl 17 */

 R1304  - AIF1 Frame Ctrl 18 */

 R1305  - AIF1 Tx Enables */

 R1306  - AIF1 Rx Enables */

 R1344  - AIF2 BCLK Ctrl */

 R1345  - AIF2 Tx Pin Ctrl */

 R1346  - AIF2 Rx Pin Ctrl */

 R1347  - AIF2 Rate Ctrl */

 R1348  - AIF2 Format */

 R1349  - AIF2 Tx BCLK Rate */

 R1350  - AIF2 Rx BCLK Rate */

 R1351  - AIF2 Frame Ctrl 1 */

 R1352  - AIF2 Frame Ctrl 2 */

 R1353  - AIF2 Frame Ctrl 3 */

 R1354  - AIF2 Frame Ctrl 4 */

 R1361  - AIF2 Frame Ctrl 11 */

 R1362  - AIF2 Frame Ctrl 12 */

 R1369  - AIF2 Tx Enables */

 R1370  - AIF2 Rx Enables */

 R1408  - AIF3 BCLK Ctrl */

 R1409  - AIF3 Tx Pin Ctrl */

 R1410  - AIF3 Rx Pin Ctrl */

 R1411  - AIF3 Rate Ctrl */

 R1412  - AIF3 Format */

 R1413  - AIF3 Tx BCLK Rate */

 R1414  - AIF3 Rx BCLK Rate */

 R1415  - AIF3 Frame Ctrl 1 */

 R1416  - AIF3 Frame Ctrl 2 */

 R1417  - AIF3 Frame Ctrl 3 */

 R1418  - AIF3 Frame Ctrl 4 */

 R1425  - AIF3 Frame Ctrl 11 */

 R1426  - AIF3 Frame Ctrl 12 */

 R1433  - AIF3 Tx Enables */

 R1434  - AIF3 Rx Enables */

 R1507  - SLIMbus Framer Ref Gear */

 R1509  - SLIMbus Rates 1 */

 R1510  - SLIMbus Rates 2 */

 R1511  - SLIMbus Rates 3 */

 R1512  - SLIMbus Rates 4 */

 R1513  - SLIMbus Rates 5 */

 R1514  - SLIMbus Rates 6 */

 R1515  - SLIMbus Rates 7 */

 R1516  - SLIMbus Rates 8 */

 R1525  - SLIMbus RX Channel Enable */

 R1526  - SLIMbus TX Channel Enable */

 R1600  - PWM1MIX Input 1 Source */

 R1601  - PWM1MIX Input 1 Volume */

 R1602  - PWM1MIX Input 2 Source */

 R1603  - PWM1MIX Input 2 Volume */

 R1604  - PWM1MIX Input 3 Source */

 R1605  - PWM1MIX Input 3 Volume */

 R1606  - PWM1MIX Input 4 Source */

 R1607  - PWM1MIX Input 4 Volume */

 R1608  - PWM2MIX Input 1 Source */

 R1609  - PWM2MIX Input 1 Volume */

 R1610  - PWM2MIX Input 2 Source */

 R1611  - PWM2MIX Input 2 Volume */

 R1612  - PWM2MIX Input 3 Source */

 R1613  - PWM2MIX Input 3 Volume */

 R1614  - PWM2MIX Input 4 Source */

 R1615  - PWM2MIX Input 4 Volume */

 R1632  - MICMIX Input 1 Source */

 R1633  - MICMIX Input 1 Volume */

 R1634  - MICMIX Input 2 Source */

 R1635  - MICMIX Input 2 Volume */

 R1636  - MICMIX Input 3 Source */

 R1637  - MICMIX Input 3 Volume */

 R1638  - MICMIX Input 4 Source */

 R1639  - MICMIX Input 4 Volume */

 R1640  - NOISEMIX Input 1 Source */

 R1641  - NOISEMIX Input 1 Volume */

 R1642  - NOISEMIX Input 2 Source */

 R1643  - NOISEMIX Input 2 Volume */

 R1644  - NOISEMIX Input 3 Source */

 R1645  - NOISEMIX Input 3 Volume */

 R1646  - NOISEMIX Input 4 Source */

 R1647  - NOISEMIX Input 4 Volume */

 R1664  - OUT1LMIX Input 1 Source */

 R1665  - OUT1LMIX Input 1 Volume */

 R1666  - OUT1LMIX Input 2 Source */

 R1667  - OUT1LMIX Input 2 Volume */

 R1668  - OUT1LMIX Input 3 Source */

 R1669  - OUT1LMIX Input 3 Volume */

 R1670  - OUT1LMIX Input 4 Source */

 R1671  - OUT1LMIX Input 4 Volume */

 R1672  - OUT1RMIX Input 1 Source */

 R1673  - OUT1RMIX Input 1 Volume */

 R1674  - OUT1RMIX Input 2 Source */

 R1675  - OUT1RMIX Input 2 Volume */

 R1676  - OUT1RMIX Input 3 Source */

 R1677  - OUT1RMIX Input 3 Volume */

 R1678  - OUT1RMIX Input 4 Source */

 R1679  - OUT1RMIX Input 4 Volume */

 R1680  - OUT2LMIX Input 1 Source */

 R1681  - OUT2LMIX Input 1 Volume */

 R1682  - OUT2LMIX Input 2 Source */

 R1683  - OUT2LMIX Input 2 Volume */

 R1684  - OUT2LMIX Input 3 Source */

 R1685  - OUT2LMIX Input 3 Volume */

 R1686  - OUT2LMIX Input 4 Source */

 R1687  - OUT2LMIX Input 4 Volume */

 R1688  - OUT2RMIX Input 1 Source */

 R1689  - OUT2RMIX Input 1 Volume */

 R1690  - OUT2RMIX Input 2 Source */

 R1691  - OUT2RMIX Input 2 Volume */

 R1692  - OUT2RMIX Input 3 Source */

 R1693  - OUT2RMIX Input 3 Volume */

 R1694  - OUT2RMIX Input 4 Source */

 R1695  - OUT2RMIX Input 4 Volume */

 R1696  - OUT3LMIX Input 1 Source */

 R1697  - OUT3LMIX Input 1 Volume */

 R1698  - OUT3LMIX Input 2 Source */

 R1699  - OUT3LMIX Input 2 Volume */

 R1700  - OUT3LMIX Input 3 Source */

 R1701  - OUT3LMIX Input 3 Volume */

 R1702  - OUT3LMIX Input 4 Source */

 R1703  - OUT3LMIX Input 4 Volume */

 R1712  - OUT4LMIX Input 1 Source */

 R1713  - OUT4LMIX Input 1 Volume */

 R1714  - OUT4LMIX Input 2 Source */

 R1715  - OUT4LMIX Input 2 Volume */

 R1716  - OUT4LMIX Input 3 Source */

 R1717  - OUT4LMIX Input 3 Volume */

 R1718  - OUT4LMIX Input 4 Source */

 R1719  - OUT4LMIX Input 4 Volume */

 R1720  - OUT4RMIX Input 1 Source */

 R1721  - OUT4RMIX Input 1 Volume */

 R1722  - OUT4RMIX Input 2 Source */

 R1723  - OUT4RMIX Input 2 Volume */

 R1724  - OUT4RMIX Input 3 Source */

 R1725  - OUT4RMIX Input 3 Volume */

 R1726  - OUT4RMIX Input 4 Source */

 R1727  - OUT4RMIX Input 4 Volume */

 R1728  - OUT5LMIX Input 1 Source */

 R1729  - OUT5LMIX Input 1 Volume */

 R1730  - OUT5LMIX Input 2 Source */

 R1731  - OUT5LMIX Input 2 Volume */

 R1732  - OUT5LMIX Input 3 Source */

 R1733  - OUT5LMIX Input 3 Volume */

 R1734  - OUT5LMIX Input 4 Source */

 R1735  - OUT5LMIX Input 4 Volume */

 R1736  - OUT5RMIX Input 1 Source */

 R1737  - OUT5RMIX Input 1 Volume */

 R1738  - OUT5RMIX Input 2 Source */

 R1739  - OUT5RMIX Input 2 Volume */

 R1740  - OUT5RMIX Input 3 Source */

 R1741  - OUT5RMIX Input 3 Volume */

 R1742  - OUT5RMIX Input 4 Source */

 R1743  - OUT5RMIX Input 4 Volume */

 R1792  - AIF1TX1MIX Input 1 Source */

 R1793  - AIF1TX1MIX Input 1 Volume */

 R1794  - AIF1TX1MIX Input 2 Source */

 R1795  - AIF1TX1MIX Input 2 Volume */

 R1796  - AIF1TX1MIX Input 3 Source */

 R1797  - AIF1TX1MIX Input 3 Volume */

 R1798  - AIF1TX1MIX Input 4 Source */

 R1799  - AIF1TX1MIX Input 4 Volume */

 R1800  - AIF1TX2MIX Input 1 Source */

 R1801  - AIF1TX2MIX Input 1 Volume */

 R1802  - AIF1TX2MIX Input 2 Source */

 R1803  - AIF1TX2MIX Input 2 Volume */

 R1804  - AIF1TX2MIX Input 3 Source */

 R1805  - AIF1TX2MIX Input 3 Volume */

 R1806  - AIF1TX2MIX Input 4 Source */

 R1807  - AIF1TX2MIX Input 4 Volume */

 R1808  - AIF1TX3MIX Input 1 Source */

 R1809  - AIF1TX3MIX Input 1 Volume */

 R1810  - AIF1TX3MIX Input 2 Source */

 R1811  - AIF1TX3MIX Input 2 Volume */

 R1812  - AIF1TX3MIX Input 3 Source */

 R1813  - AIF1TX3MIX Input 3 Volume */

 R1814  - AIF1TX3MIX Input 4 Source */

 R1815  - AIF1TX3MIX Input 4 Volume */

 R1816  - AIF1TX4MIX Input 1 Source */

 R1817  - AIF1TX4MIX Input 1 Volume */

 R1818  - AIF1TX4MIX Input 2 Source */

 R1819  - AIF1TX4MIX Input 2 Volume */

 R1820  - AIF1TX4MIX Input 3 Source */

 R1821  - AIF1TX4MIX Input 3 Volume */

 R1822  - AIF1TX4MIX Input 4 Source */

 R1823  - AIF1TX4MIX Input 4 Volume */

 R1824  - AIF1TX5MIX Input 1 Source */

 R1825  - AIF1TX5MIX Input 1 Volume */

 R1826  - AIF1TX5MIX Input 2 Source */

 R1827  - AIF1TX5MIX Input 2 Volume */

 R1828  - AIF1TX5MIX Input 3 Source */

 R1829  - AIF1TX5MIX Input 3 Volume */

 R1830  - AIF1TX5MIX Input 4 Source */

 R1831  - AIF1TX5MIX Input 4 Volume */

 R1832  - AIF1TX6MIX Input 1 Source */

 R1833  - AIF1TX6MIX Input 1 Volume */

 R1834  - AIF1TX6MIX Input 2 Source */

 R1835  - AIF1TX6MIX Input 2 Volume */

 R1836  - AIF1TX6MIX Input 3 Source */

 R1837  - AIF1TX6MIX Input 3 Volume */

 R1838  - AIF1TX6MIX Input 4 Source */

 R1839  - AIF1TX6MIX Input 4 Volume */

 R1840  - AIF1TX7MIX Input 1 Source */

 R1841  - AIF1TX7MIX Input 1 Volume */

 R1842  - AIF1TX7MIX Input 2 Source */

 R1843  - AIF1TX7MIX Input 2 Volume */

 R1844  - AIF1TX7MIX Input 3 Source */

 R1845  - AIF1TX7MIX Input 3 Volume */

 R1846  - AIF1TX7MIX Input 4 Source */

 R1847  - AIF1TX7MIX Input 4 Volume */

 R1848  - AIF1TX8MIX Input 1 Source */

 R1849  - AIF1TX8MIX Input 1 Volume */

 R1850  - AIF1TX8MIX Input 2 Source */

 R1851  - AIF1TX8MIX Input 2 Volume */

 R1852  - AIF1TX8MIX Input 3 Source */

 R1853  - AIF1TX8MIX Input 3 Volume */

 R1854  - AIF1TX8MIX Input 4 Source */

 R1855  - AIF1TX8MIX Input 4 Volume */

 R1856  - AIF2TX1MIX Input 1 Source */

 R1857  - AIF2TX1MIX Input 1 Volume */

 R1858  - AIF2TX1MIX Input 2 Source */

 R1859  - AIF2TX1MIX Input 2 Volume */

 R1860  - AIF2TX1MIX Input 3 Source */

 R1861  - AIF2TX1MIX Input 3 Volume */

 R1862  - AIF2TX1MIX Input 4 Source */

 R1863  - AIF2TX1MIX Input 4 Volume */

 R1864  - AIF2TX2MIX Input 1 Source */

 R1865  - AIF2TX2MIX Input 1 Volume */

 R1866  - AIF2TX2MIX Input 2 Source */

 R1867  - AIF2TX2MIX Input 2 Volume */

 R1868  - AIF2TX2MIX Input 3 Source */

 R1869  - AIF2TX2MIX Input 3 Volume */

 R1870  - AIF2TX2MIX Input 4 Source */

 R1871  - AIF2TX2MIX Input 4 Volume */

 R1920  - AIF3TX1MIX Input 1 Source */

 R1921  - AIF3TX1MIX Input 1 Volume */

 R1922  - AIF3TX1MIX Input 2 Source */

 R1923  - AIF3TX1MIX Input 2 Volume */

 R1924  - AIF3TX1MIX Input 3 Source */

 R1925  - AIF3TX1MIX Input 3 Volume */

 R1926  - AIF3TX1MIX Input 4 Source */

 R1927  - AIF3TX1MIX Input 4 Volume */

 R1928  - AIF3TX2MIX Input 1 Source */

 R1929  - AIF3TX2MIX Input 1 Volume */

 R1930  - AIF3TX2MIX Input 2 Source */

 R1931  - AIF3TX2MIX Input 2 Volume */

 R1932  - AIF3TX2MIX Input 3 Source */

 R1933  - AIF3TX2MIX Input 3 Volume */

 R1934  - AIF3TX2MIX Input 4 Source */

 R1935  - AIF3TX2MIX Input 4 Volume */

 R1984  - SLIMTX1MIX Input 1 Source */

 R1985  - SLIMTX1MIX Input 1 Volume */

 R1986  - SLIMTX1MIX Input 2 Source */

 R1987  - SLIMTX1MIX Input 2 Volume */

 R1988  - SLIMTX1MIX Input 3 Source */

 R1989  - SLIMTX1MIX Input 3 Volume */

 R1990  - SLIMTX1MIX Input 4 Source */

 R1991  - SLIMTX1MIX Input 4 Volume */

 R1992  - SLIMTX2MIX Input 1 Source */

 R1993  - SLIMTX2MIX Input 1 Volume */

 R1994  - SLIMTX2MIX Input 2 Source */

 R1995  - SLIMTX2MIX Input 2 Volume */

 R1996  - SLIMTX2MIX Input 3 Source */

 R1997  - SLIMTX2MIX Input 3 Volume */

 R1998  - SLIMTX2MIX Input 4 Source */

 R1999  - SLIMTX2MIX Input 4 Volume */

 R2000  - SLIMTX3MIX Input 1 Source */

 R2001  - SLIMTX3MIX Input 1 Volume */

 R2002  - SLIMTX3MIX Input 2 Source */

 R2003  - SLIMTX3MIX Input 2 Volume */

 R2004  - SLIMTX3MIX Input 3 Source */

 R2005  - SLIMTX3MIX Input 3 Volume */

 R2006  - SLIMTX3MIX Input 4 Source */

 R2007  - SLIMTX3MIX Input 4 Volume */

 R2008  - SLIMTX4MIX Input 1 Source */

 R2009  - SLIMTX4MIX Input 1 Volume */

 R2010  - SLIMTX4MIX Input 2 Source */

 R2011  - SLIMTX4MIX Input 2 Volume */

 R2012  - SLIMTX4MIX Input 3 Source */

 R2013  - SLIMTX4MIX Input 3 Volume */

 R2014  - SLIMTX4MIX Input 4 Source */

 R2015  - SLIMTX4MIX Input 4 Volume */

 R2016  - SLIMTX5MIX Input 1 Source */

 R2017  - SLIMTX5MIX Input 1 Volume */

 R2018  - SLIMTX5MIX Input 2 Source */

 R2019  - SLIMTX5MIX Input 2 Volume */

 R2020  - SLIMTX5MIX Input 3 Source */

 R2021  - SLIMTX5MIX Input 3 Volume */

 R2022  - SLIMTX5MIX Input 4 Source */

 R2023  - SLIMTX5MIX Input 4 Volume */

 R2024  - SLIMTX6MIX Input 1 Source */

 R2025  - SLIMTX6MIX Input 1 Volume */

 R2026  - SLIMTX6MIX Input 2 Source */

 R2027  - SLIMTX6MIX Input 2 Volume */

 R2028  - SLIMTX6MIX Input 3 Source */

 R2029  - SLIMTX6MIX Input 3 Volume */

 R2030  - SLIMTX6MIX Input 4 Source */

 R2031  - SLIMTX6MIX Input 4 Volume */

 R2032  - SLIMTX7MIX Input 1 Source */

 R2033  - SLIMTX7MIX Input 1 Volume */

 R2034  - SLIMTX7MIX Input 2 Source */

 R2035  - SLIMTX7MIX Input 2 Volume */

 R2036  - SLIMTX7MIX Input 3 Source */

 R2037  - SLIMTX7MIX Input 3 Volume */

 R2038  - SLIMTX7MIX Input 4 Source */

 R2039  - SLIMTX7MIX Input 4 Volume */

 R2040  - SLIMTX8MIX Input 1 Source */

 R2041  - SLIMTX8MIX Input 1 Volume */

 R2042  - SLIMTX8MIX Input 2 Source */

 R2043  - SLIMTX8MIX Input 2 Volume */

 R2044  - SLIMTX8MIX Input 3 Source */

 R2045  - SLIMTX8MIX Input 3 Volume */

 R2046  - SLIMTX8MIX Input 4 Source */

 R2047  - SLIMTX8MIX Input 4 Volume */

 R2176  - EQ1MIX Input 1 Source */

 R2177  - EQ1MIX Input 1 Volume */

 R2178  - EQ1MIX Input 2 Source */

 R2179  - EQ1MIX Input 2 Volume */

 R2180  - EQ1MIX Input 3 Source */

 R2181  - EQ1MIX Input 3 Volume */

 R2182  - EQ1MIX Input 4 Source */

 R2183  - EQ1MIX Input 4 Volume */

 R2184  - EQ2MIX Input 1 Source */

 R2185  - EQ2MIX Input 1 Volume */

 R2186  - EQ2MIX Input 2 Source */

 R2187  - EQ2MIX Input 2 Volume */

 R2188  - EQ2MIX Input 3 Source */

 R2189  - EQ2MIX Input 3 Volume */

 R2190  - EQ2MIX Input 4 Source */

 R2191  - EQ2MIX Input 4 Volume */

 R2192  - EQ3MIX Input 1 Source */

 R2193  - EQ3MIX Input 1 Volume */

 R2194  - EQ3MIX Input 2 Source */

 R2195  - EQ3MIX Input 2 Volume */

 R2196  - EQ3MIX Input 3 Source */

 R2197  - EQ3MIX Input 3 Volume */

 R2198  - EQ3MIX Input 4 Source */

 R2199  - EQ3MIX Input 4 Volume */

 R2200  - EQ4MIX Input 1 Source */

 R2201  - EQ4MIX Input 1 Volume */

 R2202  - EQ4MIX Input 2 Source */

 R2203  - EQ4MIX Input 2 Volume */

 R2204  - EQ4MIX Input 3 Source */

 R2205  - EQ4MIX Input 3 Volume */

 R2206  - EQ4MIX Input 4 Source */

 R2207  - EQ4MIX Input 4 Volume */

 R2240  - DRC1LMIX Input 1 Source */

 R2241  - DRC1LMIX Input 1 Volume */

 R2242  - DRC1LMIX Input 2 Source */

 R2243  - DRC1LMIX Input 2 Volume */

 R2244  - DRC1LMIX Input 3 Source */

 R2245  - DRC1LMIX Input 3 Volume */

 R2246  - DRC1LMIX Input 4 Source */

 R2247  - DRC1LMIX Input 4 Volume */

 R2248  - DRC1RMIX Input 1 Source */

 R2249  - DRC1RMIX Input 1 Volume */

 R2250  - DRC1RMIX Input 2 Source */

 R2251  - DRC1RMIX Input 2 Volume */

 R2252  - DRC1RMIX Input 3 Source */

 R2253  - DRC1RMIX Input 3 Volume */

 R2254  - DRC1RMIX Input 4 Source */

 R2255  - DRC1RMIX Input 4 Volume */

 R2304  - HPLP1MIX Input 1 Source */

 R2305  - HPLP1MIX Input 1 Volume */

 R2306  - HPLP1MIX Input 2 Source */

 R2307  - HPLP1MIX Input 2 Volume */

 R2308  - HPLP1MIX Input 3 Source */

 R2309  - HPLP1MIX Input 3 Volume */

 R2310  - HPLP1MIX Input 4 Source */

 R2311  - HPLP1MIX Input 4 Volume */

 R2312  - HPLP2MIX Input 1 Source */

 R2313  - HPLP2MIX Input 1 Volume */

 R2314  - HPLP2MIX Input 2 Source */

 R2315  - HPLP2MIX Input 2 Volume */

 R2316  - HPLP2MIX Input 3 Source */

 R2317  - HPLP2MIX Input 3 Volume */

 R2318  - HPLP2MIX Input 4 Source */

 R2319  - HPLP2MIX Input 4 Volume */

 R2320  - HPLP3MIX Input 1 Source */

 R2321  - HPLP3MIX Input 1 Volume */

 R2322  - HPLP3MIX Input 2 Source */

 R2323  - HPLP3MIX Input 2 Volume */

 R2324  - HPLP3MIX Input 3 Source */

 R2325  - HPLP3MIX Input 3 Volume */

 R2326  - HPLP3MIX Input 4 Source */

 R2327  - HPLP3MIX Input 4 Volume */

 R2328  - HPLP4MIX Input 1 Source */

 R2329  - HPLP4MIX Input 1 Volume */

 R2330  - HPLP4MIX Input 2 Source */

 R2331  - HPLP4MIX Input 2 Volume */

 R2332  - HPLP4MIX Input 3 Source */

 R2333  - HPLP4MIX Input 3 Volume */

 R2334  - HPLP4MIX Input 4 Source */

 R2335  - HPLP4MIX Input 4 Volume */

 R2368  - DSP1LMIX Input 1 Source */

 R2369  - DSP1LMIX Input 1 Volume */

 R2370  - DSP1LMIX Input 2 Source */

 R2371  - DSP1LMIX Input 2 Volume */

 R2372  - DSP1LMIX Input 3 Source */

 R2373  - DSP1LMIX Input 3 Volume */

 R2374  - DSP1LMIX Input 4 Source */

 R2375  - DSP1LMIX Input 4 Volume */

 R2376  - DSP1RMIX Input 1 Source */

 R2377  - DSP1RMIX Input 1 Volume */

 R2378  - DSP1RMIX Input 2 Source */

 R2379  - DSP1RMIX Input 2 Volume */

 R2380  - DSP1RMIX Input 3 Source */

 R2381  - DSP1RMIX Input 3 Volume */

 R2382  - DSP1RMIX Input 4 Source */

 R2383  - DSP1RMIX Input 4 Volume */

 R2384  - DSP1AUX1MIX Input 1 Source */

 R2392  - DSP1AUX2MIX Input 1 Source */

 R2400  - DSP1AUX3MIX Input 1 Source */

 R2408  - DSP1AUX4MIX Input 1 Source */

 R2416  - DSP1AUX5MIX Input 1 Source */

 R2424  - DSP1AUX6MIX Input 1 Source */

 R2688  - ASRC1LMIX Input 1 Source */

 R2696  - ASRC1RMIX Input 1 Source */

 R2704  - ASRC2LMIX Input 1 Source */

 R2712  - ASRC2RMIX Input 1 Source */

 R2816  - ISRC1DEC1MIX Input 1 Source */

 R2824  - ISRC1DEC2MIX Input 1 Source */

 R2848  - ISRC1INT1MIX Input 1 Source */

 R2856  - ISRC1INT2MIX Input 1 Source */

 R2880  - ISRC2DEC1MIX Input 1 Source */

 R2888  - ISRC2DEC2MIX Input 1 Source */

 R2912  - ISRC2INT1MIX Input 1 Source */

 R2920  - ISRC2INT2MIX Input 1 Source */

 R3072  - GPIO1 CTRL */

 R3073  - GPIO2 CTRL */

 R3074  - GPIO3 CTRL */

 R3075  - GPIO4 CTRL */

 R3076  - GPIO5 CTRL */

 R3087  - IRQ CTRL 1 */

 R3088  - GPIO Debounce Config */

 R3104  - Misc Pad Ctrl 1 */

 R3105  - Misc Pad Ctrl 2 */

 R3106  - Misc Pad Ctrl 3 */

 R3107  - Misc Pad Ctrl 4 */

 R3108  - Misc Pad Ctrl 5 */

 R3109  - Misc Pad Ctrl 6 */

 R3336  - Interrupt Status 1 Mask */

 R3337  - Interrupt Status 2 Mask */

 R3338  - Interrupt Status 3 Mask */

 R3339  - Interrupt Status 4 Mask */

 R3340  - Interrupt Status 5 Mask */

 R3343  - Interrupt Control */

 R3352  - IRQ2 Status 1 Mask */

 R3353  - IRQ2 Status 2 Mask */

 R3354  - IRQ2 Status 3 Mask */

 R3355  - IRQ2 Status 4 Mask */

 R3356  - IRQ2 Status 5 Mask */

 R3359  - IRQ2 Control */

 R3393  - ADSP2 IRQ0 */

 R3411  - AOD IRQ Mask IRQ1 */

 R3412  - AOD IRQ Mask IRQ2 */

 R3414  - Jack detect debounce */

 R3584  - FX_Ctrl1 */

 R3600  - EQ1_1 */

 R3601  - EQ1_2 */

 R3602  - EQ1_3 */

 R3603  - EQ1_4 */

 R3604  - EQ1_5 */

 R3605  - EQ1_6 */

 R3606  - EQ1_7 */

 R3607  - EQ1_8 */

 R3608  - EQ1_9 */

 R3609  - EQ1_10 */

 R3610  - EQ1_11 */

 R3611  - EQ1_12 */

 R3612  - EQ1_13 */

 R3613  - EQ1_14 */

 R3614  - EQ1_15 */

 R3615  - EQ1_16 */

 R3616  - EQ1_17 */

 R3617  - EQ1_18 */

 R3618  - EQ1_19 */

 R3619  - EQ1_20 */

 R3620  - EQ1_21 */

 R3622  - EQ2_1 */

 R3623  - EQ2_2 */

 R3624  - EQ2_3 */

 R3625  - EQ2_4 */

 R3626  - EQ2_5 */

 R3627  - EQ2_6 */

 R3628  - EQ2_7 */

 R3629  - EQ2_8 */

 R3630  - EQ2_9 */

 R3631  - EQ2_10 */

 R3632  - EQ2_11 */

 R3633  - EQ2_12 */

 R3634  - EQ2_13 */

 R3635  - EQ2_14 */

 R3636  - EQ2_15 */

 R3637  - EQ2_16 */

 R3638  - EQ2_17 */

 R3639  - EQ2_18 */

 R3640  - EQ2_19 */

 R3641  - EQ2_20 */

 R3642  - EQ2_21 */

 R3644  - EQ3_1 */

 R3645  - EQ3_2 */

 R3646  - EQ3_3 */

 R3647  - EQ3_4 */

 R3648  - EQ3_5 */

 R3649  - EQ3_6 */

 R3650  - EQ3_7 */

 R3651  - EQ3_8 */

 R3652  - EQ3_9 */

 R3653  - EQ3_10 */

 R3654  - EQ3_11 */

 R3655  - EQ3_12 */

 R3656  - EQ3_13 */

 R3657  - EQ3_14 */

 R3658  - EQ3_15 */

 R3659  - EQ3_16 */

 R3660  - EQ3_17 */

 R3661  - EQ3_18 */

 R3662  - EQ3_19 */

 R3663  - EQ3_20 */

 R3664  - EQ3_21 */

 R3666  - EQ4_1 */

 R3667  - EQ4_2 */

 R3668  - EQ4_3 */

 R3669  - EQ4_4 */

 R3670  - EQ4_5 */

 R3671  - EQ4_6 */

 R3672  - EQ4_7 */

 R3673  - EQ4_8 */

 R3674  - EQ4_9 */

 R3675  - EQ4_10 */

 R3676  - EQ4_11 */

 R3677  - EQ4_12 */

 R3678  - EQ4_13 */

 R3679  - EQ4_14 */

 R3680  - EQ4_15 */

 R3681  - EQ4_16 */

 R3682  - EQ4_17 */

 R3683  - EQ4_18 */

 R3684  - EQ4_19 */

 R3685  - EQ4_20 */

 R3686  - EQ4_21 */

 R3712  - DRC1 ctrl1 */

 R3713  - DRC1 ctrl2 */

 R3714  - DRC1 ctrl3 */

 R3715  - DRC1 ctrl4 */

 R3716  - DRC1 ctrl5 */

 R3776  - HPLPF1_1 */

 R3777  - HPLPF1_2 */

 R3780  - HPLPF2_1 */

 R3781  - HPLPF2_2 */

 R3784  - HPLPF3_1 */

 R3785  - HPLPF3_2 */

 R3788  - HPLPF4_1 */

 R3789  - HPLPF4_2 */

 R3808  - ASRC_ENABLE */

 R3810  - ASRC_RATE1 */

 R3811  - ASRC_RATE2 */

 R3824  - ISRC 1 CTRL 1 */

 R3825  - ISRC 1 CTRL 2 */

 R3826  - ISRC 1 CTRL 3 */

 R3827  - ISRC 2 CTRL 1 */

 R3828  - ISRC 2 CTRL 2 */

 R3829  - ISRC 2 CTRL 3 */

 R4352  - DSP1 Control 1 */

 SPDX-License-Identifier: GPL-2.0-or-later

/* NXP PCF50633 Power Management Unit (PMU) driver

 *

 * (C) 2006-2008 by Openmoko, Inc.

 * Author: Harald Welte <laforge@openmoko.org>

 * 	   Balaji Rao <balajirrao@openmoko.org>

 * All rights reserved.

 Maximum amount of time ONKEY is held before emergency action is taken */

 Read the 5 INT regs in one transaction */

		/*

		 * If this doesn't ACK the interrupt to the chip, we'll be

		 * called once again as we're level triggered.

 defeat 8s death from lowsys on A5 */

	/* We immediately read the usb and adapter status. We thus make sure

 Make sure only one of ADPINS or ADPREM is set */

	/* Some revisions of the chip don't have a 8s standby mode on

 Unmask IRQ_SECOND */

 Unmask IRQ_ONKEYR */

 Mask SECOND and ONKEYR interrupts */

 Have we just resumed ? */

 Set the resume reason filtering out non resumers */

		/* Make sure we don't pass on any ONKEY events to

 Unset masked interrupts */

	/* Make sure our interrupt handlers are not called

 Save the masks */

 Write wakeup irq masks */

 Write the saved mask registers */

 Enable all interrupts except RTC SECOND */

 SPDX-License-Identifier: GPL-2.0



 Copyright (c) 2020 MediaTek Inc.

 Find out the IRQ group */

 Find the IRQ registers */

 Disable all interrupts for initializing */

/*

 * Driver for TPS65218 Integrated power management chipsets

 *

 * Copyright (C) 2014 Texas Instruments Incorporated - https://www.ti.com/

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether expressed or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License version 2 for more details.

/**

 * tps65218_reg_write: Write a single tps65218 register.

 *

 * @tps: Device to write to.

 * @reg: Register to write to.

 * @val: Value to write.

 * @level: Password protected level

/**

 * tps65218_update_bits: Modify bits w.r.t mask, val and level.

 *

 * @tps: Device to write to.

 * @reg: Register to read-write to.

 * @mask: Mask.

 * @val: Value to write.

 * @level: Password protected level

 INT1 IRQs */

 INT2 IRQs*/

 SPDX-License-Identifier: GPL-2.0

/*

 * MFD core driver for Intel Broxton Whiskey Cove PMIC

 *

 * Copyright (C) 2015 Intel Corporation. All rights reserved.

 PMIC device registers */

 Interrupt Status Registers */

 Interrupt MASK Registers */

 Whiskey Cove PMIC share same ACPI ID between different platforms */

 sysfs interfaces to r/w PMIC registers, required by initial script */

 Add chained IRQ handler for BCU IRQs */

 Add chained IRQ handler for ADC IRQs */

 Add chained IRQ handler for CHGR IRQs */

 Add chained IRQ handler for CRIT IRQs */

	/*

	 * There is known hw bug. Upon reset BIT 5 of register

	 * BXTWC_CHGR_LVL1_IRQ is 0 which is the expected value. However,

	 * later it's set to 1(masked) automatically by hardware. So we

	 * have the software workaround here to unmaksed it in order to let

	 * charger interrutp work.

/*

 * Base driver for Marvell 88PM800

 *

 * Copyright (C) 2012 Marvell International Ltd.

 * Haojian Zhuang <haojian.zhuang@marvell.com>

 * Joseph(Yossi) Hanin <yhanin@marvell.com>

 * Qiao Zhou <zhouqiao@marvell.com>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

 Interrupt Registers */

 number of INT_ENA & INT_STATUS regs */

 Interrupt Number in 88PM800 */

*0 */
0 */

EN1b1 */

EN1b2 */

EN1b3 */

EN1b4 */

*5 */
5 */

EN2b0 */

EN2b1 */

EN2b2 */

EN2b3 */

*10 */
10 */

EN3b1 */

EN3b2 */

EN3b3 */

EN3b4 */

*15 */
15 */

EN4b1 */

EN4b2 */

EN4b3 */

*19 */
19 */

 PM800: generation identification number */

 NULL terminated */

 INT0 */

 INT1 */

 INT2 */

 INT3 */

	/*

	 * initialize GPADC without activating it turn on GPADC

	 * measurments

	/*

	 * This function configures the ADC as requires for

	 * CP implementation.CP does not "own" the ADC configuration

	 * registers and relies on AP.

	 * Reason: enable automatic ADC measurements needed

	 * for CP to get VBAT and RF temperature readings.

	/*

	 * the defult of PM800 is GPADC operates at 100Ks/s rate

	 * and Number of GPADC slots with active current bias prior

	 * to GPADC sampling = 1 slot for all GPADCs set for

	 * Temprature mesurmants

	/*

	 * irq_mode defines the way of clearing interrupt. it's read-clear by

	 * default.

 PM800 block power page */

 PM800 block GPADC */

	/*

	 * alarm wake up bit will be clear in device_irq_init(),

	 * read before that

 init subchip for PM800 */

 pm800 has 2 addtional pages to support power and gpadc. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-irq.c  --  Interrupt controller support for Wolfson WM831x PMICs

 *

 * Copyright 2009 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

		/* If there's been a change in the mask write it back

 Ignore internal-only IRQs */

	/* Rebase the IRQ into the GPIO range so we've got a sensible array

	 * index.

	/* We set the high bit to flag that we need an update; don't

	 * do the update here as we can be called with the bus lock

	 * held.

/* The processing of the primary interrupt occurs in a thread so that

	/* The touch interrupts are visible in the primary register as

	 * an optimisation; open code this to avoid complicating the

	 * main handling loop and so we can also skip iterating the

	 * descriptors.

		/* Hopefully there should only be one register to read

 Ignore any bits that we don't think are masked */

			/* Acknowledge now so we don't miss

			 * notifications while we handle.

		/* Simulate an edge triggered IRQ by polling the input

		 * status.  This is sucky but improves interoperability.

 Mask the individual interrupt sources */

 Try to dynamically allocate IRQs if no base is specified */

		/* Try to flag /IRQ as a wake source; there are a number of

		 * unconditional wake sources in the PMIC so this isn't

		 * conditional but we don't actually care *too* much if it

		 * fails.

 Enable top level interrupts, we mask at secondary level */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Simple MFD - I2C

 *

 * Author(s):

 * 	Michael Walle <michael@walle.cc>

 * 	Lee Jones <lee.jones@linaro.org>

 *

 * This driver creates a single register map with the intention for it to be

 * shared by all sub-devices.  Children can use their parent's device structure

 * (dev.parent) in order to reference it.

 *

 * Once the register map has been successfully initialised, any sub-devices

 * represented by child nodes in Device Tree or via the MFD cells in this file

 * will be subsequently registered.

 If no regmap_config is specified, use the default 8reg and 8val bits */

 If no MFD cells are spedified, use register the DT child nodes instead */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  lpc_sch.c - LPC interface for Intel Poulsbo SCH

 *

 *  LPC bridge function of the Intel SCH contains many other

 *  functional units, such as Interrupt controllers, Timers,

 *  Power Management, System Management, GPIO, RTC, and LPC

 *  Configuration Registers.

 *

 *  Copyright (c) 2010 CompuLab Ltd

 *  Copyright (c) 2014 Intel Corp.

 *  Author: Denis Turischev <denis@compulab.co.il>

 Intel Poulsbo SCH */

 Intel Tunnel Creek */

 Intel Centerton */

 Intel Quark X1000 */

 SPDX-License-Identifier: GPL-2.0

/*

 * The Gateworks System Controller (GSC) is a multi-function

 * device designed for use in Gateworks Single Board Computers.

 * The control interface is I2C, with an interrupt. The device supports

 * system functions such as push-button monitoring, multiple ADC's for

 * voltage and temperature monitoring, fan controller and watchdog monitor.

 *

 * Copyright (C) 2020 Gateworks Corporation

/*

 * The GSC suffers from an errata where occasionally during

 * ADC cycles the chip can NAK I2C transactions. To ensure we have reliable

 * register access we place retries around register access.

		/*

		 * -EAGAIN returned when the i2c host controller is busy

		 * -EIO returned when i2c device is busy

		/*

		 * -EAGAIN returned when the i2c host controller is busy

		 * -EIO returned when i2c device is busy

/*

 * gsc_powerdown - API to use GSC to power down board for a specific time

 *

 * secs - number of seconds to remain powered off

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST Microelectronics MFD: stmpe's driver

 *

 * Copyright (C) ST-Ericsson SA 2010

 *

 * Author: Rabin Vincent <rabin.vincent@stericsson.com> for ST-Ericsson

/**

 * struct stmpe_platform_data - STMPE platform data

 * @id: device id to distinguish between multiple STMPEs on the same board

 * @blocks: bitmask of blocks to enable (use STMPE_BLOCK_*)

 * @irq_trigger: IRQ trigger to use for the interrupt to the host

 * @autosleep: bool to enable/disable stmpe autosleep

 * @autosleep_timeout: inactivity timeout in milliseconds for autosleep

 * @irq_over_gpio: true if gpio is used to get irq

 * @irq_gpio: gpio number over which irq will be requested (significant only if

 *	      irq_over_gpio is true)

/**

 * stmpe_enable - enable blocks on an STMPE device

 * @stmpe:	Device to work on

 * @blocks:	Mask of blocks (enum stmpe_block values) to enable

/**

 * stmpe_disable - disable blocks on an STMPE device

 * @stmpe:	Device to work on

 * @blocks:	Mask of blocks (enum stmpe_block values) to enable

/**

 * stmpe_reg_read() - read a single STMPE register

 * @stmpe:	Device to read from

 * @reg:	Register to read

/**

 * stmpe_reg_write() - write a single STMPE register

 * @stmpe:	Device to write to

 * @reg:	Register to write

 * @val:	Value to write

/**

 * stmpe_set_bits() - set the value of a bitfield in a STMPE register

 * @stmpe:	Device to write to

 * @reg:	Register to write

 * @mask:	Mask of bits to set

 * @val:	Value to set

/**

 * stmpe_block_read() - read multiple STMPE registers

 * @stmpe:	Device to read from

 * @reg:	First register

 * @length:	Number of registers

 * @values:	Buffer to write to

/**

 * stmpe_block_write() - write multiple STMPE registers

 * @stmpe:	Device to write to

 * @reg:	First register

 * @length:	Number of registers

 * @values:	Values to write

/**

 * stmpe_set_altfunc()- set the alternate function for STMPE pins

 * @stmpe:	Device to configure

 * @pins:	Bitmask of pins to affect

 * @block:	block to enable alternate functions for

 *

 * @pins is assumed to have a bit set for each of the bits whose alternate

 * function is to be changed, numbered according to the GPIOXY numbers.

 *

 * If the GPIO module is not enabled, this function automatically enables it in

 * order to perform the change.

/*

 * GPIO (all variants)

 Start and end filled dynamically */

 gpio cell resources consist of an irq only so no resources here */

/*

 * Keypad (1601, 2401, 2403)

 Start and end filled dynamically */

/*

 * PWM (1601, 2401, 2403)

 Start and end filled dynamically */

/*

 * STMPE801

/*

 * Touchscreen (STMPE811 or STMPE610)

 Start and end filled dynamically */

/*

 * ADC (STMPE811)

 Start and end filled dynamically */

/*

 * STMPE811 or STMPE610

 0 for touchscreen, 1 for GPIO */

 Similar to 811, except number of gpios */

/*

 * STMPE1600

 * Compared to all others STMPE variant, LSB and MSB regs are located in this

 * order :	LSB   addr

 *		MSB   addr + 1

 * As there is only 2 * 8bits registers for GPMR/GPSR/IEGPIOPR, CSB index is MSB registers

/*

 * STMPE1601

 supported autosleep timeout delay (in msecs) */

	/*

	 * requests for delays longer than supported should not return the

	 * longest supported delay

/*

 * Both stmpe 1601/2403 support same layout for autosleep

 choose the best available timeout */

 at least 0x0210 and 0x0212 */

/*

 * STMPE1801

 STMPE801 and STMPE610 use bit 1 of SYS_CTRL register */

 all other STMPE variant use bit 7 of SYS_CTRL register */

 stmpe1801 do not have any gpio alternate function */

/*

 * STMPE24XX

 same as stmpe1601 */

/*

 * These devices can be connected in a 'no-irq' configuration - the irq pin

 * is not used and the device cannot interrupt the CPU. Here we only list

 * devices which support this configuration - the driver will fail probing

 * for any devices not listed here which are configured in this way.

 Disable all modules -- subdrivers should enable what they need. */

 STMPE801 and STMPE1600 don't support Edge interrupts */

 Dynamically fill in a variant's IRQ. */

 Called from client specific probe routines */

 use alternate variant info for no-irq mode, if supported */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel MAX 10 Board Management Controller chip

 *

 * Copyright (C) 2018-2020 Intel Corporation. All rights reserved.

	/*

	 * This check is to filter out the very old legacy BMC versions. In the

	 * old BMC chips, the BMC version info is stored in the old version

	 * register (M10BMC_LEGACY_BUILD_VER), so its read out value would have

	 * not been M10BMC_VER_LEGACY_INVALID (0xffffffff). But in new BMC

	 * chips that the driver supports, the value of this register should be

	 * M10BMC_VER_LEGACY_INVALID.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * RSB driver for the X-Powers' Power Management ICs

 *

 * AXP20x typically comprises an adaptive USB-Compatible PWM charger, BUCK DC-DC

 * converters, LDOs, multiple 12-bit ADCs of voltage, current and temperature

 * as well as configurable GPIOs.

 *

 * This driver supports the RSB variants.

 *

 * Copyright (C) 2015 Chen-Yu Tsai

 *

 * Author: Chen-Yu Tsai <wens@csie.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8350-irq.c  --  IRQ support for Wolfson WM8350

 *

 * Copyright 2007, 2008, 2009 Wolfson Microelectronics PLC.

 *

 * Author: Liam Girdwood, Mark Brown

/*

 * This is a threaded IRQ handler so can access I2C/SPI.  Since all

 * interrupts are clear on read the IRQ line will be reasserted and

 * the physical IRQ will be handled again if another interrupt is

 * asserted while we run - in the normal course of events this is a

 * rare occurrence so we save I2C/SPI reads.  We're also assuming that

 * it's rare to get lots of interrupts firing simultaneously so try to

 * minimise I/O.

		/* If there's been a change in the mask write it back

 Mask top level interrupts */

	/* Mask all individual interrupts by default and cache the

	 * masks.  We read the masks back since there are unwritable

 Register with genirq */

 Allow interrupts to fire */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Texas Instruments Incorporated - https://www.ti.com/

 *

 * Author: Keerthy <j-keerthy@ti.com>

 The minimum assertion time is undocumented, just guess */

 Min 1.2 ms before first I2C transaction */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Data tables for CS47L24 codec

 *

 * Copyright 2015 Cirrus Logic, Inc.

 *

 * Author: Richard Fitzgerald <rf@opensource.wolfsonmicro.com>

 R8     - Ctrl IF SPI CFG 1 */

 R32    - Tone Generator 1 */

 R33    - Tone Generator 2 */

 R34    - Tone Generator 3 */

 R35    - Tone Generator 4 */

 R36    - Tone Generator 5 */

 R48    - PWM Drive 1 */

 R49    - PWM Drive 2 */

 R50    - PWM Drive 3 */

 R65    - Sequence control */

 R97    - Sample Rate Sequence Select 1 */

 R98    - Sample Rate Sequence Select 2 */

 R99    - Sample Rate Sequence Select 3 */

 R100   - Sample Rate Sequence Select 4 */

 R112   - Comfort Noise Generator */

 R144   - Haptics Control 1 */

 R145   - Haptics Control 2 */

 R146   - Haptics phase 1 intensity */

 R147   - Haptics phase 1 duration */

 R148   - Haptics phase 2 intensity */

 R149   - Haptics phase 2 duration */

 R150   - Haptics phase 3 intensity */

 R151   - Haptics phase 3 duration */

 R256   - Clock 32k 1 */

 R257   - System Clock 1 */

 R258   - Sample rate 1 */

 R259   - Sample rate 2 */

 R260   - Sample rate 3 */

 R274   - Async clock 1 */

 R275   - Async sample rate 1 */

 R276   - Async sample rate 2 */

 R329   - Output system clock */

 R330   - Output async clock */

 R338   - Rate Estimator 1 */

 R339   - Rate Estimator 2 */

 R340   - Rate Estimator 3 */

 R341   - Rate Estimator 4 */

 R342   - Rate Estimator 5 */

 R369   - FLL1 Control 1 */

 R370   - FLL1 Control 2 */

 R371   - FLL1 Control 3 */

 R372   - FLL1 Control 4 */

 R373   - FLL1 Control 5 */

 R374   - FLL1 Control 6 */

 R376   - FLL1 Control 7 */

 R385   - FLL1 Synchroniser 1 */

 R386   - FLL1 Synchroniser 2 */

 R387   - FLL1 Synchroniser 3 */

 R388   - FLL1 Synchroniser 4 */

 R389   - FLL1 Synchroniser 5 */

 R390   - FLL1 Synchroniser 6 */

 R390   - FLL1 Synchroniser 7 */

 R393   - FLL1 Spread Spectrum */

 R394   - FLL1 GPIO Clock */

 R401   - FLL2 Control 1 */

 R402   - FLL2 Control 2 */

 R403   - FLL2 Control 3 */

 R404   - FLL2 Control 4 */

 R405   - FLL2 Control 5 */

 R406   - FLL2 Control 6 */

 R408   - FLL2 Control 7 */

 R417   - FLL2 Synchroniser 1 */

 R418   - FLL2 Synchroniser 2 */

 R419   - FLL2 Synchroniser 3 */

 R420   - FLL2 Synchroniser 4 */

 R421   - FLL2 Synchroniser 5 */

 R422   - FLL2 Synchroniser 6 */

 R422   - FLL2 Synchroniser 7 */

 R425   - FLL2 Spread Spectrum */

 R426   - FLL2 GPIO Clock */

 R536   - Mic Bias Ctrl 1 */

 R537   - Mic Bias Ctrl 2 */

 R768   - Input Enables */

 R776   - Input Rate */

 R777   - Input Volume Ramp */

 R780   - HPF Control */

 R784   - IN1L Control */

 R785   - ADC Digital Volume 1L */

 R786   - DMIC1L Control */

 R788   - IN1R Control */

 R789   - ADC Digital Volume 1R */

 R790   - DMIC1R Control */

 R792   - IN2L Control */

 R793   - ADC Digital Volume 2L */

 R794   - DMIC2L Control */

 R796   - IN2R Control */

 R797   - ADC Digital Volume 2R */

 R798   - DMIC2R Control */

 R1024  - Output Enables 1 */

 R1032  - Output Rate 1 */

 R1033  - Output Volume Ramp */

 R1040  - Output Path Config 1L */

 R1041  - DAC Digital Volume 1L */

 R1042  - DAC Volume Limit 1L */

 R1043  - Noise Gate Select 1L */

 R1045  - DAC Digital Volume 1R */

 R1046  - DAC Volume Limit 1R */

 R1047  - Noise Gate Select 1R */

 R1065  - DAC Digital Volume 4L */

 R1066  - Out Volume 4L */

 R1067  - Noise Gate Select 4L */

 R1104  - DAC AEC Control 1 */

 R1112  - Noise Gate Control */

 R1184  - HP1 Short Circuit Ctrl */

 R1280  - AIF1 BCLK Ctrl */

 R1281  - AIF1 Tx Pin Ctrl */

 R1282  - AIF1 Rx Pin Ctrl */

 R1283  - AIF1 Rate Ctrl */

 R1284  - AIF1 Format */

 R1285  - AIF1 Tx BCLK Rate */

 R1286  - AIF1 Rx BCLK Rate */

 R1287  - AIF1 Frame Ctrl 1 */

 R1288  - AIF1 Frame Ctrl 2 */

 R1289  - AIF1 Frame Ctrl 3 */

 R1290  - AIF1 Frame Ctrl 4 */

 R1291  - AIF1 Frame Ctrl 5 */

 R1292  - AIF1 Frame Ctrl 6 */

 R1293  - AIF1 Frame Ctrl 7 */

 R1294  - AIF1 Frame Ctrl 8 */

 R1295  - AIF1 Frame Ctrl 9 */

 R1296  - AIF1 Frame Ctrl 10 */

 R1297  - AIF1 Frame Ctrl 11 */

 R1298  - AIF1 Frame Ctrl 12 */

 R1299  - AIF1 Frame Ctrl 13 */

 R1300  - AIF1 Frame Ctrl 14 */

 R1301  - AIF1 Frame Ctrl 15 */

 R1302  - AIF1 Frame Ctrl 16 */

 R1303  - AIF1 Frame Ctrl 17 */

 R1304  - AIF1 Frame Ctrl 18 */

 R1305  - AIF1 Tx Enables */

 R1306  - AIF1 Rx Enables */

 R1344  - AIF2 BCLK Ctrl */

 R1345  - AIF2 Tx Pin Ctrl */

 R1346  - AIF2 Rx Pin Ctrl */

 R1347  - AIF2 Rate Ctrl */

 R1348  - AIF2 Format */

 R1349  - AIF2 Tx BCLK Rate */

 R1350  - AIF2 Rx BCLK Rate */

 R1351  - AIF2 Frame Ctrl 1 */

 R1352  - AIF2 Frame Ctrl 2 */

 R1353  - AIF2 Frame Ctrl 3 */

 R1354  - AIF2 Frame Ctrl 4 */

 R1355  - AIF2 Frame Ctrl 5 */

 R1356  - AIF2 Frame Ctrl 6 */

 R1357  - AIF2 Frame Ctrl 7 */

 R1358  - AIF2 Frame Ctrl 8 */

 R1361  - AIF2 Frame Ctrl 11 */

 R1362  - AIF2 Frame Ctrl 12 */

 R1363  - AIF2 Frame Ctrl 13 */

 R1364  - AIF2 Frame Ctrl 14 */

 R1365  - AIF2 Frame Ctrl 15 */

 R1366  - AIF2 Frame Ctrl 16 */

 R1369  - AIF2 Tx Enables */

 R1370  - AIF2 Rx Enables */

 R1408  - AIF3 BCLK Ctrl */

 R1409  - AIF3 Tx Pin Ctrl */

 R1410  - AIF3 Rx Pin Ctrl */

 R1411  - AIF3 Rate Ctrl */

 R1412  - AIF3 Format */

 R1413  - AIF3 Tx BCLK Rate */

 R1414  - AIF3 Rx BCLK Rate */

 R1415  - AIF3 Frame Ctrl 1 */

 R1416  - AIF3 Frame Ctrl 2 */

 R1417  - AIF3 Frame Ctrl 3 */

 R1418  - AIF3 Frame Ctrl 4 */

 R1425  - AIF3 Frame Ctrl 11 */

 R1426  - AIF3 Frame Ctrl 12 */

 R1433  - AIF3 Tx Enables */

 R1434  - AIF3 Rx Enables */

 R1600  - PWM1MIX Input 1 Source */

 R1601  - PWM1MIX Input 1 Volume */

 R1602  - PWM1MIX Input 2 Source */

 R1603  - PWM1MIX Input 2 Volume */

 R1604  - PWM1MIX Input 3 Source */

 R1605  - PWM1MIX Input 3 Volume */

 R1606  - PWM1MIX Input 4 Source */

 R1607  - PWM1MIX Input 4 Volume */

 R1608  - PWM2MIX Input 1 Source */

 R1609  - PWM2MIX Input 1 Volume */

 R1610  - PWM2MIX Input 2 Source */

 R1611  - PWM2MIX Input 2 Volume */

 R1612  - PWM2MIX Input 3 Source */

 R1613  - PWM2MIX Input 3 Volume */

 R1614  - PWM2MIX Input 4 Source */

 R1615  - PWM2MIX Input 4 Volume */

 R1664  - OUT1LMIX Input 1 Source */

 R1665  - OUT1LMIX Input 1 Volume */

 R1666  - OUT1LMIX Input 2 Source */

 R1667  - OUT1LMIX Input 2 Volume */

 R1668  - OUT1LMIX Input 3 Source */

 R1669  - OUT1LMIX Input 3 Volume */

 R1670  - OUT1LMIX Input 4 Source */

 R1671  - OUT1LMIX Input 4 Volume */

 R1672  - OUT1RMIX Input 1 Source */

 R1673  - OUT1RMIX Input 1 Volume */

 R1674  - OUT1RMIX Input 2 Source */

 R1675  - OUT1RMIX Input 2 Volume */

 R1676  - OUT1RMIX Input 3 Source */

 R1677  - OUT1RMIX Input 3 Volume */

 R1678  - OUT1RMIX Input 4 Source */

 R1679  - OUT1RMIX Input 4 Volume */

 R1712  - OUT4LMIX Input 1 Source */

 R1713  - OUT4LMIX Input 1 Volume */

 R1714  - OUT4LMIX Input 2 Source */

 R1715  - OUT4LMIX Input 2 Volume */

 R1716  - OUT4LMIX Input 3 Source */

 R1717  - OUT4LMIX Input 3 Volume */

 R1718  - OUT4LMIX Input 4 Source */

 R1719  - OUT4LMIX Input 4 Volume */

 R1792  - AIF1TX1MIX Input 1 Source */

 R1793  - AIF1TX1MIX Input 1 Volume */

 R1794  - AIF1TX1MIX Input 2 Source */

 R1795  - AIF1TX1MIX Input 2 Volume */

 R1796  - AIF1TX1MIX Input 3 Source */

 R1797  - AIF1TX1MIX Input 3 Volume */

 R1798  - AIF1TX1MIX Input 4 Source */

 R1799  - AIF1TX1MIX Input 4 Volume */

 R1800  - AIF1TX2MIX Input 1 Source */

 R1801  - AIF1TX2MIX Input 1 Volume */

 R1802  - AIF1TX2MIX Input 2 Source */

 R1803  - AIF1TX2MIX Input 2 Volume */

 R1804  - AIF1TX2MIX Input 3 Source */

 R1805  - AIF1TX2MIX Input 3 Volume */

 R1806  - AIF1TX2MIX Input 4 Source */

 R1807  - AIF1TX2MIX Input 4 Volume */

 R1808  - AIF1TX3MIX Input 1 Source */

 R1809  - AIF1TX3MIX Input 1 Volume */

 R1810  - AIF1TX3MIX Input 2 Source */

 R1811  - AIF1TX3MIX Input 2 Volume */

 R1812  - AIF1TX3MIX Input 3 Source */

 R1813  - AIF1TX3MIX Input 3 Volume */

 R1814  - AIF1TX3MIX Input 4 Source */

 R1815  - AIF1TX3MIX Input 4 Volume */

 R1816  - AIF1TX4MIX Input 1 Source */

 R1817  - AIF1TX4MIX Input 1 Volume */

 R1818  - AIF1TX4MIX Input 2 Source */

 R1819  - AIF1TX4MIX Input 2 Volume */

 R1820  - AIF1TX4MIX Input 3 Source */

 R1821  - AIF1TX4MIX Input 3 Volume */

 R1822  - AIF1TX4MIX Input 4 Source */

 R1823  - AIF1TX4MIX Input 4 Volume */

 R1824  - AIF1TX5MIX Input 1 Source */

 R1825  - AIF1TX5MIX Input 1 Volume */

 R1826  - AIF1TX5MIX Input 2 Source */

 R1827  - AIF1TX5MIX Input 2 Volume */

 R1828  - AIF1TX5MIX Input 3 Source */

 R1829  - AIF1TX5MIX Input 3 Volume */

 R1830  - AIF1TX5MIX Input 4 Source */

 R1831  - AIF1TX5MIX Input 4 Volume */

 R1832  - AIF1TX6MIX Input 1 Source */

 R1833  - AIF1TX6MIX Input 1 Volume */

 R1834  - AIF1TX6MIX Input 2 Source */

 R1835  - AIF1TX6MIX Input 2 Volume */

 R1836  - AIF1TX6MIX Input 3 Source */

 R1837  - AIF1TX6MIX Input 3 Volume */

 R1838  - AIF1TX6MIX Input 4 Source */

 R1839  - AIF1TX6MIX Input 4 Volume */

 R1840  - AIF1TX7MIX Input 1 Source */

 R1841  - AIF1TX7MIX Input 1 Volume */

 R1842  - AIF1TX7MIX Input 2 Source */

 R1843  - AIF1TX7MIX Input 2 Volume */

 R1844  - AIF1TX7MIX Input 3 Source */

 R1845  - AIF1TX7MIX Input 3 Volume */

 R1846  - AIF1TX7MIX Input 4 Source */

 R1847  - AIF1TX7MIX Input 4 Volume */

 R1848  - AIF1TX8MIX Input 1 Source */

 R1849  - AIF1TX8MIX Input 1 Volume */

 R1850  - AIF1TX8MIX Input 2 Source */

 R1851  - AIF1TX8MIX Input 2 Volume */

 R1852  - AIF1TX8MIX Input 3 Source */

 R1853  - AIF1TX8MIX Input 3 Volume */

 R1854  - AIF1TX8MIX Input 4 Source */

 R1855  - AIF1TX8MIX Input 4 Volume */

 R1856  - AIF2TX1MIX Input 1 Source */

 R1857  - AIF2TX1MIX Input 1 Volume */

 R1858  - AIF2TX1MIX Input 2 Source */

 R1859  - AIF2TX1MIX Input 2 Volume */

 R1860  - AIF2TX1MIX Input 3 Source */

 R1861  - AIF2TX1MIX Input 3 Volume */

 R1862  - AIF2TX1MIX Input 4 Source */

 R1863  - AIF2TX1MIX Input 4 Volume */

 R1864  - AIF2TX2MIX Input 1 Source */

 R1865  - AIF2TX2MIX Input 1 Volume */

 R1866  - AIF2TX2MIX Input 2 Source */

 R1867  - AIF2TX2MIX Input 2 Volume */

 R1868  - AIF2TX2MIX Input 3 Source */

 R1869  - AIF2TX2MIX Input 3 Volume */

 R1870  - AIF2TX2MIX Input 4 Source */

 R1871  - AIF2TX2MIX Input 4 Volume */

 R1872  - AIF2TX3MIX Input 1 Source */

 R1873  - AIF2TX3MIX Input 1 Volume */

 R1874  - AIF2TX3MIX Input 2 Source */

 R1875  - AIF2TX3MIX Input 2 Volume */

 R1876  - AIF2TX3MIX Input 3 Source */

 R1877  - AIF2TX3MIX Input 3 Volume */

 R1878  - AIF2TX3MIX Input 4 Source */

 R1879  - AIF2TX3MIX Input 4 Volume */

 R1880  - AIF2TX4MIX Input 1 Source */

 R1881  - AIF2TX4MIX Input 1 Volume */

 R1882  - AIF2TX4MIX Input 2 Source */

 R1883  - AIF2TX4MIX Input 2 Volume */

 R1884  - AIF2TX4MIX Input 3 Source */

 R1885  - AIF2TX4MIX Input 3 Volume */

 R1886  - AIF2TX4MIX Input 4 Source */

 R1887  - AIF2TX4MIX Input 4 Volume */

 R1888  - AIF2TX5MIX Input 1 Source */

 R1889  - AIF2TX5MIX Input 1 Volume */

 R1890  - AIF2TX5MIX Input 2 Source */

 R1891  - AIF2TX5MIX Input 2 Volume */

 R1892  - AIF2TX5MIX Input 3 Source */

 R1893  - AIF2TX5MIX Input 3 Volume */

 R1894  - AIF2TX5MIX Input 4 Source */

 R1895  - AIF2TX5MIX Input 4 Volume */

 R1896  - AIF2TX6MIX Input 1 Source */

 R1897  - AIF2TX6MIX Input 1 Volume */

 R1898  - AIF2TX6MIX Input 2 Source */

 R1899  - AIF2TX6MIX Input 2 Volume */

 R1900  - AIF2TX6MIX Input 3 Source */

 R1901  - AIF2TX6MIX Input 3 Volume */

 R1902  - AIF2TX6MIX Input 4 Source */

 R1903  - AIF2TX6MIX Input 4 Volume */

 R1920  - AIF3TX1MIX Input 1 Source */

 R1921  - AIF3TX1MIX Input 1 Volume */

 R1922  - AIF3TX1MIX Input 2 Source */

 R1923  - AIF3TX1MIX Input 2 Volume */

 R1924  - AIF3TX1MIX Input 3 Source */

 R1925  - AIF3TX1MIX Input 3 Volume */

 R1926  - AIF3TX1MIX Input 4 Source */

 R1927  - AIF3TX1MIX Input 4 Volume */

 R1928  - AIF3TX2MIX Input 1 Source */

 R1929  - AIF3TX2MIX Input 1 Volume */

 R1930  - AIF3TX2MIX Input 2 Source */

 R1931  - AIF3TX2MIX Input 2 Volume */

 R1932  - AIF3TX2MIX Input 3 Source */

 R1933  - AIF3TX2MIX Input 3 Volume */

 R1934  - AIF3TX2MIX Input 4 Source */

 R1935  - AIF3TX2MIX Input 4 Volume */

 R2176  - EQ1MIX Input 1 Source */

 R2177  - EQ1MIX Input 1 Volume */

 R2178  - EQ1MIX Input 2 Source */

 R2179  - EQ1MIX Input 2 Volume */

 R2180  - EQ1MIX Input 3 Source */

 R2181  - EQ1MIX Input 3 Volume */

 R2182  - EQ1MIX Input 4 Source */

 R2183  - EQ1MIX Input 4 Volume */

 R2184  - EQ2MIX Input 1 Source */

 R2185  - EQ2MIX Input 1 Volume */

 R2186  - EQ2MIX Input 2 Source */

 R2187  - EQ2MIX Input 2 Volume */

 R2188  - EQ2MIX Input 3 Source */

 R2189  - EQ2MIX Input 3 Volume */

 R2190  - EQ2MIX Input 4 Source */

 R2191  - EQ2MIX Input 4 Volume */

 R2240  - DRC1LMIX Input 1 Source */

 R2241  - DRC1LMIX Input 1 Volume */

 R2242  - DRC1LMIX Input 2 Source */

 R2243  - DRC1LMIX Input 2 Volume */

 R2244  - DRC1LMIX Input 3 Source */

 R2245  - DRC1LMIX Input 3 Volume */

 R2246  - DRC1LMIX Input 4 Source */

 R2247  - DRC1LMIX Input 4 Volume */

 R2248  - DRC1RMIX Input 1 Source */

 R2249  - DRC1RMIX Input 1 Volume */

 R2250  - DRC1RMIX Input 2 Source */

 R2251  - DRC1RMIX Input 2 Volume */

 R2252  - DRC1RMIX Input 3 Source */

 R2253  - DRC1RMIX Input 3 Volume */

 R2254  - DRC1RMIX Input 4 Source */

 R2255  - DRC1RMIX Input 4 Volume */

 R2256  - DRC2LMIX Input 1 Source */

 R2257  - DRC2LMIX Input 1 Volume */

 R2258  - DRC2LMIX Input 2 Source */

 R2259  - DRC2LMIX Input 2 Volume */

 R2260  - DRC2LMIX Input 3 Source */

 R2261  - DRC2LMIX Input 3 Volume */

 R2262  - DRC2LMIX Input 4 Source */

 R2263  - DRC2LMIX Input 4 Volume */

 R2264  - DRC2RMIX Input 1 Source */

 R2265  - DRC2RMIX Input 1 Volume */

 R2266  - DRC2RMIX Input 2 Source */

 R2267  - DRC2RMIX Input 2 Volume */

 R2268  - DRC2RMIX Input 3 Source */

 R2269  - DRC2RMIX Input 3 Volume */

 R2270  - DRC2RMIX Input 4 Source */

 R2271  - DRC2RMIX Input 4 Volume */

 R2304  - HPLP1MIX Input 1 Source */

 R2305  - HPLP1MIX Input 1 Volume */

 R2306  - HPLP1MIX Input 2 Source */

 R2307  - HPLP1MIX Input 2 Volume */

 R2308  - HPLP1MIX Input 3 Source */

 R2309  - HPLP1MIX Input 3 Volume */

 R2310  - HPLP1MIX Input 4 Source */

 R2311  - HPLP1MIX Input 4 Volume */

 R2312  - HPLP2MIX Input 1 Source */

 R2313  - HPLP2MIX Input 1 Volume */

 R2314  - HPLP2MIX Input 2 Source */

 R2315  - HPLP2MIX Input 2 Volume */

 R2316  - HPLP2MIX Input 3 Source */

 R2317  - HPLP2MIX Input 3 Volume */

 R2318  - HPLP2MIX Input 4 Source */

 R2319  - HPLP2MIX Input 4 Volume */

 R2320  - HPLP3MIX Input 1 Source */

 R2321  - HPLP3MIX Input 1 Volume */

 R2322  - HPLP3MIX Input 2 Source */

 R2323  - HPLP3MIX Input 2 Volume */

 R2324  - HPLP3MIX Input 3 Source */

 R2325  - HPLP3MIX Input 3 Volume */

 R2326  - HPLP3MIX Input 4 Source */

 R2327  - HPLP3MIX Input 4 Volume */

 R2328  - HPLP4MIX Input 1 Source */

 R2329  - HPLP4MIX Input 1 Volume */

 R2330  - HPLP4MIX Input 2 Source */

 R2331  - HPLP4MIX Input 2 Volume */

 R2332  - HPLP4MIX Input 3 Source */

 R2333  - HPLP4MIX Input 3 Volume */

 R2334  - HPLP4MIX Input 4 Source */

 R2335  - HPLP4MIX Input 4 Volume */

 R2432  - DSP2LMIX Input 1 Source */

 R2433  - DSP2LMIX Input 1 Volume */

 R2434  - DSP2LMIX Input 2 Source */

 R2435  - DSP2LMIX Input 2 Volume */

 R2436  - DSP2LMIX Input 3 Source */

 R2437  - DSP2LMIX Input 3 Volume */

 R2438  - DSP2LMIX Input 4 Source */

 R2439  - DSP2LMIX Input 4 Volume */

 R2440  - DSP2RMIX Input 1 Source */

 R2441  - DSP2RMIX Input 1 Volume */

 R2442  - DSP2RMIX Input 2 Source */

 R2443  - DSP2RMIX Input 2 Volume */

 R2444  - DSP2RMIX Input 3 Source */

 R2445  - DSP2RMIX Input 3 Volume */

 R2446  - DSP2RMIX Input 4 Source */

 R2447  - DSP2RMIX Input 4 Volume */

 R2448  - DSP2AUX1MIX Input 1 Source */

 R2456  - DSP2AUX2MIX Input 1 Source */

 R2464  - DSP2AUX3MIX Input 1 Source */

 R2472  - DSP2AUX4MIX Input 1 Source */

 R2480  - DSP2AUX5MIX Input 1 Source */

 R2488  - DSP2AUX6MIX Input 1 Source */

 R2496  - DSP3LMIX Input 1 Source */

 R2497  - DSP3LMIX Input 1 Volume */

 R2498  - DSP3LMIX Input 2 Source */

 R2499  - DSP3LMIX Input 2 Volume */

 R2500  - DSP3LMIX Input 3 Source */

 R2501  - DSP3LMIX Input 3 Volume */

 R2502  - DSP3LMIX Input 4 Source */

 R2503  - DSP3LMIX Input 4 Volume */

 R2504  - DSP3RMIX Input 1 Source */

 R2505  - DSP3RMIX Input 1 Volume */

 R2506  - DSP3RMIX Input 2 Source */

 R2507  - DSP3RMIX Input 2 Volume */

 R2508  - DSP3RMIX Input 3 Source */

 R2509  - DSP3RMIX Input 3 Volume */

 R2510  - DSP3RMIX Input 4 Source */

 R2511  - DSP3RMIX Input 4 Volume */

 R2512  - DSP3AUX1MIX Input 1 Source */

 R2520  - DSP3AUX2MIX Input 1 Source */

 R2528  - DSP3AUX3MIX Input 1 Source */

 R2536  - DSP3AUX4MIX Input 1 Source */

 R2544  - DSP3AUX5MIX Input 1 Source */

 R2552  - DSP3AUX6MIX Input 1 Source */

 R2688  - ASRC1LMIX Input 1 Source */

 R2696  - ASRC1RMIX Input 1 Source */

 R2704  - ASRC2LMIX Input 1 Source */

 R2712  - ASRC2RMIX Input 1 Source */

 R2816  - ISRC1DEC1MIX Input 1 Source */

 R2824  - ISRC1DEC2MIX Input 1 Source */

 R2832  - ISRC1DEC3MIX Input 1 Source */

 R2840  - ISRC1DEC4MIX Input 1 Source */

 R2848  - ISRC1INT1MIX Input 1 Source */

 R2856  - ISRC1INT2MIX Input 1 Source */

 R2864  - ISRC1INT3MIX Input 1 Source */

 R2872  - ISRC1INT4MIX Input 1 Source */

 R2880  - ISRC2DEC1MIX Input 1 Source */

 R2888  - ISRC2DEC2MIX Input 1 Source */

 R2896  - ISRC2DEC3MIX Input 1 Source */

 R2904  - ISRC2DEC4MIX Input 1 Source */

 R2912  - ISRC2INT1MIX Input 1 Source */

 R2920  - ISRC2INT2MIX Input 1 Source */

 R2928  - ISRC2INT3MIX Input 1 Source */

 R2936  - ISRC2INT4MIX Input 1 Source */

 R2944  - ISRC3DEC1MIX Input 1 Source */

 R2952  - ISRC3DEC2MIX Input 1 Source */

 R2960  - ISRC3DEC3MIX Input 1 Source */

 R2968  - ISRC3DEC4MIX Input 1 Source */

 R2976  - ISRC3INT1MIX Input 1 Source */

 R2984  - ISRC3INT2MIX Input 1 Source */

 R2992  - ISRC3INT3MIX Input 1 Source */

 R3000  - ISRC3INT4MIX Input 1 Source */

 R3072  - GPIO1 CTRL */

 R3073  - GPIO2 CTRL */

 R3087  - IRQ CTRL 1 */

 R3088  - GPIO Debounce Config */

 R3104  - Misc Pad Ctrl 1 */

 R3105  - Misc Pad Ctrl 2 */

 R3106  - Misc Pad Ctrl 3 */

 R3107  - Misc Pad Ctrl 4 */

 R3108  - Misc Pad Ctrl 5 */

 R3109  - Misc Pad Ctrl 6 */

 R3120  - Misc Pad Ctrl 7 */

 R3122  - Misc Pad Ctrl 9 */

 R3123  - Misc Pad Ctrl 10 */

 R3124  - Misc Pad Ctrl 11 */

 R3125  - Misc Pad Ctrl 12 */

 R3126  - Misc Pad Ctrl 13 */

 R3127  - Misc Pad Ctrl 14 */

 R3129  - Misc Pad Ctrl 16 */

 R3336  - Interrupt Status 1 Mask */

 R3337  - Interrupt Status 2 Mask */

 R3338  - Interrupt Status 3 Mask */

 R3339  - Interrupt Status 4 Mask */

 R3340  - Interrupt Status 5 Mask */

 R3341  - Interrupt Status 6 Mask */

 R3343  - Interrupt Control */

 R3352  - IRQ2 Status 1 Mask */

 R3353  - IRQ2 Status 2 Mask */

 R3354  - IRQ2 Status 3 Mask */

 R3355  - IRQ2 Status 4 Mask */

 R3356  - IRQ2 Status 5 Mask */

 R3357  - IRQ2 Status 6 Mask */

 R3359  - IRQ2 Control */

 R3584  - FX_Ctrl1 */

 R3600  - EQ1_1 */

 R3601  - EQ1_2 */

 R3602  - EQ1_3 */

 R3603  - EQ1_4 */

 R3604  - EQ1_5 */

 R3605  - EQ1_6 */

 R3606  - EQ1_7 */

 R3607  - EQ1_8 */

 R3608  - EQ1_9 */

 R3609  - EQ1_10 */

 R3610  - EQ1_11 */

 R3611  - EQ1_12 */

 R3612  - EQ1_13 */

 R3613  - EQ1_14 */

 R3614  - EQ1_15 */

 R3615  - EQ1_16 */

 R3616  - EQ1_17 */

 R3617  - EQ1_18 */

 R3618  - EQ1_19 */

 R3619  - EQ1_20 */

 R3620  - EQ1_21 */

 R3622  - EQ2_1 */

 R3623  - EQ2_2 */

 R3624  - EQ2_3 */

 R3625  - EQ2_4 */

 R3626  - EQ2_5 */

 R3627  - EQ2_6 */

 R3628  - EQ2_7 */

 R3629  - EQ2_8 */

 R3630  - EQ2_9 */

 R3631  - EQ2_10 */

 R3632  - EQ2_11 */

 R3633  - EQ2_12 */

 R3634  - EQ2_13 */

 R3635  - EQ2_14 */

 R3636  - EQ2_15 */

 R3637  - EQ2_16 */

 R3638  - EQ2_17 */

 R3639  - EQ2_18 */

 R3640  - EQ2_19 */

 R3641  - EQ2_20 */

 R3642  - EQ2_21 */

 R3712  - DRC1 ctrl1 */

 R3713  - DRC1 ctrl2 */

 R3714  - DRC1 ctrl3 */

 R3715  - DRC1 ctrl4 */

 R3716  - DRC1 ctrl5 */

 R3721  - DRC2 ctrl1 */

 R3722  - DRC2 ctrl2 */

 R3723  - DRC2 ctrl3 */

 R3724  - DRC2 ctrl4 */

 R3725  - DRC2 ctrl5 */

 R3776  - HPLPF1_1 */

 R3777  - HPLPF1_2 */

 R3780  - HPLPF2_1 */

 R3781  - HPLPF2_2 */

 R3784  - HPLPF3_1 */

 R3785  - HPLPF3_2 */

 R3788  - HPLPF4_1 */

 R3789  - HPLPF4_2 */

 R3808  - ASRC_ENABLE */

 R3810  - ASRC_RATE1 */

 R3811  - ASRC_RATE2 */

 R3824  - ISRC 1 CTRL 1 */

 R3825  - ISRC 1 CTRL 2 */

 R3826  - ISRC 1 CTRL 3 */

 R3827  - ISRC 2 CTRL 1 */

 R3828  - ISRC 2 CTRL 2 */

 R3829  - ISRC 2 CTRL 3 */

 R3830  - ISRC 3 CTRL 1 */

 R3831  - ISRC 3 CTRL 2 */

 R3832  - ISRC 3 CTRL 3 */

 R4608  - DSP2 Control 1 */

 R4864  - DSP3 Control 1 */

 DSP2 PM */

 DSP2 ZM */

 DSP2 XM */

 DSP2 YM */

 DSP3 PM */

 DSP3 ZM */

 DSP3 XM */

 DSP3 YM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I2C driver for the X-Powers' Power Management ICs

 *

 * AXP20x typically comprises an adaptive USB-Compatible PWM charger, BUCK DC-DC

 * converters, LDOs, multiple 12-bit ADCs of voltage, current and temperature

 * as well as configurable GPIOs.

 *

 * This driver supports the I2C variants.

 *

 * Copyright (C) 2014 Carlo Caione

 *

 * Author: Carlo Caione <carlo@caione.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * tps65010 - driver for tps6501x power management chips

 *

 * Copyright (C) 2004 Texas Instruments

 * Copyright (C) 2004-2005 David Brownell

-------------------------------------------------------------------------*/

-------------------------------------------------------------------------*/

/* This driver handles a family of multipurpose chips, which incorporate

 * voltage regulators, lithium ion/polymer battery charging, GPIOs, LEDs,

 * and other features often needed in portable devices like cell phones

 * or digital cameras.

 *

 * The tps65011 and tps65013 have different voltage settings compared

 * to tps65010 and tps65012.  The tps65013 has a NO_CHG status/irq.

 * All except tps65010 have "wait" mode, possibly defaulted so that

 * battery-insert != device-on.

 *

 * We could distinguish between some models by checking VDCDC1.UVLO or

 * other registers, unless they've been changed already after powerup

 * as part of board setup by a bootloader.

 copies of last register state */

-------------------------------------------------------------------------*/

	/* FIXME how can we tell whether a battery is present?

	 * likely involves a charge gauging chip (like BQ26501).

	/* registers for monitoring battery charging and status; note

	 * that reading chgstat and regstat may ack IRQs...

 ignore ackint1 */

 ignore ackint2 */

 VMAIN voltage, enable lowpower, etc */

 VCORE voltage, vibrator on/off */

 both LD0s, and their lowpower behavior */

 LEDs and GPIOs */

-------------------------------------------------------------------------*/

 handle IRQS in a task context, so we can use I2C calls */

	/* IRQs won't trigger for certain events, but we can get

	 * others by polling (normally, with external power applied).

 regstatus irqs */

 may need to shut something down ... */

 "off" usually means deep sleep */

			/* REVISIT:  this might need its own workqueue

			 * plus tweaks including deadlock avoidance ...

			 * also needs to get error handling and probably

			 * an #ifdef CONFIG_HIBERNATION

 chgstatus irqs */

		/* Unless it was turned off or disabled, we charge any

		 * battery whenever there's power available for it

		 * and the charger hasn't been disabled.

 VBUS options are readonly until reconnect */

	/* always poll to detect (a) power removal, without tps65013

	 * NO_CHG IRQ; or (b) restart of charging after stop.

 also potentially gpio-in rise or fall */

 handle IRQs and polling using keventd for now */

 vbus update fails unless VBUS is connected! */

-------------------------------------------------------------------------*/

/* offsets 0..3 == GPIO1..GPIO4

 * offsets 4..5 == LED1/nPG, LED2 (we set one of the non-BLINK modes)

 * offset 6 == vibrator motor driver

 GPIOs may be input-only */

 output */

 input */

 REVISIT we *could* report LED1/nPG and LED2 state ... */

-------------------------------------------------------------------------*/

	/* the IRQ is active low, but many gpio lines can't support that

	 * so this driver uses falling-edge triggers instead.

		/* annoying race here, ideally we'd have an option

		 * to claim the irq now and enable it later.

		 * FIXME genirq IRQF_NOAUTOEN now solves that ...

 else CHGCONFIG.POR is replaced by AUA, enabling a WAIT mode */

	/* USB hosts can't draw VBUS.  OTG devices could, later

	 * when OTG infrastructure enables it.  USB peripherals

	 * could be relying on VBUS while booting, though.

	/* unmask the "interesting" irqs, then poll once to

	 * kickstart monitoring, initialize shadowed status

	 * registers, and maybe disable VBUS draw.

 optionally register GPIOs */

 NOTE:  only partial support for inputs; nyet IRQs */

 tps65011 charging at 6.5V max */

-------------------------------------------------------------------------*/

/* Draw from VBUS:

 *   0 mA -- DON'T DRAW (might supply power instead)

 * 100 mA -- usb unit load (slowest charge rate)

 * 500 mA -- usb high power (fast battery charge)

 assumes non-SMP */

 gadget drivers call this in_irq() */

-------------------------------------------------------------------------*/

/* tps65010_set_gpio_out_value parameter:

 * gpio:  GPIO1, GPIO2, GPIO3 or GPIO4

 * value: LOW or HIGH

 Configure GPIO for output */

 Writing 1 forces a logic 0 on that GPIO and vice versa */

 set GPIO low by writing 1 */

 case HIGH: */

 set GPIO high by writing 0 */

-------------------------------------------------------------------------*/

/* tps65010_set_led parameter:

 * led:  LED1 or LED2

 * mode: ON, OFF or BLINK

-------------------------------------------------------------------------*/

/* tps65010_set_vib parameter:

 * value: ON or OFF

-------------------------------------------------------------------------*/

/* tps65010_set_low_pwr parameter:

 * mode: ON or OFF

 disable ENABLE_LP bit */

 case ON: */

 enable ENABLE_LP bit */

-------------------------------------------------------------------------*/

/* tps65010_config_vregs1 parameter:

 * value to be written to VREGS1 register

 * Note: The complete register is written, set all bits you need

-------------------------------------------------------------------------*/

/* tps65013_set_low_pwr parameter:

 * mode: ON or OFF

/* FIXME: Assumes AC or USB power is present. Setting AUA bit is not

 disable AUA bit */

 disable ENABLE_LP bit */

 case ON: */

 enable AUA bit */

 enable ENABLE_LP bit */

-------------------------------------------------------------------------*/

/* NOTE:  this MUST be initialized before the other parts of the system

 * that rely on it ... but after the i2c bus on which this relies.

 * That is, much earlier than on PC-type systems, which don't often use

 * I2C as a core system bus.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MFD driver for Active-semi ACT8945a PMIC

 *

 * Copyright (C) 2015 Atmel Corporation.

 *

 * Author: Wenyou Yang <wenyou.yang@atmel.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD driver for TWL6040 audio device

 *

 * Authors:	Misael Lopez Cruz <misael.lopez@ti.com>

 *		Jorge Eduardo Candelaria <jorge.candelaria@ti.com>

 *		Peter Ujfalusi <peter.ujfalusi@ti.com>

 *

 * Copyright:	(C) 2011 Texas Instruments, Inc.

 REG_ASICID	(ro) */

 REG_ASICREV	(ro) */

 REG_INTID	*/

 REG_INTMR	*/

 REG_NCPCTRL	*/

 REG_LDOCTL	*/

 REG_HPPLLCTL	*/

 REG_LPPLLCTL	*/

 REG_LPPLLDIV	*/

 REG_AMICBCTL	*/

 REG_DMICBCTL	*/

 REG_MICLCTL	*/

 REG_MICRCTL	*/

 REG_MICGAIN	*/

 REG_LINEGAIN	*/

 REG_HSLCTL	*/

 REG_HSRCTL	*/

 REG_HSGAIN	*/

 REG_EARCTL	*/

 REG_HFLCTL	*/

 REG_HFLGAIN	*/

 REG_HFRCTL	*/

 REG_HFRGAIN	*/

 REG_VIBCTLL	*/

 REG_VIBDATL	*/

 REG_VIBCTLR	*/

 REG_VIBDATR	*/

 REG_HKCTL1	*/

 REG_HKCTL2	*/

 REG_GPOCTL	*/

 REG_ALB	*/

 REG_DLB	*/

 0x28, REG_TRIM1 */

 0x29, REG_TRIM2 */

 0x2A, REG_TRIM3 */

 0x2B, REG_HSOTRIM */

 0x2C, REG_HFOTRIM */

 REG_ACCCTL	*/

 REG_STATUS	(ro) */

	/*

	 * Select I2C bus access to dual access registers

	 * Interrupt register is cleared on read

	 * Select fast mode for i2c (400KHz)

 twl6040 codec manual power-up sequence */

 enable high-side LDO, reference system and internal oscillator */

 enable negative charge pump */

 enable low-side LDO */

 enable low-power PLL */

 disable internal oscillator */

 twl6040 manual power-down sequence */

 enable internal oscillator */

 disable low-power PLL */

 disable low-side LDO */

 disable negative charge pump */

 disable high-side LDO, reference system and internal oscillator */

 already powered-up */

 Allow writes to the chip */

 use automatic power-up sequence */

 use manual power-up sequence */

		/*

		 * Register access can produce errors after power-up unless we

		 * wait at least 8ms based on measurements on duovero.

 Sync with the HW */

 Default PLL configuration after power up */

 already powered-down */

 use AUDPWRON line */

 power-down sequence latency */

 use manual power-down sequence */

 Set regmap to cache only and mark it as dirty */

 Force full reconfiguration when switching between PLL */

 low-power PLL divider */

 Change the sysclk configuration only if it has been canged */

 The PLL in use has not been change, we can exit */

 high-performance PLL can provide only 19.2 MHz */

 PLL enabled, active mode */

 PLL enabled, bypass mode */

 PLL enabled, active mode */

 PLL enabled, bypass mode */

 When switching to HPPLL, enable the mclk first */

			/*

			 * enable clock slicer to ensure input waveform is

			 * square

 Get the combined status of the vibra control register */

 Register 0 is not readable */

 0x2e */

 In order to operate correctly we need valid interrupt config */

 ERRATA: Automatic power-up is not possible in ES1.0 */

 Clear any pending interrupt */

	/*

	 * The main functionality of twl6040 to provide audio on OMAP4+ systems.

	 * We can add the ASoC codec child whenever this driver has been loaded.

 Vibra input driver support */

 GPO support */

 PDM clock support  */

 The chip is powered down so mark regmap to cache only and dirty */

 SPDX-License-Identifier: GPL-2.0+



 max14577.c - mfd core driver for the Maxim 14577/77836



 Copyright (C) 2014 Samsung Electronics

 Chanwoo Choi <cw00.choi@samsung.com>

 Krzysztof Kozlowski <krzk@kernel.org>



 This driver is based on max8997.c

/*

 * Table of valid charger currents for different Maxim chipsets.

 * It is placed here because it is used by both charger and regulator driver.

/*

 * maxim_charger_calc_reg_current - Calculate register value for current

 * @limits:	constraints for charger, matching the MBCICHWRC register

 * @min_ua:	minimal requested current, micro Amps

 * @max_ua:	maximum requested current, micro Amps

 * @dst:	destination to store calculated register value

 *

 * Calculates the value of MBCICHWRC (Fast Battery Charge Current) register

 * for given current and stores it under pointed 'dst'. The stored value

 * combines low bit (MBCICHWRCL) and high bits (MBCICHWRCH). It is also

 * properly shifted.

 *

 * The calculated register value matches the current which:

 *  - is always between <limits.min, limits.max>;

 *  - is always less or equal to max_ua;

 *  - is the highest possible value;

 *  - may be lower than min_ua.

 *

 * On success returns 0. On error returns -EINVAL (requested min/max current

 * is outside of given charger limits) and 'dst' is not set.

		/*

		 * Less than high_start, so set the minimal current

		 * (turn Low Bit off, 0 as high bits).

 max_ua is in range: <high_start, infinite>, cut it to limits.max */

	/*

	 * There is no risk of overflow 'max_ua' here because:

	 *  - max_ua >= limits.high_start

	 *  - BUILD_BUG checks that 'limits' are: max >= high_start + high_step

 Turn Low Bit on (use range <limits.high_start, limits.max>) ... */

 and set proper High Bits */

 Any max14577 volatile registers are also max77836 volatile. */

 INT1 interrupts */

 INT2 interrupts */

 INT3 interrupts */

 INT1 interrupts */

 INT2 interrupts */

 INT3 interrupts */

/*

 * Max77836 specific initialization code for driver probe.

 * Adds new I2C dummy device, regmap and regmap IRQ chip.

 * Unmasks Interrupt Source register.

 *

 * On success returns 0.

 * On failure returns errno and reverts any changes done so far (e.g. remove

 * I2C dummy device), except masking the INT SRC register.

 Un-mask MAX77836 Interrupt Source register */

/*

 * Max77836 specific de-initialization code for driver remove.

 Max77836 specific initialization code (additional regmap) */

	/*

	 * MUIC IRQ must be disabled during suspend because if it happens

	 * while suspended it will be handled before resuming I2C.

	 *

	 * When device is woken up from suspend (e.g. by ADC change),

	 * an interrupt occurs before resuming I2C bus controller.

	 * Interrupt handler tries to read registers but this read

	 * will fail because I2C is still suspended.

 CONFIG_PM_SLEEP */

 Valid charger current values must be provided for each chipset */

 Check for valid values for charger */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SPI bus interface to Cirrus Logic Madera codecs

 *

 * Copyright (C) 2015-2018 Cirrus Logic

 it's polite to say which codec isn't built into the kernel */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core MFD support for Cirrus Logic Madera codecs

 *

 * Copyright (C) 2015-2018 Cirrus Logic

 We only need MICVDD */

 We only need MICVDD */

 We only need MICVDD */

 We only need MICVDD */

 We only need MICVDD */

 Used by madera-i2c and madera-spi drivers */

	/*

	 * We can't use an interrupt as we need to runtime resume to do so,

	 * so we poll the status bit. This won't race with the interrupt

	 * handler because it will be blocked on runtime resume.

	 * The chip could NAK a read request while it is booting so ignore

	 * errors from regmap_read.

	/*

	 * BOOT_DONE defaults to unmasked on boot so we must ack it.

	 * Do this even after a timeout to avoid interrupt storms.

 Allow time for internal clocks to startup after reset */

	/*

	 * There are many existing out-of-tree users of these codecs that we

	 * can't break so preserve the expected behaviour of setting the line

	 * low to assert reset.

	/*

	 * A hard reset is needed for full reset of the chip. We allow running

	 * without hard reset only because it can be useful for early

	 * prototyping and some debugging, but we need to warn it's not ideal.

	/*

	 * num_childbias is an array because future codecs can have different

	 * childbiases for each micbias. Unspecified values default to 0.

 no child biases */

	/*

	 * We need writable hw config info that all children can share.

	 * Simplest to take one shared copy of pdata struct.

 Not using devm_clk_get to prevent breakage of existing DTs */

	/*

	 * On some codecs DCVDD could be supplied by the internal LDO1.

	 * For those we must add the LDO1 driver before requesting DCVDD

	 * No devm_ because we need to control shutdown order of children.

 No point continuing if the type is unknown */

	/*

	 * Don't use devres here. If the regulator is one of our children it

	 * will already have been removed before devres cleanup on this mfd

	 * driver tries to call put() on it. We need control of shutdown order.

	/*

	 * Now we can power up and verify that this is a chip we know about

	 * before we start doing any writes to its registers.

	/*

	 * It looks like a device we support. If we don't have a hard reset

	 * we can now attempt a soft reset.

 Apply hardware patch */

 Init 32k clock sourced from MCLK2 */

 No devm_ because we need to control shutdown order of children */

 Prevent any IRQs being serviced while we clean up */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Motorola CPCAP PMIC core driver

 *

 * Copyright (C) 2016 Tony Lindgren <tony@atomide.com>

/*

 * First two irq chips are the two private macro interrupt chips, the third

 * irq chip is for register banks 1 - 4 and is available for drivers to use.

 Parent SPI controller uses DMA, CPCAP and child devices do not */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * TI Palmas MFD Driver

 *

 * Copyright 2011-2012 Texas Instruments Inc.

 *

 * Author: Graeme Gregory <gg@slimlogic.co.uk>

 INT1 IRQs */

 INT2 IRQs*/

 INT3 IRQs */

 INT4 IRQs */

 INT1 IRQs */

 INT2 IRQs*/

 INT3 IRQs */

 INT4 IRQs */

 Unmask the PREQ */

 The default for this register is all masked */

 Change interrupt line output polarity */

 Change IRQ into clear on read mode for efficiency */

	/*

	 * If we are probing with DT do this the DT way and return here

	 * otherwise continue and add devices using mfd helpers.

 end */ }

 init early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-spi.c  --  SPI access for Wolfson WM831x PMICs

 *

 * Copyright 2009,2010 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * dm355evm_msp.c - driver for MSP430 firmware on DM355EVM board

 *

 * Copyright (C) 2008 David Brownell

/*

 * The DM355 is a DaVinci chip with video support but no C64+ DSP.  Its

 * EVM board has an MSP430 programmed with firmware for various board

 * support functions.  This driver exposes some of them directly, and

 * supports other drivers (e.g. RTC, input) for more complex access.

 *

 * Because this firmware is entirely board-specific, this file embeds

 * knowledge that would be passed as platform_data in a generic driver.

 *

 * This driver was tested with firmware revision A4.

----------------------------------------------------------------------*/

 REVISIT for paranoia's sake, retry reads/writes on error */

/**

 * dm355evm_msp_write - Writes a register in dm355evm_msp

 * @value: the value to be written

 * @reg: register address

 *

 * Returns result of operation - 0 is success, else negative errno

/**

 * dm355evm_msp_read - Reads a register from dm355evm_msp

 * @reg: register address

 *

 * Returns result of operation - value, or negative errno

----------------------------------------------------------------------*/

/*

 * Many of the msp430 pins are just used as fixed-direction GPIOs.

 * We could export a few more of them this way, if we wanted.

 eight leds */

 SW6 and the NTSC/nPAL jumper */

 switches on MMC/SD sockets */

	/*

	 * Note: EVMDM355_ECP_VA4.pdf suggests that Bit 2 and 4 should be

	 * checked for card detection. However on the EVM bit 1 and 3 gives

	 * this status, for 0 and 1 instance respectively. The pdf also

	 * suggests that Bit 1 and 3 should be checked for write protection.

	 * However on the EVM bit 2 and 4 gives this status,for 0 and 1

	 * instance respectively.

 mmc0 WP, nCD */

 mmc1 WP, nCD */

 could also be a CE-ATA drive */

		/*

		 * These GPIOs are on the dm355evm_msp

		 * GPIO chip at index 0..7

	/* NOTE:  there are some other signals that could be

	 * packaged as output GPIOs, but they aren't as useful

	 * as the LEDs ... so for now we don't.

 dynamic assignment */

----------------------------------------------------------------------*/

 8 == right after the LEDs */

 GPIO-ish stuff */

 LED output */

		/* NOTE:  these are the only fully programmable LEDs

		 * on the board, since GPIO-61/ds22 (and many signals

		 * going to DC7) must be used for AEMIF address lines

		 * unless the top 1 GB of NAND is unused...

 configuration inputs */

 make it easy for userspace to see these */

 MMC/SD inputs -- right after the last config input */

 RTC is a 32 bit counter, no alarm */

 input from buttons and IR remote (uses the IRQ) */

----------------------------------------------------------------------*/

 display revision status; doubles as sanity check */

 mux video input:  either tvp5146 or some external imager */

 init LED cache, and turn off the LEDs */

 export capabilities we support */

 PM hookup */

 FIXME remove children ... */

 end of list */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * TI LMU (Lighting Management Unit) Core Driver

 *

 * Copyright 2017 Texas Instruments

 *

 * Author: Milo Kim <milo.kim@ti.com>

 Delay about 1ms after HW enable pin control */

 LM3631 has additional power up sequence - enable LCD_EN bit. */

 Monitoring driver for open/short circuit detection */

	/*

	 * Get device specific data from of_match table.

	 * This data is defined by using TI_LMU_DATA() macro.

 Setup regmap */

 HW enable pin control and additional power up sequence if required */

	/*

	 * Fault circuit(open/short) can be detected by ti-lmu-fault-monitor.

	 * After fault detection is done, some devices should re-initialize

	 * configuration. The notifier enables such kind of handling.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, Sony Mobile Communications AB.

 * Copyright (c) 2013, The Linux Foundation. All rights reserved.

 * Author: Bjorn Andersson <bjorn.andersson@sonymobile.com>

 Enable message RAM clock */

		/*

		 * Fall through in all other cases, as the clock is

		 * optional. (Does not exist on all platforms.)

 Accepts NULL */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-core.c  --  Device access for Wolfson WM831x PMICs

 *

 * Copyright 2009 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

/* Current settings - values are 2*2^(reg_val/4) microamps.  These are

 * exported since they are used by multiple drivers.

/**

 * wm831x_reg_lock: Unlock user keyed registers

 *

 * The WM831x has a user key preventing writes to particularly

 * critical registers.  This function locks those registers,

 * allowing writes to them.

 *

 * @wm831x: pointer to local driver data structure

/**

 * wm831x_reg_unlock: Unlock user keyed registers

 *

 * The WM831x has a user key preventing writes to particularly

 * critical registers.  This function locks those registers,

 * preventing spurious writes.

 *

 * @wm831x: pointer to local driver data structure

 0x9716 is the value required to unlock the registers */

/**

 * wm831x_reg_read: Read a single WM831x register.

 *

 * @wm831x: Device to read from.

 * @reg: Register to read.

/**

 * wm831x_bulk_read: Read multiple WM831x registers

 *

 * @wm831x: Device to read from

 * @reg: First register

 * @count: Number of registers

 * @buf: Buffer to fill.

/**

 * wm831x_reg_write: Write a single WM831x register.

 *

 * @wm831x: Device to write to.

 * @reg: Register to write to.

 * @val: Value to write.

/**

 * wm831x_set_bits: Set the value of a bitfield in a WM831x register

 *

 * @wm831x: Device to write to.

 * @reg: Register to write to.

 * @mask: Mask of bits to set.

 * @val: Value to set (unshifted)

/*

 * Instantiate the generic non-control parts of the device.

	/* Some engineering samples do not have the ID set, rely on

	 * the device being registered correctly.

	/* This will need revisiting in future but is OK for all

	 * current parts.

 Bootstrap the user key */

 Multiply by 10 as we have many subdevices of the same type */

 The core device is up, instantiate the subdevices. */

 If this happens the bus probe function is buggy */

	/* The RTC can only be used if the 32.768kHz crystal is

	 * enabled; this can't be controlled by software at runtime.

 Treat errors as non-critical */

	/* If the charger IRQs are a wake source then make sure we ack

	 * them even if they're not actively being used (eg, no power

	 * driver or no IRQ line wired up) then acknowledge the

	 * interrupts otherwise suspend won't last very long.

 If any of the interrupts are masked read the statuses */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Interrupt driver for RICOH583 power management chip.

 *

 * Copyright (c) 2011-2012, NVIDIA CORPORATION.  All rights reserved.

 * Author: Laxman dewangan <ldewangan@nvidia.com>

 *

 * based on code

 *      Copyright (C) 2011 RICOH COMPANY,LTD

 Supporting only trigger level inetrrupt */

 Clear the status */

 Merge gpio interrupts for rising and falling case*/

 Call interrupt handler if enabled */

 Initailize all int register to 0 */

 Clear all interrupts in case they woke up active. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8350-regmap.c  --  Wolfson Microelectronics WM8350 register map

 *

 * This file splits out the tables describing the defaults and access

 * status of the WM8350 registers since they are rather large.

 *

 * Copyright 2007, 2008 Wolfson Microelectronics PLC.

/*

 * Access masks.

 Mask of readable bits */

 Mask of writable bits */

 Mask of volatile bits */

  read    write volatile */

 R0   - Reset/ID */

 R1   - ID */

 R2   - ROM Mask ID */

 R3   - System Control 1 */

 R4   - System Control 2 */

 R5   - System Hibernate */

 R6   - Interface Control */

 R7 */

 R8   - Power mgmt (1) */

 R9   - Power mgmt (2) */

 R10  - Power mgmt (3) */

 R11  - Power mgmt (4) */

 R12  - Power mgmt (5) */

 R13  - Power mgmt (6) */

 R14  - Power mgmt (7) */

 R15 */

 R16  - RTC Seconds/Minutes */

 R17  - RTC Hours/Day */

 R18  - RTC Date/Month */

 R19  - RTC Year */

 R20  - Alarm Seconds/Minutes */

 R21  - Alarm Hours/Day */

 R22  - Alarm Date/Month */

 R23  - RTC Time Control */

 R24  - System Interrupts */

 R25  - Interrupt Status 1 */

 R26  - Interrupt Status 2 */

 R27  - Power Up Interrupt Status */

 R28  - Under Voltage Interrupt status */

 R29  - Over Current Interrupt status */

 R30  - GPIO Interrupt Status */

 R31  - Comparator Interrupt Status */

 R32  - System Interrupts Mask */

 R33  - Interrupt Status 1 Mask */

 R34  - Interrupt Status 2 Mask */

 R35  - Power Up Interrupt Status Mask */

 R36  - Under Voltage Int status Mask */

 R37  - Over Current Int status Mask */

 R38  - GPIO Interrupt Status Mask */

 R39  - Comparator IntStatus Mask */

 R40  - Clock Control 1 */

 R41  - Clock Control 2 */

 R42  - FLL Control 1 */

 R43  - FLL Control 2 */

 R44  - FLL Control 3 */

 R45  - FLL Control 4 */

 R46 */

 R47 */

 R48  - DAC Control */

 R49 */

 R50  - DAC Digital Volume L */

 R51  - DAC Digital Volume R */

 R52 */

 R53  - DAC LR Rate */

 R54  - DAC Clock Control */

 R55 */

 R56 */

 R57 */

 R58  - DAC Mute */

 R59  - DAC Mute Volume */

 R60  - DAC Side */

 R61 */

 R62 */

 R63 */

 R64  - ADC Control */

 R65 */

 R66  - ADC Digital Volume L */

 R67  - ADC Digital Volume R */

 R68  - ADC Divider */

 R69 */

 R70  - ADC LR Rate */

 R71 */

 R72  - Input Control */

 R73  - IN3 Input Control */

 R74  - Mic Bias Control */

 R75 */

 R76  - Output Control */

 R77  - Jack Detect */

 R78  - Anti Pop Control */

 R79 */

 R80  - Left Input Volume */

 R81  - Right Input Volume */

 R82 */

 R83 */

 R84 */

 R85 */

 R86 */

 R87 */

 R88  - Left Mixer Control */

 R89  - Right Mixer Control */

 R90 */

 R91 */

 R92  - OUT3 Mixer Control */

 R93  - OUT4 Mixer Control */

 R94 */

 R95 */

 R96  - Output Left Mixer Volume */

 R97  - Output Right Mixer Volume */

 R98  - Input Mixer Volume L */

 R99  - Input Mixer Volume R */

 R100 - Input Mixer Volume */

 R101 */

 R102 */

 R103 */

 R104 - LOUT1 Volume */

 R105 - ROUT1 Volume */

 R106 - LOUT2 Volume */

 R107 - ROUT2 Volume */

 R108 */

 R109 */

 R110 */

 R111 - BEEP Volume */

 R112 - AI Formating */

 R113 - ADC DAC COMP */

 R114 - AI ADC Control */

 R115 - AI DAC Control */

 R116 - AIF Test */

 R117 */

 R118 */

 R119 */

 R120 */

 R121 */

 R122 */

 R123 */

 R124 */

 R125 */

 R126 */

 R127 */

 R128 - GPIO Debounce */

 R129 - GPIO Pin pull up Control */

 R130 - GPIO Pull down Control */

 R131 - GPIO Interrupt Mode */

 R132 */

 R133 - GPIO Control */

 R134 - GPIO Configuration (i/o) */

 R135 - GPIO Pin Polarity / Type */

 R136 */

 R137 */

 R138 */

 R139 */

 R140 - GPIO Function Select 1 */

 R141 - GPIO Function Select 2 */

 R142 - GPIO Function Select 3 */

 R143 - GPIO Function Select 4 */

 R144 - Digitiser Control (1) */

 R145 - Digitiser Control (2) */

 R146 */

 R147 */

 R148 */

 R149 */

 R150 */

 R151 */

 R152 - AUX1 Readback */

 R153 - AUX2 Readback */

 R154 - AUX3 Readback */

 R155 - AUX4 Readback */

 R156 - USB Voltage Readback */

 R157 - LINE Voltage Readback */

 R158 - BATT Voltage Readback */

 R159 - Chip Temp Readback */

 R160 */

 R161 */

 R162 */

 R163 - Generic Comparator Control */

 R164 - Generic comparator 1 */

 R165 - Generic comparator 2 */

 R166 - Generic comparator 3 */

 R167 - Generic comparator 4 */

 R168 - Battery Charger Control 1 */

 R169 - Battery Charger Control 2 */

 R170 - Battery Charger Control 3 */

 R171 */

 R172 - Current Sink Driver A */

 R173 - CSA Flash control */

 R174 - Current Sink Driver B */

 R175 - CSB Flash control */

 R176 - DCDC/LDO requested */

 R177 - DCDC Active options */

 R178 - DCDC Sleep options */

 R179 - Power-check comparator */

 R180 - DCDC1 Control */

 R181 - DCDC1 Timeouts */

 R182 - DCDC1 Low Power */

 R183 - DCDC2 Control */

 R184 - DCDC2 Timeouts */

 R185 */

 R186 - DCDC3 Control */

 R187 - DCDC3 Timeouts */

 R188 - DCDC3 Low Power */

 R189 - DCDC4 Control */

 R190 - DCDC4 Timeouts */

 R191 - DCDC4 Low Power */

 R192 - DCDC5 Control */

 R193 - DCDC5 Timeouts */

 R194 */

 R195 - DCDC6 Control */

 R196 - DCDC6 Timeouts */

 R197 - DCDC6 Low Power */

 R198 */

 R199 - Limit Switch Control */

 R200 - LDO1 Control */

 R201 - LDO1 Timeouts */

 R202 - LDO1 Low Power */

 R203 - LDO2 Control */

 R204 - LDO2 Timeouts */

 R205 - LDO2 Low Power */

 R206 - LDO3 Control */

 R207 - LDO3 Timeouts */

 R208 - LDO3 Low Power */

 R209 - LDO4 Control */

 R210 - LDO4 Timeouts */

 R211 - LDO4 Low Power */

 R212 */

 R213 */

 R214 */

 R215 - VCC_FAULT Masks */

 R216 - Main Bandgap Control */

 R217 - OSC Control */

 R218 - RTC Tick Control */

 R219 - Security */

 R220 - RAM BIST 1 */

 R221 */

 R222 */

 R223 */

 R224 */

 R225 - DCDC/LDO status */

 R226 - Charger status */

 R227 */

 R228 */

 R229 */

 R230 - GPIO Pin Status */

 R231 */

 R232 */

 R233 */

 R234 */

 R235 */

 R236 */

 R237 */

 R238 */

 R239 */

 R240 */

 R241 */

 R242 */

 R243 */

 R244 */

 R245 */

 R246 */

 R247 */

 R248 */

 R249 */

 R250 */

 R251 */

 R252 */

 R253 */

 R254 */

 R255 */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2009-2013, The Linux Foundation. All rights reserved.

 * Copyright (c) 2010, Google Inc.

 *

 * Original authors: Code Aurora Forum

 *

 * Author: Dima Zavin <dima@android.com>

 *  - Largely rewritten from original to not be an i2c driver.

 SSBI 2.0 controller registers */

 SSBI_CMD fields */

 SSBI_STATUS fields */

 SSBI_MODE2 fields */

 SSBI PMIC Arbiter command registers */

 SSBI_PA_CMD fields */

 REG_ADDR_7_0, REG_ADDR_8_14*/

 SSBI_PA_RD_STATUS fields */

/*

 * Via private exchange with one of the original authors, the hardware

 * should generally finish a transaction in about 5us.  The worst

 * case, is when using the arbiter and both other CPUs have just

 * started trying to use the SSBI bus will result in a time of about

 * 20us.  It should never take longer than this.

 *

 * As such, this wait merely spins, with a udelay.

/*

 * See ssbi_wait_mask for an explanation of the time and the

 * busywait.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2009-2010 Pengutronix

 * Uwe Kleine-Koenig <u.kleine-koenig@pengutronix.de>

 *

 * loosely based on an earlier driver that has

 * Copyright 2009 Pengutronix, Sascha Hauer <s.hauer@pengutronix.de>

 These are only exported for mc13xxx-i2c and mc13xxx-spi */

	/*

	 * Channels mapped through ADIN7:

	 * 7  - General purpose ADIN7

	 * 16 - UID

	 * 17 - Die temperature

 restore TSMOD */

 there is no asnprintf in the kernel :-( */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) ST-Ericsson SA 2010

 *

 * Author: Srinidhi Kasagar <srinidhi.kasagar@stericsson.com>

 * Author: Rabin Vincent <rabin.vincent@stericsson.com>

 * Author: Mattias Wallin <mattias.wallin@stericsson.com>

/*

 * Interrupt register offsets

 * Bank : 0x0E

/*

 * latch registers

/*

 * mask registers

/*

 * latch hierarchy registers

/*

 * Map interrupt numbers to the LATCH and MASK register offsets, Interrupt

 * numbers are indexed into this array with (num / 8). The interupts are

 * defined in linux/mfd/ab8500.h

 *

 * This is one off from the register names, i.e. AB8500_IT_MASK1_REG is at

 * offset 0.

 AB8500 support */

 AB9540 / AB8505 support */

 AB8540 support */

	/*

	 * Put the u8 bank and u8 register together into a an u16.

	 * The bank on higher 8 bits and register in lower 8 bits.

		/*

		 * Interrupt register 12 doesn't exist prior to AB8500 version

		 * 2.0

 The AB8500 GPIOs have two interrupts each (rising & falling). */

 Here the falling IRQ is one bit lower */

 The AB8500 GPIOs have two interrupts each (rising & falling). */

 Here the falling IRQ is one bit lower */

 Satisfies the case where type is not set. */

 Fix inconsistent ITFromLatch25 bit mapping... */

 Fix inconsistent ab8540 bit mapping... */

 ignore masked out interrupts */

		/*

		 * This handles the falling edge hwirqs from the GPIO

		 * lines. Route them back to the line registered for the

		 * rising IRQ, as this is merely a flag for the same IRQ

		 * in linux terms.

  Hierarchical interrupt version */

 If ->irq_base is zero this will give a linear mapping */

 Device list for ab8505  */

/*

 * ab8500 has switched off due to (SWITCH_OFF_STATUS):

 * 0x01 Swoff bit programming

 * 0x02 Thermal protection activation

 * 0x04 Vbat lower then BattOk falling threshold

 * 0x08 Watchdog expired

 * 0x10 Non presence of 32kHz clock

 * 0x20 Battery level lower than power on reset threshold

 * 0x40 Power on key 1 pressed longer than 10 seconds

 * 0x80 DB8500 thermal shutdown

 use mask and set to override the register turn_on_stat value */

/*

 * ab8500 has turned on due to (TURN_ON_STATUS):

 * 0x01 PORnVbat

 * 0x02 PonKey1dbF

 * 0x04 PonKey2dbF

 * 0x08 RTCAlarm

 * 0x10 MainChDet

 * 0x20 VbusDet

 * 0x40 UsbIDDetect

 * 0x80 Reserved

	/*

	 * In L9540, turn_on_status register is not updated correctly if

	 * the device is rebooted with AC/USB charger connected. Due to

	 * this, the device boots android instead of entering into charge

	 * only mode. Read the AC/USB status register to detect the charger

	 * presence and update the turn on status manually.

 Configure AB8540 */

 Configure AB8500 or AB9540 IRQ */

	/*

	 * ab8500 has switched off due to (SWITCH_OFF_STATUS):

	 * 0x01 Swoff bit programming

	 * 0x02 Thermal protection activation

	 * 0x04 Vbat lower then BattOk falling threshold

	 * 0x08 Watchdog expired

	 * 0x10 Non presence of 32kHz clock

	 * 0x20 Battery level lower than power on reset threshold

	 * 0x40 Power on key 1 pressed longer than 10 seconds

	 * 0x80 DB8500 thermal shutdown

 Clear and mask all interrupts */

		/*

		 * Interrupt register 12 doesn't exist prior to AB8500 version

		 * 2.0

 ab8540 >= cut2 */

 Add battery management devices */

 SPDX-License-Identifier: GPL-2.0+



 Copyright (c) 2012 Samsung Electronics Co., Ltd

              http:
 Sentinel */

 For each device type, the REG_ID is always the first register */

		/*

		 * If WRSTBI pin is pulled down this feature must be disabled

		 * because each Suspend to RAM will trigger buck voltage reset

		 * to default values.

/*

 * Only the common platform data elements for s5m8767 are parsed here from the

 * device tree. Other sub-modules of s5m8767 such as pmic, rtc , charger and

 * others have to parse their own platform data elements from device tree.

 *

 * The s5m8767 platform data structure is instantiated here and the drivers for

 * the sub-modules need not instantiate another instance while parsing their

 * platform data.

		/*

		 * Currently only one board with S2MPS11 needs this, so just

		 * ignore the rest.

	/*

	 * PMIC IRQ must be disabled during suspend for RTC alarm

	 * to work properly.

	 * When device is woken up from suspend, an

	 * interrupt occurs before resuming I2C bus controller.

	 * The interrupt is handled by regmap_irq_thread which tries

	 * to read RTC registers. This read fails (I2C is still

	 * suspended) and RTC Alarm interrupt is disabled.

 CONFIG_PM_SLEEP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Regmap tables for CS47L92 codec

 *

 * Copyright (C) 2016-2019 Cirrus Logic, Inc. and

 *                         Cirrus Logic International Semiconductor Ltd.

 *

 * Author: Stuart Henderson <stuarth@opensource.cirrus.com>

 R32 (0x20) - Tone Generator 1 */

 R33 (0x21) - Tone Generator 2 */

 R34 (0x22) - Tone Generator 3 */

 R35 (0x23) - Tone Generator 4 */

 R36 (0x24) - Tone Generator 5 */

 R48 (0x30) - PWM Drive 1 */

 R49 (0x31) - PWM Drive 2 */

 R50 (0x32) - PWM Drive 3 */

 R97 (0x61) - Sample Rate Sequence Select 1 */

 R98 (0x62) - Sample Rate Sequence Select 2 */

 R99 (0x63) - Sample Rate Sequence Select 3 */

 R100 (0x64) - Sample Rate Sequence Select 4 */

 R144 (0x90) - Haptics Control 1 */

 R145 (0x91) - Haptics Control 2 */

 R146 (0x92) - Haptics Phase 1 Intensity */

 R147 (0x93) - Haptics Phase 1 Duration */

 R148 (0x94) - Haptics Phase 2 Intensity */

 R149 (0x95) - Haptics Phase 2 Duration */

 R150 (0x96) - Haptics Phase 3 Intensity */

 R151 (0x97) - Haptics Phase 3 Duration */

 R160 (0xa0) - Comfort Noise Generator */

 R256 (0x100) - Clock 32k 1 */

 R257 (0x101) - System Clock 1 */

 R258 (0x102) - Sample Rate 1 */

 R259 (0x103) - Sample Rate 2 */

 R260 (0x104) - Sample Rate 3 */

 R274 (0x112) - Async Clock 1 */

 R275 (0x113) - Async Sample Rate 1 */

 R276 (0x114) - Async Sample Rate 2 */

 R288 (0x120) - DSP Clock 1 */

 R290 (0x122) - DSP Clock 2 */

 R329 (0x149) - Output System Clock */

 R330 (0x14a) - Output Async Clock */

 R338 (0x152) - Rate Estimator 1 */

 R339 (0x153) - Rate Estimator 2 */

 R340 (0x154) - Rate Estimator 3 */

 R341 (0x155) - Rate Estimator 4 */

 R342 (0x156) - Rate Estimator 5 */

 R369 (0x171) - FLL1 Control 1 */

 R370 (0x172) - FLL1 Control 2 */

 R371 (0x173) - FLL1 Control 3 */

 R372 (0x174) - FLL1 Control 4 */

 R373 (0x175) - FLL1 Control 5 */

 R374 (0x176) - FLL1 Control 6 */

 R375 (0x177) - FLL1 Control 7 */

 R376 (0x178) - FLL1 Control 8 */

 R377 (0x179) - FLL1 Control 9 */

 R378 (0x17a) - FLL1 Control 10 */

 R379 (0x17b) - FLL1 Control 11 */

 R381 (0x17d) - FLL1 Digital Test 1 */

 R385 (0x181) - FLL1 Synchroniser 1 */

 R386 (0x182) - FLL1 Synchroniser 2 */

 R387 (0x183) - FLL1 Synchroniser 3 */

 R388 (0x184) - FLL1 Synchroniser 4 */

 R389 (0x185) - FLL1 Synchroniser 5 */

 R390 (0x186) - FLL1 Synchroniser 6 */

 R398 (0x18e) - FLL1 GPIO Clock */

 R401 (0x191) - FLL2 Control 1 */

 R402 (0x192) - FLL2 Control 2 */

 R403 (0x193) - FLL2 Control 3 */

 R404 (0x194) - FLL2 Control 4 */

 R405 (0x195) - FLL2 Control 5 */

 R406 (0x196) - FLL2 Control 6 */

 R407 (0x197) - FLL2 Control 7 */

 R408 (0x198) - FLL2 Control 8 */

 R409 (0x199) - FLL2 Control 9 */

 R410 (0x19a) - FLL2 Control 10 */

 R411 (0x19b) - FLL2 Control 11 */

 R413 (0x19d) - FLL2 Digital Test 1 */

 R417 (0x1a1) - FLL2 Synchroniser 1 */

 R418 (0x1a2) - FLL2 Synchroniser 2 */

 R419 (0x1a3) - FLL2 Synchroniser 3 */

 R420 (0x1a4) - FLL2 Synchroniser 4 */

 R421 (0x1a5) - FLL2 Synchroniser 5 */

 R422 (0x1a6) - FLL2 Synchroniser 6 */

 R430 (0x1ae) - FLL2 GPIO Clock */

 R512 (0x200) - Mic Charge Pump 1 */

 R531 (0x213) - LDO2 Control 1 */

 R536 (0x218) - Mic Bias Ctrl 1 */

 R537 (0x219) - Mic Bias Ctrl 2 */

 R540 (0x21c) - Mic Bias Ctrl 5 */

 R542 (0x21e) - Mic Bias Ctrl 6 */

 R659 (0x293) - Accessory Detect Mode 1 */

 R665 (0x299) - Headphone Detect 0 */

 R667 (0x29b) - Headphone Detect 1 */

 R674 (0x2a2) - Mic Detect 1 Control 0 */

 R675 (0x2a3) - Mic Detect 1 Control 1 */

 R676 (0x2a4) - Mic Detect 1 Control 2 */

 R678 (0x2a6) - Mic Detect 1 Level 1 */

 R679 (0x2a7) - Mic Detect 1 Level 2 */

 R680 (0x2a8) - Mic Detect 1 Level 3 */

 R681 (0x2a9) - Mic Detect 1 Level 4 */

 R690 (0x2b2) - Mic Detect 2 Control 0 */

 R691 (0x2b3) - Mic Detect 2 Control 1 */

 R692 (0x2b4) - Mic Detect 2 Control 2 */

 R694 (0x2b6) - Mic Detect 2 Level 1 */

 R695 (0x2b7) - Mic Detect 2 Level 2 */

 R696 (0x2b8) - Mic Detect 2 Level 3 */

 R697 (0x2b9) - Mic Detect 2 Level 4 */

 R710 (0x2c6) - Micd Clamp control */

 R712 (0x2c8) - GP Switch 1 */

 R723 (0x2d3) - Jack Detect Analogue */

 R768 (0x300) - Input Enables */

 R776 (0x308) - Input Rate */

 R777 (0x309) - Input Volume Ramp */

 R780 (0x30c) - HPF Control */

 R784 (0x310) - IN1L Control */

 R785 (0x311) - ADC Digital Volume 1L */

 R786 (0x312) - DMIC1L Control */

 R787 (0x313) - IN1L Rate Control */

 R788 (0x314) - IN1R Control */

 R789 (0x315) - ADC Digital Volume 1R */

 R790 (0x316) - DMIC1R Control */

 R791 (0x317) - IN1R Rate Control */

 R792 (0x318) - IN2L Control */

 R793 (0x319) - ADC Digital Volume 2L */

 R794 (0x31a) - DMIC2L Control */

 R795 (0x31b) - IN2L Rate Control */

 R796 (0x31c) - IN2R Control */

 R797 (0x31d) - ADC Digital Volume 2R */

 R798 (0x31e) - DMIC2R Control */

 R799 (0x31f) - IN2R Rate Control */

 R800 (0x320) - IN3L Control */

 R801 (0x321) - ADC Digital Volume 3L */

 R802 (0x322) - DMIC3L Control */

 R803 (0x323) - IN3L Rate Control */

 R804 (0x324) - IN3R Control */

 R805 (0x325) - ADC Digital Volume 3R */

 R806 (0x326) - DMIC3R Control */

 R807 (0x327) - IN3R Rate Control */

 R808 (0x328) - IN4L Control */

 R809 (0x329) - ADC Digital Volume 4L */

 R810 (0x32a) - DMIC4L Control */

 R811 (0x32b) - IN4L Rate Control */

 R812 (0x32c) - IN4R Control */

 R813 (0x32d) - ADC Digital Volume 4R */

 R814 (0x32e) - DMIC4R Control */

 R815 (0x32f) - IN4R Rate Control */

 R1024 (0x400) - Output Enables 1 */

 R1032 (0x408) - Output Rate 1 */

 R1033 (0x409) - Output Volume Ramp */

 R1040 (0x410) - Output Path Config 1L */

 R1041 (0x411) - DAC Digital Volume 1L */

 R1042 (0x412) - Output Path Config 1 */

 R1043 (0x413) - Noise Gate Select 1L */

 R1044 (0x414) - Output Path Config 1R */

 R1045 (0x415) - DAC Digital Volume 1R */

 R1047 (0x417) - Noise Gate Select 1R */

 R1048 (0x418) - Output Path Config 2L */

 R1049 (0x419) - DAC Digital Volume 2L */

 R1050 (0x41a) - Output Path Config 2 */

 R1051 (0x41b) - Noise Gate Select 2L */

 R1052 (0x41c) - Output Path Config 2R */

 R1053 (0x41d) - DAC Digital Volume 2R */

 R1055 (0x41f) - Noise Gate Select 2R */

 R1056 (0x420) - Output Path Config 3L */

 R1057 (0x421) - DAC Digital Volume 3L */

 R1058 (0x422) - Output Path Config 3 */

 R1059 (0x423) - Noise Gate Select 3L */

 R1060 (0x424) - Output Path Config 3R */

 R1061 (0x425) - DAC Digital Volume 3R */

 R1063 (0x427) - Noise Gate Select 3R */

 R1072 (0x430) - Output Path Config 5L */

 R1073 (0x431) - DAC Digital Volume 5L */

 R1075 (0x433) - Noise Gate Select 5L */

 R1076 (0x434) - Output Path Config 5R */

 R1077 (0x435) - DAC Digital Volume 5R */

 R1079 (0x437) - Noise Gate Select 5R */

 R1104 (0x450) - DAC AEC Control 1 */

 R1105 (0x451) - DAC AEC Control 2 */

 R1112 (0x458) - Noise Gate Control */

 R1168 (0x490) - PDM SPK1 Ctrl 1 */

 R1169 (0x491) - PDM SPK1 Ctrl 2 */

 R1184 (0x4a0) - HP1 Short Circuit Ctrl */

 R1185 (0x4a1) - HP2 Short Circuit Ctrl */

 R1186 (0x4a2) - HP3 Short Circuit Ctrl */

 R1280 (0x500) - AIF1 BCLK Ctrl */

 R1281 (0x501) - AIF1 Tx Pin Ctrl */

 R1282 (0x502) - AIF1 Rx Pin Ctrl */

 R1283 (0x503) - AIF1 Rate Ctrl */

 R1284 (0x504) - AIF1 Format */

 R1286 (0x506) - AIF1 Rx BCLK Rate */

 R1287 (0x507) - AIF1 Frame Ctrl 1 */

 R1288 (0x508) - AIF1 Frame Ctrl 2 */

 R1289 (0x509) - AIF1 Frame Ctrl 3 */

 R1290 (0x50a) - AIF1 Frame Ctrl 4 */

 R1291 (0x50b) - AIF1 Frame Ctrl 5 */

 R1292 (0x50c) - AIF1 Frame Ctrl 6 */

 R1293 (0x50d) - AIF1 Frame Ctrl 7 */

 R1294 (0x50e) - AIF1 Frame Ctrl 8 */

 R1295 (0x50f) - AIF1 Frame Ctrl 9 */

 R1296 (0x510) - AIF1 Frame Ctrl 10 */

 R1297 (0x511) - AIF1 Frame Ctrl 11 */

 R1298 (0x512) - AIF1 Frame Ctrl 12 */

 R1299 (0x513) - AIF1 Frame Ctrl 13 */

 R1300 (0x514) - AIF1 Frame Ctrl 14 */

 R1301 (0x515) - AIF1 Frame Ctrl 15 */

 R1302 (0x516) - AIF1 Frame Ctrl 16 */

 R1303 (0x517) - AIF1 Frame Ctrl 17 */

 R1304 (0x518) - AIF1 Frame Ctrl 18 */

 R1305 (0x519) - AIF1 Tx Enables */

 R1306 (0x51a) - AIF1 Rx Enables */

 R1344 (0x540) - AIF2 BCLK Ctrl */

 R1345 (0x541) - AIF2 Tx Pin Ctrl */

 R1346 (0x542) - AIF2 Rx Pin Ctrl */

 R1347 (0x543) - AIF2 Rate Ctrl */

 R1348 (0x544) - AIF2 Format */

 R1350 (0x546) - AIF2 Rx BCLK Rate */

 R1351 (0x547) - AIF2 Frame Ctrl 1 */

 R1352 (0x548) - AIF2 Frame Ctrl 2 */

 R1353 (0x549) - AIF2 Frame Ctrl 3 */

 R1354 (0x54a) - AIF2 Frame Ctrl 4 */

 R1355 (0x54b) - AIF2 Frame Ctrl 5 */

 R1356 (0x54c) - AIF2 Frame Ctrl 6 */

 R1357 (0x54d) - AIF2 Frame Ctrl 7 */

 R1358 (0x54e) - AIF2 Frame Ctrl 8 */

 R1359 (0x54f) - AIF2 Frame Ctrl 9 */

 R1360 (0x550) - AIF2 Frame Ctrl 10 */

 R1361 (0x551) - AIF2 Frame Ctrl 11 */

 R1362 (0x552) - AIF2 Frame Ctrl 12 */

 R1363 (0x553) - AIF2 Frame Ctrl 13 */

 R1364 (0x554) - AIF2 Frame Ctrl 14 */

 R1365 (0x555) - AIF2 Frame Ctrl 15 */

 R1366 (0x556) - AIF2 Frame Ctrl 16 */

 R1367 (0x557) - AIF2 Frame Ctrl 17 */

 R1368 (0x558) - AIF2 Frame Ctrl 18 */

 R1369 (0x559) - AIF2 Tx Enables */

 R1370 (0x55a) - AIF2 Rx Enables */

 R1408 (0x580) - AIF3 BCLK Ctrl */

 R1409 (0x581) - AIF3 Tx Pin Ctrl */

 R1410 (0x582) - AIF3 Rx Pin Ctrl */

 R1411 (0x583) - AIF3 Rate Ctrl */

 R1412 (0x584) - AIF3 Format */

 R1414 (0x586) - AIF3 Rx BCLK Rate */

 R1415 (0x587) - AIF3 Frame Ctrl 1 */

 R1416 (0x588) - AIF3 Frame Ctrl 2 */

 R1417 (0x589) - AIF3 Frame Ctrl 3 */

 R1418 (0x58a) - AIF3 Frame Ctrl 4 */

 R1419 (0x58b) - AIF3 Frame Ctrl 5 */

 R1420 (0x58c) - AIF3 Frame Ctrl 6 */

 R1421 (0x58d) - AIF3 Frame Ctrl 7 */

 R1422 (0x58e) - AIF3 Frame Ctrl 8 */

 R1423 (0x58f) - AIF3 Frame Ctrl 9 */

 R1424 (0x590) - AIF3 Frame Ctrl 10 */

 R1425 (0x591) - AIF3 Frame Ctrl 11 */

 R1426 (0x592) - AIF3 Frame Ctrl 12 */

 R1427 (0x593) - AIF3 Frame Ctrl 13 */

 R1428 (0x594) - AIF3 Frame Ctrl 14 */

 R1429 (0x595) - AIF3 Frame Ctrl 15 */

 R1430 (0x596) - AIF3 Frame Ctrl 16 */

 R1431 (0x597) - AIF3 Frame Ctrl 17 */

 R1432 (0x598) - AIF3 Frame Ctrl 18 */

 R1433 (0x599) - AIF3 Tx Enables */

 R1434 (0x59a) - AIF3 Rx Enables */

 R1474 (0x5c2) - SPD1 Tx Control */

 R1507 (0x5e3) - SLIMBus Framer Ref Gear */

 R1509 (0x5e5) - SLIMBus Rates 1 */

 R1510 (0x5e6) - SLIMBus Rates 2 */

 R1511 (0x5e7) - SLIMBus Rates 3 */

 R1512 (0x5e8) - SLIMBus Rates 4 */

 R1513 (0x5e9) - SLIMBus Rates 5 */

 R1514 (0x5ea) - SLIMBus Rates 6 */

 R1515 (0x5eb) - SLIMBus Rates 7 */

 R1516 (0x5ec) - SLIMBus Rates 8 */

 R1525 (0x5f5) - SLIMBus RX Channel Enable */

 R1526 (0x5f6) - SLIMBus TX Channel Enable */

 R1600 (0x640) - PWM1MIX Input 1 Source */

 R1601 (0x641) - PWM1MIX Input 1 Volume */

 R1602 (0x642) - PWM1MIX Input 2 Source */

 R1603 (0x643) - PWM1MIX Input 2 Volume */

 R1604 (0x644) - PWM1MIX Input 3 Source */

 R1605 (0x645) - PWM1MIX Input 3 Volume */

 R1606 (0x646) - PWM1MIX Input 4 Source */

 R1607 (0x647) - PWM1MIX Input 4 Volume */

 R1608 (0x648) - PWM2MIX Input 1 Source */

 R1609 (0x649) - PWM2MIX Input 1 Volume */

 R1610 (0x64a) - PWM2MIX Input 2 Source */

 R1611 (0x64b) - PWM2MIX Input 2 Volume */

 R1612 (0x64c) - PWM2MIX Input 3 Source */

 R1613 (0x64d) - PWM2MIX Input 3 Volume */

 R1614 (0x64e) - PWM2MIX Input 4 Source */

 R1615 (0x64f) - PWM2MIX Input 4 Volume */

 R1664 (0x680) - OUT1LMIX Input 1 Source */

 R1665 (0x681) - OUT1LMIX Input 1 Volume */

 R1666 (0x682) - OUT1LMIX Input 2 Source */

 R1667 (0x683) - OUT1LMIX Input 2 Volume */

 R1668 (0x684) - OUT1LMIX Input 3 Source */

 R1669 (0x685) - OUT1LMIX Input 3 Volume */

 R1670 (0x686) - OUT1LMIX Input 4 Source */

 R1671 (0x687) - OUT1LMIX Input 4 Volume */

 R1672 (0x688) - OUT1RMIX Input 1 Source */

 R1673 (0x689) - OUT1RMIX Input 1 Volume */

 R1674 (0x68a) - OUT1RMIX Input 2 Source */

 R1675 (0x68b) - OUT1RMIX Input 2 Volume */

 R1676 (0x68c) - OUT1RMIX Input 3 Source */

 R1677 (0x68d) - OUT1RMIX Input 3 Volume */

 R1678 (0x68e) - OUT1RMIX Input 4 Source */

 R1679 (0x68f) - OUT1RMIX Input 4 Volume */

 R1680 (0x690) - OUT2LMIX Input 1 Source */

 R1681 (0x691) - OUT2LMIX Input 1 Volume */

 R1682 (0x692) - OUT2LMIX Input 2 Source */

 R1683 (0x693) - OUT2LMIX Input 2 Volume */

 R1684 (0x694) - OUT2LMIX Input 3 Source */

 R1685 (0x695) - OUT2LMIX Input 3 Volume */

 R1686 (0x696) - OUT2LMIX Input 4 Source */

 R1687 (0x697) - OUT2LMIX Input 4 Volume */

 R1688 (0x698) - OUT2RMIX Input 1 Source */

 R1689 (0x699) - OUT2RMIX Input 1 Volume */

 R1690 (0x69a) - OUT2RMIX Input 2 Source */

 R1691 (0x69b) - OUT2RMIX Input 2 Volume */

 R1692 (0x69c) - OUT2RMIX Input 3 Source */

 R1693 (0x69d) - OUT2RMIX Input 3 Volume */

 R1694 (0x69e) - OUT2RMIX Input 4 Source */

 R1695 (0x69f) - OUT2RMIX Input 4 Volume */

 R1696 (0x6a0) - OUT3LMIX Input 1 Source */

 R1697 (0x6a1) - OUT3LMIX Input 1 Volume */

 R1698 (0x6a2) - OUT3LMIX Input 2 Source */

 R1699 (0x6a3) - OUT3LMIX Input 2 Volume */

 R1700 (0x6a4) - OUT3LMIX Input 3 Source */

 R1701 (0x6a5) - OUT3LMIX Input 3 Volume */

 R1702 (0x6a6) - OUT3LMIX Input 4 Source */

 R1703 (0x6a7) - OUT3LMIX Input 4 Volume */

 R1704 (0x6a8) - OUT3RMIX Input 1 Source */

 R1705 (0x6a9) - OUT3RMIX Input 1 Volume */

 R1706 (0x6aa) - OUT3RMIX Input 2 Source */

 R1707 (0x6ab) - OUT3RMIX Input 2 Volume */

 R1708 (0x6ac) - OUT3RMIX Input 3 Source */

 R1709 (0x6ad) - OUT3RMIX Input 3 Volume */

 R1710 (0x6ae) - OUT3RMIX Input 4 Source */

 R1711 (0x6af) - OUT3RMIX Input 4 Volume */

 R1728 (0x6c0) - OUT5LMIX Input 1 Source */

 R1729 (0x6c1) - OUT5LMIX Input 1 Volume */

 R1730 (0x6c2) - OUT5LMIX Input 2 Source */

 R1731 (0x6c3) - OUT5LMIX Input 2 Volume */

 R1732 (0x6c4) - OUT5LMIX Input 3 Source */

 R1733 (0x6c5) - OUT5LMIX Input 3 Volume */

 R1734 (0x6c6) - OUT5LMIX Input 4 Source */

 R1735 (0x6c7) - OUT5LMIX Input 4 Volume */

 R1736 (0x6c8) - OUT5RMIX Input 1 Source */

 R1737 (0x6c9) - OUT5RMIX Input 1 Volume */

 R1738 (0x6ca) - OUT5RMIX Input 2 Source */

 R1739 (0x6cb) - OUT5RMIX Input 2 Volume */

 R1740 (0x6cc) - OUT5RMIX Input 3 Source */

 R1741 (0x6cd) - OUT5RMIX Input 3 Volume */

 R1742 (0x6ce) - OUT5RMIX Input 4 Source */

 R1743 (0x6cf) - OUT5RMIX Input 4 Volume */

 R1792 (0x700) - AIF1TX1MIX Input 1 Source */

 R1793 (0x701) - AIF1TX1MIX Input 1 Volume */

 R1794 (0x702) - AIF1TX1MIX Input 2 Source */

 R1795 (0x703) - AIF1TX1MIX Input 2 Volume */

 R1796 (0x704) - AIF1TX1MIX Input 3 Source */

 R1797 (0x705) - AIF1TX1MIX Input 3 Volume */

 R1798 (0x706) - AIF1TX1MIX Input 4 Source */

 R1799 (0x707) - AIF1TX1MIX Input 4 Volume */

 R1800 (0x708) - AIF1TX2MIX Input 1 Source */

 R1801 (0x709) - AIF1TX2MIX Input 1 Volume */

 R1802 (0x70a) - AIF1TX2MIX Input 2 Source */

 R1803 (0x70b) - AIF1TX2MIX Input 2 Volume */

 R1804 (0x70c) - AIF1TX2MIX Input 3 Source */

 R1805 (0x70d) - AIF1TX2MIX Input 3 Volume */

 R1806 (0x70e) - AIF1TX2MIX Input 4 Source */

 R1807 (0x70f) - AIF1TX2MIX Input 4 Volume */

 R1808 (0x710) - AIF1TX3MIX Input 1 Source */

 R1809 (0x711) - AIF1TX3MIX Input 1 Volume */

 R1810 (0x712) - AIF1TX3MIX Input 2 Source */

 R1811 (0x713) - AIF1TX3MIX Input 2 Volume */

 R1812 (0x714) - AIF1TX3MIX Input 3 Source */

 R1813 (0x715) - AIF1TX3MIX Input 3 Volume */

 R1814 (0x716) - AIF1TX3MIX Input 4 Source */

 R1815 (0x717) - AIF1TX3MIX Input 4 Volume */

 R1816 (0x718) - AIF1TX4MIX Input 1 Source */

 R1817 (0x719) - AIF1TX4MIX Input 1 Volume */

 R1818 (0x71a) - AIF1TX4MIX Input 2 Source */

 R1819 (0x71b) - AIF1TX4MIX Input 2 Volume */

 R1820 (0x71c) - AIF1TX4MIX Input 3 Source */

 R1821 (0x71d) - AIF1TX4MIX Input 3 Volume */

 R1822 (0x71e) - AIF1TX4MIX Input 4 Source */

 R1823 (0x71f) - AIF1TX4MIX Input 4 Volume */

 R1824 (0x720) - AIF1TX5MIX Input 1 Source */

 R1825 (0x721) - AIF1TX5MIX Input 1 Volume */

 R1826 (0x722) - AIF1TX5MIX Input 2 Source */

 R1827 (0x723) - AIF1TX5MIX Input 2 Volume */

 R1828 (0x724) - AIF1TX5MIX Input 3 Source */

 R1829 (0x725) - AIF1TX5MIX Input 3 Volume */

 R1830 (0x726) - AIF1TX5MIX Input 4 Source */

 R1831 (0x727) - AIF1TX5MIX Input 4 Volume */

 R1832 (0x728) - AIF1TX6MIX Input 1 Source */

 R1833 (0x729) - AIF1TX6MIX Input 1 Volume */

 R1834 (0x72a) - AIF1TX6MIX Input 2 Source */

 R1835 (0x72b) - AIF1TX6MIX Input 2 Volume */

 R1836 (0x72c) - AIF1TX6MIX Input 3 Source */

 R1837 (0x72d) - AIF1TX6MIX Input 3 Volume */

 R1838 (0x72e) - AIF1TX6MIX Input 4 Source */

 R1839 (0x72f) - AIF1TX6MIX Input 4 Volume */

 R1840 (0x730) - AIF1TX7MIX Input 1 Source */

 R1841 (0x731) - AIF1TX7MIX Input 1 Volume */

 R1842 (0x732) - AIF1TX7MIX Input 2 Source */

 R1843 (0x733) - AIF1TX7MIX Input 2 Volume */

 R1844 (0x734) - AIF1TX7MIX Input 3 Source */

 R1845 (0x735) - AIF1TX7MIX Input 3 Volume */

 R1846 (0x736) - AIF1TX7MIX Input 4 Source */

 R1847 (0x737) - AIF1TX7MIX Input 4 Volume */

 R1848 (0x738) - AIF1TX8MIX Input 1 Source */

 R1849 (0x739) - AIF1TX8MIX Input 1 Volume */

 R1850 (0x73a) - AIF1TX8MIX Input 2 Source */

 R1851 (0x73b) - AIF1TX8MIX Input 2 Volume */

 R1852 (0x73c) - AIF1TX8MIX Input 3 Source */

 R1853 (0x73d) - AIF1TX8MIX Input 3 Volume */

 R1854 (0x73e) - AIF1TX8MIX Input 4 Source */

 R1855 (0x73f) - AIF1TX8MIX Input 4 Volume */

 R1856 (0x740) - AIF2TX1MIX Input 1 Source */

 R1857 (0x741) - AIF2TX1MIX Input 1 Volume */

 R1858 (0x742) - AIF2TX1MIX Input 2 Source */

 R1859 (0x743) - AIF2TX1MIX Input 2 Volume */

 R1860 (0x744) - AIF2TX1MIX Input 3 Source */

 R1861 (0x745) - AIF2TX1MIX Input 3 Volume */

 R1862 (0x746) - AIF2TX1MIX Input 4 Source */

 R1863 (0x747) - AIF2TX1MIX Input 4 Volume */

 R1864 (0x748) - AIF2TX2MIX Input 1 Source */

 R1865 (0x749) - AIF2TX2MIX Input 1 Volume */

 R1866 (0x74a) - AIF2TX2MIX Input 2 Source */

 R1867 (0x74b) - AIF2TX2MIX Input 2 Volume */

 R1868 (0x74c) - AIF2TX2MIX Input 3 Source */

 R1869 (0x74d) - AIF2TX2MIX Input 3 Volume */

 R1870 (0x74e) - AIF2TX2MIX Input 4 Source */

 R1871 (0x74f) - AIF2TX2MIX Input 4 Volume */

 R1872 (0x750) - AIF2TX3MIX Input 1 Source */

 R1873 (0x751) - AIF2TX3MIX Input 1 Volume */

 R1874 (0x752) - AIF2TX3MIX Input 2 Source */

 R1875 (0x753) - AIF2TX3MIX Input 2 Volume */

 R1876 (0x754) - AIF2TX3MIX Input 3 Source */

 R1877 (0x755) - AIF2TX3MIX Input 3 Volume */

 R1878 (0x756) - AIF2TX3MIX Input 4 Source */

 R1879 (0x757) - AIF2TX3MIX Input 4 Volume */

 R1880 (0x758) - AIF2TX4MIX Input 1 Source */

 R1881 (0x759) - AIF2TX4MIX Input 1 Volume */

 R1882 (0x75a) - AIF2TX4MIX Input 2 Source */

 R1883 (0x75b) - AIF2TX4MIX Input 2 Volume */

 R1884 (0x75c) - AIF2TX4MIX Input 3 Source */

 R1885 (0x75d) - AIF2TX4MIX Input 3 Volume */

 R1886 (0x75e) - AIF2TX4MIX Input 4 Source */

 R1887 (0x75f) - AIF2TX4MIX Input 4 Volume */

 R1888 (0x760) - AIF2TX5MIX Input 1 Source */

 R1889 (0x761) - AIF2TX5MIX Input 1 Volume */

 R1890 (0x762) - AIF2TX5MIX Input 2 Source */

 R1891 (0x763) - AIF2TX5MIX Input 2 Volume */

 R1892 (0x764) - AIF2TX5MIX Input 3 Source */

 R1893 (0x765) - AIF2TX5MIX Input 3 Volume */

 R1894 (0x766) - AIF2TX5MIX Input 4 Source */

 R1895 (0x767) - AIF2TX5MIX Input 4 Volume */

 R1896 (0x768) - AIF2TX6MIX Input 1 Source */

 R1897 (0x769) - AIF2TX6MIX Input 1 Volume */

 R1898 (0x76a) - AIF2TX6MIX Input 2 Source */

 R1899 (0x76b) - AIF2TX6MIX Input 2 Volume */

 R1900 (0x76c) - AIF2TX6MIX Input 3 Source */

 R1901 (0x76d) - AIF2TX6MIX Input 3 Volume */

 R1902 (0x76e) - AIF2TX6MIX Input 4 Source */

 R1903 (0x76f) - AIF2TX6MIX Input 4 Volume */

 R1904 (0x770) - AIF2TX7MIX Input 1 Source */

 R1905 (0x771) - AIF2TX7MIX Input 1 Volume */

 R1906 (0x772) - AIF2TX7MIX Input 2 Source */

 R1907 (0x773) - AIF2TX7MIX Input 2 Volume */

 R1908 (0x774) - AIF2TX7MIX Input 3 Source */

 R1909 (0x775) - AIF2TX7MIX Input 3 Volume */

 R1910 (0x776) - AIF2TX7MIX Input 4 Source */

 R1911 (0x777) - AIF2TX7MIX Input 4 Volume */

 R1912 (0x778) - AIF2TX8MIX Input 1 Source */

 R1913 (0x779) - AIF2TX8MIX Input 1 Volume */

 R1914 (0x77a) - AIF2TX8MIX Input 2 Source */

 R1915 (0x77b) - AIF2TX8MIX Input 2 Volume */

 R1916 (0x77c) - AIF2TX8MIX Input 3 Source */

 R1917 (0x77d) - AIF2TX8MIX Input 3 Volume */

 R1918 (0x77e) - AIF2TX8MIX Input 4 Source */

 R1919 (0x77f) - AIF2TX8MIX Input 4 Volume */

 R1920 (0x780) - AIF3TX1MIX Input 1 Source */

 R1921 (0x781) - AIF3TX1MIX Input 1 Volume */

 R1922 (0x782) - AIF3TX1MIX Input 2 Source */

 R1923 (0x783) - AIF3TX1MIX Input 2 Volume */

 R1924 (0x784) - AIF3TX1MIX Input 3 Source */

 R1925 (0x785) - AIF3TX1MIX Input 3 Volume */

 R1926 (0x786) - AIF3TX1MIX Input 4 Source */

 R1927 (0x787) - AIF3TX1MIX Input 4 Volume */

 R1928 (0x788) - AIF3TX2MIX Input 1 Source */

 R1929 (0x789) - AIF3TX2MIX Input 1 Volume */

 R1930 (0x78a) - AIF3TX2MIX Input 2 Source */

 R1931 (0x78b) - AIF3TX2MIX Input 2 Volume */

 R1932 (0x78c) - AIF3TX2MIX Input 3 Source */

 R1933 (0x78d) - AIF3TX2MIX Input 3 Volume */

 R1934 (0x78e) - AIF3TX2MIX Input 4 Source */

 R1935 (0x78f) - AIF3TX2MIX Input 4 Volume */

 R1936 (0x790) - AIF3TX3MIX Input 1 Source */

 R1937 (0x791) - AIF3TX3MIX Input 1 Volume */

 R1938 (0x792) - AIF3TX3MIX Input 2 Source */

 R1939 (0x793) - AIF3TX3MIX Input 2 Volume */

 R1940 (0x794) - AIF3TX3MIX Input 3 Source */

 R1941 (0x795) - AIF3TX3MIX Input 3 Volume */

 R1942 (0x796) - AIF3TX3MIX Input 4 Source */

 R1943 (0x797) - AIF3TX3MIX Input 4 Volume */

 R1944 (0x798) - AIF3TX4MIX Input 1 Source */

 R1945 (0x799) - AIF3TX4MIX Input 1 Volume */

 R1946 (0x79a) - AIF3TX4MIX Input 2 Source */

 R1947 (0x79b) - AIF3TX4MIX Input 2 Volume */

 R1948 (0x79c) - AIF3TX4MIX Input 3 Source */

 R1949 (0x79d) - AIF3TX4MIX Input 3 Volume */

 R1950 (0x79e) - AIF3TX4MIX Input 4 Source */

 R1951 (0x79f) - AIF3TX4MIX Input 4 Volume */

 R1952 (0x7a0) - AIF3TX5MIX Input 1 Source */

 R1953 (0x7a1) - AIF3TX5MIX Input 1 Volume */

 R1954 (0x7a2) - AIF3TX5MIX Input 2 Source */

 R1955 (0x7a3) - AIF3TX5MIX Input 2 Volume */

 R1956 (0x7a4) - AIF3TX5MIX Input 3 Source */

 R1957 (0x7a5) - AIF3TX5MIX Input 3 Volume */

 R1958 (0x7a6) - AIF3TX5MIX Input 4 Source */

 R1959 (0x7a7) - AIF3TX5MIX Input 4 Volume */

 R1960 (0x7a8) - AIF3TX6MIX Input 1 Source */

 R1961 (0x7a9) - AIF3TX6MIX Input 1 Volume */

 R1962 (0x7aa) - AIF3TX6MIX Input 2 Source */

 R1963 (0x7ab) - AIF3TX6MIX Input 2 Volume */

 R1964 (0x7ac) - AIF3TX6MIX Input 3 Source */

 R1965 (0x7ad) - AIF3TX6MIX Input 3 Volume */

 R1966 (0x7ae) - AIF3TX6MIX Input 4 Source */

 R1967 (0x7af) - AIF3TX6MIX Input 4 Volume */

 R1968 (0x7b0) - AIF3TX7MIX Input 1 Source */

 R1969 (0x7b1) - AIF3TX7MIX Input 1 Volume */

 R1970 (0x7b2) - AIF3TX7MIX Input 2 Source */

 R1971 (0x7b3) - AIF3TX7MIX Input 2 Volume */

 R1972 (0x7b4) - AIF3TX7MIX Input 3 Source */

 R1973 (0x7b5) - AIF3TX7MIX Input 3 Volume */

 R1974 (0x7b6) - AIF3TX7MIX Input 4 Source */

 R1975 (0x7b7) - AIF3TX7MIX Input 4 Volume */

 R1976 (0x7b8) - AIF3TX8MIX Input 1 Source */

 R1977 (0x7b9) - AIF3TX8MIX Input 1 Volume */

 R1978 (0x7ba) - AIF3TX8MIX Input 2 Source */

 R1979 (0x7bb) - AIF3TX8MIX Input 2 Volume */

 R1980 (0x7bc) - AIF3TX8MIX Input 3 Source */

 R1981 (0x7bd) - AIF3TX8MIX Input 3 Volume */

 R1982 (0x7be) - AIF3TX8MIX Input 4 Source */

 R1983 (0x7bf) - AIF3TX8MIX Input 4 Volume */

 R1984 (0x7c0) - SLIMTX1MIX Input 1 Source */

 R1985 (0x7c1) - SLIMTX1MIX Input 1 Volume */

 R1986 (0x7c2) - SLIMTX1MIX Input 2 Source */

 R1987 (0x7c3) - SLIMTX1MIX Input 2 Volume */

 R1988 (0x7c4) - SLIMTX1MIX Input 3 Source */

 R1989 (0x7c5) - SLIMTX1MIX Input 3 Volume */

 R1990 (0x7c6) - SLIMTX1MIX Input 4 Source */

 R1991 (0x7c7) - SLIMTX1MIX Input 4 Volume */

 R1992 (0x7c8) - SLIMTX2MIX Input 1 Source */

 R1993 (0x7c9) - SLIMTX2MIX Input 1 Volume */

 R1994 (0x7ca) - SLIMTX2MIX Input 2 Source */

 R1995 (0x7cb) - SLIMTX2MIX Input 2 Volume */

 R1996 (0x7cc) - SLIMTX2MIX Input 3 Source */

 R1997 (0x7cd) - SLIMTX2MIX Input 3 Volume */

 R1998 (0x7ce) - SLIMTX2MIX Input 4 Source */

 R1999 (0x7cf) - SLIMTX2MIX Input 4 Volume */

 R2000 (0x7d0) - SLIMTX3MIX Input 1 Source */

 R2001 (0x7d1) - SLIMTX3MIX Input 1 Volume */

 R2002 (0x7d2) - SLIMTX3MIX Input 2 Source */

 R2003 (0x7d3) - SLIMTX3MIX Input 2 Volume */

 R2004 (0x7d4) - SLIMTX3MIX Input 3 Source */

 R2005 (0x7d5) - SLIMTX3MIX Input 3 Volume */

 R2006 (0x7d6) - SLIMTX3MIX Input 4 Source */

 R2007 (0x7d7) - SLIMTX3MIX Input 4 Volume */

 R2008 (0x7d8) - SLIMTX4MIX Input 1 Source */

 R2009 (0x7d9) - SLIMTX4MIX Input 1 Volume */

 R2010 (0x7da) - SLIMTX4MIX Input 2 Source */

 R2011 (0x7db) - SLIMTX4MIX Input 2 Volume */

 R2012 (0x7dc) - SLIMTX4MIX Input 3 Source */

 R2013 (0x7dd) - SLIMTX4MIX Input 3 Volume */

 R2014 (0x7de) - SLIMTX4MIX Input 4 Source */

 R2015 (0x7df) - SLIMTX4MIX Input 4 Volume */

 R2016 (0x7e0) - SLIMTX5MIX Input 1 Source */

 R2017 (0x7e1) - SLIMTX5MIX Input 1 Volume */

 R2018 (0x7e2) - SLIMTX5MIX Input 2 Source */

 R2019 (0x7e3) - SLIMTX5MIX Input 2 Volume */

 R2020 (0x7e4) - SLIMTX5MIX Input 3 Source */

 R2021 (0x7e5) - SLIMTX5MIX Input 3 Volume */

 R2022 (0x7e6) - SLIMTX5MIX Input 4 Source */

 R2023 (0x7e7) - SLIMTX5MIX Input 4 Volume */

 R2024 (0x7e8) - SLIMTX6MIX Input 1 Source */

 R2025 (0x7e9) - SLIMTX6MIX Input 1 Volume */

 R2026 (0x7ea) - SLIMTX6MIX Input 2 Source */

 R2027 (0x7eb) - SLIMTX6MIX Input 2 Volume */

 R2028 (0x7ec) - SLIMTX6MIX Input 3 Source */

 R2029 (0x7ed) - SLIMTX6MIX Input 3 Volume */

 R2030 (0x7ee) - SLIMTX6MIX Input 4 Source */

 R2031 (0x7ef) - SLIMTX6MIX Input 4 Volume */

 R2032 (0x7f0) - SLIMTX7MIX Input 1 Source */

 R2033 (0x7f1) - SLIMTX7MIX Input 1 Volume */

 R2034 (0x7f2) - SLIMTX7MIX Input 2 Source */

 R2035 (0x7f3) - SLIMTX7MIX Input 2 Volume */

 R2036 (0x7f4) - SLIMTX7MIX Input 3 Source */

 R2037 (0x7f5) - SLIMTX7MIX Input 3 Volume */

 R2038 (0x7f6) - SLIMTX7MIX Input 4 Source */

 R2039 (0x7f7) - SLIMTX7MIX Input 4 Volume */

 R2040 (0x7f8) - SLIMTX8MIX Input 1 Source */

 R2041 (0x7f9) - SLIMTX8MIX Input 1 Volume */

 R2042 (0x7fa) - SLIMTX8MIX Input 2 Source */

 R2043 (0x7fb) - SLIMTX8MIX Input 2 Volume */

 R2044 (0x7fc) - SLIMTX8MIX Input 3 Source */

 R2045 (0x7fd) - SLIMTX8MIX Input 3 Volume */

 R2046 (0x7fe) - SLIMTX8MIX Input 4 Source */

 R2047 (0x7ff) - SLIMTX8MIX Input 4 Volume */

 R2048 (0x800) - SPDIF1TX1MIX Input 1 Source */

 R2049 (0x801) - SPDIF1TX1MIX Input 1 Volume */

 R2056 (0x808) - SPDIF1TX2MIX Input 1 Source */

 R2057 (0x809) - SPDIF1TX2MIX Input 1 Volume */

 R2176 (0x880) - EQ1MIX Input 1 Source */

 R2177 (0x881) - EQ1MIX Input 1 Volume */

 R2178 (0x882) - EQ1MIX Input 2 Source */

 R2179 (0x883) - EQ1MIX Input 2 Volume */

 R2180 (0x884) - EQ1MIX Input 3 Source */

 R2181 (0x885) - EQ1MIX Input 3 Volume */

 R2182 (0x886) - EQ1MIX Input 4 Source */

 R2183 (0x887) - EQ1MIX Input 4 Volume */

 R2184 (0x888) - EQ2MIX Input 1 Source */

 R2185 (0x889) - EQ2MIX Input 1 Volume */

 R2186 (0x88a) - EQ2MIX Input 2 Source */

 R2187 (0x88b) - EQ2MIX Input 2 Volume */

 R2188 (0x88c) - EQ2MIX Input 3 Source */

 R2189 (0x88d) - EQ2MIX Input 3 Volume */

 R2190 (0x88e) - EQ2MIX Input 4 Source */

 R2191 (0x88f) - EQ2MIX Input 4 Volume */

 R2192 (0x890) - EQ3MIX Input 1 Source */

 R2193 (0x891) - EQ3MIX Input 1 Volume */

 R2194 (0x892) - EQ3MIX Input 2 Source */

 R2195 (0x893) - EQ3MIX Input 2 Volume */

 R2196 (0x894) - EQ3MIX Input 3 Source */

 R2197 (0x895) - EQ3MIX Input 3 Volume */

 R2198 (0x896) - EQ3MIX Input 4 Source */

 R2199 (0x897) - EQ3MIX Input 4 Volume */

 R2200 (0x898) - EQ4MIX Input 1 Source */

 R2201 (0x899) - EQ4MIX Input 1 Volume */

 R2202 (0x89a) - EQ4MIX Input 2 Source */

 R2203 (0x89b) - EQ4MIX Input 2 Volume */

 R2204 (0x89c) - EQ4MIX Input 3 Source */

 R2205 (0x89d) - EQ4MIX Input 3 Volume */

 R2206 (0x89e) - EQ4MIX Input 4 Source */

 R2207 (0x89f) - EQ4MIX Input 4 Volume */

 R2240 (0x8c0) - DRC1LMIX Input 1 Source */

 R2241 (0x8c1) - DRC1LMIX Input 1 Volume */

 R2242 (0x8c2) - DRC1LMIX Input 2 Source */

 R2243 (0x8c3) - DRC1LMIX Input 2 Volume */

 R2244 (0x8c4) - DRC1LMIX Input 3 Source */

 R2245 (0x8c5) - DRC1LMIX Input 3 Volume */

 R2246 (0x8c6) - DRC1LMIX Input 4 Source */

 R2247 (0x8c7) - DRC1LMIX Input 4 Volume */

 R2248 (0x8c8) - DRC1RMIX Input 1 Source */

 R2249 (0x8c9) - DRC1RMIX Input 1 Volume */

 R2250 (0x8ca) - DRC1RMIX Input 2 Source */

 R2251 (0x8cb) - DRC1RMIX Input 2 Volume */

 R2252 (0x8cc) - DRC1RMIX Input 3 Source */

 R2253 (0x8cd) - DRC1RMIX Input 3 Volume */

 R2254 (0x8ce) - DRC1RMIX Input 4 Source */

 R2255 (0x8cf) - DRC1RMIX Input 4 Volume */

 R2256 (0x8d0) - DRC2LMIX Input 1 Source */

 R2257 (0x8d1) - DRC2LMIX Input 1 Volume */

 R2258 (0x8d2) - DRC2LMIX Input 2 Source */

 R2259 (0x8d3) - DRC2LMIX Input 2 Volume */

 R2260 (0x8d4) - DRC2LMIX Input 3 Source */

 R2261 (0x8d5) - DRC2LMIX Input 3 Volume */

 R2262 (0x8d6) - DRC2LMIX Input 4 Source */

 R2263 (0x8d7) - DRC2LMIX Input 4 Volume */

 R2264 (0x8d8) - DRC2RMIX Input 1 Source */

 R2265 (0x8d9) - DRC2RMIX Input 1 Volume */

 R2266 (0x8da) - DRC2RMIX Input 2 Source */

 R2267 (0x8db) - DRC2RMIX Input 2 Volume */

 R2268 (0x8dc) - DRC2RMIX Input 3 Source */

 R2269 (0x8dd) - DRC2RMIX Input 3 Volume */

 R2270 (0x8de) - DRC2RMIX Input 4 Source */

 R2271 (0x8df) - DRC2RMIX Input 4 Volume */

 R2304 (0x900) - HPLP1MIX Input 1 Source */

 R2305 (0x901) - HPLP1MIX Input 1 Volume */

 R2306 (0x902) - HPLP1MIX Input 2 Source */

 R2307 (0x903) - HPLP1MIX Input 2 Volume */

 R2308 (0x904) - HPLP1MIX Input 3 Source */

 R2309 (0x905) - HPLP1MIX Input 3 Volume */

 R2310 (0x906) - HPLP1MIX Input 4 Source */

 R2311 (0x907) - HPLP1MIX Input 4 Volume */

 R2312 (0x908) - HPLP2MIX Input 1 Source */

 R2313 (0x909) - HPLP2MIX Input 1 Volume */

 R2314 (0x90a) - HPLP2MIX Input 2 Source */

 R2315 (0x90b) - HPLP2MIX Input 2 Volume */

 R2316 (0x90c) - HPLP2MIX Input 3 Source */

 R2317 (0x90d) - HPLP2MIX Input 3 Volume */

 R2318 (0x90e) - HPLP2MIX Input 4 Source */

 R2319 (0x90f) - HPLP2MIX Input 4 Volume */

 R2320 (0x910) - HPLP3MIX Input 1 Source */

 R2321 (0x911) - HPLP3MIX Input 1 Volume */

 R2322 (0x912) - HPLP3MIX Input 2 Source */

 R2323 (0x913) - HPLP3MIX Input 2 Volume */

 R2324 (0x914) - HPLP3MIX Input 3 Source */

 R2325 (0x915) - HPLP3MIX Input 3 Volume */

 R2326 (0x916) - HPLP3MIX Input 4 Source */

 R2327 (0x917) - HPLP3MIX Input 4 Volume */

 R2328 (0x918) - HPLP4MIX Input 1 Source */

 R2329 (0x919) - HPLP4MIX Input 1 Volume */

 R2330 (0x91a) - HPLP4MIX Input 2 Source */

 R2331 (0x91b) - HPLP4MIX Input 2 Volume */

 R2332 (0x91c) - HPLP4MIX Input 3 Source */

 R2333 (0x91d) - HPLP4MIX Input 3 Volume */

 R2334 (0x91e) - HPLP4MIX Input 4 Source */

 R2335 (0x91f) - HPLP4MIX Input 4 Volume */

 R2368 (0x940) - DSP1LMIX Input 1 Source */

 R2369 (0x941) - DSP1LMIX Input 1 Volume */

 R2370 (0x942) - DSP1LMIX Input 2 Source */

 R2371 (0x943) - DSP1LMIX Input 2 Volume */

 R2372 (0x944) - DSP1LMIX Input 3 Source */

 R2373 (0x945) - DSP1LMIX Input 3 Volume */

 R2374 (0x946) - DSP1LMIX Input 4 Source */

 R2375 (0x947) - DSP1LMIX Input 4 Volume */

 R2376 (0x948) - DSP1RMIX Input 1 Source */

 R2377 (0x949) - DSP1RMIX Input 1 Volume */

 R2378 (0x94a) - DSP1RMIX Input 2 Source */

 R2379 (0x94b) - DSP1RMIX Input 2 Volume */

 R2380 (0x94c) - DSP1RMIX Input 3 Source */

 R2381 (0x94d) - DSP1RMIX Input 3 Volume */

 R2382 (0x94e) - DSP1RMIX Input 4 Source */

 R2383 (0x94f) - DSP1RMIX Input 4 Volume */

 R2384 (0x950) - DSP1AUX1MIX Input 1 Source */

 R2392 (0x958) - DSP1AUX2MIX Input 1 Source */

 R2400 (0x960) - DSP1AUX3MIX Input 1 Source */

 R2408 (0x968) - DSP1AUX4MIX Input 1 Source */

 R2416 (0x970) - DSP1AUX5MIX Input 1 Source */

 R2424 (0x978) - DSP1AUX6MIX Input 1 Source */

 R2688 (0xa80) - ASRC1 1LMIX Input 1 Source */

 R2696 (0xa88) - ASRC1 1RMIX Input 1 Source */

 R2704 (0xa90) - ASRC1 2LMIX Input 1 Source */

 R2712 (0xa98) - ASRC1 2RMIX Input 1 Source */

 R2816 (0xb00) - ISRC1DEC1MIX Input 1 Source */

 R2824 (0xb08) - ISRC1DEC2MIX Input 1 Source */

 R2848 (0xb20) - ISRC1INT1MIX Input 1 Source */

 R2856 (0xb28) - ISRC1INT2MIX Input 1 Source */

 R2880 (0xb40) - ISRC2DEC1MIX Input 1 Source */

 R2888 (0xb48) - ISRC2DEC2MIX Input 1 Source */

 R2912 (0xb60) - ISRC2INT1MIX Input 1 Source */

 R2920 (0xb68) - ISRC2INT2MIX Input 1 Source */

 R3520 (0xdc0) - DFC1MIX Input 1 Source */

 R3528 (0xdc8) - DFC2MIX Input 1 Source */

 R3536 (0xdd0) - DFC3MIX Input 1 Source */

 R3544 (0xdd8) - DFC4MIX Input 1 Source */

 R3552 (0xde0) - DFC5MIX Input 1 Source */

 R3560 (0xde8) - DFC6MIX Input 1 Source */

 R3568 (0xdf0) - DFC7MIX Input 1 Source */

 R3576 (0xdf8) - DFC8MIX Input 1 Source */

 R3584 (0xe00) - FX Ctrl 1 */

 R3600 (0xe10) - EQ1 1 */

 R3601 (0xe11) - EQ1 2 */

 R3602 (0xe12) - EQ1 3 */

 R3603 (0xe13) - EQ1 4 */

 R3604 (0xe14) - EQ1 5 */

 R3605 (0xe15) - EQ1 6 */

 R3606 (0xe16) - EQ1 7 */

 R3607 (0xe17) - EQ1 8 */

 R3608 (0xe18) - EQ1 9 */

 R3609 (0xe19) - EQ1 10 */

 R3610 (0xe1a) - EQ1 11 */

 R3611 (0xe1b) - EQ1 12 */

 R3612 (0xe1c) - EQ1 13 */

 R3613 (0xe1d) - EQ1 14 */

 R3614 (0xe1e) - EQ1 15 */

 R3615 (0xe1f) - EQ1 16 */

 R3616 (0xe20) - EQ1 17 */

 R3617 (0xe21) - EQ1 18 */

 R3618 (0xe22) - EQ1 19 */

 R3619 (0xe23) - EQ1 20 */

 R3620 (0xe24) - EQ1 21 */

 R3622 (0xe26) - EQ2 1 */

 R3623 (0xe27) - EQ2 2 */

 R3624 (0xe28) - EQ2 3 */

 R3625 (0xe29) - EQ2 4 */

 R3626 (0xe2a) - EQ2 5 */

 R3627 (0xe2b) - EQ2 6 */

 R3628 (0xe2c) - EQ2 7 */

 R3629 (0xe2d) - EQ2 8 */

 R3630 (0xe2e) - EQ2 9 */

 R3631 (0xe2f) - EQ2 10 */

 R3632 (0xe30) - EQ2 11 */

 R3633 (0xe31) - EQ2 12 */

 R3634 (0xe32) - EQ2 13 */

 R3635 (0xe33) - EQ2 14 */

 R3636 (0xe34) - EQ2 15 */

 R3637 (0xe35) - EQ2 16 */

 R3638 (0xe36) - EQ2 17 */

 R3639 (0xe37) - EQ2 18 */

 R3640 (0xe38) - EQ2 19 */

 R3641 (0xe39) - EQ2 20 */

 R3642 (0xe3a) - EQ2 21 */

 R3644 (0xe3c) - EQ3 1 */

 R3645 (0xe3d) - EQ3 2 */

 R3646 (0xe3e) - EQ3 3 */

 R3647 (0xe3f) - EQ3 4 */

 R3648 (0xe40) - EQ3 5 */

 R3649 (0xe41) - EQ3 6 */

 R3650 (0xe42) - EQ3 7 */

 R3651 (0xe43) - EQ3 8 */

 R3652 (0xe44) - EQ3 9 */

 R3653 (0xe45) - EQ3 10 */

 R3654 (0xe46) - EQ3 11 */

 R3655 (0xe47) - EQ3 12 */

 R3656 (0xe48) - EQ3 13 */

 R3657 (0xe49) - EQ3 14 */

 R3658 (0xe4a) - EQ3 15 */

 R3659 (0xe4b) - EQ3 16 */

 R3660 (0xe4c) - EQ3 17 */

 R3661 (0xe4d) - EQ3 18 */

 R3662 (0xe4e) - EQ3 19 */

 R3663 (0xe4f) - EQ3 20 */

 R3664 (0xe50) - EQ3 21 */

 R3666 (0xe52) - EQ4 1 */

 R3667 (0xe53) - EQ4 2 */

 R3668 (0xe54) - EQ4 3 */

 R3669 (0xe55) - EQ4 4 */

 R3670 (0xe56) - EQ4 5 */

 R3671 (0xe57) - EQ4 6 */

 R3672 (0xe58) - EQ4 7 */

 R3673 (0xe59) - EQ4 8 */

 R3674 (0xe5a) - EQ4 9 */

 R3675 (0xe5b) - EQ4 10 */

 R3676 (0xe5c) - EQ4 11 */

 R3677 (0xe5d) - EQ4 12 */

 R3678 (0xe5e) - EQ4 13 */

 R3679 (0xe5f) - EQ4 14 */

 R3680 (0xe60) - EQ4 15 */

 R3681 (0xe61) - EQ4 16 */

 R3682 (0xe62) - EQ4 17 */

 R3683 (0xe63) - EQ4 18 */

 R3684 (0xe64) - EQ4 19 */

 R3685 (0xe65) - EQ4 20 */

 R3686 (0xe66) - EQ4 21 */

 R3712 (0xe80) - DRC1 Ctrl 1 */

 R3713 (0xe81) - DRC1 Ctrl 2 */

 R3714 (0xe82) - DRC1 Ctrl 3 */

 R3715 (0xe83) - DRC1 Ctrl 4 */

 R3716 (0xe84) - DRC1 Ctrl 5 */

 R3720 (0xe88) - DRC2 Ctrl 1 */

 R3721 (0xe89) - DRC2 Ctrl 2 */

 R3722 (0xe8a) - DRC2 Ctrl 3 */

 R3723 (0xe8b) - DRC2 Ctrl 4 */

 R3724 (0xe8c) - DRC2 Ctrl 5 */

 R3776 (0xec0) - HPLPF1 1 */

 R3777 (0xec1) - HPLPF1 2 */

 R3780 (0xec4) - HPLPF2 1 */

 R3781 (0xec5) - HPLPF2 2 */

 R3784 (0xec8) - HPLPF3 1 */

 R3785 (0xec9) - HPLPF3 2 */

 R3788 (0xecc) - HPLPF4 1 */

 R3789 (0xecd) - HPLPF4 2 */

 R3808 (0xee0) - ASRC1 Enable */

 R3810 (0xee2) - ASRC1 Rate 1 */

 R3811 (0xee3) - ASRC1 Rate 2 */

 R3824 (0xef0) - ISRC1 Ctrl 1 */

 R3825 (0xef1) - ISRC1 Ctrl 2 */

 R3826 (0xef2) - ISRC1 Ctrl 3 */

 R3827 (0xef3) - ISRC2 Ctrl 1 */

 R3828 (0xef4) - ISRC2 Ctrl 2 */

 R3829 (0xef5) - ISRC2 Ctrl 3 */

 R4288 (0x10c0) - AUXPDM1 Ctrl 0 */

 R4289 (0x10c1) - AUXPDM1 Ctrl 1 */

 R5248 (0x1480) - DFC1 Ctrl W0 */

 R5250 (0x1482) - DFC1 Rx W0 */

 R5252 (0x1484) - DFC1 Tx W0 */

 R5254 (0x1486) - DFC2 Ctrl W0 */

 R5256 (0x1488) - DFC2 Rx W0 */

 R5258 (0x148a) - DFC2 Tx W0 */

 R5260 (0x148c) - DFC3 Ctrl W0 */

 R5262 (0x148e) - DFC3 Rx W0 */

 R5264 (0x1490) - DFC3 Tx W0 */

 R5266 (0x1492) - DFC4 Ctrl W0 */

 R5268 (0x1494) - DFC4 Rx W0 */

 R5270 (0x1496) - DFC4 Tx W0 */

 R5272 (0x1498) - DFC5 Ctrl W0 */

 R5274 (0x149a) - DFC5 Rx W0 */

 R5276 (0x149c) - DFC5 Tx W0 */

 R5278 (0x149e) - DFC6 Ctrl W0 */

 R5280 (0x14a0) - DFC6 Rx W0 */

 R5282 (0x14a2) - DFC6 Tx W0 */

 R5284 (0x14a4) - DFC7 Ctrl W0 */

 R5286 (0x14a6) - DFC7 Rx W0 */

 R5288 (0x14a8) - DFC7 Tx W0 */

 R5290 (0x14aa) - DFC8 Ctrl W0 */

 R5292 (0x14ac) - DFC8 Rx W0 */

 R5294 (0x14ae) - DFC8 Tx W0 */

 R5888 (0x1700) - GPIO1 Ctrl 1 */

 R5889 (0x1701) - GPIO1 Ctrl 2 */

 R5890 (0x1702) - GPIO2 Ctrl 1 */

 R5891 (0x1703) - GPIO2 Ctrl 2 */

 R5892 (0x1704) - GPIO3 Ctrl 1 */

 R5893 (0x1705) - GPIO3 Ctrl 2 */

 R5894 (0x1706) - GPIO4 Ctrl 1 */

 R5895 (0x1707) - GPIO4 Ctrl 2 */

 R5896 (0x1708) - GPIO5 Ctrl 1 */

 R5897 (0x1709) - GPIO5 Ctrl 2 */

 R5898 (0x170a) - GPIO6 Ctrl 1 */

 R5899 (0x170b) - GPIO6 Ctrl 2 */

 R5900 (0x170c) - GPIO7 Ctrl 1 */

 R5901 (0x170d) - GPIO7 Ctrl 2 */

 R5902 (0x170e) - GPIO8 Ctrl 1 */

 R5903 (0x170f) - GPIO8 Ctrl 2 */

 R5904 (0x1710) - GPIO9 Ctrl 1 */

 R5905 (0x1711) - GPIO9 Ctrl 2 */

 R5906 (0x1712) - GPIO10 Ctrl 1 */

 R5907 (0x1713) - GPIO10 Ctrl 2 */

 R5908 (0x1714) - GPIO11 Ctrl 1 */

 R5909 (0x1715) - GPIO11 Ctrl 2 */

 R5910 (0x1716) - GPIO12 Ctrl 1 */

 R5911 (0x1717) - GPIO12 Ctrl 2 */

 R5912 (0x1718) - GPIO13 Ctrl 1 */

 R5913 (0x1719) - GPIO13 Ctrl 2 */

 R5914 (0x171a) - GPIO14 Ctrl 1 */

 R5915 (0x171b) - GPIO14 Ctrl 2 */

 R5916 (0x171c) - GPIO15 Ctrl 1 */

 R5917 (0x171d) - GPIO15 Ctrl 2 */

 R5918 (0x171e) - GPIO16 Ctrl 1 */

 R5919 (0x171f) - GPIO16 Ctrl 2 */

 R6208 (0x1840) - IRQ1 Mask 1 */

 R6209 (0x1841) - IRQ1 Mask 2 */

 R6210 (0x1842) - IRQ1 Mask 3 */

 R6211 (0x1843) - IRQ1 Mask 4 */

 R6212 (0x1844) - IRQ1 Mask 5 */

 R6213 (0x1845) - IRQ1 Mask 6 */

 R6214 (0x1846) - IRQ1 Mask 7 */

 R6215 (0x1847) - IRQ1 Mask 8 */

 R6216 (0x1848) - IRQ1 Mask 9 */

 R6217 (0x1849) - IRQ1 Mask 10 */

 R6218 (0x184a) - IRQ1 Mask 11 */

 R6219 (0x184b) - IRQ1 Mask 12 */

 R6220 (0x184c) - IRQ1 Mask 13 */

 R6221 (0x184d) - IRQ1 Mask 14 */

 R6222 (0x184e) - IRQ1 Mask 15 */

 R6223 (0x184f) - IRQ1 Mask 16 */

 R6224 (0x1850) - IRQ1 Mask 17 */

 R6225 (0x1851) - IRQ1 Mask 18 */

 R6226 (0x1852) - IRQ1 Mask 19 */

 R6227 (0x1853) - IRQ1 Mask 20 */

 R6228 (0x1854) - IRQ1 Mask 21 */

 R6229 (0x1855) - IRQ1 Mask 22 */

 R6230 (0x1856) - IRQ1 Mask 23 */

 R6231 (0x1857) - IRQ1 Mask 24 */

 R6232 (0x1858) - IRQ1 Mask 25 */

 R6233 (0x1859) - IRQ1 Mask 26 */

 R6234 (0x185a) - IRQ1 Mask 27 */

 R6235 (0x185b) - IRQ1 Mask 28 */

 R6236 (0x185c) - IRQ1 Mask 29 */

 R6237 (0x185d) - IRQ1 Mask 30 */

 R6238 (0x185e) - IRQ1 Mask 31 */

 R6239 (0x185f) - IRQ1 Mask 32 */

 R6240 (0x1860) - IRQ1 Mask 33 */

 R6662 (0x1a06) - Interrupt Debounce 7 */

 R6784 (0x1a80) - IRQ1 Ctrl */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/mfd/mfd-core.c

 *

 * core MFD support

 * Copyright (c) 2006 Ian Molton

 * Copyright (c) 2007,2008 Dmitry Baryshkov

	/*

	 * MFD child device gets its ACPI handle either from the ACPI device

	 * directly under the parent that matches the either _HID or _CID, or

	 * _ADR or it will use the parent handle if is no ID is given.

	 *

	 * Note that use of _ADR is a grey area in the ACPI specification,

	 * though at least Intel Galileo Gen 2 is using it to distinguish

	 * the children devices.

 Skip if OF node has previously been allocated to a device */

 No of_reg defined - allocate first free compatible match */

 We only care about each node's first defined address */

 OF node does not contatin a 'reg' property to match to */

 No match */

 Ignore 'disabled' devices error free */

 Find out base to use */

 Unable to create mappings for IRQ ranges. */

/**

 * mfd_add_devices - register child devices

 *

 * @parent:	Pointer to parent device.

 * @id:		Can be PLATFORM_DEVID_AUTO to let the Platform API take care

 *		of device numbering, or will be added to a device's cell_id.

 * @cells:	Array of (struct mfd_cell)s describing child devices.

 * @n_devs:	Number of child devices to register.

 * @mem_base:	Parent register range resource for child devices.

 * @irq_base:	Base of the range of virtual interrupt numbers allocated for

 *		this MFD device. Unused if @domain is specified.

 * @domain:	Interrupt domain to create mappings for hardware interrupts.

/**

 * devm_mfd_add_devices - Resource managed version of mfd_add_devices()

 *

 * Returns 0 on success or an appropriate negative error number on failure.

 * All child-devices of the MFD will automatically be removed when it gets

 * unbinded.

 *

 * @dev:	Pointer to parent device.

 * @id:		Can be PLATFORM_DEVID_AUTO to let the Platform API take care

 *		of device numbering, or will be added to a device's cell_id.

 * @cells:	Array of (struct mfd_cell)s describing child devices.

 * @n_devs:	Number of child devices to register.

 * @mem_base:	Parent register range resource for child devices.

 * @irq_base:	Base of the range of virtual interrupt numbers allocated for

 *		this MFD device. Unused if @domain is specified.

 * @domain:	Interrupt domain to create mappings for hardware interrupts.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD driver for wl1273 FM radio and audio codec submodules.

 *

 * Copyright (C) 2011 Nokia Corporation

 * Author: Matti Aaltonen <matti.j.aaltonen@nokia.com>

/**

 * wl1273_fm_set_audio() -	Set audio mode.

 * @core:			A pointer to the device struct.

 * @new_mode:			The new audio mode.

 *

 * Audio modes are WL1273_AUDIO_DIGITAL and WL1273_AUDIO_ANALOG.

/**

 * wl1273_fm_set_volume() -	Set volume.

 * @core:			A pointer to the device struct.

 * @volume:			The new volume value.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel LPSS ACPI support.

 *

 * Copyright (C) 2015, Intel Corporation

 *

 * Authors: Andy Shevchenko <andriy.shevchenko@linux.intel.com>

 *          Mika Westerberg <mika.westerberg@linux.intel.com>

 SPT */

 CNL */

 BXT */

 APL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Base driver for Marvell 88PM8607

 *

 * Copyright (C) 2009 Marvell International Ltd.

 *

 * Author: Haojian Zhuang <haojian.zhuang@marvell.com>

 RGB1 Red LED */

 RGB1 Green LED */

 RGB1 Blue LED */

 RGB2 Red LED */

 RGB2 Green LED */

 RGB2 Blue LED */

 Headset microphone insertion or removal */

 Hook-switch press or release */

 Headset insertion or removal */

 Audio short */

 enable or not */

 bit offset in mask register */

 Load cached value. In initial, all IRQs are masked */

 update mask into registers */

		/*

		 * irq_mode defines the way of clearing interrupt. If it's 1,

		 * clear IRQ by write. Otherwise, clear it by read.

		 * This control bit is valid from 88PM8607 B0 steping.

 mask all IRQs */

 clear interrupt status by write */

 clear interrupt status by read */

 Update voting status */

 If reference group is off - turn on*/

 Enable Reference group Vsys */

Enable Internal Oscillator */

 Update status (only if writes succeed) */

 Update voting status */

	/*

	 * If reference group is off and this is the last client to release

	 * - turn off

 Disable Reference group Vsys */

 Disable Internal Oscillator */

 init portofino reference group voting and status */

 Disable Reference group Vsys */

 Disable Internal Oscillator */

 parse DT to get platform data */

	/*

	 * Both client and companion client shares same platform driver.

	 * Driver distinguishes them by pdata->companion_addr.

	 * pdata->companion_addr is only assigned if companion chip exists.

	 * At the same time, the companion_addr shouldn't equal to client

	 * address.

 SPDX-License-Identifier: GPL-2.0

/*

 * STM32 Low-Power Timer parent driver.

 * Copyright (C) STMicroelectronics 2017

 * Author: Fabrice Gasnier <fabrice.gasnier@st.com>

 * Inspired by Benjamin Gaignard's stm32-timers driver

	/*

	 * Quadrature encoder mode bit can only be written and read back when

	 * Low-Power Timer supports it.

/*

 * I2C access driver for TI TPS65912x PMICs

 *

 * Copyright (C) 2015 Texas Instruments Incorporated - https://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether expressed or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License version 2 for more details.

 *

 * Based on the TPS65218 driver and the previous TPS65912 driver by

 * Margarita Olaya Cabrera <magi@slimlogic.co.uk>

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arizona-spi.c  --  Arizona SPI bus interface

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

/*

 * The ACPI resources for the device only describe external GPIO-s. They do

 * not provide mappings for the GPIO-s coming from the Arizona codec itself.

/*

 * The AOSP 3.5 mm Headset: Accessory Specification gives the following values:

 * Function A Play/Pause:           0 ohm

 * Function D Voice assistant:    135 ohm

 * Function B Volume Up           240 ohm

 * Function C Volume Down         470 ohm

 * Minimum Mic DC resistance     1000 ohm

 * Minimum Ear speaker impedance   16 ohm

 * Note the first max value below must be less then the min. speaker impedance,

 * to allow CTIA/OMTP detection to work. The other max values are the closest

 * value from extcon-arizona.c:arizona_micd_levels halfway 2 button resistances.

 Add mappings for the 2 ACPI declared GPIOs used for reset and ldo-ena */

 Add lookups for the SoCs own GPIOs used for micdet-polarity and spkVDD-enable */

 Enable 32KHz clock from SoC to codec for jack-detect */

	/*

	 * Some DSDTs wrongly declare the IRQ trigger-type as IRQF_TRIGGER_FALLING

	 * The IRQ line will stay low when a new IRQ event happens between reading

	 * the IRQ status flags and acknowledging them. When the IRQ line stays

	 * low like this the IRQ will never trigger again when its type is set

	 * to IRQF_TRIGGER_FALLING. Correct the IRQ trigger-type to fix this.

	 *

	 * Note theoretically it is possible that some boards are not capable

	 * of handling active low level interrupts. In that case setting the

	 * flag to IRQF_TRIGGER_FALLING would not be a bug (and we would need

	 * to work around this) but so far all known usages of IRQF_TRIGGER_FALLING

	 * are a bug in the board's DSDT.

 Wait 200 ms after jack insertion */

 Use standard AOSP values for headset-button mappings */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) ST-Ericsson SA 2010

 *

 * Author: Mattias Wallin <mattias.wallin@stericsson.com> for ST-Ericsson.

/*

 * AB8500 register access

 * ======================

 *

 * read:

 * # echo BANK  >  <debugfs>/ab8500/register-bank

 * # echo ADDR  >  <debugfs>/ab8500/register-address

 * # cat <debugfs>/ab8500/register-value

 *

 * write:

 * # echo BANK  >  <debugfs>/ab8500/register-bank

 * # echo ADDR  >  <debugfs>/ab8500/register-address

 * # echo VALUE >  <debugfs>/ab8500/register-value

 *

 * read all registers from a bank:

 * # echo BANK  >  <debugfs>/ab8500/register-bank

 * # cat <debugfs>/ab8500/all-bank-register

 *

 * BANK   target AB8500 register bank

 * ADDR   target AB8500 register address

 * VALUE  decimal or 0x-prefixed hexadecimal

 *

 *

 * User Space notification on AB8500 IRQ

 * =====================================

 *

 * Allows user space entity to be notified when target AB8500 IRQ occurs.

 * When subscribed, a sysfs entry is created in ab8500.i2c platform device.

 * One can pool this file to get target IRQ occurence information.

 *

 * subscribe to an AB8500 IRQ:

 * # echo IRQ  >  <debugfs>/ab8500/irq-subscribe

 *

 * unsubscribe from an AB8500 IRQ:

 * # echo IRQ  >  <debugfs>/ab8500/irq-unsubscribe

 *

 *

 * AB8500 register formated read/write access

 * ==========================================

 *

 * Read:  read data, data>>SHIFT, data&=MASK, output data

 *        [0xABCDEF98] shift=12 mask=0xFFF => 0x00000CDE

 * Write: read data, data &= ~(MASK<<SHIFT), data |= (VALUE<<SHIFT), write data

 *        [0xABCDEF98] shift=12 mask=0xFFF value=0x123 => [0xAB123F98]

 *

 * Usage:

 * # echo "CMD [OPTIONS] BANK ADRESS [VALUE]" > $debugfs/ab8500/hwreg

 *

 * CMD      read      read access

 *          write     write access

 *

 * BANK     target reg bank

 * ADDRESS  target reg address

 * VALUE    (write) value to be updated

 *

 * OPTIONS

 *  -d|-dec            (read) output in decimal

 *  -h|-hexa           (read) output in 0x-hexa (default)

 *  -l|-w|-b           32bit (default), 16bit or 8bit reg access

 *  -m|-mask MASK      0x-hexa mask (default 0xFFFFFFFF)

 *  -s|-shift SHIFT    bit shift value (read:left, write:right)

 *  -o|-offset OFFSET  address offset to add to ADDRESS value

 *

 * Warning: bit shift operation is applied to bit-mask.

 * Warning: bit shift direction depends on read or right command.

/**

 * struct ab8500_reg_range

 * @first: the first address of the range

 * @last: the last address of the range

 * @perm: access permissions for the range

/**

 * struct ab8500_prcmu_ranges

 * @num_ranges: the number of ranges in the list

 * @bankid: bank identifier

 * @range: the list of register ranges

 hwreg- "mask" and "shift" entries ressources */

 target bank */

 target address */

 format */

 read/write mask, applied before any bit shift */

 bit shift (read:right shift, write:left shift */

 fmt bit #0: 0=hexa, 1=dec */

 default: invalid phys addr */

 default: 32bit access, hex output */

 default: no mask */

 default: no bit shift */

			/*

			 * 0x80-0x8B are SIM registers and should

			 * not be accessed from here

			/*

			 * 0x80-0x8B are SIM registers and should

			 * not be accessed from here

 Latch registers should not be read here */

 LatchHier registers should not be read here */

 Latch registers should not be read here */

 LatchHier registers should not be read here */

	/*

	 * This makes it possible to use poll for events (EPOLLPRI | EPOLLERR)

	 * from userspace on sysfs file named <irq-nr>

 Prints to seq_file or log_buf */

				/*

				 * Error is not returned here since

				 * the output is wanted in any case

 Dump registers to kernel log */

 Default buf size in seq_read is not enough */

/*

 * Interrupt status

/*

 * - HWREG DB8500 formated routines

 Config 1 will allow APE side to read SIM registers */

/*

 * return length of an ASCII numerical value, 0 is string is not a

 * numerical value.

 * string shall start at value 1st char.

 * string can be tailed with \0 or space or newline chars only.

 * value can be decimal or hexadecimal (prefixed 0x or 0X).

/*

 * parse hwreg input data.

 * update global hwreg_cfg only if input data syntax is ok.

 default: invalid phys addr */

 default: invalid phys addr */

 default: 32bit access, hex output */

 default: no mask */

 default: no bit shift */

 read or write ? */

 OPTIONS -l|-w|-b -s -m -o */

 get arg BANK and ADDRESS */

 args are ok, update target cfg (mainly for read) */

 Get userspace string and assure termination */

 get args and process */

/*

 * - irq subscribe/unsubscribe stuff

/*

 * Userspace should use poll() on this file. When an event occur

 * the blocking poll will be released.

	/*

	 * This will create a sysfs file named <irq-nr> which userspace can

	 * use to select or poll and get the AB8500 events

 Set irq count to 0 when unsubscribe */

/*

 * - several debugfs nodes fops

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Toshiba T7L66XB core mfd support

 *

 * Copyright (c) 2005, 2007, 2008 Ian Molton

 * Copyright (c) 2008 Dmitry Baryshkov

 *

 * T7L66 features:

 *

 * Supported in this driver:

 * SD/MMC

 * SM/NAND flash controller

 *

 * As yet not supported

 * GPIO interface (on NAND pins)

 * Serial interface

 * TFT 'interface converter'

 * PCMCIA interface logic

 b Revision ID	*/

 b Interrupt Mask	*/

 b Device control	*/

 b Interrupt Status	*/

 b GPO output control	*/

 b GPO output enable	*/

 w GPI status		*/

 b Active pullup down ctrl */

 USB enable		*/

 MMC enable		*/

--------------------------------------------------------------------------*/

 Lock to protect registers requiring read/modify/write ops. */

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

 Handle the T7L66XB interrupt mux */

--------------------------------------------------------------------------*/

 Install the IRQ handler */

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

 Mask all interrupts */

--------------------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - 2016 Samsung Electronics Co., Ltd.

 *

 * Authors: Inha Song <ideal.song@samsung.com>

 *          Sylwester Nawrocki <s.nawrocki@samsung.com>

 *

 * Samsung Exynos SoC series Low Power Audio Subsystem driver.

 *

 * This module provides regmap for the Top SFR region and instantiates

 * devices for IP blocks like DMAC, I2S, UART.

 LPASS Top register definitions */

 pointer to the LPASS TOP regmap */

 Unmask SFR, DMA and I2S interrupt */

 Mask any unmasked IP interrupt sources */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8350-core.c  --  Device access for Wolfson WM8350

 *

 * Copyright 2007, 2008 Wolfson Microelectronics PLC.

 *

 * Author: Liam Girdwood

 make sure we never pull up and down at the same time */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  htc-i2cpld.c

 *  Chip driver for an unknown CPLD chip found on omap850 HTC devices like

 *  the HTC Wizard and HTC Herald.

 *  The cpld is located on the i2c bus and acts as an input/output GPIO

 *  extender.

 *

 *  Copyright (C) 2009 Cory Maccarrone <darkstar6262@gmail.com>

 *

 *  Based on work done in the linwizard project

 *  Copyright (C) 2008-2009 Angelo Arrifano <miknix@gmail.com>

 chip info */

 Output details */

 Input details */

	/*

	 * Work structure to allow for setting values outside of any

	 * possible interrupt context

 irq info */

 htcpld info */

/* There does not appear to be a way to proactively mask interrupts

 * on the htcpld chip itself.  So, we simply ignore interrupts that

 We only allow edge triggering */

/* To properly dispatch IRQ events, we need to read from the

 * chip.  This is an I2C action that could possibly sleep

 * (which is bad in interrupt context) -- so we use a threaded

 * interrupt handler to get around that.

	/*

	 * For each chip, do a read of the chip and trigger any interrupts

	 * desired.  The interrupts will be triggered from LSB to MSB (i.e.

	 * bit 0 first, then bit 1, etc.)

	 *

	 * For chips that have no interrupt range specified, just skip 'em.

 Scan the chip */

 Throw a warning and skip this chip */

 Save away the old value so we can compare it */

 Write the new value */

		/*

		 * For each bit in the data (starting at bit 0), trigger

		 * associated interrupts.

			/* Run the IRQ handler, but only if the bit value

	/*

	 * In order to continue receiving interrupts, the int_reset_gpio must

	 * be asserted.

/*

 * The GPIO set routines can be called from interrupt context, especially if,

 * for example they're attached to the led-gpio framework and a trigger is

 * enabled.  As such, we declared work above in the htcpld_chip structure,

 * and that work is scheduled in the set routine.  The kernel can then run

 * the I2C functions, which will sleep, in process context.

	/*

	 * No-op: this function can only be called on the input chip.

	 * We do however make sure the offset is within range.

 Get the platform and driver data */

 Setup irq handlers */

 Get the platform and driver data */

 Eek, no such I2C adapter!  Bail out. */

 Add the I2C device.  This calls the probe() function. */

 I2C device registration failed, contineu with the next */

 Reset the chip */

 Get the platform and driver data */

 Get the platform and driver data */

 Setup the GPIO chips */

 Add the GPIO chips */

 Get the platform and driver data */

 Setup each chip's output GPIOs */

 Add the chips as best we can */

 Setup the HTCPLD chips */

 Setup the interrupts for the chip */

 Register the chip with I2C */

 Register the chips with the GPIO subsystem */

 Unregister the chip from i2c and continue */

 Find chained irq */

 Setup the chained interrupt handler */

 Set the driver data */

 Setup the htcpld chips */

 Request the GPIO(s) for the int reset and set them up */

			/*

			 * If it failed, that sucks, but we can probably

			 * continue on without it.

			/*

			 * If it failed, that sucks, but we can probably

			 * continue on without it.

 The I2C Driver -- used internally */

 The Core Driver */

 Register the I2C Chip driver */

 Probe for our chips */

 SPDX-License-Identifier: GPL-2.0

/*

 * SGI IOC3 multifunction device driver

 *

 * Copyright (C) 2018, 2019 Thomas Bogendoerfer <tbogendoerfer@suse.de>

 *

 * Based on work by:

 *   Stanislaw Skowronek <skylark@unaligned.org>

 *   Joshua Kinard <kumba@gentoo.org>

 *   Brent Casavant <bcasavan@sgi.com> - IOC4 master driver

 *   Pat Gefre <pfg@sgi.com> - IOC3 serial port IRQ demuxer

 Bitmask for selecting which IRQs are level triggered */

 size of m48t35 registers */

 1.2 us latency timer (40 cycles at 33 MHz) */

 Set level IRQs for every interrupt contained in IOC3_LVL_MASK */

 Mask off not enabled interrupts */

/*

 * System boards/BaseIOs use more interrupt pins of the bridge ASIC

 * to which the IOC3 is connected. Since the IOC3 MFD driver

 * knows wiring of these extra pins, we use the map_irq function

 * to get interrupts activated

 Set gpio pins for RS232/RS422 mode selection */

 Select RS232 mode for uart a */

 Select RS232 mode for uart b */

 Switch both ports to 16650 mode */

 Wait until mode switch is done */

 Enable One-Wire bus */

 Generate unique identifier */

 Helper macro for filling ioc3_info array */

 Clear IRQs */

 Read subsystem vendor id and subsystem id */

 Treat everything not identified by PCI subid as CAD DUO */

 Set up per-IOC3 data */

	/*

	 * Map all IOC3 registers.  These are shared between subdevices

	 * so the main IOC3 module manages them.

 Track PCI-device specific data */

 Remove all already added MFD devices */

 Clear and disable all IRQs */

 Release resources */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Core, IRQ and I2C device driver for DA9061 and DA9062 PMICs

 * Copyright (C) 2015-2017  Dialog Semiconductor

 EVENT A */

 EVENT B */

 EVENT C */

 EVENT A */

 EVENT B */

 EVENT C */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/drivers/mfd/aat2870-core.c

 *

 * Copyright (c) 2011, NVIDIA Corporation.

 * Author: Jin Park <jinyoungp@nvidia.com>

 readable, writeable, value */

 0x00 AAT2870_BL_CH_EN */

 0x01 AAT2870_BLM */

 0x02 AAT2870_BLS */

 0x03 AAT2870_BL1 */

 0x04 AAT2870_BL2 */

 0x05 AAT2870_BL3 */

 0x06 AAT2870_BL4 */

 0x07 AAT2870_BL5 */

 0x08 AAT2870_BL6 */

 0x09 AAT2870_BL7 */

 0x0A AAT2870_BL8 */

 0x0B AAT2870_FLR */

 0x0C AAT2870_FM */

 0x0D AAT2870_FS */

 0x0E AAT2870_ALS_CFG0 */

 0x0F AAT2870_ALS_CFG1 */

 0x10 AAT2870_ALS_CFG2 */

 0x11 AAT2870_AMB */

 0x12 AAT2870_ALS0 */

 0x13 AAT2870_ALS1 */

 0x14 AAT2870_ALS2 */

 0x15 AAT2870_ALS3 */

 0x16 AAT2870_ALS4 */

 0x17 AAT2870_ALS5 */

 0x18 AAT2870_ALS6 */

 0x19 AAT2870_ALS7 */

 0x1A AAT2870_ALS8 */

 0x1B AAT2870_ALS9 */

 0x1C AAT2870_ALSA */

 0x1D AAT2870_ALSB */

 0x1E AAT2870_ALSC */

 0x1F AAT2870_ALSD */

 0x20 AAT2870_ALSE */

 0x21 AAT2870_ALSF */

 0x22 AAT2870_SUB_SET */

 0x23 AAT2870_SUB_CTRL */

 0x24 AAT2870_LDO_AB */

 0x25 AAT2870_LDO_CD */

 0x26 AAT2870_LDO_EN */

 Truncate count; min() would cause a warning */

 CONFIG_DEBUG_FS */

 restore registers */

 CONFIG_PM_SLEEP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * wm8998-tables.c  --  data tables for wm8998-class codecs

 *

 * Copyright 2014 Wolfson Microelectronics plc

 *

 * Author: Richard Fitzgerald <rf@opensource.wolfsonmicro.com>

 We use a function so we can use ARRAY_SIZE() */

 R9     - Ctrl IF I2C1 CFG 1 */

 R11    - Ctrl IF I2C1 CFG 2 */

 R32    - Tone Generator 1 */

 R33    - Tone Generator 2 */

 R34    - Tone Generator 3 */

 R35    - Tone Generator 4 */

 R36    - Tone Generator 5 */

 R48    - PWM Drive 1 */

 R49    - PWM Drive 2 */

 R50    - PWM Drive 3 */

 R64    - Wake control */

 R65    - Sequence control */

 R97    - Sample Rate Sequence Select 1 */

 R98    - Sample Rate Sequence Select 2 */

 R99    - Sample Rate Sequence Select 3 */

 R100   - Sample Rate Sequence Select 4 */

 R102   - Always On Triggers Sequence Select 1 */

 R103   - Always On Triggers Sequence Select 2 */

 R104   - Always On Triggers Sequence Select 3 */

 R105   - Always On Triggers Sequence Select 4 */

 R106   - Always On Triggers Sequence Select 5 */

 R107   - Always On Triggers Sequence Select 6 */

 R144   - Haptics Control 1 */

 R145   - Haptics Control 2 */

 R146   - Haptics phase 1 intensity */

 R147   - Haptics phase 1 duration */

 R148   - Haptics phase 2 intensity */

 R149   - Haptics phase 2 duration */

 R150   - Haptics phase 3 intensity */

 R151   - Haptics phase 3 duration */

 R256   - Clock 32k 1 */

 R257   - System Clock 1 */

 R258   - Sample rate 1 */

 R259   - Sample rate 2 */

 R260   - Sample rate 3 */

 R274   - Async clock 1 */

 R275   - Async sample rate 1 */

 R276   - Async sample rate 2 */

 R329   - Output system clock */

 R330   - Output async clock */

 R338   - Rate Estimator 1 */

 R339   - Rate Estimator 2 */

 R340   - Rate Estimator 3 */

 R341   - Rate Estimator 4 */

 R342   - Rate Estimator 5 */

 R353   - Dynamic Frequency Scaling 1 */

 R369   - FLL1 Control 1 */

 R370   - FLL1 Control 2 */

 R371   - FLL1 Control 3 */

 R372   - FLL1 Control 4 */

 R373   - FLL1 Control 5 */

 R374   - FLL1 Control 6 */

 R377   - FLL1 Control 7 */

 R385   - FLL1 Synchroniser 1 */

 R386   - FLL1 Synchroniser 2 */

 R387   - FLL1 Synchroniser 3 */

 R388   - FLL1 Synchroniser 4 */

 R389   - FLL1 Synchroniser 5 */

 R390   - FLL1 Synchroniser 6 */

 R391   - FLL1 Synchroniser 7 */

 R393   - FLL1 Spread Spectrum */

 R394   - FLL1 GPIO Clock */

 R401   - FLL2 Control 1 */

 R402   - FLL2 Control 2 */

 R403   - FLL2 Control 3 */

 R404   - FLL2 Control 4 */

 R405   - FLL2 Control 5 */

 R406   - FLL2 Control 6 */

 R409   - FLL2 Control 7 */

 R417   - FLL2 Synchroniser 1 */

 R418   - FLL2 Synchroniser 2 */

 R419   - FLL2 Synchroniser 3 */

 R420   - FLL2 Synchroniser 4 */

 R421   - FLL2 Synchroniser 5 */

 R422   - FLL2 Synchroniser 6 */

 R423   - FLL2 Synchroniser 7 */

 R425   - FLL2 Spread Spectrum */

 R426   - FLL2 GPIO Clock */

 R512   - Mic Charge Pump 1 */

 R528   - LDO1 Control 1 */

 R530   - LDO1 Control 2 */

 R531   - LDO2 Control 1 */

 R536   - Mic Bias Ctrl 1 */

 R537   - Mic Bias Ctrl 2 */

 R538   - Mic Bias Ctrl 3 */

 R659   - Accessory Detect Mode 1 */

 R667   - Headphone Detect 1 */

 R674   - Micd Clamp control */

 R675   - Mic Detect 1 */

 R676   - Mic Detect 2 */

 R678   - Mic Detect Level 1 */

 R679   - Mic Detect Level 2 */

 R680   - Mic Detect Level 3 */

 R681   - Mic Detect Level 4 */

 R715   - Isolation control */

 R723   - Jack detect analogue */

 R768   - Input Enables */

 R776   - Input Rate */

 R777   - Input Volume Ramp */

 R780   - HPF Control */

 R784   - IN1L Control */

 R785   - ADC Digital Volume 1L */

 R786   - DMIC1L Control */

 R788   - IN1R Control */

 R789   - ADC Digital Volume 1R */

 R790   - DMIC1R Control */

 R792   - IN2L Control */

 R793   - ADC Digital Volume 2L */

 R794   - DMIC2L Control */

 R1024  - Output Enables 1 */

 R1032  - Output Rate 1 */

 R1033  - Output Volume Ramp */

 R1040  - Output Path Config 1L */

 R1041  - DAC Digital Volume 1L */

 R1043  - Noise Gate Select 1L */

 R1044  - Output Path Config 1R */

 R1045  - DAC Digital Volume 1R */

 R1047  - Noise Gate Select 1R */

 R1048  - Output Path Config 2L */

 R1049  - DAC Digital Volume 2L */

 R1051  - Noise Gate Select 2L */

 R1052  - Output Path Config 2R */

 R1053  - DAC Digital Volume 2R */

 R1055  - Noise Gate Select 2R */

 R1056  - Output Path Config 3L */

 R1057  - DAC Digital Volume 3L */

 R1059  - Noise Gate Select 3L */

 R1064  - Output Path Config 4L */

 R1065  - DAC Digital Volume 4L */

 R1067  - Noise Gate Select 4L */

 R1068  - Output Path Config 4R */

 R1069  - DAC Digital Volume 4R */

 R1071  - Noise Gate Select 4R */

 R1072  - Output Path Config 5L */

 R1073  - DAC Digital Volume 5L */

 R1075  - Noise Gate Select 5L */

 R1076  - Output Path Config 5R */

 R1077  - DAC Digital Volume 5R */

 R1079  - Noise Gate Select 5R */

 R1088  - DRE Enable */

 R1089  - DRE Control 1 */

 R1089  - DRE Control 2 */

 R1089  - DRE Control 3 */

 R1096  - EDRE Enable */

 R1104  - DAC AEC Control 1 */

 R1105  - DAC AEC Control 2 */

 R1112  - Noise Gate Control */

 R1168  - PDM SPK1 CTRL 1 */

 R1169  - PDM SPK1 CTRL 2 */

 R1178  - HP_TEST_CTRL_13 */

 R1280  - AIF1 BCLK Ctrl */

 R1281  - AIF1 Tx Pin Ctrl */

 R1282  - AIF1 Rx Pin Ctrl */

 R1283  - AIF1 Rate Ctrl */

 R1284  - AIF1 Format */

 R1286  - AIF1 Rx BCLK Rate */

 R1287  - AIF1 Frame Ctrl 1 */

 R1288  - AIF1 Frame Ctrl 2 */

 R1289  - AIF1 Frame Ctrl 3 */

 R1290  - AIF1 Frame Ctrl 4 */

 R1291  - AIF1 Frame Ctrl 5 */

 R1292  - AIF1 Frame Ctrl 6 */

 R1293  - AIF1 Frame Ctrl 7 */

 R1294  - AIF1 Frame Ctrl 8 */

 R1297  - AIF1 Frame Ctrl 11 */

 R1298  - AIF1 Frame Ctrl 12 */

 R1299  - AIF1 Frame Ctrl 13 */

 R1300  - AIF1 Frame Ctrl 14 */

 R1301  - AIF1 Frame Ctrl 15 */

 R1302  - AIF1 Frame Ctrl 16 */

 R1305  - AIF1 Tx Enables */

 R1306  - AIF1 Rx Enables */

 R1344  - AIF2 BCLK Ctrl */

 R1345  - AIF2 Tx Pin Ctrl */

 R1346  - AIF2 Rx Pin Ctrl */

 R1347  - AIF2 Rate Ctrl */

 R1348  - AIF2 Format */

 R1350  - AIF2 Rx BCLK Rate */

 R1351  - AIF2 Frame Ctrl 1 */

 R1352  - AIF2 Frame Ctrl 2 */

 R1353  - AIF2 Frame Ctrl 3 */

 R1354  - AIF2 Frame Ctrl 4 */

 R1355  - AIF2 Frame Ctrl 5 */

 R1356  - AIF2 Frame Ctrl 6 */

 R1357  - AIF2 Frame Ctrl 7 */

 R1358  - AIF2 Frame Ctrl 8 */

 R1361  - AIF2 Frame Ctrl 11 */

 R1362  - AIF2 Frame Ctrl 12 */

 R1363  - AIF2 Frame Ctrl 13 */

 R1364  - AIF2 Frame Ctrl 14 */

 R1365  - AIF2 Frame Ctrl 15 */

 R1366  - AIF2 Frame Ctrl 16 */

 R1369  - AIF2 Tx Enables */

 R1370  - AIF2 Rx Enables */

 R1408  - AIF3 BCLK Ctrl */

 R1409  - AIF3 Tx Pin Ctrl */

 R1410  - AIF3 Rx Pin Ctrl */

 R1411  - AIF3 Rate Ctrl */

 R1412  - AIF3 Format */

 R1414  - AIF3 Rx BCLK Rate */

 R1415  - AIF3 Frame Ctrl 1 */

 R1416  - AIF3 Frame Ctrl 2 */

 R1417  - AIF3 Frame Ctrl 3 */

 R1418  - AIF3 Frame Ctrl 4 */

 R1425  - AIF3 Frame Ctrl 11 */

 R1426  - AIF3 Frame Ctrl 12 */

 R1433  - AIF3 Tx Enables */

 R1434  - AIF3 Rx Enables */

 R1474  - SPD1 TX Control */

 R1475  - SPD1 TX Channel Status 1 */

 R1476  - SPD1 TX Channel Status 2 */

 R1477  - SPD1 TX Channel Status 3 */

 R1507  - SLIMbus Framer Ref Gear */

 R1509  - SLIMbus Rates 1 */

 R1510  - SLIMbus Rates 2 */

 R1513  - SLIMbus Rates 5 */

 R1514  - SLIMbus Rates 6 */

 R1515  - SLIMbus Rates 7 */

 R1525  - SLIMbus RX Channel Enable */

 R1526  - SLIMbus TX Channel Enable */

 R1600  - PWM1MIX Input 1 Source */

 R1601  - PWM1MIX Input 1 Volume */

 R1602  - PWM1MIX Input 2 Source */

 R1603  - PWM1MIX Input 2 Volume */

 R1604  - PWM1MIX Input 3 Source */

 R1605  - PWM1MIX Input 3 Volume */

 R1606  - PWM1MIX Input 4 Source */

 R1607  - PWM1MIX Input 4 Volume */

 R1608  - PWM2MIX Input 1 Source */

 R1609  - PWM2MIX Input 1 Volume */

 R1610  - PWM2MIX Input 2 Source */

 R1611  - PWM2MIX Input 2 Volume */

 R1612  - PWM2MIX Input 3 Source */

 R1613  - PWM2MIX Input 3 Volume */

 R1614  - PWM2MIX Input 4 Source */

 R1615  - PWM2MIX Input 4 Volume */

 R1664  - OUT1LMIX Input 1 Source */

 R1665  - OUT1LMIX Input 1 Volume */

 R1666  - OUT1LMIX Input 2 Source */

 R1667  - OUT1LMIX Input 2 Volume */

 R1668  - OUT1LMIX Input 3 Source */

 R1669  - OUT1LMIX Input 3 Volume */

 R1670  - OUT1LMIX Input 4 Source */

 R1671  - OUT1LMIX Input 4 Volume */

 R1672  - OUT1RMIX Input 1 Source */

 R1673  - OUT1RMIX Input 1 Volume */

 R1674  - OUT1RMIX Input 2 Source */

 R1675  - OUT1RMIX Input 2 Volume */

 R1676  - OUT1RMIX Input 3 Source */

 R1677  - OUT1RMIX Input 3 Volume */

 R1678  - OUT1RMIX Input 4 Source */

 R1679  - OUT1RMIX Input 4 Volume */

 R1680  - OUT2LMIX Input 1 Source */

 R1681  - OUT2LMIX Input 1 Volume */

 R1682  - OUT2LMIX Input 2 Source */

 R1683  - OUT2LMIX Input 2 Volume */

 R1684  - OUT2LMIX Input 3 Source */

 R1685  - OUT2LMIX Input 3 Volume */

 R1686  - OUT2LMIX Input 4 Source */

 R1687  - OUT2LMIX Input 4 Volume */

 R1688  - OUT2RMIX Input 1 Source */

 R1689  - OUT2RMIX Input 1 Volume */

 R1690  - OUT2RMIX Input 2 Source */

 R1691  - OUT2RMIX Input 2 Volume */

 R1692  - OUT2RMIX Input 3 Source */

 R1693  - OUT2RMIX Input 3 Volume */

 R1694  - OUT2RMIX Input 4 Source */

 R1695  - OUT2RMIX Input 4 Volume */

 R1696  - OUT3LMIX Input 1 Source */

 R1697  - OUT3LMIX Input 1 Volume */

 R1698  - OUT3LMIX Input 2 Source */

 R1699  - OUT3LMIX Input 2 Volume */

 R1700  - OUT3LMIX Input 3 Source */

 R1701  - OUT3LMIX Input 3 Volume */

 R1702  - OUT3LMIX Input 4 Source */

 R1703  - OUT3LMIX Input 4 Volume */

 R1712  - OUT4LMIX Input 1 Source */

 R1713  - OUT4LMIX Input 1 Volume */

 R1714  - OUT4LMIX Input 2 Source */

 R1715  - OUT4LMIX Input 2 Volume */

 R1716  - OUT4LMIX Input 3 Source */

 R1717  - OUT4LMIX Input 3 Volume */

 R1718  - OUT4LMIX Input 4 Source */

 R1719  - OUT4LMIX Input 4 Volume */

 R1720  - OUT4RMIX Input 1 Source */

 R1721  - OUT4RMIX Input 1 Volume */

 R1722  - OUT4RMIX Input 2 Source */

 R1723  - OUT4RMIX Input 2 Volume */

 R1724  - OUT4RMIX Input 3 Source */

 R1725  - OUT4RMIX Input 3 Volume */

 R1726  - OUT4RMIX Input 4 Source */

 R1727  - OUT4RMIX Input 4 Volume */

 R1728  - OUT5LMIX Input 1 Source */

 R1729  - OUT5LMIX Input 1 Volume */

 R1730  - OUT5LMIX Input 2 Source */

 R1731  - OUT5LMIX Input 2 Volume */

 R1732  - OUT5LMIX Input 3 Source */

 R1733  - OUT5LMIX Input 3 Volume */

 R1734  - OUT5LMIX Input 4 Source */

 R1735  - OUT5LMIX Input 4 Volume */

 R1736  - OUT5RMIX Input 1 Source */

 R1737  - OUT5RMIX Input 1 Volume */

 R1738  - OUT5RMIX Input 2 Source */

 R1739  - OUT5RMIX Input 2 Volume */

 R1740  - OUT5RMIX Input 3 Source */

 R1741  - OUT5RMIX Input 3 Volume */

 R1742  - OUT5RMIX Input 4 Source */

 R1743  - OUT5RMIX Input 4 Volume */

 R1792  - AIF1TX1MIX Input 1 Source */

 R1793  - AIF1TX1MIX Input 1 Volume */

 R1794  - AIF1TX1MIX Input 2 Source */

 R1795  - AIF1TX1MIX Input 2 Volume */

 R1796  - AIF1TX1MIX Input 3 Source */

 R1797  - AIF1TX1MIX Input 3 Volume */

 R1798  - AIF1TX1MIX Input 4 Source */

 R1799  - AIF1TX1MIX Input 4 Volume */

 R1800  - AIF1TX2MIX Input 1 Source */

 R1801  - AIF1TX2MIX Input 1 Volume */

 R1802  - AIF1TX2MIX Input 2 Source */

 R1803  - AIF1TX2MIX Input 2 Volume */

 R1804  - AIF1TX2MIX Input 3 Source */

 R1805  - AIF1TX2MIX Input 3 Volume */

 R1806  - AIF1TX2MIX Input 4 Source */

 R1807  - AIF1TX2MIX Input 4 Volume */

 R1808  - AIF1TX3MIX Input 1 Source */

 R1809  - AIF1TX3MIX Input 1 Volume */

 R1810  - AIF1TX3MIX Input 2 Source */

 R1811  - AIF1TX3MIX Input 2 Volume */

 R1812  - AIF1TX3MIX Input 3 Source */

 R1813  - AIF1TX3MIX Input 3 Volume */

 R1814  - AIF1TX3MIX Input 4 Source */

 R1815  - AIF1TX3MIX Input 4 Volume */

 R1816  - AIF1TX4MIX Input 1 Source */

 R1817  - AIF1TX4MIX Input 1 Volume */

 R1818  - AIF1TX4MIX Input 2 Source */

 R1819  - AIF1TX4MIX Input 2 Volume */

 R1820  - AIF1TX4MIX Input 3 Source */

 R1821  - AIF1TX4MIX Input 3 Volume */

 R1822  - AIF1TX4MIX Input 4 Source */

 R1823  - AIF1TX4MIX Input 4 Volume */

 R1824  - AIF1TX5MIX Input 1 Source */

 R1825  - AIF1TX5MIX Input 1 Volume */

 R1826  - AIF1TX5MIX Input 2 Source */

 R1827  - AIF1TX5MIX Input 2 Volume */

 R1828  - AIF1TX5MIX Input 3 Source */

 R1829  - AIF1TX5MIX Input 3 Volume */

 R1830  - AIF1TX5MIX Input 4 Source */

 R1831  - AIF1TX5MIX Input 4 Volume */

 R1832  - AIF1TX6MIX Input 1 Source */

 R1833  - AIF1TX6MIX Input 1 Volume */

 R1834  - AIF1TX6MIX Input 2 Source */

 R1835  - AIF1TX6MIX Input 2 Volume */

 R1836  - AIF1TX6MIX Input 3 Source */

 R1837  - AIF1TX6MIX Input 3 Volume */

 R1838  - AIF1TX6MIX Input 4 Source */

 R1839  - AIF1TX6MIX Input 4 Volume */

 R1856  - AIF2TX1MIX Input 1 Source */

 R1857  - AIF2TX1MIX Input 1 Volume */

 R1858  - AIF2TX1MIX Input 2 Source */

 R1859  - AIF2TX1MIX Input 2 Volume */

 R1860  - AIF2TX1MIX Input 3 Source */

 R1861  - AIF2TX1MIX Input 3 Volume */

 R1862  - AIF2TX1MIX Input 4 Source */

 R1863  - AIF2TX1MIX Input 4 Volume */

 R1864  - AIF2TX2MIX Input 1 Source */

 R1865  - AIF2TX2MIX Input 1 Volume */

 R1866  - AIF2TX2MIX Input 2 Source */

 R1867  - AIF2TX2MIX Input 2 Volume */

 R1868  - AIF2TX2MIX Input 3 Source */

 R1869  - AIF2TX2MIX Input 3 Volume */

 R1870  - AIF2TX2MIX Input 4 Source */

 R1871  - AIF2TX2MIX Input 4 Volume */

 R1872  - AIF2TX3MIX Input 1 Source */

 R1873  - AIF2TX3MIX Input 1 Volume */

 R1874  - AIF2TX3MIX Input 2 Source */

 R1875  - AIF2TX3MIX Input 2 Volume */

 R1876  - AIF2TX3MIX Input 3 Source */

 R1877  - AIF2TX3MIX Input 3 Volume */

 R1878  - AIF2TX3MIX Input 4 Source */

 R1879  - AIF2TX3MIX Input 4 Volume */

 R1880  - AIF2TX4MIX Input 1 Source */

 R1881  - AIF2TX4MIX Input 1 Volume */

 R1882  - AIF2TX4MIX Input 2 Source */

 R1883  - AIF2TX4MIX Input 2 Volume */

 R1884  - AIF2TX4MIX Input 3 Source */

 R1885  - AIF2TX4MIX Input 3 Volume */

 R1886  - AIF2TX4MIX Input 4 Source */

 R1887  - AIF2TX4MIX Input 4 Volume */

 R1888  - AIF2TX5MIX Input 1 Source */

 R1889  - AIF2TX5MIX Input 1 Volume */

 R1890  - AIF2TX5MIX Input 2 Source */

 R1891  - AIF2TX5MIX Input 2 Volume */

 R1892  - AIF2TX5MIX Input 3 Source */

 R1893  - AIF2TX5MIX Input 3 Volume */

 R1894  - AIF2TX5MIX Input 4 Source */

 R1895  - AIF2TX5MIX Input 4 Volume */

 R1896  - AIF2TX6MIX Input 1 Source */

 R1897  - AIF2TX6MIX Input 1 Volume */

 R1898  - AIF2TX6MIX Input 2 Source */

 R1899  - AIF2TX6MIX Input 2 Volume */

 R1900  - AIF2TX6MIX Input 3 Source */

 R1901  - AIF2TX6MIX Input 3 Volume */

 R1902  - AIF2TX6MIX Input 4 Source */

 R1903  - AIF2TX6MIX Input 4 Volume */

 R1920  - AIF3TX1MIX Input 1 Source */

 R1921  - AIF3TX1MIX Input 1 Volume */

 R1922  - AIF3TX1MIX Input 2 Source */

 R1923  - AIF3TX1MIX Input 2 Volume */

 R1924  - AIF3TX1MIX Input 3 Source */

 R1925  - AIF3TX1MIX Input 3 Volume */

 R1926  - AIF3TX1MIX Input 4 Source */

 R1927  - AIF3TX1MIX Input 4 Volume */

 R1928  - AIF3TX2MIX Input 1 Source */

 R1929  - AIF3TX2MIX Input 1 Volume */

 R1930  - AIF3TX2MIX Input 2 Source */

 R1931  - AIF3TX2MIX Input 2 Volume */

 R1932  - AIF3TX2MIX Input 3 Source */

 R1933  - AIF3TX2MIX Input 3 Volume */

 R1934  - AIF3TX2MIX Input 4 Source */

 R1935  - AIF3TX2MIX Input 4 Volume */

 R1984  - SLIMTX1MIX Input 1 Source */

 R1985  - SLIMTX1MIX Input 1 Volume */

 R1992  - SLIMTX2MIX Input 1 Source */

 R1993  - SLIMTX2MIX Input 1 Volume */

 R2000  - SLIMTX3MIX Input 1 Source */

 R2001  - SLIMTX3MIX Input 1 Volume */

 R2008  - SLIMTX4MIX Input 1 Source */

 R2009  - SLIMTX4MIX Input 1 Volume */

 R2016  - SLIMTX5MIX Input 1 Source */

 R2017  - SLIMTX5MIX Input 1 Volume */

 R2024  - SLIMTX6MIX Input 1 Source */

 R2025  - SLIMTX6MIX Input 1 Volume */

 R2048  - SPDIF1TX1MIX Input 1 Source */

 R2049  - SPDIF1TX1MIX Input 1 Volume */

 R2056  - SPDIF1TX2MIX Input 1 Source */

 R2057  - SPDIF1TX2MIX Input 1 Volume */

 R2176  - EQ1MIX Input 1 Source */

 R2177  - EQ1MIX Input 1 Volume */

 R2184  - EQ2MIX Input 1 Source */

 R2185  - EQ2MIX Input 1 Volume */

 R2192  - EQ3MIX Input 1 Source */

 R2193  - EQ3MIX Input 1 Volume */

 R2200  - EQ4MIX Input 1 Source */

 R2201  - EQ4MIX Input 1 Volume */

 R2240  - DRC1LMIX Input 1 Source */

 R2241  - DRC1LMIX Input 1 Volume */

 R2248  - DRC1RMIX Input 1 Source */

 R2249  - DRC1RMIX Input 1 Volume */

 R2304  - HPLP1MIX Input 1 Source */

 R2305  - HPLP1MIX Input 1 Volume */

 R2306  - HPLP1MIX Input 2 Source */

 R2307  - HPLP1MIX Input 2 Volume */

 R2308  - HPLP1MIX Input 3 Source */

 R2309  - HPLP1MIX Input 3 Volume */

 R2310  - HPLP1MIX Input 4 Source */

 R2311  - HPLP1MIX Input 4 Volume */

 R2312  - HPLP2MIX Input 1 Source */

 R2313  - HPLP2MIX Input 1 Volume */

 R2314  - HPLP2MIX Input 2 Source */

 R2315  - HPLP2MIX Input 2 Volume */

 R2316  - HPLP2MIX Input 3 Source */

 R2317  - HPLP2MIX Input 3 Volume */

 R2318  - HPLP2MIX Input 4 Source */

 R2319  - HPLP2MIX Input 4 Volume */

 R2320  - HPLP3MIX Input 1 Source */

 R2321  - HPLP3MIX Input 1 Volume */

 R2322  - HPLP3MIX Input 2 Source */

 R2323  - HPLP3MIX Input 2 Volume */

 R2324  - HPLP3MIX Input 3 Source */

 R2325  - HPLP3MIX Input 3 Volume */

 R2326  - HPLP3MIX Input 4 Source */

 R2327  - HPLP3MIX Input 4 Volume */

 R2328  - HPLP4MIX Input 1 Source */

 R2329  - HPLP4MIX Input 1 Volume */

 R2330  - HPLP4MIX Input 2 Source */

 R2331  - HPLP4MIX Input 2 Volume */

 R2332  - HPLP4MIX Input 3 Source */

 R2333  - HPLP4MIX Input 3 Volume */

 R2334  - HPLP4MIX Input 4 Source */

 R2335  - HPLP4MIX Input 4 Volume */

 R2688  - ASRC1LMIX Input 1 Source */

 R2696  - ASRC1RMIX Input 1 Source */

 R2704  - ASRC2LMIX Input 1 Source */

 R2712  - ASRC2RMIX Input 1 Source */

 R2816  - ISRC1DEC1MIX Input 1 Source */

 R2824  - ISRC1DEC2MIX Input 1 Source */

 R2832  - ISRC1DEC3MIX Input 1 Source */

 R2840  - ISRC1DEC4MIX Input 1 Source */

 R2848  - ISRC1INT1MIX Input 1 Source */

 R2856  - ISRC1INT2MIX Input 1 Source */

 R2864  - ISRC1INT3MIX Input 1 Source */

 R2872  - ISRC1INT4MIX Input 1 Source */

 R2880  - ISRC2DEC1MIX Input 1 Source */

 R2888  - ISRC2DEC2MIX Input 1 Source */

 R2912  - ISRC2INT1MIX Input 1 Source */

 R2920  - ISRC2INT2MIX Input 1 Source */

 R3072  - GPIO1 CTRL */

 R3073  - GPIO2 CTRL */

 R3074  - GPIO3 CTRL */

 R3075  - GPIO4 CTRL */

 R3076  - GPIO5 CTRL */

 R3087  - IRQ CTRL 1 */

 R3088  - GPIO Debounce Config */

 R3096  - GP Switch 1 */

 R3104  - Misc Pad Ctrl 1 */

 R3105  - Misc Pad Ctrl 2 */

 R3106  - Misc Pad Ctrl 3 */

 R3107  - Misc Pad Ctrl 4 */

 R3108  - Misc Pad Ctrl 5 */

 R3109  - Misc Pad Ctrl 6 */

 R3336  - Interrupt Status 1 Mask */

 R3337  - Interrupt Status 2 Mask */

 R3338  - Interrupt Status 3 Mask */

 R3339  - Interrupt Status 4 Mask */

 R3340  - Interrupt Status 5 Mask */

 R3343  - Interrupt Control */

 R3352  - IRQ2 Status 1 Mask */

 R3353  - IRQ2 Status 2 Mask */

 R3354  - IRQ2 Status 3 Mask */

 R3355  - IRQ2 Status 4 Mask */

 R3356  - IRQ2 Status 5 Mask */

 R3359  - IRQ2 Control */

 R3411  - AOD IRQ Mask IRQ1 */

 R3412  - AOD IRQ Mask IRQ2 */

 R3414  - Jack detect debounce */

 R3584  - FX_Ctrl1 */

 R3600  - EQ1_1 */

 R3601  - EQ1_2 */

 R3602  - EQ1_3 */

 R3603  - EQ1_4 */

 R3604  - EQ1_5 */

 R3605  - EQ1_6 */

 R3606  - EQ1_7 */

 R3607  - EQ1_8 */

 R3608  - EQ1_9 */

 R3609  - EQ1_10 */

 R3610  - EQ1_11 */

 R3611  - EQ1_12 */

 R3612  - EQ1_13 */

 R3613  - EQ1_14 */

 R3614  - EQ1_15 */

 R3615  - EQ1_16 */

 R3616  - EQ1_17 */

 R3617  - EQ1_18 */

 R3618  - EQ1_19 */

 R3619  - EQ1_20 */

 R3620  - EQ1_21 */

 R3622  - EQ2_1 */

 R3623  - EQ2_2 */

 R3624  - EQ2_3 */

 R3625  - EQ2_4 */

 R3626  - EQ2_5 */

 R3627  - EQ2_6 */

 R3628  - EQ2_7 */

 R3629  - EQ2_8 */

 R3630  - EQ2_9 */

 R3631  - EQ2_10 */

 R3632  - EQ2_11 */

 R3633  - EQ2_12 */

 R3634  - EQ2_13 */

 R3635  - EQ2_14 */

 R3636  - EQ2_15 */

 R3637  - EQ2_16 */

 R3638  - EQ2_17 */

 R3639  - EQ2_18 */

 R3640  - EQ2_19 */

 R3641  - EQ2_20 */

 R3642  - EQ2_21 */

 R3644  - EQ3_1 */

 R3645  - EQ3_2 */

 R3646  - EQ3_3 */

 R3647  - EQ3_4 */

 R3648  - EQ3_5 */

 R3649  - EQ3_6 */

 R3650  - EQ3_7 */

 R3651  - EQ3_8 */

 R3652  - EQ3_9 */

 R3653  - EQ3_10 */

 R3654  - EQ3_11 */

 R3655  - EQ3_12 */

 R3656  - EQ3_13 */

 R3657  - EQ3_14 */

 R3658  - EQ3_15 */

 R3659  - EQ3_16 */

 R3660  - EQ3_17 */

 R3661  - EQ3_18 */

 R3662  - EQ3_19 */

 R3663  - EQ3_20 */

 R3664  - EQ3_21 */

 R3666  - EQ4_1 */

 R3667  - EQ4_2 */

 R3668  - EQ4_3 */

 R3669  - EQ4_4 */

 R3670  - EQ4_5 */

 R3671  - EQ4_6 */

 R3672  - EQ4_7 */

 R3673  - EQ4_8 */

 R3674  - EQ4_9 */

 R3675  - EQ4_10 */

 R3676  - EQ4_11 */

 R3677  - EQ4_12 */

 R3678  - EQ4_13 */

 R3679  - EQ4_14 */

 R3680  - EQ4_15 */

 R3681  - EQ4_16 */

 R3682  - EQ4_17 */

 R3683  - EQ4_18 */

 R3684  - EQ4_19 */

 R3685  - EQ4_20 */

 R3686  - EQ4_21 */

 R3712  - DRC1 ctrl1 */

 R3713  - DRC1 ctrl2 */

 R3714  - DRC1 ctrl3 */

 R3715  - DRC1 ctrl4 */

 R3716  - DRC1 ctrl5 */

 R3776  - HPLPF1_1 */

 R3777  - HPLPF1_2 */

 R3780  - HPLPF2_1 */

 R3781  - HPLPF2_2 */

 R3784  - HPLPF3_1 */

 R3785  - HPLPF3_2 */

 R3788  - HPLPF4_1 */

 R3789  - HPLPF4_2 */

 R3808  - ASRC_ENABLE */

 R3810  - ASRC_RATE1 */

 R3811  - ASRC_RATE2 */

 R3824  - ISRC 1 CTRL 1 */

 R3825  - ISRC 1 CTRL 2 */

 R3826  - ISRC 1 CTRL 3 */

 R3827  - ISRC 2 CTRL 1 */

 R3828  - ISRC 2 CTRL 2 */

 R3829  - ISRC 2 CTRL 3 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD core driver for Rockchip RK808/RK818

 *

 * Copyright (c) 2014, Fuzhou Rockchip Electronics Co., Ltd

 *

 * Author: Chris Zhong <zyw@rock-chips.com>

 * Author: Zhang Qing <zhangqing@rock-chips.com>

 *

 * Copyright (C) 2016 PHYTEC Messtechnik GmbH

 *

 * Author: Wadim Egorov <w.egorov@phytec.de>

	/*

	 * Notes:

	 * - Technically the ROUND_30s bit makes RTC_CTRL_REG volatile, but

	 *   we don't use that feature.  It's better to cache.

	 * - It's unlikely we care that RK808_DEVCTRL_REG is volatile since

	 *   bits are cleared in case when we shutoff anyway, but better safe.

	/*

	 * Notes:

	 * - Technically the ROUND_30s bit makes RTC_CTRL_REG volatile, but

	 *   we don't use that feature.  It's better to cache.

 Codec specific registers */

 from vendor driver, CODEC_AREF_RTCFG0 not defined in data sheet */

 from vendor driver, CODEC_AADC_CFG1 not defined in data sheet */

 from vendor driver, CODEC_ADAC_CFG0 not defined in data sheet */

 from vendor driver, CODEC_ADAC_CFG0 not defined in data sheet */

 improve efficiency */

 close charger when usb lower then 3.4V */

 no action when vref */

 enable HDMI 5V */

 INT_STS */

 INT_STS2 */

 INT_STS */

 INT_STS2 */

 Read chip variant */

	/**

	 * pm_power_off may points to a function from another module.

	 * Check if the pointer is set by us and only then overwrite it.

/*

 * Copyright (C) 2016 Texas Instruments Incorporated - https://www.ti.com/

 *

 * Author: Keerthy <j-keerthy@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 SPDX-License-Identifier: GPL-2.0+



 max8997-irq.c - Interrupt controller support for MAX8997



 Copyright (C) 2011 Samsung Electronics Co.Ltd

 MyungJoo Ham <myungjoo.ham@samsung.com>



 This driver is based on max8998-irq.c

 PMIC INT1 ~ INT4 */

		/*

		 * TODO: FUEL GAUGE

		 *

		 * This is to be supported by Max17042 driver. When

		 * an interrupt incurs here, it should be relayed to a

		 * Max17042 device that is connected (probably by

		 * platform-data). However, we do not have interrupt

		 * handling in Max17042 driver currently. The Max17042 IRQ

		 * driver should be ready to be used as a stand-alone device and

		 * a Max8997-dependent device. Because it is not ready in

		 * Max17042-side and it is not too critical in operating

		 * Max8997, we do not implement this in initial releases.

 MUIC INT1 ~ INT3 */

 GPIO Interrupt */

 Flash Status Interrupt */

 Apply masking */

 Report */

 Mask individual interrupt sources */

 SPDX-License-Identifier: GPL-2.0-or-later

 /* I2C access for DA9055 PMICs.

 *

 * Copyright(c) 2012 Dialog Semiconductor Ltd.

 *

 * Author: David Dajun Chen <dchen@diasemi.com>

/*

 * DO NOT change the device Ids. The naming is intentionally specific as both

 * the PMIC and CODEC parts of this chip are instantiated separately as I2C

 * devices (both have configurable I2C addresses, and are to all intents and

 * purposes separate). As a result there are specific DA9055 ids for PMIC

 * and CODEC, which must be different to operate together.

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2019, Linaro Limited

 Add 1msec delay for VOUT to settle */

	/*

	 * For WCD934X, it takes about 600us for the Vout_A and

	 * Vout_D to be ready after BUCK_SIDO is powered up.

	 * SYS_RST_N shouldn't be pulled high during this time

 SPDX-License-Identifier: GPL-2.0+

/*

 * Core driver for Renesas Synchronization Management Unit (SMU) devices.

 *

 * Copyright (C) 2021 Integrated Device Technology, Inc., a Renesas Company.

 SPDX-License-Identifier: GPL-2.0-or-later

/* NXP PCF50633 ADC Driver

 *

 * (C) 2006-2008 by Openmoko, Inc.

 * Author: Balaji Rao <balajirrao@openmoko.org>

 * All rights reserved.

 *

 * Broken down from monstrous PCF50633 driver mainly by

 * Harald Welte, Andy Green and Werner Almesberger

 *

 *  NOTE: This driver does not yet support subtractive ADC mode, which means

 *  you can do only one measurement per read request.

 Private stuff */

 kill ratiometric, but enable ACCSW biasing */

 start ADC conversion on selected channel */

 req is freed when the result is ready, in interrupt handler */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 Free Electrons

 * Copyright (C) 2014 Atmel

 *

 * Author: Boris BREZILLON <boris.brezillon@free-electrons.com>

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0+

/*

 * Device access for Dialog DA9063 modules

 *

 * Copyright 2012 Dialog Semiconductors Ltd.

 * Copyright 2013 Philipp Zabel, Pengutronix

 *

 * Author: Krystian Garbaciak, Dialog Semiconductor

 * Author: Michal Hajduk, Dialog Semiconductor

 *

 Only present on DA9063 , not on DA9063L */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Base driver for Maxim MAX8925

 *

 * Copyright (C) 2009-2010 Marvell International Ltd.

 *	Haojian Zhuang <haojian.zhuang@marvell.com>

 register in ADC component */

 register in RTC component */

 enable or not */

 bit offset in mask register */

 TSC IRQ should be serviced in max8925_tsc_irq() */

 non TSC IRQ should be serviced in max8925_irq() */

 Load cached value. In initial, all IRQs are masked */

 1 -- disable, 0 -- enable */

 update mask into registers */

 clear all interrupts */

 mask all interrupts except for TSC */

 request irq handler for pmic main irq*/

 request irq handler for pmic tsc irq*/

 mask TSC interrupt */

 enable ADC to control internal reference */

 enable internal reference for ADC */

 check for internal reference IRQ */

 enaable ADC scheduler, interval is 1 second */

 enable Momentary Power Loss */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Broadcom BCM590xx PMU

 *

 * Copyright 2014 Linaro Limited

 * Author: Matt Porter <mporter@linaro.org>

 Secondary I2C slave address is the base address with A(2) asserted */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * tps65910.c  --  TI TPS6591x chip family multi-function driver

 *

 * Copyright 2010 Texas Instruments Inc.

 *

 * Author: Graeme Gregory <gg@slimlogic.co.uk>

 * Author: Jorge Eduardo Candelaria <jedu@slimlogic.co.uk>

 INT_STS */

 INT_STS2 */

 INT_STS2 */

 INT_STS */

 INT_STS2 */

	/*

	 * Caching all regulator registers.

	 * All regualator register address range is same for

	 * TPS65910 and TPS65911

 Check for non-existing register */

 enabling SLEEP device state */

	/*

	 * The PWR_OFF bit needs to be set separately, before transitioning

	 * to the OFF state. It enables the "sequential" power-off mode on

	 * TPS65911, it's a NO-OP on TPS65910.

	/* Work around silicon erratum SWCZ010: the tps65910 may miss the

	 * first I2C transfer. So issue a dummy transfer before the first

	 * real transfer.

 init early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * System Control Driver

 *

 * Copyright (C) 2012 Freescale Semiconductor, Inc.

 * Copyright (C) 2012 Linaro Ltd.

 *

 * Author: Dong Aisheng <dong.aisheng@linaro.org>

 Parse the device's DT node for an endianness specification */

	/*

	 * search for reg-io-width property in DT. If it is not provided,

	 * default to 4 bytes. regmap_init_mmio will return an error if values

	 * are invalid so there is no need to check them here.

 Ignore missing hwlock, it's optional. */

 clock is optional */

/*

 * It behaves the same as syscon_regmap_lookup_by_phandle() except where

 * there is no regmap phandle. In this case, instead of returning -ENODEV,

 * the function returns NULL.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) ST-Ericsson SA 2010

 *

 * Author: Hanumath Prasad <hanumath.prasad@stericsson.com> for ST-Ericsson

 * Author: Rabin Vincent <rabin.vincent@stericsson.com> for ST-Ericsson

/*

 * enum tc3589x_version - indicates the TC3589x version

/**

 * tc3589x_reg_read() - read a single TC3589x register

 * @tc3589x:	Device to read from

 * @reg:	Register to read

/**

 * tc3589x_reg_write() - write a single TC3589x register

 * @tc3589x:	Device to write to

 * @reg:	Register to read

 * @data:	Value to write

/**

 * tc3589x_block_read() - read multiple TC3589x registers

 * @tc3589x:	Device to read from

 * @reg:	First register

 * @length:	Number of registers

 * @values:	Buffer to write to

/**

 * tc3589x_block_write() - write multiple TC3589x registers

 * @tc3589x:	Device to write to

 * @reg:	First register

 * @length:	Number of registers

 * @values:	Values to write

/**

 * tc3589x_set_bits() - set the value of a bitfield in a TC3589x register

 * @tc3589x:	Device to write to

 * @reg:	Register to write

 * @mask:	Mask of bits to set

 * @val:	Value to set

	/*

	 * A dummy read or write (to any register) appears to be necessary to

	 * have the last interrupt clear (for example, GPIO IC write) take

	 * effect. In such a case, recheck for any interrupt which is still

	 * pending.

	/*

	 * Put everything except the IRQ module into reset;

	 * also spare the GPIO module for any pin initialization

	 * done during pre-kernel boot

 Clear the reset interrupt. */

 Legacy compatible string */

 When not probing from device tree we have this ID */

 put the system to sleep mode */

 enable the system into operation */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ROHM BD9571MWV-M and BD9574MVF-M core driver

 *

 * Copyright (C) 2017 Marek Vasut <marek.vasut+renesas@gmail.com>

 * Copyright (C) 2020 Renesas Electronics Corporation

 *

 * Based on the TPS65086 driver

 Read the PMIC product code */

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD core driver for the Richtek RT5033.

 *

 * RT5033 comprises multiple sub-devices switcing charger, fuel gauge,

 * flash LED, current source, LDO and BUCK regulators.

 *

 * Copyright (C) 2014 Samsung Electronics, Co., Ltd.

 * Author: Beomho Seo <beomho.seo@samsung.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8350-core.c  --  Device access for Wolfson WM8350

 *

 * Copyright 2007, 2008 Wolfson Microelectronics PLC.

 *

 * Author: Liam Girdwood, Mark Brown

 debug */

/*

 * WM8350 Device IO

/*

 * Safe read, modify, write methods

/**

 * wm8350_reg_lock()

 *

 * The WM8350 has a hardware lock which can be used to prevent writes to

 * some registers (generally those which can cause particularly serious

 * problems if misused).  This function enables that lock.

 *

 * @wm8350: pointer to local driver data structure

/**

 * wm8350_reg_unlock()

 *

 * The WM8350 has a hardware lock which can be used to prevent writes to

 * some registers (generally those which can cause particularly serious

 * problems if misused).  This function disables that lock so updates

 * can be performed.  For maximum safety this should be done only when

 * required.

 *

 * @wm8350: pointer to local driver data structure

 Turn on the ADC */

	/* If a late IRQ left the completion signalled then consume

	/* We ignore the result of the completion and just check for a

	 * conversion result, allowing us to soldier on if the IRQ

 Turn off the ADC */

/*

 * Register a client device.  This is non-fatal since there is no need to

 * fail the entire device init due to a single platform device failing.

 get WM8350 revision and config mode */

 For safety we refuse to run on unknown hardware */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Acer Iconia Tab A500 Embedded Controller Driver

 *

 * Copyright 2020 GRATE-driver project

/*

 * Controller's firmware expects specific command opcodes to be used for the

 * corresponding registers. Unsupported commands are skipped by the firmware.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Spreadtrum Communications Inc.

 PMIC charger detection definition */

/*

 * Since different PMICs of SC27xx series can have different interrupt

 * base address and irq number, we should save irq number and irq base

 * in the device data structure.

 Now we only support one PMIC register to read every time. */

 Copy address to read from into first element of SPI buffer. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Lochnagar I2C bus interface

 *

 * Copyright (c) 2012-2018 Cirrus Logic, Inc. and

 *                         Cirrus Logic International Semiconductor Ltd.

 *

 * Author: Charles Keepax <ckeepax@opensource.cirrus.com>

 The reset register will return the device ID when read */

/**

 * lochnagar_update_config - Synchronise the boards analogue configuration to

 *                           the hardware.

 *

 * @lochnagar: A pointer to the primary core data structure.

 *

 * Return: Zero on success or an appropriate negative error code on failure.

	/*

	 * Toggle the ANALOGUE_PATH_UPDATE bit and wait for the device to

	 * acknowledge that any outstanding changes to the analogue

	 * configuration have been applied.

 Leave the Lochnagar in reset for a reasonable amount of time */

 Bring Lochnagar out of reset */

 Identify Lochnagar */

 Wait for Lochnagar to boot */

 Identify firmware */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Core support for ATC260x PMICs

 *

 * Copyright (C) 2019 Manivannan Sadhasivam <manivannan.sadhasivam@linaro.org>

 * Copyright (C) 2020 Cristian Ciocaltea <cristian.ciocaltea@gmail.com>

	/*

	 * Using regmap within an atomic context (e.g. accessing a PMIC when

	 * powering system down) is normally allowed only if the regmap type

	 * is MMIO and the regcache type is either REGCACHE_NONE or

	 * REGCACHE_FLAT. For slow buses like I2C and SPI, the regmap is

	 * internally protected by a mutex which is acquired non-atomically.

	 *

	 * Let's improve this by using a customized locking scheme inspired

	 * from I2C atomic transfer. See i2c_in_atomic_xfer_mode() for a

	 * starting point.

 Assert reset */

 De-assert reset */

 Initialize interrupt block */

 Disable all interrupt sources */

 Enable EXTIRQ pad */

/**

 * atc260x_match_device(): Setup ATC260x variant related fields

 *

 * @atc260x: ATC260x device to setup (.dev field must be set)

 * @regmap_cfg: regmap config associated with this ATC260x device

 *

 * This lets the ATC260x core configure the MFD cells and register maps

 * for later use.

/**

 * atc260x_device_probe(): Probe a configured ATC260x device

 *

 * @atc260x: ATC260x device to probe (must be configured)

 *

 * This function lets the ATC260x core register the ATC260x MFD devices

 * and IRQCHIP. The ATC260x device passed in must be fully configured

 * with atc260x_match_device, its IRQ set, and regmap created.

 Initialize the hardware */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel LPSS PCI support.

 *

 * Copyright (C) 2015, Intel Corporation

 *

 * Authors: Andy Shevchenko <andriy.shevchenko@linux.intel.com>

 *          Mika Westerberg <mika.westerberg@linux.intel.com>

 Probably it is enough to set this for iDMA capable devices only */

 CML-LP */

 CML-H */

 BXT A-Step */

 BXT B-Step */

 EBG */

 GLK */

 ICL-LP */

 ICL-N */

 TGL-H */

 EHL */

 JSL */

 ADL-P */

 ADL-M */

 APL */

 ADL-S */

 LKF */

 SPT-LP */

 CNL-LP */

 TGL-LP */

 SPT-H */

 KBL-H */

 CNL-H */

 CML-V */

 SPDX-License-Identifier: GPL-2.0-only



 Copyright (C) 2019 ROHM Semiconductors



 ROHM BD71828/BD71815 PMIC driver

	/*

	 * We use BD71837 driver to drive the clock block. Only differences to

	 * BD70528 clock gate are the register address and mask.

		/*

		 * For now make all charger registers volatile because many

		 * needs to be and because the charger block is not that

		 * performance critical.

/*

 * Mapping of main IRQ register bits to sub-IRQ register offsets so that we can

 * access corect sub-IRQ registers based on bits that are set in main IRQ

 * register. BD71815 and BD71828 have same sub-register-block offests.

 RTC IRQ */

 TEMP IRQ */

 BAT MON IRQ */

 BAT IRQ */

 CHG IRQ */

 VSYS IRQ */

 DCIN IRQ */

 BUCK IRQ */

 DCIN1 interrupts */

 DCIN2 interrupts */

 Vsys */

 Charger */

 Battery */

 Battery Mon 1 */

 Battery Mon 2 */

 Battery Mon 3 (Coulomb counter) */

 Battery Mon 4 */

 Temperature */

 RTC Alarm */

 DCIN1 interrupts */

 DCIN2 interrupts */

 Vsys */

 Charger */

 Battery */

 Battery Mon 1 */

 Battery Mon 2 */

 Battery Mon 3 (Coulomb counter) */

 Battery Mon 4 */

 Temperature */

 RTC Alarm */

		/*

		 * If BD71817 support is needed we should be able to handle it

		 * with proper DT configs + BD71815 drivers + power-button.

		 * BD71815 data-sheet does not list the power-button IRQ so we

		 * don't use it.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014 MediaTek Inc.

 * Author: Flora Fu, MediaTek

	/*

	 * mt6397 MFD is child device of soc pmic wrapper.

	 * Regmap is set from its parent.

 sentinel */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2021 ROHM Semiconductors

 *

 * ROHM BD9576MUF and BD9573MUF PMIC driver

/*

 * Due to the BD9576MUF nasty IRQ behaiour we don't always populate IRQs.

 * These will be added to regulator resources only if IRQ information for the

 * PMIC is populated in device-tree.

		/*

		 * BD9573 only supports fatal IRQs which we can not handle

		 * because SoC is going to lose the power.

	/*

	 * BD9576 behaves badly. It kepts IRQ line asserted for the whole

	 * duration of detected HW condition (like over temperature). So we

	 * don't require IRQ to be populated.

	 * If IRQ information is not given, then we mask all IRQs and do not

	 * provide IRQ resources to regulator driver - which then just omits

	 * the notifiers.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * twl4030-irq.c - TWL4030/TPS659x0 irq support

 *

 * Copyright (C) 2005-2006 Texas Instruments, Inc.

 *

 * Modifications to defer interrupt handling to a kernel thread:

 * Copyright (C) 2006 MontaVista Software, Inc.

 *

 * Based on tlv320aic23.c:

 * Copyright (c) by Kai Svahn <kai.svahn@nokia.com>

 *

 * Code cleanup and modifications to IRQ handler.

 * by syed khasim <x0khasim@ti.com>

/*

 * TWL4030 IRQ handling has two stages in hardware, and thus in software.

 * The Primary Interrupt Handler (PIH) stage exposes status bits saying

 * which Secondary Interrupt Handler (SIH) stage is raising an interrupt.

 * SIH modules are more traditional IRQ components, which support per-IRQ

 * enable/disable and trigger controls; they do most of the work.

 *

 * These chips are designed to support IRQ handling from two different

 * I2C masters.  Each has a dedicated IRQ line, and dedicated IRQ status

 * and mask registers in the PIH and SIH modules.

 *

 * We set up IRQs starting at a platform-specified base, always starting

 * with PIH and the SIH for PWR_INT and then usually adding GPIO:

 *	base + 0  .. base + 7	PIH

 *	base + 8  .. base + 15	SIH for PWR_INT

 *	base + 16 .. base + 33	SIH for GPIO

 PIH register offsets */

 for testing */

 Linux could (eventually) use either IRQ line */

 module id */

 for SIH_CTRL */

 valid in isr/imr */

 bytelen of ISR/IMR/SIR */

 bytelen of EDR */

 number of supported irq lines */

 SIR ignored -- set interrupt, for testing only */

 + 2 bytes padding */

 register naming policies are inconsistent ... */

/*

 * Order in this table matches order in PIH_ISR.  That is,

 * BIT(n) in PIH_ISR is sih_modules[n].

 sih_modules_twl4030 is used both in twl4030 and twl5030 */

 Note: *all* of these IRQs default to no-trigger */

 Note: most of these IRQs default to no-trigger */

 USB doesn't use the same SIH organization */

 there are no SIH modules #6 or #7 ... */

 Note: *all* of these IRQs default to no-trigger */

 Note: most of these IRQs default to no-trigger */

 USB doesn't use the same SIH organization */

		/*

		 * ECI/DBI doesn't use the same SIH organization.

		 * For example, it supports only one interrupt output line.

		 * That is, the interrupts are seen on both INT1 and INT2 lines.

 Audio accessory */

 Note: most of these IRQs default to no-trigger */

----------------------------------------------------------------------*/

/*

 * handle_twl4030_pih() is the desc->handle method for the twl4030 interrupt.

 * This is a chained interrupt, so there is no desc->action method for it.

 * Now we need to query the interrupt controller in the twl4030 to determine

 * which module is generating the interrupt request.  However, we can't do i2c

 * transactions in interrupt context, so we must defer that work to a kernel

 * thread.  All we do here is acknowledge and mask the interrupt and wakeup

 * the kernel thread.

----------------------------------------------------------------------*/

/*

 * twl4030_init_sih_modules() ... start from a known state where no

 * IRQs will be coming in, and where we can quickly enable them then

 * handle them as they arrive.  Mask all IRQs: maybe init SIH_CTRL.

 *

 * NOTE:  we don't touch EDR registers here; they stay with hardware

 * defaults or whatever the last value was.  Note that when both EDR

 * bits for an IRQ are clear, that's as if its IMR bit is set...

 line 0 == int1_n signal; line 1 == int2_n signal */

 disable all interrupts on our line */

 skip USB -- it's funky */

 Not all the SIH modules support multiple interrupt lines */

		/*

		 * Maybe disable "exclusive" mode; buffer second pending irq;

		 * set Clear-On-Read (COR) bit.

		 *

		 * NOTE that sometimes COR polarity is documented as being

		 * inverted:  for MADC, COR=1 means "clear on write".

		 * And for PWR_INT it's not documented...

 skip USB */

 Not all the SIH modules support multiple interrupt lines */

		/*

		 * Clear pending interrupt status.  Either the read was

		 * enough, or we need to write those bits.  Repeat, in

		 * case an IRQ is pending (PENDDIS=0) ... that's not

		 * uncommon with PWR_INT.PWRON.

			/*

			 * else COR=1 means read sufficed.

			 * (for most SIH modules...)

----------------------------------------------------------------------*/

----------------------------------------------------------------------*/

/*

 * All irq_chip methods get issued from code holding irq_desc[irq].lock,

 * which can't perform the underlying I2C operations (because they sleep).

 * So we must hand them off to a thread (workqueue) and cope with asynch

 * completion, potentially including some re-ordering, of these requests.

 byte[0] gets overwritten as we write ... */

 write the whole mask ... simpler than subsetting it */

		/*

		 * Read, reserving first byte for write scratch.  Yes, this

		 * could be cached for some speedup ... but be careful about

		 * any processor on the other IRQ line, EDR registers are

		 * shared.

 Modify only the bits we know must change */

 Write */

----------------------------------------------------------------------*/

 FIXME need retry-on-error ... */

/*

 * Generic handler for SIH interrupts ... we "know" this is called

 * in task context, with IRQs enabled.

 reading ISR acks the IRQs, using clear-on-read mode */

 REVISIT:  recover; eventually mask it all, etc */

 returns the first IRQ used by this SIH bank, or negative errno */

 only support modules with standard clear-on-read for now */

 replace generic PIH handler (handle_simple_irq) */

 FIXME need a call to reverse twl4030_sih_setup() ... */

----------------------------------------------------------------------*/

 FIXME pass in which interrupt line we'll use ... */

	/*

	 * TWL core and pwr interrupts must be contiguous because

	 * the hwirqs numbers are defined contiguously from 1 to 15.

	 * Create only one domain for both.

	/*

	 * Mask and clear all TWL4030 interrupts since initially we do

	 * not have any TWL4030 module interrupt handlers present

	/*

	 * Install an irq handler for each of the SIH modules;

	 * clone dummy irq_chip since PIH can't *do* anything

 ... and the PWR_INT module ... */

 install an irq handler to demultiplex the TWL4030 interrupt */

 clean up twl4030_sih_setup */

 FIXME undo twl_init_irq() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel Platform Monitoring Technology PMT driver

 *

 * Copyright (c) 2020, Intel Corporation.

 * All Rights Reserved.

 *

 * Author: David E. Box <david.e.box@linux.intel.com>

 Intel DVSEC capability vendor space offsets */

 PMT capabilities */

 Watcher capability not supported */

 Crashlog capability not supported */

 Use shift instead of mask to read discovery table offset */

 DVSEC not present (provided in driver data) */

 DG1 Platform with DVSEC quirk*/

	/*

	 * The PMT DVSEC contains the starting offset and count for a block of

	 * discovery tables, each providing access to monitoring facilities for

	 * a section of the device. Create a resource list of these tables to

	 * provide to the driver.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Regmap tables for CS47L35 codec

 *

 * Copyright (C) 2015-2017 Cirrus Logic

 R32 (0x20) - Tone Generator 1 */

 R33 (0x21) - Tone Generator 2 */

 R34 (0x22) - Tone Generator 3 */

 R35 (0x23) - Tone Generator 4 */

 R36 (0x24) - Tone Generator 5 */

 R48 (0x30) - PWM Drive 1 */

 R49 (0x31) - PWM Drive 2 */

 R50 (0x32) - PWM Drive 3 */

 R97 (0x61) - Sample Rate Sequence Select 1 */

 R98 (0x62) - Sample Rate Sequence Select 2 */

 R99 (0x63) - Sample Rate Sequence Select 3 */

 R100 (0x64) - Sample Rate Sequence Select 4*/

 R102 (0x66) - Always On Triggers Sequence Select 1*/

 R103 (0x67) - Always On Triggers Sequence Select 2*/

 R144 (0x90) - Haptics Control 1 */

 R145 (0x91) - Haptics Control 2 */

 R146 (0x92) - Haptics phase 1 intensity */

 R147 (0x93) - Haptics phase 1 duration */

 R148 (0x94) - Haptics phase 2 intensity */

 R149 (0x95) - Haptics phase 2 duration */

 R150 (0x96) - Haptics phase 3 intensity */

 R151 (0x97) - Haptics phase 3 duration */

 R160 (0xa0) - Comfort Noise Generator */

 R256 (0x100) - Clock 32k 1 */

 R257 (0x101) - System Clock 1 */

 R258 (0x102) - Sample rate 1 */

 R259 (0x103) - Sample rate 2 */

 R260 (0x104) - Sample rate 3 */

 R288 (0x120) - DSP Clock 1 */

 R290 (0x122) - DSP Clock 2 */

 R329 (0x149) - Output system clock */

 R330 (0x14a) - Output async clock */

 R338 (0x152) - Rate Estimator 1 */

 R339 (0x153) - Rate Estimator 2 */

 R340 (0x154) - Rate Estimator 3 */

 R341 (0x155) - Rate Estimator 4 */

 R342 (0x156) - Rate Estimator 5 */

 R369 (0x171) - FLL1 Control 1 */

 R370 (0x172) - FLL1 Control 2 */

 R371 (0x173) - FLL1 Control 3 */

 R372 (0x174) - FLL1 Control 4 */

 R373 (0x175) - FLL1 Control 5 */

 R374 (0x176) - FLL1 Control 6 */

 R377 (0x179) - FLL1 Control 7 */

 R378 (0x17a) - FLL1 EFS2 */

 R383 (0x17f) - FLL1 Synchroniser 1 */

 R384 (0x180) - FLL1 Synchroniser 2 */

 R385 (0x181) - FLL1 Synchroniser 3 */

 R386 (0x182) - FLL1 Synchroniser 4 */

 R387 (0x183) - FLL1 Synchroniser 5 */

 R388 (0x184) - FLL1 Synchroniser 6 */

 R389 (0x185) - FLL1 Synchroniser 7 */

 R391 (0x187) - FLL1 Spread Spectrum */

 R392 (0x188) - FLL1 GPIO Clock */

 R512 (0x200) - Mic Charge Pump 1 */

 R523 (0x20b) - HP Charge Pump 8 */

 R531 (0x213) - LDO2 Control 1 */

 R536 (0x218) - Mic Bias Ctrl 1 */

 R537 (0x219) - Mic Bias Ctrl 2 */

 R540 (0x21c) - Mic Bias Ctrl 5 */

 R542 (0x21e) - Mic Bias Ctrl 6 */

 R638 (0x27e) - EDRE HP stereo control */

 R659 (0x293) - Accessory Detect Mode 1 */

 R667 (0x29b) - Headphone Detect 1 */

 R675 (0x2a3) - Mic Detect Control 1 */

 R676 (0x2a4) - Mic Detect Control 2 */

 R678 (0x2a6) - Mic Detect Level 1 */

 R679 (0x2a7) - Mic Detect Level 2 */

 R680 (0x2a8) - Mic Detect Level 3 */

 R681 (0x2a9) - Mic Detect Level 4 */

 R710 (0x2c5) - Mic Clamp control */

 R712 (0x2c8) - GP switch 1 */

 R723 (0x2d3) - Jack detect analogue */

 R768 (0x300) - Input Enables */

 R776 (0x308) - Input Rate */

 R777 (0x309) - Input Volume Ramp */

 R780 (0x30c) - HPF Control */

 R784 (0x310) - IN1L Control */

 R785 (0x311) - ADC Digital Volume 1L */

 R786 (0x312) - DMIC1L Control */

 R788 (0x314) - IN1R Control */

 R789 (0x315) - ADC Digital Volume 1R */

 R790 (0x316) - DMIC1R Control */

 R792 (0x318) - IN2L Control */

 R793 (0x319) - ADC Digital Volume 2L */

 R794 (0x31a) - DMIC2L Control */

 R796 (0x31c) - IN2R Control */

 R797 (0x31d) - ADC Digital Volume 2R */

 R798 (0x31e) - DMIC2R Control */

 R1024 (0x400) - Output Enables 1 */

 R1032 (0x408) - Output Rate 1 */

 R1033 (0x409) - Output Volume Ramp */

 R1040 (0x410) - Output Path Config 1L */

 R1041 (0x411) - DAC Digital Volume 1L */

 R1043 (0x413) - Noise Gate Select 1L */

 R1044 (0x414) - Output Path Config 1R */

 R1045 (0x415) - DAC Digital Volume 1R */

 R1047 (0x417) - Noise Gate Select 1R */

 R1064 (0x428) - Output Path Config 4L */

 R1065 (0x429) - DAC Digital Volume 4L */

 R1067 (0x42b) - Noise Gate Select 4L */

 R1072 (0x430) - Output Path Config 5L */

 R1073 (0x431) - DAC Digital Volume 5L */

 R1075 (0x433) - Noise Gate Select 5L */

 R1076 (0x434) - Output Path Config 5R */

 R1077 (0x435) - DAC Digital Volume 5R */

 R1079 (0x437) - Noise Gate Select 5R */

 R1104 (0x450) - DAC AEC Control 1 */

 R1105 (0x451) - DAC AEC Control 2 */

 R1112 (0x458) - Noise Gate Control */

 R1168 (0x490) - PDM SPK1 CTRL 1 */

 R1169 (0x491) - PDM SPK1 CTRL 2 */

 R1184 (0x4a0) - HP1 Short Circuit Ctrl */

 R1192 (0x4a8) - HP Test Ctrl 5 */

 R1193 (0x4a9) - HP Test Ctrl 6 */

 R1280 (0x500) - AIF1 BCLK Ctrl */

 R1281 (0x501) - AIF1 Tx Pin Ctrl */

 R1282 (0x502) - AIF1 Rx Pin Ctrl */

 R1283 (0x503) - AIF1 Rate Ctrl */

 R1284 (0x504) - AIF1 Format */

 R1286 (0x506) - AIF1 Rx BCLK Rate */

 R1287 (0x507) - AIF1 Frame Ctrl 1 */

 R1288 (0x508) - AIF1 Frame Ctrl 2 */

 R1289 (0x509) - AIF1 Frame Ctrl 3 */

 R1290 (0x50a) - AIF1 Frame Ctrl 4 */

 R1291 (0x50b) - AIF1 Frame Ctrl 5 */

 R1292 (0x50c) - AIF1 Frame Ctrl 6 */

 R1293 (0x50d) - AIF1 Frame Ctrl 7 */

 R1294 (0x50e) - AIF1 Frame Ctrl 8 */

 R1297 (0x511) - AIF1 Frame Ctrl 11 */

 R1298 (0x512) - AIF1 Frame Ctrl 12 */

 R1299 (0x513) - AIF1 Frame Ctrl 13 */

 R1300 (0x514) - AIF1 Frame Ctrl 14 */

 R1301 (0x515) - AIF1 Frame Ctrl 15 */

 R1302 (0x516) - AIF1 Frame Ctrl 16 */

 R1305 (0x519) - AIF1 Tx Enables */

 R1306 (0x51a) - AIF1 Rx Enables */

 R1344 (0x540) - AIF2 BCLK Ctrl */

 R1345 (0x541) - AIF2 Tx Pin Ctrl */

 R1346 (0x542) - AIF2 Rx Pin Ctrl */

 R1347 (0x543) - AIF2 Rate Ctrl */

 R1348 (0x544) - AIF2 Format */

 R1350 (0x546) - AIF2 Rx BCLK Rate */

 R1351 (0x547) - AIF2 Frame Ctrl 1 */

 R1352 (0x548) - AIF2 Frame Ctrl 2 */

 R1353 (0x549) - AIF2 Frame Ctrl 3 */

 R1354 (0x54a) - AIF2 Frame Ctrl 4 */

 R1361 (0x551) - AIF2 Frame Ctrl 11 */

 R1362 (0x552) - AIF2 Frame Ctrl 12 */

 R1369 (0x559) - AIF2 Tx Enables */

 R1370 (0x55a) - AIF2 Rx Enables */

 R1408 (0x580) - AIF3 BCLK Ctrl */

 R1409 (0x581) - AIF3 Tx Pin Ctrl */

 R1410 (0x582) - AIF3 Rx Pin Ctrl */

 R1411 (0x583) - AIF3 Rate Ctrl */

 R1412 (0x584) - AIF3 Format */

 R1414 (0x586) - AIF3 Rx BCLK Rate */

 R1415 (0x587) - AIF3 Frame Ctrl 1 */

 R1416 (0x588) - AIF3 Frame Ctrl 2 */

 R1417 (0x589) - AIF3 Frame Ctrl 3 */

 R1418 (0x58a) - AIF3 Frame Ctrl 4 */

 R1425 (0x591) - AIF3 Frame Ctrl 11 */

 R1426 (0x592) - AIF3 Frame Ctrl 12 */

 R1433 (0x599) - AIF3 Tx Enables */

 R1434 (0x59a) - AIF3 Rx Enables */

 R1474 (0x5c2) - SPD1 TX Control */

 R1507 (0x5e3) - SLIMbus Framer Ref Gear */

 R1509 (0x5e5) - SLIMbus Rates 1 */

 R1510 (0x5e6) - SLIMbus Rates 2 */

 R1511 (0x5e7) - SLIMbus Rates 3 */

 R1513 (0x5e9) - SLIMbus Rates 5 */

 R1514 (0x5ea) - SLIMbus Rates 6 */

 R1515 (0x5eb) - SLIMbus Rates 7 */

 R1525 (0x5f5) - SLIMbus RX Channel Enable */

 R1526 (0x5f6) - SLIMbus TX Channel Enable */

 R1600 (0x640) - PWM1MIX Input 1 Source */

 R1601 (0x641) - PWM1MIX Input 1 Volume */

 R1602 (0x642) - PWM1MIX Input 2 Source */

 R1603 (0x643) - PWM1MIX Input 2 Volume */

 R1604 (0x644) - PWM1MIX Input 3 Source */

 R1605 (0x645) - PWM1MIX Input 3 Volume */

 R1606 (0x646) - PWM1MIX Input 4 Source */

 R1607 (0x647) - PWM1MIX Input 4 Volume */

 R1608 (0x648) - PWM2MIX Input 1 Source */

 R1609 (0x649) - PWM2MIX Input 1 Volume */

 R1610 (0x64a) - PWM2MIX Input 2 Source */

 R1611 (0x64b) - PWM2MIX Input 2 Volume */

 R1612 (0x64c) - PWM2MIX Input 3 Source */

 R1613 (0x64d) - PWM2MIX Input 3 Volume */

 R1614 (0x64e) - PWM2MIX Input 4 Source */

 R1615 (0x64f) - PWM2MIX Input 4 Volume */

 R1664 (0x680) - OUT1LMIX Input 1 Source */

 R1665 (0x681) - OUT1LMIX Input 1 Volume */

 R1666 (0x682) - OUT1LMIX Input 2 Source */

 R1667 (0x683) - OUT1LMIX Input 2 Volume */

 R1668 (0x684) - OUT1LMIX Input 3 Source */

 R1669 (0x685) - OUT1LMIX Input 3 Volume */

 R1670 (0x686) - OUT1LMIX Input 4 Source */

 R1671 (0x687) - OUT1LMIX Input 4 Volume */

 R1672 (0x688) - OUT1RMIX Input 1 Source */

 R1673 (0x689) - OUT1RMIX Input 1 Volume */

 R1674 (0x68a) - OUT1RMIX Input 2 Source */

 R1675 (0x68b) - OUT1RMIX Input 2 Volume */

 R1672 (0x68c) - OUT1RMIX Input 3 Source */

 R1673 (0x68d) - OUT1RMIX Input 3 Volume */

 R1674 (0x68e) - OUT1RMIX Input 4 Source */

 R1675 (0x68f) - OUT1RMIX Input 4 Volume */

 R1712 (0x6b0) - OUT4LMIX Input 1 Source */

 R1713 (0x6b1) - OUT4LMIX Input 1 Volume */

 R1714 (0x6b2) - OUT4LMIX Input 2 Source */

 R1715 (0x6b3) - OUT4LMIX Input 2 Volume */

 R1716 (0x6b4) - OUT4LMIX Input 3 Source */

 R1717 (0x6b5) - OUT4LMIX Input 3 Volume */

 R1718 (0x6b6) - OUT4LMIX Input 4 Source */

 R1719 (0x6b7) - OUT4LMIX Input 4 Volume */

 R1728 (0x6c0) - OUT5LMIX Input 1 Source */

 R1729 (0x6c1) - OUT5LMIX Input 1 Volume */

 R1730 (0x6c2) - OUT5LMIX Input 2 Source */

 R1731 (0x6c3) - OUT5LMIX Input 2 Volume */

 R1732 (0x6c4) - OUT5LMIX Input 3 Source */

 R1733 (0x6c5) - OUT5LMIX Input 3 Volume */

 R1734 (0x6c6) - OUT5LMIX Input 4 Source */

 R1735 (0x6c7) - OUT5LMIX Input 4 Volume */

 R1736 (0x6c8) - OUT5RMIX Input 1 Source */

 R1737 (0x6c9) - OUT5RMIX Input 1 Volume */

 R1738 (0x6ca) - OUT5RMIX Input 2 Source */

 R1739 (0x6cb) - OUT5RMIX Input 2 Volume */

 R1740 (0x6cc) - OUT5RMIX Input 3 Source */

 R1741 (0x6cd) - OUT5RMIX Input 3 Volume */

 R1742 (0x6ce) - OUT5RMIX Input 4 Source */

 R1743 (0x6cf) - OUT5RMIX Input 4 Volume */

 R1792 (0x700) - AIF1TX1MIX Input 1 Source */

 R1793 (0x701) - AIF1TX1MIX Input 1 Volume */

 R1794 (0x702) - AIF1TX1MIX Input 2 Source */

 R1795 (0x703) - AIF1TX1MIX Input 2 Volume */

 R1796 (0x704) - AIF1TX1MIX Input 3 Source */

 R1797 (0x705) - AIF1TX1MIX Input 3 Volume */

 R1798 (0x706) - AIF1TX1MIX Input 4 Source */

 R1799 (0x707) - AIF1TX1MIX Input 4 Volume */

 R1800 (0x708) - AIF1TX2MIX Input 1 Source */

 R1801 (0x709) - AIF1TX2MIX Input 1 Volume */

 R1802 (0x70a) - AIF1TX2MIX Input 2 Source */

 R1803 (0x70b) - AIF1TX2MIX Input 2 Volume */

 R1804 (0x70c) - AIF1TX2MIX Input 3 Source */

 R1805 (0x70d) - AIF1TX2MIX Input 3 Volume */

 R1806 (0x70e) - AIF1TX2MIX Input 4 Source */

 R1807 (0x70f) - AIF1TX2MIX Input 4 Volume */

 R1808 (0x710) - AIF1TX3MIX Input 1 Source */

 R1809 (0x711) - AIF1TX3MIX Input 1 Volume */

 R1810 (0x712) - AIF1TX3MIX Input 2 Source */

 R1811 (0x713) - AIF1TX3MIX Input 2 Volume */

 R1812 (0x714) - AIF1TX3MIX Input 3 Source */

 R1813 (0x715) - AIF1TX3MIX Input 3 Volume */

 R1814 (0x716) - AIF1TX3MIX Input 4 Source */

 R1815 (0x717) - AIF1TX3MIX Input 4 Volume */

 R1816 (0x718) - AIF1TX4MIX Input 1 Source */

 R1817 (0x719) - AIF1TX4MIX Input 1 Volume */

 R1818 (0x71a) - AIF1TX4MIX Input 2 Source */

 R1819 (0x71b) - AIF1TX4MIX Input 2 Volume */

 R1820 (0x71c) - AIF1TX4MIX Input 3 Source */

 R1821 (0x71d) - AIF1TX4MIX Input 3 Volume */

 R1822 (0x71e) - AIF1TX4MIX Input 4 Source */

 R1823 (0x71f) - AIF1TX4MIX Input 4 Volume */

 R1824 (0x720) - AIF1TX5MIX Input 1 Source */

 R1825 (0x721) - AIF1TX5MIX Input 1 Volume */

 R1826 (0x722) - AIF1TX5MIX Input 2 Source */

 R1827 (0x723) - AIF1TX5MIX Input 2 Volume */

 R1828 (0x724) - AIF1TX5MIX Input 3 Source */

 R1829 (0x725) - AIF1TX5MIX Input 3 Volume */

 R1830 (0x726) - AIF1TX5MIX Input 4 Source */

 R1831 (0x727) - AIF1TX5MIX Input 4 Volume */

 R1832 (0x728) - AIF1TX6MIX Input 1 Source */

 R1833 (0x729) - AIF1TX6MIX Input 1 Volume */

 R1834 (0x72a) - AIF1TX6MIX Input 2 Source */

 R1835 (0x72b) - AIF1TX6MIX Input 2 Volume */

 R1836 (0x72c) - AIF1TX6MIX Input 3 Source */

 R1837 (0x72d) - AIF1TX6MIX Input 3 Volume */

 R1838 (0x72e) - AIF1TX6MIX Input 4 Source */

 R1839 (0x72f) - AIF1TX6MIX Input 4 Volume */

 R1856 (0x740) - AIF2TX1MIX Input 1 Source */

 R1857 (0x741) - AIF2TX1MIX Input 1 Volume */

 R1858 (0x742) - AIF2TX1MIX Input 2 Source */

 R1859 (0x743) - AIF2TX1MIX Input 2 Volume */

 R1860 (0x744) - AIF2TX1MIX Input 3 Source */

 R1861 (0x745) - AIF2TX1MIX Input 3 Volume */

 R1862 (0x746) - AIF2TX1MIX Input 4 Source */

 R1863 (0x747) - AIF2TX1MIX Input 4 Volume */

 R1864 (0x748) - AIF2TX2MIX Input 1 Source */

 R1865 (0x749) - AIF2TX2MIX Input 1 Volume */

 R1866 (0x74a) - AIF2TX2MIX Input 2 Source */

 R1867 (0x74b) - AIF2TX2MIX Input 2 Volume */

 R1868 (0x74c) - AIF2TX2MIX Input 3 Source */

 R1869 (0x74d) - AIF2TX2MIX Input 3 Volume */

 R1870 (0x74e) - AIF2TX2MIX Input 4 Source */

 R1871 (0x74f) - AIF2TX2MIX Input 4 Volume */

 R1920 (0x780) - AIF3TX1MIX Input 1 Source */

 R1921 (0x781) - AIF3TX1MIX Input 1 Volume */

 R1922 (0x782) - AIF3TX1MIX Input 2 Source */

 R1923 (0x783) - AIF3TX1MIX Input 2 Volume */

 R1924 (0x784) - AIF3TX1MIX Input 3 Source */

 R1925 (0x785) - AIF3TX1MIX Input 3 Volume */

 R1926 (0x786) - AIF3TX1MIX Input 4 Source */

 R1927 (0x787) - AIF3TX1MIX Input 4 Volume */

 R1928 (0x788) - AIF3TX2MIX Input 1 Source */

 R1929 (0x789) - AIF3TX2MIX Input 1 Volume */

 R1930 (0x78a) - AIF3TX2MIX Input 2 Source */

 R1931 (0x78b) - AIF3TX2MIX Input 2 Volume */

 R1932 (0x78c) - AIF3TX2MIX Input 3 Source */

 R1933 (0x78d) - AIF3TX2MIX Input 3 Volume */

 R1934 (0x78e) - AIF3TX2MIX Input 4 Source */

 R1935 (0x78f) - AIF3TX2MIX Input 4 Volume */

 R1984 (0x7c0) - SLIMTX1MIX Input 1 Source */

 R1985 (0x7c1) - SLIMTX1MIX Input 1 Volume */

 R1986 (0x7c2) - SLIMTX1MIX Input 2 Source */

 R1987 (0x7c3) - SLIMTX1MIX Input 2 Volume */

 R1988 (0x7c4) - SLIMTX1MIX Input 3 Source */

 R1989 (0x7c5) - SLIMTX1MIX Input 3 Volume */

 R1990 (0x7c6) - SLIMTX1MIX Input 4 Source */

 R1991 (0x7c7) - SLIMTX1MIX Input 4 Volume */

 R1992 (0x7c8) - SLIMTX2MIX Input 1 Source */

 R1993 (0x7c9) - SLIMTX2MIX Input 1 Volume */

 R1994 (0x7ca) - SLIMTX2MIX Input 2 Source */

 R1995 (0x7cb) - SLIMTX2MIX Input 2 Volume */

 R1996 (0x7cc) - SLIMTX2MIX Input 3 Source */

 R1997 (0x7cd) - SLIMTX2MIX Input 3 Volume */

 R1998 (0x7ce) - SLIMTX2MIX Input 4 Source */

 R1999 (0x7cf) - SLIMTX2MIX Input 4 Volume */

 R2000 (0x7d0) - SLIMTX3MIX Input 1 Source */

 R2001 (0x7d1) - SLIMTX3MIX Input 1 Volume */

 R2002 (0x7d2) - SLIMTX3MIX Input 2 Source */

 R2003 (0x7d3) - SLIMTX3MIX Input 2 Volume */

 R2004 (0x7d4) - SLIMTX3MIX Input 3 Source */

 R2005 (0x7d5) - SLIMTX3MIX Input 3 Volume */

 R2006 (0x7d6) - SLIMTX3MIX Input 4 Source */

 R2007 (0x7d7) - SLIMTX3MIX Input 4 Volume */

 R2008 (0x7d8) - SLIMTX4MIX Input 1 Source */

 R2009 (0x7d9) - SLIMTX4MIX Input 1 Volume */

 R2010 (0x7da) - SLIMTX4MIX Input 2 Source */

 R2011 (0x7db) - SLIMTX4MIX Input 2 Volume */

 R2012 (0x7dc) - SLIMTX4MIX Input 3 Source */

 R2013 (0x7dd) - SLIMTX4MIX Input 3 Volume */

 R2014 (0x7de) - SLIMTX4MIX Input 4 Source */

 R2015 (0x7df) - SLIMTX4MIX Input 4 Volume */

 R2016 (0x7e0) - SLIMTX5MIX Input 1 Source */

 R2017 (0x7e1) - SLIMTX5MIX Input 1 Volume */

 R2018 (0x7e2) - SLIMTX5MIX Input 2 Source */

 R2019 (0x7e3) - SLIMTX5MIX Input 2 Volume */

 R2020 (0x7e4) - SLIMTX5MIX Input 3 Source */

 R2021 (0x7e5) - SLIMTX5MIX Input 3 Volume */

 R2022 (0x7e6) - SLIMTX5MIX Input 4 Source */

 R2023 (0x7e7) - SLIMTX5MIX Input 4 Volume */

 R2024 (0x7e8) - SLIMTX6MIX Input 1 Source */

 R2025 (0x7e9) - SLIMTX6MIX Input 1 Volume */

 R2026 (0x7ea) - SLIMTX6MIX Input 2 Source */

 R2027 (0x7eb) - SLIMTX6MIX Input 2 Volume */

 R2028 (0x7ec) - SLIMTX6MIX Input 3 Source */

 R2029 (0x7ed) - SLIMTX6MIX Input 3 Volume */

 R2030 (0x7ee) - SLIMTX6MIX Input 4 Source */

 R2031 (0x7ef) - SLIMTX6MIX Input 4 Volume */

 R2048 (0x800) - SPDIF1TX1MIX Input 1 Source*/

 R2049 (0x801) - SPDIF1TX1MIX Input 1 Volume*/

 R2056 (0x808) - SPDIF1TX2MIX Input 1 Source*/

 R2057 (0x809) - SPDIF1TX2MIX Input 1 Volume*/

 R2176 (0x880) - EQ1MIX Input 1 Source */

 R2177 (0x881) - EQ1MIX Input 1 Volume */

 R2178 (0x882) - EQ1MIX Input 2 Source */

 R2179 (0x883) - EQ1MIX Input 2 Volume */

 R2180 (0x884) - EQ1MIX Input 3 Source */

 R2181 (0x885) - EQ1MIX Input 3 Volume */

 R2182 (0x886) - EQ1MIX Input 4 Source */

 R2183 (0x887) - EQ1MIX Input 4 Volume */

 R2184 (0x888) - EQ2MIX Input 1 Source */

 R2185 (0x889) - EQ2MIX Input 1 Volume */

 R2186 (0x88a) - EQ2MIX Input 2 Source */

 R2187 (0x88b) - EQ2MIX Input 2 Volume */

 R2188 (0x88c) - EQ2MIX Input 3 Source */

 R2189 (0x88d) - EQ2MIX Input 3 Volume */

 R2190 (0x88e) - EQ2MIX Input 4 Source */

 R2191 (0x88f) - EQ2MIX Input 4 Volume */

 R2192 (0x890) - EQ3MIX Input 1 Source */

 R2193 (0x891) - EQ3MIX Input 1 Volume */

 R2194 (0x892) - EQ3MIX Input 2 Source */

 R2195 (0x893) - EQ3MIX Input 2 Volume */

 R2196 (0x894) - EQ3MIX Input 3 Source */

 R2197 (0x895) - EQ3MIX Input 3 Volume */

 R2198 (0x896) - EQ3MIX Input 4 Source */

 R2199 (0x897) - EQ3MIX Input 4 Volume */

 R2200 (0x898) - EQ4MIX Input 1 Source */

 R2201 (0x899) - EQ4MIX Input 1 Volume */

 R2202 (0x89a) - EQ4MIX Input 2 Source */

 R2203 (0x89b) - EQ4MIX Input 2 Volume */

 R2204 (0x89c) - EQ4MIX Input 3 Source */

 R2205 (0x89d) - EQ4MIX Input 3 Volume */

 R2206 (0x89e) - EQ4MIX Input 4 Source */

 R2207 (0x89f) - EQ4MIX Input 4 Volume */

 R2240 (0x8c0) - DRC1LMIX Input 1 Source */

 R2241 (0x8c1) - DRC1LMIX Input 1 Volume */

 R2242 (0x8c2) - DRC1LMIX Input 2 Source */

 R2243 (0x8c3) - DRC1LMIX Input 2 Volume */

 R2244 (0x8c4) - DRC1LMIX Input 3 Source */

 R2245 (0x8c5) - DRC1LMIX Input 3 Volume */

 R2246 (0x8c6) - DRC1LMIX Input 4 Source */

 R2247 (0x8c7) - DRC1LMIX Input 4 Volume */

 R2248 (0x8c8) - DRC1RMIX Input 1 Source */

 R2249 (0x8c9) - DRC1RMIX Input 1 Volume */

 R2250 (0x8ca) - DRC1RMIX Input 2 Source */

 R2251 (0x8cb) - DRC1RMIX Input 2 Volume */

 R2252 (0x8cc) - DRC1RMIX Input 3 Source */

 R2253 (0x8cd) - DRC1RMIX Input 3 Volume */

 R2254 (0x8ce) - DRC1RMIX Input 4 Source */

 R2255 (0x8cf) - DRC1RMIX Input 4 Volume */

 R2256 (0x8d0) - DRC2LMIX Input 1 Source */

 R2257 (0x8d1) - DRC2LMIX Input 1 Volume */

 R2258 (0x8d2) - DRC2LMIX Input 2 Source */

 R2259 (0x8d3) - DRC2LMIX Input 2 Volume */

 R2260 (0x8d4) - DRC2LMIX Input 3 Source */

 R2261 (0x8d5) - DRC2LMIX Input 3 Volume */

 R2262 (0x8d6) - DRC2LMIX Input 4 Source */

 R2263 (0x8d7) - DRC2LMIX Input 4 Volume */

 R2264 (0x8d8) - DRC2RMIX Input 1 Source */

 R2265 (0x8d9) - DRC2RMIX Input 1 Volume */

 R2266 (0x8da) - DRC2RMIX Input 2 Source */

 R2267 (0x8db) - DRC2RMIX Input 2 Volume */

 R2268 (0x8dc) - DRC2RMIX Input 3 Source */

 R2269 (0x8dd) - DRC2RMIX Input 3 Volume */

 R2270 (0x8de) - DRC2RMIX Input 4 Source */

 R2271 (0x8df) - DRC2RMIX Input 4 Volume */

 R2304 (0x900) - HPLP1MIX Input 1 Source */

 R2305 (0x901) - HPLP1MIX Input 1 Volume */

 R2306 (0x902) - HPLP1MIX Input 2 Source */

 R2307 (0x903) - HPLP1MIX Input 2 Volume */

 R2308 (0x904) - HPLP1MIX Input 3 Source */

 R2309 (0x905) - HPLP1MIX Input 3 Volume */

 R2310 (0x906) - HPLP1MIX Input 4 Source */

 R2311 (0x907) - HPLP1MIX Input 4 Volume */

 R2312 (0x908) - HPLP2MIX Input 1 Source */

 R2313 (0x909) - HPLP2MIX Input 1 Volume */

 R2314 (0x90a) - HPLP2MIX Input 2 Source */

 R2315 (0x90b) - HPLP2MIX Input 2 Volume */

 R2316 (0x90c) - HPLP2MIX Input 3 Source */

 R2317 (0x90d) - HPLP2MIX Input 3 Volume */

 R2318 (0x90e) - HPLP2MIX Input 4 Source */

 R2319 (0x90f) - HPLP2MIX Input 4 Volume */

 R2320 (0x910) - HPLP3MIX Input 1 Source */

 R2321 (0x911) - HPLP3MIX Input 1 Volume */

 R2322 (0x912) - HPLP3MIX Input 2 Source */

 R2323 (0x913) - HPLP3MIX Input 2 Volume */

 R2324 (0x914) - HPLP3MIX Input 3 Source */

 R2325 (0x915) - HPLP3MIX Input 3 Volume */

 R2326 (0x916) - HPLP3MIX Input 4 Source */

 R2327 (0x917) - HPLP3MIX Input 4 Volume */

 R2328 (0x918) - HPLP4MIX Input 1 Source */

 R2329 (0x919) - HPLP4MIX Input 1 Volume */

 R2330 (0x91a) - HPLP4MIX Input 2 Source */

 R2331 (0x91b) - HPLP4MIX Input 2 Volume */

 R2332 (0x91c) - HPLP4MIX Input 3 Source */

 R2333 (0x91d) - HPLP4MIX Input 3 Volume */

 R2334 (0x91e) - HPLP4MIX Input 4 Source */

 R2335 (0x91f) - HPLP4MIX Input 4 Volume */

 R2368 (0x940) - DSP1LMIX Input 1 Source */

 R2369 (0x941) - DSP1LMIX Input 1 Volume */

 R2370 (0x942) - DSP1LMIX Input 2 Source */

 R2371 (0x943) - DSP1LMIX Input 2 Volume */

 R2372 (0x944) - DSP1LMIX Input 3 Source */

 R2373 (0x945) - DSP1LMIX Input 3 Volume */

 R2374 (0x946) - DSP1LMIX Input 4 Source */

 R2375 (0x947) - DSP1LMIX Input 4 Volume */

 R2376 (0x948) - DSP1RMIX Input 1 Source */

 R2377 (0x949) - DSP1RMIX Input 1 Volume */

 R2378 (0x94a) - DSP1RMIX Input 2 Source */

 R2379 (0x94b) - DSP1RMIX Input 2 Volume */

 R2380 (0x94c) - DSP1RMIX Input 3 Source */

 R2381 (0x94d) - DSP1RMIX Input 3 Volume */

 R2382 (0x94e) - DSP1RMIX Input 4 Source */

 R2383 (0x94f) - DSP1RMIX Input 4 Volume */

 R2384 (0x950) - DSP1AUX1MIX Input 1 Source */

 R2392 (0x958) - DSP1AUX2MIX Input 1 Source */

 R2400 (0x960) - DSP1AUX3MIX Input 1 Source */

 R2408 (0x968) - DSP1AUX4MIX Input 1 Source */

 R2416 (0x970) - DSP1AUX5MIX Input 1 Source */

 R2424 (0x978) - DSP1AUX6MIX Input 1 Source */

 R2432 (0x980) - DSP2LMIX Input 1 Source */

 R2433 (0x981) - DSP2LMIX Input 1 Volume */

 R2434 (0x982) - DSP2LMIX Input 2 Source */

 R2435 (0x983) - DSP2LMIX Input 2 Volume */

 R2436 (0x984) - DSP2LMIX Input 3 Source */

 R2437 (0x985) - DSP2LMIX Input 3 Volume */

 R2438 (0x986) - DSP2LMIX Input 4 Source */

 R2439 (0x987) - DSP2LMIX Input 4 Volume */

 R2440 (0x988) - DSP2RMIX Input 1 Source */

 R2441 (0x989) - DSP2RMIX Input 1 Volume */

 R2442 (0x98a) - DSP2RMIX Input 2 Source */

 R2443 (0x98b) - DSP2RMIX Input 2 Volume */

 R2444 (0x98c) - DSP2RMIX Input 3 Source */

 R2445 (0x98d) - DSP2RMIX Input 3 Volume */

 R2446 (0x98e) - DSP2RMIX Input 4 Source */

 R2447 (0x98f) - DSP2RMIX Input 4 Volume */

 R2448 (0x990) - DSP2AUX1MIX Input 1 Source */

 R2456 (0x998) - DSP2AUX2MIX Input 1 Source */

 R2464 (0x9a0) - DSP2AUX3MIX Input 1 Source */

 R2472 (0x9a8) - DSP2AUX4MIX Input 1 Source */

 R2480 (0x9b0) - DSP2AUX5MIX Input 1 Source */

 R2488 (0x9b8) - DSP2AUX6MIX Input 1 Source */

 R2496 (0x9c0) - DSP3LMIX Input 1 Source */

 R2497 (0x9c1) - DSP3LMIX Input 1 Volume */

 R2498 (0x9c2) - DSP3LMIX Input 2 Source */

 R2499 (0x9c3) - DSP3LMIX Input 2 Volume */

 R2500 (0x9c4) - DSP3LMIX Input 3 Source */

 R2501 (0x9c5) - DSP3LMIX Input 3 Volume */

 R2502 (0x9c6) - DSP3LMIX Input 4 Source */

 R2503 (0x9c7) - DSP3LMIX Input 4 Volume */

 R2504 (0x9c8) - DSP3RMIX Input 1 Source */

 R2505 (0x9c9) - DSP3RMIX Input 1 Volume */

 R2506 (0x9ca) - DSP3RMIX Input 2 Source */

 R2507 (0x9cb) - DSP3RMIX Input 2 Volume */

 R2508 (0x9cc) - DSP3RMIX Input 3 Source */

 R2509 (0x9cd) - DSP3RMIX Input 3 Volume */

 R2510 (0x9ce) - DSP3RMIX Input 4 Source */

 R2511 (0x9cf) - DSP3RMIX Input 4 Volume */

 R2512 (0x9d0) - DSP3AUX1MIX Input 1 Source */

 R2520 (0x9d8) - DSP3AUX2MIX Input 1 Source */

 R2528 (0x9e0) - DSP3AUX3MIX Input 1 Source */

 R2536 (0x9e8) - DSP3AUX4MIX Input 1 Source */

 R2544 (0x9f0) - DSP3AUX5MIX Input 1 Source */

 R2552 (0x9f8) - DSP3AUX6MIX Input 1 Source */

 R2816 (0xb00) - ISRC1DEC1MIX Input 1 Source*/

 R2824 (0xb08) - ISRC1DEC2MIX Input 1 Source*/

 R2832 (0xb10) - ISRC1DEC3MIX Input 1 Source*/

 R2840 (0xb18) - ISRC1DEC4MIX Input 1 Source*/

 R2848 (0xb20) - ISRC1INT1MIX Input 1 Source*/

 R2856 (0xb28) - ISRC1INT2MIX Input 1 Source*/

 R2864 (0xb30) - ISRC1INT3MIX Input 1 Source*/

 R2872 (0xb38) - ISRC1INT4MIX Input 1 Source*/

 R2880 (0xb40) - ISRC2DEC1MIX Input 1 Source*/

 R2888 (0xb48) - ISRC2DEC2MIX Input 1 Source*/

 R2896 (0xb50) - ISRC2DEC3MIX Input 1 Source*/

 R2904 (0xb58) - ISRC2DEC4MIX Input 1 Source*/

 R2912 (0xb60) - ISRC2INT1MIX Input 1 Source*/

 R2920 (0xb68) - ISRC2INT2MIX Input 1 Source*/

 R2928 (0xb70) - ISRC2INT3MIX Input 1 Source*/

 R2936 (0xb78) - ISRC2INT4MIX Input 1 Source*/

 R3584 (0xe00) - FX Ctrl1 */

 R3600 (0xe10) - EQ1_1 */

 R3601 (0xe11) - EQ1_2 */

 R3602 (0xe12) - EQ1_3 */

 R3603 (0xe13) - EQ1_4 */

 R3604 (0xe14) - EQ1_5 */

 R3605 (0xe15) - EQ1_6 */

 R3606 (0xe16) - EQ1_7 */

 R3607 (0xe17) - EQ1_8 */

 R3608 (0xe18) - EQ1_9 */

 R3609 (0xe19) - EQ1_10 */

 R3610 (0xe1a) - EQ1_11 */

 R3611 (0xe1b) - EQ1_12 */

 R3612 (0xe1c) - EQ1_13 */

 R3613 (0xe1d) - EQ1_14 */

 R3614 (0xe1e) - EQ1_15 */

 R3615 (0xe1f) - EQ1_16 */

 R3616 (0xe20) - EQ1_17 */

 R3617 (0xe21) - EQ1_18 */

 R3618 (0xe22) - EQ1_19 */

 R3619 (0xe23) - EQ1_20 */

 R3620 (0xe24) - EQ1_21 */

 R3622 (0xe26) - EQ2_1 */

 R3623 (0xe27) - EQ2_2 */

 R3624 (0xe28) - EQ2_3 */

 R3625 (0xe29) - EQ2_4 */

 R3626 (0xe2a) - EQ2_5 */

 R3627 (0xe2b) - EQ2_6 */

 R3628 (0xe2c) - EQ2_7 */

 R3629 (0xe2d) - EQ2_8 */

 R3630 (0xe2e) - EQ2_9 */

 R3631 (0xe2f) - EQ2_10 */

 R3632 (0xe30) - EQ2_11 */

 R3633 (0xe31) - EQ2_12 */

 R3634 (0xe32) - EQ2_13 */

 R3635 (0xe33) - EQ2_14 */

 R3636 (0xe34) - EQ2_15 */

 R3637 (0xe35) - EQ2_16 */

 R3638 (0xe36) - EQ2_17 */

 R3639 (0xe37) - EQ2_18 */

 R3640 (0xe38) - EQ2_19 */

 R3641 (0xe39) - EQ2_20 */

 R3642 (0xe3a) - EQ2_21 */

 R3644 (0xe3c) - EQ3_1 */

 R3645 (0xe3d) - EQ3_2 */

 R3646 (0xe3e) - EQ3_3 */

 R3647 (0xe3f) - EQ3_4 */

 R3648 (0xe40) - EQ3_5 */

 R3649 (0xe41) - EQ3_6 */

 R3650 (0xe42) - EQ3_7 */

 R3651 (0xe43) - EQ3_8 */

 R3652 (0xe44) - EQ3_9 */

 R3653 (0xe45) - EQ3_10 */

 R3654 (0xe46) - EQ3_11 */

 R3655 (0xe47) - EQ3_12 */

 R3656 (0xe48) - EQ3_13 */

 R3657 (0xe49) - EQ3_14 */

 R3658 (0xe4a) - EQ3_15 */

 R3659 (0xe4b) - EQ3_16 */

 R3660 (0xe4c) - EQ3_17 */

 R3661 (0xe4d) - EQ3_18 */

 R3662 (0xe4e) - EQ3_19 */

 R3663 (0xe4f) - EQ3_20 */

 R3664 (0xe50) - EQ3_21 */

 R3666 (0xe52) - EQ4_1 */

 R3667 (0xe53) - EQ4_2 */

 R3668 (0xe54) - EQ4_3 */

 R3669 (0xe55) - EQ4_4 */

 R3670 (0xe56) - EQ4_5 */

 R3671 (0xe57) - EQ4_6 */

 R3672 (0xe58) - EQ4_7 */

 R3673 (0xe59) - EQ4_8 */

 R3674 (0xe5a) - EQ4_9 */

 R3675 (0xe5b) - EQ4_10 */

 R3676 (0xe5c) - EQ4_11 */

 R3677 (0xe5d) - EQ4_12 */

 R3678 (0xe5e) - EQ4_13 */

 R3679 (0xe5f) - EQ4_14 */

 R3680 (0xe60) - EQ4_15 */

 R3681 (0xe61) - EQ4_16 */

 R3682 (0xe62) - EQ4_17 */

 R3683 (0xe63) - EQ4_18 */

 R3684 (0xe64) - EQ4_19 */

 R3685 (0xe65) - EQ4_20 */

 R3686 (0xe66) - EQ4_21 */

 R3712 (0xe80) - DRC1 ctrl1 */

 R3713 (0xe81) - DRC1 ctrl2 */

 R3714 (0xe82) - DRC1 ctrl3 */

 R3715 (0xe83) - DRC1 ctrl4 */

 R3716 (0xe84) - DRC1 ctrl5 */

 R3720 (0xe88) - DRC2 ctrl1 */

 R3721 (0xe89) - DRC2 ctrl2 */

 R3722 (0xe8a) - DRC2 ctrl3 */

 R3723 (0xe8b) - DRC2 ctrl4 */

 R3724 (0xe8c) - DRC2 ctrl5 */

 R3776 (0xec0) - HPLPF1_1 */

 R3777 (0xec1) - HPLPF1_2 */

 R3780 (0xec4) - HPLPF2_1 */

 R3781 (0xec5) - HPLPF2_2 */

 R3784 (0xec8) - HPLPF3_1 */

 R3785 (0xec9) - HPLPF3_2 */

 R3788 (0xecc) - HPLPF4_1 */

 R3789 (0xecd) - HPLPF4_2 */

 R3824 (0xef0) - ISRC 1 CTRL 1 */

 R3825 (0xef1) - ISRC 1 CTRL 2 */

 R3826 (0xef2) - ISRC 1 CTRL 3 */

 R3827 (0xef3) - ISRC 2 CTRL 1 */

 R3828 (0xef4) - ISRC 2 CTRL 2 */

 R3829 (0xef5) - ISRC 2 CTRL 3 */

 R5888 (0x1700) - GPIO1 Control 1 */

 R5889 (0x1701) - GPIO1 Control 2 */

 R5890 (0x1702) - GPIO2 Control 1 */

 R5891 (0x1703) - GPIO2 Control 2 */

 R5892 (0x1704) - GPIO3 Control 1 */

 R5893 (0x1705) - GPIO3 Control 2 */

 R5894 (0x1706) - GPIO4 Control 1 */

 R5895 (0x1707) - GPIO4 Control 2 */

 R5896 (0x1708) - GPIO5 Control 1 */

 R5897 (0x1709) - GPIO5 Control 2 */

 R5898 (0x170a) - GPIO6 Control 1 */

 R5899 (0x170b) - GPIO6 Control 2 */

 R5900 (0x170c) - GPIO7 Control 1 */

 R5901 (0x170d) - GPIO7 Control 2 */

 R5902 (0x170e) - GPIO8 Control 1 */

 R5903 (0x170f) - GPIO8 Control 2 */

 R5904 (0x1710) - GPIO9 Control 1 */

 R5905 (0x1711) - GPIO9 Control 2 */

 R5906 (0x1712) - GPIO10 Control 1 */

 R5907 (0x1713) - GPIO10 Control 2 */

 R5908 (0x1714) - GPIO11 Control 1 */

 R5909 (0x1715) - GPIO11 Control 2 */

 R5910 (0x1716) - GPIO12 Control 1 */

 R5911 (0x1717) - GPIO12 Control 2 */

 R5912 (0x1718) - GPIO13 Control 1 */

 R5913 (0x1719) - GPIO13 Control 2 */

 R5914 (0x171a) - GPIO14 Control 1 */

 R5915 (0x171b) - GPIO14 Control 2 */

 R5916 (0x171c) - GPIO15 Control 1 */

 R5917 (0x171d) - GPIO15 Control 2 */

 R5918 (0x171e) - GPIO16 Control 1 */

 R5919 (0x171f) - GPIO16 Control 2 */

 R6208 (0x1840) - IRQ1 Mask 1 */

 R6209 (0x1841) - IRQ1 Mask 2 */

 R6210 (0x1842) - IRQ1 Mask 3 */

 R6211 (0x1843) - IRQ1 Mask 4 */

 R6212 (0x1844) - IRQ1 Mask 5 */

 R6213 (0x1845) - IRQ1 Mask 6 */

 R6214 (0x1846) - IRQ1 Mask 7 */

 R6215 (0x1847) - IRQ1 Mask 8 */

 R6216 (0x1848) - IRQ1 Mask 9 */

 R6217 (0x1849) - IRQ1 Mask 10 */

 R6218 (0x184a) - IRQ1 Mask 11 */

 R6219 (0x184b) - IRQ1 Mask 12 */

 R6220 (0x184c) - IRQ1 Mask 13 */

 R6221 (0x184d) - IRQ1 Mask 14 */

 R6222 (0x184e) - IRQ1 Mask 15 */

 R6223 (0x184f) - IRQ1 Mask 16 */

 R6224 (0x1850) - IRQ1 Mask 17 */

 R6225 (0x1851) - IRQ1 Mask 18 */

 R6226 (0x1852) - IRQ1 Mask 19 */

 R6227 (0x1853) - IRQ1 Mask 20 */

 R6228 (0x1854) - IRQ1 Mask 21 */

 R6229 (0x1855) - IRQ1 Mask 22 */

 R6230 (0x1856) - IRQ1 Mask 23 */

 R6231 (0x1857) - IRQ1 Mask 24 */

 R6232 (0x1858) - IRQ1 Mask 25 */

 R6233 (0x1859) - IRQ1 Mask 26 */

 R6234 (0x185a) - IRQ1 Mask 27 */

 R6235 (0x185b) - IRQ1 Mask 28 */

 R6236 (0x185c) - IRQ1 Mask 29 */

 R6237 (0x185d) - IRQ1 Mask 30 */

 R6238 (0x185e) - IRQ1 Mask 31 */

 R6239 (0x185f) - IRQ1 Mask 32 */

 R6240 (0x1860) - IRQ1 Mask 33 */

 R6662 (0x1a06) - Interrupt Debounce 7 */

 R6784 (0x1a80) - IRQ1 CTRL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/drivers/mfd/ucb1x00-assabet.c

 *

 *  Copyright (C) 2001-2003 Russell King, All Rights Reserved.

 *

 *  We handle the machine-specific bits of the UCB1x00 driver here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright(c) 2009 Ian Molton <spyro@f2s.com>

 Enable access to MMC CTL regs. (flag in COMMAND_REG)*/

 Enable the MMC/SD Control registers */

 Disable SD power during suspend */

 The below is required but why? FIXME */

 Power down SD bus */

 Enable the MMC/SD Control registers */

 SPDX-License-Identifier: GPL-2.0+



 Interrupt controller support for MAX8998



 Copyright (C) 2010 Samsung Electronics Co.Ltd

 Author: Joonyoung Shim <jy0922.shim@samsung.com>

		/*

		 * If there's been a change in the mask write it back

		 * to the hardware.

 Apply masking */

 Report */

 Mask the individual interrupt sources */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * RDC321x MFD southbridge driver

 *

 * Copyright (C) 2007-2010 Florian Fainelli <florian@openwrt.org>

 * Copyright (C) 2010 Bernhard Loos <bernhardloos@googlemail.com>

 SPDX-License-Identifier: GPL-2.0+

/* I2C support for Dialog DA9063

 *

 * Copyright 2012 Dialog Semiconductor Ltd.

 * Copyright 2013 Philipp Zabel, Pengutronix

 *

 * Author: Krystian Garbaciak, Dialog Semiconductor

/*

 * Raw I2C access required for just accessing chip and variant info before we

 * know which device is present. The info read from the device using this

 * approach is then used to select the correct regmap tables.

 Determine page info based on register address */

 Write reg address, page selection */

 Select register address */

 Read data */

/*

 * Variant specific regmap configs

 If SMBus is not available and only I2C is possible, enter I2C mode */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Azoteq IQS620A/621/622/624/625 Multi-Function Sensors

 *

 * Copyright (C) 2019 Jeff LaBundy <jeff@labundy.com>

 *

 * These devices rely on application-specific register settings and calibration

 * data developed in and exported from a suite of GUIs offered by the vendor. A

 * separate tool converts the GUIs' ASCII-based output into a standard firmware

 * file parsed by the driver.

 *

 * Link to datasheets and GUIs: https://www.azoteq.com/

 *

 * Link to conversion tool: https://github.com/jlabundy/iqs62x-h2bin.git

		/*

		 * In case ATI is in progress, wait for it to complete before

		 * lowering the core clock frequency.

		/*

		 * The IQS625 default interval divider is below the minimum

		 * permissible value, and the datasheet mandates that it is

		 * corrected during initialization (unless an updated value

		 * has already been provided by firmware).

		 *

		 * To protect against an unacceptably low user-entered value

		 * stored in the firmware, the same check is extended to the

		 * IQS624 as well.

	/*

	 * Place the device in streaming mode at first so as not to miss the

	 * limited number of interrupts that would otherwise occur after ATI

	 * completes. The device is subsequently placed in event mode by the

	 * interrupt handler.

	 *

	 * In the meantime, mask interrupts during ATI to prevent the device

	 * from soliciting I2C traffic until the noise-sensitive ATI process

	 * is complete.

	/*

	 * The following delay gives the device time to deassert its RDY output

	 * in case a communication window was open while the REDO_ATI field was

	 * written. This prevents an interrupt from being serviced prematurely.

	/*

	 * The device asserts the RDY output to signal the beginning of a

	 * communication window, which is closed by an I2C stop condition.

	 * As such, all interrupt status is captured in a single read and

	 * broadcast to any interested sub-device drivers.

	/*

	 * The device resets itself in response to the I2C master stalling

	 * communication past a fixed timeout. In this case, all registers

	 * are restored and any interested sub-device drivers are notified.

	/*

	 * Reset and ATI events are not broadcast to the sub-device drivers

	 * until ATI has completed. Any other events that may have occurred

	 * during ATI are ignored.

	/*

	 * Once the communication window is closed, a small delay is added to

	 * ensure the device's RDY output has been deasserted by the time the

	 * interrupt handler returns.

 0x10 */

 0x12 */

 0x13 */

 0x16 */

 0x10 */

 0x13 */

 0x16 */

 0x10 */

 0x12 */

 0x13 */

 0x16 */

 0x17 */

 0x18 */

 0x19 */

 0x10 */

 0x12 */

 0x14 */

 0x16 */

 0x17 */

 0x18 */

 0x19 */

 0x10 */

 0x13 */

 0x14 */

 0x16 */

 0x17 */

 0x18 */

 0x19 */

 0x10 */

 0x12 */

 0x14 */

 0x16 */

 0x17 */

 0x18 */

 0x10 */

 0x11 */

 0x12 */

	/*

	 * The following sequence validates the device's product and software

	 * numbers. It then determines if the device is factory-calibrated by

	 * checking for nonzero values in the device's designated calibration

	 * registers (if applicable). Depending on the device, the absence of

	 * calibration data indicates a reduced feature set or invalid device.

	 *

	 * For devices given in both calibrated and uncalibrated versions, the

	 * calibrated version (e.g. IQS620AT) appears first in the iqs62x_devs

	 * array. The uncalibrated version (e.g. IQS620A) appears next and has

	 * the same product and software numbers, but no calibration registers

	 * are specified.

		/*

		 * Read each of the device's designated calibration registers,

		 * if any, and exit from the inner loop early if any are equal

		 * to zero (indicating the device is uncalibrated). This could

		 * be acceptable depending on the device (e.g. IQS620A instead

		 * of IQS620AT).

		/*

		 * If the number of nonzero values read from the device equals

		 * the number of designated calibration registers (which could

		 * be zero), exit from the outer loop early to signal that the

		 * device's product and software numbers match a known device,

		 * and the device is calibrated (if applicable).

	/*

	 * As per the datasheet, automatic mode switching must be disabled

	 * before the device is placed in or taken out of halt mode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * cs5535-mfd.c - core MFD driver for CS5535/CS5536 southbridges

 *

 * The CS5535 and CS5536 has an ISA bridge on the PCI bus that is

 * used for accessing GPIOs, MFGPTs, ACPI, etc.  Each subdevice has

 * an IO range that's specified in a single BAR.  The BAR order is

 * hardcoded in the CS553x specifications.

 *

 * Copyright (c) 2010  Andres Salomon <dilinger@queued.net>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2009-2010 Pengutronix

 * Uwe Kleine-Koenig <u.kleine-koenig@pengutronix.de>

 *

 * loosely based on an earlier driver that has

 * Copyright 2009 Pengutronix, Sascha Hauer <s.hauer@pengutronix.de>

 sentinel */

 sentinel */ }

 include errata fix for spi audio problems */

/*

 * We cannot use regmap-spi generic bus implementation here.

 * The MC13783 chip will get corrupted if CS signal is deasserted

 * and on i.Mx31 SoC (the target SoC for MC13783 PMIC) the SPI controller

 * has the following errata (DSPhl22960):

 * "The CSPI negates SS when the FIFO becomes empty with

 * SSCTL= 0. Software cannot guarantee that the FIFO will not

 * drain because of higher priority interrupts and the

 * non-realtime characteristics of the operating system. As a

 * result, the SS will negate before all of the data has been

 * transferred to/from the peripheral."

 * We workaround this by accessing the SPI controller with a

 * single transfert.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DB8500 PRCM Unit driver

 *

 * Copyright (C) STMicroelectronics 2009

 * Copyright (C) ST-Ericsson SA 2010

 *

 * Author: Kumar Sanghvi <kumar.sanghvi@stericsson.com>

 * Author: Sundar Iyer <sundar.iyer@stericsson.com>

 * Author: Mattias Nilsson <mattias.i.nilsson@stericsson.com>

 *

 * U8500 PRCM Unit interface driver

 Index of different voltages to be used when accessing AVSData */

 4 BYTES */

 2 bytes */

 16 bytes */

 Req Mailboxes */

 12 bytes  */

 12 bytes  */

 16 bytes  */

 372 bytes  */

 4 bytes  */

 4 bytes  */

 Ack Mailboxes */

 52 bytes  */

 4 bytes */

 4 bytes */

 4 bytes */

 4 bytes */

 4 bytes */

 Mailbox 0 headers */

 Mailbox 0 REQs */

 Mailbox 0 ACKs */

 Mailbox 1 headers */

 Mailbox 1 Requests */

 Mailbox 1 ACKs */

 Mailbox 2 headers */

 Mailbox 2 REQs */

 Mailbox 2 ACKs */

 Mailbox 3 headers */

 Mailbox 3 Requests */

 Mailbox 4 headers */

 Mailbox 4 Requests */

 Mailbox 5 Requests */

 Mailbox 5 ACKs */

/*

 * Wakeups/IRQs

/*

 * This vector maps irq numbers to the bits in the bit field used in

 * communication with the PRCMU firmware.

 *

 * The reason for having this is to keep the irq numbers contiguous even though

 * the bits in the bit field are not. (The bits also have a tendency to move

 * around, to further complicate matters.)

/*

 * mb0_transfer - state needed for mailbox 0 communication.

 * @lock:		The transaction lock.

 * @dbb_events_lock:	A lock used to handle concurrent access to (parts of)

 *			the request data.

 * @mask_work:		Work structure used for (un)masking wakeup interrupts.

 * @req:		Request data that need to persist between requests.

/*

 * mb1_transfer - state needed for mailbox 1 communication.

 * @lock:	The transaction lock.

 * @work:	The transaction completion structure.

 * @ape_opp:	The current APE OPP.

 * @ack:	Reply ("acknowledge") data.

/*

 * mb2_transfer - state needed for mailbox 2 communication.

 * @lock:            The transaction lock.

 * @work:            The transaction completion structure.

 * @auto_pm_lock:    The autonomous power management configuration lock.

 * @auto_pm_enabled: A flag indicating whether autonomous PM is enabled.

 * @req:             Request data that need to persist between requests.

 * @ack:             Reply ("acknowledge") data.

/*

 * mb3_transfer - state needed for mailbox 3 communication.

 * @lock:		The request lock.

 * @sysclk_lock:	A lock used to handle concurrent sysclk requests.

 * @sysclk_work:	Work structure used for sysclk requests.

/*

 * mb4_transfer - state needed for mailbox 4 communication.

 * @lock:	The transaction lock.

 * @work:	The transaction completion structure.

/*

 * mb5_transfer - state needed for mailbox 5 communication.

 * @lock:	The transaction lock.

 * @work:	The transaction completion structure.

 * @ack:	Reply ("acknowledge") data.

 Spinlocks */

 Global var to runtime determine TCDM base for v2 or v1 */

/**

 * prcmu_set_rc_a2p - This function is used to run few power state sequences

 * @val: Value to be set, i.e. transition requested

 * Returns: 0 on success, -EINVAL on invalid argument

 *

 * This function is used to run the following power state sequences -

 * any state to ApReset,  ApDeepSleep to ApExecute, ApExecute to ApDeepSleep

/**

 * prcmu_get_rc_p2a - This function is used to get power state sequences

 * Returns: the power transition that has last happened

 *

 * This function can return the following transitions-

 * any state to ApReset,  ApDeepSleep to ApExecute, ApExecute to ApDeepSleep

/**

 * prcmu_get_xp70_current_state - Return the current XP70 power mode

 * Returns: Returns the current AP(ARM) power mode: init,

 * apBoot, apExecute, apDeepSleep, apSleep, apIdle, apReset

/**

 * prcmu_config_clkout - Configure one of the programmable clock outputs.

 * @clkout:	The CLKOUT number (0 or 1).

 * @source:	The clock to be used (one of the PRCMU_CLKSRC_*).

 * @div:	The divider to be applied.

 *

 * Configures one of the programmable clock outputs (CLKOUTs).

 * @div should be in the range [1,63] to request a configuration, or 0 to

 * inform that the configuration is no longer requested.

 This function should only be called while mb0_transfer.lock is held. */

/**

 * db8500_prcmu_set_arm_opp - set the appropriate ARM OPP

 * @opp: The new ARM operating point to which transition is to be made

 * Returns: 0 on success, non-zero on failure

 *

 * This function sets the the operating point of the ARM.

/**

 * db8500_prcmu_get_arm_opp - get the current ARM OPP

 *

 * Returns: the current ARM OPP

/**

 * db8500_prcmu_get_ddr_opp - get the current DDR OPP

 *

 * Returns: the current DDR OPP

 Divide the frequency of certain clocks by 2 for APE_50_PARTLY_25_OPP. */

 Grab the HW semaphore. */

 Release the HW semaphore. */

/**

 * db8500_prcmu_set_ape_opp - set the appropriate APE OPP

 * @opp: The new APE operating point to which transition is to be made

 * Returns: 0 on success, non-zero on failure

 *

 * This function sets the operating point of the APE.

/**

 * db8500_prcmu_get_ape_opp - get the current APE OPP

 *

 * Returns: the current APE OPP

/**

 * db8500_prcmu_request_ape_opp_100_voltage - Request APE OPP 100% voltage

 * @enable: true to request the higher voltage, false to drop a request.

 *

 * Calls to this function to enable and disable requests must be balanced.

/**

 * prcmu_release_usb_wakeup_state - release the state required by a USB wakeup

 *

 * This function releases the power state requirements of a USB wakeup.

/**

 * db8500_prcmu_set_epod - set the state of a EPOD (power domain)

 * @epod_id: The EPOD to set

 * @epod_state: The new EPOD state

 *

 * This function sets the state of a EPOD (power domain). It may not be called

 * from interrupt context.

 check argument */

 set flag if retention is possible */

 check argument */

 get lock */

 wait for mailbox */

 fill in mailbox */

	/*

	 * The current firmware version does not handle errors correctly,

	 * and we cannot recover if there is an error.

	 * This is expected to change when the firmware is updated.

/**

 * prcmu_configure_auto_pm - Configure autonomous power management.

 * @sleep: Configuration for ApSleep.

 * @idle:  Configuration for ApIdle.

	/*

	 * The autonomous power management configuration is done through

	 * fields in mailbox 2, but these fields are only used as shared

	 * variables - i.e. there is no need to send a message.

	/*

	 * The firmware only sends an ACK if we want to enable the

	 * SysClk, and it succeeds.

	/*

	 * On the U8420_CLKSEL firmware, the ULP (Ultra Low Power)

	 * PLL is disabled so we cannot use doze mode, this will

	 * stop the clock on this firmware.

 Grab the HW semaphore. */

 Release the HW semaphore. */

/**

 * db8500_prcmu_request_clock() - Request for a clock to be enabled or disabled.

 * @clock:      The clock for which the request is made.

 * @enable:     Whether the clock should be enabled (true) or disabled (false).

 *

 * This function should only be used by the clock implementation.

 * Do not use it from any other place!

 External ARMCLKFIX clock */

 Check PRCM_ARM_CHGCLKREQ divider */

 Check PRCM_ARMCLKFIX_MGT divider */

 ARM PLL */

 The DB8520 has slightly higher ARMSS max frequency */

 Find the corresponding arm opp from the cpufreq table. */

 Return the last valid value, even if a match was not found. */

 Grab the HW semaphore. */

 Release the HW semaphore. */

 Find the corresponding arm opp from the cpufreq table. */

 Set the new arm opp. */

 else */	PRCM_DSI_PLLOUT_SEL_PHI_4;

/*

 * timeout is 28 bit, in ms.

			    /*

			     * Put the lowest 28 bits of timeout at

			     * offset 4. Four first bits are used for id.

/**

 * prcmu_abb_read() - Read register value(s) from the ABB.

 * @slave:	The I2C slave address.

 * @reg:	The (start) register address.

 * @value:	The read out value(s).

 * @size:	The number of registers to read.

 *

 * Reads register value(s) from the ABB.

 * @size has to be 1 for the current firmware version.

/**

 * prcmu_abb_write_masked() - Write masked register value(s) to the ABB.

 * @slave:	The I2C slave address.

 * @reg:	The (start) register address.

 * @value:	The value(s) to write.

 * @mask:	The mask(s) to use.

 * @size:	The number of registers to write.

 *

 * Writes masked register value(s) to the ABB.

 * For each @value, only the bits set to 1 in the corresponding @mask

 * will be written. The other bits are not changed.

 * @size has to be 1 for the current firmware version.

/**

 * prcmu_abb_write() - Write register value(s) to the ABB.

 * @slave:	The I2C slave address.

 * @reg:	The (start) register address.

 * @value:	The value(s) to write.

 * @size:	The number of registers to write.

 *

 * Writes register value(s) to the ABB.

 * @size has to be 1 for the current firmware version.

/**

 * prcmu_ac_wake_req - should be called whenever ARM wants to wakeup Modem

	/*

	 * Force Modem Wake-up before hostaccess_req ping-pong.

	 * It prevents Modem to enter in Sleep while acking the hostaccess

	 * request. The 31us delay has been calculated by HWI.

/**

 * prcmu_ac_sleep_req - called when ARM no longer needs to talk to modem

/**

 * db8500_prcmu_system_reset - System reset

 *

 * Saves the reset reason code and then sets the APE_SOFTRST register which

 * fires interrupt to fw

 *

 * @reset_code: The reason for system reset

/**

 * db8500_prcmu_get_reset_code - Retrieve SW reset reason code

 *

 * Retrieves the reset reason code stored by prcmu_system_reset() before

 * last restart.

/**

 * db8500_prcmu_modem_reset - ask the PRCMU to reset modem

	/*

	 * No need to check return from PRCMU as modem should go in reset state

	 * This state is already managed by upper layer

 All wakeups will be used, so create mappings for all */

	/*

	 * This is a temporary remap to bring up the clocks. It is

	 * subsequently replaces with a real remap. After the merge of

	 * the mailbox subsystem all of this early code goes away, and the

	 * clock driver can probe independently. An early initcall will

	 * still be needed, but it can be diverted into drivers/clk/ux500.

/*

 * Power domain switches (ePODs) modeled as regulators for the DB8500 SoC

 "v-mmc" changed to "vcore" in the mainline kernel */

 "v-uart" changed to "vcore" in the mainline kernel */

 AV8100 regulator */

 SVA MMDSP regulator switch */

 SVA pipe regulator switch */

 SIA MMDSP regulator switch */

 SIA pipe regulator switch */

 ESRAM1 and 2 regulator switch */

 ESRAM3 and 4 regulator switch */

 dependency to u8500-vape is handled outside regulator framework */

 "ret" means "retention" */

 dependency to u8500-vape is handled outside regulator framework */

 dependency to u8500-vape is handled outside regulator framework */

 dependency to u8500-vape is handled outside regulator framework */

		/*

		 * esram12 is set in retention and supplied by Vsafe when Vape is off,

		 * no need to hold Vape

		/*

		 * esram34 is set in retention and supplied by Vsafe when Vape is off,

		 * no need to hold Vape

 Look up the device node, sneak the IRQ out of it */

 Clean up the mailbox interrupts after pre-kernel code. */

 TODO: Remove restriction when clk definitions are available. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8994-regmap.c  --  Register map data for WM8994 series devices

 *

 * Copyright 2011 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 R1    - Power Management (1) */

 R2    - Power Management (2) */

 R3    - Power Management (3) */

 R4    - Power Management (4) */

 R5    - Power Management (5) */

 R6    - Power Management (6) */

 R21   - Input Mixer (1) */

 R24   - Left Line Input 1&2 Volume */

 R25   - Left Line Input 3&4 Volume */

 R26   - Right Line Input 1&2 Volume */

 R27   - Right Line Input 3&4 Volume */

 R28   - Left Output Volume */

 R29   - Right Output Volume */

 R30   - Line Outputs Volume */

 R31   - HPOUT2 Volume */

 R32   - Left OPGA Volume */

 R33   - Right OPGA Volume */

 R34   - SPKMIXL Attenuation */

 R35   - SPKMIXR Attenuation */

 R36   - SPKOUT Mixers */

 R37   - ClassD */

 R38   - Speaker Volume Left */

 R39   - Speaker Volume Right */

 R40   - Input Mixer (2) */

 R41   - Input Mixer (3) */

 R42   - Input Mixer (4) */

 R43   - Input Mixer (5) */

 R44   - Input Mixer (6) */

 R45   - Output Mixer (1) */

 R46   - Output Mixer (2) */

 R47   - Output Mixer (3) */

 R48   - Output Mixer (4) */

 R49   - Output Mixer (5) */

 R50   - Output Mixer (6) */

 R51   - HPOUT2 Mixer */

 R52   - Line Mixer (1) */

 R53   - Line Mixer (2) */

 R54   - Speaker Mixer */

 R55   - Additional Control */

 R56   - AntiPOP (1) */

 R57   - AntiPOP (2) */

 R59   - LDO 1 */

 R60   - LDO 2 */

 R61   - MICBIAS1 */

 R62   - MICBIAS2 */

 R76   - Charge Pump (1) */

 R77   - Charge Pump (2) */

 R81   - Class W (1) */

 R85   - DC Servo (2) */

 R89   - DC Servo (4) */

 R96   - Analogue HP (1) */

 R197  - Class D Test (5) */

 R208  - Mic Detect 1 */

 R209  - Mic Detect 2 */

 R257  - Control Interface */

 R512  - AIF1 Clocking (1) */

 R513  - AIF1 Clocking (2) */

 R516  - AIF2 Clocking (1) */

 R517  - AIF2 Clocking (2) */

 R520  - Clocking (1) */

 R521  - Clocking (2) */

 R528  - AIF1 Rate */

 R529  - AIF2 Rate */

 R544  - FLL1 Control (1) */

 R545  - FLL1 Control (2) */

 R546  - FLL1 Control (3) */

 R547  - FLL1 Control (4) */

 R548  - FLL1 Control (5) */

 R550  - FLL1 EFS 1 */

 R551  - FLL1 EFS 2 */

 R576  - FLL2Control (1) */

 R577  - FLL2Control (2) */

 R578  - FLL2Control (3) */

 R579  - FLL2 Control (4) */

 R580  - FLL2Control (5) */

 R582  - FLL2 EFS 1 */

 R583  - FLL2 EFS 2 */

 R768  - AIF1 Control (1) */

 R769  - AIF1 Control (2) */

 R770  - AIF1 Master/Slave */

 R771  - AIF1 BCLK */

 R772  - AIF1ADC LRCLK */

 R773  - AIF1DAC LRCLK */

 R774  - AIF1DAC Data */

 R775  - AIF1ADC Data */

 R784  - AIF2 Control (1) */

 R785  - AIF2 Control (2) */

 R786  - AIF2 Master/Slave */

 R787  - AIF2 BCLK */

 R788  - AIF2ADC LRCLK */

 R789  - AIF2DAC LRCLK */

 R790  - AIF2DAC Data */

 R791  - AIF2ADC Data */

 R792  - AIF2TX Control */

 R800  - AIF3 Control (1) */

 R801  - AIF3 Control (2) */

 R802  - AIF3DAC Data */

 R803  - AIF3ADC Data */

 R1024 - AIF1 ADC1 Left Volume */

 R1025 - AIF1 ADC1 Right Volume */

 R1026 - AIF1 DAC1 Left Volume */

 R1027 - AIF1 DAC1 Right Volume */

 R1040 - AIF1 ADC1 Filters */

 R1041 - AIF1 ADC2 Filters */

 R1056 - AIF1 DAC1 Filters (1) */

 R1057 - AIF1 DAC1 Filters (2) */

 R1058 - AIF1 DAC2 Filters (1) */

 R1059 - AIF1 DAC2 Filters (2) */

 R1072 - AIF1 DAC1 Noise Gate */

 R1073 - AIF1 DAC2 Noise Gate */

 R1088 - AIF1 DRC1 (1) */

 R1089 - AIF1 DRC1 (2) */

 R1090 - AIF1 DRC1 (3) */

 R1091 - AIF1 DRC1 (4) */

 R1092 - AIF1 DRC1 (5) */

 R1104 - AIF1 DRC2 (1) */

 R1105 - AIF1 DRC2 (2) */

 R1106 - AIF1 DRC2 (3) */

 R1107 - AIF1 DRC2 (4) */

 R1108 - AIF1 DRC2 (5) */

 R1152 - AIF1 DAC1 EQ Gains (1) */

 R1153 - AIF1 DAC1 EQ Gains (2) */

 R1154 - AIF1 DAC1 EQ Band 1 A */

 R1155 - AIF1 DAC1 EQ Band 1 B */

 R1156 - AIF1 DAC1 EQ Band 1 PG */

 R1157 - AIF1 DAC1 EQ Band 2 A */

 R1158 - AIF1 DAC1 EQ Band 2 B */

 R1159 - AIF1 DAC1 EQ Band 2 C */

 R1160 - AIF1 DAC1 EQ Band 2 PG */

 R1161 - AIF1 DAC1 EQ Band 3 A */

 R1162 - AIF1 DAC1 EQ Band 3 B */

 R1163 - AIF1 DAC1 EQ Band 3 C */

 R1164 - AIF1 DAC1 EQ Band 3 PG */

 R1165 - AIF1 DAC1 EQ Band 4 A */

 R1166 - AIF1 DAC1 EQ Band 4 B */

 R1167 - AIF1 DAC1 EQ Band 4 C */

 R1168 - AIF1 DAC1 EQ Band 4 PG */

 R1169 - AIF1 DAC1 EQ Band 5 A */

 R1170 - AIF1 DAC1 EQ Band 5 B */

 R1171 - AIF1 DAC1 EQ Band 5 PG */

 R1172 - AIF1 DAC1 EQ Band 1 C */

 R1184 - AIF1 DAC2 EQ Gains (1) */

 R1185 - AIF1 DAC2 EQ Gains (2) */

 R1186 - AIF1 DAC2 EQ Band 1 A */

 R1187 - AIF1 DAC2 EQ Band 1 B */

 R1188 - AIF1 DAC2 EQ Band 1 PG */

 R1189 - AIF1 DAC2 EQ Band 2 A */

 R1190 - AIF1 DAC2 EQ Band 2 B */

 R1191 - AIF1 DAC2 EQ Band 2 C */

 R1192 - AIF1 DAC2 EQ Band 2 PG */

 R1193 - AIF1 DAC2 EQ Band 3 A */

 R1194 - AIF1 DAC2 EQ Band 3 B */

 R1195 - AIF1 DAC2 EQ Band 3 C */

 R1196 - AIF1 DAC2 EQ Band 3 PG */

 R1197 - AIF1 DAC2 EQ Band 4 A */

 R1198 - AIF1 DAC2 EQ Band 4 B */

 R1199 - AIF1 DAC2 EQ Band 4 C */

 R1200 - AIF1 DAC2 EQ Band 4 PG */

 R1201 - AIF1 DAC2 EQ Band 5 A */

 R1202 - AIF1 DAC2 EQ Band 5 B */

 R1203 - AIF1 DAC2 EQ Band 5 PG */

 R1204 - AIF1 DAC2 EQ Band 1 C */

 R1280 - AIF2 ADC Left Volume */

 R1281 - AIF2 ADC Right Volume */

 R1282 - AIF2 DAC Left Volume */

 R1283 - AIF2 DAC Right Volume */

 R1296 - AIF2 ADC Filters */

 R1312 - AIF2 DAC Filters (1) */

 R1313 - AIF2 DAC Filters (2) */

 R1328 - AIF2 DAC Noise Gate */

 R1344 - AIF2 DRC (1) */

 R1345 - AIF2 DRC (2) */

 R1346 - AIF2 DRC (3) */

 R1347 - AIF2 DRC (4) */

 R1348 - AIF2 DRC (5) */

 R1408 - AIF2 EQ Gains (1) */

 R1409 - AIF2 EQ Gains (2) */

 R1410 - AIF2 EQ Band 1 A */

 R1411 - AIF2 EQ Band 1 B */

 R1412 - AIF2 EQ Band 1 PG */

 R1413 - AIF2 EQ Band 2 A */

 R1414 - AIF2 EQ Band 2 B */

 R1415 - AIF2 EQ Band 2 C */

 R1416 - AIF2 EQ Band 2 PG */

 R1417 - AIF2 EQ Band 3 A */

 R1418 - AIF2 EQ Band 3 B */

 R1419 - AIF2 EQ Band 3 C */

 R1420 - AIF2 EQ Band 3 PG */

 R1421 - AIF2 EQ Band 4 A */

 R1422 - AIF2 EQ Band 4 B */

 R1423 - AIF2 EQ Band 4 C */

 R1424 - AIF2 EQ Band 4 PG */

 R1425 - AIF2 EQ Band 5 A */

 R1426 - AIF2 EQ Band 5 B */

 R1427 - AIF2 EQ Band 5 PG */

 R1428 - AIF2 EQ Band 1 C */

 R1536 - DAC1 Mixer Volumes */

 R1537 - DAC1 Left Mixer Routing */

 R1538 - DAC1 Right Mixer Routing */

 R1539 - AIF2ADC Mixer Volumes */

 R1540 - AIF2ADC Left Mixer Routing */

 R1541 - AIF2ADC Right Mixer Routing */

 R1542 - AIF1 ADC1 Left Mixer Routing */

 R1543 - AIF1 ADC1 Right Mixer Routing */

 R1544 - AIF1 ADC2 Left Mixer Routing */

 R1545 - AIF1 ADC2 Right Mixer Routing */

 R1552 - DAC1 Left Volume */

 R1553 - DAC1 Right Volume */

 R1554 - AIF2TX Left Volume */

 R1555 - AIF2TX Right Volume */

 R1556 - DAC Softmute */

 R1568 - Oversampling */

 R1569 - Sidetone */

 R1792 - GPIO 1 */

 R1793 - Pull Control (MCLK2) */

 R1794 - Pull Control (BCLK2) */

 R1795 - Pull Control (DACLRCLK2) */

 R1796 - Pull Control (DACDAT2) */

 R1799 - GPIO 8 */

 R1800 - GPIO 9 */

 R1801 - GPIO 10 */

 R1802 - GPIO 11 */

 R1824 - Pull Control (1) */

 R1825 - Pull Control (2) */

 R1842 - Interrupt Raw Status 2 */

 R1848 - Interrupt Status 1 Mask */

 R1849 - Interrupt Status 2 Mask */

 R1856 - Interrupt Control */

 R1864 - IRQ Debounce */

 R1     - Power Management (1) */ 

 R2     - Power Management (2) */ 

 R3     - Power Management (3) */ 

 R4     - Power Management (4) */ 

 R5     - Power Management (5) */ 

 R6     - Power Management (6) */ 

 R21    - Input Mixer (1) */ 

 R24    - Left Line Input 1&2 Volume */ 

 R25    - Left Line Input 3&4 Volume */ 

 R26    - Right Line Input 1&2 Volume */ 

 R27    - Right Line Input 3&4 Volume */ 

 R28    - Left Output Volume */ 

 R29    - Right Output Volume */ 

 R30    - Line Outputs Volume */ 

 R31    - HPOUT2 Volume */ 

 R32    - Left OPGA Volume */ 

 R33    - Right OPGA Volume */ 

 R34    - SPKMIXL Attenuation */ 

 R35    - SPKMIXR Attenuation */ 

 R36    - SPKOUT Mixers */ 

 R37    - ClassD */ 

 R38    - Speaker Volume Left */ 

 R39    - Speaker Volume Right */ 

 R40    - Input Mixer (2) */ 

 R41    - Input Mixer (3) */ 

 R42    - Input Mixer (4) */ 

 R43    - Input Mixer (5) */ 

 R44    - Input Mixer (6) */ 

 R45    - Output Mixer (1) */ 

 R46    - Output Mixer (2) */ 

 R47    - Output Mixer (3) */ 

 R48    - Output Mixer (4) */ 

 R49    - Output Mixer (5) */ 

 R50    - Output Mixer (6) */ 

 R51    - HPOUT2 Mixer */ 

 R52    - Line Mixer (1) */ 

 R53    - Line Mixer (2) */ 

 R54    - Speaker Mixer */ 

 R55    - Additional Control */ 

 R56    - AntiPOP (1) */ 

 R57    - AntiPOP (2) */ 

 R58    - MICBIAS */ 

 R59    - LDO 1 */ 

 R60    - LDO 2 */ 

 R76    - Charge Pump (1) */ 

 R81    - Class W (1) */ 

 R85    - DC Servo (2) */ 

 R87    - DC Servo (4) */ 

 R96    - Analogue HP (1) */ 

 R257   - Control Interface */ 

 R272   - Write Sequencer Ctrl (1) */ 

 R273   - Write Sequencer Ctrl (2) */ 

 R512   - AIF1 Clocking (1) */ 

 R513   - AIF1 Clocking (2) */ 

 R516   - AIF2 Clocking (1) */ 

 R517   - AIF2 Clocking (2) */ 

 R520   - Clocking (1) */ 

 R521   - Clocking (2) */ 

 R528   - AIF1 Rate */ 

 R529   - AIF2 Rate */ 

 R544   - FLL1 Control (1) */ 

 R545   - FLL1 Control (2) */ 

 R546   - FLL1 Control (3) */ 

 R547   - FLL1 Control (4) */ 

 R548   - FLL1 Control (5) */ 

 R576   - FLL2 Control (1) */ 

 R577   - FLL2 Control (2) */ 

 R578   - FLL2 Control (3) */ 

 R579   - FLL2 Control (4) */ 

 R580   - FLL2 Control (5) */ 

 R768   - AIF1 Control (1) */ 

 R769   - AIF1 Control (2) */ 

 R770   - AIF1 Master/Slave */ 

 R771   - AIF1 BCLK */ 

 R772   - AIF1ADC LRCLK */ 

 R773   - AIF1DAC LRCLK */ 

 R774   - AIF1DAC Data */ 

 R775   - AIF1ADC Data */ 

 R784   - AIF2 Control (1) */ 

 R785   - AIF2 Control (2) */ 

 R786   - AIF2 Master/Slave */ 

 R787   - AIF2 BCLK */ 

 R788   - AIF2ADC LRCLK */ 

 R789   - AIF2DAC LRCLK */ 

 R790   - AIF2DAC Data */ 

 R791   - AIF2ADC Data */ 

 R1024  - AIF1 ADC1 Left Volume */ 

 R1025  - AIF1 ADC1 Right Volume */ 

 R1026  - AIF1 DAC1 Left Volume */ 

 R1027  - AIF1 DAC1 Right Volume */ 

 R1028  - AIF1 ADC2 Left Volume */ 

 R1029  - AIF1 ADC2 Right Volume */ 

 R1030  - AIF1 DAC2 Left Volume */ 

 R1031  - AIF1 DAC2 Right Volume */ 

 R1040  - AIF1 ADC1 Filters */ 

 R1041  - AIF1 ADC2 Filters */ 

 R1056  - AIF1 DAC1 Filters (1) */ 

 R1057  - AIF1 DAC1 Filters (2) */ 

 R1058  - AIF1 DAC2 Filters (1) */ 

 R1059  - AIF1 DAC2 Filters (2) */ 

 R1088  - AIF1 DRC1 (1) */ 

 R1089  - AIF1 DRC1 (2) */ 

 R1090  - AIF1 DRC1 (3) */ 

 R1091  - AIF1 DRC1 (4) */ 

 R1092  - AIF1 DRC1 (5) */ 

 R1104  - AIF1 DRC2 (1) */ 

 R1105  - AIF1 DRC2 (2) */ 

 R1106  - AIF1 DRC2 (3) */ 

 R1107  - AIF1 DRC2 (4) */ 

 R1108  - AIF1 DRC2 (5) */ 

 R1152  - AIF1 DAC1 EQ Gains (1) */ 

 R1153  - AIF1 DAC1 EQ Gains (2) */ 

 R1154  - AIF1 DAC1 EQ Band 1 A */ 

 R1155  - AIF1 DAC1 EQ Band 1 B */ 

 R1156  - AIF1 DAC1 EQ Band 1 PG */ 

 R1157  - AIF1 DAC1 EQ Band 2 A */ 

 R1158  - AIF1 DAC1 EQ Band 2 B */ 

 R1159  - AIF1 DAC1 EQ Band 2 C */ 

 R1160  - AIF1 DAC1 EQ Band 2 PG */ 

 R1161  - AIF1 DAC1 EQ Band 3 A */ 

 R1162  - AIF1 DAC1 EQ Band 3 B */ 

 R1163  - AIF1 DAC1 EQ Band 3 C */ 

 R1164  - AIF1 DAC1 EQ Band 3 PG */ 

 R1165  - AIF1 DAC1 EQ Band 4 A */ 

 R1166  - AIF1 DAC1 EQ Band 4 B */ 

 R1167  - AIF1 DAC1 EQ Band 4 C */ 

 R1168  - AIF1 DAC1 EQ Band 4 PG */ 

 R1169  - AIF1 DAC1 EQ Band 5 A */ 

 R1170  - AIF1 DAC1 EQ Band 5 B */ 

 R1171  - AIF1 DAC1 EQ Band 5 PG */ 

 R1184  - AIF1 DAC2 EQ Gains (1) */ 

 R1185  - AIF1 DAC2 EQ Gains (2) */ 

 R1186  - AIF1 DAC2 EQ Band 1 A */ 

 R1187  - AIF1 DAC2 EQ Band 1 B */ 

 R1188  - AIF1 DAC2 EQ Band 1 PG */ 

 R1189  - AIF1 DAC2 EQ Band 2 A */ 

 R1190  - AIF1 DAC2 EQ Band 2 B */ 

 R1191  - AIF1 DAC2 EQ Band 2 C */ 

 R1192  - AIF1 DAC2 EQ Band 2 PG */ 

 R1193  - AIF1 DAC2 EQ Band 3 A */ 

 R1194  - AIF1 DAC2 EQ Band 3 B */ 

 R1195  - AIF1 DAC2 EQ Band 3 C */ 

 R1196  - AIF1 DAC2 EQ Band 3 PG */ 

 R1197  - AIF1 DAC2 EQ Band 4 A */ 

 R1198  - AIF1 DAC2 EQ Band 4 B */ 

 R1199  - AIF1 DAC2 EQ Band 4 C */ 

 R1200  - AIF1 DAC2 EQ Band 4 PG */ 

 R1201  - AIF1 DAC2 EQ Band 5 A */ 

 R1202  - AIF1 DAC2 EQ Band 5 B */ 

 R1203  - AIF1 DAC2 EQ Band 5 PG */ 

 R1280  - AIF2 ADC Left Volume */ 

 R1281  - AIF2 ADC Right Volume */ 

 R1282  - AIF2 DAC Left Volume */ 

 R1283  - AIF2 DAC Right Volume */ 

 R1296  - AIF2 ADC Filters */ 

 R1312  - AIF2 DAC Filters (1) */ 

 R1313  - AIF2 DAC Filters (2) */ 

 R1344  - AIF2 DRC (1) */ 

 R1345  - AIF2 DRC (2) */ 

 R1346  - AIF2 DRC (3) */ 

 R1347  - AIF2 DRC (4) */ 

 R1348  - AIF2 DRC (5) */ 

 R1408  - AIF2 EQ Gains (1) */ 

 R1409  - AIF2 EQ Gains (2) */ 

 R1410  - AIF2 EQ Band 1 A */ 

 R1411  - AIF2 EQ Band 1 B */ 

 R1412  - AIF2 EQ Band 1 PG */ 

 R1413  - AIF2 EQ Band 2 A */ 

 R1414  - AIF2 EQ Band 2 B */ 

 R1415  - AIF2 EQ Band 2 C */ 

 R1416  - AIF2 EQ Band 2 PG */ 

 R1417  - AIF2 EQ Band 3 A */ 

 R1418  - AIF2 EQ Band 3 B */ 

 R1419  - AIF2 EQ Band 3 C */ 

 R1420  - AIF2 EQ Band 3 PG */ 

 R1421  - AIF2 EQ Band 4 A */ 

 R1422  - AIF2 EQ Band 4 B */ 

 R1423  - AIF2 EQ Band 4 C */ 

 R1424  - AIF2 EQ Band 4 PG */ 

 R1425  - AIF2 EQ Band 5 A */ 

 R1426  - AIF2 EQ Band 5 B */ 

 R1427  - AIF2 EQ Band 5 PG */ 

 R1536  - DAC1 Mixer Volumes */ 

 R1537  - DAC1 Left Mixer Routing */ 

 R1538  - DAC1 Right Mixer Routing */ 

 R1539  - DAC2 Mixer Volumes */ 

 R1540  - DAC2 Left Mixer Routing */ 

 R1541  - DAC2 Right Mixer Routing */ 

 R1542  - AIF1 ADC1 Left Mixer Routing */ 

 R1543  - AIF1 ADC1 Right Mixer Routing */ 

 R1544  - AIF1 ADC2 Left Mixer Routing */ 

 R1545  - AIF1 ADC2 Right mixer Routing */ 

 R1552  - DAC1 Left Volume */ 

 R1553  - DAC1 Right Volume */ 

 R1554  - DAC2 Left Volume */ 

 R1555  - DAC2 Right Volume */ 

 R1556  - DAC Softmute */ 

 R1568  - Oversampling */ 

 R1569  - Sidetone */ 

 R1792  - GPIO 1 */ 

 R1793  - GPIO 2 */ 

 R1794  - GPIO 3 */ 

 R1795  - GPIO 4 */ 

 R1796  - GPIO 5 */ 

 R1797  - GPIO 6 */ 

 R1798  - GPIO 7 */ 

 R1799  - GPIO 8 */ 

 R1800  - GPIO 9 */ 

 R1801  - GPIO 10 */ 

 R1802  - GPIO 11 */ 

 R1824  - Pull Control (1) */ 

 R1825  - Pull Control (2) */ 

 R1848  - Interrupt Status 1 Mask */ 

 R1849  - Interrupt Status 2 Mask */ 

 R1856  - Interrupt Control */ 

 R1864  - IRQ Debounce */ 

 R1     - Power Management (1) */

 R2     - Power Management (2) */

 R3     - Power Management (3) */

 R4     - Power Management (4) */

 R5     - Power Management (5) */

 R6     - Power Management (6) */

 R21    - Input Mixer (1) */

 R24    - Left Line Input 1&2 Volume */

 R25    - Left Line Input 3&4 Volume */

 R26    - Right Line Input 1&2 Volume */

 R27    - Right Line Input 3&4 Volume */

 R28    - Left Output Volume */

 R29    - Right Output Volume */

 R30    - Line Outputs Volume */

 R31    - HPOUT2 Volume */

 R32    - Left OPGA Volume */

 R33    - Right OPGA Volume */

 R34    - SPKMIXL Attenuation */

 R35    - SPKMIXR Attenuation */

 R36    - SPKOUT Mixers */

 R37    - ClassD */

 R38    - Speaker Volume Left */

 R39    - Speaker Volume Right */

 R40    - Input Mixer (2) */

 R41    - Input Mixer (3) */

 R42    - Input Mixer (4) */

 R43    - Input Mixer (5) */

 R44    - Input Mixer (6) */

 R45    - Output Mixer (1) */

 R46    - Output Mixer (2) */

 R47    - Output Mixer (3) */

 R48    - Output Mixer (4) */

 R49    - Output Mixer (5) */

 R50    - Output Mixer (6) */

 R51    - HPOUT2 Mixer */

 R52    - Line Mixer (1) */

 R53    - Line Mixer (2) */

 R54    - Speaker Mixer */

 R55    - Additional Control */

 R56    - AntiPOP (1) */

 R57    - AntiPOP (2) */

 R59    - LDO 1 */

 R60    - LDO 2 */

 R61    - MICBIAS1 */

 R62    - MICBIAS2 */

 R76    - Charge Pump (1) */

 R77    - Charge Pump (2) */

 R81    - Class W (1) */

 R85    - DC Servo (2) */

 R87    - DC Servo (4) */

 R96    - Analogue HP (1) */

 R197   - Class D Test (5) */

 R208   - Mic Detect 1 */

 R209   - Mic Detect 2 */

 R257   - Control Interface */

 R272   - Write Sequencer Ctrl (1) */

 R273   - Write Sequencer Ctrl (2) */

 R512   - AIF1 Clocking (1) */

 R513   - AIF1 Clocking (2) */

 R516   - AIF2 Clocking (1) */

 R517   - AIF2 Clocking (2) */

 R520   - Clocking (1) */

 R521   - Clocking (2) */

 R528   - AIF1 Rate */

 R529   - AIF2 Rate */

 R544   - FLL1 Control (1) */

 R545   - FLL1 Control (2) */

 R546   - FLL1 Control (3) */

 R547   - FLL1 Control (4) */

 R548   - FLL1 Control (5) */

 R550   - FLL1 EFS 1 */

 R551   - FLL1 EFS 2 */

 R576   - FLL2Control (1) */

 R577   - FLL2Control (2) */

 R578   - FLL2Control (3) */

 R579   - FLL2 Control (4) */

 R580   - FLL2Control (5) */

 R582   - FLL2 EFS 1 */

 R583   - FLL2 EFS 2 */

 R768   - AIF1 Control (1) */

 R769   - AIF1 Control (2) */

 R770   - AIF1 Master/Slave */

 R771   - AIF1 BCLK */

 R772   - AIF1ADC LRCLK */

 R773   - AIF1DAC LRCLK */

 R774   - AIF1DAC Data */

 R775   - AIF1ADC Data */

 R784   - AIF2 Control (1) */

 R785   - AIF2 Control (2) */

 R786   - AIF2 Master/Slave */

 R787   - AIF2 BCLK */

 R788   - AIF2ADC LRCLK */

 R789   - AIF2DAC LRCLK */

 R790   - AIF2DAC Data */

 R791   - AIF2ADC Data */

 R800   - AIF3 Control (1) */

 R801   - AIF3 Control (2) */

 R802   - AIF3DAC Data */

 R803   - AIF3ADC Data */

 R1024  - AIF1 ADC1 Left Volume */

 R1025  - AIF1 ADC1 Right Volume */

 R1026  - AIF1 DAC1 Left Volume */

 R1027  - AIF1 DAC1 Right Volume */

 R1028  - AIF1 ADC2 Left Volume */

 R1029  - AIF1 ADC2 Right Volume */

 R1030  - AIF1 DAC2 Left Volume */

 R1031  - AIF1 DAC2 Right Volume */

 R1040  - AIF1 ADC1 Filters */

 R1041  - AIF1 ADC2 Filters */

 R1056  - AIF1 DAC1 Filters (1) */

 R1057  - AIF1 DAC1 Filters (2) */

 R1058  - AIF1 DAC2 Filters (1) */

 R1059  - AIF1 DAC2 Filters (2) */

 R1072  - AIF1 DAC1 Noise Gate */

 R1073  - AIF1 DAC2 Noise Gate */

 R1088  - AIF1 DRC1 (1) */

 R1089  - AIF1 DRC1 (2) */

 R1090  - AIF1 DRC1 (3) */

 R1091  - AIF1 DRC1 (4) */

 R1092  - AIF1 DRC1 (5) */

 R1104  - AIF1 DRC2 (1) */

 R1105  - AIF1 DRC2 (2) */

 R1106  - AIF1 DRC2 (3) */

 R1107  - AIF1 DRC2 (4) */

 R1108  - AIF1 DRC2 (5) */

 R1152  - AIF1 DAC1 EQ Gains (1) */

 R1153  - AIF1 DAC1 EQ Gains (2) */

 R1154  - AIF1 DAC1 EQ Band 1 A */

 R1155  - AIF1 DAC1 EQ Band 1 B */

 R1156  - AIF1 DAC1 EQ Band 1 PG */

 R1157  - AIF1 DAC1 EQ Band 2 A */

 R1158  - AIF1 DAC1 EQ Band 2 B */

 R1159  - AIF1 DAC1 EQ Band 2 C */

 R1160  - AIF1 DAC1 EQ Band 2 PG */

 R1161  - AIF1 DAC1 EQ Band 3 A */

 R1162  - AIF1 DAC1 EQ Band 3 B */

 R1163  - AIF1 DAC1 EQ Band 3 C */

 R1164  - AIF1 DAC1 EQ Band 3 PG */

 R1165  - AIF1 DAC1 EQ Band 4 A */

 R1166  - AIF1 DAC1 EQ Band 4 B */

 R1167  - AIF1 DAC1 EQ Band 4 C */

 R1168  - AIF1 DAC1 EQ Band 4 PG */

 R1169  - AIF1 DAC1 EQ Band 5 A */

 R1170  - AIF1 DAC1 EQ Band 5 B */

 R1171  - AIF1 DAC1 EQ Band 5 PG */

 R1172  - AIF1 DAC1 EQ Band 1 C */

 R1184  - AIF1 DAC2 EQ Gains (1) */

 R1185  - AIF1 DAC2 EQ Gains (2) */

 R1186  - AIF1 DAC2 EQ Band 1 A */

 R1187  - AIF1 DAC2 EQ Band 1 B */

 R1188  - AIF1 DAC2 EQ Band 1 PG */

 R1189  - AIF1 DAC2 EQ Band 2 A */

 R1190  - AIF1 DAC2 EQ Band 2 B */

 R1191  - AIF1 DAC2 EQ Band 2 C */

 R1192  - AIF1 DAC2 EQ Band 2 PG */

 R1193  - AIF1 DAC2 EQ Band 3 A */

 R1194  - AIF1 DAC2 EQ Band 3 B */

 R1195  - AIF1 DAC2 EQ Band 3 C */

 R1196  - AIF1 DAC2 EQ Band 3 PG */

 R1197  - AIF1 DAC2 EQ Band 4 A */

 R1198  - AIF1 DAC2 EQ Band 4 B */

 R1199  - AIF1 DAC2 EQ Band 4 C */

 R1200  - AIF1 DAC2 EQ Band 4 PG */

 R1201  - AIF1 DAC2 EQ Band 5 A */

 R1202  - AIF1 DAC2 EQ Band 5 B */

 R1203  - AIF1 DAC2 EQ Band 5 PG */

 R1204  - AIF1 DAC2EQ Band 1 C */

 R1280  - AIF2 ADC Left Volume */

 R1281  - AIF2 ADC Right Volume */

 R1282  - AIF2 DAC Left Volume */

 R1283  - AIF2 DAC Right Volume */

 R1296  - AIF2 ADC Filters */

 R1312  - AIF2 DAC Filters (1) */

 R1313  - AIF2 DAC Filters (2) */

 R1328  - AIF2 DAC Noise Gate */

 R1344  - AIF2 DRC (1) */

 R1345  - AIF2 DRC (2) */

 R1346  - AIF2 DRC (3) */

 R1347  - AIF2 DRC (4) */

 R1348  - AIF2 DRC (5) */

 R1408  - AIF2 EQ Gains (1) */

 R1409  - AIF2 EQ Gains (2) */

 R1410  - AIF2 EQ Band 1 A */

 R1411  - AIF2 EQ Band 1 B */

 R1412  - AIF2 EQ Band 1 PG */

 R1413  - AIF2 EQ Band 2 A */

 R1414  - AIF2 EQ Band 2 B */

 R1415  - AIF2 EQ Band 2 C */

 R1416  - AIF2 EQ Band 2 PG */

 R1417  - AIF2 EQ Band 3 A */

 R1418  - AIF2 EQ Band 3 B */

 R1419  - AIF2 EQ Band 3 C */

 R1420  - AIF2 EQ Band 3 PG */

 R1421  - AIF2 EQ Band 4 A */

 R1422  - AIF2 EQ Band 4 B */

 R1423  - AIF2 EQ Band 4 C */

 R1424  - AIF2 EQ Band 4 PG */

 R1425  - AIF2 EQ Band 5 A */

 R1426  - AIF2 EQ Band 5 B */

 R1427  - AIF2 EQ Band 5 PG */

 R1428  - AIF2 EQ Band 1 C */

 R1536  - DAC1 Mixer Volumes */

 R1537  - DAC1 Left Mixer Routing */

 R1538  - DAC1 Right Mixer Routing */

 R1539  - DAC2 Mixer Volumes */

 R1540  - DAC2 Left Mixer Routing */

 R1541  - DAC2 Right Mixer Routing */

 R1542  - AIF1 ADC1 Left Mixer Routing */

 R1543  - AIF1 ADC1 Right Mixer Routing */

 R1544  - AIF1 ADC2 Left Mixer Routing */

 R1545  - AIF1 ADC2 Right mixer Routing */

 R1552  - DAC1 Left Volume */

 R1553  - DAC1 Right Volume */

 R1554  - DAC2 Left Volume */

 R1555  - DAC2 Right Volume */

 R1556  - DAC Softmute */

 R1568  - Oversampling */

 R1569  - Sidetone */

 R1792  - GPIO 1 */

 R1793  - Pull Control (MCLK2) */

 R1794  - Pull Control (BCLK2) */

 R1795  - Pull Control (DACLRCLK2) */

 R1796  - Pull Control (DACDAT2) */

 R1797  - GPIO 6 */

 R1799  - GPIO 8 */

 R1800  - GPIO 9 */

 R1801  - GPIO 10 */

 R1802  - GPIO 11 */

 R1824  - Pull Control (1) */

 R1825  - Pull Control (2) */

 R1848  - Interrupt Status 1 Mask */

 R1849  - Interrupt Status 2 Mask */

 R1856  - Interrupt Control */

 R1864  - IRQ Debounce */

 R2304  - DSP2_Program */

 R2305  - DSP2_Config */

 R2573  - DSP2_ExecControl */

 R9216  - MBC Band 1 K (1) */

 R9217  - MBC Band 1 K (2) */

 R9218  - MBC Band 1 N1 (1) */

 R9219  - MBC Band 1 N1 (2) */

 R9220  - MBC Band 1 N2 (1) */

 R9221  - MBC Band 1 N2 (2) */

 R9222  - MBC Band 1 N3 (1) */

 R9223  - MBC Band 1 N3 (2) */

 R9224  - MBC Band 1 N4 (1) */

 R9225  - MBC Band 1 N4 (2) */

 R9226  - MBC Band 1 N5 (1) */

 R9227  - MBC Band 1 N5 (2) */

 R9228  - MBC Band 1 X1 (1) */

 R9229  - MBC Band 1 X1 (2) */

 R9230  - MBC Band 1 X2 (1) */

 R9231  - MBC Band 1 X2 (2) */

 R9232  - MBC Band 1 X3 (1) */

 R9233  - MBC Band 1 X3 (2) */

 R9234  - MBC Band 1 Attack (1) */

 R9235  - MBC Band 1 Attack (2) */

 R9236  - MBC Band 1 Decay (1) */

 R9237  - MBC Band 1 Decay (2) */

 R9238  - MBC Band 2 K (1) */

 R9239  - MBC Band 2 K (2) */

 R9240  - MBC Band 2 N1 (1) */

 R9241  - MBC Band 2 N1 (2) */

 R9242  - MBC Band 2 N2 (1) */

 R9243  - MBC Band 2 N2 (2) */

 R9244  - MBC Band 2 N3 (1) */

 R9245  - MBC Band 2 N3 (2) */

 R9246  - MBC Band 2 N4 (1) */

 R9247  - MBC Band 2 N4 (2) */

 R9248  - MBC Band 2 N5 (1) */

 R9249  - MBC Band 2 N5 (2) */

 R9250  - MBC Band 2 X1 (1) */

 R9251  - MBC Band 2 X1 (2) */

 R9252  - MBC Band 2 X2 (1) */

 R9253  - MBC Band 2 X2 (2) */

 R9254  - MBC Band 2 X3 (1) */

 R9255  - MBC Band 2 X3 (2) */

 R9256  - MBC Band 2 Attack (1) */

 R9257  - MBC Band 2 Attack (2) */

 R9258  - MBC Band 2 Decay (1) */

 R9259  - MBC Band 2 Decay (2) */

 R9260  - MBC_B2_PG2 (1) */

 R9261  - MBC_B2_PG2 (2) */

 R9262  - MBC_B1_PG2 (1) */

 R9263  - MBC_B1_PG2 (2) */

 R9728  - MBC Crossover (1) */

 R9729  - MBC Crossover (2) */

 R9730  - MBC HPF (1) */

 R9731  - MBC HPF (2) */

 R9734  - MBC LPF (1) */

 R9735  - MBC LPF (2) */

 R9738  - MBC RMS Limit (1) */

 R9739  - MBC RMS Limit (2) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014-2015 Pengutronix, Markus Pargmann <mpa@pengutronix.de>

	/*

	 * According to the datasheet the ADC clock should never

	 * exceed 1,75 MHz. Base clock is the IPG and the ADC unit uses

	 * a funny clock divider. To keep the ADC conversion time constant

	 * adapt the ADC internal clock divider to the IPG clock rate.

 adc clock = IPG clock / (2 * div + 2) */

	/*

	 * the ADC clock divider changes its behaviour when values below 4

	 * are used: it is fixed to "/ 10" in this case

 setup clock according to the datasheet */

 Enable clock and reset the component */

 Setup powersaving mode, but enable internal reference voltage */

 Sentinel */ }

 SPDX-License-Identifier: GPL-2.0+



 max77693.c - mfd core driver for the MAX 77693



 Copyright (C) 2012 Samsung Electronics

 SangYoung Son <hello.son@samsung.com>



 This program is not provided / owned by Maxim Integrated Products.



 This driver is based on max8997.c

 Charger, Flash LED */

	/*

	 * Initialize register map for MUIC device because use regmap-muic

	 * instance of MUIC device when irq of max77693 is initialized

	 * before call max77693-muic probe() function.

 Unmask interrupts from all blocks in interrupt source register */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * omap-usb-host.c - The USBHS core driver for OMAP EHCI & OHCI

 *

 * Copyright (C) 2011-2013 Texas Instruments Incorporated - https://www.ti.com

 * Author: Keshava Munegowda <keshava_mgowda@ti.com>

 * Author: Roger Quadros <rogerq@ti.com>

 OMAP USBHOST Register addresses  */

 UHH Register Set */

 OMAP4-specific defines */

 Values of UHH_REVISION - Note: these are not given in the TRM */

 OMAP3 */

 OMAP4 */

-------------------------------------------------------------------------*/

-------------------------------------------------------------------------*/

-------------------------------------------------------------------------*/

/*

 * Map 'enum usbhs_omap_port_mode' found in <linux/platform_data/usb-omap.h>

 * to the device tree binding portN-mode found in

 * 'Documentation/devicetree/bindings/mfd/omap-usb-host.txt'

 as HSIC mode needs utmi_clk */

 as utmi_clks were used in HSIC mode */

 bypass ULPI only if none of the ports use PHY mode */

 Clear port mode fields for PHY mode */

 setup ULPI bypass and burst configurations */

 newer revisions */

 get port modes */

 get 'enum usbhs_omap_port_mode' from port mode string */

 get flags */

/**

 * usbhs_omap_probe - initialize TI-based HCDs

 *

 * Allocates basic resources for this USB host controller.

 *

 * @pdev: Pointer to this device's platform device structure

 For DT boot we populate platform data from OF node */

 Initialize the TLL subsystem */

	/* we need to call runtime suspend before we update omap->nports

	 * to prevent unbalanced clk_disable()

	/*

	 * If platform data contains nports then use that

	 * else make out number of ports from USBHS revision

 Set all clocks as invalid to begin with */

 for OMAP3 i.e. USBHS REV1 */

 for OMAP4+ i.e. USBHS REV2+ */

 clock names are indexed from 1*/

		/* If a clock is not found we won't bail out as not all

		 * platforms have all clocks and we can function without

		 * them

/**

 * usbhs_omap_remove - shutdown processing for UHH & TLL HCDs

 * @pdev: USB Host Controller being removed

 *

 * Reverses the effect of usbhs_omap_probe().

 remove children */

/*

 * init before ehci and ohci drivers;

 * The usbhs core driver should be initialized much before

 * the omap ehci and ohci probe functions are called.

 * This usbhs core driver should be initialized after

 * usb tll driver

 SPDX-License-Identifier: GPL-2.0-only

/*

 * wm8997-tables.c  --  WM8997 data tables

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Charles Keepax <ckeepax@opensource.wolfsonmicro.com>

 We use a function so we can use ARRAY_SIZE() */

 R9     - Ctrl IF I2C1 CFG 1 */

 R22    - Write Sequencer Ctrl 0 */

 R23    - Write Sequencer Ctrl 1 */

 R24    - Write Sequencer Ctrl 2 */

 R32    - Tone Generator 1 */

 R33    - Tone Generator 2 */

 R34    - Tone Generator 3 */

 R35    - Tone Generator 4 */

 R36    - Tone Generator 5 */

 R48    - PWM Drive 1 */

 R49    - PWM Drive 2 */

 R50    - PWM Drive 3 */

 R64    - Wake control */

 R65    - Sequence control */

 R97    - Sample Rate Sequence Select 1 */

 R98    - Sample Rate Sequence Select 2 */

 R99    - Sample Rate Sequence Select 3 */

 R100   - Sample Rate Sequence Select 4 */

 R104   - AlwaysOn Triggers Seq Select 3 */

 R105   - AlwaysOn Triggers Seq Select 4 */

 R106   - AlwaysOn Triggers Seq Select 5 */

 R107   - AlwaysOn Triggers Seq Select 6 */

 R112   - Comfort Noise Generator */

 R144   - Haptics Control 1 */

 R145   - Haptics Control 2 */

 R146   - Haptics phase 1 intensity */

 R147   - Haptics phase 1 duration */

 R148   - Haptics phase 2 intensity */

 R149   - Haptics phase 2 duration */

 R150   - Haptics phase 3 intensity */

 R151   - Haptics phase 3 duration */

 R256   - Clock 32k 1 */

 R257   - System Clock 1 */

 R258   - Sample rate 1 */

 R259   - Sample rate 2 */

 R260   - Sample rate 3 */

 R274   - Async clock 1 */

 R275   - Async sample rate 1 */

 R329   - Output system clock */

 R330   - Output async clock */

 R338   - Rate Estimator 1 */

 R339   - Rate Estimator 2 */

 R340   - Rate Estimator 3 */

 R341   - Rate Estimator 4 */

 R342   - Rate Estimator 5 */

 R353   - Dynamic Frequency Scaling 1 */

 R369   - FLL1 Control 1 */

 R370   - FLL1 Control 2 */

 R371   - FLL1 Control 3 */

 R372   - FLL1 Control 4 */

 R373   - FLL1 Control 5 */

 R374   - FLL1 Control 6 */

 R375   - FLL1 Loop Filter Test 1 */

 R385   - FLL1 Synchroniser 1 */

 R386   - FLL1 Synchroniser 2 */

 R387   - FLL1 Synchroniser 3 */

 R388   - FLL1 Synchroniser 4 */

 R389   - FLL1 Synchroniser 5 */

 R390   - FLL1 Synchroniser 6 */

 R393   - FLL1 Spread Spectrum */

 R394   - FLL1 GPIO Clock */

 R401   - FLL2 Control 1 */

 R402   - FLL2 Control 2 */

 R403   - FLL2 Control 3 */

 R404   - FLL2 Control 4 */

 R405   - FLL2 Control 5 */

 R406   - FLL2 Control 6 */

 R407   - FLL2 Loop Filter Test 1 */

 R417   - FLL2 Synchroniser 1 */

 R418   - FLL2 Synchroniser 2 */

 R419   - FLL2 Synchroniser 3 */

 R420   - FLL2 Synchroniser 4 */

 R421   - FLL2 Synchroniser 5 */

 R422   - FLL2 Synchroniser 6 */

 R425   - FLL2 Spread Spectrum */

 R426   - FLL2 GPIO Clock */

 R512   - Mic Charge Pump 1 */

 R528   - LDO1 Control 1 */

 R530   - LDO1 Control 2 */

 R531   - LDO2 Control 1 */

 R536   - Mic Bias Ctrl 1 */

 R537   - Mic Bias Ctrl 2 */

 R538   - Mic Bias Ctrl 3 */

 R659   - Accessory Detect Mode 1 */

 R667   - Headphone Detect 1 */

 R675   - Mic Detect 1 */

 R676   - Mic Detect 2 */

 R707   - Mic noise mix control 1 */

 R715   - Isolation control */

 R723   - Jack detect analogue */

 R768   - Input Enables */

 R776   - Input Rate */

 R777   - Input Volume Ramp */

 R784   - IN1L Control */

 R785   - ADC Digital Volume 1L */

 R786   - DMIC1L Control */

 R788   - IN1R Control */

 R789   - ADC Digital Volume 1R */

 R790   - DMIC1R Control */

 R792   - IN2L Control */

 R793   - ADC Digital Volume 2L */

 R794   - DMIC2L Control */

 R796   - IN2R Control */

 R797   - ADC Digital Volume 2R */

 R798   - DMIC2R Control */

 R1024  - Output Enables 1 */

 R1032  - Output Rate 1 */

 R1033  - Output Volume Ramp */

 R1040  - Output Path Config 1L */

 R1041  - DAC Digital Volume 1L */

 R1042  - DAC Volume Limit 1L */

 R1043  - Noise Gate Select 1L */

 R1044  - Output Path Config 1R */

 R1045  - DAC Digital Volume 1R */

 R1046  - DAC Volume Limit 1R */

 R1047  - Noise Gate Select 1R */

 R1056  - Output Path Config 3L */

 R1057  - DAC Digital Volume 3L */

 R1058  - DAC Volume Limit 3L */

 R1059  - Noise Gate Select 3L */

 R1064  - Output Path Config 4L */

 R1065  - DAC Digital Volume 4L */

 R1066  - Out Volume 4L */

 R1067  - Noise Gate Select 4L */

 R1072  - Output Path Config 5L */

 R1073  - DAC Digital Volume 5L */

 R1074  - DAC Volume Limit 5L */

 R1075  - Noise Gate Select 5L */

 R1077  - DAC Digital Volume 5R */

 R1078  - DAC Volume Limit 5R */

 R1079  - Noise Gate Select 5R */

 R1104  - DAC AEC Control 1 */

 R1112  - Noise Gate Control */

 R1168  - PDM SPK1 CTRL 1 */

 R1169  - PDM SPK1 CTRL 2 */

 R1280  - AIF1 BCLK Ctrl */

 R1281  - AIF1 Tx Pin Ctrl */

 R1282  - AIF1 Rx Pin Ctrl */

 R1283  - AIF1 Rate Ctrl */

 R1284  - AIF1 Format */

 R1285  - AIF1 Tx BCLK Rate */

 R1286  - AIF1 Rx BCLK Rate */

 R1287  - AIF1 Frame Ctrl 1 */

 R1288  - AIF1 Frame Ctrl 2 */

 R1289  - AIF1 Frame Ctrl 3 */

 R1290  - AIF1 Frame Ctrl 4 */

 R1291  - AIF1 Frame Ctrl 5 */

 R1292  - AIF1 Frame Ctrl 6 */

 R1293  - AIF1 Frame Ctrl 7 */

 R1294  - AIF1 Frame Ctrl 8 */

 R1295  - AIF1 Frame Ctrl 9 */

 R1296  - AIF1 Frame Ctrl 10 */

 R1297  - AIF1 Frame Ctrl 11 */

 R1298  - AIF1 Frame Ctrl 12 */

 R1299  - AIF1 Frame Ctrl 13 */

 R1300  - AIF1 Frame Ctrl 14 */

 R1301  - AIF1 Frame Ctrl 15 */

 R1302  - AIF1 Frame Ctrl 16 */

 R1303  - AIF1 Frame Ctrl 17 */

 R1304  - AIF1 Frame Ctrl 18 */

 R1305  - AIF1 Tx Enables */

 R1306  - AIF1 Rx Enables */

 R1344  - AIF2 BCLK Ctrl */

 R1345  - AIF2 Tx Pin Ctrl */

 R1346  - AIF2 Rx Pin Ctrl */

 R1347  - AIF2 Rate Ctrl */

 R1348  - AIF2 Format */

 R1349  - AIF2 Tx BCLK Rate */

 R1350  - AIF2 Rx BCLK Rate */

 R1351  - AIF2 Frame Ctrl 1 */

 R1352  - AIF2 Frame Ctrl 2 */

 R1353  - AIF2 Frame Ctrl 3 */

 R1354  - AIF2 Frame Ctrl 4 */

 R1361  - AIF2 Frame Ctrl 11 */

 R1362  - AIF2 Frame Ctrl 12 */

 R1369  - AIF2 Tx Enables */

 R1370  - AIF2 Rx Enables */

 R1507  - SLIMbus Framer Ref Gear */

 R1509  - SLIMbus Rates 1 */

 R1510  - SLIMbus Rates 2 */

 R1511  - SLIMbus Rates 3 */

 R1512  - SLIMbus Rates 4 */

 R1513  - SLIMbus Rates 5 */

 R1514  - SLIMbus Rates 6 */

 R1515  - SLIMbus Rates 7 */

 R1516  - SLIMbus Rates 8 */

 R1525  - SLIMbus RX Channel Enable */

 R1526  - SLIMbus TX Channel Enable */

 R1600  - PWM1MIX Input 1 Source */

 R1601  - PWM1MIX Input 1 Volume */

 R1602  - PWM1MIX Input 2 Source */

 R1603  - PWM1MIX Input 2 Volume */

 R1604  - PWM1MIX Input 3 Source */

 R1605  - PWM1MIX Input 3 Volume */

 R1606  - PWM1MIX Input 4 Source */

 R1607  - PWM1MIX Input 4 Volume */

 R1608  - PWM2MIX Input 1 Source */

 R1609  - PWM2MIX Input 1 Volume */

 R1610  - PWM2MIX Input 2 Source */

 R1611  - PWM2MIX Input 2 Volume */

 R1612  - PWM2MIX Input 3 Source */

 R1613  - PWM2MIX Input 3 Volume */

 R1614  - PWM2MIX Input 4 Source */

 R1615  - PWM2MIX Input 4 Volume */

 R1632  - MICMIX Input 1 Source */

 R1633  - MICMIX Input 1 Volume */

 R1634  - MICMIX Input 2 Source */

 R1635  - MICMIX Input 2 Volume */

 R1636  - MICMIX Input 3 Source */

 R1637  - MICMIX Input 3 Volume */

 R1638  - MICMIX Input 4 Source */

 R1639  - MICMIX Input 4 Volume */

 R1640  - NOISEMIX Input 1 Source */

 R1641  - NOISEMIX Input 1 Volume */

 R1642  - NOISEMIX Input 2 Source */

 R1643  - NOISEMIX Input 2 Volume */

 R1644  - NOISEMIX Input 3 Source */

 R1645  - NOISEMIX Input 3 Volume */

 R1646  - NOISEMIX Input 4 Source */

 R1647  - NOISEMIX Input 4 Volume */

 R1664  - OUT1LMIX Input 1 Source */

 R1665  - OUT1LMIX Input 1 Volume */

 R1666  - OUT1LMIX Input 2 Source */

 R1667  - OUT1LMIX Input 2 Volume */

 R1668  - OUT1LMIX Input 3 Source */

 R1669  - OUT1LMIX Input 3 Volume */

 R1670  - OUT1LMIX Input 4 Source */

 R1671  - OUT1LMIX Input 4 Volume */

 R1672  - OUT1RMIX Input 1 Source */

 R1673  - OUT1RMIX Input 1 Volume */

 R1674  - OUT1RMIX Input 2 Source */

 R1675  - OUT1RMIX Input 2 Volume */

 R1676  - OUT1RMIX Input 3 Source */

 R1677  - OUT1RMIX Input 3 Volume */

 R1678  - OUT1RMIX Input 4 Source */

 R1679  - OUT1RMIX Input 4 Volume */

 R1696  - OUT3LMIX Input 1 Source */

 R1697  - OUT3LMIX Input 1 Volume */

 R1698  - OUT3LMIX Input 2 Source */

 R1699  - OUT3LMIX Input 2 Volume */

 R1700  - OUT3LMIX Input 3 Source */

 R1701  - OUT3LMIX Input 3 Volume */

 R1702  - OUT3LMIX Input 4 Source */

 R1703  - OUT3LMIX Input 4 Volume */

 R1712  - OUT4LMIX Input 1 Source */

 R1713  - OUT4LMIX Input 1 Volume */

 R1714  - OUT4LMIX Input 2 Source */

 R1715  - OUT4LMIX Input 2 Volume */

 R1716  - OUT4LMIX Input 3 Source */

 R1717  - OUT4LMIX Input 3 Volume */

 R1718  - OUT4LMIX Input 4 Source */

 R1719  - OUT4LMIX Input 4 Volume */

 R1728  - OUT5LMIX Input 1 Source */

 R1729  - OUT5LMIX Input 1 Volume */

 R1730  - OUT5LMIX Input 2 Source */

 R1731  - OUT5LMIX Input 2 Volume */

 R1732  - OUT5LMIX Input 3 Source */

 R1733  - OUT5LMIX Input 3 Volume */

 R1734  - OUT5LMIX Input 4 Source */

 R1735  - OUT5LMIX Input 4 Volume */

 R1736  - OUT5RMIX Input 1 Source */

 R1737  - OUT5RMIX Input 1 Volume */

 R1738  - OUT5RMIX Input 2 Source */

 R1739  - OUT5RMIX Input 2 Volume */

 R1740  - OUT5RMIX Input 3 Source */

 R1741  - OUT5RMIX Input 3 Volume */

 R1742  - OUT5RMIX Input 4 Source */

 R1743  - OUT5RMIX Input 4 Volume */

 R1792  - AIF1TX1MIX Input 1 Source */

 R1793  - AIF1TX1MIX Input 1 Volume */

 R1794  - AIF1TX1MIX Input 2 Source */

 R1795  - AIF1TX1MIX Input 2 Volume */

 R1796  - AIF1TX1MIX Input 3 Source */

 R1797  - AIF1TX1MIX Input 3 Volume */

 R1798  - AIF1TX1MIX Input 4 Source */

 R1799  - AIF1TX1MIX Input 4 Volume */

 R1800  - AIF1TX2MIX Input 1 Source */

 R1801  - AIF1TX2MIX Input 1 Volume */

 R1802  - AIF1TX2MIX Input 2 Source */

 R1803  - AIF1TX2MIX Input 2 Volume */

 R1804  - AIF1TX2MIX Input 3 Source */

 R1805  - AIF1TX2MIX Input 3 Volume */

 R1806  - AIF1TX2MIX Input 4 Source */

 R1807  - AIF1TX2MIX Input 4 Volume */

 R1808  - AIF1TX3MIX Input 1 Source */

 R1809  - AIF1TX3MIX Input 1 Volume */

 R1810  - AIF1TX3MIX Input 2 Source */

 R1811  - AIF1TX3MIX Input 2 Volume */

 R1812  - AIF1TX3MIX Input 3 Source */

 R1813  - AIF1TX3MIX Input 3 Volume */

 R1814  - AIF1TX3MIX Input 4 Source */

 R1815  - AIF1TX3MIX Input 4 Volume */

 R1816  - AIF1TX4MIX Input 1 Source */

 R1817  - AIF1TX4MIX Input 1 Volume */

 R1818  - AIF1TX4MIX Input 2 Source */

 R1819  - AIF1TX4MIX Input 2 Volume */

 R1820  - AIF1TX4MIX Input 3 Source */

 R1821  - AIF1TX4MIX Input 3 Volume */

 R1822  - AIF1TX4MIX Input 4 Source */

 R1823  - AIF1TX4MIX Input 4 Volume */

 R1824  - AIF1TX5MIX Input 1 Source */

 R1825  - AIF1TX5MIX Input 1 Volume */

 R1826  - AIF1TX5MIX Input 2 Source */

 R1827  - AIF1TX5MIX Input 2 Volume */

 R1828  - AIF1TX5MIX Input 3 Source */

 R1829  - AIF1TX5MIX Input 3 Volume */

 R1830  - AIF1TX5MIX Input 4 Source */

 R1831  - AIF1TX5MIX Input 4 Volume */

 R1832  - AIF1TX6MIX Input 1 Source */

 R1833  - AIF1TX6MIX Input 1 Volume */

 R1834  - AIF1TX6MIX Input 2 Source */

 R1835  - AIF1TX6MIX Input 2 Volume */

 R1836  - AIF1TX6MIX Input 3 Source */

 R1837  - AIF1TX6MIX Input 3 Volume */

 R1838  - AIF1TX6MIX Input 4 Source */

 R1839  - AIF1TX6MIX Input 4 Volume */

 R1840  - AIF1TX7MIX Input 1 Source */

 R1841  - AIF1TX7MIX Input 1 Volume */

 R1842  - AIF1TX7MIX Input 2 Source */

 R1843  - AIF1TX7MIX Input 2 Volume */

 R1844  - AIF1TX7MIX Input 3 Source */

 R1845  - AIF1TX7MIX Input 3 Volume */

 R1846  - AIF1TX7MIX Input 4 Source */

 R1847  - AIF1TX7MIX Input 4 Volume */

 R1848  - AIF1TX8MIX Input 1 Source */

 R1849  - AIF1TX8MIX Input 1 Volume */

 R1850  - AIF1TX8MIX Input 2 Source */

 R1851  - AIF1TX8MIX Input 2 Volume */

 R1852  - AIF1TX8MIX Input 3 Source */

 R1853  - AIF1TX8MIX Input 3 Volume */

 R1854  - AIF1TX8MIX Input 4 Source */

 R1855  - AIF1TX8MIX Input 4 Volume */

 R1856  - AIF2TX1MIX Input 1 Source */

 R1857  - AIF2TX1MIX Input 1 Volume */

 R1858  - AIF2TX1MIX Input 2 Source */

 R1859  - AIF2TX1MIX Input 2 Volume */

 R1860  - AIF2TX1MIX Input 3 Source */

 R1861  - AIF2TX1MIX Input 3 Volume */

 R1862  - AIF2TX1MIX Input 4 Source */

 R1863  - AIF2TX1MIX Input 4 Volume */

 R1864  - AIF2TX2MIX Input 1 Source */

 R1865  - AIF2TX2MIX Input 1 Volume */

 R1866  - AIF2TX2MIX Input 2 Source */

 R1867  - AIF2TX2MIX Input 2 Volume */

 R1868  - AIF2TX2MIX Input 3 Source */

 R1869  - AIF2TX2MIX Input 3 Volume */

 R1870  - AIF2TX2MIX Input 4 Source */

 R1871  - AIF2TX2MIX Input 4 Volume */

 R1984  - SLIMTX1MIX Input 1 Source */

 R1985  - SLIMTX1MIX Input 1 Volume */

 R1986  - SLIMTX1MIX Input 2 Source */

 R1987  - SLIMTX1MIX Input 2 Volume */

 R1988  - SLIMTX1MIX Input 3 Source */

 R1989  - SLIMTX1MIX Input 3 Volume */

 R1990  - SLIMTX1MIX Input 4 Source */

 R1991  - SLIMTX1MIX Input 4 Volume */

 R1992  - SLIMTX2MIX Input 1 Source */

 R1993  - SLIMTX2MIX Input 1 Volume */

 R1994  - SLIMTX2MIX Input 2 Source */

 R1995  - SLIMTX2MIX Input 2 Volume */

 R1996  - SLIMTX2MIX Input 3 Source */

 R1997  - SLIMTX2MIX Input 3 Volume */

 R1998  - SLIMTX2MIX Input 4 Source */

 R1999  - SLIMTX2MIX Input 4 Volume */

 R2000  - SLIMTX3MIX Input 1 Source */

 R2001  - SLIMTX3MIX Input 1 Volume */

 R2002  - SLIMTX3MIX Input 2 Source */

 R2003  - SLIMTX3MIX Input 2 Volume */

 R2004  - SLIMTX3MIX Input 3 Source */

 R2005  - SLIMTX3MIX Input 3 Volume */

 R2006  - SLIMTX3MIX Input 4 Source */

 R2007  - SLIMTX3MIX Input 4 Volume */

 R2008  - SLIMTX4MIX Input 1 Source */

 R2009  - SLIMTX4MIX Input 1 Volume */

 R2010  - SLIMTX4MIX Input 2 Source */

 R2011  - SLIMTX4MIX Input 2 Volume */

 R2012  - SLIMTX4MIX Input 3 Source */

 R2013  - SLIMTX4MIX Input 3 Volume */

 R2014  - SLIMTX4MIX Input 4 Source */

 R2015  - SLIMTX4MIX Input 4 Volume */

 R2016  - SLIMTX5MIX Input 1 Source */

 R2017  - SLIMTX5MIX Input 1 Volume */

 R2018  - SLIMTX5MIX Input 2 Source */

 R2019  - SLIMTX5MIX Input 2 Volume */

 R2020  - SLIMTX5MIX Input 3 Source */

 R2021  - SLIMTX5MIX Input 3 Volume */

 R2022  - SLIMTX5MIX Input 4 Source */

 R2023  - SLIMTX5MIX Input 4 Volume */

 R2024  - SLIMTX6MIX Input 1 Source */

 R2025  - SLIMTX6MIX Input 1 Volume */

 R2026  - SLIMTX6MIX Input 2 Source */

 R2027  - SLIMTX6MIX Input 2 Volume */

 R2028  - SLIMTX6MIX Input 3 Source */

 R2029  - SLIMTX6MIX Input 3 Volume */

 R2030  - SLIMTX6MIX Input 4 Source */

 R2031  - SLIMTX6MIX Input 4 Volume */

 R2032  - SLIMTX7MIX Input 1 Source */

 R2033  - SLIMTX7MIX Input 1 Volume */

 R2034  - SLIMTX7MIX Input 2 Source */

 R2035  - SLIMTX7MIX Input 2 Volume */

 R2036  - SLIMTX7MIX Input 3 Source */

 R2037  - SLIMTX7MIX Input 3 Volume */

 R2038  - SLIMTX7MIX Input 4 Source */

 R2039  - SLIMTX7MIX Input 4 Volume */

 R2040  - SLIMTX8MIX Input 1 Source */

 R2041  - SLIMTX8MIX Input 1 Volume */

 R2042  - SLIMTX8MIX Input 2 Source */

 R2043  - SLIMTX8MIX Input 2 Volume */

 R2044  - SLIMTX8MIX Input 3 Source */

 R2045  - SLIMTX8MIX Input 3 Volume */

 R2046  - SLIMTX8MIX Input 4 Source */

 R2047  - SLIMTX8MIX Input 4 Volume */

 R2176  - EQ1MIX Input 1 Source */

 R2177  - EQ1MIX Input 1 Volume */

 R2178  - EQ1MIX Input 2 Source */

 R2179  - EQ1MIX Input 2 Volume */

 R2180  - EQ1MIX Input 3 Source */

 R2181  - EQ1MIX Input 3 Volume */

 R2182  - EQ1MIX Input 4 Source */

 R2183  - EQ1MIX Input 4 Volume */

 R2184  - EQ2MIX Input 1 Source */

 R2185  - EQ2MIX Input 1 Volume */

 R2186  - EQ2MIX Input 2 Source */

 R2187  - EQ2MIX Input 2 Volume */

 R2188  - EQ2MIX Input 3 Source */

 R2189  - EQ2MIX Input 3 Volume */

 R2190  - EQ2MIX Input 4 Source */

 R2191  - EQ2MIX Input 4 Volume */

 R2192  - EQ3MIX Input 1 Source */

 R2193  - EQ3MIX Input 1 Volume */

 R2194  - EQ3MIX Input 2 Source */

 R2195  - EQ3MIX Input 2 Volume */

 R2196  - EQ3MIX Input 3 Source */

 R2197  - EQ3MIX Input 3 Volume */

 R2198  - EQ3MIX Input 4 Source */

 R2199  - EQ3MIX Input 4 Volume */

 R2200  - EQ4MIX Input 1 Source */

 R2201  - EQ4MIX Input 1 Volume */

 R2202  - EQ4MIX Input 2 Source */

 R2203  - EQ4MIX Input 2 Volume */

 R2204  - EQ4MIX Input 3 Source */

 R2205  - EQ4MIX Input 3 Volume */

 R2206  - EQ4MIX Input 4 Source */

 R2207  - EQ4MIX Input 4 Volume */

 R2240  - DRC1LMIX Input 1 Source */

 R2241  - DRC1LMIX Input 1 Volume */

 R2242  - DRC1LMIX Input 2 Source */

 R2243  - DRC1LMIX Input 2 Volume */

 R2244  - DRC1LMIX Input 3 Source */

 R2245  - DRC1LMIX Input 3 Volume */

 R2246  - DRC1LMIX Input 4 Source */

 R2247  - DRC1LMIX Input 4 Volume */

 R2248  - DRC1RMIX Input 1 Source */

 R2249  - DRC1RMIX Input 1 Volume */

 R2250  - DRC1RMIX Input 2 Source */

 R2251  - DRC1RMIX Input 2 Volume */

 R2252  - DRC1RMIX Input 3 Source */

 R2253  - DRC1RMIX Input 3 Volume */

 R2254  - DRC1RMIX Input 4 Source */

 R2255  - DRC1RMIX Input 4 Volume */

 R2304  - HPLP1MIX Input 1 Source */

 R2305  - HPLP1MIX Input 1 Volume */

 R2306  - HPLP1MIX Input 2 Source */

 R2307  - HPLP1MIX Input 2 Volume */

 R2308  - HPLP1MIX Input 3 Source */

 R2309  - HPLP1MIX Input 3 Volume */

 R2310  - HPLP1MIX Input 4 Source */

 R2311  - HPLP1MIX Input 4 Volume */

 R2312  - HPLP2MIX Input 1 Source */

 R2313  - HPLP2MIX Input 1 Volume */

 R2314  - HPLP2MIX Input 2 Source */

 R2315  - HPLP2MIX Input 2 Volume */

 R2316  - HPLP2MIX Input 3 Source */

 R2317  - HPLP2MIX Input 3 Volume */

 R2318  - HPLP2MIX Input 4 Source */

 R2319  - HPLP2MIX Input 4 Volume */

 R2320  - HPLP3MIX Input 1 Source */

 R2321  - HPLP3MIX Input 1 Volume */

 R2322  - HPLP3MIX Input 2 Source */

 R2323  - HPLP3MIX Input 2 Volume */

 R2324  - HPLP3MIX Input 3 Source */

 R2325  - HPLP3MIX Input 3 Volume */

 R2326  - HPLP3MIX Input 4 Source */

 R2327  - HPLP3MIX Input 4 Volume */

 R2328  - HPLP4MIX Input 1 Source */

 R2329  - HPLP4MIX Input 1 Volume */

 R2330  - HPLP4MIX Input 2 Source */

 R2331  - HPLP4MIX Input 2 Volume */

 R2332  - HPLP4MIX Input 3 Source */

 R2333  - HPLP4MIX Input 3 Volume */

 R2334  - HPLP4MIX Input 4 Source */

 R2335  - HPLP4MIX Input 4 Volume */

 R2816  - ISRC1DEC1MIX Input 1 Source */

 R2824  - ISRC1DEC2MIX Input 1 Source */

 R2848  - ISRC1INT1MIX Input 1 Source */

 R2856  - ISRC1INT2MIX Input 1 Source */

 R2880  - ISRC2DEC1MIX Input 1 Source */

 R2888  - ISRC2DEC2MIX Input 1 Source */

 R2912  - ISRC2INT1MIX Input 1 Source */

 R2920  - ISRC2INT2MIX Input 1 Source */

 R3072  - GPIO1 CTRL */

 R3073  - GPIO2 CTRL */

 R3074  - GPIO3 CTRL */

 R3075  - GPIO4 CTRL */

 R3076  - GPIO5 CTRL */

 R3087  - IRQ CTRL 1 */

 R3088  - GPIO Debounce Config */

 R3104  - Misc Pad Ctrl 1 */

 R3105  - Misc Pad Ctrl 2 */

 R3106  - Misc Pad Ctrl 3 */

 R3107  - Misc Pad Ctrl 4 */

 R3108  - Misc Pad Ctrl 5 */

 R3336  - Interrupt Status 1 Mask */

 R3337  - Interrupt Status 2 Mask */

 R3338  - Interrupt Status 3 Mask */

 R3339  - Interrupt Status 4 Mask */

 R3340  - Interrupt Status 5 Mask */

 R3343  - Interrupt Control */

 R3352  - IRQ2 Status 1 Mask */

 R3354  - IRQ2 Status 3 Mask */

 R3355  - IRQ2 Status 4 Mask */

 R3356  - IRQ2 Status 5 Mask */

 R3359  - IRQ2 Control */

 R3411  - AOD IRQ Mask IRQ1 */

 R3412  - AOD IRQ Mask IRQ2 */

 R3414  - Jack detect debounce */

 R3584  - FX_Ctrl1 */

 R3600  - EQ1_1 */

 R3601  - EQ1_2 */

 R3602  - EQ1_3 */

 R3603  - EQ1_4 */

 R3604  - EQ1_5 */

 R3605  - EQ1_6 */

 R3606  - EQ1_7 */

 R3607  - EQ1_8 */

 R3608  - EQ1_9 */

 R3609  - EQ1_10 */

 R3610  - EQ1_11 */

 R3611  - EQ1_12 */

 R3612  - EQ1_13 */

 R3613  - EQ1_14 */

 R3614  - EQ1_15 */

 R3615  - EQ1_16 */

 R3616  - EQ1_17 */

 R3617  - EQ1_18 */

 R3618  - EQ1_19 */

 R3619  - EQ1_20 */

 R3620  - EQ1_21 */

 R3622  - EQ2_1 */

 R3623  - EQ2_2 */

 R3624  - EQ2_3 */

 R3625  - EQ2_4 */

 R3626  - EQ2_5 */

 R3627  - EQ2_6 */

 R3628  - EQ2_7 */

 R3629  - EQ2_8 */

 R3630  - EQ2_9 */

 R3631  - EQ2_10 */

 R3632  - EQ2_11 */

 R3633  - EQ2_12 */

 R3634  - EQ2_13 */

 R3635  - EQ2_14 */

 R3636  - EQ2_15 */

 R3637  - EQ2_16 */

 R3638  - EQ2_17 */

 R3639  - EQ2_18 */

 R3640  - EQ2_19 */

 R3641  - EQ2_20 */

 R3642  - EQ2_21 */

 R3644  - EQ3_1 */

 R3645  - EQ3_2 */

 R3646  - EQ3_3 */

 R3647  - EQ3_4 */

 R3648  - EQ3_5 */

 R3649  - EQ3_6 */

 R3650  - EQ3_7 */

 R3651  - EQ3_8 */

 R3652  - EQ3_9 */

 R3653  - EQ3_10 */

 R3654  - EQ3_11 */

 R3655  - EQ3_12 */

 R3656  - EQ3_13 */

 R3657  - EQ3_14 */

 R3658  - EQ3_15 */

 R3659  - EQ3_16 */

 R3660  - EQ3_17 */

 R3661  - EQ3_18 */

 R3662  - EQ3_19 */

 R3663  - EQ3_20 */

 R3664  - EQ3_21 */

 R3666  - EQ4_1 */

 R3667  - EQ4_2 */

 R3668  - EQ4_3 */

 R3669  - EQ4_4 */

 R3670  - EQ4_5 */

 R3671  - EQ4_6 */

 R3672  - EQ4_7 */

 R3673  - EQ4_8 */

 R3674  - EQ4_9 */

 R3675  - EQ4_10 */

 R3676  - EQ4_11 */

 R3677  - EQ4_12 */

 R3678  - EQ4_13 */

 R3679  - EQ4_14 */

 R3680  - EQ4_15 */

 R3681  - EQ4_16 */

 R3682  - EQ4_17 */

 R3683  - EQ4_18 */

 R3684  - EQ4_19 */

 R3685  - EQ4_20 */

 R3686  - EQ4_21 */

 R3712  - DRC1 ctrl1 */

 R3713  - DRC1 ctrl2 */

 R3714  - DRC1 ctrl3 */

 R3715  - DRC1 ctrl4 */

 R3716  - DRC1 ctrl5 */

 R3776  - HPLPF1_1 */

 R3777  - HPLPF1_2 */

 R3780  - HPLPF2_1 */

 R3781  - HPLPF2_2 */

 R3784  - HPLPF3_1 */

 R3785  - HPLPF3_2 */

 R3788  - HPLPF4_1 */

 R3789  - HPLPF4_2 */

 R3824  - ISRC 1 CTRL 1 */

 R3825  - ISRC 1 CTRL 2 */

 R3826  - ISRC 1 CTRL 3 */

 R3827  - ISRC 2 CTRL 1 */

 R3828  - ISRC 2 CTRL 2 */

 R3829  - ISRC 2 CTRL 3 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AB8500 system control driver

 *

 * Copyright (C) ST-Ericsson SA 2010

 * Author: Mattias Nilsson <mattias.i.nilsson@stericsson.com> for ST Ericsson.

 RtcCtrl bits */

	/*

	 * If we have a charger connected and we're powering off,

	 * reboot into charge-only mode.

 Check if battery is known */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020 MediaTek Inc.

 *

 * Author: Gene Chen <gene_chen@richtek.com>

 prealloca read size = i2c device addr + i2c reg addr + val ... + crc8 */

 prealloca write size = i2c device addr + i2c reg addr + val ... + crc8 + dummy byte */

 reg 0 -> 0 ~ 7 */

 REG 1 -> 8 ~ 15 */

 REG 2 -> 16 ~ 23 */

 REG 3 -> 24 ~ 31 */

 REG 4 -> 32 ~ 39 */

 REG 5 -> 40 ~ 47 */

 REG 6 -> 48 ~ 55 */

 REG 7 -> 56 ~ 63 */

 REG 8 -> 64 ~ 71 */

 REG 9 -> 72 ~ 79 */

 REG 10 -> 80 ~ 87 */

 REG 11 -> 88 ~ 95 */

 REG 12 -> 96 ~ 103 */

 REG 13 -> 104 ~ 111 */

 REG 14 -> 112 ~ 119 */

 REG 15 -> 120 ~ 127 */

 Address is already in encoded [5:0] */

 Due to PMIC and LDO CRC access size limit */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * twl_core.c - driver for TWL4030/TWL5030/TWL60X0/TPS659x0 PM

 * and audio CODEC devices

 *

 * Copyright (C) 2005-2006 Texas Instruments, Inc.

 *

 * Modifications to defer interrupt handling to a kernel thread:

 * Copyright (C) 2006 MontaVista Software, Inc.

 *

 * Based on tlv320aic23.c:

 * Copyright (c) by Kai Svahn <kai.svahn@nokia.com>

 *

 * Code cleanup and modifications to IRQ handler.

 * by syed khasim <x0khasim@ti.com>

 Register descriptions for audio */

/*

 * The TWL4030 "Triton 2" is one of a family of a multi-function "Power

 * Management and System Companion Device" chips originally designed for

 * use in OMAP2 and OMAP 3 based systems.  Its control interfaces use I2C,

 * often at around 3 Mbit/sec, including for interrupt handling.

 *

 * This driver core provides genirq support for the interrupts emitted,

 * by the various modules, and exports register access primitives.

 *

 * FIXME this driver currently requires use of the first interrupt line

 * (and associated registers).

 Triton Core internal information (BEGIN) */

 Base Address defns for twl4030_map[] */

 subchip/slave 0 - USB ID */

 subchip/slave 1 - AUD ID */

 subchip/slave 2 - AUX ID */

 Replaces Main Charge */

#define TWL5031_BASEADD_INTERRUPTS	0x00B9 /* Different than TWL4030's

 subchip/slave 3 - POWER ID */

 Triton Core internal information (END) */

 subchip/slave 0 0x48 - POWER */

 PM_RECEIVER */

 subchip/slave 1 0x49 - FEATURE */

 subchip/slave 2 0x4A - DFT */

 subchip/slave 3 0x4B - AUDIO */

 Few power values */

 some fields in R_CFG_BOOT */

----------------------------------------------------------------------*/

 Structure for each TWL4030/TWL6030 Slave */

 mapping the module id to slave id and base address */

 Slave ID */

 base address */

 The core driver is ready to be used */

 TWL IDCODE Register value */

	/*

	 * NOTE:  don't change this table without updating the

	 * <linux/mfd/twl.h> defines for TWL4030_MODULE_*

	 * so they continue to match the order in this table.

 Common IPs */

 TWL4030 specific IPs */

 Audio Registers */

 CODEC_MODE	*/

 OPTION	*/

 0x03  Unused	*/

 MICBIAS_CTL	*/

 ANAMICL	*/

 ANAMICR	*/

 AVADC_CTL	*/

 ADCMICSEL	*/

 DIGMIXING	*/

 ATXL1PGA	*/

 ATXR1PGA	*/

 AVTXL2PGA	*/

 AVTXR2PGA	*/

 AUDIO_IF	*/

 VOICE_IF	*/

 ARXR1PGA	*/

 ARXL1PGA	*/

 ARXR2PGA	*/

 ARXL2PGA	*/

 VRXPGA	*/

 VSTPGA	*/

 VRX2ARXPGA	*/

 AVDAC_CTL	*/

 ARX2VTXPGA	*/

 ARXL1_APGA_CTL*/

 ARXR1_APGA_CTL*/

 ARXL2_APGA_CTL*/

 ARXR2_APGA_CTL*/

 ATX2ARXPGA	*/

 BT_IF		*/

 BTPGA		*/

 BTSTPGA	*/

 EAR_CTL	*/

 HS_SEL	*/

 HS_GAIN_SET	*/

 HS_POPN_SET	*/

 PREDL_CTL	*/

 PREDR_CTL	*/

 PRECKL_CTL	*/

 PRECKR_CTL	*/

 HFL_CTL	*/

 HFR_CTL	*/

 ALC_CTL	*/

 ALC_SET1	*/

 ALC_SET2	*/

 BOOST_CTL	*/

 SOFTVOL_CTL	*/

 DTMF_FREQSEL	*/

 DTMF_TONEXT1H	*/

 DTMF_TONEXT1L	*/

 DTMF_TONEXT2H	*/

 DTMF_TONEXT2L	*/

 DTMF_TONOFF	*/

 DTMF_WANONOFF	*/

 I2S_RX_SCRAMBLE_H */

 I2S_RX_SCRAMBLE_M */

 I2S_RX_SCRAMBLE_L */

 APLL_CTL */

 DTMF_CTL */

 DTMF_PGA_CTL2	(0x3C) */

 DTMF_PGA_CTL1	(0x3D) */

 MISC_SET_1 */

 PCMBTMUX */

 0x40 - 0x42  Unused */

 RX_PATH_SEL */

 VDL_APGA_CTL */

 VIBRA_CTL */

 VIBRA_SET */

 VIBRA_PWM_SET	*/

 ANAMIC_GAIN	*/

 MISC_SET_2	*/

 End of Audio Registers */

 Address 0x48 */

 Address 0x49 */

 Address 0x4a */

 Address 0x4b */

	/*

	 * NOTE:  don't change this table without updating the

	 * <linux/mfd/twl.h> defines for TWL4030_MODULE_*

	 * so they continue to match the order in this table.

 Common IPs */

 TWL6030 specific IPs */

 Address 0x48 */

 Address 0x49 */

 Address 0x4a */

----------------------------------------------------------------------*/

 TWL4030 class have four slave address */

 TWL6030 class have three slave address */

 Exported Functions */

/**

 * twl_get_regmap - Get the regmap associated with the given module

 * @mod_no: module number

 *

 * Returns the regmap pointer or NULL in case of failure.

/**

 * twl_i2c_write - Writes a n bit register in TWL4030/TWL5030/TWL60X0

 * @mod_no: module number

 * @value: an array of num_bytes+1 containing data to write

 * @reg: register address (just offset will do)

 * @num_bytes: number of bytes to transfer

 *

 * Returns 0 on success or else a negative error code.

/**

 * twl_i2c_read - Reads a n bit register in TWL4030/TWL5030/TWL60X0

 * @mod_no: module number

 * @value: an array of num_bytes containing data to be read

 * @reg: register address (just offset will do)

 * @num_bytes: number of bytes to transfer

 *

 * Returns 0 on success or else a negative error code.

/**

 * twl_set_regcache_bypass - Configure the regcache bypass for the regmap associated

 *			 with the module

 * @mod_no: module number

 * @enable: Regcache bypass state

 *

 * Returns 0 else failure.

----------------------------------------------------------------------*/

/**

 * twl_read_idcode_register - API to read the IDCODE register.

 *

 * Unlocks the IDCODE register and read the 32 bit value.

/**

 * twl_get_type - API to get TWL Si type.

 *

 * Api to get the TWL Si type from IDCODE value.

/**

 * twl_get_version - API to get TWL Si version.

 *

 * Api to get the TWL Si version from IDCODE value.

/**

 * twl_get_hfclk_rate - API to get TWL external HFCLK clock rate.

 *

 * Api to get the TWL HFCLK rate based on BOOT_CFG register.

 regulator framework demands init_data ... */

 If we have existing drv_data, just add the flags */

 add new driver data struct, used only during init */

 NOTE:  we currently ignore regulator IRQs, e.g. for short circuits */

/*

 * NOTE:  We know the first 8 IRQs after pdata->base_irq are

 * for the PIH, and the next are for the PWR_INT SIH, since

 * that's how twl_init_irq() sets things up.

		/*

		 * REVISIT platform_data here currently might expose the

		 * "msecure" line ... but for now we just expect board

		 * setup to tell the chip "it's always ok to SET_TIME".

		 * Eventually, Linux might become more aware of such

		 * HW security concerns, and "least privilege".

 First add the regulators so that they can be used by transceiver */

 this is a template that gets copied */

 irq0 = USB_PRES, irq1 = USB */

 we need to connect regulators to this transceiver */

 twl4030 regulators */

 maybe add LDOs that are omitted on cost-reduced parts */

 irq0 = CHG_PRES, irq1 = BCI */

----------------------------------------------------------------------*/

/*

 * These three functions initialize the on-chip clock framework,

 * letting it generate the right frequencies for USB, MADC, and

 * other purposes.

 effect->MADC+USB ck en */

----------------------------------------------------------------------*/

 sentinel */ },

 NOTE: This driver only handles a single twl4030/tps659x0 chip */

 The charger base address is different in twl6032 */

 setup clock framework */

 read TWL IDCODE Register */

 Maybe init the T2 Interrupt subsystem */

	/*

	 * Disable TWL4030/TWL5030 I2C Pull-up on I2C1 and I2C4(SR) interface.

	 * Program I2C_SCL_CTRL_PU(bit 0)=0, I2C_SDA_CTRL_PU (bit 2)=0,

	 * SR_I2C_SCL_CTRL_PU(bit 4)=0 and SR_I2C_SDA_CTRL_PU(bit 6)=0.

	 *

	 * Also, always enable SmartReflex bit as that's needed for omaps to

	 * to do anything over I2C4 for voltage scaling even if SmartReflex

	 * is disabled. Without the SmartReflex bit omap sys_clkreq idle

	 * signal will never trigger for retention idle.

 "Triton 2" */

 T2 updated */

 TWL5030 updated */

 catalog version of twl5030 */

 fewer LDOs and DACs; no charger */

 fewer LDOs; no codec or charger */

	{ "tps65921", TPS_SUBSET },	/* fewer LDOs; no codec, no LED

 "Phoenix power chip" */

 "Phoenix lite" */

 end of list */ },

 One Client Driver , 4 Clients */

 SPDX-License-Identifier: GPL-2.0+

/*

 * MP2629 parent driver for ADC and battery charger

 *

 * Copyright 2020 Monolithic Power Systems, Inc

 *

 * Author: Saravanan Sekar <sravanhome@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lm3533-core.c -- LM3533 Core

 *

 * Copyright (C) 2011-2012 Texas Instruments

 *

 * Author: Johan Hovold <jhovold@gmail.com>

/*

 * HVLED output config -- output hvled controlled by backlight bl

/*

 * LVLED output config -- output lvled controlled by LED led

/*

 * Output config:

 *

 * output_hvled<nr>	0-1

 * output_lvled<nr>	0-3

 zone */

 adc */

 fault */

 zone */

/*

 * tps6507x.c  --  TPS6507x chip family multi-function driver

 *

 *  Copyright (c) 2010 RidgeRun (todd.fischer@ridgerun.com)

 *

 * Author: Todd Fischer

 *         todd.fischer@ridgerun.com

 *

 * Credits:

 *

 *    Using code from wm831x-*.c, wm8400-core, Wolfson Microelectronics PLC.

 *

 * For licencing details see kernel-base/COPYING

 *

 Write register */

 Read data */

 we add 1 byte for device register */

 init early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0+



 MFD core driver for the Maxim MAX77843



 Copyright (C) 2015 Samsung Electronics

 Author: Jaewon Kim <jaewon02.kim@samsung.com>

 Author: Beomho Seo <beomho.seo@samsung.com>

 TOPSYS interrupts */

 Charger and Charger regulator use same regmap. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  MEN 14F021P00 Board Management Controller (BMC) MFD Core Driver.

 *

 *  Copyright (C) 2014 MEN Mikro Elektronik Nuernberg GmbH

	/*

	 * Production mode should be not active after delivery of the Board.

	 * To be sure we check it, inform the user and exit the mode

	 * if active.

	/*

	 * We have to exit the Production Mode of the BMC to activate the

	 * Watchdog functionality and the BIOS life sign monitoring.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2007-2010 ST-Ericsson

 * Register access functions for the ABX500 Mixed Signal IC family.

 * Author: Mattias Wallin <mattias.wallin@stericsson.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Device access for Dialog DA9055 PMICs.

 *

 * Copyright(c) 2012 Dialog Semiconductor Ltd.

 *

 * Author: David Dajun Chen <dchen@diasemi.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * lm3533-ctrlbank.c -- LM3533 Generic Control Bank interface

 *

 * Copyright (C) 2011-2012 Texas Instruments

 *

 * Author: Johan Hovold <jhovold@gmail.com>

/*

 * Full-scale current.

 *

 * imax		5000 - 29800 uA (800 uA step)

/*

 * PWM-input control mask:

 *

 *   bit 5 - PWM-input enabled in Zone 4

 *   bit 4 - PWM-input enabled in Zone 3

 *   bit 3 - PWM-input enabled in Zone 2

 *   bit 2 - PWM-input enabled in Zone 1

 *   bit 1 - PWM-input enabled in Zone 0

 *   bit 0 - PWM-input enabled

/*

 * tps65217.c

 *

 * TPS65217 chip family multi-function driver

 *

 * Copyright (C) 2011 Texas Instruments Incorporated - https://www.ti.com/

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 Mask all interrupt sources */

/**

 * tps65217_reg_read: Read a single tps65217 register.

 *

 * @tps: Device to read from.

 * @reg: Register to read.

 * @val: Contians the value

/**

 * tps65217_reg_write: Write a single tps65217 register.

 *

 * @tps: Device to write to.

 * @reg: Register to write to.

 * @val: Value to write.

 * @level: Password protected level

/**

 * tps65217_update_bits: Modify bits w.r.t mask, val and level.

 *

 * @tps: Device to write to.

 * @reg: Register to read-write to.

 * @mask: Mask.

 * @val: Value to write.

 * @level: Password protected level

 sentinel */ },

 Don't tell children about IRQ resources which won't fire */

 Set the PMIC to shutdown on PWR_EN toggle */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver for TI TPS65090 PMIC family

 *

 * Copyright (c) 2012, NVIDIA CORPORATION.  All rights reserved.

 *

 * Author: Venu Byravarasu <vbyravarasu@nvidia.com>

 INT1 IRQs*/

 INT2 IRQs*/

 Nearly all registers have status bits mixed in, except a few */

 Don't tell children they have an IRQ that'll never fire */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Compaq iPAQ h3xxx Atmel microcontroller companion support

 *

 * This is an Atmel AT90LS8535 with a special flashed-in firmware that

 * implements the special protocol used by this driver.

 *

 * based on previous kernel 2.4 version by Andrew Christian

 * Author : Alessandro Gardich <gremlin@gremlin.it>

 * Author : Dmitry Artamonow <mad_soft@inbox.ru>

 * Author : Linus Walleij <linus.walleij@linaro.org>

 Enable interrupt */

 Handle synchronous messages */

 Looking for SOF */

 Next byte is the id and len */

 Looking for id and len byte */

 Looking for 'len' data bytes */

 Looking for the checksum */

 Bytes 4-7 are "pack", byte 8 is "boot type" */

 Feed the random pool with this */

 Stop interrupts */

 Initialize Serial channel protocol frame */

 Reset the state machine */

 Set up interrupts */

 Select UART mode */

 Clean up CR3 */

 Format: 8N1 */

 Baud rate: 115200 */

 Clear SR0 */

 Enable RX int, disable TX int */

 Clear the Receiver IDLE bit */

 Clear break bits */

 Check version */

/*

 * Copyright (C) 2015 Texas Instruments Incorporated - https://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether expressed or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License version 2 for more details.

 *

 * Based on the TPS65912 driver

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * TI LP8788 MFD - core interface

 *

 * Copyright 2012 Texas Instruments

 *

 * Author: Milo(Woogyom) Kim <milo.kim@ti.com>

 Charger Interrupts */

 Power Routing Switch Interrupts */

 Battery Interrupts */

 4 bucks */

 12 digital ldos */

 10 analog ldos */

 ADC */

 battery charger */

 rtc */

 backlight */

 current sink for vibrator */

 current sink for keypad LED */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST Microelectronics MFD: stmpe's spi client specific driver

 *

 * Copyright (C) ST Microelectronics SA 2011

 *

 * Author: Viresh Kumar <vireshk@kernel.org> for ST Microelectronics

 This register is only present for stmpe811 */

 don't exceed max specified rate - 1MHz - Limitation of STMPE */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Driver for Atmel Flexcom

 *

 * Copyright (C) 2015 Atmel Corporation

 *

 * Author: Cyrille Pitchen <cyrille.pitchen@atmel.com>

 I/O register offsets */

 Mode Register */

 Version Register */

 Mode Register bit fields */

 Operating Mode */

	/*

	 * Set the Operating Mode in the Mode Register: only the selected device

	 * is clocked. Hence, registers of the other serial devices remain

	 * inaccessible and are read as zero. Also the external I/O lines of the

	 * Flexcom are muxed to reach the selected device.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 2018-2019, Intel Corporation.

 *  Copyright (C) 2012 Freescale Semiconductor, Inc.

 *  Copyright (C) 2012 Linaro Ltd.

 *

 *  Based on syscon driver.

/**

 * struct altr_sysmgr - Altera SOCFPGA System Manager

 * @regmap: the regmap used for System Manager accesses.

/**

 * s10_protected_reg_write

 * Write to a protected SMC register.

 * @base: Base address of System Manager

 * @reg:  Address offset of register

 * @val:  Value to write

 * Return: INTEL_SIP_SMC_STATUS_OK (0) on success

 *	   INTEL_SIP_SMC_REG_ERROR on error

 *	   INTEL_SIP_SMC_RETURN_UNKNOWN_FUNCTION if not supported

/**

 * s10_protected_reg_read

 * Read the status of a protected SMC register

 * @base: Base address of System Manager.

 * @reg:  Address of register

 * @val:  Value read.

 * Return: INTEL_SIP_SMC_STATUS_OK (0) on success

 *	   INTEL_SIP_SMC_REG_ERROR on error

 *	   INTEL_SIP_SMC_RETURN_UNKNOWN_FUNCTION if not supported

/**

 * altr_sysmgr_regmap_lookup_by_phandle

 * Find the sysmgr previous configured in probe() and return regmap property.

 * Return: regmap if found or error if not found.

 *

 * @np: Pointer to device's Device Tree node

 * @property: Device Tree property name which references the sysmgr

 Need physical address for SMCC call */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Toshiba TC6387XB support

 * Copyright (c) 2005 Ian Molton

 *

 * This file contains TC6387XB base support.

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Device driver for MFD hi655x PMIC

 *

 * Copyright (c) 2016 HiSilicon Ltd.

 *

 * Authors:

 * Chen Feng <puck.chen@hisilicon.com>

 * Fei  Wang <w.f@huawei.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD driver for twl4030 audio submodule, which contains an audio codec, and

 * the vibra control.

 *

 * Author: Peter Ujfalusi <peter.ujfalusi@ti.com>

 *

 * Copyright:   (C) 2009 Nokia Corporation

/*

 * Modify the resource, the function returns the content of the register

 * after the modification.

/*

 * Enable the resource.

 * The function returns with error or the content of the register

 Resource was disabled, enable it */

/*

 * Disable the resource.

 * The function returns with error or the content of the register

 Resource can be disabled now */

 Configure APLL_INFREQ and disable APLL if enabled */

 Codec power */

 PLL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Toshiba TC6393XB SoC support

 *

 * Copyright(c) 2005-2006 Chris Humbert

 * Copyright(c) 2005 Dirk Opfer

 * Copyright(c) 2005 Ian Molton <spyro@f2s.com>

 * Copyright(c) 2007 Dmitry Baryshkov

 *

 * Based on code written by Sharp/Lineo for 2.4 kernels

 * Based on locomo.c

 b Revision ID	*/

 b Interrupt Status	*/

 b Interrupt Mask	*/

 b Interrupt Routing	*/

 w GP Enable		*/

 b3 GPI Status	*/

 b3 GPI INT Mask	*/

 b3 GPI Edge Detect Enable */

 b3 GPI Level Invert	*/

 b3 GPO Data Set	*/

 b3 GPO Data OE Control */

 b3 GP Internal Active Register Control */

 b3 GP INTERNAL Active Register Level Control */

 b3 GPI Buffer Control */

 w GPa Internal Active Register Control */

 w GPa Internal Active Register Level Control */

 w GPa Buffer Control */

 w Clock Control	*/

 w PLL2 Control	*/

 l PLL1 Control	*/

 b Device Internal Active Register Control */

 b Device Buffer Off Control */

 b Function Enable	*/

 w Mode Control	*/

 b Configuration Control */

 b Debug		*/

 USB host enable */

 polysilicon TFT enable */

 SLCD enable */

 bits 8 - 16 are unknown */

--------------------------------------------------------------------------*/

 3,6 Mhz */

 protects RMW cycles */

--------------------------------------------------------------------------*/

 SMD buffer on */

 We can't properly store/restore OHCI state, so fail here */

--------------------------------------------------------------------------*/

 XXX: does dsr also represent inputs? */

--------------------------------------------------------------------------*/

--------------------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver for HTC PASIC3 LED/DS1WM chip.

 *

 * Copyright (C) 2006 Philipp Zabel <philipp.zabel@gmail.com>

/*

 * write to a secondary register on the PASIC3

 for leds-pasic3 */

/*

 * read from a secondary register on the PASIC3

 for leds-pasic3 */

/*

 * LEDs

/*

 * DS1WM

 calculate bus shift from mem resource */

 the first 5 PASIC3 registers control the DS1WM */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD core driver for the X-Powers' Power Management ICs

 *

 * AXP20x typically comprises an adaptive USB-Compatible PWM charger, BUCK DC-DC

 * converters, LDOs, multiple 12-bit ADCs of voltage, current and temperature

 * as well as configurable GPIOs.

 *

 * This file contains the interface independent core functions.

 *

 * Copyright (C) 2014 Carlo Caione

 *

 * Author: Carlo Caione <carlo@caione.org>

 AXP22x ranges are shared with the AXP809, as they cover the same range */

 AXP288 ranges are shared with the AXP803, as they cover the same range */

 AXP803 and AXP813/AXP818 share the same interrupts */

 some IRQs are compatible with axp20x models */

 Give capacitors etc. time to drain to avoid kernel panic msg. */

		/*

		 * Don't register the power key part if in slave mode or

		 * if there is no interrupt line.

		/*

		 * The IRQ table given in the datasheet is incorrect.

		 * In IRQ enable/status registers 1, there are separate

		 * IRQs for ACIN and VBUS, instead of bits [7:5] being

		 * the same as bits [4:2]. So it shares the same IRQs

		 * as the AXP803, rather than the AXP288.

	/*

	 * The AXP806 supports either master/standalone or slave mode.

	 * Slave mode allows sharing the serial bus, even with multiple

	 * AXP806 which all have the same hardware address.

	 *

	 * This is done with extra "serial interface address extension",

	 * or AXP806_BUS_ADDR_EXT, and "register address extension", or

	 * AXP806_REG_ADDR_EXT, registers. The former is read-only, with

	 * 1 bit customizable at the factory, and 1 bit depending on the

	 * state of an external pin. The latter is writable. The device

	 * will only respond to operations to its other registers when

	 * the these device addressing bits (in the upper 4 bits of the

	 * registers) match.

	 *

	 * By default we support an AXP806 chained to an AXP809 in slave

	 * mode. Boards which use an AXP806 in master mode can set the

	 * property "x-powers,master-mode" to override the default.

 Only if there is an interrupt line connected towards the CPU. */

/*

 *

 * Handle TWL4030 Power initialization

 *

 * Copyright (C) 2008 Nokia Corporation

 * Copyright (C) 2006 Texas Instruments, Inc

 *

 * Written by 	Kalle Jokiniemi

 *		Peter De Schrijver <peter.de-schrijver@nokia.com>

 * Several fixes by Amit Kucheria <amit.kucheria@verdurent.com>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

 Register bits for P1, P2 and P3_SW_EVENTS */

 Register bits for CFG_P1_TRANSITION (also for P2 and P3) */

 Start on watchdog */

 Start on VBUS */

 Start on battery insert */

 Start on RTC */

 Start on USB host */

 Start on charger */

 Start on PWRON button */

 resource - hfclk */

 PM events */

/* resource configuration registers

   <RESOURCE>_DEV_GRP   at address 'n+0'

   <RESOURCE>_TYPE      at address 'n+1'

   <RESOURCE>_REMAP     at address 'n+2'

   <RESOURCE>_DEDICATED at address 'n+3'

 Bit positions in the registers */

 <RESOURCE>_DEV_GRP */

 <RESOURCE>_TYPE */

 <RESOURCE>_REMAP */

/*

 * Usable values for .remap_sleep and .remap_off

 * Based on table "5.3.3 Resource Operating modes"

/*

 * Macros to configure the PM register states for various resources.

 * Note that we can make MSG_SINGULAR etc private to this driver once

 * omap3 has been made DT only.

 typically 2 32 KiHz cycles */

/*

 * It seems that type1 and type2 is just the resource init order

 * number for the type1 and type2 group.

 Set SLEEP to ACTIVE SEQ address for P3 */

 P3 LVL_WAKEUP should be on LEVEL */

 Set SLEEP to ACTIVE SEQ address for P1 and P2 */

 P1/P2 LVL_WAKEUP should be on LEVEL */

 Disabling AC charger effect on sleep-active transitions */

 Set ACTIVE to SLEEP SEQ address in T2 memory*/

 Set WARM RESET SEQ address for P1 */

 P1/P2/P3 enable WARMRESET */

 Set resource group */

 Set resource types */

 Set remap states */

 Make sure the script isn't going beyond last valid address (0x3f) */

 Reset any existing sleep script to avoid hangs on reboot */

/*

 * In master mode, start the power off sequence.

 * After a successful execution, TWL shuts down the power to the SoC

 * and all peripherals connected to it.

 Disable start on charger or VBUS as it can break poweroff */

 Generic warm reset configuration for omap3 */

 Recommended generic default idle configuration for off-idle */

 Broadcast message to put res to sleep */

 Broadcast message to put res to active */

 Broadcast message to put res to active */

/*

 * Recommended configuration based on "Recommended Sleep

 * Sequences for the Zoom Platform":

 * http://omappedia.com/wiki/File:Recommended_Sleep_Sequences_Zoom.pdf

 * Note that the type1 and type2 seem to be just the init order number

 * for type1 and type2 groups as specified in the document mentioned

 * above.

 Resource #20 USB charge pump skipped */

 Terminator */ },

 Disable 32 KiHz oscillator during idle */

 Terminator */ },

 CONFIG_OF */

 Board has to be wired properly to use this feature */

 Default for SEQ_OFFSYNC is set, lets ensure this */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Base driver for Dialog Semiconductor DA9030/DA9034

 *

 * Copyright (C) 2008 Compulab, Ltd.

 *	Mike Rapoport <mike@compulab.co.il>

 *

 * Copyright (C) 2006-2008 Marvell International Ltd.

 *	Eric Miao <eric.miao@marvell.com>

 avoid SRAM power off during sleep*/

 Enable the ONKEY power down functionality */

 workaround to make LEDs work */

 make ADTV1 and SDTV1 effective */

 mask and clear all IRQs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/drivers/mfd/ucb1x00-core.c

 *

 *  Copyright (C) 2001 Russell King, All Rights Reserved.

 *

 *  The UCB1x00 core driver provides basic services for handling IO,

 *  the ADC, interrupts, and accessing registers.  It is designed

 *  such that everything goes through this layer, thereby providing

 *  a consistent locking methodology, as well as allowing the drivers

 *  to be used on other non-MCP-enabled hardware platforms.

 *

 *  Note that all locks are private to this file.  Nothing else may

 *  touch them.

/**

 *	ucb1x00_io_set_dir - set IO direction

 *	@ucb: UCB1x00 structure describing chip

 *	@in:  bitfield of IO pins to be set as inputs

 *	@out: bitfield of IO pins to be set as outputs

 *

 *	Set the IO direction of the ten general purpose IO pins on

 *	the UCB1x00 chip.  The @in bitfield has priority over the

 *	@out bitfield, in that if you specify a pin as both input

 *	and output, it will end up as an input.

 *

 *	ucb1x00_enable must have been called to enable the comms

 *	before using this function.

 *

 *	This function takes a spinlock, disabling interrupts.

/**

 *	ucb1x00_io_write - set or clear IO outputs

 *	@ucb:   UCB1x00 structure describing chip

 *	@set:   bitfield of IO pins to set to logic '1'

 *	@clear: bitfield of IO pins to set to logic '0'

 *

 *	Set the IO output state of the specified IO pins.  The value

 *	is retained if the pins are subsequently configured as inputs.

 *	The @clear bitfield has priority over the @set bitfield -

 *	outputs will be cleared.

 *

 *	ucb1x00_enable must have been called to enable the comms

 *	before using this function.

 *

 *	This function takes a spinlock, disabling interrupts.

/**

 *	ucb1x00_io_read - read the current state of the IO pins

 *	@ucb: UCB1x00 structure describing chip

 *

 *	Return a bitfield describing the logic state of the ten

 *	general purpose IO pins.

 *

 *	ucb1x00_enable must have been called to enable the comms

 *	before using this function.

 *

 *	This function does not take any mutexes or spinlocks.

/*

 * UCB1300 data sheet says we must:

 *  1. enable ADC	=> 5us (including reference startup time)

 *  2. select input	=> 51*tsibclk  => 4.3us

 *  3. start conversion	=> 102*tsibclk => 8.5us

 * (tsibclk = 1/11981000)

 * Period between SIB 128-bit frames = 10.7us

/**

 *	ucb1x00_adc_enable - enable the ADC converter

 *	@ucb: UCB1x00 structure describing chip

 *

 *	Enable the ucb1x00 and ADC converter on the UCB1x00 for use.

 *	Any code wishing to use the ADC converter must call this

 *	function prior to using it.

 *

 *	This function takes the ADC mutex to prevent two or more

 *	concurrent uses, and therefore may sleep.  As a result, it

 *	can only be called from process context, not interrupt

 *	context.

 *

 *	You should release the ADC as soon as possible using

 *	ucb1x00_adc_disable.

/**

 *	ucb1x00_adc_read - read the specified ADC channel

 *	@ucb: UCB1x00 structure describing chip

 *	@adc_channel: ADC channel mask

 *	@sync: wait for syncronisation pulse.

 *

 *	Start an ADC conversion and wait for the result.  Note that

 *	synchronised ADC conversions (via the ADCSYNC pin) must wait

 *	until the trigger is asserted and the conversion is finished.

 *

 *	This function currently spins waiting for the conversion to

 *	complete (2 frames max without sync).

 *

 *	If called for a synchronised ADC conversion, it may sleep

 *	with the ADC mutex held.

 yield to other processes */

/**

 *	ucb1x00_adc_disable - disable the ADC converter

 *	@ucb: UCB1x00 structure describing chip

 *

 *	Disable the ADC converter and release the ADC mutex.

/*

 * UCB1x00 Interrupt handling.

 *

 * The UCB1x00 can generate interrupts when the SIBCLK is stopped.

 * Since we need to read an internal register, we must re-enable

 * SIBCLK to talk to the chip.  We leave the clock running until

 * we have finished processing all interrupts from the chip.

/*

 * Try to probe our interrupt, rather than relying on lots of

 * hard-coded machine dependencies.  For reference, the expected

 * IRQ mappings are:

 *

 *  	Machine		Default IRQ

 *	adsbitsy	IRQ_GPCIN4

 *	cerf		IRQ_GPIO_UCB1200_IRQ

 *	flexanet	IRQ_GPIO_GUI

 *	freebird	IRQ_GPIO_FREEBIRD_UCB1300_IRQ

 *	graphicsclient	ADS_EXT_IRQ(8)

 *	graphicsmaster	ADS_EXT_IRQ(8)

 *	lart		LART_IRQ_UCB1200

 *	omnimeter	IRQ_GPIO23

 *	pfs168		IRQ_GPIO_UCB1300_IRQ

 *	simpad		IRQ_GPIO_UCB1300_IRQ

 *	shannon		SHANNON_IRQ_GPIO_IRQ_CODEC

 *	yopy		IRQ_GPIO_UCB1200_IRQ

	/*

	 * Enable the ADC interrupt.

	/*

	 * Cause an ADC interrupt.

	/*

	 * Wait for the conversion to complete.

	/*

	 * Disable and clear interrupt.

	/*

	 * Read triggered interrupt.

 Tell the platform to deassert the UCB1x00 reset */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/mfd/si476x-i2c.c -- Core device driver for si476x MFD

 * device

 *

 * Copyright (C) 2012 Innovative Converged Devices(ICD)

 * Copyright (C) 2013 Andrey Smirnov

 *

 * Author: Andrey Smirnov <andrew.smirnov@gmail.com>

/**

 * si476x_core_config_pinmux() - pin function configuration function

 *

 * @core: Core device structure

 *

 * Configure the functions of the pins of the radio chip.

 *

 * The function returns zero in case of succes or negative error code

 * otherwise.

/**

 * si476x_core_start() - early chip startup function

 * @core: Core device structure

 * @soft: When set, this flag forces "soft" startup, where "soft"

 * power down is the one done by sending appropriate command instead

 * of using reset pin of the tuner

 *

 * Perform required startup sequence to correctly power

 * up the chip and perform initial configuration. It does the

 * following sequence of actions:

 *       1. Claims and enables the power supplies VD and VIO1 required

 *          for I2C interface of the chip operation.

 *       2. Waits for 100us, pulls the reset line up, enables irq,

 *          waits for another 100us as it is specified by the

 *          datasheet.

 *       3. Sends 'POWER_UP' command to the device with all provided

 *          information about power-up parameters.

 *       4. Configures, pin multiplexor, disables digital audio and

 *          configures interrupt sources.

 *

 * The function returns zero in case of succes or negative error code

 * otherwise.

/**

 * si476x_core_stop() - chip power-down function

 * @core: Core device structure

 * @soft: When set, function sends a POWER_DOWN command instead of

 * bringing reset line low

 *

 * Power down the chip by performing following actions:

 * 1. Disable IRQ or stop the polling worker

 * 2. Send the POWER_DOWN command if the power down is soft or bring

 *    reset line low if not.

 *

 * The function returns zero in case of succes or negative error code

 * otherwise.

		/* TODO: This probably shoud be a configurable option,

		 * so it is possible to have the chips keep their

		 * oscillators running

	/* We couldn't disable those before

	 * 'si476x_core_cmd_power_down' since we expect to get CTS

/**

 * si476x_core_set_power_state() - set the level at which the power is

 * supplied for the chip.

 * @core: Core device structure

 * @next_state: enum si476x_power_state describing power state to

 *              switch to.

 *

 * Switch on all the required power supplies

 *

 * This function returns 0 in case of suvccess and negative error code

 * otherwise.

	/*

	   It is not clear form the datasheet if it is possible to

	   work with device if not all power domains are operational.

	   So for now the power-up policy is "power-up all the things!"

			/*

			 * Startup timing diagram recommends to have a

			 * 100 us delay between enabling of the power

			 * supplies and turning the tuner on.

/**

 * si476x_core_report_drainer_stop() - mark the completion of the RDS

 * buffer drain porcess by the worker.

 *

 * @core: Core device structure

/**

 * si476x_core_start_rds_drainer_once() - start RDS drainer worker if

 * ther is none working, do nothing otherwise

 *

 * @core: Datastructure corresponding to the chip.

/**

 * si476x_core_drain_rds_fifo() - RDS buffer drainer.

 * @work: struct work_struct being ppassed to the function by the

 * kernel.

 *

 * Drain the contents of the RDS FIFO of

/**

 * si476x_core_pronounce_dead()

 *

 * @core: Core device structure

 *

 * Mark the device as being dead and wake up all potentially waiting

 * threads of execution.

 *

 Wake up al possible waiting processes */

/**

 * si476x_core_i2c_xfer()

 *

 * @core: Core device structure

 * @type: Transfer type

 * @buf: Transfer buffer for/with data

 * @count: Transfer buffer size

 *

 * Perfrom and I2C transfer(either read or write) and keep a counter

 * of I/O errors. If the error counter rises above the threshold

 * pronounce device dead.

 *

 * The function returns zero on succes or negative error code on

 * failure.

/**

 * si476x_core_get_status()

 * @core: Core device structure

 *

 * Get the status byte of the core device by berforming one byte I2C

 * read.

 *

 * The function returns a status value or a negative error code on

 * error.

/**

 * si476x_core_get_and_signal_status() - IRQ dispatcher

 * @core: Core device structure

 *

 * Dispatch the arrived interrupt request based on the value of the

 * status byte reported by the tuner.

 *

		/* Unfortunately completions could not be used for

		 * signalling CTS since this flag cannot be cleared

		 * in status byte, and therefore once it becomes true

		 * multiple calls to 'complete' would cause the

		 * commands following the current one to be completed

/**

 * si476x_core_fwver_to_revision()

 * @core: Core device structure

 * @func: Selects the boot function of the device:

 *         *_BOOTLOADER  - Boot loader

 *         *_FM_RECEIVER - FM receiver

 *         *_AM_RECEIVER - AM receiver

 *         *_WB_RECEIVER - Weatherband receiver

 * @major:  Firmware major number

 * @minor1: Firmware first minor number

 * @minor2: Firmware second minor number

 *

 * Convert a chip's firmware version number into an offset that later

 * will be used to as offset in "vtable" of tuner functions

 *

 * This function returns a positive offset in case of success and a -1

 * in case of failure.

 FALLTHROUGH */

/**

 * si476x_core_get_revision_info()

 * @core: Core device structure

 *

 * Get the firmware version number of the device. It is done in

 * following three steps:

 *    1. Power-up the device

 *    2. Send the 'FUNC_INFO' command

 *    3. Powering the device down.

 *

 * The function return zero on success and a negative error code on

 * failure.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Driver for Motorola PCAP2 as present in EZX phones

 *

 * Copyright (C) 2006 Harald Welte <laforge@openezx.org>

 * Copyright (C) 2009 Daniel Ribeiro <drwyrm@gmail.com>

 IO */

 IRQ */

 ADC */

 IO */

 IRQ */

 We can't service/ack irqs that are assigned to port 2 */

 ADC */

 queue is empty, save power */

 start conversion on requested bank, save TS_M bits */

 read requested channels results */

 pass the results and release memory */

 trigger next conversion (if any) on queue */

 This will be freed after we have a result */

 start conversion */

 subdevs */

 remove all registered subdevs */

 cleanup ADC */

 cleanup irqchip */

 platform data is required */

 setup spi */

 setup irq */

 redirect interrupts to AP, except adcdone2 */

 setup irq chip */

 mask/ack all PCAP interrupts */

 ADC */

 setup subdevs */

 board specific quirks */

 destroy_workqueue: */

 SPDX-License-Identifier: GPL-2.0-or-later

/* NXP PCF50633 Power Management Unit (PMU) driver

 *

 * (C) 2006-2008 by Openmoko, Inc.

 * Author: Harald Welte <laforge@openmoko.org>

 * 	   Balaji Rao <balajirrao@openmoko.org>

 * All rights reserved.

 Read a block of up to 32 regs  */

 Write a block of up to 32 regs  */

 sysfs attributes */

 must be ascending */

 terminator */

 put in device directory */

 Create sub devices */

 end of list */}

 SPDX-License-Identifier: GPL-2.0+



 max8997.c - mfd core driver for the Maxim 8966 and 8997



 Copyright (C) 2011 Samsung Electronics

 MyungJoo Ham <myungjoo.ham@samsung.com>



 This driver is based on max8998.c

/*

 * Only the common platform data elements for max8997 are parsed here from the

 * device tree. Other sub-modules of max8997 such as pmic, rtc and others have

 * to parse their own platform data elements from device tree.

 *

 * The max8997 platform data structure is instantiated here and the drivers for

 * the sub-modules need not instantiate another instance while parsing their

 * platform data.

	/*

	 * TODO: enable others (flash, muic, rtc, battery, ...) and

	 * check the return value

 MAX8997 has a power button input. */

 init early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0-only

/* ADC MFD core driver for sunxi platforms

 *

 * Copyright (c) 2016 Quentin Schulz <quentin.schulz@free-electrons.com>

 sentinel */ }

 Disable all interrupts */

 SPDX-License-Identifier: GPL-2.0+

/*

 * PM MFD driver for Broadcom BCM2835

 *

 * This driver binds to the PM block and creates the MFD device for

 * the WDT and power drivers.

	/* We'll use the presence of the AXI ASB regs in the

	 * bcm2835-pm binding as the key for whether we can reference

	 * the full PM register range and support power domains.

 SPDX-License-Identifier: GPL-2.0

/*

 * MFD core driver for Intel Cherrytrail Whiskey Cove PMIC

 *

 * Copyright (C) 2017 Hans de Goede <hdegoede@redhat.com>

 *

 * Based on various non upstream patches to support the CHT Whiskey Cove PMIC:

 * Copyright (C) 2013-2015 Intel Corporation. All rights reserved.

 PMIC device registers */

 Whiskey Cove PMIC share same ACPI ID between different platforms */

 Level 1 IRQs (level 2 IRQs are handled in the child device drivers) */

 There is no irq 6 */

/*

 * The CHT Whiskey Cove covers multiple I2C addresses, with a 1 Byte

 * register address space per I2C address, so we use 16 bit register

 * addresses where the high 8 bits contain the I2C client address.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver for STw4810/STw4811

 *

 * Copyright (C) 2013 ST-Ericsson SA

 * Written on behalf of Linaro for ST-Ericsson

 *

 * Author: Linus Walleij <linus.walleij@linaro.org>

/*

 * This driver can only access the non-USB portions of STw4811, the register

 * range 0x00-0x10 dealing with USB is bound to the two special I2C pins used

 * for USB control.

 Registers inside the power control address space */

/**

 * stw481x_get_pctl_reg() - get a power control register

 * @stw481x: handle to the stw481x chip

 * @reg: power control register to fetch

 *

 * The power control registers is a set of one-time-programmable registers

 * in its own register space, accessed by writing addess bits to these

 * two registers: bits 7,6,5 of PCTL_REG_LO corresponds to the 3 LSBs of

 * the address and bits 8,9 of PCTL_REG_HI corresponds to the 2 MSBs of

 * the address, forming an address space of 5 bits, i.e. 32 registers

 * 0x00 ... 0x1f can be obtained.

 Voltages multiplied by 100 */

 Save bit 4 */

/*

 * MFD cells - we have one cell which is selected operation

 * mode, and we always have a GPIO cell.

 Set up and register the platform devices. */

 One state holder for all drivers, this is simple */

/*

 * This ID table is completely unused, as this is a pure

 * device-tree probed driver, but it has to be here due to

 * the structure of the I2C core.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 Free Electrons

 *

 * Author: Boris BREZILLON <boris.brezillon@free-electrons.com>

 *

 * Allwinner PRCM (Power/Reset/Clock Management) driver

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8350-i2c.c  --  Generic I2C driver for Wolfson WM8350 PMIC

 *

 * Copyright 2007, 2008 Wolfson Microelectronics PLC.

 *

 * Author: Liam Girdwood

 *         linux@wolfsonmicro.com

 init early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) STMicroelectronics 2016

 * Author: Benjamin Gaignard <benjamin.gaignard@st.com>

 DIER register DMA enable bits */

/**

 * stm32_timers_dma_burst_read - Read from timers registers using DMA.

 *

 * Read from STM32 timers registers using DMA on a single event.

 * @dev: reference to stm32_timers MFD device

 * @buf: DMA'able destination buffer

 * @id: stm32_timers_dmas event identifier (ch[1..4], up, trig or com)

 * @reg: registers start offset for DMA to read from (like CCRx for capture)

 * @num_reg: number of registers to read upon each DMA request, starting @reg.

 * @bursts: number of bursts to read (e.g. like two for pwm period capture)

 * @tmo_ms: timeout (milliseconds)

 Sanity check */

 Select DMA channel in use */

 Prepare DMA read from timer registers, using DMA burst mode */

 Setup and enable timer DMA burst mode */

 Clear pending flags before enabling DMA request */

 Backup ARR to restore it after getting the maximum value */

	/*

	 * Only the available bits will be written so when readback

	 * we get the maximum value of auto reload register

 Optional DMA support: get valid DMA channel(s) or NULL */

 Save the first error code to return */

 Timer physical addr for DMA */

	/*

	 * Don't use devm_ here: enfore of_platform_depopulate() happens before

	 * DMA are released, to avoid race on DMA.

 end node */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

	/*

	 * In early versions of PM8941 and PM8226, the major revision number

	 * started incrementing from 0 (eg 0 = v1.0, 1 = v2.0).

	 * Increment the major revision number here if the chip is an early

	 * version of PM8941 or PM8226.

 Only the first slave id for a PMIC contains this information */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Device driver for Hi6421 PMIC

 *

 * Copyright (c) <2011-2014> HiSilicon Technologies Co., Ltd.

 *              http://www.hisilicon.com

 * Copyright (c) <2013-2017> Linaro Ltd.

 *              https://www.linaro.org

 *

 * Author: Guodong Xu <guodong.xu@linaro.org>

 set over-current protection debounce 8ms */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * I2C access for DA9052 PMICs.

 *

 * Copyright(c) 2011 Dialog Semiconductor Ltd.

 *

 * Author: David Dajun Chen <dchen@diasemi.com>

 I2C safe register check */

/*

 * There is an issue with DA9052 and DA9053_AA/BA/BB PMIC where the PMIC

 * gets lockup up or fails to respond following a system reset.

 * This fix is to follow any read or write with a dummy read to a safe

 * register.

 A dummy read to a safe register address. */

		/*

		 * For other chips parking of I2C register

		 * to a safe place is not required.

/*

 * According to errata item 24, multiwrite mode should be avoided

 * in order to prevent register data corruption after power-down.

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/mfd/si476x-cmd.c -- Subroutines implementing command

 * protocol of si476x series of chips

 *

 * Copyright (C) 2012 Innovative Converged Devices(ICD)

 * Copyright (C) 2013 Andrey Smirnov

 *

 * Author: Andrey Smirnov <andrew.smirnov@gmail.com>

/**

 * si476x_core_send_command() - sends a command to si476x and waits its

 * response

 * @core:     si476x_device structure for the device we are

 *            communicating with

 * @command:  command id

 * @args:     command arguments we are sending

 * @argn:     actual size of @args

 * @resp:     buffer to place the expected response from the device

 * @respn:    actual size of @resp

 * @usecs:    amount of time to wait before reading the response (in

 *            usecs)

 *

 * Function returns 0 on succsess and negative error code on

 * failure

 First send the command and its arguments */

	/* Set CTS to zero only after the command is send to avoid

 if (unlikely(command == CMD_POWER_DOWN) */

	/*

	  When working in polling mode, for some reason the tuner will

	  report CTS bit as being set in the first status byte read,

	  but all the consequtive ones will return zeros until the

	  tuner is actually completed the POWER_UP command. To

	  workaround that we wait for second CTS to be reported

 Then get the response */

/**

 * si476x_core_cmd_func_info() - send 'FUNC_INFO' command to the device

 * @core: device to send the command to

 * @info:  struct si476x_func_info to fill all the information

 *         returned by the command

 *

 * The command requests the firmware and patch version for currently

 * loaded firmware (dependent on the function of the device FM/AM/WB)

 *

 * Function returns 0 on succsess and negative error code on

 * failure

/**

 * si476x_core_cmd_set_property() - send 'SET_PROPERTY' command to the device

 * @core:    device to send the command to

 * @property: property address

 * @value:    property value

 *

 * Function returns 0 on succsess and negative error code on

 * failure

/**

 * si476x_core_cmd_get_property() - send 'GET_PROPERTY' command to the device

 * @core:    device to send the command to

 * @property: property address

 *

 * Function return the value of property as u16 on success or a

 * negative error on failure

/**

 * si476x_core_cmd_dig_audio_pin_cfg() - send 'DIG_AUDIO_PIN_CFG' command to

 * the device

 * @core: device to send the command to

 * @dclk:  DCLK pin function configuration:

 *	   #SI476X_DCLK_NOOP     - do not modify the behaviour

 *         #SI476X_DCLK_TRISTATE - put the pin in tristate condition,

 *                                 enable 1MOhm pulldown

 *         #SI476X_DCLK_DAUDIO   - set the pin to be a part of digital

 *                                 audio interface

 * @dfs:   DFS pin function configuration:

 *         #SI476X_DFS_NOOP      - do not modify the behaviour

 *         #SI476X_DFS_TRISTATE  - put the pin in tristate condition,

 *                             enable 1MOhm pulldown

 *      SI476X_DFS_DAUDIO    - set the pin to be a part of digital

 *                             audio interface

 * @dout: - DOUT pin function configuration:

 *      SI476X_DOUT_NOOP       - do not modify the behaviour

 *      SI476X_DOUT_TRISTATE   - put the pin in tristate condition,

 *                               enable 1MOhm pulldown

 *      SI476X_DOUT_I2S_OUTPUT - set this pin to be digital out on I2S

 *                               port 1

 *      SI476X_DOUT_I2S_INPUT  - set this pin to be digital in on I2S

 *                               port 1

 * @xout: - XOUT pin function configuration:

 *	SI476X_XOUT_NOOP        - do not modify the behaviour

 *      SI476X_XOUT_TRISTATE    - put the pin in tristate condition,

 *                                enable 1MOhm pulldown

 *      SI476X_XOUT_I2S_INPUT   - set this pin to be digital in on I2S

 *                                port 1

 *      SI476X_XOUT_MODE_SELECT - set this pin to be the input that

 *                                selects the mode of the I2S audio

 *                                combiner (analog or HD)

 *                                [SI4761/63/65/67 Only]

 *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_zif_pin_cfg - send 'ZIF_PIN_CFG_COMMAND'

 * @core: - device to send the command to

 * @iqclk: - IQCL pin function configuration:

 *       SI476X_IQCLK_NOOP     - do not modify the behaviour

 *       SI476X_IQCLK_TRISTATE - put the pin in tristate condition,

 *                               enable 1MOhm pulldown

 *       SI476X_IQCLK_IQ       - set pin to be a part of I/Q interace

 *                               in master mode

 * @iqfs: - IQFS pin function configuration:

 *       SI476X_IQFS_NOOP     - do not modify the behaviour

 *       SI476X_IQFS_TRISTATE - put the pin in tristate condition,

 *                              enable 1MOhm pulldown

 *       SI476X_IQFS_IQ       - set pin to be a part of I/Q interace

 *                              in master mode

 * @iout: - IOUT pin function configuration:

 *       SI476X_IOUT_NOOP     - do not modify the behaviour

 *       SI476X_IOUT_TRISTATE - put the pin in tristate condition,

 *                              enable 1MOhm pulldown

 *       SI476X_IOUT_OUTPUT   - set pin to be I out

 * @qout: - QOUT pin function configuration:

 *       SI476X_QOUT_NOOP     - do not modify the behaviour

 *       SI476X_QOUT_TRISTATE - put the pin in tristate condition,

 *                              enable 1MOhm pulldown

 *       SI476X_QOUT_OUTPUT   - set pin to be Q out

 *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_ic_link_gpo_ctl_pin_cfg - send

 * 'IC_LINK_GPIO_CTL_PIN_CFG' comand to the device

 * @core: - device to send the command to

 * @icin: - ICIN pin function configuration:

 *      SI476X_ICIN_NOOP      - do not modify the behaviour

 *      SI476X_ICIN_TRISTATE  - put the pin in tristate condition,

 *                              enable 1MOhm pulldown

 *      SI476X_ICIN_GPO1_HIGH - set pin to be an output, drive it high

 *      SI476X_ICIN_GPO1_LOW  - set pin to be an output, drive it low

 *      SI476X_ICIN_IC_LINK   - set the pin to be a part of Inter-Chip link

 * @icip: - ICIP pin function configuration:

 *      SI476X_ICIP_NOOP      - do not modify the behaviour

 *      SI476X_ICIP_TRISTATE  - put the pin in tristate condition,

 *                              enable 1MOhm pulldown

 *      SI476X_ICIP_GPO1_HIGH - set pin to be an output, drive it high

 *      SI476X_ICIP_GPO1_LOW  - set pin to be an output, drive it low

 *      SI476X_ICIP_IC_LINK   - set the pin to be a part of Inter-Chip link

 * @icon: - ICON pin function configuration:

 *      SI476X_ICON_NOOP     - do not modify the behaviour

 *      SI476X_ICON_TRISTATE - put the pin in tristate condition,

 *                             enable 1MOhm pulldown

 *      SI476X_ICON_I2S      - set the pin to be a part of audio

 *                             interface in slave mode (DCLK)

 *      SI476X_ICON_IC_LINK  - set the pin to be a part of Inter-Chip link

 * @icop: - ICOP pin function configuration:

 *      SI476X_ICOP_NOOP     - do not modify the behaviour

 *      SI476X_ICOP_TRISTATE - put the pin in tristate condition,

 *                             enable 1MOhm pulldown

 *      SI476X_ICOP_I2S      - set the pin to be a part of audio

 *                             interface in slave mode (DOUT)

 *                             [Si4761/63/65/67 Only]

 *      SI476X_ICOP_IC_LINK  - set the pin to be a part of Inter-Chip link

 *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_ana_audio_pin_cfg - send 'ANA_AUDIO_PIN_CFG' to the

 * device

 * @core: - device to send the command to

 * @lrout: - LROUT pin function configuration:

 *       SI476X_LROUT_NOOP     - do not modify the behaviour

 *       SI476X_LROUT_TRISTATE - put the pin in tristate condition,

 *                               enable 1MOhm pulldown

 *       SI476X_LROUT_AUDIO    - set pin to be audio output

 *       SI476X_LROUT_MPX      - set pin to be MPX output

 *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_intb_pin_cfg_a10 - send 'INTB_PIN_CFG' command to the device

 * @core: - device to send the command to

 * @intb: - INTB pin function configuration:

 *      SI476X_INTB_NOOP     - do not modify the behaviour

 *      SI476X_INTB_TRISTATE - put the pin in tristate condition,

 *                             enable 1MOhm pulldown

 *      SI476X_INTB_DAUDIO   - set pin to be a part of digital

 *                             audio interface in slave mode

 *      SI476X_INTB_IRQ      - set pin to be an interrupt request line

 * @a1: - A1 pin function configuration:

 *      SI476X_A1_NOOP     - do not modify the behaviour

 *      SI476X_A1_TRISTATE - put the pin in tristate condition,

 *                           enable 1MOhm pulldown

 *      SI476X_A1_IRQ      - set pin to be an interrupt request line

 *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_am_rsq_status - send 'AM_RSQ_STATUS' command to the

 * device

 * @core:  - device to send the command to

 * @rsqargs: - pointer to a structure containing a group of sub-args

 *             relevant to sending the RSQ status command

 * @report: - all signal quality information returned by the command

 *           (if NULL then the output of the command is ignored)

 *

 * Function returns 0 on success and negative error code on failure

	/*

	 * Besides getting received signal quality information this

	 * command can be used to just acknowledge different interrupt

	 * flags in those cases it is useless to copy and parse

	 * received data so user can pass NULL, and thus avoid

	 * unnecessary copying.

/**

 * si476x_core_cmd_fm_seek_start - send 'FM_SEEK_START' command to the

 * device

 * @core:  - device to send the command to

 * @seekup: - if set the direction of the search is 'up'

 * @wrap:   - if set seek wraps when hitting band limit

 *

 * This function begins search for a valid station. The station is

 * considered valid when 'FM_VALID_SNR_THRESHOLD' and

 * 'FM_VALID_RSSI_THRESHOLD' and 'FM_VALID_MAX_TUNE_ERROR' criteria

 * are met.

} *

 * Function returns 0 on success and negative error code on failure

/**

 * si476x_core_cmd_fm_rds_status - send 'FM_RDS_STATUS' command to the

 * device

 * @core: - device to send the command to

 * @status_only: - if set the data is not removed from RDSFIFO,

 *                RDSFIFOUSED is not decremented and data in all the

 *                rest RDS data contains the last valid info received

 * @mtfifo: if set the command clears RDS receive FIFO

 * @intack: if set the command clards the RDSINT bit.

 * @report: - all signal quality information returned by the command

 *           (if NULL then the output of the command is ignored)

 *

 * Function returns 0 on success and negative error code on failure

	/*

	 * Besides getting RDS status information this command can be

	 * used to just acknowledge different interrupt flags in those

	 * cases it is useless to copy and parse received data so user

	 * can pass NULL, and thus avoid unnecessary copying.

/**

 * si476x_core_cmd_fm_phase_div_status() - get the phase diversity

 * status

 *

 * @core: si476x device

 *

 * NOTE caller must hold core lock

 *

 * Function returns the value of the status bit in case of success and

 * negative error code in case of failre.

/**

 * si476x_core_cmd_am_seek_start - send 'FM_SEEK_START' command to the

 * device

 * @core:  - device to send the command to

 * @seekup: - if set the direction of the search is 'up'

 * @wrap:   - if set seek wraps when hitting band limit

 *

 * This function begins search for a valid station. The station is

 * considered valid when 'FM_VALID_SNR_THRESHOLD' and

 * 'FM_VALID_RSSI_THRESHOLD' and 'FM_VALID_MAX_TUNE_ERROR' criteria

 * are met.

 *

 * Function returns 0 on success and negative error code on failure

 Reserved, always 0xF7 */

		0x3F & puargs->xcload,	/* First two bits are reserved to be

		ctsen << 7 | intsel << 6 | 0x07, /* Last five bits

						   * are reserved to

 Reserved, always 0x11 */

		0x3F & puargs->xcload,	/* First two bits are reserved to be

	/*

	 * Besides getting received signal quality information this

	 * command can be used to just acknowledge different interrupt

	 * flags in those cases it is useless to copy and parse

	 * received data so user can pass NULL, and thus avoid

	 * unnecessary copying.

	/*

	 * Besides getting received signal quality information this

	 * command can be used to just acknowledge different interrupt

	 * flags in those cases it is useless to copy and parse

	 * received data so user can pass NULL, and thus avoid

	 * unnecessary copying.

	/*

	 * Besides getting received signal quality information this

	 * command can be used to just acknowledge different interrupt

	 * flags in those cases it is useless to copy and parse

	 * received data so user can pass NULL, and thus avoid

	 * unnecessary copying.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * SPI access for Dialog DA9052 PMICs.

 *

 * Copyright(c) 2011 Dialog Semiconductor Ltd.

 *

 * Author: David Dajun Chen <dchen@diasemi.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/* NXP PCF50633 GPIO Driver

 *

 * (C) 2006-2008 by Openmoko, Inc.

 * Author: Balaji Rao <balajirrao@openmoko.org>

 * All rights reserved.

 *

 * Broken down from monstrous PCF50633 driver mainly by

 * Harald Welte, Andy Green and Werner Almesberger

 the *ENA register is always one after the *OUT register */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device driver for regulators in HISI PMIC IC

 *

 * Copyright (c) 2013 Linaro Ltd.

 * Copyright (c) 2011 Hisilicon.

 * Copyright (c) 2020-2021 Huawei Technologies Co., Ltd.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Altera Arria10 DevKit System Resource MFD Driver

 *

 * Author: Thor Thayer <tthayer@opensource.altera.com>

 *

 * Copyright Intel Corporation (C) 2014-2016. All Rights Reserved

 *

 * SPI access for Altera Arria10 MAX5 System Resource Chip

 *

 * Adapted from DA9052

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-auxadc.c  --  AUXADC for Wolfson WM831x PMICs

 *

 * Copyright 2009-2011 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 Enqueue the request */

 Enable the conversion if not already running */

 We convert at the fastest rate possible */

 Wait for an interrupt */

 Disable this conversion, we're about to complete all users */

 Turn off the entire convertor if idle */

 Wake up any threads waiting for this request */

 We force a single source at present */

	/* If we're not using interrupts then poll the

 Did it complete? */

/**

 * wm831x_auxadc_read: Read a value from the WM831x AUXADC

 *

 * @wm831x: Device to read from.

 * @input: AUXADC input to read.

/**

 * wm831x_auxadc_read_uv: Read a voltage from the WM831x AUXADC

 *

 * @wm831x: Device to read from.

 * @input: AUXADC input to read.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Regmap tables for CS47L90 codec

 *

 * Copyright (C) 2015-2017 Cirrus Logic

 R32 (0x20) - Tone Generator 1 */

 R33 (0x21) - Tone Generator 2 */

 R34 (0x22) - Tone Generator 3 */

 R35 (0x23) - Tone Generator 4 */

 R36 (0x24) - Tone Generator 5 */

 R48 (0x30) - PWM Drive 1 */

 R49 (0x31) - PWM Drive 2 */

 R50 (0x32) - PWM Drive 3 */

 R97 (0x61) - Sample Rate Sequence Select 1 */

 R98 (0x62) - Sample Rate Sequence Select 2 */

 R99 (0x63) - Sample Rate Sequence Select 3 */

 R100 (0x64) - Sample Rate Sequence Select 4 */

 R102 (0x66) - Always On Triggers Sequence Select 1 */

 R103 (0x67) - Always On Triggers Sequence Select 2 */

 R144 (0x90) - Haptics Control 1 */

 R145 (0x91) - Haptics Control 2 */

 R146 (0x92) - Haptics phase 1 intensity */

 R147 (0x93) - Haptics phase 1 duration */

 R148 (0x94) - Haptics phase 2 intensity */

 R149 (0x95) - Haptics phase 2 duration */

 R150 (0x96) - Haptics phase 3 intensity */

 R151 (0x97) - Haptics phase 3 duration */

 R160 (0xa0) - Comfort Noise Generator */

 R256 (0x100) - Clock 32k 1 */

 R257 (0x101) - System Clock 1 */

 R258 (0x102) - Sample rate 1 */

 R259 (0x103) - Sample rate 2 */

 R260 (0x104) - Sample rate 3 */

 R274 (0x112) - Async clock 1 */

 R275 (0x113) - Async sample rate 1 */

 R276 (0x114) - Async sample rate 2 */

 R288 (0x120) - DSP Clock 1 */

 R290 (0x122) - DSP Clock 2 */

 R329 (0x149) - Output system clock */

 R330 (0x14a) - Output async clock */

 R338 (0x152) - Rate Estimator 1 */

 R339 (0x153) - Rate Estimator 2 */

 R340 (0x154) - Rate Estimator 3 */

 R341 (0x155) - Rate Estimator 4 */

 R342 (0x156) - Rate Estimator 5 */

 R369 (0x171) - FLL1 Control 1 */

 R370 (0x172) - FLL1 Control 2 */

 R371 (0x173) - FLL1 Control 3 */

 R372 (0x174) - FLL1 Control 4 */

 R373 (0x175) - FLL1 Control 5 */

 R374 (0x176) - FLL1 Control 6 */

 R377 (0x179) - FLL1 Control 7 */

 R377 (0x17a) - FLL1 Efs 2 */

 R385 (0x181) - FLL1 Synchroniser 1 */

 R386 (0x182) - FLL1 Synchroniser 2 */

 R387 (0x183) - FLL1 Synchroniser 3 */

 R388 (0x184) - FLL1 Synchroniser 4 */

 R389 (0x185) - FLL1 Synchroniser 5 */

 R390 (0x186) - FLL1 Synchroniser 6 */

 R391 (0x187) - FLL1 Synchroniser 7 */

 R393 (0x189) - FLL1 Spread Spectrum */

 R394 (0x18a) - FLL1 GPIO Clock */

 R401 (0x191) - FLL2 Control 1 */

 R402 (0x192) - FLL2 Control 2 */

 R403 (0x193) - FLL2 Control 3 */

 R404 (0x194) - FLL2 Control 4 */

 R405 (0x195) - FLL2 Control 5 */

 R406 (0x196) - FLL2 Control 6 */

 R409 (0x199) - FLL2 Control 7 */

 R410 (0x19a) - FLL2 Efs 2 */

 R417 (0x1a1) - FLL2 Synchroniser 1 */

 R418 (0x1a2) - FLL2 Synchroniser 2 */

 R419 (0x1a3) - FLL2 Synchroniser 3 */

 R420 (0x1a4) - FLL2 Synchroniser 4 */

 R421 (0x1a5) - FLL2 Synchroniser 5 */

 R422 (0x1a6) - FLL2 Synchroniser 6 */

 R423 (0x1a7) - FLL2 Synchroniser 7 */

 R425 (0x1a9) - FLL2 Spread Spectrum */

 R426 (0x1aa) - FLL2 GPIO Clock */

 R465 (0x1d1) - FLLAO_CONTROL_1 */

 R466 (0x1d2) - FLLAO_CONTROL_2 */

 R467 (0x1d3) - FLLAO_CONTROL_3 */

 R468 (0x1d4) - FLLAO_CONTROL_4 */

 R469 (0x1d5) - FLLAO_CONTROL_5 */

 R470 (0x1d6) - FLLAO_CONTROL_6 */

 R472 (0x1d8) - FLLAO_CONTROL_7 */

 R474 (0x1da) - FLLAO_CONTROL_8 */

 R475 (0x1db) - FLLAO_CONTROL_9 */

 R476 (0x1dc) - FLLAO_CONTROL_10 */

 R477 (0x1dd) - FLLAO_CONTROL_11 */

 R512 (0x200) - Mic Charge Pump 1 */

 R531 (0x213) - LDO2 Control 1 */

 R536 (0x218) - Mic Bias Ctrl 1 */

 R537 (0x219) - Mic Bias Ctrl 2 */

 R540 (0x21c) - Mic Bias Ctrl 5 */

 R542 (0x21e) - Mic Bias Ctrl 6 */

 R638 (0x27e) - EDRE HP stereo control */

 R659 (0x293) - Accessory Detect Mode 1 */

 R665 (0x299) - Headphone Detect 0 */

 R667 (0x29b) - Headphone Detect 1 */

 R674 (0x2a2) - Mic Detect 1 Control 0 */

 R675 (0x2a3) - Mic Detect 1 Control 1 */

 R676 (0x2a4) - Mic Detect 1 Control 2 */

 R678 (0x2a6) - Mic Detect 1 Level 1 */

 R679 (0x2a7) - Mic Detect 1 Level 2 */

 R680 (0x2a8) - Mic Detect 1 Level 3 */

 R681 (0x2a9) - Mic Detect 1 Level 4 */

 R690 (0x2b2) - Mic Detect 2 Control 0 */

 R691 (0x2b3) - Mic Detect 2 Control 1 */

 R692 (0x2b4) - Mic Detect 2 Control 2 */

 R694 (0x2b6) - Mic Detect 2 Level 1 */

 R695 (0x2b7) - Mic Detect 2 Level 2 */

 R696 (0x2b8) - Mic Detect 2 Level 3 */

 R697 (0x2b9) - Mic Detect 2 Level 4 */

 R710 (0x2c6) - Mic Clamp control */

 R712 (0x2c8) - GP switch 1 */

 R723 (0x2d3) - Jack detect analogue */

 R768 (0x300) - Input Enables */

 R776 (0x308) - Input Rate */

 R777 (0x309) - Input Volume Ramp */

 R780 (0x30C) - HPF Control */

 R784 (0x310) - IN1L Control */

 R785 (0x311) - ADC Digital Volume 1L */

 R786 (0x312) - DMIC1L Control */

 R787 (0x313) - IN1L Rate Control */

 R788 (0x314) - IN1R Control */

 R789 (0x315) - ADC Digital Volume 1R */

 R790 (0x316) - DMIC1R Control */

 R791 (0x317) - IN1R Rate Control */

 R792 (0x318) - IN2L Control */

 R793 (0x319) - ADC Digital Volume 2L */

 R794 (0x31a) - DMIC2L Control */

 R795 (0x31b) - IN2L Rate Control */

 R796 (0x31c) - IN2R Control */

 R797 (0x31d) - ADC Digital Volume 2R */

 R798 (0x31e) - DMIC2R Control */

 R799 (0x31f) - IN2R Rate Control */

 R800 (0x320) - IN3L Control */

 R801 (0x321) - ADC Digital Volume 3L */

 R802 (0x322) - DMIC3L Control */

 R803 (0x323) - IN3L Rate Control */

 R804 (0x324) - IN3R Control */

 R805 (0x325) - ADC Digital Volume 3R */

 R806 (0x326) - DMIC3R Control */

 R807 (0x327) - IN3R Rate Control */

 R808 (0x328) - IN4 Control */

 R809 (0x329) - ADC Digital Volume 4L */

 R810 (0x32a) - DMIC4L Control */

 R811 (0x32b) - IN4L Rate Control */

 R812 (0x32c) - IN4R Control */

 R813 (0x32d) - ADC Digital Volume 4R */

 R814 (0x32e) - DMIC4R Control */

 R815 (0x32f) - IN4R Rate Control */

 R816 (0x330) - IN5L Control */

 R817 (0x331) - ADC Digital Volume 5L */

 R818 (0x332) - DMIC5L Control */

 R819 (0x333) - IN5L Rate Control */

 R820 (0x334) - IN5R Control */

 R821 (0x335) - ADC Digital Volume 5R */

 R822 (0x336) - DMIC5R Control */

 R823 (0x337) - IN5R Rate Control */

 R1024 (0x400) - Output Enables 1 */

 R1032 (0x408) - Output Rate 1 */

 R1033 (0x409) - Output Volume Ramp */

 R1040 (0x410) - Output Path Config 1L */

 R1041 (0x411) - DAC Digital Volume 1L */

 R1042 (0x412) - Output Path Config 1 */

 R1043 (0x413) - Noise Gate Select 1L */

 R1044 (0x414) - Output Path Config 1R */

 R1045 (0x415) - DAC Digital Volume 1R */

 R1047 (0x417) - Noise Gate Select 1R */

 R1048 (0x418) - Output Path Config 2L */

 R1049 (0x419) - DAC Digital Volume 2L */

 R1050 (0x41a) - Output Path Config 2 */

 R1051 (0x41b) - Noise Gate Select 2L */

 R1052 (0x41c) - Output Path Config 2R */

 R1053 (0x41d) - DAC Digital Volume 2R */

 R1055 (0x41f) - Noise Gate Select 2R */

 R1056 (0x420) - Output Path Config 3L */

 R1057 (0x421) - DAC Digital Volume 3L */

 R1059 (0x423) - Noise Gate Select 3L */

 R1060 (0x424) - Output Path Config 3R */

 R1061 (0x425) - DAC Digital Volume 3R */

 R1063 (0x427) - Noise Gate Select 3R */

 R1072 (0x430) - Output Path Config 5L */

 R1073 (0x431) - DAC Digital Volume 5L */

 R1075 (0x433) - Noise Gate Select 5L */

 R1076 (0x434) - Output Path Config 5R */

 R1077 (0x435) - DAC Digital Volume 5R */

 R1079 (0x437) - Noise Gate Select 5R */

 R1104 (0x450) - DAC AEC Control 1 */

 R1104 (0x450) - DAC AEC Control 2 */

 R1112 (0x458) - Noise Gate Control */

 R1168 (0x490) - PDM SPK1 CTRL 1 */

 R1169 (0x491) - PDM SPK1 CTRL 2 */

 R1184 (0x4a0) - HP1 Short Circuit Ctrl */

 R1185 (0x4a1) - HP2 Short Circuit Ctrl */

 R1186 (0x4a2) - HP3 Short Circuit Ctrl */

 R1280 (0x500) - AIF1 BCLK Ctrl */

 R1281 (0x501) - AIF1 Tx Pin Ctrl */

 R1282 (0x502) - AIF1 Rx Pin Ctrl */

 R1283 (0x503) - AIF1 Rate Ctrl */

 R1284 (0x504) - AIF1 Format */

 R1286 (0x506) - AIF1 Rx BCLK Rate */

 R1287 (0x507) - AIF1 Frame Ctrl 1 */

 R1288 (0x508) - AIF1 Frame Ctrl 2 */

 R1289 (0x509) - AIF1 Frame Ctrl 3 */

 R1290 (0x50a) - AIF1 Frame Ctrl 4 */

 R1291 (0x50b) - AIF1 Frame Ctrl 5 */

 R1292 (0x50c) - AIF1 Frame Ctrl 6 */

 R1293 (0x50d) - AIF1 Frame Ctrl 7 */

 R1294 (0x50e) - AIF1 Frame Ctrl 8 */

 R1295 (0x50f) - AIF1 Frame Ctrl 9 */

 R1296 (0x510) - AIF1 Frame Ctrl 10 */

 R1297 (0x511) - AIF1 Frame Ctrl 11 */

 R1298 (0x512) - AIF1 Frame Ctrl 12 */

 R1299 (0x513) - AIF1 Frame Ctrl 13 */

 R1300 (0x514) - AIF1 Frame Ctrl 14 */

 R1301 (0x515) - AIF1 Frame Ctrl 15 */

 R1302 (0x516) - AIF1 Frame Ctrl 16 */

 R1303 (0x517) - AIF1 Frame Ctrl 17 */

 R1304 (0x518) - AIF1 Frame Ctrl 18 */

 R1305 (0x519) - AIF1 Tx Enables */

 R1306 (0x51a) - AIF1 Rx Enables */

 R1344 (0x540) - AIF2 BCLK Ctrl */

 R1345 (0x541) - AIF2 Tx Pin Ctrl */

 R1346 (0x542) - AIF2 Rx Pin Ctrl */

 R1347 (0x543) - AIF2 Rate Ctrl */

 R1348 (0x544) - AIF2 Format */

 R1350 (0x546) - AIF2 Rx BCLK Rate */

 R1351 (0x547) - AIF2 Frame Ctrl 1 */

 R1352 (0x548) - AIF2 Frame Ctrl 2 */

 R1353 (0x549) - AIF2 Frame Ctrl 3 */

 R1354 (0x54a) - AIF2 Frame Ctrl 4 */

 R1355 (0x54b) - AIF2 Frame Ctrl 5 */

 R1356 (0x54c) - AIF2 Frame Ctrl 6 */

 R1357 (0x54d) - AIF2 Frame Ctrl 7 */

 R1358 (0x54e) - AIF2 Frame Ctrl 8 */

 R1359 (0x54f) - AIF2 Frame Ctrl 9 */

 R1360 (0x550) - AIF2 Frame Ctrl 10 */

 R1361 (0x551) - AIF2 Frame Ctrl 11 */

 R1362 (0x552) - AIF2 Frame Ctrl 12 */

 R1363 (0x553) - AIF2 Frame Ctrl 13 */

 R1364 (0x554) - AIF2 Frame Ctrl 14 */

 R1365 (0x555) - AIF2 Frame Ctrl 15 */

 R1366 (0x556) - AIF2 Frame Ctrl 16 */

 R1367 (0x557) - AIF2 Frame Ctrl 17 */

 R1368 (0x558) - AIF2 Frame Ctrl 18 */

 R1369 (0x559) - AIF2 Tx Enables */

 R1370 (0x55a) - AIF2 Rx Enables */

 R1408 (0x580) - AIF3 BCLK Ctrl */

 R1409 (0x581) - AIF3 Tx Pin Ctrl */

 R1410 (0x582) - AIF3 Rx Pin Ctrl */

 R1411 (0x583) - AIF3 Rate Ctrl */

 R1412 (0x584) - AIF3 Format */

 R1414 (0x586) - AIF3 Rx BCLK Rate */

 R1415 (0x587) - AIF3 Frame Ctrl 1 */

 R1416 (0x588) - AIF3 Frame Ctrl 2 */

 R1417 (0x589) - AIF3 Frame Ctrl 3 */

 R1418 (0x58a) - AIF3 Frame Ctrl 4 */

 R1425 (0x591) - AIF3 Frame Ctrl 11 */

 R1426 (0x592) - AIF3 Frame Ctrl 12 */

 R1433 (0x599) - AIF3 Tx Enables */

 R1434 (0x59a) - AIF3 Rx Enables */

 R1440 (0x5a0) - AIF4 BCLK Ctrl */

 R1441 (0x5a1) - AIF4 Tx Pin Ctrl */

 R1442 (0x5a2) - AIF4 Rx Pin Ctrl */

 R1443 (0x5a3) - AIF4 Rate Ctrl */

 R1444 (0x5a4) - AIF4 Format */

 R1446 (0x5a6) - AIF4 Rx BCLK Rate */

 R1447 (0x5a7) - AIF4 Frame Ctrl 1 */

 R1448 (0x5a8) - AIF4 Frame Ctrl 2 */

 R1449 (0x5a9) - AIF4 Frame Ctrl 3 */

 R1450 (0x5aa) - AIF4 Frame Ctrl 4 */

 R1457 (0x5b1) - AIF4 Frame Ctrl 11 */

 R1458 (0x5b2) - AIF4 Frame Ctrl 12 */

 R1465 (0x5b9) - AIF4 Tx Enables */

 R1466 (0x5ba) - AIF4 Rx Enables */

 R1474 (0x5c2) - SPD1 TX Control */

 R1507 (0x5e3) - SLIMbus Framer Ref Gear */

 R1509 (0x5e5) - SLIMbus Rates 1 */

 R1510 (0x5e6) - SLIMbus Rates 2 */

 R1511 (0x5e7) - SLIMbus Rates 3 */

 R1512 (0x5e8) - SLIMbus Rates 4 */

 R1513 (0x5e9) - SLIMbus Rates 5 */

 R1514 (0x5ea) - SLIMbus Rates 6 */

 R1515 (0x5eb) - SLIMbus Rates 7 */

 R1516 (0x5ec) - SLIMbus Rates 8 */

 R1525 (0x5f5) - SLIMbus RX Channel Enable */

 R1526 (0x5F6) - SLIMbus TX Channel Enable */

 R1600 (0x640) - PWM1MIX Input 1 Source */

 R1601 (0x641) - PWM1MIX Input 1 Volume */

 R1602 (0x642) - PWM1MIX Input 2 Source */

 R1603 (0x643) - PWM1MIX Input 2 Volume */

 R1604 (0x644) - PWM1MIX Input 3 Source */

 R1605 (0x645) - PWM1MIX Input 3 Volume */

 R1606 (0x646) - PWM1MIX Input 4 Source */

 R1607 (0x647) - PWM1MIX Input 4 Volume */

 R1608 (0x648) - PWM2MIX Input 1 Source */

 R1609 (0x649) - PWM2MIX Input 1 Volume */

 R1610 (0x64a) - PWM2MIX Input 2 Source */

 R1611 (0x64b) - PWM2MIX Input 2 Volume */

 R1612 (0x64c) - PWM2MIX Input 3 Source */

 R1613 (0x64d) - PWM2MIX Input 3 Volume */

 R1614 (0x64e) - PWM2MIX Input 4 Source */

 R1615 (0x64f) - PWM2MIX Input 4 Volume */

 R1664 (0x680) - OUT1LMIX Input 1 Source */

 R1665 (0x681) - OUT1LMIX Input 1 Volume */

 R1666 (0x682) - OUT1LMIX Input 2 Source */

 R1667 (0x683) - OUT1LMIX Input 2 Volume */

 R1668 (0x684) - OUT1LMIX Input 3 Source */

 R1669 (0x685) - OUT1LMIX Input 3 Volume */

 R1670 (0x686) - OUT1LMIX Input 4 Source */

 R1671 (0x687) - OUT1LMIX Input 4 Volume */

 R1672 (0x688) - OUT1RMIX Input 1 Source */

 R1673 (0x689) - OUT1RMIX Input 1 Volume */

 R1674 (0x68a) - OUT1RMIX Input 2 Source */

 R1675 (0x68b) - OUT1RMIX Input 2 Volume */

 R1672 (0x68c) - OUT1RMIX Input 3 Source */

 R1673 (0x68d) - OUT1RMIX Input 3 Volume */

 R1674 (0x68e) - OUT1RMIX Input 4 Source */

 R1675 (0x68f) - OUT1RMIX Input 4 Volume */

 R1680 (0x690) - OUT2LMIX Input 1 Source */

 R1681 (0x691) - OUT2LMIX Input 1 Volume */

 R1682 (0x692) - OUT2LMIX Input 2 Source */

 R1683 (0x693) - OUT2LMIX Input 2 Volume */

 R1684 (0x694) - OUT2LMIX Input 3 Source */

 R1685 (0x695) - OUT2LMIX Input 3 Volume */

 R1686 (0x696) - OUT2LMIX Input 4 Source */

 R1687 (0x697) - OUT2LMIX Input 4 Volume */

 R1688 (0x698) - OUT2RMIX Input 1 Source */

 R1689 (0x699) - OUT2RMIX Input 1 Volume */

 R1690 (0x69a) - OUT2RMIX Input 2 Source */

 R1691 (0x69b) - OUT2RMIX Input 2 Volume */

 R1692 (0x69c) - OUT2RMIX Input 3 Source */

 R1693 (0x69d) - OUT2RMIX Input 3 Volume */

 R1694 (0x69e) - OUT2RMIX Input 4 Source */

 R1695 (0x69f) - OUT2RMIX Input 4 Volume */

 R1696 (0x6a0) - OUT3LMIX Input 1 Source */

 R1697 (0x6a1) - OUT3LMIX Input 1 Volume */

 R1698 (0x6a2) - OUT3LMIX Input 2 Source */

 R1699 (0x6a3) - OUT3LMIX Input 2 Volume */

 R1700 (0x6a4) - OUT3LMIX Input 3 Source */

 R1701 (0x6a5) - OUT3LMIX Input 3 Volume */

 R1702 (0x6a6) - OUT3LMIX Input 4 Source */

 R1703 (0x6a7) - OUT3LMIX Input 4 Volume */

 R1704 (0x6a8) - OUT3RMIX Input 1 Source */

 R1705 (0x6a9) - OUT3RMIX Input 1 Volume */

 R1706 (0x6aa) - OUT3RMIX Input 2 Source */

 R1707 (0x6ab) - OUT3RMIX Input 2 Volume */

 R1708 (0x6ac) - OUT3RMIX Input 3 Source */

 R1709 (0x6ad) - OUT3RMIX Input 3 Volume */

 R1710 (0x6ae) - OUT3RMIX Input 4 Source */

 R1711 (0x6af) - OUT3RMIX Input 4 Volume */

 R1728 (0x6c0) - OUT5LMIX Input 1 Source */

 R1729 (0x6c1) - OUT5LMIX Input 1 Volume */

 R1730 (0x6c2) - OUT5LMIX Input 2 Source */

 R1731 (0x6c3) - OUT5LMIX Input 2 Volume */

 R1732 (0x6c4) - OUT5LMIX Input 3 Source */

 R1733 (0x6c5) - OUT5LMIX Input 3 Volume */

 R1734 (0x6c6) - OUT5LMIX Input 4 Source */

 R1735 (0x6c7) - OUT5LMIX Input 4 Volume */

 R1736 (0x6c8) - OUT5RMIX Input 1 Source */

 R1737 (0x6c9) - OUT5RMIX Input 1 Volume */

 R1738 (0x6ca) - OUT5RMIX Input 2 Source */

 R1739 (0x6cb) - OUT5RMIX Input 2 Volume */

 R1740 (0x6cc) - OUT5RMIX Input 3 Source */

 R1741 (0x6cd) - OUT5RMIX Input 3 Volume */

 R1742 (0x6ce) - OUT5RMIX Input 4 Source */

 R1743 (0x6cf) - OUT5RMIX Input 4 Volume */

 R1792 (0x700) - AIF1TX1MIX Input 1 Source */

 R1793 (0x701) - AIF1TX1MIX Input 1 Volume */

 R1794 (0x702) - AIF1TX1MIX Input 2 Source */

 R1795 (0x703) - AIF1TX1MIX Input 2 Volume */

 R1796 (0x704) - AIF1TX1MIX Input 3 Source */

 R1797 (0x705) - AIF1TX1MIX Input 3 Volume */

 R1798 (0x706) - AIF1TX1MIX Input 4 Source */

 R1799 (0x707) - AIF1TX1MIX Input 4 Volume */

 R1800 (0x708) - AIF1TX2MIX Input 1 Source */

 R1801 (0x709) - AIF1TX2MIX Input 1 Volume */

 R1802 (0x70a) - AIF1TX2MIX Input 2 Source */

 R1803 (0x70b) - AIF1TX2MIX Input 2 Volume */

 R1804 (0x70c) - AIF1TX2MIX Input 3 Source */

 R1805 (0x70d) - AIF1TX2MIX Input 3 Volume */

 R1806 (0x70e) - AIF1TX2MIX Input 4 Source */

 R1807 (0x70f) - AIF1TX2MIX Input 4 Volume */

 R1808 (0x710) - AIF1TX3MIX Input 1 Source */

 R1809 (0x711) - AIF1TX3MIX Input 1 Volume */

 R1810 (0x712) - AIF1TX3MIX Input 2 Source */

 R1811 (0x713) - AIF1TX3MIX Input 2 Volume */

 R1812 (0x714) - AIF1TX3MIX Input 3 Source */

 R1813 (0x715) - AIF1TX3MIX Input 3 Volume */

 R1814 (0x716) - AIF1TX3MIX Input 4 Source */

 R1815 (0x717) - AIF1TX3MIX Input 4 Volume */

 R1816 (0x718) - AIF1TX4MIX Input 1 Source */

 R1817 (0x719) - AIF1TX4MIX Input 1 Volume */

 R1818 (0x71a) - AIF1TX4MIX Input 2 Source */

 R1819 (0x71b) - AIF1TX4MIX Input 2 Volume */

 R1820 (0x71c) - AIF1TX4MIX Input 3 Source */

 R1821 (0x71d) - AIF1TX4MIX Input 3 Volume */

 R1822 (0x71e) - AIF1TX4MIX Input 4 Source */

 R1823 (0x71f) - AIF1TX4MIX Input 4 Volume */

 R1824 (0x720) - AIF1TX5MIX Input 1 Source */

 R1825 (0x721) - AIF1TX5MIX Input 1 Volume */

 R1826 (0x722) - AIF1TX5MIX Input 2 Source */

 R1827 (0x723) - AIF1TX5MIX Input 2 Volume */

 R1828 (0x724) - AIF1TX5MIX Input 3 Source */

 R1829 (0x725) - AIF1TX5MIX Input 3 Volume */

 R1830 (0x726) - AIF1TX5MIX Input 4 Source */

 R1831 (0x727) - AIF1TX5MIX Input 4 Volume */

 R1832 (0x728) - AIF1TX6MIX Input 1 Source */

 R1833 (0x729) - AIF1TX6MIX Input 1 Volume */

 R1834 (0x72a) - AIF1TX6MIX Input 2 Source */

 R1835 (0x72b) - AIF1TX6MIX Input 2 Volume */

 R1836 (0x72c) - AIF1TX6MIX Input 3 Source */

 R1837 (0x72d) - AIF1TX6MIX Input 3 Volume */

 R1838 (0x72e) - AIF1TX6MIX Input 4 Source */

 R1839 (0x72f) - AIF1TX6MIX Input 4 Volume */

 R1840 (0x730) - AIF1TX7MIX Input 1 Source */

 R1841 (0x731) - AIF1TX7MIX Input 1 Volume */

 R1842 (0x732) - AIF1TX7MIX Input 2 Source */

 R1843 (0x733) - AIF1TX7MIX Input 2 Volume */

 R1844 (0x734) - AIF1TX7MIX Input 3 Source */

 R1845 (0x735) - AIF1TX7MIX Input 3 Volume */

 R1846 (0x736) - AIF1TX7MIX Input 4 Source */

 R1847 (0x737) - AIF1TX7MIX Input 4 Volume */

 R1848 (0x738) - AIF1TX8MIX Input 1 Source */

 R1849 (0x739) - AIF1TX8MIX Input 1 Volume */

 R1850 (0x73a) - AIF1TX8MIX Input 2 Source */

 R1851 (0x73b) - AIF1TX8MIX Input 2 Volume */

 R1852 (0x73c) - AIF1TX8MIX Input 3 Source */

 R1853 (0x73d) - AIF1TX8MIX Input 3 Volume */

 R1854 (0x73e) - AIF1TX8MIX Input 4 Source */

 R1855 (0x73f) - AIF1TX8MIX Input 4 Volume */

 R1856 (0x740) - AIF2TX1MIX Input 1 Source */

 R1857 (0x741) - AIF2TX1MIX Input 1 Volume */

 R1858 (0x742) - AIF2TX1MIX Input 2 Source */

 R1859 (0x743) - AIF2TX1MIX Input 2 Volume */

 R1860 (0x744) - AIF2TX1MIX Input 3 Source */

 R1861 (0x745) - AIF2TX1MIX Input 3 Volume */

 R1862 (0x746) - AIF2TX1MIX Input 4 Source */

 R1863 (0x747) - AIF2TX1MIX Input 4 Volume */

 R1864 (0x748) - AIF2TX2MIX Input 1 Source */

 R1865 (0x749) - AIF2TX2MIX Input 1 Volume */

 R1866 (0x74a) - AIF2TX2MIX Input 2 Source */

 R1867 (0x74b) - AIF2TX2MIX Input 2 Volume */

 R1868 (0x74c) - AIF2TX2MIX Input 3 Source */

 R1869 (0x74d) - AIF2TX2MIX Input 3 Volume */

 R1870 (0x74e) - AIF2TX2MIX Input 4 Source */

 R1871 (0x74f) - AIF2TX2MIX Input 4 Volume */

 R1872 (0x750) - AIF2TX3MIX Input 1 Source */

 R1873 (0x751) - AIF2TX3MIX Input 1 Volume */

 R1874 (0x752) - AIF2TX3MIX Input 2 Source */

 R1875 (0x753) - AIF2TX3MIX Input 2 Volume */

 R1876 (0x754) - AIF2TX3MIX Input 3 Source */

 R1877 (0x755) - AIF2TX3MIX Input 3 Volume */

 R1878 (0x756) - AIF2TX3MIX Input 4 Source */

 R1879 (0x757) - AIF2TX3MIX Input 4 Volume */

 R1880 (0x758) - AIF2TX4MIX Input 1 Source */

 R1881 (0x759) - AIF2TX4MIX Input 1 Volume */

 R1882 (0x75a) - AIF2TX4MIX Input 2 Source */

 R1883 (0x75b) - AIF2TX4MIX Input 2 Volume */

 R1884 (0x75c) - AIF2TX4MIX Input 3 Source */

 R1885 (0x75d) - AIF2TX4MIX Input 3 Volume */

 R1886 (0x75e) - AIF2TX4MIX Input 4 Source */

 R1887 (0x75f) - AIF2TX4MIX Input 4 Volume */

 R1888 (0x760) - AIF2TX5MIX Input 1 Source */

 R1889 (0x761) - AIF2TX5MIX Input 1 Volume */

 R1890 (0x762) - AIF2TX5MIX Input 2 Source */

 R1891 (0x763) - AIF2TX5MIX Input 2 Volume */

 R1892 (0x764) - AIF2TX5MIX Input 3 Source */

 R1893 (0x765) - AIF2TX5MIX Input 3 Volume */

 R1894 (0x766) - AIF2TX5MIX Input 4 Source */

 R1895 (0x767) - AIF2TX5MIX Input 4 Volume */

 R1896 (0x768) - AIF2TX6MIX Input 1 Source */

 R1897 (0x769) - AIF2TX6MIX Input 1 Volume */

 R1898 (0x76a) - AIF2TX6MIX Input 2 Source */

 R1899 (0x76b) - AIF2TX6MIX Input 2 Volume */

 R1900 (0x76c) - AIF2TX6MIX Input 3 Source */

 R1901 (0x76d) - AIF2TX6MIX Input 3 Volume */

 R1902 (0x76e) - AIF2TX6MIX Input 4 Source */

 R1903 (0x76f) - AIF2TX6MIX Input 4 Volume */

 R1904 (0x770) - AIF2TX7MIX Input 1 Source */

 R1905 (0x771) - AIF2TX7MIX Input 1 Volume */

 R1906 (0x772) - AIF2TX7MIX Input 2 Source */

 R1907 (0x773) - AIF2TX7MIX Input 2 Volume */

 R1908 (0x774) - AIF2TX7MIX Input 3 Source */

 R1909 (0x775) - AIF2TX7MIX Input 3 Volume */

 R1910 (0x776) - AIF2TX7MIX Input 4 Source */

 R1911 (0x777) - AIF2TX7MIX Input 4 Volume */

 R1912 (0x778) - AIF2TX8MIX Input 1 Source */

 R1913 (0x779) - AIF2TX8MIX Input 1 Volume */

 R1914 (0x77a) - AIF2TX8MIX Input 2 Source */

 R1915 (0x77b) - AIF2TX8MIX Input 2 Volume */

 R1916 (0x77c) - AIF2TX8MIX Input 3 Source */

 R1917 (0x77d) - AIF2TX8MIX Input 3 Volume */

 R1918 (0x77e) - AIF2TX8MIX Input 4 Source */

 R1919 (0x77f) - AIF2TX8MIX Input 4 Volume */

 R1920 (0x780) - AIF3TX1MIX Input 1 Source */

 R1921 (0x781) - AIF3TX1MIX Input 1 Volume */

 R1922 (0x782) - AIF3TX1MIX Input 2 Source */

 R1923 (0x783) - AIF3TX1MIX Input 2 Volume */

 R1924 (0x784) - AIF3TX1MIX Input 3 Source */

 R1925 (0x785) - AIF3TX1MIX Input 3 Volume */

 R1926 (0x786) - AIF3TX1MIX Input 4 Source */

 R1927 (0x787) - AIF3TX1MIX Input 4 Volume */

 R1928 (0x788) - AIF3TX2MIX Input 1 Source */

 R1929 (0x789) - AIF3TX2MIX Input 1 Volume */

 R1930 (0x78a) - AIF3TX2MIX Input 2 Source */

 R1931 (0x78b) - AIF3TX2MIX Input 2 Volume */

 R1932 (0x78c) - AIF3TX2MIX Input 3 Source */

 R1933 (0x78d) - AIF3TX2MIX Input 3 Volume */

 R1934 (0x78e) - AIF3TX2MIX Input 4 Source */

 R1935 (0x78f) - AIF3TX2MIX Input 4 Volume */

 R1952 (0x7a0) - AIF4TX1MIX Input 1 Source */

 R1953 (0x7a1) - AIF4TX1MIX Input 1 Volume */

 R1954 (0x7a2) - AIF4TX1MIX Input 2 Source */

 R1955 (0x7a3) - AIF4TX1MIX Input 2 Volume */

 R1956 (0x7a4) - AIF4TX1MIX Input 3 Source */

 R1957 (0x7a5) - AIF4TX1MIX Input 3 Volume */

 R1958 (0x7a6) - AIF4TX1MIX Input 4 Source */

 R1959 (0x7a7) - AIF4TX1MIX Input 4 Volume */

 R1960 (0x7a8) - AIF4TX2MIX Input 1 Source */

 R1961 (0x7a9) - AIF4TX2MIX Input 1 Volume */

 R1962 (0x7aa) - AIF4TX2MIX Input 2 Source */

 R1963 (0x7ab) - AIF4TX2MIX Input 2 Volume */

 R1964 (0x7ac) - AIF4TX2MIX Input 3 Source */

 R1965 (0x7ad) - AIF4TX2MIX Input 3 Volume */

 R1966 (0x7ae) - AIF4TX2MIX Input 4 Source */

 R1967 (0x7af) - AIF4TX2MIX Input 4 Volume */

 R1984 (0x7c0) - SLIMTX1MIX Input 1 Source */

 R1985 (0x7c1) - SLIMTX1MIX Input 1 Volume */

 R1986 (0x7c2) - SLIMTX1MIX Input 2 Source */

 R1987 (0x7c3) - SLIMTX1MIX Input 2 Volume */

 R1988 (0x7c4) - SLIMTX1MIX Input 3 Source */

 R1989 (0x7c5) - SLIMTX1MIX Input 3 Volume */

 R1990 (0x7c6) - SLIMTX1MIX Input 4 Source */

 R1991 (0x7c7) - SLIMTX1MIX Input 4 Volume */

 R1992 (0x7c8) - SLIMTX2MIX Input 1 Source */

 R1993 (0x7c9) - SLIMTX2MIX Input 1 Volume */

 R1994 (0x7ca) - SLIMTX2MIX Input 2 Source */

 R1995 (0x7cb) - SLIMTX2MIX Input 2 Volume */

 R1996 (0x7cc) - SLIMTX2MIX Input 3 Source */

 R1997 (0x7cd) - SLIMTX2MIX Input 3 Volume */

 R1998 (0x7ce) - SLIMTX2MIX Input 4 Source */

 R1999 (0x7cf) - SLIMTX2MIX Input 4 Volume */

 R2000 (0x7d0) - SLIMTX3MIX Input 1 Source */

 R2001 (0x7d1) - SLIMTX3MIX Input 1 Volume */

 R2002 (0x7d2) - SLIMTX3MIX Input 2 Source */

 R2003 (0x7d3) - SLIMTX3MIX Input 2 Volume */

 R2004 (0x7d4) - SLIMTX3MIX Input 3 Source */

 R2005 (0x7d5) - SLIMTX3MIX Input 3 Volume */

 R2006 (0x7d6) - SLIMTX3MIX Input 4 Source */

 R2007 (0x7d7) - SLIMTX3MIX Input 4 Volume */

 R2008 (0x7d8) - SLIMTX4MIX Input 1 Source */

 R2009 (0x7d9) - SLIMTX4MIX Input 1 Volume */

 R2010 (0x7da) - SLIMTX4MIX Input 2 Source */

 R2011 (0x7db) - SLIMTX4MIX Input 2 Volume */

 R2012 (0x7dc) - SLIMTX4MIX Input 3 Source */

 R2013 (0x7dd) - SLIMTX4MIX Input 3 Volume */

 R2014 (0x7de) - SLIMTX4MIX Input 4 Source */

 R2015 (0x7df) - SLIMTX4MIX Input 4 Volume */

 R2016 (0x7e0) - SLIMTX5MIX Input 1 Source */

 R2017 (0x7e1) - SLIMTX5MIX Input 1 Volume */

 R2018 (0x7e2) - SLIMTX5MIX Input 2 Source */

 R2019 (0x7e3) - SLIMTX5MIX Input 2 Volume */

 R2020 (0x7e4) - SLIMTX5MIX Input 3 Source */

 R2021 (0x7e5) - SLIMTX5MIX Input 3 Volume */

 R2022 (0x7e6) - SLIMTX5MIX Input 4 Source */

 R2023 (0x7e7) - SLIMTX5MIX Input 4 Volume */

 R2024 (0x7e8) - SLIMTX6MIX Input 1 Source */

 R2025 (0x7e9) - SLIMTX6MIX Input 1 Volume */

 R2026 (0x7ea) - SLIMTX6MIX Input 2 Source */

 R2027 (0x7eb) - SLIMTX6MIX Input 2 Volume */

 R2028 (0x7ec) - SLIMTX6MIX Input 3 Source */

 R2029 (0x7ed) - SLIMTX6MIX Input 3 Volume */

 R2030 (0x7ee) - SLIMTX6MIX Input 4 Source */

 R2031 (0x7ef) - SLIMTX6MIX Input 4 Volume */

 R2032 (0x7f0) - SLIMTX7MIX Input 1 Source */

 R2033 (0x7f1) - SLIMTX7MIX Input 1 Volume */

 R2034 (0x7f2) - SLIMTX7MIX Input 2 Source */

 R2035 (0x7f3) - SLIMTX7MIX Input 2 Volume */

 R2036 (0x7f4) - SLIMTX7MIX Input 3 Source */

 R2037 (0x7f5) - SLIMTX7MIX Input 3 Volume */

 R2038 (0x7f6) - SLIMTX7MIX Input 4 Source */

 R2039 (0x7f7) - SLIMTX7MIX Input 4 Volume */

 R2040 (0x7f8) - SLIMTX8MIX Input 1 Source */

 R2041 (0x7f9) - SLIMTX8MIX Input 1 Volume */

 R2042 (0x7fa) - SLIMTX8MIX Input 2 Source */

 R2043 (0x7fb) - SLIMTX8MIX Input 2 Volume */

 R2044 (0x7fc) - SLIMTX8MIX Input 3 Source */

 R2045 (0x7fd) - SLIMTX8MIX Input 3 Volume */

 R2046 (0x7fe) - SLIMTX8MIX Input 4 Source */

 R2047 (0x7ff) - SLIMTX8MIX Input 4 Volume */

 R2048 (0x800) - SPDIF1TX1MIX Input 1 Source */

 R2049 (0x801) - SPDIF1TX1MIX Input 1 Volume */

 R2056 (0x808) - SPDIF1TX2MIX Input 1 Source */

 R2057 (0x809) - SPDIF1TX2MIX Input 1 Volume */

 R2176 (0x880) - EQ1MIX Input 1 Source */

 R2177 (0x881) - EQ1MIX Input 1 Volume */

 R2178 (0x882) - EQ1MIX Input 2 Source */

 R2179 (0x883) - EQ1MIX Input 2 Volume */

 R2180 (0x884) - EQ1MIX Input 3 Source */

 R2181 (0x885) - EQ1MIX Input 3 Volume */

 R2182 (0x886) - EQ1MIX Input 4 Source */

 R2183 (0x887) - EQ1MIX Input 4 Volume */

 R2184 (0x888) - EQ2MIX Input 1 Source */

 R2185 (0x889) - EQ2MIX Input 1 Volume */

 R2186 (0x88a) - EQ2MIX Input 2 Source */

 R2187 (0x88b) - EQ2MIX Input 2 Volume */

 R2188 (0x88c) - EQ2MIX Input 3 Source */

 R2189 (0x88d) - EQ2MIX Input 3 Volume */

 R2190 (0x88e) - EQ2MIX Input 4 Source */

 R2191 (0x88f) - EQ2MIX Input 4 Volume */

 R2192 (0x890) - EQ3MIX Input 1 Source */

 R2193 (0x891) - EQ3MIX Input 1 Volume */

 R2194 (0x892) - EQ3MIX Input 2 Source */

 R2195 (0x893) - EQ3MIX Input 2 Volume */

 R2196 (0x894) - EQ3MIX Input 3 Source */

 R2197 (0x895) - EQ3MIX Input 3 Volume */

 R2198 (0x896) - EQ3MIX Input 4 Source */

 R2199 (0x897) - EQ3MIX Input 4 Volume */

 R2200 (0x898) - EQ4MIX Input 1 Source */

 R2201 (0x899) - EQ4MIX Input 1 Volume */

 R2202 (0x89a) - EQ4MIX Input 2 Source */

 R2203 (0x89b) - EQ4MIX Input 2 Volume */

 R2204 (0x89c) - EQ4MIX Input 3 Source */

 R2205 (0x89d) - EQ4MIX Input 3 Volume */

 R2206 (0x89e) - EQ4MIX Input 4 Source */

 R2207 (0x89f) - EQ4MIX Input 4 Volume */

 R2240 (0x8c0) - DRC1LMIX Input 1 Source */

 R2241 (0x8c1) - DRC1LMIX Input 1 Volume */

 R2242 (0x8c2) - DRC1LMIX Input 2 Source */

 R2243 (0x8c3) - DRC1LMIX Input 2 Volume */

 R2244 (0x8c4) - DRC1LMIX Input 3 Source */

 R2245 (0x8c5) - DRC1LMIX Input 3 Volume */

 R2246 (0x8c6) - DRC1LMIX Input 4 Source */

 R2247 (0x8c7) - DRC1LMIX Input 4 Volume */

 R2248 (0x8c8) - DRC1RMIX Input 1 Source */

 R2249 (0x8c9) - DRC1RMIX Input 1 Volume */

 R2250 (0x8ca) - DRC1RMIX Input 2 Source */

 R2251 (0x8cb) - DRC1RMIX Input 2 Volume */

 R2252 (0x8cc) - DRC1RMIX Input 3 Source */

 R2253 (0x8cd) - DRC1RMIX Input 3 Volume */

 R2254 (0x8ce) - DRC1RMIX Input 4 Source */

 R2255 (0x8cf) - DRC1RMIX Input 4 Volume */

 R2256 (0x8d0) - DRC2LMIX Input 1 Source */

 R2257 (0x8d1) - DRC2LMIX Input 1 Volume */

 R2258 (0x8d2) - DRC2LMIX Input 2 Source */

 R2259 (0x8d3) - DRC2LMIX Input 2 Volume */

 R2260 (0x8d4) - DRC2LMIX Input 3 Source */

 R2261 (0x8d5) - DRC2LMIX Input 3 Volume */

 R2262 (0x8d6) - DRC2LMIX Input 4 Source */

 R2263 (0x8d7) - DRC2LMIX Input 4 Volume */

 R2264 (0x8d8) - DRC2RMIX Input 1 Source */

 R2265 (0x8d9) - DRC2RMIX Input 1 Volume */

 R2266 (0x8da) - DRC2RMIX Input 2 Source */

 R2267 (0x8db) - DRC2RMIX Input 2 Volume */

 R2268 (0x8dc) - DRC2RMIX Input 3 Source */

 R2269 (0x8dd) - DRC2RMIX Input 3 Volume */

 R2270 (0x8de) - DRC2RMIX Input 4 Source */

 R2271 (0x8df) - DRC2RMIX Input 4 Volume */

 R2304 (0x900) - HPLP1MIX Input 1 Source */

 R2305 (0x901) - HPLP1MIX Input 1 Volume */

 R2306 (0x902) - HPLP1MIX Input 2 Source */

 R2307 (0x903) - HPLP1MIX Input 2 Volume */

 R2308 (0x904) - HPLP1MIX Input 3 Source */

 R2309 (0x905) - HPLP1MIX Input 3 Volume */

 R2310 (0x906) - HPLP1MIX Input 4 Source */

 R2311 (0x907) - HPLP1MIX Input 4 Volume */

 R2312 (0x908) - HPLP2MIX Input 1 Source */

 R2313 (0x909) - HPLP2MIX Input 1 Volume */

 R2314 (0x90a) - HPLP2MIX Input 2 Source */

 R2315 (0x90b) - HPLP2MIX Input 2 Volume */

 R2316 (0x90c) - HPLP2MIX Input 3 Source */

 R2317 (0x90d) - HPLP2MIX Input 3 Volume */

 R2318 (0x90e) - HPLP2MIX Input 4 Source */

 R2319 (0x90f) - HPLP2MIX Input 4 Volume */

 R2320 (0x910) - HPLP3MIX Input 1 Source */

 R2321 (0x911) - HPLP3MIX Input 1 Volume */

 R2322 (0x912) - HPLP3MIX Input 2 Source */

 R2323 (0x913) - HPLP3MIX Input 2 Volume */

 R2324 (0x914) - HPLP3MIX Input 3 Source */

 R2325 (0x915) - HPLP3MIX Input 3 Volume */

 R2326 (0x916) - HPLP3MIX Input 4 Source */

 R2327 (0x917) - HPLP3MIX Input 4 Volume */

 R2328 (0x918) - HPLP4MIX Input 1 Source */

 R2329 (0x919) - HPLP4MIX Input 1 Volume */

 R2330 (0x91a) - HPLP4MIX Input 2 Source */

 R2331 (0x91b) - HPLP4MIX Input 2 Volume */

 R2332 (0x91c) - HPLP4MIX Input 3 Source */

 R2333 (0x91d) - HPLP4MIX Input 3 Volume */

 R2334 (0x91e) - HPLP4MIX Input 4 Source */

 R2335 (0x91f) - HPLP4MIX Input 4 Volume */

 R2368 (0x940) - DSP1LMIX Input 1 Source */

 R2369 (0x941) - DSP1LMIX Input 1 Volume */

 R2370 (0x942) - DSP1LMIX Input 2 Source */

 R2371 (0x943) - DSP1LMIX Input 2 Volume */

 R2372 (0x944) - DSP1LMIX Input 3 Source */

 R2373 (0x945) - DSP1LMIX Input 3 Volume */

 R2374 (0x946) - DSP1LMIX Input 4 Source */

 R2375 (0x947) - DSP1LMIX Input 4 Volume */

 R2376 (0x948) - DSP1RMIX Input 1 Source */

 R2377 (0x949) - DSP1RMIX Input 1 Volume */

 R2378 (0x94a) - DSP1RMIX Input 2 Source */

 R2379 (0x94b) - DSP1RMIX Input 2 Volume */

 R2380 (0x94c) - DSP1RMIX Input 3 Source */

 R2381 (0x94d) - DSP1RMIX Input 3 Volume */

 R2382 (0x94e) - DSP1RMIX Input 4 Source */

 R2383 (0x94f) - DSP1RMIX Input 4 Volume */

 R2384 (0x950) - DSP1AUX1MIX Input 1 Source */

 R2392 (0x958) - DSP1AUX2MIX Input 1 Source */

 R2400 (0x960) - DSP1AUX3MIX Input 1 Source */

 R2408 (0x968) - DSP1AUX4MIX Input 1 Source */

 R2416 (0x970) - DSP1AUX5MIX Input 1 Source */

 R2424 (0x978) - DSP1AUX6MIX Input 1 Source */

 R2432 (0x980) - DSP2LMIX Input 1 Source */

 R2433 (0x981) - DSP2LMIX Input 1 Volume */

 R2434 (0x982) - DSP2LMIX Input 2 Source */

 R2435 (0x983) - DSP2LMIX Input 2 Volume */

 R2436 (0x984) - DSP2LMIX Input 3 Source */

 R2437 (0x985) - DSP2LMIX Input 3 Volume */

 R2438 (0x986) - DSP2LMIX Input 4 Source */

 R2439 (0x987) - DSP2LMIX Input 4 Volume */

 R2440 (0x988) - DSP2RMIX Input 1 Source */

 R2441 (0x989) - DSP2RMIX Input 1 Volume */

 R2442 (0x98a) - DSP2RMIX Input 2 Source */

 R2443 (0x98b) - DSP2RMIX Input 2 Volume */

 R2444 (0x98c) - DSP2RMIX Input 3 Source */

 R2445 (0x98d) - DSP2RMIX Input 3 Volume */

 R2446 (0x98e) - DSP2RMIX Input 4 Source */

 R2447 (0x98f) - DSP2RMIX Input 4 Volume */

 R2448 (0x990) - DSP2AUX1MIX Input 1 Source */

 R2456 (0x998) - DSP2AUX2MIX Input 1 Source */

 R2464 (0x9a0) - DSP2AUX3MIX Input 1 Source */

 R2472 (0x9a8) - DSP2AUX4MIX Input 1 Source */

 R2480 (0x9b0) - DSP2AUX5MIX Input 1 Source */

 R2488 (0x9b8) - DSP2AUX6MIX Input 1 Source */

 R2496 (0x9c0) - DSP3LMIX Input 1 Source */

 R2497 (0x9c1) - DSP3LMIX Input 1 Volume */

 R2498 (0x9c2) - DSP3LMIX Input 2 Source */

 R2499 (0x9c3) - DSP3LMIX Input 2 Volume */

 R2500 (0x9c4) - DSP3LMIX Input 3 Source */

 R2501 (0x9c5) - DSP3LMIX Input 3 Volume */

 R2502 (0x9c6) - DSP3LMIX Input 4 Source */

 R2503 (0x9c7) - DSP3LMIX Input 4 Volume */

 R2504 (0x9c8) - DSP3RMIX Input 1 Source */

 R2505 (0x9c9) - DSP3RMIX Input 1 Volume */

 R2506 (0x9ca) - DSP3RMIX Input 2 Source */

 R2507 (0x9cb) - DSP3RMIX Input 2 Volume */

 R2508 (0x9cc) - DSP3RMIX Input 3 Source */

 R2509 (0x9cd) - DSP3RMIX Input 3 Volume */

 R2510 (0x9ce) - DSP3RMIX Input 4 Source */

 R2511 (0x9cf) - DSP3RMIX Input 4 Volume */

 R2512 (0x9d0) - DSP3AUX1MIX Input 1 Source */

 R2520 (0x9d8) - DSP3AUX2MIX Input 1 Source */

 R2528 (0x9e0) - DSP3AUX3MIX Input 1 Source */

 R2536 (0x9e8) - DSP3AUX4MIX Input 1 Source */

 R2544 (0x9f0) - DSP3AUX5MIX Input 1 Source */

 R2552 (0x9f8) - DSP3AUX6MIX Input 1 Source */

 R2560 (0xa00) - DSP4LMIX Input 1 Source */

 R2561 (0xa01) - DSP4LMIX Input 1 Volume */

 R2562 (0xa02) - DSP4LMIX Input 2 Source */

 R2563 (0xa03) - DSP4LMIX Input 2 Volume */

 R2564 (0xa04) - DSP4LMIX Input 3 Source */

 R2565 (0xa05) - DSP4LMIX Input 3 Volume */

 R2566 (0xa06) - DSP4LMIX Input 4 Source */

 R2567 (0xa07) - DSP4LMIX Input 4 Volume */

 R2568 (0xa08) - DSP4RMIX Input 1 Source */

 R2569 (0xa09) - DSP4RMIX Input 1 Volume */

 R2570 (0xa0a) - DSP4RMIX Input 2 Source */

 R2571 (0xa0b) - DSP4RMIX Input 2 Volume */

 R2572 (0xa0c) - DSP4RMIX Input 3 Source */

 R2573 (0xa0d) - DSP4RMIX Input 3 Volume */

 R2574 (0xa0e) - DSP4RMIX Input 4 Source */

 R2575 (0xa0f) - DSP4RMIX Input 4 Volume */

 R2576 (0xa10) - DSP4AUX1MIX Input 1 Source */

 R2584 (0xa18) - DSP4AUX2MIX Input 1 Source */

 R2592 (0xa20) - DSP4AUX3MIX Input 1 Source */

 R2600 (0xa28) - DSP4AUX4MIX Input 1 Source */

 R2608 (0xa30) - DSP4AUX5MIX Input 1 Source */

 R2616 (0xa38) - DSP4AUX6MIX Input 1 Source */

 R2624 (0xa40) - DSP5LMIX Input 1 Source */

 R2625 (0xa41) - DSP5LMIX Input 1 Volume */

 R2626 (0xa42) - DSP5LMIX Input 2 Source */

 R2627 (0xa43) - DSP5LMIX Input 2 Volume */

 R2628 (0xa44) - DSP5LMIX Input 3 Source */

 R2629 (0xa45) - DSP5LMIX Input 3 Volume */

 R2630 (0xa46) - DSP5LMIX Input 4 Source */

 R2631 (0xa47) - DSP5LMIX Input 4 Volume */

 R2632 (0xa48) - DSP5RMIX Input 1 Source */

 R2633 (0xa49) - DSP5RMIX Input 1 Volume */

 R2634 (0xa4a) - DSP5RMIX Input 2 Source */

 R2635 (0xa4b) - DSP5RMIX Input 2 Volume */

 R2636 (0xa4c) - DSP5RMIX Input 3 Source */

 R2637 (0xa4d) - DSP5RMIX Input 3 Volume */

 R2638 (0xa4e) - DSP5RMIX Input 4 Source */

 R2639 (0xa4f) - DSP5RMIX Input 4 Volume */

 R2640 (0xa50) - DSP5AUX1MIX Input 1 Source */

 R2658 (0xa58) - DSP5AUX2MIX Input 1 Source */

 R2656 (0xa60) - DSP5AUX3MIX Input 1 Source */

 R2664 (0xa68) - DSP5AUX4MIX Input 1 Source */

 R2672 (0xa70) - DSP5AUX5MIX Input 1 Source */

 R2680 (0xa78) - DSP5AUX6MIX Input 1 Source */

 R2688 (0xa80) - ASRC1_1LMIX Input 1 Source */

 R2696 (0xa88) - ASRC1_1RMIX Input 1 Source */

 R2704 (0xa90) - ASRC1_2LMIX Input 1 Source */

 R2712 (0xa98) - ASRC1_2RMIX Input 1 Source */

 R2720 (0xaa0) - ASRC2_1LMIX Input 1 Source */

 R2728 (0xaa8) - ASRC2_1RMIX Input 1 Source */

 R2736 (0xab0) - ASRC2_2LMIX Input 1 Source */

 R2744 (0xab8) - ASRC2_2RMIX Input 1 Source */

 R2816 (0xb00) - ISRC1DEC1MIX Input 1 Source*/

 R2824 (0xb08) - ISRC1DEC2MIX Input 1 Source*/

 R2832 (0xb10) - ISRC1DEC3MIX Input 1 Source*/

 R2840 (0xb18) - ISRC1DEC4MIX Input 1 Source*/

 R2848 (0xb20) - ISRC1INT1MIX Input 1 Source*/

 R2856 (0xb28) - ISRC1INT2MIX Input 1 Source*/

 R2864 (0xb30) - ISRC1INT3MIX Input 1 Source*/

 R2872 (0xb38) - ISRC1INT4MIX Input 1 Source*/

 R2880 (0xb40) - ISRC2DEC1MIX Input 1 Source*/

 R2888 (0xb48) - ISRC2DEC2MIX Input 1 Source*/

 R2896 (0xb50) - ISRC2DEC3MIX Input 1 Source*/

 R2904 (0xb58) - ISRC2DEC4MIX Input 1 Source*/

 R2912 (0xb60) - ISRC2INT1MIX Input 1 Source*/

 R2920 (0xb68) - ISRC2INT2MIX Input 1 Source*/

 R2928 (0xb70) - ISRC2INT3MIX Input 1 Source*/

 R2936 (0xb78) - ISRC2INT4MIX Input 1 Source*/

 R2944 (0xb80) - ISRC3DEC1MIX Input 1 Source*/

 R2952 (0xb88) - ISRC3DEC2MIX Input 1 Source*/

 R2976 (0xb80) - ISRC3INT1MIX Input 1 Source*/

 R2984 (0xb88) - ISRC3INT2MIX Input 1 Source*/

 R3008 (0xbc0) - ISRC4DEC1MIX Input 1 Source */

 R3016 (0xbc8) - ISRC4DEC2MIX Input 1 Source */

 R3040 (0xbe0) - ISRC4INT1MIX Input 1 Source */

 R3048 (0xbe8) - ISRC4INT2MIX Input 1 Source */

 R3072 (0xc00) - DSP6LMIX Input 1 Source */

 R3073 (0xc01) - DSP6LMIX Input 1 Volume */

 R3074 (0xc02) - DSP6LMIX Input 2 Source */

 R3075 (0xc03) - DSP6LMIX Input 2 Volume */

 R3076 (0xc04) - DSP6LMIX Input 3 Source */

 R3077 (0xc05) - DSP6LMIX Input 3 Volume */

 R3078 (0xc06) - DSP6LMIX Input 4 Source */

 R3079 (0xc07) - DSP6LMIX Input 4 Volume */

 R3080 (0xc08) - DSP6RMIX Input 1 Source */

 R3081 (0xc09) - DSP6RMIX Input 1 Volume */

 R3082 (0xc0a) - DSP6RMIX Input 2 Source */

 R3083 (0xc0b) - DSP6RMIX Input 2 Volume */

 R3084 (0xc0c) - DSP6RMIX Input 3 Source */

 R3085 (0xc0d) - DSP6RMIX Input 3 Volume */

 R3086 (0xc0e) - DSP6RMIX Input 4 Source */

 R3087 (0xc0f) - DSP6RMIX Input 4 Volume */

 R3088 (0xc10) - DSP6AUX1MIX Input 1 Source */

 R3088 (0xc18) - DSP6AUX2MIX Input 1 Source */

 R3088 (0xc20) - DSP6AUX3MIX Input 1 Source */

 R3088 (0xc28) - DSP6AUX4MIX Input 1 Source */

 R3088 (0xc30) - DSP6AUX5MIX Input 1 Source */

 R3088 (0xc38) - DSP6AUX6MIX Input 1 Source */

 R3136 (0xc40) - DSP7LMIX Input 1 Source */

 R3137 (0xc41) - DSP7LMIX Input 1 Volume */

 R3138 (0xc42) - DSP7LMIX Input 2 Source */

 R3139 (0xc43) - DSP7LMIX Input 2 Volume */

 R3140 (0xc44) - DSP7LMIX Input 3 Source */

 R3141 (0xc45) - DSP7lMIX Input 3 Volume */

 R3142 (0xc46) - DSP7lMIX Input 4 Source */

 R3143 (0xc47) - DSP7LMIX Input 4 Volume */

 R3144 (0xc48) - DSP7RMIX Input 1 Source */

 R3145 (0xc49) - DSP7RMIX Input 1 Volume */

 R3146 (0xc4a) - DSP7RMIX Input 2 Source */

 R3147 (0xc4b) - DSP7RMIX Input 2 Volume */

 R3148 (0xc4c) - DSP7RMIX Input 3 Source */

 R3159 (0xc4d) - DSP7RMIX Input 3 Volume */

 R3150 (0xc4e) - DSP7RMIX Input 4 Source */

 R3151 (0xc4f) - DSP7RMIX Input 4 Volume */

 R3152 (0xc50) - DSP7AUX1MIX Input 1 Source */

 R3160 (0xc58) - DSP7AUX2MIX Input 1 Source */

 R3168 (0xc60) - DSP7AUX3MIX Input 1 Source */

 R3176 (0xc68) - DSP7AUX4MIX Input 1 Source */

 R3184 (0xc70) - DSP7AUX5MIX Input 1 Source */

 R3192 (0xc78) - DSP7AUX6MIX Input 1 Source */

 R3520 (0xdc0) - DFC1MIX Input 1 Source */

 R3528 (0xdc8) - DFC2MIX Input 1 Source */

 R3536 (0xdd0) - DFC3MIX Input 1 Source */

 R3544 (0xdd8) - DFC4MIX Input 1 Source */

 R3552 (0xde0) - DFC5MIX Input 1 Source */

 R3560 (0xde8) - DFC6MIX Input 1 Source */

 R3568 (0xdf0) - DFC7MIX Input 1 Source */

 R3576 (0xdf8) - DFC8MIX Input 1 Source */

 R3584 (0xe00) - FX_Ctrl1 */

 R3600 (0xe10) - EQ1_1 */

 R3601 (0xe11) - EQ1_2 */

 R3602 (0xe12) - EQ1_3 */

 R3603 (0xe13) - EQ1_4 */

 R3604 (0xe14) - EQ1_5 */

 R3605 (0xe15) - EQ1_6 */

 R3606 (0xe16) - EQ1_7 */

 R3607 (0xe17) - EQ1_8 */

 R3608 (0xe18) - EQ1_9 */

 R3609 (0xe19) - EQ1_10 */

 R3610 (0xe1a) - EQ1_11 */

 R3611 (0xe1b) - EQ1_12 */

 R3612 (0xe1c) - EQ1_13 */

 R3613 (0xe1d) - EQ1_14 */

 R3614 (0xe1e) - EQ1_15 */

 R3615 (0xe1f) - EQ1_16 */

 R3616 (0xe20) - EQ1_17 */

 R3617 (0xe21) - EQ1_18 */

 R3618 (0xe22) - EQ1_19 */

 R3619 (0xe23) - EQ1_20 */

 R3620 (0xe24) - EQ1_21 */

 R3622 (0xe26) - EQ2_1 */

 R3623 (0xe27) - EQ2_2 */

 R3624 (0xe28) - EQ2_3 */

 R3625 (0xe29) - EQ2_4 */

 R3626 (0xe2a) - EQ2_5 */

 R3627 (0xe2b) - EQ2_6 */

 R3628 (0xe2c) - EQ2_7 */

 R3629 (0xe2d) - EQ2_8 */

 R3630 (0xe2e) - EQ2_9 */

 R3631 (0xe2f) - EQ2_10 */

 R3632 (0xe30) - EQ2_11 */

 R3633 (0xe31) - EQ2_12 */

 R3634 (0xe32) - EQ2_13 */

 R3635 (0xe33) - EQ2_14 */

 R3636 (0xe34) - EQ2_15 */

 R3637 (0xe35) - EQ2_16 */

 R3638 (0xe36) - EQ2_17 */

 R3639 (0xe37) - EQ2_18 */

 R3640 (0xe38) - EQ2_19 */

 R3641 (0xe39) - EQ2_20 */

 R3642 (0xe3a) - EQ2_21 */

 R3644 (0xe3c) - EQ3_1 */

 R3645 (0xe3d) - EQ3_2 */

 R3646 (0xe3e) - EQ3_3 */

 R3647 (0xe3f) - EQ3_4 */

 R3648 (0xe40) - EQ3_5 */

 R3649 (0xe41) - EQ3_6 */

 R3650 (0xe42) - EQ3_7 */

 R3651 (0xe43) - EQ3_8 */

 R3652 (0xe44) - EQ3_9 */

 R3653 (0xe45) - EQ3_10 */

 R3654 (0xe46) - EQ3_11 */

 R3655 (0xe47) - EQ3_12 */

 R3656 (0xe48) - EQ3_13 */

 R3657 (0xe49) - EQ3_14 */

 R3658 (0xe4a) - EQ3_15 */

 R3659 (0xe4b) - EQ3_16 */

 R3660 (0xe4c) - EQ3_17 */

 R3661 (0xe4d) - EQ3_18 */

 R3662 (0xe4e) - EQ3_19 */

 R3663 (0xe4f) - EQ3_20 */

 R3664 (0xe50) - EQ3_21 */

 R3666 (0xe52) - EQ4_1 */

 R3667 (0xe53) - EQ4_2 */

 R3668 (0xe54) - EQ4_3 */

 R3669 (0xe55) - EQ4_4 */

 R3670 (0xe56) - EQ4_5 */

 R3671 (0xe57) - EQ4_6 */

 R3672 (0xe58) - EQ4_7 */

 R3673 (0xe59) - EQ4_8 */

 R3674 (0xe5a) - EQ4_9 */

 R3675 (0xe5b) - EQ4_10 */

 R3676 (0xe5c) - EQ4_11 */

 R3677 (0xe5d) - EQ4_12 */

 R3678 (0xe5e) - EQ4_13 */

 R3679 (0xe5f) - EQ4_14 */

 R3680 (0xe60) - EQ4_15 */

 R3681 (0xe61) - EQ4_16 */

 R3682 (0xe62) - EQ4_17 */

 R3683 (0xe63) - EQ4_18 */

 R3684 (0xe64) - EQ4_19 */

 R3685 (0xe65) - EQ4_20 */

 R3686 (0xe66) - EQ4_21 */

 R3712 (0xe80) - DRC1 ctrl1 */

 R3713 (0xe81) - DRC1 ctrl2 */

 R3714 (0xe82) - DRC1 ctrl3 */

 R3715 (0xe83) - DRC1 ctrl4 */

 R3716 (0xe84) - DRC1 ctrl5 */

 R3720 (0xe88) - DRC2 ctrl1 */

 R3721 (0xe89) - DRC2 ctrl2 */

 R3722 (0xe8a) - DRC2 ctrl3 */

 R3723 (0xe8b) - DRC2 ctrl4 */

 R3724 (0xe8c) - DRC2 ctrl5 */

 R3776 (0xec0) - HPLPF1_1 */

 R3777 (0xec1) - HPLPF1_2 */

 R3780 (0xec4) - HPLPF2_1 */

 R3781 (0xec5) - HPLPF2_2 */

 R3784 (0xec8) - HPLPF3_1 */

 R3785 (0xec9) - HPLPF3_2 */

 R3788 (0xecc) - HPLPF4_1 */

 R3789 (0xecd) - HPLPF4_2 */

 R3792 (0xed0) - ASRC2_ENABLE */

 R3794 (0xed2) - ASRC2_RATE1 */

 R3795 (0xed3) - ASRC2_RATE2 */

 R3808 (0xee0) - ASRC1_ENABLE */

 R3810 (0xee2) - ASRC1_RATE1 */

 R3811 (0xee3) - ASRC1_RATE2 */

 R3824 (0xef0) - ISRC 1 CTRL 1 */

 R3825 (0xef1) - ISRC 1 CTRL 2 */

 R3826 (0xef2) - ISRC 1 CTRL 3 */

 R3827 (0xef3) - ISRC 2 CTRL 1 */

 R3828 (0xef4) - ISRC 2 CTRL 2 */

 R3829 (0xef5) - ISRC 2 CTRL 3 */

 R3830 (0xef6) - ISRC 3 CTRL 1 */

 R3831 (0xef7) - ISRC 3 CTRL 2 */

 R3832 (0xef8) - ISRC 3 CTRL 3 */

 R3833 (0xef9) - ISRC 4 CTRL 1 */

 R3834 (0xefa) - ISRC 4 CTRL 2 */

 R3835 (0xefb) - ISRC 4 CTRL 3 */

 R3841 (0xf01) - ANC_SRC */

 R3842 (0xf02) - DSP Status */

 R3848 (0xf08) - ANC Coefficient */

 R3849 (0xf09) - ANC Coefficient */

 R3850 (0xf0a) - ANC Coefficient */

 R3851 (0xf0b) - ANC Coefficient */

 R3852 (0xf0c) - ANC Coefficient */

 R3853 (0xf0d) - ANC Coefficient */

 R3854 (0xf0e) - ANC Coefficient */

 R3855 (0xf0f) - ANC Coefficient */

 R3856 (0xf10) - ANC Coefficient */

 R3857 (0xf11) - ANC Coefficient */

 R3858 (0xf12) - ANC Coefficient */

 R3861 (0xf15) - FCL Filter Control */

 R3863 (0xf17) - FCL ADC Reformatter Control */

 R3864 (0xf18) - ANC Coefficient */

 R3865 (0xf19) - ANC Coefficient */

 R3866 (0xf1a) - ANC Coefficient */

 R3867 (0xf1b) - ANC Coefficient */

 R3868 (0xf1c) - ANC Coefficient */

 R3869 (0xf1d) - ANC Coefficient */

 R3870 (0xf1e) - ANC Coefficient */

 R3871 (0xf1f) - ANC Coefficient */

 R3872 (0xf20) - ANC Coefficient */

 R3873 (0xf21) - ANC Coefficient */

 R3874 (0xf22) - ANC Coefficient */

 R3875 (0xf23) - ANC Coefficient */

 R3876 (0xf24) - ANC Coefficient */

 R3877 (0xf25) - ANC Coefficient */

 R3878 (0xf26) - ANC Coefficient */

 R3879 (0xf27) - ANC Coefficient */

 R3880 (0xf28) - ANC Coefficient */

 R3881 (0xf29) - ANC Coefficient */

 R3882 (0xf2a) - ANC Coefficient */

 R3883 (0xf2b) - ANC Coefficient */

 R3884 (0xf2c) - ANC Coefficient */

 R3885 (0xf2d) - ANC Coefficient */

 R3886 (0xf2e) - ANC Coefficient */

 R3887 (0xf2f) - ANC Coefficient */

 R3888 (0xf30) - ANC Coefficient */

 R3889 (0xf31) - ANC Coefficient */

 R3890 (0xf32) - ANC Coefficient */

 R3891 (0xf33) - ANC Coefficient */

 R3892 (0xf34) - ANC Coefficient */

 R3893 (0xf35) - ANC Coefficient */

 R3894 (0xf36) - ANC Coefficient */

 R3895 (0xf37) - ANC Coefficient */

 R3896 (0xf38) - ANC Coefficient */

 R3897 (0xf39) - ANC Coefficient */

 R3898 (0xf3a) - ANC Coefficient */

 R3899 (0xf3b) - ANC Coefficient */

 R3900 (0xf3c) - ANC Coefficient */

 R3901 (0xf3d) - ANC Coefficient */

 R3902 (0xf3e) - ANC Coefficient */

 R3903 (0xf3f) - ANC Coefficient */

 R3904 (0xf40) - ANC Coefficient */

 R3905 (0xf41) - ANC Coefficient */

 R3906 (0xf42) - ANC Coefficient */

 R3907 (0xf43) - ANC Coefficient */

 R3908 (0xf44) - ANC Coefficient */

 R3909 (0xf45) - ANC Coefficient */

 R3910 (0xf46) - ANC Coefficient */

 R3911 (0xf47) - ANC Coefficient */

 R3912 (0xf48) - ANC Coefficient */

 R3913 (0xf49) - ANC Coefficient */

 R3914 (0xf4a) - ANC Coefficient */

 R3915 (0xf4b) - ANC Coefficient */

 R3916 (0xf4c) - ANC Coefficient */

 R3917 (0xf4d) - ANC Coefficient */

 R3918 (0xf4e) - ANC Coefficient */

 R3919 (0xf4f) - ANC Coefficient */

 R3920 (0xf50) - ANC Coefficient */

 R3921 (0xf51) - ANC Coefficient */

 R3922 (0xf52) - ANC Coefficient */

 R3923 (0xf53) - ANC Coefficient */

 R3924 (0xf54) - ANC Coefficient */

 R3925 (0xf55) - ANC Coefficient */

 R3926 (0xf56) - ANC Coefficient */

 R3927 (0xf57) - ANC Coefficient */

 R3928 (0xf58) - ANC Coefficient */

 R3929 (0xf59) - ANC Coefficient */

 R3930 (0xf5a) - ANC Coefficient */

 R3931 (0xf5b) - ANC Coefficient */

 R3932 (0xf5c) - ANC Coefficient */

 R3933 (0xf5d) - ANC Coefficient */

 R3934 (0xf5e) - ANC Coefficient */

 R3935 (0xf5f) - ANC Coefficient */

 R3936 (0xf60) - ANC Coefficient */

 R3937 (0xf61) - ANC Coefficient */

 R3938 (0xf62) - ANC Coefficient */

 R3939 (0xf63) - ANC Coefficient */

 R3940 (0xf64) - ANC Coefficient */

 R3941 (0xf65) - ANC Coefficient */

 R3942 (0xf66) - ANC Coefficient */

 R3943 (0xf67) - ANC Coefficient */

 R3944 (0xf68) - ANC Coefficient */

 R3945 (0xf69) - ANC Coefficient */

 R3953 (0xf71) - FCR Filter Control */

 R3955 (0xf73) - FCR ADC Reformatter Control */

 R3956 (0xf74) - ANC Coefficient */

 R3957 (0xf75) - ANC Coefficient */

 R3958 (0xf76) - ANC Coefficient */

 R3959 (0xf77) - ANC Coefficient */

 R3960 (0xf78) - ANC Coefficient */

 R3961 (0xf79) - ANC Coefficient */

 R3962 (0xf7a) - ANC Coefficient */

 R3963 (0xf7b) - ANC Coefficient */

 R3964 (0xf7c) - ANC Coefficient */

 R3965 (0xf7d) - ANC Coefficient */

 R3966 (0xf7e) - ANC Coefficient */

 R3967 (0xf7f) - ANC Coefficient */

 R3968 (0xf80) - ANC Coefficient */

 R3969 (0xf81) - ANC Coefficient */

 R3970 (0xf82) - ANC Coefficient */

 R3971 (0xf83) - ANC Coefficient */

 R3972 (0xf84) - ANC Coefficient */

 R3973 (0xf85) - ANC Coefficient */

 R3974 (0xf86) - ANC Coefficient */

 R3975 (0xf87) - ANC Coefficient */

 R3976 (0xf88) - ANC Coefficient */

 R3977 (0xf89) - ANC Coefficient */

 R3978 (0xf8a) - ANC Coefficient */

 R3979 (0xf8b) - ANC Coefficient */

 R3980 (0xf8c) - ANC Coefficient */

 R3981 (0xf8d) - ANC Coefficient */

 R3982 (0xf8e) - ANC Coefficient */

 R3983 (0xf8f) - ANC Coefficient */

 R3984 (0xf90) - ANC Coefficient */

 R3985 (0xf91) - ANC Coefficient */

 R3986 (0xf92) - ANC Coefficient */

 R3987 (0xf93) - ANC Coefficient */

 R3988 (0xf94) - ANC Coefficient */

 R3989 (0xf95) - ANC Coefficient */

 R3990 (0xf96) - ANC Coefficient */

 R3991 (0xf97) - ANC Coefficient */

 R3992 (0xf98) - ANC Coefficient */

 R3993 (0xf99) - ANC Coefficient */

 R3994 (0xf9a) - ANC Coefficient */

 R3995 (0xf9b) - ANC Coefficient */

 R3996 (0xf9c) - ANC Coefficient */

 R3997 (0xf9d) - ANC Coefficient */

 R3998 (0xf9e) - ANC Coefficient */

 R3999 (0xf9f) - ANC Coefficient */

 R4000 (0xfa0) - ANC Coefficient */

 R4001 (0xfa1) - ANC Coefficient */

 R4002 (0xfa2) - ANC Coefficient */

 R4003 (0xfa3) - ANC Coefficient */

 R4004 (0xfa4) - ANC Coefficient */

 R4005 (0xfa5) - ANC Coefficient */

 R4006 (0xfa6) - ANC Coefficient */

 R4007 (0xfa7) - ANC Coefficient */

 R4008 (0xfa8) - ANC Coefficient */

 R4009 (0xfa9) - ANC Coefficient */

 R4010 (0xfaa) - ANC Coefficient */

 R4011 (0xfab) - ANC Coefficient */

 R4012 (0xfac) - ANC Coefficient */

 R4013 (0xfad) - ANC Coefficient */

 R4014 (0xfae) - ANC Coefficient */

 R4015 (0xfaf) - ANC Coefficient */

 R4016 (0xfb0) - ANC Coefficient */

 R4017 (0xfb1) - ANC Coefficient */

 R4018 (0xfb2) - ANC Coefficient */

 R4019 (0xfb3) - ANC Coefficient */

 R4020 (0xfb4) - ANC Coefficient */

 R4021 (0xfb5) - ANC Coefficient */

 R4022 (0xfb6) - ANC Coefficient */

 R4023 (0xfb7) - ANC Coefficient */

 R4024 (0xfb8) - ANC Coefficient */

 R4025 (0xfb9) - ANC Coefficient */

 R4026 (0xfba) - ANC Coefficient */

 R4027 (0xfbb) - ANC Coefficient */

 R4028 (0xfbc) - ANC Coefficient */

 R4029 (0xfbd) - ANC Coefficient */

 R4030 (0xfbe) - ANC Coefficient */

 R4031 (0xfbf) - ANC Coefficient */

 R4032 (0xfc0) - ANC Coefficient */

 R4033 (0xfc1) - ANC Coefficient */

 R4034 (0xfc2) - ANC Coefficient */

 R4035 (0xfc3) - ANC Coefficient */

 R4036 (0xfc4) - ANC Coefficient */

 R4037 (0xfc5) - ANC Coefficient */

 R5248 (0x1480) - DFC1_CTRL */

 R5250 (0x1482) - DFC1_RX */

 R5252 (0x1486) - DFC1_TX */

 R5254 (0x1486) - DFC2_CTRL */

 R5256 (0x1488) - DFC2_RX */

 R5258 (0x148a) - DFC2_TX */

 R5260 (0x148c) - DFC3_CTRL */

 R5262 (0x148e) - DFC3_RX */

 R5264 (0x1490) - DFC3_TX */

 R5266 (0x1492) - DFC4_CTRL */

 R5268 (0x1494) - DFC4_RX */

 R5270 (0x1496) - DFC4_TX */

 R5272 (0x1498) - DFC5_CTRL */

 R5274 (0x149a) - DFC5_RX */

 R5276 (0x149c) - DFC5_TX */

 R5278 (0x149e) - DFC6_CTRL */

 R5280 (0x14a0) - DFC6_RX */

 R5282 (0x14a2) - DFC6_TX */

 R5284 (0x14a4) - DFC7_CTRL */

 R5286 (0x14a6) - DFC7_RX */

 R5288 (0x14a8) - DFC7_TX */

 R5290 (0x14aa) - DFC8_CTRL */

 R5292 (0x14ac) - DFC8_RX */

 R5294 (0x14ae) - DFC8_TX */

 R5888 (0x1700) - GPIO1 Control 1 */

 R5889 (0x1701) - GPIO1 Control 2 */

 R5890 (0x1702) - GPIO2 Control 1 */

 R5891 (0x1702) - GPIO2 Control 2 */

 R5892 (0x1704) - GPIO3 Control 1 */

 R5893 (0x1705) - GPIO3 Control 2 */

 R5894 (0x1706) - GPIO4 Control 1 */

 R5895 (0x1707) - GPIO4 Control 2 */

 R5896 (0x1708) - GPIO5 Control 1 */

 R5897 (0x1709) - GPIO5 Control 2 */

 R5898 (0x170a) - GPIO6 Control 1 */

 R5899 (0x170b) - GPIO6 Control 2 */

 R5900 (0x170c) - GPIO7 Control 1 */

 R5901 (0x170d) - GPIO7 Control 2 */

 R5902 (0x170e) - GPIO8 Control 1 */

 R5903 (0x170f) - GPIO8 Control 2 */

 R5904 (0x1710) - GPIO9 Control 1 */

 R5905 (0x1711) - GPIO9 Control 2 */

 R5906 (0x1712) - GPIO10 Control 1 */

 R5907 (0x1713) - GPIO10 Control 2 */

 R5908 (0x1714) - GPIO11 Control 1 */

 R5909 (0x1715) - GPIO11 Control 2 */

 R5910 (0x1716) - GPIO12 Control 1 */

 R5911 (0x1717) - GPIO12 Control 2 */

 R5912 (0x1718) - GPIO13 Control 1 */

 R5913 (0x1719) - GPIO13 Control 2 */

 R5914 (0x171a) - GPIO14 Control 1 */

 R5915 (0x171b) - GPIO14 Control 2 */

 R5916 (0x171c) - GPIO15 Control 1 */

 R5917 (0x171d) - GPIO15 Control 2 */

 R5918 (0x171e) - GPIO16 Control 1 */

 R5919 (0x171f) - GPIO16 Control 2 */

 R5920 (0x1720) - GPIO17 Control 1 */

 R5921 (0x1721) - GPIO17 Control 2 */

 R5922 (0x1722) - GPIO18 Control 1 */

 R5923 (0x1723) - GPIO18 Control 2 */

 R5924 (0x1724) - GPIO19 Control 1 */

 R5925 (0x1725) - GPIO19 Control 2 */

 R5926 (0x1726) - GPIO20 Control 1 */

 R5927 (0x1727) - GPIO20 Control 2 */

 R5928 (0x1728) - GPIO21 Control 1 */

 R5929 (0x1729) - GPIO21 Control 2 */

 R5930 (0x172a) - GPIO22 Control 1 */

 R5931 (0x172b) - GPIO22 Control 2 */

 R5932 (0x172c) - GPIO23 Control 1 */

 R5933 (0x172d) - GPIO23 Control 2 */

 R5934 (0x172e) - GPIO24 Control 1 */

 R5935 (0x172f) - GPIO24 Control 2 */

 R5936 (0x1730) - GPIO25 Control 1 */

 R5937 (0x1731) - GPIO25 Control 2 */

 R5938 (0x1732) - GPIO26 Control 1 */

 R5939 (0x1733) - GPIO26 Control 2 */

 R5940 (0x1734) - GPIO27 Control 1 */

 R5941 (0x1735) - GPIO27 Control 2 */

 R5942 (0x1736) - GPIO28 Control 1 */

 R5943 (0x1737) - GPIO28 Control 2 */

 R5944 (0x1738) - GPIO29 Control 1 */

 R5945 (0x1739) - GPIO29 Control 2 */

 R5946 (0x173a) - GPIO30 Control 1 */

 R5947 (0x173b) - GPIO30 Control 2 */

 R5948 (0x173c) - GPIO31 Control 1 */

 R5949 (0x173d) - GPIO31 Control 2 */

 R5950 (0x173e) - GPIO32 Control 1 */

 R5951 (0x173f) - GPIO32 Control 2 */

 R5952 (0x1740) - GPIO33 Control 1 */

 R5953 (0x1741) - GPIO33 Control 2 */

 R5954 (0x1742) - GPIO34 Control 1 */

 R5955 (0x1743) - GPIO34 Control 2 */

 R5956 (0x1744) - GPIO35 Control 1 */

 R5957 (0x1745) - GPIO35 Control 2 */

 R5958 (0x1746) - GPIO36 Control 1 */

 R5959 (0x1747) - GPIO36 Control 2 */

 R5960 (0x1748) - GPIO37 Control 1 */

 R5961 (0x1749) - GPIO37 Control 2 */

 R5962 (0x174a) - GPIO38 Control 1 */

 R5963 (0x174b) - GPIO38 Control 2 */

 R6208 (0x1840) - IRQ1 Mask 1 */

 R6209 (0x1841) - IRQ1 Mask 2 */

 R6210 (0x1842) - IRQ1 Mask 3 */

 R6211 (0x1843) - IRQ1 Mask 4 */

 R6212 (0x1844) - IRQ1 Mask 5 */

 R6213 (0x1845) - IRQ1 Mask 6 */

 R6214 (0x1846) - IRQ1 Mask 7 */

 R6215 (0x1847) - IRQ1 Mask 8 */

 R6216 (0x1848) - IRQ1 Mask 9 */

 R6217 (0x1849) - IRQ1 Mask 10 */

 R6218 (0x184a) - IRQ1 Mask 11 */

 R6219 (0x184b) - IRQ1 Mask 12 */

 R6220 (0x184c) - IRQ1 Mask 13 */

 R6221 (0x184d) - IRQ1 Mask 14 */

 R6222 (0x184e) - IRQ1 Mask 15 */

 R6223 (0x184f) - IRQ1 Mask 16 */

 R6224 (0x1850) - IRQ1 Mask 17 */

 R6225 (0x1851) - IRQ1 Mask 18 */

 R6226 (0x1852) - IRQ1 Mask 19 */

 R6227 (0x1853) - IRQ1 Mask 20 */

 R6228 (0x1854) - IRQ1 Mask 21 */

 R6229 (0x1855) - IRQ1 Mask 22 */

 R6230 (0x1856) - IRQ1 Mask 23 */

 R6231 (0x1857) - IRQ1 Mask 24 */

 R6232 (0x1858) - IRQ1 Mask 25 */

 R6233 (0x1859) - IRQ1 Mask 26 */

 R6234 (0x185a) - IRQ1 Mask 27 */

 R6235 (0x185b) - IRQ1 Mask 28 */

 R6236 (0x185c) - IRQ1 Mask 29 */

 R6237 (0x185d) - IRQ1 Mask 30 */

 R6238 (0x185e) - IRQ1 Mask 31 */

 R6239 (0x185f) - IRQ1 Mask 32 */

 R6240 (0x1860) - IRQ1 Mask 33 */

 R6662 (0x1a06) - Interrupt Debounce 7 */

 R6784 (0x1a80) - IRQ1 CTRL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I2C driver for Maxim MAX8925

 *

 * Copyright (C) 2009 Marvell International Ltd.

 *	Haojian Zhuang <haojian.zhuang@marvell.com>

 parse DT to get platform data */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Maxim MAX77620 MFD Driver

 *

 * Copyright (C) 2016 NVIDIA CORPORATION. All rights reserved.

 *

 * Author:

 *	Laxman Dewangan <ldewangan@nvidia.com>

 *	Chaitanya Bandi <bandik@nvidia.com>

 *	Mallikarjun Kasoju <mkasoju@nvidia.com>

/****************** Teminology used in driver ********************

 * Here are some terminology used from datasheet for quick reference:

 * Flexible Power Sequence (FPS):

 * The Flexible Power Sequencer (FPS) allows each regulator to power up under

 * hardware or software control. Additionally, each regulator can power on

 * independently or among a group of other regulators with an adjustable

 * power-up and power-down delays (sequencing). GPIO1, GPIO2, and GPIO3 can

 * be programmed to be part of a sequence allowing external regulators to be

 * sequenced along with internal regulators. 32KHz clock can be programmed to

 * be part of a sequence.

 * There is 3 FPS confguration registers and all resources are configured to

 * any of these FPS or no FPS.

/*

 * MAX77620 and MAX20024 has the following steps of the interrupt handling

 * for TOP interrupts:

 * 1. When interrupt occurs from PMIC, mask the PMIC interrupt by setting GLBLM.

 * 2. Read IRQTOP and service the interrupt.

 * 3. Once all interrupts has been checked and serviced, the interrupt service

 *    routine un-masks the hardware interrupt line by clearing GLBLM.

/* max77620_get_fps_period_reg_value:  Get FPS bit field value from

 *				       requested periods.

 * MAX77620 supports the FPS period of 40, 80, 160, 320, 540, 1280, 2560

 * and 5120 microseconds. MAX20024 supports the FPS period of 20, 40, 80,

 * 160, 320, 540, 1280 and 2560 microseconds.

 * The FPS register has 3 bits field to set the FPS period as

 * bits		max77620		max20024

 * 000		40			20

 * 001		80			40

 * :::

/* max77620_config_fps: Configure FPS configuration registers

 *			based on platform specific information.

 Enable wake on EN0 pin */

 For MAX20024, SLPEN will be POR reset if CLRSE is b11 */

 CID4 is OTP Version  and CID5 is ES version */

	/*

	 * For MAX20024: No need to configure SLPEN on suspend as

	 * it will be configured on Init.

 Disable WK_EN0 */

	/*

	 * For MAX20024: No need to configure WKEN0 on resume as

	 * it is configured on Init.

 Enable WK_EN0 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for the Intel Broxton PMC

 *

 * (C) Copyright 2014 - 2020 Intel Corporation

 *

 * This driver is based on Intel SCU IPC driver (intel_scu_ipc.c) by

 * Sreedhara DS <sreedhara.ds@intel.com>

 *

 * The PMC (Power Management Controller) running on the ARC processor

 * communicates with another entity running in the IA (Intel Architecture)

 * core through an IPC (Intel Processor Communications) mechanism which in

 * turn sends messages between the IA and the PMC.

 Residency with clock rate at 19.2MHz to usecs */

 Resources exported from IFWI */

/*

 * BIOS does not create an ACPI device for each PMC function, but

 * exports multiple resources from one ACPI device (IPC) for multiple

 * functions. This driver is responsible for creating a child device and

 * to export resources for those functions.

 Commands */

/**

 * intel_pmc_gcr_read64() - Read a 64-bit PMC GCR register

 * @pmc: PMC device pointer

 * @offset: offset of GCR register from GCR address base

 * @data: data pointer for storing the register output

 *

 * Reads the 64-bit PMC GCR register at given offset.

 *

 * Return: Negative value on error or 0 on success.

/**

 * intel_pmc_gcr_update() - Update PMC GCR register bits

 * @pmc: PMC device pointer

 * @offset: offset of GCR register from GCR address base

 * @mask: bit mask for update operation

 * @val: update value

 *

 * Updates the bits of given GCR register as specified by

 * @mask and @val.

 *

 * Return: Negative value on error or 0 on success.

 Check whether the bit update is successful */

/**

 * intel_pmc_s0ix_counter_read() - Read S0ix residency

 * @pmc: PMC device pointer

 * @data: Out param that contains current S0ix residency count.

 *

 * Writes to @data how many usecs the system has been in low-power S0ix

 * state.

 *

 * Return: An error code or 0 on success.

/**

 * simplecmd_store() - Send a simple IPC command

 * @dev: Device under the attribute is

 * @attr: Attribute in question

 * @buf: Buffer holding data to be stored to the attribute

 * @count: Number of bytes in @buf

 *

 * Expects a string with two integers separated with space. These two

 * values hold command and subcommand that is send to PMC.

 *

 * Return: Number number of bytes written (@count) or negative errno in

 *	   case of error.

/**

 * northpeak_store() - Enable or disable Northpeak

 * @dev: Device under the attribute is

 * @attr: Attribute in question

 * @buf: Buffer holding data to be stored to the attribute

 * @count: Number of bytes in @buf

 *

 * Expects an unsigned integer. Non-zero enables Northpeak and zero

 * disables it.

 *

 * Return: Number number of bytes written (@count) or negative errno in

 *	   case of error.

 Northpeak is enabled if subcmd == 1 and disabled if it is 0 */

 IPC registers */

 GCR registers */

 Only register iTCO watchdog if there is no WDAT ACPI table */

 BIOS data register */

 BIOS interface register */

 ISP data register, optional */

 ISP interface register, optional */

 GTD data register, optional */

 GTD interface register, optional */

 Telemetry SSRAM is optional */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device access for Dollar Cove TI PMIC

 *

 * Copyright (c) 2014, Intel Corporation.

 *   Author: Ramakrishna Pallala <ramakrishna.pallala@intel.com>

 *

 * Cleanup and forward-ported

 *   Copyright (c) 2017 Takashi Iwai <tiwai@suse.de>

 Level 1 IRQs */

 power button */

 thermal */

 ADC */

 No IRQ 3 */

 battery */

 power source */

 No IRQ 6 */

 battery */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8994-core.c  --  Device access for Wolfson WM8994

 *

 * Copyright 2009 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

/*

 * Supplies for the main bulk of CODEC; the LDO supplies are ignored

 * and should be handled via the standard regulator API supply

 * management.

	/* Don't actually go through with the suspend if the CODEC is

	/* Disable LDO pulldowns while the device is suspended if we

	/* Explicitly put the device into reset in case regulators

	 * don't get disabled in order to ensure consistent restart.

	/* Restore GPIO registers to prevent problems with mismatched

	 * pin configurations.

 In case one of the GPIOs is used as a wake input. */

 We may have lied to the PM core about suspending */

 Disable LDO pulldowns while the device is active */

/*

 * Instantiate the generic non-control parts of the device.

 Add the on-chip regulators first for bootstrapping */

	/*

	 * Can't use devres helper here as some of the supplies are provided by

	 * wm8994->dev's children (regulators) and those regulators are

	 * unregistered by the devres core before the supplies are freed.

 Revision C did not change the relevant layer */

	/* Explicitly put the device into reset in case regulators

	 * don't get disabled in order to ensure we know the device

	 * state.

 GPIO configuration is only applied if it's non-zero */

 Disable unneeded pulls */

	/* In some system designs where the regulators are not in use,

	 * we can achieve a small reduction in leakage currents by

	 * floating LDO outputs.  This bit makes no difference if the

	 * LDOs are enabled, it only affects cases where the LDOs were

	 * in operation and are then disabled.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * cros_ec_dev - expose the Chrome OS Embedded Controller to user-space

 *

 * Copyright (C) 2014 Google, Inc.

/**

 * struct cros_feature_to_name - CrOS feature id to name/short description.

 * @id: The feature identifier.

 * @name: Device name associated with the feature id.

 * @desc: Short name that will be displayed.

/**

 * struct cros_feature_to_cells - CrOS feature id to mfd cells association.

 * @id: The feature identifier.

 * @mfd_cells: Pointer to the array of mfd cells that needs to be added.

 * @num_cells: Number of mfd cells into the array.

 Not cached yet */

 Not cached yet */

		/*

		 * Check whether this is actually a dedicated MCU rather

		 * than an standard EC.

			/*

			 * Help userspace differentiating ECs from other MCU,

			 * regardless of the probing order.

	/*

	 * Add the class device

 check whether this EC is a sensor hub. */

	/*

	 * The following subdevices can be detected by sending the

	 * EC_FEATURE_GET_CMD Embedded Controller device.

	/*

	 * Lightbar is a special case. Newer devices support autodetection,

	 * but older ones do not.

	/*

	 * The PD notifier driver cell is separate since it only needs to be

	 * explicitly added on platforms that don't have the PD notifier ACPI

	 * device entry defined.

	/*

	 * The following subdevices cannot be detected by sending the

	 * EC_FEATURE_GET_CMD to the Embedded Controller device.

 Check whether this EC instance has a VBC NVRAM */

 sentinel */ }

 Register the driver */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/drivers/mfd/mcp-sa11x0.c

 *

 *  Copyright (C) 2001-2005 Russell King

 *

 *  SA11x0 MCP (Multimedia Communications Port) driver.

 *

 *  MCP read/write timeouts from Jordi Colomer, rehacked by rmk.

 Register offsets */

/*

 * Write data to the device.  The bit should be set after 3 subframe

 * times (each frame is 64 clocks).  We wait a maximum of 6 subframes.

 * We really should try doing something more productive while we

 * wait.

/*

 * Read data from the device.  The bit should be set after 3 subframe

 * times (each frame is 64 clocks).  We wait a maximum of 6 subframes.

 * We really should try doing something more productive while we

 * wait.

/*

 * Our methods.

	/*

	 * Initialise device.  Note that we initially

	 * set the sampling rate to minimum.

	/*

	 * Calculate the read/write timeout (us) from the bit clock

	 * rate.  This is the period for 3 64-bit frames.  Always

	 * round this time up.

/*

 * This needs re-working

 SPDX-License-Identifier: GPL-2.0-only

/*

 * TI LP8788 MFD - interrupt handler

 *

 * Copyright 2012 Texas Instruments

 *

 * Author: Milo(Woogyom) Kim <milo.kim@ti.com>

 register address */

/*

 * struct lp8788_irq_data

 * @lp               : used for accessing to lp8788 registers

 * @irq_lock         : mutex for enabling/disabling the interrupt

 * @domain           : IRQ domain for handling nested interrupt

 * @enabled          : status of enabled interrupt

 reporting only if the irq is enabled */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AS3711 PMIC MFC driver

 *

 * Copyright (C) 2012 Renesas Electronics Corporation

 * Author: Guennadi Liakhovetski, <g.liakhovetski@gmx.de>

/*

 * Ok to have it static: it is only used during probing and multiple I2C devices

 * cannot be probed simultaneously. Just make sure to avoid stale data.

	/*

	 * We can reuse as3711_subdevs[],

	 * it will be copied in mfd_add_devices()

 Initialise early */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Janz CMOD-IO MODULbus Carrier Board PCI Driver

 *

 * Copyright (c) 2010 Ira W. Snyder <iws@ovro.caltech.edu>

 *

 * Lots of inspiration and code was copied from drivers/mfd/sm501.c

 Size of each MODULbus module in PCI BAR4 */

 Maximum number of MODULbus modules on a CMOD-IO carrier board */

 Module Parameters */

 Unique Device Id */

 Parent PCI device */

 PLX control registers */

 hex switch position */

 mfd-core API */

/*

 * Subdevices using the mfd-core API

 Setup the subdevice ID -- must be unique */

 Add platform data */

 MODULbus registers -- PCI BAR3 is big-endian MODULbus access */

 PLX Control Registers -- PCI BAR4 is interrupt and other registers */

	/*

	 * IRQ

	 *

	 * The start and end fields are used as an offset to the irq_base

	 * parameter passed into the mfd_add_devices() function call. All

	 * devices share the same IRQ.

 Probe each submodule using kernel parameters */

 print an error message if no modules were probed */

/*

 * SYSFS Attributes

/*

 * PCI Driver

 Hardware Initialization */

 Onboard configuration registers */

 Read the hex switch on the carrier board */

 Add the MODULbus number (hex switch value) to the device's sysfs */

	/*

	 * Disable all interrupt lines, each submodule will enable its

	 * own interrupt line if needed

 Register drivers for all submodules */

 The list of devices that this module will support */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST Microelectronics MFD: stmpe's i2c client specific driver

 *

 * Copyright (C) ST-Ericsson SA 2010

 * Copyright (C) ST Microelectronics SA 2011

 *

 * Author: Rabin Vincent <rabin.vincent@stericsson.com> for ST-Ericsson

 * Author: Viresh Kumar <vireshk@kernel.org> for ST Microelectronics

		/*

		 * This happens when the I2C ID matches the node name

		 * but no real compatible string has been given.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * drivers/mfd/si476x-prop.c -- Subroutines to access

 * properties of si476x chips

 *

 * Copyright (C) 2012 Innovative Converged Devices(ICD)

 * Copyright (C) 2013 Andrey Smirnov

 *

 * Author: Andrey Smirnov <andrew.smirnov@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Nano River Technologies viperboard driver

 *

 *  This is the core driver for the viperboard. There are cell drivers

 *  available for I2C, ADC and both GPIOs. SPI is not yet supported.

 *  The drivers do not support all features the board exposes. See user

 *  manual of the viperboard.

 *

 *  (C) 2012 by Lemonage GmbH

 *  Author: Lars Poeschel <poeschel@lemonage.de>

 *  All rights reserved.

 Nano River Technologies */

 Terminating entry */

 allocate memory for our device state and initialize it */

 save our data pointer in this interface device */

 get version information, major first, minor then */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2021, The Linux Foundation. All rights reserved.

	/*

	 * Set TEMP_ALARM peripheral's TYPE so that the regmap-irq framework

	 * reads this as the default value instead of zero, the HW default.

	 * This is required to enable the writing of TYPE registers in

	 * regmap_irq_sync_unlock().

 Do the same for GPIO1 and GPIO2 peripherals */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Wolfson WM97xx -- Core device

 *

 * Copyright (C) 2017 Robert Jarzmik

 *

 * Features:

 *  - an AC97 audio codec

 *  - a touchscreen driver

 *  - a GPIO block

 Speaker Output Volume */

 Headphone Output Volume */

 Out3/OUT4 Volume */

 Mono Volume */

 LINEIN Volume */

 DAC PGA Volume */

 MIC PGA Volume */

 MIC Routing Control */

 Record PGA Volume */

 Record Routing */

 PCBEEP Volume */

 VxDAC Volume */

 AUXDAC Volume */

 Output PGA Mux */

 DAC 3D control */

 DAC Tone Control*/

 MIC Input Select & Bias */

 Output Volume Mapping & Jack */

 Powerdown Ctrl/Stat*/

 Extended Audio ID */

 Extended Audio Start/Ctrl */

 Audio DACs Sample Rate */

 AUXDAC Sample Rate */

 Audio ADCs Sample Rate */

 PCM codec control */

 SPDIF control */

 Powerdown 1 */

 Powerdown 2 */

 General Purpose */

 Fast Power-Up Control */

 MCLK/PLL Control */

 MCLK/PLL Control */

 GPIO Pin Configuration */

 GPIO Pin Polarity / Type */

 GPIO Pin Sticky */

 GPIO Pin Wake-Up */

 GPIO Pin Status */

 GPIO Pin Sharing */

 GPIO PullUp/PullDown */

 Additional Functions 1 */

 Additional Functions 2 */

 ALC Control */

 ALC / Noise Gate Control */

 AUXDAC input control */

 Digitiser Reg 1 */

 Digitiser Reg 2 */

 Digitiser Reg 3 */

 Digitiser Read Back */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sky81452.c	SKY81452 MFD driver

 *

 * Copyright 2014 Skyworks Solutions Inc.

 * Author : Gyungoh Yoo <jack.yoo@skyworksinc.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Freescale MXS Low Resolution Analog-to-Digital Converter driver

 *

 * Copyright (c) 2012 DENX Software Engineering, GmbH.

 * Copyright (c) 2017 Ksenija Stanojevic <ksenija.stanojevic@gmail.com>

 *

 * Authors:

 *  Marek Vasut <marex@denx.de>

 *  Ksenija Stanojevic <ksenija.stanojevic@gmail.com>

 sentinel */ }

 to an error message for i.MX23 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Device access for Crystal Cove PMIC

 *

 * Copyright (C) 2013, 2014 Intel Corporation. All rights reserved.

 *

 * Author: Yang, Bin <bin.yang@intel.com>

 * Author: Zhu, Lejun <lejun.zhu@linux.intel.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DA9052 interrupt support

 *

 * Author: Fabio Estevam <fabio.estevam@freescale.com>

 * Based on arizona-irq.c, which is:

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Arizona interrupt support

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

	/*

	 * For pretty much all potential sources a register cache sync

	 * won't help, we've just got a software bug somewhere.

			/*

			 * Check the AOD status register to determine whether

			 * the nested IRQ handler should be called.

		/*

		 * Check if one of the main interrupts is asserted and only

		 * check that domain if it is.

		/*

		 * Poll the IRQ pin status to see if we're really done

		 * if the interrupt controller can't do it for us.

 Disable all wake sources by default */

 Read the flags from the interrupt controller if not specified */

 Device default */

 Allocate a virtual IRQ domain to distribute to the regmap domains */

 Used to emulate edge trigger and to work around broken pinmux */

 Make sure the boot done IRQ is unmasked for resumes */

 Handle control interface errors in the core */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * omap-usb-tll.c - The USB TLL driver for OMAP EHCI & OHCI

 *

 * Copyright (C) 2012-2013 Texas Instruments Incorporated - https://www.ti.com

 * Author: Keshava Munegowda <keshava_mgowda@ti.com>

 * Author: Roger Quadros <rogerq@ti.com>

 TLL Register Set */

 Values of USBTLL_REVISION - Note: these are not given in the TRM */

 OMAP3 */

 OMAP 3630 */

 OMAP4 */

 OMAP5 */

 only PHY and UNUSED modes don't need TLL */

 num. of channels */

 must be the last member */

-------------------------------------------------------------------------*/

 serialize access to tll_dev */

-------------------------------------------------------------------------*/

-------------------------------------------------------------------------*/

/*

 * convert the port-mode enum to a value we can use in the FSLSMODE

 * field of USBTLL_CHANNEL_CONF

/**

 * usbtll_omap_probe - initialize TI-based HCDs

 *

 * Allocates basic resources for this USB host controller.

 *

 * @pdev: Pointer to this device's platform device structure

 only after this can omap_tll_enable/disable work */

/**

 * usbtll_omap_remove - shutdown processing for UHH & TLL HCDs

 * @pdev: USB Host Controller being removed

 *

 * Reverses the effect of usbtll_omap_probe().

 Program Common TLL register */

 Enable channels now */

				/*

				 * Disable UTMI AutoIdle, BitStuffing

				 * and use SDR Mode. Enable ULPI AutoIdle.

				/*

				 * HSIC Mode requires UTMI port configurations

/*

 * init before usbhs core driver;

 * The usbtll driver should be initialized before

 * the usbhs core driver probe function is called.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux multi-function-device driver (MFD) for the integrated peripherals

 * of the VIA VX855 chipset

 *

 * Copyright (C) 2009 VIA Technologies, Inc.

 * Copyright (C) 2010 One Laptop per Child

 * Author: Harald Welte <HaraldWelte@viatech.com>

 * All rights reserved.

/* offset into pci config space indicating the 16bit register containing

 ACPI I/O Space registers */

 Processor Power Management */

 General Purpose Power Management */

		/* we must ignore resource conflicts, for reasons outlined in

	/* mask out the lowest seven bits, as they are always zero, but

	/* As the region identified here includes many non-GPIO things, we

	/* we always return -ENODEV here in order to enable other

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) STMicroelectronics 2018

 Author: Pascal Paillet <p.paillet@st.com>

 Initialize PMIC IRQ Chip & associated IRQ domains */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/drivers/mfd/mcp-core.c

 *

 *  Copyright (C) 2001 Russell King

 *

 *  Generic MCP (Multimedia Communications Port) layer.  All MCP locking

 *  is solely held within this file.

/**

 *	mcp_set_telecom_divisor - set the telecom divisor

 *	@mcp: MCP interface structure

 *	@div: SIB clock divisor

 *

 *	Set the telecom divisor on the MCP interface.  The resulting

 *	sample rate is SIBCLOCK/div.

/**

 *	mcp_set_audio_divisor - set the audio divisor

 *	@mcp: MCP interface structure

 *	@div: SIB clock divisor

 *

 *	Set the audio divisor on the MCP interface.

/**

 *	mcp_reg_write - write a device register

 *	@mcp: MCP interface structure

 *	@reg: 4-bit register index

 *	@val: 16-bit data value

 *

 *	Write a device register.  The MCP interface must be enabled

 *	to prevent this function hanging.

/**

 *	mcp_reg_read - read a device register

 *	@mcp: MCP interface structure

 *	@reg: 4-bit register index

 *

 *	Read a device register and return its value.  The MCP interface

 *	must be enabled to prevent this function hanging.

/**

 *	mcp_enable - enable the MCP interface

 *	@mcp: MCP interface to enable

 *

 *	Enable the MCP interface.  Each call to mcp_enable will need

 *	a corresponding call to mcp_disable to disable the interface.

/**

 *	mcp_disable - disable the MCP interface

 *	@mcp: MCP interface to disable

 *

 *	Disable the MCP interface.  The MCP interface will only be

 *	disabled once the number of calls to mcp_enable matches the

 *	number of calls to mcp_disable.

/*

 * Core functions for TI TPS65912x PMICs

 *

 * Copyright (C) 2015 Texas Instruments Incorporated - https://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether expressed or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License version 2 for more details.

 *

 * Based on the TPS65218 driver and the previous TPS65912 driver by

 * Margarita Olaya Cabrera <magi@slimlogic.co.uk>

 INT_STS IRQs */

 INT_STS2 IRQs */

 INT_STS3 IRQs */

 INT_STS4 IRQs */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel SoC PMIC MFD Driver

 *

 * Copyright (C) 2013, 2014 Intel Corporation. All rights reserved.

 *

 * Author: Yang, Bin <bin.yang@intel.com>

 * Author: Zhu, Lejun <lejun.zhu@linux.intel.com>

 Crystal Cove PMIC shares same ACPI ID between different platforms */

 PWM consumed by the Intel GFX */

	/*

	 * There are 2 different Crystal Cove PMICs a Bay Trail and Cherry

	 * Trail version, use _HRV to differentiate between the 2.

 Add lookup table for crc-pwm */

 remove crc-pwm lookup table */

 SPDX-License-Identifier: GPL-2.0



 Copyright (c) 2019 MediaTek Inc.

 Mask all interrupt sources */

 SPDX-License-Identifier: GPL-2.0-only

/* linux/drivers/mfd/sm501.c

 *

 * Copyright (C) 2006 Simtec Electronics

 *	Ben Dooks <ben@simtec.co.uk>

 *	Vincent Sanders <vince@simtec.co.uk>

 *

 * SM501 MFD driver

 to get back to parent. */

 address of control reg. */

 no gpio support, empty definition for sm501_devdata. */

/* sm501_dump_clk

 *

 * Print out the current clock configuration for the device

/* sm501_sync_regs

 *

 * ensure the

	/* during suspend/resume, we are currently not allowed to sleep,

	 * so change to using mdelay() instead of msleep() if we

/* sm501_misc_control

 *

 * alters the miscellaneous control parameters

/* sm501_modify_reg

 *

 * Modify a register in the SM501 which may be shared with other

 * drivers.

/* sm501_unit_power

 *

 * alters the power active gate to set specific units on or off

 get current power mode */

 clock value structure. */

/* sm501_calc_clock

 *

 * Calculates the nearest discrete clock frequency that

 * can be achieved with the specified input clock.

 *   the maximum divisor is 3 or 5

	/* try dividers 1 and 3 for CRT and for panel,

 try all 8 shift values.*/

 Calculate difference to requested clock */

 If it is less than the current, use it */

/* sm501_calc_pll

 *

 * Calculates the nearest discrete clock frequency that can be

 * achieved using the programmable PLL.

 *   the maximum divisor is 3 or 5

	/*

	 * The SM502 datasheet doesn't specify the min/max values for M and N.

	 * N = 1 at least doesn't work in practice.

 Return best clock. */

/* sm501_select_clock

 *

 * Calculates the nearest discrete clock frequency that can be

 * achieved using the 288MHz and 336MHz PLLs.

 *   the maximum divisor is 3 or 5

 Try 288MHz and 336MHz clocks. */

 Return best clock. */

/* sm501_set_clock

 *

 * set one of the four clock sources to the closest available frequency to

 *  the one specified

 the actual frequency achieved */

	/* find achivable discrete frequency and setup register value

	 * accordingly, V2XCLK, MCLK and M1XCLK are the same P2XCLK

		/* This clock is divided in half so to achieve the

		 * requested frequency the value must be multiplied by

 SM502 -> use the programmable PLL */

 bottom 3 bits are shift */

 /3 divider required */

 /5 divider required */

 select the programmable PLL */

 bottom 3 bits are shift */

 /3 divider required */

 /5 divider required */

 which mclk pll is source */

		/* This clock is divided in half so to achieve the

 bottom 3 bits are shift */

 /3 divider required */

 which mclk pll is source */

 These clocks are the same and not further divided */

 bottom 3 bits are shift */

 /3 divider required */

 which mclk pll is source */

 this is bad */

 find current mode */

/* sm501_find_clock

 *

 * finds the closest available frequency for a given clock

 the frequency achieveable by the 501 */

 SM502 -> use the programmable PLL */

 error */

/* sm501_device_release

 *

 * A release function for the platform devices we create to allow us to

 * free any items we allocated

/* sm501_create_subdev

 *

 * Create a skeleton platform device with resources for passing to a

 * sub-driver

/* sm501_register_device

 *

 * Register a platform device created with sm501_create_subdev()

/* sm501_create_subio

 *

 * Fill in an IO resource for a sub device

/* sm501_create_mem

 *

 * Fill in an MEM resource for a sub device

 adjust memory size */

/* sm501_create_irq

 *

 * Fill in an IRQ resource for a sub device

 check and modify if this pin is not set as gpio. */

 Register both our chips. */

 Create a gpiod lookup using gpiochip-local offsets */

	/* note, we can't use either of the pin numbers, as the i2c-gpio

	 * driver uses the platform.id field to generate the bus number

	 * to register with the i2c core; The i2c core doesn't have enough

	 * entries to deal with anything we currently use.

/* dbg_regs_show

 *

 * Debug attribute to attach to parent device to show core registers

/* sm501_init_reg

 *

 * Helper function for the init code to setup a register

 *

 * clear the bits which are set in r->mask, and then set

 * the bits set in r->set.

/* sm501_init_regs

 *

 * Setup core register values

/* Check the PLL sources for the M1CLK and M1XCLK

 *

 * If the M1CLK and M1XCLKs are not sourced from the same PLL, then

 * there is a risk (see errata AB-5) that the SM501 will cease proper

 * function. If this happens, then it is likely the SM501 will

 * hang the system.

/* sm501_init_dev

 *

 * Common init code for an SM501

 disable irqs */

 check to see if we have some device initialisation */

 always create a framebuffer */

 power management support */

 check to see if we are in the same state as when suspended */

		/* our suspend causes the controller state to change,

		 * either by something attempting setup, power loss,

 dump our state from resume */

 Initialisation data for PCI devices */

 24bit panel */

 SDRAM timing */

	/* Errata AB-3 says that 72MHz is the fastest available

 set a default set of platform data */

 set a hopefully unique id for our child platform devices */

	/* if the system is big-endian, we most probably have a

	 * translation in the IO layer making the PCI bus little endian

 check our resources */

 make our resources ready for sharing */

 end */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Driver for STMicroelectronics Multi-Function eXpander (STMFX) core

 *

 * Copyright (C) 2019 STMicroelectronics

 * Author(s): Amelie Delaunay <amelie.delaunay@st.com>.

	/*

	 * IDD and TS have priority in STMFX FW, so if IDD and TS are enabled,

	 * ALTGPIO function is disabled by STMFX FW. If IDD or TS is enabled,

	 * the number of aGPIO available decreases. To avoid GPIO management

	 * disturbance, abort IDD or TS function enable in this case.

 If TS is enabled, aGPIO[3:0] cannot be used */

 If IDD is enabled, aGPIO[7:4] cannot be used */

	/*

	 * There is no ACK for GPIO, MFX_REG_IRQ_PENDING_GPIO is a logical OR

	 * of MFX_REG_IRQ_GPI _PENDING1/_PENDING2/_PENDING3

	/*

	 * Check that ID is the complement of the I2C address:

	 * STMFX I2C address follows the 7-bit format (MSB), that's why

	 * client->addr is shifted.

	 *

	 * STMFX_I2C_ADDR|       STMFX         |        Linux

	 *   input pin   | I2C device address  | I2C device address

	 *---------------------------------------------------------

	 *       0       | b: 1000 010x h:0x84 |       0x42

	 *       1       | b: 1000 011x h:0x86 |       0x43

 Reset STMFX - supply has been stopped during suspend */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-otp.c  --  OTP for Wolfson WM831x PMICs

 *

 * Copyright 2009 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 In bytes */

 Read the unique ID from the chip into id */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I2C driver for Marvell 88PM80x

 *

 * Copyright (C) 2012 Marvell International Ltd.

 * Haojian Zhuang <haojian.zhuang@marvell.com>

 * Joseph(Yossi) Hanin <yhanin@marvell.com>

 * Qiao Zhou <zhouqiao@marvell.com>

 88pm80x chips have same definition for chip id register. */

 88PM800 chip id number */

 88PM805 chip id number */

 88PM860 chip id number */

/*

 * workaround: some registers needed by pm805 are defined in pm800, so

 * need to use this global variable to maintain the relation between

 * pm800 and pm805. would remove it after HW chip fixes the issue.

	/*

	 * workaround: set g_pm80x_chip to the first probed chip. if the

	 * second chip is probed, just point to the companion to each

	 * other so that pm805 can access those specific register. would

	 * remove it after HW chip fixes the issue.

	/*

	 * workaround: clear the dependency between pm800 and pm805.

	 * would remove it after HW chip fixes the issue.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel Quark MFD PCI driver for I2C & GPIO

 *

 * Copyright(c) 2014 Intel Corporation.

 *

 * Intel Quark PCI device for I2C and GPIO controller sharing the same

 * PCI function. This PCI driver will split the 2 devices into their

 * respective drivers.

 PCI BAR for register base address */

 ACPI _ADR value to match the child node */

 The Quark I2C controller source clock */

 This is used as a place holder and will be modified at run-time */

 This is used as a place holder and will be modified at run-time */

 Normal mode by default */

 This driver only requires 1 IRQ vector */

 SPDX-License-Identifier: BSD-2-Clause OR GPL-2.0-or-later

/*

 * ENE KB3930 Embedded Controller Driver

 *

 * Copyright (C) 2020 Lubomir Rintel

 I2C registers that are multiplexing access to the EC RAM. */

 EC RAM registers. */

	/*

	 * This creates a 10 Hz wave on EC_GPIO_WAVE that signals a

	 * shutdown request to the EC. Once the EC detects it, it will

	 * proceed to turn the power off or reset the board depending on

	 * the value of EC_GPIO_OFF_MODE.

 Currently we only support the cells present on Dell Ariel model. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver for TI TPS6586x PMIC family

 *

 * Copyright (c) 2010 CompuLab Ltd.

 * Mike Rapoport <mike@compulab.co.il>

 *

 * Based on da903x.c.

 * Copyright (C) 2008 Compulab, Ltd.

 * Mike Rapoport <mike@compulab.co.il>

 * Copyright (C) 2006-2008 Marvell International Ltd.

 * Eric Miao <eric.miao@marvell.com>

 interrupt control registers */

 interrupt mask registers */

 device id */

 Maximum register */

 Cache all interrupt mask register */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm8994-irq.c  --  Interrupt controller support for Wolfson WM8994

 *

 * Copyright 2010 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 select user or default irq flags */

 use a GPIO for edge triggered controllers */

 Enable top level interrupt if it was masked */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * driver/mfd/asic3.c

 *

 * Compaq ASIC3 support.

 *

 * Copyright 2001 Compaq Computer Corporation.

 * Copyright 2004-2005 Phil Blundell

 * Copyright 2007-2008 OpenedHand Ltd.

 *

 * Authors: Phil Blundell <pb@handhelds.org>,

 *	    Samuel Ortiz <sameo@openedhand.com>

 IRQs */

 Check all ten register bits */

 Handle GPIO IRQs */

 Clearing IntStatus */

 Handle remaining IRQs in the status register */

 They start at bit 4 and go up */

		/*

		 * if type == IRQ_TYPE_NONE, we should mask interrupts, but

		 * be careful to not unmask them if mask was also called.

		 * Probably need internal state for mask.

 turn on clock to IRQ controller */

 GPIOs */

 Input is 0, Output is 1 */

 Enable all GPIOs */

 MFD cells (SPI, PWM, LED, DS1WM, MMC) */

 Turn on external clocks and the OWM clock */

 Reset and enable DS1WM */

 Not sure if it must be done bit by bit, but leaving as-is */

	/* CLK32 used for card detection and for interruption detection

	 * when HCLK is stopped.

 HCLK 24.576 MHz, BCLK 12.288 MHz: */

 Enable SD card slot 3.3V power supply */

 ASIC3_SD_CTRL_BASE assumes 32-bit addressing, TMIO is 16-bit */

 Put in suspend mode */

 Disable clocks */

 DS1WM */

 MMC */

 Core */

 calculate bus shift from mem resource */

	/* Making a per-device copy is only needed for the

	 * theoretical case of multiple ASIC3s on one board:

 SPDX-License-Identifier: GPL-2.0-only

/*

 * max8907.c - mfd driver for MAX8907

 *

 * Copyright (C) 2010 Gyungoh Yoo <jack.yoo@maxim-ic.com>

 * Copyright (C) 2010-2012, NVIDIA CORPORATION. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Core driver for WM8400.

 *

 * Copyright 2008 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

/*

 * wm8400_init - Generic initialisation

 *

 * The WM8400 can be configured as either an I2C or SPI device.  Probe

 * functions for each bus set up the accessors then call into this to

 * set up the device itself.

 Check that this is actually a WM8400 */

/**

 * wm8400_reset_codec_reg_cache - Reset cached codec registers to

 * their default values.

 *

 * @wm8400: pointer to local driver data structure

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Touchscreen driver for UCB1x00-based touchscreens

 *

 *  Copyright (C) 2001 Russell King, All Rights Reserved.

 *  Copyright (C) 2005 Pavel Machek

 *

 * 21-Jan-2002 <jco@ict.es> :

 *

 * Added support for synchronous A/D mode. This mode is useful to

 * avoid noise induced in the touchpanel by the LCD, provided that

 * the UCB1x00 has a valid LCD sync signal routed to its ADCSYNC pin.

 * It is important to note that the signal connected to the ADCSYNC

 * pin should provide pulses even when the LCD is blanked, otherwise

 * a pen touch needed to unblank the LCD will never be read.

/*

 * Switch to interrupt mode.

/*

 * Switch to pressure mode, and read pressure.  We don't need to wait

 * here, since both plates are being driven.

/*

 * Switch to X position mode and measure Y plate.  We switch the plate

 * configuration in pressure mode, then switch to position mode.  This

 * gives a faster response time.  Even so, we need to wait about 55us

 * for things to stabilise.

/*

 * Switch to Y position mode and measure X plate.  We switch the plate

 * configuration in pressure mode, then switch to position mode.  This

 * gives a faster response time.  Even so, we need to wait about 55us

 * for things to stabilise.

/*

 * Switch to X plate resistance mode.  Set MX to ground, PX to

 * supply.  Measure current.

/*

 * Switch to Y plate resistance mode.  Set MY to ground, PY to

 * supply.  Measure current.

/*

 * This is a RT kernel thread that handles the ADC accesses

 * (mainly so we can use semaphores in the UCB1200 core code

 * to serialise accesses to the ADC).

		/*

		 * Switch back to interrupt mode.

			/*

			 * If we spat out a valid sample set last time,

			 * spit out a "pen off" sample here.

			/*

			 * Filtering is policy.  Policy belongs in user

			 * space.  We therefore leave it to user space

			 * to do any filtering they please.

/*

 * We only detect touch screen _touches_ with this interrupt

 * handler, and even then we just schedule our task.

	/*

	 * If we do this at all, we should allow the user to

	 * measure and read the X and Y resistance at any time.

/*

 * Release touchscreen resources.  Disable IRQs.

/*

 * Initialisation.

 SPDX-License-Identifier: GPL-2.0+

/*

 * SPI driver for Renesas Synchronization Management Unit (SMU) devices.

 *

 * Copyright (C) 2021 Integrated Device Technology, Inc., a Renesas Company.

	/*

	 * 4-wire SPI is a shift register, so for every byte you send,

	 * you get one back at the same time. Example read from 0xC024,

	 * which has value of 0x2D

	 *

	 * MOSI:

	 *       7C 00 C0 #Set page register

	 *       A4 00    #MSB is set, so this is read command

	 * MISO:

	 *       XX 2D    #XX is a dummy byte from sending A4 and we

	 *                 need to throw it away

/*

 * 1-byte (1B) offset addressing:

 * 16-bit register address: the lower 7 bits of the register address come

 * from the offset addr byte and the upper 9 bits come from the page register.

 Simply return if we are on the same page */

 Remember the last page */

 Initialize regmap */

/*

 * Base driver for Marvell 88PM805

 *

 * Copyright (C) 2012 Marvell International Ltd.

 * Haojian Zhuang <haojian.zhuang@marvell.com>

 * Joseph(Yossi) Hanin <yhanin@marvell.com>

 * Qiao Zhou <zhouqiao@marvell.com>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

 NULL terminated */

 Interrupt Number in 88PM805 */

0 */

1 */

5 */

10 */

11 */

 Headset microphone insertion or removal */

 Audio short HP1 */

 Audio short HP2 */

 INT0 */

 INT1 */

	/*

	 * irq_mode defines the way of clearing interrupt. it's read-clear by

	 * default.

	/*

	 * PM805_INT_STATUS is under 32K clock domain, so need to

	 * add proper delay before the next I2C register access.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Base driver for Analog Devices ADP5520/ADP5501 MFD PMICs

 * LCD Backlight: drivers/video/backlight/adp5520_bl

 * LEDs		: drivers/led/leds-adp5520

 * GPIO		: drivers/gpio/adp5520-gpio (ADP5520 only)

 * Keys		: drivers/input/keyboard/adp5520-keys (ADP5520 only)

 *

 * Copyright 2009 Analog Devices Inc.

 *

 * Author: Michael Hennerich <michael.hennerich@analog.com>

 *

 * Derived from da903x:

 * Copyright (C) 2008 Compulab, Ltd.

 *	Mike Rapoport <mike@compulab.co.il>

 *

 * Copyright (C) 2006-2008 Marvell International Ltd.

 *	Eric Miao <eric.miao@marvell.com>

 ACK, Sticky bits are W1C */

 All other bits are W1C */

 SPDX-License-Identifier: GPL-2.0+

/*

 * TQ-Systems PLD MFD core driver, based on vendor driver by

 * Vadim V.Vlasov <vvlasov@dev.rtsoft.ru>

 *

 * Copyright (c) 2015 TQ-Systems GmbH

 * Copyright (c) 2019 Andrew Lunn <andrew@lunn.ch>

/*

 * The IRQ resource must be first, since it is updated with the

 * configured IRQ in the probe function.

 4K EEPROM at 0x50 */

 Assumes the IRQ resource is first. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * wm831x-i2c.c  --  I2C access for Wolfson WM831x PMICs

 *

 * Copyright 2009,2010 Wolfson Microelectronics PLC.

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Arizona-i2c.c  --  Arizona I2C bus interface

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 SPDX-License-Identifier: GPL-2.0-or-later



 Copyright (C) 2018 ROHM Semiconductors



 ROHM BD71837MWV and BD71847MWV PMIC driver



 Datasheet for BD71837MWV available from

 https:
 Initialise early so consumer devices can complete system boot */

 SPDX-License-Identifier: GPL-2.0+

/*

 * I2C driver for Renesas Synchronization Management Unit (SMU) devices.

 *

 * Copyright (C) 2021 Integrated Device Technology, Inc., a Renesas Company.

/*

 * 16-bit register address: the lower 8 bits of the register address come

 * from the offset addr byte and the upper 8 bits come from the page register.

/*

 * 15-bit register address: the lower 7 bits of the register address come

 * from the offset addr byte and the upper 8 bits come from the page register.

 SPDX-License-Identifier: GPL-2.0+

/* Interrupt support for Dialog DA9063

 *

 * Copyright 2012 Dialog Semiconductor Ltd.

 * Copyright 2013 Philipp Zabel, Pengutronix

 *

 * Author: Michal Hajduk, Dialog Semiconductor

 DA9063 event A register */

 DA9063 event B register */

 DA9063 event C register */

 DA9063 event D register */

 DA9063 event A register */

 DA9063 event B register */

 DA9063 event C register */

 DA9063 event D register */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2004 Texas Instruments, Inc.

 *

 * Some parts based tps65010.c:

 * Copyright (C) 2004 Texas Instruments and

 * Copyright (C) 2004-2005 David Brownell

 *

 * Some parts based on tlv320aic24.c:

 * Copyright (C) by Kai Svahn <kai.svahn@nokia.com>

 *

 * Changes for interrupt handling and clean-up by

 * Tony Lindgren <tony@atomide.com> and Imre Deak <imre.deak@nokia.com>

 * Cleanup and generalized support for voltage setting by

 * Juha Yrjola

 * Added support for controlling VCORE and regulator sleep states,

 * Amit Kucheria <amit.kucheria@nokia.com>

 * Copyright (C) 2005, 2006 Nokia Corporation

 MMC slot 1 card change */

 MMC slot 2 card change */

 MMC DAT1 low in slot 1 */

 MMC DAT1 low in slot 2 */

 Low battery */

 Hot die detect */

 UVLO detect */

 Thermal shutdown */

 RTC timer */

 RTC alarm */

 RTC error */

 Push button */

 Reserved */

 Reserved */

 Reserved */

 Reserved */

 VCORE_CTRL1 register */

 GPIO_CTRL register */

 MCT_CTRL1 register */

 MCT_CTRL2 register */

 MCT_CTRL3 register */

 MCT_PIN_ST register */

 Adds a handler for an interrupt. Does not run in interrupt context */

 Removes handler for an interrupt */

/*

 * Gets scheduled when a card detect interrupt happens. Note that in some cases

 * this line is wired to card cover switch rather than the card detect switch

 * in each slot. In this case the cards are not seen by menelaus.

 * FIXME: Add handling for D1 too

/*

 * Toggles the MMC slots between open-drain and push-pull mode.

 Disable autonomous shutdown */

 Wait for voltage to stabilize */

/*

 * Vcore can be programmed in two ways:

 * SW-controlled: Required voltage is programmed into VCORE_CTRL1

 * HW-controlled: Required range (roof-floor) is programmed into VCORE_CTRL3

 * and VCORE_CTRL4

 *

 * Call correct 'set' function accordingly

 HW mode, turn OFF byte comparator */

-----------------------------------------------------------------------*/

 Handles Menelaus interrupts. Does not run in interrupt context */

/*

 * We cannot use I2C in interrupt context, so we just schedule work.

-----------------------------------------------------------------------*/

/*

 * The RTC needs to be set once, then it runs on backup battery power.

 * It supports alarms, including system wake alarms (from some modes);

 * and 1/second IRQs if requested.

 block read date and time registers */

 write date and time registers */

 now commit the write */

 block read alarm registers */

 NOTE we *could* check if actually pending... */

 clear previous alarm enable */

 write alarm registers */

 enable alarm if requested */

 report 1/sec update */

 alarm IRQ */

 1/second "update" IRQ */

 REVISIT no compensation register support ... */

 report alarm */

 then disable it; alarms are oneshot */

 assume 32KDETEN pin is pulled high */

 support RTC alarm; it can issue wakeups */

 be sure RTC is enabled; allow 1/sec irqs; leave 12hr mode alone */

 nothing */

-----------------------------------------------------------------------*/

 If a true probe check the device */

 Ack and disable all Menelaus interrupts */

 Set output buffer strengths */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * TI Touch Screen / ADC MFD driver

 *

 * Copyright (C) 2012 Texas Instruments Incorporated - https://www.ti.com/

		/*

		 * Sequencer should either be idle or

		 * busy applying the charge step.

 Allocate memory for device */

		/*

		 * When adding support for the magnetic stripe reader, here is

		 * the place to look for the number of tracks used from device

		 * tree. Let's default to 0 for now.

	/*

	 * The TSC_ADC_Subsystem has 2 clock domains: OCP_CLK and ADC_CLK.

	 * ADCs produce a 12-bit sample every 15 ADC_CLK cycles.

	 * am33xx ADCs expect to capture 200ksps.

	 * am47xx ADCs expect to capture 867ksps.

	 * We need ADC clocks respectively running at 3MHz and 13MHz.

	 * These frequencies are valid since TSC_ADC_SS controller design

	 * assumes the OCP clock is at least 6x faster than the ADC clock.

	/*

	 * Set the control register bits. tscadc->ctrl stores the configuration

	 * of the CTRL register but not the subsystem enable bit which must be

	 * added manually when timely.

 Enable the TSC module enable bit */

 TSC or MAG Cell */

 ADC Cell */

/*

 * SPI access driver for TI TPS65912x PMICs

 *

 * Copyright (C) 2015 Texas Instruments Incorporated - https://www.ti.com/

 *	Andrew F. Davis <afd@ti.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether expressed or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License version 2 for more details.

 *

 * Based on the TPS65218 driver and the previous TPS65912 driver by

 * Margarita Olaya Cabrera <magi@slimlogic.co.uk>

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2011, Code Aurora Forum. All rights reserved.

 level select */

 mask falling edge */

 mask rising edge */

 clear interrupt */

 PMIC4 revision */

 PMIC4 revision 2 */

 MUST BE AT THE END OF THIS STRUCT */

 Check IRQ bits */

 block # */

 on pm8xxx series masters start from bit 1 of the root */

 Read allowed masters for blocks. */

 Convert block offset to global block number */

 Check IRQ bits */

 bits 1 through 7 marks the first 7 blocks in master 0 */

 bit 0 marks if master 1 contains any bits */

 Read PMIC chip revision */

 Read PMIC chip revision 2 */

 SPDX-License-Identifier: GPL-2.0-or-later



 Copyright (C) 2019 ROHM Semiconductors



 ROHM BD70528 PMIC driver

	/*

	 * We use BD71837 driver to drive the clock block. Only differences to

	 * BD70528 clock gate are the register address and mask.

		/*

		 * WDT control reg is special. Magic values must be written to

		 * it in order to change the control. Should not be cached.

		/*

		 * BD70528 also contains a few other registers which require

		 * magic sequences to be written in order to update the value.

		 * At least SHIPMODE, HWRESET, WARMRESET,and STANDBY

/*

 * Mapping of main IRQ register bits to sub-IRQ register offsets so that we can

 * access corect sub-IRQ registers based on bits that are set in main IRQ

 * register.

 Shutdown */

 Power failure */

 VR FAULT */

 PMU interrupts */

 Charger 1 and Charger 2 */

 RTC */

 GPIO */

 Invalid operation */

	/*

	 * Disallow type setting for all IRQs by default as most of them do not

	 * support setting type.

 Set IRQ typesetting information for GPIO pins 0 - 3 */

	/*

	 * BD70528 IRQ controller is not touching the main mask register.

	 * So enable the GPIO block interrupts at main level. We can just leave

	 * them enabled as the IRQ controller should disable IRQs from

	 * sub-registers when IRQ is disabled or freed.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel Sunrisepoint LPSS core support.

 *

 * Copyright (C) 2015, Intel Corporation

 *

 * Authors: Andy Shevchenko <andriy.shevchenko@linux.intel.com>

 *          Mika Westerberg <mika.westerberg@linux.intel.com>

 *          Heikki Krogerus <heikki.krogerus@linux.intel.com>

 *          Jarkko Nikula <jarkko.nikula@linux.intel.com>

 Offsets from lpss->priv */

 This matches the type field in CAPS register */

/*

 * Cells needs to be ordered so that the iDMA is created first. This is

 * because we need to be sure the DMA is available when the host controller

 * driver is probed.

 Cache the values into lpss structure */

	/*

	 * Program latency tolerance (LTR) accordingly what has been asked

	 * by the PM QoS layer or disable it in case we were passed

	 * negative value or PM_QOS_LATENCY_ANY.

 Cache the values into lpss structure */

 Bring out the device from reset */

 Set the device in reset state */

 Make sure that SPI multiblock DMA transfers are re-enabled */

 Root clock */

	/*

	 * Support for clock divider only if it has some preset value.

	 * Otherwise we assume that the divider is not used.

 Clock for the host controller */

	/*

	 * Resume both child devices before entering system sleep. This

	 * ensures that they are in proper state before they get suspended.

 Save device context */

	/*

	 * If the device type is not UART, then put the controller into

	 * reset. UART cannot be put into reset since S3/S0ix fail when

	 * no_console_suspend flag is enabled.

 Restore device context */

/*

 * Ensure the DMA driver is loaded before the host controller device appears,

 * so that the host controller driver can request its DMA channels as early

 * as possible.

 *

 * If the DMA module is not there that's OK as well.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver for TPS61050/61052 boost converters, used for while LED

 * driving, audio power amplification, white LED flash, and generic

 * boost conversion. Additionally it provides a 1-bit GPIO pin (out or in)

 * and a flash synchronization pin to synchronize flash events when used as

 * flashgun.

 *

 * Copyright (C) 2011 ST-Ericsson SA

 * Written on behalf of Linaro for ST-Ericsson

 *

 * Author: Linus Walleij <linus.walleij@linaro.org>

/*

 * MFD cells - we always have a GPIO cell and we have one cell

 * which is selected operation mode.

 Put chip in shutdown mode */

 SPDX-License-Identifier: GPL-2.0



 Copyright (C) 2018 BayLibre SAS

 Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>



 Core MFD driver for MAXIM 77650/77651 charger/power-supply.

 Programming manual: https:
	/*

	 * This IC has a low-power mode which reduces the quiescent current

	 * consumption to ~5.6uA but is only suitable for systems consuming

	 * less than ~2mA. Since this is not likely the case even on

	 * linux-based wearables - keep the chip in normal power mode.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Device access for Dialog DA9052 PMICs.

 *

 * Copyright(c) 2011 Dialog Semiconductor Ltd.

 *

 * Author: David Dajun Chen <dchen@diasemi.com>

/*

 * TBAT look-up table is computed from the R90 reg (8 bit register)

 * reading as below. The battery temperature is in milliCentigrade

 * TBAT = (1/(t1+1/298) - 273) * 1000 mC

 * where t1 = (1/B)* ln(( ADCval * 2.5)/(R25*ITBAT*255))

 * Default values are R25 = 10e3, B = 3380, ITBAT = 50e-6

 * Example:

 * R25=10E3, B=3380, ITBAT=50e-6, ADCVAL=62d calculates

 * TBAT = 20015 mili degrees Centrigrade

 *

 Channel gets activated on enabling the Conversion bit */

 Wait for an interrupt */

 ARRAY_SIZE check is not needed since TBAT is a 8-bit register */

	/*

	 * Check if touchscreen pins are used are analogue input instead

	 * of having a touchscreen connected to them. The analogue input

	 * functionality will be provided by hwmon driver (if enabled).

 SPDX-License-Identifier: GPL-2.0+

/*

 * Multifunction core driver for Zodiac Inflight Innovations RAVE

 * Supervisory Processor(SP) MCU that is connected via dedicated UART

 * port

 *

 * Copyright (C) 2017 Zodiac Inflight Innovations

/*

 * UART protocol using following entities:

 *  - message to MCU => ACK response

 *  - event from MCU => event ACK

 *

 * Frame structure:

 * <STX> <DATA> <CHECKSUM> <ETX>

 * Where:

 * - STX - is start of transmission character

 * - ETX - end of transmission

 * - DATA - payload

 * - CHECKSUM - checksum calculated on <DATA>

 *

 * If <DATA> or <CHECKSUM> contain one of control characters, then it is

 * escaped using <DLE> control code. Added <DLE> does not participate in

 * checksum calculation.

/*

 * We don't store STX, ETX and unescaped bytes, so Rx is only

 * DATA + CSUM

/*

 * For Tx we have to have space for everything, STX, EXT and

 * potentially stuffed DATA + CSUM data + csum

/**

 * enum rave_sp_deframer_state - Possible state for de-framer

 *

 * @RAVE_SP_EXPECT_SOF:		 Scanning input for start-of-frame marker

 * @RAVE_SP_EXPECT_DATA:	 Got start of frame marker, collecting frame

 * @RAVE_SP_EXPECT_ESCAPED_DATA: Got escape character, collecting escaped byte

/**

 * struct rave_sp_deframer - Device protocol deframer

 *

 * @state:  Current state of the deframer

 * @data:   Buffer used to collect deframed data

 * @length: Number of bytes de-framed so far

/**

 * struct rave_sp_reply - Reply as per RAVE device protocol

 *

 * @length:	Expected reply length

 * @data:	Buffer to store reply payload in

 * @code:	Expected reply code

 * @ackid:	Expected reply ACK ID

 * @received:   Successful reply reception completion

/**

 * struct rave_sp_checksum - Variant specific checksum implementation details

 *

 * @length:	Calculated checksum length

 * @subroutine:	Utilized checksum algorithm implementation

/**

 * struct rave_sp_variant_cmds - Variant specific command routines

 *

 * @translate:	Generic to variant specific command mapping routine

 * @get_status: Variant specific implementation of CMD_GET_STATUS

/**

 * struct rave_sp_variant - RAVE supervisory processor core variant

 *

 * @checksum:	Variant specific checksum implementation

 * @cmd:	Variant specific command pointer table

 *

/**

 * struct rave_sp - RAVE supervisory processor core

 *

 * @serdev:			Pointer to underlying serdev

 * @deframer:			Stored state of the protocol deframer

 * @ackid:			ACK ID used in last reply sent to the device

 * @bus_lock:			Lock to serialize access to the device

 * @reply_lock:			Lock protecting @reply

 * @reply:			Pointer to memory to store reply payload

 *

 * @variant:			Device variant specific information

 * @event_notifier_list:	Input event notification chain

 *

 * @part_number_firmware:	Firmware version

 * @part_number_bootloader:	Bootloader version

	/*

	 * While the rest of the wire protocol is little-endian,

	 * CCITT-16 CRC in RDU2 device is sent out in big-endian order.

	/*

	 * There isn't a single rule that describes command code ->

	 * ACK code transformation, but, going through various

	 * versions of ICDs, there appear to be three distinct groups

	 * that can be described by simple transformation.

		/*

		 * Commands implemented by firmware found in RDU1 and

		 * older devices all seem to obey the following rule

		/*

		 * Events emitted by all versions of the firmare use

		 * least significant bit to get an ACK code

		/*

		 * Commands implemented by firmware found in RDU2 are

		 * similar to "old" commands, but they use slightly

		 * different offset

			/*

			 * We are relying on memcpy(dst, src, 0) to be a no-op

			 * when handling commands that have a no-payload reply

			/*

			 * Treat special byte values first

				/*

				 * Once we extracted a complete frame

				 * out of a stream, we call it done

				 * and proceed to bailing out while

				 * resetting the framer to initial

				 * state, regardless if we've consumed

				 * all of the stream or not.

				/*

				 * If we encounter second "start of

				 * the frame" marker before seeing

				 * corresponding "end of frame", we

				 * reset the framer and ignore both:

				 * frame started by first SOF and

				 * frame started by current SOF.

				 *

				 * NOTE: The above means that only the

				 * frame started by third SOF, sent

				 * after this one will have a chance

				 * to get throught.

				/*

				 * If we encounter escape sequence we

				 * need to skip it and collect the

				 * byte that follows. We do it by

				 * forcing the next iteration of the

				 * encompassing while loop.

			/*

			 * For the rest of the bytes, that are not

			 * speical snoflakes, we do the same thing

			 * that we do to escaped data - collect it in

			 * deframer buffer

				/*

				 * If the amount of data we've

				 * accumulated for current frame so

				 * far starts to exceed the capacity

				 * of deframer's buffer, there's

				 * nothing else we can do but to

				 * discard that data and start

				 * assemblying a new frame again

			/*

			 * We've extracted out special byte, now we

			 * can go back to regular data collecting

	/*

	 * The only way to get out of the above loop and end up here

	 * is throught consuming all of the supplied data, so here we

	 * report that we processed it all.

	/*

	 * NOTE: A number of codepaths that will drop us here will do

	 * so before consuming all 'size' bytes of the data passed by

	 * serdev layer. We rely on the fact that serdev layer will

	 * re-execute this handler with the remainder of the Rx bytes

	 * once we report actual number of bytes that we processed.

		/*

		 * As per RDU2 ICD 3.4.47 CMD_GET_COPPER_REV code is

		 * different from that for RDU1 and it is set to 0x28.

	/*

	 * All of the following command codes were taken from "Table :

	 * Communications Protocol Message Types" in section 3.3

	 * "MESSAGE TYPES" of Rave PIC24 ICD.

	/*

	 * NOTE: The format string below uses %02d to display u16

	 * intentionally for the sake of backwards compatibility with

	 * legacy software.

 sentinel */ }

	/*

	 * Those strings already have a \n embedded, so there's no

	 * need to have one in format string.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MFD core driver for Ricoh RN5T618 PMIC

 *

 * Copyright (C) 2014 Beniamino Galvani <b.galvani@gmail.com>

 * Copyright (C) 2016 Toradex AG

 disable automatic repower-on */

 start power-off sequence */

	/*

	 * Re-power factor detection on PMIC side is not instant. 1ms

	 * proved to be enough time until reset takes effect.

 SPDX-License-Identifier: GPL-2.0+

/*

 * I2C bus interface for ATC260x PMICs

 *

 * Copyright (C) 2019 Manivannan Sadhasivam <manivannan.sadhasivam@linaro.org>

 * Copyright (C) 2020 Cristian Ciocaltea <cristian.ciocaltea@gmail.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Core driver access RC5T583 power management chip.

 *

 * Copyright (c) 2011-2012, NVIDIA CORPORATION.  All rights reserved.

 * Author: Laxman dewangan <ldewangan@nvidia.com>

 *

 * Based on code

 *	Copyright (C) 2011 RICOH COMPANY,LTD

  Clear ONOFFSEL register */

 Clear sleep sequence register */

 Enable caching in interrupt registers */

 This is gpio input register */

 Enable caching in gpio registers */

 Enable caching in sleep seq registers */

 Enable caching of regulator registers */

 Still continue with warning, if irq init fails */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2021 Richtek Technology Corp.

 *

 * Author: ChiYuan Huang <cy_huang@richtek.com>

	/*

	 * Used to prevent the abnormal shutdown.

	 * If SCL/SDA both keep low for one second to reset HW.

 Disable WLED and DSV outputs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * wm5110-tables.c  --  WM5110 data tables

 *

 * Copyright 2012 Wolfson Microelectronics plc

 *

 * Author: Mark Brown <broonie@opensource.wolfsonmicro.com>

 Add extra headphone write sequence locations */

 We use a function so we can use ARRAY_SIZE() */

 R8     - Ctrl IF SPI CFG 1 */

 R9     - Ctrl IF I2C1 CFG 1 */

 R10    - Ctrl IF I2C2 CFG 1 */

 R11    - Ctrl IF I2C1 CFG 2 */

 R12    - Ctrl IF I2C2 CFG 2 */

 R32    - Tone Generator 1 */

 R33    - Tone Generator 2 */

 R34    - Tone Generator 3 */

 R35    - Tone Generator 4 */

 R36    - Tone Generator 5 */

 R48    - PWM Drive 1 */

 R49    - PWM Drive 2 */

 R50    - PWM Drive 3 */

 R64    - Wake control */

 R65    - Sequence control */

 R66    - Spare Triggers */

 R97    - Sample Rate Sequence Select 1 */

 R98    - Sample Rate Sequence Select 2 */

 R99    - Sample Rate Sequence Select 3 */

 R100   - Sample Rate Sequence Select 4 */

 R102   - Always On Triggers Sequence Select 1 */

 R103   - Always On Triggers Sequence Select 2 */

 R104   - Always On Triggers Sequence Select 3 */

 R105   - Always On Triggers Sequence Select 4 */

 R106   - Always On Triggers Sequence Select 5 */

 R107   - Always On Triggers Sequence Select 6 */

 R112   - Comfort Noise Generator */

 R144   - Haptics Control 1 */

 R145   - Haptics Control 2 */

 R146   - Haptics phase 1 intensity */

 R147   - Haptics phase 1 duration */

 R148   - Haptics phase 2 intensity */

 R149   - Haptics phase 2 duration */

 R150   - Haptics phase 3 intensity */

 R151   - Haptics phase 3 duration */

 R256   - Clock 32k 1 */

 R257   - System Clock 1 */

 R258   - Sample rate 1 */

 R259   - Sample rate 2 */

 R260   - Sample rate 3 */

 R274   - Async clock 1 */

 R275   - Async sample rate 1 */

 R276   - Async sample rate 2 */

 R329   - Output system clock */

 R330   - Output async clock */

 R338   - Rate Estimator 1 */

 R339   - Rate Estimator 2 */

 R340   - Rate Estimator 3 */

 R341   - Rate Estimator 4 */

 R342   - Rate Estimator 5 */

 R369   - FLL1 Control 1 */

 R370   - FLL1 Control 2 */

 R371   - FLL1 Control 3 */

 R372   - FLL1 Control 4 */

 R373   - FLL1 Control 5 */

 R374   - FLL1 Control 6 */

 R376   - FLL1 Control 7 */

 R385   - FLL1 Synchroniser 1 */

 R386   - FLL1 Synchroniser 2 */

 R387   - FLL1 Synchroniser 3 */

 R388   - FLL1 Synchroniser 4 */

 R389   - FLL1 Synchroniser 5 */

 R390   - FLL1 Synchroniser 6 */

 R390   - FLL1 Synchroniser 7 */

 R393   - FLL1 Spread Spectrum */

 R394   - FLL1 GPIO Clock */

 R401   - FLL2 Control 1 */

 R402   - FLL2 Control 2 */

 R403   - FLL2 Control 3 */

 R404   - FLL2 Control 4 */

 R405   - FLL2 Control 5 */

 R406   - FLL2 Control 6 */

 R408   - FLL2 Control 7 */

 R417   - FLL2 Synchroniser 1 */

 R418   - FLL2 Synchroniser 2 */

 R419   - FLL2 Synchroniser 3 */

 R420   - FLL2 Synchroniser 4 */

 R421   - FLL2 Synchroniser 5 */

 R422   - FLL2 Synchroniser 6 */

 R422   - FLL2 Synchroniser 7 */

 R425   - FLL2 Spread Spectrum */

 R426   - FLL2 GPIO Clock */

 R512   - Mic Charge Pump 1 */

 R528   - LDO1 Control 1 */

 R531   - LDO2 Control 1 */

 R536   - Mic Bias Ctrl 1 */

 R537   - Mic Bias Ctrl 2 */

 R538   - Mic Bias Ctrl 3 */

 R659   - Accessory Detect Mode 1 */

 R667   - Headphone Detect 1 */

 R674   - Micd clamp control */

 R675   - Mic Detect 1 */

 R676   - Mic Detect 2 */

 R678   - Mic Detect Level 1 */

 R679   - Mic Detect Level 2 */

 R680   - Mic Detect Level 3 */

 R681   - Mic Detect Level 4 */

 R707   - Mic noise mix control 1 */

 R715   - Isolation control */

 R723   - Jack detect analogue */

 R768   - Input Enables */

 R776   - Input Rate */

 R777   - Input Volume Ramp */

 R780   - HPF Control */

 R784   - IN1L Control */

 R785   - ADC Digital Volume 1L */

 R786   - DMIC1L Control */

 R788   - IN1R Control */

 R789   - ADC Digital Volume 1R */

 R790   - DMIC1R Control */

 R792   - IN2L Control */

 R793   - ADC Digital Volume 2L */

 R794   - DMIC2L Control */

 R796   - IN2R Control */

 R797   - ADC Digital Volume 2R */

 R798   - DMIC2R Control */

 R800   - IN3L Control */

 R801   - ADC Digital Volume 3L */

 R802   - DMIC3L Control */

 R804   - IN3R Control */

 R805   - ADC Digital Volume 3R */

 R806   - DMIC3R Control */

 R808   - IN4L Control */

 R809   - ADC Digital Volume 4L */

 R810   - DMIC4L Control */

 R812   - IN4R Control */

 R813   - ADC Digital Volume 4R */

 R814   - DMIC4R Control */

 R1024  - Output Enables 1 */

 R1032  - Output Rate 1 */

 R1033  - Output Volume Ramp */

 R1040  - Output Path Config 1L */

 R1041  - DAC Digital Volume 1L */

 R1042  - DAC Volume Limit 1L */

 R1043  - Noise Gate Select 1L */

 R1044  - Output Path Config 1R */

 R1045  - DAC Digital Volume 1R */

 R1046  - DAC Volume Limit 1R */

 R1047  - Noise Gate Select 1R */

 R1048  - Output Path Config 2L */

 R1049  - DAC Digital Volume 2L */

 R1050  - DAC Volume Limit 2L */

 R1051  - Noise Gate Select 2L */

 R1052  - Output Path Config 2R */

 R1053  - DAC Digital Volume 2R */

 R1054  - DAC Volume Limit 2R */

 R1055  - Noise Gate Select 2R */

 R1056  - Output Path Config 3L */

 R1057  - DAC Digital Volume 3L */

 R1058  - DAC Volume Limit 3L */

 R1059  - Noise Gate Select 3L */

 R1060  - Output Path Config 3R */

 R1061  - DAC Digital Volume 3R */

 R1062  - DAC Volume Limit 3R */

 R1063  - Noise Gate Select 3R */

 R1064  - Output Path Config 4L */

 R1065  - DAC Digital Volume 4L */

 R1066  - Out Volume 4L */

 R1067  - Noise Gate Select 4L */

 R1068  - Output Path Config 4R */

 R1069  - DAC Digital Volume 4R */

 R1070  - Out Volume 4R */

 R1071  - Noise Gate Select 4R */

 R1072  - Output Path Config 5L */

 R1073  - DAC Digital Volume 5L */

 R1074  - DAC Volume Limit 5L */

 R1075  - Noise Gate Select 5L */

 R1076  - Output Path Config 5R */

 R1077  - DAC Digital Volume 5R */

 R1078  - DAC Volume Limit 5R */

 R1079  - Noise Gate Select 5R */

 R1080  - Output Path Config 6L */

 R1081  - DAC Digital Volume 6L */

 R1082  - DAC Volume Limit 6L */

 R1083  - Noise Gate Select 6L */

 R1084  - Output Path Config 6R */

 R1085  - DAC Digital Volume 6R */

 R1086  - DAC Volume Limit 6R */

 R1087  - Noise Gate Select 6R */

 R1088  - DRE Enable */

 R1104  - DAC AEC Control 1 */

 R1112  - Noise Gate Control */

 R1168  - PDM SPK1 CTRL 1 */

 R1169  - PDM SPK1 CTRL 2 */

 R1170  - PDM SPK2 CTRL 1 */

 R1171  - PDM SPK2 CTRL 2 */

 R1184  - HP1 Short Circuit Ctrl */

 R1185  - HP2 Short Circuit Ctrl */

 R1186  - HP3 Short Circuit Ctrl */

 R1280  - AIF1 BCLK Ctrl */

 R1281  - AIF1 Tx Pin Ctrl */

 R1282  - AIF1 Rx Pin Ctrl */

 R1283  - AIF1 Rate Ctrl */

 R1284  - AIF1 Format */

 R1285  - AIF1 Tx BCLK Rate */

 R1286  - AIF1 Rx BCLK Rate */

 R1287  - AIF1 Frame Ctrl 1 */

 R1288  - AIF1 Frame Ctrl 2 */

 R1289  - AIF1 Frame Ctrl 3 */

 R1290  - AIF1 Frame Ctrl 4 */

 R1291  - AIF1 Frame Ctrl 5 */

 R1292  - AIF1 Frame Ctrl 6 */

 R1293  - AIF1 Frame Ctrl 7 */

 R1294  - AIF1 Frame Ctrl 8 */

 R1295  - AIF1 Frame Ctrl 9 */

 R1296  - AIF1 Frame Ctrl 10 */

 R1297  - AIF1 Frame Ctrl 11 */

 R1298  - AIF1 Frame Ctrl 12 */

 R1299  - AIF1 Frame Ctrl 13 */

 R1300  - AIF1 Frame Ctrl 14 */

 R1301  - AIF1 Frame Ctrl 15 */

 R1302  - AIF1 Frame Ctrl 16 */

 R1303  - AIF1 Frame Ctrl 17 */

 R1304  - AIF1 Frame Ctrl 18 */

 R1305  - AIF1 Tx Enables */

 R1306  - AIF1 Rx Enables */

 R1344  - AIF2 BCLK Ctrl */

 R1345  - AIF2 Tx Pin Ctrl */

 R1346  - AIF2 Rx Pin Ctrl */

 R1347  - AIF2 Rate Ctrl */

 R1348  - AIF2 Format */

 R1349  - AIF2 Tx BCLK Rate */

 R1350  - AIF2 Rx BCLK Rate */

 R1351  - AIF2 Frame Ctrl 1 */

 R1352  - AIF2 Frame Ctrl 2 */

 R1353  - AIF2 Frame Ctrl 3 */

 R1354  - AIF2 Frame Ctrl 4 */

 R1355  - AIF2 Frame Ctrl 5 */

 R1356  - AIF2 Frame Ctrl 6 */

 R1357  - AIF2 Frame Ctrl 7 */

 R1358  - AIF2 Frame Ctrl 8 */

 R1361  - AIF2 Frame Ctrl 11 */

 R1362  - AIF2 Frame Ctrl 12 */

 R1363  - AIF2 Frame Ctrl 13 */

 R1364  - AIF2 Frame Ctrl 14 */

 R1365  - AIF2 Frame Ctrl 15 */

 R1366  - AIF2 Frame Ctrl 16 */

 R1369  - AIF2 Tx Enables */

 R1370  - AIF2 Rx Enables */

 R1408  - AIF3 BCLK Ctrl */

 R1409  - AIF3 Tx Pin Ctrl */

 R1410  - AIF3 Rx Pin Ctrl */

 R1411  - AIF3 Rate Ctrl */

 R1412  - AIF3 Format */

 R1413  - AIF3 Tx BCLK Rate */

 R1414  - AIF3 Rx BCLK Rate */

 R1415  - AIF3 Frame Ctrl 1 */

 R1416  - AIF3 Frame Ctrl 2 */

 R1417  - AIF3 Frame Ctrl 3 */

 R1418  - AIF3 Frame Ctrl 4 */

 R1425  - AIF3 Frame Ctrl 11 */

 R1426  - AIF3 Frame Ctrl 12 */

 R1433  - AIF3 Tx Enables */

 R1434  - AIF3 Rx Enables */

 R1507  - SLIMbus Framer Ref Gear */

 R1509  - SLIMbus Rates 1 */

 R1510  - SLIMbus Rates 2 */

 R1511  - SLIMbus Rates 3 */

 R1512  - SLIMbus Rates 4 */

 R1513  - SLIMbus Rates 5 */

 R1514  - SLIMbus Rates 6 */

 R1515  - SLIMbus Rates 7 */

 R1516  - SLIMbus Rates 8 */

 R1525  - SLIMbus RX Channel Enable */

 R1526  - SLIMbus TX Channel Enable */

 R1600  - PWM1MIX Input 1 Source */

 R1601  - PWM1MIX Input 1 Volume */

 R1602  - PWM1MIX Input 2 Source */

 R1603  - PWM1MIX Input 2 Volume */

 R1604  - PWM1MIX Input 3 Source */

 R1605  - PWM1MIX Input 3 Volume */

 R1606  - PWM1MIX Input 4 Source */

 R1607  - PWM1MIX Input 4 Volume */

 R1608  - PWM2MIX Input 1 Source */

 R1609  - PWM2MIX Input 1 Volume */

 R1610  - PWM2MIX Input 2 Source */

 R1611  - PWM2MIX Input 2 Volume */

 R1612  - PWM2MIX Input 3 Source */

 R1613  - PWM2MIX Input 3 Volume */

 R1614  - PWM2MIX Input 4 Source */

 R1615  - PWM2MIX Input 4 Volume */

 R1632  - MICMIX Input 1 Source */

 R1633  - MICMIX Input 1 Volume */

 R1634  - MICMIX Input 2 Source */

 R1635  - MICMIX Input 2 Volume */

 R1636  - MICMIX Input 3 Source */

 R1637  - MICMIX Input 3 Volume */

 R1638  - MICMIX Input 4 Source */

 R1639  - MICMIX Input 4 Volume */

 R1640  - NOISEMIX Input 1 Source */

 R1641  - NOISEMIX Input 1 Volume */

 R1642  - NOISEMIX Input 2 Source */

 R1643  - NOISEMIX Input 2 Volume */

 R1644  - NOISEMIX Input 3 Source */

 R1645  - NOISEMIX Input 3 Volume */

 R1646  - NOISEMIX Input 4 Source */

 R1647  - NOISEMIX Input 4 Volume */

 R1664  - OUT1LMIX Input 1 Source */

 R1665  - OUT1LMIX Input 1 Volume */

 R1666  - OUT1LMIX Input 2 Source */

 R1667  - OUT1LMIX Input 2 Volume */

 R1668  - OUT1LMIX Input 3 Source */

 R1669  - OUT1LMIX Input 3 Volume */

 R1670  - OUT1LMIX Input 4 Source */

 R1671  - OUT1LMIX Input 4 Volume */

 R1672  - OUT1RMIX Input 1 Source */

 R1673  - OUT1RMIX Input 1 Volume */

 R1674  - OUT1RMIX Input 2 Source */

 R1675  - OUT1RMIX Input 2 Volume */

 R1676  - OUT1RMIX Input 3 Source */

 R1677  - OUT1RMIX Input 3 Volume */

 R1678  - OUT1RMIX Input 4 Source */

 R1679  - OUT1RMIX Input 4 Volume */

 R1680  - OUT2LMIX Input 1 Source */

 R1681  - OUT2LMIX Input 1 Volume */

 R1682  - OUT2LMIX Input 2 Source */

 R1683  - OUT2LMIX Input 2 Volume */

 R1684  - OUT2LMIX Input 3 Source */

 R1685  - OUT2LMIX Input 3 Volume */

 R1686  - OUT2LMIX Input 4 Source */

 R1687  - OUT2LMIX Input 4 Volume */

 R1688  - OUT2RMIX Input 1 Source */

 R1689  - OUT2RMIX Input 1 Volume */

 R1690  - OUT2RMIX Input 2 Source */

 R1691  - OUT2RMIX Input 2 Volume */

 R1692  - OUT2RMIX Input 3 Source */

 R1693  - OUT2RMIX Input 3 Volume */

 R1694  - OUT2RMIX Input 4 Source */

 R1695  - OUT2RMIX Input 4 Volume */

 R1696  - OUT3LMIX Input 1 Source */

 R1697  - OUT3LMIX Input 1 Volume */

 R1698  - OUT3LMIX Input 2 Source */

 R1699  - OUT3LMIX Input 2 Volume */

 R1700  - OUT3LMIX Input 3 Source */

 R1701  - OUT3LMIX Input 3 Volume */

 R1702  - OUT3LMIX Input 4 Source */

 R1703  - OUT3LMIX Input 4 Volume */

 R1704  - OUT3RMIX Input 1 Source */

 R1705  - OUT3RMIX Input 1 Volume */

 R1706  - OUT3RMIX Input 2 Source */

 R1707  - OUT3RMIX Input 2 Volume */

 R1708  - OUT3RMIX Input 3 Source */

 R1709  - OUT3RMIX Input 3 Volume */

 R1710  - OUT3RMIX Input 4 Source */

 R1711  - OUT3RMIX Input 4 Volume */

 R1712  - OUT4LMIX Input 1 Source */

 R1713  - OUT4LMIX Input 1 Volume */

 R1714  - OUT4LMIX Input 2 Source */

 R1715  - OUT4LMIX Input 2 Volume */

 R1716  - OUT4LMIX Input 3 Source */

 R1717  - OUT4LMIX Input 3 Volume */

 R1718  - OUT4LMIX Input 4 Source */

 R1719  - OUT4LMIX Input 4 Volume */

 R1720  - OUT4RMIX Input 1 Source */

 R1721  - OUT4RMIX Input 1 Volume */

 R1722  - OUT4RMIX Input 2 Source */

 R1723  - OUT4RMIX Input 2 Volume */

 R1724  - OUT4RMIX Input 3 Source */

 R1725  - OUT4RMIX Input 3 Volume */

 R1726  - OUT4RMIX Input 4 Source */

 R1727  - OUT4RMIX Input 4 Volume */

 R1728  - OUT5LMIX Input 1 Source */

 R1729  - OUT5LMIX Input 1 Volume */

 R1730  - OUT5LMIX Input 2 Source */

 R1731  - OUT5LMIX Input 2 Volume */

 R1732  - OUT5LMIX Input 3 Source */

 R1733  - OUT5LMIX Input 3 Volume */

 R1734  - OUT5LMIX Input 4 Source */

 R1735  - OUT5LMIX Input 4 Volume */

 R1736  - OUT5RMIX Input 1 Source */

 R1737  - OUT5RMIX Input 1 Volume */

 R1738  - OUT5RMIX Input 2 Source */

 R1739  - OUT5RMIX Input 2 Volume */

 R1740  - OUT5RMIX Input 3 Source */

 R1741  - OUT5RMIX Input 3 Volume */

 R1742  - OUT5RMIX Input 4 Source */

 R1743  - OUT5RMIX Input 4 Volume */

 R1744  - OUT6LMIX Input 1 Source */

 R1745  - OUT6LMIX Input 1 Volume */

 R1746  - OUT6LMIX Input 2 Source */

 R1747  - OUT6LMIX Input 2 Volume */

 R1748  - OUT6LMIX Input 3 Source */

 R1749  - OUT6LMIX Input 3 Volume */

 R1750  - OUT6LMIX Input 4 Source */

 R1751  - OUT6LMIX Input 4 Volume */

 R1752  - OUT6RMIX Input 1 Source */

 R1753  - OUT6RMIX Input 1 Volume */

 R1754  - OUT6RMIX Input 2 Source */

 R1755  - OUT6RMIX Input 2 Volume */

 R1756  - OUT6RMIX Input 3 Source */

 R1757  - OUT6RMIX Input 3 Volume */

 R1758  - OUT6RMIX Input 4 Source */

 R1759  - OUT6RMIX Input 4 Volume */

 R1792  - AIF1TX1MIX Input 1 Source */

 R1793  - AIF1TX1MIX Input 1 Volume */

 R1794  - AIF1TX1MIX Input 2 Source */

 R1795  - AIF1TX1MIX Input 2 Volume */

 R1796  - AIF1TX1MIX Input 3 Source */

 R1797  - AIF1TX1MIX Input 3 Volume */

 R1798  - AIF1TX1MIX Input 4 Source */

 R1799  - AIF1TX1MIX Input 4 Volume */

 R1800  - AIF1TX2MIX Input 1 Source */

 R1801  - AIF1TX2MIX Input 1 Volume */

 R1802  - AIF1TX2MIX Input 2 Source */

 R1803  - AIF1TX2MIX Input 2 Volume */

 R1804  - AIF1TX2MIX Input 3 Source */

 R1805  - AIF1TX2MIX Input 3 Volume */

 R1806  - AIF1TX2MIX Input 4 Source */

 R1807  - AIF1TX2MIX Input 4 Volume */

 R1808  - AIF1TX3MIX Input 1 Source */

 R1809  - AIF1TX3MIX Input 1 Volume */

 R1810  - AIF1TX3MIX Input 2 Source */

 R1811  - AIF1TX3MIX Input 2 Volume */

 R1812  - AIF1TX3MIX Input 3 Source */

 R1813  - AIF1TX3MIX Input 3 Volume */

 R1814  - AIF1TX3MIX Input 4 Source */

 R1815  - AIF1TX3MIX Input 4 Volume */

 R1816  - AIF1TX4MIX Input 1 Source */

 R1817  - AIF1TX4MIX Input 1 Volume */

 R1818  - AIF1TX4MIX Input 2 Source */

 R1819  - AIF1TX4MIX Input 2 Volume */

 R1820  - AIF1TX4MIX Input 3 Source */

 R1821  - AIF1TX4MIX Input 3 Volume */

 R1822  - AIF1TX4MIX Input 4 Source */

 R1823  - AIF1TX4MIX Input 4 Volume */

 R1824  - AIF1TX5MIX Input 1 Source */

 R1825  - AIF1TX5MIX Input 1 Volume */

 R1826  - AIF1TX5MIX Input 2 Source */

 R1827  - AIF1TX5MIX Input 2 Volume */

 R1828  - AIF1TX5MIX Input 3 Source */

 R1829  - AIF1TX5MIX Input 3 Volume */

 R1830  - AIF1TX5MIX Input 4 Source */

 R1831  - AIF1TX5MIX Input 4 Volume */

 R1832  - AIF1TX6MIX Input 1 Source */

 R1833  - AIF1TX6MIX Input 1 Volume */

 R1834  - AIF1TX6MIX Input 2 Source */

 R1835  - AIF1TX6MIX Input 2 Volume */

 R1836  - AIF1TX6MIX Input 3 Source */

 R1837  - AIF1TX6MIX Input 3 Volume */

 R1838  - AIF1TX6MIX Input 4 Source */

 R1839  - AIF1TX6MIX Input 4 Volume */

 R1840  - AIF1TX7MIX Input 1 Source */

 R1841  - AIF1TX7MIX Input 1 Volume */

 R1842  - AIF1TX7MIX Input 2 Source */

 R1843  - AIF1TX7MIX Input 2 Volume */

 R1844  - AIF1TX7MIX Input 3 Source */

 R1845  - AIF1TX7MIX Input 3 Volume */

 R1846  - AIF1TX7MIX Input 4 Source */

 R1847  - AIF1TX7MIX Input 4 Volume */

 R1848  - AIF1TX8MIX Input 1 Source */

 R1849  - AIF1TX8MIX Input 1 Volume */

 R1850  - AIF1TX8MIX Input 2 Source */

 R1851  - AIF1TX8MIX Input 2 Volume */

 R1852  - AIF1TX8MIX Input 3 Source */

 R1853  - AIF1TX8MIX Input 3 Volume */

 R1854  - AIF1TX8MIX Input 4 Source */

 R1855  - AIF1TX8MIX Input 4 Volume */

 R1856  - AIF2TX1MIX Input 1 Source */

 R1857  - AIF2TX1MIX Input 1 Volume */

 R1858  - AIF2TX1MIX Input 2 Source */

 R1859  - AIF2TX1MIX Input 2 Volume */

 R1860  - AIF2TX1MIX Input 3 Source */

 R1861  - AIF2TX1MIX Input 3 Volume */

 R1862  - AIF2TX1MIX Input 4 Source */

 R1863  - AIF2TX1MIX Input 4 Volume */

 R1864  - AIF2TX2MIX Input 1 Source */

 R1865  - AIF2TX2MIX Input 1 Volume */

 R1866  - AIF2TX2MIX Input 2 Source */

 R1867  - AIF2TX2MIX Input 2 Volume */

 R1868  - AIF2TX2MIX Input 3 Source */

 R1869  - AIF2TX2MIX Input 3 Volume */

 R1870  - AIF2TX2MIX Input 4 Source */

 R1871  - AIF2TX2MIX Input 4 Volume */

 R1872  - AIF2TX3MIX Input 1 Source */

 R1873  - AIF2TX3MIX Input 1 Volume */

 R1874  - AIF2TX3MIX Input 2 Source */

 R1875  - AIF2TX3MIX Input 2 Volume */

 R1876  - AIF2TX3MIX Input 3 Source */

 R1877  - AIF2TX3MIX Input 3 Volume */

 R1878  - AIF2TX3MIX Input 4 Source */

 R1879  - AIF2TX3MIX Input 4 Volume */

 R1880  - AIF2TX4MIX Input 1 Source */

 R1881  - AIF2TX4MIX Input 1 Volume */

 R1882  - AIF2TX4MIX Input 2 Source */

 R1883  - AIF2TX4MIX Input 2 Volume */

 R1884  - AIF2TX4MIX Input 3 Source */

 R1885  - AIF2TX4MIX Input 3 Volume */

 R1886  - AIF2TX4MIX Input 4 Source */

 R1887  - AIF2TX4MIX Input 4 Volume */

 R1888  - AIF2TX5MIX Input 1 Source */

 R1889  - AIF2TX5MIX Input 1 Volume */

 R1890  - AIF2TX5MIX Input 2 Source */

 R1891  - AIF2TX5MIX Input 2 Volume */

 R1892  - AIF2TX5MIX Input 3 Source */

 R1893  - AIF2TX5MIX Input 3 Volume */

 R1894  - AIF2TX5MIX Input 4 Source */

 R1895  - AIF2TX5MIX Input 4 Volume */

 R1896  - AIF2TX6MIX Input 1 Source */

 R1897  - AIF2TX6MIX Input 1 Volume */

 R1898  - AIF2TX6MIX Input 2 Source */

 R1899  - AIF2TX6MIX Input 2 Volume */

 R1900  - AIF2TX6MIX Input 3 Source */

 R1901  - AIF2TX6MIX Input 3 Volume */

 R1902  - AIF2TX6MIX Input 4 Source */

 R1903  - AIF2TX6MIX Input 4 Volume */

 R1920  - AIF3TX1MIX Input 1 Source */

 R1921  - AIF3TX1MIX Input 1 Volume */

 R1922  - AIF3TX1MIX Input 2 Source */

 R1923  - AIF3TX1MIX Input 2 Volume */

 R1924  - AIF3TX1MIX Input 3 Source */

 R1925  - AIF3TX1MIX Input 3 Volume */

 R1926  - AIF3TX1MIX Input 4 Source */

 R1927  - AIF3TX1MIX Input 4 Volume */

 R1928  - AIF3TX2MIX Input 1 Source */

 R1929  - AIF3TX2MIX Input 1 Volume */

 R1930  - AIF3TX2MIX Input 2 Source */

 R1931  - AIF3TX2MIX Input 2 Volume */

 R1932  - AIF3TX2MIX Input 3 Source */

 R1933  - AIF3TX2MIX Input 3 Volume */

 R1934  - AIF3TX2MIX Input 4 Source */

 R1935  - AIF3TX2MIX Input 4 Volume */

 R1984  - SLIMTX1MIX Input 1 Source */

 R1985  - SLIMTX1MIX Input 1 Volume */

 R1986  - SLIMTX1MIX Input 2 Source */

 R1987  - SLIMTX1MIX Input 2 Volume */

 R1988  - SLIMTX1MIX Input 3 Source */

 R1989  - SLIMTX1MIX Input 3 Volume */

 R1990  - SLIMTX1MIX Input 4 Source */

 R1991  - SLIMTX1MIX Input 4 Volume */

 R1992  - SLIMTX2MIX Input 1 Source */

 R1993  - SLIMTX2MIX Input 1 Volume */

 R1994  - SLIMTX2MIX Input 2 Source */

 R1995  - SLIMTX2MIX Input 2 Volume */

 R1996  - SLIMTX2MIX Input 3 Source */

 R1997  - SLIMTX2MIX Input 3 Volume */

 R1998  - SLIMTX2MIX Input 4 Source */

 R1999  - SLIMTX2MIX Input 4 Volume */

 R2000  - SLIMTX3MIX Input 1 Source */

 R2001  - SLIMTX3MIX Input 1 Volume */

 R2002  - SLIMTX3MIX Input 2 Source */

 R2003  - SLIMTX3MIX Input 2 Volume */

 R2004  - SLIMTX3MIX Input 3 Source */

 R2005  - SLIMTX3MIX Input 3 Volume */

 R2006  - SLIMTX3MIX Input 4 Source */

 R2007  - SLIMTX3MIX Input 4 Volume */

 R2008  - SLIMTX4MIX Input 1 Source */

 R2009  - SLIMTX4MIX Input 1 Volume */

 R2010  - SLIMTX4MIX Input 2 Source */

 R2011  - SLIMTX4MIX Input 2 Volume */

 R2012  - SLIMTX4MIX Input 3 Source */

 R2013  - SLIMTX4MIX Input 3 Volume */

 R2014  - SLIMTX4MIX Input 4 Source */

 R2015  - SLIMTX4MIX Input 4 Volume */

 R2016  - SLIMTX5MIX Input 1 Source */

 R2017  - SLIMTX5MIX Input 1 Volume */

 R2018  - SLIMTX5MIX Input 2 Source */

 R2019  - SLIMTX5MIX Input 2 Volume */

 R2020  - SLIMTX5MIX Input 3 Source */

 R2021  - SLIMTX5MIX Input 3 Volume */

 R2022  - SLIMTX5MIX Input 4 Source */

 R2023  - SLIMTX5MIX Input 4 Volume */

 R2024  - SLIMTX6MIX Input 1 Source */

 R2025  - SLIMTX6MIX Input 1 Volume */

 R2026  - SLIMTX6MIX Input 2 Source */

 R2027  - SLIMTX6MIX Input 2 Volume */

 R2028  - SLIMTX6MIX Input 3 Source */

 R2029  - SLIMTX6MIX Input 3 Volume */

 R2030  - SLIMTX6MIX Input 4 Source */

 R2031  - SLIMTX6MIX Input 4 Volume */

 R2032  - SLIMTX7MIX Input 1 Source */

 R2033  - SLIMTX7MIX Input 1 Volume */

 R2034  - SLIMTX7MIX Input 2 Source */

 R2035  - SLIMTX7MIX Input 2 Volume */

 R2036  - SLIMTX7MIX Input 3 Source */

 R2037  - SLIMTX7MIX Input 3 Volume */

 R2038  - SLIMTX7MIX Input 4 Source */

 R2039  - SLIMTX7MIX Input 4 Volume */

 R2040  - SLIMTX8MIX Input 1 Source */

 R2041  - SLIMTX8MIX Input 1 Volume */

 R2042  - SLIMTX8MIX Input 2 Source */

 R2043  - SLIMTX8MIX Input 2 Volume */

 R2044  - SLIMTX8MIX Input 3 Source */

 R2045  - SLIMTX8MIX Input 3 Volume */

 R2046  - SLIMTX8MIX Input 4 Source */

 R2047  - SLIMTX8MIX Input 4 Volume */

 R2176  - EQ1MIX Input 1 Source */

 R2177  - EQ1MIX Input 1 Volume */

 R2178  - EQ1MIX Input 2 Source */

 R2179  - EQ1MIX Input 2 Volume */

 R2180  - EQ1MIX Input 3 Source */

 R2181  - EQ1MIX Input 3 Volume */

 R2182  - EQ1MIX Input 4 Source */

 R2183  - EQ1MIX Input 4 Volume */

 R2184  - EQ2MIX Input 1 Source */

 R2185  - EQ2MIX Input 1 Volume */

 R2186  - EQ2MIX Input 2 Source */

 R2187  - EQ2MIX Input 2 Volume */

 R2188  - EQ2MIX Input 3 Source */

 R2189  - EQ2MIX Input 3 Volume */

 R2190  - EQ2MIX Input 4 Source */

 R2191  - EQ2MIX Input 4 Volume */

 R2192  - EQ3MIX Input 1 Source */

 R2193  - EQ3MIX Input 1 Volume */

 R2194  - EQ3MIX Input 2 Source */

 R2195  - EQ3MIX Input 2 Volume */

 R2196  - EQ3MIX Input 3 Source */

 R2197  - EQ3MIX Input 3 Volume */

 R2198  - EQ3MIX Input 4 Source */

 R2199  - EQ3MIX Input 4 Volume */

 R2200  - EQ4MIX Input 1 Source */

 R2201  - EQ4MIX Input 1 Volume */

 R2202  - EQ4MIX Input 2 Source */

 R2203  - EQ4MIX Input 2 Volume */

 R2204  - EQ4MIX Input 3 Source */

 R2205  - EQ4MIX Input 3 Volume */

 R2206  - EQ4MIX Input 4 Source */

 R2207  - EQ4MIX Input 4 Volume */

 R2240  - DRC1LMIX Input 1 Source */

 R2241  - DRC1LMIX Input 1 Volume */

 R2242  - DRC1LMIX Input 2 Source */

 R2243  - DRC1LMIX Input 2 Volume */

 R2244  - DRC1LMIX Input 3 Source */

 R2245  - DRC1LMIX Input 3 Volume */

 R2246  - DRC1LMIX Input 4 Source */

 R2247  - DRC1LMIX Input 4 Volume */

 R2248  - DRC1RMIX Input 1 Source */

 R2249  - DRC1RMIX Input 1 Volume */

 R2250  - DRC1RMIX Input 2 Source */

 R2251  - DRC1RMIX Input 2 Volume */

 R2252  - DRC1RMIX Input 3 Source */

 R2253  - DRC1RMIX Input 3 Volume */

 R2254  - DRC1RMIX Input 4 Source */

 R2255  - DRC1RMIX Input 4 Volume */

 R2256  - DRC2LMIX Input 1 Source */

 R2257  - DRC2LMIX Input 1 Volume */

 R2258  - DRC2LMIX Input 2 Source */

 R2259  - DRC2LMIX Input 2 Volume */

 R2260  - DRC2LMIX Input 3 Source */

 R2261  - DRC2LMIX Input 3 Volume */

 R2262  - DRC2LMIX Input 4 Source */

 R2263  - DRC2LMIX Input 4 Volume */

 R2264  - DRC2RMIX Input 1 Source */

 R2265  - DRC2RMIX Input 1 Volume */

 R2266  - DRC2RMIX Input 2 Source */

 R2267  - DRC2RMIX Input 2 Volume */

 R2268  - DRC2RMIX Input 3 Source */

 R2269  - DRC2RMIX Input 3 Volume */

 R2270  - DRC2RMIX Input 4 Source */

 R2271  - DRC2RMIX Input 4 Volume */

 R2304  - HPLP1MIX Input 1 Source */

 R2305  - HPLP1MIX Input 1 Volume */

 R2306  - HPLP1MIX Input 2 Source */

 R2307  - HPLP1MIX Input 2 Volume */

 R2308  - HPLP1MIX Input 3 Source */

 R2309  - HPLP1MIX Input 3 Volume */

 R2310  - HPLP1MIX Input 4 Source */

 R2311  - HPLP1MIX Input 4 Volume */

 R2312  - HPLP2MIX Input 1 Source */

 R2313  - HPLP2MIX Input 1 Volume */

 R2314  - HPLP2MIX Input 2 Source */

 R2315  - HPLP2MIX Input 2 Volume */

 R2316  - HPLP2MIX Input 3 Source */

 R2317  - HPLP2MIX Input 3 Volume */

 R2318  - HPLP2MIX Input 4 Source */

 R2319  - HPLP2MIX Input 4 Volume */

 R2320  - HPLP3MIX Input 1 Source */

 R2321  - HPLP3MIX Input 1 Volume */

 R2322  - HPLP3MIX Input 2 Source */

 R2323  - HPLP3MIX Input 2 Volume */

 R2324  - HPLP3MIX Input 3 Source */

 R2325  - HPLP3MIX Input 3 Volume */

 R2326  - HPLP3MIX Input 4 Source */

 R2327  - HPLP3MIX Input 4 Volume */

 R2328  - HPLP4MIX Input 1 Source */

 R2329  - HPLP4MIX Input 1 Volume */

 R2330  - HPLP4MIX Input 2 Source */

 R2331  - HPLP4MIX Input 2 Volume */

 R2332  - HPLP4MIX Input 3 Source */

 R2333  - HPLP4MIX Input 3 Volume */

 R2334  - HPLP4MIX Input 4 Source */

 R2335  - HPLP4MIX Input 4 Volume */

 R2368  - DSP1LMIX Input 1 Source */

 R2369  - DSP1LMIX Input 1 Volume */

 R2370  - DSP1LMIX Input 2 Source */

 R2371  - DSP1LMIX Input 2 Volume */

 R2372  - DSP1LMIX Input 3 Source */

 R2373  - DSP1LMIX Input 3 Volume */

 R2374  - DSP1LMIX Input 4 Source */

 R2375  - DSP1LMIX Input 4 Volume */

 R2376  - DSP1RMIX Input 1 Source */

 R2377  - DSP1RMIX Input 1 Volume */

 R2378  - DSP1RMIX Input 2 Source */

 R2379  - DSP1RMIX Input 2 Volume */

 R2380  - DSP1RMIX Input 3 Source */

 R2381  - DSP1RMIX Input 3 Volume */

 R2382  - DSP1RMIX Input 4 Source */

 R2383  - DSP1RMIX Input 4 Volume */

 R2384  - DSP1AUX1MIX Input 1 Source */

 R2392  - DSP1AUX2MIX Input 1 Source */

 R2400  - DSP1AUX3MIX Input 1 Source */

 R2408  - DSP1AUX4MIX Input 1 Source */

 R2416  - DSP1AUX5MIX Input 1 Source */

 R2424  - DSP1AUX6MIX Input 1 Source */

 R2432  - DSP2LMIX Input 1 Source */

 R2433  - DSP2LMIX Input 1 Volume */

 R2434  - DSP2LMIX Input 2 Source */

 R2435  - DSP2LMIX Input 2 Volume */

 R2436  - DSP2LMIX Input 3 Source */

 R2437  - DSP2LMIX Input 3 Volume */

 R2438  - DSP2LMIX Input 4 Source */

 R2439  - DSP2LMIX Input 4 Volume */

 R2440  - DSP2RMIX Input 1 Source */

 R2441  - DSP2RMIX Input 1 Volume */

 R2442  - DSP2RMIX Input 2 Source */

 R2443  - DSP2RMIX Input 2 Volume */

 R2444  - DSP2RMIX Input 3 Source */

 R2445  - DSP2RMIX Input 3 Volume */

 R2446  - DSP2RMIX Input 4 Source */

 R2447  - DSP2RMIX Input 4 Volume */

 R2448  - DSP2AUX1MIX Input 1 Source */

 R2456  - DSP2AUX2MIX Input 1 Source */

 R2464  - DSP2AUX3MIX Input 1 Source */

 R2472  - DSP2AUX4MIX Input 1 Source */

 R2480  - DSP2AUX5MIX Input 1 Source */

 R2488  - DSP2AUX6MIX Input 1 Source */

 R2496  - DSP3LMIX Input 1 Source */

 R2497  - DSP3LMIX Input 1 Volume */

 R2498  - DSP3LMIX Input 2 Source */

 R2499  - DSP3LMIX Input 2 Volume */

 R2500  - DSP3LMIX Input 3 Source */

 R2501  - DSP3LMIX Input 3 Volume */

 R2502  - DSP3LMIX Input 4 Source */

 R2503  - DSP3LMIX Input 4 Volume */

 R2504  - DSP3RMIX Input 1 Source */

 R2505  - DSP3RMIX Input 1 Volume */

 R2506  - DSP3RMIX Input 2 Source */

 R2507  - DSP3RMIX Input 2 Volume */

 R2508  - DSP3RMIX Input 3 Source */

 R2509  - DSP3RMIX Input 3 Volume */

 R2510  - DSP3RMIX Input 4 Source */

 R2511  - DSP3RMIX Input 4 Volume */

 R2512  - DSP3AUX1MIX Input 1 Source */

 R2520  - DSP3AUX2MIX Input 1 Source */

 R2528  - DSP3AUX3MIX Input 1 Source */

 R2536  - DSP3AUX4MIX Input 1 Source */

 R2544  - DSP3AUX5MIX Input 1 Source */

 R2552  - DSP3AUX6MIX Input 1 Source */

 R2560  - DSP4LMIX Input 1 Source */

 R2561  - DSP4LMIX Input 1 Volume */

 R2562  - DSP4LMIX Input 2 Source */

 R2563  - DSP4LMIX Input 2 Volume */

 R2564  - DSP4LMIX Input 3 Source */

 R2565  - DSP4LMIX Input 3 Volume */

 R2566  - DSP4LMIX Input 4 Source */

 R2567  - DSP4LMIX Input 4 Volume */

 R2568  - DSP4RMIX Input 1 Source */

 R2569  - DSP4RMIX Input 1 Volume */

 R2570  - DSP4RMIX Input 2 Source */

 R2571  - DSP4RMIX Input 2 Volume */

 R2572  - DSP4RMIX Input 3 Source */

 R2573  - DSP4RMIX Input 3 Volume */

 R2574  - DSP4RMIX Input 4 Source */

 R2575  - DSP4RMIX Input 4 Volume */

 R2576  - DSP4AUX1MIX Input 1 Source */

 R2584  - DSP4AUX2MIX Input 1 Source */

 R2592  - DSP4AUX3MIX Input 1 Source */

 R2600  - DSP4AUX4MIX Input 1 Source */

 R2608  - DSP4AUX5MIX Input 1 Source */

 R2616  - DSP4AUX6MIX Input 1 Source */

 R2688  - ASRC1LMIX Input 1 Source */

 R2696  - ASRC1RMIX Input 1 Source */

 R2704  - ASRC2LMIX Input 1 Source */

 R2712  - ASRC2RMIX Input 1 Source */

 R2816  - ISRC1DEC1MIX Input 1 Source */

 R2824  - ISRC1DEC2MIX Input 1 Source */

 R2832  - ISRC1DEC3MIX Input 1 Source */

 R2840  - ISRC1DEC4MIX Input 1 Source */

 R2848  - ISRC1INT1MIX Input 1 Source */

 R2856  - ISRC1INT2MIX Input 1 Source */

 R2864  - ISRC1INT3MIX Input 1 Source */

 R2872  - ISRC1INT4MIX Input 1 Source */

 R2880  - ISRC2DEC1MIX Input 1 Source */

 R2888  - ISRC2DEC2MIX Input 1 Source */

 R2896  - ISRC2DEC3MIX Input 1 Source */

 R2904  - ISRC2DEC4MIX Input 1 Source */

 R2912  - ISRC2INT1MIX Input 1 Source */

 R2920  - ISRC2INT2MIX Input 1 Source */

 R2928  - ISRC2INT3MIX Input 1 Source */

 R2936  - ISRC2INT4MIX Input 1 Source */

 R2944  - ISRC3DEC1MIX Input 1 Source */

 R2952  - ISRC3DEC2MIX Input 1 Source */

 R2960  - ISRC3DEC3MIX Input 1 Source */

 R2968  - ISRC3DEC4MIX Input 1 Source */

 R2976  - ISRC3INT1MIX Input 1 Source */

 R2984  - ISRC3INT2MIX Input 1 Source */

 R2992  - ISRC3INT3MIX Input 1 Source */

 R3000  - ISRC3INT4MIX Input 1 Source */

 R3072  - GPIO1 CTRL */

 R3073  - GPIO2 CTRL */

 R3074  - GPIO3 CTRL */

 R3075  - GPIO4 CTRL */

 R3076  - GPIO5 CTRL */

 R3087  - IRQ CTRL 1 */

 R3088  - GPIO Debounce Config */

 R3096  - GP Switch 1 */

 R3104  - Misc Pad Ctrl 1 */

 R3105  - Misc Pad Ctrl 2 */

 R3106  - Misc Pad Ctrl 3 */

 R3107  - Misc Pad Ctrl 4 */

 R3108  - Misc Pad Ctrl 5 */

 R3109  - Misc Pad Ctrl 6 */

 R3120  - Misc Pad Ctrl 7 */

 R3121  - Misc Pad Ctrl 8 */

 R3122  - Misc Pad Ctrl 9 */

 R3123  - Misc Pad Ctrl 10 */

 R3124  - Misc Pad Ctrl 11 */

 R3125  - Misc Pad Ctrl 12 */

 R3126  - Misc Pad Ctrl 13 */

 R3127  - Misc Pad Ctrl 14 */

 R3128  - Misc Pad Ctrl 15 */

 R3129  - Misc Pad Ctrl 16 */

 R3130  - Misc Pad Ctrl 17 */

 R3131  - Misc Pad Ctrl 18 */

 R3336  - Interrupt Status 1 Mask */

 R3337  - Interrupt Status 2 Mask */

 R3338  - Interrupt Status 3 Mask */

 R3339  - Interrupt Status 4 Mask */

 R3340  - Interrupt Status 5 Mask */

 R3341  - Interrupt Status 6 Mask */

 R3343  - Interrupt Control */

 R3352  - IRQ2 Status 1 Mask */

 R3353  - IRQ2 Status 2 Mask */

 R3354  - IRQ2 Status 3 Mask */

 R3355  - IRQ2 Status 4 Mask */

 R3356  - IRQ2 Status 5 Mask */

 R3357  - IRQ2 Status 6 Mask */

 R3359  - IRQ2 Control */

 R3411  - AOD IRQ Mask IRQ1 */

 R3412  - AOD IRQ Mask IRQ2 */

 R3414  - Jack detect debounce */

 R3584  - FX_Ctrl1 */

 R3600  - EQ1_1 */

 R3601  - EQ1_2 */

 R3602  - EQ1_3 */

 R3603  - EQ1_4 */

 R3604  - EQ1_5 */

 R3605  - EQ1_6 */

 R3606  - EQ1_7 */

 R3607  - EQ1_8 */

 R3608  - EQ1_9 */

 R3609  - EQ1_10 */

 R3610  - EQ1_11 */

 R3611  - EQ1_12 */

 R3612  - EQ1_13 */

 R3613  - EQ1_14 */

 R3614  - EQ1_15 */

 R3615  - EQ1_16 */

 R3616  - EQ1_17 */

 R3617  - EQ1_18 */

 R3618  - EQ1_19 */

 R3619  - EQ1_20 */

 R3620  - EQ1_21 */

 R3622  - EQ2_1 */

 R3623  - EQ2_2 */

 R3624  - EQ2_3 */

 R3625  - EQ2_4 */

 R3626  - EQ2_5 */

 R3627  - EQ2_6 */

 R3628  - EQ2_7 */

 R3629  - EQ2_8 */

 R3630  - EQ2_9 */

 R3631  - EQ2_10 */

 R3632  - EQ2_11 */

 R3633  - EQ2_12 */

 R3634  - EQ2_13 */

 R3635  - EQ2_14 */

 R3636  - EQ2_15 */

 R3637  - EQ2_16 */

 R3638  - EQ2_17 */

 R3639  - EQ2_18 */

 R3640  - EQ2_19 */

 R3641  - EQ2_20 */

 R3642  - EQ2_21 */

 R3644  - EQ3_1 */

 R3645  - EQ3_2 */

 R3646  - EQ3_3 */

 R3647  - EQ3_4 */

 R3648  - EQ3_5 */

 R3649  - EQ3_6 */

 R3650  - EQ3_7 */

 R3651  - EQ3_8 */

 R3652  - EQ3_9 */

 R3653  - EQ3_10 */

 R3654  - EQ3_11 */

 R3655  - EQ3_12 */

 R3656  - EQ3_13 */

 R3657  - EQ3_14 */

 R3658  - EQ3_15 */

 R3659  - EQ3_16 */

 R3660  - EQ3_17 */

 R3661  - EQ3_18 */

 R3662  - EQ3_19 */

 R3663  - EQ3_20 */

 R3664  - EQ3_21 */

 R3666  - EQ4_1 */

 R3667  - EQ4_2 */

 R3668  - EQ4_3 */

 R3669  - EQ4_4 */

 R3670  - EQ4_5 */

 R3671  - EQ4_6 */

 R3672  - EQ4_7 */

 R3673  - EQ4_8 */

 R3674  - EQ4_9 */

 R3675  - EQ4_10 */

 R3676  - EQ4_11 */

 R3677  - EQ4_12 */

 R3678  - EQ4_13 */

 R3679  - EQ4_14 */

 R3680  - EQ4_15 */

 R3681  - EQ4_16 */

 R3682  - EQ4_17 */

 R3683  - EQ4_18 */

 R3684  - EQ4_19 */

 R3685  - EQ4_20 */

 R3686  - EQ4_21 */

 R3712  - DRC1 ctrl1 */

 R3713  - DRC1 ctrl2 */

 R3714  - DRC1 ctrl3 */

 R3715  - DRC1 ctrl4 */

 R3716  - DRC1 ctrl5 */

 R3721  - DRC2 ctrl1 */

 R3722  - DRC2 ctrl2 */

 R3723  - DRC2 ctrl3 */

 R3724  - DRC2 ctrl4 */

 R3725  - DRC2 ctrl5 */

 R3776  - HPLPF1_1 */

 R3777  - HPLPF1_2 */

 R3780  - HPLPF2_1 */

 R3781  - HPLPF2_2 */

 R3784  - HPLPF3_1 */

 R3785  - HPLPF3_2 */

 R3788  - HPLPF4_1 */

 R3789  - HPLPF4_2 */

 R3808  - ASRC_ENABLE */

 R3810  - ASRC_RATE1 */

 R3811  - ASRC_RATE2 */

 R3824  - ISRC 1 CTRL 1 */

 R3825  - ISRC 1 CTRL 2 */

 R3826  - ISRC 1 CTRL 3 */

 R3827  - ISRC 2 CTRL 1 */

 R3828  - ISRC 2 CTRL 2 */

 R3829  - ISRC 2 CTRL 3 */

 R3830  - ISRC 3 CTRL 1 */

 R3831  - ISRC 3 CTRL 2 */

 R3832  - ISRC 3 CTRL 3 */

 R3840  - Clock Control */

 R3841  - ANC_SRC */

 R3848  - ANC Coefficient */

 R3849  - ANC Coefficient */

 R3850  - ANC Coefficient */

 R3851  - ANC Coefficient */

 R3852  - ANC Coefficient */

 R3853  - ANC Coefficient */

 R3854  - ANC Coefficient */

 R3855  - ANC Coefficient */

 R3856  - ANC Coefficient */

 R3857  - ANC Coefficient */

 R3858  - ANC Coefficient */

 R3861  - FCL Filter Control */

 R3863  - FCL ADC Reformatter Control */

 R3864  - ANC Coefficient */

 R3865  - ANC Coefficient */

 R3866  - ANC Coefficient */

 R3867  - ANC Coefficient */

 R3868  - ANC Coefficient */

 R3869  - ANC Coefficient */

 R3870  - ANC Coefficient */

 R3871  - ANC Coefficient */

 R3872  - ANC Coefficient */

 R3873  - ANC Coefficient */

 R3874  - ANC Coefficient */

 R3875  - ANC Coefficient */

 R3876  - ANC Coefficient */

 R3877  - ANC Coefficient */

 R3878  - ANC Coefficient */

 R3879  - ANC Coefficient */

 R3880  - ANC Coefficient */

 R3881  - ANC Coefficient */

 R3882  - ANC Coefficient */

 R3883  - ANC Coefficient */

 R3884  - ANC Coefficient */

 R3885  - ANC Coefficient */

 R3886  - ANC Coefficient */

 R3887  - ANC Coefficient */

 R3888  - ANC Coefficient */

 R3889  - ANC Coefficient */

 R3890  - ANC Coefficient */

 R3891  - ANC Coefficient */

 R3892  - ANC Coefficient */

 R3893  - ANC Coefficient */

 R3894  - ANC Coefficient */

 R3895  - ANC Coefficient */

 R3896  - ANC Coefficient */

 R3897  - ANC Coefficient */

 R3898  - ANC Coefficient */

 R3899  - ANC Coefficient */

 R3900  - ANC Coefficient */

 R3901  - ANC Coefficient */

 R3902  - ANC Coefficient */

 R3903  - ANC Coefficient */

 R3904  - ANC Coefficient */

 R3905  - ANC Coefficient */

 R3906  - ANC Coefficient */

 R3907  - ANC Coefficient */

 R3908  - ANC Coefficient */

 R3909  - ANC Coefficient */

 R3910  - ANC Coefficient */

 R3911  - ANC Coefficient */

 R3912  - ANC Coefficient */

 R3913  - ANC Coefficient */

 R3914  - ANC Coefficient */

 R3915  - ANC Coefficient */

 R3916  - ANC Coefficient */

 R3917  - ANC Coefficient */

 R3918  - ANC Coefficient */

 R3919  - ANC Coefficient */

 R3920  - ANC Coefficient */

 R3921  - ANC Coefficient */

 R3922  - ANC Coefficient */

 R3923  - ANC Coefficient */

 R3924  - ANC Coefficient */

 R3925  - ANC Coefficient */

 R3926  - ANC Coefficient */

 R3927  - ANC Coefficient */

 R3928  - ANC Coefficient */

 R3929  - ANC Coefficient */

 R3930  - ANC Coefficient */

 R3931  - ANC Coefficient */

 R3932  - ANC Coefficient */

 R3933  - ANC Coefficient */

 R3934  - ANC Coefficient */

 R3935  - ANC Coefficient */

 R3936  - ANC Coefficient */

 R3937  - ANC Coefficient */

 R3938  - ANC Coefficient */

 R3939  - ANC Coefficient */

 R3940  - ANC Coefficient */

 R3941  - ANC Coefficient */

 R3942  - ANC Coefficient */

 R3943  - ANC Coefficient */

 R3944  - ANC Coefficient */

 R3945  - ANC Coefficient */

 R3952  - FCR Filter Control */

 R3954  - FCR ADC Reformatter Control */

 R3955  - ANC Coefficient */

 R3956  - ANC Coefficient */

 R3957  - ANC Coefficient */

 R3958  - ANC Coefficient */

 R3959  - ANC Coefficient */

 R3960  - ANC Coefficient */

 R3961  - ANC Coefficient */

 R3962  - ANC Coefficient */

 R3963  - ANC Coefficient */

 R3964  - ANC Coefficient */

 R3965  - ANC Coefficient */

 R3966  - ANC Coefficient */

 R3967  - ANC Coefficient */

 R3968  - ANC Coefficient */

 R3969  - ANC Coefficient */

 R3970  - ANC Coefficient */

 R3971  - ANC Coefficient */

 R3972  - ANC Coefficient */

 R3973  - ANC Coefficient */

 R3974  - ANC Coefficient */

 R3975  - ANC Coefficient */

 R3976  - ANC Coefficient */

 R3977  - ANC Coefficient */

 R3978  - ANC Coefficient */

 R3979  - ANC Coefficient */

 R3980  - ANC Coefficient */

 R3981  - ANC Coefficient */

 R3982  - ANC Coefficient */

 R3983  - ANC Coefficient */

 R3984  - ANC Coefficient */

 R3985  - ANC Coefficient */

 R3986  - ANC Coefficient */

 R3987  - ANC Coefficient */

 R3988  - ANC Coefficient */

 R3989  - ANC Coefficient */

 R3990  - ANC Coefficient */

 R3991  - ANC Coefficient */

 R3992  - ANC Coefficient */

 R3993  - ANC Coefficient */

 R3994  - ANC Coefficient */

 R3995  - ANC Coefficient */

 R3996  - ANC Coefficient */

 R3997  - ANC Coefficient */

 R3998  - ANC Coefficient */

 R3999  - ANC Coefficient */

 R4000  - ANC Coefficient */

 R4001  - ANC Coefficient */

 R4002  - ANC Coefficient */

 R4003  - ANC Coefficient */

 R4004  - ANC Coefficient */

 R4005  - ANC Coefficient */

 R4006  - ANC Coefficient */

 R4007  - ANC Coefficient */

 R4008  - ANC Coefficient */

 R4009  - ANC Coefficient */

 R4010  - ANC Coefficient */

 R4011  - ANC Coefficient */

 R4012  - ANC Coefficient */

 R4013  - ANC Coefficient */

 R4014  - ANC Coefficient */

 R4015  - ANC Coefficient */

 R4016  - ANC Coefficient */

 R4017  - ANC Coefficient */

 R4018  - ANC Coefficient */

 R4019  - ANC Coefficient */

 R4020  - ANC Coefficient */

 R4021  - ANC Coefficient */

 R4022  - ANC Coefficient */

 R4023  - ANC Coefficient */

 R4024  - ANC Coefficient */

 R4025  - ANC Coefficient */

 R4026  - ANC Coefficient */

 R4027  - ANC Coefficient */

 R4028  - ANC Coefficient */

 R4029  - ANC Coefficient */

 R4030  - ANC Coefficient */

 R4031  - ANC Coefficient */

 R4032  - ANC Coefficient */

 R4033  - ANC Coefficient */

 R4034  - ANC Coefficient */

 R4035  - ANC Coefficient */

 R4036  - ANC Coefficient */

 R4352  - DSP1 Control 1 */

 R4608  - DSP2 Control 1 */

 R4864  - DSP3 Control 1 */

 R5120  - DSP4 Control 1 */

/*

 * Retu/Tahvo MFD driver

 *

 * Copyright (C) 2004, 2005 Nokia Corporation

 *

 * Based on code written by Juha Yrjölä, David Weinehall and Mikko Ylinen.

 * Rewritten by Aaro Koskinen.

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License. See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the

 * GNU General Public License for more details.

 Registers */

 ASIC ID and revision */

 Bit indicating Vilma */

 Interrupt ID */

 Interrupt mask (Retu) */

 Interrupt mask (Tahvo) */

 Interrupt sources */

 Power button */

 Retu device registered for the power off. */

 Ignore power button state */

 Expire watchdog immediately */

 Wait for poweroff */

 Mask all interrupts. */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Macintosh Nubus Interface Code

 *

 *      Originally by Alan Cox

 *

 *      Mostly rewritten by David Huggins-Daines, C. Scott Ananian,

 *      and others.

 Constants */

/* This is, of course, the size in bytelanes, rather than the size in

 Globals */

/* Meaning of "bytelanes":



   The card ROM may appear on any or all bytes of each long word in

   NuBus memory.  The low 4 bits of the "map" value found in the

   format block (at the top of the slot address space, as well as at

   the top of the MacOS ROM) tells us which bytelanes, i.e. which byte

   offsets within each longword, are valid.  Thus:



   A map of 0x0f, as found in the MacOS ROM, means that all bytelanes

   are valid.



   A map of 0xf0 means that no bytelanes are valid (We pray that we

   will never encounter this, but stranger things have happened)



   A map of 0xe1 means that only the MSB of each long word is actually

   part of the card ROM.  (We hope to never encounter NuBus on a

   little-endian machine.  Again, stranger things have happened)



   A map of 0x78 means that only the LSB of each long word is valid.



   Etcetera, etcetera.  Hopefully this clears up some confusion over

 This will hold the result */

 Now, functions to read the sResource tree */

/* Each sResource entry consists of a 1-byte ID and a 3-byte data

   field.  If that data field contains an offset, then obviously we

   have to expand it from a 24-bit signed number to a 32-bit signed

 24bit negative */

	/*

	 *	Returns the first byte after the card. We then walk

	 *	backwards to get the lane register and the config

	/* Essentially, just step over the bytelanes using whatever

 And return the value */

/* These two are for pulling resource data blocks (i.e. stuff that's

 If possible, write out full buffers */

 If not, write out individual bytes */

 This is a slyly renamed version of the above */

	/* Now dereference it (the first directory is always the board

 Do this first, otherwise nubus_rewind & co are off by 4 */

 This moves nd->ptr forward */

 EOL marker, as per the Apple docs */

 Mark it as done */

 First byte is the resource ID */

 Low 3 bytes might contain data (or might not) */

 Driver interface functions, more or less like in pci.c */

/* Initialization functions - decide which slots contain stuff worth

   looking at, and print out lots and lots of information from the

 mVidParams */

 mTable */

 Actually we should probably panic if this fails */

			/* MacOS driver.  If we were NetBSD we might

			/* We will need this in order to support

			   multiple framebuffers.  It might be handy

 Ditto */

			/* Local/Private resources have their own

 This is *really* cool. */

 Should be 32x32 if my memory serves me correctly */

 These are all strings, we think */

			/* This type is always the same, and is not

			   useful except insofar as it tells us that

 WTF isn't this in the functional resources? */

 Same goes for this */

 Move to the start of the format block */

 Actually we should probably panic if this fails */

 Dump the format block for debugging purposes */

	/* rom_length is *supposed* to be the total length of the

	 * ROM.  In practice it is the "amount of ROM used to compute

	 * the CRC."  So some jokers decide to set it to zero and

	 * set the crc to zero so they don't have to do any math.

	 * See the Performa 460 ROM, for example.  Those Apple "engineers".

 Directory offset should be small and negative... */

	/*

	 *	I wonder how the CRC is meant to work -

	 *		any takers ?

	 * CSA: According to MAC docs, not all cards pass the CRC anyway,

	 * since the initial Macintosh ROM releases skipped the check.

 Set up the directory pointer */

 We're ready to rock */

	/* Each slot should have one board resource and any number of

	 * functional resources.  So we'll fill in some fields in the

	 * struct nubus_board from the board resource, then walk down

	 * the list of functional resources, spinning out a nubus_rsrc

	 * for each of them.

 We can't have this! */

		/* Resources should appear in ascending ID order. This sanity

		 * check prevents duplicate resource IDs.

		/* The last byte of the format block consists of two

		   nybbles which are "mirror images" of each other.

		/* Check that this value is actually *on* one of the

 Looks promising.  Let's put it on the list. */

 SPDX-License-Identifier: GPL-2.0



 Bus implementation for the NuBus subsystem.



 Copyright (C) 2017 Finn Thain

 SPDX-License-Identifier: GPL-2.0

/* drivers/nubus/proc.c: Proc FS interface for NuBus.



   By David Huggins-Daines <dhd@debian.org>



   Much code and many ideas from drivers/pci/proc.c:

   Copyright (c) 1997, 1998 Martin Mares <mj@atrey.karlin.mff.cuni.cz>



   This is initially based on the Zorro and PCI interfaces.  However,

   it works somewhat differently.  The intent is to provide a

   structure in /proc analogous to the structure of the NuBus ROM

   resources.



   Therefore each board function gets a directory, which may in turn

   contain subdirectories.  Each slot resource is a file.  Unrecognized

   resources are empty files, since every resource ID requires a special

   case (e.g. if the resource ID implies a directory or block, then its

   value has to be interpreted as a slot ROM pointer etc.).

/*

 * /proc/bus/nubus/devices stuff

/*

 * /proc/bus/nubus/x/ stuff

/* The PDE private data for any directory under /proc/bus/nubus/x/

 * is the bytelanes value for the board in slot x.

/* The PDE private data for a file under /proc/bus/nubus/x/ is a pointer to

 * an instance of the following structure, which gives the location and size

 * of the resource data in the slot ROM. For slot resources which hold only a

 * small integer, this integer value is stored directly and size is set to 0.

 * A NULL private data pointer indicates an unrecognized resource.

/*

 * /proc/nubus stuff

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-can-transceiver.c - phy driver for CAN transceivers

 *

 * Copyright (C) 2021 Texas Instruments Incorporated - https://www.ti.com

 *

 Power on function */

 Power off function */

 SPDX-License-Identifier: GPL-2.0 */

/*

 * Copyright (C) 2013 NVIDIA Corporation

 * Copyright (C) 2018 Cadence Design Systems Inc.

/*

 * Minimum D-PHY timings based on MIPI D-PHY specification. Derived

 * from the valid ranges specified in Section 6.9, Table 14, Page 41

 * of the D-PHY specification (v1.2).

	/*

	 * The MIPI D-PHY specification (Section 6.9, v1.2, Table 14, Page 40)

	 * contains this formula as:

	 *

	 *     T_HS-TRAIL = max(n * 8 * ui, 60 + n * 4 * ui)

	 *

	 * where n = 1 for forward-direction HS mode and n = 4 for reverse-

	 * direction HS mode. There's only one setting and this function does

	 * not parameterize on anything other that ui, so this code will

	 * assumes that reverse-direction HS mode is supported and uses n = 4.

/*

 * Validate D-PHY configuration according to MIPI D-PHY specification

 * (v1.2, Section Section 6.9 "Global Operation Timing Parameters").

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PHY driver for NXP LPC18xx/43xx internal USB OTG PHY

 *

 * Copyright (C) 2015 Joachim Eastwood <manabian@gmail.com>

 USB OTG PHY register offset and bit in CREG */

 The PHY must be clocked at 480 MHz */

 The bit in CREG is cleared to enable the PHY */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IMG Pistachio USB PHY driver

 *

 * Copyright (C) 2015 Google, Inc.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * AppliedMicro X-Gene Multi-purpose PHY driver

 *

 * Copyright (c) 2014, Applied Micro Circuits Corporation

 * Author: Loc Ho <lho@apm.com>

 *         Tuan Phan <tphan@apm.com>

 *         Suman Tripathi <stripathi@apm.com>

 *

 * The APM X-Gene PHY consists of two PLL clock macro's (CMU) and lanes.

 * The first PLL clock macro is used for internal reference clock. The second

 * PLL clock macro is used to generate the clock for the PHY. This driver

 * configures the first PLL CMU, the second PLL CMU, and programs the PHY to

 * operate according to the mode of operation. The first PLL CMU is only

 * required if internal clock is enabled.

 *

 * Logical Layer Out Of HW module units:

 *

 * -----------------

 * | Internal      |    |------|

 * | Ref PLL CMU   |----|      |     -------------    ---------

 * ------------ ----    | MUX  |-----|PHY PLL CMU|----| Serdes|

 *                      |      |     |           |    ---------

 * External Clock ------|      |     -------------

 *                      |------|

 *

 * The Ref PLL CMU CSR (Configuration System Registers) is accessed

 * indirectly from the SDS offset at 0x2000. It is only required for

 * internal reference clock.

 * The PHY PLL CMU CSR is accessed indirectly from the SDS offset at 0x0000.

 * The Serdes CSR is accessed indirectly from the SDS offset at 0x0400.

 *

 * The Ref PLL CMU can be located within the same PHY IP or outside the PHY IP

 * due to shared Ref PLL CMU. For PHY with Ref PLL CMU shared with another IP,

 * it is located outside the PHY IP. This is the case for the PHY located

 * at 0x1f23a000 (SATA Port 4/5). For such PHY, another resource is required

 * to located the SDS/Ref PLL CMU module and its clock for that IP enabled.

 *

 * Currently, this driver only supports Gen3 SATA mode with external clock.

 Max 2 lanes per a PHY unit */

 Register offset inside the PHY */

 Some default Serdes parameters */

 SATA Clock/Reset CSR */

 SDS CSR used for PHY Indirect access */

 PLL Clock Macro Unit (CMU) CSR accessing from SDS indirectly */

 PHY lane CSR accessing from SDS indirectly */

 Clock macro type */

 Clock macro is the internal reference clock */

 Clock macro is the PLL for the Serdes */

 Switch the MUX to ATA */

 Switch the MUX to SGMII */

 External differential */

 Internal differential */

 Internal single ended */

 List them for simple reference */

 Index for override parameter per lane */

 Tx speed */

 Tx freq boost and gain control */

 Tx eye tuning */

 Tx eye tuning direction */

 Tx amplitude control */

 Tx emphasis taps 1st pre-cursor */

 Tx emphasis taps 2nd pre-cursor */

 Tx emphasis taps post-cursor */

 Mode of operation */

 Input clock selection */

 PHY CSR base addr */

 Optional clock */

 Override Serdes parameters */

/*

 * For chip earlier than A3 version, enable this flag.

 * To enable, pass boot argument phy_xgene.preA3Chip=1

 Force a barrier */

 Force a barrier */

 Force a barrier */

 Set the reset sequence delay for TX ready assertion */

 Set the programmable stage delays between various enable stages */

 Configure clock type */

 Select external clock mux */

 Select CMOS as reference clock  */

 Select internal clock mux */

 Select CMOS as reference clock  */

		/*

		 * NOTE: This clock type is NOT support for controller

		 *	 whose internal clock shared in the PCIe controller

		 *

		 * Select internal clock mux

 Select CML as reference clock */

 Set VCO calibration voltage threshold */

 Set the VCO calibration counter */

 Configure PLL for calibration */

 Configure the PLL for either 100MHz or 50MHz */

 Configure the VCO */

 Disable force PLL lock */

 Setup PLL loop filter */

 Enable or disable manual calibration */

 Configure lane for 20-bits */

 Configure for SATA */

 Disable state machine bypass */

 Set VCO calibration threshold */

 Set CTLE Override and override waiting from state machine */

 Set SSC modulation value */

 Enable SSC, set vertical step and DSM value */

 Reset the PLL */

 Force VCO calibration to restart */

 Set boost control for quarter, half, and full rate */

 Set boost control value */

		/* Latch VTT value based on the termination to ground and

		 * enable TX FIFO

 Configure Tx for 20-bits */

 Set pre-emphasis first 1 and 2, and post-emphasis values */

 Set TX amplitude value */

 Configure Rx for 20-bits */

 Set CDR and LOS values and enable Rx SSC */

 Set phase adjust upper/lower limits */

 Enable Latch Off; disable SUMOS and Tx termination */

 Set period error latch to 512T and enable BWL */

 Set DFE loop preset value */

 Set Eye Monitor counter width to 12-bit */

 Set BW select tap X for DFE loop */

 Set BW select tap X for frequency adjust loop */

 Set BW select tap X for phase adjust loop */

		/*

		 * Set Rx LOS filter clock rate, sample rate, and threshold

		 * windows

 Release PHY main reset */

 Force a barrier */

		/*

		 * As per PHY design spec, the PLL reset requires a minimum

		 * of 800us.

		/*

		 * As per PHY design spec, the PLL auto calibration requires

		 * a minimum of 800us.

		/*

		 * As per PHY design spec, the PLL requires a minimum of

		 * 800us to settle.

	/*

	 * Configure the termination resister calibration

	 * The serial receive pins, RXP/RXN, have TERMination resistor

	 * that is required to be calibrated.

	/*

	 * The serial transmit pins, TXP/TXN, have Pull-UP and Pull-DOWN

	 * resistors that are required to the calibrated.

	 * Configure the pull DOWN calibration

 Configure the pull UP calibration */

 Poll the PLL calibration completion status for at least 1 ms */

		/*

		 * As per PHY design spec, PLL calibration status requires

		 * a minimum of 10us to be updated.

 Configure the PHY for operation */

 Place PHY into reset */

 Force a barrier */

 Release PHY lane from reset (active high) */

 Force a barrier */

 Release all PHY module out of reset except PHY main reset */

 Force a barrier */

 Set the operation speed */

 Configure the clock macro unit (CMU) clock type */

 Configure the clock macro */

 Enable SSC if enabled */

 Configure PHY lanes */

 Set Rx/Tx 20-bit */

 Start PLL calibration and try for three times */

 If failed, toggle the VCO power signal and start again */

 Even on failure, allow to continue any way */

/*

 * Receiver Offset Calibration:

 *

 * Calibrate the receiver signal path offset in two steps - summar and

 * latch calibrations

 Start SUMMER calibration */

	/*

	 * As per PHY design spec, the Summer calibration requires a minimum

	 * of 100us to complete.

	/*

	 * As per PHY design spec, the auto calibration requires a minimum

	 * of 100us to complete.

 Start latch calibration */

	/*

	 * As per PHY design spec, the latch calibration requires a minimum

	 * of 100us to complete.

 Configure the PHY lane for calibration */

 Reset digital Rx */

 As per PHY design spec, the reset requires a minimum of 100us. */

 Enable RX Hi-Z termination */

 Turn off DFE */

 DFE Presets to zero */

	/*

	 * Receiver Offset Calibration:

	 * Calibrate the receiver signal path offset in two steps - summar

	 * and latch calibration.

	 * Runs the "Receiver Offset Calibration multiple times to determine

	 * the average value to use.

 Start the calibration */

 Check for failure. If passed, sum them for averaging */

 Update latch manual calibration with average value */

 Update SUMMER calibration with average value */

 Disable RX Hi-Z termination */

 Turn on DFE */

 Set DFE preset */

 Setup clock properly after PHY configuration */

 HW requires an toggle of the clock */

 Compute average value */

 Does not exist, load default */

 Retrieve optional clock */

 Load override paramaters */

 Default to Gen3 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel LGM USB PHY driver

 *

 * Copyright (C) 2020 Intel Corporation.

	/*

	 * Out-of-band reset of the controller after PHY reset will cause

	 * controller malfunctioning, so we should use in-band controller

	 * reset only and leave the controller de-asserted here.

 Need to wait at least 20us after de-assert the controller */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * phy-core.c  --  Generic Phy framework.

 *

 * Copyright (C) 2013 Texas Instruments Incorporated - http://www.ti.com

 *

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

/**

 * phy_create_lookup() - allocate and register PHY/device association

 * @phy: the phy of the association

 * @con_id: connection ID string on device

 * @dev_id: the device of the association

 *

 * Creates and registers phy_lookup entry.

/**

 * phy_remove_lookup() - find and remove PHY/device association

 * @phy: the phy of the association

 * @con_id: connection ID string on device

 * @dev_id: the device of the association

 *

 * Finds and unregisters phy_lookup entry that was created with

 * phy_create_lookup().

 Override possible ret == -ENOTSUPP */

 Override possible ret == -ENOTSUPP */

 Override possible ret == -ENOTSUPP */

/**

 * phy_calibrate() - Tunes the phy hw parameters for current configuration

 * @phy: the phy returned by phy_get()

 *

 * Used to calibrate phy hardware, typically by adjusting some parameters in

 * runtime, which are otherwise lost after host controller reset and cannot

 * be applied in phy_init() or phy_power_on().

 *

 * Returns: 0 if successful, an negative error code otherwise

/**

 * phy_configure() - Changes the phy parameters

 * @phy: the phy returned by phy_get()

 * @opts: New configuration to apply

 *

 * Used to change the PHY parameters. phy_init() must have been called

 * on the phy. The configuration will be applied on the current phy

 * mode, that can be changed using phy_set_mode().

 *

 * Returns: 0 if successful, an negative error code otherwise

/**

 * phy_validate() - Checks the phy parameters

 * @phy: the phy returned by phy_get()

 * @mode: phy_mode the configuration is applicable to.

 * @submode: PHY submode the configuration is applicable to.

 * @opts: Configuration to check

 *

 * Used to check that the current set of parameters can be handled by

 * the phy. Implementations are free to tune the parameters passed as

 * arguments if needed by some implementation detail or

 * constraints. It will not change any actual configuration of the

 * PHY, so calling it as many times as deemed fit will have no side

 * effect.

 *

 * Returns: 0 if successful, an negative error code otherwise

/**

 * _of_phy_get() - lookup and obtain a reference to a phy by phandle

 * @np: device_node for which to get the phy

 * @index: the index of the phy

 *

 * Returns the phy associated with the given phandle value,

 * after getting a refcount to it or -ENODEV if there is no such phy or

 * -EPROBE_DEFER if there is a phandle to the phy, but the device is

 * not yet loaded. This function uses of_xlate call back function provided

 * while registering the phy_provider to find the phy instance.

 This phy type handled by the usb-phy subsystem for now */

/**

 * of_phy_get() - lookup and obtain a reference to a phy using a device_node.

 * @np: device_node for which to get the phy

 * @con_id: name of the phy from device's point of view

 *

 * Returns the phy driver, after getting a refcount to it; or

 * -ENODEV if there is no such phy. The caller is responsible for

 * calling phy_put() to release that count.

/**

 * of_phy_put() - release the PHY

 * @phy: the phy returned by of_phy_get()

 *

 * Releases a refcount the caller received from of_phy_get().

/**

 * phy_put() - release the PHY

 * @dev: device that wants to release this phy

 * @phy: the phy returned by phy_get()

 *

 * Releases a refcount the caller received from phy_get().

/**

 * devm_phy_put() - release the PHY

 * @dev: device that wants to release this phy

 * @phy: the phy returned by devm_phy_get()

 *

 * destroys the devres associated with this phy and invokes phy_put

 * to release the phy.

/**

 * of_phy_simple_xlate() - returns the phy instance from phy provider

 * @dev: the PHY provider device

 * @args: of_phandle_args (not used here)

 *

 * Intended to be used by phy provider for the common case where #phy-cells is

 * 0. For other cases where #phy-cells is greater than '0', the phy provider

 * should provide a custom of_xlate function that reads the *args* and returns

 * the appropriate phy.

/**

 * phy_get() - lookup and obtain a reference to a phy.

 * @dev: device that requests this phy

 * @string: the phy name as given in the dt data or the name of the controller

 * port for non-dt case

 *

 * Returns the phy driver, after getting a refcount to it; or

 * -ENODEV if there is no such phy.  The caller is responsible for

 * calling phy_put() to release that count.

/**

 * phy_optional_get() - lookup and obtain a reference to an optional phy.

 * @dev: device that requests this phy

 * @string: the phy name as given in the dt data or the name of the controller

 * port for non-dt case

 *

 * Returns the phy driver, after getting a refcount to it; or

 * NULL if there is no such phy.  The caller is responsible for

 * calling phy_put() to release that count.

/**

 * devm_phy_get() - lookup and obtain a reference to a phy.

 * @dev: device that requests this phy

 * @string: the phy name as given in the dt data or phy device name

 * for non-dt case

 *

 * Gets the phy using phy_get(), and associates a device with it using

 * devres. On driver detach, release function is invoked on the devres data,

 * then, devres data is freed.

/**

 * devm_phy_optional_get() - lookup and obtain a reference to an optional phy.

 * @dev: device that requests this phy

 * @string: the phy name as given in the dt data or phy device name

 * for non-dt case

 *

 * Gets the phy using phy_get(), and associates a device with it using

 * devres. On driver detach, release function is invoked on the devres

 * data, then, devres data is freed. This differs to devm_phy_get() in

 * that if the phy does not exist, it is not considered an error and

 * -ENODEV will not be returned. Instead the NULL phy is returned,

 * which can be passed to all other phy consumer calls.

/**

 * devm_of_phy_get() - lookup and obtain a reference to a phy.

 * @dev: device that requests this phy

 * @np: node containing the phy

 * @con_id: name of the phy from device's point of view

 *

 * Gets the phy using of_phy_get(), and associates a device with it using

 * devres. On driver detach, release function is invoked on the devres data,

 * then, devres data is freed.

/**

 * devm_of_phy_get_by_index() - lookup and obtain a reference to a phy by index.

 * @dev: device that requests this phy

 * @np: node containing the phy

 * @index: index of the phy

 *

 * Gets the phy using _of_phy_get(), then gets a refcount to it,

 * and associates a device with it using devres. On driver detach,

 * release function is invoked on the devres data,

 * then, devres data is freed.

 *

/**

 * phy_create() - create a new phy

 * @dev: device that is creating the new phy

 * @node: device node of the phy

 * @ops: function pointers for performing phy operations

 *

 * Called to create a phy using phy framework.

 phy-supply */

 calls phy_release() which frees resources */

/**

 * devm_phy_create() - create a new phy

 * @dev: device that is creating the new phy

 * @node: device node of the phy

 * @ops: function pointers for performing phy operations

 *

 * Creates a new PHY device adding it to the PHY class.

 * While at that, it also associates the device with the phy using devres.

 * On driver detach, release function is invoked on the devres data,

 * then, devres data is freed.

/**

 * phy_destroy() - destroy the phy

 * @phy: the phy to be destroyed

 *

 * Called to destroy the phy.

/**

 * devm_phy_destroy() - destroy the PHY

 * @dev: device that wants to release this phy

 * @phy: the phy returned by devm_phy_get()

 *

 * destroys the devres associated with this phy and invokes phy_destroy

 * to destroy the phy.

/**

 * __of_phy_provider_register() - create/register phy provider with the framework

 * @dev: struct device of the phy provider

 * @children: device node containing children (if different from dev->of_node)

 * @owner: the module owner containing of_xlate

 * @of_xlate: function pointer to obtain phy instance from phy provider

 *

 * Creates struct phy_provider from dev and of_xlate function pointer.

 * This is used in the case of dt boot for finding the phy instance from

 * phy provider.

 *

 * If the PHY provider doesn't nest children directly but uses a separate

 * child node to contain the individual children, the @children parameter

 * can be used to override the default. If NULL, the default (dev->of_node)

 * will be used. If non-NULL, the device node must be a child (or further

 * descendant) of dev->of_node. Otherwise an ERR_PTR()-encoded -EINVAL

 * error code is returned.

	/*

	 * If specified, the device node containing the children must itself

	 * be the provider's device node or a child (or further descendant)

	 * thereof.

/**

 * __devm_of_phy_provider_register() - create/register phy provider with the

 * framework

 * @dev: struct device of the phy provider

 * @children: device node containing children (if different from dev->of_node)

 * @owner: the module owner containing of_xlate

 * @of_xlate: function pointer to obtain phy instance from phy provider

 *

 * Creates struct phy_provider from dev and of_xlate function pointer.

 * This is used in the case of dt boot for finding the phy instance from

 * phy provider. While at that, it also associates the device with the

 * phy provider using devres. On driver detach, release function is invoked

 * on the devres data, then, devres data is freed.

/**

 * of_phy_provider_unregister() - unregister phy provider from the framework

 * @phy_provider: phy provider returned by of_phy_provider_register()

 *

 * Removes the phy_provider created using of_phy_provider_register().

/**

 * devm_of_phy_provider_unregister() - remove phy provider from the framework

 * @dev: struct device of the phy provider

 * @phy_provider: phy provider returned by of_phy_provider_register()

 *

 * destroys the devres associated with this phy provider and invokes

 * of_phy_provider_unregister to unregister the phy provider.

/**

 * phy_release() - release the phy

 * @dev: the dev member within phy

 *

 * When the last reference to the device is removed, it is called

 * from the embedded kobject as release method.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2014 Marvell Technology Group Ltd.

 *

 * Antoine Tenart <antoine.tenart@free-electrons.com>

 * Jisheng Zhang <jszhang@marvell.com>

 USB_PHY_PLL */

 USB_PHY_PLL_CONTROL */

 USB_PHY_TX_CTRL0 */

 USB_PHY_TX_CTRL1 */

 USB_PHY_TX_CTRL2 */

 USB_PHY_RX_CTRL */

 USB_PHY_ANALOG */

 Berlin 2 */

 Berlin 2CD/Q */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	phy-mvebu-sata.c: SATA Phy driver for the Marvell mvebu SoCs.

 *

 *	Copyright (C) 2013 Andrew Lunn <andrew@lunn.ch>

 Enable PLL and IVREF */

 Enable PHY */

 Disable PLL and IVREF */

 Disable PHY */

 The boot loader may of left it on. Turn it off. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * USB cluster support for Armada 375 platform.

 *

 * Copyright (C) 2014 Marvell

 *

 * Gregory CLEMENT <gregory.clement@free-electrons.com>

 *

 * Armada 375 comes with an USB2 host and device controller and an

 * USB3 controller. The USB cluster control register allows to manage

 * common features of both USB controllers.

/*

 * Only one controller can use this PHY. We shouldn't have the case

 * when two controllers want to use this PHY. But if this case occurs

 * then we provide a phy to the first one and return an error for the

 * next one. This error has also to be an error returned by

 * devm_phy_optional_get() so different from ENODEV for USB2. In the

 * USB3 case it still optional and we use ENODEV.

	/*

	 * Either the phy had never been requested and then the first

	 * usb claiming it can get it, or it had already been

	 * requested in this case, we only allow to use it with the

	 * same configuration.

 Store which phy mode is used for next test */

 end of list */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * Marvell Berlin SATA PHY driver

 *

 * Copyright (C) 2014 Marvell Technology Group Ltd.

 *

 * Antoine Ténart <antoine.tenart@free-electrons.com>

 register 0x01 */

 register 0x02 */

 register 0x23 */

 register 0x25 */

 select register */

 set bits */

 Power on PHY */

 Configure MBus */

 set PHY mode and ref freq to 25 MHz */

 set PHY up to 6 Gbps */

 set 40 bits width */

 use max pll rate */

 set Gen3 controller speed */

 Power down PHY */

 Make sure the PHY is off */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 Marvell International Ltd. All rights reserved.

 * Copyright (C) 2018,2019 Lubomir Rintel <lkundrak@v3.sk>

 USB2_PLL_REG0 */

 This is for Ax stepping */

 This is for B0 stepping */

 USB2_TX_REG0 */

 USB2_TX_REG1 */

 USB2_TX_REG2 */

 USB2_RX_REG0 */

 USB2_ANA_REG1*/

 USB2_OTG_REG0 */

	/*

	 * PLL VCO and TX Impedance Calibration Timing:

	 *

	 *                _____________________________________

	 * PU  __________|

	 *                        _____________________________

	 * VCOCAL START _________|

	 *                                 ___

	 * REG_RCAL_START ________________|   |________|_______

	 *               | 200us | 400us  | 40| 400us  | USB PHY READY

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 Marvell

 *

 * Antoine Tenart <antoine.tenart@free-electrons.com>

 Relative to priv->base */

 Relative to priv->regmap */

 0: Ethernet/SATA */

/*

 * A lane is described by the following bitfields:

 * [ 1- 0]: COMPHY polarity invertion

 * [ 2- 7]: COMPHY speed

 * [ 5-11]: COMPHY port index

 * [12-16]: COMPHY mode

 * [17]: Clock source

 * [18-20]: PCIe width (x1, x2, x4)

 SGMII 1G */

 2500BASE-X */

 SFI: 0x9 (is treated like XFI) */

 lane 0 */

 lane 1 */

 lane 2 */

 lane 3 */

 lane 4 */

 lane 5 */

 Ignore PCIe submode: it represents the width */

 Unused PHY mux value is 0x0 */

 Select baud rates and PLLs */

 reset */

 de-assert reset */

 wait until clocks are ready */

 exlicitly disable 40B, the bits isn't clear on reset */

 refclk selection */

 power and pll selection */

 SERDES external config */

 check rx/tx pll */

 rx init */

 check rx */

 Speed divider */

 DFE resolution */

 Impedance calibration */

 rx training timer */

 tx train peak to peak hold */

 preset coeff */

 External rx regulator */

 digital reset */

 Try SMC flow first */

 Fallback to Linux's implementation */

 PCIe submode represents the width */

 Fallback to Linux's implementation */

	/*

	 * Ignore error if clocks have not been initialized properly for DT

	 * compatibility reasons.

	/*

	 * Hack to retrieve a physical offset relative to this CP that will be

	 * given to the firmware

		/*

		 * All modes are supported in this driver so we could call

		 * mvebu_comphy_power_off(phy) here to avoid relying on the

		 * bootloader/firmware configuration, but for compatibility

		 * reasons we cannot de-configure the COMPHY without being sure

		 * that the firmware is up-to-date and fully-featured.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Linaro, Ltd.

 * Rob Herring <robh@kernel.org>

 *

 * Based on vendor driver:

 * Copyright (C) 2013 Marvell Inc.

 * Author: Chao Xie <xiechao.mail@gmail.com>

 Set reference clock */

 Turn on PLL */

 Make sure PHY PLL is locked */

 Avoid SE0 state when resume for some device will take it as reset */

 Enable HSIC PHY */

	/*

	 *  Calibration Timing

	 *		   ____________________________

	 *  CAL START   ___|

	 *			   ____________________

	 *  CAL_DONE    ___________|

	 *		   | 400us |

 Make sure PHY Calibration is ready */

 Waiting for HSIC connect int*/

 Turn off PLL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Linaro, Ltd.

 * Rob Herring <robh@kernel.org>

 *

 * Based on vendor driver:

 * Copyright (C) 2013 Marvell Inc.

 * Author: Chao Xie <xiechao.mail@gmail.com>

 USB PXA1928 PHY mapping */

 PHY_28NM_PLL_REG0 */

 PHY_28NM_PLL_REG1 */

 PHY_28NM_CAL_REG */

 PHY_28NM_TX_REG0 */

 PHY_28NM_RX_REG0 */

 PHY_28NM_RX_REG1 */

 PHY_28NM_DIG_REG0 */

 PHY_28NM_OTG_REG */

 PHY_28NM_PLL_REG0 */

 PHY_28NM_PLL_REG1 */

 PHY_28NM_TX_REG0 */

 PHY_28NM_RX_REG0 */

 PHY_28NM_DIG_REG0 */

 PHY_28NM_OTG_REG */

	/*

	 *  Calibration Timing

	 *		   ____________________________

	 *  CAL START   ___|

	 *			   ____________________

	 *  CAL_DONE    ___________|

	 *		   | 400us |

 Make sure PHY Calibration is ready */

 Make sure PHY PLL is ready */

 power down PHY Analog part */

 power down PHY OTG part */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Marvell

 *

 * Authors:

 *   Konstantin Porotchkin <kostap@marvell.com>

 *

 * Marvell CP110 UTMI PHY driver

 CP110 UTMI register macro definetions */

/**

 * struct mvebu_cp110_utmi - PHY driver data

 *

 * @regs: PHY registers

 * @syscom: Regmap with system controller registers

 * @dev: device driver handle

 * @caps: PHY capabilities

/**

 * struct mvebu_cp110_utmi_port - PHY port data

 *

 * @priv: PHY driver data

 * @id: PHY port ID

 * @dr_mode: PHY connection: USB_DR_MODE_HOST or USB_DR_MODE_PERIPHERAL

	/*

	 * Setup PLL.

	 * The reference clock is the frequency of quartz resonator

	 * connected to pins REFCLK_XIN and REFCLK_XOUT of the SoC.

	 * Register init values are matching the 40MHz default clock.

	 * The crystal used for all platform boards is now 25MHz.

	 * See the functional specification for details.

 Impedance Calibration Threshold Setting */

 Set LS TX driver strength coarse control */

 Disable SQ and enable analog squelch detect */

	/*

	 * Set External squelch calibration number and

	 * enable the External squelch calibration

	/*

	 * Set Control VDAT Reference Voltage - 0.325V and

	 * Control VSRC Reference Voltage - 0.6V

 Power down UTMI PHY port */

 skip PLL shutdown if there are active UTMI PHY ports */

 PLL Power down if all UTMI PHYs are down */

 It is necessary to power off UTMI before configuration */

	/*

	 * If UTMI port is connected to USB Device controller,

	 * configure the USB MUX prior to UTMI PHY initialization.

	 * The single USB device controller can be connected

	 * to UTMI0 or to UTMI1 PHY port, but not to both.

 Set Test suspendm mode and enable Test UTMI select */

 Wait for UTMI power down */

 PHY port setup first */

 Power UP UTMI PHY */

 Disable Test UTMI select */

 Wait for impedance calibration */

 Wait for PLL calibration */

 Wait for PLL ready */

 PLL Power up */

 Get system controller region */

 Get UTMI memory region */

 Retrieve PHY capabilities */

 Instantiate the PHY */

 Ensure the PHY is powered off */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (C) 2020 Lubomir Rintel <lkundrak@v3.sk>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Marvell

 *

 * Authors:

 *   Igal Liberman <igall@marvell.com>

 *   Miquèl Raynal <miquel.raynal@bootlin.com>

 *

 * Marvell A3700 UTMI PHY driver

 Armada 3700 UTMI PHY registers */

 Armada 3700 USB miscellaneous registers */

/**

 * struct mvebu_a3700_utmi_caps - PHY capabilities

 *

 * @usb32: Flag indicating which PHY is in use (impacts the register map):

 *           - The UTMI PHY wired to the USB3/USB2 controller (otg)

 *           - The UTMI PHY wired to the USB2 controller (host only)

 * @ops: PHY operations

/**

 * struct mvebu_a3700_utmi - PHY driver data

 *

 * @regs: PHY registers

 * @usb_misc: Regmap with USB miscellaneous registers including PHY ones

 * @caps: PHY capabilities

 * @phy: PHY handle

	/*

	 * Setup PLL. 40MHz clock used to be the default, being 25MHz now.

	 * See "PLL Settings for Typical REFCLK" table.

 Enable PHY pull up and disable USB2 suspend */

 Power up OTG module */

 Disable PHY charger detection */

 Disable PHY DP/DM pull-down (used for device mode) */

 Wait for PLL calibration */

 Wait for impedance calibration */

 Wait for squelch calibration */

 Wait for PLL to be locked */

 Disable PHY pull-up and enable USB2 suspend */

 Power down OTG module */

 Get UTMI memory region */

 Get miscellaneous Host/PHY region */

 Retrieve PHY capabilities */

 Instantiate the PHY */

 Ensure the PHY is powered off */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2011 Marvell International Ltd. All rights reserved.

 * Copyright (C) 2018 Lubomir Rintel <lkundrak@v3.sk>

 phy regs */

 For UTMICTRL Register */

 pxa168 */

 For UTMI_PLL Register */

 For UTMI_TX Register */

 For UTMI_RX Register */

/*****************************************************************************

 * The registers read/write routines

 Initialize the USB PHY power */

 UTMI_PLL settings */

 UTMI_TX */

 UTMI_RX */

 UTMI_IVREF */

		/*

		 * fixing Microsoft Altair board interface with NEC hub issue -

		 * Set UTMI_IVREF from 0x4a3 to 0x4bf

 toggle VCOCAL_START bit of UTMI_PLL */

 toggle REG_RCAL_START bit of UTMI_TX */

 Make sure PHY PLL is ready */

 Turn on UTMI PHY OTG extension */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Marvell

 *

 * Authors:

 *   Evan Wang <xswang@marvell.com>

 *   Miquèl Raynal <miquel.raynal@bootlin.com>

 *

 * Structure inspired from phy-mvebu-cp110-comphy.c written by Antoine Tenart.

 * SMC call initial support done by Grzegorz Jaszczyk.

 COMPHY Fast SMC function identifiers */

 SGMII 1G */

 2500BASE-X */

 lane 0 */

 lane 1 */

 lane 2 */

 Unused PHY mux value is 0x0 */

 Just remember the mode, ->power_on() will do the real setup */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Russell King, Deep Blue Solutions Ltd.

 *

 * Partly derived from CP110 comphy driver by Antoine Tenart

 * <antoine.tenart@bootlin.com>

/*

 * We only support changing the speed for comphys configured for GBE.

 * Since that is all we do, we only poll for PLL ready status.

 Optional */

 SPDX-License-Identifier: GPL-2.0

/*

 * Meson G12A USB2 PHY driver

 *

 * Copyright (C) 2017 Martin Blumenstingl <martin.blumenstingl@googlemail.com>

 * Copyright (C) 2017 Amlogic, Inc. All rights reserved

 * Copyright (C) 2019 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 usb2_otg_aca_en == 0 */

 PLL Setup : 24MHz * 20 / 1 = 480MHz */

 UnReset PLL */

 PHY Tuning */

 Analog Settings */

 Tuning Disconnect Threshold */

 Analog Settings */

 set_mode is not needed, mode setting is handled via the UTMI bus */

 Sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Amlogic G12A USB3 + PCIE Combo PHY driver

 *

 * Copyright (C) 2017 Amlogic, Inc. All rights reserved

 * Copyright (C) 2019 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

 Switch PHY to USB3 */

 TODO figure out how to handle when PCIe was set in the bootloader */

	/*

	 * WORKAROUND: There is SSPHY suspend bug due to

	 * which USB enumerates

	 * in HS mode instead of SS mode. Workaround it by asserting

	 * LANE0.TX_ALT_BLOCK.EN_ALT_BUS to enable TX to use alt bus

	 * mode

	/*

	 * Fix RX Equalization setting as follows

	 * LANE0.RX_OVRD_IN_HI. RX_EQ_EN set to 0

	 * LANE0.RX_OVRD_IN_HI.RX_EQ_EN_OVRD set to 1

	 * LANE0.RX_OVRD_IN_HI.RX_EQ set to 3

	 * LANE0.RX_OVRD_IN_HI.RX_EQ_OVRD set to 1

	/*

	 * Set EQ and TX launch amplitudes as follows

	 * LANE0.TX_OVRD_DRV_LO.PREEMPH set to 22

	 * LANE0.TX_OVRD_DRV_LO.AMPLITUDE set to 127

	 * LANE0.TX_OVRD_DRV_LO.EN set to 1.

 MPLL_LOOP_CTL.PROP_CNTRL = 8 */

 SPDX-License-Identifier: GPL-2.0

/*

 * Amlogic AXG PCIE PHY driver

 *

 * Copyright (C) 2020 Remi Pommarel <repk@triplefau.lt>

 SPDX-License-Identifier: GPL-2.0

/*

 * Amlogic AXG MIPI + PCIE analog PHY driver

 *

 * Copyright (C) 2019 Remi Pommarel <repk@triplefau.lt>

 If PHY was already powered on, setup the DSI analog part */

 If reconfiguring, disable & reconfigure */

 Get the hhi system controller node */

 SPDX-License-Identifier: GPL-2.0

/*

 * Meson AXG MIPI DPHY driver

 *

 * Copyright (C) 2018 Amlogic, Inc. All rights reserved

 * Copyright (C) 2020 BayLibre, SAS

 * Author: Neil Armstrong <narmstrong@baylibre.com>

/* [31] soft reset for the phy.

 *		1: reset. 0: dessert the reset.

 * [30] clock lane soft reset.

 * [29] data byte lane 3 soft reset.

 * [28] data byte lane 2 soft reset.

 * [27] data byte lane 1 soft reset.

 * [26] data byte lane 0 soft reset.

 * [25] mipi dsi pll clock selection.

 *		1:  clock from fixed 850Mhz clock source. 0: from VID2 PLL.

 * [12] mipi HSbyteclk enable.

 * [11] mipi divider clk selection.

 *		1: select the mipi DDRCLKHS from clock divider.

 *		0: from PLL clock.

 * [10] mipi clock divider control.

 *		1: /4. 0: /2.

 * [9]  mipi divider output enable.

 * [8]  mipi divider counter enable.

 * [7]  PLL clock enable.

 * [5]  LPDT data endian.

 *		1 = transfer the high bit first. 0 : transfer the low bit first.

 * [4]  HS data endian.

 * [3]  force data byte lane in stop mode.

 * [2]  force data byte lane 0 in receiver mode.

 * [1]  write 1 to sync the txclkesc input. the internal logic have to

 *	use txclkesc to decide Txvalid and Txready.

 * [0]  enalbe the MIPI DPHY TxDDRClk.

/* [31] clk lane tx_hs_en control selection.

 *		1: from register. 0: use clk lane state machine.

 * [30] register bit for clock lane tx_hs_en.

 * [29] clk lane tx_lp_en contrl selection.

 *		1: from register. 0: from clk lane state machine.

 * [28] register bit for clock lane tx_lp_en.

 * [27] chan0 tx_hs_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [26] register bit for chan0 tx_hs_en.

 * [25] chan0 tx_lp_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [24] register bit from chan0 tx_lp_en.

 * [23] chan0 rx_lp_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [22] register bit from chan0 rx_lp_en.

 * [21] chan0 contention detection enable control selection.

 *		1: from register. 0: from chan0 state machine.

 * [20] register bit from chan0 contention dectection enable.

 * [19] chan1 tx_hs_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [18] register bit for chan1 tx_hs_en.

 * [17] chan1 tx_lp_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [16] register bit from chan1 tx_lp_en.

 * [15] chan2 tx_hs_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [14] register bit for chan2 tx_hs_en.

 * [13] chan2 tx_lp_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [12] register bit from chan2 tx_lp_en.

 * [11] chan3 tx_hs_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [10] register bit for chan3 tx_hs_en.

 * [9]  chan3 tx_lp_en control selection.

 *		1: from register. 0: from chan0 state machine.

 * [8]  register bit from chan3 tx_lp_en.

 * [4]  clk chan power down. this bit is also used as the power down

 *	of the whole MIPI_DSI_PHY.

 * [3]  chan3 power down.

 * [2]  chan2 power down.

 * [1]  chan1 power down.

 * [0]  chan0 power down.

/* [24]   rx turn watch dog triggered.

 * [23]   rx esc watchdog  triggered.

 * [22]   mbias ready.

 * [21]   txclkesc  synced and ready.

 * [20:17] clk lane state. {mbias_ready, tx_stop, tx_ulps, tx_hs_active}

 * [16:13] chan3 state{0, tx_stop, tx_ulps, tx_hs_active}

 * [12:9]  chan2 state.{0, tx_stop, tx_ulps, tx_hs_active}

 * [8:5]   chan1 state. {0, tx_stop, tx_ulps, tx_hs_active}

 * [4:0]   chan0 state. {TX_STOP, tx_ULPS, hs_active, direction, rxulpsesc}

/* [31:24] TCLK_PREPARE.

 * [23:16] TCLK_ZERO.

 * [15:8]  TCLK_POST.

 * [7:0]   TCLK_TRAIL.

/* [31:24] THS_PREPARE.

 * [23:16] THS_ZERO.

 * [15:8]  THS_TRAIL.

 * [7:0]   THS_EXIT.

/* [31:24] tTA_GET.

 * [23:16] tTA_GO.

 * [15:8]  tTA_SURE.

 * [7:0]   tLPX.

 wait time to  MIPI DIS analog ready. */

 TINIT. */

 TWAKEUP. */

/* when in RxULPS check state, after the the logic enable the analog,

 *	how long we should wait to check the lP state .

 Watchdog for RX low power state no finished. */

/* tMBIAS,  after send power up signals to analog,

 *	how long we should wait for analog powered up.

/* [31:8]  reserved for future.

 * [7:0]   tCLK_PRE.

 watchdog for turn around waiting time. */

/* When in RxULPS state, how frequency we should to check

 *	if the TX side out of ULPS state.

 enable phy clock */

 enable the DSI PLL clock . */

 enable pll clock which connected to DDR clock path */

 enable the clock divider counter */

 enable the divider clock out */

 enable the byte clock generation. */

 Calculate lanebyteclk period in ps */

 Powerup the analog circuit */

 Trigger a sync active for esc_clk */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Meson GXL and GXM USB2 PHY driver

 *

 * Copyright (C) 2017 Martin Blumenstingl <martin.blumenstingl@googlemail.com>

 bits [31:27] are read-only */

 bits [31:14] are read-only */

 reset the PHY and wait until settings are stabilized */

 power off the PHY by putting it into reset mode */

 power on the PHY by taking it out of reset mode */

 start in host mode */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Meson8, Meson8b and GXBB USB2 PHY driver

 *

 * Copyright (C) 2016 Martin Blumenstingl <martin.blumenstingl@googlemail.com>

 bits [31:26], [24:21] and [15:3] seem to be read-only */

 reset the PHY */

 power off the PHY by putting it into reset mode */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Broadcom SATA3 AHCI Controller PHY Driver

 *

 * Copyright (C) 2016 Broadcom

 Register offset between PHYs in PCB space */

/* The older SATA PHY registers duplicated per port registers within the map,

 * rather than having a separate map per port.

 Register offset between PHYs in PHY control space */

 These defaults were characterized by H/W group */

 override the TX spread spectrum setting */

 set fixed min freq */

 set fixed max freq depending on SSC config */

 Reduce CP tail current to 1/16th of its default value */

 Turn off CP tail current boost */

 Set a specific AEQ equalizer value */

 Set RX PPM val center frequency */

 Set proportional loop bandwith Gen1/2/3 */

 Set CDR integral loop acquisition bandwidth for Gen1/2/3 */

 Set CDR integral loop locking bandwidth to 1 for Gen 1/2/3 */

 Set no guard band and clamp CDR */

 Turn on/off SSC */

 NS2 SATA PLL1 defaults were characterized by H/W group */

 Configure OOB control */

 Configure PHY PLL register bank 1 */

 Configure PHY BLOCK0 register bank */

 Set oob_clk_sel to refclk/2 */

 Strobe PHY reset using PHY control register */

 Wait for PHY PLL lock by polling pll_lock bit */

 PLL did not lock; give up */

 Configure OOB control */

 Wait for pll_seq_done bit */

 PLL did not lock; give up */

 SR PHY PLL0 registers */

 SR PHY PLL1 registers */

 Configure PHY PLL register bank 1 */

 Configure PHY PLL register bank 0 */

 Wait for PHY PLL lock by polling pll_lock bit */

 PLL did not lock; give up */

 Invert Tx polarity */

 Configure OOB control to handle 100MHz reference clock */

 Acquire PLL lock */

 PLL did not lock; give up */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2016-2018 Broadcom

 we have up to 8 PAXB based RC. The 9th one is always PAXC */

/**

 * struct sr_pcie_phy - Stingray PCIe PHY

 *

 * @core: pointer to the Stingray PCIe PHY core control

 * @index: PHY index

 * @phy: pointer to the kernel PHY device

/**

 * struct sr_pcie_phy_core - Stingray PCIe PHY core control

 *

 * @dev: pointer to device

 * @base: base register of PCIe SS

 * @cdru: regmap to the CDRU device

 * @mhb: regmap to the MHB device

 * @pipemux: pipemuex strap

 * @phys: array of PCIe PHYs

/*

 * PCIe PIPEMUX lookup table

 *

 * Each array index represents a PIPEMUX strap setting

 * The array element represents a bitmap where a set bit means the PCIe

 * core and associated serdes has been enabled as RC and is available for use

 PIPEMUX = 0, EP 1x16 */

 PIPEMUX = 1, EP 1x8 + RC 1x8, core 7 */

 PIPEMUX = 2, EP 4x4 */

 PIPEMUX = 3, RC 2x8, cores 0, 7 */

 PIPEMUX = 4, RC 4x4, cores 0, 1, 6, 7 */

 PIPEMUX = 5, RC 8x2, all 8 cores */

 PIPEMUX = 6, RC 3x4 + 2x2, cores 0, 2, 3, 6, 7 */

 PIPEMUX = 7, RC 1x4 + 6x2, cores 0, 2, 3, 4, 5, 6, 7 */

 PIPEMUX = 8, EP 1x8 + RC 4x2, cores 4, 5, 6, 7 */

 PIPEMUX = 9, EP 1x8 + RC 2x4, cores 6, 7 */

 PIPEMUX = 10, EP 2x4 + RC 2x4, cores 1, 6 */

 PIPEMUX = 11, EP 2x4 + RC 4x2, cores 2, 3, 4, 5 */

 PIPEMUX = 12, EP 1x4 + RC 6x2, cores 2, 3, 4, 5, 6, 7 */

 PIPEMUX = 13, RC 2x4 + RC 1x4 + 2x2, cores 2, 3, 6 */

/*

 * Return true if the strap setting is valid

/*

 * Read the PCIe PIPEMUX from strap

	/*

	 * Read PIPEMUX configuration register to determine the pipemux setting

	 *

	 * In the case when the value indicates using HW strap, fall back to

	 * use HW strap

/*

 * Given a PIPEMUX strap and PCIe core index, this function returns true if the

 * PCIe core needs to be enabled

	/*

	 * Check whether this PHY is for root complex or not. If yes, return

	 * zero so the host driver can proceed to enumeration. If not, return

	 * an error and that will force the host driver to bail out

 read the PCIe PIPEMUX strap setting */

/*

 * Copyright (C) 2016 Broadcom

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 select the AFE 100MHz block page */

 set the 100 MHz reference clock amplitude to 2.05 v */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2018, Broadcom */

/*

 * This module contains USB PHY initialization for power up and S3 resume

 * for newer Synopsys based USB hardware first used on the bcm7216.

 Register definitions for syscon piarbctl registers */

 Register definitions for the USB CTRL block */

 Register definitions for the USB_PHY block in 7211b0 */

/* Register definitions for the MDIO registers in the DWC2 block of

 * the 7211b0.

 * NOTE: The PHY's MDIO registers are only accessible through the

 * legacy DesignWare USB controller even though it's not being used.

 Register definitions for the BDC EC block in 7211b0 */

 5-bit address */

 5-bit address */

 select bank */

 Set the eye */

 Assert reset */

 De-assert reset */

 override ipp strap pin (if it exits) */

 Override the default OC and PP polarity */

	/*

	 * If we're changing IPP, make sure power is off long enough

	 * to turn off any connected devices.

 Switch from legacy USB OTG controller to new STB USB controller */

 1 millisecond - for USB clocks to settle down */

 undo possible suspend settings */

 temporarily enable FSM so PHY comes up properly */

 Init the PHY */

 wait for lock */

 Set the PHY_MODE */

	/*

	 * The BDC controller will get occasional failures with

	 * the default "Read Transaction Size" of 6 (1024 bytes).

	 * Set it to 4 (256 bytes).

	/*

	 * Disable FSM, otherwise the PHY will auto suspend when no

	 * device is connected and will be reset on resume.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * phy-brcm-usb-init.c - Broadcom USB Phy chip specific init functions

 *

 * Copyright (C) 2014-2017 Broadcom

/*

 * This module contains USB PHY initialization for power up and S3 resume

 Register definitions for the USB CTRL block */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 opt */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 option */

 Register definitions for the XHCI EC block */

 default */

 3390B0 */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7250b0 */

 USB_CTRL_SETUP_STRAP_IPP_SEL_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_USB_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_DEVICE_CTL1_PORT_MODE_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7271a0 */

 USB_CTRL_SETUP_SCB1_EN_MASK */

 USB_CTRL_SETUP_SCB2_EN_MASK */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7364a0 */

 USB_CTRL_SETUP_STRAP_IPP_SEL_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_USB_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_DEVICE_CTL1_PORT_MODE_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7366c0 */

 USB_CTRL_SETUP_STRAP_IPP_SEL_MASK */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_DEVICE_CTL1_PORT_MODE_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 74371A0 */

 USB_CTRL_SETUP_STRAP_IPP_SEL_MASK */

 USB_CTRL_SETUP_OC3_DISABLE_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_USB_PWRDN_MASK */

 USB_CTRL_USB_DEVICE_CTL1_PORT_MODE_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_USB_PM_USB20_HC_RESETB_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7439B0 */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7445d0 */

 USB_CTRL_SETUP_STRAP_IPP_SEL_MASK */

 USB_CTRL_USB_PM_BDC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB_PM_USB_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_USB_DEVICE_CTL1_PORT_MODE_MASK */

 USB_CTRL_USB_PM_SOFT_RESET_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7260a0 */

 USB_CTRL_SETUP_SCB1_EN_MASK */

 USB_CTRL_SETUP_SCB2_EN_MASK */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 7278a0 */

 USB_CTRL_SETUP_SCB1_EN_MASK */

 USB_CTRL_SETUP_SCB2_EN_MASK */

USB_CTRL_SETUP_SS_EHCI64BIT_EN_MASK */

 USB_CTRL_PLL_CTL_PLL_IDDQ_PWRDN_MASK */

 USB_CTRL_USB30_CTL1_XHC_SOFT_RESETB_MASK */

 USB_CTRL_USB30_CTL1_USB3_IOC_MASK */

 USB_CTRL_USB30_CTL1_USB3_IPP_MASK */

 USB_CTRL_SETUP_CC_DRD_MODE_ENABLE_MASK */

 USB_CTRL_SETUP_STRAP_CC_DRD_MODE_ENABLE_SEL_MASK */

 USB_CTRL_USB_PM_USB20_HC_RESETB_MASK */

 USB_CTRL_SETUP ENDIAN bits */

 wait for the 60MHz parallel to serial shifter */

 wait for the 60MHz parallel to serial shifter */

 wait for the 60MHz parallel to serial shifter */

 wait for the 60MHz parallel to serial shifter */

 first disable FSM but also leave it that way */

 to allow normal suspend/resume */

 reset USB 2.0 PLL */

 PLL reset period */

 Give PLL enough time to lock */

 Increase USB 2.0 TX level to meet spec requirement */

 Set correct window for PLL lock detect */

 Re-enable USB 3.0 pipe reset */

 Set correct default for sigdet */

 Set correct default for SKIP align */

 Let EQ freeze after TSEQ */

	/*

	 * On newer B53 based SoC's, the reference clock for the

	 * 3.0 PLL has been changed from 50MHz to 54MHz so the

	 * PLL needs to be reprogrammed.

	 * See SWLINUX-4006.

	 *

	 * On the 7364C0, the reference clock for the

	 * 3.0 PLL has been changed from 50MHz to 54MHz to

	 * work around a MOCA issue.

	 * See SWLINUX-4169.

 set USB 3.0 PLL to accept 54Mhz reference clock */

 both ports */

 restart PLL sequence */

 Give PLL enough time to lock */

 Enable USB 3.0 TX spread spectrum */

	/* Currently, USB 3.0 SSC is enabled via port 0 MDIO registers,

	 * which should have been adequate. However, due to a bug in the

	 * USB 3.0 PHY, it must be enabled via both ports (HWUSB3DVT-26).

	/*

	 * This is a workaround for HW7445-1869 where a DMA write ends up

	 * doing a read pre-fetch after the end of the DMA buffer. This

	 * causes a problem when the DMA buffer is at the end of physical

	 * memory, causing the pre-fetch read to access non-existent memory,

	 * and the chip bondout has MEMC2 disabled. When the pre-fetch read

	 * tries to use the disabled MEMC2, it hangs the bus. The workaround

	 * is to disable MEMC2 access in the usb controller which avoids

	 * the hang.

 set cfg_pick_ss_lock */

 Reset USB 3.0 PHY for workaround to take effect */

 Assert reset */

 De-assert reset */

/*

 * Return the best map table family. The order is:

 *   - exact match of chip and major rev

 *   - exact match of chip and closest older major rev

 *   - default chip/rev.

 * NOTE: The minor rev is always ignored.

 If no match, return the default family */

	/* Starting with the 7445d0, there are no longer separate 3.0

	 * versions of IOC and IPP.

 Never use the strap, it's going away. */

 override ipp strap pin (if it exits) */

 Override the default OC and PP polarity */

	/*

	 * If we're changing IPP, make sure power is off long enough

	 * to turn off any connected devices.

 Clear any pending wake conditions */

 Take USB out of power down */

 1 millisecond - for USB clocks to settle down */

 1 millisecond - for USB clocks to settle down */

		/*

		 * HW7439-637: 7439a0 and its derivatives do not have large

		 * enough descriptor storage for this.

 Block auto PLL suspend by USB2 PHY (Sasi) */

 Suppress overcurrent indication from USB30 ports for A0 */

	/*

	 * Make sure the the second and third memory controller

	 * interfaces are enabled if they exist.

		/*

		 * Don't enable this so the memory controller doesn't read

		 * into memory holes. NOTE: This bit is low true on 7366C0.

 Setup the endian bits */

 Enable LS keep alive fix for certain keyboards */

		/*

		 * Make the burst size 512 bytes to fix a hardware bug

		 * on the 7255a0. See HW7255-24.

 1 millisecond - for USB clocks to settle down */

		/*

		 * The PHY3_SOFT_RESETB bits default to the wrong state.

	/*

	 * Kick start USB3 PHY

	 * Make sure it's low to insure a rising edge.

/*

 * Copyright (C) 2017 Broadcom

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 Disable Host and Device Mode */

 Bring PHY and PHY_PLL out of Reset */

 port over current Polarity */

 Host connected */

 Disconnected */

 Device connected */

 create extcon */

 Shutdown all ports. They can be powered up as required */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * BCM6328 USBH PHY Controller Driver

 *

 * Copyright (C) 2020 Álvaro Fernández Rojas <noltari@gmail.com>

 * Copyright (C) 2015 Simon Arlott

 *

 * Derived from bcm963xx_4.12L.06B_consumer/kernel/linux/arch/mips/bcm963xx/setup.c:

 * Copyright (C) 2002 Broadcom Corporation

 *

 * Derived from OpenWrt patches:

 * Copyright (C) 2013 Jonas Gorski <jonas.gorski@gmail.com>

 * Copyright (C) 2013 Florian Fainelli <f.fainelli@gmail.com>

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 USBH control register offsets */

 Registers */

 PLLC bits to set/clear for power on */

 Setup bits to set/clear for power on */

 Swap Control bits to set */

 Test Port Control value to set if non-zero */

 USB Sim Control bits to set */

 UTMI Control 1 bits to set */

	/*

	 * The magic value comes for the original vendor BSP

	 * and is needed for USB to work. Datasheet does not

	 * help, so the magic value is used as-is.

 Configure to work in native CPU endian */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * phy-brcm-usb.c - Broadcom USB Phy Driver

 *

 * Copyright (C) 2015-2017 Broadcom

 serialize phy init */

	/*

	 * Use a lock to make sure a second caller waits until

	 * the base phy is inited before using it.

 If both xhci and eohci are gone, reset everything else */

	/*

	 * values 0 and 1 are for backward compatibility with

	 * device tree nodes from older bootloaders.

 sentinel */ }

 Older DT nodes have ctrl and optional xhci_ec by index only */

 XHCI_EC registers are optional */

 make sure invert settings are correct */

	/*

	 * Create sysfs entries for mode.

	 * Remove "dual_select" attribute if not in dual mode

 Get piarbctl syscon if it exists */

 start with everything off */

		/*

		 * Handle the clocks unless needed for wake. This has

		 * to work for both older XHCI->3.0-clks, EOHCI->2.0-clks

		 * and newer XHCI->2.0-clks/3.0-clks.

	/*

	 * Initialize anything that was previously initialized.

	 * Uninitialize anything that wasn't previously initialized.

 CONFIG_PM_SLEEP */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2016-2018 Broadcom

 USB PHY registers */

 Set pctl with mode and soft reset */

 Maximum timeout for PLL reset done */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * phy-bcm-kona-usb2.c - Broadcom Kona USB2 Phy Driver

 *

 * Copyright (C) 2013 Linaro Limited

 * Matt Porter <mporter@linaro.org>

 Configure and power PHY */

 Soft reset PHY */

 Reset needs to be asserted for 2ms */

 The Kona PHY supports an 8-bit wide UTMI interface */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Broadcom Northstar USB 3.0 PHY Driver

 *

 * Copyright (C) 2016 Rafał Miłecki <rafal@milecki.pl>

 * Copyright (C) 2016 Broadcom

 *

 * All magic values used for initialization (and related comments) were obtained

 * from Broadcom's SDK:

 * Copyright (c) Broadcom Corp, 2012

 Registers of PLL30 block */

 Registers of TX PMD block */

 Registers of PIPE block */

 USB3 PLL Block */

 Assert Ana_Pllseq start */

 Assert CML Divider ratio to 26 */

 Asserting PLL Reset */

 Deaaserting PLL Reset */

 Deasserting USB3 system reset */

 PLL frequency monitor enable */

 PIPE Block */

 CMPMAX & CMPMINTH setting */

 DEGLITCH MIN & MAX setting */

 TXPMD block */

 Enabling SSC */

 PLL30 block */

 Enable SSC */

 Deasserting USB3 system reset */

 Perform USB3 system soft reset */

/**************************************************

 * MDIO driver code

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Broadcom Northstar USB 2.0 PHY Driver

 *

 * Copyright (C) 2016 Rafał Miłecki <zajec5@gmail.com>

 Calculate ndiv based on a solid 1920 MHz that is for USB2 PHY */

 Unlock DMU PLL settings with some magic value */

 Write USB 2.0 PLL control setting */

 Lock DMU PLL settings */

/*

 * Copyright (C) 2015 Broadcom Corporation

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

/**

 * struct cygnus_pcie_phy - Cygnus PCIe PHY device

 * @core: pointer to the Cygnus PCIe PHY core control

 * @id: internal ID to identify the Cygnus PCIe PHY

 * @phy: pointer to the kernel PHY device

/**

 * struct cygnus_pcie_phy_core - Cygnus PCIe PHY core control

 * @dev: pointer to device

 * @base: base register

 * @lock: mutex to protect access to individual PHYs

 * @phys: pointer to Cygnus PHY device

		/*

		 * Wait 50 ms for the PCIe Serdes to stabilize after the analog

		 * front end is brought up

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Ingenic SoCs USB PHY driver

 * Copyright (c) Paul Cercueil <paul@crapouillou.net>

 * Copyright (c) 漆鹏振 (Qi Pengzhen) <aric.pzqi@ingenic.com>

 * Copyright (c) 周琰杰 (Zhou Yanjie) <zhouyanjie@wanyeetech.com>

 OTGPHY register offsets */

 bits within the USBPCR register */

 bits within the USBRDTR register */

 bits within the USBPCR1 register */

 Wait for PHY to reset */

 rdt */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2017, The Linux Foundation. All rights reserved.

 QPHY_SW_RESET bit */

 QPHY_POWER_DOWN_CONTROL */

 QPHY_START_CONTROL bits */

 QPHY_PCS_STATUS bit */

 QPHY_PCS_READY_STATUS & QPHY_COM_PCS_READY_STATUS bit */

 QPHY_V3_DP_COM_RESET_OVRD_CTRL register bits */

 DP PHY soft reset */

 mux to select DP PHY reset control, 0:HW control, 1: software reset */

 USB3 PHY soft reset */

 mux to select USB3 PHY reset control, 0:HW control, 1: software reset */

 QPHY_V3_DP_COM_PHY_MODE_CTRL register bits */

 enables USB3 mode */

 enables DP mode */

 QPHY_PCS_AUTONOMOUS_MODE_CTRL register bits */

 QPHY_PCS_LFPS_RXTERM_IRQ_CLEAR register bits */

 QPHY_PCS_LFPS_RXTERM_IRQ_STATUS register bits */

 QPHY_V3_PCS_MISC_CLAMP_ENABLE register bits */

 enables i/o clamp_n */

 Define the assumed distance between lanes for underspecified device trees. */

	/*

	 * register part of layout ?

	 * if yes, then offset gives index in the reg-layout

	/*

	 * mask of lanes for which this register is written

	 * for cases when second lane needs different values

 set of registers with offsets different per-PHY */

 Common block control registers */

 PCS registers */

 PCS_MISC registers */

 Keep last to ensure regs_layout arrays are properly initialized */

 PLL and Loop filter settings */

 SSC settings */

 PLL and Loop filter settings */

 SSC settings */

 FLL settings */

 Lock Det settings */

 FLL settings */

 Lock Det settings */

 FLL settings */

 Lock Det settings */

 Rate B */

 8 us */

 Rate B */

 Rate B */

 Lock Det settings */

 Rate B */

 struct qmp_phy_cfg - per-PHY initialization config */

 phy-type - PCIE/UFS/USB */

 number of lanes provided by phy */

 Init sequence for PHY blocks - serdes, tx, rx, pcs */

 Init sequence for DP PHY block link rates */

 DP PHY callbacks */

 clock ids to be requested */

 resets to be requested */

 regulators to be requested */

 array of registers with different offsets */

 bit offset of PHYSTATUS in QPHY_PCS_STATUS register */

 true, if PHY has a separate PHY_COM control block */

 true, if PHY has a reset for individual lanes */

 true, if PHY needs delay after POWER_DOWN */

 power_down delay in usec */

 true, if PHY has a separate DP_COM control block */

 true, if PHY has secondary tx/rx lanes to be configured */

 true, if PCS block has no separate SW_RESET register */

/**

 * struct qmp_phy - per-lane phy descriptor

 *

 * @phy: generic phy

 * @cfg: phy specific configuration

 * @serdes: iomapped memory space for phy's serdes (i.e. PLL)

 * @tx: iomapped memory space for lane's tx

 * @rx: iomapped memory space for lane's rx

 * @pcs: iomapped memory space for lane's pcs

 * @tx2: iomapped memory space for second lane's tx (in dual lane PHYs)

 * @rx2: iomapped memory space for second lane's rx (in dual lane PHYs)

 * @pcs_misc: iomapped memory space for lane's pcs_misc

 * @pipe_clk: pipe lock

 * @index: lane index

 * @qmp: QMP phy to which this lane belongs

 * @lane_rst: lane's reset controller

 * @mode: current PHY mode

/**

 * struct qcom_qmp - structure holding QMP phy block attributes

 *

 * @dev: device

 * @dp_com: iomapped memory space for phy's dp_com control block

 *

 * @clks: array of clocks required by phy

 * @resets: array of resets required by phy

 * @vregs: regulator supplies bulk data

 *

 * @phys: array of per-lane phy descriptors

 * @phy_mutex: mutex lock for PHY common block initialization

 * @init_count: phy common block initialization count

 * @ufs_reset: optional UFS PHY reset handle

 ensure that above write is through */

 ensure that above write is through */

 list of clocks required by phy */

 the primary usb3 phy on sm8250 doesn't have a ref clock */

 usb3 phy on sdx55 doesn't have com_aux clock */

 list of resets */

 list of regulators */

 list of resets */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 us */

 Other link rates aren't supported */

 Turn on BIAS current for PHY/PLL */

 TODO: Move check to config check */

 Enable MUX to use Cursor values from these registers */

	/*

	 * TODO: Assume orientation is CC1 for now and two lanes, need to

	 * use type-c connector to understand orientation and lanes.

	 *

	 * Otherwise val changes to be like below if this code understood

	 * the orientation of the type-c cable.

	 *

	 * if (lane_cnt == 4 || orientation == ORIENTATION_CC2)

	 *	val |= DP_PHY_PD_CTL_LANE_0_1_PWRDN;

	 * if (lane_cnt == 4 || orientation == ORIENTATION_CC1)

	 *	val |= DP_PHY_PD_CTL_LANE_2_3_PWRDN;

	 * if (orientation == ORIENTATION_CC2)

	 *	writel(0x4c, qphy->pcs + QSERDES_V3_DP_PHY_MODE);

 Other link rates aren't supported */

/*

 * We need to calibrate the aux setting here as many times

 * as the caller tries

 Turn on BIAS current for PHY/PLL */

 Program default values before writing proper values */

 Other link rates aren't supported */

	/*

	 * At least for 7nm DP PHY this has to be done after enabling link

	 * clock.

/*

 * We need to calibrate the aux setting here as many times

 * as the caller tries

 turn on regulator supplies */

 override hardware control for reset of qmp phy */

 Default type-c orientation, i.e CC1 */

 bring both QMP USB and QMP DP PHYs PCS block out of reset */

		/*

		 * Get UFS reset, which is delayed until now to avoid a

		 * circular dependency where UFS needs its PHY, but the PHY

		 * needs this UFS reset.

 Tx, Rx, and PCS configurations */

 Configuration for other LANE for USB-DP combo PHY */

 Configure special DP tx tunings */

 Configure link rate, swing, etc. */

	/*

	 * Pull out PHY from POWER DOWN state.

	 * This is active low enable signal to power-down PHY.

 Pull PHY out of reset state */

 start SerDes and Phy-Coding-Sublayer */

 Assert DP PHY power down */

 PHY reset */

 stop SerDes and Phy-Coding-Sublayer */

 Put PHY into POWER DOWN state: active low */

 Clear any pending interrupts status */

 Writing 1 followed by 0 clears the interrupt */

 Enable required PHY autonomous mode interrupts */

 Enable i/o clamp_n for autonomous mode */

 Disable i/o clamp_n on resume for normal mode */

 Writing 1 followed by 0 clears the interrupt */

 Supported only for USB3 PHY and luckily USB3 is the first phy */

 Supported only for USB3 PHY and luckily USB3 is the first phy */

/*

 * Register a fixed rate pipe clock.

 *

 * The <s>_pipe_clksrc generated by PHY goes to the GCC that gate

 * controls it. The <s>_pipe_clk coming out of the GCC is requested

 * by the PHY driver for its operations.

 * We register the <s>_pipe_clksrc here. The gcc driver takes care

 * of assigning this <s>_pipe_clksrc as parent to <s>_pipe_clk.

 * Below picture shows this relationship.

 *

 *         +---------------+

 *         |   PHY block   |<<---------------------------------------+

 *         |               |                                         |

 *         |   +-------+   |                   +-----+               |

 *   I/P---^-->|  PLL  |---^--->pipe_clksrc--->| GCC |--->pipe_clk---+

 *    clk  |   +-------+   |                   +-----+

 *         +---------------+

 controllers using QMP phys use 125MHz pipe clock interface */

	/*

	 * Roll a devm action because the clock provider is the child node, but

	 * the child node is not actually a device.

/*

 * Display Port PLL driver block diagram for branch clocks

 *

 *              +------------------------------+

 *              |         DP_VCO_CLK           |

 *              |                              |

 *              |    +-------------------+     |

 *              |    |   (DP PLL/VCO)    |     |

 *              |    +---------+---------+     |

 *              |              v               |

 *              |   +----------+-----------+   |

 *              |   | hsclk_divsel_clk_src |   |

 *              |   +----------+-----------+   |

 *              +------------------------------+

 *                              |

 *          +---------<---------v------------>----------+

 *          |                                           |

 * +--------v----------------+                          |

 * |    dp_phy_pll_link_clk  |                          |

 * |     link_clk            |                          |

 * +--------+----------------+                          |

 *          |                                           |

 *          |                                           |

 *          v                                           v

 * Input to DISPCC block                                |

 * for link clk, crypto clk                             |

 * and interface clock                                  |

 *                                                      |

 *                                                      |

 *      +--------<------------+-----------------+---<---+

 *      |                     |                 |

 * +----v---------+  +--------v-----+  +--------v------+

 * | vco_divided  |  | vco_divided  |  | vco_divided   |

 * |    _clk_src  |  |    _clk_src  |  |    _clk_src   |

 * |              |  |              |  |               |

 * |divsel_six    |  |  divsel_two  |  |  divsel_four  |

 * +-------+------+  +-----+--------+  +--------+------+

 *         |                 |                  |

 *         v---->----------v-------------<------v

 *                         |

 *              +----------+-----------------+

 *              |   dp_phy_pll_vco_div_clk   |

 *              +---------+------------------+

 *                        |

 *                        v

 *              Input to DISPCC block

 *              for DP pixel clock

 *

 5.4 and 8.1 GHz are same link rate as 2.7GHz, i.e. div 4 and div 6 */

	/*

	 * Roll a devm action because the clock provider is the child node, but

	 * the child node is not actually a device.

	/*

	 * Get memory resources for each phy lane:

	 * Resources are indexed as: tx -> 0; rx -> 1; pcs -> 2.

	 * For dual lane PHYs: tx2 -> 3, rx2 -> 4, pcs_misc (optional) -> 5

	 * For single lane PHYs: pcs_misc (optional) -> 3.

	/*

	 * If this is a dual-lane PHY, then there should be registers for the

	 * second lane. Some old device trees did not specify this, so fall

	 * back to old legacy behavior of assuming they can be reached at an

	 * offset from the first lane.

 In the old version, pcs_misc is at index 3. */

	/*

	 * Get PHY's Pipe clock, if any. USB3 and PCIe are PIPE3

	 * based phys, so they essentially have pipe clock. So,

	 * we return error in case phy is USB3 or PIPE type.

	 * Otherwise, we initialize pipe clock to NULL for

	 * all phys that don't need this.

 Get lane reset, if any */

 It's a combo phy */

 It's a combo phy */

 It's a combo phy */

 Get the specific init parameters of QMP phy */

 Setup clks and regulators */

 per PHY serdes; usually located at base address */

 per PHY dp_com; if PHY has dp_com control block */

 Only two serdes for combo PHY */

 do we have a rogue child node ? */

	/*

	 * Prevent runtime pm from being ON by default. Users can enable

	 * it using power/control in sysfs.

 Create per-lane phy */

		/*

		 * Register the pipe clock provided by phy.

		 * See function description to see details of this pipe clock.

 SPDX-License-Identifier: GPL-2.0-only

 USB QSCRATCH Hardware registers */

 PHY_CTRL_REG */

 QSCRATCH_GENERAL_CFG */

 USB QSCRATCH Hardware registers */

 PHY_CTRL_REG */

 SSPHY control registers - Does this need 0x30? */

 SSPHY SoC version specific values */

 Override value for rx_eq */

 Override value for transmit preemphasis */

 Override value for mpll */

 QSCRATCH PHY_PARAM_CTRL1 fields */

 RX OVRD IN HI bits */

 TX OVRD DRV LO register bits */

 MPLL bits */

 SS CAP register bits */

/**

 * Write register and read back masked value to confirm it is written

 *

 * @base - QCOM DWC3 PHY base virtual address.

 * @offset - register offset.

 * @mask - register bitmask specifying what should be updated

 * @val - value to write.

 retain other bits */

 Read back to see if val was written */

 clear other bits */

/**

 * Write SSPHY register

 *

 * @base - QCOM DWC3 PHY base virtual address.

 * @addr - SSPHY address to write.

 * @val - value to write.

/**

 * Read SSPHY register.

 *

 * @base - QCOM DWC3 PHY base virtual address.

 * @addr - SSPHY address to read.

	/*

	 * Due to hardware bug, first read of SSPHY register might be

	 * incorrect. Hence as workaround, SW should perform SSPHY register

	 * read twice, but use only second read and ignore first read.

 throwaway read */

	/*

	 * HSPHY Initialization: Enable UTMI clock, select 19.2MHz fsel

	 * enable clamping, and disable RETENTION (power-on default is ENABLED)

 use core clock if external reference is not present */

 Disable (bypass) VBUS and ID filters */

 reset phy */

 clear REF_PAD if we don't have XO clk */

 wait for ref clk to become stable, this can take up to 30ms */

	/*

	 * WORKAROUND: There is SSPHY suspend bug due to which USB enumerates

	 * in HS mode instead of SS mode. Workaround it by asserting

	 * LANE0.TX_ALT_BLOCK.EN_ALT_BUS to enable TX to use alt bus mode

	/*

	 * Fix RX Equalization setting as follows

	 * LANE0.RX_OVRD_IN_HI. RX_EQ_EN set to 0

	 * LANE0.RX_OVRD_IN_HI.RX_EQ_EN_OVRD set to 1

	 * LANE0.RX_OVRD_IN_HI.RX_EQ set based on SoC version

	 * LANE0.RX_OVRD_IN_HI.RX_EQ_OVRD set to 1

	/*

	 * Set EQ and TX launch amplitudes as follows

	 * LANE0.TX_OVRD_DRV_LO.PREEMPH set based on SoC version

	 * LANE0.TX_OVRD_DRV_LO.AMPLITUDE set to 110

	 * LANE0.TX_OVRD_DRV_LO.EN set to 1.

	/*

	 * Set the QSCRATCH PHY_PARAM_CTRL1 parameters as follows

	 * TX_FULL_SWING [26:20] amplitude to 110

	 * TX_DEEMPH_6DB [19:14] to 32

	 * TX_DEEMPH_3_5DB [13:8] set based on SoC version

	 * LOS_BIAS [7:3] to 9

	/* Sequence to put SSPHY in low power state:

	 * 1. Clear REF_PHY_EN in PHY_CTRL_REG

	 * 2. Clear REF_USE_PAD in PHY_CTRL_REG

	 * 3. Set TEST_POWERED_DOWN in PHY_CTRL_REG to enable PHY retention

 Sentinel */ }

 Parse device node to probe HSIO settings */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2017, 2019, The Linux Foundation. All rights reserved.

 QUSB2PHY_PLL_STATUS register bits */

 QUSB2PHY_PLL_COMMON_STATUS_ONE register bits */

 QUSB2PHY_PORT_POWERDOWN register bits */

 QUSB2PHY_PWR_CTRL1 register bits */

 QUSB2PHY_INTR_CTRL register bits */

 QUSB2PHY_PLL_CORE_INPUT_OVERRIDE register bits */

 QUSB2PHY_IMP_CTRL1 register bits */

 QUSB2PHY_PLL_BIAS_CONTROL_2 register bits */

 QUSB2PHY_CHG_CONTROL_2 register bits */

 QUSB2PHY_PORT_TUNE1 register bits */

 QUSB2PHY_PORT_TUNE2 register bits */

	/*

	 * register part of layout ?

	 * if yes, then offset gives index in the reg-layout

 set of registers with offsets different per-PHY */

 number of entries in the table */

 offset to PHY_CLK_SCHEME register in TCSR map */

 array of registers with different offsets */

 true if PHY has PLL_TEST register to select clk_scheme */

 true if TUNE1 register must be updated by fused value, else TUNE2 */

 true if PHY has PLL_CORE_INPUT_OVERRIDE register to reset PLL */

 true if PHY default clk scheme is single-ended */

 autoresume not used */

/* struct override_param - structure holding qusb2 v2 phy overriding param

 * set override true if the  device tree property exists and read and assign

 * to value

/*struct override_params - structure holding qusb2 v2 phy overriding params

 * @imp_res_offset: rescode offset to be updated in IMP_CTRL1 register

 * @hstx_trim: HSTX_TRIM to be updated in TUNE1 register

 * @preemphasis: Amplitude Pre-Emphasis to be updated in TUNE1 register

 * @preemphasis_width: half/full-width Pre-Emphasis updated via TUNE1

 * @bias_ctrl: bias ctrl to be updated in BIAS_CONTROL_2 register

 * @charge_ctrl: charge ctrl to be updated in CHG_CTRL2 register

 * @hsdisc_trim: disconnect threshold to be updated in TUNE2 register

/**

 * struct qusb2_phy - structure holding qusb2 phy attributes

 *

 * @phy: generic phy

 * @base: iomapped memory space for qubs2 phy

 *

 * @cfg_ahb_clk: AHB2PHY interface clock

 * @ref_clk: phy reference clock

 * @iface_clk: phy interface clock

 * @phy_reset: phy reset control

 * @vregs: regulator supplies bulk data

 *

 * @tcsr: TCSR syscon register map

 * @cell: nvmem cell containing phy tuning value

 *

 * @overrides: pointer to structure for all overriding tuning params

 *

 * @cfg: phy config data

 * @has_se_clk_scheme: indicate if PHY has single-ended ref clock scheme

 * @phy_initialized: indicate if PHY has been initialized

 * @mode: current PHY mode

 Ensure above write is completed */

 Ensure above write is completed */

 Ensure above write is completed */

/*

 * Update board specific PHY tuning override values if specified from

 * device tree.

/*

 * Fetches HS Tx tuning value from nvmem and sets the

 * QUSB2PHY_PORT_TUNE1/2 register.

 * For error case, skip setting the value and use the default value.

 efuse register is optional */

	/*

	 * Read efuse register having TUNE2/1 parameter's high nibble.

	 * If efuse register shows value as 0x0 (indicating value is not

	 * fused), or if we fail to find a valid efuse register setting,

	 * then use default value for high nibble that we have already

	 * set while configuring the phy.

 Fused TUNE1/2 value is the higher nibble only */

	/*

	 * Enable DP/DM interrupts to detect line state changes based on current

	 * speed. In other words, enable the triggers _opposite_ of what the

	 * current D+/D- levels are e.g. if currently D+ high, D- low

	 * (HS 'J'/Suspend), configure the mask to trigger on D+ low OR D- high

 No device connected, enable both DP/DM high interrupt */

 hold core PLL into reset */

 enable phy auto-resume only if device is connected on bus */

 Autoresume bit has to be toggled in order to enable it */

 bring core PLL out of reset */

 turn on regulator supplies */

 enable ahb interface clock to program phy */

 Perform phy reset */

 100 us delay to keep PHY in reset mode */

 Disable the PHY */

 save reset value to override reference clock scheme later */

 Override board specific PHY tuning values */

 Set efuse value for tuning the PHY */

 Enable the PHY */

 Required to get phy pll lock successfully */

	/*

	 * Not all the SoCs have got a readable TCSR_PHY_CLK_SCHEME

	 * register in the TCSR so, if there's none, use the default

	 * value hardcoded in the configuration.

	/*

	 * read TCSR_PHY_CLK_SCHEME register to check if single-ended

	 * clock scheme is selected. If yes, then disable differential

	 * ref_clk and use single-ended clock, otherwise use differential

	 * ref_clk only.

 is it a differential clock scheme ? */

 ensure above write is through */

 Required to get phy pll lock successfully */

 Disable the PHY */

		/*

		 * Deprecated. Only here to support legacy device

		 * trees that didn't include "qcom,qusb2-v2-phy"

 Get the specific init parameters of QMP phy */

	/*

	 * Prevent runtime pm from being ON by default. Users can enable

	 * it using power/control in sysfs.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Atheros AR71XX/9XXX USB PHY driver

 *

 * Copyright (C) 2015-2018 Alban Bedel <albeu@free.fr>

	/* The suspend override logic is inverted, hence the no prefix

	 * to make the code a bit easier to understand.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Linaro Ltd

 setup initial state */

 NUL terminate */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2009-2018, Linux Foundation. All rights reserved.

 * Copyright (c) 2018-2020, Linaro Limited

 PHY register and bit definitions */

 Clear any existing interrupts before enabling the interrupts */

 Make sure the interrupts are cleared */

 No device connected */

 Clear any pending interrupts */

 Device match data is optional. */

	/*

	 * The Femto PHY is POR reset in the following scenarios.

	 *

	 * 1. After overriding the parameter registers.

	 * 2. Low power mode exit from PHY retention.

	 *

	 * Ensure that SIDDQ is cleared before bringing the PHY

	 * out of reset.

	/*

	 * As per databook, 10 usec delay is required between

	 * PHY POR assert and de-assert.

	/*

	 * As per databook, it takes 75 usec for PHY to stabilize

	 * after the reset.

 Get device match data */

/*

 * The macro is used to define an initialization sequence.  Each tuple

 * is meant to program 'value' into phy register at 'offset' with 'delay'

 * in us followed.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014-2017, The Linux Foundation. All rights reserved.

 * Copyright (c) 2019, Linaro Ltd.

 Program REF_CLK source */

 Don't use PAD for refclock */

 Program SSP ENABLE */

 Assert Phy SW Reset */

 Program Tx Amplitude */

 Program De-Emphasis */

 Program Rx_Eq */

 Program Tx0_term_offset */

 disable Tx2Rx Loopback */

 De-assert Phy SW Reset */

/*

 * Register a fixed rate pipe clock.

 *

 * The <s>_pipe_clksrc generated by PHY goes to the GCC that gate

 * controls it. The <s>_pipe_clk coming out of the GCC is requested

 * by the PHY driver for its operations.

 * We register the <s>_pipe_clksrc here. The gcc driver takes care

 * of assigning this <s>_pipe_clksrc as parent to <s>_pipe_clk.

 * Below picture shows this relationship.

 *

 *         +---------------+

 *         |   PHY block   |<<---------------------------------------+

 *         |               |                                         |

 *         |   +-------+   |                   +-----+               |

 *   I/P---^-->|  PLL  |---^--->pipe_clksrc--->| GCC |--->pipe_clk---+

 *    clk  |   +-------+   |                   +-----+

 *         +---------------+

 controllers using QMP phys use 250MHz pipe clock interface */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2012-2014,2017 The Linux Foundation. All rights reserved.

 * Copyright (c) 2018-2020, Linaro Limited

 PHY_CTRL bits */

 if reset_com is present, reset_phy is no longer optional */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

 Setting SSC_EN to 1 */

 Setting PHY_RESET to 1 */

 Setting REF_SSP_EN to 1 */

 make sure all changes complete before we let the PHY out of reset */

 sleep for max. 50us more to combine processor wakeups */

 Clearing PHY_RESET to 0 */

 Setting PHY_RESET to 1 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

 PHY registers */

 default timeout set to 1 sec */

 Helper function to do poll and timeout */

 SATA phy initialization */

 Make sure the power down happens before power up */

 Write UNIPHYPLL registers to configure PLL */

 make sure global config LDO power down happens before power up */

 PLL Lock wait */

 TX Calibration */

 RX Calibration */

 SATA phy calibrated succesfully, power up to functional mode */

 Power down PHY */

 Power down PLL block */

 SPDX-License-Identifier: GPL-2.0-only

/**

 * Copyright (C) 2016 Linaro Ltd

 Set periodic calibration interval to ~2.048sec in HSIC_IO_CAL_REG */

 Enable periodic IO calibration in HSIC_CFG register */

 Configure pins for HSIC functionality */

 Enable HSIC mode in HSIC_CFG register */

 Disable auto-resume */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2020, The Linux Foundation. All rights reserved.

/**

 * struct qcom_snps_hsphy - snps hs phy attributes

 *

 * @phy: generic phy

 * @base: iomapped memory space for snps hs phy

 *

 * @cfg_ahb_clk: AHB2PHY interface clock

 * @ref_clk: phy reference clock

 * @iface_clk: phy interface clock

 * @phy_reset: phy reset control

 * @vregs: regulator supplies bulk data

 * @phy_initialized: if PHY has been initialized correctly

 * @mode: contains the current mode the PHY is in

 Ensure above write is completed */

 Enable auto-resume to meet remote wakeup timing */

	/*

	 * Prevent runtime pm from being ON by default. Users can enable

	 * it using power/control in sysfs.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2018 John Crispin <john@phrozen.org>

 *

 * Based on code from

 * Allwinner Technology Co., Ltd. <www.allwinnertech.com>

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-zynqmp.c - PHY driver for Xilinx ZynqMP GT.

 *

 * Copyright (C) 2018-2020 Xilinx Inc.

 *

 * Author: Anurag Kumar Vulisha <anuragku@xilinx.com>

 * Author: Subbaraya Sundeep <sundeep.lkml@gmail.com>

 * Author: Laurent Pinchart <laurent.pinchart@ideasonboard.com>

 *

 * This driver is tested for USB, SATA and Display Port currently.

 * Other controllers PCIe and SGMII should also work but that is

 * experimental as of now.

/*

 * Lane Registers

 TX De-emphasis parameters */

 DN Resistor calibration code parameters */

 PMA control parameters */

 PCS control parameters */

 PLL Test Mode register parameters */

 PLL SSC step size offsets */

 SSC step size parameters */

 Reference clock selection parameters */

 Calibration digital logic parameters */

/*

 * Global Registers

 Refclk selection parameters */

 Inter Connect Matrix parameters */

 Inter Connect Matrix allowed protocols */

 Test Mode common reset control  parameters */

 Bus width parameters */

 Number of GT lanes */

 SIOU SATA control register */

 Total number of controllers */

 Protocol Type parameters */

 USB controller 0 */

 USB controller 1 */

 SATA controller lane 0 */

 SATA controller lane 1 */

 PCIe controller lane 0 */

 PCIe controller lane 1 */

 PCIe controller lane 2 */

 PCIe controller lane 3 */

 Display Port controller lane 0 */

 Display Port controller lane 1 */

 Ethernet SGMII controller 0 */

 Ethernet SGMII controller 1 */

 Ethernet SGMII controller 2 */

 Ethernet SGMII controller 3 */

 Timeout values */

/**

 * struct xpsgtr_ssc - structure to hold SSC settings for a lane

 * @refclk_rate: PLL reference clock frequency

 * @pll_ref_clk: value to be written to register for corresponding ref clk rate

 * @steps: number of steps of SSC (Spread Spectrum Clock)

 * @step_size: step size of each step

/**

 * struct xpsgtr_phy - representation of a lane

 * @phy: pointer to the kernel PHY device

 * @type: controller which uses this lane

 * @lane: lane number

 * @protocol: protocol in which the lane operates

 * @skip_phy_init: skip phy_init() if true

 * @dev: pointer to the xpsgtr_dev instance

 * @refclk: reference clock index

/**

 * struct xpsgtr_dev - representation of a ZynMP GT device

 * @dev: pointer to device

 * @serdes: serdes base address

 * @siou: siou base address

 * @gtr_mutex: mutex for locking

 * @phys: PHY lanes

 * @refclk_sscs: spread spectrum settings for the reference clocks

 * @clk: reference clocks

 * @tx_term_fix: fix for GT issue

 * @saved_icm_cfg0: stored value of ICM CFG0 register

 * @saved_icm_cfg1: stored value of ICM CFG1 register

 mutex for locking */

/*

 * Configuration Data

 lookup table to hold all settings needed for a ref clock frequency */

/*

 * I/O Accessors

/*

 * Hardware Configuration

 Wait for the PLL to lock (with a timeout). */

 Configure PLL and spread-sprectrum clock. */

 Enable lane clock sharing, if required */

 Lane3 Ref Clock Selection Register */

 SSC step size [7:0] */

 SSC step size [15:8] */

 SSC step size [23:16] */

 SSC steps [7:0] */

 SSC steps [10:8] */

 SSC step size [24:25] */

 Configure the lane protocol. */

 We already checked 0 <= lane <= 3 */

 Bypass (de)scrambler and 8b/10b decoder and encoder. */

 DP-specific initialization. */

 SATA-specific initialization. */

 SGMII-specific initialization. */

 Set SGMII protocol TX and RX bus width to 10 bits. */

 Configure TX de-emphasis and margining for DP. */

/*

 * PHY Operations

	/*

	 * As USB may save the snapshot of the states during hibernation, doing

	 * phy_init() will put the USB controller into reset, resulting in the

	 * losing of the saved snapshot. So try to avoid phy_init() for USB

	 * except when gtr_phy->skip_phy_init is false (this happens when FPD is

	 * shutdown during suspend or when gt lane is changed from current one)

/*

 * There is a functional issue in the GT. The TX termination resistance can be

 * out of spec due to a issue in the calibration logic. This is the workaround

 * to fix it, required for XCZU9EG silicon.

 Enabling Test Mode control for CMN Rest */

 Set Test Mode reset */

	/*

	 * As a part of work around sequence for PMOS calibration fix,

	 * we need to configure any lane ICM_CFG to valid protocol. This

	 * will deassert the CMN_Resetn signal.

 Clear Test Mode reset */

 Reading NMOS Register Code */

 Set Test Mode reset */

 Writing NMOS register values back [5:3] */

 Writing NMOS register value [2:0] */

 Clear Test Mode reset */

 Skip initialization if not required. */

 Enable coarse code saturation limiting logic. */

	/*

	 * Configure the PLL, the lane protocol, and perform protocol-specific

	 * initialization.

 Skip initialization if not required. */

	/*

	 * Wait for the PLL to lock. For DP, only wait on DP0 to avoid

	 * cumulating waits for both lanes. The user is expected to initialize

	 * lane 0 last.

/*

 * OF Xlate Support

 Set the lane type and protocol based on the PHY type and instance number. */

/*

 * Valid combinations of controllers and lanes (Interconnect Matrix).

 Translate OF phandle and args to PHY instance. */

	/*

	 * Get the PHY parameters from the OF arguments and derive the lane

	 * type.

	/*

	 * Ensure that the Interconnect Matrix is obeyed, i.e a given lane type

	 * is allowed to operate on the lane.

/*

 * Power Management

 Save the snapshot ICM_CFG registers. */

 Return if no GT lanes got configured before suspend. */

 Check if the ICM configurations changed after suspend. */

 Update the skip_phy_init for all gtr_phy instances. */

/*

 * Probe & Platform Driver

		/*

		 * Get the spread spectrum (SSC) settings for the reference

		 * clock rate.

 Acquire resources. */

 Create PHYs. */

 Register the PHY provider. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SATA SerDes(PHY) driver

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Authors: Girish K S <ks.giri@samsung.com>

 *         Yuvaraj Kumar C D <yuvaraj.cd@samsung.com>

 High speed enable for Gen3 */

 release cmu reset */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung Exynos SoC series Display Port PHY driver

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Jingoo Han <jg1.han@samsung.com>

 Disable power isolation on DP-PHY */

 Enable power isolation on DP-PHY */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * UFS PHY driver data for Samsung EXYNOS7 SoC

 *

 * Copyright (C) 2020 Samsung Electronics Co., Ltd.

 Calibration for phy initialization */

 Calibration for HS mode series A/B */

 Setting order: 1st(0x16, 2nd(0x15) */

 Calibration for HS mode series A/B atfer PMC */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * UFS PHY driver data for Samsung EXYNOSAUTO v9 SoC

 *

 * Copyright (C) 2021 Samsung Electronics Co., Ltd.

 Calibration for phy initialization */

 Calibration for HS mode series A/B */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SoC USB 1.1/2.0 PHY driver - Exynos 4210 support

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Kamil Debski <k.debski@samsung.com>

 Exynos USB PHY registers */

 PHY power control */

 PHY clock control */

 PHY reset control */

 Isolation, configured in the power management unit */

 USBYPHY1 Floating prevention */

 Mode switching SUB Device <-> Host */

/*

 * exynos4210_rate_to_clk() converts the supplied clock rate to the value that

 * can be written to the phy register.

		/* The following delay is necessary for the reset sequence to be

 Order of initialisation is important - first power then isolation */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SoC USB 1.1/2.0 PHY driver - S5PV210 support

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Authors: Kamil Debski <k.debski@samsung.com>

 Exynos USB PHY registers */

 PHY power control */

 PHY clock control */

 PHY reset control */

 Isolation, configured in the power management unit */

/*

 * s5pv210_rate_to_clk() converts the supplied clock rate to the value that

 * can be written to the phy register.

		/* The following delay is necessary for the reset sequence to be

		 * completed

 SPDX-License-Identifier: GPL-2.0-only

/*

 * UFS PHY driver for Samsung SoC

 *

 * Copyright (C) 2020 Samsung Electronics Co., Ltd.

 * Author: Seungwon Jeon <essuuj@gmail.com>

 * Author: Alim Akhtar <alim.akhtar@samsung.com>

 *

 lane index */

	/**

	 * In Samsung ufshci, PHY need to be calibrated at different

	 * stages / state mainly before Linkstartup, after Linkstartup,

	 * before power mode change and after power mode change.

	 * Below state machine to make sure to calibrate PHY in each

	 * state. Here after configuring PHY in a given state, will

	 * change the state to next state so that next state phy

	 * calibration value can be programed

 Change back to INIT state */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SoC USB 1.1/2.0 PHY driver - Exynos 5250 support

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Kamil Debski <k.debski@samsung.com>

 Exynos USB PHY registers */

 Normal host */

 HSIC0 & HSIC1 */

 EHCI control */

 OHCI control */

 USBOTG */

 Isolation, configured in the power management unit */

 Mode swtich register */

/*

 * exynos5250_rate_to_clk() converts the supplied clock rate to the value that

 * can be written to the phy register.

 EXYNOS_5250_FSEL_MASK */

 OTG configuration */

 The clock */

 Reset */

 Ref clock */

 Host registers configuration */

 The clock */

 Reset */

 OTG configuration */

 The clock */

 Reset */

 Ref clock */

 HSIC phy configuration */

		/* The following delay is necessary for the reset sequence to be

 Enable EHCI DMA burst */

 OHCI settings */

 Following code is based on the old driver */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SoC USB 1.1/2.0 PHY driver

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Kamil Debski <k.debski@samsung.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung SoC USB 1.1/2.0 PHY driver - Exynos 4x12 support

 *

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Kamil Debski <k.debski@samsung.com>

 Exynos USB PHY registers */

 PHY power control */

 PHY clock control */

 PHY reset control */

/* The following bit defines are presented in the

 * order taken from the Exynos4412 reference manual.

 *

 * During experiments with the hardware and debugging

 * it was determined that the hardware behaves contrary

 * to the manual.

 *

 * The following bit values were chaned accordingly to the

 * results of real hardware experiments.

 Isolation, configured in the power management unit */

 Mode switching SUB Device <-> Host */

/*

 * exynos4x12_rate_to_clk() converts the supplied clock rate to the value that

 * can be written to the phy register.

 EXYNOS_4x12_UPHYCLK_PHYFSEL_MASK */

		/* The following delay is necessary for the reset sequence to be

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung Exynos SoC series PCIe PHY driver

 *

 * Phy provider for PCIe controller on Exynos SoC series

 *

 * Copyright (C) 2017-2020 Samsung Electronics Co., Ltd.

 * Jaehoon Chung <jh80.chung@samsung.com>

 Sysreg FSYS register offsets and bits for Exynos5433 */

 PMU PCIE PHY isolation control */

 For Exynos pcie phy */

 Exynos5433 specific functions */

 PHY refclk 24MHz */

 band gap reference on */

 jitter tuning */

 D0 uninit.. */

 24MHz */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung Exynos5 SoC series USB DRD PHY driver

 *

 * Phy provider for USB 3.0 DRD controller on Exynos5 SoC series

 *

 * Copyright (C) 2014 Samsung Electronics Co., Ltd.

 * Author: Vivek Gautam <gautam.vivek@samsung.com>

 Exynos USB PHY registers */

 Exynos5: USB 3.0 DRD PHY registers */

 USB 3.0 DRD PHY SS Function Control Reg; accessed by CR_PORT */

/**

 * struct exynos5_usbdrd_phy - driver data for USB 3.0 PHY

 * @dev: pointer to device instance of this platform device

 * @reg_phy: usb phy controller register memory base

 * @clk: phy clock for register access

 * @pipeclk: clock for pipe3 phy

 * @utmiclk: clock for utmi+ phy

 * @itpclk: clock for ITP generation

 * @drv_data: pointer to SoC level driver data structure

 * @phys: array for 'EXYNOS5_DRDPHYS_NUM' number of PHY

 *	    instances each with its 'phy' and 'phy_cfg'.

 * @extrefclk: frequency select settings when using 'separate

 *	       reference clocks' for SS and HS operations

 * @ref_clk: reference clock to PHY block from which PHY's

 *	     operational clocks are derived

 * @vbus: VBUS regulator for phy

 * @vbus_boost: Boost regulator for VBUS present on few Exynos boards

/*

 * exynos5_rate_to_clk() converts the supplied clock rate to the value that

 * can be written to the phy register.

 EXYNOS5_FSEL_MASK */

/*

 * Sets the pipe3 phy's clk as EXTREFCLK (XXTI) which is internal clock

 * from clock core. Further sets multiplier values and spread spectrum

 * clock settings for SuperSpeed operations.

 restore any previous reference clock settings */

 Use EXTREFCLK as ref clock */

 FSEL settings corresponding to reference clock */

/*

 * Sets the utmi phy's clk as EXTREFCLK (XXTI) which is internal clock

 * from clock core. Further sets the FSEL values for HighSpeed operations.

 restore any previous reference clock settings */

 Set Tx De-Emphasis level */

 Set Loss-of-Signal Detector sensitivity */

 Set Tx De-Emphasis level */

 UTMI Power Control */

 Reset USB 3.0 PHY */

	/*

	 * Setting the Frame length Adj value[6:1] to default 0x20

	 * See xHCI 1.0 spec, 5.2.4

 Select PHY CLK source */

 This bit must be set for both HS and SS operations */

 UTMI or PIPE3 specific init */

 reference clock settings */

 Digital power supply in normal operating mode */

 Enable ref clock for SS function */

 Enable spread spectrum */

 Power down HS Bias and PLL blocks in suspend mode */

 Reset the port */

 Resetting the PHYCLKRST enable bits to reduce leakage current */

 Control PHYTEST to remove leakage current */

 Enable VBUS supply */

 Power-on PHY*/

 Power-off the PHY */

 Disable VBUS supply */

 Write Address */

 Write Data */

/*

 * Calibrate few PHY parameters using CR_PORT register to meet

 * SuperSpeed requirements on Exynos5420 and Exynos5800 systems,

 * which have 28nm USB 3.0 DRD PHY.

	/*

	 * Change los_bias to (0x5) for 28nm PHY from a

	 * default value (0x0); los_level is set as default

	 * (0x9) as also reflected in los_level[30:26] bits

	 * of PHYPARAM0 register.

	/*

	 * Set tx_vboost_lvl to (0x5) for 28nm PHY Tuning,

	 * to raise Tx signal level from its default value of (0x4)

	/*

	 * Set proper time to wait for RxDetect measurement, for

	 * desired reference clock of PHY, by tuning the CR_PORT

	 * register LANE0.TX_DEBUG which is internal to PHY.

	 * This fixes issue with few USB 3.0 devices, which are

	 * not detected (not even generate interrupts on the bus

	 * on insertion) without this change.

	 * e.g. Samsung SUM-TSB16S 3.0 USB drive.

	/*

	 * Exynos5420 SoC has multiple channels for USB 3.0 PHY, with

	 * each having separate power control registers.

	 * 'channel' facilitates to set such registers.

 Get Vbus regulators */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Samsung S5P/Exynos SoC series MIPI CSIS/DSIM DPHY driver

 *

 * Copyright (C) 2013,2016 Samsung Electronics Co., Ltd.

 * Author: Sylwester Nawrocki <s.nawrocki@samsung.com>

 EXYNOS_MIPI_PHY_ID_CSIS0 */

 EXYNOS_MIPI_PHY_ID_DSIM0 */

 EXYNOS_MIPI_PHY_ID_CSIS1 */

 EXYNOS_MIPI_PHY_ID_DSIM1 */

 EXYNOS_MIPI_PHY_ID_CSIS0 */

 EXYNOS_MIPI_PHY_ID_DSIM0 */

 EXYNOS_MIPI_PHY_ID_CSIS1 */

 EXYNOS_MIPI_PHY_ID_DSIM1 */

 EXYNOS_MIPI_PHY_ID_CSIS2 */

 EXYNOS_MIPI_PHY_ID_CSIS0 */

 EXYNOS_MIPI_PHY_ID_DSIM0 */

 EXYNOS_MIPI_PHY_ID_CSIS1 */

 EXYNOS_MIPI_PHY_ID_DSIM1 */

 EXYNOS_MIPI_PHY_ID_CSIS2 */

 disable in PMU sysreg */

 PHY reset */

 enable in PMU sysreg */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2015 Linaro Ltd.

 * Copyright (c) 2015 HiSilicon Limited.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Phy provider for USB 3.1 controller on HiSilicon Kirin970 platform

 *

 * Copyright (C) 2017-2020 Hilisicon Electronics Co., Ltd.

 *		http://www.huawei.com

 *

 * Authors: Yu Chen <chenyu56@huawei.com>

 Clock up */

 Clock down */

 usb refclk iso disable */

 enable usb_tcxo_en */

 select usbphy clk from abb */

 assert controller */

 Exit from IDDQ mode */

 Release USB31 PHY out of TestPowerDown mode */

 Deassert phy */

 Tell the PHY power is stable */

 Enable SSC */

 Deassert controller */

 Set fake vbus valid signal */

 Assert phy */

 disable usb_tcxo_en */

 node of hi3670 phy is a sub-node of usb3_otg_bc */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2014 Linaro Ltd.

 * Copyright (c) 2014 HiSilicon Limited.

 reset phy */

 ensure PHYCTRL setting takes effect */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * HiSilicon INNO USB2 PHY Driver.

 *

 * Copyright (c) 2016-2017 HiSilicon Technologies Co., Ltd.

 unit:us */

 unit:us */

 unit:ms */

 unit:ms */

 unit:ms */

 unit:us */

 rising edge active */

 low active */

 The phy clk is controlled by the port0 register 0x06. */

 Set up phy registers */

 SPDX-License-Identifier: GPL-2.0

/*

 * Phy provider for USB 3.0 controller on HiSilicon 3660 platform

 *

 * Copyright (C) 2017-2018 Hilisicon Electronics Co., Ltd.

 *		http://www.huawei.com

 *

 * Authors: Yu Chen <chenyu56@huawei.com>

 This value config the default txtune parameter of the usb 2.0 phy */

 usb refclk iso disable */

 enable usb_tcxo_en */

 assert phy */

 enable phy ref clk */

 exit from IDDQ mode */

 delay for exit from IDDQ mode */

 deassert phy */

 delay for phy deasserted */

 fake vbus valid signal */

 delay for vbus valid */

 assert phy */

 disable usb_tcxo_en */

 node of hi3660 phy is a sub-node of usb3_otg_bc */

 SPDX-License-Identifier: GPL-2.0

/*

 * PCIe phy driver for Kirin 970

 *

 * Copyright (C) 2017 HiSilicon Electronics Co., Ltd.

 *		https://www.huawei.com

 * Copyright (C) 2021 Huawei Technologies Co., Ltd.

 *		https://www.huawei.com

 *

 * Authors:

 *	Mauro Carvalho Chehab <mchehab+huawei@kernel.org>

 *	Manivannan Sadhasivam <mani@kernel.org>

 *

 * Based on:

 *	https://lore.kernel.org/lkml/4c9d6581478aa966698758c0420933f5defab4dd.1612335031.git.mchehab+huawei@kernel.org/

 PCIe CTRL registers */

 PCIe PHY registers */

 hi3670 pciephy register */

 define ie,oe cfg */

 noc power domain */

 info located in sysctrl */

 peri_crg ctrl */

 Time for delay */

 Registers in PCIePHY */

 There's no optional eye_param property. Set array to default */

 change 2p mem_ctrl */

 output, pull down */

 Handle phy_reset and lane0_reset to HW */

 fix chip bug: TxDetectRx fail */

 pd = 0 */

 choose FNPLL */

 gt_clk_pcie_hp/gt_clk_pcie_debounce open */

 gt_clk_pcie_hp/gt_clk_pcie_debounce close */

 enable hard gt mode */

 disable hard gt mode */

 disable soft gt mode */

 set ie cfg */

 set oe cfg */

 set phy_debounce in&out time */

 select oe_gt_mode */

 en hard gt mode */

 disable soft gt mode */

 disable hard gt mode */

 disable soft gt mode */

 choose 100MHz clk src: Bit[8]==1 pad, Bit[8]==0 pll */

	/*

	 * We might just use NULL instead of the APB name, as the

	 * pcie-kirin currently registers directly just one regmap (although

	 * the DWC driver register other regmaps).

	 *

	 * Yet, it sounds safer to warrant that it will be accessing the

	 * right regmap. So, let's use the named version.

	/*

	 * The code under hi3670_pcie_get_resources_from_pcie() need to

	 * access the reset-gpios and the APB registers, both from the

	 * pcie-kirin driver.

	 *

	 * The APB is obtained via the pcie driver's regmap

	 * Such kind of resource can only be obtained during the PCIe

	 * power_on sequence, as the code inside pcie-kirin needs to

	 * be already probed, as it needs to register the APB regmap.

 Power supply for Host */

 ISO disable, PCIeCtrl, PHY assert and clk gate clear */

 pull down phy_test_powerdown signal */

 deassert controller perst_n */

 Drop power supply for Host */

	/*

	 * FIXME: The enabled clocks should be disabled here by calling

	 * kirin_pcie_clk_ctrl(phy, false);

	 * However, some clocks used at Kirin 970 should be marked as

	 * CLK_IS_CRITICAL at clk-hi3670 driver, as powering such clocks off

	 * cause an Asynchronous SError interrupt, which produces panic().

	 * While clk-hi3670 is not fixed, we cannot risk disabling clocks here.

 syscon */

 clocks */

 registers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * COMBPHY driver for HiSilicon STB SoCs

 *

 * Copyright (C) 2016-2017 HiSilicon Co., Ltd. http://www.hisilicon.com

 *

 * Authors: Jianguo Sun <sunjianguo1@huawei.com>

 Set up address and data for the write */

 Flip strobe control to trigger the write */

 Clear bypass bit to enable encoding/decoding */

 Enable EP clock */

 Need to wait for EP clock stable */

 Configure nano phy registers as suggested by vendor */

 Disable EP clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-uniphier-usb2.c - PHY driver for UniPhier USB2 controller

 * Copyright 2015-2018 Socionext Inc.

 * Author:

 *      Kunihiko Hayashi <hayashi.kunihiko@socionext.com>

 LD11 */

 Pro4 */

 Pro4 */

 Pro4 */

 Pro4 */

 get number of data */

 sentinel */ }

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-uniphier-pcie.c - PHY driver for UniPhier PCIe controller

 * Copyright 2018, Socionext Inc.

 * Author: Kunihiko Hayashi <hayashi.kunihiko@socionext.com>

 PHY */

 =1:manual */

 =1:deasssert */

 SG */

 enable for EQ adjustment */

 EQ adjustment value */

 Tx VCO adjustment value */

 need to read TESTO twice after accessing TESTI */

 read previous data */

 update value */

 read current data as dummy */

 support only 1 port */

 legacy controller doesn't have phy_reset and parameters */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-uniphier-ahci.c - PHY driver for UniPhier AHCI controller

 * Copyright 2016-2020, Socionext Inc.

 * Author: Kunihiko Hayashi <hayashi.kunihiko@socionext.com>

 for PXs2/PXs3 */

 wait until PLL is ready */

 setup port parameter */

 dummy read 25 times to make a wait time for the phy to stabilize */

 Sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-uniphier-usb3hs.c - HS-PHY driver for Socionext UniPhier USB3 controller

 * Copyright 2015-2018 Socionext Inc.

 * Author:

 *      Kunihiko Hayashi <hayashi.kunihiko@socionext.com>

 * Contributors:

 *      Motoya Tanigawa <tanigawa.motoya@socionext.com>

 *      Masami Hiramatsu <masami.hiramatsu@linaro.org>

 RX sync mode */

 RX sync length */

 LS mode slew rate */

 FS/LS slew rate */

		/*

		 * call trim_func only when trimming parameters that aren't

		 * all-zero can be acquired. All-zero parameters mean nothing

		 * has been written to nvmem.

 use default parameters without trimming values */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * phy-uniphier-usb3ss.c - SS-PHY driver for Socionext UniPhier USB3 controller

 * Copyright 2015-2018 Socionext Inc.

 * Author:

 *      Kunihiko Hayashi <hayashi.kunihiko@socionext.com>

 * Contributors:

 *      Motoya Tanigawa <tanigawa.motoya@socionext.com>

 *      Masami Hiramatsu <masami.hiramatsu@linaro.org>

 RxPLL charge pump current */

 RxPLL charge pump current 2 */

 TxPLL charge pump current */

 Bandgap voltage */

 Clock Data Recovery setting */

 VCO control */

 TxPLL VCO tuning */

 TxPLL voltage */

 need to read TESTO twice after accessing TESTI */

 read previous data */

 update value */

 read current data as dummy */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0+

 Copyright (c) 2017 NXP. */

 USB3.0 PHY signal fsel for 24M ref */

 Disable alt_clk_en and use internal MPLL clocks */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright 2017,2018 NXP

 * Copyright 2019 Purism SPC

 DPHY registers */

 PHY power on is active low */

 DPHY PLL parameters */

 DPHY register values */

/*

 * Find a ratio close to the desired one using continued fraction

 * approximation ending either at exact match or maximum allowed

 * nominator, denominator.

	/*

	 * CM ranges between 16 and 255

	 * CN ranges between 1 and 32

	 * CO is power of 2: 1, 2, 4, 8

 LP clock period */

 ps */

 hs_prepare: in lp clock periods */

 00: lp_t, 01: 1.5 * lp_t, 10: 2 * lp_t, 11: 2.5 * lp_t */

 clk_prepare: in lp clock periods */

 00: lp_t, 01: 1.5 * lp_t */

 hs_zero: formula from NXP BSP */

 clk_zero: formula from NXP BSP */

 clk_trail, hs_trail: formula from NXP BSP */

 rxhs_settle: formula from NXP BSP */

 Update the configuration */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Allwinner sun4i USB phy driver

 *

 * Copyright (C) 2014-2015 Hans de Goede <hdegoede@redhat.com>

 *

 * Based on code from

 * Allwinner Technology Co., Ltd. <www.allwinnertech.com>

 *

 * Modelled after: Samsung S5P/Exynos SoC series MIPI CSIS/DSIM DPHY driver

 * Copyright (C) 2013 Samsung Electronics Co., Ltd.

 * Author: Sylwester Nawrocki <s.nawrocki@samsung.com>

 ISCR, Interface Status and Control bits */

 sunxi has the phy id/vbus pins not connected, so we use the force bits */

 Common Control Bits for Both PHYs */

 Private Control Bits for Each PHY */

 A83T specific control bits for PHY0 */

 A83T specific control bits for PHY2 HSIC */

/*

 * Note do not raise the debounce time, we must report Vusb high within 100ms

 * otherwise we get Vbus errors

 guard access to phyctl reg */

 phy0 / otg related variables */

 SoCs newer than A33 need us to set phyctl to 0 explicitly */

 clear the address portion */

 set the address */

 set the data bit and clear usbc bit*/

 pulse usbc_bit */

 A83T USB2 is HSIC */

 Enable USB 45 Ohm resistor calibration */

 Adjust PHY's magnitude and rate */

 Disconnect threshold adjustment */

 Enable pull-ups */

 Force ISCR and cable state updates */

 Disable pull-ups */

 Fallback to peripheral mode */

 Fallback: report vbus as high */

	/*

	 * The A31/A23/A33 companion pmics (AXP221/AXP223) do not

	 * generate vbus change interrupts when the board is driving

	 * vbus using the N_VBUSEN pin on the pmic, so we must poll

	 * when using the pmic for vbus-det _and_ we're driving vbus.

 For phy0 only turn on Vbus if we don't have an ext. Vbus */

 We must report Vbus high within OTG_TIME_A_WAIT_VRISE msec. */

	/*

	 * phy0 vbus typically slowly discharges, sometimes this causes the

	 * Vbus gpio to not trigger an edge irq on Vbus off, so force a rescan.

 Force reprocessing of id */

 Host mode. Route phy0 to EHCI/OHCI */

 Peripheral mode. Route phy0 to MUSB */

 id-change, force session end if we've no vbus detection */

 When entering host mode (id = 0) force end the session now */

 When leaving host mode force end the session here */

 Enable PHY0 passby for host mode only. */

 Re-route PHY0 if necessary */

 vbus or id changed, let the pins settle and then scan them */

 Properties on the vbus_power_supply changed, scan vbus_det */

 The first PHY is always tied to OTG, and never HSIC */

 HSIC needs secondary clock */

 No pmu for musb */

 Stop detect work */

 Stop detect work */

 Stop detect work */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2016 Allwinnertech Co., Ltd.

 * Copyright (C) 2017-2018 Bootlin

 *

 * Maxime Ripard <maxime.ripard@free-electrons.com>

 SPDX-License-Identifier: GPL-2.0+

/*

 * Allwinner sun50i(H6) USB 3.0 phy driver

 *

 * Copyright (C) 2017 Icenowy Zheng <icenowy@aosc.io>

 *

 * Based on phy-sun9i-usb.c, which is:

 *

 * Copyright (C) 2014-2015 Chen-Yu Tsai <wens@csie.org>

 *

 * Based on code from Allwinner BSP, which is:

 *

 * Copyright (c) 2010-2015 Allwinner Technology Co., Ltd.

 Interface Status and Control Registers */

 USB2.0 Interface Status and Control Register */

 PIPE Clock Control Register */

 PHY External Control Register */

 PHY Tune High Register */

	/*

	 * All the magic numbers written to the PHY_TUNE_{LOW_HIGH}

	 * registers are directly taken from the BSP USB3 driver from

	 * Allwiner.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Allwinner sun9i USB phy driver

 *

 * Copyright (C) 2014-2015 Chen-Yu Tsai <wens@csie.org>

 *

 * Based on phy-sun4i-usb.c from

 * Hans de Goede <hdegoede@redhat.com>

 *

 * and code from

 * Allwinner Technology Co., Ltd. <www.allwinnertech.com>

 usb1 HSIC specific bits */

 SPDX-License-Identifier: (GPL-2.0 OR MIT)

/*

 * SerDes PHY driver for Microsemi Ocelot

 *

 * Copyright (c) 2018 Microsemi

 *

 Not used when in QSGMII or PCIe mode */

 Test pattern */

 OB + DES + IB + SER CFG */

 Wait for PLL bringup */

 Wait for calibration */

 IB CFG */

 As of now only PHY_MODE_ETHERNET is supported */

 PCIe not supported yet */

 SERDES6G(0) is the only SerDes capable of QSGMII */

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas R-Car Gen2 PHY driver

 *

 * Copyright (C) 2014 Renesas Solutions Corp.

 * Copyright (C) 2014 Cogent Embedded, Inc.

 * Copyright (C) 2019 Renesas Electronics Corp.

 From technical update */

 Low Power Status register (LPSTS) */

 USB General control register (UGCTRL) */

 USB General control register 2 (UGCTRL2) */

 USB General status register (UGSTS) */

 From technical update */

	/*

	 * Try to acquire exclusive access to PHY.  The first driver calling

	 * phy_init()  on a given channel wins, and all attempts  to use another

	 * PHY on this channel will fail until phy_exit() is called by the first

	 * driver.   Achieving this with cmpxcgh() should be SMP-safe.

 Skip if it's not USBHS */

 Power on USBHS PHY */

 Timed out waiting for the PLL lock */

 Skip if it's not USBHS */

 Power off USBHS PHY */

 Power on USBHS PHY */

 As per the data sheet wait 340 micro sec for power stable */

 Power off USBHS PHY */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas R-Car Gen3 PCIe PHY driver

 *

 * Copyright (C) 2018 Cogent Embedded, Inc.

 R8A77980 only */

 PHY control register (PHY_CTRL) */

 Power on the PCIe PHY */

 Power off the PCIe PHY */

	/*

	 * devm_phy_create() will call pm_runtime_enable(&phy->dev);

	 * And then, phy-core will manage runtime PM for this device.

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas R-Car Gen3 for USB3.0 PHY driver

 *

 * Copyright (C) 2017 Renesas Electronics Corporation

 USB30_CLKSET0 */

 USB30_CLKSET1 */

 1: reset */

 1: USB_EXTAL */

 Write B'01 */

 1: USB3S0_CLK_P */

 USB30_SSC_SET */

 USB30_PHY_ENABLE */

 USB30_VBUS_EN */

 Enables VBUS detection anyway */

	/*

	 * devm_phy_create() will call pm_runtime_enable(&phy->dev);

	 * And then, phy-core will manage runtime pm for this device.

 SPDX-License-Identifier: GPL-2.0

/*

 * Renesas R-Car Gen3 for USB2.0 PHY driver

 *

 * Copyright (C) 2015-2017 Renesas Electronics Corporation

 *

 * This is based on the phy-rcar-gen2 driver:

 * Copyright (C) 2014 Renesas Solutions Corp.

 * Copyright (C) 2014 Cogent Embedded, Inc.

****** USB2.0 Host registers (original offset is +0x200) *******/

 INT_ENABLE */

 For EHCI */

 For OHCI */

 USBCTR */

 SPD_RSM_TIMSET */

 OC_TIMSET */

 COMMCTRL */

 1 = Peripheral mode */

 OBINTSTA and OBINTEN */

 VBCTRL */

 LINECTRL1 */

 ADPCTRL */

 1 = ID sampling is enabled */

  RZ/G2L specific */

 platform_device's device */

 protects rphys[...].powered */

/*

 * Combination about is_otg_channel and uses_otg_pins:

 *

 * Parameters				|| Behaviors

 * is_otg_channel	| uses_otg_pins	|| irqs		| role sysfs

 * ---------------------+---------------++--------------+------------

 * true			| true		|| enabled	| enabled

 * true                 | false		|| disabled	| enabled

 * false                | any		|| disabled	| disabled

 is_b_device: true is B-Device. false is A-Device. */

 If current and new mode is the same, this returns the error */

 And is_host must be false */

 A-Peripheral */

 B-Peripheral */

 And is_host must be true */

 A-Host */

 B-Host */

 Should not use functions of read-modify-write a register */

 Initialize USB2 part */

 Initialize otg part */

 The powered flag should be set for any other phys anyway */

 sentinel */ },

 For old version dts */

 Prevent invalid args count */

	/*

	 * If one of device nodes has other dr_mode except UNKNOWN,

	 * this function returns UNKNOWN. To achieve backward compatibility,

	 * this loop starts the index as 0.

 get irq number here and request_irq for OTG in phy_init */

	/*

	 * devm_phy_create() will call pm_runtime_enable(&phy->dev);

	 * And then, phy-core will manage runtime pm for this device.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) Fuzhou Rockchip Electronics Co.Ltd

 * Author: Chris Zhong <zyw@rock-chips.com>

 *         Kever Yang <kever.yang@rock-chips.com>

 *

 * The ROCKCHIP Type-C PHY has two PLL clocks. The first PLL clock

 * is used for USB3, the second PLL clock is used for DP. This Type-C PHY has

 * 3 working modes: USB3 only mode, DP only mode, and USB3+DP mode.

 * At USB3 only mode, both PLL clocks need to be initialized, this allows the

 * PHY to switch mode between USB3 and USB3+DP, without disconnecting the USB

 * device.

 * In The DP only mode, only the DP PLL needs to be powered on, and the 4 lanes

 * are all used for DP.

 *

 * This driver gets extcon cable state and property, then decides which mode to

 * select:

 *

 * 1. USB3 only mode:

 *    EXTCON_USB or EXTCON_USB_HOST state is true, and

 *    EXTCON_PROP_USB_SS property is true.

 *    EXTCON_DISP_DP state is false.

 *

 * 2. DP only mode:

 *    EXTCON_DISP_DP state is true, and

 *    EXTCON_PROP_USB_SS property is false.

 *    If EXTCON_USB_HOST state is true, it is DP + USB2 mode, since the USB2 phy

 *    is a separate phy, so this case is still DP only mode.

 *

 * 3. USB3+DP mode:

 *    EXTCON_USB_HOST and EXTCON_DISP_DP are both true, and

 *    EXTCON_PROP_USB_SS property is true.

 *

 * This Type-C PHY driver supports normal and flip orientation. The orientation

 * is reported by the EXTCON_PROP_USB_TYPEC_POLARITY property: true is flip

 * orientation, false is normal orientation.

 For CMN_TXPUCAL_CTRL, CMN_TXPDCAL_CTRL */

/*

 * For CMN_TXPUCAL_CTRL, CMN_TXPDCAL_CTRL,

 *     CMN_TXPU_ADJ_CTRL, CMN_TXPDCAL_CTRL

 *

 * NOTE: some of these registers are documented to be 2's complement

 * signed numbers, but then documented to be always positive.  Weird.

 * In such a case, using CMN_CALIB_CODE_POS() avoids the unnecessary

 * sign extension.

 Use this for "n" in macros like "_MULT_XXX" to target the aux channel */

/*

 * Selects which PLL clock will be driven on the analog high speed

 * clock 0: PLL 0 div 1

 * clock 1: PLL 1 div 2

/**

 * struct rockchip_usb3phy_port_cfg - usb3-phy port configuration.

 * @reg: the base address for usb3-phy config.

 * @typec_conn_dir: the register of type-c connector direction.

 * @usb3tousb2_en: the register of type-c force usb2 to usb2 enable.

 * @external_psm: the register of type-c phy external psm clock.

 * @pipe_status: the register of type-c phy pipe status.

 * @usb3_host_disable: the register of type-c usb3 host disable.

 * @usb3_host_port: the register of type-c usb3 host port.

 * @uphy_dp_sel: the register of type-c phy DP select control.

 mutex to protect access to individual PHYs */

 sentinel */ }

	/*

	 * cmn_ref_clk_sel = 3, select the 24Mhz for clk parent

	 * cmn_psm_clk_dig_div = 2, set the clk division to 2

		/*

		 * The following PHY configuration assumes a 24 MHz reference

		 * clock.

 load the configuration of PLL0 */

 set the default mode to RBR */

 load the configuration of PLL1 */

	/*

	 * Select the polarity of the xcvr:

	 * 1, Reverses the polarity (If TYPEC, Pulls ups aux_p and pull

	 * down aux_m)

	 * 0, Normal polarity (if TYPEC, pulls up aux_m and pulls down

	 * aux_p)

	/*

	 * Calculate calibration code as per docs: use an average of the

	 * pull down and pull up.  Then add in adjustments.

 disable txda_cal_latch_en for rewrite the calibration values */

 write the calibration, then delay 10 ms as sample in docs */

	/*

	 * Enable signal for latch that sample and holds calibration values.

	 * Activate this signal for 1 clock cycle to sample new calibration

	 * values.

 set TX Voltage Level and TX Deemphasis to 0 */

 re-enable decap */

	/*

	 * Programs txda_drv_ldo_prog[15:0], Sets driver LDO

	 * voltage 16'h1001 for DP-AUX-TX and RX

 re-enables Bandgap reference for LDO */

	/*

	 * re-enables the transmitter pre-driver, driver data selection MUX,

	 * and receiver detect circuits.

	/*

	 * Do all the undocumented magic:

	 * - Turn on TXDA_DP_AUX_EN, whatever that is, even though sample

	 *   never shows this going on.

	 * - Turn on TXDA_DECAP_EN (and TXDA_DECAP_EN_DEL) even though

	 *   docs say for aux it's always 0.

	 * - Turn off the LDO and BGREF, which we just spent time turning

	 *   on above (???).

	 *

	 * Without this magic, things seem worse.

	/*

	 * Undo the work we did to set the LDO voltage.

	 * This doesn't seem to help nor hurt, but it kinda goes with the

	 * undocumented magic above.

 Don't set voltage swing to 400 mV peak to peak (differential) */

 Init TXDA_CYA_AUXDA_CYA for unknown magic reasons */

	/*

	 * More undocumented magic, presumably the goal of which is to

	 * make the "auxda_source_aux_oen" be ignored and instead to decide

	 * about "high impedance state" based on what software puts in the

	 * register TXDA_COEFF_CALC_CTRL (see TX_HIGH_Z).  Since we only

	 * program that register once and we don't set the bit TX_HIGH_Z,

	 * presumably the goal here is that we should never put the analog

	 * driver in high impedance state.

 DP-only mode; fall back to USB2 */

 wait TCPHY for pipe ready */

 enable usb3 host */

	/*

	 * If the PHY has been power on, but the mode is not DP only mode,

	 * re-init the PHY for setting all of 4 lanes to DP.

 select external psm clock */

 find out a proper config which can be matched with dt. */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Rockchip DP PHY driver

 *

 * Copyright (C) 2016 FuZhou Rockchip Co., Ltd.

 * Author: Yakir Yang <ykk@@rock-chips.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Rockchip PCIe PHY driver

 *

 * Copyright (C) 2016 Shawn Lin <shawn.lin@rock-chips.com>

 * Copyright (C) 2016 ROCKCHIP, Inc.

/*

 * The higher 16-bit of this register is used for write protection

 * only if BIT(x + 16) set to 1 the BIT(x) can be written.

	/*

	 * No documented timeout value for phy operation below,

	 * so we make it large enough here. And we use loop-break

	 * method which should not be harmful.

 parse #phy-cells to see if it's legacy PHY model */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright (c) 2017 Rockchip Electronics Co. Ltd.

 *

 * Author: Zheng Yang <zhengyang@rock-chips.com>

 *         Heiko Stuebner <heiko@sntech.de>

 REG: 0x00 */

 REG: 0x01 */

 REG: 0x02 */

 REG: 0x03 */

 REG: 0x04 */

 REG: 0xaa */

 REG: 0xe0 */

 REG: 0xe1 */

 REG: 0xe2 */

 REG: 0xe3 */

 REG: 0xe4 */

 REG: 0xe5 */

 REG: 0xe6 */

 REG: 0xe8 */

 REG: 0xe9 */

 REG: 0xea */

 REG: 0xeb */

 REG: 0xee */

 REG: 0xef */

 REG: 0xf0 */

 REG: 0xf1 */

 REG: 0xf2 */

 REG: 0x01 */

 REG: 0x02 */

 REG:0x05 */

 REG:0x07 */

 for all RK3328_INT_TMDS_*, ESD_DET as defined in 0xc8-0xcb */

 REG: 0xa0 */

 REG: 0xa1 */

 REG: 0xa2 */

 unset means center spread */

 REG: 0xa3 */

 REG: 0xa4*/

 REG: 0xa5 */

 REG: 0xa6 */

 REG: 0xa9 */

 REG: 0xaa */

 REG:0xab */

 REG: 0xac */

 REG: 0xad */

 REG: 0xaf */

 REG: 0xb0 */

 REG: 0xb2 */

 REG:0xc5 */

 REG:0xc6 */

 REG:0xc7 */

 REG 0xc8 - 0xcb */

 resistors can be used in parallel */

 REG: 0xd1 */

 REG: 0xd2 */

 REG: 0xd3 */

 platform data */

 clk provider */

 sentinel */ }

 sentinel */ }

 phy tuning values for an undocumented set of registers */

 sentinel */ },

 phy tuning values for an undocumented set of registers */

 sentinel */ },

/*

 * The register description of the IP block does not use any distinct names

 * but instead the databook simply numbers the registers in one-increments.

 * As the registers are obviously 32bit sized, the inno_* functions

 * translate the databook register names to the actual registers addresses.

 Power down PRE-PLL */

 Power up PRE-PLL */

 Wait for Pre-PLL lock */

 Configure pre-pll */

 Wait for Pre-PLL lock */

 optional override of the clock name */

	/*

	 * Use phy internal register control

	 * rxsense/poweron/pllpd/pdataen signal.

 manual power down post-PLL */

 Post-PLL update */

 Wait for post PLL lock */

	/*

	 * Use phy internal register control

	 * rxsense/poweron/pllpd/pdataen signal.

 Disable phy irq */

 try to read the chip-version */

 set ESD detection threshold for TMDS CLK, D2, D1 and D0 */

 Set termination resistor to 100ohm */

 clk termination resistor is 50ohm (parallel resistors) */

 data termination resistor for D2, D1 and D0 is 150ohm */

 Wait for post PLL lock */

 Enable PHY IRQ */

 Disable PHY IRQ */

	/*

	 * Refpclk needs to be on, on at least the rk3328 for still

	 * unknown reasons.

 only the newer rk3328 hdmiphy has an interrupt */

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 Rockchip Electronics Co. Ltd.

 *

 * Author: Wyon Bi <bivvy.bi@rock-chips.com>

/*

 * The offset address[7:0] is distributed two parts, one from the bit7 to bit5

 * is the first address, the other from the bit4 to bit0 is the second address.

 * when you configure the registers, you must set both of them. The Clock Lane

 * and Data Lane use the same registers with the same second address, but the

 * first address is different.

 Analog Register Part: reg00 */

 Analog Register Part: reg01 */

 Analog Register Part: reg03 */

 Analog Register Part: reg04 */

 Analog Register Part: reg05 */

 Analog Register Part: reg06 */

 Analog Register Part: reg07 */

 Analog Register Part: reg08 */

 Digital Register Part: reg00 */

 Digital Register Part: reg01 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg05 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg06 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg07 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg08 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg09 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg0a */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg0c */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg0d */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg0e */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg10 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg11 */

 Clock/Data0/Data1/Data2/Data3 Lane Register Part: reg12 */

 LVDS Register Part: reg00 */

 LVDS Register Part: reg01 */

 LVDS Register Part: reg03 */

 LVDS Register Part: reg0b */

	/*

	 * The PLL output frequency can be calculated using a simple formula:

	 * PLL_Output_Frequency = (FREF / PREDIV * FBDIV) / 2

	 * PLL_Output_Frequency: it is equal to DDR-Clock-Frequency * 2

 5Mhz < Fref / prediv < 40MHz */

		/*

		 * The possible settings of feedback divider are

		 * 12, 13, 14, 16, ~ 511

 Select MIPI mode */

 Configure PLL */

 Enable PLL and LDO */

 Reset analog */

 Reset digital */

	/*

	 * The value of counter for HS Ths-exit

	 * Ths-exit = Tpin_txbyteclkhs * value

	/*

	 * The value of counter for HS Tclk-post

	 * Tclk-post = Tpin_txbyteclkhs * value

	/*

	 * The value of counter for HS Tclk-pre

	 * Tclk-pre = Tpin_txbyteclkhs * value

	/*

	 * The value of counter for HS Tlpx Time

	 * Tlpx = Tpin_txbyteclkhs * (2 + value)

	/*

	 * The value of counter for HS Tta-go

	 * Tta-go for turnaround

	 * Tta-go = Ttxclkesc * value

	/*

	 * The value of counter for HS Tta-sure

	 * Tta-sure for turnaround

	 * Tta-sure = Ttxclkesc * value

	/*

	 * The value of counter for HS Tta-wait

	 * Tta-wait for turnaround

	 * Tta-wait = Ttxclkesc * value

 Enable all lanes on analog part */

 Sample clock reverse direction */

 Select LVDS mode */

 Configure PLL */

 Enable PLL and Bandgap */

 Reset LVDS digital logic */

 Enable LVDS digital logic */

 Enable LVDS analog driver */

 Bandgap power on */

 Enable power work */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Rockchip USB2.0 PHY with Innosilicon IP block driver

 *

 * Copyright (C) 2016 Fuzhou Rockchip Electronics Co., Ltd

/**

 * enum usb_chg_state - Different states involved in USB charger detection.

 * @USB_CHG_STATE_UNDEFINED:	USB charger is not connected or detection

 *				process is not yet started.

 * @USB_CHG_STATE_WAIT_FOR_DCD:	Waiting for Data pins contact.

 * @USB_CHG_STATE_DCD_DONE:	Data pin contact is detected.

 * @USB_CHG_STATE_PRIMARY_DONE:	Primary detection is completed (Detects

 *				between SDP and DCP/CDP).

 * @USB_CHG_STATE_SECONDARY_DONE: Secondary detection is completed (Detects

 *				  between DCP and CDP).

 * @USB_CHG_STATE_DETECTED:	USB charger type is determined.

/**

 * struct rockchip_chg_det_reg - usb charger detect registers

 * @cp_det: charging port detected successfully.

 * @dcp_det: dedicated charging port detected successfully.

 * @dp_det: assert data pin connect successfully.

 * @idm_sink_en: open dm sink curren.

 * @idp_sink_en: open dp sink current.

 * @idp_src_en: open dm source current.

 * @rdm_pdwn_en: open dm pull down resistor.

 * @vdm_src_en: open dm voltage source.

 * @vdp_src_en: open dp voltage source.

 * @opmode: utmi operational mode.

/**

 * struct rockchip_usb2phy_port_cfg - usb-phy port configuration.

 * @phy_sus: phy suspend register.

 * @bvalid_det_en: vbus valid rise detection enable register.

 * @bvalid_det_st: vbus valid rise detection status register.

 * @bvalid_det_clr: vbus valid rise detection clear register.

 * @ls_det_en: linestate detection enable register.

 * @ls_det_st: linestate detection state register.

 * @ls_det_clr: linestate detection clear register.

 * @utmi_avalid: utmi vbus avalid status register.

 * @utmi_bvalid: utmi vbus bvalid status register.

 * @utmi_ls: utmi linestate state register.

 * @utmi_hstdet: utmi host disconnect register.

/**

 * struct rockchip_usb2phy_cfg - usb-phy configuration.

 * @reg: the address offset of grf for usb-phy config.

 * @num_ports: specify how many ports that the phy has.

 * @clkout_ctl: keep on/turn off output clk of phy.

 * @port_cfgs: usb-phy port configurations.

 * @chg_det: charger detection registers.

/**

 * struct rockchip_usb2phy_port - usb-phy port data.

 * @phy: generic phy.

 * @port_id: flag for otg port or host port.

 * @suspended: phy suspended flag.

 * @vbus_attached: otg device vbus status.

 * @bvalid_irq: IRQ number assigned for vbus valid rise detection.

 * @ls_irq: IRQ number assigned for linestate detection.

 * @otg_mux_irq: IRQ number which multiplex otg-id/otg-bvalid/linestate

 *		 irqs to one irq in otg-port.

 * @mutex: for register updating in sm_work.

 * @chg_work: charge detect work.

 * @otg_sm_work: OTG state machine work.

 * @sm_work: HOST state machine work.

 * @port_cfg: port register configuration, assigned by driver data.

 * @event_nb: hold event notification callback.

 * @state: define OTG enumeration states before device reset.

 * @mode: the dr_mode of the controller.

/**

 * struct rockchip_usb2phy - usb2.0 phy driver data.

 * @dev: pointer to device.

 * @grf: General Register Files regmap.

 * @usbgrf: USB General Register Files regmap.

 * @clk: clock struct of phy input clk.

 * @clk480m: clock struct of phy output clk.

 * @clk480m_hw: clock struct of phy output clk management.

 * @chg_state: states involved in USB charger detection.

 * @chg_type: USB charger types.

 * @dcd_retries: The retry count used to track Data contact

 *		 detection process.

 * @edev: extcon device for notification registration

 * @phy_cfg: phy register configuration, assigned by driver data.

 * @ports: phy port instance.

 turn on 480m clk output if it is off */

 waiting for the clk become stable */

 turn off 480m clk output */

 optional override of the clockname */

 register the clock */

 Initialize extcon device */

 clear bvalid status and enable bvalid detect irq */

 If OTG works in host only mode, do nothing. */

 clear linestate and enable linestate detect irq */

 waiting for the utmi_clk to become stable */

 put the controller in non-driving mode */

 Start DCD processing stage 1 */

 get data contact detection status */

 stage 2 */

 stage 4 */

 Turn off DCD circuitry */

 Voltage Source on DP, Probe on DM */

 stage 3 */

 Voltage Source on DM, Probe on DP  */

 floating charger found */

 Turn off voltage source */

 put the controller in normal mode */

/*

 * The function manage host-phy port state and suspend/resume phy port

 * to save power.

 *

 * we rely on utmi_linestate and utmi_hostdisconnect to identify whether

 * devices is disconnect or not. Besides, we do not need care it is FS/LS

 * disconnected or HS disconnected, actually, we just only need get the

 * device is disconnected at last through rearm the delayed work,

 * to suspend the phy port in _PHY_STATE_DISCONNECT_ case.

 *

 * NOTE: It may invoke *phy_powr_off or *phy_power_on which will invoke

 * some clk related APIs, so do not invoke it from interrupt context directly.

 stitch on utmi_ls and utmi_hstdet as phy state */

		/*

		 * For FS/LS device, the online state share with connect state

		 * from utmi_ls and utmi_hstdet register, so we distinguish

		 * them via suspended flag.

		 *

		 * Plus, there are two cases, one is D- Line pull-up, and D+

		 * line pull-down, the state is 4; another is D+ line pull-up,

		 * and D- line pull-down, the state is 2.

 D- line pull-up, D+ line pull-down */

 D+ line pull-up, D- line pull-down */

		/*

		 * activate the linestate detection to get the next device

		 * plug-in irq.

		/*

		 * we don't need to rearm the delayed work when the phy port

		 * is suspended.

 disable linestate detect irq and clear its status */

	/*

	 * In this case for host phy port, a new device is plugged in,

	 * meanwhile, if the phy port is suspended, we need rearm the work to

	 * resume it and mange its states; otherwise, we do nothing about that.

 clear bvalid detect irq pending status */

	/*

	 * set suspended flag to true, but actually don't

	 * put phy in suspend mode, it aims to enable usb

	 * phy and clock in power_on() called by usb controller

	 * driver during probe.

	/*

	 * Some SoCs use one interrupt with otg-id/otg-bvalid/linestate

	 * interrupts muxed together, so probe the otg-mux interrupt first,

	 * if not found, then look for the regular interrupts one by one.

 find out a proper config which can be matched with dt. */

 This driver aims to support both otg-port and host-port */

 initialize otg/host port separately */

 to prevent out of boundary */

 sentinel */ }

 sentinel */ }

 sentinel */ }

 sentinel */ }

 sentinel */ }

 sentinel */ }

 SPDX-License-Identifier: GPL-2.0

/*

 * Rockchip MIPI RX Innosilicon DPHY driver

 *

 * Copyright (C) 2021 Fuzhou Rockchip Electronics Co., Ltd.

 GRF */

 PHY */

 not present on all variants */

 offset after ths_settle_offset */

 offset after calib_offset */

 Configure the count time of the THS-SETTLE by protocol. */

 Calibration reception enable */

/*

 * The higher 16-bit of this register is used for write protection

 * only if BIT(x + 16) set to 1 the BIT(x) can be written.

 rk1808 & rk3326 */

 These tables must be sorted by .range_h ascending. */

 pass with phy_mipi_dphy_get_default_config (with pixel rate?) */

 phy start */

 set data lane num and enable clock lane */

 Reset dphy analog part */

 Reset dphy digital part */

 not into receive mode/wait stopstate */

 enable calibration */

 disable all lanes */

 disable pll and ldo */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Rockchip usb PHY driver

 *

 * Copyright (C) 2014 Yunzhi Li <lyz@rock-chips.com>

 * Copyright (C) 2014 ROCKCHIP, Inc.

 bits present on rk3188 and rk3288 phys */

 Power down usb phy analog blocks by set siddq 1 */

 Power up usb phy analog blocks by set siddq 0 */

	/*

	 * When acting as uart-pipe, just keep clock on otherwise

	 * only power up usb phy when it use, so disable it when init

 sentinel */ }

	/*

	 * COMMON_ON and DISABLE settings are described in the TRM,

	 * but were not present in the original code.

	 * Also disable the analog phy components to save power.

/*

 * Enable the bypass of uart2 data through the otg usb phy.

 * See description of rk3288-variant for details.

 sentinel */ }

/*

 * Enable the bypass of uart2 data through the otg usb phy.

 * Original description in the TRM.

 * 1. Disable the OTG block by setting OTGDISABLE0 to 1’b1.

 * 2. Disable the pull-up resistance on the D+ line by setting

 *    OPMODE0[1:0] to 2’b01.

 * 3. To ensure that the XO, Bias, and PLL blocks are powered down in Suspend

 *    mode, set COMMONONN to 1’b1.

 * 4. Place the USB PHY in Suspend mode by setting SUSPENDM0 to 1’b0.

 * 5. Set BYPASSSEL0 to 1’b1.

 * 6. To transmit data, controls BYPASSDMEN0, and BYPASSDMDATA0.

 * To receive data, monitor FSVPLUS0.

 *

 * The actual code in the vendor kernel does some things differently.

 sentinel */ }

 SPDX-License-Identifier: (GPL-2.0+ OR MIT)

/*

 * Rockchip MIPI Synopsys DPHY RX0 driver

 *

 * Copyright (C) 2019 Collabora, Ltd.

 *

 * Based on:

 *

 * drivers/media/platform/rockchip/isp1/mipi_dphy_sy.c

 * in https://chromium.googlesource.com/chromiumos/third_party/kernel,

 * chromeos-4.4 branch.

 *

 * Copyright (C) 2017 Fuzhou Rockchip Electronics Co., Ltd.

 *   Jacob Chen <jacob2.chen@rock-chips.com>

 *   Shunqian Zheng <zhengsq@rock-chips.com>

 rk3288 only */

 below is for rk3399 only */

 Update high word */

	/*

	 * With the falling edge on TESTCLK, the TESTDIN[7:0] signal content

	 * is latched internally as the current test code. Test data is

	 * programmed internally by rising edge on TESTCLK.

	 * This code assumes that TESTCLK is already 1.

 Disable lane turn around, which is ignored in receive mode */

 dphy start */

 set clock lane */

 HS hsfreq_range & lane 0  settle bypass */

 HS RX Control of lane0 */

 HS RX Control of lane1 */

 HS RX Control of lane2 */

 HS RX Control of lane3 */

 HS RX Data Lanes Settle State Time Control */

 Normal operation */

 pass with phy_mipi_dphy_get_default_config (with pixel rate?) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Rockchip emmc PHY driver

 *

 * Copyright (C) 2016 Shawn Lin <shawn.lin@rock-chips.com>

 * Copyright (C) 2016 ROCKCHIP, Inc.

/*

 * The higher 16-bit of this register is used for write protection

 * only if BIT(x + 16) set to 1 the BIT(x) can be written.

 Register definition */

	/*

	 * Keep phyctrl_pdb and phyctrl_endll low to allow

	 * initialization of CALIO state M/C DFFs

 Already finish power_off above */

		/*

		 * In order for tuning delays to be accurate we need to be

		 * pretty spot on for the DLL range, so warn if we're too

		 * far off.  Also warn if we're above the 200 MHz max.  Don't

		 * warn for really slow rates since we won't be tuning then.

	/*

	 * According to the user manual, calpad calibration

	 * cycle takes more than 2us without the minimal recommended

	 * value, so we may need a little margin here

	/*

	 * According to the user manual, it asks driver to wait 5us for

	 * calpad busy trimming. However it is documented that this value is

	 * PVT(A.K.A process,voltage and temperature) relevant, so some

	 * failure cases are found which indicates we should be more tolerant

	 * to calpad busy trimming.

 Set the frequency of the DLL operation */

 Turn on the DLL */

	/*

	 * We turned on the DLL even though the rate was 0 because we the

	 * clock might be turned on later.  ...but we can't wait for the DLL

	 * to lock when the rate is 0 because it will never lock with no

	 * input clock.

	 *

	 * Technically we should be checking the lock later when the clock

	 * is turned on, but for now we won't.

	/*

	 * After enabling analog DLL circuits docs say that we need 10.2 us if

	 * our source clock is at 50 MHz and that lock time scales linearly

	 * with clock speed.  If we are powering on the PHY and the card clock

	 * is super slow (like 100 kHZ) this could take as long as 5.1 ms as

	 * per the math: 10.2 us * (50000000 Hz / 100000 Hz) => 5.1 ms

	 * Hopefully we won't be running at 100 kHz, but we should still make

	 * sure we wait long enough.

	 *

	 * NOTE: There appear to be corner cases where the DLL seems to take

	 * extra long to lock for reasons that aren't understood.  In some

	 * extreme cases we've seen it take up to over 10ms (!).  We'll be

	 * generous and give it 50ms.

	/*

	 * We purposely get the clock here and not in probe to avoid the

	 * circular dependency problem.  We expect:

	 * - PHY driver to probe

	 * - SDHCI driver to start probe

	 * - SDHCI driver to register it's clock

	 * - SDHCI driver to get the PHY

	 * - SDHCI driver to init the PHY

	 *

	 * The clock is optional, using clk_get_optional() to get the clock

	 * and do error processing if the return value != NULL

	 *

	 * NOTE: we don't do anything special for EPROBE_DEFER here.  Given the

	 * above expected use case, EPROBE_DEFER isn't sensible to expect, so

	 * it's just like any other error.

 Power down emmc phy analog blocks */

 Drive impedance: from DTS */

 Output tap delay: enable */

 Output tap delay */

 Internal pull-down for strobe line */

 Power up emmc phy analog blocks */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Microchip Sparx5 Switch SerDes driver

 *

 * Copyright (c) 2020 Microchip Technology Inc. and its subsidiaries.

 *

 * The Sparx5 Chip Register Model can be browsed at this location:

 * https://github.com/microchip-ung/sparx-5_reginfo

 * and the datasheet is available here:

 * https://ww1.microchip.com/downloads/en/DeviceDoc/SparX-5_Family_L2L3_Enterprise_10G_Ethernet_Switches_Datasheet_00003822B.pdf

 UDL if-width: 10/16/20/32/64 */

 Enable/disable CMU cfg */

 Device/Mode serdes uses */

 Omit initial power-cycle */

 Enable inversion of output data */

 Enable inversion of input data */

 Set output level */

 Rate of network interface */

 Set output level to  half/full */

 Mute Output Buffer */

 interface width: 10/16/20/32/64 */

 Device/Mode serdes uses */

 Rate of network interface */

 Set output level to  half/full */

 Enable/disable CMU cfg */

 Omit initial power-cycle */

 Enable inversion of output data */

 Enable inversion of input data */

 Set output level to  half/full */

 Set output level */

 Mute Output Buffer */

 ETH_MEDIA_DEFAULT */

 ETH_MEDIA_SR */

 ETH_MEDIA_DAC */

 SPX5_SD25G28_MODE_PRESET_25000 */

 SPX5_SD25G28_MODE_PRESET_10000 */

 SPX5_SD25G28_MODE_PRESET_5000 */

 SPX5_SD25G28_MODE_PRESET_SD_2G5 */

 SPX5_SD25G28_MODE_PRESET_1000BASEX */

 ETH_MEDIA_DEFAULT */

 ETH_MEDIA_SR */

 ETH_MEDIA_DAC */

 SPX5_SD10G28_MODE_PRESET_10000 */

 SPX5_SD10G28_MODE_PRESET_SFI_5000_6G */

 SPX5_SD10G28_MODE_PRESET_SFI_5000_10G */

 SPX5_SD10G28_MODE_PRESET_QSGMII */

 SPX5_SD10G28_MODE_PRESET_SD_2G5 */

 SPX5_SD10G28_MODE_PRESET_1000BASEX */

 map from SD25G28 interface width to configuration value */

 map from SD10G28 interface width to configuration value */

 Not supported */

 Note: SerDes SD10G_LANE_1 is configured in 10G_LAN mode */

 Power down serdes TX driver */

 6G and 10G */

 The same Serdes mode is used for both SGMII and 1000BaseX */

 This is for 100BASE-FX */

 0x610808000: sd_cmu_0 */

 0x610810000: sd_cmu_1 */

 0x610818000: sd_cmu_2 */

 0x610820000: sd_cmu_3 */

 0x610828000: sd_cmu_4 */

 0x610830000: sd_cmu_5 */

 0x610838000: sd_cmu_6 */

 0x610840000: sd_cmu_7 */

 0x610848000: sd_cmu_8 */

 0x610850000: sd_cmu_cfg_0 */

 0x610858000: sd_cmu_cfg_1 */

 0x610860000: sd_cmu_cfg_2 */

 0x610868000: sd_cmu_cfg_3 */

 0x610870000: sd_cmu_cfg_4 */

 0x610878000: sd_cmu_cfg_5 */

 0x610880000: sd_cmu_cfg_6 */

 0x610888000: sd_cmu_cfg_7 */

 0x610890000: sd_cmu_cfg_8 */

 0x610898000: sd6g_lane_0 */

 0x6108a0000: sd6g_lane_1 */

 0x6108a8000: sd6g_lane_2 */

 0x6108b0000: sd6g_lane_3 */

 0x6108b8000: sd6g_lane_4 */

 0x6108c0000: sd6g_lane_5 */

 0x6108c8000: sd6g_lane_6 */

 0x6108d0000: sd6g_lane_7 */

 0x6108d8000: sd6g_lane_8 */

 0x6108e0000: sd6g_lane_9 */

 0x6108e8000: sd6g_lane_10 */

 0x6108f0000: sd6g_lane_11 */

 0x6108f8000: sd6g_lane_12 */

 0x610900000: sd10g_lane_0 */

 0x610908000: sd10g_lane_1 */

 0x610910000: sd10g_lane_2 */

 0x610918000: sd10g_lane_3 */

 0x6109a8000: sd_lane_0 */

 0x6109b0000: sd_lane_1 */

 0x6109b8000: sd_lane_2 */

 0x6109c0000: sd_lane_3 */

 0x6109c8000: sd_lane_4 */

 0x6109d0000: sd_lane_5 */

 0x6109d8000: sd_lane_6 */

 0x6109e0000: sd_lane_7 */

 0x6109e8000: sd_lane_8 */

 0x6109f0000: sd_lane_9 */

 0x6109f8000: sd_lane_10 */

 0x610a00000: sd_lane_11 */

 0x610a08000: sd_lane_12 */

 0x610a10000: sd_lane_13 */

 0x610a18000: sd_lane_14 */

 0x610a20000: sd_lane_15 */

 0x610a28000: sd_lane_16 */

 0x610c08000: sd_cmu_9 */

 0x610c10000: sd_cmu_10 */

 0x610c18000: sd_cmu_11 */

 0x610c20000: sd_cmu_12 */

 0x610c28000: sd_cmu_13 */

 0x610c30000: sd_cmu_cfg_9 */

 0x610c38000: sd_cmu_cfg_10 */

 0x610c40000: sd_cmu_cfg_11 */

 0x610c48000: sd_cmu_cfg_12 */

 0x610c50000: sd_cmu_cfg_13 */

 0x610c58000: sd10g_lane_4 */

 0x610c60000: sd10g_lane_5 */

 0x610c68000: sd10g_lane_6 */

 0x610c70000: sd10g_lane_7 */

 0x610c78000: sd10g_lane_8 */

 0x610c80000: sd10g_lane_9 */

 0x610c88000: sd10g_lane_10 */

 0x610c90000: sd10g_lane_11 */

 0x610c98000: sd25g_lane_0 */

 0x610ca0000: sd25g_lane_1 */

 0x610ca8000: sd25g_lane_2 */

 0x610cb0000: sd25g_lane_3 */

 0x610cb8000: sd25g_lane_4 */

 0x610cc0000: sd25g_lane_5 */

 0x610cc8000: sd25g_lane_6 */

 0x610cd0000: sd25g_lane_7 */

 0x610d58000: sd_lane_17 */

 0x610d60000: sd_lane_18 */

 0x610d68000: sd_lane_19 */

 0x610d70000: sd_lane_20 */

 0x610d78000: sd_lane_21 */

 0x610d80000: sd_lane_22 */

 0x610d88000: sd_lane_23 */

 0x610d90000: sd_lane_24 */

 0x610d98000: sd_lane_25g_25 */

 0x610da0000: sd_lane_25g_26 */

 0x610da8000: sd_lane_25g_27 */

 0x610db0000: sd_lane_25g_28 */

 0x610db8000: sd_lane_25g_29 */

 0x610dc0000: sd_lane_25g_30 */

 0x610dc8000: sd_lane_25g_31 */

 0x610dd0000: sd_lane_25g_32 */

 Client lookup function, uses serdes index */

 Check validity: ERR_PTR(-ENODEV) if not valid */

 Get coreclock */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel Keem Bay USB PHY driver

 * Copyright (C) 2020 Intel Corporation

 USS (USB Subsystem) clock control registers */

 USS clock/reset bit fields */

 USS APB slave registers */

 Wait 30us to ensure all analog blocks are powered up. */

	/*

	 * According to Keem Bay datasheet, wait minimum 20us after clock

	 * enable before bringing PHYs out of reset.

	/*

	 * According to Keem Bay datasheet, wait 2us after disabling the

	 * clock into the USB 3.x parallel interface.

	/*

	 * According to Keem Bay datasheet, wait 20us after setting the

	 * SRAM load done bit, before releasing the controller reset.

 Setup USB subsystem clocks */

 and turn on the DWC3 core, prior to DWC3 driver init. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Intel Keem Bay eMMC PHY driver

 * Copyright (C) 2020 Intel Corporation

 eMMC/SD/SDIO core/phy configuration registers */

 From ACS_eMMC51_16nFFC_RO1100_Userguide_v1p0.pdf p17 */

	/*

	 * Keep phyctrl_pdb and phyctrl_endll low to allow

	 * initialization of CALIO state M/C DFFs

 Already finish power off above */

 Check for EMMC clock rate*/

	/*

	 * According to the user manual, calpad calibration

	 * cycle takes more than 2us without the minimal recommended

	 * value, so we may need a little margin here

	/*

	 * According to the user manual, it asks driver to wait 5us for

	 * calpad busy trimming. However it is documented that this value is

	 * PVT(A.K.A. process, voltage and temperature) relevant, so some

	 * failure cases are found which indicates we should be more tolerant

	 * to calpad busy trimming.

 Set the frequency of the DLL operation */

 Turn on the DLL */

	/*

	 * We turned on the DLL even though the rate was 0 because we the

	 * clock might be turned on later.  ...but we can't wait for the DLL

	 * to lock when the rate is 0 because it will never lock with no

	 * input clock.

	 *

	 * Technically we should be checking the lock later when the clock

	 * is turned on, but for now we won't.

	/*

	 * After enabling analog DLL circuits docs say that we need 10.2 us if

	 * our source clock is at 50 MHz and that lock time scales linearly

	 * with clock speed. If we are powering on the PHY and the card clock

	 * is super slow (like 100kHz) this could take as long as 5.1 ms as

	 * per the math: 10.2 us * (50000000 Hz / 100000 Hz) => 5.1 ms

	 * hopefully we won't be running at 100 kHz, but we should still make

	 * sure we wait long enough.

	 *

	 * NOTE: There appear to be corner cases where the DLL seems to take

	 * extra long to lock for reasons that aren't understood. In some

	 * extreme cases we've seen it take up to over 10ms (!). We'll be

	 * generous and give it 50ms.

	/*

	 * We purposely get the clock here and not in probe to avoid the

	 * circular dependency problem. We expect:

	 * - PHY driver to probe

	 * - SDHCI driver to start probe

	 * - SDHCI driver to register it's clock

	 * - SDHCI driver to get the PHY

	 * - SDHCI driver to init the PHY

	 *

	 * The clock is optional, so upon any error just return it like

	 * any other error to user.

 Delay chain based txclk: enable */

 Output tap delay: enable */

 Output tap delay */

 Power up eMMC phy analog blocks */

 Power down eMMC phy analog blocks */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel Combo-PHY driver

 *

 * Copyright (C) 2019-2020 Intel Corporation.

/*

 * Clock Register bit fields to enable clocks

 * for ComboPhy according to the mode.

 ComboPhy mode Register values */

 Register: 0 is enable, 1 is disable */

 Register: 0 is enable, 1 is disable */

 Delay for stable clock PLL */

 Delay to ensure reset process is done */

 Delay to ensure reset process is done */

 trigger auto RX adaptation */

 Wait RX adaptation to finish */

 Stop RX adaptation */

	/*

	 * syscfg and hsiocfg variables stores the handle of the registers set

	 * in which ComboPhy subsystem specific registers are subset. Using

	 * Register map framework to access the registers set.

 In dual lane mode skip phy creation for the second phy */

 SPDX-License-Identifier: GPL-2.0

/*

 * Intel eMMC PHY driver

 * Copyright (C) 2019 Intel, Corp.

 eMMC phy register definitions */

	/*

	 * Keep phyctrl_pdb and phyctrl_endll low to allow

	 * initialization of CALIO state M/C DFFs

 Already finish power_off above */

	/*

	 * According to the user manual, calpad calibration

	 * cycle takes more than 2us without the minimal recommended

	 * value, so we may need a little margin here

	/*

	 * According to the user manual, it asks driver to wait 5us for

	 * calpad busy trimming. However it is documented that this value is

	 * PVT(A.K.A process,voltage and temperature) relevant, so some

	 * failure cases are found which indicates we should be more tolerant

	 * to calpad busy trimming.

 Set the frequency of the DLL operation */

 Turn on the DLL */

	/*

	 * After enabling analog DLL circuits docs say that we need 10.2 us if

	 * our source clock is at 50 MHz and that lock time scales linearly

	 * with clock speed.  If we are powering on the PHY and the card clock

	 * is super slow (like 100 kHZ) this could take as long as 5.1 ms as

	 * per the math: 10.2 us * (50000000 Hz / 100000 Hz) => 5.1 ms

	 * Hopefully we won't be running at 100 kHz, but we should still make

	 * sure we wait long enough.

	 *

	 * NOTE: There appear to be corner cases where the DLL seems to take

	 * extra long to lock for reasons that aren't understood.  In some

	 * extreme cases we've seen it take up to over 10ms (!).  We'll be

	 * generous and give it 50ms.

	/*

	 * We purposely get the clock here and not in probe to avoid the

	 * circular dependency problem. We expect:

	 * - PHY driver to probe

	 * - SDHCI driver to start probe

	 * - SDHCI driver to register it's clock

	 * - SDHCI driver to get the PHY

	 * - SDHCI driver to init the PHY

	 *

	 * The clock is optional, so upon any error just return it like

	 * any other error to user.

	 *

 Drive impedance: 50 Ohm */

 Output tap delay: disable */

 Output tap delay */

 Power up eMMC phy analog blocks */

 Power down eMMC phy analog blocks */

 Get eMMC phy (accessed via chiptop) regmap */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 John Crispin <john@phrozen.org>

 *

 * Based on code from

 * Allwinner Technology Co., Ltd. <www.allwinnertech.com>

 enable the phy */

 setup host mode */

 deassert the reset lines */

	/*

	 * The SDK kernel had a delay of 100ms. however on device

	 * testing showed that 10ms is enough

 print some status info */

 disable the phy */

 assert the reset lines */

 The MT7628 and MT7688 require extra setup of PHY registers. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Mediatek MT7621 PCI PHY Driver

 * Author: Sergio Paracuellos <sergio.paracuellos@gmail.com>

/**

 * struct mt7621_pci_phy - Mt7621 Pcie PHY core

 * @dev: pointer to device

 * @regmap: kernel regmap pointer

 * @phy: pointer to the kernel PHY device

 * @sys_clk: pointer to the system XTAL clock

 * @port_base: base register

 * @has_dual_port: if the phy has dual ports.

 * @bypass_pipe_rst: mark if 'mt7621_bypass_pipe_rst'

 * needs to be executed. Depends on chip revision.

	/*

	 * We cannot use 'regmap_write_bits' here because internally

	 * 'set' is masked before is set to the value that will be

	 * written to the register. That way results in no reliable

	 * pci setup. Avoid to mask 'set' before set value to 'val'

	 * completely avoid the problem.

 Set PCIe Port PHY to disable SSC */

 Debug Xtal Type */

 disable port */

 40MHz Xtal */

 Set Pre-divider ratio (for host mode) */

 25MHz Xal */

 Select feedback clock */

 DDS NCPO PCW (for host mode) */

 DDS SSC dither period control */

 DDS SSC dither amplitude control */

 20MHz Xtal */

 DDS clock inversion */

 Set PLL bits */

 40MHz Xtal */

 set force mode enable of da_pe1_mstckdiv */

 Enable PHY and disable force mode */

 Disable PHY */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Lantiq XWAY SoC RCU module based USB 1.1/2.0 PHY driver

 *

 * Copyright (C) 2016 Martin Blumenstingl <martin.blumenstingl@googlemail.com>

 * Copyright (C) 2017 Hauke Mehrtens <hauke@hauke-m.de>

 Transmitter HS Pre-Emphasis Enable */

 Disconnect Threshold */

 Configure core to host mode */

 Select DMA endianness (Host-endian: big-endian) */

	/*

	 * at least the xrx200 usb2 phy requires some extra time to be

	 * operational after enabling the clock

 Reset USB core through reset controller */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PCIe PHY driver for Lantiq VRX200 and ARX300 SoCs.

 *

 * Copyright (C) 2019 Martin Blumenstingl <martin.blumenstingl@googlemail.com>

 *

 * Based on the BSP (called "UGW") driver:

 *  Copyright (C) 2009-2015 Lei Chuanhua <chuanhua.lei@lantiq.com>

 *  Copyright (C) 2016 Intel Corporation

 *

 * TODO: PHY modes other than 36MHz (without "SSC")

 PLL Setting */

 increase the bias reference voltage */

 Endcnt */

 predrv_ser_en */

 ctrl_lim */

 ctrl */

 predrv_ser_en */

 RTERM */

 Improved 100MHz clock output  */

 Reduced CDR BW to avoid glitches */

 const_sdm */

 pllmod */

 enable load_en */

 disable load_en */

 TX2 modulation */

 TX1 modulation */

 Make sure PHY PLL is stable */

 Enable PDI to access PCIe PHY register */

 Configure PLL and PHY clock */

 Enable the PCIe PHY and make PLL setting take effect */

 Check if we are in "startup ready" status */

 sentinel */ },

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 MediaTek Inc.

 * Author: Jie Qiu <jie.qiu@mediatek.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2015 MediaTek Inc.

 Power up core and enable PLL */

 Enable DSI Lane LDO outputs, disable pad tie low */

 Enable pad tie low, disable DSI Lane LDO outputs */

 Disable PLL and power down core */

 If can't get the "mipi_tx->mipitx_drive", set it default 0x8 */

 check the mipitx_drive valid */

 SPDX-License-Identifier: GPL-2.0

/*

 * MediaTek USB3.1 gen2 xsphy Driver

 *

 * Copyright (c) 2018 MediaTek Inc.

 * Author: Chunfeng Yun <chunfeng.yun@mediatek.com>

 *

 u2 phy banks */

 u3 phy shared banks */

 u3 phy banks */

 MHZ */

 reference clock of anolog phy */

 only for HQA test */

 u2 eye diagram */

 only shared u3 sif */

 MHZ, reference clock for slew rate calibrate */

 coefficient for slew rate calibrate */

 use force value */

 enable USB ring oscillator */

 wait clock stable */

 enable free run clock */

 set cycle count as 1024 */

 enable frequency meter */

 ignore return value */

 disable frequency meter */

 disable free run clock */

 (1024 / FM_OUT) x reference clock frequency x coefficient */

 if FM detection fail, set default value */

 set HS slew rate */

 disable USB ring oscillator */

 DP/DM BC1.1 path Disable */

 optional, may not exist if no u3 phys */

 get banks shared by multiple u3 phys */

 update parameters of slew rate calibrate if exist */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 MediaTek Inc.

 * Author: jitao.shi <jitao.shi@mediatek.com>

 BG_LPF_EN / BG_CORE_EN */

 Switch OFF each Lane */

 Switch ON each Lane */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 MediaTek Inc.

 * Author: Stanley Chu <stanley.chu@mediatek.com>

 mphy register and offsets */

 release DA_MP_PLL_PWR_ON */

 release DA_MP_PLL_ISO_EN */

 release DA_MP_CDR_PWR_ON */

 release DA_MP_CDR_ISO_EN */

 release DA_MP_RX0_SQ_EN */

 delay 1us to wait DIFZ stable */

 release DIFZ */

 force DIFZ */

 force DA_MP_RX0_SQ_EN */

 force DA_MP_CDR_ISO_EN */

 force DA_MP_CDR_PWR_ON */

 force DA_MP_PLL_ISO_EN */

 force DA_MP_PLL_PWR_ON */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2015 MediaTek Inc.

 * Author: Chunfeng Yun <chunfeng.yun@mediatek.com>

 *

 version V1 sub-banks offset base address */

 banks shared by multiple phys */

 shared by u3 phys */

 shared by u2 phys */

 shared by u3 phys */

 u2 phy bank */

 u3/pcie/sata phy banks */

 version V2/V3 sub-banks offset base address */

 V3: U2FREQ is not used anymore, but reserved */

 u2 phy banks */

 u3/pcie/sata phy banks */

 MHZ */

 SATA register setting */

 CDR Charge Pump P-path current adjustment */

 Symbol lock count selection */

 COMWAK GAP width window */

 COMINIT GAP width window */

 COMWAK GAP width window */

 COMINIT GAP width window */

 TX driver tail current control for 0dB de-empahsis mdoe for Gen1 speed */

 Loop filter R1 resistance adjustment for Gen1 speed */

 I-path capacitance adjustment for Gen1 */

 RX Gen1 LEQ tuning step */

 PHY switch between pcie/usb3/sgmii/sata */

 avoid RX sensitivity level degradation only for mt8173 */

	/*

	 * workaround only for mt8195, HW fix it for others of V3,

	 * u2phy should use integer mode instead of fractional mode of

	 * 48M PLL, fix it by switching PLL to 26M from default 48M

 include u3phyd_bank2 */

 include u3phya_da */

 only shared sif */

 MHZ, reference clock for slew rate calibrate */

 coefficient for slew rate calibrate */

 HW V3 doesn't support slew rate cal anymore */

 use force value */

 enable USB ring oscillator */

enable free run clock */

 set cycle count as 1024, and select u2 channel */

 enable frequency meter */

 ignore return value */

 disable frequency meter */

disable free run clock */

 ( 1024 / FM_OUT ) x reference clock frequency x coef */

 if FM detection fail, set default value */

 set HS slew rate */

 disable USB ring oscillator */

 gating PCIe Analog XTAL clock */

 gating XSQ */

 switch to USB function, and enable usb pll */

 disable switch 100uA current to SSUSB */

 DP/DM BC1.1 path Disable */

 Workaround only for mt8195, HW fix it for others (V3) */

 OTG Enable */

 OTG Disable */

 ref clk drive */

 SSC delta -5000ppm */

 change pll BW 0.6M */

 Tx Detect Rx Timing: 10us -> 5us */

 wait for PCIe subsys register to active */

 charge current adjustment */

 BC1.2 path Enable */

 type switch for usb3/pcie/sgmii/sata */

 type switch function is optional */

 <=3 */

 nothing to do, only used to set type */

 SATA phy of V1 needn't it if not shared with PCIe or USB */

 get banks shared by multiple phys */

 update parameters of slew rate calibrate if exist */

 digital (& analog) clock */

 analog clock */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 MediaTek Inc.

 * Author: jitao.shi <jitao.shi@mediatek.com>

	/*

	 * PLL PCW config

	 * PCW bit 24~30 = integer part of pcw

	 * PCW bit 0~23 = fractional part of pcw

	 * pcw = data_Rate*4*txdiv/(Ref_clk*2);

	 * Post DIV =4, so need data_Rate*4

	 * Ref_clk is 26MHz

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014 MediaTek Inc.

 * Author: Jie Qiu <jie.qiu@mediatek.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 MediaTek Inc.

 * Author: Chunhui Dai <chunhui.dai@mediatek.com>

 SPDX-License-Identifier: GPL-2.0+

/*

 * P2U (PIPE to UPHY) driver for Tegra T194 SoC

 *

 * Copyright (C) 2019 NVIDIA Corporation.

 *

 * Author: Vidya Sagar <vidyas@nvidia.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014-2020, NVIDIA CORPORATION.  All rights reserved.

 * Copyright (C) 2015 Google, Inc.

 USB2 SLEEPWALK registers */

 phase A */

 phase B */

 phase C */

 phase D */

 must be called under padctl->lock */

 must be called under padctl->lock */

 ensure sleepwalk logic is disabled */

 ensure sleepwalk logics are in low power mode */

 set debounce time */

 ensure fake events of sleepwalk logic are desiabled */

 ensure wake events of sleepwalk logic are not latched */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the pad */

 save state per speed */

 enable the trigger of the sleepwalk logic */

	/*

	 * Reset the walk pointer and clear the alarm of the sleepwalk logic,

	 * as well as capture the configuration of the USB2.0 pad.

 program electrical parameters read from XUSB PADCTL */

	/*

	 * Set up the pull-ups and pull-downs of the signals during the four

	 * stages of sleepwalk. If a device is connected, program sleepwalk

	 * logic to maintain a J and keep driving K upon seeing remote wake.

 J state: D+/D- = high/low, K state: D+/D- = low/high */

 J state: D+/D- = low/high, K state: D+/D- = high/low */

 power up the line state detectors of the pad */

 switch the electric control of the USB2.0 pad to PMC */

 set the wake signaling trigger events */

 enable the wake detection */

 disable the wake detection */

 switch the electric control of the USB2.0 pad to XUSB or USB2 */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the port */

 clear alarm of the sleepwalk logic */

 ensure sleepwalk logic is disabled */

 ensure sleepwalk logics are in low power mode */

 set debounce time */

 ensure fake events of sleepwalk logic are desiabled */

 ensure wake events of sleepwalk logic are not latched */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the port */

 save state, HSIC always comes up as HS */

 enable the trigger of the sleepwalk logic */

	/*

	 * Reset the walk pointer and clear the alarm of the sleepwalk logic,

	 * as well as capture the configuration of the USB2.0 port.

	/*

	 * Set up the pull-ups and pull-downs of the signals during the four

	 * stages of sleepwalk. Maintain a HSIC IDLE and keep driving HSIC

	 * RESUME upon remote wake.

 power up the line state detectors of the port */

 set the wake signaling trigger events */

 enable the wake detection */

 disable the wake detection */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the port */

 clear alarm of the sleepwalk logic */

			/*

			 * When port is peripheral only or role transitions to

			 * USB_ROLE_NONE from USB_ROLE_DEVICE, regulator is not

			 * be enabled.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, NVIDIA CORPORATION.  All rights reserved.

	/*

	 * TODO: move this code into the PCIe/SATA PHY ->power_on() callbacks

	 * and conditionalize based on mux function? This seems to work, but

	 * might not be the exact proper sequence.

 Enable SATA PHY when SATA lane is used */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2016-2020, NVIDIA CORPORATION.  All rights reserved.

 FUSE USB_CALIB registers */

 XUSB PADCTL registers */

 XUSB AO registers */

 phase A */

 phase B */

 phase C */

 phase D */

 phase A */

 phase B */

 phase C */

 phase D */

 UTMI bias and tracking */

 padctl context */

 USB 2.0 UTMI PHY support */

 ensure sleepwalk logic is disabled */

 ensure sleepwalk logics are in low power mode */

 set debounce time */

 ensure fake events of sleepwalk logic are desiabled */

 ensure wake events of sleepwalk logic are not latched */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the pad */

 save state per speed */

 enable the trigger of the sleepwalk logic */

	/* reset the walk pointer and clear the alarm of the sleepwalk logic,

	 * as well as capture the configuration of the USB2.0 pad

	/* setup the pull-ups and pull-downs of the signals during the four

	 * stages of sleepwalk.

	 * if device is connected, program sleepwalk logic to maintain a J and

	 * keep driving K upon seeing remote wake.

 J state: D+/D- = high/low, K state: D+/D- = low/high */

 J state: D+/D- = low/high, K state: D+/D- = high/low */

 power up the line state detectors of the pad */

 switch the electric control of the USB2.0 pad to XUSB_AO */

 set the wake signaling trigger events */

 enable the wake detection */

 disable the wake detection */

 switch the electric control of the USB2.0 pad to XUSB vcore logic */

 disable wake event triggers of sleepwalk logic */

 power down the line state detectors of the port */

 clear alarm of the sleepwalk logic */

			/*

			 * When port is peripheral only or role transitions to

			 * USB_ROLE_NONE from USB_ROLE_DEVICE, regulator is not

			 * enabled.

 TODO: pad power saving */

 TODO: pad power saving */

 SuperSpeed PHY support */

 TODO implement */

 TODO implement */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014-2020, NVIDIA CORPORATION.  All rights reserved.

 skip disabled lanes */

 XXX move this into ->probe() to avoid string comparison */

 skip single function lanes */

 choose function */

	/*

	 * USB role switch driver needs parent driver owner info. This is a

	 * suboptimal solution. TODO: Need to revisit this in a follow-up patch

	 * where an optimal solution is possible with changes to USB role

	 * switch driver.

	/*

	 * Assign phy dev to usb-phy dev. Host/device drivers can use phy

	 * reference to retrieve usb-phy details.

 populate connector entry */

 usb-role-switch property is mandatory for OTG/Peripheral modes */

	/*

	 * USB2 ports don't require additional properties, but if the port is

	 * marked as disabled there is no reason to register it.

 XXX */

	/*

	 * If there is no supplemental configuration in the device tree the

	 * port is unusable. But it is valid to configure only a single port,

	 * hence return 0 instead of an error to allow ports to be optional.

 Disable usb3_port_fake usage by default and assign if needed */

 for backwards compatibility with old device trees */

	/*

	 * This is slightly ugly. A better implementation would be to keep a

	 * registry of pad controllers, but since there will almost certainly

	 * only ever be one per SoC that would be a little overkill.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * omap-control-phy.c - The PHY part of control module.

 *

 * Copyright (C) 2013 Texas Instruments Incorporated - http://www.ti.com

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

/**

 * omap_control_pcie_pcs - set the PCS delay count

 * @dev: the control module device

 * @delay: 8 bit delay value

/**

 * omap_control_phy_power - power on/off the phy using control module reg

 * @dev: the control module device

 * @on: 0 or 1, based on powering on or off the PHY

/**

 * omap_control_usb_host_mode - set AVALID, VBUSVALID and ID pin in grounded

 * @ctrl_phy: struct omap_control_phy *

 *

 * Writes to the mailbox register to notify the usb core that a usb

 * device has been connected.

/**

 * omap_control_usb_device_mode - set AVALID, VBUSVALID and ID pin in high

 * impedance

 * @ctrl_phy: struct omap_control_phy *

 *

 * Writes to the mailbox register to notify the usb core that it has been

 * connected to a usb host.

/**

 * omap_control_usb_set_sessionend - Enable SESSIONEND and IDIG to high

 * impedance

 * @ctrl_phy: struct omap_control_phy *

 *

 * Writes to the mailbox register to notify the usb core it's now in

 * disconnected state.

/**

 * omap_control_usb_set_mode - Calls to functions to set USB in one of host mode

 * or device mode or to denote disconnected state

 * @dev: the control module device

 * @mode: The mode to which usb should be configured

 *

 * This is an API to write to the mailbox register to notify the usb core that

 * a usb device has been connected.

 SPDX-License-Identifier: GPL-2.0

/**

 * PCIe SERDES driver for AM654x SoC

 *

 * Copyright (C) 2018 - 2019 Texas Instruments Incorporated - http://www.ti.com/

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

 in microseconds */

 in microseconds */

 CMU PLL Control */

 CMU VCO bias current and VREG setting */

 AHB PMA Lane Configuration */

 AGC and Signal detect threshold for Gen3 */

 CMU Master Reset */

 P2S ring buffer initial startup pointer difference */

 Lane 1 Master Reset */

 CMU OK Status */

 Mid-speed initial calibration control */

 High-speed initial calibration control */

 Mid-speed recalibration control */

 High-speed recalibration control */

 ATT configuration */

 Edge based boost adaptation window length */

 COMRXEQ control 3 & 4 */

 COMRXEQ control 14, 15 and 16*/

 Threshold for errors in pattern data  */

 COMRXEQ control 25 */

 Mid-speed rate change calibration control */

 High-speed rate change calibration control */

 Serdes reset */

 Tx Enable Value */

 Rx Enable Value */

 PLL Enable Value */

 PLL ready for use */

 sentinel */

 Enable TX */

 Enable RX */

 Disable TX */

 Disable RX */

	/*

	 * Each combination maps to one of

	 * "Figure 12-1986. SerDes Reference Clock Distribution"

	 * in TRM.

	 /* Parent of CMU refclk, Left output, Right output

	  * either of EXT_REFCLK, LICLK, RICLK

 0000 */

 0001 */

 0010 */

 0011 */

 0100 */

 0101 */

 0110 */

 0111 */

 1000 */

 1001 */

 1010 */

 1011 */

 1100 */

 1101 */

 1110 */

 1111 */

 get existing setting */

 change parent of this clock. others left intact */

 Find the match */

		/*

		 * This can never happen, unless we missed

		 * a valid combination in serdes_am654_mux_table.

 SPDX-License-Identifier: GPL-2.0-only

/**

 * tusb1210.c - TUSB1210 USB ULPI PHY driver

 *

 * Copyright (C) 2015 Intel Corporation

 *

 * Author: Heikki Krogerus <heikki.krogerus@linux.intel.com>

 Restore the optional eye diagram optimization value */

 nothing */

	/*

	 * VENDOR_SPECIFIC2 register in TUSB1210 can be used for configuring eye

	 * diagram optimization and DP/DM swap.

 High speed output drive strength configuration */

 High speed output impedance configuration */

 DP/DM swap control */

 TUSB1210 */

 TUSB1211 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * phy-da8xx-usb - TI DaVinci DA8xx USB PHY driver

 *

 * Copyright (C) 2016 David Lechner <david@lechnology.com>

 Force VBUS valid, ID = 0 */

 Force VBUS valid, ID = 1 */

 Don't override the VBUS/ID comparators */

 SPDX-License-Identifier: GPL-2.0

/**

 * Wrapper driver for SERDES used in J721E

 *

 * Copyright (C) 2019 Texas Instruments Incorporated - http://www.ti.com/

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

 To include mux clocks, divider clocks and gate clocks */

		/*

		 * Mux value to be configured for each of the input clocks

		 * in the order populated in device tree

		/*

		 * Mux value to be configured for each of the input clocks

		 * in the order populated in device tree

 ms */

 if typec-dir gpio was specified, set LN10 SWAP bit based on that */

 use min. debounce from Type-C spec if not provided in DT  */

 Reset for each of the lane and one for the entire SERDES */

/*

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the

 * GNU General Public License for more details.

/*

 * TRM has two sets of USB_CTRL registers.. The correct register bits

 * are in TRM section 24.9.8.2 USB_CTRL Register. The TRM documents the

 * phy as being SR70LX Synopsys USB 2.0 OTG nanoPHY. It also seems at

 * least dm816x rev c ignores writes to USB_CTRL register, but the TI

 * kernel is writing to those so it's possible that later revisions

 * have worknig USB_CTRL register.

 *

 * Also note that At least USB_CTRL register seems to be dm816x specific

 * according to the TRM. It's possible that USBPHY_CTRL is more generic,

 * but that would have to be checked against the SR70LX documentation

 * which does not seem to be publicly available.

 *

 * Finally, the phy on dm814x and am335x is different from dm816x.

 1 = PLL ref clock */

 Enable the first phy */

 Enable the second phy */

 Shared between phy0 and phy1 */

 Set PLL ref clock and put phys to sleep */

	/*

	 * TI kernel sets these values for "symmetrical eye diagram and

	 * better signal quality" so let's assume somebody checked the

	 * values with a scope and set them here too.

	/*

	 * Note that at least dm816x rev c does not seem to do

	 * anything with the USB_CTRL register. But let's follow

	 * what the TI tree is doing in case later revisions use

	 * USB_CTRL.

	/*

	 * According to sprs614e.pdf, the first usb_ctrl is shared and

	 * the second instance for usb_ctrl is reserved.. Also the

	 * register bits are different from earlier TRMs.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * phy-ti-pipe3 - PIPE3 PHY driver.

 *

 * Copyright (C) 2013 Texas Instruments Incorporated - http://www.ti.com

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

/*

 * This is an Empirical value that works, need to confirm the actual

 * value required for the PIPE3PHY_PLL_CONFIGURATION2.PLL_IDLE status

 * to be correctly reflected in the PIPE3PHY_PLL_STATUS register.

 in milliseconds */

 in milliseconds */

 ctrl. reg. acces */

 ctrl. reg. acces */

 ctrl. reg. acces */

 reg. index within syscon */

 power reg. index within syscon */

 pcs reg. index in syscon */

 12 MHz */

 16.8 MHz */

 19.2 MHz */

 20 MHz */

 26 MHz */

 38.4 MHz */

 Terminator */

 12 MHz */

 16.8 MHz */

 19.2 MHz */

 20 MHz */

 26 MHz */

 38.4 MHz */

 Terminator */

 DRA75x TRM Table 26-17 Preferred USB3_PHY_RX SCP Register Settings */

 DRA75x TRM Table 26-9 Preferred SATA_PHY_RX SCP Register Settings */

 Not in TRM preferred settings */

 Not in TRM preferred settings */

 for 1.5 GHz DPLL clock */

 DRA75x TRM Table 26-62 Preferred PCIe_PHY_RX SCP Register Settings */

	/*

	 * For PCIe, TX and RX must be powered on simultaneously.

	 * For USB and SATA, TX must be powered on before RX

	/*

	 * Set pcie_pcs register to 0x96 for proper functioning of phy

	 * as recommended in AM572x TRM SPRUHZ6, section 18.5.2.2, table

	 * 18-1804.

 Bring it out of IDLE if it is IDLE */

 SATA has issues if re-programmed when locked */

 Program the DPLL */

	/* If dpll_reset_syscon is not present we wont power down SATA DPLL

	 * due to Errata i783

 PCIe doesn't have internal DPLL */

 Put DPLL in IDLE mode */

 wait for LDO and Oscillator to power down */

 i783: SATA needs control bit toggle after PLL unlock */

		/* older DTBs have missing refclk in SATA PHY

		 * so don't bail out in case of SATA PHY.

	/*

	 * Prevent auto-disable of refclk for SATA PHY due to Errata i783

 SPDX-License-Identifier: GPL-2.0

/*

 * Texas Instruments CPSW Port's PHY Interface Mode selection Driver

 *

 * Copyright (C) 2018 Texas Instruments Incorporated - http://www.ti.com/

 *

 * Based on cpsw-phy-sel.c driver created by Mugunthan V N <mugunthanvnm@ti.com>

 AM33xx SoC specific definitions for the CONTROL port */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * omap-usb2.c - USB PHY, talking to USB controller on TI SoCs.

 *

 * Copyright (C) 2012-2020 Texas Instruments Incorporated - http://www.ti.com

 * Author: Kishon Vijay Abraham I <kishon@ti.com>

 SoC Specific USB2_OTG register definitions */

 Driver Flags */

 ctrl. reg. acces */

 power reg. index within syscon */

/**

 * omap_usb2_set_comparator - links the comparator present in the system with

 *	this phy

 * @comparator - the companion phy(comparator) for this phy

 *

 * The phy companion driver should call this API passing the phy_companion

 * filled with set_vbus and start_srp to be used by usb phy.

 *

 * For use by phy companion driver

		/*

		 *

		 * Reduce the sensitivity of internal PHY by enabling the

		 * DISCON_BYP_LATCH of the USB2PHY_ANA_CONFIG1 register. This

		 * resolves issues with certain devices which can otherwise

		 * be prone to false disconnects.

		 *

 sentinel */ }

	/*

	 * Errata i2075: USB2PHY: USB2PHY Charger Detect is Enabled by

	 * Default Without VBUS Presence.

	 *

	 * AM654x SR1.0 has a silicon bug due to which D+ is pulled high after

	 * POR, which could cause enumeration failure with some USB hubs.

	 * Disabling the USB2_PHY Charger Detect function will put D+

	 * into the normal state.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * twl4030_usb - TWL4030 USB transceiver, talking to OMAP OTG controller

 *

 * Copyright (C) 2004-2007 Texas Instruments

 * Copyright (C) 2008 Nokia Corporation

 * Contact: Felipe Balbi <felipe.balbi@nokia.com>

 *

 * Current status:

 *	- HS USB ULPI mode works.

 *	- 3-pin mode support may be added in future.

 Register defines */

 not valid for "latch" reg */

 not valid for "latch" reg */

 not valid for "fall" regs */

 bits 0 and 1 */

 following registers do not have separate _clr and _set registers */

 In module TWL_MODULE_PM_MASTER */

 In module TWL_MODULE_PM_RECEIVER */

 In module TWL4030_MODULE_INTBR */

/*

 * If VBUS is valid or ID is ground, then we know a

 * cable is present and we need to be runtime-enabled

 TWL4030 internal USB regulator supplies */

 for vbus reporting with irqs disabled */

 pin configuration */

 internal define on top of container_of */

-------------------------------------------------------------------------*/

 Failed once: Try again */

 Failed again: Return error */

-------------------------------------------------------------------------*/

-------------------------------------------------------------------------*/

		/*

		 * if clocks are off, registers are not updated,

		 * but we can assume we don't drive VBUS in this case

	/*

	 * For ID/VBUS sensing, see manual section 15.4.8 ...

	 * except when using only battery backup power, two

	 * comparators produce VBUS_PRES and ID_PRES signals,

	 * which don't match docs elsewhere.  But ... BIT(7)

	 * and BIT(2) of STS_HW_CONDITIONS, respectively, do

	 * seem to match up.  If either is true the USB_PRES

	 * signal is active, the OTG module is activated, and

	 * its interrupt may be raised (may wake the system).

	/* REVISIT this assumes host and peripheral controllers

	 * are registered, and that both are active...

 FIXME: power on defaults */

 enable DPLL to access PHY registers over I2C */

 let ULPI control the DPLL clock */

	/*

	 * we need enabled runtime on resume,

	 * so turn irq off here, so we do not get it early

	 * note: wakeup on usb plug works independently of this

 check whether cable status changed */

	/*

	 * Disabling usb3v1 regulator (= writing 0 to VUSB3V1_DEV_GRP

	 * in twl4030) resets the VUSB_DEDICATED2 register. This reset

	 * enables VUSB3V1_SLEEP bit that remaps usb3v1 ACTIVE state to

	 * SLEEP. We work around this by clearing the bit after usv3v1

	 * is re-activated. This ensures that VUSB3V1 is really active.

	/*

	 * According to the TPS65950 TRM, there has to be at least 50ms

	 * delay between setting POWER_CTRL_OTG_ENAB and enabling charging

	 * so wait here so that a fully enabled phy can be expected after

	 * resume

 Enable writing to power configuration registers */

 Keep VUSB3V1 LDO in sleep state until VBUS/ID change detected*/

twl_i2c_write_u8(TWL_MODULE_PM_RECEIVER, 0, VUSB_DEDICATED2);*/

 input to VUSB3V1 LDO is from VBAT, not VBUS */

 Initialize 3.1V regulator */

 Initialize 1.5V regulator */

 Initialize 1.8V regulator */

 disable access to power configuration registers */

 don't schedule during sleep - irq works right then */

 init mutex for workqueue */

	/* Our job is to use irqs and status from the power module

	 * to keep the transceiver disabled when nothing's connected.

	 *

	 * FIXME we actually shouldn't start enabling it until the

	 * USB controller drivers have said they're ready, by calling

	 * set_host() and/or set_peripheral() ... OTG_capable boards

	 * need both handles, otherwise just one suffices.

 set transceiver mode to power on defaults */

 idle ulpi before powering off */

	/* autogate 60MHz ULPI clock,

	 * clear dpll clock request for i2c access,

	 * disable 32KHz

 disable complete OTG block */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST spear1340-miphy driver

 *

 * Copyright (C) 2014 ST Microelectronics

 * Pratyush Anand <pratyush.anand@gmail.com>

 * Mohit Kumar <mohit.kumar.dhaka@gmail.com>

 SPEAr1340 Registers */

 Power Management Registers */

 PCIE - SATA configuration registers */

 PCIE CFG MASks */

 phy mode: 0 for SATA 1 for PCIe */

 regmap for any soc specific misc registers */

 phy struct pointer */

 Switch on sata power domain */

 Wait for SATA power domain on */

 Disable PCIE SATA Controller reset */

 Wait for SATA reset de-assert completion */

 Enable PCIE SATA Controller reset */

 Wait for SATA power domain off */

 Switch off sata power domain */

 Wait for SATA reset assert completion */

 SPDX-License-Identifier: GPL-2.0

/*

 * STMicroelectronics STM32 USB PHY Controller driver

 *

 * Copyright (C) 2018 STMicroelectronics

 * Author(s): Amelie Delaunay <amelie.delaunay@st.com>.

 STM32_USBPHYC_PLL bit fields */

 STM32_USBPHYC_MISC bit fields */

 STM32_USBPHYC_MONITOR bit fields */

 STM32_USBPHYC_TUNE bit fields */

 STM32_USBPHYC_VERSION bit fields */

	/*    _

	 *   | FVCO = INFF*2*(NDIV + FRACT/2^16) when DITHER_DISABLE[1] = 1

	 *   | FVCO = 2880MHz

	 *  <

	 *   | NDIV = integer part of input bits to set the LDF

	 *   |_FRACT = fractional part of input bits to set the LDF

	 *  =>	PLLNDIV = integer part of (FVCO / (INFF*2))

	 *  =>	PLLFRACIN = fractional part of(FVCO / INFF*2) * 2^16

	 * <=>  PLLFRACIN = ((FVCO / (INFF*2)) - PLLNDIV) * 2^16

 Wait for minimum width of powerdown pulse (ENABLE = Low) */

 Check if a phy port is still active or clk48 in use */

	/*

	 * Check if a phy port or clk48 prepare has configured the pll

	 * and ensure the PLL is enabled

		/*

		 * PLL shouldn't be enabled without known consumer,

		 * disable it and reinit n_pll_cons

 Check that PLL Lock input to PHY is High */

 Backup OTP compensation code */

 Decreases HS driver DC level */

 Increases HS driver DC level */

 Restore OTP compensation code */

	/*

	 * By default, if no st,xxx tuning property is used, usbphyc_phy->tune is equal to

	 * STM32_USBPHYC_TUNE reset value (LFSCAPEN | SHTCCTCTLPROT | OTPCOMP).

 Configure the UTMI switch for PHY port#2 */

	/*

	 * Wait for minimum width of powerdown pulse (ENABLE = Low):

	 * we have to ensure the PLL is disabled before phys initialization.

 Configure phy tuning */

 Ensure PHYs are not active, to allow PLL disabling */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 STMicroelectronics

 *

 * STMicroelectronics Generic PHY driver for STiH407 USB2.

 *

 * Author: Giuseppe Cavallaro <peppe.cavallaro@st.com>

 Default PHY_SEL and REFCLKSEL configuration */

 ports parameters overriding */

	/*

	 * Only port reset is asserted, phy global reset is kept untouched

	 * as other ports may still be active. When all ports are in reset

	 * state, assumption is made that power will be cut off on the phy, in

	 * case of suspend for instance. Theoretically, asserting individual

	 * reset (like here) or global reset should be equivalent.

 Reset port by default: only deassert it in phy init */

sentinel */ },

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ST SPEAr1310-miphy driver

 *

 * Copyright (C) 2014 ST Microelectronics

 * Pratyush Anand <pratyush.anand@gmail.com>

 * Mohit Kumar <mohit.kumar.dhaka@gmail.com>

 SPEAr1310 Registers */

 instance id of this phy */

 phy mode: 0 for SATA 1 for PCIe */

 regmap for any soc specific misc registers */

 phy struct pointer */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014 STMicroelectronics

 *

 * STMicroelectronics PHY driver MiPHY28lp (for SoC STiH407).

 *

 * Author: Alexandre Torgue <alexandre.torgue@st.com>

 MiPHY registers */

/*

 * On STiH407 the glue logic can be different among MiPHY devices; for example:

 * MiPHY0: OSC_FORCE_EXT means:

 *  0: 30MHz crystal clk - 1: 100MHz ext clk routed through MiPHY1

 * MiPHY1: OSC_FORCE_EXT means:

 *  1: 30MHz crystal clk - 0: 100MHz ext clk routed through MiPHY1

 * Some devices have not the possibility to check if the osc is ready.

 SATA / PCIe defines */

 Sysconfig registers offsets needed to configure the device */

 Putting Macro in reset */

 Bringing the MIPHY-CPU registers out of reset */

 Applying PLL Settings */

 PLL Ratio */

 Banked settings */

 TX buffer Settings */

 RX Buffer Settings */

 Banked settings */

 TX buffer Settings */

 RX Buffer Settings */

 Waiting for Compensation to complete */

 Poll for HFC ready after reset release */

 Compensation measurement */

 TX compensation offset to re-center TX impedance */

 MIPHY Reset */

 Compensate Tx impedance to avoid out of range values */

	/*

	 * Enable the SSC on PLL for all banks

	 * SSC Modulation @ 31 KHz and 4000 ppm modulation amp

 Add value to each reference clock cycle  */

 and define the period length of the SSC */

 Clear any previous request */

 requests the PLL to take in account new parameters */

 To be sure there is no other pending requests */

 Compensate Tx impedance to avoid out of range values */

	/*

	 * Enable the SSC on PLL for all banks

	 * SSC Modulation @ 31 KHz and 4000 ppm modulation amp

 Validate Step component */

 Validate Period component */

 Clear any previous request */

 requests the PLL to take in account new parameters */

 To be sure there is no other pending requests */

 Compensate Tx impedance to avoid out of range values */

 Putting Macro in reset */

 PLL calibration */

 Banked settings Gen1/Gen2/Gen3 */

 Power control */

 Input bridge enable, manual input bridge control */

 Macro out of reset */

 Poll for HFC ready after reset release */

 Compensation measurement */

 Invert Rx polarity */

 Putting Macro in reset */

 PLL calibration */

 Banked settings Gen1/Gen2 */

 Power control */

 Input bridge enable, manual input bridge control */

 Macro out of reset */

 Poll for HFC ready after reset release */

 Compensation measurement */

 Putting Macro in reset */

 PLL calibration */

 Writing The Speed Rate */

 RX Channel compensation and calibration */

 TX compensation offset to re-center TX impedance */

 Enable GENSEL_SEL and SSC */

 TX_SEL=0 swing preemp forced by pipe registres */

 MIPHY Bias boost */

 SSC modulation */

 MIPHY TX control */

 Validate Step component */

 Validate Period component */

 Clear any previous request */

 requests the PLL to take in account new parameters */

 To be sure there is no other pending requests */

 Rx PI controller settings */

 MIPHY RX input bridge control */

 INPUT_BRIDGE_EN_SW=1, manual input bridge control[0]=1 */

 MIPHY Reset for usb3 */

	/*

	 * For PCIe and USB3 check only that PLL and HFC are ready

	 * For SATA check also that phy is ready!

 MiPHY reset and sysconf setup */

 Configure the glue-logic */

 MiPHY path and clocking init */

 initialize miphy */

 Configure the glue-logic */

 MiPHY path and clocking init */

 initialize miphy */

 PIPE Wrapper Configuration */

 Rise_0 */

 Rise_1 */

 Fall_0 */

 Fall-1 */

 Threshold_0 */

 Threshold_1 */

 Wait for phy_ready */

 MiPHY path and clocking init */

 initialize miphy */

 PIPE Wrapper Configuration */

 pipe Wrapper usb3 TX swing de-emph margin PREEMPH[7:4], SWING[3:0] */

 SPDX-License-Identifier: GPL-2.0

/*

 * Cadence Sierra PHY Driver

 *

 * Copyright (c) 2018 Cadence Design Systems

 * Author: Alan Douglas <adouglas@cadence.com>

 *

 PHY register offsets */

 Initialise the PHY registers, unless auto configured */

 Take the PHY lane group out of reset */

 Get init data for this PHY */

 Enable APB */

 Check that PHY is present */

 If more than one subnode, configure the PHY as multilink */

	/*

	 * The device level resets will be put automatically.

	 * Need to put the subnode resets here though.

 refclk100MHz_32b_PCIe_cmn_pll_ext_ssc */

 refclk100MHz_32b_PCIe_ln_ext_ssc */

 refclk100MHz_20b_USB_cmn_pll_ext_ssc */

 refclk100MHz_20b_USB_ln_ext_ssc */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Cadence Torrent SD0801 PHY driver.

 *

 * Copyright 2018 Cadence Design Systems, Inc.

 *

 in Mbps */

/*

 * register offsets from DPTX PHY register block base (i.e MHDP

 * register base + 0x30a00)

/*

 * register offsets from SD0801 PHY register block base (i.e MHDP

 * register base + 0x500000)

 PMA TX Lane registers */

 PMA RX Lane registers */

 PHY PCS common registers */

 PHY PCS lane registers */

 PHY PMA common registers */

 DPTX registers base */

 SD0801 registers base */

 Maximum link bit rate to use (in Mbps) */

 Powerstate A1 is unused */

 PHY mmr access functions */

 DPTX mmr access functions */

/*

 * Structure used to store values of PHY registers for voltage-related

 * coefficients, for particular voltage swing and pre-emphasis level. Values

 * are shared across all physical lanes.

 Value of DRV_DIAG_TX_DRV register to use */

 Value of TX_TXCC_MGNFS_MULT_000 register to use */

 Value of TX_TXCC_CPOST_MULT_00 register to use */

/*

 * Array consists of values of voltage-related registers for sd0801 PHY. A value

 * of 0xFFFF is a placeholder for invalid combination, and will never be used.

 voltage swing 0, pre-emphasis 0->3 */

 voltage swing 1, pre-emphasis 0->3 */

 voltage swing 2, pre-emphasis 0->3 */

 voltage swing 3, pre-emphasis 0->3 */

/*

 * Set registers responsible for enabling and configuring SSC, with second and

 * third register values provided by parameters.

 Assumes 19.2 MHz refclock */

 Setting VCO for 10.8GHz */

 Setting VCO for 9.72GHz */

 Setting VCO for 8.64GHz */

 Setting VCO for 8.1GHz */

 Set reset register values to disable SSC */

/*

 * Set registers responsible for enabling and configuring SSC, with second

 * register value provided by a parameter.

 Assumes 25 MHz refclock */

 Setting VCO for 10.8GHz */

 Setting VCO for 9.72GHz */

 Setting VCO for 8.64GHz */

 Setting VCO for 8.1GHz */

 Set reset register values to disable SSC */

 Assumes 100 MHz refclock */

 Setting VCO for 10.8GHz */

 Setting VCO for 9.72GHz */

 Setting VCO for 8.64GHz */

 Setting VCO for 8.1GHz */

/*

 * Enable or disable PLL for selected lanes.

	/*

	 * Used to determine, which bits to check for or enable in

	 * PHY_PMA_XCVR_PLLCLK_EN register.

 Used to enable or disable lanes. */

	/* Select values of registers and mask, depending on enabled lane

	 * count.

 lane 0 */

 lanes 0-1 */

 lanes 0-3, all */

 Wait for acknowledgment from PHY. */

 Register value for power state for a single byte. */

 Powerstate A3 */

	/* Select values of registers and mask, depending on enabled

	 * lane count.

 lane 0 */

 lanes 0-1 */

 lanes 0-3, all */

 Set power state A<n>. */

 Wait, until PHY acknowledges power state completion. */

	/*

	 * waiting for ACK of pma_xcvr_pllclk_en_ln_*, only for the

	 * master lane

 PMA lane configuration to deal with multi-link operation */

/*

 * Perform register operations related to setting link rate, once powerstate is

 * set and PLL disable request was processed.

 Disable the cmn_pll0_en before re-programming the new data rate. */

	/*

	 * Wait for PLL ready de-assertion.

	 * For PLL0 - PHY_PMA_CMN_CTRL2[2] == 1

 DP Rate Change - VCO Output settings. */

 PMA common configuration 19.2MHz */

 PMA common configuration 25MHz */

 PMA common configuration 100MHz */

 Enable the cmn_pll0_en. */

	/*

	 * Wait for PLL ready assertion.

	 * For PLL0 - PHY_PMA_CMN_CTRL2[0] == 1

/*

 * Verify, that parameters to configure PHY with are correct.

 If changing link rate was required, verify it's supported. */

 valid bit rate */

 Verify lane count. */

 valid lane count. */

 Check against actual number of PHY's lanes. */

	/*

	 * If changing voltages is required, check swing and pre-emphasis

	 * levels, per-lane.

 Lane count verified previously. */

			/* Sum of voltage swing and pre-emphasis levels cannot

			 * exceed 3.

 Set power state A0 and PLL clock enable to 0 on enabled lanes. */

 Lane 0 is always enabled. */

 lane 1 */

 lanes 2 and 3 */

 Configure lane count as required. */

 clear pma_tx_elec_idle_ln_* bits. */

 Assert pma_tx_elec_idle_ln_* for disabled lanes. */

 reset the link by asserting phy_l00_reset_n low */

	/*

	 * Assert lane reset on unused lanes and lane 0 so they remain in reset

	 * and powered down when re-enabling the link

 release phy_l0*_reset_n based on used laneCount */

 Wait, until PHY gets ready after releasing PHY reset signal. */

 release pma_xcvr_pllclk_en_ln_*, only for the master lane */

 Configure link rate as required. */

 Configure voltage swing and pre-emphasis for all enabled lanes. */

		/*

		 * Write 1 to register bit TX_DIAG_ACYA[0] to freeze the

		 * current state of the analog TX driver.

		/*

		 * Write 0 to register bit TX_DIAG_ACYA[0] to allow the state of

		 * analog TX driver to reflect the new programmed one.

 Take the PHY lane group out of reset */

 Take the PHY out of reset */

	/*

	 * Wait for cmn_ready assertion

	 * PHY_PMA_CMN_CTRL1[0] == 1

 enable AUX */

	/*

	 * Set lines power state to A0

	 * Set lines pll clk enable to 0

	/*

	 * release phy_l0*_reset_n and pma_tx_elec_idle_ln_* based on

	 * used lanes

 release pma_xcvr_pllclk_en_ln_*, only for the master lane */

	/*

	 * PHY PMA registers configuration functions

	 * Initialize PHY with max supported link rate, without SSC.

 take out of reset */

 Valid Ref Clock Rate */

 Enable Derived reference clock as default */

	/**

	 * Spread spectrum generation is not required or supported

	 * for SGMII/QSGMII

 PHY configuration specific registers for single link */

		/**

		 * First array value in link_cmn_vals must be of

		 * PHY_PLL_CFG register

 PHY PCS common registers configurations */

 PMA common registers configurations */

 PMA TX lane registers configurations */

 PMA RX lane registers configurations */

 Give 5ms to 10ms delay for the PIPE clock to be stable */

 Maximum 2 links (subnodes) are supported */

	/**

	 * First configure the PHY for first link with phy_t1. Get the array

	 * values as [phy_t1][phy_t2][ssc].

			/**

			 * If first link with phy_t1 is configured, then

			 * configure the PHY for second link with phy_t2.

			 * Get the array values as [phy_t2][phy_t1][ssc].

		/**

		 * PHY configuration specific registers:

		 * link_cmn_vals depend on combination of PHY types being

		 * configured and are common for both PHY types, so array

		 * values should be same for [phy_t1][phy_t2][ssc] and

		 * [phy_t2][phy_t1][ssc].

		 * xcvr_diag_vals also depend on combination of PHY types

		 * being configured, but these can be different for particular

		 * PHY type and are per lane.

			/**

			 * First array value in link_cmn_vals must be of

			 * PHY_PLL_CFG register

 PHY PCS common registers configurations */

 PMA common registers configurations */

 PMA TX lane registers configurations */

 PMA RX lane registers configurations */

 Take the PHY out of reset */

 Get init data for this PHY */

 Enable APB */

 PHY subnode name must be 'phy'. */

 Get SSC mode */

 valid number of lanes */

 valid bit rate */

 DPTX registers */

 Single DisplayPort(DP) link configuration */

 Single DP, 19.2 MHz Ref clk, no SSC */

 Single DP, 25 MHz Ref clk, no SSC */

 Single DP, 100 MHz Ref clk, no SSC */

 USB and SGMII/QSGMII link configuration */

 PCIe and USB Unique SSC link configuration */

 USB 100 MHz Ref clk, internal SSC */

 Single USB link configuration */

 USB PHY PCS common configuration */

 USB 100 MHz Ref clk, no SSC */

 Single link USB, 100 MHz Ref clk, internal SSC */

 PCIe and SGMII/QSGMII Unique SSC link configuration */

 SGMII 100 MHz Ref clk, no SSC */

 SGMII 100 MHz Ref clk, internal SSC */

 QSGMII 100 MHz Ref clk, no SSC */

 QSGMII 100 MHz Ref clk, internal SSC */

 Single SGMII/QSGMII link configuration */

 Multi link PCIe, 100 MHz Ref clk, internal SSC */

 Single link PCIe, 100 MHz Ref clk, internal SSC */

 PCIe, 100 MHz Ref clk, no SSC & external SSC */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Salvo PHY is a 28nm PHY, it is a legacy PHY, and only

 * for USB3 and USB2.

 *

 * Copyright (c) 2019-2020 NXP

 PHY register definition */

 TB_ADDR_TX_RCVDETSC_CTRL */

/*

 * Below bringup sequence pair are from Cadence PHY's User Guide

 * and NXP platform tuning results.

 Change rx detect parameter */

 RXDET_IN_P3_32KHZ, Receiver detect slow clock enable */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Copyright: 2017-2018 Cadence Design Systems, Inc.

 DPHY registers */

 Default wakeup time is 800 ns (in a simulated environment). */

/*

 * This is the reference implementation of DPHY hooks. Specific integration of

 * this IP may have to re-implement some of them depending on how they decided

 * to wire things in the SoC.

	/*

	 * Configure the internal PSM clk divider so that the DPHY has a

	 * 1MHz clk (or something close).

	/*

	 * Configure attach clk lanes to data lanes: the DPHY has 2 clk lanes

	 * and 8 data lanes, each clk lane can be attache different set of

	 * data lanes. The 2 groups are named 'left' and 'right', so here we

	 * just say that we want the 'left' clk lane to drive the 'left' data

	 * lanes.

	/*

	 * Configure the DPHY PLL that will be used to generate the TX byte

	 * clk.

 Start TX state machine. */

 sentinel */ },

/*

 * Motorola CPCAP PMIC USB PHY driver

 * Copyright (C) 2017 Tony Lindgren <tony@atomide.com>

 *

 * Some parts based on earlier Motorola Linux kernel tree code in

 * board-mapphone-usb.c and cpcap-usb-det.c:

 * Copyright (C) 2007 - 2011 Motorola, Inc.

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of the GNU General Public License as

 * published by the Free Software Foundation version 2.

 *

 * This program is distributed "as is" WITHOUT ANY WARRANTY of any

 * kind, whether express or implied; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the

 * GNU General Public License for more details.

 CPCAP_REG_USBC1 register bits */

 CPCAP_REG_USBC2 register bits */

 CPCAP_REG_USBC3 register bits */

 Seems to disable USB lines */

 We need to kick the VBUS as USB A-host */

 No VBUS needed with docks */

		/*

		 * Force check state again after musb has reoriented,

		 * otherwise devices won't enumerate after loading PHY

		 * driver.

 Otherwise assume we're connected to a USB host */

 Default to debug UART mode */

 REG_INT_0 */

 REG_INT1 */

 REG_INT_3 */

/*

 * Optional pins and modes. At least Motorola mapphone devices

 * are using two GPIOs and dynamic pinctrl to multiplex PHY pins

 * to UART, ULPI or UTMI mode.

 Disable lines to prevent glitches from waking up mdm6600 */

 Enable UART mode */

 Disable lines to prevent glitches from waking up mdm6600 */

 Enable USB mode */

 SPDX-License-Identifier: GPL-2.0

/*

 * Motorola Mapphone MDM6600 modem GPIO controlled USB PHY driver

 * Copyright (C) 2018 Tony Lindgren <tony@atomide.com>

 PHY enable 2.2s to 3.5s */

 8s more total for MDM6600 */

 time on after GPIO toggle */

 modem after USB suspend */

 modem response after idle */

 USB PHY enable */

 Device power */

 Device reset */

 out USB mode0 and OOB wake */

 out USB mode1, in OOB wake */

/*

 * MDM6600 command codes. These are based on Motorola Mapphone Linux

 * kernel tree.

 Reroute USB to CPCAP PHY */

 Reroute USB to CPCAP PHY */

 Request normal USB mode */

 Request device power off */

/*

 * MDM6600 status codes. These are based on Motorola Mapphone Linux

 * kernel tree.

 Seems to be really off */

 MDM6600 USB flashing mode */

 MDM6600 normal USB mode */

 mdm6600 phy enabled */

 mdm6600 boot done */

 mdm6600 respnds on n_gsm */

 Allow aggressive PM for USB, it's only needed for n_gsm port */

 Paired with phy_pm_runtime_put() in phy_mdm6600_power_on() */

/**

 * phy_mdm6600_cmd() - send a command request to mdm6600

 * @ddata: device driver data

 * @val: value of cmd to be set

 *

 * Configures the three command request GPIOs to the specified value.

/**

 * phy_mdm6600_status() - read mdm6600 status lines

 * @work: work structure

/**

 * phy_mdm6600_wakeirq_thread - handle mode1 line OOB wake after booting

 * @irq: interrupt

 * @data: interrupt handler data

 *

 * GPIO mode1 is used initially as output to configure the USB boot

 * mode for mdm6600. After booting it is used as input for OOB wake

 * signal from mdm6600 to the SoC. Just use it for debug info only

 * for now.

 Just wake-up and kick the autosuspend timer */

/**

 * phy_mdm6600_init_irq() - initialize mdm6600 status IRQ lines

 * @ddata: device driver data

 low = phy disabled */

 low = off */

 high = reset */

/**

 * phy_mdm6600_init_lines() - initialize mdm6600 GPIO lines

 * @ddata: device driver data

 MDM6600 control lines */

 MDM6600 USB start-up mode output lines */

 MDM6600 status input lines */

 MDM6600 cmd output lines */

/**

 * phy_mdm6600_device_power_on() - power on mdm6600 device

 * @ddata: device driver data

 *

 * To get the integrated USB phy in MDM6600 takes some hoops. We must ensure

 * the shared USB bootmode GPIOs are configured, then request modem start-up,

 * reset and power-up.. And then we need to recycle the shared USB bootmode

 * GPIOs as they are also used for Out of Band (OOB) wake for the USB and

 * TS 27.010 serial mux.

	/*

	 * Shared GPIOs must be low for normal USB mode. After booting

	 * they are used for OOB wake signaling. These can be also used

	 * to configure USB flashing mode later on based on a module

	 * parameter.

 Request start-up mode */

 Request a reset first */

 Toggle power GPIO to request mdm6600 to start */

	/*

	 * Looks like the USB PHY needs between 2.2 to 4 seconds.

	 * If we try to use it before that, we will get L3 errors

	 * from omap-usb-host trying to access the PHY. See also

	 * phy_mdm6600_init() for -EPROBE_DEFER.

 Booting up the rest of MDM6600 will take total about 8 seconds */

 Reconfigure mode1 GPIO as input for OOB wake */

/**

 * phy_mdm6600_device_power_off() - power off mdm6600 device

 * @ddata: device driver data

/*

 * USB suspend puts mdm6600 into low power mode. For any n_gsm using apps,

 * we need to keep the modem awake by kicking it's mode0 GPIO. This will

 * keep the modem awake for about 1.2 seconds. When no n_gsm apps are using

 * the modem, runtime PM auto mode can be enabled so modem can enter low

 * power mode.

	/*

	 * The modem does not always stay awake 1.2 seconds after toggling

	 * the wake GPIO, and sometimes it idles after about some 600 ms

	 * making writes time out.

 Active state selected in phy_mdm6600_power_on() */

	/*

	 * See phy_mdm6600_device_power_on(). We should be able

	 * to remove this eventually when ohci-platform can deal

	 * with -EPROBE_DEFER.

	/*

	 * Enable PM runtime only after PHY has been powered up properly.

	 * It is currently only needed after USB suspends mdm6600 and n_gsm

	 * needs to access the device. We don't want to do this earlier as

	 * gpio mode0 pin doubles as mdm6600 wake-up gpio.

 SPDX-License-Identifier: GPL-2.0

/*

 * This is a V4L2 PCI Skeleton Driver. It gives an initial skeleton source

 * for use with other PCI drivers.

 *

 * This skeleton PCI driver assumes that the card has an S-Video connector as

 * input 0 and an HDMI connector as input 1.

 *

 * Copyright 2014 Cisco Systems, Inc. and/or its affiliates. All rights reserved.

 *

 * This program is free software; you may redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; version 2 of the License.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,

 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF

 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND

 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS

 * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN

 * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN

 * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE

 * SOFTWARE.

/**

 * struct skeleton - All internal data for one instance of device

 * @pdev: PCI device

 * @v4l2_dev: top-level v4l2 device struct

 * @vdev: video node structure

 * @ctrl_handler: control handler structure

 * @lock: ioctl serialization mutex

 * @std: current SDTV standard

 * @timings: current HDTV timings

 * @format: current pix format

 * @input: current video input (0 = SDTV, 1 = HDTV)

 * @queue: vb2 video capture queue

 * @qlock: spinlock controlling access to buf_list and sequence

 * @buf_list: list of buffers queued for DMA

 * @field: the field (TOP/BOTTOM/other) of the current buffer

 * @sequence: frame sequence counter

 { PCI_DEVICE(PCI_VENDOR_ID_, PCI_DEVICE_ID_) }, */

/*

 * HDTV: this structure has the capabilities of the HDTV receiver.

 * It is used to constrain the huge list of possible formats based

 * upon the hardware capabilities.

 keep this initialization for compatibility with GCC < 4.4.6 */

 min/max width */

 min/max height */

 min/max pixelclock*/

 Supported standards */

 capabilities */

/*

 * Supported SDTV standards. This does the same job as skel_timings_cap, but

 * for standard TV formats.

/*

 * Interrupt handler: typically interrupts happen after a new frame has been

 * captured. It is the job of the handler to remove the new frame from the

 * internal list and give it back to the vb2 framework, updating the sequence

 * counter, field and timestamp at the same time.

 handle interrupt */

 Once a new frame has been captured, mark it as done like this: */

/*

 * Setup the constraints of the queue: besides setting the number of planes

 * per buffer and the size and allocation context of each plane, it also

 * checks if sufficient buffers have been allocated. Usually 3 is a good

 * minimum number: many DMA engines need a minimum of 2 buffers in the

 * queue and you need to have another available for userspace processing.

		/*

		 * You cannot use read() with FIELD_ALTERNATE since the field

		 * information (TOP/BOTTOM) cannot be passed back to the user.

/*

 * Prepare the buffer for queueing to the DMA engine: check and set the

 * payload size.

/*

 * Queue this buffer to the DMA engine.

 TODO: Update any DMA pointers if necessary */

/*

 * Start streaming. First check if the minimum number of buffers have been

 * queued. If not, then return -ENOBUFS and the vb2 framework will call

 * this function again the next time a buffer has been queued until enough

 * buffers are available to actually start the DMA engine.

 TODO: start DMA */

		/*

		 * In case of an error, return all active buffers to the

		 * QUEUED state

/*

 * Stop the DMA engine. Any remaining buffers in the DMA queue are dequeued

 * and passed on to the vb2 framework marked as STATE_ERROR.

 TODO: stop DMA */

 Release all active buffers */

/*

 * The vb2 queue ops. Note that since q->lock is set we can use the standard

 * vb2_ops_wait_prepare/finish helper functions. If q->lock would be NULL,

 * then this driver would have to provide these ops.

/*

 * Required ioctl querycap. Note that the version field is prefilled with

 * the version of the kernel.

/*

 * Helper function to check and correct struct v4l2_pix_format. It's used

 * not only in VIDIOC_TRY/S_FMT, but also elsewhere if changes to the SDTV

 * standard, HDTV timings or the video input would require updating the

 * current format.

 S-Video input */

 HDMI input */

	/*

	 * The YUYV format is four bytes for every two pixels, so bytesperline

	 * is width * 2.

	/*

	 * Due to historical reasons providing try_fmt with an unsupported

	 * pixelformat will return -EINVAL for video receivers. Webcam drivers,

	 * however, will silently correct the pixelformat. Some video capture

	 * applications rely on this behavior...

	/*

	 * It is not allowed to change the format while buffers for use with

	 * streaming have already been allocated.

 TODO: change format */

 S_STD is not supported on the HDMI input */

	/*

	 * No change, so just return. Some applications call S_STD again after

	 * the buffers for streaming have been set up, so we have to allow for

	 * this behavior.

	/*

	 * Changing the standard implies a format change, which is not allowed

	 * while buffers for use with streaming have already been allocated.

 TODO: handle changing std */

 Update the internal format */

 G_STD is not supported on the HDMI input */

/*

 * Query the current standard as seen by the hardware. This function shall

 * never actually change the standard, it just detects and reports.

 * The framework will initially set *std to tvnorms (i.e. the set of

 * supported standards by this input), and this function should just AND

 * this value. If there is no signal, then *std should be set to 0.

 QUERY_STD is not supported on the HDMI input */

	/*

	 * Query currently seen standard. Initial value of *std is

	 * V4L2_STD_ALL. This function should look something like this:

 Use signal information to reduce the number of possible standards */

 S_DV_TIMINGS is not supported on the S-Video input */

 Quick sanity check */

 Check if the timings are part of the CEA-861 timings. */

 Return 0 if the new timings are the same as the current timings. */

	/*

	 * Changing the timings implies a format change, which is not allowed

	 * while buffers for use with streaming have already been allocated.

 TODO: Configure new timings */

 Save timings */

 Update the internal format */

 G_DV_TIMINGS is not supported on the S-Video input */

 ENUM_DV_TIMINGS is not supported on the S-Video input */

/*

 * Query the current timings as seen by the hardware. This function shall

 * never actually change the timings, it just detects and reports.

 * If no signal is detected, then return -ENOLINK. If the hardware cannot

 * lock to the signal, then return -ENOLCK. If the signal is out of range

 * of the capabilities of the system (e.g., it is possible that the receiver

 * can lock but that the DMA engine it is connected to cannot handle

 * pixelclocks above a certain frequency), then -ERANGE is returned.

 QUERY_DV_TIMINGS is not supported on the S-Video input */

	/*

	 * Query currently seen timings. This function should look

	 * something like this:

 Useful for debugging */

 DV_TIMINGS_CAP is not supported on the S-Video input */

	/*

	 * Changing the input implies a format change, which is not allowed

	 * while buffers for use with streaming have already been allocated.

	/*

	 * Update tvnorms. The tvnorms value is used by the core to implement

	 * VIDIOC_ENUMSTD so it has to be correct. If tvnorms == 0, then

	 * ENUMSTD will return -ENODATA.

 Update the internal format */

 The control handler. */

	/*struct skeleton *skel =

 TODO: set brightness to ctrl->val */

 TODO: set contrast to ctrl->val */

 TODO: set saturation to ctrl->val */

 TODO: set hue to ctrl->val */

/* ------------------------------------------------------------------

	File operations for the device

/*

 * The set of all supported ioctls. Note that all the streaming ioctls

 * use the vb2 helper functions that take care of all the locking and

 * that also do ownership tracking (i.e. only the filehandle that requested

 * the buffers can call the streaming ioctls, all other filehandles will

 * receive -EBUSY if they attempt to call the same streaming ioctls).

 *

 * The last three ioctls also use standard helper functions: these implement

 * standard behavior for drivers with controls.

/*

 * The set of file operations. Note that all these ops are standard core

 * helper functions.

/*

 * The initial setup of this device instance. Note that the initial state of

 * the driver should be complete. So the initial format, standard, timings

 * and video input should all be initialized to some reasonable value.

 The initial timings are chosen to be 720p60. */

 Enable PCI */

 Allocate a new instance */

 Allocate the interrupt */

 Fill in the initial format-related settings */

 Initialize the top-level structure */

 Add the controls */

 Initialize the vb2 queue */

	/*

	 * Assume that this DMA engine needs to have at least two buffers

	 * available before it can be started. The start_streaming() op

	 * won't be called until at least this many buffers are queued up.

	/*

	 * The serialization lock for the streaming ioctls. This is the same

	 * as the main serialization lock, but if some of the non-streaming

	 * ioctls could take a long time to execute, then you might want to

	 * have a different lock here to prevent VIDIOC_DQBUF from being

	 * blocked while waiting for another action to finish. This is

	 * generally not needed for PCI devices, but USB devices usually do

	 * want a separate lock here.

	/*

	 * Since this driver can only do 32-bit DMA we must make sure that

	 * the vb2 core will allocate the buffers in 32-bit DMA memory.

 Initialize the video_device structure */

	/*

	 * There is nothing to clean up, so release is set to an empty release

	 * function. The release callback must be non-NULL.

	/*

	 * The main serialization lock. All ioctls are serialized by this

	 * lock. Exception: if q->lock is set, then the streaming ioctls

	 * are serialized by that separate lock.

 Supported SDTV standards, if any */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 	cn_test.c

 * 

 * 2004+ Copyright (c) Evgeniy Polyakov <zbr@ioremap.net>

 * All rights reserved.

/*

 * Do not remove this function even if no one is using it as

 * this is an example of how to get notifications about new

 * connector user registration

	/*

	 * Idx.

	/*

	 * Val 0.

	/*

	 * Val 1.

netlink_broadcast(nls, skb, 0, ctl->group, GFP_ATOMIC);

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * 	ucon.c

 *

 * Copyright (c) 2004+ Evgeniy Polyakov <zbr@ioremap.net>

 Hopefully your userspace connector.h matches this kernel */

 getopt() outputs an error for us */

 bitmask of requested groups */

 Additional group number */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * data_breakpoint.c - Sample HW Breakpoint file to watch kernel data address

 *

 * usage: insmod data_breakpoint.ko ksym=<ksym_name>

 *

 * This file is a kernel module that places a breakpoint over ksym_name kernel

 * variable using Hardware Breakpoint register. The corresponding handler which

 * prints a backtrace is invoked every time a write operation is performed on

 * that variable.

 *

 * Copyright (C) IBM Corporation, 2009

 *

 * Author: K.Prasad <prasad@linux.vnet.ibm.com>

 Needed by all modules */

 Needed for KERN_INFO */

 Needed for the macros */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Any file that uses trace points, must include the header.

 * But only one file, must include the header by defining

 * CREATE_TRACE_POINTS first.  This will make the C code that

 * creates the handles for the trace points.

 Silly tracepoints */

 More silly tracepoints */

	/*

	 * We shouldn't be able to start a trace when the module is

	 * unloading (there's other locks to prevent that). But

	 * for consistency sake, we still take the thread_mutex.

 SPDX-License-Identifier: GPL-2.0

/*

 * Sample kobject implementation

 *

 * Copyright (C) 2004-2007 Greg Kroah-Hartman <greg@kroah.com>

 * Copyright (C) 2007 Novell Inc.

/*

 * This module shows how to create a simple subdirectory in sysfs called

 * /sys/kernel/kobject-example  In that directory, 3 files are created:

 * "foo", "baz", and "bar".  If an integer is written to these files, it can be

 * later read out of it.

/*

 * The "foo" file where a static variable is read from and written to.

 Sysfs attributes cannot be world-writable. */

/*

 * More complex function where we determine which variable is being accessed by

 * looking at the attribute for the "baz" and "bar" files.

/*

 * Create a group of attributes so that we can create and destroy them all

 * at once.

 need to NULL terminate the list of attributes */

/*

 * An unnamed attribute group will put all of the attributes directly in

 * the kobject directory.  If we specify a name, a subdirectory will be

 * created for the attributes with the directory being the name of the

 * attribute group.

	/*

	 * Create a simple kobject with the name of "kobject_example",

	 * located under /sys/kernel/

	 *

	 * As this is a simple directory, no uevent will be sent to

	 * userspace.  That is why this function should not be used for

	 * any type of dynamic kobjects, where the name and number are

	 * not known ahead of time.

 Create the files associated with this kobject */

 SPDX-License-Identifier: GPL-2.0

/*

 * Sample kset and ktype implementation

 *

 * Copyright (C) 2004-2007 Greg Kroah-Hartman <greg@kroah.com>

 * Copyright (C) 2007 Novell Inc.

/*

 * This module shows how to create a kset in sysfs called

 * /sys/kernel/kset-example

 * Then tree kobjects are created and assigned to this kset, "foo", "baz",

 * and "bar".  In those kobjects, attributes of the same name are also

 * created and if an integer is written to these files, it can be later

 * read out of it.

/*

 * This is our "object" that we will create a few of and register them with

 * sysfs.

 a custom attribute that works just for a struct foo_obj. */

/*

 * The default show function that must be passed to sysfs.  This will be

 * called by sysfs for whenever a show function is called by the user on a

 * sysfs file associated with the kobjects we have registered.  We need to

 * transpose back from a "default" kobject to our custom struct foo_obj and

 * then call the show function for that specific object.

/*

 * Just like the default show function above, but this one is for when the

 * sysfs "store" is requested (when a value is written to a file.)

 Our custom sysfs_ops that we will associate with our ktype later on */

/*

 * The release function for our object.  This is REQUIRED by the kernel to

 * have.  We free the memory held in our object here.

 *

 * NEVER try to get away with just a "blank" release function to try to be

 * smarter than the kernel.  Turns out, no one ever is...

/*

 * The "foo" file where the .foo variable is read from and written to.

 Sysfs attributes cannot be world-writable. */

/*

 * More complex function where we determine which variable is being accessed by

 * looking at the attribute for the "baz" and "bar" files.

/*

 * Create a group of attributes so that we can create and destroy them all

 * at once.

 need to NULL terminate the list of attributes */

/*

 * Our own ktype for our kobjects.  Here we specify our sysfs ops, the

 * release function, and the set of default attributes we want created

 * whenever a kobject of this type is registered with the kernel.

 allocate the memory for the whole object */

	/*

	 * As we have a kset for this kobject, we need to set it before calling

	 * the kobject core.

	/*

	 * Initialize and add the kobject to the kernel.  All the default files

	 * will be created here.  As we have already specified a kset for this

	 * kobject, we don't have to set a parent for the kobject, the kobject

	 * will be placed beneath that kset automatically.

	/*

	 * We are always responsible for sending the uevent that the kobject

	 * was added to the system.

	/*

	 * Create a kset with the name of "kset_example",

	 * located under /sys/kernel/

	/*

	 * Create three objects and register them with our kset

 SPDX-License-Identifier: GPL-2.0-only

 for handle_mm_fault() */

 CONFIG_X86_64 */

 CONFIG_S390 */

 SPDX-License-Identifier: GPL-2.0-only

 for handle_mm_fault() */

 CONFIG_X86_64 */

 CONFIG_S390 */

 SPDX-License-Identifier: GPL-2.0-only

 for wake_up_process() */

 CONFIG_X86_64 */

 CONFIG_S390 */

 SPDX-License-Identifier: GPL-2.0-only

 CONFIG_X86_64 */

 CONFIG_S390 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Any file that uses trace points, must include the header.

 * But only one file, must include the header by defining

 * CREATE_TRACE_POINTS first.  This will make the C code that

 * creates the handles for the trace points.

	/*

	 * Disable tracing for event "sample_event".

/*

 * mytimer: Timer setup to disable tracing for event "sample_event". This

 * timer is only for the purposes of the sample module to demonstrate access of

 * Ftrace instances from within kernel.

	/*

	 * Printing count value using trace_array_printk() - trace_printk()

	 * equivalent for the instance buffers.

	/*

	 * Tracepoint for event "sample_event". This will print the

	 * current value of count and current jiffies.

	/*

	 * Enable tracing for "sample_event".

	/*

	 * Adding timer - mytimer. This timer will disable tracing after

	 * delay seconds.

	 *

	/*

	 * trace_array_put() decrements the reference counter associated with

	 * the trace array - "tr". We are done using the trace array, hence

	 * decrement the reference counter so that it can be destroyed using

	 * trace_array_destroy().

	/*

	 * Return a pointer to the trace array with name "sample-instance" if it

	 * exists, else create a new trace array.

	 *

	 * NOTE: This function increments the reference counter

	 * associated with the trace array - "tr".

	/*

	 * If context specific per-cpu buffers havent already been allocated.

	/*

	 * We are unloading our module and no longer require the trace array.

	 * Remove/destroy "tr" using trace_array_destroy()

 SPDX-License-Identifier: BSD-3-Clause

/*

 * Simple Landlock sandbox manager able to launch a process restricted by a

 * user-defined filesystem access control policy.

 *

 * Copyright © 2017-2020 Mickaël Salaün <mic@digikod.net>

 * Copyright © 2020 ANSSI

 Prevents users to forget a setting. */

		/*

		 * Allows to not use all possible restrictions (e.g. use

		 * LL_FS_RO without LL_FS_RW).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * samples/kmemleak/kmemleak-test.c

 *

 * Copyright (C) 2008 ARM Limited

 * Written by Catalin Marinas <catalin.marinas@arm.com>

/*

 * Some very simple testing. This function needs to be extended for

 * proper testing.

 make some orphan objects */

	/*

	 * Add elements to a list. They should only appear as orphan

	 * after the module is removed.

	/*

	 * Remove the list elements without actually freeing the

	 * memory.

 SPDX-License-Identifier: GPL-2.0

/*

 * Seccomp BPF helper functions

 *

 * Copyright (c) 2012 The Chromium OS Authors <chromium-os-dev@chromium.org>

 * Author: Will Drewry <wad@chromium.org>

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using prctl(PR_ATTACH_SECCOMP_FILTER).

	/*

	* Walk it once, backwards, to build the label table and do fixups.

	* Since backward jumps are disallowed by BPF, this is easy.

 fall through */

 Simple lookup table for labels. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Seccomp filter example for x86 (32-bit and 64-bit) with BPF macros

 *

 * Copyright (c) 2012 The Chromium OS Authors <chromium-os-dev@chromium.org>

 * Author: Will Drewry <wad@chromium.org>

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using prctl(PR_SET_SECCOMP, 2, ...).

 Redirect stderr messages to stdout. Doesn't handle EINTR, etc */

 Grab the system call number */

 Jump table for the allowed syscalls */

 Check that read is only using stdin. */

 Check that write is only using stdout */

 Trap attempts to write to stderr */

 SUPPORTED_ARCH */

/*

 * This sample is x86-only.  Since kernel samples are compiled with the

 * host toolchain, a non-x86 host will result in using only the main()

 * below.

 SUPPORTED_ARCH */

 SPDX-License-Identifier: GPL-2.0

/*

 * Naive system call dropper built on seccomp_filter.

 *

 * Copyright (c) 2012 The Chromium OS Authors <chromium-os-dev@chromium.org>

 * Author: Will Drewry <wad@chromium.org>

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using prctl(PR_SET_SECCOMP, 2, ...).

 *

 * When run, returns the specified errno for the specified

 * system call number against the given architecture.

 *

 Only allow bind mounts. */

	/*

	 * Ok, let's read the task's memory to see where they wanted their

	 * mount to go.

	/*

	 * Now we avoid a TOCTOU: we referred to a pid by its pid, but since

	 * the pid that made the syscall may have died, we need to confirm that

	 * the pid is still valid after we open its /proc/pid/mem file. We can

	 * ask the listener fd this as follows.

	 *

	 * Note that this check should occur *after* any task-specific

	 * resources are opened, to make sure that the task has not died and

	 * we're not wrongly reading someone else's state in order to make

	 * decisions.

	/*

	 * Phew, we've got the right /proc/pid/mem. Now we can read it. Note

	 * that to avoid another TOCTOU, we should read all of the pointer args

	 * before we decide to allow the syscall.

	/*

	 * Our policy is to only allow bind mounts inside /tmp. This isn't very

	 * interesting, because we could do unprivlieged bind mounts with user

	 * namespaces already, but you get the idea.

	/* Even if we didn't allow it because of policy, generating the

	 * response was be a success, because we want to tell the worker EPERM.

		/*

		 * Drop privileges. We definitely can't mount as uid 1000.

		/*

		 * Send the listener to the parent; also serves as

		 * synchronization.

		/*

		 * Try a bad mount just for grins.

		/*

		 * Ok, we expect this one to succeed.

	/*

	 * Get the listener from the child.

	/*

	 * Fork a task to handle the requests. This isn't strictly necessary,

	 * but it makes the particular writing of this sample easier, since we

	 * can just wait ofr the tracee to exit and kill the tracer.

			/*

			 * ENOENT here means that the task may have gotten a

			 * signal and restarted the syscall. It's up to the

			 * handler to decide what to do in this case, but for

			 * the sample code, we just ignore it. Probably

			 * something better should happen, like undoing the

			 * mount, or keeping track of the args to make sure we

			 * don't do it again.

 SPDX-License-Identifier: GPL-2.0

/*

 * Seccomp BPF example using a macro-based generator.

 *

 * Copyright (c) 2012 The Chromium OS Authors <chromium-os-dev@chromium.org>

 * Author: Will Drewry <wad@chromium.org>

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using prctl(PR_ATTACH_SECCOMP_FILTER).

 TODO: LOAD_SYSCALL_NR(arch) and enforce an arch */

 Don't passthrough into a label */

 Now get killed */

 SPDX-License-Identifier: GPL-2.0

/*

 * UHID Example

 *

 * Copyright (c) 2012-2013 David Herrmann <dh.herrmann@gmail.com>

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using uhid.

/*

 * UHID Example

 * This example emulates a basic 3 buttons mouse with wheel over UHID. Run this

 * program as root and then use the following keys to control the mouse:

 *   q: Quit the application

 *   1: Toggle left button (down, up, ...)

 *   2: Toggle right button

 *   3: Toggle middle button

 *   a: Move mouse left

 *   d: Move mouse right

 *   w: Move mouse up

 *   s: Move mouse down

 *   r: Move wheel up

 *   f: Move wheel down

 *

 * Additionally to 3 button mouse, 3 keyboard LEDs are also supported (LED_NUML,

 * LED_CAPSL and LED_SCROLLL). The device doesn't generate any related keyboard

 * events, though. You need to manually write the EV_LED/LED_XY/1 activation

 * input event to the evdev device to see it being sent to this device.

 *

 * If uhid is not available as /dev/uhid, then you can pass a different path as

 * first argument.

 * If <linux/uhid.h> is not installed in /usr, then compile this with:

 *   gcc -o ./uhid_test -Wall -I./include ./samples/uhid/uhid-example.c

 * And ignore the warning about kernel headers. However, it is recommended to

 * use the installed uhid.h if available.

/*

 * HID Report Desciptor

 * We emulate a basic 3 button mouse with wheel and 3 keyboard LEDs. This is

 * the report-descriptor as the kernel will parse it:

 *

 * INPUT(1)[INPUT]

 *   Field(0)

 *     Physical(GenericDesktop.Pointer)

 *     Application(GenericDesktop.Mouse)

 *     Usage(3)

 *       Button.0001

 *       Button.0002

 *       Button.0003

 *     Logical Minimum(0)

 *     Logical Maximum(1)

 *     Report Size(1)

 *     Report Count(3)

 *     Report Offset(0)

 *     Flags( Variable Absolute )

 *   Field(1)

 *     Physical(GenericDesktop.Pointer)

 *     Application(GenericDesktop.Mouse)

 *     Usage(3)

 *       GenericDesktop.X

 *       GenericDesktop.Y

 *       GenericDesktop.Wheel

 *     Logical Minimum(-128)

 *     Logical Maximum(127)

 *     Report Size(8)

 *     Report Count(3)

 *     Report Offset(8)

 *     Flags( Variable Relative )

 * OUTPUT(2)[OUTPUT]

 *   Field(0)

 *     Application(GenericDesktop.Keyboard)

 *     Usage(3)

 *       LED.NumLock

 *       LED.CapsLock

 *       LED.ScrollLock

 *     Logical Minimum(0)

 *     Logical Maximum(1)

 *     Report Size(1)

 *     Report Count(3)

 *     Report Offset(0)

 *     Flags( Variable Absolute )

 *

 * This is the mapping that we expect:

 *   Button.0001 ---> Key.LeftBtn

 *   Button.0002 ---> Key.RightBtn

 *   Button.0003 ---> Key.MiddleBtn

 *   GenericDesktop.X ---> Relative.X

 *   GenericDesktop.Y ---> Relative.Y

 *   GenericDesktop.Wheel ---> Relative.Wheel

 *   LED.NumLock ---> LED.NumLock

 *   LED.CapsLock ---> LED.CapsLock

 *   LED.ScrollLock ---> LED.ScrollLock

 *

 * This information can be verified by reading /sys/kernel/debug/hid/<dev>/rdesc

 * This file should print the same information as showed above.

 USAGE_PAGE (Generic Desktop) */

 USAGE (Mouse) */

 COLLECTION (Application) */

 USAGE (Pointer) */

 COLLECTION (Physical) */

 REPORT_ID (1) */

 USAGE_PAGE (Button) */

 USAGE_MINIMUM (Button 1) */

 USAGE_MAXIMUM (Button 3) */

 LOGICAL_MINIMUM (0) */

 LOGICAL_MAXIMUM (1) */

 REPORT_COUNT (3) */

 REPORT_SIZE (1) */

 INPUT (Data,Var,Abs) */

 REPORT_COUNT (1) */

 REPORT_SIZE (5) */

 INPUT (Cnst,Var,Abs) */

 USAGE_PAGE (Generic Desktop) */

 USAGE (X) */

 USAGE (Y) */

 USAGE (WHEEL) */

 LOGICAL_MINIMUM (-127) */

 LOGICAL_MAXIMUM (127) */

 REPORT_SIZE (8) */

 REPORT_COUNT (3) */

 INPUT (Data,Var,Rel) */

 END_COLLECTION */

 END_COLLECTION */

 USAGE_PAGE (Generic Desktop) */

 USAGE (Keyboard) */

 COLLECTION (Application) */

 REPORT_ID (2) */

 USAGE_PAGE (Led) */

 USAGE_MINIMUM (1) */

 USAGE_MAXIMUM (3) */

 LOGICAL_MINIMUM (0) */

 LOGICAL_MAXIMUM (1) */

 REPORT_COUNT (3) */

 REPORT_SIZE (1) */

 Output (Data,Var,Abs) */

 REPORT_COUNT (1) */

 REPORT_SIZE (5) */

 Output (Cnst,Var,Abs) */

 END_COLLECTION */

/* This parses raw output reports sent by the kernel to the device. A normal

 * uhid program shouldn't do this but instead just forward the raw report.

 * However, for ducomentational purposes, we try to detect LED events here and

 LED messages are adverised via OUTPUT reports; ignore the rest */

 LED reports have length 2 bytes */

 first byte is report-id which is 0x02 for LEDs in our rdesc */

 print flags payload */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * configfs_example_macros.c - This file is a demonstration module

 *      containing a number of configfs subsystems.  It uses the helper

 *      macros defined by configfs.h

 *

 * Based on sysfs:

 *      sysfs is Copyright (C) 2001, 2002, 2003 Patrick Mochel

 *

 * configfs Copyright (C) 2005 Oracle.  All rights reserved.

/*

 * 01-childless

 *

 * This first example is a childless subsystem.  It cannot create

 * any config_items.  It just has attributes.

 *

 * Note that we are enclosing the configfs_subsystem inside a container.

 * This is not necessary if a subsystem has no attributes directly

 * on the subsystem.  See the next example, 02-simple-children, for

 * such a subsystem.

 ----------------------------------------------------------------- */

/*

 * 02-simple-children

 *

 * This example merely has a simple one-attribute child.  Note that

 * there is no extra attribute structure, as the child's attribute is

 * known from the get-go.  Also, there is no container for the

 * subsystem, as it has no attributes of its own.

/*

 * Note that, since no extra work is required on ->drop_item(),

 * no ->drop_item() is provided.

 ----------------------------------------------------------------- */

/*

 * 03-group-children

 *

 * This example reuses the simple_children group from above.  However,

 * the simple_children group is not the subsystem itself, it is a

 * child of the subsystem.  Creation of a group in the subsystem creates

 * a new simple_children group.  That group can then have simple_child

 * children of its own.

/*

 * Note that, since no extra work is required on ->drop_item(),

 * no ->drop_item() is provided.

 ----------------------------------------------------------------- */

/*

 * We're now done with our subsystem definitions.

 * For convenience in this module, here's a list of them all.  It

 * allows the init function to easily register them.  Most modules

 * will only have one subsystem, and will only call register_subsystem

 * on it directly.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-callbacks-mod.c - (un)patching callbacks demo support module

 *

 *

 * Purpose

 * -------

 *

 * Simple module to demonstrate livepatch (un)patching callbacks.

 *

 *

 * Usage

 * -----

 *

 * This module is not intended to be standalone.  See the "Usage"

 * section of livepatch-callbacks-demo.c.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * livepatch-sample.c - Kernel Live Patching Sample Module

 *

 * Copyright (C) 2014 Seth Jennings <sjenning@redhat.com>

/*

 * This (dumb) live patch overrides the function that prints the

 * kernel boot cmdline when /proc/cmdline is read.

 *

 * Example:

 *

 * $ cat /proc/cmdline

 * <your cmdline>

 *

 * $ insmod livepatch-sample.ko

 * $ cat /proc/cmdline

 * this has been live patched

 *

 * $ echo 0 > /sys/kernel/livepatch/livepatch_sample/enabled

 * $ cat /proc/cmdline

 * <your cmdline>

 name being NULL means vmlinux */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-shadow-fix1.c - Shadow variables, livepatch demo

 *

 * Purpose

 * -------

 *

 * Fixes the memory leak introduced in livepatch-shadow-mod through the

 * use of a shadow variable.  This fix demonstrates the "extending" of

 * short-lived data structures by patching its allocation and release

 * functions.

 *

 *

 * Usage

 * -----

 *

 * This module is not intended to be standalone.  See the "Usage"

 * section of livepatch-shadow-mod.c.

 Shadow variable enums */

 Allocate new dummies every second */

 Check for expired dummies after a few new ones have been allocated */

 Dummies expire after a few cleanup instances */

/*

 * The constructor makes more sense together with klp_shadow_get_or_alloc().

 * In this example, it would be safe to assign the pointer also to the shadow

 * variable returned by klp_shadow_alloc().  But we wanted to show the more

 * complicated use of the API.

	/*

	 * Patch: save the extra memory location into a SV_LEAK shadow

	 * variable.  A patched dummy_free routine can later fetch this

	 * pointer to handle resource release.

	/*

	 * Patch: fetch the saved SV_LEAK shadow variable, detach and

	 * free it.  Note: handle cases where this shadow variable does

	 * not exist (ie, dummy structures allocated before this livepatch

	 * was loaded.)

 Cleanup any existing SV_LEAK shadow variables */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-shadow-fix2.c - Shadow variables, livepatch demo

 *

 * Purpose

 * -------

 *

 * Adds functionality to livepatch-shadow-mod's in-flight data

 * structures through a shadow variable.  The livepatch patches a

 * routine that periodically inspects data structures, incrementing a

 * per-data-structure counter, creating the counter if needed.

 *

 *

 * Usage

 * -----

 *

 * This module is not intended to be standalone.  See the "Usage"

 * section of livepatch-shadow-mod.c.

 Shadow variable enums */

	/*

	 * Patch: handle in-flight dummy structures, if they do not

	 * already have a SV_COUNTER shadow variable, then attach a

	 * new one.

 Patch: copy the memory leak patch from the fix1 module. */

	/*

	 * Patch: fetch the SV_COUNTER shadow variable and display

	 * the final count.  Detach the shadow variable.

 Cleanup any existing SV_COUNTER shadow variables */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-shadow-mod.c - Shadow variables, buggy module demo

 *

 * Purpose

 * -------

 *

 * As a demonstration of livepatch shadow variable API, this module

 * introduces memory leak behavior that livepatch modules

 * livepatch-shadow-fix1.ko and livepatch-shadow-fix2.ko correct and

 * enhance.

 *

 * WARNING - even though the livepatch-shadow-fix modules patch the

 * memory leak, please load these modules at your own risk -- some

 * amount of memory may leaked before the bug is patched.

 *

 *

 * Usage

 * -----

 *

 * Step 1 - Load the buggy demonstration module:

 *

 *   insmod samples/livepatch/livepatch-shadow-mod.ko

 *

 * Watch dmesg output for a few moments to see new dummy being allocated

 * and a periodic cleanup check.  (Note: a small amount of memory is

 * being leaked.)

 *

 *

 * Step 2 - Load livepatch fix1:

 *

 *   insmod samples/livepatch/livepatch-shadow-fix1.ko

 *

 * Continue watching dmesg and note that now livepatch_fix1_dummy_free()

 * and livepatch_fix1_dummy_alloc() are logging messages about leaked

 * memory and eventually leaks prevented.

 *

 *

 * Step 3 - Load livepatch fix2 (on top of fix1):

 *

 *   insmod samples/livepatch/livepatch-shadow-fix2.ko

 *

 * This module extends functionality through shadow variables, as a new

 * "check" counter is added to the dummy structure.  Periodic dmesg

 * messages will log these as dummies are cleaned up.

 *

 *

 * Step 4 - Cleanup

 *

 * Unwind the demonstration by disabling the livepatch fix modules, then

 * removing them and the demo module:

 *

 *   echo 0 > /sys/kernel/livepatch/livepatch_shadow_fix2/enabled

 *   echo 0 > /sys/kernel/livepatch/livepatch_shadow_fix1/enabled

 *   rmmod livepatch-shadow-fix2

 *   rmmod livepatch-shadow-fix1

 *   rmmod livepatch-shadow-mod

 Allocate new dummies every second */

 Check for expired dummies after a few new ones have been allocated */

 Dummies expire after a few cleanup instances */

/*

 * Keep a list of all the dummies so we can clean up any residual ones

 * on module exit

 Oops, forgot to save leak! */

/*

 * alloc_work_func: allocates new dummy structures, allocates additional

 *                  memory, aptly named "leak", but doesn't keep

 *                  permanent record of it.

/*

 * cleanup_work_func: frees dummy structures.  Without knownledge of

 *                    "leak", it leaks the additional memory that

 *                    alloc_work_func created.

 Kick out and free any expired dummies */

 Wait for any dummies at work */

 Cleanup residual dummies */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-callbacks-demo.c - (un)patching callbacks livepatch demo

 *

 *

 * Purpose

 * -------

 *

 * Demonstration of registering livepatch (un)patching callbacks.

 *

 *

 * Usage

 * -----

 *

 * Step 1 - load the simple module

 *

 *   insmod samples/livepatch/livepatch-callbacks-mod.ko

 *

 *

 * Step 2 - load the demonstration livepatch (with callbacks)

 *

 *   insmod samples/livepatch/livepatch-callbacks-demo.ko

 *

 *

 * Step 3 - cleanup

 *

 *   echo 0 > /sys/kernel/livepatch/livepatch_callbacks_demo/enabled

 *   rmmod livepatch_callbacks_demo

 *   rmmod livepatch_callbacks_mod

 *

 * Watch dmesg output to see livepatch enablement, callback execution

 * and patching operations for both vmlinux and module targets.

 *

 * NOTE: swap the insmod order of livepatch-callbacks-mod.ko and

 *       livepatch-callbacks-demo.ko to observe what happens when a

 *       target module is loaded after a livepatch with callbacks.

 *

 * NOTE: 'pre_patch_ret' is a module parameter that sets the pre-patch

 *       callback return status.  Try setting up a non-zero status

 *       such as -19 (-ENODEV):

 *

 *       # Load demo livepatch, vmlinux is patched

 *       insmod samples/livepatch/livepatch-callbacks-demo.ko

 *

 *       # Setup next pre-patch callback to return -ENODEV

 *       echo -19 > /sys/module/livepatch_callbacks_demo/parameters/pre_patch_ret

 *

 *       # Module loader refuses to load the target module

 *       insmod samples/livepatch/livepatch-callbacks-mod.ko

 *       insmod: ERROR: could not insert module samples/livepatch/livepatch-callbacks-mod.ko: No such device

 *

 * NOTE: There is a second target module,

 *       livepatch-callbacks-busymod.ko, available for experimenting

 *       with livepatch (un)patch callbacks.  This module contains

 *       a 'sleep_secs' parameter that parks the module on one of the

 *       functions that the livepatch demo module wants to patch.

 *       Modifying this value and tweaking the order of module loads can

 *       effectively demonstrate stalled patch transitions:

 *

 *       # Load a target module, let it park on 'busymod_work_func' for

 *       # thirty seconds

 *       insmod samples/livepatch/livepatch-callbacks-busymod.ko sleep_secs=30

 *

 *       # Meanwhile load the livepatch

 *       insmod samples/livepatch/livepatch-callbacks-demo.ko

 *

 *       # ... then load and unload another target module while the

 *       # transition is in progress

 *       insmod samples/livepatch/livepatch-callbacks-mod.ko

 *       rmmod samples/livepatch/livepatch-callbacks-mod.ko

 *

 *       # Finally cleanup

 *       echo 0 > /sys/kernel/livepatch/livepatch_callbacks_demo/enabled

 *       rmmod samples/livepatch/livepatch-callbacks-demo.ko

 Executed on object patching (ie, patch enablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 Executed on object unpatching (ie, patch disablement) */

 vmlinux */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/*

 * livepatch-callbacks-busymod.c - (un)patching callbacks demo support module

 *

 *

 * Purpose

 * -------

 *

 * Simple module to demonstrate livepatch (un)patching callbacks.

 *

 *

 * Usage

 * -----

 *

 * This module is not intended to be standalone.  See the "Usage"

 * section of livepatch-callbacks-mod.c.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2021, Collabora Ltd.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sample dynamic sized record fifo implementation

 *

 * Copyright (C) 2010 Stefani Seibold <stefani@seibold.net>

/*

 * This module shows how to create a variable sized record fifo.

 fifo size in elements (bytes) */

 name of the proc entry */

 lock for procfs read access */

 lock for procfs write access */

/*

 * define DYNAMIC in this example for a dynamically allocated fifo.

 *

 * Otherwise the fifo storage will be a part of the fifo structure.

/*

 * struct kfifo_rec_ptr_1 and  STRUCT_KFIFO_REC_1 can handle records of a

 * length between 0 and 255 bytes.

 *

 * struct kfifo_rec_ptr_2 and  STRUCT_KFIFO_REC_2 can handle records of a

 * length between 0 and 65535 bytes.

 show the size of the next record in the fifo */

 put in variable length data */

 skip first element of the fifo */

 show the first record without removing from the fifo */

 check the correctness of all values in the fifo */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sample kfifo int type implementation

 *

 * Copyright (C) 2010 Stefani Seibold <stefani@seibold.net>

/*

 * This module shows how to create a int type fifo.

 fifo size in elements (ints) */

 name of the proc entry */

 lock for procfs read access */

 lock for procfs write access */

/*

 * define DYNAMIC in this example for a dynamically allocated fifo.

 *

 * Otherwise the fifo storage will be a part of the fifo structure.

 put values into the fifo */

 show the number of used elements */

 get max of 2 elements from the fifo */

 and put it back to the end of the fifo */

 skip first element of the fifo */

 put values into the fifo until is full */

 show the first value without removing from the fifo */

 check the correctness of all values in the fifo */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sample fifo dma implementation

 *

 * Copyright (C) 2010 Stefani Seibold <stefani@seibold.net>

/*

 * This module shows how to handle fifo dma operations.

 fifo size in elements (bytes) */

 kick away first byte */

	/*

	 * Configure the kfifo buffer to receive data from DMA input.

	 *

	 *  .--------------------------------------.

	 *  | 0 | 1 | 2 | ... | 12 | 13 | ... | 31 |

	 *  |---|------------------|---------------|

	 *   \_/ \________________/ \_____________/

	 *    \          \                  \

	 *     \          \_allocated data   \

	 *      \_*free space*                \_*free space*

	 *

	 * We need two different SG entries: one for the free space area at the

	 * end of the kfifo buffer (19 bytes) and another for the first free

	 * byte at the beginning, after the kfifo_skip().

 fifo is full and no sgl was created */

 receive data */

 put here your code to setup and exectute the dma operation */

 ... */

 example: zero bytes received */

 finish the dma operation and update the received data */

 Prepare to transmit data, example: 8 bytes */

 no data was available and no sgl was created */

 put here your code to setup and exectute the dma operation */

 ... */

 example: 5 bytes transmitted */

 finish the dma operation and update the transmitted data */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Sample kfifo byte stream implementation

 *

 * Copyright (C) 2010 Stefani Seibold <stefani@seibold.net>

/*

 * This module shows how to create a byte stream fifo.

 fifo size in elements (bytes) */

 name of the proc entry */

 lock for procfs read access */

 lock for procfs write access */

/*

 * define DYNAMIC in this example for a dynamically allocated fifo.

 *

 * Otherwise the fifo storage will be a part of the fifo structure.

 put string into the fifo */

 put values into the fifo */

 show the number of used elements */

 get max of 5 bytes from the fifo */

 get max of 2 elements from the fifo */

 and put it back to the end of the fifo */

 skip first element of the fifo */

 put values into the fifo until is full */

 show the first value without removing from the fifo */

 check the correctness of all values in the fifo */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Remote processor messaging - sample client driver

 *

 * Copyright (C) 2011 Texas Instruments, Inc.

 * Copyright (C) 2011 Google, Inc.

 *

 * Ohad Ben-Cohen <ohad@wizery.com>

 * Brian Swetland <swetland@google.com>

 samples should not live forever */

 send a new message now */

 send a message to our remote processor */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2020-2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.

/**

 * DOC: Sample flow of using the ioctl interface provided by the Nitro Enclaves (NE)

 * kernel driver.

 *

 * Usage

 * -----

 *

 * Load the nitro_enclaves module, setting also the enclave CPU pool. The

 * enclave CPUs need to be full cores from the same NUMA node. CPU 0 and its

 * siblings have to remain available for the primary / parent VM, so they

 * cannot be included in the enclave CPU pool.

 *

 * See the cpu list section from the kernel documentation.

 * https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html#cpu-lists

 *

 *	insmod drivers/virt/nitro_enclaves/nitro_enclaves.ko

 *	lsmod

 *

 *	The CPU pool can be set at runtime, after the kernel module is loaded.

 *

 *	echo <cpu-list> > /sys/module/nitro_enclaves/parameters/ne_cpus

 *

 *	NUMA and CPU siblings information can be found using:

 *

 *	lscpu

 *	/proc/cpuinfo

 *

 * Check the online / offline CPU list. The CPUs from the pool should be

 * offlined.

 *

 *	lscpu

 *

 * Check dmesg for any warnings / errors through the NE driver lifetime / usage.

 * The NE logs contain the "nitro_enclaves" or "pci 0000:00:02.0" pattern.

 *

 *	dmesg

 *

 * Setup hugetlbfs huge pages. The memory needs to be from the same NUMA node as

 * the enclave CPUs.

 *

 * https://www.kernel.org/doc/html/latest/admin-guide/mm/hugetlbpage.html

 *

 * By default, the allocation of hugetlb pages are distributed on all possible

 * NUMA nodes. Use the following configuration files to set the number of huge

 * pages from a NUMA node:

 *

 *	/sys/devices/system/node/node<X>/hugepages/hugepages-2048kB/nr_hugepages

 *	/sys/devices/system/node/node<X>/hugepages/hugepages-1048576kB/nr_hugepages

 *

 *	or, if not on a system with multiple NUMA nodes, can also set the number

 *	of 2 MiB / 1 GiB huge pages using

 *

 *	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

 *	/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages

 *

 *	In this example 256 hugepages of 2 MiB are used.

 *

 * Build and run the NE sample.

 *

 *	make -C samples/nitro_enclaves clean

 *	make -C samples/nitro_enclaves

 *	./samples/nitro_enclaves/ne_ioctl_sample <path_to_enclave_image>

 *

 * Unload the nitro_enclaves module.

 *

 *	rmmod nitro_enclaves

 *	lsmod

/**

 * NE_DEV_NAME - Nitro Enclaves (NE) misc device that provides the ioctl interface.

/**

 * NE_POLL_WAIT_TIME - Timeout in seconds for each poll event.

/**

 * NE_POLL_WAIT_TIME_MS - Timeout in milliseconds for each poll event.

/**

 * NE_SLEEP_TIME - Amount of time in seconds for the process to keep the enclave alive.

/**

 * NE_DEFAULT_NR_VCPUS - Default number of vCPUs set for an enclave.

/**

 * NE_MIN_MEM_REGION_SIZE - Minimum size of a memory region - 2 MiB.

/**

 * NE_DEFAULT_NR_MEM_REGIONS - Default number of memory regions of 2 MiB set for

 *			       an enclave.

/**

 * NE_IMAGE_LOAD_HEARTBEAT_CID - Vsock CID for enclave image loading heartbeat logic.

/**

 * NE_IMAGE_LOAD_HEARTBEAT_PORT - Vsock port for enclave image loading heartbeat logic.

/**

 * NE_IMAGE_LOAD_HEARTBEAT_VALUE - Heartbeat value for enclave image loading.

/**

 * struct ne_user_mem_region - User space memory region set for an enclave.

 * @userspace_addr:	Address of the user space memory region.

 * @memory_size:	Size of the user space memory region.

/**

 * ne_create_vm() - Create a slot for the enclave VM.

 * @ne_dev_fd:		The file descriptor of the NE misc device.

 * @slot_uid:		The generated slot uid for the enclave.

 * @enclave_fd :	The generated file descriptor for the enclave.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

/**

 * ne_poll_enclave_fd() - Thread function for polling the enclave fd.

 * @data:	Argument provided for the polling function.

 *

 * Context: Process context.

 * Return:

 * * NULL on success / failure.

 Keep on polling until the current process is terminated. */

/**

 * ne_alloc_user_mem_region() - Allocate a user space memory region for an enclave.

 * @ne_user_mem_region:	User space memory region allocated using hugetlbfs.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

	/**

	 * Check available hugetlb encodings for different huge page sizes in

	 * include/uapi/linux/mman.h.

/**

 * ne_load_enclave_image() - Place the enclave image in the enclave memory.

 * @enclave_fd :		The file descriptor associated with the enclave.

 * @ne_user_mem_regions:	User space memory regions allocated for the enclave.

 * @enclave_image_path :	The file path of the enclave image.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

/**

 * ne_set_user_mem_region() - Set a user space memory region for the given enclave.

 * @enclave_fd :		The file descriptor associated with the enclave.

 * @ne_user_mem_region :	User space memory region to be set for the enclave.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

/**

 * ne_free_mem_regions() - Unmap all the user space memory regions that were set

 *			   aside for the enclave.

 * @ne_user_mem_regions:	The user space memory regions associated with an enclave.

 *

 * Context: Process context.

/**

 * ne_add_vcpu() - Add a vCPU to the given enclave.

 * @enclave_fd :	The file descriptor associated with the enclave.

 * @vcpu_id:		vCPU id to be set for the enclave, either provided or

 *			auto-generated (if provided vCPU id is 0).

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

/**

 * ne_start_enclave() - Start the given enclave.

 * @enclave_fd :		The file descriptor associated with the enclave.

 * @enclave_start_info :	Enclave metadata used for starting e.g. vsock CID.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

/**

 * ne_start_enclave_check_booted() - Start the enclave and wait for a heartbeat

 *				     from it, on a newly created vsock channel,

 *				     to check it has booted.

 * @enclave_fd :	The file descriptor associated with the enclave.

 *

 * Context: Process context.

 * Return:

 * * 0 on success.

 * * Negative return value on failure.

	/*

	 * Read the heartbeat value that the init process in the enclave sends

	 * after vsock connect.

 Write the heartbeat value back. */

		/*

		 * The vCPU is chosen from the enclave vCPU pool, if the value

		 * of the vcpu_id is 0.

 SPDX-License-Identifier: GPL-2.0

/*

 * Mediated virtual PCI display host device driver

 *

 * Emulate enough of qemu stdvga to make bochs-drm.ko happy.  That is

 * basically the vram memory bar and the bochs dispi interface vbe

 * registers in the mmio register bar.	Specifically it does *not*

 * include any legacy vga stuff.  Device looks a lot like "qemu -device

 * secondary-vga".

 *

 *   (c) Gerd Hoffmann <kraxel@redhat.com>

 *

 * based on mtty driver which is:

 *   Copyright (c) 2016, NVIDIA CORPORATION. All rights reserved.

 *	 Author: Neo Jia <cjia@nvidia.com>

 *		 Kirti Wankhede <kwankhede@nvidia.com>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 State of each mdev device */

 vga ioports remapped */

 bochs dispi interface */

 qemu extended regs */

 edid block */

 bochs dispi interface */

 read-only regs */

 free in mbochs_release_dmabuf() */

 nothing */

 SPDX-License-Identifier: GPL-2.0

/*

 * Framebuffer driver for mdpy (mediated virtual pci display device).

 *

 * See mdpy-defs.h for device specs

 *

 *   (c) Gerd Hoffmann <kraxel@redhat.com>

 *

 * Using some code snippets from simplefb and cirrusfb.

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms and conditions of the GNU General Public License,

 * version 2, as published by the Free Software Foundation.

 *

 * This program is distributed in the hope it will be useful, but WITHOUT

 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or

 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for

 * more details.

 end of list */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Mediated virtual PCI serial host device driver

 *

 * Copyright (c) 2016, NVIDIA CORPORATION. All rights reserved.

 *     Author: Neo Jia <cjia@nvidia.com>

 *             Kirti Wankhede <kwankhede@nvidia.com>

 *

 * Sample driver that creates mdev device that simulates serial port over PCI

 * card.

/*

 * #defines

/*

 * Global Structures

 loop back buffer */

 8 registers */

 loop back buffer */

 FIFO control register */

 interrupt trigger level */

 State of each mdev device */

 function prototypes */

 Helper functions */

 PCI dev ID */

 Control: I/O+, Mem-, BusMaster- */

 Status: capabilities list absent */

 Rev ID */

 programming interface class : 16550-compatible serial controller */

 Sub class : 00 */

 Base class : Simple Communication controllers */

 base address registers */

 BAR0: IO space */

 BAR1: IO space */

 Subsystem ID */

 Cap Ptr */

 interrupt pin (INTA#) */

 Vendor specific data */

 device control */

 device status */

 do nothing */

 interrupt line */

		/*

		 * Interrupt Pin is hardwired to INTA.

		 * This field is write protected by hardware

 BAR0 */

 BAR1 */

 BAR2 */

 BAR3 */

 BAR4 */

 Handle data written by guest */

 if DLAB set, data is LSB of divisor */

 save in TX buffer */

			/*

			 * Trigger interrupt if receive data interrupt is

			 * enabled and fifo reached trigger level

 trigger interrupt */

			/*

			 * Trigger interrupt if receiver line status interrupt

			 * is enabled

 if DLAB set, data is MSB of divisor */

 clear loop back FIFO */

		/*

		 * Set trigger level to 1 otherwise or  implement timer with

		 * timeout of 4 characters and on expiring that timer set

		 * Recevice data timeout in IIR register

 do nothing */

 Handle read requests by guest */

 if DLAB set, data is LSB of divisor */

 return data in tx buffer */

		/*

		 *  Trigger interrupt if tx buffer empty interrupt is

		 *  enabled and fifo is empty

 Interrupt priority 1: Parity, overrun, framing or break */

 Interrupt priority 2: Fifo trigger level reached */

 Interrupt priotiry 3: transmitter holding register empty */

 Interrupt priotiry 4: Modem status: CTS, DSR, RI or DCD  */

 bit0: 0=> interrupt pending, 1=> no interrupt is pending */

 set bit 6 & 7 to be 16550 compatible */

 atleast one char in FIFO */

 if FIFO overrun */

 transmit FIFO empty and tramsitter empty */

 if AFE is 1 and FIFO have space, set CTS bit */

 1M mem BAR treated as 32-bit BAR */

 mem unknown type treated as 32-bit BAR */

 SPDX-License-Identifier: GPL-2.0

/*

 * Mediated virtual PCI display host device driver

 *

 * See mdpy-defs.h for device specs

 *

 *   (c) Gerd Hoffmann <kraxel@redhat.com>

 *

 * based on mtty driver which is:

 *   Copyright (c) 2016, NVIDIA CORPORATION. All rights reserved.

 *	 Author: Neo Jia <cjia@nvidia.com>

 *		 Kirti Wankhede <kwankhede@nvidia.com>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 State of each mdev device */

 vendor specific capability for the config registers */

 vendor cap */

 next ptr */

 initialize with gray gradient */

 unused */

 nothing */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Filename: cfag12864b-example.c

 *     Version: 0.1.0

 * Description: cfag12864b LCD userspace example program

 *

 *      Author: Copyright (C) Miguel Ojeda <ojeda@kernel.org>

 *        Date: 2006-10-31

/*

 * ------------------------

 * start of cfag12864b code

 * ------------------------

/*

 * init a cfag12864b framebuffer device

 *

 * No error:       return = 0

 * Unable to open: return = -1

 * Unable to mmap: return = -2

/*

 * exit a cfag12864b framebuffer device

/*

 * set (x, y) pixel

/*

 * unset (x, y) pixel

/*

 * is set (x, y) pixel?

 *

 * Pixel off: return = 0

 * Pixel on:  return = 1

/*

 * not (x, y) pixel

/*

 * fill (set all pixels)

/*

 * clear (unset all pixels)

/*

 * format a [128*64] matrix

 *

 * Pixel off: src[i] = 0

 * Pixel on:  src[i] > 0

/*

 * blit buffer to lcd

/*

 * ----------------------

 * end of cfag12864b code

 * ----------------------

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Here's a sample kernel module showing the use of kprobes to dump a

 * stack trace and selected registers when kernel_clone() is called.

 *

 * For more information on theory of operation of kprobes, see

 * Documentation/trace/kprobes.rst

 *

 * You will see the trace data in /var/log/messages and on the console

 * whenever kernel_clone() is invoked to create a new process.

 For each probe you need to allocate a kprobe structure */

 kprobe pre_handler: called just before the probed instruction is executed */

 A dump_stack() here will give a stack backtrace */

 kprobe post_handler: called after the probed instruction is executed */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kretprobe_example.c

 *

 * Here's a sample kernel module showing the use of return probes to

 * report the return value and total time taken for probed function

 * to run.

 *

 * usage: insmod kretprobe_example.ko func=<func_name>

 *

 * If no func_name is specified, kernel_clone is instrumented

 *

 * For more information on theory of operation of kretprobes, see

 * Documentation/trace/kprobes.rst

 *

 * Build and insert the kernel module as done in the kprobe example.

 * You will see the trace data in /var/log/messages and on the console

 * whenever the probed function returns. (Some messages may be suppressed

 * if syslogd is configured to eliminate duplicate messages.)

 per-instance private data */

 Here we use the entry_hanlder to timestamp function entry */

 Skip kernel threads */

/*

 * Return-probe handler: Log the return value and duration. Duration may turn

 * out to be zero consistently, depending upon the granularity of time

 * accounting on the platform.

 Probe up to 20 instances concurrently. */

 nmissed > 0 suggests that maxactive was set too low. */

 SPDX-License-Identifier: GPL-2.0-only

 Must not be static to force gcc to consider these non constant */

 Kick off printing in irq context */

/******************************************************************************

 * Intel Management Engine Interface (Intel MEI) Linux driver

 * Intel MEI Interface Header

 *

 * This file is provided under a dual BSD/GPLv2 license.  When using or

 * redistributing this file, you may do so under either license.

 *

 * GPL LICENSE SUMMARY

 *

 * Copyright(c) 2012 Intel Corporation. All rights reserved.

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of version 2 of the GNU General Public License as

 * published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 * General Public License for more details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this program; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110,

 * USA

 *

 * The full GNU General Public License is included in this distribution

 * in the file called LICENSE.GPL.

 *

 * Contact Information:

 *	Intel Corporation.

 *	linux-mei@linux.intel.com

 *	http://www.intel.com

 *

 * BSD LICENSE

 *

 * Copyright(c) 2003 - 2012 Intel Corporation. All rights reserved.

 * All rights reserved.

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 *

 *  * Redistributions of source code must retain the above copyright

 *    notice, this list of conditions and the following disclaimer.

 *  * Redistributions in binary form must reproduce the above copyright

 *    notice, this list of conditions and the following disclaimer in

 *    the documentation and/or other materials provided with the

 *    distribution.

 *  * Neither the name Intel Corporation nor the names of its

 *    contributors may be used to endorse or promote products derived

 *    from this software without specific prior written permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS

 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT

 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR

 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT

 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,

 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT

 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,

 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY

 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT

 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE

 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

/*****************************************************************************

 * Intel Management Engine Interface

 rc < 0 */

/***************************************************************************

 * Intel Advanced Management Technology ME Client

/***************************************************************************

 * Intel Advanced Management Technology Host Interface

 length - sizeof(status) */

************************* end of amt_host_if_command ***********************/

 SPDX-License-Identifier: GPL-2.0

/*

 * Sample in-kernel QMI client driver

 *

 * Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.

 * Copyright (C) 2017 Linaro Ltd.

/*

 * ping_write() - ping_pong debugfs file write handler

 * @file:	debugfs file context

 * @user_buf:	reference to the user data (ignored)

 * @count:	number of bytes in @user_buf

 * @ppos:	offset in @file to write

 *

 * This function allows user space to send out a ping_pong QMI encoded message

 * to the associated remote test service and will return with the result of the

 * transaction. It serves as an example of how to provide a custom response

 * handler.

 *

 * Return: @count, or negative errno on failure.

/*

 * data_write() - data debugfs file write handler

 * @file:	debugfs file context

 * @user_buf:	reference to the user data

 * @count:	number of bytes in @user_buf

 * @ppos:	offset in @file to write

 *

 * This function allows user space to send out a data QMI encoded message to

 * the associated remote test service and will return with the result of the

 * transaction. It serves as an example of how to have the QMI helpers decode a

 * transaction response into a provided object automatically.

 *

 * Return: @count, or negative errno on failure.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Verify that the pid has not been recycled and our /proc/<pid> handle

	 * is still valid.

 Process exists, just not allowed to signal it. */

 SPDX-License-Identifier: GPL-2.0

 Cleanup happens when the mount namespace dies. */

/*

 * Created by: Jason Wessel <jason.wessel@windriver.com>

 *

 * Copyright (c) 2010 Wind River Systems, Inc.  All Rights Reserved.

 *

 * This file is licensed under the terms of the GNU General Public

 * License version 2. This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

/*

 * All kdb shell command call backs receive argc and argv, where

 * argv[0] is the command the end user typed

	/*

	 * Registration of a dynamically added kdb command is done with

	 * kdb_register().

 SPDX-License-Identifier: GPL-2.0

/*

 * A sample program to run a User VM on the ACRN hypervisor

 *

 * This sample runs in a Service VM, which is a privileged VM of ACRN.

 * CONFIG_ACRN_HSM need to be enabled in the Service VM.

 *

 * Guest VM code in guest16.s will be executed after the VM launched.

 *

 * Copyright (C) 2020 Intel Corporation. All rights reserved.

github.com/projectacrn/acrn-hypervisor/blob/master/hypervisor/include/common/vm_uuids.h */

 setup guest memory */

 setup vcpu registers */

 CR0_ET | CR0_NE */

 create an ioreq client for this VM */

 run vm */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Test the statx() system call.

 *

 * Note that the output of this program is intended to look like the output of

 * /bin/stat where possible.

 *

 * Copyright (C) 2015 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 STATX_ATTR_ flags: */

 63-56 */

 55-48 */

 47-40 */

 39-32 */

 31-24	0x00000000-ff000000 */

 23-16	0x00000000-00ff0000 */

 15- 8	0x00000000-0000ff00 */

  7- 0	0x00000000-000000ff */

 Not supported */

 Not set */

 SPDX-License-Identifier: GPL-2.0-or-later

/* fd-based mount test.

 *

 * Copyright (C) 2017 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 Hope -1 isn't a syscall */

 Mount a publically available AFS filesystem */

 SPDX-License-Identifier: GPL-2.0

/*

 * Hidraw Userspace Example

 *

 * Copyright (c) 2010 Alan Ott <alan@signal11.us>

 * Copyright (c) 2010 Signal 11 Software

 *

 * The code may be used by anyone for any purpose,

 * and can serve as a starting point for developing

 * applications using hidraw.

 Linux */

/*

 * Ugly hack to work around failing compilation on systems that don't

 * yet populate new version of hidraw.h to userspace.

 Unix */

 C */

	/* Open the Device with non-blocking reads. In real life,

 Get Report Descriptor Size */

 Get Report Descriptor */

 Get Raw Name */

 Get Physical Location */

 Get Raw Info */

 Set Feature */

 Report Number */

 Get Feature */

 Report Number */

 Send a Report to the Device */

 Report Number */

 Get a report from the device */

 SPDX-License-Identifier: GPL-2.0

/* Use watch_queue API to watch for notifications.

 *

 * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * Consume and display events.

 SPDX-License-Identifier: GPL-2.0

offsetof(struct iphdr, ihl)*/);

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

 Bind server to ephemeral port on lo */

 Is the server's getsockname = the socket getpeername */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 General args */

 bpf_tunnel_key.remote_ipv4 expects host byte orders */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Facebook

/* This program verifies bpf attachment to tracepoint sys_enter_* and sys_exit_*.

 * This requires kernel CONFIG_FTRACE_SYSCALLS to be set.

 load BPF program */

	/* current load_bpf_file has perf_event_open default pid = -1

	 * and cpu = 0, which permits attached bpf execution on

	 * all cpus for all pid's. bpf program execution ignores

	 * cpu affinity.

 trigger some "open" operations */

 verify the map */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2019 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Sample Host Bandwidth Manager (HBM) BPF program.

 *

 * A cgroup skb BPF egress program to limit cgroup output bandwidth.

 * It uses a modified virtual token bucket queue to limit average

 * egress bandwidth. The implementation uses credits instead of tokens.

 * Negative credits imply that queueing would have happened (this is

 * a virtual queue, so no queueing is done by it. However, queueing may

 * occur at the actual qdisc (which is not used for rate limiting).

 *

 * This implementation uses 3 thresholds, one to start marking packets and

 * the other two to drop packets:

 *                                  CREDIT

 *        - <--------------------------|------------------------> +

 *              |    |          |      0

 *              |  Large pkt    |

 *              |  drop thresh  |

 *   Small pkt drop             Mark threshold

 *       thresh

 *

 * The effect of marking depends on the type of packet:

 * a) If the packet is ECN enabled and it is a TCP packet, then the packet

 *    is ECN marked.

 * b) If the packet is a TCP packet, then we probabilistically call tcp_cwr

 *    to reduce the congestion window. The current implementation uses a linear

 *    distribution (0% probability at marking threshold, 100% probability

 *    at drop threshold).

 * c) If the packet is not a TCP packet, then it is dropped.

 *

 * If the credit is below the drop threshold, the packet is dropped. If it

 * is a TCP packet, then it also calls tcp_cwr since packets dropped by

 * by a cgroup skb BPF program do not automatically trigger a call to

 * tcp_cwr in the current kernel code.

 *

 * This BPF program actually uses 2 drop thresholds, one threshold

 * for larger packets (>= 120 bytes) and another for smaller packets. This

 * protects smaller packets such as SYNs, ACKs, etc.

 *

 * The default bandwidth limit is set at 1Gbps but this can be changed by

 * a user program through a shared BPF map. In addition, by default this BPF

 * program does not limit connections using loopback. This behavior can be

 * overwritten by the user program. There is also an option to calculate

 * some statistics, such as percent of packets marked or dropped, which

 * the user program can access.

 *

 * A latter patch provides such a program (hbm.c)

 We may want to account for the length of headers in len

 calculation, like ETH header + overhead, specially if it

 is a gso packet. But I am not doing it right now.

 Begin critical section

	/* delta < 0 implies that another process with a curtime greater

	 * than ours beat us to the critical section and already added

	 * the new credit, so we should not add it ourselves

 End critical section

 Check if we should update rate

 Set flags (drop, congestion, cwr)

 Dropping => we are congested, so ignore congestion flag

 Very congested, set drop packet

 Congested, set congestion flag

 Do congestion control

 Problem if too many small packets?

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

 test two functions in the corresponding *_kern.c file */

 test nondebug fs kprobe */

 set a kprobe on "bpf_check + 0x5", which is x64 specific */

 test nondebug fs uprobe */

	/* the calculation of uprobe file offset is based on gcc 7.3.1 on x64

	 * and the default linker script, which defines __executable_start as

	 * the start of the .text section. The calculation could be different

	 * on different systems with different compilers. The right way is

	 * to parse the ELF file. We took a shortcut here.

 test debug fs uprobe */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2018 Facebook

 *

 * BPF program to automatically reflect TOS option from received syn packet

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2016 Facebook

 in case the last line has no \n */

		/* It is mostly redundant and just allow the parent

		 * process to update next_shced_cpu for the next child

		 * process

 Give 20% more than the active working set */

/* Copyright (C) 2017 Cavium, Inc.

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2 of the GNU General Public License

 * as published by the Free Software Foundation.

 Key for lpm_trie*/

 Map for trie implementation*/

 Map for counter*/

 Map for ARP table*/

 Map to keep the exact match entries in the route table*/

 Function to set source and destination mac of the packet */

 Parse IPV4 packet to get SRC, DST IP and protocol */

 Check for exact match, this would give a faster lookup*/

 Look up in the trie for lpm*/

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (C) 2017 Cavium, Inc.

 Get the mac address of the interface given interface name */

/* Function to parse the route entry returned by netlink

 * Updates the route entry related map entries

					/* Rereading the route table to check if

					 * there is an entry with the same

					 * prefix but a different metric as the

					 * deleted enty.

 Function to read the existing route table  when the process is launched*/

/* Function to parse the arp entry returned by netlink

 * Updates the arp entry related map entries

 Function to read the existing arp table  when the process is launched*/

/* Function to keep track and update changes in route and arp table

 * Give regular statistics of packets forwarded

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

/* simple per-protocol drop counter

 default, set below */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Covalent IO, Inc. http://covalent.io

 devmap_xmit tracepoint not available */

 Load 2nd xdp prog on egress. */

		/* First try with struct bpf_devmap_val as value for generic

		 * mode, then fallback to sizeof(int) for older kernels.

 Loading dummy XDP prog on out-device */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2015 BMW Car IT GmbH

 clear screen */

 ignore CPUs without data (maybe offline?) */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB

/*

 * ibumad BPF sample user side

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Copyright(c) 2018 Ira Weiny, Intel Corporation

 Detach tracepoints */

 Do one final dump when exiting */

 load BPF program */

 Detach tracepoints */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 John Fastabend <john.r.fastabend@intel.com>

 Loading dummy XDP prog on out-device */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set initial congestion window and initial receive

 * window to 40 packets and send and receive buffers to 1.5MB. This

 * would usually be done after doing appropriate checks that indicate

 * the hosts are far enough away (i.e. large RTT).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

	/* Usually there would be a check to insure the hosts are far

	 * from each other so it makes sense to increase buffer sizes

 Set sndbuf and rcvbuf of active connections */

 Set sndbuf and rcvbuf of passive connections */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

	/* Populate the lru_hash_map for LRU_HASH_LOOKUP perf test.

	 *

	 * It is fine that the user requests for a map with

	 * num_map_entries < 32 and some of the later lru hash lookup

	 * may return not found.  For LRU map, we are not interested

	 * in such small map performance.

		/* If CPU is not 0, create inner_lru hash map and insert the fd

		 * value into the array_of_lru_hash map. In case of CPU 0,

		 * 'inner_lru_hash_map' was statically inserted on the map init

 Only change the max_entries for the enabled test(s) */

 resize BPF map prior to loading */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2017 - 2018 Intel Corporation. */

 Make sure to read a full line at a time */

 Extract interrupt number from line */

 sum up interrupts across all cores */

 right close */

/*

 * This function code has been taken from

 * Linux kernel lib/checksum.c

 add up 16-bit and 16-bit for 16+c bit */

 add up carry.. */

/*

 * This function code has been taken from

 * Linux kernel lib/checksum.c

/*

 *	This is a version of ip_compute_csum() optimized for IP headers,

 *	which always checksum on 4 octet boundaries.

 *	This function code has been taken from

 *	Linux kernel lib/checksum.c

/*

 * Fold a partial checksum

 * This function code has been taken from

 * Linux kernel include/asm-generic/checksum.h

/*

 * This function code has been taken from

 * Linux kernel lib/checksum.c

 add up 32-bit and 32-bit for 32+c bit */

 add up carry.. */

/*

 * This function code has been taken from

 * Linux kernel lib/checksum.c

/*

 * This function has been taken from

 * Linux kernel include/asm-generic/checksum.h

 udp hdr and data */

 ethernet header */

 IP header */

 20 byte header */

 IP header checksum */

 UDP header */

 UDP data */

 UDP header checksum */

		/* We recommend that you set the fill ring size >= HW RX ring size +

		 * AF_XDP RX ring size. Make sure you fill up the fill ring

		 * with buffers at regular intervals, and you will with this setting

		 * avoid allocation failures in the driver. These are usually quite

		 * expensive since drivers have not been written to assume that

		 * allocation failures are common. For regular sockets, kernel

		 * allocated memory is used that only runs out in OOM situations

		 * that should be rare.

 default, set below */

	/* In copy mode, Tx is driven by a syscall so we need to use e.g. sendto() to

	 * really send the packets. In zero-copy mode we do not have to do this, since Tx

	 * is driven by the NAPI loop. So as an optimization, we do not have to call

	 * sendto() all the time in zero-copy mode for l2fwd.

 re-add completed Tx buffers */

 Reserve memory for the umem. Use hugepages if unaligned chunk mode */

 Create sockets... */

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

 Attach prog only when symbol exists */

/* Copyright (c) 2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* Protocol dispatch routine. It tail-calls next BPF program depending

 * on eth proto. Note, we could have used ...

 *

 *   bpf_tail_call(skb, &jmp_table, proto);

 *

 * ... but it would need large prog_array and cannot be optimised given

 * the map key is not static.

 user poor man's per_cpu until native support is ready */

 some simple stats for user space consumption */

offsetof(struct iphdr, ihl)*/);

 SPDX-License-Identifier: GPL-2.0 OR Linux-OpenIB

/*

 * ibumad BPF sample kernel side

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Copyright(c) 2018 Ira Weiny, Intel Corporation

 class; u32 required */

 count of mads read */

 Room for all Classes */

 class; u32 required */

 count of mads written */

 Room for all Classes */

/* Taken from the current format defined in

 * include/trace/events/ib_umad.h

 * and

 * /sys/kernel/debug/tracing/events/ib_umad/ib_umad_read/format

 * /sys/kernel/debug/tracing/events/ib_umad/ib_umad_write/format

/* Copyright (c) 2016, Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 add kprobes to all possible *spin* functions */

 and to inner bpf helpers */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 copy of 'struct ethhdr' without __packed */

 10.2.1.102 */

 2401:db02:0:0:0:0:0:66 */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set SYN and SYN-ACK RTOs to 10ms when using IPv6 addresses

 * and the first 5.5 bytes of the IPv6 addresses are the same (in this example

 * that means both hosts are in the same datacenter).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

 Check for TIMEOUT_INIT operation and IPv6 addresses */

		/* If the first 5.5 bytes of the IPv6 address are the same

		 * then both hosts are in the same datacenter

		 * so use an RTO of 10ms

/* Copyright (c) 2016 PLUMgrid

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 Handle VLAN tagged packet */

 Handle double VLAN tagged packet */

 SPDX-License-Identifier: GPL-2.0

/*  Copyright(c) 2017-2018 Jesper Dangaard Brouer, Red Hat Inc.

 *

 * XDP monitor tool, based on tracepoints

 SPDX-License-Identifier: GPL-2.0

 devmap_xmit tracepoint not available */

 Update mac_map with all egress interfaces' mac addr */

 bind prog_fd to each interface */

		/* Add all the interfaces to forward group and attach

		 * egress devmap program if exist

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2017 - 2018 Intel Corporation. */

 Unset fd for given ifindex */

/* SPDX-License-Identifier: GPL-2.0

 * Copyright (c) 2018 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program shows how to use bpf_xdp_adjust_tail() by

 * generating ICMPv4 "packet to big" (unreachable/ df bit set frag needed

 * to be more preice in case of v4)" where receiving packets bigger then

 * 600 bytes.

 volatile to prevent compiler optimizations */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program shows how to use bpf_xdp_adjust_head() by

 * encapsulating the incoming packet in an IPv4/v6 header

 * and then XDP_TX it out.

 It only does v4-in-v4 */

 The vip key is found.  Add an IP header and send it out */

 It only does v6-in-v6 */

 The vip key is found.  Add an IP header and send it out */

 SPDX-License-Identifier: GPL-2.0

 MIPS n64 syscalls start at 5000 */

/* install fake seccomp program to enable seccomp code path inside the kernel,

 * so that our kprobe attached to seccomp_phase1() can be triggered

 load BPF program */

 register only syscalls to PROG_ARRAY */

 compiler workaround */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set initial receive window to 40 packets when using IPv6

 * and the first 5.5 bytes of the IPv6 addresses are not the same (in this

 * example that means both hosts are not the same datacenter).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

 Check for RWND_INIT operation and IPv6 addresses */

		/* If the first 5.5 bytes of the IPv6 address are not the same

		 * then both hosts are not in the same datacenter

		 * so use a larger initial advertized window (40 packets)

	/* block PF_INET6, SOCK_RAW, IPPROTO_ICMPV6 sockets

	 * ie., make ping6 fail

	/* block PF_INET, SOCK_RAW, IPPROTO_ICMP sockets

	 * ie., make ping fail

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* kprobe is NOT a stable ABI

 * kernel functions can be removed, renamed or completely change semantics.

 * Number of arguments and their positions can change, etc.

 * In such case this bpf+kprobe example will no longer be meaningful

	/* attaches to kprobe __netif_receive_skb_core,

	 * looks for packets on loobpack device and prints them

 non-portable! works for the given kernel only */

 using bpf_trace_printk() for DEBUG ONLY */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set initial receive window to 40 packets and send

 * and receive buffers to 1.5MB. This would usually be done after

 * doing appropriate checks that indicate the hosts are far enough

 * away (i.e. large RTT).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

	/* Usually there would be a check to insure the hosts are far

	 * from each other so it makes sense to increase buffer sizes

 Set sndbuf and rcvbuf of active connections */

 Nothing to do */

 Set sndbuf and rcvbuf of passive connections */

 SPDX-License-Identifier: GPL-2.0-only

 load BPF program */

/* This test is a demo of using get_socket_uid and get_socket_cookie

 * helper function to do per socket based network traffic monitoring.

 * It requires iptables version higher then 1.6.1. to load pinned eBPF

 * program into the xt_bpf match.

 *

 * TEST:

 * ./run_cookie_uid_helper_example.sh -option

 * option:

 *	-t: do traffic monitoring test, the program will continuously

 * print out network traffic happens after program started A sample

 * output is shown below:

 *

 * cookie: 877, uid: 0x3e8, Pakcet Count: 20, Bytes Count: 11058

 * cookie: 132, uid: 0x0, Pakcet Count: 2, Bytes Count: 286

 * cookie: 812, uid: 0x3e8, Pakcet Count: 3, Bytes Count: 1726

 * cookie: 802, uid: 0x3e8, Pakcet Count: 2, Bytes Count: 104

 * cookie: 877, uid: 0x3e8, Pakcet Count: 20, Bytes Count: 11058

 * cookie: 831, uid: 0x3e8, Pakcet Count: 2, Bytes Count: 104

 * cookie: 0, uid: 0x0, Pakcet Count: 6, Bytes Count: 712

 * cookie: 880, uid: 0xfffe, Pakcet Count: 1, Bytes Count: 70

 *

 *	-s: do getsockopt SO_COOKIE test, the program will set up a pair of

 * UDP sockets and send packets between them. And read out the traffic data

 * directly from the ebpf map based on the socket cookie.

 *

 * Clean up: if using shell script, the script file will delete the iptables

 * rule and unmount the bpf program when exit. Else the iptables rule need

 * to be deleted by hand, see run_cookie_uid_helper_example.sh for detail.

		/*

		 * Save sk_buff for future usage. value stored in R6 to R10 will

		 * not be reset after a bpf helper function call.

		/*

		 * pc1: BPF_FUNC_get_socket_cookie takes one parameter,

		 * R1: sk_buff

 pc2-4: save &socketCookie to r7 for future usage*/

		/*

		 * pc5-8: set up the registers for BPF_FUNC_map_lookup_elem,

		 * it takes two parameters (R1: map_fd,  R2: &socket_cookie)

		/*

		 * pc9. if r0 != 0x0, go to pc+14, since we have the cookie

		 * stored already

		 * Otherwise do pc10-22 to setup a new data entry.

		/*

		 * Place a struct stats in the R10 stack and sequentially

		 * place the member value into the memory. Packets value

		 * is set by directly place a IMM value 1 into the stack.

		/*

		 * __sk_buff is a special struct used for eBPF program to

		 * directly access some sk_buff field.

		/*

		 * add new map entry using BPF_FUNC_map_update_elem, it takes

		 * 4 parameters (R1: map_fd, R2: &socket_cookie, R3: &stats,

		 * R4: flags)

		/*

		 * pc24-30 update the packet info to a exist data entry, it can

		 * be done by directly write to pointers instead of using

		 * BPF_FUNC_map_update_elem helper function

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2017 Jesper Dangaard Brouer, Red Hat, Inc. */

 Parse commands line args */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 PLUMgrid

/* simple per-protocol drop counter

 default, set below */

 SPDX-License-Identifier: GPL-2.0

 Copyright (c) 2018 Facebook */

/* Copyright (c) 2016 PLUMgrid

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 Handle VLAN tagged packet */

 Handle double VLAN tagged packet */

/* eBPF example program:

 *

 * - Loads eBPF program

 *

 *   The eBPF program sets the sk_bound_dev_if index in new AF_INET{6}

 *   sockets opened by processes in the cgroup.

 *

 * - Attaches the new program to a cgroup using BPF_PROG_ATTACH

 save pointer to context */

 r0 = verdict */

 set sk_bound_dev_if on socket */

 set mark on socket */

 get uid of process */

 if uid is 0, use given mark, else use the uid as the mark */

 set the mark on the new socket */

 set priority on socket */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 counts, stackmap */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2017 Facebook

/* SPDX-License-Identifier: GPL-2.0

 * Copyright (c) 2018 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* simple "icmp packet too big sent" counter

 default, set below */

 static global var 'max_pcktsz' is accessible from .data section */

 fetch icmpcnt map */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 use inner_lru as inner map */

 statically initialize the first element */

 SPDX-License-Identifier: GPL-2.0

/* Copyright 2016 Netflix, Inc.

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 E2BIG not tested for this example only */

/* Copyright (c) 2017 Covalent IO, Inc. http://covalent.io

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

/* The 2nd xdp prog on egress does not support skb mode, so we define two

 * maps, tx_port_general and tx_port_native.

 store egress interface mac address */

 Redirect require an XDP bpf_prog loaded on the TX device */

/* Copyright (c) 2016 Thomas Graf <tgraf@tgraf.ch>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

 SPDX-License-Identifier: GPL-2.0

 counters, values, values2 */

 Move to target CPU */

 Open perf event and attach to the perf_event_array */

pid*/, cpugroup_fd*/, 0);

 Trigger the kprobe */

 Check the value */

 The above bpf_map_lookup_elem should trigger the second kprobe */

 Intel Instruction Retired */

 From /sys/bus/event_source/devices/msr/ */

 below tests may fail in qemu */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2017-18 David Ahern <dsahern@gmail.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

 Adding ifindex as a possible egress TX port */

	/* TODO: Remember to cleanup map, when adding use of shared map

	 *  bpf_map_delete_elem((map_fd, &idx);

			/* If not, the error message will be:

			 *  "cannot pass map_type 14 into func bpf_map_lookup_elem#1"

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Sample BPF program to set send and receive buffers to 150KB, sndcwnd clamp

 * to 100 packets and SYN and SYN_ACK RTOs to 10ms when both hosts are within

 * the same datacenter. For his example, we assume they are within the same

 * datacenter when the first 5.5 bytes of their IPv6 addresses are the same.

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

	/* Check that both hosts are within same datacenter. For this example

	 * it is the case when the first 5.5 bytes of their IPv6 addresses are

	 * the same.

 Set sndbuf and rcvbuf of active connections */

 Set sndbuf and rcvbuf of passive connections */

/* SPDX-License-Identifier: GPL-2.0

 * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.

 *

 *  Example howto extract XDP RX-queue info

/* Config setup from with userspace

 *

 * User-side setup ifindex in config_map, to verify that

 * ctx->ingress_ifindex is correct (against configured ifindex)

 Common stats data record (shared with userspace) */

 Stats per rx_queue_index (per CPU) */

 Global stats record */

	/* Accessing ctx->ingress_ifindex, cause BPF to rewrite BPF

	 * instructions inside kernel to access xdp_rxq->dev->ifindex

 Simple test: check ctx provided ifindex is as expected */

 count this error case */

	/* Update stats per rx_queue_index. Handle if rx_queue_index

	 * is larger than stats map can contain info for.

 Default: Don't touch packet data, only count packets */

 Avoid compiler removing this: Drop non 802.3 Ethertypes */

		/* XDP_TX requires changing MAC-addrs, else HW may drop.

		 * Can also be enabled with --swapmac (for test purposes)

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 load BPF program */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 General args */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Facebook

 counts, stackmap */

 clear stack map */

 system wide perf event, no need to inherit */

 open perf_event on all cpus */

	/* per task perf event, enable inherit so the "dd ..." command can be traced properly.

	 * Enabling inherit will cause bpf_perf_prog_read_time helper failure.

 open task bound event */

 Intel Instruction Retired */

 Intel MEM_UOPS_RETIRED.LOCK_LOADS */

 Request to record lock address from PEBS */

 Record address value requires precise event */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

 10^9 */

 Key=1 keeps unknown errors */

 Keyed from Unknown */

 For percpu maps, userspace gets a value per possible CPU */

 Get time as close as possible to reading map contents */

 Record and sum values from each CPU */

 cpumap enqueue stats */

 calc average bulk size */

 calc average bulk size */

 Fold out errors after heading */

 calc avg bulk */

 calc avg bulk */

 Find matching entry from stats_prev map */

 calc avg bulk */

 We are responsible for filling out totals */

 Skip idle streams of redirection */

 calc avg bulk */

 Pointer swap trick */

 Pretty print numbers */

 Just exit, nothing to cleanup right now */

 SPDX-License-Identifier: GPL-2.0

/* This XDP program is only needed for the XDP_SHARED_UMEM mode.

 * If you do not use this mode, libbpf can supply an XDP program for you.

 SPDX-License-Identifier: GPL-2.0

 Metadata will be in the perf event before the packet data. */

		/* The XDP perf_event_output handler will use the upper 32 bits

		 * of the flags argument as a number of bytes to include of the

		 * packet payload in the event data. If the size is too big, the

		 * call to bpf_perf_event_output will fail and return -EFAULT.

		 *

		 * See bpf_xdp_event_output in net/core/filter.c.

		 *

		 * The BPF_F_CURRENT_CPU flag means that the event output fd

		 * will be indexed by the CPU number in the event map.

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* SPDX-License-Identifier: GPL-2.0

 * Copyright (c) 2017 Jesper Dangaard Brouer, Red Hat Inc.

 Exit return codes */

 10^9 */

 Common stats data record shared with _kern.c */

 For percpu maps, userspace gets a value per possible CPU */

 Get time as close as possible to reading map contents */

 Record and sum values from each CPU */

 Header */

 stats_global_map */

 rx_queue_index_map */

 Last RXQ in map catch overflows */

 Pointer swap trick */

 Default: Don't touch packet memory */

 for \0 */] = { 0 };

 Default action */

 Parse commands line args */

 Required option */

 Parse action string */

 XDP_TX requires changing MAC-addrs, else HW may drop */

 Trick to pretty printf with thousands separators use %' */

 User-side setup ifindex in config_map */

 Remove XDP program when program is interrupted or killed */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 ignore warmup */

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2020 Intel Corporation. */

/* This program illustrates the packet forwarding between multiple AF_XDP

 * sockets in multi-threaded environment. All threads are sharing a common

 * buffer pool, with each socket having its own private buffer cache.

 *

 * Example 1: Single thread handling two sockets. The packets received by socket

 * A (interface IFA, queue QA) are forwarded to socket B (interface IFB, queue

 * QB), while the packets received by socket B are forwarded to socket A. The

 * thread is running on CPU core X:

 *

 *         ./xsk_fwd -i IFA -q QA -i IFB -q QB -c X

 *

 * Example 2: Two threads, each handling two sockets. The thread running on CPU

 * core X forwards all the packets received by socket A to socket B, and all the

 * packets received by socket B to socket A. The thread running on CPU core Y is

 * performing the same packet forwarding between sockets C and D:

 *

 *         ./xsk_fwd -i IFA -q QA -i IFB -q QB -i IFC -q QC -i IFD -q QD

 *         -c CX -c CY

/*

 * Buffer pool and buffer cache

 *

 * For packet forwarding, the packet buffers are typically allocated from the

 * pool for packet reception and freed back to the pool for further reuse once

 * the packet transmission is completed.

 *

 * The buffer pool is shared between multiple threads. In order to minimize the

 * access latency to the shared buffer pool, each thread creates one (or

 * several) buffer caches, which, unlike the buffer pool, are private to the

 * thread that creates them and therefore cannot be shared with other threads.

 * The access to the shared pool is only needed either (A) when the cache gets

 * empty due to repeated buffer allocations and it needs to be replenished from

 * the pool, or (B) when the cache gets full due to repeated buffer free and it

 * needs to be flushed back to the pull.

 *

 * In a packet forwarding system, a packet received on any input port can

 * potentially be transmitted on any output port, depending on the forwarding

 * configuration. For AF_XDP sockets, for this to work with zero-copy of the

 * packet buffers when, it is required that the buffer pool memory fits into the

 * UMEM area shared by all the sockets.

/* This buffer pool implementation organizes the buffers into equally sized

 * slabs of *n_buffers_per_slab*. Initially, there are *n_slabs* slabs in the

 * pool that are completely filled with buffer pointers (full slabs).

 *

 * Each buffer cache has a slab for buffer allocation and a slab for buffer

 * free, with both of these slabs initially empty. When the cache's allocation

 * slab goes empty, it is swapped with one of the available full slabs from the

 * pool, if any is available. When the cache's free slab goes full, it is

 * swapped for one of the empty slabs from the pool, which is guaranteed to

 * succeed.

 *

 * Partially filled slabs never get traded between the cache and the pool

 * (except when the cache itself is destroyed), which enables fast operation

 * through pointer swapping.

 mmap prep. */

 bpool internals dimensioning. */

 bpool memory allocation. */

 bpool memory initialization. */

 lock. */

 mmap. */

 umem. */

	/* In order to keep this example simple, the case of freeing any

	 * existing buffers from the cache back to the pool is ignored.

/* To work correctly, the implementation requires that the *n_buffers* input

 * argument is never greater than the buffer pool's *n_buffers_per_slab*. This

 * is typically the case, with one exception taking place when large number of

 * buffers are allocated at init time (e.g. for the UMEM fill queue setup).

	/*

	 * Consumer slab is not empty: Use what's available locally. Do not

	 * look for more buffers from the pool when the ask can only be

	 * partially satisfied.

	/*

	 * Consumer slab is empty: look to trade the current consumer slab

	 * (full) for a full slab from the pool, if any is available.

	/*

	 * Producer slab is not yet full: store the current buffer to it.

	/*

	 * Producer slab is full: trade the cache's current producer slab

	 * (full) for an empty slab from the pool, then store the current

	 * buffer to the new producer slab. As one full slab exists in the

	 * cache, it is guaranteed that there is at least one empty slab

	 * available in the pool.

/*

 * Port

 *

 * Each of the forwarding ports sits on top of an AF_XDP socket. In order for

 * packet forwarding to happen with no packet buffer copy, all the sockets need

 * to share the same UMEM area, which is used as the buffer pool memory.

	/* To keep this example simple, the code to free the buffers from the

	 * socket's receive and transmit queues, as well as from the UMEM fill

	 * and completion queues, is not included.

 Memory allocation and initialization. */

 bcache. */

 xsk socket. */

 umem fq. */

 Free buffers for FQ replenish. */

 RXQ. */

 UMEM FQ. */

 UMEM CQ. */

 TXQ. */

/*

 * Thread

 *

 * Packet forwarding threads.

 RX. */

 Process & TX. */

/*

 * Process

 Parse the input arguments. */

 reset getopt lib */

 Check the input arguments. */

 Parse args. */

 Buffer pool initialization. */

 Ports initialization. */

 Threads. */

 Print statistics. */

 Threads completion. */

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

 my_map, my_hist_map */

 load BPF program */

 start 'ping' in the background to have some kfree_skb events */

 start 'dd' in the background to have plenty of 'write' syscalls */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set base_rtt to 80us when host is running TCP-NV and

 * both hosts are in the same datacenter (as determined by IPv6 prefix).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* Check if both hosts are in the same datacenter. For this

	 * example they are if the 1st 5.5 bytes in the IPv6 address

	 * are the same.

 Set base_rtt to 80us */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2019 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Example program for Host Bandwidth Managment

 *

 * This program loads a cgroup skb BPF program to enforce cgroup output

 * (egress) or input (ingress) bandwidth limits.

 *

 * USAGE: hbm [-d] [-l] [-n <id>] [-r <rate>] [-s] [-t <secs>] [-w] [-h] [prog]

 *   Where:

 *    -d	Print BPF trace debug buffer

 *    -l	Also limit flows doing loopback

 *    -n <#>	To create cgroup \"/hbm#\" and attach prog

 *		Default is /hbm1

 *    --no_cn   Do not return cn notifications

 *    -r <rate>	Rate limit in Mbps

 *    -s	Get HBM stats (marked, dropped, etc.)

 *    -t <time>	Exit after specified seconds (default is 0)

 *    -w	Work conserving flag. cgroup can increase its bandwidth

 *		beyond the rate limit specified while there is available

 *		bandwidth. Current implementation assumes there is only

 *		NIC (eth0), but can be extended to support multiple NICs.

 *		Currrently only supported for egress.

 *    -h	Print this info

 *    prog	BPF program file name. Name defaults to hbm_out_kern.o

 cgroup rate limit in Mbps */

 can grow if rate conserving is enabled */

	Future support of ingress

	if (!outFlag)

		outFname = "hbm_in.log";

 load BPF program */

 in us */

 9.5 Gbps */

				/* can increase cgroup rate limit, but first

				 * check if we are using the current limit.

				 * Currently increasing by 6.25%, unknown

				 * if that is the optimal rate.

				/* Need to decrease cgroup rate limit.

				 * Currently decreasing by 12.5%, unknown

				 * if that is optimal

 Get stats!

 Future support of ingress

		if (!outFlag)

			sprintf(fname, "hbm.%d.in", cg_id);

		else

 Marked Pkts and Bytes

 Dropped Pkts and Bytes

 ECN CE markings

 Average cwnd

 Average rtt

 Average credit

 Return values stats

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2015 PLUMgrid, http://plumgrid.com

 clear screen */

 object was allocated more then 1 sec ago */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

 General args */

 Map-related args */

 Prog-related args */

/* Copyright (c) 2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* kprobe is NOT a stable ABI. If kernel internals change this bpf+kprobe

 * example will no longer be meaningful

 get ip address of kmem_cache_alloc_node() caller */

/*

 * Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 map #0 */

 map #1 */

 map #2 */

 map #3 */

 map #4 */ 
 use inner_a as inner map */

 map #5 */ 
 use inner_a as inner map */

 map #6 */ 
 use inner_h as inner map */

/* Copyright (c) 2017 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * BPF program to set congestion control to dctcp when both hosts are

 * in the same datacenter (as deteremined by IPv6 prefix).

 *

 * Use "bpftool cgroup attach $cg sock_ops $prog" to load this BPF program.

	/* For testing purposes, only execute rest of BPF program

	 * if neither port numberis 55601

	/* Check if both hosts are in the same datacenter. For this

	 * example they are if the 1st 5.5 bytes in the IPv6 address

	 * are the same.

/* eBPF example program:

 * - creates arraymap in kernel with key 4 bytes and value 8 bytes

 *

 * - loads eBPF program:

 *   r0 = skb->data[ETH_HLEN + offsetof(struct iphdr, protocol)];

 *   *(u32*)(fp - 4) = r0;

 *   // assuming packet is IPv4, lookup ip->proto in a map

 *   value = bpf_map_lookup_elem(map_fd, fp - 4);

 *   if (value)

 *        (*(u64*)value) += 1;

 *

 * - attaches this program to loopback interface "lo" raw socket

 *

 * - every second user space reads map[tcp], map[udp], map[icmp] to see

 *   how many packets of given protocol were seen on "lo"

 R0 = ip->proto */),

 *(u32 *)(fp - 4) = r0 */

 r2 = fp - 4 */

 r1 = 1 */

 r0 = 0 */

 maps, programs, raw sockets will auto cleanup on process exit */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 from /sys/kernel/debug/tracing/events/task/task_rename/format */

 from /sys/kernel/debug/tracing/events/random/urandom_read/format */

 SPDX-License-Identifier: GPL-2.0

 map to store egress interfaces mac addresses */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2016 John Fastabend <john.r.fastabend@intel.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

 Redirect require an XDP bpf_prog loaded on the TX device */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 taken from /sys/kernel/debug/tracing/events/sched/sched_switch/format */

 record previous thread sleep time */

 record previous thread sleep time */

 calculate current thread's delta time */

 missed start or filtered */

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 * Copyright (c) 2015 BMW Car IT GmbH

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* We need to stick to static allocated memory (an array instead of

 * hash table) because managing dynamic memory from the

 * trace_preempt_[on|off] tracepoints hooks is not supported.

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 copy of 'struct ethhdr' without __packed */

 single length check */

/* Copyright (c) 2016 Thomas Graf <tgraf@tgraf.ch>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

 Test: Pass all packets through */

 Test: Verify context information can be accessed */

 Test: Ensure skb->cb[] buffer is cleared */

 Test: Verify skb data can be read */

 Test: Verify skb data can be modified */

 Drop all packets */

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* Copyright (c) 2016 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 copy of 'struct ethhdr' without __packed */

 single length check */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 Clear screen */

 Header */

/*

 * This function is copied from 'idlestat' tool function

 * idlestat_wake_all() in idlestate.c.

 *

 * It sets the self running task affinity to cpus one by one so can wake up

 * the specific CPU to handle scheduling; this results in all cpus can be

 * waken up once and produce ftrace event 'trace_cpu_idle'.

 Keep track of the CPUs we will run on */

 Pointless to wake up ourself */

 Pointless to wake CPUs we will not run on */

 Enable all the CPUs of the original mask */

/*

 * It's possible to have no any frequency change for long time and cannot

 * get ftrace event 'trace_cpu_frequency' for long period, this introduces

 * big deviation for pstate statistics.

 *

 * To solve this issue, below code forces to set 'scaling_max_freq' to 208MHz

 * for triggering ftrace event 'trace_cpu_frequency' and then recovery back to

 * the maximum frequency value 1.2GHz.

 load BPF program */

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* kprobe is NOT a stable ABI. If kernel internals change this bpf+kprobe

 * example will no longer be meaningful

	/* the lines below are computing index = log10(delta)*10

	 * using integer arithmetic

	 * index = 29 ~ 1 usec

	 * index = 59 ~ 1 msec

	 * index = 89 ~ 1 sec

	 * index = 99 ~ 10sec or more

	 * log10(x)*10 = log2(x)*10/log2(10) = log2(x)*3

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright(c) 2017 Jesper Dangaard Brouer, Red Hat, Inc.

	/* Add a CPU entry to cpumap, as this allocate a cpu entry in

	 * the kernel for the cpu.

	/* Inform bpf_prog's that a new CPU is available to select

	 * from via some control maps.

 When not replacing/updating existing entry, bump the count */

/* CPUs are zero-indexed. Thus, add a special sentinel default value

 * in map cpus_available to mark CPU index'es not configured

 Stress cpumap management code by concurrently changing underlying cpumap */

	/* Changing qsize will cause kernel to free and alloc a new

	 * bpf_cpu_map_entry, with an associated/complicated tear-down

	 * procedure.

 Custom BPF program */

 Use built-in pass/drop programs */

 Use built-in devmap redirect */

 Disabled */

	/* Notice: Choosing the queue size is very important when CPU is

	 * configured with power-saving states.

	 *

	 * If deepest state take 133 usec to wakeup from (133/10^6). When link

	 * speed is 10Gbit/s ((10*10^9/8) in bytes/sec). How many bytes can

	 * arrive with in 133 usec at this speed: (10*10^9/8)*(133/10^6) =

	 * 166250 bytes. With MTU size packets this is 110 packets, and with

	 * minimum Ethernet (MAC-preamble + intergap) 84 bytes is 1979 packets.

	 *

	 * Setting default cpumap queue to 2048 as worst-case (small packet)

	 * should be +64 packet due kthread wakeup call (due to xdp_do_flush)

	 * worst-case is 2043 packets.

	 *

	 * Sysadm can configured system to avoid deep-sleep via:

	 *   tuned-adm profile network-latency

 Selecting eBPF prog to load */

 Add multiple CPUs */

/* Copyright (c) 2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 MIPS n64 syscalls start at 5000 */

 dispatch into next BPF program depending on syscall number */

 fall through -> unknown syscall */

 we jump here when syscall number == __NR_write */

/* Copyright (c) 2016 Sargun Dhillon <sargun@sargun.me>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* kprobe is NOT a stable ABI

 * kernel functions can be removed, renamed or completely change semantics.

 * Number of arguments and their positions can change, etc.

 * In such case this bpf+kprobe example will no longer be meaningful

 *

 * This example sits on a syscall, and the syscall ABI is relatively stable

 * of course, across platforms, and over time, the ABI may change.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2017 Facebook

 Test case #0 */

 Test case #1 */

 Test case #2 */

 load BPF program */

 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 load BPF program */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2017-18 David Ahern <dsahern@gmail.com>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it will be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU

 * General Public License for more details.

 from include/net/ip.h */

	/*

	 * Some rc (return codes) from bpf_fib_lookup() are important,

	 * to understand how this XDP-prog interacts with network stack.

	 *

	 * BPF_FIB_LKUP_RET_NO_NEIGH:

	 *  Even if route lookup was a success, then the MAC-addresses are also

	 *  needed.  This is obtained from arp/neighbour table, but if table is

	 *  (still) empty then BPF_FIB_LKUP_RET_NO_NEIGH is returned.  To avoid

	 *  doing ARP lookup directly from XDP, then send packet to normal

	 *  network stack via XDP_PASS and expect it will do ARP resolution.

	 *

	 * BPF_FIB_LKUP_RET_FWD_DISABLED:

	 *  The bpf_fib_lookup respect sysctl net.ipv{4,6}.conf.all.forwarding

	 *  setting, and will return BPF_FIB_LKUP_RET_FWD_DISABLED if not

	 *  enabled this on ingress device.

		/* Verify egress index has been configured as TX-port.

		 * (Note: User can still have inserted an egress ifindex that

		 * doesn't support XDP xmit, which will result in packet drops).

		 *

		 * Note: lookup in devmap supported since 0cdbb4b09a0.

		 * If not supported will fail with:

		 *  cannot pass map_type 14 into func bpf_map_lookup_elem#1:

 SPDX-License-Identifier: GPL-2.0

/*

 * The CPU number, cstate number and pstate number are based

 * on 96boards Hikey with octa CA53 CPUs.

 *

 * Every CPU have three idle states for cstate:

 *   WFI, CPU_OFF, CLUSTER_OFF

 *

 * Every CPU have 5 operating points:

 *   208MHz, 432MHz, 729MHz, 960MHz, 1200MHz

 *

 * This code is based on these assumption and other platforms

 * need to adjust these definitions.

/*

 * my_map structure is used to record cstate and pstate index and

 * timestamp (Idx, Ts), when new event incoming we need to update

 * combination for new state index and timestamp (Idx`, Ts`).

 *

 * Based on (Idx, Ts) and (Idx`, Ts`) we can calculate the time

 * interval for the previous state: Duration(Idx) = Ts` - Ts.

 *

 * Every CPU has one below array for recording state index and

 * timestamp, and record for cstate and pstate saperately:

 *

 * +--------------------------+

 * | cstate timestamp         |

 * +--------------------------+

 * | cstate index             |

 * +--------------------------+

 * | pstate timestamp         |

 * +--------------------------+

 * | pstate index             |

 * +--------------------------+

 cstate_duration records duration time for every idle state per CPU */

 pstate_duration records duration time for every operating point per CPU */

/*

 * The trace events for cpu_idle and cpu_frequency are taken from:

 * /sys/kernel/debug/tracing/events/power/cpu_idle/format

 * /sys/kernel/debug/tracing/events/power/cpu_frequency/format

 *

 * These two events have same format, so define one common structure.

 calculate pstate index, returns MAX_PSTATE_ENTRIES for failure */

	/*

	 * When state doesn't equal to (u32)-1, the cpu will enter

	 * one idle state; for this case we need to record interval

	 * for the pstate.

	 *

	 *                 OPP2

	 *            +---------------------+

	 *     OPP1   |                     |

	 *   ---------+                     |

	 *                                  |  Idle state

	 *                                  +---------------

	 *

	 *            |<- pstate duration ->|

	 *            ^                     ^

	 *           pts                  cur_ts

 record pstate after have first cpu_frequency event */

	/*

	 * When state equal to (u32)-1, the cpu just exits from one

	 * specific idle state; for this case we need to record

	 * interval for the pstate.

	 *

	 *       OPP2

	 *   -----------+

	 *              |                          OPP1

	 *              |                     +-----------

	 *              |     Idle state      |

	 *              +---------------------+

	 *

	 *              |<- cstate duration ->|

	 *              ^                     ^

	 *             cts                  cur_ts

 Update timestamp for pstate as new start time */

 When CPU is in idle, bail out to skip pstate statistics */

	/*

	 * The cpu changes to another different OPP (in below diagram

	 * change frequency from OPP3 to OPP1), need recording interval

	 * for previous frequency OPP3 and update timestamp as start

	 * time for new frequency OPP1.

	 *

	 *                 OPP3

	 *            +---------------------+

	 *     OPP2   |                     |

	 *   ---------+                     |

	 *                                  |    OPP1

	 *                                  +---------------

	 *

	 *            |<- pstate duration ->|

	 *            ^                     ^

	 *           pts                  cur_ts

/* SPDX-License-Identifier: GPL-2.0

 * Copyright (c) 2018 Jesper Dangaard Brouer, Red Hat Inc.

 *

 * Example howto transfer info from XDP to SKB, e.g. skb->mark

 * -----------------------------------------------------------

 * This uses the XDP data_meta infrastructure, and is a cooperation

 * between two bpf-programs (1) XDP and (2) clsact at TC-ingress hook.

 *

 * Notice: This example does not use the BPF C-loader,

 * but instead rely on the iproute2 TC tool for loading BPF-objects.

/*

 * This struct is stored in the XDP 'data_meta' area, which is located

 * just in-front-of the raw packet payload data.  The meaning is

 * specific to these two BPF programs that use it as a communication

 * channel.  XDP adjust/increase the area via a bpf-helper, and TC use

 * boundary checks to see if data have been provided.

 *

 * The struct must be 4 byte aligned, which here is enforced by the

 * struct __attribute__((aligned(4))).

	/* Reserve space in-front of data pointer for our meta info.

	 * (Notice drivers not supporting data_meta will fail here!)

	/* Notice: Kernel-side verifier requires that loading of

	 * ctx->data MUST happen _after_ helper bpf_xdp_adjust_meta(),

	 * as pkt-data pointers are invalidated.  Helpers that require

	 * this are determined/marked by bpf_helper_changes_pkt_data()

 Check data_meta have room for meta_info struct */

 Check XDP gave us some data_meta */

 Skip "accept" if no data_meta is avail */

 Hint: See func tc_cls_act_is_valid_access() for BPF_WRITE access */

 Transfer XDP-mark to SKB-mark */

/* Manually attaching these programs:

export DEV=ixgbe2

export FILE=xdp2skb_meta_kern.o



# via TC command

tc qdisc del dev $DEV clsact 2> /dev/null

tc qdisc add dev $DEV clsact

tc filter  add dev $DEV ingress prio 1 handle 1 bpf da obj $FILE sec tc_mark

tc filter show dev $DEV ingress



# XDP via IP command:

ip link set dev $DEV xdp off

ip link set dev $DEV xdp obj $FILE sec xdp_mark



# Use iptable to "see" if SKBs are marked

iptables -I INPUT -p icmp -m mark --mark 41  # == 0x29

iptables -I INPUT -p icmp -m mark --mark 42  # == 0x2a



# Hint: catch XDP_ABORTED errors via

perf record -e xdp:*

perf script



 SPDX-License-Identifier: GPL-2.0-only

/* Copyright (c) 2016 Sargun Dhillon <sargun@sargun.me>

 load BPF program */

	/*

	 * The installed helper program catched the sync call, and should

	 * write it to the map.

 Verify the negative scenario; leave the cgroup */

 SPDX-License-Identifier: GPL-2.0

/* Refer to samples/bpf/tcp_bpf.readme for the instructions on

 * how to run this sample program.

 SPDX-License-Identifier: GPL-2.0

/* eBPF example program:

 *

 * - Loads eBPF program

 *

 *   The eBPF program loads a filter from file and attaches the

 *   program to a cgroup using BPF_PROG_ATTACH

 load BPF program */

/*  XDP redirect to CPUs via cpumap (BPF_MAP_TYPE_CPUMAP)

 *

 *  GPLv2, Copyright(c) 2017 Jesper Dangaard Brouer, Red Hat, Inc.

 Special map type that can XDP_REDIRECT frames to another CPU */

/* Set of maps controlling available CPU, and for iterating through

 * selectable redirect CPUs.

 Helper parse functions */

 Skip non 802.3 Ethertypes */

 Handle VLAN tagged packet */

 Handle double VLAN tagged packet */

 Only use first entry in cpus_available */

 Only use first entry in cpus_available */

 Validate packet length is minimum Eth header size */

 Read packet data, and use it (drop non 802.3 Ethertypes) */

 Just skip */

 Extract L4 protocol */

 ARP packet handled on separate CPU */

 Choose CPU based on L4 protocol */

 Just skip */

 Extract L4 protocol */

 ARP packet handled on separate CPU */

 Choose CPU based on L4 protocol */

 DDoS filter UDP port 9 (pktgen) */

 Hashing initval */

/* Load-Balance traffic based on hashing IP-addrs + L4-proto.  The

 * hashing scheme is symmetric, meaning swapping IP src/dest still hit

 * same CPU.

 Just skip */

 Hash for IPv4 and IPv6 */

 ARP packet handled on CPU idx 0 */

 Choose CPU based on hash */

 SPDX-License-Identifier: GPL-2.0

  GPLv2, Copyright(c) 2017 Jesper Dangaard Brouer, Red Hat, Inc. */

/* These can be set before loading so that redundant comparisons can be DCE'd by

 * the verifier, and only actual matches are tried after loading tp_btf program.

 * This allows sample to filter tracepoint stats based on net_device.

 Find if b is part of set a, but if a is empty set then evaluate to true */

 Indicate event was filtered (no further processing)*/

	/*

	 * Returning 1 here would allow e.g. a perf-record tracepoint

	 * to see and record these events, but it doesn't work well

	 * in-practice as stopping perf-record also unload this

	 * bpf_prog.  Plus, there is additional overhead of doing so.

 Record bulk events, then userspace can calc average bulk size */

	/* Inception: It's possible to detect overload situations, via

	 * this tracepoint.  This can be used for creating a feedback

	 * loop to XDP, which can take appropriate actions to mitigate

	 * this overload situation.

 Count times kthread yielded CPU via schedule call */

 Record bulk events, then userspace can calc average bulk size */

 Record error cases, where no frame were sent */

 Catch API error of drv ndo_xdp_xmit sent more than count */

/* Copyright (c) 2013-2015 PLUMgrid, http://plumgrid.com

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

/* kprobe is NOT a stable ABI. If kernel internals change this bpf+kprobe

 * example will no longer be meaningful

	/* read ip of kfree_skb caller.

	 * non-portable version of __builtin_return_address(0)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sampleip: sample instruction pointer and frequency count in a BPF map.

 *

 * Copyright 2016 Netflix, Inc.

 pid */, i,

 group_fd */, 0 
 used for sorting */

 fetch IPs and counts */

 sort and print */

 process arguments */

 initialize kernel symbol translation */

 create perf FDs for each CPU */

 load BPF program */

 do sampling */

 output sample counts */

 SPDX-License-Identifier: GPL-2.0

/* Copyright (c) 2019 Facebook

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 *

 * Sample Host Bandwidth Manager (HBM) BPF program.

 *

 * A cgroup skb BPF egress program to limit cgroup output bandwidth.

 * It uses a modified virtual token bucket queue to limit average

 * egress bandwidth. The implementation uses credits instead of tokens.

 * Negative credits imply that queueing would have happened (this is

 * a virtual queue, so no queueing is done by it. However, queueing may

 * occur at the actual qdisc (which is not used for rate limiting).

 *

 * This implementation uses 3 thresholds, one to start marking packets and

 * the other two to drop packets:

 *                                  CREDIT

 *        - <--------------------------|------------------------> +

 *              |    |          |      0

 *              |  Large pkt    |

 *              |  drop thresh  |

 *   Small pkt drop             Mark threshold

 *       thresh

 *

 * The effect of marking depends on the type of packet:

 * a) If the packet is ECN enabled and it is a TCP packet, then the packet

 *    is ECN marked.

 * b) If the packet is a TCP packet, then we probabilistically call tcp_cwr

 *    to reduce the congestion window. The current implementation uses a linear

 *    distribution (0% probability at marking threshold, 100% probability

 *    at drop threshold).

 * c) If the packet is not a TCP packet, then it is dropped.

 *

 * If the credit is below the drop threshold, the packet is dropped. If it

 * is a TCP packet, then it also calls tcp_cwr since packets dropped by

 * by a cgroup skb BPF program do not automatically trigger a call to

 * tcp_cwr in the current kernel code.

 *

 * This BPF program actually uses 2 drop thresholds, one threshold

 * for larger packets (>= 120 bytes) and another for smaller packets. This

 * protects smaller packets such as SYNs, ACKs, etc.

 *

 * The default bandwidth limit is set at 1Gbps but this can be changed by

 * a user program through a shared BPF map. In addition, by default this BPF

 * program does not limit connections using loopback. This behavior can be

 * overwritten by the user program. There is also an option to calculate

 * some statistics, such as percent of packets marked or dropped, which

 * a user program, such as hbm, can access.

 Check if we should ignore loopback traffic

 We may want to account for the length of headers in len

 calculation, like ETH header + overhead, specially if it

 is a gso packet. But I am not doing it right now.

 Begin critical section

 bound bursts to 100us

 negative delta is a credit that allows bursts

 End critical section

 Set EDT of packet

 Check if we should update rate

 Set flags (drop, congestion, cwr)

 last packet will be sent in the future, bound latency

 Do congestion control

 Problem if too many small packets?

 SPDX-License-Identifier: GPL-2.0

 load BPF program */

 attach BPF program to socket */

/* eBPF example program:

 *

 * - Creates arraymap in kernel with 4 bytes keys and 8 byte values

 *

 * - Loads eBPF program

 *

 *   The eBPF program accesses the map passed in to store two pieces of

 *   information. The number of invocations of the program, which maps

 *   to the number of packets received, is stored to key 0. Key 1 is

 *   incremented on each iteration by the number of bytes stored in

 *   the skb.

 *

 * - Attaches the new program to a cgroup using BPF_PROG_ATTACH

 *

 * - Every second, reads map[0] and map[1] to see how many bytes and

 *   packets were seen on any socket of tasks in the given cgroup.

 save r6 so it's not clobbered by BPF_CALL */

 Count packets */

 r0 = 0 */

 *(u32 *)(fp - 4) = r0 */

 r2 = fp - 4 */

 load map fd to r1 */

 r1 = 1 */

 Count bytes */

 r0 = 1 */

 *(u32 *)(fp - 4) = r0 */

 r2 = fp - 4 */

 r1 = skb->len */

 r0 = verdict */

/* Copyright (c) 2016 Sargun Dhillon <sargun@sargun.me>

 *

 * This program is free software; you can redistribute it and/or

 * modify it under the terms of version 2 of the GNU General Public

 * License as published by the Free Software Foundation.

 Writes the last PID that called sync to a map at index 0 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This implements the various checks for CONFIG_HARDENED_USERCOPY*,

 * which are designed to protect kernel memory from needless exposure

 * and overwrite under many unintended conditions. This code is based

 * on PAX_USERCOPY, which is:

 *

 * Copyright (C) 2001-2016 PaX Team, Bradley Spengler, Open Source

 * Security Inc.

/*

 * Checks if a given pointer and length is contained by the current

 * stack frame (if possible).

 *

 * Returns:

 *	NOT_STACK: not at all on the stack

 *	GOOD_FRAME: fully within a valid stack frame

 *	GOOD_STACK: fully on the stack (when can't do frame-checking)

 *	BAD_STACK: error condition (invalid stack position or bad stack frame)

 Object is not on the stack at all. */

	/*

	 * Reject: object partially overlaps the stack (passing the

	 * check above means at least one end is within the stack,

	 * so if this check fails, the other end is outside the stack).

 Check if object is safely within a valid frame. */

/*

 * If these functions are reached, then CONFIG_HARDENED_USERCOPY has found

 * an unexpected state during a copy_from_user() or copy_to_user() call.

 * There are several checks being performed on the buffer by the

 * __check_object_size() function. Normal stack buffer usage should never

 * trip the checks, and kernel text addressing will always trip the check.

 * For cache objects, it is checking that only the whitelisted range of

 * bytes for a given cache is being accessed (via the cache's usersize and

 * useroffset fields). To adjust a cache whitelist, use the usercopy-aware

 * kmem_cache_create_usercopy() function to create the cache (and

 * carefully audit the whitelist range).

	/*

	 * For greater effect, it would be nice to do do_group_exit(),

	 * but BUG() actually hooks all the lock-breaking and per-arch

	 * Oops code, so that is used here instead.

 Returns true if any portion of [ptr,ptr+n) over laps with [low,high). */

 Does not overlap if entirely above or entirely below. */

 Is this address range in the kernel text area? */

	/*

	 * Some architectures have virtual memory mappings with a secondary

	 * mapping of the kernel text, i.e. there is more than one virtual

	 * kernel address that points to the kernel image. It is usually

	 * when there is a separate linear physical memory mapping, in that

	 * __pa() is not just the reverse of __va(). This can be detected

	 * and checked:

 No different mapping: we're done. */

 Check the secondary mapping... */

 Reject if object wraps past end of memory. */

 Reject if NULL or ZERO-allocation. */

 Checks for allocs that are marked in some way as spanning multiple pages. */

	/*

	 * Sometimes the kernel data regions are not marked Reserved (see

	 * check below). And sometimes [_sdata,_edata) does not cover

	 * rodata and/or bss, so check each range explicitly.

 Allow reads of kernel rodata region (if not marked as Reserved). */

 Allow kernel data region (if not marked as Reserved). */

 Allow kernel bss region (if not marked as Reserved). */

 Is the object wholly within one base page? */

 Allow if fully inside the same compound (__GFP_COMP) page. */

	/*

	 * Reject if range is entirely either Reserved (i.e. special or

	 * device memory), or CMA. Otherwise, reject since the object spans

	 * several independently allocated pages.

	/*

	 * When CONFIG_HIGHMEM=y, kmap_to_page() will give either the

	 * highmem page or fallback to virt_to_page(). The following

	 * is effectively a highmem-aware virt_to_head_page().

 Check slab allocator for flags and size. */

 Verify object does not incorrectly span multiple pages. */

/*

 * Validates that the given object is:

 * - not bogus address

 * - fully contained by stack (or stack frame, when available)

 * - fully within SLAB object (or object whitelist area, when available)

 * - not in kernel text

 Skip all tests if size is zero. */

 Check for invalid addresses. */

 Check for bad stack object. */

 Object is not touching the current process stack. */

		/*

		 * Object is either in the correct frame (when it

		 * is possible to check) or just generally on the

		 * process stack (when frame checking not available).

 Check for bad heap object. */

 Check for object in kernel to avoid text exposure. */

 SPDX-License-Identifier: GPL-2.0

/*

 *	linux/mm/msync.c

 *

 * Copyright (C) 1994-1999  Linus Torvalds

/*

 * The msync() system call.

/*

 * MS_SYNC syncs the entire file - including mappings.

 *

 * MS_ASYNC does not start I/O (it used to, up to 2.5.67).

 * Nor does it marks the relevant pages dirty (it used to up to 2.6.17).

 * Now it doesn't do anything, since dirty pages are properly tracked.

 *

 * The application may now run fsync() to

 * write out the dirty pages and wait on the writeout and check the result.

 * Or the application may run fadvise(FADV_DONTNEED) against the fd to start

 * async writeout immediately.

 * So by _not_ starting I/O in MS_ASYNC we provide complete flexibility to

 * applications.

	/*

	 * If the interval [start,end) covers some unmapped address ranges,

	 * just ignore them, but return -ENOMEM at the end. Besides, if the

	 * flag is MS_ASYNC (w/o MS_INVALIDATE) the result would be -ENOMEM

	 * anyway and there is nothing left to do, so return immediately.

 Still start < end. */

 Here start < vma->vm_end. */

 Here vma->vm_start <= start < vma->vm_end. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/percpu-vm.c - vmalloc area based chunk allocation

 *

 * Copyright (C) 2010		SUSE Linux Products GmbH

 * Copyright (C) 2010		Tejun Heo <tj@kernel.org>

 *

 * Chunks are mapped into vmalloc areas and populated page by page.

 * This is the default chunk allocator.

 must not be used on pre-mapped chunk */

/**

 * pcpu_get_pages - get temp pages array

 *

 * Returns pointer to array of pointers to struct page which can be indexed

 * with pcpu_page_idx().  Note that there is only one array and accesses

 * should be serialized by pcpu_alloc_mutex.

 *

 * RETURNS:

 * Pointer to temp pages array on success.

/**

 * pcpu_free_pages - free pages which were allocated for @chunk

 * @chunk: chunk pages were allocated for

 * @pages: array of pages to be freed, indexed by pcpu_page_idx()

 * @page_start: page index of the first page to be freed

 * @page_end: page index of the last page to be freed + 1

 *

 * Free pages [@page_start and @page_end) in @pages for all units.

 * The pages were allocated for @chunk.

/**

 * pcpu_alloc_pages - allocates pages for @chunk

 * @chunk: target chunk

 * @pages: array to put the allocated pages into, indexed by pcpu_page_idx()

 * @page_start: page index of the first page to be allocated

 * @page_end: page index of the last page to be allocated + 1

 * @gfp: allocation flags passed to the underlying allocator

 *

 * Allocate pages [@page_start,@page_end) into @pages for all units.

 * The allocation is for @chunk.  Percpu core doesn't care about the

 * content of @pages and will pass it verbatim to pcpu_map_pages().

/**

 * pcpu_pre_unmap_flush - flush cache prior to unmapping

 * @chunk: chunk the regions to be flushed belongs to

 * @page_start: page index of the first page to be flushed

 * @page_end: page index of the last page to be flushed + 1

 *

 * Pages in [@page_start,@page_end) of @chunk are about to be

 * unmapped.  Flush cache.  As each flushing trial can be very

 * expensive, issue flush on the whole region at once rather than

 * doing it for each cpu.  This could be an overkill but is more

 * scalable.

/**

 * pcpu_unmap_pages - unmap pages out of a pcpu_chunk

 * @chunk: chunk of interest

 * @pages: pages array which can be used to pass information to free

 * @page_start: page index of the first page to unmap

 * @page_end: page index of the last page to unmap + 1

 *

 * For each cpu, unmap pages [@page_start,@page_end) out of @chunk.

 * Corresponding elements in @pages were cleared by the caller and can

 * be used to carry information to pcpu_free_pages() which will be

 * called after all unmaps are finished.  The caller should call

 * proper pre/post flush functions.

/**

 * pcpu_post_unmap_tlb_flush - flush TLB after unmapping

 * @chunk: pcpu_chunk the regions to be flushed belong to

 * @page_start: page index of the first page to be flushed

 * @page_end: page index of the last page to be flushed + 1

 *

 * Pages [@page_start,@page_end) of @chunk have been unmapped.  Flush

 * TLB for the regions.  This can be skipped if the area is to be

 * returned to vmalloc as vmalloc will handle TLB flushing lazily.

 *

 * As with pcpu_pre_unmap_flush(), TLB flushing also is done at once

 * for the whole region.

/**

 * pcpu_map_pages - map pages into a pcpu_chunk

 * @chunk: chunk of interest

 * @pages: pages array containing pages to be mapped

 * @page_start: page index of the first page to map

 * @page_end: page index of the last page to map + 1

 *

 * For each cpu, map pages [@page_start,@page_end) into @chunk.  The

 * caller is responsible for calling pcpu_post_map_flush() after all

 * mappings are complete.

 *

 * This function is responsible for setting up whatever is necessary for

 * reverse lookup (addr -> chunk).

/**

 * pcpu_post_map_flush - flush cache after mapping

 * @chunk: pcpu_chunk the regions to be flushed belong to

 * @page_start: page index of the first page to be flushed

 * @page_end: page index of the last page to be flushed + 1

 *

 * Pages [@page_start,@page_end) of @chunk have been mapped.  Flush

 * cache.

 *

 * As with pcpu_pre_unmap_flush(), TLB flushing also is done at once

 * for the whole region.

/**

 * pcpu_populate_chunk - populate and map an area of a pcpu_chunk

 * @chunk: chunk of interest

 * @page_start: the start page

 * @page_end: the end page

 * @gfp: allocation flags passed to the underlying memory allocator

 *

 * For each cpu, populate and map pages [@page_start,@page_end) into

 * @chunk.

 *

 * CONTEXT:

 * pcpu_alloc_mutex, does GFP_KERNEL allocation.

/**

 * pcpu_depopulate_chunk - depopulate and unmap an area of a pcpu_chunk

 * @chunk: chunk to depopulate

 * @page_start: the start page

 * @page_end: the end page

 *

 * For each cpu, depopulate and unmap pages [@page_start,@page_end)

 * from @chunk.

 *

 * Caller is required to call pcpu_post_unmap_tlb_flush() if not returning the

 * region back to vmalloc() which will lazily flush the tlb.

 *

 * CONTEXT:

 * pcpu_alloc_mutex.

	/*

	 * If control reaches here, there must have been at least one

	 * successful population attempt so the temp pages array must

	 * be available now.

 unmap and free */

 no extra restriction */

/**

 * pcpu_should_reclaim_chunk - determine if a chunk should go into reclaim

 * @chunk: chunk of interest

 *

 * This is the entry point for percpu reclaim.  If a chunk qualifies, it is then

 * isolated and managed in separate lists at the back of pcpu_slot: sidelined

 * and to_depopulate respectively.  The to_depopulate list holds chunks slated

 * for depopulation.  They no longer contribute to pcpu_nr_empty_pop_pages once

 * they are on this list.  Once depopulated, they are moved onto the sidelined

 * list which enables them to be pulled back in for allocation if no other chunk

 * can suffice the allocation.

 do not reclaim either the first chunk or reserved chunk */

	/*

	 * If it is isolated, it may be on the sidelined list so move it back to

	 * the to_depopulate list.  If we hit at least 1/4 pages empty pages AND

	 * there is no system-wide shortage of empty pages aside from this

	 * chunk, move it to the to_depopulate list.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	linux/mm/filemap.c

 *

 * Copyright (C) 1994-1999  Linus Torvalds

/*

 * This file handles the generic file mmap semantics used by

 * most "normal" filesystems (but you don't /have/ to use this:

 * the NFS filesystem used to do this differently, for example)

/*

 * FIXME: remove all knowledge of the buffer layer from the core VM

 for try_to_free_buffers */

/*

 * Shared mappings implemented 30.11.1994. It's not fully working yet,

 * though.

 *

 * Shared mappings now work. 15.8.1995  Bruno.

 *

 * finished 'unifying' the page and buffer cache and SMP-threaded the

 * page-cache, 21.05.1999, Ingo Molnar <mingo@redhat.com>

 *

 * SMP-threaded pagemap-LRU 1999, Andrea Arcangeli <andrea@suse.de>

/*

 * Lock ordering:

 *

 *  ->i_mmap_rwsem		(truncate_pagecache)

 *    ->private_lock		(__free_pte->__set_page_dirty_buffers)

 *      ->swap_lock		(exclusive_swap_page, others)

 *        ->i_pages lock

 *

 *  ->i_rwsem

 *    ->invalidate_lock		(acquired by fs in truncate path)

 *      ->i_mmap_rwsem		(truncate->unmap_mapping_range)

 *

 *  ->mmap_lock

 *    ->i_mmap_rwsem

 *      ->page_table_lock or pte_lock	(various, mainly in memory.c)

 *        ->i_pages lock	(arch-dependent flush_dcache_mmap_lock)

 *

 *  ->mmap_lock

 *    ->invalidate_lock		(filemap_fault)

 *      ->lock_page		(filemap_fault, access_process_vm)

 *

 *  ->i_rwsem			(generic_perform_write)

 *    ->mmap_lock		(fault_in_readable->do_page_fault)

 *

 *  bdi->wb.list_lock

 *    sb_lock			(fs/fs-writeback.c)

 *    ->i_pages lock		(__sync_single_inode)

 *

 *  ->i_mmap_rwsem

 *    ->anon_vma.lock		(vma_adjust)

 *

 *  ->anon_vma.lock

 *    ->page_table_lock or pte_lock	(anon_vma_prepare and various)

 *

 *  ->page_table_lock or pte_lock

 *    ->swap_lock		(try_to_unmap_one)

 *    ->private_lock		(try_to_unmap_one)

 *    ->i_pages lock		(try_to_unmap_one)

 *    ->lruvec->lru_lock	(follow_page->mark_page_accessed)

 *    ->lruvec->lru_lock	(check_pte_range->isolate_lru_page)

 *    ->private_lock		(page_remove_rmap->set_page_dirty)

 *    ->i_pages lock		(page_remove_rmap->set_page_dirty)

 *    bdi.wb->list_lock		(page_remove_rmap->set_page_dirty)

 *    ->inode->i_lock		(page_remove_rmap->set_page_dirty)

 *    ->memcg->move_lock	(page_remove_rmap->lock_page_memcg)

 *    bdi.wb->list_lock		(zap_pte_range->set_page_dirty)

 *    ->inode->i_lock		(zap_pte_range->set_page_dirty)

 *    ->private_lock		(zap_pte_range->__set_page_dirty_buffers)

 *

 * ->i_mmap_rwsem

 *   ->tasklist_lock            (memory_failure, collect_procs_ao)

 hugetlb pages are represented by a single entry in the xarray */

 Leave page->index set: truncation lookup relies upon it */

	/*

	 * if we're uptodate, flush out into the cleancache, otherwise

	 * invalidate any existing cleancache entries.  We can't leave

	 * stale data around in the cleancache once our page is gone

			/*

			 * All vmas have already been torn down, so it's

			 * a good bet that actually the page is unmapped,

			 * and we'd prefer not to leak it: if we're wrong,

			 * some other bad page check should catch it later.

 hugetlb pages do not participate in page cache accounting. */

	/*

	 * At this point page must be either written or cleaned by

	 * truncate.  Dirty page here signals a bug and loss of

	 * unwritten data.

	 *

	 * This fixes dirty accounting after removing the page entirely

	 * but leaves PageDirty set: it has no effect for truncated

	 * page and anyway will be cleared before returning page into

	 * buddy allocator.

/*

 * Delete a page from the page cache and free it. Caller has to make

 * sure the page is locked and that nobody else uses it - or that usage

 * is safe.  The caller must hold the i_pages lock.

/**

 * delete_from_page_cache - delete page from page cache

 * @page: the page which the kernel is trying to remove from page cache

 *

 * This must be called only on pages that have been verified to be in the page

 * cache and locked.  It will never put the page into the free list, the caller

 * has a reference on the page.

/*

 * page_cache_delete_batch - delete several pages from page cache

 * @mapping: the mapping to which pages belong

 * @pvec: pagevec with pages to delete

 *

 * The function walks over mapping->i_pages and removes pages passed in @pvec

 * from the mapping. The function expects @pvec to be sorted by page index

 * and is optimised for it to be dense.

 * It tolerates holes in @pvec (mapping entries at those indices are not

 * modified). The function expects only THP head pages to be present in the

 * @pvec.

 *

 * The function expects the i_pages lock to be held.

 A swap/dax/shadow entry got inserted? Skip it. */

		/*

		 * A page got inserted in our range? Skip it. We have our

		 * pages locked so they are protected from being removed.

		 * If we see a page whose index is higher than ours, it

		 * means our page has been removed, which shouldn't be

		 * possible because we're holding the PageLock.

 Leave page->index set: truncation lookup relies on it */

		/*

		 * Move to the next page in the vector if this is a regular

		 * page or the index is of the last sub-page of this compound

		 * page.

 Check for outstanding write errors */

 Check for outstanding write errors */

/**

 * filemap_fdatawrite_wbc - start writeback on mapping dirty pages in range

 * @mapping:	address space structure to write

 * @wbc:	the writeback_control controlling the writeout

 *

 * Call writepages on the mapping using the provided wbc to control the

 * writeout.

 *

 * Return: %0 on success, negative error code otherwise.

/**

 * __filemap_fdatawrite_range - start writeback on mapping dirty pages in range

 * @mapping:	address space structure to write

 * @start:	offset in bytes where the range starts

 * @end:	offset in bytes where the range ends (inclusive)

 * @sync_mode:	enable synchronous operation

 *

 * Start writeback against all of a mapping's dirty pages that lie

 * within the byte offsets <start, end> inclusive.

 *

 * If sync_mode is WB_SYNC_ALL then this is a "data integrity" operation, as

 * opposed to a regular memory cleansing writeback.  The difference between

 * these two operations is that if a dirty page/buffer is encountered, it must

 * be waited upon, and not just skipped over.

 *

 * Return: %0 on success, negative error code otherwise.

/**

 * filemap_flush - mostly a non-blocking flush

 * @mapping:	target address_space

 *

 * This is a mostly non-blocking flush.  Not suitable for data-integrity

 * purposes - I/O may not be started against all dirty pages.

 *

 * Return: %0 on success, negative error code otherwise.

/**

 * filemap_range_has_page - check if a page exists in range.

 * @mapping:           address space within which to check

 * @start_byte:        offset in bytes where the range starts

 * @end_byte:          offset in bytes where the range ends (inclusive)

 *

 * Find at least one page in the range supplied, usually used to check if

 * direct writing in this range will trigger a writeback.

 *

 * Return: %true if at least one page exists in the specified range,

 * %false otherwise.

 Shadow entries don't count */

		/*

		 * We don't need to try to pin this page; we're about to

		 * release the RCU lock anyway.  It is enough to know that

		 * there was a page here recently.

/**

 * filemap_fdatawait_range - wait for writeback to complete

 * @mapping:		address space structure to wait for

 * @start_byte:		offset in bytes where the range starts

 * @end_byte:		offset in bytes where the range ends (inclusive)

 *

 * Walk the list of under-writeback pages of the given address space

 * in the given range and wait for all of them.  Check error status of

 * the address space and return it.

 *

 * Since the error status of the address space is cleared by this function,

 * callers are responsible for checking the return value and handling and/or

 * reporting the error.

 *

 * Return: error status of the address space.

/**

 * filemap_fdatawait_range_keep_errors - wait for writeback to complete

 * @mapping:		address space structure to wait for

 * @start_byte:		offset in bytes where the range starts

 * @end_byte:		offset in bytes where the range ends (inclusive)

 *

 * Walk the list of under-writeback pages of the given address space in the

 * given range and wait for all of them.  Unlike filemap_fdatawait_range(),

 * this function does not clear error status of the address space.

 *

 * Use this function if callers don't handle errors themselves.  Expected

 * call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),

 * fsfreeze(8)

/**

 * file_fdatawait_range - wait for writeback to complete

 * @file:		file pointing to address space structure to wait for

 * @start_byte:		offset in bytes where the range starts

 * @end_byte:		offset in bytes where the range ends (inclusive)

 *

 * Walk the list of under-writeback pages of the address space that file

 * refers to, in the given range and wait for all of them.  Check error

 * status of the address space vs. the file->f_wb_err cursor and return it.

 *

 * Since the error status of the file is advanced by this function,

 * callers are responsible for checking the return value and handling and/or

 * reporting the error.

 *

 * Return: error status of the address space vs. the file->f_wb_err cursor.

/**

 * filemap_fdatawait_keep_errors - wait for writeback without clearing errors

 * @mapping: address space structure to wait for

 *

 * Walk the list of under-writeback pages of the given address space

 * and wait for all of them.  Unlike filemap_fdatawait(), this function

 * does not clear error status of the address space.

 *

 * Use this function if callers don't handle errors themselves.  Expected

 * call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),

 * fsfreeze(8)

 *

 * Return: error status of the address space.

 Returns true if writeback might be needed or already in progress. */

/**

 * filemap_range_needs_writeback - check if range potentially needs writeback

 * @mapping:           address space within which to check

 * @start_byte:        offset in bytes where the range starts

 * @end_byte:          offset in bytes where the range ends (inclusive)

 *

 * Find at least one page in the range supplied, usually used to check if

 * direct writing in this range will trigger a writeback. Used by O_DIRECT

 * read/write with IOCB_NOWAIT, to see if the caller needs to do

 * filemap_write_and_wait_range() before proceeding.

 *

 * Return: %true if the caller should do filemap_write_and_wait_range() before

 * doing O_DIRECT to a page in this range, %false otherwise.

/**

 * filemap_write_and_wait_range - write out & wait on a file range

 * @mapping:	the address_space for the pages

 * @lstart:	offset in bytes where the range starts

 * @lend:	offset in bytes where the range ends (inclusive)

 *

 * Write out and wait upon file offsets lstart->lend, inclusive.

 *

 * Note that @lend is inclusive (describes the last byte to be written) so

 * that this function can be used to write to the very end-of-file (end = -1).

 *

 * Return: error status of the address space.

		/*

		 * Even if the above returned error, the pages may be

		 * written partially (e.g. -ENOSPC), so we wait for it.

		 * But the -EIO is special case, it may indicate the worst

		 * thing (e.g. bug) happened, so we avoid waiting for it.

 Clear any previously stored errors */

/**

 * file_check_and_advance_wb_err - report wb error (if any) that was previously

 * 				   and advance wb_err to current one

 * @file: struct file on which the error is being reported

 *

 * When userland calls fsync (or something like nfsd does the equivalent), we

 * want to report any writeback errors that occurred since the last fsync (or

 * since the file was opened if there haven't been any).

 *

 * Grab the wb_err from the mapping. If it matches what we have in the file,

 * then just quickly return 0. The file is all caught up.

 *

 * If it doesn't match, then take the mapping value, set the "seen" flag in

 * it and try to swap it into place. If it works, or another task beat us

 * to it with the new value, then update the f_wb_err and return the error

 * portion. The error at this point must be reported via proper channels

 * (a'la fsync, or NFS COMMIT operation, etc.).

 *

 * While we handle mapping->wb_err with atomic operations, the f_wb_err

 * value is protected by the f_lock since we must ensure that it reflects

 * the latest value swapped in for this file descriptor.

 *

 * Return: %0 on success, negative error code otherwise.

 Locklessly handle the common case where nothing has changed */

 Something changed, must use slow path */

	/*

	 * We're mostly using this function as a drop in replacement for

	 * filemap_check_errors. Clear AS_EIO/AS_ENOSPC to emulate the effect

	 * that the legacy code would have had on these flags.

/**

 * file_write_and_wait_range - write out & wait on a file range

 * @file:	file pointing to address_space with pages

 * @lstart:	offset in bytes where the range starts

 * @lend:	offset in bytes where the range ends (inclusive)

 *

 * Write out and wait upon file offsets lstart->lend, inclusive.

 *

 * Note that @lend is inclusive (describes the last byte to be written) so

 * that this function can be used to write to the very end-of-file (end = -1).

 *

 * After writing out and waiting on the data, we check and advance the

 * f_wb_err cursor to the latest value, and return any errors detected there.

 *

 * Return: %0 on success, negative error code otherwise.

 See comment of filemap_write_and_wait() */

/**

 * replace_page_cache_page - replace a pagecache page with a new one

 * @old:	page to be replaced

 * @new:	page to replace with

 *

 * This function replaces a page in the pagecache with a new one.  On

 * success it acquires the pagecache reference for the new page and

 * drops it for the old page.  Both the old and new pages must be

 * locked.  This function does not add the new page to the LRU, the

 * caller must do that.

 *

 * The remove + add is atomic.  This function cannot fail.

 hugetlb pages do not participate in page cache accounting. */

 entry may have been split before we acquired lock */

 hugetlb pages do not participate in page cache accounting */

 Leave page->index set: truncation relies upon it */

/**

 * add_to_page_cache_locked - add a locked page to the pagecache

 * @page:	page to add

 * @mapping:	the page's address_space

 * @offset:	page index

 * @gfp_mask:	page allocation mode

 *

 * This function is used to add a page to the pagecache. It must be locked.

 * This function does not add the page to the LRU.  The caller must do that.

 *

 * Return: %0 on success, negative error code otherwise.

		/*

		 * The folio might have been evicted from cache only

		 * recently, in which case it should be activated like

		 * any other repeatedly accessed folio.

		 * The exception is folios getting rewritten; evicting other

		 * data from the working set, only to cache data that will

		 * get overwritten with something else, is a waste of memory.

/*

 * filemap_invalidate_lock_two - lock invalidate_lock for two mappings

 *

 * Lock exclusively invalidate_lock of any passed mapping that is not NULL.

 *

 * @mapping1: the first mapping to lock

 * @mapping2: the second mapping to lock

/*

 * filemap_invalidate_unlock_two - unlock invalidate_lock for two mappings

 *

 * Unlock exclusive invalidate_lock of any passed mapping that is not NULL.

 *

 * @mapping1: the first mapping to unlock

 * @mapping2: the second mapping to unlock

/*

 * In order to wait for pages to become available there must be

 * waitqueues associated with pages. By using a hash table of

 * waitqueues where the bucket discipline is to maintain all

 * waiters on the same queue and wake all when any of the pages

 * become available, and for the woken contexts to check to be

 * sure the appropriate page became available, this saves space

 * at a cost of "thundering herd" phenomena during rare hash

 * collisions.

/*

 * The page wait code treats the "wait->flags" somewhat unusually, because

 * we have multiple different kinds of waits, not just the usual "exclusive"

 * one.

 *

 * We have:

 *

 *  (a) no special bits set:

 *

 *	We're just waiting for the bit to be released, and when a waker

 *	calls the wakeup function, we set WQ_FLAG_WOKEN and wake it up,

 *	and remove it from the wait queue.

 *

 *	Simple and straightforward.

 *

 *  (b) WQ_FLAG_EXCLUSIVE:

 *

 *	The waiter is waiting to get the lock, and only one waiter should

 *	be woken up to avoid any thundering herd behavior. We'll set the

 *	WQ_FLAG_WOKEN bit, wake it up, and remove it from the wait queue.

 *

 *	This is the traditional exclusive wait.

 *

 *  (c) WQ_FLAG_EXCLUSIVE | WQ_FLAG_CUSTOM:

 *

 *	The waiter is waiting to get the bit, and additionally wants the

 *	lock to be transferred to it for fair lock behavior. If the lock

 *	cannot be taken, we stop walking the wait queue without waking

 *	the waiter.

 *

 *	This is the "fair lock handoff" case, and in addition to setting

 *	WQ_FLAG_WOKEN, we set WQ_FLAG_DONE to let the waiter easily see

 *	that it now has the lock.

	/*

	 * If it's a lock handoff wait, we get the bit for it, and

	 * stop walking (and do not wake it up) if we can't.

	/*

	 * We are holding the wait-queue lock, but the waiter that

	 * is waiting for this will be checking the flags without

	 * any locking.

	 *

	 * So update the flags atomically, and wake up the waiter

	 * afterwards to avoid any races. This store-release pairs

	 * with the load-acquire in folio_wait_bit_common().

	/*

	 * Ok, we have successfully done what we're waiting for,

	 * and we can unconditionally remove the wait entry.

	 *

	 * Note that this pairs with the "finish_wait()" in the

	 * waiter, and has to be the absolute last thing we do.

	 * After this list_del_init(&wait->entry) the wait entry

	 * might be de-allocated and the process might even have

	 * exited.

		/*

		 * Take a breather from holding the lock,

		 * allow pages that finish wake up asynchronously

		 * to acquire the lock and remove themselves

		 * from wait queue

	/*

	 * It is possible for other pages to have collided on the waitqueue

	 * hash, so in that case check for a page match. That prevents a long-

	 * term waiter

	 *

	 * It is still possible to miss a case here, when we woke page waiters

	 * and removed them from the waitqueue, but there are still other

	 * page waiters.

		/*

		 * It's possible to miss clearing Waiters here, when we woke

		 * our page waiters, but the hashed waitqueue has waiters for

		 * other pages on it.

		 *

		 * That's okay, it's a rare case. The next waker will clear it.

/*

 * A choice of three behaviors for folio_wait_bit_common():

	EXCLUSIVE,	/* Hold ref to page and take the bit when woken, like

			 * __folio_lock() waiting on then setting PG_locked.

	SHARED,		/* Hold ref to page and check the bit when woken, like

			 * wait_on_page_writeback() waiting on PG_writeback.

	DROP,		/* Drop ref to page before wait, no check when woken,

			 * like put_and_wait_on_page_locked() on PG_locked.

/*

 * Attempt to check (or get) the folio flag, and mark us done

 * if successful.

 How many times do we accept lock stealing from under a waiter? */

	/*

	 * Do one last check whether we can get the

	 * page bit synchronously.

	 *

	 * Do the folio_set_waiters() marking before that

	 * to let any waker we _just_ missed know they

	 * need to wake us up (otherwise they'll never

	 * even go to the slow case that looks at the

	 * page queue), and add ourselves to the wait

	 * queue if we need to sleep.

	 *

	 * This part needs to be done under the queue

	 * lock to avoid races.

	/*

	 * From now on, all the logic will be based on

	 * the WQ_FLAG_WOKEN and WQ_FLAG_DONE flag, to

	 * see whether the page bit testing has already

	 * been done by the wake function.

	 *

	 * We can drop our reference to the folio.

	/*

	 * Note that until the "finish_wait()", or until

	 * we see the WQ_FLAG_WOKEN flag, we need to

	 * be very careful with the 'wait->flags', because

	 * we may race with a waker that sets them.

 Loop until we've been woken or interrupted */

 If we were non-exclusive, we're done */

 If the waker got the lock for us, we're done */

		/*

		 * Otherwise, if we're getting the lock, we need to

		 * try to get it ourselves.

		 *

		 * And if that fails, we'll have to retry this all.

	/*

	 * If a signal happened, this 'finish_wait()' may remove the last

	 * waiter from the wait-queues, but the folio waiters bit will remain

	 * set. That's ok. The next wakeup will take care of it, and trying

	 * to do it here would be difficult and prone to races.

	/*

	 * NOTE! The wait->flags weren't stable until we've done the

	 * 'finish_wait()', and we could have exited the loop above due

	 * to a signal, and had a wakeup event happen after the signal

	 * test but before the 'finish_wait()'.

	 *

	 * So only after the finish_wait() can we reliably determine

	 * if we got woken up or not, so we can now figure out the final

	 * return value based on that state without races.

	 *

	 * Also note that WQ_FLAG_WOKEN is sufficient for a non-exclusive

	 * waiter, but an exclusive one requires WQ_FLAG_DONE.

/**

 * put_and_wait_on_page_locked - Drop a reference and wait for it to be unlocked

 * @page: The page to wait for.

 * @state: The sleep state (TASK_KILLABLE, TASK_UNINTERRUPTIBLE, etc).

 *

 * The caller should hold a reference on @page.  They expect the page to

 * become unlocked relatively soon, but do not wish to hold up migration

 * (for example) by holding the reference while waiting for the page to

 * come unlocked.  After this function returns, the caller should not

 * dereference @page.

 *

 * Return: 0 if the page was unlocked or -EINTR if interrupted by a signal.

/**

 * folio_add_wait_queue - Add an arbitrary waiter to a folio's wait queue

 * @folio: Folio defining the wait queue of interest

 * @waiter: Waiter to add to the queue

 *

 * Add an arbitrary @waiter to the wait queue for the nominated @folio.

/*

 * PG_waiters is the high bit in the same byte as PG_lock.

 *

 * On x86 (and on many other architectures), we can clear PG_lock and

 * test the sign bit at the same time. But if the architecture does

 * not support that special operation, we just do this all by hand

 * instead.

 *

 * The read of PG_waiters has to be after (or concurrently with) PG_locked

 * being cleared, but a memory barrier should be unnecessary since it is

 * in the same byte as PG_locked.

 smp_mb__after_atomic(); */

/**

 * folio_unlock - Unlock a locked folio.

 * @folio: The folio.

 *

 * Unlocks the folio and wakes up any thread sleeping on the page lock.

 *

 * Context: May be called from interrupt or process context.  May not be

 * called from NMI context.

 Bit 7 allows x86 to check the byte's sign bit */

/**

 * folio_end_private_2 - Clear PG_private_2 and wake any waiters.

 * @folio: The folio.

 *

 * Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for

 * it.  The folio reference held for PG_private_2 being set is released.

 *

 * This is, for example, used when a netfs folio is being written to a local

 * disk cache, thereby allowing writes to the cache for the same folio to be

 * serialised.

/**

 * folio_wait_private_2 - Wait for PG_private_2 to be cleared on a folio.

 * @folio: The folio to wait on.

 *

 * Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio.

/**

 * folio_wait_private_2_killable - Wait for PG_private_2 to be cleared on a folio.

 * @folio: The folio to wait on.

 *

 * Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio or until a

 * fatal signal is received by the calling task.

 *

 * Return:

 * - 0 if successful.

 * - -EINTR if a fatal signal was encountered.

/**

 * folio_end_writeback - End writeback against a folio.

 * @folio: The folio.

	/*

	 * folio_test_clear_reclaim() could be used here but it is an

	 * atomic operation and overkill in this particular case. Failing

	 * to shuffle a folio marked for immediate reclaim is too mild

	 * a gain to justify taking an atomic operation penalty at the

	 * end of every folio writeback.

	/*

	 * Writeback does not hold a folio reference of its own, relying

	 * on truncation to wait for the clearing of PG_writeback.

	 * But here we must make sure that the folio is not freed and

	 * reused before the folio_wake().

/*

 * After completing I/O on a page, call this routine to update the page

 * flags appropriately

/**

 * __folio_lock - Get a lock on the folio, assuming we need to sleep to get it.

 * @folio: The folio to lock

	/*

	 * If we were successful now, we know we're still on the

	 * waitqueue as we're still under the lock. This means it's

	 * safe to remove and return success, we know the callback

	 * isn't going to trigger.

/*

 * Return values:

 * true - folio is locked; mmap_lock is still held.

 * false - folio is not locked.

 *     mmap_lock has been released (mmap_read_unlock(), unless flags had both

 *     FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_RETRY_NOWAIT set, in

 *     which case mmap_lock is still held.

 *

 * If neither ALLOW_RETRY nor KILLABLE are set, will always return true

 * with the folio locked and the mmap_lock unperturbed.

		/*

		 * CAUTION! In this case, mmap_lock is not released

		 * even though return 0.

/**

 * page_cache_next_miss() - Find the next gap in the page cache.

 * @mapping: Mapping.

 * @index: Index.

 * @max_scan: Maximum range to search.

 *

 * Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the

 * gap with the lowest index.

 *

 * This function may be called under the rcu_read_lock.  However, this will

 * not atomically search a snapshot of the cache at a single point in time.

 * For example, if a gap is created at index 5, then subsequently a gap is

 * created at index 10, page_cache_next_miss covering both indices may

 * return 10 if called under the rcu_read_lock.

 *

 * Return: The index of the gap if found, otherwise an index outside the

 * range specified (in which case 'return - index >= max_scan' will be true).

 * In the rare case of index wrap-around, 0 will be returned.

/**

 * page_cache_prev_miss() - Find the previous gap in the page cache.

 * @mapping: Mapping.

 * @index: Index.

 * @max_scan: Maximum range to search.

 *

 * Search the range [max(index - max_scan + 1, 0), index] for the

 * gap with the highest index.

 *

 * This function may be called under the rcu_read_lock.  However, this will

 * not atomically search a snapshot of the cache at a single point in time.

 * For example, if a gap is created at index 10, then subsequently a gap is

 * created at index 5, page_cache_prev_miss() covering both indices may

 * return 5 if called under the rcu_read_lock.

 *

 * Return: The index of the gap if found, otherwise an index outside the

 * range specified (in which case 'index - return >= max_scan' will be true).

 * In the rare case of wrap-around, ULONG_MAX will be returned.

/*

 * Lockless page cache protocol:

 * On the lookup side:

 * 1. Load the folio from i_pages

 * 2. Increment the refcount if it's not zero

 * 3. If the folio is not found by xas_reload(), put the refcount and retry

 *

 * On the removal side:

 * A. Freeze the page (by zeroing the refcount if nobody else has a reference)

 * B. Remove the page from i_pages

 * C. Return the page to the page allocator

 *

 * This means that any page may have its reference count temporarily

 * increased by a speculative page cache (or fast GUP) lookup as it can

 * be allocated by another user before the RCU grace period expires.

 * Because the refcount temporarily acquired here may end up being the

 * last refcount on the page, any page allocation must be freeable by

 * folio_put().

/*

 * mapping_get_entry - Get a page cache entry.

 * @mapping: the address_space to search

 * @index: The page cache index.

 *

 * Looks up the page cache entry at @mapping & @index.  If it is a folio,

 * it is returned with an increased refcount.  If it is a shadow entry

 * of a previously evicted folio, or a swap entry from shmem/tmpfs,

 * it is returned without further action.

 *

 * Return: The folio, swap or shadow entry, %NULL if nothing is found.

	/*

	 * A shadow entry of a recently evicted page, or a swap entry from

	 * shmem/tmpfs.  Return it without attempting to raise page count.

/**

 * __filemap_get_folio - Find and get a reference to a folio.

 * @mapping: The address_space to search.

 * @index: The page index.

 * @fgp_flags: %FGP flags modify how the folio is returned.

 * @gfp: Memory allocation flags to use if %FGP_CREAT is specified.

 *

 * Looks up the page cache entry at @mapping & @index.

 *

 * @fgp_flags can be zero or more of these flags:

 *

 * * %FGP_ACCESSED - The folio will be marked accessed.

 * * %FGP_LOCK - The folio is returned locked.

 * * %FGP_ENTRY - If there is a shadow / swap / DAX entry, return it

 *   instead of allocating a new folio to replace it.

 * * %FGP_CREAT - If no page is present then a new page is allocated using

 *   @gfp and added to the page cache and the VM's LRU list.

 *   The page is returned locked and with an increased refcount.

 * * %FGP_FOR_MMAP - The caller wants to do its own locking dance if the

 *   page is already in cache.  If the page was allocated, unlock it before

 *   returning so the caller can do the same dance.

 * * %FGP_WRITE - The page will be written to by the caller.

 * * %FGP_NOFS - __GFP_FS will get cleared in gfp.

 * * %FGP_NOWAIT - Don't get blocked by page lock.

 * * %FGP_STABLE - Wait for the folio to be stable (finished writeback)

 *

 * If %FGP_LOCK or %FGP_CREAT are specified then the function may sleep even

 * if the %GFP flags specified for %FGP_CREAT are atomic.

 *

 * If there is a page cache page, it is returned with an increased refcount.

 *

 * Return: The found folio or %NULL otherwise.

 Has the page been truncated? */

 Clear idle flag for buffer write */

 Init accessed so avoid atomic mark_page_accessed later */

		/*

		 * filemap_add_folio locks the page, and for mmap

		 * we expect an unlocked page.

	/*

	 * A shadow entry of a recently evicted page, a swap

	 * entry from shmem/tmpfs or a DAX entry.  Return it

	 * without attempting to raise page count.

 Has the page moved or been split? */

/**

 * find_get_entries - gang pagecache lookup

 * @mapping:	The address_space to search

 * @start:	The starting page cache index

 * @end:	The final page index (inclusive).

 * @pvec:	Where the resulting entries are placed.

 * @indices:	The cache indices corresponding to the entries in @entries

 *

 * find_get_entries() will search for and return a batch of entries in

 * the mapping.  The entries are placed in @pvec.  find_get_entries()

 * takes a reference on any actual pages it returns.

 *

 * The search returns a group of mapping-contiguous page cache entries

 * with ascending indexes.  There may be holes in the indices due to

 * not-present pages.

 *

 * Any shadow entries of evicted pages, or swap entries from

 * shmem/tmpfs, are included in the returned array.

 *

 * If it finds a Transparent Huge Page, head or tail, find_get_entries()

 * stops at that page: the caller is likely to have a better way to handle

 * the compound page as a whole, and then skip its extent, than repeatedly

 * calling find_get_entries() to return all its tails.

 *

 * Return: the number of pages and shadow entries which were found.

		/*

		 * Terminate early on finding a THP, to allow the caller to

		 * handle it all at once; but continue if this is hugetlbfs.

/**

 * find_lock_entries - Find a batch of pagecache entries.

 * @mapping:	The address_space to search.

 * @start:	The starting page cache index.

 * @end:	The final page index (inclusive).

 * @pvec:	Where the resulting entries are placed.

 * @indices:	The cache indices of the entries in @pvec.

 *

 * find_lock_entries() will return a batch of entries from @mapping.

 * Swap, shadow and DAX entries are included.  Pages are returned

 * locked and with an incremented refcount.  Pages which are locked by

 * somebody else or under writeback are skipped.  Only the head page of

 * a THP is returned.  Pages which are partially outside the range are

 * not returned.

 *

 * The entries have ascending indexes.  The indices may not be consecutive

 * due to not-present entries, THP pages, pages which could not be locked

 * or pages under writeback.

 *

 * Return: The number of entries which were found.

 Final THP may cross MAX_LFS_FILESIZE on 32-bit */

/**

 * find_get_pages_range - gang pagecache lookup

 * @mapping:	The address_space to search

 * @start:	The starting page index

 * @end:	The final page index (inclusive)

 * @nr_pages:	The maximum number of pages

 * @pages:	Where the resulting pages are placed

 *

 * find_get_pages_range() will search for and return a group of up to @nr_pages

 * pages in the mapping starting at index @start and up to index @end

 * (inclusive).  The pages are placed at @pages.  find_get_pages_range() takes

 * a reference against the returned pages.

 *

 * The search returns a group of mapping-contiguous pages with ascending

 * indexes.  There may be holes in the indices due to not-present pages.

 * We also update @start to index the next page for the traversal.

 *

 * Return: the number of pages which were found. If this number is

 * smaller than @nr_pages, the end of specified range has been

 * reached.

 Skip over shadow, swap and DAX entries */

	/*

	 * We come here when there is no page beyond @end. We take care to not

	 * overflow the index @start as it confuses some of the callers. This

	 * breaks the iteration when there is a page at index -1 but that is

	 * already broken anyway.

/**

 * find_get_pages_contig - gang contiguous pagecache lookup

 * @mapping:	The address_space to search

 * @index:	The starting page index

 * @nr_pages:	The maximum number of pages

 * @pages:	Where the resulting pages are placed

 *

 * find_get_pages_contig() works exactly like find_get_pages(), except

 * that the returned number of pages are guaranteed to be contiguous.

 *

 * Return: the number of pages which were found.

		/*

		 * If the entry has been swapped out, we can stop looking.

		 * No current caller is looking for DAX entries.

 Has the page moved or been split? */

/**

 * find_get_pages_range_tag - Find and return head pages matching @tag.

 * @mapping:	the address_space to search

 * @index:	the starting page index

 * @end:	The final page index (inclusive)

 * @tag:	the tag index

 * @nr_pages:	the maximum number of pages

 * @pages:	where the resulting pages are placed

 *

 * Like find_get_pages(), except we only return head pages which are tagged

 * with @tag.  @index is updated to the index immediately after the last

 * page we return, ready for the next iteration.

 *

 * Return: the number of pages which were found.

		/*

		 * Shadow entries should never be tagged, but this iteration

		 * is lockless so there is a window for page reclaim to evict

		 * a page we saw tagged.  Skip over it.

	/*

	 * We come here when we got to @end. We take care to not overflow the

	 * index @index as it confuses some of the callers. This breaks the

	 * iteration when there is a page at index -1 but that is already

	 * broken anyway.

/*

 * CD/DVDs are error prone. When a medium error occurs, the driver may fail

 * a _large_ part of the i/o request. Imagine the worst scenario:

 *

 *      ---R__________________________________________B__________

 *         ^ reading here                             ^ bad block(assume 4k)

 *

 * read(R) => miss => readahead(R...B) => media error => frustrating retries

 * => failing the whole request => read(R) => read(R+1) =>

 * readahead(R+1...B+1) => bang => read(R+2) => read(R+3) =>

 * readahead(R+3...B+2) => bang => read(R+3) => read(R+4) =>

 * readahead(R+4...B+3) => bang => read(R+4) => read(R+5) => ......

 *

 * It is going insane. Fix it by quickly scaling down the readahead size.

/*

 * filemap_get_read_batch - Get a batch of pages for read

 *

 * Get a batch of pages which represent a contiguous range of bytes

 * in the file.  No tail pages will be returned.  If @index is in the

 * middle of a THP, the entire THP will be returned.  The last page in

 * the batch may have Readahead set or be not Uptodate so that the

 * caller can take the appropriate action.

 Has the page moved or been split? */

	/*

	 * A previous I/O error may have been due to temporary failures,

	 * eg. multipath errors.  PG_error will be set again if readpage

	 * fails.

 Start the actual read. The read will unlock the page. */

 pipes can't handle partially uptodate pages */

	/*

	 * Protect against truncate / hole punch. Grabbing invalidate_lock here

	 * assures we cannot instantiate and bring uptodate new pagecache pages

	 * after evicting page cache during truncate and before actually

	 * freeing blocks.  Note that we could release invalidate_lock after

	 * inserting the page into page cache as the locked page would then be

	 * enough to synchronize with hole punching. But there are code paths

	 * such as filemap_update_page() filling in partially uptodate pages or

	 * ->readpages() that need to hold invalidate_lock while mapping blocks

	 * for IO so let's hold the lock here as well to keep locking rules

	 * simple.

/**

 * filemap_read - Read data from the page cache.

 * @iocb: The iocb to read.

 * @iter: Destination for the data.

 * @already_read: Number of bytes already read by the caller.

 *

 * Copies data from the page cache.  If the data is not currently present,

 * uses the readahead and readpage address_space operations to fetch it.

 *

 * Return: Total number of bytes copied, including those already read by

 * the caller.  If an error happens before any bytes are copied, returns

 * a negative error number.

		/*

		 * If we've already successfully copied some data, then we

		 * can no longer safely return -EIOCBQUEUED. Hence mark

		 * an async read NOWAIT at that point.

		/*

		 * i_size must be checked after we know the pages are Uptodate.

		 *

		 * Checking i_size after the check allows us to calculate

		 * the correct value for "nr", which means the zero-filled

		 * part of the page is not copied back to userspace (unless

		 * another truncate extends the file - this is desired though).

		/*

		 * Once we start copying data, we don't want to be touching any

		 * cachelines that might be contended:

		/*

		 * When a sequential read accesses a page several times, only

		 * mark it as accessed the first time.

			/*

			 * If users can be writing to this page using arbitrary

			 * virtual addresses, take care about potential aliasing

			 * before reading the page on the kernel side.

/**

 * generic_file_read_iter - generic filesystem read routine

 * @iocb:	kernel I/O control block

 * @iter:	destination for the data read

 *

 * This is the "read_iter()" routine for all filesystems

 * that can use the page cache directly.

 *

 * The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall

 * be returned when no data can be read without waiting for I/O requests

 * to complete; it doesn't prevent readahead.

 *

 * The IOCB_NOIO flag in iocb->ki_flags indicates that no new I/O

 * requests shall be made for the read or for readahead.  When no data

 * can be read, -EAGAIN shall be returned.  When readahead would be

 * triggered, a partial, possibly empty read shall be returned.

 *

 * Return:

 * * number of bytes copied, even for partial reads

 * * negative error code (or 0 if IOCB_NOIO) if nothing was read

 skip atime */

		/*

		 * Btrfs can have a short DIO read if we encounter

		 * compressed extents, so if there was an error, or if

		 * we've already read everything we wanted to, or if

		 * there was a short read because we hit EOF, go ahead

		 * and return.  Otherwise fallthrough to buffered io for

		 * the rest of the read.  Buffered reads will not work for

		 * DAX files, so don't bother trying.

/**

 * mapping_seek_hole_data - Seek for SEEK_DATA / SEEK_HOLE in the page cache.

 * @mapping: Address space to search.

 * @start: First byte to consider.

 * @end: Limit of search (exclusive).

 * @whence: Either SEEK_HOLE or SEEK_DATA.

 *

 * If the page cache knows which blocks contain holes and which blocks

 * contain data, your filesystem can use this function to implement

 * SEEK_HOLE and SEEK_DATA.  This is useful for filesystems which are

 * entirely memory-based such as tmpfs, and filesystems which support

 * unwritten extents.

 *

 * Return: The requested offset on success, or -ENXIO if @whence specifies

 * SEEK_DATA and there is no data after @start.  There is an implicit hole

 * after @end - 1, so SEEK_HOLE returns @end if all the bytes between @start

 * and @end contain data.

/*

 * lock_page_maybe_drop_mmap - lock the page, possibly dropping the mmap_lock

 * @vmf - the vm_fault for this fault.

 * @page - the page to lock.

 * @fpin - the pointer to the file we may pin (or is already pinned).

 *

 * This works similar to lock_page_or_retry in that it can drop the mmap_lock.

 * It differs in that it actually returns the page locked if it returns 1 and 0

 * if it couldn't lock the page.  If we did have to drop the mmap_lock then fpin

 * will point to the pinned file and needs to be fput()'ed at a later point.

	/*

	 * NOTE! This will make us return with VM_FAULT_RETRY, but with

	 * the mmap_lock still held. That's how FAULT_FLAG_RETRY_NOWAIT

	 * is supposed to work. We have way too many special cases..

			/*

			 * We didn't have the right flags to drop the mmap_lock,

			 * but all fault_handlers only check for fatal signals

			 * if we return VM_FAULT_RETRY, so we need to drop the

			 * mmap_lock here and return 0 if we don't have a fpin.

/*

 * Synchronous readahead happens when we don't even find a page in the page

 * cache at all.  We don't want to perform IO under the mmap sem, so if we have

 * to drop the mmap sem we return the file that was pinned in order for us to do

 * that.  If we didn't pin a file then we return NULL.  The file that is

 * returned needs to be fput()'ed when we're done with it.

 If we don't want any read-ahead, don't bother */

 Avoid banging the cache line if not needed */

	/*

	 * Do we miss much more than hit in this file? If so,

	 * stop bothering with read-ahead. It will only hurt.

	/*

	 * mmap read-around

/*

 * Asynchronous readahead happens when we find the page and PG_readahead,

 * so we want to possibly extend the readahead further.  We return the file that

 * was pinned if we have to drop the mmap_lock in order to do IO.

 If we don't want any read-ahead, don't bother */

/**

 * filemap_fault - read in file data for page fault handling

 * @vmf:	struct vm_fault containing details of the fault

 *

 * filemap_fault() is invoked via the vma operations vector for a

 * mapped memory region to read in file data during a page fault.

 *

 * The goto's are kind of ugly, but this streamlines the normal case of having

 * it in the page cache, and handles the special cases reasonably without

 * having a lot of duplicated code.

 *

 * vma->vm_mm->mmap_lock must be held on entry.

 *

 * If our return value has VM_FAULT_RETRY set, it's because the mmap_lock

 * may be dropped before doing I/O or by lock_page_maybe_drop_mmap().

 *

 * If our return value does not have VM_FAULT_RETRY set, the mmap_lock

 * has not been released.

 *

 * We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set.

 *

 * Return: bitwise-OR of %VM_FAULT_ codes.

	/*

	 * Do we have something in the page cache already?

		/*

		 * We found the page, so try async readahead before waiting for

		 * the lock.

 No page in the page cache at all */

		/*

		 * See comment in filemap_create_page() why we need

		 * invalidate_lock

 Did it get truncated? */

	/*

	 * We have a locked page in the page cache, now we need to check

	 * that it's up-to-date. If not, it is going to be due to an error.

		/*

		 * The page was in cache and uptodate and now it is not.

		 * Strange but possible since we didn't hold the page lock all

		 * the time. Let's drop everything get the invalidate lock and

		 * try again.

	/*

	 * We've made it this far and we had to drop our mmap_lock, now is the

	 * time to return to the upper layer and have it re-find the vma and

	 * redo the fault.

	/*

	 * Found the page and have a reference on it.

	 * We must recheck i_size under page lock.

	/*

	 * Umm, take care of errors if the page isn't up-to-date.

	 * Try to re-read it _once_. We do this synchronously,

	 * because there really aren't any performance issues here

	 * and we need to check for errors.

	/*

	 * We dropped the mmap_lock, we need to return to the fault handler to

	 * re-find the vma and come back and find our hopefully still populated

	 * page.

 Huge page is mapped? No need to proceed. */

 The page is mapped successfully, reference consumed. */

 See comment in handle_pte_fault() */

 Has the page moved or been split? */

 We're about to handle the fault */

 no need to invalidate: a not-present page won't be cached */

	/*

	 * We mark the page dirty already here so that when freeze is in

	 * progress, we are guaranteed that writeback during freezing will

	 * see the dirty page and writeprotect it again.

 This is used for a general mmap of a disk file */

/*

 * This is for filesystems which do not implement ->writepage.

 CONFIG_MMU */

 Presumably ENOMEM for xarray node */

	/*

	 * Page is not up to date and may be locked due to one of the following

	 * case a: Page is being filled and the page lock is held

	 * case b: Read/write error clearing the page uptodate status

	 * case c: Truncation in progress (page locked)

	 * case d: Reclaim in progress

	 *

	 * Case a, the page will be up to date when the page is unlocked.

	 *    There is no need to serialise on the page lock here as the page

	 *    is pinned so the lock gives no additional protection. Even if the

	 *    page is truncated, the data is still valid if PageUptodate as

	 *    it's a race vs truncate race.

	 * Case b, the page will not be up to date

	 * Case c, the page may be truncated but in itself, the data may still

	 *    be valid after IO completes as it's a read vs truncate race. The

	 *    operation must restart if the page is not uptodate on unlock but

	 *    otherwise serialising on page lock to stabilise the mapping gives

	 *    no additional guarantees to the caller as the page lock is

	 *    released before return.

	 * Case d, similar to truncation. If reclaim holds the page lock, it

	 *    will be a race with remove_mapping that determines if the mapping

	 *    is valid on unlock but otherwise the data is valid and there is

	 *    no need to serialise with page lock.

	 *

	 * As the page lock gives no additional guarantee, we optimistically

	 * wait on the page to be unlocked and check if it's up to date and

	 * use the page if it is. Otherwise, the page lock is required to

	 * distinguish between the different cases. The motivation is that we

	 * avoid spurious serialisations and wakeups when multiple processes

	 * wait on the same page for IO to complete.

 Distinguish between all the cases under the safety of the lock */

 Case c or d, restart the operation */

 Someone else locked and filled the page in a very small window */

	/*

	 * A previous I/O error may have been due to temporary

	 * failures.

	 * Clear page error before actual read, PG_error will be

	 * set again if read page fails.

/**

 * read_cache_page - read into page cache, fill it if needed

 * @mapping:	the page's address_space

 * @index:	the page index

 * @filler:	function to perform the read

 * @data:	first arg to filler(data, page) function, often left as NULL

 *

 * Read into the page cache. If a page already exists, and PageUptodate() is

 * not set, try to fill the page and wait for it to become unlocked.

 *

 * If the page does not get brought uptodate, return -EIO.

 *

 * The function expects mapping->invalidate_lock to be already held.

 *

 * Return: up to date page on success, ERR_PTR() on failure.

/**

 * read_cache_page_gfp - read into page cache, using specified page allocation flags.

 * @mapping:	the page's address_space

 * @index:	the page index

 * @gfp:	the page allocator flags to use if allocating

 *

 * This is the same as "read_mapping_page(mapping, index, NULL)", but with

 * any new page allocations done using the specified allocation flags.

 *

 * If the page does not get brought uptodate, return -EIO.

 *

 * The function expects mapping->invalidate_lock to be already held.

 *

 * Return: up to date page on success, ERR_PTR() on failure.

/*

 * Warn about a page cache invalidation failure during a direct I/O write.

 If there are pages to writeback, return */

	/*

	 * After a write we want buffered reads to be sure to go to disk to get

	 * the new data.  We invalidate clean cached page from the region we're

	 * about to write.  We do this *before* the write so that we can return

	 * without clobbering -EIOCBQUEUED from ->direct_IO().

	/*

	 * If a page can not be invalidated, return 0 to fall back

	 * to buffered write.

	/*

	 * Finally, try again to invalidate clean pages which might have been

	 * cached by non-direct readahead, or faulted in by get_user_pages()

	 * if the source of the write was an mmap'ed region of the file

	 * we're writing.  Either one is a pretty crazy thing to do,

	 * so we don't support it 100%.  If this invalidation

	 * fails, tough, the write still worked...

	 *

	 * Most of the time we do not need this since dio_complete() will do

	 * the invalidation for us. However there are some file systems that

	 * do not end up with dio_complete() being called, so let's not break

	 * them by removing it completely.

	 *

	 * Noticeable example is a blkdev_direct_IO().

	 *

	 * Skip invalidation for async writes or if mapping has no pages.

 Offset into pagecache page */

 Bytes to write to page */

 Bytes copied from user */

		/*

		 * Bring in the user page that we will copy from _first_.

		 * Otherwise there's a nasty deadlock on copying from the

		 * same page as we're writing to, without it being marked

		 * up-to-date.

			/*

			 * A short copy made ->write_end() reject the

			 * thing entirely.  Might be memory poisoning

			 * halfway through, might be a race with munmap,

			 * might be severe memory pressure.

/**

 * __generic_file_write_iter - write data to a file

 * @iocb:	IO state structure (file, offset, etc.)

 * @from:	iov_iter with data to write

 *

 * This function does all the work needed for actually writing data to a

 * file. It does all basic checks, removes SUID from the file, updates

 * modification times and calls proper subroutines depending on whether we

 * do direct IO or a standard buffered write.

 *

 * It expects i_rwsem to be grabbed unless we work on a block device or similar

 * object which does not need locking at all.

 *

 * This function does *not* take care of syncing data in case of O_SYNC write.

 * A caller has to handle it. This is mainly due to the fact that we want to

 * avoid syncing under i_rwsem.

 *

 * Return:

 * * number of bytes written, even for truncated writes

 * * negative error code if no data has been written at all

 We can write back this queue in page reclaim */

		/*

		 * If the write stopped short of completing, fall back to

		 * buffered writes.  Some filesystems do this for writes to

		 * holes, for example.  For DAX files, a buffered write will

		 * not succeed (even if it did, DAX does not handle dirty

		 * page-cache pages correctly).

		/*

		 * If generic_perform_write() returned a synchronous error

		 * then we want to return the number of bytes which were

		 * direct-written, or the error code if that was zero.  Note

		 * that this differs from normal direct-io semantics, which

		 * will return -EFOO even if some bytes were written.

		/*

		 * We need to ensure that the page cache pages are written to

		 * disk and invalidated to preserve the expected O_DIRECT

		 * semantics.

			/*

			 * We don't know how much we wrote, so just return

			 * the number of bytes which were direct-written

/**

 * generic_file_write_iter - write data to a file

 * @iocb:	IO state structure

 * @from:	iov_iter with data to write

 *

 * This is a wrapper around __generic_file_write_iter() to be used by most

 * filesystems. It takes care of syncing the file in case of O_SYNC file

 * and acquires i_rwsem as needed.

 * Return:

 * * negative error code if no data has been written at all of

 *   vfs_fsync_range() failed for a synchronous write

 * * number of bytes written, even for truncated writes

/**

 * try_to_release_page() - release old fs-specific metadata on a page

 *

 * @page: the page which the kernel is trying to free

 * @gfp_mask: memory allocation flags (and I/O mode)

 *

 * The address_space is to try to release any data against the page

 * (presumably at page->private).

 *

 * This may also be called if PG_fscache is set on a page, indicating that the

 * page is known to the local caching routines.

 *

 * The @gfp_mask argument specifies whether I/O may be performed to release

 * this page (__GFP_IO), and whether the call may block (__GFP_RECLAIM & __GFP_FS).

 *

 * Return: %1 if the release was successful, otherwise return zero.

 SPDX-License-Identifier: GPL-2.0

/*

 *	mm/mremap.c

 *

 *	(C) Copyright 1996 Linus Torvalds

 *

 *	Address space accounting code	<alan@lxorguk.ukuu.org.uk>

 *	(C) Copyright 2002 Red Hat Inc, All Rights Reserved

	/*

	 * Set soft dirty bit so we can notice

	 * in userspace the ptes were moved.

	/*

	 * When need_rmap_locks is true, we take the i_mmap_rwsem and anon_vma

	 * locks to ensure that rmap will always observe either the old or the

	 * new ptes. This is the easiest way to avoid races with

	 * truncate_pagecache(), page migration, etc...

	 *

	 * When need_rmap_locks is false, we use other ways to avoid

	 * such races:

	 *

	 * - During exec() shift_arg_pages(), we use a specially tagged vma

	 *   which rmap call sites look for using vma_is_temporary_stack().

	 *

	 * - During mremap(), new_vma is often known to be placed after vma

	 *   in rmap traversal order. This ensures rmap will always observe

	 *   either the old pte, or the new pte, or both (the page table locks

	 *   serialize access to individual ptes, but only rmap traversal

	 *   order guarantees that we won't miss both the old and new ptes).

	/*

	 * We don't have to worry about the ordering of src and dst

	 * pte locks because exclusive mmap_lock prevents deadlock.

		/*

		 * If we are remapping a valid PTE, make sure

		 * to flush TLB before we drop the PTL for the

		 * PTE.

		 *

		 * NOTE! Both old and new PTL matter: the old one

		 * for racing with page_mkclean(), the new one to

		 * make sure the physical page stays valid until

		 * the TLB entry for the old mapping has been

		 * flushed.

	/*

	 * The destination pmd shouldn't be established, free_pgtables()

	 * should have released it.

	 *

	 * However, there's a case during execve() where we use mremap

	 * to move the initial stack, and in that case the target area

	 * may overlap the source area (always moving down).

	 *

	 * If everything is PMD-aligned, that works fine, as moving

	 * each pmd down will clear the source pmd. But if we first

	 * have a few 4kB-only pages that get moved down, and then

	 * hit the "now the rest is PMD-aligned, let's do everything

	 * one pmd at a time", we will still have the old (now empty

	 * of any 4kB pages, but still there) PMD in the page table

	 * tree.

	 *

	 * Warn on it once - because we really should try to figure

	 * out how to do this better - but then say "I won't move

	 * this pmd".

	 *

	 * One alternative might be to just unmap the target pmd at

	 * this point, and verify that it really is empty. We'll see.

	/*

	 * We don't have to worry about the ordering of src and dst

	 * ptlocks because exclusive mmap_lock prevents deadlock.

 Clear the pmd */

	/*

	 * The destination pud shouldn't be established, free_pgtables()

	 * should have released it.

	/*

	 * We don't have to worry about the ordering of src and dst

	 * ptlocks because exclusive mmap_lock prevents deadlock.

 Clear the pud */

	/*

	 * The destination pud shouldn't be established, free_pgtables()

	 * should have released it.

	/*

	 * We don't have to worry about the ordering of src and dst

	 * ptlocks because exclusive mmap_lock prevents deadlock.

 Clear the pud */

 Set the new pud */

 mark soft_ditry when we add pud level soft dirty support */

/*

 * Returns an extent of the corresponding size for the pgt_entry specified if

 * valid. Else returns a smaller extent bounded by the end of the source and

 * destination pgt_entry.

 even if next overflowed, extent below will be ok */

/*

 * Attempts to speedup the move by moving entry at the level corresponding to

 * pgt_entry. Returns true if the move was successful, else false.

 See comment in move_ptes() */

		/*

		 * If extent is PUD-sized try to speed up the move by moving at the

		 * PUD level if possible.

 We ignore and continue on error? */

			/*

			 * If the extent is PMD-sized, try to speed the move by

			 * moving at the PMD level if possible.

 how much done */

	/*

	 * We'd prefer to avoid failure later on in do_munmap:

	 * which may split one vma into three before unmapping.

	/*

	 * Advise KSM to break any KSM pages in the area to be moved:

	 * it would be confusing if they were to turn up at the new

	 * location, where they happen to coincide with different KSM

	 * pages recently unmapped.  But leave vma->vm_flags as it was,

	 * so KSM can come around to merge on vma and new_vma afterwards.

		/*

		 * On error, move entries back from new area to old,

		 * which will succeed since page tables still there,

		 * and then proceed to unmap new area instead of old.

 Conceal VM_ACCOUNT so old reservation is not undone */

	/*

	 * If we failed to move page tables we still do total_vm increment

	 * since do_munmap() will decrement it by old_len == new_len.

	 *

	 * Since total_vm is about to be raised artificially high for a

	 * moment, we need to restore high watermark afterwards: if stats

	 * are taken meanwhile, total_vm and hiwater_vm appear too high.

	 * If this were a serious issue, we'd add a flag to do_munmap().

 Tell pfnmap has moved from this vma */

 We always clear VM_LOCKED[ONFAULT] on the old vma */

		/*

		 * anon_vma links of the old vma is no longer needed after its page

		 * table has been moved.

 Because we won't unmap we don't need to touch locked_vm */

 OOM: unable to split vma, just get accounts right */

 Restore VM_ACCOUNT if one or two pieces of vma left */

	/*

	 * !old_len is a special case where an attempt is made to 'duplicate'

	 * a mapping.  This makes no sense for private mappings as it will

	 * instead create a fresh/new mapping unrelated to the original.  This

	 * is contrary to the basic idea of mremap which creates new mappings

	 * based on the original.  There are no known use cases for this

	 * behavior.  As a result, fail such attempts.

 We can't remap across vm area boundaries */

 Need to be careful about a growing mapping */

 Ensure the old/new locations do not overlap */

	/*

	 * move_vma() need us to stay 4 maps below the threshold, otherwise

	 * it will bail out at the very beginning.

	 * That is a problem if we have already unmaped the regions here

	 * (new_addr, and old_addr), because userspace will not know the

	 * state of the vma's after it gets -ENOMEM.

	 * So, to avoid such scenario we can pre-compute if the whole

	 * operation has high chances to success map-wise.

	 * Worst-scenario case is when both vma's (new_addr and old_addr) get

	 * split in 3 before unmapping it.

	 * That means 2 more maps (1 for each) to the ones we already hold.

	 * Check whether current map count plus 2 still leads us to 4 maps below

	 * the threshold, otherwise return -ENOMEM here to be more safe.

 MREMAP_DONTUNMAP expands by old_len since old_len == new_len */

 We got a new mapping */

 overflow */

 intersection */

/*

 * Expand (or shrink) an existing mapping, potentially moving it at the

 * same time (controlled by the MREMAP_MAYMOVE flag and available VM space)

 *

 * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise

 * This option implies MREMAP_MAYMOVE.

	/*

	 * There is a deliberate asymmetry here: we strip the pointer tag

	 * from the old address but leave the new address alone. This is

	 * for consistency with mmap(), where we prevent the creation of

	 * aliasing mappings in userspace by leaving the tag bits of the

	 * mapping address intact. A non-zero tag will cause the subsequent

	 * range checks to reject the address as invalid.

	 *

	 * See Documentation/arm64/tagged-address-abi.rst for more information.

	/*

	 * MREMAP_DONTUNMAP is always a move and it does not allow resizing

	 * in the process.

	/*

	 * We allow a zero old-len as a special case

	 * for DOS-emu "duplicate shm area" thing. But

	 * a zero new-len is nonsensical.

 addrs must be huge page aligned */

		/*

		 * Don't allow remap expansion, because the underlying hugetlb

		 * reservation is not yet capable to handle split reservation.

	/*

	 * Always allow a shrinking remap: that just unmaps

	 * the unnecessary pages..

	 * __do_munmap does all the needed commit accounting, and

	 * downgrades mmap_lock to read if so directed.

 Returning 1 indicates mmap_lock is downgraded to read. */

	/*

	 * Ok, we need to grow..

	/* old_len exactly to the end of the area..

 can we just expand the current mapping? */

	/*

	 * We weren't able to just expand or shrink the area,

	 * we need to create a new one and move it..

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/vmstat.c

 *

 *  Manages VM statistics

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  zoned VM statistics

 *  Copyright (C) 2006 Silicon Graphics, Inc.,

 *		Christoph Lameter <christoph@lameter.com>

 *  Copyright (C) 2008-2014 Christoph Lameter

 zero numa counters within a zone */

 zero numa counters of all the populated zones */

 zero global numa counters */

/*

 * Accumulate the vm event counters across all CPUs.

 * The result is unavoidably approximate - it can change

 * during and after execution of this function.

/*

 * Fold the foreign cpu events into our own.

 *

 * This is adding to the events on one processor

 * but keeps the global counts constant.

 CONFIG_VM_EVENT_COUNTERS */

/*

 * Manage combined zone based / global counters

 *

 * vm_stat contains the global counters

	/*

	 * As vmstats are not up to date, there is drift between the estimated

	 * and real values. For high thresholds and a high number of CPUs, it

	 * is possible for the min watermark to be breached while the estimated

	 * value looks fine. The pressure threshold is a reduced value such

	 * that even the maximum amount of drift will not accidentally breach

	 * the min watermark

	/*

	 * Maximum threshold is 125

 memory in 128 MB units */

	/*

	 * The threshold scales with the number of processors and the amount

	 * of memory per zone. More memory means that we can defer updates for

	 * longer, more processors could lead to more contention.

 	 * fls() is used to have a cheap way of logarithmic scaling.

	 *

	 * Some sample thresholds:

	 *

	 * Threshold	Processors	(fls)	Zonesize	fls(mem)+1

	 * ------------------------------------------------------------------

	 * 8		1		1	0.9-1 GB	4

	 * 16		2		2	0.9-1 GB	4

	 * 20 		2		2	1-2 GB		5

	 * 24		2		2	2-4 GB		6

	 * 28		2		2	4-8 GB		7

	 * 32		2		2	8-16 GB		8

	 * 4		2		2	<128M		1

	 * 30		4		3	2-4 GB		5

	 * 48		4		3	8-16 GB		8

	 * 32		8		4	1-2 GB		4

	 * 32		8		4	0.9-1GB		4

	 * 10		16		5	<128M		1

	 * 40		16		5	900M		4

	 * 70		64		7	2-4 GB		5

	 * 84		64		7	4-8 GB		6

	 * 108		512		9	4-8 GB		6

	 * 125		1024		10	8-16 GB		8

	 * 125		1024		10	16-32 GB	9

	/*

	 * Maximum threshold is 125

/*

 * Refresh the thresholds for each zone.

 Zero current pgdat thresholds */

 Base nodestat threshold on the largest populated zone. */

		/*

		 * Only set percpu_drift_mark if there is a danger that

		 * NR_FREE_PAGES reports the low watermark is ok when in fact

		 * the min watermark could be breached by an allocation

/*

 * For use when we know that interrupts are disabled,

 * or when we know that preemption is disabled and that

 * particular counter cannot be updated from interrupt context.

	/*

	 * Accurate vmstat updates require a RMW. On !PREEMPT_RT kernels,

	 * atomicity is provided by IRQs being disabled -- either explicitly

	 * or via local_lock_irq. On PREEMPT_RT, local_lock_irq only disables

	 * CPU migrations and preemption potentially corrupts a counter so

	 * disable preemption.

		/*

		 * Only cgroups use subpage accounting right now; at

		 * the global level, these items still change in

		 * multiples of whole pages. Store them as pages

		 * internally to keep the per-cpu counters compact.

 See __mod_node_page_state */

/*

 * Optimized increment and decrement functions.

 *

 * These are only for a single page and therefore can take a struct page *

 * argument instead of struct zone *. This allows the inclusion of the code

 * generated for page_zone(page) into the optimized functions.

 *

 * No overflow check is necessary and therefore the differential can be

 * incremented or decremented in place which may allow the compilers to

 * generate better code.

 * The increment or decrement is known and therefore one boundary check can

 * be omitted.

 *

 * NOTE: These functions are very performance sensitive. Change only

 * with care.

 *

 * Some processors have inc/dec instructions that are atomic vs an interrupt.

 * However, the code must first determine the differential location in a zone

 * based on the processor number and then inc/dec the counter. There is no

 * guarantee without disabling preemption that the processor will not change

 * in between and therefore the atomicity vs. interrupt cannot be exploited

 * in a useful way here.

 See __mod_node_page_state */

 See __mod_node_page_state */

 See __mod_node_page_state */

 See __mod_node_page_state */

/*

 * If we have cmpxchg_local support then we do not need to incur the overhead

 * that comes with local_irq_save/restore if we use this_cpu_cmpxchg.

 *

 * mod_state() modifies the zone counter state through atomic per cpu

 * operations.

 *

 * Overstep mode specifies how overstep should handled:

 *     0       No overstepping

 *     1       Overstepping half of threshold

 *     -1      Overstepping minus half of threshold

 overflow to zone counters */

		/*

		 * The fetching of the stat_threshold is racy. We may apply

		 * a counter threshold to the wrong the cpu if we get

		 * rescheduled while executing here. However, the next

		 * counter update will apply the threshold again and

		 * therefore bring the counter under the threshold again.

		 *

		 * Most of the time the thresholds are the same anyways

		 * for all cpus in a zone.

 Overflow must be added to zone counters */

		/*

		 * Only cgroups use subpage accounting right now; at

		 * the global level, these items still change in

		 * multiples of whole pages. Store them as pages

		 * internally to keep the per-cpu counters compact.

 overflow to node counters */

		/*

		 * The fetching of the stat_threshold is racy. We may apply

		 * a counter threshold to the wrong the cpu if we get

		 * rescheduled while executing here. However, the next

		 * counter update will apply the threshold again and

		 * therefore bring the counter under the threshold again.

		 *

		 * Most of the time the thresholds are the same anyways

		 * for all cpus in a node.

 Overflow must be added to node counters */

/*

 * Use interrupt disable to serialize counter updates

/*

 * Fold a differential into the global counters.

 * Returns the number of counters updated.

/*

 * Update the zone counters for the current cpu.

 *

 * Note that refresh_cpu_vm_stats strives to only access

 * node local memory. The per cpu pagesets on remote zones are placed

 * in the memory local to the processor using that pageset. So the

 * loop over all zones will access a series of cachelines local to

 * the processor.

 *

 * The call to zone_page_state_add updates the cachelines with the

 * statistics in the remote zone struct as well as the global cachelines

 * with the global counters. These could cause remote node cache line

 * bouncing and will have to be only done when necessary.

 *

 * The function returns the number of global counters updated.

 3 seconds idle till flush */

			/*

			 * Deal with draining the remote pageset of this

			 * processor

			 *

			 * Check if there are pages remaining in this pageset

			 * if not then there is nothing to expire.

			/*

			 * We never drain zones local to this processor.

/*

 * Fold the data for an offline cpu into the global array.

 * There cannot be any access by the offline cpu and therefore

 * synchronization is simplified.

/*

 * this is only called if !populated_zone(zone), which implies no other users of

 * pset->vm_stat_diff[] exist.

/*

 * Determine the per node value of a stat item. This function

 * is called frequently in a NUMA machine, so try to be as

 * frugal as possible.

 Determine the per node value of a numa stat item. */

/*

 * Determine the per node value of a stat item.

/*

 * Calculate the number of free pages in a zone, how many contiguous

 * pages are free and how many are large enough to satisfy an allocation of

 * the target size. Note that this function makes no attempt to estimate

 * how many suitable free blocks there *might* be if MOVABLE pages were

 * migrated. Calculating that is possible, but expensive and can be

 * figured out from userspace

		/*

		 * Count number of free blocks.

		 *

		 * Access to nr_free is lockless as nr_free is used only for

		 * diagnostic purposes. Use data_race to avoid KCSAN warning.

 Count free base pages */

 Count the suitable free blocks */

/*

 * A fragmentation index only makes sense if an allocation of a requested

 * size would fail. If that is true, the fragmentation index indicates

 * whether external fragmentation or a lack of memory was the problem.

 * The value can be used to determine if page reclaim or compaction

 * should be used

 Fragmentation index only makes sense when a request would fail */

	/*

	 * Index is between 0 and 1 so return within 3 decimal places

	 *

	 * 0 => allocation would fail due to lack of memory

	 * 1 => allocation would fail due to fragmentation

/*

 * Calculates external fragmentation within a zone wrt the given order.

 * It is defined as the percentage of pages found in blocks of size

 * less than 1 << order. It returns values in range [0, 100].

 Same as __fragmentation index but allocs contig_page_info on stack */

 enum zone_stat_item counters */

 enum numa_stat_item counters */

 enum node_stat_item counters */

 enum writeback_stat_item counters */

 enum vm_event_item counters */

 CONFIG_MEMORY_BALLOON */

 CONFIG_DEBUG_TLBFLUSH */

 CONFIG_VM_EVENT_COUNTERS || CONFIG_MEMCG */

 CONFIG_PROC_FS || CONFIG_SYSFS || CONFIG_NUMA || CONFIG_MEMCG */

/*

 * Walk zones in a node and print using a callback.

 * If @assert_populated is true, only use callback for zones that are populated.

		/*

		 * Access to nr_free is lockless as nr_free is used only for

		 * printing purposes. Use data_race to avoid KCSAN warning.

/*

 * This walks the free areas for each zone.

				/*

				 * Cap the free_list iteration because it might

				 * be really large and we are under a spinlock

				 * so a long time spent here could trigger a

				 * hard lockup detector. Anyway this is a

				 * debugging tool so knowing there is a handful

				 * of pages of this order should be more than

				 * sufficient.

 Print out the free pages at each order for each migatetype */

 Print header */

 Print counts */

 Print out the number of pageblocks for each migratetype */

/*

 * Print out the number of pageblocks for each migratetype that contain pages

 * of other types. This gives an indication of how well fallbacks are being

 * contained by rmqueue_fallback(). It requires information from PAGE_OWNER

 * to determine what is going on

 CONFIG_PAGE_OWNER */

/*

 * This prints out statistics in relation to grouping pages by mobility.

 * It is expensive to collect so do not constantly read the file.

 check memoryless node */

 If unpopulated, no other information is useful */

/*

 * Output information about zones in @pgdat.  All zones are printed regardless

 * of whether they are populated or not: lowmem_reserve_ratio operates on the

 * set of all zones and userspace would not be aware of such zones if they are

 * suppressed here (zoneinfo displays the effect of lowmem_reserve_ratio).

	.start	= frag_start, /* iterate over all zones. The same as in

 sectors -> kbytes */

		/*

		 * We've come to the end - add any deprecated counters to avoid

		 * breaking userspace which might depend on them being present.

 CONFIG_PROC_FS */

	/*

	 * The regular update, every sysctl_stat_interval, may come later

	 * than expected: leaving a significant amount in per_cpu buckets.

	 * This is particularly misleading when checking a quantity of HUGE

	 * pages, immediately after running a test.  /proc/sys/vm/stat_refresh,

	 * which can equally be echo'ed to or cat'ted from (by root),

	 * can be used to update the stats just before reading them.

	 *

	 * Oh, and since global_zone_page_state() etc. are so careful to hide

	 * transiently negative values, report an error here if any of

	 * the stats is negative, so we know to go looking for imbalance.

		/*

		 * Skip checking stats known to go negative occasionally.

		/*

		 * Skip checking stats known to go negative occasionally.

 CONFIG_PROC_FS */

		/*

		 * Counters were updated so we expect more updates

		 * to occur in the future. Keep on running the

		 * update worker thread.

/*

 * Check if the diffs for a certain cpu indicate that

 * an update is needed.

		/*

		 * The fast way of checking if there are any vmstat diffs.

/*

 * Switch off vmstat processing and then fold all the remaining differentials

 * until the diffs stay at zero. The function is used by NOHZ and can only be

 * invoked when tick processing is not active.

	/*

	 * Just refresh counters and do not care about the pending delayed

	 * vmstat_update. It doesn't fire that often to matter and canceling

	 * it would be too expensive from this path.

	 * vmstat_shepherd will take care about that for us.

/*

 * Shepherd worker thread that checks the

 * differentials of processors that have their worker

 * threads for vm statistics updates disabled because of

 * inactivity.

 Check processors whose vmstat worker threads have been disabled */

/*

 * Return an index indicating how much of the available free memory is

 * unusable for an allocation of the requested size.

 No free memory is interpreted as all free memory is unusable */

	/*

	 * Index should be a value between 0 and 1. Return a value to 3

	 * decimal places.

	 *

	 * 0 => no fragmentation

	 * 1 => high fragmentation

/*

 * Display unusable free space index

 *

 * The unusable free space index measures how much of the available free

 * memory cannot be used to satisfy an allocation of a given size and is a

 * value between 0 and 1. The higher the value, the more of free memory is

 * unusable and by implication, the worse the external fragmentation is. This

 * can be expressed as a percentage by multiplying by 100.

 check memoryless node */

 Alloc on stack as interrupts are disabled for zone walk */

/*

 * Display fragmentation index for orders that allocations would fail for

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2009  Red Hat, Inc.

/*

 * By default, transparent hugepage support is disabled in order to avoid

 * risking an increased memory footprint for applications that are not

 * guaranteed to benefit from it. When transparent hugepage support is

 * enabled, it is for all mappings, and khugepaged scans all mappings.

 * Defrag is invoked by khugepaged hugepage allocations and by page faults

 * for all hugepage allocations.

 The addr is used to check if the vma size fits */

 We take additional reference here. It will be put back by shrinker */

	/*

	 * Counter should never go to zero here. Only shrinker can put

	 * last reference.

 we can free zero page only if last reference remains */

 CONFIG_SYSFS */

		/*

		 * Hardware doesn't support hugepages, hence disable

		 * DAX PMD support.

	/*

	 * hugepages can't be allocated by the buddy allocator

	/*

	 * we use page->mapping and page->index in second tail page

	 * as list_head: assuming THP order >= 2

	/*

	 * By default disable transparent hugepages on smaller systems,

	 * where the extra memory used could hurt more than TLB overhead

	 * is likely to save.  The admin can still enable it through /sys.

	/*

	 * we use page->mapping and page->indexlru in second tail page

	 * as list_head: assuming THP order >= 2

	/*

	 * The failure might be due to length padding. The caller will retry

	 * without the padding.

	/*

	 * Do not try to align to THP boundary if allocation at the address

	 * hint succeeds.

	/*

	 * The memory barrier inside __SetPageUptodate makes sure that

	 * clear_huge_page writes become visible before the set_pmd_at()

	 * write.

 Deliver the page fault to userland */

/*

 * always: directly stall for all thp allocations

 * defer: wake kswapd and fail if not immediately available

 * defer+madvise: wake kswapd and directly stall for MADV_HUGEPAGE, otherwise

 *		  fail if not immediately available

 * madvise: directly stall for MADV_HUGEPAGE, otherwise fail if not immediately

 *	    available

 * never: never stall for any thp allocation

 Always do synchronous compaction */

 Kick kcompactd and fail quickly */

 Synchronous compaction if madvised, otherwise kick kcompactd */

 Only do synchronous compaction if madvised */

 Caller must hold page table lock. */

/**

 * vmf_insert_pfn_pmd_prot - insert a pmd size pfn

 * @vmf: Structure describing the fault

 * @pfn: pfn to insert

 * @pgprot: page protection to use

 * @write: whether it's a write fault

 *

 * Insert a pmd size pfn. See vmf_insert_pfn() for additional info and

 * also consult the vmf_insert_mixed_prot() documentation when

 * @pgprot != @vmf->vma->vm_page_prot.

 *

 * Return: vm_fault_t value.

	/*

	 * If we had pmd_special, we could avoid all these restrictions,

	 * but we need to be consistent with PTEs and architectures that

	 * can't support a 'special' bit.

/**

 * vmf_insert_pfn_pud_prot - insert a pud size pfn

 * @vmf: Structure describing the fault

 * @pfn: pfn to insert

 * @pgprot: page protection to use

 * @write: whether it's a write fault

 *

 * Insert a pud size pfn. See vmf_insert_pfn() for additional info and

 * also consult the vmf_insert_mixed_prot() documentation when

 * @pgprot != @vmf->vma->vm_page_prot.

 *

 * Return: vm_fault_t value.

	/*

	 * If we had pud_special, we could avoid all these restrictions,

	 * but we need to be consistent with PTEs and architectures that

	 * can't support a 'special' bit.

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

	/*

	 * When we COW a devmap PMD entry, we split it into PTEs, so we should

	 * not be in this function with `flags & FOLL_COW` set.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

 pass */;

	/*

	 * device mapped pages can only be returned if the

	 * caller will manage the page reference count.

 Skip if can be re-fill on fault */

	/*

	 * When page table lock is held, the huge zero pmd should not be

	 * under splitting since we don't split the page itself, only pmd to

	 * a page table.

		/*

		 * get_huge_zero_page() will never allocate a new page here,

		 * since we already have a zero page to copy. It just takes a

		 * reference.

	/*

	 * If this page is a potentially pinned page, split and retry the fault

	 * with smaller page size.  Normally this should not happen because the

	 * userspace should use MADV_DONTFORK upon pinned regions.  This is a

	 * best effort that the pinned pages won't be replaced by another

	 * random page during the coming copy-on-write.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

 pass */;

	/*

	 * device mapped pages can only be returned if the

	 * caller will manage the page reference count.

	 *

	 * At least one of FOLL_GET | FOLL_PIN must be set, so assert that here:

	/*

	 * When page table lock is held, the huge zero pud should not be

	 * under splitting since we don't split the page itself, only pud to

	 * a page table.

 No huge zero pud yet */

 Please refer to comments in copy_huge_pmd() */

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 Lock page for reuse_swap_page() */

	/*

	 * We can only reuse the page if nobody else maps the huge page or it's

	 * part.

/*

 * FOLL_FORCE can write to even unwritable pmd's, but only

 * after we've gone through a COW cycle and they are dirty.

 Avoid dumping huge zero page */

 Full NUMA hinting faults to serialise migration in fault paths */

		/*

		 * We don't mlock() pte-mapped THPs. This way we can avoid

		 * leaking mlocked pages into non-VM_LOCKED VMAs.

		 *

		 * For anon THP:

		 *

		 * In most cases the pmd is the only mapping of the page as we

		 * break COW for the mlock() -- see gup_flags |= FOLL_WRITE for

		 * writable private mappings in populate_vma_page_range().

		 *

		 * The only scenario when we have the page shared here is if we

		 * mlocking read-only mapping shared over fork(). We skip

		 * mlocking such pages.

		 *

		 * For file THP:

		 *

		 * We can expect PageDoubleMap() to be stable under page lock:

		 * for file pages we set it in page_add_file_rmap(), which

		 * requires page to be locked.

 NUMA hinting page fault entry point for trans huge pmds */

 See similar comment in do_numa_page for explanation */

 Restore the PMD */

/*

 * Return true if we do MADV_FREE successfully on entire pmd page.

 * Otherwise, return false.

	/*

	 * If other processes are mapping this page, we couldn't discard

	 * the page unless they all do MADV_FREE so let's skip the page.

	/*

	 * If user want to discard part-pages of THP, split it so MADV_FREE

	 * will deactivate only them.

	/*

	 * For architectures like ppc64 we look at deposited pgtable

	 * when calling pmdp_huge_get_and_clear. So do the

	 * pgtable_trans_huge_withdraw after finishing pmdp related

	 * operations.

	/*

	 * With split pmd lock we also need to move preallocated

	 * PTE page table if new_pmd is on different PMD page table.

	 *

	 * We also don't deposit and withdraw tables for file pages.

	/*

	 * The destination pmd shouldn't be established, free_pgtables()

	 * should have release it.

	/*

	 * We don't have to worry about the ordering of src and dst

	 * ptlocks because exclusive mmap_lock prevents deadlock.

/*

 * Returns

 *  - 0 if PMD could not be locked

 *  - 1 if PMD was locked but protections unchanged and TLB flush unnecessary

 *      or if prot_numa but THP migration is not supported

 *  - HPAGE_PMD_NR if protections changed and TLB flush necessary

			/*

			 * A protection check is difficult so

			 * just be safe and disable write

	/*

	 * Avoid trapping faults against the zero page. The read-only

	 * data is likely to be read-cached on the local CPU and

	 * local/remote hits to the zero page are not interesting.

	/*

	 * In case prot_numa, we are under mmap_read_lock(mm). It's critical

	 * to not clear pmd intermittently to avoid race with MADV_DONTNEED

	 * which is also under mmap_read_lock(mm):

	 *

	 *	CPU0:				CPU1:

	 *				change_huge_pmd(prot_numa=1)

	 *				 pmdp_huge_get_and_clear_notify()

	 * madvise_dontneed()

	 *  zap_pmd_range()

	 *   pmd_trans_huge(*pmd) == 0 (without ptl)

	 *   // skip the pmd

	 *				 set_pmd_at();

	 *				 // pmd is re-established

	 *

	 * The race makes MADV_DONTNEED miss the huge pmd and don't clear it

	 * which may break userspace.

	 *

	 * pmdp_invalidate() is required to make sure we don't miss

	 * dirty/young flags set by hardware.

		/*

		 * Leave the write bit to be handled by PF interrupt

		 * handler, then things like COW could be properly

		 * handled.

/*

 * Returns page table lock pointer if a given pmd maps a thp, NULL otherwise.

 *

 * Note that if it returns page table lock pointer, this routine returns without

 * unlocking page table lock. So callers must unlock it.

/*

 * Returns true if a given pud maps a thp, false otherwise.

 *

 * Note that if it returns true, this routine returns without unlocking page

 * table lock. So callers must unlock it.

	/*

	 * For architectures like ppc64 we look at deposited pgtable

	 * when calling pudp_huge_get_and_clear. So do the

	 * pgtable_trans_huge_withdraw after finishing pudp related

	 * operations.

 No zero page support yet */

 No support for anonymous PUD pages yet */

	/*

	 * No need to double call mmu_notifier->invalidate_range() callback as

	 * the above pudp_huge_clear_flush_notify() did already call it.

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

	/*

	 * Leave pmd empty until pte is filled note that it is fine to delay

	 * notification until mmu_notifier_invalidate_range_end() as we are

	 * replacing a zero pmd write protected page with a zero pte write

	 * protected page.

	 *

	 * See Documentation/vm/mmu_notifier.rst

 make pte visible before pmd */

		/*

		 * We are going to unmap this huge page. So

		 * just go ahead and zap it

		/*

		 * FIXME: Do we want to invalidate secondary mmu by calling

		 * mmu_notifier_invalidate_range() see comments below inside

		 * __split_huge_pmd() ?

		 *

		 * We are going from a zero huge page write protected to zero

		 * small page also write protected so it does not seems useful

		 * to invalidate secondary mmu at this time.

	/*

	 * Up to this point the pmd is present and huge and userland has the

	 * whole access to the hugepage during the split (which happens in

	 * place). If we overwrite the pmd with the not-huge version pointing

	 * to the pte here (which of course we could if all CPUs were bug

	 * free), userland could trigger a small page size TLB miss on the

	 * small sized TLB while the hugepage TLB entry is still established in

	 * the huge TLB. Some CPU doesn't like that.

	 * See http://support.amd.com/TechDocs/41322_10h_Rev_Gd.pdf, Erratum

	 * 383 on page 105. Intel should be safe but is also warns that it's

	 * only safe if the permission and cache attributes of the two entries

	 * loaded in the two TLB is identical (which should be the case here).

	 * But it is generally safer to never allow small and huge TLB entries

	 * for the same virtual address to be loaded simultaneously. So instead

	 * of doing "pmd_populate(); flush_pmd_tlb_range();" we first mark the

	 * current pmd notpresent (atomically because here the pmd_trans_huge

	 * must remain set at all times on the pmd until the split is complete

	 * for this pmd), then we flush the SMP TLB and finally we write the

	 * non-huge version of the pmd entry with pmd_populate.

	/*

	 * Withdraw the table only after we mark the pmd entry invalid.

	 * This's critical for some architectures (Power).

		/*

		 * Note that NUMA hinting access restrictions are not

		 * transferred to avoid any possibility of altering

		 * permissions across VMAs.

		/*

		 * Set PG_double_map before dropping compound_mapcount to avoid

		 * false-negative page_mapped().

 Last compound_mapcount is gone. */

 No need in mapcount reference anymore */

 make pte visible before pmd */

	/*

	 * If caller asks to setup a migration entries, we need a page to check

	 * pmd against. Otherwise we can end up replacing wrong page.

			/*

			 * An anonymous page must be locked, to ensure that a

			 * concurrent reuse_swap_page() sees stable mapcount;

			 * but reuse_swap_page() is not used on shmem or file,

			 * and page lock must not be taken when zap_pmd_range()

			 * calls __split_huge_pmd() while i_mmap_lock is held.

	/*

	 * No need to double call mmu_notifier->invalidate_range() callback.

	 * They are 3 cases to consider inside __split_huge_pmd_locked():

	 *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious

	 *  2) __split_huge_zero_page_pmd() read only zero page and any write

	 *    fault will trigger a flush_notify before pointing to a new page

	 *    (it is fine if the secondary mmu keeps pointing to the old zero

	 *    page in the meantime)

	 *  3) Split a huge pmd into pte pointing to the same page. No need

	 *     to invalidate secondary tlb entry they are all still valid.

	 *     any further changes to individual pte will notify. So no need

	 *     to call mmu_notifier->invalidate_range()

	/*

	 * If the new address isn't hpage aligned and it could previously

	 * contain an hugepage: check if we need to split an huge pmd.

 Check if we need to split start first. */

 Check if we need to split end next. */

	/*

	 * If we're also updating the vma->vm_next->vm_start,

	 * check if we need to split it.

	/*

	 * Anon pages need migration entries to preserve them, but file

	 * pages can simply be left unmapped, then faulted back on demand.

	 * If that is ever changed (perhaps for mlock), update remap_page().

 If unmap_page() uses try_to_migrate() on file, remove this check */

 page reclaim is reclaiming a huge page */

 head is still on lru (and we have it frozen) */

	/*

	 * Clone page flags before unfreezing refcount.

	 *

	 * After successful get_page_unless_zero() might follow flags change,

	 * for example lock_page() which set PG_waiters.

 ->mapping in first tail page is compound_mapcount */

 Page flags must be visible before we make the page non-compound. */

	/*

	 * Clear PageTail before unfreezing page refcount.

	 *

	 * After successful get_page_unless_zero() might follow put_page()

	 * which needs correct compound_head().

 Finally unfreeze refcount. Additional reference from page cache. */

	/*

	 * always add to the tail because some iterators expect new

	 * pages to show after the currently processed elements - e.g.

	 * migrate_pages

 complete memcg works before add pages to LRU */

 lock lru list/PageCompound, ref frozen by page_ref_freeze */

 Some pages can be beyond EOF: drop them from page cache */

 Caller disabled irqs, so they are still disabled here */

 See comment in __split_huge_page_tail() */

 Additional pin to swap cache */

 Additional pin to page cache */

		/*

		 * Subpages may be freed if there wasn't any mapping

		 * like if add_to_swap() is running on a lru page that

		 * had its mapping zapped. And freeing these pages

		 * requires taking the lru_lock so we do the put_page

		 * of the tail pages after the split is complete.

 File pages has compound_mapcount included in _mapcount */

/*

 * This calculates accurately how many mappings a transparent hugepage

 * has (unlike page_mapcount() which isn't fully accurate). This full

 * accuracy is primarily needed to know if copy-on-write faults can

 * reuse the page and change the mapping to read-write instead of

 * copying them. At the same time this returns the total_mapcount too.

 *

 * The function returns the highest mapcount any one of the subpages

 * has. If the return value is one, even if different processes are

 * mapping different subpages of the transparent hugepage, they can

 * all reuse it, because each process is reusing a different subpage.

 *

 * The total_mapcount is instead counting all virtual mappings of the

 * subpages. If the total_mapcount is equal to "one", it tells the

 * caller all mappings belong to the same "mm" and in turn the

 * anon_vma of the transparent hugepage can become the vma->anon_vma

 * local one as no other process may be mapping any of the subpages.

 *

 * It would be more accurate to replace page_mapcount() with

 * page_trans_huge_mapcount(), however we only use

 * page_trans_huge_mapcount() in the copy-on-write faults where we

 * need full accuracy to avoid breaking page pinning, because

 * page_trans_huge_mapcount() is slower than page_mapcount().

 hugetlbfs shouldn't call it */

 Racy check whether the huge page can be split */

 Additional pins from page cache */

/*

 * This function splits huge page into normal pages. @page can point to any

 * subpage of huge page to split. Split doesn't change the position of @page.

 *

 * Only caller must hold pin on the @page, otherwise split fails with -EBUSY.

 * The huge page must be locked.

 *

 * If @list is null, tail pages will be added to LRU list, otherwise, to @list.

 *

 * Both head page and tail pages will inherit mapping, flags, and so on from

 * the hugepage.

 *

 * GUP pin and PG_locked transferred to @page. Rest subpages can be freed if

 * they are not mapped.

 *

 * Returns 0 if the hugepage is split successfully.

 * Returns -EBUSY if the page is pinned or if anon_vma disappeared from under

 * us.

		/*

		 * The caller does not necessarily hold an mmap_lock that would

		 * prevent the anon_vma disappearing so we first we take a

		 * reference to it and then lock the anon_vma for write. This

		 * is similar to page_lock_anon_vma_read except the write lock

		 * is taken to serialise against parallel split or collapse

		 * operations.

 Truncated ? */

		/*

		 *__split_huge_page() may need to trim off pages beyond EOF:

		 * but on 32-bit, i_size_read() takes an irq-unsafe seqlock,

		 * which cannot be nested inside the page tree lock. So note

		 * end now: i_size itself may be changed at any moment, but

		 * head page lock is good enough to serialize the trimming.

	/*

	 * Racy check if we can split the page, before unmap_page() will

	 * split PMDs

 block interrupt reentry in xa_lock and spinlock */

		/*

		 * Check if the head page is present in page cache.

		 * We assume all tail are present too, if head is there.

 Prevent deferred_split_scan() touching ->_refcount */

	/*

	 * The try_to_unmap() in page reclaim path might reach here too,

	 * this may cause a race condition to corrupt deferred split queue.

	 * And, if page reclaim is already handling the same page, it is

	 * unnecessary to handle it again in shrinker.

	 *

	 * Check PageSwapCache to determine if the page is being

	 * handled by page reclaim since THP swap would add the page into

	 * swap cache before calling try_to_unmap().

 Take pin on all head pages to avoid freeing them under us */

 We lost race with put_compound_page() */

 split_huge_page() removes page from list on success */

	/*

	 * Stop shrinker if we didn't split any page, but the queue is empty.

	 * This can happen if pages were freed under us.

 Find the task_struct from pid */

 Find the mm_struct */

	/*

	 * always increase addr by PAGE_SIZE, since we could have a PTE page

	 * table filled with PTE-mapped THPs, each of which is distinct.

 skip special VMA and hugetlb VMA */

 FOLL_DUMP to ignore special (like zero) pages */

 hold pid, start_vaddr, end_vaddr or file_path, off_start, off_end */

/*

 * Resizable virtual memory filesystem for Linux.

 *

 * Copyright (C) 2000 Linus Torvalds.

 *		 2000 Transmeta Corp.

 *		 2000-2001 Christoph Rohland

 *		 2000-2001 SAP AG

 *		 2002 Red Hat Inc.

 * Copyright (C) 2002-2011 Hugh Dickins.

 * Copyright (C) 2011 Google Inc.

 * Copyright (C) 2002-2005 VERITAS Software Corporation.

 * Copyright (C) 2004 Andi Kleen, SuSE Labs

 *

 * Extended attribute support for tmpfs:

 * Copyright (c) 2004, Luke Kenneth Casson Leighton <lkcl@lkcl.net>

 * Copyright (c) 2004 Red Hat, Inc., James Morris <jmorris@redhat.com>

 *

 * tiny-shmem:

 * Copyright (c) 2004, 2008 Matt Mackall <mpm@selenic.com>

 *

 * This file is released under the GPL.

/*

 * This virtual memory filesystem is heavily based on the ramfs. It

 * extends ramfs by the ability to use swap and honor resource limits

 * which makes it a completely usable filesystem.

 Pretend that each entry is of this size in directory's i_size */

 Symlink up to this size is kmalloc'ed instead of using a swappable page */

/*

 * shmem_fallocate communicates with shmem_fault or shmem_writepage via

 * inode->i_private (with i_rwsem making sure that it has only one user at

 * a time): we would prefer not to enlarge the shmem inode just for that.

 faults into hole wait for punch to end */

 start of range currently being fallocated */

 the next page offset to be fallocated */

 how many new pages have been fallocated */

 how often writepage refused to swap out */

/*

 * shmem_file_setup pre-accounts the whole fixed size of a VM object,

 * for shared memory and for shared anonymous (/dev/zero) mappings

 * (unless MAP_NORESERVE and sysctl_overcommit_memory <= 1),

 * consistent with the pre-accounting of private mappings ...

/*

 * ... whereas tmpfs objects are accounted incrementally as

 * pages are allocated, in order to allow large sparse files.

 * shmem_getpage reports shmem_acct_block failure as -ENOSPC not -ENOMEM,

 * so that a failure on a sparse tmpfs mapping will give SIGBUS not OOM.

/*

 * shmem_reserve_inode() performs bookkeeping to reserve a shmem inode, and

 * produces a novel ino for the newly allocated inode.

 *

 * It may also be called when making a hard link to permit the space needed by

 * each dentry. However, in that case, no new inode number is needed since that

 * internally draws from another pool of inode numbers (currently global

 * get_next_ino()). This case is indicated by passing NULL as inop.

				/*

				 * Emulate get_next_ino uint wraparound for

				 * compatibility

		/*

		 * __shmem_file_setup, one of our callers, is lock-free: it

		 * doesn't hold stat_lock in shmem_reserve_inode since

		 * max_inodes is always 0, and is called from potentially

		 * unknown contexts. As such, use a per-cpu batched allocator

		 * which doesn't require the per-sb stat_lock unless we are at

		 * the batch boundary.

		 *

		 * We don't need to worry about inode{32,64} since SB_KERNMOUNT

		 * shmem mounts are not exposed to userspace, so we don't need

		 * to worry about things like glibc compatibility.

/**

 * shmem_recalc_inode - recalculate the block usage of an inode

 * @inode: inode to recalc

 *

 * We have to calculate the free blocks since the mm can drop

 * undirtied hole pages behind our back.

 *

 * But normally   info->alloced == inode->i_mapping->nrpages + info->swapped

 * So mm freed is info->alloced - (inode->i_mapping->nrpages + info->swapped)

 *

 * It has to be called with the spinlock held.

 nrpages adjustment first, then shmem_recalc_inode() when balanced */

 nrpages adjustment done by __delete_from_page_cache() or caller */

/*

 * Replace item expected in xarray by a new item, while holding xa_lock.

/*

 * Sometimes, before we decide whether to proceed or to fail, we must check

 * that an entry was not already brought back from swap by a racing thread.

 *

 * Checking page is not enough: by the time a SwapCache page is locked, it

 * might be reused, and again be SwapCache, using the same swap as before.

/*

 * Definitions for "huge tmpfs": tmpfs mounted with the huge= option

 *

 * SHMEM_HUGE_NEVER:

 *	disables huge pages for the mount;

 * SHMEM_HUGE_ALWAYS:

 *	enables huge pages for the mount;

 * SHMEM_HUGE_WITHIN_SIZE:

 *	only allocate huge pages if the page will be fully within i_size,

 *	also respect fadvise()/madvise() hints;

 * SHMEM_HUGE_ADVISE:

 *	only allocate huge pages if requested with fadvise()/madvise();

/*

 * Special values.

 * Only can be set via /sys/kernel/mm/transparent_hugepage/shmem_enabled:

 *

 * SHMEM_HUGE_DENY:

 *	disables huge on shm_mnt and all mounts, for emergency use;

 * SHMEM_HUGE_FORCE:

 *	enables huge on shm_mnt and all mounts, w/o needing option, for testing;

 *

 ifdef here to avoid bloating shmem.o when not necessary */

 pin the inode */

 inode is about to be evicted */

 Check if there's anything to gain */

 No huge page at the end of the file: nothing to split */

		/*

		 * Leave the inode on the list if we failed to lock

		 * the page at this time.

		 *

		 * Waiting for the lock may lead to deadlock in the

		 * reclaim path.

 If split failed leave the inode on the list */

 !CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

/*

 * Like add_to_page_cache_locked, but error if expected item has gone.

/*

 * Like delete_from_page_cache, but substitutes swap for page.

/*

 * Remove swap entry from page cache, free the swap and its page cache.

/*

 * Determine (in bytes) how many of the shmem object's pages mapped by the

 * given offsets are swapped out.

 *

 * This is safe to call without i_rwsem or the i_pages lock thanks to RCU,

 * as long as the inode doesn't go away and racy results are not a problem.

/*

 * Determine (in bytes) how many of the shmem object's pages mapped by the

 * given vma is swapped out.

 *

 * This is safe to call without i_rwsem or the i_pages lock thanks to RCU,

 * as long as the inode doesn't go away and racy results are not a problem.

 Be careful as we don't hold info->lock */

	/*

	 * The easier cases are when the shmem object has nothing in swap, or

	 * the vma maps it whole. Then we can simply use the stats that we

	 * already track.

 Here comes the more involved part */

/*

 * SysV IPC SHM_UNLOCK restore Unevictable pages to their evictable lists.

	/*

	 * Minor point, but we might as well stop if someone else SHM_LOCKs it.

/*

 * Check whether a hole-punch or truncation needs to split a huge page,

 * returning true if no split was required, or the split has been successful.

 *

 * Eviction (or truncation to 0 size) should never need to split a huge page;

 * but in rare cases might do so, if shmem_undo_range() failed to trylock on

 * head, and then succeeded to trylock on tail.

 *

 * A split can only succeed when there are no additional references on the

 * huge page: so the split below relies upon find_get_entries() having stopped

 * when it found a subpage of the huge page, without getting further references.

 Just proceed to delete a huge page wholly within the range punched */

 Try to split huge page, so we can truly punch the hole or truncate */

/*

 * Remove range of pages and swap entries from page cache, and free them.

 * If !unfalloc, truncate or punch hole; if unfalloc, undo failed fallocate.

 unsigned, so actually very big */

 If all gone or hole-punch or unfalloc, we're done */

 But if truncating, restart to make sure all gone */

 Swap was replaced by page: retry */

 Page was replaced by swap: retry */

 Wipe the page and don't get stuck */

 protected by i_rwsem */

 unmap again to remove racily COWed private pages */

 Wait while shmem_unuse() is scanning this inode... */

 ...but beware of the race if we peeked too early */

/*

 * Move the swapped pages for an inode to page cache. Returns the count

 * of pages swapped in, or the error in case of failure.

/*

 * If swap found in inode, free it and move page from swapcache to filecache.

/*

 * Read all the shared memory data that resides in the swap

 * device 'type' back into memory, so the swap device can be

 * unused.

		/*

		 * Drop the swaplist mutex while searching the inode for swap;

		 * but before doing so, make sure shmem_evict_inode() will not

		 * remove placeholder inode from swaplist, nor let it be freed

		 * (igrab() would protect from unlink, but not from unmount).

/*

 * Move the page from the page cache to the swap cache.

	/*

	 * If /sys/kernel/mm/transparent_hugepage/shmem_enabled is "always" or

	 * "force", drivers/gpu/drm/i915/gem/i915_gem_shmem.c gets huge pages,

	 * and its shmem_writeback() needs them to be split when swapping.

 Ensure the subpages are still dirty */

	/*

	 * Our capabilities prevent regular writeback or sync from ever calling

	 * shmem_writepage; but a stacking filesystem might use ->writepage of

	 * its underlying filesystem, in which case tmpfs should write out to

	 * swap only in response to memory pressure, and not for the writeback

	 * threads or sync.

 Still happens? Tell us about it! */

	/*

	 * This is somewhat ridiculous, but without plumbing a SWAP_MAP_FALLOC

	 * value into swapfile.c, the only way we can correctly account for a

	 * fallocated page arriving here is now to initialize it and write it.

	 *

	 * That's okay for a page already fallocated earlier, but if we have

	 * not yet completed the fallocation, then (a) we want to keep track

	 * of this page in case we have to undo it, and (b) it may not be a

	 * good idea to continue anyway, once we're pushing into swap.  So

	 * reactivate the page, and let shmem_fallocate() quit when too many.

	/*

	 * Add inode to shmem_unuse()'s list of swapped-out inodes,

	 * if it's not already there.  Do it now before the page is

	 * moved to swap cache, when its pagelock no longer protects

	 * the inode from eviction.  But don't unlock the mutex until

	 * we've incremented swapped, because shmem_unuse_inode() will

	 * prune a !swapped inode from the swaplist under this mutex.

 Return with page locked */

 show nothing */

 prevent replace/use races */

 !CONFIG_NUMA || !CONFIG_TMPFS */

 CONFIG_NUMA && CONFIG_TMPFS */

 Create a pseudo vma that just contains the policy */

 Bias interleave by inode number to distribute better across nodes */

 Drop reference taken by mpol_shared_policy_lookup() */

/*

 * Make sure huge_gfp is always more limited than limit_gfp.

 * Some of the flags set permissions, while others set limitations.

 Allow allocations only from the originally specified zones. */

	/*

	 * Minimize the result gfp by taking the union with the deny flags,

	 * and the intersection of the allow flags.

/*

 * When a page is moved from swapcache to shmem filecache (either by the

 * usual swapin of shmem_getpage_gfp(), or by the less common swapoff of

 * shmem_unuse_inode()), it may have been read in earlier from swap, in

 * ignorance of the mapping it belongs to.  If that mapping has special

 * constraints (like the gma500 GEM driver, which requires RAM below 4GB),

 * we may need to copy to a suitable page before moving to filecache.

 *

 * In a future release, this may well be extended to respect cpuset and

 * NUMA mempolicy, and applied also to anonymous pages in do_swap_page();

 * but for now it is a simple matter of zone.

	/*

	 * We have arrived here because our zones are constrained, so don't

	 * limit chance of success by further cpuset and node constraints.

	/*

	 * Our caller will very soon move newpage out of swapcache, but it's

	 * a nice clean interface for us to replace oldpage by newpage there.

		/*

		 * Is this possible?  I think not, now that our callers check

		 * both PageSwapCache and page_private after getting page lock;

		 * but be defensive.  Reverse old to newpage for clear and free.

/*

 * Swap in the page pointed to by *pagep.

 * Caller has to make sure that *pagep contains a valid swapped page.

 * Returns 0 and the page in pagep if success. On failure, returns the

 * error code and NULL in *pagep.

 Look it up and read it in.. */

 Or update major stats only when swapin succeeds?? */

 Here we actually start the io */

 We have to do this with page locked to prevent races */

	/*

	 * Some architectures may have to restore extra metadata to the

	 * physical page after reading from swap.

/*

 * shmem_getpage_gfp - find page in cache, or get from swap, or allocate

 *

 * If we allocate a new one we do not mark it dirty. That's up to the

 * vm. If we swap it in we mark it dirty since we also free the swap

 * entry since a page cannot live in both the swap and page cache.

 *

 * vma, vmf, and fault_type are only supplied by shmem_fault:

 * otherwise they are NULL.

 fallocated page */

	/*

	 * SGP_READ: succeed on hole, with NULL page, letting caller zero.

	 * SGP_NOALLOC: fail on hole, with NULL page, letting caller fail.

	/*

	 * Fast cache lookup and swap lookup did not find it: allocate.

 Never use a huge page for shmem_symlink() */

		/*

		 * Try to reclaim some space by splitting a huge page

		 * beyond i_size on the filesystem.

		/*

		 * Part of the huge page is beyond i_size: subject

		 * to shrink under memory pressure.

		/*

		 * _careful to defend against unlocked access to

		 * ->shrink_list in shmem_unused_huge_shrink()

	/*

	 * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.

	/*

	 * Let SGP_WRITE caller clear ends if write does not fill page;

	 * but SGP_FALLOC on a page fallocated earlier must initialize

	 * it now, lest undo on failure cancel our earlier guarantee.

 Perhaps the file has been truncated since we checked */

	/*

	 * Error recovery.

/*

 * This is like autoremove_wake_function, but it removes the wait queue

 * entry unconditionally - even if something else had already woken the

 * target.

	/*

	 * Trinity finds that probing a hole which tmpfs is punching can

	 * prevent the hole-punch from ever completing: which in turn

	 * locks writers out with its hold on i_rwsem.  So refrain from

	 * faulting pages into the hole while it's being punched.  Although

	 * shmem_undo_range() does remove the additions, it may be unable to

	 * keep up, as each new page needs its own unmap_mapping_range() call,

	 * and the i_mmap tree grows ever slower to scan if new vmas are added.

	 *

	 * It does not matter if we sometimes reach this check just before the

	 * hole-punch begins, so that one fault then races with the punch:

	 * we just need to make racing faults a rare case.

	 *

	 * The implementation below would be much simpler if we just used a

	 * standard mutex or completion: but we cannot take i_rwsem in fault,

	 * and bloating every shmem inode for this unlikely case would be sad.

			/*

			 * shmem_falloc_waitq points into the shmem_fallocate()

			 * stack of the hole-punching task: shmem_falloc_waitq

			 * is usually invalid by the time we reach here, but

			 * finish_wait() does not dereference it in that case;

			 * though i_lock needed lest racing with wake_up_all().

	/*

	 * Our priority is to support MAP_SHARED mapped hugely;

	 * and support MAP_PRIVATE mapped hugely too, until it is COWed.

	 * But if caller specified an address hint and we allocated area there

	 * successfully, respect that as before.

			/*

			 * Called directly from mm/mmap.c, or drivers/char/mem.c

			 * for "/dev/zero", to create a shared anonymous object.

	/*

	 * What serializes the accesses to info->flags?

	 * ipc_lock_object() when called from shmctl_do_lock(),

	 * no serialization needed when called from shm_destroy().

 arm64 - allow memory tagging on RAM-based files */

 Some things misbehave if size == 0 on a directory */

			/*

			 * Must not load anything in the rbtree,

			 * mpol_free_shared_policy will not be called.

		/*

		 * We may have got a page, returned -ENOENT triggering a retry,

		 * and now we find ourselves with -ENOMEM. Release the page, to

		 * avoid a BUG_ON in our caller.

 COPY */

 fallback to copy_from_user outside mmap_lock */

 don't free the page */

 ZEROPAGE */

 CONFIG_USERFAULTFD */

 i_rwsem is held by caller */

	/*

	 * Might this read be for a stacking filesystem?  Then when reading

	 * holes of a sparse file, we actually need to allocate those pages,

	 * and even mark them dirty, so it cannot exceed the max_blocks limit.

		/*

		 * We must evaluate after, since reads (unlike writes)

		 * are called without i_rwsem protection against truncate

			/*

			 * If users can be writing to this page using arbitrary

			 * virtual addresses, take care about potential aliasing

			 * before reading the page on the kernel side.

			/*

			 * Mark the page accessed if we read the beginning.

		/*

		 * Ok, we have the page, and it's up-to-date, so

		 * now we can copy it to user space...

 We're holding i_rwsem so we can access i_size directly */

 protected by i_rwsem */

 No need to unmap again: hole-punching leaves COWed pages */

 We need to check rlimit even when FALLOC_FL_KEEP_SIZE */

 Try to avoid a swapstorm if len is impossible to satisfy */

	/*

	 * info->fallocend is only relevant when huge pages might be

	 * involved: to prevent split_huge_page() freeing fallocated

	 * pages when FALLOC_FL_KEEP_SIZE committed beyond i_size.

		/*

		 * Good, the fallocate(2) manpage permits EINTR: we may have

		 * been interrupted because we are using up too much memory.

 Remove the !PageUptodate pages we added */

		/*

		 * Here is a more important optimization than it appears:

		 * a second SGP_FALLOC on the same huge page will clear it,

		 * making it PageUptodate and un-undoable if we fail later.

 Beware 32-bit wraparound */

		/*

		 * Inform shmem_writepage() how far we have reached.

		 * No need for lock or barrier: we have the page lock.

		/*

		 * If !PageUptodate, leave it that way so that freeable pages

		 * can be recognized if we need to rollback on error later.

		 * But set_page_dirty so that memory pressure will swap rather

		 * than free the pages we are allocating (and SGP_CACHE pages

		 * might still be clean: we now need to mark those dirty too).

 else leave those fields 0 like simple_statfs */

/*

 * File creation. Allocate an inode, and we're done..

 Extra count - pin the dentry in core */

/*

 * Link a file..

	/*

	 * No ordinary (disk based) filesystem counts links as inodes;

	 * but each new link needs a new dentry, pinning lowmem, and

	 * tmpfs dentries cannot be pruned until they are unlinked.

	 * But if an O_TMPFILE file is linked into the tmpfs, the

	 * first link must skip that, to get the accounting right.

 New dentry reference */

 Extra pinning count for the created dentry */

 Undo the count from "create" - this does all the work */

	/*

	 * Cheat and hash the whiteout while the old dentry is still in

	 * place, instead of playing games with FS_RENAME_DOES_D_MOVE.

	 *

	 * d_lookup() will consistently find one of them at this point,

	 * not sure which one, but that isn't even important.

/*

 * The VFS layer already does all the dentry stuff for rename,

 * we just have to decrement the usage count for the target if

 * it exists so that the VFS layer correctly free's it when it

 * gets overwritten.

/*

 * Superblocks without xattr inode operations may get some security.* xattr

 * support from the LSM "for free". As soon as we have any other xattrs

 * like ACLs, we also need to implement the security.* handlers at

 * filesystem level, though.

/*

 * Callback for security_inode_init_security() for acquiring xattrs.

 CONFIG_TMPFS_XATTR */

 Find any alias of inode, but prefer a hashed alias */

		/* Unfortunately insert_inode_hash is not idempotent,

		 * so as we hash inodes here rather than at creation

		 * time, we need a lock to ensure we only try

		 * to do it once

			/*

			 * NUL-terminate this option: unfortunately,

			 * mount options form a comma-separated list,

			 * but mpol's nodelist may also contain commas.

/*

 * Reconfigure a shmem filesystem.

 *

 * Note that we disallow change from limited->unlimited blocks/inodes while any

 * are in use; but we must separately disallow unlimited->limited, because in

 * that case we have no record of how much is already in use.

	/*

	 * Preserve previous mempolicy unless mpol remount option was specified.

 transfers initial ref */

	/*

	 * Showing inode{64,32} might be useful even if it's the system default,

	 * since then people don't have to resort to checking both here and

	 * /proc/config.gz to confirm 64-bit inums were successfully applied

	 * (which may not even exist if IKCONFIG_PROC isn't enabled).

	 *

	 * We hide it when inode64 isn't the default and we are using 32-bit

	 * inodes, since that probably just means the feature isn't even under

	 * consideration.

	 *

	 * As such:

	 *

	 *                     +-----------------+-----------------+

	 *                     | TMPFS_INODE64=y | TMPFS_INODE64=n |

	 *  +------------------+-----------------+-----------------+

	 *  | full_inums=true  | show            | show            |

	 *  | full_inums=false | show            | hide            |

	 *  +------------------+-----------------+-----------------+

	 *

 Rightly or wrongly, show huge mount option unmasked by shmem_huge */

 CONFIG_TMPFS */

 Round up to L1_CACHE_BYTES to resist false sharing */

	/*

	 * Per default we only allow half of the physical ram per

	 * tmpfs instance, limiting inodes to one per page of lowmem;

	 * but the internal instance is left unlimited.

 just in case it was patched */

 CONFIG_TRANSPARENT_HUGEPAGE && CONFIG_SYSFS */

 !CONFIG_SHMEM */

/*

 * tiny-shmem: simple shmemfs and tmpfs using ramfs code

 *

 * This is intended for small system where the benefits of the full

 * shmem code (swap-backed and resource-limited) are outweighed by

 * their complexity. On systems without swap this code should be

 * effectively equivalent, but much lighter weight.

 CONFIG_SHMEM */

 common code */

 It is unlinked */

/**

 * shmem_kernel_file_setup - get an unlinked file living in tmpfs which must be

 * 	kernel internal.  There will be NO LSM permission checks against the

 * 	underlying inode.  So users of this interface must do LSM checks at a

 *	higher layer.  The users are the big_key and shm implementations.  LSM

 *	checks are provided at the key or shm level rather than the inode.

 * @name: name for dentry (to be seen in /proc/<pid>/maps

 * @size: size to be set for the file

 * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size

/**

 * shmem_file_setup - get an unlinked file living in tmpfs

 * @name: name for dentry (to be seen in /proc/<pid>/maps

 * @size: size to be set for the file

 * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size

/**

 * shmem_file_setup_with_mnt - get an unlinked file living in tmpfs

 * @mnt: the tmpfs mount where the file will be created

 * @name: name for dentry (to be seen in /proc/<pid>/maps

 * @size: size to be set for the file

 * @flags: VM_NORESERVE suppresses pre-accounting of the entire object size

/**

 * shmem_zero_setup - setup a shared anonymous mapping

 * @vma: the vma to be mmapped is prepared by do_mmap

	/*

	 * Cloning a new file under mmap_lock leads to a lock ordering conflict

	 * between XFS directory reading and selinux: since this file is only

	 * accessible to the user through its mapping, use S_PRIVATE flag to

	 * bypass file security, in the same way as shmem_kernel_file_setup().

/**

 * shmem_read_mapping_page_gfp - read into page cache, using specified page allocation flags.

 * @mapping:	the page's address_space

 * @index:	the page index

 * @gfp:	the page allocator flags to use if allocating

 *

 * This behaves as a tmpfs "read_cache_page_gfp(mapping, index, gfp)",

 * with any new page allocations done using the specified allocation flags.

 * But read_cache_page_gfp() uses the ->readpage() method: which does not

 * suit tmpfs, since it may have pages in swapcache, and needs to find those

 * for itself; although drivers/gpu/drm i915 and ttm rely upon this support.

 *

 * i915_gem_object_get_pages_gtt() mixes __GFP_NORETRY | __GFP_NOWARN in

 * with the mapping_gfp_mask(), to avoid OOMing the machine unnecessarily.

	/*

	 * The tiny !SHMEM case uses ramfs without swap

 SPDX-License-Identifier: GPL-2.0-only

/*

 * z3fold.c

 *

 * Author: Vitaly Wool <vitaly.wool@konsulko.com>

 * Copyright (C) 2016, Sony Mobile Communications Inc.

 *

 * This implementation is based on zbud written by Seth Jennings.

 *

 * z3fold is an special purpose allocator for storing compressed pages. It

 * can store up to three compressed pages per page which improves the

 * compression ratio of zbud while retaining its main concepts (e. g. always

 * storing an integral number of objects per page) and simplicity.

 * It still has simple and deterministic reclaim properties that make it

 * preferable to a higher density approach (with no requirement on integral

 * number of object per page) when reclaim is used.

 *

 * As in zbud, pages are divided into "chunks".  The size of the chunks is

 * fixed at compile time and is determined by NCHUNKS_ORDER below.

 *

 * z3fold doesn't export any API and is meant to be used via zpool API.

/*

 * NCHUNKS_ORDER determines the internal allocation granularity, effectively

 * adjusting internal fragmentation.  It also determines the number of

 * freelists maintained in each pool. NCHUNKS_ORDER of 6 means that the

 * allocation granularity will be in chunks of size PAGE_SIZE/64. Some chunks

 * in the beginning of an allocated page are occupied by z3fold header, so

 * NCHUNKS will be calculated to 63 (or 62 in case CONFIG_DEBUG_SPINLOCK=y),

 * which shows the max number of free chunks in z3fold page, also there will

 * be 63, or 62, respectively, freelists per pool.

/*****************

 * Structures

	/*

	 * we are using BUDDY_MASK in handle_to_buddy etc. so there should

	 * be enough slots to hold all possible variants

 back link */

/*

 * struct z3fold_header - z3fold page metadata occupying first chunks of each

 *			z3fold page, except for HEADLESS pages

 * @buddy:		links the z3fold page into the relevant list in the

 *			pool

 * @page_lock:		per-page lock

 * @refcount:		reference count for the z3fold page

 * @work:		work_struct for page layout optimization

 * @slots:		pointer to the structure holding buddy slots

 * @pool:		pointer to the containing pool

 * @cpu:		CPU which this page "belongs" to

 * @first_chunks:	the size of the first buddy in chunks, 0 if free

 * @middle_chunks:	the size of the middle buddy in chunks, 0 if free

 * @last_chunks:	the size of the last buddy in chunks, 0 if free

 * @first_num:		the starting number (for the first handle)

 * @mapped_count:	the number of objects currently mapped

/**

 * struct z3fold_pool - stores metadata for each z3fold pool

 * @name:	pool name

 * @lock:	protects pool unbuddied/lru lists

 * @stale_lock:	protects pool stale page list

 * @unbuddied:	per-cpu array of lists tracking z3fold pages that contain 2-

 *		buddies; the list each z3fold page is added to depends on

 *		the size of its free region.

 * @lru:	list tracking the z3fold pages in LRU order by most recently

 *		added buddy.

 * @stale:	list of pages marked for freeing

 * @pages_nr:	number of z3fold pages in the pool.

 * @c_handle:	cache for z3fold_buddy_slots allocation

 * @ops:	pointer to a structure of user defined operations specified at

 *		pool creation time.

 * @zpool:	zpool driver

 * @zpool_ops:	zpool operations structure with an evict callback

 * @compact_wq:	workqueue for page layout background optimization

 * @release_wq:	workqueue for safe page release

 * @work:	work_struct for safe page release

 * @inode:	inode for z3fold pseudo filesystem

 *

 * This structure is allocated at pool creation time and maintains metadata

 * pertaining to a particular z3fold pool.

/*

 * Internal z3fold page flags

 by either reclaim or free */

/*

 * handle flags, go under HANDLE_FLAG_MASK

/*

 * Forward declarations

/*****************

 * Helpers

 Converts an allocation size in bytes to size in z3fold chunks */

 It will be freed separately in free_handle(). */

 Lock a z3fold page */

 Try to lock a z3fold page */

 Unlock a z3fold page */

 return locked z3fold page if it's not headless */

 simple case, nothing else to do */

 Initializes the z3fold header of a newly allocated z3fold page */

 Resets the struct page fields and frees the page */

 Helper function to build the index */

/*

 * Encodes the handle of a particular buddy within a z3fold page

 * Pool lock should be held as this function accesses first_num

	/*

	 * For a headless page, its handle is its pointer with the extra

	 * PAGE_HEADLESS bit set

 otherwise, return pointer to encoded handle */

 only for LAST bud, returns zero otherwise */

/*

 * (handle & BUDDY_MASK) < zhdr->first_num is possible in encode_handle

 *  but that doesn't matter. because the masking will result in the

 *  correct buddy number.

/*

 * Returns the number of free chunks in a z3fold page.

 * NB: can't be used with HEADLESS pages.

	/*

	 * If there is a middle object, pick up the bigger free space

	 * either before or after it. Otherwise just subtract the number

	 * of chunks occupied by the first and the last objects.

 Add to the appropriate unbuddied list */

	/*

	 * No need to protect slots here -- all the slots are "local" and

	 * the page lock is already taken

 Has to be called with lock held */

 can't move middle chunk, it's used */

 nothing to compact */

 move to the beginning */

	/*

	 * moving data is expensive, so let's only do that if

	 * there's substantial gain (at least BIG_CHUNK_GAP chunks)

 returns _locked_ z3fold page header or NULL */

 First, try to find an unbuddied z3fold page. */

 Re-check under lock. */

		/*

		 * this page could not be removed from its unbuddied

		 * list while pool lock was held, and then we've taken

		 * page lock so kref_put could not be called before

		 * we got here, so it's safe to just call kref_get()

 look for _exact_ match on other cpus' lists */

/*

 * API Functions

/**

 * z3fold_create_pool() - create a new z3fold pool

 * @name:	pool name

 * @gfp:	gfp flags when allocating the z3fold pool structure

 * @ops:	user-defined operations for the z3fold pool

 *

 * Return: pointer to the new z3fold pool or NULL if the metadata allocation

 * failed.

/**

 * z3fold_destroy_pool() - destroys an existing z3fold pool

 * @pool:	the z3fold pool to be destroyed

 *

 * The pool should be emptied before this function is called.

	/*

	 * We need to destroy pool->compact_wq before pool->release_wq,

	 * as any pending work on pool->compact_wq will call

	 * queue_work(pool->release_wq, &pool->work).

	 *

	 * There are still outstanding pages until both workqueues are drained,

	 * so we cannot unregister migration until then.

/**

 * z3fold_alloc() - allocates a region of a given size

 * @pool:	z3fold pool from which to allocate

 * @size:	size in bytes of the desired allocation

 * @gfp:	gfp flags used if the pool needs to grow

 * @handle:	handle of the new allocation

 *

 * This function will attempt to find a free region in the pool large enough to

 * satisfy the allocation request.  A search of the unbuddied lists is

 * performed first. If no suitable free region is found, then a new page is

 * allocated and added to the pool to satisfy the request.

 *

 * gfp should not set __GFP_HIGHMEM as highmem pages cannot be used

 * as z3fold pool pages.

 *

 * Return: 0 if success and handle is set, otherwise -EINVAL if the size or

 * gfp arguments are invalid or -ENOMEM if the pool was unable to allocate

 * a new page.

		/*

		 * Before allocating a page, let's see if we can take one from

		 * the stale pages list. cancel_work_sync() can sleep so we

		 * limit this case to the contexts where we can sleep

 Add/move z3fold page to beginning of LRU */

/**

 * z3fold_free() - frees the allocation associated with the given handle

 * @pool:	pool in which the allocation resided

 * @handle:	handle associated with the allocation returned by z3fold_alloc()

 *

 * In the case that the z3fold page in which the allocation resides is under

 * reclaim, as indicated by the PG_reclaim flag being set, this function

 * only sets the first|last_chunks to 0.  The page is actually freed

 * once both buddies are evicted (see z3fold_reclaim_page() below).

		/* if a headless page is under reclaim, just leave.

		 * NB: we use test_and_set_bit for a reason: if the bit

		 * has not been set before, we release this page

		 * immediately so we don't care about its value any more.

 Non-headless case */

 the page has not been claimed by us */

/**

 * z3fold_reclaim_page() - evicts allocations from a pool page and frees it

 * @pool:	pool from which a page will attempt to be evicted

 * @retries:	number of pages on the LRU list for which eviction will

 *		be attempted before failing

 *

 * z3fold reclaim is different from normal system reclaim in that it is done

 * from the bottom, up. This is because only the bottom layer, z3fold, has

 * information on how the allocations are organized within each z3fold page.

 * This has the potential to create interesting locking situations between

 * z3fold and the user, however.

 *

 * To avoid these, this is how z3fold_reclaim_page() should be called:

 *

 * The user detects a page should be reclaimed and calls z3fold_reclaim_page().

 * z3fold_reclaim_page() will remove a z3fold page from the pool LRU list and

 * call the user-defined eviction handler with the pool and handle as

 * arguments.

 *

 * If the handle can not be evicted, the eviction handler should return

 * non-zero. z3fold_reclaim_page() will add the z3fold page back to the

 * appropriate list and try the next z3fold page on the LRU up to

 * a user defined number of retries.

 *

 * If the handle is successfully evicted, the eviction handler should

 * return 0 _and_ should have called z3fold_free() on the handle. z3fold_free()

 * contains logic to delay freeing the page if the page is under reclaim,

 * as indicated by the setting of the PG_reclaim flag on the underlying page.

 *

 * If all buddies in the z3fold page are successfully evicted, then the

 * z3fold page can be freed.

 *

 * Returns: 0 if page is successfully freed, otherwise -EINVAL if there are

 * no pages to evict or an eviction handler is not registered, -EAGAIN if

 * the retry limit was hit.

				/*

				 * For non-headless pages, we wait to do this

				 * until we have the page lock to avoid racing

				 * with __z3fold_alloc(). Headless pages don't

				 * have a lock (and __z3fold_alloc() will never

				 * see them), but we still need to test and set

				 * PAGE_CLAIMED to avoid racing with

				 * z3fold_free(), so just do it now before

				 * leaving the loop.

 can't evict at this point */

			/* test_and_set_bit is of course atomic, but we still

			 * need to do it under page lock, otherwise checking

			 * that bit in __z3fold_alloc wouldn't make sense

 can't evict such page */

			/*

			 * We need encode the handles before unlocking, and

			 * use our local slots structure because z3fold_free

			 * can zero out zhdr->slots and we can't do much

			 * about that

			/*

			 * it's safe to unlock here because we hold a

			 * reference to this page

 Issue the eviction callback(s) */

			/*

			 * if we are here, the page is still not completely

			 * free. Take the global pool lock then to be able

			 * to add it back to the lru list

 We started off locked to we need to lock the pool back */

/**

 * z3fold_map() - maps the allocation associated with the given handle

 * @pool:	pool in which the allocation resides

 * @handle:	handle associated with the allocation to be mapped

 *

 * Extracts the buddy number from handle and constructs the pointer to the

 * correct starting chunk within the page.

 *

 * Returns: a pointer to the mapped allocation

/**

 * z3fold_unmap() - unmaps the allocation associated with the given handle

 * @pool:	pool in which the allocation resides

 * @handle:	handle associated with the allocation to be unmapped

/**

 * z3fold_get_pool_size() - gets the z3fold pool size in pages

 * @pool:	pool whose size is being queried

 *

 * Returns: size in pages of the given pool.

	/*

	 * z3fold_page_isolate() ensures that new_zhdr->buddy is empty,

	 * so we only have to reinitialize it.

/*****************

 * zpool

	/*

	 * Make sure the z3fold header is not larger than the page size and

	 * there has remaining spaces for its buddy.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DMA Pool allocator

 *

 * Copyright 2001 David Brownell

 * Copyright 2007 Intel Corporation

 *   Author: Matthew Wilcox <willy@linux.intel.com>

 *

 * This allocator returns small blocks of a given size which are DMA-able by

 * the given device.  It uses the dma_alloc_coherent page allocator to get

 * new pages, then splits them up into blocks of the required size.

 * Many older drivers still have their own code to do this.

 *

 * The current design of this allocator is fairly simple.  The pool is

 * represented by the 'struct dma_pool' which keeps a doubly-linked list of

 * allocated pages.  Each page in the page_list is split into blocks of at

 * least 'size' bytes.  Free blocks are tracked in an unsorted singly-linked

 * list of free blocks within the page.  Used blocks aren't tracked, but we

 * keep a count of how many are currently allocated from each page.

 the pool */

 cacheable header for 'allocation' bytes */

 per-pool info, no real statistics yet */

/**

 * dma_pool_create - Creates a pool of consistent memory blocks, for dma.

 * @name: name of pool, for diagnostics

 * @dev: device that will be doing the DMA

 * @size: size of the blocks in this pool.

 * @align: alignment requirement for blocks; must be a power of two

 * @boundary: returned blocks won't cross this power of two boundary

 * Context: not in_interrupt()

 *

 * Given one of these pools, dma_pool_alloc()

 * may be used to allocate memory.  Such memory will all have "consistent"

 * DMA mappings, accessible by the device and its driver without using

 * cache flushing primitives.  The actual size of blocks allocated may be

 * larger than requested because of alignment.

 *

 * If @boundary is nonzero, objects returned from dma_pool_alloc() won't

 * cross that size boundary.  This is useful for devices which have

 * addressing restrictions on individual DMA transfers, such as not crossing

 * boundaries of 4KBytes.

 *

 * Return: a dma allocation pool with the requested characteristics, or

 * %NULL if one can't be created.

	/*

	 * pools_lock ensures that the ->dma_pools list does not get corrupted.

	 * pools_reg_lock ensures that there is not a race between

	 * dma_pool_create() and dma_pool_destroy() or within dma_pool_create()

	 * when the first invocation of dma_pool_create() failed on

	 * device_create_file() and the second assumes that it has been done (I

	 * know it is a short window).

/**

 * dma_pool_destroy - destroys a pool of dma memory blocks.

 * @pool: dma pool that will be destroyed

 * Context: !in_interrupt()

 *

 * Caller guarantees that no more memory from the pool is in use,

 * and that nothing will try to use the pool after this call.

 leak the still-in-use consistent memory */

/**

 * dma_pool_alloc - get a block of consistent memory

 * @pool: dma pool that will produce the block

 * @mem_flags: GFP_* bitmask

 * @handle: pointer to dma address of block

 *

 * Return: the kernel virtual address of a currently unused block,

 * and reports its dma address through the handle.

 * If such a memory block can't be allocated, %NULL is returned.

 pool_alloc_page() might sleep, so temporarily drop &pool->lock */

 page->offset is stored in first 4 bytes */

			/*

			 * Dump the first 4 bytes even if they are not

			 * POOL_POISON_FREED

/**

 * dma_pool_free - put block back into dma pool

 * @pool: the dma pool holding the block

 * @vaddr: virtual address of block

 * @dma: dma address of block

 *

 * Caller promises neither device nor driver will again touch this block

 * unless it is first re-allocated.

	/*

	 * Resist a temptation to do

	 *    if (!is_page_busy(page)) pool_free_page(pool, page);

	 * Better have a few empty pages hang around.

/*

 * Managed DMA pool

/**

 * dmam_pool_create - Managed dma_pool_create()

 * @name: name of pool, for diagnostics

 * @dev: device that will be doing the DMA

 * @size: size of the blocks in this pool.

 * @align: alignment requirement for blocks; must be a power of two

 * @allocation: returned blocks won't cross this boundary (or zero)

 *

 * Managed dma_pool_create().  DMA pool created with this function is

 * automatically destroyed on driver detach.

 *

 * Return: a managed dma allocation pool with the requested

 * characteristics, or %NULL if one can't be created.

/**

 * dmam_pool_destroy - Managed dma_pool_destroy()

 * @pool: dma pool that will be destroyed

 *

 * Managed dma_pool_destroy().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/balloon_compaction.c

 *

 * Common interface for making balloon pages movable by compaction.

 *

 * Copyright (C) 2012, Red Hat, Inc.  Rafael Aquini <aquini@redhat.com>

	/*

	 * Block others from accessing the 'page' when we get around to

	 * establishing additional references. We should be the only one

	 * holding a reference to the 'page' at this point. If we are not, then

	 * memory corruption is possible and we should stop execution.

/**

 * balloon_page_list_enqueue() - inserts a list of pages into the balloon page

 *				 list.

 * @b_dev_info: balloon device descriptor where we will insert a new page to

 * @pages: pages to enqueue - allocated using balloon_page_alloc.

 *

 * Driver must call this function to properly enqueue balloon pages before

 * definitively removing them from the guest system.

 *

 * Return: number of pages that were enqueued.

/**

 * balloon_page_list_dequeue() - removes pages from balloon's page list and

 *				 returns a list of the pages.

 * @b_dev_info: balloon device descriptor where we will grab a page from.

 * @pages: pointer to the list of pages that would be returned to the caller.

 * @n_req_pages: number of requested pages.

 *

 * Driver must call this function to properly de-allocate a previous enlisted

 * balloon pages before definitively releasing it back to the guest system.

 * This function tries to remove @n_req_pages from the ballooned pages and

 * return them to the caller in the @pages list.

 *

 * Note that this function may fail to dequeue some pages even if the balloon

 * isn't empty - since the page list can be temporarily empty due to compaction

 * of isolated pages.

 *

 * Return: number of pages that were added to the @pages list.

		/*

		 * Block others from accessing the 'page' while we get around to

		 * establishing additional references and preparing the 'page'

		 * to be released by the balloon driver.

 raced with isolation */

/*

 * balloon_page_alloc - allocates a new page for insertion into the balloon

 *			page list.

 *

 * Driver must call this function to properly allocate a new balloon page.

 * Driver must call balloon_page_enqueue before definitively removing the page

 * from the guest system.

 *

 * Return: struct page for the allocated page or NULL on allocation failure.

/*

 * balloon_page_enqueue - inserts a new page into the balloon page list.

 *

 * @b_dev_info: balloon device descriptor where we will insert a new page

 * @page: new page to enqueue - allocated using balloon_page_alloc.

 *

 * Drivers must call this function to properly enqueue a new allocated balloon

 * page before definitively removing the page from the guest system.

 *

 * Drivers must not call balloon_page_enqueue on pages that have been pushed to

 * a list with balloon_page_push before removing them with balloon_page_pop. To

 * enqueue a list of pages, use balloon_page_list_enqueue instead.

/*

 * balloon_page_dequeue - removes a page from balloon's page list and returns

 *			  its address to allow the driver to release the page.

 * @b_dev_info: balloon device descriptor where we will grab a page from.

 *

 * Driver must call this function to properly dequeue a previously enqueued page

 * before definitively releasing it back to the guest system.

 *

 * Caller must perform its own accounting to ensure that this

 * function is called only if some pages are actually enqueued.

 *

 * Note that this function may fail to dequeue some pages even if there are

 * some enqueued pages - since the page list can be temporarily empty due to

 * the compaction of isolated pages.

 *

 * TODO: remove the caller accounting requirements, and allow caller to wait

 * until all pages can be dequeued.

 *

 * Return: struct page for the dequeued page, or NULL if no page was dequeued.

		/*

		 * If we are unable to dequeue a balloon page because the page

		 * list is empty and there are no isolated pages, then something

		 * went out of track and some balloon pages are lost.

		 * BUG() here, otherwise the balloon driver may get stuck in

		 * an infinite loop while attempting to release all its pages.

 move_to_new_page() counterpart for a ballooned page */

	/*

	 * We can not easily support the no copy case here so ignore it as it

	 * is unlikely to be used with balloon pages. See include/linux/hmm.h

	 * for a user of the MIGRATE_SYNC_NO_COPY mode.

 CONFIG_BALLOON_COMPACTION */

 SPDX-License-Identifier: GPL-2.0

/*

 * High memory handling common code and variables.

 *

 * (C) 1999 Andrea Arcangeli, SuSE GmbH, andrea@suse.de

 *          Gerhard Wichert, Siemens AG, Gerhard.Wichert@pdb.siemens.de

 *

 *

 * Redesigned the x86 32-bit VM architecture to deal with

 * 64-bit physical space. With current x86 CPUs this

 * means up to 64 Gigabytes physical RAM.

 *

 * Rewrote high memory support to move the page cache into

 * high memory. Implemented permanent (schedulable) kmaps

 * based on Linus' idea.

 *

 * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>

/*

 * Virtual_count is not a pure "count".

 *  0 means that it is not mapped, and has not been mapped

 *    since a TLB flush - it is usable.

 *  1 means that there are no users, but it has been mapped

 *    since the last TLB flush - so we can't use it.

 *  n means that there are (n-1) current users of it.

/*

 * Architecture with aliasing data cache may define the following family of

 * helper functions in its asm/highmem.h to control cache color of virtual

 * addresses where physical memory pages are mapped by kmap.

/*

 * Determine color of virtual address where the page should be mapped.

/*

 * Get next index for mapping inside PKMAP region for page with given color.

/*

 * Determine if page index inside PKMAP region (pkmap_nr) of given color

 * has wrapped around PKMAP region end. When this happens an attempt to

 * flush all unused PKMAP slots is made.

/*

 * Get the number of PKMAP entries of the given color. If no free slot is

 * found after checking that many entries, kmap will sleep waiting for

 * someone to call kunmap and free PKMAP slot.

/*

 * Get head of a wait queue for PKMAP entries of the given color.

 * Wait queues for different mapping colors should be independent to avoid

 * unnecessary wakeups caused by freeing of slots of other colors.

/*

 * Most architectures have no use for kmap_high_get(), so let's abstract

 * the disabling of IRQ out of the locking in that case to save on a

 * potential useless overhead.

		/*

		 * zero means we don't have anything to do,

		 * >1 means that it is still in use. Only

		 * a count of 1 means that it is free but

		 * needs to be unmapped

 sanity check */

		/*

		 * Don't need an atomic fetch-and-clear op here;

		 * no-one has the page mapped, and cannot get at

		 * its virtual address (and hence PTE) without first

		 * getting the kmap_lock (which is held here).

		 * So no dangers, even with speculative execution.

 Find an empty entry */

 Found a usable entry */

		/*

		 * Sleep for somebody else to unmap their entries

 Somebody else might have mapped it while we slept */

 Re-start */

/**

 * kmap_high - map a highmem page into memory

 * @page: &struct page to map

 *

 * Returns the page's virtual memory address.

 *

 * We cannot call this from interrupts, as it may block.

	/*

	 * For highmem pages, we can't trust "virtual" until

	 * after we have the lock.

/**

 * kmap_high_get - pin a highmem page into memory

 * @page: &struct page to pin

 *

 * Returns the page's current virtual memory address, or NULL if no mapping

 * exists.  If and only if a non null address is returned then a

 * matching call to kunmap_high() is necessary.

 *

 * This can be called from any context.

/**

 * kunmap_high - unmap a highmem page into memory

 * @page: &struct page to unmap

 *

 * If ARCH_NEEDS_KMAP_HIGH_GET is not defined then this may be called

 * only from user context.

	/*

	 * A count must never go down to zero

	 * without a TLB flush!

		/*

		 * Avoid an unnecessary wake_up() function call.

		 * The common case is pkmap_count[] == 1, but

		 * no waiters.

		 * The tasks queued in the wait-queue are guarded

		 * by both the lock in the wait-queue-head and by

		 * the kmap_lock.  As the kmap_lock is held here,

		 * no need for the wait-queue-head's lock.  Simply

		 * test if the queue is empty.

 do wake-up, if needed, race-free outside of the spin lock */

 CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_HIGHMEM */

/*

 * With DEBUG_KMAP_LOCAL the stack depth is doubled and every second

 * slot is unused which acts as a guard page

 Unmap a local mapping which was obtained by kmap_high_get() */

		/*

		 * Set by the arch if __kmap_pte[-idx] does not produce

		 * the correct entry.

	/*

	 * Disable migration so resulting virtual address is stable

	 * across preemption.

	/*

	 * To broaden the usage of the actual kmap_local() machinery always map

	 * pages when debugging is enabled and the architecture has no problems

	 * with alias mappings.

 Try kmap_high_get() if architecture has it enabled */

 This _should_ never happen! See above. */

		/*

		 * Handle mappings which were obtained by kmap_high_get()

		 * first as the virtual address of such mappings is below

		 * PAGE_OFFSET. Warn for all other addresses which are in

		 * the user space part of the virtual address space.

/*

 * Invoked before switch_to(). This is safe even when during or after

 * clearing the maps an interrupt which needs a kmap_local happens because

 * the task::kmap_ctrl.idx is not modified by the unmapping code so a

 * nested kmap_local will use the next unused index and restore the index

 * on unmap. The already cleared kmaps of the outgoing task are irrelevant

 * because the interrupt context does not know about them. The same applies

 * when scheduling back in for an interrupt which happens before the

 * restore is complete.

 Clear kmaps */

 With debug all even slots are unmapped and act as guard */

		/*

		 * This is a horrible hack for XTENSA to calculate the

		 * coloured PTE index. Uses the PFN encoded into the pteval

		 * and the map index calculation because the actual mapped

		 * virtual address is not stored in task::kmap_ctrl.

		 * For any sane architecture this is optimized out.

 Restore kmaps */

 With debug all even slots are unmapped and act as guard */

 See comment in __kmap_local_sched_out() */

/*

 * Describes one page->virtual association

/*

 * Hash table bucket

 List of page_address_maps */

 Protect this bucket's list */

/**

 * page_address - get the mapped virtual address of a page

 * @page: &struct page to get the virtual address of

 *

 * Returns the page's virtual address.

/**

 * set_page_address - set a page's virtual address

 * @page: &struct page to set

 * @virtual: virtual address to use

 Add */

 Remove */

 defined(HASHED_PAGE_VIRTUAL) */

 SPDX-License-Identifier: GPL-2.0

/*

 * This is an optimization for KASAN=y case. Since all kasan page tables

 * eventually point to the kasan_early_shadow_page we could call note_page()

 * right away without walking through lower level page tables. This saves

 * us dozens of seconds (minutes for 5-level config) while checking for

 * W+X mapping or reading kernel_page_tables debugfs file.

 Flush out the last page */

 SPDX-License-Identifier: GPL-2.0

/*

 *	linux/mm/mlock.c

 *

 *  (C) Copyright 1995 Linus Torvalds

 *  (C) Copyright 2002 Christoph Hellwig

/*

 * Mlocked pages are marked with PageMlocked() flag for efficient testing

 * in vmscan and, possibly, the fault path; and to support semi-accurate

 * statistics.

 *

 * An mlocked page [PageMlocked(page)] is unevictable.  As such, it will

 * be placed on the LRU "unevictable" list, rather than the [in]active lists.

 * The unevictable list is an LRU sibling list to the [in]active lists.

 * PageUnevictable is set to indicate the unevictable state.

 *

 * When lazy mlocking via vmscan, it is important to ensure that the

 * vma's VM_LOCKED status is not concurrently being modified, otherwise we

 * may have mlocked a page that is being munlocked. So lazy mlock must take

 * the mmap_lock for read, and verify that the vma really is locked

 * (see mm/rmap.c).

/*

 *  LRU accounting for clear_page_mlock()

	/*

	 * The previous TestClearPageMlocked() corresponds to the smp_mb()

	 * in __pagevec_lru_add_fn().

	 *

	 * See __pagevec_lru_add_fn for more explanation.

		/*

		 * We lost the race. the page already moved to evictable list.

/*

 * Mark page as mlocked if not already.

 * If page on LRU, isolate and putback to move to unevictable list.

 Serialize with page migration */

/*

 * Finish munlock after successful page isolation

 *

 * Page must be locked. This is a wrapper for page_mlock()

 * and putback_lru_page() with munlock accounting.

	/*

	 * Optimization: if the page was mapped just once, that's our mapping

	 * and we don't need to check all the other vmas.

 Did try_to_unlock() succeed or punt? */

/*

 * Accounting for page isolation fail during munlock

 *

 * Performs accounting when page isolation fails in munlock. There is nothing

 * else to do because it means some other task has already removed the page

 * from the LRU. putback_lru_page() will take care of removing the page from

 * the unevictable list, if necessary. vmscan [page_referenced()] will move

 * the page back to the unevictable list if some other vma has it mlocked.

/**

 * munlock_vma_page - munlock a vma page

 * @page: page to be unlocked, either a normal page or THP page head

 *

 * returns the size of the page as a page mask (0 for normal page,

 *         HPAGE_PMD_NR - 1 for THP head page)

 *

 * called from munlock()/munmap() path with page supposedly on the LRU.

 * When we munlock a page, because the vma where we found the page is being

 * munlock()ed or munmap()ed, we want to check whether other vmas hold the

 * page locked so that we can leave it on the unevictable lru list and not

 * bother vmscan with it.  However, to walk the page's rmap list in

 * page_mlock() we must isolate the page from the LRU.  If some other

 * task has removed the page from the LRU, we won't be able to do that.

 * So we clear the PageMlocked as we might not get another chance.  If we

 * can't isolate the page, we leave it for putback_lru_page() and vmscan

 * [page_referenced()/try_to_unmap()] to deal with.

 For page_mlock() and to serialize with page migration */

 Potentially, PTE-mapped THP: do not skip the rest PTEs */

/*

 * convert get_user_pages() return value to posix mlock() error

/*

 * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()

 *

 * The fast path is available only for evictable pages with single mapping.

 * Then we can bypass the per-cpu pvec and get better performance.

 * when mapcount > 1 we need page_mlock() which can fail.

 * when !page_evictable(), we need the full redo logic of putback_lru_page to

 * avoid leaving evictable page in unevictable list.

 *

 * In case of success, @page is added to @pvec and @pgrescued is incremented

 * in case that the page was previously unevictable. @page is also unlocked.

/*

 * Putback multiple evictable pages to the LRU

 *

 * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of

 * the pages might have meanwhile become unevictable but that is OK.

	/*

	 *__pagevec_lru_add() calls release_pages() so we don't call

	 * put_page() explicitly

/*

 * Munlock a batch of pages from the same zone

 *

 * The work is split to two main phases. First phase clears the Mlocked flag

 * and attempts to isolate the pages, all under a single zone lru lock.

 * The second phase finishes the munlock only for pages where isolation

 * succeeded.

 *

 * Note that the pagevec may be modified during the process.

 Phase 1: page isolation */

			/*

			 * We already have pin from follow_page_mask()

			 * so we can spare the get_page() here.

		/*

		 * We won't be munlocking this page in the next phase

		 * but we still need to release the follow_page_mask()

		 * pin. We cannot do it under lru_lock however. If it's

		 * the last pin, __page_cache_release() would deadlock.

 Now we can release pins of pages that we are not munlocking */

 Phase 2: page munlock */

				/*

				 * Slow path. We don't want to lose the last

				 * pin before unlock_page()

 for putback_lru_page() */

 from follow_page_mask() */

	/*

	 * Phase 3: page putback for pages that qualified for the fast path

	 * This will also call put_page() to return pin from follow_page_mask()

/*

 * Fill up pagevec for __munlock_pagevec using pte walk

 *

 * The function expects that the struct page corresponding to @start address is

 * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.

 *

 * The rest of @pvec is filled by subsequent pages within the same pmd and same

 * zone, as long as the pte's are present and vm_normal_page() succeeds. These

 * pages also get pinned.

 *

 * Returns the address of the next page that should be scanned. This equals

 * @start + PAGE_SIZE when no page could be added by the pte walk.

	/*

	 * Initialize pte walk starting at the already pinned page where we

	 * are sure that there is a pte, as it was pinned under the same

	 * mmap_lock write op.

 Make sure we do not cross the page table boundary */

 The page next to the pinned page is the first we will try to get */

		/*

		 * Break if page could not be obtained or the page's node+zone does not

		 * match

		/*

		 * Do not use pagevec for PTE-mapped THP,

		 * munlock_vma_pages_range() will handle them.

		/*

		 * Increase the address that will be returned *before* the

		 * eventual break due to pvec becoming full by adding the page

/*

 * munlock_vma_pages_range() - munlock all pages in the vma range.'

 * @vma - vma containing range to be munlock()ed.

 * @start - start address in @vma of the range

 * @end - end of range in @vma.

 *

 *  For mremap(), munmap() and exit().

 *

 * Called with @vma VM_LOCKED.

 *

 * Returns with VM_LOCKED cleared.  Callers must be prepared to

 * deal with this.

 *

 * We don't save and restore VM_LOCKED here because pages are

 * still on lru.  In unmap path, pages might be scanned by reclaim

 * and re-mlocked by page_mlock/try_to_unmap before we unmap and

 * free them.  This will result in freeing mlocked pages.

		/*

		 * Although FOLL_DUMP is intended for get_dump_page(),

		 * it just so happens that its special treatment of the

		 * ZERO_PAGE (returning an error instead of doing get_page)

		 * suits munlock very well (and if somehow an abnormal page

		 * has sneaked into the range, we won't oops here: great).

 follow_page_mask() */

				/*

				 * Any THP page found by follow_page_mask() may

				 * have gotten split before reaching

				 * munlock_vma_page(), so we need to compute

				 * the page_mask here instead.

 follow_page_mask() */

				/*

				 * Non-huge pages are handled in batches via

				 * pagevec. The pin from follow_page_mask()

				 * prevents them from collapsing by THP.

				/*

				 * Try to fill the rest of pagevec using fast

				 * pte walk. This will also update start to

				 * the next page to process. Then munlock the

				 * pagevec.

/*

 * mlock_fixup  - handle mlock[all]/munlock[all] requests.

 *

 * Filters out "special" vmas -- VM_LOCKED never gets set for these, and

 * munlock is a no-op.  However, for some special vmas, we go ahead and

 * populate the ptes.

 *

 * For vmas that pass the filters, merge/split as appropriate.

 don't set VM_LOCKED or VM_LOCKONFAULT and don't count */

	/*

	 * Keep track of amount of locked VM.

	/*

	 * vm_flags is protected by the mmap_lock held in write mode.

	 * It's okay if try_to_unmap_one unmaps a page just after we

	 * set VM_LOCKED, populate_vma_page_range will bring it back.

 Here we know that  vma->vm_start <= nstart < vma->vm_end. */

/*

 * Go through vma areas and sum size of mlocked

 * vma pages, as return value.

 * Note deferred memory locking case(mlock2(,,MLOCK_ONFAULT)

 * is also counted.

 * Return value: previously mlocked page counts

		/*

		 * It is possible that the regions requested intersect with

		 * previously mlocked areas, that part area in "mm->locked_vm"

		 * should not be counted to new mlock increment count. So check

		 * and adjust locked count if necessary.

 check against resource limits */

/*

 * Take the MCL_* flags passed into mlockall (or 0 if called from munlockall)

 * and translate into the appropriate modifications to mm->def_flags and/or the

 * flags for all current VMAs.

 *

 * There are a couple of subtleties with this.  If mlockall() is called multiple

 * times with different flags, the values do not necessarily stack.  If mlockall

 * is called once including the MCL_FUTURE flag and then a second time without

 * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm->def_flags.

 Ignore errors */

/*

 * Objects with different lifetime than processes (SHM_LOCK and SHM_HUGETLB

 * shm segments) get accounted against the user_struct instead.

 SPDX-License-Identifier: GPL-2.0

 No fault-injection for bootstrap cache */

 CONFIG_FAULT_INJECTION_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Idle page tracking only considers user memory pages, for other types of

 * pages the idle flag is always unset and an attempt to set it is silently

 * ignored.

 *

 * We treat a page as a user memory page if it is on an LRU list, because it is

 * always safe to pass such a page to rmap_walk(), which is essential for idle

 * page tracking. With such an indicator of user pages we can skip isolated

 * pages, but since there are not usually many of them, it will hardly affect

 * the overall result.

 *

 * This function tries to get a user memory page by pfn as described above.

			/*

			 * For PTE-mapped THP, one sub page is referenced,

			 * the whole THP is referenced.

 unexpected pmd-mapped page? */

		/*

		 * We cleared the referenced bit in a mapping to this page. To

		 * avoid interference with page reclaim, mark it young so that

		 * page_referenced() will return > 0.

	/*

	 * Since rwc.arg is unused, rwc is effectively immutable, so we

	 * can make it static const to save some cycles and stack.

				/*

				 * The page might have been referenced via a

				 * pte, in which case it is not idle. Clear

				 * refs and recheck.

/*

 *

 * Copyright IBM Corporation, 2012

 * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>

 *

 * Cgroup v2

 * Copyright (C) 2019 Red Hat, Inc.

 * Author: Giuseppe Scrivano <gscrivan@redhat.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2.1 of the GNU Lesser General Public License

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it would be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

 *

/*

 * Should be called with hugetlb_lock held.

 * Since we are holding hugetlb_lock, pages cannot get moved from

 * active list or uncharged from the cgroup, So no need to get

 * page reference and test for page active here. This function

 * cannot fail.

	/*

	 * We can have pages in active list without any cgroup

	 * ie, hugepage with less than 3 pages. We can safely

	 * ignore those pages.

 root has no limit */

 Take the pages off the local counter */

/*

 * Force the hugetlb cgroup to empty the hugetlb resources by moving them to

 * the parent cgroup.

	/*

	 * We don't charge any cgroup if the compound page have less

	 * than 3 pages.

	/* Reservations take a reference to the css because they do not get

	 * reparented.

 Should be called with hugetlb_lock held */

/*

 * Should be called with hugetlb_lock held

		/*

		 * Only do css_put(rg->css) when we delete the entire region

		 * because one file_region must hold exactly one css reference.

 Can't set limit on root */

 format the size */

 Add the limit file */

 Add the reservation limit file */

 Add the current usage file */

 Add the current reservation usage file */

 Add the events file */

 Add the events.local file */

 NULL terminate the last cft */

 format the size */

 Add the limit file */

 Add the reservation limit file */

 Add the usage file */

 Add the reservation usage file */

 Add the MAX usage file */

 Add the MAX reservation usage file */

 Add the failcntfile */

 Add the reservation failcntfile */

 NULL terminate the last cft */

		/*

		 * Add cgroup control files only if the huge page consists

		 * of more than two normal pages. This is because we use

		 * page[2].private for storing cgroup details.

/*

 * hugetlb_lock will make sure a parallel cgroup rmdir won't happen

 * when we migrate hugepages

 move the h_cg details to new cgroup */

 terminate */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This kernel test validates architecture page table helpers and

 * accessors and helps in verifying their continued compliance with

 * expected generic MM semantics.

 *

 * Copyright (C) 2019 ARM Ltd.

 *

 * Author: Anshuman Khandual <anshuman.khandual@arm.com>

/*

 * Please refer Documentation/vm/arch_pgtable_helpers.rst for the semantics

 * expectations that are being validated here. All future changes in here

 * or the documentation need to be in sync.

/*

 * On s390 platform, the lower 4 bits are used to identify given page table

 * entry type. But these bits might affect the ability to clear entries with

 * pxx_clear() because of how dynamic page table folding works on s390. So

 * while loading up the entries do not change the lower 4 bits. It does not

 * have affect any other platform. Also avoid the 62nd bit on ppc64 that is

 * used to mark a pte entry.

	/*

	 * This test needs to be executed after the given page table entry

	 * is created with pfn_pte() to make sure that protection_map[idx]

	 * does not have the dirty bit enabled from the beginning. This is

	 * important for platforms like arm64 where (!PTE_RDONLY) indicate

	 * dirty bit being set.

	/*

	 * Architectures optimize set_pte_at by avoiding TLB flush.

	 * This requires set_pte_at to be not used to update an

	 * existing pte entry. Clear pte before we do set_pte_at

	 *

	 * flush_dcache_page() is called after set_pte_at() to clear

	 * PG_arch_1 for the page on ARM64. The page flag isn't cleared

	 * when it's released and page allocation check will fail when

	 * the page is allocated again. For architectures other than ARM64,

	 * the unexpected overhead of cache flushing is acceptable.

	/*

	 * This test needs to be executed after the given page table entry

	 * is created with pfn_pmd() to make sure that protection_map[idx]

	 * does not have the dirty bit enabled from the beginning. This is

	 * important for platforms like arm64 where (!PTE_RDONLY) indicate

	 * dirty bit being set.

	/*

	 * A huge page does not point to next level page table

	 * entry. Hence this must qualify as pmd_bad().

	/*

	 * flush_dcache_page() is called after set_pmd_at() to clear

	 * PG_arch_1 for the page on ARM64. The page flag isn't cleared

	 * when it's released and page allocation check will fail when

	 * the page is allocated again. For architectures other than ARM64,

	 * the unexpected overhead of cache flushing is acceptable.

 Align the address wrt HPAGE_PMD_SIZE */

  Clear the pte entries  */

	/*

	 * PMD based THP is a leaf entry.

	/*

	 * This test needs to be executed after the given page table entry

	 * is created with pfn_pud() to make sure that protection_map[idx]

	 * does not have the dirty bit enabled from the beginning. This is

	 * important for platforms like arm64 where (!PTE_RDONLY) indicate

	 * dirty bit being set.

	/*

	 * A huge page does not point to next level page table

	 * entry. Hence this must qualify as pud_bad().

	/*

	 * flush_dcache_page() is called after set_pud_at() to clear

	 * PG_arch_1 for the page on ARM64. The page flag isn't cleared

	 * when it's released and page allocation check will fail when

	 * the page is allocated again. For architectures other than ARM64,

	 * the unexpected overhead of cache flushing is acceptable.

 Align the address wrt HPAGE_PUD_SIZE */

 __PAGETABLE_PMD_FOLDED */

 __PAGETABLE_PMD_FOLDED */

	/*

	 * PUD based THP is a leaf entry.

 !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 !CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

	/*

	 * X86 defined pmd_set_huge() verifies that the given

	 * PMD is not a populated non-leaf entry.

	/*

	 * X86 defined pud_set_huge() verifies that the given

	 * PUD is not a populated non-leaf entry.

 !CONFIG_HAVE_ARCH_HUGE_VMAP */

 CONFIG_HAVE_ARCH_HUGE_VMAP */

	/*

	 * This entry points to next level page table page.

	 * Hence this must not qualify as pud_bad().

 !__PAGETABLE_PUD_FOLDED */

 PAGETABLE_PUD_FOLDED */

	/*

	 * This entry points to next level page table page.

	 * Hence this must not qualify as p4d_bad().

	/*

	 * This entry points to next level page table page.

	 * Hence this must not qualify as pgd_bad().

 !__PAGETABLE_P4D_FOLDED */

 PAGETABLE_P4D_FOLDED */

	/*

	 * flush_dcache_page() is called after set_pte_at() to clear

	 * PG_arch_1 for the page on ARM64. The page flag isn't cleared

	 * when it's released and page allocation check will fail when

	 * the page is allocated again. For architectures other than ARM64,

	 * the unexpected overhead of cache flushing is acceptable.

	/*

	 * This entry points to next level page table page.

	 * Hence this must not qualify as pmd_bad().

 !CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

 !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_ARCH_HAS_PTE_DEVMAP */

 !CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

 !CONFIG_ARCH_ENABLE_THP_MIGRATION */

 CONFIG_ARCH_ENABLE_THP_MIGRATION */

	/*

	 * swap_migration_tests() requires a dedicated page as it needs to

	 * be locked before creating a migration entry from it. Locking the

	 * page that actually maps kernel text ('start_kernel') can be real

	 * problematic. Lets use the allocated page explicitly for this

	 * purpose.

	/*

	 * make_migration_entry() expects given page to be

	 * locked, otherwise it stumbles upon a BUG_ON().

	/*

	 * Accessing the page associated with the pfn is safe here,

	 * as it was previously derived from a real kernel symbol.

 CONFIG_ARCH_WANT_GENERAL_HUGETLB */

 !CONFIG_HUGETLB_PAGE */

 CONFIG_HUGETLB_PAGE */

	/*

	 * pmd_trans_huge() and pmd_present() must return positive after

	 * MMU invalidation with pmd_mkinvalid(). This behavior is an

	 * optimization for transparent huge page. pmd_trans_huge() must

	 * be true if pmd_page() returns a valid THP to avoid taking the

	 * pmd_lock when others walk over non transhuge pmds (i.e. there

	 * are no THP allocated). Especially when splitting a THP and

	 * removing the present bit from the pmd, pmd_trans_huge() still

	 * needs to return true. pmd_present() should be true whenever

	 * pmd_trans_huge() returns true.

 __HAVE_ARCH_PMDP_INVALIDATE */

	/*

	 * pud_mkinvalid() has been dropped for now. Enable back

	 * these tests when it comes back with a modified pud_present().

	 *

	 * WARN_ON(!pud_trans_huge(pud_mkinvalid(pud_mkhuge(pud))));

	 * WARN_ON(!pud_present(pud_mkinvalid(pud_mkhuge(pud))));

 !CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD */

 !CONFIG_TRANSPARENT_HUGEPAGE */

 CONFIG_TRANSPARENT_HUGEPAGE */

 Free (huge) page */

 Free page table entries */

 Free vma and mm struct */

	/*

	 * Initialize the debugging data.

	 *

	 * protection_map[0] (or even protection_map[8]) will help create

	 * page table entries with PROT_NONE permission as required for

	 * pxx_protnone_tests().

 Allocate mm and vma */

	/*

	 * Allocate page table entries. They will be modified in the tests.

	 * Lets save the page table entries so that they can be released

	 * when the tests are completed.

	/*

	 * PFN for mapping at PTE level is determined from a standard kernel

	 * text symbol. But pfns for higher page table levels are derived by

	 * masking lower bits of this real pfn. These derived pfns might not

	 * exist on the platform but that does not really matter as pfn_pxx()

	 * helpers will still create appropriate entries for the test. This

	 * helps avoid large memory block allocations to be used for mapping

	 * at higher page table levels in some of the tests.

	/*

	 * Allocate (huge) pages because some of the tests need to access

	 * the data in the pages. The corresponding tests will be skipped

	 * if we fail to allocate (huge) pages.

	/*

	 * Iterate over the protection_map[] to make sure that all

	 * the basic page table transformation validations just hold

	 * true irrespective of the starting protection value for a

	 * given page table entry.

	/*

	 * Both P4D and PGD level tests are very basic which do not

	 * involve creating page table entries from the protection

	 * value and the given pfn. Hence just keep them out from

	 * the above iteration for now to save some test execution

	 * time.

	/*

	 * Page table modifying tests. They need to hold

	 * proper page table lock.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * zswap.c - zswap driver file

 *

 * zswap is a backend for frontswap that takes pages that are in the process

 * of being swapped out and attempts to compress and store them in a

 * RAM-based memory pool.  This can result in a significant I/O reduction on

 * the swap device and, in the case where decompressing from RAM is faster

 * than reading from the swap device, can also improve workload performance.

 *

 * Copyright (C) 2012  Seth Jennings <sjenning@linux.vnet.ibm.com>

/*********************************

* statistics

 Total bytes used by the compressed storage */

 The number of compressed pages currently stored in zswap */

 The number of same-value filled pages currently stored in zswap */

/*

 * The statistics below are not protected from concurrent access for

 * performance reasons so they may not be a 100% accurate.  However,

 * they do provide useful information on roughly how many times a

 * certain event is occurring.

 Pool limit was hit (see zswap_max_pool_percent) */

 Pages written back when pool limit was reached */

 Store failed due to a reclaim failure after pool limit was reached */

 Compressed page was too big for the allocator to (optimally) store */

 Store failed because underlying allocator could not get memory */

 Store failed because the entry metadata could not be allocated (rare) */

 Duplicate store was encountered (rare) */

 Shrinker work queue */

 Pool limit was hit, we need to calm down */

/*********************************

* tunables

 Enable/disable zswap */

 Crypto compressor to use */

 Compressed storage zpool to use */

 The maximum percentage of memory that the compressed pool can occupy */

 The threshold for accepting new pages after the max_pool_percent was hit */

 of max pool size */

 Enable/disable handling same-value filled pages (enabled by default) */

/*********************************

* data structures

/*

 * struct zswap_entry

 *

 * This structure contains the metadata for tracking a single compressed

 * page within zswap.

 *

 * rbnode - links the entry into red-black tree for the appropriate swap type

 * offset - the swap offset for the entry.  Index into the red-black tree.

 * refcount - the number of outstanding reference to the entry. This is needed

 *            to protect against premature freeing of the entry by code

 *            concurrent calls to load, invalidate, and writeback.  The lock

 *            for the zswap_tree structure that contains the entry must

 *            be held while changing the refcount.  Since the lock must

 *            be held, there is no reason to also make refcount atomic.

 * length - the length in bytes of the compressed page data.  Needed during

 *          decompression. For a same value filled page length is 0.

 * pool - the zswap_pool the entry's data is in

 * handle - zpool allocation handle that stores the compressed page data

 * value - value of the same-value filled pages which have same content

/*

 * The tree lock in the zswap_tree struct protects a few things:

 * - the rbtree

 * - the refcount field of each entry in the tree

 RCU-protected iteration */

 protects zswap_pools list modification */

 pool counter to provide unique names to zpool */

 used by param callback function */

 fatal error during init */

 init completed, but couldn't create the initial pool */

/*********************************

* helpers and fwd declarations

/*********************************

* zswap entry functions

/*********************************

* rbtree functions

/*

 * In the case that a entry with the same offset is found, a pointer to

 * the existing entry is stored in dupentry and the function returns -EEXIST

/*

 * Carries out the common pattern of freeing and entry's zpool allocation,

 * freeing the entry itself, and decrementing the number of stored pages.

 caller must hold the tree lock */

/* caller must hold the tree lock

* remove from the tree and free it, if nobody reference the entry

 caller must hold the tree lock */

/*********************************

* per-cpu code

/*

 * If users dynamically change the zpool type and compressor at runtime, i.e.

 * zswap is running, zswap can have more than one zpool on one cpu, but they

 * are sharing dtsmem. So we need this mutex to be per-cpu.

	/*

	 * if the backend of acomp is async zip, crypto_req_done() will wakeup

	 * crypto_wait_req(); if the backend of acomp is scomp, the callback

	 * won't be called, crypto_wait_req() will return without blocking.

/*********************************

* pool functions

 type and compressor must be null-terminated */

 if we can't get it, it's about to be destroyed */

 'zswap' + 32 char (max) num + \0 */

		/* if either are unset, pool initialization failed, and we

		 * need both params to be set correctly before trying to

		 * create a pool.

 unique name for each pool specifically required by zsmalloc */

	/* being the current pool takes 1 ref; this func expects the

	 * caller to always add the new pool as the current pool

 nobody should have been able to get a kref... */

 pool is now off zswap_pools list and has no references. */

/*********************************

* param callbacks

 val must be a null-terminated string */

 no change required */

	/* if this is load-time (pre-init) param setting,

	 * don't create a pool; that's done during init.

		/* add the possibly pre-existing pool to the end of the pools

		 * list; if it's new (and empty) then it'll be removed and

		 * destroyed by the put after we drop the lock

		/* if initial pool creation failed, and this pool creation also

		 * failed, maybe both compressor and zpool params were bad.

		 * Allow changing this param, so pool creation will succeed

		 * when the other param is changed. We already verified this

		 * param is ok in the zpool_has_pool() or crypto_has_acomp()

		 * checks above.

	/* drop the ref from either the old current pool,

	 * or the new pool we failed to add

/*********************************

* writeback code

 return enum for zswap_get_swap_cache_page */

/*

 * zswap_get_swap_cache_page

 *

 * This is an adaption of read_swap_cache_async()

 *

 * This function tries to find a page with the given swap entry

 * in the swapper_space address space (the swap cache).  If the page

 * is found, it is returned in retpage.  Otherwise, a page is allocated,

 * added to the swap cache, and returned in retpage.

 *

 * If success, the swap cache page is returned in retpage

 * Returns ZSWAP_SWAPCACHE_EXIST if page was already in the swap cache

 * Returns ZSWAP_SWAPCACHE_NEW if the new page needs to be populated,

 *     the new page is added to swapcache and locked

 * Returns ZSWAP_SWAPCACHE_FAIL on error

/*

 * Attempts to free an entry by adding a page to the swap cache,

 * decompressing the entry data into the page, and issuing a

 * bio write to write the page back to the swap device.

 *

 * This can be thought of as a "resumed writeback" of the page

 * to the swap device.  We are basically resuming the same swap

 * writeback path that was intercepted with the frontswap_store()

 * in the first place.  After the page has been decompressed into

 * the swap cache, the compressed version stored by zswap can be

 * freed.

 extract swpentry from data */

 here */

 find and ref zswap entry */

 entry was invalidated */

 try to allocate swap cache page */

 no memory or invalidate happened */

 page is already in the swap cache, ignore for now */

 page is locked */

 decompress */

 page is up to date */

 move it to the tail of the inactive list after end_writeback */

 start writeback */

 drop local reference */

	/*

	* There are two possible situations for entry here:

	* (1) refcount is 1(normal case),  entry is valid and on the tree

	* (2) refcount is 0, entry is freed and not on the tree

	*     because invalidate happened during writeback

	*  search the tree and free the entry if find entry

	/*

	* if we get here due to ZSWAP_SWAPCACHE_EXIST

	* a load may be happening concurrently.

	* it is safe and okay to not free the entry.

	* if we free the entry in the following put

	* it is also okay to return !0

/*********************************

* frontswap hooks

 attempts to compress and store an single page */

 THP isn't supported */

 reclaim space if needed */

 allocate entry */

 if entry is successfully added, it keeps the reference */

 compress */

 zswap_dstmem is of size (PAGE_SIZE * 2). Reflect same in sg_list */

	/*

	 * it maybe looks a little bit silly that we send an asynchronous request,

	 * then wait for its completion synchronously. This makes the process look

	 * synchronous in fact.

	 * Theoretically, acomp supports users send multiple acomp requests in one

	 * acomp instance, then get those requests done simultaneously. but in this

	 * case, frontswap actually does store and load page by page, there is no

	 * existing method to send the second page before the first page is done

	 * in one thread doing frontswap.

	 * but in different threads running on different cpu, we have different

	 * acomp instance, so multiple threads can do (de)compression in parallel.

 store */

 populate entry */

 map */

 remove from rbtree */

 update stats */

/*

 * returns 0 if the page was successfully decompressed

 * return -1 on entry not found or error

 find */

 entry was written back */

 decompress */

 frees an entry in zswap */

 find */

 entry was written back */

 remove from rbtree */

 drop the initial reference from entry creation */

 frees all zswap entries for the given swap type */

 walk the tree and free everything */

/*********************************

* debugfs functions

/*********************************

* module init and exit

 if built-in, we aren't unloaded on failure; don't allow use */

 must be late so crypto has time to come up */

 SPDX-License-Identifier: GPL-2.0

 depends on mm.h include */

/*

 * SwapCgroup implements "lookup" and "exchange" operations.

 * In typical usage, this swap_cgroup is accessed via memcg's charge/uncharge

 * against SwapCache. At swap_free(), this is accessed directly from swap.

 *

 * This means,

 *  - we have no race in "exchange" when we're accessed via SwapCache because

 *    SwapCache(and its swp_entry) is under lock.

 *  - When called via swap_free(), there is no user of this entry and no race.

 * Then, we don't need lock around "exchange".

 *

 * TODO: we can push these buffers out to HIGHMEM.

/*

 * allocate buffer for swap_cgroup.

/**

 * swap_cgroup_cmpxchg - cmpxchg mem_cgroup's id for this swp_entry.

 * @ent: swap entry to be cmpxchged

 * @old: old id

 * @new: new id

 *

 * Returns old id at success, 0 at failure.

 * (There is no mem_cgroup using 0 as its id)

/**

 * swap_cgroup_record - record mem_cgroup for a set of swap entries

 * @ent: the first swap entry to be recorded into

 * @id: mem_cgroup to be recorded

 * @nr_ents: number of swap entries to be recorded

 *

 * Returns old value at success, 0 at failure.

 * (Of course, old value can be 0.)

/**

 * lookup_swap_cgroup_id - lookup mem_cgroup id tied to swap entry

 * @ent: swap entry to be looked up.

 *

 * Returns ID of mem_cgroup at success. 0 at failure. (0 is invalid ID)

 memory shortage */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Linux VM pressure

 *

 * Copyright 2012 Linaro Ltd.

 *		  Anton Vorontsov <anton.vorontsov@linaro.org>

 *

 * Based on ideas from Andrew Morton, David Rientjes, KOSAKI Motohiro,

 * Leonid Moiseichuk, Mel Gorman, Minchan Kim and Pekka Enberg.

/*

 * The window size (vmpressure_win) is the number of scanned pages before

 * we try to analyze scanned/reclaimed ratio. So the window is used as a

 * rate-limit tunable for the "low" level notification, and also for

 * averaging the ratio for medium/critical levels. Using small window

 * sizes can cause lot of false positives, but too big window size will

 * delay the notifications.

 *

 * As the vmscan reclaimer logic works with chunks which are multiple of

 * SWAP_CLUSTER_MAX, it makes sense to use it for the window size as well.

 *

 * TODO: Make the window size depend on machine size, as we do for vmstat

 * thresholds. Currently we set it to 512 pages (2MB for 4KB pages).

/*

 * These thresholds are used when we account memory pressure through

 * scanned/reclaimed ratio. The current values were chosen empirically. In

 * essence, they are percents: the higher the value, the more number

 * unsuccessful reclaims there were.

/*

 * When there are too little pages left to scan, vmpressure() may miss the

 * critical pressure as number of pages will be less than "window size".

 * However, in that case the vmscan priority will raise fast as the

 * reclaimer will try to scan LRUs more deeply.

 *

 * The vmscan logic considers these special priorities:

 *

 * prio == DEF_PRIORITY (12): reclaimer starts with that value

 * prio <= DEF_PRIORITY - 2 : kswapd becomes somewhat overwhelmed

 * prio == 0                : close to OOM, kernel scans every page in an lru

 *

 * Any value in this range is acceptable for this tunable (i.e. from 12 to

 * 0). Current value for the vmpressure_level_critical_prio is chosen

 * empirically, but the number, in essence, means that we consider

 * critical level when scanning depth is ~10% of the lru size (vmscan

 * scans 'lru_size >> prio' pages, so it is actually 12.5%, or one

 * eights).

	/*

	 * reclaimed can be greater than scanned for things such as reclaimed

	 * slab pages. shrink_node() just adds reclaimed pages without a

	 * related increment to scanned pages.

	/*

	 * We calculate the ratio (in percents) of how many pages were

	 * scanned vs. reclaimed in a given time frame (window). Note that

	 * time is in VM reclaimer's "ticks", i.e. number of pages

	 * scanned. This makes it possible to set desired reaction time

	 * and serves as a ratelimit.

	/*

	 * Several contexts might be calling vmpressure(), so it is

	 * possible that the work was rescheduled again before the old

	 * work context cleared the counters. In that case we will run

	 * just after the old work returns, but then scanned might be zero

	 * here. No need for any locks here since we don't care if

	 * vmpr->reclaimed is in sync.

/**

 * vmpressure() - Account memory pressure through scanned/reclaimed ratio

 * @gfp:	reclaimer's gfp mask

 * @memcg:	cgroup memory controller handle

 * @tree:	legacy subtree mode

 * @scanned:	number of pages scanned

 * @reclaimed:	number of pages reclaimed

 *

 * This function should be called from the vmscan reclaim path to account

 * "instantaneous" memory pressure (scanned/reclaimed ratio). The raw

 * pressure index is then further refined and averaged over time.

 *

 * If @tree is set, vmpressure is in traditional userspace reporting

 * mode: @memcg is considered the pressure root and userspace is

 * notified of the entire subtree's reclaim efficiency.

 *

 * If @tree is not set, reclaim efficiency is recorded for @memcg, and

 * only in-kernel users are notified.

 *

 * This function does not return any value.

	/*

	 * Here we only want to account pressure that userland is able to

	 * help us with. For example, suppose that DMA zone is under

	 * pressure; if we notify userland about that kind of pressure,

	 * then it will be mostly a waste as it will trigger unnecessary

	 * freeing of memory by userland (since userland is more likely to

	 * have HIGHMEM/MOVABLE pages instead of the DMA fallback). That

	 * is why we include only movable, highmem and FS/IO pages.

	 * Indirect reclaim (kswapd) sets sc->gfp_mask to GFP_KERNEL, so

	 * we account it too.

	/*

	 * If we got here with no pages scanned, then that is an indicator

	 * that reclaimer was unable to find any shrinkable LRUs at the

	 * current scanning depth. But it does not mean that we should

	 * report the critical pressure, yet. If the scanning priority

	 * (scanning depth) goes too high (deep), we will be notified

	 * through vmpressure_prio(). But so far, keep calm.

 For now, no users for root-level efficiency */

			/*

			 * Let the socket buffer allocator know that

			 * we are having trouble reclaiming LRU pages.

			 *

			 * For hysteresis keep the pressure state

			 * asserted for a second in which subsequent

			 * pressure events can occur.

/**

 * vmpressure_prio() - Account memory pressure through reclaimer priority level

 * @gfp:	reclaimer's gfp mask

 * @memcg:	cgroup memory controller handle

 * @prio:	reclaimer's priority

 *

 * This function should be called from the reclaim path every time when

 * the vmscan's reclaiming priority (scanning depth) changes.

 *

 * This function does not return any value.

	/*

	 * We only use prio for accounting critical level. For more info

	 * see comment for vmpressure_level_critical_prio variable above.

	/*

	 * OK, the prio is below the threshold, updating vmpressure

	 * information before shrinker dives into long shrinking of long

	 * range vmscan. Passing scanned = vmpressure_win, reclaimed = 0

	 * to the vmpressure() basically means that we signal 'critical'

	 * level.

/**

 * vmpressure_register_event() - Bind vmpressure notifications to an eventfd

 * @memcg:	memcg that is interested in vmpressure notifications

 * @eventfd:	eventfd context to link notifications with

 * @args:	event arguments (pressure level threshold, optional mode)

 *

 * This function associates eventfd context with the vmpressure

 * infrastructure, so that the notifications will be delivered to the

 * @eventfd. The @args parameter is a comma-delimited string that denotes a

 * pressure level threshold (one of vmpressure_str_levels, i.e. "low", "medium",

 * or "critical") and an optional mode (one of vmpressure_str_modes, i.e.

 * "hierarchy" or "local").

 *

 * To be used as memcg event method.

 *

 * Return: 0 on success, -ENOMEM on memory failure or -EINVAL if @args could

 * not be parsed.

 Find required level */

 Find optional mode */

/**

 * vmpressure_unregister_event() - Unbind eventfd from vmpressure

 * @memcg:	memcg handle

 * @eventfd:	eventfd context that was used to link vmpressure with the @cg

 *

 * This function does internal manipulations to detach the @eventfd from

 * the vmpressure notifications, and then frees internal resources

 * associated with the @eventfd (but the @eventfd itself is not freed).

 *

 * To be used as memcg event method.

/**

 * vmpressure_init() - Initialize vmpressure control structure

 * @vmpr:	Structure to be initialized

 *

 * This function should be called on every allocated vmpressure structure

 * before any usage.

/**

 * vmpressure_cleanup() - shuts down vmpressure control structure

 * @vmpr:	Structure to be cleaned up

 *

 * This function should be called before the structure in which it is

 * embedded is cleaned up.

	/*

	 * Make sure there is no pending work before eventfd infrastructure

	 * goes away.

 SPDX-License-Identifier: GPL-2.0-or-later

/* memcontrol.c - Memory Controller

 *

 * Copyright IBM Corporation, 2007

 * Author Balbir Singh <balbir@linux.vnet.ibm.com>

 *

 * Copyright 2007 OpenVZ SWsoft Inc

 * Author: Pavel Emelianov <xemul@openvz.org>

 *

 * Memory thresholds

 * Copyright (C) 2009 Nokia Corporation

 * Author: Kirill A. Shutemov

 *

 * Kernel Memory Controller

 * Copyright (C) 2012 Parallels Inc. and Google Inc.

 * Authors: Glauber Costa and Suleiman Souhlal

 *

 * Native page reclaim

 * Charge lifetime sanitation

 * Lockless page tracking & accounting

 * Unified hierarchy configuration model

 * Copyright (C) 2015 Red Hat, Inc., Johannes Weiner

 *

 * Per memcg lru locking

 * Copyright (C) 2020 Alibaba, Inc, Alex Shi

 Active memory cgroup to use from an interrupt context */

 Socket memory accounting disabled? */

 Kernel memory accounting disabled? */

 Whether the swap controller is active */

 Whether legacy memory+swap accounting is active */

/*

 * Cgroups above their limits are maintained in a RB-Tree, independent of

 * their hierarchy representation

 for OOM */

/*

 * cgroup_event represents events which userspace want to receive.

	/*

	 * memcg which the event belongs to.

	/*

	 * eventfd to signal userspace about the event.

	/*

	 * Each of these stored in a list by the cgroup.

	/*

	 * register_event() callback will be used to add new userspace

	 * waiter for changes related to this event.  Use eventfd_signal()

	 * on eventfd to send notification to userspace.

	/*

	 * unregister_event() callback will be called when userspace closes

	 * the eventfd or on cgroup removing.  This callback must be set,

	 * if you want provide notification functionality.

	/*

	 * All fields below needed to unregister event when

	 * userspace closes eventfd.

 Stuffs for move charges at task migration. */

/*

 * Types of charges to be moved.

 "mc" and its members are protected by cgroup_mutex */

 for from, to */

 a task moving charges */

 a waitq for other context */

/*

 * Maximum loops in mem_cgroup_hierarchical_reclaim(), used for soft

 * limit reclaim to prevent infinite loops, if they ever occur.

 for encoding cft->private value on file */

 Used for OOM notifier */

/*

 * Iteration constructs for visiting all cgroups (under a tree).  If

 * loops are exited prematurely (break), mem_cgroup_iter_break() must

 * be used for reference counting.

 Some nice accessors for the vmpressure. */

	/*

	 * At this point all allocated objects are freed, and

	 * objcg->nr_charged_bytes can't have an arbitrary byte value.

	 * However, it can be PAGE_SIZE or (x * PAGE_SIZE).

	 *

	 * The following sequence can lead to it:

	 * 1) CPU0: objcg == stock->cached_objcg

	 * 2) CPU1: we do a small allocation (e.g. 92 bytes),

	 *          PAGE_SIZE bytes are charged

	 * 3) CPU1: a process from another memcg is allocating something,

	 *          the stock if flushed,

	 *          objcg->nr_charged_bytes = PAGE_SIZE - 92

	 * 5) CPU0: we do release this object,

	 *          92 bytes are added to stock->nr_bytes

	 * 6) CPU0: stock is flushed,

	 *          92 bytes are added to objcg->nr_charged_bytes

	 *

	 * In the result, nr_charged_bytes == PAGE_SIZE.

	 * This page will be uncharged in obj_cgroup_release().

 1) Ready to reparent active objcg. */

 2) Reparent active objcg and already reparented objcgs to parent. */

 3) Move already reparented objcgs to the parent's list */

/*

 * This will be used as a shrinker list's index.

 * The main reason for not using cgroup id for this:

 *  this works better in sparse environments, where we have a lot of memcgs,

 *  but only a few kmem-limited. Or also, if we have, for instance, 200

 *  memcgs, and none but the 200th is kmem-limited, we'd have to have a

 *  200 entry array for that.

 *

 * The current size of the caches array is stored in memcg_nr_cache_ids. It

 * will double each time we have to increase it.

 Protects memcg_nr_cache_ids */

/*

 * MIN_SIZE is different than 1, because we would like to avoid going through

 * the alloc/free process all the time. In a small machine, 4 kmem-limited

 * cgroups is a reasonable guess. In the future, it could be a parameter or

 * tunable, but that is strictly not necessary.

 *

 * MAX_SIZE should be as large as the number of cgrp_ids. Ideally, we could get

 * this constant directly from cgroup, but it is understandable that this is

 * better kept as an internal representation in cgroup.c. In any case, the

 * cgrp_id space is not getting any smaller, and we don't have to necessarily

 * increase ours as well if it increases.

/*

 * A lot of the calls to the cache allocation functions are expected to be

 * inlined by the compiler. Since the calls to memcg_slab_pre_alloc_hook() are

 * conditional to this static branch, we'll have to allow modules that does

 * kmem_cache_alloc and the such to see this symbol as well

/**

 * mem_cgroup_css_from_page - css of the memcg associated with a page

 * @page: page of interest

 *

 * If memcg is bound to the default hierarchy, css of the memcg associated

 * with @page is returned.  The returned css remains associated with @page

 * until it is released.

 *

 * If memcg is bound to a traditional hierarchy, the css of root_mem_cgroup

 * is returned.

/**

 * page_cgroup_ino - return inode number of the memcg a page is charged to

 * @page: the page

 *

 * Look up the closest online ancestor of the memory cgroup @page is charged to

 * and return its inode number or 0 if @page is not charged to any cgroup. It

 * is safe to call this function without holding a reference to @page.

 *

 * Note, this function is inherently racy, because there is nothing to prevent

 * the cgroup inode from getting torn down and potentially reallocated a moment

 * after page_cgroup_ino() returns, so it only should be used by callers that

 * do not care (such as procfs interfaces).

	/*

	 * Necessary to update all ancestors when hierarchy is used.

	 * because their event counter is not touched.

		/*

		 * We have to update the tree if mz is on RB-tree or

		 * mem is over its softlimit.

 if on-tree, remove it */

			/*

			 * Insert again. mz->usage_in_excess will be updated.

			 * If excess is 0, no tree ops.

 Nothing to reclaim from */

	/*

	 * Remove the node now but someone else can add it back,

	 * we will to add it back at the end of reclaim to its correct

	 * position in the tree.

/*

 * memcg and lruvec stats flushing

 *

 * Many codepaths leading to stats update or read are performance sensitive and

 * adding stats flushing in such codepaths is not desirable. So, to optimize the

 * flushing the kernel does:

 *

 * 1) Periodically and asynchronously flush the stats every 2 seconds to not let

 *    rstat update tree grow unbounded.

 *

 * 2) Flush the stats synchronously on reader side only when there are more than

 *    (MEMCG_CHARGE_BATCH * nr_cpus) update events. Though this optimization

 *    will let stats be out of sync by atmost (MEMCG_CHARGE_BATCH * nr_cpus) but

 *    only for 2 seconds due to (1).

/**

 * __mod_memcg_state - update cgroup memory statistics

 * @memcg: the memory cgroup

 * @idx: the stat item - can be enum memcg_stat_item or enum node_stat_item

 * @val: delta to add to the counter, can be negative

 idx can be of type enum memcg_stat_item or node_stat_item. */

 Update memcg */

 Update lruvec */

/**

 * __mod_lruvec_state - update lruvec memory statistics

 * @lruvec: the lruvec

 * @idx: the stat item

 * @val: delta to add to the counter, can be negative

 *

 * The lruvec is the intersection of the NUMA node and a cgroup. This

 * function updates the all three counters that are affected by a

 * change of state at this level: per-node, per-cgroup, per-lruvec.

 Update node */

 Update memcg and lruvec */

 rmap on tail pages */

 Untracked pages have no memcg, no lruvec. Update only the node */

	/*

	 * Untracked pages have no memcg, no lruvec. Update only the

	 * node. If we reparent the slab objects to the root memcg,

	 * when we free the slab object, we need to update the per-memcg

	 * vmstats to keep it correct for the root memcg.

/*

 * mod_objcg_mlstate() may be called with irq enabled, so

 * mod_memcg_lruvec_state() should be used.

/**

 * __count_memcg_events - account VM events in a cgroup

 * @memcg: the memory cgroup

 * @idx: the event item

 * @count: the number of events that occurred

 pagein of a big page is an event. So, ignore page size */

 for event */

 from time_after() in jiffies.h */

/*

 * Check events in order.

 *

 threshold event is triggered in finer grain than soft limit */

	/*

	 * mm_update_next_owner() may clear mm->owner to NULL

	 * if it races with swapoff, page migration, etc.

	 * So this can be called with p == NULL.

/**

 * get_mem_cgroup_from_mm: Obtain a reference on given mm_struct's memcg.

 * @mm: mm from which memcg should be extracted. It can be NULL.

 *

 * Obtain a reference on mm->memcg and returns it if successful. If mm

 * is NULL, then the memcg is chosen as follows:

 * 1) The active memcg, if set.

 * 2) current->mm->memcg, if available

 * 3) root memcg

 * If mem_cgroup is disabled, NULL is returned.

	/*

	 * Page cache insertions can happen without an

	 * actual mm context, e.g. during disk probing

	 * on boot, loopback IO, acct() writes etc.

	 *

	 * No need to css_get on root memcg as the reference

	 * counting is disabled on the root level in the

	 * cgroup core. See CSS_NO_REF.

 remote memcg must hold a ref */

 Allow remote memcg charging from any context. */

 Memcg to charge can't be determined. */

/**

 * mem_cgroup_iter - iterate over memory cgroup hierarchy

 * @root: hierarchy root

 * @prev: previously returned memcg, NULL on first invocation

 * @reclaim: cookie for shared reclaim walks, NULL for full walks

 *

 * Returns references to children of the hierarchy below @root, or

 * @root itself, or %NULL after a full round-trip.

 *

 * Caller must pass the return value in @prev on subsequent

 * invocations for reference counting, or use mem_cgroup_iter_break()

 * to cancel a hierarchy walk before the round-trip is complete.

 *

 * Reclaimers can specify a node in @reclaim to divide up the memcgs

 * in the hierarchy among all concurrent reclaimers operating on the

 * same node.

			/*

			 * css reference reached zero, so iter->position will

			 * be cleared by ->css_released. However, we should not

			 * rely on this happening soon, because ->css_released

			 * is called from a work queue, and by busy-waiting we

			 * might block it. So we clear iter->position right

			 * away.

			/*

			 * Reclaimers share the hierarchy walk, and a

			 * new one might jump in right at the end of

			 * the hierarchy - make sure they see at least

			 * one group and restart from the beginning.

		/*

		 * Verify the css and acquire a reference.  The root

		 * is provided by the caller, so we know it's alive

		 * and kicking, and don't take an extra reference.

		/*

		 * The position could have already been updated by a competing

		 * thread, so check that the value hasn't changed since we read

		 * it to avoid reclaiming from the same cgroup twice.

/**

 * mem_cgroup_iter_break - abort a hierarchy walk prematurely

 * @root: hierarchy root

 * @prev: last visited hierarchy member as returned by mem_cgroup_iter()

	/*

	 * When cgruop1 non-hierarchy mode is used,

	 * parent_mem_cgroup() does not walk all the way up to the

	 * cgroup root (root_mem_cgroup). So we have to handle

	 * dead_memcg from cgroup root separately.

/**

 * mem_cgroup_scan_tasks - iterate over tasks of a memory cgroup hierarchy

 * @memcg: hierarchy root

 * @fn: function to call for each task

 * @arg: argument passed to @fn

 *

 * This function iterates over tasks attached to @memcg or to any of its

 * descendants and calls @fn for each task. If @fn returns a non-zero

 * value, the function breaks the iteration loop and returns the value.

 * Otherwise, it will iterate over all tasks and return 0.

 *

 * This function must not be called for the root memory cgroup.

/**

 * folio_lruvec_lock - Lock the lruvec for a folio.

 * @folio: Pointer to the folio.

 *

 * These functions are safe to use under any of the following conditions:

 * - folio locked

 * - folio_test_lru false

 * - folio_memcg_lock()

 * - folio frozen (refcount of 0)

 *

 * Return: The lruvec this folio is on with its lock held.

/**

 * folio_lruvec_lock_irq - Lock the lruvec for a folio.

 * @folio: Pointer to the folio.

 *

 * These functions are safe to use under any of the following conditions:

 * - folio locked

 * - folio_test_lru false

 * - folio_memcg_lock()

 * - folio frozen (refcount of 0)

 *

 * Return: The lruvec this folio is on with its lock held and interrupts

 * disabled.

/**

 * folio_lruvec_lock_irqsave - Lock the lruvec for a folio.

 * @folio: Pointer to the folio.

 * @flags: Pointer to irqsave flags.

 *

 * These functions are safe to use under any of the following conditions:

 * - folio locked

 * - folio_test_lru false

 * - folio_memcg_lock()

 * - folio frozen (refcount of 0)

 *

 * Return: The lruvec this folio is on with its lock held and interrupts

 * disabled.

/**

 * mem_cgroup_update_lru_size - account for adding or removing an lru page

 * @lruvec: mem_cgroup per zone lru vector

 * @lru: index of lru list the page is sitting on

 * @zid: zone id of the accounted pages

 * @nr_pages: positive when adding or negative when removing

 *

 * This function must be called under lru_lock, just before a page is added

 * to or just after a page is removed from an lru list (that ordering being

 * so as to allow it to check that lru_size 0 is consistent with list_empty).

/**

 * mem_cgroup_margin - calculate chargeable space of a memory cgroup

 * @memcg: the memory cgroup

 *

 * Returns the maximum amount of memory @mem can be charged with, in

 * pages.

/*

 * A routine for checking "mem" is under move_account() or not.

 *

 * Checking a cgroup is mc.from or mc.to or under hierarchy of

 * moving cgroups. This is for waiting at high-memory pressure

 * caused by "move".

	/*

	 * Unlike task_move routines, we access mc.to, mc.from not under

	 * mutual exclusion by cgroup_mutex. Here, we take spinlock instead.

 moving charge context might have finished. */

 The memory events */

 Translate stat items to the correct unit for memory.stat output */

	/*

	 * Provide statistics on the state of the memory subsystem as

	 * well as cumulative event counters that show past behavior.

	 *

	 * This list is ordered following a combination of these gradients:

	 * 1) generic big picture -> specifics and details

	 * 2) reflecting userspace activity -> reflecting kernel heuristics

	 *

	 * Current memory state:

 Accumulated memory events */

 CONFIG_TRANSPARENT_HUGEPAGE */

 The above should easily fit into one page */

/**

 * mem_cgroup_print_oom_context: Print OOM information relevant to

 * memory controller.

 * @memcg: The memory cgroup that went over limit

 * @p: Task that is going to be killed

 *

 * NOTE: @memcg and @p's mem_cgroup can be different when hierarchy is

 * enabled

/**

 * mem_cgroup_print_oom_meminfo: Print OOM memory information relevant to

 * memory controller.

 * @memcg: The memory cgroup that went over limit

/*

 * Return the memory (and swap, if configured) limit for a memcg.

 v1 */

 Calculate swap excess capacity from memsw limit */

	/*

	 * A few threads which were not waiting at mutex_lock_killable() can

	 * fail to bail out. Therefore, check again after holding oom_lock.

				/*

				 * If we have not been able to reclaim

				 * anything, it might because there are

				 * no reclaimable pages under this hierarchy

				/*

				 * We want to do more targeted reclaim.

				 * excess >> 2 is not to excessive so as to

				 * reclaim too much, nor too less that we keep

				 * coming back to reclaim from this cgroup

/*

 * Check OOM-Killer is already running under our hierarchy.

 * If someone is running, return false.

			/*

			 * this subtree of our hierarchy is already locked

			 * so we cannot give a lock.

		/*

		 * OK, we failed to lock the whole subtree so we have

		 * to clean up what we set up to the failing subtree

	/*

	 * Be careful about under_oom underflows because a child memcg

	 * could have been added after mem_cgroup_mark_under_oom.

	/*

	 * For the following lockless ->under_oom test, the only required

	 * guarantee is that it must see the state asserted by an OOM when

	 * this function is called as a result of userland actions

	 * triggered by the notification of the OOM.  This is trivially

	 * achieved by invoking mem_cgroup_mark_under_oom() before

	 * triggering notification.

	/*

	 * We are in the middle of the charge context here, so we

	 * don't want to block when potentially sitting on a callstack

	 * that holds all kinds of filesystem and mm locks.

	 *

	 * cgroup1 allows disabling the OOM killer and waiting for outside

	 * handling until the charge can succeed; remember the context and put

	 * the task to sleep at the end of the page fault when all locks are

	 * released.

	 *

	 * On the other hand, in-kernel OOM killer allows for an async victim

	 * memory reclaim (oom_reaper) and that means that we are not solely

	 * relying on the oom victim to make a forward progress and we can

	 * invoke the oom killer here.

	 *

	 * Please note that mem_cgroup_out_of_memory might fail to find a

	 * victim and then we have to bail out from the charge path.

/**

 * mem_cgroup_oom_synchronize - complete memcg OOM handling

 * @handle: actually kill/wait or just clean up the OOM state

 *

 * This has to be called at the end of a page fault if the memcg OOM

 * handler was enabled.

 *

 * Memcg supports userspace OOM handling where failed allocations must

 * sleep on a waitqueue until the userspace task resolves the

 * situation.  Sleeping directly in the charge context with all kinds

 * of locks held is not a good idea, instead we remember an OOM state

 * in the task and mem_cgroup_oom_synchronize() has to be called at

 * the end of the page fault to complete the OOM handling.

 *

 * Returns %true if an ongoing memcg OOM situation was detected and

 * completed, %false otherwise.

 OOM is global, do not handle */

		/*

		 * There is no guarantee that an OOM-lock contender

		 * sees the wakeups triggered by the OOM kill

		 * uncharges.  Wake any sleepers explicitly.

/**

 * mem_cgroup_get_oom_group - get a memory cgroup to clean up after OOM

 * @victim: task to be killed by the OOM killer

 * @oom_domain: memcg in case of memcg OOM, NULL in case of system-wide OOM

 *

 * Returns a pointer to a memory cgroup, which has to be cleaned up

 * by killing all belonging OOM-killable tasks.

 *

 * Caller has to call mem_cgroup_put() on the returned non-NULL memcg.

	/*

	 * If the victim task has been asynchronously moved to a different

	 * memory cgroup, we might end up killing tasks outside oom_domain.

	 * In this case it's better to ignore memory.group.oom.

	/*

	 * Traverse the memory cgroup hierarchy from the victim task's

	 * cgroup up to the OOMing cgroup (or root) to find the

	 * highest-level memory cgroup with oom.group set.

/**

 * folio_memcg_lock - Bind a folio to its memcg.

 * @folio: The folio.

 *

 * This function prevents unlocked LRU folios from being moved to

 * another cgroup.

 *

 * It ensures lifetime of the bound memcg.  The caller is responsible

 * for the lifetime of the folio.

	/*

	 * The RCU lock is held throughout the transaction.  The fast

	 * path can get away without acquiring the memcg->move_lock

	 * because page moving starts with an RCU grace period.

	/*

	 * When charge migration first begins, we can have multiple

	 * critical sections holding the fast-path RCU lock and one

	 * holding the slowpath move_lock. Track the task who has the

	 * move_lock for unlock_page_memcg().

/**

 * folio_memcg_unlock - Release the binding between a folio and its memcg.

 * @folio: The folio.

 *

 * This releases the binding created by folio_memcg_lock().  This does

 * not change the accounting of this folio to its memcg, but it does

 * permit others to change it.

 this never be root cgroup */

/*

 * Most kmem_cache_alloc() calls are from user context. The irq disable/enable

 * sequence used in this case to access content from object stock is slow.

 * To optimize for user context access, there are now two object stocks for

 * task context and interrupt context access respectively.

 *

 * The task context object stock can be accessed by disabling preemption only

 * which is cheap in non-preempt kernel. The interrupt context object stock

 * can only be accessed after disabling interrupt. User context code can

 * access interrupt object stock, but not vice versa.

/**

 * consume_stock: Try to consume stocked charge on this cpu.

 * @memcg: memcg to consume from.

 * @nr_pages: how many pages to charge.

 *

 * The charges will only happen if @memcg matches the current cpu's memcg

 * stock, and at least @nr_pages are available in that stock.  Failure to

 * service an allocation will refill the stock.

 *

 * returns true if successful, false otherwise.

/*

 * Returns stocks cached in percpu and reset cached information.

	/*

	 * The only protection from cpu hotplug (memcg_hotplug_cpu_dead) vs.

	 * drain_stock races is that we always operate on local CPU stock

	 * here with IRQ disabled

/*

 * Cache charges(val) to local per_cpu area.

 * This will be consumed by consume_stock() function, later.

 reset if necessary */

/*

 * Drains all per-CPU charge caches for given root_memcg resp. subtree

 * of the hierarchy under it.

 If someone's already draining, avoid adding running more workers. */

	/*

	 * Notify other cpus that system-wide "drain" is running

	 * We do not care about races with the cpu hotplug because cpu down

	 * as well as workers from this path always operate on the local

	 * per-cpu data. CPU up doesn't touch memcg_stock at all.

/*

 * Clamp the maximum sleep time per allocation batch to 2 seconds. This is

 * enough to still cause a significant slowdown in most cases, while still

 * allowing diagnostics and tracing to proceed without becoming stuck.

/*

 * When calculating the delay, we use these either side of the exponentiation to

 * maintain precision and scale to a reasonable number of jiffies (see the table

 * below.

 *

 * - MEMCG_DELAY_PRECISION_SHIFT: Extra precision bits while translating the

 *   overage ratio to a delay.

 * - MEMCG_DELAY_SCALING_SHIFT: The number of bits to scale down the

 *   proposed penalty in order to reduce to a reasonable number of jiffies, and

 *   to produce a reasonable delay curve.

 *

 * MEMCG_DELAY_SCALING_SHIFT just happens to be a number that produces a

 * reasonable delay curve compared to precision-adjusted overage, not

 * penalising heavily at first, but still making sure that growth beyond the

 * limit penalises misbehaviour cgroups by slowing them down exponentially. For

 * example, with a high of 100 megabytes:

 *

 *  +-------+------------------------+

 *  | usage | time to allocate in ms |

 *  +-------+------------------------+

 *  | 100M  |                      0 |

 *  | 101M  |                      6 |

 *  | 102M  |                     25 |

 *  | 103M  |                     57 |

 *  | 104M  |                    102 |

 *  | 105M  |                    159 |

 *  | 106M  |                    230 |

 *  | 107M  |                    313 |

 *  | 108M  |                    409 |

 *  | 109M  |                    518 |

 *  | 110M  |                    639 |

 *  | 111M  |                    774 |

 *  | 112M  |                    921 |

 *  | 113M  |                   1081 |

 *  | 114M  |                   1254 |

 *  | 115M  |                   1439 |

 *  | 116M  |                   1638 |

 *  | 117M  |                   1849 |

 *  | 118M  |                   2000 |

 *  | 119M  |                   2000 |

 *  | 120M  |                   2000 |

 *  +-------+------------------------+

	/*

	 * Prevent division by 0 in overage calculation by acting as if

	 * it was a threshold of 1 page

/*

 * Get the number of jiffies that we should penalise a mischievous cgroup which

 * is exceeding its memory.high by checking both it and its ancestors.

	/*

	 * We use overage compared to memory.high to calculate the number of

	 * jiffies to sleep (penalty_jiffies). Ideally this value should be

	 * fairly lenient on small overages, and increasingly harsh when the

	 * memcg in question makes it clear that it has no intention of stopping

	 * its crazy behaviour, so we exponentially increase the delay based on

	 * overage amount.

	/*

	 * Factor in the task's own contribution to the overage, such that four

	 * N-sized allocations are throttled approximately the same as one

	 * 4N-sized allocation.

	 *

	 * MEMCG_CHARGE_BATCH pages is nominal, so work out how much smaller or

	 * larger the current charge patch is than that.

/*

 * Scheduled by try_charge() to be executed from the userland return path

 * and reclaims memory over the high limit.

	/*

	 * The allocating task should reclaim at least the batch size, but for

	 * subsequent retries we only want to do what's necessary to prevent oom

	 * or breaching resource isolation.

	 *

	 * This is distinct from memory.max or page allocator behaviour because

	 * memory.high is currently batched, whereas memory.max and the page

	 * allocator run every time an allocation is made.

	/*

	 * memory.high is breached and reclaim is unable to keep up. Throttle

	 * allocators proactively to slow down excessive growth.

	/*

	 * Clamp the max delay per usermode return so as to still keep the

	 * application moving forwards and also permit diagnostics, albeit

	 * extremely slowly.

	/*

	 * Don't sleep if the amount of jiffies this memcg owes us is so low

	 * that it's not even worth doing, in an attempt to be nice to those who

	 * go only a small amount over their memory.high value and maybe haven't

	 * been aggressively reclaimed enough yet.

	/*

	 * If reclaim is making forward progress but we're still over

	 * memory.high, we want to encourage that rather than doing allocator

	 * throttling.

	/*

	 * If we exit early, we're guaranteed to die (since

	 * schedule_timeout_killable sets TASK_KILLABLE). This means we don't

	 * need to account for any ill-begotten jiffies to pay them off later.

	/*

	 * Memcg doesn't have a dedicated reserve for atomic

	 * allocations. But like the global atomic pool, we need to

	 * put the burden of reclaim on regular allocation requests

	 * and let these go through as privileged allocations.

	/*

	 * Prevent unbounded recursion when reclaim operations need to

	 * allocate memory. This might exceed the limits temporarily,

	 * but we prefer facilitating memory reclaim and getting back

	 * under the limit over triggering OOM kills in these cases.

	/*

	 * Even though the limit is exceeded at this point, reclaim

	 * may have been able to free some pages.  Retry the charge

	 * before killing the task.

	 *

	 * Only for regular pages, though: huge pages are rather

	 * unlikely to succeed so close to the limit, and we fall back

	 * to regular pages anyway in case of failure.

	/*

	 * At task move, charge accounts can be doubly counted. So, it's

	 * better to wait until the end of task_move if something is going on.

 Avoid endless loop for tasks bypassed by the oom killer */

	/*

	 * keep retrying as long as the memcg oom killer is able to make

	 * a forward progress or bypass the charge if the oom killer

	 * couldn't make any progress.

	/*

	 * The allocation either can't fail or will lead to more memory

	 * being freed very soon.  Allow memory usage go over the limit

	 * temporarily by force charging it.

	/*

	 * If the hierarchy is above the normal consumption range, schedule

	 * reclaim on returning to userland.  We can perform reclaim here

	 * if __GFP_RECLAIM but let's always punt for simplicity and so that

	 * GFP_KERNEL can consistently be used during reclaim.  @memcg is

	 * not recorded as it most likely matches current's and won't

	 * change in the meantime.  As high limit is checked again before

	 * reclaim, the cost of mismatch is negligible.

 Don't bother a random interrupted task */

			/*

			 * The allocating tasks in this cgroup will need to do

			 * reclaim or be throttled to prevent further growth

			 * of the memory or swap footprints.

			 *

			 * Target some best-effort fairness between the tasks,

			 * and distribute reclaim work and delay penalties

			 * based on how much each task is actually allocating.

	/*

	 * Any of the following ensures page's memcg stability:

	 *

	 * - the page lock

	 * - LRU isolation

	 * - lock_page_memcg()

	 * - exclusive reference

/*

 * The allocated objcg pointers array is not accounted directly.

 * Moreover, it should not come from DMA buffer and is not readily

 * reclaimable. So those GFP bits should be masked off.

		/*

		 * If the slab page is brand new and nobody can yet access

		 * it's memcg_data, no synchronization is required and

		 * memcg_data can be simply assigned.

		/*

		 * If the slab page is already in use, somebody can allocate

		 * and assign obj_cgroups in parallel. In this case the existing

		 * objcg vector should be reused.

/*

 * Returns a pointer to the memory cgroup to which the kernel object is charged.

 *

 * A passed kernel object can be a slab object or a generic kernel page, so

 * different mechanisms for getting the memory cgroup pointer should be used.

 * In certain cases (e.g. kernel stacks or large kmallocs with SLUB) the caller

 * can not know for sure how the kernel object is implemented.

 * mem_cgroup_from_obj() can be safely used in such cases.

 *

 * The caller must ensure the memcg lifetime, e.g. by taking rcu_read_lock(),

 * cgroup_mutex, etc.

	/*

	 * Slab objects are accounted individually, not per-page.

	 * Memcg membership data for each individual object is saved in

	 * the page->obj_cgroups.

	/*

	 * page_memcg_check() is used here, because page_has_obj_cgroups()

	 * check above could fail because the object cgroups vector wasn't set

	 * at that moment, but it can be set concurrently.

	 * page_memcg_check(page) will guarantee that a proper memory

	 * cgroup pointer or NULL will be returned.

	/*

	 * There's no space for the new id in memcg_caches arrays,

	 * so we have to grow them.

/*

 * obj_cgroup_uncharge_pages: uncharge a number of kernel pages from a objcg

 * @objcg: object cgroup to uncharge

 * @nr_pages: number of pages to uncharge

/*

 * obj_cgroup_charge_pages: charge a number of kernel pages to a objcg

 * @objcg: object cgroup to charge

 * @gfp: reclaim mode

 * @nr_pages: number of pages to charge

 *

 * Returns 0 on success, an error code on failure.

/**

 * __memcg_kmem_charge_page: charge a kmem page to the current memory cgroup

 * @page: page to charge

 * @gfp: reclaim mode

 * @order: allocation order

 *

 * Returns 0 on success, an error code on failure.

/**

 * __memcg_kmem_uncharge_page: uncharge a kmem page

 * @page: page to uncharge

 * @order: allocation order

	/*

	 * Save vmstat data in stock and skip vmstat array update unless

	 * accumulating over a page of vmstat data or when pgdat or idx

	 * changes.

 Flush the existing cached vmstat data */

	/*

	 * Even for large object >= PAGE_SIZE, the vmstat data will still be

	 * cached locally at least once before pushing it out.

		/*

		 * The leftover is flushed to the centralized per-memcg value.

		 * On the next attempt to refill obj stock it will be moved

		 * to a per-cpu stock (probably, on an other CPU), see

		 * refill_obj_stock().

		 *

		 * How often it's flushed is a trade-off between the memory

		 * limit enforcement accuracy and potential CPU contention,

		 * so it might be changed in the future.

	/*

	 * Flush the vmstat data in current stock

 reset if necessary */

 Allow uncharge when objcg changes */

	/*

	 * In theory, objcg->nr_charged_bytes can have enough

	 * pre-charged bytes to satisfy the allocation. However,

	 * flushing objcg->nr_charged_bytes requires two atomic

	 * operations, and objcg->nr_charged_bytes can't be big.

	 * The shared objcg->nr_charged_bytes can also become a

	 * performance bottleneck if all tasks of the same memcg are

	 * trying to update it. So it's better to ignore it and try

	 * grab some new pages. The stock's nr_bytes will be flushed to

	 * objcg->nr_charged_bytes later on when objcg changes.

	 *

	 * The stock's nr_bytes may contain enough pre-charged bytes

	 * to allow one less page from being charged, but we can't rely

	 * on the pre-charged bytes not being changed outside of

	 * consume_obj_stock() or refill_obj_stock(). So ignore those

	 * pre-charged bytes as well when charging pages. To avoid a

	 * page uncharge right after a page charge, we set the

	 * allow_uncharge flag to false when calling refill_obj_stock()

	 * to temporarily allow the pre-charged bytes to exceed the page

	 * size limit. The maximum reachable value of the pre-charged

	 * bytes is (sizeof(object) + PAGE_SIZE - 2) if there is no data

	 * race.

 CONFIG_MEMCG_KMEM */

/*

 * Because page_memcg(head) is not set on tails, set it now.

/**

 * mem_cgroup_move_swap_account - move swap charge and swap_cgroup's record.

 * @entry: swap entry to be moved

 * @from:  mem_cgroup which the entry is moved from

 * @to:  mem_cgroup which the entry is moved to

 *

 * It succeeds only when the swap_cgroup's record for this entry is the same

 * as the mem_cgroup's id of @from.

 *

 * Returns 0 on success, -EINVAL on failure.

 *

 * The caller must have charged to @to, IOW, called page_counter_charge() about

 * both res and memsw, and called css_get().

		/*

		 * Make sure that the new limit (memsw or memory limit) doesn't

		 * break our basic invariant rule memory.max <= memsw.max.

	/*

	 * Do not even bother to check the largest node if the root

	 * is empty. Do it lockless to prevent lock bouncing. Races

	 * are acceptable as soft limit is best effort anyway.

	/*

	 * This loop can run a while, specially if mem_cgroup's continuously

	 * keep exceeding their soft limit and putting the system under

	 * pressure

		/*

		 * If we failed to reclaim anything from this memory cgroup

		 * it is time to move on to the next cgroup

		/*

		 * One school of thought says that we should not add

		 * back the node to the tree if reclaim returns 0.

		 * But our reclaim could return 0, simply because due

		 * to priority we are exposing a smaller subset of

		 * memory to reclaim from. Consider this as a longer

		 * term TODO.

 If excess == 0, no tree ops */

		/*

		 * Could not reclaim anything and there are no more

		 * mem cgroups to try or we seem to be looping without

		 * reclaiming anything.

/*

 * Reclaims as many pages from the given memcg as possible.

 *

 * Caller is responsible for holding css reference for memcg.

 we call try-to-free pages for make this cgroup empty */

 try to free all pages in this cgroup */

	/*

	 * After we have finished memcg_reparent_objcgs(), all list_lrus

	 * corresponding to this cgroup are guaranteed to remain empty.

	 * The ordering is imposed by list_lru_node->lock taken by

	 * memcg_drain_all_list_lrus().

 CONFIG_MEMCG_KMEM */

		/*

		 * The active flag needs to be written after the static_key

		 * update. This is what guarantees that the socket activation

		 * function is the last one to run. See mem_cgroup_sk_alloc()

		 * for details, and note that we don't mark any socket as

		 * belonging to this memcg until that flag is up.

		 *

		 * We need to do this, because static_keys will span multiple

		 * sites, but we can't control their order. If we mark a socket

		 * as accounted, but the accounting functions are not patched in

		 * yet, we'll lose accounting.

		 *

		 * We never race with the readers in mem_cgroup_sk_alloc(),

		 * because when this value change, the code to process it is not

		 * patched in yet.

/*

 * The user of this function is...

 * RES_LIMIT.

 Can't set limit on root */

 kmem.limit_in_bytes is deprecated. */

	/*

	 * No kind of locking is needed in here, because ->can_attach() will

	 * check this value once in the beginning of the process, and then carry

	 * on with stale data. This means that changes to this value will only

	 * affect task migrations starting after the change.

 CONFIG_NUMA */

 Universal VM events cgroup1 shows, original sort order */

 Hierarchical information */

	/*

	 * current_threshold points to threshold just below or equal to usage.

	 * If it's not true, a threshold was crossed after last

	 * call of __mem_cgroup_threshold().

	/*

	 * Iterate backward over array of thresholds starting from

	 * current_threshold and check if a threshold is crossed.

	 * If none of thresholds below usage is crossed, we read

	 * only one element of the array here.

 i = current_threshold + 1 */

	/*

	 * Iterate forward over array of thresholds starting from

	 * current_threshold+1 and check if a threshold is crossed.

	 * If none of thresholds above usage is crossed, we read

	 * only one element of the array here.

 Update current_threshold */

 Check if a threshold crossed before adding a new one */

 Allocate memory for new array of thresholds */

 Copy thresholds (if any) to new array */

 Add new threshold */

 Sort thresholds. Registering of new threshold isn't time-critical */

 Find current threshold */

			/*

			 * new->current_threshold will not be used until

			 * rcu_assign_pointer(), so it's safe to increment

			 * it here.

 Free old spare buffer and save old primary buffer as spare */

 To be sure that nobody uses thresholds */

 Check if a threshold crossed before removing */

 Calculate new number of threshold */

 If no items related to eventfd have been cleared, nothing to do */

 Set thresholds array to NULL if we don't have thresholds */

 Copy thresholds and find current threshold */

			/*

			 * new->current_threshold will not be used

			 * until rcu_assign_pointer(), so it's safe to increment

			 * it here.

 Swap primary and spare array */

 To be sure that nobody uses thresholds */

 If all events are unregistered, free the spare array */

 already in OOM ? */

 cannot set to root cgroup and only 0 and 1 are allowed */

/**

 * mem_cgroup_wb_stats - retrieve writeback related stats from its memcg

 * @wb: bdi_writeback in question

 * @pfilepages: out parameter for number of file pages

 * @pheadroom: out parameter for number of allocatable pages according to memcg

 * @pdirty: out parameter for number of dirty pages

 * @pwriteback: out parameter for number of pages under writeback

 *

 * Determine the numbers of file, headroom, dirty, and writeback pages in

 * @wb's memcg.  File, dirty and writeback are self-explanatory.  Headroom

 * is a bit more involved.

 *

 * A memcg's headroom is "min(max, high) - used".  In the hierarchy, the

 * headroom is calculated as the lowest headroom of itself and the

 * ancestors.  Note that this doesn't consider the actual amount of

 * available memory in the system.  The caller should further cap

 * *@pheadroom accordingly.

/*

 * Foreign dirty flushing

 *

 * There's an inherent mismatch between memcg and writeback.  The former

 * tracks ownership per-page while the latter per-inode.  This was a

 * deliberate design decision because honoring per-page ownership in the

 * writeback path is complicated, may lead to higher CPU and IO overheads

 * and deemed unnecessary given that write-sharing an inode across

 * different cgroups isn't a common use-case.

 *

 * Combined with inode majority-writer ownership switching, this works well

 * enough in most cases but there are some pathological cases.  For

 * example, let's say there are two cgroups A and B which keep writing to

 * different but confined parts of the same inode.  B owns the inode and

 * A's memory is limited far below B's.  A's dirty ratio can rise enough to

 * trigger balance_dirty_pages() sleeps but B's can be low enough to avoid

 * triggering background writeback.  A will be slowed down without a way to

 * make writeback of the dirty pages happen.

 *

 * Conditions like the above can lead to a cgroup getting repeatedly and

 * severely throttled after making some progress after each

 * dirty_expire_interval while the underlying IO device is almost

 * completely idle.

 *

 * Solving this problem completely requires matching the ownership tracking

 * granularities between memcg and writeback in either direction.  However,

 * the more egregious behaviors can be avoided by simply remembering the

 * most recent foreign dirtying events and initiating remote flushes on

 * them when local writeback isn't enough to keep the memory clean enough.

 *

 * The following two functions implement such mechanism.  When a foreign

 * page - a page whose memcg and writeback ownerships don't match - is

 * dirtied, mem_cgroup_track_foreign_dirty() records the inode owning

 * bdi_writeback on the page owning memcg.  When balance_dirty_pages()

 * decides that the memcg needs to sleep due to high dirty ratio, it calls

 * mem_cgroup_flush_foreign() which queues writeback on the recorded

 * foreign bdi_writebacks which haven't expired.  Both the numbers of

 * recorded bdi_writebacks and concurrent in-flight foreign writebacks are

 * limited to MEMCG_CGWB_FRN_CNT.

 *

 * The mechanism only remembers IDs and doesn't hold any object references.

 * As being wrong occasionally doesn't matter, updates and accesses to the

 * records are lockless and racy.

	/*

	 * Pick the slot to use.  If there is already a slot for @wb, keep

	 * using it.  If not replace the oldest one which isn't being

	 * written out.

		/*

		 * Re-using an existing one.  Update timestamp lazily to

		 * avoid making the cacheline hot.  We want them to be

		 * reasonably up-to-date and significantly shorter than

		 * dirty_expire_interval as that's what expires the record.

		 * Use the shorter of 1s and dirty_expire_interval / 8.

 replace the oldest free one */

 issue foreign writeback flushes for recorded foreign dirtying events */

		/*

		 * If the record is older than dirty_expire_interval,

		 * writeback on it has already started.  No need to kick it

		 * off again.  Also, don't start a new one if there's

		 * already one in flight.

 CONFIG_CGROUP_WRITEBACK */

 CONFIG_CGROUP_WRITEBACK */

/*

 * DO NOT USE IN NEW FILES.

 *

 * "cgroup.event_control" implementation.

 *

 * This is way over-engineered.  It tries to support fully configurable

 * events for each user.  Such level of flexibility is completely

 * unnecessary especially in the light of the planned unified hierarchy.

 *

 * Please deprecate this and replace with something simpler if at all

 * possible.

/*

 * Unregister event and free resources.

 *

 * Gets called from workqueue.

 Notify userspace the event is going away. */

/*

 * Gets called on EPOLLHUP on eventfd when user closes it.

 *

 * Called with wqh->lock held and interrupts disabled.

		/*

		 * If the event has been detached at cgroup removal, we

		 * can simply return knowing the other side will cleanup

		 * for us.

		 *

		 * We can't race against event freeing since the other

		 * side will require wqh->lock via remove_wait_queue(),

		 * which we hold.

			/*

			 * We are in atomic context, but cgroup_event_remove()

			 * may sleep, so we have to call it in workqueue.

/*

 * DO NOT USE IN NEW FILES.

 *

 * Parse input and register new cgroup event handler.

 *

 * Input must be in format '<event_fd> <control_fd> <args>'.

 * Interpretation of args is defined by control file implementation.

 the process need read permission on control file */

 AV: shouldn't we check that it's been opened for read instead? */

	/*

	 * Determine the event callbacks and set them in @event.  This used

	 * to be done via struct cftype but cgroup core no longer knows

	 * about these events.  The following is crude but the whole thing

	 * is for compatibility anyway.

	 *

	 * DO NOT ADD NEW FILES.

	/*

	 * Verify @cfile should belong to @css.  Also, remaining events are

	 * automatically removed on cgroup destruction but the removal is

	 * asynchronous, so take an extra ref on @css.

 XXX: for compat */

 terminate */

/*

 * Private memory cgroup IDR

 *

 * Swap-out records and page cache shadow entries need to store memcg

 * references in constrained space, so we maintain an ID space that is

 * limited to 16 bit (MEM_CGROUP_ID_MAX), limiting the total number of

 * memory-controlled cgroups to 64k.

 *

 * However, there usually are many references to the offline CSS after

 * the cgroup has been destroyed, such as page cache or reclaimable

 * slab objects, that don't need to hang on to the ID. We want to keep

 * those dead CSS from occupying IDs, or we might quickly exhaust the

 * relatively small ID space and prevent the creation of new cgroups

 * even when there are much fewer than 64k cgroups - possibly none.

 *

 * Maintain a private 16-bit ID space for memcg, and allow the ID to

 * be freed and recycled when it's no longer needed, which is usually

 * when the CSS is offlined.

 *

 * The only exception to that are records of swapped out tmpfs/shmem

 * pages that need to be attributed to live ancestors on swapin. But

 * those references are manageable from userspace.

 Memcg ID pins CSS */

/**

 * mem_cgroup_from_id - look up a memcg from a memcg id

 * @id: the memcg id to look up

 *

 * Caller must hold rcu_read_lock().

	/*

	 * This routine is called against possible nodes.

	 * But it's BUG to call kmalloc() against offline node.

	 *

	 * TODO: this routine can waste much memory for nodes which will

	 *       never be onlined. It's better to use memory hotplug callback

	 *       function.

 The following stuff does not apply to the root */

	/*

	 * A memcg must be visible for expand_shrinker_info()

	 * by the time the maps are allocated. So, we allocate maps

	 * here, when for_each_mem_cgroup() can't skip it.

 Online state pins memcg ID, memcg ID pins CSS */

	/*

	 * Unregister events and notify userspace.

	 * Notify userspace about cgroup removing only after rmdir of cgroup

	 * directory to avoid race between userspace and kernelspace.

 Need to offline kmem if online_css() fails */

/**

 * mem_cgroup_css_reset - reset the states of a mem_cgroup

 * @css: the target css

 *

 * Reset the states of the mem_cgroup associated with @css.  This is

 * invoked when the userland requests disabling on the default hierarchy

 * but the memcg is pinned through dependency.  The memcg should stop

 * applying policies and should revert to the vanilla state as it may be

 * made visible again.

 *

 * The current implementation only resets the essential configurations.

 * This needs to be expanded to cover all the visible parts.

		/*

		 * Collect the aggregated propagation counts of groups

		 * below us. We're in a per-cpu loop here and this is

		 * a global counter, so the first cycle will get them.

 Add CPU changes on this level since the last flush */

 Aggregate counts on this level and propagate upwards */

 Handlers for move charge at task migration. */

 Try a single bulk charge without reclaim first, kswapd may wake */

 Try charges one by one with reclaim, but do not retry */

	/*

	 * Handle MEMORY_DEVICE_PRIVATE which are ZONE_DEVICE page belonging to

	 * a device and because they are not accessible by CPU they are store

	 * as special swap entry in the CPU page table.

		/*

		 * MEMORY_DEVICE_PRIVATE means ZONE_DEVICE page and which have

		 * a refcount of 1 when free (unlike normal page)

	/*

	 * Because lookup_swap_cache() updates some statistics counter,

	 * we call find_get_page() with swapper_space directly.

 anonymous vma */

 page is moved even if it's not RSS of this task(page-faulted). */

 shmem/tmpfs may report page out on swap: account for that too. */

/**

 * mem_cgroup_move_account - move account of the page

 * @page: the page

 * @compound: charge the page as compound or small page

 * @from: mem_cgroup which the page is moved from.

 * @to:	mem_cgroup which the page is moved to. @from != @to.

 *

 * The caller must make sure the page is not on LRU (isolate_page() is useful.)

 *

 * This function doesn't do "charge" to new cgroup and doesn't do "uncharge"

 * from old cgroup.

	/*

	 * Prevent mem_cgroup_migrate() from looking at

	 * page's memory cgroup of its source page while we change it.

	/*

	 * All state has been migrated, let's switch to the new memcg.

	 *

	 * It is safe to change page's memcg here because the page

	 * is referenced, charged, isolated, and locked: we can't race

	 * with (un)charging, migration, LRU putback, or anything else

	 * that would rely on a stable page's memory cgroup.

	 *

	 * Note that lock_page_memcg is a memcg lock, not a page lock,

	 * to save space. As soon as we switch page's memory cgroup to a

	 * new memcg that isn't locked, the above state can change

	 * concurrently again. Make sure we're truly done with it.

/**

 * get_mctgt_type - get target type of moving charge

 * @vma: the vma the pte to be checked belongs

 * @addr: the address corresponding to the pte to be checked

 * @ptent: the pte to be checked

 * @target: the pointer the target page or swap ent will be stored(can be NULL)

 *

 * Returns

 *   0(MC_TARGET_NONE): if the pte is not a target for move charge.

 *   1(MC_TARGET_PAGE): if the page corresponding to this pte is a target for

 *     move charge. if @target is not NULL, the page is stored in target->page

 *     with extra refcnt got(Callers should handle it).

 *   2(MC_TARGET_SWAP): if the swap entry corresponding to this pte is a

 *     target for charge migration. if @target is not NULL, the entry is stored

 *     in target->ent.

 *   3(MC_TARGET_DEVICE): like MC_TARGET_PAGE  but page is MEMORY_DEVICE_PRIVATE

 *     (so ZONE_DEVICE page and thus not on the lru).

 *     For now we such page is charge like a regular page would be as for all

 *     intent and purposes it is just special memory taking the place of a

 *     regular page.

 *

 *     See Documentations/vm/hmm.txt and include/linux/hmm.h

 *

 * Called with pte lock held.

		/*

		 * Do only loose check w/o serialization.

		 * mem_cgroup_move_account() checks the page is valid or

		 * not under LRU exclusion.

	/*

	 * There is a swap entry and a page doesn't exist or isn't charged.

	 * But we cannot move a tail-page in a THP.

/*

 * We don't consider PMD mapped swapping or file mapped pages because THP does

 * not support them for now.

 * Caller should make sure that pmd_trans_huge(pmd) is true.

		/*

		 * Note their can not be MC_TARGET_DEVICE for now as we do not

		 * support transparent huge page with MEMORY_DEVICE_PRIVATE but

		 * this might change.

 increment precharge temporarily */

 cancels all extra charges on mc.from and mc.to, and wakes up all waiters. */

 we must uncharge all the leftover precharges from mc.to */

	/*

	 * we didn't uncharge from mc.from at mem_cgroup_move_account(), so

	 * we must uncharge here.

 we must fixup refcnts and charges */

 uncharge swap account from the old cgroup */

		/*

		 * we charged both to->memory and to->memsw, so we

		 * should uncharge to->memory.

	/*

	 * we must clear moving_task before waking up waiters at the end of

	 * task migration.

 unneeded init to make gcc happy */

 charge immigration isn't supported on the default hierarchy */

	/*

	 * Multi-process migrations only happen on the default hierarchy

	 * where charge immigration is not used.  Perform charge

	 * immigration if @tset contains a leader and whine if there are

	 * multiple.

	/*

	 * We are now committed to this value whatever it is. Changes in this

	 * tunable will only affect upcoming migrations, not the current one.

	 * So we need to save it, and keep it going.

 We move charges only when we move a owner of the mm */

 We set mc.moving_task later */

			/*

			 * We can have a part of the split pmd here. Moving it

			 * can be done but it would be too convoluted so simply

			 * ignore such a partial THP and keep it in original

			 * memcg. There should be somebody mapping the head.

 we uncharge from mc.from later. */

 get_mctgt_type() gets the page */

 we fixup other refcnts and charges later. */

		/*

		 * We have consumed all precharges we got in can_attach().

		 * We try charge one by one, but don't do any additional

		 * charges to mc.to if we have failed in charge once in attach()

		 * phase.

	/*

	 * Signal lock_page_memcg() to take the memcg's move_lock

	 * while we're moving its pages to another memcg. Then wait

	 * for already started RCU-only updates to finish.

		/*

		 * Someone who are holding the mmap_lock might be waiting in

		 * waitq. So we cancel all extra charges, wake up all waiters,

		 * and retry. Because we cancel precharges, we might not be able

		 * to move enough charges, but moving charge is a best-effort

		 * feature anyway, so it wouldn't be a big problem.

	/*

	 * When we have consumed all precharges and failed in doing

	 * additional charge, the page walk just aborts.

 !CONFIG_MMU */

 terminate */

/*

 * This function calculates an individual cgroup's effective

 * protection which is derived from its own memory.min/low, its

 * parent's and siblings' settings, as well as the actual memory

 * distribution in the tree.

 *

 * The following rules apply to the effective protection values:

 *

 * 1. At the first level of reclaim, effective protection is equal to

 *    the declared protection in memory.min and memory.low.

 *

 * 2. To enable safe delegation of the protection configuration, at

 *    subsequent levels the effective protection is capped to the

 *    parent's effective protection.

 *

 * 3. To make complex and dynamic subtrees easier to configure, the

 *    user is allowed to overcommit the declared protection at a given

 *    level. If that is the case, the parent's effective protection is

 *    distributed to the children in proportion to how much protection

 *    they have declared and how much of it they are utilizing.

 *

 *    This makes distribution proportional, but also work-conserving:

 *    if one cgroup claims much more protection than it uses memory,

 *    the unused remainder is available to its siblings.

 *

 * 4. Conversely, when the declared protection is undercommitted at a

 *    given level, the distribution of the larger parental protection

 *    budget is NOT proportional. A cgroup's protection from a sibling

 *    is capped to its own memory.min/low setting.

 *

 * 5. However, to allow protecting recursive subtrees from each other

 *    without having to declare each individual cgroup's fixed share

 *    of the ancestor's claim to protection, any unutilized -

 *    "floating" - protection from up the tree is distributed in

 *    proportion to each cgroup's *usage*. This makes the protection

 *    neutral wrt sibling cgroups and lets them compete freely over

 *    the shared parental protection budget, but it protects the

 *    subtree as a whole from neighboring subtrees.

 *

 * Note that 4. and 5. are not in conflict: 4. is about protecting

 * against immediate siblings whereas 5. is about protecting against

 * neighboring subtrees.

	/*

	 * If all cgroups at this level combined claim and use more

	 * protection then what the parent affords them, distribute

	 * shares in proportion to utilization.

	 *

	 * We are using actual utilization rather than the statically

	 * claimed protection in order to be work-conserving: claimed

	 * but unused protection is available to siblings that would

	 * otherwise get a smaller chunk than what they claimed.

	/*

	 * Ok, utilized protection of all children is within what the

	 * parent affords them, so we know whatever this child claims

	 * and utilizes is effectively protected.

	 *

	 * If there is unprotected usage beyond this value, reclaim

	 * will apply pressure in proportion to that amount.

	 *

	 * If there is unutilized protection, the cgroup will be fully

	 * shielded from reclaim, but we do return a smaller value for

	 * protection than what the group could enjoy in theory. This

	 * is okay. With the overcommit distribution above, effective

	 * protection is always dependent on how memory is actually

	 * consumed among the siblings anyway.

	/*

	 * If the children aren't claiming (all of) the protection

	 * afforded to them by the parent, distribute the remainder in

	 * proportion to the (unprotected) memory of each cgroup. That

	 * way, cgroups that aren't explicitly prioritized wrt each

	 * other compete freely over the allowance, but they are

	 * collectively protected from neighboring trees.

	 *

	 * We're using unprotected memory for the weight so that if

	 * some cgroups DO claim explicit protection, we don't protect

	 * the same bytes twice.

	 *

	 * Check both usage and parent_usage against the respective

	 * protected values. One should imply the other, but they

	 * aren't read atomically - make sure the division is sane.

/**

 * mem_cgroup_calculate_protection - check if memory consumption is in the normal range

 * @root: the top ancestor of the sub-tree being checked

 * @memcg: the memory cgroup to check

 *

 * WARNING: This function is not stateless! It can only be used as part

 *          of a top-down tree iteration, not for isolated queries.

	/*

	 * Effective values of the reclaim targets are ignored so they

	 * can be stale. Have a look at mem_cgroup_protection for more

	 * details.

	 * TODO: calculation should be more robust so that we do not need

	 * that special casing.

 No parent means a non-hierarchical mode on v1 memcg */

/**

 * mem_cgroup_swapin_charge_page - charge a newly allocated page for swapin

 * @page: page to charge

 * @mm: mm context of the victim

 * @gfp: reclaim mode

 * @entry: swap entry for which the page is allocated

 *

 * This function charges a page allocated for swapin. Please call this before

 * adding the page to the swapcache.

 *

 * Returns 0 on success. Otherwise, an error code is returned.

/*

 * mem_cgroup_swapin_uncharge_swap - uncharge swap slot

 * @entry: swap entry for which the page is charged

 *

 * Call this function after successfully adding the charged page to swapcache.

 *

 * Note: This function assumes the page for which swap slot is being uncharged

 * is order 0 page.

	/*

	 * Cgroup1's unified memory+swap counter has been charged with the

	 * new swapcache page, finish the transfer by uncharging the swap

	 * slot. The swap slot would also get uncharged when it dies, but

	 * it can stick around indefinitely and we'd count the page twice

	 * the entire time.

	 *

	 * Cgroup2 has separate resource counters for memory and swap,

	 * so this is a non-issue here. Memory and swap charge lifetimes

	 * correspond 1:1 to page and swap slot lifetimes: we charge the

	 * page to memory here, and uncharge swap when the slot is freed.

		/*

		 * The swap entry might not get freed for a long time,

		 * let's not wait for it.  The page already received a

		 * memory+swap charge, drop the swap entry duplicate.

 drop reference from uncharge_folio */

	/*

	 * Nobody should be changing or seriously looking at

	 * folio memcg or objcg at this point, we have fully

	 * exclusive access to the folio.

		/*

		 * This get matches the put at the end of the function and

		 * kmem pages do not hold memcg references anymore.

 pairs with css_put in uncharge_batch */

 LRU pages aren't accounted at the root level */

 Don't touch folio->lru of any random page, pre-check: */

/**

 * __mem_cgroup_uncharge_list - uncharge a list of page

 * @page_list: list of pages to uncharge

 *

 * Uncharge a list of pages previously charged with

 * __mem_cgroup_charge().

/**

 * mem_cgroup_migrate - Charge a folio's replacement.

 * @old: Currently circulating folio.

 * @new: Replacement folio.

 *

 * Charge @new as a replacement folio for @old. @old will

 * be uncharged upon free.

 *

 * Both folios must be locked, @new->mapping must be set up.

 Page cache replacement: new folio already charged? */

 Force-charge the new page. The old one will be freed soon */

 Do not associate the sock with unrelated interrupted task's memcg. */

/**

 * mem_cgroup_charge_skmem - charge socket memory

 * @memcg: memcg to charge

 * @nr_pages: number of pages to charge

 * @gfp_mask: reclaim mode

 *

 * Charges @nr_pages to @memcg. Returns %true if the charge fit within

 * @memcg's configured limit, %false if it doesn't.

/**

 * mem_cgroup_uncharge_skmem - uncharge socket memory

 * @memcg: memcg to uncharge

 * @nr_pages: number of pages to uncharge

/*

 * subsys_initcall() for memory controller.

 *

 * Some parts like memcg_hotplug_cpu_dead() have to be initialized from this

 * context because of lock dependencies (cgroup_lock -> cpu hotplug) but

 * basically everything that doesn't depend on a specific mem_cgroup structure

 * should be initialized from here.

	/*

	 * Currently s32 type (can refer to struct batched_lruvec_stat) is

	 * used for per-memcg-per-cpu caching of per-node statistics. In order

	 * to work fine, we should make sure that the overfill threshold can't

	 * exceed S32_MAX / PAGE_SIZE.

		/*

		 * The root cgroup cannot be destroyed, so it's refcount must

		 * always be >= 1.

/**

 * mem_cgroup_swapout - transfer a memsw charge to swap

 * @page: page whose memsw charge to transfer

 * @entry: swap entry to move the charge to

 *

 * Transfer the memsw charge of @page to @entry.

	/*

	 * In case the memcg owning these pages has been offlined and doesn't

	 * have an ID allocated to it anymore, charge the closest online

	 * ancestor for the swap instead and transfer the memory+swap charge.

 Get references for the tail pages, too */

	/*

	 * Interrupts should be disabled here because the caller holds the

	 * i_pages lock which is taken with interrupts-off. It is

	 * important here to have the interrupts disabled because it is the

	 * only synchronisation we have for updating the per-CPU variables.

/**

 * __mem_cgroup_try_charge_swap - try charging swap space for a page

 * @page: page being added to swap

 * @entry: swap entry to charge

 *

 * Try to charge @page's memcg for the swap space at @entry.

 *

 * Returns 0 on success, -ENOMEM on failure.

 Get references for the tail pages, too */

/**

 * __mem_cgroup_uncharge_swap - uncharge swap space

 * @entry: swap entry to uncharge

 * @nr_pages: the amount of swap space to uncharge

 terminate */

 terminate */

/*

 * If mem_cgroup_swap_init() is implemented as a subsys_initcall()

 * instead of a core_initcall(), this could mean cgroup_memory_noswap still

 * remains set to false even when memcg is disabled via "cgroup_disable=memory"

 * boot parameter. This may result in premature OOPS inside

 * mem_cgroup_get_nr_swap_pages() function in corner cases.

 No memory control -> no swap control */

 CONFIG_MEMCG_SWAP */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/kmemleak.c

 *

 * Copyright (C) 2008 ARM Limited

 * Written by Catalin Marinas <catalin.marinas@arm.com>

 *

 * For more information on the algorithm and kmemleak usage, please see

 * Documentation/dev-tools/kmemleak.rst.

 *

 * Notes on locking

 * ----------------

 *

 * The following locks and mutexes are used by kmemleak:

 *

 * - kmemleak_lock (raw_spinlock_t): protects the object_list modifications and

 *   accesses to the object_tree_root. The object_list is the main list

 *   holding the metadata (struct kmemleak_object) for the allocated memory

 *   blocks. The object_tree_root is a red black tree used to look-up

 *   metadata based on a pointer to the corresponding memory block.  The

 *   kmemleak_object structures are added to the object_list and

 *   object_tree_root in the create_object() function called from the

 *   kmemleak_alloc() callback and removed in delete_object() called from the

 *   kmemleak_free() callback

 * - kmemleak_object.lock (raw_spinlock_t): protects a kmemleak_object.

 *   Accesses to the metadata (e.g. count) are protected by this lock. Note

 *   that some members of this structure may be protected by other means

 *   (atomic or kmemleak_lock). This lock is also held when scanning the

 *   corresponding memory block to avoid the kernel freeing it via the

 *   kmemleak_free() callback. This is less heavyweight than holding a global

 *   lock like kmemleak_lock during scanning.

 * - scan_mutex (mutex): ensures that only one thread may scan the memory for

 *   unreferenced objects at a time. The gray_list contains the objects which

 *   are already referenced or marked as false positives and need to be

 *   scanned. This list is only modified during a scanning episode when the

 *   scan_mutex is held. At the end of a scan, the gray_list is always empty.

 *   Note that the kmemleak_object.use_count is incremented when an object is

 *   added to the gray_list and therefore cannot be freed. This mutex also

 *   prevents multiple users of the "kmemleak" debugfs file together with

 *   modifications to the memory scanning parameters including the scan_thread

 *   pointer

 *

 * Locks and mutexes are acquired/nested in the following order:

 *

 *   scan_mutex [-> object->lock] -> kmemleak_lock -> other_object->lock (SINGLE_DEPTH_NESTING)

 *

 * No kmemleak_lock and object->lock nesting is allowed outside scan_mutex

 * regions.

 *

 * The kmemleak_object structures have a use_count incremented or decremented

 * using the get_object()/put_object() functions. When the use_count becomes

 * 0, this count can no longer be incremented and put_object() schedules the

 * kmemleak_object freeing via an RCU callback. All calls to the get_object()

 * function must be protected by rcu_read_lock() to avoid accessing a freed

 * structure.

/*

 * Kmemleak configuration and common defines.

 stack trace length */

 minimum object age for reporting */

 delay before the first scan */

 subsequent auto scanning delay */

 maximum size of a scanned block */

 GFP bitmask for kmemleak internal allocations */

 scanning area inside a memory block */

/*

 * Structure holding the metadata for each allocated memory block.

 * Modifications to such objects should be made while holding the

 * object->lock. Insertions or deletions from object_list, gray_list or

 * rb_node are already protected by the corresponding locks or mutex (see

 * the notes on locking above). These objects are reference-counted

 * (use_count) and freed using the RCU mechanism.

 object status flags */

 object_list lockless traversal */

 object usage count; object freed when use_count == 0 */

 pass surplus references to this pointer */

 minimum number of a pointers found before it is considered leak */

 the total number of pointers found pointing to this object */

 checksum for detecting modified objects */

 memory ranges to be scanned inside an object (empty for all) */

 creation timestamp */

 pid of the current task */

 executable name */

 flag representing the memory block allocation status */

 flag set after the first reporting of an unreference object */

 flag set to not scan the object */

 flag set to fully scan the object when scan_area allocation failed */

 number of bytes to print per line; must be 16 or 32 */

 number of bytes to print at a time (1, 2, 4, 8) */

 include ASCII after the hex output */

 max number of lines to be printed */

 the list of all allocated objects */

 the list of gray-colored objects (see color_gray comment below) */

 memory pool allocation */

 search tree for object boundaries */

 protecting the access to object_list and object_tree_root */

 allocation caches for kmemleak internal data */

 set if tracing memory operations is enabled */

 same as above but only for the kmemleak_free() callback */

 set in the late_initcall if there were no errors */

 set if a kmemleak warning was issued */

 set if a fatal kmemleak error has occurred */

 minimum and maximum address that may be valid pointers */

 used to avoid reporting of recently allocated objects */

 delay between automatic memory scannings */

 enables or disables the task stacks scanning */

 protects the memory scanning, parameters and debug/kmemleak file access */

 setting kmemleak=on, will set this var, skipping the disable */

 If there are leaks that can be reported */

/*

 * Print a warning and dump the stack trace.

/*

 * Macro invoked when a serious kmemleak condition occurred and cannot be

 * recovered from. Kmemleak will be disabled and further allocation/freeing

 * tracing no longer available.

/*

 * Printing of the objects hex dump to the seq file. The number of lines to be

 * printed is limited to HEX_MAX_LINES to prevent seq file spamming. The

 * actual number of printed bytes depends on HEX_ROW_SIZE. It must be called

 * with the object->lock held.

 limit the number of lines to HEX_MAX_LINES */

/*

 * Object colors, encoded with count and min_count:

 * - white - orphan object, not enough references to it (count < min_count)

 * - gray  - not orphan, not marked as false positive (min_count == 0) or

 *		sufficient references to it (count >= min_count)

 * - black - ignore, it doesn't contain references (e.g. text section)

 *		(min_count == -1). No function defined for this color.

 * Newly created objects don't have any color assigned (object->count == -1)

 * before the next memory scan when they become white.

/*

 * Objects are considered unreferenced only if their color is white, they have

 * not be deleted and have a minimum age to avoid false positives caused by

 * pointers temporarily stored in CPU registers.

/*

 * Printing of the unreferenced objects information to the seq file. The

 * print_unreferenced function must be called with the object->lock held.

/*

 * Print the kmemleak_object information. This function is used mainly for

 * debugging special cases when kmemleak operations. It must be called with

 * the object->lock held.

/*

 * Look-up a memory block metadata (kmemleak_object) in the object search

 * tree based on a pointer value. If alias is 0, only values pointing to the

 * beginning of the memory block are allowed. The kmemleak_lock must be held

 * when calling this function.

/*

 * Increment the object use_count. Return 1 if successful or 0 otherwise. Note

 * that once an object's use_count reached 0, the RCU freeing was already

 * registered and the object should no longer be used. This function must be

 * called under the protection of rcu_read_lock().

/*

 * Memory pool allocation and freeing. kmemleak_lock must not be held.

 try the slab allocator first */

 slab allocation failed, try the memory pool */

/*

 * Return the object to either the slab allocator or the memory pool.

 add the object to the memory pool free list */

/*

 * RCU callback to free a kmemleak_object.

	/*

	 * Once use_count is 0 (guaranteed by put_object), there is no other

	 * code accessing this object, hence no need for locking.

/*

 * Decrement the object use_count. Once the count is 0, free the object using

 * an RCU callback. Since put_object() may be called via the kmemleak_free() ->

 * delete_object() path, the delayed RCU freeing ensures that there is no

 * recursive call to the kernel allocator. Lock-less RCU object_list traversal

 * is also possible.

 should only get here after delete_object was called */

	/*

	 * It may be too early for the RCU callbacks, however, there is no

	 * concurrent object_list traversal when !object_cache and all objects

	 * came from the memory pool. Free the object directly.

/*

 * Look up an object in the object search tree and increase its use_count.

 check whether the object is still available */

/*

 * Remove an object from the object_tree_root and object_list. Must be called

 * with the kmemleak_lock held _if_ kmemleak is still enabled.

/*

 * Look up an object in the object search tree and remove it from both

 * object_tree_root and object_list. The returned object's use_count should be

 * at least 1, as initially set by create_object().

/*

 * Save stack trace to the given array of MAX_TRACE size.

/*

 * Create the metadata (struct kmemleak_object) corresponding to an allocated

 * memory block and add it to the object_list and object_tree_root.

 white color initially */

 task information */

		/*

		 * There is a small chance of a race with set_task_comm(),

		 * however using get_task_comm() here may cause locking

		 * dependency issues with current->alloc_lock. In the worst

		 * case, the command line is not correct.

 kernel backtrace */

			/*

			 * No need for parent->lock here since "parent" cannot

			 * be freed while the kmemleak_lock is held.

/*

 * Mark the object as not allocated and schedule RCU freeing via put_object().

	/*

	 * Locking here also ensures that the corresponding memory block

	 * cannot be freed when it is being scanned.

/*

 * Look up the metadata (struct kmemleak_object) corresponding to ptr and

 * delete it.

/*

 * Look up the metadata (struct kmemleak_object) corresponding to ptr and

 * delete it. If the memory block is partially freed, the function may create

 * additional metadata for the remaining parts of the block.

	/*

	 * Create one or two objects that may result from the memory block

	 * split. Note that partial freeing is only done by free_bootmem() and

	 * this happens before kmemleak_init() is called.

/*

 * Mark an object permanently as gray-colored so that it can no longer be

 * reported as a leak. This is used in general to mark a false positive.

/*

 * Mark the object as black-colored so that it is ignored from scans and

 * reporting.

/*

 * Add a scanning area to the object. If at least one such area is added,

 * kmemleak will only scan these ranges rather than the whole memory block.

 mark the object for full scan to avoid false positives */

/*

 * Any surplus references (object already gray) to 'ptr' are passed to

 * 'excess_ref'. This is used in the vmalloc() case where a pointer to

 * vm_struct may be used as an alternative reference to the vmalloc'ed object

 * (see free_thread_stack()).

/*

 * Set the OBJECT_NO_SCAN flag for the object corresponding to the give

 * pointer. Such object will not be scanned by kmemleak but references to it

 * are searched.

/**

 * kmemleak_alloc - register a newly allocated object

 * @ptr:	pointer to beginning of the object

 * @size:	size of the object

 * @min_count:	minimum number of references to this object. If during memory

 *		scanning a number of references less than @min_count is found,

 *		the object is reported as a memory leak. If @min_count is 0,

 *		the object is never reported as a leak. If @min_count is -1,

 *		the object is ignored (not scanned and not reported as a leak)

 * @gfp:	kmalloc() flags used for kmemleak internal memory allocations

 *

 * This function is called from the kernel allocators when a new object

 * (memory block) is allocated (kmem_cache_alloc, kmalloc etc.).

/**

 * kmemleak_alloc_percpu - register a newly allocated __percpu object

 * @ptr:	__percpu pointer to beginning of the object

 * @size:	size of the object

 * @gfp:	flags used for kmemleak internal memory allocations

 *

 * This function is called from the kernel percpu allocator when a new object

 * (memory block) is allocated (alloc_percpu).

	/*

	 * Percpu allocations are only scanned and not reported as leaks

	 * (min_count is set to 0).

/**

 * kmemleak_vmalloc - register a newly vmalloc'ed object

 * @area:	pointer to vm_struct

 * @size:	size of the object

 * @gfp:	__vmalloc() flags used for kmemleak internal memory allocations

 *

 * This function is called from the vmalloc() kernel allocator when a new

 * object (memory block) is allocated.

	/*

	 * A min_count = 2 is needed because vm_struct contains a reference to

	 * the virtual address of the vmalloc'ed block.

/**

 * kmemleak_free - unregister a previously registered object

 * @ptr:	pointer to beginning of the object

 *

 * This function is called from the kernel allocators when an object (memory

 * block) is freed (kmem_cache_free, kfree, vfree etc.).

/**

 * kmemleak_free_part - partially unregister a previously registered object

 * @ptr:	pointer to the beginning or inside the object. This also

 *		represents the start of the range to be freed

 * @size:	size to be unregistered

 *

 * This function is called when only a part of a memory block is freed

 * (usually from the bootmem allocator).

/**

 * kmemleak_free_percpu - unregister a previously registered __percpu object

 * @ptr:	__percpu pointer to beginning of the object

 *

 * This function is called from the kernel percpu allocator when an object

 * (memory block) is freed (free_percpu).

/**

 * kmemleak_update_trace - update object allocation stack trace

 * @ptr:	pointer to beginning of the object

 *

 * Override the object allocation stack trace for cases where the actual

 * allocation place is not always useful.

/**

 * kmemleak_not_leak - mark an allocated object as false positive

 * @ptr:	pointer to beginning of the object

 *

 * Calling this function on an object will cause the memory block to no longer

 * be reported as leak and always be scanned.

/**

 * kmemleak_ignore - ignore an allocated object

 * @ptr:	pointer to beginning of the object

 *

 * Calling this function on an object will cause the memory block to be

 * ignored (not scanned and not reported as a leak). This is usually done when

 * it is known that the corresponding block is not a leak and does not contain

 * any references to other allocated memory blocks.

/**

 * kmemleak_scan_area - limit the range to be scanned in an allocated object

 * @ptr:	pointer to beginning or inside the object. This also

 *		represents the start of the scan area

 * @size:	size of the scan area

 * @gfp:	kmalloc() flags used for kmemleak internal memory allocations

 *

 * This function is used when it is known that only certain parts of an object

 * contain references to other objects. Kmemleak will only scan these areas

 * reducing the number false negatives.

/**

 * kmemleak_no_scan - do not scan an allocated object

 * @ptr:	pointer to beginning of the object

 *

 * This function notifies kmemleak not to scan the given memory block. Useful

 * in situations where it is known that the given object does not contain any

 * references to other objects. Kmemleak will not scan such objects reducing

 * the number of false negatives.

/**

 * kmemleak_alloc_phys - similar to kmemleak_alloc but taking a physical

 *			 address argument

 * @phys:	physical address of the object

 * @size:	size of the object

 * @min_count:	minimum number of references to this object.

 *              See kmemleak_alloc()

 * @gfp:	kmalloc() flags used for kmemleak internal memory allocations

/**

 * kmemleak_free_part_phys - similar to kmemleak_free_part but taking a

 *			     physical address argument

 * @phys:	physical address if the beginning or inside an object. This

 *		also represents the start of the range to be freed

 * @size:	size to be unregistered

/**

 * kmemleak_not_leak_phys - similar to kmemleak_not_leak but taking a physical

 *			    address argument

 * @phys:	physical address of the object

/**

 * kmemleak_ignore_phys - similar to kmemleak_ignore but taking a physical

 *			  address argument

 * @phys:	physical address of the object

/*

 * Update an object's checksum and return true if it was modified.

/*

 * Update an object's references. object->lock must be held by the caller.

 non-orphan, ignored or new */

	/*

	 * Increase the object's reference count (number of pointers to the

	 * memory block). If this count reaches the required minimum, the

	 * object's color will become gray and it will be added to the

	 * gray_list.

 put_object() called when removing from gray_list */

/*

 * Memory scanning is a long process and it needs to be interruptible. This

 * function checks whether such interrupt condition occurred.

	/*

	 * This function may be called from either process or kthread context,

	 * hence the need to check for both stop conditions.

/*

 * Scan a memory block (exclusive range) for valid pointers and add those

 * found to the gray list.

		/*

		 * No need for get_object() here since we hold kmemleak_lock.

		 * object->use_count cannot be dropped to 0 while the object

		 * is still present in object_tree_root and object_list

		 * (with updates protected by kmemleak_lock).

 self referenced, ignore */

		/*

		 * Avoid the lockdep recursive warning on object->lock being

		 * previously acquired in scan_object(). These locks are

		 * enclosed by scan_mutex.

 only pass surplus references (object already gray) */

 no need for update_refs() if object already gray */

 circular reference, ignore */

/*

 * Scan a large memory block in MAX_SCAN_SIZE chunks to reduce the latency.

/*

 * Scan a memory block corresponding to a kmemleak_object. A condition is

 * that object->use_count >= 1.

	/*

	 * Once the object->lock is acquired, the corresponding memory block

	 * cannot be freed (the same lock is acquired in delete_object).

 already freed object */

/*

 * Scan the objects already referenced (gray objects). More objects will be

 * referenced and, if there are no memory leaks, all the objects are scanned.

	/*

	 * The list traversal is safe for both tail additions and removals

	 * from inside the loop. The kmemleak objects cannot be freed from

	 * outside the loop because their use_count was incremented.

 may add new objects to the list */

 remove the object from the list and release it */

/*

 * Scan data sections and all the referenced memory blocks allocated via the

 * kernel's standard allocators. This function must be called with the

 * scan_mutex held.

 prepare the kmemleak_object's */

		/*

		 * With a few exceptions there should be a maximum of

		 * 1 reference to any object at this point.

 reset the reference count (whiten the object) */

 per-cpu sections scanning */

	/*

	 * Struct page scanning for each node.

 only scan pages belonging to this node */

 only scan if page is in use */

	/*

	 * Scanning the task stacks (may introduce false negatives).

	/*

	 * Scan the objects already referenced from the sections scanned

	 * above.

	/*

	 * Check for new or unreferenced objects modified since the previous

	 * scan and color them gray until the next scan.

 color it gray temporarily */

	/*

	 * Re-scan the gray list for modified unreferenced objects.

	/*

	 * If scanning was stopped do not report any new unreferenced objects.

	/*

	 * Scanning result reporting.

/*

 * Thread function performing automatic memory scanning. Unreferenced objects

 * at the end of a memory scan are reported but only the first time.

	/*

	 * Wait before the first scan to allow the system to fully initialize.

 wait before the next scan */

/*

 * Start the automatic memory scanning thread. This function must be called

 * with the scan_mutex held.

/*

 * Stop the automatic memory scanning thread.

/*

 * Iterate over the object_list and return the first valid object at or after

 * the required position with its use_count incremented. The function triggers

 * a memory scanning when the pos argument points to the first position.

/*

 * Return the next object in the object_list. The function decrements the

 * use_count of the previous object and increases that of the next one.

/*

 * Decrement the use_count of the last object required, if any.

		/*

		 * kmemleak_seq_start may return ERR_PTR if the scan_mutex

		 * waiting was interrupted, so only release it if !IS_ERR.

/*

 * Print the information for an unreferenced object to the seq file.

/*

 * We use grey instead of black to ensure we can do future scans on the same

 * objects. If we did not do future scans these black objects could

 * potentially contain references to newly allocated objects in the future and

 * we'd end up with false positives.

/*

 * File write operation to configure kmemleak at run-time. The following

 * commands can be written to the /sys/kernel/debug/kmemleak file:

 *   off	- disable kmemleak (irreversible)

 *   stack=on	- enable the task stacks scanning

 *   stack=off	- disable the tasks stacks scanning

 *   scan=on	- start the automatic memory scanning thread

 *   scan=off	- stop the automatic memory scanning thread

 *   scan=...	- set the automatic memory scanning period in seconds (0 to

 *		  disable it)

 *   scan	- trigger a memory scan

 *   clear	- mark all current reported unreferenced kmemleak objects as

 *		  grey to ignore printing them, or free all kmemleak objects

 *		  if kmemleak has been disabled.

 *   dump=...	- dump information about the object found at the given address

 ignore the rest of the buffer, only one command at a time */

	/*

	 * Kmemleak has already been disabled, no need for RCU list traversal

	 * or kmemleak_lock held.

/*

 * Stop the memory scanning thread and free the kmemleak internal objects if

 * no previous scan thread (otherwise, kmemleak may still have some useful

 * information on memory leaks).

	/*

	 * Once it is made sure that kmemleak_scan has stopped, it is safe to no

	 * longer track object freeing. Ordering of the scan thread stopping and

	 * the memory accesses below is guaranteed by the kthread_stop()

	 * function.

/*

 * Disable kmemleak. No memory allocation/freeing will be traced once this

 * function is called. Disabling kmemleak is an irreversible operation.

 atomically check whether it was already invoked */

 stop any memory operation tracing */

 check whether it is too early for a kernel thread */

/*

 * Allow boot-time kmemleak disabling (enabled by default).

/*

 * Kmemleak initialization.

 register the data/bss sections */

 only register .data..ro_after_init if not within .data */

/*

 * Late initialization function.

		/*

		 * Some error occurred and kmemleak was disabled. There is a

		 * small chance that kmemleak_disable() was called immediately

		 * after setting kmemleak_initialized and we may end up with

		 * two clean-up threads but serialized by scan_mutex.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/nommu.c

 *

 *  Replacement code for mm functions to support CPU's that don't

 *  have any form of memory management unit (thus no virtual memory).

 *

 *  See Documentation/admin-guide/mm/nommu-mmap.rst

 *

 *  Copyright (c) 2004-2008 David Howells <dhowells@redhat.com>

 *  Copyright (c) 2000-2003 David McCullough <davidm@snapgear.com>

 *  Copyright (c) 2000-2001 D Jeff Dionne <jeff@uClinux.org>

 *  Copyright (c) 2002      Greg Ungerer <gerg@snapgear.com>

 *  Copyright (c) 2007-2010 Paul Mundt <lethal@linux-sh.org>

 list of mapped, potentially shareable regions */

/*

 * Return the total memory allocated for this pointer, not

 * just what the caller asked for.

 *

 * Doesn't have to be accurate, i.e. may have races.

	/*

	 * If the object we have should not have ksize performed on it,

	 * return size of 0

	/*

	 * If the allocator sets PageSlab, we know the pointer came from

	 * kmalloc().

	/*

	 * If it's not a compound page, see if we have a matching VMA

	 * region. This test is intentionally done in reverse order,

	 * so if there's no VMA, we still fall through and hand back

	 * PAGE_SIZE for 0-order pages.

	/*

	 * The ksize() function is only guaranteed to work for pointers

	 * returned by kmalloc(). So handle arbitrary pointers here.

/**

 * follow_pfn - look up PFN at a user virtual address

 * @vma: memory mapping

 * @address: user virtual address

 * @pfn: location to store found PFN

 *

 * Only IO mappings and raw PFN mappings are allowed.

 *

 * Returns zero and the pfn at @pfn on success, -ve otherwise.

	/*

	 *  You can't specify __GFP_HIGHMEM with kmalloc() since kmalloc()

	 * returns only a logical address.

 Don't allow overflow */

/*

 *	vmalloc  -  allocate virtually contiguous memory

 *

 *	@size:		allocation size

 *

 *	Allocate enough pages to cover @size from the page level

 *	allocator and map them into contiguous kernel virtual space.

 *

 *	For tight control over page level allocator and protection flags

 *	use __vmalloc() instead.

/*

 *	vzalloc - allocate virtually contiguous memory with zero fill

 *

 *	@size:		allocation size

 *

 *	Allocate enough pages to cover @size from the page level

 *	allocator and map them into contiguous kernel virtual space.

 *	The memory allocated is set to zero.

 *

 *	For tight control over page level allocator and protection flags

 *	use __vmalloc() instead.

/**

 * vmalloc_node - allocate memory on a specific node

 * @size:	allocation size

 * @node:	numa node

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 *

 * For tight control over page level allocator and protection flags

 * use __vmalloc() instead.

/**

 * vzalloc_node - allocate memory on a specific node with zero fill

 * @size:	allocation size

 * @node:	numa node

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 * The memory allocated is set to zero.

 *

 * For tight control over page level allocator and protection flags

 * use __vmalloc() instead.

/**

 * vmalloc_32  -  allocate virtually contiguous memory (32bit addressable)

 *	@size:		allocation size

 *

 *	Allocate enough 32bit PA addressable pages to cover @size from the

 *	page level allocator and map them into contiguous kernel virtual space.

/**

 * vmalloc_32_user - allocate zeroed virtually contiguous 32bit memory

 *	@size:		allocation size

 *

 * The resulting memory area is 32bit addressable and zeroed so it can be

 * mapped to userspace without leaking data.

 *

 * VM_USERMAP is set on the corresponding VMA so that subsequent calls to

 * remap_vmalloc_range() are permissible.

	/*

	 * We'll have to sort out the ZONE_DMA bits for 64-bit,

	 * but for now this can simply use vmalloc_user() directly.

/*

 *  sys_brk() for the most part doesn't need the global kernel

 *  lock, except when an application is doing something nasty

 *  like trying to un-brk an area that has already been mapped

 *  to a regular file.  in this case, the unmapping will need

 *  to invoke file system routines that need the global lock.

	/*

	 * Always allow shrinking brk

	/*

	 * Ok, looks good - let it rip.

/*

 * initialise the percpu counter for VM and region record slabs

/*

 * validate the region tree

 * - the caller must hold the region lock

/*

 * add a region into the global tree

/*

 * delete a region from the global tree

/*

 * free a contiguous series of pages

/*

 * release a reference to a region

 * - the caller must hold the region semaphore for writing, which this releases

 * - the region may not have been added to the tree yet, in which case vm_top

 *   will equal vm_start

		/* IO memory and memory shared directly out of the pagecache

/*

 * release a reference to a region

/*

 * add a VMA into a process's mm_struct in the appropriate place in the list

 * and tree and add to the address space's page tree also if not an anonymous

 * page

 * - should be called with mm->mmap_lock held writelocked

 add the VMA to the mapping */

 add the VMA to the tree */

		/* sort by: start addr, end addr, VMA struct addr in that order

 add VMA to the VMA list also */

/*

 * delete a VMA from its owning mm_struct and address space

 if the vma is cached, invalidate the entire cache */

 remove the VMA from the mapping */

 remove from the MM's tree and list */

/*

 * destroy a VMA record

/*

 * look up the first VMA in which addr resides, NULL if none

 * - should be called with mm->mmap_lock at least held readlocked

 check the cache first */

	/* trawl the list (there may be multiple mappings in which addr

/*

 * find a VMA

 * - we don't extend stack VMAs under NOMMU conditions

/*

 * expand a stack to a given address

 * - not supported under NOMMU conditions

/*

 * look up the first VMA exactly that exactly matches addr

 * - should be called with mm->mmap_lock at least held readlocked

 check the cache first */

	/* trawl the list (there may be multiple mappings in which addr

/*

 * determine whether a mapping should be permitted and, if so, what sort of

 * mapping we're capable of supporting

 do the simple checks first */

 Careful about overflows.. */

 offset overflow? */

 files must support mmap */

		/* work out if what we've got could possibly be shared

		 * - we support chardevs that provide their own "memory"

		 * - we support files/blockdevs that are memory backed

			/* no explicit capabilities set, so assume some

		/* eliminate any capabilities that we can't support on this

 The file shall have been opened with read permission. */

 do checks for writing, appending and locking */

 we mustn't privatise shared mappings */

			/* we're going to read the file into private memory we

			/* we don't permit a private writable mapping to be

		/* handle executable mappings and implied executable

 handle implication of PROT_EXEC by PROT_READ */

 backing file is not executable, try to copy */

		/* anonymous mappings are always memory backed and can be

		 * privately mapped

 handle PROT_EXEC implication by PROT_READ */

 allow the security API to have its say */

 looks okay */

/*

 * we've determined that we can make the mapping, now translate what we

 * now know into VMA flags

 vm_flags |= mm->def_flags; */

 attempt to share read-only copies of mapped file chunks */

		/* overlay a shareable mapping on the backing device or inode

		 * if possible - used for chardevs, ramfs/tmpfs/shmfs and

	/* refuse to let anyone share private mappings with this process if

	 * it's being traced - otherwise breakpoints set in it may interfere

	 * with another untraced process

/*

 * set up a shared mapping on a file (the driver or filesystem provides and

 * pins the storage)

	/* getting -ENOSYS indicates that direct mmap isn't possible (as

	 * opposed to tried but failed) so we can only give a suitable error as

/*

 * set up a private mapping or an anonymous shared mapping

	/* invoke the file's mapping function so that it can keep track of

	 * shared mappings on devices or memory

	 * - VM_MAYSHARE will be set if it may attempt to share

 shouldn't return success if we're not sharing */

		/* getting an ENOSYS error indicates that direct mmap isn't

		 * possible (as opposed to tried but failed) so we'll try to

	/* allocate some memory to hold the mapping

	 * - note that this may not return a page-aligned address if the object

	 *   we're allocating is smaller than a page

 we don't want to allocate a power-of-2 sized page set */

 read the contents of a file into the copy */

 clear the last little bit */

/*

 * handle mapping creation for uClinux

	/* decide whether we should attempt the mapping, and if so what sort of

 we ignore the address hint */

	/* we've determined that we can make the mapping, now translate what we

 we're going to need to record the mapping */

	/* if we want to share, we need to check for regions created by other

	 * mmap() calls that overlap with our proposed mapping

	 * - we can only share with a superset match on most regular files

	 * - shared mappings on character devices and memory backed files are

	 *   permitted to overlap inexactly as far as we are concerned for in

	 *   these cases, sharing is handled in the driver or filesystem rather

	 *   than here

 search for overlapping mappings on the same file */

			/* handle inexactly overlapping matches between

 new mapping is not a subset of the region */

 we've found a region we can share */

		/* obtain the address at which to make a shared mapping

		 * - this is the hook for quasi-memory character devices to

		 *   tell us the location of a shared mapping

				/* the driver refused to tell us where to site

				 * the mapping so we'll have to attempt to copy

	/* set up the mapping

	 * - the region is filled in if NOMMU_MAP_DIRECT is still set

 clear anonymous mappings that don't ask for uninitialized data */

 okay... we have a mapping; now we have to register it */

	/* we flush the region from the icache only when the first executable

 __ARCH_WANT_SYS_OLD_MMAP */

/*

 * split a vma into two pieces at address 'addr', a new vma is allocated either

 * for the first part or the tail.

	/* we're only permitted to split anonymous regions (these should have

 most fields are the same, copy all, and then fixup */

/*

 * shrink a VMA by removing the specified chunk from either the beginning or

 * the end

	/* adjust the VMA's pointers, which may reposition it in the MM's tree

 cut the backing region down to size */

/*

 * release a mapping

 * - under NOMMU conditions the chunk to be unmapped must be backed by a single

 *   VMA, though it need not cover the whole VMA

 find the first potentially overlapping VMA */

 we're allowed to split an anonymous VMA but not a file-backed one */

 the chunk must be a subset of the VMA found */

/*

 * release all the mappings made in a process's VM space

/*

 * expand (or shrink) an existing mapping, potentially moving it at the same

 * time (controlled by the MREMAP_MAYMOVE flag and available VM space)

 *

 * under NOMMU conditions, we only permit changing a mapping's size, and only

 * as long as it stays within the region allocated by do_mmap_private() and the

 * block is not shareable

 *

 * MREMAP_FIXED is not supported under NOMMU conditions

 insanity checks first */

 all checks complete - do it */

 the access must start within one of the target process's mappings */

 don't overrun this mapping */

 only read or write mappings where it is permitted */

/**

 * access_remote_vm - access another process' address space

 * @mm:		the mm_struct of the target address space

 * @addr:	start address to access

 * @buf:	source or destination buffer

 * @len:	number of bytes to transfer

 * @gup_flags:	flags modifying lookup behaviour

 *

 * The caller must hold a reference on @mm.

/*

 * Access another process' address space.

 * - source/target buffer must be kernel space

/**

 * nommu_shrink_inode_mappings - Shrink the shared mappings on an inode

 * @inode: The inode to check

 * @size: The current filesize of the inode

 * @newsize: The proposed filesize of the inode

 *

 * Check the shared mappings on an inode on behalf of a shrinking truncate to

 * make sure that any outstanding VMAs aren't broken and then shrink the

 * vm_regions that extend beyond so that do_mmap() doesn't

 * automatically grant mappings that are too large.

 search for VMAs that fall within the dead zone */

		/* found one - only interested if it's shared out of the page

 not quite true, but near enough */

	/* reduce any regions that overlap the dead zone - if in existence,

	 * these will be pointed to by VMAs that don't overlap the dead zone

	 *

	 * we don't check for any regions that start beyond the EOF as there

	 * shouldn't be any

/*

 * Initialise sysctl_user_reserve_kbytes.

 *

 * This is intended to prevent a user from starting a single memory hogging

 * process, such that they cannot recover (kill the hog) in OVERCOMMIT_NEVER

 * mode.

 *

 * The default value is min(3% of free memory, 128MB)

 * 128MB is enough to recover with sshd/login, bash, and top/kill.

/*

 * Initialise sysctl_admin_reserve_kbytes.

 *

 * The purpose of sysctl_admin_reserve_kbytes is to allow the sys admin

 * to log in and kill a memory hogging process.

 *

 * Systems with more than 256MB will reserve 8MB, enough to recover

 * with sshd, bash, and top in OVERCOMMIT_GUESS. Smaller systems will

 * only reserve 3% of free pages by default.

 SPDX-License-Identifier: GPL-2.0

/*

 * Slab allocator functions that are independent of the allocator strategy

 *

 * (C) 2012 Christoph Lameter <cl@linux.com>

/*

 * Set of flags that will prevent slab merging

/*

 * Merge control. If this is set then no merging of slab caches will occur.

/*

 * Determine the size of a slab object

 It confuses parsers */

/*

 * Figure out what the alignment of the objects will be given a set of

 * flags, a user specified alignment and the size of the objects.

	/*

	 * If the user wants hardware cache aligned objects then follow that

	 * suggestion if the object is sufficiently large.

	 *

	 * The hardware cache alignment cannot override the specified

	 * alignment though. If that is greater then use it.

/*

 * Find a mergeable slab cache

	/*

	 * We may have set a slab to be unmergeable during bootstrap.

		/*

		 * Check if alignment is compatible.

		 * Courtesy of Adrian Drzewiecki

/**

 * kmem_cache_create_usercopy - Create a cache with a region suitable

 * for copying to userspace

 * @name: A string which is used in /proc/slabinfo to identify this cache.

 * @size: The size of objects to be created in this cache.

 * @align: The required alignment for the objects.

 * @flags: SLAB flags

 * @useroffset: Usercopy region offset

 * @usersize: Usercopy region size

 * @ctor: A constructor for the objects.

 *

 * Cannot be called within a interrupt, but can be interrupted.

 * The @ctor is run when new pages are allocated by the cache.

 *

 * The flags are

 *

 * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)

 * to catch references to uninitialised memory.

 *

 * %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check

 * for buffer overruns.

 *

 * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware

 * cacheline.  This can be beneficial if you're counting cycles as closely

 * as davem.

 *

 * Return: a pointer to the cache on success, NULL on failure.

	/*

	 * If no slub_debug was enabled globally, the static key is not yet

	 * enabled by setup_slub_debug(). Enable it if the cache is being

	 * created with any of the debugging flags passed explicitly.

 Refuse requests with allocator specific flags */

	/*

	 * Some allocators will constraint the set of valid flags to a subset

	 * of all flags. We expect them to define CACHE_CREATE_MASK in this

	 * case, and we'll just provide them with a sanitized version of the

	 * passed flags.

 Fail closed on bad usersize of useroffset values. */

/**

 * kmem_cache_create - Create a cache.

 * @name: A string which is used in /proc/slabinfo to identify this cache.

 * @size: The size of objects to be created in this cache.

 * @align: The required alignment for the objects.

 * @flags: SLAB flags

 * @ctor: A constructor for the objects.

 *

 * Cannot be called within a interrupt, but can be interrupted.

 * The @ctor is run when new pages are allocated by the cache.

 *

 * The flags are

 *

 * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)

 * to catch references to uninitialised memory.

 *

 * %SLAB_RED_ZONE - Insert `Red` zones around the allocated memory to check

 * for buffer overruns.

 *

 * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware

 * cacheline.  This can be beneficial if you're counting cycles as closely

 * as davem.

 *

 * Return: a pointer to the cache on success, NULL on failure.

	/*

	 * On destruction, SLAB_TYPESAFE_BY_RCU kmem_caches are put on the

	 * @slab_caches_to_rcu_destroy list.  The slab pages are freed

	 * through RCU and the associated kmem_cache are dereferenced

	 * while freeing the pages, so the kmem_caches should be freed only

	 * after the pending RCU operations are finished.  As rcu_barrier()

	 * is a pretty slow operation, we batch all pending destructions

	 * asynchronously.

 free asan quarantined objects */

/**

 * kmem_cache_shrink - Shrink a cache.

 * @cachep: The cache to shrink.

 *

 * Releases as many slabs as possible for a cache.

 * To help debugging, a zero exit status indicates all slabs were released.

 *

 * Return: %0 if all slabs were released, non-zero otherwise

/**

 * kmem_valid_obj - does the pointer reference a valid slab object?

 * @object: pointer to query.

 *

 * Return: %true if the pointer is to a not-yet-freed object from

 * kmalloc() or kmem_cache_alloc(), either %true or %false if the pointer

 * is to an already-freed object, and %false otherwise.

 Some arches consider ZERO_SIZE_PTR to be a valid address. */

/**

 * kmem_dump_obj - Print available slab provenance information

 * @object: slab object for which to find provenance information.

 *

 * This function uses pr_cont(), so that the caller is expected to have

 * printed out whatever preamble is appropriate.  The provenance information

 * depends on the type of object and on how much debugging is enabled.

 * For a slab-cache object, the fact that it is a slab object is printed,

 * and, if available, the slab name, return address, and stack trace from

 * the allocation and last free path of that object.

 *

 * This function will splat if passed a pointer to a non-slab object.

 * If you are not sure what type of object you have, you should instead

 * use mem_dump_obj().

 Create a cache during boot when no slab services are available yet */

	/*

	 * For power of two sizes, guarantee natural alignment for kmalloc

	 * caches, regardless of SL*B debugging options.

 Exempt from merging for now */

bugs.llvm.org/show_bug.cgi?id=42570 */ };

/*

 * Conversion table for small slabs sizes / 8 to the index in the

 * kmalloc array. This is necessary for slabs < 192 since we have non power

 * of two cache sizes there. The size of larger slabs can be determined using

 * fls.

 8 */

 16 */

 24 */

 32 */

 40 */

 48 */

 56 */

 64 */

 72 */

 80 */

 88 */

 96 */

 104 */

 112 */

 120 */

 128 */

 136 */

 144 */

 152 */

 160 */

 168 */

 176 */

 184 */

 192 */

/*

 * Find the kmem_cache structure that serves a given size of

 * allocation

/*

 * kmalloc_info[] is to make slub_debug=,kmalloc-xx option work at boot time.

 * kmalloc_index() supports up to 2^25=32MB, so the final entry of the table is

 * kmalloc-32M.

/*

 * Patch up the size_index table if we have strange large alignment

 * requirements for the kmalloc array. This is only the case for

 * MIPS it seems. The standard arches will not generate any code here.

 *

 * Largest permitted alignment is 256 bytes due to the way we

 * handle the index determination for the smaller caches.

 *

 * Make sure that nothing crazy happens if someone starts tinkering

 * around with ARCH_KMALLOC_MINALIGN

		/*

		 * The 96 byte size cache is not used if the alignment

		 * is 64 byte.

		/*

		 * The 192 byte sized cache is not used if the alignment

		 * is 128 byte. Redirect kmalloc to use the 256 byte cache

		 * instead.

	/*

	 * If CONFIG_MEMCG_KMEM is enabled, disable cache merging for

	 * KMALLOC_NORMAL caches.

/*

 * Create the kmalloc array. Some of the regular kmalloc arrays

 * may already have been created because they were needed to

 * enable allocations for slab creation.

	/*

	 * Including KMALLOC_CGROUP if CONFIG_MEMCG_KMEM defined

			/*

			 * Caches that are not of the two-to-the-power-of size.

			 * These have to be created immediately after the

			 * earlier power of two caches

 Kmalloc array is now usable */

 !CONFIG_SLOB */

/*

 * To avoid unnecessary overhead, we pass through large allocation requests

 * directly to the page allocator. We use __GFP_COMP, because we will need to

 * know the allocation order to free the pages properly in kfree.

 As ret might get tagged, call kmemleak hook after KASAN. */

 Randomize a generic freelist */

 Fisher-Yates shuffle */

 Create a random sequence per cache */

 Get best entropy at this stage of boot */

 Destroy the per-cache random freelist sequence */

 CONFIG_SLAB_FREELIST_RANDOM */

	/*

	 * Output format version, so at least we can change it

	 * without _too_ many complaints.

	/*

	 * Here acquiring slab_mutex is risky since we don't prefer to get

	 * sleep in oom path. But, without mutex hold, it may introduce a

	 * risk of crash.

	 * Use mutex_trylock to protect the list traverse, dump nothing

	 * without acquiring the mutex.

	/*

	 * Deprecated.

	 * Please, take a look at tools/cgroup/slabinfo.py .

/*

 * slabinfo_op - iterator that generates /proc/slabinfo

 *

 * Output layout:

 * cache-name

 * num-active-objs

 * total-objs

 * object size

 * num-active-slabs

 * total-slabs

 * num-pages-per-slab

 * + further values on SMP and with statistics enabled

 CONFIG_SLAB || CONFIG_SLUB_DEBUG */

 Don't use instrumented ksize to allow precise KASAN poisoning. */

 If the object still fits, repoison it precisely. */

 Disable KASAN checks as the object's redzone is accessed. */

/**

 * krealloc - reallocate memory. The contents will remain unchanged.

 * @p: object to reallocate memory for.

 * @new_size: how many bytes of memory are required.

 * @flags: the type of memory to allocate.

 *

 * The contents of the object pointed to are preserved up to the

 * lesser of the new and old sizes (__GFP_ZERO flag is effectively ignored).

 * If @p is %NULL, krealloc() behaves exactly like kmalloc().  If @new_size

 * is 0 and @p is not a %NULL pointer, the object pointed to is freed.

 *

 * Return: pointer to the allocated memory or %NULL in case of error

/**

 * kfree_sensitive - Clear sensitive information in memory before freeing

 * @p: object to free memory of

 *

 * The memory of the object @p points to is zeroed before freed.

 * If @p is %NULL, kfree_sensitive() does nothing.

 *

 * Note: this function zeroes the whole allocated buffer which can be a good

 * deal bigger than the requested buffer size passed to kmalloc(). So be

 * careful when using this function in performance sensitive code.

/**

 * ksize - get the actual amount of memory allocated for a given object

 * @objp: Pointer to the object

 *

 * kmalloc may internally round up allocations and return more memory

 * than requested. ksize() can be used to determine the actual amount of

 * memory allocated. The caller may use this additional memory, even though

 * a smaller amount of memory was initially specified with the kmalloc call.

 * The caller must guarantee that objp points to a valid object previously

 * allocated with either kmalloc() or kmem_cache_alloc(). The object

 * must not be freed during the duration of the call.

 *

 * Return: size of the actual memory used by @objp in bytes

	/*

	 * We need to first check that the pointer to the object is valid, and

	 * only then unpoison the memory. The report printed from ksize() is

	 * more useful, then when it's printed later when the behaviour could

	 * be undefined due to a potential use-after-free or double-free.

	 *

	 * We use kasan_check_byte(), which is supported for the hardware

	 * tag-based KASAN mode, unlike kasan_check_read/write().

	 *

	 * If the pointed to memory is invalid, we return 0 to avoid users of

	 * ksize() writing to and potentially corrupting the memory region.

	 *

	 * We want to perform the check before __ksize(), to avoid potentially

	 * crashing in __ksize() due to accessing invalid metadata.

	/*

	 * We assume that ksize callers could use whole allocated area,

	 * so we need to unpoison this area.

 Tracepoints definitions. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/truncate.c - code for taking down pages from address_spaces

 *

 * Copyright (C) 2002, Linus Torvalds

 *

 * 10Sep2002	Andrew Morton

 *		Initial version.

#include <linux/buffer_head.h>	/* grr. try_to_release_page,

/*

 * Regular page slots are stabilized by the page lock even without the tree

 * itself locked.  These unlocked entries need verification under the tree

 * lock.

/*

 * Unconditionally remove exceptional entries. Usually called from truncate

 * path. Note that the pagevec may be altered by this function by removing

 * exceptional entries similar to what pagevec_remove_exceptionals does.

 Handled by shmem itself */

/*

 * Invalidate exceptional entry if easily possible. This handles exceptional

 * entries for invalidate_inode_pages().

 Handled by shmem itself, or for DAX we do nothing. */

/*

 * Invalidate exceptional entry if clean. This handles exceptional entries for

 * invalidate_inode_pages2() so for DAX it evicts only clean entries.

 Handled by shmem itself */

/**

 * do_invalidatepage - invalidate part or all of a page

 * @page: the page which is affected

 * @offset: start of the range to invalidate

 * @length: length of the range to invalidate

 *

 * do_invalidatepage() is called when all or part of the page has become

 * invalidated by a truncate operation.

 *

 * do_invalidatepage() does not have to release all buffers, but it must

 * ensure that no dirty buffer is left outside @offset and that no I/O

 * is underway against any of the blocks which are outside the truncation

 * point.  Because the caller is about to free (and possibly reuse) those

 * blocks on-disk.

/*

 * If truncate cannot remove the fs-private metadata from the page, the page

 * becomes orphaned.  It will be left on the LRU and may even be mapped into

 * user pagetables if we're racing with filemap_fault().

 *

 * We need to bail out if page->mapping is no longer equal to the original

 * mapping.  This happens a) when the VM reclaimed the page while we waited on

 * its lock, b) when a concurrent invalidate_mapping_pages got there first and

 * c) when tmpfs swizzles a page between a tmpfs inode and swapper_space.

	/*

	 * Some filesystems seem to re-dirty the page even after

	 * the VM has canceled the dirty bit (eg ext3 journaling).

	 * Hence dirty accounting check is placed after invalidation.

/*

 * This is for invalidate_mapping_pages().  That function can be called at

 * any time, and is not supposed to throw away dirty pages.  But pages can

 * be marked dirty at any time too, so use remove_mapping which safely

 * discards clean, unused pages.

 *

 * Returns non-zero if the page was successfully invalidated.

/*

 * Used to get rid of pages on hardware memory corruption.

	/*

	 * Only punch for normal data pages for now.

	 * Handling other types like directories would need more auditing.

/*

 * Safely invalidate one page from its pagecache mapping.

 * It only drops clean, unused pages. The page must be locked.

 *

 * Returns 1 if the page is successfully invalidated, otherwise 0.

/**

 * truncate_inode_pages_range - truncate range of pages specified by start & end byte offsets

 * @mapping: mapping to truncate

 * @lstart: offset from which to truncate

 * @lend: offset to which to truncate (inclusive)

 *

 * Truncate the page cache, removing the pages that are between

 * specified offsets (and zeroing out partial pages

 * if lstart or lend + 1 is not page aligned).

 *

 * Truncate takes two passes - the first pass is nonblocking.  It will not

 * block on page locks and it will not block on writeback.  The second pass

 * will wait.  This is to prevent as much IO as possible in the affected region.

 * The first pass will remove most pages, so the search cost of the second pass

 * is low.

 *

 * We pass down the cache-hot hint to the page freeing code.  Even if the

 * mapping is large, it is probably the case that the final pages are the most

 * recently touched, and freeing happens in ascending file offset order.

 *

 * Note that since ->invalidatepage() accepts range to invalidate

 * truncate_inode_pages_range is able to handle cases where lend + 1 is not

 * page aligned properly.

 inclusive */

 exclusive */

 inclusive */

 exclusive */

 Offsets within partial pages */

	/*

	 * 'start' and 'end' always covers the range of pages to be fully

	 * truncated. Partial pages are covered with 'partial_start' at the

	 * start of the range and 'partial_end' at the end of the range.

	 * Note that 'end' is exclusive while 'lend' is inclusive.

		/*

		 * lend == -1 indicates end-of-file so we have to set 'end'

		 * to the highest possible pgoff_t and since the type is

		 * unsigned we're using -1.

 Truncation within a single page */

	/*

	 * If the truncation happened within a single page no pages

	 * will be released, just zeroed, so we can bail out now.

 If all gone from start onwards, we're done */

 Otherwise restart to make sure all gone */

 We rely upon deletion not changing page->index */

/**

 * truncate_inode_pages - truncate *all* the pages from an offset

 * @mapping: mapping to truncate

 * @lstart: offset from which to truncate

 *

 * Called under (and serialised by) inode->i_rwsem and

 * mapping->invalidate_lock.

 *

 * Note: When this function returns, there can be a page in the process of

 * deletion (inside __delete_from_page_cache()) in the specified range.  Thus

 * mapping->nrpages can be non-zero when this function returns even after

 * truncation of the whole mapping.

/**

 * truncate_inode_pages_final - truncate *all* pages before inode dies

 * @mapping: mapping to truncate

 *

 * Called under (and serialized by) inode->i_rwsem.

 *

 * Filesystems have to use this in the .evict_inode path to inform the

 * VM that this is the final truncate and the inode is going away.

	/*

	 * Page reclaim can not participate in regular inode lifetime

	 * management (can't call iput()) and thus can race with the

	 * inode teardown.  Tell it when the address space is exiting,

	 * so that it does not install eviction information after the

	 * final truncate has begun.

		/*

		 * As truncation uses a lockless tree lookup, cycle

		 * the tree lock to make sure any ongoing tree

		 * modification that does not see AS_EXITING is

		 * completed before starting the final truncate.

	/*

	 * Cleancache needs notification even if there are no pages or shadow

	 * entries.

 We rely upon deletion not changing page->index */

			/*

			 * Invalidation is a hint that the page is no longer

			 * of interest and try to speed up its reclaim.

 It is likely on the pagevec of a remote CPU */

/**

 * invalidate_mapping_pages - Invalidate all clean, unlocked cache of one inode

 * @mapping: the address_space which holds the cache to invalidate

 * @start: the offset 'from' which to invalidate

 * @end: the offset 'to' which to invalidate (inclusive)

 *

 * This function removes pages that are clean, unmapped and unlocked,

 * as well as shadow entries. It will not block on IO activity.

 *

 * If you want to remove all the pages of one inode, regardless of

 * their use and writeback state, use truncate_inode_pages().

 *

 * Return: the number of the cache entries that were invalidated

/**

 * invalidate_mapping_pagevec - Invalidate all the unlocked pages of one inode

 * @mapping: the address_space which holds the pages to invalidate

 * @start: the offset 'from' which to invalidate

 * @end: the offset 'to' which to invalidate (inclusive)

 * @nr_pagevec: invalidate failed page number for caller

 *

 * This helper is similar to invalidate_mapping_pages(), except that it accounts

 * for pages that are likely on a pagevec and counts them in @nr_pagevec, which

 * will be used by the caller.

/*

 * This is like invalidate_complete_page(), except it ignores the page's

 * refcount.  We do this because invalidate_inode_pages2() needs stronger

 * invalidation guarantees, and cannot afford to leave pages behind because

 * shrink_page_list() has a temp ref on them, or because they're transiently

 * sitting in the lru_cache_add() pagevecs.

 pagecache ref */

/**

 * invalidate_inode_pages2_range - remove range of pages from an address_space

 * @mapping: the address_space

 * @start: the page offset 'from' which to invalidate

 * @end: the page offset 'to' which to invalidate (inclusive)

 *

 * Any pages which are found to be mapped into pagetables are unmapped prior to

 * invalidation.

 *

 * Return: -EBUSY if any pages could not be invalidated.

 We rely upon deletion not changing page->index */

				/*

				 * If page is mapped, before taking its lock,

				 * zap the rest of the file in one hit.

	/*

	 * For DAX we invalidate page tables after invalidating page cache.  We

	 * could invalidate page tables while invalidating each entry however

	 * that would be expensive. And doing range unmapping before doesn't

	 * work as we have no cheap way to find whether page cache entry didn't

	 * get remapped later.

/**

 * invalidate_inode_pages2 - remove all pages from an address_space

 * @mapping: the address_space

 *

 * Any pages which are found to be mapped into pagetables are unmapped prior to

 * invalidation.

 *

 * Return: -EBUSY if any pages could not be invalidated.

/**

 * truncate_pagecache - unmap and remove pagecache that has been truncated

 * @inode: inode

 * @newsize: new file size

 *

 * inode's new i_size must already be written before truncate_pagecache

 * is called.

 *

 * This function should typically be called before the filesystem

 * releases resources associated with the freed range (eg. deallocates

 * blocks). This way, pagecache will always stay logically coherent

 * with on-disk format, and the filesystem would not have to deal with

 * situations such as writepage being called for a page that has already

 * had its underlying blocks deallocated.

	/*

	 * unmap_mapping_range is called twice, first simply for

	 * efficiency so that truncate_inode_pages does fewer

	 * single-page unmaps.  However after this first call, and

	 * before truncate_inode_pages finishes, it is possible for

	 * private pages to be COWed, which remain after

	 * truncate_inode_pages finishes, hence the second

	 * unmap_mapping_range call must be made for correctness.

/**

 * truncate_setsize - update inode and pagecache for a new file size

 * @inode: inode

 * @newsize: new file size

 *

 * truncate_setsize updates i_size and performs pagecache truncation (if

 * necessary) to @newsize. It will be typically be called from the filesystem's

 * setattr function when ATTR_SIZE is passed in.

 *

 * Must be called with a lock serializing truncates and writes (generally

 * i_rwsem but e.g. xfs uses a different lock) and before all filesystem

 * specific block truncation has been performed.

/**

 * pagecache_isize_extended - update pagecache after extension of i_size

 * @inode:	inode for which i_size was extended

 * @from:	original inode size

 * @to:		new inode size

 *

 * Handle extension of inode size either caused by extending truncate or by

 * write starting after current i_size. We mark the page straddling current

 * i_size RO so that page_mkwrite() is called on the nearest write access to

 * the page.  This way filesystem can be sure that page_mkwrite() is called on

 * the page before user writes to the page via mmap after the i_size has been

 * changed.

 *

 * The function must be called after i_size is updated so that page fault

 * coming after we unlock the page will already see the new i_size.

 * The function must be called while we still hold i_rwsem - this not only

 * makes sure i_size is stable but also that userspace cannot observe new

 * i_size value before we are prepared to store mmap writes at new inode size.

 Page straddling @from will not have any hole block created? */

 Page not cached? Nothing to do */

	/*

	 * See clear_page_dirty_for_io() for details why set_page_dirty()

	 * is needed.

/**

 * truncate_pagecache_range - unmap and remove pagecache that is hole-punched

 * @inode: inode

 * @lstart: offset of beginning of hole

 * @lend: offset of last byte of hole

 *

 * This function should typically be called before the filesystem

 * releases resources associated with the freed range (eg. deallocates

 * blocks). This way, pagecache will always stay logically coherent

 * with on-disk format, and the filesystem would not have to deal with

 * situations such as writepage being called for a page that has already

 * had its underlying blocks deallocated.

	/*

	 * This rounding is currently just for example: unmap_mapping_range

	 * expands its hole outwards, whereas we want it to contract the hole

	 * inwards.  However, existing callers of truncate_pagecache_range are

	 * doing their own page rounding first.  Note that unmap_mapping_range

	 * allows holelen 0 for all, and we allow lend -1 for end of file.

	/*

	 * Unlike in truncate_pagecache, unmap_mapping_range is called only

	 * once (before truncating pagecache), and without "even_cows" flag:

	 * hole-punching should not remove private COWed pages from the hole.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/percpu-km.c - kernel memory based chunk allocation

 *

 * Copyright (C) 2010		SUSE Linux Products GmbH

 * Copyright (C) 2010		Tejun Heo <tj@kernel.org>

 *

 * Chunks are allocated as a contiguous kernel memory using gfp

 * allocation.  This is to be used on nommu architectures.

 *

 * To use percpu-km,

 *

 * - define CONFIG_NEED_PER_CPU_KM from the arch Kconfig.

 *

 * - CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK must not be defined.  It's

 *   not compatible with PER_CPU_KM.  EMBED_FIRST_CHUNK should work

 *   fine.

 *

 * - NUMA is not supported.  When setting up the first chunk,

 *   @cpu_distance_fn should be NULL or report all CPUs to be nearer

 *   than or at LOCAL_DISTANCE.

 *

 * - It's best if the chunk size is power of two multiple of

 *   PAGE_SIZE.  Because each chunk is allocated as a contiguous

 *   kernel memory block using alloc_pages(), memory will be wasted if

 *   chunk size is not aligned.  percpu-km code will whine about it.

 nothing */

 nada */

 all units must be in a single group */

 SPDX-License-Identifier: GPL-2.0

/*

 * For dynamically allocated mm_structs, there is a dynamically sized cpumask

 * at the end of the structure, the size of which depends on the maximum CPU

 * number the system can see. That way we allocate only as much memory for

 * mm_cpumask() as needed for the hundreds, or thousands of processes that

 * a system typically runs.

 *

 * Since there is only one init_mm in the entire system, keep it simple

 * and size this cpu_bitmask to NR_CPUS.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/mm/slab.c

 * Written by Mark Hemment, 1996/97.

 * (markhe@nextd.demon.co.uk)

 *

 * kmem_cache_destroy() + some cleanup - 1999 Andrea Arcangeli

 *

 * Major cleanup, different bufctl logic, per-cpu arrays

 *	(c) 2000 Manfred Spraul

 *

 * Cleanup, make the head arrays unconditional, preparation for NUMA

 * 	(c) 2002 Manfred Spraul

 *

 * An implementation of the Slab Allocator as described in outline in;

 *	UNIX Internals: The New Frontiers by Uresh Vahalia

 *	Pub: Prentice Hall	ISBN 0-13-101908-2

 * or with a little more detail in;

 *	The Slab Allocator: An Object-Caching Kernel Memory Allocator

 *	Jeff Bonwick (Sun Microsystems).

 *	Presented at: USENIX Summer 1994 Technical Conference

 *

 * The memory is organized in caches, one cache for each object type.

 * (e.g. inode_cache, dentry_cache, buffer_head, vm_area_struct)

 * Each cache consists out of many slabs (they are small (usually one

 * page long) and always contiguous), and each slab contains multiple

 * initialized objects.

 *

 * This means, that your constructor is used only for newly allocated

 * slabs and you must pass objects with the same initializations to

 * kmem_cache_free.

 *

 * Each cache can only support one memory type (GFP_DMA, GFP_HIGHMEM,

 * normal). If you need a special memory type, then must create a new

 * cache for that memory type.

 *

 * In order to reduce fragmentation, the slabs are sorted in 3 groups:

 *   full slabs with 0 free objects

 *   partial slabs

 *   empty slabs with no allocated objects

 *

 * If partial slabs exist, then new allocations come from these slabs,

 * otherwise from empty slabs or new slabs are allocated.

 *

 * kmem_cache_destroy() CAN CRASH if you try to allocate from the cache

 * during kmem_cache_destroy(). The caller must prevent concurrent allocs.

 *

 * Each cache has a short per-cpu head array, most allocs

 * and frees go into that array, and if that array overflows, then 1/2

 * of the entries in the array are given back into the global cache.

 * The head array is strictly LIFO and should improve the cache hit rates.

 * On SMP, it additionally reduces the spinlock operations.

 *

 * The c_cpuarray may not be read with enabled local interrupts -

 * it's changed with a smp_call_function().

 *

 * SMP synchronization:

 *  constructors and destructors are called without any locking.

 *  Several members in struct kmem_cache and struct slab never change, they

 *	are accessed without any locking.

 *  The per-cpu arrays are never accessed from the wrong cpu, no locking,

 *  	and local interrupts are disabled so slab code is preempt-safe.

 *  The non-constant members are protected with a per-cache irq spinlock.

 *

 * Many thanks to Mark Hemment, who wrote another per-cpu slab patch

 * in 2000 - many ideas in the current implementation are derived from

 * his patch.

 *

 * Further notes from the original documentation:

 *

 * 11 April '97.  Started multi-threading - markhe

 *	The global cache-chain is protected by the mutex 'slab_mutex'.

 *	The sem is only needed when accessing/extending the cache-chain, which

 *	can never happen inside an interrupt (kmem_cache_create(),

 *	kmem_cache_shrink() and kmem_cache_reap()).

 *

 *	At present, each engine can be growing a cache.  This should be blocked.

 *

 * 15 March 2005. NUMA slab allocator.

 *	Shai Fultheim <shai@scalex86.org>.

 *	Shobhit Dayal <shobhit@calsoftinc.com>

 *	Alok N Kataria <alokk@calsoftinc.com>

 *	Christoph Lameter <christoph@lameter.com>

 *

 *	Modified the slab allocator to be node aware on NUMA systems.

 *	Each node has its own list of partial, free and full slabs.

 *	All object allocations for a node occur from node specific slab lists.

/*

 * DEBUG	- 1 for kmem_cache_create() to honour; SLAB_RED_ZONE & SLAB_POISON.

 *		  0 for faster, smaller code (especially in the critical paths).

 *

 * STATS	- 1 to collect stats for /proc/slabinfo.

 *		  0 for faster, smaller code (especially in the critical paths).

 *

 * FORCED_DEBUG	- 1 enables SLAB_RED_ZONE and SLAB_POISON (if possible)

 Shouldn't this be in a header file somewhere? */

/*

 * struct array_cache

 *

 * Purpose:

 * - LIFO ordering, to hand out cache-warm objects from _alloc

 * - reduce the number of linked list operations

 * - reduce spinlock operations

 *

 * The limit is stored in the per-cpu structure to reduce the data cache

 * footprint.

 *

	void *entry[];	/*

			 * Must have this definition in here for the proper

			 * alignment of array_cache. Also simplifies accessing

			 * the entries.

/*

 * Need this for bootstrapping a per node allocator.

/*

 * Optimization question: fewer reaps means less probability for unnecessary

 * cpucache drain/refill cycles.

 *

 * OTOH the cpuarrays can contain lots of objects,

 * which could lock up otherwise freeable slabs.

/*

 * memory layout of objects:

 * 0		: objp

 * 0 .. cachep->obj_offset - BYTES_PER_WORD - 1: padding. This ensures that

 * 		the end of an object is aligned with the end of the real

 * 		allocation. Catches writes behind the end of the allocation.

 * cachep->obj_offset - BYTES_PER_WORD .. cachep->obj_offset - 1:

 * 		redzone word.

 * cachep->obj_offset: The real object.

 * cachep->size - 2* BYTES_PER_WORD: redzone word [BYTES_PER_WORD long]

 * cachep->size - 1* BYTES_PER_WORD: last caller address

 *					[BYTES_PER_WORD long]

/*

 * Do not go above this order unless 0 objects fit into the slab or

 * overridden on the command line.

 internal cache of cache description objs */

/*

 * Calculate the number of objects and left-over bytes for a given buffer size.

	/*

	 * The slab management structure can be either off the slab or

	 * on it. For the latter case, the memory allocated for a

	 * slab is used for:

	 *

	 * - @buffer_size bytes for each object

	 * - One freelist_idx_t for each object

	 *

	 * We don't need to consider alignment of freelist because

	 * freelist will be at the end of slab page. The objects will be

	 * at the correct alignment.

	 *

	 * If the slab management structure is off the slab, then the

	 * alignment will already be calculated into the size. Because

	 * the slabs are all pages aligned, the objects will be at the

	 * correct alignment when allocated.

/*

 * By default on NUMA we use alien caches to stage the freeing of

 * objects allocated from other nodes. This causes massive memory

 * inefficiencies when using fake NUMA setup to split memory into a

 * large number of small nodes, so it can be disabled on the command

 * line

/*

 * Special reaping functions for NUMA systems called from cache_reap().

 * These take care of doing round robin flushing of alien caches (containing

 * objects freed on different nodes from which they were allocated) and the

 * flushing of remote pcps by calling drain_node_pages.

/*

 * Initiate the reap timer running on the target CPU.  We run at around 1 to 2Hz

 * via the workqueue/eventd.

 * Add the CPU number into the expiration time to minimize the possibility of

 * the CPUs getting into lockstep and contending for the global cache chain

 * lock.

	/*

	 * The array_cache structures contain pointers to free object.

	 * However, when such objects are allocated or transferred to another

	 * cache the pointers are not cleared and they could be counted as

	 * valid references during a kmemleak scan. Therefore, kmemleak must

	 * not scan such objects.

/*

 * Transfer objects in one arraycache to another.

 * Locking must be handled by the caller.

 *

 * Return the number of entries transferred.

 Figure out how many entries to transfer */

 &alien->lock must be held by alien callers. */

 Avoid trivial double-free. */

 CONFIG_NUMA */

		/*

		 * Stuff objects into the remote nodes shared array first.

		 * That way we could avoid the overhead of putting the objects

		 * into the free lists and getting them back later.

/*

 * Called from cache_reap() to regularly drain alien caches round robin.

	/*

	 * Make sure we are not freeing a object from another node to the array

	 * cache on this cpu.

/*

 * Construct gfp mask to allocate from a specific node but do not reclaim or

 * warn about failures.

	/*

	 * Set up the kmem_cache_node for cpu before we can

	 * begin anything. Make sure some other cpu on this

	 * node has not already allocated this

	/*

	 * The kmem_cache_nodes don't come and go as CPUs

	 * come and go.  slab_mutex is sufficient

	 * protection here.

/*

 * Allocates and initializes node for a node on each slab cache, used for

 * either memory or cpu hotplug.  If memory is being hot-added, the kmem_cache_node

 * will be allocated off-node since memory is not yet online for the new node.

 * When hotplugging memory or a cpu, existing node are not replaced if

 * already in use.

 *

 * Must hold slab_mutex.

	/*

	 * To protect lockless access to n->shared during irq disabled context.

	 * If n->shared isn't NULL in irq disabled context, accessing to it is

	 * guaranteed to be valid until irq is re-enabled, because it will be

	 * freed after synchronize_rcu().

 Free limit for this kmem_cache_node */

 cpu is dead; no one can alloc from it. */

	/*

	 * In the previous loop, all the objects were freed to

	 * the respective cache's slabs,  now we can go ahead and

	 * shrink each nodelist to its limit.

	/*

	 * We need to do this right in the beginning since

	 * alloc_arraycache's are going to use this list.

	 * kmalloc_node allows us to add the slab to the right

	 * kmem_cache_node and not this cpu's kmem_cache_node

	/*

	 * Now we can go ahead with allocating the shared arrays and

	 * array caches

/*

 * This is called for a failed online attempt and for a successful

 * offline.

 *

 * Even if all the cpus of a node are down, we don't free the

 * kmem_cache_node of any cache. This to avoid a race between cpu_down, and

 * a kmalloc allocation from another cpu for memory from the node of

 * the cpu going down.  The kmem_cache_node structure is usually allocated from

 * kmem_cache_create() and gets destroyed at kmem_cache_destroy().

	/*

	 * Shutdown cache reaper. Note that the slab_mutex is held so

	 * that if cache_reap() is invoked it cannot do anything

	 * expensive but will only modify reap_work and reschedule the

	 * timer.

 Now the cache_reaper is guaranteed to be not running. */

/*

 * Drains freelist for a node on each slab cache, used for memory hot-remove.

 * Returns -EBUSY if all objects cannot be drained so that the node is not

 * removed.

 *

 * Must hold slab_mutex.

 CONFIG_NUMA */

/*

 * swap the static kmem_cache_node with kmalloced memory

	/*

	 * Do not assume that spinlocks can be initialized via memcpy:

/*

 * For setting up all the kmem_cache_node for cache whose buffer_size is same as

 * size of kmem_cache_node.

/*

 * Initialisation.  Called after the page allocator have been initialised and

 * before smp_init().

	/*

	 * Fragmentation resistance on low memory - only use bigger

	 * page orders on machines with more than 32MB of memory if

	 * not overridden on the command line.

	/* Bootstrap is tricky, because several objects are allocated

	 * from caches that do not exist yet:

	 * 1) initialize the kmem_cache cache: it contains the struct

	 *    kmem_cache structures of all caches, except kmem_cache itself:

	 *    kmem_cache is statically allocated.

	 *    Initially an __init data area is used for the head array and the

	 *    kmem_cache_node structures, it's replaced with a kmalloc allocated

	 *    array at the end of the bootstrap.

	 * 2) Create the first kmalloc cache.

	 *    The struct kmem_cache for the new cache is allocated normally.

	 *    An __init data area is used for the head array.

	 * 3) Create the remaining kmalloc caches, with minimally sized

	 *    head arrays.

	 * 4) Replace the __init data head arrays for kmem_cache and the first

	 *    kmalloc cache with kmalloc allocated arrays.

	 * 5) Replace the __init data for kmem_cache_node for kmem_cache and

	 *    the other cache's with kmalloc allocated memory.

	 * 6) Resize the head arrays of the kmalloc caches to their final sizes.

 1) create the kmem_cache */

	/*

	 * struct kmem_cache size depends on nr_node_ids & nr_cpu_ids

	/*

	 * Initialize the caches that provide memory for the  kmem_cache_node

	 * structures first.  Without this, further allocations will bug.

 5) Replace the bootstrap kmem_cache_node */

 6) resize the head arrays to their final sizes */

 Done! */

	/*

	 * Register a memory hotplug callback that initializes and frees

	 * node.

	/*

	 * The reap timers are started later, with a module init call: That part

	 * of the kernel is not yet operational.

	/*

	 * Register the timers that return unneeded pages to the page allocator

/*

 * Interface to system's page allocator. No need to hold the

 * kmem_cache_node ->list_lock.

 *

 * If we requested dmaable memory, we will get it. Even if we

 * did not request dmaable memory, we might get it, but that

 * would be relatively rare and ignorable.

 Record if ALLOC_NO_WATERMARKS was set when allocating the slab */

/*

 * Interface to system's page release.

 In union with page->mapping where page allocator expects NULL */

 Mismatch ! */

 Print header */

 Hexdump the affected line */

 Limit to 5 lines */

		/* Print some data about the neighboring objects, if they

		 * exist:

/**

 * slab_destroy - destroy and release all objects in a slab

 * @cachep: cache pointer being destroyed

 * @page: page pointer being destroyed

 *

 * Destroy all the objs in a slab page, and release the mem back to the system.

 * Before calling the slab page must have been unlinked from the cache. The

 * kmem_cache_node ->list_lock is not held/needed.

	/*

	 * From now on, we don't use freelist

	 * although actual page can be freed in rcu context

/*

 * Update the size of the caches before calling slabs_destroy as it may

 * recursively call kfree.

/**

 * calculate_slab_order - calculate size (page order) of slabs

 * @cachep: pointer to the cache that is being created

 * @size: size of objects to be created in this cache.

 * @flags: slab allocation flags

 *

 * Also calculates the number of objects per slab.

 *

 * This could be made much more intelligent.  For now, try to avoid using

 * high order pages for slabs.  When the gfp() functions are more friendly

 * towards high-order requests, this should be changed.

 *

 * Return: number of left-over bytes in a slab

 Can't handle number of objects more than SLAB_OBJ_MAX_NUM */

			/*

			 * Needed to avoid possible looping condition

			 * in cache_grow_begin()

 check if off slab has enough benefit */

 Found something acceptable - save it away */

		/*

		 * A VFS-reclaimable slab tends to have most allocations

		 * as GFP_NOFS and we really don't want to have to be allocating

		 * higher-order pages when we are unable to shrink dcache.

		/*

		 * Large number of objects is good, but very large slabs are

		 * currently bad for the gfp()s.

		/*

		 * Acceptable internal fragmentation?

 Creation of first cache (kmem_cache). */

 For kmem_cache_node */

		/*

		 * Adjust the object sizes so that we clear

		 * the complete object on kzalloc.

	/*

	 * If slab auto-initialization on free is enabled, store the freelist

	 * off-slab, so that its contents don't end up in one of the allocated

	 * objects.

	/*

	 * Always use on-slab management when SLAB_NOLEAKTRACE

	 * to avoid recursive calls into kmemleak.

	/*

	 * Size is large, assume best to place the slab management obj

	 * off-slab (should allow better packing of objs).

	/*

	 * If the slab has been placed off-slab, and we have enough space then

	 * move it on-slab. This is at the expense of any extra colouring.

/**

 * __kmem_cache_create - Create a cache.

 * @cachep: cache management descriptor

 * @flags: SLAB flags

 *

 * Returns a ptr to the cache on success, NULL on failure.

 * Cannot be called within a int, but can be interrupted.

 * The @ctor is run when new pages are allocated by the cache.

 *

 * The flags are

 *

 * %SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)

 * to catch references to uninitialised memory.

 *

 * %SLAB_RED_ZONE - Insert `Red' zones around the allocated memory to check

 * for buffer overruns.

 *

 * %SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware

 * cacheline.  This can be beneficial if you're counting cycles as closely

 * as davem.

 *

 * Return: a pointer to the created cache or %NULL in case of error

	/*

	 * Enable redzoning and last user accounting, except for caches with

	 * large objects, if the increased size would increase the object size

	 * above the next power of two: caches with object sizes just above a

	 * power of two have a significant amount of internal fragmentation.

	/*

	 * Check that size is in terms of words.  This is needed to avoid

	 * unaligned accesses for some archs when redzoning is used, and makes

	 * sure any on-slab bufctl's are also correctly aligned.

		/* If redzoning, ensure that the second redzone is suitably

 3) caller mandated alignment */

 disable debug if necessary */

	/*

	 * 4) Store it.

 Offset must be a multiple of the alignment. */

	/*

	 * Both debugging options require word-alignment which is calculated

	 * into align above.

 add space for red zone words */

		/* user store requires one word storage behind the end of

		 * the real object. But if the second red zone needs to be

		 * aligned to 64 bits, we must allow that much space.

	/*

	 * We should restrict the number of objects in a slab to implement

	 * byte sized index. Refer comment on SLAB_OBJ_MIN_SIZE definition.

	/*

	 * To activate debug pagealloc, off-slab management is necessary

	 * requirement. In early phase of initialization, small sized slab

	 * doesn't get initialized so it would not be possible. So, we need

	 * to check size >= 256. It guarantees that all necessary small

	 * sized slab is initialized in current slab initialization sequence.

	/*

	 * If we're going to use the generic kernel_map_pages()

	 * poisoning, then it's going to smash the contents of

	 * the redzone and userword anyhow, so switch them off.

/*

 * Remove slabs from the list of free slabs.

 * Specify the number of slabs to drain in tofree.

 *

 * Returns the actual number of slabs released.

		/*

		 * Safe to drop the lock. The slab is no longer linked

		 * to the cache.

 NUMA: free the node structures */

/*

 * Get the memory for a slab management obj.

 *

 * For a slab cache when the slab descriptor is off-slab, the

 * slab descriptor can't come from the same cache which is being created,

 * Because if it is the case, that means we defer the creation of

 * the kmalloc_{dma,}_cache of size sizeof(slab descriptor) to this point.

 * And we eventually call down to __kmem_cache_create(), which

 * in turn looks up in the kmalloc_{dma,}_caches for the desired-size one.

 * This is a "chicken-and-egg" problem.

 *

 * So the off-slab slab descriptor shall come from the kmalloc_{dma,}_caches,

 * which are all initialized during kmem_cache_init().

 Slab management obj is off-slab. */

 We will use last bytes at the slab for freelist */

		/*

		 * Constructors are not allowed to allocate memory from the same

		 * cache which they are a constructor for.  Otherwise, deadlock.

		 * They must also be threaded.

 need to poison the objs? */

 Hold information during a freelist initialization */

/*

 * Initialize the state based on the randomization method available.

 * return true if the pre-computed list is available, false otherwise.

 Use best entropy available to define a random shift */

 Use a random state if the pre-computed list is not available */

 Get the next entry on the list and randomize it using a random shift */

 Swap two freelist entries */

/*

 * Shuffle the freelist initialization state based on pre-computed lists.

 * return true if the list was successfully shuffled, false otherwise.

 Take a random entry as the objfreelist */

	/*

	 * On early boot, generate the list dynamically.

	 * Later use a pre-computed list for speed.

 Fisher-Yates shuffle */

 CONFIG_SLAB_FREELIST_RANDOM */

 Try to randomize the freelist if enabled */

 constructor could break poison info */

 Verify double free bug */

/*

 * Map pages beginning at addr to the given cache and slab. This is required

 * for the slab allocator to be able to lookup the cache and slab of a

 * virtual address for kfree, ksize, and slab debugging.

/*

 * Grow (by 1) the number of slabs within a cache.  This is called by

 * kmem_cache_alloc() when there are no active objs left in a cache.

	/*

	 * Be lazy and only check for valid flags here,  keeping it out of the

	 * critical path in kmem_cache_alloc().

	/*

	 * Get mem for the objs.  Attempt to allocate a physical page from

	 * 'nodeid'.

 Get colour for the slab, and cal the next value. */

	/*

	 * Call kasan_poison_slab() before calling alloc_slabmgmt(), so

	 * page_address() in the latter returns a non-tagged pointer,

	 * as it should be for slab pages.

 Get slab management. */

/*

 * Perform extra freeing checks:

 * - detect bad pointers.

 * - POISON/RED_ZONE checking

	/*

	 * Redzone is ok.

 move slabp to correct slabp list: */

 Poisoning will be done without holding the lock */

 Try to find non-pfmemalloc slab if needed */

 No need to keep pfmemalloc slab if we have enough free objects */

 Move pfmemalloc slab to the end of list to speed up next search */

/*

 * Slab list should be fixed up by fixup_slab_list() for existing slab

 * or cache_grow_end() for new slab

	/*

	 * There must be at least one object available for

	 * allocation.

		/*

		 * If there was little recent activity on this cache, then

		 * perform only a partial refill.  Otherwise we could generate

		 * refill bouncing.

 See if we can refill from the shared array */

 Get slab alloc is to come from. */

 Check if we can use obj in pfmemalloc slab */

		/*

		 * cache_grow_begin() can reenable interrupts,

		 * then ac could change.

	/*

	 * the 'ac' may be updated by cache_alloc_refill(),

	 * and kmemleak_erase() requires its correct value.

	/*

	 * To avoid a false negative, if an object that is in one of the

	 * per-CPU caches is leaked, we need to make sure kmemleak doesn't

	 * treat the array pointers as a reference to the object.

/*

 * Try allocating on another node if PFA_SPREAD_SLAB is a mempolicy is set.

 *

 * If we are in_interrupt, then process context, including cpusets and

 * mempolicy, may not apply and should not be used for allocation policy.

/*

 * Fallback function if there was no memory available and no objects on a

 * certain node and fall back is permitted. First we scan all the

 * available node for available objects. If that fails then we

 * perform an allocation without specifying a node. This allows the page

 * allocator to do its reclaim / fallback magic. We then insert the

 * slab into the proper nodelist and then allocate from it.

	/*

	 * Look through allowed nodes for objects available

	 * from existing per node queues.

		/*

		 * This allocation will be performed within the constraints

		 * of the current cpuset / memory policy requirements.

		 * We may trigger various forms of reclaim on the allowed

		 * set and go into memory reserves if necessary.

			/*

			 * Another processor may allocate the objects in

			 * the slab since we are not holding any locks.

/*

 * A interface to enable slab creation on nodeid

 This slab isn't counted yet so don't update free_objects */

 Node not bootstrapped yet */

		/*

		 * Use the locally cached objects if possible.

		 * However ____cache_alloc does not allow fallback

		 * to other nodes. It may fail while we still have

		 * objects on other nodes available.

 ___cache_alloc_node can fall back to other nodes */

	/*

	 * We may just have run out of memory on the local node.

	 * ____cache_alloc_node() knows how to locate memory on other nodes

 CONFIG_NUMA */

/*

 * Caller needs to acquire correct kmem_cache_node's list_lock

 * @list: List of detached free slabs should be freed by caller

 fixup slab chains */

			/* Unconditionally move a slab to the end of the

			 * partial list on free - maximum time for the

			 * other objects to be freed, too.

/*

 * Release an obj back to its cache. If the obj has a constructed state, it must

 * be in this state _before_ it is released.  Called with disabled ints.

	/*

	 * As memory initialization might be integrated into KASAN,

	 * kasan_slab_free and initialization memset must be

	 * kept together to avoid discrepancies in behavior.

 KASAN might put objp into memory quarantine, delaying its reuse. */

 Use KCSAN to help debug racy use-after-free. */

	/*

	 * Skip calling cache_free_alien() when the platform is not numa.

	 * This will avoid cache misses that happen while accessing slabp (which

	 * is per page memory  reference) to get nodeid. Instead use a global

	 * variable to skip the call, which is mostly likely to be present in

	 * the cache.

/**

 * kmem_cache_alloc - Allocate an object

 * @cachep: The cache to allocate from.

 * @flags: See kmalloc().

 *

 * Allocate an object from this cache.  The flags are only relevant

 * if the cache has no available objects.

 *

 * Return: pointer to the new object or %NULL in case of error

	/*

	 * memcg and kmem_cache debug support and memory initialization.

	 * Done outside of the IRQ disabled section.

 FIXME: Trace call missing. Christoph would like a bulk variant */

/**

 * kmem_cache_alloc_node - Allocate an object on the specified node

 * @cachep: The cache to allocate from.

 * @flags: See kmalloc().

 * @nodeid: node number of the target node.

 *

 * Identical to kmem_cache_alloc but it will allocate memory on the given

 * node, which can improve the performance for cpu bound structures.

 *

 * Fallback to other node is possible if __GFP_THISNODE is not set.

 *

 * Return: pointer to the new object or %NULL in case of error

 CONFIG_NUMA */

/**

 * __do_kmalloc - allocate memory

 * @size: how many bytes of memory are required.

 * @flags: the type of memory to allocate (see kmalloc).

 * @caller: function caller for debug tracking of the caller

 *

 * Return: pointer to the allocated memory or %NULL in case of error

/**

 * kmem_cache_free - Deallocate an object

 * @cachep: The cache the allocation was from.

 * @objp: The previously allocated object.

 *

 * Free an object which was previously allocated from this

 * cache.

 called via kfree_bulk */

 FIXME: add tracing */

/**

 * kfree - free previously allocated memory

 * @objp: pointer returned by kmalloc.

 *

 * If @objp is NULL, no operation is performed.

 *

 * Don't free memory not originally allocated by kmalloc()

 * or you will run into trouble.

/*

 * This initializes kmem_cache_node or resizes various caches for all nodes.

 Cache is not active yet. Roll back what we did */

 Always called with the slab_mutex held */

	/*

	 * Without a previous cpu_cache there's no need to synchronize remote

	 * cpus, so skip the IPIs.

 Called with slab_mutex held always */

	/*

	 * The head array serves three purposes:

	 * - create a LIFO ordering, i.e. return objects that are cache-warm

	 * - reduce the number of spinlock operations.

	 * - reduce the number of linked list operations on the slab and

	 *   bufctl chains: array operations are cheaper.

	 * The numbers are guessed, we should auto-tune as described by

	 * Bonwick.

	/*

	 * CPU bound tasks (e.g. network routing) can exhibit cpu bound

	 * allocation behaviour: Most allocs on one cpu, most free operations

	 * on another cpu. For these cases, an efficient object passing between

	 * cpus is necessary. This is provided by a shared array. The array

	 * replaces Bonwick's magazine layer.

	 * On uniprocessor, it's functionally equivalent (but less efficient)

	 * to a larger limit. Thus disabled by default.

	/*

	 * With debugging enabled, large batchcount lead to excessively long

	 * periods with disabled local interrupts. Limit the batchcount

/*

 * Drain an array if it contains any elements taking the node lock only if

 * necessary. Note that the node listlock also protects the array_cache

 * if drain_array() is used on the shared array.

 ac from n->shared can be freed if we don't hold the slab_mutex. */

/**

 * cache_reap - Reclaim memory from caches.

 * @w: work descriptor

 *

 * Called from workqueue/eventd every few seconds.

 * Purpose:

 * - clear the per-cpu caches for this CPU.

 * - return freeable pages to the main free memory pool.

 *

 * If we cannot acquire the cache chain mutex then just give up - we'll try

 * again on the next iteration.

 Give up. Setup the next iteration. */

		/*

		 * We only take the node lock if absolutely necessary and we

		 * have established with reasonable certainty that

		 * we can do some work if the lock was obtained.

		/*

		 * These are racy checks but it does not matter

		 * if we skip one check or scan twice.

 Set up the next iteration */

 node stats */

 cpu stats */

/**

 * slabinfo_write - Tuning for the slab allocator

 * @file: unused

 * @buffer: user buffer

 * @count: data length

 * @ppos: unused

 *

 * Return: %0 on success, negative error code otherwise.

 Find the cache in the chain of caches. */

/*

 * Rejects incorrectly sized objects and objects that are to be copied

 * to/from userspace but do not fall entirely within the containing slab

 * cache's usercopy region.

 *

 * Returns NULL if check passes, otherwise const char * to name of cache

 * to indicate an error.

 Find and validate object. */

 Find offset within object. */

 Allow address range falling entirely within usercopy region. */

 CONFIG_HARDENED_USERCOPY */

/**

 * __ksize -- Uninstrumented ksize.

 * @objp: pointer to the object

 *

 * Unlike ksize(), __ksize() is uninstrumented, and does not provide the same

 * safety checks as ksize() with KASAN instrumentation enabled.

 *

 * Return: size of the actual memory used by @objp in bytes

/*

 * memfd_create system call and file sealing support

 *

 * Code was originally included in shmem.c, and broken out to facilitate

 * use by hugetlbfs as well as tmpfs.

 *

 * This file is released under the GPL.

/*

 * We need a tag: a new tag would expand every xa_node by 8 bytes,

 * so reuse a tag which we firmly believe is never set or cleared on tmpfs

 * or hugetlbfs because they are memory only filesystems.

 about 150ms max */

/*

 * Setting SEAL_WRITE requires us to verify there's no pending writer. However,

 * via get_user_pages(), drivers might have some pending I/O without any active

 * user-space mappings (eg., direct-IO, AIO). Therefore, we look at all pages

 * and see whether it has an elevated ref-count. If so, we tag them and wait for

 * them to be dropped.

 * The caller must guarantee that no new user will acquire writable references

 * to those pages to avoid races.

				/*

				 * On the last scan, we clean up all those tags

				 * we inserted; but make a note that we still

				 * found pages pinned.

	/*

	 * SEALING

	 * Sealing allows multiple parties to share a tmpfs or hugetlbfs file

	 * but restrict access to a specific subset of file operations. Seals

	 * can only be added, but never removed. This way, mutually untrusted

	 * parties can share common memory regions with a well-defined policy.

	 * A malicious peer can thus never perform unwanted operations on a

	 * shared object.

	 *

	 * Seals are only supported on special tmpfs or hugetlbfs files and

	 * always affect the whole underlying inode. Once a seal is set, it

	 * may prevent some kinds of access to the file. Currently, the

	 * following seals are defined:

	 *   SEAL_SEAL: Prevent further seals from being set on this file

	 *   SEAL_SHRINK: Prevent the file from shrinking

	 *   SEAL_GROW: Prevent the file from growing

	 *   SEAL_WRITE: Prevent write access to the file

	 *

	 * As we don't require any trust relationship between two parties, we

	 * must prevent seals from being removed. Therefore, sealing a file

	 * only adds a given set of seals to the file, it never touches

	 * existing seals. Furthermore, the "setting seals"-operation can be

	 * sealed itself, which basically prevents any further seal from being

	 * added.

	 *

	 * Semantics of sealing are only defined on volatile files. Only

	 * anonymous tmpfs and hugetlbfs files support sealing. More

	 * importantly, seals are never written to disk. Therefore, there's

	 * no plan to support it on other file types.

 disallow upper 32bit */

 Allow huge page size encoding in flags. */

 length includes terminating zero */

 terminating-zero may have changed after strnlen_user() returned */

/*

 * mm/rmap.c - physical to virtual reverse mappings

 *

 * Copyright 2001, Rik van Riel <riel@conectiva.com.br>

 * Released under the General Public License (GPL).

 *

 * Simple, low overhead reverse mapping scheme.

 * Please try to keep this thing as modular as possible.

 *

 * Provides methods for unmapping each kind of mapped page:

 * the anon methods track anonymous pages, and

 * the file methods track pages belonging to an inode.

 *

 * Original design by Rik van Riel <riel@conectiva.com.br> 2001

 * File methods by Dave McCracken <dmccr@us.ibm.com> 2003, 2004

 * Anonymous methods by Andrea Arcangeli <andrea@suse.de> 2004

 * Contributions by Hugh Dickins 2003, 2004

/*

 * Lock ordering in mm:

 *

 * inode->i_rwsem	(while writing or truncating, not reading or faulting)

 *   mm->mmap_lock

 *     mapping->invalidate_lock (in filemap_fault)

 *       page->flags PG_locked (lock_page)   * (see hugetlbfs below)

 *         hugetlbfs_i_mmap_rwsem_key (in huge_pmd_share)

 *           mapping->i_mmap_rwsem

 *             hugetlb_fault_mutex (hugetlbfs specific page fault mutex)

 *             anon_vma->rwsem

 *               mm->page_table_lock or pte_lock

 *                 swap_lock (in swap_duplicate, swap_info_get)

 *                   mmlist_lock (in mmput, drain_mmlist and others)

 *                   mapping->private_lock (in __set_page_dirty_buffers)

 *                     lock_page_memcg move_lock (in __set_page_dirty_buffers)

 *                       i_pages lock (widely used)

 *                         lruvec->lru_lock (in folio_lruvec_lock_irq)

 *                   inode->i_lock (in set_page_dirty's __mark_inode_dirty)

 *                   bdi.wb->list_lock (in set_page_dirty's __mark_inode_dirty)

 *                     sb_lock (within inode_lock in fs/fs-writeback.c)

 *                     i_pages lock (widely used, in set_page_dirty,

 *                               in arch-dependent flush_dcache_mmap_lock,

 *                               within bdi.wb->list_lock in __sync_single_inode)

 *

 * anon_vma->rwsem,mapping->i_mmap_rwsem   (memory_failure, collect_procs_anon)

 *   ->tasklist_lock

 *     pte map lock

 *

 * * hugetlbfs PageHuge() pages take locks in this order:

 *         mapping->i_mmap_rwsem

 *           hugetlb_fault_mutex (hugetlbfs specific page fault mutex)

 *             page->flags PG_locked (lock_page)

 Reference for first vma */

		/*

		 * Initialise the anon_vma root to point to itself. If called

		 * from fork, the root will be reset to the parents anon_vma.

	/*

	 * Synchronize against page_lock_anon_vma_read() such that

	 * we can safely hold the lock without the anon_vma getting

	 * freed.

	 *

	 * Relies on the full mb implied by the atomic_dec_and_test() from

	 * put_anon_vma() against the acquire barrier implied by

	 * down_read_trylock() from page_lock_anon_vma_read(). This orders:

	 *

	 * page_lock_anon_vma_read()	VS	put_anon_vma()

	 *   down_read_trylock()		  atomic_dec_and_test()

	 *   LOCK				  MB

	 *   atomic_read()			  rwsem_is_locked()

	 *

	 * LOCK should suffice since the actual taking of the lock must

	 * happen _before_ what follows.

/**

 * __anon_vma_prepare - attach an anon_vma to a memory region

 * @vma: the memory region in question

 *

 * This makes sure the memory mapping described by 'vma' has

 * an 'anon_vma' attached to it, so that we can associate the

 * anonymous pages mapped into it with that anon_vma.

 *

 * The common case will be that we already have one, which

 * is handled inline by anon_vma_prepare(). But if

 * not we either need to find an adjacent mapping that we

 * can re-use the anon_vma from (very common when the only

 * reason for splitting a vma has been mprotect()), or we

 * allocate a new one.

 *

 * Anon-vma allocations are very subtle, because we may have

 * optimistically looked up an anon_vma in page_lock_anon_vma_read()

 * and that may actually touch the rwsem even in the newly

 * allocated vma (it depends on RCU to make sure that the

 * anon_vma isn't actually destroyed).

 *

 * As a result, we need to do proper anon_vma locking even

 * for the new allocation. At the same time, we do not want

 * to do any locking for the common case of already having

 * an anon_vma.

 *

 * This must be called with the mmap_lock held for reading.

 page_table_lock to protect against threads */

 vma reference or self-parent link for new root */

/*

 * This is a useful helper function for locking the anon_vma root as

 * we traverse the vma->anon_vma_chain, looping over anon_vma's that

 * have the same vma.

 *

 * Such anon_vma's should have the same root, so you'd expect to see

 * just a single mutex_lock for the whole traversal.

/*

 * Attach the anon_vmas from src to dst.

 * Returns 0 on success, -ENOMEM on failure.

 *

 * anon_vma_clone() is called by __vma_adjust(), __split_vma(), copy_vma() and

 * anon_vma_fork(). The first three want an exact copy of src, while the last

 * one, anon_vma_fork(), may try to reuse an existing anon_vma to prevent

 * endless growth of anon_vma. Since dst->anon_vma is set to NULL before call,

 * we can identify this case by checking (!dst->anon_vma && src->anon_vma).

 *

 * If (!dst->anon_vma && src->anon_vma) is true, this function tries to find

 * and reuse existing anon_vma which has no vmas and only one child anon_vma.

 * This prevents degradation of anon_vma hierarchy to endless linear chain in

 * case of constantly forking task. On the other hand, an anon_vma with more

 * than one child isn't reused even if there was no alive vma, thus rmap

 * walker has a good chance of avoiding scanning the whole hierarchy when it

 * searches where page is mapped.

		/*

		 * Reuse existing anon_vma if its degree lower than two,

		 * that means it has no vma and only one anon_vma child.

		 *

		 * Do not chose parent anon_vma, otherwise first child

		 * will always reuse it. Root anon_vma is never reused:

		 * it has self-parent reference and at least one child.

	/*

	 * dst->anon_vma is dropped here otherwise its degree can be incorrectly

	 * decremented in unlink_anon_vmas().

	 * We can safely do this because callers of anon_vma_clone() don't care

	 * about dst->anon_vma if anon_vma_clone() failed.

/*

 * Attach vma to its own anon_vma, as well as to the anon_vmas that

 * the corresponding VMA in the parent process is attached to.

 * Returns 0 on success, non-zero on failure.

 Don't bother if the parent process has no anon_vma here. */

 Drop inherited anon_vma, we'll reuse existing or allocate new. */

	/*

	 * First, attach the new VMA to the parent VMA's anon_vmas,

	 * so rmap can find non-COWed pages in child processes.

 An existing anon_vma has been reused, all done then. */

 Then add our own anon_vma. */

	/*

	 * The root anon_vma's rwsem is the lock actually used when we

	 * lock any of the anon_vmas in this anon_vma tree.

	/*

	 * With refcounts, an anon_vma can stay around longer than the

	 * process it belongs to. The root anon_vma needs to be pinned until

	 * this anon_vma is freed, because the lock lives in the root.

 Mark this anon_vma as the one where our new (COWed) pages go. */

	/*

	 * Unlink each anon_vma chained to the VMA.  This list is ordered

	 * from newest to oldest, ensuring the root anon_vma gets freed last.

		/*

		 * Leave empty anon_vmas on the list - we'll need

		 * to free them outside the lock.

		/*

		 * vma would still be needed after unlink, and anon_vma will be prepared

		 * when handle fault.

	/*

	 * Iterate the list once more, it now only contains empty and unlinked

	 * anon_vmas, destroy them. Could not do before due to __put_anon_vma()

	 * needing to write-acquire the anon_vma->root->rwsem.

/*

 * Getting a lock on a stable anon_vma from a page off the LRU is tricky!

 *

 * Since there is no serialization what so ever against page_remove_rmap()

 * the best this function can do is return a refcount increased anon_vma

 * that might have been relevant to this page.

 *

 * The page might have been remapped to a different anon_vma or the anon_vma

 * returned may already be freed (and even reused).

 *

 * In case it was remapped to a different anon_vma, the new anon_vma will be a

 * child of the old anon_vma, and the anon_vma lifetime rules will therefore

 * ensure that any anon_vma obtained from the page will still be valid for as

 * long as we observe page_mapped() [ hence all those page_mapped() tests ].

 *

 * All users of this function must be very careful when walking the anon_vma

 * chain and verify that the page in question is indeed mapped in it

 * [ something equivalent to page_mapped_in_vma() ].

 *

 * Since anon_vma's slab is SLAB_TYPESAFE_BY_RCU and we know from

 * page_remove_rmap() that the anon_vma pointer from page->mapping is valid

 * if there is a mapcount, we can dereference the anon_vma after observing

 * those.

	/*

	 * If this page is still mapped, then its anon_vma cannot have been

	 * freed.  But if it has been unmapped, we have no security against the

	 * anon_vma structure being freed and reused (for another anon_vma:

	 * SLAB_TYPESAFE_BY_RCU guarantees that - so the atomic_inc_not_zero()

	 * above cannot corrupt).

/*

 * Similar to page_get_anon_vma() except it locks the anon_vma.

 *

 * Its a little more complex as it tries to keep the fast path to a single

 * atomic op -- the trylock. If we fail the trylock, we fall back to getting a

 * reference like with page_get_anon_vma() and then block on the mutex.

		/*

		 * If the page is still mapped, then this anon_vma is still

		 * its anon_vma, and holding the mutex ensures that it will

		 * not go away, see anon_vma_free().

 trylock failed, we got to sleep */

 we pinned the anon_vma, its safe to sleep */

		/*

		 * Oops, we held the last refcount, release the lock

		 * and bail -- can't simply use put_anon_vma() because

		 * we'll deadlock on the anon_vma_lock_write() recursion.

/*

 * Flush TLB entries for recently unmapped pages from remote CPUs. It is

 * important if a PTE was dirty when it was unmapped that it's flushed

 * before any IO is initiated on the page to prevent lost writes. Similarly,

 * it must be flushed before freeing to prevent data leakage.

 Flush iff there are potentially writable TLB entries that can race with IO */

	/*

	 * Ensure compiler does not re-order the setting of tlb_flush_batched

	 * before the PTE is cleared.

	/*

	 * If the PTE was dirty then it's best to assume it's writable. The

	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()

	 * before the page is queued for IO.

/*

 * Returns true if the TLB flush should be deferred to the end of a batch of

 * unmap operations to reduce IPIs.

 If remote CPUs need to be flushed then defer batch the flush */

/*

 * Reclaim unmaps pages under the PTL but do not flush the TLB prior to

 * releasing the PTL if TLB flushes are batched. It's possible for a parallel

 * operation such as mprotect or munmap to race between reclaim unmapping

 * the page and flushing the page. If this race occurs, it potentially allows

 * access to data via a stale TLB entry. Tracking all mm's that have TLB

 * batching in flight would be expensive during reclaim so instead track

 * whether TLB batching occurred in the past and if so then do a flush here

 * if required. This will cost one additional flush per reclaim cycle paid

 * by the first operation at risk such as mprotect and mumap.

 *

 * This must be called under the PTL so that an access to tlb_flush_batched

 * that is potentially a "reclaim vs mprotect/munmap/etc" race will synchronise

 * via the PTL.

		/*

		 * Do not allow the compiler to re-order the clearing of

		 * tlb_flush_batched before the tlb is flushed.

 CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */

/*

 * At what user virtual address is page expected in vma?

 * Caller should check the page is actually part of the vma.

		/*

		 * Note: swapoff's unuse_vma() is more efficient with this

		 * check, and needs it to match anon_vma when KSM is active.

	/*

	 * Some THP functions use the sequence pmdp_huge_clear_flush(), set_pmd_at()

	 * without holding anon_vma lock for write.  So when looking for a

	 * genuine pmde (in which to find pte), test present and !THP together.

/*

 * arg: page_referenced_arg will be passed

 To break the loop */

				/*

				 * Don't treat a reference through

				 * a sequentially read mapping as such.

				 * If the page has been used in another mapping,

				 * we will catch it; if this other mapping is

				 * already gone, the unmap path will have set

				 * PG_referenced or activated the page.

 unexpected pmd-mapped page? */

 To break the loop */

/**

 * page_referenced - test if the page was referenced

 * @page: the page to test

 * @is_locked: caller holds lock on the page

 * @memcg: target memory cgroup

 * @vm_flags: collect encountered vma->vm_flags who actually referenced the page

 *

 * Quick test_and_clear_referenced for all mappings to a page,

 * returns the number of ptes which referenced the page.

	/*

	 * If we are reclaiming on behalf of a cgroup, skip

	 * counting on behalf of references from different

	 * cgroups

	/*

	 * We have to assume the worse case ie pmd for invalidation. Note that

	 * the page can not be free from this function.

 unexpected pmd-mapped page? */

		/*

		 * No need to call mmu_notifier_invalidate_range() as we are

		 * downgrading page table protection not changing it to point

		 * to a new page.

		 *

		 * See Documentation/vm/mmu_notifier.rst

/**

 * page_move_anon_rmap - move a page to our anon_vma

 * @page:	the page to move to our anon_vma

 * @vma:	the vma the page belongs to

 *

 * When a page belongs exclusively to one process after a COW event,

 * that page can be moved into the anon_vma that belongs to just that

 * process, so the rmap code will not search the parent or sibling

 * processes.

	/*

	 * Ensure that anon_vma and the PAGE_MAPPING_ANON bit are written

	 * simultaneously, so a concurrent reader (eg page_referenced()'s

	 * PageAnon()) will not see one without the other.

/**

 * __page_set_anon_rmap - set up new anonymous rmap

 * @page:	Page or Hugepage to add to rmap

 * @vma:	VM area to add page to.

 * @address:	User virtual address of the mapping	

 * @exclusive:	the page is exclusively owned by the current process

	/*

	 * If the page isn't exclusively mapped into this vma,

	 * we must use the _oldest_ possible anon_vma for the

	 * page mapping!

	/*

	 * page_idle does a lockless/optimistic rmap scan on page->mapping.

	 * Make sure the compiler doesn't split the stores of anon_vma and

	 * the PAGE_MAPPING_ANON type identifier, otherwise the rmap code

	 * could mistake the mapping for a struct address_space and crash.

/**

 * __page_check_anon_rmap - sanity check anonymous rmap addition

 * @page:	the page to add the mapping to

 * @vma:	the vm area in which the mapping is added

 * @address:	the user virtual address mapped

	/*

	 * The page's anon-rmap details (mapping and index) are guaranteed to

	 * be set up correctly at this point.

	 *

	 * We have exclusion against page_add_anon_rmap because the caller

	 * always holds the page locked.

	 *

	 * We have exclusion against page_add_new_anon_rmap because those pages

	 * are initially only visible via the pagetables, and the pte is locked

	 * over the call to page_add_new_anon_rmap.

/**

 * page_add_anon_rmap - add pte mapping to an anonymous page

 * @page:	the page to add the mapping to

 * @vma:	the vm area in which the mapping is added

 * @address:	the user virtual address mapped

 * @compound:	charge the page as compound or small page

 *

 * The caller needs to hold the pte lock, and the page must be locked in

 * the anon_vma case: to serialize mapping,index checking after setting,

 * and to ensure that PageAnon is not being upgraded racily to PageKsm

 * (but PageKsm is never downgraded to PageAnon).

/*

 * Special version of the above for do_swap_page, which often runs

 * into pages that are exclusively owned by the current process.

 * Everybody else should continue to use page_add_anon_rmap above.

		/*

		 * We use the irq-unsafe __{inc|mod}_zone_page_stat because

		 * these counters are not modified in interrupt context, and

		 * pte lock(a spinlock) is held, which implies preemption

		 * disabled.

 address might be in next vma when migration races vma_adjust */

/**

 * page_add_new_anon_rmap - add pte mapping to a new anonymous page

 * @page:	the page to add the mapping to

 * @vma:	the vm area in which the mapping is added

 * @address:	the user virtual address mapped

 * @compound:	charge the page as compound or small page

 *

 * Same as page_add_anon_rmap but must only be called on *new* pages.

 * This means the inc-and-test can be bypassed.

 * Page does not have to be locked.

 increment count (starts at -1) */

 Anon THP always mapped first with PMD */

 increment count (starts at -1) */

/**

 * page_add_file_rmap - add pte mapping to a file page

 * @page: the page to add the mapping to

 * @compound: charge the page as compound or small page

 *

 * The caller needs to hold the pte lock.

 Hugepages are not counted in NR_FILE_MAPPED for now. */

 hugetlb pages are always mapped with pmds */

 page still mapped by someone else? */

	/*

	 * We use the irq-unsafe __{inc|mod}_lruvec_page_state because

	 * these counters are not modified in interrupt context, and

	 * pte lock(a spinlock) is held, which implies preemption disabled.

 Hugepages are not counted in NR_ANON_PAGES for now. */

		/*

		 * Subpages can be mapped with PTEs too. Check how many of

		 * them are still mapped.

		/*

		 * Queue the page for deferred split if at least one small

		 * page of the compound page is unmapped, but at least one

		 * small page is still mapped.

/**

 * page_remove_rmap - take down pte mapping from a page

 * @page:	page to remove mapping from

 * @compound:	uncharge the page as compound or small page

 *

 * The caller needs to hold the pte lock.

 page still mapped by someone else? */

	/*

	 * We use the irq-unsafe __{inc|mod}_zone_page_stat because

	 * these counters are not modified in interrupt context, and

	 * pte lock(a spinlock) is held, which implies preemption disabled.

	/*

	 * It would be tidy to reset the PageAnon mapping here,

	 * but that might overwrite a racing page_add_anon_rmap

	 * which increments mapcount after us but sets mapping

	 * before us: so leave the reset to free_unref_page,

	 * and remember that it's only reliable while mapped.

	 * Leaving it set also helps swapoff to reinstate ptes

	 * faster for those pages still in swapcache.

/*

 * @arg: enum ttu_flags will be passed to this argument

	/*

	 * When racing against e.g. zap_pte_range() on another cpu,

	 * in between its ptep_get_and_clear_full() and page_remove_rmap(),

	 * try_to_unmap() may return before page_mapped() has become false,

	 * if page table locking is skipped: use TTU_SYNC to wait for that.

	/*

	 * For THP, we have to assume the worse case ie pmd for invalidation.

	 * For hugetlb, it could be much worse if we need to do pud

	 * invalidation in the case of pmd sharing.

	 *

	 * Note that the page can not be free in this function as call of

	 * try_to_unmap() must hold a reference on the page.

		/*

		 * If sharing is possible, start and end will be adjusted

		 * accordingly.

		/*

		 * If the page is mlock()d, we cannot swap it out.

			/*

			 * PTE-mapped THP are never marked as mlocked: so do

			 * not set it on a DoubleMap THP, nor on an Anon THP

			 * (which may still be PTE-mapped after DoubleMap was

			 * cleared).  But stop unmapping even in those cases.

 Unexpected PMD-mapped THP? */

			/*

			 * To call huge_pmd_unshare, i_mmap_rwsem must be

			 * held in write mode.  Caller needs to explicitly

			 * do this outside rmap routines.

				/*

				 * huge_pmd_unshare unmapped an entire PMD

				 * page.  There is no way of knowing exactly

				 * which PMDs may be cached for this mm, so

				 * we must flush them all.  start/end were

				 * already adjusted above to cover this range.

				/*

				 * The ref count of the PMD page was dropped

				 * which is part of the way map counting

				 * is done for shared PMDs.  Return 'true'

				 * here.  When there is no other sharing,

				 * huge_pmd_unshare returns false and we will

				 * unmap the actual page and drop map count

				 * to zero.

 Nuke the page table entry. */

			/*

			 * We clear the PTE but do not flush so potentially

			 * a remote CPU could still be writing to the page.

			 * If the entry was previously clean then the

			 * architecture must guarantee that a clear->dirty

			 * transition on a cached TLB entry is written through

			 * and traps if the PTE is unmapped.

 Move the dirty bit to the page. Now the pte is gone. */

 Update high watermark before we lower rss */

			/*

			 * The guest indicated that the page content is of no

			 * interest anymore. Simply discard the pte, vmscan

			 * will take care of the rest.

			 * A future reference will then fault in a new zero

			 * page. When userfaultfd is active, we must not drop

			 * this page though, as its main user (postcopy

			 * migration) will not expect userfaults on already

			 * copied pages.

 We have to invalidate as we cleared the pte */

			/*

			 * Store the swap location in the pte.

			 * See handle_pte_fault() ...

 We have to invalidate as we cleared the pte */

 MADV_FREE page check */

 Invalidate as we cleared the pte */

				/*

				 * If the page was redirtied, it cannot be

				 * discarded. Remap the page to page table.

 Invalidate as we cleared the pte */

			/*

			 * This is a locked file-backed page, thus it cannot

			 * be removed from the page cache and replaced by a new

			 * page before mmu_notifier_invalidate_range_end, so no

			 * concurrent thread might update its page table to

			 * point at new page while a device still is using this

			 * page.

			 *

			 * See Documentation/vm/mmu_notifier.rst

		/*

		 * No need to call mmu_notifier_invalidate_range() it has be

		 * done above for all cases requiring it to happen under page

		 * table lock before mmu_notifier_invalidate_range_end()

		 *

		 * See Documentation/vm/mmu_notifier.rst

/**

 * try_to_unmap - try to remove all page table mappings to a page

 * @page: the page to get unmapped

 * @flags: action and flags

 *

 * Tries to remove all the page table entries which are mapping this

 * page, used in the pageout path.  Caller must hold the page lock.

 *

 * It is the caller's responsibility to check if the page is still

 * mapped when needed (use TTU_SYNC to prevent accounting races).

/*

 * @arg: enum ttu_flags will be passed to this argument.

 *

 * If TTU_SPLIT_HUGE_PMD is specified any PMD mappings will be split into PTEs

 * containing migration entries.

	/*

	 * When racing against e.g. zap_pte_range() on another cpu,

	 * in between its ptep_get_and_clear_full() and page_remove_rmap(),

	 * try_to_migrate() may return before page_mapped() has become false,

	 * if page table locking is skipped: use TTU_SYNC to wait for that.

	/*

	 * unmap_page() in mm/huge_memory.c is the only user of migration with

	 * TTU_SPLIT_HUGE_PMD and it wants to freeze.

	/*

	 * For THP, we have to assume the worse case ie pmd for invalidation.

	 * For hugetlb, it could be much worse if we need to do pud

	 * invalidation in the case of pmd sharing.

	 *

	 * Note that the page can not be free in this function as call of

	 * try_to_unmap() must hold a reference on the page.

		/*

		 * If sharing is possible, start and end will be adjusted

		 * accordingly.

 PMD-mapped THP migration entry */

 Unexpected PMD-mapped THP? */

			/*

			 * To call huge_pmd_unshare, i_mmap_rwsem must be

			 * held in write mode.  Caller needs to explicitly

			 * do this outside rmap routines.

				/*

				 * huge_pmd_unshare unmapped an entire PMD

				 * page.  There is no way of knowing exactly

				 * which PMDs may be cached for this mm, so

				 * we must flush them all.  start/end were

				 * already adjusted above to cover this range.

				/*

				 * The ref count of the PMD page was dropped

				 * which is part of the way map counting

				 * is done for shared PMDs.  Return 'true'

				 * here.  When there is no other sharing,

				 * huge_pmd_unshare returns false and we will

				 * unmap the actual page and drop map count

				 * to zero.

 Nuke the page table entry. */

 Move the dirty bit to the page. Now the pte is gone. */

 Update high watermark before we lower rss */

			/*

			 * Store the pfn of the page in a special migration

			 * pte. do_swap_page() will wait until the migration

			 * pte is removed and then restart fault handling.

			/*

			 * pteval maps a zone device page and is therefore

			 * a swap pte.

			/*

			 * No need to invalidate here it will synchronize on

			 * against the special swap migration pte.

			 *

			 * The assignment to subpage above was computed from a

			 * swap PTE which results in an invalid pointer.

			 * Since only PAGE_SIZE pages can currently be

			 * migrated, just set it to page. This will need to be

			 * changed when hugepage migrations to device private

			 * memory are supported.

			/*

			 * The guest indicated that the page content is of no

			 * interest anymore. Simply discard the pte, vmscan

			 * will take care of the rest.

			 * A future reference will then fault in a new zero

			 * page. When userfaultfd is active, we must not drop

			 * this page though, as its main user (postcopy

			 * migration) will not expect userfaults on already

			 * copied pages.

 We have to invalidate as we cleared the pte */

			/*

			 * Store the pfn of the page in a special migration

			 * pte. do_swap_page() will wait until the migration

			 * pte is removed and then restart fault handling.

			/*

			 * No need to invalidate here it will synchronize on

			 * against the special swap migration pte.

		/*

		 * No need to call mmu_notifier_invalidate_range() it has be

		 * done above for all cases requiring it to happen under page

		 * table lock before mmu_notifier_invalidate_range_end()

		 *

		 * See Documentation/vm/mmu_notifier.rst

/**

 * try_to_migrate - try to replace all page table mappings with swap entries

 * @page: the page to replace page table entries for

 * @flags: action and flags

 *

 * Tries to remove all the page table entries which are mapping this page and

 * replace them with special swap entries. Caller must hold the page lock.

	/*

	 * Migration always ignores mlock and only supports TTU_RMAP_LOCKED and

	 * TTU_SPLIT_HUGE_PMD and TTU_SYNC flags.

	/*

	 * During exec, a temporary VMA is setup and later moved.

	 * The VMA is moved under the anon_vma lock but not the

	 * page tables leading to a race where migration cannot

	 * find the migration ptes. Rather than increasing the

	 * locking requirements of exec(), migration skips

	 * temporary VMAs until after exec() completes.

/*

 * Walks the vma's mapping a page and mlocks the page if any locked vma's are

 * found. Once one is found the page is locked and the scan can be terminated.

 An un-locked vma doesn't have any pages to lock, continue the scan */

		/*

		 * Need to recheck under the ptl to serialise with

		 * __munlock_pagevec_fill() after VM_LOCKED is cleared in

		 * munlock_vma_pages_range().

			/*

			 * PTE-mapped THP are never marked as mlocked; but

			 * this function is never called on a DoubleMap THP,

			 * nor on an Anon THP (which may still be PTE-mapped

			 * after DoubleMap was cleared).

			/*

			 * No need to scan further once the page is marked

			 * as mlocked.

/**

 * page_mlock - try to mlock a page

 * @page: the page to be mlocked

 *

 * Called from munlock code. Checks all of the VMAs mapping the page and mlocks

 * the page if any are found. The page will be returned with PG_mlocked cleared

 * if it is not mapped by any locked vmas.

 Anon THP are only marked as mlocked when singly mapped */

 Unexpected PMD-mapped THP? */

 Nuke the page table entry. */

 Move the dirty bit to the page. Now the pte is gone. */

		/*

		 * Check that our target page is still mapped at the expected

		 * address.

		/*

		 * Store the pfn of the page in a special migration

		 * pte. do_swap_page() will wait until the migration

		 * pte is removed and then restart fault handling.

		/*

		 * There is a reference on the page for the swap entry which has

		 * been removed, so shouldn't take another.

/**

 * page_make_device_exclusive - mark the page exclusively owned by a device

 * @page: the page to replace page table entries for

 * @mm: the mm_struct where the page is expected to be mapped

 * @address: address where the page is expected to be mapped

 * @owner: passed to MMU_NOTIFY_EXCLUSIVE range notifier callbacks

 *

 * Tries to remove all the page table entries which are mapping this page and

 * replace them with special device exclusive swap entries to grant a device

 * exclusive access to the page. Caller must hold the page lock.

 *

 * Returns false if the page is still mapped, or if it could not be unmapped

 * from the expected address. Otherwise returns true (success).

	/*

	 * Restrict to anonymous pages for now to avoid potential writeback

	 * issues. Also tail pages shouldn't be passed to rmap_walk so skip

	 * those.

/**

 * make_device_exclusive_range() - Mark a range for exclusive use by a device

 * @mm: mm_struct of assoicated target process

 * @start: start of the region to mark for exclusive device access

 * @end: end address of region

 * @pages: returns the pages which were successfully marked for exclusive access

 * @owner: passed to MMU_NOTIFY_EXCLUSIVE range notifier to allow filtering

 *

 * Returns: number of pages found in the range by GUP. A page is marked for

 * exclusive access only if the page pointer is non-NULL.

 *

 * This function finds ptes mapping page(s) to the given address range, locks

 * them and replaces mappings with special swap entries preventing userspace CPU

 * access. On fault these entries are replaced with the original mapping after

 * calling MMU notifiers.

 *

 * A driver using this to program access from a device must use a mmu notifier

 * critical section to hold a device specific lock during programming. Once

 * programming is complete it should drop the page lock and reference after

 * which point CPU access to the page will revoke the exclusive access.

	/*

	 * Note: remove_migration_ptes() cannot use page_lock_anon_vma_read()

	 * because that depends on page_mapped(); but not all its usages

	 * are holding mmap_lock. Users without mmap_lock are required to

	 * take a reference count to prevent the anon_vma disappearing

/*

 * rmap_walk_anon - do something to anonymous page using the object-based

 * rmap method

 * @page: the page to be handled

 * @rwc: control variable according to each walk type

 *

 * Find all the mappings of a page using the mapping pointer and the vma chains

 * contained in the anon_vma struct it points to.

 *

 * When called from page_mlock(), the mmap_lock of the mm containing the vma

 * where the page was found will be held for write.  So, we won't recheck

 * vm_flags for that VMA.  That should be OK, because that vma shouldn't be

 * LOCKED.

 anon_vma disappear under us? */

/*

 * rmap_walk_file - do something to file page using the object-based rmap method

 * @page: the page to be handled

 * @rwc: control variable according to each walk type

 *

 * Find all the mappings of a page using the mapping pointer and the vma chains

 * contained in the address_space struct it points to.

 *

 * When called from page_mlock(), the mmap_lock of the mm containing the vma

 * where the page was found will be held for write.  So, we won't recheck

 * vm_flags for that VMA.  That should be OK, because that vma shouldn't be

 * LOCKED.

	/*

	 * The page lock not only makes sure that page->mapping cannot

	 * suddenly be NULLified by truncation, it makes sure that the

	 * structure at mapping cannot be freed and reused yet,

	 * so we can safely take mapping->i_mmap_rwsem.

 Like rmap_walk, but caller holds relevant rmap lock */

 no ksm support for now */

/*

 * The following two functions are for anonymous (private mapped) hugepages.

 * Unlike common anonymous pages, anonymous hugepages have no accounting code

 * and no lru code, because we handle hugepages differently from common pages.

 address might be in next vma when migration races vma_adjust */

 CONFIG_HUGETLB_PAGE */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Procedures for maintaining information about logical memory blocks.

 *

 * Peter Bergner, IBM Corp.	June 2001.

 * Copyright (C) 2001 Peter Bergner.

/**

 * DOC: memblock overview

 *

 * Memblock is a method of managing memory regions during the early

 * boot period when the usual kernel memory allocators are not up and

 * running.

 *

 * Memblock views the system memory as collections of contiguous

 * regions. There are several types of these collections:

 *

 * * ``memory`` - describes the physical memory available to the

 *   kernel; this may differ from the actual physical memory installed

 *   in the system, for instance when the memory is restricted with

 *   ``mem=`` command line parameter

 * * ``reserved`` - describes the regions that were allocated

 * * ``physmem`` - describes the actual physical memory available during

 *   boot regardless of the possible restrictions and memory hot(un)plug;

 *   the ``physmem`` type is only available on some architectures.

 *

 * Each region is represented by struct memblock_region that

 * defines the region extents, its attributes and NUMA node id on NUMA

 * systems. Every memory type is described by the struct memblock_type

 * which contains an array of memory regions along with

 * the allocator metadata. The "memory" and "reserved" types are nicely

 * wrapped with struct memblock. This structure is statically

 * initialized at build time. The region arrays are initially sized to

 * %INIT_MEMBLOCK_REGIONS for "memory" and %INIT_MEMBLOCK_RESERVED_REGIONS

 * for "reserved". The region array for "physmem" is initially sized to

 * %INIT_PHYSMEM_REGIONS.

 * The memblock_allow_resize() enables automatic resizing of the region

 * arrays during addition of new regions. This feature should be used

 * with care so that memory allocated for the region array will not

 * overlap with areas that should be reserved, for example initrd.

 *

 * The early architecture setup should tell memblock what the physical

 * memory layout is by using memblock_add() or memblock_add_node()

 * functions. The first function does not assign the region to a NUMA

 * node and it is appropriate for UMA systems. Yet, it is possible to

 * use it on NUMA systems as well and assign the region to a NUMA node

 * later in the setup process using memblock_set_node(). The

 * memblock_add_node() performs such an assignment directly.

 *

 * Once memblock is setup the memory can be allocated using one of the

 * API variants:

 *

 * * memblock_phys_alloc*() - these functions return the **physical**

 *   address of the allocated memory

 * * memblock_alloc*() - these functions return the **virtual** address

 *   of the allocated memory.

 *

 * Note, that both API variants use implicit assumptions about allowed

 * memory ranges and the fallback methods. Consult the documentation

 * of memblock_alloc_internal() and memblock_alloc_range_nid()

 * functions for more elaborate description.

 *

 * As the system boot progresses, the architecture specific mem_init()

 * function frees all the memory to the buddy page allocator.

 *

 * Unless an architecture enables %CONFIG_ARCH_KEEP_MEMBLOCK, the

 * memblock data structures (except "physmem") will be discarded after the

 * system initialization completes.

 empty dummy entry */

 empty dummy entry */

 empty dummy entry */

/*

 * keep a pointer to &memblock.memory in the text section to use it in

 * __next_mem_range() and its helpers.

 *  For architectures that do not keep memblock data after init, this

 * pointer will be reset to NULL at memblock_discard()

 adjust *@size so that (@base + *@size) doesn't overflow, return new size */

/*

 * Address comparison utilities

/**

 * __memblock_find_range_bottom_up - find free area utility in bottom-up

 * @start: start of candidate range

 * @end: end of candidate range, can be %MEMBLOCK_ALLOC_ANYWHERE or

 *       %MEMBLOCK_ALLOC_ACCESSIBLE

 * @size: size of free area to find

 * @align: alignment of free area to find

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 * @flags: pick from blocks based on memory attributes

 *

 * Utility called from memblock_find_in_range_node(), find free area bottom-up.

 *

 * Return:

 * Found address on success, 0 on failure.

/**

 * __memblock_find_range_top_down - find free area utility, in top-down

 * @start: start of candidate range

 * @end: end of candidate range, can be %MEMBLOCK_ALLOC_ANYWHERE or

 *       %MEMBLOCK_ALLOC_ACCESSIBLE

 * @size: size of free area to find

 * @align: alignment of free area to find

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 * @flags: pick from blocks based on memory attributes

 *

 * Utility called from memblock_find_in_range_node(), find free area top-down.

 *

 * Return:

 * Found address on success, 0 on failure.

/**

 * memblock_find_in_range_node - find free area in given range and node

 * @size: size of free area to find

 * @align: alignment of free area to find

 * @start: start of candidate range

 * @end: end of candidate range, can be %MEMBLOCK_ALLOC_ANYWHERE or

 *       %MEMBLOCK_ALLOC_ACCESSIBLE

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 * @flags: pick from blocks based on memory attributes

 *

 * Find @size free area aligned to @align in the specified range and node.

 *

 * Return:

 * Found address on success, 0 on failure.

 pump up @end */

 avoid allocating the first page */

/**

 * memblock_find_in_range - find free area in given range

 * @start: start of candidate range

 * @end: end of candidate range, can be %MEMBLOCK_ALLOC_ANYWHERE or

 *       %MEMBLOCK_ALLOC_ACCESSIBLE

 * @size: size of free area to find

 * @align: alignment of free area to find

 *

 * Find @size free area aligned to @align in the specified range.

 *

 * Return:

 * Found address on success, 0 on failure.

 Special case for empty arrays */

/**

 * memblock_discard - discard memory and reserved arrays if they were allocated

/**

 * memblock_double_array - double the size of the memblock regions array

 * @type: memblock type of the regions array being doubled

 * @new_area_start: starting address of memory range to avoid overlap with

 * @new_area_size: size of memory range to avoid overlap with

 *

 * Double the size of the @type regions array. If memblock is being used to

 * allocate memory for a new reserved regions array and there is a previously

 * allocated memory range [@new_area_start, @new_area_start + @new_area_size]

 * waiting to be reserved, ensure the memory used by the new array does

 * not overlap.

 *

 * Return:

 * 0 on success, -1 on failure.

	/* We don't allow resizing until we know about the reserved regions

	 * of memory that aren't suitable for allocation

 Calculate new doubled size */

	/*

	 * We need to allocated new one align to PAGE_SIZE,

	 *   so we can free them completely later.

 Retrieve the slab flag */

 Try to find some space for it */

 only exclude range when trying to double reserved.regions */

	/*

	 * Found space, we now need to move the array over before we add the

	 * reserved region since it may be our reserved array itself that is

	 * full.

 Free old array. We needn't free it if the array is the static one */

	/*

	 * Reserve the new array if that comes from the memblock.  Otherwise, we

	 * needn't do it

 Update slab flag */

/**

 * memblock_merge_regions - merge neighboring compatible regions

 * @type: memblock type to scan

 *

 * Scan @type and merge neighboring compatible regions.

 cnt never goes below 1 */

 move forward from next + 1, index of which is i + 2 */

/**

 * memblock_insert_region - insert new memblock region

 * @type:	memblock type to insert into

 * @idx:	index for the insertion point

 * @base:	base address of the new region

 * @size:	size of the new region

 * @nid:	node id of the new region

 * @flags:	flags of the new region

 *

 * Insert new memblock region [@base, @base + @size) into @type at @idx.

 * @type must already have extra room to accommodate the new region.

/**

 * memblock_add_range - add new memblock region

 * @type: memblock type to add new region into

 * @base: base address of the new region

 * @size: size of the new region

 * @nid: nid of the new region

 * @flags: flags of the new region

 *

 * Add new memblock region [@base, @base + @size) into @type.  The new region

 * is allowed to overlap with existing ones - overlaps don't affect already

 * existing regions.  @type is guaranteed to be minimal (all neighbouring

 * compatible regions are merged) after the addition.

 *

 * Return:

 * 0 on success, -errno on failure.

 special case for empty array */

	/*

	 * The following is executed twice.  Once with %false @insert and

	 * then with %true.  The first counts the number of regions needed

	 * to accommodate the new area.  The second actually inserts them.

		/*

		 * @rgn overlaps.  If it separates the lower part of new

		 * area, insert that portion.

 area below @rend is dealt with, forget about it */

 insert the remaining portion */

	/*

	 * If this was the first round, resize array and repeat for actual

	 * insertions; otherwise, merge and return.

/**

 * memblock_add_node - add new memblock region within a NUMA node

 * @base: base address of the new region

 * @size: size of the new region

 * @nid: nid of the new region

 * @flags: flags of the new region

 *

 * Add new memblock region [@base, @base + @size) to the "memory"

 * type. See memblock_add_range() description for mode details

 *

 * Return:

 * 0 on success, -errno on failure.

/**

 * memblock_add - add new memblock region

 * @base: base address of the new region

 * @size: size of the new region

 *

 * Add new memblock region [@base, @base + @size) to the "memory"

 * type. See memblock_add_range() description for mode details

 *

 * Return:

 * 0 on success, -errno on failure.

/**

 * memblock_isolate_range - isolate given range into disjoint memblocks

 * @type: memblock type to isolate range for

 * @base: base of range to isolate

 * @size: size of range to isolate

 * @start_rgn: out parameter for the start of isolated region

 * @end_rgn: out parameter for the end of isolated region

 *

 * Walk @type and ensure that regions don't cross the boundaries defined by

 * [@base, @base + @size).  Crossing regions are split at the boundaries,

 * which may create at most two more regions.  The index of the first

 * region inside the range is returned in *@start_rgn and end in *@end_rgn.

 *

 * Return:

 * 0 on success, -errno on failure.

 we'll create at most two more regions */

			/*

			 * @rgn intersects from below.  Split and continue

			 * to process the next region - the new top half.

			/*

			 * @rgn intersects from above.  Split and redo the

			 * current region - the new bottom half.

 @rgn is fully contained, record it */

/**

 * memblock_free - free boot memory allocation

 * @ptr: starting address of the  boot memory allocation

 * @size: size of the boot memory block in bytes

 *

 * Free boot memory block previously allocated by memblock_alloc_xx() API.

 * The freeing memory will not be released to the buddy allocator.

/**

 * memblock_phys_free - free boot memory block

 * @base: phys starting address of the  boot memory block

 * @size: size of the boot memory block in bytes

 *

 * Free boot memory block previously allocated by memblock_alloc_xx() API.

 * The freeing memory will not be released to the buddy allocator.

/**

 * memblock_setclr_flag - set or clear flag for a memory region

 * @base: base address of the region

 * @size: size of the region

 * @set: set or clear the flag

 * @flag: the flag to update

 *

 * This function isolates region [@base, @base + @size), and sets/clears flag

 *

 * Return: 0 on success, -errno on failure.

/**

 * memblock_mark_hotplug - Mark hotpluggable memory with flag MEMBLOCK_HOTPLUG.

 * @base: the base phys addr of the region

 * @size: the size of the region

 *

 * Return: 0 on success, -errno on failure.

/**

 * memblock_clear_hotplug - Clear flag MEMBLOCK_HOTPLUG for a specified region.

 * @base: the base phys addr of the region

 * @size: the size of the region

 *

 * Return: 0 on success, -errno on failure.

/**

 * memblock_mark_mirror - Mark mirrored memory with flag MEMBLOCK_MIRROR.

 * @base: the base phys addr of the region

 * @size: the size of the region

 *

 * Return: 0 on success, -errno on failure.

/**

 * memblock_mark_nomap - Mark a memory region with flag MEMBLOCK_NOMAP.

 * @base: the base phys addr of the region

 * @size: the size of the region

 *

 * The memory regions marked with %MEMBLOCK_NOMAP will not be added to the

 * direct mapping of the physical memory. These regions will still be

 * covered by the memory map. The struct page representing NOMAP memory

 * frames in the memory map will be PageReserved()

 *

 * Note: if the memory being marked %MEMBLOCK_NOMAP was allocated from

 * memblock, the caller must inform kmemleak to ignore that memory

 *

 * Return: 0 on success, -errno on failure.

/**

 * memblock_clear_nomap - Clear flag MEMBLOCK_NOMAP for a specified region.

 * @base: the base phys addr of the region

 * @size: the size of the region

 *

 * Return: 0 on success, -errno on failure.

 we never skip regions when iterating memblock.reserved or physmem */

 only memory regions are associated with nodes, check it */

 skip hotpluggable memory regions if needed */

 if we want mirror memory skip non-mirror memory regions */

 skip nomap memory unless we were asked for it explicitly */

 skip driver-managed memory unless we were asked for it explicitly */

/**

 * __next_mem_range - next function for for_each_free_mem_range() etc.

 * @idx: pointer to u64 loop variable

 * @nid: node selector, %NUMA_NO_NODE for all nodes

 * @flags: pick from blocks based on memory attributes

 * @type_a: pointer to memblock_type from where the range is taken

 * @type_b: pointer to memblock_type which excludes memory from being taken

 * @out_start: ptr to phys_addr_t for start address of the range, can be %NULL

 * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL

 * @out_nid: ptr to int for nid of the range, can be %NULL

 *

 * Find the first area from *@idx which matches @nid, fill the out

 * parameters, and update *@idx for the next iteration.  The lower 32bit of

 * *@idx contains index into type_a and the upper 32bit indexes the

 * areas before each region in type_b.	For example, if type_b regions

 * look like the following,

 *

 *	0:[0-16), 1:[32-48), 2:[128-130)

 *

 * The upper 32bit indexes the following regions.

 *

 *	0:[0-0), 1:[16-32), 2:[48-128), 3:[130-MAX)

 *

 * As both region arrays are sorted, the function advances the two indices

 * in lockstep and returns each intersection.

 scan areas before each reservation */

			/*

			 * if idx_b advanced past idx_a,

			 * break out to advance idx_a

 if the two regions intersect, we're done */

				/*

				 * The region which ends first is

				 * advanced for the next iteration.

 signal end of iteration */

/**

 * __next_mem_range_rev - generic next function for for_each_*_range_rev()

 *

 * @idx: pointer to u64 loop variable

 * @nid: node selector, %NUMA_NO_NODE for all nodes

 * @flags: pick from blocks based on memory attributes

 * @type_a: pointer to memblock_type from where the range is taken

 * @type_b: pointer to memblock_type which excludes memory from being taken

 * @out_start: ptr to phys_addr_t for start address of the range, can be %NULL

 * @out_end: ptr to phys_addr_t for end address of the range, can be %NULL

 * @out_nid: ptr to int for nid of the range, can be %NULL

 *

 * Finds the next range from type_a which is not marked as unsuitable

 * in type_b.

 *

 * Reverse of __next_mem_range().

 scan areas before each reservation */

			/*

			 * if idx_b advanced past idx_a,

			 * break out to advance idx_a

 if the two regions intersect, we're done */

 signal end of iteration */

/*

 * Common iterator interface used to define for_each_mem_pfn_range().

/**

 * memblock_set_node - set node ID on memblock regions

 * @base: base of area to set node ID for

 * @size: size of area to set node ID for

 * @type: memblock type to set node ID for

 * @nid: node ID to set

 *

 * Set the nid of memblock @type regions in [@base, @base + @size) to @nid.

 * Regions which cross the area boundaries are split as necessary.

 *

 * Return:

 * 0 on success, -errno on failure.

/**

 * __next_mem_pfn_range_in_zone - iterator for for_each_*_range_in_zone()

 *

 * @idx: pointer to u64 loop variable

 * @zone: zone in which all of the memory blocks reside

 * @out_spfn: ptr to ulong for start pfn of the range, can be %NULL

 * @out_epfn: ptr to ulong for end pfn of the range, can be %NULL

 *

 * This function is meant to be a zone/pfn specific wrapper for the

 * for_each_mem_range type iterators. Specifically they are used in the

 * deferred memory init routines and as such we were duplicating much of

 * this logic throughout the code. So instead of having it in multiple

 * locations it seemed like it would make more sense to centralize this to

 * one new iterator that does everything they need.

		/*

		 * Verify the end is at least past the start of the zone and

		 * that we have at least one PFN to initialize.

 if we went too far just stop searching */

 signal end of iteration */

 CONFIG_DEFERRED_STRUCT_PAGE_INIT */

/**

 * memblock_alloc_range_nid - allocate boot memory block

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @start: the lower bound of the memory region to allocate (phys address)

 * @end: the upper bound of the memory region to allocate (phys address)

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 * @exact_nid: control the allocation fall back to other nodes

 *

 * The allocation is performed from memory region limited by

 * memblock.current_limit if @end == %MEMBLOCK_ALLOC_ACCESSIBLE.

 *

 * If the specified node can not hold the requested memory and @exact_nid

 * is false, the allocation falls back to any node in the system.

 *

 * For systems with memory mirroring, the allocation is attempted first

 * from the regions with mirroring enabled and then retried from any

 * memory region.

 *

 * In addition, function sets the min_count to 0 using kmemleak_alloc_phys for

 * allocated boot memory block, so that it is never reported as leaks.

 *

 * Return:

 * Physical address of allocated memory block on success, %0 on failure.

 Can't use WARNs this early in boot on powerpc */

	/*

	 * Skip kmemleak for those places like kasan_init() and

	 * early_pgtable_alloc() due to high volume.

		/*

		 * The min_count is set to 0 so that memblock allocated

		 * blocks are never reported as leaks. This is because many

		 * of these blocks are only referred via the physical

		 * address which is not looked up by kmemleak.

/**

 * memblock_phys_alloc_range - allocate a memory block inside specified range

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @start: the lower bound of the memory region to allocate (physical address)

 * @end: the upper bound of the memory region to allocate (physical address)

 *

 * Allocate @size bytes in the between @start and @end.

 *

 * Return: physical address of the allocated memory block on success,

 * %0 on failure.

/**

 * memblock_phys_alloc_try_nid - allocate a memory block from specified NUMA node

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 *

 * Allocates memory block from the specified NUMA node. If the node

 * has no available memory, attempts to allocated from any node in the

 * system.

 *

 * Return: physical address of the allocated memory block on success,

 * %0 on failure.

/**

 * memblock_alloc_internal - allocate boot memory block

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @min_addr: the lower bound of the memory region to allocate (phys address)

 * @max_addr: the upper bound of the memory region to allocate (phys address)

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 * @exact_nid: control the allocation fall back to other nodes

 *

 * Allocates memory block using memblock_alloc_range_nid() and

 * converts the returned physical address to virtual.

 *

 * The @min_addr limit is dropped if it can not be satisfied and the allocation

 * will fall back to memory below @min_addr. Other constraints, such

 * as node and mirrored memory will be handled again in

 * memblock_alloc_range_nid().

 *

 * Return:

 * Virtual address of allocated memory block on success, NULL on failure.

	/*

	 * Detect any accidental use of these APIs after slab is ready, as at

	 * this moment memblock may be deinitialized already and its

	 * internal data may be destroyed (after execution of memblock_free_all)

 retry allocation without lower limit */

/**

 * memblock_alloc_exact_nid_raw - allocate boot memory block on the exact node

 * without zeroing memory

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @min_addr: the lower bound of the memory region from where the allocation

 *	  is preferred (phys address)

 * @max_addr: the upper bound of the memory region from where the allocation

 *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to

 *	      allocate only from memory limited by memblock.current_limit value

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 *

 * Public function, provides additional debug information (including caller

 * info), if enabled. Does not zero allocated memory.

 *

 * Return:

 * Virtual address of allocated memory block on success, NULL on failure.

/**

 * memblock_alloc_try_nid_raw - allocate boot memory block without zeroing

 * memory and without panicking

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @min_addr: the lower bound of the memory region from where the allocation

 *	  is preferred (phys address)

 * @max_addr: the upper bound of the memory region from where the allocation

 *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to

 *	      allocate only from memory limited by memblock.current_limit value

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 *

 * Public function, provides additional debug information (including caller

 * info), if enabled. Does not zero allocated memory, does not panic if request

 * cannot be satisfied.

 *

 * Return:

 * Virtual address of allocated memory block on success, NULL on failure.

/**

 * memblock_alloc_try_nid - allocate boot memory block

 * @size: size of memory block to be allocated in bytes

 * @align: alignment of the region and block's size

 * @min_addr: the lower bound of the memory region from where the allocation

 *	  is preferred (phys address)

 * @max_addr: the upper bound of the memory region from where the allocation

 *	      is preferred (phys address), or %MEMBLOCK_ALLOC_ACCESSIBLE to

 *	      allocate only from memory limited by memblock.current_limit value

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 *

 * Public function, provides additional debug information (including caller

 * info), if enabled. This function zeroes the allocated memory.

 *

 * Return:

 * Virtual address of allocated memory block on success, NULL on failure.

/**

 * memblock_free_late - free pages directly to buddy allocator

 * @base: phys starting address of the  boot memory block

 * @size: size of the boot memory block in bytes

 *

 * This is only useful when the memblock allocator has already been torn

 * down, but we are still initializing the system.  Pages are released directly

 * to the buddy allocator.

/*

 * Remaining API functions

 lowest address */

	/*

	 * translate the memory @limit size into the max address within one of

	 * the memory memblock regions, if the @limit exceeds the total size

	 * of those regions, max_addr will keep original value PHYS_ADDR_MAX

 @limit exceeds the total size of the memory, do nothing */

 truncate both memory and reserved regions */

 remove all the MAP regions */

 truncate the reserved regions */

 @limit exceeds the total size of the memory, do nothing */

/**

 * memblock_is_region_memory - check if a region is a subset of memory

 * @base: base of region to check

 * @size: size of region to check

 *

 * Check if the region [@base, @base + @size) is a subset of a memory block.

 *

 * Return:

 * 0 if false, non-zero if true

/**

 * memblock_is_region_reserved - check if a region intersects reserved memory

 * @base: base of region to check

 * @size: size of region to check

 *

 * Check if the region [@base, @base + @size) intersects a reserved

 * memory block.

 *

 * Return:

 * True if they intersect, false if not.

	/*

	 * Convert start_pfn/end_pfn to a struct page pointer.

	/*

	 * Convert to physical addresses, and round start upwards and end

	 * downwards.

	/*

	 * If there are free pages between these, free the section of the

	 * memmap array.

/*

 * The mem_map array can get very big.  Free the unused area of the memory map.

	/*

	 * This relies on each bank being in address order.

	 * The banks are sorted previously in bootmem_init().

		/*

		 * Take care not to free memmap entries that don't exist

		 * due to SPARSEMEM sections which aren't present.

		/*

		 * Align down here since many operations in VM subsystem

		 * presume that there are no holes in the memory map inside

		 * a pageblock

		/*

		 * If we had a previous bank, and there is a space

		 * between the current bank and the previous, free it.

		/*

		 * Align up here since many operations in VM subsystem

		 * presume that there are no holes in the memory map inside

		 * a pageblock

 initialize struct pages for the reserved regions */

 and also treat struct pages for the NOMAP regions as PageReserved */

	/*

	 * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id

	 *  because in some case like Node0 doesn't have RAM installed

	 *  low ram will be on Node1

/**

 * memblock_free_all - release free pages to the buddy allocator

 CONFIG_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/memory_hotplug.c

 *

 *  Copyright (C)

/*

 * memory_hotplug.memmap_on_memory parameter

/*

 * memory_hotplug.online_policy: configure online behavior when onlining without

 * specifying a zone (MMOP_ONLINE)

 *

 * "contig-zones": keep zone contiguous

 * "auto-movable": online memory to ZONE_MOVABLE if the configuration

 *                 (auto_movable_ratio, auto_movable_numa_aware) allows for it

/*

 * memory_hotplug.auto_movable_ratio: specify maximum MOVABLE:KERNEL ratio

 *

 * The ratio represent an upper limit and the kernel might decide to not

 * online some memory to ZONE_MOVABLE -- e.g., because hotplugged KERNEL memory

 * doesn't allow for more MOVABLE memory.

/*

 * memory_hotplug.auto_movable_numa_aware: consider numa node stats

 CONFIG_NUMA */

/*

 * online_page_callback contains pointer to current page onlining function.

 * Initially it is generic_online_page(). If it is required it could be

 * changed by calling set_online_page_callback() for callback registration

 * and restore_online_page_callback() for generic callback restore.

 add this memory to iomem resource */

	/*

	 * Make sure value parsed from 'mem=' only restricts memory adding

	 * while booting, so that memory hotplug won't be impacted. Please

	 * refer to document of 'mem=' in kernel-parameters.txt for more

	 * details.

	/*

	 * Request ownership of the new memory range.  This might be

	 * a child of an existing resource that was present but

	 * not marked as busy.

	/*

	 * Disallow all operations smaller than a sub-section and only

	 * allow operations smaller than a section for

	 * SPARSEMEM_VMEMMAP. Note that check_hotplug_memory_range()

	 * enforces a larger memory_block_size_bytes() granularity for

	 * memory that will be marked online, so this check should only

	 * fire for direct arch_{add,remove}_memory() users outside of

	 * add_memory_resource().

/*

 * Return page for the valid pfn only if the page is online. All pfn

 * walkers which rely on the fully initialized page->flags and others

 * should use this rather than pfn_valid && pfn_to_page

	/*

	 * Save some code text when online_section() +

	 * pfn_section_valid() are sufficient.

	/*

	 * Slowpath: when ZONE_DEVICE collides with

	 * ZONE_{NORMAL,MOVABLE} within the same section some pfns in

	 * the section may be 'offline' but 'valid'. Only

	 * get_dev_pagemap() can determine sub-section online status.

 The presence of a pgmap indicates ZONE_DEVICE offline pfn */

/*

 * Reasonably generic function for adding memory.  It is

 * expected that archs that support memory hotplug will

 * call this function after deciding the zone to which to

 * add the new pages.

		/*

		 * Validate altmap is within bounds of the total request

 Select all remaining pages up to the next section boundary */

 find the smallest valid pfn in the range [start_pfn, end_pfn) */

 find the biggest valid pfn in the range [start_pfn, end_pfn). */

 pfn is the end pfn of a memory section. */

		/*

		 * If the section is smallest section in the zone, it need

		 * shrink zone->zone_start_pfn and zone->zone_spanned_pages.

		 * In this case, we find second smallest valid mem_section

		 * for shrinking zone.

		/*

		 * If the section is biggest section in the zone, it need

		 * shrink zone->spanned_pages.

		 * In this case, we find second biggest valid mem_section for

		 * shrinking zone.

 No need to lock the zones, they can't change. */

 Poison struct pages because they are now uninitialized again. */

 Select all remaining pages up to the next section boundary */

	/*

	 * Zone shrinking code cannot properly deal with ZONE_DEVICE. So

	 * we will not try to shrink the zones - which is okay as

	 * set_zone_contiguous() cannot deal with ZONE_DEVICE either way.

/**

 * __remove_pages() - remove sections of pages

 * @pfn: starting pageframe (must be aligned to start of a section)

 * @nr_pages: number of pages to remove (must be multiple of section size)

 * @altmap: alternative device page map or %NULL if default memmap is used

 *

 * Generic helper function to remove section mappings and sysfs entries

 * for the section of the memory we are removing. Caller needs to make

 * sure that pages are marked reserved and zones are adjust properly by

 * calling offline_pages().

 Select all remaining pages up to the next section boundary */

	/*

	 * Freeing the page with debug_pagealloc enabled will try to unmap it,

	 * so we should map it first. This is better than introducing a special

	 * case in page freeing fast path.

	/*

	 * Online the pages in MAX_ORDER - 1 aligned chunks. The callback might

	 * decide to not expose all pages to the buddy (e.g., expose them

	 * later). We account all pages as being online and belonging to this

	 * zone ("present").

	 * When using memmap_on_memory, the range might not be aligned to

	 * MAX_ORDER_NR_PAGES - 1, but pageblock aligned. __ffs() will detect

	 * this and the first chunk to online will be pageblock_nr_pages.

 mark all involved sections as online */

 check which state of node_states will be changed when online memory */

/*

 * Associate the pfn range with the given zone, initializing the memmaps

 * and resizing the pgdat/zone data to span the added pages. After this

 * call, all affected pages are PG_reserved.

 *

 * All aligned pageblocks are initialized to the specified migratetype

 * (usually MIGRATE_MOVABLE). Besides setting the migratetype, no related

 * zone stats (e.g., nr_isolate_pageblock) are touched.

	/*

	 * Subsection population requires care in pfn_to_online_page().

	 * Set the taint to enable the slow path detection of

	 * ZONE_DEVICE pages in an otherwise  ZONE_{NORMAL,MOVABLE}

	 * section.

	/*

	 * TODO now we have a visible range of pages which are not associated

	 * with their zone properly. Not nice but set_pfnblock_flags_mask

	 * expects the zone spans the pfn range. All the pages in the range

	 * are reserved so nobody should be touching them so we should be safe

		/*

		 * CMA pages (never on hotplugged memory) behave like

		 * ZONE_MOVABLE.

 CONFIG_CMA */

	/*

	 * We don't support modifying the config while the auto-movable online

	 * policy is already enabled. Just avoid the division by zero below.

	/*

	 * Calculate how many early kernel pages this group requires to

	 * satisfy the configured zone ratio.

 Walk all relevant zones and collect MOVABLE vs. KERNEL stats. */

 TODO: cache values */

	/*

	 * Kernel memory inside dynamic memory group allows for more MOVABLE

	 * memory within the same group. Remove the effect of all but the

	 * current group from the stats.

	/*

	 * Test if we could online the given number of pages to ZONE_MOVABLE

	 * and still stay in the configured ratio.

/*

 * Returns a default kernel memory zone for the given pfn range.

 * If no kernel zone covers this pfn range it will automatically go

 * to the ZONE_NORMAL.

/*

 * Determine to which zone to online memory dynamically based on user

 * configuration and system stats. We care about the following ratio:

 *

 *   MOVABLE : KERNEL

 *

 * Whereby MOVABLE is memory in ZONE_MOVABLE and KERNEL is memory in

 * one of the kernel zones. CMA pages inside one of the kernel zones really

 * behaves like ZONE_MOVABLE, so we treat them accordingly.

 *

 * We don't allow for hotplugged memory in a KERNEL zone to increase the

 * amount of MOVABLE memory we can have, so we end up with:

 *

 *   MOVABLE : KERNEL_EARLY

 *

 * Whereby KERNEL_EARLY is memory in one of the kernel zones, available sinze

 * boot. We base our calculation on KERNEL_EARLY internally, because:

 *

 * a) Hotplugged memory in one of the kernel zones can sometimes still get

 *    hotunplugged, especially when hot(un)plugging individual memory blocks.

 *    There is no coordination across memory devices, therefore "automatic"

 *    hotunplugging, as implemented in hypervisors, could result in zone

 *    imbalances.

 * b) Early/boot memory in one of the kernel zones can usually not get

 *    hotunplugged again (e.g., no firmware interface to unplug, fragmented

 *    with unmovable allocations). While there are corner cases where it might

 *    still work, it is barely relevant in practice.

 *

 * Exceptions are dynamic memory groups, which allow for more MOVABLE

 * memory within the same memory group -- because in that case, there is

 * coordination within the single memory device managed by a single driver.

 *

 * We rely on "present pages" instead of "managed pages", as the latter is

 * highly unreliable and dynamic in virtualized environments, and does not

 * consider boot time allocations. For example, memory ballooning adjusts the

 * managed pages when inflating/deflating the balloon, and balloon compaction

 * can even migrate inflated pages between zones.

 *

 * Using "present pages" is better but some things to keep in mind are:

 *

 * a) Some memblock allocations, such as for the crashkernel area, are

 *    effectively unused by the kernel, yet they account to "present pages".

 *    Fortunately, these allocations are comparatively small in relevant setups

 *    (e.g., fraction of system memory).

 * b) Some hotplugged memory blocks in virtualized environments, esecially

 *    hotplugged by virtio-mem, look like they are completely present, however,

 *    only parts of the memory block are actually currently usable.

 *    "present pages" is an upper limit that can get reached at runtime. As

 *    we base our calculations on KERNEL_EARLY, this is not an issue.

 If anything is !MOVABLE online the rest !MOVABLE. */

		/*

		 * Take a look at all online sections in the current unit.

		 * We can safely assume that all pages within a section belong

		 * to the same zone, because dynamic memory groups only deal

		 * with hotplugged memory.

 If anything is !MOVABLE online the rest !MOVABLE. */

	/*

	 * Online MOVABLE if we could *currently* online all remaining parts

	 * MOVABLE. We expect to (add+) online them immediately next, so if

	 * nobody interferes, all will be MOVABLE if possible.

 CONFIG_NUMA */

	/*

	 * We inherit the existing zone in a simple case where zones do not

	 * overlap in the given range

	/*

	 * If the range doesn't belong to any zone or two zones overlap in the

	 * given range then we use movable zone only if movable_node is

	 * enabled because we always online to a kernel zone by default.

/*

 * This function should only be called by memory_block_{online,offline},

 * and {online,offline}_pages.

	/*

	 * We only support onlining/offlining/adding/removing of complete

	 * memory blocks; therefore, either all is either early or hotplugged.

	/*

	 * It might be that the vmemmap_pages fully span sections. If that is

	 * the case, mark those sections online here as otherwise they will be

	 * left offline.

	/*

	 * It might be that the vmemmap_pages fully span sections. If that is

	 * the case, mark those sections offline here as otherwise they will be

	 * left online.

        /*

	 * The pages associated with this vmemmap have been offlined, so

	 * we can reset its state here.

	/*

	 * {on,off}lining is constrained to full memory sections (or more

	 * precisely to memory blocks from the user space POV).

	 * memmap_on_memory is an exception because it reserves initial part

	 * of the physical memory space for vmemmaps. That space is pageblock

	 * aligned.

 associate pfn range with the zone */

	/*

	 * Fixup the number of isolated pageblocks before marking the sections

	 * onlining, such that undo_isolate_page_range() works correctly.

	/*

	 * If this zone is not populated, then it is not in zonelist.

	 * This means the page allocator ignores this zone.

	 * So, zonelist must be updated after online.

 Basic onlining is complete, allow allocation of onlined pages. */

	/*

	 * Freshly onlined pages aren't shuffled (e.g., all pages are placed to

	 * the tail of the freelist when undoing isolation). Shuffle the whole

	 * zone to make sure the just onlined pages are properly distributed

	 * across the whole freelist - to create an initial shuffle.

 reinitialise watermarks and update pcp limits */

 we are OK calling __meminit stuff here - we have CONFIG_MEMORY_HOTPLUG */

		/*

		 * Reset the nr_zones, order and highest_zoneidx before reuse.

		 * Note that kswapd will init kswapd_highest_zoneidx properly

		 * when it starts in the near future.

 we can use NODE_DATA(nid) from here */

 init node's zones as empty zones, we don't have any present pages.*/

	/*

	 * The node we allocated has no zone fallback lists. For avoiding

	 * to access not-initialized zonelist, build here.

	/*

	 * When memory is hot-added, all the memory is in offline state. So

	 * clear all zones' present_pages because they will be updated in

	 * online_pages() and offline_pages().

/*

 * __try_online_node - online a node if offlined

 * @nid: the node ID

 * @set_node_online: Whether we want to online the node

 * called by cpu_up() to online a node without onlined memory.

 *

 * Returns:

 * 1 -> a new node has been allocated

 * 0 -> the node is already online

 * -ENOMEM -> the node could not be allocated

/*

 * Users of this function always want to online/register the node

 memory range must be block size aligned */

	/*

	 * Besides having arch support and the feature enabled at runtime, we

	 * need a few more assumptions to hold true:

	 *

	 * a) We span a single memory block: memory onlining/offlinin;g happens

	 *    in memory block granularity. We don't want the vmemmap of online

	 *    memory blocks to reside on offline memory blocks. In the future,

	 *    we might want to support variable-sized memory blocks to make the

	 *    feature more versatile.

	 *

	 * b) The vmemmap pages span complete PMDs: We don't want vmemmap code

	 *    to populate memory from the altmap for unrelated parts (i.e.,

	 *    other memory blocks)

	 *

	 * c) The vmemmap pages (and thereby the pages that will be exposed to

	 *    the buddy) have to cover full pageblocks: memory onlining/offlining

	 *    code requires applicable ranges to be page-aligned, for example, to

	 *    set the migratetypes properly.

	 *

	 * TODO: Although we have a check here to make sure that vmemmap pages

	 *       fully populate a PMD, it is not the right place to check for

	 *       this. A much better solution involves improving vmemmap code

	 *       to fallback to base pages when trying to populate vmemmap using

	 *       altmap as an alternative source of memory, and we do not exactly

	 *       populate a single PMD.

/*

 * NOTE: The caller must call lock_device_hotplug() to serialize hotplug

 * and online/offline operations (triggered e.g. by sysfs).

 *

 * we are OK calling __meminit stuff here - we have CONFIG_MEMORY_HOTPLUG

	/*

	 * Self hosted memmap array

 call arch's memory hotadd */

 create memory block devices after memory was added */

		/* If sysfs file of new node can't be created, cpu on the node

		 * can't be hot-added. There is no rollback way now.

		 * So, check by BUG_ON() to catch it reluctantly..

		 * We online node here. We can't roll back from here.

 link memory sections under this node.*/

 create new memmap entry */

 device_online() will take the lock when calling online_pages() */

	/*

	 * In case we're allowed to merge the resource, flag it and trigger

	 * merging now that adding succeeded.

 online pages if requested */

 rollback pgdat allocation and others */

 requires device_hotplug_lock, see add_memory_resource() */

/*

 * Add special, driver-managed memory to the system as system RAM. Such

 * memory is not exposed via the raw firmware-provided memmap as system

 * RAM, instead, it is detected and added by a driver - during cold boot,

 * after a reboot, and after kexec.

 *

 * Reasons why this memory should not be used for the initial memmap of a

 * kexec kernel or for placing kexec images:

 * - The booting kernel is in charge of determining how this memory will be

 *   used (e.g., use persistent memory as system RAM)

 * - Coordination with a hypervisor is required before this memory

 *   can be used (e.g., inaccessible parts).

 *

 * For this memory, no entries in /sys/firmware/memmap ("raw firmware-provided

 * memory map") are created. Also, the created memory resource is flagged

 * with IORESOURCE_SYSRAM_DRIVER_MANAGED, so in-kernel users can special-case

 * this memory as well (esp., not place kexec images onto it).

 *

 * The resource_name (visible via /proc/iomem) has to have the format

 * "System RAM ($DRIVER)".

/*

 * Platforms should define arch_get_mappable_range() that provides

 * maximum possible addressable physical memory range for which the

 * linear mapping could be created. The platform returned address

 * range must adhere to these following semantics.

 *

 * - range.start <= range.end

 * - Range includes both end points [range.start..range.end]

 *

 * There is also a fallback definition provided here, allowing the

 * entire possible physical address range in case any platform does

 * not define arch_get_mappable_range().

/*

 * Confirm all pages in a range [start, end) belong to the same zone (skipping

 * memory holes). When true, return the zone.

 Make sure the memory section is present first */

 Check if we got outside of the zone */

/*

 * Scan pfn range [start,end) to find movable/migratable pages (LRU pages,

 * non-lru movable pages and hugepages). Will skip over most unmovable

 * pages (esp., pages that can be skipped when offlining), but bail out on

 * definitely unmovable pages.

 *

 * Returns:

 *	0 in case a movable page is found and movable_pfn was updated.

 *	-ENOENT in case no movable page was found.

 *	-EBUSY in case a definitely unmovable page was found.

		/*

		 * PageOffline() pages that are not marked __PageMovable() and

		 * have a reference count > 0 (after MEM_GOING_OFFLINE) are

		 * definitely unmovable. If their reference count would be 0,

		 * they could at least be skipped when offlining memory.

		/*

		 * This test is racy as we hold no reference or lock.  The

		 * hugetlb page could have been free'ed and head is no longer

		 * a hugetlb page before the following check.  In such unlikely

		 * cases false positives and negatives are possible.  Calling

		 * code must deal with these scenarios.

		/*

		 * HWPoison pages have elevated reference counts so the migration would

		 * fail on them. It also doesn't make any sense to migrate them in the

		 * first place. Still try to unmap such a page in case it is still mapped

		 * (e.g. current hwpoison implementation doesn't unmap KSM pages but keep

		 * the unmap as the catch all safety net).

		/*

		 * We can skip free pages. And we can deal with pages on

		 * LRU and non-lru movable pages.

 Success */

		/*

		 * We have checked that migration range is on a single zone so

		 * we can use the nid of the first page to all the others.

		/*

		 * try to allocate from a different node but reuse this node

		 * if there are no other online nodes to be used (e.g. we are

		 * offlining a part of the only existing node)

 check which state of node_states will be changed when offline memory */

	/*

	 * Check whether node_states[N_NORMAL_MEMORY] will be changed.

	 * If the memory to be offline is within the range

	 * [0..ZONE_NORMAL], and it is the last present memory there,

	 * the zones in that range will become empty after the offlining,

	 * thus we can determine that we need to clear the node from

	 * node_states[N_NORMAL_MEMORY].

	/*

	 * We have accounted the pages from [0..ZONE_NORMAL); ZONE_HIGHMEM

	 * does not apply as we don't support 32bit.

	 * Here we count the possible pages from ZONE_MOVABLE.

	 * If after having accounted all the pages, we see that the nr_pages

	 * to be offlined is over or equal to the accounted pages,

	 * we know that the node will become empty, and so, we can clear

	 * it for N_MEMORY as well.

	/*

	 * {on,off}lining is constrained to full memory sections (or more

	 * precisely to memory blocks from the user space POV).

	 * memmap_on_memory is an exception because it reserves initial part

	 * of the physical memory space for vmemmaps. That space is pageblock

	 * aligned.

	/*

	 * Don't allow to offline memory blocks that contain holes.

	 * Consequently, memory blocks with holes can never get onlined

	 * via the hotplug path - online_pages() - as hotplugged memory has

	 * no holes. This way, we e.g., don't have to worry about marking

	 * memory holes PG_reserved, don't need pfn_valid() checks, and can

	 * avoid using walk_system_ram_range() later.

	/* This makes hotplug much easier...and readable.

	/*

	 * Disable pcplists so that page isolation cannot race with freeing

	 * in a way that pages from isolated pageblock are left on pcplists.

 set above range as isolated */

				/*

				 * TODO: fatal migration failures should bail

				 * out

		/*

		 * Dissolve free hugepages in the memory block before doing

		 * offlining actually in order to make hugetlbfs's object

		 * counting consistent.

 Mark all sections offline and remove free pages from the buddy. */

	/*

	 * The memory sections are marked offline, and the pageblock flags

	 * effectively stale; nobody should be touching them. Fixup the number

	 * of isolated pageblocks, memory onlining will properly revert this.

 removal success */

 reinitialise watermarks and update pcp limits */

 pushback to free area */

	/*

	 * If not set, continue with the next block.

			/*

			 * the cpu on this node isn't removed, and we can't

			 * offline this node.

	/*

	 * If a memory block belongs to multiple nodes, the stored nid is not

	 * reliable. However, such blocks are always online (e.g., cannot get

	 * offlined) and, therefore, are still spanned by the node.

/**

 * try_offline_node

 * @nid: the node ID

 *

 * Offline a node if all memory sections and cpus of the node are removed.

 *

 * NOTE: The caller must call lock_device_hotplug() to serialize hotplug

 * and online/offline operations before this call.

	/*

	 * If the node still spans pages (especially ZONE_DEVICE), don't

	 * offline it. A node spans memory after move_pfn_range_to_zone(),

	 * e.g., after the memory block was onlined.

	/*

	 * Especially offline memory blocks might not be spanned by the

	 * node. They will get spanned by the node once they get onlined.

	 * However, they link to the node in sysfs and can get onlined later.

	/*

	 * all memory/cpu of this node are removed, we can offline this

	 * node now.

	/*

	 * All memory blocks must be offlined before removing memory.  Check

	 * whether all memory blocks in question are offline and return error

	 * if this is not the case.

	 *

	 * While at it, determine the nid. Note that if we'd have mixed nodes,

	 * we'd only try to offline the last determined one -- which is good

	 * enough for the cases we care about.

	/*

	 * We only support removing memory added with MHP_MEMMAP_ON_MEMORY in

	 * the same granularity it was added - a single memory block.

			/*

			 * Let remove_pmd_table->free_hugepage_table do the

			 * right thing if we used vmem_altmap when hot-adding

			 * the range.

 remove memmap entry */

	/*

	 * Memory block device removal under the device_hotplug_lock is

	 * a barrier against racing online attempts.

/**

 * __remove_memory - Remove memory if every memory block is offline

 * @start: physical address of the region to remove

 * @size: size of the region to remove

 *

 * NOTE: The caller must call lock_device_hotplug() to serialize hotplug

 * and online/offline operations before this call, as required by

 * try_offline_node().

	/*

	 * trigger BUG() if some memory is not offlined prior to calling this

	 * function

/*

 * Remove memory if every memory block is offline, otherwise return -EBUSY is

 * some memory is not offline

	/*

	 * Sense the online_type via the zone of the memory block. Offlining

	 * with multiple zones within one memory block will be rejected

	 * by offlining code ... so we don't care about that.

	/*

	 * Default is MMOP_OFFLINE - change it only if offlining succeeded,

	 * so try_reonline_memory_block() can do the right thing.

 Ignore if already offline. */

 Continue processing all remaining memory blocks. */

/*

 * Try to offline and remove memory. Might take a long time to finish in case

 * memory is still in use. Primarily useful for memory devices that logically

 * unplugged all memory (so it's no longer in use) and want to offline + remove

 * that memory.

	/*

	 * We'll remember the old online type of each memory block, so we can

	 * try to revert whatever we did when offlining one memory block fails

	 * after offlining some others succeeded.

	/*

	 * Initialize all states to MMOP_OFFLINE, so when we abort processing in

	 * try_offline_memory_block(), we'll skip all unprocessed blocks in

	 * try_reonline_memory_block().

	/*

	 * In case we succeeded to offline all memory, remove it.

	 * This cannot fail as it cannot get onlined in the meantime.

	/*

	 * Rollback what we did. While memory onlining might theoretically fail

	 * (nacked by a notifier), it barely ever happens.

 CONFIG_MEMORY_HOTREMOVE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * zbud.c

 *

 * Copyright (C) 2013, Seth Jennings, IBM

 *

 * Concepts based on zcache internal zbud allocator by Dan Magenheimer.

 *

 * zbud is an special purpose allocator for storing compressed pages.  Contrary

 * to what its name may suggest, zbud is not a buddy allocator, but rather an

 * allocator that "buddies" two compressed pages together in a single memory

 * page.

 *

 * While this design limits storage density, it has simple and deterministic

 * reclaim properties that make it preferable to a higher density approach when

 * reclaim will be used.

 *

 * zbud works by storing compressed pages, or "zpages", together in pairs in a

 * single memory page called a "zbud page".  The first buddy is "left

 * justified" at the beginning of the zbud page, and the last buddy is "right

 * justified" at the end of the zbud page.  The benefit is that if either

 * buddy is freed, the freed buddy space, coalesced with whatever slack space

 * that existed between the buddies, results in the largest possible free region

 * within the zbud page.

 *

 * zbud also provides an attractive lower bound on density. The ratio of zpages

 * to zbud pages can not be less than 1.  This ensures that zbud can never "do

 * harm" by using more pages to store zpages than the uncompressed zpages would

 * have used on their own.

 *

 * zbud pages are divided into "chunks".  The size of the chunks is fixed at

 * compile time and determined by NCHUNKS_ORDER below.  Dividing zbud pages

 * into chunks allows organizing unbuddied zbud pages into a manageable number

 * of unbuddied lists according to the number of free chunks available in the

 * zbud page.

 *

 * The zbud API differs from that of conventional allocators in that the

 * allocation function, zbud_alloc(), returns an opaque handle to the user,

 * not a dereferenceable pointer.  The user must map the handle using

 * zbud_map() in order to get a usable pointer by which to access the

 * allocation data and unmap the handle with zbud_unmap() when operations

 * on the allocation data are complete.

/*****************

 * Structures

/*

 * NCHUNKS_ORDER determines the internal allocation granularity, effectively

 * adjusting internal fragmentation.  It also determines the number of

 * freelists maintained in each pool. NCHUNKS_ORDER of 6 means that the

 * allocation granularity will be in chunks of size PAGE_SIZE/64. As one chunk

 * in allocated page is occupied by zbud header, NCHUNKS will be calculated to

 * 63 which shows the max number of free chunks in zbud page, also there will be

 * 63 freelists per pool.

/**

 * struct zbud_pool - stores metadata for each zbud pool

 * @lock:	protects all pool fields and first|last_chunk fields of any

 *		zbud page in the pool

 * @unbuddied:	array of lists tracking zbud pages that only contain one buddy;

 *		the lists each zbud page is added to depends on the size of

 *		its free region.

 * @buddied:	list tracking the zbud pages that contain two buddies;

 *		these zbud pages are full

 * @lru:	list tracking the zbud pages in LRU order by most recently

 *		added buddy.

 * @pages_nr:	number of zbud pages in the pool.

 * @ops:	pointer to a structure of user defined operations specified at

 *		pool creation time.

 * @zpool:	zpool driver

 * @zpool_ops:	zpool operations structure with an evict callback

 *

 * This structure is allocated at pool creation time and maintains metadata

 * pertaining to a particular zbud pool.

		/*

		 * Reuse unbuddied[0] as buddied on the ground that

		 * unbuddied[0] is unused.

/*

 * struct zbud_header - zbud page metadata occupying the first chunk of each

 *			zbud page.

 * @buddy:	links the zbud page into the unbuddied/buddied lists in the pool

 * @lru:	links the zbud page into the lru list in the pool

 * @first_chunks:	the size of the first buddy in chunks, 0 if free

 * @last_chunks:	the size of the last buddy in chunks, 0 if free

/*****************

 * Helpers

 Just to make the code easier to read */

 Converts an allocation size in bytes to size in zbud chunks */

 Initializes the zbud header of a newly allocated zbud page */

 Resets the struct page fields and frees the page */

/*

 * Encodes the handle of a particular buddy within a zbud page

 * Pool lock should be held as this function accesses first|last_chunks

	/*

	 * For now, the encoded handle is actually just the pointer to the data

	 * but this might not always be the case.  A little information hiding.

	 * Add CHUNK_SIZE to the handle if it is the first allocation to jump

	 * over the zbud header in the first chunk.

 skip over zbud header */

 bud == LAST */

 Returns the zbud page where a given handle is stored */

 Returns the number of free chunks in a zbud page */

	/*

	 * Rather than branch for different situations, just use the fact that

	 * free buddies have a length of zero to simplify everything.

/*****************

 * API Functions

/**

 * zbud_create_pool() - create a new zbud pool

 * @gfp:	gfp flags when allocating the zbud pool structure

 * @ops:	user-defined operations for the zbud pool

 *

 * Return: pointer to the new zbud pool or NULL if the metadata allocation

 * failed.

/**

 * zbud_destroy_pool() - destroys an existing zbud pool

 * @pool:	the zbud pool to be destroyed

 *

 * The pool should be emptied before this function is called.

/**

 * zbud_alloc() - allocates a region of a given size

 * @pool:	zbud pool from which to allocate

 * @size:	size in bytes of the desired allocation

 * @gfp:	gfp flags used if the pool needs to grow

 * @handle:	handle of the new allocation

 *

 * This function will attempt to find a free region in the pool large enough to

 * satisfy the allocation request.  A search of the unbuddied lists is

 * performed first. If no suitable free region is found, then a new page is

 * allocated and added to the pool to satisfy the request.

 *

 * gfp should not set __GFP_HIGHMEM as highmem pages cannot be used

 * as zbud pool pages.

 *

 * Return: 0 if success and handle is set, otherwise -EINVAL if the size or

 * gfp arguments are invalid or -ENOMEM if the pool was unable to allocate

 * a new page.

 First, try to find an unbuddied zbud page. */

 Couldn't find unbuddied zbud page, create new one */

 Add to unbuddied list */

 Add to buddied list */

 Add/move zbud page to beginning of LRU */

/**

 * zbud_free() - frees the allocation associated with the given handle

 * @pool:	pool in which the allocation resided

 * @handle:	handle associated with the allocation returned by zbud_alloc()

 *

 * In the case that the zbud page in which the allocation resides is under

 * reclaim, as indicated by the PG_reclaim flag being set, this function

 * only sets the first|last_chunks to 0.  The page is actually freed

 * once both buddies are evicted (see zbud_reclaim_page() below).

 If first buddy, handle will be page aligned */

 zbud page is under reclaim, reclaim will free */

 Remove from existing buddy list */

 zbud page is empty, free */

 Add to unbuddied list */

/**

 * zbud_reclaim_page() - evicts allocations from a pool page and frees it

 * @pool:	pool from which a page will attempt to be evicted

 * @retries:	number of pages on the LRU list for which eviction will

 *		be attempted before failing

 *

 * zbud reclaim is different from normal system reclaim in that the reclaim is

 * done from the bottom, up.  This is because only the bottom layer, zbud, has

 * information on how the allocations are organized within each zbud page. This

 * has the potential to create interesting locking situations between zbud and

 * the user, however.

 *

 * To avoid these, this is how zbud_reclaim_page() should be called:

 *

 * The user detects a page should be reclaimed and calls zbud_reclaim_page().

 * zbud_reclaim_page() will remove a zbud page from the pool LRU list and call

 * the user-defined eviction handler with the pool and handle as arguments.

 *

 * If the handle can not be evicted, the eviction handler should return

 * non-zero. zbud_reclaim_page() will add the zbud page back to the

 * appropriate list and try the next zbud page on the LRU up to

 * a user defined number of retries.

 *

 * If the handle is successfully evicted, the eviction handler should

 * return 0 _and_ should have called zbud_free() on the handle. zbud_free()

 * contains logic to delay freeing the page if the page is under reclaim,

 * as indicated by the setting of the PG_reclaim flag on the underlying page.

 *

 * If all buddies in the zbud page are successfully evicted, then the

 * zbud page can be freed.

 *

 * Returns: 0 if page is successfully freed, otherwise -EINVAL if there are

 * no pages to evict or an eviction handler is not registered, -EAGAIN if

 * the retry limit was hit.

 Protect zbud page against free */

		/*

		 * We need encode the handles before unlocking, since we can

		 * race with free that will set (first|last)_chunks to 0

 Issue the eviction callback(s) */

			/*

			 * Both buddies are now free, free the zbud page and

			 * return success.

 add to unbuddied list */

 add to buddied list */

 add to beginning of LRU */

/**

 * zbud_map() - maps the allocation associated with the given handle

 * @pool:	pool in which the allocation resides

 * @handle:	handle associated with the allocation to be mapped

 *

 * While trivial for zbud, the mapping functions for others allocators

 * implementing this allocation API could have more complex information encoded

 * in the handle and could create temporary mappings to make the data

 * accessible to the user.

 *

 * Returns: a pointer to the mapped allocation

/**

 * zbud_unmap() - maps the allocation associated with the given handle

 * @pool:	pool in which the allocation resides

 * @handle:	handle associated with the allocation to be unmapped

/**

 * zbud_get_pool_size() - gets the zbud pool size in pages

 * @pool:	pool whose size is being queried

 *

 * Returns: size in pages of the given pool.  The pool lock need not be

 * taken to access pages_nr.

/*****************

 * zpool

 Make sure the zbud header will fit in one chunk */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.

 * Authors: David Chinner and Glauber Costa

 *

 * Generic LRU infrastructure

	/*

	 * Either lock or RCU protects the array of per cgroup lists

	 * from relocation (see memcg_update_list_lru_node).

 CONFIG_MEMCG_KMEM */

 Set shrinker bit if the first element was added */

		/*

		 * decrement nr_to_walk first so that we don't livelock if we

		 * get stuck on large numbers of LRU_RETRY items

			/*

			 * If the lru lock has been dropped, our list

			 * traversal is now invalid and so we have to

			 * restart from scratch.

			/*

			 * The lru lock has been dropped, our list traversal is

			 * now invalid and so we have to restart from scratch.

	/*

	 * This is called when shrinker has already been unregistered,

	 * and nobody can use it. So, there is no need to use kvfree_rcu().

	/* do not bother shrinking the array back to the old size, because we

	/*

	 * Since list_lru_{add,del} may be called under an IRQ-safe lock,

	 * we have to use IRQ-safe primitives here to avoid deadlock.

 CONFIG_MEMCG_KMEM */

 Do this so a list_lru_destroy() doesn't crash: */

 Already destroyed or not yet initialized? */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Frontswap frontend

 *

 * This code provides the generic "frontend" layer to call a matching

 * "backend" driver implementation of frontswap.  See

 * Documentation/vm/frontswap.rst for more information.

 *

 * Copyright (C) 2009-2012 Oracle Corp.  All rights reserved.

 * Author: Dan Magenheimer

/*

 * frontswap_ops are added by frontswap_register_ops, and provide the

 * frontswap "backend" implementation functions.  Multiple implementations

 * may be registered, but implementations can never deregister.  This

 * is a simple singly-linked list of all registered implementations.

/*

 * If enabled, frontswap_store will return failure even on success.  As

 * a result, the swap subsystem will always write the page to swap, in

 * effect converting frontswap into a writethrough cache.  In this mode,

 * there is no direct reduction in swap writes, but a frontswap backend

 * can unilaterally "reclaim" any pages in use with no data loss, thus

 * providing increases control over maximum memory usage due to frontswap.

/*

 * If enabled, the underlying tmem implementation is capable of doing

 * exclusive gets, so frontswap_load, on a successful tmem_get must

 * mark the page as no longer in frontswap AND mark it dirty.

/*

 * Counters available via /sys/kernel/debug/frontswap (if debugfs is

 * properly configured).  These are for information only so are not protected

 * against increment races.

/*

 * Due to the asynchronous nature of the backends loading potentially

 * _after_ the swap system has been activated, we have chokepoints

 * on all frontswap functions to not call the backend until the backend

 * has registered.

 *

 * This would not guards us against the user deciding to call swapoff right as

 * we are calling the backend to initialize (so swapon is in action).

 * Fortunately for us, the swapon_mutex has been taken by the callee so we are

 * OK. The other scenario where calls to frontswap_store (called via

 * swap_writepage) is racing with frontswap_invalidate_area (called via

 * swapoff) is again guarded by the swap subsystem.

 *

 * While no backend is registered all calls to frontswap_[store|load|

 * invalidate_area|invalidate_page] are ignored or fail.

 *

 * The time between the backend being registered and the swap file system

 * calling the backend (via the frontswap_* functions) is indeterminate as

 * frontswap_ops is not atomic_t (or a value guarded by a spinlock).

 * That is OK as we are comfortable missing some of these calls to the newly

 * registered backend.

 *

 * Obviously the opposite (unloading the backend) must be done after all

 * the frontswap_[store|load|invalidate_area|invalidate_page] start

 * ignoring or failing the requests.  However, there is currently no way

 * to unload a backend once it is registered.

/*

 * Register operations for frontswap

 the new ops needs to know the currently active swap devices */

	/*

	 * Setting frontswap_ops must happen after the ops->init() calls

	 * above; cmpxchg implies smp_mb() which will ensure the init is

	 * complete at this point.

	/*

	 * On the very unlikely chance that a swap device was added or

	 * removed between setting the "a" list bits and the ops init

	 * calls, we re-check and do init or invalidate for any changed

	 * bits.

/*

 * Enable/disable frontswap writethrough (see above).

/*

 * Enable/disable frontswap exclusive gets (see above).

/*

 * Called when a swap device is swapon'd.

	/*

	 * p->frontswap is a bitmap that we MUST have to figure out which page

	 * has gone in frontswap. Without it there is no point of continuing.

	/*

	 * Irregardless of whether the frontswap backend has been loaded

	 * before this function or it will be later, we _MUST_ have the

	 * p->frontswap set to something valid to work properly.

/*

 * "Store" data from a page to frontswap and associate it with the page's

 * swaptype and offset.  Page must be locked and in the swap cache.

 * If frontswap already contains a page with matching swaptype and

 * offset, the frontswap implementation may either overwrite the data and

 * return success or invalidate the page from frontswap and return failure.

	/*

	 * If a dup, we must remove the old page first; we can't leave the

	 * old page no matter if the store of the new page succeeds or fails,

	 * and we can't rely on the new page replacing the old page as we may

	 * not store to the same implementation that contains the old page.

 Try to store in each implementation, until one succeeds. */

 successful store */

 report failure so swap also writes to swap device */

/*

 * "Get" data from frontswap associated with swaptype and offset that were

 * specified when the data was put to frontswap and use it to fill the

 * specified page with data. Page must be locked and in the swap cache.

 Try loading from each implementation, until one succeeds. */

 successful load */

/*

 * Invalidate any data from frontswap associated with the specified swaptype

 * and offset so that a subsequent "get" will fail.

/*

 * Invalidate all data from frontswap associated with all offsets for the

 * specified swaptype.

 unuse all */

 ensure there is enough RAM to fetch pages from frontswap */

/*

 * Used to check if it's necessary and feasible to unuse pages.

 * Return 1 when nothing to do, 0 when need to shrink pages,

 * error code when there is an error.

 Nothing to do */

/*

 * Frontswap, like a true swap device, may unnecessarily retain pages

 * under certain circumstances; "shrink" frontswap is essentially a

 * "partial swapoff" and works by calling try_to_unuse to attempt to

 * unuse enough frontswap pages to attempt to -- subject to memory

 * constraints -- reduce the number of pages in frontswap to the

 * number given in the parameter target_pages.

	/*

	 * we don't want to hold swap_lock while doing a very

	 * lengthy try_to_unuse, but swap_list may change

	 * so restart scan from swap_active_head each time

/*

 * Count and return the number of frontswap pages across all

 * swap devices.  This is exported so that backend drivers can

 * determine current usage without reading debugfs.

 SPDX-License-Identifier: GPL-2.0

/*

 * Workingset detection

 *

 * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner

/*

 *		Double CLOCK lists

 *

 * Per node, two clock lists are maintained for file pages: the

 * inactive and the active list.  Freshly faulted pages start out at

 * the head of the inactive list and page reclaim scans pages from the

 * tail.  Pages that are accessed multiple times on the inactive list

 * are promoted to the active list, to protect them from reclaim,

 * whereas active pages are demoted to the inactive list when the

 * active list grows too big.

 *

 *   fault ------------------------+

 *                                 |

 *              +--------------+   |            +-------------+

 *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+

 *              +--------------+                +-------------+    |

 *                     |                                           |

 *                     +-------------- promotion ------------------+

 *

 *

 *		Access frequency and refault distance

 *

 * A workload is thrashing when its pages are frequently used but they

 * are evicted from the inactive list every time before another access

 * would have promoted them to the active list.

 *

 * In cases where the average access distance between thrashing pages

 * is bigger than the size of memory there is nothing that can be

 * done - the thrashing set could never fit into memory under any

 * circumstance.

 *

 * However, the average access distance could be bigger than the

 * inactive list, yet smaller than the size of memory.  In this case,

 * the set could fit into memory if it weren't for the currently

 * active pages - which may be used more, hopefully less frequently:

 *

 *      +-memory available to cache-+

 *      |                           |

 *      +-inactive------+-active----+

 *  a b | c d e f g h i | J K L M N |

 *      +---------------+-----------+

 *

 * It is prohibitively expensive to accurately track access frequency

 * of pages.  But a reasonable approximation can be made to measure

 * thrashing on the inactive list, after which refaulting pages can be

 * activated optimistically to compete with the existing active pages.

 *

 * Approximating inactive page access frequency - Observations:

 *

 * 1. When a page is accessed for the first time, it is added to the

 *    head of the inactive list, slides every existing inactive page

 *    towards the tail by one slot, and pushes the current tail page

 *    out of memory.

 *

 * 2. When a page is accessed for the second time, it is promoted to

 *    the active list, shrinking the inactive list by one slot.  This

 *    also slides all inactive pages that were faulted into the cache

 *    more recently than the activated page towards the tail of the

 *    inactive list.

 *

 * Thus:

 *

 * 1. The sum of evictions and activations between any two points in

 *    time indicate the minimum number of inactive pages accessed in

 *    between.

 *

 * 2. Moving one inactive page N page slots towards the tail of the

 *    list requires at least N inactive page accesses.

 *

 * Combining these:

 *

 * 1. When a page is finally evicted from memory, the number of

 *    inactive pages accessed while the page was in cache is at least

 *    the number of page slots on the inactive list.

 *

 * 2. In addition, measuring the sum of evictions and activations (E)

 *    at the time of a page's eviction, and comparing it to another

 *    reading (R) at the time the page faults back into memory tells

 *    the minimum number of accesses while the page was not cached.

 *    This is called the refault distance.

 *

 * Because the first access of the page was the fault and the second

 * access the refault, we combine the in-cache distance with the

 * out-of-cache distance to get the complete minimum access distance

 * of this page:

 *

 *      NR_inactive + (R - E)

 *

 * And knowing the minimum access distance of a page, we can easily

 * tell if the page would be able to stay in cache assuming all page

 * slots in the cache were available:

 *

 *   NR_inactive + (R - E) <= NR_inactive + NR_active

 *

 * which can be further simplified to

 *

 *   (R - E) <= NR_active

 *

 * Put into words, the refault distance (out-of-cache) can be seen as

 * a deficit in inactive list space (in-cache).  If the inactive list

 * had (R - E) more page slots, the page would not have been evicted

 * in between accesses, but activated instead.  And on a full system,

 * the only thing eating into inactive list space is active pages.

 *

 *

 *		Refaulting inactive pages

 *

 * All that is known about the active list is that the pages have been

 * accessed more than once in the past.  This means that at any given

 * time there is actually a good chance that pages on the active list

 * are no longer in active use.

 *

 * So when a refault distance of (R - E) is observed and there are at

 * least (R - E) active pages, the refaulting page is activated

 * optimistically in the hope that (R - E) active pages are actually

 * used less frequently than the refaulting page - or even not used at

 * all anymore.

 *

 * That means if inactive cache is refaulting with a suitable refault

 * distance, we assume the cache workingset is transitioning and put

 * pressure on the current active list.

 *

 * If this is wrong and demotion kicks in, the pages which are truly

 * used more frequently will be reactivated while the less frequently

 * used once will be evicted from memory.

 *

 * But if this is right, the stale pages will be pushed out of memory

 * and the used pages get to stay in cache.

 *

 *		Refaulting active pages

 *

 * If on the other hand the refaulting pages have recently been

 * deactivated, it means that the active list is no longer protecting

 * actively used cache from reclaim. The cache is NOT transitioning to

 * a different workingset; the existing workingset is thrashing in the

 * space allocated to the page cache.

 *

 *

 *		Implementation

 *

 * For each node's LRU lists, a counter for inactive evictions and

 * activations is maintained (node->nonresident_age).

 *

 * On eviction, a snapshot of this counter (along with some bits to

 * identify the node) is stored in the now empty page cache

 * slot of the evicted page.  This is called a shadow entry.

 *

 * On cache misses for which there are shadow entries, an eligible

 * refault distance will immediately activate the refaulting page.

/*

 * Eviction timestamps need to be able to cover the full range of

 * actionable refaults. However, bits are tight in the xarray

 * entry, and after storing the identifier for the lruvec there might

 * not be enough left to represent every single actionable refault. In

 * that case, we have to sacrifice granularity for distance, and group

 * evictions into coarser buckets by shaving off lower timestamp bits.

/**

 * workingset_age_nonresident - age non-resident entries as LRU ages

 * @lruvec: the lruvec that was aged

 * @nr_pages: the number of pages to count

 *

 * As in-memory pages are aged, non-resident pages need to be aged as

 * well, in order for the refault distances later on to be comparable

 * to the in-memory dimensions. This function allows reclaim and LRU

 * operations to drive the non-resident aging along in parallel.

	/*

	 * Reclaiming a cgroup means reclaiming all its children in a

	 * round-robin fashion. That means that each cgroup has an LRU

	 * order that is composed of the LRU orders of its child

	 * cgroups; and every page has an LRU position not just in the

	 * cgroup that owns it, but in all of that group's ancestors.

	 *

	 * So when the physical inactive list of a leaf cgroup ages,

	 * the virtual inactive lists of all its parents, including

	 * the root cgroup's, age as well.

/**

 * workingset_eviction - note the eviction of a page from memory

 * @target_memcg: the cgroup that is causing the reclaim

 * @page: the page being evicted

 *

 * Return: a shadow entry to be stored in @page->mapping->i_pages in place

 * of the evicted @page so that a later refault can be detected.

 Page is fully exclusive and pins page's memory cgroup pointer */

 XXX: target_memcg can be NULL, go through lruvec */

/**

 * workingset_refault - Evaluate the refault of a previously evicted folio.

 * @folio: The freshly allocated replacement folio.

 * @shadow: Shadow entry of the evicted folio.

 *

 * Calculates and evaluates the refault distance of the previously

 * evicted folio in the context of the node and the memcg whose memory

 * pressure caused the eviction.

	/*

	 * Look up the memcg associated with the stored ID. It might

	 * have been deleted since the folio's eviction.

	 *

	 * Note that in rare events the ID could have been recycled

	 * for a new cgroup that refaults a shared folio. This is

	 * impossible to tell from the available data. However, this

	 * should be a rare and limited disturbance, and activations

	 * are always speculative anyway. Ultimately, it's the aging

	 * algorithm's job to shake out the minimum access frequency

	 * for the active cache.

	 *

	 * XXX: On !CONFIG_MEMCG, this will always return NULL; it

	 * would be better if the root_mem_cgroup existed in all

	 * configurations instead.

	/*

	 * Calculate the refault distance

	 *

	 * The unsigned subtraction here gives an accurate distance

	 * across nonresident_age overflows in most cases. There is a

	 * special case: usually, shadow entries have a short lifetime

	 * and are either refaulted or reclaimed along with the inode

	 * before they get too old.  But it is not impossible for the

	 * nonresident_age to lap a shadow entry in the field, which

	 * can then result in a false small refault distance, leading

	 * to a false activation should this old entry actually

	 * refault again.  However, earlier kernels used to deactivate

	 * unconditionally with *every* reclaim invocation for the

	 * longest time, so the occasional inappropriate activation

	 * leading to pressure on the active list is not a problem.

	/*

	 * The activation decision for this folio is made at the level

	 * where the eviction occurred, as that is where the LRU order

	 * during folio reclaim is being determined.

	 *

	 * However, the cgroup that will own the folio is the one that

	 * is actually experiencing the refault event.

	/*

	 * Compare the distance to the existing workingset size. We

	 * don't activate pages that couldn't stay resident even if

	 * all the memory was available to the workingset. Whether

	 * workingset competition needs to consider anon or not depends

	 * on having swap.

 Folio was active prior to eviction */

 XXX: Move to lru_cache_add() when it supports new vs putback */

/**

 * workingset_activation - note a page activation

 * @folio: Folio that is being activated.

	/*

	 * Filter non-memcg pages here, e.g. unmap can call

	 * mark_page_accessed() on VDSO pages.

	 *

	 * XXX: See workingset_refault() - this should return

	 * root_mem_cgroup even for !CONFIG_MEMCG.

/*

 * Shadow entries reflect the share of the working set that does not

 * fit into memory, so their number depends on the access pattern of

 * the workload.  In most cases, they will refault or get reclaimed

 * along with the inode, but a (malicious) workload that streams

 * through files with a total size several times that of available

 * memory, while preventing the inodes from being reclaimed, can

 * create excessive amounts of shadow nodes.  To keep a lid on this,

 * track shadow nodes and reclaim them when they grow way past the

 * point where they would still be useful.

	/*

	 * Track non-empty nodes that contain only shadow entries;

	 * unlink those that contain pages or are being freed.

	 *

	 * Avoid acquiring the list_lru lock when the nodes are

	 * already where they should be. The list_empty() test is safe

	 * as node->private_list is protected by the i_pages lock.

 For __inc_lruvec_page_state */

	/*

	 * Approximate a reasonable limit for the nodes

	 * containing shadow entries. We don't need to keep more

	 * shadow entries than possible pages on the active list,

	 * since refault distances bigger than that are dismissed.

	 *

	 * The size of the active list converges toward 100% of

	 * overall page cache as memory grows, with only a tiny

	 * inactive list. Assume the total cache size for that.

	 *

	 * Nodes might be sparsely populated, with only one shadow

	 * entry in the extreme case. Obviously, we cannot keep one

	 * node for every eligible shadow entry, so compromise on a

	 * worst-case density of 1/8th. Below that, not all eligible

	 * refaults can be detected anymore.

	 *

	 * On 64-bit with 7 xa_nodes per page and 64 slots

	 * each, this will reclaim shadow entries when they consume

	 * ~1.8% of available memory:

	 *

	 * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE

	/*

	 * Page cache insertions and deletions synchronously maintain

	 * the shadow node LRU under the i_pages lock and the

	 * lru_lock.  Because the page cache tree is emptied before

	 * the inode can be destroyed, holding the lru_lock pins any

	 * address_space that has nodes on the LRU.

	 *

	 * We can then safely transition to the i_pages lock to

	 * pin only the address_space of the particular node we want

	 * to reclaim, take the node off-LRU, and drop the lru_lock.

 Coming from the list, invert the lock order */

	/*

	 * The nodes should only contain one or more shadow entries,

	 * no pages, so we expect to be able to remove them all and

	 * delete and free the empty node afterwards.

 list_lru lock nests inside the IRQ-safe i_pages lock */

 ->count reports only fully expendable nodes */

/*

 * Our list_lru->lock is IRQ-safe as it nests inside the IRQ-safe

 * i_pages lock.

	/*

	 * Calculate the eviction bucket size to cover the longest

	 * actionable refault distance, which is currently half of

	 * memory (totalram_pages/2). However, memory hotplug may add

	 * some more pages at runtime, so keep working with up to

	 * double the initial memory by using totalram_pages as-is.

 SPDX-License-Identifier: GPL-2.0

/*

 * CMA SysFS Interface

 *

 * Copyright (c) 2021 Minchan Kim <minchan@kernel.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/page-writeback.c

 *

 * Copyright (C) 2002, Linus Torvalds.

 * Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra

 *

 * Contains functions related to writing back dirty pages at the

 * address_space level.

 *

 * 10Apr2002	Andrew Morton

 *		Initial version

/*

 * Sleep at most 200ms at a time in balance_dirty_pages().

/*

 * Try to keep balance_dirty_pages() call intervals higher than this many pages

 * by raising pause time to max_pause when falls below it.

/*

 * Estimate write bandwidth at 200ms intervals.

/*

 * After a CPU has dirtied this many pages, balance_dirty_pages_ratelimited

 * will look to see if it needs to force writeback or throttling.

 The following parameters are exported via /proc/sys/vm */

/*

 * Start background writeback (via writeback threads) at this percentage

/*

 * dirty_background_bytes starts at 0 (disabled) so that it is a function of

 * dirty_background_ratio * the amount of dirtyable memory

/*

 * free highmem will not be subtracted from the total free memory

 * for calculating free ratios if vm_highmem_is_dirtyable is true

/*

 * The generator of dirty data starts writeback at this percentage

/*

 * vm_dirty_bytes starts at 0 (disabled) so that it is a function of

 * vm_dirty_ratio * the amount of dirtyable memory

/*

 * The interval between `kupdate'-style writebacks

 centiseconds */

/*

 * The longest time for which data is allowed to remain dirty

 centiseconds */

/*

 * Flag that puts the machine in "laptop mode". Doubles as a timeout in jiffies:

 * a full sync is triggered after this time elapses without any disk activity.

 End of sysctl-exported parameters */

 consolidated parameters for balance_dirty_pages() and its subroutines */

 only set in memcg dtc's */

 dirtyable */

 file_dirty + write + nfs */

 dirty threshold */

 dirty background threshold */

 per-wb counterparts */

/*

 * Length of period for aging writeout fractions of bdis. This is an

 * arbitrarily chosen number. The longer the period, the slower fractions will

 * reflect changes in current writeout rate.

	/*

	 * @wb may already be clean by the time control reaches here and

	 * the total may not include its bw.

 CONFIG_CGROUP_WRITEBACK */

 CONFIG_CGROUP_WRITEBACK */

/*

 * In a memory zone, there is a certain amount of pages we consider

 * available for the page cache, which is essentially the number of

 * free and reclaimable pages, minus some zone reserves to protect

 * lowmem and the ability to uphold the zone's watermarks without

 * requiring writeback.

 *

 * This number of dirtyable pages is the base value of which the

 * user-configurable dirty ratio is the effective number of pages that

 * are allowed to be actually dirtied.  Per individual zone, or

 * globally by using the sum of dirtyable pages over all zones.

 *

 * Because the user is allowed to specify the dirty limit globally as

 * absolute number of bytes, calculating the per-zone dirty limit can

 * require translating the configured limit into a percentage of

 * global dirtyable memory first.

/**

 * node_dirtyable_memory - number of dirtyable pages in a node

 * @pgdat: the node

 *

 * Return: the node's number of pages potentially available for dirty

 * page cache.  This is the base value for the per-node dirty limits.

	/*

	 * Pages reserved for the kernel should not be considered

	 * dirtyable, to prevent a situation where reclaim has to

	 * clean pages in order to balance the zones.

 watch for underflows */

	/*

	 * Unreclaimable memory (kernel memory or anonymous memory

	 * without swap) can bring down the dirtyable pages below

	 * the zone's dirty balance reserve and the above calculation

	 * will underflow.  However we still want to add in nodes

	 * which are below threshold (negative values) to get a more

	 * accurate calculation but make sure that the total never

	 * underflows.

	/*

	 * Make sure that the number of highmem pages is never larger

	 * than the number of the total dirtyable memory. This can only

	 * occur in very strange VM situations but we want to make sure

	 * that this does not occur.

/**

 * global_dirtyable_memory - number of globally dirtyable pages

 *

 * Return: the global number of pages potentially available for dirty

 * page cache.  This is the base value for the global dirty limits.

	/*

	 * Pages reserved for the kernel should not be considered

	 * dirtyable, to prevent a situation where reclaim has to

	 * clean pages in order to balance the zones.

 Ensure that we never return 0 */

/**

 * domain_dirty_limits - calculate thresh and bg_thresh for a wb_domain

 * @dtc: dirty_throttle_control of interest

 *

 * Calculate @dtc->thresh and ->bg_thresh considering

 * vm_dirty_{bytes|ratio} and dirty_background_{bytes|ratio}.  The caller

 * must ensure that @dtc->avail is set before calling this function.  The

 * dirty limits will be lifted by 1/4 for real-time tasks.

 convert ratios to per-PAGE_SIZE for higher precision */

 gdtc is !NULL iff @dtc is for memcg domain */

		/*

		 * The byte settings can't be applied directly to memcg

		 * domains.  Convert them to ratios by scaling against

		 * globally available memory.  As the ratios are in

		 * per-PAGE_SIZE, they can be obtained by dividing bytes by

		 * number of pages.

 we should eventually report the domain in the TP */

/**

 * global_dirty_limits - background-writeback and dirty-throttling thresholds

 * @pbackground: out parameter for bg_thresh

 * @pdirty: out parameter for thresh

 *

 * Calculate bg_thresh and thresh for global_wb_domain.  See

 * domain_dirty_limits() for details.

/**

 * node_dirty_limit - maximum number of dirty pages allowed in a node

 * @pgdat: the node

 *

 * Return: the maximum number of dirty pages allowed in a node, based

 * on the node's dirtyable memory.

/**

 * node_dirty_ok - tells whether a node is within its dirty limits

 * @pgdat: the node to check

 *

 * Return: %true when the dirty pages in @pgdat are within the node's

 * dirty limit, %false if the limit is exceeded.

 0 has a special meaning... */

 First event after period switching was turned off? */

		/*

		 * We can race with other __bdi_writeout_inc calls here but

		 * it does not cause any harm since the resulting time when

		 * timer will fire and what is in writeout_period_time will be

		 * roughly the same.

/*

 * Increment @wb's writeout completion count and the global writeout

 * completion count. Called from __folio_end_writeback().

/*

 * On idle system, we can be called long after we scheduled because we use

 * deferred timers so count with missed periods.

		/*

		 * Aging has zeroed all fractions. Stop wasting CPU on period

		 * updates.

/*

 * bdi_min_ratio keeps the sum of the minimum dirty shares of all

 * registered backing devices, which, for obvious reasons, can not

 * exceed 100%.

/*

 * Memory which can be further allocated to a memcg domain is capped by

 * system-wide clean memory excluding the amount being used in the domain.

/**

 * __wb_calc_thresh - @wb's share of dirty throttling threshold

 * @dtc: dirty_throttle_context of interest

 *

 * Note that balance_dirty_pages() will only seriously take it as a hard limit

 * when sleeping max_pause per page is not enough to keep the dirty pages under

 * control. For example, when the device is completely stalled due to some error

 * conditions, or when there are 1000 dd tasks writing to a slow 10MB/s USB key.

 * In the other normal situations, it acts more gently by throttling the tasks

 * more (rather than completely block them) when the wb dirty pages go high.

 *

 * It allocates high/low dirty limits to fast/slow devices, in order to prevent

 * - starving fast devices

 * - piling up dirty pages (that will take long time to sync) on slow devices

 *

 * The wb's share of dirty limit will be adapting to its throughput and

 * bounded by the bdi->min_ratio and/or bdi->max_ratio parameters, if set.

 *

 * Return: @wb's dirty limit in pages. The term "dirty" in the context of

 * dirty balancing includes all PG_dirty and PG_writeback pages.

	/*

	 * Calculate this BDI's share of the thresh ratio.

/*

 *                           setpoint - dirty 3

 *        f(dirty) := 1.0 + (----------------)

 *                           limit - setpoint

 *

 * it's a 3rd order polynomial that subjects to

 *

 * (1) f(freerun)  = 2.0 => rampup dirty_ratelimit reasonably fast

 * (2) f(setpoint) = 1.0 => the balance point

 * (3) f(limit)    = 0   => the hard limit

 * (4) df/dx      <= 0	 => negative feedback control

 * (5) the closer to setpoint, the smaller |df/dx| (and the reverse)

 *     => fast response on large errors; small oscillation near setpoint

/*

 * Dirty position control.

 *

 * (o) global/bdi setpoints

 *

 * We want the dirty pages be balanced around the global/wb setpoints.

 * When the number of dirty pages is higher/lower than the setpoint, the

 * dirty position control ratio (and hence task dirty ratelimit) will be

 * decreased/increased to bring the dirty pages back to the setpoint.

 *

 *     pos_ratio = 1 << RATELIMIT_CALC_SHIFT

 *

 *     if (dirty < setpoint) scale up   pos_ratio

 *     if (dirty > setpoint) scale down pos_ratio

 *

 *     if (wb_dirty < wb_setpoint) scale up   pos_ratio

 *     if (wb_dirty > wb_setpoint) scale down pos_ratio

 *

 *     task_ratelimit = dirty_ratelimit * pos_ratio >> RATELIMIT_CALC_SHIFT

 *

 * (o) global control line

 *

 *     ^ pos_ratio

 *     |

 *     |            |<===== global dirty control scope ======>|

 * 2.0  * * * * * * *

 *     |            .*

 *     |            . *

 *     |            .   *

 *     |            .     *

 *     |            .        *

 *     |            .            *

 * 1.0 ................................*

 *     |            .                  .     *

 *     |            .                  .          *

 *     |            .                  .              *

 *     |            .                  .                 *

 *     |            .                  .                    *

 *   0 +------------.------------------.----------------------*------------->

 *           freerun^          setpoint^                 limit^   dirty pages

 *

 * (o) wb control line

 *

 *     ^ pos_ratio

 *     |

 *     |            *

 *     |              *

 *     |                *

 *     |                  *

 *     |                    * |<=========== span ============>|

 * 1.0 .......................*

 *     |                      . *

 *     |                      .   *

 *     |                      .     *

 *     |                      .       *

 *     |                      .         *

 *     |                      .           *

 *     |                      .             *

 *     |                      .               *

 *     |                      .                 *

 *     |                      .                   *

 *     |                      .                     *

 * 1/4 ...............................................* * * * * * * * * * * *

 *     |                      .                         .

 *     |                      .                           .

 *     |                      .                             .

 *   0 +----------------------.-------------------------------.------------->

 *                wb_setpoint^                    x_intercept^

 *

 * The wb control line won't drop below pos_ratio=1/4, so that wb_dirty can

 * be smoothly throttled down to normal if it starts high in situations like

 * - start writing to a slow SD card and a fast disk at the same time. The SD

 *   card's wb_dirty may rush to many times higher than wb_setpoint.

 * - the wb dirty thresh drops quickly due to change of JBOD workload

 dirty pages' target balance point */

 for scaling up/down the rate limit */

	/*

	 * global setpoint

	 *

	 * See comment for pos_ratio_polynom().

	/*

	 * The strictlimit feature is a tool preventing mistrusted filesystems

	 * from growing a large number of dirty pages before throttling. For

	 * such filesystems balance_dirty_pages always checks wb counters

	 * against wb limits. Even if global "nr_dirty" is under "freerun".

	 * This is especially important for fuse which sets bdi->max_ratio to

	 * 1% by default. Without strictlimit feature, fuse writeback may

	 * consume arbitrary amount of RAM because it is accounted in

	 * NR_WRITEBACK_TEMP which is not involved in calculating "nr_dirty".

	 *

	 * Here, in wb_position_ratio(), we calculate pos_ratio based on

	 * two values: wb_dirty and wb_thresh. Let's consider an example:

	 * total amount of RAM is 16GB, bdi->max_ratio is equal to 1%, global

	 * limits are set by default to 10% and 20% (background and throttle).

	 * Then wb_thresh is 1% of 20% of 16GB. This amounts to ~8K pages.

	 * wb_calc_thresh(wb, bg_thresh) is about ~4K pages. wb_setpoint is

	 * about ~6K pages (as the average of background and throttle wb

	 * limits). The 3rd order polynomial will provide positive feedback if

	 * wb_dirty is under wb_setpoint and vice versa.

	 *

	 * Note, that we cannot use global counters in these calculations

	 * because we want to throttle process writing to a strictlimit wb

	 * much earlier than global "freerun" is reached (~23MB vs. ~2.3GB

	 * in the example above).

		/*

		 * Typically, for strictlimit case, wb_setpoint << setpoint

		 * and pos_ratio >> wb_pos_ratio. In the other words global

		 * state ("dirty") is not limiting factor and we have to

		 * make decision based on wb counters. But there is an

		 * important case when global pos_ratio should get precedence:

		 * global limits are exceeded (e.g. due to activities on other

		 * wb's) while given strictlimit wb is below limit.

		 *

		 * "pos_ratio * wb_pos_ratio" would work for the case above,

		 * but it would look too non-natural for the case of all

		 * activity in the system coming from a single strictlimit wb

		 * with bdi->max_ratio == 100%.

		 *

		 * Note that min() below somewhat changes the dynamics of the

		 * control system. Normally, pos_ratio value can be well over 3

		 * (when globally we are at freerun and wb is well below wb

		 * setpoint). Now the maximum pos_ratio in the same situation

		 * is 2. We might want to tweak this if we observe the control

		 * system is too slow to adapt.

	/*

	 * We have computed basic pos_ratio above based on global situation. If

	 * the wb is over/under its share of dirty pages, we want to scale

	 * pos_ratio further down/up. That is done by the following mechanism.

	/*

	 * wb setpoint

	 *

	 *        f(wb_dirty) := 1.0 + k * (wb_dirty - wb_setpoint)

	 *

	 *                        x_intercept - wb_dirty

	 *                     := --------------------------

	 *                        x_intercept - wb_setpoint

	 *

	 * The main wb control line is a linear function that subjects to

	 *

	 * (1) f(wb_setpoint) = 1.0

	 * (2) k = - 1 / (8 * write_bw)  (in single wb case)

	 *     or equally: x_intercept = wb_setpoint + 8 * write_bw

	 *

	 * For single wb case, the dirty pages are observed to fluctuate

	 * regularly within range

	 *        [wb_setpoint - write_bw/2, wb_setpoint + write_bw/2]

	 * for various filesystems, where (2) can yield in a reasonable 12.5%

	 * fluctuation range for pos_ratio.

	 *

	 * For JBOD case, wb_thresh (not wb_dirty!) could fluctuate up to its

	 * own size, so move the slope over accordingly and choose a slope that

	 * yields 100% pos_ratio fluctuation on suddenly doubled wb_thresh.

	/*

	 * It's very possible that wb_thresh is close to 0 not because the

	 * device is slow, but that it has remained inactive for long time.

	 * Honour such devices a reasonable good (hopefully IO efficient)

	 * threshold, so that the occasional writes won't be blocked and active

	 * writes can rampup the threshold quickly.

	/*

	 * scale global setpoint to wb's:

	 *	wb_setpoint = setpoint * wb_thresh / thresh

	/*

	 * Use span=(8*write_bw) in single wb case as indicated by

	 * (thresh - wb_thresh ~= 0) and transit to wb_thresh in JBOD case.

	 *

	 *        wb_thresh                    thresh - wb_thresh

	 * span = --------- * (8 * write_bw) + ------------------ * wb_thresh

	 *         thresh                           thresh

	/*

	 * wb reserve area, safeguard against dirty pool underrun and disk idle

	 * It may push the desired control point of global dirty pages higher

	 * than setpoint.

	/*

	 * bw = written * HZ / elapsed

	 *

	 *                   bw * elapsed + write_bandwidth * (period - elapsed)

	 * write_bandwidth = ---------------------------------------------------

	 *                                          period

	 *

	 * @written may have decreased due to folio_account_redirty().

	 * Avoid underflowing @bw calculation.

	/*

	 * one more level of smoothing, for filtering out sudden spikes

 keep avg > 0 to guarantee that tot > 0 if there are dirty wbs */

	/*

	 * Follow up in one step.

	/*

	 * Follow down slowly. Use the higher one as the target, because thresh

	 * may drop below dirty. This is exactly the reason to introduce

	 * dom->dirty_limit which is guaranteed to lie above the dirty pages.

	/*

	 * check locklessly first to optimize away locking for the most time

/*

 * Maintain wb->dirty_ratelimit, the base dirty throttle rate.

 *

 * Normal wb tasks will be curbed at or below it in long term.

 * Obviously it should be around (write_bw / N) when there are N dd tasks.

	/*

	 * The dirty rate will match the writeout rate in long term, except

	 * when dirty pages are truncated by userspace or re-dirtied by FS.

	/*

	 * task_ratelimit reflects each dd's dirty rate for the past 200ms.

 it helps rampup dirty_ratelimit from tiny values */

	/*

	 * A linear estimation of the "balanced" throttle rate. The theory is,

	 * if there are N dd tasks, each throttled at task_ratelimit, the wb's

	 * dirty_rate will be measured to be (N * task_ratelimit). So the below

	 * formula will yield the balanced rate limit (write_bw / N).

	 *

	 * Note that the expanded form is not a pure rate feedback:

	 *	rate_(i+1) = rate_(i) * (write_bw / dirty_rate)		     (1)

	 * but also takes pos_ratio into account:

	 *	rate_(i+1) = rate_(i) * (write_bw / dirty_rate) * pos_ratio  (2)

	 *

	 * (1) is not realistic because pos_ratio also takes part in balancing

	 * the dirty rate.  Consider the state

	 *	pos_ratio = 0.5						     (3)

	 *	rate = 2 * (write_bw / N)				     (4)

	 * If (1) is used, it will stuck in that state! Because each dd will

	 * be throttled at

	 *	task_ratelimit = pos_ratio * rate = (write_bw / N)	     (5)

	 * yielding

	 *	dirty_rate = N * task_ratelimit = write_bw		     (6)

	 * put (6) into (1) we get

	 *	rate_(i+1) = rate_(i)					     (7)

	 *

	 * So we end up using (2) to always keep

	 *	rate_(i+1) ~= (write_bw / N)				     (8)

	 * regardless of the value of pos_ratio. As long as (8) is satisfied,

	 * pos_ratio is able to drive itself to 1.0, which is not only where

	 * the dirty count meet the setpoint, but also where the slope of

	 * pos_ratio is most flat and hence task_ratelimit is least fluctuated.

	/*

	 * balanced_dirty_ratelimit ~= (write_bw / N) <= write_bw

	/*

	 * We could safely do this and return immediately:

	 *

	 *	wb->dirty_ratelimit = balanced_dirty_ratelimit;

	 *

	 * However to get a more stable dirty_ratelimit, the below elaborated

	 * code makes use of task_ratelimit to filter out singular points and

	 * limit the step size.

	 *

	 * The below code essentially only uses the relative value of

	 *

	 *	task_ratelimit - dirty_ratelimit

	 *	= (pos_ratio - 1) * dirty_ratelimit

	 *

	 * which reflects the direction and size of dirty position error.

	/*

	 * dirty_ratelimit will follow balanced_dirty_ratelimit iff

	 * task_ratelimit is on the same side of dirty_ratelimit, too.

	 * For example, when

	 * - dirty_ratelimit > balanced_dirty_ratelimit

	 * - dirty_ratelimit > task_ratelimit (dirty pages are above setpoint)

	 * lowering dirty_ratelimit will help meet both the position and rate

	 * control targets. Otherwise, don't update dirty_ratelimit if it will

	 * only help meet the rate target. After all, what the users ultimately

	 * feel and care are stable dirty rate and small position error.

	 *

	 * |task_ratelimit - dirty_ratelimit| is used to limit the step size

	 * and filter out the singular points of balanced_dirty_ratelimit. Which

	 * keeps jumping around randomly and can even leap far away at times

	 * due to the small 200ms estimation period of dirty_rate (we want to

	 * keep that period small to reduce time lags).

	/*

	 * For strictlimit case, calculations above were based on wb counters

	 * and limits (starting from pos_ratio = wb_position_ratio() and up to

	 * balanced_dirty_ratelimit = task_ratelimit * write_bw / dirty_rate).

	 * Hence, to calculate "step" properly, we have to use wb_dirty as

	 * "dirty" and wb_setpoint as "setpoint".

	 *

	 * We rampup dirty_ratelimit forcibly if wb_dirty is low because

	 * it's possible that wb_thresh is close to zero due to inactivity

	 * of backing device.

	/*

	 * Don't pursue 100% rate matching. It's impossible since the balanced

	 * rate itself is constantly fluctuating. So decrease the track speed

	 * when it gets close to the target. Helps eliminate pointless tremors.

	/*

	 * Lockless checks for elapsed time are racy and delayed update after

	 * IO completion doesn't do it at all (to make sure written pages are

	 * accounted reasonably quickly). Make sure elapsed >= 1 to avoid

	 * division errors.

		/*

		 * @mdtc is always NULL if !CGROUP_WRITEBACK but the

		 * compiler has no way to figure that out.  Help it.

 Interval after which we consider wb idle and don't estimate bandwidth */

/*

 * After a task dirtied this many pages, balance_dirty_pages_ratelimited()

 * will look to see if it needs to start dirty throttling.

 *

 * If dirty_poll_interval is too low, big NUMA machines will call the expensive

 * global_zone_page_state() too often. So scale it near-sqrt to the safety margin

 * (the number of pages we may dirty without exceeding the dirty limits).

	/*

	 * Limit pause time for small memory systems. If sleeping for too long

	 * time, a small pool of dirty/writeback pages may go empty and disk go

	 * idle.

	 *

	 * 8 serves as the safety ratio.

 target pause */

 estimated next pause */

 target nr_dirtied_pause */

 target for 10ms pause on 1-dd case */

	/*

	 * Scale up pause time for concurrent dirtiers in order to reduce CPU

	 * overheads.

	 *

	 * (N * 10ms) on 2^N concurrent tasks.

	/*

	 * This is a bit convoluted. We try to base the next nr_dirtied_pause

	 * on the much more stable dirty_ratelimit. However the next pause time

	 * will be computed based on task_ratelimit and the two rate limits may

	 * depart considerably at some time. Especially if task_ratelimit goes

	 * below dirty_ratelimit/2 and the target pause is max_pause, the next

	 * pause time will be max_pause*2 _trimmed down_ to max_pause.  As a

	 * result task_ratelimit won't be executed faithfully, which could

	 * eventually bring down dirty_ratelimit.

	 *

	 * We apply two rules to fix it up:

	 * 1) try to estimate the next pause time and if necessary, use a lower

	 *    nr_dirtied_pause so as not to exceed max_pause. When this happens,

	 *    nr_dirtied_pause will be "dancing" with task_ratelimit.

	 * 2) limit the target pause time to max_pause/2, so that the normal

	 *    small fluctuations of task_ratelimit won't trigger rule (1) and

	 *    nr_dirtied_pause will remain as stable as dirty_ratelimit.

	/*

	 * Tiny nr_dirtied_pause is found to hurt I/O performance in the test

	 * case fio-mmap-randwrite-64k, which does 16*{sync read, async write}.

	 * When the 16 consecutive reads are often interrupted by some dirty

	 * throttling pause during the async writes, cfq will go into idles

	 * (deadline is fine). So push nr_dirtied_pause as high as possible

	 * until reaches DIRTY_POLL_THRESH=32 pages.

	/*

	 * The minimal pause time will normally be half the target pause time.

	/*

	 * wb_thresh is not treated as some limiting factor as

	 * dirty_thresh, due to reasons

	 * - in JBOD setup, wb_thresh can fluctuate a lot

	 * - in a system with HDD and USB key, the USB key may somehow

	 *   go into state (wb_dirty >> wb_thresh) either because

	 *   wb_dirty starts high, or because wb_thresh drops low.

	 *   In this case we don't want to hard throttle the USB key

	 *   dirtiers for 100 seconds until wb_dirty drops under

	 *   wb_thresh. Instead the auxiliary wb control line in

	 *   wb_position_ratio() will let the dirtier task progress

	 *   at some rate <= (write_bw / 2) for bringing down wb_dirty.

	/*

	 * In order to avoid the stacked BDI deadlock we need

	 * to ensure we accurately count the 'dirty' pages when

	 * the threshold is low.

	 *

	 * Otherwise it would be possible to get thresh+n pages

	 * reported dirty, even though there are thresh-m pages

	 * actually dirty; with m+n sitting in the percpu

	 * deltas.

/*

 * balance_dirty_pages() must be called by processes which are generating dirty

 * data.  It looks at the number of dirty pages in the machine and will force

 * the caller to wait once crossing the (background_thresh + dirty_thresh) / 2.

 * If we're over `background_thresh' then the writeback threads are woken to

 * perform some writeout.

 = file_dirty */

 stop bogus uninit warnings */

			/*

			 * If @wb belongs to !root memcg, repeat the same

			 * basic calculations for the memcg domain.

		/*

		 * Throttle it only when the background writeback cannot

		 * catch-up. This avoids (excessively) small writeouts

		 * when the wb limits are ramping up in case of !strictlimit.

		 *

		 * In strictlimit case make decision based on the wb counters

		 * and limits. Small writeouts when the wb limits are ramping

		 * up are the price we consciously pay for strictlimit-ing.

		 *

		 * If memcg domain is in effect, @dirty should be under

		 * both global and memcg freerun ceilings.

		/*

		 * Calculate global domain's pos_ratio and select the

		 * global dtc by default.

				/*

				 * LOCAL_THROTTLE tasks must not be throttled

				 * when below the per-wb freerun ceiling.

			/*

			 * If memcg domain is in effect, calculate its

			 * pos_ratio.  @wb should satisfy constraints from

			 * both global and memcg domains.  Choose the one

			 * w/ lower pos_ratio.

					/*

					 * LOCAL_THROTTLE tasks must not be

					 * throttled when below the per-wb

					 * freerun ceiling.

 throttle according to the chosen dtc */

		/*

		 * For less than 1s think time (ext3/4 may block the dirtier

		 * for up to 800ms from time to time on 1-HDD; so does xfs,

		 * however at much less frequency), try to compensate it in

		 * future periods by updating the virtual time; otherwise just

		 * do a reset, as it may be a light dirtier.

 for occasional dropped task_ratelimit */

		/*

		 * This is typically equal to (dirty < thresh) and can also

		 * keep "1000+ dd on a slow USB stick" under control.

		/*

		 * In the case of an unresponsive NFS server and the NFS dirty

		 * pages exceeds dirty_thresh, give the other good wb's a pipe

		 * to go through, so that tasks on them still remain responsive.

		 *

		 * In theory 1 page is enough to keep the consumer-producer

		 * pipe going: the flusher cleans 1 page => the task dirties 1

		 * more page. However wb_dirty has accounting errors.  So use

		 * the larger and more IO friendly wb_stat_error.

	/*

	 * In laptop mode, we wait until hitting the higher threshold before

	 * starting background writeout, and then write out all the way down

	 * to the lower threshold.  So slow writers cause minimal disk activity.

	 *

	 * In normal mode, we start background writeout at the lower

	 * background_thresh, to keep the amount of dirty memory low.

/*

 * Normal tasks are throttled by

 *	loop {

 *		dirty tsk->nr_dirtied_pause pages;

 *		take a snap in balance_dirty_pages();

 *	}

 * However there is a worst case. If every task exit immediately when dirtied

 * (tsk->nr_dirtied_pause - 1) pages, balance_dirty_pages() will never be

 * called to throttle the page dirties. The solution is to save the not yet

 * throttled page dirties in dirty_throttle_leaks on task exit and charge them

 * randomly into the running tasks. This works well for the above worst case,

 * as the new task will pick up and accumulate the old task's leaked dirty

 * count and eventually get throttled.

/**

 * balance_dirty_pages_ratelimited - balance dirty memory state

 * @mapping: address_space which was dirtied

 *

 * Processes which are dirtying memory should call in here once for each page

 * which was newly dirtied.  The function will periodically check the system's

 * dirty state and will initiate writeback if needed.

 *

 * Once we're over the dirty memory limit we decrease the ratelimiting

 * by a lot, to prevent individual processes from overshooting the limit

 * by (ratelimit_pages) each.

	/*

	 * This prevents one CPU to accumulate too many dirtied pages without

	 * calling into balance_dirty_pages(), which can happen when there are

	 * 1000+ tasks, all of them start dirtying pages at exactly the same

	 * time, hence all honoured too large initial task->nr_dirtied_pause.

	/*

	 * Pick up the dirtied pages by the exited tasks. This avoids lots of

	 * short-lived tasks (eg. gcc invocations in a kernel build) escaping

	 * the dirty throttling and livelock other long-run dirtiers.

/**

 * wb_over_bg_thresh - does @wb need to be written back?

 * @wb: bdi_writeback of interest

 *

 * Determines whether background writeback should keep writing @wb or it's

 * clean enough.

 *

 * Return: %true if writeback should continue.

	/*

	 * Similar to balance_dirty_pages() but ignores pages being written

	 * as we're trying to decide whether to put more under writeback.

 ditto, ignore writeback */

/*

 * sysctl handler for /proc/sys/vm/dirty_writeback_centisecs

	/*

	 * Writing 0 to dirty_writeback_interval will disable periodic writeback

	 * and a different non-zero value will wakeup the writeback threads.

	 * wb_wakeup_delayed() would be more appropriate, but it's a pain to

	 * iterate over all bdis and wbs.

	 * The reason we do this is to make the change take effect immediately.

/*

 * We've spun up the disk and we're in laptop mode: schedule writeback

 * of all dirty data a few seconds from now.  If the flush is already scheduled

 * then push it back - the user is still using the disk.

/*

 * We're in laptop mode and we've just synced. The sync's writes will have

 * caused another writeback to be scheduled by laptop_io_completion.

 * Nothing needs to be written back anymore, so we unschedule the writeback.

/*

 * If ratelimit_pages is too high then we can get into dirty-data overload

 * if a large number of processes all perform writes at the same time.

 *

 * Here we set ratelimit_pages to a level which ensures that when all CPUs are

 * dirtying in parallel, we cannot go more than 3% (1/32) over the dirty memory

 * thresholds.

/*

 * Called early on to tune the page writeback dirty limits.

 *

 * We used to scale dirty pages according to how total memory

 * related to pages that could be allocated for buffers.

 *

 * However, that was when we used "dirty_ratio" to scale with

 * all memory, and we don't do that any more. "dirty_ratio"

 * is now applied to total non-HIGHPAGE memory, and as such we can't

 * get into the old insane situation any more where we had

 * large amounts of dirty pages compared to a small amount of

 * non-HIGHMEM memory.

 *

 * But we might still want to scale the dirty_ratio by how

 * much memory the box has..

/**

 * tag_pages_for_writeback - tag pages to be written by write_cache_pages

 * @mapping: address space structure to write

 * @start: starting page index

 * @end: ending page index (inclusive)

 *

 * This function scans the page range from @start to @end (inclusive) and tags

 * all pages that have DIRTY tag set with a special TOWRITE tag. The idea is

 * that write_cache_pages (or whoever calls this function) will then use

 * TOWRITE tag to identify pages eligible for writeback.  This mechanism is

 * used to avoid livelocking of writeback by a process steadily creating new

 * dirty pages in the file (thus it is important for this function to be quick

 * so that it can tag pages faster than a dirtying process can create them).

/**

 * write_cache_pages - walk the list of dirty pages of the given address space and write all of them.

 * @mapping: address space structure to write

 * @wbc: subtract the number of written pages from *@wbc->nr_to_write

 * @writepage: function called for each page

 * @data: data passed to writepage function

 *

 * If a page is already under I/O, write_cache_pages() skips it, even

 * if it's dirty.  This is desirable behaviour for memory-cleaning writeback,

 * but it is INCORRECT for data-integrity system calls such as fsync().  fsync()

 * and msync() need to guarantee that all the data which was dirty at the time

 * the call was made get new I/O started against them.  If wbc->sync_mode is

 * WB_SYNC_ALL then we were called for data integrity and we must wait for

 * existing IO to complete.

 *

 * To avoid livelocks (when other process dirties new pages), we first tag

 * pages which should be written back with TOWRITE tag and only then start

 * writing them. For data-integrity sync we have to be careful so that we do

 * not miss some pages (e.g., because some other process has cleared TOWRITE

 * tag we set). The rule we follow is that TOWRITE tag can be cleared only

 * by the process clearing the DIRTY tag (and submitting the page for IO).

 *

 * To avoid deadlocks between range_cyclic writeback and callers that hold

 * pages in PageWriteback to aggregate IO until write_cache_pages() returns,

 * we do not loop back to the start of the file. Doing so causes a page

 * lock/page writeback access order inversion - we should only ever lock

 * multiple pages in ascending page->index order, and looping back to the start

 * of the file violates that rule and causes deadlocks.

 *

 * Return: %0 on success, negative error code otherwise

 Inclusive */

 prev offset */

			/*

			 * Page truncated or invalidated. We can freely skip it

			 * then, even for data integrity operations: the page

			 * has disappeared concurrently, so there could be no

			 * real expectation of this data integrity operation

			 * even if there is now a new, dirty page at the same

			 * pagecache address.

 someone wrote it for us */

				/*

				 * Handle errors according to the type of

				 * writeback. There's no need to continue for

				 * background writeback. Just push done_index

				 * past this page so media errors won't choke

				 * writeout for the entire file. For integrity

				 * writeback, we must process the entire dirty

				 * set regardless of errors because the fs may

				 * still have state to clear for each page. In

				 * that case we continue processing and return

				 * the first error.

			/*

			 * We stop writing back only if we are not doing

			 * integrity sync. In case of integrity sync we have to

			 * keep going until we have written all the pages

			 * we tagged for writeback prior to entering this loop.

	/*

	 * If we hit the last page and there is more work to be done: wrap

	 * back the index back to the start of the file for the next

	 * time we are called.

/*

 * Function used by generic_writepages to call the real writepage

 * function and set the mapping flags on error

/**

 * generic_writepages - walk the list of dirty pages of the given address space and writepage() all of them.

 * @mapping: address space structure to write

 * @wbc: subtract the number of written pages from *@wbc->nr_to_write

 *

 * This is a library function, which implements the writepages()

 * address_space_operation.

 *

 * Return: %0 on success, negative error code otherwise

 deal with chardevs and other special file */

		/*

		 * Lacking an allocation context or the locality or writeback

		 * state of any of the inode's pages, throttle based on

		 * writeback activity on the local node. It's as good a

		 * guess as any.

	/*

	 * Usually few pages are written by now from those we've just submitted

	 * but if there's constant writeback being submitted, this makes sure

	 * writeback bandwidth is updated once in a while.

/**

 * folio_write_one - write out a single folio and wait on I/O.

 * @folio: The folio to write.

 *

 * The folio must be locked by the caller and will be unlocked upon return.

 *

 * Note that the mapping's AS_EIO/AS_ENOSPC flags will be cleared when this

 * function returns.

 *

 * Return: %0 on success, negative error code otherwise

/*

 * For address_spaces which do not use buffers nor write back.

/*

 * Helper function for set_page_dirty family.

 *

 * Caller must hold lock_page_memcg().

 *

 * NOTE: This relies on being atomic wrt interrupts.

/*

 * Helper function for deaccounting dirty page without writeback.

 *

 * Caller must hold lock_page_memcg().

/*

 * Mark the folio dirty, and set it dirty in the page cache, and mark

 * the inode dirty.

 *

 * If warn is true, then emit a warning if the folio is not uptodate and has

 * not been truncated.

 *

 * The caller must hold lock_page_memcg().

 Race with truncate? */

/**

 * filemap_dirty_folio - Mark a folio dirty for filesystems which do not use buffer_heads.

 * @mapping: Address space this folio belongs to.

 * @folio: Folio to be marked as dirty.

 *

 * Filesystems which do not use buffer heads should call this function

 * from their set_page_dirty address space operation.  It ignores the

 * contents of folio_get_private(), so if the filesystem marks individual

 * blocks as dirty, the filesystem should handle that itself.

 *

 * This is also sometimes used by filesystems which use buffer_heads when

 * a single buffer is being dirtied: we want to set the folio dirty in

 * that case, but not all the buffers.  This is a "bottom-up" dirtying,

 * whereas __set_page_dirty_buffers() is a "top-down" dirtying.

 *

 * The caller must ensure this doesn't race with truncation.  Most will

 * simply hold the folio lock, but e.g. zap_pte_range() calls with the

 * folio mapped and the pte lock held, which also locks out truncation.

 !PageAnon && !swapper_space */

/**

 * folio_account_redirty - Manually account for redirtying a page.

 * @folio: The folio which is being redirtied.

 *

 * Most filesystems should call folio_redirty_for_writepage() instead

 * of this fuction.  If your filesystem is doing writeback outside the

 * context of a writeback_control(), it can call this when redirtying

 * a folio, to de-account the dirty counters (NR_DIRTIED, WB_DIRTIED,

 * tsk->nr_dirtied), so that they match the written counters (NR_WRITTEN,

 * WB_WRITTEN) in long term. The mismatches will lead to systematic errors

 * in balanced_dirty_ratelimit and the dirty pages position control.

/**

 * folio_redirty_for_writepage - Decline to write a dirty folio.

 * @wbc: The writeback control.

 * @folio: The folio.

 *

 * When a writepage implementation decides that it doesn't want to write

 * @folio for some reason, it should call this function, unlock @folio and

 * return 0.

 *

 * Return: True if we redirtied the folio.  False if someone else dirtied

 * it first.

/**

 * folio_mark_dirty - Mark a folio as being modified.

 * @folio: The folio.

 *

 * For folios with a mapping this should be done under the page lock

 * for the benefit of asynchronous memory errors who prefer a consistent

 * dirty state. This rule can be broken in some special cases,

 * but should be better not to.

 *

 * Return: True if the folio was newly dirtied, false if it was already dirty.

		/*

		 * readahead/lru_deactivate_page could remain

		 * PG_readahead/PG_reclaim due to race with end_page_writeback

		 * About readahead, if the page is written, the flags would be

		 * reset. So no problem.

		 * About lru_deactivate_page, if the page is redirty, the flag

		 * will be reset. So no problem. but if the page is used by readahead

		 * it will confuse readahead and make it restart the size rampup

		 * process. But it's a trivial problem.

/*

 * set_page_dirty() is racy if the caller has no reference against

 * page->mapping->host, and if the page is unlocked.  This is because another

 * CPU could truncate the page off the mapping and then free the mapping.

 *

 * Usually, the page _is_ locked, or the caller is a user-space process which

 * holds a reference on the inode by having an open file.

 *

 * In other cases, the page should be locked before running set_page_dirty().

/*

 * This cancels just the dirty bit on the kernel page itself, it does NOT

 * actually remove dirty bits on any mmap's that may be around. It also

 * leaves the page tagged dirty, so any sync activity will still find it on

 * the dirty lists, and in particular, clear_page_dirty_for_io() will still

 * look at the dirty bits in the VM.

 *

 * Doing this should *normally* only ever be done when a page is truncated,

 * and is not actually mapped anywhere at all. However, fs/buffer.c does

 * this when it notices that somebody has cleaned out all the buffers on a

 * page without actually doing it through the VM. Can you say "ext3 is

 * horribly ugly"? Thought you could.

/*

 * Clear a folio's dirty flag, while caring for dirty memory accounting.

 * Returns true if the folio was previously dirty.

 *

 * This is for preparing to put the folio under writeout.  We leave

 * the folio tagged as dirty in the xarray so that a concurrent

 * write-for-sync can discover it via a PAGECACHE_TAG_DIRTY walk.

 * The ->writepage implementation will run either folio_start_writeback()

 * or folio_mark_dirty(), at which stage we bring the folio's dirty flag

 * and xarray dirty tag back into sync.

 *

 * This incoherency between the folio's dirty flag and xarray tag is

 * unfortunate, but it only exists while the folio is locked.

		/*

		 * Yes, Virginia, this is indeed insane.

		 *

		 * We use this sequence to make sure that

		 *  (a) we account for dirty stats properly

		 *  (b) we tell the low-level filesystem to

		 *      mark the whole folio dirty if it was

		 *      dirty in a pagetable. Only to then

		 *  (c) clean the folio again and return 1 to

		 *      cause the writeback.

		 *

		 * This way we avoid all nasty races with the

		 * dirty bit in multiple places and clearing

		 * them concurrently from different threads.

		 *

		 * Note! Normally the "folio_mark_dirty(folio)"

		 * has no effect on the actual dirty bit - since

		 * that will already usually be set. But we

		 * need the side effects, and it can help us

		 * avoid races.

		 *

		 * We basically use the folio "master dirty bit"

		 * as a serialization point for all the different

		 * threads doing their things.

		/*

		 * We carefully synchronise fault handlers against

		 * installing a dirty pte and marking the folio dirty

		 * at this point.  We do this by having them hold the

		 * page lock while dirtying the folio, and folios are

		 * always locked coming in here, so we get the desired

		 * exclusion.

	/*

	 * Make sure estimate of writeback throughput gets updated after

	 * writeback completed. We delay the update by BANDWIDTH_INTERVAL

	 * (which is the interval other bandwidth updates use for batching) so

	 * that if multiple inodes end writeback at a similar time, they get

	 * batched into one bandwidth update.

			/*

			 * We can come through here when swapping

			 * anonymous folios, so we don't necessarily

			 * have an inode to track for sync.

	/*

	 * If writeback has been triggered on a page that cannot be made

	 * accessible, it is too late to recover here.

/**

 * folio_wait_writeback - Wait for a folio to finish writeback.

 * @folio: The folio to wait for.

 *

 * If the folio is currently being written back to storage, wait for the

 * I/O to complete.

 *

 * Context: Sleeps.  Must be called in process context and with

 * no spinlocks held.  Caller should hold a reference on the folio.

 * If the folio is not locked, writeback may start again after writeback

 * has finished.

/**

 * folio_wait_writeback_killable - Wait for a folio to finish writeback.

 * @folio: The folio to wait for.

 *

 * If the folio is currently being written back to storage, wait for the

 * I/O to complete or a fatal signal to arrive.

 *

 * Context: Sleeps.  Must be called in process context and with

 * no spinlocks held.  Caller should hold a reference on the folio.

 * If the folio is not locked, writeback may start again after writeback

 * has finished.

 * Return: 0 on success, -EINTR if we get a fatal signal while waiting.

/**

 * folio_wait_stable() - wait for writeback to finish, if necessary.

 * @folio: The folio to wait on.

 *

 * This function determines if the given folio is related to a backing

 * device that requires folio contents to be held stable during writeback.

 * If so, then it will wait for any pending writeback to complete.

 *

 * Context: Sleeps.  Must be called in process context and with

 * no spinlocks held.  Caller should hold a reference on the folio.

 * If the folio is not locked, writeback may start again after writeback

 * has finished.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Contiguous Memory Allocator

 *

 * Copyright (c) 2010-2011 by Samsung Electronics.

 * Copyright IBM Corporation, 2013

 * Copyright LG Electronics Inc., 2014

 * Written by:

 *	Marek Szyprowski <m.szyprowski@samsung.com>

 *	Michal Nazarewicz <mina86@mina86.com>

 *	Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>

 *	Joonsoo Kim <iamjoonsoo.kim@lge.com>

/*

 * Find the offset of the base PFN from the specified align_order.

 * The value returned is represented in order_per_bits.

	/*

	 * alloc_contig_range() requires the pfn range specified to be in the

	 * same zone. Simplify by forcing the entire CMA resv range to be in the

	 * same zone.

 Expose all pages to the buddy, they are useless for CMA. */

/**

 * cma_init_reserved_mem() - create custom contiguous area from reserved memory

 * @base: Base address of the reserved area

 * @size: Size of the reserved area (in bytes),

 * @order_per_bit: Order of pages represented by one bit on bitmap.

 * @name: The name of the area. If this parameter is NULL, the name of

 *        the area will be set to "cmaN", where N is a running counter of

 *        used areas.

 * @res_cma: Pointer to store the created cma region.

 *

 * This function creates custom contiguous area from already reserved memory.

 Sanity checks */

 ensure minimal alignment required by mm core */

 alignment should be aligned with order_per_bit */

	/*

	 * Each reserved area must be initialised later, when more kernel

	 * subsystems (like slab allocator) are available.

/**

 * cma_declare_contiguous_nid() - reserve custom contiguous area

 * @base: Base address of the reserved area optional, use 0 for any

 * @size: Size of the reserved area (in bytes),

 * @limit: End address of the reserved memory (optional, 0 for any).

 * @alignment: Alignment for the CMA area, should be power of 2 or zero

 * @order_per_bit: Order of pages represented by one bit on bitmap.

 * @fixed: hint about where to place the reserved area

 * @name: The name of the area. See function cma_init_reserved_mem()

 * @res_cma: Pointer to store the created cma region.

 * @nid: nid of the free area to find, %NUMA_NO_NODE for any node

 *

 * This function reserves memory from early allocator. It should be

 * called by arch specific code once the early allocator (memblock or bootmem)

 * has been activated and all other subsystems have already allocated/reserved

 * memory. This function allows to create custom reserved areas.

 *

 * If @fixed is true, reserve contiguous area at exactly @base.  If false,

 * reserve in range from @base to @limit.

	/*

	 * We can't use __pa(high_memory) directly, since high_memory

	 * isn't a valid direct map VA, and DEBUG_VIRTUAL will (validly)

	 * complain. Find the boundary by adding one to the last valid

	 * address.

	/*

	 * Sanitise input arguments.

	 * Pages both ends in CMA area could be merged into adjacent unmovable

	 * migratetype page by page allocator's buddy algorithm. In the case,

	 * you couldn't get a contiguous memory, which is not what we want.

 size should be aligned with order_per_bit */

	/*

	 * If allocating at a fixed base the request region must not cross the

	 * low/high memory boundary.

	/*

	 * If the limit is unspecified or above the memblock end, its effective

	 * value will be the memblock end. Set it explicitly to simplify further

	 * checks.

 Reserve memory */

		/*

		 * All pages in the reserved area must come from the same zone.

		 * If the requested region crosses the low/high memory boundary,

		 * try allocating from high memory first and fall back to low

		 * memory in case of failure.

		/*

		 * If there is enough memory, try a bottom-up allocation first.

		 * It will place the new cma area close to the start of the node

		 * and guarantee that the compaction is moving pages out of the

		 * cma area and not into it.

		 * Avoid using first 4GB to not interfere with constrained zones

		 * like DMA/DMA32.

		/*

		 * kmemleak scans/reads tracked objects for pointers to other

		 * objects but this address isn't mapped and accessible

/**

 * cma_alloc() - allocate pages from contiguous area

 * @cma:   Contiguous memory region for which the allocation is performed.

 * @count: Requested number of pages.

 * @align: Requested alignment of pages (in PAGE_SIZE order).

 * @no_warn: Avoid printing message about failed allocation

 *

 * This function allocates part of contiguous memory on specific

 * contiguous memory area.

		/*

		 * It's safe to drop the lock here. We've marked this region for

		 * our exclusive use. If the migration fails we will take the

		 * lock again and unmark it.

 try again with a bit different memory target */

	/*

	 * CMA can allocate multiple page blocks, which results in different

	 * blocks being marked with different tags. Reset the tags to ignore

	 * those page blocks.

/**

 * cma_release() - release allocated pages

 * @cma:   Contiguous memory region for which the allocation is performed.

 * @pages: Allocated pages.

 * @count: Number of allocated pages.

 *

 * This function releases memory allocated by cma_alloc().

 * It returns false when provided pages do not belong to contiguous area and

 * true otherwise.

	/*

	 * Add the page and check if we are full. If so

	 * force a flush.

 MMU_GATHER_NO_GATHER */

/*

 * Semi RCU freeing of the page directories.

 *

 * This is needed by some architectures to implement software pagetable walkers.

 *

 * gup_fast() and other software pagetable walkers do a lockless page-table

 * walk and therefore needs some synchronization with the freeing of the page

 * directories. The chosen means to accomplish that is by disabling IRQs over

 * the walk.

 *

 * Architectures that use IPIs to flush TLBs will then automagically DTRT,

 * since we unlink the page, flush TLBs, free the page. Since the disabling of

 * IRQs delays the completion of the TLB flush we can never observe an already

 * freed page.

 *

 * Architectures that do not have this (PPC) need to delay the freeing by some

 * other means, this is that means.

 *

 * What we do is batch the freed directory pages (tables) and RCU free them.

 * We use the sched RCU variant, as that guarantees that IRQ/preempt disabling

 * holds off grace periods.

 *

 * However, in order to batch these pages we need to allocate storage, this

 * allocation is deep inside the MM code and can thus easily fail on memory

 * pressure. To guarantee progress we fall back to single table freeing, see

 * the implementation of tlb_remove_table_one().

 *

 Simply deliver the interrupt */

	/*

	 * This isn't an RCU grace period and hence the page-tables cannot be

	 * assumed to be actually RCU-freed.

	 *

	 * It is however sufficient for software page-table walkers that rely on

	 * IRQ disabling.

 !CONFIG_MMU_GATHER_RCU_TABLE_FREE */

 CONFIG_MMU_GATHER_RCU_TABLE_FREE */

/*

 * If we want tlb_remove_table() to imply TLB invalidates.

		/*

		 * Invalidate page-table caches used by hardware walkers. Then

		 * we still need to RCU-sched wait while freeing the pages

		 * because software walkers can still be in-flight.

 !CONFIG_MMU_GATHER_TABLE_FREE */

 CONFIG_MMU_GATHER_TABLE_FREE */

/**

 * tlb_gather_mmu - initialize an mmu_gather structure for page-table tear-down

 * @tlb: the mmu_gather structure to initialize

 * @mm: the mm_struct of the target address space

 *

 * Called to initialize an (on-stack) mmu_gather structure for page-table

 * tear-down from @mm.

/**

 * tlb_gather_mmu_fullmm - initialize an mmu_gather structure for page-table tear-down

 * @tlb: the mmu_gather structure to initialize

 * @mm: the mm_struct of the target address space

 *

 * In this case, @mm is without users and we're going to destroy the

 * full address space (exit/execve).

 *

 * Called to initialize an (on-stack) mmu_gather structure for page-table

 * tear-down from @mm.

/**

 * tlb_finish_mmu - finish an mmu_gather structure

 * @tlb: the mmu_gather structure to finish

 *

 * Called at the end of the shootdown operation to free up any resources that

 * were required.

	/*

	 * If there are parallel threads are doing PTE changes on same range

	 * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB

	 * flush by batching, one thread may end up seeing inconsistent PTEs

	 * and result in having stale TLB entries.  So flush TLB forcefully

	 * if we detect parallel PTE batching threads.

	 *

	 * However, some syscalls, e.g. munmap(), may free page tables, this

	 * needs force flush everything in the given range. Otherwise this

	 * may result in having stale TLB entries for some architectures,

	 * e.g. aarch64, that could specify flush what level TLB.

		/*

		 * The aarch64 yields better performance with fullmm by

		 * avoiding multiple CPUs spamming TLBI messages at the

		 * same time.

		 *

		 * On x86 non-fullmm doesn't yield significant difference

		 * against fullmm.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Cleancache frontend

 *

 * This code provides the generic "frontend" layer to call a matching

 * "backend" driver implementation of cleancache.  See

 * Documentation/vm/cleancache.rst for more information.

 *

 * Copyright (C) 2009-2010 Oracle Corp. All rights reserved.

 * Author: Dan Magenheimer

/*

 * cleancache_ops is set by cleancache_register_ops to contain the pointers

 * to the cleancache "backend" implementation functions.

/*

 * Counters available via /sys/kernel/debug/cleancache (if debugfs is

 * properly configured.  These are for information only so are not protected

 * against increment races.

/*

 * Register operations for cleancache. Returns 0 on success.

	/*

	 * A cleancache backend can be built as a module and hence loaded after

	 * a cleancache enabled filesystem has called cleancache_init_fs. To

	 * handle such a scenario, here we call ->init_fs or ->init_shared_fs

	 * for each active super block. To differentiate between local and

	 * shared filesystems, we temporarily initialize sb->cleancache_poolid

	 * to CLEANCACHE_NO_BACKEND or CLEANCACHE_NO_BACKEND_SHARED

	 * respectively in case there is no backend registered at the time

	 * cleancache_init_fs or cleancache_init_shared_fs is called.

	 *

	 * Since filesystems can be mounted concurrently with cleancache

	 * backend registration, we have to be careful to guarantee that all

	 * cleancache enabled filesystems that has been mounted by the time

	 * cleancache_register_ops is called has got and all mounted later will

	 * get cleancache_poolid. This is assured by the following statements

	 * tied together:

	 *

	 * a) iterate_supers skips only those super blocks that has started

	 *    ->kill_sb

	 *

	 * b) if iterate_supers encounters a super block that has not finished

	 *    ->mount yet, it waits until it is finished

	 *

	 * c) cleancache_init_fs is called from ->mount and

	 *    cleancache_invalidate_fs is called from ->kill_sb

	 *

	 * d) we call iterate_supers after cleancache_ops has been set

	 *

	 * From a) it follows that if iterate_supers skips a super block, then

	 * either the super block is already dead, in which case we do not need

	 * to bother initializing cleancache for it, or it was mounted after we

	 * initiated iterate_supers. In the latter case, it must have seen

	 * cleancache_ops set according to d) and initialized cleancache from

	 * ->mount by itself according to c). This proves that we call

	 * ->init_fs at least once for each active super block.

	 *

	 * From b) and c) it follows that if iterate_supers encounters a super

	 * block that has already started ->init_fs, it will wait until ->mount

	 * and hence ->init_fs has finished, then check cleancache_poolid, see

	 * that it has already been set and therefore do nothing. This proves

	 * that we call ->init_fs no more than once for each super block.

	 *

	 * Combined together, the last two paragraphs prove the function

	 * correctness.

	 *

	 * Note that various cleancache callbacks may proceed before this

	 * function is called or even concurrently with it, but since

	 * CLEANCACHE_NO_BACKEND is negative, they will all result in a noop

	 * until the corresponding ->init_fs has been actually called and

	 * cleancache_ops has been set.

 Called by a cleancache-enabled filesystem at time of mount */

 Called by a cleancache-enabled clustered filesystem at time of mount */

/*

 * If the filesystem uses exportable filehandles, use the filehandle as

 * the key, else use the inode number.

/*

 * "Get" data from cleancache associated with the poolid/inode/index

 * that were specified when the data was put to cleanache and, if

 * successful, use it to fill the specified page with data and return 0.

 * The pageframe is unchanged and returns -1 if the get fails.

 * Page must be locked by caller.

 *

 * The function has two checks before any action is taken - whether

 * a backend is registered and whether the sb->cleancache_poolid

 * is correct.

/*

 * "Put" data from a page to cleancache and associate it with the

 * (previously-obtained per-filesystem) poolid and the page's,

 * inode and page index.  Page must be locked.  Note that a put_page

 * always "succeeds", though a subsequent get_page may succeed or fail.

 *

 * The function has two checks before any action is taken - whether

 * a backend is registered and whether the sb->cleancache_poolid

 * is correct.

/*

 * Invalidate any data from cleancache associated with the poolid and the

 * page's inode and page index so that a subsequent "get" will fail.

 *

 * The function has two checks before any action is taken - whether

 * a backend is registered and whether the sb->cleancache_poolid

 * is correct.

 careful... page->mapping is NULL sometimes when this is called */

/*

 * Invalidate all data from cleancache associated with the poolid and the

 * mappings's inode so that all subsequent gets to this poolid/inode

 * will fail.

 *

 * The function has two checks before any action is taken - whether

 * a backend is registered and whether the sb->cleancache_poolid

 * is correct.

/*

 * Called by any cleancache-enabled filesystem at time of unmount;

 * note that pool_id is surrendered and may be returned by a subsequent

 * cleancache_init_fs or cleancache_init_shared_fs.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008, 2009 Intel Corporation

 * Authors: Andi Kleen, Fengguang Wu

 *

 * High level machine check handler. Handles pages reported by the

 * hardware as being corrupted usually due to a multi-bit ECC memory or cache

 * failure.

 * 

 * In addition there is a "soft offline" entry point that allows stop using

 * not-yet-corrupted-by-suspicious pages without killing anything.

 *

 * Handles page cache pages in various states.	The tricky part

 * here is that we can access any page asynchronously in respect to 

 * other VM users, because memory failures could happen anytime and 

 * anywhere. This could violate some of their assumptions. This is why 

 * this code has to be extremely careful. Generally it tries to use 

 * normal locking rules, as in get the standard locks, even if that means 

 * the error handling takes potentially a long time.

 *

 * It can be very tempting to add handling for obscure cases here.

 * In general any code for handling new cases should only be added iff:

 * - You know how to test it.

 * - You have a test that can be added to mce-test

 *   https://git.kernel.org/cgit/utils/cpu/mce/mce-test.git/

 * - The case actually shows up as a frequent (top 10) page state in

 *   tools/vm/page-types when running a real workload.

 * 

 * There are several operations here with exponential complexity because

 * of unsuitable VM data structures. For example the operation to map back 

 * from RMAP chains to processes has to walk the complete process list and 

 * has non linear complexity with the number. But since memory corruptions

 * are rare we hope to get away with this. This avoids impacting the core 

 * VM.

		/*

		 * Doing this check for free pages is also fine since dissolve_free_huge_page

		 * returns 0 for non-hugetlb pages as well.

			/*

			 * We could fail to take off the target page from buddy

			 * for example due to racy page allocation, but that's

			 * acceptable because soft-offlined page is not broken

			 * and if someone really want to use it, they should

			 * take it.

	/*

	 * page_mapping() does not accept slab pages.

/*

 * This allows stress tests to limit test scope to a collection of tasks

 * by putting them under some memcg. This prevents killing unrelated/important

 * processes such as /sbin/init. Note that the target task may share clean

 * pages with init (eg. libc text), which is harmless. If the target task

 * share _dirty_ pages with another task B, the test scheme must make sure B

 * is also included in the memcg. At last, due to race conditions this filter

 * can only guarantee that the page either belongs to the memcg tasks, or is

 * a freed page.

/*

 * Kill all processes that have a poisoned page mapped and then isolate

 * the page.

 *

 * General strategy:

 * Find all processes having the page mapped and kill them.

 * But we keep a page reference around so that the page is not

 * actually freed yet.

 * Then stash the page away

 *

 * There's no convenient way to get back to mapped processes

 * from the VMAs. So do a brute-force search over all

 * running processes.

 *

 * Remember that machine checks are not common (or rather

 * if they are common you have other problems), so this shouldn't

 * be a performance issue.

 *

 * Also there are some races possible while we get from the

 * error detection to actually handle it.

/*

 * Send all the processes who have the page mapped a signal.

 * ``action optional'' if they are not immediately affected by the error

 * ``action required'' if error happened in current execution context

 Signal other processes sharing the page if they have PF_MCE_EARLY set. */

		/*

		 * Don't use force here, it's convenient if the signal

		 * can be temporarily blocked.

		 * This could cause a loop when the user sets SIGBUS

		 * to SIG_IGN, but hopefully no one will do that?

 synchronous? */

/*

 * Unknown page type encountered. Try to check whether it can turn PageLRU by

 * lru_add_drain_all.

	/*

	 * TODO: Could shrink slab caches here if a lightweight range-based

	 * shrinker will be available.

/*

 * Failure handling: if we can't find or can't kill a process there's

 * not much we can do.	We just print a message and ignore otherwise.

/*

 * Schedule a process for later kill.

 * Uses GFP_ATOMIC allocations to avoid potential recursions in the VM.

	/*

	 * Send SIGKILL if "tk->addr == -EFAULT". Also, as

	 * "tk->size_shift" is always non-zero for !is_zone_device_page(),

	 * so "tk->size_shift == 0" effectively checks no mapping on

	 * ZONE_DEVICE. Indeed, when a devdax page is mmapped N times

	 * to a process' address space, it's possible not all N VMAs

	 * contain mappings for the page, but at least one VMA does.

	 * Only deliver SIGBUS with payload derived from the VMA that

	 * has a mapping for the page.

/*

 * Kill the processes that have been collected earlier.

 *

 * Only do anything when FORCEKILL is set, otherwise just free the

 * list (this is used for clean pages which do not need killing)

 * Also when FAIL is set do a force kill because something went

 * wrong earlier.

			/*

			 * In case something went wrong with munmapping

			 * make sure the process doesn't catch the

			 * signal and then access the memory. Just kill it.

			/*

			 * In theory the process could have mapped

			 * something else on the address in-between. We could

			 * check for that, but we need to tell the

			 * process anyways.

/*

 * Find a dedicated thread which is supposed to handle SIGBUS(BUS_MCEERR_AO)

 * on behalf of the thread group. Return task_struct of the (first found)

 * dedicated thread if found, and return NULL otherwise.

 *

 * We already hold read_lock(&tasklist_lock) in the caller, so we don't

 * have to call rcu_read_lock/unlock() in this function.

/*

 * Determine whether a given process is "early kill" process which expects

 * to be signaled when some page under the process is hwpoisoned.

 * Return task_struct of the dedicated thread (main thread unless explicitly

 * specified) if the process is "early kill" and otherwise returns NULL.

 *

 * Note that the above is true for Action Optional case. For Action Required

 * case, it's only meaningful to the current thread which need to be signaled

 * with SIGBUS, this error is Action Optional for other non current

 * processes sharing the same error page,if the process is "early kill", the

 * task_struct of the dedicated thread will also be returned.

	/*

	 * Comparing ->mm here because current task might represent

	 * a subthread, while tsk always points to the main thread.

/*

 * Collect processes when the error hit an anonymous page.

 Not actually mapped anymore */

/*

 * Collect processes when the error hit a file mapped page.

			/*

			 * Send early kill signal to tasks where a vma covers

			 * the page but the corrupted page is not necessarily

			 * mapped it in its pte.

			 * Assume applications who requested early kill want

			 * to be informed of all such data corruptions.

/*

 * Collect the processes who have the corrupted page mapped to kill.

/*

 * Sends SIGBUS to the current process with error info.

 *

 * This function is intended to handle "Action Required" MCEs on already

 * hardware poisoned pages. They could happen, for example, when

 * memory_failure() failed to unmap the error page at the first call, or

 * when multiple local machine checks happened on different CPUs.

 *

 * MCE handler currently has no easy access to the error virtual address,

 * so this function walks page table to find it. The returned virtual address

 * is proper in most cases, but it could be wrong when the application

 * process has multiple entries mapping the error page.

/*

 * XXX: It is possible that a page is isolated from LRU cache,

 * and then kept in swap cache or failed to remove from page cache.

 * The page count will stop it from being freed by unpoison.

 * Stress tests should be aware of this memory leak problem.

		/*

		 * Clear sensible page flags, so that the buddy system won't

		 * complain when the page is unpoison-and-freed.

		/*

		 * Poisoned page might never drop its ref count to 0 so we have

		 * to uncharge it manually from its memcg.

		/*

		 * drop the page count elevated by isolate_lru_page()

		/*

		 * If the file system doesn't support it just invalidate

		 * This fails on dirty or anything with private pages

 Callback ->action() has to unlock the relevant page inside it. */

/*

 * Return true if page is still referenced by others, otherwise return

 * false.

 *

 * The extra_pins is true when one extra refcount is expected.

/*

 * Error hit kernel page.

 * Do nothing, try to be lucky and not touch this instead. For a few cases we

 * could be more sophisticated.

/*

 * Page in unknown state. Do nothing.

/*

 * Clean (or cleaned) page cache page.

	/*

	 * For anonymous pages we're done the only reference left

	 * should be the one m_f() holds.

	/*

	 * Now truncate the page in the page cache. This is really

	 * more like a "temporary hole punch"

	 * Don't do this for block devices when someone else

	 * has a reference, because it could be file system metadata

	 * and that's not safe to truncate.

		/*

		 * Page has been teared down in the meanwhile

	/*

	 * Truncation is a bit tricky. Enable it per file system for now.

	 *

	 * Open: to take i_rwsem or not for this? Right now we don't.

/*

 * Dirty pagecache page

 * Issues: when the error hit a hole page the error is not properly

 * propagated.

 TBD: print more information about the file. */

		/*

		 * IO error will be reported by write(), fsync(), etc.

		 * who check the mapping.

		 * This way the application knows that something went

		 * wrong with its dirty file data.

		 *

		 * There's one open issue:

		 *

		 * The EIO will be only reported on the next IO

		 * operation and then cleared through the IO map.

		 * Normally Linux has two mechanisms to pass IO error

		 * first through the AS_EIO flag in the address space

		 * and then through the PageError flag in the page.

		 * Since we drop pages on memory failure handling the

		 * only mechanism open to use is through AS_AIO.

		 *

		 * This has the disadvantage that it gets cleared on

		 * the first operation that returns an error, while

		 * the PageError bit is more sticky and only cleared

		 * when the page is reread or dropped.  If an

		 * application assumes it will always get error on

		 * fsync, but does other operations on the fd before

		 * and the page is dropped between then the error

		 * will not be properly reported.

		 *

		 * This can already happen even without hwpoisoned

		 * pages: first on metadata IO errors (which only

		 * report through AS_EIO) or when the page is dropped

		 * at the wrong time.

		 *

		 * So right now we assume that the application DTRT on

		 * the first EIO, but we're not worse than other parts

		 * of the kernel.

/*

 * Clean and dirty swap cache.

 *

 * Dirty swap cache page is tricky to handle. The page could live both in page

 * cache and swap cache(ie. page is freshly swapped in). So it could be

 * referenced concurrently by 2 types of PTEs:

 * normal PTEs and swap PTEs. We try to handle them consistently by calling

 * try_to_unmap(TTU_IGNORE_HWPOISON) to convert the normal PTEs to swap PTEs,

 * and then

 *      - clear dirty bit to prevent IO

 *      - remove from LRU

 *      - but keep in the swap cache, so that when we return to it on

 *        a later page fault, we know the application is accessing

 *        corrupted data and shall be killed (we installed simple

 *        interception code in do_swap_page to catch it).

 *

 * Clean swap cache pages can be directly isolated. A later page fault will

 * bring in the known good data from disk.

 Trigger EIO in shmem: */

/*

 * Huge pages. Needs work.

 * Issues:

 * - Error on hugepage is contained in hugepage unit (not in raw page unit.)

 *   To narrow down kill region to one page, we need to break up pmd.

		/*

		 * migration entry prevents later access on error anonymous

		 * hugepage, so we can free and dissolve it into buddy to

		 * save healthy subpages.

/*

 * Various page states we can handle.

 *

 * A page state is defined by its current page->flags bits.

 * The table matches them in order and calls the right handler.

 *

 * This is quite tricky because we can access page at any time

 * in its live cycle, so all accesses have to be extremely careful.

 *

 * This is not complete. More states could be added.

 * For any missing state don't attempt recovery.

	/*

	 * free pages are specially detected outside this table:

	 * PG_buddy pages only make a small fraction of all free pages.

	/*

	 * Could in theory check if slab page is free or if we can drop

	 * currently unused objects without touching them. But just

	 * treat it as standard kernel for now.

	/*

	 * Catchall entry: must be at end.

/*

 * "Dirty/Clean" indication is not 100% accurate due to the possibility of

 * setting PG_dirty outside page lock. See also comment above set_page_dirty().

 page p should be unlocked after returning from ps->action().  */

 Could do more checks here if page looks ok */

	/*

	 * Could adjust zone counters here to correct for the missing page.

/*

 * Return true if a page type of a given page is supported by hwpoison

 * mechanism (while handling could fail), otherwise false.  This function

 * does not return true for hugetlb or device memory pages, so it's assumed

 * to be called only in the context where we never have such pages.

	/*

	 * This check prevents from calling get_hwpoison_unless_zero()

	 * for any unsupported type of page in order to reduce the risk of

	 * unexpected races caused by taking a page refcount.

 We raced with an allocation, retry. */

 We raced with put_page, retry. */

			/*

			 * We raced with (possibly temporary) unhandlable

			 * page, retry.

		/*

		 * A page we cannot handle. Check whether we can turn

		 * it into something we can handle.

/**

 * get_hwpoison_page() - Get refcount for memory error handling

 * @p:		Raw error page (hit by memory error)

 * @flags:	Flags controlling behavior of error handling

 *

 * get_hwpoison_page() takes a page refcount of an error page to handle memory

 * error on it, after checking that the error page is in a well-defined state

 * (defined as a page-type we can successfully handle the memor error on it,

 * such as LRU page and hugetlb page).

 *

 * Memory error handling could be triggered at any time on any type of page,

 * so it's prone to race with typical memory management lifecycle (like

 * allocation and free).  So to avoid such races, get_hwpoison_page() takes

 * extra care for the error page's state (as done in __get_hwpoison_page()),

 * and has some retry logic in get_any_page().

 *

 * Return: 0 on failure,

 *         1 on success for in-use pages in a well-defined state,

 *         -EIO for pages on which we can not handle memory errors,

 *         -EBUSY when get_hwpoison_page() has raced with page lifecycle

 *         operations like allocation and free.

/*

 * Do all that is necessary to remove user space mappings. Unmap

 * the pages and send SIGBUS to the processes if the data was dirty.

	/*

	 * Here we are interested only in user-mapped pages, so skip any

	 * other types of pages.

	/*

	 * This check implies we don't kill processes if their pages

	 * are in the swap cache early. Those are always late kills.

	/*

	 * Propagate the dirty bit from PTEs to struct page first, because we

	 * need this to decide if we should kill or just drop the page.

	 * XXX: the dirty test could be racy: set_page_dirty() may not always

	 * be called inside page lock (it's recommended but not enforced).

	/*

	 * First collect all the processes that have the page

	 * mapped in dirty form.  This has to be done before try_to_unmap,

	 * because ttu takes the rmap data structures down.

	 *

	 * Error handling: We ignore errors here because

	 * there's nothing that can be done.

			/*

			 * For hugetlb pages in shared mappings, try_to_unmap

			 * could potentially call huge_pmd_unshare.  Because of

			 * this, take semaphore in write mode here and set

			 * TTU_RMAP_LOCKED to indicate we have taken the lock

			 * at this higher level.

	/*

	 * try_to_unmap() might put mlocked page in lru cache, so call

	 * shake_page() again to ensure that it's flushed.

	/*

	 * Now that the dirty bit has been propagated to the

	 * struct page and all unmaps done we can decide if

	 * killing is needed or not.  Only kill when the page

	 * was dirty or the process is not restartable,

	 * otherwise the tokill list is merely

	 * freed.  When there was a problem unmapping earlier

	 * use a more force-full uncatchable kill to prevent

	 * any accesses to the poisoned memory.

	/*

	 * The first check uses the current page flags which may not have any

	 * relevant information. The second check with the saved page flags is

	 * carried out only if the first check can't determine the page status.

			/*

			 * Check "filter hit" and "race with other subpage."

	/*

	 * TODO: hwpoison for pud-sized hugetlb doesn't work right now, so

	 * simply disable it. In order to make it work properly, we need

	 * make sure that:

	 *  - conversion of a pud that maps an error hugetlb into hwpoison

	 *    entry properly works, and

	 *  - other mm code walking over page table is aware of pud-aligned

	 *    hwpoison entries.

		/*

		 * Drop the extra refcount in case we come from madvise().

 device metadata space is not recoverable */

	/*

	 * Prevent the inode from being freed while we are interrogating

	 * the address_space, typically this would be handled by

	 * lock_page(), but dax pages do not use the page lock. This

	 * also prevents changes to the mapping of this pfn until

	 * poison signaling is complete.

		/*

		 * TODO: Handle HMM pages which may need coordination

		 * with device-side memory.

	/*

	 * Use this flag as an indication that the dax page has been

	 * remapped UC to prevent speculative consumption of poison.

	/*

	 * Unlike System-RAM there is no possibility to swap in a

	 * different physical page at a given virtual address, so all

	 * userspace consumption of ZONE_DEVICE memory necessitates

	 * SIGBUS (i.e. MF_MUST_KILL)

		/*

		 * Unmap the largest mapping to avoid breaking up

		 * device-dax mappings which are constant size. The

		 * actual size of the mapping being torn down is

		 * communicated in siginfo, see kill_proc()

 drop pgmap ref acquired in caller */

/**

 * memory_failure - Handle memory failure of a page.

 * @pfn: Page Number of the corrupted page

 * @flags: fine tune action taken

 *

 * This function is called by the low level machine check code

 * of an architecture when it detects hardware memory corruption

 * of a page. It tries its best to recover, which includes

 * dropping pages, killing processes etc.

 *

 * The function is primarily of use for corruptions that

 * happen outside the current execution context (e.g. when

 * detected by a background scrubber)

 *

 * Must run in process context (e.g. a work queue) with interrupts

 * enabled and no spinlocks hold.

	/*

	 * We need/can do nothing about count=0 pages.

	 * 1) it's a free page, and therefore in safe hand:

	 *    prep_new_page() will be the gate keeper.

	 * 2) it's part of a non-compound high order page.

	 *    Implies some kernel user: cannot stop them from

	 *    R/W the page; let's pray that the page has been

	 *    used and will be freed some time later.

	 * In fact it's dangerous to directly bump up page count from 0,

	 * that may make page_ref_freeze()/page_ref_unfreeze() mismatch.

 We lost the race, try again */

		/*

		 * The flag must be set after the refcount is bumped

		 * otherwise it may race with THP split.

		 * And the flag can't be set in get_hwpoison_page() since

		 * it is called by soft offline too and it is just called

		 * for !MF_COUNT_INCREASE.  So here seems to be the best

		 * place.

		 *

		 * Don't need care about the above error handling paths for

		 * get_hwpoison_page() since they handle either free page

		 * or unhandlable page.  The refcount is bumped iff the

		 * page is a valid handlable page.

	/*

	 * We ignore non-LRU pages for good reasons.

	 * - PG_locked is only well defined for LRU pages and a few others

	 * - to avoid races with __SetPageLocked()

	 * - to avoid races with __SetPageSlab*() (and more non-atomic ops)

	 * The check (unnecessarily) ignores LRU pages being isolated and

	 * walked by the page reclaim code, however that's not a big loss.

	/*

	 * The page could have changed compound pages during the locking.

	 * If this happens just bail out.

	/*

	 * We use page flags to determine what action should be taken, but

	 * the flags can be modified by the error containment action.  One

	 * example is an mlocked page, where PG_mlocked is cleared by

	 * page_remove_rmap() in try_to_unmap_one(). So to determine page status

	 * correctly, we save a copy of the page flags at this time.

	/*

	 * unpoison always clear PG_hwpoison inside page lock

	/*

	 * __munlock_pagevec may clear a writeback page's LRU flag without

	 * page_lock. We need wait writeback completion for this page or it

	 * may trigger vfs BUG while evict inode.

	/*

	 * It's very difficult to mess with pages currently under IO

	 * and in many cases impossible, so we just avoid it here.

	/*

	 * Now take care of user space mappings.

	 * Abort on fail: __delete_from_page_cache() assumes unmapped page.

	/*

	 * Torn down by someone else?

/**

 * memory_failure_queue - Schedule handling memory failure of a page.

 * @pfn: Page Number of the corrupted page

 * @flags: Flags for memory failure handling

 *

 * This function is called by the low level hardware error handler

 * when it detects hardware memory corruption of a page. It schedules

 * the recovering of error page, including dropping pages, killing

 * processes etc.

 *

 * The function is primarily of use for corruptions that

 * happen outside the current execution context (e.g. when

 * detected by a background scrubber)

 *

 * Can run in IRQ context.

/*

 * Process memory_failure work queued on the specified CPU.

 * Used to avoid return-to-userspace racing with the memory_failure workqueue.

/**

 * unpoison_memory - Unpoison a previously poisoned page

 * @pfn: Page number of the to be unpoisoned page

 *

 * Software-unpoison a page that has been poisoned by

 * memory_failure() earlier.

 *

 * This is only done on the software-level, so it only works

 * for linux injected failures, not real hardware failures

 *

 * Returns 0 for success, otherwise -errno.

	/*

	 * unpoison_memory() can encounter thp only when the thp is being

	 * worked by memory_failure() and the page lock is not held yet.

	 * In such case, we yield to memory_failure() and make unpoison fail.

	/*

	 * This test is racy because PG_hwpoison is set outside of page lock.

	 * That's acceptable because that won't trigger kernel panic. Instead,

	 * the PG_hwpoison page will be caught and isolated on the entrance to

	 * the free buddy page pool.

	/*

	 * If we succeed to isolate the page, we grabbed another refcount on

	 * the page, so we can safely drop the one we got from get_any_pages().

	 * If we failed to isolate the page, it means that we cannot go further

	 * and we will return an error, so drop the reference we got from

	 * get_any_pages() as well.

/*

 * __soft_offline_page handles hugetlb-pages and non-hugetlb pages.

 * If the page is a non-dirty unmapped page-cache page, it simply invalidates.

 * If the page is mapped, it migrates the contents over.

	/*

	 * Check PageHWPoison again inside page lock because PageHWPoison

	 * is set by memory_failure() outside page lock. Note that

	 * memory_failure() also double-checks PageHWPoison inside page lock,

	 * so there's no race between soft_offline_page() and memory_failure().

		/*

		 * Try to invalidate first. This should work for

		 * non dirty unmapped page cache pages.

	/*

	 * RED-PEN would be better to keep it isolated here, but we

	 * would need to fix isolation locking first.

/**

 * soft_offline_page - Soft offline a page.

 * @pfn: pfn to soft-offline

 * @flags: flags. Same as memory_failure().

 *

 * Returns 0 on success, otherwise negated errno.

 *

 * Soft offline a page, by migration or invalidation,

 * without killing anything. This is for the case when

 * a page is not corrupted yet (so it's still valid to access),

 * but has had a number of corrected errors and is better taken

 * out.

 *

 * The actual policy on when to do that is maintained by

 * user space.

 *

 * This should never impact any application or cause data loss,

 * however it might take some time.

 *

 * This is not a 100% solution for all memory, but tries to be

 * ``good enough'' for the majority of memory.

 Only online pages can be soft-offlined (esp., not ZONE_DEVICE). */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Swap reorganised 29.12.95, Stephen Tweedie.

 *  kswapd added: 7.1.96  sct

 *  Removed kswapd_ctl limits, and swap out as many pages as needed

 *  to bring the system back to freepages.high: 2.4.97, Rik van Riel.

 *  Zone aware kswapd started 02/00, Kanoj Sarcar (kanoj@sgi.com).

 *  Multiqueue VM started 5.8.00, Rik van Riel.

#include <linux/buffer_head.h>	/* for try_to_release_page(),

 How many pages shrink_list() should reclaim */

	/*

	 * Nodemask of nodes allowed by the caller. If NULL, all nodes

	 * are scanned.

	/*

	 * The memory cgroup that hit its limit and as a result is the

	 * primary target of this reclaim invocation.

	/*

	 * Scan pressure balancing between anon and file LRUs

 Can active pages be deactivated as part of reclaim? */

 Writepage batching in laptop mode; RECLAIM_WRITE */

 Can mapped pages be reclaimed? */

 Can pages be swapped as part of reclaim? */

	/*

	 * Cgroup memory below memory.low is protected as long as we

	 * don't threaten to OOM. If any cgroup is reclaimed at

	 * reduced force or passed over entirely due to its memory.low

	 * setting (memcg_low_skipped), and nothing is reclaimed as a

	 * result, then go back for one more cycle that reclaims the protected

	 * memory (memcg_low_reclaim) to avert OOM.

 One of the zones is ready for compaction */

 There is easily reclaimable cold cache in the current node */

 The file pages on the current node are dangerously low */

 Always discard instead of demoting to lower tier memory */

 Allocation order */

 Scan (total_size >> priority) pages at once */

 The highest zone to isolate pages for reclaim from */

 This context's GFP mask */

 Incremented by the number of inactive pages that were scanned */

 Number of pages freed so far during a call to shrink_zones() */

 for recording the reclaimed slab by now */

/*

 * From 0 .. 200.  Higher means more swappy.

 Check for an overwrite */

 Check for the nulling of an already-nulled member */

 The shrinker_info is expanded in a batch of BITS_PER_LONG */

 Not yet online memcg */

 map: set all old bits, clear all new bits */

 nr_deferred: copy old values, clear all new values */

 Pairs with smp mb in shrink_slab() */

 This may call shrinker, so it must use down_read_trylock() */

 Prevent from concurrent shrinker_info expand */

/**

 * writeback_throttling_sane - is the usual dirty throttling mechanism available?

 * @sc: scan_control in question

 *

 * The normal page dirty throttling mechanism in balance_dirty_pages() is

 * completely broken with the legacy memcg and direct stalling in

 * shrink_page_list() is used for throttling instead, which lacks all the

 * niceties such as fairness, adaptive pausing, bandwidth proportional

 * allocation and configurability.

 *

 * This function tests whether the vmscan currently in progress can assume

 * that the normal dirty throttling mechanism is operational.

 It is pointless to do demotion in memcg reclaim */

		/*

		 * For non-memcg reclaim, is there

		 * space in any swap device?

 Is the memcg below its swap limit? */

	/*

	 * The page can not be swapped.

	 *

	 * Can it be reclaimed from this node via demotion?

/*

 * This misses isolated pages which are not accounted for to save counters.

 * As the data only determines if reclaim or compaction continues, it is

 * not expected that isolated pages will be a dominating factor.

/**

 * lruvec_lru_size -  Returns the number of pages on the given LRU list.

 * @lruvec: lru vector

 * @lru: lru to use

 * @zone_idx: zones to consider (use MAX_NR_ZONES for the whole LRU list)

/*

 * Add a shrinker callback to be called from the vm.

/*

 * Remove one

/**

 * synchronize_shrinkers - Wait for all running shrinkers to complete.

 *

 * This is equivalent to calling unregister_shrink() and register_shrinker(),

 * but atomically and with less overhead. This is useful to guarantee that all

 * shrinker invocations have seen an update, before freeing memory, similar to

 * rcu.

	/*

	 * copy the current shrinker scan count into a local variable

	 * and zero it so that other concurrent shrinker invocations

	 * don't also do this scanning work.

		/*

		 * These objects don't require any IO to create. Trim

		 * them aggressively under memory pressure to keep

		 * them from causing refetches in the IO caches.

	/*

	 * Normally, we should not scan less than batch_size objects in one

	 * pass to avoid too frequent shrinker calls, but if the slab has less

	 * than batch_size objects in total and we are really tight on memory,

	 * we will try to reclaim all available objects, otherwise we can end

	 * up failing allocations although there are plenty of reclaimable

	 * objects spread over several slabs with usage less than the

	 * batch_size.

	 *

	 * We detect the "tight on memory" situations by looking at the total

	 * number of objects we want to scan (total_scan). If it is greater

	 * than the total number of objects on slab (freeable), we must be

	 * scanning at high prio and therefore should try to reclaim as much as

	 * possible.

	/*

	 * The deferred work is increased by any new work (delta) that wasn't

	 * done, decreased by old deferred work that was done now.

	 *

	 * And it is capped to two times of the freeable items.

	/*

	 * move the unused scan count back into the shrinker in a

	 * manner that handles concurrent updates.

 Call non-slab shrinkers even though kmem is disabled */

			/*

			 * After the shrinker reported that it had no objects to

			 * free, but before we cleared the corresponding bit in

			 * the memcg shrinker map, a new object might have been

			 * added. To make sure, we have the bit set in this

			 * case, we invoke the shrinker one more time and reset

			 * the bit if it reports that it is not empty anymore.

			 * The memory barrier here pairs with the barrier in

			 * set_shrinker_bit():

			 *

			 * list_lru_add()     shrink_slab_memcg()

			 *   list_add_tail()    clear_bit()

			 *   <MB>               <MB>

			 *   set_bit()          do_shrink_slab()

 CONFIG_MEMCG */

 CONFIG_MEMCG */

/**

 * shrink_slab - shrink slab caches

 * @gfp_mask: allocation context

 * @nid: node whose slab caches to target

 * @memcg: memory cgroup whose slab caches to target

 * @priority: the reclaim priority

 *

 * Call the shrink functions to age shrinkable caches.

 *

 * @nid is passed along to shrinkers with SHRINKER_NUMA_AWARE set,

 * unaware shrinkers will receive a node id of 0 instead.

 *

 * @memcg specifies the memory cgroup to target. Unaware shrinkers

 * are called only if it is the root cgroup.

 *

 * @priority is sc->priority, we take the number of objects and >> by priority

 * in order to get the scan target.

 *

 * Returns the number of reclaimed slab objects.

	/*

	 * The root memcg might be allocated even though memcg is disabled

	 * via "cgroup_disable=memory" boot parameter.  This could make

	 * mem_cgroup_is_root() return false, then just run memcg slab

	 * shrink, but skip global shrink.  This may result in premature

	 * oom.

		/*

		 * Bail out if someone want to register a new shrinker to

		 * prevent the registration from being stalled for long periods

		 * by parallel ongoing shrinking.

	/*

	 * A freeable page cache page is referenced only by the caller

	 * that isolated the page, the page cache and optional buffer

	 * heads at page->private.

/*

 * We detected a synchronous write error writing a page out.  Probably

 * -ENOSPC.  We need to propagate that into the address_space for a subsequent

 * fsync(), msync() or close().

 *

 * The tricky part is that after writepage we cannot touch the mapping: nothing

 * prevents it from being freed up.  But we have a ref on the page and once

 * that page is locked, the mapping is pinned.

 *

 * We're allowed to run sleeping lock_page() here because we know the caller has

 * __GFP_FS.

	/*

	 * Do not throttle IO workers, kthreads other than kswapd or

	 * workqueues. They may be required for reclaim to make

	 * forward progress (e.g. journalling workqueues or kthreads).

	/*

	 * These figures are pulled out of thin air.

	 * VMSCAN_THROTTLE_ISOLATED is a transient condition based on too many

	 * parallel reclaimers which is a short-lived event so the timeout is

	 * short. Failing to make progress or waiting on writeback are

	 * potentially long-lived events so use a longer timeout. This is shaky

	 * logic as a failure to make progress could be due to anything from

	 * writeback to a slow device to excessive references pages at the tail

	 * of the inactive LRU.

/*

 * Account for pages written if tasks are throttled waiting on dirty

 * pages to clean. If enough pages have been cleaned since throttling

 * started then wakeup the throttled tasks.

	/*

	 * This is an inaccurate read as the per-cpu deltas may not

	 * be synchronised. However, given that the system is

	 * writeback throttled, it is not worth taking the penalty

	 * of getting an accurate count. At worst, the throttle

	 * timeout guarantees forward progress.

 possible outcome of pageout() */

 failed to write page out, page is locked */

 move page to the active list, page is locked */

 page has been sent to the disk successfully, page is unlocked */

 page is clean and locked */

/*

 * pageout is called by shrink_page_list() for each dirty page.

 * Calls ->writepage().

	/*

	 * If the page is dirty, only perform writeback if that write

	 * will be non-blocking.  To prevent this allocation from being

	 * stalled by pagecache activity.  But note that there may be

	 * stalls if we need to run get_block().  We could test

	 * PagePrivate for that.

	 *

	 * If this process is currently in __generic_file_write_iter() against

	 * this page's queue, we can perform writeback even if that

	 * will block.

	 *

	 * If the page is swapcache, write it back even if that would

	 * block, for some throttling. This happens by accident, because

	 * swap_backing_dev_info is bust: it doesn't reflect the

	 * congestion state of the swapdevs.  Easy to fix, if needed.

		/*

		 * Some data journaling orphaned pages can have

		 * page->mapping == NULL while being dirty with clean buffers.

 synchronous write or broken a_ops? */

/*

 * Same as remove_mapping, but if the page is removed from the mapping, it

 * gets returned with a refcount of 0.

	/*

	 * The non racy check for a busy page.

	 *

	 * Must be careful with the order of the tests. When someone has

	 * a ref to the page, it may be possible that they dirty it then

	 * drop the reference. So if PageDirty is tested before page_count

	 * here, then the following race may occur:

	 *

	 * get_user_pages(&page);

	 * [user mapping goes away]

	 * write_to(page);

	 *				!PageDirty(page)    [good]

	 * SetPageDirty(page);

	 * put_page(page);

	 *				!page_count(page)   [good, discard it]

	 *

	 * [oops, our write_to data is lost]

	 *

	 * Reversing the order of the tests ensures such a situation cannot

	 * escape unnoticed. The smp_rmb is needed to ensure the page->flags

	 * load is not satisfied before that of page->_refcount.

	 *

	 * Note that if SetPageDirty is always performed via set_page_dirty,

	 * and thus under the i_pages lock, then this ordering is not required.

 note: atomic_cmpxchg in page_ref_freeze provides the smp_rmb */

		/*

		 * Remember a shadow entry for reclaimed file cache in

		 * order to detect refaults, thus thrashing, later on.

		 *

		 * But don't store shadows in an address space that is

		 * already exiting.  This is not just an optimization,

		 * inode reclaim needs to empty out the radix tree or

		 * the nodes are lost.  Don't plant shadows behind its

		 * back.

		 *

		 * We also don't store shadows for DAX mappings because the

		 * only page cache pages found in these are zero pages

		 * covering holes, and because we don't want to mix DAX

		 * exceptional entries and shadow exceptional entries in the

		 * same address_space.

/*

 * Attempt to detach a locked page from its ->mapping.  If it is dirty or if

 * someone else has a ref on the page, abort and return 0.  If it was

 * successfully detached, return 1.  Assumes the caller has a single ref on

 * this page.

		/*

		 * Unfreezing the refcount with 1 rather than 2 effectively

		 * drops the pagecache ref for us without requiring another

		 * atomic operation.

/**

 * putback_lru_page - put previously isolated page onto appropriate LRU list

 * @page: page to be put back to appropriate lru list

 *

 * Add previously isolated @page to appropriate LRU list.

 * Page may still be unevictable for other reasons.

 *

 * lru_lock must not be held, interrupts must be enabled.

 drop ref from isolate */

	/*

	 * Mlock lost the isolation race with us.  Let try_to_unmap()

	 * move the page to the unevictable list.

		/*

		 * All mapped pages start out with page table

		 * references from the instantiating fault, so we need

		 * to look twice if a mapped file page is used more

		 * than once.

		 *

		 * Mark it and spare it for another trip around the

		 * inactive list.  Another page table reference will

		 * lead to its activation.

		 *

		 * Note: the mark is set for activated pages as well

		 * so that recently deactivated but used pages are

		 * quickly recovered.

		/*

		 * Activate file-backed executable pages after first usage.

 Reclaim if clean, defer dirty pages to writeback */

 Check if a page is dirty or under writeback */

	/*

	 * Anonymous pages are not handled by flushers and must be written

	 * from reclaim context. Do not stall reclaim based on them

 By default assume that the page flags are accurate */

 Verify dirty/writeback state if the filesystem supports it */

		/*

		 * Allocate from 'node', or fail quickly and quietly.

		 * When this happens, 'page' will likely just be discarded

		 * instead of migrated.

/*

 * Take pages on @demote_list and attempt to demote them to

 * another node.  Pages which are not demoted are left on

 * @demote_pages.

 Demotion ignores all cpuset and mempolicy settings */

/*

 * shrink_page_list() returns the number of reclaimed pages

 Account the number of base pages even though THP */

		/*

		 * The number of dirty pages determines if a node is marked

		 * reclaim_congested. kswapd will stall and start writing

		 * pages if the tail of the LRU is all dirty unqueued pages.

		/*

		 * Treat this page as congested if the underlying BDI is or if

		 * pages are cycling through the LRU so quickly that the

		 * pages marked for immediate reclaim are making it to the

		 * end of the LRU a second time.

		/*

		 * If a page at the tail of the LRU is under writeback, there

		 * are three cases to consider.

		 *

		 * 1) If reclaim is encountering an excessive number of pages

		 *    under writeback and this page is both under writeback and

		 *    PageReclaim then it indicates that pages are being queued

		 *    for IO but are being recycled through the LRU before the

		 *    IO can complete. Waiting on the page itself risks an

		 *    indefinite stall if it is impossible to writeback the

		 *    page due to IO error or disconnected storage so instead

		 *    note that the LRU is being scanned too quickly and the

		 *    caller can stall after page list has been processed.

		 *

		 * 2) Global or new memcg reclaim encounters a page that is

		 *    not marked for immediate reclaim, or the caller does not

		 *    have __GFP_FS (or __GFP_IO if it's simply going to swap,

		 *    not to fs). In this case mark the page for immediate

		 *    reclaim and continue scanning.

		 *

		 *    Require may_enter_fs because we would wait on fs, which

		 *    may not have submitted IO yet. And the loop driver might

		 *    enter reclaim, and deadlock if it waits on a page for

		 *    which it is needed to do the write (loop masks off

		 *    __GFP_IO|__GFP_FS for this reason); but more thought

		 *    would probably show more reasons.

		 *

		 * 3) Legacy memcg encounters a page that is already marked

		 *    PageReclaim. memcg does not have any dirty pages

		 *    throttling so we could easily OOM just because too many

		 *    pages are in writeback and there is nothing else to

		 *    reclaim. Wait for the writeback to complete.

		 *

		 * In cases 1) and 2) we activate the pages to get them out of

		 * the way while we continue scanning for clean pages on the

		 * inactive list and refilling from the active list. The

		 * observation here is that waiting for disk writes is more

		 * expensive than potentially causing reloads down the line.

		 * Since they're marked for immediate reclaim, they won't put

		 * memory pressure on the cache working set any longer than it

		 * takes to write them to disk.

 Case 1 above */

 Case 2 above */

				/*

				 * This is slightly racy - end_page_writeback()

				 * might have just cleared PageReclaim, then

				 * setting PageReclaim here end up interpreted

				 * as PageReadahead - but that does not matter

				 * enough to care.  What we do want is for this

				 * page to have PageReclaim set next time memcg

				 * reclaim reaches the tests above, so it will

				 * then wait_on_page_writeback() to avoid OOM;

				 * and it's also appropriate in global reclaim.

 Case 3 above */

 then go back and try same page again */

 try to reclaim the page below */

		/*

		 * Before reclaiming the page, try to relocate

		 * its contents to another node.

		/*

		 * Anonymous process memory has backing store?

		 * Try to allocate it some swap space here.

		 * Lazyfree page could be freed directly

 cannot split THP, skip it */

					/*

					 * Split pages without a PMD map right

					 * away. Chances are some or all of the

					 * tail pages can be freed without IO.

 Fallback to swap normal pages */

 Adding to swap updated mapping */

 Split file THP */

		/*

		 * THP may get split above, need minus tail pages and update

		 * nr_pages to avoid accounting tail pages twice.

		 *

		 * The tail pages that are added into swap cache successfully

		 * reach here.

		/*

		 * The page is mapped into the page tables of one or more

		 * processes. Try to unmap it here.

			/*

			 * Only kswapd can writeback filesystem pages

			 * to avoid risk of stack overflow. But avoid

			 * injecting inefficient single-page IO into

			 * flusher writeback as much as possible: only

			 * write pages when we've encountered many

			 * dirty pages, and when we've already scanned

			 * the rest of the LRU for clean pages and see

			 * the same dirty pages again (PageReclaim).

				/*

				 * Immediately reclaim when written back.

				 * Similar in principal to deactivate_page()

				 * except we already have the page isolated

				 * and know it's dirty

			/*

			 * Page is dirty. Flush the TLB if a writable entry

			 * potentially exists to avoid CPU writes after IO

			 * starts and then write it out here.

				/*

				 * A synchronous write - probably a ramdisk.  Go

				 * ahead and try to reclaim the page.

 try to free the page below */

		/*

		 * If the page has buffers, try to free the buffer mappings

		 * associated with this page. If we succeed we try to free

		 * the page as well.

		 *

		 * We do this even if the page is PageDirty().

		 * try_to_release_page() does not perform I/O, but it is

		 * possible for a page to have PageDirty set, but it is actually

		 * clean (all its buffers are clean).  This happens if the

		 * buffers were written out directly, with submit_bh(). ext3

		 * will do this, as well as the blockdev mapping.

		 * try_to_release_page() will discover that cleanness and will

		 * drop the buffers and mark the page clean - it can be freed.

		 *

		 * Rarely, pages can have buffers and no ->mapping.  These are

		 * the pages which were not successfully invalidated in

		 * truncate_cleanup_page().  We try to drop those buffers here

		 * and if that worked, and the page is no longer mapped into

		 * process address space (page_count == 1) it can be freed.

		 * Otherwise, leave the page on the LRU so it is swappable.

					/*

					 * rare race with speculative reference.

					 * the speculative reference will free

					 * this page shortly, so we may

					 * increment nr_reclaimed here (and

					 * leave it off the LRU).

 follow __remove_mapping for reference */

			/*

			 * The page has only one reference left, which is

			 * from the isolation. After the caller puts the

			 * page back on lru and drops the reference, the

			 * page will be freed anyway. It doesn't matter

			 * which lru it goes. So we don't bother checking

			 * PageDirty here.

		/*

		 * THP may get swapped out in a whole, need account

		 * all base pages.

		/*

		 * Is there need to periodically free_page_list? It would

		 * appear not as the counts should be low

		/*

		 * The tail pages that are failed to add into swap cache

		 * reach here.  Fixup nr_scanned and nr_pages.

 Not a candidate for swapping, so reclaim swap space. */

 'page_list' is always empty here */

 Migrate pages selected for demotion */

 Pages that could not be demoted are still in @demote_pages */

 Pages which failed to demoted go back on @page_list for retry: */

	/*

	 * We should be safe here since we are only dealing with file pages and

	 * we are not kswapd and therefore cannot write dirty file pages. But

	 * call memalloc_noreclaim_save() anyway, just in case these conditions

	 * change in the future.

	/*

	 * Since lazyfree pages are isolated from file LRU from the beginning,

	 * they will rotate back to anonymous LRU in the end if it failed to

	 * discard so isolated count will be mismatched.

	 * Compensate the isolated count for both LRU lists.

/*

 * Attempt to remove the specified page from its LRU.  Only take this page

 * if it is of the appropriate PageActive status.  Pages which are being

 * freed elsewhere are also ignored.

 *

 * page:	page to consider

 * mode:	one of the LRU isolation modes defined above

 *

 * returns true on success, false on failure.

 Only take pages on the LRU. */

 Compaction should not handle unevictable pages but CMA can do so */

	/*

	 * To minimise LRU disruption, the caller can indicate that it only

	 * wants to isolate pages it will be able to operate on without

	 * blocking - clean pages for the most part.

	 *

	 * ISOLATE_ASYNC_MIGRATE is used to indicate that it only wants to pages

	 * that it is possible to migrate without blocking

 All the caller can do on PageWriteback is block */

			/*

			 * Only pages without mappings or that have a

			 * ->migratepage callback are possible to migrate

			 * without blocking. However, we can be racing with

			 * truncation so it's necessary to lock the page

			 * to stabilise the mapping as truncation holds

			 * the page lock until after the page is removed

			 * from the page cache.

/*

 * Update LRU sizes after isolating pages. The LRU size updates must

 * be complete before mem_cgroup_update_lru_size due to a sanity check.

/*

 * Isolating page from the lruvec to fill in @dst list by nr_to_scan times.

 *

 * lruvec->lru_lock is heavily contended.  Some of the functions that

 * shrink the lists perform better by taking out a batch of pages

 * and working on them outside the LRU lock.

 *

 * For pagecache intensive workloads, this function is the hottest

 * spot in the kernel (apart from copy_*_user functions).

 *

 * Lru_lock must be held before calling this function.

 *

 * @nr_to_scan:	The number of eligible pages to look through on the list.

 * @lruvec:	The LRU vector to pull pages from.

 * @dst:	The temp list to put pages on to.

 * @nr_scanned:	The number of pages that were scanned.

 * @sc:		The scan_control struct for this reclaim session

 * @lru:	LRU list id for isolating

 *

 * returns how many pages were moved onto *@dst.

		/*

		 * Do not count skipped pages because that makes the function

		 * return with no isolated pages if the LRU mostly contains

		 * ineligible pages.  This causes the VM to not reclaim any

		 * pages, triggering a premature OOM.

		 *

		 * Account all tail pages of THP.  This would not cause

		 * premature OOM since __isolate_lru_page() returns -EBUSY

		 * only when the page is being freed somewhere else.

 It is being freed elsewhere */

		/*

		 * Be careful not to clear PageLRU until after we're

		 * sure the page is not being freed elsewhere -- the

		 * page release code relies on it.

 Another thread is already isolating this page */

	/*

	 * Splice any skipped pages to the start of the LRU list. Note that

	 * this disrupts the LRU order when reclaiming for lower zones but

	 * we cannot splice to the tail. If we did then the SWAP_CLUSTER_MAX

	 * scanning would soon rescan the same pages to skip and put the

	 * system at risk of premature OOM.

/**

 * isolate_lru_page - tries to isolate a page from its LRU list

 * @page: page to isolate from its LRU list

 *

 * Isolates a @page from an LRU list, clears PageLRU and adjusts the

 * vmstat statistic corresponding to whatever LRU list the page was on.

 *

 * Returns 0 if the page was removed from an LRU list.

 * Returns -EBUSY if the page was not on an LRU list.

 *

 * The returned page will have PageLRU() cleared.  If it was found on

 * the active list, it will have PageActive set.  If it was found on

 * the unevictable list, it will have the PageUnevictable bit set. That flag

 * may need to be cleared by the caller before letting the page go.

 *

 * The vmstat statistic corresponding to the list on which the page was

 * found will be decremented.

 *

 * Restrictions:

 *

 * (1) Must be called with an elevated refcount on the page. This is a

 *     fundamental difference from isolate_lru_pages (which is called

 *     without a stable reference).

 * (2) the lru_lock must not be held.

 * (3) interrupts must be enabled.

/*

 * A direct reclaimer may isolate SWAP_CLUSTER_MAX pages from the LRU list and

 * then get rescheduled. When there are massive number of tasks doing page

 * allocation, such sleeping direct reclaimers may keep piling up on each CPU,

 * the LRU list will go small and be scanned faster than necessary, leading to

 * unnecessary swapping, thrashing and OOM.

	/*

	 * GFP_NOIO/GFP_NOFS callers are allowed to isolate more pages, so they

	 * won't get blocked by normal direct-reclaimers, forming a circular

	 * deadlock.

 Wake up tasks throttled due to too_many_isolated. */

/*

 * move_pages_to_lru() moves pages from private @list to appropriate LRU list.

 * On return, @list is reused as a list of pages to be freed by the caller.

 *

 * Returns the number of pages moved to the given lruvec.

		/*

		 * The SetPageLRU needs to be kept here for list integrity.

		 * Otherwise:

		 *   #0 move_pages_to_lru             #1 release_pages

		 *   if !put_page_testzero

		 *				      if (put_page_testzero())

		 *				        !PageLRU //skip lru_lock

		 *     SetPageLRU()

		 *     list_add(&page->lru,)

		 *                                        list_add(&page->lru,)

		/*

		 * All pages were isolated from the same lruvec (and isolation

		 * inhibits memcg migration).

	/*

	 * To save our caller's stack, now use input list for pages to free.

/*

 * If a kernel thread (such as nfsd for loop-back mounts) services

 * a backing device by writing to the page cache it sets PF_LOCAL_THROTTLE.

 * In that case we should only throttle if the backing device it is

 * writing to is congested.  In other cases it is safe to throttle.

/*

 * shrink_inactive_list() is a helper for shrink_node().  It returns the number

 * of reclaimed pages

 wait a bit for the reclaimer. */

 We are about to die and free our memory. Return now. */

	/*

	 * If dirty pages are scanned that are not queued for IO, it

	 * implies that flushers are not doing their job. This can

	 * happen when memory pressure pushes dirty pages to the end of

	 * the LRU before the dirty limits are breached and the dirty

	 * data has expired. It can also happen when the proportion of

	 * dirty pages grows not through writes but through memory

	 * pressure reclaiming all the clean cache. And in some cases,

	 * the flushers simply cannot keep up with the allocation

	 * rate. Nudge the flusher threads in case they are asleep.

/*

 * shrink_active_list() moves pages from the active LRU to the inactive LRU.

 *

 * We move them the other way if the page is referenced by one or more

 * processes.

 *

 * If the pages are mostly unmapped, the processing is fast and it is

 * appropriate to hold lru_lock across the whole operation.  But if

 * the pages are mapped, the processing is slow (page_referenced()), so

 * we should drop lru_lock around each page.  It's impossible to balance

 * this, so instead we remove the pages from the LRU while processing them.

 * It is safe to rely on PG_active against the non-LRU pages in here because

 * nobody will play with that bit on a non-LRU page.

 *

 * The downside is that we have to touch page->_refcount against each page.

 * But we had to alter page->flags anyway.

 The pages which were snipped off */

			/*

			 * Identify referenced, file-backed active pages and

			 * give them one more trip around the active list. So

			 * that executable code get better chances to stay in

			 * memory under moderate memory pressure.  Anon pages

			 * are not likely to be evicted by use-once streaming

			 * IO, plus JVM can create lots of anon VM_EXEC pages,

			 * so we ignore them here.

 we are de-activating */

	/*

	 * Move pages back to the lru list.

 Keep all free pages in l_active list */

/*

 * The inactive anon list should be small enough that the VM never has

 * to do too much work.

 *

 * The inactive file list should be small enough to leave most memory

 * to the established workingset on the scan-resistant active list,

 * but large enough to avoid thrashing the aggregate readahead window.

 *

 * Both inactive lists should also be large enough that each inactive

 * page has a chance to be referenced again before it is reclaimed.

 *

 * If that fails and refaulting is observed, the inactive list grows.

 *

 * The inactive_ratio is the target ratio of ACTIVE to INACTIVE pages

 * on this LRU, maintained by the pageout code. An inactive_ratio

 * of 3 means 3:1 or 25% of the pages are kept on the inactive list.

 *

 * total     target    max

 * memory    ratio     inactive

 * -------------------------------------

 *   10MB       1         5MB

 *  100MB       1        50MB

 *    1GB       3       250MB

 *   10GB      10       0.9GB

 *  100GB      31         3GB

 *    1TB     101        10GB

 *   10TB     320        32GB

/*

 * Determine how aggressively the anon and file LRU lists should be

 * scanned.  The relative value of each set of LRU lists is determined

 * by looking at the fraction of the pages scanned we did rotate back

 * onto the active list instead of evict.

 *

 * nr[0] = anon inactive pages to scan; nr[1] = anon active pages to scan

 * nr[2] = file inactive pages to scan; nr[3] = file active pages to scan

 gcc */

 If we have no swap space, do not bother scanning anon pages. */

	/*

	 * Global reclaim will swap to prevent OOM even with no

	 * swappiness, but memcg users want to use this knob to

	 * disable swapping for individual groups completely when

	 * using the memory controller's swap limit feature would be

	 * too expensive.

	/*

	 * Do not apply any pressure balancing cleverness when the

	 * system is close to OOM, scan both anon and file equally

	 * (unless the swappiness setting disagrees with swapping).

	/*

	 * If the system is almost out of file pages, force-scan anon.

	/*

	 * If there is enough inactive page cache, we do not reclaim

	 * anything from the anonymous working right now.

	/*

	 * Calculate the pressure balance between anon and file pages.

	 *

	 * The amount of pressure we put on each LRU is inversely

	 * proportional to the cost of reclaiming each list, as

	 * determined by the share of pages that are refaulting, times

	 * the relative IO cost of bringing back a swapped out

	 * anonymous page vs reloading a filesystem page (swappiness).

	 *

	 * Although we limit that influence to ensure no list gets

	 * left behind completely: at least a third of the pressure is

	 * applied, before swappiness.

	 *

	 * With swappiness at 100, anon and file have equal IO cost.

			/*

			 * Scale a cgroup's reclaim pressure by proportioning

			 * its current usage to its memory.low or memory.min

			 * setting.

			 *

			 * This is important, as otherwise scanning aggression

			 * becomes extremely binary -- from nothing as we

			 * approach the memory protection threshold, to totally

			 * nominal as we exceed it.  This results in requiring

			 * setting extremely liberal protection thresholds. It

			 * also means we simply get no protection at all if we

			 * set it too low, which is not ideal.

			 *

			 * If there is any protection in place, we reduce scan

			 * pressure by how much of the total memory used is

			 * within protection thresholds.

			 *

			 * There is one special case: in the first reclaim pass,

			 * we skip over all groups that are within their low

			 * protection. If that fails to reclaim enough pages to

			 * satisfy the reclaim goal, we come back and override

			 * the best-effort low protection. However, we still

			 * ideally want to honor how well-behaved groups are in

			 * that case instead of simply punishing them all

			 * equally. As such, we reclaim them based on how much

			 * memory they are using, reducing the scan pressure

			 * again by how much of the total memory used is under

			 * hard protection.

 memory.low scaling, make sure we retry before OOM */

 Avoid TOCTOU with earlier protection check */

			/*

			 * Minimally target SWAP_CLUSTER_MAX pages to keep

			 * reclaim moving forwards, avoiding decrementing

			 * sc->priority further than desirable.

		/*

		 * If the cgroup's already been deleted, make sure to

		 * scrape out the remaining cache.

 Scan lists relative to size */

			/*

			 * Scan types proportional to swappiness and

			 * their relative recent reclaim efficiency.

			 * Make sure we don't miss the last page on

			 * the offlined memory cgroups because of a

			 * round-off error.

 Scan one type exclusively */

 Look ma, no brain */

/*

 * Anonymous LRU management is a waste if there is

 * ultimately no way to reclaim the memory.

 Aging the anon LRU is valuable if swap is present: */

 Also valuable if anon pages can be demoted: */

 Record the original scan target for proportional adjustments later */

	/*

	 * Global reclaiming within direct reclaim at DEF_PRIORITY is a normal

	 * event that can occur when there is little memory pressure e.g.

	 * multiple streaming readers/writers. Hence, we do not abort scanning

	 * when the requested number of pages are reclaimed when scanning at

	 * DEF_PRIORITY on the assumption that the fact we are direct

	 * reclaiming implies that kswapd is not keeping up and it is best to

	 * do a batch of work at once. For memcg reclaim one check is made to

	 * abort proportional reclaim if either the file or anon lru has already

	 * dropped to zero at the first pass.

		/*

		 * For kswapd and memcg, reclaim at least the number of pages

		 * requested. Ensure that the anon and file LRUs are scanned

		 * proportionally what was requested by get_scan_count(). We

		 * stop reclaiming one LRU and reduce the amount scanning

		 * proportional to the original scan target.

		/*

		 * It's just vindictive to attack the larger once the smaller

		 * has gone to zero.  And given the way we stop scanning the

		 * smaller below, this makes sure that we only make one nudge

		 * towards proportionality once we've got nr_to_reclaim.

 Stop scanning the smaller of the LRU */

		/*

		 * Recalculate the other LRU scan count based on its original

		 * scan target and the percentage scanning already complete

	/*

	 * Even if we did not try to evict anon pages at all, we want to

	 * rebalance the anon lru active/inactive ratio.

 Use reclaim/compaction for costly allocs or under memory pressure */

/*

 * Reclaim/compaction is used for high-order allocation requests. It reclaims

 * order-0 pages before compacting the zone. should_continue_reclaim() returns

 * true if more pages should be reclaimed such that when the page allocator

 * calls try_to_compact_pages() that it will have enough free pages to succeed.

 * It will give up earlier than that if there is difficulty reclaiming pages.

 If not in reclaim/compaction mode, stop */

	/*

	 * Stop if we failed to reclaim any pages from the last SWAP_CLUSTER_MAX

	 * number of pages that were scanned. This will return to the caller

	 * with the risk reclaim/compaction and the resulting allocation attempt

	 * fails. In the past we have tried harder for __GFP_RETRY_MAYFAIL

	 * allocations through requiring that the full LRU list has been scanned

	 * first, by assuming that zero delta of sc->nr_scanned means full LRU

	 * scan, but that approximation was wrong, and there were corner cases

	 * where always a non-zero amount of pages were scanned.

 If compaction would go ahead or the allocation would succeed, stop */

 check next zone */

	/*

	 * If we have not reclaimed enough pages for compaction and the

	 * inactive lists are large enough, continue reclaiming

		/*

		 * This loop can become CPU-bound when target memcgs

		 * aren't eligible for reclaim - either because they

		 * don't have any reclaimable pages, or because their

		 * memory is explicitly protected. Avoid soft lockups.

			/*

			 * Hard protection.

			 * If there is no reclaimable memory, OOM.

			/*

			 * Soft protection.

			 * Respect the protection only as long as

			 * there is an unprotected supply

			 * of reclaimable memory from other cgroups.

 Record the group's reclaim efficiency */

	/*

	 * Flush the memory cgroup stats, so that we read accurate per-memcg

	 * lruvec stats for heuristics.

	/*

	 * Determine the scan balance between anon and file LRUs.

	/*

	 * Target desirable inactive:active list ratios for the anon

	 * and file LRU lists.

		/*

		 * When refaults are being observed, it means a new

		 * workingset is being established. Deactivate to get

		 * rid of any stale active pages quickly.

	/*

	 * If we have plenty of inactive file pages that aren't

	 * thrashing, try to reclaim those first before touching

	 * anonymous pages.

	/*

	 * Prevent the reclaimer from falling into the cache trap: as

	 * cache pages start out inactive, every cache fault will tip

	 * the scan balance towards the file LRU.  And as the file LRU

	 * shrinks, so does the window for rotation from references.

	 * This means we have a runaway feedback loop where a tiny

	 * thrashing file LRU becomes infinitely more attractive than

	 * anon pages.  Try to detect this based on file LRU size.

		/*

		 * Consider anon: if that's low too, this isn't a

		 * runaway file reclaim problem, but rather just

		 * extreme pressure. Reclaim as per usual then.

 Record the subtree's reclaim efficiency */

		/*

		 * If reclaim is isolating dirty pages under writeback,

		 * it implies that the long-lived page allocation rate

		 * is exceeding the page laundering rate. Either the

		 * global limits are not being effective at throttling

		 * processes due to the page distribution throughout

		 * zones or there is heavy usage of a slow backing

		 * device. The only option is to throttle from reclaim

		 * context which is not ideal as there is no guarantee

		 * the dirtying process is throttled in the same way

		 * balance_dirty_pages() manages.

		 *

		 * Once a node is flagged PGDAT_WRITEBACK, kswapd will

		 * count the number of pages under pages flagged for

		 * immediate reclaim and stall if any are encountered

		 * in the nr_immediate check below.

 Allow kswapd to start writing pages during reclaim.*/

		/*

		 * If kswapd scans pages marked for immediate

		 * reclaim and under writeback (nr_immediate), it

		 * implies that pages are cycling through the LRU

		 * faster than they are written so forcibly stall

		 * until some pages complete writeback.

	/*

	 * Tag a node/memcg as congested if all the dirty pages were marked

	 * for writeback and immediate reclaim (counted in nr.congested).

	 *

	 * Legacy memcg will stall in page writeback so avoid forcibly

	 * stalling in reclaim_throttle().

	/*

	 * Stall direct reclaim for IO completions if the lruvec is

	 * node is congested. Allow kswapd to continue until it

	 * starts encountering unqueued dirty pages or cycling through

	 * the LRU too quickly.

	/*

	 * Kswapd gives up on balancing particular nodes after too

	 * many failures to reclaim anything from them and goes to

	 * sleep. On reclaim progress, reset the failure counter. A

	 * successful direct reclaim run will revive a dormant kswapd.

/*

 * Returns true if compaction should go ahead for a costly-order request, or

 * the allocation would already succeed without compaction. Return false if we

 * should reclaim first.

 Allocation should succeed already. Don't reclaim. */

 Compaction cannot yet proceed. Do reclaim. */

	/*

	 * Compaction is already possible, but it takes time to run and there

	 * are potentially other callers using the pages just freed. So proceed

	 * with reclaim to make a buffer of free pages available to give

	 * compaction a reasonable chance of completing and allocating the page.

	 * Note that we won't actually reclaim the whole buffer in one attempt

	 * as the target watermark in should_continue_reclaim() is lower. But if

	 * we are already above the high+gap watermark, don't reclaim at all.

	/*

	 * If reclaim is making progress greater than 12% efficiency then

	 * wake all the NOPROGRESS throttled tasks.

	/*

	 * Do not throttle kswapd on NOPROGRESS as it will throttle on

	 * VMSCAN_THROTTLE_WRITEBACK if there are too many pages under

	 * writeback and marked for immediate reclaim at the tail of

	 * the LRU.

 Throttle if making no progress at high prioities. */

/*

 * This is the direct reclaim path, for page-allocating processes.  We only

 * try to reclaim pages from zones which will satisfy the caller's allocation

 * request.

 *

 * If a zone is deemed to be full of pinned pages then just give it a light

 * scan then give up on it.

	/*

	 * If the number of buffer_heads in the machine exceeds the maximum

	 * allowed level, force direct reclaim to scan the highmem zone as

	 * highmem pages could be pinning lowmem pages storing buffer_heads

		/*

		 * Take care memory controller reclaiming has small influence

		 * to global LRU.

			/*

			 * If we already have plenty of memory free for

			 * compaction in this zone, don't free any more.

			 * Even though compaction is invoked for any

			 * non-zero order, only frequent costly order

			 * reclamation is disruptive enough to become a

			 * noticeable problem, like transparent huge

			 * page allocations.

			/*

			 * Shrink each node in the zonelist once. If the

			 * zonelist is ordered by zone (not the default) then a

			 * node may be shrunk multiple times but in that case

			 * the user prefers lower zones being preserved.

			/*

			 * This steals pages from memory cgroups over softlimit

			 * and returns the number of reclaimed pages and

			 * scanned pages. This works for global memory pressure

			 * and balancing, not for a memcg's limit.

 need some check for avoid more shrink_zone() */

 See comment about same check for global reclaim above */

	/*

	 * Restore to original mask to avoid the impact on the caller if we

	 * promoted it to __GFP_HIGHMEM.

/*

 * This is the main entry point to direct page reclaim.

 *

 * If a full scan of the inactive list fails to free enough memory then we

 * are "out of memory" and something needs to be killed.

 *

 * If the caller is !__GFP_FS then the probability of a failure is reasonably

 * high - the zone may be full of dirty or under-writeback pages, which this

 * caller can't do much about.  We kick the writeback threads and take explicit

 * naps in the hope that some of these pages can be written.  But if the

 * allocating task holds filesystem locks which prevent writeout this might not

 * work, and the allocation attempt will fail.

 *

 * returns:	0, if no pages reclaimed

 * 		else, the number of pages reclaimed

		/*

		 * If we're getting trouble reclaiming, start doing

		 * writepage even in laptop mode.

 Aborted reclaim to try compaction? don't OOM, then */

	/*

	 * We make inactive:active ratio decisions based on the node's

	 * composition of memory, but a restrictive reclaim_idx or a

	 * memory.low cgroup setting can exempt large amounts of

	 * memory from reclaim. Neither of which are very common, so

	 * instead of doing costly eligibility calculations of the

	 * entire cgroup subtree up front, we assume the estimates are

	 * good, and retry with forcible deactivation if that fails.

 Untapped cgroup reserves?  Don't OOM, retry. */

 If there are no reserves (unexpected config) then do not throttle */

 kswapd must be awake if processes are being throttled */

/*

 * Throttle direct reclaimers if backing storage is backed by the network

 * and the PFMEMALLOC reserve for the preferred node is getting dangerously

 * depleted. kswapd will continue to make progress and wake the processes

 * when the low watermark is reached.

 *

 * Returns true if a fatal signal was delivered during throttling. If this

 * happens, the page allocator should not consider triggering the OOM killer.

	/*

	 * Kernel threads should not be throttled as they may be indirectly

	 * responsible for cleaning pages necessary for reclaim to make forward

	 * progress. kjournald for example may enter direct reclaim while

	 * committing a transaction where throttling it could forcing other

	 * processes to block on log_wait_commit().

	/*

	 * If a fatal signal is pending, this process should not throttle.

	 * It should return quickly so it can exit and free its memory

	/*

	 * Check if the pfmemalloc reserves are ok by finding the first node

	 * with a usable ZONE_NORMAL or lower zone. The expectation is that

	 * GFP_KERNEL will be required for allocating network buffers when

	 * swapping over the network so ZONE_HIGHMEM is unusable.

	 *

	 * Throttling is based on the first usable node and throttled processes

	 * wait on a queue until kswapd makes progress and wakes them. There

	 * is an affinity then between processes waking up and where reclaim

	 * progress has been made assuming the process wakes on the same node.

	 * More importantly, processes running on remote nodes will not compete

	 * for remote pfmemalloc reserves and processes on different nodes

	 * should make reasonable progress.

 Throttle based on the first usable node */

 If no zone was usable by the allocation flags then do not throttle */

 Account for the throttling */

	/*

	 * If the caller cannot enter the filesystem, it's possible that it

	 * is due to the caller holding an FS lock or performing a journal

	 * transaction in the case of a filesystem like ext[3|4]. In this case,

	 * it is not safe to block on pfmemalloc_wait as kswapd could be

	 * blocked waiting on the same lock. Instead, throttle for up to a

	 * second before continuing.

 Throttle until kswapd wakes the process */

	/*

	 * scan_control uses s8 fields for order, priority, and reclaim_idx.

	 * Confirm they are large enough for max values.

	/*

	 * Do not enter reclaim if fatal signal was delivered while throttled.

	 * 1 is returned so that the page allocator does not OOM kill at this

	 * point.

 Only used by soft limit reclaim. Do not reuse for anything else. */

	/*

	 * NOTE: Although we can get the priority field, using it

	 * here is not a good idea, since it limits the pages we can scan.

	 * if we don't reclaim here, the shrink_node from balance_pgdat

	 * will pick up pages from other mem cgroup's as well. We hack

	 * the priority and make it zero.

	/*

	 * Traverse the ZONELIST_FALLBACK zonelist of the current node to put

	 * equal pressure on all the nodes. This is based on the assumption that

	 * the reclaim does not bail out early.

	/*

	 * Check for watermark boosts top-down as the higher zones

	 * are more likely to be boosted. Both watermarks and boosts

	 * should not be checked at the same time as reclaim would

	 * start prematurely when there is no boosting and a lower

	 * zone is balanced.

/*

 * Returns true if there is an eligible zone balanced for the request order

 * and highest_zoneidx

	/*

	 * Check watermarks bottom-up as lower zones are more likely to

	 * meet watermarks.

	/*

	 * If a node has no populated zone within highest_zoneidx, it does not

	 * need balancing by definition. This can happen if a zone-restricted

	 * allocation tries to wake a remote kswapd.

 Clear pgdat state for congested, dirty or under writeback. */

/*

 * Prepare kswapd for sleeping. This verifies that there are no processes

 * waiting in throttle_direct_reclaim() and that watermarks have been met.

 *

 * Returns true if kswapd is ready to sleep

	/*

	 * The throttled processes are normally woken up in balance_pgdat() as

	 * soon as allow_direct_reclaim() is true. But there is a potential

	 * race between when kswapd checks the watermarks and a process gets

	 * throttled. There is also a potential race if processes get

	 * throttled, kswapd wakes, a large process exits thereby balancing the

	 * zones, which causes kswapd to exit balance_pgdat() before reaching

	 * the wake up checks. If kswapd is going to sleep, no process should

	 * be sleeping on pfmemalloc_wait, so wake them now if necessary. If

	 * the wake up is premature, processes will wake kswapd and get

	 * throttled again. The difference from wake ups in balance_pgdat() is

	 * that here we are under prepare_to_wait().

 Hopeless node, leave it to direct reclaim */

/*

 * kswapd shrinks a node of pages that are at or below the highest usable

 * zone that is currently unbalanced.

 *

 * Returns true if kswapd scanned at least the requested number of pages to

 * reclaim or if the lack of progress was due to pages under writeback.

 * This is used to determine if the scanning priority needs to be raised.

 Reclaim a number of pages proportional to the number of zones */

	/*

	 * Historically care was taken to put equal pressure on all zones but

	 * now pressure is applied based on node LRU order.

	/*

	 * Fragmentation may mean that the system cannot be rebalanced for

	 * high-order allocations. If twice the allocation size has been

	 * reclaimed then recheck watermarks only at order-0 to prevent

	 * excessive reclaim. Assume that a process requested a high-order

	 * can direct reclaim/compact.

 Page allocator PCP high watermark is lowered if reclaim is active. */

/*

 * For kswapd, balance_pgdat() will reclaim pages across a node from zones

 * that are eligible for use by the caller until at least one zone is

 * balanced.

 *

 * Returns the order kswapd finished reclaiming at.

 *

 * kswapd scans the zones in the highmem->normal->dma direction.  It skips

 * zones which have free_pages > high_wmark_pages(zone), but once a zone is

 * found to have free_pages <= high_wmark_pages(zone), any page in that zone

 * or lower is eligible for reclaim until at least one usable zone is

 * balanced.

	/*

	 * Account for the reclaim boost. Note that the zone boost is left in

	 * place so that parallel allocations that are near the watermark will

	 * stall or direct reclaim until kswapd is finished.

		/*

		 * If the number of buffer_heads exceeds the maximum allowed

		 * then consider reclaiming from all zones. This has a dual

		 * purpose -- on 64-bit systems it is expected that

		 * buffer_heads are stripped during active rotation. On 32-bit

		 * systems, highmem pages can pin lowmem memory and shrinking

		 * buffers can relieve lowmem pressure. Reclaim may still not

		 * go ahead if all eligible zones for the original allocation

		 * request are balanced to avoid excessive reclaim from kswapd.

		/*

		 * If the pgdat is imbalanced then ignore boosting and preserve

		 * the watermarks for a later time and restart. Note that the

		 * zone watermarks will be still reset at the end of balancing

		 * on the grounds that the normal reclaim should be enough to

		 * re-evaluate if boosting is required when kswapd next wakes.

		/*

		 * If boosting is not active then only reclaim if there are no

		 * eligible zones. Note that sc.reclaim_idx is not used as

		 * buffer_heads_over_limit may have adjusted it.

 Limit the priority of boosting to avoid reclaim writeback */

		/*

		 * Do not writeback or swap pages for boosted reclaim. The

		 * intent is to relieve pressure not issue sub-optimal IO

		 * from reclaim context. If no pages are reclaimed, the

		 * reclaim will be aborted.

		/*

		 * Do some background aging of the anon list, to give

		 * pages a chance to be referenced before reclaiming. All

		 * pages are rotated regardless of classzone as this is

		 * about consistent aging.

		/*

		 * If we're getting trouble reclaiming, start doing writepage

		 * even in laptop mode.

 Call soft limit reclaim before calling shrink_node. */

		/*

		 * There should be no need to raise the scanning priority if

		 * enough pages are already being scanned that that high

		 * watermark would be met at 100% efficiency.

		/*

		 * If the low watermark is met there is no need for processes

		 * to be throttled on pfmemalloc_wait as they should not be

		 * able to safely make forward progress. Wake them

 Check if kswapd should be suspending */

		/*

		 * Raise priority if scanning rate is too low or there was no

		 * progress in reclaiming pages

		/*

		 * If reclaim made no progress for a boost, stop reclaim as

		 * IO cannot be queued and it could be an infinite loop in

		 * extreme circumstances.

 If reclaim was boosted, account for the reclaim done in this pass */

 Increments are under the zone lock */

		/*

		 * As there is now likely space, wakeup kcompact to defragment

		 * pageblocks.

	/*

	 * Return the order kswapd stopped reclaiming at as

	 * prepare_kswapd_sleep() takes it into account. If another caller

	 * entered the allocator slow path while kswapd was awake, order will

	 * remain at the higher level.

/*

 * The pgdat->kswapd_highest_zoneidx is used to pass the highest zone index to

 * be reclaimed by kswapd from the waker. If the value is MAX_NR_ZONES which is

 * not a valid index then either kswapd runs for first time or kswapd couldn't

 * sleep after previous reclaim attempt (node is still unbalanced). In that

 * case return the zone index of the previous kswapd reclaim cycle.

	/*

	 * Try to sleep for a short interval. Note that kcompactd will only be

	 * woken if it is possible to sleep for a short interval. This is

	 * deliberate on the assumption that if reclaim cannot keep an

	 * eligible zone balanced that it's also unlikely that compaction will

	 * succeed.

		/*

		 * Compaction records what page blocks it recently failed to

		 * isolate pages from and skips them in the future scanning.

		 * When kswapd is going to sleep, it is reasonable to assume

		 * that pages and compaction may succeed so reset the cache.

		/*

		 * We have freed the memory, now we should compact it to make

		 * allocation of the requested order possible.

		/*

		 * If woken prematurely then reset kswapd_highest_zoneidx and

		 * order. The values will either be from a wakeup request or

		 * the previous request that slept prematurely.

	/*

	 * After a short sleep, check if it was a premature sleep. If not, then

	 * go fully to sleep until explicitly woken up.

		/*

		 * vmstat counters are not perfectly accurate and the estimated

		 * value for counters such as NR_FREE_PAGES can deviate from the

		 * true value by nr_online_cpus * threshold. To avoid the zone

		 * watermarks being breached while under pressure, we reduce the

		 * per-cpu vmstat threshold while kswapd is awake and restore

		 * them before going back to sleep.

/*

 * The background pageout daemon, started as a kernel thread

 * from the init process.

 *

 * This basically trickles out pages so that we have _some_

 * free memory available even if there is no other activity

 * that frees anything up. This is needed for things like routing

 * etc, where we otherwise might have all activity going on in

 * asynchronous contexts that cannot page things out.

 *

 * If there are applications that are active memory-allocators

 * (most normal use), this basically shouldn't matter.

	/*

	 * Tell the memory management that we're a "memory allocator",

	 * and that if we need more memory we should get access to it

	 * regardless (see "__alloc_pages()"). "kswapd" should

	 * never get caught in the normal page freeing logic.

	 *

	 * (Kswapd normally doesn't need memory anyway, but sometimes

	 * you need a small amount of memory in order to be able to

	 * page out something else, and this flag essentially protects

	 * us from recursively trying to free more memory as we're

	 * trying to free the first piece of memory in the first place).

 Read the new order and highest_zoneidx */

		/*

		 * We can speed up thawing tasks if we don't call balance_pgdat

		 * after returning from the refrigerator

		/*

		 * Reclaim begins at the requested order but if a high-order

		 * reclaim fails then kswapd falls back to reclaiming for

		 * order-0. If that happens, kswapd will consider sleeping

		 * for the order it finished reclaiming at (reclaim_order)

		 * but kcompactd is woken to compact for the original

		 * request (alloc_order).

/*

 * A zone is low on free memory or too fragmented for high-order memory.  If

 * kswapd should reclaim (direct reclaim is deferred), wake it up for the zone's

 * pgdat.  It will wake up kcompactd after reclaiming memory.  If kswapd reclaim

 * has failed or is not needed, still wake up kcompactd if only compaction is

 * needed.

 Hopeless node, leave it to direct reclaim if possible */

		/*

		 * There may be plenty of free memory available, but it's too

		 * fragmented for high-order allocations.  Wake up kcompactd

		 * and rely on compaction_suitable() to determine if it's

		 * needed.  If it fails, it will defer subsequent attempts to

		 * ratelimit its work.

/*

 * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of

 * freed pages.

 *

 * Rather than trying to age LRUs the aim is to preserve the overall

 * LRU order by reclaiming preferentially

 * inactive > active > active referenced > active mapped

 CONFIG_HIBERNATION */

/*

 * This kswapd start function will be called by init and node-hot-add.

 * On node-hot-add, kswapd will moved to proper cpus if cpus are hot-added.

 failure at boot is fatal */

/*

 * Called by memory hotplug when all memory in a node is offlined.  Caller must

 * hold mem_hotplug_begin/end().

/*

 * Node reclaim mode

 *

 * If non-zero call node_reclaim when the number of free pages falls below

 * the watermarks.

/*

 * Priority for NODE_RECLAIM. This determines the fraction of pages

 * of a node considered for each zone_reclaim. 4 scans 1/16th of

 * a zone.

/*

 * Percentage of pages in a zone that must be unmapped for node_reclaim to

 * occur.

/*

 * If the number of slab pages in a zone grows beyond this percentage then

 * slab reclaim needs to occur.

	/*

	 * It's possible for there to be more file mapped pages than

	 * accounted for by the pages on the file LRU lists because

	 * tmpfs pages accounted for as ANON can also be FILE_MAPPED

 Work out how many page cache pages we can reclaim in this reclaim_mode */

	/*

	 * If RECLAIM_UNMAP is set, then all file pages are considered

	 * potentially reclaimable. Otherwise, we have to worry about

	 * pages like swapcache and node_unmapped_file_pages() provides

	 * a better estimate

 If we can't clean pages, remove dirty pages from consideration */

 Watch for any possible underflows due to delta */

/*

 * Try to free up some pages from this node through reclaim.

 Minimum pages needed in order to stay on node */

	/*

	 * We need to be able to allocate from the reserves for RECLAIM_UNMAP

	 * and we also need to be able to write out pages for RECLAIM_WRITE

	 * and RECLAIM_UNMAP.

		/*

		 * Free memory by calling shrink node with increasing

		 * priorities until we have enough memory freed.

	/*

	 * Node reclaim reclaims unmapped file backed pages and

	 * slab pages if we are over the defined limits.

	 *

	 * A small portion of unmapped file backed pages is needed for

	 * file I/O otherwise pages read by file I/O will be immediately

	 * thrown out if the node is overallocated. So we do not reclaim

	 * if less than a specified percentage of the node is used by

	 * unmapped file backed pages.

	/*

	 * Do not scan if the allocation should not be delayed.

	/*

	 * Only run node reclaim on the local node or on nodes that do not

	 * have associated processors. This will favor the local processor

	 * over remote processors and spread off node memory allocations

	 * as wide as possible.

/**

 * check_move_unevictable_pages - check pages for evictability and move to

 * appropriate zone lru list

 * @pvec: pagevec with lru pages to check

 *

 * Checks pages for evictability, if an evictable page is in the unevictable

 * lru list, moves it to the appropriate evictable lru list. This function

 * should be only used for lru pages.

 block memcg migration during page moving between lru */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/mm/mmzone.c

 *

 * management codes for pgdats, zones and page flags

/*

 * next_zone - helper magic for for_each_zone()

 CONFIG_NUMA */

 Returns the next zone at or below highest_zoneidx in a zonelist */

	/*

	 * Find the next suitable zone to use for the allocation.

	 * Only filter based on nodemask if it's set

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 1993  Linus Torvalds

 *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999

 *  SMP-safe vmalloc/vfree/ioremap, Tigran Aivazian <tigran@veritas.com>, May 2000

 *  Major rework to support vmap/vunmap, Christoph Hellwig, SGI, August 2002

 *  Numa awareness, Christoph Lameter, SGI, June 2005

 *  Improving global KVA allocator, Uladzislau Rezki, Sony, May 2019

 CONFIG_HAVE_ARCH_HUGE_VMAP */

 CONFIG_HAVE_ARCH_HUGE_VMAP */

 CONFIG_HAVE_ARCH_HUGE_VMALLOC */

 CONFIG_HAVE_ARCH_HUGE_VMALLOC */

** Page table manipulation functions ***/

/*

 * vunmap_range_noflush is similar to vunmap_range, but does not

 * flush caches or TLBs.

 *

 * The caller is responsible for calling flush_cache_vmap() before calling

 * this function, and flush_tlb_kernel_range after it has returned

 * successfully (and before the addresses are expected to cause a page fault

 * or be re-mapped for something else, if TLB flushes are being delayed or

 * coalesced).

 *

 * This is an internal function only. Do not use outside mm/.

/**

 * vunmap_range - unmap kernel virtual addresses

 * @addr: start of the VM area to unmap

 * @end: end of the VM area to unmap (non-inclusive)

 *

 * Clears any present PTEs in the virtual address range, flushes TLBs and

 * caches. Any subsequent access to the address before it has been re-mapped

 * is a kernel bug.

	/*

	 * nr is a running index into the array which helps higher level

	 * callers keep track of where we're up to.

/*

 * vmap_pages_range_noflush is similar to vmap_pages_range, but does not

 * flush caches.

 *

 * The caller is responsible for calling flush_cache_vmap() after this

 * function returns successfully and before the addresses are accessed.

 *

 * This is an internal function only. Do not use outside mm/.

/**

 * vmap_pages_range - map pages to a kernel virtual address

 * @addr: start of the VM area to map

 * @end: end of the VM area to map (non-inclusive)

 * @prot: page protection flags to use

 * @pages: pages to map (always PAGE_SIZE pages)

 * @page_shift: maximum shift that the pages may be mapped with, @pages must

 * be aligned and contiguous up to at least this shift.

 *

 * RETURNS:

 * 0 on success, -errno on failure.

	/*

	 * ARM, x86-64 and sparc64 put modules in a special place,

	 * and fall back on vmalloc() if that fails. Others

	 * just put it in the vmalloc space.

/*

 * Walk a vmap address to the struct page it maps. Huge vmap mappings will

 * return the tail page that corresponds to the base page address, which

 * matches small vmap mappings.

	/*

	 * XXX we might need to change this if we add VIRTUAL_BUG_ON for

	 * architectures that do not vmalloc module space

 XXX: no allowance for huge pgd */

/*

 * Map a vmalloc()-space virtual address to the physical page frame number.

** Global kva allocator ***/

 Export for kexec only */

/*

 * This kmem_cache is used for vmap_area objects. Instead of

 * allocating from slab we reuse an object from this cache to

 * make things faster. Especially in "no edge" splitting of

 * free block.

/*

 * This linked list is used in pair with free_vmap_area_root.

 * It gives O(1) access to prev/next to perform fast coalescing.

/*

 * This augment red-black tree represents the free vmap space.

 * All vmap_area objects in this tree are sorted by va->va_start

 * address. It is used for allocation and merging when a vmap

 * object is released.

 *

 * Each vmap_area node contains a maximum available free block

 * of its sub-tree, right or left. Therefore it is possible to

 * find a lowest match of free area.

/*

 * Preload a CPU with one object for "no edge" split case. The

 * aim is to get rid of allocations from the atomic context, thus

 * to use more permissive allocation masks.

/*

 * Gets called when remove the node and rotate.

/*

 * This function returns back addresses of parent node

 * and its left or right link for further processing.

 *

 * Otherwise NULL is returned. In that case all further

 * steps regarding inserting of conflicting overlap range

 * have to be declined and actually considered as a bug.

	/*

	 * Go to the bottom of the tree. When we hit the last point

	 * we end up with parent rb_node and correct direction, i name

	 * it link, where the new va->rb_node will be attached to.

		/*

		 * During the traversal we also do some sanity check.

		 * Trigger the BUG() if there are sides(left/right)

		 * or full overlaps.

		/*

		 * The red-black tree where we try to find VA neighbors

		 * before merging or inserting is empty, i.e. it means

		 * there is no free vmap space. Normally it does not

		 * happen but we handle this case anyway.

	/*

	 * VA is still not in the list, but we can

	 * identify its future previous list_head node.

 Insert to the rb-tree */

		/*

		 * Some explanation here. Just perform simple insertion

		 * to the tree. We do not set va->subtree_max_size to

		 * its current size before calling rb_insert_augmented().

		 * It is because of we populate the tree from the bottom

		 * to parent levels when the node _is_ in the tree.

		 *

		 * Therefore we set subtree_max_size to zero after insertion,

		 * to let __augment_tree_propagate_from() puts everything to

		 * the correct order later on.

 Address-sort this list */

/*

 * This function populates subtree_max_size from bottom to upper

 * levels starting from VA point. The propagation must be done

 * when VA size is modified by changing its va_start/va_end. Or

 * in case of newly inserting of VA to the tree.

 *

 * It means that __augment_tree_propagate_from() must be called:

 * - After VA has been inserted to the tree(free path);

 * - After VA has been shrunk(allocation path);

 * - After VA has been increased(merging path).

 *

 * Please note that, it does not mean that upper parent nodes

 * and their subtree_max_size are recalculated all the time up

 * to the root node.

 *

 *       4--8

 *        /\

 *       /  \

 *      /    \

 *    2--2  8--8

 *

 * For example if we modify the node 4, shrinking it to 2, then

 * no any modification is required. If we shrink the node 2 to 1

 * its subtree_max_size is updated only, and set to 1. If we shrink

 * the node 8 to 6, then its subtree_max_size is set to 6 and parent

 * node becomes 4--6.

	/*

	 * Populate the tree from bottom towards the root until

	 * the calculated maximum available size of checked node

	 * is equal to its current one.

/*

 * Merge de-allocated chunk of VA memory with previous

 * and next free blocks. If coalesce is not done a new

 * free area is inserted. If VA has been merged, it is

 * freed.

 *

 * Please note, it can return NULL in case of overlap

 * ranges, followed by WARN() report. Despite it is a

 * buggy behaviour, a system can be alive and keep

 * ongoing.

	/*

	 * Find a place in the tree where VA potentially will be

	 * inserted, unless it is merged with its sibling/siblings.

	/*

	 * Get next node of VA to check if merging can be done.

	/*

	 * start            end

	 * |                |

	 * |<------VA------>|<-----Next----->|

	 *                  |                |

	 *                  start            end

 Free vmap_area object. */

 Point to the new merged area. */

	/*

	 * start            end

	 * |                |

	 * |<-----Prev----->|<------VA------>|

	 *                  |                |

	 *                  start            end

			/*

			 * If both neighbors are coalesced, it is important

			 * to unlink the "next" node first, followed by merging

			 * with "previous" one. Otherwise the tree might not be

			 * fully populated if a sibling's augmented value is

			 * "normalized" because of rotation operations.

 Free vmap_area object. */

 Point to the new merged area. */

 Can be overflowed due to big size or alignment. */

/*

 * Find the first free block(lowest start address) in the tree,

 * that will accomplish the request corresponding to passing

 * parameters.

 Start from the root. */

			/*

			 * Does not make sense to go deeper towards the right

			 * sub-tree if it does not have a free block that is

			 * equal or bigger to the requested search size.

			/*

			 * OK. We roll back and find the first right sub-tree,

			 * that will satisfy the search criteria. It can happen

			 * due to "vstart" restriction or an alignment overhead

			 * that is bigger then PAGE_SIZE.

					/*

					 * Shift the vstart forward. Please note, we update it with

					 * parent's start address adding "1" because we do not want

					 * to enter same sub-tree after it has already been checked

					 * and no suitable free block found there.

 full fit */

 left edge fit */

 right edge fit */

 no edge fit */

 Check if it is within VA. */

 Now classify. */

		/*

		 * No need to split VA, it fully fits.

		 *

		 * |               |

		 * V      NVA      V

		 * |---------------|

		/*

		 * Split left edge of fit VA.

		 *

		 * |       |

		 * V  NVA  V   R

		 * |-------|-------|

		/*

		 * Split right edge of fit VA.

		 *

		 *         |       |

		 *     L   V  NVA  V

		 * |-------|-------|

		/*

		 * Split no edge of fit VA.

		 *

		 *     |       |

		 *   L V  NVA  V R

		 * |---|-------|---|

			/*

			 * For percpu allocator we do not do any pre-allocation

			 * and leave it as it is. The reason is it most likely

			 * never ends up with NE_FIT_TYPE splitting. In case of

			 * percpu allocations offsets and sizes are aligned to

			 * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE

			 * are its main fitting cases.

			 *

			 * There are a few exceptions though, as an example it is

			 * a first allocation (early boot up) when we have "one"

			 * big free space that has to be split.

			 *

			 * Also we can hit this path in case of regular "vmap"

			 * allocations, if "this" current CPU was not preloaded.

			 * See the comment in alloc_vmap_area() why. If so, then

			 * GFP_NOWAIT is used instead to get an extra object for

			 * split purpose. That is rare and most time does not

			 * occur.

			 *

			 * What happens if an allocation gets failed. Basically,

			 * an "overflow" path is triggered to purge lazily freed

			 * areas to free some memory, then, the "retry" path is

			 * triggered to repeat one more time. See more details

			 * in alloc_vmap_area() function.

		/*

		 * Build the remainder.

		/*

		 * Shrink this VA to remaining size.

 type == NE_FIT_TYPE */

/*

 * Returns a start address of the newly allocated area, if success.

 * Otherwise a vend is returned that indicates failure.

 Check the "vend" restriction. */

 Classify what we have found. */

 Update the free vmap_area. */

/*

 * Free a region of KVA allocated by alloc_vmap_area

	/*

	 * Remove from the busy tree/list.

	/*

	 * Insert/Merge it back to the free tree/list.

	/*

	 * Preload this CPU with one extra vmap_area object. It is used

	 * when fit type of free area is NE_FIT_TYPE. It guarantees that

	 * a CPU that does an allocation is preloaded.

	 *

	 * We do it in non-atomic context, thus it allows us to use more

	 * permissive allocation masks to be more stable under low memory

	 * condition and high memory pressure.

/*

 * Allocate a region of KVA of the specified size and alignment, within the

 * vstart and vend.

	/*

	 * Only scan the relevant parts containing pointers to other objects

	 * to avoid false negatives.

	/*

	 * If an allocation fails, the "vend" address is

	 * returned. Therefore trigger the overflow path.

/*

 * lazy_max_pages is the maximum amount of virtual address space we gather up

 * before attempting to purge with a TLB flush.

 *

 * There is a tradeoff here: a larger number will cover more kernel page tables

 * and take slightly longer to purge, but it will linearly reduce the number of

 * global TLB flushes that must be performed. It would seem natural to scale

 * this number up linearly with the number of CPUs (because vmapping activity

 * could also scale linearly with the number of CPUs), however it is likely

 * that in practice, workloads might be constrained in other ways that mean

 * vmap activity will not scale linearly with CPUs. Also, I want to be

 * conservative and not introduce a big latency on huge systems, so go with

 * a less aggressive log scale. It will still be an improvement over the old

 * code, and it will be simple to change the scale factor if we find that it

 * becomes a problem on bigger systems.

/*

 * Serialize vmap purging.  There is no actual critical section protected

 * by this look, but we want to avoid concurrent calls for performance

 * reasons and to make the pcpu_get_vm_areas more deterministic.

 for per-CPU blocks */

/*

 * called before a call to iounmap() if the caller wants vm_area_struct's

 * immediately freed.

 CONFIG_X86_64 */

/*

 * Purges all lazily-freed vmap areas.

		/*

		 * Finally insert or merge lazily-freed area. It is

		 * detached and there is no need to "unlink" it from

		 * anything.

/*

 * Kick off a purge of the outstanding lazy areas. Don't bother if somebody

 * is already purging.

/*

 * Kick off a purge of the outstanding lazy areas.

/*

 * Free a vmap area, caller ensuring that the area has been unmapped

 * and flush_cache_vunmap had been called for the correct range

 * previously.

	/*

	 * Merge or place it to the purge tree/list.

 After this point, we may free va at any time */

/*

 * Free and unmap a vmap area

** Per cpu kva allocator ***/

/*

 * vmap space is limited especially on 32 bit architectures. Ensure there is

 * room for at least 16 percpu vmap blocks per CPU.

/*

 * If we had a constant VMALLOC_START and VMALLOC_END, we'd like to be able

 * to #define VMALLOC_SPACE		(VMALLOC_END-VMALLOC_START). Guess

 * instead (we just need a rough idea)

 256K with 4K pages */

 4MB with 4K pages */

 can't use min() */

 can't use max() */

< dirty range */

 Queue of free and dirty vmap blocks, for allocation and flushing purposes */

/*

 * XArray of vmap blocks, indexed by address, to quickly find a vmap block

 * in the free path. Could get rid of this if we change the API to return a

 * "cookie" from alloc, to be passed to free. But no big deal yet.

/*

 * We should probably have a fallback mechanism to allocate virtual memory

 * out of partially filled vmap blocks. However vmap block sizing should be

 * fairly reasonable according to the vmalloc size, so it shouldn't be a

 * big problem.

/**

 * new_vmap_block - allocates new vmap_block and occupies 2^order pages in this

 *                  block. Of course pages number can't exceed VMAP_BBMAP_BITS

 * @order:    how many 2^order pages should be occupied in newly allocated block

 * @gfp_mask: flags for the page level allocator

 *

 * Return: virtual address in a newly allocated block or ERR_PTR(-errno)

 At least something should be left free */

 prevent further allocs after releasing lock */

 prevent purging it again */

		/*

		 * Allocating 0 bytes isn't what caller wants since

		 * get_order(0) returns funny result. Just warn and terminate

		 * early.

 Allocate new block if nothing was found */

 Expand dirty range */

/**

 * vm_unmap_aliases - unmap outstanding lazy aliases in the vmap layer

 *

 * The vmap/vmalloc layer lazily flushes kernel virtual mappings primarily

 * to amortize TLB flushing overheads. What this means is that any page you

 * have now, may, in a former life, have been mapped into kernel virtual

 * address by the vmap layer and so there might be some CPUs with TLB entries

 * still referencing that page (additional to the regular 1:1 kernel mapping).

 *

 * vm_unmap_aliases flushes all such lazy mappings. After it returns, we can

 * be sure that none of the pages we have control over will have any aliases

 * from the vmap layer.

/**

 * vm_unmap_ram - unmap linear kernel address space set up by vm_map_ram

 * @mem: the pointer returned by vm_map_ram

 * @count: the count passed to that vm_map_ram call (cannot unmap partial)

/**

 * vm_map_ram - map pages linearly into kernel virtual address (vmalloc space)

 * @pages: an array of pointers to the pages to be mapped

 * @count: number of pages

 * @node: prefer to allocate data structures on this node

 *

 * If you use this function for less than VMAP_MAX_ALLOC pages, it could be

 * faster than vmap so it's good.  But if you mix long-life and short-life

 * objects with vm_map_ram(), it could consume lots of address space through

 * fragmentation (especially on a 32bit machine).  You could see failures in

 * the end.  Please use this function for short-lived objects.

 *

 * Returns: a pointer to the address that has been mapped, or %NULL on failure

/**

 * vm_area_add_early - add vmap area early during boot

 * @vm: vm_struct to add

 *

 * This function is used to add fixed kernel vm area to vmlist before

 * vmalloc_init() is called.  @vm->addr, @vm->size, and @vm->flags

 * should contain proper values and the other fields should be zero.

 *

 * DO NOT USE THIS FUNCTION UNLESS YOU KNOW WHAT YOU'RE DOING.

/**

 * vm_area_register_early - register vmap area early during boot

 * @vm: vm_struct to register

 * @align: requested alignment

 *

 * This function is used to register kernel vm area before

 * vmalloc_init() is called.  @vm->size and @vm->flags should contain

 * proper values on entry and other fields should be zero.  On return,

 * vm->addr contains the allocated address.

 *

 * DO NOT USE THIS FUNCTION UNLESS YOU KNOW WHAT YOU'RE DOING.

	/*

	 *     B     F     B     B     B     F

	 * -|-----|.....|-----|-----|-----|.....|-

	 *  |           The KVA space           |

	 *  |<--------------------------------->|

	/*

	 * Create the cache for vmap_area objects.

 Import existing vmlist entries. */

	/*

	 * Now we can initialize a free vmap space.

	/*

	 * Before removing VM_UNINITIALIZED,

	 * we should make sure that vm has proper values.

	 * Pair with smp_rmb() in show_numa_info().

/**

 * get_vm_area - reserve a contiguous kernel virtual area

 * @size:	 size of the area

 * @flags:	 %VM_IOREMAP for I/O mappings or VM_ALLOC

 *

 * Search an area of @size in the kernel virtual mapping area,

 * and reserved it for out purposes.  Returns the area descriptor

 * on success or %NULL on failure.

 *

 * Return: the area descriptor on success or %NULL on failure.

/**

 * find_vm_area - find a continuous kernel virtual area

 * @addr:	  base address

 *

 * Search for the kernel VM area starting at @addr, and return it.

 * It is up to the caller to do all required locking to keep the returned

 * pointer valid.

 *

 * Return: the area descriptor on success or %NULL on failure.

/**

 * remove_vm_area - find and remove a continuous kernel virtual area

 * @addr:	    base address

 *

 * Search for the kernel VM area starting at @addr, and remove it.

 * This function returns the found VM area, but using it is NOT safe

 * on SMP machines, except for its size or flags.

 *

 * Return: the area descriptor on success or %NULL on failure.

 HUGE_VMALLOC passes small pages to set_direct_map */

 Handle removing and resetting vm mappings related to the vm_struct. */

 If this is not VM_FLUSH_RESET_PERMS memory, no need for the below. */

	/*

	 * If not deallocating pages, just do the flush of the VM area and

	 * return.

	/*

	 * If execution gets here, flush the vm mapping and reset the direct

	 * map. Find the start and end range of the direct mappings to make sure

	 * the vm_unmap_aliases() flush includes the direct map.

	/*

	 * Set direct map to something invalid so that it won't be cached if

	 * there are any accesses after the TLB flush, then flush the TLB and

	 * reset the direct map permissions to the default.

	/*

	 * Use raw_cpu_ptr() because this can be called from preemptible

	 * context. Preemption is absolutely fine here, because the llist_add()

	 * implementation is lockless, so it works even if we are adding to

	 * another cpu's list. schedule_work() should be fine with this too.

/**

 * vfree_atomic - release memory allocated by vmalloc()

 * @addr:	  memory base address

 *

 * This one is just like vfree() but can be called in any atomic context

 * except NMIs.

/**

 * vfree - Release memory allocated by vmalloc()

 * @addr:  Memory base address

 *

 * Free the virtually continuous memory area starting at @addr, as obtained

 * from one of the vmalloc() family of APIs.  This will usually also free the

 * physical memory underlying the virtual allocation, but that memory is

 * reference counted, so it will not be freed until the last user goes away.

 *

 * If @addr is NULL, no operation is performed.

 *

 * Context:

 * May sleep if called *not* from interrupt context.

 * Must not be called in NMI context (strictly speaking, it could be

 * if we have CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG, but making the calling

 * conventions for vfree() arch-dependent would be a really bad idea).

/**

 * vunmap - release virtual mapping obtained by vmap()

 * @addr:   memory base address

 *

 * Free the virtually contiguous memory area starting at @addr,

 * which was created from the page array passed to vmap().

 *

 * Must not be called in interrupt context.

/**

 * vmap - map an array of pages into virtually contiguous space

 * @pages: array of page pointers

 * @count: number of pages to map

 * @flags: vm_area->flags

 * @prot: page protection for the mapping

 *

 * Maps @count pages from @pages into contiguous kernel virtual space.

 * If @flags contains %VM_MAP_PUT_PAGES the ownership of the pages array itself

 * (which must be kmalloc or vmalloc memory) and one reference per pages in it

 * are transferred from the caller to vmap(), and will be freed / dropped when

 * vfree() is called on the return value.

 *

 * Return: the address of the area or %NULL on failure

 In bytes */

	/*

	 * Your top guard is someone else's bottom guard. Not having a top

	 * guard compromises someone else's mappings too.

/**

 * vmap_pfn - map an array of PFNs into virtually contiguous space

 * @pfns: array of PFNs

 * @count: number of pages to map

 * @prot: page protection for the mapping

 *

 * Maps @count PFNs from @pfns into contiguous kernel virtual space and returns

 * the start address of the mapping.

 CONFIG_VMAP_PFN */

	/*

	 * For order-0 pages we make use of bulk allocator, if

	 * the page array is partly or not at all populated due

	 * to fails, fallback to a single page allocator that is

	 * more permissive.

			/*

			 * A maximum allowed request is hard-coded and is 100

			 * pages per call. That is done in order to prevent a

			 * long preemption off scenario in the bulk-allocator

			 * so the range is [1:100].

			/* memory allocation should consider mempolicy, we can't

			 * wrongly use nearest node when nid == NUMA_NO_NODE,

			 * otherwise memory may be allocated in only one node,

			 * but mempolcy want to alloc memory by interleaving.

			/*

			 * If zero or pages were obtained partly,

			 * fallback to a single page allocator.

		/*

		 * Compound pages required for remap_vmalloc_page if

		 * high-order pages.

 High-order pages or fallback path if "bulk" fails. */

		/*

		 * Careful, we allocate and map page-order pages, but

		 * tracking is done per PAGE_SIZE page so as to keep the

		 * vm_struct APIs independent of the physical/mapped size.

 Please note that the recursion is strictly bounded. */

	/*

	 * If not enough pages were obtained to accomplish an

	 * allocation request, free them via __vfree() if any.

/**

 * __vmalloc_node_range - allocate virtually contiguous memory

 * @size:		  allocation size

 * @align:		  desired alignment

 * @start:		  vm area range start

 * @end:		  vm area range end

 * @gfp_mask:		  flags for the page level allocator

 * @prot:		  protection mask for the allocated pages

 * @vm_flags:		  additional vm area flags (e.g. %VM_NO_GUARD)

 * @node:		  node to use for allocation or NUMA_NO_NODE

 * @caller:		  caller's return address

 *

 * Allocate enough pages to cover @size from the page level

 * allocator with @gfp_mask flags. Please note that the full set of gfp

 * flags are not supported. GFP_KERNEL would be a preferred allocation mode

 * but GFP_NOFS and GFP_NOIO are supported as well. Zone modifiers are not

 * supported. From the reclaim modifiers__GFP_DIRECT_RECLAIM is required (aka

 * GFP_NOWAIT is not supported) and only __GFP_NOFAIL is supported (aka

 * __GFP_NORETRY and __GFP_RETRY_MAYFAIL are not supported).

 * __GFP_NOWARN can be used to suppress error messages about failures.

 *

 * Map them into contiguous kernel virtual space, using a pagetable

 * protection of @prot.

 *

 * Return: the address of the area or %NULL on failure

		/*

		 * Try huge pages. Only try for PAGE_KERNEL allocations,

		 * others like modules don't yet expect huge pages in

		 * their allocations due to apply_to_page_range not

		 * supporting them.

	/*

	 * In this function, newly allocated vm_struct has VM_UNINITIALIZED

	 * flag. It means that vm_struct is not fully initialized.

	 * Now, it is fully initialized, so remove this flag here.

/**

 * __vmalloc_node - allocate virtually contiguous memory

 * @size:	    allocation size

 * @align:	    desired alignment

 * @gfp_mask:	    flags for the page level allocator

 * @node:	    node to use for allocation or NUMA_NO_NODE

 * @caller:	    caller's return address

 *

 * Allocate enough pages to cover @size from the page level allocator with

 * @gfp_mask flags.  Map them into contiguous kernel virtual space.

 *

 * Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL

 * and __GFP_NOFAIL are not supported

 *

 * Any use of gfp flags outside of GFP_KERNEL should be consulted

 * with mm people.

 *

 * Return: pointer to the allocated memory or %NULL on error

/*

 * This is only for performance analysis of vmalloc and stress purpose.

 * It is required by vmalloc test module, therefore do not use it other

 * than that.

/**

 * vmalloc - allocate virtually contiguous memory

 * @size:    allocation size

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 *

 * For tight control over page level allocator and protection flags

 * use __vmalloc() instead.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vmalloc_no_huge - allocate virtually contiguous memory using small pages

 * @size:    allocation size

 *

 * Allocate enough non-huge pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vzalloc - allocate virtually contiguous memory with zero fill

 * @size:    allocation size

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 * The memory allocated is set to zero.

 *

 * For tight control over page level allocator and protection flags

 * use __vmalloc() instead.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vmalloc_user - allocate zeroed virtually contiguous memory for userspace

 * @size: allocation size

 *

 * The resulting memory area is zeroed so it can be mapped to userspace

 * without leaking data.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vmalloc_node - allocate memory on a specific node

 * @size:	  allocation size

 * @node:	  numa node

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 *

 * For tight control over page level allocator and protection flags

 * use __vmalloc() instead.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vzalloc_node - allocate memory on a specific node with zero fill

 * @size:	allocation size

 * @node:	numa node

 *

 * Allocate enough pages to cover @size from the page level

 * allocator and map them into contiguous kernel virtual space.

 * The memory allocated is set to zero.

 *

 * Return: pointer to the allocated memory or %NULL on error

/*

 * 64b systems should always have either DMA or DMA32 zones. For others

 * GFP_DMA32 should do the right thing and use the normal zone.

/**

 * vmalloc_32 - allocate virtually contiguous memory (32bit addressable)

 * @size:	allocation size

 *

 * Allocate enough 32bit PA addressable pages to cover @size from the

 * page level allocator and map them into contiguous kernel virtual space.

 *

 * Return: pointer to the allocated memory or %NULL on error

/**

 * vmalloc_32_user - allocate zeroed virtually contiguous 32bit memory

 * @size:	     allocation size

 *

 * The resulting memory area is 32bit addressable and zeroed so it can be

 * mapped to userspace without leaking data.

 *

 * Return: pointer to the allocated memory or %NULL on error

/*

 * small helper routine , copy contents to buf from addr.

 * If the page is not present, fill zero.

		/*

		 * To do safe access to this _mapped_ area, we need

		 * lock. But adding lock here means that we need to add

		 * overhead of vmalloc()/vfree() calls for this _debug_

		 * interface, rarely used. Instead of that, we'll use

		 * kmap() and get small overhead in this access function.

 We can expect USER0 is not used -- see vread() */

/**

 * vread() - read vmalloc area in a safe way.

 * @buf:     buffer for reading data

 * @addr:    vm address.

 * @count:   number of bytes to be read.

 *

 * This function checks that addr is a valid vmalloc'ed area, and

 * copy data from that area to a given buffer. If the given memory range

 * of [addr...addr+count) includes some valid address, data is copied to

 * proper area of @buf. If there are memory holes, they'll be zero-filled.

 * IOREMAP area is treated as memory hole and no copy is done.

 *

 * If [addr...addr+count) doesn't includes any intersects with alive

 * vm_struct area, returns 0. @buf should be kernel's buffer.

 *

 * Note: In usual ops, vread() is never necessary because the caller

 * should know vmalloc() area is valid and can use memcpy().

 * This is for routines which have to access vmalloc area without

 * any information, as /proc/kcore.

 *

 * Return: number of bytes for which addr and buf should be increased

 * (same number as @count) or %0 if [addr...addr+count) doesn't

 * include any intersection with valid vmalloc area

 Don't allow overflow */

 no intersects with alive vmap_area */

 IOREMAP area is treated as memory hole */

 zero-fill memory holes */

/**

 * remap_vmalloc_range_partial - map vmalloc pages to userspace

 * @vma:		vma to cover

 * @uaddr:		target user address to start at

 * @kaddr:		virtual address of vmalloc kernel memory

 * @pgoff:		offset from @kaddr to start at

 * @size:		size of map area

 *

 * Returns:	0 for success, -Exxx on failure

 *

 * This function checks that @kaddr is a valid vmalloc'ed area,

 * and that it is big enough to cover the range starting at

 * @uaddr in @vma. Will return failure if that criteria isn't

 * met.

 *

 * Similar to remap_pfn_range() (see mm/memory.c)

/**

 * remap_vmalloc_range - map vmalloc pages to userspace

 * @vma:		vma to cover (map full range of vma)

 * @addr:		vmalloc memory

 * @pgoff:		number of pages into addr before first page to map

 *

 * Returns:	0 for success, -Exxx on failure

 *

 * This function checks that addr is a valid vmalloc'ed area, and

 * that it is big enough to cover the vma. Will return failure if

 * that criteria isn't met.

 *

 * Similar to remap_pfn_range() (see mm/memory.c)

/**

 * pvm_find_va_enclose_addr - find the vmap_area @addr belongs to

 * @addr: target address

 *

 * Returns: vmap_area if it is found. If there is no such area

 *   the first highest(reverse order) vmap_area is returned

 *   i.e. va->va_start < addr && va->va_end < addr or NULL

 *   if there are no any areas before @addr.

/**

 * pvm_determine_end_from_reverse - find the highest aligned address

 * of free block below VMALLOC_END

 * @va:

 *   in - the VA we start the search(reverse order);

 *   out - the VA with the highest aligned end address.

 * @align: alignment for required highest address

 *

 * Returns: determined end address within vmap_area

/**

 * pcpu_get_vm_areas - allocate vmalloc areas for percpu allocator

 * @offsets: array containing offset of each area

 * @sizes: array containing size of each area

 * @nr_vms: the number of areas to allocate

 * @align: alignment, all entries in @offsets and @sizes must be aligned to this

 *

 * Returns: kmalloc'd vm_struct pointer array pointing to allocated

 *	    vm_structs on success, %NULL on failure

 *

 * Percpu allocator wants to use congruent vm areas so that it can

 * maintain the offsets among percpu areas.  This function allocates

 * congruent vmalloc areas for it with GFP_KERNEL.  These areas tend to

 * be scattered pretty far, distance between two areas easily going up

 * to gigabytes.  To avoid interacting with regular vmallocs, these

 * areas are allocated from top.

 *

 * Despite its complicated look, this allocator is rather simple. It

 * does everything top-down and scans free blocks from the end looking

 * for matching base. While scanning, if any of the areas do not fit the

 * base address is pulled down to fit the area. Scanning is repeated till

 * all the areas fit and then all necessary data structures are inserted

 * and the result is returned.

 verify parameters and allocate data structures */

 is everything aligned properly? */

 detect the area with the highest address */

 start scanning - we scan from the top, begin with the last area */

		/*

		 * base might have underflowed, add last_end before

		 * comparing.

		/*

		 * Fitting base has not been found.

		/*

		 * If required width exceeds current VA block, move

		 * base downwards and then recheck.

		/*

		 * If this VA does not fit, move base downwards and recheck.

		/*

		 * This area fits, move on to the previous one.  If

		 * the previous one is the terminal one, we're done.

 we've found a fitting base, insert all va's */

 It is a BUG(), but trigger recovery instead. */

 It is a BUG(), but trigger recovery instead. */

 Allocated area. */

 populate the kasan shadow space */

 insert all vm's */

	/*

	 * Remove previously allocated areas. There is no

	 * need in removing these areas from the busy tree,

	 * because they are inserted only on the final step

	 * and when pcpu_get_vm_areas() is success.

 Before "retry", check if we recover. */

	/*

	 * We release all the vmalloc shadows, even the ones for regions that

	 * hadn't been successfully added. This relies on kasan_release_vmalloc

	 * being able to tolerate this case.

/**

 * pcpu_free_vm_areas - free vmalloc areas for percpu allocator

 * @vms: vm_struct pointer array returned by pcpu_get_vm_areas()

 * @nr_vms: the number of allocated areas

 *

 * Free vm_structs and the array allocated by pcpu_get_vm_areas().

 CONFIG_SMP */

 Pair with smp_wmb() in clear_vm_uninitialized_flag() */

	/*

	 * s_show can encounter race with remove_vm_area, !vm on behalf

	 * of vmap area is being tear down or vm_map_ram allocation.

	/*

	 * As a final step, dump "unpurged" areas.

 SPDX-License-Identifier: GPL-2.0

/*

 *	linux/mm/madvise.c

 *

 * Copyright (C) 1999  Linus Torvalds

 * Copyright (C) 2002  Christoph Hellwig

/*

 * Any behaviour which results in changes to the vma->vm_flags needs to

 * take mmap_lock for writing. Others, which simply traverse vmas, need

 * to only take it for reading.

 be safe, default to 1. list exceptions explicitly */

/*

 * We can potentially split a vm area into separate

 * areas, each area with its own behavior.

 MADV_WIPEONFORK is only supported on anonymous memory. */

	/*

	 * vm_flags is protected by the mmap_lock held in write mode.

	/*

	 * madvise() returns EAGAIN if kernel resources, such as

	 * slab, are temporarily unavailable.

 Push any new pages onto the LRU now */

 CONFIG_SWAP */

/*

 * Schedule all required I/O operations.  Do not wait for completion.

 Push any new pages onto the LRU now */

 no bad return value, but ignore advice */

	/*

	 * Filesystem's fadvise may need to take various locks.  We need to

	 * explicitly grab a reference because the vma (and hence the

	 * vma's reference to the file) can go away as soon as we drop

	 * mmap_lock.

 tell sys_madvise we drop mmap_lock */

 Do not interfere with other mappings of this page */

		/*

		 * Creating a THP page is expensive so split it only if we

		 * are sure it's worth. Split it if we are only owner.

 Do not interfere with other mappings of this page */

		/*

		 * We are deactivating a page for accelerating reclaiming.

		 * VM couldn't reclaim the page unless we clear PG_young.

		 * As a side effect, it makes confuse idle-page tracking

		 * because they will miss recent referenced history.

	/*

	 * paging out pagecache only for non-anonymous mappings that correspond

	 * to the files the calling process could (if tried) open for writing;

	 * otherwise we'd be including shared non-exclusive mappings, which

	 * opens a side channel.

		/*

		 * If the pte has swp_entry, just clear page table to

		 * prevent swap-in which is more expensive rather than

		 * (page allocation + zeroing).

		/*

		 * If pmd isn't transhuge but the page is THP and

		 * is owned by only this process, split it and

		 * deactivate all pages.

			/*

			 * If page is shared with others, we couldn't clear

			 * PG_dirty of the page.

			/*

			 * Some of architecture(ex, PPC) don't update TLB

			 * with set_pte_at and tlb_remove_tlb_entry so for

			 * the portability, remap the pte with old|clean

			 * after pte clearing.

 MADV_FREE works for only anon vma at the moment */

/*

 * Application no longer needs these pages.  If the pages are dirty,

 * it's OK to just throw them away.  The app will be more careful about

 * data it wants to keep.  Be sure to free swap resources too.  The

 * zap_page_range call sets things up for shrink_active_list to actually free

 * these pages later if no one else has touched them in the meantime,

 * although we could add these pages to a global reuse list for

 * shrink_active_list to pick up before reclaiming other pages.

 *

 * NB: This interface discards data rather than pushes it out to swap,

 * as some implementations do.  This has performance implications for

 * applications like large transactional databases which want to discard

 * pages in anonymous maps after committing to backing store the data

 * that was kept in them.  There is no reason to write this data out to

 * the swap area if the application is discarding it.

 *

 * An interface that causes the system to free clean pages and flush

 * dirty pages is already available as msync(MS_INVALIDATE).

 mmap_lock has been dropped, prev is stale */

			/*

			 * This "vma" under revalidation is the one

			 * with the lowest vma->vm_start where start

			 * is also < vma->vm_end. If start <

			 * vma->vm_start it means an hole materialized

			 * in the user address space within the

			 * virtual range passed to MADV_DONTNEED

			 * or MADV_FREE.

			/*

			 * Don't fail if end > vma->vm_end. If the old

			 * vma was split while the mmap_lock was

			 * released the effect of the concurrent

			 * operation may not cause madvise() to

			 * have an undefined result. There may be an

			 * adjacent next vma that we'll walk

			 * next. userfaultfd_remove() will generate an

			 * UFFD_EVENT_REMOVE repetition on the

			 * end-vma->vm_end range, but the manager can

			 * handle a repetition fine.

		/*

		 * We might have temporarily dropped the lock. For example,

		 * our VMA might have been split.

 Populate (prefault) page tables readable/writable. */

 Incompatible mappings / permissions. */

 VM_FAULT_SIGBUS or VM_FAULT_SIGSEGV */

/*

 * Application wants to free up the pages and associated backing store.

 * This is effectively punching a hole into the middle of a file.

 tell sys_madvise we drop mmap_lock */

	/*

	 * Filesystem's fallocate may need to take i_rwsem.  We need to

	 * explicitly grab a reference because the vma (and hence the

	 * vma's reference to the file) can go away as soon as we drop

	 * mmap_lock.

 mmap_lock was not released by userfaultfd_remove() */

/*

 * Error injection support for memory error handling.

		/*

		 * When soft offlining hugepages, after migrating the page

		 * we dissolve it, therefore in the second loop "page" will

		 * no longer be a compound page.

/*

 * The madvise(2) system call.

 *

 * Applications can use madvise() to advise the kernel how it should

 * handle paging I/O in this VM area.  The idea is to help the kernel

 * use appropriate read-ahead and caching techniques.  The information

 * provided is advisory only, and can be safely disregarded by the

 * kernel without affecting the correct operation of the application.

 *

 * behavior values:

 *  MADV_NORMAL - the default behavior is to read clusters.  This

 *		results in some read-ahead and read-behind.

 *  MADV_RANDOM - the system should read the minimum amount of data

 *		on any access, since it is unlikely that the appli-

 *		cation will need more than what it asks for.

 *  MADV_SEQUENTIAL - pages in the given range will probably be accessed

 *		once, so they can be aggressively read ahead, and

 *		can be freed soon after they are accessed.

 *  MADV_WILLNEED - the application is notifying the system to read

 *		some pages ahead.

 *  MADV_DONTNEED - the application is finished with the given range,

 *		so the kernel can free resources associated with it.

 *  MADV_FREE - the application marks pages in the given range as lazy free,

 *		where actual purges are postponed until memory pressure happens.

 *  MADV_REMOVE - the application wants to free up the given range of

 *		pages and associated backing store.

 *  MADV_DONTFORK - omit this area from child's address space when forking:

 *		typically, to avoid COWing pages pinned by get_user_pages().

 *  MADV_DOFORK - cancel MADV_DONTFORK: no longer omit this area when forking.

 *  MADV_WIPEONFORK - present the child process with zero-filled memory in this

 *              range after a fork.

 *  MADV_KEEPONFORK - undo the effect of MADV_WIPEONFORK

 *  MADV_HWPOISON - trigger memory error handler as if the given memory range

 *		were corrupted by unrecoverable hardware memory failure.

 *  MADV_SOFT_OFFLINE - try to soft-offline the given range of memory.

 *  MADV_MERGEABLE - the application recommends that KSM try to merge pages in

 *		this area with pages of identical content from other such areas.

 *  MADV_UNMERGEABLE- cancel MADV_MERGEABLE: no longer merge pages with others.

 *  MADV_HUGEPAGE - the application wants to back the given range by transparent

 *		huge pages in the future. Existing pages might be coalesced and

 *		new pages might be allocated as THP.

 *  MADV_NOHUGEPAGE - mark the given range as not worth being backed by

 *		transparent huge pages so the existing pages will not be

 *		coalesced into THP and new pages will not be allocated as THP.

 *  MADV_DONTDUMP - the application wants to prevent pages in the given range

 *		from being included in its core dump.

 *  MADV_DODUMP - cancel MADV_DONTDUMP: no longer exclude from core dump.

 *  MADV_COLD - the application is not expected to use this memory soon,

 *		deactivate pages in this range so that they can be reclaimed

 *		easily if memory pressure happens.

 *  MADV_PAGEOUT - the application is not expected to use this memory soon,

 *		page out the pages in this range immediately.

 *  MADV_POPULATE_READ - populate (prefault) page tables readable by

 *		triggering read faults if required

 *  MADV_POPULATE_WRITE - populate (prefault) page tables writable by

 *		triggering write faults if required

 *

 * return values:

 *  zero    - success

 *  -EINVAL - start + len < 0, start is not page-aligned,

 *		"behavior" is not a valid value, or application

 *		is attempting to release locked or shared pages,

 *		or the specified address range includes file, Huge TLB,

 *		MAP_SHARED or VMPFNMAP range.

 *  -ENOMEM - addresses in the specified range are not currently

 *		mapped, or are outside the AS of the process.

 *  -EIO    - an I/O error occurred while paging in data.

 *  -EBADF  - map exists, but area maps something that isn't a file.

 *  -EAGAIN - a kernel resource was temporarily unavailable.

 Check to see whether len was rounded up from small -ve to zero */

	/*

	 * If the interval [start,end) covers some unmapped address

	 * ranges, just ignore them, but return -ENOMEM at the end.

	 * - different from the way of handling in mlock etc.

 Still start < end. */

 Here start < (end|vma->vm_end). */

 Here vma->vm_start <= start < (end|vma->vm_end) */

 Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */

 madvise_remove dropped mmap_lock */

 Require PTRACE_MODE_READ to avoid leaking ASLR metadata. */

	/*

	 * Require CAP_SYS_NICE for influencing process performance. Note that

	 * only non-destructive hints are currently supported.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/readahead.c - address_space-level file readahead.

 *

 * Copyright (C) 2002, Linus Torvalds

 *

 * 09Apr2002	Andrew Morton

 *		Initial version.

/*

 * Initialise a struct file's readahead state.  Assumes that the caller has

 * memset *ra to zero.

/*

 * see if a page needs releasing upon read_cache_pages() failure

 * - the caller of read_cache_pages() may have set PG_private or PG_fscache

 *   before calling, such as the NFS fs marking pages that are cached locally

 *   on disk, thus we need to give the fs a chance to clean up in the event of

 *   an error

/*

 * release a list of pages, invalidating them first if need be

/**

 * read_cache_pages - populate an address space with some pages & start reads against them

 * @mapping: the address_space

 * @pages: The address of a list_head which contains the target pages.  These

 *   pages have their ->index populated and are otherwise uninitialised.

 * @filler: callback routine for filling a single page.

 * @data: private data for the callback routine.

 *

 * Hides the details of the LRU cache etc from the filesystems.

 *

 * Returns: %0 on success, error return by @filler otherwise

 Clean up the remaining pages */

 Clean up the remaining pages */

/**

 * page_cache_ra_unbounded - Start unchecked readahead.

 * @ractl: Readahead control.

 * @nr_to_read: The number of pages to read.

 * @lookahead_size: Where to start the next readahead.

 *

 * This function is for filesystems to call when they want to start

 * readahead beyond a file's stated i_size.  This is almost certainly

 * not the function you want to call.  Use page_cache_async_readahead()

 * or page_cache_sync_readahead() instead.

 *

 * Context: File is referenced by caller.  Mutexes may be held by caller.

 * May sleep, but will not reenter filesystem to reclaim memory.

	/*

	 * Partway through the readahead operation, we will have added

	 * locked pages to the page cache, but will not yet have submitted

	 * them for I/O.  Adding another page may need to allocate memory,

	 * which can trigger memory reclaim.  Telling the VM we're in

	 * the middle of a filesystem operation will cause it to not

	 * touch file-backed pages, preventing a deadlock.  Most (all?)

	 * filesystems already specify __GFP_NOFS in their mapping's

	 * gfp_mask, but let's be explicit here.

	/*

	 * Preallocate as many pages as we will need.

			/*

			 * Page already present?  Kick off the current batch

			 * of contiguous pages before continuing with the

			 * next batch.  This page may be the one we would

			 * have intended to mark as Readahead, but we don't

			 * have a stable reference to this page, and it's

			 * not worth getting one just for that.

	/*

	 * Now start the IO.  We ignore I/O errors - if the page is not

	 * uptodate then the caller will launch readpage again, and

	 * will then handle the error.

/*

 * do_page_cache_ra() actually reads a chunk of disk.  It allocates

 * the pages first, then submits them for I/O. This avoids the very bad

 * behaviour which would occur if page allocations are causing VM writeback.

 * We really don't want to intermingle reads and writes like that.

 The last page we want to read */

 Don't read past the page containing the last byte of the file */

/*

 * Chunk the readahead into 2 megabyte units, so that we don't pin too much

 * memory at once.

	/*

	 * If the request exceeds the readahead window, allow the read to

	 * be up to the optimal hardware IO size

/*

 * Set the initial window size, round to next power of 2 and square

 * for small size, x 4 for medium, and x 2 for large

 * for 128k (32 page) max ra

 * 1-2 page = 16k, 3-4 page 32k, 5-8 page = 64k, > 8 page = 128k initial

/*

 *  Get the previous window size, ramp it up, and

 *  return it as the new window size.

/*

 * On-demand readahead design.

 *

 * The fields in struct file_ra_state represent the most-recently-executed

 * readahead attempt:

 *

 *                        |<----- async_size ---------|

 *     |------------------- size -------------------->|

 *     |==================#===========================|

 *     ^start             ^page marked with PG_readahead

 *

 * To overlap application thinking time and disk I/O time, we do

 * `readahead pipelining': Do not wait until the application consumed all

 * readahead pages and stalled on the missing page at readahead_index;

 * Instead, submit an asynchronous readahead I/O as soon as there are

 * only async_size pages left in the readahead window. Normally async_size

 * will be equal to size, for maximum pipelining.

 *

 * In interleaved sequential reads, concurrent streams on the same fd can

 * be invalidating each other's readahead state. So we flag the new readahead

 * page at (start+size-async_size) with PG_readahead, and use it as readahead

 * indicator. The flag won't be set on already cached pages, to avoid the

 * readahead-for-nothing fuss, saving pointless page cache lookups.

 *

 * prev_pos tracks the last visited byte in the _previous_ read request.

 * It should be maintained by the caller, and will be used for detecting

 * small random reads. Note that the readahead algorithm checks loosely

 * for sequential patterns. Hence interleaved reads might be served as

 * sequential ones.

 *

 * There is a special-case: if the first page which the application tries to

 * read happens to be the first page of the file, it is assumed that a linear

 * read is about to happen and the window is immediately set to the initial size

 * based on I/O request size and the max_readahead.

 *

 * The code ramps up the readahead size aggressively at first, but slow down as

 * it approaches max_readhead.

/*

 * Count contiguously cached pages from @index-1 to @index-@max,

 * this count is a conservative estimation of

 * 	- length of the sequential read sequence, or

 * 	- thrashing threshold in memory tight systems

/*

 * page cache context based read-ahead

	/*

	 * not enough history pages:

	 * it could be a random read

	/*

	 * starts from beginning of file:

	 * it is a strong indication of long-run stream (or whole-file-read)

/*

 * A minimal readahead algorithm for trivial sequential/random reads.

	/*

	 * If the request exceeds the readahead window, allow the read to

	 * be up to the optimal hardware IO size

	/*

	 * start of file

	/*

	 * It's the expected callback index, assume sequential access.

	 * Ramp up sizes, and push forward the readahead window.

	/*

	 * Hit a marked page without valid readahead state.

	 * E.g. interleaved reads.

	 * Query the pagecache for async_size, which normally equals to

	 * readahead size. Ramp it up and use it as the new readahead size.

 old async_size */

	/*

	 * oversize read

	/*

	 * sequential cache miss

	 * trivial case: (index - prev_index) == 1

	 * unaligned reads: (index - prev_index) == 0

	/*

	 * Query the page cache and look for the traces(cached history pages)

	 * that a sequential stream would leave behind.

	/*

	 * standalone, small random read

	 * Read as is, and do not pollute the readahead state.

	/*

	 * Will this read hit the readahead marker made by itself?

	 * If so, trigger the readahead marker hit now, and merge

	 * the resulted next readahead window into the current one.

	 * Take care of maximum IO pages as above.

	/*

	 * Even if read-ahead is disabled, issue this request as read-ahead

	 * as we'll need it to satisfy the requested range. The forced

	 * read-ahead will do the right thing and limit the read to just the

	 * requested range, which we'll set to 1 page for this case.

 be dumb */

 do read-ahead */

 no read-ahead */

	/*

	 * Same bit is used for PG_readahead and PG_reclaim.

	/*

	 * Defer asynchronous read-ahead on IO congestion.

 do read-ahead */

	/*

	 * The readahead() syscall is intended to run only on files

	 * that can execute readahead. If readahead is not possible

	 * on this file, then we must return -EINVAL.

/**

 * readahead_expand - Expand a readahead request

 * @ractl: The request to be expanded

 * @new_start: The revised start

 * @new_len: The revised size of the request

 *

 * Attempt to expand a readahead request outwards from the current size to the

 * specified size by inserting locked pages before and after the current window

 * to increase the size to the new window.  This may involve the insertion of

 * THPs, in which case the window may get expanded even beyond what was

 * requested.

 *

 * The algorithm will stop if it encounters a conflicting page already in the

 * pagecache and leave a smaller expansion than requested.

 *

 * The caller must check for this by examining the revised @ractl object for a

 * different expansion than was requested.

 Expand the leading edge downwards */

 Page apparently present */

 Expand the trailing edge upwards */

 Page apparently present */

 SPDX-License-Identifier: GPL-2.0

/*

 * Provide common bits of early_ioremap() support for architectures needing

 * temporary mappings during boot before ioremap() is available.

 *

 * This is mostly a direct copy of the x86 early_ioremap implementation.

 *

 * (C) Copyright 1995 1996, 2014 Linus Torvalds

 *

/*

 * Generally, ioremap() is available after paging_init() has been called.

 * Architectures wanting to allow early_ioremap after paging_init() can

 * define __late_set_fixmap and __late_clear_fixmap to do the right thing.

 Don't allow wraparound or zero size */

	/*

	 * Mappings have to be page-aligned

	/*

	 * Mappings have to fit in the FIX_BTMAP area.

	/*

	 * Ok, go for it..

 Remap an IO device */

 Remap memory */

 CONFIG_MMU */

 Remap memory */

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/mmap.c

 *

 * Written by obz.

 *

 * Address space accounting code	<alan@lxorguk.ukuu.org.uk>

/* description of effects of mapping type and prot in current implementation.

 * this is due to the limited x86 page protection hardware.  The expected

 * behavior is in parens:

 *

 * map_type	prot

 *		PROT_NONE	PROT_READ	PROT_WRITE	PROT_EXEC

 * MAP_SHARED	r: (no) no	r: (yes) yes	r: (no) yes	r: (no) yes

 *		w: (no) no	w: (no) no	w: (yes) yes	w: (no) no

 *		x: (no) no	x: (no) yes	x: (no) yes	x: (yes) yes

 *

 * MAP_PRIVATE	r: (no) no	r: (yes) yes	r: (no) yes	r: (no) yes

 *		w: (no) no	w: (no) no	w: (copy) copy	w: (no) no

 *		x: (no) no	x: (no) yes	x: (no) yes	x: (yes) yes

 *

 * On arm64, PROT_EXEC has the following behaviour for both MAP_SHARED and

 * MAP_PRIVATE (with Enhanced PAN supported):

 *								r: (no) no

 *								w: (no) no

 *								x: (yes) yes

 Update vma->vm_page_prot to reflect vma->vm_flags. */

 remove_protection_ptes reads vma->vm_page_prot without mmap_lock */

/*

 * Requires inode->i_mapping->i_mmap_rwsem

/*

 * Unlink a file-based vm structure from its interval tree, to hide

 * vma from rmap and vmtruncate before freeing its page tables.

/*

 * Close a vm structure and free it, returning the next.

	/*

	 * CONFIG_COMPAT_BRK can still be overridden by setting

	 * randomize_va_space to 2, which will still cause mm->start_brk

	 * to be arbitrarily shifted

	/*

	 * Check against rlimit here. If this check is done later after the test

	 * of oldbrk with newbrk then it can escape the test and let the data

	 * segment grow beyond its set limit the in case where the limit is

	 * not page aligned -Ram Gupta

	/*

	 * Always allow shrinking brk.

	 * __do_munmap() may downgrade mmap_lock to read.

		/*

		 * mm->brk must to be protected by write mmap_lock so update it

		 * before downgrading mmap_lock. When __do_munmap() fails,

		 * mm->brk will be restored from origbrk.

 Check against existing mmap mappings. */

 Ok, looks good - let it rip. */

	/*

	 * Note: in the rare case of a VM_GROWSDOWN above a VM_GROWSUP, we

	 * allow two stack_guard_gaps between them here, and when choosing

	 * an unmapped area; whereas when expanding we only require one.

	 * That's a little inconsistent, but keeps the code here simpler.

/*

 * Update augmented rbtree rb_subtree_gap values after vma->vm_start or

 * vma->vm_prev->vm_end values changed, without modifying the vma's position

 * in the rbtree.

	/*

	 * As it turns out, RB_DECLARE_CALLBACKS_MAX() already created

	 * a callback function that does exactly what we want.

 All rb_subtree_gap values must be consistent prior to insertion */

	/*

	 * Note rb_erase_augmented is a fairly large inline function,

	 * so make sure we instantiate it only once with our desired

	 * augmented rbtree callbacks.

	/*

	 * All rb_subtree_gap values must be consistent prior to erase,

	 * with the possible exception of

	 *

	 * a. the "next" vma being erased if next->vm_start was reduced in

	 *    __vma_adjust() -> __vma_unlink()

	 * b. the vma being erased in detach_vmas_to_be_unmapped() ->

	 *    vma_rb_erase()

/*

 * vma has some anon_vma assigned, and is already inserted on that

 * anon_vma's interval trees.

 *

 * Before updating the vma's vm_start / vm_end / vm_pgoff fields, the

 * vma must be removed from the anon_vma's interval trees using

 * anon_vma_interval_tree_pre_update_vma().

 *

 * After the update, the vma will be reinserted using

 * anon_vma_interval_tree_post_update_vma().

 *

 * The entire update must be protected by exclusive mmap_lock and by

 * the root anon_vma's mutex.

 Fail if an existing vma overlaps the area */

/*

 * vma_next() - Get the next VMA.

 * @mm: The mm_struct.

 * @vma: The current vma.

 *

 * If @vma is NULL, return the first vma in the mm.

 *

 * Returns: The next VMA after @vma.

/*

 * munmap_vma_range() - munmap VMAs that overlap a range.

 * @mm: The mm struct

 * @start: The start of the range.

 * @len: The length of the range.

 * @pprev: pointer to the pointer that will be set to previous vm_area_struct

 * @rb_link: the rb_node

 * @rb_parent: the parent rb_node

 *

 * Find all the vm_area_struct that overlap from @start to

 * @end and munmap them.  Set @pprev to the previous vm_area_struct.

 *

 * Returns: -ENOMEM on munmap failure or 0 on success.

 Find first overlapping mapping */

 Iterate over the rest of the overlaps */

 Update tracking information for the gap following the new vma. */

	/*

	 * vma->vm_prev wasn't known when we followed the rbtree to find the

	 * correct insertion point for that vma. As a result, we could not

	 * update the vma vm_rb parents rb_subtree_gap values on the way down.

	 * So, we first insert the vma with a zero rb_subtree_gap value

	 * (to be consistent with what we did on the way down), and then

	 * immediately update the gap to the correct value. Finally we

	 * rebalance the rbtree after all augmented values have been set.

/*

 * Helper for vma_adjust() in the split_vma insert case: insert a vma into the

 * mm's list and rbtree.  It has already been inserted into the interval tree.

 Kill the cache */

/*

 * We cannot adjust vm_start, vm_end, vm_pgoff fields of a vma that

 * is already present in an i_mmap tree without adjusting the tree.

 * The following helper function should be used when such adjustments

 * are necessary.  The "insert" vma (if any) is to be inserted

 * before we drop the necessary locks.

			/*

			 * vma expands, overlapping all the next, and

			 * perhaps the one after too (mprotect case 6).

			 * The only other cases that gets here are

			 * case 1, case 7 and case 8.

				/*

				 * The only case where we don't expand "vma"

				 * and we expand "next" instead is case 8.

				/*

				 * remove_next == 3 means we're

				 * removing "vma" and that to do so we

				 * swapped "vma" and "next".

				/*

				 * case 1, 6, 7, remove_next == 2 is case 6,

				 * remove_next == 1 is case 1 or 7.

 trim end to next, for case 6 first pass */

			/*

			 * If next doesn't have anon_vma, import from vma after

			 * next, if the vma overlaps with it.

			/*

			 * vma expands, overlapping part of the next:

			 * mprotect case 5 shifting the boundary up.

			/*

			 * vma shrinks, and !insert tells it's not

			 * split_vma inserting another: so it must be

			 * mprotect case 4 shifting the boundary down.

		/*

		 * Easily overlooked: when mprotect shifts the boundary,

		 * make sure the expanding vma has anon_vma set if the

		 * shrinking vma had, to cover any anon pages imported.

			/*

			 * Put into interval tree now, so instantiated pages

			 * are visible to arm/parisc __flush_dcache_page

			 * throughout; but we cannot insert into address

			 * space until vma start or end is updated.

		/*

		 * vma_merge has merged next into vma, and needs

		 * us to remove next before dropping the locks.

			/*

			 * vma is not before next if they've been

			 * swapped.

			 *

			 * pre-swap() next->vm_start was reduced so

			 * tell validate_mm_rb to ignore pre-swap()

			 * "next" (which is stored in post-swap()

			 * "vma").

		/*

		 * split_vma has split insert from vma, and needs

		 * us to insert it before dropping the locks

		 * (it may either follow vma or precede it).

		/*

		 * In mprotect's case 6 (see comments on vma_merge),

		 * we must remove another next too. It would clutter

		 * up the code too much to do both in one go.

			/*

			 * If "next" was removed and vma->vm_end was

			 * expanded (up) over it, in turn

			 * "next->vm_prev->vm_end" changed and the

			 * "vma->vm_next" gap must be updated.

			/*

			 * For the scope of the comment "next" and

			 * "vma" considered pre-swap(): if "vma" was

			 * removed, next->vm_start was expanded (down)

			 * over it and the "next" gap must be updated.

			 * Because of the swap() the post-swap() "vma"

			 * actually points to pre-swap() "next"

			 * (post-swap() "next" as opposed is now a

			 * dangling pointer).

			/*

			 * If remove_next == 2 we obviously can't

			 * reach this path.

			 *

			 * If remove_next == 3 we can't reach this

			 * path because pre-swap() next is always not

			 * NULL. pre-swap() "next" is not being

			 * removed and its next->vm_end is not altered

			 * (and furthermore "end" already matches

			 * next->vm_end in remove_next == 3).

			 *

			 * We reach this only in the remove_next == 1

			 * case if the "next" vma that was removed was

			 * the highest vma of the mm. However in such

			 * case next->vm_end == "end" and the extended

			 * "vma" has vma->vm_end == next->vm_end so

			 * mm->highest_vm_end doesn't need any update

			 * in remove_next == 1 case.

/*

 * If the vma has a ->close operation then the driver probably needs to release

 * per-vma resources, so we don't attempt to merge those.

	/*

	 * VM_SOFTDIRTY should not prevent from VMA merging, if we

	 * match the flags but dirty bit -- the caller should mark

	 * merged VMA as dirty. If dirty bit won't be excluded from

	 * comparison, we increase pressure on the memory system forcing

	 * the kernel to generate new VMAs when old one could be

	 * extended instead.

	/*

	 * The list_is_singular() test is to avoid merging VMA cloned from

	 * parents. This can improve scalability caused by anon_vma lock.

/*

 * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)

 * in front of (at a lower virtual address and file offset than) the vma.

 *

 * We cannot merge two vmas if they have differently assigned (non-NULL)

 * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.

 *

 * We don't check here for the merged mmap wrapping around the end of pagecache

 * indices (16TB on ia32) because do_mmap() does not permit mmap's which

 * wrap, nor mmaps which cover the final page at index -1UL.

/*

 * Return true if we can merge this (vm_flags,anon_vma,file,vm_pgoff)

 * beyond (at a higher virtual address and file offset than) the vma.

 *

 * We cannot merge two vmas if they have differently assigned (non-NULL)

 * anon_vmas, nor if same anon_vma is assigned but offsets incompatible.

/*

 * Given a mapping request (addr,end,vm_flags,file,pgoff), figure out

 * whether that can be merged with its predecessor or its successor.

 * Or both (it neatly fills a hole).

 *

 * In most cases - when called for mmap, brk or mremap - [addr,end) is

 * certain not to be mapped by the time vma_merge is called; but when

 * called for mprotect, it is certain to be already mapped (either at

 * an offset within prev, or at the start of next), and the flags of

 * this area are about to be changed to vm_flags - and the no-change

 * case has already been eliminated.

 *

 * The following mprotect cases have to be considered, where AAAA is

 * the area passed down from mprotect_fixup, never extending beyond one

 * vma, PPPPPP is the prev vma specified, and NNNNNN the next vma after:

 *

 *     AAAA             AAAA                   AAAA

 *    PPPPPPNNNNNN    PPPPPPNNNNNN       PPPPPPNNNNNN

 *    cannot merge    might become       might become

 *                    PPNNNNNNNNNN       PPPPPPPPPPNN

 *    mmap, brk or    case 4 below       case 5 below

 *    mremap move:

 *                        AAAA               AAAA

 *                    PPPP    NNNN       PPPPNNNNXXXX

 *                    might become       might become

 *                    PPPPPPPPPPPP 1 or  PPPPPPPPPPPP 6 or

 *                    PPPPPPPPNNNN 2 or  PPPPPPPPXXXX 7 or

 *                    PPPPNNNNNNNN 3     PPPPXXXXXXXX 8

 *

 * It is important for case 8 that the vma NNNN overlapping the

 * region AAAA is never going to extended over XXXX. Instead XXXX must

 * be extended in region AAAA and NNNN must be removed. This way in

 * all cases where vma_merge succeeds, the moment vma_adjust drops the

 * rmap_locks, the properties of the merged vma will be already

 * correct for the whole merged range. Some of those properties like

 * vm_page_prot/vm_flags may be accessed by rmap_walks and they must

 * be correct for the whole merged range immediately after the

 * rmap_locks are released. Otherwise if XXXX would be removed and

 * NNNN would be extended over the XXXX range, remove_migration_ptes

 * or other rmap walkers (if working on addresses beyond the "end"

 * parameter) may establish ptes with the wrong permissions of NNNN

 * instead of the right permissions of XXXX.

	/*

	 * We later require that vma->vm_flags == vm_flags,

	 * so this tests vma->vm_flags & VM_SPECIAL, too.

 cases 6, 7, 8 */

 verify some invariant that must be enforced by the caller */

	/*

	 * Can it merge with the predecessor?

		/*

		 * OK, it can.  Can we now merge in the successor as well?

 cases 1, 6 */

 cases 2, 5, 7 */

	/*

	 * Can this new request be merged in front of next?

 case 4 */

 cases 3, 8 */

			/*

			 * In case 3 area is already equal to next and

			 * this is a noop, but in case 8 "area" has

			 * been removed and next was expanded over it.

/*

 * Rough compatibility check to quickly see if it's even worth looking

 * at sharing an anon_vma.

 *

 * They need to have the same vm_file, and the flags can only differ

 * in things that mprotect may change.

 *

 * NOTE! The fact that we share an anon_vma doesn't _have_ to mean that

 * we can merge the two vma's. For example, we refuse to merge a vma if

 * there is a vm_ops->close() function, because that indicates that the

 * driver is doing some kind of reference counting. But that doesn't

 * really matter for the anon_vma sharing case.

/*

 * Do some basic sanity checking to see if we can re-use the anon_vma

 * from 'old'. The 'a'/'b' vma's are in VM order - one of them will be

 * the same as 'old', the other will be the new one that is trying

 * to share the anon_vma.

 *

 * NOTE! This runs with mm_sem held for reading, so it is possible that

 * the anon_vma of 'old' is concurrently in the process of being set up

 * by another page fault trying to merge _that_. But that's ok: if it

 * is being set up, that automatically means that it will be a singleton

 * acceptable for merging, so we can do all of this optimistically. But

 * we do that READ_ONCE() to make sure that we never re-load the pointer.

 *

 * IOW: that the "list_is_singular()" test on the anon_vma_chain only

 * matters for the 'stable anon_vma' case (ie the thing we want to avoid

 * is to return an anon_vma that is "complex" due to having gone through

 * a fork).

 *

 * We also make sure that the two vma's are compatible (adjacent,

 * and with the same memory policies). That's all stable, even with just

 * a read lock on the mm_sem.

/*

 * find_mergeable_anon_vma is used by anon_vma_prepare, to check

 * neighbouring vmas for a suitable anon_vma, before it goes off

 * to allocate a new anon_vma.  It checks because a repetitive

 * sequence of mprotects and faults may otherwise lead to distinct

 * anon_vmas being allocated, preventing vma merge in subsequent

 * mprotect.

 Try next first. */

 Try prev next. */

	/*

	 * We might reach here with anon_vma == NULL if we can't find

	 * any reusable anon_vma.

	 * There's no absolute need to look only at touching neighbours:

	 * we could search further afield for "compatible" anon_vmas.

	 * But it would probably just be a waste of time searching,

	 * or lead to too many vmas hanging off the same anon_vma.

	 * We're trying to allow mprotect remerging later on,

	 * not trying to minimize memory used for anon_vmas.

/*

 * If a hint addr is less than mmap_min_addr change hint to be as

 * low as possible but still greater than mmap_min_addr

  mlock MCL_FUTURE? */

 Special "we do even unsigned file positions" case */

 Yes, random drivers might want more. But I'm tired of buggy drivers */

/*

 * The caller must write-lock current->mm->mmap_lock.

	/*

	 * Does the application expect PROT_READ to imply PROT_EXEC?

	 *

	 * (the exception is when the underlying filesystem is noexec

	 *  mounted, in which case we dont add PROT_EXEC.)

 force arch specific MAP_FIXED handling in get_unmapped_area */

 Careful about overflows.. */

 offset overflow? */

 Too many mappings? */

	/* Obtain the address to map to. we verify (or select) it and ensure

	 * that it represents a valid section of the address space.

	/* Do simple checking here so the lower-level routines won't have

	 * to. we assume access permissions have been handled by the open

	 * of the memory object, so we don't do any here.

			/*

			 * Force use of MAP_SHARED_VALIDATE with non-legacy

			 * flags. E.g. MAP_SYNC is dangerous to use with

			 * MAP_SHARED as you don't know which consistency model

			 * you will get. We silently ignore unsupported flags

			 * with MAP_SHARED to preserve backward compatibility.

			/*

			 * Make sure we don't allow writing to an append-only

			 * file..

			/*

			 * Ignore pgoff.

			/*

			 * Set pgoff according to addr for anon_vma.

	/*

	 * Set 'VM_NORESERVE' if we should not account for the

	 * memory use of this mapping.

 We honor MAP_NORESERVE if allowed to overcommit */

 hugetlb applies strict overcommit unless MAP_NORESERVE */

		/*

		 * VM_NORESERVE is used because the reservations will be

		 * taken when vm_ops->mmap() is called

		 * A dummy user value is used because we are not locking

		 * memory so no accounting is necessary

 __ARCH_WANT_SYS_OLD_MMAP */

/*

 * Some shared mappings will want the pages marked read-only

 * to track write events. If so, we'll downgrade vm_page_prot

 * to the private version (using protection_map[] without the

 * VM_SHARED bit).

 If it was private or non-writable, the write bit is already clear */

 The backer wishes to know when pages are first written to? */

	/* The open routine did something to the protections that pgprot_modify

 Do we need to track softdirty? */

 Specialty mapping? */

 Can the mapping track the dirty pages? */

/*

 * We account for memory if it's a private writeable mapping,

 * not hugepages and VM_NORESERVE wasn't set.

	/*

	 * hugetlb has its own accounting separate from the core VM

	 * VM_HUGETLB may not be set yet so we cannot check for that flag.

 Check against address space limit. */

		/*

		 * MAP_FIXED may remove pages of mappings that intersects with

		 * requested mapping. Account for the pages it would unmap.

 Clear old maps, set up prev, rb_link, rb_parent, and uf */

	/*

	 * Private writable mapping: check memory availability

	/*

	 * Can we just expand an old mapping?

	/*

	 * Determine the object being mapped and call the appropriate

	 * specific mapper. the address has already been validated, but

	 * not unmapped, but the maps are removed from the list.

		/* Can addr have changed??

		 *

		 * Answer: Yes, several device drivers can do it in their

		 *         f_op->mmap method. -DaveM

		 * Bug: If addr is changed, prev, rb_link, rb_parent should

		 *      be updated for vma_link()

		/* If vm_flags changed after call_mmap(), we should try merge vma again

		 * as we may succeed this time.

				/* ->mmap() can change vma->vm_file and fput the original file. So

				 * fput the vma->vm_file here or we would add an extra fput for file

				 * and cause general protection fault ultimately.

 Update vm_flags to pick up the change. */

 Allow architectures to sanity-check the vm_flags */

 Once vma denies write, undo our temporary denial count */

	/*

	 * New (or expanded) vma always get soft dirty status.

	 * Otherwise user-space soft-dirty page tracker won't

	 * be able to distinguish situation when vma area unmapped,

	 * then new mapped in-place (which must be aimed as

	 * a completely new data area).

 Undo any partial mapping done by a device driver. */

	/*

	 * We implement the search by looking for an rbtree node that

	 * immediately follows a suitable gap. That is,

	 * - gap_start = vma->vm_prev->vm_end <= info->high_limit - length;

	 * - gap_end   = vma->vm_start        >= info->low_limit  + length;

	 * - gap_end - gap_start >= length

 Adjust search length to account for worst case alignment overhead */

 Adjust search limits by the desired length */

 Check if rbtree root looks promising */

 Visit left subtree if it looks promising */

 Check if current node has a suitable gap */

 Visit right subtree if it looks promising */

 Go back up the rbtree to find next candidate node */

 Check highest gap, which does not precede any rbtree node */

 Only for VM_BUG_ON below */

 We found a suitable gap. Clip it with the original low_limit. */

 Adjust gap address to the desired alignment */

 Adjust search length to account for worst case alignment overhead */

	/*

	 * Adjust search limits by the desired length.

	 * See implementation comment at top of unmapped_area().

 Check highest gap, which does not precede any rbtree node */

 Check if rbtree root looks promising */

 Visit right subtree if it looks promising */

 Check if current node has a suitable gap */

 Visit left subtree if it looks promising */

 Go back up the rbtree to find next candidate node */

 We found a suitable gap. Clip it with the original high_limit. */

 Compute highest gap address at the desired alignment */

/*

 * Search for an unmapped address range.

 *

 * We are looking for a range that:

 * - does not intersect with any VMA;

 * - is contained within the [low_limit, high_limit) interval;

 * - is at least the desired size.

 * - satisfies (begin_addr & align_mask) == (align_offset & align_mask)

/* Get an address range which is currently unmapped.

 * For shmat() with addr=0.

 *

 * Ugly calling convention alert:

 * Return value with the low bits set means error value,

 * ie

 *	if (ret & ~PAGE_MASK)

 *		error = ret;

 *

 * This function "knows" that -ENOMEM has the bits set.

/*

 * This mmap-allocator allocates new areas top-down from below the

 * stack's low limit (the base):

 requested length too big for entire address space */

 requesting a specific address */

	/*

	 * A failed mmap() very likely causes application failure,

	 * so fall back to the bottom-up function here. This scenario

	 * can happen with large stack limits and large mmap()

	 * allocations.

 Careful about overflows.. */

		/*

		 * mmap_region() will call shmem_zero_setup() to create a file,

		 * so use shmem's get_unmapped_area in case it can be huge.

		 * do_mmap() will clear pgoff, so match alignment.

 Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */

 Check the cache first. */

/*

 * Same as find_vma, but also return a pointer to the previous VMA in *pprev.

/*

 * Verify that the stack growth is acceptable and

 * update accounting. This is shared with both the

 * grow-up and grow-down cases.

 address space limit tests */

 Stack limit test */

 mlock limit tests */

 Check to ensure the stack will not grow into a hugetlb-only region */

	/*

	 * Overcommit..  This must be the final test, as it will

	 * update security statistics.

/*

 * PA-RISC uses this for its stack; IA64 for its Register Backing Store.

 * vma is the last one with address > vma->vm_end.  Have to extend vma.

 Guard against exceeding limits of the address space. */

 Enforce stack_guard_gap */

 Guard against overflow */

 Check that both stack segments have the same anon_vma? */

 We must make sure the anon_vma is allocated. */

	/*

	 * vma->vm_start/vm_end cannot change under us because the caller

	 * is required to hold the mmap_lock in read mode.  We need the

	 * anon_vma lock to serialize against concurrent expand_stacks.

 Somebody else might have raced and expanded it already */

				/*

				 * vma_gap_update() doesn't support concurrent

				 * updates, but we only hold a shared mmap_lock

				 * lock here, so we need to protect against

				 * concurrent vma expansions.

				 * anon_vma_lock_write() doesn't help here, as

				 * we don't guarantee that all growable vmas

				 * in a mm share the same root anon vma.

				 * So, we reuse mm->page_table_lock to guard

				 * against concurrent vma expansions.

 CONFIG_STACK_GROWSUP || CONFIG_IA64 */

/*

 * vma is the first one with address < vma->vm_start.  Have to extend vma.

 Enforce stack_guard_gap */

 Check that both stack segments have the same anon_vma? */

 We must make sure the anon_vma is allocated. */

	/*

	 * vma->vm_start/vm_end cannot change under us because the caller

	 * is required to hold the mmap_lock in read mode.  We need the

	 * anon_vma lock to serialize against concurrent expand_stacks.

 Somebody else might have raced and expanded it already */

				/*

				 * vma_gap_update() doesn't support concurrent

				 * updates, but we only hold a shared mmap_lock

				 * lock here, so we need to protect against

				 * concurrent vma expansions.

				 * anon_vma_lock_write() doesn't help here, as

				 * we don't guarantee that all growable vmas

				 * in a mm share the same root anon vma.

				 * So, we reuse mm->page_table_lock to guard

				 * against concurrent vma expansions.

 enforced gap between the expanding stack and other mappings. */

 don't alter vm_end if the coredump is running */

/*

 * Ok - we have the memory areas we should free on the vma list,

 * so release them, and do the vma updates.

 *

 * Called with the mm semaphore held.

 Update high watermark before we lower total_vm */

/*

 * Get rid of page table information in the indicated region.

 *

 * Called with the mm semaphore held.

/*

 * Create a list of vma's touched by the unmap, removing them from the mm's

 * vma list as we go..

 Kill the cache */

	/*

	 * Do not downgrade mmap_lock if we are next to VM_GROWSDOWN or

	 * VM_GROWSUP VMA. Such VMAs can change their size under

	 * down_read(mmap_lock) and collide with the VMA we are about to unmap.

/*

 * __split_vma() bypasses sysctl_max_map_count checking.  We use this where it

 * has already been checked or doesn't make sense to fail.

 Success. */

 Clean everything up if vma_adjust failed. */

/*

 * Split a vma into two pieces at address 'addr', a new vma is allocated

 * either for the first part or the tail.

/* Munmap is split into 2 main parts -- this part which finds

 * what needs doing, and the areas themselves, which do the

 * work.  This now handles partial unmappings.

 * Jeremy Fitzhardinge <jeremy@goop.org>

	/*

	 * arch_unmap() might do unmaps itself.  It must be called

	 * and finish any rbtree manipulation before this code

	 * runs and also starts to manipulate the rbtree.

 Find the first overlapping VMA where start < vma->vm_end */

	/*

	 * If we need to split any vma, do it now to save pain later.

	 *

	 * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially

	 * unmapped vm_area_struct will remain in use: so lower split_vma

	 * places tmp vma above, and higher split_vma places tmp vma below.

		/*

		 * Make sure that map_count on return from munmap() will

		 * not exceed its limit; but let map_count go just above

		 * its limit temporarily, to help free resources as expected.

 Does it split the last one? */

		/*

		 * If userfaultfd_unmap_prep returns an error the vmas

		 * will remain split, but userland will get a

		 * highly unexpected error anyway. This is no

		 * different than the case where the first of the two

		 * __split_vma fails, but we don't undo the first

		 * split, despite we could. This is unlikely enough

		 * failure that it's not worth optimizing it for.

	/*

	 * unlock any mlock()ed ranges before detaching vmas

 Detach vmas from rbtree */

 Fix up all other VM information */

	/*

	 * Returning 1 indicates mmap_lock is downgraded.

	 * But 1 is not legal return value of vm_munmap() and munmap(), reset

	 * it to 0 before return.

/*

 * Emulation of deprecated remap_file_pages() syscall.

 Does pgoff wrap? */

 hole between vmas ? */

/*

 *  this is really a simplified "do_mmap".  it only handles

 *  anonymous maps.  eventually we may be able to do some

 *  brk-specific accounting here.

 Until we need other flags, refuse anything except VM_EXEC. */

 Clear old maps, set up prev, rb_link, rb_parent, and uf */

 Check against address space limits *after* clearing old maps... */

 Can we just expand an old private anonymous mapping? */

	/*

	 * create a vma struct for an anonymous mapping

 Release all mmaps. */

 mm's last user has gone, and its about to be pulled down */

		/*

		 * Manually reap the mm to free as much memory as possible.

		 * Then, as the oom reaper does, set MMF_OOM_SKIP to disregard

		 * this mm from further consideration.  Taking mm->mmap_lock for

		 * write after setting MMF_OOM_SKIP will guarantee that the oom

		 * reaper will not run on this mm again after mmap_lock is

		 * dropped.

		 *

		 * Nothing can be holding mm->mmap_lock here and the above call

		 * to mmu_notifier_release(mm) ensures mmu notifier callbacks in

		 * __oom_reap_task_mm() will not block.

		 *

		 * This needs to be done before calling munlock_vma_pages_all(),

		 * which clears VM_LOCKED, otherwise the oom reaper cannot

		 * reliably test it.

 Can happen if dup_mmap() received an OOM */

 update_hiwater_rss(mm) here? but nobody should be looking */

 Use -1 here to ensure all VMAs in the mm are unmapped */

	/*

	 * Walk the list again, actually closing and freeing it,

	 * with preemption enabled, without holding any MM locks.

/* Insert vm structure into process list sorted by address

 * and into the inode's i_mmap tree.  If vm_file is non-NULL

 * then i_mmap_rwsem is taken here.

	/*

	 * The vm_pgoff of a purely anonymous vma should be irrelevant

	 * until its first write fault, when page's anon_vma and index

	 * are set.  But now set the vm_pgoff it will almost certainly

	 * end up with (unless mremap moves it elsewhere before that

	 * first wfault), so /proc/pid/maps tells a consistent story.

	 *

	 * By setting it to reflect the virtual start address of the

	 * vma, merges and splits can happen in a seamless way, just

	 * using the existing file pgoff checks and manipulations.

	 * Similarly in do_mmap and in do_brk_flags.

/*

 * Copy the vma structure to a new location in the same mm,

 * prior to moving page table entries, to effect an mremap move.

	/*

	 * If anonymous vma has not yet been faulted, update new pgoff

	 * to match new location, to increase its chance of merging.

 should never get here */

		/*

		 * Source vma may have been merged into new_vma

			/*

			 * The only way we can get a vma_merge with

			 * self during an mremap is if the vma hasn't

			 * been faulted in yet and we were allowed to

			 * reset the dst vma->vm_pgoff to the

			 * destination address of the mremap to allow

			 * the merge to happen. mremap must change the

			 * vm_pgoff linearity between src and dst vmas

			 * (in turn preventing a vma_merge) to be

			 * safe. It is only safe to keep the vm_pgoff

			 * linear if there are no pages mapped yet.

/*

 * Return true if the calling process may expand its vm space by the passed

 * number of pages

 Workaround for Valgrind */

/*

 * Having a close hook prevents vma merging regardless of flags.

	/*

	 * Forbid splitting special mappings - kernel has expectations over

	 * the number of pages in mapping. Together with VM_DONTEXPAND

	 * the size of vma should stay the same over the special mapping's

	 * lifetime.

 vDSO code relies that VVAR can't be accessed remotely */

/*

 * Called with mm->mmap_lock held for writing.

 * Insert a new vma covering the given region, with the given flags.

 * Its pages are supplied by the given array of struct page *.

 * The array can be shorter than len >> PAGE_SHIFT if it's null-terminated.

 * The region past the last page supplied will always produce SIGBUS.

 * The array pointer and the pages it points to are assumed to stay alive

 * for as long as this mapping might exist.

		/*

		 * The LSB of head.next can't change from under us

		 * because we hold the mm_all_locks_mutex.

		/*

		 * We can safely modify head.next after taking the

		 * anon_vma->root->rwsem. If some other vma in this mm shares

		 * the same anon_vma we won't take it again.

		 *

		 * No need of atomic instructions here, head.next

		 * can't change from under us thanks to the

		 * anon_vma->root->rwsem.

		/*

		 * AS_MM_ALL_LOCKS can't change from under us because

		 * we hold the mm_all_locks_mutex.

		 *

		 * Operations on ->flags have to be atomic because

		 * even if AS_MM_ALL_LOCKS is stable thanks to the

		 * mm_all_locks_mutex, there may be other cpus

		 * changing other bitflags in parallel to us.

/*

 * This operation locks against the VM for all pte/vma/mm related

 * operations that could ever happen on a certain mm. This includes

 * vmtruncate, try_to_unmap, and all page faults.

 *

 * The caller must take the mmap_lock in write mode before calling

 * mm_take_all_locks(). The caller isn't allowed to release the

 * mmap_lock until mm_drop_all_locks() returns.

 *

 * mmap_lock in write mode is required in order to block all operations

 * that could modify pagetables and free pages without need of

 * altering the vma layout. It's also needed in write mode to avoid new

 * anon_vmas to be associated with existing vmas.

 *

 * A single task can't take more than one mm_take_all_locks() in a row

 * or it would deadlock.

 *

 * The LSB in anon_vma->rb_root.rb_node and the AS_MM_ALL_LOCKS bitflag in

 * mapping->flags avoid to take the same lock twice, if more than one

 * vma in this mm is backed by the same anon_vma or address_space.

 *

 * We take locks in following order, accordingly to comment at beginning

 * of mm/rmap.c:

 *   - all hugetlbfs_i_mmap_rwsem_key locks (aka mapping->i_mmap_rwsem for

 *     hugetlb mapping);

 *   - all i_mmap_rwsem locks;

 *   - all anon_vma->rwseml

 *

 * We can take all locks within these types randomly because the VM code

 * doesn't nest them and we protected from parallel mm_take_all_locks() by

 * mm_all_locks_mutex.

 *

 * mm_take_all_locks() and mm_drop_all_locks are expensive operations

 * that may have to take thousand of locks.

 *

 * mm_take_all_locks() can fail if it's interrupted by signals.

		/*

		 * The LSB of head.next can't change to 0 from under

		 * us because we hold the mm_all_locks_mutex.

		 *

		 * We must however clear the bitflag before unlocking

		 * the vma so the users using the anon_vma->rb_root will

		 * never see our bitflag.

		 *

		 * No need of atomic instructions here, head.next

		 * can't change from under us until we release the

		 * anon_vma->root->rwsem.

		/*

		 * AS_MM_ALL_LOCKS can't change to 0 from under us

		 * because we hold the mm_all_locks_mutex.

/*

 * The mmap_lock cannot be released by the caller until

 * mm_drop_all_locks() returns.

/*

 * initialise the percpu counter for VM

/*

 * Initialise sysctl_user_reserve_kbytes.

 *

 * This is intended to prevent a user from starting a single memory hogging

 * process, such that they cannot recover (kill the hog) in OVERCOMMIT_NEVER

 * mode.

 *

 * The default value is min(3% of free memory, 128MB)

 * 128MB is enough to recover with sshd/login, bash, and top/kill.

/*

 * Initialise sysctl_admin_reserve_kbytes.

 *

 * The purpose of sysctl_admin_reserve_kbytes is to allow the sys admin

 * to log in and kill a memory hogging process.

 *

 * Systems with more than 256MB will reserve 8MB, enough to recover

 * with sshd, bash, and top in OVERCOMMIT_GUESS. Smaller systems will

 * only reserve 3% of free pages by default.

/*

 * Reinititalise user and admin reserves if memory is added or removed.

 *

 * The default user reserve max is 128MB, and the default max for the

 * admin reserve is 8MB. These are usually, but not always, enough to

 * enable recovery from a memory hogging process using login/sshd, a shell,

 * and tools like top. It may make sense to increase or even disable the

 * reserve depending on the existence of swap or variations in the recovery

 * tools. So, the admin may have changed them.

 *

 * If memory is added and the reserves have been eliminated or increased above

 * the default max, then we'll trust the admin.

 *

 * If memory is removed and there isn't enough free memory, then we

 * need to reset the reserves.

 *

 * Otherwise keep the reserve set by the admin.

 Default max is 128MB. Leave alone if modified by operator. */

 Default max is 8MB.  Leave alone if modified by operator. */

 SPDX-License-Identifier: GPL-2.0

/*

 * SLOB Allocator: Simple List Of Blocks

 *

 * Matt Mackall <mpm@selenic.com> 12/30/03

 *

 * NUMA support by Paul Mundt, 2007.

 *

 * How SLOB works:

 *

 * The core of SLOB is a traditional K&R style heap allocator, with

 * support for returning aligned objects. The granularity of this

 * allocator is as little as 2 bytes, however typically most architectures

 * will require 4 bytes on 32-bit and 8 bytes on 64-bit.

 *

 * The slob heap is a set of linked list of pages from alloc_pages(),

 * and within each page, there is a singly-linked list of free blocks

 * (slob_t). The heap is grown on demand. To reduce fragmentation,

 * heap pages are segregated into three lists, with objects less than

 * 256 bytes, objects less than 1024 bytes, and all other objects.

 *

 * Allocation from heap involves first searching for a page with

 * sufficient free blocks (using a next-fit-like approach) followed by

 * a first-fit scan of the page. Deallocation inserts objects back

 * into the free list in address order, so this is effectively an

 * address-ordered first fit.

 *

 * Above this is an implementation of kmalloc/kfree. Blocks returned

 * from kmalloc are prepended with a 4-byte header with the kmalloc size.

 * If kmalloc is asked for objects of PAGE_SIZE or larger, it calls

 * alloc_pages() directly, allocating compound pages so the page order

 * does not have to be separately tracked.

 * These objects are detected in kfree() because PageSlab()

 * is false for them.

 *

 * SLAB is emulated on top of SLOB by simply calling constructors and

 * destructors for every SLAB allocation. Objects are returned with the

 * 4-byte alignment unless the SLAB_HWCACHE_ALIGN flag is set, in which

 * case the low-level allocator will fragment blocks to create the proper

 * alignment. Again, objects of page-size or greater are allocated by

 * calling alloc_pages(). As SLAB objects know their size, no separate

 * size bookkeeping is necessary and there is essentially no allocation

 * space overhead, and compound pages aren't needed for multi-page

 * allocations.

 *

 * NUMA support in SLOB is fairly simplistic, pushing most of the real

 * logic down to the page allocator, and simply doing the node accounting

 * on the upper levels. In the event that a node id is explicitly

 * provided, __alloc_pages_node() with the specified node id is used

 * instead. The common case (or when the node id isn't explicitly provided)

 * will default to the current node, as per numa_node_id().

 *

 * Node aware pages are still inserted in to the global freelist, and

 * these are scanned for by matching against the node id encoded in the

 * page flags. As a result, block allocations that can be satisfied from

 * the freelist will only be done so on pages residing on the same node,

 * in order to prevent random node placement.

 struct reclaim_state */

/*

 * slob_block has a field 'units', which indicates size of block if +ve,

 * or offset of next block if -ve (in SLOB_UNITs).

 *

 * Free blocks of size 1 unit simply contain the offset of the next block.

 * Those with larger size contain their size in the first SLOB_UNIT of

 * memory, and the offset of the next free block in the second SLOB_UNIT.

/*

 * All partially free slob pages go on these lists.

/*

 * slob_page_free: true for pages on free_slob_pages list.

/*

 * struct slob_rcu is inserted at the tail of allocated slob blocks, which

 * were created with a SLAB_TYPESAFE_BY_RCU slab. slob_rcu is used to free

 * the block using call_rcu.

/*

 * slob_lock protects all slob allocator structures.

/*

 * Encode the given size and next info into a free slob block s.

/*

 * Return the size of a slob block.

/*

 * Return the next free slob block pointer after this one.

/*

 * Returns true if s is the last free block in its page.

/*

 * slob_page_alloc() - Allocate a slob block within a given slob_page sp.

 * @sp: Page to look in.

 * @size: Size of the allocation.

 * @align: Allocation alignment.

 * @align_offset: Offset in the allocated block that will be aligned.

 * @page_removed_from_list: Return parameter.

 *

 * Tries to find a chunk of memory at least @size bytes big within @page.

 *

 * Return: Pointer to memory if allocated, %NULL otherwise.  If the

 *         allocation fills up @page then the page is removed from the

 *         freelist, in this case @page_removed_from_list will be set to

 *         true (set to false otherwise).

		/*

		 * 'aligned' will hold the address of the slob block so that the

		 * address 'aligned'+'align_offset' is aligned according to the

		 * 'align' parameter. This is for kmalloc() which prepends the

		 * allocated block with its size, so that the block itself is

		 * aligned when needed.

 room enough? */

 need to fragment head to align? */

 exact fit? unlink. */

 fragment */

/*

 * slob_alloc: entry point into the slob allocator.

 Iterate through each partially free page, try to find room */

		/*

		 * If there's a node specification, search for a partial

		 * page with a matching node id in the freelist.

 Enough room on this page? */

		/*

		 * If slob_page_alloc() removed sp from the list then we

		 * cannot call list functions on sp.  If so allocation

		 * did not fragment the page anyway so optimisation is

		 * unnecessary.

			/*

			 * Improve fragment distribution and reduce our average

			 * search time by starting our next search here. (see

			 * Knuth vol 1, sec 2.5, pg 449)

 Not enough space: must allocate a new page */

/*

 * slob_free: entry point into the slob allocator.

 Go directly to page allocator. Do not pass slob allocator */

 This slob page is about to become partially free. Easy! */

	/*

	 * Otherwise the page is already partially free, so find reinsertion

	 * point.

/*

 * End of slob allocator proper. Begin kmem_cache_alloc and kmalloc frontend.

		/*

		 * For power of two sizes, guarantee natural alignment for

		 * kmalloc()'d objects.

 can't use ksize for kmem_cache_alloc memory, only kmalloc */

 leave room for rcu footer at the end of object */

 No way to check for remaining objects */

 SPDX-License-Identifier: GPL-2.0

/*

 * mm/fadvise.c

 *

 * Copyright (C) 2002, Linus Torvalds

 *

 * 11Jan2003	Andrew Morton

 *		Initial version.

/*

 * POSIX_FADV_WILLNEED could set PG_Referenced, and POSIX_FADV_NOREUSE could

 * deactivate the pages and clear PG_Referenced.

 inclusive */

 no bad return value, but ignore advice */

	/*

	 * Careful about overflows. Len == 0 means "as much as possible".  Use

	 * unsigned math because signed overflows are undefined and UBSan

	 * complains.

 inclusive */

 First and last PARTIAL page! */

 Careful about overflow on the "+1" */

		/*

		 * First and last FULL page! Partial pages are deliberately

		 * preserved on the expectation that it is better to preserve

		 * needed memory than to discard unneeded memory.

		/*

		 * The page at end_index will be inclusively discarded according

		 * by invalidate_mapping_pages(), so subtracting 1 from

		 * end_index means we will skip the last page.  But if endbyte

		 * is page aligned or is at the end of file, we should not skip

		 * that page - discarding the last page is safe enough.

			/* First page is tricky as 0 - 1 = -1, but pgoff_t

			 * is unsigned, so the end_index >= start_index

			 * check below would be true and we'll discard the whole

			 * file cache which is not what was asked.

			/*

			 * It's common to FADV_DONTNEED right after

			 * the read or write that instantiates the

			 * pages, in which case there will be some

			 * sitting on the local LRU cache. Try to

			 * avoid the expensive remote drain and the

			 * second cache tree walk below by flushing

			 * them out right away.

			/*

			 * If fewer pages were invalidated than expected then

			 * it is possible that some of the pages were on

			 * a per-cpu pagevec for a remote CPU. Drain all

			 * pagevecs and try again.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/swap.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * This file contains the default values for the operation of the

 * Linux VM subsystem. Fine-tuning documentation can be found in

 * Documentation/admin-guide/sysctl/vm.rst.

 * Started 18.12.91

 * Swap aging added 23.2.95, Stephen Tweedie.

 * Buffermem limits added 12.3.98, Rik van Riel.

 How many pages do we try to swap or page in/out together? */

 Protecting only lru_rotate.pvec which requires disabling interrupts */

/*

 * The following struct pagevec are grouped together because they are protected

 * by disabling preemption (and interrupts remain enabled).

/*

 * This path almost never happens for VM activity - pages are normally

 * freed via pagevecs.  But it gets used by networking.

	/*

	 * __page_cache_release() is supposed to be called for thp, not for

	 * hugetlb. This is because hugetlb page does never have PageLRU set

	 * (it's never listed to any LRU lists) and no memcg routines should

	 * be called for hugetlb (it has a separate hugetlb_cgroup.)

		/*

		 * The page belongs to the device that created pgmap. Do

		 * not return it to page allocator.

/**

 * put_pages_list() - release a list of pages

 * @pages: list of pages threaded on page->lru

 *

 * Release a list of pages which are strung together on page.lru.

 Cannot be PageLRU because it's passed to us using the lru */

/*

 * get_kernel_pages() - pin kernel pages in memory

 * @kiov:	An array of struct kvec structures

 * @nr_segs:	number of segments to pin

 * @write:	pinning for read/write, currently ignored

 * @pages:	array that receives pointers to the pages pinned.

 *		Should be at least nr_segs long.

 *

 * Returns number of pages pinned. This may be fewer than the number

 * requested. If nr_pages is 0 or negative, returns 0. If no pages

 * were pinned, returns -errno. Each page returned must be released

 * with a put_page() call when it is finished with.

 block memcg migration during page moving between lru */

 return true if pagevec needs to drain */

/*

 * Writeback is about to end against a folio which has been marked for

 * immediate reclaim.  If it still appears to be reclaimable, move it

 * to the tail of the inactive list.

 *

 * folio_rotate_reclaimable() must disable IRQs, to prevent nasty races.

		/*

		 * Hold lruvec->lru_lock is safe here, since

		 * 1) The pinned lruvec in reclaim, or

		 * 2) From a pre-LRU page during refault (which also holds the

		 *    rcu lock, so would be safe even if the page was on the LRU

		 *    and could move simultaneously to a new lruvec).

 Record cost event */

		/*

		 * Decay previous events

		 *

		 * Because workloads change over time (and to avoid

		 * overflow) we keep these statistics as a floating

		 * average, which ends up weighing recent refaults

		 * more than old ones.

	/*

	 * Search backwards on the optimistic assumption that the page being

	 * activated has just been added to this pagevec. Note that only

	 * the local pagevec is examined as a !PageLRU page could be in the

	 * process of being released, reclaimed, migrated or on a remote

	 * pagevec that is currently being drained. Furthermore, marking

	 * a remote pagevec's page PageActive potentially hits a race where

	 * a page is marked PageActive just after it is added to the inactive

	 * list causing accounting errors and BUG_ON checks to trigger.

/*

 * Mark a page as having seen activity.

 *

 * inactive,unreferenced	->	inactive,referenced

 * inactive,referenced		->	active,unreferenced

 * active,unreferenced		->	active,referenced

 *

 * When a newly allocated page is not yet visible, so safe for non-atomic ops,

 * __SetPageReferenced(page) may be substituted for mark_page_accessed(page).

		/*

		 * Unevictable pages are on the "LRU_UNEVICTABLE" list. But,

		 * this list is never rotated or maintained, so marking an

		 * evictable page accessed has no effect.

		/*

		 * If the page is on the LRU, queue it for activation via

		 * lru_pvecs.activate_page. Otherwise, assume the page is on a

		 * pagevec, mark it active and it'll be moved to the active

		 * LRU on the next drain.

/**

 * folio_add_lru - Add a folio to an LRU list.

 * @folio: The folio to be added to the LRU.

 *

 * Queue the folio for addition to the LRU. The decision on whether

 * to add the page to the [in]active [file|anon] list is deferred until the

 * pagevec is drained. This gives a chance for the caller of folio_add_lru()

 * have the folio added to the active list using folio_mark_accessed().

/**

 * lru_cache_add_inactive_or_unevictable

 * @page:  the page to be added to LRU

 * @vma:   vma in which page is mapped for determining reclaimability

 *

 * Place @page on the inactive or unevictable LRU list, depending on its

 * evictability.

		/*

		 * We use the irq-unsafe __mod_zone_page_state because this

		 * counter is not modified from interrupt context, and the pte

		 * lock is held(spinlock), which implies preemption disabled.

/*

 * If the page can not be invalidated, it is moved to the

 * inactive list to speed up its reclaim.  It is moved to the

 * head of the list, rather than the tail, to give the flusher

 * threads some time to write it out, as this is much more

 * effective than the single-page writeout from reclaim.

 *

 * If the page isn't page_mapped and dirty/writeback, the page

 * could reclaim asap using PG_reclaim.

 *

 * 1. active, mapped page -> none

 * 2. active, dirty/writeback page -> inactive, head, PG_reclaim

 * 3. inactive, mapped page -> none

 * 4. inactive, dirty/writeback page -> inactive, head, PG_reclaim

 * 5. inactive, clean -> inactive, tail

 * 6. Others -> none

 *

 * In 4, why it moves inactive's head, the VM expects the page would

 * be write it out by flusher threads as this is much more effective

 * than the single-page writeout from reclaim.

 Some processes are using the page */

		/*

		 * PG_reclaim could be raced with end_page_writeback

		 * It can make readahead confusing.  But race window

		 * is _really_ small and  it's non-critical problem.

		/*

		 * The page's writeback ends up during pagevec

		 * We move that page into tail of inactive.

		/*

		 * Lazyfree pages are clean anonymous pages.  They have

		 * PG_swapbacked flag cleared, to distinguish them from normal

		 * anonymous pages

/*

 * Drain pages out of the cpu's pagevecs.

 * Either "cpu" is the current CPU, and preemption has already been

 * disabled; or "cpu" is being hot-unplugged, and is already dead.

 Disabling interrupts below acts as a compiler barrier. */

 No harm done if a racing interrupt already did this */

/**

 * deactivate_file_page - forcefully deactivate a file page

 * @page: page to deactivate

 *

 * This function hints the VM that @page is a good reclaim candidate,

 * for example if its invalidation fails due to the page being dirty

 * or under writeback.

	/*

	 * In a workload with many unevictable page such as mprotect,

	 * unevictable page deactivation for accelerating reclaim is pointless.

/*

 * deactivate_page - deactivate a page

 * @page: page to deactivate

 *

 * deactivate_page() moves @page to the inactive list if @page was on the active

 * list and was not an unevictable page.  This is done to accelerate the reclaim

 * of @page.

/**

 * mark_page_lazyfree - make an anon page lazyfree

 * @page: page to deactivate

 *

 * mark_page_lazyfree() moves @page to the inactive file list.

 * This is done to accelerate the reclaim of @page.

/*

 * It's called from per-cpu workqueue context in SMP case so

 * lru_add_drain_cpu and invalidate_bh_lrus_cpu should run on

 * the same cpu. It shouldn't be a problem in !SMP case since

 * the core is only one and the locks will disable preemption.

/*

 * Doesn't need any cpu hotplug locking because we do rely on per-cpu

 * kworkers being shut down before our page_alloc_cpu_dead callback is

 * executed on the offlined cpu.

 * Calling this function with cpu hotplug locks held can actually lead

 * to obscure indirect dependencies via WQ context.

	/*

	 * lru_drain_gen - Global pages generation number

	 *

	 * (A) Definition: global lru_drain_gen = x implies that all generations

	 *     0 < n <= x are already *scheduled* for draining.

	 *

	 * This is an optimization for the highly-contended use case where a

	 * user space workload keeps constantly generating a flow of pages for

	 * each CPU.

	/*

	 * Make sure nobody triggers this path before mm_percpu_wq is fully

	 * initialized.

	/*

	 * Guarantee pagevec counter stores visible by this CPU are visible to

	 * other CPUs before loading the current drain generation.

	/*

	 * (B) Locally cache global LRU draining generation number

	 *

	 * The read barrier ensures that the counter is loaded before the mutex

	 * is taken. It pairs with smp_mb() inside the mutex critical section

	 * at (D).

	/*

	 * (C) Exit the draining operation if a newer generation, from another

	 * lru_add_drain_all(), was already scheduled for draining. Check (A).

	/*

	 * (D) Increment global generation number

	 *

	 * Pairs with smp_load_acquire() at (B), outside of the critical

	 * section. Use a full memory barrier to guarantee that the new global

	 * drain generation number is stored before loading pagevec counters.

	 *

	 * This pairing must be done here, before the for_each_online_cpu loop

	 * below which drains the page vectors.

	 *

	 * Let x, y, and z represent some system CPU numbers, where x < y < z.

	 * Assume CPU #z is in the middle of the for_each_online_cpu loop

	 * below and has already reached CPU #y's per-cpu data. CPU #x comes

	 * along, adds some pages to its per-cpu vectors, then calls

	 * lru_add_drain_all().

	 *

	 * If the paired barrier is done at any later step, e.g. after the

	 * loop, CPU #x will just exit at (C) and miss flushing out all of its

	 * added pages.

 CONFIG_SMP */

/*

 * lru_cache_disable() needs to be called before we start compiling

 * a list of pages to be migrated using isolate_lru_page().

 * It drains pages on LRU cache and then disable on all cpus until

 * lru_cache_enable is called.

 *

 * Must be paired with a call to lru_cache_enable().

	/*

	 * lru_add_drain_all in the force mode will schedule draining on

	 * all online CPUs so any calls of lru_cache_disabled wrapped by

	 * local_lock or preemption disabled would be ordered by that.

	 * The atomic operation doesn't need to have stronger ordering

	 * requirements because that is enforeced by the scheduling

	 * guarantees.

/**

 * release_pages - batched put_page()

 * @pages: array of pages to release

 * @nr: number of pages

 *

 * Decrement the reference count on all the pages in @pages.  If it

 * fell to zero, remove the page from the LRU and free it.

		/*

		 * Make sure the IRQ-safe lock-holding time does not get

		 * excessive with a continuous string of pages from the

		 * same lruvec. The lock is held only if lruvec != NULL.

			/*

			 * ZONE_DEVICE pages that return 'false' from

			 * page_is_devmap_managed() do not require special

			 * processing, and instead, expect a call to

			 * put_page_testzero().

/*

 * The pages which we're about to release may be in the deferred lru-addition

 * queues.  That would prevent them from really being freed right now.  That's

 * OK from a correctness point of view but is inefficient - those pages may be

 * cache-warm and we want to give them back to the page allocator ASAP.

 *

 * So __pagevec_release() will drain those queues here.  __pagevec_lru_add()

 * and __pagevec_lru_add_active() call release_pages() directly to avoid

 * mutual recursion.

	/*

	 * A folio becomes evictable in two ways:

	 * 1) Within LRU lock [munlock_vma_page() and __munlock_pagevec()].

	 * 2) Before acquiring LRU lock to put the folio on the correct LRU

	 *    and then

	 *   a) do PageLRU check with lock [check_move_unevictable_pages]

	 *   b) do PageLRU check before lock [clear_page_mlock]

	 *

	 * (1) & (2a) are ok as LRU lock will serialize them. For (2b), we need

	 * following strict ordering:

	 *

	 * #0: __pagevec_lru_add_fn		#1: clear_page_mlock

	 *

	 * folio_set_lru()			folio_test_clear_mlocked()

	 * smp_mb() // explicit ordering	// above provides strict

	 *					// ordering

	 * folio_test_mlocked()			folio_test_lru()

	 *

	 *

	 * if '#1' does not observe setting of PG_lru by '#0' and

	 * fails isolation, the explicit barrier will make sure that

	 * folio_evictable check will put the folio on the correct

	 * LRU. Without smp_mb(), folio_set_lru() can be reordered

	 * after folio_test_mlocked() check and can make '#1' fail the

	 * isolation of the folio whose mlocked bit is cleared (#0 is

	 * also looking at the same folio) and the evictable folio will

	 * be stranded on an unevictable LRU.

/*

 * Add the passed pages to the LRU, then drop the caller's refcount

 * on them.  Reinitialises the caller's pagevec.

/**

 * pagevec_remove_exceptionals - pagevec exceptionals pruning

 * @pvec:	The pagevec to prune

 *

 * find_get_entries() fills both pages and XArray value entries (aka

 * exceptional entries) into the pagevec.  This function prunes all

 * exceptionals from @pvec without leaving holes, so that it can be

 * passed on to page-only pagevec operations.

/**

 * pagevec_lookup_range - gang pagecache lookup

 * @pvec:	Where the resulting pages are placed

 * @mapping:	The address_space to search

 * @start:	The starting page index

 * @end:	The final page index

 *

 * pagevec_lookup_range() will search for & return a group of up to PAGEVEC_SIZE

 * pages in the mapping starting from index @start and upto index @end

 * (inclusive).  The pages are placed in @pvec.  pagevec_lookup() takes a

 * reference against the pages in @pvec.

 *

 * The search returns a group of mapping-contiguous pages with ascending

 * indexes.  There may be holes in the indices due to not-present pages. We

 * also update @start to index the next page for the traversal.

 *

 * pagevec_lookup_range() returns the number of pages which were found. If this

 * number is smaller than PAGEVEC_SIZE, the end of specified range has been

 * reached.

/*

 * Perform any setup for the swap system

 Use a smaller cluster for small-memory machines */

	/*

	 * Right now other parts of the system means that we

	 * _really_ don't want to cluster much more

	/*

	 * devmap page refcounts are 1-based, rather than 0-based: if

	 * refcount is 1, then the page is free and the refcount is

	 * stable because nobody holds a reference on the page.

 SPDX-License-Identifier: GPL-2.0

 request page reporting */

 Check to see if we are in desired state */

	/*

	 * If reporting is already active there is nothing we need to do.

	 * Test against 0 as that represents PAGE_REPORTING_IDLE.

	/*

	 * Delay the start of work to allow a sizable queue to build. For

	 * now we are limiting this to running no more than once every

	 * couple of seconds.

 notify prdev of free page reporting request */

	/*

	 * We use RCU to protect the pr_dev_info pointer. In almost all

	 * cases this should be present, however in the unlikely case of

	 * a shutdown this will be NULL and we should exit.

	/*

	 * Drain the now reported pages back into their respective

	 * free lists/areas. We assume at least one page is populated.

 If the pages were not reported due to error skip flagging */

		/*

		 * If page was not comingled with another page we can

		 * consider the result to be "reported" since the page

		 * hasn't been modified, otherwise we will need to

		 * report on the new larger page when we make our way

		 * up to that higher order.

 reinitialize scatterlist now that it is empty */

/*

 * The page reporting cycle consists of 4 stages, fill, report, drain, and

 * idle. We will cycle through the first 3 stages until we cannot obtain a

 * full scatterlist of pages, in that case we will switch to idle.

	/*

	 * Perform early check, if free area is empty there is

	 * nothing to process so we can skip this free_list.

	/*

	 * Limit how many calls we will be making to the page reporting

	 * device for this list. By doing this we avoid processing any

	 * given list for too long.

	 *

	 * The current value used allows us enough calls to process over a

	 * sixteenth of the current list plus one additional call to handle

	 * any pages that may have already been present from the previous

	 * list processed. This should result in us reporting all pages on

	 * an idle system in about 30 seconds.

	 *

	 * The division here should be cheap since PAGE_REPORTING_CAPACITY

	 * should always be a power of 2.

 loop through free list adding unreported pages to sg list */

 We are going to skip over the reported pages. */

		/*

		 * If we fully consumed our budget then update our

		 * state to indicate that we are requesting additional

		 * processing and exit this list.

 Attempt to pull page from list and place in scatterlist */

 Add page to scatter list */

		/*

		 * Make the first non-reported page in the free list

		 * the new head of the free list before we release the

		 * zone lock.

 release lock before waiting on report processing */

 begin processing pages in local list */

 reset offset since the full list was reported */

 update budget to reflect call to report function */

 reacquire zone lock and resume processing */

 flush reported pages from the sg list */

		/*

		 * Reset next to first entry, the old next isn't valid

		 * since we dropped the lock to report the pages

 exit on error */

 Rotate any leftover pages to the head of the freelist */

 Generate minimum watermark to be able to guarantee progress */

	/*

	 * Cancel request if insufficient free memory or if we failed

	 * to allocate page reporting statistics for the zone.

 Process each free list starting from lowest order/mt */

 We do not pull pages from the isolate free list */

 report the leftover pages before going idle */

 flush any remaining pages out from the last report */

	/*

	 * Change the state to "Active" so that we can track if there is

	 * anyone requests page reporting after we complete our pass. If

	 * the state is not altered by the end of the pass we will switch

	 * to idle and quit scheduling reporting runs.

 allocate scatterlist to store pages being reported on */

	/*

	 * If the state has reverted back to requested then there may be

	 * additional pages to be processed. We will defer for 2s to allow

	 * more pages to accumulate.

 nothing to do if already in use */

	/*

	 * Update the page reporting order if it's specified by driver.

	 * Otherwise, it falls back to @pageblock_order.

 initialize state and work structures */

 Begin initial flush of zones */

 Assign device to allow notifications */

 enable page reporting notification */

 Disable page reporting notification */

 Flush any existing work, and lock it out */

 SPDX-License-Identifier: GPL-2.0

/*

 * TODO: teach PAGE_OWNER_STACK_DEPTH (__dump_page_owner and save_stack)

 * to use off stack temporal storage

	/*

	 * Avoid recursion.

	 *

	 * Sometimes page metadata allocation tracking requires more

	 * memory to be allocated:

	 * - when new stack trace is saved to stack depot

	 * - when backtrace itself is calculated (ia64)

	/*

	 * We don't clear the bit on the old folio as it's going to be freed

	 * after migration. Until then, the info can be useful in case of

	 * a bug, and the overall stats will be off a bit only temporarily.

	 * Also, migrate_misplaced_transhuge_page() can still fail the

	 * migration and then we want the old folio to retain the info. But

	 * in that case we also don't need to explicitly clear the info from

	 * the new page, which will be freed.

 Scan block by block. First and last block may be incomplete */

	/*

	 * Walk the zone in pageblock_nr_pages steps. If a page block spans

	 * a zone boundary, it will be double counted between zones. This does

	 * not matter as the mixed block count will still be correct

 The pageblock is online, no need to recheck. */

 Print counts */

 Print information relevant to grouping pages by mobility */

 Find a valid PFN or the start of a MAX_ORDER_NR_PAGES area */

 Find an allocated page */

		/*

		 * If the new page is in a new MAX_ORDER_NR_PAGES area,

		 * validate the area as existing, skip it if not

		/*

		 * Some pages could be missed by concurrent allocation or free,

		 * because we don't hold the zone lock.

		/*

		 * Although we do have the info about past allocation of free

		 * pages, it's not relevant for current memory usage.

		/*

		 * Don't print "tail" pages of high-order allocations as that

		 * would inflate the stats.

		/*

		 * Access to page_ext->handle isn't synchronous so we should

		 * be careful to access it.

 Record the next PFN to read in the file offset */

	/*

	 * Walk the zone in pageblock_nr_pages steps. If a page block spans

	 * a zone boundary, it will be double counted between zones. This does

	 * not matter as the mixed block count will still be correct

			/*

			 * To avoid having to grab zone->lock, be a little

			 * careful when reading buddy page order. The only

			 * danger is that we skip too much and potentially miss

			 * some early allocated pages, which is better than

			 * heavy lock contention.

 Maybe overlapping zone */

 Found early allocated page */

 SPDX-License-Identifier: GPL-2.0

/*

 * Memory Migration functionality - linux/mm/migrate.c

 *

 * Copyright (C) 2006 Silicon Graphics, Inc., Christoph Lameter

 *

 * Page migration was first developed in the context of the memory hotplug

 * project. The main authors of the migration code are:

 *

 * IWAMOTO Toshihiro <iwamoto@valinux.co.jp>

 * Hirokazu Takahashi <taka@valinux.co.jp>

 * Dave Hansen <haveblue@us.ibm.com>

 * Christoph Lameter

	/*

	 * Avoid burning cycles with pages that are yet under __free_pages(),

	 * or just got freed under us.

	 *

	 * In case we 'win' a race for a movable page being freed under us and

	 * raise its refcount preventing __free_pages() from doing its job

	 * the put_page() at the end of this block will take care of

	 * release this page, thus avoiding a nasty leakage.

	/*

	 * Check PageMovable before holding a PG_lock because page's owner

	 * assumes anybody doesn't touch PG_lock of newly allocated page

	 * so unconditionally grabbing the lock ruins page's owner side.

	/*

	 * As movable pages are not isolated from LRU lists, concurrent

	 * compaction threads can race against page migration functions

	 * as well as race against the releasing a page.

	 *

	 * In order to avoid having an already isolated movable page

	 * being (wrongly) re-isolated while it is under migration,

	 * or to avoid attempting to isolate pages being released,

	 * lets be sure we have the page lock

	 * before proceeding with the movable page isolation steps.

 Driver shouldn't use PG_isolated bit of page->flags */

/*

 * Put previously isolated pages back onto the appropriate lists

 * from where they were once taken off for compaction/migration.

 *

 * This function shall be used whenever the isolated pageset has been

 * built from lru, balloon, hugetlbfs page. See isolate_migratepages_range()

 * and isolate_huge_page().

		/*

		 * We isolated non-lru movable page so here we can use

		 * __PageMovable because LRU page's mapping cannot have

		 * PAGE_MAPPING_MOVABLE.

/*

 * Restore a potential migration pte to a working pte entry

 PMD-mapped THP migration entry */

		/*

		 * Recheck VMA as permissions can change since migration started

 No need to invalidate - it was non-present before */

/*

 * Get rid of all migration entries and replace them by

 * references to the indicated page.

/*

 * Something used the pte of a page under migration. We need to

 * get to the page and wait until migration is finished.

 * When we return from this function the fault will be retried.

	/*

	 * Once page cache replacement of page migration started, page_count

	 * is zero; but we must not call put_and_wait_on_page_locked() without

	 * a ref. Use get_page_unless_zero(), and just fault again if it fails.

	/*

	 * Device private pages have an extra refcount as they are

	 * ZONE_DEVICE pages.

/*

 * Replace the page in the mapping.

 *

 * The number of remaining references must be:

 * 1 for anonymous pages without a mapping

 * 2 for pages with a mapping

 * 3 for pages with a mapping and PagePrivate/PagePrivate2 set.

 Anonymous page without mapping */

 No turning back from here */

	/*

	 * Now we know that no one else is looking at the folio:

	 * no turning back from here.

 add cache reference */

 Move dirty while page refs frozen and newpage not yet exposed */

	/*

	 * Drop cache reference from old page by unfreezing

	 * to one less reference.

	 * We know this isn't the last reference.

 Leave irq disabled to prevent preemption while updating stats */

	/*

	 * If moved to a different zone then also account

	 * the page for that zone. Other VM counters will be

	 * taken care of when we establish references to the

	 * new page and drop references to the old page.

	 *

	 * Note that anonymous pages are accounted for

	 * via NR_FILE_PAGES and NR_ANON_MAPPED if they

	 * are mapped to swap space.

/*

 * The expected number of remaining references is the same as that

 * of folio_migrate_mapping().

/*

 * Copy the flags and some other ancillary information

 Move dirty on pages not done by folio_migrate_mapping() */

	/*

	 * Copy NUMA information to the new page, to prevent over-eager

	 * future migrations of this same page.

	/*

	 * Please do not reorder this without considering how mm/ksm.c's

	 * get_ksm_page() depends upon ksm_migrate_page() and PageSwapCache().

 page->private contains hugetlb specific flags */

	/*

	 * If any waiters have accumulated on the new page then

	 * wake them up.

	/*

	 * PG_readahead shares the same bit with PG_reclaim.  The above

	 * end_page_writeback() may clear PG_readahead mistakenly, so set the

	 * bit after that.

/************************************************************

 *                    Migration functions

/*

 * Common logic to directly migrate a single LRU page suitable for

 * pages that do not use PagePrivate/PagePrivate2.

 *

 * Pages are locked upon entry and exit.

 Writeback must be complete */

 Returns true if all buffers are successfully locked */

 Simple case, sync compaction */

 async case, we cannot block on lock_buffer so use trylock_buffer */

			/*

			 * We failed to lock the buffer and cannot stall in

			 * async migration. Release the taken locks

 Check whether page does not have extra refs before we do more work */

/*

 * Migration function for pages with buffers. This function can only be used

 * if the underlying filesystem guarantees that no other references to "page"

 * exist. For example attached buffer heads are accessed only under page lock.

/*

 * Same as above except that this variant is more careful and checks that there

 * are also no buffer head references. This function is the right one for

 * mappings where buffer heads are directly looked up and referenced (such as

 * block device mappings).

/*

 * Writeback a page to clean the dirty state

 No write method for the address space */

 Someone else already triggered a write */

	/*

	 * A dirty page may imply that the underlying filesystem has

	 * the page on some queue. So the page must be clean for

	 * migration. Writeout may mean we loose the lock and the

	 * page state is no longer what we checked for earlier.

	 * At this point we know that the migration attempt cannot

	 * be successful.

 unlocked. Relock */

/*

 * Default handling if a filesystem does not provide a migration function.

 Only writeback pages in full synchronous migration */

	/*

	 * Buffers may be managed in a filesystem specific way.

	 * We must have no buffers or drop them.

/*

 * Move a page to a newly allocated page

 * The page is locked and all ptes have been successfully removed.

 *

 * The new page will have replaced the old page if this function

 * is successful.

 *

 * Return value:

 *   < 0 - error code

 *  MIGRATEPAGE_SUCCESS - success

			/*

			 * Most pages have a mapping and most filesystems

			 * provide a migratepage callback. Anonymous pages

			 * are part of swap space which also has its own

			 * migratepage callback. This is the most common path

			 * for page migration.

		/*

		 * In case of non-lru page, it could be released after

		 * isolation step. In that case, we shouldn't try migration.

	/*

	 * When successful, old pagecache page->mapping must be cleared before

	 * page is freed; but stats require that PageAnon be left as PageAnon.

			/*

			 * We clear PG_movable under page_lock so any compactor

			 * cannot try to migrate this page.

		/*

		 * Anonymous and movable page->mapping will be cleared by

		 * free_pages_prepare so don't reset it here for keeping

		 * the type to work PageAnon, for example.

		/*

		 * It's not safe for direct compaction to call lock_page.

		 * For example, during page readahead pages are added locked

		 * to the LRU. Later, when the IO completes the pages are

		 * marked uptodate and unlocked. However, the queueing

		 * could be merging multiple pages for one bio (e.g.

		 * mpage_readahead). If an allocation happens for the

		 * second or third page, the process can end up locking

		 * the same page twice and deadlocking. Rather than

		 * trying to be clever about what pages can be locked,

		 * avoid the use of lock_page for direct compaction

		 * altogether.

		/*

		 * Only in the case of a full synchronous migration is it

		 * necessary to wait for PageWriteback. In the async case,

		 * the retry loop is too short and in the sync-light case,

		 * the overhead of stalling is too much

	/*

	 * By try_to_migrate(), page->mapcount goes down to 0 here. In this case,

	 * we cannot notice that anon_vma is freed while we migrates a page.

	 * This get_anon_vma() delays freeing anon_vma pointer until the end

	 * of migration. File cache pages are no problem because of page_lock()

	 * File Caches may use write_page() or lock_page() in migration, then,

	 * just care Anon page here.

	 *

	 * Only page_get_anon_vma() understands the subtleties of

	 * getting a hold on an anon_vma from outside one of its mms.

	 * But if we cannot get anon_vma, then we won't need it anyway,

	 * because that implies that the anon page is no longer mapped

	 * (and cannot be remapped so long as we hold the page lock).

	/*

	 * Block others from accessing the new page when we get around to

	 * establishing additional references. We are usually the only one

	 * holding a reference to newpage at this point. We used to have a BUG

	 * here if trylock_page(newpage) fails, but would like to allow for

	 * cases where there might be a race with the previous use of newpage.

	 * This is much like races on refcount of oldpage: just don't BUG().

	/*

	 * Corner case handling:

	 * 1. When a new swap-cache page is read into, it is added to the LRU

	 * and treated as swapcache but it has no rmap yet.

	 * Calling try_to_unmap() against a page->mapping==NULL page will

	 * trigger a BUG.  So handle it here.

	 * 2. An orphaned page (see truncate_cleanup_page) might have

	 * fs-private metadata. The page can be picked up due to memory

	 * offlining.  Everywhere else except page reclaim, the page is

	 * invisible to the vm, so the page can not be migrated.  So try to

	 * free the metadata, so the page can be freed.

 Establish migration ptes */

 Drop an anon_vma reference if we took one */

	/*

	 * If migration is successful, decrease refcount of the newpage

	 * which will not free the page because new page owner increased

	 * refcounter. As well, if it is LRU page, add the page to LRU

	 * list in here. Use the old state of the isolated source page to

	 * determine if we migrated a LRU page. newpage was already unlocked

	 * and possibly modified by its owner - don't rely on the page

	 * state.

/*

 * node_demotion[] example:

 *

 * Consider a system with two sockets.  Each socket has

 * three classes of memory attached: fast, medium and slow.

 * Each memory class is placed in its own NUMA node.  The

 * CPUs are placed in the node with the "fast" memory.  The

 * 6 NUMA nodes (0-5) might be split among the sockets like

 * this:

 *

 *	Socket A: 0, 1, 2

 *	Socket B: 3, 4, 5

 *

 * When Node 0 fills up, its memory should be migrated to

 * Node 1.  When Node 1 fills up, it should be migrated to

 * Node 2.  The migration path start on the nodes with the

 * processors (since allocations default to this node) and

 * fast memory, progress through medium and end with the

 * slow memory:

 *

 *	0 -> 1 -> 2 -> stop

 *	3 -> 4 -> 5 -> stop

 *

 * This is represented in the node_demotion[] like this:

 *

 *	{  1, // Node 0 migrates to 1

 *	   2, // Node 1 migrates to 2

 *	  -1, // Node 2 does not migrate

 *	   4, // Node 3 migrates to 4

 *	   5, // Node 4 migrates to 5

 *	  -1} // Node 5 does not migrate

/*

 * Writes to this array occur without locking.  Cycles are

 * not allowed: Node X demotes to Y which demotes to X...

 *

 * If multiple reads are performed, a single rcu_read_lock()

 * must be held over all reads to ensure that no cycles are

 * observed.

/**

 * next_demotion_node() - Get the next node in the demotion path

 * @node: The starting node to lookup the next node

 *

 * Return: node id for next memory node in the demotion path hierarchy

 * from @node; NUMA_NO_NODE if @node is terminal.  This does not keep

 * @node online or guarantee that it *continues* to be the next demotion

 * target.

	/*

	 * node_demotion[] is updated without excluding this

	 * function from running.  RCU doesn't provide any

	 * compiler barriers, so the READ_ONCE() is required

	 * to avoid compiler reordering or read merging.

	 *

	 * Make sure to use RCU over entire code blocks if

	 * node_demotion[] reads need to be consistent.

/*

 * Obtain the lock on page, remove all ptes and migrate the page

 * to the newly allocated page in newpage.

 page was freed from under us. So we are done. */

		/*

		 * A page that has been migrated has all references

		 * removed and will be freed. A page that has not been

		 * migrated will have kept its references and be restored.

	/*

	 * If migration is successful, releases reference grabbed during

	 * isolation. Otherwise, restore the page to right list unless

	 * we want to retry.

		/*

		 * Compaction can migrate also non-LRU pages which are

		 * not accounted to NR_ISOLATED_*. They can be recognized

		 * as __PageMovable

			/*

			 * We release the page in page_handle_poison.

/*

 * Counterpart of unmap_and_move_page() for hugepage migration.

 *

 * This function doesn't wait the completion of hugepage I/O

 * because there is no race between I/O and migration for hugepage.

 * Note that currently hugepage I/O occurs only in direct I/O

 * where no lock is held and PG_writeback is irrelevant,

 * and writeback status of all subpages are counted in the reference

 * count of the head page (i.e. if all subpages of a 2MB hugepage are

 * under direct I/O, the reference of the head page is 512 and a bit more.)

 * This means that when we try to migrate hugepage whose subpages are

 * doing direct I/O, some references remain after try_to_unmap() and

 * hugepage migration fails without data corruption.

 *

 * There is also no race when direct I/O is issued on the page under migration,

 * because then pte is replaced with migration swap entry and direct I/O code

 * will wait in the page fault for migration to complete.

	/*

	 * Migratability of hugepages depends on architectures and their size.

	 * This check is necessary because some callers of hugepage migration

	 * like soft offline and memory hotremove don't walk through page

	 * tables or check whether the hugepage is pmd-based or not before

	 * kicking migration.

 page was freed from under us. So we are done. */

	/*

	 * Check for pages which are in the process of being freed.  Without

	 * page_mapping() set, hugetlbfs specific move page routine will not

	 * be called and we could leak usage counts for subpools.

			/*

			 * In shared mappings, try_to_unmap could potentially

			 * call huge_pmd_unshare.  Because of this, take

			 * semaphore in write mode here and set TTU_RMAP_LOCKED

			 * to let lower levels know we have taken the lock.

	/*

	 * If migration was not successful and there's a freeing callback, use

	 * it.  Otherwise, put_page() will drop the reference grabbed during

	 * isolation.

/*

 * migrate_pages - migrate the pages specified in a list, to the free pages

 *		   supplied as the target for the page migration

 *

 * @from:		The list of pages to be migrated.

 * @get_new_page:	The function used to allocate free pages to be used

 *			as the target of the page migration.

 * @put_new_page:	The function used to free target pages if migration

 *			fails, or NULL if no special handling is necessary.

 * @private:		Private data to be passed on to get_new_page()

 * @mode:		The migration mode that specifies the constraints for

 *			page migration, if any.

 * @reason:		The reason for page migration.

 * @ret_succeeded:	Set to the number of pages migrated successfully if

 *			the caller passes a non-NULL pointer.

 *

 * The function returns after 10 attempts or if no pages are movable any more

 * because the list has become empty or no retryable pages exist any more.

 * It is caller's responsibility to call putback_movable_pages() to return pages

 * to the LRU or free list only if ret != 0.

 *

 * Returns the number of pages that were not migrated, or an error code.

			/*

			 * THP statistics is based on the source huge page.

			 * Capture required information that might get lost

			 * during migration.

			/*

			 * The rules are:

			 *	Success: non hugetlb page will be freed, hugetlb

			 *		 page will be put back

			 *	-EAGAIN: stay on the from list

			 *	-ENOMEM: stay on the from list

			 *	Other errno: put on ret_pages list then splice to

			 *		     from list

			/*

			 * THP migration might be unsupported or the

			 * allocation could've failed so we should

			 * retry on the same page with the THP split

			 * to base pages.

			 *

			 * Head page is retried immediately and tail

			 * pages are added to the tail of the list so

			 * we encounter them after the rest of the list

			 * is processed.

 THP migration is unsupported */

 Hugetlb migration is unsupported */

				/*

				 * When memory is low, don't bother to try to migrate

				 * other pages, just exit.

				 * THP NUMA faulting doesn't split THP to retry.

				/*

				 * Permanent failure (-EBUSY, etc.):

				 * unlike -EAGAIN case, the failed page is

				 * removed from migration page list and not

				 * retried in the next outer loop.

	/*

	 * Put the permanent failure page back to migration list, they

	 * will be put back to the right list by the caller.

		/*

		 * clear __GFP_RECLAIM to make the migration callback

		 * consistent with regular THP allocations.

/*

 * Resolves the given address to a struct page, isolates it from the LRU and

 * puts it to the given pagelist.

 * Returns:

 *     errno - if the page cannot be found/isolated

 *     0 - when it doesn't have to be migrated because it is already on the

 *         target node

 *     1 - when it has been queued

 FOLL_DUMP to ignore special (like zero) pages */

	/*

	 * Either remove the duplicate refcount from

	 * isolate_lru_page() or drop the page ref if it was

	 * not isolated.

		/*

		 * Positive err means the number of failed

		 * pages to migrate.  Since we are going to

		 * abort and return the number of non-migrated

		 * pages, so need to include the rest of the

		 * nr_pages that have not been attempted as

		 * well.

/*

 * Migrate an array of page address onto an array of nodes and fill

 * the corresponding array of status.

		/*

		 * Errors in the page lookup or isolation are not fatal and we simply

		 * report them via status

 The page is successfully queued for migration */

		/*

		 * If the page is already on the target node (!err), store the

		 * node, otherwise, store the err.

 Make sure we do not overwrite the existing error */

/*

 * Determine the nodes of an array of pages and store it in an array of status.

 FOLL_DUMP to ignore special (like zero) pages */

/*

 * Determine the nodes of a user array of pages and store it in

 * a user array of status.

	/*

	 * There is no need to check if current process has the right to modify

	 * the specified process when they are same.

 Find the mm_struct */

	/*

	 * Check if this process has the right to modify the specified

	 * process. Use the regular "ptrace_may_access()" checks.

/*

 * Move a list of pages in the address space of the currently executing

 * process.

 Check flags */

/*

 * Returns true if this is a safe migration target node for misplaced NUMA

 * pages. Currently it only checks the watermarks which crude

 Avoid waking kswapd by allocating pages_to_migrate pages. */

 Do not migrate THP mapped by multiple processes */

 Avoid migrating to a node that is nearly full */

	/*

	 * Isolating the page has taken another reference, so the

	 * caller's reference can be safely dropped without the page

	 * disappearing underneath us during migration.

/*

 * Attempt to migrate a misplaced page to the specified destination

 * node. Caller is expected to have an elevated reference count on

 * the page that will be dropped by this function before returning.

	/*

	 * PTE mapped THP or HugeTLB page can't reach here so the page could

	 * be either base page or THP.  And it must be head page if it is

	 * THP.

	/*

	 * Don't migrate file pages that are mapped in multiple processes

	 * with execute permissions as they are probably shared libraries.

	/*

	 * Also do not migrate dirty pages as not all filesystems can move

	 * dirty pages in MIGRATE_ASYNC mode which is a waste of cycles.

 CONFIG_NUMA_BALANCING */

 CONFIG_NUMA */

 Only allow populating anonymous memory. */

			/*

			 * Only care about unaddressable device page special

			 * page table entry. Other special swap entries are not

			 * migratable, and we ignore regular swapped page.

 FIXME support THP */

		/*

		 * By getting a reference on the page we pin it and that blocks

		 * any kind of migration. Side effect is that it "freezes" the

		 * pte.

		 *

		 * We drop this reference after isolating the page from the lru

		 * for non device page (device page are not on the lru and thus

		 * can't be dropped from it).

		/*

		 * Optimize for the common case where page is only mapped once

		 * in one process. If we can lock the page, then we can safely

		 * set up a special migration page table entry now.

 Setup special migration page table entry */

			/*

			 * This is like regular unmap: we remove the rmap and

			 * drop page refcount. Page won't be freed, as we took

			 * a reference just above.

 Only flush the TLB if we actually modified any entries */

/*

 * migrate_vma_collect() - collect pages over a range of virtual addresses

 * @migrate: migrate struct containing all migration information

 *

 * This will walk the CPU page table. For each virtual address backed by a

 * valid page, it updates the src array and takes a reference on the page, in

 * order to pin the page until we lock it and unmap it.

	/*

	 * Note that the pgmap_owner is passed to the mmu notifier callback so

	 * that the registered device driver can skip invalidating device

	 * private page mappings that won't be migrated.

/*

 * migrate_vma_check_page() - check if page is pinned or not

 * @page: struct page to check

 *

 * Pinned pages cannot be migrated. This is the same test as in

 * folio_migrate_mapping(), except that here we allow migration of a

 * ZONE_DEVICE page.

	/*

	 * One extra ref because caller holds an extra reference, either from

	 * isolate_lru_page() for a regular page, or migrate_vma_collect() for

	 * a device page.

	/*

	 * FIXME support THP (transparent huge page), it is bit more complex to

	 * check them than regular pages, because they can be mapped with a pmd

	 * or with a pte (split pte mapping).

 Page from ZONE_DEVICE have one extra reference */

		/*

		 * Private page can never be pin as they have no valid pte and

		 * GUP will fail for those. Yet if there is a pending migration

		 * a thread might try to wait on the pte migration entry and

		 * will bump the page reference count. Sadly there is no way to

		 * differentiate a regular pin from migration wait. Hence to

		 * avoid 2 racing thread trying to migrate back to CPU to enter

		 * infinite loop (one stopping migration because the other is

		 * waiting on pte migration entry). We always return true here.

		 *

		 * FIXME proper solution is to rework migration_entry_wait() so

		 * it does not need to take a reference on page.

 For file back page */

/*

 * migrate_vma_unmap() - replace page mapping with special migration pte entry

 * @migrate: migrate struct containing all migration information

 *

 * Isolate pages from the LRU and replace mappings (CPU page table pte) with a

 * special migration pte entry and check if it has been pinned. Pinned pages are

 * restored because we cannot migrate them.

 *

 * This is the last step before we call the device driver callback to allocate

 * destination memory and copy contents of original page over to new page.

 ZONE_DEVICE pages are not on LRU */

 Drain CPU's pagevec */

 Drop the reference we took in collect */

/**

 * migrate_vma_setup() - prepare to migrate a range of memory

 * @args: contains the vma, start, and pfns arrays for the migration

 *

 * Returns: negative errno on failures, 0 when 0 or more pages were migrated

 * without an error.

 *

 * Prepare to migrate a range of memory virtual address range by collecting all

 * the pages backing each virtual address in the range, saving them inside the

 * src array.  Then lock those pages and unmap them. Once the pages are locked

 * and unmapped, check whether each page is pinned or not.  Pages that aren't

 * pinned have the MIGRATE_PFN_MIGRATE flag set (by this function) in the

 * corresponding src array entry.  Then restores any pages that are pinned, by

 * remapping and unlocking those pages.

 *

 * The caller should then allocate destination memory and copy source memory to

 * it for all those entries (ie with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE

 * flag set).  Once these are allocated and copied, the caller must update each

 * corresponding entry in the dst array with the pfn value of the destination

 * page and with MIGRATE_PFN_VALID. Destination pages must be locked via

 * lock_page().

 *

 * Note that the caller does not have to migrate all the pages that are marked

 * with MIGRATE_PFN_MIGRATE flag in src array unless this is a migration from

 * device memory to system memory.  If the caller cannot migrate a device page

 * back to system memory, then it must return VM_FAULT_SIGBUS, which has severe

 * consequences for the userspace process, so it must be avoided if at all

 * possible.

 *

 * For empty entries inside CPU page table (pte_none() or pmd_none() is true) we

 * do set MIGRATE_PFN_MIGRATE flag inside the corresponding source array thus

 * allowing the caller to allocate device memory for those unbacked virtual

 * addresses.  For this the caller simply has to allocate device memory and

 * properly set the destination entry like for regular migration.  Note that

 * this can still fail, and thus inside the device driver you must check if the

 * migration was successful for those entries after calling migrate_vma_pages(),

 * just like for regular migration.

 *

 * After that, the callers must call migrate_vma_pages() to go over each entry

 * in the src array that has the MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag

 * set. If the corresponding entry in dst array has MIGRATE_PFN_VALID flag set,

 * then migrate_vma_pages() to migrate struct page information from the source

 * struct page to the destination struct page.  If it fails to migrate the

 * struct page information, then it clears the MIGRATE_PFN_MIGRATE flag in the

 * src array.

 *

 * At this point all successfully migrated pages have an entry in the src

 * array with MIGRATE_PFN_VALID and MIGRATE_PFN_MIGRATE flag set and the dst

 * array entry with MIGRATE_PFN_VALID flag set.

 *

 * Once migrate_vma_pages() returns the caller may inspect which pages were

 * successfully migrated, and which were not.  Successfully migrated pages will

 * have the MIGRATE_PFN_MIGRATE flag set for their src array entry.

 *

 * It is safe to update device page table after migrate_vma_pages() because

 * both destination and source page are still locked, and the mmap_lock is held

 * in read mode (hence no one can unmap the range being migrated).

 *

 * Once the caller is done cleaning up things and updating its page table (if it

 * chose to do so, this is not an obligation) it finally calls

 * migrate_vma_finalize() to update the CPU page table to point to new pages

 * for successfully migrated pages or otherwise restore the CPU page table to

 * point to the original source pages.

	/*

	 * At this point pages are locked and unmapped, and thus they have

	 * stable content and can safely be copied to destination memory that

	 * is allocated by the drivers.

/*

 * This code closely matches the code in:

 *   __handle_mm_fault()

 *     handle_pte_fault()

 *       do_anonymous_page()

 * to map in an anonymous zero page but the struct page will be a ZONE_DEVICE

 * private page.

 Only allow populating anonymous memory */

	/*

	 * Use pte_alloc() instead of pte_alloc_map().  We can't run

	 * pte_offset_map() on pmds where a huge pmd might be created

	 * from a different thread.

	 *

	 * pte_alloc_map() is safe to use under mmap_write_lock(mm) or when

	 * parallel threads are excluded by other means.

	 *

	 * Here we only have mmap_read_lock(mm).

 See the comment in pte_alloc_one_map() */

	/*

	 * The memory barrier inside __SetPageUptodate makes sure that

	 * preceding stores to the page contents become visible before

	 * the set_pte_at() write.

			/*

			 * For now we only support migrating to un-addressable

			 * device memory.

	/*

	 * Check for userfaultfd but do not deliver the fault. Instead,

	 * just back off.

 No need to invalidate - it was non-present before */

/**

 * migrate_vma_pages() - migrate meta-data from src page to dst page

 * @migrate: migrate struct containing all migration information

 *

 * This migrates struct page meta-data from source struct page to destination

 * struct page. This effectively finishes the migration from source page to the

 * destination page.

				/*

				 * For now only support private anonymous when

				 * migrating to un-addressable device memory.

				/*

				 * Other types of ZONE_DEVICE page are not

				 * supported.

	/*

	 * No need to double call mmu_notifier->invalidate_range() callback as

	 * the above ptep_clear_flush_notify() inside migrate_vma_insert_page()

	 * did already call it.

/**

 * migrate_vma_finalize() - restore CPU page table entry

 * @migrate: migrate struct containing all migration information

 *

 * This replaces the special migration pte entry with either a mapping to the

 * new page if migration was successful for that page, or to the original page

 * otherwise.

 *

 * This also unlocks the pages and puts them back on the lru, or drops the extra

 * refcount, for device pages.

 CONFIG_DEVICE_PRIVATE */

 Disable reclaim-based migration. */

	/*

	 * Ensure that the "disable" is visible across the system.

	 * Readers will see either a combination of before+disable

	 * state or disable+after.  They will never see before and

	 * after state together.

	 *

	 * The before+after state together might have cycles and

	 * could cause readers to do things like loop until this

	 * function finishes.  This ensures they can only see a

	 * single "bad" read and would, for instance, only loop

	 * once.

/*

 * Find an automatic demotion target for 'node'.

 * Failing here is OK.  It might just indicate

 * being at the end of a chain.

	/*

	 * Can not set a migration target on a

	 * node with it already set.

	 *

	 * No need for READ_ONCE() here since this

	 * in the write path for node_demotion[].

	 * This should be the only thread writing.

/*

 * When memory fills up on a node, memory contents can be

 * automatically migrated to another node instead of

 * discarded at reclaim.

 *

 * Establish a "migration path" which will start at nodes

 * with CPUs and will follow the priorities used to build the

 * page allocator zonelists.

 *

 * The difference here is that cycles must be avoided.  If

 * node0 migrates to node1, then neither node1, nor anything

 * node1 migrates to can migrate to node0.

 *

 * This function can run simultaneously with readers of

 * node_demotion[].  However, it can not run simultaneously

 * with itself.  Exclusion is provided by memory hotplug events

 * being single-threaded.

	/*

	 * Avoid any oddities like cycles that could occur

	 * from changes in the topology.  This will leave

	 * a momentary gap when migration is disabled.

	/*

	 * Allocations go close to CPUs, first.  Assume that

	 * the migration path starts at the nodes with CPUs.

	/*

	 * To avoid cycles in the migration "graph", ensure

	 * that migration sources are not future targets by

	 * setting them in 'used_targets'.  Do this only

	 * once per pass so that multiple source nodes can

	 * share a target node.

	 *

	 * 'used_targets' will become unavailable in future

	 * passes.  This limits some opportunities for

	 * multiple source nodes to share a destination.

		/*

		 * Visit targets from this pass in the next pass.

		 * Eventually, every node will have been part of

		 * a pass, and will become set in 'used_targets'.

	/*

	 * 'next_pass' contains nodes which became migration

	 * targets in this pass.  Make additional passes until

	 * no more migrations targets are available.

/*

 * For callers that do not hold get_online_mems() already.

/*

 * This leaves migrate-on-reclaim transiently disabled between

 * the MEM_GOING_OFFLINE and MEM_OFFLINE events.  This runs

 * whether reclaim-based migration is enabled or not, which

 * ensures that the user can turn reclaim-based migration at

 * any time without needing to recalculate migration targets.

 *

 * These callbacks already hold get_online_mems().  That is why

 * __set_migration_target_nodes() can be used as opposed to

 * set_migration_target_nodes().

	/*

	 * Only update the node migration order when a node is

	 * changing status, like online->offline.  This avoids

	 * the overhead of synchronize_rcu() in most cases.

		/*

		 * Make sure there are not transient states where

		 * an offline node is a migration target.  This

		 * will leave migration disabled until the offline

		 * completes and the MEM_OFFLINE case below runs.

		/*

		 * Recalculate the target nodes once the node

		 * reaches its final state (online or offline).

		/*

		 * MEM_GOING_OFFLINE disabled all the migration

		 * targets.  Reenable them.

/*

 * React to hotplug events that might affect the migration targets

 * like events that online or offline NUMA nodes.

 *

 * The ordering is also currently dependent on which nodes have

 * CPUs.  That means we need CPU on/offline notification too.

	/*

	 * In the unlikely case that this fails, the automatic

	 * migration targets may become suboptimal for nodes

	 * where N_CPU changes.  With such a small impact in a

	 * rare case, do not bother trying to do anything special.

 CONFIG_HOTPLUG_CPU */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * rodata_test.c: functional test for mark_rodata_ro function

 *

 * (C) Copyright 2008 Intel Corporation

 * Author: Arjan van de Ven <arjan@linux.intel.com>

 test 1: read the value */

 If this test fails, some previous testrun has clobbered the state */

 test 2: write to the variable; this should fault */

 test 3: check the value hasn't changed */

 test 4: check if the rodata section is PAGE_SIZE aligned */

 SPDX-License-Identifier: GPL-2.0

/*

 * struct page extension

 *

 * This is the feature to manage memory for extended data per page.

 *

 * Until now, we must modify struct page itself to store extra data per page.

 * This requires rebuilding the kernel and it is really time consuming process.

 * And, sometimes, rebuild is impossible due to third party module dependency.

 * At last, enlarging struct page could cause un-wanted system behaviour change.

 *

 * This feature is intended to overcome above mentioned problems. This feature

 * allocates memory for extended data per page in certain place rather than

 * the struct page itself. This memory can be accessed by the accessor

 * functions provided by this code. During the boot process, it checks whether

 * allocation of huge chunk of memory is needed or not. If not, it avoids

 * allocating memory at all. With this advantage, we can include this feature

 * into the kernel in default and can avoid rebuild and solve related problems.

 *

 * To help these things to work well, there are two callbacks for clients. One

 * is the need callback which is mandatory if user wants to avoid useless

 * memory allocation at boot-time. The other is optional, init callback, which

 * is used to do proper initialization after memory is allocated.

 *

 * The need callback is used to decide whether extended memory allocation is

 * needed or not. Sometimes users want to deactivate some features in this

 * boot and extra memory would be unnecessary. In this case, to avoid

 * allocating huge chunk of memory, each clients represent their need of

 * extra memory through the need callback. If one of the need callbacks

 * returns true, it means that someone needs extra memory so that

 * page extension core should allocates memory for page extension. If

 * none of need callbacks return true, memory isn't needed at all in this boot

 * and page extension core can skip to allocate memory. As result,

 * none of memory is wasted.

 *

 * When need callback returns true, page_ext checks if there is a request for

 * extra memory through size in struct page_ext_operations. If it is non-zero,

 * extra space is allocated for each page_ext entry and offset is returned to

 * user through offset in struct page_ext_operations.

 *

 * The init callback is used to do proper initialization after page extension

 * is completely initialized. In sparse memory system, extra memory is

 * allocated some time later than memmap is allocated. In other words, lifetime

 * of memory for page extension isn't same with memmap for struct page.

 * Therefore, clients can't store extra data until page extension is

 * initialized, even if pages are allocated and used freely. This could

 * cause inadequate state of extra data per page, so, to prevent it, client

 * can utilize this callback to initialize the state of it correctly.

	/*

	 * The sanity checks the page allocator does upon freeing a

	 * page can reach here before the page_ext arrays are

	 * allocated when feeding a range of pages to the allocator

	 * for the first time during bootup or memory hotplug.

	/*

	 * Need extra space if node range is not aligned with

	 * MAX_ORDER_NR_PAGES. When page allocator's buddy algorithm

	 * checks buddy's status, range could be out of exact node range.

 CONFIG_SPARSEMEM */

	/*

	 * The sanity checks the page allocator does upon freeing a

	 * page can reach here before the page_ext arrays are

	 * allocated when feeding a range of pages to the allocator

	 * for the first time during bootup or memory hotplug.

	/*

	 * The value stored in section->page_ext is (base - pfn)

	 * and it does not point to the memory block allocated above,

	 * causing kmemleak false positives.

	/*

	 * The passed "pfn" may not be aligned to SECTION.  For the calculation

	 * we need to apply a mask.

		/*

		 * In this case, "nid" already exists and contains valid memory.

		 * "start_pfn" passed to us is a pfn which is an arg for

		 * online__pages(), and start_pfn should exist.

 rollback */

		/*

		 * start_pfn and end_pfn may not be aligned to SECTION and the

		 * page->flags of out of node pages are not initialized.  So we

		 * scan [start_pfn, the biggest section's pfn < end_pfn) here.

			/*

			 * Nodes's pfns can be overlapping.

			 * We know some arch can have a nodes layout such as

			 * -------------pfn-------------->

			 * N0 | N1 | N2 | N0 | N1 | N2|....

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/mm/page_isolation.c

	/*

	 * We assume the caller intended to SET migrate type to isolate.

	 * If it is already set, then someone else must have raced and

	 * set it before us.

	/*

	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.

	 * We just check MOVABLE pages.

		/*

		 * printk() with zone->lock held will likely trigger a

		 * lockdep splat, so defer it here.

	/*

	 * Because freepage with more than pageblock_order on isolated

	 * pageblock is restricted to merge due to freepage counting problem,

	 * it is possible that there is free buddy page.

	 * move_freepages_block() doesn't care of merge so we need other

	 * approach in order to merge them. Isolation and free will make

	 * these pages to be merged.

				/*

				 * Isolating a free page in an isolated pageblock

				 * is expected to always work as watermarks don't

				 * apply here.

	/*

	 * If we isolate freepage with more than pageblock_order, there

	 * should be no freepage in the range, so we could avoid costly

	 * pageblock scanning for freepage moving.

	 *

	 * We didn't actually touch any of the isolated pages, so place them

	 * to the tail of the freelist. This is an optimization for memory

	 * onlining - just onlined memory won't immediately be considered for

	 * allocation.

/**

 * start_isolate_page_range() - make page-allocation-type of range of pages to

 * be MIGRATE_ISOLATE.

 * @start_pfn:		The lower PFN of the range to be isolated.

 * @end_pfn:		The upper PFN of the range to be isolated.

 *			start_pfn/end_pfn must be aligned to pageblock_order.

 * @migratetype:	Migrate type to set in error recovery.

 * @flags:		The following flags are allowed (they can be combined in

 *			a bit mask)

 *			MEMORY_OFFLINE - isolate to offline (!allocate) memory

 *					 e.g., skip over PageHWPoison() pages

 *					 and PageOffline() pages.

 *			REPORT_FAILURE - report details about the failure to

 *			isolate the range

 *

 * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in

 * the range will never be allocated. Any free pages and pages freed in the

 * future will not be allocated again. If specified range includes migrate types

 * other than MOVABLE or CMA, this will fail with -EBUSY. For isolating all

 * pages in the range finally, the caller have to free all pages in the range.

 * test_page_isolated() can be used for test it.

 *

 * There is no high level synchronization mechanism that prevents two threads

 * from trying to isolate overlapping ranges. If this happens, one thread

 * will notice pageblocks in the overlapping range already set to isolate.

 * This happens in set_migratetype_isolate, and set_migratetype_isolate

 * returns an error. We then clean up by restoring the migration type on

 * pageblocks we may have modified and return -EBUSY to caller. This

 * prevents two threads from simultaneously working on overlapping ranges.

 *

 * Please note that there is no strong synchronization with the page allocator

 * either. Pages might be freed while their page blocks are marked ISOLATED.

 * A call to drain_all_pages() after isolation can flush most of them. However

 * in some cases pages might still end up on pcp lists and that would allow

 * for their allocation even when they are in fact isolated already. Depending

 * on how strong of a guarantee the caller needs, zone_pcp_disable/enable()

 * might be used to flush and disable pcplist before isolation and enable after

 * unisolation.

 *

 * Return: 0 on success and -EBUSY if any part of range cannot be isolated.

/*

 * Make isolated pages available again.

/*

 * Test all pages in the range is free(means isolated) or not.

 * all pages in [start_pfn...end_pfn) must be in the same zone.

 * zone->lock must be held before call this.

 *

 * Returns the last tested pfn.

			/*

			 * If the page is on a free list, it has to be on

			 * the correct MIGRATE_ISOLATE freelist. There is no

			 * simple way to verify that as VM_BUG_ON(), though.

 A HWPoisoned page cannot be also PageBuddy */

			/*

			 * The responsible driver agreed to skip PageOffline()

			 * pages when offlining memory by dropping its

			 * reference in MEM_GOING_OFFLINE.

 Caller should ensure that requested range is in a single zone */

	/*

	 * Note: pageblock_nr_pages != MAX_ORDER. Then, chunks of free pages

	 * are not aligned to pageblock_nr_pages.

	 * Then we just check migratetype first.

 Check all pages are free or marked as ISOLATED */

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2015 Intel Corporation. All rights reserved. */

/*

 * The memremap() and memremap_pages() interfaces are alternately used

 * to map persistent memory namespaces. These interfaces place different

 * constraints on the alignment and size of the mapping (namespace).

 * memremap() can map individual PAGE_SIZE pages. memremap_pages() can

 * only map subsections (2MB), and at least one architecture (PowerPC)

 * the minimum mapping granularity of memremap_pages() is 16MB.

 *

 * The role of memremap_compat_align() is to communicate the minimum

 * arch supported alignment of a namespace such that it can freely

 * switch modes without violating the arch constraint. Namely, do not

 * allow a namespace to be PAGE_SIZE aligned since that namespace may be

 * reconfigured into a mode that requires SUBSECTION_SIZE alignment.

 CONFIG_DEV_PAGEMAP_OPS */

	/*

	 * Undo the pgmap ref assignment for the internal case as the

	 * caller may re-enable the same pgmap.

 make sure to access a memmap that was actually initialized */

 pages are dead and unused, undo the arch mapping */

	/*

	 * For device private memory we call add_pages() as we only need to

	 * allocate and initialize struct page for the device memory. More-

	 * over the device memory is un-accessible thus we do not want to

	 * create a linear mapping for the memory like arch_add_memory()

	 * would do.

	 *

	 * For all other device memory types, which are accessible by

	 * the CPU, we do want the linear mapping and thus use

	 * arch_add_memory().

	/*

	 * Initialization of the pages has been deferred until now in order

	 * to allow us to do the work while not holding the hotplug lock.

/*

 * Not device managed version of dev_memremap_pages, undone by

 * memunmap_pages().  Please use dev_memremap_pages if you have a struct

 * device available.

	/*

	 * Clear the pgmap nr_range as it will be incremented for each

	 * successfully processed range. This communicates how many

	 * regions to unwind in the abort case.

/**

 * devm_memremap_pages - remap and provide memmap backing for the given resource

 * @dev: hosting device for @res

 * @pgmap: pointer to a struct dev_pagemap

 *

 * Notes:

 * 1/ At a minimum the res and type members of @pgmap must be initialized

 *    by the caller before passing it to this function

 *

 * 2/ The altmap field may optionally be initialized, in which case

 *    PGMAP_ALTMAP_VALID must be set in pgmap->flags.

 *

 * 3/ The ref field may optionally be provided, in which pgmap->ref must be

 *    'live' on entry and will be killed and reaped at

 *    devm_memremap_pages_release() time, or if this routine fails.

 *

 * 4/ range is expected to be a host memory range that could feasibly be

 *    treated as a "System RAM" range, i.e. not a device mmio range, but

 *    this is not enforced.

 number of pfns from base where pfn_to_page() is valid */

/**

 * get_dev_pagemap() - take a new live reference on the dev_pagemap for @pfn

 * @pfn: page frame number to lookup page_map

 * @pgmap: optional known pgmap that already has a reference

 *

 * If @pgmap is non-NULL and covers @pfn it will be returned as-is.  If @pgmap

 * is non-NULL but does not cover @pfn the reference to it will be released.

	/*

	 * In the cached case we're already holding a live reference.

 fall back to slow path lookup */

 notify page idle for dax */

	/*

	 * When a device_private page is freed, the page->mapping field

	 * may still contain a (stale) mapping value. For example, the

	 * lower bits of page->mapping may still identify the page as an

	 * anonymous page. Ultimately, this entire field is just stale

	 * and wrong, and it will cause errors if not cleared.  One

	 * example is:

	 *

	 *  migrate_vma_pages()

	 *    migrate_vma_insert_page()

	 *      page_add_new_anon_rmap()

	 *        __page_set_anon_rmap()

	 *          ...checks page->mapping, via PageAnon(page) call,

	 *            and incorrectly concludes that the page is an

	 *            anonymous page. Therefore, it incorrectly,

	 *            silently fails to set up the new anon rmap.

	 *

	 * For other types of ZONE_DEVICE pages, migration is either

	 * handled differently or not done at all, so there is no need

	 * to clear page->mapping.

 CONFIG_DEV_PAGEMAP_OPS */

 SPDX-License-Identifier: GPL-2.0

/*

 * sparse memory mappings.

/*

 * Permanent SPARSEMEM data:

 *

 * 1) mem_section	- memory sections, mem_map's for valid memory

/*

 * If we did not store the node number in the page then we have to

 * do a lookup in the section_to_node_table in order to find which

 * node the page belongs to.

 !NODE_NOT_IN_PAGE_FLAGS */

	/*

	 * An existing section is possible in the sub-section hotplug

	 * case. First hot-add instantiates, follow-on hot-add reuses

	 * the existing section.

	 *

	 * The mem_hotplug_lock resolves the apparent race below.

 !SPARSEMEM_EXTREME */

/*

 * During early boot, before section_mem_map is used for an actual

 * mem_map, we use section_mem_map to store the section's NUMA

 * node.  This keeps us from having to use another data structure.  The

 * node information is cleared just before we store the real mem_map.

 Validate the physical addressing limitations of the model */

	/*

	 * Sanity checks - do not allow an architecture to pass

	 * in larger pfns than the maximum scope of sparsemem:

/*

 * There are a number of times that we loop over NR_MEM_SECTIONS,

 * looking for section_present() on each.  But, when we have very

 * large physical address spaces, NR_MEM_SECTIONS can also be

 * very large which makes the loops quite long.

 *

 * Keeping track of this gives us an easy way to break out of

 * those loops early.

 Record a memory area against a node. */

/*

 * Mark all memblocks as present using memory_present().

 * This is a convenience function that is useful to mark all of the systems

 * memory as present during initialization.

/*

 * Subtle, we encode the real pfn into the mem_map such that

 * the identity pfn - section_mem_map will return the actual

 * physical page frame number.

/*

 * Decode mem_map from the coded memmap

 mask off the extra low bits of information */

 CONFIG_MEMORY_HOTPLUG */

	/*

	 * A page may contain usemaps for other sections preventing the

	 * page being freed and making a section unremovable while

	 * other sections referencing the usemap remain active. Similarly,

	 * a pgdat can prevent a section being removed. If section A

	 * contains a pgdat and section B contains the usemap, both

	 * sections become inter-dependent. This allocates usemaps

	 * from the same section as the pgdat where possible to avoid

	 * this problem.

 First call */

 skip redundant message */

	/*

	 * There is a circular dependency.

	 * Some platforms allow un-removable section because they will just

	 * gather other removable sections for dynamic partitioning.

	 * Just notify un-removable section's number here.

 CONFIG_MEMORY_HOTREMOVE */

 !CONFIG_SPARSEMEM_VMEMMAP */

 forgot to call sparse_buffer_fini()? */

	/*

	 * Pre-allocated buffer is mainly used by __populate_section_memmap

	 * and we want it to be properly aligned to the section size - this is

	 * especially the case for VMEMMAP which maps memmap to PMDs

 Free redundant aligned space */

/*

 * Initialize sparse on a specific node. The node spans [pnum_begin, pnum_end)

 * And number of present sections in this node is map_count.

 We failed to allocate, mark all the following pnums as not present */

/*

 * Allocate the accumulated non-linear sections, allocate a mem_map

 * for each and record the physical to section mapping.

 Setup pageblock_order for HUGETLB_PAGE_SIZE_VARIABLE */

 Init node with sections in range [pnum_begin, pnum_end) */

 cover the last node */

 Mark all memory sections within the pfn range as online */

 onlining code should never touch invalid ranges */

 Mark all memory sections within the pfn range as offline */

		/*

		 * TODO this needs some double checking. Offlining code makes

		 * sure to check pfn_valid but those checks might be just bogus

		/*

		 * When this function is called, the removing section is

		 * logical offlined state. This means all pages are isolated

		 * from page allocator. If removing section's memmap is placed

		 * on the same section, it must not be freed.

		 * If it is freed, page allocator may allocate it which will

		 * be removed physically soon.

 CONFIG_SPARSEMEM_VMEMMAP */

/*

 * To deactivate a memory region, there are 3 cases to handle across

 * two configurations (SPARSEMEM_VMEMMAP={y,n}):

 *

 * 1. deactivation of a partial hot-added section (only possible in

 *    the SPARSEMEM_VMEMMAP=y case).

 *      a) section was present at memory init.

 *      b) section was hot-added post memory init.

 * 2. deactivation of a complete hot-added section.

 * 3. deactivation of a complete section from memory init.

 *

 * For 1, when subsection_map does not empty we will not be freeing the

 * usage map, but still need to free the vmemmap range.

 *

 * For 2 and 3, the SPARSEMEM_VMEMMAP={y,n} cases are unified

		/*

		 * When removing an early section, the usage map is kept (as the

		 * usage maps of other sections fall into the same page). It

		 * will be re-used when re-adding the section - which is then no

		 * longer an early section. If the usage map is PageReserved, it

		 * was allocated during boot.

		/*

		 * Mark the section invalid so that valid_section()

		 * return false. This prevents code from dereferencing

		 * ms->usage array.

	/*

	 * The memmap of early sections is always fully populated. See

	 * section_activate() and pfn_valid() .

	/*

	 * The early init code does not consider partially populated

	 * initial sections, it simply assumes that memory will never be

	 * referenced.  If we hot-add memory into such a section then we

	 * do not need to populate the memmap and can simply reuse what

	 * is already there.

/**

 * sparse_add_section - add a memory section, or populate an existing one

 * @nid: The node to add section on

 * @start_pfn: start pfn of the memory range

 * @nr_pages: number of pfns to add in the section

 * @altmap: device page map

 *

 * This is only intended for hotplug.

 *

 * Note that only VMEMMAP supports sub-section aligned hotplug,

 * the proper alignment and size are gated by check_pfn_span().

 *

 *

 * Return:

 * * 0		- On success.

 * * -EEXIST	- Section has been present.

 * * -ENOMEM	- Out of memory.

	/*

	 * Poison uninitialized struct pages in order to catch invalid flags

	 * combinations.

 Align memmap to section boundary in the subsection case */

	/*

	 * A further optimization is to have per section refcounted

	 * num_poisoned_pages.  But that would need more space per memmap, so

	 * for now just do a quick global check to speed up this routine in the

	 * absence of bad pages.

 CONFIG_MEMORY_HOTPLUG */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/oom_kill.c

 * 

 *  Copyright (C)  1998,2000  Rik van Riel

 *	Thanks go out to Claus Fischer for some serious inspiration and

 *	for goading me into coding this file...

 *  Copyright (C)  2010  Google, Inc.

 *	Rewritten by David Rientjes

 *

 *  The routines in this file are used to kill a process when

 *  we're seriously out of memory. This gets called from __alloc_pages()

 *  in mm/page_alloc.c when we really run out of memory.

 *

 *  Since we won't call these routines often (on a well-configured

 *  machine) this file will double as a 'coding guide' and a signpost

 *  for newbie kernel hackers. It features several pointers to major

 *  kernel subsystems and hints as to where to find out what things do.

/*

 * Serializes oom killer invocations (out_of_memory()) from all contexts to

 * prevent from over eager oom killing (e.g. when the oom killer is invoked

 * from different domains).

 *

 * oom_killer_disable() relies on this lock to stabilize oom_killer_disabled

 * and mark_oom_victim

 Serializes oom_score_adj and oom_score_adj_min updates */

/**

 * oom_cpuset_eligible() - check task eligibility for kill

 * @start: task struct of which task to consider

 * @oc: pointer to struct oom_control

 *

 * Task eligibility is determined by whether or not a candidate task, @tsk,

 * shares the same mempolicy nodes as current if it is bound by such a policy

 * and whether or not it has the same set of allowed cpuset nodes.

 *

 * This function is assuming oom-killer context and 'current' has triggered

 * the oom-killer.

			/*

			 * If this is a mempolicy constrained oom, tsk's

			 * cpuset is irrelevant.  Only return true if its

			 * mempolicy intersects current, otherwise it may be

			 * needlessly killed.

			/*

			 * This is not a mempolicy constrained oom, so only

			 * check the mems of tsk's cpuset.

 CONFIG_NUMA */

/*

 * The process p may have detached its own ->mm while exiting or through

 * kthread_use_mm(), but one or more of its subthreads may still have a valid

 * pointer.  Return p, or any of its subthreads with a valid ->mm, with

 * task_lock() held.

/*

 * order == -1 means the oom kill is required by sysrq, otherwise only

 * for display purposes.

 return true if the task is not adequate as candidate victim task. */

/*

 * Check whether unreclaimable slab amount is greater than

 * all user memory(LRU pages).

 * dump_unreclaimable_slab() could help in the case that

 * oom due to too much unreclaimable slab used by kernel.

/**

 * oom_badness - heuristic function to determine which candidate task to kill

 * @p: task struct of which task we should calculate

 * @totalpages: total present RAM allowed for page allocation

 *

 * The heuristic for determining which task to kill is made to be as simple and

 * predictable as possible.  The goal is to return the highest value for the

 * task consuming the most memory to avoid subsequent oom failures.

	/*

	 * Do not even consider tasks which are explicitly marked oom

	 * unkillable or have been already oom reaped or the are in

	 * the middle of vfork

	/*

	 * The baseline for the badness score is the proportion of RAM that each

	 * task's rss, pagetable and swap space use.

 Normalize to oom_score_adj units */

/*

 * Determine the type of allocation constraint.

 Default to all available memory */

	/*

	 * Reach here only when __GFP_NOFAIL is used. So, we should avoid

	 * to kill current.We have to random task kill in this case.

	 * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.

	/*

	 * This is not a __GFP_THISNODE allocation, so a truncated nodemask in

	 * the page allocator means a mempolicy is in effect.  Cpuset policy

	 * is enforced in get_page_from_freelist().

 Check this allocation failure is caused by cpuset's wall function */

 p may not have freeable memory in nodemask */

	/*

	 * This task already has access to memory reserves and is being killed.

	 * Don't allow any other task to have access to the reserves unless

	 * the task has MMF_OOM_SKIP because chances that it would release

	 * any memory is quite low.

	/*

	 * If task is allocating a lot of memory and has been marked to be

	 * killed first if it triggers an oom, then select it.

/*

 * Simple selection loop. We choose the process with the highest number of

 * 'points'. In case scan was aborted, oc->chosen is set to -1.

 p may not have freeable memory in nodemask */

		/*

		 * All of p's threads have already detached their mm's. There's

		 * no need to report them; they can't be oom killed anyway.

/**

 * dump_tasks - dump current memory state of all system tasks

 * @oc: pointer to struct oom_control

 *

 * Dumps the current memory state of all eligible tasks.  Tasks not in the same

 * memcg, not in the same cpuset, or bound to a disjoint set of mempolicy nodes

 * are not shown.

 * State information includes task's pid, uid, tgid, vm size, rss,

 * pgtables_bytes, swapents, oom_score_adj value, and name.

 one line summary of the oom killer context. */

/*

 * Number of OOM victims in flight

/*

 * task->mm can be NULL if the task is the exited group leader.  So to

 * determine whether the task is using a particular mm, we examine all the

 * task's threads: if one of those is using this mm then this task was also

 * using it.

/*

 * OOM Reaper kernel thread which tries to reap the memory used by the OOM

 * victim (if that is possible) to help the OOM killer to move on.

	/*

	 * Tell all users of get_user/copy_from_user etc... that the content

	 * is no longer stable. No barriers really needed because unmapping

	 * should imply barriers already and the reader would hit a page fault

	 * if it stumbled over a reaped memory.

		/*

		 * Only anonymous pages have a good chance to be dropped

		 * without additional steps which we cannot afford as we

		 * are OOM already.

		 *

		 * We do not even care about fs backed pages because all

		 * which are reclaimable have already been reclaimed and

		 * we do not want to block exit_mmap by keeping mm ref

		 * count elevated without a good reason.

/*

 * Reaps the address space of the give task.

 *

 * Returns true on success and false if none or part of the address space

 * has been reclaimed and the caller should retry later.

	/*

	 * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't

	 * work on the mm anymore. The check for MMF_OOM_SKIP must run

	 * under mmap_lock for reading because it serializes against the

	 * mmap_write_lock();mmap_write_unlock() cycle in exit_mmap().

 failed to reap part of the address space. Try again later */

 Retry the mmap_read_trylock(mm) a few times */

	/*

	 * Hide this mm from OOM killer because it has been either reaped or

	 * somebody can't call mmap_write_unlock(mm).

 Drop a reference taken by wake_oom_reaper */

 mm is already queued? */

 CONFIG_MMU */

/**

 * mark_oom_victim - mark the given task as OOM victim

 * @tsk: task to mark

 *

 * Has to be called with oom_lock held and never after

 * oom has been disabled already.

 *

 * tsk->mm has to be non NULL and caller has to guarantee it is stable (either

 * under task_lock or operate on the current).

 OOM killer might race with memcg OOM */

 oom_mm is bound to the signal struct life time. */

	/*

	 * Make sure that the task is woken up from uninterruptible sleep

	 * if it is frozen because OOM killer wouldn't be able to free

	 * any memory and livelock. freezing_slow_path will tell the freezer

	 * that TIF_MEMDIE tasks should be ignored.

/**

 * exit_oom_victim - note the exit of an OOM victim

/**

 * oom_killer_enable - enable OOM killer

/**

 * oom_killer_disable - disable OOM killer

 * @timeout: maximum timeout to wait for oom victims in jiffies

 *

 * Forces all page allocations to fail rather than trigger OOM killer.

 * Will block and wait until all OOM victims are killed or the given

 * timeout expires.

 *

 * The function cannot be called when there are runnable user tasks because

 * the userspace would see unexpected allocation failures as a result. Any

 * new usage of this function should be consulted with MM people.

 *

 * Returns true if successful and false if the OOM killer cannot be

 * disabled.

	/*

	 * Make sure to not race with an ongoing OOM killer. Check that the

	 * current is not killed (possibly due to sharing the victim's memory).

	/*

	 * A coredumping process may sleep for an extended period in

	 * coredump_task_exit(), so the oom killer cannot assume that

	 * the process will promptly exit and release memory.

/*

 * Checks whether the given task is dying or exiting and likely to

 * release its address space. This means that all threads and processes

 * sharing the same mm have to be killed or exiting.

 * Caller has to make sure that task->mm is stable (hold task_lock or

 * it operates on the current).

	/*

	 * Skip tasks without mm because it might have passed its exit_mm and

	 * exit_oom_victim. oom_reaper could have rescued that but do not rely

	 * on that for now. We can consider find_lock_task_mm in future.

	/*

	 * This task has already been drained by the oom reaper so there are

	 * only small chances it will free some more

	/*

	 * Make sure that all tasks which share the mm with the given tasks

	 * are dying as well to make sure that a) nobody pins its mm and

	 * b) the task is also reapable by the oom reaper.

 Get a reference to safely compare mm after task_unlock(victim) */

 Raise event before sending signal: task reaper must see this */

	/*

	 * We should send SIGKILL before granting access to memory reserves

	 * in order to prevent the OOM victim from depleting the memory

	 * reserves from the user space under its control.

	/*

	 * Kill all user processes sharing victim->mm in other thread groups, if

	 * any.  They don't get access to memory reserves, though, to avoid

	 * depletion of all memory.  This prevents mm->mmap_lock livelock when an

	 * oom killed thread cannot exit because it requires the semaphore and

	 * its contended by another thread trying to allocate memory itself.

	 * That thread will now get access to memory reserves since it has a

	 * pending fatal signal.

		/*

		 * No kthread_use_mm() user needs to read from the userspace so

		 * we are ok to reap it.

/*

 * Kill provided task unless it's secured by setting

 * oom_score_adj to OOM_SCORE_ADJ_MIN.

	/*

	 * If the task is already exiting, don't alarm the sysadmin or kill

	 * its children or threads, just give it access to memory reserves

	 * so it can die quickly

	/*

	 * Do we need to kill the entire memory cgroup?

	 * Or even one of the ancestor memory cgroups?

	 * Check this out before killing the victim task.

	/*

	 * If necessary, kill all tasks in the selected memory cgroup.

/*

 * Determines whether the kernel must panic because of the panic_on_oom sysctl.

		/*

		 * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel

		 * does not panic for cpuset, mempolicy, or memcg allocation

		 * failures.

 Do not panic for oom kills triggered by sysrq */

/**

 * out_of_memory - kill the "best" process when we run out of memory

 * @oc: pointer to struct oom_control

 *

 * If we run out of memory, we have the choice between either

 * killing a random task (bad), letting the system crash (worse)

 * OR try to be smart about which process to kill. Note that we

 * don't have to be perfect here, we just have to be good.

 Got some memory back in the last second. */

	/*

	 * If current has a pending SIGKILL or is exiting, then automatically

	 * select it.  The goal is to allow it to allocate so that it may

	 * quickly exit and free its memory.

	/*

	 * The OOM killer does not compensate for IO-less reclaim.

	 * pagefault_out_of_memory lost its gfp context so we have to

	 * make sure exclude 0 mask - all other users should have at least

	 * ___GFP_DIRECT_RECLAIM to get here. But mem_cgroup_oom() has to

	 * invoke the OOM killer even if it is a GFP_NOFS allocation.

	/*

	 * Check if there were limitations on the allocation (only relevant for

	 * NUMA and memcg) that may require different handling.

 Found nothing?!?! */

		/*

		 * If we got here due to an actual allocation at the

		 * system level, we cannot survive this and will enter

		 * an endless loop in the allocator. Bail out now.

/*

 * The pagefault handler calls here because some allocation has failed. We have

 * to take care of the memcg OOM here because this is the only safe context without

 * any locks held but let the oom killer triggered from the allocation context care

 * about the global OOM.

	/*

	 * Make sure to choose a thread which still has a reference to mm

	 * during the group exit

 Error only if the work has not been done already */

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0-only

 Equivalent to calling put_page() @refs times. */

	/*

	 * Calling put_page() for each ref is unnecessarily slow. Only the last

	 * ref needs a put_page().

/*

 * Return the compound head page with ref appropriately incremented,

 * or NULL if that failed.

	/*

	 * At this point we have a stable reference to the head page; but it

	 * could be that between the compound_head() lookup and the refcount

	 * increment, the compound page was split, in which case we'd end up

	 * holding a reference on a page that has nothing to do with the page

	 * we were given anymore.

	 * So now that the head page is stable, recheck that the pages still

	 * belong together.

/**

 * try_grab_compound_head() - attempt to elevate a page's refcount, by a

 * flags-dependent amount.

 *

 * Even though the name includes "compound_head", this function is still

 * appropriate for callers that have a non-compound @page to get.

 *

 * @page:  pointer to page to be grabbed

 * @refs:  the value to (effectively) add to the page's refcount

 * @flags: gup flags: these are the FOLL_* flag values.

 *

 * "grab" names in this file mean, "look at flags to decide whether to use

 * FOLL_PIN or FOLL_GET behavior, when incrementing the page's refcount.

 *

 * Either FOLL_PIN or FOLL_GET (or neither) must be set, but not both at the

 * same time. (That's true throughout the get_user_pages*() and

 * pin_user_pages*() APIs.) Cases:

 *

 *    FOLL_GET: page's refcount will be incremented by @refs.

 *

 *    FOLL_PIN on compound pages that are > two pages long: page's refcount will

 *    be incremented by @refs, and page[2].hpage_pinned_refcount will be

 *    incremented by @refs * GUP_PIN_COUNTING_BIAS.

 *

 *    FOLL_PIN on normal pages, or compound pages that are two pages long:

 *    page's refcount will be incremented by @refs * GUP_PIN_COUNTING_BIAS.

 *

 * Return: head page (with refcount appropriately incremented) for success, or

 * NULL upon failure. If neither FOLL_GET nor FOLL_PIN was set, that's

 * considered failure, and furthermore, a likely bug in the caller, so a warning

 * is also emitted.

		/*

		 * Can't do FOLL_LONGTERM + FOLL_PIN gup fast path if not in a

		 * right zone, so fail and let the caller fall back to the slow

		 * path.

		/*

		 * CAUTION: Don't use compound_head() on the page before this

		 * point, the result won't be stable.

		/*

		 * When pinning a compound page of order > 1 (which is what

		 * hpage_pincount_available() checks for), use an exact count to

		 * track it, via hpage_pincount_add/_sub().

		 *

		 * However, be sure to *also* increment the normal page refcount

		 * field at least once, so that the page really is pinned.

		 * That's why the refcount from the earlier

		 * try_get_compound_head() is left intact.

/**

 * try_grab_page() - elevate a page's refcount by a flag-dependent amount

 *

 * This might not do anything at all, depending on the flags argument.

 *

 * "grab" names in this file mean, "look at flags to decide whether to use

 * FOLL_PIN or FOLL_GET behavior, when incrementing the page's refcount.

 *

 * @page:    pointer to page to be grabbed

 * @flags:   gup flags: these are the FOLL_* flag values.

 *

 * Either FOLL_PIN or FOLL_GET (or neither) may be set, but not both at the same

 * time. Cases: please see the try_grab_compound_head() documentation, with

 * "refs=1".

 *

 * Return: true for success, or if no action was required (if neither FOLL_PIN

 * nor FOLL_GET was set, nothing is done). False for failure: FOLL_GET or

 * FOLL_PIN was set, but the page could not be grabbed.

/**

 * unpin_user_page() - release a dma-pinned page

 * @page:            pointer to page to be released

 *

 * Pages that were pinned via pin_user_pages*() must be released via either

 * unpin_user_page(), or one of the unpin_user_pages*() routines. This is so

 * that such pages can be separately tracked and uniquely handled. In

 * particular, interactions with RDMA and filesystems need special handling.

/**

 * unpin_user_pages_dirty_lock() - release and optionally dirty gup-pinned pages

 * @pages:  array of pages to be maybe marked dirty, and definitely released.

 * @npages: number of pages in the @pages array.

 * @make_dirty: whether to mark the pages dirty

 *

 * "gup-pinned page" refers to a page that has had one of the get_user_pages()

 * variants called on that page.

 *

 * For each page in the @pages array, make that page (or its head page, if a

 * compound page) dirty, if @make_dirty is true, and if the page was previously

 * listed as clean. In any case, releases all pages using unpin_user_page(),

 * possibly via unpin_user_pages(), for the non-dirty case.

 *

 * Please see the unpin_user_page() documentation for details.

 *

 * set_page_dirty_lock() is used internally. If instead, set_page_dirty() is

 * required, then the caller should a) verify that this is really correct,

 * because _lock() is usually required, and b) hand code it:

 * set_page_dirty_lock(), unpin_user_page().

 *

		/*

		 * Checking PageDirty at this point may race with

		 * clear_page_dirty_for_io(), but that's OK. Two key

		 * cases:

		 *

		 * 1) This code sees the page as already dirty, so it

		 * skips the call to set_page_dirty(). That could happen

		 * because clear_page_dirty_for_io() called

		 * page_mkclean(), followed by set_page_dirty().

		 * However, now the page is going to get written back,

		 * which meets the original intention of setting it

		 * dirty, so all is well: clear_page_dirty_for_io() goes

		 * on to call TestClearPageDirty(), and write the page

		 * back.

		 *

		 * 2) This code sees the page as clean, so it calls

		 * set_page_dirty(). The page stays dirty, despite being

		 * written back, so it gets written back again in the

		 * next writeback cycle. This is harmless.

/**

 * unpin_user_page_range_dirty_lock() - release and optionally dirty

 * gup-pinned page range

 *

 * @page:  the starting page of a range maybe marked dirty, and definitely released.

 * @npages: number of consecutive pages to release.

 * @make_dirty: whether to mark the pages dirty

 *

 * "gup-pinned page range" refers to a range of pages that has had one of the

 * pin_user_pages() variants called on that page.

 *

 * For the page ranges defined by [page .. page+npages], make that range (or

 * its head pages, if a compound page) dirty, if @make_dirty is true, and if the

 * page range was previously listed as clean.

 *

 * set_page_dirty_lock() is used internally. If instead, set_page_dirty() is

 * required, then the caller should a) verify that this is really correct,

 * because _lock() is usually required, and b) hand code it:

 * set_page_dirty_lock(), unpin_user_page().

 *

/**

 * unpin_user_pages() - release an array of gup-pinned pages.

 * @pages:  array of pages to be marked dirty and released.

 * @npages: number of pages in the @pages array.

 *

 * For each page in the @pages array, release the page using unpin_user_page().

 *

 * Please see the unpin_user_page() documentation for details.

	/*

	 * If this WARN_ON() fires, then the system *might* be leaking pages (by

	 * leaving them pinned), but probably not. More likely, gup/pup returned

	 * a hard -ERRNO error to the caller, who erroneously passed it here.

/*

 * Set the MMF_HAS_PINNED if not set yet; after set it'll be there for the mm's

 * lifecycle.  Avoid setting the bit unless necessary, or it might cause write

 * cache bouncing on large SMP machines for concurrent pinned gups.

	/*

	 * When core dumping an enormous anonymous area that nobody

	 * has touched so far, we don't want to allocate unnecessary pages or

	 * page tables.  Return error instead of NULL to skip handle_mm_fault,

	 * then get_dump_page() will return NULL to leave a hole in the dump.

	 * But we can only make this optimization where a hole would surely

	 * be zero-filled if handle_mm_fault() actually did handle it.

 No page to get reference */

 Proper page table entry exists, but no corresponding struct page */

/*

 * FOLL_FORCE can write to even unwritable pte's, but only

 * after we've gone through a COW cycle and they are dirty.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

		/*

		 * KSM's break_ksm() relies upon recognizing a ksm page

		 * even while it is being migrated, so for that case we

		 * need migration_entry_wait().

		/*

		 * Only return device mapping pages in the FOLL_GET or FOLL_PIN

		 * case since they are only valid while holding the pgmap

		 * reference.

 Avoid special (like zero) pages in core dumps */

 try_grab_page() does nothing unless FOLL_GET or FOLL_PIN is set. */

	/*

	 * We need to make the page accessible if and only if we are going

	 * to access its content (the FOLL_PIN case).  Please see

	 * Documentation/core-api/pin_user_pages.rst for details.

		/*

		 * pte_mkyoung() would be more correct here, but atomic care

		 * is needed to avoid losing the dirty bit: it is easier to use

		 * mark_page_accessed().

 Do not mlock pte-mapped THP */

		/*

		 * The preliminary mapping check is mainly to avoid the

		 * pointless overhead of lock_page on the ZERO_PAGE

		 * which might bounce very badly if there is contention.

		 *

		 * If the page is already locked, we don't need to

		 * handle it now - vmscan will handle it later if and

		 * when it attempts to reclaim the page.

 push cached pages to LRU */

			/*

			 * Because we lock page here, and migration is

			 * blocked by the pte's page reference, and we

			 * know the page is still mapped, we don't even

			 * need to check for file-cache page truncation.

	/*

	 * The READ_ONCE() will stabilize the pmdval in a register or

	 * on the stack so that it will stop changing under the code.

		/*

		 * MADV_DONTNEED may convert the pmd to null because

		 * mmap_lock is held in read mode

/**

 * follow_page_mask - look up a page descriptor from a user-virtual address

 * @vma: vm_area_struct mapping @address

 * @address: virtual address to look up

 * @flags: flags modifying lookup behaviour

 * @ctx: contains dev_pagemap for %ZONE_DEVICE memory pinning and a

 *       pointer to output page_mask

 *

 * @flags can have FOLL_ flags set, defined in <linux/mm.h>

 *

 * When getting pages from ZONE_DEVICE memory, the @ctx->pgmap caches

 * the device's dev_pagemap metadata to avoid repeating expensive lookups.

 *

 * On output, the @ctx->page_mask is set according to the size of the page.

 *

 * Return: the mapped (struct page *), %NULL if no mapping exists, or

 * an error pointer if there is a mapping to something not represented

 * by a page descriptor (see also vm_normal_page()).

 make this handle hugepd */

 user gate pages are read-only */

/*

 * mmap_lock must be held on entry.  If @locked != NULL and *@flags

 * does not include FOLL_NOWAIT, the mmap_lock may be released.  If it

 * is, *@locked will be set to 0 and -EBUSY returned.

 mlock all present pages, but do not fault in new pages */

		/*

		 * Note: FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED

		 * can co-exist

	/*

	 * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when

	 * necessary, even if maybe_mkwrite decided not to set pte_write. We

	 * can thus safely do subsequent page lookups as if they were reads.

	 * But only do so when looping for pte_write is futile: in some cases

	 * userspace may also be wanting to write to the gotten user page,

	 * which a read fault here might prevent (a readonly page might get

	 * reCOWed by userspace write).

			/*

			 * We used to let the write,force case do COW in a

			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could

			 * set a breakpoint in a read-only mapping of an

			 * executable, without corrupting the file (yet only

			 * when that file had been opened for writing!).

			 * Anon pages in shared mappings are surprising: now

			 * just reject it.

		/*

		 * Is there actually any vma we can reach here which does not

		 * have VM_MAYREAD set?

	/*

	 * gups are always data accesses, not instruction

	 * fetches, so execute=false here

/**

 * __get_user_pages() - pin user pages in memory

 * @mm:		mm_struct of target mm

 * @start:	starting user address

 * @nr_pages:	number of pages from start to pin

 * @gup_flags:	flags modifying pin behaviour

 * @pages:	array that receives pointers to the pages pinned.

 *		Should be at least nr_pages long. Or NULL, if caller

 *		only intends to ensure the pages are faulted in.

 * @vmas:	array of pointers to vmas corresponding to each page.

 *		Or NULL if the caller does not require them.

 * @locked:     whether we're still with the mmap_lock held

 *

 * Returns either number of pages pinned (which may be less than the

 * number requested), or an error. Details about the return value:

 *

 * -- If nr_pages is 0, returns 0.

 * -- If nr_pages is >0, but no pages were pinned, returns -errno.

 * -- If nr_pages is >0, and some pages were pinned, returns the number of

 *    pages pinned. Again, this may be less than nr_pages.

 * -- 0 return value is possible when the fault would need to be retried.

 *

 * The caller is responsible for releasing returned @pages, via put_page().

 *

 * @vmas are valid only as long as mmap_lock is held.

 *

 * Must be called with mmap_lock held.  It may be released.  See below.

 *

 * __get_user_pages walks a process's page tables and takes a reference to

 * each struct page that each user address corresponds to at a given

 * instant. That is, it takes the page that would be accessed if a user

 * thread accesses the given user virtual address at that instant.

 *

 * This does not guarantee that the page exists in the user mappings when

 * __get_user_pages returns, and there may even be a completely different

 * page there in some cases (eg. if mmapped pagecache has been invalidated

 * and subsequently re faulted). However it does guarantee that the page

 * won't be freed completely. And mostly callers simply care that the page

 * contains data that was valid *at some point in time*. Typically, an IO

 * or similar operation cannot guarantee anything stronger anyway because

 * locks can't be held over the syscall boundary.

 *

 * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If

 * the page is written to, set_page_dirty (or set_page_dirty_lock, as

 * appropriate) must be called after the page is finished with, and

 * before put_page is called.

 *

 * If @locked != NULL, *@locked will be set to 0 when mmap_lock is

 * released by an up_read().  That can happen if @gup_flags does not

 * have FOLL_NOWAIT.

 *

 * A caller using such a combination of @locked and @gup_flags

 * must therefore hold the mmap_lock for reading only, and recognize

 * when it's been released.  Otherwise, it must be held for either

 * reading or writing and will not be released.

 *

 * In most cases, get_user_pages or get_user_pages_fast should be used

 * instead of __get_user_pages. __get_user_pages should be used only if

 * you need some special @gup_flags.

	/*

	 * If FOLL_FORCE is set then do not force a full fault as the hinting

	 * fault information is unrelated to the reference behaviour of a task

	 * using the address space

 first iteration or cross vma bound */

					/*

					 * We've got a VM_FAULT_RETRY

					 * and we've lost mmap_lock.

					 * We must stop here.

		/*

		 * If we have a pending SIGKILL, don't keep faulting pages and

		 * potentially allocating memory.

			/*

			 * Proper page table entry exists, but no corresponding

			 * struct page.

	/*

	 * The architecture might have a hardware protection

	 * mechanism other than read/write that can deny access.

	 *

	 * gup always represents data access, not instruction

	 * fetches, so execute=false here:

/**

 * fixup_user_fault() - manually resolve a user page fault

 * @mm:		mm_struct of target mm

 * @address:	user address

 * @fault_flags:flags to pass down to handle_mm_fault()

 * @unlocked:	did we unlock the mmap_lock while retrying, maybe NULL if caller

 *		does not allow retry. If NULL, the caller must guarantee

 *		that fault_flags does not contain FAULT_FLAG_ALLOW_RETRY.

 *

 * This is meant to be called in the specific scenario where for locking reasons

 * we try to access user memory in atomic context (within a pagefault_disable()

 * section), this returns -EFAULT, and we want to resolve the user fault before

 * trying again.

 *

 * Typically this is meant to be used by the futex code.

 *

 * The main difference with get_user_pages() is that this function will

 * unconditionally call handle_mm_fault() which will in turn perform all the

 * necessary SW fixup of the dirty and young bits in the PTE, while

 * get_user_pages() only guarantees to update these in the struct page.

 *

 * This is important for some architectures where those bits also gate the

 * access permission to the page because they are maintained in software.  On

 * such architectures, gup() will not be enough to make a subsequent access

 * succeed.

 *

 * This function will not return with an unlocked mmap_lock. So it has not the

 * same semantics wrt the @mm->mmap_lock as does filemap_fault().

/*

 * Please note that this function, unlike __get_user_pages will not

 * return 0 for nr_pages > 0 without FOLL_NOWAIT

 if VM_FAULT_RETRY can be returned, vmas become invalid */

 check caller initialized locked */

	/*

	 * FOLL_PIN and FOLL_GET are mutually exclusive. Traditional behavior

	 * is to set FOLL_GET if the caller wants pages[] filled in (but has

	 * carelessly failed to specify FOLL_GET), so keep doing that, but only

	 * for FOLL_GET, not for the newer FOLL_PIN.

	 *

	 * FOLL_PIN always expects pages to be non-null, but no need to assert

	 * that here, as any failures will be obvious enough.

 VM_FAULT_RETRY couldn't trigger, bypass */

 VM_FAULT_RETRY cannot return errors */

			/*

			 * VM_FAULT_RETRY didn't trigger or it was a

			 * FOLL_NOWAIT.

		/*

		 * VM_FAULT_RETRY triggered, so seek to the faulting offset.

		 * For the prefault case (!pages) we only update counts.

		/*

		 * Repeat on the address that fired VM_FAULT_RETRY

		 * with both FAULT_FLAG_ALLOW_RETRY and

		 * FAULT_FLAG_TRIED.  Note that GUP can be interrupted

		 * by fatal signals, so we need to check it before we

		 * start trying again otherwise it can loop forever.

 Continue to retry until we succeeded */

		/*

		 * We must let the caller know we temporarily dropped the lock

		 * and so the critical section protected by it was lost.

/**

 * populate_vma_page_range() -  populate a range of pages in the vma.

 * @vma:   target vma

 * @start: start address

 * @end:   end address

 * @locked: whether the mmap_lock is still held

 *

 * This takes care of mlocking the pages too if VM_LOCKED is set.

 *

 * Return either number of pages pinned in the vma, or a negative error

 * code on error.

 *

 * vma->vm_mm->mmap_lock must be held.

 *

 * If @locked is NULL, it may be held for read or write and will

 * be unperturbed.

 *

 * If @locked is non-NULL, it must held for read only and may be

 * released.  If it's released, *@locked will be set to 0.

	/*

	 * We want to touch writable mappings with a write fault in order

	 * to break COW, except for shared mappings because these don't COW

	 * and we would not want to dirty them for nothing.

	/*

	 * We want mlock to succeed for regions that have any permissions

	 * other than PROT_NONE.

	/*

	 * We made sure addr is within a VMA, so the following will

	 * not result in a stack expansion that recurses back here.

/*

 * faultin_vma_page_range() - populate (prefault) page tables inside the

 *			      given VMA range readable/writable

 *

 * This takes care of mlocking the pages, too, if VM_LOCKED is set.

 *

 * @vma: target vma

 * @start: start address

 * @end: end address

 * @write: whether to prefault readable or writable

 * @locked: whether the mmap_lock is still held

 *

 * Returns either number of processed pages in the vma, or a negative error

 * code on error (see __get_user_pages()).

 *

 * vma->vm_mm->mmap_lock must be held. The range must be page-aligned and

 * covered by the VMA.

 *

 * If @locked is NULL, it may be held for read or write and will be unperturbed.

 *

 * If @locked is non-NULL, it must held for read only and may be released.  If

 * it's released, *@locked will be set to 0.

	/*

	 * FOLL_TOUCH: Mark page accessed and thereby young; will also mark

	 *	       the page dirty with FOLL_WRITE -- which doesn't make a

	 *	       difference with !FOLL_FORCE, because the page is writable

	 *	       in the page table.

	 * FOLL_HWPOISON: Return -EHWPOISON instead of -EFAULT when we hit

	 *		  a poisoned page.

	 * FOLL_POPULATE: Always populate memory with VM_LOCKONFAULT.

	 * !FOLL_FORCE: Require proper access permissions.

	/*

	 * We want to report -EINVAL instead of -EFAULT for any permission

	 * problems or incompatible mappings.

/*

 * __mm_populate - populate and/or mlock pages within a range of address space.

 *

 * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap

 * flags. VMAs must be already marked with the desired vm_flags, and

 * mmap_lock must not be held.

		/*

		 * We want to fault in pages for [nstart; end) address range.

		 * Find first corresponding VMA.

		/*

		 * Set [nstart; nend) to intersection of desired address

		 * range with the first VMA. Also, skip undesirable VMA types.

		/*

		 * Now fault in a range of pages. populate_vma_page_range()

		 * double checks the vma flags, so that it won't mlock pages

		 * if the vma was already munlocked.

 continue at next VMA */

 0 or negative error code */

 CONFIG_MMU */

	/* calculate required read or write permissions.

	 * If FOLL_FORCE is set, we only require the "MAY" flags.

 protect what we can, including chardevs */

 !CONFIG_MMU */

/**

 * fault_in_writeable - fault in userspace address range for writing

 * @uaddr: start of address range

 * @size: size of address range

 *

 * Returns the number of bytes not faulted in (like copy_to_user() and

 * copy_from_user()).

/*

 * fault_in_safe_writeable - fault in an address range for writing

 * @uaddr: start of address range

 * @size: length of address range

 *

 * Faults in an address range using get_user_pages, i.e., without triggering

 * hardware page faults.  This is primarily useful when we already know that

 * some or all of the pages in the address range aren't in memory.

 *

 * Other than fault_in_writeable(), this function is non-destructive.

 *

 * Note that we don't pin or otherwise hold the pages referenced that we fault

 * in.  There's no guarantee that they'll stay in memory for any duration of

 * time.

 *

 * Returns the number of bytes not faulted in, like copy_to_user() and

 * copy_from_user().

/**

 * fault_in_readable - fault in userspace address range for reading

 * @uaddr: start of user address range

 * @size: size of user address range

 *

 * Returns the number of bytes not faulted in (like copy_to_user() and

 * copy_from_user()).

/**

 * get_dump_page() - pin user page in memory while writing it to core dump

 * @addr: user address

 *

 * Returns struct page pointer of user page pinned for dump,

 * to be freed afterwards by put_page().

 *

 * Returns NULL on any kind of failure - a hole must then be inserted into

 * the corefile, to preserve alignment with its headers; and also returns

 * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -

 * allowing a hole to be left in the corefile to save disk space.

 *

 * Called without mmap_lock (takes and releases the mmap_lock by itself).

 CONFIG_ELF_CORE */

/*

 * Check whether all pages are pinnable, if so return number of pages.  If some

 * pages are not pinnable, migrate them, and unpin all pages. Return zero if

 * pages were migrated, or if some pages were not successfully isolated.

 * Return negative error if migration fails.

		/*

		 * If we get a movable page, since we are going to be pinning

		 * these entries, try to move them out if possible.

	/*

	 * If list is empty, and no isolation errors, means that all pages are

	 * in the correct zone.

 CONFIG_MIGRATION */

/*

 * __gup_longterm_locked() is a wrapper for __get_user_pages_locked which

 * allows us to process the FOLL_LONGTERM flag.

	/*

	 * FOLL_PIN must only be set internally by the pin_user_pages*() APIs,

	 * never directly by the caller, so enforce that with an assertion:

	/*

	 * FOLL_PIN is a prerequisite to FOLL_LONGTERM. Another way of saying

	 * that is, FOLL_LONGTERM is a specific case, more restrictive case of

	 * FOLL_PIN.

	/*

	 * Parts of FOLL_LONGTERM behavior are incompatible with

	 * FAULT_FLAG_ALLOW_RETRY because of the FS DAX check requirement on

	 * vmas. However, this only comes up if locked is set, and there are

	 * callers that do request FOLL_LONGTERM, but do not set locked. So,

	 * allow what we can.

		/*

		 * This will check the vmas (even if our vmas arg is NULL)

		 * and return -ENOTSUPP if DAX isn't allowed in this case:

/**

 * get_user_pages_remote() - pin user pages in memory

 * @mm:		mm_struct of target mm

 * @start:	starting user address

 * @nr_pages:	number of pages from start to pin

 * @gup_flags:	flags modifying lookup behaviour

 * @pages:	array that receives pointers to the pages pinned.

 *		Should be at least nr_pages long. Or NULL, if caller

 *		only intends to ensure the pages are faulted in.

 * @vmas:	array of pointers to vmas corresponding to each page.

 *		Or NULL if the caller does not require them.

 * @locked:	pointer to lock flag indicating whether lock is held and

 *		subsequently whether VM_FAULT_RETRY functionality can be

 *		utilised. Lock must initially be held.

 *

 * Returns either number of pages pinned (which may be less than the

 * number requested), or an error. Details about the return value:

 *

 * -- If nr_pages is 0, returns 0.

 * -- If nr_pages is >0, but no pages were pinned, returns -errno.

 * -- If nr_pages is >0, and some pages were pinned, returns the number of

 *    pages pinned. Again, this may be less than nr_pages.

 *

 * The caller is responsible for releasing returned @pages, via put_page().

 *

 * @vmas are valid only as long as mmap_lock is held.

 *

 * Must be called with mmap_lock held for read or write.

 *

 * get_user_pages_remote walks a process's page tables and takes a reference

 * to each struct page that each user address corresponds to at a given

 * instant. That is, it takes the page that would be accessed if a user

 * thread accesses the given user virtual address at that instant.

 *

 * This does not guarantee that the page exists in the user mappings when

 * get_user_pages_remote returns, and there may even be a completely different

 * page there in some cases (eg. if mmapped pagecache has been invalidated

 * and subsequently re faulted). However it does guarantee that the page

 * won't be freed completely. And mostly callers simply care that the page

 * contains data that was valid *at some point in time*. Typically, an IO

 * or similar operation cannot guarantee anything stronger anyway because

 * locks can't be held over the syscall boundary.

 *

 * If gup_flags & FOLL_WRITE == 0, the page must not be written to. If the page

 * is written to, set_page_dirty (or set_page_dirty_lock, as appropriate) must

 * be called after the page is finished with, and before put_page is called.

 *

 * get_user_pages_remote is typically used for fewer-copy IO operations,

 * to get a handle on the memory by some means other than accesses

 * via the user virtual addresses. The pages may be submitted for

 * DMA to devices or accessed via their kernel linear mapping (via the

 * kmap APIs). Care should be taken to use the correct cache flushing APIs.

 *

 * See also get_user_pages_fast, for performance critical applications.

 *

 * get_user_pages_remote should be phased out in favor of

 * get_user_pages_locked|unlocked or get_user_pages_fast. Nothing

 * should use get_user_pages_remote because it cannot pass

 * FAULT_FLAG_ALLOW_RETRY to handle_mm_fault.

 CONFIG_MMU */

 !CONFIG_MMU */

/**

 * get_user_pages() - pin user pages in memory

 * @start:      starting user address

 * @nr_pages:   number of pages from start to pin

 * @gup_flags:  flags modifying lookup behaviour

 * @pages:      array that receives pointers to the pages pinned.

 *              Should be at least nr_pages long. Or NULL, if caller

 *              only intends to ensure the pages are faulted in.

 * @vmas:       array of pointers to vmas corresponding to each page.

 *              Or NULL if the caller does not require them.

 *

 * This is the same as get_user_pages_remote(), just with a less-flexible

 * calling convention where we assume that the mm being operated on belongs to

 * the current task, and doesn't allow passing of a locked parameter.  We also

 * obviously don't pass FOLL_REMOTE in here.

/**

 * get_user_pages_locked() - variant of get_user_pages()

 *

 * @start:      starting user address

 * @nr_pages:   number of pages from start to pin

 * @gup_flags:  flags modifying lookup behaviour

 * @pages:      array that receives pointers to the pages pinned.

 *              Should be at least nr_pages long. Or NULL, if caller

 *              only intends to ensure the pages are faulted in.

 * @locked:     pointer to lock flag indicating whether lock is held and

 *              subsequently whether VM_FAULT_RETRY functionality can be

 *              utilised. Lock must initially be held.

 *

 * It is suitable to replace the form:

 *

 *      mmap_read_lock(mm);

 *      do_something()

 *      get_user_pages(mm, ..., pages, NULL);

 *      mmap_read_unlock(mm);

 *

 *  to:

 *

 *      int locked = 1;

 *      mmap_read_lock(mm);

 *      do_something()

 *      get_user_pages_locked(mm, ..., pages, &locked);

 *      if (locked)

 *          mmap_read_unlock(mm);

 *

 * We can leverage the VM_FAULT_RETRY functionality in the page fault

 * paths better by using either get_user_pages_locked() or

 * get_user_pages_unlocked().

 *

	/*

	 * FIXME: Current FOLL_LONGTERM behavior is incompatible with

	 * FAULT_FLAG_ALLOW_RETRY because of the FS DAX check requirement on

	 * vmas.  As there are no users of this flag in this call we simply

	 * disallow this option for now.

	/*

	 * FOLL_PIN must only be set internally by the pin_user_pages*() APIs,

	 * never directly by the caller, so enforce that:

/*

 * get_user_pages_unlocked() is suitable to replace the form:

 *

 *      mmap_read_lock(mm);

 *      get_user_pages(mm, ..., pages, NULL);

 *      mmap_read_unlock(mm);

 *

 *  with:

 *

 *      get_user_pages_unlocked(mm, ..., pages);

 *

 * It is functionally equivalent to get_user_pages_fast so

 * get_user_pages_fast should be used instead if specific gup_flags

 * (e.g. FOLL_FORCE) are not required.

	/*

	 * FIXME: Current FOLL_LONGTERM behavior is incompatible with

	 * FAULT_FLAG_ALLOW_RETRY because of the FS DAX check requirement on

	 * vmas.  As there are no users of this flag in this call we simply

	 * disallow this option for now.

/*

 * Fast GUP

 *

 * get_user_pages_fast attempts to pin user pages by walking the page

 * tables directly and avoids taking locks. Thus the walker needs to be

 * protected from page table pages being freed from under it, and should

 * block any THP splits.

 *

 * One way to achieve this is to have the walker disable interrupts, and

 * rely on IPIs from the TLB flushing code blocking before the page table

 * pages are freed. This is unsuitable for architectures that do not need

 * to broadcast an IPI when invalidating TLBs.

 *

 * Another way to achieve this is to batch up page table containing pages

 * belonging to more than one mm_user, then rcu_sched a callback to free those

 * pages. Disabling interrupts will allow the fast_gup walker to both block

 * the rcu_sched callback, and an IPI that we broadcast for splitting THPs

 * (which is a relatively rare event). The code below adopts this strategy.

 *

 * Before activating this code, please be aware that the following assumptions

 * are currently made:

 *

 *  *) Either MMU_GATHER_RCU_TABLE_FREE is enabled, and tlb_remove_table() is used to

 *  free pages containing page tables or TLB flushing requires IPI broadcast.

 *

 *  *) ptes can be read atomically by the architecture.

 *

 *  *) access_ok is sufficient to validate userspace address ranges.

 *

 * The last two assumptions can be relaxed by the addition of helper functions.

 *

 * This code is based heavily on the PowerPC implementation by Nick Piggin.

		/*

		 * Similar to the PMD case below, NUMA hinting must take slow

		 * path using the pte_protnone check.

		/*

		 * We need to make the page accessible if and only if we are

		 * going to access its content (the FOLL_PIN case).  Please

		 * see Documentation/core-api/pin_user_pages.rst for

		 * details.

/*

 * If we can't determine whether or not a pte is special, then fail immediately

 * for ptes. Note, we can still pin HugeTLB and THP as these are guaranteed not

 * to be special.

 *

 * For a futex to be placed on a THP tail page, get_futex_key requires a

 * get_user_pages_fast_only implementation that can pin pages. Thus it's still

 * useful to have gup_huge_pmd even if we can't operate on ptes.

 CONFIG_ARCH_HAS_PTE_SPECIAL */

 hugepages are never "special" */

 CONFIG_ARCH_HAS_HUGEPD */

			/*

			 * NUMA hinting faults need to be handled in the GUP

			 * slowpath for accounting purposes and so that they

			 * can be serialised against THP migration.

			/*

			 * architecture have different format for hugetlbfs

			 * pmd format and THP pmd format

 CONFIG_HAVE_FAST_GUP */

/*

 * Check if it's allowed to use get_user_pages_fast_only() for the range, or

 * we need to fall back to the slow version:

	/*

	 * FIXME: FOLL_LONGTERM does not work with

	 * get_user_pages_unlocked() (see comments in that function)

	/*

	 * Disable interrupts. The nested form is used, in order to allow full,

	 * general purpose use of this routine.

	 *

	 * With interrupts disabled, we block page table pages from being freed

	 * from under us. See struct mmu_table_batch comments in

	 * include/asm-generic/tlb.h for more details.

	 *

	 * We do not adopt an rcu_read_lock() here as we also want to block IPIs

	 * that come from THPs splitting.

	/*

	 * When pinning pages for DMA there could be a concurrent write protect

	 * from fork() via copy_page_range(), in this case always fail fast GUP.

 Slow path: try to get the remaining pages with get_user_pages */

		/*

		 * The caller has to unpin the pages we already pinned so

		 * returning -errno is not an option

/**

 * get_user_pages_fast_only() - pin user pages in memory

 * @start:      starting user address

 * @nr_pages:   number of pages from start to pin

 * @gup_flags:  flags modifying pin behaviour

 * @pages:      array that receives pointers to the pages pinned.

 *              Should be at least nr_pages long.

 *

 * Like get_user_pages_fast() except it's IRQ-safe in that it won't fall back to

 * the regular GUP.

 * Note a difference with get_user_pages_fast: this always returns the

 * number of pages pinned, 0 if no pages were pinned.

 *

 * If the architecture does not support this function, simply return with no

 * pages pinned.

 *

 * Careful, careful! COW breaking can go either way, so a non-write

 * access can get ambiguous page results. If you call this function without

 * 'write' set, you'd better be sure that you're ok with that ambiguity.

	/*

	 * Internally (within mm/gup.c), gup fast variants must set FOLL_GET,

	 * because gup fast is always a "pin with a +1 page refcount" request.

	 *

	 * FOLL_FAST_ONLY is required in order to match the API description of

	 * this routine: no fall back to regular ("slow") GUP.

	/*

	 * As specified in the API description above, this routine is not

	 * allowed to return negative values. However, the common core

	 * routine internal_get_user_pages_fast() *can* return -errno.

	 * Therefore, correct for that here:

/**

 * get_user_pages_fast() - pin user pages in memory

 * @start:      starting user address

 * @nr_pages:   number of pages from start to pin

 * @gup_flags:  flags modifying pin behaviour

 * @pages:      array that receives pointers to the pages pinned.

 *              Should be at least nr_pages long.

 *

 * Attempt to pin user pages in memory without taking mm->mmap_lock.

 * If not successful, it will fall back to taking the lock and

 * calling get_user_pages().

 *

 * Returns number of pages pinned. This may be fewer than the number requested.

 * If nr_pages is 0 or negative, returns 0. If no pages were pinned, returns

 * -errno.

	/*

	 * The caller may or may not have explicitly set FOLL_GET; either way is

	 * OK. However, internally (within mm/gup.c), gup fast variants must set

	 * FOLL_GET, because gup fast is always a "pin with a +1 page refcount"

	 * request.

/**

 * pin_user_pages_fast() - pin user pages in memory without taking locks

 *

 * @start:      starting user address

 * @nr_pages:   number of pages from start to pin

 * @gup_flags:  flags modifying pin behaviour

 * @pages:      array that receives pointers to the pages pinned.

 *              Should be at least nr_pages long.

 *

 * Nearly the same as get_user_pages_fast(), except that FOLL_PIN is set. See

 * get_user_pages_fast() for documentation on the function arguments, because

 * the arguments here are identical.

 *

 * FOLL_PIN means that the pages must be released via unpin_user_page(). Please

 * see Documentation/core-api/pin_user_pages.rst for further details.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

/*

 * This is the FOLL_PIN equivalent of get_user_pages_fast_only(). Behavior

 * is the same, except that this one sets FOLL_PIN instead of FOLL_GET.

 *

 * The API rules are the same, too: no negative values may be returned.

	/*

	 * FOLL_GET and FOLL_PIN are mutually exclusive. Note that the API

	 * rules require returning 0, rather than -errno:

	/*

	 * FOLL_FAST_ONLY is required in order to match the API description of

	 * this routine: no fall back to regular ("slow") GUP.

	/*

	 * This routine is not allowed to return negative values. However,

	 * internal_get_user_pages_fast() *can* return -errno. Therefore,

	 * correct for that here:

/**

 * pin_user_pages_remote() - pin pages of a remote process

 *

 * @mm:		mm_struct of target mm

 * @start:	starting user address

 * @nr_pages:	number of pages from start to pin

 * @gup_flags:	flags modifying lookup behaviour

 * @pages:	array that receives pointers to the pages pinned.

 *		Should be at least nr_pages long. Or NULL, if caller

 *		only intends to ensure the pages are faulted in.

 * @vmas:	array of pointers to vmas corresponding to each page.

 *		Or NULL if the caller does not require them.

 * @locked:	pointer to lock flag indicating whether lock is held and

 *		subsequently whether VM_FAULT_RETRY functionality can be

 *		utilised. Lock must initially be held.

 *

 * Nearly the same as get_user_pages_remote(), except that FOLL_PIN is set. See

 * get_user_pages_remote() for documentation on the function arguments, because

 * the arguments here are identical.

 *

 * FOLL_PIN means that the pages must be released via unpin_user_page(). Please

 * see Documentation/core-api/pin_user_pages.rst for details.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

/**

 * pin_user_pages() - pin user pages in memory for use by other devices

 *

 * @start:	starting user address

 * @nr_pages:	number of pages from start to pin

 * @gup_flags:	flags modifying lookup behaviour

 * @pages:	array that receives pointers to the pages pinned.

 *		Should be at least nr_pages long. Or NULL, if caller

 *		only intends to ensure the pages are faulted in.

 * @vmas:	array of pointers to vmas corresponding to each page.

 *		Or NULL if the caller does not require them.

 *

 * Nearly the same as get_user_pages(), except that FOLL_TOUCH is not set, and

 * FOLL_PIN is set.

 *

 * FOLL_PIN means that the pages must be released via unpin_user_page(). Please

 * see Documentation/core-api/pin_user_pages.rst for details.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

/*

 * pin_user_pages_unlocked() is the FOLL_PIN variant of

 * get_user_pages_unlocked(). Behavior is the same, except that this one sets

 * FOLL_PIN and rejects FOLL_GET.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

/*

 * pin_user_pages_locked() is the FOLL_PIN variant of get_user_pages_locked().

 * Behavior is the same, except that this one sets FOLL_PIN and rejects

 * FOLL_GET.

	/*

	 * FIXME: Current FOLL_LONGTERM behavior is incompatible with

	 * FAULT_FLAG_ALLOW_RETRY because of the FS DAX check requirement on

	 * vmas.  As there are no users of this flag in this call we simply

	 * disallow this option for now.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Bootmem core functions.

 *

 * Copyright (c) 2020, Bytedance.

 *

 *     Author: Muchun Song <songmuchun@bytedance.com>

 *

 Get section's memmap address */

	/*

	 * Get page for the memmap's phys address

	 * XXX: need more consideration for sparse_vmemmap...

 remember memmap's page */

 CONFIG_SPARSEMEM_VMEMMAP */

 !CONFIG_SPARSEMEM_VMEMMAP */

 register section info */

		/*

		 * Some platforms can assign the same pfn to multiple nodes - on

		 * node0 as well as nodeN.  To avoid registering a pfn against

		 * multiple nodes we check that this pfn does not already

		 * reside in some other nodes.

 SPDX-License-Identifier: GPL-2.0

			/*

			 * We get here when we are trying to unmap a private

			 * device page from the process address space. Such

			 * page is not CPU accessible and thus is mapped as

			 * a special swap entry, nonetheless it still does

			 * count as a valid regular mapping for the page (and

			 * is accounted as such in page maps count).

			 *

			 * So handle this special case as if it was a normal

			 * page mapping ie lock CPU page table and returns

			 * true.

			 *

			 * For more details on device private memory see HMM

			 * (include/linux/hmm.h or mm/hmm.c).

 Handle un-addressable ZONE_DEVICE memory */

 normal page and hugetlbfs page */

 THP can be referenced by any subpage */

/**

 * check_pte - check if @pvmw->page is mapped at the @pvmw->pte

 * @pvmw: page_vma_mapped_walk struct, includes a pair pte and page for checking

 *

 * page_vma_mapped_walk() found a place where @pvmw->page is *potentially*

 * mapped. check_pte() has to validate this.

 *

 * pvmw->pte may point to empty PTE, swap PTE or PTE pointing to

 * arbitrary page.

 *

 * If PVMW_MIGRATION flag is set, returns true if @pvmw->pte contains migration

 * entry that points to @pvmw->page or any subpage in case of THP.

 *

 * If PVMW_MIGRATION flag is not set, returns true if pvmw->pte points to

 * pvmw->page or any subpage in case of THP.

 *

 * Otherwise, return false.

 *

 Handle un-addressable ZONE_DEVICE memory */

/**

 * page_vma_mapped_walk - check if @pvmw->page is mapped in @pvmw->vma at

 * @pvmw->address

 * @pvmw: pointer to struct page_vma_mapped_walk. page, vma, address and flags

 * must be set. pmd, pte and ptl must be NULL.

 *

 * Returns true if the page is mapped in the vma. @pvmw->pmd and @pvmw->pte point

 * to relevant page table entries. @pvmw->ptl is locked. @pvmw->address is

 * adjusted if needed (for PTE-mapped THPs).

 *

 * If @pvmw->pmd is set but @pvmw->pte is not, you have found PMD-mapped page

 * (usually THP). For PTE-mapped THP, you should run page_vma_mapped_walk() in

 * a loop to find all PTEs that map the THP.

 *

 * For HugeTLB pages, @pvmw->pte is set to the relevant page table entry

 * regardless of which page table level the page is mapped at. @pvmw->pmd is

 * NULL.

 *

 * Returns false if there are no more page table entries for the page in

 * the vma. @pvmw->ptl is unlocked and @pvmw->pte is unmapped.

 *

 * If you need to stop the walk before page_vma_mapped_walk() returned false,

 * use page_vma_mapped_walk_done(). It will do the housekeeping.

 The only possible pmd mapping has been handled on last iteration */

 The only possible mapping was handled on last iteration */

 when pud is not present, pte will be NULL */

	/*

	 * Seek to next pte only makes sense for THP.

	 * But more important than that optimization, is to filter out

	 * any PageKsm page: whose page->index misleads vma_address()

	 * and vma_address_end() to disaster.

		/*

		 * Make sure the pmd value isn't cached in a register by the

		 * compiler and used as a stale value after we've observed a

		 * subsequent update.

 THP pmd was split under us: handle on pte level */

			/*

			 * If PVMW_SYNC, take and drop THP pmd lock so that we

			 * cannot return prematurely, while zap_huge_pmd() has

			 * cleared *pmd but not decremented compound_mapcount().

 Did we cross page table boundary? */

/**

 * page_mapped_in_vma - check whether a page is really mapped in a VMA

 * @page: the page to test

 * @vma: the VMA to test

 *

 * Returns 1 if the page is mapped into the page tables of the VMA, 0

 * if the page is not mapped into the page tables of this VMA.  Only

 * valid for normal file or anonymous VMAs.

 SPDX-License-Identifier: GPL-2.0

 Copyright(c) 2018 Intel Corporation. All rights reserved.

/*

 * For two pages to be swapped in the shuffle, they must be free (on a

 * 'free_area' lru), have the same order, and have the same migratetype.

	/*

	 * Given we're dealing with randomly selected pfns in a zone we

	 * need to ask questions like...

 ... is the page managed by the buddy? */

 ... is the page assigned to the same zone? */

 ...is the page free and currently on a free_area list? */

	/*

	 * ...is the page on the same list as the page we will

	 * shuffle it with?

/*

 * Fisher-Yates shuffle the freelist which prescribes iterating through an

 * array, pfns in this case, and randomly swapping each entry with another in

 * the span, end_pfn - start_pfn.

 *

 * To keep the implementation simple it does not attempt to correct for sources

 * of bias in the distribution, like modulo bias or pseudo-random number

 * generator bias. I.e. the expectation is that this shuffling raises the bar

 * for attacks that exploit the predictability of page allocations, but need not

 * be a perfect shuffle.

		/*

		 * We expect page_i, in the sub-range of a zone being added

		 * (@start_pfn to @end_pfn), to more likely be valid compared to

		 * page_j randomly selected in the span @zone_start_pfn to

		 * @spanned_pages.

			/*

			 * Pick a random order aligned page in the zone span as

			 * a swap target. If the selected pfn is a hole, retry

			 * up to SHUFFLE_RETRY attempts find a random valid pfn

			 * in the zone.

		/*

		 * Each migratetype corresponds to its own list, make sure the

		 * types match otherwise we're moving pages to lists where they

		 * do not belong.

 take it easy on the zone lock */

/*

 * __shuffle_free_memory - reduce the predictability of the page allocator

 * @pgdat: node page data

	/*

	 * The lack of locking is deliberate. If 2 threads race to

	 * update the rand state it just adds to the entropy.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 KASAN still think the page is in-use, so skip it. */

	/*

	 * Page poisoning when enabled poisons each and every page

	 * that is freed to buddy. Thus no extra check is done to

	 * see if a page was poisoned.

 This function does nothing, all work is done via poison pages */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Simple NUMA memory policy for the Linux kernel.

 *

 * Copyright 2003,2004 Andi Kleen, SuSE Labs.

 * (C) Copyright 2005 Christoph Lameter, Silicon Graphics, Inc.

 *

 * NUMA policy allows the user to give hints in which node(s) memory should

 * be allocated.

 *

 * Support four policies per VMA and per process:

 *

 * The VMA policy has priority over the process policy for a page fault.

 *

 * interleave     Allocate memory interleaved over a set of nodes,

 *                with normal fallback if it fails.

 *                For VMA based allocations this interleaves based on the

 *                offset into the backing object or offset into the mapping

 *                for anonymous memory. For process policy an process counter

 *                is used.

 *

 * bind           Only allocate memory on a specific set of nodes,

 *                no fallback.

 *                FIXME: memory is allocated starting with the first node

 *                to the last. It would be better if bind would truly restrict

 *                the allocation to memory nodes instead

 *

 * preferred       Try a specific node first before normal fallback.

 *                As a special case NUMA_NO_NODE here means do the allocation

 *                on the local CPU. This is normally identical to default,

 *                but useful to set in a VMA when you have a non default

 *                process policy.

 *

 * preferred many Try a set of nodes first before normal fallback. This is

 *                similar to preferred without the special case.

 *

 * default        Allocate on the local node first, or when on a VMA

 *                use the process policy. This is what Linux always did

 *		  in a NUMA aware kernel and still does by, ahem, default.

 *

 * The process policy is applied for most non interrupt memory allocations

 * in that process' context. Interrupts ignore the policies and always

 * try to allocate on the local CPU. The VMA policy is only applied for memory

 * allocations for a VMA in the VM.

 *

 * Currently there are a few corner cases in swapping where the policy

 * is not applied, but the majority should be handled. When process policy

 * is used it is not remembered over swap outs/swap ins.

 *

 * Only the highest zone in the zone hierarchy gets policied. Allocations

 * requesting a lower zone just use default policy. This implies that

 * on systems with highmem kernel lowmem allocation don't get policied.

 * Same with GFP_DMA allocations.

 *

 * For shmfs/tmpfs/hugetlbfs shared memory the policy is shared between

 * all users and remembered even when nobody has memory mapped.

/* Notebook:

   fix mmap readahead to honour policy and enable policy for any page cache

   object

   statistics for bigpages

   global policy for page cache? currently it uses process policy. Requires

   first item above.

   handle mremap for shared memory (currently ignored for the policy)

   grows down?

   make bind policy root only? It can trigger oom much faster and the

   kernel is not always grateful with that.

 Internal flags */

 Skip checks for continuous vmas */

 Invert check for nodemask */

/* Highest zone. An specific allocation for a zone below that is not

/*

 * run-time system-wide default policy => local allocation

 never free it */

/**

 * numa_map_to_online_node - Find closest online node

 * @node: Node id to start the search

 *

 * Lookup the next closest node by distance if @nid is not online.

 preferred_node_policy is not initialised early in boot */

/*

 * mpol_set_nodemask is called after mpol_new() to set up the nodemask, if

 * any, for the new policy.  mpol_new() has already validated the nodes

 * parameter with respect to the policy mode and flags.

 *

 * Must be called holding task's alloc_lock to protect task's mems_allowed

 * and mempolicy.  May also be called holding the mmap_lock for write.

	/*

	 * Default (pol==NULL) resp. local memory policies are not a

	 * subject of any remapping. They also do not need any special

	 * constructor.

 Check N_MEMORY */

/*

 * This function just creates a new policy, does some check and simple

 * initialization. You must invoke mpol_set_nodemask() to set nodes.

	/*

	 * MPOL_PREFERRED cannot be used with MPOL_F_STATIC_NODES or

	 * MPOL_F_RELATIVE_NODES if the nodemask is empty (local allocation).

	 * All other modes require a valid pointer to a non-empty nodemask.

 Slow path of a mpol destructor. */

/*

 * mpol_rebind_policy - Migrate a policy to a different set of nodes

 *

 * Per-vma policies are protected by mmap_lock. Allocations using per-task

 * policies are protected by task->mems_allowed_seq to prevent a premature

 * OOM/allocation failure due to parallel nodemask modification.

/*

 * Wrapper for mpol_rebind_policy() that just requires task

 * pointer, and updates task mempolicy.

 *

 * Called with task's alloc_lock held.

/*

 * Rebind each vma in mm to new nodemask.

 *

 * Call holding a reference to mm.  Takes mm->mmap_lock during call.

/*

 * Check if the page's nid is in qp->nmask.

 *

 * If MPOL_MF_INVERT is set in qp->flags, check if the nid is

 * in the invert of qp->nmask.

/*

 * queue_pages_pmd() has four possible return values:

 * 0 - pages are placed on the right node or queued successfully, or

 *     special page is met, i.e. huge zero page.

 * 1 - there is unmovable page, and MPOL_MF_MOVE* & MPOL_MF_STRICT were

 *     specified.

 * 2 - THP was split.

 * -EIO - is migration entry or only MPOL_MF_STRICT was specified and an

 *        existing page was already on a node that does not follow the

 *        policy.

 go to thp migration */

/*

 * Scan through pages checking if pages follow certain conditions,

 * and move them to the pagelist if they do.

 *

 * queue_pages_pte_range() has three possible return values:

 * 0 - pages are placed on the right node or queued successfully, or

 *     special page is met, i.e. zero page.

 * 1 - there is unmovable page, and MPOL_MF_MOVE* & MPOL_MF_STRICT were

 *     specified.

 * -EIO - only MPOL_MF_STRICT was specified and an existing page was already

 *        on a node that does not follow the policy.

 THP was split, fall through to pte walk */

		/*

		 * vm_normal_page() filters out zero pages, but there might

		 * still be PageReserved pages to skip, perhaps in a VDSO.

 MPOL_MF_STRICT must be specified if we get here */

			/*

			 * Do not abort immediately since there may be

			 * temporary off LRU pages in the range.  Still

			 * need migrate other LRU pages.

		/*

		 * STRICT alone means only detecting misplaced page and no

		 * need to further check other vma.

		/*

		 * Must be STRICT with MOVE*, otherwise .test_walk() have

		 * stopped walking current vma.

		 * Detecting misplaced page but allow migrating pages which

		 * have been queued.

 With MPOL_MF_MOVE, we migrate only unshared hugepage. */

			/*

			 * Failed to isolate page but allow migrating pages

			 * which have been queued.

/*

 * This is used to mark a range of virtual addresses to be inaccessible.

 * These are later cleared by a NUMA hinting fault. Depending on these

 * faults, pages may be migrated for better NUMA placement.

 *

 * This is assuming that NUMA faults are handled using PROT_NONE. If

 * an architecture makes a different choice, it will need further

 * changes to the core.

 CONFIG_NUMA_BALANCING */

 range check first */

 hole at head side of range */

 hole at middle or tail of range */

	/*

	 * Need check MPOL_MF_STRICT to return -EIO if possible

	 * regardless of vma_migratable

 Similar to task_numa_work, skip inaccessible VMAs */

 queue pages from current vma */

/*

 * Walk through page tables and collect pages to be migrated.

 *

 * If pages found in a given range are on a set of nodes (determined by

 * @nodes and @flags,) it's isolated and queued to the pagelist which is

 * passed via @private.

 *

 * queue_pages_range() has three possible return values:

 * 1 - there is unmovable page, but MPOL_MF_MOVE* & MPOL_MF_STRICT were

 *     specified.

 * 0 - queue pages successfully or no misplaced page.

 * errno - i.e. misplaced pages with MPOL_MF_STRICT specified (-EIO) or

 *         memory range specified by nodemask and maxnode points outside

 *         your accessible address space (-EFAULT)

 whole range in hole */

/*

 * Apply policy to a single VMA

 * This must be called with the mmap_lock held for writing.

 protected by mmap_lock */

 Step 2: apply policy to a range and do splits. */

 vma_merge() joined vma && vma->next, case 8 */

 Set the process memory policy */

/*

 * Return nodemask for policy for get_mempolicy() query

 *

 * Called with task's alloc_lock held

 return empty node mask for local allocation */

 Retrieve NUMA policy */

 just so it's initialized */

		/*

		 * Do NOT fall back to task policy if the

		 * vma/shared policy at addr is NULL.  We

		 * want to return MPOL_DEFAULT in this case.

 indicates default behavior */

			/*

			 * Take a refcount on the mpol, lookup_node()

			 * will drop the mmap_lock, so after calling

			 * lookup_node() only "pol" remains valid, "vma"

			 * is stale.

		/*

		 * Internal mempolicy flags must be masked off before exposing

		 * the policy to userspace.

/*

 * page migration, thp tail pages can be passed.

	/*

	 * Avoid migrating a page that is shared with others.

			/*

			 * Non-movable page may reach here.  And, there may be

			 * temporary off LRU pages or non-LRU movable pages.

			 * Treat them as unmovable pages since they can't be

			 * isolated, so they can't be moved at the moment.  It

			 * should return -EIO for this case too.

/*

 * Migrate pages from one node to a target node.

 * Returns error or the number of pages not migrated.

	/*

	 * This does not "check" the range but isolates all pages that

	 * need migration.  Between passing in the full user address

	 * space range and MPOL_MF_DISCONTIG_OK, this call can not fail.

/*

 * Move pages between the two nodesets so as to preserve the physical

 * layout as much as possible.

 *

 * Returns the number of page that could not be moved.

	/*

	 * Find a 'source' bit set in 'tmp' whose corresponding 'dest'

	 * bit in 'to' is not also set in 'tmp'.  Clear the found 'source'

	 * bit in 'tmp', and return that <source, dest> pair for migration.

	 * The pair of nodemasks 'to' and 'from' define the map.

	 *

	 * If no pair of bits is found that way, fallback to picking some

	 * pair of 'source' and 'dest' bits that are not the same.  If the

	 * 'source' and 'dest' bits are the same, this represents a node

	 * that will be migrating to itself, so no pages need move.

	 *

	 * If no bits are left in 'tmp', or if all remaining bits left

	 * in 'tmp' correspond to the same bit in 'to', return false

	 * (nothing left to migrate).

	 *

	 * This lets us pick a pair of nodes to migrate between, such that

	 * if possible the dest node is not already occupied by some other

	 * source node, minimizing the risk of overloading the memory on a

	 * node that would happen if we migrated incoming memory to a node

	 * before migrating outgoing memory source that same node.

	 *

	 * A single scan of tmp is sufficient.  As we go, we remember the

	 * most recent <s, d> pair that moved (s != d).  If we find a pair

	 * that not only moved, but what's better, moved to an empty slot

	 * (d is not set in tmp), then we break out then, with that pair.

	 * Otherwise when we finish scanning from_tmp, we at least have the

	 * most recent <s, d> pair that moved.  If we get all the way through

	 * the scan of tmp without finding any node that moved, much less

	 * moved to an empty node, then there is nothing left worth migrating.

			/*

			 * do_migrate_pages() tries to maintain the relative

			 * node relationship of the pages established between

			 * threads and memory areas.

                         *

			 * However if the number of source nodes is not equal to

			 * the number of destination nodes we can not preserve

			 * this node relative relationship.  In that case, skip

			 * copying memory from a node that is in the destination

			 * mask.

			 *

			 * Example: [2,3,4] -> [3,4,5] moves everything.

			 *          [0-7] - > [3,4,5] moves only 0,1,2,6,7.

 Node moved. Memorize */

 dest not in remaining from nodes? */

/*

 * Allocate a new page for page migration based on vma policy.

 * Start by assuming the page is mapped by the same vma as contains @start.

 * Search forward from there, if not.  N.B., this assumes that the

 * list of pages handed to migrate_pages()--which is how we get here--

 * is in virtual address order.

	/*

	 * if !vma, alloc_page_vma() will use task or system default policy

	/*

	 * If we are using the default policy then operation

	 * on discontinuous address spaces is okay after all

/*

 * User space interface with variable sized bitmaps for nodelists.

 Copy a node mask from user space. */

	/*

	 * When the user specified more nodes than supported just check

	 * if the non supported part is all zero, one word at a time,

	 * starting at the end.

 Copy a kernel node mask to user space */

 Basic parameter sanity check used by both mbind() and set_mempolicy() */

 Set the process memory policy */

 Find the mm_struct */

	/*

	 * Check if this process has the right to modify the specified process.

	 * Use the regular "ptrace_may_access()" checks.

 Is the user allowed to access the target nodes? */

 Retrieve NUMA policy */

	/*

	 * DAX device mappings require predictable access latency, so avoid

	 * incurring periodic faults.

	/*

	 * Migration allocates pages in the highest zone. If we cannot

	 * do so then migration (at least from node to node) is not

	 * possible.

			/*

			 * shmem_alloc_page() passes MPOL_F_SHARED policy with

			 * a pseudo vma whose vma->vm_ops=NULL. Take a reference

			 * count on these policies which will be dropped by

			 * mpol_cond_put() later

/*

 * get_vma_policy(@vma, @addr)

 * @vma: virtual memory area whose policy is sought

 * @addr: address in @vma for shared policy lookup

 *

 * Returns effective policy for a VMA at specified address.

 * Falls back to current->mempolicy or system default policy, as necessary.

 * Shared policies [those marked as MPOL_F_SHARED] require an extra reference

 * count--added by the get_policy() vm_op, as appropriate--to protect against

 * freeing by another task.  It is the caller's responsibility to free the

 * extra reference for shared policies.

	/*

	 * if policy->nodes has movable memory only,

	 * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.

	 *

	 * policy->nodes is intersect with node_states[N_MEMORY].

	 * so if the following test fails, it implies

	 * policy->nodes has movable memory only.

/*

 * Return a nodemask representing a mempolicy for filtering nodes for

 * page allocation

 Lower zones don't get a nodemask applied for MPOL_BIND */

/*

 * Return the  preferred node id for 'prefer' mempolicy, and return

 * the given id for all other policies.

 *

 * policy_node() is always coupled with policy_nodemask(), which

 * secures the nodemask limit for 'bind' and 'prefer-many' policy.

		/*

		 * __GFP_THISNODE shouldn't even be used with the bind policy

		 * because we might easily break the expectation to stay on the

		 * requested node and not break the policy.

 Do dynamic interleaving for a process */

/*

 * Depending on the memory policy provide a node from which to allocate the

 * next slab entry.

		/*

		 * Follow bind policy behavior and start allocation at the

		 * first node.

/*

 * Do static interleaving for a VMA with known offset @n.  Returns the n'th

 * node in pol->nodes (starting from n=0), wrapping around if n exceeds the

 * number of present nodes.

	/*

	 * The barrier will stabilize the nodemask in a register or on

	 * the stack so that it will stop changing under the code.

	 *

	 * Between first_node() and next_node(), pol->nodes could be changed

	 * by other threads. So we put pol->nodes in a local stack.

 Determine a node number for interleave */

		/*

		 * for small pages, there is no difference between

		 * shift and PAGE_SHIFT, so the bit-shift is safe.

		 * for huge pages, since vm_pgoff is in units of small

		 * pages, we need to shift off the always 0 bits to get

		 * a useful offset.

/*

 * huge_node(@vma, @addr, @gfp_flags, @mpol)

 * @vma: virtual memory area whose policy is sought

 * @addr: address in @vma for shared policy lookup and interleave policy

 * @gfp_flags: for requested zone

 * @mpol: pointer to mempolicy pointer for reference counted mempolicy

 * @nodemask: pointer to nodemask pointer for 'bind' and 'prefer-many' policy

 *

 * Returns a nid suitable for a huge page allocation and a pointer

 * to the struct mempolicy for conditional unref after allocation.

 * If the effective policy is 'bind' or 'prefer-many', returns a pointer

 * to the mempolicy's @nodemask for filtering the zonelist.

 *

 * Must be protected by read_mems_allowed_begin()

/*

 * init_nodemask_of_mempolicy

 *

 * If the current task's mempolicy is "default" [NULL], return 'false'

 * to indicate default policy.  Otherwise, extract the policy nodemask

 * for 'bind' or 'interleave' policy into the argument nodemask, or

 * initialize the argument nodemask to contain the single node for

 * 'preferred' or 'local' policy and return 'true' to indicate presence

 * of non-default mempolicy.

 *

 * We don't bother with reference counting the mempolicy [mpol_get/put]

 * because the current task is examining it's own mempolicy and a task's

 * mempolicy is only ever changed by the task itself.

 *

 * N.B., it is the caller's responsibility to free a returned nodemask.

/*

 * mempolicy_in_oom_domain

 *

 * If tsk's mempolicy is "bind", check for intersection between mask and

 * the policy nodemask. Otherwise, return true for all other policies

 * including "interleave", as a tsk with "interleave" policy may have

 * memory allocated from all nodes in system.

 *

 * Takes task_lock(tsk) to prevent freeing of its mempolicy.

/* Allocate a page in interleaved policy.

 skip NUMA_INTERLEAVE_HIT counter update if numa stats is disabled */

	/*

	 * This is a two pass approach. The first pass will only try the

	 * preferred nodes but skip the direct reclaim and allow the

	 * allocation to fail, while the second pass will try all the

	 * nodes in system.

/**

 * alloc_pages_vma - Allocate a page for a VMA.

 * @gfp: GFP flags.

 * @order: Order of the GFP allocation.

 * @vma: Pointer to VMA or NULL if not available.

 * @addr: Virtual address of the allocation.  Must be inside @vma.

 * @node: Which node to prefer for allocation (modulo policy).

 * @hugepage: For hugepages try only the preferred node if possible.

 *

 * Allocate a page for a specific address in @vma, using the appropriate

 * NUMA policy.  When @vma is not NULL the caller must hold the mmap_lock

 * of the mm_struct of the VMA to prevent it from going away.  Should be

 * used for all allocations for pages that will be mapped into user space.

 *

 * Return: The page on success or NULL if allocation fails.

		/*

		 * For hugepage allocation and non-interleave policy which

		 * allows the current node (or other explicitly preferred

		 * node) we only try to allocate from the current/preferred

		 * node and don't fall back to other nodes, as the cost of

		 * remote accesses would likely offset THP benefits.

		 *

		 * If the policy is interleave or does not allow the current

		 * node in its nodemask, we allocate the standard way.

			/*

			 * First, try to allocate THP only on local node, but

			 * don't reclaim unnecessarily, just compact.

			/*

			 * If hugepage allocations are configured to always

			 * synchronous compact or the vma has been madvised

			 * to prefer hugepage backing, retry allowing remote

			 * memory with both reclaim and compact as well.

/**

 * alloc_pages - Allocate pages.

 * @gfp: GFP flags.

 * @order: Power of two of number of pages to allocate.

 *

 * Allocate 1 << @order contiguous pages.  The physical address of the

 * first page is naturally aligned (eg an order-3 allocation will be aligned

 * to a multiple of 8 * PAGE_SIZE bytes).  The NUMA policy of the current

 * process is honoured when in process context.

 *

 * Context: Can be called from any context, providing the appropriate GFP

 * flags are used.

 * Return: The page on success or NULL if allocation fails.

	/*

	 * No reference counting needed for current->mempolicy

	 * nor system default_policy

/* alloc pages bulk and mempolicy should be considered at the

 * same time in some situation such as vmalloc.

 *

 * It can accelerate memory allocation especially interleaving

 * allocate memory.

/*

 * If mpol_dup() sees current->cpuset == cpuset_being_rebound, then it

 * rebinds the mempolicy its copying by calling mpol_rebind_policy()

 * with the mems_allowed returned by cpuset_mems_allowed().  This

 * keeps mempolicies cpuset relative after its cpuset moves.  See

 * further kernel/cpuset.c update_nodemask().

 *

 * current's mempolicy may be rebinded by the other task(the task that changes

 * cpuset's mems), so we needn't do rebind work for current task.

 Slow path of a mempolicy duplicate */

 task's mempolicy is protected by alloc_lock */

 Slow path of a mempolicy comparison */

/*

 * Shared memory backing store policy support.

 *

 * Remember policies even when nobody has shared memory mapped.

 * The policies are kept in Red-Black tree linked from the inode.

 * They are protected by the sp->lock rwlock, which should be held

 * for any accesses to the tree.

/*

 * lookup first element intersecting start-end.  Caller holds sp->lock for

 * reading or for writing

/*

 * Insert a new shared policy into the list.  Caller holds sp->lock for

 * writing.

 Find shared policy intersecting idx */

/**

 * mpol_misplaced - check whether current page node is valid in policy

 *

 * @page: page to be checked

 * @vma: vm area where page mapped

 * @addr: virtual address where page mapped

 *

 * Lookup current policy node id for vma,addr and "compare to" page's

 * node id.  Policy determination "mimics" alloc_page_vma().

 * Called from fault path where we know the vma and faulting address.

 *

 * Return: NUMA_NO_NODE if the page is in a node that is valid for this

 * policy, or a suitable node ID to allocate a replacement page from.

 Optimize placement among multiple nodes via NUMA balancing */

		/*

		 * use current page if in policy nodemask,

		 * else select nearest allowed node, if any.

		 * If no allowed nodes, use current [!misplaced].

 Migrate the page towards the node whose CPU is referencing it */

/*

 * Drop the (possibly final) reference to task->mempolicy.  It needs to be

 * dropped after task->mempolicy is set to NULL so that any allocation done as

 * part of its kmem_cache_free(), such as by KASAN, doesn't reference a freed

 * policy.

 Replace a policy range. */

 Take care of old policies in the same range. */

 Old policy spanning whole new range. */

/**

 * mpol_shared_policy_init - initialize shared policy for inode

 * @sp: pointer to inode shared policy

 * @mpol:  struct mempolicy to install

 *

 * Install non-NULL @mpol in inode's shared policy rb-tree.

 * On entry, the current task has a reference on a non-NULL @mpol.

 * This must be released on exit.

 * This is called at get_inode() calls and we can use GFP_KERNEL.

 empty tree == default mempolicy */

 contextualize the tmpfs mount point mempolicy */

 no valid nodemask intersection */

 Create pseudo-vma that contains just the policy */

 policy covers entire file */

 adds ref */

 drop initial ref */

 drop our incoming ref on sb mpol */

 Free a backing policy store on inode delete. */

 Parsed by setup_numabalancing. override == 1 enables, -1 disables */

 CONFIG_NUMA_BALANCING */

 assumes fs == KERNEL_DS */

	/*

	 * Set interleaving policy for system init. Interleaving is only

	 * enabled across suitably sized nodes (default is >= 16MB), or

	 * fall back to the largest node if they're all smaller.

 Preserve the largest node */

 Interleave this node? */

 All too small, use the largest */

 Reset policy of current process to default */

/*

 * Parse and format mempolicy from/to strings

/**

 * mpol_parse_str - parse string to mempolicy, for tmpfs mpol mount option.

 * @str:  string containing mempolicy to parse

 * @mpol:  pointer to struct mempolicy pointer, returned on success.

 *

 * Format of input:

 *	<mode>[=<flags>][:<nodelist>]

 *

 * On success, returns 0, else 1

 terminate mode string */

 NUL-terminate mode or flags string */

		/*

		 * Insist on a nodelist of one node only, although later

		 * we use first_node(nodes) to grab a single node, so here

		 * nodelist (or nodes) cannot be empty.

		/*

		 * Default to online nodes with memory if no nodelist

		/*

		 * Don't allow a nodelist;  mpol_new() checks flags

		/*

		 * Insist on a empty nodelist

		/*

		 * Insist on a nodelist

		/*

		 * Currently, we only support two mutually exclusive

		 * mode flags.

	/*

	 * Save nodes for mpol_to_str() to show the tmpfs mount options

	 * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.

	/*

	 * Save nodes for contextualization: this will be used to "clone"

	 * the mempolicy in a specific context [cpuset] at a later time.

 Restore string for error message */

 CONFIG_TMPFS */

/**

 * mpol_to_str - format a mempolicy structure for printing

 * @buffer:  to contain formatted mempolicy string

 * @maxlen:  length of @buffer

 * @pol:  pointer to mempolicy to be formatted

 *

 * Convert @pol into a string.  If @buffer is too short, truncate the string.

 * Recommend a @maxlen of at least 32 for the longest mode, "interleave", the

 * longest flag, "relative", and to display at least a few node ids.

		/*

		 * Currently, the only defined flags are mutually exclusive

 SPDX-License-Identifier: GPL-2.0-only

/**

 * kfree_const - conditionally free memory

 * @x: pointer to the memory

 *

 * Function calls kfree only if @x is not in .rodata section.

/**

 * kstrdup - allocate space for and copy an existing string

 * @s: the string to duplicate

 * @gfp: the GFP mask used in the kmalloc() call when allocating memory

 *

 * Return: newly allocated copy of @s or %NULL in case of error

/**

 * kstrdup_const - conditionally duplicate an existing const string

 * @s: the string to duplicate

 * @gfp: the GFP mask used in the kmalloc() call when allocating memory

 *

 * Note: Strings allocated by kstrdup_const should be freed by kfree_const and

 * must not be passed to krealloc().

 *

 * Return: source string if it is in .rodata section otherwise

 * fallback to kstrdup.

/**

 * kstrndup - allocate space for and copy an existing string

 * @s: the string to duplicate

 * @max: read at most @max chars from @s

 * @gfp: the GFP mask used in the kmalloc() call when allocating memory

 *

 * Note: Use kmemdup_nul() instead if the size is known exactly.

 *

 * Return: newly allocated copy of @s or %NULL in case of error

/**

 * kmemdup - duplicate region of memory

 *

 * @src: memory region to duplicate

 * @len: memory region length

 * @gfp: GFP mask to use

 *

 * Return: newly allocated copy of @src or %NULL in case of error

/**

 * kmemdup_nul - Create a NUL-terminated string from unterminated data

 * @s: The data to stringify

 * @len: The size of the data

 * @gfp: the GFP mask used in the kmalloc() call when allocating memory

 *

 * Return: newly allocated copy of @s with NUL-termination or %NULL in

 * case of error

/**

 * memdup_user - duplicate memory region from user space

 *

 * @src: source address in user space

 * @len: number of bytes to copy

 *

 * Return: an ERR_PTR() on failure.  Result is physically

 * contiguous, to be freed by kfree().

/**

 * vmemdup_user - duplicate memory region from user space

 *

 * @src: source address in user space

 * @len: number of bytes to copy

 *

 * Return: an ERR_PTR() on failure.  Result may be not

 * physically contiguous.  Use kvfree() to free.

/**

 * strndup_user - duplicate an existing string from user space

 * @s: The string to duplicate

 * @n: Maximum number of bytes to copy, including the trailing NUL.

 *

 * Return: newly allocated copy of @s or an ERR_PTR() in case of error

/**

 * memdup_user_nul - duplicate memory region from user space and NUL-terminate

 *

 * @src: source address in user space

 * @len: number of bytes to copy

 *

 * Return: an ERR_PTR() on failure.

	/*

	 * Always use GFP_KERNEL, since copy_from_user() can sleep and

	 * cause pagefault, which makes it pointless to use GFP_NOFS

	 * or GFP_ATOMIC.

 Check if the vma is being used as a stack by this task */

/*

 * Change backing file, only valid to use during initial VMA setup.

 Changing an anonymous vma with this is illegal */

 8MB of VA */

 Is the current task 32bit ? */

 CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS */

/*

 * Leave enough space between the mmap area and the stack to honour ulimit in

 * the face of randomisation.

 Account for stack randomization if necessary */

 Values close to RLIM_INFINITY can overflow. */

/**

 * __account_locked_vm - account locked pages to an mm's locked_vm

 * @mm:          mm to account against

 * @pages:       number of pages to account

 * @inc:         %true if @pages should be considered positive, %false if not

 * @task:        task used to check RLIMIT_MEMLOCK

 * @bypass_rlim: %true if checking RLIMIT_MEMLOCK should be skipped

 *

 * Assumes @task and @mm are valid (i.e. at least one reference on each), and

 * that mmap_lock is held as writer.

 *

 * Return:

 * * 0       on success

 * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.

/**

 * account_locked_vm - account locked pages to an mm's locked_vm

 * @mm:          mm to account against, may be NULL

 * @pages:       number of pages to account

 * @inc:         %true if @pages should be considered positive, %false if not

 *

 * Assumes a non-NULL @mm is valid (i.e. at least one reference on it).

 *

 * Return:

 * * 0       on success, or if mm is NULL

 * * -ENOMEM if RLIMIT_MEMLOCK would be exceeded.

/**

 * kvmalloc_node - attempt to allocate physically contiguous memory, but upon

 * failure, fall back to non-contiguous (vmalloc) allocation.

 * @size: size of the request.

 * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.

 * @node: numa node to allocate from

 *

 * Uses kmalloc to get the memory but if the allocation fails then falls back

 * to the vmalloc allocator. Use kvfree for freeing the memory.

 *

 * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported.

 * __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is

 * preferable to the vmalloc fallback, due to visible performance drawbacks.

 *

 * Please note that any use of gfp flags outside of GFP_KERNEL is careful to not

 * fall back to vmalloc.

 *

 * Return: pointer to the allocated memory of %NULL in case of failure

	/*

	 * vmalloc uses GFP_KERNEL for some internal allocations (e.g page tables)

	 * so the given set of flags has to be compatible.

	/*

	 * We want to attempt a large physically contiguous block first because

	 * it is less likely to fragment multiple larger blocks and therefore

	 * contribute to a long term fragmentation less than vmalloc fallback.

	 * However make sure that larger requests are not too disruptive - no

	 * OOM killer and no allocation failure warnings as we have a fallback.

	/*

	 * It doesn't really make sense to fallback to vmalloc for sub page

	 * requests

 Don't even allow crazy sizes */

/**

 * kvfree() - Free memory.

 * @addr: Pointer to allocated memory.

 *

 * kvfree frees memory allocated by any of vmalloc(), kmalloc() or kvmalloc().

 * It is slightly more efficient to use kfree() or vfree() if you are certain

 * that you know which one to use.

 *

 * Context: Either preemptible task context or not-NMI interrupt.

/**

 * kvfree_sensitive - Free a data object containing sensitive information.

 * @addr: address of the data object to be freed.

 * @len: length of the data object.

 *

 * Use the special memzero_explicit() function to clear the content of a

 * kvmalloc'ed object containing sensitive data to make sure that the

 * compiler won't optimize out the data clearing.

 Neutral page->mapping pointer to address_space or anon_vma or other */

/**

 * folio_mapped - Is this folio mapped into userspace?

 * @folio: The folio.

 *

 * Return: True if any page in this folio is referenced by user page tables.

/**

 * folio_mapping - Find the mapping where this folio is stored.

 * @folio: The folio.

 *

 * For folios which are in the page cache, return the mapping that this

 * page belongs to.  Folios in the swap cache return the swap mapping

 * this page is stored in (which is different from the mapping for the

 * swap file or swap device where the data is stored).

 *

 * You can call this for folios which aren't in the swap cache or page

 * cache and it will return NULL.

 This happens if someone calls flush_dcache_page on slab page */

 Slow path of page_mapcount() for compound pages */

	/*

	 * For file THP page->_mapcount contains total number of mapping

	 * of the page: no need to look into compound_mapcount.

/**

 * folio_copy - Copy the contents of one folio to another.

 * @dst: Folio to copy to.

 * @src: Folio to copy from.

 *

 * The bytes in the folio represented by @src are copied to @dst.

 * Assumes the caller has validated that @dst is at least as large as @src.

 * Can be called in atomic context for order-0 folios, but if the folio is

 * larger, it may sleep.

 128MB */

 8MB */

	/*

	 * The deviation of sync_overcommit_as could be big with loose policy

	 * like OVERCOMMIT_ALWAYS/OVERCOMMIT_GUESS. When changing policy to

	 * strict OVERCOMMIT_NEVER, we need to reduce the deviation to comply

	 * with the strict "NEVER", and to avoid possible race condition (even

	 * though user usually won't too frequently do the switching to policy

	 * OVERCOMMIT_NEVER), the switch is done in the following order:

	 *	1. changing the batch

	 *	2. sync percpu count on each CPU

	 *	3. switch the policy

/*

 * Committed memory limit enforced when OVERCOMMIT_NEVER policy is used

/*

 * Make sure vm_committed_as in one cacheline and not cacheline shared with

 * other variables. It can be updated by several CPUs frequently.

/*

 * The global memory commitment made in the system can be a metric

 * that can be used to drive ballooning decisions when Linux is hosted

 * as a guest. On Hyper-V, the host implements a policy engine for dynamically

 * balancing memory across competing virtual machines that are hosted.

 * Several metrics drive this policy engine including the guest reported

 * memory commitment.

 *

 * The time cost of this is very low for small platforms, and for big

 * platform like a 2S/36C/72T Skylake server, in worst case where

 * vm_committed_as's spinlock is under severe contention, the time cost

 * could be about 30~40 microseconds.

/*

 * Check that a process has enough memory to allocate a new virtual

 * mapping. 0 means there is enough memory for the allocation to

 * succeed and -ENOMEM implies there is not.

 *

 * We currently support three overcommit policies, which are set via the

 * vm.overcommit_memory sysctl.  See Documentation/vm/overcommit-accounting.rst

 *

 * Strict overcommit modes added 2002 Feb 26 by Alan Cox.

 * Additional code 2002 Jul 20 by Robert Love.

 *

 * cap_sys_admin is 1 if the process has admin privileges, 0 otherwise.

 *

 * Note this is a helper function intended to be used by LSMs which

 * wish to use this logic.

	/*

	 * Sometimes we want to use more memory than we have

	/*

	 * Reserve some for root

	/*

	 * Don't let a single process grow so big a user can't recover

/**

 * get_cmdline() - copy the cmdline value to a buffer.

 * @task:     the task whose cmdline value to copy.

 * @buffer:   the buffer to copy to.

 * @buflen:   the length of the buffer. Larger cmdline values are truncated

 *            to this length.

 *

 * Return: the size of the cmdline field copied. Note that the copy does

 * not guarantee an ending NULL byte.

 Shh! No looking before we're done */

	/*

	 * If the nul at the end of args has been overwritten, then

	 * assume application is using setproctitle(3).

/**

 * mem_dump_obj - Print available provenance information

 * @object: object for which to find provenance information.

 *

 * This function uses pr_cont(), so that the caller is expected to have

 * printed out whatever preamble is appropriate.  The provenance information

 * depends on the type of object and on how much debugging is enabled.

 * For example, for a slab-cache object, the slab name is printed, and,

 * if available, the return address and stack trace from the allocation

 * and last free path of that object.

/*

 * A driver might set a page logically offline -- PageOffline() -- and

 * turn the page inaccessible in the hypervisor; after that, access to page

 * content can be fatal.

 *

 * Some special PFN walkers -- i.e., /proc/kcore -- read content of random

 * pages after checking PageOffline(); however, these PFN walkers can race

 * with drivers that set PageOffline().

 *

 * page_offline_freeze()/page_offline_thaw() allows for a subsystem to

 * synchronize with such drivers, achieving that a page cannot be set

 * PageOffline() while frozen.

 *

 * page_offline_begin()/page_offline_end() is used by drivers that care about

 * such races when setting a page PageOffline().

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/page_alloc.c

 *

 *  Manages the free list, the system allocates free pages here.

 *  Note that kmalloc() lives in slab.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *  Swap reorganised 29.12.95, Stephen Tweedie

 *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999

 *  Reshaped it to be a zoned allocator, Ingo Molnar, Red Hat, 1999

 *  Discontiguous memory support, Kanoj Sarcar, SGI, Nov 1999

 *  Zone balancing, Kanoj Sarcar, SGI, Jan 2000

 *  Per cpu hot/cold page lists, bulk allocation, Martin J. Bligh, Sept 2002

 *          (lots of bits borrowed from Ingo Molnar & Andrew Morton)

 Free Page Internal flags: for internal, non-pcp variants of free_pages(). */

 No special request */

/*

 * Skip free page reporting notification for the (possibly merged) page.

 * This does not hinder free page reporting from grabbing the page,

 * reporting it and marking it "reported" -  it only skips notifying

 * the free page reporting infrastructure about a newly freed page. For

 * example, used when temporarily pulling a page from a freelist and

 * putting it back unmodified.

/*

 * Place the (possibly merged) page to the tail of the freelist. Will ignore

 * page shuffling (relevant code - e.g., memory onlining - is expected to

 * shuffle the whole zone).

 *

 * Note: No code should rely on this flag for correctness - it's purely

 *       to allow for optimizations when handing back either fresh pages

 *       (memory onlining) or untouched pages (page isolation, free page

 *       reporting).

/*

 * Don't poison memory with KASAN (only for the tag-based modes).

 * During boot, all non-reserved memblock memory is exposed to page_alloc.

 * Poisoning all that memory lengthens boot time, especially on systems with

 * large amount of RAM. This flag is used to skip that poisoning.

 * This is only done for the tag-based KASAN modes, as those are able to

 * detect memory corruptions with the memory tags assigned by default.

 * All memory allocated normally after boot gets poisoned as usual.

 prevent >1 _updater_ of zone percpu pageset ->high and ->batch fields */

/*

 * N.B., Do NOT reference the '_numa_mem_' per cpu variable directly.

 * It will not be defined when CONFIG_HAVE_MEMORYLESS_NODES is not defined.

 * Use the accessor functions set_numa_mem(), numa_mem_id() and cpu_to_mem()

 * defined in <linux/topology.h>.

 Kernel "local memory" node */

 work_structs for global per-cpu drains */

/*

 * Array of node states.

 NUMA */

/*

 * A cached value of the page's pageblock's migratetype, used when the page is

 * put on a pcplist. Used to avoid the pageblock migratetype lookup when

 * freeing from pcplists in most cases, at the cost of possibly becoming stale.

 * Also the migratetype set in the page does not necessarily match the pcplist

 * index, e.g. page might have MIGRATE_CMA set but be on a pcplist with any

 * other index - this ensures that it will be put on the correct CMA freelist.

/*

 * The following functions are used by the suspend/hibernate code to temporarily

 * change gfp_allowed_mask in order to avoid using I/O during memory allocations

 * while devices are suspended.  To avoid races with the suspend/hibernate code,

 * they should always be called with system_transition_mutex held

 * (gfp_allowed_mask also should only be modified with system_transition_mutex

 * held, unless the suspend/hibernate code is guaranteed not to run in parallel

 * with that modification).

 CONFIG_PM_SLEEP */

/*

 * results with 256, 32 in the lowmem_reserve sysctl:

 *	1G machine -> (16M dma, 800M-16M normal, 1G-800M high)

 *	1G machine -> (16M dma, 784M normal, 224M high)

 *	NORMAL allocation will leave 784M/256 of ram reserved in the ZONE_DMA

 *	HIGHMEM allocation will leave 224M/32 of ram reserved in ZONE_NORMAL

 *	HIGHMEM allocation will leave (224M+784M)/256 of ram reserved in ZONE_DMA

 *

 * TBD: should special case ZONE_DMA32 machines here - in those we normally

 * don't need any ZONE_NORMAL reservation

 movable_zone is the "real" zone pages in ZONE_MOVABLE are taken from */

/*

 * During boot we initialize deferred pages on-demand, as needed, but once

 * page_alloc_init_late() has finished, the deferred pages are all initialized,

 * and we can permanently disable that path.

/*

 * Calling kasan_poison_pages() only after deferred memory initialization

 * has completed. Poisoning pages during deferred memory init will greatly

 * lengthen the process and cause problem in large memory systems as the

 * deferred pages initialization is done with interrupt disabled.

 *

 * Assuming that there will be no reference to those newly initialized

 * pages before they are ever allocated, this should have no effect on

 * KASAN memory tracking as the poison will be properly inserted at page

 * allocation time. The only corner case is when pages are allocated by

 * on-demand allocation and then freed again before the deferred pages

 * initialization is done, but this is not likely to happen.

 Returns true if the struct page for the pfn is uninitialised */

/*

 * Returns true when the remaining initialisation should be deferred until

 * later in the boot cycle when it can be parallelised.

	/*

	 * prev_end_pfn static that contains the end of previous zone

	 * No need to protect because called very early in boot before smp_init.

 Always populate low zones for address-constrained allocations */

	/*

	 * We start only with one section of pages, more pages are added as

	 * needed until the rest of deferred pages are initialized.

 Return a pointer to the bitmap storing bits affecting a block of pages */

 CONFIG_SPARSEMEM */

 CONFIG_SPARSEMEM */

/**

 * get_pfnblock_flags_mask - Return the requested group of flags for the pageblock_nr_pages block of pages

 * @page: The page within the block of interest

 * @pfn: The target page frame number

 * @mask: mask of bits that the caller is interested in

 *

 * Return: pageblock_bits flags

/**

 * set_pfnblock_flags_mask - Set the requested group of flags for a pageblock_nr_pages block of pages

 * @page: The page within the block of interest

 * @flags: The flags to set

 * @pfn: The target page frame number

 * @mask: mask of bits that the caller is interested in

/*

 * Temporary debugging check for pages not lying within a given zone.

	/*

	 * Allow a burst of 60 reports, then keep quiet for that minute;

	 * or allow a steady drip of one report per second.

 Leave bad fields for debug, except PageBuddy could make trouble */

 remove PageBuddy */

 Via pcp? */

/*

 * Higher-order pages are called "compound pages".  They are structured thusly:

 *

 * The first PAGE_SIZE page is called the "head page" and have PG_head set.

 *

 * The remaining PAGE_SIZE pages are called "tail pages". PageTail() is encoded

 * in bit 0 of page->compound_head. The rest of bits is pointer to head page.

 *

 * The first tail page's ->compound_dtor holds the offset in array of compound

 * page destructors. See compound_page_dtors.

 *

 * The first tail page's ->compound_order holds the order of allocation.

 * This usage means that zero-order pages may not be compound.

 Guard pages are not available for any usage */

/*

 * Enable static keys related to various memory debugging and hardening options.

 * Some override others, and depend on early params that are evaluated in the

 * order of appearance. So we need to first gather the full picture of what was

 * enabled, and then make decisions.

	/*

	 * Page poisoning is debug page alloc for some arches. If

	 * either of those options are enabled, enable poisoning.

/*

 * This function checks whether a page is free && is the buddy

 * we can coalesce a page and its buddy if

 * (a) the buddy is not in a hole (check before calling!) &&

 * (b) the buddy is in the buddy system &&

 * (c) a page and its buddy have the same order &&

 * (d) a page and its buddy are in the same zone.

 *

 * For recording whether a page is in the buddy system, we set PageBuddy.

 * Setting, clearing, and testing PageBuddy is serialized by zone->lock.

 *

 * For recording page's order, we use page_private(page).

	/*

	 * zone check is done late to avoid uselessly calculating

	 * zone/node ids for pages that could never merge.

 Do not accidentally pollute CMA or isolated regions*/

	/*

	 * Do not let lower order allocations pollute a movable pageblock.

	 * This might let an unmovable request use a reclaimable pageblock

	 * and vice-versa but no more than normal fallback logic which can

	 * have trouble finding a high-order free page.

 CONFIG_COMPACTION */

 Used for pages not on another list */

 Used for pages not on another list */

/*

 * Used for pages which are on another list. Move the pages to the tail

 * of the list - so the moved pages won't immediately be considered for

 * allocation again (e.g., optimization for memory onlining).

 clear reported state and update reported page count */

/*

 * If this is not the largest possible page, check if the buddy

 * of the next-highest order is free. If it is, it's possible

 * that pages are being freed that will coalesce soon. In case,

 * that is happening, add the free page to the tail of the list

 * so it's less likely to be used soon and more likely to be merged

 * as a higher order page

/*

 * Freeing function for a buddy system allocator.

 *

 * The concept of a buddy system is to maintain direct-mapped table

 * (containing bit values) for memory blocks of various "orders".

 * The bottom level table contains the map for the smallest allocatable

 * units of memory (here, pages), and each level above it describes

 * pairs of units from the levels below, hence, "buddies".

 * At a high level, all that happens here is marking the table entry

 * at the bottom level available, and propagating the changes upward

 * as necessary, plus some accounting needed to play nicely with other

 * parts of the VM system.

 * At each level, we keep a list of pages, which are heads of continuous

 * free pages of length of (1 << order) and marked with PageBuddy.

 * Page's order is recorded in page_private(page) field.

 * So when we are allocating or freeing one, we can derive the state of the

 * other.  That is, if we allocate a small block, and both were

 * free, the remainder of the region must be split into blocks.

 * If a block is freed, and its buddy is also free, then this

 * triggers coalescing into a block of larger size.

 *

 * -- nyc

		/*

		 * Our buddy is free or it is CONFIG_DEBUG_PAGEALLOC guard page,

		 * merge with it and move up one order.

		/* If we are here, it means order is >= pageblock_order.

		 * We want to prevent merge between freepages on isolate

		 * pageblock and normal pageblock. Without this, pageblock

		 * isolation could cause incorrect freepage or CMA accounting.

		 *

		 * We don't want to hit this code for the more frequent

		 * low-order merging.

 Notify page reporting subsystem of freed page */

/*

 * A bad page could be due to a number of fields. Instead of multiple branches,

 * try and check multiple fields with one check. The caller must do a detailed

 * check if necessary.

 Something has gone sideways, find it */

	/*

	 * We rely page->lru.next never has bit 0 set, unless the page

	 * is PageTail(). Let's make sure that's true even for poisoned ->lru.

 the first tail page: ->mapping may be compound_mapcount() */

		/*

		 * the second tail page: ->mapping is

		 * deferred_list.next -- ignore value.

 s390's use of memset() could override KASAN redzones. */

		/*

		 * Do not let hwpoison pages hit pcplists/buddy

		 * Untie memcg state and reset page's owner

	/*

	 * Check tail pages before head page information is cleared to

	 * avoid checking PageCompound for order-0 pages.

	/*

	 * As memory initialization might be integrated into KASAN,

	 * kasan_free_pages and kernel_init_free_pages must be

	 * kept together to avoid discrepancies in behavior.

	 *

	 * With hardware tag-based KASAN, memory tags must be set before the

	 * page becomes unavailable via debug_pagealloc or arch_free_page.

	/*

	 * arch_free_page() can make the page's contents inaccessible.  s390

	 * does this.  So nothing which can access the page's contents should

	 * happen after this.

/*

 * With DEBUG_VM enabled, order-0 pages are checked immediately when being freed

 * to pcp lists. With debug_pagealloc also enabled, they are also rechecked when

 * moved from pcp lists to free lists.

/*

 * With DEBUG_VM disabled, order-0 pages being freed are checked only when

 * moving from pcp lists to free list in order to reduce overhead. With

 * debug_pagealloc enabled, they are checked also immediately when being freed

 * to the pcp lists.

 CONFIG_DEBUG_VM */

/*

 * Frees a number of pages from the PCP lists

 * Assumes all pages on list are in same zone.

 * count is the number of pages to free.

	/*

	 * Ensure proper count is passed which otherwise would stuck in the

	 * below while (list_empty(list)) loop.

		/*

		 * Remove pages from lists in a round-robin fashion. A

		 * batch_free count is maintained that is incremented when an

		 * empty list is encountered.  This is so more pages are freed

		 * off fuller lists instead of spinning excessively around empty

		 * lists

 This is the only non-empty list. Free them all. */

 must delete to avoid corrupting pcp list */

 Encode order with the migratetype */

			/*

			 * We are going to put the page back to the global

			 * pool, prefetch its buddy to speed up later access

			 * under zone->lock. It is believed the overhead of

			 * an additional test and calculating buddy_pfn here

			 * can be offset by reduced memory latency later. To

			 * avoid excessive prefetching due to large count, only

			 * prefetch buddy for the first pcp->batch nr of pages.

	/*

	 * local_lock_irq held so equivalent to spin_lock_irqsave for

	 * both PREEMPT_RT and non-PREEMPT_RT configurations.

	/*

	 * Use safe version since after __free_one_page(),

	 * page->lru.next will not point to original list.

 mt has been encoded with the order (see above) */

 MIGRATE_ISOLATE page should not go to pcplists */

 Pageblock could have been isolated meanwhile */

 The shift won't overflow because ZONE_NORMAL is below 4G. */

 CONFIG_DEFERRED_STRUCT_PAGE_INIT */

/*

 * Initialised pages do not have PageReserved set. This function is

 * called for each range allocated by the bootmem allocator and

 * marks the pages PageReserved. The remaining valid pages are later

 * sent to the buddy page allocator.

 Avoid false-positive PageTail() */

			/*

			 * no need for atomic set_bit because the struct

			 * page is not visible yet so nobody should

			 * access it yet.

	/*

	 * When initializing the memmap, __init_single_page() sets the refcount

	 * of all pages to 1 ("allocated"/"not free"). We have to set the

	 * refcount of all involved pages to 0.

	/*

	 * Bypass PCP and place fresh pages right to the tail, primarily

	 * relevant for memory onlining.

/*

 * During memory init memblocks map pfns to nids. The search is expensive and

 * this caches recent lookups. The implementation of __early_pfn_to_nid

 * treats start/end as pfns.

/*

 * Required by SPARSEMEM. Given a PFN, return what node the PFN is on.

 CONFIG_NUMA */

/*

 * Check that the whole (or subset of) a pageblock given by the interval of

 * [start_pfn, end_pfn) is valid and within the same zone, before scanning it

 * with the migration of free compaction scanner.

 *

 * Return struct page pointer of start_pfn, or NULL if checks were not passed.

 *

 * It's possible on some configurations to have a setup like node0 node1 node0

 * i.e. it's possible that all pages within a zones range of pages do not

 * belong to a single zone. We assume that a border between node0 and node1

 * can occur within a single pageblock, but not a node0 node1 node0

 * interleaving within a single pageblock. It is therefore sufficient to check

 * the first and last page of a pageblock and avoid checking each individual

 * page in a pageblock.

 end_pfn is one past the range we are checking */

 This gives a shorter code than deriving page_zone(end_page) */

 We confirm that there is no hole */

 Free a large naturally-aligned chunk if possible */

 Completion tracking for deferred_init_memmap() threads */

/*

 * Returns true if page needs to be initialized or freed to buddy allocator.

 *

 * First we check if pfn is valid on architectures where it is possible to have

 * holes within pageblock_nr_pages. On systems where it is not possible, this

 * function is optimized out.

 *

 * Then, we check if a current large page is valid by only checking the validity

 * of the head pfn.

/*

 * Free pages to buddy allocator. Try to free aligned pages in

 * pageblock_nr_pages sizes.

 Free the last block of pages to allocator */

/*

 * Initialize struct pages.  We minimize pfn page lookups and scheduler checks

 * by performing it only once every pageblock_nr_pages.

 * Return number of pages initialized.

/*

 * This function is meant to pre-load the iterator for the zone init.

 * Specifically it walks through the ranges until we are caught up to the

 * first_init_pfn value and exits there. If we never encounter the value we

 * return false indicating there are no valid ranges left.

	/*

	 * Start out by walking through the ranges in this zone that have

	 * already been initialized. We don't need to do anything with them

	 * so we just need to flush them out of the system.

/*

 * Initialize and free pages. We do it in two loops: first we initialize

 * struct page, then free to buddy allocator, because while we are

 * freeing pages we can access pages that are ahead (computing buddy

 * page in __free_one_page()).

 *

 * In order to try and keep some memory in the cache we have the loop

 * broken along max page order boundaries. This way we will not cause

 * any issues with the buddy page computation.

 First we loop through and initialize the page values */

 Reset values and now loop through freeing pages as needed */

	/*

	 * Initialize and free pages in MAX_ORDER sized increments so that we

	 * can avoid introducing any issues with the buddy allocator.

 An arch may override for more concurrency. */

 Initialise remaining memory on a node */

 Bind memory initialisation thread to a local node if possible */

 Sanity check boundaries */

	/*

	 * Once we unlock here, the zone cannot be grown anymore, thus if an

	 * interrupt thread must allocate this early in boot, zone must be

	 * pre-grown prior to start of deferred page initialization.

 Only the highest zone is deferred so find it */

 If the zone is empty somebody else may have cleared out the zone */

 Sanity check that the next zone really is unpopulated */

/*

 * If this zone has deferred pages, try to grow it by initializing enough

 * deferred pages to satisfy the allocation specified by order, rounded up to

 * the nearest PAGES_PER_SECTION boundary.  So we're adding memory in increments

 * of SECTION_SIZE bytes by initializing struct pages in increments of

 * PAGES_PER_SECTION * sizeof(struct page) bytes.

 *

 * Return true when zone was grown, otherwise return false. We return true even

 * when we grow less than requested, to let the caller decide if there are

 * enough pages to satisfy the allocation.

 *

 * Note: We use noinline because this function is needed only during boot, and

 * it is called from a __ref function _deferred_grow_zone. This way we are

 * making sure that it is not inlined into permanent text section.

 Only the last zone may have deferred pages */

	/*

	 * If someone grew this zone while we were waiting for spinlock, return

	 * true, as there might be enough pages already.

 If the zone is empty somebody else may have cleared out the zone */

 Retry only once. */

	/*

	 * Initialize and free pages in MAX_ORDER sized increments so

	 * that we can avoid introducing any issues with the buddy

	 * allocator.

 update our first deferred PFN for this section */

 We should only stop along section boundaries */

 If our quota has been met we can stop here */

/*

 * deferred_grow_zone() is __init, but it is called from

 * get_page_from_freelist() during early boot until deferred_pages permanently

 * disables this call. This is why we have refdata wrapper to avoid warning,

 * and to ensure that the function body gets unloaded.

 CONFIG_DEFERRED_STRUCT_PAGE_INIT */

 There will be num_node_state(N_MEMORY) threads */

 Block until all are initialised */

	/*

	 * We initialized the rest of the deferred pages.  Permanently disable

	 * on-demand struct page initialization.

 Reinit limits that are based on free pages after the kernel is up */

 Discard memblock private memory */

 Free whole pageblock and set its migration type to MIGRATE_CMA. */

/*

 * The order of subdivision here is critical for the IO subsystem.

 * Please do not alter this order without good reasons and regression

 * testing. Specifically, as large blocks of memory are subdivided,

 * the order in which smaller blocks are delivered depends on the order

 * they're subdivided in this function. This is the primary factor

 * influencing the order in which pages are delivered to the IO

 * subsystem according to empirical testing, and this is also justified

 * by considering the behavior of a buddy system containing a single

 * large block of memory acted on by a series of small allocations.

 * This behavior is a critical factor in sglist merging's success.

 *

 * -- nyc

		/*

		 * Mark as guard pages (or page), that will allow to

		 * merge back to allocator when buddy will be freed.

		 * Corresponding page table entries will not be touched,

		 * pages will stay not present in virtual address space

 Don't complain about hwpoisoned pages */

 remove PageBuddy */

/*

 * This page is about to be returned from the page allocator

/*

 * With DEBUG_VM enabled, order-0 pages are checked for expected state when

 * being allocated from pcp lists. With debug_pagealloc also enabled, they are

 * also checked when pcp lists are refilled from the free lists.

/*

 * With DEBUG_VM disabled, free order-0 pages are checked for expected state

 * when pcp lists are being refilled from the free lists. With debug_pagealloc

 * enabled, they are also checked when being allocated from the pcp lists.

 CONFIG_DEBUG_VM */

	/*

	 * Page unpoisoning must happen before memory initialization.

	 * Otherwise, the poison pattern will be overwritten for __GFP_ZERO

	 * allocations and the page unpoisoning code will complain.

	/*

	 * As memory initialization might be integrated into KASAN,

	 * kasan_alloc_pages and kernel_init_free_pages must be

	 * kept together to avoid discrepancies in behavior.

	/*

	 * page is set pfmemalloc when ALLOC_NO_WATERMARKS was necessary to

	 * allocate the page. The expectation is that the caller is taking

	 * steps that will free more memory. The caller should avoid the page

	 * being used for !PFMEMALLOC purposes.

/*

 * Go through the free lists for the given migratetype and remove

 * the smallest available page from the freelists

 Find a page of the appropriate size in the preferred list */

/*

 * This array describes the order lists are fallen back to when

 * the free lists for the desirable migrate type are depleted

 Never used */

 Never used */

/*

 * Move the free pages in a range to the freelist tail of the requested type.

 * Note that start_page and end_pages are not aligned on a pageblock

 * boundary. If alignment is required, use move_freepages_block()

			/*

			 * We assume that pages that could be isolated for

			 * migration are movable. But we don't actually try

			 * isolating, as that would be expensive.

 Make sure we are not inadvertently changing nodes */

 Do not cross zone boundaries */

/*

 * When we are falling back to another migratetype during allocation, try to

 * steal extra free pages from the same pageblocks to satisfy further

 * allocations, instead of polluting multiple pageblocks.

 *

 * If we are stealing a relatively large buddy page, it is likely there will

 * be more free pages in the pageblock, so try to steal them all. For

 * reclaimable and unmovable allocations, we steal regardless of page size,

 * as fragmentation caused by those allocations polluting movable pageblocks

 * is worse than movable allocations stealing from unmovable and reclaimable

 * pageblocks.

	/*

	 * Leaving this order check is intended, although there is

	 * relaxed order check in next check. The reason is that

	 * we can actually steal whole pageblock if this condition met,

	 * but, below check doesn't guarantee it and that is just heuristic

	 * so could be changed anytime.

	/*

	 * Don't bother in zones that are unlikely to produce results.

	 * On small machines, including kdump capture kernels running

	 * in a small area, boosting the watermark can cause an out of

	 * memory situation immediately.

	/*

	 * high watermark may be uninitialised if fragmentation occurs

	 * very early in boot so do not boost. We do not fall

	 * through and boost by pageblock_nr_pages as failing

	 * allocations that early means that reclaim is not going

	 * to help and it may even be impossible to reclaim the

	 * boosted watermark resulting in a hang.

/*

 * This function implements actual steal behaviour. If order is large enough,

 * we can steal whole pageblock. If not, we first move freepages in this

 * pageblock to our migratetype and determine how many already-allocated pages

 * are there in the pageblock with a compatible migratetype. If at least half

 * of pages are free or compatible, we can change migratetype of the pageblock

 * itself, so pages freed in the future will be put on the correct free list.

	/*

	 * This can happen due to races and we want to prevent broken

	 * highatomic accounting.

 Take ownership for orders >= pageblock_order */

	/*

	 * Boost watermarks to increase reclaim pressure to reduce the

	 * likelihood of future fallbacks. Wake kswapd now as the node

	 * may be balanced overall and kswapd will not wake naturally.

 We are not allowed to try stealing from the whole block */

	/*

	 * Determine how many pages are compatible with our allocation.

	 * For movable allocation, it's the number of movable pages which

	 * we just obtained. For other types it's a bit more tricky.

		/*

		 * If we are falling back a RECLAIMABLE or UNMOVABLE allocation

		 * to MOVABLE pageblock, consider all non-movable pages as

		 * compatible. If it's UNMOVABLE falling back to RECLAIMABLE or

		 * vice versa, be conservative since we can't distinguish the

		 * exact migratetype of non-movable pages.

 moving whole block can fail due to zone boundary conditions */

	/*

	 * If a sufficient number of pages in the block are either free or of

	 * comparable migratability as our allocation, claim the whole block.

/*

 * Check whether there is a suitable fallback freepage with requested order.

 * If only_stealable is true, this function returns fallback_mt only if

 * we can steal other freepages all together. This would help to reduce

 * fragmentation due to mixed migratetype pages in one pageblock.

/*

 * Reserve a pageblock for exclusive use of high-order atomic allocations if

 * there are no empty page blocks that contain a page with a suitable order

	/*

	 * Limit the number reserved to 1 pageblock or roughly 1% of a zone.

	 * Check is race-prone but harmless.

 Recheck the nr_reserved_highatomic limit under the lock */

 Yoink! */

/*

 * Used when an allocation is about to fail under memory pressure. This

 * potentially hurts the reliability of high-order allocations when under

 * intense memory pressure but failed atomic allocations should be easier

 * to recover from than an OOM.

 *

 * If @force is true, try to unreserve a pageblock even though highatomic

 * pageblock is exhausted.

		/*

		 * Preserve at least one pageblock unless memory pressure

		 * is really high.

			/*

			 * In page freeing path, migratetype change is racy so

			 * we can counter several free pages in a pageblock

			 * in this loop although we changed the pageblock type

			 * from highatomic to ac->migratetype. So we should

			 * adjust the count once.

				/*

				 * It should never happen but changes to

				 * locking could inadvertently allow a per-cpu

				 * drain to add pages to MIGRATE_HIGHATOMIC

				 * while unreserving so be safe and watch for

				 * underflows.

			/*

			 * Convert to ac->migratetype and avoid the normal

			 * pageblock stealing heuristics. Minimally, the caller

			 * is doing the work and needs the pages. More

			 * importantly, if the block was always converted to

			 * MIGRATE_UNMOVABLE or another type then the number

			 * of pageblocks that cannot be completely freed

			 * may increase.

/*

 * Try finding a free buddy page on the fallback list and put it on the free

 * list of requested migratetype, possibly along with other pages from the same

 * block, depending on fragmentation avoidance heuristics. Returns true if

 * fallback was found so that __rmqueue_smallest() can grab it.

 *

 * The use of signed ints for order and current_order is a deliberate

 * deviation from the rest of this file, to make the for loop

 * condition simpler.

	/*

	 * Do not steal pages from freelists belonging to other pageblocks

	 * i.e. orders < pageblock_order. If there are no local zones free,

	 * the zonelists will be reiterated without ALLOC_NOFRAGMENT.

	/*

	 * Find the largest available free page in the other list. This roughly

	 * approximates finding the pageblock with the most free pages, which

	 * would be too costly to do exactly.

		/*

		 * We cannot steal all free pages from the pageblock and the

		 * requested migratetype is movable. In that case it's better to

		 * steal and split the smallest available page instead of the

		 * largest available page, because even if the next movable

		 * allocation falls back into a different pageblock than this

		 * one, it won't cause permanent fragmentation.

	/*

	 * This should not happen - we already found a suitable fallback

	 * when looking for the largest page.

/*

 * Do the hard work of removing an element from the buddy allocator.

 * Call me with the zone->lock already held.

		/*

		 * Balance movable allocations between regular and CMA areas by

		 * allocating from CMA when over half of the zone's free memory

		 * is in the CMA area.

/*

 * Obtain a specified number of elements from the buddy allocator, all under

 * a single hold of the lock, for efficiency.  Add them to the supplied list.

 * Returns the number of new pages which were placed at *list.

	/*

	 * local_lock_irq held so equivalent to spin_lock_irqsave for

	 * both PREEMPT_RT and non-PREEMPT_RT configurations.

		/*

		 * Split buddy pages returned by expand() are received here in

		 * physical page order. The page is added to the tail of

		 * caller's list. From the callers perspective, the linked list

		 * is ordered by page number under some conditions. This is

		 * useful for IO devices that can forward direction from the

		 * head, thus also in the physical page order. This is useful

		 * for IO devices that can merge IO requests if the physical

		 * pages are ordered properly.

	/*

	 * i pages were removed from the buddy list even if some leak due

	 * to check_pcp_refill failing so adjust NR_FREE_PAGES based

	 * on i. Do not confuse with 'allocated' which is the number of

	 * pages added to the pcp list.

/*

 * Called from the vmstat counter updater to drain pagesets of this

 * currently executing processor on remote nodes after they have

 * expired.

 *

 * Note that this function must be called with the thread pinned to

 * a single processor.

/*

 * Drain pcplists of the indicated processor and zone.

 *

 * The processor must either be the current processor and the

 * thread pinned to the current processor or a processor that

 * is not online.

/*

 * Drain pcplists of all zones on the indicated processor.

 *

 * The processor must either be the current processor and the

 * thread pinned to the current processor or a processor that

 * is not online.

/*

 * Spill all of this CPU's per-cpu pages back into the buddy allocator.

 *

 * The CPU has to be pinned. When zone parameter is non-NULL, spill just

 * the single zone's pages.

	/*

	 * drain_all_pages doesn't use proper cpu hotplug protection so

	 * we can race with cpu offline when the WQ can move this from

	 * a cpu pinned worker to an unbound one. We can operate on a different

	 * cpu which is alright but we also have to make sure to not move to

	 * a different one.

/*

 * The implementation of drain_all_pages(), exposing an extra parameter to

 * drain on all cpus.

 *

 * drain_all_pages() is optimized to only execute on cpus where pcplists are

 * not empty. The check for non-emptiness can however race with a free to

 * pcplist that has not yet increased the pcp->count from 0 to 1. Callers

 * that need the guarantee that every CPU has drained can disable the

 * optimizing racy check.

	/*

	 * Allocate in the BSS so we won't require allocation in

	 * direct reclaim path for CONFIG_CPUMASK_OFFSTACK=y

	/*

	 * Make sure nobody triggers this path before mm_percpu_wq is fully

	 * initialized.

	/*

	 * Do not drain if one is already in progress unless it's specific to

	 * a zone. Such callers are primarily CMA and memory hotplug and need

	 * the drain to be complete when the call returns.

	/*

	 * We don't care about racing with CPU hotplug event

	 * as offline notification will cause the notified

	 * cpu to drain that CPU pcps and on_each_cpu_mask

	 * disables preemption as part of its processing

			/*

			 * The pcp.count check is racy, some callers need a

			 * guarantee that no cpu is missed.

/*

 * Spill all the per-cpu pages from all CPUs back into the buddy allocator.

 *

 * When zone parameter is non-NULL, spill just the single zone's pages.

 *

 * Note that this can be extremely slow as the draining happens in a workqueue.

/*

 * Touch the watchdog for every WD_PAGE_COUNT pages.

 CONFIG_PM */

 Check for PCP disabled or boot pageset */

 Leave at least pcp->batch pages on the list */

	/*

	 * Double the number of pages freed each time there is subsequent

	 * freeing of pages without any allocation.

	/*

	 * If reclaim is active, limit the number of pages that can be

	 * stored on pcp lists

/*

 * Free a pcp page

	/*

	 * We only track unmovable, reclaimable and movable on pcp lists.

	 * Place ISOLATE pages on the isolated list because they are being

	 * offlined but treat HIGHATOMIC as movable pages so we can get those

	 * areas back if necessary. Otherwise, we may have to free

	 * excessively into the page allocator

/*

 * Free a list of 0-order pages

 Prepare pages for freeing */

		/*

		 * Free isolated pages directly to the allocator, see

		 * comment in free_unref_page.

		/*

		 * Non-isolated types over MIGRATE_PCPTYPES get added

		 * to the MIGRATE_MOVABLE pcp list.

		/*

		 * Guard against excessive IRQ disabled times when we get

		 * a large list of pages to free.

/*

 * split_page takes a non-compound higher-order page, and splits it into

 * n (1<<order) sub-pages: page[0..n]

 * Each sub-page must be freed individually.

 *

 * Note: this is probably too low level an operation for use in drivers.

 * Please consult with lkml before using this in your driver.

		/*

		 * Obey watermarks as if the page was being allocated. We can

		 * emulate a high-order watermark check with a raised order-0

		 * watermark, because we already know our high-order page

		 * exists.

 Remove page from free list */

	/*

	 * Set the pageblock if the isolated page is at least half of a

	 * pageblock

/**

 * __putback_isolated_page - Return a now-isolated page back where we got it

 * @page: Page that was isolated

 * @order: Order of the isolated page

 * @mt: The page's pageblock's migratetype

 *

 * This function is meant to return a page pulled from the free lists via

 * __isolate_free_page back to the free lists they were pulled from.

 zone lock should be held when this function is called */

 Return isolated page to tail of freelist. */

/*

 * Update NUMA hit/miss statistics

 *

 * Must be called with interrupts disabled.

 skip numa counters update if numa stats is disabled */

 Remove page from the per-cpu list, caller must protect the list */

			/*

			 * Scale batch relative to order if batch implies

			 * free pages can be stored on the PCP. Batch can

			 * be 1 for small zones or for boot pagesets which

			 * should never store free pages as the pages may

			 * belong to arbitrary zones.

 Lock and remove page from the per-cpu list */

	/*

	 * On allocation, reduce the number of pages that are batch freed.

	 * See nr_pcp_free() where free_factor is increased for subsequent

	 * frees.

/*

 * Allocate a page from the given zone. Use pcplists for order-0 allocations.

		/*

		 * MIGRATE_MOVABLE pcplist could have the pages on CMA area and

		 * we need to skip it when CMA area isn't allowed.

	/*

	 * We most definitely don't want callers attempting to

	 * allocate greater than order-1 page units with __GFP_NOFAIL.

		/*

		 * order-0 request can reach here when the pcplist is skipped

		 * due to non-CMA allocation context. HIGHATOMIC area is

		 * reserved for high-order atomic allocation, so order-0

		 * request should skip it.

 Separate test+clear to avoid unnecessary atomics */

 CONFIG_FAULT_INJECTION_DEBUG_FS */

 CONFIG_FAIL_PAGE_ALLOC */

 CONFIG_FAIL_PAGE_ALLOC */

	/*

	 * If the caller does not have rights to ALLOC_HARDER then subtract

	 * the high-atomic reserves. This will over-estimate the size of the

	 * atomic reserve but it avoids a search.

 If allocation can't use CMA areas don't use free CMA pages */

/*

 * Return true if free base pages are above 'mark'. For high-order checks it

 * will return true of the order-0 watermark is reached and there is at least

 * one free page of a suitable size. Checking now avoids taking the zone lock

 * to check in the allocation paths if no pages are free.

 free_pages may go negative - that's OK */

		/*

		 * OOM victims can try even harder than normal ALLOC_HARDER

		 * users on the grounds that it's definitely going to be in

		 * the exit path shortly and free memory. Any allocation it

		 * makes during the free path will be small and short-lived.

	/*

	 * Check watermarks for an order-0 allocation request. If these

	 * are not met, then a high-order request also cannot go ahead

	 * even if a suitable page happened to be free.

 If this is an order-0 request then the watermark is fine */

 For a high-order request, check at least one suitable page is free */

	/*

	 * Fast check for order-0 only. If this fails then the reserves

	 * need to be calculated.

	/*

	 * Ignore watermark boosting for GFP_ATOMIC order-0 allocations

	 * when checking the min watermark. The min watermark is the

	 * point where boosting is ignored so that kswapd is woken up

	 * when below the low watermark.

 CONFIG_NUMA */

 CONFIG_NUMA */

/*

 * The restriction on ZONE_DMA32 as being a suitable zone to use to avoid

 * fragmentation is subtle. If the preferred zone was HIGHMEM then

 * premature use of a lower zone may cause lowmem pressure problems that

 * are worse than fragmentation. If the next zone is ZONE_DMA then it is

 * probably too small. It only makes sense to spread allocations to avoid

 * fragmentation between the Normal and DMA32 zones.

	/*

	 * __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD

	 * to save a branch.

	/*

	 * If ZONE_DMA32 exists, assume it is the one after ZONE_NORMAL and

	 * the pointer is within zone->zone_pgdat->node_zones[]. Also assume

	 * on UMA that if Normal is populated then so is DMA32.

 CONFIG_ZONE_DMA32 */

 Must be called after current_gfp_context() which can change gfp_mask */

/*

 * get_page_from_freelist goes through the zonelist trying to allocate

 * a page.

	/*

	 * Scan zonelist, looking for a zone with enough free.

	 * See also __cpuset_node_allowed() comment in kernel/cpuset.c.

		/*

		 * When allocating a page cache page for writing, we

		 * want to get it from a node that is within its dirty

		 * limit, such that no single node holds more than its

		 * proportional share of globally allowed dirty pages.

		 * The dirty limits take into account the node's

		 * lowmem reserves and high watermark so that kswapd

		 * should be able to balance it without having to

		 * write pages from its LRU list.

		 *

		 * XXX: For now, allow allocations to potentially

		 * exceed the per-node dirty limit in the slowpath

		 * (spread_dirty_pages unset) before going into reclaim,

		 * which is important when on a NUMA setup the allowed

		 * nodes are together not big enough to reach the

		 * global limit.  The proper fix for these situations

		 * will require awareness of nodes in the

		 * dirty-throttling and the flusher threads.

			/*

			 * If moving to a remote node, retry but allow

			 * fragmenting fallbacks. Locality is more important

			 * than fragmentation avoidance.

			/*

			 * Watermark failed for this zone, but see if we can

			 * grow this zone if it contains deferred pages.

 Checked here to keep the fast path fast */

 did not scan */

 scanned but unreclaimable */

 did we reclaim enough */

			/*

			 * If this is a high-order atomic allocation then check

			 * if the pageblock should be reserved for the future

 Try again if zone has deferred pages */

	/*

	 * It's possible on a UMA machine to get through all zones that are

	 * fragmented. If avoiding fragmentation, reset and try again.

	/*

	 * This documents exceptions given to allocations in certain

	 * contexts that are allowed to allocate outside current's set

	 * of allowed nodes.

	/*

	 * fallback to ignore cpuset restriction if our nodes

	 * are depleted

	/*

	 * Acquire the oom lock.  If that fails, somebody else is

	 * making progress for us.

	/*

	 * Go through the zonelist yet one more time, keep very high watermark

	 * here, this is only to catch a parallel oom killing, we must fail if

	 * we're still under heavy pressure. But make sure that this reclaim

	 * attempt shall not depend on __GFP_DIRECT_RECLAIM && !__GFP_NORETRY

	 * allocation which will never fail due to oom_lock already held.

 Coredumps can quickly deplete all memory reserves */

 The OOM killer will not help higher order allocs */

	/*

	 * We have already exhausted all our reclaim opportunities without any

	 * success so it is time to admit defeat. We will skip the OOM killer

	 * because it is very likely that the caller has a more reasonable

	 * fallback than shooting a random task.

	 *

	 * The OOM killer may not free memory on a specific node.

 The OOM killer does not needlessly kill tasks for lowmem */

	/*

	 * XXX: GFP_NOFS allocations should rather fail than rely on

	 * other request to make a forward progress.

	 * We are in an unfortunate situation where out_of_memory cannot

	 * do much for this context but let's try it to at least get

	 * access to memory reserved if the current task is killed (see

	 * out_of_memory). Once filesystems are ready to handle allocation

	 * failures more gracefully we should just bail out here.

 Exhausted what can be done so it's blame time */

		/*

		 * Help non-failing allocations by giving them access to memory

		 * reserves

/*

 * Maximum number of compaction retries with a progress before OOM

 * killer is consider as the only way to move forward.

 Try memory compaction for high-order allocations before reclaim */

	/*

	 * At least in one zone compaction wasn't deferred or skipped, so let's

	 * count a compaction stall

 Prep a captured page if available */

 Try get a page from the freelist if available */

	/*

	 * It's bad if compaction run occurs and fails. The most likely reason

	 * is that pages exist, but not enough to satisfy watermarks.

	/*

	 * compaction considers all the zone as desperately out of memory

	 * so it doesn't really make much sense to retry except when the

	 * failure could be caused by insufficient priority

	/*

	 * compaction was skipped because there are not enough order-0 pages

	 * to work with, so we retry only if it looks like reclaim can help.

	/*

	 * make sure the compaction wasn't deferred or didn't bail out early

	 * due to locks contention before we declare that we should give up.

	 * But the next retry should use a higher priority if allowed, so

	 * we don't just keep bailing out endlessly.

	/*

	 * !costly requests are much more important than __GFP_RETRY_MAYFAIL

	 * costly ones because they are de facto nofail and invoke OOM

	 * killer to move on while costly can fail and users are ready

	 * to cope with that. 1/4 retries is rather arbitrary but we

	 * would need much more detailed feedback from compaction to

	 * make a better decision.

	/*

	 * Make sure there are attempts at the highest priority if we exhausted

	 * all retries or failed at the lower priorities.

	/*

	 * There are setups with compaction disabled which would prefer to loop

	 * inside the allocator rather than hit the oom killer prematurely.

	 * Let's give them a good hope and keep retrying while the order-0

	 * watermarks are OK.

 CONFIG_COMPACTION */

 no reclaim without waiting on it */

 this guy won't enter reclaim */

 Perform direct synchronous page reclaim */

 We now go into synchronous reclaim */

 The really slow allocator path where we enter direct reclaim */

	/*

	 * If an allocation failed after direct reclaim, it could be because

	 * pages are pinned on the per-cpu lists or in high alloc reserves.

	 * Shrink them and try again

	/*

	 * __GFP_HIGH is assumed to be the same as ALLOC_HIGH

	 * and __GFP_KSWAPD_RECLAIM is assumed to be the same as ALLOC_KSWAPD

	 * to save two branches.

	/*

	 * The caller may dip into page reserves a bit more if the caller

	 * cannot run direct reclaim, or if the caller has realtime scheduling

	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will

	 * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).

		/*

		 * Not worth trying to allocate harder for __GFP_NOMEMALLOC even

		 * if it can't schedule.

		/*

		 * Ignore cpuset mems for GFP_ATOMIC rather than fail, see the

		 * comment for __cpuset_node_allowed().

	/*

	 * !MMU doesn't have oom reaper so give access to memory reserves

	 * only to the thread with TIF_MEMDIE set

/*

 * Distinguish requests which really need access to full memory

 * reserves from oom victims which can live with a portion of it

/*

 * Checks whether it makes sense to retry the reclaim to make a forward progress

 * for the given allocation request.

 *

 * We give up when we either have tried MAX_RECLAIM_RETRIES in a row

 * without success, or when we couldn't even meet the watermark if we

 * reclaimed all remaining pages on the LRU lists.

 *

 * Returns true if a retry is viable or false to enter the oom path.

	/*

	 * Costly allocations might have made a progress but this doesn't mean

	 * their order will become available due to high fragmentation so

	 * always increment the no progress counter for them

	/*

	 * Make sure we converge to OOM if we cannot make any progress

	 * several times in the row.

 Before OOM, exhaust highatomic_reserve */

	/*

	 * Keep reclaiming pages while there is a chance this will lead

	 * somewhere.  If none of the target zones can satisfy our allocation

	 * request even if all reclaimable pages are considered then we are

	 * screwed and have to go OOM.

		/*

		 * Would the allocation succeed if we reclaimed all

		 * reclaimable pages?

	/*

	 * Memory allocation/reclaim might be called from a WQ context and the

	 * current implementation of the WQ concurrency control doesn't

	 * recognize that a particular WQ is congested if the worker thread is

	 * looping without ever sleeping. Therefore we have to do a short sleep

	 * here rather than calling cond_resched().

	/*

	 * It's possible that cpuset's mems_allowed and the nodemask from

	 * mempolicy don't intersect. This should be normally dealt with by

	 * policy_nodemask(), but it's possible to race with cpuset update in

	 * such a way the check therein was true, and then it became false

	 * before we got our cpuset_mems_cookie here.

	 * This assumes that for all allocations, ac->nodemask can come only

	 * from MPOL_BIND mempolicy (whose documented semantics is to be ignored

	 * when it does not intersect with the cpuset restrictions) or the

	 * caller can deal with a violated nodemask.

	/*

	 * When updating a task's mems_allowed or mempolicy nodemask, it is

	 * possible to race with parallel threads in such a way that our

	 * allocation can fail while the mask is being updated. If we are about

	 * to fail, check if the cpuset changed during allocation and if so,

	 * retry.

	/*

	 * We also sanity check to catch abuse of atomic reserves being used by

	 * callers that are not in atomic context.

	/*

	 * The fast path uses conservative alloc_flags to succeed only until

	 * kswapd needs to be woken up, and to avoid the cost of setting up

	 * alloc_flags precisely. So we do that now.

	/*

	 * We need to recalculate the starting point for the zonelist iterator

	 * because we might have used different nodemask in the fast path, or

	 * there was a cpuset modification and we are retrying - otherwise we

	 * could end up iterating over non-eligible zones endlessly.

	/*

	 * Check for insane configurations where the cpuset doesn't contain

	 * any suitable zone to satisfy the request - e.g. non-movable

	 * GFP_HIGHUSER allocations from MOVABLE nodes only.

	/*

	 * The adjusted alloc_flags might result in immediate success, so try

	 * that first

	/*

	 * For costly allocations, try direct compaction first, as it's likely

	 * that we have enough base pages and don't need to reclaim. For non-

	 * movable high-order allocations, do that as well, as compaction will

	 * try prevent permanent fragmentation by migrating from blocks of the

	 * same migratetype.

	 * Don't try this for allocations that are allowed to ignore

	 * watermarks, as the ALLOC_NO_WATERMARKS attempt didn't yet happen.

		/*

		 * Checks for costly allocations with __GFP_NORETRY, which

		 * includes some THP page fault allocations

			/*

			 * If allocating entire pageblock(s) and compaction

			 * failed because all zones are below low watermarks

			 * or is prohibited because it recently failed at this

			 * order, fail immediately unless the allocator has

			 * requested compaction and reclaim retry.

			 *

			 * Reclaim is

			 *  - potentially very expensive because zones are far

			 *    below their low watermarks or this is part of very

			 *    bursty high order allocations,

			 *  - not guaranteed to help because isolate_freepages()

			 *    may not iterate over freed pages as part of its

			 *    linear scan, and

			 *  - unlikely to make entire pageblocks free on its

			 *    own.

			/*

			 * Looks like reclaim/compaction is worth trying, but

			 * sync compaction could be very expensive, so keep

			 * using async compaction.

 Ensure kswapd doesn't accidentally go to sleep as long as we loop */

	/*

	 * Reset the nodemask and zonelist iterators if memory policies can be

	 * ignored. These allocations are high priority and system rather than

	 * user oriented.

 Attempt with potentially adjusted zonelist and alloc_flags */

 Caller is not willing to reclaim, we can't balance anything */

 Avoid recursion of direct reclaim */

 Try direct reclaim and then allocating */

 Try direct compaction and then allocating */

 Do not loop if specifically requested */

	/*

	 * Do not retry costly high order allocations unless they are

	 * __GFP_RETRY_MAYFAIL

	/*

	 * It doesn't make any sense to retry for the compaction if the order-0

	 * reclaim is not able to make any progress because the current

	 * implementation of the compaction depends on the sufficient amount

	 * of free memory (see __compaction_suitable)

 Deal with possible cpuset update races before we start OOM killing */

 Reclaim has failed us, start killing things */

 Avoid allocations with no watermarks from looping endlessly */

 Retry as long as the OOM killer is making progress */

 Deal with possible cpuset update races before we fail */

	/*

	 * Make sure that __GFP_NOFAIL request doesn't leak out and make sure

	 * we always retry

		/*

		 * All existing users of the __GFP_NOFAIL are blockable, so warn

		 * of any new users that actually require GFP_NOWAIT

		/*

		 * PF_MEMALLOC request from this context is rather bizarre

		 * because we cannot reclaim anything and only can loop waiting

		 * for somebody to do a work for us

		/*

		 * non failing costly orders are a hard requirement which we

		 * are not prepared for much so let's warn about these users

		 * so that we can identify them and convert them to something

		 * else.

		/*

		 * Help non-failing allocations by giving them access to memory

		 * reserves but do not use ALLOC_NO_WATERMARKS because this

		 * could deplete whole memory reserves which would just make

		 * the situation worse

		/*

		 * When we are in the interrupt context, it is irrelevant

		 * to the current task context. It means that any node ok.

 Dirty zone balancing only done in the fast path */

	/*

	 * The preferred zone is used for statistics but crucially it is

	 * also used as the starting point for the zonelist iterator. It

	 * may get reset for allocations that ignore memory policies.

/*

 * __alloc_pages_bulk - Allocate a number of order-0 pages to a list or array

 * @gfp: GFP flags for the allocation

 * @preferred_nid: The preferred NUMA node ID to allocate from

 * @nodemask: Set of nodes to allocate from, may be NULL

 * @nr_pages: The number of pages desired on the list or array

 * @page_list: Optional list to store the allocated pages

 * @page_array: Optional array to store the pages

 *

 * This is a batched version of the page allocator that attempts to

 * allocate nr_pages quickly. Pages are added to page_list if page_list

 * is not NULL, otherwise it is assumed that the page_array is valid.

 *

 * For lists, nr_pages is the number of pages that should be allocated.

 *

 * For arrays, only NULL elements are populated with pages and nr_pages

 * is the maximum number of pages that will be stored in the array.

 *

 * Returns the number of pages on the list or array.

	/*

	 * Skip populated array elements to determine if any pages need

	 * to be allocated before disabling IRQs.

 No pages requested? */

 Already populated array? */

 Bulk allocator does not support memcg accounting. */

 Use the single page allocator for one page. */

	/*

	 * PAGE_OWNER may recurse into the allocator to allocate space to

	 * save the stack with pagesets.lock held. Releasing/reacquiring

	 * removes much of the performance benefit of bulk allocation so

	 * force the caller to allocate one page at a time as it'll have

	 * similar performance to added complexity to the bulk allocator.

 May set ALLOC_NOFRAGMENT, fragmentation will return 1 page. */

 Find an allowed local zone that meets the low watermark. */

	/*

	 * If there are no allowed local zones that meets the watermarks then

	 * try to allocate a single page and reclaim if necessary.

 Attempt the batch allocation */

 Skip existing pages */

 Try and get at least one page */

/*

 * This is the 'heart' of the zoned buddy allocator.

 The gfp_t that was actually used for allocation */

	/*

	 * There are several places where we assume that the order value is sane

	 * so bail out early if the request is out of bound.

	/*

	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS

	 * resp. GFP_NOIO which has to be inherited for all allocation requests

	 * from a particular context which has been marked by

	 * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures

	 * movable zones are not used during allocation.

	/*

	 * Forbid the first pass from falling back to types that fragment

	 * memory until all local zones are considered.

 First allocation attempt */

	/*

	 * Restore the original nodemask if it was potentially replaced with

	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.

/*

 * Common helper functions. Never use with __GFP_HIGHMEM because the returned

 * address cannot represent highmem pages. Use alloc_pages and then kmap if

 * you need to access high mem.

/**

 * __free_pages - Free pages allocated with alloc_pages().

 * @page: The page pointer returned from alloc_pages().

 * @order: The order of the allocation.

 *

 * This function can free multi-page allocations that are not compound

 * pages.  It does not check that the @order passed in matches that of

 * the allocation, so it is easy to leak memory.  Freeing more memory

 * than was allocated will probably emit a warning.

 *

 * If the last reference to this page is speculative, it will be released

 * by put_page() which only frees the first page of a non-compound

 * allocation.  To prevent the remaining pages from being leaked, we free

 * the subsequent pages here.  If you want to use the page's reference

 * count to decide when to free the allocation, you should allocate a

 * compound page, and use put_page() instead of __free_pages().

 *

 * Context: May be called in interrupt context or while holding a normal

 * spinlock, but not in NMI context or while holding a raw spinlock.

/*

 * Page Fragment:

 *  An arbitrary-length arbitrary-offset area of memory which resides

 *  within a 0 or higher order page.  Multiple fragments within that page

 *  are individually refcounted, in the page's reference counter.

 *

 * The page_frag functions below provide a simple allocation framework for

 * page fragments.  This is used by the network stack and network device

 * drivers to provide a backing region of memory for use as either an

 * sk_buff->head, or to be used in the "frags" portion of skb_shared_info.

 if size can vary use size else just use PAGE_SIZE */

		/* Even if we own the page, we do not use atomic_set().

		 * This would break get_page_unless_zero() users.

 reset page count bias and offset to start of new frag */

 if size can vary use size else just use PAGE_SIZE */

 OK, page count is 0, we can safely set it */

 reset page count bias and offset to start of new frag */

/*

 * Frees a page fragment allocated out of either a compound or order 0 page.

/**

 * alloc_pages_exact - allocate an exact number physically-contiguous pages.

 * @size: the number of bytes to allocate

 * @gfp_mask: GFP flags for the allocation, must not contain __GFP_COMP

 *

 * This function is similar to alloc_pages(), except that it allocates the

 * minimum number of pages to satisfy the request.  alloc_pages() can only

 * allocate memory in power-of-two pages.

 *

 * This function is also limited by MAX_ORDER.

 *

 * Memory allocated by this function must be released by free_pages_exact().

 *

 * Return: pointer to the allocated area or %NULL in case of error.

/**

 * alloc_pages_exact_nid - allocate an exact number of physically-contiguous

 *			   pages on a node.

 * @nid: the preferred node ID where memory should be allocated

 * @size: the number of bytes to allocate

 * @gfp_mask: GFP flags for the allocation, must not contain __GFP_COMP

 *

 * Like alloc_pages_exact(), but try to allocate on node nid first before falling

 * back.

 *

 * Return: pointer to the allocated area or %NULL in case of error.

/**

 * free_pages_exact - release memory allocated via alloc_pages_exact()

 * @virt: the value returned by alloc_pages_exact.

 * @size: size of allocation, same value as passed to alloc_pages_exact().

 *

 * Release the memory allocated by a previous call to alloc_pages_exact.

/**

 * nr_free_zone_pages - count number of pages beyond high watermark

 * @offset: The zone index of the highest zone

 *

 * nr_free_zone_pages() counts the number of pages which are beyond the

 * high watermark within all zones at or below a given zone index.  For each

 * zone, the number of pages is calculated as:

 *

 *     nr_free_zone_pages = managed_pages - high_pages

 *

 * Return: number of pages beyond high watermark.

 Just pick one node, since fallback list is circular */

/**

 * nr_free_buffer_pages - count number of pages beyond high watermark

 *

 * nr_free_buffer_pages() counts the number of pages which are beyond the high

 * watermark within ZONE_DMA and ZONE_NORMAL.

 *

 * Return: number of pages beyond high watermark within ZONE_DMA and

 * ZONE_NORMAL.

	/*

	 * Estimate the amount of memory available for userspace allocations,

	 * without causing swapping.

	/*

	 * Not all the page cache can be freed, otherwise the system will

	 * start swapping. Assume at least half of the page cache, or the

	 * low watermark worth of cache, needs to stay.

	/*

	 * Part of the reclaimable slab and other kernel memory consists of

	 * items that are in use, and cannot be freed. Cap this estimate at the

	 * low watermark.

 needs to be signed */

/*

 * Determine whether the node should be displayed or not, depending on whether

 * SHOW_MEM_FILTER_NODES was passed to show_free_areas().

	/*

	 * no node mask - aka implicit memory numa policy. Do not bother with

	 * the synchronization - read_mems_allowed_begin - because we do not

	 * have to be precise here.

/*

 * Show free area list (used inside shift_scroll-lock stuff)

 * We also calculate the percentage fragmentation. We do this by counting the

 * memory on each free list with the exception of the first item on the list.

 *

 * Bits in @filter:

 * SHOW_MEM_FILTER_NODES: suppress nodes that are not allowed by current's

 *   cpuset.

/*

 * Builds allocation fallback zone lists.

 *

 * Add all populated zones of a node to the zonelist.

	/*

	 * We used to support different zonelists modes but they turned

	 * out to be just not useful. Let's keep the warning in place

	 * if somebody still use the cmd line parameter so that we do

	 * not fail it silently

/*

 * sysctl handler for numa_zonelist_order

/**

 * find_next_best_node - find the next node that should appear in a given node's fallback list

 * @node: node whose fallback list we're appending

 * @used_node_mask: nodemask_t of already used nodes

 *

 * We use a number of factors to determine which is the next node that should

 * appear on a given node's fallback list.  The node should not have appeared

 * already in @node's fallback list, and it should be the next closest node

 * according to the distance array (which contains arbitrary distance values

 * from each node to each node in the system), and should also prefer nodes

 * with no CPUs, since presumably they'll have very little allocation pressure

 * on them otherwise.

 *

 * Return: node id of the found node or %NUMA_NO_NODE if no node is found.

 Use the local node if we haven't already */

 Don't want a node to appear more than once */

 Use the distance array to find the distance */

 Penalize nodes under us ("prefer the next node") */

 Give preference to headless and unused nodes */

 Slight preference for less loaded node */

/*

 * Build zonelists ordered by node and zones within node.

 * This results in maximum locality--normal zone overflows into local

 * DMA zone, if any--but risks exhausting DMA zone.

/*

 * Build gfp_thisnode zonelists

/*

 * Build zonelists ordered by zone and nodes within zones.

 * This results in conserving DMA zone[s] until all Normal memory is

 * exhausted, but results in overflowing to remote node while memory

 * may still exist in local DMA zone.

 NUMA-aware ordering of nodes */

		/*

		 * We don't want to pressure a particular node.

		 * So adding penalty to the first node in same

		 * distance group to make it round-robin.

/*

 * Return node id of node used for "local" allocations.

 * I.e., first node id of first zone in arg node's generic zonelist.

 * Used for initializing percpu 'numa_mem', which is used primarily

 * for kernel allocations, so use GFP_KERNEL flags to locate zonelist.

 CONFIG_NUMA */

	/*

	 * Now we build the zonelist so that it contains the zones

	 * of all the other nodes.

	 * We don't want to pressure a particular node, so when

	 * building the zones for node N, we make sure that the

	 * zones coming right after the local ones are those from

	 * node N+1 (modulo N)

 CONFIG_NUMA */

/*

 * Boot pageset table. One per cpu which is going to be used for all

 * zones and all nodes. The parameters will be set in such a way

 * that an item put on a list will immediately be handed over to

 * the buddy list. This is safe since pageset manipulation is done

 * with interrupts disabled.

 *

 * The boot_pagesets must be kept even after bootup is complete for

 * unused processors and/or zones. They do play a role for bootstrapping

 * hotplugged processors.

 *

 * zoneinfo_show() and maybe other functions do

 * not check if the processor is online before following the pageset pointer.

 * Other parts of the kernel may not check if the zone is available.

 These effectively disable the pcplists in the boot pageset completely */

	/*

	 * This node is hotadded and no memory is yet present.   So just

	 * building zonelists is fine - no need to touch other nodes.

		/*

		 * We now know the "local memory node" for each node--

		 * i.e., the node of the first zone in the generic zonelist.

		 * Set up numa_mem percpu variable for on-line cpus.  During

		 * boot, only the boot cpu should be on-line;  we'll init the

		 * secondary cpus' numa_mem as they come on-line.  During

		 * node/memory hotplug, we'll fixup all on-line cpus.

	/*

	 * Initialize the boot_pagesets that are going to be used

	 * for bootstrapping processors. The real pagesets for

	 * each zone will be allocated later when the per cpu

	 * allocator is available.

	 *

	 * boot_pagesets are used also for bootstrapping offline

	 * cpus if the system is already booted because the pagesets

	 * are needed to initialize allocators on a specific cpu too.

	 * F.e. the percpu allocator needs the page allocator which

	 * needs the percpu allocator in order to allocate its pagesets

	 * (a chicken-egg dilemma).

/*

 * unless system_state == SYSTEM_BOOTING.

 *

 * __ref due to call of __init annotated helper build_all_zonelists_init

 * [protected by SYSTEM_BOOTING].

 cpuset refresh routine should be here */

 Get the number of free pages beyond high watermark in all zones. */

	/*

	 * Disable grouping by mobility if the number of pages in the

	 * system is too low to allow the mechanism to work. It would be

	 * more accurate, but expensive to check per-zone. This check is

	 * made on memory-hotadd so a system can start with mobility

	 * disabled and enable it later

 If zone is ZONE_MOVABLE but memory is mirrored, it is an overlapped init */

/*

 * Initially all pages are reserved - free ones are freed

 * up by memblock_free_all() once the early boot process is

 * done. Non-atomic initialization, single-pass.

 *

 * All aligned pageblocks are initialized to the specified migratetype

 * (usually MIGRATE_MOVABLE). Besides setting the migratetype, no related

 * zone stats (e.g., nr_isolate_pageblock) are touched.

	/*

	 * Honor reservation requested by the driver for this ZONE_DEVICE

	 * memory. We limit the total number of pages to initialize to just

	 * those that might contain the memory mapping. We will defer the

	 * ZONE_DEVICE page initialization until after we have released

	 * the hotplug lock.

		/*

		 * There can be holes in boot-time mem_map[]s handed to this

		 * function.  They do not exist on hotplugged memory.

		/*

		 * Usually, we want to mark the pageblock MIGRATE_MOVABLE,

		 * such that unmovable allocations won't be scattered all

		 * over the place during system boot.

	/*

	 * The call to memmap_init should have already taken care

	 * of the pages reserved for the memmap, so we can just jump to

	 * the end of that region and start processing the device pages.

		/*

		 * Mark page reserved as it will need to wait for onlining

		 * phase for it to be fully associated with a zone.

		 *

		 * We can use the non-atomic __set_bit operation for setting

		 * the flag as we are still initializing the pages.

		/*

		 * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer

		 * and zone_device_data.  It is a bug if a ZONE_DEVICE page is

		 * ever freed or placed on a driver-private list.

		/*

		 * Mark the block movable so that blocks are reserved for

		 * movable at startup. This will force kernel allocations

		 * to reserve their blocks rather than leaking throughout

		 * the address space during boot when many long-lived

		 * kernel allocations are made.

		 *

		 * Please note that MEMINIT_HOTPLUG path doesn't clear memmap

		 * because this is done early in section_activate()

/*

 * Only struct pages that correspond to ranges defined by memblock.memory

 * are zeroed and initialized by going through __init_single_page() during

 * memmap_init_zone_range().

 *

 * But, there could be struct pages that correspond to holes in

 * memblock.memory. This can happen because of the following reasons:

 * - physical memory bank size is not necessarily the exact multiple of the

 *   arbitrary section size

 * - early reserved memory may not be listed in memblock.memory

 * - memory layouts defined with memmap= kernel parameter may not align

 *   nicely with memmap sections

 *

 * Explicitly initialize those struct pages so that:

 * - PG_Reserved is set

 * - zone and node links point to zone and node that span the page if the

 *   hole is in the middle of a zone

 * - zone and node links point to adjacent zone/node if the hole falls on

 *   the zone boundary; the pages in such holes will be prepended to the

 *   zone/node above the hole except for the trailing pages in the last

 *   section that will be appended to the zone/node below.

	/*

	 * Initialize the memory map for hole in the range [memory_end,

	 * section_end].

	 * Append the pages in this hole to the highest zone in the last

	 * node.

	 * The call to init_unavailable_range() is outside the ifdef to

	 * silence the compiler warining about zone_id set but not used;

	 * for FLATMEM it is a nop anyway

	/*

	 * The number of pages to batch allocate is either ~0.1%

	 * of the zone or 1MB, whichever is smaller. The batch

	 * size is striking a balance between allocation latency

	 * and zone lock contention.

 We effectively *= 4 below */

	/*

	 * Clamp the batch to a 2^n - 1 value. Having a power

	 * of 2 value was found to be more likely to have

	 * suboptimal cache aliasing properties in some cases.

	 *

	 * For example if 2 tasks are alternately allocating

	 * batches of pages, one task can end up with a lot

	 * of pages of one half of the possible page colors

	 * and the other with pages of the other colors.

	/* The deferral and batching of frees should be suppressed under NOMMU

	 * conditions.

	 *

	 * The problem is that NOMMU needs to be able to allocate large chunks

	 * of contiguous memory as there's no hardware page translation to

	 * assemble apparent contiguous memory from discontiguous pages.

	 *

	 * Queueing large contiguous runs of pages for batching, however,

	 * causes the pages to actually be freed in smaller chunks.  As there

	 * can be a significant delay between the individual batches being

	 * recycled, this leads to the once large chunks of space being

	 * fragmented and becoming unavailable for high-order allocations.

		/*

		 * By default, the high value of the pcp is based on the zone

		 * low watermark so that if they are full then background

		 * reclaim will not be started prematurely.

		/*

		 * If percpu_pagelist_high_fraction is configured, the high

		 * value is based on a fraction of the managed pages in the

		 * zone.

	/*

	 * Split the high value across all online CPUs local to the zone. Note

	 * that early in boot that CPUs may not be online yet and that during

	 * CPU hotplug that the cpumask is not yet updated when a CPU is being

	 * onlined. For memory nodes that have no CPUs, split pcp->high across

	 * all online CPUs to mitigate the risk that reclaim is triggered

	 * prematurely due to pages stored on pcp lists.

	/*

	 * Ensure high is at least batch*4. The multiple is based on the

	 * historical relationship between high and batch.

/*

 * pcp->high and pcp->batch values are related and generally batch is lower

 * than high. They are also related to pcp->count such that count is lower

 * than high, and as soon as it reaches high, the pcplist is flushed.

 *

 * However, guaranteeing these relations at all times would require e.g. write

 * barriers here but also careful usage of read barriers at the read side, and

 * thus be prone to error and bad for performance. Thus the update only prevents

 * store tearing. Any new users of pcp->batch and pcp->high should ensure they

 * can cope with those fields changing asynchronously, and fully trust only the

 * pcp->count field on the local CPU with interrupts disabled.

 *

 * mutex_is_locked(&pcp_batch_high_lock) required when calling this function

 * outside of boot time (or some other assurance that no concurrent updaters

 * exist).

	/*

	 * Set batch and high values safe for a boot pageset. A true percpu

	 * pageset's initialization will update them subsequently. Here we don't

	 * need to be as careful as pageset_update() as nobody can access the

	 * pageset yet.

/*

 * Calculate and set new high and batch values for all per-cpu pagesets of a

 * zone based on the zone's size.

 Size may be 0 on !SMP && !NUMA */

/*

 * Allocate per cpu pagesets and initialize them.

 * Before this call only boot pagesets were available.

	/*

	 * Unpopulated zones continue using the boot pagesets.

	 * The numa stats for these pagesets need to be reset.

	 * Otherwise, they will end up skewing the stats of

	 * the nodes these zones are associated with.

	/*

	 * per cpu subsystem is not up at this point. The following code

	 * relies on the ability of the linker to provide the

	 * offset of a (static) per cpu variable into the per cpu area.

/**

 * get_pfn_range_for_nid - Return the start and end page frames for a node

 * @nid: The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.

 * @start_pfn: Passed by reference. On return, it will have the node start_pfn.

 * @end_pfn: Passed by reference. On return, it will have the node end_pfn.

 *

 * It returns the start and end page frame of a node based on information

 * provided by memblock_set_node(). If called for a node

 * with no available memory, a warning is printed and the start and end

 * PFNs will be 0.

/*

 * This finds a zone that can be used for ZONE_MOVABLE pages. The

 * assumption is made that zones within a node are ordered in monotonic

 * increasing memory addresses so that the "highest" populated zone is used

/*

 * The zone ranges provided by the architecture do not include ZONE_MOVABLE

 * because it is sized independent of architecture. Unlike the other zones,

 * the starting point for ZONE_MOVABLE is not fixed. It may be different

 * in each node depending on the size of each node and how evenly kernelcore

 * is distributed. This helper function adjusts the zone ranges

 * provided by the architecture for a given node by using the end of the

 * highest usable zone for ZONE_MOVABLE. This preserves the assumption that

 * zones within a node are in order of monotonic increases memory addresses

 Only adjust if ZONE_MOVABLE is on this node */

 Size ZONE_MOVABLE */

 Adjust for ZONE_MOVABLE starting within this range */

 Check if this whole range is within ZONE_MOVABLE */

/*

 * Return the number of pages a zone spans in a node, including holes

 * present_pages = zone_spanned_pages_in_node() - zone_absent_pages_in_node()

 When hotadd a new node from cpu_up(), the node should be empty */

 Get the start and end of the zone */

 Check that this node has pages within the zone's required range */

 Move the zone boundaries inside the node if necessary */

 Return the spanned pages */

/*

 * Return the number of holes in a range on a node. If nid is MAX_NUMNODES,

 * then all holes in the requested range will be accounted for.

/**

 * absent_pages_in_range - Return number of page frames in holes within a range

 * @start_pfn: The start PFN to start searching for holes

 * @end_pfn: The end PFN to stop searching for holes

 *

 * Return: the number of pages frames in memory holes within a range.

 Return the number of page frames in holes in a zone on a node */

 When hotadd a new node from cpu_up(), the node should be empty */

	/*

	 * ZONE_MOVABLE handling.

	 * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages

	 * and vice versa.

/*

 * Calculate the size of the zone->blockflags rounded to an unsigned long

 * Start by making sure zonesize is a multiple of pageblock_order by rounding

 * up. Then use 1 NR_PAGEBLOCK_BITS worth of bits per pageblock, finally

 * round what is now in bits to nearest long in bits, then return it in

 * bytes.

 CONFIG_SPARSEMEM */

 Initialise the number of pages represented by NR_PAGEBLOCK_BITS */

 Check that pageblock_nr_pages has not already been setup */

	/*

	 * Assume the largest contiguous order of interest is a huge page.

	 * This value may be variable depending on boot parameters on IA64 and

	 * powerpc.

 CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */

/*

 * When CONFIG_HUGETLB_PAGE_SIZE_VARIABLE is not set, set_pageblock_order()

 * is unused as pageblock_order is set at compile-time. See

 * include/linux/pageblock-flags.h for the values of pageblock_order based on

 * the kernel config

 CONFIG_HUGETLB_PAGE_SIZE_VARIABLE */

	/*

	 * Provide a more accurate estimation if there are holes within

	 * the zone and SPARSEMEM is in use. If there are holes within the

	 * zone, each populated memory region may cost us one or two extra

	 * memmap pages due to alignment because memmap pages for each

	 * populated regions may not be naturally aligned on page boundary.

	 * So the (present_pages >> 4) heuristic is a tradeoff for that.

/*

 * Set up the zone data structures

 * - init pgdat internals

 * - init all zones belonging to this node

 *

 * NOTE: this function is only called during memory hotplug

/*

 * Set up the zone data structures:

 *   - mark all pages reserved

 *   - mark all memory queues empty

 *   - clear the memory bitmaps

 *

 * NOTE: pgdat should get zeroed by caller.

 * NOTE: this function is only called during early init.

		/*

		 * Adjust freesize so that it accounts for how much memory

		 * is used by this zone for memmap. This affects the watermark

		 * and per-cpu initialisations

 Account for reserved pages */

 Charge for highmem memmap if there are enough kernel pages */

		/*

		 * Set an approximate value for lowmem here, it will be adjusted

		 * when the bootmem allocator frees pages into the buddy system.

		 * And all highmem pages will be managed by the buddy system.

 Skip empty nodes */

 ia64 gets its own node_mem_map, before this, without bootmem */

		/*

		 * The zone's endpoints aren't required to be MAX_ORDER

		 * aligned but the node_mem_map endpoints must be in order

		 * for the buddy allocator to function correctly.

	/*

	 * With no DISCONTIG, the global mem_map is just set as node 0's

 CONFIG_FLATMEM */

 pg_data_t should be reset to zero when it's allocated */

/*

 * Figure out the number of possible node ids.

/**

 * node_map_pfn_alignment - determine the maximum internode alignment

 *

 * This function should be called after node map is populated and sorted.

 * It calculates the maximum power of two alignment which can distinguish

 * all the nodes.

 *

 * For example, if all nodes are 1GiB and aligned to 1GiB, the return value

 * would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the

 * nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is

 * shifted, 1GiB is enough and this function will indicate so.

 *

 * This is used to test whether pfn -> nid mapping of the chosen memory

 * model has fine enough granularity to avoid incorrect mapping for the

 * populated node map.

 *

 * Return: the determined alignment in pfn's.  0 if there is no alignment

 * requirement (single node).

		/*

		 * Start with a mask granular enough to pin-point to the

		 * start pfn and tick off bits one-by-one until it becomes

		 * too coarse to separate the current node from the last.

 accumulate all internode masks */

 convert mask to number of pages */

/**

 * find_min_pfn_with_active_regions - Find the minimum PFN registered

 *

 * Return: the minimum PFN based on information provided via

 * memblock_set_node().

/*

 * early_calculate_totalpages()

 * Sum pages in active regions for movable zone.

 * Populate N_MEMORY for calculating usable_nodes.

/*

 * Find the PFN the Movable zone begins in each node. Kernel memory

 * is spread evenly between nodes as long as the nodes have enough

 * memory. When they don't, some nodes will have more kernelcore than

 * others

 save the state before borrow the nodemask */

 Need to find movable_zone earlier when movable_node is specified. */

	/*

	 * If movable_node is specified, ignore kernelcore and movablecore

	 * options.

	/*

	 * If kernelcore=mirror is specified, ignore movablecore option

	/*

	 * If kernelcore=nn% or movablecore=nn% was specified, calculate the

	 * amount of necessary memory.

	/*

	 * If movablecore= was specified, calculate what size of

	 * kernelcore that corresponds so that memory usable for

	 * any allocation type is evenly spread. If both kernelcore

	 * and movablecore are specified, then the value of kernelcore

	 * will be used for required_kernelcore if it's greater than

	 * what movablecore would have allowed.

		/*

		 * Round-up so that ZONE_MOVABLE is at least as large as what

		 * was requested by the user

	/*

	 * If kernelcore was not specified or kernelcore size is larger

	 * than totalpages, there is no ZONE_MOVABLE.

 usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */

 Spread kernelcore memory as evenly as possible throughout nodes */

		/*

		 * Recalculate kernelcore_node if the division per node

		 * now exceeds what is necessary to satisfy the requested

		 * amount of memory for the kernel

		/*

		 * As the map is walked, we track how much memory is usable

		 * by the kernel using kernelcore_remaining. When it is

		 * 0, the rest of the node is usable by ZONE_MOVABLE

 Go through each range of PFNs within this node */

 Account for what is only usable for kernelcore */

 Continue if range is now fully accounted */

					/*

					 * Push zone_movable_pfn to the end so

					 * that if we have to rebalance

					 * kernelcore across nodes, we will

					 * not double account here

			/*

			 * The usable PFN range for ZONE_MOVABLE is from

			 * start_pfn->end_pfn. Calculate size_pages as the

			 * number of pages used as kernelcore

			/*

			 * Some kernelcore has been met, update counts and

			 * break if the kernelcore for this node has been

			 * satisfied

	/*

	 * If there is still required_kernelcore, we do another pass with one

	 * less node in the count. This will push zone_movable_pfn[nid] further

	 * along on the nodes that still have memory until kernelcore is

	 * satisfied

 Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */

 restore the node_state */

 Any regular or high memory on that node ? */

/*

 * Some architectures, e.g. ARC may have ZONE_HIGHMEM below ZONE_NORMAL. For

 * such cases we allow max_zone_pfn sorted in the descending order

/**

 * free_area_init - Initialise all pg_data_t and zone data

 * @max_zone_pfn: an array of max PFNs for each zone

 *

 * This will call free_area_init_node() for each active node in the system.

 * Using the page ranges provided by memblock_set_node(), the size of each

 * zone in each node and their holes is calculated. If the maximum PFN

 * between two adjacent zones match, it is assumed that the zone is empty.

 * For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed

 * that arch_max_dma32_pfn has no pages. It is also assumed that a zone

 * starts where the previous one ended. For example, ZONE_DMA32 starts

 * at arch_max_dma_pfn.

 Record where the zone boundaries are */

 Find the PFNs that ZONE_MOVABLE begins at in each node */

 Print out the zone ranges */

 Print out the PFNs ZONE_MOVABLE begins at in each node */

	/*

	 * Print out the early node map, and initialize the

	 * subsection-map relative to active online memory ranges to

	 * enable future "sub-section" extensions of the memory map.

 Initialise every node */

 Any memory on that node */

 Value may be a percentage of total memory, otherwise bytes */

 Paranoid check for percent values greater than 100 */

 Paranoid check that UL is enough for the coremem value */

/*

 * kernelcore=size sets the amount of memory for use for allocations that

 * cannot be reclaimed or migrated.

 parse kernelcore=mirror */

/*

 * movablecore=size sets the amount of memory for use for allocations that

 * can be reclaimed or migrated.

		/*

		 * 'direct_map_addr' might be different from 'pos'

		 * because some architectures' virt_to_page()

		 * work with aliases.  Getting the direct map

		 * address ensures that we get a _writeable_

		 * alias for the memset().

		/*

		 * Perform a kasan-unchecked memset() since this memory

		 * has not been initialized.

	/*

	 * Detect special cases and adjust section sizes accordingly:

	 * 1) .init.* may be embedded into .data sections

	 * 2) .init.text.* may be out of [__init_begin, __init_end],

	 *    please refer to arch/tile/kernel/vmlinux.lds.S.

	 * 3) .rodata.* may be embedded into .text or .data sections.

/**

 * set_dma_reserve - set the specified number of pages reserved in the first zone

 * @new_dma_reserve: The number of pages to mark reserved

 *

 * The per-cpu batchsize and zone watermarks are determined by managed_pages.

 * In the DMA zone, a significant percentage may be consumed by kernel image

 * and other unfreeable allocations which can skew the watermarks badly. This

 * function may optionally be used to account for unfreeable pages in the

 * first zone (e.g., ZONE_DMA). The effect will be lower watermarks and

 * smaller per-cpu batchsize.

	/*

	 * Spill the event counters of the dead processor

	 * into the current processors event counters.

	 * This artificially elevates the count of the current

	 * processor.

	/*

	 * Zero the differential counters of the dead processor

	 * so that the vm statistics are consistent.

	 *

	 * This is only okay since the processor is dead and cannot

	 * race with what we are doing.

/*

 * calculate_totalreserve_pages - called when sysctl_lowmem_reserve_ratio

 *	or min_free_kbytes changes.

 Find valid and maximum lowmem_reserve in the zone */

 we treat the high watermark as reserved pages. */

/*

 * setup_per_zone_lowmem_reserve - called whenever

 *	sysctl_lowmem_reserve_ratio changes.  Ensures that each zone

 *	has a correct pages reserved value, so an adequate number of

 *	pages are left in the zone after a successful __alloc_pages().

 update totalreserve_pages */

 Calculate total number of !ZONE_HIGHMEM pages */

			/*

			 * __GFP_HIGH and PF_MEMALLOC allocations usually don't

			 * need highmem pages, so cap pages_min to a small

			 * value here.

			 *

			 * The WMARK_HIGH-WMARK_LOW and (WMARK_LOW-WMARK_MIN)

			 * deltas control async page reclaim, and so should

			 * not be capped for highmem.

			/*

			 * If it's a lowmem zone, reserve a number of pages

			 * proportionate to the zone's size.

		/*

		 * Set the kswapd watermarks distance according to the

		 * scale factor in proportion to available memory, but

		 * ensure a minimum size on small systems.

 update totalreserve_pages */

/**

 * setup_per_zone_wmarks - called when min_free_kbytes changes

 * or when memory is hot-{added|removed}

 *

 * Ensures that the watermark[min,low,high] values for each zone are set

 * correctly with respect to min_free_kbytes.

	/*

	 * The watermark size have changed so update the pcpu batch

	 * and high limits or the limits may be inappropriate.

/*

 * Initialise min_free_kbytes.

 *

 * For small machines we want it small (128k min).  For large machines

 * we want it large (256MB max).  But it is not linear, because network

 * bandwidth does not increase linearly with machine size.  We use

 *

 *	min_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:

 *	min_free_kbytes = sqrt(lowmem_kbytes * 16)

 *

 * which yields

 *

 * 16MB:	512k

 * 32MB:	724k

 * 64MB:	1024k

 * 128MB:	1448k

 * 256MB:	2048k

 * 512MB:	2896k

 * 1024MB:	4096k

 * 2048MB:	5792k

 * 4096MB:	8192k

 * 8192MB:	11584k

 * 16384MB:	16384k

/*

 * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so

 *	that we can call two helper functions whenever min_free_kbytes

 *	changes.

/*

 * lowmem_reserve_ratio_sysctl_handler - just a wrapper around

 *	proc_dointvec() so that we can call setup_per_zone_lowmem_reserve()

 *	whenever sysctl_lowmem_reserve_ratio changes.

 *

 * The reserve ratio obviously has absolutely no relation with the

 * minimum watermarks. The lowmem reserve ratio can only make sense

 * if in function of the boot time zone sizes.

/*

 * percpu_pagelist_high_fraction - changes the pcp->high for each zone on each

 * cpu. It is the fraction of total pages in each zone that a hot per cpu

 * pagelist can have before it gets flushed back to buddy allocator.

 Sanity checking to avoid pcp imbalance */

 No change? */

/*

 * Returns the number of pages that arch has reserved but

 * is not known to alloc_large_system_hash().

/*

 * Adaptive scale is meant to reduce sizes of hash tables on large memory

 * machines. As memory size is increased the scale is also increased but at

 * slower pace.  Starting from ADAPT_SCALE_BASE (64G), every time memory

 * quadruples the scale is increased by one, which means the size of hash table

 * only doubles, instead of quadrupling as well.

 * Because 32-bit systems cannot have large physical memory, where this scaling

 * makes sense, it is disabled on such platforms.

/*

 * allocate a large system hash table from bootmem

 * - it is assumed that the hash table must contain an exact power-of-2

 *   quantity of entries

 * - limit is the number of hash buckets, not the total allocation size

 allow the kernel cmdline to have a say */

 round applicable memory size up to nearest megabyte */

 It isn't necessary when PAGE_SIZE >= 1MB */

 limit to 1 bucket per 2^scale bytes of low memory */

 Make sure we've got at least a 0-order allocation.. */

 Makes no sense without HASH_EARLY */

 limit allocation size to 1/16 total memory by default */

			/*

			 * If bucketsize is not a power-of-two, we may free

			 * some pages at the end of hash table which

			 * alloc_pages_exact() automatically does

/*

 * This function checks whether pageblock includes unmovable pages or not.

 *

 * PageLRU check without isolation or lru_lock could race so that

 * MIGRATE_MOVABLE block might include unmovable pages. And __PageMovable

 * check without lock_page also may miss some movable non-lru pages at

 * race condition. So you can't expect this function should be exact.

 *

 * Returns a page without holding a reference. If the caller wants to

 * dereference that page (e.g., dumping), it has to make sure that it

 * cannot get removed (e.g., via memory unplug) concurrently.

 *

		/*

		 * CMA allocations (alloc_contig_range) really need to mark

		 * isolate CMA pageblocks even when they are not movable in fact

		 * so consider them movable here.

		/*

		 * Both, bootmem allocations and memory holes are marked

		 * PG_reserved and are unmovable. We can even have unmovable

		 * allocations inside ZONE_MOVABLE, for example when

		 * specifying "movablecore".

		/*

		 * If the zone is movable and we have ruled out all reserved

		 * pages then it should be reasonably safe to assume the rest

		 * is movable.

		/*

		 * Hugepages are not in LRU lists, but they're movable.

		 * THPs are on the LRU, but need to be counted as #small pages.

		 * We need not scan over tail pages because we don't

		 * handle each tail page individually in migration.

		/*

		 * We can't use page_count without pin a page

		 * because another CPU can free compound page.

		 * This check already skips compound tails of THP

		 * because their page->_refcount is zero at all time.

		/*

		 * The HWPoisoned page may be not in buddy system, and

		 * page_count() is not 0.

		/*

		 * We treat all PageOffline() pages as movable when offlining

		 * to give drivers a chance to decrement their reference count

		 * in MEM_GOING_OFFLINE in order to indicate that these pages

		 * can be offlined as there are no direct references anymore.

		 * For actually unmovable PageOffline() where the driver does

		 * not support this, we will fail later when trying to actually

		 * move these pages that still have a reference count > 0.

		 * (false negatives in this function only)

		/*

		 * If there are RECLAIMABLE pages, we need to check

		 * it.  But now, memory offline itself doesn't call

		 * shrink_node_slabs() and it still to be fixed.

 Usage: See admin-guide/dynamic-debug-howto.rst */

 [start, end) must belong to a single zone. */

 This function is based on compact_zone() from compaction.c. */

		/*

		 * On -ENOMEM, migrate_pages() bails out right away. It is pointless

		 * to retry again over this error, so do the same here.

/**

 * alloc_contig_range() -- tries to allocate given range of pages

 * @start:	start PFN to allocate

 * @end:	one-past-the-last PFN to allocate

 * @migratetype:	migratetype of the underlying pageblocks (either

 *			#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks

 *			in range must have the same migratetype and it must

 *			be either of the two.

 * @gfp_mask:	GFP mask to use during compaction

 *

 * The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES

 * aligned.  The PFN range must belong to a single zone.

 *

 * The first thing this routine does is attempt to MIGRATE_ISOLATE all

 * pageblocks in the range.  Once isolated, the pageblocks should not

 * be modified by others.

 *

 * Return: zero on success or negative error code.  On success all

 * pages which PFN is in [start, end) are allocated for the caller and

 * need to be freed with free_contig_range().

	/*

	 * What we do here is we mark all pageblocks in range as

	 * MIGRATE_ISOLATE.  Because pageblock and max order pages may

	 * have different sizes, and due to the way page allocator

	 * work, we align the range to biggest of the two pages so

	 * that page allocator won't try to merge buddies from

	 * different pageblocks and change MIGRATE_ISOLATE to some

	 * other migration type.

	 *

	 * Once the pageblocks are marked as MIGRATE_ISOLATE, we

	 * migrate the pages from an unaligned range (ie. pages that

	 * we are interested in).  This will put all the pages in

	 * range back to page allocator as MIGRATE_ISOLATE.

	 *

	 * When this is done, we take the pages in range from page

	 * allocator removing them from the buddy system.  This way

	 * page allocator will never consider using them.

	 *

	 * This lets us mark the pageblocks back as

	 * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the

	 * aligned range but not in the unaligned, original range are

	 * put back to page allocator so that buddy can use them.

	/*

	 * In case of -EBUSY, we'd like to know which page causes problem.

	 * So, just fall through. test_pages_isolated() has a tracepoint

	 * which will report the busy page.

	 *

	 * It is possible that busy pages could become available before

	 * the call to test_pages_isolated, and the range will actually be

	 * allocated.  So, if we fall through be sure to clear ret so that

	 * -EBUSY is not accidentally used or returned to caller.

	/*

	 * Pages from [start, end) are within a MAX_ORDER_NR_PAGES

	 * aligned blocks that are marked as MIGRATE_ISOLATE.  What's

	 * more, all pages in [start, end) are free in page allocator.

	 * What we are going to do is to allocate all pages from

	 * [start, end) (that is remove them from page allocator).

	 *

	 * The only problem is that pages at the beginning and at the

	 * end of interesting range may be not aligned with pages that

	 * page allocator holds, ie. they can be part of higher order

	 * pages.  Because of this, we reserve the bigger range and

	 * once this is done free the pages we are not interested in.

	 *

	 * We don't have to hold zone->lock here because the pages are

	 * isolated thus they won't get removed from buddy.

		/*

		 * outer_start page could be small order buddy page and

		 * it doesn't include start page. Adjust outer_start

		 * in this case to report failed page properly

		 * on tracepoint in test_pages_isolated()

 Make sure the range is really isolated. */

 Grab isolated pages from freelists. */

 Free head and tail (if any) */

/**

 * alloc_contig_pages() -- tries to find and allocate contiguous range of pages

 * @nr_pages:	Number of contiguous pages to allocate

 * @gfp_mask:	GFP mask to limit search and used during compaction

 * @nid:	Target node

 * @nodemask:	Mask for other possible nodes

 *

 * This routine is a wrapper around alloc_contig_range(). It scans over zones

 * on an applicable zonelist to find a contiguous pfn range which can then be

 * tried for allocation with alloc_contig_range(). This routine is intended

 * for allocation requests which can not be fulfilled with the buddy allocator.

 *

 * The allocated memory is always aligned to a page boundary. If nr_pages is a

 * power of two then the alignment is guaranteed to be to the given nr_pages

 * (e.g. 1GB request would be aligned to 1GB).

 *

 * Allocated pages can be freed with free_contig_range() or by manually calling

 * __free_page() on each allocated page.

 *

 * Return: pointer to contiguous pages on success, or NULL if not successful.

				/*

				 * We release the zone lock here because

				 * alloc_contig_range() will also lock the zone

				 * at some point. If there's an allocation

				 * spinning on this lock, it may win the race

				 * and cause alloc_contig_range() to fail...

 CONFIG_CONTIG_ALLOC */

/*

 * The zone indicated has a new number of managed_pages; batch sizes and percpu

 * page high values need to be recalculated.

/*

 * Effectively disable pcplists for the zone by setting the high limit to 0

 * and draining all cpus. A concurrent page freeing on another CPU that's about

 * to put the page on pcplist will either finish before the drain and the page

 * will be drained, or observe the new high limit and skip the pcplist.

 *

 * Must be paired with a call to zone_pcp_enable().

/*

 * All pages in the range must be in a single zone, must not contain holes,

 * must span full sections, and must be isolated before calling this function.

		/*

		 * The HWPoisoned page may be not in buddy system, and

		 * page_count() is not 0.

		/*

		 * At this point all remaining PageOffline() pages have a

		 * reference count of 0 and can simply be skipped.

/*

 * This function returns a stable result only if called under zone lock.

/*

 * Break down a higher-order page in sub-pages, and keep our target out of

 * buddy allocator.

/*

 * Take a page that will be marked as poisoned off the buddy allocator.

 SPDX-License-Identifier: GPL-2.0

/*

 * Lockless hierarchical page accounting & limiting

 *

 * Copyright (C) 2014 Red Hat, Inc., Johannes Weiner

/**

 * page_counter_cancel - take pages out of the local counter

 * @counter: counter

 * @nr_pages: number of pages to cancel

 More uncharges than charges? */

/**

 * page_counter_charge - hierarchically charge pages

 * @counter: counter

 * @nr_pages: number of pages to charge

 *

 * NOTE: This does not consider any configured counter limits.

		/*

		 * This is indeed racy, but we can live with some

		 * inaccuracy in the watermark.

/**

 * page_counter_try_charge - try to hierarchically charge pages

 * @counter: counter

 * @nr_pages: number of pages to charge

 * @fail: points first counter to hit its limit, if any

 *

 * Returns %true on success, or %false and @fail if the counter or one

 * of its ancestors has hit its configured limit.

		/*

		 * Charge speculatively to avoid an expensive CAS.  If

		 * a bigger charge fails, it might falsely lock out a

		 * racing smaller charge and send it into reclaim

		 * early, but the error is limited to the difference

		 * between the two sizes, which is less than 2M/4M in

		 * case of a THP locking out a regular page charge.

		 *

		 * The atomic_long_add_return() implies a full memory

		 * barrier between incrementing the count and reading

		 * the limit.  When racing with page_counter_set_max(),

		 * we either see the new limit or the setter sees the

		 * counter has changed and retries.

			/*

			 * This is racy, but we can live with some

			 * inaccuracy in the failcnt which is only used

			 * to report stats.

		/*

		 * Just like with failcnt, we can live with some

		 * inaccuracy in the watermark.

/**

 * page_counter_uncharge - hierarchically uncharge pages

 * @counter: counter

 * @nr_pages: number of pages to uncharge

/**

 * page_counter_set_max - set the maximum number of pages allowed

 * @counter: counter

 * @nr_pages: limit to set

 *

 * Returns 0 on success, -EBUSY if the current number of pages on the

 * counter already exceeds the specified limit.

 *

 * The caller must serialize invocations on the same counter.

		/*

		 * Update the limit while making sure that it's not

		 * below the concurrently-changing counter value.

		 *

		 * The xchg implies two full memory barriers before

		 * and after, so the read-swap-read is ordered and

		 * ensures coherency with page_counter_try_charge():

		 * that function modifies the count before checking

		 * the limit, so if it sees the old limit, we see the

		 * modified counter and retry.

/**

 * page_counter_set_min - set the amount of protected memory

 * @counter: counter

 * @nr_pages: value to set

 *

 * The caller must serialize invocations on the same counter.

/**

 * page_counter_set_low - set the amount of protected memory

 * @counter: counter

 * @nr_pages: value to set

 *

 * The caller must serialize invocations on the same counter.

/**

 * page_counter_memparse - memparse() for page counter limits

 * @buf: string to parse

 * @max: string meaning maximum possible value

 * @nr_pages: returns the result in number of pages

 *

 * Returns -EINVAL, or 0 and @nr_pages on success.  @nr_pages will be

 * limited to %PAGE_COUNTER_MAX.

 SPDX-License-Identifier: GPL-2.0

/*

 *	linux/mm/mincore.c

 *

 * Copyright (C) 1994-2006  Linus Torvalds

/*

 * The mincore() system call.

	/*

	 * Hugepages under user process are always in RAM and never

	 * swapped out, but theoretically it needs to be checked.

/*

 * Later we can get more picky about what "in core" means precisely.

 * For now, simply check to see if the page is in the page cache,

 * and is up to date; i.e. that no page-in operation would be required

 * at this time if an application were to map and access this page.

	/*

	 * When tmpfs swaps out a page from a file, any process mapping that

	 * file will not get a swp_entry_t in its pte, but rather it is like

	 * any other file mapping (ie. marked !present and faulted in with

	 * tmpfs's .fault). So swapped out tmpfs mappings are tested here.

 pte is a swap entry */

				/*

				 * migration or hwpoison entries are always

				 * uptodate

	/*

	 * Reveal pagecache information only for non-anonymous mappings that

	 * correspond to the files the calling process could (if tried) open

	 * for writing; otherwise we'd be including shared non-exclusive

	 * mappings, which opens a side channel.

/*

 * Do a chunk of "sys_mincore()". We've already checked

 * all the arguments, we hold the mmap semaphore: we should

 * just return the amount of info we're asked for.

/*

 * The mincore(2) system call.

 *

 * mincore() returns the memory residency status of the pages in the

 * current process's address space specified by [addr, addr + len).

 * The status is returned in a vector of bytes.  The least significant

 * bit of each byte is 1 if the referenced page is in memory, otherwise

 * it is zero.

 *

 * Because the status of a page can change after mincore() checks it

 * but before it returns to the application, the returned vector may

 * contain stale information.  Only locked pages are guaranteed to

 * remain in memory.

 *

 * return values:

 *  zero    - success

 *  -EFAULT - vec points to an illegal address

 *  -EINVAL - addr is not a multiple of PAGE_SIZE

 *  -ENOMEM - Addresses in the range [addr, addr + len] are

 *		invalid for the address space of this process, or

 *		specify one or more pages which are not currently

 *		mapped

 *  -EAGAIN - A kernel resource was temporarily unavailable.

 Check the start address: needs to be page-aligned.. */

 ..and we need to be passed a valid user-space range */

 This also avoids any overflows on PAGE_ALIGN */

		/*

		 * Do at most PAGE_SIZE entries per iteration, due to

		 * the temporary buffer size.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * zpool memory storage api

 *

 * Copyright (C) 2014 Dan Streetman

 *

 * This is a common frontend for memory storage pool implementations.

 * Typically, this is used to store compressed memory.

/**

 * zpool_register_driver() - register a zpool implementation.

 * @driver:	driver to register

/**

 * zpool_unregister_driver() - unregister a zpool implementation.

 * @driver:	driver to unregister.

 *

 * Module usage counting is used to prevent using a driver

 * while/after unloading, so if this is called from module

 * exit function, this should never fail; if called from

 * other than the module exit function, and this returns

 * failure, the driver is in use and must remain available.

 this assumes @type is null-terminated. */

/**

 * zpool_has_pool() - Check if the pool driver is available

 * @type:	The type of the zpool to check (e.g. zbud, zsmalloc)

 *

 * This checks if the @type pool driver is available.  This will try to load

 * the requested module, if needed, but there is no guarantee the module will

 * still be loaded and available immediately after calling.  If this returns

 * true, the caller should assume the pool is available, but must be prepared

 * to handle the @zpool_create_pool() returning failure.  However if this

 * returns false, the caller should assume the requested pool type is not

 * available; either the requested pool type module does not exist, or could

 * not be loaded, and calling @zpool_create_pool() with the pool type will

 * fail.

 *

 * The @type string must be null-terminated.

 *

 * Returns: true if @type pool is available, false if not

/**

 * zpool_create_pool() - Create a new zpool

 * @type:	The type of the zpool to create (e.g. zbud, zsmalloc)

 * @name:	The name of the zpool (e.g. zram0, zswap)

 * @gfp:	The GFP flags to use when allocating the pool.

 * @ops:	The optional ops callback.

 *

 * This creates a new zpool of the specified type.  The gfp flags will be

 * used when allocating memory, if the implementation supports it.  If the

 * ops param is NULL, then the created zpool will not be evictable.

 *

 * Implementations must guarantee this to be thread-safe.

 *

 * The @type and @name strings must be null-terminated.

 *

 * Returns: New zpool on success, NULL on failure.

/**

 * zpool_destroy_pool() - Destroy a zpool

 * @zpool:	The zpool to destroy.

 *

 * Implementations must guarantee this to be thread-safe,

 * however only when destroying different pools.  The same

 * pool should only be destroyed once, and should not be used

 * after it is destroyed.

 *

 * This destroys an existing zpool.  The zpool should not be in use.

/**

 * zpool_get_type() - Get the type of the zpool

 * @zpool:	The zpool to check

 *

 * This returns the type of the pool.

 *

 * Implementations must guarantee this to be thread-safe.

 *

 * Returns: The type of zpool.

/**

 * zpool_malloc_support_movable() - Check if the zpool supports

 *	allocating movable memory

 * @zpool:	The zpool to check

 *

 * This returns if the zpool supports allocating movable memory.

 *

 * Implementations must guarantee this to be thread-safe.

 *

 * Returns: true if the zpool supports allocating movable memory, false if not

/**

 * zpool_malloc() - Allocate memory

 * @zpool:	The zpool to allocate from.

 * @size:	The amount of memory to allocate.

 * @gfp:	The GFP flags to use when allocating memory.

 * @handle:	Pointer to the handle to set

 *

 * This allocates the requested amount of memory from the pool.

 * The gfp flags will be used when allocating memory, if the

 * implementation supports it.  The provided @handle will be

 * set to the allocated object handle.

 *

 * Implementations must guarantee this to be thread-safe.

 *

 * Returns: 0 on success, negative value on error.

/**

 * zpool_free() - Free previously allocated memory

 * @zpool:	The zpool that allocated the memory.

 * @handle:	The handle to the memory to free.

 *

 * This frees previously allocated memory.  This does not guarantee

 * that the pool will actually free memory, only that the memory

 * in the pool will become available for use by the pool.

 *

 * Implementations must guarantee this to be thread-safe,

 * however only when freeing different handles.  The same

 * handle should only be freed once, and should not be used

 * after freeing.

/**

 * zpool_shrink() - Shrink the pool size

 * @zpool:	The zpool to shrink.

 * @pages:	The number of pages to shrink the pool.

 * @reclaimed:	The number of pages successfully evicted.

 *

 * This attempts to shrink the actual memory size of the pool

 * by evicting currently used handle(s).  If the pool was

 * created with no zpool_ops, or the evict call fails for any

 * of the handles, this will fail.  If non-NULL, the @reclaimed

 * parameter will be set to the number of pages reclaimed,

 * which may be more than the number of pages requested.

 *

 * Implementations must guarantee this to be thread-safe.

 *

 * Returns: 0 on success, negative value on error/failure.

/**

 * zpool_map_handle() - Map a previously allocated handle into memory

 * @zpool:	The zpool that the handle was allocated from

 * @handle:	The handle to map

 * @mapmode:	How the memory should be mapped

 *

 * This maps a previously allocated handle into memory.  The @mapmode

 * param indicates to the implementation how the memory will be

 * used, i.e. read-only, write-only, read-write.  If the

 * implementation does not support it, the memory will be treated

 * as read-write.

 *

 * This may hold locks, disable interrupts, and/or preemption,

 * and the zpool_unmap_handle() must be called to undo those

 * actions.  The code that uses the mapped handle should complete

 * its operations on the mapped handle memory quickly and unmap

 * as soon as possible.  As the implementation may use per-cpu

 * data, multiple handles should not be mapped concurrently on

 * any cpu.

 *

 * Returns: A pointer to the handle's mapped memory area.

/**

 * zpool_unmap_handle() - Unmap a previously mapped handle

 * @zpool:	The zpool that the handle was allocated from

 * @handle:	The handle to unmap

 *

 * This unmaps a previously mapped handle.  Any locks or other

 * actions that the implementation took in zpool_map_handle()

 * will be undone here.  The memory area returned from

 * zpool_map_handle() should no longer be used after this.

/**

 * zpool_get_total_size() - The total size of the pool

 * @zpool:	The zpool to check

 *

 * This returns the total size in bytes of the pool.

 *

 * Returns: Total size of the zpool in bytes.

/**

 * zpool_evictable() - Test if zpool is potentially evictable

 * @zpool:	The zpool to test

 *

 * Zpool is only potentially evictable when it's created with struct

 * zpool_ops.evict and its driver implements struct zpool_driver.shrink.

 *

 * However, it doesn't necessarily mean driver will use zpool_ops.evict

 * in its implementation of zpool_driver.shrink. It could do internal

 * defragmentation instead.

 *

 * Returns: true if potentially evictable; false otherwise.

/**

 * zpool_can_sleep_mapped - Test if zpool can sleep when do mapped.

 * @zpool:	The zpool to test

 *

 * Returns: true if zpool can sleep; false otherwise.

 SPDX-License-Identifier: GPL-2.0

/*

 *  mm/pgtable-generic.c

 *

 *  Generic pgtable methods declared in linux/pgtable.h

 *

 *  Copyright (C) 2010  Linus Torvalds

/*

 * If a p?d_bad entry is found while walking page tables, report

 * the error, before resetting entry to p?d_none.  Usually (but

 * very seldom) called out from the p?d_none_or_clear_bad macros.

/*

 * Note that the pmd variant below can't be stub'ed out just as for p4d/pud

 * above. pmd folding is special and typically pmd_* macros refer to upper

 * level even when folded

/*

 * Only sets the access flags (dirty, accessed), as well as write

 * permission. Furthermore, we know it always gets set to a "more

 * permissive" setting, which allows most architectures to optimize

 * this. We return whether the PTE actually changed, which in turn

 * instructs the caller to do things like update__mmu_cache.  This

 * used to be done in the caller, but sparc needs minor faults to

 * force that call on sun4c so we changed this macro slightly

 FIFO */

 no "address" argument so destroys page coloring of some arch */

 FIFO */

	/*

	 * pmd and hugepage pte format are same. So we could

	 * use the same function.

 collapse entails shooting down ptes not pmd */

 CONFIG_TRANSPARENT_HUGEPAGE */

/*

 * Compatibility functions which bloat the callers too much to make inline.

 * All of the callers of these functions should be converted to use folios

 * eventually.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  mm/userfaultfd.c

 *

 *  Copyright (C) 2015  Red Hat, Inc.

	/*

	 * Make sure that the dst range is both valid and fully within a

	 * single existing vma.

	/*

	 * Check the vma is registered in uffd, this is required to

	 * enforce the VM_MAYWRITE check done at uffd registration

	 * time.

/*

 * Install PTEs, to map dst_addr (within dst_vma) to page.

 *

 * This function handles both MCOPY_ATOMIC_NORMAL and _CONTINUE for both shmem

 * and anon, and for both shared and private VMAs.

 serialize against truncate with the page table lock */

	/*

	 * Must happen after rmap, as mm_counter() checks mapping (via

	 * PageAnon()), which is set by __page_set_anon_rmap().

 No need to invalidate - it was non-present before */

 fallback to copy_from_user outside mmap_lock */

 don't free the page */

	/*

	 * The memory barrier inside __SetPageUptodate makes sure that

	 * preceding stores to the page contents become visible before

	 * the set_pte_at() write.

 the shmem MAP_PRIVATE case requires checking the i_size */

 No need to invalidate - it was non-present before */

 Handles UFFDIO_CONTINUE for all shmem VMAs (shared or private). */

	/*

	 * Note that we didn't run this because the pmd was

	 * missing, the *pmd may be already established and in

	 * turn it may also be a trans_huge_pmd.

/*

 * __mcopy_atomic processing for HUGETLB vmas.  Note that this routine is

 * called with mmap_lock held, it will release mmap_lock before returning.

	/*

	 * There is no default zero huge page for all huge page sizes as

	 * supported by hugetlb.  A PMD_SIZE huge pages may exist as used

	 * by THP.  Since we can not reliably insert a zero page, this

	 * feature is not supported.

	/*

	 * Validate alignment based on huge page size

	/*

	 * On routine entry dst_vma is set.  If we had to drop mmap_lock and

	 * retry, dst_vma will be set to NULL and we must lookup again.

	/*

	 * If not shared, ensure the dst_vma has a anon_vma.

		/*

		 * Serialize via i_mmap_rwsem and hugetlb_fault_mutex.

		 * i_mmap_rwsem ensures the dst_pte remains valid even

		 * in the case of shared pmds.  fault mutex prevents

		 * races with other faulting threads.

 !CONFIG_HUGETLB_PAGE */

 fail at build time if gcc attempts to use this */

 CONFIG_HUGETLB_PAGE */

	/*

	 * The normal page fault path for a shmem will invoke the

	 * fault, fill the hole in the file and COW it right away. The

	 * result generates plain anonymous memory. So when we are

	 * asked to fill an hole in a MAP_PRIVATE shmem mapping, we'll

	 * generate anonymous memory directly without actually filling

	 * the hole. For the MAP_PRIVATE case the robustness check

	 * only happens in the pagetable (to verify it's still none)

	 * and not in the radix tree.

	/*

	 * Sanitize the command parameters:

 Does the address range wrap, or is the span zero-sized? */

	/*

	 * If memory mappings are changing because of non-cooperative

	 * operation (e.g. mremap) running in parallel, bail out and

	 * request the user to retry later

	/*

	 * Make sure the vma is not shared, that the dst range is

	 * both valid and fully within a single existing vma.

	/*

	 * shmem_zero_setup is invoked in mmap for MAP_ANONYMOUS|MAP_SHARED but

	 * it will overwrite vm_ops, so vma_is_anonymous must return false.

	/*

	 * validate 'mode' now that we know the dst_vma: don't allow

	 * a wrprotect copy if the userfaultfd didn't register as WP.

	/*

	 * If this is a HUGETLB vma, pass off to appropriate routine

	/*

	 * Ensure the dst_vma has a anon_vma or this page

	 * would get a NULL anon_vma when moved in the

	 * dst_vma.

		/*

		 * If the dst_pmd is mapped as THP don't

		 * override it and just be strict.

 If an huge pmd materialized from under us fail */

	/*

	 * Sanitize the command parameters:

 Does the address range wrap, or is the span zero-sized? */

	/*

	 * If memory mappings are changing because of non-cooperative

	 * operation (e.g. mremap) running in parallel, bail out and

	 * request the user to retry later

	/*

	 * Make sure the vma is not shared, that the dst range is

	 * both valid and fully within a single existing vma.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/percpu.c - percpu memory allocator

 *

 * Copyright (C) 2009		SUSE Linux Products GmbH

 * Copyright (C) 2009		Tejun Heo <tj@kernel.org>

 *

 * Copyright (C) 2017		Facebook Inc.

 * Copyright (C) 2017		Dennis Zhou <dennis@kernel.org>

 *

 * The percpu allocator handles both static and dynamic areas.  Percpu

 * areas are allocated in chunks which are divided into units.  There is

 * a 1-to-1 mapping for units to possible cpus.  These units are grouped

 * based on NUMA properties of the machine.

 *

 *  c0                           c1                         c2

 *  -------------------          -------------------        ------------

 * | u0 | u1 | u2 | u3 |        | u0 | u1 | u2 | u3 |      | u0 | u1 | u

 *  -------------------  ......  -------------------  ....  ------------

 *

 * Allocation is done by offsets into a unit's address space.  Ie., an

 * area of 512 bytes at 6k in c1 occupies 512 bytes at 6k in c1:u0,

 * c1:u1, c1:u2, etc.  On NUMA machines, the mapping may be non-linear

 * and even sparse.  Access is handled by configuring percpu base

 * registers according to the cpu to unit mappings and offsetting the

 * base address using pcpu_unit_size.

 *

 * There is special consideration for the first chunk which must handle

 * the static percpu variables in the kernel image as allocation services

 * are not online yet.  In short, the first chunk is structured like so:

 *

 *                  <Static | [Reserved] | Dynamic>

 *

 * The static data is copied from the original section managed by the

 * linker.  The reserved section, if non-zero, primarily manages static

 * percpu variables from kernel modules.  Finally, the dynamic section

 * takes care of normal allocations.

 *

 * The allocator organizes chunks into lists according to free size and

 * memcg-awareness.  To make a percpu allocation memcg-aware the __GFP_ACCOUNT

 * flag should be passed.  All memcg-aware allocations are sharing one set

 * of chunks and all unaccounted allocations and allocations performed

 * by processes belonging to the root memory cgroup are using the second set.

 *

 * The allocator tries to allocate from the fullest chunk first. Each chunk

 * is managed by a bitmap with metadata blocks.  The allocation map is updated

 * on every allocation and free to reflect the current state while the boundary

 * map is only updated on allocation.  Each metadata block contains

 * information to help mitigate the need to iterate over large portions

 * of the bitmap.  The reverse mapping from page to chunk is stored in

 * the page's index.  Lastly, units are lazily backed and grow in unison.

 *

 * There is a unique conversion that goes on here between bytes and bits.

 * Each bit represents a fragment of size PCPU_MIN_ALLOC_SIZE.  The chunk

 * tracks the number of pages it is responsible for in nr_pages.  Helper

 * functions are used to convert from between the bytes, bits, and blocks.

 * All hints are managed in bits unless explicitly stated.

 *

 * To use this allocator, arch code should do the following:

 *

 * - define __addr_to_pcpu_ptr() and __pcpu_ptr_to_addr() to translate

 *   regular address to percpu pointer and back if they need to be

 *   different from the default

 *

 * - use pcpu_setup_first_chunk() during percpu area initialization to

 *   setup the first chunk containing the kernel static percpu area

/*

 * The slots are sorted by the size of the biggest continuous free area.

 * 1-31 bytes share the same slot.

 chunks in slots below this are subject to being sidelined on failed alloc */

 default addr <-> pcpu_ptr mapping, override in asm/percpu.h if necessary */

 CONFIG_SMP */

 on UP, it's always identity mapped */

 CONFIG_SMP */

 cpus with the lowest and highest unit addresses */

 the address of the first chunk which starts with the kernel static area */

 cpu -> unit */

 cpu -> unit offset */

 group information, used for vm allocation */

/*

 * The first chunk which always exists.  Note that unlike other

 * chunks, this one can be allocated and mapped in several different

 * ways and thus often doesn't live in the vmalloc area.

/*

 * Optional reserved chunk.  This chunk reserves part of the first

 * chunk and serves it for reserved allocations.  When the reserved

 * region doesn't exist, the following variable is NULL.

 all internal data structures */

 chunk create/destroy, [de]pop, map ext */

 chunk list slots */

 chunks which need their map areas extended, protected by pcpu_lock */

/*

 * The number of empty populated pages, protected by pcpu_lock.

 * The reserved chunk doesn't contribute to the count.

/*

 * The number of populated pages in use by the allocator, protected by

 * pcpu_lock.  This number is kept per a unit per chunk (i.e. when a page gets

 * allocated/deallocated, it is allocated/deallocated in all units of a chunk

 * and increments/decrements this count by 1).

/*

 * Balance work is used to populate or destroy chunks asynchronously.  We

 * try to keep the number of populated free pages between

 * PCPU_EMPTY_POP_PAGES_LOW and HIGH for atomic allocations and at most one

 * empty chunk.

/**

 * pcpu_addr_in_chunk - check if the address is served from this chunk

 * @chunk: chunk of interest

 * @addr: percpu address

 *

 * RETURNS:

 * True if the address is served from this chunk.

 size is in bytes */

 set the pointer to a chunk in a page struct */

 obtain pointer to a chunk from a page struct */

/*

 * The following are helper functions to help access bitmaps and convert

 * between bitmap offsets to address offsets.

/**

 * pcpu_check_block_hint - check against the contig hint

 * @block: block of interest

 * @bits: size of allocation

 * @align: alignment of area (max PAGE_SIZE)

 *

 * Check to see if the allocation can fit in the block's contig hint.

 * Note, a chunk uses the same hints as a block so this can also check against

 * the chunk's contig hint.

/*

 * pcpu_next_hint - determine which hint to use

 * @block: block of interest

 * @alloc_bits: size of allocation

 *

 * This determines if we should scan based on the scan_hint or first_free.

 * In general, we want to scan from first_free to fulfill allocations by

 * first fit.  However, if we know a scan_hint at position scan_hint_start

 * cannot fulfill an allocation, we can begin scanning from there knowing

 * the contig_hint will be our fallback.

	/*

	 * The three conditions below determine if we can skip past the

	 * scan_hint.  First, does the scan hint exist.  Second, is the

	 * contig_hint after the scan_hint (possibly not true iff

	 * contig_hint == scan_hint).  Third, is the allocation request

	 * larger than the scan_hint.

/**

 * pcpu_next_md_free_region - finds the next hint free area

 * @chunk: chunk of interest

 * @bit_off: chunk offset

 * @bits: size of free area

 *

 * Helper function for pcpu_for_each_md_free_region.  It checks

 * block->contig_hint and performs aggregation across blocks to find the

 * next hint.  It modifies bit_off and bits in-place to be consumed in the

 * loop.

 handles contig area across blocks */

		/*

		 * This checks three things.  First is there a contig_hint to

		 * check.  Second, have we checked this hint before by

		 * comparing the block_off.  Third, is this the same as the

		 * right contig hint.  In the last case, it spills over into

		 * the next block and should be handled by the contig area

		 * across blocks code.

 reset to satisfy the second predicate above */

/**

 * pcpu_next_fit_region - finds fit areas for a given allocation request

 * @chunk: chunk of interest

 * @alloc_bits: size of allocation

 * @align: alignment of area (max PAGE_SIZE)

 * @bit_off: chunk offset

 * @bits: size of free area

 *

 * Finds the next free region that is viable for use with a given size and

 * alignment.  This only returns if there is a valid area to be used for this

 * allocation.  block->first_free is returned if the allocation request fits

 * within the block to see if the request can be fulfilled prior to the contig

 * hint.

 handles contig area across blocks */

 check block->contig_hint */

		/*

		 * This uses the block offset to determine if this has been

		 * checked in the prior iteration.

 reset to satisfy the second predicate above */

 no valid offsets were found - fail condition */

/*

 * Metadata free area iterators.  These perform aggregation of free areas

 * based on the metadata blocks and return the offset @bit_off and size in

 * bits of the free area @bits.  pcpu_for_each_fit_region only returns when

 * a fit is found for the allocation request.

/**

 * pcpu_mem_zalloc - allocate memory

 * @size: bytes to allocate

 * @gfp: allocation flags

 *

 * Allocate @size bytes.  If @size is smaller than PAGE_SIZE,

 * kzalloc() is used; otherwise, the equivalent of vzalloc() is used.

 * This is to facilitate passing through whitelisted flags.  The

 * returned memory is always zeroed.

 *

 * RETURNS:

 * Pointer to the allocated area on success, NULL on failure.

/**

 * pcpu_mem_free - free memory

 * @ptr: memory to free

 *

 * Free @ptr.  @ptr should have been allocated using pcpu_mem_zalloc().

/**

 * pcpu_chunk_relocate - put chunk in the appropriate chunk slot

 * @chunk: chunk of interest

 * @oslot: the previous slot it was on

 *

 * This function is called after an allocation or free changed @chunk.

 * New slot according to the changed state is determined and @chunk is

 * moved to the slot.  Note that the reserved chunk is never put on

 * chunk slots.

 *

 * CONTEXT:

 * pcpu_lock.

 leave isolated chunks in-place */

/*

 * pcpu_update_empty_pages - update empty page counters

 * @chunk: chunk of interest

 * @nr: nr of empty pages

 *

 * This is used to keep track of the empty pages now based on the premise

 * a md_block covers a page.  The hint update functions recognize if a block

 * is made full or broken to calculate deltas for keeping track of free pages.

/*

 * pcpu_region_overlap - determines if two regions overlap

 * @a: start of first region, inclusive

 * @b: end of first region, exclusive

 * @x: start of second region, inclusive

 * @y: end of second region, exclusive

 *

 * This is used to determine if the hint region [a, b) overlaps with the

 * allocated region [x, y).

/**

 * pcpu_block_update - updates a block given a free area

 * @block: block of interest

 * @start: start offset in block

 * @end: end offset in block

 *

 * Updates a block given a known free area.  The region [start, end) is

 * expected to be the entirety of the free area within a block.  Chooses

 * the best starting offset if the contig hints are equal.

 promote the old contig_hint to be the new scan_hint */

				/*

				 * The old contig_hint == scan_hint.  But, the

				 * new contig is larger so hold the invariant

				 * scan_hint_start < contig_hint_start.

 start has a better alignment so use it */

			/*

			 * Knowing contig == contig_hint, update the scan_hint

			 * if it is farther than or larger than the current

			 * scan_hint.

		/*

		 * The region is smaller than the contig_hint.  So only update

		 * the scan_hint if it is larger than or equal and farther than

		 * the current scan_hint.

/*

 * pcpu_block_update_scan - update a block given a free area from a scan

 * @chunk: chunk of interest

 * @bit_off: chunk offset

 * @bits: size of free area

 *

 * Finding the final allocation spot first goes through pcpu_find_block_fit()

 * to find a block that can hold the allocation and then pcpu_alloc_area()

 * where a scan is used.  When allocations require specific alignments,

 * we can inadvertently create holes which will not be seen in the alloc

 * or free paths.

 *

 * This takes a given free area hole and updates a block as it may change the

 * scan_hint.  We need to scan backwards to ensure we don't miss free bits

 * from alignment.

 scan backwards in case of alignment skipping free bits */

/**

 * pcpu_chunk_refresh_hint - updates metadata about a chunk

 * @chunk: chunk of interest

 * @full_scan: if we should scan from the beginning

 *

 * Iterates over the metadata blocks to find the largest contig area.

 * A full scan can be avoided on the allocation path as this is triggered

 * if we broke the contig_hint.  In doing so, the scan_hint will be before

 * the contig_hint or after if the scan_hint == contig_hint.  This cannot

 * be prevented on freeing as we want to find the largest area possibly

 * spanning blocks.

 promote scan_hint to contig_hint */

/**

 * pcpu_block_refresh_hint

 * @chunk: chunk of interest

 * @index: index of the metadata block

 *

 * Scans over the block beginning at first_free and updates the block

 * metadata accordingly.

 region start, region end */

 promote scan_hint to contig_hint */

 iterate over free areas and update the contig hints */

/**

 * pcpu_block_update_hint_alloc - update hint on allocation path

 * @chunk: chunk of interest

 * @bit_off: chunk offset

 * @bits: size of request

 *

 * Updates metadata for the allocation path.  The metadata only has to be

 * refreshed by a full scan iff the chunk's contig hint is broken.  Block level

 * scans are required if the block's contig hint is broken.

 block indexes of the freed allocation */

 block offsets of the freed allocation */

	/*

	 * Calculate per block offsets.

	 * The calculation uses an inclusive range, but the resulting offsets

	 * are [start, end).  e_index always points to the last block in the

	 * range.

	/*

	 * Update s_block.

	 * block->first_free must be updated if the allocation takes its place.

	 * If the allocation breaks the contig_hint, a scan is required to

	 * restore this hint.

 block contig hint is broken - scan to fix it */

 update left and right contig manually */

	/*

	 * Update e_block.

		/*

		 * When the allocation is across blocks, the end is along

		 * the left part of the e_block.

 reset the block */

 contig hint is broken - scan to fix it */

 update in-between md_blocks */

	/*

	 * The only time a full chunk scan is required is if the chunk

	 * contig hint is broken.  Otherwise, it means a smaller space

	 * was used and therefore the chunk contig hint is still correct.

/**

 * pcpu_block_update_hint_free - updates the block hints on the free path

 * @chunk: chunk of interest

 * @bit_off: chunk offset

 * @bits: size of request

 *

 * Updates metadata for the allocation path.  This avoids a blind block

 * refresh by making use of the block contig hints.  If this fails, it scans

 * forward and backward to determine the extent of the free area.  This is

 * capped at the boundary of blocks.

 *

 * A chunk update is triggered if a page becomes free, a block becomes free,

 * or the free spans across blocks.  This tradeoff is to minimize iterating

 * over the block metadata to update chunk_md->contig_hint.

 * chunk_md->contig_hint may be off by up to a page, but it will never be more

 * than the available space.  If the contig hint is contained in one block, it

 * will be accurate.

 block indexes of the freed allocation */

 block offsets of the freed allocation */

 start and end of the whole free area */

	/*

	 * Calculate per block offsets.

	 * The calculation uses an inclusive range, but the resulting offsets

	 * are [start, end).  e_index always points to the last block in the

	 * range.

	/*

	 * Check if the freed area aligns with the block->contig_hint.

	 * If it does, then the scan to find the beginning/end of the

	 * larger free area can be avoided.

	 *

	 * start and end refer to beginning and end of the free area

	 * within each their respective blocks.  This is not necessarily

	 * the entire free area as it may span blocks past the beginning

	 * or end of the block.

		/*

		 * Scan backwards to find the extent of the free area.

		 * find_last_bit returns the starting bit, so if the start bit

		 * is returned, that means there was no last bit and the

		 * remainder of the chunk is free.

 update s_block */

 freeing in the same block */

 update e_block */

 reset md_blocks in the middle */

	/*

	 * Refresh chunk metadata when the free makes a block free or spans

	 * across blocks.  The contig_hint may be off by up to a page, but if

	 * the contig_hint is contained in a block, it will be accurate with

	 * the else condition below.

/**

 * pcpu_is_populated - determines if the region is populated

 * @chunk: chunk of interest

 * @bit_off: chunk offset

 * @bits: size of area

 * @next_off: return value for the next offset to start searching

 *

 * For atomic allocations, check if the backing pages are populated.

 *

 * RETURNS:

 * Bool if the backing pages are populated.

 * next_index is to skip over unpopulated blocks in pcpu_find_block_fit.

/**

 * pcpu_find_block_fit - finds the block index to start searching

 * @chunk: chunk of interest

 * @alloc_bits: size of request in allocation units

 * @align: alignment of area (max PAGE_SIZE bytes)

 * @pop_only: use populated regions only

 *

 * Given a chunk and an allocation spec, find the offset to begin searching

 * for a free region.  This iterates over the bitmap metadata blocks to

 * find an offset that will be guaranteed to fit the requirements.  It is

 * not quite first fit as if the allocation does not fit in the contig hint

 * of a block or chunk, it is skipped.  This errs on the side of caution

 * to prevent excess iteration.  Poor alignment can cause the allocator to

 * skip over blocks and chunks that have valid free areas.

 *

 * RETURNS:

 * The offset in the bitmap to begin searching.

 * -1 if no offset is found.

	/*

	 * This is an optimization to prevent scanning by assuming if the

	 * allocation cannot fit in the global hint, there is memory pressure

	 * and creating a new chunk would happen soon.

/*

 * pcpu_find_zero_area - modified from bitmap_find_next_zero_area_off()

 * @map: the address to base the search on

 * @size: the bitmap size in bits

 * @start: the bitnumber to start searching at

 * @nr: the number of zeroed bits we're looking for

 * @align_mask: alignment mask for zero area

 * @largest_off: offset of the largest area skipped

 * @largest_bits: size of the largest area skipped

 *

 * The @align_mask should be one less than a power of 2.

 *

 * This is a modified version of bitmap_find_next_zero_area_off() to remember

 * the largest area that was skipped.  This is imperfect, but in general is

 * good enough.  The largest remembered region is the largest failed region

 * seen.  This does not include anything we possibly skipped due to alignment.

 * pcpu_block_update_scan() does scan backwards to try and recover what was

 * lost to alignment.  While this can cause scanning to miss earlier possible

 * free areas, smaller allocations will eventually fill those holes.

 Align allocation */

 remember largest unused area with best alignment */

/**

 * pcpu_alloc_area - allocates an area from a pcpu_chunk

 * @chunk: chunk of interest

 * @alloc_bits: size of request in allocation units

 * @align: alignment of area (max PAGE_SIZE)

 * @start: bit_off to start searching

 *

 * This function takes in a @start offset to begin searching to fit an

 * allocation of @alloc_bits with alignment @align.  It needs to scan

 * the allocation map because if it fits within the block's contig hint,

 * @start will be block->first_free. This is an attempt to fill the

 * allocation prior to breaking the contig hint.  The allocation and

 * boundary maps are updated accordingly if it confirms a valid

 * free area.

 *

 * RETURNS:

 * Allocated addr offset in @chunk on success.

 * -1 if no matching area is found.

	/*

	 * Search to find a fit.

 update alloc map */

 update boundary map */

 update first free bit */

/**

 * pcpu_free_area - frees the corresponding offset

 * @chunk: chunk of interest

 * @off: addr offset into chunk

 *

 * This function determines the size of an allocation to free using

 * the boundary bitmap and clears the allocation map.

 *

 * RETURNS:

 * Number of freed bytes.

 find end index */

 update metadata */

 update first free bit */

 init the chunk's block */

/**

 * pcpu_alloc_first_chunk - creates chunks that serve the first chunk

 * @tmp_addr: the start of the region served

 * @map_size: size of the region served

 *

 * This is responsible for creating the chunks that serve the first chunk.  The

 * base_addr is page aligned down of @tmp_addr while the region end is page

 * aligned up.  Offsets are kept track of to determine the region served. All

 * this is done to appease the bitmap allocator in avoiding partial blocks.

 *

 * RETURNS:

 * Chunk serving the region at @tmp_addr of @map_size.

 region calculations */

	/*

	 * Align the end of the region with the LCM of PAGE_SIZE and

	 * PCPU_BITMAP_BLOCK_SIZE.  One of these constants is a multiple of

	 * the other.

 allocate chunk */

 first chunk is free to use */

 manage populated page bitmap */

 hide the beginning of the bitmap */

 hide the end of the bitmap */

 init metadata */

/**

 * pcpu_chunk_populated - post-population bookkeeping

 * @chunk: pcpu_chunk which got populated

 * @page_start: the start page

 * @page_end: the end page

 *

 * Pages in [@page_start,@page_end) have been populated to @chunk.  Update

 * the bookkeeping information accordingly.  Must be called after each

 * successful population.

/**

 * pcpu_chunk_depopulated - post-depopulation bookkeeping

 * @chunk: pcpu_chunk which got depopulated

 * @page_start: the start page

 * @page_end: the end page

 *

 * Pages in [@page_start,@page_end) have been depopulated from @chunk.

 * Update the bookkeeping information accordingly.  Must be called after

 * each successful depopulation.

/*

 * Chunk management implementation.

 *

 * To allow different implementations, chunk alloc/free and

 * [de]population are implemented in a separate file which is pulled

 * into this file and compiled together.  The following functions

 * should be implemented.

 *

 * pcpu_populate_chunk		- populate the specified range of a chunk

 * pcpu_depopulate_chunk	- depopulate the specified range of a chunk

 * pcpu_post_unmap_tlb_flush	- flush tlb for the specified range of a chunk

 * pcpu_create_chunk		- create a new chunk

 * pcpu_destroy_chunk		- destroy a chunk, always preceded by full depop

 * pcpu_addr_to_page		- translate address to physical address

 * pcpu_verify_alloc_info	- check alloc_info is acceptable during init

/**

 * pcpu_chunk_addr_search - determine chunk containing specified address

 * @addr: address for which the chunk needs to be determined.

 *

 * This is an internal function that handles all but static allocations.

 * Static percpu address values should never be passed into the allocator.

 *

 * RETURNS:

 * The address of the found chunk.

 is it in the dynamic region (first chunk)? */

 is it in the reserved region? */

	/*

	 * The address is relative to unit0 which might be unused and

	 * thus unmapped.  Offset the address to the unit space of the

	 * current processor before looking it up in the vmalloc

	 * space.  Note that any possible cpu id can be used here, so

	 * there's no need to worry about preemption or cpu hotplug.

 CONFIG_MEMCG_KMEM */

 CONFIG_MEMCG_KMEM */

/**

 * pcpu_alloc - the percpu allocator

 * @size: size of area to allocate in bytes

 * @align: alignment of area (max PAGE_SIZE)

 * @reserved: allocate from the reserved chunk if available

 * @gfp: allocation flags

 *

 * Allocate percpu area of @size bytes aligned at @align.  If @gfp doesn't

 * contain %GFP_KERNEL, the allocation is atomic. If @gfp has __GFP_NOWARN

 * then no warning will be triggered on invalid or failed allocation

 * requests.

 *

 * RETURNS:

 * Percpu pointer to the allocated area on success, NULL on failure.

 whitelisted flags that can be passed to the backing allocators */

	/*

	 * There is now a minimum allocation size of PCPU_MIN_ALLOC_SIZE,

	 * therefore alignment must be a minimum of that many bytes.

	 * An allocation may have internal fragmentation from rounding up

	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.

		/*

		 * pcpu_balance_workfn() allocates memory under this mutex,

		 * and it may wait for memory reclaim. Allow current task

		 * to become OOM victim, in case of memory pressure.

 serve reserved allocations from the reserved chunk if available */

 search through normal chunks */

	/*

	 * No space left.  Create a new chunk.  We don't want multiple

	 * tasks to create chunks simultaneously.  Serialize and create iff

	 * there's still no empty chunk after grabbing the mutex.

 populate if not all pages are already there */

 clear the areas and return address relative to base address */

 see the flag handling in pcpu_balance_workfn() */

/**

 * __alloc_percpu_gfp - allocate dynamic percpu area

 * @size: size of area to allocate in bytes

 * @align: alignment of area (max PAGE_SIZE)

 * @gfp: allocation flags

 *

 * Allocate zero-filled percpu area of @size bytes aligned at @align.  If

 * @gfp doesn't contain %GFP_KERNEL, the allocation doesn't block and can

 * be called from any context but is a lot more likely to fail. If @gfp

 * has __GFP_NOWARN then no warning will be triggered on invalid or failed

 * allocation requests.

 *

 * RETURNS:

 * Percpu pointer to the allocated area on success, NULL on failure.

/**

 * __alloc_percpu - allocate dynamic percpu area

 * @size: size of area to allocate in bytes

 * @align: alignment of area (max PAGE_SIZE)

 *

 * Equivalent to __alloc_percpu_gfp(size, align, %GFP_KERNEL).

/**

 * __alloc_reserved_percpu - allocate reserved percpu area

 * @size: size of area to allocate in bytes

 * @align: alignment of area (max PAGE_SIZE)

 *

 * Allocate zero-filled percpu area of @size bytes aligned at @align

 * from reserved percpu area if arch has set it up; otherwise,

 * allocation is served from the same dynamic area.  Might sleep.

 * Might trigger writeouts.

 *

 * CONTEXT:

 * Does GFP_KERNEL allocation.

 *

 * RETURNS:

 * Percpu pointer to the allocated area on success, NULL on failure.

/**

 * pcpu_balance_free - manage the amount of free chunks

 * @empty_only: free chunks only if there are no populated pages

 *

 * If empty_only is %false, reclaim all fully free chunks regardless of the

 * number of populated pages.  Otherwise, only reclaim chunks that have no

 * populated pages.

 *

 * CONTEXT:

 * pcpu_lock (can be dropped temporarily)

	/*

	 * There's no reason to keep around multiple unused chunks and VM

	 * areas can be scarce.  Destroy all free chunks except for one.

 spare the first one */

/**

 * pcpu_balance_populated - manage the amount of populated pages

 *

 * Maintain a certain amount of populated pages to satisfy atomic allocations.

 * It is possible that this is called when physical memory is scarce causing

 * OOM killer to be triggered.  We should avoid doing so until an actual

 * allocation causes the failure as it is possible that requests can be

 * serviced from already backed regions.

 *

 * CONTEXT:

 * pcpu_lock (can be dropped temporarily)

 gfp flags passed to underlying allocators */

	/*

	 * Ensure there are certain number of free populated pages for

	 * atomic allocs.  Fill up from the most packed so that atomic

	 * allocs don't increase fragmentation.  If atomic allocation

	 * failed previously, always populate the maximum amount.  This

	 * should prevent atomic allocs larger than PAGE_SIZE from keeping

	 * failing indefinitely; however, large atomic allocs are not

	 * something we support properly and can be highly unreliable and

	 * inefficient.

 best effort anyway, don't worry about synchronization */

 @chunk can't go away while pcpu_alloc_mutex is held */

 ran out of chunks to populate, create a new one and retry */

/**

 * pcpu_reclaim_populated - scan over to_depopulate chunks and free empty pages

 *

 * Scan over chunks in the depopulate list and try to release unused populated

 * pages back to the system.  Depopulated chunks are sidelined to prevent

 * repopulating these pages unless required.  Fully free chunks are reintegrated

 * and freed accordingly (1 is kept around).  If we drop below the empty

 * populated pages threshold, reintegrate the chunk if it has empty free pages.

 * Each chunk is scanned in the reverse order to keep populated pages close to

 * the beginning of the chunk.

 *

 * CONTEXT:

 * pcpu_lock (can be dropped temporarily)

 *

	/*

	 * Once a chunk is isolated to the to_depopulate list, the chunk is no

	 * longer discoverable to allocations whom may populate pages.  The only

	 * other accessor is the free path which only returns area back to the

	 * allocator not touching the populated bitmap.

		/*

		 * Scan chunk's pages in the reverse order to keep populated

		 * pages close to the beginning of the chunk.

 no more work to do */

 reintegrate chunk to prevent atomic alloc failures */

			/*

			 * If the page is empty and populated, start or

			 * extend the (i, end) range.  If i == 0, decrease

			 * i and perform the depopulation to cover the last

			 * (first) page in the chunk.

 depopulate if there is an active range */

 reset the range and continue */

 batch tlb flush per chunk to amortize cost */

/**

 * pcpu_balance_workfn - manage the amount of free chunks and populated pages

 * @work: unused

 *

 * For each chunk type, manage the number of fully free chunks and the number of

 * populated pages.  An important thing to consider is when pages are freed and

 * how they contribute to the global counts.

	/*

	 * pcpu_balance_free() is called twice because the first time we may

	 * trim pages in the active pcpu_nr_empty_pop_pages which may cause us

	 * to grow other chunks.  This then gives pcpu_reclaim_populated() time

	 * to move fully free chunks to the active list to be freed if

	 * appropriate.

/**

 * free_percpu - free percpu area

 * @ptr: pointer to area to free

 *

 * Free percpu area @ptr.

 *

 * CONTEXT:

 * Can be called from atomic context.

	/*

	 * If there are more than one fully free chunks, wake up grim reaper.

	 * If the chunk is isolated, it may be in the process of being

	 * reclaimed.  Let reclaim manage cleaning up of that chunk.

 on UP, can't distinguish from other static vars, always false */

/**

 * is_kernel_percpu_address - test whether address is from static percpu area

 * @addr: address to test

 *

 * Test whether @addr belongs to in-kernel static percpu area.  Module

 * static percpu areas are not considered.  For those, use

 * is_module_percpu_address().

 *

 * RETURNS:

 * %true if @addr is from in-kernel static percpu area, %false otherwise.

/**

 * per_cpu_ptr_to_phys - convert translated percpu address to physical address

 * @addr: the address to be converted to physical address

 *

 * Given @addr which is dereferenceable address obtained via one of

 * percpu access macros, this function translates it into its physical

 * address.  The caller is responsible for ensuring @addr stays valid

 * until this function finishes.

 *

 * percpu allocator has special setup for the first chunk, which currently

 * supports either embedding in linear address space or vmalloc mapping,

 * and, from the second one, the backing allocator (currently either vm or

 * km) provides translation.

 *

 * The addr can be translated simply without checking if it falls into the

 * first chunk. But the current code reflects better how percpu allocator

 * actually works, and the verification can discover both bugs in percpu

 * allocator itself and per_cpu_ptr_to_phys() callers. So we keep current

 * code.

 *

 * RETURNS:

 * The physical address for @addr.

	/*

	 * The following test on unit_low/high isn't strictly

	 * necessary but will speed up lookups of addresses which

	 * aren't in the first chunk.

	 *

	 * The address check is against full chunk sizes.  pcpu_base_addr

	 * points to the beginning of the first chunk including the

	 * static region.  Assumes good intent as the first chunk may

	 * not be full (ie. < pcpu_unit_pages in size).

/**

 * pcpu_alloc_alloc_info - allocate percpu allocation info

 * @nr_groups: the number of groups

 * @nr_units: the number of units

 *

 * Allocate ai which is large enough for @nr_groups groups containing

 * @nr_units units.  The returned ai's groups[0].cpu_map points to the

 * cpu_map array which is long enough for @nr_units and filled with

 * NR_CPUS.  It's the caller's responsibility to initialize cpu_map

 * pointer of other groups.

 *

 * RETURNS:

 * Pointer to the allocated pcpu_alloc_info on success, NULL on

 * failure.

/**

 * pcpu_free_alloc_info - free percpu allocation info

 * @ai: pcpu_alloc_info to free

 *

 * Free @ai which was allocated by pcpu_alloc_alloc_info().

/**

 * pcpu_dump_alloc_info - print out information about pcpu_alloc_info

 * @lvl: loglevel

 * @ai: allocation info to dump

 *

 * Print out information about @ai using loglevel @lvl.

 units per alloc, allocs per line */

/**

 * pcpu_setup_first_chunk - initialize the first percpu chunk

 * @ai: pcpu_alloc_info describing how to percpu area is shaped

 * @base_addr: mapped address

 *

 * Initialize the first percpu chunk which contains the kernel static

 * percpu area.  This function is to be called from arch percpu area

 * setup path.

 *

 * @ai contains all information necessary to initialize the first

 * chunk and prime the dynamic percpu allocator.

 *

 * @ai->static_size is the size of static percpu area.

 *

 * @ai->reserved_size, if non-zero, specifies the amount of bytes to

 * reserve after the static area in the first chunk.  This reserves

 * the first chunk such that it's available only through reserved

 * percpu allocation.  This is primarily used to serve module percpu

 * static areas on architectures where the addressing model has

 * limited offset range for symbol relocations to guarantee module

 * percpu symbols fall inside the relocatable range.

 *

 * @ai->dyn_size determines the number of bytes available for dynamic

 * allocation in the first chunk.  The area between @ai->static_size +

 * @ai->reserved_size + @ai->dyn_size and @ai->unit_size is unused.

 *

 * @ai->unit_size specifies unit size and must be aligned to PAGE_SIZE

 * and equal to or larger than @ai->static_size + @ai->reserved_size +

 * @ai->dyn_size.

 *

 * @ai->atom_size is the allocation atom size and used as alignment

 * for vm areas.

 *

 * @ai->alloc_size is the allocation size and always multiple of

 * @ai->atom_size.  This is larger than @ai->atom_size if

 * @ai->unit_size is larger than @ai->atom_size.

 *

 * @ai->nr_groups and @ai->groups describe virtual memory layout of

 * percpu areas.  Units which should be colocated are put into the

 * same group.  Dynamic VM areas will be allocated according to these

 * groupings.  If @ai->nr_groups is zero, a single group containing

 * all units is assumed.

 *

 * The caller should have mapped the first chunk at @base_addr and

 * copied static data to each unit.

 *

 * The first chunk will always contain a static and a dynamic region.

 * However, the static region is not managed by any chunk.  If the first

 * chunk also contains a reserved region, it is served by two chunks -

 * one for the reserved region and one for the dynamic region.  They

 * share the same vm, but use offset regions in the area allocation map.

 * The chunk serving the dynamic region is circulated in the chunk slots

 * and available for dynamic allocation like any other chunk.

 sanity checks */

 process group information and build config tables accordingly */

 determine low/high unit_cpu */

 we're done parsing the input, undefine BUG macro and dump config */

 determine basic parameters */

	/*

	 * Allocate chunk slots.  The slots after the active slots are:

	 *   sidelined_slot - isolated, depopulated chunks

	 *   free_slot - fully free chunks

	 *   to_depopulate_slot - isolated, chunks to depopulate

	/*

	 * The end of the static region needs to be aligned with the

	 * minimum allocation size as this offsets the reserved and

	 * dynamic region.  The first chunk ends page aligned by

	 * expanding the dynamic region, therefore the dynamic region

	 * can be shrunk to compensate while still staying above the

	 * configured sizes.

	/*

	 * Initialize first chunk.

	 * If the reserved_size is non-zero, this initializes the reserved

	 * chunk.  If the reserved_size is zero, the reserved chunk is NULL

	 * and the dynamic region is initialized here.  The first chunk,

	 * pcpu_first_chunk, will always point to the chunk that serves

	 * the dynamic region.

 init dynamic chunk if necessary */

 link the first chunk in */

 include all regions of the first chunk */

 we're done */

 nada */;

/*

 * pcpu_embed_first_chunk() is used by the generic percpu setup.

 * Build it if needed by the arch config or the generic setup is going

 * to be used.

 build pcpu_page_first_chunk() iff needed by the arch config */

 pcpu_build_alloc_info() is used by both embed and page first chunk */

/**

 * pcpu_build_alloc_info - build alloc_info considering distances between CPUs

 * @reserved_size: the size of reserved percpu area in bytes

 * @dyn_size: minimum free size for dynamic allocation in bytes

 * @atom_size: allocation atom size

 * @cpu_distance_fn: callback to determine distance between cpus, optional

 *

 * This function determines grouping of units, their mappings to cpus

 * and other parameters considering needed percpu size, allocation

 * atom size and distances between CPUs.

 *

 * Groups are always multiples of atom size and CPUs which are of

 * LOCAL_DISTANCE both ways are grouped together and share space for

 * units in the same group.  The returned configuration is guaranteed

 * to have CPUs on different nodes on different groups and >=75% usage

 * of allocated virtual address space.

 *

 * RETURNS:

 * On success, pointer to the new allocation_info is returned.  On

 * failure, ERR_PTR value is returned.

 units_per_alloc */

 this function may be called multiple times */

 calculate size_sum and ensure dyn_size is enough for early alloc */

	/*

	 * Determine min_unit_size, alloc_size and max_upa such that

	 * alloc_size is multiple of atom_size and is the smallest

	 * which can accommodate 4k aligned segments which are equal to

	 * or larger than min_unit_size.

 determine the maximum # of units that can fit in an allocation */

 group cpus according to their proximity */

 pop the group's first cpu */

	/*

	 * Wasted space is caused by a ratio imbalance of upa to group_cnt.

	 * Expand the unit_size until we use >= 75% of the units allocated.

	 * Related to atom_size, which could be much larger than the unit_size.

		/*

		 * Don't accept if wastage is over 1/3.  The

		 * greater-than comparison ensures upa==1 always

		 * passes the following check.

 and then don't consume more memory */

 allocate and fill alloc_info */

		/*

		 * Initialize base_offset as if all groups are located

		 * back-to-back.  The caller should update this to

		 * reflect actual allocation.

 BUILD_EMBED_FIRST_CHUNK || BUILD_PAGE_FIRST_CHUNK */

/**

 * pcpu_embed_first_chunk - embed the first percpu chunk into bootmem

 * @reserved_size: the size of reserved percpu area in bytes

 * @dyn_size: minimum free size for dynamic allocation in bytes

 * @atom_size: allocation atom size

 * @cpu_distance_fn: callback to determine distance between cpus, optional

 * @alloc_fn: function to allocate percpu page

 * @free_fn: function to free percpu page

 *

 * This is a helper to ease setting up embedded first percpu chunk and

 * can be called where pcpu_setup_first_chunk() is expected.

 *

 * If this function is used to setup the first chunk, it is allocated

 * by calling @alloc_fn and used as-is without being mapped into

 * vmalloc area.  Allocations are always whole multiples of @atom_size

 * aligned to @atom_size.

 *

 * This enables the first chunk to piggy back on the linear physical

 * mapping which often uses larger page size.  Please note that this

 * can result in very sparse cpu->unit mapping on NUMA machines thus

 * requiring large vmalloc address space.  Don't use this allocator if

 * vmalloc space is not orders of magnitude larger than distances

 * between node memory addresses (ie. 32bit NUMA machines).

 *

 * @dyn_size specifies the minimum dynamic area size.

 *

 * If the needed size is smaller than the minimum or specified unit

 * size, the leftover is returned using @free_fn.

 *

 * RETURNS:

 * 0 on success, -errno on failure.

 allocate, copy and determine base address & max_distance */

 allocate space for the whole group */

 kmemleak tracks the percpu allocations separately */

 warn if maximum distance is further than 75% of vmalloc space */

 and fail if we have fallback */

	/*

	 * Copy data and free unused parts.  This should happen after all

	 * allocations are complete; otherwise, we may end up with

	 * overlapping groups.

 unused unit, free whole */

 copy and return the unused part */

 base address is now known, determine group base offsets */

 BUILD_EMBED_FIRST_CHUNK */

/**

 * pcpu_page_first_chunk - map the first chunk using PAGE_SIZE pages

 * @reserved_size: the size of reserved percpu area in bytes

 * @alloc_fn: function to allocate percpu page, always called with PAGE_SIZE

 * @free_fn: function to free percpu page, always called with PAGE_SIZE

 * @populate_pte_fn: function to populate pte

 *

 * This is a helper to ease setting up page-remapped first percpu

 * chunk and can be called where pcpu_setup_first_chunk() is expected.

 *

 * This is the basic allocator.  Static percpu area is allocated

 * page-by-page into vmalloc area.

 *

 * RETURNS:

 * 0 on success, -errno on failure.

 unaligned allocations can't be freed, round up to page size */

 allocate pages */

 kmemleak tracks the percpu allocations separately */

 allocate vm area, map the pages and copy static data */

 pte already populated, the following shouldn't fail */

		/*

		 * FIXME: Archs with virtual cache should flush local

		 * cache for the linear mapping here - something

		 * equivalent to flush_cache_vmap() on the local cpu.

		 * flush_cache_vmap() can't be used as most supporting

		 * data structures are not set up yet.

 copy static data */

 we're ready, commit */

 BUILD_PAGE_FIRST_CHUNK */

/*

 * Generic SMP percpu area setup.

 *

 * The embedding helper is used because its behavior closely resembles

 * the original non-dynamic generic percpu area setup.  This is

 * important because many archs have addressing restrictions and might

 * fail if the percpu area is located far away from the previous

 * location.  As an added bonus, in non-NUMA cases, embedding is

 * generally a good idea TLB-wise because percpu area can piggy back

 * on the physical linear memory mapping which uses large page

 * mappings on applicable archs.

	/*

	 * Always reserve area for module percpu variables.  That's

	 * what the legacy allocator did.

 CONFIG_HAVE_SETUP_PER_CPU_AREA */

 CONFIG_SMP */

/*

 * UP percpu area setup.

 *

 * UP always uses km-based percpu allocator with identity mapping.

 * Static percpu variables are indistinguishable from the usual static

 * variables and don't require any special preparation.

 kmemleak tracks the percpu allocations separately */

 CONFIG_SMP */

/*

 * pcpu_nr_pages - calculate total number of populated backing pages

 *

 * This reflects the number of pages populated to back chunks.  Metadata is

 * excluded in the number exposed in meminfo as the number of backing pages

 * scales with the number of cpus and can quickly outweigh the memory used for

 * metadata.  It also keeps this calculation nice and simple.

 *

 * RETURNS:

 * Total number of populated backing pages in use by the allocator.

/*

 * Percpu allocator is initialized early during boot when neither slab or

 * workqueue is available.  Plug async management until everything is up

 * and running.

/*

 * zsmalloc memory allocator

 *

 * Copyright (C) 2011  Nitin Gupta

 * Copyright (C) 2012, 2013 Minchan Kim

 *

 * This code is released using a dual license strategy: BSD/GPL

 * You can choose the license that better fits your requirements.

 *

 * Released under the terms of 3-clause BSD License

 * Released under the terms of GNU General Public License Version 2.0

/*

 * Following is how we use various fields and flags of underlying

 * struct page(s) to form a zspage.

 *

 * Usage of struct page fields:

 *	page->private: points to zspage

 *	page->freelist(index): links together all component pages of a zspage

 *		For the huge page, this is always 0, so we use this field

 *		to store handle.

 *	page->units: first object offset in a subpage of zspage

 *

 * Usage of struct page flags:

 *	PG_private: identifies the first component page

 *	PG_owner_priv_1: identifies the huge component page

 *

/*

 * This must be power of 2 and greater than or equal to sizeof(link_free).

 * These two conditions ensure that any 'struct link_free' itself doesn't

 * span more than 1 page which avoids complex case of mapping 2 pages simply

 * to restore link_free pointer values.

/*

 * A single 'zspage' is composed of up to 2^N discontiguous 0-order (single)

 * pages. ZS_MAX_ZSPAGE_ORDER defines upper limit on N.

/*

 * Object location (<PFN>, <obj_idx>) is encoded as

 * a single (unsigned long) handle value.

 *

 * Note that object index <obj_idx> starts from 0.

 *

 * This is made more complicated by various memory models and PAE.

/*

 * If this definition of MAX_PHYSMEM_BITS is used, OBJ_INDEX_BITS will just

 * be PAGE_SHIFT

/*

 * Memory for allocating for handle keeps object position by

 * encoding <page, obj_idx> and the encoded value has a room

 * in least bit(ie, look at obj_to_location).

 * We use the bit to synchronize between object access by

 * user and migration.

/*

 * Head in allocated object should have OBJ_ALLOCATED_TAG

 * to identify the object was allocated or not.

 * It's okay to add the status bit in the least bit because

 * header keeps handle which is 4byte-aligned address so we

 * have room for two bit at least.

 ZS_MIN_ALLOC_SIZE must be multiple of ZS_ALIGN */

 each chunk includes extra space to keep handle */

/*

 * On systems with 4K page size, this gives 255 size classes! There is a

 * trader-off here:

 *  - Large number of size classes is potentially wasteful as free page are

 *    spread across these classes

 *  - Small number of size classes causes large internal fragmentation

 *  - Probably its better to use specific size classes (empirically

 *    determined). NOTE: all those class sizes must be set as multiple of

 *    ZS_ALIGN to make sure link_free itself never has to span 2 pages.

 *

 *  ZS_MIN_ALLOC_SIZE and ZS_SIZE_CLASS_DELTA must be multiple of ZS_ALIGN

 *  (reason above)

/*

 * We assign a page to ZS_ALMOST_EMPTY fullness group when:

 *	n <= N / f, where

 * n = number of allocated objects

 * N = total number of objects zspage can store

 * f = fullness_threshold_frac

 *

 * Similarly, we assign zspage to:

 *	ZS_ALMOST_FULL	when n > N / f

 *	ZS_EMPTY	when n == 0

 *	ZS_FULL		when n == N

 *

 * (see: fix_fullness_group())

	/*

	 * Size of objects stored in this class. Must be multiple

	 * of ZS_ALIGN.

 Number of PAGE_SIZE sized pages to combine to form a 'zspage' */

 huge object: pages_per_zspage == 1 && maxobj_per_zspage == 1 */

/*

 * Placed within free objects to form a singly linked list.

 * For every zspage, zspage->freeobj gives head of this list.

 *

 * This must be power of 2 and less than or equal to ZS_ALIGN

		/*

		 * Free object index;

		 * It's valid for non-allocated object

		/*

		 * Handle of allocated object.

 Compact classes */

 A wait queue for when migration races with async_free_zspage() */

 fullness list */

 copy buffer for objects that span pages */

 address of kmap_atomic()'ed pages */

 mapping mode */

	/*

	 * lsb of @obj represents handle lock while other bits

	 * represent object value the handle is pointing so

	 * updating shouldn't do store tearing.

 zpool driver */

	/*

	 * Ignore global gfp flags: zs_malloc() may be invoked from

	 * different contexts and its caller must provide a valid

	 * gfp mask.

 CONFIG_ZPOOL */

 per-cpu VM mapping areas for zspage accesses that cross page boundaries */

 Protected by class->lock */

/*

 * zsmalloc divides the pool into various size classes where each

 * class maintains a list of zspages where each zspage is divided

 * into equal sized chunks. Each allocation falls into one of these

 * classes depending on its size. This function returns index of the

 * size class which has chunk size big enough to hold the given size.

 type can be of enum type zs_stat_type or fullness_group */

 type can be of enum type zs_stat_type or fullness_group */

 type can be of enum type zs_stat_type or fullness_group */

 CONFIG_ZSMALLOC_STAT */

/*

 * For each size class, zspages are divided into different groups

 * depending on how "full" they are. This was done so that we could

 * easily find empty or nearly empty zspages when we try to shrink

 * the pool (not yet implemented). This function returns fullness

 * status of the given page.

/*

 * Each size class maintains various freelists and zspages are assigned

 * to one of these freelists based on the number of live objects they

 * have. This functions inserts the given zspage into the freelist

 * identified by <class, fullness_group>.

	/*

	 * We want to see more ZS_FULL pages and less almost empty/full.

	 * Put pages with higher ->inuse first.

/*

 * This function removes the given zspage from the freelist identified

 * by <class, fullness_group>.

/*

 * Each size class maintains zspages in different fullness groups depending

 * on the number of live objects they contain. When allocating or freeing

 * objects, the fullness status of the page can change, say, from ALMOST_FULL

 * to ALMOST_EMPTY when freeing an object. This function checks if such

 * a status change has occurred for the given page and accordingly moves the

 * page from the freelist of the old fullness group to that of the new

 * fullness group.

/*

 * We have to decide on how many pages to link together

 * to form a zspage for each size class. This is important

 * to reduce wastage due to unusable space left at end of

 * each zspage which is given as:

 *     wastage = Zp % class_size

 *     usage = Zp - wastage

 * where Zp = zspage size = k * PAGE_SIZE where k = 1, 2, ...

 *

 * For example, for size class of 3/8 * PAGE_SIZE, we should

 * link together 3 PAGE_SIZE sized pages to form a zspage

 * since then we can perfectly fit in 8 such objects.

 zspage order which gives maximum used size per KB */

/**

 * obj_to_location - get (<page>, <obj_idx>) from encoded object value

 * @obj: the encoded object value

 * @page: page object resides in zspage

 * @obj_idx: object index

/**

 * location_to_obj - get obj value encoded from (<page>, <obj_idx>)

 * @page: page object resides in zspage

 * @obj_idx: object index

 Initialize a newly allocated zspage */

		/*

		 * We now come to the last (full or partial) object on this

		 * page, which must point to the first object on the next

		 * page (if present)

			/*

			 * Reset OBJ_TAG_BITS bit to last link to tell

			 * whether it's allocated object or not.

	/*

	 * Allocate individual pages and link them together as:

	 * 1. all pages are linked together using page->freelist

	 * 2. each sub-page point to zspage using page->private

	 *

	 * we set PG_private to identify the first page (i.e. no other sub-page

	 * has this flag set).

/*

 * Allocate a zspage for the given size class

	/*

	 * Make sure we don't leak memory if a cpu UP notification

	 * and zs_init() race and both call zs_cpu_up() on the same cpu

 disable page faults to match kmap_atomic() return conditions */

 no read fastpath */

 copy object to per-cpu buffer */

 no write fastpath */

 copy per-cpu buffer to object */

 enable page faults to match kunmap_atomic() return conditions */

/**

 * zs_map_object - get address of allocated object from handle.

 * @pool: pool from which the object was allocated

 * @handle: handle returned from zs_malloc

 * @mm: mapping mode to use

 *

 * Before using an object allocated from zs_malloc, it must be mapped using

 * this function. When done with the object, it must be unmapped using

 * zs_unmap_object.

 *

 * Only one object can be mapped per cpu at a time. There is no protection

 * against nested mappings.

 *

 * This function returns with preemption and page faults disabled.

	/*

	 * Because we use per-cpu mapping areas shared among the

	 * pools/users, we can't allow mapping in interrupt context

	 * because it can corrupt another users mappings.

 From now on, migration cannot move the object */

 migration cannot move any subpage in this zspage */

 this object is contained entirely within a page */

 this object spans two pages */

/**

 * zs_huge_class_size() - Returns the size (in bytes) of the first huge

 *                        zsmalloc &size_class.

 * @pool: zsmalloc pool to use

 *

 * The function returns the size of the first huge class - any object of equal

 * or bigger size will be stored in zspage consisting of a single physical

 * page.

 *

 * Context: Any context.

 *

 * Return: the size (in bytes) of the first huge zsmalloc &size_class.

 record handle in the header of allocated chunk */

 record handle to page->index */

/**

 * zs_malloc - Allocate block of given size from pool.

 * @pool: pool to allocate from

 * @size: size of block to allocate

 * @gfp: gfp flags when allocating object

 *

 * On success, handle to the allocated object is returned,

 * otherwise 0.

 * Allocation requests with size > ZS_MAX_ALLOC_SIZE will fail.

 extra space in chunk to keep the handle */

 Now move the zspage to another fullness group, if required */

 We completely set up zspage so mark them as movable */

 Insert this object in containing zspage's freelist */

 If zspage is isolated, zs_page_putback will free the zspage */

/*

 * Find alloced object in zspage from index object and

 * return handle.

 Source spage for migration which could be a subpage of zspage */

	/* Destination page for migration which should be a first page

	 /* Starting object index within @s_page which used for live object

 Stop if there is no more space */

		/*

		 * record_obj updates handle's value to free_obj and it will

		 * invalidate lock bit(ie, HANDLE_PIN_BIT) of handle, which

		 * breaks synchronization using pin_tag(e,g, zs_free) so

		 * let's keep the lock bit.

 Remember last position in this iteration */

/*

 * putback_zspage - add @zspage into right class's fullness list

 * @class: destination class

 * @zspage: target page

 *

 * Return @zspage's fullness_group

/*

 * To prevent zspage destroy during migration, zspage freeing should

 * hold locks of all pages in the zspage.

 Number of isolated subpage for *page migration* in this zspage */

	/*

	 * Checking pool->destroying must happen after atomic_long_dec()

	 * for pool->isolated_pages above. Paired with the smp_mb() in

	 * zs_unregister_migration().

	/*

	 * Page is locked so zspage couldn't be destroyed. For detail, look at

	 * lock_zspage in free_zspage.

	/*

	 * Without class lock, fullness could be stale while class_idx is okay

	 * because class_idx is constant unless page is freed so we should get

	 * fullness again under class lock.

 zspage is isolated for object migration */

	/*

	 * If this is first time isolation for the zspage, isolate zspage from

	 * size_class to prevent further object allocation from the zspage.

	/*

	 * We cannot support the _NO_COPY case here, because copy needs to

	 * happen under the zs lock, which does not work with

	 * MIGRATE_SYNC_NO_COPY workflow.

 Concurrent compactor cannot migrate any subpage in zspage */

		/*

		 * Set "offset" to end of the page so that every loops

		 * skips unnecessary object scanning.

	/*

	 * Here, any user cannot access all objects in the zspage so let's move.

	/*

	 * Page migration is done so let's putback isolated zspage to

	 * the list if @page is final isolated subpage in the zspage.

		/*

		 * We cannot race with zs_destroy_pool() here because we wait

		 * for isolation to hit zero before we start destroying.

		 * Also, we ensure that everyone can see pool->destroying before

		 * we start waiting.

		/*

		 * Due to page_lock, we cannot free zspage immediately

		 * so let's defer.

 Function for resolving migration */

	/*

	 * We're in the process of destroying the pool, so there are no

	 * active allocations. zs_page_isolate() fails for completely free

	 * zspages, so we need only wait for the zs_pool's isolated

	 * count to hit zero.

	/*

	 * We need a memory barrier here to ensure global visibility of

	 * pool->destroying. Thus pool->isolated pages will either be 0 in which

	 * case we don't care, or it will be > 0 and pool->destroying will

	 * ensure that we wake up once isolation hits 0.

 This can block */

/*

 * Caller should hold page_lock of all pages in the zspage

 * In here, we cannot use zspage meta data.

/*

 *

 * Based on the number of unused allocated objects calculate

 * and return the number of pages that we can free.

			/*

			 * If there is no more space in dst_page, resched

			 * and see if anyone had allocated another zspage.

 Stop if we couldn't find slot */

	/*

	 * Compact classes and calculate compaction delta.

	 * Can run concurrently with a manually triggered

	 * (by user) compaction.

/**

 * zs_create_pool - Creates an allocation pool to work from.

 * @name: pool name to be created

 *

 * This function must be called before anything when using

 * the zsmalloc allocator.

 *

 * On success, a pointer to the newly created pool is returned,

 * otherwise NULL.

	/*

	 * Iterate reversely, because, size of size_class that we want to use

	 * for merging should be larger or equal to current size.

		/*

		 * We iterate from biggest down to smallest classes,

		 * so huge_class_size holds the size of the first huge

		 * class. Any object bigger than or equal to that will

		 * endup in the huge class.

			/*

			 * The object uses ZS_HANDLE_SIZE bytes to store the

			 * handle. We need to subtract it, because zs_malloc()

			 * unconditionally adds handle size before it performs

			 * size class search - so object may be smaller than

			 * huge class size, yet it still can end up in the huge

			 * class because it grows by ZS_HANDLE_SIZE extra bytes

			 * right before class lookup.

		/*

		 * size_class is used for normal zsmalloc operation such

		 * as alloc/free for that size. Although it is natural that we

		 * have one size_class for each size, there is a chance that we

		 * can get more memory utilization if we use one size_class for

		 * many different sizes whose size_class have same

		 * characteristics. So, we makes size_class point to

		 * previous size_class if possible.

 debug only, don't abort if it fails */

	/*

	 * Not critical since shrinker is only used to trigger internal

	 * defragmentation of the pool which is pretty optional thing.  If

	 * registration fails we still can use the pool normally and user can

	 * trigger compaction manually. Thus, ignore return code.

 SPDX-License-Identifier: GPL-2.0

/*

 * CMA DebugFS Interface

 *

 * Copyright (c) 2015 Sasha Levin <sasha.levin@oracle.com>

 pages counter is smaller than sizeof(int) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Memory merging support.

 *

 * This code enables dynamic sharing of identical pages found in different

 * memory areas, even if they are not shared by fork()

 *

 * Copyright (C) 2008-2009 Red Hat, Inc.

 * Authors:

 *	Izik Eidus

 *	Andrea Arcangeli

 *	Chris Wright

 *	Hugh Dickins

/**

 * DOC: Overview

 *

 * A few notes about the KSM scanning process,

 * to make it easier to understand the data structures below:

 *

 * In order to reduce excessive scanning, KSM sorts the memory pages by their

 * contents into a data structure that holds pointers to the pages' locations.

 *

 * Since the contents of the pages may change at any moment, KSM cannot just

 * insert the pages into a normal sorted tree and expect it to find anything.

 * Therefore KSM uses two data structures - the stable and the unstable tree.

 *

 * The stable tree holds pointers to all the merged pages (ksm pages), sorted

 * by their contents.  Because each such page is write-protected, searching on

 * this tree is fully assured to be working (except when pages are unmapped),

 * and therefore this tree is called the stable tree.

 *

 * The stable tree node includes information required for reverse

 * mapping from a KSM page to virtual addresses that map this page.

 *

 * In order to avoid large latencies of the rmap walks on KSM pages,

 * KSM maintains two types of nodes in the stable tree:

 *

 * * the regular nodes that keep the reverse mapping structures in a

 *   linked list

 * * the "chains" that link nodes ("dups") that represent the same

 *   write protected memory content, but each "dup" corresponds to a

 *   different KSM page copy of that content

 *

 * Internally, the regular nodes, "dups" and "chains" are represented

 * using the same struct stable_node structure.

 *

 * In addition to the stable tree, KSM uses a second data structure called the

 * unstable tree: this tree holds pointers to pages which have been found to

 * be "unchanged for a period of time".  The unstable tree sorts these pages

 * by their contents, but since they are not write-protected, KSM cannot rely

 * upon the unstable tree to work correctly - the unstable tree is liable to

 * be corrupted as its contents are modified, and so it is called unstable.

 *

 * KSM solves this problem by several techniques:

 *

 * 1) The unstable tree is flushed every time KSM completes scanning all

 *    memory areas, and then the tree is rebuilt again from the beginning.

 * 2) KSM will only insert into the unstable tree, pages whose hash value

 *    has not changed since the previous scan of all memory areas.

 * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the

 *    colors of the nodes and not on their contents, assuring that even when

 *    the tree gets "corrupted" it won't get out of balance, so scanning time

 *    remains the same (also, searching and inserting nodes in an rbtree uses

 *    the same algorithm, so we have no overhead when we flush and rebuild).

 * 4) KSM never flushes the stable tree, which means that even if it were to

 *    take 10 attempts to find a page in the unstable tree, once it is found,

 *    it is secured in the stable tree.  (When we scan a new page, we first

 *    compare it against the stable tree, and then against the unstable tree.)

 *

 * If the merge_across_nodes tunable is unset, then KSM maintains multiple

 * stable trees and multiple unstable trees: one of each for each NUMA node.

/**

 * struct mm_slot - ksm information per mm that is being scanned

 * @link: link to the mm_slots hash list

 * @mm_list: link into the mm_slots list, rooted in ksm_mm_head

 * @rmap_list: head for this mm_slot's singly-linked list of rmap_items

 * @mm: the mm that this information is valid for

/**

 * struct ksm_scan - cursor for scanning

 * @mm_slot: the current mm_slot we are scanning

 * @address: the next address inside that to be scanned

 * @rmap_list: link to the next rmap to be scanned in the rmap_list

 * @seqnr: count of completed full scans (needed when removing unstable node)

 *

 * There is only the one ksm_scan instance of this cursor structure.

/**

 * struct stable_node - node of the stable rbtree

 * @node: rb node of this ksm page in the stable tree

 * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list

 * @hlist_dup: linked into the stable_node->hlist with a stable_node chain

 * @list: linked into migrate_nodes, pending placement in the proper node tree

 * @hlist: hlist head of rmap_items using this ksm page

 * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)

 * @chain_prune_time: time of the last full garbage collection

 * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN

 * @nid: NUMA node id of stable tree in which linked (may not match kpfn)

 when node of stable tree */

 when listed for migration */

	/*

	 * STABLE_NODE_CHAIN can be any negative number in

	 * rmap_hlist_len negative range, but better not -1 to be able

	 * to reliably detect underflows.

/**

 * struct rmap_item - reverse mapping item for virtual addresses

 * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list

 * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree

 * @nid: NUMA node id of unstable tree in which linked (may not match page)

 * @mm: the memory structure this rmap_item is pointing into

 * @address: the virtual address this rmap_item tracks (+ flags in low bits)

 * @oldchecksum: previous checksum of the page at that virtual address

 * @node: rb node of this rmap_item in the unstable tree

 * @head: pointer to stable_node heading this list in the stable tree

 * @hlist: link into hlist of rmap_items hanging off that stable_node

 when stable */

 when node of unstable tree */

 + low bits used for flags below */

 when unstable */

 when node of unstable tree */

 when listed from stable tree */

 low bits of unstable tree seqnr */

 is a node of the unstable tree */

 is listed from the stable tree */

 The stable and unstable tree heads */

 Recently migrated nodes of stable tree, pending proper placement */

 The number of nodes in the stable tree */

 The number of page slots additionally sharing those nodes */

 The number of nodes in the unstable tree */

 The number of rmap_items in use: to calculate pages_volatile */

 The number of stable_node chains */

 The number of stable_node dups linked to the stable_node chains */

 Delay in pruning stale stable_node_dups in the stable_node_chains */

 Maximum number of page slots sharing a stable node */

 Number of pages ksmd should scan in one batch */

 Milliseconds ksmd should sleep between batches */

 Checksum of an empty (zeroed) page */

 Whether to merge empty (zeroed) pages with actual zero pages */

 Zeroed when merging across nodes is not allowed */

 debug safety */

	/*

	 * The allocation can take too long with GFP_KERNEL when memory is under

	 * pressure, which may lead to hung task warnings.  Adding __GFP_HIGH

	 * grants access to memory reserves, helping to avoid this problem.

 initialization failed */

/*

 * ksmd, and unmerge_and_remove_all_rmap_items(), must not touch an mm's

 * page tables after it has passed through ksm_exit() - which, if necessary,

 * takes mmap_lock briefly to serialize against them.  ksm_exit() does not set

 * a special flag: they can just back out as soon as mm_users goes to zero.

 * ksm_test_exit() is used throughout to make this test for exit: in some

 * places for correctness, in some places just to avoid unnecessary work.

/*

 * We use break_ksm to break COW on a ksm page: it's a stripped down

 *

 *	if (get_user_pages(addr, 1, FOLL_WRITE, &page, NULL) == 1)

 *		put_page(page);

 *

 * but taking great care only to touch a ksm page, in a VM_MERGEABLE vma,

 * in case the application has unmapped and remapped mm,addr meanwhile.

 * Could a ksm page appear anywhere else?  Actually yes, in a VM_PFNMAP

 * mmap of /dev/mem, where we would not want to touch it.

 *

 * FAULT_FLAG/FOLL_REMOTE are because we do this outside the context

 * of the process that owns 'vma'.  We also do not want to enforce

 * protection keys here anyway.

	/*

	 * We must loop because handle_mm_fault() may back out if there's

	 * any difficulty e.g. if pte accessed bit gets updated concurrently.

	 *

	 * VM_FAULT_WRITE is what we have been hoping for: it indicates that

	 * COW has been broken, even if the vma does not permit VM_WRITE;

	 * but note that a concurrent fault might break PageKsm for us.

	 *

	 * VM_FAULT_SIGBUS could occur if we race with truncation of the

	 * backing file, which also invalidates anonymous pages: that's

	 * okay, that truncation will have unmapped the PageKsm for us.

	 *

	 * VM_FAULT_OOM: at the time of writing (late July 2009), setting

	 * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the

	 * current task has TIF_MEMDIE set, and will be OOM killed on return

	 * to user; and ksmd, having no mm, would never be chosen for that.

	 *

	 * But if the mm is in a limited mem_cgroup, then the fault may fail

	 * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and

	 * even ksmd can fail in this way - though it's usually breaking ksm

	 * just to undo a merge it made a moment before, so unlikely to oom.

	 *

	 * That's a pity: we might therefore have more kernel pages allocated

	 * than we're counting as nodes in the stable tree; but ksm_do_scan

	 * will retry to break_cow on each pass, so should recover the page

	 * in due course.  The important thing is to not let VM_MERGEABLE

	 * be cleared while any such pages might remain in the area.

	/*

	 * It is not an accident that whenever we want to break COW

	 * to undo, we also need to drop a reference to the anon_vma.

/*

 * This helper is used for getting right index into array of tree roots.

 * When merge_across_nodes knob is set to 1, there are only two rb-trees for

 * stable and unstable pages from all nodes with roots in index 0. Otherwise,

 * every node has its own stable and unstable tree.

 debug */

		/*

		 * Put the stable node chain in the first dimension of

		 * the stable tree and at the same time remove the old

		 * stable node.

		/*

		 * Move the old stable node to the second dimension

		 * queued in the hlist_dup. The invariant is that all

		 * dup stable_nodes in the chain->hlist point to pages

		 * that are write protected and have the exact same

		 * content.

 check it's not STABLE_NODE_CHAIN or negative */

	/*

	 * We need the second aligned pointer of the migrate_nodes

	 * list_head to stay clear from the rb_parent_color union

	 * (aligned and different than any node) and also different

	 * from &migrate_nodes. This will verify that future list.h changes

	 * don't break STABLE_NODE_DUP_HEAD. Only recent gcc can handle it.

/*

 * get_ksm_page: checks if the page indicated by the stable node

 * is still its ksm page, despite having held no reference to it.

 * In which case we can trust the content of the page, and it

 * returns the gotten page; but if the page has now been zapped,

 * remove the stale node from the stable tree and return NULL.

 * But beware, the stable node's page might be being migrated.

 *

 * You would expect the stable_node to hold a reference to the ksm page.

 * But if it increments the page's count, swapping out has to wait for

 * ksmd to come around again before it can free the page, which may take

 * seconds or even minutes: much too unresponsive.  So instead we use a

 * "keyhole reference": access to the ksm page from the stable node peeps

 * out through its keyhole to see if that page still holds the right key,

 * pointing back to this stable node.  This relies on freeing a PageAnon

 * page to reset its page->mapping to NULL, and relies on no other use of

 * a page to put something that might look like our key in page->mapping.

 * is on its way to being freed; but it is an anomaly to bear in mind.

 Address dependency. */

	/*

	 * We cannot do anything with the page while its refcount is 0.

	 * Usually 0 means free, or tail of a higher-order page: in which

	 * case this node is no longer referenced, and should be freed;

	 * however, it might mean that the page is under page_ref_freeze().

	 * The __remove_mapping() case is easy, again the node is now stale;

	 * the same is in reuse_ksm_page() case; but if page is swapcache

	 * in migrate_page_move_mapping(), it might still be our page,

	 * in which case it's essential to keep the node.

		/*

		 * Another check for page->mapping != expected_mapping would

		 * work here too.  We have chosen the !PageSwapCache test to

		 * optimize the common case, when the page is or is about to

		 * be freed: PageSwapCache is cleared (under spin_lock_irq)

		 * in the ref_freeze section of __remove_mapping(); but Anon

		 * page->mapping reset to NULL later, in free_pages_prepare().

	/*

	 * We come here from above when page->mapping or !PageSwapCache

	 * suggests that the node is stale; but it might be under migration.

	 * We need smp_rmb(), matching the smp_wmb() in folio_migrate_ksm(),

	 * before checking whether node->kpfn has been changed.

/*

 * Removing rmap_item from stable or unstable tree.

 * This function will clean the information from the stable/unstable tree.

		/*

		 * Usually ksmd can and must skip the rb_erase, because

		 * root_unstable_tree was already reset to RB_ROOT.

		 * But be careful when an mm is exiting: do the rb_erase

		 * if this rmap_item was inserted by this scan, rather

		 * than left over from before.

 we're called from many long loops */

/*

 * Though it's very tempting to unmerge rmap_items from stable tree rather

 * than check every pte of a given vma, the locking doesn't quite work for

 * that - an rmap_item is assigned to the stable tree after inserting ksm

 * page and upping mmap_lock.  Nor does it fit with the way we skip dup'ing

 * rmap_items from parent to child at fork time (so as not to waste time

 * if exit comes before the next scan reaches it).

 *

 * Similarly, although we'd like to remove rmap_items (so updating counts

 * and freeing memory) when unmerging an area, it's easier to leave that

 * to the next pass of ksmd - consider, for example, how ksmd might be

 * in cmp_and_merge_page on one of the rmap_items we would be removing.

/*

 * Only called through the sysfs control interface:

		/*

		 * get_ksm_page did remove_node_from_stable_tree itself.

	/*

	 * Page could be still mapped if this races with __mmput() running in

	 * between ksm_exit() and exit_mmap(). Just refuse to let

	 * merge_across_nodes/max_page_sharing be switched.

		/*

		 * The stable node did not yet appear stale to get_ksm_page(),

		 * since that allows for an unmapped ksm page to be recognized

		 * right up until it is freed; but the node is safe to remove.

		 * This page might be in a pagevec waiting to be freed,

		 * or it might be PageSwapCache (perhaps under writeback),

		 * or it might have been removed from swapcache a moment ago.

 proceed to next nid */

 Clean up stable nodes, but don't worry if some are still busy */

 CONFIG_SYSFS */

		/*

		 * Ok this is tricky, when get_user_pages_fast() run it doesn't

		 * take any lock, therefore the check that we are going to make

		 * with the pagecount against the mapcount is racy and

		 * O_DIRECT can happen right after the check.

		 * So we clear the pte and flush the tlb before the check

		 * this assure us that no O_DIRECT can happen after the check

		 * or in the middle of the check.

		 *

		 * No need to notify as we are downgrading page table to read

		 * only not changing it to point to a new page.

		 *

		 * See Documentation/vm/mmu_notifier.rst

		/*

		 * Check that no O_DIRECT or similar I/O is in progress on the

		 * page

/**

 * replace_page - replace page in vma by new ksm page

 * @vma:      vma that holds the pte pointing to page

 * @page:     the page we are replacing by kpage

 * @kpage:    the ksm page we replace page by

 * @orig_pte: the original value of the pte

 *

 * Returns 0 on success, -EFAULT on failure.

	/*

	 * No need to check ksm_use_zero_pages here: we can only have a

	 * zero_page here if ksm_use_zero_pages was enabled already.

		/*

		 * We're replacing an anonymous page with a zero page, which is

		 * not anonymous. We need to do proper accounting otherwise we

		 * will get wrong values in /proc, and a BUG message in dmesg

		 * when tearing down the mm.

	/*

	 * No need to notify as we are replacing a read only page with another

	 * read only page with the same content.

	 *

	 * See Documentation/vm/mmu_notifier.rst

/*

 * try_to_merge_one_page - take two pages and merge them into one

 * @vma: the vma that holds the pte pointing to page

 * @page: the PageAnon page that we want to replace with kpage

 * @kpage: the PageKsm page that we want to map instead of page,

 *         or NULL the first time when we want to use page as kpage.

 *

 * This function returns 0 if the pages were merged, -EFAULT otherwise.

 ksm page forked */

	/*

	 * We need the page lock to read a stable PageSwapCache in

	 * write_protect_page().  We use trylock_page() instead of

	 * lock_page() because we don't want to wait here - we

	 * prefer to continue scanning and merging different pages,

	 * then come back to this page when it is unlocked.

	/*

	 * If this anonymous page is mapped only here, its pte may need

	 * to be write-protected.  If it's mapped elsewhere, all of its

	 * ptes are necessarily already write-protected.  But in either

	 * case, we need to lock and check page_count is not raised.

			/*

			 * While we hold page lock, upgrade page from

			 * PageAnon+anon_vma to PageKsm+NULL stable_node:

			 * stable_tree_insert() will update stable_node.

			/*

			 * Page reclaim just frees a clean page with no dirty

			 * ptes: make sure that the ksm page would be swapped.

 for final unlock */

/*

 * try_to_merge_with_ksm_page - like try_to_merge_two_pages,

 * but no new kernel page is allocated: kpage must already be a ksm page.

 *

 * This function returns 0 if the pages were merged, -EFAULT otherwise.

 Unstable nid is in union with stable anon_vma: remove first */

 Must get reference to anon_vma while still holding mmap_lock */

/*

 * try_to_merge_two_pages - take two identical pages and prepare them

 * to be merged into one page.

 *

 * This function returns the kpage if we successfully merged two identical

 * pages into one ksm page, NULL otherwise.

 *

 * Note that this function upgrades page to ksm page: if one of the pages

 * is already a ksm page, try_to_merge_with_ksm_page should be used.

		/*

		 * If that fails, we have a ksm page with only one pte

		 * pointing to it: so break it.

	/*

	 * Check that at least one mapping still exists, otherwise

	 * there's no much point to merge and share with this

	 * stable_node, as the underlying tree_page of the other

	 * sharer is going to be freed soon.

		/*

		 * We must walk all stable_node_dup to prune the stale

		 * stable nodes during lookup.

		 *

		 * get_ksm_page can drop the nodes from the

		 * stable_node->hlist if they point to freed pages

		 * (that's why we do a _safe walk). The "dup"

		 * stable_node parameter itself will be freed from

		 * under us if it returns NULL.

 skip put_page for found dup */

		/*

		 * nr is counting all dups in the chain only if

		 * prune_stale_stable_nodes is true, otherwise we may

		 * break the loop at nr == 1 even if there are

		 * multiple entries.

			/*

			 * If there's not just one entry it would

			 * corrupt memory, better BUG_ON. In KSM

			 * context with no lock held it's not even

			 * fatal.

			/*

			 * There's just one entry and it is below the

			 * deduplication limit so drop the chain.

			/*

			 * NOTE: the caller depends on the stable_node

			 * to be equal to stable_node_dup if the chain

			 * was collapsed.

			/*

			 * Just for robustness, as stable_node is

			 * otherwise left as a stable pointer, the

			 * compiler shall optimize it away at build

			 * time.

			/*

			 * If the found stable_node dup can accept one

			 * more future merge (in addition to the one

			 * that is underway) and is not at the head of

			 * the chain, put it there so next search will

			 * be quicker in the !prune_stale_stable_nodes

			 * case.

			 *

			 * NOTE: it would be inaccurate to use nr > 1

			 * instead of checking the hlist.first pointer

			 * directly, because in the

			 * prune_stale_stable_nodes case "nr" isn't

			 * the position of the found dup in the chain,

			 * but the total number of dups in the chain.

/*

 * Like for get_ksm_page, this function can free the *_stable_node and

 * *_stable_node_dup if the returned tree_page is NULL.

 *

 * It can also free and overwrite *_stable_node with the found

 * stable_node_dup if the chain is collapsed (in which case

 * *_stable_node will be equal to *_stable_node_dup like if the chain

 * never existed). It's up to the caller to verify tree_page is not

 * NULL before dereferencing *_stable_node or *_stable_node_dup.

 *

 * *_stable_node_dup is really a second output parameter of this

 * function and will be overwritten in all cases, the caller doesn't

 * need to initialize it.

		/*

		 * _stable_node_dup set to NULL means the stable_node

		 * reached the ksm_max_page_sharing limit.

 not pruning dups so s_n cannot have changed */

/*

 * stable_tree_search - search for page inside the stable tree

 *

 * This function checks if there is a page inside the stable tree

 * with identical content to the page that we are scanning right now.

 *

 * This function returns the stable tree node of identical content if found,

 * NULL otherwise.

 ksm page forked */

		/*

		 * NOTE: stable_node may have been freed by

		 * chain_prune() if the returned stable_node_dup is

		 * not NULL. stable_node_dup may have been inserted in

		 * the rbtree instead as a regular stable_node (in

		 * order to collapse the stable_node chain if a single

		 * stable_node dup was found in it). In such case the

		 * stable_node is overwritten by the calleee to point

		 * to the stable_node_dup that was collapsed in the

		 * stable rbtree and stable_node will be equal to

		 * stable_node_dup like if the chain never existed.

			/*

			 * Either all stable_node dups were full in

			 * this stable_node chain, or this chain was

			 * empty and should be rb_erased.

 rb_erase just run */

			/*

			 * Take any of the stable_node dups page of

			 * this stable_node chain to let the tree walk

			 * continue. All KSM pages belonging to the

			 * stable_node dups in a stable_node chain

			 * have the same content and they're

			 * write protected at all times. Any will work

			 * fine to continue the walk.

			/*

			 * If we walked over a stale stable_node,

			 * get_ksm_page() will call rb_erase() and it

			 * may rebalance the tree from under us. So

			 * restart the search from scratch. Returning

			 * NULL would be safe too, but we'd generate

			 * false negative insertions just because some

			 * stable_node was stale.

				/*

				 * Test if the migrated page should be merged

				 * into a stable node dup. If the mapcount is

				 * 1 we can migrate it with another KSM page

				 * without adding it to the chain.

				/*

				 * If the stable_node is a chain and

				 * we got a payload match in memcmp

				 * but we cannot merge the scanned

				 * page in any of the existing

				 * stable_node dups because they're

				 * all full, we need to wait the

				 * scanned page to find itself a match

				 * in the unstable tree to create a

				 * brand new KSM page to add later to

				 * the dups of this stable_node.

			/*

			 * Lock and unlock the stable_node's page (which

			 * might already have been migrated) so that page

			 * migration is sure to notice its raised count.

			 * It would be more elegant to return stable_node

			 * than kpage, but that involves more changes.

				/*

				 * The tree may have been rebalanced,

				 * so re-evaluate parent and new.

	/*

	 * If stable_node was a chain and chain_prune collapsed it,

	 * stable_node has been updated to be the new regular

	 * stable_node. A collapse of the chain is indistinguishable

	 * from the case there was no chain in the stable

	 * rbtree. Otherwise stable_node is the chain and

	 * stable_node_dup is the dup to replace.

 there is no chain */

 stable_node_dup could be null if it reached the limit */

	/*

	 * If stable_node was a chain and chain_prune collapsed it,

	 * stable_node has been updated to be the new regular

	 * stable_node. A collapse of the chain is indistinguishable

	 * from the case there was no chain in the stable

	 * rbtree. Otherwise stable_node is the chain and

	 * stable_node_dup is the dup to replace.

 chain is missing so create it */

	/*

	 * Add this stable_node dup that was

	 * migrated to the stable_node chain

	 * of the current nid for this page

	 * content.

/*

 * stable_tree_insert - insert stable tree node pointing to new ksm page

 * into the stable tree.

 *

 * This function returns the stable tree node just allocated on success,

 * NULL otherwise.

			/*

			 * Either all stable_node dups were full in

			 * this stable_node chain, or this chain was

			 * empty and should be rb_erased.

 rb_erase just run */

			/*

			 * Take any of the stable_node dups page of

			 * this stable_node chain to let the tree walk

			 * continue. All KSM pages belonging to the

			 * stable_node dups in a stable_node chain

			 * have the same content and they're

			 * write protected at all times. Any will work

			 * fine to continue the walk.

			/*

			 * If we walked over a stale stable_node,

			 * get_ksm_page() will call rb_erase() and it

			 * may rebalance the tree from under us. So

			 * restart the search from scratch. Returning

			 * NULL would be safe too, but we'd generate

			 * false negative insertions just because some

			 * stable_node was stale.

 chain is missing so create it */

/*

 * unstable_tree_search_insert - search for identical page,

 * else insert rmap_item into the unstable tree.

 *

 * This function searches for a page in the unstable tree identical to the

 * page currently being scanned; and if no identical page is found in the

 * tree, we insert rmap_item as a new object into the unstable tree.

 *

 * This function returns pointer to rmap_item found to be identical

 * to the currently scanned page, NULL otherwise.

 *

 * This function does both searching and inserting, because they share

 * the same walking algorithm in an rbtree.

		/*

		 * Don't substitute a ksm page for a forked page.

			/*

			 * If tree_page has been migrated to another NUMA node,

			 * it will be flushed out and put in the right unstable

			 * tree next time: only merge with it when across_nodes.

/*

 * stable_tree_append - add another rmap_item to the linked list of

 * rmap_items hanging off a given node of the stable tree, all sharing

 * the same ksm page.

	/*

	 * rmap won't find this mapping if we don't insert the

	 * rmap_item in the right stable_node

	 * duplicate. page_migration could break later if rmap breaks,

	 * so we can as well crash here. We really need to check for

	 * rmap_hlist_len == STABLE_NODE_CHAIN, but we can as well check

	 * for other negative values as an underflow if detected here

	 * for the first time (and not when decreasing rmap_hlist_len)

	 * would be sign of memory corruption in the stable_node.

 possibly non fatal but unexpected overflow, only warn */

/*

 * cmp_and_merge_page - first see if page can be merged into the stable tree;

 * if not, compare checksum to previous and if it's the same, see if page can

 * be inserted into the unstable tree, or merged with a page already there and

 * both transferred to the stable tree.

 *

 * @page: the page that we are searching identical page to.

 * @rmap_item: the reverse mapping into the virtual address of this page

		/*

		 * If it's a KSM fork, allow it to go over the sharing limit

		 * without warnings.

 We first start with searching the page inside the stable tree */

			/*

			 * The page was successfully merged:

			 * add its rmap_item to the stable tree.

	/*

	 * If the hash value of the page has changed from the last time

	 * we calculated it, this page is changing frequently: therefore we

	 * don't want to insert it in the unstable tree, and we don't want

	 * to waste our time searching for something identical to it there.

	/*

	 * Same checksum as an empty page. We attempt to merge it with the

	 * appropriate zero page if the user enabled this via sysfs.

			/*

			 * If the vma is out of date, we do not need to

			 * continue.

		/*

		 * In case of failure, the page was not really empty, so we

		 * need to continue. Otherwise we're done.

		/*

		 * If both pages we tried to merge belong to the same compound

		 * page, then we actually ended up increasing the reference

		 * count of the same compound page twice, and split_huge_page

		 * failed.

		 * Here we set a flag if that happened, and we use it later to

		 * try split_huge_page again. Since we call put_page right

		 * afterwards, the reference count will be correct and

		 * split_huge_page should succeed.

			/*

			 * The pages were successfully merged: insert new

			 * node in the stable tree and add both rmap_items.

			/*

			 * If we fail to insert the page into the stable tree,

			 * we will have 2 virtual addresses that are pointing

			 * to a ksm page left outside the stable tree,

			 * in which case we need to break_cow on both.

			/*

			 * We are here if we tried to merge two pages and

			 * failed because they both belonged to the same

			 * compound page. We will split the page now, but no

			 * merging will take place.

			 * We do not want to add the cost of a full lock; if

			 * the page is locked, it is better to skip it and

			 * perhaps try again later.

 It has already been zeroed */

		/*

		 * A number of pages can hang around indefinitely on per-cpu

		 * pagevecs, raised page count preventing write_protect_page

		 * from merging them.  Though it doesn't really matter much,

		 * it is puzzling to see some stuck in pages_volatile until

		 * other activity jostles them out, and they also prevented

		 * LTP's KSM test from succeeding deterministically; so drain

		 * them here (here rather than on entry to ksm_do_scan(),

		 * so we don't IPI too often when pages_to_scan is set low).

		/*

		 * Whereas stale stable_nodes on the stable_tree itself

		 * get pruned in the regular course of stable_tree_search(),

		 * those moved out to the migrate_nodes list can accumulate:

		 * so prune them once before each full scan.

		/*

		 * Although we tested list_empty() above, a racing __ksm_exit

		 * of the last mm on the list may have removed it since then.

	/*

	 * Nuke all the rmap_items that are above this current rmap:

	 * because there were no VM_MERGEABLE vmas with such addresses.

		/*

		 * We've completed a full scan of all vmas, holding mmap_lock

		 * throughout, and found no VM_MERGEABLE: so do the same as

		 * __ksm_exit does to remove this mm from all our lists now.

		 * This applies either when cleaning up after __ksm_exit

		 * (but beware: we can reach here even before __ksm_exit),

		 * or when all VM_MERGEABLE areas have been unmapped (and

		 * mmap_lock then protects against race with MADV_MERGEABLE).

		/*

		 * mmap_read_unlock(mm) first because after

		 * spin_unlock(&ksm_mmlist_lock) run, the "mm" may

		 * already have been freed under us by __ksm_exit()

		 * because the "mm_slot" is still hashed and

		 * ksm_scan.mm_slot doesn't point to it anymore.

 Repeat until we've completed scanning the whole list */

/**

 * ksm_do_scan  - the ksm scanner main worker function.

 * @scan_npages:  number of pages we want to scan before we return.

		/*

		 * Be somewhat over-protective for now!

 just ignore the advice */

 just ignore the advice */

 Check ksm_run too?  Would need tighter locking */

	/*

	 * When KSM_RUN_MERGE (or KSM_RUN_STOP),

	 * insert just behind the scanning cursor, to let the area settle

	 * down a little; when fork is followed by immediate exec, we don't

	 * want ksmd to waste time setting up and tearing down an rmap_list.

	 *

	 * But when KSM_RUN_UNMERGE, it's important to insert ahead of its

	 * scanning cursor, otherwise KSM pages in newly forked mms will be

	 * missed: then we might as well insert at the end of the list.

	/*

	 * This process is exiting: if it's straightforward (as is the

	 * case when ksmd was never running), free mm_slot immediately.

	 * But if it's at the cursor or has rmap_items linked to it, use

	 * mmap_lock to synchronize with any break_cows before pagetables

	 * are freed, and leave the mm_slot on the list for ksmd to free.

	 * Beware: ksm may already have noticed it exiting and freed the slot.

 no need to copy it */

 no need to copy it */

 still no need to copy it */

 let do_swap_page report the error */

	/*

	 * Rely on the page lock to protect against concurrent modifications

	 * to that page's node of the stable tree.

 Ignore the stable/unstable/sqnr flags */

			/*

			 * Initially we examine only the vma which covers this

			 * rmap_item; but later, if there is still work to do,

			 * we examine covering vmas in other mms: in case they

			 * were forked from the original since ksmd passed.

		/*

		 * newfolio->mapping was set in advance; now we need smp_wmb()

		 * to make sure that the new stable_node->kpfn is visible

		 * to get_ksm_page() before it can see that folio->mapping

		 * has gone stale (or that folio_test_swapcache has been cleared).

 CONFIG_MIGRATION */

		/*

		 * Don't get_ksm_page, page has already gone:

		 * which is why we keep kpfn instead of page*

 notify caller that tree was rebalanced */

		/*

		 * Prevent ksm_do_scan(), unmerge_and_remove_all_rmap_items()

		 * and remove_all_stable_nodes() while memory is going offline:

		 * it is unsafe for them to touch the stable tree at this time.

		 * But unmerge_ksm_pages(), rmap lookups and other entry points

		 * which do not need the ksm_thread_mutex are all safe.

		/*

		 * Most of the work is done by page migration; but there might

		 * be a few stable_nodes left over, still pointing to struct

		 * pages which have been offlined: prune those from the tree,

		 * otherwise get_ksm_page() might later try to access a

		 * non-existent struct page.

 wake_up_bit advises this */

 CONFIG_MEMORY_HOTREMOVE */

/*

 * This all compiles without CONFIG_SYSFS, but is a waste of space.

	/*

	 * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.

	 * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,

	 * breaking COW to free the pages_shared (but leaves mm_slots

	 * on the list for when ksmd may be set running again).

			/*

			 * This is the first time that we switch away from the

			 * default of merging across nodes: must now allocate

			 * a buffer to hold as many roots as may be needed.

			 * Allocate stable and unstable together:

			 * MAXSMP NODES_SHIFT 10 will use 16kB.

 Let us assume that RB_ROOT is NULL is zero */

 Stable tree is empty but not the unstable */

	/*

	 * When a KSM page is created it is shared by 2 mappings. This

	 * being a signed comparison, it implicitly verifies it's not

	 * negative.

	/*

	 * It was not worth any locking to calculate that statistic,

	 * but it might therefore sometimes be negative: conceal that.

 CONFIG_SYSFS */

 The correct value depends on page size and endianness */

 Default to false for backwards compatibility */

 no way for user to start it */

 CONFIG_SYSFS */

 There is no significance to this priority 100 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/percpu-debug.c

 *

 * Copyright (C) 2017		Facebook Inc.

 * Copyright (C) 2017		Dennis Zhou <dennis@kernel.org>

 *

 * Prints statistics about the percpu allocator and backing chunks.

/*

 * Iterates over all chunks to find the max nr_alloc entries.

/*

 * Prints out chunk state. Fragmentation is considered between

 * the beginning of the chunk to the last allocation.

 *

 * All statistics are in bytes unless stated otherwise.

 statistics */

	/*

	 * find_last_bit returns the start value if nothing found.

	 * Therefore, we must determine if it is a failure of find_last_bit

	 * and set the appropriate value.

	/*

	 * If a bit is set in the allocation map, the bound_map identifies

	 * where the allocation ends.  If the allocation is not set, the

	 * bound_map does not identify free areas as it is only kept accurate

	 * on allocation, not free.

	 *

	 * Positive values are allocations and negative values are free

	 * fragments.

	/*

	 * The negative values are free fragments and thus sorting gives the

	 * free fragments at the beginning in largest first order.

 iterate through the unallocated fragments */

 there can be at most this many free and allocated fragments */

 if the buffer allocated earlier is too small */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm/interval_tree.c - interval tree for mapping->i_mmap

 *

 * Copyright (C) 2012, Michel Lespinasse <walken@google.com>

 empty */, vma_interval_tree)

 Insert node immediately after prev in the interval tree */

 SPDX-License-Identifier: GPL-2.0

 The first entry has to be 0 to leave memtest with zeroed memory */

 yeah ;-) */

 default is disabled */

	/*

	 * Zero out any user-supplied page index that is out of range. Remember:

	 * .which_pages[] contains a 1-based set of page indices.

 Decode from 1-based, to 0-based

 Shifting the meaning of nr_pages: now it is actual number pinned: */

	/*

	 * Take an un-benchmark-timed moment to verify DMA pinned

	 * state: print a warning if any non-dma-pinned pages are found:

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/mm/mempool.c

 *

 *  memory buffer pool support. Such pools are mostly used

 *  for guaranteed, deadlock-free memory allocations during

 *  extreme VM load.

 *

 *  started by Ingo Molnar, Copyright (C) 2001

 *  debugging by David Rientjes, Copyright (C) 2015

 Mempools backed by slab allocator */

 Mempools backed by page allocator */

 Mempools backed by slab allocator */

 Mempools backed by page allocator */

 CONFIG_DEBUG_SLAB || CONFIG_SLUB_DEBUG_ON */

 CONFIG_DEBUG_SLAB || CONFIG_SLUB_DEBUG_ON */

/**

 * mempool_exit - exit a mempool initialized with mempool_init()

 * @pool:      pointer to the memory pool which was initialized with

 *             mempool_init().

 *

 * Free all reserved elements in @pool and @pool itself.  This function

 * only sleeps if the free_fn() function sleeps.

 *

 * May be called on a zeroed but uninitialized mempool (i.e. allocated with

 * kzalloc()).

/**

 * mempool_destroy - deallocate a memory pool

 * @pool:      pointer to the memory pool which was allocated via

 *             mempool_create().

 *

 * Free all reserved elements in @pool and @pool itself.  This function

 * only sleeps if the free_fn() function sleeps.

	/*

	 * First pre-allocate the guaranteed number of buffers.

/**

 * mempool_init - initialize a memory pool

 * @pool:      pointer to the memory pool that should be initialized

 * @min_nr:    the minimum number of elements guaranteed to be

 *             allocated for this pool.

 * @alloc_fn:  user-defined element-allocation function.

 * @free_fn:   user-defined element-freeing function.

 * @pool_data: optional private data available to the user-defined functions.

 *

 * Like mempool_create(), but initializes the pool in (i.e. embedded in another

 * structure).

 *

 * Return: %0 on success, negative error code otherwise.

/**

 * mempool_create - create a memory pool

 * @min_nr:    the minimum number of elements guaranteed to be

 *             allocated for this pool.

 * @alloc_fn:  user-defined element-allocation function.

 * @free_fn:   user-defined element-freeing function.

 * @pool_data: optional private data available to the user-defined functions.

 *

 * this function creates and allocates a guaranteed size, preallocated

 * memory pool. The pool can be used from the mempool_alloc() and mempool_free()

 * functions. This function might sleep. Both the alloc_fn() and the free_fn()

 * functions might sleep - as long as the mempool_alloc() function is not called

 * from IRQ contexts.

 *

 * Return: pointer to the created memory pool object or %NULL on error.

/**

 * mempool_resize - resize an existing memory pool

 * @pool:       pointer to the memory pool which was allocated via

 *              mempool_create().

 * @new_min_nr: the new minimum number of elements guaranteed to be

 *              allocated for this pool.

 *

 * This function shrinks/grows the pool. In the case of growing,

 * it cannot be guaranteed that the pool will be grown to the new

 * size immediately, but new mempool_free() calls will refill it.

 * This function may sleep.

 *

 * Note, the caller must guarantee that no mempool_destroy is called

 * while this function is running. mempool_alloc() & mempool_free()

 * might be called (eg. from IRQ contexts) while this function executes.

 *

 * Return: %0 on success, negative error code otherwise.

 Grow the pool */

 Raced, other resize will do our work */

 Raced */

/**

 * mempool_alloc - allocate an element from a specific memory pool

 * @pool:      pointer to the memory pool which was allocated via

 *             mempool_create().

 * @gfp_mask:  the usual allocation bitmask.

 *

 * this function only sleeps if the alloc_fn() function sleeps or

 * returns NULL. Note that due to preallocation, this function

 * *never* fails when called from process contexts. (it might

 * fail if called from an IRQ context.)

 * Note: using __GFP_ZERO is not supported.

 *

 * Return: pointer to the allocated element or %NULL on error.

 don't allocate emergency reserves */

 don't loop in __alloc_pages */

 failures are OK */

 paired with rmb in mempool_free(), read comment there */

		/*

		 * Update the allocation stack trace as this is more useful

		 * for debugging.

	/*

	 * We use gfp mask w/o direct reclaim or IO for the first round.  If

	 * alloc failed with that and @pool was empty, retry immediately.

 We must not sleep if !__GFP_DIRECT_RECLAIM */

 Let's wait for someone else to return an element to @pool */

	/*

	 * FIXME: this should be io_schedule().  The timeout is there as a

	 * workaround for some DM problems in 2.6.18.

/**

 * mempool_free - return an element to the pool.

 * @element:   pool element pointer.

 * @pool:      pointer to the memory pool which was allocated via

 *             mempool_create().

 *

 * this function only sleeps if the free_fn() function sleeps.

	/*

	 * Paired with the wmb in mempool_alloc().  The preceding read is

	 * for @element and the following @pool->curr_nr.  This ensures

	 * that the visible value of @pool->curr_nr is from after the

	 * allocation of @element.  This is necessary for fringe cases

	 * where @element was passed to this task without going through

	 * barriers.

	 *

	 * For example, assume @p is %NULL at the beginning and one task

	 * performs "p = mempool_alloc(...);" while another task is doing

	 * "while (!p) cpu_relax(); mempool_free(p, ...);".  This function

	 * may end up using curr_nr value which is from before allocation

	 * of @p without the following rmb.

	/*

	 * For correctness, we need a test which is guaranteed to trigger

	 * if curr_nr + #allocated == min_nr.  Testing curr_nr < min_nr

	 * without locking achieves that and refilling as soon as possible

	 * is desirable.

	 *

	 * Because curr_nr visible here is always a value after the

	 * allocation of @element, any task which decremented curr_nr below

	 * min_nr is guaranteed to see curr_nr < min_nr unless curr_nr gets

	 * incremented to min_nr afterwards.  If curr_nr gets incremented

	 * to min_nr after the allocation of @element, the elements

	 * allocated after that are subject to the same guarantee.

	 *

	 * Waiters happen iff curr_nr is 0 and the above guarantee also

	 * ensures that there will be frees which return elements to the

	 * pool waking up the waiters.

/*

 * A commonly used alloc and free fn.

/*

 * A commonly used alloc and free fn that kmalloc/kfrees the amount of memory

 * specified by pool_data

/*

 * A simple mempool-backed page allocator that allocates pages

 * of the order specified by pool_data.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/swapfile.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *  Swap reorganised 29.12.95, Stephen Tweedie

/*

 * Some modules use swappable objects and may try to swap them out under

 * memory pressure (via the shrinker). Before doing so, they may wish to

 * check to see if any swap space is available.

 protected with swap_lock. reading in vm_swap_full() doesn't need lock */

/*

 * all active swap_info_structs

 * protected with swap_lock, and ordered by priority.

/*

 * all available (active, not full) swap_info_structs

 * protected with swap_avail_lock, ordered by priority.

 * This is used by get_swap_page() instead of swap_active_head

 * because swap_active_head includes all swap_info_structs,

 * but get_swap_page() doesn't need to look at full ones.

 * This uses its own lock instead of swap_lock because when a

 * swap_info_struct changes between not-full/full, it needs to

 * add/remove itself to/from this list, but the swap_info_struct->lock

 * is held and the locking order requires swap_lock to be taken

 * before any swap_info_struct->lock.

 Activity counter to indicate that a swapon or swapoff has occurred */

 rcu_dereference() */

 may include COUNT_CONTINUED flag */

 Reclaim the swap entry anyway if possible */

/*

 * Reclaim the swap entry if there are no more mappings of the

 * corresponding page

 Reclaim the swap entry if swap is getting full*/

 returns 1 if swap entry is freed */

	/*

	 * When this function is called from scan_swap_map_slots() and it's

	 * called by vmscan.c at reclaiming pages. So, we hold a lock on a page,

	 * here. We have to use trylock for avoiding deadlock. This is a special

	 * case and you should use try_to_free_swap() with explicit lock_page()

	 * in usual operations.

/*

 * swapon tell device that all the old swap contents can be discarded,

 * to allow the swap device to optimize its wear-levelling.

 Do not discard the swap header page! */

 That will often be -EOPNOTSUPP */

 It *must* be present */

/*

 * swap allocation tell device that a cluster of swap can now be discarded,

 * to allow the swap device to optimize its wear-levelling.

/*

 * Define swap_entry_size() as constant to let compiler to optimize

 * out some code if !CONFIG_THP_SWAP

/*

 * Determine the locking method in use for this device.  Return

 * swap_cluster_info if SSD-style cluster-based locking is in place.

 Try to use fine-grained SSD-style locking if available: */

 Otherwise, fall back to traditional, coarse locking: */

		/*

		 * Nested cluster lock, but both cluster locks are

		 * only acquired when we held swap_info_struct->lock

 Add a cluster to discard list and schedule it to do discard */

	/*

	 * If scan_swap_map_slots() can't find a free cluster, it will check

	 * si->swap_map directly. To make sure the discarding cluster isn't

	 * taken by scan_swap_map_slots(), mark the swap entries bad (occupied).

	 * It will be cleared after discard

/*

 * Doing discard actually. After a cluster discard is finished, the cluster

 * will be added to free cluster list. caller should hold si->lock.

	/*

	 * If the swap is discardable, prepare discard the cluster

	 * instead of free it immediately. The cluster will be freed

	 * after discard.

/*

 * The cluster corresponding to page_nr will be used. The cluster will be

 * removed from free cluster list and its usage counter will be increased.

/*

 * The cluster corresponding to page_nr decreases one usage. If the usage

 * counter becomes 0, which means no page in the cluster is in using, we can

 * optionally discard the cluster and add it to free cluster list.

/*

 * It's possible scan_swap_map_slots() uses a free cluster in the middle of free

 * cluster list. Avoiding such abuse to avoid list corruption.

/*

 * Try to get a swap entry from current cpu's swap entry pool (a cluster). This

 * might involve allocating a new cluster for current CPU too.

			/*

			 * we don't have free cluster but have some clusters in

			 * discarding, do discard now and reclaim them, then

			 * reread cluster_next_cpu since we dropped si->lock

	/*

	 * Other CPUs can use our cluster if they can't find a free cluster,

	 * check if there is still free entry in the cluster

	/*

	 * Cross the swap address space size aligned trunk, choose

	 * another trunk randomly to avoid lock contention on swap

	 * address space if possible.

 No free swap slots available */

	/*

	 * We try to cluster swap pages by allocating them sequentially

	 * in swap.  Once we've allocated SWAPFILE_CLUSTER pages this

	 * way, however, we resort to first-free allocation, starting

	 * a new cluster.  This prevents us from scattering swap pages

	 * all over the entire swap partition, so that we reduce

	 * overall disk seek times between swap pages.  -- sct

	 * But we do now try to find an empty cluster.  -Andrea

	 * And we let swap pages go all over an SSD partition.  Hugh

	/*

	 * Use percpu scan base for SSD to reduce lock contention on

	 * cluster and swap cache.  For HDD, sequential access is more

	 * important.

 SSD algorithm */

		/*

		 * If seek is expensive, start searching for new cluster from

		 * start of partition, to minimize the span of allocated swap.

		 * If seek is cheap, that is the SWP_SOLIDSTATE si->cluster_info

		 * case, just handled by scan_swap_map_try_ssd_cluster() above.

 Locate the first empty (unaligned) cluster */

 take a break if we already got some slots */

 reuse swap entry of cache-only swap if not busy. */

 entry was freed successfully, try to use this again */

 check next one */

 got enough slots or reach max slots? */

 search for next available slot */

 time to take a break? */

 try to get more slots in cluster */

 non-ssd case, still more slots in cluster? */

	/*

	 * Even if there's no free clusters available (fragmented),

	 * try to scan a little more quickly with lock held unless we

	 * have scanned too many slots already.

	/*

	 * Should not even be attempting cluster allocations when huge

	 * page swap is disabled.  Warn and fail the allocation.

 Only single cluster request supported */

 requeue si to after same-priority siblings */

		/*

		 * if we got here, it's likely that si was almost full before,

		 * and since scan_swap_map_slots() can drop the si->lock,

		 * multiple callers probably all tried to get a page from the

		 * same si and it filled up before we could get one; or, the si

		 * filled up between us dropping swap_avail_lock and taking

		 * si->lock. Since we dropped the swap_avail_lock, the

		 * swap_avail_head list may have been modified; so if next is

		 * still in the swap_avail_head list then try it, otherwise

		 * start over if we have not gotten any slots.

		/*

		 * Or we could insist on shmem.c using a special

		 * swap_shmem_free() and free_shmem_swap_and_cache()...

/*

 * Check whether swap entry is valid in the swap device.  If so,

 * return pointer to swap_info_struct, and keep the swap entry valid

 * via preventing the swap device from being swapoff, until

 * put_swap_device() is called.  Otherwise return NULL.

 *

 * Notice that swapoff or swapoff+swapon can still happen before the

 * percpu_ref_tryget_live() in get_swap_device() or after the

 * percpu_ref_put() in put_swap_device() if there isn't any other way

 * to prevent swapoff, such as page lock, page table lock, etc.  The

 * caller must be prepared for that.  For example, the following

 * situation is possible.

 *

 *   CPU1				CPU2

 *   do_swap_page()

 *     ...				swapoff+swapon

 *     __read_swap_cache_async()

 *       swapcache_prepare()

 *         __swap_duplicate()

 *           // check swap_map

 *     // verify PTE not changed

 *

 * In __swap_duplicate(), the swap_map need to be checked before

 * changing partly because the specified swap entry may be for another

 * swap device which has been swapoff.  And in do_swap_page(), after

 * the page is read from the swap device, the PTE is verified not

 * changed with the page table locked to check whether the swap device

 * has been swapoff or swapoff+swapon.

	/*

	 * Guarantee the si->users are checked before accessing other

	 * fields of swap_info_struct.

	 *

	 * Paired with the spin_unlock() after setup_swap_info() in

	 * enable_swap_info().

/*

 * Caller has made sure that the swap device corresponding to entry

 * is still around or has not been recycled.

/*

 * Called after dropping swapcache to decrease refcnt to swap entries.

	/*

	 * Sort swap entries by swap device, so each lock is only taken once.

	 * nr_swapfiles isn't absolutely correct, but the overhead of sort() is

	 * so low that it isn't necessary to optimize further.

/*

 * How many references to page are currently swapped out?

 * This does not give an exact answer when swap count is continued,

 * but does include the high COUNT_CONTINUED flag to allow for that.

/*

 * How many references to @entry are currently swapped out?

 * This does not give an exact answer when swap count is continued,

 * but does include the high COUNT_CONTINUED flag to allow for that.

/*

 * How many references to @entry are currently swapped out?

 * This considers COUNT_CONTINUED so it returns exact answer.

 hugetlbfs shouldn't call it */

/*

 * We can write to an anon page without COW if there are no other references

 * to it.  And as a side-effect, free up its swap: because the old content

 * on disk will never be read, and seeking back there to write new content

 * later would only waste time away from clustering.

 *

 * NOTE: total_map_swapcount should not be relied upon by the caller if

 * reuse_swap_page() returns false, but it may be always overwritten

 * (see the other implementation for CONFIG_SWAP=n).

 The remaining swap count will be freed soon */

/*

 * If swap is getting full, or if there are no more mappings of this page,

 * then try_to_free_swap is called to free its swap space.

	/*

	 * Once hibernation has begun to create its image of memory,

	 * there's a danger that one of the calls to try_to_free_swap()

	 * - most probably a call from __try_to_reclaim_swap() while

	 * hibernation is allocating its own swap pages for the image,

	 * but conceivably even a call from memory reclaim - will free

	 * the swap from a page which has already been recorded in the

	 * image as a clean swapcache page, and then reuse its swap for

	 * another page of the image.  On waking from hibernation, the

	 * original page might be freed under memory pressure, then

	 * later read back in from swap, now with the wrong data.

	 *

	 * Hibernation suspends storage while it is writing the image

	 * to disk so check that here.

/*

 * Free the swap entry like above, but also try to

 * free the page cache entry if it is the last user.

 This is called for allocating swap entry, not cache */

/*

 * Find the swap type that corresponds to given device (if any).

 *

 * @offset - number of the PAGE_SIZE-sized block of the device, starting

 * from 0, in which the swap header is expected to be located.

 *

 * This is needed for the suspend to disk (aka swsusp).

/*

 * Get the (PAGE_SIZE) block corresponding to given offset on the swapdev

 * corresponding to given index in swap_info (swap type).

/*

 * Return either the total number of swap pages of given type, or the number

 * of free pages of that type (depending on @free)

 *

 * This is needed for software suspend

 CONFIG_HIBERNATION */

/*

 * No need to decide whether this PTE shares the swap entry with others,

 * just let do_wp_page work it out if a write is requested later - to

 * force COW, vm_page_prot omits write permission from any private vma.

 ksm created a completely new copy */

/*

 * Scan swap_map (or frontswap_map if frontswap parameter is true)

 * from current position to next entry still in use. Return 0

 * if there are no inuse entries after prev till end of the map.

	/*

	 * No need for swap_lock here: we're just looking

	 * for whether an entry is in use, not modifying it; false

	 * hits are okay, and sys_swapoff() has already prevented new

	 * allocations from this area (while holding swap_lock).

/*

 * If the boolean frontswap is true, only unuse pages_to_unuse pages;

 * pages_to_unuse==0 means all pages; ignored if frontswap is false

		/*

		 * Make sure that we aren't completely killing

		 * interactive performance.

		/*

		 * It is conceivable that a racing task removed this page from

		 * swap cache just before we acquired the page lock. The page

		 * might even be back in swap cache on another swap area. But

		 * that is okay, try_to_free_swap() only removes stale pages.

		/*

		 * For frontswap, we just need to unuse pages_to_unuse, if

		 * it was specified. Need not check frontswap again here as

		 * we already zeroed out pages_to_unuse if not frontswap.

	/*

	 * Lets check again to see if there are still swap entries in the map.

	 * If yes, we would need to do retry the unuse logic again.

	 * Under global memory pressure, swap entries can be reinserted back

	 * into process space after the mmlist loop above passes over them.

	 *

	 * Limit the number of retries? No: when mmget_not_zero() above fails,

	 * that mm is likely to be freeing swap from exit_mmap(), which proceeds

	 * at its own independent pace; and even shmem_writepage() could have

	 * been preempted after get_swap_page(), temporarily hiding that swap.

	 * It's easy and robust (though cpu-intensive) just to keep retrying.

/*

 * After a successful try_to_unuse, if no swap is now in use, we know

 * we can empty the mmlist.  swap_lock must be held on entry and exit.

 * Note that mmlist_lock nests inside swap_lock, and an mm must be

 * added to the mmlist just after page_duplicate - before would be racy.

/*

 * Free all of a swapdev's extent information

/*

 * Add a block range (and the corresponding page range) into this swapdev's

 * extent tree.

 *

 * This function rather assumes that it is called in ascending page order.

	/*

	 * place the new node at the right most since the

	 * function is called in ascending page order.

 Merge it */

 No merge, insert a new extent. */

/*

 * A `swap extent' is a simple thing which maps a contiguous range of pages

 * onto a contiguous range of disk blocks.  An ordered list of swap extents

 * is built at swapon time and is then used at swap_writepage/swap_readpage

 * time for locating where on disk a page belongs.

 *

 * If the swapfile is an S_ISBLK block device, a single extent is installed.

 * This is done so that the main operating code can treat S_ISBLK and S_ISREG

 * swap files identically.

 *

 * Whether the swapdev is an S_ISREG file or an S_ISBLK blockdev, the swap

 * extent list operates in PAGE_SIZE disk blocks.  Both S_ISREG and S_ISBLK

 * swapfiles are handled *identically* after swapon time.

 *

 * For S_ISREG swapfiles, setup_swap_extents() will walk all the file's blocks

 * and will parse them into an ordered extent list, in PAGE_SIZE chunks.  If

 * some stray blocks are found which do not fall within the PAGE_SIZE alignment

 * requirements, they are simply tossed out - we will never use those blocks

 * for swapping.

 *

 * For all swap devices we set S_SWAPFILE across the life of the swapon.  This

 * prevents users from writing to the swap device, which will corrupt memory.

 *

 * The amount of disk space which a single swap extent represents varies.

 * Typically it is in the 1-4 megabyte range.  So we can have hundreds of

 * extents in the list.  To avoid much list walking, we cache the previous

 * search location in `curr_swap_extent', and start new searches from there.

 * This is extremely effective.  The average number of iterations in

 * map_swap_page() has been measured at about 0.3 per page.  - akpm.

	/*

	 * the plist prio is negated because plist ordering is

	 * low-to-high, while swap ordering is high-to-low

	/*

	 * both lists are plists, and thus priority ordered.

	 * swap_active_head needs to be priority ordered for swapoff(),

	 * which on removal of any swap_info_struct with an auto-assigned

	 * (i.e. negative) priority increments the auto-assigned priority

	 * of any lower-priority swap_info_structs.

	 * swap_avail_head needs to be priority ordered for get_swap_page(),

	 * which allocates swap pages from the highest available priority

	 * swap_info_struct.

	/*

	 * Finished initializing swap device, now it's safe to reference it.

 force unuse all pages */

 re-insert swap space back into swap_list */

	/*

	 * Wait for swap operations protected by get/put_swap_device()

	 * to complete.

	 *

	 * We need synchronize_rcu() here to protect the accessing to

	 * the swap cache data structure.

 wait for anyone still in scan_swap_map_slots */

 cuts scans short */

 Destroy swap account information */

	/*

	 * Clear the SWP_USED flag after all resources are freed so that swapon

	 * can reuse this swap_info in alloc_swap_info() safely.  It is ok to

	 * not hold p->lock after we cleared its SWP_WRITEOK.

 iterator */

 CONFIG_PROC_FS */

		/*

		 * Publish the swap_info_struct after initializing it.

		 * Note that kvzalloc() above zeroes all its fields.

 rcu_assign_pointer() */

		/*

		 * Do not memset this entry: a racing procfs swap_next()

		 * would be relying on p->type to remain valid.

		/*

		 * Zoned block devices contain zones that have a sequential

		 * write only restriction.  Hence zoned block devices are not

		 * suitable for swapping.  Disallow them here.

/*

 * Find out how many pages are allowed for a single swap device. There

 * are two limiting factors:

 * 1) the number of bits for the swap offset in the swp_entry_t type, and

 * 2) the number of bits in the swap pte, as defined by the different

 * architectures.

 *

 * In order to find the largest possible bit mask, a swap entry with

 * swap type 0 and swap offset ~0UL is created, encoded to a swap pte,

 * decoded to a swp_entry_t again, and finally the swap offset is

 * extracted.

 *

 * This will mask all the bits from the initial ~0UL mask that can't

 * be encoded in either the swp_entry_t or the architecture definition

 * of a swap pte.

 Can be overridden by an architecture for additional checks. */

 swap partition endianness hack... */

 Check the swap header's sub-version */

 p->max is an unsigned int: don't overflow it */

 omit header page */

			/*

			 * Haven't marked the cluster free yet, no list

			 * operation involved

 Haven't marked the cluster free yet, no list operation involved */

		/*

		 * Not mark the cluster free yet, no list

		 * operation involved

	/*

	 * Reduce false cache line sharing between cluster_info and

	 * sharing same address space.

/*

 * Helper to sys_swapon determining if a given swap

 * backing device queue supports DISCARD operations.

	/*

	 * Read the swap header.

 OK, set up the swap map and apply the bad block list */

		/*

		 * select a random position to start with to help wear leveling

		 * SSD

 frontswap enabled? set up bit-per-page map for frontswap */

		/*

		 * When discard is enabled for swap with no particular

		 * policy flagged, we set all swap discard flags here in

		 * order to sustain backward compatibility with older

		 * swapon(8) releases.

		/*

		 * By flagging sys_swapon, a sysadmin can tell us to

		 * either do single-time area discards only, or to just

		 * perform discards for released swap page-clusters.

		 * Now it's time to adjust the p->flags accordingly.

 issue a swapon-time discard if it's still required */

	/*

	 * Flush any pending IO and dirty mappings before we start using this

	 * swap device.

/*

 * Verify that a swap entry is valid and increment its swap map count.

 *

 * Returns error code in following case.

 * - success -> 0

 * - swp_entry is invalid -> EINVAL

 * - swp_entry is migration entry -> EINVAL

 * - swap-cache reference is requested but there is already one. -> EEXIST

 * - swap-cache reference is requested but the entry is not used. -> ENOENT

 * - swap-mapped reference requested but needs continued swap count. -> ENOMEM

	/*

	 * swapin_readahead() doesn't check if a swap entry is valid, so the

	 * swap entry could be SWAP_MAP_BAD. Check here with lock held.

 set SWAP_HAS_CACHE if there is no cache and entry is used */

 someone else added cache */

 no users remaining */

 unused swap entry */

/*

 * Help swapoff by noting that swap entry belongs to shmem/tmpfs

 * (in which case its reference count is never incremented).

/*

 * Increase reference count of swap entry by 1.

 * Returns 0 for success, or -ENOMEM if a swap_count_continuation is required

 * but could not be atomically allocated.  Returns 0, just as if it succeeded,

 * if __swap_duplicate() fails for another reason (-EINVAL or -ENOENT), which

 * might occur if a page table entry has got corrupted.

/*

 * @entry: swap entry for which we allocate swap cache.

 *

 * Called when allocating swap cache for existing swap entry,

 * This can return error codes. Returns 0 at success.

 * -EEXIST means there is a swap cache.

 * Note: return code is different from swap_duplicate().

/*

 * out-of-line methods to avoid include hell.

/*

 * add_swap_count_continuation - called when a swap count is duplicated

 * beyond SWAP_MAP_MAX, it allocates a new page and links that to the entry's

 * page of the original vmalloc'ed swap_map, to hold the continuation count

 * (for that entry and for its neighbouring PAGE_SIZE swap entries).  Called

 * again when count is duplicated beyond SWAP_MAP_MAX * SWAP_CONT_MAX, etc.

 *

 * These continuation pages are seldom referenced: the common paths all work

 * on the original swap_map, only referring to a continuation page when the

 * low "digit" of a count is incremented or decremented through SWAP_MAP_MAX.

 *

 * add_swap_count_continuation(, GFP_ATOMIC) can be called while holding

 * page table locks; if it fails, add_swap_count_continuation(, GFP_KERNEL)

 * can be called after dropping locks.

	/*

	 * When debugging, it's easier to use __GFP_ZERO here; but it's better

	 * for latency not to zero a page while GFP_ATOMIC and holding locks.

		/*

		 * An acceptable race has occurred since the failing

		 * __swap_duplicate(): the swap device may be swapoff

		/*

		 * The higher the swap count, the more likely it is that tasks

		 * will race to add swap count continuation: we need to avoid

		 * over-provisioning.

	/*

	 * We are fortunate that although vmalloc_to_page uses pte_offset_map,

	 * no architecture is using highmem pages for kernel page tables: so it

	 * will not corrupt the GFP_ATOMIC caller's atomic page table kmaps.

	/*

	 * Page allocation does not initialize the page's lru field,

	 * but it does always reset its private field.

		/*

		 * If the previous map said no continuation, but we've found

		 * a continuation page, free our allocation and use this one.

		/*

		 * If this continuation count now has some space in it,

		 * free our allocation and use this one.

 now it's attached, don't free it */

/*

 * swap_count_continued - when the original swap_map count is incremented

 * from SWAP_MAP_MAX, check if there is already a continuation page to carry

 * into, carry if so, or else fail until a new continuation page is allocated;

 * when the original swap_map count is decremented from 0 with continuation,

 * borrow from the continuation and report whether it still holds more.

 * Called while __swap_duplicate() or swap_entry_free() holds swap or cluster

 * lock.

 need to add count continuation */

 initial increment from swap_map */

 jump over SWAP_CONT_MAX checks */

 incrementing */

		/*

		 * Think of how you add 1 to 999

 add count continuation */

 we didn't zero the page */

 incremented */

 decrementing */

		/*

		 * Think of how you subtract 1 from 1000

/*

 * free_swap_count_continuations - swapoff free all the continuation pages

 * appended to the swap_map, after swap_map is quiesced, before vfree'ing it.

	/*

	 * We've already scheduled a throttle, avoid taking the global swap

	 * lock.

 SPDX-License-Identifier: GPL-2.0

/*

 * Manage cache of swap slots to be used for and returned from

 * swap.

 *

 * Copyright(c) 2016 Intel Corporation.

 *

 * Author: Tim Chen <tim.c.chen@linux.intel.com>

 *

 * We allocate the swap slots from the global pool and put

 * it into local per cpu caches.  This has the advantage

 * of no needing to acquire the swap_info lock every time

 * we need a new slot.

 *

 * There is also opportunity to simply return the slot

 * to local caches without needing to acquire swap_info

 * lock.  We do not reuse the returned slots directly but

 * move them back to the global pool in a batch.  This

 * allows the slots to coalesce and reduce fragmentation.

 *

 * The swap entry allocated is marked with SWAP_HAS_CACHE

 * flag in map_count that prevents it from being allocated

 * again from the global pool.

 *

 * The swap slots cache is protected by a mutex instead of

 * a spin lock as when we search for slots with scan_swap_map,

 * we can possibly sleep.

 Serialize swap slots cache enable/disable operations */

 Must not be called with cpu hot plug lock */

 serialize with cpu hotplug operations */

 if global pool of slot caches too low, deactivate cache */

	/*

	 * Do allocation outside swap_slots_cache_mutex

	 * as kvzalloc could trigger reclaim and get_swap_page,

	 * which can lock swap_slots_cache_mutex.

 cache already allocated */

	/*

	 * We initialized alloc_lock and free_lock earlier.  We use

	 * !cache->slots or !cache->slots_ret to know if it is safe to acquire

	 * the corresponding lock and use the cache.  Memory barrier below

	 * ensures the assumption.

	/*

	 * This function is called during

	 *	1) swapoff, when we have to make sure no

	 *	   left over slots are in cache when we remove

	 *	   a swap device;

	 *      2) disabling of swap slot cache, when we run low

	 *	   on swap slots when allocating memory and need

	 *	   to return swap slots to global pool.

	 *

	 * We cannot acquire cpu hot plug lock here as

	 * this function can be invoked in the cpu

	 * hot plug path:

	 * cpu_up -> lock cpu_hotplug -> cpu hotplug state callback

	 *   -> memory allocation -> direct reclaim -> get_swap_page

	 *   -> drain_swap_slots_cache

	 *

	 * Hence the loop over current online cpu below could miss cpu that

	 * is being brought online but not yet marked as online.

	 * That is okay as we do not schedule and run anything on a

	 * cpu before it has been marked online. Hence, we will not

	 * fill any swap slots in slots cache of such cpu.

	 * There are no slots on such cpu that need to be drained.

 called with swap slot cache's alloc lock held */

 Swap slots cache may be deactivated before acquiring lock */

			/*

			 * Return slots to global pool.

			 * The current swap_map value is SWAP_HAS_CACHE.

			 * Set it to 0 to indicate it is available for

			 * allocation in global pool

	/*

	 * Preemption is allowed here, because we may sleep

	 * in refill_swap_slots_cache().  But it is safe, because

	 * accesses to the per-CPU data structure are protected by the

	 * mutex cache->alloc_lock.

	 *

	 * The alloc path here does not touch cache->slots_ret

	 * so cache->free_lock is not taken.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/mm/compaction.c

 *

 * Memory compaction for the reduction of external fragmentation. Note that

 * this heavily depends upon page migration to do all the real heavy

 * lifting

 *

 * Copyright IBM Corp. 2007-2010 Mel Gorman <mel@csn.ul.ie>

/*

 * Fragmentation score check interval for proactive compaction purposes.

/*

 * Page order with-respect-to which proactive compaction

 * calculates external fragmentation, which is used as

 * the "fragmentation score" of a node/zone.

	/*

	 * Clear registered address_space val with keeping PAGE_MAPPING_MOVABLE

	 * flag so that VM can catch up released page by driver after isolation.

	 * With it, VM migration doesn't try to put it back.

 Do not skip compaction more than 64 times */

/*

 * Compaction is deferred when compaction fails to result in a page

 * allocation success. 1 << compact_defer_shift, compactions are skipped up

 * to a limit of 1 << COMPACT_MAX_DEFER_SHIFT

 Returns true if compaction should be skipped this time */

 Avoid possible overflow */

/*

 * Update defer tracking counters after successful compaction of given order,

 * which means an allocation either succeeded (alloc_success == true) or is

 * expected to succeed.

 Returns true if restarting compaction after many failures */

 Returns true if the pageblock should be scanned for pages to isolate. */

/*

 * Compound pages of >= pageblock_order should consistently be skipped until

 * released. It is always pointless to compact pages of such order (if they are

 * migratable), and the pageblocks they occupy cannot contain any free pages.

	/*

	 * If skip is already cleared do no further checking once the

	 * restart points have been set.

	/*

	 * If clearing skip for the target scanner, do not select a

	 * non-movable pageblock as the starting point.

 Ensure the start of the pageblock or zone is online and valid */

 Ensure the end of the pageblock or zone is online and valid */

	/*

	 * Only clear the hint if a sample indicates there is either a

	 * free page or an LRU page in the block. One or other condition

	 * is necessary for the block to be a migration source/target.

/*

 * This function is called to clear all cached information on pageblocks that

 * should be skipped for page isolation when the migrate and free page scanner

 * meet.

	/*

	 * Walk the zone and update pageblock skip information. Source looks

	 * for PageLRU while target looks for PageBuddy. When the scanner

	 * is found, both PageBuddy and PageLRU are checked as the pageblock

	 * is suitable as both source and target.

 Update the migrate PFN */

 Update the free PFN */

 Leave no distance if no suitable block was reset */

 Only flush if a full compaction finished recently */

/*

 * Sets the pageblock skip bit if it was clear. Note that this is a hint as

 * locks are not required for read/writers. Returns true if it was already set.

 Do no update if skip hint is being ignored */

 Set for isolation rather than compaction */

/*

 * If no pages were isolated then mark this pageblock to be skipped in the

 * future. The information is later cleared by __reset_isolation_suitable().

 Update where async and sync compaction should restart */

 CONFIG_COMPACTION */

/*

 * Compaction requires the taking of some coarse locks that are potentially

 * very heavily contended. For async compaction, trylock and record if the

 * lock is contended. The lock will still be acquired but compaction will

 * abort when the current block is finished regardless of success rate.

 * Sync compaction acquires the lock.

 *

 * Always returns true which makes it easier to track lock state in callers.

 Track if the lock is contended in async mode */

/*

 * Compaction requires the taking of some coarse locks that are potentially

 * very heavily contended. The lock should be periodically unlocked to avoid

 * having disabled IRQs for a long time, even when there is nobody waiting on

 * the lock. It might also be that allowing the IRQs will result in

 * need_resched() becoming true. If scheduling is needed, async compaction

 * aborts. Sync compaction schedules.

 * Either compaction type will also abort if a fatal signal is pending.

 * In either case if the lock was locked, it is dropped and not regained.

 *

 * Returns true if compaction should abort due to fatal signal pending, or

 *		async compaction due to need_resched()

 * Returns false when compaction can continue (sync compaction might have

 *		scheduled)

/*

 * Isolate free pages onto a private freelist. If @strict is true, will abort

 * returning 0 on any invalid PFNs or non-free pages inside of the pageblock

 * (even though it may still end up isolating some pages).

 Strict mode is for isolation, speed is secondary */

 Isolate free pages. */

		/*

		 * Periodically drop the lock (if held) regardless of its

		 * contention, to give chance to IRQs. Abort if fatal signal

		 * pending or async compaction detects need_resched()

		/*

		 * For compound pages such as THP and hugetlbfs, we can save

		 * potentially a lot of iterations if we skip them at once.

		 * The check is racy, but we can consider only valid values

		 * and the only danger is skipping too much.

		/*

		 * If we already hold the lock, we can skip some rechecking.

		 * Note that if we hold the lock now, checked_pageblock was

		 * already set in some previous iteration (or strict is true),

		 * so it is correct to skip the suitable migration target

		 * recheck as well.

 Recheck this is a buddy page under lock */

 Found a free page, will break it into order-0 pages */

 Advance to the end of split page */

	/*

	 * There is a tiny chance that we have read bogus compound_order(),

	 * so be careful to not go outside of the pageblock.

 Record how far we have got within the block */

	/*

	 * If strict isolation is requested by CMA then check that all the

	 * pages requested were isolated. If there were any failures, 0 is

	 * returned and CMA will fail.

/**

 * isolate_freepages_range() - isolate free pages.

 * @cc:        Compaction control structure.

 * @start_pfn: The first PFN to start isolating.

 * @end_pfn:   The one-past-last PFN.

 *

 * Non-free pages, invalid PFNs, or zone boundaries within the

 * [start_pfn, end_pfn) range are considered errors, cause function to

 * undo its actions and return zero.

 *

 * Otherwise, function returns one-past-the-last PFN of isolated page

 * (which may be greater then end_pfn if end fell in a middle of

 * a free page).

 Protect pfn from changing by isolate_freepages_block */

		/*

		 * pfn could pass the block_end_pfn if isolated freepage

		 * is more than pageblock order. In this case, we adjust

		 * scanning range to right one.

		/*

		 * In strict mode, isolate_freepages_block() returns 0 if

		 * there are any holes in the block (ie. invalid PFNs or

		 * non-free pages).

		/*

		 * If we managed to isolate pages, it is always (1 << n) *

		 * pageblock_nr_pages for some non-negative n.  (Max order

		 * page may span two pageblocks).

 __isolate_free_page() does not map the pages */

 Loop terminated early, cleanup. */

 We don't use freelists for anything. */

 Similar to reclaim, but different enough that they don't share logic */

/**

 * isolate_migratepages_block() - isolate all migrate-able pages within

 *				  a single pageblock

 * @cc:		Compaction control structure.

 * @low_pfn:	The first PFN to isolate

 * @end_pfn:	The one-past-the-last PFN to isolate, within same pageblock

 * @isolate_mode: Isolation mode to be used.

 *

 * Isolate all pages that can be migrated from the range specified by

 * [low_pfn, end_pfn). The range is expected to be within same pageblock.

 * Returns errno, like -EAGAIN or -EINTR in case e.g signal pending or congestion,

 * -ENOMEM in case we could not allocate a page, or 0.

 * cc->migrate_pfn will contain the next pfn to scan.

 *

 * The pages are isolated on cc->migratepages list (not required to be empty),

 * and cc->nr_migratepages is updated accordingly.

	/*

	 * Ensure that there are not too many pages isolated from the LRU

	 * list by either parallel reclaimers or compaction. If there are,

	 * delay for some time until fewer pages are isolated

 stop isolation if there are still pages not migrated */

 async migration should just abort */

 Time to isolate some pages for migration */

			/*

			 * We have isolated all migration candidates in the

			 * previous order-aligned block, and did not skip it due

			 * to failure. We should migrate the pages now and

			 * hopefully succeed compaction.

			/*

			 * We failed to isolate in the previous order-aligned

			 * block. Set the new boundary to the end of the

			 * current block. Note we can't simply increase

			 * next_skip_pfn by 1 << order, as low_pfn might have

			 * been incremented by a higher number due to skipping

			 * a compound or a high-order buddy page in the

			 * previous loop iteration.

		/*

		 * Periodically drop the lock (if held) regardless of its

		 * contention, to give chance to IRQs. Abort completely if

		 * a fatal signal is pending.

		/*

		 * Check if the pageblock has already been marked skipped.

		 * Only the aligned PFN is checked as the caller isolates

		 * COMPACT_CLUSTER_MAX at a time so the second call must

		 * not falsely conclude that the block should be skipped.

			/*

			 * Fail isolation in case isolate_or_dissolve_huge_page()

			 * reports an error. In case of -ENOMEM, abort right away.

 Do not report -EBUSY down the chain */

				/*

				 * Hugepage was successfully isolated and placed

				 * on the cc->migratepages list.

			/*

			 * Ok, the hugepage was dissolved. Now these pages are

			 * Buddy and cannot be re-allocated because they are

			 * isolated. Fall-through as the check below handles

			 * Buddy pages.

		/*

		 * Skip if free. We read page order here without zone lock

		 * which is generally unsafe, but the race window is small and

		 * the worst thing that can happen is that we skip some

		 * potential isolation targets.

			/*

			 * Without lock, we cannot be sure that what we got is

			 * a valid page order. Consider only values in the

			 * valid order range to prevent low_pfn overflow.

		/*

		 * Regardless of being on LRU, compound pages such as THP and

		 * hugetlbfs are not to be compacted unless we are attempting

		 * an allocation much larger than the huge page size (eg CMA).

		 * We can potentially save a lot of iterations if we skip them

		 * at once. The check is racy, but we can consider only valid

		 * values and the only danger is skipping too much.

		/*

		 * Check may be lockless but that's ok as we recheck later.

		 * It's possible to migrate LRU and non-lru movable pages.

		 * Skip any other type of page

			/*

			 * __PageMovable can return false positive so we need

			 * to verify it under page_lock.

		/*

		 * Migration will fail if an anonymous page is pinned in memory,

		 * so avoid taking lru_lock and isolating it unnecessarily in an

		 * admittedly racy check.

		/*

		 * Only allow to migrate anonymous pages in GFP_NOFS context

		 * because those do not depend on fs locks.

		/*

		 * Be careful not to clear PageLRU until after we're

		 * sure the page is not being freed elsewhere -- the

		 * page release code relies on it.

 Try isolate the page */

 If we already hold the lock, we can skip some rechecking */

 Try get exclusive access under lock */

			/*

			 * Page become compound since the non-locked check,

			 * and it's on LRU. It can only be a THP so the order

			 * is safe to read and it's 0 for tail pages.

 The whole page is taken off the LRU; skip the tail pages. */

 Successfully isolated */

		/*

		 * Avoid isolating too much unless this block is being

		 * rescanned (e.g. dirty/writeback pages, parallel allocation)

		 * or a lock is contended. For contention, isolate quickly to

		 * potentially remove one source of contention.

 Avoid potential deadlock in freeing page under lru_lock */

		/*

		 * We have isolated some pages, but then failed. Release them

		 * instead of migrating, as we cannot form the cc->order buddy

		 * page anyway.

			/*

			 * The check near the loop beginning would have updated

			 * next_skip_pfn too, but this is a bit simpler.

	/*

	 * The PageBuddy() check could have potentially brought us outside

	 * the range to be scanned.

	/*

	 * Updated the cached scanner pfn once the pageblock has been scanned

	 * Pages will either be migrated in which case there is no point

	 * scanning in the near future or migration failed in which case the

	 * failure reason may persist. The block is marked for skipping if

	 * there were no pages isolated in the block or if the block is

	 * rescanned twice in a row.

/**

 * isolate_migratepages_range() - isolate migrate-able pages in a PFN range

 * @cc:        Compaction control structure.

 * @start_pfn: The first PFN to start isolating.

 * @end_pfn:   The one-past-last PFN.

 *

 * Returns -EAGAIN when contented, -EINTR in case of a signal pending, -ENOMEM

 * in case we could not allocate a page, or 0.

 Scan block by block. First and last block may be incomplete */

 CONFIG_COMPACTION || CONFIG_CMA */

 Returns true if the page is within a block suitable for migration to */

 If the page is a large free page, then disallow migration */

		/*

		 * We are checking page_order without zone->lock taken. But

		 * the only small danger is that we skip a potentially suitable

		 * pageblock, so it's not worth to check order for valid range.

 If the block is MIGRATE_MOVABLE or MIGRATE_CMA, allow migration */

 Otherwise skip the block */

/*

 * Test whether the free scanner has reached the same or lower pageblock than

 * the migration scanner, and compaction should thus terminate.

/*

 * Used when scanning for a suitable migration target which scans freelists

 * in reverse. Reorders the list such as the unscanned pages are scanned

 * first on the next iteration of the free scanner

/*

 * Similar to move_freelist_head except used by the migration scanner

 * when scanning forward. It's possible for these list operations to

 * move against each other if they search the free list exactly in

 * lockstep.

 Do not search around if there are enough pages already */

 Minimise scanning during async compaction */

 Pageblock boundaries */

 Scan before */

 Scan after */

 Skip this pageblock in the future as it's full or nearly full */

 Search orders in round-robin fashion */

 Search wrapped around? */

 Full compaction passes in a negative order */

	/*

	 * If starting the scan, use a deeper search and use the highest

	 * PFN found if a suitable one is not found.

	/*

	 * Preferred point is in the top quarter of the scan space but take

	 * a pfn from the top half if the search is problematic.

	/*

	 * Search starts from the last successful isolation order or the next

	 * order to search after a previous failure

 Shorten the scan if a candidate is found */

 Use a minimum pfn if a preferred one was not found */

 Update freepage for the list reorder below */

 Reorder to so a future search skips recent pages */

 Isolate the page if available */

 If isolation fails, abort the search */

		/*

		 * Smaller scan on next order so the total scan is related

		 * to freelist_scan_limit.

			/*

			 * Use the highest PFN found above min. If one was

			 * not found, be pessimistic for direct compaction

			 * and use the min mark.

/*

 * Based on information in the current compact_control, find blocks

 * suitable for isolating free pages from and then isolate them.

 start of current pageblock */

 exact pfn we start at */

 end of current pageblock */

 lowest pfn scanner is able to scan */

 Try a small search of the free lists for a candidate */

	/*

	 * Initialise the free scanner. The starting point is where we last

	 * successfully isolated from, zone-cached value, or the end of the

	 * zone when isolating for the first time. For looping we also need

	 * this pfn aligned down to the pageblock boundary, because we do

	 * block_start_pfn -= pageblock_nr_pages in the for loop.

	 * For ending point, take care when isolating in last pageblock of a

	 * zone which ends in the middle of a pageblock.

	 * The low boundary is the end of the pageblock the migration scanner

	 * is using.

	/*

	 * Isolate free pages until enough are available to migrate the

	 * pages on cc->migratepages. We stop searching if the migrate

	 * and free page scanners meet or enough free pages are isolated.

		/*

		 * This can iterate a massively long zone without finding any

		 * suitable migration targets, so periodically check resched.

 Check the block is suitable for migration */

 If isolation recently failed, do not retry */

 Found a block suitable for isolating free pages from. */

 Update the skip hint if the full pageblock was scanned */

 Are enough freepages isolated? */

				/*

				 * Restart at previous pageblock if more

				 * freepages can be isolated next time.

			/*

			 * If isolation failed early, do not continue

			 * needlessly.

 Adjust stride depending on isolation */

	/*

	 * Record where the free scanner will restart next time. Either we

	 * broke from the loop and set isolate_start_pfn based on the last

	 * call to isolate_freepages_block(), or we met the migration scanner

	 * and the loop terminated due to isolate_start_pfn < low_pfn

 __isolate_free_page() does not map the pages */

/*

 * This is a migrate-callback that "allocates" freepages by taking pages

 * from the isolated freelists in the block we are migrating to.

/*

 * This is a migrate-callback that "frees" freepages back to the isolated

 * freelist.  All pages on the freelist are from the same zone, so there is no

 * special handling needed for NUMA.

 possible outcome of isolate_migratepages */

 Abort compaction now */

 No pages isolated, continue scanning */

 Pages isolated, migrate */

/*

 * Allow userspace to control policy on scanning the unevictable LRU for

 * compactable pages.

/*

 * Briefly search the free lists for a migration source that already has

 * some free pages to reduce the number of pages that need migration

 * before a pageblock is free.

 Skip hints are relied on to avoid repeats on the fast search */

	/*

	 * If the migrate_pfn is not at the start of a zone or the start

	 * of a pageblock then assume this is a continuation of a previous

	 * scan restarted due to COMPACT_CLUSTER_MAX.

	/*

	 * For smaller orders, just linearly scan as the number of pages

	 * to migrate should be relatively small and does not necessarily

	 * justify freeing up a large block for a small allocation.

	/*

	 * Only allow kcompactd and direct requests for movable pages to

	 * quickly clear out a MOVABLE pageblock for allocation. This

	 * reduces the risk that a large movable pageblock is freed for

	 * an unmovable/reclaimable small allocation.

	/*

	 * When starting the migration scanner, pick any pageblock within the

	 * first half of the search space. Otherwise try and pick a pageblock

	 * within the first eighth to reduce the chances that a migration

	 * target later becomes a source.

				/*

				 * Avoid if skipped recently. Ideally it would

				 * move to the tail but even safe iteration of

				 * the list assumes an entry is deleted, not

				 * reordered.

 Reorder to so a future search skips recent pages */

	/*

	 * If fast scanning failed then use a cached entry for a page block

	 * that had free pages as the basis for starting a linear scan.

/*

 * Isolate all pages that can be migrated from the first suitable block,

 * starting at the block pointed to by the migrate scanner pfn within

 * compact_control.

	/*

	 * Start at where we last stopped, or beginning of the zone as

	 * initialized by compact_zone(). The first failure will use

	 * the lowest PFN as the starting point for linear scanning.

	/*

	 * fast_find_migrateblock marks a pageblock skipped so to avoid

	 * the isolation_suitable check below, check whether the fast

	 * search was successful.

 Only scan within a pageblock boundary */

	/*

	 * Iterate over whole pageblocks until we find the first suitable.

	 * Do not cross the free scanner.

		/*

		 * This can potentially iterate a massively long zone with

		 * many pageblocks unsuitable, so periodically check if we

		 * need to schedule.

		/*

		 * If isolation recently failed, do not retry. Only check the

		 * pageblock once. COMPACT_CLUSTER_MAX causes a pageblock

		 * to be visited multiple times. Assume skip was checked

		 * before making it "skip" so other compaction instances do

		 * not scan the same block.

		/*

		 * For async compaction, also only scan in MOVABLE blocks

		 * without huge pages. Async compaction is optimistic to see

		 * if the minimum amount of work satisfies the allocation.

		 * The cached PFN is updated as it's possible that all

		 * remaining blocks between source and target are unsuitable

		 * and the compaction scanners fail to meet.

 Perform the isolation */

		/*

		 * Either we isolated something and proceed with migration. Or

		 * we failed and compact_zone should decide if we should

		 * continue or not.

/*

 * order == -1 is expected when compacting via

 * /proc/sys/vm/compact_memory

/*

 * A zone's fragmentation score is the external fragmentation wrt to the

 * COMPACTION_HPAGE_ORDER. It returns a value in the range [0, 100].

/*

 * A weighted zone's fragmentation score is the external fragmentation

 * wrt to the COMPACTION_HPAGE_ORDER scaled by the zone's size. It

 * returns a value in the range [0, 100].

 *

 * The scaling factor ensures that proactive compaction focuses on larger

 * zones like ZONE_NORMAL, rather than smaller, specialized zones like

 * ZONE_DMA32. For smaller zones, the score value remains close to zero,

 * and thus never exceeds the high threshold for proactive compaction.

/*

 * The per-node proactive (background) compaction process is started by its

 * corresponding kcompactd thread when the node's fragmentation score

 * exceeds the high threshold. The compaction process remains active till

 * the node's score falls below the low threshold, or one of the back-off

 * conditions is met.

	/*

	 * Cap the low watermark to avoid excessive compaction

	 * activity in case a user sets the proactiveness tunable

	 * close to 100 (maximum).

 Compaction run completes if the migrate and free scanner meet */

 Let the next compaction start anew. */

		/*

		 * Mark that the PG_migrate_skip information should be cleared

		 * by kswapd when it goes to sleep. kcompactd does not set the

		 * flag itself as the decision to be clear should be directly

		 * based on an allocation request.

	/*

	 * Always finish scanning a pageblock to reduce the possibility of

	 * fallbacks in the future. This is particularly important when

	 * migration source is unmovable/reclaimable but it's not worth

	 * special casing.

 Direct compactor: Is a suitable page free? */

 Job done if page is free of the right migratetype */

 MIGRATE_MOVABLE can fallback on MIGRATE_CMA */

		/*

		 * Job done if allocation would steal freepages from

		 * other migratetype buddy lists.

 movable pages are OK in any pageblock */

			/*

			 * We are stealing for a non-movable allocation. Make

			 * sure we finish compacting the current pageblock

			 * first so it is as free as possible and we won't

			 * have to steal another one soon. This only applies

			 * to sync compaction, as async compaction operates

			 * on pageblocks of the same migratetype.

	/*

	 * If watermarks for high-order allocation are already met, there

	 * should be no need for compaction at all.

	/*

	 * Watermarks for order-0 must be met for compaction to be able to

	 * isolate free pages for migration targets. This means that the

	 * watermark and alloc_flags have to match, or be more pessimistic than

	 * the check in __isolate_free_page(). We don't use the direct

	 * compactor's alloc_flags, as they are not relevant for freepage

	 * isolation. We however do use the direct compactor's highest_zoneidx

	 * to skip over zones where lowmem reserves would prevent allocation

	 * even if compaction succeeds.

	 * For costly orders, we require low watermark instead of min for

	 * compaction to proceed to increase its chances.

	 * ALLOC_CMA is used, as pages in CMA pageblocks are considered

	 * suitable migration targets

/*

 * compaction_suitable: Is this suitable to run compaction on this zone now?

 * Returns

 *   COMPACT_SKIPPED  - If there are too few free pages for compaction

 *   COMPACT_SUCCESS  - If the allocation would succeed without compaction

 *   COMPACT_CONTINUE - If compaction should run now

	/*

	 * fragmentation index determines if allocation failures are due to

	 * low memory or external fragmentation

	 *

	 * index of -1000 would imply allocations might succeed depending on

	 * watermarks, but we already failed the high-order watermark check

	 * index towards 0 implies failure is due to lack of memory

	 * index towards 1000 implies failure is due to fragmentation

	 *

	 * Only compact if a failure would be due to fragmentation. Also

	 * ignore fragindex for non-costly orders where the alternative to

	 * a successful reclaim/compaction is OOM. Fragindex and the

	 * vm.extfrag_threshold sysctl is meant as a heuristic to prevent

	 * excessive compaction for costly orders, but it should not be at the

	 * expense of system stability.

	/*

	 * Make sure at least one zone would pass __compaction_suitable if we continue

	 * retrying the reclaim.

		/*

		 * Do not consider all the reclaimable memory because we do not

		 * want to trash just for a single high order allocation which

		 * is even not guaranteed to appear even if __compaction_suitable

		 * is happy about the watermark check.

	/*

	 * These counters track activities during zone compaction.  Initialize

	 * them before compacting a new zone.

 Compaction is likely to fail */

 huh, compaction_suitable is returning something unexpected */

	/*

	 * Clear pageblock skip if there were failures recently and compaction

	 * is about to be retried after being deferred.

	/*

	 * Setup to move all movable pages to the end of the zone. Used cached

	 * information on where the scanners should start (unless we explicitly

	 * want to compact the whole zone), but check that it is initialised

	 * by ensuring the values are within zone boundaries.

	/*

	 * Migrate has separate cached PFNs for ASYNC and SYNC* migration on

	 * the basis that some migrations will fail in ASYNC mode. However,

	 * if the cached PFNs match and pageblocks are skipped due to having

	 * no isolation candidates, then the sync state does not matter.

	 * Until a pageblock with isolation candidates is found, keep the

	 * cached PFNs in sync to avoid revisiting the same blocks.

 lru_add_drain_all could be expensive with involving other CPUs */

		/*

		 * Avoid multiple rescans which can happen if a page cannot be

		 * isolated (dirty/writeback in async mode) or if the migrated

		 * pages are being allocated before the pageblock is cleared.

		 * The first rescan will capture the entire pageblock for

		 * migration. If it fails, it'll be marked skip and scanning

		 * will proceed as normal.

			/*

			 * We haven't isolated and migrated anything, but

			 * there might still be unflushed migrations from

			 * previous cc->order aligned block.

 All pages were either migrated or will be released */

			/*

			 * migrate_pages() may return -ENOMEM when scanners meet

			 * and we want compact_finished() to detect it

			/*

			 * We failed to migrate at least one page in the current

			 * order-aligned block, so skip the rest of it.

 Draining pcplists is useless in this case */

		/*

		 * Has the migration scanner moved away from the previous

		 * cc->order aligned block where we migrated from? If yes,

		 * flush the pages that were freed, so that they can merge and

		 * compact_finished() can detect immediately if allocation

		 * would succeed.

 No more flushing until we migrate again */

 Stop if a page has been captured */

	/*

	 * Release free pages and update where the free scanner should restart,

	 * so we don't leave any returned pages behind in the next attempt.

 The cached pfn is always the first in a pageblock */

		/*

		 * Only go back, not forward. The cached pfn might have been

		 * already reset to zone end in compact_finished()

	/*

	 * Make sure the structs are really initialized before we expose the

	 * capture control, in case we are interrupted and the interrupt handler

	 * frees a page.

	/*

	 * Make sure we hide capture control first before we read the captured

	 * page pointer, otherwise an interrupt could free and capture a page

	 * and we would leak it.

	/*

	 * Technically, it is also possible that compaction is skipped but

	 * the page is still captured out of luck(IRQ came and freed the page).

	 * Returning COMPACT_SUCCESS in such cases helps in properly accounting

	 * the COMPACT[STALL|FAIL] when compaction is skipped.

/**

 * try_to_compact_pages - Direct compact to satisfy a high-order allocation

 * @gfp_mask: The GFP mask of the current allocation

 * @order: The order of the current allocation

 * @alloc_flags: The allocation flags of the current allocation

 * @ac: The context of current allocation

 * @prio: Determines how hard direct compaction should try to succeed

 * @capture: Pointer to free page created by compaction will be stored here

 *

 * This is the main entry point for direct page compaction.

	/*

	 * Check if the GFP flags allow compaction - GFP_NOIO is really

	 * tricky context because the migration might require IO

 Compact each zone in the list */

 The allocation should succeed, stop compacting */

			/*

			 * We think the allocation will succeed in this zone,

			 * but it is not certain, hence the false. The caller

			 * will repeat this with true if allocation indeed

			 * succeeds in this zone.

			/*

			 * We think that allocation won't succeed in this zone

			 * so we defer compaction there. If it ends up

			 * succeeding after all, it will be reset.

		/*

		 * We might have stopped compacting due to need_resched() in

		 * async compaction, or due to a fatal signal detected. In that

		 * case do not try further zones

/*

 * Compact all zones within a node till each zone's fragmentation score

 * reaches within proactive compaction thresholds (as determined by the

 * proactiveness tunable).

 *

 * It is possible that the function returns before reaching score targets

 * due to various back-off conditions, such as, contention on per-node or

 * per-zone locks.

 Compact all zones within a node */

 Compact all nodes in the system */

 Flush pending updates to the LRU lists */

/*

 * Tunable for proactive compaction. It determines how

 * aggressively the kernel should compact memory in the

 * background. It takes values in the range [0, 100].

/*

 * This is the entry point for compacting all nodes via

 * /proc/sys/vm/compact_memory

 Flush pending updates to the LRU lists */

 CONFIG_SYSFS && CONFIG_NUMA */

	/*

	 * With no special task, compact all zones so that a page of requested

	 * order is allocatable.

			/*

			 * Buddy pages may become stranded on pcps that could

			 * otherwise coalesce on the zone's free area for

			 * order >= cc.order.  This is ratelimited by the

			 * upcoming deferral.

			/*

			 * We use sync migration mode here, so we defer like

			 * sync direct compaction does.

	/*

	 * Regardless of success, we are done until woken up next. But remember

	 * the requested order/highest_zoneidx in case it was higher/tighter

	 * than our current ones

	/*

	 * Pairs with implicit barrier in wait_event_freezable()

	 * such that wakeups are not missed.

/*

 * The background compaction daemon, started as a kernel thread

 * from the init process.

		/*

		 * Avoid the unnecessary wakeup for proactive compaction

		 * when it is disabled.

			/*

			 * Reset the timeout value. The defer timeout from

			 * proactive compaction is lost here but that is fine

			 * as the condition of the zone changing substantionally

			 * then carrying on with the previous defer interval is

			 * not useful.

		/*

		 * Start the proactive work with default timeout. Based

		 * on the fragmentation score, this timeout is updated.

			/*

			 * Defer proactive compaction if the fragmentation

			 * score did not go down i.e. no progress made.

/*

 * This kcompactd start function will be called by init and node-hot-add.

 * On node-hot-add, kcompactd will moved to proper cpus if cpus are hot-added.

/*

 * Called by memory hotplug when all memory in a node is offlined. Caller must

 * hold mem_hotplug_begin/end().

/*

 * It's optimal to keep kcompactd on the same CPUs as their memory, but

 * not required for correctness. So if the last cpu in a node goes

 * away, we get changed to run anywhere: as the first one comes back,

 * restore their cpu bindings.

 One of our CPUs online: restore mask */

 CONFIG_COMPACTION */

