 SPDX-License-Identifier: GPL-2.0

/*

 * mm/debug.c

 *

 * mm/ specific debug routines.

 *

/*

 * Define EM() and EMe() so that MIGRATE_REASON from trace/events/migrate.h can

 * be used to populate migrate_reason_names[].

	/*

	 * Accessing the pageblock without the zone lock. It could change to

	 * "isolate" again in the meantime, but since we are just dumping the

	 * state for debugging, it should be fine to accept a bit of

	 * inaccuracy here due to racing.

		/*

		 * Corrupt page, so we cannot call page_mapping. Instead, do a

		 * safe subset of the steps that page_mapping() does. Caution:

		 * this will be misleading for tail pages, PageSwapCache pages,

		 * and potentially other situations. (See the page_mapping()

		 * implementation for what's missing here.)

	/*

	 * Avoid VM_BUG_ON() in page_mapcount().

	 * page->_mapcount space in struct page is used by sl[aou]b pages to

	 * encode own info.

		/*

		 * mapping can be invalid pointer and we don't want to crash

		 * accessing it, so probe everything depending on it carefully

			/*

			 * if dentry is corrupted, the %pd handler may still

			 * crash, but it's unlikely that we reach here with a

			 * corrupted struct page

	/*

	 * Calling vm_debug with no arguments is equivalent to requesting

	 * to enable all debugging options we can control.

 CONFIG_DEBUG_VM */

 SPDX-License-Identifier: GPL-2.0

/*

 * We want to know the real level where a entry is located ignoring any

 * folding of levels which may be happening. For example if p4d is folded then

 * a missing entry found at level 1 (p4d) is actually at level 0 (pgd).

		/*

		 * This implies that each ->pmd_entry() handler

		 * needs to know about pmd_trans_huge() pmds

		/*

		 * Check this here so we only break down trans_huge

		 * pages when we _need_ to

 CONFIG_HUGETLB_PAGE */

 CONFIG_HUGETLB_PAGE */

/*

 * Decide whether we really walk over the current vma on [@start, @end)

 * or skip it via the returned value. Return 0 if we do walk over the

 * current vma, and return 1 if we skip the vma. Negative values means

 * error, where we abort the current walk.

	/*

	 * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP

	 * range, so we don't walk over it as we do for normal vmas. However,

	 * Some callers are interested in handling hole range and they don't

	 * want to just ignore any single address range. Such users certainly

	 * define their ->pte_hole() callbacks, so let's delegate them to handle

	 * vma(VM_PFNMAP).

/**

 * walk_page_range - walk page table with caller specific callbacks

 * @mm:		mm_struct representing the target process of page table walk

 * @start:	start address of the virtual address range

 * @end:	end address of the virtual address range

 * @ops:	operation to call during the walk

 * @private:	private data for callbacks' usage

 *

 * Recursively walk the page table tree of the process represented by @mm

 * within the virtual address range [@start, @end). During walking, we can do

 * some caller-specific works for each entry, by setting up pmd_entry(),

 * pte_entry(), and/or hugetlb_entry(). If you don't set up for some of these

 * callbacks, the associated entries/pages are just ignored.

 * The return values of these callbacks are commonly defined like below:

 *

 *  - 0  : succeeded to handle the current entry, and if you don't reach the

 *         end address yet, continue to walk.

 *  - >0 : succeeded to handle the current entry, and return to the caller

 *         with caller specific value.

 *  - <0 : failed to handle the current entry, and return to the caller

 *         with error code.

 *

 * Before starting to walk page table, some callers want to check whether

 * they really want to walk over the current vma, typically by checking

 * its vm_flags. walk_page_test() and @ops->test_walk() are used for this

 * purpose.

 *

 * If operations need to be staged before and committed after a vma is walked,

 * there are two callbacks, pre_vma() and post_vma(). Note that post_vma(),

 * since it is intended to handle commit-type operations, can't return any

 * errors.

 *

 * struct mm_walk keeps current values of some common data like vma and pmd,

 * which are useful for the access from callbacks. If you want to pass some

 * caller-specific data to callbacks, @private should be helpful.

 *

 * Locking:

 *   Callers of walk_page_range() and walk_page_vma() should hold @mm->mmap_lock,

 *   because these function traverse vma list and/or access to vma's data.

 after the last vma */

 outside vma */

 inside vma */

				/*

				 * positive return values are purely for

				 * controlling the pagewalk, so should never

				 * be passed to the callers.

/*

 * Similar to walk_page_range() but can walk any page tables even if they are

 * not backed by VMAs. Because 'unusual' entries may be walked this function

 * will also not lock the PTEs for the pte_entry() callback. This is useful for

 * walking the kernel pages tables or page tables for firmware.

/**

 * walk_page_mapping - walk all memory areas mapped into a struct address_space.

 * @mapping: Pointer to the struct address_space

 * @first_index: First page offset in the address_space

 * @nr: Number of incremental page offsets to cover

 * @ops:	operation to call during the walk

 * @private:	private data for callbacks' usage

 *

 * This function walks all memory areas mapped into a struct address_space.

 * The walk is limited to only the given page-size index range, but if

 * the index boundaries cross a huge page-table entry, that entry will be

 * included.

 *

 * Also see walk_page_range() for additional information.

 *

 * Locking:

 *   This function can't require that the struct mm_struct::mmap_lock is held,

 *   since @mapping may be mapped by multiple processes. Instead

 *   @mapping->i_mmap_rwsem must be held. This might have implications in the

 *   callbacks, and it's up tho the caller to ensure that the

 *   struct mm_struct::mmap_lock is not needed.

 *

 *   Also this means that a caller can't rely on the struct

 *   vm_area_struct::vm_flags to be constant across a call,

 *   except for immutable flags. Callers requiring this shouldn't use

 *   this function.

 *

 * Return: 0 on success, negative error code on failure, positive number on

 * caller defined premature termination.

 Clip to the vma */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/mmu_notifier.c

 *

 *  Copyright (C) 2008  Qumranet, Inc.

 *  Copyright (C) 2008  SGI

 *             Christoph Lameter <cl@linux.com>

 global SRCU for all MMs */

/*

 * The mmu_notifier_subscriptions structure is allocated and installed in

 * mm->notifier_subscriptions inside the mm_take_all_locks() protected

 * critical section and it's released only when mm_count reaches zero

 * in mmdrop().

 all mmu notifiers registered in this mm are queued in this list */

 to serialize the list modifications and hlist_unhashed */

/*

 * This is a collision-retry read-side/write-side 'lock', a lot like a

 * seqcount, however this allows multiple write-sides to hold it at

 * once. Conceptually the write side is protecting the values of the PTEs in

 * this mm, such that PTES cannot be read into SPTEs (shadow PTEs) while any

 * writer exists.

 *

 * Note that the core mm creates nested invalidate_range_start()/end() regions

 * within the same thread, and runs invalidate_range_start()/end() in parallel

 * on multiple CPUs. This is designed to not reduce concurrency or block

 * progress on the mm side.

 *

 * As a secondary function, holding the full write side also serves to prevent

 * writers for the itree, this is an optimization to avoid extra locking

 * during invalidate_range_start/end notifiers.

 *

 * The write side has two states, fully excluded:

 *  - mm->active_invalidate_ranges != 0

 *  - subscriptions->invalidate_seq & 1 == True (odd)

 *  - some range on the mm_struct is being invalidated

 *  - the itree is not allowed to change

 *

 * And partially excluded:

 *  - mm->active_invalidate_ranges != 0

 *  - subscriptions->invalidate_seq & 1 == False (even)

 *  - some range on the mm_struct is being invalidated

 *  - the itree is allowed to change

 *

 * Operations on notifier_subscriptions->invalidate_seq (under spinlock):

 *    seq |= 1  # Begin writing

 *    seq++     # Release the writing state

 *    seq & 1   # True if a writer exists

 *

 * The later state avoids some expensive work on inv_end in the common case of

 * no mmu_interval_notifier monitoring the VA.

 Make invalidate_seq even */

	/*

	 * The inv_end incorporates a deferred mechanism like rtnl_unlock().

	 * Adds and removes are queued until the final inv_end happens then

	 * they are progressed. This arrangement for tree updates is used to

	 * avoid using a blocking lock during invalidate_range_start.

/**

 * mmu_interval_read_begin - Begin a read side critical section against a VA

 *                           range

 * @interval_sub: The interval subscription

 *

 * mmu_iterval_read_begin()/mmu_iterval_read_retry() implement a

 * collision-retry scheme similar to seqcount for the VA range under

 * subscription. If the mm invokes invalidation during the critical section

 * then mmu_interval_read_retry() will return true.

 *

 * This is useful to obtain shadow PTEs where teardown or setup of the SPTEs

 * require a blocking context.  The critical region formed by this can sleep,

 * and the required 'user_lock' can also be a sleeping lock.

 *

 * The caller is required to provide a 'user_lock' to serialize both teardown

 * and setup.

 *

 * The return value should be passed to mmu_interval_read_retry().

	/*

	 * If the subscription has a different seq value under the user_lock

	 * than we started with then it has collided.

	 *

	 * If the subscription currently has the same seq value as the

	 * subscriptions seq, then it is currently between

	 * invalidate_start/end and is colliding.

	 *

	 * The locking looks broadly like this:

	 *   mn_tree_invalidate_start():          mmu_interval_read_begin():

	 *                                         spin_lock

	 *                                          seq = READ_ONCE(interval_sub->invalidate_seq);

	 *                                          seq == subs->invalidate_seq

	 *                                         spin_unlock

	 *    spin_lock

	 *     seq = ++subscriptions->invalidate_seq

	 *    spin_unlock

	 *     op->invalidate_range():

	 *       user_lock

	 *        mmu_interval_set_seq()

	 *         interval_sub->invalidate_seq = seq

	 *       user_unlock

	 *

	 *                          [Required: mmu_interval_read_retry() == true]

	 *

	 *   mn_itree_inv_end():

	 *    spin_lock

	 *     seq = ++subscriptions->invalidate_seq

	 *    spin_unlock

	 *

	 *                                        user_lock

	 *                                         mmu_interval_read_retry():

	 *                                          interval_sub->invalidate_seq != seq

	 *                                        user_unlock

	 *

	 * Barriers are not needed here as any races here are closed by an

	 * eventual mmu_interval_read_retry(), which provides a barrier via the

	 * user_lock.

 Pairs with the WRITE_ONCE in mmu_interval_set_seq() */

	/*

	 * interval_sub->invalidate_seq must always be set to an odd value via

	 * mmu_interval_set_seq() using the provided cur_seq from

	 * mn_itree_inv_start_range(). This ensures that if seq does wrap we

	 * will always clear the below sleep in some reasonable time as

	 * subscriptions->invalidate_seq is even in the idle state.

	/*

	 * Notice that mmu_interval_read_retry() can already be true at this

	 * point, avoiding loops here allows the caller to provide a global

	 * time bound.

/*

 * This function can't run concurrently against mmu_notifier_register

 * because mm->mm_users > 0 during mmu_notifier_register and exit_mmap

 * runs with mm_users == 0. Other tasks may still invoke mmu notifiers

 * in parallel despite there being no task using this mm any more,

 * through the vmas outside of the exit_mmap context, such as with

 * vmtruncate. This serializes against mmu_notifier_unregister with

 * the notifier_subscriptions->lock in addition to SRCU and it serializes

 * against the other mmu notifiers with SRCU. struct mmu_notifier_subscriptions

 * can't go away from under us as exit_mmap holds an mm_count pin

 * itself.

	/*

	 * SRCU here will block mmu_notifier_unregister until

	 * ->release returns.

		/*

		 * If ->release runs before mmu_notifier_unregister it must be

		 * handled, as it's the only way for the driver to flush all

		 * existing sptes and stop the driver from establishing any more

		 * sptes before all the pages in the mm are freed.

		/*

		 * We arrived before mmu_notifier_unregister so

		 * mmu_notifier_unregister will do nothing other than to wait

		 * for ->release to finish and for mmu_notifier_unregister to

		 * return.

	/*

	 * synchronize_srcu here prevents mmu_notifier_release from returning to

	 * exit_mmap (which would proceed with freeing all pages in the mm)

	 * until the ->release method returns, if it was invoked by

	 * mmu_notifier_unregister.

	 *

	 * The notifier_subscriptions can't go away from under us because

	 * one mm_count is held by exit_mmap.

/*

 * If no young bitflag is supported by the hardware, ->clear_flush_young can

 * unmap the address and return 1 or 0 depending if the mapping previously

 * existed or not.

	/*

	 * On -EAGAIN the non-blocking caller is not allowed to call

	 * invalidate_range_end()

				/*

				 * We call all the notifiers on any EAGAIN,

				 * there is no way for a notifier to know if

				 * its start method failed, thus a start that

				 * does EAGAIN can't also do end.

		/*

		 * Must be non-blocking to get here.  If there are multiple

		 * notifiers and one or more failed start, any that succeeded

		 * start are expecting their end to be called.  Do so now.

		/*

		 * Call invalidate_range here too to avoid the need for the

		 * subsystem of having to register an invalidate_range_end

		 * call-back when there is invalidate_range already. Usually a

		 * subsystem registers either invalidate_range_start()/end() or

		 * invalidate_range(), so this will be no additional overhead

		 * (besides the pointer check).

		 *

		 * We skip call to invalidate_range() if we know it is safe ie

		 * call site use mmu_notifier_invalidate_range_only_end() which

		 * is safe to do when we know that a call to invalidate_range()

		 * already happen under page table lock.

/*

 * Same as mmu_notifier_register but here the caller must hold the mmap_lock in

 * write mode. A NULL mn signals the notifier is being registered for itree

 * mode.

		/*

		 * kmalloc cannot be called under mm_take_all_locks(), but we

		 * know that mm->notifier_subscriptions can't change while we

		 * hold the write side of the mmap_lock.

	/*

	 * Serialize the update against mmu_notifier_unregister. A

	 * side note: mmu_notifier_release can't run concurrently with

	 * us because we hold the mm_users pin (either implicitly as

	 * current->mm or explicitly with get_task_mm() or similar).

	 * We can't race against any other mmu notifier method either

	 * thanks to mm_take_all_locks().

	 *

	 * release semantics on the initialization of the

	 * mmu_notifier_subscriptions's contents are provided for unlocked

	 * readers.  acquire can only be used while holding the mmgrab or

	 * mmget, and is safe because once created the

	 * mmu_notifier_subscriptions is not freed until the mm is destroyed.

	 * As above, users holding the mmap_lock or one of the

	 * mm_take_all_locks() do not need to use acquire semantics.

 Pairs with the mmdrop in mmu_notifier_unregister_* */

/**

 * mmu_notifier_register - Register a notifier on a mm

 * @subscription: The notifier to attach

 * @mm: The mm to attach the notifier to

 *

 * Must not hold mmap_lock nor any other VM related lock when calling

 * this registration function. Must also ensure mm_users can't go down

 * to zero while this runs to avoid races with mmu_notifier_release,

 * so mm has to be current->mm or the mm should be pinned safely such

 * as with get_task_mm(). If the mm is not current->mm, the mm_users

 * pin should be released by calling mmput after mmu_notifier_register

 * returns.

 *

 * mmu_notifier_unregister() or mmu_notifier_put() must be always called to

 * unregister the notifier.

 *

 * While the caller has a mmu_notifier get the subscription->mm pointer will remain

 * valid, and can be converted to an active mm pointer via mmget_not_zero().

/**

 * mmu_notifier_get_locked - Return the single struct mmu_notifier for

 *                           the mm & ops

 * @ops: The operations struct being subscribe with

 * @mm : The mm to attach notifiers too

 *

 * This function either allocates a new mmu_notifier via

 * ops->alloc_notifier(), or returns an already existing notifier on the

 * list. The value of the ops pointer is used to determine when two notifiers

 * are the same.

 *

 * Each call to mmu_notifier_get() must be paired with a call to

 * mmu_notifier_put(). The caller must hold the write side of mm->mmap_lock.

 *

 * While the caller has a mmu_notifier get the mm pointer will remain valid,

 * and can be converted to an active mm pointer via mmget_not_zero().

 this is called after the last mmu_notifier_unregister() returned */

 debug */

/*

 * This releases the mm_count pin automatically and frees the mm

 * structure if it was the last user of it. It serializes against

 * running mmu notifiers with SRCU and against mmu_notifier_unregister

 * with the unregister lock + SRCU. All sptes must be dropped before

 * calling mmu_notifier_unregister. ->release or any other notifier

 * method may be invoked concurrently with mmu_notifier_unregister,

 * and only after mmu_notifier_unregister returned we're guaranteed

 * that ->release or any other method can't run anymore.

		/*

		 * SRCU here will force exit_mmap to wait for ->release to

		 * finish before freeing the pages.

		/*

		 * exit_mmap will block in mmu_notifier_release to guarantee

		 * that ->release is called before freeing the pages.

		/*

		 * Can not use list_del_rcu() since __mmu_notifier_release

		 * can delete it before we hold the lock.

	/*

	 * Wait for any running method to finish, of course including

	 * ->release if it was run by mmu_notifier_release instead of us.

 Pairs with the get in __mmu_notifier_register() */

/**

 * mmu_notifier_put - Release the reference on the notifier

 * @subscription: The notifier to act on

 *

 * This function must be paired with each mmu_notifier_get(), it releases the

 * reference obtained by the get. If this is the last reference then process

 * to free the notifier will be run asynchronously.

 *

 * Unlike mmu_notifier_unregister() the get/put flow only calls ops->release

 * when the mm_struct is destroyed. Instead free_notifier is always called to

 * release any resources held by the user.

 *

 * As ops->release is not guaranteed to be called, the user must ensure that

 * all sptes are dropped, and no new sptes can be established before

 * mmu_notifier_put() is called.

 *

 * This function can be called from the ops->release callback, however the

 * caller must still ensure it is called pairwise with mmu_notifier_get().

 *

 * Modules calling this function must call mmu_notifier_synchronize() in

 * their __exit functions to ensure the async work is completed.

	/*

	 * Note that the representation of the intervals in the interval tree

	 * considers the ending point as contained in the interval.

 Must call with a mmget() held */

 pairs with mmdrop in mmu_interval_notifier_remove() */

	/*

	 * If some invalidate_range_start/end region is going on in parallel

	 * we don't know what VA ranges are affected, so we must assume this

	 * new range is included.

	 *

	 * If the itree is invalidating then we are not allowed to change

	 * it. Retrying until invalidation is done is tricky due to the

	 * possibility for live lock, instead defer the add to

	 * mn_itree_inv_end() so this algorithm is deterministic.

	 *

	 * In all cases the value for the interval_sub->invalidate_seq should be

	 * odd, see mmu_interval_read_begin()

		/*

		 * The starting seq for a subscription not under invalidation

		 * should be odd, not equal to the current invalidate_seq and

		 * invalidate_seq should not 'wrap' to the new seq any time

		 * soon.

/**

 * mmu_interval_notifier_insert - Insert an interval notifier

 * @interval_sub: Interval subscription to register

 * @start: Starting virtual address to monitor

 * @length: Length of the range to monitor

 * @mm: mm_struct to attach to

 * @ops: Interval notifier operations to be called on matching events

 *

 * This function subscribes the interval notifier for notifications from the

 * mm.  Upon return the ops related to mmu_interval_notifier will be called

 * whenever an event that intersects with the given range occurs.

 *

 * Upon return the range_notifier may not be present in the interval tree yet.

 * The caller must use the normal interval notifier read flow via

 * mmu_interval_read_begin() to establish SPTEs for this range.

/**

 * mmu_interval_notifier_remove - Remove a interval notifier

 * @interval_sub: Interval subscription to unregister

 *

 * This function must be paired with mmu_interval_notifier_insert(). It cannot

 * be called from any ops callback.

 *

 * Once this returns ops callbacks are no longer running on other CPUs and

 * will not be called in future.

		/*

		 * remove is being called after insert put this on the

		 * deferred list, but before the deferred list was processed.

	/*

	 * The possible sleep on progress in the invalidation requires the

	 * caller not hold any locks held by invalidation callbacks.

 pairs with mmgrab in mmu_interval_notifier_insert() */

/**

 * mmu_notifier_synchronize - Ensure all mmu_notifiers are freed

 *

 * This function ensures that all outstanding async SRU work from

 * mmu_notifier_put() is completed. After it returns any mmu_notifier_ops

 * associated with an unused mmu_notifier will no longer be called.

 *

 * Before using the caller must ensure that all of its mmu_notifiers have been

 * fully released via mmu_notifier_put().

 *

 * Modules using the mmu_notifier_put() API should call this in their __exit

 * function to avoid module unloading races.

 Return true if the vma still have the read flag set. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Virtual Memory Map support

 *

 * (C) 2007 sgi. Christoph Lameter.

 *

 * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,

 * virt_to_page, page_address() to be implemented as a base offset

 * calculation without memory access.

 *

 * However, virtual mappings need a page table and TLBs. Many Linux

 * architectures already map their physical space using 1-1 mappings

 * via TLBs. For those arches the virtual memory map is essentially

 * for free if we use the same page size as the 1-1 mappings. In that

 * case the overhead consists of a few additional pages that are

 * allocated to create a view of memory for vmemmap.

 *

 * The architecture is expected to provide a vmemmap_populate() function

 * to instantiate the mapping.

/**

 * struct vmemmap_remap_walk - walk vmemmap page table

 *

 * @remap_pte:		called for each lowest-level entry (PTE).

 * @nr_walked:		the number of walked pte.

 * @reuse_page:		the page which is reused for the tail vmemmap pages.

 * @reuse_addr:		the virtual address of the @reuse_page page.

 * @vmemmap_pages:	the list head of the vmemmap pages that can be freed

 *			or is mapped from.

 Make pte visible before pmd. See comment in pmd_install(). */

	/*

	 * The reuse_page is found 'first' in table walk before we start

	 * remapping (which is calling @walk->remap_pte).

		/*

		 * Because the reuse address is part of the range that we are

		 * walking, skip the reuse address range.

	/*

	 * We only change the mapping of the vmemmap virtual address range

	 * [@start + PAGE_SIZE, end), so we only need to flush the TLB which

	 * belongs to the range.

/*

 * Free a vmemmap page. A vmemmap page can be allocated from the memblock

 * allocator or buddy allocator. If the PG_reserved flag is set, it means

 * that it allocated from the memblock allocator, just free it via the

 * free_bootmem_page(). Otherwise, use __free_page().

 Free a list of the vmemmap pages */

	/*

	 * Remap the tail pages as read-only to catch illegal write operation

	 * to the tail pages.

/**

 * vmemmap_remap_free - remap the vmemmap virtual address range [@start, @end)

 *			to the page which @reuse is mapped to, then free vmemmap

 *			which the range are mapped to.

 * @start:	start address of the vmemmap virtual address range that we want

 *		to remap.

 * @end:	end address of the vmemmap virtual address range that we want to

 *		remap.

 * @reuse:	reuse address.

 *

 * Return: %0 on success, negative error code otherwise.

	/*

	 * In order to make remapping routine most efficient for the huge pages,

	 * the routine of vmemmap page table walking has the following rules

	 * (see more details from the vmemmap_pte_range()):

	 *

	 * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)

	 *   should be continuous.

	 * - The @reuse address is part of the range [@reuse, @end) that we are

	 *   walking which is passed to vmemmap_remap_range().

	 * - The @reuse address is the first in the complete range.

	 *

	 * So we need to make sure that @start and @reuse meet the above rules.

		/*

		 * vmemmap_pages contains pages from the previous

		 * vmemmap_remap_range call which failed.  These

		 * are pages which were removed from the vmemmap.

		 * They will be restored in the following call.

/**

 * vmemmap_remap_alloc - remap the vmemmap virtual address range [@start, end)

 *			 to the page which is from the @vmemmap_pages

 *			 respectively.

 * @start:	start address of the vmemmap virtual address range that we want

 *		to remap.

 * @end:	end address of the vmemmap virtual address range that we want to

 *		remap.

 * @reuse:	reuse address.

 * @gfp_mask:	GFP flag for allocating vmemmap pages.

 *

 * Return: %0 on success, negative error code otherwise.

 See the comment in the vmemmap_remap_free(). */

/*

 * Allocate a block of memory to be used to back the virtual memory map

 * or to back the page tables that are used to create the mapping.

 * Uses the main allocators if they are available, else bootmem.

 If the main allocator is up use that, fallback to bootmem. */

 need to make sure size is all the same during early stage */

 SPDX-License-Identifier: GPL-2.0

/**

 * struct wp_walk - Private struct for pagetable walk callbacks

 * @range: Range for mmu notifiers

 * @tlbflush_start: Address of first modified pte

 * @tlbflush_end: Address of last modified pte + 1

 * @total: Total number of modified ptes

/**

 * wp_pte - Write-protect a pte

 * @pte: Pointer to the pte

 * @addr: The start of protecting virtual address

 * @end: The end of protecting virtual address

 * @walk: pagetable walk callback argument

 *

 * The function write-protects a pte and records the range in

 * virtual address space of touched ptes for efficient range TLB flushes.

/**

 * struct clean_walk - Private struct for the clean_record_pte function.

 * @base: struct wp_walk we derive from

 * @bitmap_pgoff: Address_space Page offset of the first bit in @bitmap

 * @bitmap: Bitmap with one bit for each page offset in the address_space range

 * covered.

 * @start: Address_space page offset of first modified pte relative

 * to @bitmap_pgoff

 * @end: Address_space page offset of last modified pte relative

 * to @bitmap_pgoff

/**

 * clean_record_pte - Clean a pte and record its address space offset in a

 * bitmap

 * @pte: Pointer to the pte

 * @addr: The start of virtual address to be clean

 * @end: The end of virtual address to be clean

 * @walk: pagetable walk callback argument

 *

 * The function cleans a pte and records the range in

 * virtual address space of touched ptes for efficient TLB flushes.

 * It also records dirty ptes in a bitmap representing page offsets

 * in the address_space, as well as the first and last of the bits

 * touched.

/*

 * wp_clean_pmd_entry - The pagewalk pmd callback.

 *

 * Dirty-tracking should take place on the PTE level, so

 * WARN() if encountering a dirty huge pmd.

 * Furthermore, never split huge pmds, since that currently

 * causes dirty info loss. The pagefault handler should do

 * that if needed.

 Huge pmd, present or migrated */

/*

 * wp_clean_pud_entry - The pagewalk pud callback.

 *

 * Dirty-tracking should take place on the PTE level, so

 * WARN() if encountering a dirty huge puds.

 * Furthermore, never split huge puds, since that currently

 * causes dirty info loss. The pagefault handler should do

 * that if needed.

 Huge pud */

/*

 * wp_clean_pre_vma - The pagewalk pre_vma callback.

 *

 * The pre_vma callback performs the cache flush, stages the tlb flush

 * and calls the necessary mmu notifiers.

	/*

	 * We're not using tlb_gather_mmu() since typically

	 * only a small subrange of PTEs are affected, whereas

	 * tlb_gather_mmu() records the full range.

/*

 * wp_clean_post_vma - The pagewalk post_vma callback.

 *

 * The post_vma callback performs the tlb flush and calls necessary mmu

 * notifiers.

/*

 * wp_clean_test_walk - The pagewalk test_walk callback.

 *

 * Won't perform dirty-tracking on COW, read-only or HUGETLB vmas.

 Skip non-applicable VMAs */

/**

 * wp_shared_mapping_range - Write-protect all ptes in an address space range

 * @mapping: The address_space we want to write protect

 * @first_index: The first page offset in the range

 * @nr: Number of incremental page offsets to cover

 *

 * Note: This function currently skips transhuge page-table entries, since

 * it's intended for dirty-tracking on the PTE level. It will warn on

 * encountering transhuge write-enabled entries, though, and can easily be

 * extended to handle them as well.

 *

 * Return: The number of ptes actually write-protected. Note that

 * already write-protected ptes are not counted.

/**

 * clean_record_shared_mapping_range - Clean and record all ptes in an

 * address space range

 * @mapping: The address_space we want to clean

 * @first_index: The first page offset in the range

 * @nr: Number of incremental page offsets to cover

 * @bitmap_pgoff: The page offset of the first bit in @bitmap

 * @bitmap: Pointer to a bitmap of at least @nr bits. The bitmap needs to

 * cover the whole range @first_index..@first_index + @nr.

 * @start: Pointer to number of the first set bit in @bitmap.

 * is modified as new bits are set by the function.

 * @end: Pointer to the number of the last set bit in @bitmap.

 * none set. The value is modified as new bits are set by the function.

 *

 * Note: When this function returns there is no guarantee that a CPU has

 * not already dirtied new ptes. However it will not clean any ptes not

 * reported in the bitmap. The guarantees are as follows:

 * a) All ptes dirty when the function starts executing will end up recorded

 *    in the bitmap.

 * b) All ptes dirtied after that will either remain dirty, be recorded in the

 *    bitmap or both.

 *

 * If a caller needs to make sure all dirty ptes are picked up and none

 * additional are added, it first needs to write-protect the address-space

 * range and make sure new writers are blocked in page_mkwrite() or

 * pfn_mkwrite(). And then after a TLB flush following the write-protection

 * pick up all dirty bits.

 *

 * This function currently skips transhuge page-table entries, since

 * it's intended for dirty-tracking on the PTE level. It will warn on

 * encountering transhuge dirty entries, though, and can easily be extended

 * to handle them as well.

 *

 * Return: The number of dirty ptes actually cleaned.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Access kernel or user memory without faulting.

 HAVE_GET_KERNEL_NOFAULT */

/**

 * copy_from_kernel_nofault(): safely attempt to read from kernel-space

 * @dst: pointer to the buffer that shall take the data

 * @src: address to read from

 * @size: size of the data chunk

 *

 * Safely read from kernel address @src to the buffer at @dst.  If a kernel

 * fault happens, handle that and return -EFAULT.  If @src is not a valid kernel

 * address, return -ERANGE.

 *

 * We ensure that the copy_from_user is executed in atomic context so that

 * do_page_fault() doesn't attempt to take mmap_lock.  This makes

 * copy_from_kernel_nofault() suitable for use within regions where the caller

 * already holds mmap_lock, or other locks which nest inside mmap_lock.

/**

 * copy_to_kernel_nofault(): safely attempt to write to a location

 * @dst: address to write to

 * @src: pointer to the data that shall be written

 * @size: size of the data chunk

 *

 * Safely write to address @dst from the buffer at @src.  If a kernel fault

 * happens, handle that and return -EFAULT.

/**

 * strncpy_from_kernel_nofault: - Copy a NUL terminated string from unsafe

 *				 address.

 * @dst:   Destination address, in kernel space.  This buffer must be at

 *         least @count bytes long.

 * @unsafe_addr: Unsafe address.

 * @count: Maximum number of bytes to copy, including the trailing NUL.

 *

 * Copies a NUL-terminated string from unsafe address to kernel buffer.

 *

 * On success, returns the length of the string INCLUDING the trailing NUL.

 *

 * If access fails, returns -EFAULT (some data may have been copied and the

 * trailing NUL added).  If @unsafe_addr is not a valid kernel address, return

 * -ERANGE.

 *

 * If @count is smaller than the length of the string, copies @count-1 bytes,

 * sets the last byte of @dst buffer to NUL and returns @count.

 HAVE_GET_KERNEL_NOFAULT */

/**

 * copy_from_user_nofault(): safely attempt to read from a user-space location

 * @dst: pointer to the buffer that shall take the data

 * @src: address to read from. This must be a user address.

 * @size: size of the data chunk

 *

 * Safely read from user address @src to the buffer at @dst. If a kernel fault

 * happens, handle that and return -EFAULT.

/**

 * copy_to_user_nofault(): safely attempt to write to a user-space location

 * @dst: address to write to

 * @src: pointer to the data that shall be written

 * @size: size of the data chunk

 *

 * Safely write to address @dst from the buffer at @src.  If a kernel fault

 * happens, handle that and return -EFAULT.

/**

 * strncpy_from_user_nofault: - Copy a NUL terminated string from unsafe user

 *				address.

 * @dst:   Destination address, in kernel space.  This buffer must be at

 *         least @count bytes long.

 * @unsafe_addr: Unsafe user address.

 * @count: Maximum number of bytes to copy, including the trailing NUL.

 *

 * Copies a NUL-terminated string from unsafe user address to kernel buffer.

 *

 * On success, returns the length of the string INCLUDING the trailing NUL.

 *

 * If access fails, returns -EFAULT (some data may have been copied

 * and the trailing NUL added).

 *

 * If @count is smaller than the length of the string, copies @count-1 bytes,

 * sets the last byte of @dst buffer to NUL and returns @count.

/**

 * strnlen_user_nofault: - Get the size of a user string INCLUDING final NUL.

 * @unsafe_addr: The string to measure.

 * @count: Maximum count (including NUL)

 *

 * Get the size of a NUL-terminated string in user space without pagefault.

 *

 * Returns the size of the string INCLUDING the terminating NUL.

 *

 * If the string is too long, returns a number larger than @count. User

 * has to check the return value against "> count".

 * On exception (or invalid count), returns 0.

 *

 * Unlike strnlen_user, this can be used from IRQ handler etc. because

 * it disables pagefaults.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/mm/page_io.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *

 *  Swap reorganised 29.12.95, 

 *  Asynchronous swapping added 30.12.95. Stephen Tweedie

 *  Removed race in async swapping. 14.4.1996. Bruno Haible

 *  Add swap of shared pages through the page cache. 20.2.1998. Stephen Tweedie

 *  Always use brw_page, life becomes simpler. 12 May 1998 Eric Biederman

		/*

		 * We failed to write the page out to swap-space.

		 * Re-dirty the page in order to avoid it being reclaimed.

		 * Also print a dire warning that things will go BAD (tm)

		 * very quickly.

		 *

		 * Also clear PG_reclaim to avoid folio_rotate_reclaimable()

	/*

	 * There is no guarantee that the page is in swap cache - the software

	 * suspend code (at least) uses end_swap_bio_read() against a non-

	 * swapcache page.  So we must check PG_swapcache before proceeding with

	 * this optimization.

	/*

	 * The swap subsystem performs lazy swap slot freeing,

	 * expecting that the page will be swapped out again.

	 * So we can avoid an unnecessary write if the page

	 * isn't redirtied.

	 * This is good for real swap storage because we can

	 * reduce unnecessary I/O and enhance wear-leveling

	 * if an SSD is used as the as swap device.

	 * But if in-memory swap device (eg zram) is used,

	 * this causes a duplicated copy between uncompressed

	 * data in VM-owned memory and compressed data in

	 * zram-owned memory.  So let's free zram-owned memory

	 * and make the VM-owned decompressed page *dirty*,

	 * so the page should be swapped out somewhere again if

	 * we again wish to reclaim it.

	/*

	 * Map all the blocks into the extent tree.  This code doesn't try

	 * to be very smart.

		/*

		 * It must be PAGE_SIZE aligned on-disk

 Discontiguity */

 exclude the header page */

		/*

		 * We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks

 force Empty message */

/*

 * We may have stale swap cache pages in memory: notice

 * them here and get rid of the unnecessary final write.

	/*

	 * Arch code may have to preserve more data than just the page

	 * contents, e.g. memory tags.

 CONFIG_MEMCG && CONFIG_BLK_CGROUP */

			/*

			 * In the case of swap-over-nfs, this can be a

			 * temporary failure if the system has limited

			 * memory for allocating transmit buffers.

			 * Mark the page dirty and avoid

			 * folio_rotate_reclaimable but rate-limit the

			 * messages but do not flag PageError like

			 * the normal direct-to-bio case as it could

			 * be temporary.

	/*

	 * Count submission time as memory stall. When the device is congested,

	 * or the submitting cgroup IO-throttled, submission can be a

	 * significant part of overall IO time.

	/*

	 * Keep this task valid during swap readpage because the oom killer may

	 * attempt to access it in the page fault retry time check.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2013 Red Hat Inc.

 *

 * Authors: Jérôme Glisse <jglisse@redhat.com>

/*

 * Refer to include/linux/hmm.h for information about heterogeneous memory

 * management or HMM for short.

/*

 * hmm_vma_fault() - fault in a range lacking valid pmd or pte(s)

 * @addr: range virtual start address (inclusive)

 * @end: range virtual end address (exclusive)

 * @required_fault: HMM_NEED_* flags

 * @walk: mm_walk structure

 * Return: -EBUSY after page fault, or page fault error

 *

 * This function will be called whenever pmd_none() or pte_none() returns true,

 * or whenever there is no page directory covering the virtual address range.

	/*

	 * So we not only consider the individual per page request we also

	 * consider the default flags requested for the range. The API can

	 * be used 2 ways. The first one where the HMM user coalesces

	 * multiple page faults into one request and sets flags per pfn for

	 * those faults. The second one where the HMM user wants to pre-

	 * fault a range with specific flags. For the latter one it is a

	 * waste to have the user pre-fill the pfn arrays with a default

	 * flags value.

 We aren't ask to do anything ... */

 Need to write fault ? */

 If CPU page table is not valid then we need to fault */

	/*

	 * If the default flags do not request to fault pages, and the mask does

	 * not allow for individual pages to be faulted, then

	 * hmm_pte_need_fault() will always return 0.

 CONFIG_TRANSPARENT_HUGEPAGE */

 stub to allow the code below to compile */

 CONFIG_TRANSPARENT_HUGEPAGE */

		/*

		 * Never fault in device private pages, but just report

		 * the PFN even if not present.

 Report error for everything else */

	/*

	 * Bypass devmap pte such as DAX page when all pfn requested

	 * flags(pfn_req_flags) are fulfilled.

	 * Since each architecture defines a struct page for the zero page, just

	 * fall through and treat it like a normal page.

 Fault any virtual address we were asked to fault */

		/*

		 * No need to take pmd_lock here, even if some other thread

		 * is splitting the huge pmd we will get that event through

		 * mmu_notifier callback.

		 *

		 * So just read pmd value and check again it's a transparent

		 * huge or device mapping one and compute corresponding pfn

		 * values.

	/*

	 * We have handled all the valid cases above ie either none, migration,

	 * huge or transparent huge. At this point either it is a valid pmd

	 * entry pointing to pte directory or it is a bad pmd that will not

	 * recover.

 hmm_vma_handle_pte() did pte_unmap() */

 Normally we don't want to split the huge page */

 Ask for the PUD to be split */

 CONFIG_HUGETLB_PAGE */

	/*

	 * vma ranges that don't have struct page backing them or map I/O

	 * devices directly cannot be handled by hmm_range_fault().

	 *

	 * If the vma does not allow read access, then assume that it does not

	 * allow write access either. HMM does not support architectures that

	 * allow write without read.

	 *

	 * If a fault is requested for an unsupported range then it is a hard

	 * failure.

 Skip this vma and continue processing the next vma. */

/**

 * hmm_range_fault - try to fault some address in a virtual address range

 * @range:	argument structure

 *

 * Returns 0 on success or one of the following error codes:

 *

 * -EINVAL:	Invalid arguments or mm or virtual address is in an invalid vma

 *		(e.g., device file vma).

 * -ENOMEM:	Out of memory.

 * -EPERM:	Invalid permission (e.g., asking for write and range is read

 *		only).

 * -EBUSY:	The range has been invalidated and the caller needs to wait for

 *		the invalidation to finish.

 * -EFAULT:     A page was requested to be valid and could not be made valid

 *              ie it has no backing VMA or it is illegal to access

 *

 * This is similar to get_user_pages(), except that it can read the page tables

 * without mutating them (ie causing faults).

 If range is no longer valid force retry. */

		/*

		 * When -EBUSY is returned the loop restarts with

		 * hmm_vma_walk.last set to an address that has not been stored

		 * in pfns. All entries < last in the pfn array are set to their

		 * output, and all >= are still at their input values.

 SPDX-License-Identifier: GPL-2.0-only

 Inject a hwpoison memory failure on a arbitrary pfn */

	/*

	 * This implies unable to support non-LRU pages.

	/*

	 * do a racy check to make sure PG_hwpoison will only be set for

	 * the targeted owner (or on a free page).

	 * memory_failure() will redo the check reliably inside page lock.

	/*

	 * Note that the below poison/unpoison interfaces do not involve

	 * hardware status change, hence do not require hardware support.

	 * They are mainly for testing hwpoison in software level.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic hugetlb support.

 * (C) Nadia Yvette Chambers, April 2004

/*

 * Minimum page order among possible hugepage sizes, set to a proper value

 * at boot time.

 for command line parsing */

/*

 * Protects updates to hugepage_freelists, hugepage_activelist, nr_huge_pages,

 * free_huge_pages, and surplus_huge_pages.

/*

 * Serializes faults on the same logical page.  This is used to

 * prevent spurious OOMs when the hugepage pool is fully utilized.

 Forward declaration */

	/* If no pages are used, and no other handles to the subpool

	 * remain, give up any reservations based on minimum size and

/*

 * Subpool accounting for allocating and reserving pages.

 * Return -ENOMEM if there are not enough resources to satisfy the

 * request.  Otherwise, return the number of pages by which the

 * global pools must be adjusted (upward).  The returned value may

 * only be different than the passed value (delta) in the case where

 * a subpool minimum size must be maintained.

 maximum size accounting */

 minimum size accounting */

			/*

			 * Asking for more reserves than those already taken on

			 * behalf of subpool.  Return difference.

 reserves already accounted for */

/*

 * Subpool accounting for freeing and unreserving pages.

 * Return the number of global page reservations that must be dropped.

 * The return value may only be different than the passed value (delta)

 * in the case where a subpool minimum size must be maintained.

 maximum size accounting */

 minimum size accounting */

	/*

	 * If hugetlbfs_put_super couldn't free spool due to an outstanding

	 * quota reference, free it now.

/* Helper that removes a struct file_region from the resv_map cache and returns

 * it for use.

 Helper that records hugetlb_cgroup uncharge info. */

		/*

		 * The caller will hold exactly one h_cg->css reference for the

		 * whole contiguous reservation region. But this area might be

		 * scattered when there are already some file_regions reside in

		 * it. As a result, many file_regions may share only one css

		 * reference. In order to ensure that one file_region must hold

		 * exactly one h_cg->css reference, we should do css_get for

		 * each file_region and leave the reference held by caller

		 * untouched.

		/* pages_per_hpage should be the same for all entries in

		 * a resv_map.

/*

 * Must be called with resv->lock held.

 *

 * Calling this with regions_needed != NULL will count the number of pages

 * to be added but will not modify the linked list. And regions_needed will

 * indicate the number of file_regions needed in the cache to carry out to add

 * the regions for this range.

	/* In this loop, we essentially handle an entry for the range

	 * [last_accounted_offset, rg->from), at every iteration, with some

	 * bounds checking.

 Skip irrelevant regions that start before our range. */

			/* If this region ends after the last accounted offset,

			 * then we need to update last_accounted_offset.

		/* When we find a region that starts beyond our range, we've

		 * finished.

		/* Add an entry for last_accounted_offset -> rg->from, and

		 * update last_accounted_offset.

	/* Handle the case where our range extends beyond

	 * last_accounted_offset.

/* Must be called with resv->lock acquired. Will drop lock to allocate entries.

	/*

	 * Check for sufficient descriptors in the cache to accommodate

	 * the number of in progress add operations plus regions_needed.

	 *

	 * This is a while loop because when we drop the lock, some other call

	 * to region_add or region_del may have consumed some region_entries,

	 * so we keep looping here until we finally have enough entries for

	 * (adds_in_progress + regions_needed).

		/* At this point, we should have enough entries in the cache

		 * for all the existing adds_in_progress. We should only be

		 * needing to allocate for regions_needed.

/*

 * Add the huge page range represented by [f, t) to the reserve

 * map.  Regions will be taken from the cache to fill in this range.

 * Sufficient regions should exist in the cache due to the previous

 * call to region_chg with the same range, but in some cases the cache will not

 * have sufficient entries due to races with other code doing region_add or

 * region_del.  The extra needed entries will be allocated.

 *

 * regions_needed is the out value provided by a previous call to region_chg.

 *

 * Return the number of new huge pages added to the map.  This number is greater

 * than or equal to zero.  If file_region entries needed to be allocated for

 * this operation and we were not able to allocate, it returns -ENOMEM.

 * region_add of regions of length 1 never allocate file_regions and cannot

 * fail; region_chg will always allocate at least 1 entry and a region_add for

 * 1 page will only require at most 1 entry.

 Count how many regions are actually needed to execute this add. */

	/*

	 * Check for sufficient descriptors in the cache to accommodate

	 * this add operation. Note that actual_regions_needed may be greater

	 * than in_regions_needed, as the resv_map may have been modified since

	 * the region_chg call. In this case, we need to make sure that we

	 * allocate extra entries, such that we have enough for all the

	 * existing adds_in_progress, plus the excess needed for this

	 * operation.

		/* region_add operation of range 1 should never need to

		 * allocate file_region entries.

/*

 * Examine the existing reserve map and determine how many

 * huge pages in the specified range [f, t) are NOT currently

 * represented.  This routine is called before a subsequent

 * call to region_add that will actually modify the reserve

 * map to add the specified range [f, t).  region_chg does

 * not change the number of huge pages represented by the

 * map.  A number of new file_region structures is added to the cache as a

 * placeholder, for the subsequent region_add call to use. At least 1

 * file_region structure is added.

 *

 * out_regions_needed is the number of regions added to the

 * resv->adds_in_progress.  This value needs to be provided to a follow up call

 * to region_add or region_abort for proper accounting.

 *

 * Returns the number of huge pages that need to be added to the existing

 * reservation map for the range [f, t).  This number is greater or equal to

 * zero.  -ENOMEM is returned if a new file_region structure or cache entry

 * is needed and can not be allocated.

 Count how many hugepages in this range are NOT represented. */

/*

 * Abort the in progress add operation.  The adds_in_progress field

 * of the resv_map keeps track of the operations in progress between

 * calls to region_chg and region_add.  Operations are sometimes

 * aborted after the call to region_chg.  In such cases, region_abort

 * is called to decrement the adds_in_progress counter. regions_needed

 * is the value returned by the region_chg call, it is used to decrement

 * the adds_in_progress counter.

 *

 * NOTE: The range arguments [f, t) are not needed or used in this

 * routine.  They are kept to make reading the calling code easier as

 * arguments will match the associated region_chg call.

/*

 * Delete the specified range [f, t) from the reserve map.  If the

 * t parameter is LONG_MAX, this indicates that ALL regions after f

 * should be deleted.  Locate the regions which intersect [f, t)

 * and either trim, delete or split the existing regions.

 *

 * Returns the number of huge pages deleted from the reserve map.

 * In the normal case, the return value is zero or more.  In the

 * case where a region must be split, a new region descriptor must

 * be allocated.  If the allocation fails, -ENOMEM will be returned.

 * NOTE: If the parameter t == LONG_MAX, then we will never split

 * a region and possibly return -ENOMEM.  Callers specifying

 * t == LONG_MAX do not need to check for -ENOMEM error.

		/*

		 * Skip regions before the range to be deleted.  file_region

		 * ranges are normally of the form [from, to).  However, there

		 * may be a "placeholder" entry in the map which is of the form

		 * (from, to) with from == to.  Check for placeholder entries

		 * at the beginning of the range to be deleted.

 Must split region */

			/*

			 * Check for an entry in the cache before dropping

			 * lock and attempting allocation.

 New entry for end of split region */

 Original entry is trimmed */

 Remove entire region */

 Trim beginning of region */

 Trim end of region */

/*

 * A rare out of memory error was encountered which prevented removal of

 * the reserve map region for a page.  The huge page itself was free'ed

 * and removed from the page cache.  This routine will adjust the subpool

 * usage count, and the global reserve count if needed.  By incrementing

 * these counts, the reserve map entry which could not be deleted will

 * appear as a "reserved" entry instead of simply dangling with incorrect

 * counts.

/*

 * Count and return the number of huge pages in the reserve map

 * that intersect with the range [f, t).

 Locate each segment we overlap with, and count that overlap. */

/*

 * Convert the address within this vma to the page offset within

 * the mapping, in pagecache page units; huge pages here.

/*

 * Return the size of the pages allocated when backing a VMA. In the majority

 * cases this will be same size as used by the page table entries.

/*

 * Return the page size being used by the MMU to back a VMA. In the majority

 * of cases, the page size used by the kernel matches the MMU size. On

 * architectures where it differs, an architecture-specific 'strong'

 * version of this symbol is required.

/*

 * Flags for MAP_PRIVATE reservations.  These are stored in the bottom

 * bits of the reservation map pointer, which are always clear due to

 * alignment.

/*

 * These helpers are used to track how many pages are reserved for

 * faults in a MAP_PRIVATE mapping. Only the process that called mmap()

 * is guaranteed to have their future faults succeed.

 *

 * With the exception of reset_vma_resv_huge_pages() which is called at fork(),

 * the reserve counters are updated with the hugetlb_lock held. It is safe

 * to reset the VMA at fork() time as it is not in use yet and there is no

 * chance of the global counters getting corrupted as a result of the values.

 *

 * The private mapping reservation is represented in a subtly different

 * manner to a shared mapping.  A shared mapping has a region map associated

 * with the underlying file, this region map represents the backing file

 * pages which have ever had a reservation assigned which this persists even

 * after the page is instantiated.  A private mapping has a region map

 * associated with the original mmap which is attached to all VMAs which

 * reference it, this region map represents those offsets which have consumed

 * reservation ie. where pages have been instantiated.

	/*

	 * Initialize these to 0. On shared mappings, 0's here indicate these

	 * fields don't do cgroup accounting. On private mappings, these will be

	 * re-initialized to the proper values, to indicate that hugetlb cgroup

	 * reservations are to be un-charged from here.

 Clear out any active regions before we release the map. */

 ... and any entries left in the cache */

	/*

	 * At inode evict time, i_mapping may not point to the original

	 * address space within the inode.  This original address space

	 * contains the pointer to the resv_map.  So, always use the

	 * address space embedded within the inode.

	 * The VERY common case is inode->mapping == &inode->i_data but,

	 * this may not be true for device special inodes.

 Reset counters to 0 and clear all HPAGE_RESV_* flags */

/*

 * Reset and decrement one ref on hugepage private reservation.

 * Called with mm->mmap_sem writer semaphore held.

 * This function should be only used by move_vma() and operate on

 * same sized vma. It should never come here with last ref on the

 * reservation.

	/*

	 * Clear the old hugetlb private page reservation.

	 * It has already been transferred to new_vma.

	 *

	 * During a mremap() operation of a hugetlb vma we call move_vma()

	 * which copies vma into new_vma and unmaps vma. After the copy

	 * operation both new_vma and vma share a reference to the resv_map

	 * struct, and at that point vma is about to be unmapped. We don't

	 * want to return the reservation to the pool at unmap of vma because

	 * the reservation still lives on in new_vma, so simply decrement the

	 * ref here and remove the resv_map reference from this vma.

 Returns true if the VMA has associated reserve pages */

		/*

		 * This address is already reserved by other process(chg == 0),

		 * so, we should decrement reserved count. Without decrementing,

		 * reserve count remains after releasing inode, because this

		 * allocated page will go into page cache and is regarded as

		 * coming from reserved pool in releasing step.  Currently, we

		 * don't have any other solution to deal with this situation

		 * properly, so add work-around here.

 Shared mappings always use reserves */

		/*

		 * We know VM_NORESERVE is not set.  Therefore, there SHOULD

		 * be a region map for all pages.  The only situation where

		 * there is no region map is if a hole was punched via

		 * fallocate.  In this case, there really are no reserves to

		 * use.  This situation is indicated if chg != 0.

	/*

	 * Only the process that called mmap() has reserves for

	 * private mappings.

		/*

		 * Like the shared case above, a hole punch or truncate

		 * could have been performed on the private mapping.

		 * Examine the value of chg to determine if reserves

		 * actually exist or were previously consumed.

		 * Very Subtle - The value of chg comes from a previous

		 * call to vma_needs_reserves().  The reserve map for

		 * private mappings has different (opposite) semantics

		 * than that of shared mappings.  vma_needs_reserves()

		 * has already taken this difference in semantics into

		 * account.  Therefore, the meaning of chg is the same

		 * as in the shared case above.  Code could easily be

		 * combined, but keeping it separate draws attention to

		 * subtle differences.

		/*

		 * no need to ask again on the same node. Pool is node rather than

		 * zone aware

	/*

	 * A child process with MAP_PRIVATE mappings created by their parent

	 * have no page reserves. This check ensures that reservations are

	 * not "stolen". The child may still get SIGKILLed

 If reserves cannot be used, ensure enough pages are in the pool */

 Fallback to all nodes if page==NULL */

/*

 * common helper functions for hstate_next_node_to_{alloc|free}.

 * We may have allocated or freed a huge page based on a different

 * nodes_allowed previously, so h->next_node_to_{alloc|free} might

 * be outside of *nodes_allowed.  Ensure that we use an allowed

 * node for alloc or free.

/*

 * returns the previously saved node ["this node"] from which to

 * allocate a persistent huge page for the pool and advance the

 * next node from which to allocate, handling wrap at end of node

 * mask.

/*

 * helper for remove_pool_huge_page() - return the previously saved

 * node ["this node"] from which to free a huge page.  Advance the

 * next node id whether or not we find a free huge page to free so

 * that the next attempt to free addresses the next node.

 used to demote non-gigantic_huge pages as well */

	/*

	 * If the page isn't allocated using the cma allocator,

	 * cma_release() returns false.

 !CONFIG_CONTIG_ALLOC */

 CONFIG_CONTIG_ALLOC */

 !CONFIG_ARCH_HAS_GIGANTIC_PAGE */

/*

 * Remove hugetlb page from lists, and update dtor so that page appears

 * as just a compound page.

 *

 * A reference is held on the page, except in the case of demote.

 *

 * Must be called with hugetlb lock held.

	/*

	 * Very subtle

	 *

	 * For non-gigantic pages set the destructor to the normal compound

	 * page dtor.  This is needed in case someone takes an additional

	 * temporary ref to the page, and freeing is delayed until they drop

	 * their reference.

	 *

	 * For gigantic pages set the destructor to the null dtor.  This

	 * destructor will never be called.  Before freeing the gigantic

	 * page destroy_compound_gigantic_page will turn the compound page

	 * into a simple group of pages.  After this the destructor does not

	 * apply.

	 *

	 * This handles the case where more than one ref is held when and

	 * after update_and_free_page is called.

	 *

	 * In the case of demote we do not ref count the page as it will soon

	 * be turned into a page of smaller size.

	/*

	 * This page is about to be managed by the hugetlb allocator and

	 * should have no users.  Drop our reference, and check for others

	 * just in case.

		/*

		 * It is VERY unlikely soneone else has taken a ref on

		 * the page.  In this case, we simply return as the

		 * hugetlb destructor (free_huge_page) will be called

		 * when this other ref is dropped.

		/*

		 * If we cannot allocate vmemmap pages, just refuse to free the

		 * page and put the page back on the hugetlb free list and treat

		 * as a surplus page.

	/*

	 * Non-gigantic pages demoted from CMA allocated gigantic pages

	 * need to be given back to CMA in free_gigantic_page.

/*

 * As update_and_free_page() can be called under any context, so we cannot

 * use GFP_KERNEL to allocate vmemmap pages. However, we can defer the

 * actual freeing in a workqueue to prevent from using GFP_ATOMIC to allocate

 * the vmemmap pages.

 *

 * free_hpage_workfn() locklessly retrieves the linked list of pages to be

 * freed and frees them one-by-one. As the page->mapping pointer is going

 * to be cleared in free_hpage_workfn() anyway, it is reused as the llist_node

 * structure of a lockless linked list of huge pages to be freed.

		/*

		 * The VM_BUG_ON_PAGE(!PageHuge(page), page) in page_hstate()

		 * is going to trigger because a previous call to

		 * remove_hugetlb_page() will set_compound_page_dtor(page,

		 * NULL_COMPOUND_DTOR), so do not use page_hstate() directly.

	/*

	 * Defer freeing to avoid using GFP_ATOMIC to allocate vmemmap pages.

	 *

	 * Only call schedule_work() if hpage_freelist is previously

	 * empty. Otherwise, schedule_work() had been called but the workfn

	 * hasn't retrieved the list yet.

	/*

	 * Can't pass hstate in here because it is called from the

	 * compound page destructor.

	/*

	 * If HPageRestoreReserve was set on page, page allocation consumed a

	 * reservation.  If the page was associated with a subpool, there

	 * would have been a page reserved in the subpool before allocation

	 * via hugepage_subpool_get_pages().  Since we are 'restoring' the

	 * reservation, do not call hugepage_subpool_put_pages() as this will

	 * remove the reserved page from the subpool.

		/*

		 * A return code of zero implies that the subpool will be

		 * under its minimum size if the reservation is not restored

		 * after page is free.  Therefore, force restore_reserve

		 * operation.

 remove the page from active list */

/*

 * Must be called with the hugetlb lock held

 we rely on prep_new_huge_page to set the destructor */

		/*

		 * For gigantic hugepages allocated through bootmem at

		 * boot, it's safer to be consistent with the not-gigantic

		 * hugepages and clear the PG_reserved bit from all tail pages

		 * too.  Otherwise drivers using get_user_pages() to access tail

		 * pages may get the reference counting wrong if they see

		 * PG_reserved set on a tail page (despite the head page not

		 * having PG_reserved set).  Enforcing this consistency between

		 * head and tail pages allows drivers to optimize away a check

		 * on the head page when they need know if put_page() is needed

		 * after get_user_pages().

		/*

		 * Subtle and very unlikely

		 *

		 * Gigantic 'page allocators' such as memblock or cma will

		 * return a set of pages with each page ref counted.  We need

		 * to turn this set of pages into a compound page with tail

		 * page ref counts set to zero.  Code such as speculative page

		 * cache adding could take a ref on a 'to be' tail page.

		 * We need to respect any increased ref count, and only set

		 * the ref count to zero if count is currently 1.  If count

		 * is not 1, we return an error.  An error return indicates

		 * the set of pages can not be converted to a gigantic page.

		 * The caller who allocated the pages should then discard the

		 * pages using the appropriate free interface.

		 *

		 * In the case of demote, the ref count will be zero.

 undo tail page modifications made above */

 need to clear PG_reserved on remaining tail pages  */

/*

 * PageHuge() only returns true for hugetlbfs pages, but not for normal or

 * transparent huge pages.  See the PageTransHuge() documentation for more

 * details.

/*

 * PageHeadHuge() only returns true for hugetlbfs head page, but not for

 * normal or transparent huge pages.

/*

 * Find and lock address space (mapping) in write mode.

 *

 * Upon entry, the page is locked which means that page_mapping() is

 * stable.  Due to locking order, we can only trylock_write.  If we can

 * not get the lock, simply return NULL to caller.

	/*

	 * By default we always try hard to allocate the page with

	 * __GFP_RETRY_MAYFAIL flag.  However, if we are allocating pages in

	 * a loop (to adjust global huge page counts) and previous allocation

	 * failed, do not continue to try hard on the same node.  Use the

	 * node_alloc_noretry bitmap to manage this state information.

	/*

	 * If we did not specify __GFP_RETRY_MAYFAIL, but still got a page this

	 * indicates an overall state change.  Clear bit so that we resume

	 * normal 'try hard' allocations.

	/*

	 * If we tried hard to get a page but failed, set bit so that

	 * subsequent attempts will not try as hard until there is an

	 * overall state change.

/*

 * Common helper to allocate a fresh hugetlb page. All specific allocators

 * should use this function to get new hugetlb pages

			/*

			 * Rare failure to convert pages to compound page.

			 * Free pages and try again - ONCE!

/*

 * Allocates a fresh page to the hugetlb allocator pool in the node interleaved

 * manner.

 free it into the hugepage allocator */

/*

 * Remove huge page from pool from next node to free.  Attempt to keep

 * persistent huge pages more or less balanced over allowed nodes.

 * This routine only 'removes' the hugetlb page.  The caller must make

 * an additional call to free the page to low level allocators.

 * Called with hugetlb_lock locked.

		/*

		 * If we're returning unused surplus pages, only examine

		 * nodes with surplus pages.

/*

 * Dissolve a given free hugepage into free buddy pages. This function does

 * nothing for in-use hugepages and non-hugepages.

 * This function returns values like below:

 *

 *  -ENOMEM: failed to allocate vmemmap pages to free the freed hugepages

 *           when the system is under memory pressure and the feature of

 *           freeing unused vmemmap pages associated with each hugetlb page

 *           is enabled.

 *  -EBUSY:  failed to dissolved free hugepages or the hugepage is in-use

 *           (allocated or reserved.)

 *       0:  successfully dissolved free hugepages or the page is not a

 *           hugepage (considered as already dissolved)

 Not to disrupt normal path by vainly holding hugetlb_lock */

		/*

		 * We should make sure that the page is already on the free list

		 * when it is dissolved.

			/*

			 * Theoretically, we should return -EBUSY when we

			 * encounter this race. In fact, we have a chance

			 * to successfully dissolve the page if we do a

			 * retry. Because the race window is quite small.

			 * If we seize this opportunity, it is an optimization

			 * for increasing the success rate of dissolving page.

		/*

		 * Normally update_and_free_page will allocate required vmemmmap

		 * before freeing the page.  update_and_free_page will fail to

		 * free the page if it can not allocate required vmemmap.  We

		 * need to adjust max_huge_pages if the page is not freed.

		 * Attempt to allocate vmemmmap here so that we can take

		 * appropriate action on failure.

			/*

			 * Move PageHWPoison flag from head page to the raw

			 * error page, which makes any subpages rather than

			 * the error page reusable.

/*

 * Dissolve free hugepages in a given pfn range. Used by memory hotplug to

 * make specified memory blocks removable from the system.

 * Note that this will dissolve a free gigantic hugepage completely, if any

 * part of it lies within the given range.

 * Also note that if dissolve_free_huge_page() returns with an error, all

 * free hugepages that were dissolved before that error are lost.

/*

 * Allocates a fresh surplus page from the page allocator.

	/*

	 * We could have raced with the pool size change.

	 * Double check that and simply deallocate the new page

	 * if we would end up overcommiting the surpluses. Abuse

	 * temporary page to workaround the nasty free_huge_page

	 * codeflow

		/*

		 * Caller requires a page with zero ref count.

		 * We will drop ref count here.  If someone else is holding

		 * a ref, the page will be freed when they drop it.  Abuse

		 * temporary page flag to accomplish this.

			/*

			 * Unexpected inflated ref count on freshly allocated

			 * huge.  Retry once.

	/*

	 * We do not account these pages as surplus because they are only

	 * temporary and will be released properly on the last reference

/*

 * Use the VMA's mpolicy to allocate a huge page from the buddy.

 Fallback to all nodes if page==NULL */

 page migration callback function */

 mempolicy aware migration callback */

/*

 * Increase the hugetlb pool such that it can accommodate a reservation

 * of size 'delta'.

	/*

	 * After retaking hugetlb_lock, we need to recalculate 'needed'

	 * because either resv_huge_pages or free_huge_pages may have changed.

		/*

		 * We were not able to allocate enough pages to

		 * satisfy the entire reservation so we free what

		 * we've allocated so far.

	/*

	 * The surplus_list now contains _at_least_ the number of extra pages

	 * needed to accommodate the reservation.  Add the appropriate number

	 * of pages to the hugetlb pool and free the extras back to the buddy

	 * allocator.  Commit the entire reservation here to prevent another

	 * process from stealing the pages as they are added to the pool but

	 * before they are reserved.

 Free the needed pages to the hugetlb pool */

 Add the page to the hugetlb allocator */

	/*

	 * Free unnecessary surplus pages to the buddy allocator.

	 * Pages have no ref count, call free_huge_page directly.

/*

 * This routine has two main purposes:

 * 1) Decrement the reservation count (resv_huge_pages) by the value passed

 *    in unused_resv_pages.  This corresponds to the prior adjustments made

 *    to the associated reservation map.

 * 2) Free any unused surplus pages that may have been allocated to satisfy

 *    the reservation.  As many as unused_resv_pages may be freed.

 Uncommit the reservation */

 Cannot return gigantic pages currently */

	/*

	 * Part (or even all) of the reservation could have been backed

	 * by pre-allocated pages. Only free surplus pages.

	/*

	 * We want to release as many surplus pages as possible, spread

	 * evenly across all nodes with memory. Iterate across these nodes

	 * until we can no longer free unreserved surplus pages. This occurs

	 * when the nodes with surplus pages have no free pages.

	 * remove_pool_huge_page() will balance the freed pages across the

	 * on-line nodes with memory and will handle the hstate accounting.

/*

 * vma_needs_reservation, vma_commit_reservation and vma_end_reservation

 * are used by the huge page allocation routines to manage reservations.

 *

 * vma_needs_reservation is called to determine if the huge page at addr

 * within the vma has an associated reservation.  If a reservation is

 * needed, the value 1 is returned.  The caller is then responsible for

 * managing the global reservation and subpool usage counts.  After

 * the huge page has been allocated, vma_commit_reservation is called

 * to add the page to the reservation map.  If the page allocation fails,

 * the reservation must be ended instead of committed.  vma_end_reservation

 * is called in such cases.

 *

 * In the normal case, vma_commit_reservation returns the same value

 * as the preceding vma_needs_reservation call.  The only time this

 * is not the case is if a reserve map was changed between calls.  It

 * is the responsibility of the caller to notice the difference and

 * take appropriate action.

 *

 * vma_add_reservation is used in error paths where a reservation must

 * be restored when a newly allocated huge page must be freed.  It is

 * to be called after calling vma_needs_reservation to determine if a

 * reservation exists.

 *

 * vma_del_reservation is used in error paths where an entry in the reserve

 * map was created during huge page allocation and must be removed.  It is to

 * be called after calling vma_needs_reservation to determine if a reservation

 * exists.

		/* We assume that vma_reservation_* routines always operate on

		 * 1 page, and that adding to resv map a 1 page entry can only

		 * ever require 1 region.

 region_add calls of range 1 should never fail. */

 region_add calls of range 1 should never fail. */

 region_add calls of range 1 should never fail. */

	/*

	 * We know private mapping must have HPAGE_RESV_OWNER set.

	 *

	 * In most cases, reserves always exist for private mappings.

	 * However, a file associated with mapping could have been

	 * hole punched or truncated after reserves were consumed.

	 * As subsequent fault on such a range will not use reserves.

	 * Subtle - The reserve map for private mappings has the

	 * opposite meaning than that of shared mappings.  If NO

	 * entry is in the reserve map, it means a reservation exists.

	 * If an entry exists in the reserve map, it means the

	 * reservation has already been consumed.  As a result, the

	 * return value of this routine is the opposite of the

	 * value returned from reserve map manipulation routines above.

/*

 * This routine is called to restore reservation information on error paths.

 * It should ONLY be called for pages allocated via alloc_huge_page(), and

 * the hugetlb mutex should remain held when calling this routine.

 *

 * It handles two specific cases:

 * 1) A reservation was in place and the page consumed the reservation.

 *    HPageRestoreReserve is set in the page.

 * 2) No reservation was in place for the page, so HPageRestoreReserve is

 *    not set.  However, alloc_huge_page always updates the reserve map.

 *

 * In case 1, free_huge_page later in the error path will increment the

 * global reserve count.  But, free_huge_page does not have enough context

 * to adjust the reservation map.  This case deals primarily with private

 * mappings.  Adjust the reserve map here to be consistent with global

 * reserve count adjustments to be made by free_huge_page.  Make sure the

 * reserve map indicates there is a reservation present.

 *

 * In case 2, simply undo reserve map modifications done by alloc_huge_page.

			/*

			 * Rare out of memory condition in reserve map

			 * manipulation.  Clear HPageRestoreReserve so that

			 * global reserve count will not be incremented

			 * by free_huge_page.  This will make it appear

			 * as though the reservation for this page was

			 * consumed.  This may prevent the task from

			 * faulting in the page at a later time.  This

			 * is better than inconsistent global huge page

			 * accounting of reserve counts.

			/*

			 * This indicates there is an entry in the reserve map

			 * not added by alloc_huge_page.  We know it was added

			 * before the alloc_huge_page call, otherwise

			 * HPageRestoreReserve would be set on the page.

			 * Remove the entry so that a subsequent allocation

			 * does not consume a reservation.

				/*

				 * VERY rare out of memory condition.  Since

				 * we can not delete the entry, set

				 * HPageRestoreReserve so that the reserve

				 * count will be incremented when the page

				 * is freed.  This reserve will be consumed

				 * on a subsequent allocation.

			/*

			 * Rare out of memory condition from

			 * vma_needs_reservation call.  Memory allocation is

			 * only attempted if a new entry is needed.  Therefore,

			 * this implies there is not an entry in the

			 * reserve map.

			 *

			 * For shared mappings, no entry in the map indicates

			 * no reservation.  We are done.

				/*

				 * For private mappings, no entry indicates

				 * a reservation is present.  Since we can

				 * not add an entry, set SetHPageRestoreReserve

				 * on the page so reserve count will be

				 * incremented when freed.  This reserve will

				 * be consumed on a subsequent allocation.

			/*

			 * No reservation present, do nothing

/*

 * alloc_and_dissolve_huge_page - Allocate a new page and dissolve the old one

 * @h: struct hstate old page belongs to

 * @old_page: Old page to dissolve

 * @list: List to isolate the page in case we need to

 * Returns 0 on success, otherwise negated error.

	/*

	 * Before dissolving the page, we need to allocate a new one for the

	 * pool to remain stable.  Here, we allocate the page and 'prep' it

	 * by doing everything but actually updating counters and adding to

	 * the pool.  This simplifies and let us do most of the processing

	 * under the lock.

	/*

	 * If all goes well, this page will be directly added to the free

	 * list in the pool.  For this the ref count needs to be zero.

	 * Attempt to drop now, and retry once if needed.  It is VERY

	 * unlikely there is another ref on the page.

	 *

	 * If someone else has a reference to the page, it will be freed

	 * when they drop their ref.  Abuse temporary page flag to accomplish

	 * this.  Retry once if there is an inflated ref count.

		/*

		 * Freed from under us. Drop new_page too.

		/*

		 * Someone has grabbed the page, try to isolate it here.

		 * Fail with -EBUSY if not possible.

		/*

		 * Page's refcount is 0 but it has not been enqueued in the

		 * freelist yet. Race window is small, so we can succeed here if

		 * we retry.

		/*

		 * Ok, old_page is still a genuine free hugepage. Remove it from

		 * the freelist and decrease the counters. These will be

		 * incremented again when calling __prep_account_new_huge_page()

		 * and enqueue_huge_page() for new_page. The counters will remain

		 * stable since this happens under the lock.

		/*

		 * Ref count on new page is already zero as it was dropped

		 * earlier.  It can be directly added to the pool free list.

		/*

		 * Pages have been replaced, we can safely free the old one.

 Page has a zero ref count, but needs a ref to be freed */

	/*

	 * The page might have been dissolved from under our feet, so make sure

	 * to carefully check the state under the lock.

	 * Return success when racing as if we dissolved the page ourselves.

	/*

	 * Fence off gigantic pages as there is a cyclic dependency between

	 * alloc_contig_range and them. Return -ENOMEM as this has the effect

	 * of bailing out right away without further retrying.

	/*

	 * Examine the region/reserve map to determine if the process

	 * has a reservation for the page to be allocated.  A return

	 * code of zero indicates a reservation exists (no change).

	/*

	 * Processes that did not create the mapping will have no

	 * reserves as indicated by the region/reserve map. Check

	 * that the allocation will not exceed the subpool limit.

	 * Allocations for MAP_NORESERVE mappings also need to be

	 * checked against any subpool limit.

		/*

		 * Even though there was no reservation in the region/reserve

		 * map, there could be reservations associated with the

		 * subpool that can be used.  This would be indicated if the

		 * return value of hugepage_subpool_get_pages() is zero.

		 * However, if avoid_reserve is specified we still avoid even

		 * the subpool reservations.

	/* If this allocation is not consuming a reservation, charge it now.

	/*

	 * glb_chg is passed to indicate whether or not a page must be taken

	 * from the global free pool (global change).  gbl_chg == 0 indicates

	 * a reservation exists for the allocation.

 Fall through */

	/* If allocation is not consuming a reservation, also store the

	 * hugetlb_cgroup pointer on the page.

		/*

		 * The page was added to the reservation map between

		 * vma_needs_reservation and vma_commit_reservation.

		 * This indicates a race with hugetlb_reserve_pages.

		 * Adjust for the subpool count incremented above AND

		 * in hugetlb_reserve_pages for the same page.  Also,

		 * the reservation count added in hugetlb_reserve_pages

		 * no longer applies.

 initialize for clang */

 do node specific alloc */

 allocate from next node when distributing huge pages */

		/*

		 * Use the beginning of the huge page to store the

		 * huge_bootmem_page struct (until gather_bootmem

		 * puts them into the mem_map).

 Put them into a private list first because mem_map is not up yet */

/*

 * Put bootmem huge pages into the standard lists after mem_map is up.

 * Note: This only applies to gigantic (order > MAX_ORDER) pages.

 add to the hugepage allocator */

 VERY unlikely inflated ref count on a tail page */

		/*

		 * We need to restore the 'stolen' pages to totalram_pages

		 * in order to fix confusing memory reports from free(1) and

		 * other side-effects, like CommitLimit going negative.

 free it into the hugepage allocator */

 skip gigantic hugepages allocation if hugetlb_cma enabled */

 do node specific alloc */

 below will do all node balanced alloc */

		/*

		 * Bit mask controlling how hard we retry per-node allocations.

		 * Ignore errors as lower level routines can deal with

		 * node_alloc_noretry == NULL.  If this kmalloc fails at boot

		 * time, we are likely in bigger trouble.

 allocations done at boot time */

 bit mask controlling how hard we retry per-node allocations */

 oversize hugepages were init'ed in early boot */

		/*

		 * Set demote order for each hstate.  Note that

		 * h->demote_order is initially 0.

		 * - We can not demote gigantic pages if runtime freeing

		 *   is not supported, so skip this.

		 * - If CMA allocation is possible, we can not demote

		 *   HUGETLB_PAGE_ORDER or smaller size pages.

	/*

	 * Collect pages to be freed on a list, and free after dropping lock

/*

 * Increment or decrement surplus_huge_pages.  Keep node-specific counters

 * balanced by operating on them in a round-robin fashion.

 * Returns 1 if an adjustment was made.

	/*

	 * Bit mask controlling how hard we retry per-node allocations.

	 * If we can not allocate the bit mask, do not attempt to allocate

	 * the requested huge pages.

	/*

	 * resize_lock mutex prevents concurrent adjustments to number of

	 * pages in hstate via the proc/sysfs interfaces.

	/*

	 * Check for a node specific request.

	 * Changing node specific huge page count may require a corresponding

	 * change to the global count.  In any case, the passed node mask

	 * (nodes_allowed) will restrict alloc/free to the specified node.

		/*

		 * User may have specified a large count value which caused the

		 * above calculation to overflow.  In this case, they wanted

		 * to allocate as many huge pages as possible.  Set count to

		 * largest possible value to align with their intention.

	/*

	 * Gigantic pages runtime allocation depend on the capability for large

	 * page range allocation.

	 * If the system does not provide this feature, return an error when

	 * the user tries to allocate gigantic pages but let the user free the

	 * boottime allocated gigantic pages.

 Fall through to decrease pool */

	/*

	 * Increase the pool size

	 * First take pages out of surplus state.  Then make up the

	 * remaining difference by allocating fresh huge pages.

	 *

	 * We might race with alloc_surplus_huge_page() here and be unable

	 * to convert a surplus huge page to a normal huge page. That is

	 * not critical, though, it just means the overall size of the

	 * pool might be one hugepage larger than it needs to be, but

	 * within all the constraints specified by the sysctls.

		/*

		 * If this allocation races such that we no longer need the

		 * page, free_huge_page will handle it by freeing the page

		 * and reducing the surplus.

 yield cpu to avoid soft lockup */

 Bail for signals. Probably ctrl-c from user */

	/*

	 * Decrease the pool size

	 * First return free pages to the buddy allocator (being careful

	 * to keep enough around to satisfy reservations).  Then place

	 * pages into surplus state as needed so the pool will shrink

	 * to the desired size as pages become free.

	 *

	 * By placing pages into the surplus state independent of the

	 * overcommit value, we are allowing the surplus pool size to

	 * exceed overcommit. There are few sane options here. Since

	 * alloc_surplus_huge_page() is checking the global counter,

	 * though, we'll note that we're not allowed to exceed surplus

	 * and won't grow the pool anywhere else. Not until one of the

	 * sysctls are changed, or the surplus pages go out of use.

	/*

	 * Collect pages to be removed on list without dropping lock

 free the pages after dropping lock */

 Allocation of vmemmmap failed, we can not demote page */

	/*

	 * Use destroy_compound_hugetlb_page_for_demote for all huge page

	 * sizes as it will not ref count pages.

	/*

	 * Taking target hstate mutex synchronizes with set_max_huge_pages.

	 * Without the mutex, pages added to target hstate could be marked

	 * as surplus.

	 *

	 * Note that we already hold h->resize_lock.  To prevent deadlock,

	 * use the convention of always taking larger size hstate mutex first.

	/*

	 * Not absolutely necessary, but for consistency update max_huge_pages

	 * based on pool changes for the demoted page.

 We should never get here if no demote order */

 internal error */

		/*

		 * global hstate attribute

		/*

		 * Node specific request.  count adjustment happens in

		 * set_max_huge_pages() after acquiring hugetlb_lock.

/*

 * hstate attribute for optionally mempolicy-based constraint on persistent

 * huge page alloc/free.

 Synchronize with other sysfs operations modifying huge pages */

		/*

		 * Check for available pages to demote each time thorough the

		 * loop as demote_pool_huge_page will drop hugetlb_lock.

 demote order must be smaller than hstate order */

 resize_lock synchronizes access to demote size and writes */

/*

 * node_hstate/s - associate per node hstate attributes, via their kobjects,

 * with node devices in node_devices[] using a parallel array.  The array

 * index of a node device or _hstate == node id.

 * This is here to avoid any static dependency of the node device driver, in

 * the base kernel, on the hugetlb module.

/*

 * A subset of global hstate attributes for node devices

/*

 * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.

 * Returns node id via non-NULL nidp.

/*

 * Unregister hstate attributes from a single node device.

 * No-op if no hstate attributes attached.

 no hstate attributes */

/*

 * Register hstate attributes for a single node device.

 * No-op if attributes already registered.

 already allocated */

/*

 * hugetlb init time:  register hstate attributes for all registered node

 * devices of nodes that have memory.  All on-line nodes should have

 * registered their associated device by this time.

	/*

	 * Let the node device driver know we're here so it can

	 * [un]register hstate attributes on node hotplug.

 !CONFIG_NUMA */

	/*

	 * Make sure HPAGE_SIZE (HUGETLB_PAGE_ORDER) hstate exists.  Some

	 * architectures depend on setup being done here.

		/*

		 * If we did not parse a default huge page size, set

		 * default_hstate_idx to HPAGE_SIZE hstate. And, if the

		 * number of huge pages for this default size was implicitly

		 * specified, set that here as well.

		 * Note that the implicit setting will overwrite an explicit

		 * setting.  A warning will be printed in this case.

 Overwritten by architectures with more huge page sizes */

/*

 * hugepages command line processing

 * hugepages normally follows a valid hugepagsz or default_hugepagsz

 * specification.  If not, ignore the hugepages value.  hugepages can also

 * be the first huge page command line  option in which case it implicitly

 * specifies the number of huge pages for the default size.

	/*

	 * !hugetlb_max_hstate means we haven't parsed a hugepagesz= parameter

	 * yet, so this hugepages= parameter goes to the "default hstate".

	 * Otherwise, it goes with the previously parsed hugepagesz or

	 * default_hugepagesz.

 Parameter is node format */

 Parse hugepages */

 Go to parse next node*/

	/*

	 * Global state is always initialized later in hugetlb_init.

	 * But we need to allocate gigantic hstates here early to still

	 * use the bootmem allocator.

/*

 * hugepagesz command line processing

 * A specific huge page size can only be specified once with hugepagesz.

 * hugepagesz is followed by hugepages on the command line.  The global

 * variable 'parsed_valid_hugepagesz' is used to determine if prior

 * hugepagesz argument was valid.

		/*

		 * hstate for this size already exists.  This is normally

		 * an error, but is allowed if the existing hstate is the

		 * default hstate.  More specifically, it is only allowed if

		 * the number of huge pages for the default hstate was not

		 * previously specified.

		/*

		 * No need to call hugetlb_add_hstate() as hstate already

		 * exists.  But, do set parsed_hstate so that a following

		 * hugepages= parameter will be applied to this hstate.

/*

 * default_hugepagesz command line input

 * Only one instance of default_hugepagesz allowed on command line.

	/*

	 * The number of default huge pages (for this size) could have been

	 * specified as the first hugetlb parameter: hugepages=X.  If so,

	 * then default_hstate_max_huge_pages is set.  If the default huge

	 * page size is gigantic (>= MAX_ORDER), then the pages must be

	 * allocated here from bootmem allocator.

	/*

	 * In order to avoid races with __do_proc_doulongvec_minmax(), we

	 * can duplicate the @table and alter the duplicate of it.

 CONFIG_NUMA */

 CONFIG_SYSCTL */

 Return the number pages of memory we physically have, in PAGE_SIZE units. */

	/*

	 * When cpuset is configured, it breaks the strict hugetlb page

	 * reservation as the accounting is done on a global variable. Such

	 * reservation is completely rubbish in the presence of cpuset because

	 * the reservation is not checked against page availability for the

	 * current cpuset. Application can still potentially OOM'ed by kernel

	 * with lack of free htlb page in cpuset that the task is in.

	 * Attempt to enforce strict accounting with cpuset is almost

	 * impossible (or too ugly) because cpuset is too fluid that

	 * task or memory node can be dynamically moved between cpusets.

	 *

	 * The change of semantics for shared hugetlb mapping with cpuset is

	 * undesirable. However, in order to preserve some of the semantics,

	 * we fall back to check against current free page availability as

	 * a best attempt and hopefully to minimize the impact of changing

	 * semantics that cpuset has.

	 *

	 * Apart from cpuset, we also have memory policy mechanism that

	 * also determines from which node the kernel will allocate memory

	 * in a NUMA system. So similar to cpuset, we also should consider

	 * the memory policy of the current task. Similar to the description

	 * above.

	/*

	 * This new VMA should share its siblings reservation map if present.

	 * The VMA will only ever have a valid reservation map pointer where

	 * it is being copied for another still existing VMA.  As that VMA

	 * has a reference to the reservation map it cannot disappear until

	 * after this open call completes.  It is therefore safe to take a

	 * new reference here without additional locking.

		/*

		 * Decrement reserve counts.  The global reserve count may be

		 * adjusted if the subpool has a minimum size.

/*

 * We cannot handle pagefaults against hugetlb pages at all.  They cause

 * handle_mm_fault() to try to instantiate regular-sized pages in the

 * hugepage VMA.  do_page_fault() is supposed to trap this, so BUG is we get

 * this far.

/*

 * When a new function is introduced to vm_operations_struct and added

 * to hugetlb_vm_ops, please consider adding the function to shm_vm_ops.

 * This is because under System V memory model, mappings created via

 * shmget/shmat with "huge page" specified are backed by hugetlbfs files,

 * their original vm_ops are overwritten with shm_vm_ops.

		/*

		 * For shared mappings i_mmap_rwsem must be held to call

		 * huge_pte_alloc, otherwise the returned ptep could go

		 * away if part of a shared pmd and another thread calls

		 * huge_pmd_unshare.

		/*

		 * If the pagetables are shared don't copy or take references.

		 * dst_pte == src_pte is the common case of src/dest sharing.

		 *

		 * However, src could have 'unshared' and dst shares with

		 * another vma.  If dst_pte !none, this implies sharing.

		 * Check here before taking page table lock, and once again

		 * after taking the lock below.

			/*

			 * Skip if src entry none.  Also, skip in the

			 * unlikely case dst entry !none as this implies

			 * sharing with another vma.

				/*

				 * COW mappings require pages in both

				 * parent and child to be set to read.

			/*

			 * This is a rare case where we see pinned hugetlb

			 * pages while they're prone to COW.  We need to do the

			 * COW earlier during fork.

			 *

			 * When pre-allocating the page or copying data, we

			 * need to be without the pgtable locks since we could

			 * sleep during the process.

 Do not use reserve as it's private owned */

 Install the new huge page if src pte stable */

 dst_entry won't change as in child */

				/*

				 * No need to notify as we are downgrading page

				 * table protection not changing it to point

				 * to a new page.

				 *

				 * See Documentation/vm/mmu_notifier.rst

	/*

	 * We don't have to worry about the ordering of src and dst ptlocks

	 * because exclusive mmap_sem (or the i_mmap_lock) prevents deadlock.

 Prevent race with file truncation */

		/* old_addr arg to huge_pmd_unshare() is a pointer and so the

		 * arg may be modified. Pass a copy instead to preserve the

		 * value in old_addr.

	/*

	 * This is a hugetlb vma, all the pte entries should point

	 * to huge page.

	/*

	 * If sharing possible, alert mmu notifiers of worst case.

			/*

			 * We just unmapped a page of PMDs by clearing a PUD.

			 * The caller's TLB flush range should cover this area.

		/*

		 * Migrating hugepage or HWPoisoned hugepage is already

		 * unmapped and its refcount is dropped, so just clear pte here.

		/*

		 * If a reference page is supplied, it is because a specific

		 * page is being unmapped, not a range. Ensure the page we

		 * are about to unmap is the actual page of interest.

			/*

			 * Mark the VMA as having unmapped its page so that

			 * future faults in this VMA will fail rather than

			 * looking like data was lost

		/*

		 * Bail out after unmapping reference page if supplied

	/*

	 * Clear this flag so that x86's huge_pmd_share page_table_shareable

	 * test will fail on a vma being torn down, and not grab a page table

	 * on its way out.  We're lucky that the flag has such an appropriate

	 * name, and can in fact be safely cleared here. We could clear it

	 * before the __unmap_hugepage_range above, but all that's necessary

	 * is to clear it before releasing the i_mmap_rwsem. This works

	 * because in the context this is called, the VMA is about to be

	 * destroyed and the i_mmap_rwsem is held.

/*

 * This is called when the original mapper is failing to COW a MAP_PRIVATE

 * mapping it owns the reserve page for. The intention is to unmap the page

 * from other VMAs and let the children be SIGKILLed if they are faulting the

 * same region.

	/*

	 * vm_pgoff is in PAGE_SIZE units, hence the different calculation

	 * from page cache lookup which is in HPAGE_SIZE units.

	/*

	 * Take the mapping lock for the duration of the table walk. As

	 * this mapping should be shared between all the VMAs,

	 * __unmap_hugepage_range() is called as the lock is already held

 Do not unmap the current VMA */

		/*

		 * Shared VMAs have their own reserves and do not affect

		 * MAP_PRIVATE accounting but it is possible that a shared

		 * VMA is using the same page so check and skip such VMAs.

		/*

		 * Unmap the page from other VMAs without their own reserves.

		 * They get marked to be SIGKILLed if they fault in these

		 * areas. This is because a future no-page fault on this VMA

		 * could insert a zeroed page instead of the data existing

		 * from the time of fork. This would look like data corruption

/*

 * Hugetlb_cow() should be called with page lock of the original hugepage held.

 * Called with hugetlb_fault_mutex_table held and pte_page locked so we

 * cannot race with other handlers or page migration.

 * Keep the pte_same checks anyway to make transition from the mutex easier.

	/* If no-one else is actually using this page, avoid the copy

	/*

	 * If the process that created a MAP_PRIVATE mapping is about to

	 * perform a COW due to a shared page count, attempt to satisfy

	 * the allocation without using the existing reserves. The pagecache

	 * page is used to determine if the reserve at this address was

	 * consumed or not. If reserves were used, a partial faulted mapping

	 * at the time of fork() could consume its reserves on COW instead

	 * of the full address range.

	/*

	 * Drop page table lock as buddy allocator may be called. It will

	 * be acquired again before returning to the caller, as expected.

		/*

		 * If a process owning a MAP_PRIVATE mapping fails to COW,

		 * it is due to references held by a child and an insufficient

		 * huge page pool. To guarantee the original mappers

		 * reliability, unmap the page from child processes. The child

		 * may get SIGKILLed if it later faults.

			/*

			 * Drop hugetlb_fault_mutex and i_mmap_rwsem before

			 * unmapping.  unmapping needs to hold i_mmap_rwsem

			 * in write mode.  Dropping i_mmap_rwsem in read mode

			 * here is OK as COW mappings do not interact with

			 * PMD sharing.

			 *

			 * Reacquire both after unmap operation.

			/*

			 * race occurs while re-acquiring page table

			 * lock, and our job is done.

	/*

	 * When the original hugepage is shared one, it does not have

	 * anon_vma prepared.

	/*

	 * Retake the page table lock to check for racing updates

	 * before the page tables are altered

 Break COW */

 Make the old page be freed below */

 No restore in case of successful pagetable update (Break COW) */

 Caller expects lock to be held */

 Return the pagecache page at a given address within a VMA */

/*

 * Return whether there is a pagecache page to back given address within VMA.

 * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.

	/*

	 * set page dirty so that it will not be removed from cache/file

	 * by non-hugetlbfs specific code paths.

		/*

		 * Hard to debug if it ends up being

		 * used by a callee that assumes

		 * something about the other

		 * uninitialized fields... same as in

		 * memory.c

	/*

	 * hugetlb_fault_mutex and i_mmap_rwsem must be

	 * dropped before handling userfault.  Reacquire

	 * after handling fault to make calling code simpler.

	/*

	 * Currently, we are forced to kill the process in the event the

	 * original mapper has unmapped pages from the child due to a failed

	 * COW. Warn that such a situation has occurred as it may not be obvious

	/*

	 * We can not race with truncation due to holding i_mmap_rwsem.

	 * i_size is modified when holding i_mmap_rwsem, so check here

	 * once for faults beyond end of file.

 Check for page in userfault range */

			/*

			 * Returning error will result in faulting task being

			 * sent SIGBUS.  The hugetlb fault mutex prevents two

			 * tasks from racing to fault in the same page which

			 * could result in false unable to allocate errors.

			 * Page migration does not take the fault mutex, but

			 * does a clear then write of pte's under page table

			 * lock.  Page fault code could race with migration,

			 * notice the clear pte and try to allocate a page

			 * here.  Before returning error, get ptl and make

			 * sure there really is no pte entry.

		/*

		 * If memory error occurs between mmap() and fault, some process

		 * don't have hwpoisoned swap entry for errored virtual address.

		 * So we need to block hugepage fault by PG_hwpoison bit check.

 Check for page in userfault range. */

	/*

	 * If we are going to COW a private mapping later, we examine the

	 * pending reservations for this page now. This will ensure that

	 * any allocations necessary to record that reservation occur outside

	 * the spinlock.

 Just decrements count, does not deallocate */

 Optimization, do the COW without a second fault */

	/*

	 * Only set HPageMigratable in newly allocated pages.  Existing pages

	 * found in the pagecache may not have HPageMigratableset if they have

	 * been isolated for migration.

 restore reserve for newly allocated pages not in page cache */

/*

 * For uniprocessor systems we always use a single mutex, so just

 * return 0 and avoid the hashing overhead.

		/*

		 * Since we hold no locks, ptep could be stale.  That is

		 * OK as we are only making decisions based on content and

		 * not actually modifying content here.

	/*

	 * Acquire i_mmap_rwsem before calling huge_pte_alloc and hold

	 * until finished with ptep.  This serves two purposes:

	 * 1) It prevents huge_pmd_unshare from being called elsewhere

	 *    and making the ptep no longer valid.

	 * 2) It synchronizes us with i_size modifications during truncation.

	 *

	 * ptep could have already be assigned via huge_pte_offset.  That

	 * is OK, as huge_pte_alloc will return the same value unless

	 * something has changed.

	/*

	 * Serialize hugepage allocation and instantiation, so that we don't

	 * get spurious allocation failures if two CPUs race to instantiate

	 * the same page in the page cache.

	/*

	 * entry could be a migration/hwpoison entry at this point, so this

	 * check prevents the kernel from going below assuming that we have

	 * an active hugepage in pagecache. This goto expects the 2nd page

	 * fault, and is_hugetlb_entry_(migration|hwpoisoned) check will

	 * properly handle it.

	/*

	 * If we are going to COW the mapping later, we examine the pending

	 * reservations for this page now. This will ensure that any

	 * allocations necessary to record that reservation occur outside the

	 * spinlock. For private mappings, we also lookup the pagecache

	 * page now as it is used to determine if a reservation has been

	 * consumed.

 Just decrements count, does not deallocate */

 Check for a racing update before calling hugetlb_cow */

	/*

	 * hugetlb_cow() requires page locks of pte_page(entry) and

	 * pagecache_page, so here we need take the former one

	 * when page != pagecache_page or !pagecache_page.

	/*

	 * Generally it's safe to hold refcount during waiting page lock. But

	 * here we just wait to defer the next page fault to avoid busy loop and

	 * the page is not used after unlocked before returning from the current

	 * page fault. So we are safe from accessing freed page, even if we wait

	 * here without taking refcount.

/*

 * Used by userfaultfd UFFDIO_COPY.  Based on mcopy_atomic_pte with

 * modifications for huge pages.

		/* If a page already exists, then it's UFFDIO_COPY for

		 * a non-missing case. Return -EEXIST.

 fallback to copy_from_user outside mmap_lock */

			/* Free the allocated page which may have

			 * consumed a reservation.

			/* Allocate a temporary page to hold the copied

			 * contents.

			/* Set the outparam pagep and return to the caller to

			 * copy the contents outside the lock. Don't free the

			 * page.

	/*

	 * The memory barrier inside __SetPageUptodate makes sure that

	 * preceding stores to the page contents become visible before

	 * the set_pte_at() write.

 Add shared, newly allocated pages to the page cache. */

		/*

		 * Serialization between remove_inode_hugepages() and

		 * huge_add_to_page_cache() below happens through the

		 * hugetlb_fault_mutex_table that here must be hold by

		 * the caller.

	/*

	 * Recheck the i_size after holding PT lock to make sure not

	 * to leave any page mapped (as page_mapped()) beyond the end

	 * of the i_size (remove_inode_hugepages() is strict about

	 * enforcing that). If we bail out here, we'll also leave a

	 * page in the radix tree in the vm_shared case beyond the end

	 * of the i_size, but remove_inode_hugepages() will take care

	 * of it as soon as we drop the hugetlb_fault_mutex_table.

 For CONTINUE on a non-shared VMA, don't set VM_WRITE for CoW. */

 No need to invalidate - it was non-present before */

 CONFIG_USERFAULTFD */

		/*

		 * If we have a pending SIGKILL, don't keep faulting pages and

		 * potentially allocating memory.

		/*

		 * Some archs (sparc64, sh*) have multiple pte_ts to

		 * each hugepage.  We have to make sure we get the

		 * first, for the page indexing below to work.

		 *

		 * Note that page table lock is not held when pte is null.

		/*

		 * When coredumping, it suits get_dump_page if we just return

		 * an error where there's an empty slot with no huge pagecache

		 * to back it.  This way, we avoid allocating a hugepage, and

		 * the sparse dumpfile avoids allocating disk blocks, but its

		 * huge holes still show up with zeroes where they need to be.

		/*

		 * We need call hugetlb_fault for both hugepages under migration

		 * (in which case hugetlb_fault waits for the migration,) and

		 * hwpoisoned hugepages (in which case we need to prevent the

		 * caller from accessing to them.) In order to do this, we use

		 * here is_swap_pte instead of is_hugetlb_entry_migration and

		 * is_hugetlb_entry_hwpoisoned. This is because it simply covers

		 * both cases, and because we can't follow correct pages

		 * directly from any kind of swap entries.

				/*

				 * Note: FAULT_FLAG_ALLOW_RETRY and

				 * FAULT_FLAG_TRIED can co-exist

				/*

				 * VM_FAULT_RETRY must not return an

				 * error, it will return zero

				 * instead.

				 *

				 * No need to update "position" as the

				 * caller will not check it after

				 * *nr_pages is set to 0.

		/*

		 * If subpage information not requested, update counters

		 * and skip the same_page loop below.

 vaddr may not be aligned to PAGE_SIZE */

			/*

			 * try_grab_compound_head() should always succeed here,

			 * because: a) we hold the ptl lock, and b) we've just

			 * checked that the huge page is present in the page

			 * tables. If the huge page is present, then the tail

			 * pages must also be present. The ptl prevents the

			 * head page and tail pages from being rearranged in

			 * any way. So this page must be available at this

			 * point, unless the page refcount overflowed:

	/*

	 * setting position is actually required only if remainder is

	 * not zero but it's faster not to add a "if (remainder)"

	 * branch.

	/*

	 * In the case of shared PMDs, the area to flush could be beyond

	 * start/end.  Set range.start/range.end to cover the maximum possible

	 * range if PMD sharing is possible.

	/*

	 * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare

	 * may have cleared our pud entry and done put_page on the page table:

	 * once we release i_mmap_rwsem, another task can do the final put_page

	 * and that page table be reused and filled with junk.  If we actually

	 * did unshare a page of pmds, flush the range corresponding to the pud.

	/*

	 * No need to call mmu_notifier_invalidate_range() we are downgrading

	 * page table protection not changing it to point to a new page.

	 *

	 * See Documentation/vm/mmu_notifier.rst

 Return true if reservation was successful, false otherwise.  */

 This should never happen */

	/*

	 * Only apply hugepage reservation if asked. At fault time, an

	 * attempt will be made for VM_NORESERVE to allocate a page

	 * without using reserves

	/*

	 * Shared mappings base their reservation on the number of pages that

	 * are already allocated on behalf of the file. Private mappings need

	 * to reserve the full area even if read-only as mprotect() may be

	 * called to make the mapping read-write. Assume !vma is a shm mapping

		/*

		 * resv_map can not be NULL as hugetlb_reserve_pages is only

		 * called for inodes for which resv_maps were created (see

		 * hugetlbfs_get_inode).

 Private mapping. */

		/* For private mappings, the hugetlb_cgroup uncharge info hangs

		 * of the resv_map.

	/*

	 * There must be enough pages in the subpool for the mapping. If

	 * the subpool has a minimum size, there may be some global

	 * reservations already in place (gbl_reserve).

	/*

	 * Check enough hugepages are available for the reservation.

	 * Hand the pages back to the subpool if there are not

	/*

	 * Account for the reservations made. Shared mappings record regions

	 * that have reservations as they are shared by multiple VMAs.

	 * When the last VMA disappears, the region map says how much

	 * the reservation was and the page cache tells how much of

	 * the reservation was consumed. Private mappings are per-VMA and

	 * only the consumed reservations are tracked. When the VMA

	 * disappears, the original reservation is the VMA size and the

	 * consumed reservations are stored in the map. Hence, nothing

	 * else has to be done for private mappings here

			/*

			 * pages in this range were added to the reserve

			 * map between region_chg and region_add.  This

			 * indicates a race with alloc_huge_page.  Adjust

			 * the subpool and reserve counts modified above

			 * based on the difference.

			/*

			 * hugetlb_cgroup_uncharge_cgroup_rsvd() will put the

			 * reference to h_cg->css. See comment below for detail.

			/*

			 * The file_regions will hold their own reference to

			 * h_cg->css. So we should release the reference held

			 * via hugetlb_cgroup_charge_cgroup_rsvd() when we are

			 * done.

 put back original number of pages, chg */

		/* Only call region_abort if the region_chg succeeded but the

		 * region_add failed or didn't run.

	/*

	 * Since this routine can be called in the evict inode path for all

	 * hugetlbfs inodes, resv_map could be NULL.

		/*

		 * region_del() can fail in the rare case where a region

		 * must be split and another region descriptor can not be

		 * allocated.  If end == LONG_MAX, it will not fail.

	/*

	 * If the subpool has a minimum size, the number of global

	 * reservations to be released may be adjusted.

	 *

	 * Note that !resv_map implies freed == 0. So (chg - freed)

	 * won't go negative.

 Allow segments to share if only one is marked locked */

	/*

	 * match the virtual addresses, permission and the alignment of the

	 * page table page.

	/*

	 * check on proper vm_flags and page table alignment

/*

 * Determine if start,end range within vma could be mapped by shared pmd.

 * If yes, adjust start and end to cover range associated with possible

 * shared pmd mappings.

	/*

	 * vma needs to span at least one aligned PUD size, and the range

	 * must be at least partially within in.

 Extend the range to be PUD aligned for a worst case scenario */

/*

 * Search for a shareable pmd page for hugetlb. In any case calls pmd_alloc()

 * and returns the corresponding pte. While this is not necessary for the

 * !shared pmd case because we can allocate the pmd later as well, it makes the

 * code much cleaner.

 *

 * This routine must be called with i_mmap_rwsem held in at least read mode if

 * sharing is possible.  For hugetlbfs, this prevents removal of any page

 * table entries associated with the address space.  This is important as we

 * are setting up sharing based on existing page table entries (mappings).

/*

 * unmap huge page backed by shared pte.

 *

 * Hugetlb pte page is ref counted at the time of mapping.  If pte is shared

 * indicated by page_count > 1, unmap is achieved by clearing pud and

 * decrementing the ref count. If count == 1, the pte page is not shared.

 *

 * Called with page table lock held and i_mmap_rwsem held in write mode.

 *

 * returns: 1 successfully unmapped a shared pte page

 *	    0 the underlying pte page is not shared, or it is the last user

 !CONFIG_ARCH_WANT_HUGE_PMD_SHARE */

 CONFIG_ARCH_WANT_HUGE_PMD_SHARE */

/*

 * huge_pte_offset() - Walk the page table to resolve the hugepage

 * entry at address @addr

 *

 * Return: Pointer to page table entry (PUD or PMD) for

 * address @addr, or NULL if a !p*d_present() entry is encountered and the

 * size @sz doesn't match the hugepage size at this level of the page

 * table.

 must be pud huge, non-present or none */

 must have a valid entry and size to go further */

 must be pmd huge, non-present or none */

 CONFIG_ARCH_WANT_GENERAL_HUGETLB */

/*

 * These functions are overwritable if your architecture needs its own

 * behavior.

 FOLL_GET and FOLL_PIN are mutually exclusive. */

	/*

	 * make sure that the address range covered by this pmd is not

	 * unmapped from other threads.

		/*

		 * try_grab_page() should always succeed here, because: a) we

		 * hold the pmd (ptl) lock, and b) we've just checked that the

		 * huge pmd (head) page is present in the page tables. The ptl

		 * prevents the head page and tail pages from being rearranged

		 * in any way. So this page must be available at this point,

		 * unless the page refcount overflowed:

		/*

		 * hwpoisoned entry is treated as no_page_table in

		 * follow_page_mask().

	/*

	 * transfer temporary state of the new huge page. This is

	 * reverse to other transitions because the newpage is going to

	 * be final while the old one will be freed so it takes over

	 * the temporary status.

	 *

	 * Also note that we have to transfer the per-node surplus state

	 * here as well otherwise the global surplus count will not match

	 * the per-node's.

		/*

		 * There is no need to transfer the per-node surplus state

		 * when we do not cross the node.

/*

 * This function will unconditionally remove all the shared pmd pgtable entries

 * within the specific vma for a hugetlbfs memory range.

	/*

	 * No need to call adjust_range_if_pmd_sharing_possible(), because

	 * we have already done the PUD_SIZE alignment.

 We don't want 'address' to be changed */

	/*

	 * No need to call mmu_notifier_invalidate_range(), see

	 * Documentation/vm/mmu_notifier.rst.

			/*

			 * Skip the separator if have one, otherwise

			 * break the parsing.

 Validate the CMA size again in case some invalid nodes specified. */

		/*

		 * If 3 GB area is requested on a machine with 4 numa nodes,

		 * let's allocate 1 GB on first three nodes and ignore the last one.

		/*

		 * Note that 'order per bit' is based on smallest size that

		 * may be returned to CMA allocator in the case of

		 * huge page demotion.

		/*

		 * hugetlb_cma_size is used to determine if allocations from

		 * cma are possible.  Set to zero if no cma regions are set up.

 CONFIG_CMA */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * bdi_lock protects bdi_tree and updates to bdi_list. bdi_list has RCU

 * reader side locking.

 bdi_wq serves all asynchronous writeback tasks */

/*

 * This function is used when the first inode for this wb is marked dirty. It

 * wakes-up the corresponding bdi thread which should then take care of the

 * periodic background write-out of dirty inodes. Since the write-out would

 * starts only 'dirty_writeback_interval' centisecs from now anyway, we just

 * set up a timer which wakes the bdi thread up later.

 *

 * Note, we wouldn't bother setting up the timer, but this function is on the

 * fast-path (used by '__mark_inode_dirty()'), so we save few context switches

 * by delaying the wake-up.

 *

 * We have to be careful not to postpone flush work if it is scheduled for

 * earlier. Thus we use queue_delayed_work().

/*

 * Initial write bandwidth: 100 MB/s

/*

 * Remove bdi from the global list and shutdown any threads we have running

 Make sure nobody queues further work */

	/*

	 * Drain work list and shutdown the delayed_work.  !WB_registered

	 * tells wb_workfn() that @wb is dying and its work_list needs to

	 * be drained no matter what.

/*

 * cgwb_lock protects bdi->cgwb_tree, blkcg->cgwb_list, offline_cgwbs and

 * memcg->cgwb_list.  bdi->cgwb_tree is also RCU protected.

 triggers blkg destruction if no online users left */

 look up again under lock and discard on blkcg mismatch */

 need to create a new one */

	/*

	 * The root wb determines the registered state of the whole bdi and

	 * memcg_cgwb_list and blkcg_cgwb_list's next pointers indicate

	 * whether they're still online.  Don't link @wb if any is dead.

	 * See wb_memcg_offline() and wb_blkcg_offline().

 we might have raced another instance of this function */

/**

 * wb_get_lookup - get wb for a given memcg

 * @bdi: target bdi

 * @memcg_css: cgroup_subsys_state of the target memcg (must have positive ref)

 *

 * Try to get the wb for @memcg_css on @bdi.  The returned wb has its

 * refcount incremented.

 *

 * This function uses css_get() on @memcg_css and thus expects its refcnt

 * to be positive on invocation.  IOW, rcu_read_lock() protection on

 * @memcg_css isn't enough.  try_get it before calling this function.

 *

 * A wb is keyed by its associated memcg.  As blkcg implicitly enables

 * memcg on the default hierarchy, memcg association is guaranteed to be

 * more specific (equal or descendant to the associated blkcg) and thus can

 * identify both the memcg and blkcg associations.

 *

 * Because the blkcg associated with a memcg may change as blkcg is enabled

 * and disabled closer to root in the hierarchy, each wb keeps track of

 * both the memcg and blkcg associated with it and verifies the blkcg on

 * each lookup.  On mismatch, the existing wb is discarded and a new one is

 * created.

 see whether the blkcg association has changed */

/**

 * wb_get_create - get wb for a given memcg, create if necessary

 * @bdi: target bdi

 * @memcg_css: cgroup_subsys_state of the target memcg (must have positive ref)

 * @gfp: allocation mask to use

 *

 * Try to get the wb for @memcg_css on @bdi.  If it doesn't exist, try to

 * create one.  See wb_get_lookup() for more details.

/*

 * cleanup_offline_cgwbs_workfn - try to release dying cgwbs

 *

 * Try to release dying cgwbs by switching attached inodes to the nearest

 * living ancestor's writeback. Processed wbs are placed at the end

 * of the list to guarantee the forward progress.

		/*

		 * If wb is dirty, cleaning up the writeback by switching

		 * attached inodes will result in an effective removal of any

		 * bandwidth restrictions, which isn't the goal.  Instead,

		 * it can be postponed until the next time, when all io

		 * will be likely completed.  If in the meantime some inodes

		 * will get re-dirtied, they should be eventually switched to

		 * a new cgwb.

/**

 * wb_memcg_offline - kill all wb's associated with a memcg being offlined

 * @memcg: memcg being offlined

 *

 * Also prevents creation of any new wb's associated with @memcg.

 prevent new wb's */

/**

 * wb_blkcg_offline - kill all wb's associated with a blkcg being offlined

 * @blkcg: blkcg being offlined

 *

 * Also prevents creation of any new wb's associated with @blkcg.

 prevent new wb's */

	/*

	 * There can be many concurrent release work items overwhelming

	 * system_wq.  Put them in a separate wq and limit concurrency.

	 * There's no point in executing many of these in parallel.

 CONFIG_CGROUP_WRITEBACK */

 CONFIG_CGROUP_WRITEBACK */

/**

 * bdi_get_by_id - lookup and get bdi from its id

 * @id: bdi id to lookup

 *

 * Find bdi matching @id and get it.  Returns NULL if the matching bdi

 * doesn't exist or is already unregistered.

 The driver needs to use separate queues per device */

/*

 * Remove bdi from bdi_list, and ensure that it is no longer visible

 make sure nobody finds us on the bdi_list anymore */

/**

 * congestion_wait - wait for a backing_dev to become uncongested

 * @sync: SYNC or ASYNC IO

 * @timeout: timeout in jiffies

 *

 * Waits for up to @timeout jiffies for a backing_dev (any backing_dev) to exit

 * write congestion.  If no backing_devs are congested then just wait for the

 * next write to be completed.

 SPDX-License-Identifier: GPL-2.0

/*

 * Re-map IO memory to kernel address space so that we can access it.

 * This is needed for high PCI addresses that aren't mapped in the

 * 640k-1MB IO memory area on PC's

 *

 * (C) Copyright 1995 1996 Linus Torvalds

 Disallow wrap-around or zero size */

 Page-align mappings */

 SPDX-License-Identifier: GPL-2.0

 default scan 8*512 pte (or vmas) every 30 second */

 during fragmentation poll the hugepage allocator once every minute */

/*

 * default collapse hugepages if there is at least one pte mapped like

 * it would have happened if the vma was large enough during page

 * fault.

/**

 * struct mm_slot - hash lookup from mm to mm_slot

 * @hash: hash collision list

 * @mm_node: khugepaged scan list headed in khugepaged_scan.mm_head

 * @mm: the mm that this information is valid for

 * @nr_pte_mapped_thp: number of pte mapped THP

 * @pte_mapped_thp: address array corresponding pte mapped THP

 pte-mapped THP in this mm */

/**

 * struct khugepaged_scan - cursor for scanning

 * @mm_head: the head of the mm list to scan

 * @mm_slot: the current mm_slot we are scanning

 * @address: the next address inside that to be scanned

 *

 * There is only the one khugepaged_scan instance of this cursor structure.

/*

 * max_ptes_none controls if khugepaged should collapse hugepages over

 * any unmapped ptes in turn potentially increasing the memory

 * footprint of the vmas. When max_ptes_none is 0 khugepaged will not

 * reduce the available free memory in the system as it

 * runs. Increasing max_ptes_none will instead potentially reduce the

 * free memory in the system during the khugepaged scan.

 CONFIG_SYSFS */

		/*

		 * qemu blindly sets MADV_HUGEPAGE on all allocations, but s390

		 * can't handle this properly after s390_enable_sie, so we simply

		 * ignore the madvise to prevent qemu from causing a SIGSEGV.

		/*

		 * If the vma become good for khugepaged to scan,

		 * register it here without waiting a page fault that

		 * may not happen any time soon.

		/*

		 * Setting VM_NOHUGEPAGE will prevent khugepaged from scanning

		 * this vma even if we leave the mm registered in khugepaged if

		 * it got registered before VM_NOHUGEPAGE was set.

 initialization failed */

 Enabled via shmem mount options or sysfs settings. */

 THP settings require madvise. */

 Only regular file is valid */

 __khugepaged_exit() must not run from under us */

	/*

	 * Insert just behind the scanning cursor, to let the area settle

	 * down a little.

	/*

	 * khugepaged only supports read-only files for non-shmem files.

	 * khugepaged does not yet work on special mappings. And

	 * file-private shmem THP is not supported.

		/*

		 * This is required to serialize against

		 * khugepaged_test_exit() (which is guaranteed to run

		 * under mmap sem read mode). Stop here (after we

		 * return all pagetables will be destroyed) until

		 * khugepaged has finished working on the pagetables

		 * under the mmap_lock.

			/*

			 * Check if we have dealt with the compound page

			 * already

		/*

		 * We can do it before isolate_lru_page because the

		 * page can't be freed from under us. NOTE: PG_lock

		 * is needed to serialize against split_huge_page

		 * when invoked from the VM.

		/*

		 * Check if the page has any GUP (or other external) pins.

		 *

		 * The page table that maps the page has been already unlinked

		 * from the page table tree and this process cannot get

		 * an additional pin on the page.

		 *

		 * New pins can come later if the page is shared across fork,

		 * but not from this process. The other process cannot write to

		 * the page, only trigger CoW.

			/*

			 * Page is in the swap cache and cannot be re-used.

			 * It cannot be collapsed into a THP.

		/*

		 * Isolate the page to avoid collapsing an hugepage

		 * currently in use by the VM.

 There should be enough young pte to collapse the page */

				/*

				 * ptl mostly unnecessary.

				/*

				 * paravirt calls inside pte_clear here are

				 * superfluous.

			/*

			 * ptl mostly unnecessary, but preempt has to

			 * be disabled to update the per-cpu stats

			 * inside page_remove_rmap().

			/*

			 * paravirt calls inside pte_clear here are

			 * superfluous.

	/*

	 * If node_reclaim_mode is disabled, then no extra effort is made to

	 * allocate memory locally.

 If there is a count for this node already, it must be acceptable */

 Defrag for khugepaged will enter direct reclaim/compaction if necessary */

 find first node with max normal pages hit */

 do some balance if several nodes have the same hit record */

	/*

	 * If the hpage allocated earlier was briefly exposed in page cache

	 * before collapse_file() failed, it is possible that racing lookups

	 * have not yet completed, and would then be unpleasantly surprised by

	 * finding the hpage reused for the same mapping at a different offset.

	 * Just release the previous allocation if there is any danger of that.

/*

 * If mmap_lock temporarily dropped, revalidate vma

 * before taking mmap_lock.

 * Return 0 if succeeds, otherwise return none-zero

 * value (scan code).

 Anon VMA expected */

/*

 * Bring missing pages in from swap, to complete THP collapse.

 * Only done if khugepaged_scan_pmd believes it is worthwhile.

 *

 * Called and returns without pte mapped or spinlocks held,

 * but with mmap_lock held to protect against vma changes.

 do_swap_page returns VM_FAULT_RETRY with released mmap_lock */

 vma is no longer available, don't continue to swapin */

 check if the pmd is still valid */

 Drain LRU add pagevec to remove extra pin on the swapped in pages */

 Only allocate from the target node */

	/*

	 * Before allocating the hugepage, release the mmap_lock read lock.

	 * The allocation can take potentially a long time if it involves

	 * sync compaction, and we do not need to hold the mmap_lock during

	 * that. We will recheck the vma after taking it again in write mode.

	/*

	 * __collapse_huge_page_swapin always returns with mmap_lock locked.

	 * If it fails, we release mmap_lock and jump out_nolock.

	 * Continuing to collapse causes inconsistency.

	/*

	 * Prevent all access to pagetables with the exception of

	 * gup_fast later handled by the ptep_clear_flush and the VM

	 * handled by the anon_vma lock + PG_lock.

 check if the pmd is still valid */

 probably unnecessary */

	/*

	 * After this gup_fast can't run anymore. This also removes

	 * any huge TLB entry from the CPU so we won't allow

	 * huge and small TLB entries for the same virtual address

	 * to avoid the risk of CPU bugs in that area.

		/*

		 * We can only use set_pmd_at when establishing

		 * hugepmds and never for establishing regular pmds that

		 * points to regular pagetables. Use pmd_populate for that

	/*

	 * All pages are isolated and locked so anon_vma rmap

	 * can't run anymore.

	/*

	 * spin_lock() below is not the equivalent of smp_wmb(), but

	 * the smp_wmb() inside __SetPageUptodate() can be reused to

	 * avoid the copy_huge_page writes to become visible after

	 * the set_pmd_at() write.

				/*

				 * Always be strict with uffd-wp

				 * enabled swap entries.  Please see

				 * comment below for pte_uffd_wp().

			/*

			 * Don't collapse the page if any of the small

			 * PTEs are armed with uffd write protection.

			 * Here we can also mark the new huge pmd as

			 * write protected if any of the small ones is

			 * marked but that could bring unknown

			 * userfault messages that falls outside of

			 * the registered range.  So, just be simple.

		/*

		 * Record which node the original page is from and save this

		 * information to khugepaged_node_load[].

		 * Khupaged will allocate hugepage from the node has the max

		 * hit record.

		/*

		 * Check if the page has any GUP (or other external) pins.

		 *

		 * Here the check is racy it may see totmal_mapcount > refcount

		 * in some cases.

		 * For example, one process with one forked child process.

		 * The parent has the PMD split due to MADV_DONTNEED, then

		 * the child is trying unmap the whole PMD, but khugepaged

		 * may be scanning the parent between the child has

		 * PageDoubleMap flag cleared and dec the mapcount.  So

		 * khugepaged may see total_mapcount > refcount.

		 *

		 * But such case is ephemeral we could always retry collapse

		 * later.  However it may report false positive if the page

		 * has excessive GUP pins (i.e. 512).  Anyway the same check

		 * will be done again later the risk seems low.

 collapse_huge_page will return with the mmap_lock released */

 free mm_slot */

		/*

		 * Not strictly needed because the mm exited already.

		 *

		 * clear_bit(MMF_VM_HUGEPAGE, &mm->flags);

 khugepaged_mm_lock actually not necessary for the below */

/*

 * Notify khugepaged that given addr of the mm is pte-mapped THP. Then

 * khugepaged should try to collapse the page table.

/**

 * collapse_pte_mapped_thp - Try to collapse a pte-mapped THP for mm at

 * address haddr.

 *

 * @mm: process address space where collapse happens

 * @addr: THP collapse address

 *

 * This function checks whether all the PTEs in the PMD are pointing to the

 * right THP. If so, retract the page table so the THP can refault in with

 * as pmd-mapped.

	/*

	 * This vm_flags may not have VM_HUGEPAGE if the page was not

	 * collapsed by this mm. But we can still collapse if the page is

	 * the valid THP. Add extra VM_HUGEPAGE so hugepage_vma_check()

	 * will not fail the vma for missing VM_HUGEPAGE

 step 1: check all mapped PTEs are to the right huge page */

 empty pte, skip */

 page swapped out, abort */

		/*

		 * Note that uprobe, debugger, or MAP_PRIVATE may change the

		 * page table, but the new page will not be a subpage of hpage.

 step 2: adjust rmap */

 step 3: set proper refcount and mm_counters. */

 step 4: collapse pmd */

		/*

		 * Check vma->anon_vma to exclude MAP_PRIVATE mappings that

		 * got written to. These VMAs are likely not worth investing

		 * mmap_write_lock(mm) as PMD-mapping is likely to be split

		 * later.

		 *

		 * Not that vma->anon_vma check is racy: it can be set up after

		 * the check but before we took mmap_lock by the fault path.

		 * But page lock would prevent establishing any new ptes of the

		 * page, so we are safe.

		 *

		 * An alternative would be drop the check, but check that page

		 * table is clear before calling pmdp_collapse_flush() under

		 * ptl. It has higher chance to recover THP for the VMA, but

		 * has higher cost too.

		/*

		 * We need exclusive mmap_lock to retract page table.

		 *

		 * We use trylock due to lock inversion: we need to acquire

		 * mmap_lock while holding page lock. Fault path does it in

		 * reverse order. Trylock is a way to avoid deadlock.

 assume page table is clear */

 Try again later */

/**

 * collapse_file - collapse filemap/tmpfs/shmem pages into huge one.

 *

 * @mm: process address space where collapse happens

 * @file: file that collapse on

 * @start: collapse start address

 * @hpage: new allocated huge page for collapse

 * @node: appointed node the new huge page allocate from

 *

 * Basic scheme is simple, details are more complex:

 *  - allocate and lock a new huge page;

 *  - scan page cache replacing old pages with the new one

 *    + swap/gup in pages if necessary;

 *    + fill in gaps;

 *    + keep old pages around in case rollback is required;

 *  - if replacing succeeds:

 *    + copy data over;

 *    + free old pages;

 *    + unlock huge page;

 *  - if replacing failed;

 *    + put all pages back and unfreeze them;

 *    + restore gaps in the page cache;

 *    + unlock and free huge page;

 Only allocate from the target node */

 This will be less messy when we use multi-index entries */

	/*

	 * At this point the new_page is locked and not up-to-date.

	 * It's safe to insert it into the page cache, because nobody would

	 * be able to map it or use it in another way until we unlock it.

				/*

				 * Stop if extent has been truncated or

				 * hole-punched, and is now completely

				 * empty.

 swap in or instantiate fallocated page */

 !is_shmem */

 drain pagevecs to help isolate_lru_page() */

				/*

				 * khugepaged only works on read-only fd,

				 * so this page is dirty because it hasn't

				 * been flushed since first write. There

				 * won't be new dirty pages.

				 *

				 * Trigger async flush here and hope the

				 * writeback is done when khugepaged

				 * revisits this page.

				 *

				 * This is a one-off situation. We are not

				 * forcing writeback in loop.

		/*

		 * The page must be locked, so we can drop the i_pages lock

		 * without racing with truncate.

 make sure the page is up to date */

		/*

		 * If file was truncated then extended, or hole-punched, before

		 * we locked the first page, then a THP might be there already.

			/*

			 * khugepaged only works on read-only fd, so this

			 * page is dirty because it hasn't been flushed

			 * since first write.

		/*

		 * The page is expected to have page_count() == 3:

		 *  - we hold a pin on it;

		 *  - one reference from page cache;

		 *  - one from isolate_lru_page;

		/*

		 * Add the page to the list to be able to undo the collapse if

		 * something go wrong.

 Finally, replace with the new page. */

		/*

		 * Paired with smp_mb() in do_dentry_open() to ensure

		 * i_writecount is up to date and the update to nr_thps is

		 * visible. Ensures the page cache will be truncated if the

		 * file is opened writable.

		/*

		 * Replacing old pages with new one has succeeded, now we

		 * need to copy the content and free the old pages.

		/*

		 * Remove pte page tables, so we can re-fault the page as huge.

 Something went wrong: roll back page cache changes */

 Put holes back where they were */

 Unfreeze the page. */

 TODO: tracepoints */

		/*

		 * We probably should check if the page is referenced here, but

		 * nobody would transfer pte_young() to PageReferenced() for us.

		 * And rmap walk here is just too costly...

 TODO: tracepoints */

	/*

	 * Don't wait for semaphore (to avoid long wait times).  Just move to

	 * the next mm on the list.

 move to next address */

 we released mmap_lock so break loop */

 exit_mmap will destroy ptes after this */

	/*

	 * Release the current mm_slot if this mm is about to die, or

	 * if we scanned all vmas of this mm.

		/*

		 * Make sure that if mm_users is reaching zero while

		 * khugepaged runs here, khugepaged_exit will find

		 * mm_slot not pointing to the exiting mm.

		/*

		 * We don't need to worry about fragmentation of

		 * ZONE_MOVABLE since it only has movable pages.

 Ensure 2 pageblocks are free to assist fragmentation avoidance */

	/*

	 * Make sure that on average at least two pageblocks are almost free

	 * of another type, one for a migratetype to fall back to and a

	 * second to avoid subsequent fallbacks of other types There are 3

	 * MIGRATE_TYPES we care about.

 don't ever allow to reserve more than 5% of the lowmem */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/mm/swap_state.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

 *  Swap reorganised 29.12.95, Stephen Tweedie

 *

 *  Rewritten to use page cache, (C) 1998 Stephen Tweedie

/*

 * swapper_space is a fiction, retained to simplify the path through

 * vmscan's shrink_page_list.

 Initial readahead hits is 4 to start up with a small window */

/*

 * add_to_swap_cache resembles add_to_page_cache_locked on swapper_space,

 * but sets SwapCache flag and private instead of mapping and index.

/*

 * This must be called only on pages that have

 * been verified to be in the swap cache.

/**

 * add_to_swap - allocate swap space for a page

 * @page: page we want to move to swap

 *

 * Allocate swap space for the page and add the page to the

 * swap cache.  Caller needs to hold the page lock. 

	/*

	 * XArray node allocations from PF_MEMALLOC contexts could

	 * completely exhaust the page allocator. __GFP_NOMEMALLOC

	 * stops emergency reserves from being allocated.

	 *

	 * TODO: this could cause a theoretical memory reclaim

	 * deadlock in the swap out path.

	/*

	 * Add it to the swap cache.

		/*

		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely

		 * clear SWAP_HAS_CACHE flag.

	/*

	 * Normally the page will be dirtied in unmap because its pte should be

	 * dirty. A special case is MADV_FREE page. The page's pte could have

	 * dirty bit cleared but the page's SwapBacked bit is still set because

	 * clearing the dirty bit and SwapBacked bit has no lock protected. For

	 * such page, unmap will not set dirty bit for it, so page reclaim will

	 * not write the page out. This can cause data corruption when the page

	 * is swap in later. Always setting the dirty bit for the page solves

	 * the problem.

/*

 * This must be called only on pages that have

 * been verified to be in the swap cache and locked.

 * It will never put the page into the free list,

 * the caller has a reference on the page.

 search the next swapcache until we meet end */

/* 

 * If we are the only user, then try to free up the swap cache. 

 * 

 * Its ok to check for PageSwapCache without the page lock

 * here because we are going to recheck again inside

 * try_to_free_swap() _with_ the lock.

 * 					- Marcelo

/* 

 * Perform a free_page(), also freeing any swap cache associated with

 * this page if it is the last user of the page.

/*

 * Passed an array of pages, drop them all from swapcache and then release

 * them.  They are removed from the LRU and freed if this is their last use.

/*

 * Lookup a swap entry in the swap cache. A found page will be returned

 * unlocked and with its refcount incremented - we rely on the kernel

 * lock getting page table operations atomic even if we drop the page

 * lock before returning.

		/*

		 * At the moment, we don't support PG_readahead for anon THP

		 * so let's bail out rather than confusing the readahead stat.

/**

 * find_get_incore_page - Find and get a page from the page or swap caches.

 * @mapping: The address_space to search.

 * @index: The page cache index.

 *

 * This differs from find_get_page() in that it will also look for the

 * page in the swap cache.

 *

 * Return: The found page or %NULL.

 Prevent swapoff from happening to us */

		/*

		 * First check the swap cache.  Since this is normally

		 * called after lookup_swap_cache() failed, re-calling

		 * that would confuse statistics.

		/*

		 * Just skip read ahead for unused swap slot.

		 * During swap_off when swap_slot_cache is disabled,

		 * we have to handle the race between putting

		 * swap entry in swap cache and marking swap slot

		 * as SWAP_HAS_CACHE.  That's done in later part of code or

		 * else swap_off will be aborted if we return NULL.

		/*

		 * Get a new page to read into from swap.  Allocate it now,

		 * before marking swap_map SWAP_HAS_CACHE, when -EEXIST will

		 * cause any racers to loop around until we add it to cache.

		/*

		 * Swap entry may have been freed since our caller observed it.

		/*

		 * We might race against __delete_from_swap_cache(), and

		 * stumble across a swap_map entry whose SWAP_HAS_CACHE

		 * has not yet been cleared.  Or race against another

		 * __read_swap_cache_async(), which has set SWAP_HAS_CACHE

		 * in swap_map, but not yet added its page to swap cache.

	/*

	 * The swap entry is ours to swap in. Prepare the new page.

 May fail (-ENOMEM) if XArray node allocation failed. */

 Caller will initiate read into locked page */

/*

 * Locate a page of swap in physical memory, reserving swap cache space

 * and reading the disk if it is not already cached.

 * A failure return means that either the page allocation failed or that

 * the swap entry is no longer in use.

	/*

	 * This heuristic has been found to work well on both sequential and

	 * random loads, swapping to hard disk or to SSD: please don't ask

	 * what the "+ 2" means, it just happens to work well, that's all.

		/*

		 * We can have no readahead hits to judge by: but must not get

		 * stuck here forever, so check for an adjacent offset instead

		 * (and don't even bother to check whether swap type is same).

 Don't shrink readahead too fast */

/**

 * swap_cluster_readahead - swap in pages in hope we need them soon

 * @entry: swap entry of this memory

 * @gfp_mask: memory allocation flags

 * @vmf: fault information

 *

 * Returns the struct page for entry and addr, after queueing swapin.

 *

 * Primitive swap readahead code. We simply read an aligned block of

 * (1 << page_cluster) entries in the swap area. This method is chosen

 * because it doesn't cost us any seek time.  We also make sure to queue

 * the 'original' request together with the readahead ones...

 *

 * This has been extended to use the NUMA policies from the mm triggering

 * the readahead.

 *

 * Caller must hold read mmap_lock if vmf->vma is not NULL.

 Read a page_cluster sized and aligned cluster around offset. */

 First page is swap header. */

 Ok, do the async read-ahead now */

 Push any new pages onto the LRU now */

 swap cache doesn't use writeback related tags */

 Copy the PTEs because the page table may be unmapped */

/**

 * swap_vma_readahead - swap in pages in hope we need them soon

 * @fentry: swap entry of this memory

 * @gfp_mask: memory allocation flags

 * @vmf: fault information

 *

 * Returns the struct page for entry and addr, after queueing swapin.

 *

 * Primitive swap readahead code. We simply read in a few pages whose

 * virtual addresses are around the fault address in the same vma.

 *

 * Caller must hold read mmap_lock if vmf->vma is not NULL.

 *

/**

 * swapin_readahead - swap in pages in hope we need them soon

 * @entry: swap entry of this memory

 * @gfp_mask: memory allocation flags

 * @vmf: fault information

 *

 * Returns the struct page for entry and addr, after queueing swapin.

 *

 * It's a main entry function for swap readahead. By the configuration,

 * it will read ahead blocks by cluster-based(ie, physical disk based)

 * or vma-based(ie, virtual address based on faulty address) readahead.

 SPDX-License-Identifier: GPL-2.0

/*

 * Free some vmemmap pages of HugeTLB

 *

 * Copyright (c) 2020, Bytedance. All rights reserved.

 *

 *     Author: Muchun Song <songmuchun@bytedance.com>

 *

 * The struct page structures (page structs) are used to describe a physical

 * page frame. By default, there is a one-to-one mapping from a page frame to

 * it's corresponding page struct.

 *

 * HugeTLB pages consist of multiple base page size pages and is supported by

 * many architectures. See hugetlbpage.rst in the Documentation directory for

 * more details. On the x86-64 architecture, HugeTLB pages of size 2MB and 1GB

 * are currently supported. Since the base page size on x86 is 4KB, a 2MB

 * HugeTLB page consists of 512 base pages and a 1GB HugeTLB page consists of

 * 4096 base pages. For each base page, there is a corresponding page struct.

 *

 * Within the HugeTLB subsystem, only the first 4 page structs are used to

 * contain unique information about a HugeTLB page. __NR_USED_SUBPAGE provides

 * this upper limit. The only 'useful' information in the remaining page structs

 * is the compound_head field, and this field is the same for all tail pages.

 *

 * By removing redundant page structs for HugeTLB pages, memory can be returned

 * to the buddy allocator for other uses.

 *

 * Different architectures support different HugeTLB pages. For example, the

 * following table is the HugeTLB page size supported by x86 and arm64

 * architectures. Because arm64 supports 4k, 16k, and 64k base pages and

 * supports contiguous entries, so it supports many kinds of sizes of HugeTLB

 * page.

 *

 * +--------------+-----------+-----------------------------------------------+

 * | Architecture | Page Size |                HugeTLB Page Size              |

 * +--------------+-----------+-----------+-----------+-----------+-----------+

 * |    x86-64    |    4KB    |    2MB    |    1GB    |           |           |

 * +--------------+-----------+-----------+-----------+-----------+-----------+

 * |              |    4KB    |   64KB    |    2MB    |    32MB   |    1GB    |

 * |              +-----------+-----------+-----------+-----------+-----------+

 * |    arm64     |   16KB    |    2MB    |   32MB    |     1GB   |           |

 * |              +-----------+-----------+-----------+-----------+-----------+

 * |              |   64KB    |    2MB    |  512MB    |    16GB   |           |

 * +--------------+-----------+-----------+-----------+-----------+-----------+

 *

 * When the system boot up, every HugeTLB page has more than one struct page

 * structs which size is (unit: pages):

 *

 *    struct_size = HugeTLB_Size / PAGE_SIZE * sizeof(struct page) / PAGE_SIZE

 *

 * Where HugeTLB_Size is the size of the HugeTLB page. We know that the size

 * of the HugeTLB page is always n times PAGE_SIZE. So we can get the following

 * relationship.

 *

 *    HugeTLB_Size = n * PAGE_SIZE

 *

 * Then,

 *

 *    struct_size = n * PAGE_SIZE / PAGE_SIZE * sizeof(struct page) / PAGE_SIZE

 *                = n * sizeof(struct page) / PAGE_SIZE

 *

 * We can use huge mapping at the pud/pmd level for the HugeTLB page.

 *

 * For the HugeTLB page of the pmd level mapping, then

 *

 *    struct_size = n * sizeof(struct page) / PAGE_SIZE

 *                = PAGE_SIZE / sizeof(pte_t) * sizeof(struct page) / PAGE_SIZE

 *                = sizeof(struct page) / sizeof(pte_t)

 *                = 64 / 8

 *                = 8 (pages)

 *

 * Where n is how many pte entries which one page can contains. So the value of

 * n is (PAGE_SIZE / sizeof(pte_t)).

 *

 * This optimization only supports 64-bit system, so the value of sizeof(pte_t)

 * is 8. And this optimization also applicable only when the size of struct page

 * is a power of two. In most cases, the size of struct page is 64 bytes (e.g.

 * x86-64 and arm64). So if we use pmd level mapping for a HugeTLB page, the

 * size of struct page structs of it is 8 page frames which size depends on the

 * size of the base page.

 *

 * For the HugeTLB page of the pud level mapping, then

 *

 *    struct_size = PAGE_SIZE / sizeof(pmd_t) * struct_size(pmd)

 *                = PAGE_SIZE / 8 * 8 (pages)

 *                = PAGE_SIZE (pages)

 *

 * Where the struct_size(pmd) is the size of the struct page structs of a

 * HugeTLB page of the pmd level mapping.

 *

 * E.g.: A 2MB HugeTLB page on x86_64 consists in 8 page frames while 1GB

 * HugeTLB page consists in 4096.

 *

 * Next, we take the pmd level mapping of the HugeTLB page as an example to

 * show the internal implementation of this optimization. There are 8 pages

 * struct page structs associated with a HugeTLB page which is pmd mapped.

 *

 * Here is how things look before optimization.

 *

 *    HugeTLB                  struct pages(8 pages)         page frame(8 pages)

 * +-----------+ ---virt_to_page---> +-----------+   mapping to   +-----------+

 * |           |                     |     0     | -------------> |     0     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     1     | -------------> |     1     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     2     | -------------> |     2     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     3     | -------------> |     3     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     4     | -------------> |     4     |

 * |    PMD    |                     +-----------+                +-----------+

 * |   level   |                     |     5     | -------------> |     5     |

 * |  mapping  |                     +-----------+                +-----------+

 * |           |                     |     6     | -------------> |     6     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     7     | -------------> |     7     |

 * |           |                     +-----------+                +-----------+

 * |           |

 * |           |

 * |           |

 * +-----------+

 *

 * The value of page->compound_head is the same for all tail pages. The first

 * page of page structs (page 0) associated with the HugeTLB page contains the 4

 * page structs necessary to describe the HugeTLB. The only use of the remaining

 * pages of page structs (page 1 to page 7) is to point to page->compound_head.

 * Therefore, we can remap pages 2 to 7 to page 1. Only 2 pages of page structs

 * will be used for each HugeTLB page. This will allow us to free the remaining

 * 6 pages to the buddy allocator.

 *

 * Here is how things look after remapping.

 *

 *    HugeTLB                  struct pages(8 pages)         page frame(8 pages)

 * +-----------+ ---virt_to_page---> +-----------+   mapping to   +-----------+

 * |           |                     |     0     | -------------> |     0     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     1     | -------------> |     1     |

 * |           |                     +-----------+                +-----------+

 * |           |                     |     2     | ----------------^ ^ ^ ^ ^ ^

 * |           |                     +-----------+                   | | | | |

 * |           |                     |     3     | ------------------+ | | | |

 * |           |                     +-----------+                     | | | |

 * |           |                     |     4     | --------------------+ | | |

 * |    PMD    |                     +-----------+                       | | |

 * |   level   |                     |     5     | ----------------------+ | |

 * |  mapping  |                     +-----------+                         | |

 * |           |                     |     6     | ------------------------+ |

 * |           |                     +-----------+                           |

 * |           |                     |     7     | --------------------------+

 * |           |                     +-----------+

 * |           |

 * |           |

 * |           |

 * +-----------+

 *

 * When a HugeTLB is freed to the buddy system, we should allocate 6 pages for

 * vmemmap pages and restore the previous mapping relationship.

 *

 * For the HugeTLB page of the pud level mapping. It is similar to the former.

 * We also can use this approach to free (PAGE_SIZE - 2) vmemmap pages.

 *

 * Apart from the HugeTLB page of the pmd/pud level mapping, some architectures

 * (e.g. aarch64) provides a contiguous bit in the translation table entries

 * that hints to the MMU to indicate that it is one of a contiguous set of

 * entries that can be cached in a single TLB entry.

 *

 * The contiguous bit is used to increase the mapping size at the pmd and pte

 * (last) level. So this type of HugeTLB page can be optimized only when its

 * size of the struct page structs is greater than 2 pages.

/*

 * There are a lot of struct page structures associated with each HugeTLB page.

 * For tail pages, the value of compound_head is the same. So we can reuse first

 * page of tail page structures. We map the virtual addresses of the remaining

 * pages of tail page structures to the first tail page struct, and then free

 * these page frames. Therefore, we need to reserve two pages as vmemmap areas.

 We cannot optimize if a "struct page" crosses page boundaries. */

/*

 * Previously discarded vmemmap pages will be allocated and remapping

 * after this function returns zero.

	/*

	 * The pages which the vmemmap virtual address range [@vmemmap_addr,

	 * @vmemmap_end) are mapped to are freed to the buddy allocator, and

	 * the range is mapped to the page which @vmemmap_reuse is mapped to.

	 * When a HugeTLB page is freed to the buddy allocator, previously

	 * discarded vmemmap pages must be allocated and remapping.

	/*

	 * Remap the vmemmap virtual address range [@vmemmap_addr, @vmemmap_end)

	 * to the page which @vmemmap_reuse is mapped to, then free the pages

	 * which the range [@vmemmap_addr, @vmemmap_end] is mapped to.

	/*

	 * There are only (RESERVE_VMEMMAP_SIZE / sizeof(struct page)) struct

	 * page structs that can be used when CONFIG_HUGETLB_PAGE_FREE_VMEMMAP,

	 * so add a BUILD_BUG_ON to catch invalid usage of the tail struct page.

	/*

	 * The head page and the first tail page are not to be freed to buddy

	 * allocator, the other pages will map to the first tail page, so they

	 * can be freed.

	 *

	 * Could RESERVE_VMEMMAP_NR be greater than @vmemmap_pages? It is true

	 * on some architectures (e.g. aarch64). See Documentation/arm64/

	 * hugetlbpage.rst for more details.

 SPDX-License-Identifier: GPL-2.0-only

/**

 * io_mapping_map_user - remap an I/O mapping to userspace

 * @iomap: the source io_mapping

 * @vma: user vma to map to

 * @addr: target user address to start at

 * @pfn: physical address of kernel memory

 * @size: size of map area

 *

 *  Note: this is only safe if the mm semaphore is held when called.

 We rely on prevalidation of the io-mapping to skip track_pfn(). */

 SPDX-License-Identifier: GPL-2.0

/*

 * SLUB: A slab allocator that limits cache line use instead of queuing

 * objects in per cpu and per node lists.

 *

 * The allocator synchronizes using per slab locks or atomic operations

 * and only uses a centralized lock to manage a pool of partial slabs.

 *

 * (C) 2007 SGI, Christoph Lameter

 * (C) 2011 Linux Foundation, Christoph Lameter

 struct reclaim_state */

/*

 * Lock order:

 *   1. slab_mutex (Global Mutex)

 *   2. node->list_lock (Spinlock)

 *   3. kmem_cache->cpu_slab->lock (Local lock)

 *   4. slab_lock(page) (Only on some arches or for debugging)

 *   5. object_map_lock (Only for debugging)

 *

 *   slab_mutex

 *

 *   The role of the slab_mutex is to protect the list of all the slabs

 *   and to synchronize major metadata changes to slab cache structures.

 *   Also synchronizes memory hotplug callbacks.

 *

 *   slab_lock

 *

 *   The slab_lock is a wrapper around the page lock, thus it is a bit

 *   spinlock.

 *

 *   The slab_lock is only used for debugging and on arches that do not

 *   have the ability to do a cmpxchg_double. It only protects:

 *	A. page->freelist	-> List of object free in a page

 *	B. page->inuse		-> Number of objects in use

 *	C. page->objects	-> Number of objects in page

 *	D. page->frozen		-> frozen state

 *

 *   Frozen slabs

 *

 *   If a slab is frozen then it is exempt from list management. It is not

 *   on any list except per cpu partial list. The processor that froze the

 *   slab is the one who can perform list operations on the page. Other

 *   processors may put objects onto the freelist but the processor that

 *   froze the slab is the only one that can retrieve the objects from the

 *   page's freelist.

 *

 *   list_lock

 *

 *   The list_lock protects the partial and full list on each node and

 *   the partial slab counter. If taken then no new slabs may be added or

 *   removed from the lists nor make the number of partial slabs be modified.

 *   (Note that the total number of slabs is an atomic value that may be

 *   modified without taking the list lock).

 *

 *   The list_lock is a centralized lock and thus we avoid taking it as

 *   much as possible. As long as SLUB does not have to handle partial

 *   slabs, operations can continue without any centralized lock. F.e.

 *   allocating a long series of objects that fill up slabs does not require

 *   the list lock.

 *

 *   cpu_slab->lock local lock

 *

 *   This locks protect slowpath manipulation of all kmem_cache_cpu fields

 *   except the stat counters. This is a percpu structure manipulated only by

 *   the local cpu, so the lock protects against being preempted or interrupted

 *   by an irq. Fast path operations rely on lockless operations instead.

 *   On PREEMPT_RT, the local lock does not actually disable irqs (and thus

 *   prevent the lockless operations), so fastpath operations also need to take

 *   the lock and are no longer lockless.

 *

 *   lockless fastpaths

 *

 *   The fast path allocation (slab_alloc_node()) and freeing (do_slab_free())

 *   are fully lockless when satisfied from the percpu slab (and when

 *   cmpxchg_double is possible to use, otherwise slab_lock is taken).

 *   They also don't disable preemption or migration or irqs. They rely on

 *   the transaction id (tid) field to detect being preempted or moved to

 *   another cpu.

 *

 *   irq, preemption, migration considerations

 *

 *   Interrupts are disabled as part of list_lock or local_lock operations, or

 *   around the slab_lock operation, in order to make the slab allocator safe

 *   to use in the context of an irq.

 *

 *   In addition, preemption (or migration on PREEMPT_RT) is disabled in the

 *   allocation slowpath, bulk allocation, and put_cpu_partial(), so that the

 *   local cpu doesn't change in the process and e.g. the kmem_cache_cpu pointer

 *   doesn't have to be revalidated in each section protected by the local lock.

 *

 * SLUB assigns one slab for allocation to each processor.

 * Allocations only occur from these slabs called cpu slabs.

 *

 * Slabs with free elements are kept on a partial list and during regular

 * operations no list for full slabs is used. If an object in a full slab is

 * freed then the slab will show up again on the partial lists.

 * We track full slabs for debugging purposes though because otherwise we

 * cannot scan all objects.

 *

 * Slabs are freed when they become empty. Teardown and setup is

 * minimal so we rely on the page allocators per cpu caches for

 * fast frees and allocs.

 *

 * page->frozen		The slab is frozen and exempt from list processing.

 * 			This means that the slab is dedicated to a purpose

 * 			such as satisfying allocations for a specific

 * 			processor. Objects may be freed in the slab while

 * 			it is frozen but slab_free will then skip the usual

 * 			list operations. It is up to the processor holding

 * 			the slab to integrate the slab into the slab lists

 * 			when the slab is no longer needed.

 *

 * 			One use of this flag is to mark slabs that are

 * 			used for allocations. Then such a slab becomes a cpu

 * 			slab. The cpu slab may be equipped with an additional

 * 			freelist that allows lockless access to

 * 			free objects in addition to the regular freelist

 * 			that requires the slab lock.

 *

 * SLAB_DEBUG_FLAGS	Slab requires special handling due to debug

 * 			options set. This moves	slab handling out of

 * 			the fast path and disables lockless freelists.

/*

 * We could simply use migrate_disable()/enable() but as long as it's a

 * function call even on !PREEMPT_RT, use inline preempt_disable() there.

 CONFIG_SLUB_DEBUG */

/*

 * Issues still to be resolved:

 *

 * - Support PAGE_ALLOC_DEBUG. Should be easy to do.

 *

 * - Variable sizing of the per node arrays

 Enable to log cmpxchg failures */

/*

 * Minimum number of partial slabs. These will be left on the partial

 * lists even if they are empty. kmem_cache_shrink may reclaim them.

/*

 * Maximum number of desirable partial slabs.

 * The existence of more partial slabs makes kmem_cache_shrink

 * sort the partial list by the number of objects in use.

/*

 * These debug flags cannot use CMPXCHG because there might be consistency

 * issues when checking or reading debug information

/*

 * Debugging flags that require metadata to be stored in the slab.  These get

 * disabled when slub_debug=O is used and a cache's min order increases with

 * metadata.

 since page.objects is u15 */

 Internal SLUB flags */

 Poison object */

 Use cmpxchg_double */

/*

 * Tracking user of a slab.

 Called from address */

 Called from address */

 Was running on cpu */

 Pid context */

 When did the operation occur */

	/*

	 * The rmw is racy on a preemptible kernel but this is acceptable, so

	 * avoid this_cpu_add()'s irq-disable overhead.

/*

 * Tracks for which NUMA nodes we have kmem_cache_nodes allocated.

 * Corresponds to node_state[N_NORMAL_MEMORY], but can temporarily

 * differ during memory hotplug/hotremove operations.

 * Protected by slab_mutex.

/********************************************************************

 * 			Core slab cache functions

/*

 * Returns freelist pointer (ptr). With hardening, this is obfuscated

 * with an XOR of the address where the pointer is held and a per-cache

 * random number.

	/*

	 * When CONFIG_KASAN_SW/HW_TAGS is enabled, ptr_addr might be tagged.

	 * Normally, this doesn't cause any issues, as both set_freepointer()

	 * and get_freepointer() are called with a pointer with the same tag.

	 * However, there are some issues with CONFIG_SLUB_DEBUG code. For

	 * example, when __free_slub() iterates over objects in a cache, it

	 * passes untagged pointers to check_object(). check_object() in turns

	 * calls get_freepointer() with an untagged pointer, which causes the

	 * freepointer to be restored incorrectly.

 Returns the freelist pointer recorded at location ptr_addr. */

 naive detection of double free or corruption */

 Loop over all objects in a slab */

	/*

	 * We take the number of objects but actually limit the number of

	 * pages on the per cpu partial list, in order to limit excessive

	 * growth of the list. For simplicity we assume that the pages will

	 * be half-full.

 CONFIG_SLUB_CPU_PARTIAL */

/*

 * Per slab locking using the pagelock

/*

 * Interrupts must be disabled (for the fallback code to work right), typically

 * by an _irqsave() lock variant. Except on PREEMPT_RT where locks are different

 * so we disable interrupts as part of slab_[un]lock().

 init to 0 to prevent spurious warnings */

/*

 * Determine a map of object in use on a page.

 *

 * Node listlock must be held to guarantee that the page does

 * not vanish from under us.

/*

 * Debug settings:

/*

 * slub is about to manipulate internal object metadata.  This memory lies

 * outside the range of the allocated object, so accessing it would normally

 * be reported by kasan as a bounds error.  metadata_access_enable() is used

 * to tell kasan that these accesses are OK.

/*

 * Object debugging

 Verify that a pointer has an address that is valid within a slab page */

/*

 * See comment in calculate_sizes().

/*

 * Return offset of the end of info block which is inuse + free pointer if

 * not overlapping with object.

 Offset of last byte */

 Beginning of the filler is the free pointer */

/*

 * Object layout:

 *

 * object address

 * 	Bytes of the object to be managed.

 * 	If the freepointer may overlay the object then the free

 *	pointer is at the middle of the object.

 *

 * 	Poisoning uses 0x6b (POISON_FREE) and the last byte is

 * 	0xa5 (POISON_END)

 *

 * object + s->object_size

 * 	Padding to reach word boundary. This is also used for Redzoning.

 * 	Padding is extended by another word if Redzoning is enabled and

 * 	object_size == inuse.

 *

 * 	We fill with 0xbb (RED_INACTIVE) for inactive objects and with

 * 	0xcc (RED_ACTIVE) for objects in use.

 *

 * object + s->inuse

 * 	Meta data starts here.

 *

 * 	A. Free pointer (if we cannot overwrite object on free)

 * 	B. Tracking data for SLAB_STORE_USER

 *	C. Padding to reach required alignment boundary or at minimum

 * 		one word if debugging is on to be able to detect writes

 * 		before the word boundary.

 *

 *	Padding is done using 0x5a (POISON_INUSE)

 *

 * object + s->size

 * 	Nothing is used beyond s->size.

 *

 * If slabcaches are merged then the object_size and inuse boundaries are mostly

 * ignored. And therefore no slab options that rely on these boundaries

 * may be used with merged slabcaches.

 The end of info */

 We also have user information there */

 Check the pad bytes at the end of a slab page */

		/*

		 * check_pad_bytes cleans up on its own.

		/*

		 * Object and freepointer overlap. Cannot check

		 * freepointer while object is allocated.

 Check free pointer validity */

		/*

		 * No choice but to zap it and thus lose the remainder

		 * of the free objects in this slab. May cause

		 * another error because the object count is now wrong.

 Slab_pad_check fixes things up after itself */

/*

 * Determine if a certain object on a page is on the freelist. Must hold the

 * slab lock to guarantee that the chains are in a consistent state.

/*

 * Tracking of fully allocated slabs for debugging purposes.

 Tracking of the number of slabs for debugging purposes */

	/*

	 * May be called early in order to allocate a slab for the

	 * kmem_cache_node structure. Solve the chicken-egg

	 * dilemma by deferring the increment of the count during

	 * bootstrap (see early_kmem_cache_node_alloc).

 Object debug checks for alloc/free paths */

 Success perform special debug activities for allocs */

		/*

		 * If this is a slab page then lets do the best we can

		 * to avoid issues in the future. Marking all objects

		 * as used avoids touching the remaining objects.

 Supports checking bulk free of a constructed freelist */

 Freepointer not overwritten by init_object(), SLAB_POISON moved it */

 Reached end of constructed freelist yet? */

/*

 * Parse a block of slub_debug options. Blocks are delimited by ';'

 *

 * @str:    start of block

 * @flags:  returns parsed flags, or DEBUG_DEFAULT_FLAGS if none specified

 * @slabs:  return start of list of slabs, or NULL when there's no list

 * @init:   assume this is initial parsing and not per-kmem-create parsing

 *

 * returns the start of next block if there's any, or NULL

 Skip any completely empty blocks */

		/*

		 * No options but restriction on slabs. This means full

		 * debugging for slabs matching a pattern.

 Determine which debug features should be switched on */

			/*

			 * Avoid enabling debugging on caches if its minimum

			 * order would increase as a result.

 Skip over the slab list */

 Skip any completely empty blocks */

		/*

		 * No options specified. Switch on full debugging.

	/*

	 * For backwards compatibility, a single list of flags with list of

	 * slabs means debugging is only changed for those slabs, so the global

	 * slub_debug should be unchanged (0 or DEBUG_DEFAULT_FLAGS, depending

	 * on CONFIG_SLUB_DEBUG_ON). We can extended that to multiple lists as

	 * long as there is no option specifying flags without a slab list.

/*

 * kmem_cache_flags - apply debugging options to the cache

 * @object_size:	the size of an object without meta data

 * @flags:		flags to set

 * @name:		name of the cache

 *

 * Debug option(s) are applied to @flags. In addition to the debug

 * option(s), if a slab name (or multiple) is specified i.e.

 * slub_debug=<Debug-Options>,<slab name1>,<slab name2> ...

 * then only the select slabs will receive the debug option(s).

	/*

	 * If the slab cache is for debugging (e.g. kmemleak) then

	 * don't store user (stack trace) information by default,

	 * but let the user enable it via the command line below.

 Go through all blocks of debug options, see if any matches our slab's name */

 Found a block that has a slab list, search it */

 !CONFIG_SLUB_DEBUG */

 CONFIG_SLUB_DEBUG */

/*

 * Hooks for other subsystems that check memory allocations. In a typical

 * production configuration these hooks all should produce no code at all.

 As ptr might get tagged, call kmemleak hook after KASAN. */

 Use KCSAN to help debug racy use-after-free. */

	/*

	 * As memory initialization might be integrated into KASAN,

	 * kasan_slab_free and initialization memset's must be

	 * kept together to avoid discrepancies in behavior.

	 *

	 * The initialization memset's clear the object and the metadata,

	 * but don't touch the SLAB redzone.

 KASAN might put x into memory quarantine, delaying its reuse. */

 Head and tail of the reconstructed freelist */

 If object's reuse doesn't have to be delayed */

 Move object to the new freelist */

			/*

			 * Adjust the reconstructed freelist depth

			 * accordingly if object's reuse is delayed.

/*

 * Slab allocation and freeing

 Pre-initialize the random sequence cache */

 Bailout if already initialised */

 Transform to an offset on the set of pages */

 Initialize each random sequence freelist per cache */

 Get the next entry on the pre-computed freelist randomized */

	/*

	 * If the target page allocation failed, the number of objects on the

	 * page might be smaller than the usual size defined by the cache.

 Shuffle the single linked freelist based on a random pre-computed sequence */

 First entry is used as the base of the freelist */

 CONFIG_SLAB_FREELIST_RANDOM */

	/*

	 * Let the initial higher-order allocation fail under memory pressure

	 * so we fall-back to the minimum order allocation.

		/*

		 * Allocation may have failed due to fragmentation.

		 * Try a lower order alloc if possible

 In union with page->mapping where page allocator expects NULL */

/*

 * Management of partially allocated slabs.

/*

 * Remove slab from the partial list, freeze it and

 * return the pointer to the freelist.

 *

 * Returns a list of objects or NULL if it fails.

	/*

	 * Zap the freelist and set the frozen bit.

	 * The old freelist is the list of objects for the

	 * per cpu allocation list.

/*

 * Try to allocate a partial slab from a specific node.

	/*

	 * Racy check. If we mistakenly see no partial slabs then we

	 * just allocate an empty slab. If we mistakenly try to get a

	 * partial slab and there is none available then get_partial()

	 * will return NULL.

/*

 * Get a page from somewhere. Search in increasing NUMA distances.

	/*

	 * The defrag ratio allows a configuration of the tradeoffs between

	 * inter node defragmentation and node local allocations. A lower

	 * defrag_ratio increases the tendency to do local allocations

	 * instead of attempting to obtain partial slabs from other nodes.

	 *

	 * If the defrag_ratio is set to 0 then kmalloc() always

	 * returns node local objects. If the ratio is higher then kmalloc()

	 * may return off node objects because partial slabs are obtained

	 * from other nodes and filled up.

	 *

	 * If /sys/kernel/slab/xx/remote_node_defrag_ratio is set to 100

	 * (which makes defrag_ratio = 1000) then every (well almost)

	 * allocation will first attempt to defrag slab caches on other nodes.

	 * This means scanning over all nodes to look for partial slabs which

	 * may be expensive if we do it every time we are trying to find a slab

	 * with available objects.

					/*

					 * Don't check read_mems_allowed_retry()

					 * here - if mems_allowed was updated in

					 * parallel, that was a harmless race

					 * between allocation and the cpuset

					 * update

 CONFIG_NUMA */

/*

 * Get a partial page, lock it and return it.

/*

 * Calculate the next globally unique transaction for disambiguation

 * during cmpxchg. The transactions start with the cpu number and are then

 * incremented by CONFIG_NR_CPUS.

/*

 * No preemption supported therefore also no need to check for

 * different cpus.

/*

 * Finishes removing the cpu slab. Merges cpu's freelist with page's freelist,

 * unfreezes the slabs and puts it on the proper list.

 * Assumes the slab has been already safely taken away from kmem_cache_cpu

 * by the caller.

	/*

	 * Stage one: Count the objects on cpu's freelist as free_delta and

	 * remember the last object in freelist_tail for later splicing.

		/*

		 * If 'nextfree' is invalid, it is possible that the object at

		 * 'freelist_iter' is already corrupted.  So isolate all objects

		 * starting at 'freelist_iter' by skipping them.

	/*

	 * Stage two: Unfreeze the page while splicing the per-cpu

	 * freelist to the head of page's freelist.

	 *

	 * Ensure that the page is unfrozen while the list presence

	 * reflects the actual number of objects during unfreeze.

	 *

	 * We setup the list membership and then perform a cmpxchg

	 * with the count. If there is a mismatch then the page

	 * is not unfrozen but the page is on the wrong list.

	 *

	 * Then we restart the process which may have to remove

	 * the page from the list that we just put it on again

	 * because the number of objects in the slab may have

	 * changed.

 Determine target state of the slab */

			/*

			 * Taking the spinlock removes the possibility

			 * that acquire_slab() will see a slab page that

			 * is frozen

			/*

			 * This also ensures that the scanning of full

			 * slabs from diagnostic functions will not see

			 * any frozen slabs.

/*

 * Unfreeze all the cpu partial slabs.

/*

 * Put a page that was just frozen (in __slab_free|get_partial_node) into a

 * partial page slot if available.

 *

 * If we did not find a slot then simply move all the partials to the

 * per node partial list.

			/*

			 * Partial array is full. Move the existing set to the

			 * per node partial list. Postpone the actual unfreezing

			 * outside of the critical section.

 CONFIG_SLUB_CPU_PARTIAL */

 CONFIG_SLUB_CPU_PARTIAL */

/*

 * Flush cpu slab.

 *

 * Called from CPU work handler with migration disabled.

/*

 * Use the cpu notifier to insure that the cpu slabs are flushed when

 * necessary.

/*

 * Check if the objects in a per cpu structure fit numa

 * locality expectations.

 CONFIG_SLUB_DEBUG */

 CONFIG_SLUB_DEBUG || CONFIG_SYSFS */

/*

 * A variant of pfmemalloc_match() that tests page flags without asserting

 * PageSlab. Intended for opportunistic checks before taking a lock and

 * rechecking that nobody else freed the page under us.

/*

 * Check the page->freelist of a page and either transfer the freelist to the

 * per cpu freelist or deactivate the page.

 *

 * The page is still frozen if the return value is not NULL.

 *

 * If this function returns NULL then the page has been unfrozen.

/*

 * Slow path. The lockless freelist is empty or we need to perform

 * debugging duties.

 *

 * Processing is still very fast if new objects have been freed to the

 * regular freelist. In that case we simply take over the regular freelist

 * as the lockless freelist and zap the regular freelist.

 *

 * If that is not working then we fall back to the partial lists. We take the

 * first element of the freelist as the object to allocate now and move the

 * rest of the freelist to the lockless freelist.

 *

 * And if we were unable to get a new slab from the partial slab lists then

 * we need to allocate a new slab. This is the slowest path since it involves

 * a call to the page allocator and the setup of a new slab.

 *

 * Version of __slab_alloc to use when we know that preemption is

 * already disabled (which is the case for bulk allocation).

		/*

		 * if the node is not online or has no normal memory, just

		 * ignore the node constraint

		/*

		 * same as above but node_match() being false already

		 * implies node != NUMA_NO_NODE

	/*

	 * By rights, we should be searching for a slab page that was

	 * PFMEMALLOC but right now, we are losing the pfmemalloc

	 * information when the page leaves the per-cpu allocator

 must check again c->page in case we got preempted and it changed */

	/*

	 * freelist is pointing to the list of objects to be used.

	 * page is pointing to the page from which the objects are obtained.

	 * That page must be frozen for per cpu allocations to work.

 we were preempted and partial list got empty */

	/*

	 * No other reference to the page yet so we can

	 * muck around with it freely without cmpxchg

 Slab failed checks. Next slab needed */

			/*

			 * For debug case, we don't load freelist so that all

			 * allocations go through alloc_debug_processing()

		/*

		 * For !pfmemalloc_match() case we don't load freelist so that

		 * we don't make further mismatched allocations easier.

/*

 * A wrapper for ___slab_alloc() for contexts where preemption is not yet

 * disabled. Compensates for possible cpu changes by refetching the per cpu area

 * pointer.

	/*

	 * We may have been preempted and rescheduled on a different

	 * cpu before disabling preemption. Need to reload cpu area

	 * pointer.

/*

 * If the object has been wiped upon free, make sure it's fully initialized by

 * zeroing out freelist pointer.

/*

 * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc)

 * have the fastpath folded into their functions. So no function call

 * overhead for requests that can be satisfied on the fastpath.

 *

 * The fastpath works by first checking if the lockless freelist can be used.

 * If not then __slab_alloc is called for slow processing.

 *

 * Otherwise we can simply pick the next object from the lockless free list.

	/*

	 * Must read kmem_cache cpu data via this cpu ptr. Preemption is

	 * enabled. We may switch back and forth between cpus while

	 * reading from one cpu area. That does not matter as long

	 * as we end up on the original cpu again when doing the cmpxchg.

	 *

	 * We must guarantee that tid and kmem_cache_cpu are retrieved on the

	 * same cpu. We read first the kmem_cache_cpu pointer and use it to read

	 * the tid. If we are preempted and switched to another cpu between the

	 * two reads, it's OK as the two are still associated with the same cpu

	 * and cmpxchg later will validate the cpu.

	/*

	 * Irqless object alloc/free algorithm used here depends on sequence

	 * of fetching cpu_slab's data. tid should be fetched before anything

	 * on c to guarantee that object and page associated with previous tid

	 * won't be used with current tid. If we fetch tid first, object and

	 * page could be one associated with next tid and our alloc/free

	 * request will be failed. In this case, we will retry. So, no problem.

	/*

	 * The transaction ids are globally unique per cpu and per operation on

	 * a per cpu queue. Thus they can be guarantee that the cmpxchg_double

	 * occurs on the right processor and that there was no operation on the

	 * linked list in between.

	/*

	 * We cannot use the lockless fastpath on PREEMPT_RT because if a

	 * slowpath has taken the local_lock_irqsave(), it is not protected

	 * against a fast path operation in an irq handler. So we need to take

	 * the slow path which uses local_lock. It is still relatively fast if

	 * there is a suitable cpu freelist.

		/*

		 * The cmpxchg will only match if there was no additional

		 * operation and if we are on the right processor.

		 *

		 * The cmpxchg does the following atomically (without lock

		 * semantics!)

		 * 1. Relocate first pointer to the current per cpu area.

		 * 2. Verify that tid and freelist have not been changed

		 * 3. If they were not changed replace tid and freelist

		 *

		 * Since this is without lock semantics the protection is only

		 * against code executing on this cpu *not* from access by

		 * other cpus.

 CONFIG_NUMA */

/*

 * Slow path handling. This may still be called frequently since objects

 * have a longer lifetime than the cpu slabs in most processing loads.

 *

 * So we still attempt to reduce cache line usage. Just take the slab

 * lock and free the item. If there is no additional partial page

 * handling required then we can return immediately.

				/*

				 * Slab was on no list before and will be

				 * partially empty

				 * We can defer the list move and instead

				 * freeze it.

 Needs to be taken off a list */

				/*

				 * Speculatively acquire the list_lock.

				 * If the cmpxchg does not succeed then we may

				 * drop the list_lock without any processing.

				 *

				 * Otherwise the list_lock will synchronize with

				 * other processors updating the list of slabs.

			/*

			 * The list lock was not taken therefore no list

			 * activity can be necessary.

			/*

			 * If we just froze the page then put it onto the

			 * per cpu partial list.

	/*

	 * Objects left in the slab. If it was not on the partial list before

	 * then add it.

		/*

		 * Slab on the partial list.

 Slab must be on the full list */

/*

 * Fastpath with forced inlining to produce a kfree and kmem_cache_free that

 * can perform fastpath freeing without additional function calls.

 *

 * The fastpath is only possible if we are freeing to the current cpu slab

 * of this processor. This typically the case if we have just allocated

 * the item before.

 *

 * If fastpath is not possible then fall back to __slab_free where we deal

 * with all sorts of special processing.

 *

 * Bulk free of a freelist with several objects (all pointing to the

 * same page) possible by specifying head and tail ptr, plus objects

 * count (cnt). Bulk free indicated by tail pointer being set.

 memcg_slab_free_hook() is already called for bulk free. */

	/*

	 * Determine the currently cpus per cpu slab.

	 * The cpu may change afterward. However that does not matter since

	 * data is retrieved via this pointer. If we are on the same cpu

	 * during the cmpxchg then the free will succeed.

 Same with comment on barrier() in slab_alloc_node() */

 CONFIG_PREEMPT_RT */

		/*

		 * We cannot use the lockless fastpath on PREEMPT_RT because if

		 * a slowpath has taken the local_lock_irqsave(), it is not

		 * protected against a fast path operation in an irq handler. So

		 * we need to take the local_lock. We shouldn't simply defer to

		 * __slab_free() as that wouldn't use the cpu freelist at all.

	/*

	 * With KASAN enabled slab_free_freelist_hook modifies the freelist

	 * to remove objects, whose reuse must be delayed.

/*

 * This function progressively scans the array with free objects (with

 * a limited look ahead) and extract objects belonging to the same

 * page.  It builds a detached freelist directly within the given

 * page/objects.  This can happen without any need for

 * synchronization, because the objects are owned by running process.

 * The freelist is build up as a single linked list in the objects.

 * The idea is, that this detached freelist can then be bulk

 * transferred to the real freelist(s), but only requiring a single

 * synchronization primitive.  Look ahead in the array is limited due

 * to performance reasons.

 Always re-init detached_freelist */

 Do we need !ZERO_OR_NULL_PTR(object) here? (for kfree) */

 Handle kalloc'ed objects */

 mark object processed */

 Derive kmem_cache from object */

 Support for memcg */

 mark object processed */

 Start new detached freelist */

 mark object processed */

 Skip processed objects */

 df->page is always set at this point */

 Opportunity build freelist */

 mark object processed */

 Limit look ahead search */

 Note that interrupts must be enabled when calling this function. */

 Note that interrupts must be enabled when calling this function. */

 memcg and kmem_cache debug support */

	/*

	 * Drain objects in the per cpu slab, while disabling local

	 * IRQs, which protects against PREEMPT and interrupts

	 * handlers invoking normal fastpath.

			/*

			 * We may have removed an object from c->freelist using

			 * the fastpath in the previous iteration; in that case,

			 * c->tid has not been bumped yet.

			 * Since ___slab_alloc() may reenable interrupts while

			 * allocating memory, we should bump c->tid now.

			/*

			 * Invoking slow path likely have side-effect

			 * of re-populating per CPU c->freelist

 goto for-loop */

	/*

	 * memcg and kmem_cache debug support and memory initialization.

	 * Done outside of the IRQ disabled fastpath loop.

/*

 * Object placement in a slab is made very easy because we always start at

 * offset 0. If we tune the size of the object to the alignment then we can

 * get the required alignment by putting one properly sized object after

 * another.

 *

 * Notice that the allocation order determines the sizes of the per cpu

 * caches. Each processor has always one slab available for allocations.

 * Increasing the allocation order reduces the number of times that slabs

 * must be moved on and off the partial lists and is therefore a factor in

 * locking overhead.

/*

 * Minimum / Maximum order of slab pages. This influences locking overhead

 * and slab fragmentation. A higher order reduces the number of partial slabs

 * and increases the number of allocations possible without having to

 * take the list_lock.

/*

 * Calculate the order of allocation given an slab object size.

 *

 * The order of allocation has significant impact on performance and other

 * system components. Generally order 0 allocations should be preferred since

 * order 0 does not cause fragmentation in the page allocator. Larger objects

 * be problematic to put into order 0 slabs because there may be too much

 * unused space left. We go to a higher order if more than 1/16th of the slab

 * would be wasted.

 *

 * In order to reach satisfactory performance we must ensure that a minimum

 * number of objects is in one slab. Otherwise we may generate too much

 * activity on the partial lists which requires taking the list_lock. This is

 * less a concern for large slabs though which are rarely used.

 *

 * slub_max_order specifies the order where we begin to stop considering the

 * number of objects in a slab as critical. If we reach slub_max_order then

 * we try to keep the page order as low as possible. So we accept more waste

 * of space in favor of a small page order.

 *

 * Higher order allocations also allow the placement of more objects in a

 * slab and thereby reduce object handling overhead. If the user has

 * requested a higher minimum order then we start with that one instead of

 * the smallest order which will fit the object.

	/*

	 * Attempt to find best configuration for a slab. This

	 * works by first attempting to generate a layout with

	 * the best configuration and backing off gradually.

	 *

	 * First we increase the acceptable waste in a slab. Then

	 * we reduce the minimum objects required in a slab.

		/*

		 * Some architectures will only update present cpus when

		 * onlining them, so don't trust the number if it's just 1. But

		 * we also don't want to use nr_cpu_ids always, as on some other

		 * architectures, there can be many possible cpus, but never

		 * onlined. Here we compromise between trying to avoid too high

		 * order on systems that appear larger than they are, and too

		 * low order on systems that appear smaller than they are.

	/*

	 * We were unable to place multiple objects in a slab. Now

	 * lets see if we can place a single object there.

	/*

	 * Doh this slab cannot be placed using slub_max_order.

	/*

	 * Must align to double word boundary for the double cmpxchg

	 * instructions to work; see __pcpu_double_call_return_bool().

/*

 * No kmalloc_node yet so do it by hand. We know that this is the first

 * slab on the node for this slabcache. There are no concurrent accesses

 * possible.

 *

 * Note that this function only works on the kmem_cache_node

 * when allocating for the kmem_cache_node. This is used for bootstrapping

 * memory on a fresh node that has no slab structures yet.

	/*

	 * No locks need to be taken here as it has just been

	 * initialized and there is no concurrent access.

	/*

	 * cpu_partial determined the maximum number of objects kept in the

	 * per cpu partial lists of a processor.

	 *

	 * Per cpu partial lists mainly contain slabs that just have one

	 * object freed. If they are used for allocation then they can be

	 * filled up again with minimal effort. The slab will never hit the

	 * per node partial lists and therefore no locking will be required.

	 *

	 * For backwards compatibility reasons, this is determined as number

	 * of objects, even though we now limit maximum number of pages, see

	 * slub_set_cpu_partial()

/*

 * calculate_sizes() determines the order and the distribution of data within

 * a slab object.

	/*

	 * Round up object size to the next word boundary. We can only

	 * place the free pointer at word boundaries and this determines

	 * the possible location of the free pointer.

	/*

	 * Determine if we can poison the object itself. If the user of

	 * the slab may touch the object after free or before allocation

	 * then we should never poison the object itself.

	/*

	 * If we are Redzoning then check if there is some space between the

	 * end of the object and the free pointer. If not then add an

	 * additional word to have some bytes to store Redzone information.

	/*

	 * With that we have determined the number of bytes in actual use

	 * by the object and redzoning.

		/*

		 * Relocate free pointer after the object if it is not

		 * permitted to overwrite the first word of the object on

		 * kmem_cache_free.

		 *

		 * This is the case if we do RCU, have a constructor or

		 * destructor, are poisoning the objects, or are

		 * redzoning an object smaller than sizeof(void *).

		 *

		 * The assumption that s->offset >= s->inuse means free

		 * pointer is outside of the object is used in the

		 * freeptr_outside_object() function. If that is no

		 * longer true, the function needs to be modified.

		/*

		 * Store freelist pointer near middle of object to keep

		 * it away from the edges of the object to avoid small

		 * sized over/underflows from neighboring allocations.

		/*

		 * Need to store information about allocs and frees after

		 * the object.

		/*

		 * Add some empty padding so that we can catch

		 * overwrites from earlier objects rather than let

		 * tracking information or the free pointer be

		 * corrupted if a user writes before the start

		 * of the object.

	/*

	 * SLUB stores one object immediately after another beginning from

	 * offset 0. In order to align the objects we have to simply size

	 * each object to conform to the alignment.

	/*

	 * Determine the number of objects per slab

		/*

		 * Disable debugging flags that store metadata if the min slab

		 * order increased.

 Enable fast mode */

	/*

	 * The larger the object size is, the more pages we want on the partial

	 * list to avoid pounding the page allocator excessively.

 Initialize the pre-computed randomized freelist if slab is up */

/*

 * Attempt to free all partial slabs on a node.

 * This is called from __kmem_cache_shutdown(). We must take list_lock

 * because sysfs file might still access partial list after the shutdowning.

/*

 * Release all resources used by a slab cache.

 Attempt to free all objects */

/********************************************************************

 *		Kmalloc subsystem

 CONFIG_NUMA */

/*

 * Rejects incorrectly sized objects and objects that are to be copied

 * to/from userspace but do not fall entirely within the containing slab

 * cache's usercopy region.

 *

 * Returns NULL if check passes, otherwise const char * to name of cache

 * to indicate an error.

 Find object and usable object size. */

 Reject impossible pointers. */

 Find offset within object. */

 Adjust for redzone and reject if within the redzone. */

 Allow address range falling entirely within usercopy region. */

 CONFIG_HARDENED_USERCOPY */

/*

 * kmem_cache_shrink discards empty slabs and promotes the slabs filled

 * up most to the head of the partial lists. New allocations will then

 * fill those up and thus they can be removed from the partial lists.

 *

 * The slabs with the least items are placed last. This results in them

 * being allocated from last increasing the chance that the last objects

 * are freed in them.

		/*

		 * Build lists of slabs to discard or promote.

		 *

		 * Note that concurrent frees may occur while we hold the

		 * list_lock. page->inuse here is the upper limit.

 Do not reread page->inuse */

 We do not keep full slabs on the list */

		/*

		 * Promote the slabs filled up most to the head of the

		 * partial list.

 Release empty slabs */

	/*

	 * If the node still has available memory. we need kmem_cache_node

	 * for it yet.

	/*

	 * We no longer free kmem_cache_node structures here, as it would be

	 * racy with all get_node() users, and infeasible to protect them with

	 * slab_mutex.

	/*

	 * If the node's memory is already available, then kmem_cache_node is

	 * already created. Nothing to do.

	/*

	 * We are bringing a node online. No memory is available yet. We must

	 * allocate a kmem_cache_node structure in order to bring the node

	 * online.

		/*

		 * The structure may already exist if the node was previously

		 * onlined and offlined.

		/*

		 * XXX: kmem_cache_alloc_node will fallback to other nodes

		 *      since memory is not yet available from the node that

		 *      is brought up.

	/*

	 * Any cache created after this point will also have kmem_cache_node

	 * initialized for the new node.

/********************************************************************

 *			Basic setup of slabs

/*

 * Used for early kmem_cache structures that were allocated using

 * the page allocator. Allocate them properly then fix up the pointers

 * that may be pointing to the wrong kmem_cache structure.

	/*

	 * This runs very early, and only the boot processor is supposed to be

	 * up.  Even if it weren't true, IRQs are not up so we couldn't fire

	 * IPIs around.

 Print slub debugging pointers without hashing */

	/*

	 * Initialize the nodemask for which we will allocate per node

	 * structures. Here we don't need taking slab_mutex yet.

 Able to allocate the per node structures */

 Now we can use the kmem_cache to allocate kmalloc slabs */

 Setup random freelists for each cache */

		/*

		 * Adjust the object sizes so that we clear

		 * the complete object on kzalloc.

 Mutex is not taken during early boot */

 Honor the call site pointer we received. */

 Honor the call site pointer we received. */

 Now we know that a valid freelist exists */

/*

 * Generate lists of code addresses where slabcache objects are allocated

 * and freed.

		/*

		 * There is nothing at "end". If we end up there

		 * we need to add something to before end.

	/*

	 * Not found. Insert new tracking element.

 CONFIG_DEBUG_FS   */

 CONFIG_SLUB_DEBUG */

 All slabs */

 Only partially allocated slabs */

 Only slabs used for cpu caches */

 Determine allocated objects not slabs */

 Determine object capacity not slabs */

	/*

	 * It is impossible to take "mem_hotplug_lock" here with "kernfs_mutex"

	 * already held which will conflict with an existing lock order:

	 *

	 * mem_hotplug_lock->slab_mutex->kernfs_mutex

	 *

	 * We don't really need mem_hotplug_lock (to hold off

	 * slab_mem_going_offline_callback) here because slab's memory hot

	 * unplug code doesn't destroy the kmem_cache->node[] data.

 Approximate half-full pages , see slub_set_cpu_partial() */

 CONFIG_SLUB_DEBUG */

 CONFIG_SLUB_STATS */

/* Create a unique string id for a slab cache:

 *

 * Format	:[flags-]size

	/*

	 * First flags affecting slabcache operations. We will only

	 * get here for aliasable slabs so we do not need to support

	 * too many flags. The flags here must cover all flags that

	 * are matched during merging to guarantee that the id is

	 * unique.

		/*

		 * Slabcache can never be merged so we can use the name proper.

		 * This is typically the case for debug situations. In that

		 * case we can catch duplicate names easily.

		/*

		 * Create a unique name for the slab as a target

		 * for the symlinks.

 Setup first alias */

/*

 * Need to buffer aliases during bootup until sysfs becomes

 * available lest we lose that information.

		/*

		 * If we have a leftover link then remove it.

 CONFIG_SYSFS */

/*

 * The /proc/slabinfo ABI

 CONFIG_SLUB_DEBUG */

 SPDX-License-Identifier: GPL-2.0

/*

 *  mm/mprotect.c

 *

 *  (C) Copyright 1994 Linus Torvalds

 *  (C) Copyright 2002 Christoph Hellwig

 *

 *  Address space accounting code	<alan@lxorguk.ukuu.org.uk>

 *  (C) Copyright 2002 Red Hat Inc, All Rights Reserved

	/*

	 * Can be called with only the mmap_lock for reading by

	 * prot_numa so we must check the pmd isn't constantly

	 * changing from under us from pmd_none to pmd_trans_huge

	 * and/or the other way around.

	/*

	 * The pmd points to a regular pte so the pmd can't change

	 * from under us even if the mmap_lock is only hold for

	 * reading.

 Get target node for single threaded private VMAs */

			/*

			 * Avoid trapping faults against the zero or KSM

			 * pages. See similar comment in change_huge_pmd.

 Avoid TLB flush if possible */

 Also skip shared copy-on-write pages */

				/*

				 * While migration can move some dirty pages,

				 * it cannot move them all from MIGRATE_ASYNC

				 * context.

				/*

				 * Don't mess with PTEs if page is already on the node

				 * a single-threaded process is running on.

				/*

				 * Leave the write bit to be handled

				 * by PF interrupt handler, then

				 * things like COW could be properly

				 * handled.

 Avoid taking write faults for known dirty pages */

				/*

				 * A protection check is difficult so

				 * just be safe and disable write

				/*

				 * We do not preserve soft-dirtiness. See

				 * copy_one_pte() for explanation.

/*

 * Used when setting automatic NUMA hinting protection where it is

 * critical that a numa hinting PMD is not confused with a bad PMD.

 See pmd_none_or_trans_huge_or_clear_bad for info on barrier */

		/*

		 * Automatic NUMA balancing walks the tables with mmap_lock

		 * held for read. It's possible a parallel update to occur

		 * between pmd_trans_huge() and a pmd_none_or_clear_bad()

		 * check leading to a false positive and clearing.

		 * Hence, it's necessary to atomically read the PMD value

		 * for all the checks.

 invoke the mmu notifier if the pmd is populated */

 huge pmd was handled */

 fall through, the trans huge pmd just split */

 Only flush the TLB if we actually modified any entries: */

	/*

	 * Do PROT_NONE PFN permission checks here when we can still

	 * bail out without undoing a lot of state. This is a rather

	 * uncommon case, so doesn't need to be very optimized.

	/*

	 * If we make a private mapping writable we increase our commit;

	 * but (without finer accounting) cannot reduce our commit if we

	 * make it unwritable again. hugetlb mapping were accounted for

	 * even if read-only so there is no need to account for them here

 Check space limits when area turns into data. */

	/*

	 * First try to merge with previous and/or next vma.

	/*

	 * vm_flags and vm_page_prot are protected by the mmap_lock

	 * held in write mode.

	/*

	 * Private VM_LOCKED VMA becoming writable: trigger COW to avoid major

	 * fault on access.

/*

 * pkey==-1 when doing a legacy mprotect()

 can't be both */

	/*

	 * If userspace did not allocate the pkey, do not let

	 * them use it here.

 Here we know that vma->vm_start <= nstart < vma->vm_end. */

 Does the application expect PROT_READ to imply PROT_EXEC */

		/*

		 * Each mprotect() call explicitly passes r/w/x permissions.

		 * If a permission is not passed to mprotect(), it must be

		 * cleared from the VMA.

 newflags >> 4 shift VM_MAY% in place of VM_% */

 Allow architectures to sanity-check the new flags */

 No flags supported yet. */

 check for unsupported init values */

	/*

	 * We could provide warnings or errors if any VMA still

	 * has the pkey set here.

 CONFIG_ARCH_HAS_PKEYS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Our various events all share the same buffer (because we don't want or need

 * to allocate a set of buffers *per event type*), so we need to protect against

 * concurrent _reg() and _unreg() calls, and count how many _reg() calls have

 * been made.

 Protected by reg_lock. */

/*

 * Size of the buffer for memcg path names. Ignoring stack trace support,

 * trace_events_hist.c uses MAX_FILTER_STR_VAL for this, so we also use it.

/*

 * How many contexts our trace events might be called in: normal, softirq, irq,

 * and NMI.

 Called with reg_lock held. */

 Wait for inflight memcg_path_buf users to finish. */

 If the refcount is going 0->1, proceed with allocating buffers. */

 Don't need to wait for inflights, they'd have gotten NULL. */

 Since we failed, undo the earlier ref increment. */

 If the refcount is going 1->0, proceed with freeing buffers. */

 !CONFIG_MEMCG */

 CONFIG_MEMCG */

/*

 * Write the given mm_struct's memcg path to a percpu buffer, and return a

 * pointer to it. If the path cannot be determined, or no buffer was available

 * (because the trace event is being unregistered), NULL is returned.

 *

 * Note: buffers are allocated per-cpu to avoid locking, so preemption must be

 * disabled by the caller before calling us, and re-enabled only after the

 * caller is done with the pointer.

 *

 * The caller must call put_memcg_path_buf() once the buffer is no longer

 * needed. This must be done while preemption is still disabled.

 CONFIG_MEMCG */

/*

 * Trace calls must be in a separate file, as otherwise there's a circular

 * dependency between linux/mmap_lock.h and trace/events/mmap_lock.h.

 CONFIG_TRACING */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/mm/memory.c

 *

 *  Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

/*

 * demand-loading started 01.12.91 - seems it is high on the list of

 * things wanted, and it should be easy to implement. - Linus

/*

 * Ok, demand-loading was easy, shared pages a little bit tricker. Shared

 * pages started 02.12.91, seems to work. - Linus.

 *

 * Tested sharing by executing about 30 /bin/sh: under the old kernel it

 * would have taken more than the 6M I have free, but it worked well as

 * far as I could see.

 *

 * Also corrected some "invalidate()"s - I wasn't doing enough of them.

/*

 * Real VM (paging to/from disk) started 18.12.91. Much more work and

 * thought has to go into this. Oh, well..

 * 19.12.91  -  works, somewhat. Sometimes I get faults, don't know why.

 *		Found it. Everything seems to work now.

 * 20.12.91  -  Ok, making the swap-device changeable like the root.

/*

 * 05.04.94  -  Multi-page memory management added for v1.1.

 *              Idea by Alex Bligh (alex@cconcepts.co.uk)

 *

 * 16.07.99  -  Support of BIGMEM added by Gerhard Wichert, Siemens AG

 *		(Gerhard.Wichert@pdb.siemens.de)

 *

 * Aug/Sep 2004 Changed to four level page tables (Andi Kleen)

/*

 * A number of key systems in x86 including ioremap() rely on the assumption

 * that high_memory defines the upper bound on direct map memory, then end

 * of ZONE_NORMAL.  Under CONFIG_DISCONTIG this means that max_low_pfn and

 * highstart_pfn must be the same; there must be no gap between ZONE_NORMAL

 * and ZONE_HIGHMEM.

/*

 * Randomize the address space (stacks, mmaps, brk, etc.).

 *

 * ( When CONFIG_COMPAT_BRK=y we exclude brk from randomization,

 *   as ancient (libc5 based) binaries can segfault. )

	/*

	 * Those arches which don't have hw access flag feature need to

	 * implement their own helper. By default, "true" means pagefault

	 * will be hit on old pte.

	/*

	 * Transitioning a PTE from 'old' to 'young' can be expensive on

	 * some architectures, even if it's performed in hardware. By

	 * default, "false" means prefaulted entries will be 'young'.

/*

 * CONFIG_MMU architectures set up ZERO_PAGE in their paging_init()

 sync counter once per 64 page faults */

 SPLIT_RSS_COUNTING */

 SPLIT_RSS_COUNTING */

/*

 * Note: this doesn't free the actual pages themselves. That

 * has been handled earlier when unmapping all the memory regions.

/*

 * This function frees user-level page tables of a process.

	/*

	 * The next few lines have given us lots of grief...

	 *

	 * Why are we testing PMD* at this top level?  Because often

	 * there will be no work to do at all, and we'd prefer not to

	 * go all the way down to the bottom just to discover that.

	 *

	 * Why all these "- 1"s?  Because 0 represents both the bottom

	 * of the address space and the top of it (using -1 for the

	 * top wouldn't help much: the masks would do the wrong thing).

	 * The rule is that addr 0 and floor 0 refer to the bottom of

	 * the address space, but end 0 and ceiling 0 refer to the top

	 * Comparisons need to use "end - 1" and "ceiling - 1" (though

	 * that end 0 case should be mythical).

	 *

	 * Wherever addr is brought up or ceiling brought down, we must

	 * be careful to reject "the opposite 0" before it confuses the

	 * subsequent tests.  But what about where end is brought down

	 * by PMD_SIZE below? no, end can't go down to 0 there.

	 *

	 * Whereas we round start (addr) and ceiling down, by different

	 * masks at different levels, in order to test whether a table

	 * now has no other vmas using it, so can be freed, we don't

	 * bother to round floor or end up - the tests don't need that.

	/*

	 * We add page table cache pages with PAGE_SIZE,

	 * (see pte_free_tlb()), flush the tlb if we need

		/*

		 * Hide vma from rmap and truncate_pagecache before freeing

		 * pgtables

			/*

			 * Optimization: gather nearby vmas into one call down

 Has another populated it ? */

		/*

		 * Ensure all pte setup (eg. pte page lock and page clearing) are

		 * visible before the pte is made visible to other CPUs by being

		 * put into page tables.

		 *

		 * The other side of the story is the pointer chasing in the page

		 * table walking code (when walking the page table without locking;

		 * ie. most of the time). Fortunately, these data accesses consist

		 * of a chain of data-dependent loads, meaning most CPUs (alpha

		 * being the notable exception) will already guarantee loads are

		 * seen in-order. See the alpha page table accessors for the

		 * smp_rmb() barriers in page table walking code.

 Could be smp_wmb__xxx(before|after)_spin_lock */

 Has another populated it ? */

 See comment in pmd_install() */

/*

 * This function is called to print an error when a bad pte

 * is found. For example, we might have a PFN-mapped pte in

 * a region that doesn't allow it.

 *

 * The calling function must still handle the error.

	/*

	 * Allow a burst of 60 reports, then keep quiet for that minute;

	 * or allow a steady drip of one report per second.

/*

 * vm_normal_page -- This function gets the "struct page" associated with a pte.

 *

 * "Special" mappings do not wish to be associated with a "struct page" (either

 * it doesn't exist, or it exists but they don't want to touch it). In this

 * case, NULL is returned here. "Normal" mappings do have a struct page.

 *

 * There are 2 broad cases. Firstly, an architecture may define a pte_special()

 * pte bit, in which case this function is trivial. Secondly, an architecture

 * may not have a spare pte bit, which requires a more complicated scheme,

 * described below.

 *

 * A raw VM_PFNMAP mapping (ie. one that is not COWed) is always considered a

 * special mapping (even if there are underlying and valid "struct pages").

 * COWed pages of a VM_PFNMAP are always normal.

 *

 * The way we recognize COWed pages within VM_PFNMAP mappings is through the

 * rules set up by "remap_pfn_range()": the vma will have the VM_PFNMAP bit

 * set, and the vm_pgoff will point to the first PFN mapped: thus every special

 * mapping will always honor the rule

 *

 *	pfn_of_page == vma->vm_pgoff + ((addr - vma->vm_start) >> PAGE_SHIFT)

 *

 * And for normal mappings this is false.

 *

 * This restricts such mappings to be a linear translation from virtual address

 * to pfn. To get around this restriction, we allow arbitrary mappings so long

 * as the vma is not a COW mapping; in that case, we know that all ptes are

 * special (because none can have been COWed).

 *

 *

 * In order to support COW of arbitrary special mappings, we have VM_MIXEDMAP.

 *

 * VM_MIXEDMAP mappings can likewise contain memory with or without "struct

 * page" backing, however the difference is that _all_ pages with a struct

 * page (that is, those where pfn_valid is true) are refcounted and considered

 * normal pages by the VM. The disadvantage is that pages are refcounted

 * (which can be slower and simply not an option for some PFNMAP users). The

 * advantage is that we don't have to follow the strict linearity rule of

 * PFNMAP mappings in order to support COWable mappings.

 *

 !CONFIG_ARCH_HAS_PTE_SPECIAL case follows: */

	/*

	 * NOTE! We still have PageReserved() pages in the page tables.

	 * eg. VDSO mappings can cause them to exist.

	/*

	 * There is no pmd_special() but there may be special pmds, e.g.

	 * in a direct-access (dax) mapping, so let's just replicate the

	 * !CONFIG_ARCH_HAS_PTE_SPECIAL case from vm_normal_page() here.

	/*

	 * NOTE! We still have PageReserved() pages in the page tables.

	 * eg. VDSO mappings can cause them to exist.

	/*

	 * No need to take a page reference as one was already

	 * created when the swap entry was made.

		/*

		 * Currently device exclusive access only supports anonymous

		 * memory so the entry shouldn't point to a filebacked page.

	/*

	 * No need to invalidate - it was non-present before. However

	 * secondary CPUs may have mappings that need invalidating.

/*

 * Tries to restore an exclusive pte if the page lock can be acquired without

 * sleeping.

/*

 * copy one vm_area from one task to the other. Assumes the page tables

 * already present in the new task to be cleared in the whole range

 * covered by this vma.

 make sure dst_mm is on swapoff's mmlist. */

			/*

			 * COW mappings require pages in both

			 * parent and child to be set to read.

		/*

		 * Update rss count even for unaddressable pages, as

		 * they should treated just like normal pages in this

		 * respect.

		 *

		 * We will likely want to have some new rss counters

		 * for unaddressable pages, at some point. But for now

		 * keep things as they are.

		/*

		 * We do not preserve soft-dirty information, because so

		 * far, checkpoint/restore is the only feature that

		 * requires that. And checkpoint/restore does not work

		 * when a device driver is involved (you cannot easily

		 * save and restore device driver state).

		/*

		 * Make device exclusive entries present by restoring the

		 * original entry then copying as for a present pte. Device

		 * exclusive entries currently only support private writable

		 * (ie. COW) mappings.

/*

 * Copy a present and normal page if necessary.

 *

 * NOTE! The usual case is that this doesn't need to do

 * anything, and can just return a positive value. That

 * will let the caller know that it can just increase

 * the page refcount and re-use the pte the traditional

 * way.

 *

 * But _if_ we need to copy it because it needs to be

 * pinned in the parent (and the child should get its own

 * copy rather than just a reference to the same page),

 * we'll do that here and return zero to let the caller

 * know we're done.

 *

 * And if we need a pre-allocated page but don't yet have

 * one, return a negative error to let the preallocation

 * code know so that it can do so outside the page table

 * lock.

	/*

	 * What we want to do is to check whether this page may

	 * have been pinned by the parent process.  If so,

	 * instead of wrprotect the pte on both sides, we copy

	 * the page immediately so that we'll always guarantee

	 * the pinned page won't be randomly replaced in the

	 * future.

	 *

	 * The page pinning checks are just "has this mm ever

	 * seen pinning", along with the (inexact) check of

	 * the page count. That might give false positives for

	 * for pinning, but it will work correctly.

	/*

	 * We have a prealloc page, all good!  Take it

	 * over and copy the page & arm it.

 All done, just insert the new page copy in the child */

 Uffd-wp needs to be delivered to dest pte as well */

/*

 * Copy one pte.  Returns 0 if succeeded, or -EAGAIN if one preallocated page

 * is required to copy this pte.

	/*

	 * If it's a COW mapping, write protect it both

	 * in the parent and the child

	/*

	 * If it's a shared mapping, mark it clean in

	 * the child

		/*

		 * We are holding two locks at this point - either of them

		 * could generate latencies in another task on another CPU.

			/*

			 * Device exclusive entry restored, continue by copying

			 * the now present pte.

 copy_present_pte() will clear `*prealloc' if consumed */

		/*

		 * If we need a pre-allocated page for this pte, drop the

		 * locks, allocate, and try again.

			/*

			 * pre-alloc page cannot be reused by next time so as

			 * to strictly follow mempolicy (e.g., alloc_page_vma()

			 * will allocate page according to address).  This

			 * could only happen if one pinned pte changed.

 We've captured and resolved the error. Reset, try again. */

 fall through */

 fall through */

	/*

	 * Don't copy ptes where a page fault will fill them correctly.

	 * Fork becomes much lighter when there are big shared or private

	 * readonly mappings. The tradeoff is that copy_page_range is more

	 * efficient than faulting.

		/*

		 * We do not free on error cases below as remove_vma

		 * gets called on error from higher level routine

	/*

	 * We need to invalidate the secondary MMU mappings only when

	 * there could be a permission downgrade on the ptes of the

	 * parent mm. And a permission downgrade will only happen if

	 * is_cow_mapping() returns true.

		/*

		 * Disabling preemption is not needed for the write side, as

		 * the read side doesn't spin, but goes to the mmap_lock.

		 *

		 * Use the raw variant of the seqcount_t write API to avoid

		 * lockdep complaining about preemptibility.

 If details->check_mapping, we leave swap entries. */

 Do the actual TLB flush before dropping ptl */

	/*

	 * If we forced a TLB flush (either due to running out of

	 * batch buffers or because we needed to flush dirty TLB

	 * entries before releasing the ptl), free the batched

	 * memory too. Restart if we didn't do everything.

 fall through */

			/*

			 * Take and drop THP pmd lock so that we cannot return

			 * prematurely, while zap_huge_pmd() has cleared *pmd,

			 * but not yet decremented compound_mapcount().

		/*

		 * Here there can be other concurrent MADV_DONTNEED or

		 * trans huge page faults running, and if the pmd is

		 * none or trans huge it can change under us. This is

		 * because MADV_DONTNEED holds the mmap_lock in read

		 * mode.

 fall through */

			/*

			 * It is undesirable to test vma->vm_file as it

			 * should be non-null for valid hugetlb area.

			 * However, vm_file will be NULL in the error

			 * cleanup path of mmap_region. When

			 * hugetlbfs ->mmap method fails,

			 * mmap_region() nullifies vma->vm_file

			 * before calling this function to clean up.

			 * Since no pte has actually been setup, it is

			 * safe to do nothing in this case.

/**

 * unmap_vmas - unmap a range of memory covered by a list of vma's

 * @tlb: address of the caller's struct mmu_gather

 * @vma: the starting vma

 * @start_addr: virtual address at which to start unmapping

 * @end_addr: virtual address at which to end unmapping

 *

 * Unmap all pages in the vma list.

 *

 * Only addresses between `start' and `end' will be unmapped.

 *

 * The VMA list must be sorted in ascending virtual address order.

 *

 * unmap_vmas() assumes that the caller will flush the whole unmapped address

 * range after unmap_vmas() returns.  So the only responsibility here is to

 * ensure that any thus-far unmapped pages are flushed before unmap_vmas()

 * drops the lock and schedules.

/**

 * zap_page_range - remove user pages in a given range

 * @vma: vm_area_struct holding the applicable pages

 * @start: starting address of pages to zap

 * @size: number of bytes to zap

 *

 * Caller must protect the VMA list

/**

 * zap_page_range_single - remove user pages in a given range

 * @vma: vm_area_struct holding the applicable pages

 * @address: starting address of pages to zap

 * @size: number of bytes to zap

 * @details: details of shared cache invalidation

 *

 * The range must fit into one VMA.

/**

 * zap_vma_ptes - remove ptes mapping the vma

 * @vma: vm_area_struct holding ptes to be zapped

 * @address: starting address of pages to zap

 * @size: number of bytes to zap

 *

 * This function only unmaps ptes assigned to VM_PFNMAP vmas.

 *

 * The entire address range must be fully contained within the vma.

 *

 Ok, finally just insert the thing.. */

/*

 * This is the old fallback for page remapping.

 *

 * For historical reasons, it only allows reserved pages. Only

 * old drivers should use this, and they needed to mark their

 * pages reserved for the old functions anyway.

/* insert_pages() amortizes the cost of spinlock operations

 * when inserting pages in a loop. Arch *must* define pte_index.

 Allocate the PTE if necessary; takes PMD lock once only. */

 ifdef pte_index */

/**

 * vm_insert_pages - insert multiple pages into user vma, batching the pmd lock.

 * @vma: user vma to map to

 * @addr: target start user address of these pages

 * @pages: source kernel pages

 * @num: in: number of pages to map. out: number of pages that were *not*

 * mapped. (0 means all pages were successfully mapped).

 *

 * Preferred over vm_insert_page() when inserting multiple pages.

 *

 * In case of error, we may have mapped a subset of the provided

 * pages. It is the caller's responsibility to account for this case.

 *

 * The same restrictions apply as in vm_insert_page().

 Defer page refcount checking till we're about to map that page. */

 ifdef pte_index */

/**

 * vm_insert_page - insert single page into user vma

 * @vma: user vma to map to

 * @addr: target user address of this page

 * @page: source kernel page

 *

 * This allows drivers to insert individual pages they've allocated

 * into a user vma.

 *

 * The page has to be a nice clean _individual_ kernel allocation.

 * If you allocate a compound page, you need to have marked it as

 * such (__GFP_COMP), or manually just split the page up yourself

 * (see split_page()).

 *

 * NOTE! Traditionally this was done with "remap_pfn_range()" which

 * took an arbitrary page protection parameter. This doesn't allow

 * that. Your vma protection will have to be set up correctly, which

 * means that if you want a shared writable mapping, you'd better

 * ask for a shared writable mapping!

 *

 * The page does not need to be reserved.

 *

 * Usually this function is called from f_op->mmap() handler

 * under mm->mmap_lock write-lock, so it can change vma->vm_flags.

 * Caller must set VM_MIXEDMAP on vma if it wants to call this

 * function from other places, for example from page-fault handler.

 *

 * Return: %0 on success, negative error code otherwise.

/*

 * __vm_map_pages - maps range of kernel pages into user vma

 * @vma: user vma to map to

 * @pages: pointer to array of source kernel pages

 * @num: number of pages in page array

 * @offset: user's requested vm_pgoff

 *

 * This allows drivers to map range of kernel pages into a user vma.

 *

 * Return: 0 on success and error code otherwise.

 Fail if the user requested offset is beyond the end of the object */

 Fail if the user requested size exceeds available object size */

/**

 * vm_map_pages - maps range of kernel pages starts with non zero offset

 * @vma: user vma to map to

 * @pages: pointer to array of source kernel pages

 * @num: number of pages in page array

 *

 * Maps an object consisting of @num pages, catering for the user's

 * requested vm_pgoff

 *

 * If we fail to insert any page into the vma, the function will return

 * immediately leaving any previously inserted pages present.  Callers

 * from the mmap handler may immediately return the error as their caller

 * will destroy the vma, removing any successfully inserted pages. Other

 * callers should make their own arrangements for calling unmap_region().

 *

 * Context: Process context. Called by mmap handlers.

 * Return: 0 on success and error code otherwise.

/**

 * vm_map_pages_zero - map range of kernel pages starts with zero offset

 * @vma: user vma to map to

 * @pages: pointer to array of source kernel pages

 * @num: number of pages in page array

 *

 * Similar to vm_map_pages(), except that it explicitly sets the offset

 * to 0. This function is intended for the drivers that did not consider

 * vm_pgoff.

 *

 * Context: Process context. Called by mmap handlers.

 * Return: 0 on success and error code otherwise.

			/*

			 * For read faults on private mappings the PFN passed

			 * in may not match the PFN we have mapped if the

			 * mapped PFN is a writeable COW page.  In the mkwrite

			 * case we are creating a writable PTE for a shared

			 * mapping and we expect the PFNs to match. If they

			 * don't match, we are likely racing with block

			 * allocation and mapping invalidation so just skip the

			 * update.

 Ok, finally just insert the thing.. */

 XXX: why not for insert_page? */

/**

 * vmf_insert_pfn_prot - insert single pfn into user vma with specified pgprot

 * @vma: user vma to map to

 * @addr: target user address of this page

 * @pfn: source kernel pfn

 * @pgprot: pgprot flags for the inserted page

 *

 * This is exactly like vmf_insert_pfn(), except that it allows drivers

 * to override pgprot on a per-page basis.

 *

 * This only makes sense for IO mappings, and it makes no sense for

 * COW mappings.  In general, using multiple vmas is preferable;

 * vmf_insert_pfn_prot should only be used if using multiple VMAs is

 * impractical.

 *

 * See vmf_insert_mixed_prot() for a discussion of the implication of using

 * a value of @pgprot different from that of @vma->vm_page_prot.

 *

 * Context: Process context.  May allocate using %GFP_KERNEL.

 * Return: vm_fault_t value.

	/*

	 * Technically, architectures with pte_special can avoid all these

	 * restrictions (same for remap_pfn_range).  However we would like

	 * consistency in testing and feature parity among all, so we should

	 * try to keep these invariants in place for everybody.

/**

 * vmf_insert_pfn - insert single pfn into user vma

 * @vma: user vma to map to

 * @addr: target user address of this page

 * @pfn: source kernel pfn

 *

 * Similar to vm_insert_page, this allows drivers to insert individual pages

 * they've allocated into a user vma. Same comments apply.

 *

 * This function should only be called from a vm_ops->fault handler, and

 * in that case the handler should return the result of this function.

 *

 * vma cannot be a COW mapping.

 *

 * As this is called only for pages that do not currently exist, we

 * do not need to flush old virtual caches or the TLB.

 *

 * Context: Process context.  May allocate using %GFP_KERNEL.

 * Return: vm_fault_t value.

 these checks mirror the abort conditions in vm_normal_page */

	/*

	 * If we don't have pte special, then we have to use the pfn_valid()

	 * based VM_MIXEDMAP scheme (see vm_normal_page), and thus we *must*

	 * refcount the page if pfn_valid is true (hence insert_page rather

	 * than insert_pfn).  If a zero_pfn were inserted into a VM_MIXEDMAP

	 * without pte special, it would there be refcounted as a normal page.

		/*

		 * At this point we are committed to insert_page()

		 * regardless of whether the caller specified flags that

		 * result in pfn_t_has_page() == false.

/**

 * vmf_insert_mixed_prot - insert single pfn into user vma with specified pgprot

 * @vma: user vma to map to

 * @addr: target user address of this page

 * @pfn: source kernel pfn

 * @pgprot: pgprot flags for the inserted page

 *

 * This is exactly like vmf_insert_mixed(), except that it allows drivers

 * to override pgprot on a per-page basis.

 *

 * Typically this function should be used by drivers to set caching- and

 * encryption bits different than those of @vma->vm_page_prot, because

 * the caching- or encryption mode may not be known at mmap() time.

 * This is ok as long as @vma->vm_page_prot is not used by the core vm

 * to set caching and encryption bits for those vmas (except for COW pages).

 * This is ensured by core vm only modifying these page table entries using

 * functions that don't touch caching- or encryption bits, using pte_modify()

 * if needed. (See for example mprotect()).

 * Also when new page-table entries are created, this is only done using the

 * fault() callback, and never using the value of vma->vm_page_prot,

 * except for page-table entries that point to anonymous pages as the result

 * of COW.

 *

 * Context: Process context.  May allocate using %GFP_KERNEL.

 * Return: vm_fault_t value.

/*

 *  If the insertion of PTE failed because someone else already added a

 *  different entry in the mean time, we treat that as success as we assume

 *  the same entry was actually inserted.

/*

 * maps a range of physical memory into the requested pages. the old

 * mappings are removed. any references to nonexistent pages results

 * in null mappings (currently treated as "copy-on-access")

/*

 * Variant of remap_pfn_range that does not call track_pfn_remap.  The caller

 * must have pre-validated the caching bits of the pgprot_t.

	/*

	 * Physically remapped pages are special. Tell the

	 * rest of the world about it:

	 *   VM_IO tells people not to look at these pages

	 *	(accesses can have side effects).

	 *   VM_PFNMAP tells the core MM that the base pages are just

	 *	raw PFN mappings, and do not have a "struct page" associated

	 *	with them.

	 *   VM_DONTEXPAND

	 *      Disable vma merging and expanding with mremap().

	 *   VM_DONTDUMP

	 *      Omit vma from core dump, even when VM_IO turned off.

	 *

	 * There's a horrible special case to handle copy-on-write

	 * behaviour that some programs depend on. We mark the "original"

	 * un-COW'ed pages by matching them up with "vma->vm_pgoff".

	 * See vm_normal_page() for details.

/**

 * remap_pfn_range - remap kernel memory to userspace

 * @vma: user vma to map to

 * @addr: target page aligned user address to start at

 * @pfn: page frame number of kernel physical memory address

 * @size: size of mapping area

 * @prot: page protection flags for this mapping

 *

 * Note: this is only safe if the mm semaphore is held when called.

 *

 * Return: %0 on success, negative error code otherwise.

/**

 * vm_iomap_memory - remap memory to userspace

 * @vma: user vma to map to

 * @start: start of the physical memory to be mapped

 * @len: size of area

 *

 * This is a simplified io_remap_pfn_range() for common driver use. The

 * driver just needs to give us the physical memory range to be mapped,

 * we'll figure out the rest from the vma information.

 *

 * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get

 * whatever write-combining details or similar.

 *

 * Return: %0 on success, negative error code otherwise.

 Check that the physical memory area passed in looks valid */

	/*

	 * You *really* shouldn't map things that aren't page-aligned,

	 * but we've historically allowed it because IO memory might

	 * just have smaller alignment.

 We start the mapping 'vm_pgoff' pages into the area */

 Can we fit all of the mapping? */

 Ok, let it rip */

/*

 * Scan a region of virtual memory, filling in page tables as necessary

 * and calling a provided function on each leaf page table.

/*

 * Scan a region of virtual memory, calling a provided function on

 * each leaf page table where it exists.

 *

 * Unlike apply_to_page_range, this does _not_ fill in page tables

 * where they are absent.

/*

 * handle_pte_fault chooses page fault handler according to an entry which was

 * read non-atomically.  Before making any commitment, on those architectures

 * or configurations (e.g. i386 with PAE) which might give a mix of unmatched

 * parts, do_swap_page must check under lock before unmapping the pte and

 * proceeding (but do_wp_page is only called after already making such a check;

 * and do_anonymous_page can safely check later on).

	/*

	 * If the source page was a PFN mapping, we don't have

	 * a "struct page" for it. We do a best-effort copy by

	 * just copying from the original user address. If that

	 * fails, we just zero-fill it. Live with it.

	/*

	 * On architectures with software "accessed" bits, we would

	 * take a double page fault, so mark it accessed here.

			/*

			 * Other thread has already handled the fault

			 * and update local tlb only

	/*

	 * This really shouldn't fail, because the page is there

	 * in the page tables. But it might just be unreadable,

	 * in which case we just give up and fill the result with

	 * zeroes.

 Re-validate under PTL if the page is still mapped */

 The PTE changed under us, update local tlb */

		/*

		 * The same page can be mapped back since last copy attempt.

		 * Try to copy again under PTL.

			/*

			 * Give a warn in case there can be some obscure

			 * use-case

	/*

	 * Special mappings (e.g. VDSO) do not have any file so fake

	 * a default GFP_KERNEL for them.

/*

 * Notify the address space that the page is about to become writable so that

 * it can prohibit this or wait for the page to get into an appropriate state.

 *

 * We do this without the lock held, so that it can sleep if it needs to.

 Restore original flags so that caller is not surprised */

 retry */

/*

 * Handle dirtying of a page in shared file mapping on a write fault.

 *

 * The function expects the page to be locked and unlocks it.

	/*

	 * Take a local copy of the address_space - page.mapping may be zeroed

	 * by truncate after unlock_page().   The address_space itself remains

	 * pinned by vma->vm_file's reference.  We rely on unlock_page()'s

	 * release semantics to prevent the compiler from undoing this copying.

	/*

	 * Throttle page dirtying rate down to writeback speed.

	 *

	 * mapping may be NULL here because some device drivers do not

	 * set page.mapping but still dirty their pages

	 *

	 * Drop the mmap_lock before waiting on IO, if we can. The file

	 * is pinning the mapping, as per above.

/*

 * Handle write page faults for pages that can be reused in the current vma

 *

 * This can happen either due to the mapping being with the VM_SHARED flag,

 * or due to us being the last reference standing to the page. In either

 * case, all we need to do here is to mark the page as writable and update

 * any related book-keeping.

	/*

	 * Clear the pages cpupid information as the existing

	 * information potentially belongs to a now completely

	 * unrelated process.

/*

 * Handle the case of a page which we actually need to copy to a new page.

 *

 * Called with mmap_lock locked and the old page referenced, but

 * without the ptl held.

 *

 * High level logic flow:

 *

 * - Allocate a page, copy the content of the old page to the new one.

 * - Handle book keeping and accounting - cgroups, mmu-notifiers, etc.

 * - Take the PTL. If the pte changed, bail out and release the allocated page

 * - If the pte is still the way we remember it, update the page table and all

 *   relevant references. This includes dropping the reference the page-table

 *   held to the old page, as well as updating the rmap.

 * - In any case, unlock the PTL and drop the reference we took to the old page.

			/*

			 * COW failed, if the fault was solved by other,

			 * it's fine. If not, userspace would re-fault on

			 * the same address and we will handle the fault

			 * from the second attempt.

	/*

	 * Re-check the pte - we dropped the lock

		/*

		 * Clear the pte entry and flush it first, before updating the

		 * pte with the new entry, to keep TLBs on different CPUs in

		 * sync. This code used to set the new PTE then flush TLBs, but

		 * that left a window where the new PTE could be loaded into

		 * some TLBs while the old PTE remains in others.

		/*

		 * We call the notify macro here because, when using secondary

		 * mmu page tables (such as kvm shadow page tables), we want the

		 * new page to be mapped directly into the secondary page table.

			/*

			 * Only after switching the pte to the new page may

			 * we remove the mapcount here. Otherwise another

			 * process may come and find the rmap count decremented

			 * before the pte is switched to the new page, and

			 * "reuse" the old page writing into it while our pte

			 * here still points into it and can be read by other

			 * threads.

			 *

			 * The critical issue is to order this

			 * page_remove_rmap with the ptp_clear_flush above.

			 * Those stores are ordered by (if nothing else,)

			 * the barrier present in the atomic_add_negative

			 * in page_remove_rmap.

			 *

			 * Then the TLB flush in ptep_clear_flush ensures that

			 * no process can access the old page before the

			 * decremented mapcount is visible. And the old page

			 * cannot be reused until after the decremented

			 * mapcount is visible. So transitively, TLBs to

			 * old page will be flushed before it can be reused.

 Free the old page.. */

	/*

	 * No need to double call mmu_notifier->invalidate_range() callback as

	 * the above ptep_clear_flush_notify() did already call it.

		/*

		 * Don't let another task, with possibly unlocked vma,

		 * keep the mlocked page.

 LRU manipulation */

/**

 * finish_mkwrite_fault - finish page fault for a shared mapping, making PTE

 *			  writeable once the page is prepared

 *

 * @vmf: structure describing the fault

 *

 * This function handles all that is needed to finish a write page fault in a

 * shared mapping due to PTE being read-only once the mapped page is prepared.

 * It handles locking of PTE and modifying it.

 *

 * The function expects the page to be locked or other protection against

 * concurrent faults / writeback (such as DAX radix tree locks).

 *

 * Return: %0 on success, %VM_FAULT_NOPAGE when PTE got changed before

 * we acquired PTE lock.

	/*

	 * We might have raced with another page fault while we released the

	 * pte_offset_map_lock.

/*

 * Handle write page faults for VM_MIXEDMAP or VM_PFNMAP for a VM_SHARED

 * mapping

/*

 * This routine handles present pages, when users try to write

 * to a shared page. It is done by copying the page to a new address

 * and decrementing the shared-page counter for the old page.

 *

 * Note that this routine assumes that the protection checks have been

 * done by the caller (the low-level page fault routine in most cases).

 * Thus we can safely just mark it writable once we've done any necessary

 * COW.

 *

 * We also mark the page dirty at this point even though the page will

 * change only once the write actually happens. This avoids a few races,

 * and potentially makes it more efficient.

 *

 * We enter with non-exclusive mmap_lock (to exclude vma changes,

 * but allow concurrent faults), with pte both mapped and locked.

 * We return with mmap_lock still held, but pte unmapped and unlocked.

	/*

	 * Userfaultfd write-protect can defer flushes. Ensure the TLB

	 * is flushed in this case before copying.

		/*

		 * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a

		 * VM_PFNMAP VMA.

		 *

		 * We should not cow pages in a shared writeable mapping.

		 * Just mark the pages writable and/or call ops->pfn_mkwrite.

	/*

	 * Take out anonymous pages first, anonymous shared vmas are

	 * not dirty accountable.

 PageKsm() doesn't necessarily raise the page refcount */

		/*

		 * Ok, we've got the only map reference, and the only

		 * page count reference, and the page is locked,

		 * it's dark out, and we're wearing sunglasses. Hit it.

	/*

	 * Ok, we need to copy. Oh, well..

/**

 * unmap_mapping_page() - Unmap single page from processes.

 * @page: The locked page to be unmapped.

 *

 * Unmap this page from any userspace process which still has it mmaped.

 * Typically, for efficiency, the range of nearby pages has already been

 * unmapped by unmap_mapping_pages() or unmap_mapping_range().  But once

 * truncation or invalidation holds the lock on a page, it may find that

 * the page has been remapped again: and then uses unmap_mapping_page()

 * to unmap it finally.

/**

 * unmap_mapping_pages() - Unmap pages from processes.

 * @mapping: The address space containing pages to be unmapped.

 * @start: Index of first page to be unmapped.

 * @nr: Number of pages to be unmapped.  0 to unmap to end of file.

 * @even_cows: Whether to unmap even private COWed pages.

 *

 * Unmap the pages in this address space from any userspace process which

 * has them mmaped.  Generally, you want to remove COWed pages as well when

 * a file is being truncated, but not when invalidating pages from the page

 * cache.

/**

 * unmap_mapping_range - unmap the portion of all mmaps in the specified

 * address_space corresponding to the specified byte range in the underlying

 * file.

 *

 * @mapping: the address space containing mmaps to be unmapped.

 * @holebegin: byte in first page to unmap, relative to the start of

 * the underlying file.  This will be rounded down to a PAGE_SIZE

 * boundary.  Note that this is different from truncate_pagecache(), which

 * must keep the partial page.  In contrast, we must get rid of

 * partial pages.

 * @holelen: size of prospective hole in bytes.  This will be rounded

 * up to a PAGE_SIZE boundary.  A holelen of zero truncates to the

 * end of the file.

 * @even_cows: 1 when truncating a file, unmap even private COWed pages;

 * but 0 when invalidating pagecache, don't throw away private data.

 Check for overflow. */

/*

 * Restore a potential device exclusive pte to a working pte entry

/*

 * We enter with non-exclusive mmap_lock (to exclude vma changes,

 * but allow concurrent faults), and pte mapped but not yet locked.

 * We return with pte unmapped and unlocked.

 *

 * We return with the mmap_lock locked or unlocked in the same cases

 * as does filemap_fault().

 Prevent swapoff from happening to us. */

 skip swapcache */

 To provide entry to swap_readpage() */

			/*

			 * Back out if somebody else faulted in this pte

			 * while we released the pte lock.

 Had to read the page from swap area: Major fault */

		/*

		 * hwpoisoned dirty swapcache pages are kept for killing

		 * owner processes (which may be unknown at hwpoison time)

	/*

	 * Make sure try_to_free_swap or reuse_swap_page or swapoff did not

	 * release the swapcache from under us.  The page pin, and pte_same

	 * test below, are not enough to exclude that.  Even if it is still

	 * swapcache, we need to check that the page's swap has not changed.

	/*

	 * Back out if somebody else already faulted in this pte.

	/*

	 * The page isn't present yet, go ahead with the fault.

	 *

	 * Be careful about the sequence of operations here.

	 * To get its accounting right, reuse_swap_page() must be called

	 * while the page is counted on swap but not yet in mapcount i.e.

	 * before page_add_anon_rmap() and swap_free(); try_to_free_swap()

	 * must be called after the swap_free(), or it will never succeed.

 ksm created a completely new copy */

		/*

		 * Hold the lock to avoid the swap entry to be reused

		 * until we take the PT lock for the pte_same() check

		 * (to avoid false positives from pte_same). For

		 * further safety release the lock after the swap_free

		 * so that the swap count won't change under a

		 * parallel locked swapcache.

 No need to invalidate - it was non-present before */

/*

 * We enter with non-exclusive mmap_lock (to exclude vma changes,

 * but allow concurrent faults), and pte mapped but not yet locked.

 * We return with mmap_lock still held, but pte unmapped and unlocked.

 File mapping without ->vm_ops ? */

	/*

	 * Use pte_alloc() instead of pte_alloc_map().  We can't run

	 * pte_offset_map() on pmds where a huge pmd might be created

	 * from a different thread.

	 *

	 * pte_alloc_map() is safe to use under mmap_write_lock(mm) or when

	 * parallel threads are excluded by other means.

	 *

	 * Here we only have mmap_read_lock(mm).

 See comment in handle_pte_fault() */

 Use the zero-page for reads */

 Deliver the page fault to userland, check inside PT lock */

 Allocate our own private page. */

	/*

	 * The memory barrier inside __SetPageUptodate makes sure that

	 * preceding stores to the page contents become visible before

	 * the set_pte_at() write.

 Deliver the page fault to userland, check inside PT lock */

 No need to invalidate - it was non-present before */

/*

 * The mmap_lock must have been held on entry, and may have been

 * released depending on flags and vma->vm_ops->fault() return value.

 * See filemap_fault() and __lock_page_retry().

	/*

	 * Preallocate pte before we take page_lock because this might lead to

	 * deadlocks for memcg reclaim which waits for pages under writeback:

	 *				lock_page(A)

	 *				SetPageWriteback(A)

	 *				unlock_page(A)

	 * lock_page(B)

	 *				lock_page(B)

	 * pte_alloc_one

	 *   shrink_page_list

	 *     wait_on_page_writeback(A)

	 *				SetPageWriteback(B)

	 *				unlock_page(B)

	 *				# flush A, B to clear the writeback

	/*

	 * We are going to consume the prealloc table,

	 * count that as nr_ptes.

	/*

	 * Just backoff if any subpage of a THP is corrupted otherwise

	 * the corrupted page may mapped by PMD silently to escape the

	 * check.  This kind of THP just can be PTE mapped.  Access to

	 * the corrupted subpage should trigger SIGBUS as expected.

	/*

	 * Archs like ppc64 need additional space to store information

	 * related to pte entry. Use the preallocated table for that.

	/*

	 * deposit and withdraw with pmd lock held

 fault is handled */

 copy-on-write page */

/**

 * finish_fault - finish page fault once we have prepared the page to fault

 *

 * @vmf: structure describing the fault

 *

 * This function handles all that is needed to finish a page fault once the

 * page to fault in is prepared. It handles locking of PTEs, inserts PTE for

 * given page, adds reverse page mapping, handles memcg charges and LRU

 * addition.

 *

 * The function expects the page to be locked and on success it consumes a

 * reference of a page being mapped (for the PTE which maps it).

 *

 * Return: %0 on success, %VM_FAULT_ code in case of error.

 Did we COW the page? */

	/*

	 * check even for read faults because we might have lost our CoWed

	 * page

 See comment in handle_pte_fault() */

 Re-check under ptl */

/*

 * fault_around_bytes must be rounded down to the nearest page order as it's

 * what do_fault_around() expects to see.

 rounddown_pow_of_two(0) is undefined */

/*

 * do_fault_around() tries to map few pages around the fault address. The hope

 * is that the pages will be needed soon and this will lower the number of

 * faults to handle.

 *

 * It uses vm_ops->map_pages() to map the pages, which skips the page if it's

 * not ready to be mapped: not up-to-date, locked, etc.

 *

 * This function is called with the page table lock taken. In the split ptlock

 * case the page table lock only protects only those entries which belong to

 * the page table corresponding to the fault address.

 *

 * This function doesn't cross the VMA boundaries, in order to call map_pages()

 * only once.

 *

 * fault_around_bytes defines how many bytes we'll try to map.

 * do_fault_around() expects it to be set to a power of two less than or equal

 * to PTRS_PER_PTE.

 *

 * The virtual address of the area that we map is naturally aligned to

 * fault_around_bytes rounded down to the machine page size

 * (and therefore to page order).  This way it's easier to guarantee

 * that we don't cross page table boundaries.

	/*

	 *  end_pgoff is either the end of the page table, the end of

	 *  the vma or nr_pages from start_pgoff, depending what is nearest.

	/*

	 * Let's call ->map_pages() first and use ->fault() as fallback

	 * if page by the offset is not ready to be mapped (cold cache or

	 * something).

	/*

	 * Check if the backing address space wants to know that the page is

	 * about to become writable

/*

 * We enter with non-exclusive mmap_lock (to exclude vma changes,

 * but allow concurrent faults).

 * The mmap_lock may have been released depending on flags and our

 * return value.  See filemap_fault() and __folio_lock_or_retry().

 * If mmap_lock is released, vma may become invalid (for example

 * by other thread calling munmap()).

	/*

	 * The VMA was not fully populated on mmap() or missing VM_DONTEXPAND

		/*

		 * If we find a migration pmd entry or a none pmd entry, which

		 * should never happen, return SIGBUS

			/*

			 * Make sure this is not a temporary clearing of pte

			 * by holding ptl and checking again. A R/M/W update

			 * of pte involves: take ptl, clearing the pte so that

			 * we don't have concurrent modification by hardware

			 * followed by an update.

 preallocated pagetable is unused: free it */

	/*

	 * The "pte" at this point cannot be used safely without

	 * validation through pte_unmap_same(). It's of NUMA type but

	 * the pfn may be screwed if the read is non atomic.

 Get the normal PTE  */

 TODO: handle PTE-mapped THP */

	/*

	 * Avoid grouping on RO pages in general. RO pages shouldn't hurt as

	 * much anyway since they can be in shared cache state. This misses

	 * the case where a mapping is writable but the process never writes

	 * to it but pte_write gets cleared during protection updates and

	 * pte_dirty has unpredictable behaviour between PTE scan updates,

	 * background writeback, dirty balancing and application behaviour.

	/*

	 * Flag if the page is shared between multiple address spaces. This

	 * is later used when determining whether to group tasks together

 Migrate to the requested node */

	/*

	 * Make it present again, depending on how arch implements

	 * non-accessible ptes, some can allow access by kernel mode.

 `inline' is required to avoid gcc 4.1.2 build error */

 COW or write-notify handled on pte level: split pmd. */

 No support for anonymous transparent PUD pages yet */

 COW or write-notify not handled on PUD level: split pud.*/

 CONFIG_TRANSPARENT_HUGEPAGE */

 No support for anonymous transparent PUD pages yet */

 CONFIG_TRANSPARENT_HUGEPAGE */

/*

 * These routines also need to handle stuff like marking pages dirty

 * and/or accessed for architectures that don't do it in hardware (most

 * RISC architectures).  The early dirtying is also good on the i386.

 *

 * There is also a hook called "update_mmu_cache()" that architectures

 * with external mmu caches can use to update those (ie the Sparc or

 * PowerPC hashed page tables that act as extended TLBs).

 *

 * We enter with non-exclusive mmap_lock (to exclude vma changes, but allow

 * concurrent faults).

 *

 * The mmap_lock may have been released depending on flags and our return value.

 * See filemap_fault() and __folio_lock_or_retry().

		/*

		 * Leave __pte_alloc() until later: because vm_ops->fault may

		 * want to allocate huge page, and if we expose page table

		 * for an instant, it will be difficult to retract from

		 * concurrent faults and from rmap lookups.

		/*

		 * If a huge pmd materialized under us just retry later.  Use

		 * pmd_trans_unstable() via pmd_devmap_trans_unstable() instead

		 * of pmd_trans_huge() to ensure the pmd didn't become

		 * pmd_trans_huge under us and then back to pmd_none, as a

		 * result of MADV_DONTNEED running immediately after a huge pmd

		 * fault in a different thread of this mm, in turn leading to a

		 * misleading pmd_trans_huge() retval. All we have to ensure is

		 * that it is a regular pmd that we can walk with

		 * pte_offset_map() and we can do that through an atomic read

		 * in C, which is what pmd_trans_unstable() provides.

		/*

		 * A regular pmd is established and it can't morph into a huge

		 * pmd from under us anymore at this point because we hold the

		 * mmap_lock read mode and khugepaged takes it in write mode.

		 * So now it's safe to run pte_offset_map().

		/*

		 * some architectures can have larger ptes than wordsize,

		 * e.g.ppc44x-defconfig has CONFIG_PTE_64BIT=y and

		 * CONFIG_32BIT=y, so READ_ONCE cannot guarantee atomic

		 * accesses.  The code below just needs a consistent view

		 * for the ifs and we later double check anyway with the

		 * ptl lock held. So here a barrier will do.

 Skip spurious TLB flush for retried page fault */

		/*

		 * This is needed only for protection faults but the arch code

		 * is not yet telling us if this is a protection fault or not.

		 * This still avoids useless tlb flushes for .text page faults

		 * with threads.

/*

 * By the time we get here, we already hold the mm semaphore

 *

 * The mmap_lock may have been released depending on flags and our

 * return value.  See filemap_fault() and __folio_lock_or_retry().

 NUMA case for anonymous PUDs would go here */

 Huge pud page fault raced with pmd_alloc? */

/**

 * mm_account_fault - Do page fault accounting

 *

 * @regs: the pt_regs struct pointer.  When set to NULL, will skip accounting

 *        of perf event counters, but we'll still do the per-task accounting to

 *        the task who triggered this page fault.

 * @address: the faulted address.

 * @flags: the fault flags.

 * @ret: the fault retcode.

 *

 * This will take care of most of the page fault accounting.  Meanwhile, it

 * will also include the PERF_COUNT_SW_PAGE_FAULTS_[MAJ|MIN] perf counter

 * updates.  However, note that the handling of PERF_COUNT_SW_PAGE_FAULTS should

 * still be in per-arch page fault handlers at the entry of page fault.

	/*

	 * We don't do accounting for some specific faults:

	 *

	 * - Unsuccessful faults (e.g. when the address wasn't valid).  That

	 *   includes arch_vma_access_permitted() failing before reaching here.

	 *   So this is not a "this many hardware page faults" counter.  We

	 *   should use the hw profiling for that.

	 *

	 * - Incomplete faults (VM_FAULT_RETRY).  They will only be counted

	 *   once they're completed.

	/*

	 * We define the fault as a major fault when the final successful fault

	 * is VM_FAULT_MAJOR, or if it retried (which implies that we couldn't

	 * handle it immediately previously).

	/*

	 * If the fault is done for GUP, regs will be NULL.  We only do the

	 * accounting for the per thread fault counters who triggered the

	 * fault, and we skip the perf event updates.

/*

 * By the time we get here, we already hold the mm semaphore

 *

 * The mmap_lock may have been released depending on flags and our

 * return value.  See filemap_fault() and __folio_lock_or_retry().

 do counter updates before entering really critical section. */

	/*

	 * Enable the memcg OOM handling for faults triggered in user

	 * space.  Kernel faults are handled more gracefully.

		/*

		 * The task may have entered a memcg OOM situation but

		 * if the allocation error was handled gracefully (no

		 * VM_FAULT_OOM), there is no need to kill anything.

		 * Just clean up the OOM state peacefully.

/*

 * Allocate p4d page table.

 * We've already handled the fast-path in-line.

 Another has populated it */

 See comment in pmd_install() */

 __PAGETABLE_P4D_FOLDED */

/*

 * Allocate page upper directory.

 * We've already handled the fast-path in-line.

 See comment in pmd_install() */

 Another has populated it */

 __PAGETABLE_PUD_FOLDED */

/*

 * Allocate page middle directory.

 * We've already handled the fast-path in-line.

 See comment in pmd_install() */

 Another has populated it */

 __PAGETABLE_PMD_FOLDED */

/**

 * follow_pte - look up PTE at a user virtual address

 * @mm: the mm_struct of the target address space

 * @address: user virtual address

 * @ptepp: location to store found PTE

 * @ptlp: location to store the lock for the PTE

 *

 * On a successful return, the pointer to the PTE is stored in @ptepp;

 * the corresponding lock is taken and its location is stored in @ptlp.

 * The contents of the PTE are only stable until @ptlp is released;

 * any further use, if any, must be protected against invalidation

 * with MMU notifiers.

 *

 * Only IO mappings and raw PFN mappings are allowed.  The mmap semaphore

 * should be taken for read.

 *

 * KVM uses this function.  While it is arguably less bad than ``follow_pfn``,

 * it is not a good general-purpose API.

 *

 * Return: zero on success, -ve otherwise.

/**

 * follow_pfn - look up PFN at a user virtual address

 * @vma: memory mapping

 * @address: user virtual address

 * @pfn: location to store found PFN

 *

 * Only IO mappings and raw PFN mappings are allowed.

 *

 * This function does not allow the caller to read the permissions

 * of the PTE.  Do not use it.

 *

 * Return: zero and the pfn at @pfn on success, -ve otherwise.

/**

 * generic_access_phys - generic implementation for iomem mmap access

 * @vma: the vma to access

 * @addr: userspace address, not relative offset within @vma

 * @buf: buffer to read/write

 * @len: length of transfer

 * @write: set to FOLL_WRITE when writing, otherwise reading

 *

 * This is a generic implementation for &vm_operations_struct.access for an

 * iomem mapping. This callback is used by access_process_vm() when the @vma is

 * not page based.

/*

 * Access another process' address space as given in mm.

 ignore errors, just check how much was successfully transferred */

			/*

			 * Check if this is a VM_IO | VM_PFNMAP VMA, which

			 * we can access using slightly different code.

/**

 * access_remote_vm - access another process' address space

 * @mm:		the mm_struct of the target address space

 * @addr:	start address to access

 * @buf:	source or destination buffer

 * @len:	number of bytes to transfer

 * @gup_flags:	flags modifying lookup behaviour

 *

 * The caller must hold a reference on @mm.

 *

 * Return: number of bytes copied from source to destination.

/*

 * Access another process' address space.

 * Source/target buffer must be kernel space,

 * Do not walk the page table directly, use get_user_pages

/*

 * Print the name of a VMA.

	/*

	 * we might be running from an atomic context so we cannot sleep

	/*

	 * Some code (nfs/sunrpc) uses socket ops on kernel memory while

	 * holding the mmap_lock, this is safe because kernel memory doesn't

	 * get paged out, therefore we'll never actually fault, and the

	 * below annotations will generate false positives.

/*

 * Process all subpages of the specified huge page with the specified

 * operation.  The target subpage will be processed last to keep its

 * cache lines hot.

 Process target subpage last to keep its cache lines hot */

 If target subpage in first half of huge page */

 Process subpages at the end of huge page */

 If target subpage in second half of huge page */

 Process subpages at the begin of huge page */

	/*

	 * Process remaining subpages in left-right-left-right pattern

	 * towards the target subpage

 CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2014 Davidlohr Bueso.

/*

 * Hash based on the pmd of addr if configured with MMU, which provides a good

 * hit rate for workloads with spatial locality.  Otherwise, use pages.

/*

 * This task may be accessing a foreign mm via (for example)

 * get_user_pages()->find_vma().  The vmacache is task-local and this

 * task's vmacache pertains to a different mm (ie, its own).  There is

 * nothing we can do here.

 *

 * Also handle the case where a kernel thread has adopted this mm via

 * kthread_use_mm(). That kernel thread's vmacache is not applicable to this mm.

		/*

		 * First attempt will always be invalid, initialize

		 * the new cache for this task here.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corporation, 2021

 *

 * Author: Mike Rapoport <rppt@linux.ibm.com>

/*

 * Define mode and flag masks to allow validation of the system call

 * parameters.

			/*

			 * If a split of large page was required, it

			 * already happened when we marked the page invalid

			 * which guarantees that this call won't fail

 pretend we are a normal file with zero size */

 make sure local flags do not confict with global fcntl.h */

 prevent secretmem mappings from ever getting PROT_EXEC */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * linux/mm/process_vm_access.c

 *

 * Copyright (C) 2010-2011 Christopher Yeoh <cyeoh@au1.ibm.com>, IBM Corp.

/**

 * process_vm_rw_pages - read/write pages from task specified

 * @pages: array of pointers to pages we want to copy

 * @offset: offset in page to start copying from/to

 * @len: number of bytes to copy

 * @iter: where to copy to/from locally

 * @vm_write: 0 means copy from, 1 means copy to

 * Returns 0 on success, error code otherwise

 Do the copy for each page */

 Maximum number of pages kmalloc'd to hold struct page's during copy */

/**

 * process_vm_rw_single_vec - read/write pages from task specified

 * @addr: start memory address of target process

 * @len: size of area to copy to/from

 * @iter: where to copy to/from locally

 * @process_pages: struct pages area that can store at least

 *  nr_pages_to_copy struct page pointers

 * @mm: mm for task

 * @task: task to read/write from

 * @vm_write: 0 means copy from, 1 means copy to

 * Returns 0 on success or on failure error code

 Work out address and page range required */

		/*

		 * Get the pages we're interested in.  We must

		 * access remotely because task/mm might not

		 * current/current->mm

 If vm_write is set, the pages need to be made dirty: */

/* Maximum number of entries for process pages array

/**

 * process_vm_rw_core - core of reading/writing pages from task specified

 * @pid: PID of process to read/write from/to

 * @iter: where to copy to/from locally

 * @rvec: iovec array specifying where to copy to/from in the other process

 * @riovcnt: size of rvec array

 * @flags: currently unused

 * @vm_write: 0 if reading from other process, 1 if writing to other process

 *

 * Returns the number of bytes read/written or error code. May

 *  return less bytes than expected if an error occurs during the copying

 *  process.

	/*

	 * Work out how many pages of struct pages we're going to need

	 * when eventually calling get_user_pages

		/* For reliability don't try to kmalloc more than

 Get process information */

		/*

		 * Explicitly map EACCES to EPERM as EPERM is a more

		 * appropriate error code for process_vw_readv/writev

 copied = space before - space after */

	/* If we have managed to copy any data at all then

	   we return the number of bytes copied. Otherwise

/**

 * process_vm_rw - check iovecs before calling core routine

 * @pid: PID of process to read/write from/to

 * @lvec: iovec array specifying where to copy to/from locally

 * @liovcnt: size of lvec array

 * @rvec: iovec array specifying where to copy to/from in the other process

 * @riovcnt: size of rvec array

 * @flags: currently unused

 * @vm_write: 0 if reading from other process, 1 if writing to other process

 *

 * Returns the number of bytes read/written or error code. May

 *  return less bytes than expected if an error occurs during the copying

 *  process.

 Check iovecs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mm_init.c - Memory initialisation verification and debugging

 *

 * Copyright 2008 IBM Corporation, 2008

 * Author Mel Gorman <mel@csn.ul.ie>

 *

 The zonelists are simply reported, validation is manual. */

 Identify the zone and nodelist */

 Print information about the zonelist */

 Iterate the zonelist */

 Check for bitmask overlaps */

 CONFIG_DEBUG_MEMORY_INIT */

	/*

	 * For policy OVERCOMMIT_NEVER, set batch size to 0.4% of

	 * (total memory/#cpus), and lift it to 25% for other policies

	 * to easy the possible lock contention for percpu_counter

	 * vm_committed_as, while the max limit is INT_MAX

 use lowest priority */

 SPDX-License-Identifier: GPL-2.0

/*

 * DAMON Primitives for The Physical Address Space

 *

 * Author: SeongJae Park <sj@kernel.org>

 CONFIG_TRANSPARENT_HUGEPAGE */

 If accessed, stop walking */

 If the region is in the last checked page, reuse the result */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common Primitives for Data Access Monitoring

 *

 * Author: SeongJae Park <sj@kernel.org>

/*

 * Get an online page for a pfn if it's in the LRU list.  Otherwise, returns

 * NULL.

 *

 * The body of this function is stolen from the 'page_idle_get_page()'.  We

 * steal rather than reuse it because the code is quite simple.

 CONFIG_MMU_NOTIFIER */

 CONFIG_MMU_NOTIFIER */

 CONFIG_TRANSPARENT_HUGEPAGE */

 If frequency is 0, higher age means it's colder */

	/*

	 * Now age_in_log is in [-DAMON_MAX_AGE_IN_LOG, DAMON_MAX_AGE_IN_LOG].

	 * Scale it to be in [0, 100] and set it as age subscore.

	/*

	 * Transform it to fit in [0, DAMOS_MAX_SCORE]

 Return coldness of the region */

 SPDX-License-Identifier: GPL-2.0

/*

 * Data Access Monitor

 *

 * Author: SeongJae Park <sjpark@amazon.de>

 Get a random number in [l, r) */

/*

 * Construct a damon_region struct

 *

 * Returns the pointer to the new struct if success, or NULL otherwise

/*

 * Add a region between two other regions

/*

 * Construct a damon_target struct

 *

 * Returns the pointer to the new struct if success, or NULL otherwise

/**

 * damon_set_targets() - Set monitoring targets.

 * @ctx:	monitoring context

 * @ids:	array of target ids

 * @nr_ids:	number of entries in @ids

 *

 * This function should not be called while the kdamond is running.

 *

 * Return: 0 on success, negative error code otherwise.

 The caller should do cleanup of the ids itself */

/**

 * damon_set_attrs() - Set attributes for the monitoring.

 * @ctx:		monitoring context

 * @sample_int:		time interval between samplings

 * @aggr_int:		time interval between aggregations

 * @primitive_upd_int:	time interval between monitoring primitive updates

 * @min_nr_reg:		minimal number of regions

 * @max_nr_reg:		maximum number of regions

 *

 * This function should not be called while the kdamond is running.

 * Every time interval is in micro-seconds.

 *

 * Return: 0 on success, negative error code otherwise.

/**

 * damon_set_schemes() - Set data access monitoring based operation schemes.

 * @ctx:	monitoring context

 * @schemes:	array of the schemes

 * @nr_schemes:	number of entries in @schemes

 *

 * This function should not be called while the kdamond of the context is

 * running.

 *

 * Return: 0 if success, or negative error code otherwise.

/**

 * damon_nr_running_ctxs() - Return number of currently running contexts.

 Returns the size upper limit for each monitoring region */

/*

 * __damon_start() - Starts monitoring with given context.

 * @ctx:	monitoring context

 *

 * This function should be called while damon_lock is hold.

 *

 * Return: 0 on success, negative error code otherwise.

/**

 * damon_start() - Starts the monitorings for a given group of contexts.

 * @ctxs:	an array of the pointers for contexts to start monitoring

 * @nr_ctxs:	size of @ctxs

 *

 * This function starts a group of monitoring threads for a group of monitoring

 * contexts.  One thread per each context is created and run in parallel.  The

 * caller should handle synchronization between the threads by itself.  If a

 * group of threads that created by other 'damon_start()' call is currently

 * running, this function does nothing but returns -EBUSY.

 *

 * Return: 0 on success, negative error code otherwise.

/*

 * __damon_stop() - Stops monitoring of given context.

 * @ctx:	monitoring context

 *

 * Return: 0 on success, negative error code otherwise.

/**

 * damon_stop() - Stops the monitorings for a given group of contexts.

 * @ctxs:	an array of the pointers for contexts to stop monitoring

 * @nr_ctxs:	size of @ctxs

 *

 * Return: 0 on success, negative error code otherwise.

 nr_running_ctxs is decremented in kdamond_fn */

/*

 * damon_check_reset_time_interval() - Check if a time interval is elapsed.

 * @baseline:	the time to check whether the interval has elapsed since

 * @interval:	the time interval (microseconds)

 *

 * See whether the given time interval has passed since the given baseline

 * time.  If so, it also updates the baseline to current time for next check.

 *

 * Return:	true if the time interval has passed, or false otherwise.

/*

 * Check whether it is time to flush the aggregated information

/*

 * Reset the aggregated monitoring results ('nr_accesses' of each region).

 Check the quota */

 Skip previously charged regions */

 Apply the scheme */

 Shouldn't be called if quota->ms and quota->sz are zero */

 New charge window starts */

 Fill up the score histogram */

 Set the min score limit */

/*

 * Merge two adjacent regions into one region

/*

 * Merge adjacent regions having similar access frequencies

 *

 * t		target affected by this merge operation

 * thres	'->nr_accesses' diff threshold for the merge

 * sz_limit	size upper limit of each region

/*

 * Merge adjacent regions having similar access frequencies

 *

 * threshold	'->nr_accesses' diff threshold for the merge

 * sz_limit	size upper limit of each region

 *

 * This function merges monitoring target regions which are adjacent and their

 * access frequencies are similar.  This is for minimizing the monitoring

 * overhead under the dynamically changeable access pattern.  If a merge was

 * unnecessarily made, later 'kdamond_split_regions()' will revert it.

/*

 * Split a region in two

 *

 * r		the region to be split

 * sz_r		size of the first sub-region that will be made

 Split every region in the given target into 'nr_subs' regions */

			/*

			 * Randomly select size of left sub-region to be at

			 * least 10 percent and at most 90% of original region

 Do not allow blank region */

/*

 * Split every target region into randomly-sized small regions

 *

 * This function splits every target region into random-sized small regions if

 * current total number of the regions is equal or smaller than half of the

 * user-specified maximum number of regions.  This is for maximizing the

 * monitoring accuracy under the dynamically changeable access patterns.  If a

 * split was unnecessarily made, later 'kdamond_merge_regions()' will revert

 * it.

 Maybe the middle of the region has different access frequency */

/*

 * Check whether it is time to check and apply the target monitoring regions

 *

 * Returns true if it is.

/*

 * Check whether current monitoring should be stopped

 *

 * The monitoring is stopped when either the user requested to stop, or all

 * monitoring targets are invalid.

 *

 * Returns true if need to stop current monitoring.

/*

 * Returns zero if the scheme is active.  Else, returns time to wait for next

 * watermark check in micro-seconds.

 higher than high watermark or lower than low watermark */

 inactive and higher than middle watermark */

 Returns negative error code if it's not activated but should return */

/*

 * The monitoring daemon that runs as a kernel thread

 SPDX-License-Identifier: GPL-2.0

/*

 * DAMON Debugfs Interface

 *

 * Author: SeongJae Park <sjpark@amazon.de>

/*

 * Returns non-empty string on success, negative error code otherwise.

 We do not accept continuous write */

/*

 * Converts a string into an array of struct damos pointers

 *

 * Returns an array of struct damos pointers that converted if the conversion

 * success, or NULL otherwise.

 Show pid numbers to debugfs users */

/*

 * Converts a string into an array of unsigned long integers

 *

 * Returns an array of unsigned long integers if the conversion success, or

 * NULL otherwise.

 target id is meaningless here, but we set it just for fun */

 remove targets with previously-set primitive */

 Configure the context for the address space type */

/*

 * Make a context of @name and create a debugfs directory for it.

 *

 * This function should be called while holding damon_dbgfs_lock.

 *

 * Returns 0 on success, negative error code otherwise.

 Trim white space */

/*

 * Remove a context of @name and its debugfs directory.

 *

 * This function should be called while holding damon_dbgfs_lock.

 *

 * Return 0 on success, negative error code otherwise.

 Trim white space */

 Remove white space */

/*

 * Functions for the initialization

 SPDX-License-Identifier: GPL-2.0

/*

 * DAMON-based page reclamation

 *

 * Author: SeongJae Park <sj@kernel.org>

/*

 * Enable or disable DAMON_RECLAIM.

 *

 * You can enable DAMON_RCLAIM by setting the value of this parameter as ``Y``.

 * Setting it as ``N`` disables DAMON_RECLAIM.  Note that DAMON_RECLAIM could

 * do no real monitoring and reclamation due to the watermarks-based activation

 * condition.  Refer to below descriptions for the watermarks parameter for

 * this.

/*

 * Time threshold for cold memory regions identification in microseconds.

 *

 * If a memory region is not accessed for this or longer time, DAMON_RECLAIM

 * identifies the region as cold, and reclaims.  120 seconds by default.

/*

 * Limit of time for trying the reclamation in milliseconds.

 *

 * DAMON_RECLAIM tries to use only up to this time within a time window

 * (quota_reset_interval_ms) for trying reclamation of cold pages.  This can be

 * used for limiting CPU consumption of DAMON_RECLAIM.  If the value is zero,

 * the limit is disabled.

 *

 * 10 ms by default.

/*

 * Limit of size of memory for the reclamation in bytes.

 *

 * DAMON_RECLAIM charges amount of memory which it tried to reclaim within a

 * time window (quota_reset_interval_ms) and makes no more than this limit is

 * tried.  This can be used for limiting consumption of CPU and IO.  If this

 * value is zero, the limit is disabled.

 *

 * 128 MiB by default.

/*

 * The time/size quota charge reset interval in milliseconds.

 *

 * The charge reset interval for the quota of time (quota_ms) and size

 * (quota_sz).  That is, DAMON_RECLAIM does not try reclamation for more than

 * quota_ms milliseconds or quota_sz bytes within quota_reset_interval_ms

 * milliseconds.

 *

 * 1 second by default.

/*

 * The watermarks check time interval in microseconds.

 *

 * Minimal time to wait before checking the watermarks, when DAMON_RECLAIM is

 * enabled but inactive due to its watermarks rule.  5 seconds by default.

/*

 * Free memory rate (per thousand) for the high watermark.

 *

 * If free memory of the system in bytes per thousand bytes is higher than

 * this, DAMON_RECLAIM becomes inactive, so it does nothing but periodically

 * checks the watermarks.  500 (50%) by default.

/*

 * Free memory rate (per thousand) for the middle watermark.

 *

 * If free memory of the system in bytes per thousand bytes is between this and

 * the low watermark, DAMON_RECLAIM becomes active, so starts the monitoring

 * and the reclaiming.  400 (40%) by default.

/*

 * Free memory rate (per thousand) for the low watermark.

 *

 * If free memory of the system in bytes per thousand bytes is lower than this,

 * DAMON_RECLAIM becomes inactive, so it does nothing but periodically checks

 * the watermarks.  In the case, the system falls back to the LRU-based page

 * granularity reclamation logic.  200 (20%) by default.

/*

 * Sampling interval for the monitoring in microseconds.

 *

 * The sampling interval of DAMON for the cold memory monitoring.  Please refer

 * to the DAMON documentation for more detail.  5 ms by default.

/*

 * Aggregation interval for the monitoring in microseconds.

 *

 * The aggregation interval of DAMON for the cold memory monitoring.  Please

 * refer to the DAMON documentation for more detail.  100 ms by default.

/*

 * Minimum number of monitoring regions.

 *

 * The minimal number of monitoring regions of DAMON for the cold memory

 * monitoring.  This can be used to set lower-bound of the monitoring quality.

 * But, setting this too high could result in increased monitoring overhead.

 * Please refer to the DAMON documentation for more detail.  10 by default.

/*

 * Maximum number of monitoring regions.

 *

 * The maximum number of monitoring regions of DAMON for the cold memory

 * monitoring.  This can be used to set upper-bound of the monitoring overhead.

 * However, setting this too low could result in bad monitoring quality.

 * Please refer to the DAMON documentation for more detail.  1000 by default.

/*

 * Start of the target memory region in physical address.

 *

 * The start physical address of memory region that DAMON_RECLAIM will do work

 * against.  By default, biggest System RAM is used as the region.

/*

 * End of the target memory region in physical address.

 *

 * The end physical address of memory region that DAMON_RECLAIM will do work

 * against.  By default, biggest System RAM is used as the region.

/*

 * PID of the DAMON thread

 *

 * If DAMON_RECLAIM is enabled, this becomes the PID of the worker thread.

 * Else, -1.

/*

 * Find biggest 'System RAM' resource and store its start and end address in

 * @start and @end, respectively.  If no System RAM is found, returns false.

		/*

		 * Do not try reclamation for more than quota_ms milliseconds

		 * or quota_sz bytes within quota_reset_interval_ms.

 Within the quota, page out older regions first. */

 Find regions having PAGE_SIZE or larger size */

 and not accessed at all */

 for min_age or more micro-seconds, and */

 page out those, as soon as found */

 under the quota. */

 (De)activate this according to the watermarks. */

 DAMON will free this on its own when finish monitoring */

 Will be freed by 'damon_set_schemes()' below */

 4242 means nothing but fun */

 SPDX-License-Identifier: GPL-2.0

/*

 * DAMON Primitives for Virtual Address Spaces

 *

 * Author: SeongJae Park <sjpark@amazon.de>

/*

 * 't->id' should be the pointer to the relevant 'struct pid' having reference

 * count.  Caller must put the returned task, unless it is NULL.

/*

 * Get the mm_struct of the given target

 *

 * Caller _must_ put the mm_struct after use, unless it is NULL.

 *

 * Returns the mm_struct of the target on success, NULL on failure

/*

 * Functions for the initial monitoring target regions construction

/*

 * Size-evenly split a region into 'nr_pieces' small regions

 *

 * Returns 0 on success, or negative error code otherwise.

 complement last region for possible rounding error */

/*

 * Find three regions separated by two biggest unmapped regions

 *

 * vma		the head vma of the target address space

 * regions	an array of three address ranges that results will be saved

 *

 * This function receives an address space and finds three regions in it which

 * separated by the two biggest unmapped regions in the space.  Please refer to

 * below comments of '__damon_va_init_regions()' function to know why this is

 * necessary.

 *

 * Returns 0 if success, or negative error code otherwise.

 Find two biggest gaps so that first_gap > second_gap > others */

 Sort the two biggest gaps by address */

 Store the result */

/*

 * Get the three regions in the given target (task)

 *

 * Returns 0 on success, negative error code otherwise.

/*

 * Initialize the monitoring target regions for the given target (task)

 *

 * t	the given target

 *

 * Because only a number of small portions of the entire address space

 * is actually mapped to the memory and accessed, monitoring the unmapped

 * regions is wasteful.  That said, because we can deal with small noises,

 * tracking every mapping is not strictly required but could even incur a high

 * overhead if the mapping frequently changes or the number of mappings is

 * high.  The adaptive regions adjustment mechanism will further help to deal

 * with the noise by simply identifying the unmapped areas as a region that

 * has no access.  Moreover, applying the real mappings that would have many

 * unmapped areas inside will make the adaptive mechanism quite complex.  That

 * said, too huge unmapped areas inside the monitoring target should be removed

 * to not take the time for the adaptive mechanism.

 *

 * For the reason, we convert the complex mappings to three distinct regions

 * that cover every mapped area of the address space.  Also the two gaps

 * between the three regions are the two biggest unmapped areas in the given

 * address space.  In detail, this function first identifies the start and the

 * end of the mappings and the two biggest unmapped areas of the address space.

 * Then, it constructs the three regions as below:

 *

 *     [mappings[0]->start, big_two_unmapped_areas[0]->start)

 *     [big_two_unmapped_areas[0]->end, big_two_unmapped_areas[1]->start)

 *     [big_two_unmapped_areas[1]->end, mappings[nr_mappings - 1]->end)

 *

 * As usual memory map of processes is as below, the gap between the heap and

 * the uppermost mmap()-ed region, and the gap between the lowermost mmap()-ed

 * region and the stack will be two biggest unmapped regions.  Because these

 * gaps are exceptionally huge areas in usual address space, excluding these

 * two biggest unmapped regions will be sufficient to make a trade-off.

 *

 *   <heap>

 *   <BIG UNMAPPED REGION 1>

 *   <uppermost mmap()-ed region>

 *   (other mmap()-ed regions and small unmapped regions)

 *   <lowermost mmap()-ed region>

 *   <BIG UNMAPPED REGION 2>

 *   <stack>

 Set the initial three regions of the target */

 Initialize '->regions_list' of every target (task) */

 the user may set the target regions as they want */

/*

 * Functions for the dynamic monitoring target regions update

/*

 * Check whether a region is intersecting an address range

 *

 * Returns true if it is.

/*

 * Update damon regions for the three big regions of the given target

 *

 * t		the given target

 * bregions	the three big regions of the target

 Remove regions which are not in the three big regions now */

 Adjust intersecting regions to fit with the three big regions */

 Get the first and last regions which intersects with br */

 no damon_region intersects with this big region */

/*

 * Update regions for current memory mappings

/*

 * Functions for the access checking of the regions

 CONFIG_TRANSPARENT_HUGEPAGE */

/*

 * Check whether the region was accessed after the last preparation

 *

 * mm	'mm_struct' for the given virtual address space

 * r	the region to be checked

 If the region is in the last checked page, reuse the result */

/*

 * Functions for the target validity check and cleanup

 CONFIG_ADVISE_SYSCALLS */

 SPDX-License-Identifier: GPL-2.0

/*

 * KFENCE reporting.

 *

 * Copyright (C) 2020, Google LLC.

 May be overridden by <asm/kfence.h>. */

 Helper function to either print to a seq_file or to console. */

/*

 * Get the number of stack entries to skip to get out of MM internals. @type is

 * optional, and if set to NULL, assumes an allocation or free stack.

 Depending on error type, find different stack entries. */

			/*

			 * kfence_handle_page_fault() may be called with pt_regs

			 * set to NULL; in that case we'll simply show the full

			 * stack trace.

			/*

			 * In case of tail calls from any of the below

			 * to any of the above.

 Also the *_bulk() variants by only checking prefixes. */

 Timestamp matches printk timestamp format. */

 Skip allocation/free internals stack. */

 stack_trace_seq_print() does not exist; open code our own. */

/*

 * Show bytes at @addr that are different from the expected canary values, up to

 * @max_bytes.

 Do not show contents of object nor read into following guard page. */

 Do not leak kernel memory in non-debug builds. */

 Require non-NULL meta, except if KFENCE_ERROR_INVALID. */

	/*

	 * Because we may generate reports in printk-unfriendly parts of the

	 * kernel, such as scheduler code, the use of printk() could deadlock.

	 * Until such time that all printing code here is safe in all parts of

	 * the kernel, accept the risk, and just get our message out (given the

	 * system might already behave unpredictably due to the memory error).

	 * As such, also disable lockdep to hide warnings, and avoid disabling

	 * lockdep for the rest of the kernel.

 Print report header. */

 Print stack trace and object info. */

 Print report footer. */

 We encountered a memory safety error, taint the kernel! */

 SPDX-License-Identifier: GPL-2.0

/*

 * KFENCE guarded object allocator and fault handling.

 *

 * Copyright (C) 2020, Google LLC.

 Disables KFENCE on the first warning assuming an irrecoverable error. */

 === Data ================================================================= */

 Using 0 to indicate KFENCE is disabled. */

 Cannot (re-)enable KFENCE on-the-fly. */

 Pool usage% threshold when currently covered allocations are skipped. */

 The pool of pages used for guard pages and objects. */

 Export for test modules. */

/*

 * Per-object metadata, with one-to-one mapping of object metadata to

 * backing pages (in __kfence_pool).

 Freelist with available objects. */

 Lock protecting freelist. */

/*

 * The static key to set up a KFENCE allocation; or if static keys are not used

 * to gate allocations, to avoid a load and compare if KFENCE is disabled.

 Gates the allocation, ensuring only one succeeds in a given period. */

/*

 * A Counting Bloom filter of allocation coverage: limits currently covered

 * allocations of the same source filling up the pool.

 *

 * Assuming a range of 15%-85% unique allocations in the pool at any point in

 * time, the below parameters provide a probablity of 0.02-0.33 for false

 * positive hits respectively:

 *

 *	P(alloc_traces) = (1 - e^(-HNUM * (alloc_traces / SIZE)) ^ HNUM

 Stack depth used to determine uniqueness of an allocation. */

/*

 * Randomness for stack hashes, making the same collisions across reboots and

 * different machines less likely.

 Statistics counters for debugfs. */

 === Internals ============================================================ */

/*

 * Adds (or subtracts) count @val for allocation stack trace hash

 * @alloc_stack_hash from Counting Bloom filter.

/*

 * Returns true if the allocation stack trace hash @alloc_stack_hash is

 * currently contained (non-zero count) in Counting Bloom filter.

 The checks do not affect performance; only called from slow-paths. */

	/*

	 * May be an invalid index if called with an address at the edge of

	 * __kfence_pool, in which case we would report an "invalid access"

	 * error.

 The checks do not affect performance; only called from slow-paths. */

 Only call with a pointer into kfence_metadata. */

	/*

	 * This metadata object only ever maps to 1 page; verify that the stored

	 * address is in the expected range.

/*

 * Update the object's metadata state, including updating the alloc/free stacks

 * depending on the state transition.

		/*

		 * Skip over 1 (this) functions; noinline ensures we do not

		 * accidentally skip over the caller by never inlining.

 Same source as printk timestamps. */

	/*

	 * Pairs with READ_ONCE() in

	 *	kfence_shutdown_cache(),

	 *	kfence_handle_page_fault().

 Write canary byte to @addr. */

 Check canary byte at @addr. */

 __always_inline this to ensure we won't do an indirect call to fn. */

	/*

	 * We'll iterate over each canary byte per-side until fn() returns

	 * false. However, we'll still iterate over the canary bytes to the

	 * right of the object even if there was an error in the canary bytes to

	 * the left of the object. Specifically, if check_canary_byte()

	 * generates an error, showing both sides might give more clues as to

	 * what the error is about when displaying which bytes were corrupted.

 Apply to left of object. */

 Apply to right of object. */

 Try to obtain a free object. */

		/*

		 * This is extremely unlikely -- we are reporting on a

		 * use-after-free, which locked meta->lock, and the reporting

		 * code via printk calls kmalloc() which ends up in

		 * kfence_alloc() and tries to grab the same object that we're

		 * reporting on. While it has never been observed, lockdep does

		 * report that there is a possibility of deadlock. Fix it by

		 * using trylock and bailing out gracefully.

 Put the object back on the freelist. */

 Unprotect if we're reusing this page. */

	/*

	 * Note: for allocations made before RNG initialization, will always

	 * return zero. We still benefit from enabling KFENCE as early as

	 * possible, even when the RNG is not yet available, as this will allow

	 * KFENCE to detect bugs due to earlier allocations. The only downside

	 * is that the out-of-bounds accesses detected are deterministic for

	 * such allocations.

 Allocate on the "right" side, re-calculate address. */

 Update remaining metadata. */

 Pairs with READ_ONCE() in kfence_shutdown_cache(). */

 Set required struct page fields. */

 Memory initialization. */

	/*

	 * We check slab_want_init_on_alloc() ourselves, rather than letting

	 * SL*B do the initialization, as otherwise we might overwrite KFENCE's

	 * redzone.

 Random "faults" by protecting the object. */

 Invalid or double-free, bail out. */

 Detect racy use-after-free, or incorrect reallocation of this page by KFENCE. */

 To check canary bytes. */

 Restore page protection if there was an OOB access. */

 Mark the object as freed. */

 Check canary bytes for memory corruption. */

	/*

	 * Clear memory if init-on-free is set. While we protect the page, the

	 * data is still there, and after a use-after-free is detected, we

	 * unprotect the page, so the data is still accessible.

 Protect to detect use-after-frees. */

 Add it to the tail of the freelist for reuse. */

 See kfence_shutdown_cache(). */

	/*

	 * Set up object pages: they must have PG_slab set, to avoid freeing

	 * these as real pages.

	 *

	 * We also want to avoid inserting kfence_free() in the kfree()

	 * fast-path in SLUB, and therefore need to ensure kfree() correctly

	 * enters __slab_free() slow-path.

 Verify we do not have a compound head page. */

	/*

	 * Protect the first 2 pages. The first page is mostly unnecessary, and

	 * merely serves as an extended guard page. However, adding one

	 * additional page in the beginning gives us an even number of pages,

	 * which simplifies the mapping of address to metadata index.

 Initialize metadata. */

 Initialize for validation in metadata_to_pageaddr(). */

 Protect the right redzone. */

	/*

	 * The pool is live and will never be deallocated from this point on.

	 * Remove the pool object from the kmemleak object tree, as it would

	 * otherwise overlap with allocations returned by kfence_alloc(), which

	 * are registered with kmemleak through the slab post-alloc hook.

	/*

	 * Only release unprotected pages, and do not try to go back and change

	 * page attributes due to risk of failing to do so as well. If changing

	 * page attributes for some pages fails, it is very likely that it also

	 * fails for the first page, and therefore expect addr==__kfence_pool in

	 * most failure cases.

 === DebugFS Interface ==================================================== */

/*

 * debugfs seq_file operations for /sys/kernel/debug/kfence/objects.

 * start_object() and next_object() return the object index + 1, because NULL is used

 * to stop iteration.

 === Allocation Gate Timer ================================================ */

 Wait queue to wake up allocation-gate timer task. */

/*

 * Set up delayed work, which will enable and disable the static key. We need to

 * use a work queue (rather than a simple timer), since enabling and disabling a

 * static key cannot be done from an interrupt.

 *

 * Note: Toggling a static branch currently causes IPIs, and here we'll end up

 * with a total of 2 IPIs to all CPUs. If this ends up a problem in future (with

 * more aggressive sampling intervals), we could get away with a variant that

 * avoids IPIs, at the cost of not immediately capturing allocations if the

 * instructions remain cached.

 Enable static key, and await allocation to happen. */

		/*

		 * During low activity with no allocations we might wait a

		 * while; let's avoid the hung task warning.

 Disable static key and reset timer. */

 === Public interface ===================================================== */

 Setting kfence_sample_interval to 0 on boot disables KFENCE. */

		/*

		 * If we observe some inconsistent cache and state pair where we

		 * should have returned false here, cache destruction is racing

		 * with either kmem_cache_alloc() or kmem_cache_free(). Taking

		 * the lock will not help, as different critical section

		 * serialization will have the same outcome.

			/*

			 * This cache still has allocations, and we should not

			 * release them back into the freelist so they can still

			 * safely be used and retain the kernel's default

			 * behaviour of keeping the allocations alive (leak the

			 * cache); however, they effectively become "zombie

			 * allocations" as the KFENCE objects are the only ones

			 * still in use and the owning cache is being destroyed.

			 *

			 * We mark them freed, so that any subsequent use shows

			 * more useful error messages that will include stack

			 * traces of the user of the object, the original

			 * allocation, and caller to shutdown_cache().

zombie=*/true);

 See above. */

	/*

	 * Perform size check before switching kfence_allocation_gate, so that

	 * we don't disable KFENCE without making an allocation.

	/*

	 * Skip allocations from non-default zones, including DMA. We cannot

	 * guarantee that pages in the KFENCE pool will have the requested

	 * properties (e.g. reside in DMAable memory).

	/*

	 * waitqueue_active() is fully ordered after the update of

	 * kfence_allocation_gate per atomic_inc_return().

		/*

		 * Calling wake_up() here may deadlock when allocations happen

		 * from within timer code. Use an irq_work to defer it.

	/*

	 * Do expensive check for coverage of allocation in slow-path after

	 * allocation_gate has already become non-zero, even though it might

	 * mean not making any allocation within a given sample interval.

	 *

	 * This ensures reasonable allocation coverage when the pool is almost

	 * full, including avoiding long-lived allocations of the same source

	 * filling up the pool (e.g. pagecache allocations).

	/*

	 * Read locklessly -- if there is a race with __kfence_alloc(), this is

	 * either a use-after-free or invalid access.

	/*

	 * Read locklessly -- if there is a race with __kfence_alloc(), this is

	 * either a use-after-free or invalid access.

	/*

	 * If the objects of the cache are SLAB_TYPESAFE_BY_RCU, defer freeing

	 * the object, as the object page may be recycled for other-typed

	 * objects once it has been freed. meta->cache may be NULL if the cache

	 * was destroyed.

 If disabled at runtime ... */

 ... unprotect and proceed. */

 This is a redzone, report a buffer overflow. */

 Data race ok; distance calculation approximate. */

 Data race ok; distance calculation approximate. */

		/*

		 * If the object was freed before we took the look we can still

		 * report this as an OOB -- the report will simply show the

		 * stacktrace of the free as well.

		/*

		 * We may race with __kfence_alloc(), and it is possible that a

		 * freed object may be reallocated. We simply report this as a

		 * use-after-free, with the stack trace showing the place where

		 * the object was re-allocated.

 This may be a UAF or OOB access, but we can't be sure. */

 Unprotect and let access proceed. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Test cases for KFENCE memory safety error detector. Since the interface with

 * which KFENCE's reports are obtained is via the console, this is the output we

 * should verify. For each test case checks the presence (or absence) of

 * generated reports. Relies on 'console' tracepoint to capture reports as they

 * appear in the kernel log.

 *

 * Copyright (C) 2020, Google LLC.

 * Author: Alexander Potapenko <glider@google.com>

 *         Marco Elver <elver@google.com>

 May be overridden by <asm/kfence.h>. */

 Report as observed from console. */

 Probe for console output: obtains observed lines of interest. */

		/*

		 * KFENCE report and related to the test.

		 *

		 * The provided @buf is not NUL-terminated; copy no more than

		 * @len bytes and let strscpy() add the missing NUL-terminator.

 Publish new nlines. */

 Check if a report related to the test exists. */

 Information we expect in a report. */

 The type or error. */

 Function pointer to expected function where access occurred. */

 Address at which the bad access occurred. */

 Is access a write. */

 Check observed report matches information in @r. */

 Doubled-checked locking. */

 Generate expected report contents. */

 Title */

 The exact offset won't match, remove it; also strip module name. */

 Access information */

 A new report is being captured. */

 Finally match expected output to what we actually observed. */

 ===== Test cases ===== */

 Cache used by tests; if NULL, allocate from kmalloc instead. */

	/*

	 * Use SLAB_NOLEAKTRACE to prevent merging with existing caches. Any

	 * other flag in SLAB_NEVER_MERGE also works. Use SLAB_ACCOUNT to

	 * allocate via memcg, if enabled.

 Must always inline to match stack trace against caller. */

/*

 * If this should be a KFENCE allocation, and on which side the allocation and

 * the closest guard page should be.

 KFENCE, any side. */

 KFENCE, left side of page. */

 KFENCE, right side of page. */

 No KFENCE allocation. */

/*

 * Try to get a guarded allocation from KFENCE. Uses either kmalloc() or the

 * current test_cache if set up.

	/*

	 * 100x the sample interval should be more than enough to ensure we get

	 * a KFENCE allocation eventually.

	/*

	 * Especially for non-preemption kernels, ensure the allocation-gate

	 * timer can catch up: after @resched_after, every failed allocation

	 * attempt yields, to ensure the allocation-gate timer is scheduled.

			/*

			 * Verify that various helpers return the right values

			 * even for KFENCE objects; these are required so that

			 * memcg accounting works correctly.

 Unreachable. */

	/*

	 * If we don't have our own cache, adjust based on alignment, so that we

	 * actually access guard pages on either side.

 Test both sides. */

 Double-free. */

 Free on invalid address. */

 Invalid address free. */

 No error. */

 Test both sides. */

/*

 * KFENCE is unable to detect an OOB if the allocation's alignment requirements

 * leave a gap between the object and the guard page. Specifically, an

 * allocation of e.g. 73 bytes is aligned on 8 and 128 bytes for SLUB or SLAB

 * respectively. Therefore it is impossible for the allocated object to

 * contiguously line up with the right guard page.

 *

 * However, we test that an access to memory beyond the gap results in KFENCE

 * detecting an OOB access.

	/*

	 * The object is offset to the right, so there won't be an OOB to the

	 * left of it.

	/*

	 * @buf must be aligned on @align, therefore buf + size belongs to the

	 * same page -> no OOB.

 Overflowing by @align bytes will result in an OOB. */

	/*

	 * The object is offset to the right, so we won't get a page

	 * fault immediately after it.

 Test cache shrinking and destroying with KFENCE. */

 Every object has at least 8 bytes. */

 Ensure that SL*B does not modify KFENCE objects on bulk free. */

 Test init-on-free works. */

 Assume it hasn't been disabled on command line. */

		/*

		 * This may fail if the page was recycled by KFENCE and then

		 * written to again -- this however, is near impossible with a

		 * default config.

 Only check first access to not fail test if page is ever re-protected. */

 Ensure that constructors work properly. */

 Test that memory is zeroed if requested. */

 PAGE_SIZE so we can use ALLOCATE_ANY. */

 Skip if we think it'd take too long. */

 Try to get same address again -- this can take a while. */

 Test SLAB_TYPESAFE_BY_RCU works. */

 Want memcache. */

	/*

	 * Up to this point, memory should not have been freed yet, and

	 * therefore there should be no KFENCE report from the above access.

 Above access to @expect.addr should not have generated a report! */

 Only after rcu_barrier() is the memory guaranteed to be freed. */

 Expect use-after-free. */

 Test krealloc(). */

 Precise size match after KFENCE alloc. */

 Check that we successfully change the size. */

 Grow. */

 Note: Might no longer be a KFENCE alloc. */

 Fill to extra bytes. */

 Shrink. */

 Free. */

 No reports yet! */

 Ensure krealloc() actually freed earlier KFENCE object. */

 Test that some objects from a bulk allocation belong to KFENCE pool. */

 Want memcache. */

	/*

	 * 100x the sample interval should be more than enough to ensure we get

	 * a KFENCE allocation eventually.

		/*

		 * kmem_cache_alloc_bulk() disables interrupts, and calling it

		 * in a tight loop may not give KFENCE a chance to switch the

		 * static branch. Call cond_resched() to let KFENCE chime in.

/*

 * KUnit does not provide a way to provide arguments to tests, and we encode

 * additional info in the name. Set up 2 tests per test case, one using the

 * default allocator, and another using a custom memcache (suffix '-memcache').

 ===== End test cases ===== */

 Any test with 'memcache' in its name will want a memcache. */

/*

 * We only want to do tracepoints setup and teardown once, therefore we have to

 * customize the init and exit functions and cannot rely on kunit_test_suite().

	/*

	 * Because we want to be able to build the test as a module, we need to

	 * iterate through all known tracepoints, since the static registration

	 * won't work here.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains generic KASAN specific error reporting code.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

	/*

	 * If shadow byte value is in [0, KASAN_GRANULE_SIZE) we can look

	 * at the next shadow byte to determine the type of the bad access.

		/*

		 * In theory it's still possible to see these shadow values

		 * due to a data race in the kernel code.

	/*

	 * If access_size is a negative number, then it has reason to be

	 * defined as out-of-bounds bug type.

	 *

	 * Casting negative numbers to size_t would indeed turn up as

	 * a large size_t and its value will be larger than ULONG_MAX/2,

	 * so that this can qualify as out-of-bounds.

 Copy token (+ 1 byte for '\0'). */

 Advance frame_descr past separator. */

	/*

	 * We need to parse the following string:

	 *    "n alloc_1 alloc_2 ... alloc_n"

	 * where alloc_i looks like

	 *    "offset size len name"

	 * or "offset size len name:line".

 access offset */

 access size */

 name length (unused) */

 object name */

 Strip line number; without filename it's not very helpful. */

 Finally, print object information. */

	/*

	 * NOTE: We currently only support printing frame information for

	 * accesses to the task's own stack.

	/*

	 * get_address_stack_frame_info only returns true if the given addr is

	 * on the current task's stack.

 CONFIG_KASAN_STACK */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains common KASAN error reporting code.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

 kasan.fault=report/panic */

	/*

	 * Make sure we don't end up in loop.

		/*

		 * This thread may hit another WARN() in the panic path.

		 * Resetting this prevents additional WARN() from panicking the

		 * system on this thread.  Other threads are blocked by the

		 * panic_mutex in panic().

	/*

	 * Memory state around the buggy address:

	 *  ff00ff00ff00ff00: 00 00 00 05 fe fe fe fe fe fe fe fe fe fe fe fe

	 *  ...

	 *

	 * The length of ">ff00ff00ff00ff00: " is

	 *    3 + (BITS_PER_LONG / 8) * 2 chars.

	 * The length of each granule metadata is 2 bytes

	 *    plus 1 byte for space.

		/*

		 * We should not pass a shadow pointer to generic

		 * function, because generic functions may try to

		 * access kasan mapping for the passed address.

 IS_ENABLED(CONFIG_KUNIT) */

 IS_ENABLED(CONFIG_KUNIT) */

 IS_ENABLED(CONFIG_KUNIT) */

 CONFIG_KASAN_HW_TAGS */

 IS_ENABLED(CONFIG_KUNIT) */

/*

 * With CONFIG_KASAN_INLINE, accesses to bogus pointers (outside the high

 * canonical half of the address space) cause out-of-bounds shadow memory reads

 * before the actual access. For addresses in the low canonical half of the

 * address space, as well as most non-canonical addresses, that out-of-bounds

 * shadow memory access lands in the non-canonical part of the address space.

 * Help the user figure out what the original bogus pointer was.

	/*

	 * For faults near the shadow address for NULL, we can be fairly certain

	 * that this is a KASAN shadow memory access.

	 * For faults that correspond to shadow for low canonical addresses, we

	 * can still be pretty sure - that shadow region is a fairly narrow

	 * chunk of the non-canonical address space.

	 * But faults that look like shadow for non-canonical addresses are a

	 * really large chunk of the address space. In that case, we still

	 * print the decoded address, but make it clear that this is not

	 * necessarily what's actually going on.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains KASAN runtime code that manages shadow memory for

 * generic and software tag-based KASAN modes.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

	/*

	 * Perform shadow offset calculation based on untagged address, as

	 * some of the callers (e.g. kasan_poison_object_data) pass tagged

	 * addresses to this function.

 Skip KFENCE memory if called explicitly outside of sl*b. */

	/*

	 * Perform shadow offset calculation based on untagged address, as

	 * some of the callers (e.g. kasan_unpoison_object_data) pass tagged

	 * addresses to this function.

	/*

	 * Skip KFENCE memory if called explicitly outside of sl*b. Also note

	 * that calls to ksize(), where size is not a multiple of machine-word

	 * size, would otherwise poison the invalid portion of the word.

 Unpoison all granules that cover the object. */

 Partially poison the last granule for the generic mode. */

	/*

	 * We can't use pud_large() or pud_huge(), the first one is

	 * arch-specific, the last one depends on HUGETLB_PAGE.  So let's abuse

	 * pud_bad(), if pud is bad then it's bad because it's huge.

		/*

		 * If shadow is mapped already than it must have been mapped

		 * during the boot. This could happen if we onlining previously

		 * offlined memory.

		/*

		 * shadow_start was either mapped during boot by kasan_init()

		 * or during memory online by __vmalloc_node_range().

		 * In the latter case we can use vfree() to free shadow.

		 * Non-NULL result of the find_vm_area() will tell us if

		 * that was the second case.

		 *

		 * Currently it's not possible to free shadow mapped

		 * during boot by kasan_init(). It's because the code

		 * to do that hasn't been written yet. So we'll just

		 * leak the memory.

	/*

	 * We need to be careful about inter-cpu effects here. Consider:

	 *

	 *   CPU#0				  CPU#1

	 * WRITE_ONCE(p, vmalloc(100));		while (x = READ_ONCE(p)) ;

	 *					p[99] = 1;

	 *

	 * With compiler instrumentation, that ends up looking like this:

	 *

	 *   CPU#0				  CPU#1

	 * // vmalloc() allocates memory

	 * // let a = area->addr

	 * // we reach kasan_populate_vmalloc

	 * // and call kasan_unpoison:

	 * STORE shadow(a), unpoison_val

	 * ...

	 * STORE shadow(a+99), unpoison_val	x = LOAD p

	 * // rest of vmalloc process		<data dependency>

	 * STORE p, a				LOAD shadow(x+99)

	 *

	 * If there is no barrier between the end of unpoisoning the shadow

	 * and the store of the result to p, the stores could be committed

	 * in a different order by CPU#0, and CPU#1 could erroneously observe

	 * poison in the shadow.

	 *

	 * We need some sort of barrier between the stores.

	 *

	 * In the vmalloc() case, this is provided by a smp_wmb() in

	 * clear_vm_uninitialized_flag(). In the per-cpu allocator and in

	 * get_vm_area() and friends, the caller gets shadow allocated but

	 * doesn't have any pages mapped into the virtual address space that

	 * has been reserved. Mapping those pages in will involve taking and

	 * releasing a page-table lock, which will provide the barrier.

/*

 * Poison the shadow for a vmalloc region. Called as part of the

 * freeing process at the time the region is freed.

/*

 * Release the backing for the vmalloc region [start, end), which

 * lies within the free region [free_region_start, free_region_end).

 *

 * This can be run lazily, long after the region was freed. It runs

 * under vmap_area_lock, so it's not safe to interact with the vmalloc/vmap

 * infrastructure.

 *

 * How does this work?

 * -------------------

 *

 * We have a region that is page aligned, labeled as A.

 * That might not map onto the shadow in a way that is page-aligned:

 *

 *                    start                     end

 *                    v                         v

 * |????????|????????|AAAAAAAA|AA....AA|AAAAAAAA|????????| < vmalloc

 *  -------- -------- --------          -------- --------

 *      |        |       |                 |        |

 *      |        |       |         /-------/        |

 *      \-------\|/------/         |/---------------/

 *              |||                ||

 *             |??AAAAAA|AAAAAAAA|AA??????|                < shadow

 *                 (1)      (2)      (3)

 *

 * First we align the start upwards and the end downwards, so that the

 * shadow of the region aligns with shadow page boundaries. In the

 * example, this gives us the shadow page (2). This is the shadow entirely

 * covered by this allocation.

 *

 * Then we have the tricky bits. We want to know if we can free the

 * partially covered shadow pages - (1) and (3) in the example. For this,

 * we are given the start and end of the free region that contains this

 * allocation. Extending our previous example, we could have:

 *

 *  free_region_start                                    free_region_end

 *  |                 start                     end      |

 *  v                 v                         v        v

 * |FFFFFFFF|FFFFFFFF|AAAAAAAA|AA....AA|AAAAAAAA|FFFFFFFF| < vmalloc

 *  -------- -------- --------          -------- --------

 *      |        |       |                 |        |

 *      |        |       |         /-------/        |

 *      \-------\|/------/         |/---------------/

 *              |||                ||

 *             |FFAAAAAA|AAAAAAAA|AAF?????|                < shadow

 *                 (1)      (2)      (3)

 *

 * Once again, we align the start of the free region up, and the end of

 * the free region down so that the shadow is page aligned. So we can free

 * page (1) - we know no allocation currently uses anything in that page,

 * because all of it is in the vmalloc free region. But we cannot free

 * page (3), because we can't be sure that the rest of it is unused.

 *

 * We only consider pages that contain part of the original region for

 * freeing: we don't try to free other pages from the free region or we'd

 * end up trying to free huge chunks of virtual address space.

 *

 * Concurrency

 * -----------

 *

 * How do we know that we're not freeing a page that is simultaneously

 * being used for a fresh allocation in kasan_populate_vmalloc(_pte)?

 *

 * We _can_ have kasan_release_vmalloc and kasan_populate_vmalloc running

 * at the same time. While we run under free_vmap_area_lock, the population

 * code does not.

 *

 * free_vmap_area_lock instead operates to ensure that the larger range

 * [free_region_start, free_region_end) is safe: because __alloc_vmap_area and

 * the per-cpu region-finding algorithm both run under free_vmap_area_lock,

 * no space identified as free will become used while we are running. This

 * means that so long as we are careful with alignment and only free shadow

 * pages entirely covered by the free region, we will not run in to any

 * trouble - any simultaneous allocations will be for disjoint regions.

 CONFIG_KASAN_VMALLOC */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains core generic KASAN code.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

/*

 * All functions below always inlined so compiler could

 * perform better optimizations in each of __asan_loadX/__assn_storeX

 * depending on memory access size X.

	/*

	 * Access crosses 8(shadow size)-byte boundary. Such access maps

	 * into 2 shadow bytes, so we need to check them both.

 Unaligned 16-bytes access maps into 3 shadow bytes. */

 to shut up compiler complaints */

 Emitted by compiler to poison alloca()ed objects. */

 Emitted by compiler to unpoison alloca()ed areas when the stack unwinds. */

 Emitted by the compiler to [un]poison local variables. */

 The object was freed and has free track set. */

 Free meta must be present with KASAN_KMALLOC_FREETRACK. */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains software tag-based KASAN specific error reporting code.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains core hardware tag-based KASAN code.

 *

 * Copyright (c) 2020 Google, Inc.

 * Author: Andrey Konovalov <andreyknvl@google.com>

 Whether KASAN is enabled at all. */

 Whether the selected mode is synchronous/asynchronous/asymmetric.*/

 Whether to collect alloc/free stack traces. */

 kasan=off/on */

 kasan.mode=sync/async/asymm */

 kasan.stacktrace=off/on */

 kasan_init_hw_tags_cpu() is called for each CPU. */

	/*

	 * There's no need to check that the hardware is MTE-capable here,

	 * as this function is only called for MTE-capable hardware.

 If KASAN is disabled via command line, don't initialize it. */

	/*

	 * Enable async or asymm modes only when explicitly requested

	 * through the command line.

 kasan_init_hw_tags() is called once on boot CPU. */

 If hardware doesn't support MTE, don't initialize KASAN. */

 If KASAN is disabled via command line, don't initialize it. */

 Enable KASAN. */

		/*

		 * Default to sync mode.

 Sync mode enabled. */

 Async mode enabled. */

 Asymm mode enabled. */

 Default to enabling stack trace collection. */

 Do nothing, kasan_flag_stacktrace keeps its default value. */

	/*

	 * This condition should match the one in post_alloc_hook() in

	 * page_alloc.c.

	/*

	 * This condition should match the one in free_pages_prepare() in

	 * page_alloc.c.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains KASAN shadow initialization code.

 *

 * Copyright (c) 2015 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

/*

 * This page serves two purposes:

 *   - It used as early shadow memory. The entire shadow region populated

 *     with this page, before we will be able to setup normal shadow memory.

 *   - Latter it reused it as zero shadow to cover large ranges of memory

 *     that allowed to access, but not handled by kasan (vmalloc/vmemmap ...).

/**

 * kasan_populate_early_shadow - populate shadow memory region with

 *                               kasan_early_shadow_page

 * @shadow_start: start of the memory range to populate

 * @shadow_end: end of the memory range to populate

			/*

			 * kasan_early_shadow_pud should be populated with pmds

			 * at this moment.

			 * [pud,pmd]_populate*() below needed only for

			 * 3,2 - level page tables where we don't have

			 * puds,pmds, so pgd_populate(), pud_populate()

			 * is noops.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains common tag-based KASAN code.

 *

 * Copyright (c) 2018 Google, Inc.

 * Copyright (c) 2020 Google, Inc.

 SPDX-License-Identifier: GPL-2.0

/*

 * KASAN quarantine.

 *

 * Author: Alexander Potapenko <glider@google.com>

 * Copyright (C) 2016 Google, Inc.

 *

 * Based on code by Dmitry Chernenkov.

 Data structure and operations for quarantine queues. */

/*

 * Each queue is a single-linked list, which also stores the total size of

 * objects inside of it.

/*

 * The object quarantine consists of per-cpu queues and a global queue,

 * guarded by quarantine_lock.

 Round-robin FIFO array of batches. */

 Total size of all objects in global_quarantine across all batches. */

 Maximum size of the global queue. */

/*

 * Target size of a batch in global_quarantine.

 * Usually equal to QUARANTINE_PERCPU_SIZE unless we have too much RAM.

/*

 * The fraction of physical memory the quarantine is allowed to occupy.

 * Quarantine doesn't support memory shrinker with SLAB allocator, so we keep

 * the ratio low to avoid OOM.

	/*

	 * As the object now gets freed from the quarantine, assume that its

	 * free track is no longer valid.

	/*

	 * If there's no metadata for this object, don't put it into

	 * quarantine.

	/*

	 * Note: irq must be disabled until after we move the batch to the

	 * global quarantine. Otherwise kasan_quarantine_remove_cache() can

	 * miss some objects belonging to the cache if they are in our local

	 * temp list. kasan_quarantine_remove_cache() executes on_each_cpu()

	 * at the beginning which ensures that it either sees the objects in

	 * per-cpu lists or in the global quarantine.

	/*

	 * srcu critical section ensures that kasan_quarantine_remove_cache()

	 * will not miss objects belonging to the cache while they are in our

	 * local to_free list. srcu is chosen because (1) it gives us private

	 * grace period domain that does not interfere with anything else,

	 * and (2) it allows synchronize_srcu() to return without waiting

	 * if there are no pending read critical sections (which is the

	 * expected case).

	/*

	 * Update quarantine size in case of hotplug. Allocate a fraction of

	 * the installed memory to quarantine minus per-cpu queue limits.

 Aim at consuming at most 1/2 of slots in quarantine. */

 Free all quarantined objects belonging to cache. */

	/*

	 * Must be careful to not miss any objects that are being moved from

	 * per-cpu list to the global quarantine in kasan_quarantine_put(),

	 * nor objects being freed in kasan_quarantine_reduce(). on_each_cpu()

	 * achieves the first goal, while synchronize_srcu() achieves the

	 * second.

 Scanning whole quarantine can take a while. */

	/* Ensure the ordering between the writing to q->offline and

	 * qlist_free_all. Otherwise, cpu_quarantine may be corrupted

	 * by interrupt.

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains core software tag-based KASAN code.

 *

 * Copyright (c) 2018 Google, Inc.

 * Author: Andrey Konovalov <andreyknvl@google.com>

/*

 * If a preemption happens between this_cpu_read and this_cpu_write, the only

 * side effect is that we'll give a few allocated in different contexts objects

 * the same tag. Since tag-based KASAN is meant to be used a probabilistic

 * bug-detection debug feature, this doesn't have significant negative impact.

 *

 * Ideally the tags use strong randomness to prevent any attempts to predict

 * them during explicit exploit attempts. But strong randomness is expensive,

 * and we did an intentional trade-off to use a PRNG. This non-atomic RMW

 * sequence has in fact positive effect, since interrupts that randomly skew

 * PRNG at unpredictable points do only good.

	/*

	 * Ignore accesses for pointers tagged with 0xff (native kernel

	 * pointer tag) to suppress false positives caused by kmap.

	 *

	 * Some kernel code was written to account for archs that don't keep

	 * high memory mapped all the time, but rather map and unmap particular

	 * pages when needed. Instead of storing a pointer to the kernel memory,

	 * this code saves the address of the page structure and offset within

	 * that page for later use. Those pages are then mapped and unmapped

	 * with kmap/kunmap when necessary and virt_to_page is used to get the

	 * virtual address of the page. For arm64 (that keeps the high memory

	 * mapped all the time), kmap is turned into a page_address call.



	 * The issue is that with use of the page_address + virt_to_page

	 * sequence the top byte value of the original pointer gets lost (gets

	 * set to KASAN_TAG_KERNEL (0xFF)).

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains hardware tag-based KASAN specific error reporting code.

 *

 * Copyright (c) 2020 Google, Inc.

 * Author: Andrey Konovalov <andreyknvl@google.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains common KASAN code.

 *

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

 *

 * Some code borrowed from https://github.com/xairy/kasan-prototype by

 *        Andrey Konovalov <andreyknvl@gmail.com>

 CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */

 Unpoison the entire stack for a task. */

 Unpoison the stack for the current task beyond a watermark sp value. */

	/*

	 * Calculate the task stack base address.  Avoid using 'current'

	 * because this function is called by early resume code which hasn't

	 * yet set up the percpu register (%gs).

 CONFIG_KASAN_STACK */

/*

 * Only allow cache merging when stack collection is disabled and no metadata

 * is present.

/*

 * Adaptive redzone policy taken from the userspace AddressSanitizer runtime.

 * For larger allocations larger redzones are used.

	/*

	 * SLAB_KASAN is used to mark caches as ones that are sanitized by

	 * KASAN. Currently this flag is used in two places:

	 * 1. In slab_ksize() when calculating the size of the accessible

	 *    memory within the object.

	 * 2. In slab_common.c to prevent merging of sanitized caches.

 Add alloc meta into redzone. */

	/*

	 * If alloc meta doesn't fit, don't add it.

	 * This can only happen with SLAB, as it has KMALLOC_MAX_SIZE equal

	 * to KMALLOC_MAX_CACHE_SIZE and doesn't fall back to page_alloc for

	 * larger sizes.

 Continue, since free meta might still fit. */

 Only the generic mode uses free meta or flexible redzones. */

	/*

	 * Add free meta into redzone when it's not possible to store

	 * it in the object. This is the case when:

	 * 1. Object is SLAB_TYPESAFE_BY_RCU, which means that it can

	 *    be touched after it was freed, or

	 * 2. Object has a constructor, which means it's expected to

	 *    retain its content until the next allocation, or

	 * 3. Object is too small.

	 * Otherwise cache->kasan_info.free_meta_offset = 0 is implied.

 If free meta doesn't fit, don't add it. */

 Calculate size with optimal redzone. */

 Limit it with KMALLOC_MAX_SIZE (relevant for SLAB only). */

 Use optimal size if the size with added metas is not large enough. */

/*

 * This function assigns a tag to an object considering the following:

 * 1. A cache might have a constructor, which might save a pointer to a slab

 *    object somewhere (e.g. in the object itself). We preassign a tag for

 *    each object in caches with constructors during slab creation and reuse

 *    the same tag each time a particular object is allocated.

 * 2. A cache might be SLAB_TYPESAFE_BY_RCU, which means objects can be

 *    accessed after being freed. We preassign tags for objects in these

 *    caches as well.

 * 3. For SLAB allocator we can't preassign tags randomly since the freelist

 *    is stored as an array of indexes instead of a linked list. Assign tags

 *    based on objects indexes, so that objects that are next to each other

 *    get different tags.

	/*

	 * If the cache neither has a constructor nor has SLAB_TYPESAFE_BY_RCU

	 * set, assign a tag when the object is being allocated (init == false).

 For caches that either have a constructor or SLAB_TYPESAFE_BY_RCU: */

 For SLAB assign tags based on the object index in the freelist. */

	/*

	 * For SLUB assign a random tag during slab creation, otherwise reuse

	 * the already assigned tag.

 Tag is ignored in set_tag() without CONFIG_KASAN_SW/HW_TAGS */

 RCU slabs could be legally used after free within the RCU period */

	/*

	 * The object will be poisoned by kasan_free_pages() or

	 * kasan_slab_free_mempool().

	/*

	 * Even though this function is only called for kmem_cache_alloc and

	 * kmalloc backed mempool allocations, those allocations can still be

	 * !PageSlab() when the size provided to kmalloc is larger than

	 * KMALLOC_MAX_SIZE, and kmalloc falls back onto page_alloc.

 Don't save alloc info for kmalloc caches in kasan_slab_alloc(). */

	/*

	 * Generate and assign random tag for tag-based modes.

	 * Tag is ignored in set_tag() for the generic mode.

	/*

	 * Unpoison the whole object.

	 * For kmalloc() allocations, kasan_kmalloc() will do precise poisoning.

 Save alloc info (if possible) for non-kmalloc() allocations. */

	/*

	 * The object has already been unpoisoned by kasan_slab_alloc() for

	 * kmalloc() or by kasan_krealloc() for krealloc().

	/*

	 * The redzone has byte-level precision for the generic mode.

	 * Partially poison the last object granule to cover the unaligned

	 * part of the redzone.

 Poison the aligned part of the redzone. */

	/*

	 * Save alloc info (if possible) for kmalloc() allocations.

	 * This also rewrites the alloc info when called from kasan_krealloc().

 Keep the tag that was set by kasan_slab_alloc(). */

	/*

	 * The object has already been unpoisoned by kasan_alloc_pages() for

	 * alloc_pages() or by kasan_krealloc() for krealloc().

	/*

	 * The redzone has byte-level precision for the generic mode.

	 * Partially poison the last object granule to cover the unaligned

	 * part of the redzone.

 Poison the aligned part of the redzone. */

	/*

	 * Unpoison the object's data.

	 * Part of it might already have been unpoisoned, but it's unknown

	 * how big that part is.

 Piggy-back on kmalloc() instrumentation to poison the redzone. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014 Samsung Electronics Co., Ltd.

 * Copyright (c) 2020 Google, Inc.

	/*

	 * If access_size is a negative number, then it has reason to be

	 * defined as out-of-bounds bug type.

	 *

	 * Casting negative numbers to size_t would indeed turn up as

	 * a large size_t and its value will be larger than ULONG_MAX/2,

	 * so that this can qualify as out-of-bounds.

 SPDX-License-Identifier: GPL-2.0

/*

 * This code fills the used part of the kernel stack with a poison value

 * before returning to userspace. It's part of the STACKLEAK feature

 * ported from grsecurity/PaX.

 *

 * Author: Alexander Popov <alex.popov@linux.com>

 *

 * STACKLEAK reduces the information which kernel stack leak bugs can

 * reveal and blocks some uninitialized stack variable attacks.

 CONFIG_STACKLEAK_RUNTIME_DISABLE */

 It would be nice not to have 'kstack_ptr' and 'boundary' on stack */

 Check that 'lowest_stack' value is sane */

 Search for the poison value in the kernel stack */

	/*

	 * One 'long int' at the bottom of the thread stack is reserved and

	 * should not be poisoned (see CONFIG_SCHED_STACK_END_CHECK=y).

	/*

	 * Now write the poison value to the kernel stack. Start from

	 * 'kstack_ptr' and move up till the new 'boundary'. We assume that

	 * the stack pointer doesn't change when we write poison.

 Reset the 'lowest_stack' value for the next syscall */

	/*

	 * Having CONFIG_STACKLEAK_TRACK_MIN_SIZE larger than

	 * STACKLEAK_SEARCH_DEPTH makes the poison search in

	 * stackleak_erase() unreliable. Let's prevent that.

 'lowest_stack' should be aligned on the register width boundary */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * kernel/configs.c

 * Echo the kernel .config file used to build the kernel

 *

 * Copyright (C) 2002 Khalid Aziz <khalid_aziz@hp.com>

 * Copyright (C) 2002 Randy Dunlap <rdunlap@xenotime.net>

 * Copyright (C) 2002 Al Stone <ahs3@fc.hp.com>

 * Copyright (C) 2002 Hewlett-Packard Company

/*

 * "IKCFG_ST" and "IKCFG_ED" are used to extract the config data from

 * a binary kernel image or a module. See scripts/extract-ikconfig.

 create the current config file */

 CONFIG_IKCONFIG_PROC */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/ksysfs.c - sysfs attributes in /sys/kernel, which

 * 		     are not related to any other subsystem

 *

 * Copyright (C) 2004 Kay Sievers <kay.sievers@vrfy.org>

 rcu_expedited and rcu_normal */

 current uevent sequence number */

 uevent helper program, used during early boot */

	/*

	 * This eventually calls into get_option() which

	 * has a ton of callers and is not const.  It is

	 * easiest to cast it away here.

 CONFIG_KEXEC_CORE */

 CONFIG_CRASH_CORE */

 whether file capabilities are enabled */

 #ifndef CONFIG_TINY_RCU */

/*

 * Make /sys/kernel/notes give the raw contents of our kernel .notes section.

 SPDX-License-Identifier: GPL-2.0-or-later

/* audit_fsnotify.c -- tracking inodes

 *

 * Copyright 2003-2009,2014-2015 Red Hat, Inc.

 * Copyright 2005 Hewlett-Packard Development Company, L.P.

 * Copyright 2005 IBM Corporation

/*

 * this mark lives on the parent directory of the inode in question.

 * but dev, ino, and path are about the child

 associated superblock device */

 associated inode number */

 insertion path */

 fsnotify mark on the inode */

 fsnotify handle. */

 fsnotify events we care about. */

 returning an error */

 Update mark data in audit rules based on fsnotify events. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/kernel/sys.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 Move somewhere else to avoid recompiling? */

/*

 * this is where the system-wide overflow UID and GID are defined, for

 * architectures that now have 32-bit UID/GID but didn't in the past

/*

 * the same as above, but for filesystems which can only store a 16-bit

 * UID and GID. as such, this is needed on all architectures

/*

 * Returns true if current's euid is same as p's uid or euid,

 * or has CAP_SYS_NICE to p's user_ns.

 *

 * Called with rcu_read_lock, creds are safe

/*

 * set the priority of a task

 * - the caller must hold the RCU read lock

 normalize: avoid signed division (rounding problems) */

 No processes for this user */

 For find_user() */

/*

 * Ugh. To avoid negative return values, "getpriority()" will

 * not return the normal nice-value, but a negated value that

 * has been offset by 20 (ie it returns 40..1 instead of -20..19)

 * to stay compatible.

 No processes for this user */

 for find_user() */

/*

 * Unprivileged users may change the real gid to the effective gid

 * or vice versa.  (BSD-style)

 *

 * If you set the real gid at all, or set the effective gid to a value not

 * equal to the real gid, then the saved gid is set to the new effective gid.

 *

 * This makes it possible for a setgid program to completely drop its

 * privileges, which is often a useful assertion to make when you are doing

 * a security audit over a program.

 *

 * The general idea is that a program which uses just setregid() will be

 * 100% compatible with BSD.  A program which uses just setgid() will be

 * 100% compatible with POSIX with saved IDs.

 *

 * SMP: There are not races, the GIDs are checked only by filesystem

 *      operations (as far as semantic preservation is concerned).

/*

 * setgid() is implemented like SysV w/ SAVED_IDS

 *

 * SMP: Same implicit races as above.

/*

 * change the user struct in a credentials set to match the new UID

	/*

	 * We don't fail in case of NPROC limit excess here because too many

	 * poorly written programs don't check set*uid() return code, assuming

	 * it never fails if called by root.  We may still enforce NPROC limit

	 * for programs doing set*uid()+execve() by harmlessly deferring the

	 * failure to the execve() stage.

/*

 * Unprivileged users may change the real uid to the effective uid

 * or vice versa.  (BSD-style)

 *

 * If you set the real uid at all, or set the effective uid to a value not

 * equal to the real uid, then the saved uid is set to the new effective uid.

 *

 * This makes it possible for a setuid program to completely drop its

 * privileges, which is often a useful assertion to make when you are doing

 * a security audit over a program.

 *

 * The general idea is that a program which uses just setreuid() will be

 * 100% compatible with BSD.  A program which uses just setuid() will be

 * 100% compatible with POSIX with saved IDs.

/*

 * setuid() is implemented like SysV with SAVED_IDS

 *

 * Note that SAVED_ID's is deficient in that a setuid root program

 * like sendmail, for example, cannot set its uid to be a normal

 * user and then switch back, because if you're root, setuid() sets

 * the saved uid too.  If you don't like this, blame the bright people

 * in the POSIX committee and/or USG.  Note that the BSD-style setreuid()

 * will allow a root program to temporarily drop privileges and be able to

 * regain them by swapping the real and effective uid.

/*

 * This function implements a generic ability to update ruid, euid,

 * and suid.  This allows you to implement the 4.4 compatible seteuid().

/*

 * Same as above, but for rgid, egid, sgid.

/*

 * "setfsuid()" sets the fsuid - the uid used for filesystem checks. This

 * is used for "access()" and for the NFS daemon (letting nfsd stay at

 * whatever uid it wants to). It normally shadows "euid", except when

 * explicitly set by setfsuid() or for access..

/*

 * Samma på svenska..

 CONFIG_MULTIUSER */

/**

 * sys_getpid - return the thread group id of the current process

 *

 * Note, despite the name, this returns the tgid not the pid.  The tgid and

 * the pid are identical unless CLONE_THREAD was specified on clone() in

 * which case the tgid is the same in all threads of the same group.

 *

 * This is SMP safe as current->tgid does not change.

 Thread ID - the internal kernel "pid" */

/*

 * Accessing ->real_parent is not SMP-safe, it could

 * change from under us. However, we can use a stale

 * value of ->real_parent under rcu_read_lock(), see

 * release_task()->call_rcu(delayed_put_task_struct).

 Only we change this so SMP safe */

 Only we change this so SMP safe */

 Only we change this so SMP safe */

 Only we change this so SMP safe */

 Convert our struct tms to the compat version. */

/*

 * This needs some heavy checking ...

 * I just haven't the stomach for it. I also don't fully

 * understand sessions/pgrp etc. Let somebody who does explain it.

 *

 * OK, I think I have the protection semantics right.... this is really

 * only important on a multi-user system anyway, to make sure one user

 * can't send a signal to a process owned by another.  -TYT, 12/12/91

 *

 * !PF_FORKNOEXEC check to conform completely to POSIX.

	/* From this point forward we keep holding onto the tasklist lock

	 * so that our parent does not change from under us. -DaveM

 All paths lead to here, thus we are safe. -DaveM */

 Fail if I am already a session leader */

	/* Fail if a process group id already exists that equals the

	 * proposed session id.

/*

 * Work around broken programs that cannot handle "Linux 3.0".

 * Instead we map 3.x to 2.6.40+x, so e.g. 3.0 would be 2.6.40

 * And we map 4.x and later versions to 2.6.60+x, so 4.0/5.0/6.0/... would be

 * 2.6.60.

/*

 * Old cruft

/*

 * Only setdomainname; getdomainname can be implemented by calling

 * uname()

/*

 *	Back compatibility for getrlimit. Needed for some apps.

 make sure you are allowed to change @tsk limits before calling this */

 protect tsk->signal and tsk->sighand from disappearing */

		/* Keep the capable check against init_user_ns until

	/*

	 * RLIMIT_CPU handling. Arm the posix CPU timer if the limit is not

	 * infinite. In case of RLIM_INFINITY the posix CPU timer code

	 * ignores the rlimit.

 rcu lock must be held */

/*

 * It would make sense to put struct rusage in the task_struct,

 * except that would make the task_struct be *really big*.  After

 * task_struct gets moved into malloc'ed memory, it would

 * make sense to do this.  It will make moving the rest of the information

 * a lot simpler!  (Which we're not doing right now because we're not

 * measuring them yet).

 *

 * When sampling multiple threads for RUSAGE_SELF, under SMP we might have

 * races with threads incrementing their own counters.  But since word

 * reads are atomic, we either get new values or old values and we don't

 * care which for the sums.  We always take the siglock to protect reading

 * the c* fields from p->signal from races with exit.c updating those

 * fields when reaping, so a sample either gets all the additions of a

 * given child after it's reaped, or none so this sample is before reaping.

 *

 * Locking:

 * We need to take the siglock for CHILDEREN, SELF and BOTH

 * for  the cases current multithreaded, non-current single threaded

 * non-current multithreaded.  Thread traversal is now safe with

 * the siglock held.

 * Strictly speaking, we donot need to take the siglock if we are current and

 * single threaded,  as no one else can take our signal_struct away, no one

 * else can  reap the  children to update signal->c* counters, and no one else

 * can race with the signal-> fields. If we do not take any lock, the

 * signal-> fields could be read out of order while another thread was just

 * exiting. So we should  place a read memory barrier when we avoid the lock.

 * On the writer side,  write memory barrier is implied in  __exit_signal

 * as __exit_signal releases  the siglock spinlock after updating the signal->

 * fields. But we don't do this yet to keep things simple.

 *

 convert pages to KBs */

	/*

	 * Because the original mm->exe_file points to executable file, make

	 * sure that this one is executable as well, to avoid breaking an

	 * overall picture.

/*

 * Check arithmetic relations of passed addresses.

 *

 * WARNING: we don't require any capability here so be very careful

 * in what is allowed for modification from userspace.

	/*

	 * Make sure the members are not somewhere outside

	 * of allowed address space.

	/*

	 * Make sure the pairs are ordered.

	/*

	 * Neither we should allow to override limits if they set.

		/*

		 * Someone is trying to cheat the auxv vector.

 Last entry must be AT_NULL as specification requires */

		/*

		 * Check if the current user is checkpoint/restore capable.

		 * At the time of this writing, it checks for CAP_SYS_ADMIN

		 * or CAP_CHECKPOINT_RESTORE.

		 * Note that a user with access to ptrace can masquerade an

		 * arbitrary program as any executable, even setuid ones.

		 * This may have implications in the tomoyo subsystem.

	/*

	 * arg_lock protects concurrent updates but we still need mmap_lock for

	 * read to exclude races with sys_brk.

	/*

	 * We don't validate if these members are pointing to

	 * real present VMAs because application may have correspond

	 * VMAs already unmapped and kernel uses these members for statistics

	 * output in procfs mostly, except

	 *

	 *  - @start_brk/@brk which are used in do_brk_flags but kernel lookups

	 *    for VMAs when updating these members so anything wrong written

	 *    here cause kernel to swear at userspace program but won't lead

	 *    to any problem in kernel itself

	/*

	 * Note this update of @saved_auxv is lockless thus

	 * if someone reads this member in procfs while we're

	 * updating -- it may get partly updated results. It's

	 * known and acceptable trade off: we leave it as is to

	 * not introduce additional locks here making the kernel

	 * more complex.

 CONFIG_CHECKPOINT_RESTORE */

	/*

	 * This doesn't move the auxiliary vector itself since it's pinned to

	 * mm_struct, but it permits filling the vector with new values.  It's

	 * up to the caller to provide sane values here, otherwise userspace

	 * tools which use this vector might be unhappy.

 Make sure the last entry is always AT_NULL */

	/*

	 * arg_lock protects concurrent updates of arg boundaries, we need

	 * mmap_lock for a) concurrent sys_brk, b) finding VMA for addr

	 * validation.

	/*

	 * If command line arguments and environment

	 * are placed somewhere else on stack, we can

	 * set them up here, ARG_START/END to setup

	 * command line arguments and ENV_START/END

	 * for environment.

	/*

	 * If task has has_child_subreaper - all its descendants

	 * already have these flag too and new descendants will

	 * inherit it on fork, skip them.

	 *

	 * If we've found child_reaper - skip descendants in

	 * it's subtree as they will never get out pidns.

 No longer implemented: */

/**

 * do_sysinfo - fill in sysinfo struct

 * @info: pointer to buffer to fill

	/*

	 * If the sum of all the available memory (i.e. ram + swap)

	 * is less than can be stored in a 32 bit unsigned long then

	 * we can be binary compatible with 2.2.x kernels.  If not,

	 * well, in that case 2.2.x was broken anyways...

	 *

	 *  -Erik Andersen <andersee@debian.org>

	/*

	 * If mem_total did not overflow, multiply all memory values by

	 * info->mem_unit and set it to 1.  This leaves things compatible

	 * with 2.2.x, and also retains compatibility with earlier 2.4.x

	 * kernels...

	/* Check to see if any memory value is too large for 32-bit and scale

	 *  down if needed

 CONFIG_COMPAT */

 SPDX-License-Identifier: GPL-2.0

/*

 * Provide kernel headers useful to build tracing programs

 * such as for running eBPF tracing tools.

 *

 * (Borrowed code from kernel/configs.c)

/*

 * Define kernel_headers_data and kernel_headers_data_end, within which the

 * compressed kernel headers are stored. The file is first compressed with xz.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	linux/kernel/softirq.c

 *

 *	Copyright (C) 1992 Linus Torvalds

 *

 *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)

/*

   - No shared variables, all the data are CPU local.

   - If a softirq needs serialization, let it serialize itself

     by its own spinlocks.

   - Even if softirq is serialized, only local cpu is marked for

     execution. Hence, we get something sort of weak cpu binding.

     Though it is still not clear, will it result in better locality

     or will not.



   Examples:

   - NET RX softirq. It is multithreaded and does not require

     any global serialization.

   - NET TX softirq. It kicks software netdevice queues, hence

     it is logically serialized per device, but this serialization

     is invisible to common code.

   - Tasklets: serialized wrt itself.

/*

 * we cannot loop indefinitely here to avoid userspace starvation,

 * but we also don't want to introduce a worst case 1/HZ latency

 * to the pending events, so lets the scheduler to balance

 * the softirq load for us.

 Interrupts are disabled: no need to stop preemption */

/*

 * If ksoftirqd is scheduled, we do not want to process pending softirqs

 * right now. Let ksoftirqd handle this at its own rate, to get fairness,

 * unless we're doing some of the synchronous softirqs.

/*

 * SOFTIRQ_OFFSET usage:

 *

 * On !RT kernels 'count' is the preempt counter, on RT kernels this applies

 * to a per CPU counter and to task::softirqs_disabled_cnt.

 *

 * - count is changed by SOFTIRQ_OFFSET on entering or leaving softirq

 *   processing.

 *

 * - count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)

 *   on local_bh_disable or local_bh_enable.

 *

 * This lets us distinguish between whether we are currently processing

 * softirq and whether we just have bh disabled.

/*

 * RT accounts for BH disabled sections in task::softirqs_disabled_cnt and

 * also in per CPU softirq_ctrl::cnt. This is necessary to allow tasks in a

 * softirq disabled section to be preempted.

 *

 * The per task counter is used for softirq_count(), in_softirq() and

 * in_serving_softirqs() because these counts are only valid when the task

 * holding softirq_ctrl::lock is running.

 *

 * The per CPU counter prevents pointless wakeups of ksoftirqd in case that

 * the task which is in a softirq disabled section is preempted or blocks.

/**

 * local_bh_blocked() - Check for idle whether BH processing is blocked

 *

 * Returns false if the per CPU softirq::cnt is 0 otherwise true.

 *

 * This is invoked from the idle task to guard against false positive

 * softirq pending warnings, which would happen when the task which holds

 * softirq_ctrl::lock was the only running task on the CPU and blocks on

 * some other lock.

 First entry of a task into a BH disabled section? */

 Required to meet the RCU bottomhalf requirements. */

	/*

	 * Track the per CPU softirq disabled state. On RT this is per CPU

	 * state to allow preemption of bottom half disabled sections.

	/*

	 * Reflect the result in the task state to prevent recursion on the

	 * local lock and to make softirq_count() & al work.

	/*

	 * If this is not reenabling soft interrupts, no point in trying to

	 * run pending ones.

	/*

	 * If this was called from non preemptible context, wake up the

	 * softirq daemon.

	/*

	 * Adjust softirq count to SOFTIRQ_OFFSET which makes

	 * in_serving_softirq() become true.

/*

 * Invoked from ksoftirqd_run() outside of the interrupt disabled section

 * to acquire the per CPU local lock for reentrancy protection.

 Counterpart to ksoftirqd_run_begin() */

 CONFIG_PREEMPT_RT */

/*

 * This one is for softirq.c-internal use, where hardirqs are disabled

 * legitimately:

	/*

	 * The preempt tracer hooks into preempt_count_add and will break

	 * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET

	 * is set and before current->softirq_enabled is cleared.

	 * We must manually increment preempt_count here and manually

	 * call the trace_preempt_off later.

	/*

	 * Were softirqs turned off above:

 CONFIG_TRACE_IRQFLAGS */

/*

 * Special-case - softirqs can safely be enabled by __do_softirq(),

 * without processing still-pending softirqs:

	/*

	 * Are softirqs going to be turned on now:

	/*

	 * Keep preemption disabled until we are done with

	 * softirq processing:

		/*

		 * Run softirq if any pending. And do it in its own stack

		 * as we may be calling this deep in a task call stack already.

		/*

		 * We can safely execute softirq on the current stack if

		 * it is the irq stack, because it should be near empty

		 * at this stage.

		/*

		 * Otherwise, irq_exit() is called on the task stack that can

		 * be potentially deep already. So call softirq in its own stack

		 * to prevent from any overrun.

 !CONFIG_PREEMPT_RT */

/*

 * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,

 * but break the loop if need_resched() is set or after 2 ms.

 * The MAX_SOFTIRQ_TIME provides a nice upper bound in most cases, but in

 * certain cases, such as stop_machine(), jiffies may cease to

 * increment and so we need the MAX_SOFTIRQ_RESTART limit as

 * well to make sure we eventually return from this method.

 *

 * These limits have been established via experimentation.

 * The two things to balance is latency against fairness -

 * we want to handle softirqs as soon as possible, but they

 * should not be able to lock up the box.

/*

 * When we run softirqs from irq_exit() and thus on the hardirq stack we need

 * to keep the lockdep irq context tracking as tight as possible in order to

 * not miss-qualify lock contexts and miss possible deadlocks.

	/*

	 * Mask out PF_MEMALLOC as the current task context is borrowed for the

	 * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC

	 * again if the socket is related to swapping.

 Reset the pending bitmask before enabling irqs */

/**

 * irq_enter_rcu - Enter an interrupt context with RCU watching

/**

 * irq_enter - Enter an interrupt context including RCU update

 Make sure that timer wheel updates are propagated */

/**

 * irq_exit_rcu() - Exit an interrupt context without updating RCU

 *

 * Also processes softirqs if needed and possible.

 must be last! */

/**

 * irq_exit - Exit an interrupt context, update RCU and lockdep

 *

 * Also processes softirqs if needed and possible.

 must be last! */

/*

 * This function must run with irqs disabled!

	/*

	 * If we're in an interrupt or softirq, we're done

	 * (this also catches softirq-disabled code). We will

	 * actually run the softirq once we return from

	 * the irq or softirq.

	 *

	 * Otherwise we wake up ksoftirqd to make sure we

	 * schedule the softirq soon.

/*

 * Tasklets

/*

 * Do not use in new code. Waiting for tasklets from atomic contexts is

 * error prone and should be avoided.

			/*

			 * Prevent a live lock when current preempted soft

			 * interrupt processing or prevents ksoftirqd from

			 * running. If the tasklet runs on a different CPU

			 * then this has no effect other than doing the BH

			 * disable/enable dance for nothing.

		/*

		 * We can safely run softirq on inline stack, as we are not deep

		 * in the task stack here.

 CPU is dead, so no lock needed. */

 Find end, append list for that CPU. */

 CONFIG_HOTPLUG_CPU */

/*

 * [ These __weak aliases are kept in a separate compilation unit, so that

 *   GCC does not inline them incorrectly. ]

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Common SMP CPU bringup/teardown functions

/*

 * For the hotplug case we keep the task structs around and reuse

 * them.

/**

 * idle_init - Initialize the idle thread for a cpu

 * @cpu:	The cpu for which the idle thread should be initialized

 *

 * Creates the thread if it does not exist.

/**

 * idle_threads_init - Initialize idle threads for all cpus

 #ifdef CONFIG_SMP */

/**

 * smpboot_thread_fn - percpu hotplug thread loop function

 * @data:	thread data pointer

 *

 * Checks for thread stop and park conditions. Calls the necessary

 * setup, cleanup, park and unpark functions for the registered

 * thread.

 *

 * Returns 1 when the thread should exit, 0 otherwise.

 cleanup must mirror setup */

 We might have been woken for stop */

 Check for state change setup */

	/*

	 * Park the thread so that it could start right on the CPU

	 * when it is available.

		/*

		 * Make sure that the task has actually scheduled out

		 * into park position, before calling the create

		 * callback. At least the migration thread callback

		 * requires that the task is off the runqueue.

 We need to destroy also the parked threads of offline cpus */

/**

 * smpboot_register_percpu_thread - Register a per_cpu thread related

 * 					    to hotplug

 * @plug_thread:	Hotplug thread descriptor

 *

 * Creates and starts the threads on all online cpus.

/**

 * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug

 * @plug_thread:	Hotplug thread descriptor

 *

 * Stops all threads on all possible cpus.

/*

 * Called to poll specified CPU's state, for example, when waiting for

 * a CPU to come online.

/*

 * If CPU has died properly, set its state to CPU_UP_PREPARE and

 * return success.  Otherwise, return -EBUSY if the CPU died after

 * cpu_wait_death() timed out.  And yet otherwise again, return -EAGAIN

 * if cpu_wait_death() timed out and the CPU still hasn't gotten around

 * to dying.  In the latter two cases, the CPU might not be set up

 * properly, but it is up to the arch-specific code to decide.

 * Finally, -EIO indicates an unanticipated problem.

 *

 * Note that it is permissible to omit this call entirely, as is

 * done in architectures that do no CPU-hotplug error checking.

 The CPU died properly, so just start it up again. */

		/*

		 * Timeout during CPU death, so let caller know.

		 * The outgoing CPU completed its processing, but after

		 * cpu_wait_death() timed out and reported the error. The

		 * caller is free to proceed, in which case the state

		 * will be reset properly by cpu_set_state_online().

		 * Proceeding despite this -EBUSY return makes sense

		 * for systems where the outgoing CPUs take themselves

		 * offline, with no post-death manipulation required from

		 * a surviving CPU.

		/*

		 * The most likely reason we got here is that there was

		 * a timeout during CPU death, and the outgoing CPU never

		 * did complete its processing.  This could happen on

		 * a virtualized system if the outgoing VCPU gets preempted

		 * for more than five seconds, and the user attempts to

		 * immediately online that same CPU.  Trying again later

		 * might return -EBUSY above, hence -EAGAIN.

 Should not happen.  Famous last words. */

/*

 * Mark the specified CPU online.

 *

 * Note that it is permissible to omit this call entirely, as is

 * done in architectures that do no CPU-hotplug error checking.

/*

 * Wait for the specified CPU to exit the idle loop and die.

 The outgoing CPU will normally get done quite quickly. */

 But if the outgoing CPU dawdles, wait increasingly long times. */

 Outgoing CPU died normally, update state. */

 atomic_read() before update. */

 Outgoing CPU still hasn't died, set state accordingly. */

/*

 * Called by the outgoing CPU to report its successful death.  Return

 * false if this report follows the surviving CPU's timing out.

 *

 * A separate "CPU_DEAD_FROZEN" is used when the surviving CPU

 * timed out.  This approach allows architectures to omit calls to

 * cpu_check_up_prepare() and cpu_set_state_online() without defeating

 * the next cpu_wait_death()'s polling loop.

 #ifdef CONFIG_HOTPLUG_CPU */

 SPDX-License-Identifier: GPL-2.0

/*

 * KUnit test of proc sysctl.

/*

 * Test that proc_dointvec will not try to use a NULL .data field even when the

 * length is non-zero.

		/*

		 * Here we are testing that proc_dointvec behaves correctly when

		 * we give it a NULL .data field. Normally this would point to a

		 * piece of memory where the value would be stored.

	/*

	 * proc_dointvec expects a buffer in user space, so we allocate one. We

	 * also need to cast it to __user so sparse doesn't get mad.

	/*

	 * We don't care what the starting length is since proc_dointvec should

	 * not try to read because .data is NULL.

	/*

	 * See above.

/*

 * Similar to the previous test, we create a struct ctrl_table that has a .data

 * field that proc_dointvec cannot do anything with; however, this time it is

 * because we tell proc_dointvec that the size is 0.

		/*

		 * So .data is no longer NULL, but we tell proc_dointvec its

		 * length is 0, so it still shouldn't try to use it.

	/*

	 * As before, we don't care what buffer length is because proc_dointvec

	 * cannot do anything because its internal .data buffer has zero length.

	/*

	 * See previous comment.

/*

 * Here we provide a valid struct ctl_table, but we try to read and write from

 * it using a buffer of zero length, so it should still fail in a similar way as

 * before.

 Good table. */

	/*

	 * However, now our read/write buffer has zero length.

/*

 * Test that proc_dointvec refuses to read when the file position is non-zero.

 Good table. */

	/*

	 * We don't care about our buffer length because we start off with a

	 * non-zero file position.

	/*

	 * proc_dointvec should refuse to read into the buffer since the file

	 * pos is non-zero.

/*

 * Test that we can read a two digit number in a sufficiently size buffer.

 * Nothing fancy.

 Good table. */

 Store 13 in the data field. */

 And we read 13 back out. */

/*

 * Same as previous test, just now with negative numbers.

 Good table. */

/*

 * Test that a simple positive write works.

 Good table. */

/*

 * Same as previous test, but now with negative numbers.

/*

 * Test that writing a value smaller than the minimum possible value is not

 * allowed.

	/*

	 * We use this rigmarole to create a string that contains a value one

	 * less than the minimum accepted value.

/*

 * Test that writing the maximum possible value works.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * umd - User mode driver support

 Flush delayed fput so exec can open the file read-only */

/**

 * umd_load_blob - Remember a blob of bytes for fork_usermode_driver

 * @info: information about usermode driver

 * @data: a blob of bytes that can be executed as a file

 * @len:  The lentgh of the blob

 *

/**

 * umd_unload_blob - Disassociate @info from a previously loaded blob

 * @info: information about usermode driver

 *

 create pipe to send data to umh */

 create pipe to receive data from umh */

 cleanup if umh_setup() was successful but exec failed */

/**

 * umd_cleanup_helper - release the resources which were allocated in umd_setup

 * @info: information about usermode driver

/**

 * fork_usermode_driver - fork a usermode driver

 * @info: information about usermode driver (shouldn't be NULL)

 *

 * Returns either negative error or zero which indicates success in

 * executing a usermode driver. In such case 'struct umd_info *info'

 * is populated with two pipes and a tgid of the process. The caller is

 * responsible for health check of the user process, killing it via

 * tgid, and closing the pipes when user process is no longer needed.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/stacktrace.c

 *

 * Stack trace management functions

 *

 *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

/**

 * stack_trace_print - Print the entries in the stack trace

 * @entries:	Pointer to storage array

 * @nr_entries:	Number of entries in the storage array

 * @spaces:	Number of leading spaces to print

/**

 * stack_trace_snprint - Print the entries in the stack trace into a buffer

 * @buf:	Pointer to the print buffer

 * @size:	Size of the print buffer

 * @entries:	Pointer to storage array

 * @nr_entries:	Number of entries in the storage array

 * @spaces:	Number of leading spaces to print

 *

 * Return: Number of bytes printed.

/**

 * stack_trace_save - Save a stack trace into a storage array

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored.

/**

 * stack_trace_save_tsk - Save a task stack trace into a storage array

 * @task:	The task to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored.

 skip this function if they are tracing us */

/**

 * stack_trace_save_regs - Save a stack trace based on pt_regs into a storage array

 * @regs:	Pointer to pt_regs to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored.

/**

 * stack_trace_save_tsk_reliable - Save task stack with verification

 * @tsk:	Pointer to the task to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 *

 * Return:	An error if it detects any unreliable features of the

 *		stack. Otherwise it guarantees that the stack trace is

 *		reliable and returns the number of entries stored.

 *

 * If the task is not 'current', the caller *must* ensure the task is inactive.

	/*

	 * If the task doesn't have a stack (e.g., a zombie), the stack is

	 * "reliably" empty.

/**

 * stack_trace_save_user - Save a user space stack trace into a storage array

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 *

 * Return: Number of trace entries stored.

 Trace user stack if not a kernel thread */

 CONFIG_ARCH_STACKWALK */

/*

 * Architectures that do not implement save_stack_trace_*()

 * get these weak aliases and once-per-bootup warnings

 * (whenever this facility is utilized - for example by procfs):

/**

 * stack_trace_save - Save a stack trace into a storage array

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored

/**

 * stack_trace_save_tsk - Save a task stack trace into a storage array

 * @task:	The task to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored

 skip this function if they are tracing us */

/**

 * stack_trace_save_regs - Save a stack trace based on pt_regs into a storage array

 * @regs:	Pointer to pt_regs to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 * @skipnr:	Number of entries to skip at the start of the stack trace

 *

 * Return: Number of trace entries stored

/**

 * stack_trace_save_tsk_reliable - Save task stack with verification

 * @tsk:	Pointer to the task to examine

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 *

 * Return:	An error if it detects any unreliable features of the

 *		stack. Otherwise it guarantees that the stack trace is

 *		reliable and returns the number of entries stored.

 *

 * If the task is not 'current', the caller *must* ensure the task is inactive.

/**

 * stack_trace_save_user - Save a user space stack trace into a storage array

 * @store:	Pointer to storage array

 * @size:	Size of the storage array

 *

 * Return: Number of trace entries stored

 CONFIG_USER_STACKTRACE_SUPPORT */

 !CONFIG_ARCH_STACKWALK */

/**

 * filter_irq_stacks - Find first IRQ stack entry in trace

 * @entries:	Pointer to stack trace array

 * @nr_entries:	Number of entries in the storage array

 *

 * Return: Number of trace entries until IRQ stack starts.

 Include the irqentry function into the stack. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/compat.c

 *

 *  Kernel compatibililty routines for e.g. 32 bit syscall support

 *  on 64 bit kernels.

 *

 *  Copyright (C) 2002-2003 Stephen Rothwell, IBM Corporation

 for MAX_SCHEDULE_TIMEOUT */

/*

 * sys_sigprocmask SIG_SETMASK sets the first (compat) word of the

 * blocked set of signals to the supplied signal set

/*

 * We currently only need the following fields from the sigevent

 * structure: sigev_value, sigev_signo, sig_notify and (sometimes

 * sigev_notify_thread_id).  The others are handled in user mode.

 * We also assume that copying sigev_value.sival_int is sufficient

 * to keep all the bits of sigev_value.sival_ptr intact.

 align bitmap up to nearest compat_long_t boundary */

 align bitmap up to nearest compat_long_t boundary */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2006 IBM Corporation

 *

 *  Author: Serge Hallyn <serue@us.ibm.com>

 *

 *  Jun 2006 - namespaces support

 *             OpenVZ, SWsoft Inc.

 *             Pavel Emelianov <xemul@openvz.org>

/*

 * Create new nsproxy and all of its the associated namespaces.

 * Return the newly created nsproxy.  Do not attach this to the task,

 * leave it to the caller to do proper locking and attach it to task.

/*

 * called from clone.  This now handles copy for nsproxy and all

 * namespaces therein.

	/*

	 * CLONE_NEWIPC must detach from the undolist: after switching

	 * to a new ipc namespace, the semaphore arrays from the old

	 * namespace are unreachable.  In clone parlance, CLONE_SYSVSEM

	 * means share undolist with parent, so we must forbid using

	 * it along with CLONE_NEWIPC.

/*

 * Called from unshare. Unshare all the namespaces part of nsproxy.

 * On success, returns the new nsproxy.

	/*

	 * We only created a temporary copy if we attached to more than just

	 * the mount namespace.

 Only create a temporary copy of fs_struct if we really need to. */

/*

 * This is the inverse operation to unshare().

 * Ordering is equivalent to the standard ordering used everywhere else

 * during unshare and process creation. The switch to the new set of

 * namespaces occurs at the point of no return after installation of

 * all requested namespaces was successful in commit_nsset().

 Take a "snapshot" of the target task's namespaces. */

	/*

	 * Install requested namespaces. The caller will have

	 * verified earlier that the requested namespaces are

	 * supported on this kernel. We don't report errors here

	 * if a namespace is requested that isn't supported.

/*

 * This is the point of no return. There are just a few namespaces

 * that do some actual work here and it's sufficiently minimal that

 * a separate ns_common operation seems unnecessary for now.

 * Unshare is doing the same thing. If we'll end up needing to do

 * more in a given namespace or a helper here is ultimately not

 * exported anymore a simple commit handler for each namespace

 * should be added to ns_common.

 transfer ownership */

 We only need to commit if we have used a temporary fs_struct. */

 transfer ownership */

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/kernel/capability.c

 *

 * Copyright (C) 1997  Andrew Main <zefram@fysh.org>

 *

 * Integrated into 2.1.97+,  Andrew G. Morgan <morgan@kernel.org>

 * 30 May 2002:	Cleanup, Robert M. Love <rml@tech9.net>

/*

 * Leveraged for setting/resetting capabilities

/*

 * More recent versions of libcap are available from:

 *

 *   http://www.kernel.org/pub/linux/libs/security/linux-privs/

/*

 * Version 2 capabilities worked fine, but the linux/capability.h file

 * that accompanied their introduction encouraged their use without

 * the necessary user-space source code changes. As such, we have

 * created a version 3 with equivalent functionality to version 2, but

 * with a header change to protect legacy source code from using

 * version 2 when it wanted to use version 1. If your system has code

 * that trips the following warning, it is using version 2 specific

 * capabilities and may be doing so insecurely.

 *

 * The remedy is to either upgrade your version of libcap (to 2.10+,

 * if the application is linked against it), or recompile your

 * application with modern kernel headers and this warning will go

 * away.

/*

 * Version check. Return the number of u32s in each capability flag

 * array, or a negative value on error.

 v3 is otherwise equivalent to v2 */

/*

 * The only thing that can change the capabilities of the current

 * process is the current process. As such, we can't be in this code

 * at the same time as we are in the process of setting capabilities

 * in this process. The net result is that we can limit our use of

 * locks to when we are reading the caps of another process.

/**

 * sys_capget - get the capabilities of a given process.

 * @header: pointer to struct that contains capability version and

 *	target pid data

 * @dataptr: pointer to struct that contains the effective, permitted,

 *	and inheritable capabilities that are returned

 *

 * Returns 0 on success and < 0 on error.

		/*

		 * Note, in the case, tocopy < _KERNEL_CAPABILITY_U32S,

		 * we silently drop the upper capabilities here. This

		 * has the effect of making older libcap

		 * implementations implicitly drop upper capability

		 * bits when they perform a: capget/modify/capset

		 * sequence.

		 *

		 * This behavior is considered fail-safe

		 * behavior. Upgrading the application to a newer

		 * version of libcap will enable access to the newer

		 * capabilities.

		 *

		 * An alternative would be to return an error here

		 * (-ERANGE), but that causes legacy applications to

		 * unexpectedly fail; the capget/modify/capset aborts

		 * before modification is attempted and the application

		 * fails.

/**

 * sys_capset - set capabilities for a process or (*) a group of processes

 * @header: pointer to struct that contains capability version and

 *	target pid data

 * @data: pointer to struct that contains the effective, permitted,

 *	and inheritable capabilities

 *

 * Set capabilities for the current process only.  The ability to any other

 * process(es) has been deprecated and removed.

 *

 * The restrictions on setting capabilities are specified as:

 *

 * I: any raised capabilities must be a subset of the old permitted

 * P: any raised capabilities must be a subset of the old permitted

 * E: must be set to a subset of new permitted

 *

 * Returns 0 on success and < 0 on error.

 may only affect current now */

/**

 * has_ns_capability - Does a task have a capability in a specific user ns

 * @t: The task in question

 * @ns: target user namespace

 * @cap: The capability to be tested for

 *

 * Return true if the specified task has the given superior capability

 * currently in effect to the specified user namespace, false if not.

 *

 * Note that this does not set PF_SUPERPRIV on the task.

/**

 * has_capability - Does a task have a capability in init_user_ns

 * @t: The task in question

 * @cap: The capability to be tested for

 *

 * Return true if the specified task has the given superior capability

 * currently in effect to the initial user namespace, false if not.

 *

 * Note that this does not set PF_SUPERPRIV on the task.

/**

 * has_ns_capability_noaudit - Does a task have a capability (unaudited)

 * in a specific user ns.

 * @t: The task in question

 * @ns: target user namespace

 * @cap: The capability to be tested for

 *

 * Return true if the specified task has the given superior capability

 * currently in effect to the specified user namespace, false if not.

 * Do not write an audit message for the check.

 *

 * Note that this does not set PF_SUPERPRIV on the task.

/**

 * has_capability_noaudit - Does a task have a capability (unaudited) in the

 * initial user ns

 * @t: The task in question

 * @cap: The capability to be tested for

 *

 * Return true if the specified task has the given superior capability

 * currently in effect to init_user_ns, false if not.  Don't write an

 * audit message for the check.

 *

 * Note that this does not set PF_SUPERPRIV on the task.

/**

 * ns_capable - Determine if the current task has a superior capability in effect

 * @ns:  The usernamespace we want the capability in

 * @cap: The capability to be tested for

 *

 * Return true if the current task has the given superior capability currently

 * available for use, false if not.

 *

 * This sets PF_SUPERPRIV on the task if the capability is available on the

 * assumption that it's about to be used.

/**

 * ns_capable_noaudit - Determine if the current task has a superior capability

 * (unaudited) in effect

 * @ns:  The usernamespace we want the capability in

 * @cap: The capability to be tested for

 *

 * Return true if the current task has the given superior capability currently

 * available for use, false if not.

 *

 * This sets PF_SUPERPRIV on the task if the capability is available on the

 * assumption that it's about to be used.

/**

 * ns_capable_setid - Determine if the current task has a superior capability

 * in effect, while signalling that this check is being done from within a

 * setid or setgroups syscall.

 * @ns:  The usernamespace we want the capability in

 * @cap: The capability to be tested for

 *

 * Return true if the current task has the given superior capability currently

 * available for use, false if not.

 *

 * This sets PF_SUPERPRIV on the task if the capability is available on the

 * assumption that it's about to be used.

/**

 * capable - Determine if the current task has a superior capability in effect

 * @cap: The capability to be tested for

 *

 * Return true if the current task has the given superior capability currently

 * available for use, false if not.

 *

 * This sets PF_SUPERPRIV on the task if the capability is available on the

 * assumption that it's about to be used.

 CONFIG_MULTIUSER */

/**

 * file_ns_capable - Determine if the file's opener had a capability in effect

 * @file:  The file we want to check

 * @ns:  The usernamespace we want the capability in

 * @cap: The capability to be tested for

 *

 * Return true if task that opened the file had a capability in effect

 * when the file was opened.

 *

 * This does not set PF_SUPERPRIV because the caller may not

 * actually be privileged.

/**

 * privileged_wrt_inode_uidgid - Do capabilities in the namespace work over the inode?

 * @ns: The user namespace in question

 * @inode: The inode in question

 *

 * Return true if the inode uid and gid are within the namespace.

/**

 * capable_wrt_inode_uidgid - Check nsown_capable and uid and gid mapped

 * @inode: The inode in question

 * @cap: The capability in question

 *

 * Return true if the current task has the given capability targeted at

 * its own user namespace and that the given inode's uid and gid are

 * mapped into the current user namespace.

/**

 * ptracer_capable - Determine if the ptracer holds CAP_SYS_PTRACE in the namespace

 * @tsk: The task that may be ptraced

 * @ns: The user namespace to search for CAP_SYS_PTRACE in

 *

 * Return true if the task that is ptracing the current task had CAP_SYS_PTRACE

 * in the specified user namespace.

 An absent tracer adds no restrictions */

 SPDX-License-Identifier: GPL-2.0

/*

 * Handling of different ABIs (personalities).

 *

 * We group personalities into execution domains which have their

 * own handlers for kernel entry points, signal mapping, etc...

 *

 * 2001-05-06	Complete rewrite,  Christoph Hellwig (hch@infradead.org)

 SPDX-License-Identifier: GPL-2.0-only

/* Kernel thread helper functions.

 *   Copyright (C) 2004 IBM Corporation, Rusty Russell.

 *   Copyright (C) 2009 Red Hat, Inc.

 *

 * Creation is done via kthreadd, so that we get a clean environment

 * even if we're invoked from userspace (think modprobe, hotplug cpu,

 * etc.).

 Information passed to kthread() from kthreadd. */

 Result passed back to kthread_create() from kthreadd. */

/*

 * Variant of to_kthread() that doesn't assume @p is a kthread.

 *

 * Per construction; when:

 *

 *   (p->flags & PF_KTHREAD) && p->set_child_tid

 *

 * the task is both a kthread and struct kthread is persistent. However

 * PF_KTHREAD on it's own is not, kernel_thread() can exec() (See umh.c and

 * begin_new_exec()).

	/*

	 * We abuse ->set_child_tid to avoid the new member and because it

	 * can't be wrongly copied by copy_process(). We also rely on fact

	 * that the caller can't exec, so PF_KTHREAD can't be cleared.

	/*

	 * Can be NULL if this kthread was created by kernel_thread()

	 * or if kmalloc() in kthread() failed.

/**

 * kthread_should_stop - should this kthread return now?

 *

 * When someone calls kthread_stop() on your kthread, it will be woken

 * and this will return true.  You should then return, and your return

 * value will be passed through to kthread_stop().

/**

 * kthread_should_park - should this kthread park now?

 *

 * When someone calls kthread_park() on your kthread, it will be woken

 * and this will return true.  You should then do the necessary

 * cleanup and call kthread_parkme()

 *

 * Similar to kthread_should_stop(), but this keeps the thread alive

 * and in a park position. kthread_unpark() "restarts" the thread and

 * calls the thread function again.

/**

 * kthread_freezable_should_stop - should this freezable kthread return now?

 * @was_frozen: optional out parameter, indicates whether %current was frozen

 *

 * kthread_should_stop() for freezable kthreads, which will enter

 * refrigerator if necessary.  This function is safe from kthread_stop() /

 * freezer deadlock and freezable kthreads should use this function instead

 * of calling try_to_freeze() directly.

/**

 * kthread_func - return the function specified on kthread creation

 * @task: kthread task in question

 *

 * Returns NULL if the task is not a kthread.

/**

 * kthread_data - return data value specified on kthread creation

 * @task: kthread task in question

 *

 * Return the data value specified when kthread @task was created.

 * The caller is responsible for ensuring the validity of @task when

 * calling this function.

/**

 * kthread_probe_data - speculative version of kthread_data()

 * @task: possible kthread task in question

 *

 * @task could be a kthread task.  Return the data value specified when it

 * was created if accessible.  If @task isn't a kthread task or its data is

 * inaccessible for any reason, %NULL is returned.  This function requires

 * that @task itself is safe to dereference.

		/*

		 * TASK_PARKED is a special state; we must serialize against

		 * possible pending wakeups to avoid store-store collisions on

		 * task->state.

		 *

		 * Such a collision might possibly result in the task state

		 * changin from TASK_PARKED and us failing the

		 * wait_task_inactive() in kthread_park().

		/*

		 * Thread is going to call schedule(), do not preempt it,

		 * or the caller of kthread_park() may spend more time in

		 * wait_task_inactive().

 Copy data: it's on kthread's stack */

 If user was SIGKILLed, I release the structure. */

	/*

	 * The new thread inherited kthreadd's priority and CPU mask. Reset

	 * back to default in case they have been changed.

 OK, tell user we're spawned, wait for stop or wakeup */

	/*

	 * Thread is going to call schedule(), do not preempt it,

	 * or the creator may spend more time in wait_task_inactive().

 called from kernel_clone() to get node information for about to be created task */

 We want our own signal handler (we take no signals by default). */

 If user was SIGKILLed, I release the structure. */

	/*

	 * Wait for completion in killable state, for I might be chosen by

	 * the OOM killer while kthreadd is trying to allocate memory for

	 * new kernel thread.

		/*

		 * If I was SIGKILLed before kthreadd (or new kernel thread)

		 * calls complete(), leave the cleanup of this structure to

		 * that thread.

		/*

		 * kthreadd (or new kernel thread) will call complete()

		 * shortly.

		/*

		 * task is already visible to other tasks, so updating

		 * COMM must be protected.

/**

 * kthread_create_on_node - create a kthread.

 * @threadfn: the function to run until signal_pending(current).

 * @data: data ptr for @threadfn.

 * @node: task and thread structures for the thread are allocated on this node

 * @namefmt: printf-style name for the thread.

 *

 * Description: This helper function creates and names a kernel

 * thread.  The thread will be stopped: use wake_up_process() to start

 * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and

 * is affine to all CPUs.

 *

 * If thread is going to be bound on a particular cpu, give its node

 * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.

 * When woken, the thread will run @threadfn() with @data as its

 * argument. @threadfn() can either return directly if it is a

 * standalone thread for which no one will call kthread_stop(), or

 * return when 'kthread_should_stop()' is true (which means

 * kthread_stop() has been called).  The return value should be zero

 * or a negative error number; it will be passed to kthread_stop().

 *

 * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).

 It's safe because the task is inactive. */

/**

 * kthread_bind - bind a just-created kthread to a cpu.

 * @p: thread created by kthread_create().

 * @cpu: cpu (might not be online, must be possible) for @k to run on.

 *

 * Description: This function is equivalent to set_cpus_allowed(),

 * except that @cpu doesn't need to be online, and the thread must be

 * stopped (i.e., just returned from kthread_create()).

/**

 * kthread_create_on_cpu - Create a cpu bound kthread

 * @threadfn: the function to run until signal_pending(current).

 * @data: data ptr for @threadfn.

 * @cpu: The cpu on which the thread should be bound,

 * @namefmt: printf-style name for the thread. Format is restricted

 *	     to "name.*%u". Code fills in cpu number.

 *

 * Description: This helper function creates and names a kernel thread

 CPU hotplug need to bind once again when unparking the thread. */

/**

 * kthread_unpark - unpark a thread created by kthread_create().

 * @k:		thread created by kthread_create().

 *

 * Sets kthread_should_park() for @k to return false, wakes it, and

 * waits for it to return. If the thread is marked percpu then its

 * bound to the cpu again.

	/*

	 * Newly created kthread was parked when the CPU was offline.

	 * The binding was lost and we need to set it again.

	/*

	 * __kthread_parkme() will either see !SHOULD_PARK or get the wakeup.

/**

 * kthread_park - park a thread created by kthread_create().

 * @k: thread created by kthread_create().

 *

 * Sets kthread_should_park() for @k to return true, wakes it, and

 * waits for it to return. This can also be called after kthread_create()

 * instead of calling wake_up_process(): the thread will park without

 * calling threadfn().

 *

 * Returns 0 if the thread is parked, -ENOSYS if the thread exited.

 * If called by the kthread itself just the park bit is set.

		/*

		 * Wait for __kthread_parkme() to complete(), this means we

		 * _will_ have TASK_PARKED and are about to call schedule().

		/*

		 * Now wait for that schedule() to complete and the task to

		 * get scheduled out.

/**

 * kthread_stop - stop a thread created by kthread_create().

 * @k: thread created by kthread_create().

 *

 * Sets kthread_should_stop() for @k to return true, wakes it, and

 * waits for it to exit. This can also be called after kthread_create()

 * instead of calling wake_up_process(): the thread will exit without

 * calling threadfn().

 *

 * If threadfn() may call do_exit() itself, the caller must ensure

 * task_struct can't go away.

 *

 * Returns the result of threadfn(), or %-EINTR if wake_up_process()

 * was never called.

 Setup a clean context for our children to inherit. */

/**

 * kthread_worker_fn - kthread function to process kthread_worker

 * @worker_ptr: pointer to initialized kthread_worker

 *

 * This function implements the main cycle of kthread worker. It processes

 * work_list until it is stopped with kthread_stop(). It sleeps when the queue

 * is empty.

 *

 * The works are not allowed to keep any locks, disable preemption or interrupts

 * when they finish. There is defined a safe point for freezing when one work

 * finishes and before a new one is started.

 *

 * Also the works must not be handled by more than one worker at the same time,

 * see also kthread_queue_work().

	/*

	 * FIXME: Update the check and remove the assignment when all kthread

	 * worker users are created using kthread_create_worker*() functions.

 mb paired w/ kthread_stop */

		/*

		 * Avoid dereferencing work after this point.  The trace

		 * event only cares about the address.

/**

 * kthread_create_worker - create a kthread worker

 * @flags: flags modifying the default behavior of the worker

 * @namefmt: printf-style name for the kthread worker (task).

 *

 * Returns a pointer to the allocated worker on success, ERR_PTR(-ENOMEM)

 * when the needed structures could not get allocated, and ERR_PTR(-EINTR)

 * when the worker was SIGKILLed.

/**

 * kthread_create_worker_on_cpu - create a kthread worker and bind it

 *	to a given CPU and the associated NUMA node.

 * @cpu: CPU number

 * @flags: flags modifying the default behavior of the worker

 * @namefmt: printf-style name for the kthread worker (task).

 *

 * Use a valid CPU number if you want to bind the kthread worker

 * to the given CPU and the associated NUMA node.

 *

 * A good practice is to add the cpu number also into the worker name.

 * For example, use kthread_create_worker_on_cpu(cpu, "helper/%d", cpu).

 *

 * CPU hotplug:

 * The kthread worker API is simple and generic. It just provides a way

 * to create, use, and destroy workers.

 *

 * It is up to the API user how to handle CPU hotplug. They have to decide

 * how to handle pending work items, prevent queuing new ones, and

 * restore the functionality when the CPU goes off and on. There are a

 * few catches:

 *

 *    - CPU affinity gets lost when it is scheduled on an offline CPU.

 *

 *    - The worker might not exist when the CPU was off when the user

 *      created the workers.

 *

 * Good practice is to implement two CPU hotplug callbacks and to

 * destroy/create the worker when the CPU goes down/up.

 *

 * Return:

 * The pointer to the allocated worker on success, ERR_PTR(-ENOMEM)

 * when the needed structures could not get allocated, and ERR_PTR(-EINTR)

 * when the worker was SIGKILLed.

/*

 * Returns true when the work could not be queued at the moment.

 * It happens when it is already pending in a worker list

 * or when it is being cancelled.

 Do not use a work with >1 worker, see kthread_queue_work() */

 insert @work before @pos in @worker */

/**

 * kthread_queue_work - queue a kthread_work

 * @worker: target kthread_worker

 * @work: kthread_work to queue

 *

 * Queue @work to work processor @task for async execution.  @task

 * must have been created with kthread_worker_create().  Returns %true

 * if @work was successfully queued, %false if it was already pending.

 *

 * Reinitialize the work if it needs to be used by another worker.

 * For example, when the worker was stopped and started again.

/**

 * kthread_delayed_work_timer_fn - callback that queues the associated kthread

 *	delayed work when the timer expires.

 * @t: pointer to the expired timer

 *

 * The format of the function is defined by struct timer_list.

 * It should have been called from irqsafe timer with irq already off.

	/*

	 * This might happen when a pending work is reinitialized.

	 * It means that it is used a wrong way.

 Work must not be used with >1 worker, see kthread_queue_work(). */

 Move the work from worker->delayed_work_list. */

	/*

	 * If @delay is 0, queue @dwork->work immediately.  This is for

	 * both optimization and correctness.  The earliest @timer can

	 * expire is on the closest next tick and delayed_work users depend

	 * on that there's no such delay when @delay is 0.

 Be paranoid and try to detect possible races already now. */

/**

 * kthread_queue_delayed_work - queue the associated kthread work

 *	after a delay.

 * @worker: target kthread_worker

 * @dwork: kthread_delayed_work to queue

 * @delay: number of jiffies to wait before queuing

 *

 * If the work has not been pending it starts a timer that will queue

 * the work after the given @delay. If @delay is zero, it queues the

 * work immediately.

 *

 * Return: %false if the @work has already been pending. It means that

 * either the timer was running or the work was queued. It returns %true

 * otherwise.

/**

 * kthread_flush_work - flush a kthread_work

 * @work: work to flush

 *

 * If @work is queued or executing, wait for it to finish execution.

 Work must not be used with >1 worker, see kthread_queue_work(). */

/*

 * Make sure that the timer is neither set nor running and could

 * not manipulate the work list_head any longer.

 *

 * The function is called under worker->lock. The lock is temporary

 * released but the timer can't be set again in the meantime.

	/*

	 * del_timer_sync() must be called to make sure that the timer

	 * callback is not running. The lock must be temporary released

	 * to avoid a deadlock with the callback. In the meantime,

	 * any queuing is blocked by setting the canceling counter.

/*

 * This function removes the work from the worker queue.

 *

 * It is called under worker->lock. The caller must make sure that

 * the timer used by delayed work is not running, e.g. by calling

 * kthread_cancel_delayed_work_timer().

 *

 * The work might still be in use when this function finishes. See the

 * current_work proceed by the worker.

 *

 * Return: %true if @work was pending and successfully canceled,

 *	%false if @work was not pending

	/*

	 * Try to remove the work from a worker list. It might either

	 * be from worker->work_list or from worker->delayed_work_list.

/**

 * kthread_mod_delayed_work - modify delay of or queue a kthread delayed work

 * @worker: kthread worker to use

 * @dwork: kthread delayed work to queue

 * @delay: number of jiffies to wait before queuing

 *

 * If @dwork is idle, equivalent to kthread_queue_delayed_work(). Otherwise,

 * modify @dwork's timer so that it expires after @delay. If @delay is zero,

 * @work is guaranteed to be queued immediately.

 *

 * Return: %false if @dwork was idle and queued, %true otherwise.

 *

 * A special case is when the work is being canceled in parallel.

 * It might be caused either by the real kthread_cancel_delayed_work_sync()

 * or yet another kthread_mod_delayed_work() call. We let the other command

 * win and return %true here. The return value can be used for reference

 * counting and the number of queued works stays the same. Anyway, the caller

 * is supposed to synchronize these operations a reasonable way.

 *

 * This function is safe to call from any context including IRQ handler.

 * See __kthread_cancel_work() and kthread_delayed_work_timer_fn()

 * for details.

 Do not bother with canceling when never queued. */

 Work must not be used with >1 worker, see kthread_queue_work() */

	/*

	 * Temporary cancel the work but do not fight with another command

	 * that is canceling the work as well.

	 *

	 * It is a bit tricky because of possible races with another

	 * mod_delayed_work() and cancel_delayed_work() callers.

	 *

	 * The timer must be canceled first because worker->lock is released

	 * when doing so. But the work can be removed from the queue (list)

	 * only when it can be queued again so that the return value can

	 * be used for reference counting.

 The number of works in the queue does not change. */

 Work must not be used with >1 worker, see kthread_queue_work(). */

	/*

	 * The work is in progress and we need to wait with the lock released.

	 * In the meantime, block any queuing by setting the canceling counter.

/**

 * kthread_cancel_work_sync - cancel a kthread work and wait for it to finish

 * @work: the kthread work to cancel

 *

 * Cancel @work and wait for its execution to finish.  This function

 * can be used even if the work re-queues itself. On return from this

 * function, @work is guaranteed to be not pending or executing on any CPU.

 *

 * kthread_cancel_work_sync(&delayed_work->work) must not be used for

 * delayed_work's. Use kthread_cancel_delayed_work_sync() instead.

 *

 * The caller must ensure that the worker on which @work was last

 * queued can't be destroyed before this function returns.

 *

 * Return: %true if @work was pending, %false otherwise.

/**

 * kthread_cancel_delayed_work_sync - cancel a kthread delayed work and

 *	wait for it to finish.

 * @dwork: the kthread delayed work to cancel

 *

 * This is kthread_cancel_work_sync() for delayed works.

 *

 * Return: %true if @dwork was pending, %false otherwise.

/**

 * kthread_flush_worker - flush all current works on a kthread_worker

 * @worker: worker to flush

 *

 * Wait until all currently executing or pending works on @worker are

 * finished.

/**

 * kthread_destroy_worker - destroy a kthread worker

 * @worker: worker to be destroyed

 *

 * Flush and destroy @worker.  The simple flush is enough because the kthread

 * worker API is used only in trivial scenarios.  There are no multi-step state

 * machines needed.

/**

 * kthread_use_mm - make the calling kthread operate on an address space

 * @mm: address space to operate on

 Hold off tlb flush IPIs while switching mm's */

	/*

	 * When a kthread starts operating on an address space, the loop

	 * in membarrier_{private,global}_expedited() may not observe

	 * that tsk->mm, and not issue an IPI. Membarrier requires a

	 * memory barrier after storing to tsk->mm, before accessing

	 * user-space memory. A full memory barrier for membarrier

	 * {PRIVATE,GLOBAL}_EXPEDITED is implicitly provided by

	 * mmdrop(), or explicitly with smp_mb().

/**

 * kthread_unuse_mm - reverse the effect of kthread_use_mm()

 * @mm: address space to operate on

	/*

	 * When a kthread stops operating on an address space, the loop

	 * in membarrier_{private,global}_expedited() may not observe

	 * that tsk->mm, and not issue an IPI. Membarrier requires a

	 * memory barrier after accessing user-space memory, before

	 * clearing tsk->mm.

 active_mm is still 'mm' */

/**

 * kthread_associate_blkcg - associate blkcg to current kthread

 * @css: the cgroup info

 *

 * Current thread must be a kthread. The thread is running jobs on behalf of

 * other threads. In some cases, we expect the jobs attach cgroup info of

 * original threads instead of that of current thread. This function stores

 * original thread's cgroup info in current kthread context for later

 * retrieval.

/**

 * kthread_blkcg - get associated blkcg css of current kthread

 *

 * Current thread must be a kthread.

/*

 * kmod - the kernel module loader

/*

 * Assuming:

 *

 * threads = div64_u64((u64) totalram_pages * (u64) PAGE_SIZE,

 *		       (u64) THREAD_SIZE * 8UL);

 *

 * If you need less than 50 threads would mean we're dealing with systems

 * smaller than 3200 pages. This assumes you are capable of having ~13M memory,

 * and this would only be an upper limit, after which the OOM killer would take

 * effect. Systems like these are very unlikely if modules are enabled.

/*

 * This is a restriction on having *all* MAX_KMOD_CONCURRENT threads

 * running at the same time without returning. When this happens we

 * believe you've somehow ended up with a recursive module dependency

 * creating a loop.

 *

 * We have no option but to fail.

 *

 * Userspace should proactively try to detect and prevent these.

/*

	modprobe_path is set via /proc/sys.

 check call_modprobe() */

 check free_modprobe_argv() */

/**

 * __request_module - try to load a kernel module

 * @wait: wait (or not) for the operation to complete

 * @fmt: printf style format string for the name of the module

 * @...: arguments as specified in the format string

 *

 * Load a module using the user mode module loader. The function returns

 * zero on success or a negative errno code or positive exit code from

 * "modprobe" on failure. Note that a successful module load does not mean

 * the module did not then unload and exit on an error of its own. Callers

 * must check that the service they requested is now available not blindly

 * invoke it.

 *

 * If module auto-loading support is disabled then this function

 * simply returns -ENOENT.

	/*

	 * We don't allow synchronous module loading from async.  Module

	 * init may invoke async_synchronize_full() which will end up

	 * waiting for this task which already is waiting for the module

	 * loading to complete, leading to a deadlock.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2004 IBM Corporation

 *

 *  Author: Serge Hallyn <serue@us.ibm.com>

/*

 * Clone a new ns copying an original utsname, setting refcount to 1

 * @old_ns: namespace to clone

 * Return ERR_PTR(-ENOMEM) on error (failure to allocate), new ns otherwise

/*

 * Copy task tsk's utsname namespace, or clone it if flags

 * specifies CLONE_NEWUTS.  In latter case, changes to the

 * utsname of this process won't be seen by parent, and vice

 * versa.

 SPDX-License-Identifier: GPL-2.0

/*

 * Detect hard lockups on a system

 *

 * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.

 *

 * Note: Most of this code is borrowed heavily from the original softlockup

 * detector, so thanks to Ingo for the initial implementation.

 * Some chunks also taken from the old x86-specific nmi watchdog code, thanks

 * to those contributors as well.

	/*

	 * Using __raw here because some code paths have

	 * preemption enabled.  If preemption is enabled

	 * then interrupts should be enabled too, in which

	 * case we shouldn't have to worry about the watchdog

	 * going off.

	/*

	 * The hrtimer runs with a period of (watchdog_threshold * 2) / 5

	 *

	 * So it runs effectively with 2.5 times the rate of the NMI

	 * watchdog. That means the hrtimer should fire 2-3 times before

	 * the NMI watchdog expires. The NMI watchdog on x86 is based on

	 * unhalted CPU cycles, so if Turbo-Mode is enabled the CPU cycles

	 * might run way faster than expected and the NMI fires in a

	 * smaller period than the one deduced from the nominal CPU

	 * frequency. Depending on the Turbo-Mode factor this might be fast

	 * enough to get the NMI period smaller than the hrtimer watchdog

	 * period and trigger false positives.

	 *

	 * The sample threshold is used to check in the NMI handler whether

	 * the minimum time between two NMI samples has elapsed. That

	 * prevents false positives.

	 *

	 * Set this to 4/5 of the actual watchdog threshold period so the

	 * hrtimer is guaranteed to fire at least once within the real

	 * watchdog threshold.

		/*

		 * If ktime is jiffies based, a stalled timer would prevent

		 * jiffies from being incremented and the filter would look

		 * at a stale timestamp and never trigger.

 Callback function for perf event subsystem */

 Ensure the watchdog never gets throttled */

	/* check for a hardlockup

	 * This is done by making sure our timer interrupt

	 * is incrementing.  The timer interrupt should have

	 * fired multiple times before we overflow'd.  If it hasn't

	 * then this is a good indication the cpu is stuck

 only print hardlockups once */

		/*

		 * Perform all-CPU dump only once to avoid multiple hardlockups

		 * generating interleaving traces

 Try to register using hardware perf events */

/**

 * hardlockup_detector_perf_enable - Enable the local event

 use original value for check */

/**

 * hardlockup_detector_perf_disable - Disable the local event

/**

 * hardlockup_detector_perf_cleanup - Cleanup disabled events and destroy them

 *

 * Called from lockup_detector_cleanup(). Serialized by the caller.

		/*

		 * Required because for_each_cpu() reports  unconditionally

		 * CPU0 as set on UP kernels. Sigh.

/**

 * hardlockup_detector_perf_stop - Globally stop watchdog events

 *

 * Special interface for x86 to handle the perf HT bug.

/**

 * hardlockup_detector_perf_restart - Globally restart watchdog events

 *

 * Special interface for x86 to handle the perf HT bug.

/**

 * hardlockup_detector_perf_init - Probe whether NMI event is available at all

 SPDX-License-Identifier: GPL-2.0-or-later

/* Helpers for initial module or kernel cmdline parsing

   Copyright (C) 2001 Rusty Russell.



 Protects all built-in parameters, modules use their own param_lock */

 Use the module's mutex, or if built-in use the built-in mutex */

 !CONFIG_SYSFS */

 This just allows us to keep track of which parameters are kmalloced. */

 Does nothing if parameter wasn't kmalloced above. */

 Find parameter */

 No one handled NULL, so do it here. */

 Args looks like "foo=bar,bar2 baz=fuz wiz". */

 Chew leading spaces */

 Stop at -- */

 Lazy bastard, eh? */

	/* This is a hack.  We can't kmalloc in early boot, and we

 Actually could be a bool or an int, for historical reasons. */

 No equals means "set"... */

 One of =[yYnN01] */

 Y and N chosen as being relatively non-coder friendly */

 Don't let them unset it once it's set! */

 This one must be bool. */

 Match bool exactly, by re-using it. */

 We break the rule and mangle the string. */

 Get the name right for errors. */

 We expect a comma-separated list of values. */

 nul-terminate and parse */

 Replace \n with comma */

 sysfs output in /sys/modules/XYZ/parameters/ */

 sysfs always hands a nul-terminated string in buf.  We rely on that. */

/*

 * add_sysfs_param - add a parameter to sysfs

 * @mk: struct module_kobject

 * @kp: the actual parameter definition to add to sysfs

 * @name: name of parameter

 *

 * Create a kobject if for a (per-module) parameter if mp NULL, and

 * create file in sysfs.  Returns an error on out of memory.  Always cleans up

 * if there's an error.

 We don't bother calling this with invisible parameters. */

 First allocation. */

 NULL-terminated attribute array. */

 Caller will cleanup via free_module_param_attrs */

 Enlarge allocations. */

 Extra pointer for NULL terminator */

 Tack new one on the end. */

 Do not allow runtime DAC changes to make param writable. */

 Fix up all the pointers, since krealloc can move us */

/*

 * module_param_sysfs_setup - setup sysfs support for one module

 * @mod: module

 * @kparam: module parameters (array)

 * @num_params: number of module parameters

 *

 * Adds sysfs entries for module parameters under

 * /sys/module/[mod->name]/parameters/

 Create the param group. */

/*

 * module_param_sysfs_remove - remove sysfs support for one module

 * @mod: module

 *

 * Remove sysfs entries for module parameters and the corresponding

 * kobject.

		/* We are positive that no one is using any param

 So that we hold reference in both cases. */

 We need to remove old parameters before adding more. */

 These should not fail at boot. */

/*

 * param_sysfs_builtin - add sysfs parameters for built-in modules

 *

 * Add module_parameters to sysfs for "modules" built into the kernel.

 *

 * The "module" name (KBUILD_MODNAME) is stored before a dot, the

 * "parameter" name is stored behind a dot in kernel_param->name. So,

 * extract the "module" name for all built-in kernel_param-eters,

 * and for all who have the same, call kernel_add_sysfs_param.

 This happens for core_param() */

 module-related sysfs stuff */

/*

 * param_sysfs_init - wrapper for built-in params support

 CONFIG_SYSFS */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Restartable sequences system call

 *

 * Copyright (C) 2015, Google, Inc.,

 * Paul Turner <pjt@google.com> and Andrew Hunter <ahh@google.com>

 * Copyright (C) 2015-2018, EfficiOS Inc.,

 * Mathieu Desnoyers <mathieu.desnoyers@efficios.com>

/*

 *

 * Restartable sequences are a lightweight interface that allows

 * user-level code to be executed atomically relative to scheduler

 * preemption and signal delivery. Typically used for implementing

 * per-cpu operations.

 *

 * It allows user-space to perform update operations on per-cpu data

 * without requiring heavy-weight atomic operations.

 *

 * Detailed algorithm of rseq user-space assembly sequences:

 *

 *                     init(rseq_cs)

 *                     cpu = TLS->rseq::cpu_id_start

 *   [1]               TLS->rseq::rseq_cs = rseq_cs

 *   [start_ip]        ----------------------------

 *   [2]               if (cpu != TLS->rseq::cpu_id)

 *                             goto abort_ip;

 *   [3]               <last_instruction_in_cs>

 *   [post_commit_ip]  ----------------------------

 *

 *   The address of jump target abort_ip must be outside the critical

 *   region, i.e.:

 *

 *     [abort_ip] < [start_ip]  || [abort_ip] >= [post_commit_ip]

 *

 *   Steps [2]-[3] (inclusive) need to be a sequence of instructions in

 *   userspace that can handle being interrupted between any of those

 *   instructions, and then resumed to the abort_ip.

 *

 *   1.  Userspace stores the address of the struct rseq_cs assembly

 *       block descriptor into the rseq_cs field of the registered

 *       struct rseq TLS area. This update is performed through a single

 *       store within the inline assembly instruction sequence.

 *       [start_ip]

 *

 *   2.  Userspace tests to check whether the current cpu_id field match

 *       the cpu number loaded before start_ip, branching to abort_ip

 *       in case of a mismatch.

 *

 *       If the sequence is preempted or interrupted by a signal

 *       at or after start_ip and before post_commit_ip, then the kernel

 *       clears TLS->__rseq_abi::rseq_cs, and sets the user-space return

 *       ip to abort_ip before returning to user-space, so the preempted

 *       execution resumes at abort_ip.

 *

 *   3.  Userspace critical section final instruction before

 *       post_commit_ip is the commit. The critical section is

 *       self-terminating.

 *       [post_commit_ip]

 *

 *   4.  <success>

 *

 *   On failure at [2], or if interrupted by preempt or signal delivery

 *   between [1] and [3]:

 *

 *       [abort_ip]

 *   F1. <failure>

	/*

	 * Reset cpu_id_start to its initial state (0).

	/*

	 * Reset cpu_id to RSEQ_CPU_ID_UNINITIALIZED, so any user coming

	 * in after unregistration can figure out that rseq needs to be

	 * registered again.

 Check for overflow. */

 Ensure that abort_ip is not in the critical section. */

 Get thread flags. */

 Take critical section flags into account. */

	/*

	 * Restart on signal can only be inhibited when restart on

	 * preempt and restart on migrate are inhibited too. Otherwise,

	 * a preempted signal handler could fail to restart the prior

	 * execution context on sigreturn.

	/*

	 * Load and clear event mask atomically with respect to

	 * scheduler preemption.

	/*

	 * The rseq_cs field is set to NULL on preemption or signal

	 * delivery on top of rseq assembly block, as well as on top

	 * of code outside of the rseq assembly block. This performs

	 * a lazy clear of the rseq_cs field.

	 *

	 * Set rseq_cs to NULL.

/*

 * Unsigned comparison will be true when ip >= start_ip, and when

 * ip < start_ip + post_commit_offset.

	/*

	 * Handle potentially not being within a critical section.

	 * If not nested over a rseq critical section, restart is useless.

	 * Clear the rseq_cs pointer and return.

/*

 * This resume handler must always be executed between any of:

 * - preemption,

 * - signal delivery,

 * and return to user-space.

 *

 * This is how we can ensure that the entire rseq critical section

 * will issue the commit instruction only if executed atomically with

 * respect to other threads scheduled on the same CPU, and with respect

 * to signal handlers.

	/*

	 * regs is NULL if and only if the caller is in a syscall path.  Skip

	 * fixup and leave rseq_cs as is so that rseq_sycall() will detect and

	 * kill a misbehaving userspace on debug kernels.

/*

 * Terminate the process if a syscall is issued within a restartable

 * sequence.

/*

 * sys_rseq - setup restartable sequences for caller thread.

 Unregister rseq for current thread. */

		/*

		 * If rseq is already registered, check whether

		 * the provided address differs from the prior

		 * one.

 Already registered. */

	/*

	 * If there was no rseq previously registered,

	 * ensure the provided rseq is properly aligned and valid.

	/*

	 * If rseq was previously inactive, and has just been

	 * registered, ensure the cpu_id_start and cpu_id fields

	 * are updated before returning to user-space.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	linux/kernel/resource.c

 *

 * Copyright (C) 1999	Linus Torvalds

 * Copyright (C) 1999	Martin Mares <mj@ucw.cz>

 *

 * Arbitrary resource management.

 constraints to be met while allocating resources */

/*

 * For memory hotplug, there is no way to free resource entries allocated

 * by boot mem after the system is up. So for reusing the resource entry

 * we need to remember the resource.

 CONFIG_PROC_FS */

 Return the conflict entry if you can't request it */

 need to restore size, and keep flags */

/**

 * request_resource_conflict - request and reserve an I/O or memory resource

 * @root: root resource descriptor

 * @new: resource descriptor desired by caller

 *

 * Returns 0 for success, conflict resource on error.

/**

 * request_resource - request and reserve an I/O or memory resource

 * @root: root resource descriptor

 * @new: resource descriptor desired by caller

 *

 * Returns 0 for success, negative error code on error.

/**

 * release_resource - release a previously reserved resource

 * @old: resource pointer

/**

 * find_next_iomem_res - Finds the lowest iomem resource that covers part of

 *			 [@start..@end].

 *

 * If a resource is found, returns 0 and @*res is overwritten with the part

 * of the resource that's within [@start..@end]; if none is found, returns

 * -ENODEV.  Returns -EINVAL for invalid parameters.

 *

 * @start:	start address of the resource searched for

 * @end:	end address of same resource

 * @flags:	flags which the resource must have

 * @desc:	descriptor the resource must have

 * @res:	return ptr, if resource found

 *

 * The caller must specify @start, @end, @flags, and @desc

 * (which may be IORES_DESC_NONE).

 If we passed the resource we are looking for, stop */

 Skip until we find a range that matches what we look for */

 Found a match, break */

 copy data */

/**

 * walk_iomem_res_desc - Walks through iomem resources and calls func()

 *			 with matching resource ranges.

 * *

 * @desc: I/O resource descriptor. Use IORES_DESC_NONE to skip @desc check.

 * @flags: I/O resource flags

 * @start: start addr

 * @end: end addr

 * @arg: function argument for the callback @func

 * @func: callback function that is called for each qualifying resource area

 *

 * All the memory ranges which overlap start,end and also match flags and

 * desc are valid candidates.

 *

 * NOTE: For a new descriptor search, define a new IORES_DESC in

 * <linux/ioport.h> and set it in 'desc' of a target resource entry.

/*

 * This function calls the @func callback against all memory ranges of type

 * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.

 * Now, this function is only for System RAM, it deals with full ranges and

 * not PFNs. If resources are not PFN-aligned, dealing with PFNs can truncate

 * ranges.

/*

 * This function calls the @func callback against all memory ranges, which

 * are ranges marked as IORESOURCE_MEM and IORESOUCE_BUSY.

/*

 * This function calls the @func callback against all memory ranges of type

 * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.

 * It is to be used only for System RAM.

/*

 * This generic page_is_ram() returns true if specified address is

 * registered as System RAM in iomem_resource list.

/**

 * region_intersects() - determine intersection of region with known resources

 * @start: region start address

 * @size: size of region

 * @flags: flags of resource (in iomem_resource)

 * @desc: descriptor of resource (in iomem_resource) or IORES_DESC_NONE

 *

 * Check if the specified region partially overlaps or fully eclipses a

 * resource identified by @flags and @desc (optional with IORES_DESC_NONE).

 * Return REGION_DISJOINT if the region does not overlap @flags/@desc,

 * return REGION_MIXED if the region overlaps @flags/@desc and another

 * resource, and return REGION_INTERSECTS if the region overlaps @flags/@desc

 * and no other defined resource. Note that REGION_INTERSECTS is also

 * returned in the case when the specified region overlaps RAM and undefined

 * memory holes.

 *

 * region_intersect() is used by memory remapping functions to ensure

 * the user is not remapping RAM and is a vast speed up over walking

 * through the resource table page by page.

/*

 * Find empty slot in the resource tree with the given range and

 * alignment constraints

	/*

	 * Skip past an allocated resource that starts at 0, since the assignment

	 * of this->start - 1 to tmp->end below would cause an underflow.

 Check for overflow after ALIGN() */

/*

 * Find empty slot in the resource tree given range and alignment.

/**

 * reallocate_resource - allocate a slot in the resource tree given range & alignment.

 *	The resource will be relocated if the new size cannot be reallocated in the

 *	current location.

 *

 * @root: root resource descriptor

 * @old:  resource descriptor desired by caller

 * @newsize: new size of the resource descriptor

 * @constraint: the size and alignment constraints to be met.

/**

 * allocate_resource - allocate empty slot in the resource tree given range & alignment.

 * 	The resource will be reallocated with a new size if it was already allocated

 * @root: root resource descriptor

 * @new: resource descriptor desired by caller

 * @size: requested resource region size

 * @min: minimum boundary to allocate

 * @max: maximum boundary to allocate

 * @align: alignment requested, in bytes

 * @alignf: alignment function, optional, called if not NULL

 * @alignf_data: arbitrary data to pass to the @alignf function

		/* resource is already allocated, try reallocating with

/**

 * lookup_resource - find an existing resource by a resource start address

 * @root: root resource descriptor

 * @start: resource start address

 *

 * Returns a pointer to the resource if found, NULL otherwise

/*

 * Insert a resource into the resource tree. If successful, return NULL,

 * otherwise return the conflicting resource (compare to __request_resource())

 duplicated insertion */

 Partial overlap? Bad, and unfixable */

/**

 * insert_resource_conflict - Inserts resource in the resource tree

 * @parent: parent of the new resource

 * @new: new resource to insert

 *

 * Returns 0 on success, conflict resource if the resource can't be inserted.

 *

 * This function is equivalent to request_resource_conflict when no conflict

 * happens. If a conflict happens, and the conflicting resources

 * entirely fit within the range of the new resource, then the new

 * resource is inserted and the conflicting resources become children of

 * the new resource.

 *

 * This function is intended for producers of resources, such as FW modules

 * and bus drivers.

/**

 * insert_resource - Inserts a resource in the resource tree

 * @parent: parent of the new resource

 * @new: new resource to insert

 *

 * Returns 0 on success, -EBUSY if the resource can't be inserted.

 *

 * This function is intended for producers of resources, such as FW modules

 * and bus drivers.

/**

 * insert_resource_expand_to_fit - Insert a resource into the resource tree

 * @root: root resource descriptor

 * @new: new resource to insert

 *

 * Insert a resource into the resource tree, possibly expanding it in order

 * to make it encompass any conflicting resources.

 Ok, expand resource to cover the conflict, then try again .. */

/**

 * remove_resource - Remove a resource in the resource tree

 * @old: resource to remove

 *

 * Returns 0 on success, -EINVAL if the resource is not valid.

 *

 * This function removes a resource previously inserted by insert_resource()

 * or insert_resource_conflict(), and moves the children (if any) up to

 * where they were before.  insert_resource() and insert_resource_conflict()

 * insert a new resource, and move any conflicting resources down to the

 * children of the new resource.

 *

 * insert_resource(), insert_resource_conflict() and remove_resource() are

 * intended for producers of resources, such as FW modules and bus drivers.

/**

 * adjust_resource - modify a resource's start and size

 * @res: resource to modify

 * @start: new start value

 * @size: new size

 *

 * Given an existing resource, change its start and size to match the

 * arguments.  Returns 0 on success, -EBUSY if it can't fit.

 * Existing children of the resource are assumed to be immutable.

 conflict covered whole area */

 failed, split and try again */

/**

 * resource_alignment - calculate resource's alignment

 * @res: resource pointer

 *

 * Returns alignment on success, 0 (invalid alignment) on failure.

/*

 * This is compatibility stuff for IO resources.

 *

 * Note how this, unlike the above, knows about

 * the IO flag meanings (busy etc).

 *

 * request_region creates a new busy region.

 *

 * release_region releases a matching busy region.

 pairs with smp_store_release() in iomem_init_inode() */

	/*

	 * Check that the initialization has completed. Losing the race

	 * is ok because it means drivers are claiming resources before

	 * the fs_initcall level of init and prevent iomem_get_mapping users

	 * from establishing mappings.

	/*

	 * The expectation is that the driver has successfully marked

	 * the resource busy by this point, so devmem_is_allowed()

	 * should start returning false, however for performance this

	 * does not iterate the entire resource range.

		/*

		 * *cringe* iomem=relaxed says "go ahead, what's the

		 * worst that can happen?"

	/*

	 * This function is only called from file open paths, hence guaranteed

	 * that fs_initcalls have completed and no need to check for NULL. But

	 * since revoke_iomem can be called before the initcall we still need

	 * the barrier to appease checkers.

		/*

		 * mm/hmm.c reserves physical addresses which then

		 * become unavailable to other users.  Conflicts are

		 * not expected.  Warn to aid debugging if encountered.

 Uhhuh, that didn't work out.. */

/**

 * __request_region - create a new busy resource region

 * @parent: parent resource descriptor

 * @start: resource start address

 * @n: resource region size

 * @name: reserving caller's ID string

 * @flags: IO resource flags

/**

 * __release_region - release a previously reserved resource region

 * @parent: parent resource descriptor

 * @start: resource start address

 * @n: resource region size

 *

 * The described resource region must match a currently busy region.

/**

 * release_mem_region_adjustable - release a previously reserved memory region

 * @start: resource start address

 * @size: resource region size

 *

 * This interface is intended for memory hot-delete.  The requested region

 * is released from a currently busy memory resource.  The requested region

 * must either match exactly or fit into a single busy resource entry.  In

 * the latter case, the remaining resource is adjusted accordingly.

 * Existing children of the busy memory resource must be immutable in the

 * request.

 *

 * Note:

 * - Additional release conditions, such as overlapping region, can be

 *   supported after they are confirmed as valid cases.

 * - When a busy memory resource gets split into two entries, the code

 *   assumes that all children remain in the lower address entry for

 *   simplicity.  Enhance this logic when necessary.

	/*

	 * We free up quite a lot of memory on memory hotunplug (esp., memap),

	 * just before releasing the region. This is highly unlikely to

	 * fail - let's play save and make it never fail as the caller cannot

	 * perform any error handling (e.g., trying to re-add memory will fail

	 * similarly).

 look for the next resource if it does not fit into */

		/*

		 * All memory regions added from memory-hotplug path have the

		 * flag IORESOURCE_SYSTEM_RAM. If the resource does not have

		 * this flag, we know that we are dealing with a resource coming

		 * from HMM/devm. HMM/devm use another mechanism to add/release

		 * a resource. This goes via devm_request_mem_region and

		 * devm_release_mem_region.

		 * HMM/devm take care to release their resources when they want,

		 * so if we are dealing with them, let us just back off here.

 found the target resource; let's adjust accordingly */

 free the whole entry */

 adjust the start */

 adjust the end */

 split into two entries - we need a new resource */

 CONFIG_MEMORY_HOTREMOVE */

 We assume either r1 or r2 is IORESOURCE_SYSRAM_MERGEABLE. */

/**

 * merge_system_ram_resource - mark the System RAM resource mergeable and try to

 *	merge it with adjacent, mergeable resources

 * @res: resource descriptor

 *

 * This interface is intended for memory hotplug, whereby lots of contiguous

 * system ram resources are added (e.g., via add_memory*()) by a driver, and

 * the actual resource boundaries are not of interest (e.g., it might be

 * relevant for DIMMs). Only resources that are marked mergeable, that have the

 * same parent, and that don't have any children are considered. All mergeable

 * resources must be immutable during the request.

 *

 * Note:

 * - The caller has to make sure that no pointers to resources that are

 *   marked mergeable are used anymore after this call - the resource might

 *   be freed and the pointer might be stale!

 * - release_mem_region_adjustable() will split on demand on memory hotunplug

 Try to merge with next item in the list. */

 Try to merge with previous item in the list. */

 CONFIG_MEMORY_HOTPLUG */

/*

 * Managed region resource

/**

 * devm_request_resource() - request and reserve an I/O or memory resource

 * @dev: device for which to request the resource

 * @root: root of the resource tree from which to request the resource

 * @new: descriptor of the resource to request

 *

 * This is a device-managed version of request_resource(). There is usually

 * no need to release resources requested by this function explicitly since

 * that will be taken care of when the device is unbound from its driver.

 * If for some reason the resource needs to be released explicitly, because

 * of ordering issues for example, drivers must call devm_release_resource()

 * rather than the regular release_resource().

 *

 * When a conflict is detected between any existing resources and the newly

 * requested resource, an error message will be printed.

 *

 * Returns 0 on success or a negative error code on failure.

/**

 * devm_release_resource() - release a previously requested resource

 * @dev: device for which to release the resource

 * @new: descriptor of the resource to release

 *

 * Releases a resource previously requested using devm_request_resource().

/*

 * Reserve I/O ports or memory based on "reserve=" kernel parameter.

			/*

			 * If the region starts below 0x10000, we assume it's

			 * I/O port space; otherwise assume it's memory.

/*

 * Check if the requested addr and size spans more than any slot in the

 * iomem resource tree.

		/*

		 * We can probably skip the resources without

		 * IORESOURCE_IO attribute?

		/*

		 * if a resource is "BUSY", it's not a hardware resource

		 * but a driver mapping of such a resource; we don't want

		 * to warn for those; some drivers legitimately map only

		 * partial hardware resources. (example: vesafb)

/*

 * Check if an address is exclusive to the kernel and must not be mapped to

 * user space, for example, via /dev/mem.

 *

 * Returns true if exclusive to the kernel, otherwise returns false.

		/*

		 * IORESOURCE_SYSTEM_RAM resources are exclusive if

		 * IORESOURCE_EXCLUSIVE is set, even if they

		 * are not busy and even if "iomem=relaxed" is set. The

		 * responsible driver dynamically adds/removes system RAM within

		 * such an area and uncontrolled access is dangerous.

		/*

		 * A resource is exclusive if IORESOURCE_EXCLUSIVE is set

		 * or CONFIG_IO_STRICT_DEVMEM is enabled and the

		 * resource is busy.

		/*

		 * A driver is claiming this region so revoke any mappings.

/**

 * devm_request_free_mem_region - find free region for device private memory

 *

 * @dev: device struct to bind the resource to

 * @size: size in bytes of the device memory to add

 * @base: resource tree to look in

 *

 * This function tries to find an empty range of physical address big enough to

 * contain the new resource, so that it can later be hotplugged as ZONE_DEVICE

 * memory, which in turn allocates struct pages.

 CONFIG_DEVICE_PRIVATE */

	/*

	 * Publish iomem revocation inode initialized.

	 * Pairs with smp_load_acquire() in revoke_iomem().

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/kernel/dma.c: A DMA channel allocator. Inspired by linux/kernel/irq.c.

 *

 * Written by Hennus Bergman, 1992.

 *

 * 1994/12/26: Changes by Alex Nash to fix a minor bug in /proc/dma.

 *   In the previous version the reported device could end up being wrong,

 *   if a device requested a DMA channel that was already in use.

 *   [It also happened to remove the sizeof(char *) == sizeof(int)

 *   assumption introduced because of those /proc/dma patches. -- Hennus]

/* A note on resource allocation:

 *

 * All drivers needing DMA channels, should allocate and release them

 * through the public routines `request_dma()' and `free_dma()'.

 *

 * In order to avoid problems, all processes should allocate resources in

 * the same sequence and release them in the reverse order.

 *

 * So, when allocating DMAs and IRQs, first allocate the IRQ, then the DMA.

 * When releasing them, first release the DMA, then release the IRQ.

 * If you don't, you may cause allocation requests to fail unnecessarily.

 * This doesn't really matter now, but it will once we get real semaphores

 * in the kernel.

/*

 *	If our port doesn't define this it has no PC like DMA

/* Channel n is busy iff dma_chan_busy[n].lock != 0.

 * DMA0 used to be reserved for DRAM refresh, but apparently not any more...

 * DMA4 is reserved for cascading.

/**

 * request_dma - request and reserve a system DMA channel

 * @dmanr: DMA channel number

 * @device_id: reserving device ID string, used in /proc/dma

 old flag was 0, now contains 1 to indicate busy */

 request_dma */

/**

 * free_dma - free a reserved system DMA channel

 * @dmanr: DMA channel number

 free_dma */

 MAX_DMA_CHANNELS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/exit.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 for audit_free() */

/*

 * This function expects the tasklist_lock write-locked.

		/*

		 * If there is any task waiting for the group exit

		 * then notify it:

	/*

	 * Accumulate here the counters for all threads as they die. We could

	 * skip the group leader because it is the last user of signal_struct,

	 * but we want to avoid the race with thread_group_cputime() which can

	 * see the empty ->thread_head list.

	/*

	 * Do this under ->siglock, we can race with another thread

	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.

	/* don't need to get the RCU readlock here - the process is dead and

	/*

	 * If we are the last non-leader member of the thread

	 * group, and the leader is zombie, then notify the

	 * group leader's parent process. (if it wants notification.)

		/*

		 * If we were the last child thread and the leader has

		 * exited already, and the leader's parent ignores SIGCHLD,

		 * then we are the one who should release the leader.

	/*

	 * Order condition vs @task, such that everything prior to the load

	 * of @task is visible. This is the condition as to why the user called

	 * rcuwait_wake() in the first place. Pairs with set_current_state()

	 * barrier (A) in rcuwait_wait_event().

	 *

	 *    WAIT                WAKE

	 *    [S] tsk = current	  [S] cond = true

	 *        MB (A)	      MB (B)

	 *    [L] cond		  [L] tsk

 (B) */

/*

 * Determine if a process group is "orphaned", according to the POSIX

 * definition in 2.2.2.52.  Orphaned process groups are not to be affected

 * by terminal-generated stop signals.  Newly orphaned process groups are

 * to receive a SIGHUP and a SIGCONT.

 *

 * "I ask you, have you ever known what it is to be an orphan?"

/*

 * Check to see if any process groups have become orphaned as

 * a result of our exiting, and if they have any stopped jobs,

 * send them a SIGHUP and then a SIGCONT. (POSIX 3.2.2.2)

		/* exit: our father is in a different pgrp than

		 * we are and we were the only connection outside.

		/* reparent: our child is in a different pgrp than

		 * we are, and it was the only connection outside.

	/*

	 * Serialize with any possible pending coredump.

	 * We must hold siglock around checking core_state

	 * and setting PF_POSTCOREDUMP.  The core-inducing thread

	 * will increment ->nr_threads for each thread in the

	 * group without PF_POSTCOREDUMP set.

		/*

		 * Implies mb(), the result of xchg() must be visible

		 * to core_state->dumper.

 see coredump_finish() */

/*

 * A task is exiting.   If it owned this mm, find a new owner for the mm.

	/*

	 * If the exiting or execing task is not the owner, it's

	 * someone else's problem.

	/*

	 * The current owner is exiting/execing and there are no other

	 * candidates.  Do not leave the mm pointing to a possibly

	 * freed task structure.

	/*

	 * Search in the children

	/*

	 * Search in the siblings

	/*

	 * Search through everything else, we should not get here often.

	/*

	 * We found no owner yet mm_users > 1: this implies that we are

	 * most likely racing with swapoff (try_to_unuse()) or /proc or

	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.

	/*

	 * The task_lock protects c->mm from changing.

	 * We always want mm->owner->mm == mm

	/*

	 * Delay read_unlock() till we have the task_lock()

	 * to ensure that c does not slip away underneath us

 CONFIG_MEMCG */

/*

 * Turn us into a lazy TLB process if we

 * aren't already..

 more a memory barrier than a real lock */

	/*

	 * When a thread stops operating on an address space, the loop

	 * in membarrier_private_expedited() may not observe that

	 * tsk->mm, and the loop in membarrier_global_expedited() may

	 * not observe a MEMBARRIER_STATE_GLOBAL_EXPEDITED

	 * rq->membarrier_state, so those would not issue an IPI.

	 * Membarrier requires a memory barrier after accessing

	 * user-space memory, before clearing tsk->mm or the

	 * rq->membarrier_state.

/*

 * When we die, we re-parent all our children, and try to:

 * 1. give them to another thread in our thread group, if such a member exists

 * 2. give it to the first ancestor process which prctl'd itself as a

 *    child_subreaper for its children (like a service manager)

 * 3. give it to the init process (PID 1) in our pid namespace

		/*

		 * Find the first ->is_child_subreaper ancestor in our pid_ns.

		 * We can't check reaper != child_reaper to ensure we do not

		 * cross the namespaces, the exiting parent could be injected

		 * by setns() + fork().

		 * We check pid->level, this is slightly more efficient than

		 * task_active_pid_ns(reaper) != task_active_pid_ns(father).

/*

* Any that need to be release_task'd are put on the @dead list.

 We don't want people slaying init. */

 If it has exited notify the new parent about this child's death. */

/*

 * This does two things:

 *

 * A.  Make init inherit all the child processes

 * B.  Check to see if any process groups have become orphaned

 *	as a result of our exiting, and if they have any stopped

 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)

 Can drop and reacquire tasklist_lock */

		/*

		 * If this is a threaded reparent there is no need to

		 * notify anyone anything has happened.

/*

 * Send signals to all our closest relatives so that they know

 * to properly mourn us..

 mt-exec, de_thread() is waiting for group leader */

	/*

	 * We can get here from a kernel oops, sometimes with preemption off.

	 * Start by checking for critical errors.

	 * Then fix up important state like USER_DS and preemption.

	 * Then do everything else.

	/*

	 * If do_exit is called because this processes oopsed, it's possible

	 * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before

	 * continuing. Amongst other possible reasons, this is to prevent

	 * mm_release()->clear_child_tid() from writing to a user-controlled

	 * kernel address.

	/*

	 * We're taking recursive faults here in do_exit. Safest is to just

	 * leave this task alone and wait for reboot.

 sets PF_EXITING */

 sync mm's RSS info before statistics gathering */

		/*

		 * If the last thread of global init has exited, panic

		 * immediately to get a useable coredump.

	/*

	 * Flush inherited counters to the parent - before the parent

	 * gets woken up by child-exit notifications.

	 *

	 * because of cgroup mode, must be called before cgroup_exit()

	/*

	 * FIXME: do that only when needed, using sched_exit tracepoint

	/*

	 * Make sure we are holding no locks:

/*

 * Take down every thread in the group.  This is called by fatal signals

 * as well as by sys_exit_group (below).

 core dumps don't get here */

 Another thread got here before we took the lock.  */

 NOTREACHED */

/*

 * this kills every thread in the thread group. Note that any externally

 * wait4()-ing process will get the correct exit code - even if this

 * thread is not the thread group leader.

 NOTREACHED */

	/*

	 * Wait for all children (clone and not) if __WALL is set or

	 * if it is traced by us.

	/*

	 * Otherwise, wait for clone children *only* if __WCLONE is set;

	 * otherwise, wait for non-clone children *only*.

	 *

	 * Note: a "clone" child here is one that reports to its parent

	 * using a signal other than SIGCHLD, or a non-leader thread which

	 * we can only see if it is traced by us.

/*

 * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold

 * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold

 * the lock and this task is uninteresting.  If we return nonzero, we have

 * released the lock and the system call should return.

	/*

	 * Move the task's state to DEAD/TRACE, only one thread can do this.

	/*

	 * We own this thread, nobody else can reap it.

	/*

	 * Check thread_group_leader() to exclude the traced sub-threads.

		/*

		 * The resource counters for the group leader are in its

		 * own task_struct.  Those for dead threads in the group

		 * are in its signal_struct, as are those for the child

		 * processes it has previously reaped.  All these

		 * accumulate in the parent's signal_struct c* fields.

		 *

		 * We don't bother to take a lock here to protect these

		 * p->signal fields because the whole thread group is dead

		 * and nobody can change them.

		 *

		 * psig->stats_lock also protects us from our sub-theads

		 * which can reap other children at the same time. Until

		 * we change k_getrusage()-like users to rely on this lock

		 * we have to take ->siglock as well.

		 *

		 * We use thread_group_cputime_adjusted() to get times for

		 * the thread group, which consolidates times for all threads

		 * in the group including the group leader.

 We dropped tasklist, ptracer could die and untrace */

 If parent wants a zombie, don't release it now */

/**

 * wait_task_stopped - Wait for %TASK_STOPPED or %TASK_TRACED

 * @wo: wait options

 * @ptrace: is the wait for ptrace

 * @p: task to wait for

 *

 * Handle sys_wait4() work for %p in state %TASK_STOPPED or %TASK_TRACED.

 *

 * CONTEXT:

 * read_lock(&tasklist_lock), which is released if return value is

 * non-zero.  Also, grabs and releases @p->sighand->siglock.

 *

 * RETURNS:

 * 0 if wait condition didn't exist and search for other wait conditions

 * should continue.  Non-zero return, -errno on failure and @p's pid on

 * success, implies that tasklist_lock is released and wait condition

 * search should terminate.

 unneeded, required by compiler */

	/*

	 * Traditionally we see ptrace'd stopped tasks regardless of options.

	/*

	 * Now we are pretty sure this task is interesting.

	 * Make sure it doesn't get reaped out from under us while we

	 * give up the lock and then examine it below.  We don't want to

	 * keep holding onto the tasklist_lock while we call getrusage and

	 * possibly take page faults for user memory.

/*

 * Handle do_wait work for one task in a live, non-stopped state.

 * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold

 * the lock and this task is uninteresting.  If we return nonzero, we have

 * released the lock and the system call should return.

 Re-check with the lock held.  */

/*

 * Consider @p for a wait by @parent.

 *

 * -ECHILD should be in ->notask_error before the first call.

 * Returns nonzero for a final return, when we have unlocked tasklist_lock.

 * Returns zero if the search for a child should continue;

 * then ->notask_error is 0 if @p is an eligible child,

 * or still -ECHILD.

	/*

	 * We can race with wait_task_zombie() from another thread.

	 * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition

	 * can't confuse the checks below.

		/*

		 * ptrace == 0 means we are the natural parent. In this case

		 * we should clear notask_error, debugger will notify us.

		/*

		 * If it is traced by its real parent's group, just pretend

		 * the caller is ptrace_do_wait() and reap this child if it

		 * is zombie.

		 *

		 * This also hides group stop state from real parent; otherwise

		 * a single stop can be reported twice as group and ptrace stop.

		 * If a ptracer wants to distinguish these two events for its

		 * own children it should create a separate process which takes

		 * the role of real parent.

 slay zombie? */

 we don't reap group leaders with subthreads */

			/*

			 * A zombie ptracee is only visible to its ptracer.

			 * Notification and reaping will be cascaded to the

			 * real parent when the ptracer detaches.

		/*

		 * Allow access to stopped/continued state via zombie by

		 * falling through.  Clearing of notask_error is complex.

		 *

		 * When !@ptrace:

		 *

		 * If WEXITED is set, notask_error should naturally be

		 * cleared.  If not, subset of WSTOPPED|WCONTINUED is set,

		 * so, if there are live subthreads, there are events to

		 * wait for.  If all subthreads are dead, it's still safe

		 * to clear - this function will be called again in finite

		 * amount time once all the subthreads are released and

		 * will then return without clearing.

		 *

		 * When @ptrace:

		 *

		 * Stopped state is per-task and thus can't change once the

		 * target task dies.  Only continued and exited can happen.

		 * Clear notask_error if WCONTINUED | WEXITED.

		/*

		 * @p is alive and it's gonna stop, continue or exit, so

		 * there always is something to wait for.

	/*

	 * Wait for stopped.  Depending on @ptrace, different stopped state

	 * is used and the two don't interact with each other.

	/*

	 * Wait for continued.  There's only one continued state and the

	 * ptracer can consume it which can confuse the real parent.  Don't

	 * use WCONTINUED from ptracer.  You don't need or want it.

/*

 * Do the work of do_wait() for one thread in the group, @tsk.

 *

 * -ECHILD should be in ->notask_error before the first call.

 * Returns nonzero for a final return, when we have unlocked tasklist_lock.

 * Returns zero if the search for a child should continue; then

 * ->notask_error is 0 if there were any eligible children,

 * or still -ECHILD.

/*

 * Optimization for waiting on PIDTYPE_PID. No need to iterate through child

 * and tracee lists to find the target task.

	/*

	 * If there is nothing that can match our criteria, just get out.

	 * We will clear ->notask_error to zero if we see any child that

	 * might later match our criteria, even if we are not able to reap

	 * it yet.

 -INT_MIN is not defined */

 upid > 0 */ {

/*

 * sys_waitpid() remains for compatibility. waitpid() should be

 * implemented by calling sys_wait4() from libc.a.

 kernel_waitid() overwrites everything in ru */

/**

 * thread_group_exited - check that a thread group has exited

 * @pid: tgid of thread group to be checked.

 *

 * Test if the thread group represented by tgid has exited (all

 * threads are zombies, dead or completely gone).

 *

 * Return: true if the thread group has exited. false otherwise.

 if that doesn't kill us, halt */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Context tracking: Probe on high level context boundaries such as kernel

 * and userspace. This includes syscalls and exceptions entry/exit.

 *

 * This is used by RCU to remove its dependency on the timer tick while a CPU

 * runs in userspace.

 *

 *  Started by Frederic Weisbecker:

 *

 * Copyright (C) 2012 Red Hat, Inc., Frederic Weisbecker <fweisbec@redhat.com>

 *

 * Many thanks to Gilad Ben-Yossef, Paul McKenney, Ingo Molnar, Andrew Morton,

 * Steven Rostedt, Peter Zijlstra for suggestions and improvements.

 *

/**

 * context_tracking_enter - Inform the context tracking that the CPU is going

 *                          enter user or guest space mode.

 *

 * This function must be called right before we switch from the kernel

 * to user or guest space, when it's guaranteed the remaining kernel

 * instructions to execute won't use any RCU read side critical section

 * because this function sets RCU in extended quiescent state.

 Kernel threads aren't supposed to go to userspace */

			/*

			 * At this stage, only low level arch entry code remains and

			 * then we'll run in userspace. We can assume there won't be

			 * any RCU read-side critical section until the next call to

			 * user_exit() or rcu_irq_enter(). Let's remove RCU's dependency

			 * on the tick.

		/*

		 * Even if context tracking is disabled on this CPU, because it's outside

		 * the full dynticks mask for example, we still have to keep track of the

		 * context transitions and states to prevent inconsistency on those of

		 * other CPUs.

		 * If a task triggers an exception in userspace, sleep on the exception

		 * handler and then migrate to another CPU, that new CPU must know where

		 * the exception returns by the time we call exception_exit().

		 * This information can only be provided by the previous CPU when it called

		 * exception_enter().

		 * OTOH we can spare the calls to vtime and RCU when context_tracking.active

		 * is false because we know that CPU is not tickless.

	/*

	 * Some contexts may involve an exception occuring in an irq,

	 * leading to that nesting:

	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()

	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()

	 * helpers are enough to protect RCU uses inside the exception. So

	 * just return immediately if we detect we are in an IRQ.

/**

 * context_tracking_exit - Inform the context tracking that the CPU is

 *                         exiting user or guest mode and entering the kernel.

 *

 * This function must be called after we entered the kernel from user or

 * guest space before any use of RCU read side critical section. This

 * potentially include any high level kernel code like syscalls, exceptions,

 * signal handling, etc...

 *

 * This call supports re-entrancy. This way it can be called from any exception

 * handler without needing to know if we came from userspace or not.

			/*

			 * We are going to run code that may use RCU. Inform

			 * RCU core about that (ie: we may need the tick again).

	/*

	 * Set TIF_NOHZ to init/0 and let it propagate to all tasks through fork

	 * This assumes that init is the only task at this early boot stage.

 SPDX-License-Identifier: GPL-2.0

/*

 * padata.c - generic interface to process data streams in parallel

 *

 * See Documentation/core-api/padata.rst for more information.

 *

 * Copyright (C) 2008, 2009 secunet Security Networks AG

 * Copyright (C) 2008, 2009 Steffen Klassert <steffen.klassert@secunet.com>

 *

 * Copyright (c) 2020 Oracle and/or its affiliates.

 * Author: Daniel Jordan <daniel.m.jordan@oracle.com>

 Work's memory is on stack */

 padata_free_works linkage */

	/*

	 * Hash the sequence numbers to the cpus by taking

	 * seq_nr mod. number of cpus in use.

 No more work items allowed to be queued. */

 Start at 1 because the current task participates in the job. */

/**

 * padata_do_parallel - padata parallelization function

 *

 * @ps: padatashell

 * @padata: object to be parallelized

 * @cb_cpu: pointer to the CPU that the serialization callback function should

 *          run on.  If it's not in the serial cpumask of @pinst

 *          (i.e. cpumask.cbcpu), this function selects a fallback CPU and if

 *          none found, returns -EINVAL.

 *

 * The parallelization callback function will run with BHs off.

 * Note: Every object which is parallelized by padata_do_parallel

 * must be seen by padata_do_serial.

 *

 * Return: 0 on success or else negative error code.

 Select an alternate fallback CPU and notify the caller. */

 Maximum works limit exceeded, run in the current task. */

/*

 * padata_find_next - Find the next object that needs serialization.

 *

 * Return:

 * * A pointer to the control struct of the next object that needs

 *   serialization, if present in one of the percpu reorder queues.

 * * NULL, if the next object that needs serialization will

 *   be parallel processed by another cpu and is not yet present in

 *   the cpu's reorder queue.

	/*

	 * Checks the rare case where two or more parallel jobs have hashed to

	 * the same CPU and one of the later ones finishes first.

	/*

	 * We need to ensure that only one cpu can work on dequeueing of

	 * the reorder queue the time. Calculating in which percpu reorder

	 * queue the next object will arrive takes some time. A spinlock

	 * would be highly contended. Also it is not clear in which order

	 * the objects arrive to the reorder queues. So a cpu could wait to

	 * get the lock just to notice that there is nothing to do at the

	 * moment. Therefore we use a trylock and let the holder of the lock

	 * care for all the objects enqueued during the holdtime of the lock.

		/*

		 * If the next object that needs serialization is parallel

		 * processed by another cpu and is still on it's way to the

		 * cpu's reorder queue, nothing to do for now.

	/*

	 * The next object that needs serialization might have arrived to

	 * the reorder queues in the meantime.

	 *

	 * Ensure reorder queue is read after pd->lock is dropped so we see

	 * new objects from another task in padata_do_serial.  Pairs with

	 * smp_mb in padata_do_serial.

/**

 * padata_do_serial - padata serialization function

 *

 * @padata: object to be serialized.

 *

 * padata_do_serial must be called for every parallelized object.

 * The serialization callback function will run with BHs off.

 Sort in ascending order of sequence number. */

	/*

	 * Ensure the addition to the reorder list is ordered correctly

	 * with the trylock of pd->lock in padata_reorder.  Pairs with smp_mb

	 * in padata_reorder.

 Restrict parallel_wq workers to pd->cpumask.pcpu. */

 So end is chunk size aligned if enough work remains. */

/**

 * padata_do_multithreaded - run a multithreaded job

 * @job: Description of the job.

 *

 * See the definition of struct padata_mt_job for more details.

 In case threads finish at different times. */

 Ensure at least one thread when size < min_chunk. */

 Single thread, no coordination needed, cut to the chase. */

	/*

	 * Chunk size is the amount of work a helper does per call to the

	 * thread function.  Load balance large jobs between threads by

	 * increasing the number of chunks, guarantee at least the minimum

	 * chunk size from the caller, and honor the caller's alignment.

 Use the current thread, which saves starting a workqueue worker. */

 Wait for all the helpers to finish. */

 Initialize all percpu queues used by serial workers */

 Initialize per-CPU reorder lists */

 Allocate and initialize the internal cpumask dependend resources. */

 Replace the internal control structure with a new one. */

 If cpumask contains no active cpu, we mark the instance as invalid. */

/**

 * padata_set_cpumask - Sets specified by @cpumask_type cpumask to the value

 *                      equivalent to @cpumask.

 * @pinst: padata instance

 * @cpumask_type: PADATA_CPU_SERIAL or PADATA_CPU_PARALLEL corresponding

 *                to parallel and serial cpumasks respectively.

 * @cpumask: the cpumask to use

 *

 * Return: 0 on success or negative error code

/*

 * Padata sysfs provides the following objects:

 * serial_cpumask   [RW] - cpumask for serial workers

 * parallel_cpumask [RW] - cpumask for parallel workers

/**

 * padata_alloc - allocate and initialize a padata instance

 * @name: used to identify the instance

 *

 * Return: new instance on success, NULL on error

/**

 * padata_free - free a padata instance

 *

 * @pinst: padata instance to free

/**

 * padata_alloc_shell - Allocate and initialize padata shell.

 *

 * @pinst: Parent padata_instance object.

 *

 * Return: new shell on success, NULL on error

/**

 * padata_free_shell - free a padata shell

 *

 * @ps: padata shell to free

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kallsyms.c: in-kernel printing of symbolic oopses and stack traces.

 *

 * Rewritten and vastly simplified by Rusty Russell for in-kernel

 * module loader:

 *   Copyright 2002 Rusty Russell <rusty@rustcorp.com.au> IBM Corporation

 *

 * ChangeLog:

 *

 * (25/Aug/2004) Paulo Marques <pmarques@grupopie.com>

 *      Changed the compression method from stem compression to "table lookup"

 *      compression (see scripts/kallsyms.c for a more complete description)

 for cond_resched */

/*

 * These will be re-linked against their real values

 * during the second link stage.

/*

 * Tell the compiler that the count isn't in the small data section if the arch

 * has one (eg: FRV).

/*

 * Expand a compressed symbol data into the resulting uncompressed string,

 * if uncompressed string is too long (>= maxlen), it will be truncated,

 * given the offset to where the symbol is in the compressed stream.

 Get the compressed symbol length from the first symbol byte. */

	/*

	 * Update the offset to return the offset for the next symbol on

	 * the compressed stream.

	/*

	 * For every byte on the compressed symbol data, copy the table

	 * entry for that byte.

 Return to offset to the next symbol. */

/*

 * Get symbol type information. This is encoded as a single char at the

 * beginning of the symbol name.

	/*

	 * Get just the first code, look it up in the token table,

	 * and return the first char from this token.

/*

 * Find the offset on the compressed stream given and index in the

 * kallsyms array.

	/*

	 * Use the closest marker we have. We have markers every 256 positions,

	 * so that should be close enough.

	/*

	 * Sequentially scan all the symbols up to the point we're searching

	 * for. Every symbol is stored in a [<len>][<len> bytes of data] format,

	 * so we just need to add the len to the current pointer for every

	 * symbol we wish to skip.

 values are unsigned offsets if --absolute-percpu is not in effect */

 ...otherwise, positive offsets are absolute values */

 ...and negative offsets are relative to kallsyms_relative_base - 1 */

	/*

	 * LLVM appends various suffixes for local functions and variables that

	 * must be promoted to global scope as part of LTO.  This can break

	 * hooking of static functions with kprobes. '.' is not a valid

	 * character in an identifier in C. Suffixes observed:

	 * - foo.llvm.[0-9a-f]+

	 * - foo.[0-9a-f]+

	 * - foo.[0-9a-f]+.cfi_jt

	/*

	 * Prior to LLVM 13, the following suffixes were observed when thinLTO

	 * and CFI are both enabled:

	 * - foo$[0-9]+

 Lookup the address for this symbol. Returns 0 if not found. */

/*

 * Iterate over all symbols in vmlinux.  For symbols from modules use

 * module_kallsyms_on_each_symbol instead.

 CONFIG_LIVEPATCH */

 This kernel should never had been booted. */

 Do a binary search on the sorted kallsyms_addresses array. */

	/*

	 * Search for the first aliased symbol. Aliased

	 * symbols are symbols with the same address.

 Search for next non-aliased symbol. */

 If we found no next symbol, we use the end of the section. */

/*

 * Lookup an address but don't bother to find any names.

 Grab name */

 See if it's in a module or a BPF JITed image. */

/*

 * Lookup an address

 * - modname is set to NULL if it's in the kernel.

 * - We guarantee that the returned name is valid until we reschedule even if.

 *   It resides in a module.

 * - We also guarantee that modname will be valid until rescheduled.

 Grab name */

 See if it's in a module. */

 Grab name */

 See if it's in a module. */

 Look up a kernel symbol and return it in a text buffer. */

 build ID should match length of sprintf */

/**

 * sprint_symbol - Look up a kernel symbol and return it in a text buffer

 * @buffer: buffer to be stored

 * @address: address to lookup

 *

 * This function looks up a kernel symbol with @address and stores its name,

 * offset, size and module name to @buffer if possible. If no symbol was found,

 * just saves its @address as is.

 *

 * This function returns the number of bytes stored in @buffer.

/**

 * sprint_symbol_build_id - Look up a kernel symbol and return it in a text buffer

 * @buffer: buffer to be stored

 * @address: address to lookup

 *

 * This function looks up a kernel symbol with @address and stores its name,

 * offset, size, module name and module build ID to @buffer if possible. If no

 * symbol was found, just saves its @address as is.

 *

 * This function returns the number of bytes stored in @buffer.

/**

 * sprint_symbol_no_offset - Look up a kernel symbol and return it in a text buffer

 * @buffer: buffer to be stored

 * @address: address to lookup

 *

 * This function looks up a kernel symbol with @address and stores its name

 * and module name to @buffer if possible. If no symbol was found, just saves

 * its @address as is.

 *

 * This function returns the number of bytes stored in @buffer.

/**

 * sprint_backtrace - Look up a backtrace symbol and return it in a text buffer

 * @buffer: buffer to be stored

 * @address: address to lookup

 *

 * This function is for stack backtrace and does the same thing as

 * sprint_symbol() but with modified/decreased @address. If there is a

 * tail-call to the function marked "noreturn", gcc optimized out code after

 * the call so that the stack-saved return address could point outside of the

 * caller. This function ensures that kallsyms will find the original caller

 * by decreasing @address.

 *

 * This function returns the number of bytes stored in @buffer.

/**

 * sprint_backtrace_build_id - Look up a backtrace symbol and return it in a text buffer

 * @buffer: buffer to be stored

 * @address: address to lookup

 *

 * This function is for stack backtrace and does the same thing as

 * sprint_symbol() but with modified/decreased @address. If there is a

 * tail-call to the function marked "noreturn", gcc optimized out code after

 * the call so that the stack-saved return address could point outside of the

 * caller. This function ensures that kallsyms will find the original caller

 * by decreasing @address. This function also appends the module build ID to

 * the @buffer if @address is within a kernel module.

 *

 * This function returns the number of bytes stored in @buffer.

 To avoid using get_symbol_offset for every symbol, we carry prefix along. */

 If iterating in core kernel symbols. */

/*

 * ftrace_mod_get_kallsym() may also get symbols for pages allocated for ftrace

 * purposes. In that case "__builtin__ftrace" is used as a module name, even

 * though "__builtin__ftrace" is not a module.

/*

 * This uses "__builtin__kprobes" as a module name for symbols for pages

 * allocated for kprobes' purposes, even though "__builtin__kprobes" is not a

 * module.

 Returns space to next name. */

/*

 * The end position (last + 1) of each additional kallsyms section is recorded

 * in iter->pos_..._end as each section is added, and so can be used to

 * determine which get_ksymbol_...() function to call next.

 Returns false if pos at or past end of file. */

 Module symbols can be accessed randomly. */

 If we're not on the desired position, reset to new position. */

 Some debugging symbols have no name.  Ignore them. */

		/*

		 * Label it "global" if it is exported,

		 * "local" if not exported.

/*

 * We show kallsyms information even to normal users if we've enabled

 * kernel profiling and are explicitly not paranoid (so kptr_restrict

 * is clear, and sysctl_perf_event_paranoid isn't set).

 *

 * Otherwise, require CAP_SYSLOG (assuming kptr_restrict isn't set to

 * block even that).

	/*

	 * We keep iterator in m->private, since normal case is to

	 * s_start from where we left off, so we avoid doing

	 * using get_symbol_offset for every symbol.

	/*

	 * Instead of checking this on every s_show() call, cache

	 * the result here at open time.

 Some debugging symbols have no name.  Ignore them. */

 CONFIG_KGDB_KDB */

 SPDX-License-Identifier: GPL-2.0

 with root here */

 index; upper bit indicates 'will prune' */

/*

 * One struct chunk is attached to each inode of interest through

 * audit_tree_mark (fsnotify mark). We replace struct chunk on tagging /

 * untagging, the mark is stable as long as there is chunk attached. The

 * association between mark and chunk is protected by hash_lock and

 * audit_tree_group->mark_mutex. Thus as long as we hold

 * audit_tree_group->mark_mutex and check that the mark is alive by

 * FSNOTIFY_MARK_FLAG_ATTACHED flag check, we are sure the mark points to

 * the current chunk.

 *

 * Rules have pointer to struct audit_tree.

 * Rules have struct list_head rlist forming a list of rules over

 * the same tree.

 * References to struct chunk are collected at audit_inode{,_child}()

 * time and used in AUDIT_TREE rule matching.

 * These references are dropped at the same time we are calling

 * audit_free_names(), etc.

 *

 * Cyclic lists galore:

 * tree.chunks anchors chunk.owners[].list			hash_lock

 * tree.rules anchors rule.rlist				audit_filter_mutex

 * chunk.trees anchors tree.same_root				hash_lock

 * chunk.hash is a hash with middle bits of watch.inode as

 * a hash function.						RCU, hash_lock

 *

 * tree is refcounted; one reference for "some rules on rules_list refer to

 * it", one for each chunk with pointer to it.

 *

 * chunk is refcounted by embedded .refs. Mark associated with the chunk holds

 * one chunk reference. This reference is dropped either when a mark is going

 * to be freed (corresponding inode goes away) or when chunk attached to the

 * mark gets replaced. This reference must be dropped using

 * audit_mark_put_chunk() to make sure the reference is dropped only after RCU

 * grace period as it protects RCU readers of the hash table.

 *

 * node.index allows to get from node.list to containing chunk.

 * MSB of that sucker is stolen to mark taggings that we might have to

 * revert - several operations have very unpleasant cleanup logics and

 * that makes a difference.  Some.

 to avoid bringing the entire thing in audit.h */

/*

 * Drop reference to the chunk that was held by the mark. This is the reference

 * that gets dropped after we've removed the chunk from the hash table and we

 * use it to make sure chunk cannot be freed before RCU grace period expires.

 Function to return search key in our hash from inode. */

 Use address pointed to by connector->obj as the key */

 hash_lock & mark->group->mark_mutex is held by caller */

	/*

	 * Make sure chunk is fully initialized before making it visible in the

	 * hash. Pairs with a data dependency barrier in READ_ONCE() in

	 * audit_tree_lookup().

 called under rcu_read_lock */

		/*

		 * We use a data dependency barrier in READ_ONCE() to make sure

		 * the chunk we see is fully initialized.

 tagging and untagging inodes with trees */

 result of earlier fallback */

	/*

	 * Make sure chunk is fully initialized before making it visible in the

	 * hash. Pairs with a data dependency barrier in READ_ONCE() in

	 * audit_tree_lookup().

	/*

	 * mark_mutex stabilizes chunk attached to the mark so we can check

	 * whether it didn't change while we've dropped hash_lock.

	/*

	 * This has to go last when updating chunk as once replace_chunk() is

	 * called, new RCU readers can see the new chunk.

 Call with group->mark_mutex held, releases it */

	/*

	 * Inserting into the hash table has to go last as once we do that RCU

	 * readers can see the chunk.

	/*

	 * Drop our initial reference. When mark we point to is getting freed,

	 * we get notification through ->freeing_mark callback and cleanup

	 * chunk pointing to this mark.

 the first tagged inode becomes root of tree */

	/*

	 * Found mark is guaranteed to be attached and mark_mutex protects mark

	 * from getting detached and thus it makes sure there is chunk attached

	 * to the mark.

 are we already there? */

	/*

	 * This has to go last when updating chunk as once replace_chunk() is

	 * called, new RCU readers can see the new chunk.

 pair to fsnotify_find_mark */

 not a half-baked one */

/*

 * Remove tree from chunks. If 'tagged' is set, remove tree only from tagged

 * chunks. The function expects tagged chunks are all at the beginning of the

 * chunks list.

 have we run out of marked? */

 Racing with audit_tree_freeing_mark()? */

/*

 * finish killing struct audit_tree

 trim the uncommitted chunks from tree */

 reorder */

 called with audit_filter_mutex */

 this could be NULL if the watch is dying else where... */

/*

 * That gets run when evict_chunk() ends up needing to kill audit_tree.

 * Runs from a separate thread.

 called with audit_filter_mutex */

 do not set rule->tree yet */

/*

 * ... and that one is done if evict_chunk() decides to delay until the end

 * of syscall.  Runs synchronously.

/*

 *  Here comes the stuff asynchronous to auditctl operations

	/*

	 * We are guaranteed to have at least one reference to the mark from

	 * either the inode or the caller of fsnotify_destroy_mark().

 SPDX-License-Identifier: GPL-2.0

 mutex to protect key modules/sites */

 These assume the key is word-aligned. */

	/*

	 * If uninitialized, we'll not update the callsites, but they still

	 * point to the trampoline and we just patched that.

			/*

			 * This can happen if the static call key is defined in

			 * a module which doesn't use it.

			 *

			 * It also happens in the has_mods case, where the

			 * 'first' entry has no sites associated with it.

				/*

				 * This skips patching built-in __exit, which

				 * is part of init_section_contains() but is

				 * not part of kernel_text_address().

				 *

				 * Skipping built-in __exit is fine since it

				 * will never be executed.

			/*

			 * For vmlinux (!mod) avoid the allocation by storing

			 * the sites pointer in the key itself. Also see

			 * __static_call_update()'s @first.

			 *

			 * This allows architectures (eg. x86) to call

			 * static_call_init() before memory allocation works.

			/*

			 * When the key has a direct sites pointer, extract

			 * that into an explicit struct static_call_mod, so we

			 * can have a list of modules.

		/*

		 * Is the key is exported, 'addr' points to the key, which

		 * means modules are allowed to call static_call_update() on

		 * it.

		 *

		 * Otherwise, the key isn't exported, and 'addr' points to the

		 * trampoline so we need to lookup the key.

		 *

		 * We go through this dance to prevent crazy modules from

		 * abusing sensitive static calls.

 CONFIG_MODULES */

 CONFIG_STATIC_CALL_SELFTEST */

 SPDX-License-Identifier: GPL-2.0

/*

 * Supplementary group IDs

 export the group_info to a user-space array */

 fill a group_info from a user-space array - it must be allocated already */

 a simple bsearch */

/**

 * set_groups - Change a group subscription in a set of credentials

 * @new: The newly prepared set of credentials to alter

 * @group_info: The group list to install

/**

 * set_current_groups - Change current's group subscription

 * @group_info: The group list to impose

 *

 * Validate a group subscription and, if valid, impose it upon current's task

 * security record.

 no need to grab task_lock here; it cannot change */

/*

 *	SMP: Our groups are copy-on-write. We can set them safely

 *	without another task interfering.

/*

 * Check whether we're fsgid/egid or in the supplemental group..

/* CPU control.

 * (C) 2001, 2002, 2003, 2004 Rusty Russell

 *

 * This code is licenced under the GPL.

/**

 * struct cpuhp_cpu_state - Per cpu hotplug state storage

 * @state:	The current cpu state

 * @target:	The target state

 * @fail:	Current CPU hotplug callback state

 * @thread:	Pointer to the hotplug thread

 * @should_run:	Thread should execute

 * @rollback:	Perform a rollback

 * @single:	Single callback invocation

 * @bringup:	Single callback bringup or teardown selector

 * @cpu:	CPU number

 * @node:	Remote CPU node; for multi-instance, do a

 *		single entry callback for install/remove

 * @last:	For multi-instance rollback, remember how far we got

 * @cb_state:	The state for a single callback (install/uninstall)

 * @result:	Result of the operation

 * @done_up:	Signal completion to the issuer of the task for cpu-up

 * @done_down:	Signal completion to the issuer of the task for cpu-down

/**

 * struct cpuhp_step - Hotplug state machine step

 * @name:	Name of the step

 * @startup:	Startup function of the step

 * @teardown:	Teardown function of the step

 * @cant_stop:	Bringup/teardown can't be stopped at this step

 * @multi_instance:	State has multiple instances which get added afterwards

 private: */

 public: */

/**

 * cpuhp_invoke_callback - Invoke the callbacks for a given state

 * @cpu:	The cpu for which the callback should be invoked

 * @state:	The state to do callbacks for

 * @bringup:	True if the bringup callback should be invoked

 * @node:	For multi-instance, do a single entry callback for install/remove

 * @lastp:	For multi-instance rollback, remember how far we got

 *

 * Called from cpu hotplug and from the state register machinery.

 *

 * Return: %0 on success or a negative errno code

 Single invocation for instance add/remove */

 State transition. Invoke on all instances */

 Rollback the instances if one failed */

		/*

		 * Rollback must not fail,

	/*

	 * The extra check for CPUHP_TEARDOWN_CPU is only for documentation

	 * purposes as that state is handled explicitly in cpu_down.

/*

 * The former STARTING/DYING states, ran with IRQs disabled and must not fail.

 Serializes the updates to cpu_online_mask, cpu_present_mask */

/*

 * The following two APIs (cpu_maps_update_begin/done) must be used when

 * attempting to serialize the updates to cpu_online_mask & cpu_present_mask.

/*

 * If set, cpu_up and cpu_down will return -EBUSY and do nothing.

 * Should always be manipulated under cpu_add_remove_lock

	/*

	 * We can't have hotplug operations before userspace starts running,

	 * and some init codepaths will knowingly not take the hotplug lock.

	 * This is all valid, so mute lockdep until it makes sense to report

	 * unheld locks.

/*

 * Wait for currently running CPU hotplug operations to complete (if any) and

 * disable future CPU hotplug (from sysfs). The 'cpu_add_remove_lock' protects

 * the 'cpu_hotplug_disabled' flag. The same lock is also acquired by the

 * hotplug path before performing hotplug operations. So acquiring that lock

 * guarantees mutual exclusion from any currently running hotplug operations.

 CONFIG_HOTPLUG_CPU */

/*

 * Architectures that need SMT-specific errata handling during SMT hotplug

 * should override this.

/*

 * The decision whether SMT is supported can only be done after the full

 * CPU identification. Called from architecture code.

	/*

	 * On x86 it's required to boot all logical CPUs at least once so

	 * that the init code can get a chance to set CR4.MCE on each

	 * CPU. Otherwise, a broadcasted MCE observing CR4.MCE=0b on any

	 * core will shutdown the machine.

 Returns true if SMT is not supported of forcefully (irreversibly) disabled */

	/*

	 * Already rolling back. No need invert the bringup value or to change

	 * the current state.

	/*

	 * If we have st->last we need to undo partial multi_instance of this

	 * state first. Otherwise start undo at the previous state.

 Regular hotplug invocation of the AP hotplug thread */

	/*

	 * Make sure the above stores are visible before should_run becomes

	 * true. Paired with the mb() above in cpuhp_thread_fun()

 Wait for the CPU to reach CPUHP_AP_ONLINE_IDLE */

 Unpark the hotplug thread of the target cpu */

	/*

	 * SMT soft disabling on X86 requires to bring the CPU out of the

	 * BIOS 'wait for SIPI' state in order to set the CR4.MCE bit.  The

	 * CPU marked itself as booted_once in notify_cpu_starting() so the

	 * cpu_smt_allowed() check will now return false if this is not the

	 * primary sibling.

	/*

	 * Some architectures have to walk the irq descriptors to

	 * setup the vector space for the cpu which comes online.

	 * Prevent irq alloc/free across the bringup.

 Arch-specific enabling code. */

	/*

	 * idle_task_exit() will have switched to &init_mm, now

	 * clean up any remaining active_mm state.

/*

 * Hotplug state machine related functions

/*

 * Get the next state to run. Empty ones will be skipped. Returns true if a

 * state must be run.

 *

 * st->state will be modified ahead of time, to match state_to_run, as if it

 * has already ran.

	/*

	 * When CPU hotplug is disabled, then taking the CPU down is not

	 * possible because takedown_cpu() and the architecture and

	 * subsystem specific mechanisms are not available. So the CPU

	 * which would be completely unplugged again needs to stay around

	 * in the current state.

/*

 * The cpu hotplug threads manage the bringup and teardown of the cpus

/*

 * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke

 * callbacks when a state gets [un]installed at runtime.

 *

 * Each invocation of this function by the smpboot thread does a single AP

 * state callback.

 *

 * It has 3 modes of operation:

 *  - single: runs st->cb_state

 *  - up:     runs ++st->state, while st->state < st->target

 *  - down:   runs st->state--, while st->state > st->target

 *

 * When complete or on error, should_run is cleared and the completion is fired.

	/*

	 * ACQUIRE for the cpuhp_should_run() load of ->should_run. Ensures

	 * that if we see ->should_run we also see the rest of the state.

	/*

	 * The BP holds the hotplug lock, but we're now running on the AP,

	 * ensure that anybody asserting the lock is held, will actually find

	 * it so.

		/*

		 * STARTING/DYING must not fail!

		/*

		 * If we fail on a rollback, we're up a creek without no

		 * paddle, no way forward, no way back. We loose, thanks for

		 * playing.

 Invoke a single callback on a remote cpu */

	/*

	 * If we are up and running, use the hotplug thread. For early calls

	 * we invoke the thread function directly.

	/*

	 * If we failed and did a partial, do a rollback.

	/*

	 * Clean up the leftovers so the next hotplug operation wont use stale

	 * data.

/*

 *

 * Serialize hotplug trainwrecks outside of the cpu_hotplug_lock

 * protected region.

 *

 * The operation is still serialized against concurrent CPU hotplug via

 * cpu_add_remove_lock, i.e. CPU map protection.  But it is _not_

 * serialized against other hotplug related activity like adding or

 * removing of state callbacks and state instances, which invoke either the

 * startup or the teardown callback of the affected state.

 *

 * This is required for subsystems which are unfixable vs. CPU hotplug and

 * evade lock inversion problems by scheduling work which has to be

 * completed _before_ cpu_up()/_cpu_down() returns.

 *

 * Don't even think about adding anything to this for any new code or even

 * drivers. It's only purpose is to keep existing lock order trainwrecks

 * working.

 *

 * For cpu_down() there might be valid reasons to finish cleanups which are

 * not required to be done under cpu_hotplug_lock, but that's a different

 * story and would be not invoked via this.

	/*

	 * cpusets delegate hotplug operations to a worker to "solve" the

	 * lock order problems. Wait for the worker, but only if tasks are

	 * _not_ frozen (suspend, hibernate) as that would wait forever.

	 *

	 * The wait is required because otherwise the hotplug operation

	 * returns with inconsistent state, which could even be observed in

	 * user space when a new CPU is brought up. The CPU plug uevent

	 * would be delivered and user space reacting on it would fail to

	 * move tasks to the newly plugged CPU up to the point where the

	 * work has finished because up to that point the newly plugged CPU

	 * is not assignable in cpusets/cgroups. On unplug that's not

	 * necessarily a visible issue, but it is still inconsistent state,

	 * which is the real problem which needs to be "fixed". This can't

	 * prevent the transient state between scheduling the work and

	 * returning from waiting for it.

/**

 * clear_tasks_mm_cpumask - Safely clear tasks' mm_cpumask for a CPU

 * @cpu: a CPU id

 *

 * This function walks all processes, finds a valid mm struct for each one and

 * then clears a corresponding bit in mm's cpumask.  While this all sounds

 * trivial, there are various non-obvious corner cases, which this function

 * tries to solve in a safe manner.

 *

 * Also note that the function uses a somewhat relaxed locking scheme, so it may

 * be called only for an already offlined CPU.

	/*

	 * This function is called after the cpu is taken down and marked

	 * offline, so its not like new tasks will ever get this cpu set in

	 * their mm mask. -- Peter Zijlstra

	 * Thus, we may use rcu_read_lock() here, instead of grabbing

	 * full-fledged tasklist_lock.

		/*

		 * Main thread might exit, but other threads may still have

		 * a valid mm. Find one.

 Take this CPU down. */

 Ensure this CPU doesn't handle any more interrupts. */

	/*

	 * Must be called from CPUHP_TEARDOWN_CPU, which means, as we are going

	 * down, that the current state is CPUHP_TEARDOWN_CPU - 1.

 Invoke the former CPU_DYING callbacks */

	/*

	 * DYING must not fail!

 Give up timekeeping duties */

 Remove CPU from timer broadcasting */

 Park the stopper thread */

 Park the smpboot threads */

	/*

	 * Prevent irq alloc/free while the dying cpu reorganizes the

	 * interrupt affinities.

	/*

	 * So now all preempt/rcu users must observe !cpu_active().

 CPU refused to die */

 Unpark the hotplug thread so we can rollback there */

	/*

	 * The teardown callback for CPUHP_AP_SCHED_STARTING will have removed

	 * all runnable tasks from the CPU, there's only the idle task left now

	 * that the migration thread is done doing the stop_machine thing.

	 *

	 * Wait for the stop thread to go away.

 Interrupts are moved away from the dying cpu, reenable alloc/free */

 This actually kills the CPU. */

	/*

	 * We cannot call complete after rcu_report_dead() so we delegate it

	 * to an online cpu.

 Requires cpu_add_remove_lock to be held */

	/*

	 * If the current CPU state is in the range of the AP hotplug thread,

	 * then we need to kick the thread.

		/*

		 * The AP side has done the error rollback already. Just

		 * return the error code..

		/*

		 * We might have stopped still in the range of the AP hotplug

		 * thread. Nothing to do anymore.

	/*

	 * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need

	 * to do the further cleanups.

	/*

	 * Do post unplug cleanup. This is still protected against

	 * concurrent CPU hotplug via cpu_add_remove_lock.

/**

 * cpu_device_down - Bring down a cpu device

 * @dev: Pointer to the cpu device to offline

 *

 * This function is meant to be used by device core cpu subsystem only.

 *

 * Other subsystems should use remove_cpu() instead.

 *

 * Return: %0 on success or a negative errno code

	/*

	 * Make certain the cpu I'm about to reboot on is online.

	 *

	 * This is inline to what migrate_to_reboot_cpu() already do.

	/*

	 * Ensure all but the reboot CPU are offline.

	/*

	 * Make sure the CPUs won't be enabled by someone else after this

	 * point. Kexec will reboot to a new kernel shortly resetting

	 * everything along the way.

CONFIG_HOTPLUG_CPU*/

/**

 * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU

 * @cpu: cpu that just started

 *

 * It must be called by the arch code on the new cpu, before the new cpu

 * enables interrupts and before the "boot" cpu returns from __cpu_up().

 Enables RCU usage on this CPU. */

	/*

	 * STARTING must not fail!

/*

 * Called from the idle task. Wake up the controlling task which brings the

 * hotplug thread of the upcoming CPU up and then delegates the rest of the

 * online bringup to the hotplug thread.

 Happens for the boot cpu */

	/*

	 * Unpart the stopper thread before we start the idle loop (and start

	 * scheduling); this ensures the stopper task is always available.

 Requires cpu_add_remove_lock to be held */

	/*

	 * The caller of cpu_up() might have raced with another

	 * caller. Nothing to do.

 Let it fail before we try to bring the cpu up */

	/*

	 * If the current CPU state is in the range of the AP hotplug thread,

	 * then we need to kick the thread once more.

		/*

		 * The AP side has done the error rollback already. Just

		 * return the error code..

	/*

	 * Try to reach the target state. We max out on the BP at

	 * CPUHP_BRINGUP_CPU. After that the AP hotplug thread is

	 * responsible for bringing it up to the target state.

/**

 * cpu_device_up - Bring up a cpu device

 * @dev: Pointer to the cpu device to online

 *

 * This function is meant to be used by device core cpu subsystem only.

 *

 * Other subsystems should use add_cpu() instead.

 *

 * Return: %0 on success or a negative errno code

/**

 * bringup_hibernate_cpu - Bring up the CPU that we hibernated on

 * @sleep_cpu: The cpu we hibernated on and should be brought up.

 *

 * On some architectures like arm64, we can hibernate on any CPU, but on

 * wake up the CPU we hibernated on might be offline as a side effect of

 * using maxcpus= for example.

 *

 * Return: %0 on success or a negative errno code

	/*

	 * We take down all of the non-boot CPUs in one shot to avoid races

	 * with the userspace trying to use the CPU hotplug at the same time

	/*

	 * Make sure the CPUs won't be enabled by someone else. We need to do

	 * this even in case of failure as all freeze_secondary_cpus() users are

	 * supposed to do thaw_secondary_cpus() on the failure path.

 Allow everyone to use the CPU hotplug again */

/*

 * When callbacks for CPU hotplug notifications are being executed, we must

 * ensure that the state of the system with respect to the tasks being frozen

 * or not, as reported by the notification, remains unchanged *throughout the

 * duration* of the execution of the callbacks.

 * Hence we need to prevent the freezer from racing with regular CPU hotplug.

 *

 * This synchronization is implemented by mutually excluding regular CPU

 * hotplug and Suspend/Hibernate call paths by hooking onto the Suspend/

 * Hibernate notifications.

	/*

	 * cpu_hotplug_pm_callback has higher priority than x86

	 * bsp_pm_callback which depends on cpu_hotplug_pm_callback

	 * to disable cpu hotplug to avoid cpu hotplug race.

 CONFIG_PM_SLEEP_SMP */

 CONFIG_SMP */

 Boot processor state steps */

	/*

	 * On the tear-down path, timers_dead_cpu() must be invoked

	 * before blk_mq_queue_reinit_notify() from notify_dead(),

	 * otherwise a RCU stall occurs.

 Kicks the plugged cpu into life */

 Final state before CPU kills itself */

	/*

	 * Last state before CPU enters the idle loop to die. Transient state

	 * for synchronization.

 First state is scheduler control. Interrupts are disabled */

	/* Entry state on starting. Interrupts enabled from here on. Transient

	/*

	 * Handled on control processor until the plugged processor manages

	 * this itself.

 Handle smpboot threads park/unpark */

	/*

	 * The dynamically registered state space is here

 Last state is scheduler control setting the cpu active */

 CPU is fully up and running. */

 Sanity check for callbacks */

/*

 * Returns a free for dynamic slot assignment of the Online state. The states

 * are protected by the cpuhp_slot_states mutex and an empty slot is identified

 * by having no name assigned.

 (Un)Install the callbacks for further cpu hotplug operations */

	/*

	 * If name is NULL, then the state gets removed.

	 *

	 * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on

	 * the first allocation from these dynamic ranges, so the removal

	 * would trigger a new allocation and clear the wrong (already

	 * empty) state, leaving the callbacks of the to be cleared state

	 * dangling, which causes wreckage on the next hotplug operation.

/*

 * Call the startup/teardown function for a step either on the AP or

 * on the current CPU.

	/*

	 * If there's nothing to do, we done.

	 * Relies on the union for multi_instance.

	/*

	 * The non AP bound callbacks can fail on bringup. On teardown

	 * e.g. module removal we crash for now.

/*

 * Called from __cpuhp_setup_state on a recoverable failure.

 *

 * Note: The teardown callbacks for rollback are not allowed to fail!

 Roll back the already executed steps on the other cpus */

 Did we invoke the startup call on that cpu ? */

	/*

	 * Try to call the startup callback for each present cpu

	 * depending on the hotplug state of the cpu.

/**

 * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state

 * @state:		The state to setup

 * @name:		Name of the step

 * @invoke:		If true, the startup function is invoked for cpus where

 *			cpu state >= @state

 * @startup:		startup callback function

 * @teardown:		teardown callback function

 * @multi_instance:	State is set up for multiple instances which get

 *			added afterwards.

 *

 * The caller needs to hold cpus read locked while calling this function.

 * Return:

 *   On success:

 *      Positive state number if @state is CPUHP_AP_ONLINE_DYN;

 *      0 for all other states

 *   On failure: proper (negative) error code

	/*

	 * Try to call the startup callback for each present cpu

	 * depending on the hotplug state of the cpu.

	/*

	 * If the requested state is CPUHP_AP_ONLINE_DYN, return the

	 * dynamically allocated state in case of success.

	/*

	 * Call the teardown callback for each present cpu depending

	 * on the hotplug state of the cpu. This function is not

	 * allowed to fail currently!

/**

 * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state

 * @state:	The state to remove

 * @invoke:	If true, the teardown function is invoked for cpus where

 *		cpu state >= @state

 *

 * The caller needs to hold cpus read locked while calling this function.

 * The teardown callback is currently not allowed to fail. Think

 * about module removal!

	/*

	 * Call the teardown callback for each present cpu depending

	 * on the hotplug state of the cpu. This function is not

	 * allowed to fail currently!

 Tell user space about the state change */

 Tell user space about the state change */

		/*

		 * As this needs to hold the cpu maps lock it's impossible

		 * to call device_offline() because that ends up calling

		 * cpu_down() which takes cpu maps lock. cpu maps lock

		 * needs to be held as this might race against in kernel

		 * abusers of the hotplug machinery (thermal management).

		 *

		 * So nothing would update device:offline state. That would

		 * leave the sysfs entry stale and prevent onlining after

		 * smt control has been changed to 'off' again. This is

		 * called under the sysfs hotplug lock, so it is properly

		 * serialized against the regular offline usage.

 Skip online CPUs and CPUs on offline nodes */

 See comment in cpuhp_smt_disable() */

	/*

	 * Cannot fail STARTING/DYING callbacks.

	/*

	 * DEAD callbacks cannot fail...

	 * ... neither can CPUHP_BRINGUP_CPU during hotunplug. The latter

	 * triggering STARTING callbacks, a failure in this state would

	 * hinder rollback.

	/*

	 * Cannot fail anything that doesn't have callbacks.

 !CONFIG_HOTPLUG_SMT */

 CONFIG_HOTPLUG_SMT */

 CONFIG_SYSFS && CONFIG_HOTPLUG_CPU */

/*

 * cpu_bit_bitmap[] is a special, "compressed" data structure that

 * represents all NR_CPUS bits binary values of 1<<nr.

 *

 * It is used by cpumask_of() to get a constant address to a CPU

 * mask value that has a single bit set only.

 cpu_bit_bitmap[0] is empty - so we can back into it */

	/*

	 * atomic_inc/dec() is required to handle the horrid abuse of this

	 * function by the reboot and kexec code which invoke it from

	 * IPI/NMI broadcasts when shutting down CPUs. Invocation from

	 * regular CPU hotplug is properly serialized.

	 *

	 * Note, that the fact that __num_online_cpus is of type atomic_t

	 * does not protect readers which are not serialized against

	 * concurrent hotplug operations.

/*

 * Activate the first processor.

 Mark the boot cpu "present", "online" etc for SMP and UP case */

/*

 * Must be called _AFTER_ setting up the per_cpu areas

/*

 * These are used for a global "mitigations=" cmdline option for toggling

 * optional CPU mitigations.

 mitigations=off */

 mitigations=auto,nosmt */

 SPDX-License-Identifier: GPL-2.0

/*

 *	Wrapper functions for 16bit uid back compatibility. All nicely tied

 *	together in the faint hope we can take the out in five years time.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Detect Hung Task

 *

 * kernel/hung_task.c - kernel thread for detecting tasks stuck in D state

 *

/*

 * The number of tasks checked:

/*

 * Limit number of tasks checked in a batch.

 *

 * This value controls the preemptibility of khungtaskd since preemption

 * is disabled during the critical section. It also controls the size of

 * the RCU grace period. So it needs to be upper-bound.

/*

 * Zero means infinite timeout - no checking done:

/*

 * Zero (default value) means use sysctl_hung_task_timeout_secs:

/*

 * Should we dump all CPUs backtraces in a hung task event?

 * Defaults to 0, can be changed via sysctl.

 CONFIG_SMP */

/*

 * Should we panic (and reboot, if panic_timeout= is set) when a

 * hung task is detected:

	/*

	 * Ensure the task is not frozen.

	 * Also, skip vfork and any other user process that freezer should skip.

	/*

	 * When a freshly created task is scheduled once, changes its state to

	 * TASK_UNINTERRUPTIBLE without having ever been switched out once, it

	 * musn't be checked.

	/*

	 * Ok, the task did not get scheduled for more than 2 minutes,

	 * complain:

/*

 * To avoid extending the RCU grace period for an unbounded amount of time,

 * periodically exit the critical section and enter a new one.

 *

 * For preemptible RCU it is sufficient to call rcu_read_unlock in order

 * to exit the grace period. For classic RCU, a reschedule is required.

/*

 * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for

 * a really long time (120 seconds). If that happens, print out

 * a warning.

	/*

	 * If the system crashed already then all bets are off,

	 * do not report extra hung tasks:

 use "==" to skip the TASK_KILLABLE tasks waiting on NFS */

 timeout of 0 will disable the watchdog */

/*

 * Process updating of timeout sysctl

/*

 * kthread which checks for tasks stuck in D state

 Disable hung task detector on suspend */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/freezer.c - Function to freeze a process

 *

 * Originally from kernel/power/process.c

 total number of freezing conditions in effect */

/* indicate whether PM freezing is in effect, protected by

 * system_transition_mutex

 protects freezing and frozen transitions */

/**

 * freezing_slow_path - slow path for testing whether a task needs to be frozen

 * @p: task to be tested

 *

 * This function is called by freezing() if system_freezing_cnt isn't zero

 * and tests whether @p needs to enter and stay in frozen state.  Can be

 * called under any context.  The freezers are responsible for ensuring the

 * target tasks see the updated state.

 Refrigerator is place where frozen processes are stored :-). */

	/* Hmm, should we be allowed to suspend when there are realtime

	/*

	 * Restore saved task state before returning.  The mb'd version

	 * needs to be used; otherwise, it might silently break

	 * synchronization which depends on ordered task state change.

/**

 * freeze_task - send a freeze request to given task

 * @p: task to send the request to

 *

 * If @p is freezing, the freeze request is sent either by sending a fake

 * signal (if it's not a kernel thread) or waking it up (if it's a kernel

 * thread).

 *

 * RETURNS:

 * %false, if @p is not freezing or already frozen; %true, otherwise

	/*

	 * This check can race with freezer_do_not_count, but worst case that

	 * will result in an extra wakeup being sent to the task.  It does not

	 * race with freezer_count(), the barriers in freezer_count() and

	 * freezer_should_skip() ensure that either freezer_count() sees

	 * freezing == true in try_to_freeze() and freezes, or

	 * freezer_should_skip() sees !PF_FREEZE_SKIP and freezes the task

	 * normally.

/**

 * set_freezable - make %current freezable

 *

 * Mark %current freezable and enter refrigerator if necessary.

	/*

	 * Modify flags while holding freezer_lock.  This ensures the

	 * freezer notices that we aren't frozen yet or the freezing

	 * condition is visible to try_to_freeze() below.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/panic.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

/*

 * This function is used through-out the kernel (including mm and fs)

 * to indicate a major problem.

/*

 * Should we dump all CPUs backtraces in an oops event?

 * Defaults to 0, can be changed via sysctl.

 CONFIG_SMP */

 Returns how long it waited in ms */

/*

 * Stop ourself in panic -- architecture code may override this

/*

 * Stop ourselves in NMI context if another CPU has already panicked. Arch code

 * may override this to prepare for crash dumping, e.g. save regs info.

/*

 * Stop other CPUs in panic.  Architecture dependent code may override this

 * with more suitable version.  For example, if the architecture supports

 * crash dump, it should save registers of each stopped CPU and disable

 * per-CPU features such as virtualization extensions.

	/*

	 * This function can be called twice in panic path, but obviously

	 * we execute this only once.

	/*

	 * Note smp_send_stop is the usual smp shutdown function, which

	 * unfortunately means it may not be hardened to work in a panic

	 * situation.

/*

 * A variant of panic() called from NMI context. We return if we've already

 * panicked on this CPU. If another CPU already panicked, loop in

 * nmi_panic_self_stop() which can provide architecture dependent code such

 * as saving register state for crash dump.

/**

 *	panic - halt the system

 *	@fmt: The text string to print

 *

 *	Display a message, then perform cleanups.

 *

 *	This function never returns.

	/*

	 * Disable local interrupts. This will prevent panic_smp_self_stop

	 * from deadlocking the first cpu that invokes the panic, since

	 * there is nothing to prevent an interrupt handler (that runs

	 * after setting panic_cpu) from invoking panic() again.

	/*

	 * It's possible to come here directly from a panic-assertion and

	 * not have preempt disabled. Some functions called from here want

	 * preempt to be disabled. No point enabling it later though...

	 *

	 * Only one CPU is allowed to execute the panic code from here. For

	 * multiple parallel invocations of panic, all other CPUs either

	 * stop themself or will wait until they are stopped by the 1st CPU

	 * with smp_send_stop().

	 *

	 * `old_cpu == PANIC_CPU_INVALID' means this is the 1st CPU which

	 * comes here, so go ahead.

	 * `old_cpu == this_cpu' means we came from nmi_panic() which sets

	 * panic_cpu to this CPU.  In this case, this is also the 1st CPU.

	/*

	 * Avoid nested stack-dumping if a panic occurs during oops processing

	/*

	 * If kgdb is enabled, give it a chance to run before we stop all

	 * the other CPUs or else we won't be able to debug processes left

	 * running on them.

	/*

	 * If we have crashed and we have a crash kernel loaded let it handle

	 * everything else.

	 * If we want to run this after calling panic_notifiers, pass

	 * the "crash_kexec_post_notifiers" option to the kernel.

	 *

	 * Bypass the panic_cpu check and call __crash_kexec directly.

		/*

		 * Note smp_send_stop is the usual smp shutdown function, which

		 * unfortunately means it may not be hardened to work in a

		 * panic situation.

		/*

		 * If we want to do crash dump after notifier calls and

		 * kmsg_dump, we will need architecture dependent extra

		 * works in addition to stopping other CPUs.

	/*

	 * Run any panic handlers, including those that might need to

	 * add information to the kmsg dump output.

	/*

	 * If you doubt kdump always works fine in any situation,

	 * "crash_kexec_post_notifiers" offers you a chance to run

	 * panic_notifiers and dumping kmsg before kdump.

	 * Note: since some panic_notifiers can make crashed kernel

	 * more unstable, it can increase risks of the kdump failure too.

	 *

	 * Bypass the panic_cpu check and call __crash_kexec directly.

	/*

	 * We may have ended up stopping the CPU holding the lock (in

	 * smp_send_stop()) while still having some valuable data in the console

	 * buffer.  Try to acquire the lock then release it regardless of the

	 * result.  The release will also print the buffers out.  Locks debug

	 * should be disabled to avoid reporting bad unlock balance when

	 * panic() is not being callled from OOPS.

		/*

		 * Delay timeout seconds before rebooting the machine.

		 * We can't use the "normal" timers since we just panicked.

		/*

		 * This will not be a clean reboot, with everything

		 * shutting down.  But if there is a chance of

		 * rebooting the system it will be rebooted.

 Make sure the user can actually press Stop-A (L1-A) */

 Do not scroll important messages printed above */

/*

 * TAINT_FORCED_RMMOD could be a per-module flag but the module

 * is being removed anyway.

/**

 * print_tainted - return a string to represent the kernel taint state.

 *

 * For individual taint flag meanings, see Documentation/admin-guide/sysctl/kernel.rst

 *

 * The string is overwritten by the next call to print_tainted(),

 * but is always NULL terminated.

/**

 * add_taint: add a taint flag if not already set.

 * @flag: one of the TAINT_* constants.

 * @lockdep_ok: whether lock debugging is still OK.

 *

 * If something bad has gone wrong, you'll want @lockdebug_ok = false, but for

 * some notewortht-but-not-corrupting cases, it can be set to true.

/*

 * It just happens that oops_enter() and oops_exit() are identically

 * implemented...

 This CPU may now print the oops message */

 We need to stall this CPU */

 This CPU gets to do the counting */

 This CPU waits for a different one */

/*

 * Return true if the calling CPU is allowed to print oops-related info.

 * This is a bit racy..

/*

 * Called when the architecture enters its oops handler, before it prints

 * anything.  If this is the first CPU to oops, and it's oopsing the first

 * time then let it proceed.

 *

 * This is all enabled by the pause_on_oops kernel boot option.  We do all

 * this to ensure that oopses don't scroll off the screen.  It has the

 * side-effect of preventing later-oopsing CPUs from mucking up the display,

 * too.

 *

 * It turns out that the CPU which is allowed to print ends up pausing for

 * the right duration, whereas all the other CPUs pause for twice as long:

 * once in oops_enter(), once in oops_exit().

 can't trust the integrity of the kernel anymore: */

/*

 * 64-bit random ID for oopses:

/*

 * Called when the architecture exits its oops handler, after printing

 * everything.

		/*

		 * This thread may hit another WARN() in the panic path.

		 * Resetting this prevents additional WARN() from panicking the

		 * system on this thread.  Other threads are blocked by the

		 * panic_mutex in panic().

 Just a warning, don't kill lockdep. */

 Support resetting WARN*_ONCE state */

 Don't care about failure */

/*

 * Called when gcc's -fstack-protector feature is used, and

 * gcc detects corruption of the on-stack canary value

 make sure panic_on_taint doesn't hold out-of-range TAINT flags */

 SPDX-License-Identifier: GPL-2.0-only

/**

 * copy_regset_to_user - fetch a thread's user_regset data into user memory

 * @target:	thread to be examined

 * @view:	&struct user_regset_view describing user thread machine state

 * @setno:	index in @view->regsets

 * @offset:	offset into the regset data, in bytes

 * @size:	amount of data to copy, in bytes

 * @data:	user-mode pointer to copy into

 SPDX-License-Identifier: GPL-2.0-only

/*

 * jump label support

 *

 * Copyright (C) 2009 Jason Baron <jbaron@redhat.com>

 * Copyright (C) 2011 Peter Zijlstra

 *

 mutex to protect coming/going of the jump_label table */

	/*

	 * Entrires are sorted by key.

	/*

	 * In the batching mode, entries should also be sorted by the code

	 * inside the already sorted list of entries, enabling a bsearch in

	 * the vector.

/*

 * There are similar definitions for the !CONFIG_JUMP_LABEL case in jump_label.h.

 * The use of 'atomic_read()' requires atomic.h and its problematic for some

 * kernel headers such as kernel.h and others. Since static_key_count() is not

 * used in the branch statements as it is for the !CONFIG_JUMP_LABEL case its ok

 * to have it be a function here. Similarly, for 'static_key_enable()' and

 * 'static_key_disable()', which require bug.h. This should allow jump_label.h

 * to be included from most/all places for CONFIG_JUMP_LABEL.

	/*

	 * -1 means the first static_key_slow_inc() is in progress.

	 *  static_key_enabled() must return true, so return 1 here.

	/*

	 * Careful if we get concurrent static_key_slow_inc() calls;

	 * later calls must wait for the first one to _finish_ the

	 * jump_label_update() process.  At the same time, however,

	 * the jump_label_update() call below wants to see

	 * static_key_enabled(&key) for jumps to be updated properly.

	 *

	 * So give a special meaning to negative key->enabled: it sends

	 * static_key_slow_inc() down the slow path, and it is non-zero

	 * so it counts as "enabled" in jump_label_update().  Note that

	 * atomic_inc_unless_negative() checks >= 0, so roll our own.

		/*

		 * Ensure that if the above cmpxchg loop observes our positive

		 * value, it must also observe all the text changes.

		/*

		 * See static_key_slow_inc().

	/*

	 * The negative count check is valid even when a negative

	 * key->enabled is in use by static_key_slow_inc(); a

	 * __static_key_slow_dec() before the first static_key_slow_inc()

	 * returns is unbalanced, because all other static_key_slow_inc()

	 * instances block while the update is in progress.

/*

 * Update code which is definitely not currently executing.

 * Architectures which need heavyweight synchronization to modify

 * running code can override this to make the non-live update case

 * cheaper.

/***

 * A 'struct static_key' uses a union such that it either points directly

 * to a table of 'struct jump_entry' or to a linked list of modules which in

 * turn point to 'struct jump_entry' tables.

 *

 * The two lower bits of the pointer are used to keep track of which pointer

 * type is in use and to store the initial branch direction, we use an access

 * function which preserves these bits.

 See the comment in linux/jump_label.h */

	/*

	 * Cannot update code that was in an init text area.

		/*

		 * This skips patching built-in __exit, which

		 * is part of init_section_contains() but is

		 * not part of kernel_text_address().

		 *

		 * Skipping built-in __exit is fine since it

		 * will never be executed.

			/*

			 * Queue is full: Apply the current queue and try again.

	/*

	 * Since we are initializing the static_key.enabled field with

	 * with the 'raw' int values (to avoid pulling in atomic.h) in

	 * jump_label.h, let's make sure that is safe. There are only two

	 * cases to check since we initialize to 0 or 1.

 rewrite NOPs */

 See the comment in linux/jump_label.h */

/***

 * key->type and key->next are the same via union.

 * This sets key->next and preserves the type bits.

 *

 * See additional comments above static_key_set_entries().

		/*

		 * NULL if the static_key is defined in a module

		 * that does not use it

/***

 * apply_jump_label_nops - patch module jump labels with arch_get_jump_label_nop()

 * @mod: module to patch

 *

 * Allow for run-time selection of the optimal nops. Before the module

 * loads patch these with arch_get_jump_label_nop(), which is specified by

 * the arch specific jump label code.

 if the module doesn't have jump label entries, just return */

 Only write NOPs for arch_branch_static(). */

 if the module doesn't have jump label entries, just return */

 Only update if we've changed from our initial state */

 No memory during module load */

 No memory during module load */

 if only one etry is left, fold it back into the static_key */

 higher than tracepoints */

 CONFIG_MODULES */

/***

 * jump_label_text_reserved - check if addr range is reserved

 * @start: start text addr

 * @end: end text addr

 *

 * checks if the text addr located between @start and @end

 * overlaps with any of the jump label patch addresses. Code

 * that wants to modify kernel text should first verify that

 * it does not overlap with any of the jump label addresses.

 * Caller must hold jump_label_mutex.

 *

 * returns 1 if there is an overlap, 0 otherwise

 if there are no users, entry can be NULL */

 STATIC_KEYS_SELFTEST */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * taskstats.c - Export per-task statistics to userland

 *

 * Copyright (C) Shailabh Nagar, IBM Corp. 2006

 *           (C) Balbir Singh,   IBM Corp. 2006

/*

 * Maximum length of a cpumask that can be specified in

 * the TASKSTATS_CMD_ATTR_REGISTER/DEREGISTER_CPUMASK attribute

	/*

	 * If new attributes are added, please revisit this allocation

/*

 * Send taskstats data in @skb to listener with nl_pid @pid

/*

 * Send taskstats data in @skb to listeners registered for @cpu's exit data

 Delete invalidated entries */

	/*

	 * Each accounting subsystem adds calls to its functions to

	 * fill in relevant parts of struct taskstsats as follows

	 *

	 *	per-task-foo(stats, tsk);

 fill in basic acct fields */

 fill in extended acct fields */

	/*

	 * Add additional stats from live tasks except zombie thread group

	 * leaders who are already counted with the dead tasks

		/*

		 * Accounting subsystem can call its functions here to

		 * fill in relevant parts of struct taskstsats as follows

		 *

		 *	per-task-foo(stats, tsk);

 calculate task elapsed time in nsec */

 Convert to micro seconds */

	/*

	 * Accounting subsystems can also add calls here to modify

	 * fields of taskstats.

	/*

	 * Each accounting subsystem calls its functions here to

	 * accumalate its per-task stats for tsk, into the per-tgid structure

	 *

	 *	per-task-foo(tsk->signal->stats, tsk);

 nop if NULL */

 Deregister or cleanup */

 Pairs with smp_store_release() below. */

 No problem if kmem_cache_zalloc() fails */

		/*

		 * Pairs with smp_store_release() above and order the

		 * kmem_cache_zalloc().

 Send pid data out on exit */

	/*

	 * Size includes space for nested attributes

 PID + STATS + TGID + STATS */

 fill the tsk->signal->stats structure */

	/*

	 * Doesn't matter if tsk is the leader or the last group member leaving

 Needed early in initialization */

/*

 * late initcall ensures initialization of statistics collection

 * mechanisms precedes initialization of the taskstats interface

 SPDX-License-Identifier: GPL-2.0-or-later

/* audit.c -- Auditing support

 * Gateway between the kernel (e.g., selinux) and the user-space audit daemon.

 * System-call specific features have moved to auditsc.c

 *

 * Copyright 2003-2007 Red Hat Inc., Durham, North Carolina.

 * All Rights Reserved.

 *

 * Written by Rickard E. (Rik) Faith <faith@redhat.com>

 *

 * Goals: 1) Integrate fully with Security Modules.

 *	  2) Minimal run-time overhead:

 *	     a) Minimal when syscall auditing is disabled (audit_enable=0).

 *	     b) Small when syscall auditing is enabled and no audit record

 *		is generated (defer as much work as possible to record

 *		generation time):

 *		i) context is allocated,

 *		ii) names from getname are stored without a copy, and

 *		iii) inode information stored from path_lookup.

 *	  3) Ability to disable syscall auditing at boot time (audit=0).

 *	  4) Usable by other parts of the kernel (if audit_log* is called,

 *	     then a syscall record will be generated automatically for the

 *	     current syscall).

 *	  5) Netlink interface to user-space.

 *	  6) Support low-overhead kernel-based filtering to minimize the

 *	     information that must be passed to user-space.

 *

 * Audit userspace, documentation, tests, and bug/issue trackers:

 * 	https://github.com/linux-audit

/* No auditing will take place until audit_initialized == AUDIT_INITIALIZED.

 Default state when kernel boots without any parameters. */

 If auditing cannot proceed, audit_failure selects what happens. */

 private audit network namespace index */

/**

 * struct audit_net - audit private network namespace data

 * @sk: communication socket

/**

 * struct auditd_connection - kernel/auditd connection state

 * @pid: auditd PID

 * @portid: netlink portid

 * @net: the associated network namespace

 * @rcu: RCU head

 *

 * Description:

 * This struct is RCU protected; you must either hold the RCU lock for reading

 * or the associated spinlock for writing.

/* If audit_rate_limit is non-zero, limit the rate of sending audit records

 * to that number per second.  This prevents DoS attacks, but results in

/* Number of outstanding audit_buffers allowed.

 The identity of the user shutting down the audit system. */

/* Records can be lost in several ways:

   0) [suppressed in audit_alloc]

   1) out of memory in audit_log_start [kmalloc of struct audit_buffer]

   2) out of memory in audit_log_move [alloc_skb]

   3) suppressed due to audit_rate_limit

   4) suppressed due to audit_backlog_limit

/* Monotonically increasing sum of time the kernel has spent

 * waiting while the backlog limit is exceeded.

 Hash for inode-based rules */

 queue msgs to send via kauditd_task */

 queue msgs due to temporary unicast send problems */

 queue msgs waiting for new auditd connection */

 queue servicing thread */

 waitqueue for callers who are blocked on the audit backlog */

/**

 * struct audit_ctl_mutex - serialize requests from userspace

 * @lock: the mutex used for locking

 * @owner: the task which owns the lock

 *

 * Description:

 * This is the lock struct used to ensure we only process userspace requests

 * in an orderly fashion.  We can't simply use a mutex/lock here because we

 * need to track lock ownership so we don't end up blocking the lock owner in

 * audit_log_start() or similar.

/* AUDIT_BUFSIZ is the size of the temporary buffer used for formatting

 * audit records.  Since printk uses a 1024 byte buffer, this buffer

/* The audit_buffer is used when formatting an audit record.  The caller

 * locks briefly to get the record off the freelist or to allocate the

 * buffer, and locks briefly to send the buffer to the netlink layer or

 * to place it on a transmit queue.  Multiple audit_buffers can be in

 formatted skb ready to send */

 NULL or associated context */

/**

 * auditd_test_task - Check to see if a given task is an audit daemon

 * @task: the task to check

 *

 * Description:

 * Return 1 if the task is a registered audit daemon, 0 otherwise.

/**

 * audit_ctl_lock - Take the audit control lock

/**

 * audit_ctl_unlock - Drop the audit control lock

/**

 * audit_ctl_owner_current - Test to see if the current task owns the lock

 *

 * Description:

 * Return true if the current task owns the audit control lock, false if it

 * doesn't own the lock.

/**

 * auditd_pid_vnr - Return the auditd PID relative to the namespace

 *

 * Description:

 * Returns the PID in relation to the namespace, 0 on failure.

/**

 * audit_get_sk - Return the audit socket for the given network namespace

 * @net: the destination network namespace

 *

 * Description:

 * Returns the sock pointer if valid, NULL otherwise.  The caller must ensure

 * that a reference is held for the network namespace while the sock is in use.

/**

 * audit_log_lost - conditionally log lost audit message event

 * @message: the message stating reason for lost audit message

 *

 * Emit at least 1 message per second, even if audit_rate_check is

 * throttling.

 * Always increment the lost messages counter.

 Something weird, deny request */

 check if we are locked */

 If we are allowed, make the change */

 Not allowed, update reason */

/**

 * auditd_conn_free - RCU helper to release an auditd connection struct

 * @rcu: RCU head

 *

 * Description:

 * Drop any references inside the auditd connection tracking struct and free

 * the memory.

/**

 * auditd_set - Set/Reset the auditd connection state

 * @pid: auditd PID

 * @portid: auditd netlink portid

 * @net: auditd network namespace pointer

 *

 * Description:

 * This function will obtain and drop network namespace references as

 * necessary.  Returns zero on success, negative values on failure.

/**

 * kauditd_printk_skb - Print the audit record to the ring buffer

 * @skb: audit record

 *

 * Whatever the reason, this packet may not make it to the auditd connection

 * so write it via printk so the information isn't completely lost.

/**

 * kauditd_rehold_skb - Handle a audit record send failure in the hold queue

 * @skb: audit record

 *

 * Description:

 * This should only be used by the kauditd_thread when it fails to flush the

 * hold queue.

 put the record back in the queue at the same place */

/**

 * kauditd_hold_skb - Queue an audit record, waiting for auditd

 * @skb: audit record

 *

 * Description:

 * Queue the audit record, waiting for an instance of auditd.  When this

 * function is called we haven't given up yet on sending the record, but things

 * are not looking good.  The first thing we want to do is try to write the

 * record via printk and then see if we want to try and hold on to the record

 * and queue it, if we have room.  If we want to hold on to the record, but we

 * don't have room, record a record lost message.

	/* at this point it is uncertain if we will ever send this to auditd so

 can we just silently drop the message? */

 if we have room, queue the message */

 we have no other options - drop the message */

/**

 * kauditd_retry_skb - Queue an audit record, attempt to send again to auditd

 * @skb: audit record

 *

 * Description:

 * Not as serious as kauditd_hold_skb() as we still have a connected auditd,

 * but for some reason we are having problems sending it audit records so

 * queue the given record and attempt to resend.

	/* NOTE: because records should only live in the retry queue for a

	 * short period of time, before either being sent or moved to the hold

/**

 * auditd_reset - Disconnect the auditd connection

 * @ac: auditd connection state

 *

 * Description:

 * Break the auditd/kauditd connection and move all the queued records into the

 * hold queue in case auditd reconnects.  It is important to note that the @ac

 * pointer should never be dereferenced inside this function as it may be NULL

 * or invalid, you can only compare the memory address!  If @ac is NULL then

 * the connection will always be reset.

 if it isn't already broken, break the connection */

 someone already registered a new auditd connection */

	/* flush the retry queue to the hold queue, but don't touch the main

/**

 * auditd_send_unicast_skb - Send a record via unicast to auditd

 * @skb: audit record

 *

 * Description:

 * Send a skb to the audit daemon, returns positive/zero values on success and

 * negative values on failure; in all cases the skb will be consumed by this

 * function.  If the send results in -ECONNREFUSED the connection with auditd

 * will be reset.  This function may sleep so callers should not hold any locks

 * where this would cause a problem.

	/* NOTE: we can't call netlink_unicast while in the RCU section so

	 *       take a reference to the network namespace and grab local

	 *       copies of the namespace, the sock, and the portid; the

	 *       namespace and sock aren't going to go away while we hold a

	 *       reference and if the portid does become invalid after the RCU

/**

 * kauditd_send_queue - Helper for kauditd_thread to flush skb queues

 * @sk: the sending sock

 * @portid: the netlink destination

 * @queue: the skb queue to process

 * @retry_limit: limit on number of netlink unicast failures

 * @skb_hook: per-skb hook for additional processing

 * @err_hook: hook called if the skb fails the netlink unicast send

 *

 * Description:

 * Run through the given queue and attempt to send the audit records to auditd,

 * returns zero on success, negative values on failure.  It is up to the caller

 * to ensure that the @sk is valid for the duration of this function.

 *

	/* NOTE: kauditd_thread takes care of all our locking, we just use

 call the skb_hook for each skb we touch */

 can we send to anyone via unicast? */

 grab an extra skb reference in case of error */

 fatal failure for our queue flush attempt? */

 yes - error processing for the queue */

 keep processing with the skb_hook */

 no - requeue to preserve ordering */

 it worked - drop the extra reference and continue */

/*

 * kauditd_send_multicast_skb - Send a record to any multicast listeners

 * @skb: audit record

 *

 * Description:

 * Write a multicast message to anyone listening in the initial network

 * namespace.  This function doesn't consume an skb as might be expected since

 * it has to copy it anyways.

	/* NOTE: we are not taking an additional reference for init_net since

	/*

	 * The seemingly wasteful skb_copy() rather than bumping the refcount

	 * using skb_get() is necessary because non-standard mods are made to

	 * the skb by the original kaudit unicast socket send routine.  The

	 * existing auditd daemon assumes this breakage.  Fixing this would

	 * require co-ordinating a change in the established protocol between

	 * the kaudit kernel subsystem and the auditd userspace code.  There is

	 * no reason for new multicast clients to continue with this

	 * non-compliance.

/**

 * kauditd_thread - Worker thread to send audit records to userspace

 * @dummy: unused

 NOTE: see the lock comments in auditd_send_unicast_skb() */

 attempt to flush the hold queue */

 attempt to flush the retry queue */

		/* process the main queue - do the multicast send and attempt

		 * unicast, dump failed record sends to the retry queue; if

		 * sk == NULL due to previous failures we will just do the

 drop our netns reference, no auditd sends past this line */

 we have processed all the queues so wake everyone */

		/* NOTE: we want to wake up if there is anything on the queue,

		 *       regardless of if an auditd is connected, as we need to

		 *       do the multicast send and rotate records from the

 wait for parent to finish and send an ACK */

	/* Ignore failure. It'll only happen if the sender goes away,

/**

 * audit_send_reply - send an audit reply message via netlink

 * @request_skb: skb of request we are replying to (used to target the reply)

 * @seq: sequence number

 * @type: audit message type

 * @done: done (last) flag

 * @multi: multi-part message flag

 * @payload: payload data

 * @size: payload size

 *

 * Allocates a skb, builds the netlink message, and sends it to the port id.

/*

 * Check for appropriate CAP_AUDIT_ capabilities on incoming audit

 * control messages.

 Only support initial user namespace for now. */

	/*

	 * We return ECONNREFUSED because it tricks userspace into thinking

	 * that audit was not configured into the kernel.  Lots of users

	 * configure their PAM stack (because that's what the distro does)

	 * to reject login if unable to send messages to audit.  If we return

	 * ECONNREFUSED the PAM stack thinks the kernel does not have audit

	 * configured in and will let login proceed.  If we return EPERM

	 * userspace will reject all logins.  This should be removed when we

	 * support non init namespaces!!

		/* Only support auditd and auditctl in initial pid namespace

 bad msg */

 if there is ever a version 2 we should handle that here */

 if we are not changing this feature, move along */

 are we changing a locked feature? */

 nothing invalid, do the changes */

 if we are not changing this feature, move along */

		/* NOTE: use pid_vnr() so the PID is relative to the current

 guard against past and future API changes */

			/* NOTE: we are using the vnr PID functions below

			 *       because the s.pid value is relative to the

			 *       namespace of the caller; at present this

			 *       doesn't matter much since you can really only

			 *       run auditd from the initial pid namespace, but

			/* Sanity check - PID values must match. Setting

 test the auditd connection */

 replacing a healthy auditd is not allowed */

 only current auditd can unregister itself */

 register a new auditd connection */

 try to process any backlog */

 unregister the auditd connection */

 exit early if there isn't at least one character to print */

 match or error */

 ensure NULL termination */

 OK, here comes... */

 guard against past and future API changes */

 check if new data is valid */

/**

 * audit_receive - receive messages from a netlink control socket

 * @skb: the message buffer

 *

 * Parse the provided skb and deal with any messages that may be present,

 * malformed skbs are discarded.

	/*

	 * len MUST be signed for nlmsg_next to be able to dec it below 0

	 * if the nlmsg_len was not aligned

 if err or if this message says it wants a response */

 Log information about who is connecting to the audit multicast socket */

 subj= */

 exe= */

 Run custom bind function on netlink socket group connect or bind requests. */

	/* NOTE: you would think that we would want to check the auditd

	 * connection and potentially reset it here if it lives in this

	 * namespace, but since the auditd connection tracking struct holds a

	 * reference to this namespace (see auditd_set()) we are only ever

 Initialize audit support at boot time. */

/*

 * Process kernel command-line parameter at boot time.

 * audit={0|off} or audit={1|on}.

/* Process kernel command-line parameter at boot time.

/**

 * audit_serial - compute a serial number for the audit record

 *

 * Compute a serial number for the audit record.  Audit records are

 * written to user-space as soon as they are generated, so a complete

 * audit record may be written in several pieces.  The timestamp of the

 * record and this serial number are used by the user-space tools to

 * determine which pieces belong to the same audit record.  The

 * (timestamp,serial) tuple is unique for each syscall and is live from

 * syscall entry to syscall exit.

 *

 * NOTE: Another possibility is to store the formatted records off the

 * audit context (for those records that have a context), and emit them

 * all at syscall exit.  However, this could delay the reporting of

 * significant errors until syscall exit (or never, if the system

 * halts).

/**

 * audit_log_start - obtain an audit buffer

 * @ctx: audit_context (may be NULL)

 * @gfp_mask: type of allocation

 * @type: audit message type

 *

 * Returns audit_buffer pointer on success or NULL on error.

 *

 * Obtain an audit buffer.  This routine does locking to obtain the

 * audit buffer, but then no locking is required for calls to

 * audit_log_*format.  If the task (ctx) is a task that is currently in a

 * syscall, then the syscall is marked as auditable and an audit record

 * will be written at syscall exit.  If there is no associated task, then

 * task context (ctx) should be NULL.

	/* NOTE: don't ever fail/sleep on these two conditions:

	 * 1. auditd generated record - since we need auditd to drain the

	 *    queue; also, when we are checking for auditd, compare PIDs using

	 *    task_tgid_vnr() since auditd_pid is set in audit_receive_msg()

	 *    using a PID anchored in the caller's namespace

	 * 2. generator holding the audit_cmd_mutex - we don't want to block

 wake kauditd to try and flush the queue */

			/* sleep if we are allowed and we haven't exhausted our

 cancel dummy context to enable supporting records */

/**

 * audit_expand - expand skb in the audit buffer

 * @ab: audit_buffer

 * @extra: space to add at tail of the skb

 *

 * Returns 0 (no space) on failed expansion, or available space if

 * successful.

/*

 * Format an audit message into the audit buffer.  If there isn't enough

 * room in the audit buffer, more room will be allocated and vsnprint

 * will be called a second time.  Currently, we assume that a printk

 * can't format message larger than 1024 bytes, so we don't either.

		/* The printk buffer is 1024 bytes long, so if we get

		 * here and AUDIT_BUFSIZ is at least 1024, then we can

/**

 * audit_log_format - format a message into the audit buffer.

 * @ab: audit_buffer

 * @fmt: format string

 * @...: optional parameters matching @fmt string

 *

 * All the work is done in audit_log_vformat.

/**

 * audit_log_n_hex - convert a buffer to hex and append it to the audit skb

 * @ab: the audit_buffer

 * @buf: buffer to convert to hex

 * @len: length of @buf to be converted

 *

 * No return value; failure to expand is silently ignored.

 *

 * This function will take the passed buf and convert it into a string of

 * ascii hex digits. The new string is placed onto the skb.

 Round the buffer request up to the next multiple */

 new string is twice the old string */

/*

 * Format a string of no more than slen characters into the audit buffer,

 * enclosed in quote marks.

 enclosing quotes + null terminator */

 don't include null terminator */

/**

 * audit_string_contains_control - does a string need to be logged in hex

 * @string: string to be checked

 * @len: max length of the string to check

/**

 * audit_log_n_untrustedstring - log a string that may contain random characters

 * @ab: audit_buffer

 * @len: length of string (not including trailing null)

 * @string: string to be logged

 *

 * This code will escape a string that is passed to it if the string

 * contains a control character, unprintable character, double quote mark,

 * or a space. Unescaped strings will start and end with a double quote mark.

 * Strings that are escaped are printed in hex (2 digits per char).

 *

 * The caller specifies the number of characters in the string to log, which may

 * or may not be the entire string.

/**

 * audit_log_untrustedstring - log a string that may contain random characters

 * @ab: audit_buffer

 * @string: string to be logged

 *

 * Same as audit_log_n_untrustedstring(), except that strlen is used to

 * determine string length.

 This is a helper-function to print the escaped d_path */

 We will allow 11 spaces for ' (deleted)' to be appended */

 Should never happen since we send PATH_MAX */

 FIXME: can we save some information here? */

/**

 * audit_log_path_denied - report a path restriction denial

 * @type: audit message type (AUDIT_ANOM_LINK, AUDIT_ANOM_CREAT, etc)

 * @operation: specific operation name

 Generate log with subject, operation, outcome. */

 global counter which is incremented every time something logs in */

 if we are unset, we don't need privs */

 if AUDIT_FEATURE_LOGINUID_IMMUTABLE means never ever allow a change*/

 it is set, you need permission */

 reject if this is not an unset and we don't allow that */

/**

 * audit_set_loginuid - set current task's loginuid

 * @loginuid: loginuid value

 *

 * Returns 0.

 *

 * Called (set) from fs/proc/base.c::proc_loginuid_write().

 are we setting or clearing? */

/**

 * audit_signal_info - record signal info for shutting down audit subsystem

 * @sig: signal value

 * @t: task being signaled

 *

 * If the audit subsystem is being terminated, record the task (pid)

 * and uid that is doing that.

/**

 * audit_log_end - end one audit record

 * @ab: the audit_buffer

 *

 * We can not do a netlink send inside an irq context because it blocks (last

 * arg, flags, is not set to MSG_DONTWAIT), so the audit buffer is placed on a

 * queue and a kthread is scheduled to remove them from the queue outside the

 * irq context.  May be called in any context.

		/* setup the netlink header, see the comments in

 queue the netlink packet and poke the kauditd thread */

/**

 * audit_log - Log an audit record

 * @ctx: audit context

 * @gfp_mask: type of allocation

 * @type: audit message type

 * @fmt: format string to use

 * @...: variable parameters matching the format string

 *

 * This is a convenience function that calls audit_log_start,

 * audit_log_vformat, and audit_log_end.  It may be called

 * in any context.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/fork.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

/*

 *  'fork.c' contains the help-routines for the 'fork' system call

 * (see also entry.S and others).

 * Fork is rather simple, once you get the hang of it, but the memory

 * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'

/*

 * Minimum number of threads to boot the kernel

/*

 * Maximum number of threads

/*

 * Protected counters by write_lock_irq(&tasklist_lock)

 Handle normal Linux uptimes. */

 The idle threads do not count.. */

 tunable limit on nr_threads */

 outer */

 #ifdef CONFIG_PROVE_RCU */

/*

 * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a

 * kmemcache based allocator.

/*

 * vmalloc() is a bit slow, and calling vfree() enough times will force a TLB

 * flush.  Try to minimize the number of calls by caching stacks.

 Mark stack accessible for KASAN. */

 Clear stale pointers from reused stack. */

	/*

	 * Allocated stacks are cached and later reused by new threads,

	 * so memcg accounting is performed manually on assigning/releasing

	 * stacks to tasks. Drop __GFP_ACCOUNT.

	/*

	 * We can't call find_vm_area() in interrupt context, and

	 * free_thread_stack() can be called in interrupt context,

	 * so cache the vm_struct.

 SLAB cache for signal_struct structures (tsk->signal) */

 SLAB cache for sighand_struct structures (tsk->sighand) */

 SLAB cache for files_struct structures (tsk->files) */

 SLAB cache for fs_struct structures (tsk->fs) */

 SLAB cache for vm_area_struct structures */

 SLAB cache for mm_struct structures (tsk->mm) */

		/*

		 * orig->shared.rb may be modified concurrently, but the clone

		 * will be reinitialized.

 All stack pages are in the same node. */

			/*

			 * If memcg_kmem_charge_page() fails, page's

			 * memory cgroup pointer is NULL, and

			 * memcg_kmem_uncharge_page() in free_thread_stack()

			 * will ignore this page.

 Better to leak the stack than to free prematurely */

	/*

	 * The task is finally done with both the stack and thread_info,

	 * so free both.

	/*

	 * If the task had a separate stack allocation, it should be gone

	 * by now.

	/*

	 * We depend on the oldmm having properly denied write access to the

	 * exe_file already.

	/*

	 * Not linked in yet - no deadlock potential:

 No ordering required: file already has been exposed. */

		/*

		 * Don't duplicate many vmas if we've been oom-killed (for

		 * example)

 sic */

			/*

			 * VM_WIPEONFORK gets a clean slate in the child.

			 * Don't prepare anon_vma until fault since we don't

			 * copy page for current vma.

 insert tmp into the share list, just after mpnt */

		/*

		 * Clear hugetlb-related page reserves for children. This only

		 * affects MAP_PRIVATE mappings. Faults generated by the child

		 * are not guaranteed to succeed, even if read-only

		/*

		 * Link in the new vma and copy the page table entries.

 a new mm has just been created */

 CONFIG_MMU */

/*

 * Called when the last reference to the mm

 * is dropped: either by a lazy thread or by

 * mmput. Free the page directory and the mm.

	/*

	 * __mmdrop is not safe to call from softirq context on x86 due to

	 * pgd_dtor so postpone it to the async context

/*

 * set_max_threads

	/*

	 * The number of threads shall be limited such that the thread

	 * structures may only consume a small part of the available memory.

 Initialized by the architecture: */

 Fetch thread_struct whitelist for the architecture. */

	/*

	 * Handle zero-sized whitelist or empty thread_struct, otherwise

	 * adjust offset to position of thread_struct in task_struct.

 CONFIG_ARCH_TASK_STRUCT_ALLOCATOR */

 create a slab on which task_structs can be allocated */

 do the arch specific task caches init */

 for overflow detection */

	/*

	 * arch_dup_task_struct() clobbers the stack-related fields.  Make

	 * sure they're properly initialized before using any stack-related

	 * functions again.

	/*

	 * We must handle setting up seccomp filters once we're under

	 * the sighand lock in case orig has changed between now and

	 * then. Until then, filter must be NULL to avoid messing up

	 * the usage counts on the error path calling free_task.

	/*

	 * One for the user space visible state that goes away when reaped.

	 * One for the scheduler.

 One for the rcu users */

/*

 * Allocate and initialize an mm_struct.

 must run before exit_mmap */

/*

 * Decrement the use count and release all resources for an mm.

/**

 * set_mm_exe_file - change a reference to the mm's executable file

 *

 * This changes mm's executable file (shown as symlink /proc/[pid]/exe).

 *

 * Main users are mmput() and sys_execve(). Callers prevent concurrent

 * invocations: in mmput() nobody alive left, in execve task is single

 * threaded.

 *

 * Can only fail if new_exe_file != NULL.

	/*

	 * It is safe to dereference the exe_file without RCU as

	 * this function is only called if nobody else can access

	 * this mm -- see comment above for justification.

		/*

		 * We expect the caller (i.e., sys_execve) to already denied

		 * write access, so this is unlikely to fail.

/**

 * replace_mm_exe_file - replace a reference to the mm's executable file

 *

 * This changes mm's executable file (shown as symlink /proc/[pid]/exe),

 * dealing with concurrent invocation and without grabbing the mmap lock in

 * write mode.

 *

 * Main user is sys_prctl(PR_SET_MM_MAP/EXE_FILE).

 Forbid mm->exe_file change if old file still mapped. */

 set the new file, lockless */

		/*

		 * Don't race with dup_mmap() getting the file and disallowing

		 * write access while someone might open the file writable.

/**

 * get_mm_exe_file - acquire a reference to the mm's executable file

 *

 * Returns %NULL if mm has no associated executable file.

 * User must release file via fput().

/**

 * get_task_exe_file - acquire a reference to the task's executable file

 *

 * Returns %NULL if task's mm (if any) has no associated executable file or

 * this is a kernel thread with borrowed mm (see the comment above get_task_mm).

 * User must release file via fput().

/**

 * get_task_mm - acquire a reference to the task's mm

 *

 * Returns %NULL if the task has no mm.  Checks PF_KTHREAD (meaning

 * this kernel workthread has transiently adopted a user mm with use_mm,

 * to do its AIO) is not set and if so returns a reference to it, after

 * bumping up the use count.  User must release the mm via mmput()

 * after use.  Typically used by /proc and ptrace.

/* Please note the differences between mmput and mm_release.

 * mmput is called whenever we stop holding onto a mm_struct,

 * error success whatever.

 *

 * mm_release is called after a mm_struct has been removed

 * from the current process.

 *

 * This difference is important for error handling, when we

 * only half set up a mm_struct for a new process and need to restore

 * the old one.  Because we mmput the new mm_struct before

 * restoring the old one. . .

 * Eric Biederman 10 January 1998

 Get rid of any cached register state */

	/*

	 * Signal userspace if we're not exiting with a core dump

	 * because we want to leave the value intact for debugging

	 * purposes.

			/*

			 * We don't check the error code - if userspace has

			 * not set up a proper pointer then tough luck.

	/*

	 * All done, finally we can wake up parent and return this mm to him.

	 * Also kthread_stop() uses this completion for synchronization.

/**

 * dup_mm() - duplicates an existing mm structure

 * @tsk: the task_struct with which the new mm will be associated.

 * @oldmm: the mm to duplicate.

 *

 * Allocates a new mm structure and duplicates the provided @oldmm structure

 * content into it.

 *

 * Return: the duplicated mm or NULL on failure.

 don't put binfmt in mmput, we haven't got module yet */

	/*

	 * Are we cloning a kernel thread?

	 *

	 * We need to steal a active VM for that..

 initialize the new vmacache entries */

 tsk->fs is already what we want */

	/*

	 * A background process may not have any files ...

	/*

	 * Share io context with parent, if CLONE_IO is set

 Reset all signal handler not set to SIG_IGN to SIG_DFL. */

		/*

		 * sighand_cachep is SLAB_TYPESAFE_BY_RCU so we can free it

		 * without an RCU grace period, see __lock_task_sighand().

/*

 * Initialize POSIX timer handling for a thread group.

 list_add(thread_node, thread_head) without INIT_LIST_HEAD() */

	/*

	 * Must be called with sighand->lock held, which is common to

	 * all threads in the group. Holding cred_guard_mutex is not

	 * needed because this new task is not yet running and cannot

	 * be racing exec.

 Ref-count the new filter user, and assign it. */

	/*

	 * Explicitly enable no_new_privs here in case it got set

	 * between the task_struct being duplicated and holding the

	 * sighand lock. The seccomp state and nnp must be in sync.

	/*

	 * If the parent gained a seccomp mode after copying thread

	 * flags and between before we held the sighand lock, we have

	 * to manually enable the seccomp thread flag here.

 #ifdef CONFIG_PREEMPT_RCU */

 #ifdef CONFIG_TASKS_RCU */

 #ifdef CONFIG_TASKS_TRACE_RCU */

/**

 * pidfd_show_fdinfo - print information about a pidfd

 * @m: proc fdinfo file

 * @f: file referencing a pidfd

 *

 * Pid:

 * This function will print the pid that a given pidfd refers to in the

 * pid namespace of the procfs instance.

 * If the pid namespace of the process is not a descendant of the pid

 * namespace of the procfs instance 0 will be shown as its pid. This is

 * similar to calling getppid() on a process whose parent is outside of

 * its pid namespace.

 *

 * NSpid:

 * If pid namespaces are supported then this function will also print

 * the pid of a given pidfd refers to for all descendant pid namespaces

 * starting from the current pid namespace of the instance, i.e. the

 * Pid field and the first entry in the NSpid field will be identical.

 * If the pid namespace of the process is not a descendant of the pid

 * namespace of the procfs instance 0 will be shown as its first NSpid

 * entry and no others will be shown.

 * Note that this differs from the Pid and NSpid fields in

 * /proc/<pid>/status where Pid and NSpid are always shown relative to

 * the  pid namespace of the procfs instance. The difference becomes

 * obvious when sending around a pidfd between pid namespaces from a

 * different branch of the tree, i.e. where no ancestral relation is

 * present between the pid namespaces:

 * - create two new pid namespaces ns1 and ns2 in the initial pid

 *   namespace (also take care to create new mount namespaces in the

 *   new pid namespace and mount procfs)

 * - create a process with a pidfd in ns1

 * - send pidfd from ns1 to ns2

 * - read /proc/self/fdinfo/<pidfd> and observe that both Pid and NSpid

 *   have exactly one entry, which is 0

		/* If nr is non-zero it means that 'pid' is valid and that

		 * ns, i.e. the pid namespace associated with the procfs

		 * instance, is in the pid namespace hierarchy of pid.

		 * Start at one below the already printed level.

/*

 * Poll support for process exit notification.

	/*

	 * Inform pollers only when the whole thread group exits.

	 * If the thread group leader exits before all other threads in the

	 * group, then poll(2) should block, similar to the wait(2) family.

 Skip if kernel thread */

 Skip if spawning a thread or using vfork */

 We need to synchronize with __set_oom_adj */

 Update the values in case they were changed after copy_signal */

/*

 * This creates a new process as a copy of the old one,

 * but does not actually start it yet.

 *

 * It copies the registers, and all the appropriate

 * parts of the process environment (as per the clone

 * flags). The actual kick-off is left to the caller.

	/*

	 * Don't allow sharing the root directory with processes in a different

	 * namespace

	/*

	 * Thread groups must share signals as well, and detached threads

	 * can only be started up within the thread group.

	/*

	 * Shared signal handlers imply shared VM. By way of the above,

	 * thread groups also imply shared VM. Blocking this case allows

	 * for various simplifications in other code.

	/*

	 * Siblings of global init remain as zombies on exit since they are

	 * not reaped by their parent (swapper). To solve this and to avoid

	 * multi-rooted process trees, prevent global and container-inits

	 * from creating siblings.

	/*

	 * If the new process will be in a different pid or user namespace

	 * do not allow it to share a thread group with the forking task.

	/*

	 * If the new process will be in a different time namespace

	 * do not allow it to share VM or a thread group with the forking task.

		/*

		 * - CLONE_DETACHED is blocked so that we can potentially

		 *   reuse it later for CLONE_PIDFD.

		 * - CLONE_THREAD is blocked until someone really needs it.

	/*

	 * Force any signals received before this point to be delivered

	 * before the fork happens.  Collect up signals sent to multiple

	 * processes that happen during the fork and delay them so that

	 * they appear to happen after the fork.

		/*

		 * Mark us an IO worker, and block any signal that isn't

		 * fatal or STOP

	/*

	 * This _must_ happen before we call free_task(), i.e. before we jump

	 * to any of the bad_fork_* labels. This is to avoid freeing

	 * p->set_child_tid which is (ab)used as a kthread's data pointer for

	 * kernel threads (PF_KTHREAD).

	/*

	 * Clear TID on mm_release()?

	/*

	 * If multiple threads are within copy_process(), then this check

	 * triggers too late. This doesn't hurt, the check is only there

	 * to stop root fork bombs.

 Must remain after dup_task_struct() */

 not blocked yet */

 Perform scheduler related setup. Assign this task to a CPU. */

 copy all the process information */

	/*

	 * This has to happen after we've potentially unshared the file

	 * descriptor table (so that the pidfd doesn't leak into the child

	 * if the fd table isn't shared).

 held by pidfile now */

	/*

	 * sigaltstack should be cleared when sharing the same VM

	/*

	 * Syscall tracing and stepping should be turned off in the

	 * child regardless of CLONE_PTRACE.

 ok, now we should be set up.. */

	/*

	 * Ensure that the cgroup subsystem policies allow the new process to be

	 * forked. It should be noted that the new process's css_set can be changed

	 * between here and cgroup_post_fork() if an organisation operation is in

	 * progress.

	/*

	 * From this point on we must avoid any synchronous user-space

	 * communication until we take the tasklist-lock. In particular, we do

	 * not want user-space to be able to predict the process start-time by

	 * stalling fork(2) after we recorded the start_time but before it is

	 * visible to the system.

	/*

	 * Make it visible to the rest of the system, but dont wake it up yet.

	 * Need tasklist lock for parent etc handling!

 CLONE_PARENT re-uses the old parent */

	/*

	 * Copy seccomp details explicitly here, in case they were changed

	 * before holding sighand lock.

 Don't start children in a dying pid namespace */

 Let kill terminate clone/fork in the middle */

 past the last point of failure */

			/*

			 * Inherit has_child_subreaper flag under the same

			 * tasklist_lock with adding child to the process tree

			 * for propagate_has_child_subreaper optimization.

 blocking */

 blocking */

 not really needed */

/*

 * This is like kernel_clone(), but shaved down and tailored to just

 * creating io_uring workers. It returns a created task, or an error pointer.

 * The returned task is inactive, and the caller must fire it up through

 * wake_up_new_task(p). All signals are blocked in the created task.

/*

 *  Ok, this is the main fork-routine.

 *

 * It copies the process, and if successful kick-starts

 * it and waits for it to finish using the VM if required.

 *

 * args->exit_signal is expected to be checked for sanity by the caller.

	/*

	 * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument

	 * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are

	 * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate

	 * field in struct clone_args and it still doesn't make sense to have

	 * them both point at the same memory location. Performing this check

	 * here has the advantage that we don't need to have a separate helper

	 * to check for legacy clone().

	/*

	 * Determine whether and which event to report to ptracer.  When

	 * called from kernel_thread or CLONE_UNTRACED is explicitly

	 * requested, no event is reported; otherwise, report if the event

	 * for the type of forking is enabled.

	/*

	 * Do this prior waking up the new thread - the thread pointer

	 * might get invalid after that point, if the thread exits quickly.

 forking complete and child started to run, tell ptracer */

/*

 * Create a kernel thread.

 can not support in nommu mode */

	/*

	 * Verify that higher 32bits of exit_signal are unset and that

	 * it is a valid signal

/**

 * clone3_stack_valid - check and prepare stack

 * @kargs: kernel clone args

 *

 * Verify that the stack arguments userspace gave us are sane.

 * In addition, set the stack direction for userspace since it's easy for us to

 * determine.

 Verify that no unknown flags are passed along. */

	/*

	 * - make the CLONE_DETACHED bit reusable for clone3

	 * - make the CSIGNAL bits reusable for clone3

/**

 * clone3 - create a new process with specific properties

 * @uargs: argument structure

 * @size:  size of @uargs

 *

 * clone3() is the extensible successor to clone()/clone2().

 * It takes a struct as argument that is versioned by its size.

 *

 * Return: On success, a positive PID for the child process.

 *         On error, a negative errno number.

	/*

	 * The mm_cpumask is located at the end of mm_struct, and is

	 * dynamically sized based on the maximum CPU number this system

	 * can have, taking hotplug into account (nr_cpu_ids).

/*

 * Check constraints on flags passed to the unshare system call.

	/*

	 * Not implemented, but pretend it works if there is nothing

	 * to unshare.  Note that unsharing the address space or the

	 * signal handlers also need to unshare the signal queues (aka

	 * CLONE_THREAD).

/*

 * Unshare the filesystem structure if it is being shared

 don't need lock here; in the worst case we'll do useless copy */

/*

 * Unshare file descriptor table if it is being shared

/*

 * unshare allows a process to 'unshare' part of the process

 * context which was originally shared using clone.  copy_*

 * functions used by kernel_clone() cannot be used here directly

 * because they modify an inactive task_struct that is being

 * constructed. Here we are modifying the current, active,

 * task_struct.

	/*

	 * If unsharing a user namespace must also unshare the thread group

	 * and unshare the filesystem root and working directories.

	/*

	 * If unsharing vm, must also unshare signal handlers.

	/*

	 * If unsharing a signal handlers, must also unshare the signal queues.

	/*

	 * If unsharing namespace, must also unshare filesystem information.

	/*

	 * CLONE_NEWIPC must also detach from the undolist: after switching

	 * to a new ipc namespace, the semaphore arrays from the old

	 * namespace are unreachable.

			/*

			 * CLONE_SYSVSEM is equivalent to sys_exit().

 Orphan segments in old ns (see sem above). */

 Install the new user namespace */

/*

 *	Helper to unshare the files of the current task.

 *	We don't want to expose copy_files internals to

 *	the exec layer of the kernel.

 SPDX-License-Identifier: GPL-2.0+



 Torture test for smp_call_function() and friends.



 Copyright (C) Facebook, 2020.



 Author: Paul E. McKenney <paulmck@kernel.org>

 Wait until there are multiple CPUs before starting test.

 Data for random primitive selection

 Need wait and no-wait versions of each,

  except for SCF_PRIM_RESCHED and

  SCF_PRIM_SINGLE_RPC.

 Communicate between caller and handler.

 -1 for not _single().

 Use to wait for all threads to start.

 An alternative IPI vector.

 Print torture statistics.  Caller must ensure serialization.

 Periodically prints torture statistics, if periodic statistics printing

 was specified via the stat_interval module parameter.

 Add a primitive to the scf_sel_array[].

 If no weight, if array would overflow, if computing three-place

 percentages would overflow, or if the scf_prim_name[] array would

 overflow, don't bother.  In the last three two cases, complain.

 Dump out weighting percentages for scf_prim_name[] array.

 Randomly pick a primitive and wait/nowait, based on weightings.

 Update statistics and occasionally burn up mass quantities of CPU time,

 if told to do so via scftorture.longwait.  Otherwise, occasionally burn

 a little bit.

 For multiple receivers.

 As above, but check for correct CPU.

 Randomly do an smp_call_function*() invocation.

 Prevent race-reduction compiler optimizations.

 Prevent race-reduction compiler optimizations.

 Prevent race-reduction compiler optimizations.

 Prevent race-reduction compiler optimizations.

 Leak rather than trash!

 Prevent race-reduction compiler optimizations.

 SCF test kthread.  Repeatedly does calls to members of the

 smp_call_function() family of functions.

 Make sure that the CPU is affinitized appropriately during testing.

 -After- the stats thread is stopped!

 -After- the last stats print has completed!

 Worker tasks invoking smp_call_function().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * async.c: Asynchronous function calls for boot performance

 *

 * (C) Copyright 2009 Intel Corporation

 * Author: Arjan van de Ven <arjan@linux.intel.com>

/*



Goals and Theory of Operation



The primary goal of this feature is to reduce the kernel boot time,

by doing various independent hardware delays and discovery operations

decoupled and not strictly serialized.



More specifically, the asynchronous function call concept allows

certain operations (primarily during system boot) to happen

asynchronously, out of order, while these operations still

have their externally visible parts happen sequentially and in-order.

(not unlike how out-of-order CPUs retire their instructions in order)



Key to the asynchronous function call implementation is the concept of

a "sequence cookie" (which, although it has an abstracted type, can be

thought of as a monotonically incrementing number).



The async core will assign each scheduled event such a sequence cookie and

pass this to the called functions.



The asynchronously called function should before doing a globally visible

operation, such as registering device numbers, call the

async_synchronize_cookie() function and pass in its own cookie. The

async_synchronize_cookie() function will make sure that all asynchronous

operations that were scheduled prior to the operation corresponding with the

cookie have completed.



Subsystem/driver initialization code that scheduled asynchronous probe

functions, but which shares global resources with other drivers/subsystems

that do not use the asynchronous call feature, need to do a full

synchronization with the async_synchronize_full() function, before returning

from their init function. This is to maintain strict ordering between the

asynchronous and synchronous parts of the kernel.



 infinity cookie */

 pending from all registered doms */

/*

 * pick the first pending entry and run it

 1) run (and print duration) */

 2) remove self from the pending queues */

 3) free the entry */

 4) wake up any waiters */

/**

 * async_schedule_node_domain - NUMA specific version of async_schedule_domain

 * @func: function to execute asynchronously

 * @data: data pointer to pass to the function

 * @node: NUMA node that we want to schedule this on or close to

 * @domain: the domain

 *

 * Returns an async_cookie_t that may be used for checkpointing later.

 * @domain may be used in the async_synchronize_*_domain() functions to

 * wait within a certain synchronization domain rather than globally.

 *

 * Note: This function may be called from atomic or non-atomic contexts.

 *

 * The node requested will be honored on a best effort basis. If the node

 * has no CPUs associated with it then the work is distributed among all

 * available CPUs.

 allow irq-off callers */

	/*

	 * If we're out of memory or if there's too much work

	 * pending already, we execute synchronously.

 low on memory.. run synchronously */

 allocate cookie and queue */

 mark that this task has queued an async job, used by module init */

 schedule for execution */

/**

 * async_schedule_node - NUMA specific version of async_schedule

 * @func: function to execute asynchronously

 * @data: data pointer to pass to the function

 * @node: NUMA node that we want to schedule this on or close to

 *

 * Returns an async_cookie_t that may be used for checkpointing later.

 * Note: This function may be called from atomic or non-atomic contexts.

 *

 * The node requested will be honored on a best effort basis. If the node

 * has no CPUs associated with it then the work is distributed among all

 * available CPUs.

/**

 * async_synchronize_full - synchronize all asynchronous function calls

 *

 * This function waits until all asynchronous function calls have been done.

/**

 * async_synchronize_full_domain - synchronize all asynchronous function within a certain domain

 * @domain: the domain to synchronize

 *

 * This function waits until all asynchronous function calls for the

 * synchronization domain specified by @domain have been done.

/**

 * async_synchronize_cookie_domain - synchronize asynchronous function calls within a certain domain with cookie checkpointing

 * @cookie: async_cookie_t to use as checkpoint

 * @domain: the domain to synchronize (%NULL for all registered domains)

 *

 * This function waits until all asynchronous function calls for the

 * synchronization domain specified by @domain submitted prior to @cookie

 * have been done.

/**

 * async_synchronize_cookie - synchronize asynchronous function calls with cookie checkpointing

 * @cookie: async_cookie_t to use as checkpoint

 *

 * This function waits until all asynchronous function calls prior to @cookie

 * have been done.

/**

 * current_is_async - is %current an async worker task?

 *

 * Returns %true if %current is an async worker task.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kexec.c - kexec_load system call

 * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>

 Verify we have a valid entry point */

 Allocate and initialize a controlling structure */

 Enable special crash kernel control page alloc policy. */

	/*

	 * Find a location for the control code buffer, and add it

	 * the vector of segments so that it's pages will also be

	 * counted as destination pages.

	/*

	 * Because we write directly to the reserved memory region when loading

	 * crash kernels we need a mutex here to prevent multiple crash kernels

	 * from attempting to load simultaneously, and to prevent a crash kernel

	 * from loading over the top of a in use crash kernel.

	 *

	 * KISS: always take the mutex.

 Uninstall image */

		/*

		 * Loading another kernel to switch to if this one

		 * crashes.  Free any current crash dump kernel before

		 * we corrupt it.

	/*

	 * Some architecture(like S390) may touch the crash memory before

	 * machine_kexec_prepare(), we must copy vmcoreinfo data after it.

 Install the new kernel and uninstall the old */

/*

 * Exec Kernel system call: for obvious reasons only root may call it.

 *

 * This call breaks up into three pieces.

 * - A generic part which loads the new kernel from the current

 *   address space, and very carefully places the data in the

 *   allocated pages.

 *

 * - A generic part that interacts with the kernel and tells all of

 *   the devices to shut down.  Preventing on-going dmas, and placing

 *   the devices in a consistent state so a later kernel can

 *   reinitialize them.

 *

 * - A machine specific part that includes the syscall number

 *   and then copies the image to it's final destination.  And

 *   jumps into the image at entry.

 *

 * kexec does not sync, or unmount filesystems so if you need

 * that to happen you need to do that yourself.

 We only trust the superuser with rebooting the system. */

 Permit LSMs and IMA to fail the kexec */

	/*

	 * kexec can be used to circumvent module loading restrictions, so

	 * prevent loading in that case

	/*

	 * Verify we have a legal set of flags

	 * This leaves us room for future extensions.

	/* Put an artificial cap on the number

	 * of segments passed to kexec_load.

 Verify we are on the appropriate architecture */

	/* Don't allow clients that don't understand the native

	 * architecture to do anything.

 SPDX-License-Identifier: GPL-2.0-only

 Allow users with CAP_SYS_RESOURCE unrestrained access */

 Allow all others at most read-only access */

 CONFIG_SYSCTL */

 Returns true on a successful get, false if the count wraps. */

 Silence compiler warning */

 Caller must hold a reference to ucounts */

		/*

		 * Grab an extra ucount reference for the caller when

		 * the rlimit count was previously 0.

	/*

	 * It is necessary to register the user directory in the

	 * default set so that registrations in the child sets work

	 * properly.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sysctl.c: General linux system control interface

 *

 * Begun 24 March 1995, Stephen Tweedie

 * Added /proc support, Dec 1995

 * Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.

 * Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.

 * Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.

 * Dynamic registration fixes, Stephen Tweedie.

 * Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.

 * Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris

 *  Horn.

 * Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.

 * Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.

 * Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill

 *  Wendling.

 * The list_for_each() macro wasn't appropriate for the sysctl loop.

 *  Removed it and replaced it with older style, 03/23/00, Bill Wendling

 Constants used for minimum and  maximum */

 this is needed for the proc_doulongvec_minmax of vm_dirty_bytes */

 this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */

/*

 * This is needed for proc_doulongvec_minmax of sysctl_hung_task_timeout_secs

 * and hung_task_check_interval_secs

/**

 * enum sysctl_writes_mode - supported sysctl write modes

 *

 * @SYSCTL_WRITES_LEGACY: each write syscall must fully contain the sysctl value

 *	to be written, and multiple writes on the same sysctl file descriptor

 *	will rewrite the sysctl value, regardless of file position. No warning

 *	is issued when the initial position is not 0.

 * @SYSCTL_WRITES_WARN: same as above but warn when the initial file position is

 *	not 0.

 * @SYSCTL_WRITES_STRICT: writes to numeric sysctl entries must always be at

 *	file position 0 and the value must be fully contained in the buffer

 *	sent to the write syscall. If dealing with strings respect the file

 *	position, but restrict this to the max length of the buffer, anything

 *	passed the max length will be ignored. Multiple writes will append

 *	to the buffer.

 *

 * These write modes control how current file position affects the behavior of

 * updating sysctl values through the proc interface on each write.

 CONFIG_PROC_SYSCTL */

 CONFIG_SYSCTL */

 CONFIG_BPF_SYSCALL && CONFIG_SYSCTL */

/*

 * /proc/sys support

 Only continue writes not past the end of buffer. */

 Start writing from beginning of buffer. */

/**

 * proc_first_pos_non_zero_ignore - check if first position is allowed

 * @ppos: file position

 * @table: the sysctl table

 *

 * Returns true if the first position is non-zero and the sysctl_writes_strict

 * mode indicates this is not allowed for numeric input types. String proc

 * handlers can ignore the return value.

/**

 * proc_dostring - read a string sysctl

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes a string from/to the user buffer. If the kernel

 * buffer provided is not large enough to hold the string, the

 * string is truncated. The copied string is %NULL-terminated.

 * If the string is being read by the user process, it is copied

 * and a newline '\n' is added. It is truncated if the buffer is

 * not large enough.

 *

 * Returns 0 on success.

/**

 * strtoul_lenient - parse an ASCII formatted integer from a buffer and only

 *                   fail on overflow

 *

 * @cp: kernel buffer containing the string to parse

 * @endp: pointer to store the trailing characters

 * @base: the base to use

 * @res: where the parsed integer will be stored

 *

 * In case of success 0 is returned and @res will contain the parsed integer,

 * @endp will hold any trailing characters.

 * This function will fail the parse on overflow. If there wasn't an overflow

 * the function will defer the decision what characters count as invalid to the

 * caller.

/**

 * proc_get_long - reads an ASCII formatted integer from a user buffer

 *

 * @buf: a kernel buffer

 * @size: size of the kernel buffer

 * @val: this is where the number will be stored

 * @neg: set to %TRUE if number is negative

 * @perm_tr: a vector which contains the allowed trailers

 * @perm_tr_len: size of the perm_tr vector

 * @tr: pointer to store the trailer character

 *

 * In case of success %0 is returned and @buf and @size are updated with

 * the amount of bytes read. If @tr is non-NULL and a trailing

 * character exists (size is non-zero after returning from this

 * function), @tr is updated with the trailing character.

	/* We don't know if the next char is whitespace thus we may accept

	 * invalid integers (e.g. 1234...a) or two integers instead of one

/**

 * proc_put_long - converts an integer to a decimal ASCII formatted string

 *

 * @buf: the user buffer

 * @size: the size of the user buffer

 * @val: the integer to be converted

 * @neg: sign of the number, %TRUE for negative

 *

 * In case of success @buf and @size are updated with the amount of bytes

 * written.

 This is in keeping with old __do_proc_dointvec() */

	/*

	 * Arrays are not supported, keep this simple. *Do not* add

	 * support for them.

/**

 * proc_dobool - read/write a bool

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string.

 *

 * Returns 0 on success.

/**

 * proc_dointvec - read a vector of integers

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string. 

 *

 * Returns 0 on success.

/**

 * proc_douintvec - read a vector of unsigned integers

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) unsigned integer

 * values from/to the user buffer, treated as an ASCII string.

 *

 * Returns 0 on success.

/*

 * Taint values can only be increased

 * This means we can safely use a temporary.

		/*

		 * If we are relying on panic_on_taint not producing

		 * false positives due to userspace input, bail out

		 * before setting the requested taint flags.

		/*

		 * Poor man's atomic or. Not worth adding a primitive

		 * to everyone's atomic.h for this

/**

 * struct do_proc_dointvec_minmax_conv_param - proc_dointvec_minmax() range checking structure

 * @min: pointer to minimum allowable value

 * @max: pointer to maximum allowable value

 *

 * The do_proc_dointvec_minmax_conv_param structure provides the

 * minimum and maximum values for doing range checking for those sysctl

 * parameters that use the proc_dointvec_minmax() handler.

	/*

	 * If writing, first do so via a temporary local int so we can

	 * bounds-check it before touching *valp.

/**

 * proc_dointvec_minmax - read a vector of integers with min/max values

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string.

 *

 * This routine will ensure the values are within the range specified by

 * table->extra1 (min) and table->extra2 (max).

 *

 * Returns 0 on success or -EINVAL on write when the range check fails.

/**

 * struct do_proc_douintvec_minmax_conv_param - proc_douintvec_minmax() range checking structure

 * @min: pointer to minimum allowable value

 * @max: pointer to maximum allowable value

 *

 * The do_proc_douintvec_minmax_conv_param structure provides the

 * minimum and maximum values for doing range checking for those sysctl

 * parameters that use the proc_douintvec_minmax() handler.

 write via temporary local uint for bounds-checking */

/**

 * proc_douintvec_minmax - read a vector of unsigned ints with min/max values

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) unsigned integer

 * values from/to the user buffer, treated as an ASCII string. Negative

 * strings are not allowed.

 *

 * This routine will ensure the values are within the range specified by

 * table->extra1 (min) and table->extra2 (max). There is a final sanity

 * check for UINT_MAX to avoid having to support wrap around uses from

 * userspace.

 *

 * Returns 0 on success or -ERANGE on write when the range check fails.

/**

 * proc_dou8vec_minmax - read a vector of unsigned chars with min/max values

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(u8) unsigned chars

 * values from/to the user buffer, treated as an ASCII string. Negative

 * strings are not allowed.

 *

 * This routine will ensure the values are within the range specified by

 * table->extra1 (min) and table->extra2 (max).

 *

 * Returns 0 on success or an error on write when the range check fails.

 Do not support arrays yet. */

/**

 * proc_doulongvec_minmax - read a vector of long integers with min/max values

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long

 * values from/to the user buffer, treated as an ASCII string.

 *

 * This routine will ensure the values are within the range specified by

 * table->extra1 (min) and table->extra2 (max).

 *

 * Returns 0 on success.

/**

 * proc_doulongvec_ms_jiffies_minmax - read a vector of millisecond values with min/max values

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned long) unsigned long

 * values from/to the user buffer, treated as an ASCII string. The values

 * are treated as milliseconds, and converted to jiffies when they are stored.

 *

 * This routine will ensure the values are within the range specified by

 * table->extra1 (min) and table->extra2 (max).

 *

 * Returns 0 on success.

/**

 * proc_dointvec_jiffies - read a vector of integers as seconds

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string. 

 * The values read are assumed to be in seconds, and are converted into

 * jiffies.

 *

 * Returns 0 on success.

/**

 * proc_dointvec_userhz_jiffies - read a vector of integers as 1/USER_HZ seconds

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: pointer to the file position

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string. 

 * The values read are assumed to be in 1/USER_HZ seconds, and 

 * are converted into jiffies.

 *

 * Returns 0 on success.

/**

 * proc_dointvec_ms_jiffies - read a vector of integers as 1 milliseconds

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 * @ppos: the current position in the file

 *

 * Reads/writes up to table->maxlen/sizeof(unsigned int) integer

 * values from/to the user buffer, treated as an ASCII string. 

 * The values read are assumed to be in 1/1000 seconds, and 

 * are converted into jiffies.

 *

 * Returns 0 on success.

/**

 * proc_do_large_bitmap - read/write from/to a large bitmap

 * @table: the sysctl table

 * @write: %TRUE if this is a write to the sysctl file

 * @buffer: the user buffer

 * @lenp: the size of the user buffer

 * @ppos: file position

 *

 * The bitmap is stored at table->data and the bitmap length (in bits)

 * in table->maxlen.

 *

 * We use a range comma separated format (e.g. 1,3-4,10-10) so that

 * large bitmaps may be represented in a compact manner. Writing into

 * the file will clear the bitmap then update it with the given input.

 *

 * Returns 0 on success.

 How much of the buffer we'll skip this pass */

 In case we stop parsing mid-number, we can reset */

			/*

			 * If we consumed the entirety of a truncated buffer or

			 * only one char is left (may be a "-"), then stop here,

			 * reset, & come back for more.

				/*

				 * If we consumed all of a truncated buffer or

				 * then stop here, reset, & come back for more.

 CONFIG_PROC_SYSCTL */

 CONFIG_PROC_SYSCTL */

 CONFIG_SCHEDSTATS */

 CONFIG_TASK_DELAY_ACCT */

 filled in by handler */

 CONFIG_NUMA_BALANCING */

 only handle a transition from default "0" to "1" */

 only handle a transition from default "0" to "1" */

 CONFIG_SMP */

 CONFIG_SMP */

 CONFIG_SMP */

 CONFIG_SMP */

	/*

	 * User-space scripts rely on the existence of this file

	 * as a feature check for perf_events being enabled.

	 *

	 * So it's an ABI, do not remove!

 CONFIG_COMPACTION */

 CONFIG_AIO */

 CONFIG_SYSCTL */

/*

 * No sense putting this after each symbol definition, twice,

 * exception granted :-)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Uniprocessor-only support functions.  The counterpart to kernel/smp.c

/*

 * Preemption is disabled here to make sure the cond_func is called under the

 * same conditions in UP and SMP.

 SPDX-License-Identifier: GPL-2.0

 Architectures may override COND_SYSCALL and COND_SYSCALL_COMPAT */

 CONFIG_ARCH_HAS_SYSCALL_WRAPPER */

/*  we can't #include <linux/syscalls.h> here,

/*

 * Non-implemented system calls get redirected here.

 COND_SYSCALL */

 COND_SYSCALL_COMPAT */

/*

 * This list is kept in the same order as include/uapi/asm-generic/unistd.h.

 * Architecture specific entries go below, followed by deprecated or obsolete

 * system calls.

 fs/xattr.c */

 fs/dcache.c */

 fs/cookies.c */

 fs/eventfd.c */

 fs/eventfd.c */

 fs/fcntl.c */

 fs/inotify_user.c */

 fs/ioctl.c */

 fs/ioprio.c */

 fs/locks.c */

 fs/namei.c */

 fs/namespace.c */

 fs/nfsctl.c */

 fs/open.c */

 fs/pipe.c */

 fs/quota.c */

 fs/readdir.c */

 fs/read_write.c */

 fs/sendfile.c */

 fs/select.c */

 fs/signalfd.c */

 fs/splice.c */

 fs/stat.c */

 fs/sync.c */

 fs/timerfd.c */

 fs/utimes.c */

 kernel/acct.c */

 kernel/capability.c */

 kernel/exec_domain.c */

 kernel/exit.c */

 kernel/fork.c */

 __ARCH_WANT_SYS_CLONE3 */

 kernel/futex/syscalls.c */

 kernel/hrtimer.c */

 kernel/itimer.c */

 kernel/kexec.c */

 kernel/module.c */

 kernel/posix-timers.c */

 kernel/printk.c */

 kernel/ptrace.c */

 kernel/sched/core.c */

 kernel/sys.c */

 kernel/time.c */

 kernel/timer.c */

 ipc/mqueue.c */

 ipc/msg.c */

 ipc/sem.c */

 ipc/shm.c */

 net/socket.c */

 mm/filemap.c */

 mm/nommu.c, also with MMU */

 security/keys/keyctl.c */

 security/landlock/syscalls.c */

 arch/example/kernel/sys_example.c */

 mm/fadvise.c */

 mm/, CONFIG_MMU only */

/*

 * Architecture specific syscalls: see further below

 fanotify */

 open by handle */

 compare kernel pointers */

 operate on Secure Computing state */

 access BPF programs and maps */

 execveat */

 membarrier */

 memory protection keys */

 memfd_secret */

/*

 * Architecture specific weak syscall entries.

 pciconfig: alpha, arm, arm64, ia64, sparc */

 sys_socketcall: arm, mips, x86, ... */

 compat syscalls for arm64, x86, ... */

 x86 */

 s390 */

 powerpc */

/*

 * Deprecated system calls which are still defined in

 * include/uapi/asm-generic/unistd.h and wanted by >= 1 arch

 __ARCH_WANT_SYSCALL_NO_FLAGS */

 __ARCH_WANT_SYSCALL_OFF_T */

 __ARCH_WANT_SYSCALL_DEPRECATED */

 optional: time32 */

/*

 * The syscalls below are not found in include/uapi/asm-generic/unistd.h

 obsolete: SGETMASK_SYSCALL */

 obsolete: SYSFS_SYSCALL */

 obsolete: __ARCH_WANT_SYS_IPC */

 obsolete: UID16 */

 restartable sequence */

 SPDX-License-Identifier: GPL-2.0-or-later

/* auditfilter.c -- filtering of audit events

 *

 * Copyright 2003-2004 Red Hat, Inc.

 * Copyright 2005 Hewlett-Packard Development Company, L.P.

 * Copyright 2005 IBM Corporation

/*

 * Locking model:

 *

 * audit_filter_mutex:

 *		Synchronizes writes and blocking reads of audit's filterlist

 *		data.  Rcu is used to traverse the filterlist and access

 *		contents of structs audit_entry, audit_watch and opaque

 *		LSM rules during filtering.  If modified, these structures

 *		must be copied and replace their counterparts in the filterlist.

 *		An audit_parent struct is not accessed during filtering, so may

 *		be written directly provided audit_filter_mutex is held.

 Audit filter lists, defined in <linux/audit.h> */

 some rules don't have associated watches */

 Initialize an audit filterlist entry. */

/* Unpack a filter field's string representation from user-space

	/* Of the currently implemented string fields, PATH_MAX

	 * defines the longest valid length.

 Translate an inode field to kernel representation. */

		/* When arch is unspecified, we must check both masks on biarch

 native */

 32bit on biarch */

 Common user-space to kernel rule translation. */

 check if an audit field is valid */

 Check for valid field type and op */

 <uapi/linux/personality.h> */

 all ops are valid */

 bit ops are only useful on syscall args */

 only equal and not equal valid ops */

 field not recognized */

 Check for select valid field values */

 Translate struct audit_rule_data to kernel's rule representation. */

 Support legacy tests for a valid loginuid */

			/* Keep currently invalid fields around in case they

 that's the temporary one */

 that's the template one */

 Pack a filter field's string representation into data block. */

 Translate kernel rule representation to struct audit_rule_data. */

 if set */

/* Compare two rules in kernel format.  Considered success if rules

 both filterkeys exist based on above type compare */

 both paths exist based on above type compare */

/* Duplicate LSM field information.  The lsm_rule is opaque, so must be

 our own copy of lsm_str */

 our own (refreshed) copy of lsm_rule */

	/* Keep currently invalid fields around in case they

/* Duplicate an audit rule.  This will be a deep copy with the exception

 * of the watch - that pointer is carried over.  The LSM specific fields

 * will be updated in the copy.  The point is to be able to replace the old

 * rule with the new rule in the filterlist, then free the old rule.

 * The rlist element is undefined; list manipulations are handled apart from

	/*

	 * note that we are OK with not refcounting here; audit_match_tree()

	 * never dereferences tree and we can't get false positives there

	 * since we'd have to have rule gone from the list *and* removed

	 * before the chunks found by lookup had been allocated, i.e. before

	 * the beginning of list scan.

	/* deep copy this information, updating the lsm_rule fields, because

/* Find an existing audit rule.

 we don't know the inode number, so must walk entire hash */

 Add rule to given filterlist if not a duplicate. */

 If any of these, don't count towards total */

 normally audit_add_tree_rule() will free it on failure */

 audit_filter_mutex is dropped and re-taken during this call */

			/*

			 * normally audit_add_tree_rule() will free it

			 * on failure

 Remove an existing rule from filterlist. */

 If any of these, don't count towards total */

 that's the temporary one */

 List rules using struct audit_rule_data. */

	/* This is a blocking read, so use audit_filter_mutex instead of rcu

 Log rule additions and removals */

/**

 * audit_rule_change - apply all rules to the specified message type

 * @type: audit message type

 * @seq: netlink audit message sequence (serial) number

 * @data: payload data

 * @datasz: size of payload data

/**

 * audit_list_rules_send - list the audit rules

 * @request_skb: skb of request we are replying to (used to target the reply)

 * @seq: netlink audit message sequence (serial) number

	/* We can't just spew out the rules here because we might fill

	 * the available socket buffer space and deadlock waiting for

	 * auditctl to read from it... which isn't ever going to

	 * happen if we're actually running in the context of auditctl

/**

 * parent_len - find the length of the parent portion of a pathname

 * @path: pathname of which to determine length

 disregard trailing slashes */

 walk backward until we find the next slash or hit beginning */

 did we find a slash? Then increment to include it in path */

/**

 * audit_compare_dname_path - compare given dentry name with last component in

 * 			      given path. Return of 0 indicates a match.

 * @dname:	dentry name that we're comparing

 * @path:	full pathname that we're comparing

 * @parentlen:	length of the parent if known. Passing in AUDIT_NAME_FULL

 * 		here indicates that we must compute this value.

 Audit by default */

 error */

		/* save the first error encountered for the

/* This function will re-initialize the lsm_rule field of all applicable rules.

 * It will traverse the filter lists serarching for rules that contain LSM

 * specific filter fields.  When such a rule is found, it is copied, the

 * LSM field is re-initialized, and the old rule is replaced with the

 audit_filter_mutex synchronizes the writers */

 SPDX-License-Identifier: GPL-2.0

/*

 * Shadow Call Stack support.

 *

 * Copyright (C) 2019 Google LLC

 Matches NR_CACHED_STACKS for VMAP_STACK */

	/*

	 * Poison the allocation to catch unintentional accesses to

	 * the shadow stack when KASAN is enabled.

	/*

	 * We cannot sleep as this can be called in interrupt context,

	 * so use this_cpu_cmpxchg to update the cache, and vfree_atomic

	 * to free the stack.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * latencytop.c: Latency display infrastructure

 *

 * (C) Copyright 2008 Intel Corporation

 * Author: Arjan van de Ven <arjan@linux.intel.com>

/*

 * CONFIG_LATENCYTOP enables a kernel latency tracking infrastructure that is

 * used by the "latencytop" userspace tool. The latency that is tracked is not

 * the 'traditional' interrupt latency (which is primarily caused by something

 * else consuming CPU), but instead, it is the latency an application encounters

 * because the kernel sleeps on its behalf for various reasons.

 *

 * This code tracks 2 levels of statistics:

 * 1) System level latency

 * 2) Per process latency

 *

 * The latency is stored in fixed sized data structures in an accumulated form;

 * if the "same" latency cause is hit twice, this will be tracked as one entry

 * in the data structure. Both the count, total accumulated latency and maximum

 * latency are tracked in this data structure. When the fixed size structure is

 * full, no new causes are tracked until the buffer is flushed by writing to

 * the /proc file; the userspace tool does this on a regular basis.

 *

 * A latency cause is identified by a stringified backtrace at the point that

 * the scheduler gets invoked. The userland tool will use this string to

 * identify the cause of the latency in human readable form.

 *

 * The information is exported via /proc/latency_stats and /proc/<pid>/latency.

 * These files look like this:

 *

 * Latency Top version : v0.1

 * 70 59433 4897 i915_irq_wait drm_ioctl vfs_ioctl do_vfs_ioctl sys_ioctl

 * |    |    |    |

 * |    |    |    +----> the stringified backtrace

 * |    |    +---------> The maximum latency for this entry in microseconds

 * |    +--------------> The accumulated latency for this entry (microseconds)

 * +-------------------> The number of times this entry is hit

 *

 * (note: the average latency is the accumulated latency divided by the number

 * of times)

 skip kernel threads for now */

 Nothing stored: */

 0 entry marks end of backtrace: */

 Allocted a new one: */

/**

 * __account_scheduler_latency - record an occurred latency

 * @tsk - the task struct of the task hitting the latency

 * @usecs - the duration of the latency in microseconds

 * @inter - 1 if the sleep was interruptible, 0 if uninterruptible

 *

 * This function is the main entry point for recording latency entries

 * as called by the scheduler.

 *

 * This function has a few special cases to deal with normal 'non-latency'

 * sleeps: specifically, interruptible sleep longer than 5 msec is skipped

 * since this usually is caused by waiting for events via select() and co.

 *

 * Negative latencies (caused by time going backwards) are also explicitly

 * skipped.

 Long interruptible waits are generally user requested... */

 Negative sleeps are time going backwards */

 Zero-time sleeps are non-interesting */

 0 entry is end of backtrace */

	/*

	 * short term hack; if we're > 32 we stop; future we recycle:

 Allocated a new one: */

/*

 * Public API and common code for kernel->userspace relay file support.

 *

 * See Documentation/filesystems/relay.rst for an overview.

 *

 * Copyright (C) 2002-2005 - Tom Zanussi (zanussi@us.ibm.com), IBM Corp

 * Copyright (C) 1999-2005 - Karim Yaghmour (karim@opersys.com)

 *

 * Moved to kernel/relay.c by Paul Mundt, 2006.

 * November 2006 - CPU hotplug support by Mathieu Desnoyers

 * 	(mathieu.desnoyers@polymtl.ca)

 *

 * This file is released under the GPL.

 list of open channels, for cpu hotplug */

/*

 * fault() vm_op implementation for relay file mapping.

/*

 * vm_ops for relay file mappings.

/*

 * allocate an array of pointers of struct page

/*

 * free an array of pointers of struct page

/**

 *	relay_mmap_buf: - mmap channel buffer to process address space

 *	@buf: relay channel buffer

 *	@vma: vm_area_struct describing memory to be mapped

 *

 *	Returns 0 if ok, negative on error

 *

 *	Caller should already have grabbed mmap_lock.

/**

 *	relay_alloc_buf - allocate a channel buffer

 *	@buf: the buffer struct

 *	@size: total size of the buffer

 *

 *	Returns a pointer to the resulting buffer, %NULL if unsuccessful. The

 *	passed in size will get page aligned, if it isn't already.

/**

 *	relay_create_buf - allocate and initialize a channel buffer

 *	@chan: the relay channel

 *

 *	Returns channel buffer if successful, %NULL otherwise.

/**

 *	relay_destroy_channel - free the channel struct

 *	@kref: target kernel reference that contains the relay channel

 *

 *	Should only be called from kref_put().

/**

 *	relay_destroy_buf - destroy an rchan_buf struct and associated buffer

 *	@buf: the buffer struct

/**

 *	relay_remove_buf - remove a channel buffer

 *	@kref: target kernel reference that contains the relay buffer

 *

 *	Removes the file from the filesystem, which also frees the

 *	rchan_buf_struct and the channel buffer.  Should only be called from

 *	kref_put().

/**

 *	relay_buf_empty - boolean, is the channel buffer empty?

 *	@buf: channel buffer

 *

 *	Returns 1 if the buffer is empty, 0 otherwise.

/**

 *	relay_buf_full - boolean, is the channel buffer full?

 *	@buf: channel buffer

 *

 *	Returns 1 if the buffer is full, 0 otherwise.

/*

 * High-level relay kernel API and associated functions.

/**

 *	wakeup_readers - wake up readers waiting on a channel

 *	@work: contains the channel buffer

 *

 *	This is the function used to defer reader waking

/**

 *	__relay_reset - reset a channel buffer

 *	@buf: the channel buffer

 *	@init: 1 if this is a first-time initialization

 *

 *	See relay_reset() for description of effect.

/**

 *	relay_reset - reset the channel

 *	@chan: the channel

 *

 *	This has the effect of erasing all data from all channel buffers

 *	and restarting the channel in its initial state.  The buffers

 *	are not freed, so any mappings are still in effect.

 *

 *	NOTE. Care should be taken that the channel isn't actually

 *	being used by anything when this call is made.

 Create file in fs */

/*

 *	relay_open_buf - create a new relay channel buffer

 *

 *	used by relay_open() and CPU hotplug.

 Only retrieve global info, nothing more, nothing less */

/**

 *	relay_close_buf - close a channel buffer

 *	@buf: channel buffer

 *

 *	Marks the buffer finalized and restores the default callbacks.

 *	The channel buffer and channel buffer data structure are then freed

 *	automatically when the last reference is given up.

/**

 *	relay_open - create a new relay channel

 *	@base_filename: base name of files to create, %NULL for buffering only

 *	@parent: dentry of parent directory, %NULL for root directory or buffer

 *	@subbuf_size: size of sub-buffers

 *	@n_subbufs: number of sub-buffers

 *	@cb: client callback functions

 *	@private_data: user-defined data

 *

 *	Returns channel pointer if successful, %NULL otherwise.

 *

 *	Creates a channel buffer for each cpu using the sizes and

 *	attributes specified.  The created channel buffer files

 *	will be named base_filename0...base_filenameN-1.  File

 *	permissions will be %S_IRUSR.

 *

 *	If opening a buffer (@parent = NULL) that you later wish to register

 *	in a filesystem, call relay_late_setup_files() once the @parent dentry

 *	is available.

 Called in atomic context. */

/**

 *	relay_late_setup_files - triggers file creation

 *	@chan: channel to operate on

 *	@base_filename: base name of files to create

 *	@parent: dentry of parent directory, %NULL for root directory

 *

 *	Returns 0 if successful, non-zero otherwise.

 *

 *	Use to setup files for a previously buffer-only channel created

 *	by relay_open() with a NULL parent dentry.

 *

 *	For example, this is useful for perfomring early tracing in kernel,

 *	before VFS is up and then exposing the early results once the dentry

 *	is available.

 Is chan already set up? */

	/*

	 * The CPU hotplug notifier ran before us and created buffers with

	 * no files associated. So it's safe to call relay_setup_buf_file()

	 * on all currently online CPUs.

 relay_channels_mutex must be held, so wait. */

/**

 *	relay_switch_subbuf - switch to a new sub-buffer

 *	@buf: channel buffer

 *	@length: size of current event

 *

 *	Returns either the length passed in or 0 if full.

 *

 *	Performs sub-buffer-switch tasks such as invoking callbacks,

 *	updating padding counts, waking up readers, etc.

			/*

			 * Calling wake_up_interruptible() from here

			 * will deadlock if we happen to be logging

			 * from the scheduler (trying to re-grab

			 * rq->lock), so defer it.

/**

 *	relay_subbufs_consumed - update the buffer's sub-buffers-consumed count

 *	@chan: the channel

 *	@cpu: the cpu associated with the channel buffer to update

 *	@subbufs_consumed: number of sub-buffers to add to current buf's count

 *

 *	Adds to the channel buffer's consumed sub-buffer count.

 *	subbufs_consumed should be the number of sub-buffers newly consumed,

 *	not the total consumed.

 *

 *	NOTE. Kernel clients don't need to call this function if the channel

 *	mode is 'overwrite'.

/**

 *	relay_close - close the channel

 *	@chan: the channel

 *

 *	Closes all channel buffers and frees the channel.

/**

 *	relay_flush - close the channel

 *	@chan: the channel

 *

 *	Flushes all channel buffers, i.e. forces buffer switch.

/**

 *	relay_file_open - open file op for relay files

 *	@inode: the inode

 *	@filp: the file

 *

 *	Increments the channel buffer refcount.

/**

 *	relay_file_mmap - mmap file op for relay files

 *	@filp: the file

 *	@vma: the vma describing what to map

 *

 *	Calls upon relay_mmap_buf() to map the file into user space.

/**

 *	relay_file_poll - poll file op for relay files

 *	@filp: the file

 *	@wait: poll table

 *

 *	Poll implemention.

/**

 *	relay_file_release - release file op for relay files

 *	@inode: the inode

 *	@filp: the file

 *

 *	Decrements the channel refcount, as the filesystem is

 *	no longer using it.

/*

 *	relay_file_read_consume - update the consumed count for the buffer

/*

 *	relay_file_read_avail - boolean, are there unconsumed bytes available?

/**

 *	relay_file_read_subbuf_avail - return bytes available in sub-buffer

 *	@read_pos: file read position

 *	@buf: relay channel buffer

/**

 *	relay_file_read_start_pos - find the first available byte to read

 *	@buf: relay channel buffer

 *

 *	If the read_pos is in the middle of padding, return the

 *	position of the first actually available byte, otherwise

 *	return the original value.

/**

 *	relay_file_read_end_pos - return the new read position

 *	@read_pos: file read position

 *	@buf: relay channel buffer

 *	@count: number of bytes to be read

/*

 *	subbuf_splice_actor - splice up to one subbuf's worth of data

	/*

	 * Adjust read len, if longer than what is available

 SPDX-License-Identifier: GPL-2.0-or-later

/* Module signature checker

 *

 * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/*

 * Verify the signature on a module.

 SPDX-License-Identifier: GPL-2.0

 all we need is ->next == NULL */

/**

 * task_work_add - ask the @task to execute @work->func()

 * @task: the task which should run the callback

 * @work: the callback to run

 * @notify: how to notify the targeted task

 *

 * Queue @work for task_work_run() below and notify the @task if @notify

 * is @TWA_RESUME or @TWA_SIGNAL. @TWA_SIGNAL works like signals, in that the

 * it will interrupt the targeted task and run the task_work. @TWA_RESUME

 * work is run only when the task exits the kernel and returns to user mode,

 * or before entering guest mode. Fails if the @task is exiting/exited and thus

 * it can't process this @work. Otherwise @work->func() will be called when the

 * @task goes through one of the aforementioned transitions, or exits.

 *

 * If the targeted task is exiting, then an error is returned and the work item

 * is not queued. It's up to the caller to arrange for an alternative mechanism

 * in that case.

 *

 * Note: there is no ordering guarantee on works queued here. The task_work

 * list is LIFO.

 *

 * RETURNS:

 * 0 if succeeds or -ESRCH.

 record the work call stack in order to print it in KASAN reports */

/**

 * task_work_cancel_match - cancel a pending work added by task_work_add()

 * @task: the task which should execute the work

 * @match: match function to call

 *

 * RETURNS:

 * The found work or NULL if not found.

	/*

	 * If cmpxchg() fails we continue without updating pprev.

	 * Either we raced with task_work_add() which added the

	 * new entry before this work, we will find it again. Or

	 * we raced with task_work_run(), *pprev == NULL/exited.

/**

 * task_work_cancel - cancel a pending work added by task_work_add()

 * @task: the task which should execute the work

 * @func: identifies the work to remove

 *

 * Find the last queued pending work with ->func == @func and remove

 * it from queue.

 *

 * RETURNS:

 * The found work or NULL if not found.

/**

 * task_work_run - execute the works added by task_work_add()

 *

 * Flush the pending works. Should be used by the core kernel code.

 * Called before the task returns to the user-mode or stops, or when

 * it exits. In the latter case task_work_add() can no longer add the

 * new work after task_work_run() returns.

		/*

		 * work->func() can do task_work_add(), do not set

		 * work_exited unless the list is empty.

		/*

		 * Synchronize with task_work_cancel(). It can not remove

		 * the first entry == work, cmpxchg(task_works) must fail.

		 * But it can remove another entry from the ->next list.

 SPDX-License-Identifier: GPL-2.0

/*

 * fail_function.c: Function-based error injection

	/*

	 * A dummy post handler is required to prohibit optimizing, because

	 * jump optimization does not support execution path overriding.

	/*

	 * Since this operation can be done after retval file is removed,

	 * It is safer to check the attr is still valid before accessing

	 * its member.

 Here we also validate @attr to ensure it still exists. */

 cut off if it is too long */

 Writing just spaces will remove all injection points */

 Writing !function will remove one injection point */

 injectable attribute is just a symlink of error_inject/list */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/kernel/acct.c

 *

 *  BSD Process Accounting for Linux

 *

 *  Author: Marco van Wieringen <mvw@planets.elm.net>

 *

 *  Some code based on ideas and code from:

 *  Thomas K. Dyas <tdyas@eden.rutgers.edu>

 *

 *  This file implements BSD-style process accounting. Whenever any

 *  process exits, an accounting record of type "struct acct" is

 *  written to the file specified with the acct() system call. It is

 *  up to user-level programs to do useful things with the accounting

 *  log. The kernel just provides the raw accounting information.

 *

 * (C) Copyright 1995 - 1997 Marco van Wieringen - ELM Consultancy B.V.

 *

 *  Plugged two leaks. 1) It didn't return acct_file into the free_filps if

 *  the file happened to be read-only. 2) If the accounting was suspended

 *  due to the lack of space it happily allowed to reopen it and completely

 *  lost the old acct_file. 3/10/98, Al Viro.

 *

 *  Now we silently close acct_file on attempt to reopen. Cleaned sys_acct().

 *  XTerms and EMACS are manifestations of pure evil. 21/10/98, AV.

 *

 *  Fixed a nasty interaction with sys_umount(). If the accounting

 *  was suspeneded we failed to stop it on umount(). Messy.

 *  Another one: remount to readonly didn't stop accounting.

 *	Question: what should we do if we have CAP_SYS_ADMIN but not

 *  CAP_SYS_PACCT? Current code does the following: umount returns -EBUSY

 *  unless we are messing with the root. In that case we are getting a

 *  real mess with do_remount_sb(). 9/11/98, AV.

 *

 *  Fixed a bunch of races (and pair of leaks). Probably not the best way,

 *  but this one obviously doesn't introduce deadlocks. Later. BTW, found

 *  one race (and leak) in BSD implementation.

 *  OK, that's better. ANOTHER race and leak in BSD variant. There always

 *  is one more bug... 10/11/98, AV.

 *

 *	Oh, fsck... Oopsable SMP race in do_process_acct() - we must hold

 * ->mmap_lock to walk the vma list of current->mm. Nasty, since it leaks

 * a struct file opened for write. Fixed. 2/6/2000, AV.

/*

 * These constants control the amount of freespace that suspend and

 * resume the process accounting system, and the time delay between

 * each check.

 * Turned into sysctl-controllable parameters. AV, 12/11/98

 >foo% free space - resume */

 <foo% free space - suspend */

 foo second timeout between checks */

/*

 * External references and all of the globals.

/*

 * Check the amount of free space and suspend/resume accordingly.

 May block */

 Difference from BSD - they don't do O_APPEND */

 nobody has seen it yet */

/**

 * sys_acct - enable/disable process accounting

 * @name: file name for accounting records or NULL to shutdown accounting

 *

 * sys_acct() is the only system call needed to implement process

 * accounting. It takes the name of the file where accounting records

 * should be written. If the filename is NULL, accounting will be

 * shutdown.

 *

 * Returns: 0 for success or negative errno values for failure.

/*

 *  encode an unsigned long into a comp_t

 *

 *  This routine has been adopted from the encode_comp_t() function in

 *  the kern_acct.c file of the FreeBSD operating system. The encoding

 *  is a 13-bit fraction with a 3-bit (base 8) exponent.

 13 bit mantissa. */

 Base 8 (3 bit) exponent. */

 Maximum fractional value. */

 Round up? */

 Base 8 exponent == 3 bit shift. */

	/*

	 * If we need to round up, do it (and handle overflow correctly).

	/*

	 * Clean it up and polish it off.

 Shift the exponent into place */

 and add on the mantissa. */

/*

 * encode an u64 into a comp2_t (24 bits)

 *

 * Format: 5 bit base 2 exponent, 20 bits mantissa.

 * The leading bit of the mantissa is not stored, but implied for

 * non-zero exponents.

 * Largest encodable value is 50 bits.

 20 bit mantissa. */

 5 bit base 2 exponent. */

 Maximum fractional value. */

 Maximum exponent. */

	/*

	 * If we need to round up, do it (and handle overflow correctly).

 Overflow. Return largest representable number instead. */

/*

 * encode an u64 into a 32 bit IEEE float

/*

 *  Write an accounting entry for an exiting process

 *

 *  The acct_process() call is the workhorse of the process

 *  accounting system. The struct acct is built here and then written

 *  into the accounting file. This function should only be called from

 *  do_exit() or when switching to a different output file.

	/*

	 * Fill the accounting struct with the needed info as recorded

	 * by the different kernel functions.

 calculate run_time in nsec*/

 convert nsec -> AHZ */

 new enlarged etime field */

 Safe as we hold the siglock */

/*

 *  do_acct_process does all actual work. Caller holds the reference to file.

	/*

	 * Accounting records are not subject to resource limits.

 Perform file operations on behalf of whoever enabled accounting */

	/*

	 * First check to see if there is enough free_space to continue

	 * the process accounting system.

 we really need to bite the bullet and change layout */

 backward-compatible 16 bit fields */

	/*

	 * Get freeze protection. If the fs is frozen, just skip the write

	 * as we could deadlock the system otherwise.

 it's been opened O_APPEND, so position is irrelevant */

/**

 * acct_collect - collect accounting information into pacct_struct

 * @exitcode: task exit code

 * @group_dead: not 0, if this thread is the last one in the process.

/**

 * acct_process - handles process accounting for an exiting task

	/*

	 * This loop is safe lockless, since current is still

	 * alive and holds its namespace, which in turn holds

	 * its parent.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Load ELF vmlinux file for the kexec_file_load syscall.

 *

 * Copyright (C) 2004  Adam Litke (agl@us.ibm.com)

 * Copyright (C) 2004  IBM Corp.

 * Copyright (C) 2005  R Sharada (sharada@in.ibm.com)

 * Copyright (C) 2006  Mohan Kumar M (mohan@in.ibm.com)

 * Copyright (C) 2016  IBM Corporation

 *

 * Based on kexec-tools' kexec-elf-exec.c and kexec-elf-ppc64.c.

 * Heavily modified for the kernel by

 * Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>.

/**

 * elf_is_ehdr_sane - check that it is safe to use the ELF header

 * @buf_len:	size of the buffer in which the ELF file is loaded.

		/*

		 * e_phnum is at most 65535 so calculating the size of the

		 * program header cannot overflow.

 Sanity check the program header table location. */

		/*

		 * e_shnum is at most 65536 so calculating

		 * the size of the section header cannot overflow.

 Sanity check the section header table location. */

/**

 * elf_is_phdr_sane - check that it is safe to use the program header

 * @buf_len:	size of the buffer in which the ELF file is loaded.

 Override the const in proghdrs, we are the ones doing the loading. */

/**

 * elf_read_phdrs - read the program headers from the buffer

 *

 * This function assumes that the program header table was checked for sanity.

 * Use elf_is_ehdr_sane() if it wasn't.

	/*

	 * e_phnum is at most 65535 so calculating the size of the

	 * program header cannot overflow.

/**

 * elf_read_from_buffer - read ELF file and sets up ELF header and ELF info

 * @buf:	Buffer to read ELF file from.

 * @len:	Size of @buf.

 * @ehdr:	Pointer to existing struct which will be populated.

 * @elf_info:	Pointer to existing struct which will be populated.

 *

 * This function allows reading ELF files with different byte order than

 * the kernel, byte-swapping the fields as needed.

 *

 * Return:

 * On success returns 0, and the caller should call

 * kexec_free_elf_info(elf_info) to free the memory allocated for the section

 * and program headers.

/**

 * kexec_free_elf_info - free memory allocated by elf_read_from_buffer

/**

 * kexec_build_elf_info - read ELF executable and check that we can use it

 Big endian vmlinux has type ET_DYN. */

		/*

		 * Kexec does not support loading interpreters.

		 * In addition this check keeps us from attempting

		 * to kexec ordinay executables.

/**

 * kexec_elf_load - load ELF executable image

 * @lowest_load_addr:	On return, will be the address where the first PT_LOAD

 *			section will be loaded in memory.

 *

 * Return:

 * 0 on success, negative value on failure.

 Read in the PT_LOAD segments. */

 SPDX-License-Identifier: GPL-2.0-or-later

/* audit_watch.c -- watching inodes

 *

 * Copyright 2003-2009 Red Hat, Inc.

 * Copyright 2005 Hewlett-Packard Development Company, L.P.

 * Copyright 2005 IBM Corporation

/*

 * Reference counting:

 *

 * audit_parent: lifetime is from audit_init_parent() to receipt of an FS_IGNORED

 * 	event.  Each audit_watch holds a reference to its associated parent.

 *

 * audit_watch: if added to lists, lifetime is from audit_init_watch() to

 * 	audit_remove_watch().  Additionally, an audit_watch may exist

 * 	temporarily to assist in searching existing filter data.  Each

 * 	audit_krule holds a reference to its associated watch.

 reference count */

 associated superblock device */

 insertion path */

 associated inode number */

 associated parent */

 entry in parent->watches list */

 anchor for krule->rlist */

 anchor for audit_watch->wlist */

 fsnotify mark on the inode */

 fsnotify handle. */

 fsnotify events we care about. */

/*

 * Find and return the audit_parent on the given inode.  If found a reference

 * is taken on this parent.

 match initial get */

 Initialize a parent watch entry. */

 Initialize a watch entry. */

 Translate a watch string to kernel representation. */

/* Duplicate the given audit watch.  The new watch's rules list is initialized

 Update inode info in audit rules based on filesystem event. */

	/* Run all of the watches on this parent looking for the one that

		/* If the update involves invalidating rules, do the inode-based

		/* updating ino will likely change which audit_hash_list we

				/*

				 * nentry->rule.watch == oentry->rule.watch so

				 * we must drop that reference and set it to our

				 * new watch.

 event applies to a single watch */

 Remove all watches & rules associated with a parent that is going away. */

 Get path information necessary for adding watches. */

 update watch filter fields */

/* Associate the given rule with an existing parent.

 put krule's ref to temporary watch */

/* Find a matching watch entry, or add this one.

	/*

	 * When we will be calling audit_add_to_parent, krule->watch might have

	 * been updated and watch might have been freed.

	 * So we need to keep a reference of watch.

 Avoid calling path_lookup under audit_filter_mutex. */

 caller expects mutex locked */

 either find an old parent or attach a new one */

		/*

		 * audit_remove_watch() drops our reference to 'parent' which

		 * can get freed. Grab our own reference to be safe.

 Update watch data in audit rules based on fsnotify events. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Clang Control Flow Integrity (CFI) error and slowpath handling.

 *

 * Copyright (C) 2021 Google LLC

 Compiler-defined handler names */

/*

 * Index type. A 16-bit index can address at most (2^16)-2 pages (taking

 * into account SHADOW_INVALID), i.e. ~256M with 4k pages.

 Page index for the beginning of the shadow */

 An array of __cfi_check locations (as indices to the shadow) */

/*

 * The shadow covers ~128M from the beginning of the module region. If

 * the region is larger, we fall back to __module_address for the rest.

 The in-memory size of struct cfi_shadow, always at least one page */

 The actual size of the shadow array, minus metadata */

 Returns the index in the shadow for the given address */

 Outside of module area */

 Cannot be addressed with shadow */

 Returns the page address for an index in the shadow */

 Returns the __cfi_check function address for the given shadow location */

 __cfi_check is always page aligned */

 Mark everything invalid */

 No previous shadow */

 If the base address didn't change, an update is not needed */

 Convert the previous shadow to the new address range */

 Module not addressable with shadow */

 For each page, store the check function index in the shadow */

 Each page must only contain one module */

 No shadow available */

 Cannot be addressed with shadow */

 !CONFIG_CFI_CLANG_SHADOW */

 CONFIG_CFI_CLANG_SHADOW */

	/*

	 * Indirect call checks can happen when RCU is not watching. Both

	 * the shadow and __module_address use RCU, so we need to wake it

	 * up if necessary.

 Don't allow unchecked modules */

 !CONFIG_MODULES */

 No modules */

 CONFIG_MODULES */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kexec.c - kexec system call core code.

 * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>

 Per cpu memory for storing cpu states in case of system crash. */

 Flag to indicate we are going to kexec a new kernel */

 Location of the reserved area for the crash kernel */

	/*

	 * If crash_kexec_post_notifiers is enabled, don't run

	 * crash_kexec() here yet, which must be run after panic

	 * notifiers in panic().

	/*

	 * There are 4 panic() calls in do_exit() path, each of which

	 * corresponds to each of these 4 conditions.

/*

 * When kexec transitions to the new kernel there is a one-to-one

 * mapping between physical and virtual addresses.  On processors

 * where you can disable the MMU this is trivial, and easy.  For

 * others it is still a simple predictable page table to setup.

 *

 * In that environment kexec copies the new kernel to its final

 * resting place.  This means I can only support memory whose

 * physical address can fit in an unsigned long.  In particular

 * addresses where (pfn << PAGE_SHIFT) > ULONG_MAX cannot be handled.

 * If the assembly stub has more restrictive requirements

 * KEXEC_SOURCE_MEMORY_LIMIT and KEXEC_DEST_MEMORY_LIMIT can be

 * defined more restrictively in <asm/kexec.h>.

 *

 * The code for the transition from the current kernel to the

 * new kernel is placed in the control_code_buffer, whose size

 * is given by KEXEC_CONTROL_PAGE_SIZE.  In the best case only a single

 * page of memory is necessary, but some architectures require more.

 * Because this memory must be identity mapped in the transition from

 * virtual to physical addresses it must live in the range

 * 0 - TASK_SIZE, as only the user space mappings are arbitrarily

 * modifiable.

 *

 * The assembly stub in the control code buffer is passed a linked list

 * of descriptor pages detailing the source pages of the new kernel,

 * and the destination addresses of those source pages.  As this data

 * structure is not used in the context of the current OS, it must

 * be self-contained.

 *

 * The code has been made to work with highmem pages and will use a

 * destination page in its final resting place (if it happens

 * to allocate it).  The end product of this is that most of the

 * physical address space, and most of RAM can be used.

 *

 * Future directions include:

 *  - allocating a page table with the control code buffer identity

 *    mapped, to simplify machine_kexec and make kexec_on_panic more

 *    reliable.

/*

 * KIMAGE_NO_DEST is an impossible destination address..., for

 * allocating pages whose destination address we do not care about.

	/*

	 * Verify we have good destination addresses.  The caller is

	 * responsible for making certain we don't attempt to load

	 * the new image into invalid or reserved areas of RAM.  This

	 * just verifies it is an address we can use.

	 *

	 * Since the kernel does everything in page size chunks ensure

	 * the destination addresses are page aligned.  Too many

	 * special cases crop of when we don't do this.  The most

	 * insidious is getting overlapping destination addresses

	 * simply because addresses are changed to page size

	 * granularity.

	/* Verify our destination addresses do not overlap.

	 * If we alloed overlapping destination addresses

	 * through very weird things can happen with no

	 * easy explanation as one segment stops on another.

 Do the segments overlap ? */

	/* Ensure our buffer sizes are strictly less than

	 * our memory sizes.  This should always be the case,

	 * and it is easier to check up front than to be surprised

	 * later on.

	/*

	 * Verify that no more than half of memory will be consumed. If the

	 * request from userspace is too large, a large amount of time will be

	 * wasted allocating pages, which can cause a soft lockup.

	/*

	 * Verify we have good destination addresses.  Normally

	 * the caller is responsible for making certain we don't

	 * attempt to load the new image into invalid or reserved

	 * areas of RAM.  But crash kernels are preloaded into a

	 * reserved area of ram.  We must ensure the addresses

	 * are in the reserved area otherwise preloading the

	 * kernel could corrupt things.

 Ensure we are within the crash kernel limits */

 Allocate a controlling structure */

 By default this does not apply */

 Initialize the list of control pages */

 Initialize the list of destination pages */

 Initialize the list of unusable pages */

	/* Control pages are special, they are the intermediaries

	 * that are needed while we copy the rest of the pages

	 * to their final resting place.  As such they must

	 * not conflict with either the destination addresses

	 * or memory the kernel is already using.

	 *

	 * The only case where we really need more than one of

	 * these are for architectures where we cannot disable

	 * the MMU and must instead generate an identity mapped

	 * page table for all of the memory.

	 *

	 * At worst this runs in O(N) of the image size.

	/* Loop while I can allocate a page and the page allocated

	 * is a destination page.

 Remember the allocated page... */

		/* Because the page is already in it's destination

		 * location we will never allocate another page at

		 * that address.  Therefore kimage_alloc_pages

		 * will not return it (again) and we don't need

		 * to give it an entry in image->segment[].

	/* Deal with the destination pages I have inadvertently allocated.

	 *

	 * Ideally I would convert multi-page allocations into single

	 * page allocations, and add everything to image->dest_pages.

	 *

	 * For now it is simpler to just free the pages.

	/* Control pages are special, they are the intermediaries

	 * that are needed while we copy the rest of the pages

	 * to their final resting place.  As such they must

	 * not conflict with either the destination addresses

	 * or memory the kernel is already using.

	 *

	 * Control pages are also the only pags we must allocate

	 * when loading a crash kernel.  All of the other pages

	 * are specified by the segments and we just memcpy

	 * into them directly.

	 *

	 * The only case where we really need more than one of

	 * these are for architectures where we cannot disable

	 * the MMU and must instead generate an identity mapped

	 * page table for all of the memory.

	 *

	 * Given the low demand this implements a very simple

	 * allocator that finds the first hole of the appropriate

	 * size in the reserved memory region, and allocates all

	 * of the memory up to and including the hole.

 See if I overlap any of the segments */

 Advance the hole to the end of the segment */

 If I don't overlap any segments I have found my hole! */

 Ensure that these pages are decrypted if SME is enabled. */

	/*

	 * For kdump, allocate one vmcoreinfo safe copy from the

	 * crash memory. as we have arch_kexec_protect_crashkres()

	 * after kexec syscall, we naturally protect it from write

	 * (even read) access under kernel direct mapping. But on

	 * the other hand, we still need to operate it when crash

	 * happens to generate vmcoreinfo note, hereby we rely on

	 * vmap for this purpose.

 Walk through and free any extra destination pages I may have */

 Walk through and free any unusable pages I have cached */

 Free the previous indirection page */

			/* Save this indirection page until we are

			 * done with it.

 Free the final indirection page */

 Handle any machine specific cleanup */

 Free the kexec control pages... */

	/*

	 * Free up any temporary buffers allocated. This might hit if

	 * error occurred much later after buffer allocation.

	/*

	 * Here we implement safeguards to ensure that a source page

	 * is not copied to its destination page before the data on

	 * the destination page is no longer useful.

	 *

	 * To do this we maintain the invariant that a source page is

	 * either its own destination page, or it is not a

	 * destination page at all.

	 *

	 * That is slightly stronger than required, but the proof

	 * that no problems will not occur is trivial, and the

	 * implementation is simply to verify.

	 *

	 * When allocating all pages normally this algorithm will run

	 * in O(N) time, but in the worst case it will run in O(N^2)

	 * time.   If the runtime is a problem the data structures can

	 * be fixed.

	/*

	 * Walk through the list of destination pages, and see if I

	 * have a match.

 Allocate a page, if we run out of memory give up */

 If the page cannot be used file it away */

 If it is the destination page we want use it */

 If the page is not a destination page use it */

		/*

		 * I know that the page is someones destination page.

		 * See if there is already a source page for this

		 * destination page.  And if so swap the source pages.

 If so move it */

			/* The old page I have found cannot be a

			 * destination page, so return it if it's

			 * gfp_flags honor the ones passed in.

 Place the page on the destination list, to be used later */

 Start with a clear page */

 For file based kexec, source pages are in kernel memory */

	/* For crash dumps kernels we simply copy the data from

	 * user space to it's destination.

	 * We do things a page at a time for the sake of kmap.

 Zero the trailing part of the page */

 For file based kexec, source pages are in kernel memory */

/*

 * No panic_cpu check version of crash_kexec().  This function is called

 * only when panic_cpu holds the current CPU number; this is the only CPU

 * which processes crash_kexec routines.

	/* Take the kexec_mutex here to prevent sys_kexec_load

	 * running on one cpu from replacing the crash kernel

	 * we are using after a panic on a different cpu.

	 *

	 * If the crash kernel was not located in a fixed area

	 * of memory the xchg(&kexec_crash_image) would be

	 * sufficient.  But since I reuse the memory...

	/*

	 * Only one CPU is allowed to execute the crash_kexec() code as with

	 * panic().  Otherwise parallel calls of panic() and crash_kexec()

	 * may stop each other.  To exclude them, we use panic_cpu here too.

 This is the 1st CPU which comes here, so go ahead. */

		/*

		 * Reset panic_cpu to allow another panic()/crash_kexec()

		 * call.

	/* Using ELF notes here is opportunistic.

	 * I need a well defined structure format

	 * for the data I pass, and I need tags

	 * on the data to indicate what information I have

	 * squirrelled away.  ELF notes happen to provide

	 * all of that, so there is no need to invent something new.

 Allocate memory for saving cpu registers. */

	/*

	 * crash_notes could be allocated across 2 vmalloc pages when percpu

	 * is vmalloc based . vmalloc doesn't guarantee 2 continuous vmalloc

	 * pages are also on 2 continuous physical pages. In this case the

	 * 2nd part of crash_notes in 2nd page could be lost since only the

	 * starting address and size of crash_notes are exported through sysfs.

	 * Here round up the size of crash_notes to the nearest power of two

	 * and pass it to __alloc_percpu as align value. This can make sure

	 * crash_notes is allocated inside one physical page.

	/*

	 * Break compile if size is bigger than PAGE_SIZE since crash_notes

	 * definitely will be in 2 pages with that.

/*

 * Move into place and start executing a preloaded standalone

 * executable.  If nothing was preloaded return an error.

		/* At this point, dpm_suspend_start() has been called,

		 * but *not* dpm_suspend_end(). We *must* call

		 * dpm_suspend_end() now.  Otherwise, drivers for

		 * some devices (e.g. interrupt controllers) become

		 * desynchronized with the actual state of the

		 * hardware at resume time, and evil weirdness ensues.

		/*

		 * migrate_to_reboot_cpu() disables CPU hotplug assuming that

		 * no further code needs to use CPU hotplug (which is true in

		 * the reboot case). However, the kexec path depends on using

		 * CPU hotplug again; so re-enable it here.

/*

 * Protection mechanism for crashkernel reserved memory after

 * the kdump kernel is loaded.

 *

 * Provide an empty default implementation here -- architecture

 * code may override this

 SPDX-License-Identifier: GPL-2.0

/* Watch queue and general notification mechanism, built on pipes

 *

 * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 *

 * See Documentation/watch_queue.rst

	/* We need to work out which note within the page this refers to, but

	 * the note might have been maximum size, so merely ANDing the offset

	 * off doesn't work.  OTOH, the note must've been more than zero size.

 No try_steal function => no stealing

 New data written to a pipe may be appended to a buffer with this type. */

/*

 * Post a notification to a watch queue.

/*

 * Apply filter rules to a notification.

 If there is a filter, the default is to reject. */

/**

 * __post_watch_notification - Post an event notification

 * @wlist: The watch list to post the event to.

 * @n: The notification record to post.

 * @cred: The creds of the process that triggered the notification.

 * @id: The ID to match on the watch.

 *

 * Post a notification of an event into a set of watch queues and let the users

 * know.

 *

 * The size of the notification should be set in n->info & WATCH_INFO_LENGTH and

 * should be in units of sizeof(*n).

/*

 * Allocate sufficient pages to preallocation for the requested number of

 * notifications.

 TODO: choose a better hard limit */

/*

 * Set the filter on a watch queue.

 Remove the old filter */

 Grab the user's filter specification */

 Ignore any unknown types */

	/* Now we need to build the internal filter from only the relevant

	 * user-specified filters.

/**

 * put_watch_queue - Dispose of a ref on a watchqueue.

 * @wqueue: The watch queue to unref.

/*

 * Discard a watch.

/**

 * init_watch - Initialise a watch

 * @watch: The watch to initialise.

 * @wqueue: The queue to assign.

 *

 * Initialise a watch and set the watch queue.

/**

 * add_watch_to_object - Add a watch on an object to a watch list

 * @watch: The watch to add

 * @wlist: The watch list to add to

 *

 * @watch->queue must have been set to point to the queue to post notifications

 * to and the watch list of the object to be watched.  @watch->cred must also

 * have been set to the appropriate credentials and a ref taken on them.

 *

 * The caller must pin the queue and the list both and must hold the list

 * locked against racing watch additions/removals.

/**

 * remove_watch_from_object - Remove a watch or all watches from an object.

 * @wlist: The watch list to remove from

 * @wq: The watch queue of interest (ignored if @all is true)

 * @id: The ID of the watch to remove (ignored if @all is true)

 * @all: True to remove all objects

 *

 * Remove a specific watch or all watches from an object.  A notification is

 * sent to the watcher to tell them that this happened.

 We now own the reference on watch that used to belong to wlist. */

	/* We don't need the watch list lock for the next bit as RCU is

	 * protecting *wqueue from deallocation.

/*

 * Remove all the watches that are contributory to a queue.  This has the

 * potential to race with removal of the watches by the destruction of the

 * objects being watched or with the distribution of notifications.

 Prevent new additions and prevent notifications from happening */

 We now own a ref on the watch. */

		/* We can't do the next bit under the queue lock as we need to

		 * get the list lock - which would cause a deadlock if someone

		 * was removing from the opposite direction at the same time or

		 * posting a notification.

 We now own a second ref on the watch. */

					/* This might need to call dput(), so

					 * we have to drop all the locks.

/**

 * get_watch_queue - Get a watch queue from its file descriptor.

 * @fd: The fd to query.

/*

 * Initialise a watch queue

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/profile.c

 *  Simple profiling. Manages a direct-mapped profile hit count buffer,

 *  with configurable resolution, support for restricting the cpus on

 *  which profiling is done, and switching between cpu time and

 *  schedule() calls via kernel command line parameters passed at boot.

 *

 *  Scheduler profiling support, Arjan van de Ven and Ingo Molnar,

 *	Red Hat, July 2004

 *  Consolidation of architecture support code for profiling,

 *	Nadia Yvette Chambers, Oracle, July 2004

 *  Amortized hit count accounting via per-cpu open-addressed hashtables

 *	to resolve timer interrupt livelocks, Nadia Yvette Chambers,

 *	Oracle, 2004

 CONFIG_SMP */

 CONFIG_SCHEDSTATS */

 only text is profiled */

 Profile event notifications */

/*

 * Each cpu has a pair of open-addressed hashtables for pending

 * profile hits. read_profile() IPI's all cpus to request them

 * to flip buffers and flushes their contents to prof_buffer itself.

 * Flip requests are serialized by the profile_flip_mutex. The sole

 * use of having a second hashtable is for avoiding cacheline

 * contention that would otherwise happen during flushes of pending

 * profile hits required for the accuracy of reported profile hits

 * and so resurrect the interrupt livelock issue.

 *

 * The open-addressed hashtables are indexed by profile buffer slot

 * and hold the number of pending hits to that profile buffer slot on

 * a cpu in an entry. When the hashtable overflows, all pending hits

 * are accounted to their corresponding profile buffer slots with

 * atomic_add() and the hashtable emptied. As numerous pending hits

 * may be accounted to a profile buffer slot in a hashtable entry,

 * this amortizes a number of atomic profile buffer increments likely

 * to be far larger than the number of entries in the hashtable,

 * particularly given that the number of distinct profile buffer

 * positions to which hits are accounted during short intervals (e.g.

 * several seconds) is usually very small. Exclusion from buffer

 * flipping is provided by interrupt disablement (note that for

 * SCHED_PROFILING or SLEEP_PROFILING profile_hit() may be called from

 * process context).

 * The hash function is meant to be lightweight as opposed to strong,

 * and was vaguely inspired by ppc64 firmware-supported inverted

 * pagetable hash functions, but uses a full hashtable full of finite

 * collision chains, not just pairs of them.

 *

 * -- nyc

	/*

	 * We buffer the global profiler buffer into a per-CPU

	 * queue and thus reduce the number of global (and possibly

	 * NUMA-alien) accesses. The write-queue is self-coalescing:

	/*

	 * Add the current hit(s) and flush the write-queue out

	 * to the global buffer:

 !CONFIG_SMP */

 !CONFIG_SMP */

 create /proc/irq/prof_cpu_mask */

/*

 * This function accesses profiling information. The returned data is

 * binary: the sampling step and the actual contents of the profile

 * buffer. Use of the program readprofile is recommended in order to

 * get meaningful info out of these data.

/*

 * Writing to /proc/profile resets the counters

 *

 * Writing a 'profiling multiplier' value into it also re-sets the profiling

 * interrupt frequency, on architectures that support this.

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kexec: kexec_file_load system call

 *

 * Copyright (C) 2014 Red Hat Inc.

 * Authors:

 *      Vivek Goyal <vgoyal@redhat.com>

/*

 * Currently this is the only default function that is exported as some

 * architectures need it to do additional handlings.

 * In the future, other default functions may be exported too if required.

 Architectures can provide this probe function */

/*

 * arch_kexec_apply_relocations_add - apply relocations of type RELA

 * @pi:		Purgatory to be relocated.

 * @section:	Section relocations applying to.

 * @relsec:	Section containing RELAs.

 * @symtab:	Corresponding symtab.

 *

 * Return: 0 on success, negative errno on error.

/*

 * arch_kexec_apply_relocations - apply relocations of type REL

 * @pi:		Purgatory to be relocated.

 * @section:	Section relocations applying to.

 * @relsec:	Section containing RELs.

 * @symtab:	Corresponding symtab.

 *

 * Return: 0 on success, negative errno on error.

/*

 * Free up memory used by kernel, initrd, and command line. This is temporary

 * memory allocation which is not needed any more after these buffers have

 * been loaded into separate segments and have been copied elsewhere.

 CONFIG_IMA_KEXEC */

 See if architecture has anything to cleanup post load */

	/*

	 * Above call should have called into bootloader to free up

	 * any data stored in kimage->image_loader_data. It should

	 * be ok now to free it up.

		/*

		 * If IMA is guaranteed to appraise a signature on the kexec

		 * image, permit it even if the kernel is otherwise locked

		 * down.

/*

 * In file mode list of segments is prepared by kernel. Copy relevant

 * data from user space, do error checking, prepare segment list

 Call arch image probe handlers */

 It is possible that there no initramfs is being loaded */

 command line should be a string with last byte null */

 IMA needs to pass the measurement list to the next kernel. */

 Call arch image load handlers */

 In case of error, free up all allocated memory in this function */

 Enable special crash kernel control page alloc policy. */

 We only trust the superuser with rebooting the system. */

 Make sure we have a legal set of flags */

	/*

	 * In case of crash, new kernel gets loaded in reserved region. It is

	 * same memory where old crash kernel might be loaded. Free any

	 * current crash dump kernel before we corrupt it.

	/*

	 * Some architecture(like S390) may touch the crash memory before

	 * machine_kexec_prepare(), we must copy vmcoreinfo data after it.

	/*

	 * Free up any temporary buffers allocated which are not needed

	 * after image has been loaded

 align down start */

		/*

		 * Make sure this does not conflict with any of existing

		 * segments

 We found a suitable memory range */

 If we are here, we found a suitable memory range */

 Success, stop navigating through remaining System RAM ranges */

		/*

		 * Make sure this does not conflict with any of existing

		 * segments

 We found a suitable memory range */

 If we are here, we found a suitable memory range */

 Success, stop navigating through remaining System RAM ranges */

 Returning 0 will take to next memory range */

 Don't use memory that will be detected and handled by a driver. */

	/*

	 * Allocate memory top down with-in ram range. Otherwise bottom up

	 * allocation.

	/*

	 * Using MEMBLOCK_NONE will properly skip MEMBLOCK_DRIVER_MANAGED. See

	 * IORESOURCE_SYSRAM_DRIVER_MANAGED handling in

	 * locate_mem_hole_callback().

			/*

			 * In memblock, end points to the first byte after the

			 * range while in kexec, end points to the last byte

			 * in the range.

			/*

			 * In memblock, end points to the first byte after the

			 * range while in kexec, end points to the last byte

			 * in the range.

/**

 * kexec_walk_resources - call func(data) on free memory regions

 * @kbuf:	Context info for the search. Also passed to @func.

 * @func:	Function to call for each memory region.

 *

 * Return: The memory walk will stop when func returns a non-zero value

 * and that value will be returned. If all free regions are visited without

 * func returning non-zero, then zero will be returned.

/**

 * kexec_locate_mem_hole - find free memory for the purgatory or the next kernel

 * @kbuf:	Parameters for the memory search.

 *

 * On success, kbuf->mem will have the start address of the memory region found.

 *

 * Return: 0 on success, negative errno on error.

 Arch knows where to place */

/**

 * arch_kexec_locate_mem_hole - Find free memory to place the segments.

 * @kbuf:                       Parameters for the memory search.

 *

 * On success, kbuf->mem will have the start address of the memory region found.

 *

 * Return: 0 on success, negative errno on error.

/**

 * kexec_add_buffer - place a buffer in a kexec segment

 * @kbuf:	Buffer contents and memory parameters.

 *

 * This function assumes that kexec_mutex is held.

 * On successful return, @kbuf->mem will have the physical address of

 * the buffer in memory.

 *

 * Return: 0 on success, negative errno on error.

 Currently adding segment this way is allowed only in file mode */

	/*

	 * Make sure we are not trying to add buffer after allocating

	 * control pages. All segments need to be placed first before

	 * any control pages are allocated. As control page allocation

	 * logic goes through list of segments to make sure there are

	 * no destination overlaps.

 Ensure minimum alignment needed for segments. */

 Walk the RAM ranges and allocate a suitable range for the buffer */

 Found a suitable memory range */

 Calculate and store the digest of segments */

		/*

		 * Skip purgatory as it will be modified once we put digest

		 * info in purgatory.

		/*

		 * Assume rest of the buffer is filled with zero and

		 * update digest accordingly.

/*

 * kexec_purgatory_setup_kbuf - prepare buffer to load purgatory.

 * @pi:		Purgatory to be loaded.

 * @kbuf:	Buffer to setup.

 *

 * Allocates the memory needed for the buffer. Caller is responsible to free

 * the memory after use.

 *

 * Return: 0 on success, negative errno on error.

/*

 * kexec_purgatory_setup_sechdrs - prepares the pi->sechdrs buffer.

 * @pi:		Purgatory to be loaded.

 * @kbuf:	Buffer prepared to store purgatory.

 *

 * Allocates the memory needed for the buffer. Caller is responsible to free

 * the memory after use.

 *

 * Return: 0 on success, negative errno on error.

	/*

	 * The section headers in kexec_purgatory are read-only. In order to

	 * have them modifiable make a temporary copy.

		/*

		 * For section of type SHT_RELA/SHT_REL,

		 * ->sh_link contains section header index of associated

		 * symbol table. And ->sh_info contains section header

		 * index of section to which relocations apply.

		/*

		 * symtab->sh_link contain section header index of associated

		 * string table.

 Invalid section number? */

		/*

		 * Respective architecture needs to provide support for applying

		 * relocations of type SHT_RELA/SHT_REL.

/*

 * kexec_load_purgatory - Load and relocate the purgatory object.

 * @image:	Image to add the purgatory to.

 * @kbuf:	Memory parameters to use.

 *

 * Allocates the memory needed for image->purgatory_info.sechdrs and

 * image->purgatory_info.purgatory_buf/kbuf->buffer. Caller is responsible

 * to free the memory after use.

 *

 * Return: 0 on success, negative errno on error.

/*

 * kexec_purgatory_find_symbol - find a symbol in the purgatory

 * @pi:		Purgatory to search in.

 * @name:	Name of the symbol.

 *

 * Return: pointer to symbol in read-only symtab on success, NULL on error.

 Invalid strtab section number */

 Go through symbols for a match */

 Found the symbol we are looking for */

	/*

	 * Returns the address where symbol will finally be loaded after

	 * kexec_load_segment()

/*

 * Get or set value of a symbol. If "get_value" is true, symbol value is

 * returned in buf otherwise symbol value is set based on value in buf.

 CONFIG_ARCH_HAS_KEXEC_PURGATORY */

 Truncate any area outside of range */

 Found completely overlapping range */

 Shift rest of the ranges to left */

				/*

				 * Continue to check if there are another overlapping ranges

				 * from the current position because of shifting the above

				 * mem ranges.

 Split original range */

 If a split happened, add the split to array */

 Split happened */

 Location where new range should go */

 Move over all ranges one slot towards the end */

 extra phdr for vmcoreinfo ELF note */

	/*

	 * kexec-tools creates an extra PT_LOAD phdr for kernel text mapping

	 * area (for example, ffffffff80000000 - ffffffffa0000000 on x86_64).

	 * I think this is required by tools like gdb. So same physical

	 * memory will be mapped in two ELF headers. One will contain kernel

	 * text virtual addresses and other will have __va(physical) addresses.

 Prepare one phdr of type PT_NOTE for each present CPU */

 Prepare one PT_NOTE header for vmcoreinfo */

 Prepare PT_LOAD type program header for kernel text region */

 Go through all the ranges in mem->ranges[] and prepare phdr */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * crash.c - kernel crash support code.

 * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>

 vmcoreinfo stuff */

 trusted vmcoreinfo, e.g. we can make a copy in the crash memory */

/*

 * parsing the "crashkernel" commandline

 *

 * this code is intended to be called from architecture specific code

/*

 * This function parses command lines in the format

 *

 *   crashkernel=ramsize-range:size[,...][@offset]

 *

 * The function returns 0 on success and -EINVAL on failure.

 for each entry of the comma-separated list */

 get the start of the range */

 if no ':' is here, than we read the end */

 match ? */

/*

 * That function parses "simple" (old) crashkernel command lines like

 *

 *	crashkernel=size[@offset]

 *

 * It returns 0 on success and -EINVAL on failure.

/*

 * That function parses "suffix"  crashkernel command lines like

 *

 *	crashkernel=size,[high|low]

 *

 * It returns 0 on success and -EINVAL on failure.

 check with suffix */

 find crashkernel and use the last one if there are more */

 skip the one with any known suffix */

	/*

	 * if the commandline contains a ':', then that's the extended

	 * syntax -- if not, it must be the classic syntax

/*

 * That function is the entry point for command line parsing and should be

 * called from the arch-specific code.

 Use the safe copy to generate vmcoreinfo note if have */

/*

 * provide an empty default implementation here -- architecture

 * code may override this

 SPDX-License-Identifier: GPL-2.0

/*

 * We don't expose the real in-memory order of objects for security reasons.

 * But still the comparison results should be suitable for sorting. So we

 * obfuscate kernel pointers values and compare the production instead.

 *

 * The obfuscation is done in two steps. First we xor the kernel pointer with

 * a random value, which puts pointer into a new position in a reordered space.

 * Secondly we multiply the xor production with a large odd random number to

 * permute its bits even more (the odd multiplier guarantees that the product

 * is unique ever after the high bits are truncated, since any odd number is

 * relative prime to 2^n).

 *

 * Note also that the obfuscation itself is invisible to userspace and if needed

 * it can be changed to an alternate scheme.

/*

 * 0 - equal, i.e. v1 = v2

 * 1 - less than, i.e. v1 < v2

 * 2 - greater than, i.e. v1 > v2

 * 3 - not equal but ordering unavailable (reserved for future)

 The caller must have pinned the task */

	/*

	 * Tasks are looked up in caller's PID namespace only.

	/*

	 * One should have enough rights to inspect task details.

 SPDX-License-Identifier: GPL-2.0-or-later

/* Task credentials management - see Documentation/security/credentials.rst

 *

 * Copyright (C) 2008 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

 init to 2 - one for init_task, one to ensure it is never freed */

/*

 * The initial credentials for the initial task

/*

 * The RCU callback to actually dispose of a set of credentials

/**

 * __put_cred - Destroy a set of credentials

 * @cred: The record to release

 *

 * Destroy a set of credentials on which no references remain.

/*

 * Clean up a task's credentials when it exits

/**

 * get_task_cred - Get another task's objective credentials

 * @task: The task to query

 *

 * Get the objective credentials of a task, pinning them so that they can't go

 * away.  Accessing a task's credentials directly is not permitted.

 *

 * The caller must also make sure task doesn't get deleted, either by holding a

 * ref on task or by holding tasklist_lock to prevent it from being unlinked.

/*

 * Allocate blank credentials, such that the credentials can be filled in at a

 * later date without risk of ENOMEM.

/**

 * prepare_creds - Prepare a new set of credentials for modification

 *

 * Prepare a new set of task credentials for modification.  A task's creds

 * shouldn't generally be modified directly, therefore this function is used to

 * prepare a new copy, which the caller then modifies and then commits by

 * calling commit_creds().

 *

 * Preparation involves making a copy of the objective creds for modification.

 *

 * Returns a pointer to the new creds-to-be if successful, NULL otherwise.

 *

 * Call commit_creds() or abort_creds() to clean up.

/*

 * Prepare credentials for current to perform an execve()

 * - The caller must hold ->cred_guard_mutex

 newly exec'd tasks don't get a thread keyring */

 inherit the session keyring; new process keyring */

/*

 * Copy credentials for the new process created by fork()

 *

 * We share if we can, but under some circumstances we have to generate a new

 * set.

 *

 * The new process gets the current process's subjective credentials as its

 * objective and subjective credentials

	/* new threads get their own thread keyrings if their parent already

	/* The process keyring is only shared between the threads in a process;

	 * anything outside of those threads doesn't inherit.

	/* If the two credentials are in the same user namespace see if

	 * the capabilities of subset are a subset of set.

	/* The credentials are in a different user namespaces

	 * therefore one is a subset of the other only if a set is an

	 * ancestor of subset and set->euid is owner of subset or one

	 * of subsets ancestors.

/**

 * commit_creds - Install new credentials upon the current task

 * @new: The credentials to be assigned

 *

 * Install a new set of credentials to the current task, using RCU to replace

 * the old set.  Both the objective and the subjective credentials pointers are

 * updated.  This function may not be called if the subjective credentials are

 * in an overridden state.

 *

 * This function eats the caller's reference to the new credentials.

 *

 * Always returns 0 thus allowing this function to be tail-called at the end

 * of, say, sys_setgid().

 we will require a ref for the subj creds too */

 dumpability changes */

		/*

		 * If a task drops privileges and becomes nondumpable,

		 * the dumpability change must become visible before

		 * the credential change; otherwise, a __ptrace_may_access()

		 * racing with this change may be able to attach to a task it

		 * shouldn't be able to attach to (as if the task had dropped

		 * privileges without becoming nondumpable).

		 * Pairs with a read barrier in __ptrace_may_access().

 alter the thread keyring */

	/* do it

	 * RLIMIT_NPROC limits on user->processes have already been checked

	 * in set_user().

 send notifications */

 release the old obj and subj refs both */

/**

 * abort_creds - Discard a set of credentials and unlock the current task

 * @new: The credentials that were going to be applied

 *

 * Discard a set of credentials that were under construction and unlock the

 * current task.

/**

 * override_creds - Override the current process's subjective credentials

 * @new: The credentials to be assigned

 *

 * Install a set of temporary override subjective credentials on the current

 * process, returning the old set for later reversion.

	/*

	 * NOTE! This uses 'get_new_cred()' rather than 'get_cred()'.

	 *

	 * That means that we do not clear the 'non_rcu' flag, since

	 * we are only installing the cred into the thread-synchronous

	 * '->cred' pointer, not the '->real_cred' pointer that is

	 * visible to other threads under RCU.

	 *

	 * Also note that we did validate_creds() manually, not depending

	 * on the validation in 'get_cred()'.

/**

 * revert_creds - Revert a temporary subjective credentials override

 * @old: The credentials to be restored

 *

 * Revert a temporary set of override subjective credentials to an old set,

 * discarding the override set.

/**

 * cred_fscmp - Compare two credentials with respect to filesystem access.

 * @a: The first credential

 * @b: The second credential

 *

 * cred_cmp() will return zero if both credentials have the same

 * fsuid, fsgid, and supplementary groups.  That is, if they will both

 * provide the same access to files based on mode/uid/gid.

 * If the credentials are different, then either -1 or 1 will

 * be returned depending on whether @a comes before or after @b

 * respectively in an arbitrary, but stable, ordering of credentials.

 *

 * Return: -1, 0, or 1 depending on comparison

	/*

	 * This optimization is needed because alloc_ucounts() uses locks

	 * for table lookups.

/*

 * initialise the credentials stuff

 allocate a slab in which we can store credentials */

/**

 * prepare_kernel_cred - Prepare a set of credentials for a kernel service

 * @daemon: A userspace daemon to be used as a reference

 *

 * Prepare a set of credentials for a kernel service.  This can then be used to

 * override a task's own credentials so that work can be done on behalf of that

 * task that requires a different subjective context.

 *

 * @daemon is used to provide a base for the security record, but can be NULL.

 * If @daemon is supplied, then the security data will be derived from that;

 * otherwise they'll be set to 0 and no groups, full capabilities and no keys.

 *

 * The caller may change these controls afterwards if desired.

 *

 * Returns the new credentials or NULL if out of memory.

/**

 * set_security_override - Set the security ID in a set of credentials

 * @new: The credentials to alter

 * @secid: The LSM security ID to set

 *

 * Set the LSM security ID in a set of credentials so that the subjective

 * security is overridden when an alternative set of credentials is used.

/**

 * set_security_override_from_ctx - Set the security ID in a set of credentials

 * @new: The credentials to alter

 * @secctx: The LSM security context to generate the security ID from.

 *

 * Set the LSM security ID in a set of credentials so that the subjective

 * security is overridden when an alternative set of credentials is used.  The

 * security ID is specified in string form as a security context to be

 * interpreted by the LSM.

/**

 * set_create_files_as - Set the LSM file create context in a set of credentials

 * @new: The credentials to alter

 * @inode: The inode to take the context from

 *

 * Change the LSM file creation context in a set of credentials to be the same

 * as the object context of the specified inode, so that the new inodes have

 * the same MAC context as that inode.

/*

 * dump invalid credentials

/*

 * report use of invalid credentials

/*

 * check the credentials on a process

/*

 * check creds for do_exit()

 CONFIG_DEBUG_CREDENTIALS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2007

 *

 *  Author: Eric Biederman <ebiederm@xmision.com>

/*

 *	Special case of dostring for the UTS structure. This has locks

 *	to observe. Should this be in kernel/sys.c ????

	/*

	 * Buffer the value in tmp_data so that proc_dostring() can be called

	 * without holding any locks.

	 * We also need to read the original value in the write==1 case to

	 * support partial writes.

		/*

		 * Write back the new value.

		 * Note that, since we dropped uts_sem, the result can

		 * theoretically be incorrect if there are two parallel writes

		 * at non-zero offsets to the same sysctl.

/*

 * Notify userspace about a change in a certain entry of uts_kern_table,

 * identified by the parameter proc.

 SPDX-License-Identifier: GPL-2.0

/*

 * Range add and subtract

 Out of slots: */

 get new start/end: */

 new start/end, will add it back at last */

 Need to add it: */

 Find the new spare: */

 count it */

 sort them */

 sort them */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic helpers for smp ipi calls

 *

 * (C) Jens Axboe <jens.axboe@oracle.com> 2008

	/*

	 * The IPIs for the smp-call-function callbacks queued by other

	 * CPUs might arrive late, either due to hardware latencies or

	 * because this CPU disabled interrupts (inside stop-machine)

	 * before the IPIs were sent. So flush out any pending callbacks

	 * explicitly (without waiting for the IPIs to arrive), to

	 * ensure that the outgoing CPU doesn't go offline with work

	 * still pending.

 Record current CSD work for current CPU, NULL to erase. */

 NULL cur_csd after unlock. */

 func and info before csd. */

 Update cur_csd before function call. */

 Or before unlock, as the case may be. */

 Other CSD_TYPE_ values might not have ->dst. */

 Direct read value trumps generated one. */

/*

 * Complain if too much time spent waiting.  Note that only

 * the CSD_TYPE_SYNC/ASYNC types provide the destination CPU,

 * so waiting on other types gets much less information.

 Before func and info. */

/*

 * csd_lock/csd_unlock used to serialize access to per-cpu csd resources

 *

 * For non-synchronous ipi calls the csd can still be in use by the

 * previous function call. For multi-cpu calls its even more interesting

 * as we'll have to ensure no other cpu is observing our csd.

	/*

	 * prevent CPU from reordering the above assignment

	 * to ->flags with any subsequent assignments to other

	 * fields of the specified call_single_data_t structure:

	/*

	 * ensure we're all done before releasing data:

	/*

	 * The list addition should be visible before sending the IPI

	 * handler locks the list to pull the entry off it because of

	 * normal cache coherency rules implied by spinlocks.

	 *

	 * If IPIs can go out of order to the cache coherency protocol

	 * in an architecture, sufficient synchronisation should be added

	 * to arch code to make it appear to obey cache coherency WRT

	 * locking and barrier primitives. Generic code isn't really

	 * equipped to do the right thing...

/*

 * Insert a previously allocated call_single_data_t element

 * for execution on the given CPU. data must already have

 * ->func, ->info, and ->flags set.

		/*

		 * We can unlock early even for the synchronous on-stack case,

		 * since we're doing this from the same CPU..

/**

 * generic_smp_call_function_single_interrupt - Execute SMP IPI callbacks

 *

 * Invoked by arch to handle an IPI for call function single.

 * Must be called with interrupts disabled.

/**

 * flush_smp_call_function_queue - Flush pending smp-call-function callbacks

 *

 * @warn_cpu_offline: If set to 'true', warn if callbacks were queued on an

 *		      offline CPU. Skip this check if set to 'false'.

 *

 * Flush any pending smp-call-function callbacks queued on this CPU. This is

 * invoked by the generic IPI handler, as well as by a CPU about to go offline,

 * to ensure that all pending IPI callbacks are run before it goes completely

 * offline.

 *

 * Loop through the call_single_queue and run all the queued callbacks.

 * Must be called with interrupts disabled.

 Special meaning of source cpu: 0 == queue empty */

 There shouldn't be any pending callbacks on an offline CPU. */

		/*

		 * We don't have to use the _safe() variant here

		 * because we are not invoking the IPI handlers yet.

	/*

	 * First; run all SYNC callbacks, people are waiting for us.

 Do we wait until *after* callback? */

	/*

	 * Second; run all !SYNC callbacks.

	/*

	 * Third; only CSD_TYPE_TTWU is left, issue those.

/*

 * smp_call_function_single - Run a function on a specific CPU

 * @func: The function to run. This must be fast and non-blocking.

 * @info: An arbitrary pointer to pass to the function.

 * @wait: If true, wait until function has completed on other CPUs.

 *

 * Returns 0 on success, else a negative status code.

	/*

	 * prevent preemption and reschedule on another processor,

	 * as well as CPU removal

	/*

	 * Can deadlock when called with interrupts disabled.

	 * We allow cpu's that are not yet online though, as no one else can

	 * send smp call function interrupt to this cpu and as such deadlocks

	 * can't happen.

	/*

	 * When @wait we can deadlock when we interrupt between llist_add() and

	 * arch_send_call_function_ipi*(); when !@wait we can deadlock due to

	 * csd_lock() on because the interrupt context uses the same csd

	 * storage.

/**

 * smp_call_function_single_async() - Run an asynchronous function on a

 * 			         specific CPU.

 * @cpu: The CPU to run on.

 * @csd: Pre-allocated and setup data structure

 *

 * Like smp_call_function_single(), but the call is asynchonous and

 * can thus be done from contexts with disabled interrupts.

 *

 * The caller passes his own pre-allocated data structure

 * (ie: embedded in an object) and is responsible for synchronizing it

 * such that the IPIs performed on the @csd are strictly serialized.

 *

 * If the function is called with one csd which has not yet been

 * processed by previous call to smp_call_function_single_async(), the

 * function will return immediately with -EBUSY showing that the csd

 * object is still in progress.

 *

 * NOTE: Be careful, there is unfortunately no current debugging facility to

 * validate the correctness of this serialization.

 *

 * Return: %0 on success or negative errno value on error

/*

 * smp_call_function_any - Run a function on any of the given cpus

 * @mask: The mask of cpus it can run on.

 * @func: The function to run. This must be fast and non-blocking.

 * @info: An arbitrary pointer to pass to the function.

 * @wait: If true, wait until function has completed.

 *

 * Returns 0 on success, else a negative status code (if no cpus were online).

 *

 * Selection preference:

 *	1) current cpu if in @mask

 *	2) any cpu of current node if in @mask

 *	3) any other online cpu in @mask

 Try for same CPU (cheapest) */

 Try for same node. */

 Any online will do: smp_call_function_single handles nr_cpu_ids. */

/*

 * Flags to be used as scf_flags argument of smp_call_function_many_cond().

 *

 * %SCF_WAIT:		Wait until function execution is completed

 * %SCF_RUN_LOCAL:	Run also locally if local cpu is set in cpumask

	/*

	 * Can deadlock when called with interrupts disabled.

	 * We allow cpu's that are not yet online though, as no one else can

	 * send smp call function interrupt to this cpu and as such deadlocks

	 * can't happen.

	/*

	 * When @wait we can deadlock when we interrupt between llist_add() and

	 * arch_send_call_function_ipi*(); when !@wait we can deadlock due to

	 * csd_lock() on because the interrupt context uses the same csd

	 * storage.

 Check if we need local execution. */

 Check if we need remote execution, i.e., any CPU excluding this one. */

		/*

		 * Choose the most efficient way to send an IPI. Note that the

		 * number of CPUs might be zero due to concurrent changes to the

		 * provided mask.

/**

 * smp_call_function_many(): Run a function on a set of CPUs.

 * @mask: The set of cpus to run on (only runs on online subset).

 * @func: The function to run. This must be fast and non-blocking.

 * @info: An arbitrary pointer to pass to the function.

 * @wait: Bitmask that controls the operation. If %SCF_WAIT is set, wait

 *        (atomically) until function has completed on other CPUs. If

 *        %SCF_RUN_LOCAL is set, the function will also be run locally

 *        if the local CPU is set in the @cpumask.

 *

 * If @wait is true, then returns once @func has returned.

 *

 * You must not call this function with disabled interrupts or from a

 * hardware interrupt handler or from a bottom half handler. Preemption

 * must be disabled when calling this function.

/**

 * smp_call_function(): Run a function on all other CPUs.

 * @func: The function to run. This must be fast and non-blocking.

 * @info: An arbitrary pointer to pass to the function.

 * @wait: If true, wait (atomically) until function has completed

 *        on other CPUs.

 *

 * Returns 0.

 *

 * If @wait is true, then returns once @func has returned; otherwise

 * it returns just before the target cpu calls @func.

 *

 * You must not call this function with disabled interrupts or from a

 * hardware interrupt handler or from a bottom half handler.

 Setup configured maximum number of CPUs to activate */

/*

 * Setup routine for controlling SMP activation

 *

 * Command-line option of "nosmp" or "maxcpus=0" will disable SMP

 * activation entirely (the MPS table probe still happens, though).

 *

 * Command-line option of "maxcpus=<NUM>", where <NUM> is an integer

 * greater than 0, limits the maximum number of CPUs activated in

 * SMP mode to <NUM>.

 this is hard limit */

 Setup number of possible processor ids */

 An arch may set nr_cpu_ids earlier if needed, so this would be redundant */

 Called by boot processor to activate the rest. */

 Any cleanup work */

/*

 * on_each_cpu_cond(): Call a function on each processor for which

 * the supplied function cond_func returns true, optionally waiting

 * for all the required CPUs to finish. This may include the local

 * processor.

 * @cond_func:	A callback function that is passed a cpu id and

 *		the info parameter. The function is called

 *		with preemption disabled. The function should

 *		return a blooean value indicating whether to IPI

 *		the specified CPU.

 * @func:	The function to run on all applicable CPUs.

 *		This must be fast and non-blocking.

 * @info:	An arbitrary pointer to pass to both functions.

 * @wait:	If true, wait (atomically) until function has

 *		completed on other CPUs.

 *

 * Preemption is disabled to protect against CPUs going offline but not online.

 * CPUs going online during the call will not be seen or sent an IPI.

 *

 * You must not call this function with disabled interrupts or

 * from a hardware interrupt handler or from a bottom half handler.

/**

 * kick_all_cpus_sync - Force all cpus out of idle

 *

 * Used to synchronize the update of pm_idle function pointer. It's

 * called after the pointer is updated and returns after the dummy

 * callback function has been executed on all cpus. The execution of

 * the function can only happen on the remote cpus after they have

 * left the idle function which had been called via pm_idle function

 * pointer. So it's guaranteed that nothing uses the previous pointer

 * anymore.

 Make sure the change is visible before we kick the cpus */

/**

 * wake_up_all_idle_cpus - break all cpus out of idle

 * wake_up_all_idle_cpus try to break all cpus which is in idle state even

 * including idle polling cpus, for non-idle cpus, we will do nothing

 * for them.

/**

 * struct smp_call_on_cpu_struct - Call a function on a specific CPU

 * @work: &work_struct

 * @done: &completion to signal

 * @func: function to call

 * @data: function's data argument

 * @ret: return value from @func

 * @cpu: target CPU (%-1 for any CPU)

 *

 * Used to call a function on a specific cpu and wait for it to return.

 * Optionally make sure the call is done on a specified physical cpu via vcpu

 * pinning in order to support virtualized environments.

 SPDX-License-Identifier: GPL-2.0

 Number of 64-bit words written per one comparison: */

/*

 * kcov descriptor (one per opened debugfs file).

 * State transitions of the descriptor:

 *  - initial state after open()

 *  - then there must be a single ioctl(KCOV_INIT_TRACE) call

 *  - then, mmap() call (several calls are allowed but not useful)

 *  - then, ioctl(KCOV_ENABLE, arg), where arg is

 *	KCOV_TRACE_PC - to trace only the PCs

 *	or

 *	KCOV_TRACE_CMP - to trace only the comparison operands

 *  - then, ioctl(KCOV_DISABLE) to disable the task.

 * Enabling/disabling ioctls can be repeated (only one task a time allowed).

	/*

	 * Reference counter. We keep one for:

	 *  - opened file descriptor

	 *  - task with enabled coverage (we can't unwire it from another task)

	 *  - each code section for remote coverage collection

 The lock protects mode, size, area and t. */

 Size of arena (in long's). */

 Coverage buffer shared with user space. */

 Task for which we collect coverage, or NULL. */

 Collecting coverage from remote (background) threads. */

 Size of remote area (in long's). */

	/*

	 * Sequence is incremented each time kcov is reenabled, used by

	 * kcov_remote_stop(), see the comment there.

 Must be called with kcov_remote_lock locked. */

 Must be called with kcov_remote_lock locked. */

 Must be called with kcov_remote_lock locked. */

 Must be called with kcov_remote_lock locked. */

	/*

	 * We are interested in code coverage as a function of a syscall inputs,

	 * so we ignore code executed in interrupts, unless we are in a remote

	 * coverage collection section in a softirq.

	/*

	 * There is some code that runs in interrupts but for which

	 * in_interrupt() returns false (e.g. preempt_schedule_irq()).

	 * READ_ONCE()/barrier() effectively provides load-acquire wrt

	 * interrupts, there are paired barrier()/WRITE_ONCE() in

	 * kcov_start().

/*

 * Entry point from instrumented code.

 * This is called once per basic-block/edge.

 The first 64-bit word is the number of subsequent PCs. */

	/*

	 * We write all comparison arguments and types as u64.

	 * The buffer was allocated for t->kcov_size unsigned longs.

 Every record is KCOV_WORDS_PER_CMP 64-bit words. */

 ifdef CONFIG_KCOV_ENABLE_COMPARISONS */

 Cache in task struct for performance. */

 See comment in check_kcov_mode(). */

 Do reset before unlock to prevent races with kcov_remote_start(). */

	/*

	 * For KCOV_ENABLE devices we want to make sure that t->kcov->t == t,

	 * which comes down to:

	 *        WARN_ON(!kcov->remote && kcov->t != t);

	 *

	 * For KCOV_REMOTE_ENABLE devices, the exiting task is either:

	 *

	 * 1. A remote task between kcov_remote_start() and kcov_remote_stop().

	 *    In this case we should print a warning right away, since a task

	 *    shouldn't be exiting when it's in a kcov coverage collection

	 *    section. Here t points to the task that is collecting remote

	 *    coverage, and t->kcov->t points to the thread that created the

	 *    kcov device. Which means that to detect this case we need to

	 *    check that t != t->kcov->t, and this gives us the following:

	 *        WARN_ON(kcov->remote && kcov->t != t);

	 *

	 * 2. The task that created kcov exiting without calling KCOV_DISABLE,

	 *    and then again we make sure that t->kcov->t == t:

	 *        WARN_ON(kcov->remote && kcov->t != t);

	 *

	 * By combining all three checks into one we get:

 Just to not leave dangling references behind. */

/*

 * Fault in a lazily-faulted vmalloc area before it can be used by

 * __santizer_cov_trace_pc(), to avoid recursion issues if any code on the

 * vmalloc fault handling path is instrumented.

		/*

		 * Enable kcov in trace mode and setup buffer size.

		 * Must happen before anything else.

		/*

		 * Size must be at least 2 to hold current position and one PC.

		 * Later we allocate size * sizeof(unsigned long) memory,

		 * that must not overflow.

		/*

		 * Enable coverage for the current task.

		 * At this point user must have been enabled trace mode,

		 * and mmapped the file. Coverage collection is disabled only

		 * at task exit or voluntary by KCOV_DISABLE. After that it can

		 * be enabled for another task.

 Put either in kcov_task_exit() or in KCOV_DISABLE. */

 Disable coverage for the current task. */

 Put either in kcov_task_exit() or in KCOV_DISABLE. */

/*

 * kcov_remote_start() and kcov_remote_stop() can be used to annotate a section

 * of code in a kernel background thread or in a softirq to allow kcov to be

 * used to collect coverage from that part of code.

 *

 * The handle argument of kcov_remote_start() identifies a code section that is

 * used for coverage collection. A userspace process passes this handle to

 * KCOV_REMOTE_ENABLE ioctl to make the used kcov device start collecting

 * coverage for the code section identified by this handle.

 *

 * The usage of these annotations in the kernel code is different depending on

 * the type of the kernel thread whose code is being annotated.

 *

 * For global kernel threads that are spawned in a limited number of instances

 * (e.g. one USB hub_event() worker thread is spawned per USB HCD) and for

 * softirqs, each instance must be assigned a unique 4-byte instance id. The

 * instance id is then combined with a 1-byte subsystem id to get a handle via

 * kcov_remote_handle(subsystem_id, instance_id).

 *

 * For local kernel threads that are spawned from system calls handler when a

 * user interacts with some kernel interface (e.g. vhost workers), a handle is

 * passed from a userspace process as the common_handle field of the

 * kcov_remote_arg struct (note, that the user must generate a handle by using

 * kcov_remote_handle() with KCOV_SUBSYSTEM_COMMON as the subsystem id and an

 * arbitrary 4-byte non-zero number as the instance id). This common handle

 * then gets saved into the task_struct of the process that issued the

 * KCOV_REMOTE_ENABLE ioctl. When this process issues system calls that spawn

 * kernel threads, the common handle must be retrieved via kcov_common_handle()

 * and passed to the spawned threads via custom annotations. Those kernel

 * threads must in turn be annotated with kcov_remote_start(common_handle) and

 * kcov_remote_stop(). All of the threads that are spawned by the same process

 * obtain the same handle, hence the name "common".

 *

 * See Documentation/dev-tools/kcov.rst for more details.

 *

 * Internally, kcov_remote_start() looks up the kcov device associated with the

 * provided handle, allocates an area for coverage collection, and saves the

 * pointers to kcov and area into the current task_struct to allow coverage to

 * be collected via __sanitizer_cov_trace_pc().

 * In turns kcov_remote_stop() clears those pointers from task_struct to stop

 * collecting coverage and copies all collected coverage into the kcov area.

	/*

	 * Check that kcov_remote_start() is not called twice in background

	 * threads nor called by user tasks (with enabled kcov).

	/*

	 * Check that kcov_remote_start() is not called twice in softirqs.

	 * Note, that kcov_remote_start() can be called from a softirq that

	 * happened while collecting coverage from a background thread.

 Put in kcov_remote_stop(). */

	/*

	 * Read kcov fields before unlock to prevent races with

	 * KCOV_DISABLE / kcov_remote_reset().

 Can only happen when in_task(). */

 Reset coverage size. */

 As arm can't divide u64 integers use log of entry size. */

 See the comment before kcov_remote_start() for usage details. */

	/*

	 * When in softirq, check if the corresponding kcov_remote_start()

	 * actually found the remote handle and started collecting coverage.

 Make sure that kcov_softirq is only set when in softirq. */

	/*

	 * KCOV_DISABLE could have been called between kcov_remote_start()

	 * and kcov_remote_stop(), hence the sequence check.

 Get in kcov_remote_start(). */

 See the comment before kcov_remote_start() for usage details. */

	/*

	 * The kcov debugfs file won't ever get removed and thus,

	 * there is no need to protect it against removal races. The

	 * use of debugfs_create_file_unsafe() is actually safe here.

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/kernel/seccomp.c

 *

 * Copyright 2004-2005  Andrea Arcangeli <andrea@cpushare.com>

 *

 * Copyright (C) 2012 Google, Inc.

 * Will Drewry <wad@chromium.org>

 *

 * This defines a simple but solid secure-computing facility.

 *

 * Mode 1 uses a fixed list of allowed system calls.

 * Mode 2 allows user-defined system call filters in the form

 *        of Berkeley Packet Filters/Linux Socket Filters.

/*

 * When SECCOMP_IOCTL_NOTIF_ID_VALID was first introduced, it had the

 * wrong direction flag in the ioctl number. This is the broken one,

 * which the kernel needs to keep supporting until all userspaces stop

 * using the wrong command number.

 The struct pid of the task whose filter triggered the notification */

 The "cookie" for this request; this is unique for this filter. */

	/*

	 * The seccomp data. This pointer is valid the entire time this

	 * notification is active, since it comes from __seccomp_filter which

	 * eclipses the entire lifecycle here.

	/*

	 * Notification states. When SECCOMP_RET_USER_NOTIF is returned, a

	 * struct seccomp_knotif is created and starts out in INIT. Once the

	 * handler reads the notification off of an FD, it transitions to SENT.

	 * If a signal is received the state transitions back to INIT and

	 * another message is sent. When the userspace handler replies, state

	 * transitions to REPLIED.

 The return values, only valid when in SECCOMP_NOTIFY_REPLIED */

	/*

	 * Signals when this has changed states, such as the listener

	 * dying, a new seccomp addfd message, or changing to REPLIED

 outstanding addfd requests */

/**

 * struct seccomp_kaddfd - container for seccomp_addfd ioctl messages

 *

 * @file: A reference to the file to install in the other task

 * @fd: The fd number to install it at. If the fd number is -1, it means the

 *      installing process should allocate the fd as normal.

 * @flags: The flags for the new file descriptor. At the moment, only O_CLOEXEC

 *         is allowed.

 * @ioctl_flags: The flags used for the seccomp_addfd ioctl.

 * @ret: The return value of the installing process. It is set to the fd num

 *       upon success (>= 0).

 * @completion: Indicates that the installing process has completed fd

 *              installation, or gone away (either due to successful

 *              reply, or signal)

 *

 To only be set on reply */

/**

 * struct notification - container for seccomp userspace notifications. Since

 * most seccomp filters will not have notification listeners attached and this

 * structure is fairly large, we store the notification-specific stuff in a

 * separate structure.

 *

 * @request: A semaphore that users of this notification can wait on for

 *           changes. Actual reads and writes are still controlled with

 *           filter->notify_lock.

 * @next_id: The id of the next request.

 * @notifications: A list of struct seccomp_knotif elements.

/**

 * struct action_cache - per-filter cache of seccomp actions per

 * arch/syscall pair

 *

 * @allow_native: A bitmap where each bit represents whether the

 *		  filter will always allow the syscall, for the

 *		  native architecture.

 * @allow_compat: A bitmap where each bit represents whether the

 *		  filter will always allow the syscall, for the

 *		  compat architecture.

 SECCOMP_ARCH_NATIVE */

/**

 * struct seccomp_filter - container for seccomp BPF programs

 *

 * @refs: Reference count to manage the object lifetime.

 *	  A filter's reference count is incremented for each directly

 *	  attached task, once for the dependent filter, and if

 *	  requested for the user notifier. When @refs reaches zero,

 *	  the filter can be freed.

 * @users: A filter's @users count is incremented for each directly

 *         attached task (filter installation, fork(), thread_sync),

 *	   and once for the dependent filter (tracked in filter->prev).

 *	   When it reaches zero it indicates that no direct or indirect

 *	   users of that filter exist. No new tasks can get associated with

 *	   this filter after reaching 0. The @users count is always smaller

 *	   or equal to @refs. Hence, reaching 0 for @users does not mean

 *	   the filter can be freed.

 * @cache: cache of arch/syscall mappings to actions

 * @log: true if all actions except for SECCOMP_RET_ALLOW should be logged

 * @prev: points to a previously installed, or inherited, filter

 * @prog: the BPF program to evaluate

 * @notif: the struct that holds all notification related information

 * @notify_lock: A lock for all notification-related accesses.

 * @wqh: A wait queue for poll if a notifier is in use.

 *

 * seccomp_filter objects are organized in a tree linked via the @prev

 * pointer.  For any task, it appears to be a singly-linked list starting

 * with current->seccomp.filter, the most recently attached or inherited filter.

 * However, multiple filters may share a @prev node, by way of fork(), which

 * results in a unidirectional tree existing in memory.  This is similar to

 * how namespaces work.

 *

 * seccomp_filter objects should never be modified after being attached

 * to a task_struct (other than @refs).

 Limit any path through the tree to 256KB worth of instructions. */

/*

 * Endianness is explicitly ignored and left for BPF program authors to manage

 * as per the specific architecture.

	/*

	 * Instead of using current_pt_reg(), we're already doing the work

	 * to safely fetch "current", so just use "task" everywhere below.

/**

 *	seccomp_check_filter - verify seccomp filter code

 *	@filter: filter to verify

 *	@flen: length of filter

 *

 * Takes a previously checked filter (by bpf_check_classic) and

 * redirects all filter code that loads struct sk_buff data

 * and related data through seccomp_bpf_load.  It also

 * enforces length and alignment checking of those loads.

 *

 * Returns 0 if the rule set is legal or -EINVAL if not.

 32-bit aligned and not out of bounds. */

 Explicitly include allowed calls. */

/**

 * seccomp_cache_check_allow - lookup seccomp cache

 * @sfilter: The seccomp filter

 * @sd: The seccomp data to lookup the cache with

 *

 * Returns true if the seccomp_data is cached and allowed.

 A native-only architecture doesn't need to check sd->arch. */

 SECCOMP_ARCH_COMPAT */

 SECCOMP_ARCH_NATIVE */

/**

 * seccomp_run_filters - evaluates all seccomp filters against @sd

 * @sd: optional seccomp data to be passed to filters

 * @match: stores struct seccomp_filter that resulted in the return value,

 *         unless filter returned SECCOMP_RET_ALLOW, in which case it will

 *         be unchanged.

 *

 * Returns valid seccomp BPF response codes.

 Make sure cross-thread synced filter points somewhere sane. */

 Ensure unexpected behavior doesn't result in failing open. */

	/*

	 * All filters in the list are evaluated and the lowest BPF return

	 * value always takes priority (ignoring the DATA).

 CONFIG_SECCOMP_FILTER */

	/*

	 * Make sure SYSCALL_WORK_SECCOMP cannot be set before the mode (and

	 * filter) is set.

 Assume default seccomp processes want spec flaw mitigation. */

 Returns 1 if the parent is an ancestor of the child. */

 NULL is the root ancestor. */

/**

 * seccomp_can_sync_threads: checks if all threads can be synchronized

 *

 * Expects sighand and cred_guard_mutex locks to be held.

 *

 * Returns 0 on success, -ve on error, or the pid of a thread which was

 * either not in the correct seccomp mode or did not have an ancestral

 * seccomp filter.

 Validate all threads being eligible for synchronization. */

 Skip current, since it is initiating the sync. */

 Return the first thread that cannot be synchronized. */

 If the pid cannot be resolved, then return -ESRCH */

 Clean up single-reference branches iteratively. */

 Notify about any unused filters in the task's former filter tree. */

 Finally drop all references to the task's former tree. */

/**

 * seccomp_filter_release - Detach the task from its filter tree,

 *			    drop its reference count, and notify

 *			    about unused filters

 *

 * This function should only be called when the task is exiting as

 * it detaches it from its filter tree. As such, READ_ONCE() and

 * barriers are not needed here, as would normally be needed.

 We are effectively holding the siglock by not having any sighand. */

 Detach task from its filter tree. */

/**

 * seccomp_sync_threads: sets all threads to use current's filter

 *

 * Expects sighand and cred_guard_mutex locks to be held, and for

 * seccomp_can_sync_threads() to have returned success already

 * without dropping the locks.

 *

 Synchronize all threads. */

 Skip current, since it needs no changes. */

 Get a task reference for the new leaf node. */

		/*

		 * Drop the task reference to the shared ancestor since

		 * current's path will hold a reference.  (This also

		 * allows a put before the assignment.)

 Make our new filter tree visible. */

		/*

		 * Don't let an unprivileged task work around

		 * the no_new_privs restriction by creating

		 * a thread that sets it up, enters seccomp,

		 * then dies.

		/*

		 * Opt the other thread into seccomp if needed.

		 * As threads are considered to be trust-realm

		 * equivalent (see ptrace_may_access), it is safe to

		 * allow one thread to transition the other.

/**

 * seccomp_prepare_filter: Prepares a seccomp filter for use.

 * @fprog: BPF program to install

 *

 * Returns filter on success or an ERR_PTR on failure.

	/*

	 * Installing a seccomp filter requires that the task has

	 * CAP_SYS_ADMIN in its namespace or be running with no_new_privs.

	 * This avoids scenarios where unprivileged tasks can affect the

	 * behavior of privileged children.

 Allocate a new seccomp_filter */

/**

 * seccomp_prepare_user_filter - prepares a user-supplied sock_fprog

 * @user_filter: pointer to the user data containing a sock_fprog.

 *

 * Returns 0 on success and non-zero otherwise.

 falls through to the if below. */

/**

 * seccomp_is_const_allow - check if filter is constant allow with given data

 * @fprog: The BPF programs

 * @sd: The seccomp data to check against, only syscall number and arch

 *      number are considered constant.

 can't optimize (non-constant value load) */

 reached return with constant values only, check allow */

 can't optimize (unknown jump) */

 can't optimize (unknown insn) */

 ran off the end of the filter?! */

 The new filter must be as restrictive as the last. */

 Before any filters, all syscalls are always allowed. */

 No bitmap change: not a cacheable action. */

 No bitmap change: continue to always allow. */

		/*

		 * Not a cacheable action: always run filters.

		 * atomic clear_bit() not needed, filter not visible yet.

/**

 * seccomp_cache_prepare - emulate the filter to find cacheable syscalls

 * @sfilter: The seccomp filter

 *

 * Returns 0 if successful or -errno if error occurred.

 SECCOMP_ARCH_COMPAT */

 SECCOMP_ARCH_NATIVE */

/**

 * seccomp_attach_filter: validate and attach filter

 * @flags:  flags to change filter behavior

 * @filter: seccomp filter to add to the current process

 *

 * Caller must be holding current->sighand->siglock lock.

 *

 * Returns 0 on success, -ve on error, or

 *   - in TSYNC mode: the pid of a thread which was either not in the correct

 *     seccomp mode or did not have an ancestral seccomp filter

 *   - in NEW_LISTENER mode: the fd of the new listener

 Validate resulting filter length. */

 4 instr penalty */

 If thread sync has been requested, check that it is possible. */

 Set log flag, if present. */

	/*

	 * If there is an existing filter, make it the prev and don't drop its

	 * task reference.

 Now that the new filter is in place, synchronize to all threads. */

 get_seccomp_filter - increments the reference count of the filter on @tsk */

 CONFIG_SECCOMP_FILTER */

 For use with seccomp_actions_logged */

	/*

	 * Emit an audit message when the action is RET_KILL_*, RET_LOG, or the

	 * FILTER_FLAG_LOG bit was set. The admin has the ability to silence

	 * any action from being logged by removing the action name from the

	 * seccomp_actions_logged sysctl.

/*

 * Secure computing mode 1 allows only read/write/exit/sigreturn.

 * To be fully secure this must be combined with rlimit

 * to limit the stack allocations too.

 negative terminated */

	/*

	 * Note: overflow is ok here, the id just needs to be unique per

	 * filter.

	/*

	 * Remove the notification, and reset the list pointers, indicating

	 * that it has been handled.

 If we fail reset and return an error to the notifier */

 Return the FD we just added */

	/*

	 * Mark the notification as completed. From this point, addfd mem

	 * might be invalidated and we can't safely read it anymore.

	/*

	 * This is where we wait for a reply from userspace.

 Check if we were woken up by a addfd message */

 If there were any pending addfd calls, clear them out */

 The process went away before we got a chance to handle it */

	/*

	 * Note that it's possible the listener died in between the time when

	 * we were notified of a response (or a signal) and when we were able to

	 * re-acquire the lock, so only delete from the list if the

	 * notification actually exists.

	 *

	 * Also note that this test is only valid because there's no way to

	 * *reattach* to a notifier right now. If one is added, we'll need to

	 * keep track of the notif itself and make sure they match here.

 Userspace requests to continue the syscall. */

	/*

	 * Make sure that any changes to mode from another thread have

	 * been seen after SYSCALL_WORK_SECCOMP was seen.

 Set low-order bits as an errno, capped at MAX_ERRNO. */

 Show the handler the original registers. */

 Let the filter pass back 16 bits of data. */

 We've been put in this state by the ptracer already. */

 ENOSYS these calls if there is no tracer attached. */

 Allow the BPF to provide the event message */

		/*

		 * The delivery of a fatal signal during event

		 * notification may silently skip tracer notification,

		 * which could leave us with a potentially unmodified

		 * syscall that the tracer would have liked to have

		 * changed. Since the process is about to die, we just

		 * force the syscall to be skipped and let the signal

		 * kill the process and correctly handle any tracer exit

		 * notifications.

 Check if the tracer forced the syscall to be skipped. */

		/*

		 * Recheck the syscall, since it may have changed. This

		 * intentionally uses a NULL struct seccomp_data to force

		 * a reload of all registers. This does not goto skip since

		 * a skip would have already been reported.

		/*

		 * Note that the "match" filter will always be NULL for

		 * this action since SECCOMP_RET_ALLOW is the starting

		 * state in seccomp_run_filters().

 Dump core only if this is the last remaining thread. */

 Show the original registers in the dump. */

 Trigger a coredump with SIGSYS */

 skip the syscall go directly to signal handling */

 may call do_exit */

 CONFIG_HAVE_ARCH_SECCOMP_FILTER */

/**

 * seccomp_set_mode_strict: internal function for setting strict seccomp

 *

 * Once current->seccomp.mode is non-zero, it may not be changed.

 *

 * Returns 0 on success or -EINVAL on failure.

	/*

	 * If this file is being closed because e.g. the task who owned it

	 * died, let's wake everyone up who was waiting on us.

		/*

		 * We do not need to wake up any pending addfd messages, as

		 * the notifier will do that for us, as this just looks

		 * like a standard reply.

 must be called with notif_lock held */

 Verify that we're not given garbage to keep struct extensible. */

	/*

	 * If we didn't find a notification, it could be that the task was

	 * interrupted by a fatal signal between the time we were woken and

	 * when we were able to acquire the rw lock.

		/*

		 * Userspace screwed up. To make sure that we keep this

		 * notification alive, let's reset it back to INIT. It

		 * may have died when we released the lock, so we need to make

		 * sure it's still around.

 Allow exactly one reply. */

	/*

	 * We do not want to allow for FD injection to occur before the

	 * notification has been picked up by a userspace handler, or after

	 * the notification has been replied to.

		/*

		 * Disallow queuing an atomic addfd + send reply while there are

		 * some addfd requests still to process.

		 *

		 * There is no clear reason to support it and allows us to keep

		 * the loop on the other side straight-forward.

 Allow exactly only one reply */

 Now we wait for it to be processed or be interrupted */

		/*

		 * We had a successful completion. The other side has already

		 * removed us from the addfd queue, and

		 * wait_for_completion_interruptible has a memory barrier upon

		 * success that lets us read this value directly without

		 * locking.

	/*

	 * Even though we were woken up by a signal and not a successful

	 * completion, a completion may have happened in the mean time.

	 *

	 * We need to check again if the addfd request has been handled,

	 * and if not, we will remove it from the queue.

 Fixed-size ioctls */

 Extensible Argument ioctls */

 The file has a reference to it now */

/*

 * Does @new_child have a listener while an ancestor also has a listener?

 * If so, we'll want to reject this filter.

 * This only has to be tested for the current process, even in the TSYNC case,

 * because TSYNC installs @child with the same parent on all threads.

 * Note that @new_child is not hooked up to its parent at this point yet, so

 * we use current->seccomp.filter.

 must be protected against concurrent TSYNC */

/**

 * seccomp_set_mode_filter: internal function for setting seccomp filter

 * @flags:  flags to change filter behavior

 * @filter: struct sock_fprog containing filter

 *

 * This function may be called repeatedly to install additional filters.

 * Every filter successfully installed will be evaluated (in reverse order)

 * for each system call the task makes.

 *

 * Once current->seccomp.mode is non-zero, it may not be changed.

 *

 * Returns 0 on success or -EINVAL on failure.

 Validate flags. */

	/*

	 * In the successful case, NEW_LISTENER returns the new listener fd.

	 * But in the failure case, TSYNC returns the thread that died. If you

	 * combine these two flags, there's no way to tell whether something

	 * succeeded or failed. So, let's disallow this combination if the user

	 * has not explicitly requested no errors from TSYNC.

 Prepare the new filter before holding any locks. */

	/*

	 * Make sure we cannot change seccomp or nnp state via TSYNC

	 * while another thread is in the middle of calling exec.

 Do not free the successfully attached filter. */

 Common entry point for both prctl and syscall. */

/**

 * prctl_set_seccomp: configures current->seccomp.mode

 * @seccomp_mode: requested mode to use

 * @filter: optional struct sock_fprog for use with SECCOMP_MODE_FILTER

 *

 * Returns 0 on success or -EINVAL on failure.

		/*

		 * Setting strict mode through prctl always ignored filter,

		 * so make sure it is always NULL here to pass the internal

		 * check in do_seccomp().

 prctl interface doesn't have flags, so they are always zero. */

	/*

	 * Note: this is only correct because the caller should be the (ptrace)

	 * tracer of the task, otherwise lock_task_sighand is needed.

		/* This must be a new non-cBPF filter, since we save

		 * every cBPF filter's orig_prog above when

		 * CONFIG_CHECKPOINT_RESTORE is enabled.

 Human readable action names for friendly sysctl interaction */

 CONFIG_SYSCTL */

 Currently CONFIG_SECCOMP_CACHE_DEBUG implies SECCOMP_ARCH_NATIVE */

	/*

	 * We don't want some sandboxed process to know what their seccomp

	 * filters consist of.

 prevent filter from being freed while we are printing it */

 SECCOMP_ARCH_COMPAT */

 CONFIG_SECCOMP_CACHE_DEBUG */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generate definitions needed by the preprocessor.

 * This code generates raw asm output which is post-processed

 * to extract and format the required data.

 Include headers that define the enum constants of interest */

 The enum constants to put into include/generated/bounds.h */

 End of constants */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * tsacct.c - System accounting over taskstats interface

 *

 * Copyright (C) Jay Lan,	<jlan@sgi.com>

/*

 * fill in basic accounting fields

 calculate task elapsed time in nsec */

 Convert to micro seconds */

 Convert to seconds for btime (note y2106 limit) */

/*

 * fill in extended accounting fields

 convert pages-nsec/1024 to Mbyte-usec, see __acct_update_integrals */

 adjust to KB unit */

	/*

	 * Divide by 1024 to avoid overflow, and to avoid division.

	 * The final unit reported to userspace is Mbyte-usecs,

	 * the rest of the math is done in xacct_add_tsk.

/**

 * acct_update_integrals - update mm integral fields in task_struct

 * @tsk: task_struct for accounting

/**

 * acct_account_cputime - update mm integral after cputime update

 * @tsk: task_struct for accounting

/**

 * acct_clear_integrals - clear the mm integral fields in task_struct

 * @tsk: task_struct whose accounting fields are cleared

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Pid namespaces

 *

 * Authors:

 *    (C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.

 *    (C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM

 *     Many thanks to Oleg Nesterov for comments and help

 *

 Write once array, filled from the beginning. */

/*

 * creates the kmem cache to allocate pids from.

 * @level: pid namespace level

 Level 0 is init_pid_ns.pid_cachep */

 Name collision forces to do allocation under mutex. */

 current can fail, but someone else can succeed. */

 Don't allow any more processes into the pid namespace */

	/*

	 * Ignore SIGCHLD causing any terminated children to autoreap.

	 * This speeds up the namespace shutdown, plus see the comment

	 * below.

	/*

	 * The last thread in the cgroup-init thread group is terminating.

	 * Find remaining pid_ts in the namespace, signal and wait for them

	 * to exit.

	 *

	 * Note:  This signals each threads in the namespace - even those that

	 * 	  belong to the same thread group, To avoid this, we would have

	 * 	  to walk the entire tasklist looking a processes in this

	 * 	  namespace, but that could be unnecessarily expensive if the

	 * 	  pid namespace has just a few processes. Or we need to

	 * 	  maintain a tasklist for each pid namespace.

	 *

	/*

	 * Reap the EXIT_ZOMBIE children we had before we ignored SIGCHLD.

	 * kernel_wait4() will also block until our children traced from the

	 * parent namespace are detached and become EXIT_DEAD.

	/*

	 * kernel_wait4() misses EXIT_DEAD children, and EXIT_ZOMBIE

	 * process whose parents processes are outside of the pid

	 * namespace.  Such processes are created with setns()+fork().

	 *

	 * If those EXIT_ZOMBIE processes are not reaped by their

	 * parents before their parents exit, they will be reparented

	 * to pid_ns->child_reaper.  Thus pidns->child_reaper needs to

	 * stay valid until they all go away.

	 *

	 * The code relies on the pid_ns->child_reaper ignoring

	 * SIGCHILD to cause those EXIT_ZOMBIE processes to be

	 * autoreaped if reparented.

	 *

	 * Semantically it is also desirable to wait for EXIT_ZOMBIE

	 * processes before allowing the child_reaper to be reaped, as

	 * that gives the invariant that when the init process of a

	 * pid namespace is reaped all of the processes in the pid

	 * namespace are gone.

	 *

	 * Once all of the other tasks are gone from the pid_namespace

	 * free_pid() will awaken this task.

	/*

	 * Writing directly to ns' last_pid field is OK, since this field

	 * is volatile in a living namespace anyway and a code writing to

	 * it should synchronize its usage with external means.

 permissions are checked in the handler */

 CONFIG_CHECKPOINT_RESTORE */

 Not reached */

	/*

	 * Only allow entering the current active pid namespace

	 * or a child of the current active pid namespace.

	 *

	 * This is required for fork to return a usable pid value and

	 * this maintains the property that processes and their

	 * children can not escape their current pid namespace.

 See if the parent is in the current namespace */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic pidhash and scalable, time-bounded PID allocator

 *

 * (C) 2002-2003 Nadia Yvette Chambers, IBM

 * (C) 2004 Nadia Yvette Chambers, Oracle

 * (C) 2002-2004 Ingo Molnar, Red Hat

 *

 * pid-structures are backing objects for tasks sharing a given ID to chain

 * against. There is very little to them aside from hashing them and

 * parking tasks using given ID's on a list.

 *

 * The hash is always changed with the tasklist_lock write-acquired,

 * and the hash is only accessed with the tasklist_lock at least

 * read-acquired, so there's no additional SMP locking needed here.

 *

 * We have a list of bitmap pages, which bitmaps represent the PID space.

 * Allocating and freeing PIDs is completely lockless. The worst-case

 * allocation scenario when all but one out of 1 million PIDs possible are

 * allocated already: the scanning of 32 list entries and at most PAGE_SIZE

 * bytes. The typical fastpath is a single successful setbit. Freeing is O(1).

 *

 * Pid namespaces:

 *    (C) 2007 Pavel Emelyanov <xemul@openvz.org>, OpenVZ, SWsoft Inc.

 *    (C) 2007 Sukadev Bhattiprolu <sukadev@us.ibm.com>, IBM

 *     Many thanks to Oleg Nesterov for comments and help

 *

/*

 * PID-map pages start out as NULL, they get allocated upon

 * first use and are never deallocated. This way a low pid_max

 * value does not cause lots of bitmaps to be allocated, but

 * the scheme scales to up to 4 million PIDs, runtime.

/*

 * Note: disable interrupts while the pidmap_lock is held as an

 * interrupt might come in and do read_lock(&tasklist_lock).

 *

 * If we don't disable interrupts there is a nasty deadlock between

 * detach_pid()->free_pid() and another cpu that does

 * spin_lock(&pidmap_lock) followed by an interrupt routine that does

 * read_lock(&tasklist_lock);

 *

 * After we clean up the tasklist_lock and know there are no

 * irq handlers that take it we can leave the interrupts enabled.

 * For now it is easier to be safe than to prove it can't happen.

 We can be called with write_lock_irq(&tasklist_lock) held */

			/* When all that is left in the pid namespace

			 * is the reaper wake up the reaper.  The reaper

			 * may be sleeping in zap_pid_ns_processes().

 Handle a fork failure of the first process */

	/*

	 * set_tid_size contains the size of the set_tid array. Starting at

	 * the most nested currently active PID namespace it tells alloc_pid()

	 * which PID to set for a process in that most nested PID namespace

	 * up to set_tid_size PID namespaces. It does not have to set the PID

	 * for a process in all nested PID namespaces but set_tid_size must

	 * never be greater than the current ns->level + 1.

			/*

			 * Also fail if a PID != 1 is requested and

			 * no PID 1 exists.

			/*

			 * If ENOSPC is returned it means that the PID is

			 * alreay in use. Return EEXIST in that case.

			/*

			 * init really needs pid 1, but after reaching the

			 * maximum wrap back to RESERVED_PIDS

			/*

			 * Store a null pointer so find_pid_ns does not find

			 * a partially initialized PID (see below).

	/*

	 * ENOMEM is not the most obvious choice especially for the case

	 * where the child subreaper has already exited and the pid

	 * namespace denies the creation of any new processes. But ENOMEM

	 * is what we have exposed to userspace for a long time and it is

	 * documented behavior for pid namespaces. So we can't easily

	 * change it even if there were an error code better suited.

 Make the PID visible to find_pid_ns. */

 On failure to allocate the first pid, reset the state */

/*

 * attach_pid() must be called with the tasklist_lock write-held.

 Swap the single entry tid lists */

 Swap the per task_struct pid */

 Swap the cached value */

 transfer_pid is an optimization of attach_pid(new), detach_pid(old) */

/*

 * Must be called under rcu_read_lock().

/*

 * Used by proc to find the first pid that is greater than or equal to nr.

 *

 * If there is a pid at nr this function is exactly the same as find_pid_ns.

/**

 * pidfd_get_task() - Get the task associated with a pidfd

 *

 * @pidfd: pidfd for which to get the task

 * @flags: flags associated with this pidfd

 *

 * Return the task associated with @pidfd. The function takes a reference on

 * the returned task. The caller is responsible for releasing that reference.

 *

 * Currently, the process identified by @pidfd is always a thread-group leader.

 * This restriction currently exists for all aspects of pidfds including pidfd

 * creation (CLONE_PIDFD cannot be used with CLONE_THREAD) and pidfd polling

 * (only supports thread group leaders).

 *

 * Return: On success, the task_struct associated with the pidfd.

 *	   On error, a negative errno number will be returned.

/**

 * pidfd_create() - Create a new pid file descriptor.

 *

 * @pid:   struct pid that the pidfd will reference

 * @flags: flags to pass

 *

 * This creates a new pid file descriptor with the O_CLOEXEC flag set.

 *

 * Note, that this function can only be called after the fd table has

 * been unshared to avoid leaking the pidfd to the new process.

 *

 * This symbol should not be explicitly exported to loadable modules.

 *

 * Return: On success, a cloexec pidfd is returned.

 *         On error, a negative errno number will be returned.

/**

 * pidfd_open() - Open new pid file descriptor.

 *

 * @pid:   pid for which to retrieve a pidfd

 * @flags: flags to pass

 *

 * This creates a new pid file descriptor with the O_CLOEXEC flag set for

 * the process identified by @pid. Currently, the process identified by

 * @pid must be a thread-group leader. This restriction currently exists

 * for all aspects of pidfds including pidfd creation (CLONE_PIDFD cannot

 * be used with CLONE_THREAD) and pidfd polling (only supports thread group

 * leaders).

 *

 * Return: On success, a cloexec pidfd is returned.

 *         On error, a negative errno number will be returned.

 Verify no one has done anything silly: */

 bump default and minimum pid_max based on number of cpus */

/**

 * sys_pidfd_getfd() - Get a file descriptor from another process

 *

 * @pidfd:	the pidfd file descriptor of the process

 * @fd:		the file descriptor number to get

 * @flags:	flags on how to get the fd (reserved)

 *

 * This syscall gets a copy of a file descriptor from another process

 * based on the pidfd, and file descriptor number. It requires that

 * the calling process has the ability to ptrace the process represented

 * by the pidfd. The process which is having its file descriptor copied

 * is otherwise unaffected.

 *

 * Return: On success, a cloexec file descriptor is returned.

 *         On error, a negative errno number will be returned.

 flags is currently unused - make sure it's unset */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/signal.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  1997-11-02  Modified for POSIX.1b signals by Richard Henderson

 *

 *  2003-06-02  Jim Houston - Concurrent Computer Corp.

 *		Changes to use preallocated sigqueue structures

 *		to allow signals to be sent reliably.

 for syscall_get_* */

/*

 * SLAB caches for signal bits.

 Is it explicitly or implicitly ignored? */

 SIGKILL and SIGSTOP may not be sent to the global init */

 Only allow kernel generated signals to this kthread */

	/*

	 * Blocked signals are never ignored, since the

	 * signal handler may change by the time it is

	 * unblocked.

	/*

	 * Tracers may want to know about even ignored signal unless it

	 * is SIGKILL which can't be reported anyway but can be ignored

	 * by SIGNAL_UNKILLABLE task.

/*

 * Re-calculate pending state from the set of locally pending

 * signals, globally pending signals, and blocked signals.

	/*

	 * We must never clear the flag in another thread, or in current

	 * when it's possible the current syscall is returning -ERESTART*.

	 * So we don't clear it here, and only callers who know they should do.

/*

 * After recalculating TIF_SIGPENDING, we need to make sure the task wakes up.

 * This is superfluous when called on current, the wakeup is a harmless no-op.

	/* Have any signals or users of TIF_SIGPENDING been delayed

	 * until after fork?

 Given the mask, find the first available signal that should be serviced. */

	/*

	 * Handle the first word specially: it contains the

	 * synchronous signals that need to be dequeued first.

 Nothing to do */

/**

 * task_set_jobctl_pending - set jobctl pending bits

 * @task: target task

 * @mask: pending bits to set

 *

 * Clear @mask from @task->jobctl.  @mask must be subset of

 * %JOBCTL_PENDING_MASK | %JOBCTL_STOP_CONSUME | %JOBCTL_STOP_SIGMASK |

 * %JOBCTL_TRAPPING.  If stop signo is being set, the existing signo is

 * cleared.  If @task is already being killed or exiting, this function

 * becomes noop.

 *

 * CONTEXT:

 * Must be called with @task->sighand->siglock held.

 *

 * RETURNS:

 * %true if @mask is set, %false if made noop because @task was dying.

/**

 * task_clear_jobctl_trapping - clear jobctl trapping bit

 * @task: target task

 *

 * If JOBCTL_TRAPPING is set, a ptracer is waiting for us to enter TRACED.

 * Clear it and wake up the ptracer.  Note that we don't need any further

 * locking.  @task->siglock guarantees that @task->parent points to the

 * ptracer.

 *

 * CONTEXT:

 * Must be called with @task->sighand->siglock held.

 advised by wake_up_bit() */

/**

 * task_clear_jobctl_pending - clear jobctl pending bits

 * @task: target task

 * @mask: pending bits to clear

 *

 * Clear @mask from @task->jobctl.  @mask must be subset of

 * %JOBCTL_PENDING_MASK.  If %JOBCTL_STOP_PENDING is being cleared, other

 * STOP bits are cleared together.

 *

 * If clearing of @mask leaves no stop or trap pending, this function calls

 * task_clear_jobctl_trapping().

 *

 * CONTEXT:

 * Must be called with @task->sighand->siglock held.

/**

 * task_participate_group_stop - participate in a group stop

 * @task: task participating in a group stop

 *

 * @task has %JOBCTL_STOP_PENDING set and is participating in a group stop.

 * Group stop states are cleared and the group stop count is consumed if

 * %JOBCTL_STOP_CONSUME was set.  If the consumption completes the group

 * stop, the appropriate `SIGNAL_*` flags are set.

 *

 * CONTEXT:

 * Must be called with @task->sighand->siglock held.

 *

 * RETURNS:

 * %true if group stop completion should be notified to the parent, %false

 * otherwise.

	/*

	 * Tell the caller to notify completion iff we are entering into a

	 * fresh group stop.  Read comment in do_signal_stop() for details.

 Have the new thread join an on-going signal group stop */

/*

 * allocate a new signal queue record

 * - this may be called without locks if and only if t == current, otherwise an

 *   appropriate lock must be held to stop the target task from exiting

	/*

	 * Protect access to @t credentials. This can go away when all

	 * callers hold rcu read lock.

	 *

	 * NOTE! A pending signal will hold on to the user refcount,

	 * and we get/put the refcount only when the sigpending count

	 * changes from/to zero.

/*

 * Flush all pending signals for this kthread.

/*

 * Flush all handlers for a task.

 if ptraced, let the tracer determine */

	/*

	 * Collect the siginfo appropriate to this signal.  Check if

	 * there is another siginfo for the same signal.

		/*

		 * Ok, it wasn't in the queue.  This must be

		 * a fast-pathed signal or we must have been

		 * out of queue space.  So zero out the info.

/*

 * Dequeue a signal and return the element to the caller, which is

 * expected to free it.

 *

 * All callers have to hold the siglock.

	/* We only dequeue private signals from ourselves, we don't let

	 * signalfd steal them

		/*

		 * itimer signal ?

		 *

		 * itimers are process shared and we restart periodic

		 * itimers in the signal delivery path to prevent DoS

		 * attacks in the high resolution timer case. This is

		 * compliant with the old way of self-restarting

		 * itimers, as the SIGALRM is a legacy signal and only

		 * queued once. Changing the restart behaviour to

		 * restart the timer in the signal dequeue path is

		 * reducing the timer noise on heavy loaded !highres

		 * systems too.

		/*

		 * Set a marker that we have dequeued a stop signal.  Our

		 * caller might release the siglock and then the pending

		 * stop signal it is about to process is no longer in the

		 * pending bitmasks, but must still be cleared by a SIGCONT

		 * (and overruled by a SIGKILL).  So those cases clear this

		 * shared flag after we've set it.  Note that this flag may

		 * remain set after the signal we return is ignored or

		 * handled.  That doesn't matter because its only purpose

		 * is to alert stop-signal processing code when another

		 * processor has come along and cleared the flag.

		/*

		 * Release the siglock to ensure proper locking order

		 * of timer locks outside of siglocks.  Note, we leave

		 * irqs disabled here, since the posix-timers code is

		 * about to disable them again anyway.

 Don't expose the si_sys_private value to userspace */

	/*

	 * Might a synchronous signal be in the queue?

	/*

	 * Return the first synchronous signal in the queue.

 Synchronous signals have a positive si_code */

	/*

	 * Check if there is another siginfo for the same signal.

/*

 * Tell a process that it has a new active signal..

 *

 * NOTE! we rely on the previous spin_lock to

 * lock interrupts for us! We can only be called with

 * "siglock" held, and the local interrupt must

 * have been disabled when that got acquired!

 *

 * No need to set need_resched since signal event passing

 * goes through ->blocked

	/*

	 * TASK_WAKEKILL also means wake it up in the stopped/traced/killable

	 * case. We don't check t->state here because there is a race with it

	 * executing another processor and just now entering stopped state.

	 * By using wake_up_state, we ensure the process will wake up and

	 * handle its death signal.

/*

 * Remove signals in mask from the pending set and queue.

 * Returns 1 if any signals were found.

 *

 * All callers must be holding the siglock.

/*

 * called with RCU read lock from check_kill_permission()

/*

 * Bad permissions for sending the signal

 * - the caller must hold the RCU read lock

 Let audit system see the signal */

			/*

			 * We don't return the error if sid == NULL. The

			 * task was unhashed, the caller must notice this.

/**

 * ptrace_trap_notify - schedule trap to notify ptracer

 * @t: tracee wanting to notify tracer

 *

 * This function schedules sticky ptrace trap which is cleared on the next

 * TRAP_STOP to notify ptracer of an event.  @t must have been seized by

 * ptracer.

 *

 * If @t is running, STOP trap will be taken.  If trapped for STOP and

 * ptracer is listening for events, tracee is woken up so that it can

 * re-trap for the new event.  If trapped otherwise, STOP trap will be

 * eventually taken without returning to userland after the existing traps

 * are finished by PTRACE_CONT.

 *

 * CONTEXT:

 * Must be called with @task->sighand->siglock held.

/*

 * Handle magic process-wide effects of stop/continue signals. Unlike

 * the signal actions, these happen immediately at signal-generation

 * time regardless of blocking, ignoring, or handling.  This does the

 * actual continuing for SIGCONT, but not the actual stopping for stop

 * signals. The process stop is done as a signal action for SIG_DFL.

 *

 * Returns true if the signal should be actually delivered, otherwise

 * it should be dropped.

		/*

		 * The process is in the middle of dying, nothing to do.

		/*

		 * This is a stop signal.  Remove SIGCONT from all queues.

		/*

		 * Remove all stop signals from all queues, wake all threads.

		/*

		 * Notify the parent with CLD_CONTINUED if we were stopped.

		 *

		 * If we were in the middle of a group stop, we pretend it

		 * was already finished, and then continued. Since SIGCHLD

		 * doesn't queue we report only CLD_STOPPED, as if the next

		 * CLD_CONTINUED was dropped.

			/*

			 * The first thread which returns from do_signal_stop()

			 * will take ->siglock, notice SIGNAL_CLD_MASK, and

			 * notify its parent. See get_signal().

/*

 * Test if P wants to take SIG.  After we've checked all threads with this,

 * it's equivalent to finding no threads not blocking SIG.  Any threads not

 * blocking SIG were ruled out because they are not running and already

 * have pending signals.  Such threads will dequeue from the shared queue

 * as soon as they're available, so putting the signal on the shared queue

 * will be equivalent to sending it to one such thread.

	/*

	 * Now find a thread we can wake up to take the signal off the queue.

	 *

	 * If the main thread wants the signal, it gets first crack.

	 * Probably the least surprising to the average bear.

		/*

		 * There is just one thread and it does not need to be woken.

		 * It will dequeue unblocked signals before it runs again.

		/*

		 * Otherwise try to find a suitable thread.

				/*

				 * No thread needs to be woken.

				 * Any eligible threads will see

				 * the signal in the queue soon.

	/*

	 * Found a killable thread.  If the signal will be fatal,

	 * then start taking the whole group down immediately.

		/*

		 * This signal will be fatal to the whole group.

			/*

			 * Start a group exit and wake everybody up.

			 * This way we don't have other threads

			 * running and doing things after a slower

			 * thread has the fatal signal pending.

	/*

	 * The signal is already in the shared-pending queue.

	 * Tell the chosen thread to wake up and dequeue it.

	/*

	 * Short-circuit ignored signals and support queuing

	 * exactly one non-rt signal, so that we can get more

	 * detailed information about the cause of the signal.

	/*

	 * Skip useless siginfo allocation for SIGKILL and kernel threads.

	/*

	 * Real-time signals must be queued if sent by sigqueue, or

	 * some other real-time mechanism.  It is implementation

	 * defined whether kill() does so.  We attempt to do so, on

	 * the principle of least surprise, but since kill is not

	 * allowed to fail with EAGAIN when low on memory we just

	 * make sure at least one signal gets delivered and don't

	 * pass on the info struct.

		/*

		 * Queue overflow, abort.  We may abort if the

		 * signal was rt and sent by user using something

		 * other than kill().

		/*

		 * This is a silent loss of information.  We still

		 * send the signal, but the *info bits are lost.

 Let multiprocess signals appear after on-going forks */

 Can't queue both a stop and a continue signal */

 Should SIGKILL or SIGSTOP be received by a pid namespace init? */

 Force if sent from an ancestor pid namespace */

 Don't ignore kernel generated signals */

 SIGKILL and SIGSTOP is special or has ids */

 A kernel generated signal? */

 From an ancestor pid namespace? */

 If reachable use the current handler */

 Always use SIG_DFL handler semantics */

 Only visible as the process exit code */

/*

 * Force a signal that the process can't ignore: if necessary

 * we unblock the signal and change any SIG_IGN to SIG_DFL.

 *

 * Note: If we unblock the signal, we always reset it to SIG_DFL,

 * since we do not want to have a signal handler that was blocked

 * be invoked when user space had explicitly blocked it.

 *

 * We don't want to have recursive SIGSEGV's etc, for example,

 * that is why we also clear SIGNAL_UNKILLABLE.

	/*

	 * Don't clear SIGNAL_UNKILLABLE for traced tasks, users won't expect

	 * debugging to leave init killable.

/*

 * Nuke all other threads in the group.

 Don't bother with already dead threads */

		/*

		 * This sighand can be already freed and even reused, but

		 * we rely on SLAB_TYPESAFE_BY_RCU and sighand_ctor() which

		 * initializes ->siglock: this slab can't go away, it has

		 * the same object type, ->siglock can't be reinitialized.

		 *

		 * We need to ensure that tsk->sighand is still the same

		 * after we take the lock, we can race with de_thread() or

		 * __exit_signal(). In the latter case the next iteration

		 * must see ->sighand == NULL.

/*

 * send signal info to all the members of a group

/*

 * __kill_pgrp_info() sends a signal to a process group: this is what the tty

 * control characters do (^C, ^Z etc)

 * - the caller must hold at least a readlock on tasklist_lock

		/*

		 * The task was unhashed in between, try again.  If it

		 * is dead, pid_task() will return NULL, if we race with

		 * de_thread() it will find the new leader.

/*

 * The usb asyncio usage of siginfo is wrong.  The glibc support

 * for asyncio which uses SI_ASYNCIO assumes the layout is SIL_RT.

 * AKA after the generic fields:

 *	kernel_pid_t	si_pid;

 *	kernel_uid32_t	si_uid;

 *	sigval_t	si_value;

 *

 * Unfortunately when usb generates SI_ASYNCIO it assumes the layout

 * after the generic fields is:

 *	void __user 	*si_addr;

 *

 * This is a practical problem when there is a 64bit big endian kernel

 * and a 32bit userspace.  As the 32bit address will encoded in the low

 * 32bits of the pointer.  Those low 32bits will be stored at higher

 * address than appear in a 32 bit pointer.  So userspace will not

 * see the address it was expecting for it's completions.

 *

 * There is nothing in the encoding that can allow

 * copy_siginfo_to_user32 to detect this confusion of formats, so

 * handle this by requiring the caller of kill_pid_usb_asyncio to

 * notice when this situration takes place and to store the 32bit

 * pointer in sival_int, instead of sival_addr of the sigval_t addr

 * parameter.

/*

 * kill_something_info() interprets pid in interesting ways just like kill(2).

 *

 * POSIX specifies that kill(-1,sig) is unspecified, but what we have

 * is probably wrong.  Should make it like BSD or SYSV.

 -INT_MIN is undefined.  Exclude this case to avoid a UBSAN warning */

/*

 * These are for backward compatibility with the rest of the kernel source.

	/*

	 * Make sure legacy kernel users don't send in bad values

	 * (normal paths check this in check_kill_permission).

/*

 * When things go south during signal handling, we

 * will force a SIGSEGV. And if the signal that caused

 * the problem was already a SIGSEGV, we'll want to

 * make sure we don't even try to deliver the signal..

/**

 * force_sig_seccomp - signals the task to allow in-process syscall emulation

 * @syscall: syscall number to send to userland

 * @reason: filter-supplied reason code to send to userland (via si_errno)

 *

 * Forces a SIGSYS with a code of SYS_SECCOMP and related sigsys info.

/* For the crazy architectures that include trap information in

 * the errno field, instead of an actual errno value.

/* For the rare architectures that include trap information using

 * si_trapno.

/* For the rare architectures that include trap information using

 * si_trapno.

/*

 * These functions support sending signals using preallocated sigqueue

 * structures.  This is needed "because realtime applications cannot

 * afford to lose notifications of asynchronous events, like timer

 * expirations or I/O completions".  In the case of POSIX Timers

 * we allocate the sigqueue structure from the timer_create.  If this

 * allocation fails we are able to report the failure to the application

 * with an EAGAIN error.

	/*

	 * We must hold ->siglock while testing q->list

	 * to serialize with collect_signal() or with

	 * __exit_signal()->flush_sigqueue().

	/*

	 * If it is queued it will be freed when dequeued,

	 * like the "regular" sigqueue.

 the signal is ignored */

		/*

		 * If an SI_TIMER entry is already queue just increment

		 * the overrun count.

/*

 * Let a parent know about the death of a child.

 * For a stopped/continued status change, use do_notify_parent_cldstop instead.

 *

 * Returns true if our parent ignored us and so we've switched to

 * self-reaping.

 do_notify_parent_cldstop should have been called instead.  */

 Wake up all pidfd waiters */

		/*

		 * This is only possible if parent == real_parent.

		 * Check if it has changed security domain.

	/*

	 * We are under tasklist_lock here so our parent is tied to

	 * us and cannot change.

	 *

	 * task_active_pid_ns will always return the same pid namespace

	 * until a task passes through release_task.

	 *

	 * write_lock() currently calls preempt_disable() which is the

	 * same as rcu_read_lock(), but according to Oleg, this is not

	 * correct to rely on this

		/*

		 * We are exiting and our parent doesn't care.  POSIX.1

		 * defines special semantics for setting SIGCHLD to SIG_IGN

		 * or setting the SA_NOCLDWAIT flag: we should be reaped

		 * automatically and not left for our parent's wait4 call.

		 * Rather than having the parent do it as a magic kind of

		 * signal handler, we just set this to tell do_exit that we

		 * can be cleaned up without becoming a zombie.  Note that

		 * we still call __wake_up_parent in this case, because a

		 * blocked sys_wait4 might now return -ECHILD.

		 *

		 * Whether we send SIGCHLD or not for SA_NOCLDWAIT

		 * is implementation-defined: we do (if you don't want

		 * it, just use SIG_IGN instead).

	/*

	 * Send with __send_signal as si_pid and si_uid are in the

	 * parent's namespaces.

/**

 * do_notify_parent_cldstop - notify parent of stopped/continued state change

 * @tsk: task reporting the state change

 * @for_ptracer: the notification is for ptracer

 * @why: CLD_{CONTINUED|STOPPED|TRAPPED} to report

 *

 * Notify @tsk's parent that the stopped/continued state has changed.  If

 * @for_ptracer is %false, @tsk's group leader notifies to its real parent.

 * If %true, @tsk reports to @tsk->parent which should be the ptracer.

 *

 * CONTEXT:

 * Must be called with tasklist_lock at least read locked.

	/*

	 * see comment in do_notify_parent() about the following 4 lines

	/*

	 * Even if SIGCHLD is not generated, we must wake up wait4 calls.

/*

 * This must be called with current->sighand->siglock held.

 *

 * This should be the path for all ptrace stops.

 * We always set current->last_siginfo while stopped here.

 * That makes it a way to test a stopped process for

 * being ptrace-stopped vs being job-control-stopped.

 *

 * If we actually decide not to stop at all because the tracer

 * is gone, we keep current->exit_code unless clear_code.

		/*

		 * The arch code has something special to do before a

		 * ptrace stop.  This is allowed to block, e.g. for faults

		 * on user stack pages.  We can't keep the siglock while

		 * calling arch_ptrace_stop, so we must release it now.

		 * To preserve proper semantics, we must do this before

		 * any signal bookkeeping like checking group_stop_count.

	/*

	 * schedule() will not sleep if there is a pending signal that

	 * can awaken the task.

	/*

	 * We're committing to trapping.  TRACED should be visible before

	 * TRAPPING is cleared; otherwise, the tracer might fail do_wait().

	 * Also, transition to TRACED and updates to ->jobctl should be

	 * atomic with respect to siglock and should be done after the arch

	 * hook as siglock is released and regrabbed across it.

	 *

	 *     TRACER				    TRACEE

	 *

	 *     ptrace_attach()

	 * [L]   wait_on_bit(JOBCTL_TRAPPING)	[S] set_special_state(TRACED)

	 *     do_wait()

	 *       set_current_state()                smp_wmb();

	 *       ptrace_do_wait()

	 *         wait_task_stopped()

	 *           task_stopped_code()

	 * [L]         task_is_traced()		[S] task_clear_jobctl_trapping();

	/*

	 * If @why is CLD_STOPPED, we're trapping to participate in a group

	 * stop.  Do the bookkeeping.  Note that if SIGCONT was delievered

	 * across siglock relocks since INTERRUPT was scheduled, PENDING

	 * could be clear now.  We act as if SIGCONT is received after

	 * TASK_TRACED is entered - ignore it.

 any trap clears pending STOP trap, STOP trap clears NOTIFY */

 entering a trap, clear TRAPPING */

		/*

		 * Notify parents of the stop.

		 *

		 * While ptraced, there are two parents - the ptracer and

		 * the real_parent of the group_leader.  The ptracer should

		 * know about every stop while the real parent is only

		 * interested in the completion of group stop.  The states

		 * for the two don't interact with each other.  Notify

		 * separately unless they're gonna be duplicates.

		/*

		 * Don't want to allow preemption here, because

		 * sys_ptrace() needs this task to be inactive.

		 *

		 * XXX: implement read_unlock_no_resched().

		/*

		 * By the time we got the lock, our tracer went away.

		 * Don't drop the lock yet, another tracer may come.

		 *

		 * If @gstop_done, the ptracer went away between group stop

		 * completion and here.  During detach, it would have set

		 * JOBCTL_STOP_PENDING on us and we'll re-enter

		 * TASK_STOPPED in do_signal_stop() on return, so notifying

		 * the real parent of the group stop completion is enough.

 tasklist protects us from ptrace_freeze_traced() */

	/*

	 * We are back.  Now reacquire the siglock before touching

	 * last_siginfo, so that we are sure to have synchronized with

	 * any signal-sending on another CPU that wants to examine it.

 LISTENING can be set only during STOP traps, clear it */

	/*

	 * Queued signals ignored us while we were stopped for tracing.

	 * So check for any that we should take before resuming user mode.

	 * This sets TIF_SIGPENDING, but never clears it.

 Let the debugger run.  */

/**

 * do_signal_stop - handle group stop for SIGSTOP and other stop signals

 * @signr: signr causing group stop if initiating

 *

 * If %JOBCTL_STOP_PENDING is not set yet, initiate group stop with @signr

 * and participate in it.  If already set, participate in the existing

 * group stop.  If participated in a group stop (and thus slept), %true is

 * returned with siglock released.

 *

 * If ptraced, this function doesn't handle stop itself.  Instead,

 * %JOBCTL_TRAP_STOP is scheduled and %false is returned with siglock

 * untouched.  The caller must ensure that INTERRUPT trap handling takes

 * places afterwards.

 *

 * CONTEXT:

 * Must be called with @current->sighand->siglock held, which is released

 * on %true return.

 *

 * RETURNS:

 * %false if group stop is already cancelled or ptrace trap is scheduled.

 * %true if participated in group stop.

 signr will be recorded in task->jobctl for retries */

		/*

		 * There is no group stop already in progress.  We must

		 * initiate one now.

		 *

		 * While ptraced, a task may be resumed while group stop is

		 * still in effect and then receive a stop signal and

		 * initiate another group stop.  This deviates from the

		 * usual behavior as two consecutive stop signals can't

		 * cause two group stops when !ptraced.  That is why we

		 * also check !task_is_stopped(t) below.

		 *

		 * The condition can be distinguished by testing whether

		 * SIGNAL_STOP_STOPPED is already set.  Don't generate

		 * group_exit_code in such case.

		 *

		 * This is not necessary for SIGNAL_STOP_CONTINUED because

		 * an intervening stop signal is required to cause two

		 * continued events regardless of ptrace.

			/*

			 * Setting state to TASK_STOPPED for a group

			 * stop is always done with the siglock held,

			 * so this check has no races.

		/*

		 * If there are no other threads in the group, or if there

		 * is a group stop in progress and we are the last to stop,

		 * report to the parent.

		/*

		 * Notify the parent of the group stop completion.  Because

		 * we're not holding either the siglock or tasklist_lock

		 * here, ptracer may attach inbetween; however, this is for

		 * group stop and should always be delivered to the real

		 * parent of the group leader.  The new ptracer will get

		 * its notification when this task transitions into

		 * TASK_TRACED.

 Now we don't run again until woken by SIGCONT or SIGKILL */

		/*

		 * While ptraced, group stop is handled by STOP trap.

		 * Schedule it and let the caller deal with it.

/**

 * do_jobctl_trap - take care of ptrace jobctl traps

 *

 * When PT_SEIZED, it's used for both group stop and explicit

 * SEIZE/INTERRUPT traps.  Both generate PTRACE_EVENT_STOP trap with

 * accompanying siginfo.  If stopped, lower eight bits of exit_code contain

 * the stop signal; otherwise, %SIGTRAP.

 *

 * When !PT_SEIZED, it's used only for group stop trap with stop signal

 * number as exit_code and no siginfo.

 *

 * CONTEXT:

 * Must be called with @current->sighand->siglock held, which may be

 * released and re-acquired before returning with intervening sleep.

/**

 * do_freezer_trap - handle the freezer jobctl trap

 *

 * Puts the task into frozen state, if only the task is not about to quit.

 * In this case it drops JOBCTL_TRAP_FREEZE.

 *

 * CONTEXT:

 * Must be called with @current->sighand->siglock held,

 * which is always released before returning.

	/*

	 * If there are other trap bits pending except JOBCTL_TRAP_FREEZE,

	 * let's make another loop to give it a chance to be handled.

	 * In any case, we'll return back.

	/*

	 * Now we're sure that there is no pending fatal signal and no

	 * pending traps. Clear TIF_SIGPENDING to not get out of schedule()

	 * immediately (if there is a non-fatal signal pending), and

	 * put the task into sleep.

	/*

	 * We do not check sig_kernel_stop(signr) but set this marker

	 * unconditionally because we do not know whether debugger will

	 * change signr. This flag has no meaning unless we are going

	 * to stop after return from ptrace_stop(). In this case it will

	 * be checked in do_signal_stop(), we should only stop if it was

	 * not cleared by SIGCONT while we were sleeping. See also the

	 * comment in dequeue_signal().

 We're back.  Did the debugger cancel the sig?  */

	/*

	 * Update the siginfo structure if the signal has

	 * changed.  If the debugger wanted something

	 * specific in the siginfo structure then it should

	 * have updated *info via PTRACE_SETSIGINFO.

 If the (new) signal is now blocked, requeue it.  */

	/*

	 * For non-generic architectures, check for TIF_NOTIFY_SIGNAL so

	 * that the arch handlers don't all have to do it. If we get here

	 * without TIF_SIGPENDING, just exit after running signal work.

	/*

	 * Do this once, we can't return to user-mode if freezing() == T.

	 * do_signal_stop() and ptrace_stop() do freezable_schedule() and

	 * thus do not need another check after return.

	/*

	 * Every stopped thread goes here after wakeup. Check to see if

	 * we should notify the parent, prepare_signal(SIGCONT) encodes

	 * the CLD_ si_code into SIGNAL_CLD_MASK bits.

		/*

		 * Notify the parent that we're continuing.  This event is

		 * always per-process and doesn't make whole lot of sense

		 * for ptracers, who shouldn't consume the state via

		 * wait(2) either, but, for backward compatibility, notify

		 * the ptracer of the group leader too unless it's gonna be

		 * a duplicate.

 Has this task already been marked for death? */

		/*

		 * If the task is leaving the frozen state, let's update

		 * cgroup counters and reset the frozen bit.

		/*

		 * Signals generated by the execution of an instruction

		 * need to be delivered before any other pending signals

		 * so that the instruction pointer in the signal stack

		 * frame points to the faulting instruction.

 will return 0 */

 Trace actually delivered signals. */

 Do nothing.  */

 Run the handler.  */

 will return non-zero "signr" value */

		/*

		 * Now we are doing the default action for this signal.

 Default is nothing. */

		/*

		 * Global init gets no signals it doesn't want.

		 * Container-init gets no signals it doesn't want from same

		 * container.

		 *

		 * Note that if global/container-init sees a sig_kernel_only()

		 * signal here, the signal must have been generated internally

		 * or must have come from an ancestor namespace. In either

		 * case, the signal cannot be dropped.

			/*

			 * The default action is to stop all threads in

			 * the thread group.  The job control signals

			 * do nothing in an orphaned pgrp, but SIGSTOP

			 * always works.  Note that siglock needs to be

			 * dropped during the call to is_orphaned_pgrp()

			 * because of lock ordering with tasklist_lock.

			 * This allows an intervening SIGCONT to be posted.

			 * We need to check for that and bail out if necessary.

 signals can be posted during this window */

 It released the siglock.  */

			/*

			 * We didn't actually stop, due to a race

			 * with SIGCONT or something like that.

		/*

		 * Anything else is fatal, maybe with a core dump.

			/*

			 * If it was able to dump core, this kills all

			 * other threads in the group and synchronizes with

			 * their demise.  If we lost the race with another

			 * thread getting here, it set group_exit_code

			 * first and our do_group_exit call below will use

			 * that value and ignore the one we pass it.

		/*

		 * PF_IO_WORKER threads will catch and exit on fatal signals

		 * themselves. They have cleanup that must be performed, so

		 * we cannot call do_exit() on their behalf.

		/*

		 * Death signals, no core dump.

 NOTREACHED */

/**

 * signal_delivered - 

 * @ksig:		kernel signal struct

 * @stepping:		nonzero if debugger single-step or block-step in use

 *

 * This function should be called when a signal has successfully been

 * delivered. It updates the blocked signals accordingly (@ksig->ka.sa.sa_mask

 * is always blocked, and the signal itself is blocked unless %SA_NODEFER

 * is set in @ksig->ka.sa.sa_flags.  Tracing is notified.

	/* A signal was successfully delivered, and the

	   saved sigmask was stored on the signal frame,

	   and will be restored by sigreturn.  So we can

/*

 * It could be that complete_signal() picked us to notify about the

 * group-wide signal. Other threads should be notified now to take

 * the shared signals in @which since we will not.

 Remove the signals this thread can handle. */

	/*

	 * @tsk is about to have PF_EXITING set - lock out users which

	 * expect stable threadgroup.

	/*

	 * From now this task is not visible for group-wide signals,

	 * see wants_signal(), do_signal_stop().

	/*

	 * If group stop has completed, deliver the notification.  This

	 * should always go to the real parent of the group leader.

/*

 * System call entry points.

/**

 *  sys_restart_syscall - restart a system call

 A set of now blocked but previously unblocked signals. */

/**

 * set_current_blocked - change current->blocked mask

 * @newset: new mask

 *

 * It is wrong to change ->blocked directly, this helper should be used

 * to ensure the process can't miss a shared signal we are going to block.

	/*

	 * In case the signal mask hasn't changed, there is nothing we need

	 * to do. The current->blocked shouldn't be modified by other task.

/*

 * This is also useful for kernel threads that want to temporarily

 * (or permanently) block certain signals.

 *

 * NOTE! Unlike the user-mode sys_sigprocmask(), the kernel

 * interface happily blocks "unblockable" signals like SIGKILL

 * and friends.

 Lockless, only current can change ->blocked, never from irq */

/*

 * The api helps set app-provided sigmasks.

 *

 * This is useful for syscalls such as ppoll, pselect, io_pgetevents and

 * epoll_pwait where a new sigmask is passed from userland for the syscalls.

 *

 * Note that it does set_restore_sigmask() in advance, so it must be always

 * paired with restore_saved_sigmask_unless() before return from syscall.

/**

 *  sys_rt_sigprocmask - change the list of currently blocked signals

 *  @how: whether to add, remove, or set signals

 *  @nset: stores pending signals

 *  @oset: previous value of signal mask if non-null

 *  @sigsetsize: size of sigset_t type

 XXX: Don't preclude handling different sized sigset_t's.  */

 XXX: Don't preclude handling different sized sigset_t's.  */

 Outside the lock because only this thread touches it.  */

/**

 *  sys_rt_sigpending - examine a pending signal that has been raised

 *			while blocked

 *  @uset: stores pending signals

 *  @sigsetsize: size of sigset_t type or larger

 Handle the exceptions */

		/*

		 * An unknown si_code might need more than

		 * sizeof(struct kernel_siginfo) bytes.  Verify all of the

		 * extra bytes are 0.  This guarantees copy_siginfo_to_user

		 * will return this data to userspace exactly.

/**

 * copy_siginfo_to_external32 - copy a kernel siginfo into a compat user siginfo

 * @to: compat siginfo destination

 * @from: kernel siginfo source

 *

 * Note: This function does not work properly for the SIGCHLD on x32, but

 * fortunately it doesn't have to.  The only valid callers for this function are

 * copy_siginfo_to_user32, which is overriden for x32 and the coredump code.

 * The latter does not care because SIGCHLD will never cause a coredump.

 CONFIG_COMPAT */

/**

 *  do_sigtimedwait - wait for queued signals specified in @which

 *  @which: queued signals to wait for

 *  @info: if non-null, the signal's siginfo is returned here

 *  @ts: upper bound on process time suspension

	/*

	 * Invert the set of allowed signals to get those we want to block.

		/*

		 * None ready, temporarily unblock those we're interested

		 * while we are sleeping in so that we'll be awakened when

		 * they arrive. Unblocking is always fine, we can avoid

		 * set_current_blocked().

/**

 *  sys_rt_sigtimedwait - synchronously wait for queued signals specified

 *			in @uthese

 *  @uthese: queued signals to wait for

 *  @uinfo: if non-null, the signal's siginfo is returned here

 *  @uts: upper bound on process time suspension

 *  @sigsetsize: size of sigset_t type

 XXX: Don't preclude handling different sized sigset_t's.  */

/**

 *  sys_kill - send a signal to a process

 *  @pid: the PID of the process

 *  @sig: signal to be sent

/*

 * Verify that the signaler and signalee either are in the same pid namespace

 * or that the signaler's pid namespace is an ancestor of the signalee's pid

 * namespace.

	/*

	 * Avoid hooking up compat syscalls and instead handle necessary

	 * conversions here. Note, this is a stop-gap measure and should not be

	 * considered a generic solution.

/**

 * sys_pidfd_send_signal - Signal a process through a pidfd

 * @pidfd:  file descriptor of the process

 * @sig:    signal to send

 * @info:   signal info

 * @flags:  future flags

 *

 * The syscall currently only signals via PIDTYPE_PID which covers

 * kill(<positive-pid>, <signal>. It does not signal threads or process

 * groups.

 * In order to extend the syscall to threads and process groups the @flags

 * argument should be used. In essence, the @flags argument will determine

 * what is signaled and not the file descriptor itself. Put in other words,

 * grouping is a property of the flags argument not a property of the file

 * descriptor.

 *

 * Return: 0 on success, negative errno on failure

 Enforce flags be set to 0 until we add an extension. */

 Is this a pidfd? */

 Only allow sending arbitrary signals to yourself. */

		/*

		 * The null signal is a permissions and process existence

		 * probe.  No signal is actually delivered.

			/*

			 * If lock_task_sighand() failed we pretend the task

			 * dies after receiving the signal. The window is tiny,

			 * and the signal is private anyway.

/**

 *  sys_tgkill - send signal to one specific thread

 *  @tgid: the thread group ID of the thread

 *  @pid: the PID of the thread

 *  @sig: signal to be sent

 *

 *  This syscall also checks the @tgid and returns -ESRCH even if the PID

 *  exists but it's not belonging to the target process anymore. This

 *  method solves the problem of threads exiting and PIDs getting reused.

 This is only valid for single tasks */

/**

 *  sys_tkill - send signal to one specific task

 *  @pid: the PID of the task

 *  @sig: signal to be sent

 *

 *  Send a signal to only one task, even if it's a CLONE_THREAD task.

 This is only valid for single tasks */

	/* Not even root can pretend to send signals from the kernel.

	 * Nor can they impersonate a kill()/tgkill(), which adds source info.

 POSIX.1b doesn't mention process groups.  */

/**

 *  sys_rt_sigqueueinfo - send signal information to a signal

 *  @pid: the PID of the thread

 *  @sig: signal to be sent

 *  @uinfo: signal info to be sent

 This is only valid for single tasks */

	/* Not even root can pretend to send signals from the kernel.

	 * Nor can they impersonate a kill()/tgkill(), which adds source info.

/*

 * For kthreads only, must not be used if cloned with CLONE_SIGHAND

	/*

	 * Make sure that we never accidentally claim to support SA_UNSUPPORTED,

	 * e.g. by having an architecture use the bit in their uapi.

	/*

	 * Clear unknown flag bits in order to allow userspace to detect missing

	 * support for flag bits and to allow the kernel to use non-uapi bits

	 * internally.

		/*

		 * POSIX 3.3.1.3:

		 *  "Setting a signal action to SIG_IGN for a signal that is

		 *   pending shall cause the pending signal to be discarded,

		 *   whether or not it is blocked."

		 *

		 *  "Setting a signal action to SIG_DFL for a signal that is

		 *   pending and whose default action is to ignore the signal

		 *   (for example, SIGCHLD), shall cause the pending signal to

		 *   be discarded, whether or not it is blocked"

 squash all but EFAULT for now */

 squash all but -EFAULT for now */

/**

 *  sys_sigpending - examine pending signals

 *  @uset: where mask of pending signal is returned

/**

 *  sys_sigprocmask - examine and change blocked signals

 *  @how: whether to add, remove, or set signals

 *  @nset: signals to add or remove (if non-null)

 *  @oset: previous value of signal mask if non-null

 *

 * Some platforms have their own version with special arguments;

 * others support only sys_rt_sigprocmask.

 __ARCH_WANT_SYS_SIGPROCMASK */

/**

 *  sys_rt_sigaction - alter an action taken by a process

 *  @sig: signal to be sent

 *  @act: new sigaction

 *  @oact: used to save the previous sigaction

 *  @sigsetsize: size of sigset_t type

 XXX: Don't preclude handling different sized sigset_t's.  */

 XXX: Don't preclude handling different sized sigset_t's.  */

 !CONFIG_ODD_RT_SIGACTION */

/*

 * For backwards compatibility.  Functionality superseded by sigprocmask.

 SMP safe */

 CONFIG_SGETMASK_SYSCALL */

/*

 * For backwards compatibility.  Functionality superseded by sigaction.

 __ARCH_WANT_SYS_SIGNAL */

/**

 *  sys_rt_sigsuspend - replace the signal mask for a value with the

 *	@unewset value until a signal is received

 *  @unewset: new signal mask value

 *  @sigsetsize: size of sigset_t type

 XXX: Don't preclude handling different sized sigset_t's.  */

 XXX: Don't preclude handling different sized sigset_t's.  */

 Verify the offsets in the two siginfos match */

 kill */

 timer */

 rt */

 sigchld */

 sigfault */

 sigpoll */

 sigsys */

 usb asyncio */

/*

 * kdb_send_sig - Allows kdb to send signals without exposing

 * signal internals.  This function checks if the required locks are

 * available before calling the main signal code, to avoid kdb

 * deadlocks.

 CONFIG_KGDB_KDB */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/workqueue.c - generic async execution with shared worker pool

 *

 * Copyright (C) 2002		Ingo Molnar

 *

 *   Derived from the taskqueue/keventd code by:

 *     David Woodhouse <dwmw2@infradead.org>

 *     Andrew Morton

 *     Kai Petzke <wpp@marie.physik.tu-berlin.de>

 *     Theodore Ts'o <tytso@mit.edu>

 *

 * Made to use alloc_percpu by Christoph Lameter.

 *

 * Copyright (C) 2010		SUSE Linux Products GmbH

 * Copyright (C) 2010		Tejun Heo <tj@kernel.org>

 *

 * This is the generic async execution mechanism.  Work items as are

 * executed in process context.  The worker pool is shared and

 * automatically managed.  There are two worker pools for each CPU (one for

 * normal work items and the other for high priority ones) and some extra

 * pools for workqueues which are not bound to any specific CPU - the

 * number of these backing pools is dynamic.

 *

 * Please read Documentation/core-api/workqueue.rst for details.

	/*

	 * worker_pool flags

	 *

	 * A bound pool is either associated or disassociated with its CPU.

	 * While associated (!DISASSOCIATED), all workers are bound to the

	 * CPU and none has %WORKER_UNBOUND set and concurrency management

	 * is in effect.

	 *

	 * While DISASSOCIATED, the cpu may be offline and all workers have

	 * %WORKER_UNBOUND set and concurrency management disabled, and may

	 * be executing on any CPU.  The pool behaves as an unbound one.

	 *

	 * Note that DISASSOCIATED should be flipped only while holding

	 * wq_pool_attach_mutex to avoid changing binding state while

	 * worker_attach_to_pool() is in progress.

 being managed */

 cpu can't serve workers */

 worker flags */

 die die die */

 is idle */

 preparing to run works */

 cpu intensive */

 worker is unbound */

 worker was rebound */

 # standard pools per cpu */

 hashed by pool->attrs */

 64 pointers */

 1/4 of busy can be idle */

 keep idle ones for 5 mins */

						/* call for help after 10ms

 and then every 100ms */

 time to breath after fail */

	/*

	 * Rescue workers are used only on emergencies and shared by

	 * all cpus.  Give MIN_NICE.

/*

 * Structure fields follow one of the following exclusion rules.

 *

 * I: Modifiable by initialization/destruction paths and read-only for

 *    everyone else.

 *

 * P: Preemption protected.  Disabling preemption is enough and should

 *    only be modified and accessed from the local cpu.

 *

 * L: pool->lock protected.  Access with pool->lock held.

 *

 * X: During normal operation, modification requires pool->lock and should

 *    be done only from local cpu.  Either disabling preemption on local

 *    cpu or grabbing pool->lock is enough for read access.  If

 *    POOL_DISASSOCIATED is set, it's identical to L.

 *

 * A: wq_pool_attach_mutex protected.

 *

 * PL: wq_pool_mutex protected.

 *

 * PR: wq_pool_mutex protected for writes.  RCU protected for reads.

 *

 * PW: wq_pool_mutex and wq->mutex protected for writes.  Either for reads.

 *

 * PWR: wq_pool_mutex and wq->mutex protected for writes.  Either or

 *      RCU for reads.

 *

 * WQ: wq->mutex protected.

 *

 * WR: wq->mutex protected for writes.  RCU protected for reads.

 *

 * MD: wq_mayday_lock protected.

 struct worker is defined in workqueue_internal.h */

 the pool lock */

 I: the associated cpu */

 I: the associated node ID */

 I: pool ID */

 X: flags */

 L: watchdog timestamp */

 L: list of pending works */

 L: total number of workers */

 L: currently idle workers */

 X: list of idle workers */

 L: worker idle timeout */

 L: SOS timer for workers */

 a workers is either on busy_hash or idle_list, or the manager */

 L: hash of busy workers */

 L: purely informational */

 A: attached workers */

 all workers detached */

 worker IDs for task name */

 I: worker attributes */

 PL: unbound_pool_hash node */

 PL: refcnt for unbound pools */

	/*

	 * The current concurrency level.  As it's likely to be accessed

	 * from other CPUs during try_to_wake_up(), put it in a separate

	 * cacheline.

	/*

	 * Destruction of pool is RCU protected to allow dereferences

	 * from get_work_pool().

/*

 * The per-pool workqueue.  While queued, the lower WORK_STRUCT_FLAG_BITS

 * of work_struct->data are used for flags and the remaining high bits

 * point to the pwq; thus, pwqs need to be aligned at two's power of the

 * number of flag bits.

 I: the associated pool */

 I: the owning workqueue */

 L: current color */

 L: flushing color */

 L: reference count */

 L: nr of in_flight works */

	/*

	 * nr_active management and WORK_STRUCT_INACTIVE:

	 *

	 * When pwq->nr_active >= max_active, new work item is queued to

	 * pwq->inactive_works instead of pool->worklist and marked with

	 * WORK_STRUCT_INACTIVE.

	 *

	 * All work items marked with WORK_STRUCT_INACTIVE do not participate

	 * in pwq->nr_active and all work items in pwq->inactive_works are

	 * marked with WORK_STRUCT_INACTIVE.  But not all WORK_STRUCT_INACTIVE

	 * work items are in pwq->inactive_works.  Some of them are ready to

	 * run in pool->worklist or worker->scheduled.  Those work itmes are

	 * only struct wq_barrier which is used for flush_work() and should

	 * not participate in pwq->nr_active.  For non-barrier work item, it

	 * is marked with WORK_STRUCT_INACTIVE iff it is in pwq->inactive_works.

 L: nr of active works */

 L: max active works */

 L: inactive works */

 WR: node on wq->pwqs */

 MD: node on wq->maydays */

	/*

	 * Release of unbound pwq is punted to system_wq.  See put_pwq()

	 * and pwq_unbound_release_workfn() for details.  pool_workqueue

	 * itself is also RCU protected so that the first pwq can be

	 * determined without grabbing wq->mutex.

/*

 * Structure used to wait for workqueue flush.

 WQ: list of flushers */

 WQ: flush color waiting for */

 flush completion */

/*

 * The externally visible workqueue.  It relays the issued work items to

 * the appropriate worker_pool through its pool_workqueues.

 WR: all pwqs of this wq */

 PR: list of all workqueues */

 protects this wq */

 WQ: current work color */

 WQ: current flush color */

 flush in progress */

 WQ: first flusher */

 WQ: flush waiters */

 WQ: flush overflow list */

 MD: pwqs requesting rescue */

 MD: rescue worker */

 WQ: drain in progress */

 WQ: saved pwq max_active */

 PW: only for unbound wqs */

 PW: only for unbound wqs */

 I: for sysfs interface */

 I: workqueue name */

	/*

	 * Destruction of workqueue_struct is RCU protected to allow walking

	 * the workqueues list without grabbing wq_pool_mutex.

	 * This is used to dump all workqueues from sysrq.

 hot fields used during command issue, aligned to cacheline */

 WQ: WQ_* flags */

 I: per-cpu pwqs */

 PWR: unbound pwqs indexed by node */

 possible CPUs of each node */

 see the comment above the definition of WQ_POWER_EFFICIENT */

 can kworkers be created yet? */

 unbound NUMA affinity enabled */

 buf for wq_update_unbound_numa_attrs(), protected by CPU hotplug exclusion */

 protects pools and workqueues list */

 protects worker attach/detach */

 protects wq->maydays list */

 wait for manager to go away */

 PR: list of all workqueues */

 PL: have wqs started freezing? */

 PL: allowable cpus for unbound wqs and work items */

 CPU where unbound work was last round robin scheduled from this CPU */

/*

 * Local execution of unbound work items is no longer guaranteed.  The

 * following always forces round-robin CPU selection on unbound work items

 * to uncover usages which depend on it.

 the per-cpu worker pools */

 PR: idr of all pools */

 PL: hash of all unbound pools keyed by pool->attrs */

 I: attributes used when instantiating standard unbound pools on demand */

 I: attributes used when instantiating ordered pools on demand */

/**

 * for_each_pool - iterate through all worker_pools in the system

 * @pool: iteration cursor

 * @pi: integer used for iteration

 *

 * This must be called either with wq_pool_mutex held or RCU read

 * locked.  If the pool needs to be used beyond the locking in effect, the

 * caller is responsible for guaranteeing that the pool stays online.

 *

 * The if/else clause exists only for the lockdep assertion and can be

 * ignored.

/**

 * for_each_pool_worker - iterate through all workers of a worker_pool

 * @worker: iteration cursor

 * @pool: worker_pool to iterate workers of

 *

 * This must be called with wq_pool_attach_mutex.

 *

 * The if/else clause exists only for the lockdep assertion and can be

 * ignored.

/**

 * for_each_pwq - iterate through all pool_workqueues of the specified workqueue

 * @pwq: iteration cursor

 * @wq: the target workqueue

 *

 * This must be called either with wq->mutex held or RCU read locked.

 * If the pwq needs to be used beyond the locking in effect, the caller is

 * responsible for guaranteeing that the pwq stays online.

 *

 * The if/else clause exists only for the lockdep assertion and can be

 * ignored.

/*

 * fixup_init is called when:

 * - an active object is initialized

/*

 * fixup_free is called when:

 * - an active object is freed

/**

 * worker_pool_assign_id - allocate ID and assign it to @pool

 * @pool: the pool pointer of interest

 *

 * Returns 0 if ID in [0, WORK_OFFQ_POOL_NONE) is allocated and assigned

 * successfully, -errno on failure.

/**

 * unbound_pwq_by_node - return the unbound pool_workqueue for the given node

 * @wq: the target workqueue

 * @node: the node ID

 *

 * This must be called with any of wq_pool_mutex, wq->mutex or RCU

 * read locked.

 * If the pwq needs to be used beyond the locking in effect, the caller is

 * responsible for guaranteeing that the pwq stays online.

 *

 * Return: The unbound pool_workqueue for @node.

	/*

	 * XXX: @node can be NUMA_NO_NODE if CPU goes offline while a

	 * delayed item is pending.  The plan is to keep CPU -> NODE

	 * mapping valid and stable across CPU on/offlines.  Once that

	 * happens, this workaround can be removed.

/*

 * While queued, %WORK_STRUCT_PWQ is set and non flag bits of a work's data

 * contain the pointer to the queued pwq.  Once execution starts, the flag

 * is cleared and the high bits contain OFFQ flags and pool ID.

 *

 * set_work_pwq(), set_work_pool_and_clear_pending(), mark_work_canceling()

 * and clear_work_data() can be used to set the pwq, pool or clear

 * work->data.  These functions should only be called while the work is

 * owned - ie. while the PENDING bit is set.

 *

 * get_work_pool() and get_work_pwq() can be used to obtain the pool or pwq

 * corresponding to a work.  Pool is available once the work has been

 * queued anywhere after initialization until it is sync canceled.  pwq is

 * available only while the work item is queued.

 *

 * %WORK_OFFQ_CANCELING is used to mark a work item which is being

 * canceled.  While being canceled, a work item may have its PENDING set

 * but stay off timer and worklist for arbitrarily long and nobody should

 * try to steal the PENDING bit.

	/*

	 * The following wmb is paired with the implied mb in

	 * test_and_set_bit(PENDING) and ensures all updates to @work made

	 * here are visible to and precede any updates by the next PENDING

	 * owner.

	/*

	 * The following mb guarantees that previous clear of a PENDING bit

	 * will not be reordered with any speculative LOADS or STORES from

	 * work->current_func, which is executed afterwards.  This possible

	 * reordering can lead to a missed execution on attempt to queue

	 * the same @work.  E.g. consider this case:

	 *

	 *   CPU#0                         CPU#1

	 *   ----------------------------  --------------------------------

	 *

	 * 1  STORE event_indicated

	 * 2  queue_work_on() {

	 * 3    test_and_set_bit(PENDING)

	 * 4 }                             set_..._and_clear_pending() {

	 * 5                                 set_work_data() # clear bit

	 * 6                                 smp_mb()

	 * 7                               work->current_func() {

	 * 8				      LOAD event_indicated

	 *				   }

	 *

	 * Without an explicit full barrier speculative LOAD on line 8 can

	 * be executed before CPU#0 does STORE on line 1.  If that happens,

	 * CPU#0 observes the PENDING bit is still set and new execution of

	 * a @work is not queued in a hope, that CPU#1 will eventually

	 * finish the queued @work.  Meanwhile CPU#1 does not see

	 * event_indicated is set, because speculative LOAD was executed

	 * before actual STORE.

 see set_work_pool_and_clear_pending() */

/**

 * get_work_pool - return the worker_pool a given work was associated with

 * @work: the work item of interest

 *

 * Pools are created and destroyed under wq_pool_mutex, and allows read

 * access under RCU read lock.  As such, this function should be

 * called under wq_pool_mutex or inside of a rcu_read_lock() region.

 *

 * All fields of the returned pool are accessible as long as the above

 * mentioned locking is in effect.  If the returned pool needs to be used

 * beyond the critical section, the caller is responsible for ensuring the

 * returned pool is and stays online.

 *

 * Return: The worker_pool @work was last associated with.  %NULL if none.

/**

 * get_work_pool_id - return the worker pool ID a given work is associated with

 * @work: the work item of interest

 *

 * Return: The worker_pool ID @work was last associated with.

 * %WORK_OFFQ_POOL_NONE if none.

/*

 * Policy functions.  These define the policies on how the global worker

 * pools are managed.  Unless noted otherwise, these functions assume that

 * they're being called with pool->lock held.

/*

 * Need to wake up a worker?  Called from anything but currently

 * running workers.

 *

 * Note that, because unbound workers never contribute to nr_running, this

 * function will always return %true for unbound pools as long as the

 * worklist isn't empty.

 Can I start working?  Called from busy but !running workers. */

 Do I need to keep working?  Called from currently running workers. */

 Do we need a new worker?  Called from manager. */

 Do we have too many workers and should some go away? */

 manager is considered idle */

/*

 * Wake up functions.

 Return the first idle worker.  Safe with preemption disabled */

/**

 * wake_up_worker - wake up an idle worker

 * @pool: worker pool to wake worker from

 *

 * Wake up the first idle worker of @pool.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

/**

 * wq_worker_running - a worker is running again

 * @task: task waking up

 *

 * This function is called when a worker returns from schedule()

/**

 * wq_worker_sleeping - a worker is going to sleep

 * @task: task going to sleep

 *

 * This function is called from schedule() when a busy worker is

 * going to sleep. Preemption needs to be disabled to protect ->sleeping

 * assignment.

	/*

	 * Rescuers, which may not have all the fields set up like normal

	 * workers, also reach here, let's not access anything before

	 * checking NOT_RUNNING.

 Return if preempted before wq_worker_running() was reached */

	/*

	 * The counterpart of the following dec_and_test, implied mb,

	 * worklist not empty test sequence is in insert_work().

	 * Please read comment there.

	 *

	 * NOT_RUNNING is clear.  This means that we're bound to and

	 * running on the local cpu w/ rq lock held and preemption

	 * disabled, which in turn means that none else could be

	 * manipulating idle_list, so dereferencing idle_list without pool

	 * lock is safe.

/**

 * wq_worker_last_func - retrieve worker's last work function

 * @task: Task to retrieve last work function of.

 *

 * Determine the last function a worker executed. This is called from

 * the scheduler to get a worker's last known identity.

 *

 * CONTEXT:

 * raw_spin_lock_irq(rq->lock)

 *

 * This function is called during schedule() when a kworker is going

 * to sleep. It's used by psi to identify aggregation workers during

 * dequeuing, to allow periodic aggregation to shut-off when that

 * worker is the last task in the system or cgroup to go to sleep.

 *

 * As this function doesn't involve any workqueue-related locking, it

 * only returns stable values when called from inside the scheduler's

 * queuing and dequeuing paths, when @task, which must be a kworker,

 * is guaranteed to not be processing any works.

 *

 * Return:

 * The last work function %current executed as a worker, NULL if it

 * hasn't executed any work yet.

/**

 * worker_set_flags - set worker flags and adjust nr_running accordingly

 * @worker: self

 * @flags: flags to set

 *

 * Set @flags in @worker->flags and adjust nr_running accordingly.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock)

 If transitioning into NOT_RUNNING, adjust nr_running. */

/**

 * worker_clr_flags - clear worker flags and adjust nr_running accordingly

 * @worker: self

 * @flags: flags to clear

 *

 * Clear @flags in @worker->flags and adjust nr_running accordingly.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock)

	/*

	 * If transitioning out of NOT_RUNNING, increment nr_running.  Note

	 * that the nested NOT_RUNNING is not a noop.  NOT_RUNNING is mask

	 * of multiple flags, not a single flag.

/**

 * find_worker_executing_work - find worker which is executing a work

 * @pool: pool of interest

 * @work: work to find worker for

 *

 * Find a worker which is executing @work on @pool by searching

 * @pool->busy_hash which is keyed by the address of @work.  For a worker

 * to match, its current execution should match the address of @work and

 * its work function.  This is to avoid unwanted dependency between

 * unrelated work executions through a work item being recycled while still

 * being executed.

 *

 * This is a bit tricky.  A work item may be freed once its execution

 * starts and nothing prevents the freed area from being recycled for

 * another work item.  If the same work item address ends up being reused

 * before the original execution finishes, workqueue will identify the

 * recycled work item as currently executing and make it wait until the

 * current execution finishes, introducing an unwanted dependency.

 *

 * This function checks the work item address and work function to avoid

 * false positives.  Note that this isn't complete as one may construct a

 * work function which can introduce dependency onto itself through a

 * recycled work item.  Well, if somebody wants to shoot oneself in the

 * foot that badly, there's only so much we can do, and if such deadlock

 * actually occurs, it should be easy to locate the culprit work function.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

 *

 * Return:

 * Pointer to worker which is executing @work if found, %NULL

 * otherwise.

/**

 * move_linked_works - move linked works to a list

 * @work: start of series of works to be scheduled

 * @head: target list to append @work to

 * @nextp: out parameter for nested worklist walking

 *

 * Schedule linked works starting from @work to @head.  Work series to

 * be scheduled starts at @work and includes any consecutive work with

 * WORK_STRUCT_LINKED set in its predecessor.

 *

 * If @nextp is not NULL, it's updated to point to the next work of

 * the last scheduled work.  This allows move_linked_works() to be

 * nested inside outer list_for_each_entry_safe().

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

	/*

	 * Linked worklist will always end before the end of the list,

	 * use NULL for list head.

	/*

	 * If we're already inside safe list traversal and have moved

	 * multiple works to the scheduled queue, the next position

	 * needs to be updated.

/**

 * get_pwq - get an extra reference on the specified pool_workqueue

 * @pwq: pool_workqueue to get

 *

 * Obtain an extra reference on @pwq.  The caller should guarantee that

 * @pwq has positive refcnt and be holding the matching pool->lock.

/**

 * put_pwq - put a pool_workqueue reference

 * @pwq: pool_workqueue to put

 *

 * Drop a reference of @pwq.  If its refcnt reaches zero, schedule its

 * destruction.  The caller should be holding the matching pool->lock.

	/*

	 * @pwq can't be released under pool->lock, bounce to

	 * pwq_unbound_release_workfn().  This never recurses on the same

	 * pool->lock as this path is taken only for unbound workqueues and

	 * the release work item is scheduled on a per-cpu workqueue.  To

	 * avoid lockdep warning, unbound pool->locks are given lockdep

	 * subclass of 1 in get_unbound_pool().

/**

 * put_pwq_unlocked - put_pwq() with surrounding pool lock/unlock

 * @pwq: pool_workqueue to put (can be %NULL)

 *

 * put_pwq() with locking.  This function also allows %NULL @pwq.

		/*

		 * As both pwqs and pools are RCU protected, the

		 * following lock operations are safe.

/**

 * pwq_dec_nr_in_flight - decrement pwq's nr_in_flight

 * @pwq: pwq of interest

 * @work_data: work_data of work which left the queue

 *

 * A work either has completed or is removed from pending queue,

 * decrement nr_in_flight of its pwq and handle workqueue flushing.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

 one down, submit an inactive one */

 is flush in progress and are we at the flushing tip? */

 are there still in-flight works? */

 this pwq is done, clear flush_color */

	/*

	 * If this was the last pwq, wake up the first flusher.  It

	 * will handle the rest.

/**

 * try_to_grab_pending - steal work item from worklist and disable irq

 * @work: work item to steal

 * @is_dwork: @work is a delayed_work

 * @flags: place to store irq state

 *

 * Try to grab PENDING bit of @work.  This function can handle @work in any

 * stable state - idle, on timer or on worklist.

 *

 * Return:

 *

 *  ========	================================================================

 *  1		if @work was pending and we successfully stole PENDING

 *  0		if @work was idle and we claimed PENDING

 *  -EAGAIN	if PENDING couldn't be grabbed at the moment, safe to busy-retry

 *  -ENOENT	if someone else is canceling @work, this state may persist

 *		for arbitrarily long

 *  ========	================================================================

 *

 * Note:

 * On >= 0 return, the caller owns @work's PENDING bit.  To avoid getting

 * interrupted while holding PENDING and @work off queue, irq must be

 * disabled on entry.  This, combined with delayed_work->timer being

 * irqsafe, ensures that we return -EAGAIN for finite short period of time.

 *

 * On successful return, >= 0, irq is disabled and the caller is

 * responsible for releasing it using local_irq_restore(*@flags).

 *

 * This function is safe to call from any context including IRQ handler.

 try to steal the timer if it exists */

		/*

		 * dwork->timer is irqsafe.  If del_timer() fails, it's

		 * guaranteed that the timer is not queued anywhere and not

		 * running on the local CPU.

 try to claim PENDING the normal way */

	/*

	 * The queueing is in progress, or it is already queued. Try to

	 * steal it from ->worklist without clearing WORK_STRUCT_PENDING.

	/*

	 * work->data is guaranteed to point to pwq only while the work

	 * item is queued on pwq->wq, and both updating work->data to point

	 * to pwq on queueing and to pool on dequeueing are done under

	 * pwq->pool->lock.  This in turn guarantees that, if work->data

	 * points to pwq which is associated with a locked pool, the work

	 * item is currently queued on that pool.

		/*

		 * A cancelable inactive work item must be in the

		 * pwq->inactive_works since a queued barrier can't be

		 * canceled (see the comments in insert_wq_barrier()).

		 *

		 * An inactive work item cannot be grabbed directly because

		 * it might have linked barrier work items which, if left

		 * on the inactive_works list, will confuse pwq->nr_active

		 * management later on and cause stall.  Make sure the work

		 * item is activated before grabbing.

 work->data points to pwq iff queued, point to pool */

/**

 * insert_work - insert a work into a pool

 * @pwq: pwq @work belongs to

 * @work: work to insert

 * @head: insertion point

 * @extra_flags: extra WORK_STRUCT_* flags to set

 *

 * Insert @work which belongs to @pwq after @head.  @extra_flags is or'd to

 * work_struct flags.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

 record the work call stack in order to print it in KASAN reports */

 we own @work, set data and link */

	/*

	 * Ensure either wq_worker_sleeping() sees the above

	 * list_add_tail() or we see zero nr_running to avoid workers lying

	 * around lazily while there are works to be processed.

/*

 * Test whether @work is being queued from another work executing on the

 * same workqueue.

	/*

	 * Return %true iff I'm a worker executing a work item on @wq.  If

	 * I'm @worker, it's safe to dereference it without locking.

/*

 * When queueing an unbound work item to a wq, prefer local CPU if allowed

 * by wq_unbound_cpumask.  Otherwise, round robin among the allowed ones to

 * avoid perturbing sensitive tasks.

	/*

	 * While a work item is PENDING && off queue, a task trying to

	 * steal the PENDING will busy-loop waiting for it to either get

	 * queued or lose PENDING.  Grabbing PENDING and queueing should

	 * happen with IRQ disabled.

 if draining, only works from the same workqueue are allowed */

 pwq which will be used unless @work is executing elsewhere */

	/*

	 * If @work was previously on a different pool, it might still be

	 * running there, in which case the work needs to be queued on that

	 * pool to guarantee non-reentrancy.

 meh... not running there, queue here */

	/*

	 * pwq is determined and locked.  For unbound pools, we could have

	 * raced with pwq release and it could already be dead.  If its

	 * refcnt is zero, repeat pwq selection.  Note that pwqs never die

	 * without another pwq replacing it in the numa_pwq_tbl or while

	 * work items are executing on it, so the retrying is guaranteed to

	 * make forward-progress.

 oops */

 pwq determined, queue */

/**

 * queue_work_on - queue work on specific cpu

 * @cpu: CPU number to execute work on

 * @wq: workqueue to use

 * @work: work to queue

 *

 * We queue the work to a specific CPU, the caller must ensure it

 * can't go away.

 *

 * Return: %false if @work was already on a queue, %true otherwise.

/**

 * workqueue_select_cpu_near - Select a CPU based on NUMA node

 * @node: NUMA node ID that we want to select a CPU from

 *

 * This function will attempt to find a "random" cpu available on a given

 * node. If there are no CPUs available on the given node it will return

 * WORK_CPU_UNBOUND indicating that we should just schedule to any

 * available CPU if we need to schedule this work.

 No point in doing this if NUMA isn't enabled for workqueues */

 Delay binding to CPU if node is not valid or online */

 Use local node/cpu if we are already there */

 Use "random" otherwise know as "first" online CPU of node */

 If CPU is valid return that, otherwise just defer */

/**

 * queue_work_node - queue work on a "random" cpu for a given NUMA node

 * @node: NUMA node that we are targeting the work for

 * @wq: workqueue to use

 * @work: work to queue

 *

 * We queue the work to a "random" CPU within a given NUMA node. The basic

 * idea here is to provide a way to somehow associate work with a given

 * NUMA node.

 *

 * This function will only make a best effort attempt at getting this onto

 * the right NUMA node. If no node is requested or the requested node is

 * offline then we just fall back to standard queue_work behavior.

 *

 * Currently the "random" CPU ends up being the first available CPU in the

 * intersection of cpu_online_mask and the cpumask of the node, unless we

 * are running on the node. In that case we just use the current CPU.

 *

 * Return: %false if @work was already on a queue, %true otherwise.

	/*

	 * This current implementation is specific to unbound workqueues.

	 * Specifically we only return the first available CPU for a given

	 * node instead of cycling through individual CPUs within the node.

	 *

	 * If this is used with a per-cpu workqueue then the logic in

	 * workqueue_select_cpu_near would need to be updated to allow for

	 * some round robin type logic.

 should have been called from irqsafe timer with irq already off */

	/*

	 * If @delay is 0, queue @dwork->work immediately.  This is for

	 * both optimization and correctness.  The earliest @timer can

	 * expire is on the closest next tick and delayed_work users depend

	 * on that there's no such delay when @delay is 0.

/**

 * queue_delayed_work_on - queue work on specific CPU after delay

 * @cpu: CPU number to execute work on

 * @wq: workqueue to use

 * @dwork: work to queue

 * @delay: number of jiffies to wait before queueing

 *

 * Return: %false if @work was already on a queue, %true otherwise.  If

 * @delay is zero and @dwork is idle, it will be scheduled for immediate

 * execution.

 read the comment in __queue_work() */

/**

 * mod_delayed_work_on - modify delay of or queue a delayed work on specific CPU

 * @cpu: CPU number to execute work on

 * @wq: workqueue to use

 * @dwork: work to queue

 * @delay: number of jiffies to wait before queueing

 *

 * If @dwork is idle, equivalent to queue_delayed_work_on(); otherwise,

 * modify @dwork's timer so that it expires after @delay.  If @delay is

 * zero, @work is guaranteed to be scheduled immediately regardless of its

 * current state.

 *

 * Return: %false if @dwork was idle and queued, %true if @dwork was

 * pending and its timer was modified.

 *

 * This function is safe to call from any context including IRQ handler.

 * See try_to_grab_pending() for details.

 -ENOENT from try_to_grab_pending() becomes %true */

 read the comment in __queue_work() */

/**

 * queue_rcu_work - queue work after a RCU grace period

 * @wq: workqueue to use

 * @rwork: work to queue

 *

 * Return: %false if @rwork was already pending, %true otherwise.  Note

 * that a full RCU grace period is guaranteed only after a %true return.

 * While @rwork is guaranteed to be executed after a %false return, the

 * execution may happen before a full RCU grace period has passed.

/**

 * worker_enter_idle - enter idle state

 * @worker: worker which is entering idle state

 *

 * @worker is entering idle state.  Update stats and idle timer if

 * necessary.

 *

 * LOCKING:

 * raw_spin_lock_irq(pool->lock).

 can't use worker_set_flags(), also called from create_worker() */

 idle_list is LIFO */

	/*

	 * Sanity check nr_running.  Because unbind_workers() releases

	 * pool->lock between setting %WORKER_UNBOUND and zapping

	 * nr_running, the warning may trigger spuriously.  Check iff

	 * unbind is not in progress.

/**

 * worker_leave_idle - leave idle state

 * @worker: worker which is leaving idle state

 *

 * @worker is leaving idle state.  Update stats.

 *

 * LOCKING:

 * raw_spin_lock_irq(pool->lock).

 on creation a worker is in !idle && prep state */

/**

 * worker_attach_to_pool() - attach a worker to a pool

 * @worker: worker to be attached

 * @pool: the target pool

 *

 * Attach @worker to @pool.  Once attached, the %WORKER_UNBOUND flag and

 * cpu-binding of @worker are kept coordinated with the pool across

 * cpu-[un]hotplugs.

	/*

	 * The wq_pool_attach_mutex ensures %POOL_DISASSOCIATED remains

	 * stable across this function.  See the comments above the flag

	 * definition for details.

/**

 * worker_detach_from_pool() - detach a worker from its pool

 * @worker: worker which is attached to its pool

 *

 * Undo the attaching which had been done in worker_attach_to_pool().  The

 * caller worker shouldn't access to the pool after detached except it has

 * other reference to the pool.

 clear leftover flags without pool->lock after it is detached */

/**

 * create_worker - create a new workqueue worker

 * @pool: pool the new worker will belong to

 *

 * Create and start a new worker which is attached to @pool.

 *

 * CONTEXT:

 * Might sleep.  Does GFP_KERNEL allocations.

 *

 * Return:

 * Pointer to the newly created worker.

 ID is needed to determine kthread name */

 successful, attach the worker to the pool */

 start the newly created worker */

/**

 * destroy_worker - destroy a workqueue worker

 * @worker: worker to be destroyed

 *

 * Destroy @worker and adjust @pool stats accordingly.  The worker should

 * be idle.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

 sanity check frenzy */

 idle_list is kept in LIFO order, check the last one */

 mayday mayday mayday */

		/*

		 * If @pwq is for an unbound wq, its base ref may be put at

		 * any time due to an attribute change.  Pin @pwq until the

		 * rescuer is done with it.

 for wq->maydays */

		/*

		 * We've been trying to create a new worker but

		 * haven't been successful.  We might be hitting an

		 * allocation deadlock.  Send distress signals to

		 * rescuers.

/**

 * maybe_create_worker - create a new worker if necessary

 * @pool: pool to create a new worker for

 *

 * Create a new worker for @pool if necessary.  @pool is guaranteed to

 * have at least one idle worker on return from this function.  If

 * creating a new worker takes longer than MAYDAY_INTERVAL, mayday is

 * sent to all rescuers with works scheduled on @pool to resolve

 * possible allocation deadlock.

 *

 * On return, need_to_create_worker() is guaranteed to be %false and

 * may_start_working() %true.

 *

 * LOCKING:

 * raw_spin_lock_irq(pool->lock) which may be released and regrabbed

 * multiple times.  Does GFP_KERNEL allocations.  Called only from

 * manager.

 if we don't make progress in MAYDAY_INITIAL_TIMEOUT, call for help */

	/*

	 * This is necessary even after a new worker was just successfully

	 * created as @pool->lock was dropped and the new worker might have

	 * already become busy.

/**

 * manage_workers - manage worker pool

 * @worker: self

 *

 * Assume the manager role and manage the worker pool @worker belongs

 * to.  At any given time, there can be only zero or one manager per

 * pool.  The exclusion is handled automatically by this function.

 *

 * The caller can safely start processing works on false return.  On

 * true return, it's guaranteed that need_to_create_worker() is false

 * and may_start_working() is true.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock) which may be released and regrabbed

 * multiple times.  Does GFP_KERNEL allocations.

 *

 * Return:

 * %false if the pool doesn't need management and the caller can safely

 * start processing works, %true if management function was performed and

 * the conditions that the caller verified before calling the function may

 * no longer be true.

/**

 * process_one_work - process single work

 * @worker: self

 * @work: work to process

 *

 * Process @work.  This function contains all the logics necessary to

 * process a single work including synchronization against and

 * interaction with other workers on the same cpu, queueing and

 * flushing.  As long as context requirement is met, any worker can

 * call this function to process a work.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock) which is released and regrabbed.

	/*

	 * It is permissible to free the struct work_struct from

	 * inside the function that is called from it, this we need to

	 * take into account for lockdep too.  To avoid bogus "held

	 * lock freed" warnings as well as problems when looking into

	 * work->lockdep_map, make a copy and use that here.

 ensure we're on the correct CPU */

	/*

	 * A single work shouldn't be executed concurrently by

	 * multiple workers on a single cpu.  Check whether anyone is

	 * already processing the work.  If so, defer the work to the

	 * currently executing one.

 claim and dequeue */

	/*

	 * Record wq name for cmdline and debug reporting, may get

	 * overridden through set_worker_desc().

	/*

	 * CPU intensive works don't participate in concurrency management.

	 * They're the scheduler's responsibility.  This takes @worker out

	 * of concurrency management and the next code block will chain

	 * execution of the pending work items.

	/*

	 * Wake up another worker if necessary.  The condition is always

	 * false for normal per-cpu workers since nr_running would always

	 * be >= 1 at this point.  This is used to chain execution of the

	 * pending work items for WORKER_NOT_RUNNING workers such as the

	 * UNBOUND and CPU_INTENSIVE ones.

	/*

	 * Record the last pool and clear PENDING which should be the last

	 * update to @work.  Also, do this inside @pool->lock so that

	 * PENDING and queued state changes happen together while IRQ is

	 * disabled.

	/*

	 * Strictly speaking we should mark the invariant state without holding

	 * any locks, that is, before these two lock_map_acquire()'s.

	 *

	 * However, that would result in:

	 *

	 *   A(W1)

	 *   WFC(C)

	 *		A(W1)

	 *		C(C)

	 *

	 * Which would create W1->C->W1 dependencies, even though there is no

	 * actual deadlock possible. There are two solutions, using a

	 * read-recursive acquire on the work(queue) 'locks', but this will then

	 * hit the lockdep limitation on recursive locks, or simply discard

	 * these locks.

	 *

	 * AFAICT there is no possible deadlock scenario between the

	 * flush_work() and complete() primitives (except for single-threaded

	 * workqueues), so hiding them isn't a problem.

	/*

	 * While we must be careful to not use "work" after this, the trace

	 * point will only record its address.

	/*

	 * The following prevents a kworker from hogging CPU on !PREEMPTION

	 * kernels, where a requeueing work item waiting for something to

	 * happen could deadlock with stop_machine as such work item could

	 * indefinitely requeue itself while all other CPUs are trapped in

	 * stop_machine. At the same time, report a quiescent RCU state so

	 * the same condition doesn't freeze RCU.

 clear cpu intensive status */

 tag the worker for identification in schedule() */

 we're done with it, release */

/**

 * process_scheduled_works - process scheduled works

 * @worker: self

 *

 * Process all scheduled works.  Please note that the scheduled list

 * may change while processing a work, so this function repeatedly

 * fetches a work from the top and executes it.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock) which may be released and regrabbed

 * multiple times.

/**

 * worker_thread - the worker thread function

 * @__worker: self

 *

 * The worker thread function.  All workers belong to a worker_pool -

 * either a per-cpu one or dynamic unbound one.  These workers process all

 * work items regardless of their specific target workqueue.  The only

 * exception is work items which belong to workqueues with a rescuer which

 * will be explained in rescuer_thread().

 *

 * Return: 0

 tell the scheduler that this is a workqueue worker */

 am I supposed to die? */

 no more worker necessary? */

 do we need to manage? */

	/*

	 * ->scheduled list can only be filled while a worker is

	 * preparing to process a work or actually processing it.

	 * Make sure nobody diddled with it while I was sleeping.

	/*

	 * Finish PREP stage.  We're guaranteed to have at least one idle

	 * worker or that someone else has already assumed the manager

	 * role.  This is where @worker starts participating in concurrency

	 * management if applicable and concurrency management is restored

	 * after being rebound.  See rebind_workers() for details.

 optimization path, not strictly necessary */

	/*

	 * pool->lock is held and there's no work to process and no need to

	 * manage, sleep.  Workers are woken up only while holding

	 * pool->lock or from local cpu, so setting the current state

	 * before releasing pool->lock is enough to prevent losing any

	 * event.

/**

 * rescuer_thread - the rescuer thread function

 * @__rescuer: self

 *

 * Workqueue rescuer thread function.  There's one rescuer for each

 * workqueue which has WQ_MEM_RECLAIM set.

 *

 * Regular work processing on a pool may block trying to create a new

 * worker which uses GFP_KERNEL allocation which has slight chance of

 * developing into deadlock if some works currently on the same queue

 * need to be processed to satisfy the GFP_KERNEL allocation.  This is

 * the problem rescuer solves.

 *

 * When such condition is possible, the pool summons rescuers of all

 * workqueues which have works queued on the pool and let them process

 * those works so that forward progress can be guaranteed.

 *

 * This should happen rarely.

 *

 * Return: 0

	/*

	 * Mark rescuer as worker too.  As WORKER_PREP is never cleared, it

	 * doesn't participate in concurrency management.

	/*

	 * By the time the rescuer is requested to stop, the workqueue

	 * shouldn't have any work pending, but @wq->maydays may still have

	 * pwq(s) queued.  This can happen by non-rescuer workers consuming

	 * all the work items before the rescuer got to them.  Go through

	 * @wq->maydays processing before acting on should_stop so that the

	 * list is always empty on exit.

 see whether any pwq is asking for help */

		/*

		 * Slurp in all works issued via this workqueue and

		 * process'em.

			/*

			 * The above execution of rescued work items could

			 * have created more to rescue through

			 * pwq_activate_first_inactive() or chained

			 * queueing.  Let's put @pwq back on mayday list so

			 * that such back-to-back work items, which may be

			 * being used to relieve memory pressure, don't

			 * incur MAYDAY_INTERVAL delay inbetween.

				/*

				 * Queue iff we aren't racing destruction

				 * and somebody else hasn't queued it already.

		/*

		 * Put the reference grabbed by send_mayday().  @pool won't

		 * go away while we're still attached to it.

		/*

		 * Leave this pool.  If need_more_worker() is %true, notify a

		 * regular worker; otherwise, we end up with 0 concurrency

		 * and stalling the execution.

 rescuers should never participate in concurrency management */

/**

 * check_flush_dependency - check for flush dependency sanity

 * @target_wq: workqueue being flushed

 * @target_work: work item being flushed (NULL for workqueue flushes)

 *

 * %current is trying to flush the whole @target_wq or @target_work on it.

 * If @target_wq doesn't have %WQ_MEM_RECLAIM, verify that %current is not

 * reclaiming memory or running on a workqueue which doesn't have

 * %WQ_MEM_RECLAIM as that can break forward-progress guarantee leading to

 * a deadlock.

 purely informational */

/**

 * insert_wq_barrier - insert a barrier work

 * @pwq: pwq to insert barrier into

 * @barr: wq_barrier to insert

 * @target: target work to attach @barr to

 * @worker: worker currently executing @target, NULL if @target is not executing

 *

 * @barr is linked to @target such that @barr is completed only after

 * @target finishes execution.  Please note that the ordering

 * guarantee is observed only with respect to @target and on the local

 * cpu.

 *

 * Currently, a queued barrier can't be canceled.  This is because

 * try_to_grab_pending() can't determine whether the work to be

 * grabbed is at the head of the queue and thus can't clear LINKED

 * flag of the previous work while there must be a valid next work

 * after a work with LINKED flag set.

 *

 * Note that when @worker is non-NULL, @target may be modified

 * underneath us, so we can't reliably determine pwq from @target.

 *

 * CONTEXT:

 * raw_spin_lock_irq(pool->lock).

	/*

	 * debugobject calls are safe here even with pool->lock locked

	 * as we know for sure that this will not trigger any of the

	 * checks and call back into the fixup functions where we

	 * might deadlock.

 The barrier work item does not participate in pwq->nr_active. */

	/*

	 * If @target is currently being executed, schedule the

	 * barrier to the worker; otherwise, put it after @target.

 there can already be other linked works, inherit and set */

/**

 * flush_workqueue_prep_pwqs - prepare pwqs for workqueue flushing

 * @wq: workqueue being flushed

 * @flush_color: new flush color, < 0 for no-op

 * @work_color: new work color, < 0 for no-op

 *

 * Prepare pwqs for workqueue flushing.

 *

 * If @flush_color is non-negative, flush_color on all pwqs should be

 * -1.  If no pwq has in-flight commands at the specified color, all

 * pwq->flush_color's stay at -1 and %false is returned.  If any pwq

 * has in flight commands, its pwq->flush_color is set to

 * @flush_color, @wq->nr_pwqs_to_flush is updated accordingly, pwq

 * wakeup logic is armed and %true is returned.

 *

 * The caller should have initialized @wq->first_flusher prior to

 * calling this function with non-negative @flush_color.  If

 * @flush_color is negative, no flush color update is done and %false

 * is returned.

 *

 * If @work_color is non-negative, all pwqs should have the same

 * work_color which is previous to @work_color and all will be

 * advanced to @work_color.

 *

 * CONTEXT:

 * mutex_lock(wq->mutex).

 *

 * Return:

 * %true if @flush_color >= 0 and there's something to flush.  %false

 * otherwise.

/**

 * flush_workqueue - ensure that any scheduled work has run to completion.

 * @wq: workqueue to flush

 *

 * This function sleeps until all work items which were queued on entry

 * have finished execution, but it is not livelocked by new incoming ones.

	/*

	 * Start-to-wait phase

		/*

		 * Color space is not full.  The current work_color

		 * becomes our flush_color and work_color is advanced

		 * by one.

 no flush in progress, become the first flusher */

 nothing to flush, done */

 wait in queue */

		/*

		 * Oops, color space is full, wait on overflow queue.

		 * The next flush completion will assign us

		 * flush_color and transfer to flusher_queue.

	/*

	 * Wake-up-and-cascade phase

	 *

	 * First flushers are responsible for cascading flushes and

	 * handling overflow.  Non-first flushers can simply return.

 we might have raced, check again with mutex held */

 complete all the flushers sharing the current flush color */

 this flush_color is finished, advance by one */

 one color has been freed, handle overflow queue */

			/*

			 * Assign the same color to all overflowed

			 * flushers, advance work_color and append to

			 * flusher_queue.  This is the start-to-wait

			 * phase for these overflowed flushers.

		/*

		 * Need to flush more colors.  Make the next flusher

		 * the new first flusher and arm pwqs.

		/*

		 * Meh... this color is already done, clear first

		 * flusher and repeat cascading.

/**

 * drain_workqueue - drain a workqueue

 * @wq: workqueue to drain

 *

 * Wait until the workqueue becomes empty.  While draining is in progress,

 * only chain queueing is allowed.  IOW, only currently pending or running

 * work items on @wq can queue further work items on it.  @wq is flushed

 * repeatedly until it becomes empty.  The number of flushing is determined

 * by the depth of chaining and should be relatively short.  Whine if it

 * takes too long.

	/*

	 * __queue_work() needs to test whether there are drainers, is much

	 * hotter than drain_workqueue() and already looks at @wq->flags.

	 * Use __WQ_DRAINING so that queue doesn't have to check nr_drainers.

 see the comment in try_to_grab_pending() with the same code */

	/*

	 * Force a lock recursion deadlock when using flush_work() inside a

	 * single-threaded or rescuer equipped workqueue.

	 *

	 * For single threaded workqueues the deadlock happens when the work

	 * is after the work issuing the flush_work(). For rescuer equipped

	 * workqueues the deadlock happens when the rescuer stalls, blocking

	 * forward progress.

/**

 * flush_work - wait for a work to finish executing the last queueing instance

 * @work: the work to flush

 *

 * Wait until @work has finished execution.  @work is guaranteed to be idle

 * on return if it hasn't been requeued since flush started.

 *

 * Return:

 * %true if flush_work() waited for the work to finish execution,

 * %false if it was already idle.

		/*

		 * If someone else is already canceling, wait for it to

		 * finish.  flush_work() doesn't work for PREEMPT_NONE

		 * because we may get scheduled between @work's completion

		 * and the other canceling task resuming and clearing

		 * CANCELING - flush_work() will return false immediately

		 * as @work is no longer busy, try_to_grab_pending() will

		 * return -ENOENT as @work is still being canceled and the

		 * other canceling task won't be able to clear CANCELING as

		 * we're hogging the CPU.

		 *

		 * Let's wait for completion using a waitqueue.  As this

		 * may lead to the thundering herd problem, use a custom

		 * wake function which matches @work along with exclusive

		 * wait and wakeup.

 tell other tasks trying to grab @work to back off */

	/*

	 * This allows canceling during early boot.  We know that @work

	 * isn't executing.

	/*

	 * Paired with prepare_to_wait() above so that either

	 * waitqueue_active() is visible here or !work_is_canceling() is

	 * visible there.

/**

 * cancel_work_sync - cancel a work and wait for it to finish

 * @work: the work to cancel

 *

 * Cancel @work and wait for its execution to finish.  This function

 * can be used even if the work re-queues itself or migrates to

 * another workqueue.  On return from this function, @work is

 * guaranteed to be not pending or executing on any CPU.

 *

 * cancel_work_sync(&delayed_work->work) must not be used for

 * delayed_work's.  Use cancel_delayed_work_sync() instead.

 *

 * The caller must ensure that the workqueue on which @work was last

 * queued can't be destroyed before this function returns.

 *

 * Return:

 * %true if @work was pending, %false otherwise.

/**

 * flush_delayed_work - wait for a dwork to finish executing the last queueing

 * @dwork: the delayed work to flush

 *

 * Delayed timer is cancelled and the pending work is queued for

 * immediate execution.  Like flush_work(), this function only

 * considers the last queueing instance of @dwork.

 *

 * Return:

 * %true if flush_work() waited for the work to finish execution,

 * %false if it was already idle.

/**

 * flush_rcu_work - wait for a rwork to finish executing the last queueing

 * @rwork: the rcu work to flush

 *

 * Return:

 * %true if flush_rcu_work() waited for the work to finish execution,

 * %false if it was already idle.

/**

 * cancel_delayed_work - cancel a delayed work

 * @dwork: delayed_work to cancel

 *

 * Kill off a pending delayed_work.

 *

 * Return: %true if @dwork was pending and canceled; %false if it wasn't

 * pending.

 *

 * Note:

 * The work callback function may still be running on return, unless

 * it returns %true and the work doesn't re-arm itself.  Explicitly flush or

 * use cancel_delayed_work_sync() to wait on it.

 *

 * This function is safe to call from any context including IRQ handler.

/**

 * cancel_delayed_work_sync - cancel a delayed work and wait for it to finish

 * @dwork: the delayed work cancel

 *

 * This is cancel_work_sync() for delayed works.

 *

 * Return:

 * %true if @dwork was pending, %false otherwise.

/**

 * schedule_on_each_cpu - execute a function synchronously on each online CPU

 * @func: the function to call

 *

 * schedule_on_each_cpu() executes @func on each online CPU using the

 * system workqueue and blocks until all CPUs have completed.

 * schedule_on_each_cpu() is very slow.

 *

 * Return:

 * 0 on success, -errno on failure.

/**

 * execute_in_process_context - reliably execute the routine with user context

 * @fn:		the function to execute

 * @ew:		guaranteed storage for the execute work structure (must

 *		be available when the work executes)

 *

 * Executes the function immediately if process context is available,

 * otherwise schedules the function for delayed execution.

 *

 * Return:	0 - function was executed

 *		1 - function was scheduled for execution

/**

 * free_workqueue_attrs - free a workqueue_attrs

 * @attrs: workqueue_attrs to free

 *

 * Undo alloc_workqueue_attrs().

/**

 * alloc_workqueue_attrs - allocate a workqueue_attrs

 *

 * Allocate a new workqueue_attrs, initialize with default settings and

 * return it.

 *

 * Return: The allocated new workqueue_attr on success. %NULL on failure.

	/*

	 * Unlike hash and equality test, this function doesn't ignore

	 * ->no_numa as it is used for both pool and wq attrs.  Instead,

	 * get_unbound_pool() explicitly clears ->no_numa after copying.

 hash value of the content of @attr */

 content equality test */

/**

 * init_worker_pool - initialize a newly zalloc'd worker_pool

 * @pool: worker_pool to initialize

 *

 * Initialize a newly zalloc'd @pool.  It also allocates @pool->attrs.

 *

 * Return: 0 on success, -errno on failure.  Even on failure, all fields

 * inside @pool proper are initialized and put_unbound_pool() can be called

 * on @pool safely to release it.

 shouldn't fail above this point */

 This returns with the lock held on success (pool manager is inactive). */

/**

 * put_unbound_pool - put a worker_pool

 * @pool: worker_pool to put

 *

 * Put @pool.  If its refcnt reaches zero, it gets destroyed in RCU

 * safe manner.  get_unbound_pool() calls this function on its failure path

 * and this function should be able to release pools which went through,

 * successfully or not, init_worker_pool().

 *

 * Should be called with wq_pool_mutex held.

 sanity checks */

 release id and unhash */

	/*

	 * Become the manager and destroy all workers.  This prevents

	 * @pool's workers from blocking on attach_mutex.  We're the last

	 * manager and @pool gets freed with the flag set.

	 * Because of how wq_manager_inactive() works, we will hold the

	 * spinlock after a successful wait.

 shut down the timers */

 RCU protected to allow dereferences from get_work_pool() */

/**

 * get_unbound_pool - get a worker_pool with the specified attributes

 * @attrs: the attributes of the worker_pool to get

 *

 * Obtain a worker_pool which has the same attributes as @attrs, bump the

 * reference count and return it.  If there already is a matching

 * worker_pool, it will be used; otherwise, this function attempts to

 * create a new one.

 *

 * Should be called with wq_pool_mutex held.

 *

 * Return: On success, a worker_pool with the same attributes as @attrs.

 * On failure, %NULL.

 do we already have a matching pool? */

 if cpumask is contained inside a NUMA node, we belong to that node */

 nope, create a new one */

 see put_pwq() */

	/*

	 * no_numa isn't a worker_pool attribute, always clear it.  See

	 * 'struct workqueue_attrs' comments for detail.

 create and start the initial worker */

 install */

/*

 * Scheduled on system_wq by put_pwq() when an unbound pwq hits zero refcnt

 * and needs to be destroyed.

	/*

	 * when @pwq is not linked, it doesn't hold any reference to the

	 * @wq, and @wq is invalid to access.

	/*

	 * If we're the last pwq going away, @wq is already dead and no one

	 * is gonna access it anymore.  Schedule RCU free.

/**

 * pwq_adjust_max_active - update a pwq's max_active to the current setting

 * @pwq: target pool_workqueue

 *

 * If @pwq isn't freezing, set @pwq->max_active to the associated

 * workqueue's saved_max_active and activate inactive work items

 * accordingly.  If @pwq is freezing, clear @pwq->max_active to zero.

 for @wq->saved_max_active */

 fast exit for non-freezable wqs */

 this function can be called during early boot w/ irq disabled */

	/*

	 * During [un]freezing, the caller is responsible for ensuring that

	 * this function is called at least once after @workqueue_freezing

	 * is updated and visible.

		/*

		 * Need to kick a worker after thawed or an unbound wq's

		 * max_active is bumped. In realtime scenarios, always kicking a

		 * worker will cause interference on the isolated cpu cores, so

		 * let's kick iff work items were activated.

 initialize newly allocated @pwq which is associated with @wq and @pool */

 sync @pwq with the current state of its associated wq and link it */

 may be called multiple times, ignore if already linked */

 set the matching work_color */

 sync max_active to the current setting */

 link in @pwq */

 obtain a pool matching @attr and create a pwq associating the pool and @wq */

/**

 * wq_calc_node_cpumask - calculate a wq_attrs' cpumask for the specified node

 * @attrs: the wq_attrs of the default pwq of the target workqueue

 * @node: the target NUMA node

 * @cpu_going_down: if >= 0, the CPU to consider as offline

 * @cpumask: outarg, the resulting cpumask

 *

 * Calculate the cpumask a workqueue with @attrs should use on @node.  If

 * @cpu_going_down is >= 0, that cpu is considered offline during

 * calculation.  The result is stored in @cpumask.

 *

 * If NUMA affinity is not enabled, @attrs->cpumask is always used.  If

 * enabled and @node has online CPUs requested by @attrs, the returned

 * cpumask is the intersection of the possible CPUs of @node and

 * @attrs->cpumask.

 *

 * The caller is responsible for ensuring that the cpumask of @node stays

 * stable.

 *

 * Return: %true if the resulting @cpumask is different from @attrs->cpumask,

 * %false if equal.

 does @node have any online CPUs @attrs wants? */

 yeap, return possible CPUs in @node that @attrs wants */

 install @pwq into @wq's numa_pwq_tbl[] for @node and return the old pwq */

 link_pwq() can handle duplicate calls */

 context to store the prepared attrs & pwqs before applying */

 target workqueue */

 attrs to apply */

 queued for batching commit */

 free the resources after success or abort */

 allocate the attrs and pwqs for later installation */

	/*

	 * Calculate the attrs of the default pwq.

	 * If the user configured cpumask doesn't overlap with the

	 * wq_unbound_cpumask, we fallback to the wq_unbound_cpumask.

	/*

	 * We may create multiple pwqs with differing cpumasks.  Make a

	 * copy of @new_attrs which will be modified and used to obtain

	 * pools.

	/*

	 * If something goes wrong during CPU up/down, we'll fall back to

	 * the default pwq covering whole @attrs->cpumask.  Always create

	 * it even if we don't use it immediately.

 save the user configured attrs and sanitize it. */

 set attrs and install prepared pwqs, @ctx points to old pwqs on return */

 all pwqs have been created successfully, let's install'em */

 save the previous pwq and install the new one */

 @dfl_pwq might not have been used, ensure it's linked */

 CPUs should stay stable across pwq creations and installations */

 only unbound workqueues can change attributes */

 creating multiple pwqs breaks ordering guarantee */

 the ctx has been prepared successfully, let's commit it */

/**

 * apply_workqueue_attrs - apply new workqueue_attrs to an unbound workqueue

 * @wq: the target workqueue

 * @attrs: the workqueue_attrs to apply, allocated with alloc_workqueue_attrs()

 *

 * Apply @attrs to an unbound workqueue @wq.  Unless disabled, on NUMA

 * machines, this function maps a separate pwq to each NUMA node with

 * possibles CPUs in @attrs->cpumask so that work items are affine to the

 * NUMA node it was issued on.  Older pwqs are released as in-flight work

 * items finish.  Note that a work item which repeatedly requeues itself

 * back-to-back will stay on its current pwq.

 *

 * Performs GFP_KERNEL allocations.

 *

 * Assumes caller has CPU hotplug read exclusion, i.e. cpus_read_lock().

 *

 * Return: 0 on success and -errno on failure.

/**

 * wq_update_unbound_numa - update NUMA affinity of a wq for CPU hot[un]plug

 * @wq: the target workqueue

 * @cpu: the CPU coming up or going down

 * @online: whether @cpu is coming up or going down

 *

 * This function is to be called from %CPU_DOWN_PREPARE, %CPU_ONLINE and

 * %CPU_DOWN_FAILED.  @cpu is being hot[un]plugged, update NUMA affinity of

 * @wq accordingly.

 *

 * If NUMA affinity can't be adjusted due to memory allocation failure, it

 * falls back to @wq->dfl_pwq which may not be optimal but is always

 * correct.

 *

 * Note that when the last allowed CPU of a NUMA node goes offline for a

 * workqueue with a cpumask spanning multiple nodes, the workers which were

 * already executing the work items for the workqueue will lose their CPU

 * affinity and may execute on any CPU.  This is similar to how per-cpu

 * workqueues behave on CPU_DOWN.  If a workqueue user wants strict

 * affinity, it's the user's responsibility to flush the work item from

 * CPU_DOWN_PREPARE.

	/*

	 * We don't wanna alloc/free wq_attrs for each wq for each CPU.

	 * Let's use a preallocated one.  The following buf is protected by

	 * CPU hotplug exclusion.

	/*

	 * Let's determine what needs to be done.  If the target cpumask is

	 * different from the default pwq's, we need to compare it to @pwq's

	 * and create a new one if they don't match.  If the target cpumask

	 * equals the default pwq's, the default pwq should be used.

 create a new pwq */

 Install the new pwq. */

 there should only be single pwq for ordering guarantee */

/*

 * Workqueues which may be used during memory reclaim should have a rescuer

 * to guarantee forward progress.

	/*

	 * Unbound && max_active == 1 used to imply ordered, which is no

	 * longer the case on NUMA machines due to per-node pools.  While

	 * alloc_ordered_workqueue() is the right way to create an ordered

	 * workqueue, keep the previous behavior to avoid subtle breakages

	 * on NUMA.

 see the comment above the definition of WQ_POWER_EFFICIENT */

 allocate wq and format name */

 init wq */

	/*

	 * wq_pool_mutex protects global freeze state and workqueues list.

	 * Grab it, adjust max_active and add the new @wq to workqueues

	 * list.

/**

 * destroy_workqueue - safely terminate a workqueue

 * @wq: target workqueue

 *

 * Safely destroy a workqueue. All work currently pending will be done first.

	/*

	 * Remove it from sysfs first so that sanity check failure doesn't

	 * lead to sysfs name conflicts.

 drain it before proceeding with destruction */

 kill rescuer, if sanity checks fail, leave it w/o rescuer */

 this prevents new queueing */

 rescuer will empty maydays list before exiting */

	/*

	 * Sanity checks - grab all the locks so that we wait for all

	 * in-flight operations which may do put_pwq().

	/*

	 * wq list is used to freeze wq, remove from list after

	 * flushing is complete in case freeze races us.

		/*

		 * The base ref is never dropped on per-cpu pwqs.  Directly

		 * schedule RCU free.

		/*

		 * We're the sole accessor of @wq at this point.  Directly

		 * access numa_pwq_tbl[] and dfl_pwq to put the base refs.

		 * @wq will be freed when the last pwq is released.

		/*

		 * Put dfl_pwq.  @wq may be freed any time after dfl_pwq is

		 * put.  Don't access it afterwards.

/**

 * workqueue_set_max_active - adjust max_active of a workqueue

 * @wq: target workqueue

 * @max_active: new max_active value.

 *

 * Set max_active of @wq to @max_active.

 *

 * CONTEXT:

 * Don't call from IRQ context.

 disallow meddling with max_active for ordered workqueues */

/**

 * current_work - retrieve %current task's work struct

 *

 * Determine if %current task is a workqueue worker and what it's working on.

 * Useful to find out the context that the %current task is running in.

 *

 * Return: work struct if %current task is a workqueue worker, %NULL otherwise.

/**

 * current_is_workqueue_rescuer - is %current workqueue rescuer?

 *

 * Determine whether %current is a workqueue rescuer.  Can be used from

 * work functions to determine whether it's being run off the rescuer task.

 *

 * Return: %true if %current is a workqueue rescuer. %false otherwise.

/**

 * workqueue_congested - test whether a workqueue is congested

 * @cpu: CPU in question

 * @wq: target workqueue

 *

 * Test whether @wq's cpu workqueue for @cpu is congested.  There is

 * no synchronization around this function and the test result is

 * unreliable and only useful as advisory hints or for debugging.

 *

 * If @cpu is WORK_CPU_UNBOUND, the test is performed on the local CPU.

 * Note that both per-cpu and unbound workqueues may be associated with

 * multiple pool_workqueues which have separate congested states.  A

 * workqueue being congested on one CPU doesn't mean the workqueue is also

 * contested on other CPUs / NUMA nodes.

 *

 * Return:

 * %true if congested, %false otherwise.

/**

 * work_busy - test whether a work is currently pending or running

 * @work: the work to be tested

 *

 * Test whether @work is currently pending or running.  There is no

 * synchronization around this function and the test result is

 * unreliable and only useful as advisory hints or for debugging.

 *

 * Return:

 * OR'd bitmask of WORK_BUSY_* bits.

/**

 * set_worker_desc - set description for the current work item

 * @fmt: printf-style format string

 * @...: arguments for the format string

 *

 * This function can be called by a running work function to describe what

 * the work item is about.  If the worker task gets dumped, this

 * information will be printed out together to help debugging.  The

 * description can be at most WORKER_DESC_LEN including the trailing '\0'.

/**

 * print_worker_info - print out worker information and description

 * @log_lvl: the log level to use when printing

 * @task: target task

 *

 * If @task is a worker and currently executing a work item, print out the

 * name of the workqueue being serviced and worker description set with

 * set_worker_desc() by the currently executing work item.

 *

 * This function can be safely called on any task as long as the

 * task_struct itself is accessible.  While safe, this function isn't

 * synchronized and may print out mixups or garbages of limited length.

	/*

	 * This function is called without any synchronization and @task

	 * could be in any state.  Be careful with dereferences.

	/*

	 * Carefully copy the associated workqueue's workfn, name and desc.

	 * Keep the original last '\0' in case the original is garbage.

/**

 * show_one_workqueue - dump state of specified workqueue

 * @wq: workqueue whose state will be printed

 Nothing to print for idle workqueue */

			/*

			 * Defer printing to avoid deadlocks in console

			 * drivers that queue work while holding locks

			 * also taken in their write paths.

		/*

		 * We could be printing a lot from atomic context, e.g.

		 * sysrq-t -> show_all_workqueues(). Avoid triggering

		 * hard lockup.

/**

 * show_one_worker_pool - dump state of specified worker pool

 * @pool: worker pool whose state will be printed

	/*

	 * Defer printing to avoid deadlocks in console drivers that

	 * queue work while holding locks also taken in their write

	 * paths.

	/*

	 * We could be printing a lot from atomic context, e.g.

	 * sysrq-t -> show_all_workqueues(). Avoid triggering

	 * hard lockup.

/**

 * show_all_workqueues - dump workqueue state

 *

 * Called from a sysrq handler or try_to_freeze_tasks() and prints out

 * all busy workqueues and pools.

 used to show worker information through /proc/PID/{comm,stat,status} */

 always show the actual comm */

 stabilize PF_WQ_WORKER and worker pool association */

			/*

			 * ->desc tracks information (wq name or

			 * set_worker_desc()) for the latest execution.  If

			 * current, prepend '+', otherwise '-'.

/*

 * CPU hotplug.

 *

 * There are two challenges in supporting CPU hotplug.  Firstly, there

 * are a lot of assumptions on strong associations among work, pwq and

 * pool which make migrating pending and scheduled works very

 * difficult to implement without impacting hot paths.  Secondly,

 * worker pools serve mix of short, long and very long running works making

 * blocked draining impractical.

 *

 * This is solved by allowing the pools to be disassociated from the CPU

 * running as an unbound one and allowing it to be reattached later if the

 * cpu comes back online.

		/*

		 * We've blocked all attach/detach operations. Make all workers

		 * unbound and set DISASSOCIATED.  Before this, all workers

		 * except for the ones which are still executing works from

		 * before the last CPU down must be on the cpu.  After

		 * this, they may become diasporas.

		/*

		 * Call schedule() so that we cross rq->lock and thus can

		 * guarantee sched callbacks see the %WORKER_UNBOUND flag.

		 * This is necessary as scheduler callbacks may be invoked

		 * from other cpus.

		/*

		 * Sched callbacks are disabled now.  Zap nr_running.

		 * After this, nr_running stays zero and need_more_worker()

		 * and keep_working() are always true as long as the

		 * worklist is not empty.  This pool now behaves as an

		 * unbound (in terms of concurrency management) pool which

		 * are served by workers tied to the pool.

		/*

		 * With concurrency management just turned off, a busy

		 * worker blocking could lead to lengthy stalls.  Kick off

		 * unbound chain execution of currently pending work items.

/**

 * rebind_workers - rebind all workers of a pool to the associated CPU

 * @pool: pool of interest

 *

 * @pool->cpu is coming online.  Rebind all workers to the CPU.

	/*

	 * Restore CPU affinity of all workers.  As all idle workers should

	 * be on the run-queue of the associated CPU before any local

	 * wake-ups for concurrency management happen, restore CPU affinity

	 * of all workers first and then clear UNBOUND.  As we're called

	 * from CPU_ONLINE, the following shouldn't fail.

		/*

		 * A bound idle worker should actually be on the runqueue

		 * of the associated CPU for local wake-ups targeting it to

		 * work.  Kick all idle workers so that they migrate to the

		 * associated CPU.  Doing this in the same loop as

		 * replacing UNBOUND with REBOUND is safe as no worker will

		 * be bound before @pool->lock is released.

		/*

		 * We want to clear UNBOUND but can't directly call

		 * worker_clr_flags() or adjust nr_running.  Atomically

		 * replace UNBOUND with another NOT_RUNNING flag REBOUND.

		 * @worker will clear REBOUND using worker_clr_flags() when

		 * it initiates the next execution cycle thus restoring

		 * concurrency management.  Note that when or whether

		 * @worker clears REBOUND doesn't affect correctness.

		 *

		 * WRITE_ONCE() is necessary because @worker->flags may be

		 * tested without holding any lock in

		 * wq_worker_running().  Without it, NOT_RUNNING test may

		 * fail incorrectly leading to premature concurrency

		 * management operations.

/**

 * restore_unbound_workers_cpumask - restore cpumask of unbound workers

 * @pool: unbound pool of interest

 * @cpu: the CPU which is coming up

 *

 * An unbound pool may end up with a cpumask which doesn't have any online

 * CPUs.  When a worker of such pool get scheduled, the scheduler resets

 * its cpus_allowed.  If @cpu is in @pool's cpumask which didn't have any

 * online CPU before, cpus_allowed of all its workers should be restored.

 is @cpu allowed for @pool? */

 as we're called from CPU_ONLINE, the following shouldn't fail */

 update NUMA affinity of unbound workqueues */

 unbinding per-cpu workers should happen on the local CPU */

 update NUMA affinity of unbound workqueues */

/**

 * work_on_cpu - run a function in thread context on a particular cpu

 * @cpu: the cpu to run on

 * @fn: the function to run

 * @arg: the function arg

 *

 * It is up to the caller to ensure that the cpu doesn't go offline.

 * The caller must not hold any locks which would prevent @fn from completing.

 *

 * Return: The value @fn returns.

/**

 * work_on_cpu_safe - run a function in thread context on a particular cpu

 * @cpu: the cpu to run on

 * @fn:  the function to run

 * @arg: the function argument

 *

 * Disables CPU hotplug and calls work_on_cpu(). The caller must not hold

 * any locks which would prevent @fn from completing.

 *

 * Return: The value @fn returns.

 CONFIG_SMP */

/**

 * freeze_workqueues_begin - begin freezing workqueues

 *

 * Start freezing workqueues.  After this function returns, all freezable

 * workqueues will queue new works to their inactive_works list instead of

 * pool->worklist.

 *

 * CONTEXT:

 * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.

/**

 * freeze_workqueues_busy - are freezable workqueues still busy?

 *

 * Check whether freezing is complete.  This function must be called

 * between freeze_workqueues_begin() and thaw_workqueues().

 *

 * CONTEXT:

 * Grabs and releases wq_pool_mutex.

 *

 * Return:

 * %true if some freezable workqueues are still busy.  %false if freezing

 * is complete.

		/*

		 * nr_active is monotonically decreasing.  It's safe

		 * to peek without lock.

/**

 * thaw_workqueues - thaw workqueues

 *

 * Thaw workqueues.  Normal queueing is restored and all collected

 * frozen works are transferred to their respective pool worklists.

 *

 * CONTEXT:

 * Grabs and releases wq_pool_mutex, wq->mutex and pool->lock's.

 restore max_active and repopulate worklist */

 CONFIG_FREEZER */

 creating multiple pwqs breaks ordering guarantee */

/**

 *  workqueue_set_unbound_cpumask - Set the low-level unbound cpumask

 *  @cpumask: the cpumask to set

 *

 *  The low-level workqueues cpumask is a global cpumask that limits

 *  the affinity of all unbound workqueues.  This function check the @cpumask

 *  and apply it to all unbound workqueues and updates all pwqs of them.

 *

 *  Return:	0	- Success

 *  		-EINVAL	- Invalid @cpumask

 *  		-ENOMEM	- Failed to allocate memory for attrs or pwqs.

	/*

	 * Not excluding isolated cpus on purpose.

	 * If the user wishes to include them, we allow that.

 save the old wq_unbound_cpumask. */

 update wq_unbound_cpumask at first and apply it to wqs. */

 restore the wq_unbound_cpumask when failed. */

/*

 * Workqueues with WQ_SYSFS flag set is visible to userland via

 * /sys/bus/workqueue/devices/WQ_NAME.  All visible workqueues have the

 * following attributes.

 *

 *  per_cpu	RO bool	: whether the workqueue is per-cpu or unbound

 *  max_active	RW int	: maximum number of in-flight work items

 *

 * Unbound workqueues have the following extra attributes.

 *

 *  pool_ids	RO int	: the associated pool IDs for each node

 *  nice	RW int	: nice value of the workers

 *  cpumask	RW mask	: bitmask of allowed CPUs for the workers

 *  numa	RW bool	: whether enable NUMA affinity

 prepare workqueue_attrs for sysfs store operations */

/**

 * workqueue_sysfs_register - make a workqueue visible in sysfs

 * @wq: the workqueue to register

 *

 * Expose @wq in sysfs under /sys/bus/workqueue/devices.

 * alloc_workqueue*() automatically calls this function if WQ_SYSFS is set

 * which is the preferred method.

 *

 * Workqueue user should use this function directly iff it wants to apply

 * workqueue_attrs before making the workqueue visible in sysfs; otherwise,

 * apply_workqueue_attrs() may race against userland updating the

 * attributes.

 *

 * Return: 0 on success, -errno on failure.

	/*

	 * Adjusting max_active or creating new pwqs by applying

	 * attributes breaks ordering guarantee.  Disallow exposing ordered

	 * workqueues.

	/*

	 * unbound_attrs are created separately.  Suppress uevent until

	 * everything is ready.

/**

 * workqueue_sysfs_unregister - undo workqueue_sysfs_register()

 * @wq: the workqueue to unregister

 *

 * If @wq is registered to sysfs by workqueue_sysfs_register(), unregister.

 CONFIG_SYSFS */

 CONFIG_SYSFS */

/*

 * Workqueue watchdog.

 *

 * Stall may be caused by various bugs - missing WQ_MEM_RECLAIM, illegal

 * flush dependency, a concurrency managed work item which stays RUNNING

 * indefinitely.  Workqueue stalls can be very difficult to debug as the

 * usual warning mechanisms don't trigger and internal workqueue state is

 * largely opaque.

 *

 * Workqueue watchdog monitors all worker pools periodically and dumps

 * state if some pools failed to make forward progress for a while where

 * forward progress is defined as the first item on ->worklist changing.

 *

 * This mechanism is controlled through the kernel parameter

 * "workqueue.watchdog_thresh" which can be updated at runtime through the

 * corresponding sysfs parameter file.

		/*

		 * If a virtual machine is stopped by the host it can look to

		 * the watchdog like a stall.

 get the latest of pool and touched timestamps */

 did we stall? */

 CONFIG_WQ_WATCHDOG */

 CONFIG_WQ_WATCHDOG */

	/*

	 * We want masks of possible CPUs of each node which isn't readily

	 * available.  Build one from cpu_to_node() which should have been

	 * fully initialized by now.

/**

 * workqueue_init_early - early init for workqueue subsystem

 *

 * This is the first half of two-staged workqueue subsystem initialization

 * and invoked as soon as the bare basics - memory allocation, cpumasks and

 * idr are up.  It sets up all the data structures and system workqueues

 * and allows early boot code to create workqueues and queue/cancel work

 * items.  Actual work item execution starts only after kthreads can be

 * created and scheduled right before early initcalls.

 initialize CPU pools */

 alloc pool ID */

 create default unbound and ordered wq attrs */

		/*

		 * An ordered wq should have only one pwq as ordering is

		 * guaranteed by max_active which is enforced by pwqs.

		 * Turn off NUMA so that dfl_pwq is used for all nodes.

/**

 * workqueue_init - bring workqueue subsystem fully online

 *

 * This is the latter half of two-staged workqueue subsystem initialization

 * and invoked as soon as kthreads can be created and scheduled.

 * Workqueues have been created and work items queued on them, but there

 * are no kworkers executing the work items yet.  Populate the worker pools

 * with the initial workers and enable future kworker creations.

	/*

	 * It'd be simpler to initialize NUMA in workqueue_init_early() but

	 * CPU to node mapping may not be available that early on some

	 * archs such as power and arm64.  As per-cpu pools created

	 * previously could be missing node hint and unbound pools NUMA

	 * affinity, fix them up.

	 *

	 * Also, while iterating workqueues, create rescuers if requested.

 create the initial workers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Request a notification when the current cpu returns to userspace.  Must be

 * called in atomic context.  The notifier will also be called in atomic

 * context.

/*

 * Removes a registered user return notifier.  Must be called from atomic

 * context, and from the same cpu registration occurred in.

 Calls registered user return notifiers */

 SPDX-License-Identifier: GPL-2.0

/*

 * Detect hard and soft lockups on a system

 *

 * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.

 *

 * Note: Most of this code is borrowed heavily from the original softlockup

 * detector, so thanks to Ingo for the initial implementation.

 * Some chunks also taken from the old x86-specific nmi watchdog code, thanks

 * to those contributors as well.

 CONFIG_SMP */

/*

 * Should we panic when a soft-lockup or hard-lockup occurs:

/*

 * We may not want to enable hard lockup detection by default in all cases,

 * for example when running the kernel as a guest on a hypervisor. In these

 * cases this function can be called to disable hard lockup detection. This

 * function should only be executed once by the boot processor before the

 * kernel command line parameters are parsed, because otherwise it is not

 * possible to override this in hardlockup_panic_setup().

 CONFIG_HARDLOCKUP_DETECTOR */

/*

 * These functions can be overridden if an architecture implements its

 * own hardlockup detector.

 *

 * watchdog_nmi_enable/disable can be implemented to start and stop when

 * softlockup watchdog start and stop. The arch must select the

 * SOFTLOCKUP_DETECTOR Kconfig.

 Return 0, if a NMI watchdog is available. Error code otherwise */

/**

 * watchdog_nmi_stop - Stop the watchdog for reconfiguration

 *

 * The reconfiguration steps are:

 * watchdog_nmi_stop();

 * update_variables();

 * watchdog_nmi_start();

/**

 * watchdog_nmi_start - Start the watchdog after reconfiguration

 *

 * Counterpart to watchdog_nmi_stop().

 *

 * The following variables have been updated in update_variables() and

 * contain the currently valid configuration:

 * - watchdog_enabled

 * - watchdog_thresh

 * - watchdog_cpumask

/**

 * lockup_detector_update_enable - Update the sysctl enable bit

 *

 * Caller needs to make sure that the NMI/perf watchdogs are off, so this

 * can't race with watchdog_nmi_disable().

/*

 * Delay the soflockup report when running a known slow code.

 * It does _not_ affect the timestamp of the last successdul reschedule.

 Global variables, exported for sysctl */

 Timestamp taken after the last successful reschedule. */

 Timestamp of the last softlockup report. */

/*

 * Hard-lockup warnings should be triggered after just a few seconds. Soft-

 * lockups can have false positives under extreme conditions. So we generally

 * want a higher threshold for soft lockups than for hard lockups. So we couple

 * the thresholds with a factor: we make the soft threshold twice the amount of

 * time the hard threshold is.

/*

 * Returns seconds, approximately.  We don't need nanosecond

 * resolution, and we don't need to waste time with a big divide when

 * 2^30ns == 1.074s.

 2^30 ~= 10^9 */

	/*

	 * convert watchdog_thresh from seconds to ns

	 * the divide by 5 is to give hrtimer several chances (two

	 * or three with the current relation between the soft

	 * and hard thresholds) to increment before the

	 * hardlockup detector generates a warning

 Commands for resetting the watchdog */

/**

 * touch_softlockup_watchdog_sched - touch watchdog on scheduler stalls

 *

 * Call when the scheduler may have stalled for legitimate reasons

 * preventing the watchdog task from executing - e.g. the scheduler

 * entering idle state.  This should only be used for scheduler events.

 * Use touch_softlockup_watchdog() for everything else.

	/*

	 * Preemption can be enabled.  It doesn't matter which CPU's watchdog

	 * report period gets restarted here, so use the raw_ operation.

	/*

	 * watchdog_mutex cannpt be taken here, as this might be called

	 * from (soft)interrupt context, so the access to

	 * watchdog_allowed_cpumask might race with a concurrent update.

	 *

	 * The watchdog time stamp can race against a concurrent real

	 * update as well, the only side effect might be a cycle delay for

	 * the softlockup check.

 Warn about unreasonable delays. */

 watchdog detector functions */

/*

 * The watchdog feed function - touches the timestamp.

 *

 * It only runs once every sample_period seconds (4 seconds by

 * default) to reset the softlockup timestamp. If this gets delayed

 * for more than 2*watchdog_thresh seconds then the debug-printout

 * triggers in watchdog_timer_fn().

 watchdog kicker functions */

 kick the hardlockup detector */

 kick the softlockup detector */

 .. and repeat */

	/*

	 * Read the current timestamp first. It might become invalid anytime

	 * when a virtual machine is stopped by the host or when the watchog

	 * is touched from NMI.

	/*

	 * If a virtual machine is stopped by the host it can look to

	 * the watchdog like a soft lockup. This function touches the watchdog.

	/*

	 * The stored timestamp is comparable with @now only when not touched.

	 * It might get touched anytime from NMI. Make sure that is_softlockup()

	 * uses the same (valid) value.

 Reset the interval when touched by known problematic code. */

			/*

			 * If the time stamp was touched atomically

			 * make sure the scheduler tick is up to date.

 Check for a softlockup. */

		/*

		 * Prevent multiple soft-lockup reports if one cpu is already

		 * engaged in dumping all cpu back traces.

 Start period for the next softlockup warning. */

	/*

	 * Start the timer first to prevent the NMI watchdog triggering

	 * before the timer has a chance to fire.

 Initialize timestamp */

 Enable the perf event */

	/*

	 * Disable the perf event first. That prevents that a large delay

	 * between disabling the timer and disabling the perf event causes

	 * the perf NMI to detect a false positive.

	/*

	 * Must be called outside the cpus locked section to prevent

	 * recursive locking in the perf code.

/*

 * Create the watchdog infrastructure and configure the detector(s).

	/*

	 * If sysctl is off and watchdog got disabled on the command line,

	 * nothing to do here.

 CONFIG_SOFTLOCKUP_DETECTOR */

 !CONFIG_SOFTLOCKUP_DETECTOR */

/**

 * lockup_detector_cleanup - Cleanup after cpu hotplug or sysctl changes

 *

 * Caller must not hold the cpu hotplug rwsem.

/**

 * lockup_detector_soft_poweroff - Interface to stop lockup detector(s)

 *

 * Special interface for parisc. It prevents lockup detector warnings from

 * the default pm_poweroff() function which busy loops forever.

 Propagate any changes to the watchdog infrastructure */

 Remove impossible cpus to keep sysctl output clean. */

/*

 * common function for watchdog, nmi_watchdog and soft_watchdog parameter

 *

 * caller             | table->data points to      | 'which'

 * -------------------|----------------------------|--------------------------

 * proc_watchdog      | watchdog_user_enabled      | NMI_WATCHDOG_ENABLED |

 *                    |                            | SOFT_WATCHDOG_ENABLED

 * -------------------|----------------------------|--------------------------

 * proc_nmi_watchdog  | nmi_watchdog_user_enabled  | NMI_WATCHDOG_ENABLED

 * -------------------|----------------------------|--------------------------

 * proc_soft_watchdog | soft_watchdog_user_enabled | SOFT_WATCHDOG_ENABLED

		/*

		 * On read synchronize the userspace interface. This is a

		 * racy snapshot.

/*

 * /proc/sys/kernel/watchdog

/*

 * /proc/sys/kernel/nmi_watchdog

/*

 * /proc/sys/kernel/soft_watchdog

/*

 * /proc/sys/kernel/watchdog_thresh

/*

 * The cpumask is the mask of possible cpus that the watchdog can run

 * on, not the mask of cpus it is actually running on.  This allows the

 * user to specify a mask that will include cpus that have not yet

 * been brought online, if desired.

 CONFIG_SYSCTL */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/kernel/ptrace.c

 *

 * (C) Copyright 1999 Linus Torvalds

 *

 * Common interfaces for "ptrace()" which we do not want

 * to continually duplicate across every architecture.

 for syscall_get_* */

/*

 * Access another process' address space via ptrace.

 * Source/target buffer must be kernel space,

 * Do not walk the page table directly, use get_user_pages

/*

 * ptrace a task: make the debugger its new parent and

 * move it to the ptrace list.

 *

 * Must be called with the tasklist lock write-held.

/**

 * __ptrace_unlink - unlink ptracee and restore its execution state

 * @child: ptracee to be unlinked

 *

 * Remove @child from the ptrace list, move it back to the original parent,

 * and restore the execution state so that it conforms to the group stop

 * state.

 *

 * Unlinking can happen via two paths - explicit PTRACE_DETACH or ptracer

 * exiting.  For PTRACE_DETACH, unless the ptracee has been killed between

 * ptrace_check_attach() and here, it's guaranteed to be in TASK_TRACED.

 * If the ptracer is exiting, the ptracee can be in any state.

 *

 * After detach, the ptracee should be in a state which conforms to the

 * group stop.  If the group is stopped or in the process of stopping, the

 * ptracee should be put into TASK_STOPPED; otherwise, it should be woken

 * up from TASK_TRACED.

 *

 * If the ptracee is in TASK_TRACED and needs to be moved to TASK_STOPPED,

 * it goes through TRACED -> RUNNING -> STOPPED transition which is similar

 * to but in the opposite direction of what happens while attaching to a

 * stopped task.  However, in this direction, the intermediate RUNNING

 * state is not hidden even from the current ptracer and if it immediately

 * re-attaches and performs a WNOHANG wait(2), it may fail.

 *

 * CONTEXT:

 * write_lock_irq(tasklist_lock)

	/*

	 * Clear all pending traps and TRAPPING.  TRAPPING should be

	 * cleared regardless of JOBCTL_STOP_PENDING.  Do it explicitly.

	/*

	 * Reinstate JOBCTL_STOP_PENDING if group stop is in effect and

	 * @child isn't dead.

		/*

		 * This is only possible if this thread was cloned by the

		 * traced task running in the stopped group, set the signal

		 * for the future reports.

		 * FIXME: we should change ptrace_init_task() to handle this

		 * case.

	/*

	 * If transition to TASK_STOPPED is pending or in TASK_TRACED, kick

	 * @child in the butt.  Note that @resume should be used iff @child

	 * is in TASK_TRACED; otherwise, we might unduly disrupt

	 * TASK_KILLABLE sleeps.

	/*

	 * The tracee changed its pid but the PTRACE_EVENT_EXEC event

	 * was not wait()'ed, most probably debugger targets the old

	 * leader which was destroyed in de_thread().

 Ensure that nothing can wake it up, even SIGKILL */

 Lockless, nobody but us can set this flag */

	/*

	 * PTRACE_LISTEN can allow ptrace_trap_notify to wake us up remotely.

	 * Recheck state under the lock to close this race.

/**

 * ptrace_check_attach - check whether ptracee is ready for ptrace operation

 * @child: ptracee to check for

 * @ignore_state: don't check whether @child is currently %TASK_TRACED

 *

 * Check whether @child is being ptraced by %current and ready for further

 * ptrace operations.  If @ignore_state is %false, @child also should be in

 * %TASK_TRACED state and on return the child is guaranteed to be traced

 * and not executing.  If @ignore_state is %true, @child can be in any

 * state.

 *

 * CONTEXT:

 * Grabs and releases tasklist_lock and @child->sighand->siglock.

 *

 * RETURNS:

 * 0 on success, -ESRCH if %child is not ready.

	/*

	 * We take the read lock around doing both checks to close a

	 * possible race where someone else was tracing our child and

	 * detached between these two checks.  After this locked check,

	 * we are sure that this is our traced child and that can only

	 * be changed by us so it's not changing right after this.

		/*

		 * child->sighand can't be NULL, release_task()

		 * does ptrace_unlink() before __exit_signal().

			/*

			 * This can only happen if may_ptrace_stop() fails and

			 * ptrace_stop() changes ->state back to TASK_RUNNING,

			 * so we should not worry about leaking __TASK_TRACED.

 Returns 0 on success, -errno on denial. */

	/* May we inspect the given task?

	 * This check is used both for attaching with ptrace

	 * and for allowing access to sensitive information in /proc.

	 *

	 * ptrace_attach denies several cases that /proc allows

	 * because setting up the necessary parent/child relationship

	 * or halting the specified task is impossible.

 Don't let security modules deny introspection */

		/*

		 * Using the euid would make more sense here, but something

		 * in userland might rely on the old behavior, and this

		 * shouldn't be a security problem since

		 * PTRACE_MODE_REALCREDS implies that the caller explicitly

		 * used a syscall that requests access to another process

		 * (and not a filesystem syscall to procfs).

	/*

	 * If a task drops privileges and becomes nondumpable (through a syscall

	 * like setresuid()) while we are trying to access it, we must ensure

	 * that the dumpability is read after the credentials; otherwise,

	 * we may be able to attach to a task that we shouldn't be able to

	 * attach to (as if the task had dropped privileges without becoming

	 * nondumpable).

	 * Pairs with a write barrier in commit_creds().

	/*

	 * Protect exec's credential calculations against our interference;

	 * SUID, SGID and LSM creds get determined differently

	 * under ptrace.

 SEIZE doesn't trap tracee on attach */

	/*

	 * If the task is already STOPPED, set JOBCTL_TRAP_STOP and

	 * TRAPPING, and kick it so that it transits to TRACED.  TRAPPING

	 * will be cleared if the child completes the transition or any

	 * event which clears the group stop states happens.  We'll wait

	 * for the transition to complete before returning from this

	 * function.

	 *

	 * This hides STOPPED -> RUNNING -> TRACED transition from the

	 * attaching thread but a different thread in the same group can

	 * still observe the transient RUNNING state.  IOW, if another

	 * thread's WNOHANG wait(2) on the stopped tracee races against

	 * ATTACH, the wait(2) may fail due to the transient RUNNING.

	 *

	 * The following task_is_stopped() test is safe as both transitions

	 * in and out of STOPPED are protected by siglock.

		/*

		 * We do not bother to change retval or clear JOBCTL_TRAPPING

		 * if wait_on_bit() was interrupted by SIGKILL. The tracer will

		 * not return to user-mode, it will exit and clear this bit in

		 * __ptrace_unlink() if it wasn't already cleared by the tracee;

		 * and until then nobody can ptrace this task.

/**

 * ptrace_traceme  --  helper for PTRACE_TRACEME

 *

 * Performs checks and sets PT_PTRACED.

 * Should be used by all ptrace implementations for PTRACE_TRACEME.

 Are we already being traced? */

		/*

		 * Check PF_EXITING to ensure ->real_parent has not passed

		 * exit_ptrace(). Otherwise we don't report the error but

		 * pretend ->real_parent untraces us right after return.

/*

 * Called with irqs disabled, returns true if childs should reap themselves.

/*

 * Called with tasklist_lock held for writing.

 * Unlink a traced task, and clean it up if it was a traced zombie.

 * Return true if it needs to be reaped with release_task().

 * (We can't call release_task() here because we already hold tasklist_lock.)

 *

 * If it's a zombie, our attachedness prevented normal parent notification

 * or self-reaping.  Do notification now if it would have happened earlier.

 * If it should reap itself, return true.

 *

 * If it's our own child, there is no notification to do. But if our normal

 * children self-reap, then this child was prevented by ptrace and we must

 * reap it now, in that case we must also wake up sub-threads sleeping in

 * do_wait().

 Mark it as in the process of being reaped. */

 Architecture-specific hardware disable .. */

	/*

	 * We rely on ptrace_freeze_traced(). It can't be killed and

	 * untraced by another thread, it can't be a zombie.

	/*

	 * tasklist_lock avoids the race with wait_task_stopped(), see

	 * the comment in ptrace_resume().

/*

 * Detach all tasks we were using ptrace on. Called with tasklist held

 * for writing.

 Avoid intermediate state when all opts are cleared */

 unknown flags */

 Ensure arg.off fits in an unsigned long */

 beyond the end of the list */

	/*

	 * Change ->exit_code and ->state under siglock to avoid the race

	 * with wait_task_stopped() in between; a non-zero ->exit_code will

	 * wrongly look like another report from tracee.

	 *

	 * Note that we need siglock even if ->exit_code == data and/or this

	 * status was not reported yet, the new status must not be cleared by

	 * wait_task_stopped() after resume.

	 *

	 * If data == 0 we do not care if wait_task_stopped() reports the old

	 * status and clears the code too; this can't race with the tracee, it

	 * takes siglock after resume.

/*

 * This is declared in linux/regset.h and defined in machine-dependent

 * code.  We put the export here, near the primary machine-neutral use,

 * to ensure no machine forgets it.

 args is the last field in struct ptrace_syscall_info.entry */

	/*

	 * As struct ptrace_syscall_info.entry is currently a subset

	 * of struct ptrace_syscall_info.seccomp, it makes sense to

	 * initialize that subset using ptrace_get_syscall_info_entry().

	 * This can be reconsidered in the future if these structures

	 * diverge significantly enough.

 ret_data is the last field in struct ptrace_syscall_info.seccomp */

 is_error is the last field in struct ptrace_syscall_info.exit */

	/*

	 * This does not need lock_task_sighand() to access

	 * child->last_siginfo because ptrace_freeze_traced()

	 * called earlier by ptrace_check_attach() ensures that

	 * the tracee cannot go away and clear its last_siginfo.

 CONFIG_HAVE_ARCH_TRACEHOOK */

		/*

		 * Every thread does recalc_sigpending() after resume, so

		 * retarget_shared_pending() and recalc_sigpending() are not

		 * called here.

		/*

		 * Stop tracee without any side-effect on signal or job

		 * control.  At least one trap is guaranteed to happen

		 * after this request.  If @child is already trapped, the

		 * current trap is not disturbed and another trap will

		 * happen after the current trap is ended with PTRACE_CONT.

		 *

		 * The actual trap might not be PTRACE_EVENT_STOP trap but

		 * the pending condition is cleared regardless.

		/*

		 * INTERRUPT doesn't disturb existing trap sans one

		 * exception.  If ptracer issued LISTEN for the current

		 * STOP, this INTERRUPT should clear LISTEN and re-trap

		 * tracee into STOP.

		/*

		 * Listen for events.  Tracee must be in STOP.  It's not

		 * resumed per-se but is not considered to be in TRACED by

		 * wait(2) or ptrace(2).  If an async event (e.g. group

		 * stop state change) happens, tracee will enter STOP trap

		 * again.  Alternatively, ptracer can issue INTERRUPT to

		 * finish listening and re-trap tracee into STOP.

			/*

			 * If NOTIFY is set, it means event happened between

			 * start of this trap and now.  Trigger re-trap.

 detach a process that was attached. */

 already dead */

		/*

		 * Some architectures need to do book-keeping after

		 * a ptrace attach.

		/*

		 * Some architectures need to do book-keeping after

		 * a ptrace attach.

 CONFIG_COMPAT */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra

 *

 * Provides a framework for enqueueing and running callbacks from hardirq

 * context. The enqueueing is NMI-safe.

/*

 * Claim the entry so that no one else will poke at it.

	/*

	 * If the work is already pending, no need to raise the IPI.

	 * The pairing smp_mb() in irq_work_single() makes sure

	 * everything we did before is visible.

	/*

	 * Lame architectures will get the timer tick callback

 Enqueue on current CPU, work must already be claimed and preempt disabled */

 If the work is "lazy", handle it from next tick if any */

 Enqueue the irq work @work on the current CPU */

 Only queue if not already pending */

 Queue the entry and raise the IPI if needed. */

/*

 * Enqueue the irq_work @work on @cpu unless it's already pending

 * somewhere.

 *

 * Can be re-enqueued while the callback is still in progress.

 CONFIG_SMP: */

 All work should have been flushed before going offline */

 Only queue if not already pending */

 Arch remote IPI send/receive backend aren't NMI safe */

		/*

		 * On PREEMPT_RT the items which are not marked as

		 * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work

		 * item is used on the remote CPU to wake the thread.

 CONFIG_SMP */

 All work should have been flushed before going offline */

	/*

	 * Clear the PENDING bit, after this point the @work can be re-used.

	 * The PENDING bit acts as a lock, and we own it, so we can clear it

	 * without atomic ops.

	/*

	 * See irq_work_claim().

	/*

	 * Clear the BUSY bit, if set, and return to the free state if no-one

	 * else claimed it meanwhile.

	/*

	 * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed

	 * in a per-CPU thread in preemptible context. Only the items which are

	 * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.

/*

 * hotplug calls this through:

 *  hotplug_cfd() -> flush_smp_call_function_queue()

/*

 * Synchronize against the irq_work @entry, ensures the entry is not

 * currently in use.

 SPDX-License-Identifier: GPL-2.0-only

	/* Start with the same capabilities as init but useless for doing

	 * anything as the capabilities are bound to the new user namespace.

 tgcred will be cleared in our caller bc CLONE_THREAD won't be set */

/*

 * Create a new user namespace, deriving the creator from the user in the

 * passed credentials, and replacing that user with the new root user for the

 * new namespace.

 *

 * This is called by copy_creds(), which will finish setting the target task's

 * credentials.

	/*

	 * Verify that we can not violate the policy of which files

	 * may be accessed that is specified by the root directory,

	 * by verifying that the root directory is at the root of the

	 * mount namespace which allows all files to be accessed.

	/* The creator needs a mapping in the parent user namespace

	 * or else we won't be able to reasonably tell userspace who

	 * created a user_namespace.

 Leave the new->user_ns reference with the new user namespace. */

 Inherit USERNS_SETGROUPS_ALLOWED from our parent */

/**

 * idmap_key struct holds the information necessary to find an idmapping in a

 * sorted idmap array. It is passed to cmp_map_id() as first argument.

 true  -> id from kid; false -> kid from id */

 id to find */

 == 0 unless used with map_id_range_down() */

/**

 * cmp_map_id - Function to be passed to bsearch() to find the requested

 * idmapping. Expects struct idmap_key to be passed via @k.

 handle map_id_{down,up}() */

/**

 * map_id_range_down_max - Find idmap via binary search in ordered idmap array.

 * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.

/**

 * map_id_range_down_base - Find idmap via binary search in static extent array.

 * Can only be called if number of mappings is equal or less than

 * UID_GID_MAP_MAX_BASE_EXTENTS.

 Find the matching extent */

 Map the id or note failure */

/**

 * map_id_up_base - Find idmap via binary search in static extent array.

 * Can only be called if number of mappings is equal or less than

 * UID_GID_MAP_MAX_BASE_EXTENTS.

 Find the matching extent */

/**

 * map_id_up_max - Find idmap via binary search in ordered idmap array.

 * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.

 Map the id or note failure */

/**

 *	make_kuid - Map a user-namespace uid pair into a kuid.

 *	@ns:  User namespace that the uid is in

 *	@uid: User identifier

 *

 *	Maps a user-namespace uid pair into a kernel internal kuid,

 *	and returns that kuid.

 *

 *	When there is no mapping defined for the user-namespace uid

 *	pair INVALID_UID is returned.  Callers are expected to test

 *	for and handle INVALID_UID being returned.  INVALID_UID

 *	may be tested for using uid_valid().

 Map the uid to a global kernel uid */

/**

 *	from_kuid - Create a uid from a kuid user-namespace pair.

 *	@targ: The user namespace we want a uid in.

 *	@kuid: The kernel internal uid to start with.

 *

 *	Map @kuid into the user-namespace specified by @targ and

 *	return the resulting uid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	If @kuid has no mapping in @targ (uid_t)-1 is returned.

 Map the uid from a global kernel uid */

/**

 *	from_kuid_munged - Create a uid from a kuid user-namespace pair.

 *	@targ: The user namespace we want a uid in.

 *	@kuid: The kernel internal uid to start with.

 *

 *	Map @kuid into the user-namespace specified by @targ and

 *	return the resulting uid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	Unlike from_kuid from_kuid_munged never fails and always

 *	returns a valid uid.  This makes from_kuid_munged appropriate

 *	for use in syscalls like stat and getuid where failing the

 *	system call and failing to provide a valid uid are not an

 *	options.

 *

 *	If @kuid has no mapping in @targ overflowuid is returned.

/**

 *	make_kgid - Map a user-namespace gid pair into a kgid.

 *	@ns:  User namespace that the gid is in

 *	@gid: group identifier

 *

 *	Maps a user-namespace gid pair into a kernel internal kgid,

 *	and returns that kgid.

 *

 *	When there is no mapping defined for the user-namespace gid

 *	pair INVALID_GID is returned.  Callers are expected to test

 *	for and handle INVALID_GID being returned.  INVALID_GID may be

 *	tested for using gid_valid().

 Map the gid to a global kernel gid */

/**

 *	from_kgid - Create a gid from a kgid user-namespace pair.

 *	@targ: The user namespace we want a gid in.

 *	@kgid: The kernel internal gid to start with.

 *

 *	Map @kgid into the user-namespace specified by @targ and

 *	return the resulting gid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	If @kgid has no mapping in @targ (gid_t)-1 is returned.

 Map the gid from a global kernel gid */

/**

 *	from_kgid_munged - Create a gid from a kgid user-namespace pair.

 *	@targ: The user namespace we want a gid in.

 *	@kgid: The kernel internal gid to start with.

 *

 *	Map @kgid into the user-namespace specified by @targ and

 *	return the resulting gid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	Unlike from_kgid from_kgid_munged never fails and always

 *	returns a valid gid.  This makes from_kgid_munged appropriate

 *	for use in syscalls like stat and getgid where failing the

 *	system call and failing to provide a valid gid are not options.

 *

 *	If @kgid has no mapping in @targ overflowgid is returned.

/**

 *	make_kprojid - Map a user-namespace projid pair into a kprojid.

 *	@ns:  User namespace that the projid is in

 *	@projid: Project identifier

 *

 *	Maps a user-namespace uid pair into a kernel internal kuid,

 *	and returns that kuid.

 *

 *	When there is no mapping defined for the user-namespace projid

 *	pair INVALID_PROJID is returned.  Callers are expected to test

 *	for and handle INVALID_PROJID being returned.  INVALID_PROJID

 *	may be tested for using projid_valid().

 Map the uid to a global kernel uid */

/**

 *	from_kprojid - Create a projid from a kprojid user-namespace pair.

 *	@targ: The user namespace we want a projid in.

 *	@kprojid: The kernel internal project identifier to start with.

 *

 *	Map @kprojid into the user-namespace specified by @targ and

 *	return the resulting projid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	If @kprojid has no mapping in @targ (projid_t)-1 is returned.

 Map the uid from a global kernel uid */

/**

 *	from_kprojid_munged - Create a projiid from a kprojid user-namespace pair.

 *	@targ: The user namespace we want a projid in.

 *	@kprojid: The kernel internal projid to start with.

 *

 *	Map @kprojid into the user-namespace specified by @targ and

 *	return the resulting projid.

 *

 *	There is always a mapping into the initial user_namespace.

 *

 *	Unlike from_kprojid from_kprojid_munged never fails and always

 *	returns a valid projid.  This makes from_kprojid_munged

 *	appropriate for use in syscalls like stat and where

 *	failing the system call and failing to provide a valid projid are

 *	not an options.

 *

 *	If @kprojid has no mapping in @targ OVERFLOW_PROJID is returned.

 Does the upper range intersect a previous extent? */

 Does the lower range intersect a previous extent? */

/**

 * insert_extent - Safely insert a new idmap extent into struct uid_gid_map.

 * Takes care to allocate a 4K block of memory if the number of mappings exceeds

 * UID_GID_MAP_MAX_BASE_EXTENTS.

 Allocate memory for 340 mappings. */

		/* Copy over memory. Only set up memory for the forward pointer.

		 * Defer the memory setup for the reverse pointer.

 cmp function to sort() forward mappings */

 cmp function to sort() reverse mappings */

/**

 * sort_idmaps - Sorts an array of idmap entries.

 * Can only be called if number of mappings exceeds UID_GID_MAP_MAX_BASE_EXTENTS.

 Sort forward array. */

 Only copy the memory from forward we actually need. */

 Sort reverse array. */

/**

 * verify_root_map() - check the uid 0 mapping

 * @file: idmapping file

 * @map_ns: user namespace of the target process

 * @new_map: requested idmap

 *

 * If a process requests mapping parent uid 0 into the new ns, verify that the

 * process writing the map had the CAP_SETFCAP capability as the target process

 * will be able to write fscaps that are valid in ancestor user namespaces.

 *

 * Return: true if the mapping is allowed, false if not.

		/* The process unshared its ns and is writing to its own

		 * /proc/self/uid_map.  User already has full capabilites in

		 * the new namespace.  Verify that the parent had CAP_SETFCAP

		 * when it unshared.

		/* Process p1 is writing to uid_map of p2, who is in a child

		 * user namespace to p1's.  Verify that the opener of the map

		 * file has CAP_SETFCAP against the parent of the new map

 Only allow < page size writes at the beginning of the file */

 Slurp in the user data */

	/*

	 * The userns_state_mutex serializes all writes to any given map.

	 *

	 * Any map is only ever written once.

	 *

	 * An id map fits within 1 cache line on most architectures.

	 *

	 * On read nothing needs to be done unless you are on an

	 * architecture with a crazy cache coherency model like alpha.

	 *

	 * There is a one time data dependency between reading the

	 * count of the extents and the values of the extents.  The

	 * desired behavior is to see the values of the extents that

	 * were written before the count of the extents.

	 *

	 * To achieve this smp_wmb() is used on guarantee the write

	 * order and smp_rmb() is guaranteed that we don't have crazy

	 * architectures returning stale data.

 Only allow one successful write to the map */

	/*

	 * Adjusting namespace settings requires capabilities on the target.

 Parse the user data */

 Find the end of line and ensure I don't look past it */

 Verify there is not trailing junk on the line */

 Verify we have been given valid starting values */

		/* Verify count is not zero and does not cause the

		 * extent to wrap

 Do the ranges in extent overlap any previous extents? */

 Be very certain the new map actually exists */

 Validate the user is allowed to use user id's mapped to. */

	/* Map the lower ids from the parent user namespace to the

	 * kernel global id space.

		/* Fail if we can not map the specified extent to

		 * the kernel global id space.

	/*

	 * If we want to use binary search for lookup, this clones the extent

	 * array and sorts both copies.

 Install the map */

 Anyone can set any valid project id no capability needed */

	/* Don't allow mappings that would allow anything that wouldn't

	 * be allowed without the establishment of unprivileged mappings.

 Allow anyone to set a mapping that doesn't require privilege */

	/* Allow the specified ids if we have the appropriate capability

	 * (CAP_SETUID or CAP_SETGID) over the parent user namespace.

	 * And the opener of the id file also has the appropriate capability.

 Only allow a very narrow range of strings to be written */

 What was written? */

 What is being requested? */

 Verify there is not trailing junk on the line */

		/* Enabling setgroups after setgroups has been disabled

		 * is not allowed.

		/* Permanently disabling setgroups after setgroups has

		 * been enabled by writing the gid_map is not allowed.

 Report a successful write */

	/* It is not safe to use setgroups until a gid mapping in

	 * the user namespace has been established.

 Is setgroups allowed? */

/*

 * Returns true if @child is the same namespace or a descendant of

 * @ancestor.

	/* Don't allow gaining capabilities by reentering

	 * the same user namespace.

 Tasks that share a thread group must share a user namespace */

 See if the owner is in the current user namespace */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Module signature checker

 *

 * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.

 * Written by David Howells (dhowells@redhat.com)

/**

 * mod_check_sig - check that the given signature is sane

 *

 * @ms:		Signature to check.

 * @file_len:	Size of the file to which @ms is appended.

 * @name:	What is being checked. Used for error messages.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * kernel/stop_machine.c

 *

 * Copyright (C) 2008, 2005	IBM Corporation.

 * Copyright (C) 2008, 2005	Rusty Russell rusty@rustcorp.com.au

 * Copyright (C) 2010		SUSE Linux Products GmbH

 * Copyright (C) 2010		Tejun Heo <tj@kernel.org>

/*

 * Structure to determine completion condition and record errors.  May

 * be shared by works on different cpus.

 nr left to execute */

 collected return value */

 fired if nr_todo reaches 0 */

 the actual stopper, one per every possible cpu, enabled on online cpus */

 is this stopper enabled? */

 list of pending works */

 for stop_cpus */

	/*

	 * If @task is a stopper task, it cannot migrate and task_cpu() is

	 * stable.

 static data for stop_cpus */

 signal completion unless @done is NULL */

 queue @work to @stopper.  if offline, @work is completed immediately */

/**

 * stop_one_cpu - stop a cpu

 * @cpu: cpu to stop

 * @fn: function to execute

 * @arg: argument to @fn

 *

 * Execute @fn(@arg) on @cpu.  @fn is run in a process context with

 * the highest priority preempting any task on the cpu and

 * monopolizing it.  This function returns after the execution is

 * complete.

 *

 * This function doesn't guarantee @cpu stays online till @fn

 * completes.  If @cpu goes down in the middle, execution may happen

 * partially or fully on different cpus.  @fn should either be ready

 * for that or the caller should ensure that @cpu stays online until

 * this function completes.

 *

 * CONTEXT:

 * Might sleep.

 *

 * RETURNS:

 * -ENOENT if @fn(@arg) was not executed because @cpu was offline;

 * otherwise, the return value of @fn.

	/*

	 * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup

	 * cycle by doing a preemption:

 This controls the threads on each CPU. */

 Dummy starting state for thread. */

 Awaiting everyone to be scheduled. */

 Disable interrupts. */

 Run the function */

 Exit */

 Like num_online_cpus(), but hotplug cpu uses us, so we need this. */

 Reset ack counter. */

 Last one to ack a state moves to the next state. */

 This is the cpu_stop function which stops the CPU. */

	/*

	 * When called from stop_machine_from_inactive_cpu(), irq might

	 * already be disabled.  Save the state and restore it on exit.

 Simple state machine */

 Chill out and ensure we re-read multi_stop_state. */

			/*

			 * At this stage all other CPUs we depend on must spin

			 * in the same loop. Any reason for hard-lockup should

			 * be detected and reported on their side.

	/*

	 * The waking up of stopper threads has to happen in the same

	 * scheduling context as the queueing.  Otherwise, there is a

	 * possibility of one of the above stoppers being woken up by another

	 * CPU, and preempting us. This will cause us to not wake up the other

	 * stopper forever.

	/*

	 * Ensure that if we race with __stop_cpus() the stoppers won't get

	 * queued up in reverse order leading to system deadlock.

	 *

	 * We can't miss stop_cpus_in_progress if queue_stop_cpus_work() has

	 * queued a work on cpu1 but not on cpu2, we hold both locks.

	 *

	 * It can be falsely true but it is safe to spin until it is cleared,

	 * queue_stop_cpus_work() does everything under preempt_disable().

/**

 * stop_two_cpus - stops two cpus

 * @cpu1: the cpu to stop

 * @cpu2: the other cpu to stop

 * @fn: function to execute

 * @arg: argument to @fn

 *

 * Stops both the current and specified CPU and runs @fn on one of them.

 *

 * returns when both are completed.

/**

 * stop_one_cpu_nowait - stop a cpu but don't wait for completion

 * @cpu: cpu to stop

 * @fn: function to execute

 * @arg: argument to @fn

 * @work_buf: pointer to cpu_stop_work structure

 *

 * Similar to stop_one_cpu() but doesn't wait for completion.  The

 * caller is responsible for ensuring @work_buf is currently unused

 * and will remain untouched until stopper starts executing @fn.

 *

 * CONTEXT:

 * Don't care.

 *

 * RETURNS:

 * true if cpu_stop_work was queued successfully and @fn will be called,

 * false otherwise.

	/*

	 * Disable preemption while queueing to avoid getting

	 * preempted by a stopper which might wait for other stoppers

	 * to enter @fn which can lead to deadlock.

/**

 * stop_cpus - stop multiple cpus

 * @cpumask: cpus to stop

 * @fn: function to execute

 * @arg: argument to @fn

 *

 * Execute @fn(@arg) on online cpus in @cpumask.  On each target cpu,

 * @fn is run in a process context with the highest priority

 * preempting any task on the cpu and monopolizing it.  This function

 * returns after all executions are complete.

 *

 * This function doesn't guarantee the cpus in @cpumask stay online

 * till @fn completes.  If some cpus go down in the middle, execution

 * on the cpu may happen partially or fully on different cpus.  @fn

 * should either be ready for that or the caller should ensure that

 * the cpus stay online until this function completes.

 *

 * All stop_cpus() calls are serialized making it safe for @fn to wait

 * for all cpus to start executing it.

 *

 * CONTEXT:

 * Might sleep.

 *

 * RETURNS:

 * -ENOENT if @fn(@arg) was not executed at all because all cpus in

 * @cpumask were offline; otherwise, 0 if all executions of @fn

 * returned 0, any non zero return value if any returned non zero.

 static works are used, process one request at a time */

 cpu stop callbacks must not sleep, make in_atomic() == T */

	/*

	 * Lockless. cpu_stopper_thread() will take stopper->lock and flush

	 * the pending works before it parks, until then it is fine to queue

	 * the new works.

		/*

		 * Handle the case where stop_machine() is called

		 * early in boot before stop_machine() has been

		 * initialized.

 Set the initial state and stop all online cpus. */

 No CPUs can come up or down during this. */

/**

 * stop_machine_from_inactive_cpu - stop_machine() from inactive CPU

 * @fn: the function to run

 * @data: the data ptr for the @fn()

 * @cpus: the cpus to run the @fn() on (NULL = any online cpu)

 *

 * This is identical to stop_machine() but can be called from a CPU which

 * is not active.  The local CPU is in the process of hotplug (so no other

 * CPU hotplug can start) and not marked active and doesn't have enough

 * context to sleep.

 *

 * This function provides stop_machine() functionality for such state by

 * using busy-wait for synchronization and executing @fn directly for local

 * CPU.

 *

 * CONTEXT:

 * Local CPU is inactive.  Temporarily stops all active CPUs.

 *

 * RETURNS:

 * 0 if all executions of @fn returned 0, any non zero return value if any

 * returned non zero.

 Local CPU must be inactive and CPU hotplug in progress. */

 +1 for local */

 No proper task established and can't sleep - busy wait for lock. */

 Schedule work on other CPUs and execute directly for local CPU */

 Busy wait for completion. */

 SPDX-License-Identifier: GPL-2.0-or-later

/* Rewritten by Rusty Russell, on the backs of many others...

   Copyright (C) 2001 Rusty Russell, 2002 Rusty Russell IBM.



/*

 * mutex protecting text section modification (dynamic code patching).

 * some users need to sleep (allocating memory...) while they hold this lock.

 *

 * Note: Also protects SMP-alternatives modification on x86.

 *

 * NOT exported to modules - patching kernel text is a really delicate matter.

 Cleared by build time tools if the table is already sorted. */

 Sort the kernel's built-in exception table */

 Given an address, look for it in the kernel exception table */

 Given an address, look for it in the exception tables. */

	/*

	 * There might be init symbols in saved stacktraces.

	 * Give those symbols a chance to be printed in

	 * backtraces (such as lockdep traces).

	 *

	 * Since we are after the module-symbols check, there's

	 * no danger of address overlap:

	/*

	 * If a stack dump happens while RCU is not watching, then

	 * RCU needs to be notified that it requires to start

	 * watching again. This can happen either by tracing that

	 * triggers a stack trace, or a WARN() that happens during

	 * coming back from idle, or cpu on or offlining.

	 *

	 * is_module_text_address() as well as the kprobe slots,

	 * is_bpf_text_address() and is_bpf_image_address require

	 * RCU to be watching.

 Treat this like an NMI as it can happen anywhere */

/*

 * On some architectures (PPC64, IA64) function pointers

 * are actually only tokens to some data that then holds the

 * real function address. As a result, to find if a function

 * pointer is part of the kernel text, we need to do some

 * special dereferencing first.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Kernel Probes (KProbes)

 *

 * Copyright (C) IBM Corporation, 2002, 2004

 *

 * 2002-Oct	Created by Vamsi Krishna S <vamsi_krishna@in.ibm.com> Kernel

 *		Probes initial implementation (includes suggestions from

 *		Rusty Russell).

 * 2004-Aug	Updated by Prasanna S Panchamukhi <prasanna@in.ibm.com> with

 *		hlists and exceptions notifier as suggested by Andi Kleen.

 * 2004-July	Suparna Bhattacharya <suparna@in.ibm.com> added jumper probes

 *		interface to access function arguments.

 * 2004-Sep	Prasanna S Panchamukhi <prasanna@in.ibm.com> Changed Kprobes

 *		exceptions notifier to be first on the priority list.

 * 2005-May	Hien Nguyen <hien@us.ibm.com>, Jim Keniston

 *		<jkenisto@us.ibm.com> and Prasanna S Panchamukhi

 *		<prasanna@in.ibm.com> added function-return probes.

/* kprobe_table can be accessed by

 * - Normal hlist traversal and RCU add/del under 'kprobe_mutex' is held.

 * Or

 * - RCU hlist traversal under disabling preempt (breakpoint handlers)

 NOTE: change this value only with 'kprobe_mutex' held */

 This protects 'kprobe_table' and 'optimizing_list' */

/*

 * Blacklist -- list of 'struct kprobe_blacklist_entry' to store info where

 * kprobes can not probe.

/*

 * 'kprobe::ainsn.insn' points to the copy of the instruction to be

 * single-stepped. x86_64, POWER4 and above have no-exec support and

 * stepping on the instruction on a vmalloced/kmalloced/data page

 * is a recipe for disaster

 Page of instruction slots */

	/*

	 * Use module_alloc() so this page is within +/- 2GB of where the

	 * kernel image and loaded module images reside. This is required

	 * for most of the architectures.

	 * (e.g. x86-64 needs this to handle the %rip-relative fixups.)

/**

 * __get_insn_slot() - Find a slot on an executable page for an instruction.

 * We allocate an executable page if there's no room on existing ones.

 Since the slot array is not protected by rcu, we need a mutex */

 kip->nused is broken. Fix it. */

 If there are any garbage slots, collect it and try again. */

 All out of space.  Need to allocate a new page. */

 Record the perf ksymbol register event after adding the page */

 Return true if all garbages are collected, otherwise false. */

		/*

		 * Page is no longer in use.  Free it unless

		 * it's the last one.  We keep the last one

		 * so as not to have to set it up again the

		 * next time somebody inserts a probe.

			/*

			 * Record perf ksymbol unregister event before removing

			 * the page.

 Ensure no-one is interrupted on the garbages */

 we will collect all garbages */

 Could not find this slot. */

 Mark and sweep: this may sleep */

 Check double free */

/*

 * Check given address is on the page of kprobe instruction slots.

 * This will be used for checking whether the address on a stack

 * is on a text area or not.

 For optimized_kprobe buffer */

 .insn_size is initialized later */

 We have preemption disabled.. so it is safe to use __ versions */

/*

 * This routine is called either:

 *	- under the 'kprobe_mutex' - during kprobe_[un]register().

 *				OR

 *	- with preemption disabled - from architecture specific code.

 Return true if 'p' is an aggregator */

 Return true if 'p' is unused */

 Keep all fields in the kprobe consistent. */

 NOTE: This is protected by 'kprobe_mutex'. */

/*

 * Call all 'kprobe::pre_handler' on the list, but ignores its return value.

 * This must be called from arch-dep optimized caller.

 Free optimized instructions and optimized_kprobe */

 Return true if the kprobe is ready for optimization. */

 Return true if the kprobe is disarmed. Note: p must be on hash list */

 If kprobe is not aggr/opt probe, just return kprobe is disabled */

 Return true if the probe is queued on (un)optimizing lists */

/*

 * Return an optimized kprobe whose optimizing code replaces

 * instructions including 'addr' (exclude breakpoint).

 Don't check i == 0, since that is a breakpoint case. */

 Optimization staging list, protected by 'kprobe_mutex' */

/*

 * Optimize (replace a breakpoint with a jump) kprobes listed on

 * 'optimizing_list'.

	/*

	 * The optimization/unoptimization refers 'online_cpus' via

	 * stop_machine() and cpu-hotplug modifies the 'online_cpus'.

	 * And same time, 'text_mutex' will be held in cpu-hotplug and here.

	 * This combination can cause a deadlock (cpu-hotplug tries to lock

	 * 'text_mutex' but stop_machine() can not be done because

	 * the 'online_cpus' has been changed)

	 * To avoid this deadlock, caller must have locked cpu-hotplug

	 * for preventing cpu-hotplug outside of 'text_mutex' locking.

 Optimization never be done when disarmed */

/*

 * Unoptimize (replace a jump with a breakpoint and remove the breakpoint

 * if need) kprobes listed on 'unoptimizing_list'.

 See comment in do_optimize_kprobes() */

 Unoptimization must be done anytime */

 Loop on 'freeing_list' for disarming */

 Switching from detour code to origin */

 Disarm probes if marked disabled */

			/*

			 * Remove unused probes from hash list. After waiting

			 * for synchronization, these probes are reclaimed.

			 * (reclaiming is done by do_free_cleaned_kprobes().)

 Reclaim all kprobes on the 'freeing_list' */

			/*

			 * This must not happen, but if there is a kprobe

			 * still in use, keep it on kprobes hash list.

 Start optimizer after OPTIMIZE_DELAY passed */

 Kprobe jump optimizer */

	/*

	 * Step 1: Unoptimize kprobes and collect cleaned (unused and disarmed)

	 * kprobes before waiting for quiesence period.

	/*

	 * Step 2: Wait for quiesence period to ensure all potentially

	 * preempted tasks to have normally scheduled. Because optprobe

	 * may modify multiple instructions, there is a chance that Nth

	 * instruction is preempted. In that case, such tasks can return

	 * to 2nd-Nth byte of jump instruction. This wait is for avoiding it.

	 * Note that on non-preemptive kernel, this is transparently converted

	 * to synchronoze_sched() to wait for all interrupts to have completed.

 Step 3: Optimize kprobes after quiesence period */

 Step 4: Free cleaned kprobes after quiesence period */

 Step 5: Kick optimizer again if needed */

 Wait for completing optimization and unoptimization */

 This will also make 'optimizing_work' execute immmediately */

 'optimizing_work' might not have been queued yet, relax */

 Optimize kprobe if p is ready to be optimized */

 Check if the kprobe is disabled or not ready for optimization. */

 kprobes with 'post_handler' can not be optimized */

 Check there is no other kprobes at the optimized instructions */

 Check if it is already optimized. */

 This is under unoptimizing. Just dequeue the probe */

	/*

	 * On the 'unoptimizing_list' and 'optimizing_list',

	 * 'op' must have OPTIMIZED flag

 Short cut to direct unoptimizing */

 Unoptimize a kprobe if p is optimized */

 This is not an optprobe nor optimized */

 Queued in unoptimizing queue */

				/*

				 * Forcibly unoptimize the kprobe here, and queue it

				 * in the freeing list for release afterwards.

 Dequeue from the optimizing queue */

 Optimized kprobe case */

 Forcibly update the code: this is a special case */

 Cancel unoptimizing for reusing */

	/*

	 * Unused kprobe MUST be on the way of delayed unoptimizing (means

	 * there is still a relative jump) and disabled.

 Enable the probe again */

 Optimize it again. (remove from 'op->list') */

 Remove optimized instructions */

 Dequeue from the (un)optimization queue */

 Enqueue if it is unused */

		/*

		 * Remove unused probes from the hash list. After waiting

		 * for synchronization, this probe is reclaimed.

		 * (reclaiming is done by do_free_cleaned_kprobes().)

 Don't touch the code, because it is already freed. */

 Try to prepare optimized instructions */

 Allocate new optimized_kprobe and try to prepare optimized instructions. */

/*

 * Prepare an optimized_kprobe and optimize it.

 * NOTE: 'p' must be a normal registered kprobe.

 Impossible to optimize ftrace-based kprobe. */

 For preparing optimization, jump_label_text_reserved() is called. */

 If failed to setup optimizing, fallback to kprobe. */

 This just kicks optimizer thread. */

 If optimization is already allowed, just return. */

 If optimization is already prohibited, just return. */

 Wait for unoptimizing completion. */

 CONFIG_SYSCTL */

 Put a breakpoint for a probe. */

 Find the overlapping optimized kprobes. */

 Fallback to unoptimized kprobe */

 Try to optimize (add kprobe to a list) */

 Remove the breakpoint of a probe. */

 Try to unoptimize */

 If another kprobe was blocked, re-optimize it. */

	/*

	 * TODO: Since unoptimization and real disarming will be done by

	 * the worker thread, we can not check whether another probe are

	 * unoptimized because of this probe here. It should be re-optimized

	 * by the worker thread.

 !CONFIG_OPTPROBES */

	/*

	 * If the optimized kprobe is NOT supported, the aggr kprobe is

	 * released at the same time that the last aggregated kprobe is

	 * unregistered.

	 * Thus there should be no chance to reuse unused kprobe.

 CONFIG_OPTPROBES */

	/*

	 * At this point, sinec ops is not registered, we should be sefe from

	 * registering empty filter.

 !CONFIG_KPROBES_ON_FTRACE */

 Must ensure p->addr is really on ftrace */

/*

 * Aggregate handlers for multiple kprobes support - these handlers

 * take care of invoking the individual kprobe handlers on p->list

 Walks the list and increments 'nmissed' if 'p' has child probes. */

/*

 * This function is called from delayed_put_task_struct() when a task is

 * dead and cleaned up to recycle any kretprobe instances associated with

 * this task. These left over instances represent probed functions that

 * have been called but will never return.

 Early boot, not yet initialized. */

 Add the new probe to 'ap->list'. */

 Fall back to normal kprobe */

/*

 * Fill in the required fields of the aggregator kprobe. Replace the

 * earlier kprobe in the hlist with the aggregator kprobe.

 Copy the insn slot of 'p' to 'ap'. */

 We don't care the kprobe which has gone. */

/*

 * This registers the second or subsequent kprobe at the same address.

 For preparing optimization, jump_label_text_reserved() is called */

 If 'orig_p' is not an 'aggr_kprobe', create new one. */

 This probe is going to die. Rescue it */

		/*

		 * Attempting to insert new probe at the same location that

		 * had a probe in the module vaddr area which already

		 * freed. So, the instruction slot has already been

		 * released. We need a new slot for the new probe.

			/*

			 * Even if fail to allocate new slot, don't need to

			 * free the 'ap'. It will be used next time, or

			 * freed by unregister_kprobe().

 Prepare optimized instructions if possible. */

		/*

		 * Clear gone flag to prevent allocating new slot again, and

		 * set disabled flag because it is not armed yet.

 Copy the insn slot of 'p' to 'ap'. */

 Arm the breakpoint again. */

 The '__kprobes' functions and entry code must not be probed. */

	/*

	 * If 'kprobe_blacklist' is defined, check the address and

	 * reject any probe registration in the prohibited area.

 Check if the address is on a suffixed-symbol */

/*

 * If 'symbol_name' is specified, look it up and add the 'offset'

 * to it. This way, we can specify a relative address to a symbol.

 * This returns encoded errors if it fails to look up symbol or invalid

 * combination of parameters.

/*

 * Check the 'p' is valid and return the aggregator kprobe

 * at the same address.

 kprobe p is a valid probe */

/*

 * Warn and return error if the kprobe is being re-registered since

 * there must be a software bug.

 Given address is not on the instruction boundary */

 !CONFIG_KPROBES_ON_FTRACE */

 Ensure it is not in reserved area nor out of text */

 Check if 'p' is probing a module. */

		/*

		 * We must hold a refcount of the probed module while updating

		 * its code to prohibit unexpected unloading.

		/*

		 * If the module freed '.init.text', we couldn't insert

		 * kprobes in there.

 Adjust probe address from symbol */

 User can pass only KPROBE_FLAG_DISABLED to register_kprobe */

 Since this may unoptimize 'old_p', locking 'text_mutex'. */

 Prevent text modification */

 Try to optimize kprobe */

 Check if all probes on the 'ap' are disabled. */

			/*

			 * Since there is an active probe on the list,

			 * we can't disable this 'ap'.

 Get an original kprobe for return */

 Disable probe if it is a child probe */

 Try to disarm and disable this/parent probe */

			/*

			 * If 'kprobes_all_disarmed' is set, 'orig_p'

			 * should have already been disarmed, so

			 * skip unneed disarming process.

/*

 * Unregister a kprobe without a scheduler synchronization.

 Disable kprobe. This will disarm it if needed. */

		/*

		 * This probe is an independent(and non-optimized) kprobe

		 * (not an aggrprobe). Remove from the hash list.

 Following process expects this probe is an aggrprobe */

		/*

		 * !disarmed could be happen if the probe is under delayed

		 * unoptimizing.

 If disabling probe has special handlers, update aggrprobe */

		/*

		 * Remove from the aggrprobe: this path will do nothing in

		 * __unregister_kprobe_bottom().

			/*

			 * Try to optimize this probe again, because post

			 * handler may have been changed.

 This is an independent kprobe */

 This is the last child of an aggrprobe */

 Otherwise, do nothing. */

 we need to be notified first */

 This assumes the 'tsk' is the current task or the is not running. */

/**

 * kretprobe_find_ret_addr -- Find correct return address modified by kretprobe

 * @tsk: Target task

 * @fp: A frame pointer

 * @cur: a storage of the loop cursor llist_node pointer for next call

 *

 * Find the correct return address modified by a kretprobe on @tsk in unsigned

 * long type. If it finds the return address, this returns that address value,

 * or this returns 0.

 * The @tsk must be 'current' or a task which is not running. @fp is a hint

 * to get the currect return address - which is compared with the

 * kretprobe_instance::fp field. The @cur is a loop cursor for searching the

 * kretprobe return addresses on the @tsk. The '*@cur' should be NULL at the

 * first call, but '@cur' itself must NOT NULL.

	/*

	 * Do nothing by default. Please fill this to update the fake return

	 * address on the stack with the correct one on each arch if possible.

 Find correct address and all nodes for this frame. */

	/*

	 * Set the return address as the instruction pointer, because if the

	 * user handler calls stack_trace_save_regs() with this 'regs',

	 * the stack trace will start from the instruction pointer.

 Run the user handler of the nodes. */

 Unlink all nodes for this frame. */

 Recycle free instances. */

/*

 * This kprobe pre_handler is registered with every kretprobe. When probe

 * hits it will set up the return probe.

/**

 * kprobe_on_func_entry() -- check whether given address is function entry

 * @addr: Target address

 * @sym:  Target symbol name

 * @offset: The offset from the symbol or the address

 *

 * This checks whether the given @addr+@offset or @sym+@offset is on the

 * function entry address or not.

 * This returns 0 if it is the function entry, or -EINVAL if it is not.

 * And also it returns -ENOENT if it fails the symbol or address lookup.

 * Caller must pass @addr or @sym (either one must be NULL), or this

 * returns -EINVAL.

 If only 'rp->kp.addr' is specified, check reregistering kprobes */

 Pre-allocate memory for max kretprobe instances */

 Establish function entry probe point */

 CONFIG_KRETPROBES */

 CONFIG_KRETPROBES */

 Set the kprobe gone and remove its instruction buffer. */

		/*

		 * If this is an aggr_kprobe, we have to list all the

		 * chained probes and mark them GONE.

	/*

	 * Here, we can remove insn_slot safely, because no thread calls

	 * the original probed function (which will be freed soon) any more.

	/*

	 * The module is going away. We should disarm the kprobe which

	 * is using ftrace, because ftrace framework is still available at

	 * 'MODULE_STATE_GOING' notification.

 Disable one kprobe */

 Disable this kprobe */

 Enable one kprobe */

 Check whether specified probe is valid. */

 This kprobe has gone, we couldn't enable it. */

 Caller must NOT call this in usual path. This is only for critical case */

 Add all symbols in given area into kprobe blacklist */

 In case of alias symbol */

 Remove all symbols in given area from kprobe blacklist */

/*

 * Lookup and populate the kprobe_blacklist.

 *

 * Unlike the kretprobe blacklist, we'll need to determine

 * the range of addresses that belong to the said functions,

 * since a kprobe need not necessarily be at the beginning

 * of a function.

 Symbols in '__kprobes_text' are blacklisted */

 Symbols in 'noinstr' section are blacklisted */

 Module notifier call back, checking kprobes on the module */

	/*

	 * When 'MODULE_STATE_GOING' was notified, both of module '.text' and

	 * '.init.text' sections would be freed. When 'MODULE_STATE_LIVE' was

	 * notified, only '.init.text' section would be freed. We need to

	 * disable kprobes which have been inserted in the sections.

				/*

				 * The vaddr this probe is installed will soon

				 * be vfreed buy not synced to disk. Hence,

				 * disarming the breakpoint isn't needed.

				 *

				 * Note, this will also move any optimized probes

				 * that are pending to be removed from their

				 * corresponding lists to the 'freeing_list' and

				 * will not be touched by the delayed

				 * kprobe_optimizer() work handler.

 Kill all kprobes on initmem because the target code has been freed. */

 FIXME allocate the probe table, currently defined statically */

 initialize all list heads */

 lookup the function address from its name */

 By default, kprobes are armed */

 Init 'kprobe_optinsn_slots' for allocation */

	/*

	 * Enable kprobe optimization - this kicks the optimizer which

	 * depends on synchronize_rcu_tasks() and ksoftirqd, that is

	 * not spawned in early initcall. So delay the optimization.

 try to use %pS */

 Nothing to do */

 kprobes/blacklist -- shows which functions can not be probed */

	/*

	 * If '/proc/kallsyms' is not showing kernel address, we won't

	 * show them here either.

 If kprobes are armed, just return */

	/*

	 * optimize_kprobe() called by arm_kprobe() checks

	 * kprobes_all_disarmed, so set kprobes_all_disarmed before

	 * arm_kprobe.

 Arming kprobes doesn't optimize kprobe itself */

 Arm all kprobes on a best-effort basis */

 If kprobes are already disarmed, just return */

 Disarm all kprobes on a best-effort basis */

 Wait for disarming all kprobes by optimizer */

/*

 * XXX: The debugfs bool file interface doesn't allow for callbacks

 * when the bool state is switched. We can reuse that facility when

 * available

 CONFIG_DEBUG_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Google, Inc.

 *

 * Author:

 *	Colin Cross <ccross@android.com>

/*

 * atomic_notifiers use a spinlock_t, which can block under PREEMPT_RT.

 * Notifications for cpu_pm will be issued by the idle task itself, which can

 * never block, IOW it requires using a raw_spinlock_t.

	/*

	 * This introduces a RCU read critical section, which could be

	 * disfunctional in cpu idle. Copy RCU_NONIDLE code to let RCU know

	 * this.

/**

 * cpu_pm_register_notifier - register a driver with cpu_pm

 * @nb: notifier block to register

 *

 * Add a driver to a list of drivers that are notified about

 * CPU and CPU cluster low power entry and exit.

 *

 * This function has the same return conditions as raw_notifier_chain_register.

/**

 * cpu_pm_unregister_notifier - unregister a driver with cpu_pm

 * @nb: notifier block to be unregistered

 *

 * Remove a driver from the CPU PM notifier list.

 *

 * This function has the same return conditions as raw_notifier_chain_unregister.

/**

 * cpu_pm_enter - CPU low power entry notifier

 *

 * Notifies listeners that a single CPU is entering a low power state that may

 * cause some blocks in the same power domain as the cpu to reset.

 *

 * Must be called on the affected CPU with interrupts disabled.  Platform is

 * responsible for ensuring that cpu_pm_enter is not called twice on the same

 * CPU before cpu_pm_exit is called. Notified drivers can include VFP

 * co-processor, interrupt controller and its PM extensions, local CPU

 * timers context save/restore which shouldn't be interrupted. Hence it

 * must be called with interrupts disabled.

 *

 * Return conditions are same as __raw_notifier_call_chain.

/**

 * cpu_pm_exit - CPU low power exit notifier

 *

 * Notifies listeners that a single CPU is exiting a low power state that may

 * have caused some blocks in the same power domain as the cpu to reset.

 *

 * Notified drivers can include VFP co-processor, interrupt controller

 * and its PM extensions, local CPU timers context save/restore which

 * shouldn't be interrupted. Hence it must be called with interrupts disabled.

 *

 * Return conditions are same as __raw_notifier_call_chain.

/**

 * cpu_cluster_pm_enter - CPU cluster low power entry notifier

 *

 * Notifies listeners that all cpus in a power domain are entering a low power

 * state that may cause some blocks in the same power domain to reset.

 *

 * Must be called after cpu_pm_enter has been called on all cpus in the power

 * domain, and before cpu_pm_exit has been called on any cpu in the power

 * domain. Notified drivers can include VFP co-processor, interrupt controller

 * and its PM extensions, local CPU timers context save/restore which

 * shouldn't be interrupted. Hence it must be called with interrupts disabled.

 *

 * Must be called with interrupts disabled.

 *

 * Return conditions are same as __raw_notifier_call_chain.

/**

 * cpu_cluster_pm_exit - CPU cluster low power exit notifier

 *

 * Notifies listeners that all cpus in a power domain are exiting form a

 * low power state that may have caused some blocks in the same power domain

 * to reset.

 *

 * Must be called after cpu_cluster_pm_enter has been called for the power

 * domain, and before cpu_pm_exit has been called on any cpu in the power

 * domain. Notified drivers can include VFP co-processor, interrupt controller

 * and its PM extensions, local CPU timers context save/restore which

 * shouldn't be interrupted. Hence it must be called with interrupts disabled.

 *

 * Return conditions are same as __raw_notifier_call_chain.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Test cases for API provided by resource.c and ioport.h

 SPDX-License-Identifier: GPL-2.0-only

/*

 * stores the physical address of elf header of crash image

 *

 * Note: elfcorehdr_addr is not just limited to vmcore. It is also used by

 * is_kdump_kernel() to determine if we are booting after a panic. Hence put

 * it under CONFIG_CRASH_DUMP and not CONFIG_PROC_VMCORE.

/*

 * stores the size of elf header of crash image

/*

 * elfcorehdr= specifies the location of elf core header stored by the crashed

 * kernel. This option will be passed by kexec loader to the capture kernel.

 *

 * Syntax: elfcorehdr=[size[KMG]@]offset[KMG]

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2002 Richard Henderson

 * Copyright (C) 2001 Rusty Russell, 2002, 2010 Rusty Russell IBM.

/*

 * Modules' sections will be aligned on page boundaries

 * to ensure complete separation of code and data, but

 * only when CONFIG_ARCH_HAS_STRICT_MODULE_RWX=y

 If this is set, the section belongs in the init part of the module */

/*

 * Mutex protects:

 * 1) List of modules (also safely readable with preempt_disable),

 * 2) module_use links,

 * 3) module_addr_min/module_addr_max.

 * (delete and add uses RCU list operations).

 Work queue for freeing init sections in success case */

/*

 * Use a latched RB-tree for __module_address(); this allows us to use

 * RCU-sched lookups of the address from any context.

 *

 * This is conditional on PERF_EVENTS || TRACING because those can really hit

 * __module_address() hard by doing a lot of stack unwinding; potentially from

 * NMI context.

/*

 * These modifications: insert, remove_init and remove; are serialized by the

 * module_mutex.

 MODULES_TREE_LOOKUP */

 MODULES_TREE_LOOKUP */

/*

 * Bounds of module text, for speeding up __module_address.

 * Protected by module_mutex.

 kdb needs the list of modules */

 CONFIG_KGDB_KDB */

/*

 * Export sig_enforce kernel cmdline parameter to allow other subsystems rely

 * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.

 Block module loading/unloading? */

 Waiting for a module to finish initializing? */

/*

 * We require a truly strong try_module_get(): 0 means success.

 * Otherwise an error is returned due to ongoing or failed

 * initialization etc.

/*

 * A thread that wants to hold a reference to a module only while it

 * is running can call this to safely exit.  nfsd and lockd use this.

 Find a module section: 0 means not found. */

 Alloc bit cleared means "ignore it." */

 Find a module section, or NULL. */

 Section 0 has sh_addr 0. */

 Find a module section, or NULL.  Fill in number of "objects" in section. */

 Section 0 has sh_addr 0 and sh_size 0. */

 Find a module section: 0 means not found. Ignores SHF_ALLOC flag. */

/*

 * Find a module section, or NULL. Fill in number of "objects" in section.

 * Ignores SHF_ALLOC flag.

 Section 0 has sh_addr 0 and sh_size 0. */

 Provided by the linker */

 Input */

 Output */

/*

 * Find an exported symbol and return it, along with, (optional) crc and

 * (optional) module which owns it.  Needs preempt disabled or module_mutex.

/*

 * Search for module by name: must hold module_mutex (or preempt disabled

 * for read-only access).

/**

 * is_module_percpu_address() - test whether address is from module static percpu

 * @addr: address to test

 *

 * Test whether @addr belongs to module static percpu area.

 *

 * Return: %true if @addr is from module static percpu area

 ... !CONFIG_SMP */

 UP modules shouldn't have this section: ENOMEM isn't quite right */

 pcpusec should be 0, and size of that section should be 0. */

 CONFIG_SMP */

 MODULE_REF_BASE is the base reference count by kmodule loader. */

 Init the unload section of the module. */

	/*

	 * Initialize reference counter to MODULE_REF_BASE.

	 * refcnt == 0 means module is going.

 Hold reference count during initialization. */

 Does a already use b? */

/*

 * Module a uses b

 *  - we add 'a' as a "source", 'b' as a "target" of module use

 *  - the module_use is added to the list of 'b' sources (so

 *    'b' can walk the list to see who sourced them), and of 'a'

 *    targets (so 'a' can see what modules it targets).

 Module a uses b: caller needs module_mutex() */

 If module isn't available, we fail. */

 Clear the unload stuff of the module. */

 CONFIG_MODULE_FORCE_UNLOAD */

 Try to release refcount of module, 0 means success. */

 Try to decrement refcnt which we set at loading */

 Someone can put this right now, recover with checking */

 If it's not unused, quit unless we're forcing. */

 Mark it as dying. */

/**

 * module_refcount() - return the refcount or -1 if unloading

 * @mod:	the module we're checking

 *

 * Return:

 *	-1 if the module is in the process of unloading

 *	otherwise the number of references in the kernel to the module

 This exists whether we can unload or not */

 Other modules depend on us: get rid of them first. */

 Doing init or already dying? */

 FIXME: if (force), slam module count damn the torpedoes */

 If it has an init func, it must have an exit func to unload */

 This module can't be removed */

 Stop the machine so refcounts can't move and disable module. */

 Final destruction now no one is using it. */

 Store the name of the last unloaded module for diagnostic purposes */

 someone could wait for the module in add_unformed_module() */

	/*

	 * Always include a trailing , so userspace can differentiate

	 * between this and the old multi-field proc format.

 Note this assumes addr is a function, which it currently always is. */

	/*

	 * Even though we hold a reference on the module; we still need to

	 * disable preemption in order to safely traverse the data structure.

 Note: here, we can fail to get a reference */

 Failed to put refcount */

 !CONFIG_MODULE_UNLOAD */

 We don't know the usage count, or what modules are using. */

 CONFIG_MODULE_UNLOAD */

 Exporting module didn't supply crcs?  OK, we're already tainted. */

 No versions at all?  modprobe --force does this. */

 Broken toolchain. Warn once, then let it go.. */

	/*

	 * Since this should be found in kernel (which can't be removed), no

	 * locking is necessary -- use preempt_disable() to placate lockdep.

 First part is kernel version, which we ignore if module has crcs. */

 CONFIG_MODVERSIONS */

 Resolve a symbol for this module.  I.e. if we find one, record usage. */

	/*

	 * The module_mutex should not be a heavily contended lock;

	 * if we get the occasional sleep here, we'll go an extra iteration

	 * in the wait_event_interruptible(), which is harmless.

 We must make copy under the lock if we failed to get ref. */

/*

 * /sys/module/foo/sections stuff

 * J. Corbet <corbet@lwn.net>

 "0x", "\n" */ + (BITS_PER_LONG / 4))

	/*

	 * Since we're a binary read handler, we must account for the

	 * trailing NUL byte that sprintf will write: if "buf" is

	 * too small to hold the NUL, or the NUL is exactly the last

	 * byte, the read will look like it got truncated by one byte.

	 * Since there is no way to ask sprintf nicely to not write

	 * the NUL, we have to use a bounce buffer.

 Count loaded sections and allocate structures */

 Setup section attributes. */

		/*

		 * We are positive that no one is using any sect attrs

		 * at this point.  Deallocate immediately.

/*

 * /sys/module/foo/notes/.section.name gives contents of SHT_NOTE sections.

	/*

	 * The caller checked the pos and count against our size.

 failed to create section attributes, so can't create notes */

 Count notes sections and allocate structures.  */

 CONFIG_KALLSYMS */

 pick a field to test for end of list */

 !CONFIG_SYSFS */

 CONFIG_SYSFS */

/*

 * LKM RO/NX protection: protect module's text/ro-data

 * from modification and any data from execution.

 *

 * General layout of module is:

 *          [text] [read-only-data] [ro-after-init] [writable data]

 * text_size -----^                ^               ^               ^

 * ro_size ------------------------|               |               |

 * ro_after_init_size -----------------------------|               |

 * size -----------------------------------------------------------|

 *

 * These values are always page-aligned (as is base)

/*

 * Since some arches are moving towards PAGE_KERNEL module allocations instead

 * of PAGE_KERNEL_EXEC, keep frob_text() and module_enable_x() outside of the

 * CONFIG_STRICT_MODULE_RWX block below because they are needed regardless of

 * whether we are strict.

 !CONFIG_ARCH_HAS_STRICT_MODULE_RWX */

 CONFIG_ARCH_HAS_STRICT_MODULE_RWX */

 !CONFIG_STRICT_MODULE_RWX */

  CONFIG_STRICT_MODULE_RWX */

/*

 * Persist Elf information about a module. Copy the Elf header,

 * section header table, section string table, and symtab section

 * index from info to mod->klp_info.

 Elf header */

 Elf section header table */

 Elf section name string table */

 Elf symbol section index */

	/*

	 * For livepatch modules, core_kallsyms.symtab is a complete

	 * copy of the original symbol table. Adjust sh_addr to point

	 * to core_kallsyms.symtab since the copy of the symtab in module

	 * init memory is freed at the end of do_init_module().

 !CONFIG_LIVEPATCH */

 CONFIG_LIVEPATCH */

	/*

	 * This memory may be RO, and freeing RO memory in an interrupt is not

	 * supported by vmalloc.

 Free a module, remove from lists, etc. */

	/*

	 * We leave it in list to prevent duplicate loads, but make sure

	 * that noone uses it while it's being deconstructed.

 Remove dynamic debug info */

 Arch-specific cleanup. */

 Module unload stuff */

 Free any allocated parameters. */

 Now we can delete it from the lists */

 Unlink carefully: kallsyms could be walking list. */

 Remove this module from bug list, this uses list_del_rcu */

 Wait for RCU-sched synchronizing before releasing mod->list and buglist. */

 Clean up CFI for the module. */

 This may be empty, but that's OK */

 Free lock-classes; relies on the preceding sync_rcu(). */

 Finally, free the core (containing the module structure) */

/*

 * Ensure that an exported symbol [global namespace] does not already exist

 * in the kernel or in some other module's exported symbol table.

 *

 * You must hold the module_mutex.

	/*

	 * On x86, PIC code and Clang non-PIC code may have call foo@PLT. GNU as

	 * before 2.37 produces an unreferenced _GLOBAL_OFFSET_TABLE_ on x86-64.

	 * i386 has a similar problem but may not deserve a fix.

	 *

	 * If we ever have to ignore many symbols, consider refactoring the code to

	 * only warn if referenced by a relocation.

 Change all symbols so that st_value encodes the pointer directly. */

 Ignore common symbols */

			/*

			 * We compiled with -fno-common.  These are not

			 * supposed to happen.

 Don't need to do anything */

 Livepatch symbols are resolved by livepatch */

 Ok if resolved.  */

 Ok if weak or ignored.  */

 Divert to percpu allocation if a percpu var. */

 Now do relocations. */

 Not a valid relocation section? */

 Don't bother with non-allocated sections */

 Additional bytes needed by arch in front of individual sections */

 default implementation just returns zero */

 Update size with this section: return offset. */

/*

 * Lay out the SHF_ALLOC sections in a way not dissimilar to how ld

 * might -- code, read-only data, read-write data, small data.  Tally

 * sizes, and place the offsets into sh_entsize fields: high bit means it

 * belongs in init.

		/*

		 * NOTE: all executable code must be the first section

		 * in this array; otherwise modify the text_size

		 * finder in the two loops below

 executable */

 RO: text and ro-data */

 RO after init */

 whole core */

 executable */

 RO: text and ro-data */

			/*

			 * RO after init doesn't apply to init_layout (only

			 * core_layout), so it just takes the value of ro_size.

 whole init */

 Parse tag=value strings from .modinfo section */

 Skip non-zero chars */

 Skip any zero padding. */

	/*

	 * get_modinfo() calls made before rewrite_section_headers()

	 * must use sh_offset, as sh_addr isn't set!

 Lookup exported symbol in given range of kernel_symbols */

 As per nm */

/*

 * We only allocate and copy the strings needed by the parts of symtab

 * we keep.  This is simple, but has the effect of making multiple

 * copies of duplicates.  We could be more sophisticated, see

 * linux-kernel thread starting with

 * <73defb5e4bca04a6431392cc341112b1@localhost>.

 Put symbol section at end of init part of module. */

 Compute total space required for the core symbols' strtab. */

 Append room for core symbols at end of core part. */

 Put string table section at end of init part of module. */

 We'll tack temporary mod_kallsyms on the end. */

/*

 * We use the full symtab and strtab which layout_symtab arranged to

 * be appended to the init section.  Later we switch to the cut-down

 * core-only ones.

 Set up to point into init section. */

 Make sure we get permanent strtab: don't use info->strtab. */

	/*

	 * Now populate the cut down core kallsyms for after init

	 * and set types up while we still have access to sections.

 CONFIG_KALLSYMS */

 only scan the sections containing data */

 Scan all writable sections that's not executable */

	/*

	 * Require flags == 0, as a module with version information

	 * removed is no longer the module that was signed

 We truncate the module to discard the signature */

	/*

	 * We don't permit modules to be loaded into the trusted kernels

	 * without a valid signature on them, but if we're not enforcing,

	 * certain errors are non-fatal.

		/*

		 * All other errors are fatal, including lack of memory,

		 * unparseable signatures, and signature check failures --

		 * even if signatures aren't required.

 !CONFIG_MODULE_SIG */

 !CONFIG_MODULE_SIG */

	/*

	 * Check for both overflow and offset/size being

	 * too large.

/*

 * Sanity checks against invalid binaries, wrong arch, weird elf version.

 *

 * Also do basic validity checks against section offsets and sizes, the

 * section name string table, and the indices used for it (sh_name).

	/*

	 * e_shnum is 16 bits, and sizeof(Elf_Shdr) is

	 * known and small. So e_shnum * sizeof(Elf_Shdr)

	 * will not overflow unsigned long on any platform.

	/*

	 * Verify if the section name table index is valid.

	/*

	 * The section name table must be NUL-terminated, as required

	 * by the spec. This makes strcmp and pr_* calls that access

	 * strings in the section safe.

	/*

	 * The code assumes that section 0 has a length of zero and

	 * an addr of zero, so check for it.

 !CONFIG_LIVEPATCH */

 CONFIG_LIVEPATCH */

 Sets info->hdr and info->len. */

 Suck in entire file: we'll want most of it. */

 This should always be true, but let's be sure. */

		/*

		 * Mark all sections sh_addr with their address in the

		 * temporary image.

 Track but don't keep modinfo and version sections. */

/*

 * Set up our basic convenience variables (pointers to section headers,

 * search for module section index etc), and do some basic section

 * verification.

 *

 * Set info->mod to the temporary copy of the module in info->hdr. The final one

 * will be allocated in move_module().

 Try to find a name early so we can log errors with a module name */

 Find internal symbols and strings. */

 This is temporary: point mod into copy of data. */

	/*

	 * If we didn't load the .modinfo 'name' field earlier, fall back to

	 * on-disk struct mod 'name' field.

 Pretend no __versions section! */

 This is allowed: modprobe --force will invalidate it. */

 Set up license info based on the info section */

		/*

		 * This shouldn't happen with same compiler and binutils

		 * building all parts of the module.

 sechdrs[0].sh_size is always zero */

 Do the allocs. */

	/*

	 * The pointer to this block is stored in the module structure

	 * which is inside the block. Just mark it as not being a

	 * leak.

		/*

		 * The pointer to this block is stored in the module structure

		 * which is inside the block. This block doesn't need to be

		 * scanned as it contains data and code that will be freed

		 * after the module is initialized.

 Transfer each section which specifies SHF_ALLOC */

 Update sh_addr to point to copy in image. */

	/*

	 * ndiswrapper is under GPL by itself, but loads proprietary modules.

	 * Don't use add_taint_module(), as it would prevent ndiswrapper from

	 * using GPL-only symbols it needs.

 driverloader was caught wrongly pretending to be under GPL */

 lve claims to be GPL but upstream won't provide source */

	/*

	 * Flush the instruction cache, since we've played with text.

	 * Do it before processing of module parameters, so the module

	 * can provide parameter accessor functions of its own.

 module_blacklist is a comma-separated list of module names */

 Allow arches to frob section contents and sizes.  */

 We will do a special allocation for per-cpu sections later. */

	/*

	 * Mark ro_after_init section with SHF_RO_AFTER_INIT so that

	 * layout_sections() can put it in the right place.

	 * Note: ro_after_init sections also have SHF_{WRITE,ALLOC} set.

	/*

	 * Mark the __jump_table section as ro_after_init as well: these data

	 * structures are never modified, with the exception of entries that

	 * refer to code in the __init section, which are annotated as such

	 * at module load time.

	/*

	 * Determine total sizes, and put offsets in sh_entsize.  For now

	 * this is done generically; there doesn't appear to be any

	 * special cases for the architectures.

 Allocate and move to the final place */

 Module has been copied to its final place now: return it. */

 mod is no longer valid after this! */

 Sort exception table now relocations are done. */

 Copy relocated percpu area over. */

 Setup kallsyms-specific fields. */

 Arch-specific module finalizing. */

 Is this module of this name done loading?  No locks held. */

	/*

	 * The module_mutex should not be a heavily contended lock;

	 * if we get the occasional sleep here, we'll go an extra iteration

	 * in the wait_event_interruptible(), which is harmless.

 Call module constructors. */

 For freeing module_init on success, in case kallsyms traversing */

/*

 * This is where the real work happens.

 *

 * Keep it uninlined to provide a reliable breakpoint target, e.g. for the gdb

 * helper command 'lx-symbols'.

	/*

	 * We want to find out whether @mod uses async during init.  Clear

	 * PF_USED_ASYNC.  async_schedule*() will set it.

 Start the module */

 Now it's a first class citizen! */

 Delay uevent until module has finished its init routine */

	/*

	 * We need to finish all async code before the module init sequence

	 * is done.  This has potential to deadlock.  For example, a newly

	 * detected block device can trigger request_module() of the

	 * default iosched from async probing task.  Once userland helper

	 * reaches here, async_synchronize_full() will wait on the async

	 * task waiting on request_module() and deadlock.

	 *

	 * This deadlock is avoided by perfomring async_synchronize_full()

	 * iff module init queued any async jobs.  This isn't a full

	 * solution as it will deadlock the same if module loading from

	 * async jobs nests more than once; however, due to the various

	 * constraints, this hack seems to be the best option for now.

	 * Please refer to the following thread for details.

	 *

	 * http://thread.gmane.org/gmane.linux.kernel/1420814

 Drop initial reference. */

 Switch to core kallsyms now init is done: kallsyms may be walking! */

 .BTF is not SHF_ALLOC and will get removed, so sanitize pointer */

	/*

	 * We want to free module_init, but be aware that kallsyms may be

	 * walking this with preempt disabled.  In all the failure paths, we

	 * call synchronize_rcu(), but we don't want to slow down the success

	 * path. module_memfree() cannot be called in an interrupt, so do the

	 * work and call synchronize_rcu() in a work queue.

	 *

	 * Note that module_alloc() on most architectures creates W+X page

	 * mappings which won't be cleaned up until do_free_init() runs.  Any

	 * code such as mark_rodata_ro() which depends on those mappings to

	 * be cleaned up needs to sync with the queued work - ie

	 * rcu_barrier()

 Try to protect us from buggy refcounters. */

/*

 * We try to place it in the list now to make sure it's unique before

 * we dedicate too many resources.  In particular, temporary percpu

 * memory exhaustion.

 Wait in case it fails to load. */

 Find duplicate symbols (must be called under lock). */

 This relies on module_mutex for list integrity. */

	/*

	 * Mark state as coming so strong_try_module_get() ignores us,

	 * but kallsyms etc. can see us.

 Check for magic 'dyndbg' arg */

/*

 * Allocate and load the module: note that size of section 0 is always

 * zero, and we rely on this for optional sections.

	/*

	 * Do the signature check (if any) first. All that

	 * the signature check needs is info->len, it does

	 * not need any of the section info. That can be

	 * set up later. This will minimize the chances

	 * of a corrupt module causing problems before

	 * we even get to the signature check.

	 *

	 * The check will also adjust info->len by stripping

	 * off the sig length at the end of the module, making

	 * checks against info->len more correct.

	/*

	 * Do basic sanity checks against the ELF header and

	 * sections.

	/*

	 * Everything checks out, so set up the section info

	 * in the info structure.

	/*

	 * Now that we know we have the correct module name, check

	 * if it's blacklisted.

 Check module struct version now, before we try to use module. */

 Figure out module layout, and allocate all the memory. */

 Reserve our place in the list. */

 To avoid stressing percpu allocator, do this once we're unique. */

 Now module is in final location, initialize linked lists, etc. */

	/*

	 * Now we've got everything in the final locations, we can

	 * find optional sections.

 Set up MODINFO_ATTR fields */

 Fix up syms, so that st_value is a pointer to location. */

 Setup CFI for the module. */

 Now copy in args */

 Ftrace init must be called in the MODULE_STATE_UNFORMED state */

 Finally it's fully formed, ready to start executing. */

 Module is ready to execute: parsing args may do that. */

 Link in to sysfs. */

 Get rid of temporary copy. */

 Done! */

 module_bug_cleanup needs module_mutex protection */

 Unlink carefully: kallsyms could be walking list. */

 Wait for RCU-sched synchronizing before releasing mod->list. */

 Free lock-classes; relies on the preceding sync_rcu() */

/*

 * This ignores the intensely annoying "mapping symbols" found

 * in ARM ELF files: $a, $t and $d.

/*

 * Given a module and address, find the corresponding symbol and return its name

 * while providing its size and offset if needed.

 At worse, next value is at end of module */

	/*

	 * Scan for closest preceding symbol, and next symbol. (ELF

	 * starts real symbols at 1).

		/*

		 * We ignore unnamed symbols: they're uninformative

		 * and inserted at a whim.

/*

 * For kallsyms to ask for address resolution.  NULL means not found.  Careful

 * not to lock to avoid deadlock on oopses, simply disable preemption.

 Make a copy in here where it's safe */

 Given a module and name of symbol, find and return the symbol's value */

 Look for this name: can be of form module:name. */

 Don't lock: we're in enough trouble already. */

 We hold module_mutex: no need for rcu_dereference_sched */

 CONFIG_LIVEPATCH */

 CONFIG_KALLSYMS */

 Fix init/exit functions to point to the CFI jump table */

 Maximum number of characters written by module_flags() */

 Keep in sync with MODULE_FLAGS_BUF_SIZE !!! */

 Show a - for module-is-being-unloaded */

 Show a + for module-is-being-loaded */

 Called by the /proc file system to return a list of modules. */

 We always ignore unformed modules. */

 Informative for users. */

 Used by oprofile and other similar tools. */

 Taints info */

/*

 * Format: modulename size refcount deps address

 *

 * Where refcount is a number or -, and deps is a comma-separated list

 * of depends or -.

/*

 * This also sets the "private" pointer to non-NULL if the

 * kernel pointers should be hidden (so you can just test

 * "m->private" to see if you should keep the values private).

 *

 * We use the same logic as for /proc/kallsyms.

 Given an address, look for it in the module exception tables. */

	/*

	 * Now, if we found one, we are running inside it now, hence

	 * we cannot unload the module, hence no refcnt needed.

/**

 * is_module_address() - is this address inside a module?

 * @addr: the address to check.

 *

 * See is_module_text_address() if you simply want to see if the address

 * is code (not data).

/**

 * __module_address() - get the module which contains an address.

 * @addr: the address.

 *

 * Must be called with preempt disabled or module mutex held so that

 * module doesn't get freed during this.

/**

 * is_module_text_address() - is this address inside module code?

 * @addr: the address to check.

 *

 * See is_module_address() if you simply want to see if the address is

 * anywhere in a module.  See kernel_text_address() for testing if an

 * address corresponds to kernel or module code.

/**

 * __module_text_address() - get the module whose code contains an address.

 * @addr: the address.

 *

 * Must be called with preempt disabled or module mutex held so that

 * module doesn't get freed during this.

 Make sure it's within the text section. */

 Don't grab lock, we're oopsing. */

 Most callers should already have preempt disabled, but make sure */

/*

 * Generate the signature for all relevant module structures here.

 * If these change, we don't want to try to parse the module.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * The "user cache".

 *

 * (C) Copyright 1991-2000 Linus Torvalds

 *

 * We have a per-user structure to keep track of how many

 * processes, files etc the user has claimed, in order to be

 * able to have per-user limits for system resources. 

/*

 * userns count is 1 for root user, 1 for init_uts_ns,

 * and 1 for... ?

/*

 * UID task count cache, to get fast user lookup in "alloc_uid"

 * when changing user ID's (ie setuid() and friends).

/*

 * The uidhash_lock is mostly taken from process context, but it is

 * occasionally also taken from softirq/tasklet context, when

 * task-structs get RCU-freed. Hence all locking must be softirq-safe.

 * But free_uid() is also called with local interrupts disabled, and running

 * local_bh_enable() with local interrupts disabled is an error - we'll run

 * softirq callbacks, and they can unconditionally enable interrupts, and

 * the caller of free_uid() didn't expect that..

 root_user.__count is 1, for init task cred */

/*

 * These routines must be called with the uidhash spinlock held!

/* IRQs are disabled and uidhash_lock is held upon function entry.

 * IRQ state (as stored in flags) is restored and uidhash_lock released

 * upon function exit.

/*

 * Locate the user_struct for the passed UID.  If found, take a ref on it.  The

 * caller must undo that ref with free_uid().

 *

 * If the user_struct could not be found, return NULL.

		/*

		 * Before adding this, check whether we raced

		 * on adding the same user already..

 Insert the root user immediately (init already runs as root) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Simple stack backtrace regression test module

 *

 * (C) Copyright 2008 Intel Corporation

 * Author: Arjan van de Ven <arjan@linux.intel.com>

 SPDX-License-Identifier: GPL-2.0+

/*

 * Common functions for in-kernel torture tests.

 *

 * Copyright (C) IBM Corporation, 2014

 *

 * Author: Paul E. McKenney <paulmck@linux.ibm.com>

 *	Based on kernel/rcu/torture.c.

 Mediate rmmod and system shutdown.  Concurrent rmmod & shutdown illegal! */

 Normal operation. */

 System shutdown with torture running. */

 Normal rmmod of torture. */

/*

 * Sleep if needed from VERBOSE_TOROUT*().

/*

 * Schedule a high-resolution-timer sleep in nanoseconds, with a 32-bit

 * nanosecond random fuzz.  This function and its friends desynchronize

 * testing from the timer wheel.

/*

 * Schedule a high-resolution-timer sleep in microseconds, with a 32-bit

 * nanosecond (not microsecond!) random fuzz.

/*

 * Schedule a high-resolution-timer sleep in milliseconds, with a 32-bit

 * microsecond (not millisecond!) random fuzz.

/*

 * Schedule a high-resolution-timer sleep in jiffies, with an

 * implied one-jiffy random fuzz.  This is intended to replace calls to

 * schedule_timeout_interruptible() and friends.

/*

 * Schedule a high-resolution-timer sleep in milliseconds, with a 32-bit

 * millisecond (not second!) random fuzz.

/*

 * Variables for online-offline handling.  Only present if CPU hotplug

 * is enabled, otherwise does nothing.

/*

 * Some torture testing leverages confusion as to the number of online

 * CPUs.  This function returns the torture-testing view of this number,

 * which allows torture tests to load-balance appropriately.

/*

 * Attempt to take a CPU offline.  Return false if the CPU is already

 * offline or if it is not subject to CPU-hotplug operations.  The

 * caller can detect other failures by looking at the statistics.

 Can't offline the last CPU. */

 PCI probe frequently disables hotplug during boot.

/*

 * Attempt to bring a CPU online.  Return false if the CPU is already

 * online or if it is not subject to CPU-hotplug operations.  The

 * caller can detect other failures by looking at the statistics.

 PCI probe frequently disables hotplug during boot.

/*

 * Get everything online at the beginning and ends of tests.

/*

 * Execute random CPU-hotplug operations at the interval specified

 * by the onoff_interval.

 #ifdef CONFIG_HOTPLUG_CPU */

/*

 * Initiate online-offline handling.

 #ifdef CONFIG_HOTPLUG_CPU */

 #else #ifdef CONFIG_HOTPLUG_CPU */

/*

 * Clean up after online/offline testing.

 #ifdef CONFIG_HOTPLUG_CPU */

/*

 * Print online/offline testing statistics.

 #ifdef CONFIG_HOTPLUG_CPU */

/*

 * Were all the online/offline operations successful?

 #ifdef CONFIG_HOTPLUG_CPU */

 #else #ifdef CONFIG_HOTPLUG_CPU */

 prime */

 prime */

/*

 * Crude but fast random-number generator.  Uses a linear congruential

 * generator, with occasional help from cpu_clock().

/*

 * Variables for shuffling.  The idea is to ensure that each CPU stays

 * idle for an extended period to test interactions with dyntick idle,

 * as well as interactions with any per-CPU variables.

 In jiffies. */

 Force all torture tasks off this CPU */

/*

 * Register a task to be shuffled.  If there is no memory, just splat

 * and don't bother registering.

/*

 * Unregister all tasks, for example, at the end of the torture run.

/* Shuffle tasks such that we allow shuffle_idle_cpu to become idle.

 * A special case is when shuffle_idle_cpu = -1, in which case we allow

 * the tasks to run on all CPUs.

 No point in shuffling if there is only one online CPU (ex: UP) */

 Advance to the next CPU.  Upon overflow, don't idle any CPUs. */

/* Shuffle tasks across CPUs, with the intent of allowing each CPU in the

 * system to become idle at a time and cut off its timer ticks. This is meant

 * to test the support for such tickless idle CPU in RCU.

/*

 * Start the shuffler, with shuffint in jiffies.

 Create the shuffler thread */

/*

 * Stop the shuffling.

/*

 * Variables for auto-shutdown.  This allows "lights out" torture runs

 * to be fully scripted.

 time to system shutdown. */

/*

 * Absorb kthreads into a kernel function that won't return, so that

 * they won't ever access module text or data again.

/*

 * Cause the torture test to shutdown the system after the test has

 * run for the time specified by the shutdown_secs parameter.

 OK, shut down the system. */

 Avoid self-kill deadlock. */

 Shut down the system. */

/*

 * Start up the shutdown task.

/*

 * Detect and respond to a system shutdown.

/*

 * Shut down the shutdown task.  Say what???  Heh!  This can happen if

 * the torture module gets an rmmod before the shutdown time arrives.  ;-)

/*

 * Variables for stuttering, which means to periodically pause and

 * restart testing in order to catch bugs that appear when load is

 * suddenly applied to or removed from the system.

/*

 * Block until the stutter interval ends.  This must be called periodically

 * by all running kthreads that need to be subject to stuttering.

/*

 * Cause the torture test to "stutter", starting and stopping all

 * threads periodically.

/*

 * Initialize and kick off the torture_stutter kthread.

/*

 * Cleanup after the torture_stutter kthread.

/*

 * Initialize torture module.  Please note that this is -not- invoked via

 * the usual module_init() mechanism, but rather by an explicit call from

 * the client torture module.  This call must be paired with a later

 * torture_init_end().

 *

 * The runnable parameter points to a flag that controls whether or not

 * the test is currently runnable.  If there is no such flag, pass in NULL.

/*

 * Tell the torture module that initialization is complete.

/*

 * Clean up torture module.  Please note that this is -not- invoked via

 * the usual module_exit() mechanism, but rather by an explicit call from

 * the client torture module.  Returns true if a race with system shutdown

 * is detected, otherwise, all kthreads started by functions in this file

 * will be shut down.

 *

 * This must be called before the caller starts shutting down its own

 * kthreads.

 *

 * Both torture_cleanup_begin() and torture_cleanup_end() must be paired,

 * in order to correctly perform the cleanup. They are separated because

 * threads can still need to reference the torture_type type, thus nullify

 * only after completing all other relevant calls.

/*

 * Is it time for the current torture test to stop?

/*

 * Is it time for the current torture test to stop?  This is the irq-safe

 * version, hence no check for kthread_should_stop().

/*

 * Each kthread must wait for kthread_should_stop() before returning from

 * its top-level function, otherwise segfaults ensue.  This function

 * prints a "stopping" message and waits for kthread_should_stop(), and

 * should be called from all torture kthreads immediately prior to

 * returning.

/*

 * Create a generic torture kthread that is immediately runnable.  If you

 * need the kthread to be stopped so that you can do something to it before

 * it starts, you will need to open-code your own.

/*

 * Stop a generic kthread, emitting a message.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/reboot.c

 *

 *  Copyright (C) 2013  Linus Torvalds

/*

 * this indicates whether you can reboot with ctrl-alt-del: the default is yes

/*

 * This variable is used privately to keep track of whether or not

 * reboot_type is still set to its default value (i.e., reboot= hasn't

 * been set on the command line).  This is needed so that we can

 * suppress DMI scanning for reboot quirks.  Without it, it's

 * impossible to override a faulty reboot quirk without recompiling.

/*

 * If set, this is used for preparing the system to power off.

/**

 *	emergency_restart - reboot the system

 *

 *	Without shutting down any hardware or taking any locks

 *	reboot the system.  This is called when we know we are in

 *	trouble so this is our best effort to reboot.  This is

 *	safe to call in interrupt context.

/**

 *	register_reboot_notifier - Register function to be called at reboot time

 *	@nb: Info about notifier function to be called

 *

 *	Registers a function with the list of functions

 *	to be called at reboot time.

 *

 *	Currently always returns zero, as blocking_notifier_chain_register()

 *	always returns zero.

/**

 *	unregister_reboot_notifier - Unregister previously registered reboot notifier

 *	@nb: Hook to be unregistered

 *

 *	Unregisters a previously registered reboot

 *	notifier function.

 *

 *	Returns zero on success, or %-ENOENT on failure.

/*

 *	Notifier list for kernel code which wants to be called

 *	to restart the system.

/**

 *	register_restart_handler - Register function to be called to reset

 *				   the system

 *	@nb: Info about handler function to be called

 *	@nb->priority:	Handler priority. Handlers should follow the

 *			following guidelines for setting priorities.

 *			0:	Restart handler of last resort,

 *				with limited restart capabilities

 *			128:	Default restart handler; use if no other

 *				restart handler is expected to be available,

 *				and/or if restart functionality is

 *				sufficient to restart the entire system

 *			255:	Highest priority restart handler, will

 *				preempt all other restart handlers

 *

 *	Registers a function with code to be called to restart the

 *	system.

 *

 *	Registered functions will be called from machine_restart as last

 *	step of the restart sequence (if the architecture specific

 *	machine_restart function calls do_kernel_restart - see below

 *	for details).

 *	Registered functions are expected to restart the system immediately.

 *	If more than one function is registered, the restart handler priority

 *	selects which function will be called first.

 *

 *	Restart handlers are expected to be registered from non-architecture

 *	code, typically from drivers. A typical use case would be a system

 *	where restart functionality is provided through a watchdog. Multiple

 *	restart handlers may exist; for example, one restart handler might

 *	restart the entire system, while another only restarts the CPU.

 *	In such cases, the restart handler which only restarts part of the

 *	hardware is expected to register with low priority to ensure that

 *	it only runs if no other means to restart the system is available.

 *

 *	Currently always returns zero, as atomic_notifier_chain_register()

 *	always returns zero.

/**

 *	unregister_restart_handler - Unregister previously registered

 *				     restart handler

 *	@nb: Hook to be unregistered

 *

 *	Unregisters a previously registered restart handler function.

 *

 *	Returns zero on success, or %-ENOENT on failure.

/**

 *	do_kernel_restart - Execute kernel restart handler call chain

 *

 *	Calls functions registered with register_restart_handler.

 *

 *	Expected to be called from machine_restart as last step of the restart

 *	sequence.

 *

 *	Restarts the system immediately if a restart handler function has been

 *	registered. Otherwise does nothing.

 The boot cpu is always logical cpu 0 */

 Make certain the cpu I'm about to reboot on is online */

 Prevent races with other tasks migrating this task */

 Make certain I only run on the appropriate processor */

/**

 *	kernel_restart - reboot the system

 *	@cmd: pointer to buffer containing command to execute for restart

 *		or %NULL

 *

 *	Shutdown everything and perform a clean reboot.

 *	This is not safe to call in interrupt context.

/**

 *	kernel_halt - halt the system

 *

 *	Shutdown everything and perform a clean system halt.

/**

 *	kernel_power_off - power_off the system

 *

 *	Shutdown everything and perform a clean system power_off.

/*

 * Reboot system call: for obvious reasons only root may call it,

 * and even root needs to set up some magic numbers in the registers

 * so that some mistake won't make this reboot the whole machine.

 * You can also set the meaning of the ctrl-alt-del-key here.

 *

 * reboot doesn't sync: do that yourself before calling this.

 We only trust the superuser with rebooting the system. */

 For safety, we require "magic" arguments. */

	/*

	 * If pid namespaces are enabled and the current task is in a child

	 * pid_namespace, the command is handled by reboot_pid_ns() which will

	 * call do_exit().

	/* Instead of trying to make the power_off code look like

	 * halt when pm_power_off is not set do it the easy way.

/*

 * This function gets called by ctrl-alt-del - ie the keyboard interrupt.

 * As it's called within an interrupt, it may NOT sync: the only choice

 * is whether to reboot at once, or just ignore the ctrl-alt-del.

		/*

		 * I guess this should try to kick off some daemon to sync and

		 * poweroff asap.  Or not even bother syncing if we're doing an

		 * emergency shutdown?

/**

 * orderly_poweroff - Trigger an orderly system poweroff

 * @force: force poweroff if command execution fails

 *

 * This may be called from any context to trigger a system shutdown.

 * If the orderly shutdown fails, it will force an immediate shutdown.

 do not override the pending "true" */

/**

 * orderly_reboot - Trigger an orderly system reboot

 *

 * This may be called from any context to trigger a system reboot.

 * If the orderly reboot fails, it will force an immediate reboot.

/**

 * hw_failure_emergency_poweroff_func - emergency poweroff work after a known delay

 * @work: work_struct associated with the emergency poweroff function

 *

 * This function is called in very critical situations to force

 * a kernel poweroff after a configurable timeout value.

	/*

	 * We have reached here after the emergency shutdown waiting period has

	 * expired. This means orderly_poweroff has not been able to shut off

	 * the system for some reason.

	 *

	 * Try to shut down the system immediately using kernel_power_off

	 * if populated

	/*

	 * Worst of the worst case trigger emergency restart

/**

 * hw_failure_emergency_poweroff - Trigger an emergency system poweroff

 *

 * This may be called from any critical situation to trigger a system shutdown

 * after a given period of time. If time is negative this is not scheduled.

/**

 * hw_protection_shutdown - Trigger an emergency system poweroff

 *

 * @reason:		Reason of emergency shutdown to be printed.

 * @ms_until_forced:	Time to wait for orderly shutdown before tiggering a

 *			forced shudown. Negative value disables the forced

 *			shutdown.

 *

 * Initiate an emergency system shutdown in order to protect hardware from

 * further damage. Usage examples include a thermal protection or a voltage or

 * current regulator failures.

 * NOTE: The request is ignored if protection shutdown is already pending even

 * if the previous request has given a large timeout for forced shutdown.

 * Can be called from any context.

 Shutdown should be initiated only once. */

	/*

	 * Queue a backup emergency shutdown in the event of

	 * orderly_poweroff failure

		/*

		 * Having anything passed on the command line via

		 * reboot= will cause us to disable DMI checking

		 * below.

			/*

			 * reboot_cpu is s[mp]#### with #### being the processor

			 * to be used for rebooting. Skip 's' or 'smp' prefix.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2008-2014 Mathieu Desnoyers

 Protected by tracepoints_mutex */

 Keep the latest get_state snapshot. */

 Set to 1 to enable tracepoint debug output */

/*

 * Tracepoint module list mutex protects the local module list.

 Local list of struct tp_module */

 CONFIG_MODULES */

/*

 * tracepoints_mutex protects the builtin and module tracepoints.

 * tracepoints_mutex nests inside tracepoint_module_list_mutex.

/*

 * Note about RCU :

 * It is used to delay the free of multiple probes array until a quiescent

 * state is reached.

 Called in removal of a func but failed to allocate a new tp_funcs */

 SRCU is initialized at core_initcall */

		/*

		 * We can't free probes if SRCU is not initialized yet.

		 * Postpone the freeing till after SRCU is initialized.

		/*

		 * Tracepoint probes are protected by both sched RCU and SRCU,

		 * by calling the SRCU callback in the sched RCU callback we

		 * cover both cases. So let us chain the SRCU and sched RCU

		 * callbacks to wait for both grace periods.

 Iterate over old probe array. */

 Counter for probes */

 Insertion position into new array */

 (N -> N+1), (N != 0, 1) probes */

 Skip stub functions. */

 + 2 : one for new probe, one for NULL func */

 Insert before probes of lower priority */

 nr_probes now points to the end of the new array */

 must point at end of array */

 (N -> M), (N > 1, M >= 0) probes */

	/*

	 * If probe is NULL, then nr_probes = nr_del = 0, and then the

	 * entire entry will be removed.

 N -> 0, (N > 1) */

 N -> M, (N > 1, M > 0) */

 + 1 for NULL */

			/*

			 * Failed to allocate, replace the old function

			 * with calls to tp_stub_func.

/*

 * Count the number of functions (enum tp_func_state) in a tp_funcs array.

 3 or more */

 Synthetic events do not have static call sites */

/*

 * Add the probe function to a tracepoint.

	/*

	 * rcu_assign_pointer has as smp_store_release() which makes sure

	 * that the new probe callbacks array is consistent before setting

	 * a pointer to it.  This array is referenced by __DO_TRACE from

	 * include/linux/tracepoint.h using rcu_dereference_sched().

 0->1 */

		/*

		 * Make sure new static func never uses old data after a

		 * 1->0->1 transition sequence.

 Set static call to first function */

 Both iterator and static call handle NULL tp->funcs */

 1->2 */

 Set iterator static call */

		/*

		 * Iterator callback installed before updating tp->funcs.

		 * Requires ordering between RCU assign/dereference and

		 * static call update/call.

 N->N+1 (N>1) */

		/*

		 * Make sure static func never uses incorrect data after a

		 * N->...->2->1 (N>1) transition sequence.

/*

 * Remove a probe function from a tracepoint.

 * Note: only waiting an RCU period after setting elem->call to the empty

 * function insures that the original callback is not used anymore. This insured

 * by preempt_disable around the call site.

 Failed allocating new tp_funcs, replaced func with stub */

 1->0 */

 Removed last function */

 Set iterator static call */

 Both iterator and static call handle NULL tp->funcs */

		/*

		 * Make sure new static func never uses old data after a

		 * 1->0->1 transition sequence.

 2->1 */

		/*

		 * Make sure static func never uses incorrect data after a

		 * N->...->2->1 (N>2) transition sequence. If the first

		 * element's data has changed, then force the synchronization

		 * to prevent current readers that have loaded the old data

		 * from calling the new function.

 Set static call to first function */

 N->N-1 (N>2) */

		/*

		 * Make sure static func never uses incorrect data after a

		 * N->...->2->1 (N>2) transition sequence.

/**

 * tracepoint_probe_register_prio_may_exist -  Connect a probe to a tracepoint with priority

 * @tp: tracepoint

 * @probe: probe handler

 * @data: tracepoint data

 * @prio: priority of this function over other registered functions

 *

 * Same as tracepoint_probe_register_prio() except that it will not warn

 * if the tracepoint is already registered.

/**

 * tracepoint_probe_register_prio -  Connect a probe to a tracepoint with priority

 * @tp: tracepoint

 * @probe: probe handler

 * @data: tracepoint data

 * @prio: priority of this function over other registered functions

 *

 * Returns 0 if ok, error value on error.

 * Note: if @tp is within a module, the caller is responsible for

 * unregistering the probe before the module is gone. This can be

 * performed either with a tracepoint module going notifier, or from

 * within module exit functions.

/**

 * tracepoint_probe_register -  Connect a probe to a tracepoint

 * @tp: tracepoint

 * @probe: probe handler

 * @data: tracepoint data

 *

 * Returns 0 if ok, error value on error.

 * Note: if @tp is within a module, the caller is responsible for

 * unregistering the probe before the module is gone. This can be

 * performed either with a tracepoint module going notifier, or from

 * within module exit functions.

/**

 * tracepoint_probe_unregister -  Disconnect a probe from a tracepoint

 * @tp: tracepoint

 * @probe: probe function pointer

 * @data: tracepoint data

 *

 * Returns 0 if ok, error value on error.

/**

 * register_tracepoint_module_notifier - register tracepoint coming/going notifier

 * @nb: notifier block

 *

 * Notifiers registered with this function are called on module

 * coming/going with the tracepoint_module_list_mutex held.

 * The notifier block callback should expect a "struct tp_module" data

 * pointer.

/**

 * unregister_tracepoint_module_notifier - unregister tracepoint coming/going notifier

 * @nb: notifier block

 *

 * The notifier block callback should expect a "struct tp_module" data

 * pointer.

/*

 * Ensure the tracer unregistered the module's probes before the module

 * teardown is performed. Prevents leaks of probe and data pointers.

	/*

	 * We skip modules that taint the kernel, especially those with different

	 * module headers (for forced load), to make sure we don't cause a crash.

	 * Staging, out-of-tree, and unsigned GPL modules are fine.

			/*

			 * Called the going notifier before checking for

			 * quiescence.

	/*

	 * In the case of modules that were tainted at "coming", we'll simply

	 * walk through the list without finding it. We cannot use the "tainted"

	 * flag on "going", in case a module taints the kernel only after being

	 * loaded.

 CONFIG_MODULES */

/**

 * for_each_kernel_tracepoint - iteration on all kernel tracepoints

 * @fct: callback

 * @priv: private data

 NB: reg/unreg are called while guarded with the tracepoints_mutex */

 SPDX-License-Identifier: GPL-2.0-or-later

/* delayacct.c - per-task delay accounting

 *

 * Copyright (C) Shailabh Nagar, IBM Corp. 2006

 Delay accounting turned on/off */

/*

 * Finish delay accounting for a statistic using its timestamps (@start),

 * accumalator (@total) and @count

/*

 * We cannot rely on the `current` macro, as we haven't yet switched back to

 * the process being woken.

	/*

	 * No locking available for sched_info (and too expensive to add one)

	 * Mitigate by taking snapshot of values

 zero XXX_total, non-zero XXX_count implies XXX stat overflowed */

 SPDX-License-Identifier: GPL-2.0-or-later

/* auditsc.c -- System-call auditing support

 * Handles all system-call specific auditing features.

 *

 * Copyright 2003-2004 Red Hat Inc., Durham, North Carolina.

 * Copyright 2005 Hewlett-Packard Development Company, L.P.

 * Copyright (C) 2005, 2006 IBM Corporation

 * All Rights Reserved.

 *

 * Written by Rickard E. (Rik) Faith <faith@redhat.com>

 *

 * Many of the ideas implemented here are from Stephen C. Tweedie,

 * especially the idea of avoiding a copy by using getname.

 *

 * The method for actual interception of syscall entry and exit (not in

 * this file -- see entry.S) is based on a GPL'd patch written by

 * okir@suse.de and Copyright 2003 SuSE Linux AG.

 *

 * POSIX message queue support added by George Wilson <ltcgcw@us.ibm.com>,

 * 2006.

 *

 * The support of additional filter rules compares (>, <, >=, <=) was

 * added by Dustin Kirkland <dustin.kirkland@us.ibm.com>, 2005.

 *

 * Modified by Amy Griffis <amy.griffis@hp.com> to collect additional

 * filesystem information.

 *

 * Subject and object context labeling support added by <danjones@us.ibm.com>

 * and <dustin.kirkland@us.ibm.com> for LSPP certification compliance.

 struct open_how

 flags stating the success for a syscall */

/* no execve audit message should be longer than this (userspace limits),

 max length to print of cmdline/proctitle value during audit */

 number of audit rules */

 determines whether we collect data for signals sent */

 Number of target pids per aux struct. */

 32bit on biarch */

/*

 * We keep a linked list of fixed-sized (31 pointer) arrays of audit_chunk *;

 * ->first_trees points to its beginning, ->trees - to the current end of data.

 * ->tree_count is the number of free entries in array pointed to by ->trees.

 * Original condition is (NULL, NULL, 0); as soon as it grows we never revert to NULL,

 * "empty" becomes (p, p, 31) afterwards.  We don't shrink the list (and seriously,

 * it's going to remain 1-element for almost any setup) until we free context itself.

 * References in it _are_ dropped - at the same time we free/drop aux stuff.

 we started with empty chain */

 if the very first allocation has failed, nothing to do */

 full ones */

 partial */

 process to file object comparisons */

 uid comparisons */

 auid comparisons */

 euid comparisons */

 suid comparisons */

 gid comparisons */

 egid comparisons */

 sgid comparison */

 Determine if any context name data matches a rule's watch data */

/* Compare a task_struct with an audit_rule.  Return 1 on match, 0

 * otherwise.

 *

 * If task_creation is true, this is an explicit indication that we are

 * filtering a task rule at task creation time.  This and tsk == current are

 * the only situations where tsk->cred may be accessed without an rcu read lock.

			/* NOTE: this may return negative values indicating

			   a temporary error.  We simply treat this as a

			   match for now to avoid losing information that

			   may be wanted.   An error message will also be

			/* The above note for AUDIT_SUBJ_USER...AUDIT_SUBJ_CLR

 Find files that match */

 Find ipc objects that match */

 ignore this field for filtering */

/* At process creation time, we can determine if system-call auditing is

 * completely disabled for this task.  Since we only have the task

 * structure at this point, we can only check uid and gid.

/**

 * audit_filter_uring - apply filters to an io_uring operation

 * @tsk: associated task

 * @ctx: audit context

/* At syscall exit time, this filter is called if the audit_state is

 * not low enough that auditing cannot take place, but is also not

 * high enough that we already know we have to write an audit record

 * (i.e., the state is AUDIT_STATE_BUILD).

/*

 * Given an audit_name check the inode hash table to see if they match.

 * Called holding the rcu read lock to protect the use of audit_inode_hash

/* At syscall exit time, this filter is called if any audit_names have been

 * collected during syscall processing.  We only check rules in sublists at hash

 * buckets applicable to the inode numbers in audit_names.

 * Regarding audit_state, same rules apply as for audit_filter_syscall().

/**

 * audit_reset_context - reset a audit_context structure

 * @ctx: the audit_context to reset

 *

 * All fields in the audit_context will be reset to an initial state, all

 * references held by fields will be dropped, and private memory will be

 * released.  When this function returns the audit_context will be suitable

 * for reuse, so long as the passed context is not NULL or a dummy context.

 if ctx is non-null, reset the "ctx->state" regardless */

	/*

	 * NOTE: It shouldn't matter in what order we release the fields, so

	 *       release them in the order in which they appear in the struct;

	 *       this gives us some hope of quickly making sure we are

	 *       resetting the audit_context properly.

	 *

	 *       Other things worth mentioning:

	 *       - we don't reset "dummy"

	 *       - we don't reset "state", we do reset "current_state"

	 *       - we preserve "filterkey" if "state" is AUDIT_STATE_RECORD

	 *       - much of this is likely overkill, but play it safe for now

	 *       - we really need to work on improving the audit_context struct

/**

 * audit_alloc - allocate an audit context block for a task

 * @tsk: task

 *

 * Filter on the task information and allocate a per-task audit context

 * if necessary.  Doing so turns on system call auditing for the

 * specified task.  This is called from copy_process, so no lock is

 * needed.

/**

 * audit_alloc_kernel - allocate an audit_context for a kernel task

 * @tsk: the kernel task

 *

 * Similar to the audit_alloc() function, but intended for kernel private

 * threads.  Returns zero on success, negative values on failure.

	/*

	 * At the moment we are just going to call into audit_alloc() to

	 * simplify the code, but there two things to keep in mind with this

	 * approach:

	 *

	 * 1. Filtering internal kernel tasks is a bit laughable in almost all

	 * cases, but there is at least one case where there is a benefit:

	 * the '-a task,never' case allows the admin to effectively disable

	 * task auditing at runtime.

	 *

	 * 2. The {set,clear}_task_syscall_work() ops likely have zero effect

	 * on these internal kernel tasks, but they probably don't hurt either.

 resetting is extra work, but it is likely just noise */

	/* NOTE: this buffer needs to be large enough to hold all the non-arg

	 *       data we put in the audit record for this argument (see the

	/* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the

	 *       current value of 7500 is not as important as the fact that it

	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle

 scratch buffer to hold the userspace args */

		/* NOTE: we don't ever want to trust this value for anything

		 *       serious, but the audit record format insists we

		 *       provide an argument length for really long arguments,

		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but

		 *       to use strncpy_from_user() to obtain this value for

		 *       recording in the log, although we don't use it

 read more data from userspace */

 can we make more room in the buffer? */

 fetch as much as we can of the argument */

 unable to copy from userspace */

 buffer is not large enough */

				/* NOTE: if we are going to span multiple

				 *       buffers force the encoding so we stand

				 *       a chance at a sane len_full value and

 try to use a trusted value for len_full */

 length of the buffer in the audit record? */

 write as much as we can to the audit log */

			/* NOTE: some magic numbers here - basically if we

			 *       can't fit a reasonable amount of data into the

			 *       existing audit buffer, flush it and start with

 create the non-arg portion of the arg record */

 log the arg in the audit record */

 encoding */

 quotes */

				/* don't subtract the "2" because we still need

 ready to move to the next argument? */

 NOTE: the caller handles the final audit_log_end() call */

 catch the case where proctitle is only 1 non-print character */

/*

 * audit_log_name - produce AUDIT_PATH record from struct audit_names

 * @context: audit_context for the task

 * @n: audit_names structure with reportable details

 * @path: optional path to report instead of audit_names->name

 * @record_num: record number to report when handling a list of names

 * @call_panic: optional pointer to int that will be updated if secid fails

 log the full path */

			/* name was specified as a relative path and the

			 * directory component is the cwd

 log the name's directory component */

 log the audit_names record type */

 audit_panic or being filtered */

 Not  cached */

 Historically called this from procfs naming */

/**

 * audit_log_uring - generate a AUDIT_URINGOP record

 * @ctx: the audit context

 audit_panic has been called */

 Send end of event record to help user space know we are finished */

/**

 * __audit_free - free a per-task audit context

 * @tsk: task whose audit context block to free

 *

 * Called from copy_process, do_exit, and the io_uring code

 this may generate CONFIG_CHANGE records */

	/* We are called either by do_exit() or the fork() error handling code;

	 * in the former case tsk == current and in the latter tsk is a

	 * random task_struct that doesn't doesn't have any meaningful data we

	 * need to log via audit_log_exit().

 TODO: verify this case is real and valid */

/**

 * audit_return_fixup - fixup the return codes in the audit_context

 * @ctx: the audit_context

 * @success: true/false value to indicate if the operation succeeded or not

 * @code: operation return code

 *

 * We need to fixup the return code in the audit logs if the actual return

 * codes are later going to be fixed by the arch specific signal handlers.

	/*

	 * This is actually a test for:

	 * (rc == ERESTARTSYS ) || (rc == ERESTARTNOINTR) ||

	 * (rc == ERESTARTNOHAND) || (rc == ERESTART_RESTARTBLOCK)

	 *

	 * but is faster than a bunch of ||

/**

 * __audit_uring_entry - prepare the kernel task's audit context for io_uring

 * @op: the io_uring opcode

 *

 * This is similar to audit_syscall_entry() but is intended for use by io_uring

 * operations.  This function should only ever be called from

 * audit_uring_entry() as we rely on the audit context checking present in that

 * function.

	/*

	 * NOTE: It's possible that we can be called from the process' context

	 *       before it returns to userspace, and before audit_syscall_exit()

	 *       is called.  In this case there is not much to do, just record

	 *       the io_uring details and return.

/**

 * __audit_uring_exit - wrap up the kernel task's audit context after io_uring

 * @success: true/false value to indicate if the operation succeeded or not

 * @code: operation return code

 *

 * This is similar to audit_syscall_exit() but is intended for use by io_uring

 * operations.  This function should only ever be called from

 * audit_uring_exit() as we rely on the audit context checking present in that

 * function.

		/*

		 * NOTE: See the note in __audit_uring_entry() about the case

		 *       where we may be called from process context before we

		 *       return to userspace via audit_syscall_exit().  In this

		 *       case we simply emit a URINGOP record and bail, the

		 *       normal syscall exit handling will take care of

		 *       everything else.

		 *       It is also worth mentioning that when we are called,

		 *       the current process creds may differ from the creds

		 *       used during the normal syscall processing; keep that

		 *       in mind if/when we move the record generation code.

		/*

		 * We need to filter on the syscall info here to decide if we

		 * should emit a URINGOP record.  I know it seems odd but this

		 * solves the problem where users have a filter to block *all*

		 * syscall records in the "exit" filter; we want to preserve

		 * the behavior here.

 this may generate CONFIG_CHANGE records */

 run through both filters to ensure we set the filterkey properly */

/**

 * __audit_syscall_entry - fill in an audit record at syscall entry

 * @major: major syscall type (function)

 * @a1: additional syscall register 1

 * @a2: additional syscall register 2

 * @a3: additional syscall register 3

 * @a4: additional syscall register 4

 *

 * Fill in audit context at syscall entry.  This only happens if the

 * audit context was created when the task was created and the state or

 * filters demand the audit context be built.  If the state from the

 * per-task filter or from the per-syscall filter is AUDIT_STATE_RECORD,

 * then the record will be written at syscall exit time (otherwise, it

 * will only be written if another part of the kernel requests that it

 * be written).

/**

 * __audit_syscall_exit - deallocate audit context after a system call

 * @success: success value of the syscall

 * @return_code: return value of the syscall

 *

 * Tear down after system call.  If the audit context has been marked as

 * auditable (either because of the AUDIT_STATE_RECORD state from

 * filtering, or because some other part of the kernel wrote an audit

 * message), then write out the syscall information.  In call cases,

 * free the names stored from getname().

 this may generate CONFIG_CHANGE records */

 run through both filters to ensure we set the filterkey properly */

 in this order */

 just a race with rename */

 OK, got more space */

 too bad */

/**

 * __audit_reusename - fill out filename with info from existing entry

 * @uptr: userland ptr to pathname

 *

 * Search the audit_names list for the current audit context. If there is an

 * existing entry with a matching "uptr" then return the filename

 * associated with that audit_name. If not, return NULL.

/**

 * __audit_getname - add a name to the list

 * @name: name to add

 *

 * Add a name to the list of audit names for this context.

 * Called from fs/namei.c:getname().

 Copy inode data into an audit_names. */

/**

 * __audit_inode - store the inode and device from a lookup

 * @name: name being audited

 * @dentry: dentry being audited

 * @flags: attributes for this particular entry

	/*

	 * If we have a pointer to an audit_names entry already, then we can

	 * just use it directly if the type is correct.

 valid inode number, use that for the comparison */

 inode number has not been set, check the name */

 no inode and no name (?!) ... this is odd ... */

 match the correct record type */

 unable to find an entry with both a matching name and type */

/**

 * __audit_inode_child - collect inode info for created/removed objects

 * @parent: inode of dentry parent

 * @dentry: dentry being audited

 * @type:   AUDIT_TYPE_* value that we're looking for

 *

 * For syscalls that create or remove filesystem objects, audit_inode

 * can only collect information for the filesystem object's parent.

 * This call updates the audit context with the child's information.

 * Syscalls that create a new filesystem object must be hooked after

 * the object is created.  Syscalls that remove a filesystem object

 * must be hooked prior, in order to capture the target inode during

 * unsuccessful attempts.

 look for a parent entry first */

 is there a matching child entry? */

 can only match entries that have a name */

 create a new, "anonymous" parent record */

		/* Re-use the name belonging to the slot for a matching parent

		 * directory. All names for this context are relinquished in

/**

 * auditsc_get_stamp - get local copies of audit_context values

 * @ctx: audit_context for the task

 * @t: timespec64 to store time recorded in the audit_context

 * @serial: serial value that is recorded in the audit_context

 *

 * Also sets the context as auditable.

/**

 * __audit_mq_open - record audit data for a POSIX MQ open

 * @oflag: open flag

 * @mode: mode bits

 * @attr: queue attributes

 *

/**

 * __audit_mq_sendrecv - record audit data for a POSIX MQ timed send/receive

 * @mqdes: MQ descriptor

 * @msg_len: Message length

 * @msg_prio: Message priority

 * @abs_timeout: Message timeout in absolute time

 *

/**

 * __audit_mq_notify - record audit data for a POSIX MQ notify

 * @mqdes: MQ descriptor

 * @notification: Notification event

 *

/**

 * __audit_mq_getsetattr - record audit data for a POSIX MQ get/set attribute

 * @mqdes: MQ descriptor

 * @mqstat: MQ flags

 *

/**

 * __audit_ipc_obj - record audit data for ipc object

 * @ipcp: ipc permissions

 *

/**

 * __audit_ipc_set_perm - record audit data for new ipc permissions

 * @qbytes: msgq bytes

 * @uid: msgq user id

 * @gid: msgq group id

 * @mode: msgq mode (permissions)

 *

 * Called only after audit_ipc_obj().

/**

 * __audit_socketcall - record audit data for sys_socketcall

 * @nargs: number of args, which should not be more than AUDITSC_ARGS.

 * @args: args array

 *

/**

 * __audit_fd_pair - record audit data for pipe and socketpair

 * @fd1: the first file descriptor

 * @fd2: the second file descriptor

 *

/**

 * __audit_sockaddr - record audit data for sys_bind, sys_connect, sys_sendto

 * @len: data length in user space

 * @a: data address in kernel space

 *

 * Returns 0 for success or NULL context or < 0 on error.

/**

 * audit_signal_info_syscall - record signal info for syscalls

 * @t: task being signaled

 *

 * If the audit subsystem is being terminated, record the task (pid)

 * and uid that is doing that.

	/* optimize the common case by putting first signal recipient directly

/**

 * __audit_log_bprm_fcaps - store information about a loading bprm and relevant fcaps

 * @bprm: pointer to the bprm being processed

 * @new: the proposed new credentials

 * @old: the old credentials

 *

 * Simply check if the proc already has the caps given by the file and if not

 * store the priv escalation info for later auditing at the end of the syscall

 *

 * -Eric

/**

 * __audit_log_capset - store information about the arguments to the capset syscall

 * @new: the new credentials

 * @old: the old (current) credentials

 *

 * Record the arguments userspace sent to sys_capset for later printing by the

 * audit system if applicable

 subj= */

/**

 * audit_core_dumps - record information about processes that end abnormally

 * @signr: signal value

 *

 * If a process ends with a core dump, something fishy is going on and we

 * should record the event for investigation.

 don't care for those */

/**

 * audit_seccomp - record information about a seccomp action

 * @syscall: syscall number

 * @signr: signal value

 * @code: the seccomp action

 *

 * Record the information associated with a seccomp action. Event filtering for

 * seccomp actions that are not to be logged is done in seccomp_log().

 * Therefore, this function forces auditing independent of the audit_enabled

 * and dummy context state because seccomp actions should be logged even when

 * audit is not in use.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * umh - the kernel usermode helper

	/*

	 * See call_usermodehelper_exec(). If xchg() returns NULL

	 * we own sub_info, the UMH_KILLABLE caller has gone away

	 * or the caller used UMH_NO_WAIT.

/*

 * This is the task which runs the usermode application

	/*

	 * Initial kernel threads share ther FS with init, in order to

	 * get the init root directory. But we've now created a new

	 * thread that is going to execve a user process and has its own

	 * 'struct fs_struct'. Reset umask to the default.

	/*

	 * Our parent (unbound workqueue) runs with elevated scheduling

	 * priority. Avoid propagating that into the userspace child.

	/*

	 * call_usermodehelper_exec_sync() will call umh_complete

	 * if UHM_WAIT_PROC.

 Handles UMH_WAIT_PROC.  */

 If SIGCLD is ignored do_wait won't populate the status. */

 Restore default kernel sig handler */

/*

 * We need to create the usermodehelper kernel thread from a task that is affine

 * to an optimized set of CPUs (or nohz housekeeping ones) such that they

 * inherit a widest affinity irrespective of call_usermodehelper() callers with

 * possibly reduced affinity (eg: per-cpu workqueues). We don't want

 * usermodehelper targets to contend a busy CPU.

 *

 * Unbound workqueues provide such wide affinity and allow to block on

 * UMH_WAIT_PROC requests without blocking pending request (up to some limit).

 *

 * Besides, workqueues provide the privilege level that caller might not have

 * to perform the usermodehelper request.

 *

		/*

		 * Use CLONE_PARENT to reparent it to kthreadd; we do not

		 * want to pollute current->children, and we need a parent

		 * that always ignores SIGCHLD to ensure auto-reaping.

/*

 * If set, call_usermodehelper_exec() will exit immediately returning -EBUSY

 * (used for preventing user land processes from being created after the user

 * land has been frozen during a system-wide hibernation or suspend operation).

 * Should always be manipulated under umhelper_sem acquired for write.

 Number of helpers running */

/*

 * Wait queue head used by usermodehelper_disable() to wait for all running

 * helpers to finish.

/*

 * Used by usermodehelper_read_lock_wait() to wait for usermodehelper_disabled

 * to become 'false'.

/*

 * Time to wait for running_helpers to become zero before the setting of

 * usermodehelper_disabled in usermodehelper_disable() fails

/**

 * __usermodehelper_set_disable_depth - Modify usermodehelper_disabled.

 * @depth: New value to assign to usermodehelper_disabled.

 *

 * Change the value of usermodehelper_disabled (under umhelper_sem locked for

 * writing) and wakeup tasks waiting for it to change.

/**

 * __usermodehelper_disable - Prevent new helpers from being started.

 * @depth: New value to assign to usermodehelper_disabled.

 *

 * Set usermodehelper_disabled to @depth and wait for running helpers to exit.

	/*

	 * From now on call_usermodehelper_exec() won't start any new

	 * helpers, so it is sufficient if running_helpers turns out to

	 * be zero at one point (it may be increased later, but that

	 * doesn't matter).

/**

 * call_usermodehelper_setup - prepare to call a usermode helper

 * @path: path to usermode executable

 * @argv: arg vector for process

 * @envp: environment for process

 * @gfp_mask: gfp mask for memory allocation

 * @init: an init function

 * @cleanup: a cleanup function

 * @data: arbitrary context sensitive data

 *

 * Returns either %NULL on allocation failure, or a subprocess_info

 * structure.  This should be passed to call_usermodehelper_exec to

 * exec the process and free the structure.

 *

 * The init function is used to customize the helper process prior to

 * exec.  A non-zero return code causes the process to error out, exit,

 * and return the failure to the calling process

 *

 * The cleanup function is just before the subprocess_info is about to

 * be freed.  This can be used for freeing the argv and envp.  The

 * Function must be runnable in either a process context or the

 * context in which call_usermodehelper_exec is called.

/**

 * call_usermodehelper_exec - start a usermode application

 * @sub_info: information about the subprocess

 * @wait: wait for the application to finish and return status.

 *        when UMH_NO_WAIT don't wait at all, but you get no useful error back

 *        when the program couldn't be exec'ed. This makes it safe to call

 *        from interrupt context.

 *

 * Runs a user-space application.  The application is started

 * asynchronously if wait is not set, and runs as a child of system workqueues.

 * (ie. it runs with full root capabilities and optimized affinity).

 *

 * Note: successful return value does not guarantee the helper was called at

 * all. You can't rely on sub_info->{init,cleanup} being called even for

 * UMH_WAIT_* wait modes as STATIC_USERMODEHELPER_PATH="" turns all helpers

 * into a successful no-op.

	/*

	 * If there is no binary for us to call, then just return and get out of

	 * here.  This allows us to set STATIC_USERMODEHELPER_PATH to "" and

	 * disable all call_usermodehelper() calls.

	/*

	 * Set the completion pointer only if there is a waiter.

	 * This makes it possible to use umh_complete to free

	 * the data structure in case of UMH_NO_WAIT.

 task has freed sub_info */

 umh_complete() will see NULL and free sub_info */

 fallthrough, umh_complete() was already called */

/**

 * call_usermodehelper() - prepare and start a usermode application

 * @path: path to usermode executable

 * @argv: arg vector for process

 * @envp: environment for process

 * @wait: wait for the application to finish and return status.

 *        when UMH_NO_WAIT don't wait at all, but you get no useful error back

 *        when the program couldn't be exec'ed. This makes it safe to call

 *        from interrupt context.

 *

 * This function is the equivalent to use call_usermodehelper_setup() and

 * call_usermodehelper_exec().

	/*

	 * convert from the global kernel_cap_t to the ulong array to print to

	 * userspace if this is a read.

	/*

	 * actually read or write and array of ulongs from userspace.  Remember

	 * these are least significant 32 bits first

	/*

	 * convert from the sysctl array of ulongs to the kernel_cap_t

	 * internal representation

	/*

	 * Drop everything not in the new_cap (but don't add things)

 SPDX-License-Identifier: GPL-2.0 */

 temporary while we convert existing ioremap_cache users to memremap */

 In the simple case just return the existing linear address */

 fallback to arch_memremap_wb */

/**

 * memremap() - remap an iomem_resource as cacheable memory

 * @offset: iomem resource start address

 * @size: size of remap

 * @flags: any of MEMREMAP_WB, MEMREMAP_WT, MEMREMAP_WC,

 *		  MEMREMAP_ENC, MEMREMAP_DEC

 *

 * memremap() is "ioremap" for cases where it is known that the resource

 * being mapped does not have i/o side effects and the __iomem

 * annotation is not applicable. In the case of multiple flags, the different

 * mapping types will be attempted in the order listed below until one of

 * them succeeds.

 *

 * MEMREMAP_WB - matches the default mapping for System RAM on

 * the architecture.  This is usually a read-allocate write-back cache.

 * Moreover, if MEMREMAP_WB is specified and the requested remap region is RAM

 * memremap() will bypass establishing a new mapping and instead return

 * a pointer into the direct map.

 *

 * MEMREMAP_WT - establish a mapping whereby writes either bypass the

 * cache or are written through to memory and never exist in a

 * cache-dirty state with respect to program visibility.  Attempts to

 * map System RAM with this mapping type will fail.

 *

 * MEMREMAP_WC - establish a writecombine mapping, whereby writes may

 * be coalesced together (e.g. in the CPU's write buffers), but is otherwise

 * uncached. Attempts to map System RAM with this mapping type will fail.

 Try all mapping types requested until one returns non-NULL */

		/*

		 * MEMREMAP_WB is special in that it can be satisfied

		 * from the direct map.  Some archs depend on the

		 * capability of memremap() to autodetect cases where

		 * the requested range is potentially in System RAM.

	/*

	 * If we don't have a mapping yet and other request flags are

	 * present then we will be attempting to establish a new virtual

	 * address mapping.  Enforce that this mapping is not aliasing

	 * System RAM.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *	Notifier list for kernel code which wants to be called

 *	at shutdown. This is used to stop any idling DMA operations

 *	and the like.

/*

 *	Notifier chain core routines.  The exported routines below

 *	are layered on top of these, with appropriate locking added.

/**

 * notifier_call_chain - Informs the registered notifiers about an event.

 *	@nl:		Pointer to head of the blocking notifier chain

 *	@val:		Value passed unmodified to notifier function

 *	@v:		Pointer passed unmodified to notifier function

 *	@nr_to_call:	Number of notifier functions to be called. Don't care

 *			value of this parameter is -1.

 *	@nr_calls:	Records the number of notifications sent. Don't care

 *			value of this field is NULL.

 *	@returns:	notifier_call_chain returns the value returned by the

 *			last notifier function called.

/**

 * notifier_call_chain_robust - Inform the registered notifiers about an event

 *                              and rollback on error.

 * @nl:		Pointer to head of the blocking notifier chain

 * @val_up:	Value passed unmodified to the notifier function

 * @val_down:	Value passed unmodified to the notifier function when recovering

 *              from an error on @val_up

 * @v		Pointer passed unmodified to the notifier function

 *

 * NOTE:	It is important the @nl chain doesn't change between the two

 *		invocations of notifier_call_chain() such that we visit the

 *		exact same notifier callbacks; this rules out any RCU usage.

 *

 * Returns:	the return value of the @val_up call.

/*

 *	Atomic notifier chain routines.  Registration and unregistration

 *	use a spinlock, and call_chain is synchronized by RCU (no locks).

/**

 *	atomic_notifier_chain_register - Add notifier to an atomic notifier chain

 *	@nh: Pointer to head of the atomic notifier chain

 *	@n: New entry in notifier chain

 *

 *	Adds a notifier to an atomic notifier chain.

 *

 *	Currently always returns zero.

/**

 *	atomic_notifier_chain_unregister - Remove notifier from an atomic notifier chain

 *	@nh: Pointer to head of the atomic notifier chain

 *	@n: Entry to remove from notifier chain

 *

 *	Removes a notifier from an atomic notifier chain.

 *

 *	Returns zero on success or %-ENOENT on failure.

/**

 *	atomic_notifier_call_chain - Call functions in an atomic notifier chain

 *	@nh: Pointer to head of the atomic notifier chain

 *	@val: Value passed unmodified to notifier function

 *	@v: Pointer passed unmodified to notifier function

 *

 *	Calls each function in a notifier chain in turn.  The functions

 *	run in an atomic context, so they must not block.

 *	This routine uses RCU to synchronize with changes to the chain.

 *

 *	If the return value of the notifier can be and'ed

 *	with %NOTIFY_STOP_MASK then atomic_notifier_call_chain()

 *	will return immediately, with the return value of

 *	the notifier function which halted execution.

 *	Otherwise the return value is the return value

 *	of the last notifier function called.

/*

 *	Blocking notifier chain routines.  All access to the chain is

 *	synchronized by an rwsem.

/**

 *	blocking_notifier_chain_register - Add notifier to a blocking notifier chain

 *	@nh: Pointer to head of the blocking notifier chain

 *	@n: New entry in notifier chain

 *

 *	Adds a notifier to a blocking notifier chain.

 *	Must be called in process context.

 *

 *	Currently always returns zero.

	/*

	 * This code gets used during boot-up, when task switching is

	 * not yet working and interrupts must remain disabled.  At

	 * such times we must not call down_write().

/**

 *	blocking_notifier_chain_unregister - Remove notifier from a blocking notifier chain

 *	@nh: Pointer to head of the blocking notifier chain

 *	@n: Entry to remove from notifier chain

 *

 *	Removes a notifier from a blocking notifier chain.

 *	Must be called from process context.

 *

 *	Returns zero on success or %-ENOENT on failure.

	/*

	 * This code gets used during boot-up, when task switching is

	 * not yet working and interrupts must remain disabled.  At

	 * such times we must not call down_write().

	/*

	 * We check the head outside the lock, but if this access is

	 * racy then it does not matter what the result of the test

	 * is, we re-check the list after having taken the lock anyway:

/**

 *	blocking_notifier_call_chain - Call functions in a blocking notifier chain

 *	@nh: Pointer to head of the blocking notifier chain

 *	@val: Value passed unmodified to notifier function

 *	@v: Pointer passed unmodified to notifier function

 *

 *	Calls each function in a notifier chain in turn.  The functions

 *	run in a process context, so they are allowed to block.

 *

 *	If the return value of the notifier can be and'ed

 *	with %NOTIFY_STOP_MASK then blocking_notifier_call_chain()

 *	will return immediately, with the return value of

 *	the notifier function which halted execution.

 *	Otherwise the return value is the return value

 *	of the last notifier function called.

	/*

	 * We check the head outside the lock, but if this access is

	 * racy then it does not matter what the result of the test

	 * is, we re-check the list after having taken the lock anyway:

/*

 *	Raw notifier chain routines.  There is no protection;

 *	the caller must provide it.  Use at your own risk!

/**

 *	raw_notifier_chain_register - Add notifier to a raw notifier chain

 *	@nh: Pointer to head of the raw notifier chain

 *	@n: New entry in notifier chain

 *

 *	Adds a notifier to a raw notifier chain.

 *	All locking must be provided by the caller.

 *

 *	Currently always returns zero.

/**

 *	raw_notifier_chain_unregister - Remove notifier from a raw notifier chain

 *	@nh: Pointer to head of the raw notifier chain

 *	@n: Entry to remove from notifier chain

 *

 *	Removes a notifier from a raw notifier chain.

 *	All locking must be provided by the caller.

 *

 *	Returns zero on success or %-ENOENT on failure.

/**

 *	raw_notifier_call_chain - Call functions in a raw notifier chain

 *	@nh: Pointer to head of the raw notifier chain

 *	@val: Value passed unmodified to notifier function

 *	@v: Pointer passed unmodified to notifier function

 *

 *	Calls each function in a notifier chain in turn.  The functions

 *	run in an undefined context.

 *	All locking must be provided by the caller.

 *

 *	If the return value of the notifier can be and'ed

 *	with %NOTIFY_STOP_MASK then raw_notifier_call_chain()

 *	will return immediately, with the return value of

 *	the notifier function which halted execution.

 *	Otherwise the return value is the return value

 *	of the last notifier function called.

/*

 *	SRCU notifier chain routines.    Registration and unregistration

 *	use a mutex, and call_chain is synchronized by SRCU (no locks).

/**

 *	srcu_notifier_chain_register - Add notifier to an SRCU notifier chain

 *	@nh: Pointer to head of the SRCU notifier chain

 *	@n: New entry in notifier chain

 *

 *	Adds a notifier to an SRCU notifier chain.

 *	Must be called in process context.

 *

 *	Currently always returns zero.

	/*

	 * This code gets used during boot-up, when task switching is

	 * not yet working and interrupts must remain disabled.  At

	 * such times we must not call mutex_lock().

/**

 *	srcu_notifier_chain_unregister - Remove notifier from an SRCU notifier chain

 *	@nh: Pointer to head of the SRCU notifier chain

 *	@n: Entry to remove from notifier chain

 *

 *	Removes a notifier from an SRCU notifier chain.

 *	Must be called from process context.

 *

 *	Returns zero on success or %-ENOENT on failure.

	/*

	 * This code gets used during boot-up, when task switching is

	 * not yet working and interrupts must remain disabled.  At

	 * such times we must not call mutex_lock().

/**

 *	srcu_notifier_call_chain - Call functions in an SRCU notifier chain

 *	@nh: Pointer to head of the SRCU notifier chain

 *	@val: Value passed unmodified to notifier function

 *	@v: Pointer passed unmodified to notifier function

 *

 *	Calls each function in a notifier chain in turn.  The functions

 *	run in a process context, so they are allowed to block.

 *

 *	If the return value of the notifier can be and'ed

 *	with %NOTIFY_STOP_MASK then srcu_notifier_call_chain()

 *	will return immediately, with the return value of

 *	the notifier function which halted execution.

 *	Otherwise the return value is the return value

 *	of the last notifier function called.

/**

 *	srcu_init_notifier_head - Initialize an SRCU notifier head

 *	@nh: Pointer to head of the srcu notifier chain

 *

 *	Unlike other sorts of notifier heads, SRCU notifier heads require

 *	dynamic initialization.  Be sure to call this routine before

 *	calling any of the other SRCU notifier routines for this head.

 *

 *	If an SRCU notifier head is deallocated, it must first be cleaned

 *	up by calling srcu_cleanup_notifier_head().  Otherwise the head's

 *	per-cpu data (used by the SRCU mechanism) will leak.

 CONFIG_SRCU */

 SPDX-License-Identifier: GPL-2.0

/*

 * Implement CPU time clocks for the POSIX clock interface.

/*

 * Called after updating RLIMIT_CPU to run cpu timer and update

 * tsk->signal->posix_cputimers.bases[clock].nextevt expiration cache if

 * necessary. Needs siglock protection since other code may update the

 * expiration cache as well.

/*

 * Functions for validating access to tasks.

	/*

	 * If the encoded PID is 0, then the timer is targeted at current

	 * or the process to which current belongs.

	/*

	 * For clock_gettime(PROCESS) allow finding the process by

	 * with the pid of the current task.  The code needs the tgid

	 * of the process so that pid_task(pid, PIDTYPE_TGID) can be

	 * used to find the process.

	/*

	 * For processes require that pid identifies a process.

/*

 * Update expiry time from increment, and increase overrun count,

 * given the current clock sample.

 Don't use (incr*2 < delta), incr*2 might overflow. */

 Check whether all cache entries contain U64_MAX, i.e. eternal expiry time */

			/*

			 * If sched_clock is using a cycle counter, we

			 * don't have any idea of its true resolution

			 * exported, but it is much more than 1s/HZ.

	/*

	 * You can never reset a CPU clock, but we check for other errors

	 * in the call before failing with EPERM.

/*

 * Sample a per-thread clock for the given task. clkid is validated.

/*

 * Set cputime to sum_cputime if sum_cputime > cputime. Use cmpxchg

 * to avoid race conditions with concurrent updates to cputime.

/**

 * thread_group_sample_cputime - Sample cputime for a given task

 * @tsk:	Task for which cputime needs to be started

 * @samples:	Storage for time samples

 *

 * Called from sys_getitimer() to calculate the expiry time of an active

 * timer. That means group cputime accounting is already active. Called

 * with task sighand lock held.

 *

 * Updates @times with an uptodate sample of the thread group cputimes.

/**

 * thread_group_start_cputime - Start cputime and return a sample

 * @tsk:	Task for which cputime needs to be started

 * @samples:	Storage for time samples

 *

 * The thread group cputime accounting is avoided when there are no posix

 * CPU timers armed. Before starting a timer it's required to check whether

 * the time accounting is active. If not, a full update of the atomic

 * accounting store needs to be done and the accounting enabled.

 *

 * Updates @times with an uptodate sample of the thread group cputimes.

 Check if cputimer isn't running. This is accessed without locking. */

		/*

		 * The POSIX timer interface allows for absolute time expiry

		 * values through the TIMER_ABSTIME flag, therefore we have

		 * to synchronize the timer to the clock every time we start it.

		/*

		 * We're setting timers_active without a lock. Ensure this

		 * only gets written to in one operation. We set it after

		 * update_gt_cputime() as a small optimization, but

		 * barriers are not required because update_gt_cputime()

		 * can handle concurrent updates.

/*

 * Sample a process (thread group) clock for the given task clkid. If the

 * group's cputime accounting is already enabled, read the atomic

 * store. Otherwise a full update is required.  clkid is already validated.

/*

 * Validate the clockid_t for a new CPU-clock timer, and initialize the timer.

 * This is called from sys_timer_create() and do_cpu_nanosleep() with the

 * new timer already all-zeros initialized.

	/*

	 * If posix timer expiry is handled in task work context then

	 * timer::it_lock can be taken without disabling interrupts as all

	 * other locking happens in task context. This requires a separate

	 * lock class key otherwise regular posix timer expiry would record

	 * the lock class being taken in interrupt context and generate a

	 * false positive warning.

/*

 * Force recalculating the base earliest expiration on the next tick.

 * This will also re-evaluate the need to keep around the process wide

 * cputime counter and tick dependency and eventually shut these down

 * if necessary.

/*

 * Dequeue the timer and reset the base if it was its earliest expiration.

 * It makes sure the next tick recalculates the base next expiration so we

 * don't keep the costly process wide cputime counter around for a random

 * amount of time, along with the tick dependency.

 *

 * If another timer gets queued between this and the next tick, its

 * expiration will update the base next event if necessary on the next

 * tick.

/*

 * Clean up a CPU-clock timer that is about to be destroyed.

 * This is called from timer deletion with the timer already locked.

 * If we return TIMER_RETRY, it's necessary to release the timer's lock

 * and try again.  (This happens when the timer is in the middle of firing.)

	/*

	 * Protect against sighand release/switch in exit/exec and process/

	 * thread timer list entry concurrent read/writes.

		/*

		 * This raced with the reaping of the task. The exit cleanup

		 * should have removed this timer from the timer queue.

/*

 * Clean out CPU timers which are still armed when a thread exits. The

 * timers are only removed from the list. No other updates are done. The

 * corresponding posix timers are still accessible, but cannot be rearmed.

 *

 * This must be called with the siglock held.

/*

 * These are both called with the siglock held, when the current thread

 * is being reaped.  When the final (leader) thread in the group is reaped,

 * posix_cpu_timers_exit_group will be called after posix_cpu_timers_exit.

/*

 * Insert the timer on the appropriate list before any timers that

 * expire later.  This must be called with the sighand lock held.

	/*

	 * We are the new earliest-expiring POSIX 1.b timer, hence

	 * need to update expiration cache. Take into account that

	 * for process timers we share expiration cache with itimers

	 * and RLIMIT_CPU and for thread timers with RLIMIT_RTTIME.

/*

 * The timer is locked, fire it and arrange for its reload.

		/*

		 * User don't want any signal.

		/*

		 * This a special case for clock_nanosleep,

		 * not a normal timer from sys_timer_create.

		/*

		 * One-shot timer.  Clear it as soon as it's fired.

		/*

		 * The signal did not get queued because the signal

		 * was ignored, so we won't get any callback to

		 * reload the timer.  But we need to keep it

		 * ticking in case the signal is deliverable next time.

/*

 * Guts of sys_timer_settime for CPU timers.

 * This is called with the timer locked and interrupts disabled.

 * If we return TIMER_RETRY, it's necessary to release the timer's lock

 * and try again.  (This happens when the timer is in the middle of firing.)

		/*

		 * If p has just been reaped, we can no

		 * longer get any information about it at all.

	/*

	 * Use the to_ktime conversion because that clamps the maximum

	 * value to KTIME_MAX and avoid multiplication overflows.

	/*

	 * Protect against sighand release/switch in exit/exec and p->cpu_timers

	 * and p->signal->cpu_timers read/write in arm_timer()

	/*

	 * If p has just been reaped, we can no

	 * longer get any information about it at all.

	/*

	 * Disarm any old timer after extracting its expiry time.

	/*

	 * We need to sample the current value to convert the new

	 * value from to relative and absolute, and to convert the

	 * old value from absolute to relative.  To set a process

	 * timer, we need a sample to balance the thread expiry

	 * times (in arm_timer).  With an absolute time, we must

	 * check if it's already passed.  In short, we need a sample.

			/*

			 * Update the timer in case it has overrun already.

			 * If it has, we'll report it as having overrun and

			 * with the next reloaded timer already ticking,

			 * though we are swallowing that pending

			 * notification here to install the new setting.

		/*

		 * We are colliding with the timer actually firing.

		 * Punt after filling in the timer's old value, and

		 * disable this firing since we are already reporting

		 * it as an overrun (thanks to bump_cpu_timer above).

	/*

	 * Install the new expiry time (or zero).

	 * For a timer with no notification action, we don't actually

	 * arm the timer (we'll just fake it for timer_gettime).

	/*

	 * Install the new reload setting, and

	 * set up the signal and overrun bookkeeping.

	/*

	 * This acts as a modification timestamp for the timer,

	 * so any automatic reload attempt will punt on seeing

	 * that we have reset the timer manually.

			/*

			 * The designated time already passed, so we notify

			 * immediately, even if the thread never runs to

			 * accumulate more time on this clock.

		/*

		 * Make sure we don't keep around the process wide cputime

		 * counter or the tick dependency if they are not necessary.

	/*

	 * Easy part: convert the reload time.

	/*

	 * Sample the clock to take the difference with the expiry time.

		/*

		 * The timer should have expired already, but the firing

		 * hasn't taken place yet.  Say it's just about to expire.

 Limit the number of timers to expire at once */

/*

 * Check for any per-thread CPU timers that have fired and move them off

 * the tsk->cpu_timers[N] list onto the firing list.  Here we update the

 * tsk->it_*_expires values to reflect the remaining thread CPU timers.

	/*

	 * Check for the special case thread timers.

 Task RT timeout is accounted in jiffies. RTTIME is usec */

 At the hard limit, send SIGKILL. No further action. */

 At the soft limit, send a SIGXCPU every second */

 Turn off the active flag. This is done without locking. */

/*

 * Check for any per-thread CPU timers that have fired and move them

 * off the tsk->*_timers list onto the firing list.  Per-thread timers

 * have already been taken off.

	/*

	 * If there are no active process wide timers (POSIX 1.b, itimers,

	 * RLIMIT_CPU) nothing to check. Also skip the process wide timer

	 * processing when there is already another task handling them.

	/*

	 * Signify that a thread is checking for process timers.

	 * Write access to this field is protected by the sighand lock.

	/*

	 * Collect the current process totals. Group accounting is active

	 * so the sample can be taken directly.

	/*

	 * Check for the special case process timers.

 RLIMIT_CPU is in seconds. Samples are nanoseconds */

 At the hard limit, send SIGKILL. No further action. */

 At the soft limit, send a SIGXCPU every second */

 Update the expiry cache */

/*

 * This is called from the signal code (via posixtimer_rearm)

 * when the last timer signal was delivered and we have to reload the timer.

 Protect timer list r/w in arm_timer() */

	/*

	 * Fetch the current sample and update the timer's expiry time.

	/*

	 * Now re-arm for the new expiry time.

/**

 * task_cputimers_expired - Check whether posix CPU timers are expired

 *

 * @samples:	Array of current samples for the CPUCLOCK clocks

 * @pct:	Pointer to a posix_cputimers container

 *

 * Returns true if any member of @samples is greater than the corresponding

 * member of @pct->bases[CLK].nextevt. False otherwise

/**

 * fastpath_timer_check - POSIX CPU timers fast path.

 *

 * @tsk:	The task (thread) being checked.

 *

 * Check the task and thread group timers.  If both are zero (there are no

 * timers set) return false.  Otherwise snapshot the task and thread group

 * timers and compare them with the corresponding expiration times.  Return

 * true if a timer has expired, else return false.

	/*

	 * Check if thread group timers expired when timers are active and

	 * no other thread in the group is already handling expiry for

	 * thread group cputimers. These fields are read without the

	 * sighand lock. However, this is fine because this is meant to be

	 * a fastpath heuristic to determine whether we should try to

	 * acquire the sighand lock to handle timer expiry.

	 *

	 * In the worst case scenario, if concurrently timers_active is set

	 * or expiry_active is cleared, but the current thread doesn't see

	 * the change yet, the timer checks are delayed until the next

	 * thread in the group gets a scheduler interrupt to handle the

	 * timer. This isn't an issue in practice because these types of

	 * delays with signals actually getting sent are expected.

/*

 * Clear existing posix CPU timers task work.

	/*

	 * A copied work entry from the old task is not meaningful, clear it.

	 * N.B. init_task_work will not do this.

/*

 * Initialize posix CPU timers task work in init task. Out of line to

 * keep the callback static and to avoid header recursion hell.

/*

 * Note: All operations on tsk->posix_cputimer_work.scheduled happen either

 * in hard interrupt context or in task context with interrupts

 * disabled. Aside of that the writer/reader interaction is always in the

 * context of the current task, which means they are strict per CPU.

 Schedule task work to actually expire the timers */

	/*

	 * On !RT kernels interrupts are disabled while collecting expired

	 * timers, so no tick can happen and the fast path check can be

	 * reenabled without further checks.

	/*

	 * On RT enabled kernels ticks can happen while the expired timers

	 * are collected under sighand lock. But any tick which observes

	 * the CPUTIMERS_WORK_SCHEDULED bit set, does not run the fastpath

	 * checks. So reenabling the tick work has do be done carefully:

	 *

	 * Disable interrupts and run the fast path check if jiffies have

	 * advanced since the collecting of expired timers started. If

	 * jiffies have not advanced or the fast path check did not find

	 * newly expired timers, reenable the fast path check in the timer

	 * interrupt. If there are newly expired timers, return false and

	 * let the collection loop repeat.

 CONFIG_POSIX_CPU_TIMERS_TASK_WORK */

 CONFIG_POSIX_CPU_TIMERS_TASK_WORK */

		/*

		 * On RT locking sighand lock does not disable interrupts,

		 * so this needs to be careful vs. ticks. Store the current

		 * jiffies value.

		/*

		 * Here we take off tsk->signal->cpu_timers[N] and

		 * tsk->cpu_timers[N] all the timers that are firing, and

		 * put them on the firing list.

		/*

		 * The above timer checks have updated the expiry cache and

		 * because nothing can have queued or modified timers after

		 * sighand lock was taken above it is guaranteed to be

		 * consistent. So the next timer interrupt fastpath check

		 * will find valid data.

		 *

		 * If timer expiry runs in the timer interrupt context then

		 * the loop is not relevant as timers will be directly

		 * expired in interrupt context. The stub function below

		 * returns always true which allows the compiler to

		 * optimize the loop out.

		 *

		 * If timer expiry is deferred to task work context then

		 * the following rules apply:

		 *

		 * - On !RT kernels no tick can have happened on this CPU

		 *   after sighand lock was acquired because interrupts are

		 *   disabled. So reenabling task work before dropping

		 *   sighand lock and reenabling interrupts is race free.

		 *

		 * - On RT kernels ticks might have happened but the tick

		 *   work ignored posix CPU timer handling because the

		 *   CPUTIMERS_WORK_SCHEDULED bit is set. Reenabling work

		 *   must be done very carefully including a check whether

		 *   ticks have happened since the start of the timer

		 *   expiry checks. posix_cpu_timers_enable_work() takes

		 *   care of that and eventually lets the expiry checks

		 *   run again.

	/*

	 * We must release sighand lock before taking any timer's lock.

	 * There is a potential race with timer deletion here, as the

	 * siglock now protects our private firing list.  We have set

	 * the firing flag in each timer, so that a deletion attempt

	 * that gets the timer lock before we do will give it up and

	 * spin until we've taken care of that timer below.

	/*

	 * Now that all the timers on our list have the firing flag,

	 * no one will touch their list entries but us.  We'll take

	 * each timer's lock before clearing its firing flag, so no

	 * timer call will interfere.

		/*

		 * spin_lock() is sufficient here even independent of the

		 * expiry context. If expiry happens in hard interrupt

		 * context it's obvious. For task work context it's safe

		 * because all other operations on timer::it_lock happen in

		 * task context (syscall or exit).

		/*

		 * The firing flag is -1 if we collided with a reset

		 * of the timer, which already reported this

		 * almost-firing as an overrun.  So don't generate an event.

/*

 * This is called from the timer interrupt handler.  The irq handler has

 * already updated our counts.  We need to check if any timers fire now.

 * Interrupts are disabled.

	/*

	 * If the actual expiry is deferred to task work context and the

	 * work is already scheduled there is no point to do anything here.

	/*

	 * The fast path checks that there are no expired thread or thread

	 * group timers.  If that's so, just return.

/*

 * Set one of the process-wide special case CPU timers or RLIMIT_CPU.

 * The tsk->sighand->siglock must be held by the caller.

		/*

		 * We are setting itimer. The *oldval is absolute and we update

		 * it to be relative, *newval argument is relative and we update

		 * it to be absolute.

 Just about to fire. */

	/*

	 * Update expiration cache if this is the earliest timer. CPUCLOCK_PROF

	 * expiry cache is also used by RLIMIT_CPU!.

	/*

	 * Set up a temporary timer and then wait for it to go off.

				/*

				 * Our timer fired and was reset, below

				 * deletion can not fail.

			/*

			 * Block until cpu_timer_fire (or a signal) wakes us.

		/*

		 * We were interrupted by a signal.

			/*

			 * Timer is now unarmed, deletion can not fail.

			/*

			 * We need to handle case when timer was or is in the

			 * middle of firing. In other cases we already freed

			 * resources.

			/*

			 * It actually did fire already.

		/*

		 * Report back to the user the time still remaining.

	/*

	 * Diagnose required errors first.

 SPDX-License-Identifier: GPL-2.0

/*

 * Alarmtimer interface

 *

 * This interface provides a timer which is similar to hrtimers,

 * but triggers a RTC alarm if the box is suspend.

 *

 * This interface is influenced by the Android RTC Alarm timer

 * interface.

 *

 * Copyright (C) 2010 IBM Corporation

 *

 * Author: John Stultz <john.stultz@linaro.org>

/**

 * struct alarm_base - Alarm timer bases

 * @lock:		Lock for syncrhonized access to the base

 * @timerqueue:		Timerqueue head managing the list of events

 * @get_ktime:		Function to read the time correlating to the base

 * @get_timespec:	Function to read the namespace time correlating to the base

 * @base_clockid:	clockid for the base

 freezer information to handle clock_nanosleep triggered wakeups */

 rtc timer and device for setting alarm wakeups at suspend */

/**

 * alarmtimer_get_rtcdev - Return selected rtcdevice

 *

 * This function returns the rtc device to use for wakealarms.

 hold a reference so it doesn't go away */

/**

 * alarmtimer_enqueue - Adds an alarm timer to an alarm_base timerqueue

 * @base: pointer to the base where the timer is being run

 * @alarm: pointer to alarm being enqueued.

 *

 * Adds alarm to a alarm_base timerqueue

 *

 * Must hold base->lock when calling.

/**

 * alarmtimer_dequeue - Removes an alarm timer from an alarm_base timerqueue

 * @base: pointer to the base where the timer is running

 * @alarm: pointer to alarm being removed

 *

 * Removes alarm to a alarm_base timerqueue

 *

 * Must hold base->lock when calling.

/**

 * alarmtimer_fired - Handles alarm hrtimer being fired.

 * @timer: pointer to hrtimer being run

 *

 * When a alarm timer fires, this runs through the timerqueue to

 * see which alarms expired, and runs those. If there are more alarm

 * timers queued for the future, we set the hrtimer to fire when

 * the next future alarm timer expires.

/**

 * alarmtimer_suspend - Suspend time callback

 * @dev: unused

 *

 * When we are going into suspend, we look through the bases

 * to see which is the soonest timer to expire. We then

 * set an rtc timer to fire that far into the future, which

 * will wake us from suspend.

 If we have no rtcdev, just return */

 Find the soonest timer to expire*/

 Setup an rtc timer to fire that far in the future */

 Set alarm, if in the past reject suspend briefly to handle */

/**

 * alarm_init - Initialize an alarm structure

 * @alarm: ptr to alarm to be initialized

 * @type: the type of the alarm

 * @function: callback that is run when the alarm fires

/**

 * alarm_start - Sets an absolute alarm to fire

 * @alarm: ptr to alarm to set

 * @start: time to run the alarm

/**

 * alarm_start_relative - Sets a relative alarm to fire

 * @alarm: ptr to alarm to set

 * @start: time relative to now to run the alarm

/**

 * alarm_try_to_cancel - Tries to cancel an alarm timer

 * @alarm: ptr to alarm to be canceled

 *

 * Returns 1 if the timer was canceled, 0 if it was not running,

 * and -1 if the callback was running

/**

 * alarm_cancel - Spins trying to cancel an alarm timer until it is done

 * @alarm: ptr to alarm to be canceled

 *

 * Returns 1 if the timer was canceled, 0 if it was not active.

		/*

		 * This (and the ktime_add() below) is the

		 * correction for exact:

/**

 * clock2alarm - helper that converts from clockid to alarmtypes

 * @clockid: clockid.

/**

 * alarm_handle_timer - Callback for posix timers

 * @alarm: alarm that fired

 * @now: time at the timer expiration

 *

 * Posix timer callback for expired alarm timers.

 *

 * Return: whether the timer is to be restarted

		/*

		 * Handle ignored signals and rearm the timer. This will go

		 * away once we handle ignored signals proper.

/**

 * alarm_timer_rearm - Posix timer callback for rearming timer

 * @timr:	Pointer to the posixtimer data struct

/**

 * alarm_timer_forward - Posix timer callback for forwarding timer

 * @timr:	Pointer to the posixtimer data struct

 * @now:	Current time to forward the timer against

/**

 * alarm_timer_remaining - Posix timer callback to retrieve remaining time

 * @timr:	Pointer to the posixtimer data struct

 * @now:	Current time to calculate against

/**

 * alarm_timer_try_to_cancel - Posix timer callback to cancel a timer

 * @timr:	Pointer to the posixtimer data struct

/**

 * alarm_timer_wait_running - Posix timer callback to wait for a timer

 * @timr:	Pointer to the posixtimer data struct

 *

 * Called from the core code when timer cancel detected that the callback

 * is running. @timr is unlocked and rcu read lock is held to prevent it

 * from being freed.

/**

 * alarm_timer_arm - Posix timer callback to arm a timer

 * @timr:	Pointer to the posixtimer data struct

 * @expires:	The new expiry time

 * @absolute:	Expiry value is absolute time

 * @sigev_none:	Posix timer does not deliver signals

/**

 * alarm_clock_getres - posix getres interface

 * @which_clock: clockid

 * @tp: timespec to fill

 *

 * Returns the granularity of underlying alarm base clock

/**

 * alarm_clock_get_timespec - posix clock_get_timespec interface

 * @which_clock: clockid

 * @tp: timespec to fill.

 *

 * Provides the underlying alarm base time in a tasks time namespace.

/**

 * alarm_clock_get_ktime - posix clock_get_ktime interface

 * @which_clock: clockid

 *

 * Provides the underlying alarm base time in the root namespace.

/**

 * alarm_timer_create - posix timer_create interface

 * @new_timer: k_itimer pointer to manage

 *

 * Initializes the k_itimer structure.

/**

 * alarmtimer_nsleep_wakeup - Wakeup function for alarm_timer_nsleep

 * @alarm: ptr to alarm that fired

 * @now: time at the timer expiration

 *

 * Wakes up the task that set the alarmtimer

 *

 * Return: ALARMTIMER_NORESTART

/**

 * alarmtimer_do_nsleep - Internal alarmtimer nsleep implementation

 * @alarm: ptr to alarmtimer

 * @absexp: absolute expiration time

 * @type: alarm type (BOOTTIME/REALTIME).

 *

 * Sets the alarm timer and sleeps until it is fired or interrupted.

/**

 * alarm_timer_nsleep_restart - restartblock alarmtimer nsleep

 * @restart: ptr to restart block

 *

 * Handles restarted clock_nanosleep calls

/**

 * alarm_timer_nsleep - alarmtimer nanosleep

 * @which_clock: clockid

 * @flags: determines abstime or relative

 * @tsreq: requested sleep time (abs or rel)

 *

 * Handles clock_nanosleep calls against _ALARM clockids

 Convert (if necessary) to absolute time */

 abs timers don't set remaining time or restart */

 CONFIG_POSIX_TIMERS */

 Suspend hook structures */

/**

 * alarmtimer_init - Initialize alarm timer code

 *

 * This function initializes the alarm bases and registers

 * the posix clock ids.

 Initialize alarm bases */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains functions which manage clock event devices.

 *

 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner

 The registered clock event devices */

 Protection for the above */

 Protection for unbind operations */

	/*

	 * Upper bound sanity check. If the backwards conversion is

	 * not equal latch, we know that the above shift overflowed.

	/*

	 * Scaled math oddities:

	 *

	 * For mult <= (1 << shift) we can safely add mult - 1 to

	 * prevent integer rounding loss. So the backwards conversion

	 * from nsec to device ticks will be correct.

	 *

	 * For mult > (1 << shift), i.e. device frequency is > 1GHz we

	 * need to be careful. Adding mult - 1 will result in a value

	 * which when converted back to device ticks can be larger

	 * than latch by up to (mult - 1) >> shift. For the min_delta

	 * calculation we still want to apply this in order to stay

	 * above the minimum device ticks limit. For the upper limit

	 * we would end up with a latch value larger than the upper

	 * limit of the device, so we omit the add to stay below the

	 * device upper boundary.

	 *

	 * Also omit the add if it would overflow the u64 boundary.

 Deltas less than 1usec are pointless noise */

/**

 * clockevents_delta2ns - Convert a latch value (device ticks) to nanoseconds

 * @latch:	value to convert

 * @evt:	pointer to clock event device descriptor

 *

 * Math helper, returns latch value converted to nanoseconds (bound checked)

 Transition with new state-specific callbacks */

 The clockevent device is getting replaced. Shut it down. */

 Core internal bug */

 Core internal bug */

 Core internal bug */

/**

 * clockevents_switch_state - set the operating state of a clock event device

 * @dev:	device to modify

 * @state:	new state

 *

 * Must be called with interrupts disabled !

		/*

		 * A nsec2cyc multiplicator of 0 is invalid and we'd crash

		 * on it, so fix it up and emit a warning:

/**

 * clockevents_shutdown - shutdown the device and clear next_event

 * @dev:	device to shutdown

/**

 * clockevents_tick_resume -	Resume the tick device before using it again

 * @dev:			device to resume

 Limit min_delta to a jiffie */

/**

 * clockevents_increase_min_delta - raise minimum delta of a clock event device

 * @dev:       device to increase the minimum delta

 *

 * Returns 0 on success, -ETIME when the minimum delta reached the limit.

 Nothing to do if we already reached the limit */

/**

 * clockevents_program_min_delta - Set clock event device to the minimum delay.

 * @dev:	device to program

 *

 * Returns 0 on success, -ETIME when the retry loop failed.

			/*

			 * We tried 3 times to program the device with the

			 * given min_delta_ns. Try to increase the minimum

			 * delta, if that fails as well get out of here.

 CONFIG_GENERIC_CLOCKEVENTS_MIN_ADJUST */

/**

 * clockevents_program_min_delta - Set clock event device to the minimum delay.

 * @dev:	device to program

 *

 * Returns 0 on success, -ETIME when the retry loop failed.

 CONFIG_GENERIC_CLOCKEVENTS_MIN_ADJUST */

/**

 * clockevents_program_event - Reprogram the clock event device.

 * @dev:	device to program

 * @expires:	absolute expiry time (monotonic clock)

 * @force:	program minimum delay if expires can not be set

 *

 * Returns 0 on success, -ETIME when the event is in the past.

 We must be in ONESHOT state here */

 Shortcut for clockevent devices that can deal with ktime. */

/*

 * Called after a notify add to make devices available which were

 * released from the notifier call.

/*

 * Try to install a replacement clock event device

/*

 * Called with clockevents_mutex and clockevents_lock held

 Fast track. Device is unused */

/*

 * SMP function call to unbind a device

/*

 * Issues smp function call to unbind a per cpu device. Called with

 * clockevents_mutex held.

/*

 * Unbind a clockevents device.

/**

 * clockevents_register_device - register a clock event device

 * @dev:	device to register

 Initialize state to DETACHED */

	/*

	 * Calculate the maximum number of seconds we can sleep. Limit

	 * to 10 minutes for hardware which can program more than

	 * 32bit ticks so we still get reasonable conversion values.

/**

 * clockevents_config_and_register - Configure and register a clock event device

 * @dev:	device to register

 * @freq:	The clock frequency

 * @min_delta:	The minimum clock ticks to program in oneshot mode

 * @max_delta:	The maximum clock ticks to program in oneshot mode

 *

 * min/max_delta can be 0 for devices which do not support oneshot mode.

/**

 * clockevents_update_freq - Update frequency and reprogram a clock event device.

 * @dev:	device to modify

 * @freq:	new device frequency

 *

 * Reconfigure and reprogram a clock event device in oneshot

 * mode. Must be called on the cpu for which the device delivers per

 * cpu timer events. If called for the broadcast device the core takes

 * care of serialization.

 *

 * Returns 0 on success, -ETIME when the event is in the past.

/*

 * Noop handler when we shut down an event device

/**

 * clockevents_exchange_device - release and request clock devices

 * @old:	device to release (can be NULL)

 * @new:	device to request (can be NULL)

 *

 * Called from various tick functions with clockevents_lock held and

 * interrupts disabled.

	/*

	 * Caller releases a clock event device. We queue it into the

	 * released list and do a notify add later.

/**

 * clockevents_suspend - suspend clock devices

/**

 * clockevents_resume - resume clock devices

/**

 * tick_offline_cpu - Take CPU out of the broadcast mechanism

 * @cpu:	The outgoing CPU

 *

 * Called on the outgoing CPU after it took itself offline.

/**

 * tick_cleanup_dead_cpu - Cleanup the tick and clockevents of a dead cpu

 * @cpu:	The dead CPU

	/*

	 * Unregister the clock event devices which were

	 * released from the users in the notify chain.

	/*

	 * Now check whether the CPU has left unused per cpu devices

 We don't support the abomination of removable broadcast devices */

	/*

	 * We hold clockevents_mutex, so ce can't go away

 SYSFS */

 SPDX-License-Identifier: GPL-2.0

/*

 * NTP state machine interfaces and logic.

 *

 * This code was mainly moved from kernel/timer.c and kernel/time.c

 * Please see those files for relevant copyright info and historical

 * changelogs.

/*

 * NTP timekeeping variables:

 *

 * Note: All of the NTP state is protected by the timekeeping locks.

 USER_HZ period (usecs): */

 SHIFTED_HZ period (nsecs): */

 usecs */

/*

 * phase-lock loop variables

/*

 * clock synchronization status

 *

 * (TIME_ERROR prevents overwriting the CMOS clock)

 clock status bits:							*/

 time adjustment (nsecs):						*/

 pll time constant:							*/

 maximum error (usecs):						*/

 estimated error (usecs):						*/

 frequency offset (scaled nsecs/secs):				*/

 time at last adjustment (secs):					*/

 constant (boot-param configurable) NTP tick adjustment (upscaled)	*/

 second value of the next pending leapsecond, or TIME64_MAX if no leap */

/*

 * The following variables are used when a pulse-per-second (PPS) signal

 * is available. They establish the engineering parameters of the clock

 * discipline loop when controlled by the PPS signal.

 PPS signal watchdog max (s) */

 popcorn spike threshold (shift) */

 min freq interval (s) (shift) */

 max freq interval (s) (shift) */

#define PPS_INTCOUNT	4	/* number of consecutive good intervals to

				   increase pps_shift or consecutive bad

 max PPS freq wander (ns/s) */

 signal watchdog counter */

 phase median filter */

 current jitter (ns) */

 beginning of the last freq interval */

 current interval duration (s) (shift) */

 interval counter */

 frequency offset (scaled ns/s) */

 current stability (scaled ns/s) */

/*

 * PPS signal quality monitors

 calibration intervals */

 jitter limit exceeded */

 stability limit exceeded */

 calibration errors */

/* PPS kernel consumer compensates the whole phase error immediately.

 * Otherwise, reduce the offset by a fixed factor times the time constant.

	/* the PPS calibration interval may end

/**

 * pps_clear - Clears the PPS state variables

/* Decrease pps_valid to indicate that another second has passed since

 * the last PPS signal. When it reaches 0, indicate that PPS signal is

 * missing.

		/* PPS signal lost when either PPS time or

		 * PPS frequency synchronization requested

		/* PPS jitter exceeded when

		/* PPS wander exceeded or calibration error when

		 * PPS frequency synchronization requested

 !CONFIG_NTP_PPS */

 PPS is not implemented, so these are zero */

 CONFIG_NTP_PPS */

/**

 * ntp_synced - Returns 1 if the NTP status is not UNSYNC

 *

/*

 * NTP methods:

/*

 * Update (tick_length, tick_length_base, tick_nsec), based

 * on (tick_usec, ntp_tick_adj, time_freq):

	/*

	 * Don't wait for the next second_overflow, apply

	 * the change to the tick length immediately:

 Make sure the multiplication below won't overflow */

	/*

	 * Scale the phase adjustment and

	 * clamp to the operating range.

	/*

	 * Select how the frequency is to be controlled

	 * and in which mode (PLL or FLL).

	/*

	 * Clamp update interval to reduce PLL gain with low

	 * sampling rate (e.g. intermittent network connection)

	 * to avoid instability.

/**

 * ntp_clear - Clears the NTP state variables

 stop active adjtime() */

 Clear PPS state variables */

/**

 * ntp_get_next_leap - Returns the next leapsecond in CLOCK_REALTIME ktime_t

 *

 * Provides the time of the next leapsecond against CLOCK_REALTIME in

 * a ktime_t format. Returns KTIME_MAX if no leapsecond is pending.

/*

 * this routine handles the overflow of the microsecond field

 *

 * The tricky bits of code to handle the accurate clock support

 * were provided by Dave Mills (Mills@UDEL.EDU) of NTP fame.

 * They were originally developed for SUN and DEC kernels.

 * All the kudos should go to Dave for this stuff.

 *

 * Also handles leap second processing, and returns leap offset

	/*

	 * Leap second processing. If in leap-insert state at the end of the

	 * day, the system clock is set back one second; if in leap-delete

	 * state, the system clock is set ahead one second.

 Bump the maxerror field */

 Compute the phase adjustment for the next second */

 Check PPS signal */

/*

 * Check whether @now is correct versus the required time to update the RTC

 * and calculate the value which needs to be written to the RTC so that the

 * next seconds increment of the RTC after the write is aligned with the next

 * seconds increment of clock REALTIME.

 *

 * tsched     t1 write(t2.tv_sec - 1sec))	t2 RTC increments seconds

 *

 * t2.tv_nsec == 0

 * tsched = t2 - set_offset_nsec

 * newval = t2 - NSEC_PER_SEC

 *

 * ==> neval = tsched + set_offset_nsec - NSEC_PER_SEC

 *

 * As the execution of this code is not guaranteed to happen exactly at

 * tsched this allows it to happen within a fuzzy region:

 *

 *	abs(now - tsched) < FUZZ

 *

 * If @now is not inside the allowed window the function returns false.

 Allowed error in tv_nsec, arbitrarily set to 5 jiffies in ns. */

 Save NTP synchronized time to the RTC */

 First call might not have the correct offset */

 Store the update offset and let the caller try again */

/*

 * If we have an externally synchronized Linux clock, then update RTC clock

 * accordingly every ~11 minutes. Generally RTCs can only store second

 * precision, but many RTCs will adjust the phase of their second tick to

 * match the moment of update. This infrastructure arranges to call to the RTC

 * set at the correct moment to phase synchronize the RTC second tick over

 * with the kernel clock.

	/*

	 * The default synchronization offset is 500ms for the deprecated

	 * update_persistent_clock64() under the assumption that it uses

	 * the infamous CMOS clock (MC146818).

	/*

	 * Don't update if STA_UNSYNC is set and if ntp_notify_cmos_timer()

	 * managed to schedule the work between the timer firing and the

	 * work being able to rearm the timer. Wait for the timer to expire.

 If @now is not in the allowed window, try again */

 Take timezone adjusted RTCs into account */

 Try the legacy RTC first. */

 Try the RTC class */

	/*

	 * When the work is currently executed but has not yet the timer

	 * rearmed this queues the work immediately again. No big issue,

	 * just a pointless work scheduled.

 CONFIG_GENERIC_CMOS_UPDATE) || defined(CONFIG_RTC_SYSTOHC) */

 !CONFIG_GENERIC_CMOS_UPDATE) || defined(CONFIG_RTC_SYSTOHC) */

/*

 * Propagate a new txc->status value into the NTP state:

 restart PPS frequency calibration */

	/*

	 * If we turn on PLL adjustments then reset the

	 * reference time to current time.

 only set allowed bits */

 update pps_freq */

/*

 * adjtimex mainly allows reading (and writing, if superuser) of

 * kernel time-keeping variables. used by xntpd.

 adjtime() is independent from ntp_adjtime() */

 If there are input parameters, then process them: */

 mostly `TIME_OK' */

 check for errors */

 fill PPS status fields */

 Handle leapsec adjustments */

/* actually struct pps_normtime is good old struct timespec, but it is

 * semantically different (and it is the reason why it was invented):

 * pps_normtime.nsec has a range of ( -NSEC_PER_SEC / 2, NSEC_PER_SEC / 2 ]

 seconds */

 nanoseconds */

/* normalize the timestamp so that nsec is in the

 get current phase correction and jitter */

 TODO: test various filters */

 add the sample to the phase filter */

/* decrease frequency calibration interval length.

 * It is halved after four consecutive unstable intervals.

/* increase frequency calibration interval length.

 * It is doubled after four consecutive stable intervals.

/* update clock frequency based on MONOTONIC_RAW clock PPS signal

 * timestamps

 *

 * At the end of the calibration interval the difference between the

 * first and last MONOTONIC_RAW clock timestamps divided by the length

 * of the interval becomes the frequency update. If the interval was

 * too long, the data are discarded.

 * Returns the difference between old and new frequency values.

 check if the frequency interval was too long */

	/* here the raw frequency offset and wander (stability) is

	 * calculated. If the wander is less than the wander threshold

	 * the interval is increased; otherwise it is decreased.

 good sample */

	/* the stability metric is calculated as the average of recent

	 * frequency changes, but is used only for performance

	 * monitoring

 if enabled, the system clock frequency is updated */

 correct REALTIME clock phase error against PPS signal */

 add the sample to the median filter */

	/* Nominal jitter is due to PPS signal noise. If it exceeds the

	 * threshold, the sample is discarded; otherwise, if so enabled,

	 * the time offset is updated.

 correct the time using the phase offset */

 cancel running adjtime() */

 update jitter */

/*

 * __hardpps() - discipline CPU clock oscillator to external PPS signal

 *

 * This routine is called at each PPS signal arrival in order to

 * discipline the CPU clock oscillator to the PPS signal. It takes two

 * parameters: REALTIME and MONOTONIC_RAW clock timestamps. The former

 * is used to correct clock phase error and the latter is used to

 * correct the frequency.

 *

 * This code is based on David Mills's reference nanokernel

 * implementation. It was mostly rewritten but keeps the same idea.

 clear the error bits, they will be set again if needed */

 indicate signal presence */

	/* when called for the first time,

 ok, now we have a base for frequency calculation */

	/* check that the signal is in the range

 restart the frequency calibration interval */

 signal is ok */

 check if the current frequency interval is finished */

 restart the frequency calibration interval */

 CONFIG_NTP_PPS */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains functions which emulate a local clock-event

 * device via a broadcast event source.

 *

 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner

/*

 * Broadcast support for broken x86 hardware, where the local apic

 * timer stops in C3 state.

/*

 * Debugging: see timer_list.c

/*

 * Start the device in periodic mode

/*

 * Check, if the device can be utilized as broadcast device:

	/*

	 * If we woke up early and the tick was reprogrammed in the

	 * meantime then this may be spurious but harmless.

/*

 * Conditionally install/replace broadcast device

	/*

	 * If the system already runs in oneshot mode, switch the newly

	 * registered broadcast device to oneshot mode explicitly.

	/*

	 * Inform all cpus about this. We might be in a situation

	 * where we did not switch to oneshot mode because the per cpu

	 * devices are affected by CLOCK_EVT_FEAT_C3STOP and the lack

	 * of a oneshot capable broadcast device. Without that

	 * notification the systems stays stuck in periodic mode

	 * forever.

/*

 * Check, if the device is the broadcast device

/*

 * Check, if the device is dysfunctional and a placeholder, which

 * needs to be handled by the broadcast device.

	/*

	 * Devices might be registered with both periodic and oneshot

	 * mode disabled. This signals, that the device needs to be

	 * operated from the broadcast device and is a placeholder for

	 * the cpu local device.

		/*

		 * Clear the broadcast bit for this cpu if the

		 * device is not power state affected.

		/*

		 * Clear the broadcast bit if the CPU is not in

		 * periodic broadcast on state.

			/*

			 * If the system is in oneshot mode we can

			 * unconditionally clear the oneshot mask bit,

			 * because the CPU is running and therefore

			 * not in an idle state which causes the power

			 * state affected device to stop. Let the

			 * caller initialize the device.

			/*

			 * If the system is in periodic mode, check

			 * whether the broadcast device can be

			 * switched off now.

			/*

			 * If we kept the cpu in the broadcast mask,

			 * tell the caller to leave the per cpu device

			 * in shutdown state. The periodic interrupt

			 * is delivered by the broadcast device, if

			 * the broadcast device exists and is not

			 * hrtimer based.

/*

 * Broadcast the event to the cpus, which are set in the mask (mangled).

	/*

	 * Check, if the current cpu is in the mask

		/*

		 * We only run the local handler, if the broadcast

		 * device is not hrtimer based. Otherwise we run into

		 * a hrtimer recursion.

		 *

		 * local timer_interrupt()

		 *   local_handler()

		 *     expire_hrtimers()

		 *       bc_handler()

		 *         local_handler()

		 *	     expire_hrtimers()

		/*

		 * It might be necessary to actually check whether the devices

		 * have different broadcast functions. For now, just use the

		 * one of the first device. This works as long as we have this

		 * misfeature only on x86 (lapic)

/*

 * Periodic broadcast:

 * - invoke the broadcast handlers

/*

 * Event handler for periodic broadcast ticks

 Handle spurious interrupts gracefully */

	/*

	 * We run the handler of the local cpu after dropping

	 * tick_broadcast_lock because the handler might deadlock when

	 * trying to switch to oneshot mode.

/**

 * tick_broadcast_control - Enable/disable or force broadcast mode

 * @mode:	The selected broadcast mode

 *

 * Called when the system enters a state where affected tick devices

 * might stop. Note: TICK_BROADCAST_FORCE cannot be undone.

 Protects also the local clockevent device. */

	/*

	 * Is the device not affected by the powerstate ?

			/*

			 * Only shutdown the cpu local device, if:

			 *

			 * - the broadcast device exists

			 * - the broadcast device is not a hrtimer based one

			 * - the broadcast device is in periodic mode to

			 *   avoid a hiccup during switch to oneshot mode

/*

 * Set the periodic handler depending on broadcast on/off

/*

 * Remove a CPU from broadcasting

/*

 * This is called from tick_resume_local() on a resuming CPU. That's

 * called from the core resume function, tick_unfreeze() and the magic XEN

 * resume hackery.

 *

 * In none of these cases the broadcast device mode can change and the

 * bit of the resuming CPU in the broadcast mask is safe as well.

/*

 * Exposed for debugging: see timer_list.c

/*

 * Called before going idle with interrupts disabled. Checks whether a

 * broadcast event from the other core is about to happen. We detected

 * that in tick_broadcast_oneshot_control(). The callsite can use this

 * to avoid a deep idle transition as we are about to get the

 * broadcast IPI right away.

/*

 * Set broadcast interrupt affinity

/*

 * Called from irq_enter() when idle was interrupted to reenable the

 * per cpu device.

		/*

		 * We might be in the middle of switching over from

		 * periodic to oneshot. If the CPU has not yet

		 * switched over, leave the device alone.

/*

 * Handle oneshot mode broadcasting

 Find all expired events */

		/*

		 * Required for !SMP because for_each_cpu() reports

		 * unconditionally CPU0 as set on UP kernels.

			/*

			 * Mark the remote cpu in the pending mask, so

			 * it can avoid reprogramming the cpu local

			 * timer in tick_broadcast_oneshot_control().

	/*

	 * Remove the current cpu from the pending mask. The event is

	 * delivered immediately in tick_do_broadcast() !

 Take care of enforced broadcast requests */

	/*

	 * Sanity check. Catch the case where we try to broadcast to

	 * offline cpus.

	/*

	 * Wakeup the cpus which have an expired event.

	/*

	 * Two reasons for reprogram:

	 *

	 * - The global event did not expire any CPU local

	 * events. This happens in dyntick mode, as the maximum PIT

	 * delta is quite small.

	 *

	 * - There are pending events on sleeping CPUs which were not

	 * in the event mask

	/*

	 * For hrtimer based broadcasting we cannot shutdown the cpu

	 * local device if our own event is the first one to expire or

	 * if we own the broadcast timer.

		/*

		 * If the current CPU owns the hrtimer broadcast

		 * mechanism, it cannot go deep idle and we do not add

		 * the CPU to the broadcast mask. We don't have to go

		 * through the EXIT path as the local timer is not

		 * shutdown.

		/*

		 * If the broadcast device is in periodic mode, we

		 * return.

 If it is a hrtimer based broadcast, return busy */

 Conditionally shut down the local timer. */

			/*

			 * We only reprogram the broadcast timer if we

			 * did not mark ourself in the force mask and

			 * if the cpu local event is earlier than the

			 * broadcast event. If the current CPU is in

			 * the force mask, then we are going to be

			 * woken by the IPI right away; we return

			 * busy, so the CPU does not try to go deep

			 * idle.

				/*

				 * In case of hrtimer broadcasts the

				 * programming might have moved the

				 * timer to this cpu. If yes, remove

				 * us from the broadcast mask and

				 * return busy.

			/*

			 * The cpu which was handling the broadcast

			 * timer marked this cpu in the broadcast

			 * pending mask and fired the broadcast

			 * IPI. So we are going to handle the expired

			 * event anyway via the broadcast IPI

			 * handler. No need to reprogram the timer

			 * with an already expired event.

			/*

			 * Bail out if there is no next event.

			/*

			 * If the pending bit is not set, then we are

			 * either the CPU handling the broadcast

			 * interrupt or we got woken by something else.

			 *

			 * We are no longer in the broadcast mask, so

			 * if the cpu local expiry time is already

			 * reached, we would reprogram the cpu local

			 * timer with an already expired event.

			 *

			 * This can lead to a ping-pong when we return

			 * to idle and therefore rearm the broadcast

			 * timer before the cpu local timer was able

			 * to fire. This happens because the forced

			 * reprogramming makes sure that the event

			 * will happen in the future and depending on

			 * the min_delta setting this might be far

			 * enough out that the ping-pong starts.

			 *

			 * If the cpu local next_event has expired

			 * then we know that the broadcast timer

			 * next_event has expired as well and

			 * broadcast is about to be handled. So we

			 * avoid reprogramming and enforce that the

			 * broadcast handler, which did not run yet,

			 * will invoke the cpu local handler.

			 *

			 * We cannot call the handler directly from

			 * here, because we might be in a NOHZ phase

			 * and we did not go through the irq_enter()

			 * nohz fixups.

			/*

			 * We got woken by something else. Reprogram

			 * the cpu local timer device.

 We may have transitioned to oneshot mode while idle */

	/*

	 * If there is no broadcast or wakeup device, tell the caller not

	 * to go into deep idle.

/*

 * Reset the one shot broadcast for a cpu

 *

 * Called with tick_broadcast_lock held

	/*

	 * Protect against concurrent updates (store /load tearing on

	 * 32bit). It does not matter if the time is already in the

	 * past. The broadcast device which is about to be programmed will

	 * fire in any case.

/**

 * tick_broadcast_setup_oneshot - setup the broadcast device

 Set it up only once ! */

		/*

		 * We must be careful here. There might be other CPUs

		 * waiting for periodic broadcast. We need to set the

		 * oneshot_mask bits for those and program the

		 * broadcast device to fire.

		/*

		 * The first cpu which switches to oneshot mode sets

		 * the bit for all other cpus which are in the general

		 * (periodic) broadcast mask. So the bit is set and

		 * would prevent the first broadcast enter after this

		 * to program the bc device.

/*

 * Select oneshot operating mode for the broadcast device

 This moves the broadcast assignment to this CPU: */

/*

 * Remove a dying CPU from broadcasting

	/*

	 * Clear the broadcast masks for the dead cpu, but do not stop

	 * the broadcast device!

/*

 * Check, whether the broadcast device is in one shot mode

/*

 * Check whether the broadcast device supports oneshot.

 SPDX-License-Identifier: GPL-2.0+

/*

 * debugfs file to track time spent in suspend

 *

 * Copyright (c) 2011, Google, Inc.

 Cap bin index so we don't overflow the array */

 SPDX-License-Identifier: LGPL-2.0+

/*

 * Copyright (C) 1993, 1994, 1995, 1996, 1997 Free Software Foundation, Inc.

 * This file is part of the GNU C Library.

 * Contributed by Paul Eggert (eggert@twinsun.com).

 *

 * The GNU C Library is free software; you can redistribute it and/or

 * modify it under the terms of the GNU Library General Public License as

 * published by the Free Software Foundation; either version 2 of the

 * License, or (at your option) any later version.

 *

 * The GNU C Library is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 * Library General Public License for more details.

 *

 * You should have received a copy of the GNU Library General Public

 * License along with the GNU C Library; see the file COPYING.LIB.  If not,

 * write to the Free Software Foundation, Inc., 59 Temple Place - Suite 330,

 * Boston, MA 02111-1307, USA.

/*

 * Converts the calendar time to broken-down time representation

 *

 * 2009-7-14:

 *   Moved from glibc-2.6 to kernel by Zhaolei<zhaolei@cn.fujitsu.com>

 * 2021-06-02:

 *   Reimplemented by Cassio Neri <cassio.neri@gmail.com>

/**

 * time64_to_tm - converts the calendar time to local broken-down time

 *

 * @totalsecs:	the number of seconds elapsed since 00:00:00 on January 1, 1970,

 *		Coordinated Universal Time (UTC).

 * @offset:	offset seconds adding to totalsecs.

 * @result:	pointer to struct tm variable to receive broken-down time

 January 1, 1970 was a Thursday. */

	/*

	 * The following algorithm is, basically, Proposition 6.3 of Neri

	 * and Schneider [1]. In a few words: it works on the computational

	 * (fictitious) calendar where the year starts in March, month = 2

	 * (*), and finishes in February, month = 13. This calendar is

	 * mathematically convenient because the day of the year does not

	 * depend on whether the year is leap or not. For instance:

	 *

	 * March 1st		0-th day of the year;

	 * ...

	 * April 1st		31-st day of the year;

	 * ...

	 * January 1st		306-th day of the year; (Important!)

	 * ...

	 * February 28th	364-th day of the year;

	 * February 29th	365-th day of the year (if it exists).

	 *

	 * After having worked out the date in the computational calendar

	 * (using just arithmetics) it's easy to convert it to the

	 * corresponding date in the Gregorian calendar.

	 *

	 * [1] "Euclidean Affine Functions and Applications to Calendar

	 * Algorithms". https://arxiv.org/abs/2102.06959

	 *

	 * (*) The numbering of months follows tm more closely and thus,

	 * is slightly different from [1].

	/*

	 * Recall that January 1st is the 306-th day of the year in the

	 * computational (not Gregorian) calendar.

 Convert to the Gregorian calendar and adjust to Unix time. */

 Convert to tm's format. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Dummy stubs used when CONFIG_POSIX_TIMERS=n

 *

 * Created by:  Nicolas Pitre, July 2016

 * Copyright:   (C) 2016 Linaro Limited

 Architectures may override SYS_NI and COMPAT_SYS_NI */

/*

 * We preserve minimal support for CLOCK_REALTIME and CLOCK_MONOTONIC

 * as it is easy to remain compatible with little code. CLOCK_BOOTTIME

 * is also included for convenience as at least systemd uses it.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2019 ARM Ltd.

 *

 * Generic implementation of update_vsyscall and update_vsyscall_tz.

 *

 * Based on the x86 specific implementation.

 CLOCK_MONOTONIC */

 Copy MONOTONIC time for BOOTTIME */

 Add the boot offset */

 CLOCK_BOOTTIME */

 CLOCK_MONOTONIC_RAW */

 CLOCK_TAI */

 copy vsyscall data */

 CLOCK_REALTIME also required for time() */

 CLOCK_REALTIME_COARSE */

 CLOCK_MONOTONIC_COARSE */

	/*

	 * Read without the seqlock held by clock_getres().

	 * Note: No need to have a second copy.

	/*

	 * If the current clocksource is not VDSO capable, then spare the

	 * update of the high resolution parts.

/**

 * vdso_update_begin - Start of a VDSO update section

 *

 * Allows architecture code to safely update the architecture specific VDSO

 * data. Disables interrupts, acquires timekeeper lock to serialize against

 * concurrent updates from timekeeping and invalidates the VDSO data

 * sequence counter to prevent concurrent readers from accessing

 * inconsistent data.

 *

 * Returns: Saved interrupt flags which need to be handed in to

 * vdso_update_end().

/**

 * vdso_update_end - End of a VDSO update section

 * @flags:	Interrupt flags as returned from vdso_update_begin()

 *

 * Pairs with vdso_update_begin(). Marks vdso data consistent, invokes data

 * synchronization if the architecture requires it, drops timekeeper lock

 * and restores interrupt flags.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  This file contains the interface functions for the various time related

 *  system calls: time, stime, gettimeofday, settimeofday, adjtime

 *

 * Modification history:

 *

 * 1993-09-02    Philip Gladstone

 *      Created file with time related functions from sched/core.c and adjtimex()

 * 1993-10-08    Torsten Duwe

 *      adjtime interface update and CMOS clock write code

 * 1995-08-13    Torsten Duwe

 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)

 * 1999-01-16    Ulrich Windl

 *	Introduced error checking for many cases in adjtimex().

 *	Updated NTP code according to technical memorandum Jan '96

 *	"A Kernel Model for Precision Timekeeping" by Dave Mills

 *	Allow time_constant larger than MAXTC(6) for NTP v4 (MAXTC == 10)

 *	(Even though the technical memorandum forbids it)

 * 2004-07-14	 Christoph Lameter

 *	Added getnstimeofday to allow the posix timer functions to return

 *	with nanosecond accuracy

/*

 * The timezone where the local system is located.  Used as a default by some

 * programs who obtain this value by using gettimeofday.

/*

 * sys_time() can be implemented in user-level using

 * sys_gettimeofday().  Is this for backwards compatibility?  If so,

 * why not move it into the appropriate arch directory (for those

 * architectures that need it).

/*

 * sys_stime() can be implemented in user-level using

 * sys_settimeofday().  Is this for backwards compatibility?  If so,

 * why not move it into the appropriate arch directory (for those

 * architectures that need it).

 __ARCH_WANT_SYS_TIME */

 old_time32_t is a 32 bit "long" and needs to get converted. */

 __ARCH_WANT_SYS_TIME32 */

/*

 * In case for some reason the CMOS clock has not already been running

 * in UTC, but in some local time: The first time we set the timezone,

 * we will warp the clock so that it is ticking UTC time instead of

 * local time. Presumably, if someone is setting the timezone then we

 * are running in an environment where the programs understand about

 * timezones. This should be done at boot time in the /etc/rc script,

 * as soon as possible, so that the clock can be set right. Otherwise,

 * various programs will get confused when the clock gets warped.

 Verify we're within the +-15 hrs range */

 Local copy of parameter */

	/* Copy the user data space into the kernel copy

	 * structure. But bear in mind that the structures

	 * may change

/*

 * Convert jiffies to milliseconds and back.

 *

 * Avoid unnecessary multiplications/divisions in the

 * two most common HZ cases:

	/*

	 * Hz usually doesn't go much further MSEC_PER_SEC.

	 * jiffies_to_usecs() and usecs_to_jiffies() depend on that.

/*

 * mktime64 - Converts date to seconds.

 * Converts Gregorian date to seconds since 1970-01-01 00:00:00.

 * Assumes input in normal date format, i.e. 1980-12-31 23:59:59

 * => year=1980, mon=12, day=31, hour=23, min=59, sec=59.

 *

 * [For the Julian calendar (which was used in Russia before 1917,

 * Britain & colonies before 1752, anywhere else before 1582,

 * and is still in use by some communities) leave out the

 * -year/100+year/400 terms, and add 10.]

 *

 * This algorithm was first published by Gauss (I think).

 *

 * A leap second can be indicated by calling this function with sec as

 * 60 (allowable under ISO 8601).  The leap second is treated the same

 * as the following second since they don't exist in UNIX time.

 *

 * An encoding of midnight at the end of the day as 24:00:00 - ie. midnight

 * tomorrow - (allowable under ISO 8601) is supported.

 1..12 -> 11,12,1..10 */

 Puts Feb last since it has leap day */

 now have hours - midnight tomorrow handled here */

 now have minutes */

 finally seconds */

/**

 * set_normalized_timespec - set timespec sec and nsec parts and normalize

 *

 * @ts:		pointer to timespec variable to be set

 * @sec:	seconds to set

 * @nsec:	nanoseconds to set

 *

 * Set seconds and nanoseconds field of a timespec variable and

 * normalize to the timespec storage format

 *

 * Note: The tv_nsec part is always in the range of

 *	0 <= tv_nsec < NSEC_PER_SEC

 * For negative values only the tv_sec field is negative !

		/*

		 * The following asm() prevents the compiler from

		 * optimising this loop into a modulo operation. See

		 * also __iter_div_u64_rem() in include/linux/time.h

/**

 * ns_to_timespec64 - Convert nanoseconds to timespec64

 * @nsec:       the nanoseconds value to be converted

 *

 * Returns the timespec64 representation of the nsec parameter.

		/*

		 * With negative times, tv_sec points to the earlier

		 * second, and tv_nsec counts the nanoseconds since

		 * then, so tv_nsec is always a positive number.

/**

 * msecs_to_jiffies: - convert milliseconds to jiffies

 * @m:	time in milliseconds

 *

 * conversion is done as follows:

 *

 * - negative values mean 'infinite timeout' (MAX_JIFFY_OFFSET)

 *

 * - 'too large' values [that would result in larger than

 *   MAX_JIFFY_OFFSET values] mean 'infinite timeout' too.

 *

 * - all other values are converted to jiffies by either multiplying

 *   the input value by a factor or dividing it with a factor and

 *   handling any 32-bit overflows.

 *   for the details see __msecs_to_jiffies()

 *

 * msecs_to_jiffies() checks for the passed in value being a constant

 * via __builtin_constant_p() allowing gcc to eliminate most of the

 * code, __msecs_to_jiffies() is called if the value passed does not

 * allow constant folding and the actual conversion must be done at

 * runtime.

 * the _msecs_to_jiffies helpers are the HZ dependent conversion

 * routines found in include/linux/jiffies.h

	/*

	 * Negative value, means infinite timeout:

/*

 * The TICK_NSEC - 1 rounds up the value to the next resolution.  Note

 * that a remainder subtract here would not do the right thing as the

 * resolution values don't fall on second boundaries.  I.e. the line:

 * nsec -= nsec % TICK_NSEC; is NOT a correct resolution rounding.

 * Note that due to the small error in the multiplier here, this

 * rounding is incorrect for sufficiently large values of tv_nsec, but

 * well formed timespecs should have tv_nsec < NSEC_PER_SEC, so we're

 * OK.

 *

 * Rather, we just shift the bits off the right.

 *

 * The >> (NSEC_JIFFIE_SC - SEC_JIFFIE_SC) converts the scaled nsec

 * value to a scaled second value.

	/*

	 * Convert jiffies to nanoseconds and separate with

	 * one divide.

/*

 * Convert jiffies/jiffies_64 to clock_t and back.

 Don't worry about loss of precision here .. */

 .. but do try to contain it here */

 Nothing to do */

	/*

	 * There are better ways that don't overflow early,

	 * but even this doesn't overflow in hundreds of years

	 * in 64 bits, so..

	/*

         * max relative error 5.7e-8 (1.8s per year) for USER_HZ <= 1024,

         * overflow after 64.99 years.

         * exact for HZ=60, 72, 90, 120, 144, 180, 300, 600, 900, ...

/**

 * nsecs_to_jiffies64 - Convert nsecs in u64 to jiffies64

 *

 * @n:	nsecs in u64

 *

 * Unlike {m,u}secs_to_jiffies, type of input is not unsigned int but u64.

 * And this doesn't return MAX_JIFFY_OFFSET since this function is designed

 * for scheduler, not for use in device drivers to calculate timeout value.

 *

 * note:

 *   NSEC_PER_SEC = 10^9 = (5^9 * 2^9) = (1953125 * 512)

 *   ULLONG_MAX ns = 18446744073.709551615 secs = about 584 years

 Common case, HZ = 100, 128, 200, 250, 256, 500, 512, 1000 etc. */

 overflow after 292 years if HZ = 1024 */

	/*

	 * Generic case - optimized for cases where HZ is a multiple of 3.

	 * overflow after 64.99 years, exact for HZ = 60, 72, 90, 120 etc.

/**

 * nsecs_to_jiffies - Convert nsecs in u64 to jiffies

 *

 * @n:	nsecs in u64

 *

 * Unlike {m,u}secs_to_jiffies, type of input is not unsigned int but u64.

 * And this doesn't return MAX_JIFFY_OFFSET since this function is designed

 * for scheduler, not for use in device drivers to calculate timeout value.

 *

 * note:

 *   NSEC_PER_SEC = 10^9 = (5^9 * 2^9) = (1953125 * 512)

 *   ULLONG_MAX ns = 18446744073.709551615 secs = about 584 years

/*

 * Add two timespec64 values and do a safety check for overflow.

 * It's assumed that both values are valid (>= 0).

 * And, each timespec64 is in normalized form.

 Zero out the padding in compat mode */

 In 32-bit mode, this drops the padding */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Kernel timekeeping code and accessor functions. Based on code from

 *  timer.c, moved in commit 8524070b7982.

 Update timekeeper when a tick has passed */

 Update timekeeper on a direct frequency change */

/*

 * The most important data for readout fits into a single 64 byte

 * cache line.

 flag for if timekeeping is suspended */

/**

 * struct tk_fast - NMI safe timekeeper

 * @seq:	Sequence counter for protecting updates. The lowest bit

 *		is the index for the tk_read_base array

 * @base:	tk_read_base array. Access is indexed by the lowest bit of

 *		@seq.

 *

 * See @update_fast_timekeeper() below.

 Suspend-time cycles value for halted fast timekeeper. */

/*

 * Boot time initialization which allows local_clock() to be utilized

 * during early boot when clocksources are not available. local_clock()

 * returns nanoseconds already so no conversion is required, hence mult=1

 * and shift=0. When the first proper clocksource is installed then

 * the fast time keepers are updated with the correct values.

	/*

	 * Verify consistency of: offset_real = -wall_to_monotonic

	 * before modifying anything

	/*

	 * Timespec representation for VDSO update to avoid 64bit division

	 * on every update.

/*

 * tk_clock_read - atomic clocksource read() helper

 *

 * This helper is necessary to use in the read paths because, while the

 * seqcount ensures we don't return a bad value while structures are updated,

 * it doesn't protect from potential crashes. There is the possibility that

 * the tkr's clocksource may change between the read reference, and the

 * clock reference passed to the read function.  This can cause crashes if

 * the wrong clocksource is passed to the wrong read function.

 * This isn't necessary to use when holding the timekeeper_lock or doing

 * a read of the fast-timekeeper tkrs (which is protected by its own locking

 * and update logic).

 5 minute rate-limiting */

	/*

	 * Since we're called holding a seqcount, the data may shift

	 * under us while we're doing the calculation. This can cause

	 * false positives, since we'd note a problem but throw the

	 * results away. So nest another seqcount here to atomically

	 * grab the points we are checking with.

	/*

	 * Try to catch underflows by checking if we are seeing small

	 * mask-relative negative values.

 Cap delta value to the max_cycles values to avoid mult overflows */

 read clocksource */

 calculate the delta since the last update_wall_time */

/**

 * tk_setup_internals - Set up internals to use clocksource clock.

 *

 * @tk:		The target timekeeper to setup.

 * @clock:		Pointer to clocksource.

 *

 * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment

 * pair and interval request.

 *

 * Unless you're the timekeeping code, you should not be using this!

 Do the ns -> cycle conversion first, using original mult */

 Go back from cycles -> shifted ns */

 if changing clocks, convert xtime_nsec shift units */

	/*

	 * The timekeeper keeps its own mult values for the currently

	 * active clocksource. These value will be adjusted via NTP

	 * to counteract clock drifting.

 Timekeeper helper functions. */

 calculate the delta since the last update_wall_time */

/**

 * update_fast_timekeeper - Update the fast and NMI safe monotonic timekeeper.

 * @tkr: Timekeeping readout base from which we take the update

 * @tkf: Pointer to NMI safe timekeeper

 *

 * We want to use this from any context including NMI and tracing /

 * instrumenting the timekeeping code itself.

 *

 * Employ the latch technique; see @raw_write_seqcount_latch.

 *

 * So if a NMI hits the update of base[0] then it will use base[1]

 * which is still consistent. In the worst case this can result is a

 * slightly wrong timestamp (a few nanoseconds). See

 * @ktime_get_mono_fast_ns.

 Force readers off to base[1] */

 Update base[0] */

 Force readers back to base[0] */

 Update base[1] */

/**

 * ktime_get_mono_fast_ns - Fast NMI safe access to clock monotonic

 *

 * This timestamp is not guaranteed to be monotonic across an update.

 * The timestamp is calculated by:

 *

 *	now = base_mono + clock_delta * slope

 *

 * So if the update lowers the slope, readers who are forced to the

 * not yet updated second array are still using the old steeper slope.

 *

 * tmono

 * ^

 * |    o  n

 * |   o n

 * |  u

 * | o

 * |o

 * |12345678---> reader order

 *

 * o = old slope

 * u = update

 * n = new slope

 *

 * So reader 6 will observe time going backwards versus reader 5.

 *

 * While other CPUs are likely to be able to observe that, the only way

 * for a CPU local observation is when an NMI hits in the middle of

 * the update. Timestamps taken from that NMI context might be ahead

 * of the following timestamps. Callers need to be aware of that and

 * deal with it.

/**

 * ktime_get_raw_fast_ns - Fast NMI safe access to clock monotonic raw

 *

 * Contrary to ktime_get_mono_fast_ns() this is always correct because the

 * conversion factor is not affected by NTP/PTP correction.

/**

 * ktime_get_boot_fast_ns - NMI safe and fast access to boot clock.

 *

 * To keep it NMI safe since we're accessing from tracing, we're not using a

 * separate timekeeper with updates to monotonic clock and boot offset

 * protected with seqcounts. This has the following minor side effects:

 *

 * (1) Its possible that a timestamp be taken after the boot offset is updated

 * but before the timekeeper is updated. If this happens, the new boot offset

 * is added to the old timekeeping making the clock appear to update slightly

 * earlier:

 *    CPU 0                                        CPU 1

 *    timekeeping_inject_sleeptime64()

 *    __timekeeping_inject_sleeptime(tk, delta);

 *                                                 timestamp();

 *    timekeeping_update(tk, TK_CLEAR_NTP...);

 *

 * (2) On 32-bit systems, the 64-bit boot offset (tk->offs_boot) may be

 * partially updated.  Since the tk->offs_boot update is a rare event, this

 * should be a rare occurrence which postprocessing should be able to handle.

 *

 * The caveats vs. timestamp ordering as documented for ktime_get_fast_ns()

 * apply as well.

/**

 * ktime_get_real_fast_ns: - NMI safe and fast access to clock realtime.

 *

 * See ktime_get_fast_ns() for documentation of the time stamp ordering.

/**

 * ktime_get_fast_timestamps: - NMI safe timestamps

 * @snapshot:	Pointer to timestamp storage

 *

 * Stores clock monotonic, boottime and realtime timestamps.

 *

 * Boot time is a racy access on 32bit systems if the sleep time injection

 * happens late during resume and not in timekeeping_resume(). That could

 * be avoided by expanding struct tk_read_base with boot offset for 32bit

 * and adding more overhead to the update. As this is a hard to observe

 * once per resume event which can be filtered with reasonable effort using

 * the accurate mono/real timestamps, it's probably not worth the trouble.

 *

 * Aside of that it might be possible on 32 and 64 bit to observe the

 * following when the sleep time injection happens late:

 *

 * CPU 0				CPU 1

 * timekeeping_resume()

 * ktime_get_fast_timestamps()

 *	mono, real = __ktime_get_real_fast()

 *					inject_sleep_time()

 *					   update boot offset

 *	boot = mono + bootoffset;

 *

 * That means that boot time already has the sleep time adjustment, but

 * real time does not. On the next readout both are in sync again.

 *

 * Preventing this for 64bit is not really feasible without destroying the

 * careful cache layout of the timekeeper because the sequence count and

 * struct tk_read_base would then need two cache lines instead of one.

 *

 * Access to the time keeper clock source is disabled across the innermost

 * steps of suspend/resume. The accessors still work, but the timestamps

 * are frozen until time keeping is resumed which happens very early.

 *

 * For regular suspend/resume there is no observable difference vs. sched

 * clock, but it might affect some of the nasty low level debug printks.

 *

 * OTOH, access to sched clock is not guaranteed across suspend/resume on

 * all systems either so it depends on the hardware in use.

 *

 * If that turns out to be a real problem then this could be mitigated by

 * using sched clock in a similar way as during early boot. But it's not as

 * trivial as on early boot because it needs some careful protection

 * against the clock monotonic timestamp jumping backwards on resume.

/**

 * halt_fast_timekeeper - Prevent fast timekeeper from accessing clocksource.

 * @tk: Timekeeper to snapshot.

 *

 * It generally is unsafe to access the clocksource after timekeeping has been

 * suspended, so take a snapshot of the readout base of @tk and use it as the

 * fast timekeeper's readout base while suspended.  It will return the same

 * number of cycles every time until timekeeping is resumed at which time the

 * proper readout base for the fast timekeeper will be restored automatically.

/**

 * pvclock_gtod_register_notifier - register a pvclock timedata update listener

 * @nb: Pointer to the notifier block to register

/**

 * pvclock_gtod_unregister_notifier - unregister a pvclock

 * timedata update listener

 * @nb: Pointer to the notifier block to unregister

/*

 * tk_update_leap_state - helper to update the next_leap_ktime

 Convert to monotonic time */

/*

 * Update the ktime_t based scalar nsec members of the timekeeper

	/*

	 * The xtime based monotonic readout is:

	 *	nsec = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec + now();

	 * The ktime based monotonic readout is:

	 *	nsec = base_mono + now();

	 * ==> base_mono = (xtime_sec + wtm_sec) * 1e9 + wtm_nsec

	/*

	 * The sum of the nanoseconds portions of xtime and

	 * wall_to_monotonic can be greater/equal one second. Take

	 * this into account before updating tk->ktime_sec.

 Update the monotonic raw base */

 must hold timekeeper_lock */

	/*

	 * The mirroring of the data to the shadow-timekeeper needs

	 * to happen last here to ensure we don't over-write the

	 * timekeeper structure on the next update with stale data

/**

 * timekeeping_forward_now - update clock to the current time

 * @tk:		Pointer to the timekeeper to update

 *

 * Forward the current clock to update its state since the last call to

 * update_wall_time(). This is useful before significant clock changes,

 * as it avoids having to deal with this time offset explicitly.

/**

 * ktime_get_real_ts64 - Returns the time of day in a timespec64.

 * @ts:		pointer to the timespec to be set

 *

 * Returns the time of day in a timespec64 (WARN if suspended).

/**

 * ktime_mono_to_any() - convert monotonic time to any other time

 * @tmono:	time to convert.

 * @offs:	which offset to use

/**

 * ktime_get_raw - Returns the raw monotonic time in ktime_t format

/**

 * ktime_get_ts64 - get the monotonic clock in timespec64 format

 * @ts:		pointer to timespec variable

 *

 * The function calculates the monotonic clock from the realtime

 * clock and the wall_to_monotonic offset and stores the result

 * in normalized timespec64 format in the variable pointed to by @ts.

/**

 * ktime_get_seconds - Get the seconds portion of CLOCK_MONOTONIC

 *

 * Returns the seconds portion of CLOCK_MONOTONIC with a single non

 * serialized read. tk->ktime_sec is of type 'unsigned long' so this

 * works on both 32 and 64 bit systems. On 32 bit systems the readout

 * covers ~136 years of uptime which should be enough to prevent

 * premature wrap arounds.

/**

 * ktime_get_real_seconds - Get the seconds portion of CLOCK_REALTIME

 *

 * Returns the wall clock seconds since 1970.

 *

 * For 64bit systems the fast access to tk->xtime_sec is preserved. On

 * 32bit systems the access must be protected with the sequence

 * counter to provide "atomic" access to the 64bit tk->xtime_sec

 * value.

/**

 * __ktime_get_real_seconds - The same as ktime_get_real_seconds

 * but without the sequence counter protect. This internal function

 * is called just when timekeeping lock is already held.

/**

 * ktime_get_snapshot - snapshots the realtime/monotonic raw clocks with counter

 * @systime_snapshot:	pointer to struct receiving the system time snapshot

 Scale base by mult/div checking for overflow */

/**

 * adjust_historical_crosststamp - adjust crosstimestamp previous to current interval

 * @history:			Snapshot representing start of history

 * @partial_history_cycles:	Cycle offset into history (fractional part)

 * @total_history_cycles:	Total history length in cycles

 * @discontinuity:		True indicates clock was set on history period

 * @ts:				Cross timestamp that should be adjusted using

 *	partial/total ratio

 *

 * Helper function used by get_device_system_crosststamp() to correct the

 * crosstimestamp corresponding to the start of the current interval to the

 * system counter value (timestamp point) provided by the driver. The

 * total_history_* quantities are the total history starting at the provided

 * reference point and ending at the start of the current interval. The cycle

 * count between the driver timestamp point and the start of the current

 * interval is partial_history_cycles.

 Interpolate shortest distance from beginning or end of history */

	/*

	 * Scale the monotonic raw time delta by:

	 *	partial_history_cycles / total_history_cycles

	/*

	 * If there is a discontinuity in the history, scale monotonic raw

	 *	correction by:

	 *	mult(real)/mult(raw) yielding the realtime correction

	 * Otherwise, calculate the realtime correction similar to monotonic

	 *	raw calculation

 Fixup monotonic raw and real time time values */

/*

 * cycle_between - true if test occurs chronologically between before and after

/**

 * get_device_system_crosststamp - Synchronously capture system/device timestamp

 * @get_time_fn:	Callback to get simultaneous device time and

 *	system counter from the device driver

 * @ctx:		Context passed to get_time_fn()

 * @history_begin:	Historical reference point used to interpolate system

 *	time when counter provided by the driver is before the current interval

 * @xtstamp:		Receives simultaneously captured system and device time

 *

 * Reads a timestamp from a device and correlates it to system time

		/*

		 * Try to synchronously capture device time and a system

		 * counter value calling back into the device driver

		/*

		 * Verify that the clocksource associated with the captured

		 * system counter value is the same as the currently installed

		 * timekeeper clocksource

		/*

		 * Check whether the system counter value provided by the

		 * device driver is on the current timekeeping interval.

	/*

	 * Interpolate if necessary, adjusting back from the start of the

	 * current interval

		/*

		 * Check that the counter value occurs after the provided

		 * history reference and that the history doesn't cross a

		 * clocksource change

/**

 * do_settimeofday64 - Sets the time of day.

 * @ts:     pointer to the timespec64 variable containing the new time

 *

 * Sets the time of day to the new time and update NTP and notify hrtimers

 Signal hrtimers about time change */

/**

 * timekeeping_inject_offset - Adds or subtracts from the current time.

 * @ts:		Pointer to the timespec variable containing the offset

 *

 * Adds or subtracts an offset value from the current time.

 Make sure the proposed value is valid */

 even if we error out, we forwarded the time, so call update */

 Signal hrtimers about time change */

/*

 * Indicates if there is an offset between the system clock and the hardware

 * clock/persistent clock/rtc.

/*

 * Adjust the time obtained from the CMOS to be UTC time instead of

 * local time.

 *

 * This is ugly, but preferable to the alternatives.  Otherwise we

 * would either need to write a program to do it in /etc/rc (and risk

 * confusion if the program gets run more than once; it would also be

 * hard to make the program warp the clock precisely n hours)  or

 * compile in the timezone information into the kernel.  Bad, bad....

 *

 *						- TYT, 1992-01-01

 *

 * The best thing to do is to keep the CMOS clock in universal time (UTC)

 * as real UNIX machines always do it. This avoids all headaches about

 * daylight saving times and warping kernel clocks.

/*

 * __timekeeping_set_tai_offset - Sets the TAI offset from UTC and monotonic

/*

 * change_clocksource - Swaps clocksources if a new one is available

 *

 * Accumulates current time interval and initializes new clocksource

	/*

	 * If the cs is in module, get a module reference. Succeeds

	 * for built-in code (owner == NULL) as well.

/**

 * timekeeping_notify - Install a new clock source

 * @clock:		pointer to the clock source

 *

 * This function is called from clocksource.c after a new, better clock

 * source has been registered. The caller holds the clocksource_mutex.

/**

 * ktime_get_raw_ts64 - Returns the raw monotonic time in a timespec

 * @ts:		pointer to the timespec64 to be set

 *

 * Returns the raw monotonic time (completely un-modified by ntp)

/**

 * timekeeping_valid_for_hres - Check if timekeeping is suitable for hres

/**

 * timekeeping_max_deferment - Returns max time the clocksource can be deferred

/**

 * read_persistent_clock64 -  Return time from the persistent clock.

 * @ts: Pointer to the storage for the readout value

 *

 * Weak dummy function for arches that do not yet support it.

 * Reads the time from the battery backed persistent clock.

 * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported.

 *

 *  XXX - Do be sure to remove it once all arches implement it.

/**

 * read_persistent_wall_and_boot_offset - Read persistent clock, and also offset

 *                                        from the boot.

 *

 * Weak dummy function for arches that do not yet support it.

 * @wall_time:	- current time as returned by persistent clock

 * @boot_offset: - offset that is defined as wall_time - boot_time

 *

 * The default function calculates offset based on the current value of

 * local_clock(). This way architectures that support sched_clock() but don't

 * support dedicated boot time clock will provide the best estimate of the

 * boot time.

/*

 * Flag reflecting whether timekeeping_resume() has injected sleeptime.

 *

 * The flag starts of false and is only set when a suspend reaches

 * timekeeping_suspend(), timekeeping_resume() sets it to false when the

 * timekeeper clocksource is not stopping across suspend and has been

 * used to update sleep time. If the timekeeper clocksource has stopped

 * then the flag stays true and is used by the RTC resume code to decide

 * whether sleeptime must be injected and if so the flag gets false then.

 *

 * If a suspend fails before reaching timekeeping_resume() then the flag

 * stays false and prevents erroneous sleeptime injection.

 Flag for if there is a persistent clock on this platform */

/*

 * timekeeping_init - Initializes the clocksource and common timekeeping values

	/*

	 * We want set wall_to_mono, so the following is true:

	 * wall time + wall_to_mono = boot time

 time in seconds when suspend began for persistent clock */

/**

 * __timekeeping_inject_sleeptime - Internal function to add sleep interval

 * @tk:		Pointer to the timekeeper to be updated

 * @delta:	Pointer to the delta value in timespec64 format

 *

 * Takes a timespec offset measuring a suspend interval and properly

 * adds the sleep offset to the timekeeping variables.

/**

 * We have three kinds of time sources to use for sleep time

 * injection, the preference order is:

 * 1) non-stop clocksource

 * 2) persistent clock (ie: RTC accessible when irqs are off)

 * 3) RTC

 *

 * 1) and 2) are used by timekeeping, 3) by RTC subsystem.

 * If system has neither 1) nor 2), 3) will be used finally.

 *

 *

 * If timekeeping has injected sleeptime via either 1) or 2),

 * 3) becomes needless, so in this case we don't need to call

 * rtc_resume(), and this is what timekeeping_rtc_skipresume()

 * means.

/**

 * 1) can be determined whether to use or not only when doing

 * timekeeping_resume() which is invoked after rtc_suspend(),

 * so we can't skip rtc_suspend() surely if system has 1).

 *

 * But if system has 2), 2) will definitely be used, so in this

 * case we don't need to call rtc_suspend(), and this is what

 * timekeeping_rtc_skipsuspend() means.

/**

 * timekeeping_inject_sleeptime64 - Adds suspend interval to timeekeeping values

 * @delta: pointer to a timespec64 delta value

 *

 * This hook is for architectures that cannot support read_persistent_clock64

 * because their RTC/persistent clock is only accessible when irqs are enabled.

 * and also don't have an effective nonstop clocksource.

 *

 * This function should only be called by rtc_resume(), and allows

 * a suspend offset to be injected into the timekeeping values.

 Signal hrtimers about time change */

/**

 * timekeeping_resume - Resumes the generic timekeeping subsystem.

	/*

	 * After system resumes, we need to calculate the suspended time and

	 * compensate it for the OS time. There are 3 sources that could be

	 * used: Nonstop clocksource during suspend, persistent clock and rtc

	 * device.

	 *

	 * One specific platform may have 1 or 2 or all of them, and the

	 * preference will be:

	 *	suspend-nonstop clocksource -> persistent clock -> rtc

	 * The less preferred source will only be tried if there is no better

	 * usable source. The rtc part is handled separately in rtc core code.

 Re-base the last cycle value */

 Resume the clockevent device(s) and hrtimers */

 Notify timerfd as resume is equivalent to clock_was_set() */

	/*

	 * On some systems the persistent_clock can not be detected at

	 * timekeeping_init by its return value, so if we see a valid

	 * value returned, update the persistent_clock_exists flag.

	/*

	 * Since we've called forward_now, cycle_last stores the value

	 * just read from the current clocksource. Save this to potentially

	 * use in suspend timing.

		/*

		 * To avoid drift caused by repeated suspend/resumes,

		 * which each can add ~1 second drift error,

		 * try to compensate so the difference in system time

		 * and persistent_clock time stays close to constant.

			/*

			 * if delta_delta is too large, assume time correction

			 * has occurred and set old_delta to the current delta.

 Otherwise try to adjust old_system to compensate */

 sysfs resume/suspend bits for timekeeping */

/*

 * Apply a multiplier adjustment to the timekeeper

	/*

	 * So the following can be confusing.

	 *

	 * To keep things simple, lets assume mult_adj == 1 for now.

	 *

	 * When mult_adj != 1, remember that the interval and offset values

	 * have been appropriately scaled so the math is the same.

	 *

	 * The basic idea here is that we're increasing the multiplier

	 * by one, this causes the xtime_interval to be incremented by

	 * one cycle_interval. This is because:

	 *	xtime_interval = cycle_interval * mult

	 * So if mult is being incremented by one:

	 *	xtime_interval = cycle_interval * (mult + 1)

	 * Its the same as:

	 *	xtime_interval = (cycle_interval * mult) + cycle_interval

	 * Which can be shortened to:

	 *	xtime_interval += cycle_interval

	 *

	 * So offset stores the non-accumulated cycles. Thus the current

	 * time (in shifted nanoseconds) is:

	 *	now = (offset * adj) + xtime_nsec

	 * Now, even though we're adjusting the clock frequency, we have

	 * to keep time consistent. In other words, we can't jump back

	 * in time, and we also want to avoid jumping forward in time.

	 *

	 * So given the same offset value, we need the time to be the same

	 * both before and after the freq adjustment.

	 *	now = (offset * adj_1) + xtime_nsec_1

	 *	now = (offset * adj_2) + xtime_nsec_2

	 * So:

	 *	(offset * adj_1) + xtime_nsec_1 =

	 *		(offset * adj_2) + xtime_nsec_2

	 * And we know:

	 *	adj_2 = adj_1 + 1

	 * So:

	 *	(offset * adj_1) + xtime_nsec_1 =

	 *		(offset * (adj_1+1)) + xtime_nsec_2

	 *	(offset * adj_1) + xtime_nsec_1 =

	 *		(offset * adj_1) + offset + xtime_nsec_2

	 * Canceling the sides:

	 *	xtime_nsec_1 = offset + xtime_nsec_2

	 * Which gives us:

	 *	xtime_nsec_2 = xtime_nsec_1 - offset

	 * Which simplifies to:

	 *	xtime_nsec -= offset

 NTP adjustment caused clocksource mult overflow */

/*

 * Adjust the timekeeper's multiplier to the correct frequency

 * and also to reduce the accumulated error value.

	/*

	 * Determine the multiplier from the current NTP tick length.

	 * Avoid expensive division when the tick length doesn't change.

	/*

	 * If the clock is behind the NTP time, increase the multiplier by 1

	 * to catch up with it. If it's ahead and there was a remainder in the

	 * tick division, the clock will slow down. Otherwise it will stay

	 * ahead until the tick length changes to a non-divisible value.

	/*

	 * It may be possible that when we entered this function, xtime_nsec

	 * was very small.  Further, if we're slightly speeding the clocksource

	 * in the code above, its possible the required corrective factor to

	 * xtime_nsec could cause it to underflow.

	 *

	 * Now, since we have already accumulated the second and the NTP

	 * subsystem has been notified via second_overflow(), we need to skip

	 * the next update.

/*

 * accumulate_nsecs_to_secs - Accumulates nsecs into secs

 *

 * Helper function that accumulates the nsecs greater than a second

 * from the xtime_nsec field to the xtime_secs field.

 * It also calls into the NTP code to handle leapsecond processing.

		/*

		 * Skip NTP update if this second was accumulated before,

		 * i.e. xtime_nsec underflowed in timekeeping_adjust()

 Figure out if its a leap sec and apply if needed */

/*

 * logarithmic_accumulation - shifted accumulation of cycles

 *

 * This functions accumulates a shifted interval of cycles into

 * a shifted interval nanoseconds. Allows for O(log) accumulation

 * loop.

 *

 * Returns the unconsumed cycles.

 If the offset is smaller than a shifted interval, do nothing */

 Accumulate one shifted interval */

 Accumulate raw time */

 Accumulate error between NTP and clock interval */

/*

 * timekeeping_advance - Updates the timekeeper to the current time and

 * current NTP tick length

 Make sure we're fully resumed: */

 Check if there's really nothing to do */

 Do some additional sanity checking */

	/*

	 * With NO_HZ we may have to accumulate many cycle_intervals

	 * (think "ticks") worth of time at once. To do this efficiently,

	 * we calculate the largest doubling multiple of cycle_intervals

	 * that is smaller than the offset.  We then accumulate that

	 * chunk in one go, and then try to consume the next smaller

	 * doubled multiple.

 Bound shift to one less than what overflows tick_length */

 Adjust the multiplier to correct NTP error */

	/*

	 * Finally, make sure that after the rounding

	 * xtime_nsec isn't larger than NSEC_PER_SEC

	/*

	 * Update the real timekeeper.

	 *

	 * We could avoid this memcpy by switching pointers, but that

	 * requires changes to all other timekeeper usage sites as

	 * well, i.e. move the timekeeper pointer getter into the

	 * spinlocked/seqcount protected sections. And we trade this

	 * memcpy under the tk_core.seq against one before we start

	 * updating.

 The memcpy must come last. Do not put anything here! */

/**

 * update_wall_time - Uses the current clocksource to increment the wall time

 *

/**

 * getboottime64 - Return the real time of system boot.

 * @ts:		pointer to the timespec64 to be set

 *

 * Returns the wall-time of boot in a timespec64.

 *

 * This is based on the wall_to_monotonic offset and the total suspend

 * time. Calls to settimeofday will affect the value returned (which

 * basically means that however wrong your real time clock is at boot time,

 * you get the right time here).

/*

 * Must hold jiffies_lock

/**

 * ktime_get_update_offsets_now - hrtimer helper

 * @cwsseq:	pointer to check and store the clock was set sequence number

 * @offs_real:	pointer to storage for monotonic -> realtime offset

 * @offs_boot:	pointer to storage for monotonic -> boottime offset

 * @offs_tai:	pointer to storage for monotonic -> clock tai offset

 *

 * Returns current monotonic time and updates the offsets if the

 * sequence number in @cwsseq and timekeeper.clock_was_set_seq are

 * different.

 *

 * Called from hrtimer_interrupt() or retrigger_next_event()

 Handle leapsecond insertion adjustments */

/*

 * timekeeping_validate_timex - Ensures the timex is ok for use in do_adjtimex

 singleshot must not be used with any other mode bits */

 In order to modify anything, you gotta be super-user! */

		/*

		 * if the quartz is off by more than 10% then

		 * something is VERY wrong!

 In order to inject time, you gotta be super-user! */

		/*

		 * Validate if a timespec/timeval used to inject a time

		 * offset is valid.  Offsets can be positive or negative, so

		 * we don't check tv_sec. The value of the timeval/timespec

		 * is the sum of its fields,but *NOTE*:

		 * The field tv_usec/tv_nsec must always be non-negative and

		 * we can't have more nanoseconds/microseconds than a second.

	/*

	 * Check for potential multiplication overflows that can

	 * only happen on 64-bit systems:

/**

 * do_adjtimex() - Accessor function to NTP __do_adjtimex function

 Validate the data before disabling interrupts */

 Update the multiplier immediately if frequency was set directly */

/**

 * hardpps() - Accessor function to NTP __hardpps function

 CONFIG_NTP_PPS */

 SPDX-License-Identifier: GPL-2.0+

/*

 * This file contains the jiffies based clocksource.

 *

 * Copyright (C) 2004, 2005 IBM, John Stultz (johnstul@us.ibm.com)

/*

 * The Jiffies based clocksource is the lowest common

 * denominator clock source which should function on

 * all systems. It has the same coarse resolution as

 * the timer interrupt frequency HZ and it suffers

 * inaccuracies caused by missed or lost timer

 * interrupts and the inability for the timer

 * interrupt hardware to accurately tick at the

 * requested HZ value. It is also not recommended

 * for "tick-less" systems.

 lowest valid rating*/

 details above */

 Calc cycles per tick */

 shift_hz stores hz<<8 for extra accuracy */

 Calculate nsec_per_tick using shift_hz */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 *  Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 *  Copyright(C) 2006-2007  Timesys Corp., Thomas Gleixner

 *

 *  High-resolution kernel timers

 *

 *  In contrast to the low-resolution timeout API, aka timer wheel,

 *  hrtimers provide finer resolution and accuracy depending on system

 *  configuration and capabilities.

 *

 *  Started by: Thomas Gleixner and Ingo Molnar

 *

 *  Credits:

 *	Based on the original timer wheel code

 *

 *	Help, testing, suggestions, bugfixes, improvements were

 *	provided by:

 *

 *	George Anzinger, Andrew Morton, Steven Rostedt, Roman Zippel

 *	et. al.

/*

 * Masks for selecting the soft and hard context timers from

 * cpu_base->active

/*

 * The timer bases:

 *

 * There are more clockids than hrtimer bases. Thus, we index

 * into the timer bases by the hrtimer_base_type enum. When trying

 * to reach a base using a clockid, hrtimer_clockid_to_base()

 * is used to convert from clockid to the proper hrtimer_base_type.

 Make sure we catch unsupported clockids */

/*

 * Functions and macros which are different for UP/SMP systems are kept in a

 * single place

/*

 * We require the migration_base for lock_hrtimer_base()/switch_hrtimer_base()

 * such that hrtimer_callback_running() can unconditionally dereference

 * timer->base->cpu_base

/*

 * We are using hashed locking: holding per_cpu(hrtimer_bases)[n].lock

 * means that all timers which are tied to this base via timer->base are

 * locked, and the base itself is locked too.

 *

 * So __run_timers/migrate_timers can safely modify all timers which could

 * be found on the lists/queues.

 *

 * When the timer's base is locked, and the timer removed from list, it is

 * possible to set timer->base = &migration_base and drop the lock: the timer

 * remains locked.

 The timer has migrated to another CPU: */

/*

 * We do not migrate the timer when it is expiring before the next

 * event on the target cpu. When high resolution is enabled, we cannot

 * reprogram the target cpu hardware and we would cause it to fire

 * late. To keep it simple, we handle the high resolution enabled and

 * disabled case similar.

 *

 * Called with cpu_base->lock of target cpu held.

/*

 * We switch the timer base to a power-optimized selected CPU target,

 * if:

 *	- NO_HZ_COMMON is enabled

 *	- timer migration is enabled

 *	- the timer callback is not running

 *	- the timer is not the first expiring timer on the new target

 *

 * If one of the above requirements is not fulfilled we move the timer

 * to the current CPU or leave it on the previously assigned CPU if

 * the timer callback is currently running.

		/*

		 * We are trying to move timer to new_base.

		 * However we can't change timer's base while it is running,

		 * so we keep it on the same CPU. No hassle vs. reprogramming

		 * the event source in the high resolution case. The softirq

		 * code will take care of this when the timer function has

		 * completed. There is no conflict as we hold the lock until

		 * the timer is enqueued.

 See the comment in lock_hrtimer_base() */

 CONFIG_SMP */

 !CONFIG_SMP */

/*

 * Functions for the union type storage format of ktime_t which are

 * too large for inlining:

/*

 * Divide a ktime value by a nanosecond value

 Make sure the divisor is less than 2^32: */

 BITS_PER_LONG >= 64 */

/*

 * Add two ktime values and do a safety check for overflow:

	/*

	 * We use KTIME_SEC_MAX here, the maximum timeout which we can

	 * return to user space in a timespec:

/*

 * fixup_init is called when:

 * - an active object is initialized

/*

 * fixup_activate is called when:

 * - an active object is activated

 * - an unknown non-static object is activated

/*

 * fixup_free is called when:

 * - an active object is freed

 Get to the next timer in the queue. */

 Skip cpu_base update if a timer is being excluded. */

	/*

	 * clock_was_set() might have changed base->offset of any of

	 * the clock bases so the result might be negative. Fix it up

	 * to prevent a false positive in clockevents_program_event().

/*

 * Recomputes cpu_base::*next_timer and returns the earliest expires_next

 * but does not set cpu_base::*expires_next, that is done by

 * hrtimer[_force]_reprogram and hrtimer_interrupt only. When updating

 * cpu_base::*expires_next right away, reprogramming logic would no longer

 * work.

 *

 * When a softirq is pending, we can ignore the HRTIMER_ACTIVE_SOFT bases,

 * those timers will get run whenever the softirq gets handled, at the end of

 * hrtimer_run_softirq(), hrtimer_update_softirq_timer() will re-add these bases.

 *

 * Therefore softirq values are those from the HRTIMER_ACTIVE_SOFT clock bases.

 * The !softirq values are the minima across HRTIMER_ACTIVE_ALL, unless an actual

 * softirq is pending, in which case they're the minima of HRTIMER_ACTIVE_HARD.

 *

 * @active_mask must be one of:

 *  - HRTIMER_ACTIVE_ALL,

 *  - HRTIMER_ACTIVE_SOFT, or

 *  - HRTIMER_ACTIVE_HARD.

	/*

	 * If the soft interrupt has already been activated, ignore the

	 * soft bases. They will be handled in the already raised soft

	 * interrupt.

		/*

		 * Update the soft expiry time. clock_settime() might have

		 * affected it.

	/*

	 * If a softirq timer is expiring first, update cpu_base->next_timer

	 * and program the hardware with the soft expiry time.

/*

 * Is the high resolution mode active ?

	/*

	 * If hres is not active, hardware does not have to be

	 * reprogrammed yet.

	 *

	 * If a hang was detected in the last timer interrupt then we

	 * leave the hang delay active in the hardware. We want the

	 * system to make progress. That also prevents the following

	 * scenario:

	 * T1 expires 50ms from now

	 * T2 expires 5s from now

	 *

	 * T1 is removed, so this code is called and would reprogram

	 * the hardware to 5s from now. Any hrtimer_start after that

	 * will not reprogram the hardware due to hang_detected being

	 * set. So we'd effectively block all timers until the T2 event

	 * fires.

/*

 * Reprogram the event source with checking both queues for the

 * next event

 * Called with interrupts disabled and base->lock held

 High resolution timer related functions */

/*

 * High resolution timer enabled ?

/*

 * Enable / Disable high resolution mode

/*

 * hrtimer_high_res_enabled - query, if the highres mode is enabled

/*

 * Switch to high resolution mode

 "Retrigger" the interrupt to get things going */

 CONFIG_HIGH_RES_TIMERS */

/*

 * Retrigger next event is called after clock was set with interrupts

 * disabled through an SMP function call or directly from low level

 * resume code.

 *

 * This is only invoked when:

 *	- CONFIG_HIGH_RES_TIMERS is enabled.

 *	- CONFIG_NOHZ_COMMON is enabled

 *

 * For the other cases this function is empty and because the call sites

 * are optimized out it vanishes as well, i.e. no need for lots of

 * #ifdeffery.

	/*

	 * When high resolution mode or nohz is active, then the offsets of

	 * CLOCK_REALTIME/TAI/BOOTTIME have to be updated. Otherwise the

	 * next tick will take care of that.

	 *

	 * If high resolution mode is active then the next expiring timer

	 * must be reevaluated and the clock event device reprogrammed if

	 * necessary.

	 *

	 * In the NOHZ case the update of the offset and the reevaluation

	 * of the next expiring timer is enough. The return from the SMP

	 * function call will take care of the reprogramming in case the

	 * CPU was in a NOHZ idle sleep.

/*

 * When a timer is enqueued and expires earlier than the already enqueued

 * timers, we have to check, whether it expires earlier than the timer for

 * which the clock event device was armed.

 *

 * Called with interrupts disabled and base->cpu_base.lock held

	/*

	 * CLOCK_REALTIME timer might be requested with an absolute

	 * expiry time which is less than base->offset. Set it to 0.

		/*

		 * soft hrtimer could be started on a remote CPU. In this

		 * case softirq_expires_next needs to be updated on the

		 * remote CPU. The soft hrtimer will not expire before the

		 * first hard hrtimer on the remote CPU -

		 * hrtimer_check_target() prevents this case.

	/*

	 * If the timer is not on the current cpu, we cannot reprogram

	 * the other cpus clock event device.

	/*

	 * If the hrtimer interrupt is running, then it will reevaluate the

	 * clock bases and reprogram the clock event device.

	/*

	 * Update the base offsets unconditionally so the following

	 * checks whether the SMP function call is required works.

	 *

	 * The update is safe even when the remote CPU is in the hrtimer

	 * interrupt or the hrtimer soft interrupt and expiring affected

	 * bases. Either it will see the update before handling a base or

	 * it will see it when it finishes the processing and reevaluates

	 * the next expiring timer.

	/*

	 * If the sequence did not change over the update then the

	 * remote CPU already handled it.

	/*

	 * If the remote CPU is currently handling an hrtimer interrupt, it

	 * will reevaluate the first expiring timer of all clock bases

	 * before reprogramming. Nothing to do here.

	/*

	 * Walk the affected clock bases and check whether the first expiring

	 * timer in a clock base is moving ahead of the first expiring timer of

	 * @cpu_base. If so, the IPI must be invoked because per CPU clock

	 * event devices cannot be remotely reprogrammed.

 Extra check for softirq clock bases */

/*

 * Clock was set. This might affect CLOCK_REALTIME, CLOCK_TAI and

 * CLOCK_BOOTTIME (for late sleep time injection).

 *

 * This requires to update the offsets for these clocks

 * vs. CLOCK_MONOTONIC. When high resolution timers are enabled, then this

 * also requires to eventually reprogram the per CPU clock event devices

 * when the change moves an affected timer ahead of the first expiring

 * timer on that CPU. Obviously remote per CPU clock event devices cannot

 * be reprogrammed. The other reason why an IPI has to be sent is when the

 * system is in !HIGH_RES and NOHZ mode. The NOHZ mode updates the offsets

 * in the tick, which obviously might be stopped, so this has to bring out

 * the remote CPU which might sleep in idle to get this sorted.

 Avoid interrupting CPUs if possible */

/*

 * Called from timekeeping code to reprogram the hrtimer interrupt device

 * on all cpus and to notify timerfd.

/*

 * Called during resume either directly from via timekeeping_resume()

 * or in the case of s2idle from tick_unfreeze() to ensure that the

 * hrtimers are up to date.

 Retrigger on the local CPU */

/*

 * Counterpart to lock_hrtimer_base above:

/**

 * hrtimer_forward - forward the timer expiry

 * @timer:	hrtimer to forward

 * @now:	forward past this time

 * @interval:	the interval to forward

 *

 * Forward the timer expiry so it will expire in the future.

 * Returns the number of overruns.

 *

 * Can be safely called from the callback function of @timer. If

 * called from other contexts @timer must neither be enqueued nor

 * running the callback and the caller needs to take care of

 * serialization.

 *

 * Note: This only updates the timer expiry value and does not requeue

 * the timer.

		/*

		 * This (and the ktime_add() below) is the

		 * correction for exact:

/*

 * enqueue_hrtimer - internal function to (re)start a timer

 *

 * The timer is inserted in expiry order. Insertion into the

 * red black tree is O(log(n)). Must hold the base lock.

 *

 * Returns 1 when the new timer is the leftmost timer in the tree.

 Pairs with the lockless read in hrtimer_is_queued() */

/*

 * __remove_hrtimer - internal function to remove a timer

 *

 * Caller must hold the base lock.

 *

 * High resolution timer mode reprograms the clock event device when the

 * timer is the one which expires next. The caller can disable this by setting

 * reprogram to zero. This is useful, when the context does a reprogramming

 * anyway (e.g. timer interrupt)

 Pairs with the lockless read in hrtimer_is_queued() */

	/*

	 * Note: If reprogram is false we do not update

	 * cpu_base->next_timer. This happens when we remove the first

	 * timer on a remote cpu. No harm as we never dereference

	 * cpu_base->next_timer. So the worst thing what can happen is

	 * an superfluous call to hrtimer_force_reprogram() on the

	 * remote cpu later on if the same timer gets enqueued again.

/*

 * remove hrtimer, called with base lock held

		/*

		 * Remove the timer and force reprogramming when high

		 * resolution mode is active and the timer is on the current

		 * CPU. If we remove a timer on another CPU, reprogramming is

		 * skipped. The interrupt event on this CPU is fired and

		 * reprogramming happens in the interrupt handler. This is a

		 * rare case and less expensive than a smp call.

		/*

		 * If the timer is not restarted then reprogramming is

		 * required if the timer is local. If it is local and about

		 * to be restarted, avoid programming it twice (on removal

		 * and a moment later when it's requeued).

	/*

	 * CONFIG_TIME_LOW_RES indicates that the system has no way to return

	 * granular time values. For relative timers we add hrtimer_resolution

	 * (i.e. one jiffie) to prevent short timeouts.

	/*

	 * Find the next SOFT expiration.

	/*

	 * reprogramming needs to be triggered, even if the next soft

	 * hrtimer expires at the same time than the next hard

	 * hrtimer. cpu_base->softirq_expires_next needs to be updated!

	/*

	 * cpu_base->*next_timer is recomputed by __hrtimer_get_next_event()

	 * cpu_base->*expires_next is only set by hrtimer_reprogram()

	/*

	 * If the timer is on the local cpu base and is the first expiring

	 * timer then this might end up reprogramming the hardware twice

	 * (on removal and on enqueue). To avoid that by prevent the

	 * reprogram on removal, keep the timer local to the current CPU

	 * and enforce reprogramming after it is queued no matter whether

	 * it is the new first expiring timer again or not.

	/*

	 * Remove an active timer from the queue. In case it is not queued

	 * on the current CPU, make sure that remove_hrtimer() updates the

	 * remote data correctly.

	 *

	 * If it's on the current CPU and the first expiring timer, then

	 * skip reprogramming, keep the timer local and enforce

	 * reprogramming later if it was the first expiring timer.  This

	 * avoids programming the underlying clock event twice (once at

	 * removal and once after enqueue).

 Switch the timer base, if necessary: */

	/*

	 * Timer was forced to stay on the current CPU to avoid

	 * reprogramming on removal and enqueue. Force reprogram the

	 * hardware by evaluating the new first expiring timer.

/**

 * hrtimer_start_range_ns - (re)start an hrtimer

 * @timer:	the timer to be added

 * @tim:	expiry time

 * @delta_ns:	"slack" range for the timer

 * @mode:	timer mode: absolute (HRTIMER_MODE_ABS) or

 *		relative (HRTIMER_MODE_REL), and pinned (HRTIMER_MODE_PINNED);

 *		softirq based mode is considered for debug purpose only!

	/*

	 * Check whether the HRTIMER_MODE_SOFT bit and hrtimer.is_soft

	 * match on CONFIG_PREEMPT_RT = n. With PREEMPT_RT check the hard

	 * expiry mode because unmarked timers are moved to softirq expiry.

/**

 * hrtimer_try_to_cancel - try to deactivate a timer

 * @timer:	hrtimer to stop

 *

 * Returns:

 *

 *  *  0 when the timer was not active

 *  *  1 when the timer was active

 *  * -1 when the timer is currently executing the callback function and

 *    cannot be stopped

	/*

	 * Check lockless first. If the timer is not active (neither

	 * enqueued nor running the callback, nothing to do here.  The

	 * base lock does not serialize against a concurrent enqueue,

	 * so we can avoid taking it.

/*

 * The counterpart to hrtimer_cancel_wait_running().

 *

 * If there is a waiter for cpu_base->expiry_lock, then it was waiting for

 * the timer callback to finish. Drop expiry_lock and reacquire it. That

 * allows the waiter to acquire the lock and make progress.

/*

 * This function is called on PREEMPT_RT kernels when the fast path

 * deletion of a timer failed because the timer callback function was

 * running.

 *

 * This prevents priority inversion: if the soft irq thread is preempted

 * in the middle of a timer callback, then calling del_timer_sync() can

 * lead to two issues:

 *

 *  - If the caller is on a remote CPU then it has to spin wait for the timer

 *    handler to complete. This can result in unbound priority inversion.

 *

 *  - If the caller originates from the task which preempted the timer

 *    handler on the same CPU, then spin waiting for the timer handler to

 *    complete is never going to end.

 Lockless read. Prevent the compiler from reloading it below */

	/*

	 * Just relax if the timer expires in hard interrupt context or if

	 * it is currently on the migration base.

	/*

	 * Mark the base as contended and grab the expiry lock, which is

	 * held by the softirq across the timer callback. Drop the lock

	 * immediately so the softirq can expire the next timer. In theory

	 * the timer could already be running again, but that's more than

	 * unlikely and just causes another wait loop.

/**

 * hrtimer_cancel - cancel a timer and wait for the handler to finish.

 * @timer:	the timer to be cancelled

 *

 * Returns:

 *  0 when the timer was not active

 *  1 when the timer was active

/**

 * __hrtimer_get_remaining - get remaining time for the timer

 * @timer:	the timer to read

 * @adjust:	adjust relative timers when CONFIG_TIME_LOW_RES=y

/**

 * hrtimer_get_next_event - get the time until next expiry event

 *

 * Returns the next expiry time or KTIME_MAX if no timer is pending.

/**

 * hrtimer_next_event_without - time until next expiry event w/o one timer

 * @exclude:	timer to exclude

 *

 * Returns the next expiry time over all timers except for the @exclude one or

 * KTIME_MAX if none of them is pending.

	/*

	 * On PREEMPT_RT enabled kernels hrtimers which are not explicitly

	 * marked for hard interrupt expiry mode are moved into soft

	 * interrupt context for latency reasons and because the callbacks

	 * can invoke functions which might sleep on RT, e.g. spin_lock().

	/*

	 * POSIX magic: Relative CLOCK_REALTIME timers are not affected by

	 * clock modifications, so they needs to become CLOCK_MONOTONIC to

	 * ensure POSIX compliance.

/**

 * hrtimer_init - initialize a timer to the given clock

 * @timer:	the timer to be initialized

 * @clock_id:	the clock to be used

 * @mode:       The modes which are relevant for initialization:

 *              HRTIMER_MODE_ABS, HRTIMER_MODE_REL, HRTIMER_MODE_ABS_SOFT,

 *              HRTIMER_MODE_REL_SOFT

 *

 *              The PINNED variants of the above can be handed in,

 *              but the PINNED bit is ignored as pinning happens

 *              when the hrtimer is started

/*

 * A timer is active, when it is enqueued into the rbtree or the

 * callback function is running or it's in the state of being migrated

 * to another cpu.

 *

 * It is important for this function to not return a false negative.

/*

 * The write_seqcount_barrier()s in __run_hrtimer() split the thing into 3

 * distinct sections:

 *

 *  - queued:	the timer is queued

 *  - callback:	the timer is being ran

 *  - post:	the timer is inactive or (re)queued

 *

 * On the read side we ensure we observe timer->state and cpu_base->running

 * from the same section, if anything changed while we looked at it, we retry.

 * This includes timer->base changing because sequence numbers alone are

 * insufficient for that.

 *

 * The sequence numbers are required because otherwise we could still observe

 * a false negative if the read side got smeared over multiple consecutive

 * __run_hrtimer() invocations.

	/*

	 * Separate the ->running assignment from the ->state assignment.

	 *

	 * As with a regular write barrier, this ensures the read side in

	 * hrtimer_active() cannot observe base->running == NULL &&

	 * timer->state == INACTIVE.

	/*

	 * Clear the 'is relative' flag for the TIME_LOW_RES case. If the

	 * timer is restarted with a period then it becomes an absolute

	 * timer. If its not restarted it does not matter.

	/*

	 * The timer is marked as running in the CPU base, so it is

	 * protected against migration to a different CPU even if the lock

	 * is dropped.

	/*

	 * Note: We clear the running state after enqueue_hrtimer and

	 * we do not reprogram the event hardware. Happens either in

	 * hrtimer_start_range_ns() or in hrtimer_interrupt()

	 *

	 * Note: Because we dropped the cpu_base->lock above,

	 * hrtimer_start_range_ns() can have popped in and enqueued the timer

	 * for us already.

	/*

	 * Separate the ->running assignment from the ->state assignment.

	 *

	 * As with a regular write barrier, this ensures the read side in

	 * hrtimer_active() cannot observe base->running.timer == NULL &&

	 * timer->state == INACTIVE.

			/*

			 * The immediate goal for using the softexpires is

			 * minimizing wakeups, not running timers at the

			 * earliest interrupt after their soft expiration.

			 * This allows us to avoid using a Priority Search

			 * Tree, which can answer a stabbing query for

			 * overlapping intervals and instead use the simple

			 * BST we already have.

			 * We don't add extra wakeups by delaying timers that

			 * are right-of a not yet expired timer, because that

			 * timer will have to trigger a wakeup anyway.

/*

 * High resolution timer interrupt

 * Called with interrupts disabled

	/*

	 * We set expires_next to KTIME_MAX here with cpu_base->lock

	 * held to prevent that a timer is enqueued in our queue via

	 * the migration code. This does not affect enqueueing of

	 * timers which run their callback and need to be requeued on

	 * this CPU.

 Reevaluate the clock bases for the [soft] next expiry */

	/*

	 * Store the new expiry value so the migration code can verify

	 * against it.

 Reprogramming necessary ? */

	/*

	 * The next timer was already expired due to:

	 * - tracing

	 * - long lasting callbacks

	 * - being scheduled away when running in a VM

	 *

	 * We need to prevent that we loop forever in the hrtimer

	 * interrupt routine. We give it 3 attempts to avoid

	 * overreacting on some spurious event.

	 *

	 * Acquire base lock for updating the offsets and retrieving

	 * the current time.

	/*

	 * Give the system a chance to do something else than looping

	 * here. We stored the entry time, so we know exactly how long

	 * we spent here. We schedule the next event this amount of

	 * time away.

	/*

	 * Limit it to a sensible value as we enforce a longer

	 * delay. Give the CPU at least 100ms to catch up.

 called with interrupts disabled */

 CONFIG_HIGH_RES_TIMERS */

 !CONFIG_HIGH_RES_TIMERS */

/*

 * Called from run_local_timers in hardirq context every jiffy

	/*

	 * This _is_ ugly: We have to check periodically, whether we

	 * can switch to highres and / or nohz mode. The clocksource

	 * switch happens with xtime_lock held. Notification from

	 * there only sets the check bit in the tick_oneshot code,

	 * otherwise we might deadlock vs. xtime_lock.

/*

 * Sleep related functions:

/**

 * hrtimer_sleeper_start_expires - Start a hrtimer sleeper timer

 * @sl:		sleeper to be started

 * @mode:	timer mode abs/rel

 *

 * Wrapper around hrtimer_start_expires() for hrtimer_sleeper based timers

 * to allow PREEMPT_RT to tweak the delivery mode (soft/hardirq context)

	/*

	 * Make the enqueue delivery mode check work on RT. If the sleeper

	 * was initialized for hard interrupt delivery, force the mode bit.

	 * This is a special case for hrtimer_sleepers because

	 * hrtimer_init_sleeper() determines the delivery mode on RT so the

	 * fiddling with this decision is avoided at the call sites.

	/*

	 * On PREEMPT_RT enabled kernels hrtimers which are not explicitly

	 * marked for hard interrupt expiry mode are moved into soft

	 * interrupt context either for latency reasons or because the

	 * hrtimer callback takes regular spinlocks or invokes other

	 * functions which are not suitable for hard interrupt context on

	 * PREEMPT_RT.

	 *

	 * The hrtimer_sleeper callback is RT compatible in hard interrupt

	 * context, but there is a latency concern: Untrusted userspace can

	 * spawn many threads which arm timers for the same expiry time on

	 * the same CPU. That causes a latency spike due to the wakeup of

	 * a gazillion threads.

	 *

	 * OTOH, privileged real-time user space applications rely on the

	 * low latency of hard interrupt wakeups. If the current task is in

	 * a real-time scheduling class, mark the mode for hard interrupt

	 * expiry.

/**

 * hrtimer_init_sleeper - initialize sleeper to the given clock

 * @sl:		sleeper to be initialized

 * @clock_id:	the clock to be used

 * @mode:	timer mode abs/rel

 Absolute timers do not update the rmtp value and restart: */

/*

 * Functions related to boot-time initialization:

		/*

		 * Mark it as ENQUEUED not INACTIVE otherwise the

		 * timer could be seen as !active and just vanish away

		 * under us on another CPU

		/*

		 * Enqueue the timers on the new cpu. This does not

		 * reprogram the event device in case the timer

		 * expires before the earliest on this CPU, but we run

		 * hrtimer_interrupt after we migrated everything to

		 * sort out already expired timers and reprogram the

		 * event device.

	/*

	 * this BH disable ensures that raise_softirq_irqoff() does

	 * not wakeup ksoftirqd (and acquire the pi-lock) while

	 * holding the cpu_base lock

	/*

	 * The caller is globally serialized and nobody else

	 * takes two locks at once, deadlock is not possible.

	/*

	 * The migration might have changed the first expiring softirq

	 * timer on this CPU. Update it.

 Check, if we got expired work to do */

 CONFIG_HOTPLUG_CPU */

/**

 * schedule_hrtimeout_range_clock - sleep until timeout

 * @expires:	timeout value (ktime_t)

 * @delta:	slack in expires timeout (ktime_t)

 * @mode:	timer mode

 * @clock_id:	timer clock to be used

	/*

	 * Optimize when a zero timeout value is given. It does not

	 * matter whether this is an absolute or a relative time.

	/*

	 * A NULL parameter means "infinite"

/**

 * schedule_hrtimeout_range - sleep until timeout

 * @expires:	timeout value (ktime_t)

 * @delta:	slack in expires timeout (ktime_t)

 * @mode:	timer mode

 *

 * Make the current task sleep until the given expiry time has

 * elapsed. The routine will return immediately unless

 * the current task state has been set (see set_current_state()).

 *

 * The @delta argument gives the kernel the freedom to schedule the

 * actual wakeup to a time that is both power and performance friendly.

 * The kernel give the normal best effort behavior for "@expires+@delta",

 * but may decide to fire the timer earlier, but no earlier than @expires.

 *

 * You can set the task state as follows -

 *

 * %TASK_UNINTERRUPTIBLE - at least @timeout time is guaranteed to

 * pass before the routine returns unless the current task is explicitly

 * woken up, (e.g. by wake_up_process()).

 *

 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is

 * delivered to the current task or the current task is explicitly woken

 * up.

 *

 * The current task state is guaranteed to be TASK_RUNNING when this

 * routine returns.

 *

 * Returns 0 when the timer has expired. If the task was woken before the

 * timer expired by a signal (only possible in state TASK_INTERRUPTIBLE) or

 * by an explicit wakeup, it returns -EINTR.

/**

 * schedule_hrtimeout - sleep until timeout

 * @expires:	timeout value (ktime_t)

 * @mode:	timer mode

 *

 * Make the current task sleep until the given expiry time has

 * elapsed. The routine will return immediately unless

 * the current task state has been set (see set_current_state()).

 *

 * You can set the task state as follows -

 *

 * %TASK_UNINTERRUPTIBLE - at least @timeout time is guaranteed to

 * pass before the routine returns unless the current task is explicitly

 * woken up, (e.g. by wake_up_process()).

 *

 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is

 * delivered to the current task or the current task is explicitly woken

 * up.

 *

 * The current task state is guaranteed to be TASK_RUNNING when this

 * routine returns.

 *

 * Returns 0 when the timer has expired. If the task was woken before the

 * timer expired by a signal (only possible in state TASK_INTERRUPTIBLE) or

 * by an explicit wakeup, it returns -EINTR.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Based on clocksource code. See commit 74d23cc704d1

/**

 * timecounter_read_delta - get nanoseconds since last call of this function

 * @tc:         Pointer to time counter

 *

 * When the underlying cycle counter runs over, this will be handled

 * correctly as long as it does not run over more than once between

 * calls.

 *

 * The first call to this function for a new time counter initializes

 * the time tracking and returns an undefined result.

 read cycle counter: */

 calculate the delta since the last timecounter_read_delta(): */

 convert to nanoseconds: */

 update time stamp of timecounter_read_delta() call: */

 increment time by nanoseconds since last call */

/*

 * This is like cyclecounter_cyc2ns(), but it is used for computing a

 * time previous to the time stored in the cycle counter.

	/*

	 * Instead of always treating cycle_tstamp as more recent

	 * than tc->cycle_last, detect when it is too far in the

	 * future and treat it as old time stamp instead.

 SPDX-License-Identifier: GPL-2.0

/*

 * Author: Andrei Vagin <avagin@openvz.org>

 * Author: Dmitry Safonov <dima@arista.com>

	/*

	 * Check that @tim value is in [offset, KTIME_MAX + offset]

	 * and subtract offset.

		/*

		 * User can specify @tim *absolute* value - if it's lesser than

		 * the time namespace's offset - it's already expired.

/**

 * clone_time_ns - Clone a time namespace

 * @user_ns:	User namespace which owns a new namespace.

 * @old_ns:	Namespace to clone

 *

 * Clone @old_ns and set the clone refcount to 1

 *

 * Return: The new namespace or ERR_PTR.

/**

 * copy_time_ns - Create timens_for_children from @old_ns

 * @flags:	Cloning flags

 * @user_ns:	User namespace which owns a new namespace.

 * @old_ns:	Namespace to clone

 *

 * If CLONE_NEWTIME specified in @flags, creates a new timens_for_children;

 * adds a refcounter to @old_ns otherwise.

 *

 * Return: timens_for_children namespace or ERR_PTR.

/*

 * A time namespace VVAR page has the same layout as the VVAR page which

 * contains the system wide VDSO data.

 *

 * For a normal task the VVAR pages are installed in the normal ordering:

 *     VVAR

 *     PVCLOCK

 *     HVCLOCK

 *     TIMENS   <- Not really required

 *

 * Now for a timens task the pages are installed in the following order:

 *     TIMENS

 *     PVCLOCK

 *     HVCLOCK

 *     VVAR

 *

 * The check for vdso_data->clock_mode is in the unlikely path of

 * the seq begin magic. So for the non-timens case most of the time

 * 'seq' is even, so the branch is not taken.

 *

 * If 'seq' is odd, i.e. a concurrent update is in progress, the extra check

 * for vdso_data->clock_mode is a non-issue. The task is spin waiting for the

 * update to finish and for 'seq' to become even anyway.

 *

 * Timens page has vdso_data->clock_mode set to VDSO_CLOCKMODE_TIMENS which

 * enforces the time namespace handling path.

/*

 * Protects possibly multiple offsets writers racing each other

 * and tasks entering the namespace.

 Fast-path, taken by every task in namespace except the first. */

 Nothing to-do: vvar_page has been already initialized. */

 create_new_namespaces() already incremented the ref counter */

		/*

		 * KTIME_SEC_MAX is divided by 2 to be sure that KTIME_MAX is

		 * still unreachable.

 Don't report errors after this line */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains functions which manage high resolution tick

 * related events.

 *

 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner

/**

 * tick_program_event

		/*

		 * We don't need the clock event device any more, stop it.

		/*

		 * We need the clock event again, configure it in ONESHOT mode

		 * before using it.

/**

 * tick_resume_oneshot - resume oneshot mode

/**

 * tick_setup_oneshot - setup the event device for oneshot mode (hres or nohz)

/**

 * tick_switch_to_oneshot - switch to oneshot mode

/**

 * tick_check_oneshot_mode - check whether the system is in oneshot mode

 *

 * returns 1 when either nohz or highres are enabled. otherwise 0.

/**

 * tick_init_highres - switch to high resolution mode

 *

 * Called with interrupts disabled.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1992 Darren Senn

 These are all the functions necessary to implement itimers */

/**

 * itimer_get_remtime - get remaining time for the timer

 *

 * @timer: the timer to read

 *

 * Returns the delta between the expiry time and now, which can be

 * less than zero or 1usec for an pending expired timer

	/*

	 * Racy but safe: if the itimer expires after the above

	 * hrtimer_get_remtime() call but before this condition

	 * then we return 0 - which is correct.

 about to fire */

/*

 * The timer is automagically restarted, when interval != 0

/*

 * Returns true if the timeval is in canonical form

 We are sharing ->siglock with it_real_fn() */

/**

 * alarm_setitimer - set alarm in seconds

 *

 * @seconds:	number of seconds until alarm

 *		0 disables the alarm

 *

 * Returns the remaining time in seconds of a pending timer or 0 when

 * the timer is not active.

 *

 * On 32 bit machines the seconds value is limited to (INT_MAX/2) to avoid

 * negative timeval settings which would cause immediate expiry.

	/*

	 * We can't return 0 if we have an alarm pending ...  And we'd

	 * better return too much than too little anyway

/*

 * For backwards compatibility?  This can be done in libc so Alpha

 * and all newer ports shouldn't need it.

 Validate the timevals in value. */

 Validate the timevals in value.  */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Kernel internal timers

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  1997-01-28  Modified by Finn Arne Gangstad to make timers scale better.

 *

 *  1997-09-10  Updated NTP code according to technical memorandum Jan '96

 *              "A Kernel Model for Precision Timekeeping" by Dave Mills

 *  1998-12-24  Fixed a xtime SMP race (we need the xtime_lock rw spinlock to

 *              serialize accesses to xtime/lost_ticks).

 *                              Copyright (C) 1998  Andrea Arcangeli

 *  1999-03-10  Improved NTP compatibility by Ulrich Windl

 *  2002-05-31	Move sys_sysinfo here and make its locking sane, Robert Love

 *  2000-10-05  Implemented scalable SMP per-CPU timer handling.

 *                              Copyright (C) 2000, 2001, 2002  Ingo Molnar

 *              Designed by David S. Miller, Alexey Kuznetsov and Ingo Molnar

/*

 * The timer wheel has LVL_DEPTH array levels. Each level provides an array of

 * LVL_SIZE buckets. Each level is driven by its own clock and therefor each

 * level has a different granularity.

 *

 * The level granularity is:		LVL_CLK_DIV ^ lvl

 * The level clock frequency is:	HZ / (LVL_CLK_DIV ^ level)

 *

 * The array level of a newly armed timer depends on the relative expiry

 * time. The farther the expiry time is away the higher the array level and

 * therefor the granularity becomes.

 *

 * Contrary to the original timer wheel implementation, which aims for 'exact'

 * expiry of the timers, this implementation removes the need for recascading

 * the timers into the lower array levels. The previous 'classic' timer wheel

 * implementation of the kernel already violated the 'exact' expiry by adding

 * slack to the expiry time to provide batched expiration. The granularity

 * levels provide implicit batching.

 *

 * This is an optimization of the original timer wheel implementation for the

 * majority of the timer wheel use cases: timeouts. The vast majority of

 * timeout timers (networking, disk I/O ...) are canceled before expiry. If

 * the timeout expires it indicates that normal operation is disturbed, so it

 * does not matter much whether the timeout comes with a slight delay.

 *

 * The only exception to this are networking timers with a small expiry

 * time. They rely on the granularity. Those fit into the first wheel level,

 * which has HZ granularity.

 *

 * We don't have cascading anymore. timers with a expiry time above the

 * capacity of the last wheel level are force expired at the maximum timeout

 * value of the last wheel level. From data sampling we know that the maximum

 * value observed is 5 days (network connection tracking), so this should not

 * be an issue.

 *

 * The currently chosen array constants values are a good compromise between

 * array size and granularity.

 *

 * This results in the following granularity and range levels:

 *

 * HZ 1000 steps

 * Level Offset  Granularity            Range

 *  0      0         1 ms                0 ms -         63 ms

 *  1     64         8 ms               64 ms -        511 ms

 *  2    128        64 ms              512 ms -       4095 ms (512ms - ~4s)

 *  3    192       512 ms             4096 ms -      32767 ms (~4s - ~32s)

 *  4    256      4096 ms (~4s)      32768 ms -     262143 ms (~32s - ~4m)

 *  5    320     32768 ms (~32s)    262144 ms -    2097151 ms (~4m - ~34m)

 *  6    384    262144 ms (~4m)    2097152 ms -   16777215 ms (~34m - ~4h)

 *  7    448   2097152 ms (~34m)  16777216 ms -  134217727 ms (~4h - ~1d)

 *  8    512  16777216 ms (~4h)  134217728 ms - 1073741822 ms (~1d - ~12d)

 *

 * HZ  300

 * Level Offset  Granularity            Range

 *  0	   0         3 ms                0 ms -        210 ms

 *  1	  64        26 ms              213 ms -       1703 ms (213ms - ~1s)

 *  2	 128       213 ms             1706 ms -      13650 ms (~1s - ~13s)

 *  3	 192      1706 ms (~1s)      13653 ms -     109223 ms (~13s - ~1m)

 *  4	 256     13653 ms (~13s)    109226 ms -     873810 ms (~1m - ~14m)

 *  5	 320    109226 ms (~1m)     873813 ms -    6990503 ms (~14m - ~1h)

 *  6	 384    873813 ms (~14m)   6990506 ms -   55924050 ms (~1h - ~15h)

 *  7	 448   6990506 ms (~1h)   55924053 ms -  447392423 ms (~15h - ~5d)

 *  8    512  55924053 ms (~15h) 447392426 ms - 3579139406 ms (~5d - ~41d)

 *

 * HZ  250

 * Level Offset  Granularity            Range

 *  0	   0         4 ms                0 ms -        255 ms

 *  1	  64        32 ms              256 ms -       2047 ms (256ms - ~2s)

 *  2	 128       256 ms             2048 ms -      16383 ms (~2s - ~16s)

 *  3	 192      2048 ms (~2s)      16384 ms -     131071 ms (~16s - ~2m)

 *  4	 256     16384 ms (~16s)    131072 ms -    1048575 ms (~2m - ~17m)

 *  5	 320    131072 ms (~2m)    1048576 ms -    8388607 ms (~17m - ~2h)

 *  6	 384   1048576 ms (~17m)   8388608 ms -   67108863 ms (~2h - ~18h)

 *  7	 448   8388608 ms (~2h)   67108864 ms -  536870911 ms (~18h - ~6d)

 *  8    512  67108864 ms (~18h) 536870912 ms - 4294967288 ms (~6d - ~49d)

 *

 * HZ  100

 * Level Offset  Granularity            Range

 *  0	   0         10 ms               0 ms -        630 ms

 *  1	  64         80 ms             640 ms -       5110 ms (640ms - ~5s)

 *  2	 128        640 ms            5120 ms -      40950 ms (~5s - ~40s)

 *  3	 192       5120 ms (~5s)     40960 ms -     327670 ms (~40s - ~5m)

 *  4	 256      40960 ms (~40s)   327680 ms -    2621430 ms (~5m - ~43m)

 *  5	 320     327680 ms (~5m)   2621440 ms -   20971510 ms (~43m - ~5h)

 *  6	 384    2621440 ms (~43m) 20971520 ms -  167772150 ms (~5h - ~1d)

 *  7	 448   20971520 ms (~5h) 167772160 ms - 1342177270 ms (~1d - ~15d)

 Clock divisor for the next level */

/*

 * The time start value for each level to select the bucket at enqueue

 * time. We start from the last possible delta of the previous level

 * so that we can later add an extra LVL_GRAN(n) to n (see calc_index()).

 Size of each clock level */

 Level depth */

 The cutoff (max. capacity of the wheel) */

/*

 * The resulting wheel size. If NOHZ is configured we allocate two

 * wheels so we have a separate storage for the deferrable timers.

 !CONFIG_SMP */

 NO_HZ_COMMON */

	/*

	 * We don't want all cpus firing their timers at once hitting the

	 * same lock or cachelines, so we skew each extra cpu with an extra

	 * 3 jiffies. This 3 jiffies came originally from the mm/ code which

	 * already did this.

	 * The skew is done by adding 3*cpunr, then round, then subtract this

	 * extra offset again.

	/*

	 * If the target jiffie is just after a whole second (which can happen

	 * due to delays of the timer irq, long irq off times etc etc) then

	 * we should round down to the whole second, not up. Use 1/4th second

	 * as cutoff for this rounding as an extreme upper bound for this.

	 * But never round down if @force_up is set.

 round down */

 round up */

 now that we have rounded, subtract the extra skew again */

	/*

	 * Make sure j is still in the future. Otherwise return the

	 * unmodified value.

/**

 * __round_jiffies - function to round jiffies to a full second

 * @j: the time in (absolute) jiffies that should be rounded

 * @cpu: the processor number on which the timeout will happen

 *

 * __round_jiffies() rounds an absolute time in the future (in jiffies)

 * up or down to (approximately) full seconds. This is useful for timers

 * for which the exact time they fire does not matter too much, as long as

 * they fire approximately every X seconds.

 *

 * By rounding these timers to whole seconds, all such timers will fire

 * at the same time, rather than at various times spread out. The goal

 * of this is to have the CPU wake up less, which saves power.

 *

 * The exact rounding is skewed for each processor to avoid all

 * processors firing at the exact same time, which could lead

 * to lock contention or spurious cache line bouncing.

 *

 * The return value is the rounded version of the @j parameter.

/**

 * __round_jiffies_relative - function to round jiffies to a full second

 * @j: the time in (relative) jiffies that should be rounded

 * @cpu: the processor number on which the timeout will happen

 *

 * __round_jiffies_relative() rounds a time delta  in the future (in jiffies)

 * up or down to (approximately) full seconds. This is useful for timers

 * for which the exact time they fire does not matter too much, as long as

 * they fire approximately every X seconds.

 *

 * By rounding these timers to whole seconds, all such timers will fire

 * at the same time, rather than at various times spread out. The goal

 * of this is to have the CPU wake up less, which saves power.

 *

 * The exact rounding is skewed for each processor to avoid all

 * processors firing at the exact same time, which could lead

 * to lock contention or spurious cache line bouncing.

 *

 * The return value is the rounded version of the @j parameter.

 Use j0 because jiffies might change while we run */

/**

 * round_jiffies - function to round jiffies to a full second

 * @j: the time in (absolute) jiffies that should be rounded

 *

 * round_jiffies() rounds an absolute time in the future (in jiffies)

 * up or down to (approximately) full seconds. This is useful for timers

 * for which the exact time they fire does not matter too much, as long as

 * they fire approximately every X seconds.

 *

 * By rounding these timers to whole seconds, all such timers will fire

 * at the same time, rather than at various times spread out. The goal

 * of this is to have the CPU wake up less, which saves power.

 *

 * The return value is the rounded version of the @j parameter.

/**

 * round_jiffies_relative - function to round jiffies to a full second

 * @j: the time in (relative) jiffies that should be rounded

 *

 * round_jiffies_relative() rounds a time delta  in the future (in jiffies)

 * up or down to (approximately) full seconds. This is useful for timers

 * for which the exact time they fire does not matter too much, as long as

 * they fire approximately every X seconds.

 *

 * By rounding these timers to whole seconds, all such timers will fire

 * at the same time, rather than at various times spread out. The goal

 * of this is to have the CPU wake up less, which saves power.

 *

 * The return value is the rounded version of the @j parameter.

/**

 * __round_jiffies_up - function to round jiffies up to a full second

 * @j: the time in (absolute) jiffies that should be rounded

 * @cpu: the processor number on which the timeout will happen

 *

 * This is the same as __round_jiffies() except that it will never

 * round down.  This is useful for timeouts for which the exact time

 * of firing does not matter too much, as long as they don't fire too

 * early.

/**

 * __round_jiffies_up_relative - function to round jiffies up to a full second

 * @j: the time in (relative) jiffies that should be rounded

 * @cpu: the processor number on which the timeout will happen

 *

 * This is the same as __round_jiffies_relative() except that it will never

 * round down.  This is useful for timeouts for which the exact time

 * of firing does not matter too much, as long as they don't fire too

 * early.

 Use j0 because jiffies might change while we run */

/**

 * round_jiffies_up - function to round jiffies up to a full second

 * @j: the time in (absolute) jiffies that should be rounded

 *

 * This is the same as round_jiffies() except that it will never

 * round down.  This is useful for timeouts for which the exact time

 * of firing does not matter too much, as long as they don't fire too

 * early.

/**

 * round_jiffies_up_relative - function to round jiffies up to a full second

 * @j: the time in (relative) jiffies that should be rounded

 *

 * This is the same as round_jiffies_relative() except that it will never

 * round down.  This is useful for timeouts for which the exact time

 * of firing does not matter too much, as long as they don't fire too

 * early.

/*

 * Helper function to calculate the array index for a given expiry

 * time.

	/*

	 * The timer wheel has to guarantee that a timer does not fire

	 * early. Early expiry can happen due to:

	 * - Timer is armed at the edge of a tick

	 * - Truncation of the expiry time in the outer wheel levels

	 *

	 * Round up with level granularity to prevent this.

		/*

		 * Force expire obscene large timeouts to expire at the

		 * capacity limit of the wheel.

	/*

	 * TODO: This wants some optimizing similar to the code below, but we

	 * will do that when we switch from push to pull for deferrable timers.

	/*

	 * We might have to IPI the remote CPU if the base is idle and the

	 * timer is not deferrable. If the other CPU is on the way to idle

	 * then it can't set base->is_idle as we hold the base lock:

/*

 * Enqueue the timer into the hash bucket, mark it pending in

 * the bitmap, store the index in the timer flags then wake up

 * the target CPU if needed.

	/*

	 * Check whether this is the new first expiring timer. The

	 * effective expiry time of the timer is required here

	 * (bucket_expiry) instead of timer->expires.

		/*

		 * Set the next expiry time and kick the CPU so it

		 * can reevaluate the wheel:

/*

 * fixup_init is called when:

 * - an active object is initialized

 Stub timer callback for improperly used timers. */

/*

 * fixup_activate is called when:

 * - an active object is activated

 * - an unknown non-static object is activated

/*

 * fixup_free is called when:

 * - an active object is freed

/*

 * fixup_assert_init is called when:

 * - an untracked/uninit-ed object is found

/**

 * init_timer_key - initialize a timer

 * @timer: the timer to be initialized

 * @func: timer callback function

 * @flags: timer flags

 * @name: name of the timer

 * @key: lockdep class key of the fake lock used for tracking timer

 *       sync lock dependencies

 *

 * init_timer_key() must be done to a timer prior calling *any* of the

 * other timer functions.

	/*

	 * If the timer is deferrable and NO_HZ_COMMON is set then we need

	 * to use the deferrable base.

	/*

	 * If the timer is deferrable and NO_HZ_COMMON is set then we need

	 * to use the deferrable base.

	/*

	 * No need to forward if we are close enough below jiffies.

	 * Also while executing timers, base->clk is 1 offset ahead

	 * of jiffies to avoid endless requeuing to current jiffies.

	/*

	 * If the next expiry value is > jiffies, then we fast forward to

	 * jiffies otherwise we forward to the next expiry value.

/*

 * We are using hashed locking: Holding per_cpu(timer_bases[x]).lock means

 * that all timers which are tied to this base are locked, and the base itself

 * is locked too.

 *

 * So __run_timers/migrate_timers can safely modify all timers which could

 * be found in the base->vectors array.

 *

 * When a timer is migrating then the TIMER_MIGRATING flag is set and we need

 * to wait until the migration is done.

		/*

		 * We need to use READ_ONCE() here, otherwise the compiler

		 * might re-read @tf between the check for TIMER_MIGRATING

		 * and spin_lock().

	/*

	 * This is a common optimization triggered by the networking code - if

	 * the timer is re-modified to have the same timeout or ends up in the

	 * same array bucket then just return:

		/*

		 * The downside of this optimization is that it can result in

		 * larger granularity than you would get from adding a new

		 * timer with this expiry.

		/*

		 * We lock timer base and calculate the bucket index right

		 * here. If the timer ends up in the same bucket, then we

		 * just update the expiry time and avoid the whole

		 * dequeue/enqueue dance.

		/*

		 * Retrieve and compare the array index of the pending

		 * timer. If it matches set the expiry to the new value so a

		 * subsequent call will exit in the expires check above.

		/*

		 * We are trying to schedule the timer on the new base.

		 * However we can't change timer's base while it is running,

		 * otherwise del_timer_sync() can't detect that the timer's

		 * handler yet has not finished. This also guarantees that the

		 * timer is serialized wrt itself.

 See the comment in lock_timer_base() */

	/*

	 * If 'idx' was calculated above and the base time did not advance

	 * between calculating 'idx' and possibly switching the base, only

	 * enqueue_timer() is required. Otherwise we need to (re)calculate

	 * the wheel index via internal_add_timer().

/**

 * mod_timer_pending - modify a pending timer's timeout

 * @timer: the pending timer to be modified

 * @expires: new timeout in jiffies

 *

 * mod_timer_pending() is the same for pending timers as mod_timer(),

 * but will not re-activate and modify already deleted timers.

 *

 * It is useful for unserialized use of timers.

/**

 * mod_timer - modify a timer's timeout

 * @timer: the timer to be modified

 * @expires: new timeout in jiffies

 *

 * mod_timer() is a more efficient way to update the expire field of an

 * active timer (if the timer is inactive it will be activated)

 *

 * mod_timer(timer, expires) is equivalent to:

 *

 *     del_timer(timer); timer->expires = expires; add_timer(timer);

 *

 * Note that if there are multiple unserialized concurrent users of the

 * same timer, then mod_timer() is the only safe way to modify the timeout,

 * since add_timer() cannot modify an already running timer.

 *

 * The function returns whether it has modified a pending timer or not.

 * (ie. mod_timer() of an inactive timer returns 0, mod_timer() of an

 * active timer returns 1.)

/**

 * timer_reduce - Modify a timer's timeout if it would reduce the timeout

 * @timer:	The timer to be modified

 * @expires:	New timeout in jiffies

 *

 * timer_reduce() is very similar to mod_timer(), except that it will only

 * modify a running timer if that would reduce the expiration time (it will

 * start a timer that isn't running).

/**

 * add_timer - start a timer

 * @timer: the timer to be added

 *

 * The kernel will do a ->function(@timer) callback from the

 * timer interrupt at the ->expires point in the future. The

 * current time is 'jiffies'.

 *

 * The timer's ->expires, ->function fields must be set prior calling this

 * function.

 *

 * Timers with an ->expires field in the past will be executed in the next

 * timer tick.

/**

 * add_timer_on - start a timer on a particular CPU

 * @timer: the timer to be added

 * @cpu: the CPU to start it on

 *

 * This is not very scalable on SMP. Double adds are not possible.

	/*

	 * If @timer was on a different CPU, it should be migrated with the

	 * old base locked to prevent other operations proceeding with the

	 * wrong base locked.  See lock_timer_base().

/**

 * del_timer - deactivate a timer.

 * @timer: the timer to be deactivated

 *

 * del_timer() deactivates a timer - this works on both active and inactive

 * timers.

 *

 * The function returns whether it has deactivated a pending timer or not.

 * (ie. del_timer() of an inactive timer returns 0, del_timer() of an

 * active timer returns 1.)

/**

 * try_to_del_timer_sync - Try to deactivate a timer

 * @timer: timer to delete

 *

 * This function tries to deactivate a timer. Upon successful (ret >= 0)

 * exit the timer is not queued and the handler is not running on any CPU.

/*

 * The counterpart to del_timer_wait_running().

 *

 * If there is a waiter for base->expiry_lock, then it was waiting for the

 * timer callback to finish. Drop expiry_lock and reacquire it. That allows

 * the waiter to acquire the lock and make progress.

/*

 * This function is called on PREEMPT_RT kernels when the fast path

 * deletion of a timer failed because the timer callback function was

 * running.

 *

 * This prevents priority inversion, if the softirq thread on a remote CPU

 * got preempted, and it prevents a life lock when the task which tries to

 * delete a timer preempted the softirq thread running the timer callback

 * function.

		/*

		 * Mark the base as contended and grab the expiry lock,

		 * which is held by the softirq across the timer

		 * callback. Drop the lock immediately so the softirq can

		 * expire the next timer. In theory the timer could already

		 * be running again, but that's more than unlikely and just

		 * causes another wait loop.

/**

 * del_timer_sync - deactivate a timer and wait for the handler to finish.

 * @timer: the timer to be deactivated

 *

 * This function only differs from del_timer() on SMP: besides deactivating

 * the timer it also makes sure the handler has finished executing on other

 * CPUs.

 *

 * Synchronization rules: Callers must prevent restarting of the timer,

 * otherwise this function is meaningless. It must not be called from

 * interrupt contexts unless the timer is an irqsafe one. The caller must

 * not hold locks which would prevent completion of the timer's

 * handler. The timer's handler must not call add_timer_on(). Upon exit the

 * timer is not queued and the handler is not running on any CPU.

 *

 * Note: For !irqsafe timers, you must not hold locks that are held in

 *   interrupt context while calling this function. Even if the lock has

 *   nothing to do with the timer in question.  Here's why::

 *

 *    CPU0                             CPU1

 *    ----                             ----

 *                                     <SOFTIRQ>

 *                                       call_timer_fn();

 *                                       base->running_timer = mytimer;

 *    spin_lock_irq(somelock);

 *                                     <IRQ>

 *                                        spin_lock(somelock);

 *    del_timer_sync(mytimer);

 *    while (base->running_timer == mytimer);

 *

 * Now del_timer_sync() will never return and never release somelock.

 * The interrupt on the other CPU is waiting to grab somelock but

 * it has interrupted the softirq that CPU0 is waiting to finish.

 *

 * The function returns whether it has deactivated a pending timer or not.

	/*

	 * If lockdep gives a backtrace here, please reference

	 * the synchronization rules above.

	/*

	 * don't use it in hardirq context, because it

	 * could lead to deadlock.

	/*

	 * Must be able to sleep on PREEMPT_RT because of the slowpath in

	 * del_timer_wait_running().

	/*

	 * It is permissible to free the timer from inside the

	 * function that is called from it, this we need to take into

	 * account for lockdep too. To avoid bogus "held lock freed"

	 * warnings as well as problems when looking into

	 * timer->lockdep_map, make a copy and use that here.

	/*

	 * Couple the lock chain with the lock chain at

	 * del_timer_sync() by acquiring the lock_map around the fn()

	 * call here and in del_timer_sync().

		/*

		 * Restore the preempt count. That gives us a decent

		 * chance to survive and extract information. If the

		 * callback kept a lock held, bad luck, but not worse

		 * than the BUG() we had.

	/*

	 * This value is required only for tracing. base->clk was

	 * incremented directly before expire_timers was called. But expiry

	 * is related to the old base->clk value.

 Is it time to look at the next level? */

 Shift clock for the next level granularity */

/*

 * Find the next pending bucket of a level. Search from level start (@offset)

 * + @clk upwards and if nothing there, search from start of the level

 * (@offset) up to @offset + clk.

/*

 * Search the first expiring timer in the various clock levels. Caller must

 * hold base->lock.

			/*

			 * If the next expiration happens before we reach

			 * the next level, no need to check further.

		/*

		 * Clock for the next level. If the current level clock lower

		 * bits are zero, we look at the next level as is. If not we

		 * need to advance it by one because that's going to be the

		 * next expiring bucket in that level. base->clk is the next

		 * expiring jiffie. So in case of:

		 *

		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0

		 *  0    0    0    0    0    0

		 *

		 * we have to look at all levels @index 0. With

		 *

		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0

		 *  0    0    0    0    0    2

		 *

		 * LVL0 has the next expiring bucket @index 2. The upper

		 * levels have the next expiring bucket @index 1.

		 *

		 * In case that the propagation wraps the next level the same

		 * rules apply:

		 *

		 * LVL5 LVL4 LVL3 LVL2 LVL1 LVL0

		 *  0    0    0    0    F    2

		 *

		 * So after looking at LVL0 we get:

		 *

		 * LVL5 LVL4 LVL3 LVL2 LVL1

		 *  0    0    0    1    0

		 *

		 * So no propagation from LVL1 to LVL2 because that happened

		 * with the add already, but then we need to propagate further

		 * from LVL2 to LVL3.

		 *

		 * So the simple check whether the lower bits of the current

		 * level are 0 or not is sufficient for all cases.

/*

 * Check, if the next hrtimer event is before the next timer wheel

 * event:

	/*

	 * If high resolution timers are enabled

	 * hrtimer_get_next_event() returns KTIME_MAX.

	/*

	 * If the next timer is already expired, return the tick base

	 * time so the tick is fired immediately.

	/*

	 * Round up to the next jiffie. High resolution timers are

	 * off, so the hrtimers are expired in the tick and we need to

	 * make sure that this tick really expires the timer to avoid

	 * a ping pong of the nohz stop code.

	 *

	 * Use DIV_ROUND_UP_ULL to prevent gcc calling __divdi3

/**

 * get_next_timer_interrupt - return the time (clock mono) of the next timer

 * @basej:	base time jiffies

 * @basem:	base time clock monotonic

 *

 * Returns the tick aligned clock monotonic time of the next pending

 * timer or KTIME_MAX if no timer is pending.

	/*

	 * Pretend that there is no timer pending if the cpu is offline.

	 * Possible pending timers will be migrated later to an active cpu.

	/*

	 * We have a fresh next event. Check whether we can forward the

	 * base. We can only do that when @basej is past base->clk

	 * otherwise we might rewind base->clk.

		/*

		 * If we expect to sleep more than a tick, mark the base idle.

		 * Also the tick is stopped so any added timer must forward

		 * the base clk itself to keep granularity small. This idle

		 * logic is only maintained for the BASE_STD base, deferrable

		 * timers may still see large granularity skew (by design).

/**

 * timer_clear_idle - Clear the idle state of the timer base

 *

 * Called with interrupts disabled

	/*

	 * We do this unlocked. The worst outcome is a remote enqueue sending

	 * a pointless IPI, but taking the lock would just make the window for

	 * sending the IPI a few instructions smaller for the cost of taking

	 * the lock in the exit from idle path.

/**

 * __run_timers - run all expired timers (if any) on this CPU.

 * @base: the timer vector to be processed.

		/*

		 * The only possible reason for not finding any expired

		 * timer at this clk is that all matching timers have been

		 * dequeued.

/*

 * This function runs timers and the timer-tq in bottom half context.

/*

 * Called by the local, per-CPU timer interrupt on SMP.

 Raise the softirq only if required. */

 CPU is awake, so check the deferrable base. */

/*

 * Called from the timer interrupt handler to charge one tick to the current

 * process.  user_tick is 1 if the tick is user time, 0 for system.

 Note: this timer irq context must be accounted for as well. */

/*

 * Since schedule_timeout()'s timer is defined on the stack, it must store

 * the target task on the stack as well.

/**

 * schedule_timeout - sleep until timeout

 * @timeout: timeout value in jiffies

 *

 * Make the current task sleep until @timeout jiffies have elapsed.

 * The function behavior depends on the current task state

 * (see also set_current_state() description):

 *

 * %TASK_RUNNING - the scheduler is called, but the task does not sleep

 * at all. That happens because sched_submit_work() does nothing for

 * tasks in %TASK_RUNNING state.

 *

 * %TASK_UNINTERRUPTIBLE - at least @timeout jiffies are guaranteed to

 * pass before the routine returns unless the current task is explicitly

 * woken up, (e.g. by wake_up_process()).

 *

 * %TASK_INTERRUPTIBLE - the routine may return early if a signal is

 * delivered to the current task or the current task is explicitly woken

 * up.

 *

 * The current task state is guaranteed to be %TASK_RUNNING when this

 * routine returns.

 *

 * Specifying a @timeout value of %MAX_SCHEDULE_TIMEOUT will schedule

 * the CPU away without a bound on the timeout. In this case the return

 * value will be %MAX_SCHEDULE_TIMEOUT.

 *

 * Returns 0 when the timer has expired otherwise the remaining time in

 * jiffies will be returned. In all cases the return value is guaranteed

 * to be non-negative.

		/*

		 * These two special cases are useful to be comfortable

		 * in the caller. Nothing more. We could take

		 * MAX_SCHEDULE_TIMEOUT from one of the negative value

		 * but I' d like to return a valid offset (>=0) to allow

		 * the caller to do everything it want with the retval.

		/*

		 * Another bit of PARANOID. Note that the retval will be

		 * 0 since no piece of kernel is supposed to do a check

		 * for a negative retval of schedule_timeout() (since it

		 * should never happens anyway). You just have the printk()

		 * that will tell you if something is gone wrong and where.

 Remove the timer from the object tracker */

/*

 * We can use __set_current_state() here because schedule_timeout() calls

 * schedule() unconditionally.

/*

 * Like schedule_timeout_uninterruptible(), except this task will not contribute

 * to load average.

		/*

		 * The caller is globally serialized and nobody else

		 * takes two locks at once, deadlock is not possible.

		/*

		 * The current CPUs base clock might be stale. Update it

		 * before moving the timers over.

 CONFIG_HOTPLUG_CPU */

/**

 * msleep - sleep safely even with waitqueue interruptions

 * @msecs: Time in milliseconds to sleep for

/**

 * msleep_interruptible - sleep waiting for signals

 * @msecs: Time in milliseconds to sleep for

/**

 * usleep_range - Sleep for an approximate time

 * @min: Minimum time in usecs to sleep

 * @max: Maximum time in usecs to sleep

 *

 * In non-atomic context where the exact wakeup time is flexible, use

 * usleep_range() instead of udelay().  The sleep improves responsiveness

 * by avoiding the CPU-hogging busy-wait of udelay(), and the range reduces

 * power usage by allowing hrtimers to take advantage of an already-

 * scheduled interrupt instead of scheduling a new one just for this sleep.

 Do not return before the requested sleep time has elapsed */

 SPDX-License-Identifier: LGPL-2.1+

/*

 * Traditional implementation of leap year evaluation.

/*

 * Gets the last day of a month.

/*

 * Advances a date by one day.

/*

 * Checks every day in a 160000 years interval centered at 1970-01-01

 * against the expected result.

	/*

	 * 80000 years	= (80000 / 400) * 400 years

	 *		= (80000 / 400) * 146097 days

	 *		= (80000 / 400) * 146097 * 86400 seconds

 SPDX-License-Identifier: GPL-2.0

/*

 * This file contains the base functions to manage periodic tick

 * related events.

 *

 * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner

/*

 * Tick devices

/*

 * Tick next event: keeps track of the tick time. It's updated by the

 * CPU which handles the tick and protected by jiffies_lock. There is

 * no requirement to write hold the jiffies seqcount for it.

/*

 * tick_do_timer_cpu is a timer core internal variable which holds the CPU NR

 * which is responsible for calling do_timer(), i.e. the timekeeping stuff. This

 * variable has two functions:

 *

 * 1) Prevent a thundering herd issue of a gazillion of CPUs trying to grab the

 *    timekeeping lock all at once. Only the CPU which is assigned to do the

 *    update is handling it.

 *

 * 2) Hand off the duty in the NOHZ idle case by setting the value to

 *    TICK_DO_TIMER_NONE, i.e. a non existing CPU. So the next cpu which looks

 *    at it will take over and keep the time keeping alive.  The handover

 *    procedure also covers cpu hotplug.

/*

 * tick_do_timer_boot_cpu indicates the boot CPU temporarily owns

 * tick_do_timer_cpu and it should be taken over by an eligible secondary

 * when one comes online.

/*

 * Debugging: see timer_list.c

/**

 * tick_is_oneshot_available - check for a oneshot capable event device

/*

 * Periodic tick

 Keep track of the next tick event */

/*

 * Event handler for periodic ticks

	/*

	 * The cpu might have transitioned to HIGHRES or NOHZ mode via

	 * update_process_times() -> run_local_timers() ->

	 * hrtimer_run_queues().

		/*

		 * Setup the next period for devices, which do not have

		 * periodic mode:

		/*

		 * Have to be careful here. If we're in oneshot mode,

		 * before we call tick_periodic() in a loop, we need

		 * to be sure we're using a real hardware clocksource.

		 * Otherwise we could get trapped in an infinite

		 * loop, as the tick_periodic() increments jiffies,

		 * which then will increment time, possibly causing

		 * the loop to trigger again and again.

/*

 * Setup the device for a periodic tick

 Broadcast setup ? */

/*

 * Setup the tick device

	/*

	 * First device setup ?

		/*

		 * If no cpu took the do_timer update, assign it to

		 * this cpu:

			/*

			 * The boot CPU may be nohz_full, in which case set

			 * tick_do_timer_boot_cpu so the first housekeeping

			 * secondary that comes up will take do_timer from

			 * us.

		/*

		 * Startup in periodic mode first.

	/*

	 * When the device is not per cpu, pin the interrupt to the

	 * current cpu:

	/*

	 * When global broadcasting is active, check if the current

	 * device is registered as a placeholder for broadcast mode.

	 * This allows us to handle this x86 misfeature in a generic

	 * way. This function also returns !=0 when we keep the

	 * current active broadcast state for this CPU.

 Check if irq affinity can be set */

 Prefer an existing cpu local device */

 Prefer oneshot capable device */

	/*

	 * Use the higher rated one, but prefer a CPU local device with a lower

	 * rating than a non-CPU local device

/*

 * Check whether the new device is a better fit than curdev. curdev

 * can be NULL !

/*

 * Check, if the new registered device should be used. Called with

 * clockevents_lock held and interrupts disabled.

	/*

	 * Replace the eventually existing device by the new

	 * device. If the current device is the broadcast device, do

	 * not give it back to the clockevents layer !

	/*

	 * Can the new device be used as a broadcast device ?

/**

 * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode

 * @state:	The target state (enter/exit)

 *

 * The system enters/leaves a state, where affected devices might stop

 * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups.

 *

 * Called with interrupts disabled, so clockevents_lock is not

 * required here because the local clock event device cannot go away

 * under us.

/*

 * Transfer the do_timer job away from a dying cpu.

 *

 * Called with interrupts disabled. No locking required. If

 * tick_do_timer_cpu is owned by this cpu, nothing can change it.

/*

 * Shutdown an event device on a given cpu:

 *

 * This is called on a life CPU, when a CPU is dead. So we cannot

 * access the hardware device itself.

 * We just set the mode and remove it from the lists.

		/*

		 * Prevent that the clock events layer tries to call

		 * the set mode function!

/**

 * tick_suspend_local - Suspend the local tick device

 *

 * Called from the local cpu for freeze with interrupts disabled.

 *

 * No locks required. Nothing can change the per cpu device.

/**

 * tick_resume_local - Resume the local tick device

 *

 * Called from the local CPU for unfreeze or XEN resume magic.

 *

 * No locks required. Nothing can change the per cpu device.

	/*

	 * Ensure that hrtimers are up to date and the clockevents device

	 * is reprogrammed correctly when high resolution timers are

	 * enabled.

/**

 * tick_suspend - Suspend the tick and the broadcast device

 *

 * Called from syscore_suspend() via timekeeping_suspend with only one

 * CPU online and interrupts disabled or from tick_unfreeze() under

 * tick_freeze_lock.

 *

 * No locks required. Nothing can change the per cpu device.

/**

 * tick_resume - Resume the tick and the broadcast device

 *

 * Called from syscore_resume() via timekeeping_resume with only one

 * CPU online and interrupts disabled.

 *

 * No locks required. Nothing can change the per cpu device.

/**

 * tick_freeze - Suspend the local tick and (possibly) timekeeping.

 *

 * Check if this is the last online CPU executing the function and if so,

 * suspend timekeeping.  Otherwise suspend the local tick.

 *

 * Call with interrupts disabled.  Must be balanced with %tick_unfreeze().

 * Interrupts must not be enabled before the subsequent %tick_unfreeze().

/**

 * tick_unfreeze - Resume the local tick and (possibly) timekeeping.

 *

 * Check if this is the first CPU executing the function and if so, resume

 * timekeeping.  Otherwise resume the local tick.

 *

 * Call with interrupts disabled.  Must be balanced with %tick_freeze().

 * Interrupts must not be enabled after the preceding %tick_freeze().

 CONFIG_SUSPEND */

/**

 * tick_init - initialize the tick control

 SPDX-License-Identifier: GPL-2.0

/*

 * List pending timers

 *

 * Copyright(C) 2006, Red Hat, Inc., Ingo Molnar

/*

 * This allows printing both to /proc/timer_list and

 * to the console (on SysRq-Q):

	/*

	 * Crude but we have to do this O(N*N) thing, because

	 * we have to unlock the base when printing:

 SPDX-License-Identifier: GPL-2.0

/*

 * Timer tick function for architectures that lack generic clockevents,

 * consolidated here from m68k/ia64/parisc/arm.

/**

 * legacy_timer_tick() - advances the timekeeping infrastructure

 * @ticks:	number of ticks, that have elapsed since the last call.

 *

 * This is used by platforms that have not been converted to

 * generic clockevents.

 *

 * If 'ticks' is zero, the CPU is not handling timekeeping, so

 * only perform process accounting and profiling.

 *

 * Must be called with interrupts disabled.

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>

 *  Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar

 *  Copyright(C) 2006-2007  Timesys Corp., Thomas Gleixner

 *

 *  No idle tick implementation for low and high resolution timers

 *

 *  Started by: Thomas Gleixner and Ingo Molnar

/*

 * Per-CPU nohz control structure

/*

 * The time, when the last jiffy update happened. Write access must hold

 * jiffies_lock and jiffies_seq. tick_nohz_next_event() needs to get a

 * consistent view of jiffies and last_jiffies_update.

/*

 * Must be called with interrupts disabled !

	/*

	 * 64bit can do a quick check without holding jiffies lock and

	 * without looking at the sequence count. The smp_load_acquire()

	 * pairs with the update done later in this function.

	 *

	 * 32bit cannot do that because the store of tick_next_period

	 * consists of two 32bit stores and the first store could move it

	 * to a random point in the future.

		/*

		 * Avoid contention on jiffies_lock and protect the quick

		 * check with the sequence count.

 Quick check failed, i.e. update is required. */

	/*

	 * Reevaluate with the lock held. Another CPU might have done the

	 * update already.

 Slow path for long idle sleep times */

 Advance jiffies to complete the jiffies_seq protected job */

	/*

	 * Keep the tick_next_period variable up to date.

		/*

		 * Pairs with smp_load_acquire() in the lockless quick

		 * check above and ensures that the update to jiffies_64 is

		 * not reordered vs. the store to tick_next_period, neither

		 * by the compiler nor by the CPU.

		/*

		 * A plain store is good enough on 32bit as the quick check

		 * above is protected by the sequence count.

	/*

	 * Release the sequence count. calc_global_load() below is not

	 * protected by it, but jiffies_lock needs to be held to prevent

	 * concurrent invocations.

/*

 * Initialize and return retrieve the jiffies update.

 Did we start the jiffies update yet ? */

	/*

	 * Check if the do_timer duty was dropped. We don't care about

	 * concurrency: This happens only when the CPU in charge went

	 * into a long sleep. If two CPUs happen to assign themselves to

	 * this duty, then the jiffies update is still serialized by

	 * jiffies_lock.

	 *

	 * If nohz_full is enabled, this should not happen because the

	 * tick_do_timer_cpu never relinquishes.

 Check, if the jiffies need an update */

	/*

	 * When we are idle and the tick is stopped, we have to touch

	 * the watchdog as we might not schedule for a really long

	 * time. This happens on complete idle SMP systems while

	 * waiting on the login prompt. We also increment the "start of

	 * idle" jiffy stamp so the idle accounting adjustment we do

	 * when we go busy again does not account too much ticks.

		/*

		 * In case the current tick fired too early past its expected

		 * expiration, make sure we don't bypass the next clock reprogramming

		 * to the same deadline.

 Empty, the tick restart happens on tick_nohz_irq_exit() */

/*

 * Kick this CPU if it's full dynticks in order to force it to

 * re-evaluate its dependency on the tick and restart it if necessary.

 * This kick, unlike tick_nohz_full_kick_cpu() and tick_nohz_full_kick_all(),

 * is NMI safe.

/*

 * Kick the CPU if it's full dynticks in order to force it to

 * re-evaluate its dependency on the tick and restart it if necessary.

	/*

	 * If the task is not running, run_posix_cpu_timers()

	 * has nothing to elapse, IPI can then be spared.

	 *

	 * activate_task()                      STORE p->tick_dep_mask

	 *   STORE p->on_rq

	 * __schedule() (switch to task 'p')    smp_mb() (atomic_fetch_or())

	 *   LOCK rq->lock                      LOAD p->on_rq

	 *   smp_mb__after_spin_lock()

	 *   tick_nohz_task_switch()

	 *     LOAD p->tick_dep_mask

	/*

	 * If the task concurrently migrates to another CPU,

	 * we guarantee it sees the new tick dependency upon

	 * schedule.

	 *

	 * set_task_cpu(p, cpu);

	 *   STORE p->cpu = @cpu

	 * __schedule() (switch to task 'p')

	 *   LOCK rq->lock

	 *   smp_mb__after_spin_lock()          STORE p->tick_dep_mask

	 *   tick_nohz_task_switch()            smp_mb() (atomic_fetch_or())

	 *      LOAD p->tick_dep_mask           LOAD p->cpu

/*

 * Kick all full dynticks CPUs in order to force these to re-evaluate

 * their dependency on the tick and restart it if necessary.

/*

 * Set a global tick dependency. Used by perf events that rely on freq and

 * by unstable clock.

/*

 * Set per-CPU tick dependency. Used by scheduler and perf events in order to

 * manage events throttling.

 Perf needs local kick that is NMI safe */

 Remote irq work not NMI-safe */

/*

 * Set a per-task tick dependency. RCU need this. Also posix CPU timers

 * in order to elapse per task timers.

/*

 * Set a per-taskgroup tick dependency. Posix CPU timers need this in order to elapse

 * per process timers.

/*

 * Re-evaluate the need for the tick as we switch the current task.

 * It might need the tick due to per task/process properties:

 * perf events, posix CPU timers, ...

 Get the boot-time nohz CPU list from the kernel parameters. */

	/*

	 * The tick_do_timer_cpu CPU handles housekeeping duty (unbound

	 * timers, workqueues, timekeeping, ...) on behalf of full dynticks

	 * CPUs. It must remain online when nohz full is enabled.

	/*

	 * Full dynticks uses irq work to drive the tick rescheduling on safe

	 * locking contexts. But then we need irq work to raise its own

	 * interrupts to avoid circular dependency on the tick

/*

 * NOHZ - aka dynamic tick functionality

/*

 * NO HZ enabled ?

/*

 * Enable / Disable tickless mode

/**

 * tick_nohz_update_jiffies - update jiffies when idle was interrupted

 *

 * Called from interrupt entry when the CPU was idle

 *

 * In case the sched_tick was stopped on this CPU, we have to check if jiffies

 * must be updated. Otherwise an interrupt handler could use a stale jiffy

 * value. We do this unconditionally on any CPU, as we don't know whether the

 * CPU, which has the update task assigned is in a long sleep.

/*

 * Updates the per-CPU time idle statistics counters

/**

 * get_cpu_idle_time_us - get the total idle time of a CPU

 * @cpu: CPU number to query

 * @last_update_time: variable to store update time in. Do not update

 * counters if NULL.

 *

 * Return the cumulative idle time (since boot) for a given

 * CPU, in microseconds.

 *

 * This time is measured via accounting rather than sampling,

 * and is as accurate as ktime_get() is.

 *

 * This function returns -1 if NOHZ is not enabled.

/**

 * get_cpu_iowait_time_us - get the total iowait time of a CPU

 * @cpu: CPU number to query

 * @last_update_time: variable to store update time in. Do not update

 * counters if NULL.

 *

 * Return the cumulative iowait time (since boot) for a given

 * CPU, in microseconds.

 *

 * This time is measured via accounting rather than sampling,

 * and is as accurate as ktime_get() is.

 *

 * This function returns -1 if NOHZ is not enabled.

 Forward the time to expire in the future */

	/*

	 * Reset to make sure next tick stop doesn't get fooled by past

	 * cached clock deadline.

 Read jiffies and the time when jiffies were updated last */

	/*

	 * Keep the periodic tick, when RCU, architecture or irq_work

	 * requests it.

	 * Aside of that check whether the local timer softirq is

	 * pending. If so its a bad idea to call get_next_timer_interrupt()

	 * because there is an already expired timer, so it will request

	 * immediate expiry, which rearms the hardware timer with a

	 * minimal delta which brings us back to this place

	 * immediately. Lather, rinse and repeat...

		/*

		 * Get the next pending timer. If high resolution

		 * timers are enabled this only takes the timer wheel

		 * timers into account. If high resolution timers are

		 * disabled this also looks at the next expiring

		 * hrtimer.

 Take the next rcu event into account */

	/*

	 * If the tick is due in the next period, keep it ticking or

	 * force prod the timer.

		/*

		 * Tell the timer code that the base is not idle, i.e. undo

		 * the effect of get_next_timer_interrupt():

		/*

		 * We've not stopped the tick yet, and there's a timer in the

		 * next period, so no point in stopping it either, bail.

	/*

	 * If this CPU is the one which had the do_timer() duty last, we limit

	 * the sleep time to the timekeeping max_deferment value.

	 * Otherwise we can sleep as long as we want.

 Calculate the next expiry time */

 Make sure we won't be trying to stop it twice in a row. */

	/*

	 * If this CPU is the one which updates jiffies, then give up

	 * the assignment and let it be taken by the CPU which runs

	 * the tick timer next, which might be this CPU as well. If we

	 * don't drop this here the jiffies might be stale and

	 * do_timer() never invoked. Keep track of the fact that it

	 * was the one which had the do_timer() duty last.

 Skip reprogram of event if its not changed */

 Sanity check: make sure clockevent is actually programmed */

	/*

	 * nohz_stop_sched_tick can be called several times before

	 * the nohz_restart_sched_tick is called. This happens when

	 * interrupts arrive which do not cause a reschedule. In the

	 * first call we save the current tick time, so we can restart

	 * the scheduler tick in nohz_restart_sched_tick.

	/*

	 * If the expiration time == KTIME_MAX, then we simply stop

	 * the tick timer.

 CONFIG_NO_HZ_FULL */

 Update jiffies first */

	/*

	 * Clear the timer idle flag, so we avoid IPIs on remote queueing and

	 * the clock forward checks in the enqueue path:

	/*

	 * Cancel the scheduled timer and restore the tick

	/*

	 * If this CPU is offline and it is the one which updates

	 * jiffies, then give up the assignment and let it be taken by

	 * the CPU which runs the tick timer next. If we don't drop

	 * this here the jiffies might be stale and do_timer() never

	 * invoked.

		/*

		 * Make sure the CPU doesn't get fooled by obsolete tick

		 * deadline if it comes back online later.

		/*

		 * Keep the tick alive to guarantee timekeeping progression

		 * if there are full dynticks CPUs around

 Should not happen for nohz-full */

	/*

	 * If tick_nohz_get_sleep_length() ran tick_nohz_next_event(), the

	 * tick timer expiration time is known already.

/**

 * tick_nohz_idle_stop_tick - stop the idle tick from the idle task

 *

 * When the next event is more than a tick into the future, stop the idle tick

	/*

	 * Undo the effect of get_next_timer_interrupt() called from

	 * tick_nohz_next_event().

/**

 * tick_nohz_idle_enter - prepare for entering idle on the current CPU

 *

 * Called when we start the idle loop.

/**

 * tick_nohz_irq_exit - update next tick event from interrupt exit

 *

 * When an interrupt fires while we are idle and it doesn't cause

 * a reschedule, it may still add, modify or delete a timer, enqueue

 * an RCU callback, etc...

 * So we need to re-calculate and reprogram the next tick event.

/**

 * tick_nohz_idle_got_tick - Check whether or not the tick handler has run

/**

 * tick_nohz_get_next_hrtimer - return the next expiration time for the hrtimer

 * or the tick, whatever that expires first. Note that, if the tick has been

 * stopped, it returns the next hrtimer.

 *

 * Called from power state control code with interrupts disabled

/**

 * tick_nohz_get_sleep_length - return the expected length of the current sleep

 * @delta_next: duration until the next event if the tick cannot be stopped

 *

 * Called from power state control code with interrupts disabled.

 *

 * The return value of this function and/or the value returned by it through the

 * @delta_next pointer can be negative which must be taken into account by its

 * callers.

	/*

	 * The idle entry time is expected to be a sufficient approximation of

	 * the current time at this point.

	/*

	 * If the next highres timer to expire is earlier than next_event, the

	 * idle governor needs to know that.

/**

 * tick_nohz_get_idle_calls_cpu - return the current idle calls counter value

 * for a particular CPU.

 *

 * Called from the schedutil frequency scaling governor in scheduler context.

/**

 * tick_nohz_get_idle_calls - return the current idle calls counter value

 *

 * Called from the schedutil frequency scaling governor in scheduler context.

	/*

	 * We stopped the tick in idle. Update process times would miss the

	 * time we slept as update_process_times does only a 1 tick

	 * accounting. Enforce that this is accounted to idle !

	/*

	 * We might be one off. Do not randomly account a huge number of ticks!

/**

 * tick_nohz_idle_exit - restart the idle tick from the idle task

 *

 * Restart the idle tick when the CPU is woken up from idle

 * This also exit the RCU extended quiescent state. The CPU

 * can use RCU again after this function is called.

/*

 * The nohz low res interrupt handler

 No need to reprogram if we are running tickless  */

 One update is enough */

/**

 * tick_nohz_switch_to_nohz - switch to nohz mode

	/*

	 * Recycle the hrtimer in ts, so we can share the

	 * hrtimer_forward with the highres code.

 Get the next period */

 CONFIG_NO_HZ_COMMON */

/*

 * Called from irq_enter to notify about the possible interruption of idle()

/*

 * High resolution timer specific code

/*

 * We rearm the timer until we get disabled by the idle code.

 * Called with interrupts disabled.

	/*

	 * Do not call, when we are not in irq context and have

	 * no valid regs pointer

 No need to reprogram if we are in idle or full dynticks mode */

/**

 * tick_setup_sched_timer - setup the tick emulation timer

	/*

	 * Emulate tick processing via per-CPU hrtimers:

 Get the next period (per-CPU) */

 Offset the tick to avert jiffies_lock contention. */

 HIGH_RES_TIMERS */

/**

 * Async notification about clocksource changes

/*

 * Async notification about clock event changes

/**

 * Check, if a change happened, which makes oneshot possible.

 *

 * Called cyclic from the hrtimer softirq (driven by the timer

 * softirq) allow_nohz signals, that we can switch into low-res nohz

 * mode, because high resolution timers are disabled (either compile

 * or runtime). Called with interrupts disabled.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Unit test for the clocksource watchdog.

 *

 * Copyright (C) 2021 Facebook, Inc.

 *

 * Author: Paul E. McKenney <paulmck@kernel.org>

 for spin_unlock_irq() using preempt_count() m68k */

 Watchdog kthread's task_struct pointer for debug purposes. */

 lowest valid rating*/

 details above */

 Reset the clocksource if needed. */

 Run the specified series of watchdog tests. */

	/*

	 * Verify that jiffies-like clocksources get the manually

	 * specified uncertainty margin.

	/*

	 * Verify that tsc-like clocksources are assigned a reasonable

	 * uncertainty margin.

 Verify tsc-like stability with various numbers of errors injected. */

 Verify tsc-like stability with clock-value-fuzz error injection. */

 Cleanup function. */

 Create watchdog-test task. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * 2002-10-15  Posix Clocks & timers

 *                           by George Anzinger george@mvista.com

 *			     Copyright (C) 2002 2003 by MontaVista Software.

 *

 * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.

 *			     Copyright (C) 2004 Boris Hu

 *

 * These are all the functions necessary to implement POSIX clocks & timers

/*

 * Management arrays for POSIX timers. Timers are now kept in static hash table

 * with 512 entries.

 * Timer ids are allocated by local routine, which selects proper hash head by

 * key, constructed from current->signal address and per signal struct counter.

 * This keeps timer ids unique per process, but now they can intersect between

 * processes.

/*

 * Lets keep our timers in a slab cache :-)

/*

 * we assume that the new SIGEV_THREAD_ID shares no bits with the other

 * SIGEV values.  Here we put out an error if this assumption fails.

/*

 * The timer ID is turned into a timer address by idr_find().

 * Verifying a valid ID consists of:

 *

 * a) checking that idr_find() returns other than -1.

 * b) checking that the timer id matches the one in the timer itself.

 * c) that the timer owner is in the callers thread group.

/*

 * CLOCKs: The POSIX standard calls for a couple of clocks and allows us

 *	    to implement others.  This structure defines the various

 *	    clocks.

 *

 * RESOLUTION: Clock resolution is used to round up timer and interval

 *	    times, NOT to report clock times, which are reported with as

 *	    much resolution as the system can muster.  In some cases this

 *	    resolution may depend on the underlying clock hardware and

 *	    may not be quantifiable until run time, and only then is the

 *	    necessary code is written.	The standard says we should say

 *	    something about this issue in the documentation...

 *

 * FUNCTIONS: The CLOCKs structure defines possible functions to

 *	    handle various clock functions.

 *

 *	    The standard POSIX timer management code assumes the

 *	    following: 1.) The k_itimer struct (sched.h) is used for

 *	    the timer.  2.) The list, it_lock, it_clock, it_id and

 *	    it_pid fields are not modified by timer code.

 *

 * Permissions: It is assumed that the clock_settime() function defined

 *	    for each clock will take care of permission checks.	 Some

 *	    clocks may be set able by any user (i.e. local process

 *	    clocks) others not.	 Currently the only set able clock we

 *	    have is CLOCK_REALTIME and its high res counter part, both of

 *	    which we beg off on and pass to do_sys_settimeofday().

 Loop over all possible ids completed */

 Get clock_realtime */

 Set clock_realtime */

/*

 * Get monotonic time for posix timers

/*

 * Get monotonic-raw time for posix timers

/*

 * Initialize everything, well, just everything in Posix clocks/timers ;)

/*

 * The siginfo si_overrun field and the return value of timer_getoverrun(2)

 * are of type int. Clamp the overrun value to INT_MAX

/*

 * This function is exported for use by the signal deliver code.  It is

 * called just prior to the info block being released and passes that

 * block to us.  It's function is to update the overrun entry AND to

 * restart the timer.  It should only be called if the timer is to be

 * restarted (i.e. we have flagged this in the sys_private entry of the

 * info block).

 *

 * To protect against the timer going away while the interrupt is queued,

 * we require that the it_requeue_pending flag be set.

	/*

	 * FIXME: if ->sigq is queued we can race with

	 * dequeue_signal()->posixtimer_rearm().

	 *

	 * If dequeue_signal() sees the "right" value of

	 * si_sys_private it calls posixtimer_rearm().

	 * We re-queue ->sigq and drop ->it_lock().

	 * posixtimer_rearm() locks the timer

	 * and re-schedules it while ->sigq is pending.

	 * Not really bad, but not that we want.

 If we failed to send the signal the timer stops. */

/*

 * This function gets called when a POSIX.1b interval timer expires.  It

 * is used as a callback from the kernel internal timer.  The

 * run_timer_list code ALWAYS calls with interrupts on.



 * This code is for CLOCK_REALTIME* and CLOCK_MONOTONIC* timers.

		/*

		 * signal was not sent because of sig_ignor

		 * we will not get a call back to restart it AND

		 * it should be restarted.

			/*

			 * FIXME: What we really want, is to stop this

			 * timer completely and restart it in case the

			 * SIG_IGN is removed. This is a non trivial

			 * change which involves sighand locking

			 * (sigh !), which we don't want to do late in

			 * the release cycle.

			 *

			 * For now we just let timers with an interval

			 * less than a jiffie expire every jiffie to

			 * avoid softirq starvation in case of SIG_IGN

			 * and a very small interval, which would put

			 * the timer right back on the softirq pending

			 * list. By moving now ahead of time we trick

			 * hrtimer_forward() to expire the timer

			 * later, while we still maintain the overrun

			 * accuracy, but have some inconsistency in

			 * the timer_gettime() case. This is at least

			 * better than a starved softirq. A more

			 * complex fix which solves also another related

			 * inconsistency is already in the pipeline.

 Create a POSIX.1b interval timer. */

	/*

	 * In the case of the timer belonging to another task, after

	 * the task is unlocked, the timer is owned by the other task

	 * and may cease to exist at any time.  Don't use or modify

	 * new_timer after the unlock call.

/*

 * Locking issues: We need to protect the result of the id look up until

 * we get the timer locked down so it is not deleted under us.  The

 * removal is done under the idr spinlock so we use that here to bridge

 * the find to the timer lock.  To avoid a dead lock, the timer id MUST

 * be release with out holding the timer lock.

	/*

	 * timer_t could be any type >= int and we want to make sure any

	 * @timer_id outside positive int range fails lookup.

/*

 * Get the time remaining on a POSIX.1b interval timer.  This function

 * is ALWAYS called with spin_lock_irq on the timer, thus it must not

 * mess with irq.

 *

 * We have a couple of messes to clean up here.  First there is the case

 * of a timer that has a requeue pending.  These timers should appear to

 * be in the timer list with an expiry as if we were to requeue them

 * now.

 *

 * The second issue is the SIGEV_NONE timer which may be active but is

 * not really ever put in the timer list (to save system resources).

 * This timer may be expired, and if so, we will do it here.  Otherwise

 * it is the same as a requeue pending timer WRT to what we should

 * report.

 interval timer ? */

		/*

		 * SIGEV_NONE oneshot timers are never queued. Check them

		 * below.

	/*

	 * When a requeue is pending or this is a SIGEV_NONE timer move the

	 * expiry time forward by intervals, so expiry is > now.

 Return 0 only, when the timer is expired and not pending */

		/*

		 * A single shot SIGEV_NONE timer must return 0, when

		 * it is expired !

 Get the time remaining on a POSIX.1b interval timer. */

 Get the time remaining on a POSIX.1b interval timer. */

/*

 * Get the number of overruns of a POSIX.1b interval timer.  This is to

 * be the overrun of the timer last delivered.  At the same time we are

 * accumulating overruns on the next timer.  The overrun is frozen when

 * the signal is delivered, either at the notify time (if the info block

 * is not queued) or at the actual delivery time (as we are informed by

 * the call back to posixtimer_rearm().  So all we need to do is

 * to pick up the frozen overrun.

	/*

	 * Posix magic: Relative CLOCK_REALTIME timers are not affected by

	 * clock modifications, so they become CLOCK_MONOTONIC based under the

	 * hood. See hrtimer_init(). Update timr->kclock, so the generic

	 * functions which use timr->kclock->clock_get_*() work.

	 *

	 * Note: it_clock stays unmodified, because the next timer_set() might

	 * use ABSTIME, so it needs to switch back.

/*

 * On PREEMPT_RT this prevent priority inversion against softirq kthread in

 * case it gets preempted while executing a timer callback. See comments in

 * hrtimer_cancel_wait_running. For PREEMPT_RT=n this just results in a

 * cpu_relax().

 Prevent kfree(timer) after dropping the lock */

 Relock the timer. It might be not longer hashed. */

 Set a POSIX.1b interval timer. */

 Prevent rearming by clearing the interval */

	/*

	 * Careful here. On SMP systems the timer expiry function could be

	 * active and spinning on timr->it_lock.

 Switch off the timer when it_value is zero */

 We already got the old time...

 Unlocks and relocks the timer if it still exists */

 Set a POSIX.1b interval timer */

 Delete a POSIX.1b interval timer. */

 Unlocks and relocks the timer if it still exists */

	/*

	 * This keeps any tasks waiting on the spin lock from thinking

	 * they got something (see the lock code above).

/*

 * return timer owned by the process, used by exit_itimers

/*

 * This is called by do_exit or de_thread, only when there are no more

 * references to the shared signal_struct.

/*

 * nanosleep for monotonic and realtime clocks

 SPDX-License-Identifier: GPL-2.0+

/*

 * This file contains the functions which manage clocksource drivers.

 *

 * Copyright (C) 2004, 2005 IBM, John Stultz (johnstul@us.ibm.com)

 for spin_unlock_irq() using preempt_count() m68k */

/**

 * clocks_calc_mult_shift - calculate mult/shift factors for scaled math of clocks

 * @mult:	pointer to mult variable

 * @shift:	pointer to shift variable

 * @from:	frequency to convert from

 * @to:		frequency to convert to

 * @maxsec:	guaranteed runtime conversion range in seconds

 *

 * The function evaluates the shift/mult pair for the scaled math

 * operations of clocksources and clockevents.

 *

 * @to and @from are frequency values in HZ. For clock sources @to is

 * NSEC_PER_SEC == 1GHz and @from is the counter frequency. For clock

 * event @to is the counter frequency and @from is NSEC_PER_SEC.

 *

 * The @maxsec conversion range argument controls the time frame in

 * seconds which must be covered by the runtime conversion with the

 * calculated mult and shift factors. This guarantees that no 64bit

 * overflow happens when the input value of the conversion is

 * multiplied with the calculated mult factor. Larger ranges may

 * reduce the conversion accuracy by choosing smaller mult and shift

 * factors.

	/*

	 * Calculate the shift factor which is limiting the conversion

	 * range:

	/*

	 * Find the conversion shift/mult pair which has the best

	 * accuracy and fits the maxsec conversion range:

/*[Clocksource internal variables]---------

 * curr_clocksource:

 *	currently selected clocksource.

 * suspend_clocksource:

 *	used to calculate the suspend time.

 * clocksource_list:

 *	linked list with the registered clocksources

 * clocksource_mutex:

 *	protects manipulations to curr_clocksource and the clocksource_list

 * override_name:

 *	Name of the user-specified clocksource.

/*

 * Threshold: 0.0312s, when doubled: 0.0625s.

 * Also a default for cs->uncertainty_margin when registering clocks.

/*

 * Maximum permissible delay between two readouts of the watchdog

 * clocksource surrounding a read of the clocksource being validated.

 * This delay could be due to SMIs, NMIs, or to VCPU preemptions.  Used as

 * a lower bound for cs->uncertainty_margin values when registering clocks.

/*

 * Interval: 0.5sec.

	/*

	 * We cannot directly run clocksource_watchdog_kthread() here, because

	 * clocksource_select() calls timekeeping_notify() which uses

	 * stop_machine(). One cannot use stop_machine() from a workqueue() due

	 * lock inversions wrt CPU hotplug.

	 *

	 * Also, we only ever run this work once or twice during the lifetime

	 * of the kernel, so there is no point in creating a more permanent

	 * kthread for this.

	 *

	 * If kthread_run fails the next watchdog scan over the

	 * watchdog_list will find the unstable clock again.

	/*

	 * If the clocksource is registered clocksource_watchdog_kthread() will

	 * re-rate and re-select.

 kick clocksource_watchdog_kthread() */

/**

 * clocksource_mark_unstable - mark clocksource unstable via watchdog

 * @cs:		clocksource to be marked unstable

 *

 * This function is called by the x86 TSC code to mark clocksources as unstable;

 * it defers demotion and re-selection to a kthread.

 Check all of the CPUs. */

 If no checking desired, or no other CPU to check, leave. */

 Make sure to select at least one CPU other than the current CPU. */

 Force a sane value for the boot parameter. */

	/*

	 * Randomly select the specified number of CPUs.  If the same

	 * CPU is selected multiple times, that CPU is checked only once,

	 * and no replacement CPU is selected.  This gracefully handles

	 * situations where verify_n_cpus is greater than the number of

	 * CPUs that are currently online.

 Don't verify ourselves. */

 Clocksource already marked unstable? */

 Clock readout unreliable, so give it up. */

 Clocksource initialized ? */

 save these in case we print them */

 Check the deviation from the watchdog clocksource. */

 Mark it valid for high-res. */

			/*

			 * clocksource_done_booting() will sort it if

			 * finished_booting is not set yet.

			/*

			 * If this is not the current clocksource let

			 * the watchdog thread reselect it. Due to the

			 * change to high res this clocksource might

			 * be preferred now. If it is the current

			 * clocksource let the tick code know about

			 * that change.

	/*

	 * We only clear the watchdog_reset_pending, when we did a

	 * full cycle through all clocksources.

	/*

	 * Cycle through CPUs to check if the CPUs stay synchronized

	 * to each other.

	/*

	 * Arm timer if not already pending: could race with concurrent

	 * pair clocksource_stop_watchdog() clocksource_start_watchdog().

 cs is a clocksource to be watched. */

 cs is a watchdog. */

 save current watchdog */

 cs is a clocksource to be watched. */

 Skip current if we were requested for a fallback. */

 Pick the best watchdog. */

 If we failed to find a fallback restore the old one. */

 If we changed the watchdog we need to reset cycles. */

 Check if the watchdog timer needs to be started. */

 cs is a watched clocksource. */

 Check if the watchdog timer needs to be stopped. */

 Do any required per-CPU skew verification. */

 Check if the watchdog timer needs to be stopped. */

 CONFIG_CLOCKSOURCE_WATCHDOG */

 CONFIG_CLOCKSOURCE_WATCHDOG */

	/*

	 * Skip the clocksource which will be stopped in suspend state.

	/*

	 * The nonstop clocksource can be selected as the suspend clocksource to

	 * calculate the suspend time, so it should not supply suspend/resume

	 * interfaces to suspend the nonstop clocksource when system suspends.

 Pick the best rating. */

/**

 * clocksource_suspend_select - Select the best clocksource for suspend timing

 * @fallback:	if select a fallback clocksource

 Skip current if we were requested for a fallback. */

/**

 * clocksource_start_suspend_timing - Start measuring the suspend timing

 * @cs:			current clocksource from timekeeping

 * @start_cycles:	current cycles from timekeeping

 *

 * This function will save the start cycle values of suspend timer to calculate

 * the suspend time when resuming system.

 *

 * This function is called late in the suspend process from timekeeping_suspend(),

 * that means processes are frozen, non-boot cpus and interrupts are disabled

 * now. It is therefore possible to start the suspend timer without taking the

 * clocksource mutex.

	/*

	 * If current clocksource is the suspend timer, we should use the

	 * tkr_mono.cycle_last value as suspend_start to avoid same reading

	 * from suspend timer.

/**

 * clocksource_stop_suspend_timing - Stop measuring the suspend timing

 * @cs:		current clocksource from timekeeping

 * @cycle_now:	current cycles from timekeeping

 *

 * This function will calculate the suspend time from suspend timer.

 *

 * Returns nanoseconds since suspend started, 0 if no usable suspend clocksource.

 *

 * This function is called early in the resume process from timekeeping_resume(),

 * that means there is only one cpu, no processes are running and the interrupts

 * are disabled. It is therefore possible to stop the suspend timer without

 * taking the clocksource mutex.

	/*

	 * If current clocksource is the suspend timer, we should use the

	 * tkr_mono.cycle_last value from timekeeping as current cycle to

	 * avoid same reading from suspend timer.

	/*

	 * Disable the suspend timer to save power if current clocksource is

	 * not the suspend timer.

/**

 * clocksource_suspend - suspend the clocksource(s)

/**

 * clocksource_resume - resume the clocksource(s)

/**

 * clocksource_touch_watchdog - Update watchdog

 *

 * Update the watchdog after exception contexts such as kgdb so as not

 * to incorrectly trip the watchdog. This might fail when the kernel

 * was stopped in code which holds watchdog_lock.

/**

 * clocksource_max_adjustment- Returns max adjustment amount

 * @cs:         Pointer to clocksource

 *

	/*

	 * We won't try to correct for more than 11% adjustments (110,000 ppm),

/**

 * clocks_calc_max_nsecs - Returns maximum nanoseconds that can be converted

 * @mult:	cycle to nanosecond multiplier

 * @shift:	cycle to nanosecond divisor (power of two)

 * @maxadj:	maximum adjustment value to mult (~11%)

 * @mask:	bitmask for two's complement subtraction of non 64 bit counters

 * @max_cyc:	maximum cycle value before potential overflow (does not include

 *		any safety margin)

 *

 * NOTE: This function includes a safety margin of 50%, in other words, we

 * return half the number of nanoseconds the hardware counter can technically

 * cover. This is done so that we can potentially detect problems caused by

 * delayed timers or bad hardware, which might result in time intervals that

 * are larger than what the math used can handle without overflows.

	/*

	 * Calculate the maximum number of cycles that we can pass to the

	 * cyc2ns() function without overflowing a 64-bit result.

	/*

	 * The actual maximum number of cycles we can defer the clocksource is

	 * determined by the minimum of max_cycles and mask.

	 * Note: Here we subtract the maxadj to make sure we don't sleep for

	 * too long if there's a large negative adjustment.

 return the max_cycles value as well if requested */

 Return 50% of the actual maximum, so we can detect bad values */

/**

 * clocksource_update_max_deferment - Updates the clocksource max_idle_ns & max_cycles

 * @cs:         Pointer to clocksource to be updated

 *

	/*

	 * We pick the clocksource with the highest rating. If oneshot

	 * mode is active, we pick the highres valid clocksource with

	 * the best rating.

 Find the best suitable clocksource */

 Check for the override clocksource. */

		/*

		 * Check to make sure we don't switch to a non-highres

		 * capable clocksource if the tick code is in oneshot

		 * mode (highres or nohz)

 Override clocksource cannot be used. */

				/*

				 * The override cannot be currently verified.

				 * Deferring to let the watchdog check.

 Override clocksource can be used. */

/**

 * clocksource_select - Select the best clocksource available

 *

 * Private function. Must hold clocksource_mutex when called.

 *

 * Select the clocksource with the best rating, or the clocksource,

 * which is selected by userspace override.

/*

 * clocksource_done_booting - Called near the end of core bootup

 *

 * Hack to avoid lots of clocksource churn at boot time.

 * We use fs_initcall because we want this to start before

 * device_initcall but after subsys_initcall.

	/*

	 * Run the watchdog first to eliminate unstable clock sources

/*

 * Enqueue the clocksource sorted by rating

 Keep track of the place, where to insert */

/**

 * __clocksource_update_freq_scale - Used update clocksource with new freq

 * @cs:		clocksource to be registered

 * @scale:	Scale factor multiplied against freq to get clocksource hz

 * @freq:	clocksource frequency (cycles per second) divided by scale

 *

 * This should only be called from the clocksource->enable() method.

 *

 * This *SHOULD NOT* be called directly! Please use the

 * __clocksource_update_freq_hz() or __clocksource_update_freq_khz() helper

 * functions.

	/*

	 * Default clocksources are *special* and self-define their mult/shift.

	 * But, you're not special, so you should specify a freq value.

		/*

		 * Calc the maximum number of seconds which we can run before

		 * wrapping around. For clocksources which have a mask > 32-bit

		 * we need to limit the max sleep time to have a good

		 * conversion precision. 10 minutes is still a reasonable

		 * amount. That results in a shift value of 24 for a

		 * clocksource with mask >= 40-bit and f >= 4GHz. That maps to

		 * ~ 0.06ppm granularity for NTP.

	/*

	 * If the uncertainty margin is not specified, calculate it.

	 * If both scale and freq are non-zero, calculate the clock

	 * period, but bound below at 2*WATCHDOG_MAX_SKEW.  However,

	 * if either of scale or freq is zero, be very conservative and

	 * take the tens-of-milliseconds WATCHDOG_THRESHOLD value for the

	 * uncertainty margin.  Allow stupidly small uncertainty margins

	 * to be specified by the caller for testing purposes, but warn

	 * to discourage production use of this capability.

	/*

	 * Ensure clocksources that have large 'mult' values don't overflow

	 * when adjusted.

	/*

	 * Only warn for *special* clocksources that self-define

	 * their mult/shift values and don't specify a freq.

/**

 * __clocksource_register_scale - Used to install new clocksources

 * @cs:		clocksource to be registered

 * @scale:	Scale factor multiplied against freq to get clocksource hz

 * @freq:	clocksource frequency (cycles per second) divided by scale

 *

 * Returns -EBUSY if registration fails, zero otherwise.

 *

 * This *SHOULD NOT* be called directly! Please use the

 * clocksource_register_hz() or clocksource_register_khz helper functions.

 Initialize mult/shift and max_idle_ns */

 Add clocksource to the clocksource list */

/**

 * clocksource_change_rating - Change the rating of a registered clocksource

 * @cs:		clocksource to be changed

 * @rating:	new rating

/*

 * Unbind clocksource @cs. Called with clocksource_mutex held

 Select and try to install a replacement watchdog. */

 Select and try to install a replacement clock source */

		/*

		 * Select and try to install a replacement suspend clocksource.

		 * If no replacement suspend clocksource, we will just let the

		 * clocksource go and have no suspend clocksource.

/**

 * clocksource_unregister - remove a registered clocksource

 * @cs:	clocksource to be unregistered

/**

 * current_clocksource_show - sysfs interface for current clocksource

 * @dev:	unused

 * @attr:	unused

 * @buf:	char buffer to be filled with clocksource list

 *

 * Provides sysfs interface for listing current clocksource.

 strings from sysfs write are not 0 terminated! */

 strip of \n: */

/**

 * current_clocksource_store - interface for manually overriding clocksource

 * @dev:	unused

 * @attr:	unused

 * @buf:	name of override clocksource

 * @count:	length of buffer

 *

 * Takes input from sysfs interface for manually overriding the default

 * clocksource selection.

/**

 * unbind_clocksource_store - interface for manually unbinding clocksource

 * @dev:	unused

 * @attr:	unused

 * @buf:	unused

 * @count:	length of buffer

 *

 * Takes input from sysfs interface for manually unbinding a clocksource.

/**

 * available_clocksource_show - sysfs interface for listing clocksource

 * @dev:	unused

 * @attr:	unused

 * @buf:	char buffer to be filled with clocksource list

 *

 * Provides sysfs interface for listing registered clocksources

		/*

		 * Don't show non-HRES clocksource if the tick code is

		 * in one shot mode (highres=on or nohz=on)

 CONFIG_SYSFS */

/**

 * boot_override_clocksource - boot clock override

 * @str:	override name

 *

 * Takes a clocksource= boot argument and uses it

 * as the clocksource override name.

/**

 * boot_override_clock - Compatibility layer for deprecated boot option

 * @str:	override name

 *

 * DEPRECATED! Takes a clock= boot argument and uses it

 * as the clocksource override name

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic sched_clock() support, to extend low level hardware time

 * counters to full 64-bit ns values.

/**

 * struct clock_data - all data needed for sched_clock() (including

 *                     registration of a new clock source)

 *

 * @seq:		Sequence counter for protecting updates. The lowest

 *			bit is the index for @read_data.

 * @read_data:		Data required to read from sched_clock.

 * @wrap_kt:		Duration for which clock can run before wrapping.

 * @rate:		Tick rate of the registered clock.

 * @actual_read_sched_clock: Registered hardware level clock read function.

 *

 * The ordering of this structure has been chosen to optimize cache

 * performance. In particular 'seq' and 'read_data[0]' (combined) should fit

 * into a single 64-byte cache line.

	/*

	 * We don't need to use get_jiffies_64 on 32-bit arches here

	 * because we register with BITS_PER_LONG

/*

 * Updating the data required to read the clock.

 *

 * sched_clock() will never observe mis-matched data even if called from

 * an NMI. We do this by maintaining an odd/even copy of the data and

 * steering sched_clock() to one or the other using a sequence counter.

 * In order to preserve the data cache profile of sched_clock() as much

 * as possible the system reverts back to the even copy when the update

 * completes; the odd copy is used *only* during an update.

 update the backup (odd) copy with the new data */

 steer readers towards the odd copy */

 now its safe for us to update the normal (even) copy */

 switch readers back to the even copy */

/*

 * Atomically update the sched_clock() epoch.

 Cannot register a sched_clock with interrupts on */

 Calculate the mult/shift to convert counter ticks to ns. */

 Calculate how many nanosecs until we risk wrapping */

 Update epoch for new counter and update 'epoch_ns' from old counter*/

 update timeout for clock wrap */

 Calculate the ns resolution of this counter */

 Enable IRQ time accounting if we have a fast enough sched_clock() */

	/*

	 * If no sched_clock() function has been provided at that point,

	 * make it the final one.

	/*

	 * Start the timer to keep sched_clock() properly updated and

	 * sets the initial epoch.

/*

 * Clock read function for use when the clock is suspended.

 *

 * This function makes it appear to sched_clock() as if the clock

 * stopped counting at its last update.

 *

 * This function must only be called from the critical

 * section in sched_clock(). It relies on the read_seqcount_retry()

 * at the end of the critical section to be sure we observe the

 * correct copy of 'epoch_cyc'.

 SPDX-License-Identifier: GPL-2.0 */

 Division by reciprocal multiplication. */

/* Adjustment factor when a ceiling value is used.  Use as:

/* Compute the appropriate mul/adj values as well as a shift count,

   which brings the mul value into the range 2^b-1 <= x < 2^b.  Such

   a shift value will be correct in the signed integer range and off

 Automatically generated by kernel/time/timeconst.bc */\n"

 Time conversion constants for HZ == ", hz, " */\n"

 KERNEL_TIMECONST_H */\n"

 SPDX-License-Identifier: GPL-2.0+

/*

 * Support for dynamic clock devices

 *

 * Copyright (C) 2010 OMICRON electronics GmbH

/*

 * Returns NULL if the posix_clock instance attached to 'fp' is old and stale.

 SPDX-License-Identifier: GPL-2.0

/*

 * udelay() test kernel module

 *

 * Test is executed by writing and reading to /sys/kernel/debug/udelay_test

 * Tests are configured by writing: USECS ITERATIONS

 * Tests are executed by reading from the same file.

 * Specifying usecs of 0 or negative values will run multiples tests.

 *

 * Copyright (C) 2014 Google, Inc.

 Allow udelay to be up to 0.5% fast */

 SPDX-License-Identifier: GPL-2.0

/*

 * Emulate a local clock event device via a pseudo clock device.

	/*

	 * Note, we cannot cancel the timer here as we might

	 * run into the following live lock scenario:

	 *

	 * cpu 0		cpu1

	 * lock(broadcast_lock);

	 *			hrtimer_interrupt()

	 *			bc_handler()

	 *			   tick_handle_oneshot_broadcast();

	 *			    lock(broadcast_lock);

	 * hrtimer_cancel()

	 *  wait_for_callback()

/*

 * This is called from the guts of the broadcast code when the cpu

 * which is about to enter idle has the earliest broadcast timer event.

	/*

	 * This is called either from enter/exit idle code or from the

	 * broadcast handler. In all cases tick_broadcast_lock is held.

	 *

	 * hrtimer_cancel() cannot be called here neither from the

	 * broadcast handler nor from the enter/exit idle code. The idle

	 * code can run into the problem described in bc_shutdown() and the

	 * broadcast handler cannot wait for itself to complete for obvious

	 * reasons.

	 *

	 * Each caller tries to arm the hrtimer on its own CPU, but if the

	 * hrtimer callback function is currently running, then

	 * hrtimer_start() cannot move it and the timer stays on the CPU on

	 * which it is assigned at the moment.

	 *

	 * As this can be called from idle code, the hrtimer_start()

	 * invocation has to be wrapped with RCU_NONIDLE() as

	 * hrtimer_start() can call into tracing.

		/*

		 * The core tick broadcast mode expects bc->bound_on to be set

		 * correctly to prevent a CPU which has the broadcast hrtimer

		 * armed from going deep idle.

		 *

		 * As tick_broadcast_lock is held, nothing can change the cpu

		 * base which was just established in hrtimer_start() above. So

		 * the below access is safe even without holding the hrtimer

		 * base lock.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 HiSilicon Limited.

 average map latency in 100ns */

 standard deviation of map latency */

 as above */

 how many threads will do map/unmap in parallel */

 how long the test will last */

 which numa node this benchmark will run on */

 DMA addressing capability */

 DMA data direction */

 time for DMA transmission in ns */

 how many PAGE_SIZE will do map/unmap once a time */

 For future use */

		/*

		 * for a non-coherent device, if we don't stain them in the

		 * cache, this will give an underestimate of the real-world

		 * overhead of BIDIRECTIONAL or TO_DEVICE mappings;

		 * 66 means evertything goes well! 66 is lucky.

 Pretend DMA is transmitting */

 calculate sum and sum of squares */

 clear the old value in the previous benchmark */

 wait for the completion of benchmark threads */

 average latency */

 standard deviation of latency */

		/*

		 * restore the original dma_mask as many devices' dma_mask are

		 * set by architectures, acpi, busses. When we bind them back

		 * to their original drivers, those drivers shouldn't see

		 * dma_mask changed by benchmark

	/*

	 * we only permit a device bound with this driver, 2nd probe

	 * will fail

 SPDX-License-Identifier: GPL-2.0

/*

 * arch-independent dma-mapping routines

 *

 * Copyright (c) 2006  SUSE Linux Products GmbH

 * Copyright (c) 2006  Tejun Heo <teheo@suse.de>

 for max_pfn */

/*

 * Managed DMA API

/**

 * dmam_free_coherent - Managed dma_free_coherent()

 * @dev: Device to free coherent memory for

 * @size: Size of allocation

 * @vaddr: Virtual address of the memory to free

 * @dma_handle: DMA handle of the memory to free

 *

 * Managed dma_free_coherent().

/**

 * dmam_alloc_attrs - Managed dma_alloc_attrs()

 * @dev: Device to allocate non_coherent memory for

 * @size: Size of allocation

 * @dma_handle: Out argument for allocated DMA handle

 * @gfp: Allocation flags

 * @attrs: Flags in the DMA_ATTR_* namespace.

 *

 * Managed dma_alloc_attrs().  Memory allocated using this function will be

 * automatically released on driver detach.

 *

 * RETURNS:

 * Pointer to allocated memory on success, NULL on failure.

/*

 * Check if the devices uses a direct mapping for streaming DMA operations.

 * This allows IOMMU drivers to set a bypass mode if the DMA mask is large

 * enough.

/**

 * dma_map_sg_attrs - Map the given buffer for DMA

 * @dev:	The device for which to perform the DMA operation

 * @sg:		The sg_table object describing the buffer

 * @nents:	Number of entries to map

 * @dir:	DMA direction

 * @attrs:	Optional DMA attributes for the map operation

 *

 * Maps a buffer described by a scatterlist passed in the sg argument with

 * nents segments for the @dir DMA operation by the @dev device.

 *

 * Returns the number of mapped entries (which can be less than nents)

 * on success. Zero is returned for any error.

 *

 * dma_unmap_sg_attrs() should be used to unmap the buffer with the

 * original sg and original nents (not the value returned by this funciton).

/**

 * dma_map_sgtable - Map the given buffer for DMA

 * @dev:	The device for which to perform the DMA operation

 * @sgt:	The sg_table object describing the buffer

 * @dir:	DMA direction

 * @attrs:	Optional DMA attributes for the map operation

 *

 * Maps a buffer described by a scatterlist stored in the given sg_table

 * object for the @dir DMA operation by the @dev device. After success, the

 * ownership for the buffer is transferred to the DMA domain.  One has to

 * call dma_sync_sgtable_for_cpu() or dma_unmap_sgtable() to move the

 * ownership of the buffer back to the CPU domain before touching the

 * buffer by the CPU.

 *

 * Returns 0 on success or a negative error code on error. The following

 * error codes are supported with the given meaning:

 *

 *   -EINVAL	An invalid argument, unaligned access or other error

 *		in usage. Will not succeed if retried.

 *   -ENOMEM	Insufficient resources (like memory or IOVA space) to

 *		complete the mapping. Should succeed if retried later.

 *   -EIO	Legacy error code with an unknown meaning. eg. this is

 *		returned if a lower level call returned DMA_MAPPING_ERROR.

/*

 * The whole dma_get_sgtable() idea is fundamentally unsafe - it seems

 * that the intention is to allow exporting memory allocated via the

 * coherent DMA APIs through the dma_buf API, which only accepts a

 * scattertable.  This presents a couple of problems:

 * 1. Not all memory allocated via the coherent DMA APIs is backed by

 *    a struct page

 * 2. Passing coherent DMA memory into the streaming APIs is not allowed

 *    as we will try to flush the memory through a different alias to that

 *    actually being used (and the flushes are redundant.)

/*

 * Return the page attributes used for mapping dma_alloc_* memory, either in

 * kernel space if remapping is needed, or to userspace through dma_mmap_*.

 CONFIG_MMU */

/**

 * dma_can_mmap - check if a given device supports dma_mmap_*

 * @dev: device to check

 *

 * Returns %true if @dev supports dma_mmap_coherent() and dma_mmap_attrs() to

 * map DMA allocations to userspace.

/**

 * dma_mmap_attrs - map a coherent DMA allocation into user space

 * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

 * @vma: vm_area_struct describing requested user mapping

 * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs

 * @dma_addr: device-view address returned from dma_alloc_attrs

 * @size: size of memory originally requested in dma_alloc_attrs

 * @attrs: attributes of mapping properties requested in dma_alloc_attrs

 *

 * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user

 * space.  The coherent DMA buffer must not be freed by the driver until the

 * user space mapping has been released.

	/*

	 * We require every DMA ops implementation to at least support a 32-bit

	 * DMA mask (and use bounce buffering if that isn't supported in

	 * hardware).  As the direct mapping code has its own routine to

	 * actually report an optimal mask we default to 32-bit here as that

	 * is the right thing for most IOMMUs, and at least not actively

	 * harmful in general.

 let the implementation decide on the zone to allocate from: */

	/*

	 * On non-coherent platforms which implement DMA-coherent buffers via

	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting

	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to

	 * sleep on some machines, and b) an indication that the driver is

	 * probably misusing the coherent API anyway.

	/*

	 * ->dma_supported sets the bypass flag, so we must always call

	 * into the method here unless the device is truly direct mapped.

	/*

	 * Truncate the mask to the actually supported dma_addr_t width to

	 * avoid generating unsupportable addresses.

	/*

	 * Truncate the mask to the actually supported dma_addr_t width to

	 * avoid generating unsupportable addresses.

 can't merge */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Contiguous Memory Allocator for DMA mapping framework

 * Copyright (c) 2010-2011 by Samsung Electronics.

 * Written by:

 *	Marek Szyprowski <m.szyprowski@samsung.com>

 *	Michal Nazarewicz <mina86@mina86.com>

 *

 * Contiguous Memory Allocator

 *

 *   The Contiguous Memory Allocator (CMA) makes it possible to

 *   allocate big contiguous chunks of memory after the system has

 *   booted.

 *

 * Why is it needed?

 *

 *   Various devices on embedded systems have no scatter-getter and/or

 *   IO map support and require contiguous blocks of memory to

 *   operate.  They include devices such as cameras, hardware video

 *   coders, etc.

 *

 *   Such devices often require big memory buffers (a full HD frame

 *   is, for instance, more than 2 mega pixels large, i.e. more than 6

 *   MB of memory), which makes mechanisms such as kmalloc() or

 *   alloc_page() ineffective.

 *

 *   At the same time, a solution where a big memory region is

 *   reserved for a device is suboptimal since often more memory is

 *   reserved then strictly required and, moreover, the memory is

 *   inaccessible to page system even if device drivers don't use it.

 *

 *   CMA tries to solve this issue by operating on memory regions

 *   where only movable pages can be allocated from.  This way, kernel

 *   can use the memory for pagecache and when device driver requests

 *   it, allocated pages can be migrated.

/*

 * Default global CMA area size can be defined in kernel's .config.

 * This is useful mainly for distro maintainers to create a kernel

 * that works correctly for most supported systems.

 * The size can be set in bytes or as a percentage of the total memory

 * in the system.

 *

 * Users, who want to set the size of global CMA area for their system

 * should use cma= kernel parameter.

/**

 * dma_contiguous_reserve() - reserve area(s) for contiguous memory handling

 * @limit: End address of the reserved memory (optional, 0 for any).

 *

 * This function reserves memory from early allocator. It should be

 * called by arch specific code once the early allocator (memblock or bootmem)

 * has been activated and all other subsystems have already allocated/reserved

 * memory.

/**

 * dma_contiguous_reserve_area() - reserve custom contiguous area

 * @size: Size of the reserved area (in bytes),

 * @base: Base address of the reserved area optional, use 0 for any

 * @limit: End address of the reserved memory (optional, 0 for any).

 * @res_cma: Pointer to store the created cma region.

 * @fixed: hint about where to place the reserved area

 *

 * This function reserves memory from early allocator. It should be

 * called by arch specific code once the early allocator (memblock or bootmem)

 * has been activated and all other subsystems have already allocated/reserved

 * memory. This function allows to create custom reserved areas for specific

 * devices.

 *

 * If @fixed is true, reserve contiguous area at exactly @base.  If false,

 * reserve in range from @base to @limit.

 Architecture specific contiguous memory fixup. */

/**

 * dma_alloc_from_contiguous() - allocate pages from contiguous area

 * @dev:   Pointer to device for which the allocation is performed.

 * @count: Requested number of pages.

 * @align: Requested alignment of pages (in PAGE_SIZE order).

 * @no_warn: Avoid printing message about failed allocation.

 *

 * This function allocates memory buffer for specified device. It uses

 * device specific contiguous memory area if available or the default

 * global one. Requires architecture specific dev_get_cma_area() helper

 * function.

/**

 * dma_release_from_contiguous() - release allocated pages

 * @dev:   Pointer to device for which the pages were allocated.

 * @pages: Allocated pages.

 * @count: Number of allocated pages.

 *

 * This function releases memory allocated by dma_alloc_from_contiguous().

 * It returns false when provided pages do not belong to contiguous area and

 * true otherwise.

/**

 * dma_alloc_contiguous() - allocate contiguous pages

 * @dev:   Pointer to device for which the allocation is performed.

 * @size:  Requested allocation size.

 * @gfp:   Allocation flags.

 *

 * tries to use device specific contiguous memory area if available, or it

 * tries to use per-numa cma, if the allocation fails, it will fallback to

 * try default global one.

 *

 * Note that it bypass one-page size of allocations from the per-numa and

 * global area as the addresses within one page are always contiguous, so

 * there is no need to waste CMA pages for that kind; it also helps reduce

 * fragmentations.

 CMA can be used only in the context which permits sleeping */

/**

 * dma_free_contiguous() - release allocated pages

 * @dev:   Pointer to device for which the pages were allocated.

 * @page:  Pointer to the allocated pages.

 * @size:  Size of allocated pages.

 *

 * This function releases memory allocated by dma_alloc_contiguous(). As the

 * cma_release returns false when provided pages do not belong to contiguous

 * area and true otherwise, this function then does a fallback __free_pages()

 * upon a false-return.

 if dev has its own cma, free page from there */

		/*

		 * otherwise, page is from either per-numa cma or default cma

 not in any cma, free from buddy */

/*

 * Support for reserved memory regions defined in device tree

 Architecture specific contiguous memory fixup. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Coherent per-device memory handling.

 * Borrowed from i386

/*

 * Declare a region of memory to be handed out by dma_alloc_coherent() when it

 * is asked for coherent memory for this device.  This shall only be used

 * from platform code, usually based on the device tree description.

 *

 * phys_addr is the CPU physical address to which the memory is currently

 * assigned (this will be ioremapped so the CPU can access the region).

 *

 * device_addr is the DMA address the device needs to be programmed with to

 * actually address this memory (this will be handed out as the dma_addr_t in

 * dma_alloc_coherent()).

 *

 * size is the size of the area (must be a multiple of PAGE_SIZE).

 *

 * As a simplification for the platforms, only *one* such region of memory may

 * be declared per device.

	/*

	 * Memory was found in the coherent area.

/**

 * dma_alloc_from_dev_coherent() - allocate memory from device coherent pool

 * @dev:	device from which we allocate memory

 * @size:	size of requested memory area

 * @dma_handle:	This will be filled with the correct dma handle

 * @ret:	This pointer will be filled with the virtual address

 *		to allocated area.

 *

 * This function should be only called from per-arch dma_alloc_coherent()

 * to support allocation from per-device coherent memory pools.

 *

 * Returns 0 if dma_alloc_coherent should continue with allocating from

 * generic memory areas, or !0 if dma_alloc_coherent should return @ret.

/**

 * dma_release_from_dev_coherent() - free memory to device coherent memory pool

 * @dev:	device from which the memory was allocated

 * @order:	the order of pages allocated

 * @vaddr:	virtual address of allocated pages

 *

 * This checks whether the memory was allocated from the per-device

 * coherent memory pool and if so, releases that memory.

 *

 * Returns 1 if we correctly released the memory, or 0 if the caller should

 * proceed with releasing memory from generic pools.

/**

 * dma_mmap_from_dev_coherent() - mmap memory from the device coherent pool

 * @dev:	device from which the memory was allocated

 * @vma:	vm_area for the userspace memory

 * @vaddr:	cpu address returned by dma_alloc_from_dev_coherent

 * @size:	size of the memory buffer allocated

 * @ret:	result from remap_pfn_range()

 *

 * This checks whether the memory was allocated from the per-device

 * coherent memory pool and if so, maps that memory to the provided vma.

 *

 * Returns 1 if @vaddr belongs to the device coherent pool and the caller

 * should return @ret, or 0 if they should proceed with mapping memory from

 * generic areas.

 CONFIG_DMA_GLOBAL_POOL */

/*

 * Support for reserved memory regions defined in device tree

 CONFIG_DMA_GLOBAL_POOL */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2014 The Linux Foundation

/*

 * Remaps an array of PAGE_SIZE pages into another vm_area.

 * Cannot be used in non-sleeping contexts

/*

 * Remaps an allocated contiguous region into another vm_area.

 * Cannot be used in non-sleeping contexts

/*

 * Unmaps a range previously mapped by dma_common_*_remap

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Dynamic DMA mapping support.

 *

 * This implementation is a fallback for platforms that do not support

 * I/O TLBs (aka DMA address translation hardware).

 * Copyright (C) 2000 Asit Mallick <Asit.K.Mallick@intel.com>

 * Copyright (C) 2000 Goutham Rao <goutham.rao@intel.com>

 * Copyright (C) 2000, 2003 Hewlett-Packard Co

 *	David Mosberger-Tang <davidm@hpl.hp.com>

 *

 * 03/05/07 davidm	Switch from PCI-DMA to generic device DMA API.

 * 00/12/13 davidm	Rename to swiotlb.c and add mark_clean() to avoid

 *			unnecessary i-cache flushing.

 * 04/07/.. ak		Better overflow handling. Assorted fixes.

 * 05/09/10 linville	Add support for syncing ranges, support syncing for

 *			DMA_BIDIRECTIONAL mappings, miscellaneous cleanup.

 * 08/12/11 beckyb	Add highmem support

/*

 * Minimum IO TLB size to bother booting with.  Systems with mainly

 * 64bit capable cards will only lightly use the swiotlb.  If we can't

 * allocate a contiguous 1MB, we're probably in trouble anyway.

/*

 * Max segment that we can provide which (if pages are contingous) will

 * not be bounced (unless SWIOTLB_FORCE is set).

 avoid tail segment of size < IO_TLB_SEGSIZE */

	/*

	 * If swiotlb parameter has not been specified, give a chance to

	 * architectures such as those supporting memory encryption to

	 * adjust/expand SWIOTLB size for their use.

/*

 * Early SWIOTLB allocation may be too early to allow an architecture to

 * perform the desired operations.  This function allows the architecture to

 * call SWIOTLB when the operations are possible.  It needs to be called

 * before the SWIOTLB memory is used.

 protect against double initialization */

/*

 * Statically reserve bounce buffer space and initialize bounce buffer data

 * structures for the software IO TLB used to implement the DMA API.

 Get IO TLB memory from the low pages */

/*

 * Systems with larger DMA zones (those that don't support ISA) can

 * initialize the swiotlb later using the slab allocator if needed.

 * This should be just like above, but with some error catching.

	/*

	 * Get IO TLB memory from the low pages

 protect against double initialization */

/*

 * Return the offset into a iotlb slot required to keep the device happy.

/*

 * Bounce: copy the swiotlb buffer from or back to the original dma location

 The buffer does not have a mapping.  Map it in and copy */

/*

 * Carefully handle integer overflow which can occur when boundary_mask == ~0UL.

/*

 * Find a suitable number of IO TLB entries size that will fit this request and

 * allocate a buffer from that IO TLB pool.

	/*

	 * For mappings with an alignment requirement don't bother looping to

	 * unaligned slots once we found an aligned one.  For allocations of

	 * PAGE_SIZE or larger only look for page aligned allocations.

		/*

		 * If we find a slot that indicates we have 'nslots' number of

		 * contiguous buffers, we allocate the buffers from that slot

		 * and mark the entries as '0' indicating unavailable.

	/*

	 * Update the indices to avoid searching in the next round.

	/*

	 * Save away the mapping from the original address to the DMA address.

	 * This is needed when we sync the memory.  Then we sync the buffer if

	 * needed.

	/*

	 * Return the buffer to the free list by setting the corresponding

	 * entries to indicate the number of contiguous entries available.

	 * While returning the entries to the free list, we merge the entries

	 * with slots below and above the pool being returned.

	/*

	 * Step 1: return the slots to the free list, merging the slots with

	 * superceeding slots

	/*

	 * Step 2: merge the returned slots with the preceding slots, if

	 * available (non zero)

/*

 * tlb_addr is the physical address of the bounce buffer to unmap.

	/*

	 * First, sync the memory before unmapping the entry

/*

 * Create a swiotlb mapping for the buffer at @paddr, and in case of DMAing

 * to the device copy the data into it as well.

 Ensure that the address returned is DMA'ble */

	/*

	 * Since multiple devices can share the same pool, the private data,

	 * io_tlb_mem struct, will be initialized by the first device attached

	 * to it.

 CONFIG_DMA_RESTRICTED_POOL */

 SPDX-License-Identifier: GPL-2.0

/*

 * Dummy DMA ops that always fail.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018-2020 Christoph Hellwig.

 *

 * DMA operations that map physical memory directly without using an IOMMU.

 for max_pfn */

/*

 * Most architectures use ZONE_DMA for the first 16 Megabytes, but some use

 * it for entirely different regions. In that case the arch code needs to

 * override the variable below for dma-direct to work properly.

	/*

	 * Optimistically try the zone that the physical address mask falls

	 * into first.  If that returns memory that isn't actually addressable

	 * we will fallback to the next lower zone and try again.

	 *

	 * Note that GFP_DMA32 and GFP_DMA are no ops without the corresponding

	 * zones.

 remove any dirty cache lines on the kernel alias */

 return the page pointer as the opaque cookie */

	/*

	 * Remapping or decrypting memory may block. If either is required and

	 * we can't block, allocate the memory from the atomic pools.

	 * If restricted DMA (i.e., is_swiotlb_for_alloc) is required, one must

	 * set up another device coherent pool by shared-dma-pool and use

	 * dma_alloc_from_dev_coherent instead.

 we always manually zero the memory once we are done */

 remove any dirty cache lines on the kernel alias */

 create a coherent mapping */

		/*

		 * Depending on the cma= arguments and per-arch setup

		 * dma_alloc_contiguous could return highmem pages.

		 * Without remapping there is no way to return them here,

		 * so log an error and fail.

 If memory cannot be re-encrypted, it must be leaked */

 cpu_addr is a struct page cookie, not a kernel address */

 If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */

		/*

		 * Depending on the cma= arguments and per-arch setup

		 * dma_alloc_contiguous could return highmem pages.

		 * Without remapping there is no way to return them here,

		 * so log an error and fail.

 If cpu_addr is not from an atomic pool, dma_free_from_pool() fails */

	/*

	 * Because 32-bit DMA masks are so common we expect every architecture

	 * to be able to satisfy them - either by not supporting more physical

	 * memory, or by providing a ZONE_DMA32.  If neither is the case, the

	 * architecture needs to use an IOMMU instead of the direct mapping.

	/*

	 * This check needs to be against the actual bit mask value, so use

	 * phys_to_dma_unencrypted() here so that the SME encryption mask isn't

	 * part of the check.

 If SWIOTLB is active, use its maximum mapping size */

/**

 * dma_direct_set_offset - Assign scalar offset for a single DMA range.

 * @dev:	device pointer; needed to "own" the alloced memory.

 * @cpu_start:  beginning of memory region covered by this offset.

 * @dma_start:  beginning of DMA/PCI region covered by this offset.

 * @size:	size of the region.

 *

 * This is for the simple case of a uniform offset which cannot

 * be discovered by "dma-ranges".

 *

 * It returns -ENOMEM if out of memory, -EINVAL if a map

 * already exists, 0 otherwise.

 *

 * Note: any call to this from a driver is a bug.  The mapping needs

 * to be described by the device tree or other firmware interfaces.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008 Advanced Micro Devices, Inc.

 *

 * Author: Joerg Roedel <joerg.roedel@amd.com>

 If the pool runs out, add this many new entries at once */

/**

 * struct dma_debug_entry - track a dma_map* or dma_alloc_coherent mapping

 * @list: node on pre-allocated free_entries list

 * @dev: 'dev' argument to dma_map_{page|single|sg} or dma_alloc_coherent

 * @size: length of the mapping

 * @type: single, page, sg, coherent

 * @direction: enum dma_data_direction

 * @sg_call_ents: 'nents' from dma_map_sg

 * @sg_mapped_ents: 'mapped_ents' from dma_map_sg

 * @pfn: page frame of the start address

 * @offset: offset of mapping relative to pfn

 * @map_err_type: track whether dma_mapping_error() was checked

 * @stacktrace: support backtraces when a violation is detected

 Hash list to save the allocated dma addresses */

 List of pre-allocated dma_debug_entry's */

 Lock for the list above */

 Global disable flag - will be set in case of an error */

 Early initialization disable flag, set at the end of dma_debug_init */

 Global error count */

 Global error show enable*/

 Number of errors to show */

 number of preallocated entries requested by kernel cmdline */

 per-driver filter related state */

/*

 * The access to some variables in this macro is racy. We can't use atomic_t

 * here because all these variables are exported to debugfs. Some of them even

 * writeable. This is also the reason why a lock won't help much. But anyway,

 * the races are no big deal. Here is why:

 *

 *   error_count: the addition is racy, but the worst thing that can happen is

 *                that we don't count some errors

 *   show_num_errors: the subtraction is racy. Also no big deal because in

 *                    worst case this will result in one warning more in the

 *                    system log than the user configured. This variable is

 *                    writeable via debugfs.

 driver filter off */

 driver filter on and initialized */

 driver filter on, but we can't filter on a NULL device... */

 driver filter on but not yet initialized */

 lock to protect against change of current_driver_name */

/*

 * Hash related functions

 *

 * Every DMA-API request is saved into a struct dma_debug_entry. To

 * have quick access to these structs they are stored into a hash.

	/*

	 * Hash function is based on the dma address.

	 * We use bits 20-27 here as the index into the hash

/*

 * Request exclusive access to a hash bucket for a given dma_debug_entry.

/*

 * Give up exclusive access to the hash bucket

/*

 * Search a given entry in the hash bucket list

		/*

		 * Some drivers map the same physical address multiple

		 * times. Without a hardware IOMMU this results in the

		 * same device addresses being put into the dma-debug

		 * hash multiple times too. This can result in false

		 * positives being reported. Therefore we implement a

		 * best-fit algorithm here which returns the entry from

		 * the hash which fits best to the reference value

		 * instead of the first-fit.

 perfect-fit - return the result */

			/*

			 * We found an entry that fits better then the

			 * previous one or it is the 1st match.

	/*

	 * If we have multiple matches but no perfect-fit, just return

	 * NULL.

		/*

		 * Nothing found, go back a hash bucket

/*

 * Add an entry to a hash bucket

/*

 * Remove entry from a hash bucket list

/*

 * Dump mapping entries for debugging purposes

/*

 * For each mapping (initial cacheline in the case of

 * dma_alloc_coherent/dma_map_page, initial cacheline in each page of a

 * scatterlist, or the cacheline specified in dma_map_single) insert

 * into this tree using the cacheline as the key. At

 * dma_unmap_{single|sg|page} or dma_free_coherent delete the entry.  If

 * the entry already exists at insertion time add a tag as a reference

 * count for the overlapping mappings.  For now, the overlap tracking

 * just ensures that 'unmaps' balance 'maps' before marking the

 * cacheline idle, but we should also be flagging overlaps as an API

 * violation.

 *

 * Memory usage is mostly constrained by the maximum number of available

 * dma-debug entries in that we need a free dma_debug_entry before

 * inserting into the tree.  In the case of dma_map_page and

 * dma_alloc_coherent there is only one dma_debug_entry and one

 * dma_active_cacheline entry to track per event.  dma_map_sg(), on the

 * other hand, consumes a single dma_debug_entry, but inserts 'nents'

 * entries into the tree.

	/* If we overflowed the overlap counter then we're potentially

	 * leaking dma-mappings.

	/* If the device is not writing memory then we don't have any

	 * concerns about the cpu consuming stale data.  This mitigates

	 * legitimate usages of overlapping mappings.

 ...mirror the insert case */

	/* since we are counting overlaps the final put of the

	 * cacheline will occur when the overlap count is 0.

	 * active_cacheline_dec_overlap() returns -1 in that case

/*

 * Wrapper function for adding an entry to the hash.

 * This function takes care of locking itself.

 Shout each time we tick over some multiple of the initial pool */

/* struct dma_entry allocator

 *

 * The next two functions implement the allocator for

 * struct dma_debug_entries.

	/*

	 * add to beginning of the list - this way the entries are

	 * more likely cache hot when they are reallocated.

/*

 * DMA-API debugging init code

 *

 * The init code does two things:

 *   1. Initialize core data structures

 *   2. Preallocate a given number of dma_debug_entry structs

	/*

	 * We can't copy to userspace directly because current_driver_name can

	 * only be read under the driver_name_lock with irqs disabled. So

	 * create a temporary copy first.

	/*

	 * We can't copy from userspace directly. Access to

	 * current_driver_name is protected with a write_lock with irqs

	 * disabled. Since copy_from_user can fault and may sleep we

	 * need to copy to temporary buffer first

	/*

	 * Now handle the string we got from userspace very carefully.

	 * The rules are:

	 *         - only use the first token we got

	 *         - token delimiter is everything looking like a space

	 *           character (' ', '\n', '\t' ...)

	 *

		/*

		 * If the first character userspace gave us is not

		 * alphanumerical then assume the filter should be

		 * switched off.

	/*

	 * Now parse out the first token and use it as the name for the

	 * driver to filter for.

	/* Do not use dma_debug_initialized here, since we really want to be

	 * called to set dma_debug_initialized

 must drop lock before calling dma_mapping_error */

	/*

	 * This may be no bug in reality - but most implementations of the

	 * DMA API don't handle this properly, so check for it here

	/*

	 * Drivers should use dma_mapping_error() to check the returned

	 * addresses of dma_map_single() and dma_map_page().

	 * If not, print this warning message. See Documentation/core-api/dma-api.rst.

 Stack is direct-mapped. */

 Stack is vmalloced. */

	/*

	 * Either the driver forgot to set dma_parms appropriately, or

	 * whoever generated the list forgot to check them.

	/*

	 * In some cases this could potentially be the DMA API

	 * implementation's fault, but it would usually imply that

	 * the scatterlist was built inappropriately to begin with.

		/*

		 * The same physical address can be mapped multiple

		 * times. Without a hardware IOMMU this results in the

		 * same device addresses being put into the dma-debug

		 * hash multiple times too. This can result in false

		 * positives being reported. Therefore we implement a

		 * best-fit algorithm here which updates the first entry

		 * from the hash which fits the reference value and is

		 * not currently listed as being checked.

 handle vmalloc and linear addresses */

 handle vmalloc and linear addresses */

 SPDX-License-Identifier: GPL-2.0

/*

 * Helpers for DMA ops implementations.  These generally rely on the fact that

 * the allocated memory contains normal pages in the direct kernel mapping.

/*

 * Create scatter-list for the already allocated DMA buffer.

/*

 * Create userspace mapping for the DMA-coherent memory.

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012 ARM Ltd.

 * Copyright (C) 2020 Google LLC

 Size can be defined by the coherent_pool command line */

 Dynamic background expansion when the atomic pool is near capacity */

 CMA can't cross zone boundaries, see cma_activate_area() */

 Cannot allocate larger than MAX_ORDER-1 */

	/*

	 * Memory in the atomic DMA pools must be unencrypted, the pools do not

	 * shrink so no re-encryption occurs in dma_direct_free().

 Decrypt succeeded but encrypt failed, purposely leak */

	/*

	 * If coherent_pool was not used on the command line, default the pool

	 * sizes to 128KB per 1GB of memory, min 128KB, max MAX_ORDER-1.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * This is invoked from the outer guest loop with interrupts and

	 * preemption enabled.

	 *

	 * KVM invokes xfer_to_guest_mode_work_pending() with interrupts

	 * disabled in the inner loop before going into guest mode. No need

	 * to disable interrupts here.

 SPDX-License-Identifier: GPL-2.0

 See comment for enter_from_user_mode() in entry-common.h */

	/*

	 * Handle Syscall User Dispatch.  This must comes first, since

	 * the ABI here can be something that doesn't make sense for

	 * other syscall_work features.

 Handle ptrace */

 Do seccomp after ptrace, to catch any tracer changes. */

 Either of the above might have changed the syscall number */

 See comment for exit_to_user_mode() in entry-common.h */

 Workaround to allow gradual conversion of architecture code */

	/*

	 * Before returning to user space ensure that all pending work

	 * items have been completed.

 Architecture specific TIF work */

		/*

		 * Disable interrupts and reevaluate the work flags as they

		 * might have changed while interrupts and preemption was

		 * enabled above.

 Check if any of the above work has queued a deferred wakeup */

 Return the latest work state for arch_exit_to_user_mode() */

 Flush pending rcuog wakeup before the last need_resched() check */

 Ensure that the address limit is intact and no locks are held */

/*

 * If SYSCALL_EMU is set, then the only reason to report is when

 * SINGLESTEP is set (i.e. PTRACE_SYSEMU_SINGLESTEP).  This syscall

 * instruction has been already reported in syscall_enter_from_user_mode().

	/*

	 * If the syscall was rolled back due to syscall user dispatching,

	 * then the tracers below are not invoked for the same reason as

	 * the entry side was not invoked in syscall_trace_enter(): The ABI

	 * of these syscalls is unknown.

/*

 * Syscall specific exit to user mode preparation. Runs with interrupts

 * enabled.

	/*

	 * Do one-time syscall specific work. If these work items are

	 * enabled, we want to run them exactly once per syscall exit with

	 * interrupts enabled.

	/*

	 * If this entry hit the idle task invoke rcu_irq_enter() whether

	 * RCU is watching or not.

	 *

	 * Interrupts can nest when the first interrupt invokes softirq

	 * processing on return which enables interrupts.

	 *

	 * Scheduler ticks in the idle task can mark quiescent state and

	 * terminate a grace period, if and only if the timer interrupt is

	 * not nested into another interrupt.

	 *

	 * Checking for rcu_is_watching() here would prevent the nesting

	 * interrupt to invoke rcu_irq_enter(). If that nested interrupt is

	 * the tick then rcu_flavor_sched_clock_irq() would wrongfully

	 * assume that it is the first interrupt and eventually claim

	 * quiescent state and end grace periods prematurely.

	 *

	 * Unconditionally invoke rcu_irq_enter() so RCU state stays

	 * consistent.

	 *

	 * TINY_RCU does not support EQS, so let the compiler eliminate

	 * this part when enabled.

		/*

		 * If RCU is not watching then the same careful

		 * sequence vs. lockdep and tracing is required

		 * as in irqentry_enter_from_user_mode().

	/*

	 * If RCU is watching then RCU only wants to check whether it needs

	 * to restart the tick in NOHZ mode. rcu_irq_enter_check_tick()

	 * already contains a warning when RCU is not watching, so no point

	 * in having another one here.

 Sanity check RCU and thread stack */

 Check whether this returns to user mode */

		/*

		 * If RCU was not watching on entry this needs to be done

		 * carefully and needs the same ordering of lockdep/tracing

		 * and RCU as the return to user mode path.

 Tell the tracer that IRET will enable interrupts */

 Covers both tracing and lockdep */

		/*

		 * IRQ flags state is correct already. Just tell RCU if it

		 * was not watching on entry.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Collabora Ltd.

		/*

		 * access_ok() is performed once, at prctl time, when

		 * the selector is loaded by userspace.

		/*

		 * Validate the direct dispatcher region just for basic

		 * sanity against overflow and a 0-sized dispatcher

		 * region.  If the user is able to submit a syscall from

		 * an address, that address is obviously valid.

 SPDX-License-Identifier: GPL-2.0

/*

 * Userspace indexing of printk formats

 The base dir for module formats, typically debugfs/printk/index/ */

 vmlinux, comes from linker symbols */

	/*

	 * Make show() print the header line. Do not update *pos because

	 * pi_next() still has to return the entry at index 0 later.

/*

 * We need both ESCAPE_ANY and explicit characters from ESCAPE_SPECIAL in @only

 * because otherwise ESCAPE_NAP will cause double quotes and backslashes to be

 * ignored for quoting.

		/*

		 * LOGLEVEL_DEFAULT here means "use the same level as the

		 * message we're continuing from", not the default message

		 * loglevel, so don't display it as such.

 we don't care about other module states */

 debugfs comes up on core and must be initialised first */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * printk_safe.c - Safe printk for printk-deadlock-prone contexts

 Can be preempted by NMI. */

 Can be preempted by NMI. */

 Allow to pass printk() to kdb but avoid a recursion. */

	/*

	 * Use the main logbuf even in NMI. But avoid calling console

	 * drivers that might have their own locks.

 No obstacles. */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/**

 * DOC: printk_ringbuffer overview

 *

 * Data Structure

 * --------------

 * The printk_ringbuffer is made up of 3 internal ringbuffers:

 *

 *   desc_ring

 *     A ring of descriptors and their meta data (such as sequence number,

 *     timestamp, loglevel, etc.) as well as internal state information about

 *     the record and logical positions specifying where in the other

 *     ringbuffer the text strings are located.

 *

 *   text_data_ring

 *     A ring of data blocks. A data block consists of an unsigned long

 *     integer (ID) that maps to a desc_ring index followed by the text

 *     string of the record.

 *

 * The internal state information of a descriptor is the key element to allow

 * readers and writers to locklessly synchronize access to the data.

 *

 * Implementation

 * --------------

 *

 * Descriptor Ring

 * ~~~~~~~~~~~~~~~

 * The descriptor ring is an array of descriptors. A descriptor contains

 * essential meta data to track the data of a printk record using

 * blk_lpos structs pointing to associated text data blocks (see

 * "Data Rings" below). Each descriptor is assigned an ID that maps

 * directly to index values of the descriptor array and has a state. The ID

 * and the state are bitwise combined into a single descriptor field named

 * @state_var, allowing ID and state to be synchronously and atomically

 * updated.

 *

 * Descriptors have four states:

 *

 *   reserved

 *     A writer is modifying the record.

 *

 *   committed

 *     The record and all its data are written. A writer can reopen the

 *     descriptor (transitioning it back to reserved), but in the committed

 *     state the data is consistent.

 *

 *   finalized

 *     The record and all its data are complete and available for reading. A

 *     writer cannot reopen the descriptor.

 *

 *   reusable

 *     The record exists, but its text and/or meta data may no longer be

 *     available.

 *

 * Querying the @state_var of a record requires providing the ID of the

 * descriptor to query. This can yield a possible fifth (pseudo) state:

 *

 *   miss

 *     The descriptor being queried has an unexpected ID.

 *

 * The descriptor ring has a @tail_id that contains the ID of the oldest

 * descriptor and @head_id that contains the ID of the newest descriptor.

 *

 * When a new descriptor should be created (and the ring is full), the tail

 * descriptor is invalidated by first transitioning to the reusable state and

 * then invalidating all tail data blocks up to and including the data blocks

 * associated with the tail descriptor (for the text ring). Then

 * @tail_id is advanced, followed by advancing @head_id. And finally the

 * @state_var of the new descriptor is initialized to the new ID and reserved

 * state.

 *

 * The @tail_id can only be advanced if the new @tail_id would be in the

 * committed or reusable queried state. This makes it possible that a valid

 * sequence number of the tail is always available.

 *

 * Descriptor Finalization

 * ~~~~~~~~~~~~~~~~~~~~~~~

 * When a writer calls the commit function prb_commit(), record data is

 * fully stored and is consistent within the ringbuffer. However, a writer can

 * reopen that record, claiming exclusive access (as with prb_reserve()), and

 * modify that record. When finished, the writer must again commit the record.

 *

 * In order for a record to be made available to readers (and also become

 * recyclable for writers), it must be finalized. A finalized record cannot be

 * reopened and can never become "unfinalized". Record finalization can occur

 * in three different scenarios:

 *

 *   1) A writer can simultaneously commit and finalize its record by calling

 *      prb_final_commit() instead of prb_commit().

 *

 *   2) When a new record is reserved and the previous record has been

 *      committed via prb_commit(), that previous record is automatically

 *      finalized.

 *

 *   3) When a record is committed via prb_commit() and a newer record

 *      already exists, the record being committed is automatically finalized.

 *

 * Data Ring

 * ~~~~~~~~~

 * The text data ring is a byte array composed of data blocks. Data blocks are

 * referenced by blk_lpos structs that point to the logical position of the

 * beginning of a data block and the beginning of the next adjacent data

 * block. Logical positions are mapped directly to index values of the byte

 * array ringbuffer.

 *

 * Each data block consists of an ID followed by the writer data. The ID is

 * the identifier of a descriptor that is associated with the data block. A

 * given data block is considered valid if all of the following conditions

 * are met:

 *

 *   1) The descriptor associated with the data block is in the committed

 *      or finalized queried state.

 *

 *   2) The blk_lpos struct within the descriptor associated with the data

 *      block references back to the same data block.

 *

 *   3) The data block is within the head/tail logical position range.

 *

 * If the writer data of a data block would extend beyond the end of the

 * byte array, only the ID of the data block is stored at the logical

 * position and the full data block (ID and writer data) is stored at the

 * beginning of the byte array. The referencing blk_lpos will point to the

 * ID before the wrap and the next data block will be at the logical

 * position adjacent the full data block after the wrap.

 *

 * Data rings have a @tail_lpos that points to the beginning of the oldest

 * data block and a @head_lpos that points to the logical position of the

 * next (not yet existing) data block.

 *

 * When a new data block should be created (and the ring is full), tail data

 * blocks will first be invalidated by putting their associated descriptors

 * into the reusable state and then pushing the @tail_lpos forward beyond

 * them. Then the @head_lpos is pushed forward and is associated with a new

 * descriptor. If a data block is not valid, the @tail_lpos cannot be

 * advanced beyond it.

 *

 * Info Array

 * ~~~~~~~~~~

 * The general meta data of printk records are stored in printk_info structs,

 * stored in an array with the same number of elements as the descriptor ring.

 * Each info corresponds to the descriptor of the same index in the

 * descriptor ring. Info validity is confirmed by evaluating the corresponding

 * descriptor before and after loading the info.

 *

 * Usage

 * -----

 * Here are some simple examples demonstrating writers and readers. For the

 * examples a global ringbuffer (test_rb) is available (which is not the

 * actual ringbuffer used by printk)::

 *

 *	DEFINE_PRINTKRB(test_rb, 15, 5);

 *

 * This ringbuffer allows up to 32768 records (2 ^ 15) and has a size of

 * 1 MiB (2 ^ (15 + 5)) for text data.

 *

 * Sample writer code::

 *

 *	const char *textstr = "message text";

 *	struct prb_reserved_entry e;

 *	struct printk_record r;

 *

 *	// specify how much to allocate

 *	prb_rec_init_wr(&r, strlen(textstr) + 1);

 *

 *	if (prb_reserve(&e, &test_rb, &r)) {

 *		snprintf(r.text_buf, r.text_buf_size, "%s", textstr);

 *

 *		r.info->text_len = strlen(textstr);

 *		r.info->ts_nsec = local_clock();

 *		r.info->caller_id = printk_caller_id();

 *

 *		// commit and finalize the record

 *		prb_final_commit(&e);

 *	}

 *

 * Note that additional writer functions are available to extend a record

 * after it has been committed but not yet finalized. This can be done as

 * long as no new records have been reserved and the caller is the same.

 *

 * Sample writer code (record extending)::

 *

 *		// alternate rest of previous example

 *

 *		r.info->text_len = strlen(textstr);

 *		r.info->ts_nsec = local_clock();

 *		r.info->caller_id = printk_caller_id();

 *

 *		// commit the record (but do not finalize yet)

 *		prb_commit(&e);

 *	}

 *

 *	...

 *

 *	// specify additional 5 bytes text space to extend

 *	prb_rec_init_wr(&r, 5);

 *

 *	// try to extend, but only if it does not exceed 32 bytes

 *	if (prb_reserve_in_last(&e, &test_rb, &r, printk_caller_id()), 32) {

 *		snprintf(&r.text_buf[r.info->text_len],

 *			 r.text_buf_size - r.info->text_len, "hello");

 *

 *		r.info->text_len += 5;

 *

 *		// commit and finalize the record

 *		prb_final_commit(&e);

 *	}

 *

 * Sample reader code::

 *

 *	struct printk_info info;

 *	struct printk_record r;

 *	char text_buf[32];

 *	u64 seq;

 *

 *	prb_rec_init_rd(&r, &info, &text_buf[0], sizeof(text_buf));

 *

 *	prb_for_each_record(0, &test_rb, &seq, &r) {

 *		if (info.seq != seq)

 *			pr_warn("lost %llu records\n", info.seq - seq);

 *

 *		if (info.text_len > r.text_buf_size) {

 *			pr_warn("record %llu text truncated\n", info.seq);

 *			text_buf[r.text_buf_size - 1] = 0;

 *		}

 *

 *		pr_info("%llu: %llu: %s\n", info.seq, info.ts_nsec,

 *			&text_buf[0]);

 *	}

 *

 * Note that additional less convenient reader functions are available to

 * allow complex record access.

 *

 * ABA Issues

 * ~~~~~~~~~~

 * To help avoid ABA issues, descriptors are referenced by IDs (array index

 * values combined with tagged bits counting array wraps) and data blocks are

 * referenced by logical positions (array index values combined with tagged

 * bits counting array wraps). However, on 32-bit systems the number of

 * tagged bits is relatively small such that an ABA incident is (at least

 * theoretically) possible. For example, if 4 million maximally sized (1KiB)

 * printk messages were to occur in NMI context on a 32-bit system, the

 * interrupted context would not be able to recognize that the 32-bit integer

 * completely wrapped and thus represents a different data block than the one

 * the interrupted context expects.

 *

 * To help combat this possibility, additional state checking is performed

 * (such as using cmpxchg() even though set() would suffice). These extra

 * checks are commented as such and will hopefully catch any ABA issue that

 * a 32-bit system might experience.

 *

 * Memory Barriers

 * ~~~~~~~~~~~~~~~

 * Multiple memory barriers are used. To simplify proving correctness and

 * generating litmus tests, lines of code related to memory barriers

 * (loads, stores, and the associated memory barriers) are labeled::

 *

 *	LMM(function:letter)

 *

 * Comments reference the labels using only the "function:letter" part.

 *

 * The memory barrier pairs and their ordering are:

 *

 *   desc_reserve:D / desc_reserve:B

 *     push descriptor tail (id), then push descriptor head (id)

 *

 *   desc_reserve:D / data_push_tail:B

 *     push data tail (lpos), then set new descriptor reserved (state)

 *

 *   desc_reserve:D / desc_push_tail:C

 *     push descriptor tail (id), then set new descriptor reserved (state)

 *

 *   desc_reserve:D / prb_first_seq:C

 *     push descriptor tail (id), then set new descriptor reserved (state)

 *

 *   desc_reserve:F / desc_read:D

 *     set new descriptor id and reserved (state), then allow writer changes

 *

 *   data_alloc:A (or data_realloc:A) / desc_read:D

 *     set old descriptor reusable (state), then modify new data block area

 *

 *   data_alloc:A (or data_realloc:A) / data_push_tail:B

 *     push data tail (lpos), then modify new data block area

 *

 *   _prb_commit:B / desc_read:B

 *     store writer changes, then set new descriptor committed (state)

 *

 *   desc_reopen_last:A / _prb_commit:B

 *     set descriptor reserved (state), then read descriptor data

 *

 *   _prb_commit:B / desc_reserve:D

 *     set new descriptor committed (state), then check descriptor head (id)

 *

 *   data_push_tail:D / data_push_tail:A

 *     set descriptor reusable (state), then push data tail (lpos)

 *

 *   desc_push_tail:B / desc_reserve:D

 *     set descriptor reusable (state), then push descriptor tail (id)

 Determine the data array index from a logical position. */

 Determine the desc array index from an ID or sequence number. */

 Determine how many times the data array has wrapped. */

 Determine if a logical position refers to a data-less block. */

 Get the logical position at index 0 of the current wrap. */

 Get the ID for the same index of the previous wrap as the given ID. */

/*

 * A data block: mapped directly to the beginning of the data block area

 * specified as a logical position within the data ring.

 *

 * @id:   the ID of the associated descriptor

 * @data: the writer data

 *

 * Note that the size of a data block is only known by its associated

 * descriptor.

/*

 * Return the descriptor associated with @n. @n can be either a

 * descriptor ID or a sequence number.

/*

 * Return the printk_info associated with @n. @n can be either a

 * descriptor ID or a sequence number.

/*

 * Increase the data size to account for data block meta data plus any

 * padding so that the adjacent data block is aligned on the ID size.

/*

 * Sanity checker for reserve size. The ringbuffer code assumes that a data

 * block does not exceed the maximum possible size that could fit within the

 * ringbuffer. This function provides that basic size check so that the

 * assumption is safe.

	/*

	 * Ensure the alignment padded size could possibly fit in the data

	 * array. The largest possible data block must still leave room for

	 * at least the ID of the next block.

 Query the state of a descriptor. */

/*

 * Get a copy of a specified descriptor and return its queried state. If the

 * descriptor is in an inconsistent state (miss or reserved), the caller can

 * only expect the descriptor's @state_var field to be valid.

 *

 * The sequence number and caller_id can be optionally retrieved. Like all

 * non-state_var data, they are only valid if the descriptor is in a

 * consistent state.

 Check the descriptor state. */

 LMM(desc_read:A) */

		/*

		 * The descriptor is in an inconsistent state. Set at least

		 * @state_var so that the caller can see the details of

		 * the inconsistent state.

	/*

	 * Guarantee the state is loaded before copying the descriptor

	 * content. This avoids copying obsolete descriptor content that might

	 * not apply to the descriptor state. This pairs with _prb_commit:B.

	 *

	 * Memory barrier involvement:

	 *

	 * If desc_read:A reads from _prb_commit:B, then desc_read:C reads

	 * from _prb_commit:A.

	 *

	 * Relies on:

	 *

	 * WMB from _prb_commit:A to _prb_commit:B

	 *    matching

	 * RMB from desc_read:A to desc_read:C

 LMM(desc_read:B) */

	/*

	 * Copy the descriptor data. The data is not valid until the

	 * state has been re-checked. A memcpy() for all of @desc

	 * cannot be used because of the atomic_t @state_var field.

 LMM(desc_read:C) */

 also part of desc_read:C */

 also part of desc_read:C */

	/*

	 * 1. Guarantee the descriptor content is loaded before re-checking

	 *    the state. This avoids reading an obsolete descriptor state

	 *    that may not apply to the copied content. This pairs with

	 *    desc_reserve:F.

	 *

	 *    Memory barrier involvement:

	 *

	 *    If desc_read:C reads from desc_reserve:G, then desc_read:E

	 *    reads from desc_reserve:F.

	 *

	 *    Relies on:

	 *

	 *    WMB from desc_reserve:F to desc_reserve:G

	 *       matching

	 *    RMB from desc_read:C to desc_read:E

	 *

	 * 2. Guarantee the record data is loaded before re-checking the

	 *    state. This avoids reading an obsolete descriptor state that may

	 *    not apply to the copied data. This pairs with data_alloc:A and

	 *    data_realloc:A.

	 *

	 *    Memory barrier involvement:

	 *

	 *    If copy_data:A reads from data_alloc:B, then desc_read:E

	 *    reads from desc_make_reusable:A.

	 *

	 *    Relies on:

	 *

	 *    MB from desc_make_reusable:A to data_alloc:B

	 *       matching

	 *    RMB from desc_read:C to desc_read:E

	 *

	 *    Note: desc_make_reusable:A and data_alloc:B can be different

	 *          CPUs. However, the data_alloc:B CPU (which performs the

	 *          full memory barrier) must have previously seen

	 *          desc_make_reusable:A.

 LMM(desc_read:D) */

	/*

	 * The data has been copied. Return the current descriptor state,

	 * which may have changed since the load above.

 LMM(desc_read:E) */

/*

 * Take a specified descriptor out of the finalized state by attempting

 * the transition from finalized to reusable. Either this context or some

 * other context will have been successful.

 LMM(desc_make_reusable:A) */

/*

 * Given the text data ring, put the associated descriptor of each

 * data block from @lpos_begin until @lpos_end into the reusable state.

 *

 * If there is any problem making the associated descriptor reusable, either

 * the descriptor has not yet been finalized or another writer context has

 * already pushed the tail lpos past the problematic data block. Regardless,

 * on error the caller can re-load the tail lpos to determine the situation.

 Loop until @lpos_begin has advanced to or beyond @lpos_end. */

		/*

		 * Load the block ID from the data block. This is a data race

		 * against a writer that may have newly reserved this data

		 * area. If the loaded value matches a valid descriptor ID,

		 * the blk_lpos of that descriptor will be checked to make

		 * sure it points back to this data block. If the check fails,

		 * the data area has been recycled by another writer.

 LMM(data_make_reusable:A) */

 LMM(data_make_reusable:B) */

			/*

			 * This data block is invalid if the descriptor

			 * does not point back to it.

			/*

			 * This data block is invalid if the descriptor

			 * does not point back to it.

 Advance @lpos_begin to the next data block. */

/*

 * Advance the data ring tail to at least @lpos. This function puts

 * descriptors into the reusable state if the tail is pushed beyond

 * their associated data block.

 If @lpos is from a data-less block, there is nothing to do. */

	/*

	 * Any descriptor states that have transitioned to reusable due to the

	 * data tail being pushed to this loaded value will be visible to this

	 * CPU. This pairs with data_push_tail:D.

	 *

	 * Memory barrier involvement:

	 *

	 * If data_push_tail:A reads from data_push_tail:D, then this CPU can

	 * see desc_make_reusable:A.

	 *

	 * Relies on:

	 *

	 * MB from desc_make_reusable:A to data_push_tail:D

	 *    matches

	 * READFROM from data_push_tail:D to data_push_tail:A

	 *    thus

	 * READFROM from desc_make_reusable:A to this CPU

 LMM(data_push_tail:A) */

	/*

	 * Loop until the tail lpos is at or beyond @lpos. This condition

	 * may already be satisfied, resulting in no full memory barrier

	 * from data_push_tail:D being performed. However, since this CPU

	 * sees the new tail lpos, any descriptor states that transitioned to

	 * the reusable state must already be visible.

		/*

		 * Make all descriptors reusable that are associated with

		 * data blocks before @lpos.

			/*

			 * 1. Guarantee the block ID loaded in

			 *    data_make_reusable() is performed before

			 *    reloading the tail lpos. The failed

			 *    data_make_reusable() may be due to a newly

			 *    recycled data area causing the tail lpos to

			 *    have been previously pushed. This pairs with

			 *    data_alloc:A and data_realloc:A.

			 *

			 *    Memory barrier involvement:

			 *

			 *    If data_make_reusable:A reads from data_alloc:B,

			 *    then data_push_tail:C reads from

			 *    data_push_tail:D.

			 *

			 *    Relies on:

			 *

			 *    MB from data_push_tail:D to data_alloc:B

			 *       matching

			 *    RMB from data_make_reusable:A to

			 *    data_push_tail:C

			 *

			 *    Note: data_push_tail:D and data_alloc:B can be

			 *          different CPUs. However, the data_alloc:B

			 *          CPU (which performs the full memory

			 *          barrier) must have previously seen

			 *          data_push_tail:D.

			 *

			 * 2. Guarantee the descriptor state loaded in

			 *    data_make_reusable() is performed before

			 *    reloading the tail lpos. The failed

			 *    data_make_reusable() may be due to a newly

			 *    recycled descriptor causing the tail lpos to

			 *    have been previously pushed. This pairs with

			 *    desc_reserve:D.

			 *

			 *    Memory barrier involvement:

			 *

			 *    If data_make_reusable:B reads from

			 *    desc_reserve:F, then data_push_tail:C reads

			 *    from data_push_tail:D.

			 *

			 *    Relies on:

			 *

			 *    MB from data_push_tail:D to desc_reserve:F

			 *       matching

			 *    RMB from data_make_reusable:B to

			 *    data_push_tail:C

			 *

			 *    Note: data_push_tail:D and desc_reserve:F can

			 *          be different CPUs. However, the

			 *          desc_reserve:F CPU (which performs the

			 *          full memory barrier) must have previously

			 *          seen data_push_tail:D.

 LMM(data_push_tail:B) */

 LMM(data_push_tail:C) */

 Another CPU pushed the tail. Try again. */

		/*

		 * Guarantee any descriptor states that have transitioned to

		 * reusable are stored before pushing the tail lpos. A full

		 * memory barrier is needed since other CPUs may have made

		 * the descriptor states reusable. This pairs with

		 * data_push_tail:A.

 LMM(data_push_tail:D) */

/*

 * Advance the desc ring tail. This function advances the tail by one

 * descriptor, thus invalidating the oldest descriptor. Before advancing

 * the tail, the tail descriptor is made reusable and all data blocks up to

 * and including the descriptor's data block are invalidated (i.e. the data

 * ring tail is pushed past the data block of the descriptor being made

 * reusable).

		/*

		 * If the ID is exactly 1 wrap behind the expected, it is

		 * in the process of being reserved by another writer and

		 * must be considered reserved.

		/*

		 * The ID has changed. Another writer must have pushed the

		 * tail and recycled the descriptor already. Success is

		 * returned because the caller is only interested in the

		 * specified tail being pushed, which it was.

	/*

	 * Data blocks must be invalidated before their associated

	 * descriptor can be made available for recycling. Invalidating

	 * them later is not possible because there is no way to trust

	 * data blocks once their associated descriptor is gone.

	/*

	 * Check the next descriptor after @tail_id before pushing the tail

	 * to it because the tail must always be in a finalized or reusable

	 * state. The implementation of prb_first_seq() relies on this.

	 *

	 * A successful read implies that the next descriptor is less than or

	 * equal to @head_id so there is no risk of pushing the tail past the

	 * head.

 LMM(desc_push_tail:A) */

		/*

		 * Guarantee any descriptor states that have transitioned to

		 * reusable are stored before pushing the tail ID. This allows

		 * verifying the recycled descriptor state. A full memory

		 * barrier is needed since other CPUs may have made the

		 * descriptor states reusable. This pairs with desc_reserve:D.

 LMM(desc_push_tail:B) */

		/*

		 * Guarantee the last state load from desc_read() is before

		 * reloading @tail_id in order to see a new tail ID in the

		 * case that the descriptor has been recycled. This pairs

		 * with desc_reserve:D.

		 *

		 * Memory barrier involvement:

		 *

		 * If desc_push_tail:A reads from desc_reserve:F, then

		 * desc_push_tail:D reads from desc_push_tail:B.

		 *

		 * Relies on:

		 *

		 * MB from desc_push_tail:B to desc_reserve:F

		 *    matching

		 * RMB from desc_push_tail:A to desc_push_tail:D

		 *

		 * Note: desc_push_tail:B and desc_reserve:F can be different

		 *       CPUs. However, the desc_reserve:F CPU (which performs

		 *       the full memory barrier) must have previously seen

		 *       desc_push_tail:B.

 LMM(desc_push_tail:C) */

		/*

		 * Re-check the tail ID. The descriptor following @tail_id is

		 * not in an allowed tail state. But if the tail has since

		 * been moved by another CPU, then it does not matter.

 LMM(desc_push_tail:D) */

 Reserve a new descriptor, invalidating the oldest if necessary. */

 LMM(desc_reserve:A) */

		/*

		 * Guarantee the head ID is read before reading the tail ID.

		 * Since the tail ID is updated before the head ID, this

		 * guarantees that @id_prev_wrap is never ahead of the tail

		 * ID. This pairs with desc_reserve:D.

		 *

		 * Memory barrier involvement:

		 *

		 * If desc_reserve:A reads from desc_reserve:D, then

		 * desc_reserve:C reads from desc_push_tail:B.

		 *

		 * Relies on:

		 *

		 * MB from desc_push_tail:B to desc_reserve:D

		 *    matching

		 * RMB from desc_reserve:A to desc_reserve:C

		 *

		 * Note: desc_push_tail:B and desc_reserve:D can be different

		 *       CPUs. However, the desc_reserve:D CPU (which performs

		 *       the full memory barrier) must have previously seen

		 *       desc_push_tail:B.

 LMM(desc_reserve:B) */

 LMM(desc_reserve:C) */

			/*

			 * Make space for the new descriptor by

			 * advancing the tail.

		/*

		 * 1. Guarantee the tail ID is read before validating the

		 *    recycled descriptor state. A read memory barrier is

		 *    sufficient for this. This pairs with desc_push_tail:B.

		 *

		 *    Memory barrier involvement:

		 *

		 *    If desc_reserve:C reads from desc_push_tail:B, then

		 *    desc_reserve:E reads from desc_make_reusable:A.

		 *

		 *    Relies on:

		 *

		 *    MB from desc_make_reusable:A to desc_push_tail:B

		 *       matching

		 *    RMB from desc_reserve:C to desc_reserve:E

		 *

		 *    Note: desc_make_reusable:A and desc_push_tail:B can be

		 *          different CPUs. However, the desc_push_tail:B CPU

		 *          (which performs the full memory barrier) must have

		 *          previously seen desc_make_reusable:A.

		 *

		 * 2. Guarantee the tail ID is stored before storing the head

		 *    ID. This pairs with desc_reserve:B.

		 *

		 * 3. Guarantee any data ring tail changes are stored before

		 *    recycling the descriptor. Data ring tail changes can

		 *    happen via desc_push_tail()->data_push_tail(). A full

		 *    memory barrier is needed since another CPU may have

		 *    pushed the data ring tails. This pairs with

		 *    data_push_tail:B.

		 *

		 * 4. Guarantee a new tail ID is stored before recycling the

		 *    descriptor. A full memory barrier is needed since

		 *    another CPU may have pushed the tail ID. This pairs

		 *    with desc_push_tail:C and this also pairs with

		 *    prb_first_seq:C.

		 *

		 * 5. Guarantee the head ID is stored before trying to

		 *    finalize the previous descriptor. This pairs with

		 *    _prb_commit:B.

 LMM(desc_reserve:D) */

	/*

	 * If the descriptor has been recycled, verify the old state val.

	 * See "ABA Issues" about why this verification is performed.

 LMM(desc_reserve:E) */

	/*

	 * Assign the descriptor a new ID and set its state to reserved.

	 * See "ABA Issues" about why cmpxchg() instead of set() is used.

	 *

	 * Guarantee the new descriptor ID and state is stored before making

	 * any other changes. A write memory barrier is sufficient for this.

	 * This pairs with desc_read:D.

 LMM(desc_reserve:F) */

 Now data in @desc can be modified: LMM(desc_reserve:G) */

 Determine the end of a data block. */

 First check if the data block does not wrap. */

 Wrapping data blocks store their data at the beginning. */

/*

 * Allocate a new data block, invalidating the oldest data block(s)

 * if necessary. This function also associates the data block with

 * a specified descriptor.

 Specify a data-less block. */

 Failed to allocate, specify a data-less block. */

		/*

		 * 1. Guarantee any descriptor states that have transitioned

		 *    to reusable are stored before modifying the newly

		 *    allocated data area. A full memory barrier is needed

		 *    since other CPUs may have made the descriptor states

		 *    reusable. See data_push_tail:A about why the reusable

		 *    states are visible. This pairs with desc_read:D.

		 *

		 * 2. Guarantee any updated tail lpos is stored before

		 *    modifying the newly allocated data area. Another CPU may

		 *    be in data_make_reusable() and is reading a block ID

		 *    from this area. data_make_reusable() can handle reading

		 *    a garbage block ID value, but then it must be able to

		 *    load a new tail lpos. A full memory barrier is needed

		 *    since other CPUs may have updated the tail lpos. This

		 *    pairs with data_push_tail:B.

 LMM(data_alloc:A) */

 LMM(data_alloc:B) */

 Wrapping data blocks store their data at the beginning. */

		/*

		 * Store the ID on the wrapped block for consistency.

		 * The printk_ringbuffer does not actually use it.

/*

 * Try to resize an existing data block associated with the descriptor

 * specified by @id. If the resized data block should become wrapped, it

 * copies the old data to the new data block. If @size yields a data block

 * with the same or less size, the data block is left as is.

 *

 * Fail if this is not the last allocated data block or if there is not

 * enough space or it is not possible make enough space.

 *

 * Return a pointer to the beginning of the entire data buffer or NULL on

 * failure.

 Reallocation only works if @blk_lpos is the newest data block. */

 Keep track if @blk_lpos was a wrapping data block. */

 If the data block does not increase, there is nothing to do. */

 The memory barrier involvement is the same as data_alloc:A. */

 LMM(data_realloc:A) */

 Wrapping data blocks store their data at the beginning. */

		/*

		 * Store the ID on the wrapped block for consistency.

		 * The printk_ringbuffer does not actually use it.

			/*

			 * Since the allocated space is now in the newly

			 * created wrapping data block, copy the content

			 * from the old data block.

 Return the number of bytes used by a data block. */

 Data-less blocks take no space. */

 Data block does not wrap. */

	/*

	 * For wrapping data blocks, the trailing (wasted) space is

	 * also counted.

/*

 * Given @blk_lpos, return a pointer to the writer data from the data block

 * and calculate the size of the data part. A NULL pointer is returned if

 * @blk_lpos specifies values that could never be legal.

 *

 * This function (used by readers) performs strict validation on the lpos

 * values to possibly detect bugs in the writer code. A WARN_ON_ONCE() is

 * triggered if an internal error is detected.

 Data-less data block description. */

 Regular data block: @begin less than @next and in same wrap. */

 Wrapping data block: @begin is one wrap behind @next. */

 Illegal block description. */

 A valid data block will always be aligned to the ID size. */

 A valid data block will always have at least an ID. */

 Subtract block ID space from size to reflect data size. */

/*

 * Attempt to transition the newest descriptor from committed back to reserved

 * so that the record can be modified by a writer again. This is only possible

 * if the descriptor is not yet finalized and the provided @caller_id matches.

	/*

	 * To reduce unnecessarily reopening, first check if the descriptor

	 * state and caller ID are correct.

	/*

	 * Guarantee the reserved state is stored before reading any

	 * record data. A full memory barrier is needed because @state_var

	 * modification is followed by reading. This pairs with _prb_commit:B.

	 *

	 * Memory barrier involvement:

	 *

	 * If desc_reopen_last:A reads from _prb_commit:B, then

	 * prb_reserve_in_last:A reads from _prb_commit:A.

	 *

	 * Relies on:

	 *

	 * WMB from _prb_commit:A to _prb_commit:B

	 *    matching

	 * MB If desc_reopen_last:A to prb_reserve_in_last:A

 LMM(desc_reopen_last:A) */

/**

 * prb_reserve_in_last() - Re-reserve and extend the space in the ringbuffer

 *                         used by the newest record.

 *

 * @e:         The entry structure to setup.

 * @rb:        The ringbuffer to re-reserve and extend data in.

 * @r:         The record structure to allocate buffers for.

 * @caller_id: The caller ID of the caller (reserving writer).

 * @max_size:  Fail if the extended size would be greater than this.

 *

 * This is the public function available to writers to re-reserve and extend

 * data.

 *

 * The writer specifies the text size to extend (not the new total size) by

 * setting the @text_buf_size field of @r. To ensure proper initialization

 * of @r, prb_rec_init_wr() should be used.

 *

 * This function will fail if @caller_id does not match the caller ID of the

 * newest record. In that case the caller must reserve new data using

 * prb_reserve().

 *

 * Context: Any context. Disables local interrupts on success.

 * Return: true if text data could be extended, otherwise false.

 *

 * On success:

 *

 *   - @r->text_buf points to the beginning of the entire text buffer.

 *

 *   - @r->text_buf_size is set to the new total size of the buffer.

 *

 *   - @r->info is not touched so that @r->info->text_len could be used

 *     to append the text.

 *

 *   - prb_record_text_space() can be used on @e to query the new

 *     actually used space.

 *

 * Important: All @r->info fields will already be set with the current values

 *            for the record. I.e. @r->info->text_len will be less than

 *            @text_buf_size. Writers can use @r->info->text_len to know

 *            where concatenation begins and writers should update

 *            @r->info->text_len after concatenating.

 Transition the newest descriptor back to the reserved state. */

 Now the writer has exclusive access: LMM(prb_reserve_in_last:A) */

	/*

	 * Set the @e fields here so that prb_commit() can be used if

	 * anything fails from now on.

	/*

	 * desc_reopen_last() checked the caller_id, but there was no

	 * exclusive access at that point. The descriptor may have

	 * changed since then.

		/*

		 * Increase the buffer size to include the original size. If

		 * the meta data (@text_len) is not sane, use the full data

		 * block size.

 prb_commit() re-enabled interrupts. */

 Make it clear to the caller that the re-reserve failed. */

/*

 * Attempt to finalize a specified descriptor. If this fails, the descriptor

 * is either already final or it will finalize itself when the writer commits.

 LMM(desc_make_final:A) */

/**

 * prb_reserve() - Reserve space in the ringbuffer.

 *

 * @e:  The entry structure to setup.

 * @rb: The ringbuffer to reserve data in.

 * @r:  The record structure to allocate buffers for.

 *

 * This is the public function available to writers to reserve data.

 *

 * The writer specifies the text size to reserve by setting the

 * @text_buf_size field of @r. To ensure proper initialization of @r,

 * prb_rec_init_wr() should be used.

 *

 * Context: Any context. Disables local interrupts on success.

 * Return: true if at least text data could be allocated, otherwise false.

 *

 * On success, the fields @info and @text_buf of @r will be set by this

 * function and should be filled in by the writer before committing. Also

 * on success, prb_record_text_space() can be used on @e to query the actual

 * space used for the text data block.

 *

 * Important: @info->text_len needs to be set correctly by the writer in

 *            order for data to be readable and/or extended. Its value

 *            is initialized to 0.

	/*

	 * Descriptors in the reserved state act as blockers to all further

	 * reservations once the desc_ring has fully wrapped. Disable

	 * interrupts during the reserve/commit window in order to minimize

	 * the likelihood of this happening.

 Descriptor reservation failures are tracked. */

	/*

	 * All @info fields (except @seq) are cleared and must be filled in

	 * by the writer. Save @seq before clearing because it is used to

	 * determine the new sequence number.

	/*

	 * Set the @e fields here so that prb_commit() can be used if

	 * text data allocation fails.

	/*

	 * Initialize the sequence number if it has "never been set".

	 * Otherwise just increment it by a full wrap.

	 *

	 * @seq is considered "never been set" if it has a value of 0,

	 * _except_ for @infos[0], which was specially setup by the ringbuffer

	 * initializer and therefore is always considered as set.

	 *

	 * See the "Bootstrap" comment block in printk_ringbuffer.h for

	 * details about how the initializer bootstraps the descriptors.

	/*

	 * New data is about to be reserved. Once that happens, previous

	 * descriptors are no longer able to be extended. Finalize the

	 * previous descriptor now so that it can be made available to

	 * readers. (For seq==0 there is no previous descriptor.)

 If text data allocation fails, a data-less record is committed. */

 prb_commit() re-enabled interrupts. */

 Record full text space used by record. */

 Make it clear to the caller that the reserve failed. */

 Commit the data (possibly finalizing it) and restore interrupts. */

 Now the writer has finished all writing: LMM(_prb_commit:A) */

	/*

	 * Set the descriptor as committed. See "ABA Issues" about why

	 * cmpxchg() instead of set() is used.

	 *

	 * 1  Guarantee all record data is stored before the descriptor state

	 *    is stored as committed. A write memory barrier is sufficient

	 *    for this. This pairs with desc_read:B and desc_reopen_last:A.

	 *

	 * 2. Guarantee the descriptor state is stored as committed before

	 *    re-checking the head ID in order to possibly finalize this

	 *    descriptor. This pairs with desc_reserve:D.

	 *

	 *    Memory barrier involvement:

	 *

	 *    If prb_commit:A reads from desc_reserve:D, then

	 *    desc_make_final:A reads from _prb_commit:B.

	 *

	 *    Relies on:

	 *

	 *    MB _prb_commit:B to prb_commit:A

	 *       matching

	 *    MB desc_reserve:D to desc_make_final:A

 LMM(_prb_commit:B) */

 Restore interrupts, the reserve/commit window is finished. */

/**

 * prb_commit() - Commit (previously reserved) data to the ringbuffer.

 *

 * @e: The entry containing the reserved data information.

 *

 * This is the public function available to writers to commit data.

 *

 * Note that the data is not yet available to readers until it is finalized.

 * Finalizing happens automatically when space for the next record is

 * reserved.

 *

 * See prb_final_commit() for a version of this function that finalizes

 * immediately.

 *

 * Context: Any context. Enables local interrupts.

	/*

	 * If this descriptor is no longer the head (i.e. a new record has

	 * been allocated), extending the data for this record is no longer

	 * allowed and therefore it must be finalized.

 LMM(prb_commit:A) */

/**

 * prb_final_commit() - Commit and finalize (previously reserved) data to

 *                      the ringbuffer.

 *

 * @e: The entry containing the reserved data information.

 *

 * This is the public function available to writers to commit+finalize data.

 *

 * By finalizing, the data is made immediately available to readers.

 *

 * This function should only be used if there are no intentions of extending

 * this data using prb_reserve_in_last().

 *

 * Context: Any context. Enables local interrupts.

/*

 * Count the number of lines in provided text. All text has at least 1 line

 * (even if @text_size is 0). Each '\n' processed is counted as an additional

 * line.

/*

 * Given @blk_lpos, copy an expected @len of data into the provided buffer.

 * If @line_count is provided, count the number of lines in the data.

 *

 * This function (used by readers) performs strict validation on the data

 * size to possibly detect bugs in the writer code. A WARN_ON_ONCE() is

 * triggered if an internal error is detected.

 Caller might not want any data. */

	/*

	 * Actual cannot be less than expected. It can be more than expected

	 * because of the trailing alignment padding.

	 *

	 * Note that invalid @len values can occur because the caller loads

	 * the value during an allowed data race.

 Caller interested in the line count? */

 Caller interested in the data content? */

 LMM(copy_data:A) */

/*

 * This is an extended version of desc_read(). It gets a copy of a specified

 * descriptor. However, it also verifies that the record is finalized and has

 * the sequence number @seq. On success, 0 is returned.

 *

 * Error return values:

 * -EINVAL: A finalized record with sequence number @seq does not exist.

 * -ENOENT: A finalized record with sequence number @seq exists, but its data

 *          is not available. This is a valid record, so readers should

 *          continue with the next record.

	/*

	 * An unexpected @id (desc_miss) or @seq mismatch means the record

	 * does not exist. A descriptor in the reserved or committed state

	 * means the record does not yet exist for the reader.

	/*

	 * A descriptor in the reusable state may no longer have its data

	 * available; report it as existing but with lost data. Or the record

	 * may actually be a record with lost data.

/*

 * Copy the ringbuffer data from the record with @seq to the provided

 * @r buffer. On success, 0 is returned.

 *

 * See desc_read_finalized_seq() for error return values.

 Extract the ID, used to specify the descriptor to read. */

 Get a local copy of the correct descriptor (if available). */

	/*

	 * If @r is NULL, the caller is only interested in the availability

	 * of the record.

 If requested, copy meta data. */

 Copy text data. If it fails, this is a data-less record. */

 Ensure the record is still finalized and has the same @seq. */

 Get the sequence number of the tail descriptor. */

 LMM(prb_first_seq:A) */

 LMM(prb_first_seq:B) */

		/*

		 * This loop will not be infinite because the tail is

		 * _always_ in the finalized or reusable state.

		/*

		 * Guarantee the last state load from desc_read() is before

		 * reloading @tail_id in order to see a new tail in the case

		 * that the descriptor has been recycled. This pairs with

		 * desc_reserve:D.

		 *

		 * Memory barrier involvement:

		 *

		 * If prb_first_seq:B reads from desc_reserve:F, then

		 * prb_first_seq:A reads from desc_push_tail:B.

		 *

		 * Relies on:

		 *

		 * MB from desc_push_tail:B to desc_reserve:F

		 *    matching

		 * RMB prb_first_seq:B to prb_first_seq:A

 LMM(prb_first_seq:C) */

/*

 * Non-blocking read of a record. Updates @seq to the last finalized record

 * (which may have no data available).

 *

 * See the description of prb_read_valid() and prb_read_valid_info()

 * for details.

			/*

			 * Behind the tail. Catch up and try again. This

			 * can happen for -ENOENT and -EINVAL cases.

 Record exists, but no data available. Skip. */

 Non-existent/non-finalized record. Must stop. */

/**

 * prb_read_valid() - Non-blocking read of a requested record or (if gone)

 *                    the next available record.

 *

 * @rb:  The ringbuffer to read from.

 * @seq: The sequence number of the record to read.

 * @r:   A record data buffer to store the read record to.

 *

 * This is the public function available to readers to read a record.

 *

 * The reader provides the @info and @text_buf buffers of @r to be

 * filled in. Any of the buffer pointers can be set to NULL if the reader

 * is not interested in that data. To ensure proper initialization of @r,

 * prb_rec_init_rd() should be used.

 *

 * Context: Any context.

 * Return: true if a record was read, otherwise false.

 *

 * On success, the reader must check r->info.seq to see which record was

 * actually read. This allows the reader to detect dropped records.

 *

 * Failure means @seq refers to a not yet written record.

/**

 * prb_read_valid_info() - Non-blocking read of meta data for a requested

 *                         record or (if gone) the next available record.

 *

 * @rb:         The ringbuffer to read from.

 * @seq:        The sequence number of the record to read.

 * @info:       A buffer to store the read record meta data to.

 * @line_count: A buffer to store the number of lines in the record text.

 *

 * This is the public function available to readers to read only the

 * meta data of a record.

 *

 * The reader provides the @info, @line_count buffers to be filled in.

 * Either of the buffer pointers can be set to NULL if the reader is not

 * interested in that data.

 *

 * Context: Any context.

 * Return: true if a record's meta data was read, otherwise false.

 *

 * On success, the reader must check info->seq to see which record meta data

 * was actually read. This allows the reader to detect dropped records.

 *

 * Failure means @seq refers to a not yet written record.

/**

 * prb_first_valid_seq() - Get the sequence number of the oldest available

 *                         record.

 *

 * @rb: The ringbuffer to get the sequence number from.

 *

 * This is the public function available to readers to see what the

 * first/oldest valid sequence number is.

 *

 * This provides readers a starting point to begin iterating the ringbuffer.

 *

 * Context: Any context.

 * Return: The sequence number of the first/oldest record or, if the

 *         ringbuffer is empty, 0 is returned.

/**

 * prb_next_seq() - Get the sequence number after the last available record.

 *

 * @rb:  The ringbuffer to get the sequence number from.

 *

 * This is the public function available to readers to see what the next

 * newest sequence number available to readers will be.

 *

 * This provides readers a sequence number to jump to if all currently

 * available records should be skipped.

 *

 * Context: Any context.

 * Return: The sequence number of the next newest (not yet available) record

 *         for readers.

 Search forward from the oldest descriptor. */

/**

 * prb_init() - Initialize a ringbuffer to use provided external buffers.

 *

 * @rb:       The ringbuffer to initialize.

 * @text_buf: The data buffer for text data.

 * @textbits: The size of @text_buf as a power-of-2 value.

 * @descs:    The descriptor buffer for ringbuffer records.

 * @descbits: The count of @descs items as a power-of-2 value.

 * @infos:    The printk_info buffer for ringbuffer records.

 *

 * This is the public function available to writers to setup a ringbuffer

 * during runtime using provided buffers.

 *

 * This must match the initialization of DEFINE_PRINTKRB().

 *

 * Context: Any context.

/**

 * prb_record_text_space() - Query the full actual used ringbuffer space for

 *                           the text data of a reserved entry.

 *

 * @e: The successfully reserved entry to query.

 *

 * This is the public function available to writers to see how much actual

 * space is used in the ringbuffer to store the text data of the specified

 * entry.

 *

 * This function is only valid if @e has been successfully reserved using

 * prb_reserve().

 *

 * Context: Any context.

 * Return: The size in bytes used by the text data of the associated record.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  linux/kernel/printk.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 * Modified to make sys_syslog() more flexible: added commands to

 * return the last 4k of kernel messages, regardless of whether

 * they've been read or not.  Added option to suppress kernel printk's

 * to the console.  Added hook for sending the console messages

 * elsewhere, in preparation for a serial line console (someday).

 * Ted Ts'o, 2/11/93.

 * Modified for sysctl support, 1/8/97, Chris Horn.

 * Fixed SMP synchronization, 08/08/99, Manfred Spraul

 *     manfred@colorfullife.com

 * Rewrote bits to get rid of console_lock

 *	01Mar01 Andrew Morton

 console_loglevel */

 default_message_loglevel */

 minimum_console_loglevel */

 default_console_loglevel */

/*

 * Low level drivers may need that to know if they can schedule in

 * their unblank() callback or not. So let's export it.

/*

 * console_sem protects the console_drivers list, and also

 * provides serialisation for access to the entire console

 * driver system.

/*

 * System may need to suppress printk message under certain

 * circumstances, like after kernel panic happens.

 Keep both the 'on' and 'off' bits clear, i.e. ratelimit by default: */

	/*

	 * Set sysctl string accordingly:

 else "ratelimit" which is set by default. */

	/*

	 * Sysctl cannot change it anymore. The kernel command line setting of

	 * this parameter is to force the setting to be permanent throughout the

	 * runtime of the system. This is a precation measure against userspace

	 * trying to be a smarta** and attempting to change it up on us.

		/*

		 * Do not accept an unknown string OR a known string with

		 * trailing crap...

 ... and restore old setting. */

 Number of registered extended console drivers. */

/*

 * Helper macros to handle lockdep when locking/unlocking console_sem. We use

 * macros instead of functions so that _RET_IP_ contains useful information.

	/*

	 * Here and in __up_console_sem() we need to be in safe mode,

	 * because spindump/WARN/etc from under console ->lock will

	 * deadlock in printk()->down_trylock_console_sem() otherwise.

/*

 * This is used for debugging the mess that is the VT code by

 * keeping track if we have the console semaphore held. It's

 * definitely not the perfect debug tool (we don't know if _WE_

 * hold it and are racing, but it helps tracking those weird code

 * paths in the console code where we end up in places I want

 * locked without the console semaphore held).

/*

 * If exclusive_console is non-NULL then only this console is to be printed to.

/*

 *	Array of consoles built from command line options (console=)

 Flag: console code may call schedule() */

/*

 * The printk log buffer consists of a sequenced collection of records, each

 * containing variable length message text. Every record also contains its

 * own meta-data (@info).

 *

 * Every record meta-data carries the timestamp in microseconds, as well as

 * the standard userspace syslog level and syslog facility. The usual kernel

 * messages use LOG_KERN; userspace-injected messages always carry a matching

 * syslog facility, by default LOG_USER. The origin of every message can be

 * reliably determined that way.

 *

 * The human readable log message of a record is available in @text, the

 * length of the message text in @text_len. The stored message is not

 * terminated.

 *

 * Optionally, a record can carry a dictionary of properties (key/value

 * pairs), to provide userspace with a machine-readable message context.

 *

 * Examples for well-defined, commonly used property names are:

 *   DEVICE=b12:8               device identifier

 *                                b12:8         block dev_t

 *                                c127:3        char dev_t

 *                                n8            netdev ifindex

 *                                +sound:card0  subsystem:devname

 *   SUBSYSTEM=pci              driver-core subsystem name

 *

 * Valid characters in property names are [a-zA-Z0-9.-_]. Property names

 * and values are terminated by a '\0' character.

 *

 * Example of record values:

 *   record.text_buf                = "it's a line" (unterminated)

 *   record.info.seq                = 56

 *   record.info.ts_nsec            = 36863

 *   record.info.text_len           = 11

 *   record.info.facility           = 0 (LOG_KERN)

 *   record.info.flags              = 0

 *   record.info.level              = 3 (LOG_ERR)

 *   record.info.caller_id          = 299 (task 299)

 *   record.info.dev_info.subsystem = "pci" (terminated)

 *   record.info.dev_info.device    = "+pci:0000:00:01.0" (terminated)

 *

 * The 'struct printk_info' buffer must never be directly exported to

 * userspace, it is a kernel-private implementation detail that might

 * need to be changed in the future, when the requirements change.

 *

 * /dev/kmsg exports the structured data in the following line format:

 *   "<level>,<sequnum>,<timestamp>,<contflag>[,additional_values, ... ];<message text>\n"

 *

 * Users of the export format should ignore possible additional values

 * separated by ',', and find the message after the ';' character.

 *

 * The optional key/value pairs are attached as continuation lines starting

 * with a space character and terminated by a newline. All possible

 * non-prinatable characters are escaped in the "\xff" notation.

 syslog_lock protects syslog_* variables and write access to clear_seq. */

 All 3 protected by @syslog_lock. */

 the next printk record to read by syslog(READ) or /proc/kmsg */

 All 3 protected by @console_sem. */

 the next printk record to write to the console */

/*

 * The next printk record to read after the last 'clear' command. There are

 * two copies (updated with seqcount_latch) so that reads can locklessly

 * access a valid value. Writers are synchronized by @syslog_lock.

 the maximum size of a formatted record (i.e. with prefix added per line) */

 the maximum size allowed to be reserved for a record */

 record buffer */

/*

 * Define the average message size. This only affects the number of

 * descriptors that will be available. Underestimating is better than

 * overestimating (too many available descriptors is better than not enough).

 32 character average length */

/*

 * We cannot access per-CPU data (e.g. per-CPU flush irq_work) before

 * per_cpu_areas are initialised. This variable is set to true when

 * it's safe to access per-CPU data.

 Must be called under syslog_lock. */

 Can be called from any context. */

 Return log buffer address */

 Return log buffer size */

/*

 * Define how much of the log buffer we could take at maximum. The value

 * must be greater than two. Note that only half of the buffer is available

 * when the index points to the middle.

	/*

	 * The message should not take the whole buffer. Otherwise, it might

	 * get removed too soon.

 enable the warning message (if there is room) */

	/*

	 * Unless restricted, we allow "read all" and "get buffer size"

	 * for everybody.

	/*

	 * If this is from /proc/kmsg and we've already opened it, then we've

	 * already done the capabilities checks at open time.

		/*

		 * For historical reasons, accept CAP_SYS_ADMIN too, with

		 * a warning.

 escape non-printable characters */

 dict prefix */

 /dev/kmsg - userspace message inject/listen interface */

 LOG_USER */

 Ignore when user logging is disabled. */

 Ratelimit when not explicitly enabled. */

	/*

	 * Extract and skip the syslog prefix <[0-9]*>. Coming from userspace

	 * the decimal value represents 32bit, the lower 3 bit are the log

	 * level, the rest are the log facility.

	 *

	 * If no prefix or no userspace facility is specified, we

	 * enforce LOG_USER, to be able to reliably distinguish

	 * kernel-generated messages from userspace-injected ones.

 our last seen message is gone, return error and reset */

/*

 * Be careful when modifying this function!!!

 *

 * Only few operations are supported because the device works only with the

 * entire variable length messages (records). Non-standard values are

 * returned in the other cases and has been this way for quite some time.

 * User space applications might depend on this behavior.

 the first record */

		/*

		 * The first record after the last SYSLOG_ACTION_CLEAR,

		 * like issued by 'dmesg -c'. Reading /dev/kmsg itself

		 * changes no global state, and does not clear anything.

 after the last record */

 return error when data has vanished underneath us */

 write-only does not need any file context */

/*

 * This appends the listed symbols to /proc/vmcore

 *

 * /proc/vmcore is used by various utilities, like crash and makedumpfile to

 * obtain access to symbols that are otherwise very difficult to locate.  These

 * symbols are specifically used so that utilities can access and extract the

 * dmesg log from a vmcore file after a crash.

	/*

	 * Export struct size and field offsets. User space tools can

	 * parse it and detect any changes to structure down the line.

 requested log_buf_len from kernel cmdline */

 we practice scaling the ring buffer by powers of 2 */

 save requested log_buf_len since it's too early to process it */

	/*

	 * archs should set up cpu_possible_bits properly with

	 * set_cpu_possible() after setup_arch() but just in

	 * case lets ensure this is valid.

 by default this will only continue through for large > 64 CPUs */

 !CONFIG_SMP */

 CONFIG_SMP */

	/*

	 * Some archs call setup_log_buf() multiple times - first is very

	 * early, e.g. from setup_arch(), and second - when percpu_areas

	 * are initialised.

	/*

	 * Copy any remaining messages that might have appeared from

	 * NMI context after copying but before switching to the

	 * dynamic buffer.

 msecs delay after each printk during bootup */

 based on boot_delay */

 some guess */

		/*

		 * use (volatile) jiffies to prevent

		 * compiler reduction; loop termination via jiffies

		 * is secondary and may or may not happen.

/*

 * Prepare the record for printing. The text is shifted within the given

 * buffer to avoid a need for another one. The following operations are

 * done:

 *

 *   - Add prefix for each line.

 *   - Drop truncated lines that no longer fit into the buffer.

 *   - Add the trailing newline that has been removed in vprintk_store().

 *   - Add a string terminator.

 *

 * Since the produced string is always terminated, the maximum possible

 * return value is @r->text_buf_size - 1;

 *

 * Return: The length of the updated/prepared text, including the added

 * prefixes and the newline. The terminator is not counted. The dropped

 * line(s) are not counted.

	/*

	 * If the message was truncated because the buffer was not large

	 * enough, treat the available text as if it were the full text.

	/*

	 * @text_len: bytes of unprocessed text

	 * @line_len: bytes of current line _without_ newline

	 * @text:     pointer to beginning of current line

	 * @len:      number of bytes prepared in r->text_buf

 Drop truncated line(s). */

		/*

		 * Truncate the text if there is not enough space to add the

		 * prefix and a trailing newline and a terminator.

 Drop even the current line if no space. */

		/*

		 * Increment the prepared length to include the text and

		 * prefix that were just moved+copied. Also increment for the

		 * newline at the end of this line. If this is the last line,

		 * there is no newline, but it will be added immediately below.

			/*

			 * This is the last line. Add the trailing newline

			 * removed in vprintk_store().

		/*

		 * Advance beyond the added prefix and the related line with

		 * its newline.

		/*

		 * The remaining text has only decreased by the line with its

		 * newline.

		 *

		 * Note that @text_len can become zero. It happens when @text

		 * ended with a newline (either due to truncation or the

		 * original string ending with "\n\n"). The loop is correctly

		 * repeated and (if not truncated) an empty line with a prefix

		 * will be prepared.

	/*

	 * If a buffer was provided, it will be terminated. Space for the

	 * string terminator is guaranteed to be available. The terminator is

	 * not counted in the return value.

	/*

	 * Each line will be preceded with a prefix. The intermediate

	 * newlines are already within the text, but a final trailing

	 * newline will be added.

/*

 * Beginning with @start_seq, find the first record where it and all following

 * records up to (but not including) @max_seq fit into @size.

 *

 * @max_seq is simply an upper bound and does not need to exist. If the caller

 * does not require an upper bound, -1 can be used for @max_seq.

 Determine the size of the records up to @max_seq. */

	/*

	 * Adjust the upper bound for the next loop to avoid subtracting

	 * lengths that were never added.

	/*

	 * Move first record forward until length fits into the buffer. Ignore

	 * newest messages that were not counted in the above cycle. Messages

	 * might appear and get lost in the meantime. This is a best effort

	 * that prevents an infinite loop that could occur with a retry.

 The caller is responsible for making sure @size is greater than 0. */

	/*

	 * Wait for the @syslog_seq record to be available. @syslog_seq may

	 * change while waiting.

	/*

	 * Copy records that fit into the buffer. The above cycle makes sure

	 * that the first record is always available.

 message is gone, move to next valid one */

		/*

		 * To keep reading/counting partial line consistent,

		 * use printk_time value as of the beginning of a line.

 message fits into buffer, move forward */

 partial read(), remember position */

	/*

	 * Find first record that fits, including all following records,

	 * into the user-provided buffer for this dump.

 Close log */

 Open log */

 Read from log */

 Read/clear last kernel messages */

 Read last kernel messages */

 Clear ring buffer */

 Disable logging to console */

 Enable logging to console */

 Set level of messages printed to console */

 Implicitly re-enable logging to console */

 Number of chars in the log buffer */

 No unread messages. */

 messages are gone, move to first one */

			/*

			 * Short-cut for poll(/"proc/kmsg") which simply checks

			 * for pending data, not the size; return the count of

			 * records, not the length.

 Size of the log buffer */

/*

 * Special console_lock variants that help to reduce the risk of soft-lockups.

 * They allow to pass console_lock to another printk() call using a busy wait.

/**

 * console_lock_spinning_enable - mark beginning of code where another

 *	thread might safely busy wait

 *

 * This basically converts console_lock into a spinlock. This marks

 * the section where the console_lock owner can not sleep, because

 * there may be a waiter spinning (like a spinlock). Also it must be

 * ready to hand over the lock at the end of the section.

 The waiter may spin on us after setting console_owner */

/**

 * console_lock_spinning_disable_and_check - mark end of code where another

 *	thread was able to busy wait and check if there is a waiter

 *

 * This is called at the end of the section where spinning is allowed.

 * It has two functions. First, it is a signal that it is no longer

 * safe to start busy waiting for the lock. Second, it checks if

 * there is a busy waiter and passes the lock rights to her.

 *

 * Important: Callers lose the lock if there was a busy waiter.

 *	They must not touch items synchronized by console_lock

 *	in this case.

 *

 * Return: 1 if the lock rights were passed, 0 otherwise.

 The waiter is now free to continue */

	/*

	 * Hand off console_lock to waiter. The waiter will perform

	 * the up(). After this, the waiter is the console_lock owner.

/**

 * console_trylock_spinning - try to get console_lock by busy waiting

 *

 * This allows to busy wait for the console_lock when the current

 * owner is running in specially marked sections. It means that

 * the current owner is running and cannot reschedule until it

 * is ready to lose the lock.

 *

 * Return: 1 if we got the lock, 0 othrewise

	/*

	 * If there is an active printk() writing to the

	 * consoles, instead of having it write our data too,

	 * see if we can offload that load from the active

	 * printer, and do some printing ourselves.

	 * Go into a spin only if there isn't already a waiter

	 * spinning, and there is an active printer, and

	 * that active printer isn't us (recursive printk?).

 We spin waiting for the owner to release us */

 Owner will clear console_waiter on hand off */

	/*

	 * The owner passed the console lock to us.

	 * Since we did not spin on console lock, annotate

	 * this as a trylock. Otherwise lockdep will

	 * complain.

/*

 * Call the console drivers, asking them to write out

 * log_buf[start] to log_buf[end - 1].

 * The console_lock must be held.

/*

 * Recursion is tracked separately on each CPU. If NMIs are supported, an

 * additional NMI context per CPU is also separately tracked. Until per-CPU

 * is available, a separate "early tracking" is performed.

/*

 * Recursion is limited to keep the output sane. printk() should not require

 * more than 1 level of recursion (allowing, for example, printk() to trigger

 * a WARN), but a higher value is used in case some printk-internal errors

 * exist, such as the ringbuffer validation checks failing.

/*

 * Return a pointer to the dedicated counter for the CPU+context of the

 * caller.

/*

 * Enter recursion tracking. Interrupts are disabled to simplify tracking.

 * The caller must check the boolean return value to see if the recursion is

 * allowed. On failure, interrupts are not disabled.

 *

 * @recursion_ptr must be a variable of type (u8 *) and is the same variable

 * that is passed to printk_exit_irqrestore().

 Exit recursion tracking, restoring interrupts. */

/**

 * printk_parse_prefix - Parse level and control flags.

 *

 * @text:     The terminated text message.

 * @level:    A pointer to the current level value, will be updated.

 * @flags:    A pointer to the current printk_info flags, will be updated.

 *

 * @level may be NULL if the caller is not interested in the parsed value.

 * Otherwise the variable pointed to by @level must be set to

 * LOGLEVEL_DEFAULT in order to be updated with the parsed value.

 *

 * @flags may be NULL if the caller is not interested in the parsed value.

 * Otherwise the variable pointed to by @flags will be OR'd with the parsed

 * value.

 *

 * Return: The length of the parsed level and control flags.

 KERN_CONT */

 Mark and strip a trailing newline. */

 Strip log level and control flags. */

	/*

	 * Since the duration of printk() can vary depending on the message

	 * and state of the ringbuffer, grab the timestamp now so that it is

	 * close to the call of printk(). This provides a more deterministic

	 * timestamp with respect to the caller.

	/*

	 * The sprintf needs to come first since the syslog prefix might be

	 * passed in as a parameter. An extra byte must be reserved so that

	 * later the vscnprintf() into the reserved buffer has room for the

	 * terminating '\0', which is not counted by vsnprintf().

 Extract log level or control flags. */

	/*

	 * Explicitly initialize the record before every prb_reserve() call.

	 * prb_reserve_in_last() and prb_reserve() purposely invalidate the

	 * structure when they fail.

 truncate the message if it is too long for empty buffer */

 fill message */

 A message without a trailing newline can be continued. */

 Suppress unimportant messages after panic happens */

 If called from the scheduler, we can not call up(). */

		/*

		 * Disable preemption to avoid being preempted while holding

		 * console_sem which would prevent anyone from printing to

		 * console

		/*

		 * Try to acquire and then immediately release the console

		 * semaphore.  The release will print out buffers and wake up

		 * /dev/kmsg and syslog() users.

 CONFIG_PRINTK */

 CONFIG_PRINTK */

	/*

	 *	See if this tty is not yet registered, and

	 *	if we have a slot free.

/*

 * Set up a console.  Called via do_early_param() in init/main.c

 * for each "console=" parameter in the boot command line.

 4 for "ttyS" */

	/*

	 * console="" or console=null have been suggested as a way to

	 * disable console output. Use ttynull that has been created

	 * for exactly this purpose.

	/*

	 * Decode str into name, index, options.

/**

 * add_preferred_console - add a device to the list of preferred consoles.

 * @name: device name

 * @idx: device index

 * @options: options for this console

 *

 * The last preferred console added will be used for kernel messages

 * and stdin/out/err for init.  Normally this is used by console_setup

 * above to handle user-supplied console arguments; however it can also

 * be used by arch-specific code either to override the user or more

 * commonly to provide a default console (ie from PROM variables) when

 * the user has not supplied one.

/**

 * suspend_console - suspend the console subsystem

 *

 * This disables printk() while we go into suspend states

/**

 * console_cpu_notify - print deferred console messages after CPU hotplug

 * @cpu: unused

 *

 * If printk() is called from a CPU that is not online yet, the messages

 * will be printed on the console only if there are CON_ANYTIME consoles.

 * This function is called when a new CPU comes online (or fails to come

 * up) or goes offline.

 If trylock fails, someone else is doing the printing */

/**

 * console_lock - lock the console system for exclusive use.

 *

 * Acquires a lock which guarantees that the caller has

 * exclusive access to the console system and the console_drivers list.

 *

 * Can sleep, returns nothing.

/**

 * console_trylock - try to lock the console system for exclusive use.

 *

 * Try to acquire a lock which guarantees that the caller has exclusive

 * access to the console system and the console_drivers list.

 *

 * returns 1 on success, and 0 on failure to acquire the lock.

/*

 * Check if we have any console that is capable of printing while cpu is

 * booting or shutting down. Requires console_sem.

/*

 * Can we actually use the console at this time on this cpu?

 *

 * Console drivers may assume that per-cpu resources have been allocated. So

 * unless they're explicitly marked as being able to cope (CON_ANYTIME) don't

 * call them until this CPU is officially up.

/**

 * console_unlock - unlock the console system

 *

 * Releases the console_lock which the caller holds on the console system

 * and the console driver list.

 *

 * While the console_lock was held, console output may have been buffered

 * by printk().  If this is the case, console_unlock(); emits

 * the output prior to releasing the lock.

 *

 * If there is output waiting, we wake /dev/kmsg and syslog() users.

 *

 * console_unlock(); may be called from any context.

	/*

	 * Console drivers are called with interrupts disabled, so

	 * @console_may_schedule should be cleared before; however, we may

	 * end up dumping a lot of lines, for example, if called from

	 * console registration path, and should invoke cond_resched()

	 * between lines if allowable.  Not doing so can cause a very long

	 * scheduling stall on a slow console leading to RCU stall and

	 * softlockup warnings which exacerbate the issue with more

	 * messages practically incapacitating the system.

	 *

	 * console_trylock() is not able to detect the preemptive

	 * context reliably. Therefore the value must be stored before

	 * and cleared after the "again" goto label.

	/*

	 * We released the console_sem lock, so we need to recheck if

	 * cpu is online and (if not) is there at least one CON_ANYTIME

	 * console.

			/*

			 * Skip record we have buffered and already printed

			 * directly to the console when we received it, and

			 * record that has level above the console loglevel.

 Output to all consoles once old messages replayed. */

		/*

		 * Handle extended console text first because later

		 * record_print_text() will modify the record buffer in-place.

		/*

		 * While actively printing out messages, if another printk()

		 * were to occur on another CPU, it may wait for this one to

		 * finish. This task can not be preempted if there is a

		 * waiter waiting to take over.

		 *

		 * Interrupts are disabled because the hand over to a waiter

		 * must not be interrupted until the hand over is completed

		 * (@console_waiter is cleared).

 don't trace print latency */

 Get consistent value of the next-to-be-used sequence number. */

	/*

	 * Someone could have filled up the buffer again, so re-check if there's

	 * something to flush. In case we cannot trylock the console_sem again,

	 * there's a new owner and the console_unlock() from them will do the

	 * flush, no worries.

/**

 * console_conditional_schedule - yield the CPU if required

 *

 * If the console code is currently allowed to sleep, and

 * if this CPU should yield the CPU to another task, do

 * so here.

 *

 * Must be called within console_lock();.

	/*

	 * console_unblank can no longer be called in interrupt context unless

	 * oops_in_progress is set to 1..

/**

 * console_flush_on_panic - flush console content on panic

 * @mode: flush all messages in buffer or just the pending ones

 *

 * Immediately output all pending messages no matter what.

	/*

	 * If someone else is holding the console lock, trylock will fail

	 * and may_schedule may be set.  Ignore and proceed to unlock so

	 * that messages are flushed out.  As this can be called from any

	 * context and we don't want to get preempted while flushing,

	 * ensure may_schedule is cleared.

/*

 * Return the console tty driver structure and its associated index

/*

 * Prevent further output on the passed console device so that (for example)

 * serial drivers can disable console output before suspending a port, and can

 * re-enable output afterwards.

/*

 * This is called by register_console() to try to match

 * the newly registered console with any of the ones selected

 * by either the command line or add_preferred_console() and

 * setup/enable it.

 *

 * Care need to be taken with consoles that are statically

 * enabled such as netconsole

 default matching */

	/*

	 * Some consoles, such as pstore and netconsole, can be enabled even

	 * without matching. Accept the pre-enabled consoles only when match()

	 * and setup() had a chance to be called.

/*

 * The console driver calls this routine during kernel initialization

 * to register the console printing procedure with printk() and to

 * print any messages that were printed by the kernel before the

 * console driver was initialized.

 *

 * This can happen pretty early during the boot process (because of

 * early_printk) - sometimes before setup_arch() completes - be careful

 * of what kernel features are used - they may not be initialised yet.

 *

 * There are two types of consoles - bootconsoles (early_printk) and

 * "real" consoles (everything which is not a bootconsole) which are

 * handled differently.

 *  - Any number of bootconsoles can be registered at any time.

 *  - As soon as a "real" console is registered, all bootconsoles

 *    will be unregistered automatically.

 *  - Once a "real" console is registered, any attempt to register a

 *    bootconsoles will be rejected

	/*

	 * before we register a new CON_BOOT console, make sure we don't

	 * already have a valid console

	/*

	 *	See if we want to use this console driver. If we

	 *	didn't select a console we take the first one

	 *	that registers here.

 See if this console matches one we selected on the command line */

 If not, try to match against the platform default(s) */

 printk() messages are not printed to the Braille console. */

	/*

	 * If we have a bootconsole, and are switching to a real console,

	 * don't print everything out again, since when the boot console, and

	 * the real console are the same physical device, it's annoying to

	 * see the beginning boot messages twice

	/*

	 *	Put this console in the list - keep the

	 *	preferred driver at the head of the list.

 Ensure this flag is always set for the head of the list */

		/*

		 * console_unlock(); will print out the buffered messages

		 * for us.

		 *

		 * We're about to replay the log buffer.  Only do this to the

		 * just-registered console to avoid excessive message spam to

		 * the already-registered consoles.

		 *

		 * Set exclusive_console with disabled interrupts to reduce

		 * race window with eventual console_flush_on_panic() that

		 * ignores console_lock.

 Get a consistent copy of @syslog_seq. */

	/*

	 * By unregistering the bootconsoles after we enable the real console

	 * we get the "console xxx enabled" message on all the consoles -

	 * boot consoles, real consoles, etc - this is to ensure that end

	 * users know there might be something in the kernel's log buffer that

	 * went to the bootconsole (that they do not see on the real console)

		/* We need to iterate through all boot consoles, to make

		 * sure we print everything out, before we unregister them.

	/*

	 * If this isn't the last console and it has CON_CONSDEV set, we

	 * need to set it on the next preferred console.

/*

 * Initialize the console device. This is called *early*, so

 * we can't necessarily depend on lots of kernel help here.

 * Just do some early initializations, and do the complex setup

 * later.

 Setup the default TTY line discipline. */

	/*

	 * set up the console device so that later boot sequences can

	 * inform about problems etc..

/*

 * Some boot consoles access data that is in the init section and which will

 * be discarded after the initcalls have been run. To make sure that no code

 * will access this data, unregister the boot consoles in a late initcall.

 *

 * If for some reason, such as deferred probe or the driver being a loadable

 * module, the real console hasn't registered yet at this point, there will

 * be a brief interval in which no messages are logged to the console, which

 * makes it difficult to diagnose problems that occur during this time.

 *

 * To mitigate this problem somewhat, only unregister consoles whose memory

 * intersects with the init section. Note that all other boot consoles will

 * get unregistered when the real preferred console is registered.

 Check addresses that might be used for enabled consoles. */

			/*

			 * Please, consider moving the reported consoles out

			 * of the init section.

/*

 * Delayed printk version, for scheduler-internal messages:

 If trylock fails, someone else is doing the printing */

/*

 * printk rate limiting, lifted from the networking subsystem.

 *

 * This enforces a rate limit: not more than 10 kernel messages

 * every 5s to make a denial-of-service attack impossible.

/**

 * printk_timed_ratelimit - caller-controlled printk ratelimiting

 * @caller_jiffies: pointer to caller's state

 * @interval_msecs: minimum interval between prints

 *

 * printk_timed_ratelimit() returns true if more than @interval_msecs

 * milliseconds have elapsed since the last time printk_timed_ratelimit()

 * returned true.

/**

 * kmsg_dump_register - register a kernel log dumper.

 * @dumper: pointer to the kmsg_dumper structure

 *

 * Adds a kernel log dumper to the system. The dump callback in the

 * structure will be called when the kernel oopses or panics and must be

 * set. Returns zero on success and %-EINVAL or %-EBUSY otherwise.

 The dump callback needs to be set */

 Don't allow registering multiple times */

/**

 * kmsg_dump_unregister - unregister a kmsg dumper.

 * @dumper: pointer to the kmsg_dumper structure

 *

 * Removes a dump device from the system. Returns zero on success and

 * %-EINVAL otherwise.

/**

 * kmsg_dump - dump kernel log to kernel message dumpers.

 * @reason: the reason (oops, panic etc) for dumping

 *

 * Call each of the registered dumper's dump() callback, which can

 * retrieve the kmsg records with kmsg_dump_get_line() or

 * kmsg_dump_get_buffer().

		/*

		 * If client has not provided a specific max_reason, default

		 * to KMSG_DUMP_OOPS, unless always_kmsg_dump was set.

 invoke dumper which will iterate over records */

/**

 * kmsg_dump_get_line - retrieve one kmsg log line

 * @iter: kmsg dump iterator

 * @syslog: include the "<4>" prefixes

 * @line: buffer to copy the line to

 * @size: maximum size of the buffer

 * @len: length of line placed into buffer

 *

 * Start at the beginning of the kmsg buffer, with the oldest kmsg

 * record, and copy one record into the provided buffer.

 *

 * Consecutive calls will return the next available record moving

 * towards the end of the buffer with the youngest messages.

 *

 * A return value of FALSE indicates that there are no more records to

 * read.

 Read text or count text lines? */

/**

 * kmsg_dump_get_buffer - copy kmsg log lines

 * @iter: kmsg dump iterator

 * @syslog: include the "<4>" prefixes

 * @buf: buffer to copy the line to

 * @size: maximum size of the buffer

 * @len_out: length of line placed into buffer

 *

 * Start at the end of the kmsg buffer and fill the provided buffer

 * with as many of the *youngest* kmsg records that fit into it.

 * If the buffer is large enough, all available kmsg records will be

 * copied with a single call.

 *

 * Consecutive calls will fill the buffer with the next block of

 * available older records, not including the earlier retrieved ones.

 *

 * A return value of FALSE indicates that there are no more records to

 * read.

 messages are gone, move to first available one */

 last entry */

	/*

	 * Find first record that fits, including all following records,

	 * into the user-provided buffer for this dump. Pass in size-1

	 * because this function (by way of record_print_text()) will

	 * not write more than size-1 bytes of text into @buf.

	/*

	 * Next kmsg_dump_get_buffer() invocation will dump block of

	 * older records stored right before this one.

 Adjust record to store to remaining buffer space. */

/**

 * kmsg_dump_rewind - reset the iterator

 * @iter: kmsg dump iterator

 *

 * Reset the dumper's iterator so that kmsg_dump_get_line() and

 * kmsg_dump_get_buffer() can be called again and used multiple

 * times within the same dumper.dump() callback.

/**

 * __printk_wait_on_cpu_lock() - Busy wait until the printk cpu-reentrant

 *                               spinning lock is not owned by any CPU.

 *

 * Context: Any context.

/**

 * __printk_cpu_trylock() - Try to acquire the printk cpu-reentrant

 *                          spinning lock.

 *

 * If no processor has the lock, the calling processor takes the lock and

 * becomes the owner. If the calling processor is already the owner of the

 * lock, this function succeeds immediately.

 *

 * Context: Any context. Expects interrupts to be disabled.

 * Return: 1 on success, otherwise 0.

	/*

	 * Guarantee loads and stores from this CPU when it is the lock owner

	 * are _not_ visible to the previous lock owner. This pairs with

	 * __printk_cpu_unlock:B.

	 *

	 * Memory barrier involvement:

	 *

	 * If __printk_cpu_trylock:A reads from __printk_cpu_unlock:B, then

	 * __printk_cpu_unlock:A can never read from __printk_cpu_trylock:B.

	 *

	 * Relies on:

	 *

	 * RELEASE from __printk_cpu_unlock:A to __printk_cpu_unlock:B

	 * of the previous CPU

	 *    matching

	 * ACQUIRE from __printk_cpu_trylock:A to __printk_cpu_trylock:B

	 * of this CPU

 LMM(__printk_cpu_trylock:A) */

		/*

		 * This CPU is now the owner and begins loading/storing

		 * data: LMM(__printk_cpu_trylock:B)

 This CPU is already the owner. */

/**

 * __printk_cpu_unlock() - Release the printk cpu-reentrant spinning lock.

 *

 * The calling processor must be the owner of the lock.

 *

 * Context: Any context. Expects interrupts to be disabled.

	/*

	 * This CPU is finished loading/storing data:

	 * LMM(__printk_cpu_unlock:A)

	/*

	 * Guarantee loads and stores from this CPU when it was the

	 * lock owner are visible to the next lock owner. This pairs

	 * with __printk_cpu_trylock:A.

	 *

	 * Memory barrier involvement:

	 *

	 * If __printk_cpu_trylock:A reads from __printk_cpu_unlock:B,

	 * then __printk_cpu_trylock:B reads from __printk_cpu_unlock:A.

	 *

	 * Relies on:

	 *

	 * RELEASE from __printk_cpu_unlock:A to __printk_cpu_unlock:B

	 * of this CPU

	 *    matching

	 * ACQUIRE from __printk_cpu_trylock:A to __printk_cpu_trylock:B

	 * of the next CPU

 LMM(__printk_cpu_unlock:B) */

 CONFIG_SMP */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * READ this before attempting to hack on futexes!

 *

 * Basic futex operation and ordering guarantees

 * =============================================

 *

 * The waiter reads the futex value in user space and calls

 * futex_wait(). This function computes the hash bucket and acquires

 * the hash bucket lock. After that it reads the futex user space value

 * again and verifies that the data has not changed. If it has not changed

 * it enqueues itself into the hash bucket, releases the hash bucket lock

 * and schedules.

 *

 * The waker side modifies the user space value of the futex and calls

 * futex_wake(). This function computes the hash bucket and acquires the

 * hash bucket lock. Then it looks for waiters on that futex in the hash

 * bucket and wakes them.

 *

 * In futex wake up scenarios where no tasks are blocked on a futex, taking

 * the hb spinlock can be avoided and simply return. In order for this

 * optimization to work, ordering guarantees must exist so that the waiter

 * being added to the list is acknowledged when the list is concurrently being

 * checked by the waker, avoiding scenarios like the following:

 *

 * CPU 0                               CPU 1

 * val = *futex;

 * sys_futex(WAIT, futex, val);

 *   futex_wait(futex, val);

 *   uval = *futex;

 *                                     *futex = newval;

 *                                     sys_futex(WAKE, futex);

 *                                       futex_wake(futex);

 *                                       if (queue_empty())

 *                                         return;

 *   if (uval == val)

 *      lock(hash_bucket(futex));

 *      queue();

 *     unlock(hash_bucket(futex));

 *     schedule();

 *

 * This would cause the waiter on CPU 0 to wait forever because it

 * missed the transition of the user space value from val to newval

 * and the waker did not find the waiter in the hash bucket queue.

 *

 * The correct serialization ensures that a waiter either observes

 * the changed user space value before blocking or is woken by a

 * concurrent waker:

 *

 * CPU 0                                 CPU 1

 * val = *futex;

 * sys_futex(WAIT, futex, val);

 *   futex_wait(futex, val);

 *

 *   waiters++; (a)

 *   smp_mb(); (A) <-- paired with -.

 *                                  |

 *   lock(hash_bucket(futex));      |

 *                                  |

 *   uval = *futex;                 |

 *                                  |        *futex = newval;

 *                                  |        sys_futex(WAKE, futex);

 *                                  |          futex_wake(futex);

 *                                  |

 *                                  `--------> smp_mb(); (B)

 *   if (uval == val)

 *     queue();

 *     unlock(hash_bucket(futex));

 *     schedule();                         if (waiters)

 *                                           lock(hash_bucket(futex));

 *   else                                    wake_waiters(futex);

 *     waiters--; (b)                        unlock(hash_bucket(futex));

 *

 * Where (A) orders the waiters increment and the futex value read through

 * atomic operations (see futex_hb_waiters_inc) and where (B) orders the write

 * to futex and the waiters read (see futex_hb_waiters_pending()).

 *

 * This yields the following case (where X:=waiters, Y:=futex):

 *

 *	X = Y = 0

 *

 *	w[X]=1		w[Y]=1

 *	MB		MB

 *	r[Y]=y		r[X]=x

 *

 * Which guarantees that x==0 && y==0 is impossible; which translates back into

 * the guarantee that we cannot both miss the futex variable change and the

 * enqueue.

 *

 * Note that a new waiter is accounted for in (a) even when it is possible that

 * the wait call can return error, in which case we backtrack from it in (b).

 * Refer to the comment in futex_q_lock().

 *

 * Similarly, in order to account for waiters being requeued on another

 * address we always increment the waiters for the destination bucket before

 * acquiring the lock. It then decrements them again  after releasing it -

 * the code that actually moves the futex(es) between hash buckets (requeue_futex)

 * will do the additional required waiter count housekeeping. This is done for

 * double_lock_hb() and double_unlock_hb(), respectively.

/*

 * The hash bucket lock must be held when this is called.

 * Afterwards, the futex_q must not be accessed. Callers

 * must ensure to later call wake_up_q() for the actual

 * wakeups to occur.

	/*

	 * The waiting task can free the futex_q as soon as q->lock_ptr = NULL

	 * is written, without taking any locks. This is possible in the event

	 * of a spurious wakeup, for example. A memory barrier is required here

	 * to prevent the following store to lock_ptr from getting ahead of the

	 * plist_del in __futex_unqueue().

	/*

	 * Queue the task for later wakeup for after we've released

	 * the hb->lock.

/*

 * Wake up waiters matching bitset queued on this futex (uaddr).

 Make sure we really have tasks to wakeup */

 Check if one of the bits is set in both bitsets */

			/*

			 * kill this print and return -EINVAL when userspace

			 * is sane again

/*

 * Wake up all waiters hashed on the physical page that is mapped

 * to this virtual address:

			/*

			 * we don't get EFAULT from MMU faults if we don't have

			 * an MMU, but we might get them from range checking

/**

 * futex_wait_queue() - futex_queue() and wait for wakeup, timeout, or signal

 * @hb:		the futex hash bucket, must be locked by the caller

 * @q:		the futex_q to queue up on

 * @timeout:	the prepared hrtimer_sleeper, or null for no timeout

	/*

	 * The task state is guaranteed to be set before another task can

	 * wake it. set_current_state() is implemented using smp_store_mb() and

	 * futex_queue() calls spin_unlock() upon completion, both serializing

	 * access to the hash list and forcing another memory barrier.

 Arm the timer */

	/*

	 * If we have been removed from the hash list, then another task

	 * has tried to wake us, and we can skip the call to schedule().

		/*

		 * If the timer has already expired, current will already be

		 * flagged for rescheduling. Only call schedule if there

		 * is no timeout, or if it has yet to expire.

/**

 * unqueue_multiple - Remove various futexes from their hash bucket

 * @v:	   The list of futexes to unqueue

 * @count: Number of futexes in the list

 *

 * Helper to unqueue a list of futexes. This can't fail.

 *

 * Return:

 *  - >=0 - Index of the last futex that was awoken;

 *  - -1  - No futex was awoken

/**

 * futex_wait_multiple_setup - Prepare to wait and enqueue multiple futexes

 * @vs:		The futex list to wait on

 * @count:	The size of the list

 * @woken:	Index of the last woken futex, if any. Used to notify the

 *		caller that it can return this index to userspace (return parameter)

 *

 * Prepare multiple futexes in a single step and enqueue them. This may fail if

 * the futex list is invalid or if any futex was already awoken. On success the

 * task is ready to interruptible sleep.

 *

 * Return:

 *  -  1 - One of the futexes was woken by another thread

 *  -  0 - Success

 *  - <0 - -EFAULT, -EWOULDBLOCK or -EINVAL

	/*

	 * Enqueuing multiple futexes is tricky, because we need to enqueue

	 * each futex on the list before dealing with the next one to avoid

	 * deadlocking on the hash bucket. But, before enqueuing, we need to

	 * make sure that current->state is TASK_INTERRUPTIBLE, so we don't

	 * lose any wake events, which cannot be done before the get_futex_key

	 * of the next key, because it calls get_user_pages, which can sleep.

	 * Thus, we fetch the list of futexes keys in two steps, by first

	 * pinning all the memory keys in the futex key, and only then we read

	 * each key and queue the corresponding futex.

	 *

	 * Private futexes doesn't need to recalculate hash in retry, so skip

	 * get_futex_key() when retrying.

			/*

			 * The bucket lock can't be held while dealing with the

			 * next futex. Queue each futex at this moment so hb can

			 * be unlocked.

		/*

		 * Even if something went wrong, if we find out that a futex

		 * was woken, we don't return error and return this index to

		 * userspace

			/*

			 * If we need to handle a page fault, we need to do so

			 * without any lock and any enqueued futex (otherwise

			 * we could lose some wakeup). So we do it here, after

			 * undoing all the work done so far. In success, we

			 * retry all the work.

/**

 * futex_sleep_multiple - Check sleeping conditions and sleep

 * @vs:    List of futexes to wait for

 * @count: Length of vs

 * @to:    Timeout

 *

 * Sleep if and only if the timeout hasn't expired and no futex on the list has

 * been woken up.

/**

 * futex_wait_multiple - Prepare to wait on and enqueue several futexes

 * @vs:		The list of futexes to wait on

 * @count:	The number of objects

 * @to:		Timeout before giving up and returning to userspace

 *

 * Entry point for the FUTEX_WAIT_MULTIPLE futex operation, this function

 * sleeps on a group of futexes and returns on the first futex that is

 * wake, or after the timeout has elapsed.

 *

 * Return:

 *  - >=0 - Hint to the futex that was awoken

 *  - <0  - On error

 A futex was woken during setup */

		/*

		 * The final case is a spurious wakeup, for

		 * which just retry.

/**

 * futex_wait_setup() - Prepare to wait on a futex

 * @uaddr:	the futex userspace address

 * @val:	the expected value

 * @flags:	futex flags (FLAGS_SHARED, etc.)

 * @q:		the associated futex_q

 * @hb:		storage for hash_bucket pointer to be returned to caller

 *

 * Setup the futex_q and locate the hash_bucket.  Get the futex value and

 * compare it with the expected value.  Handle atomic faults internally.

 * Return with the hb lock held on success, and unlocked on failure.

 *

 * Return:

 *  -  0 - uaddr contains val and hb has been locked;

 *  - <1 - -EFAULT or -EWOULDBLOCK (uaddr does not contain val) and hb is unlocked

	/*

	 * Access the page AFTER the hash-bucket is locked.

	 * Order is important:

	 *

	 *   Userspace waiter: val = var; if (cond(val)) futex_wait(&var, val);

	 *   Userspace waker:  if (cond(var)) { var = new; futex_wake(&var); }

	 *

	 * The basic logical guarantee of a futex is that it blocks ONLY

	 * if cond(var) is known to be true at the time of blocking, for

	 * any cond.  If we locked the hash-bucket after testing *uaddr, that

	 * would open a race condition where we could block indefinitely with

	 * cond(var) false, which would violate the guarantee.

	 *

	 * On the other hand, we insert q and release the hash-bucket only

	 * after testing *uaddr.  This guarantees that futex_wait() will NOT

	 * absorb a wakeup if *uaddr does not match the desired values

	 * while the syscall executes.

	/*

	 * Prepare to wait on uaddr. On success, it holds hb->lock and q

	 * is initialized.

 futex_queue and wait for wakeup, timeout, or a signal. */

 If we were woken (and unqueued), we succeeded, whatever. */

	/*

	 * We expect signal_pending(current), but we might be the

	 * victim of a spurious wakeup as well.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for robust futexes: the kernel cleans up held futexes at

 * thread exit time.

 *

 * Implementation: user-space maintains a per-thread list of locks it

 * is holding. Upon do_exit(), the kernel carefully walks this list,

 * and marks all locks that are owned by this thread with the

 * FUTEX_OWNER_DIED bit, and wakes up a waiter (if any). The list is

 * always manipulated with the lock held, so the list is private and

 * per-thread. Userspace also maintains a per-thread 'list_op_pending'

 * field, to allow the kernel to clean up if the thread dies after

 * acquiring the lock, but just before it could have added itself to

 * the list. There can only be one such pending lock.

/**

 * sys_set_robust_list() - Set the robust-futex list head of a task

 * @head:	pointer to the list-head

 * @len:	length of the list-head, as userspace expects

	/*

	 * The kernel knows only one size for now:

/**

 * sys_get_robust_list() - Get the robust-futex list head of a task

 * @pid:	pid of the process [zero for current task]

 * @head_ptr:	pointer to a list-head pointer, the kernel fills it in

 * @len_ptr:	pointer to a length field, the kernel fills in the header size

 Mask of available flags for each futex in futex_waitv list */

/**

 * futex_parse_waitv - Parse a waitv array from userspace

 * @futexv:	Kernel side list of waiters to be filled

 * @uwaitv:     Userspace list to be parsed

 * @nr_futexes: Length of futexv

 *

 * Return: Error code on failure, 0 on success

/**

 * sys_futex_waitv - Wait on a list of futexes

 * @waiters:    List of futexes to wait on

 * @nr_futexes: Length of futexv

 * @flags:      Flag for timeout (monotonic/realtime)

 * @timeout:	Optional absolute timeout.

 * @clockid:	Clock to be used for the timeout, realtime or monotonic.

 *

 * Given an array of `struct futex_waitv`, wait on each uaddr. The thread wakes

 * if a futex_wake() is performed at any uaddr. The syscall returns immediately

 * if any waiter has *uaddr != val. *timeout is an optional timeout value for

 * the operation. Each waiter has individual flags. The `flags` argument for

 * the syscall should be used solely for specifying the timeout as realtime, if

 * needed. Flags for private futexes, sizes, etc. should be used on the

 * individual flags of each waiter.

 *

 * Returns the array index of one of the woken futexes. No further information

 * is provided: any number of other futexes may also have been woken by the

 * same event, and if more than one futex was woken, the retrned index may

 * refer to any one of them. (It is not necessaryily the futex with the

 * smallest index, nor the one most recently woken, nor...)

 This syscall supports no flags for now */

		/*

		 * Since there's no opcode for futex_waitv, use

		 * FUTEX_WAIT_BITSET that uses absolute timeout as well

 CONFIG_COMPAT */

 CONFIG_COMPAT_32BIT_TIME */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * On PREEMPT_RT, the hash bucket lock is a 'sleeping' spinlock with an

 * underlying rtmutex. The task which is about to be requeued could have

 * just woken up (timeout, signal). After the wake up the task has to

 * acquire hash bucket lock, which is held by the requeue code.  As a task

 * can only be blocked on _ONE_ rtmutex at a time, the proxy lock blocking

 * and the hash bucket lock blocking would collide and corrupt state.

 *

 * On !PREEMPT_RT this is not a problem and everything could be serialized

 * on hash bucket lock, but aside of having the benefit of common code,

 * this allows to avoid doing the requeue when the task is already on the

 * way out and taking the hash bucket lock of the original uaddr1 when the

 * requeue has been completed.

 *

 * The following state transitions are valid:

 *

 * On the waiter side:

 *   Q_REQUEUE_PI_NONE		-> Q_REQUEUE_PI_IGNORE

 *   Q_REQUEUE_PI_IN_PROGRESS	-> Q_REQUEUE_PI_WAIT

 *

 * On the requeue side:

 *   Q_REQUEUE_PI_NONE		-> Q_REQUEUE_PI_INPROGRESS

 *   Q_REQUEUE_PI_IN_PROGRESS	-> Q_REQUEUE_PI_DONE/LOCKED

 *   Q_REQUEUE_PI_IN_PROGRESS	-> Q_REQUEUE_PI_NONE (requeue failed)

 *   Q_REQUEUE_PI_WAIT		-> Q_REQUEUE_PI_DONE/LOCKED

 *   Q_REQUEUE_PI_WAIT		-> Q_REQUEUE_PI_IGNORE (requeue failed)

 *

 * The requeue side ignores a waiter with state Q_REQUEUE_PI_IGNORE as this

 * signals that the waiter is already on the way out. It also means that

 * the waiter is still on the 'wait' futex, i.e. uaddr1.

 *

 * The waiter side signals early wakeup to the requeue side either through

 * setting state to Q_REQUEUE_PI_IGNORE or to Q_REQUEUE_PI_WAIT depending

 * on the current state. In case of Q_REQUEUE_PI_IGNORE it can immediately

 * proceed to take the hash bucket lock of uaddr1. If it set state to WAIT,

 * which means the wakeup is interleaving with a requeue in progress it has

 * to wait for the requeue side to change the state. Either to DONE/LOCKED

 * or to IGNORE. DONE/LOCKED means the waiter q is now on the uaddr2 futex

 * and either blocked (DONE) or has acquired it (LOCKED). IGNORE is set by

 * the requeue side when the requeue attempt failed via deadlock detection

 * and therefore the waiter q is still on the uaddr1 futex.

 list gets initialized in futex_queue()*/

/**

 * requeue_futex() - Requeue a futex_q from one hb to another

 * @q:		the futex_q to requeue

 * @hb1:	the source hash_bucket

 * @hb2:	the target hash_bucket

 * @key2:	the new key for the requeued futex_q

	/*

	 * If key1 and key2 hash to the same bucket, no need to

	 * requeue.

	/*

	 * Set state to Q_REQUEUE_PI_IN_PROGRESS unless an early wakeup has

	 * already set Q_REQUEUE_PI_IGNORE to signal that requeue should

	 * ignore the waiter.

		/*

		 * futex_proxy_trylock_atomic() might have set it to

		 * IN_PROGRESS and a interleaved early wake to WAIT.

		 *

		 * It was considered to have an extra state for that

		 * trylock, but that would just add more conditionals

		 * all over the place for a dubious value.

 Requeue succeeded. Set DONE or LOCKED */

 Deadlock, no early wakeup interleave */

 Deadlock, early wakeup interleave. */

 If the waiter interleaved with the requeue let it know */

 Is requeue done already? */

		/*

		 * If not done, then tell the requeue code to either ignore

		 * the waiter or to wake it up once the requeue is done.

 If the requeue was in progress, wait for it to complete */

	/*

	 * Requeue is now either prohibited or complete. Reread state

	 * because during the wait above it might have changed. Nothing

	 * will modify q->requeue_state after this point.

/**

 * requeue_pi_wake_futex() - Wake a task that acquired the lock during requeue

 * @q:		the futex_q

 * @key:	the key of the requeue target futex

 * @hb:		the hash_bucket of the requeue target futex

 *

 * During futex_requeue, with requeue_pi=1, it is possible to acquire the

 * target futex if it is uncontended or via a lock steal.

 *

 * 1) Set @q::key to the requeue target futex key so the waiter can detect

 *    the wakeup on the right futex.

 *

 * 2) Dequeue @q from the hash bucket.

 *

 * 3) Set @q::rt_waiter to NULL so the woken up task can detect atomic lock

 *    acquisition.

 *

 * 4) Set the q->lock_ptr to the requeue target hb->lock for the case that

 *    the waiter has to fixup the pi state.

 *

 * 5) Complete the requeue state so the waiter can make progress. After

 *    this point the waiter task can return from the syscall immediately in

 *    case that the pi state does not have to be fixed up.

 *

 * 6) Wake the waiter task.

 *

 * Must be called with both q->lock_ptr and hb->lock held.

 Signal locked state to the waiter */

/**

 * futex_proxy_trylock_atomic() - Attempt an atomic lock for the top waiter

 * @pifutex:		the user address of the to futex

 * @hb1:		the from futex hash bucket, must be locked by the caller

 * @hb2:		the to futex hash bucket, must be locked by the caller

 * @key1:		the from futex key

 * @key2:		the to futex key

 * @ps:			address to store the pi_state pointer

 * @exiting:		Pointer to store the task pointer of the owner task

 *			which is in the middle of exiting

 * @set_waiters:	force setting the FUTEX_WAITERS bit (1) or not (0)

 *

 * Try and get the lock on behalf of the top waiter if we can do it atomically.

 * Wake the top waiter if we succeed.  If the caller specified set_waiters,

 * then direct futex_lock_pi_atomic() to force setting the FUTEX_WAITERS bit.

 * hb1 and hb2 must be held by the caller.

 *

 * @exiting is only set when the return value is -EBUSY. If so, this holds

 * a refcount on the exiting task on return and the caller needs to drop it

 * after waiting for the exit to complete.

 *

 * Return:

 *  -  0 - failed to acquire the lock atomically;

 *  - >0 - acquired the lock, return value is vpid of the top_waiter

 *  - <0 - error

	/*

	 * Find the top_waiter and determine if there are additional waiters.

	 * If the caller intends to requeue more than 1 waiter to pifutex,

	 * force futex_lock_pi_atomic() to set the FUTEX_WAITERS bit now,

	 * as we have means to handle the possible fault.  If not, don't set

	 * the bit unnecessarily as it will force the subsequent unlock to enter

	 * the kernel.

 There are no waiters, nothing for us to do. */

	/*

	 * Ensure that this is a waiter sitting in futex_wait_requeue_pi()

	 * and waiting on the 'waitqueue' futex which is always !PI.

 Ensure we requeue to the expected futex. */

 Ensure that this does not race against an early wakeup */

	/*

	 * Try to take the lock for top_waiter and set the FUTEX_WAITERS bit

	 * in the contended case or if @set_waiters is true.

	 *

	 * In the contended case PI state is attached to the lock owner. If

	 * the user space lock can be acquired then PI state is attached to

	 * the new owner (@top_waiter->task) when @set_waiters is true.

		/*

		 * Lock was acquired in user space and PI state was

		 * attached to @top_waiter->task. That means state is fully

		 * consistent and the waiter can return to user space

		 * immediately after the wakeup.

 Rewind top_waiter::requeue_state */

		/*

		 * futex_lock_pi_atomic() did not acquire the user space

		 * futex, but managed to establish the proxy lock and pi

		 * state. top_waiter::requeue_state cannot be fixed up here

		 * because the waiter is not enqueued on the rtmutex

		 * yet. This is handled at the callsite depending on the

		 * result of rt_mutex_start_proxy_lock() which is

		 * guaranteed to be reached with this function returning 0.

/**

 * futex_requeue() - Requeue waiters from uaddr1 to uaddr2

 * @uaddr1:	source futex user address

 * @flags:	futex flags (FLAGS_SHARED, etc.)

 * @uaddr2:	target futex user address

 * @nr_wake:	number of waiters to wake (must be 1 for requeue_pi)

 * @nr_requeue:	number of waiters to requeue (0-INT_MAX)

 * @cmpval:	@uaddr1 expected value (or %NULL)

 * @requeue_pi:	if we are attempting to requeue from a non-pi futex to a

 *		pi futex (pi to pi requeue is not supported)

 *

 * Requeue waiters on uaddr1 to uaddr2. In the requeue_pi case, try to acquire

 * uaddr2 atomically on behalf of the top waiter.

 *

 * Return:

 *  - >=0 - on success, the number of tasks requeued or woken;

 *  -  <0 - on error

	/*

	 * When PI not supported: return -ENOSYS if requeue_pi is true,

	 * consequently the compiler knows requeue_pi is always false past

	 * this point which will optimize away all the conditional code

	 * further down.

		/*

		 * Requeue PI only works on two distinct uaddrs. This

		 * check is only valid for private futexes. See below.

		/*

		 * futex_requeue() allows the caller to define the number

		 * of waiters to wake up via the @nr_wake argument. With

		 * REQUEUE_PI, waking up more than one waiter is creating

		 * more problems than it solves. Waking up a waiter makes

		 * only sense if the PI futex @uaddr2 is uncontended as

		 * this allows the requeue code to acquire the futex

		 * @uaddr2 before waking the waiter. The waiter can then

		 * return to user space without further action. A secondary

		 * wakeup would just make the futex_wait_requeue_pi()

		 * handling more complex, because that code would have to

		 * look up pi_state and do more or less all the handling

		 * which the requeue code has to do for the to be requeued

		 * waiters. So restrict the number of waiters to wake to

		 * one, and only wake it up when the PI futex is

		 * uncontended. Otherwise requeue it and let the unlock of

		 * the PI futex handle the wakeup.

		 *

		 * All REQUEUE_PI users, e.g. pthread_cond_signal() and

		 * pthread_cond_broadcast() must use nr_wake=1.

		/*

		 * requeue_pi requires a pi_state, try to allocate it now

		 * without any locks in case it fails.

	/*

	 * The check above which compares uaddrs is not sufficient for

	 * shared futexes. We need to compare the keys:

		/*

		 * Attempt to acquire uaddr2 and wake the top waiter. If we

		 * intend to requeue waiters, force setting the FUTEX_WAITERS

		 * bit.  We force this here where we are able to easily handle

		 * faults rather in the requeue loop below.

		 *

		 * Updates topwaiter::requeue_state if a top waiter exists.

		/*

		 * At this point the top_waiter has either taken uaddr2 or

		 * is waiting on it. In both cases pi_state has been

		 * established and an initial refcount on it. In case of an

		 * error there's nothing.

		 *

		 * The top waiter's requeue_state is up to date:

		 *

		 *  - If the lock was acquired atomically (ret == 1), then

		 *    the state is Q_REQUEUE_PI_LOCKED.

		 *

		 *    The top waiter has been dequeued and woken up and can

		 *    return to user space immediately. The kernel/user

		 *    space state is consistent. In case that there must be

		 *    more waiters requeued the WAITERS bit in the user

		 *    space futex is set so the top waiter task has to go

		 *    into the syscall slowpath to unlock the futex. This

		 *    will block until this requeue operation has been

		 *    completed and the hash bucket locks have been

		 *    dropped.

		 *

		 *  - If the trylock failed with an error (ret < 0) then

		 *    the state is either Q_REQUEUE_PI_NONE, i.e. "nothing

		 *    happened", or Q_REQUEUE_PI_IGNORE when there was an

		 *    interleaved early wakeup.

		 *

		 *  - If the trylock did not succeed (ret == 0) then the

		 *    state is either Q_REQUEUE_PI_IN_PROGRESS or

		 *    Q_REQUEUE_PI_WAIT if an early wakeup interleaved.

		 *    This will be cleaned up in the loop below, which

		 *    cannot fail because futex_proxy_trylock_atomic() did

		 *    the same sanity checks for requeue_pi as the loop

		 *    below does.

 We hold a reference on the pi state. */

			/*

			 * futex_proxy_trylock_atomic() acquired the user space

			 * futex. Adjust task_count.

		/*

		 * If the above failed, then pi_state is NULL and

		 * waiter::requeue_state is correct.

			/*

			 * Two reasons for this:

			 * - EBUSY: Owner is exiting and we just wait for the

			 *   exit to complete.

			 * - EAGAIN: The user space value changed.

			/*

			 * Handle the case where the owner is in the middle of

			 * exiting. Wait for the exit to complete otherwise

			 * this task might loop forever, aka. live lock.

		/*

		 * FUTEX_WAIT_REQUEUE_PI and FUTEX_CMP_REQUEUE_PI should always

		 * be paired with each other and no other futex ops.

		 *

		 * We should never be requeueing a futex_q with a pi_state,

		 * which is awaiting a futex_unlock_pi().

 Plain futexes just wake or requeue and are done */

 Ensure we requeue to the expected futex for requeue_pi. */

		/*

		 * Requeue nr_requeue waiters and possibly one more in the case

		 * of requeue_pi if we couldn't acquire the lock atomically.

		 *

		 * Prepare the waiter to take the rt_mutex. Take a refcount

		 * on the pi_state and store the pointer in the futex_q

		 * object of the waiter.

 Don't requeue when the waiter is already on the way out. */

			/*

			 * Early woken waiter signaled that it is on the

			 * way out. Drop the pi_state reference and try the

			 * next waiter. @this->pi_state is still NULL.

			/*

			 * We got the lock. We do neither drop the refcount

			 * on pi_state nor clear this->pi_state because the

			 * waiter needs the pi_state for cleaning up the

			 * user space value. It will drop the refcount

			 * after doing so. this::requeue_state is updated

			 * in the wakeup as well.

 Waiter is queued, move it to hb2 */

			/*

			 * rt_mutex_start_proxy_lock() detected a potential

			 * deadlock when we tried to queue that waiter.

			 * Drop the pi_state reference which we took above

			 * and remove the pointer to the state from the

			 * waiters futex_q object.

			/*

			 * We stop queueing more waiters and let user space

			 * deal with the mess.

	/*

	 * We took an extra initial reference to the pi_state in

	 * futex_proxy_trylock_atomic(). We need to drop it here again.

/**

 * handle_early_requeue_pi_wakeup() - Handle early wakeup on the initial futex

 * @hb:		the hash_bucket futex_q was original enqueued on

 * @q:		the futex_q woken while waiting to be requeued

 * @timeout:	the timeout associated with the wait (NULL if none)

 *

 * Determine the cause for the early wakeup.

 *

 * Return:

 *  -EWOULDBLOCK or -ETIMEDOUT or -ERESTARTNOINTR

	/*

	 * With the hb lock held, we avoid races while we process the wakeup.

	 * We only need to hold hb (and not hb2) to ensure atomicity as the

	 * wakeup code can't change q.key from uaddr to uaddr2 if we hold hb.

	 * It can't be requeued from uaddr2 to something else since we don't

	 * support a PI aware source futex for requeue.

	/*

	 * We were woken prior to requeue by a timeout or a signal.

	 * Unqueue the futex_q and determine which it was.

 Handle spurious wakeups gracefully */

/**

 * futex_wait_requeue_pi() - Wait on uaddr and take uaddr2

 * @uaddr:	the futex we initially wait on (non-pi)

 * @flags:	futex flags (FLAGS_SHARED, FLAGS_CLOCKRT, etc.), they must be

 *		the same type, no requeueing from private to shared, etc.

 * @val:	the expected value of uaddr

 * @abs_time:	absolute timeout

 * @bitset:	32 bit wakeup bitset set by userspace, defaults to all

 * @uaddr2:	the pi futex we will take prior to returning to user-space

 *

 * The caller will wait on uaddr and will be requeued by futex_requeue() to

 * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake

 * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to

 * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;

 * without one, the pi logic would not know which task to boost/deboost, if

 * there was a need to.

 *

 * We call schedule in futex_wait_queue() when we enqueue and return there

 * via the following--

 * 1) wakeup on uaddr2 after an atomic lock acquisition by futex_requeue()

 * 2) wakeup on uaddr2 after a requeue

 * 3) signal

 * 4) timeout

 *

 * If 3, cleanup and return -ERESTARTNOINTR.

 *

 * If 2, we may then block on trying to take the rt_mutex and return via:

 * 5) successful lock

 * 6) signal

 * 7) timeout

 * 8) other lock acquisition failure

 *

 * If 6, return -EWOULDBLOCK (restarting the syscall would do the same).

 *

 * If 4 or 7, we cleanup and return with -ETIMEDOUT.

 *

 * Return:

 *  -  0 - On success;

 *  - <0 - On error

	/*

	 * The waiter is allocated on our stack, manipulated by the requeue

	 * code while we sleep on uaddr.

	/*

	 * Prepare to wait on uaddr. On success, it holds hb->lock and q

	 * is initialized.

	/*

	 * The check above which compares uaddrs is not sufficient for

	 * shared futexes. We need to compare the keys:

 Queue the futex_q, drop the hb lock, wait for wakeup. */

 The waiter is still on uaddr1 */

 The requeue acquired the lock */

			/*

			 * Drop the reference to the pi state which the

			 * requeue_pi() code acquired for us.

			/*

			 * Adjust the return value. It's either -EFAULT or

			 * success (1) but the caller expects 0 for success.

 Requeue completed. Current is 'pi_blocked_on' the rtmutex */

 Current is not longer pi_blocked_on */

		/*

		 * Fixup the pi_state owner and possibly acquire the lock if we

		 * haven't already.

		/*

		 * If fixup_pi_owner() returned an error, propagate that.  If it

		 * acquired the lock, clear -ETIMEDOUT or -EINTR.

			/*

			 * We've already been requeued, but cannot restart

			 * by calling futex_lock_pi() directly. We could

			 * restart this syscall, but it would detect that

			 * the user space "val" changed and return

			 * -EWOULDBLOCK.  Save the overhead of the restart

			 * and return -EWOULDBLOCK directly.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Fast Userspace Mutexes (which I call "Futexes!").

 *  (C) Rusty Russell, IBM 2002

 *

 *  Generalized futexes, futex requeueing, misc fixes by Ingo Molnar

 *  (C) Copyright 2003 Red Hat Inc, All Rights Reserved

 *

 *  Removed page pinning, fix privately mapped COW pages and other cleanups

 *  (C) Copyright 2003, 2004 Jamie Lokier

 *

 *  Robust futex support started by Ingo Molnar

 *  (C) Copyright 2006 Red Hat Inc, All Rights Reserved

 *  Thanks to Thomas Gleixner for suggestions, analysis and fixes.

 *

 *  PI-futex support started by Ingo Molnar and Thomas Gleixner

 *  Copyright (C) 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *  Copyright (C) 2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>

 *

 *  PRIVATE futexes by Eric Dumazet

 *  Copyright (C) 2007 Eric Dumazet <dada1@cosmosbay.com>

 *

 *  Requeue-PI support by Darren Hart <dvhltc@us.ibm.com>

 *  Copyright (C) IBM Corporation, 2009

 *  Thanks to Thomas Gleixner for conceptual design and careful reviews.

 *

 *  Thanks to Ben LaHaise for yelling "hashed waitqueues" loudly

 *  enough at me, Linus for the original (flawed) idea, Matthew

 *  Kirkwood for proof-of-concept implementation.

 *

 *  "The futexes are also cursed."

 *  "But they come in a choice of three flavours!"

/*

 * The base of the bucket array and its size are always used together

 * (after initialization only in futex_hash()), so ensure that they

 * reside in the same cacheline.

/*

 * Fault injections for futexes.

 CONFIG_FAULT_INJECTION_DEBUG_FS */

 CONFIG_FAIL_FUTEX */

/**

 * futex_hash - Return the hash bucket in the global hash

 * @key:	Pointer to the futex key for which the hash is calculated

 *

 * We hash on the keys returned from get_futex_key (see below) and return the

 * corresponding hash bucket in the global hash.

/**

 * futex_setup_timer - set up the sleeping hrtimer.

 * @time:	ptr to the given timeout value

 * @timeout:	the hrtimer_sleeper structure to be set up

 * @flags:	futex flags

 * @range_ns:	optional range in ns

 *

 * Return: Initialized hrtimer_sleeper structure or NULL if no timeout

 *	   value given

	/*

	 * If range_ns is 0, calling hrtimer_set_expires_range_ns() is

	 * effectively the same as calling hrtimer_set_expires().

/*

 * Generate a machine wide unique identifier for this inode.

 *

 * This relies on u64 not wrapping in the life-time of the machine; which with

 * 1ns resolution means almost 585 years.

 *

 * This further relies on the fact that a well formed program will not unmap

 * the file while it has a (shared) futex waiting on it. This mapping will have

 * a file reference which pins the mount and inode.

 *

 * If for some reason an inode gets evicted and read back in again, it will get

 * a new sequence number and will _NOT_ match, even though it is the exact same

 * file.

 *

 * It is important that futex_match() will never have a false-positive, esp.

 * for PI futexes that can mess up the state. The above argues that false-negatives

 * are only possible for malformed programs.

 Does the inode already have a sequence number? */

/**

 * get_futex_key() - Get parameters which are the keys for a futex

 * @uaddr:	virtual address of the futex

 * @fshared:	false for a PROCESS_PRIVATE futex, true for PROCESS_SHARED

 * @key:	address where result is stored.

 * @rw:		mapping needs to be read/write (values: FUTEX_READ,

 *              FUTEX_WRITE)

 *

 * Return: a negative error code or 0

 *

 * The key words are stored in @key on success.

 *

 * For shared mappings (when @fshared), the key is:

 *

 *   ( inode->i_sequence, page->index, offset_within_page )

 *

 * [ also see get_inode_sequence_number() ]

 *

 * For private mappings (or when !@fshared), the key is:

 *

 *   ( current->mm, address, 0 )

 *

 * This allows (cross process, where applicable) identification of the futex

 * without keeping the page pinned for the duration of the FUTEX_WAIT.

 *

 * lock_page() might sleep, the caller should not hold a spinlock.

	/*

	 * The futex address must be "naturally" aligned.

	/*

	 * PROCESS_PRIVATE futexes are fast.

	 * As the mm cannot disappear under us and the 'key' only needs

	 * virtual address, we dont even have to find the underlying vma.

	 * Note : We do have to check 'uaddr' is a valid user address,

	 *        but access_ok() should be faster than find_vma()

 Ignore any VERIFY_READ mapping (futex common case) */

	/*

	 * If write access is not required (eg. FUTEX_WAIT), try

	 * and get read-only access.

	/*

	 * The treatment of mapping from this point on is critical. The page

	 * lock protects many things but in this context the page lock

	 * stabilizes mapping, prevents inode freeing in the shared

	 * file-backed region case and guards against movement to swap cache.

	 *

	 * Strictly speaking the page lock is not needed in all cases being

	 * considered here and page lock forces unnecessarily serialization

	 * From this point on, mapping will be re-verified if necessary and

	 * page lock will be acquired only if it is unavoidable

	 *

	 * Mapping checks require the head page for any compound page so the

	 * head page and mapping is looked up now. For anonymous pages, it

	 * does not matter if the page splits in the future as the key is

	 * based on the address. For filesystem-backed pages, the tail is

	 * required as the index of the page determines the key. For

	 * base pages, there is no tail page and tail == page.

	/*

	 * If page->mapping is NULL, then it cannot be a PageAnon

	 * page; but it might be the ZERO_PAGE or in the gate area or

	 * in a special mapping (all cases which we are happy to fail);

	 * or it may have been a good file page when get_user_pages_fast

	 * found it, but truncated or holepunched or subjected to

	 * invalidate_complete_page2 before we got the page lock (also

	 * cases which we are happy to fail).  And we hold a reference,

	 * so refcount care in invalidate_complete_page's remove_mapping

	 * prevents drop_caches from setting mapping to NULL beneath us.

	 *

	 * The case we do have to guard against is when memory pressure made

	 * shmem_writepage move it from filecache to swapcache beneath us:

	 * an unlikely race, but we do need to retry for page->mapping.

		/*

		 * Page lock is required to identify which special case above

		 * applies. If this is really a shmem page then the page lock

		 * will prevent unexpected transitions.

	/*

	 * Private mappings are handled in a simple way.

	 *

	 * If the futex key is stored on an anonymous page, then the associated

	 * object is the mm which is implicitly pinned by the calling process.

	 *

	 * NOTE: When userspace waits on a MAP_SHARED mapping, even if

	 * it's a read-only handle, it's expected that futexes attach to

	 * the object not the particular process.

		/*

		 * A RO anonymous page will never change and thus doesn't make

		 * sense for futex operations.

 ref taken on mm */

		/*

		 * The associated futex object in this case is the inode and

		 * the page->mapping must be traversed. Ordinarily this should

		 * be stabilised under page lock but it's not strictly

		 * necessary in this case as we just want to pin the inode, not

		 * update the radix tree or anything like that.

		 *

		 * The RCU read lock is taken as the inode is finally freed

		 * under RCU. If the mapping still matches expectations then the

		 * mapping->host can be safely accessed as being a valid inode.

 inode-based key */

/**

 * fault_in_user_writeable() - Fault in user address and verify RW access

 * @uaddr:	pointer to faulting user space address

 *

 * Slow path to fixup the fault we just took in the atomic write

 * access to @uaddr.

 *

 * We have no generic implementation of a non-destructive write to the

 * user address. We know that we faulted in the atomic pagefault

 * disabled section so we can as well avoid the #PF overhead by

 * calling get_user_pages() right away.

/**

 * futex_top_waiter() - Return the highest priority waiter on a futex

 * @hb:		the hash bucket the futex_q's reside in

 * @key:	the futex key (to distinguish it from other futex futex_q's)

 *

 * Must be called with the hb lock held.

/**

 * wait_for_owner_exiting - Block until the owner has exited

 * @ret: owner's current futex lock status

 * @exiting:	Pointer to the exiting task

 *

 * Caller must hold a refcount on @exiting.

	/*

	 * No point in doing state checking here. If the waiter got here

	 * while the task was in exec()->exec_futex_release() then it can

	 * have any FUTEX_STATE_* value when the waiter has acquired the

	 * mutex. OK, if running, EXITING or DEAD if it reached exit()

	 * already. Highly unlikely and not a problem. Just one more round

	 * through the futex maze.

/**

 * __futex_unqueue() - Remove the futex_q from its futex_hash_bucket

 * @q:	The futex_q to unqueue

 *

 * The q->lock_ptr must not be NULL and must be held by the caller.

 The key must be already stored in q->key. */

	/*

	 * Increment the counter before taking the lock so that

	 * a potential waker won't miss a to-be-slept task that is

	 * waiting for the spinlock. This is safe as all futex_q_lock()

	 * users end up calling futex_queue(). Similarly, for housekeeping,

	 * decrement the counter at futex_q_unlock() when some error has

	 * occurred and we don't end up adding the task to the list.

 implies smp_mb(); (A) */

	/*

	 * The priority used to register this element is

	 * - either the real thread-priority for the real-time threads

	 * (i.e. threads with a priority lower than MAX_RT_PRIO)

	 * - or MAX_RT_PRIO for non-RT threads.

	 * Thus, all RT-threads are woken first in priority order, and

	 * the others are woken last, in FIFO order.

/**

 * futex_unqueue() - Remove the futex_q from its futex_hash_bucket

 * @q:	The futex_q to unqueue

 *

 * The q->lock_ptr must not be held by the caller. A call to futex_unqueue() must

 * be paired with exactly one earlier call to futex_queue().

 *

 * Return:

 *  - 1 - if the futex_q was still queued (and we removed unqueued it);

 *  - 0 - if the futex_q was already removed by the waking thread

 In the common case we don't take the spinlock, which is nice. */

	/*

	 * q->lock_ptr can change between this read and the following spin_lock.

	 * Use READ_ONCE to forbid the compiler from reloading q->lock_ptr and

	 * optimizing lock_ptr out of the logic below.

		/*

		 * q->lock_ptr can change between reading it and

		 * spin_lock(), causing us to take the wrong lock.  This

		 * corrects the race condition.

		 *

		 * Reasoning goes like this: if we have the wrong lock,

		 * q->lock_ptr must have changed (maybe several times)

		 * between reading it and the spin_lock().  It can

		 * change again after the spin_lock() but only if it was

		 * already changed before the spin_lock().  It cannot,

		 * however, change back to the original value.  Therefore

		 * we can detect whether we acquired the correct lock.

/*

 * PI futexes can not be requeued and must remove themselves from the

 * hash bucket. The hash bucket lock (i.e. lock_ptr) is held.

 Constants for the pending_op argument of handle_futex_death */

/*

 * Process a futex-list entry, check whether it's owned by the

 * dying task, and do notification if so:

 Futex address must be 32bit aligned */

	/*

	 * Special case for regular (non PI) futexes. The unlock path in

	 * user space has two race scenarios:

	 *

	 * 1. The unlock path releases the user space futex value and

	 *    before it can execute the futex() syscall to wake up

	 *    waiters it is killed.

	 *

	 * 2. A woken up waiter is killed before it can acquire the

	 *    futex in user space.

	 *

	 * In both cases the TID validation below prevents a wakeup of

	 * potential waiters which can cause these waiters to block

	 * forever.

	 *

	 * In both cases the following conditions are met:

	 *

	 *	1) task->robust_list->list_op_pending != NULL

	 *	   @pending_op == true

	 *	2) User space futex value == 0

	 *	3) Regular futex: @pi == false

	 *

	 * If these conditions are met, it is safe to attempt waking up a

	 * potential waiter without touching the user space futex value and

	 * trying to set the OWNER_DIED bit. The user space futex value is

	 * uncontended and the rest of the user space mutex state is

	 * consistent, so a woken waiter will just take over the

	 * uncontended futex. Setting the OWNER_DIED bit would create

	 * inconsistent state and malfunction of the user space owner died

	 * handling.

	/*

	 * Ok, this dying thread is truly holding a futex

	 * of interest. Set the OWNER_DIED bit atomically

	 * via cmpxchg, and if the value had FUTEX_WAITERS

	 * set, wake up a waiter (if any). (We have to do a

	 * futex_wake() even if OWNER_DIED is already set -

	 * to handle the rare but possible case of recursive

	 * thread-death.) The rest of the cleanup is done in

	 * userspace.

	/*

	 * We are not holding a lock here, but we want to have

	 * the pagefault_disable/enable() protection because

	 * we want to handle the fault gracefully. If the

	 * access fails we try to fault in the futex with R/W

	 * verification via get_user_pages. get_user() above

	 * does not guarantee R/W access. If that fails we

	 * give up and leave the futex locked.

	/*

	 * Wake robust non-PI futexes here. The wakeup of

	 * PI futexes happens in exit_pi_state():

/*

 * Fetch a robust-list pointer. Bit 0 signals PI futexes:

/*

 * Walk curr->robust_list (very carefully, it's a userspace list!)

 * and mark any locks found there dead, and notify any waiters.

 *

 * We silently return on any sign of list-walking problem.

	/*

	 * Fetch the list head (which was registered earlier, via

	 * sys_set_robust_list()):

	/*

	 * Fetch the relative futex offset:

	/*

	 * Fetch any possibly pending lock-add first, and handle it

	 * if it exists:

 avoid warning with gcc */

		/*

		 * Fetch the next entry in the list before calling

		 * handle_futex_death:

		/*

		 * A pending lock might already be on the list, so

		 * don't process it twice:

		/*

		 * Avoid excessively long or circular lists:

/*

 * Fetch a robust-list pointer. Bit 0 signals PI futexes:

/*

 * Walk curr->robust_list (very carefully, it's a userspace list!)

 * and mark any locks found there dead, and notify any waiters.

 *

 * We silently return on any sign of list-walking problem.

	/*

	 * Fetch the list head (which was registered earlier, via

	 * sys_set_robust_list()):

	/*

	 * Fetch the relative futex offset:

	/*

	 * Fetch any possibly pending lock-add first, and handle it

	 * if it exists:

 avoid warning with gcc */

		/*

		 * Fetch the next entry in the list before calling

		 * handle_futex_death:

		/*

		 * A pending lock might already be on the list, so

		 * dont process it twice:

		/*

		 * Avoid excessively long or circular lists:

/*

 * This task is holding PI mutexes at exit time => bad.

 * Kernel cleans up PI-state, but userspace is likely hosed.

 * (Robust-futex cleanup is separate and might save the day for userspace.)

	/*

	 * We are a ZOMBIE and nobody can enqueue itself on

	 * pi_state_list anymore, but we have to be careful

	 * versus waiters unqueueing themselves:

		/*

		 * We can race against put_pi_state() removing itself from the

		 * list (a waiter going away). put_pi_state() will first

		 * decrement the reference count and then modify the list, so

		 * its possible to see the list entry but fail this reference

		 * acquire.

		 *

		 * In that case; drop the locks to let put_pi_state() make

		 * progress and retry the loop.

		/*

		 * We dropped the pi-lock, so re-check whether this

		 * task still owns the PI-state:

 retain curr->pi_lock for the loop invariant */

/**

 * futex_exit_recursive - Set the tasks futex state to FUTEX_STATE_DEAD

 * @tsk:	task to set the state on

 *

 * Set the futex exit state of the task lockless. The futex waiter code

 * observes that state when a task is exiting and loops until the task has

 * actually finished the futex cleanup. The worst case for this is that the

 * waiter runs through the wait loop until the state becomes visible.

 *

 * This is called from the recursive fault handling path in do_exit().

 *

 * This is best effort. Either the futex exit code has run already or

 * not. If the OWNER_DIED bit has been set on the futex then the waiter can

 * take it over. If not, the problem is pushed back to user space. If the

 * futex exit code did not run yet, then an already queued waiter might

 * block forever, but there is nothing which can be done about that.

 If the state is FUTEX_STATE_EXITING then futex_exit_mutex is held */

	/*

	 * Prevent various race issues against a concurrent incoming waiter

	 * including live locks by forcing the waiter to block on

	 * tsk->futex_exit_mutex when it observes FUTEX_STATE_EXITING in

	 * attach_to_pi_owner().

	/*

	 * Switch the state to FUTEX_STATE_EXITING under tsk->pi_lock.

	 *

	 * This ensures that all subsequent checks of tsk->futex_state in

	 * attach_to_pi_owner() must observe FUTEX_STATE_EXITING with

	 * tsk->pi_lock held.

	 *

	 * It guarantees also that a pi_state which was queued right before

	 * the state change under tsk->pi_lock by a concurrent waiter must

	 * be observed in exit_pi_state_list().

	/*

	 * Lockless store. The only side effect is that an observer might

	 * take another loop until it becomes visible.

	/*

	 * Drop the exit protection. This unblocks waiters which observed

	 * FUTEX_STATE_EXITING to reevaluate the state.

	/*

	 * The state handling is done for consistency, but in the case of

	 * exec() there is no way to prevent further damage as the PID stays

	 * the same. But for the unlikely and arguably buggy case that a

	 * futex is held on exec(), this provides at least as much state

	 * consistency protection which is possible.

	/*

	 * Reset the state to FUTEX_STATE_OK. The task is alive and about

	 * exec a new binary.

	/*

	 * This will fail and we want it. Some arch implementations do

	 * runtime detection of the futex_atomic_cmpxchg_inatomic()

	 * functionality. We want to know that before we call in any

	 * of the complex code paths. Also we want to prevent

	 * registration of robust lists in that case. NULL is

	 * guaranteed to fault and we get -EFAULT on functional

	 * implementation, the non-functional ones will return

	 * -ENOSYS.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * PI code:

 pi_mutex gets initialized later */

/*

 * Drops a reference to the pi_state object and frees or caches it

 * when the last reference is gone.

	/*

	 * If pi_state->owner is NULL, the owner is most probably dying

	 * and has cleaned up the pi_state already

		/*

		 * pi_state->list is already empty.

		 * clear pi_state->owner.

		 * refcount is at 0 - put it back to 1.

/*

 * We need to check the following states:

 *

 *      Waiter | pi_state | pi->owner | uTID      | uODIED | ?

 *

 * [1]  NULL   | ---      | ---       | 0         | 0/1    | Valid

 * [2]  NULL   | ---      | ---       | >0        | 0/1    | Valid

 *

 * [3]  Found  | NULL     | --        | Any       | 0/1    | Invalid

 *

 * [4]  Found  | Found    | NULL      | 0         | 1      | Valid

 * [5]  Found  | Found    | NULL      | >0        | 1      | Invalid

 *

 * [6]  Found  | Found    | task      | 0         | 1      | Valid

 *

 * [7]  Found  | Found    | NULL      | Any       | 0      | Invalid

 *

 * [8]  Found  | Found    | task      | ==taskTID | 0/1    | Valid

 * [9]  Found  | Found    | task      | 0         | 0      | Invalid

 * [10] Found  | Found    | task      | !=taskTID | 0/1    | Invalid

 *

 * [1]	Indicates that the kernel can acquire the futex atomically. We

 *	came here due to a stale FUTEX_WAITERS/FUTEX_OWNER_DIED bit.

 *

 * [2]	Valid, if TID does not belong to a kernel thread. If no matching

 *      thread is found then it indicates that the owner TID has died.

 *

 * [3]	Invalid. The waiter is queued on a non PI futex

 *

 * [4]	Valid state after exit_robust_list(), which sets the user space

 *	value to FUTEX_WAITERS | FUTEX_OWNER_DIED.

 *

 * [5]	The user space value got manipulated between exit_robust_list()

 *	and exit_pi_state_list()

 *

 * [6]	Valid state after exit_pi_state_list() which sets the new owner in

 *	the pi_state but cannot access the user space value.

 *

 * [7]	pi_state->owner can only be NULL when the OWNER_DIED bit is set.

 *

 * [8]	Owner and user space value match

 *

 * [9]	There is no transient state which sets the user space TID to 0

 *	except exit_robust_list(), but this is indicated by the

 *	FUTEX_OWNER_DIED bit. See [4]

 *

 * [10] There is no transient state which leaves owner and user space

 *	TID out of sync. Except one error case where the kernel is denied

 *	write access to the user address, see fixup_pi_state_owner().

 *

 *

 * Serialization and lifetime rules:

 *

 * hb->lock:

 *

 *	hb -> futex_q, relation

 *	futex_q -> pi_state, relation

 *

 *	(cannot be raw because hb can contain arbitrary amount

 *	 of futex_q's)

 *

 * pi_mutex->wait_lock:

 *

 *	{uval, pi_state}

 *

 *	(and pi_mutex 'obviously')

 *

 * p->pi_lock:

 *

 *	p->pi_state_list -> pi_state->list, relation

 *	pi_mutex->owner -> pi_state->owner, relation

 *

 * pi_state->refcount:

 *

 *	pi_state lifetime

 *

 *

 * Lock order:

 *

 *   hb->lock

 *     pi_mutex->wait_lock

 *       p->pi_lock

 *

/*

 * Validate that the existing waiter has a pi_state and sanity check

 * the pi_state against the user space value. If correct, attach to

 * it.

	/*

	 * Userspace might have messed up non-PI and PI futexes [3]

	/*

	 * We get here with hb->lock held, and having found a

	 * futex_top_waiter(). This means that futex_lock_pi() of said futex_q

	 * has dropped the hb->lock in between futex_queue() and futex_unqueue_pi(),

	 * which in turn means that futex_lock_pi() still has a reference on

	 * our pi_state.

	 *

	 * The waiter holding a reference on @pi_state also protects against

	 * the unlocked put_pi_state() in futex_unlock_pi(), futex_lock_pi()

	 * and futex_wait_requeue_pi() as it cannot go to 0 and consequently

	 * free pi_state before we can take a reference ourselves.

	/*

	 * Now that we have a pi_state, we can acquire wait_lock

	 * and do the state validation.

	/*

	 * Since {uval, pi_state} is serialized by wait_lock, and our current

	 * uval was read without holding it, it can have changed. Verify it

	 * still is what we expect it to be, otherwise retry the entire

	 * operation.

	/*

	 * Handle the owner died case:

		/*

		 * exit_pi_state_list sets owner to NULL and wakes the

		 * topmost waiter. The task which acquires the

		 * pi_state->rt_mutex will fixup owner.

			/*

			 * No pi state owner, but the user space TID

			 * is not 0. Inconsistent state. [5]

			/*

			 * Take a ref on the state and return success. [4]

		/*

		 * If TID is 0, then either the dying owner has not

		 * yet executed exit_pi_state_list() or some waiter

		 * acquired the rtmutex in the pi state, but did not

		 * yet fixup the TID in user space.

		 *

		 * Take a ref on the state and return success. [6]

		/*

		 * If the owner died bit is not set, then the pi_state

		 * must have an owner. [7]

	/*

	 * Bail out if user space manipulated the futex value. If pi

	 * state exists then the owner TID must be the same as the

	 * user space TID. [9/10]

	/*

	 * If the futex exit state is not yet FUTEX_STATE_DEAD, tell the

	 * caller that the alleged owner is busy.

	/*

	 * Reread the user space value to handle the following situation:

	 *

	 * CPU0				CPU1

	 *

	 * sys_exit()			sys_futex()

	 *  do_exit()			 futex_lock_pi()

	 *                                futex_lock_pi_atomic()

	 *   exit_signals(tsk)		    No waiters:

	 *    tsk->flags |= PF_EXITING;	    *uaddr == 0x00000PID

	 *  mm_release(tsk)		    Set waiter bit

	 *   exit_robust_list(tsk) {	    *uaddr = 0x80000PID;

	 *      Set owner died		    attach_to_pi_owner() {

	 *    *uaddr = 0xC0000000;	     tsk = get_task(PID);

	 *   }				     if (!tsk->flags & PF_EXITING) {

	 *  ...				       attach();

	 *  tsk->futex_state =               } else {

	 *	FUTEX_STATE_DEAD;              if (tsk->futex_state !=

	 *					  FUTEX_STATE_DEAD)

	 *				         return -EAGAIN;

	 *				       return -ESRCH; <--- FAIL

	 *				     }

	 *

	 * Returning ESRCH unconditionally is wrong here because the

	 * user space value has been changed by the exiting task.

	 *

	 * The same logic applies to the case where the exiting task is

	 * already gone.

 If the user space value has changed, try again. */

	/*

	 * The exiting task did not have a robust list, the robust list was

	 * corrupted or the user space value in *uaddr is simply bogus.

	 * Give up and tell user space.

	/*

	 * No existing pi state. First waiter. [2]

	 *

	 * This creates pi_state, we have hb->lock held, this means nothing can

	 * observe this state, wait_lock is irrelevant.

	/*

	 * Initialize the pi_mutex in locked state and make @p

	 * the owner of it:

 Store the key for possible exit cleanups: */

	/*

	 * Assignment without holding pi_state->pi_mutex.wait_lock is safe

	 * because there is no concurrency as the object is not published yet.

/*

 * Lookup the task for the TID provided from user space and attach to

 * it after doing proper sanity checks.

	/*

	 * We are the first waiter - try to look up the real owner and attach

	 * the new pi_state to it, but bail out when TID = 0 [1]

	 *

	 * The !pid check is paranoid. None of the call sites should end up

	 * with pid == 0, but better safe than sorry. Let the caller retry

	/*

	 * We need to look at the task state to figure out, whether the

	 * task is exiting. To protect against the change of the task state

	 * in futex_exit_release(), we do this protected by p->pi_lock:

		/*

		 * The task is on the way out. When the futex state is

		 * FUTEX_STATE_DEAD, we know that the task has finished

		 * the cleanup:

		/*

		 * If the owner task is between FUTEX_STATE_EXITING and

		 * FUTEX_STATE_DEAD then store the task pointer and keep

		 * the reference on the task struct. The calling code will

		 * drop all locks, wait for the task to reach

		 * FUTEX_STATE_DEAD and then drop the refcount. This is

		 * required to prevent a live lock when the current task

		 * preempted the exiting task between the two states.

 If user space value changed, let the caller retry */

/**

 * futex_lock_pi_atomic() - Atomic work required to acquire a pi aware futex

 * @uaddr:		the pi futex user address

 * @hb:			the pi futex hash bucket

 * @key:		the futex key associated with uaddr and hb

 * @ps:			the pi_state pointer where we store the result of the

 *			lookup

 * @task:		the task to perform the atomic lock work for.  This will

 *			be "current" except in the case of requeue pi.

 * @exiting:		Pointer to store the task pointer of the owner task

 *			which is in the middle of exiting

 * @set_waiters:	force setting the FUTEX_WAITERS bit (1) or not (0)

 *

 * Return:

 *  -  0 - ready to wait;

 *  -  1 - acquired the lock;

 *  - <0 - error

 *

 * The hb->lock must be held by the caller.

 *

 * @exiting is only set when the return value is -EBUSY. If so, this holds

 * a refcount on the exiting task on return and the caller needs to drop it

 * after waiting for the exit to complete.

	/*

	 * Read the user space value first so we can validate a few

	 * things before proceeding further.

	/*

	 * Detect deadlocks.

	/*

	 * Lookup existing state first. If it exists, try to attach to

	 * its pi_state.

	/*

	 * No waiter and user TID is 0. We are here because the

	 * waiters or the owner died bit is set or called from

	 * requeue_cmp_pi or for whatever reason something took the

	 * syscall.

		/*

		 * We take over the futex. No other waiters and the user space

		 * TID is 0. We preserve the owner died bit.

 The futex requeue_pi code can enforce the waiters bit */

		/*

		 * If the waiter bit was requested the caller also needs PI

		 * state attached to the new owner of the user space futex.

		 *

		 * @task is guaranteed to be alive and it cannot be exiting

		 * because it is either sleeping or waiting in

		 * futex_requeue_pi_wakeup_sync().

		 *

		 * No need to do the full attach_to_pi_owner() exercise

		 * because @task is known and valid.

	/*

	 * First waiter. Set the waiters bit before attaching ourself to

	 * the owner. If owner tries to unlock, it will be forced into

	 * the kernel and blocked on hb->lock.

	/*

	 * If the update of the user space value succeeded, we try to

	 * attach to the owner. If that fails, no harm done, we only

	 * set the FUTEX_WAITERS bit in the user space variable.

/*

 * Caller must hold a reference on @pi_state.

		/*

		 * As per the comment in futex_unlock_pi() this should not happen.

		 *

		 * When this happens, give up our locks and try again, giving

		 * the futex_lock_pi() instance time to complete, either by

		 * waiting on the rtmutex or removing itself from the futex

		 * queue.

	/*

	 * We pass it to the next owner. The WAITERS bit is always kept

	 * enabled while there is PI state around. We cleanup the owner

	 * died bit, because we are the owner.

		/*

		 * If a unconditional UNLOCK_PI operation (user space did not

		 * try the TID->0 transition) raced with a waiter setting the

		 * FUTEX_WAITERS flag between get_user() and locking the hash

		 * bucket lock, retry the operation.

		/*

		 * This is a point of no return; once we modified the uval

		 * there is no going back and subsequent operations must

		 * not fail.

	/*

	 * We are here because either:

	 *

	 *  - we stole the lock and pi_state->owner needs updating to reflect

	 *    that (@argowner == current),

	 *

	 * or:

	 *

	 *  - someone stole our lock and we need to fix things to point to the

	 *    new owner (@argowner == NULL).

	 *

	 * Either way, we have to replace the TID in the user space variable.

	 * This must be atomic as we have to preserve the owner died bit here.

	 *

	 * Note: We write the user space value _before_ changing the pi_state

	 * because we can fault here. Imagine swapped out pages or a fork

	 * that marked all the anonymous memory readonly for cow.

	 *

	 * Modifying pi_state _before_ the user space value would leave the

	 * pi_state in an inconsistent state when we fault here, because we

	 * need to drop the locks to handle the fault. This might be observed

	 * in the PID checks when attaching to PI state .

			/*

			 * We raced against a concurrent self; things are

			 * already fixed up. Nothing to do.

 We got the lock. pi_state is correct. Tell caller. */

		/*

		 * The trylock just failed, so either there is an owner or

		 * there is a higher priority waiter than this one.

		/*

		 * If the higher priority waiter has not yet taken over the

		 * rtmutex then newowner is NULL. We can't return here with

		 * that state because it's inconsistent vs. the user space

		 * state. So drop the locks and try again. It's a valid

		 * situation and not any different from the other retry

		 * conditions.

			/*

			 * We raced against a concurrent self; things are

			 * already fixed up. Nothing to do.

 Owner died? */

	/*

	 * We fixed up user space. Now we need to fix the pi_state

	 * itself.

	/*

	 * In order to reschedule or handle a page fault, we need to drop the

	 * locks here. In the case of a fault, this gives the other task

	 * (either the highest priority waiter itself or the task which stole

	 * the rtmutex) the chance to try the fixup of the pi_state. So once we

	 * are back from handling the fault we need to check the pi_state after

	 * reacquiring the locks and before trying to do another fixup. When

	 * the fixup has been done already we simply return.

	 *

	 * Note: we hold both hb->lock and pi_mutex->wait_lock. We can safely

	 * drop hb->lock since the caller owns the hb -> futex_q relation.

	 * Dropping the pi_mutex->wait_lock requires the state revalidate.

	/*

	 * Check if someone else fixed it for us:

 Retry if err was -EAGAIN or the fault in succeeded */

	/*

	 * fault_in_user_writeable() failed so user state is immutable. At

	 * best we can make the kernel state consistent but user state will

	 * be most likely hosed and any subsequent unlock operation will be

	 * rejected due to PI futex rule [10].

	 *

	 * Ensure that the rtmutex owner is also the pi_state owner despite

	 * the user space value claiming something different. There is no

	 * point in unlocking the rtmutex if current is the owner as it

	 * would need to wait until the next waiter has taken the rtmutex

	 * to guarantee consistent state. Keep it simple. Userspace asked

	 * for this wreckaged state.

	 *

	 * The rtmutex has an owner - either current or some other

	 * task. See the EAGAIN loop above.

/**

 * fixup_pi_owner() - Post lock pi_state and corner case management

 * @uaddr:	user address of the futex

 * @q:		futex_q (contains pi_state and access to the rt_mutex)

 * @locked:	if the attempt to take the rt_mutex succeeded (1) or not (0)

 *

 * After attempting to lock an rt_mutex, this function is called to cleanup

 * the pi_state owner as well as handle race conditions that may allow us to

 * acquire the lock. Must be called with the hb lock held.

 *

 * Return:

 *  -  1 - success, lock taken;

 *  -  0 - success, lock not taken;

 *  - <0 - on error (-EFAULT)

		/*

		 * Got the lock. We might not be the anticipated owner if we

		 * did a lock-steal - fix up the PI-state in that case:

		 *

		 * Speculative pi_state->owner read (we don't hold wait_lock);

		 * since we own the lock pi_state->owner == current is the

		 * stable state, anything else needs more attention.

	/*

	 * If we didn't get the lock; check if anybody stole it from us. In

	 * that case, we need to fix up the uval to point to them instead of

	 * us, otherwise bad things happen. [10]

	 *

	 * Another speculative read; pi_state->owner == current is unstable

	 * but needs our attention.

	/*

	 * Paranoia check. If we did not take the lock, then we should not be

	 * the owner of the rt_mutex. Warn and establish consistent state.

/*

 * Userspace tried a 0 -> TID atomic transition of the futex value

 * and failed. The kernel side here does the whole locking operation:

 * if there are waiters then it will block as a consequence of relying

 * on rt-mutexes, it does PI, etc. (Due to races the kernel might see

 * a 0 value of the futex too.).

 *

 * Also serves as futex trylock_pi()'ing, and due semantics.

		/*

		 * Atomic work succeeded and we got the lock,

		 * or failed. Either way, we do _not_ block.

 We got the lock. */

			/*

			 * Two reasons for this:

			 * - EBUSY: Task is exiting and we just wait for the

			 *   exit to complete.

			 * - EAGAIN: The user space value changed.

			/*

			 * Handle the case where the owner is in the middle of

			 * exiting. Wait for the exit to complete otherwise

			 * this task might loop forever, aka. live lock.

	/*

	 * Only actually queue now that the atomic ops are done:

 Fixup the trylock return value: */

	/*

	 * On PREEMPT_RT_FULL, when hb->lock becomes an rt_mutex, we must not

	 * hold it while doing rt_mutex_start_proxy(), because then it will

	 * include hb->lock in the blocking chain, even through we'll not in

	 * fact hold it while blocking. This will lead it to report -EDEADLK

	 * and BUG when futex_unlock_pi() interleaves with this.

	 *

	 * Therefore acquire wait_lock while holding hb->lock, but drop the

	 * latter before calling __rt_mutex_start_proxy_lock(). This

	 * interleaves with futex_unlock_pi() -- which does a similar lock

	 * handoff -- such that the latter can observe the futex_q::pi_state

	 * before __rt_mutex_start_proxy_lock() is done.

	/*

	 * __rt_mutex_start_proxy_lock() unconditionally enqueues the @rt_waiter

	 * such that futex_unlock_pi() is guaranteed to observe the waiter when

	 * it sees the futex_q::pi_state.

	/*

	 * If we failed to acquire the lock (deadlock/signal/timeout), we must

	 * first acquire the hb->lock before removing the lock from the

	 * rt_mutex waitqueue, such that we can keep the hb and rt_mutex wait

	 * lists consistent.

	 *

	 * In particular; it is important that futex_unlock_pi() can not

	 * observe this inconsistency.

	/*

	 * Fixup the pi_state owner and possibly acquire the lock if we

	 * haven't already.

	/*

	 * If fixup_pi_owner() returned an error, propagate that.  If it acquired

	 * the lock, clear our -ETIMEDOUT or -EINTR.

/*

 * Userspace attempted a TID -> 0 atomic transition, and failed.

 * This is the in-kernel slowpath: we look up the PI state (if any),

 * and do the rt-mutex unlock.

	/*

	 * We release only a lock we actually own:

	/*

	 * Check waiters first. We do not trust user space values at

	 * all and we at least want to know if user space fiddled

	 * with the futex value instead of blindly unlocking.

		/*

		 * If current does not own the pi_state then the futex is

		 * inconsistent and user space fiddled with the futex value.

		/*

		 * By taking wait_lock while still holding hb->lock, we ensure

		 * there is no point where we hold neither; and therefore

		 * wake_futex_p() must observe a state consistent with what we

		 * observed.

		 *

		 * In particular; this forces __rt_mutex_start_proxy() to

		 * complete such that we're guaranteed to observe the

		 * rt_waiter. Also see the WARN in wake_futex_pi().

 drops pi_state->pi_mutex.wait_lock */

		/*

		 * Success, we're done! No tricky corner cases.

		/*

		 * The atomic access to the futex value generated a

		 * pagefault, so retry the user-access and the wakeup:

		/*

		 * A unconditional UNLOCK_PI op raced against a waiter

		 * setting the FUTEX_WAITERS bit. Try again.

		/*

		 * wake_futex_pi has detected invalid state. Tell user

		 * space.

	/*

	 * We have no kernel internal state, i.e. no waiters in the

	 * kernel. Waiters which are about to queue themselves are stuck

	 * on hb->lock. So we can safely ignore them. We do neither

	 * preserve the WAITERS bit not the OWNER_DIED one. We are the

	 * owner.

	/*

	 * If uval has changed, let user space handle it.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Process number limiting controller for cgroups.

 *

 * Used to allow a cgroup hierarchy to stop any new processes from fork()ing

 * after a certain limit is reached.

 *

 * Since it is trivial to hit the task limit without hitting any kmemcg limits

 * in place, PIDs are a fundamental resource. As such, PID exhaustion must be

 * preventable in the scope of a cgroup hierarchy by allowing resource limiting

 * of the number of tasks in a cgroup.

 *

 * In order to use the `pids` controller, set the maximum number of tasks in

 * pids.max (this is not available in the root cgroup for obvious reasons). The

 * number of processes currently in the cgroup is given by pids.current.

 * Organisational operations are not blocked by cgroup policies, so it is

 * possible to have pids.current > pids.max. However, it is not possible to

 * violate a cgroup policy through fork(). fork() will return -EAGAIN if forking

 * would cause a cgroup policy to be violated.

 *

 * To set a cgroup to have no limit, set pids.max to "max". This is the default

 * for all new cgroups (N.B. that PID limits are hierarchical, so the most

 * stringent limit in the hierarchy is followed).

 *

 * pids.current tracks all child cgroup hierarchies, so parent/pids.current is

 * a superset of parent/child/pids.current.

 *

 * Copyright (C) 2015 Aleksa Sarai <cyphar@cyphar.com>

	/*

	 * Use 64-bit types so that we can safely represent "max" as

	 * %PIDS_MAX = (%PID_MAX_LIMIT + 1).

 Handle for "pids.events" */

 Number of times fork failed because limit was hit. */

/**

 * pids_cancel - uncharge the local pid count

 * @pids: the pid cgroup state

 * @num: the number of pids to cancel

 *

 * This function will WARN if the pid count goes under 0, because such a case is

 * a bug in the pids controller proper.

	/*

	 * A negative count (or overflow for that matter) is invalid,

	 * and indicates a bug in the `pids` controller proper.

/**

 * pids_uncharge - hierarchically uncharge the pid count

 * @pids: the pid cgroup state

 * @num: the number of pids to uncharge

/**

 * pids_charge - hierarchically charge the pid count

 * @pids: the pid cgroup state

 * @num: the number of pids to charge

 *

 * This function does *not* follow the pid limit set. It cannot fail and the new

 * pid count may exceed the limit. This is only used for reverting failed

 * attaches, where there is no other way out than violating the limit.

/**

 * pids_try_charge - hierarchically try to charge the pid count

 * @pids: the pid cgroup state

 * @num: the number of pids to charge

 *

 * This function follows the set limit. It will fail if the charge would cause

 * the new value to exceed the hierarchical limit. Returns 0 if the charge

 * succeeded, otherwise -EAGAIN.

		/*

		 * Since new is capped to the maximum number of pid_t, if

		 * p->limit is %PIDS_MAX then we know that this test will never

		 * fail.

		/*

		 * No need to pin @old_css between here and cancel_attach()

		 * because cgroup core protects it from being freed before

		 * the migration completes or fails.

/*

 * task_css_check(true) in pids_can_fork() and pids_cancel_fork() relies

 * on cgroup_threadgroup_change_begin() held by the copy_process().

 Only log the first time events_limit is incremented. */

	/*

	 * Limit updates don't need to be mutex'd, since it isn't

	 * critical that any racing fork()s follow the new limit.

 terminate */

/*

 *  kernel/cpuset.c

 *

 *  Processor and Memory placement constraints for sets of tasks.

 *

 *  Copyright (C) 2003 BULL SA.

 *  Copyright (C) 2004-2007 Silicon Graphics, Inc.

 *  Copyright (C) 2006 Google, Inc

 *

 *  Portions derived from Patrick Mochel's sysfs code.

 *  sysfs is Copyright (c) 2001-3 Patrick Mochel

 *

 *  2003-10-10 Written by Simon Derr.

 *  2003-10-22 Updates by Stephen Hemminger.

 *  2004 May-July Rework by Paul Jackson.

 *  2006 Rework by Paul Menage to use generic cgroups

 *  2008 Rework of the scheduler domains and CPU hotplug handling

 *       by Max Krasnyansky

 *

 *  This file is subject to the terms and conditions of the GNU General Public

 *  License.  See the file COPYING in the main directory of the Linux

 *  distribution for more details.

/*

 * There could be abnormal cpuset configurations for cpu or memory

 * node binding, add this key to provide a quick low-cost judgement

 * of the situation.

 See "Frequency meter" comments, below. */

 unprocessed events count */

 most recent output value */

 clock (secs) when val computed */

 guards read or write of above */

 "unsigned long" so bitops work */

	/*

	 * On default hierarchy:

	 *

	 * The user-configured masks can only be changed by writing to

	 * cpuset.cpus and cpuset.mems, and won't be limited by the

	 * parent masks.

	 *

	 * The effective masks is the real masks that apply to the tasks

	 * in the cpuset. They may be changed if the configured masks are

	 * changed or hotplug happens.

	 *

	 * effective_mask == configured_mask & parent's effective_mask,

	 * and if it ends up empty, it will inherit the parent's mask.

	 *

	 *

	 * On legacy hierarchy:

	 *

	 * The user-configured masks are always the same with effective masks.

 user-configured CPUs and Memory Nodes allow to tasks */

 effective CPUs and Memory Nodes allow to tasks */

	/*

	 * CPUs allocated to child sub-partitions (default hierarchy only)

	 * - CPUs granted by the parent = effective_cpus U subparts_cpus

	 * - effective_cpus and subparts_cpus are mutually exclusive.

	 *

	 * effective_cpus contains only onlined CPUs, but subparts_cpus

	 * may have offlined ones.

	/*

	 * This is old Memory Nodes tasks took on.

	 *

	 * - top_cpuset.old_mems_allowed is initialized to mems_allowed.

	 * - A new cpuset's old_mems_allowed is initialized when some

	 *   task is moved into it.

	 * - old_mems_allowed is used in cpuset_migrate_mm() when we change

	 *   cpuset.mems_allowed and have tasks' nodemask updated, and

	 *   then old_mems_allowed is updated to mems_allowed.

 memory_pressure filter */

	/*

	 * Tasks are being attached to this cpuset.  Used to prevent

	 * zeroing cpus/mems_allowed between ->can_attach() and ->attach().

 partition number for rebuild_sched_domains() */

 for custom sched domain */

 number of CPUs in subparts_cpus */

 partition root state */

	/*

	 * Default hierarchy only:

	 * use_parent_ecpus - set if using parent's effective_cpus

	 * child_ecpus_count - # of children with use_parent_ecpus set

 Handle for cpuset.cpus.partition */

/*

 * Partition root states:

 *

 *   0 - not a partition root

 *

 *   1 - partition root

 *

 *  -1 - invalid partition root

 *       None of the cpus in cpus_allowed can be put into the parent's

 *       subparts_cpus. In this case, the cpuset is not a real partition

 *       root anymore.  However, the CPU_EXCLUSIVE bit will still be set

 *       and the cpuset can be restored back to a partition root if the

 *       parent cpuset can give more CPUs back to this child cpuset.

/*

 * Temporary cpumasks for working with partitions that are passed among

 * functions to avoid memory allocation in inner functions.

 For partition root */

 For update_cpumasks_hier() */

 Retrieve the cpuset for a task */

 bits in struct cpuset flags field */

 convenient tests for these bits */

/*

 * Send notification event of whenever partition_root_state changes.

/**

 * cpuset_for_each_child - traverse online children of a cpuset

 * @child_cs: loop cursor pointing to the current child

 * @pos_css: used for iteration

 * @parent_cs: target cpuset to walk children of

 *

 * Walk @child_cs through the online children of @parent_cs.  Must be used

 * with RCU read locked.

/**

 * cpuset_for_each_descendant_pre - pre-order walk of a cpuset's descendants

 * @des_cs: loop cursor pointing to the current descendant

 * @pos_css: used for iteration

 * @root_cs: target cpuset to walk ancestor of

 *

 * Walk @des_cs through the online descendants of @root_cs.  Must be used

 * with RCU read locked.  The caller may modify @pos_css by calling

 * css_rightmost_descendant() to skip subtree.  @root_cs is included in the

 * iteration and the first node to be visited.

/*

 * There are two global locks guarding cpuset structures - cpuset_rwsem and

 * callback_lock. We also require taking task_lock() when dereferencing a

 * task's cpuset pointer. See "The task_lock() exception", at the end of this

 * comment.  The cpuset code uses only cpuset_rwsem write lock.  Other

 * kernel subsystems can use cpuset_read_lock()/cpuset_read_unlock() to

 * prevent change to cpuset structures.

 *

 * A task must hold both locks to modify cpusets.  If a task holds

 * cpuset_rwsem, it blocks others wanting that rwsem, ensuring that it

 * is the only task able to also acquire callback_lock and be able to

 * modify cpusets.  It can perform various checks on the cpuset structure

 * first, knowing nothing will change.  It can also allocate memory while

 * just holding cpuset_rwsem.  While it is performing these checks, various

 * callback routines can briefly acquire callback_lock to query cpusets.

 * Once it is ready to make the changes, it takes callback_lock, blocking

 * everyone else.

 *

 * Calls to the kernel memory allocator can not be made while holding

 * callback_lock, as that would risk double tripping on callback_lock

 * from one of the callbacks into the cpuset code from within

 * __alloc_pages().

 *

 * If a task is only holding callback_lock, then it has read-only

 * access to cpusets.

 *

 * Now, the task_struct fields mems_allowed and mempolicy may be changed

 * by other task, we use alloc_lock in the task_struct fields to protect

 * them.

 *

 * The cpuset_common_file_read() handlers only hold callback_lock across

 * small pieces of code, such as when reading out possibly multi-word

 * cpumasks and nodemasks.

 *

 * Accessing a task's cpuset should be done in accordance with the

 * guidelines for accessing subsystem state in kernel/cgroup.c

/*

 * CPU / memory hotplug is handled asynchronously.

/*

 * Cgroup v2 behavior is used on the "cpus" and "mems" control files when

 * on default hierarchy or when the cpuset_v2_mode flag is set by mounting

 * the v1 cpuset cgroup filesystem with the "cpuset_v2_mode" mount option.

 * With v2 behavior, "cpus" and "mems" are always what the users have

 * requested and won't be changed by hotplug events. Only the effective

 * cpus or mems will be affected.

/*

 * Return in pmask the portion of a task's cpusets's cpus_allowed that

 * are online and are capable of running the task.  If none are found,

 * walk up the cpuset hierarchy until we find one that does have some

 * appropriate cpus.

 *

 * One way or another, we guarantee to return some non-empty subset

 * of cpu_online_mask.

 *

 * Call with callback_lock or cpuset_rwsem held.

			/*

			 * The top cpuset doesn't have any online cpu as a

			 * consequence of a race between cpuset_hotplug_work

			 * and cpu hotplug notifier.  But we know the top

			 * cpuset's effective_cpus is on its way to be

			 * identical to cpu_online_mask.

/*

 * Return in *pmask the portion of a cpusets's mems_allowed that

 * are online, with memory.  If none are online with memory, walk

 * up the cpuset hierarchy until we find one that does have some

 * online mems.  The top cpuset always has some mems online.

 *

 * One way or another, we guarantee to return some non-empty subset

 * of node_states[N_MEMORY].

 *

 * Call with callback_lock or cpuset_rwsem held.

/*

 * update task's spread flag if cpuset's page/slab spread flag is set

 *

 * Call with callback_lock or cpuset_rwsem held.

/*

 * is_cpuset_subset(p, q) - Is cpuset p a subset of cpuset q?

 *

 * One cpuset is a subset of another if all its allowed CPUs and

 * Memory Nodes are a subset of the other, and its exclusive flags

 * are only set if the other's are set.  Call holding cpuset_rwsem.

/**

 * alloc_cpumasks - allocate three cpumasks for cpuset

 * @cs:  the cpuset that have cpumasks to be allocated.

 * @tmp: the tmpmasks structure pointer

 * Return: 0 if successful, -ENOMEM otherwise.

 *

 * Only one of the two input arguments should be non-NULL.

/**

 * free_cpumasks - free cpumasks in a tmpmasks structure

 * @cs:  the cpuset that have cpumasks to be free.

 * @tmp: the tmpmasks structure pointer

/**

 * alloc_trial_cpuset - allocate a trial cpuset

 * @cs: the cpuset that the trial cpuset duplicates

/**

 * free_cpuset - free the cpuset

 * @cs: the cpuset to be freed

/*

 * validate_change() - Used to validate that any proposed cpuset change

 *		       follows the structural rules for cpusets.

 *

 * If we replaced the flag and mask values of the current cpuset

 * (cur) with those values in the trial cpuset (trial), would

 * our various subset and exclusive rules still be valid?  Presumes

 * cpuset_rwsem held.

 *

 * 'cur' is the address of an actual, in-use cpuset.  Operations

 * such as list traversal that depend on the actual address of the

 * cpuset in the list must use cur below, not trial.

 *

 * 'trial' is the address of bulk structure copy of cur, with

 * perhaps one or more of the fields cpus_allowed, mems_allowed,

 * or flags changed to new, trial values.

 *

 * Return 0 if valid, -errno if not.

 Each of our child cpusets must be a subset of us */

 Remaining checks don't apply to root cpuset */

 On legacy hierarchy, we must be a subset of our parent cpuset. */

	/*

	 * If either I or some sibling (!= me) is exclusive, we can't

	 * overlap

	/*

	 * Cpusets with tasks - existing or newly being attached - can't

	 * be changed to have empty cpus_allowed or mems_allowed.

	/*

	 * We can't shrink if we won't have enough room for SCHED_DEADLINE

	 * tasks.

/*

 * Helper routine for generate_sched_domains().

 * Do cpusets a, b have overlapping effective cpus_allowed masks?

 skip the whole subtree if @cp doesn't have any CPU */

 Must be called with cpuset_rwsem held.  */

 jump label reference count + the top-level cpuset */

/*

 * generate_sched_domains()

 *

 * This function builds a partial partition of the systems CPUs

 * A 'partial partition' is a set of non-overlapping subsets whose

 * union is a subset of that set.

 * The output of this function needs to be passed to kernel/sched/core.c

 * partition_sched_domains() routine, which will rebuild the scheduler's

 * load balancing domains (sched domains) as specified by that partial

 * partition.

 *

 * See "What is sched_load_balance" in Documentation/admin-guide/cgroup-v1/cpusets.rst

 * for a background explanation of this.

 *

 * Does not return errors, on the theory that the callers of this

 * routine would rather not worry about failures to rebuild sched

 * domains when operating in the severe memory shortage situations

 * that could cause allocation failures below.

 *

 * Must be called with cpuset_rwsem held.

 *

 * The three key local variables below are:

 *    cp - cpuset pointer, used (together with pos_css) to perform a

 *	   top-down scan of all cpusets. For our purposes, rebuilding

 *	   the schedulers sched domains, we can ignore !is_sched_load_

 *	   balance cpusets.

 *  csa  - (for CpuSet Array) Array of pointers to all the cpusets

 *	   that need to be load balanced, for convenient iterative

 *	   access by the subsequent code that finds the best partition,

 *	   i.e the set of domains (subsets) of CPUs such that the

 *	   cpus_allowed of every cpuset marked is_sched_load_balance

 *	   is a subset of one of these domains, while there are as

 *	   many such domains as possible, each as small as possible.

 * doms  - Conversion of 'csa' to an array of cpumasks, for passing to

 *	   the kernel/sched/core.c routine partition_sched_domains() in a

 *	   convenient format, that can be easily compared to the prior

 *	   value to determine what partition elements (sched domains)

 *	   were changed (added or removed.)

 *

 * Finding the best partition (set of domains):

 *	The triple nested loops below over i, j, k scan over the

 *	load balanced cpusets (using the array of cpuset pointers in

 *	csa[]) looking for pairs of cpusets that have overlapping

 *	cpus_allowed, but which don't have the same 'pn' partition

 *	number and gives them in the same partition number.  It keeps

 *	looping on the 'restart' label until it can no longer find

 *	any such pairs.

 *

 *	The union of the cpus_allowed masks from the set of

 *	all cpusets having the same 'pn' value then form the one

 *	element of the partition (one sched domain) to be passed to

 *	partition_sched_domains().

 top-down scan of cpusets */

 array of all cpuset ptrs */

 how many cpuset ptrs in csa so far */

 indices for partition finding loops */

 resulting partition; i.e. sched domains */

 attributes for custom domains */

 number of sched domains in result */

 next empty doms[] struct cpumask slot */

 Special case for the 99% of systems with one, full, sched domain */

		/*

		 * Continue traversing beyond @cp iff @cp has some CPUs and

		 * isn't load balancing.  The former is obvious.  The

		 * latter: All child cpusets contain a subset of the

		 * parent's cpus, so just skip them, and then we call

		 * update_domain_attr_tree() to calc relax_domain_level of

		 * the corresponding sched domain.

		 *

		 * If root is load-balancing, we can skip @cp if it

		 * is a subset of the root's effective_cpus.

 skip @cp's subtree if not a partition root */

 Find the best partition (set of sched domains) */

 one less element */

	/*

	 * Now we know how many domains to create.

	 * Convert <csn, csa> to <ndoms, doms> and populate cpu masks.

	/*

	 * The rest of the code, including the scheduler, can deal with

	 * dattr==NULL case. No need to abort if alloc fails.

 Skip completed partitions */

 Done with this partition */

	/*

	 * Fallback to the default domain if kmalloc() failed.

	 * See comments in partition_sched_domains().

	/*

	 * Clear default root domain DL accounting, it will be computed again

	 * if a task belongs to it.

/*

 * Rebuild scheduler domains.

 *

 * If the flag 'sched_load_balance' of any cpuset with non-empty

 * 'cpus' changes, or if the 'cpus' allowed changes in any cpuset

 * which has that flag enabled, or if any cpuset with a non-empty

 * 'cpus' is removed, then call this routine to rebuild the

 * scheduler's dynamic sched domains.

 *

 * Call with cpuset_rwsem held.  Takes cpus_read_lock().

	/*

	 * If we have raced with CPU hotplug, return early to avoid

	 * passing doms with offlined cpu to partition_sched_domains().

	 * Anyways, cpuset_hotplug_workfn() will rebuild sched domains.

	 *

	 * With no CPUs in any subpartitions, top_cpuset's effective CPUs

	 * should be the same as the active CPUs, so checking only top_cpuset

	 * is enough to detect racing CPU offlines.

	/*

	 * With subpartition CPUs, however, the effective CPUs of a partition

	 * root should be only a subset of the active CPUs.  Since a CPU in any

	 * partition root could be offlined, all must be checked.

 Generate domain masks and attrs */

 Have scheduler rebuild the domains */

 !CONFIG_SMP */

 CONFIG_SMP */

/**

 * update_tasks_cpumask - Update the cpumasks of tasks in the cpuset.

 * @cs: the cpuset in which each task's cpus_allowed mask needs to be changed

 *

 * Iterate through each task of @cs updating its cpus_allowed to the

 * effective cpuset's.  As this function is called with cpuset_rwsem held,

 * cpuset membership stays stable.

/**

 * compute_effective_cpumask - Compute the effective cpumask of the cpuset

 * @new_cpus: the temp variable for the new effective_cpus mask

 * @cs: the cpuset the need to recompute the new effective_cpus mask

 * @parent: the parent cpuset

 *

 * If the parent has subpartition CPUs, include them in the list of

 * allowable CPUs in computing the new effective_cpus mask. Since offlined

 * CPUs are not removed from subparts_cpus, we have to use cpu_active_mask

 * to mask those out.

/*

 * Commands for update_parent_subparts_cpumask

 Enable partition root	 */

 Disable partition root	 */

 Update parent's subparts_cpus */

/**

 * update_parent_subparts_cpumask - update subparts_cpus mask of parent cpuset

 * @cpuset:  The cpuset that requests change in partition root state

 * @cmd:     Partition root state change command

 * @newmask: Optional new cpumask for partcmd_update

 * @tmp:     Temporary addmask and delmask

 * Return:   0, 1 or an error code

 *

 * For partcmd_enable, the cpuset is being transformed from a non-partition

 * root to a partition root. The cpus_allowed mask of the given cpuset will

 * be put into parent's subparts_cpus and taken away from parent's

 * effective_cpus. The function will return 0 if all the CPUs listed in

 * cpus_allowed can be granted or an error code will be returned.

 *

 * For partcmd_disable, the cpuset is being transofrmed from a partition

 * root back to a non-partition root. Any CPUs in cpus_allowed that are in

 * parent's subparts_cpus will be taken away from that cpumask and put back

 * into parent's effective_cpus. 0 should always be returned.

 *

 * For partcmd_update, if the optional newmask is specified, the cpu

 * list is to be changed from cpus_allowed to newmask. Otherwise,

 * cpus_allowed is assumed to remain the same. The cpuset should either

 * be a partition root or an invalid partition root. The partition root

 * state may change if newmask is NULL and none of the requested CPUs can

 * be granted by the parent. The function will return 1 if changes to

 * parent's subparts_cpus and effective_cpus happen or 0 otherwise.

 * Error code should only be returned when newmask is non-NULL.

 *

 * The partcmd_enable and partcmd_disable commands are used by

 * update_prstate(). The partcmd_update command is used by

 * update_cpumasks_hier() with newmask NULL and update_cpumask() with

 * newmask set.

 *

 * The checking is more strict when enabling partition root than the

 * other two commands.

 *

 * Because of the implicit cpu exclusive nature of a partition root,

 * cpumask changes that violates the cpu exclusivity rule will not be

 * permitted when checked by validate_change(). The validate_change()

 * function will also prevent any changes to the cpu list if it is not

 * a superset of children's cpu lists.

 Moving cpus from effective_cpus to subparts_cpus */

 Moving cpus from subparts_cpus to effective_cpus */

 Partition error? */

	/*

	 * The parent must be a partition root.

	 * The new cpumask, if present, or the current cpus_allowed must

	 * not be empty.

	/*

	 * Enabling/disabling partition root is not allowed if there are

	 * online children.

	/*

	 * Enabling partition root is not allowed if not all the CPUs

	 * can be granted from parent's effective_cpus or at least one

	 * CPU will be left after that.

	/*

	 * A cpumask update cannot make parent's effective_cpus become empty.

		/*

		 * partcmd_update with newmask:

		 *

		 * delmask = cpus_allowed & ~newmask & parent->subparts_cpus

		 * addmask = newmask & parent->effective_cpus

		 *		     & ~parent->subparts_cpus

		/*

		 * Return error if the new effective_cpus could become empty.

			/*

			 * As some of the CPUs in subparts_cpus might have

			 * been offlined, we need to compute the real delmask

			 * to confirm that.

		/*

		 * partcmd_update w/o newmask:

		 *

		 * addmask = cpus_allowed & parent->effective_cpus

		 *

		 * Note that parent's subparts_cpus may have been

		 * pre-shrunk in case there is a change in the cpu list.

		 * So no deletion is needed.

		/*

		 * Check for possible transition between PRS_ENABLED

		 * and PRS_ERROR.

		/*

		 * Set part_error if previously in invalid state.

 Nothing need to be done */

		/*

		 * Remove all its cpus from parent's subparts_cpus.

	/*

	 * Change the parent's subparts_cpus.

	 * Newly added CPUs will be removed from effective_cpus and

	 * newly deleted ones will be added back to effective_cpus.

		/*

		 * Some of the CPUs in subparts_cpus might have been offlined.

/*

 * update_cpumasks_hier - Update effective cpumasks and tasks in the subtree

 * @cs:  the cpuset to consider

 * @tmp: temp variables for calculating effective_cpus & partition setup

 *

 * When configured cpumask is changed, the effective cpumasks of this cpuset

 * and all its descendants need to be updated.

 *

 * On legacy hierarchy, effective_cpus will be the same with cpu_allowed.

 *

 * Called with cpuset_rwsem held

		/*

		 * If it becomes empty, inherit the effective mask of the

		 * parent, which is guaranteed to have some CPUs.

		/*

		 * Skip the whole subtree if the cpumask remains the same

		 * and has no partition root state.

		/*

		 * update_parent_subparts_cpumask() should have been called

		 * for cs already in update_cpumask(). We should also call

		 * update_tasks_cpumask() again for tasks in the parent

		 * cpuset if the parent's subparts_cpus changes.

				/*

				 * If parent is not a partition root or an

				 * invalid partition root, clear its state

				 * and its CS_CPU_EXCLUSIVE flag.

				/*

				 * clear_bit() is an atomic operation and

				 * readers aren't interested in the state

				 * of CS_CPU_EXCLUSIVE anyway. So we can

				 * just update the flag without holding

				 * the callback_lock.

				/*

				 * When parent is invalid, it has to be too.

			/*

			 * Make sure that effective_cpus & subparts_cpus

			 * are mutually exclusive.

			 *

			 * In the unlikely event that effective_cpus

			 * becomes empty. we clear cp->nr_subparts_cpus and

			 * let its child partition roots to compete for

			 * CPUs again.

		/*

		 * On legacy hierarchy, if the effective cpumask of any non-

		 * empty cpuset is changed, we need to rebuild sched domains.

		 * On default hierarchy, the cpuset needs to be a partition

		 * root as well.

/**

 * update_sibling_cpumasks - Update siblings cpumasks

 * @parent:  Parent cpuset

 * @cs:      Current cpuset

 * @tmp:     Temp variables

	/*

	 * Check all its siblings and call update_cpumasks_hier()

	 * if their use_parent_ecpus flag is set in order for them

	 * to use the right effective_cpus value.

/**

 * update_cpumask - update the cpus_allowed mask of a cpuset and all tasks in it

 * @cs: the cpuset to consider

 * @trialcs: trial cpuset

 * @buf: buffer of cpu numbers written to this cpuset

 top_cpuset.cpus_allowed tracks cpu_online_mask; it's read-only */

	/*

	 * An empty cpus_allowed is ok only if the cpuset has no tasks.

	 * Since cpulist_parse() fails on an empty mask, we special case

	 * that parsing.  The validate_change() call ensures that cpusets

	 * with tasks have cpus.

 Nothing to do if the cpus didn't change */

	/*

	 * Use the cpumasks in trialcs for tmpmasks when they are pointers

	 * to allocated cpumasks.

 Cpumask of a partition root cannot be empty */

	/*

	 * Make sure that subparts_cpus is a subset of cpus_allowed.

		/*

		 * For partition root, update the cpumasks of sibling

		 * cpusets if they use parent's effective_cpus.

/*

 * Migrate memory region from one set of nodes to another.  This is

 * performed asynchronously as it can be called from process migration path

 * holding locks involved in process management.  All mm migrations are

 * performed in the queued order and can be waited for by flushing

 * cpuset_migrate_mm_wq.

 on a wq worker, no need to worry about %current's mems_allowed */

/*

 * cpuset_change_task_nodemask - change task's mems_allowed and mempolicy

 * @tsk: the task to change

 * @newmems: new nodes that the task will be set

 *

 * We use the mems_allowed_seq seqlock to safely update both tsk->mems_allowed

 * and rebind an eventual tasks' mempolicy. If the task is allocating in

 * parallel, it might temporarily see an empty intersection, which results in

 * a seqlock check and retry before OOM or allocation failure.

/**

 * update_tasks_nodemask - Update the nodemasks of tasks in the cpuset.

 * @cs: the cpuset in which each task's mems_allowed mask needs to be changed

 *

 * Iterate through each task of @cs updating its mems_allowed to the

 * effective cpuset's.  As this function is called with cpuset_rwsem held,

 * cpuset membership stays stable.

 protected by cpuset_rwsem */

 causes mpol_dup() rebind */

	/*

	 * The mpol_rebind_mm() call takes mmap_lock, which we couldn't

	 * take while holding tasklist_lock.  Forks can happen - the

	 * mpol_dup() cpuset_being_rebound check will catch such forks,

	 * and rebind their vma mempolicies too.  Because we still hold

	 * the global cpuset_rwsem, we know that no other rebind effort

	 * will be contending for the global variable cpuset_being_rebound.

	 * It's ok if we rebind the same mm twice; mpol_rebind_mm()

	 * is idempotent.  Also migrate pages in each mm to new nodes.

	/*

	 * All the tasks' nodemasks have been updated, update

	 * cs->old_mems_allowed.

 We're done rebinding vmas to this cpuset's new mems_allowed. */

/*

 * update_nodemasks_hier - Update effective nodemasks and tasks in the subtree

 * @cs: the cpuset to consider

 * @new_mems: a temp variable for calculating new effective_mems

 *

 * When configured nodemask is changed, the effective nodemasks of this cpuset

 * and all its descendants need to be updated.

 *

 * On legacy hierarchy, effective_mems will be the same with mems_allowed.

 *

 * Called with cpuset_rwsem held

		/*

		 * If it becomes empty, inherit the effective mask of the

		 * parent, which is guaranteed to have some MEMs.

 Skip the whole subtree if the nodemask remains the same. */

/*

 * Handle user request to change the 'mems' memory placement

 * of a cpuset.  Needs to validate the request, update the

 * cpusets mems_allowed, and for each task in the cpuset,

 * update mems_allowed and rebind task's mempolicy and any vma

 * mempolicies and if the cpuset is marked 'memory_migrate',

 * migrate the tasks pages to the new memory.

 *

 * Call with cpuset_rwsem held. May take callback_lock during call.

 * Will take tasklist_lock, scan tasklist for tasks in cpuset cs,

 * lock each such tasks mm->mmap_lock, scan its vma's and rebind

 * their mempolicies to the cpusets new mems_allowed.

	/*

	 * top_cpuset.mems_allowed tracks node_stats[N_MEMORY];

	 * it's read-only

	/*

	 * An empty mems_allowed is ok iff there are no tasks in the cpuset.

	 * Since nodelist_parse() fails on an empty mask, we special case

	 * that parsing.  The validate_change() call ensures that cpusets

	 * with tasks have memory.

 Too easy - nothing to do */

 use trialcs->mems_allowed as a temp variable */

/**

 * update_tasks_flags - update the spread flags of tasks in the cpuset.

 * @cs: the cpuset in which each task's spread flags needs to be changed

 *

 * Iterate through each task of @cs updating its spread flags.  As this

 * function is called with cpuset_rwsem held, cpuset membership stays

 * stable.

/*

 * update_flag - read a 0 or a 1 in a file and update associated flag

 * bit:		the bit to update (see cpuset_flagbits_t)

 * cs:		the cpuset to update

 * turning_on: 	whether the flag is being set or cleared

 *

 * Call with cpuset_rwsem held.

/*

 * update_prstate - update partititon_root_state

 * cs: the cpuset to update

 * new_prs: new partition root state

 *

 * Call with cpuset_rwsem held.

	/*

	 * Cannot force a partial or invalid partition root to a full

	 * partition root.

		/*

		 * Turning on partition root requires setting the

		 * CS_CPU_EXCLUSIVE bit implicitly as well and cpus_allowed

		 * cannot be NULL.

		/*

		 * Turning off partition root will clear the

		 * CS_CPU_EXCLUSIVE bit.

 Turning off CS_CPU_EXCLUSIVE will not return error */

	/*

	 * Update cpumask of parent's tasks except when it is the top

	 * cpuset as some system daemons cannot be mapped to other CPUs.

/*

 * Frequency meter - How fast is some event occurring?

 *

 * These routines manage a digitally filtered, constant time based,

 * event frequency meter.  There are four routines:

 *   fmeter_init() - initialize a frequency meter.

 *   fmeter_markevent() - called each time the event happens.

 *   fmeter_getrate() - returns the recent rate of such events.

 *   fmeter_update() - internal routine used to update fmeter.

 *

 * A common data structure is passed to each of these routines,

 * which is used to keep track of the state required to manage the

 * frequency meter and its digital filter.

 *

 * The filter works on the number of events marked per unit time.

 * The filter is single-pole low-pass recursive (IIR).  The time unit

 * is 1 second.  Arithmetic is done using 32-bit integers scaled to

 * simulate 3 decimal digits of precision (multiplied by 1000).

 *

 * With an FM_COEF of 933, and a time base of 1 second, the filter

 * has a half-life of 10 seconds, meaning that if the events quit

 * happening, then the rate returned from the fmeter_getrate()

 * will be cut in half each 10 seconds, until it converges to zero.

 *

 * It is not worth doing a real infinitely recursive filter.  If more

 * than FM_MAXTICKS ticks have elapsed since the last filter event,

 * just compute FM_MAXTICKS ticks worth, by which point the level

 * will be stable.

 *

 * Limit the count of unprocessed events to FM_MAXCNT, so as to avoid

 * arithmetic overflow in the fmeter_update() routine.

 *

 * Given the simple 32 bit integer arithmetic used, this meter works

 * best for reporting rates between one per millisecond (msec) and

 * one per 32 (approx) seconds.  At constant rates faster than one

 * per msec it maxes out at values just under 1,000,000.  At constant

 * rates between one per msec, and one per second it will stabilize

 * to a value N*1000, where N is the rate of events per second.

 * At constant rates between one per second and one per 32 seconds,

 * it will be choppy, moving up on the seconds that have an event,

 * and then decaying until the next event.  At rates slower than

 * about one in 32 seconds, it decays all the way back to zero between

 * each event.

 coefficient for half-life of 10 secs */

 useless computing more ticks than this */

 limit cnt to avoid overflow */

 faux fixed point scale */

 Initialize a frequency meter */

 Internal meter update - process cnt events and update value */

 Process any previous ticks, then bump cnt by one (times scale). */

 Process any previous ticks, then return current value. */

 Called by cgroups to determine if a cpuset is usable; cpuset_rwsem held */

 used later by cpuset_attach() */

 allow moving tasks into an empty cpuset if on default hierarchy */

	/*

	 * Mark attach is in progress.  This makes validate_change() fail

	 * changes which zero cpus/mems_allowed.

/*

 * Protected by cpuset_rwsem.  cpus_attach is used only by cpuset_attach()

 * but we can't allocate it dynamically there.  Define it global and

 * allocate from cpuset_init().

 static buf protected by cpuset_rwsem */

		/*

		 * can_attach beforehand should guarantee that this doesn't

		 * fail.  TODO: have a better way to handle failure here

	/*

	 * Change mm for all threadgroup leaders. This is expensive and may

	 * sleep and should be moved outside migration path proper.

			/*

			 * old_mems_allowed is the same with mems_allowed

			 * here, except if this task is being moved

			 * automatically due to hotplug.  In that case

			 * @mems_allowed has been updated and is empty, so

			 * @old_mems_allowed is the right nodesets that we

			 * migrate mm from.

 The various types of files and directories in a cpuset file system */

/*

 * Common handling for a write to a "cpus" or "mems" file.

	/*

	 * CPU or memory hotunplug may leave @cs w/o any execution

	 * resources, in which case the hotplug code asynchronously updates

	 * configuration and transfers all tasks to the nearest ancestor

	 * which can execute.

	 *

	 * As writes to "cpus" or "mems" may restore @cs's execution

	 * resources, wait for the previously scheduled operations before

	 * proceeding, so that we don't end up keep removing tasks added

	 * after execution capability is restored.

	 *

	 * cpuset_hotplug_work calls back into cgroup core via

	 * cgroup_transfer_tasks() and waiting for it from a cgroupfs

	 * operation like this one can lead to a deadlock through kernfs

	 * active_ref protection.  Let's break the protection.  Losing the

	 * protection is okay as we check whether @cs is online after

	 * grabbing cpuset_rwsem anyway.  This only happens on the legacy

	 * hierarchies.

/*

 * These ascii lists should be read in a single call, by using a user

 * buffer large enough to hold the entire map.  If read in smaller

 * chunks, there is no guarantee of atomicity.  Since the display format

 * used, list of ranges of sequential numbers, is variable length,

 * and since these maps can change value dynamically, one could read

 * gibberish by doing partial reads while a list was changing.

 Unreachable but makes gcc happy */

 Unreachable but makes gcc happy */

	/*

	 * Convert "root" to ENABLED, and convert "member" to DISABLED.

/*

 * for the common functions, 'private' gives the type of file

 terminate */

/*

 * This is currently a minimal set for the default hierarchy. It can be

 * expanded later on by migrating more features and control files from v1.

 terminate */

/*

 *	cpuset_css_alloc - allocate a cpuset css

 *	cgrp:	control group that the new cpuset will be part of

 Set CS_MEMORY_MIGRATE for default hierarchy */

	/*

	 * Clone @parent's configuration if CGRP_CPUSET_CLONE_CHILDREN is

	 * set.  This flag handling is implemented in cgroup core for

	 * histrical reasons - the flag may be specified during mount.

	 *

	 * Currently, if any sibling cpusets have exclusive cpus or mem, we

	 * refuse to clone the configuration - thereby refusing the task to

	 * be entered, and as a result refusing the sys_unshare() or

	 * clone() which initiated it.  If this becomes a problem for some

	 * users who wish to allow that scenario, then this could be

	 * changed to grant parent->cpus_allowed-sibling_cpus_exclusive

	 * (and likewise for mems) to the new cgroup.

/*

 * If the cpuset being removed has its flag 'sched_load_balance'

 * enabled, then simulate turning sched_load_balance off, which

 * will call rebuild_sched_domains_locked(). That is not needed

 * in the default hierarchy where only changes in partition

 * will cause repartitioning.

 *

 * If the cpuset has the 'sched.partition' flag enabled, simulate

 * turning 'sched.partition" off.

/*

 * Make sure the new task conform to the current state of its parent,

 * which could have been changed by cpuset just after it inherits the

 * state from the parent and before it sits on the cgroup's task list.

/**

 * cpuset_init - initialize cpusets at system boot

 *

 * Description: Initialize top_cpuset

/*

 * If CPU and/or memory hotplug handlers, below, unplug any CPUs

 * or memory nodes, we need to walk over the cpuset hierarchy,

 * removing that CPU or node from all cpusets.  If this removes the

 * last CPU or node from a cpuset, then move the tasks in the empty

 * cpuset to its next-highest non-empty parent.

	/*

	 * Find its next-highest non-empty parent, (top cpuset

	 * has online cpus, so can't be empty).

	/*

	 * Don't call update_tasks_cpumask() if the cpuset becomes empty,

	 * as the tasks will be migratecd to an ancestor.

	/*

	 * Move tasks to the nearest ancestor with execution resources,

	 * This is full cgroup operation which will also call back into

	 * cpuset. Should be done outside any lock.

/**

 * cpuset_hotplug_update_tasks - update tasks in a cpuset for hotunplug

 * @cs: cpuset in interest

 * @tmp: the tmpmasks structure pointer

 *

 * Compare @cs's cpu and mem masks against top_cpuset and if some have gone

 * offline, update @cs accordingly.  If @cs ends up with no CPU or memory,

 * all its tasks are moved to the nearest ancestor with both resources.

	/*

	 * We have raced with task attaching. We wait until attaching

	 * is finished, so we won't attach a task to an empty cpuset.

		/*

		 * Make sure that CPUs allocated to child partitions

		 * do not show up in effective_cpus.

	/*

	 * In the unlikely event that a partition root has empty

	 * effective_cpus or its parent becomes erroneous, we have to

	 * transition it to the erroneous state.

		/*

		 * If the effective_cpus is empty because the child

		 * partitions take away all the CPUs, we can keep

		 * the current partition and let the child partitions

		 * fight for available CPUs.

	/*

	 * On the other hand, an erroneous partition root may be transitioned

	 * back to a regular one or a partition root with no CPU allocated

	 * from the parent may change to erroneous.

/**

 * cpuset_hotplug_workfn - handle CPU/memory hotunplug for a cpuset

 *

 * This function is called after either CPU or memory configuration has

 * changed and updates cpuset accordingly.  The top_cpuset is always

 * synchronized to cpu_active_mask and N_MEMORY, which is necessary in

 * order to make cpusets transparent (of no affect) on systems that are

 * actively using CPU hotplug but making no active use of cpusets.

 *

 * Non-root cpusets are only affected by offlining.  If any CPUs or memory

 * nodes have been taken down, cpuset_hotplug_update_tasks() is invoked on

 * all descendants.

 *

 * Note that CPU offlining during suspend is ignored.  We don't modify

 * cpusets across suspend/resume cycles at all.

 fetch the available cpus/mems and find out which changed how */

	/*

	 * If subparts_cpus is populated, it is likely that the check below

	 * will produce a false positive on cpus_updated when the cpu list

	 * isn't changed. It is extra work, but it is better to be safe.

	/*

	 * In the rare case that hotplug removes all the cpus in subparts_cpus,

	 * we assumed that cpus are updated.

 synchronize cpus_allowed to cpu_active_mask */

		/*

		 * Make sure that CPUs allocated to child partitions

		 * do not show up in effective_cpus. If no CPU is left,

		 * we clear the subparts_cpus & let the child partitions

		 * fight for the CPUs again.

 we don't mess with cpumasks of tasks in top_cpuset */

 synchronize mems_allowed to N_MEMORY */

 if cpus or mems changed, we need to propagate to descendants */

 rebuild sched domains if cpus_allowed has changed */

	/*

	 * We're inside cpu hotplug critical region which usually nests

	 * inside cgroup synchronization.  Bounce actual hotplug processing

	 * to a work item to avoid reverse locking order.

/*

 * Keep top_cpuset.mems_allowed tracking node_states[N_MEMORY].

 * Call this routine anytime after node_states[N_MEMORY] changes.

 * See cpuset_update_active_cpus() for CPU hotplug handling.

 ??! */

/**

 * cpuset_init_smp - initialize cpus_allowed

 *

 * Description: Finish top cpuset after cpu, node maps are initialized

/**

 * cpuset_cpus_allowed - return cpus_allowed mask from a tasks cpuset.

 * @tsk: pointer to task_struct from which to obtain cpuset->cpus_allowed.

 * @pmask: pointer to struct cpumask variable to receive cpus_allowed set.

 *

 * Description: Returns the cpumask_var_t cpus_allowed of the cpuset

 * attached to the specified @tsk.  Guaranteed to return some non-empty

 * subset of cpu_online_mask, even if this means going outside the

 * tasks cpuset.

/**

 * cpuset_cpus_allowed_fallback - final fallback before complete catastrophe.

 * @tsk: pointer to task_struct with which the scheduler is struggling

 *

 * Description: In the case that the scheduler cannot find an allowed cpu in

 * tsk->cpus_allowed, we fall back to task_cs(tsk)->cpus_allowed. In legacy

 * mode however, this value is the same as task_cs(tsk)->effective_cpus,

 * which will not contain a sane cpumask during cases such as cpu hotplugging.

 * This is the absolute last resort for the scheduler and it is only used if

 * _every_ other avenue has been traveled.

 *

 * Returns true if the affinity of @tsk was changed, false otherwise.

	/*

	 * We own tsk->cpus_allowed, nobody can change it under us.

	 *

	 * But we used cs && cs->cpus_allowed lockless and thus can

	 * race with cgroup_attach_task() or update_cpumask() and get

	 * the wrong tsk->cpus_allowed. However, both cases imply the

	 * subsequent cpuset_change_cpumask()->set_cpus_allowed_ptr()

	 * which takes task_rq_lock().

	 *

	 * If we are called after it dropped the lock we must see all

	 * changes in tsk_cs()->cpus_allowed. Otherwise we can temporary

	 * set any mask even if it is not right from task_cs() pov,

	 * the pending set_cpus_allowed_ptr() will fix things.

	 *

	 * select_fallback_rq() will fix things ups and set cpu_possible_mask

	 * if required.

/**

 * cpuset_mems_allowed - return mems_allowed mask from a tasks cpuset.

 * @tsk: pointer to task_struct from which to obtain cpuset->mems_allowed.

 *

 * Description: Returns the nodemask_t mems_allowed of the cpuset

 * attached to the specified @tsk.  Guaranteed to return some non-empty

 * subset of node_states[N_MEMORY], even if this means going outside the

 * tasks cpuset.

/**

 * cpuset_nodemask_valid_mems_allowed - check nodemask vs. current mems_allowed

 * @nodemask: the nodemask to be checked

 *

 * Are any of the nodes in the nodemask allowed in current->mems_allowed?

/*

 * nearest_hardwall_ancestor() - Returns the nearest mem_exclusive or

 * mem_hardwall ancestor to the specified cpuset.  Call holding

 * callback_lock.  If no ancestor is mem_exclusive or mem_hardwall

 * (an unusual configuration), then returns the root cpuset.

/**

 * cpuset_node_allowed - Can we allocate on a memory node?

 * @node: is this an allowed node?

 * @gfp_mask: memory allocation flags

 *

 * If we're in interrupt, yes, we can always allocate.  If @node is set in

 * current's mems_allowed, yes.  If it's not a __GFP_HARDWALL request and this

 * node is set in the nearest hardwalled cpuset ancestor to current's cpuset,

 * yes.  If current has access to memory reserves as an oom victim, yes.

 * Otherwise, no.

 *

 * GFP_USER allocations are marked with the __GFP_HARDWALL bit,

 * and do not allow allocations outside the current tasks cpuset

 * unless the task has been OOM killed.

 * GFP_KERNEL allocations are not so marked, so can escape to the

 * nearest enclosing hardwalled ancestor cpuset.

 *

 * Scanning up parent cpusets requires callback_lock.  The

 * __alloc_pages() routine only calls here with __GFP_HARDWALL bit

 * _not_ set if it's a GFP_KERNEL allocation, and all nodes in the

 * current tasks mems_allowed came up empty on the first pass over

 * the zonelist.  So only GFP_KERNEL allocations, if all nodes in the

 * cpuset are short of memory, might require taking the callback_lock.

 *

 * The first call here from mm/page_alloc:get_page_from_freelist()

 * has __GFP_HARDWALL set in gfp_mask, enforcing hardwall cpusets,

 * so no allocation on a node outside the cpuset is allowed (unless

 * in interrupt, of course).

 *

 * The second pass through get_page_from_freelist() doesn't even call

 * here for GFP_ATOMIC calls.  For those calls, the __alloc_pages()

 * variable 'wait' is not set, and the bit ALLOC_CPUSET is not set

 * in alloc_flags.  That logic and the checks below have the combined

 * affect that:

 *	in_interrupt - any node ok (current task context irrelevant)

 *	GFP_ATOMIC   - any node ok

 *	tsk_is_oom_victim   - any node ok

 *	GFP_KERNEL   - any node in enclosing hardwalled cpuset ok

 *	GFP_USER     - only nodes in current tasks mems allowed ok.

 current cpuset ancestors */

 is allocation in zone z allowed? */

	/*

	 * Allow tasks that have access to memory reserves because they have

	 * been OOM killed to get memory anywhere.

 If hardwall request, stop here */

 Let dying task have memory */

 Not hardwall and node outside mems_allowed: scan up cpusets */

/**

 * cpuset_mem_spread_node() - On which node to begin search for a file page

 * cpuset_slab_spread_node() - On which node to begin search for a slab page

 *

 * If a task is marked PF_SPREAD_PAGE or PF_SPREAD_SLAB (as for

 * tasks in a cpuset with is_spread_page or is_spread_slab set),

 * and if the memory allocation used cpuset_mem_spread_node()

 * to determine on which node to start looking, as it will for

 * certain page cache or slab cache pages such as used for file

 * system buffers and inode caches, then instead of starting on the

 * local node to look for a free page, rather spread the starting

 * node around the tasks mems_allowed nodes.

 *

 * We don't have to worry about the returned node being offline

 * because "it can't happen", and even if it did, it would be ok.

 *

 * The routines calling guarantee_online_mems() are careful to

 * only set nodes in task->mems_allowed that are online.  So it

 * should not be possible for the following code to return an

 * offline node.  But if it did, that would be ok, as this routine

 * is not returning the node where the allocation must be, only

 * the node where the search should start.  The zonelist passed to

 * __alloc_pages() will include all nodes.  If the slab allocator

 * is passed an offline node, it will fall back to the local node.

 * See kmem_cache_alloc_node().

/**

 * cpuset_mems_allowed_intersects - Does @tsk1's mems_allowed intersect @tsk2's?

 * @tsk1: pointer to task_struct of some task.

 * @tsk2: pointer to task_struct of some other task.

 *

 * Description: Return true if @tsk1's mems_allowed intersects the

 * mems_allowed of @tsk2.  Used by the OOM killer to determine if

 * one of the task's memory usage might impact the memory available

 * to the other.

/**

 * cpuset_print_current_mems_allowed - prints current's cpuset and mems_allowed

 *

 * Description: Prints current's name, cpuset name, and cached copy of its

 * mems_allowed to the kernel log.

/*

 * Collection of memory_pressure is suppressed unless

 * this flag is enabled by writing "1" to the special

 * cpuset file 'memory_pressure_enabled' in the root cpuset.

/**

 * cpuset_memory_pressure_bump - keep stats of per-cpuset reclaims.

 *

 * Keep a running average of the rate of synchronous (direct)

 * page reclaim efforts initiated by tasks in each cpuset.

 *

 * This represents the rate at which some task in the cpuset

 * ran low on memory on all nodes it was allowed to use, and

 * had to enter the kernels page reclaim code in an effort to

 * create more free memory by tossing clean pages or swapping

 * or writing dirty pages.

 *

 * Display to user space in the per-cpuset read-only file

 * "memory_pressure".  Value displayed is an integer

 * representing the recent rate of entry into the synchronous

 * (direct) page reclaim by any task attached to the cpuset.

/*

 * proc_cpuset_show()

 *  - Print tasks cpuset path into seq_file.

 *  - Used for /proc/<pid>/cpuset.

 *  - No need to task_lock(tsk) on this tsk->cpuset reference, as it

 *    doesn't really matter if tsk->cpuset changes after we read it,

 *    and we take cpuset_rwsem, keeping cpuset_attach() from changing it

 *    anyway.

 CONFIG_PROC_PID_CPUSET */

 Display task mems_allowed in /proc/<pid>/status file. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * RDMA resource limiting controller for cgroups.

 *

 * Used to allow a cgroup hierarchy to stop processes from consuming

 * additional RDMA resources after a certain limit is reached.

 *

 * Copyright (C) 2016 Parav Pandit <pandit.parav@gmail.com>

/*

 * Protects list of resource pools maintained on per cgroup basis

 * and rdma device list.

/*

 * resource table definition as to be seen by the user.

 * Need to add entries to it when more resources are

 * added/defined at IB verb/core layer.

 resource tracker for each resource of rdma cgroup */

/*

 * resource pool object which represents per cgroup, per device

 * resources. There are multiple instances of this object per cgroup,

 * therefore it cannot be embedded within rdma_cgroup structure. It

 * is maintained as list.

 count active user tasks of this pool */

 total number counts which are set to max */

/**

 * uncharge_cg_locked - uncharge resource for rdma cgroup

 * @cg: pointer to cg to uncharge and all parents in hierarchy

 * @device: pointer to rdmacg device

 * @index: index of the resource to uncharge in cg (resource pool)

 *

 * It also frees the resource pool which was created as part of

 * charging operation when there are no resources attached to

 * resource pool.

	/*

	 * rpool cannot be null at this stage. Let kernel operate in case

	 * if there a bug in IB stack or rdma controller, instead of crashing

	 * the system.

	/*

	 * A negative count (or overflow) is invalid,

	 * it indicates a bug in the rdma controller.

		/*

		 * No user of the rpool and all entries are set to max, so

		 * safe to delete this rpool.

/**

 * rdmacg_uncharge_hierarchy - hierarchically uncharge rdma resource count

 * @device: pointer to rdmacg device

 * @stop_cg: while traversing hirerchy, when meet with stop_cg cgroup

 *           stop uncharging

 * @index: index of the resource to uncharge in cg in given resource pool

/**

 * rdmacg_uncharge - hierarchically uncharge rdma resource count

 * @device: pointer to rdmacg device

 * @index: index of the resource to uncharge in cgroup in given resource pool

/**

 * rdmacg_try_charge - hierarchically try to charge the rdma resource

 * @rdmacg: pointer to rdma cgroup which will own this resource

 * @device: pointer to rdmacg device

 * @index: index of the resource to charge in cgroup (resource pool)

 *

 * This function follows charging resource in hierarchical way.

 * It will fail if the charge would cause the new value to exceed the

 * hierarchical limit.

 * Returns 0 if the charge succeeded, otherwise -EAGAIN, -ENOMEM or -EINVAL.

 * Returns pointer to rdmacg for this resource when charging is successful.

 *

 * Charger needs to account resources on two criteria.

 * (a) per cgroup & (b) per device resource usage.

 * Per cgroup resource usage ensures that tasks of cgroup doesn't cross

 * the configured limits. Per device provides granular configuration

 * in multi device usage. It allocates resource pool in the hierarchy

 * for each parent it come across for first resource. Later on resource

 * pool will be available. Therefore it will be much faster thereon

 * to charge/uncharge.

	/*

	 * hold on to css, as cgroup can be removed but resource

	 * accounting happens on css.

/**

 * rdmacg_register_device - register rdmacg device to rdma controller.

 * @device: pointer to rdmacg device whose resources need to be accounted.

 *

 * If IB stack wish a device to participate in rdma cgroup resource

 * tracking, it must invoke this API to register with rdma cgroup before

 * any user space application can start using the RDMA resources.

/**

 * rdmacg_unregister_device - unregister rdmacg device from rdma controller.

 * @device: pointer to rdmacg device which was previously registered with rdma

 *          controller using rdmacg_register_device().

 *

 * IB stack must invoke this after all the resources of the IB device

 * are destroyed and after ensuring that no more resources will be created

 * when this API is invoked.

	/*

	 * Synchronize with any active resource settings,

	 * usage query happening via configfs.

	/*

	 * Now that this device is off the cgroup list, its safe to free

	 * all the rpool resources.

 parse resource options */

 extract the device name first */

 acquire lock to synchronize with hot plug devices */

 now set the new limits of the rpool */

		/*

		 * No user of the rpool and all entries are set to max, so

		 * safe to delete this rpool.

 terminate */

/**

 * rdmacg_css_offline - cgroup css_offline callback

 * @css: css of interest

 *

 * This function is called when @css is about to go away and responsible

 * for shooting down all rdmacg associated with @css. As part of that it

 * marks all the resource pool entries to max value, so that when resources are

 * uncharged, associated resource pool can be freed as well.

SPDX-License-Identifier: GPL-2.0

/*

 * Propagate the cgroup frozen state upwards by the cgroup tree.

	/*

	 * If the new state is frozen, some freezing ancestor cgroups may change

	 * their state too, depending on if all their descendants are frozen.

	 *

	 * Otherwise, all ancestor cgroups are forced into the non-frozen state.

/*

 * Revisit the cgroup frozen state.

 * Checks if the cgroup is really frozen and perform all state transitions.

	/*

	 * If the cgroup has to be frozen (CGRP_FREEZE bit set),

	 * and all tasks are frozen and/or stopped, let's consider

	 * the cgroup frozen. Otherwise it's not frozen.

 Already there? */

 Already there? */

 Update the state of ancestor cgroups. */

/*

 * Increment cgroup's nr_frozen_tasks.

/*

 * Decrement cgroup's nr_frozen_tasks.

/*

 * Enter frozen/stopped state, if not yet there. Update cgroup's counters,

 * and revisit the state of the cgroup, if necessary.

/*

 * Conditionally leave frozen/stopped state. Update cgroup's counters,

 * and revisit the state of the cgroup, if necessary.

 *

 * If always_leave is not set, and the cgroup is freezing,

 * we're racing with the cgroup freezing. In this case, we don't

 * drop the frozen counter to avoid a transient switch to

 * the unfrozen state.

/*

 * Freeze or unfreeze the task by setting or clearing the JOBCTL_TRAP_FREEZE

 * jobctl bit.

 If the task is about to die, don't bother with freezing it. */

/*

 * Freeze or unfreeze all tasks in the given cgroup.

		/*

		 * Ignore kernel threads here. Freezing cgroups containing

		 * kthreads isn't supported.

	/*

	 * Cgroup state should be revisited here to cover empty leaf cgroups

	 * and cgroups which descendants are already in the desired state.

/*

 * Adjust the task state (freeze or unfreeze) and revisit the state of

 * source and destination cgroups.

	/*

	 * Kernel threads are not supposed to be frozen at all.

	/*

	 * It's not necessary to do changes if both of the src and dst cgroups

	 * are not freezing and task is not frozen.

	/*

	 * Adjust counters of freezing and frozen tasks.

	 * Note, that if the task is frozen, but the destination cgroup is not

	 * frozen, we bump both counters to keep them balanced.

	/*

	 * Force the task to the desired state.

	/*

	 * Nothing changed? Just exit.

	/*

	 * Propagate changes downwards the cgroup tree.

			/*

			 * Already frozen because of ancestor's settings?

			/*

			 * Still frozen because of ancestor's settings?

		/*

		 * Do change actual state: freeze or unfreeze.

	/*

	 * Even if the actual state hasn't changed, let's notify a user.

	 * The state can be enforced by an ancestor cgroup: the cgroup

	 * can already be in the desired state or it can be locked in the

	 * opposite state, so that the transition will never happen.

	 * In both cases it's better to notify a user, that there is

	 * nothing to wait for.

 SPDX-License-Identifier: GPL-2.0

 cgroup namespaces */

 Allow only sysadmin to create cgroup namespace. */

 It is not safe to take cgroup_mutex here */

 Don't need to do anything if we are attaching to our own cgroupns. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Miscellaneous cgroup controller

 *

 * Copyright 2020 Google LLC

 * Author: Vipin Sharma <vipinsh@google.com>

 Miscellaneous res name, keep it in sync with enum misc_res_type */

 AMD SEV ASIDs resource */

 AMD SEV-ES ASIDs resource */

 Root misc cgroup */

/*

 * Miscellaneous resources capacity for the entire machine. 0 capacity means

 * resource is not initialized or not present in the host.

 *

 * root_cg.max and capacity are independent of each other. root_cg.max can be

 * more than the actual capacity. We are using Limits resource distribution

 * model of cgroup for miscellaneous controller.

/**

 * parent_misc() - Get the parent of the passed misc cgroup.

 * @cgroup: cgroup whose parent needs to be fetched.

 *

 * Context: Any context.

 * Return:

 * * struct misc_cg* - Parent of the @cgroup.

 * * %NULL - If @cgroup is null or the passed cgroup does not have a parent.

/**

 * valid_type() - Check if @type is valid or not.

 * @type: misc res type.

 *

 * Context: Any context.

 * Return:

 * * true - If valid type.

 * * false - If not valid type.

/**

 * misc_cg_res_total_usage() - Get the current total usage of the resource.

 * @type: misc res type.

 *

 * Context: Any context.

 * Return: Current total usage of the resource.

/**

 * misc_cg_set_capacity() - Set the capacity of the misc cgroup res.

 * @type: Type of the misc res.

 * @capacity: Supported capacity of the misc res on the host.

 *

 * If capacity is 0 then the charging a misc cgroup fails for that type.

 *

 * Context: Any context.

 * Return:

 * * %0 - Successfully registered the capacity.

 * * %-EINVAL - If @type is invalid.

/**

 * misc_cg_cancel_charge() - Cancel the charge from the misc cgroup.

 * @type: Misc res type in misc cg to cancel the charge from.

 * @cg: Misc cgroup to cancel charge from.

 * @amount: Amount to cancel.

 *

 * Context: Any context.

/**

 * misc_cg_try_charge() - Try charging the misc cgroup.

 * @type: Misc res type to charge.

 * @cg: Misc cgroup which will be charged.

 * @amount: Amount to charge.

 *

 * Charge @amount to the misc cgroup. Caller must use the same cgroup during

 * the uncharge call.

 *

 * Context: Any context.

 * Return:

 * * %0 - If successfully charged.

 * * -EINVAL - If @type is invalid or misc res has 0 capacity.

 * * -EBUSY - If max limit will be crossed or total usage will be more than the

 *	      capacity.

/**

 * misc_cg_uncharge() - Uncharge the misc cgroup.

 * @type: Misc res type which was charged.

 * @cg: Misc cgroup which will be uncharged.

 * @amount: Charged amount.

 *

 * Context: Any context.

/**

 * misc_cg_max_show() - Show the misc cgroup max limit.

 * @sf: Interface file

 * @v: Arguments passed

 *

 * Context: Any context.

 * Return: 0 to denote successful print.

/**

 * misc_cg_max_write() - Update the maximum limit of the cgroup.

 * @of: Handler for the file.

 * @buf: Data from the user. It should be either "max", 0, or a positive

 *	 integer.

 * @nbytes: Number of bytes of the data.

 * @off: Offset in the file.

 *

 * User can pass data like:

 * echo sev 23 > misc.max, OR

 * echo sev max > misc.max

 *

 * Context: Any context.

 * Return:

 * * >= 0 - Number of bytes processed in the input.

 * * -EINVAL - If buf is not valid.

 * * -ERANGE - If number is bigger than the unsigned long capacity.

/**

 * misc_cg_current_show() - Show the current usage of the misc cgroup.

 * @sf: Interface file

 * @v: Arguments passed

 *

 * Context: Any context.

 * Return: 0 to denote successful print.

/**

 * misc_cg_capacity_show() - Show the total capacity of misc res on the host.

 * @sf: Interface file

 * @v: Arguments passed

 *

 * Only present in the root cgroup directory.

 *

 * Context: Any context.

 * Return: 0 to denote successful print.

 Misc cgroup interface files */

/**

 * misc_cg_alloc() - Allocate misc cgroup.

 * @parent_css: Parent cgroup.

 *

 * Context: Process context.

 * Return:

 * * struct cgroup_subsys_state* - css of the allocated cgroup.

 * * ERR_PTR(-ENOMEM) - No memory available to allocate.

/**

 * misc_cg_free() - Free the misc cgroup.

 * @css: cgroup subsys object.

 *

 * Context: Any context.

 Cgroup controller callbacks */

 SPDX-License-Identifier: GPL-2.0-only

/**

 * cgroup_rstat_updated - keep track of updated rstat_cpu

 * @cgrp: target cgroup

 * @cpu: cpu on which rstat_cpu was updated

 *

 * @cgrp's rstat_cpu on @cpu was updated.  Put it on the parent's matching

 * rstat_cpu->updated_children list.  See the comment on top of

 * cgroup_rstat_cpu definition for details.

	/*

	 * Speculative already-on-list test. This may race leading to

	 * temporary inaccuracies, which is fine.

	 *

	 * Because @parent's updated_children is terminated with @parent

	 * instead of NULL, we can tell whether @cgrp is on the list by

	 * testing the next pointer for NULL.

 put @cgrp and all ancestors on the corresponding updated lists */

		/*

		 * Both additions and removals are bottom-up.  If a cgroup

		 * is already in the tree, all ancestors are.

 Root has no parent to link it to, but mark it busy */

/**

 * cgroup_rstat_cpu_pop_updated - iterate and dismantle rstat_cpu updated tree

 * @pos: current position

 * @root: root of the tree to traversal

 * @cpu: target cpu

 *

 * Walks the updated rstat_cpu tree on @cpu from @root.  %NULL @pos starts

 * the traversal and %NULL return indicates the end.  During traversal,

 * each returned cgroup is unlinked from the tree.  Must be called with the

 * matching cgroup_rstat_cpu_lock held.

 *

 * The only ordering guarantee is that, for a parent and a child pair

 * covered by a given traversal, if a child is visited, its parent is

 * guaranteed to be visited afterwards.

	/*

	 * We're gonna walk down to the first leaf and visit/remove it.  We

	 * can pick whatever unvisited node as the starting point.

 walk down to the first leaf */

	/*

	 * Unlink @pos from the tree.  As the updated_children list is

	 * singly linked, we have to walk it to find the removal point.

	 * However, due to the way we traverse, @pos will be the first

	 * child in most cases. The only exception is @root.

 only happens for @root */

 see cgroup_rstat_flush() */

 if @may_sleep, play nice and yield if necessary */

/**

 * cgroup_rstat_flush - flush stats in @cgrp's subtree

 * @cgrp: target cgroup

 *

 * Collect all per-cpu stats in @cgrp's subtree into the global counters

 * and propagate them upwards.  After this function returns, all cgroups in

 * the subtree have up-to-date ->stat.

 *

 * This also gets all cgroups in the subtree including @cgrp off the

 * ->updated_children lists.

 *

 * This function may block.

/**

 * cgroup_rstat_flush_irqsafe - irqsafe version of cgroup_rstat_flush()

 * @cgrp: target cgroup

 *

 * This function can be called from any context.

/**

 * cgroup_rstat_flush_hold - flush stats in @cgrp's subtree and hold

 * @cgrp: target cgroup

 *

 * Flush stats in @cgrp's subtree and prevent further flushes.  Must be

 * paired with cgroup_rstat_flush_release().

 *

 * This function may block.

/**

 * cgroup_rstat_flush_release - release cgroup_rstat_flush_hold()

 the root cgrp has rstat_cpu preallocated */

 ->updated_children list is self terminated */

 sanity check */

/*

 * Functions for cgroup basic resource statistics implemented on top of

 * rstat.

 Root-level stats are sourced from system-wide CPU stats */

 fetch the current per-cpu values */

 propagate percpu delta to global */

 propagate global delta to parent (unless that's root) */

/*

 * compute the cputime for the root cgroup by getting the per cpu data

 * at a global level, then categorizing the fields in a manner consistent

 * with how it is done by __cgroup_account_cputime_field for each bit of

 * cpu time attributed to a cgroup.

/*

 * cgroup_freezer.c -  control group freezer subsystem

 *

 * Copyright IBM Corporation, 2007

 *

 * Author : Cedric Le Goater <clg@fr.ibm.com>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of version 2.1 of the GNU Lesser General Public License

 * as published by the Free Software Foundation.

 *

 * This program is distributed in the hope that it would be useful, but

 * WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

/*

 * A cgroup is freezing if any FREEZING flags are set.  FREEZING_SELF is

 * set if "FROZEN" is written to freezer.state cgroupfs file, and cleared

 * for "THAWED".  FREEZING_PARENT is set if the parent freezer is FREEZING

 * for whatever reason.  IOW, a cgroup has FREEZING_PARENT set if one of

 * its ancestors has FREEZING_SELF set.

 freezer is fully online */

 this freezer is freezing */

 the parent freezer is freezing */

 this and its descendants frozen */

 mask for all FREEZING flags */

/**

 * freezer_css_online - commit creation of a freezer css

 * @css: css being created

 *

 * We're committing to creation of @css.  Mark it online and inherit

 * parent's freezing state while holding both parent's and our

 * freezer->lock.

/**

 * freezer_css_offline - initiate destruction of a freezer css

 * @css: css being destroyed

 *

 * @css is going away.  Mark it dead and decrement system_freezing_count if

 * it was holding one.

/*

 * Tasks can be migrated into a different freezer anytime regardless of its

 * current state.  freezer_attach() is responsible for making new tasks

 * conform to the current state.

 *

 * Freezer state changes and task migration are synchronized via

 * @freezer->lock.  freezer_attach() makes the new tasks conform to the

 * current state and all following state changes can see the new tasks.

	/*

	 * Make the new tasks conform to the current state of @new_css.

	 * For simplicity, when migrating any task to a FROZEN cgroup, we

	 * revert it to FREEZING and let update_if_frozen() determine the

	 * correct state later.

	 *

	 * Tasks in @tset are on @new_css but may not conform to its

	 * current state before executing the following - !frozen tasks may

	 * be visible in a FROZEN cgroup and frozen tasks in a THAWED one.

 clear FROZEN and propagate upwards */

/**

 * freezer_fork - cgroup post fork callback

 * @task: a task which has just been forked

 *

 * @task has just been created and should conform to the current state of

 * the cgroup_freezer it belongs to.  This function may race against

 * freezer_attach().  Losing to freezer_attach() means that we don't have

 * to do anything as freezer_attach() will put @task into the appropriate

 * state.

	/*

	 * The root cgroup is non-freezable, so we can skip locking the

	 * freezer.  This is safe regardless of race with task migration.

	 * If we didn't race or won, skipping is obviously the right thing

	 * to do.  If we lost and root is the new cgroup, noop is still the

	 * right thing to do.

/**

 * update_if_frozen - update whether a cgroup finished freezing

 * @css: css of interest

 *

 * Once FREEZING is initiated, transition to FROZEN is lazily updated by

 * calling this function.  If the current state is FREEZING but not FROZEN,

 * this function checks whether all tasks of this cgroup and the descendant

 * cgroups finished freezing and, if so, sets FROZEN.

 *

 * The caller is responsible for grabbing RCU read lock and calling

 * update_if_frozen() on all descendants prior to invoking this function.

 *

 * Task states and freezer state might disagree while tasks are being

 * migrated into or out of @css, so we can't verify task states against

 * @freezer state here.  See freezer_attach() for details.

 are all (live) children frozen? */

 are all tasks frozen? */

			/*

			 * freezer_should_skip() indicates that the task

			 * should be skipped when determining freezing

			 * completion.  Consider it frozen in addition to

			 * the usual frozen condition.

 update states bottom-up */

/**

 * freezer_apply_state - apply state change to a single cgroup_freezer

 * @freezer: freezer to apply state change to

 * @freeze: whether to freeze or unfreeze

 * @state: CGROUP_FREEZING_* flag to set or clear

 *

 * Set or clear @state on @cgroup according to @freeze, and perform

 * freezing or thawing as necessary.

 also synchronizes against task migration, see freezer_attach() */

/**

 * freezer_change_state - change the freezing state of a cgroup_freezer

 * @freezer: freezer of interest

 * @freeze: whether to freeze or thaw

 *

 * Freeze or thaw @freezer according to @freeze.  The operations are

 * recursive - all descendants of @freezer will be affected.

	/*

	 * Update all its descendants in pre-order traversal.  Each

	 * descendant will try to inherit its parent's FREEZING state as

	 * CGROUP_FREEZING_PARENT.

 terminate */

/*

 *  Generic process-grouping system.

 *

 *  Based originally on the cpuset system, extracted by Paul Menage

 *  Copyright (C) 2006 Google, Inc

 *

 *  Notifications support

 *  Copyright (C) 2009 Nokia Corporation

 *  Author: Kirill A. Shutemov

 *

 *  Copyright notices from the original cpuset code:

 *  --------------------------------------------------

 *  Copyright (C) 2003 BULL SA.

 *  Copyright (C) 2004-2006 Silicon Graphics, Inc.

 *

 *  Portions derived from Patrick Mochel's sysfs code.

 *  sysfs is Copyright (c) 2001-3 Patrick Mochel

 *

 *  2003-10-10 Written by Simon Derr.

 *  2003-10-22 Updates by Stephen Hemminger.

 *  2004 May-July Rework by Paul Jackson.

 *  ---------------------------------------------------

 *

 *  This file is subject to the terms and conditions of the GNU General Public

 *  License.  See the file COPYING in the main directory of the Linux

 *  distribution for more details.

 let's not notify more than 100 times per second */

/*

 * To avoid confusing the compiler (and generating warnings) with code

 * that attempts to access what would be a 0-element array (i.e. sized

 * to a potentially empty array when CGROUP_SUBSYS_COUNT == 0), this

 * constant expression can be added.

/*

 * cgroup_mutex is the master lock.  Any modification to cgroup or its

 * hierarchy must be performed while holding it.

 *

 * css_set_lock protects task->cgroups pointer, the list of css_set

 * objects, and the chain of tasks off each css_set.

 *

 * These locks are exported if CONFIG_PROVE_RCU so that accessors in

 * cgroup.h can use them for lockdep annotations.

/*

 * Protects cgroup_idr and css_idr so that IDs can be released without

 * grabbing cgroup_mutex.

/*

 * Protects cgroup_file->kn for !self csses.  It synchronizes notifications

 * against file removal/re-creation across css hiding.

/*

 * cgroup destruction makes heavy use of work items and there can be a lot

 * of concurrent destructions.  Use a separate workqueue so that cgroup

 * destruction work items don't end up filling up max_active of system_wq

 * which may lead to deadlock.

 generate an array of cgroup subsystem pointers */

 array of cgroup subsystem names */

 array of static_keys for cgroup_subsys_enabled() and cgroup_subsys_on_dfl() */

 the default hierarchy */

/*

 * The default hierarchy always exists but is hidden until mounted for the

 * first time.  This is for backward compatibility.

 some controllers are not supported in the default hierarchy */

 some controllers are implicitly enabled on the default hierarchy */

 some controllers can be threaded on the default hierarchy */

 The list of hierarchy roots */

 hierarchy ID allocation and mapping, protected by cgroup_mutex */

/*

 * Assign a monotonically increasing serial number to csses.  It guarantees

 * cgroups with bigger numbers are newer than those with smaller numbers.

 * Also, as csses are always appended to the parent's ->children list, it

 * guarantees that sibling csses are always sorted in the ascending serial

 * number order on the list.  Protected by cgroup_mutex.

/*

 * These bitmasks identify subsystems with specific features to avoid

 * having to do iterative checks repeatedly.

 cgroup namespace for init task */

 cgroup optional features */

/**

 * cgroup_ssid_enabled - cgroup subsys enabled test by subsys ID

 * @ssid: subsys ID of interest

 *

 * cgroup_subsys_enabled() can only be used with literal subsys names which

 * is fine for individual subsystems but unsuitable for cgroup core.  This

 * is slower static_key_enabled() based test indexed by @ssid.

/**

 * cgroup_on_dfl - test whether a cgroup is on the default hierarchy

 * @cgrp: the cgroup of interest

 *

 * The default hierarchy is the v2 interface of cgroup and this function

 * can be used to test whether a cgroup is on the default hierarchy for

 * cases where a subsystem should behave differently depending on the

 * interface version.

 *

 * List of changed behaviors:

 *

 * - Mount options "noprefix", "xattr", "clone_children", "release_agent"

 *   and "name" are disallowed.

 *

 * - When mounting an existing superblock, mount options should match.

 *

 * - Remount is disallowed.

 *

 * - rename(2) is disallowed.

 *

 * - "tasks" is removed.  Everything should be at process granularity.  Use

 *   "cgroup.procs" instead.

 *

 * - "cgroup.procs" is not sorted.  pids will be unique unless they got

 *   recycled in-between reads.

 *

 * - "release_agent" and "notify_on_release" are removed.  Replacement

 *   notification mechanism will be implemented.

 *

 * - "cgroup.clone_children" is removed.

 *

 * - "cgroup.subtree_populated" is available.  Its value is 0 if the cgroup

 *   and its descendants contain no task; otherwise, 1.  The file also

 *   generates kernfs notification which can be monitored through poll and

 *   [di]notify when the value of the file changes.

 *

 * - cpuset: tasks will be kept in empty cpusets when hotplug happens and

 *   take masks of ancestors with non-empty cpus/mems, instead of being

 *   moved to an ancestor.

 *

 * - cpuset: a task can be moved into an empty cpuset, and again it takes

 *   masks of ancestors.

 *

 * - blkcg: blk-throttle becomes properly hierarchical.

 *

 * - debug: disallowed on the default hierarchy.

 IDR wrappers which synchronize using cgroup_idr_lock */

 can @cgrp host both domain and threaded children? */

	/*

	 * Root isn't under domain level resource control exempting it from

	 * the no-internal-process constraint, so it can serve as a thread

	 * root and a parent of resource domains at the same time.

 can @cgrp become a thread root? Should always be true for a thread root */

 mixables don't care */

 domain roots can't be nested under threaded */

 can only have either domain or threaded children */

 and no domain controllers can be enabled */

 is @cgrp root of a threaded subtree? */

 thread root should be a domain */

 a domain w/ threaded children is a thread root */

	/*

	 * A domain which has tasks and explicit threaded controllers

	 * enabled is a thread root.

 a domain which isn't connected to the root w/o brekage can't be used */

 the cgroup itself can be a thread root */

 but the ancestors can't be unless mixable */

 subsystems visibly enabled on a cgroup */

 threaded cgroups can only have threaded controllers */

 subsystems enabled on a cgroup */

 threaded cgroups can only have threaded controllers */

/**

 * cgroup_css - obtain a cgroup's css for the specified subsystem

 * @cgrp: the cgroup of interest

 * @ss: the subsystem of interest (%NULL returns @cgrp->self)

 *

 * Return @cgrp's css (cgroup_subsys_state) associated with @ss.  This

 * function must be called either under cgroup_mutex or rcu_read_lock() and

 * the caller is responsible for pinning the returned css if it wants to

 * keep accessing it outside the said locks.  This function may return

 * %NULL if @cgrp doesn't have @subsys_id enabled.

/**

 * cgroup_tryget_css - try to get a cgroup's css for the specified subsystem

 * @cgrp: the cgroup of interest

 * @ss: the subsystem of interest

 *

 * Find and get @cgrp's css associated with @ss.  If the css doesn't exist

 * or is offline, %NULL is returned.

/**

 * cgroup_e_css_by_mask - obtain a cgroup's effective css for the specified ss

 * @cgrp: the cgroup of interest

 * @ss: the subsystem of interest (%NULL returns @cgrp->self)

 *

 * Similar to cgroup_css() but returns the effective css, which is defined

 * as the matching css of the nearest ancestor including self which has @ss

 * enabled.  If @ss is associated with the hierarchy @cgrp is on, this

 * function is guaranteed to return non-NULL css.

	/*

	 * This function is used while updating css associations and thus

	 * can't test the csses directly.  Test ss_mask.

/**

 * cgroup_e_css - obtain a cgroup's effective css for the specified subsystem

 * @cgrp: the cgroup of interest

 * @ss: the subsystem of interest

 *

 * Find and get the effective css of @cgrp for @ss.  The effective css is

 * defined as the matching css of the nearest ancestor including self which

 * has @ss enabled.  If @ss is not mounted on the hierarchy @cgrp is on,

 * the root css is returned, so this function always returns a valid css.

 *

 * The returned css is not guaranteed to be online, and therefore it is the

 * callers responsibility to try get a reference for it.

/**

 * cgroup_get_e_css - get a cgroup's effective css for the specified subsystem

 * @cgrp: the cgroup of interest

 * @ss: the subsystem of interest

 *

 * Find and get the effective css of @cgrp for @ss.  The effective css is

 * defined as the matching css of the nearest ancestor including self which

 * has @ss enabled.  If @ss is not mounted on the hierarchy @cgrp is on,

 * the root css is returned, so this function always returns a valid css.

 * The returned css must be put using css_put().

/**

 * __cgroup_task_count - count the number of tasks in a cgroup. The caller

 * is responsible for taking the css_set_lock.

 * @cgrp: the cgroup in question

/**

 * cgroup_task_count - count the number of tasks in a cgroup.

 * @cgrp: the cgroup in question

	/*

	 * This is open and unprotected implementation of cgroup_css().

	 * seq_css() is only called from a kernfs file operation which has

	 * an active reference on the file.  Because all the subsystem

	 * files are drained before a css is disassociated with a cgroup,

	 * the matching css from the cgroup's subsys table is guaranteed to

	 * be and stay valid until the enclosing operation is complete.

/**

 * for_each_css - iterate all css's of a cgroup

 * @css: the iteration cursor

 * @ssid: the index of the subsystem, CGROUP_SUBSYS_COUNT after reaching the end

 * @cgrp: the target cgroup to iterate css's of

 *

 * Should be called under cgroup_[tree_]mutex.

/**

 * for_each_e_css - iterate all effective css's of a cgroup

 * @css: the iteration cursor

 * @ssid: the index of the subsystem, CGROUP_SUBSYS_COUNT after reaching the end

 * @cgrp: the target cgroup to iterate css's of

 *

 * Should be called under cgroup_[tree_]mutex.

/**

 * do_each_subsys_mask - filter for_each_subsys with a bitmask

 * @ss: the iteration cursor

 * @ssid: the index of @ss, CGROUP_SUBSYS_COUNT after reaching the end

 * @ss_mask: the bitmask

 *

 * The block will only run for cases where the ssid-th bit (1 << ssid) of

 * @ss_mask is set.

 iterate over child cgrps, lock should be held throughout iteration */

 walk live descendants in pre order */

 walk live descendants in postorder */

/*

 * The default css_set - used by init and its children prior to any

 * hierarchies being mounted. It contains a pointer to the root state

 * for each subsystem. Also used to anchor the list of css_sets. Not

 * reference-counted, to improve performance when child cgroups

 * haven't been created.

	/*

	 * The following field is re-initialized when this cset gets linked

	 * in cgroup_init().  However, let's initialize the field

	 * statically too so that the default cgroup can be accessed safely

	 * early during boot.

 1 for init_css_set */

/**

 * css_set_populated - does a css_set contain any tasks?

 * @cset: target css_set

 *

 * css_set_populated() should be the same as !!cset->nr_tasks at steady

 * state. However, css_set_populated() can be called while a task is being

 * added to or removed from the linked list before the nr_tasks is

 * properly updated. Hence, we can't just look at ->nr_tasks here.

/**

 * cgroup_update_populated - update the populated count of a cgroup

 * @cgrp: the target cgroup

 * @populated: inc or dec populated count

 *

 * One of the css_sets associated with @cgrp is either getting its first

 * task or losing the last.  Update @cgrp->nr_populated_* accordingly.  The

 * count is propagated towards root so that a given cgroup's

 * nr_populated_children is zero iff none of its descendants contain any

 * tasks.

 *

 * @cgrp's interface file "cgroup.populated" is zero if both

 * @cgrp->nr_populated_csets and @cgrp->nr_populated_children are zero and

 * 1 otherwise.  When the sum changes from or to zero, userland is notified

 * that the content of the interface file has changed.  This can be used to

 * detect when @cgrp and its descendants become populated or empty.

/**

 * css_set_update_populated - update populated state of a css_set

 * @cset: target css_set

 * @populated: whether @cset is populated or depopulated

 *

 * @cset is either getting the first task or losing the last.  Update the

 * populated counters of all associated cgroups accordingly.

/*

 * @task is leaving, advance task iterators which are pointing to it so

 * that they can resume at the next position.  Advancing an iterator might

 * remove it from the list, use safe walk.  See css_task_iter_skip() for

 * details.

/**

 * css_set_move_task - move a task from one css_set to another

 * @task: task being moved

 * @from_cset: css_set @task currently belongs to (may be NULL)

 * @to_cset: new css_set @task is being moved to (may be NULL)

 * @use_mg_tasks: move to @to_cset->mg_tasks instead of ->tasks

 *

 * Move @task from @from_cset to @to_cset.  If @task didn't belong to any

 * css_set, @from_cset can be NULL.  If @task is being disassociated

 * instead of moved, @to_cset can be NULL.

 *

 * This function automatically handles populated counter updates and

 * css_task_iter adjustments but the caller is responsible for managing

 * @from_cset and @to_cset's reference counts.

		/*

		 * We are synchronized through cgroup_threadgroup_rwsem

		 * against PF_EXITING setting such that we can't race

		 * against cgroup_exit()/cgroup_free() dropping the css_set.

/*

 * hash table for cgroup groups. This improves the performance to find

 * an existing css_set. This hash doesn't (currently) take into

 * account cgroups in empty hierarchies.

 This css_set is dead. Unlink it and release cgroup and css refs */

/**

 * compare_css_sets - helper function for find_existing_css_set().

 * @cset: candidate css_set being tested

 * @old_cset: existing css_set for a task

 * @new_cgrp: cgroup that's being entered by the task

 * @template: desired set of css pointers in css_set (pre-calculated)

 *

 * Returns true if "cset" matches "old_cset" except for the hierarchy

 * which "new_cgrp" belongs to, for which it should match "new_cgrp".

	/*

	 * On the default hierarchy, there can be csets which are

	 * associated with the same set of cgroups but different csses.

	 * Let's first ensure that csses match.

 @cset's domain should match the default cgroup's */

	/*

	 * Compare cgroup pointers in order to distinguish between

	 * different cgroups in hierarchies.  As different cgroups may

	 * share the same effective css, this comparison is always

	 * necessary.

 See if we reached the end - both lists are equal length. */

 Locate the cgroups associated with these links. */

 Hierarchies should be linked in the same order. */

		/*

		 * If this hierarchy is the hierarchy of the cgroup

		 * that's changing, then we need to check that this

		 * css_set points to the new cgroup; if it's any other

		 * hierarchy, then this css_set should point to the

		 * same cgroup as the old css_set.

/**

 * find_existing_css_set - init css array and find the matching css_set

 * @old_cset: the css_set that we're using before the cgroup transition

 * @cgrp: the cgroup that we're moving into

 * @template: out param for the new set of csses, should be clear on entry

	/*

	 * Build the set of subsystem state objects that we want to see in the

	 * new css_set. While subsystems can change globally, the entries here

	 * won't change, so no need for locking.

			/*

			 * @ss is in this hierarchy, so we want the

			 * effective css from @cgrp.

			/*

			 * @ss is not in this hierarchy, so we don't want

			 * to change the css.

 This css_set matches what we need */

 No existing cgroup group matched */

/**

 * allocate_cgrp_cset_links - allocate cgrp_cset_links

 * @count: the number of links to allocate

 * @tmp_links: list_head the allocated links are put on

 *

 * Allocate @count cgrp_cset_link structures and chain them on @tmp_links

 * through ->cset_link.  Returns 0 on success or -errno.

/**

 * link_css_set - a helper function to link a css_set to a cgroup

 * @tmp_links: cgrp_cset_link objects allocated by allocate_cgrp_cset_links()

 * @cset: the css_set to be linked

 * @cgrp: the destination cgroup

	/*

	 * Always add links to the tail of the lists so that the lists are

	 * in chronological order.

/**

 * find_css_set - return a new css_set with one cgroup updated

 * @old_cset: the baseline css_set

 * @cgrp: the cgroup to be updated

 *

 * Return a new css_set that's equivalent to @old_cset, but with @cgrp

 * substituted into the appropriate hierarchy.

	/* First see if we already have a cgroup group that matches

 Allocate all the cgrp_cset_link objects that we'll need */

	/* Copy the set of subsystem state objects generated in

 Add reference counts and links from the new css_set. */

 Add @cset to the hash table */

	/*

	 * If @cset should be threaded, look up the matching dom_cset and

	 * link them up.  We first fully initialize @cset then look for the

	 * dom_cset.  It's simpler this way and safe as @cset is guaranteed

	 * to stay empty until we return.

 Rebind all subsystems back to the default hierarchy */

	/*

	 * Release all the links from cset_links to this hierarchy's

	 * root cgroup

/*

 * look up cgroup associated with current task's cgroup namespace on the

 * specified hierarchy

 look up cgroup associated with given css_set on the specified hierarchy */

/*

 * Return the cgroup for "task" from the given hierarchy. Must be

 * called with cgroup_mutex and css_set_lock held.

	/*

	 * No need to lock the task - since we hold css_set_lock the

	 * task can't change groups.

/*

 * A task must hold cgroup_mutex to modify cgroups.

 *

 * Any task can increment and decrement the count field without lock.

 * So in general, code holding cgroup_mutex can't rely on the count

 * field not changing.  However, if the count goes to zero, then only

 * cgroup_attach_task() can increment it again.  Because a count of zero

 * means that no tasks are currently attached, therefore there is no

 * way a task attached to that cgroup can fork (the other way to

 * increment the count).  So code holding cgroup_mutex can safely

 * assume that if the count is zero, it will stay zero. Similarly, if

 * a task holds cgroup_mutex on a cgroup with zero count, it

 * knows that the cgroup won't be removed, as cgroup_rmdir()

 * needs that mutex.

 *

 * A cgroup can only be deleted if both its 'count' of using tasks

 * is zero, and its list of 'children' cgroups is empty.  Since all

 * tasks in the system use _some_ cgroup, and since there is always at

 * least one task in the system (init, pid == 1), therefore, root cgroup

 * always has either children cgroups and/or using tasks.  So we don't

 * need a special hack to ensure that root cgroup cannot be deleted.

 *

 * P.S.  One more locking exception.  RCU is used to guard the

 * update of a tasks cgroup pointer by cgroup_attach_task()

/**

 * cgroup_file_mode - deduce file mode of a control file

 * @cft: the control file in question

 *

 * S_IRUGO for read, S_IWUSR for write.

/**

 * cgroup_calc_subtree_ss_mask - calculate subtree_ss_mask

 * @subtree_control: the new subtree_control mask to consider

 * @this_ss_mask: available subsystems

 *

 * On the default hierarchy, a subsystem may request other subsystems to be

 * enabled together through its ->depends_on mask.  In such cases, more

 * subsystems than specified in "cgroup.subtree_control" may be enabled.

 *

 * This function calculates which subsystems need to be enabled if

 * @subtree_control is to be applied while restricted to @this_ss_mask.

		/*

		 * Mask out subsystems which aren't available.  This can

		 * happen only if some depended-upon subsystems were bound

		 * to non-default hierarchies.

/**

 * cgroup_kn_unlock - unlocking helper for cgroup kernfs methods

 * @kn: the kernfs_node being serviced

 *

 * This helper undoes cgroup_kn_lock_live() and should be invoked before

 * the method finishes if locking succeeded.  Note that once this function

 * returns the cgroup returned by cgroup_kn_lock_live() may become

 * inaccessible any time.  If the caller intends to continue to access the

 * cgroup, it should pin it before invoking this function.

/**

 * cgroup_kn_lock_live - locking helper for cgroup kernfs methods

 * @kn: the kernfs_node being serviced

 * @drain_offline: perform offline draining on the cgroup

 *

 * This helper is to be used by a cgroup kernfs method currently servicing

 * @kn.  It breaks the active protection, performs cgroup locking and

 * verifies that the associated cgroup is alive.  Returns the cgroup if

 * alive; otherwise, %NULL.  A successful return should be undone by a

 * matching cgroup_kn_unlock() invocation.  If @drain_offline is %true, the

 * cgroup is drained of offlining csses before return.

 *

 * Any cgroup kernfs method implementation which requires locking the

 * associated cgroup should use this helper.  It avoids nesting cgroup

 * locking under kernfs active protection and allows all kernfs operations

 * including self-removal.

	/*

	 * We're gonna grab cgroup_mutex which nests outside kernfs

	 * active_ref.  cgroup liveliness check alone provides enough

	 * protection against removal.  Ensure @cgrp stays accessible and

	 * break the active_ref protection.

/**

 * css_clear_dir - remove subsys files in a cgroup directory

 * @css: target css

/**

 * css_populate_dir - create subsys files in a cgroup directory

 * @css: target css

 *

 * On failure, no file is added.

		/*

		 * If @ss has non-root csses attached to it, can't move.

		 * If @ss is an implicit controller, it is exempt from this

		 * rule and can be stolen.

 can't move between two non-dummy roots either */

		/*

		 * Collect ssid's that need to be disabled from default

		 * hierarchy.

		/*

		 * Controllers from default hierarchy that need to be rebound

		 * are all disabled together in one go.

 disable from the source */

 rebind */

 default hierarchy doesn't enable controllers by default */

	/*

	 * We're accessing css_set_count without locking css_set_lock here,

	 * but that's OK - it can only be increased by someone holding

	 * cgroup_lock, and that's us.  Later rebinding may disable

	 * controllers on the default hierarchy and thus create new csets,

	 * which can't be more than the existing ones.  Allocate 2x.

	/*

	 * There must be no failure case after here, since rebinding takes

	 * care of subsystems' refcounts, which are explicitly dropped in

	 * the failure exit path.

	/*

	 * Link the root cgroup in this hierarchy into all the css_set

	 * objects.

	/*

	 * In non-init cgroup namespace, instead of root cgroup's dentry,

	 * we return the dentry corresponding to the cgroupns->root_cgrp.

/*

 * Destroy a cgroup filesystem context.

/*

 * Initialise the cgroup filesystem creation/reconfiguration context.  Notably,

 * we select the namespace we're going to use.

	/*

	 * If @root doesn't have any children, start killing it.

	 * This prevents new mounts by disabling percpu_ref_tryget_live().

	 *

	 * And don't kill the default root.

/*

 * This is ugly, but preserves the userspace API for existing cpuset

 * users. If someone tries to mount the "cpuset" filesystem, we

 * silently switch it to mount "cgroup" instead

/**

 * task_cgroup_path - cgroup path of a task in the first cgroup hierarchy

 * @task: target task

 * @buf: the buffer to write the path into

 * @buflen: the length of the buffer

 *

 * Determine @task's cgroup on the first (the one with the lowest non-zero

 * hierarchy_id) cgroup hierarchy and copy its path into @buf.  This

 * function grabs cgroup_mutex and shouldn't be used inside locks used by

 * cgroup controller callbacks.

 *

 * Return value is the same as kernfs_path().

 if no hierarchy exists, everyone is in "/" */

/**

 * cgroup_migrate_add_task - add a migration target task to a migration context

 * @task: target task

 * @mgctx: target migration context

 *

 * Add @task, which is a migration target, to @mgctx->tset.  This function

 * becomes noop if @task doesn't need to be migrated.  @task's css_set

 * should have been added as a migration source and @task->cg_list will be

 * moved from the css_set's tasks list to mg_tasks one.

 @task either already exited or can't exit until the end */

 cgroup_threadgroup_rwsem protects racing against forks */

/**

 * cgroup_taskset_first - reset taskset and return the first task

 * @tset: taskset of interest

 * @dst_cssp: output variable for the destination css

 *

 * @tset iteration is initialized and the first task is returned.

/**

 * cgroup_taskset_next - iterate to the next task in taskset

 * @tset: taskset of interest

 * @dst_cssp: output variable for the destination css

 *

 * Return the next task in @tset.  Iteration must have been initialized

 * with cgroup_taskset_first().

			/*

			 * This function may be called both before and

			 * after cgroup_taskset_migrate().  The two cases

			 * can be distinguished by looking at whether @cset

			 * has its ->mg_dst_cset set.

/**

 * cgroup_migrate_execute - migrate a taskset

 * @mgctx: migration context

 *

 * Migrate tasks in @mgctx as setup by migration preparation functions.

 * This function fails iff one of the ->can_attach callbacks fails and

 * guarantees that either all or none of the tasks in @mgctx are migrated.

 * @mgctx is consumed regardless of success.

 check that we can legitimately attach to the cgroup */

	/*

	 * Now that we're guaranteed success, proceed to move all tasks to

	 * the new cgroup.  There are no failure cases after here, so this

	 * is the commit point.

			/*

			 * If the source or destination cgroup is frozen,

			 * the task might require to change its state.

	/*

	 * Migration is committed, all target tasks are now on dst_csets.

	 * Nothing is sensitive to fork() after this point.  Notify

	 * controllers that migration is complete.

	/*

	 * Re-initialize the cgroup_taskset structure in case it is reused

	 * again in another cgroup_migrate_add_task()/cgroup_migrate_execute()

	 * iteration.

/**

 * cgroup_migrate_vet_dst - verify whether a cgroup can be migration destination

 * @dst_cgrp: destination cgroup to test

 *

 * On the default hierarchy, except for the mixable, (possible) thread root

 * and threaded cgroups, subtree_control must be zero for migration

 * destination cgroups with tasks so that child cgroups don't compete

 * against tasks.

 v1 doesn't have any restriction */

 verify @dst_cgrp can host resources */

 mixables don't care */

	/*

	 * If @dst_cgrp is already or can become a thread root or is

	 * threaded, it doesn't matter.

 apply no-internal-process constraint */

/**

 * cgroup_migrate_finish - cleanup after attach

 * @mgctx: migration context

 *

 * Undo cgroup_migrate_add_src() and cgroup_migrate_prepare_dst().  See

 * those functions for details.

/**

 * cgroup_migrate_add_src - add a migration source css_set

 * @src_cset: the source css_set to add

 * @dst_cgrp: the destination cgroup

 * @mgctx: migration context

 *

 * Tasks belonging to @src_cset are about to be migrated to @dst_cgrp.  Pin

 * @src_cset and add it to @mgctx->src_csets, which should later be cleaned

 * up by cgroup_migrate_finish().

 *

 * This function may be called without holding cgroup_threadgroup_rwsem

 * even if the target is a process.  Threads may be created and destroyed

 * but as long as cgroup_mutex is not dropped, no new css_set can be put

 * into play and the preloaded css_sets are guaranteed to cover all

 * migrations.

	/*

	 * If ->dead, @src_set is associated with one or more dead cgroups

	 * and doesn't contain any migratable tasks.  Ignore it early so

	 * that the rest of migration path doesn't get confused by it.

/**

 * cgroup_migrate_prepare_dst - prepare destination css_sets for migration

 * @mgctx: migration context

 *

 * Tasks are about to be moved and all the source css_sets have been

 * preloaded to @mgctx->preloaded_src_csets.  This function looks up and

 * pins all destination css_sets, links each to its source, and append them

 * to @mgctx->preloaded_dst_csets.

 *

 * This function must be called after cgroup_migrate_add_src() has been

 * called on each migration source css_set.  After migration is performed

 * using cgroup_migrate(), cgroup_migrate_finish() must be called on

 * @mgctx.

 look up the dst cset for each src cset and link it to src */

		/*

		 * If src cset equals dst, it's noop.  Drop the src.

		 * cgroup_migrate() will skip the cset too.  Note that we

		 * can't handle src == dst as some nodes are used by both.

/**

 * cgroup_migrate - migrate a process or task to a cgroup

 * @leader: the leader of the process or the task to migrate

 * @threadgroup: whether @leader points to the whole process or a single task

 * @mgctx: migration context

 *

 * Migrate a process or task denoted by @leader.  If migrating a process,

 * the caller must be holding cgroup_threadgroup_rwsem.  The caller is also

 * responsible for invoking cgroup_migrate_add_src() and

 * cgroup_migrate_prepare_dst() on the targets before invoking this

 * function and following up with cgroup_migrate_finish().

 *

 * As long as a controller's ->can_attach() doesn't fail, this function is

 * guaranteed to succeed.  This means that, excluding ->can_attach()

 * failure, when migrating multiple targets, the success or failure can be

 * decided for all targets by invoking group_migrate_prepare_dst() before

 * actually starting migrating.

	/*

	 * Prevent freeing of tasks while we take a snapshot. Tasks that are

	 * already PF_EXITING could be freed from underneath us unless we

	 * take an rcu_read_lock.

/**

 * cgroup_attach_task - attach a task or a whole threadgroup to a cgroup

 * @dst_cgrp: the cgroup to attach to

 * @leader: the task or the leader of the threadgroup to be attached

 * @threadgroup: attach the whole threadgroup?

 *

 * Call holding cgroup_mutex and cgroup_threadgroup_rwsem.

 look up all src csets */

 prepare dst csets and commit */

	/*

	 * If we migrate a single thread, we don't care about threadgroup

	 * stability. If the thread is `current`, it won't exit(2) under our

	 * hands or change PID through exec(2). We exclude

	 * cgroup_update_dfl_csses and other cgroup_{proc,thread}s_write

	 * callers by cgroup_mutex.

	 * Therefore, we can skip the global lock.

	/*

	 * kthreads may acquire PF_NO_SETAFFINITY during initialization.

	 * If userland migrates such a kthread to a non-root cgroup, it can

	 * become trapped in a cpuset, or RT kthread may be born in a

	 * cgroup with no rt_runtime allocated.  Just say no.

 release reference from cgroup_procs_write_start() */

 show controllers which are enabled from the parent */

 show controllers which are enabled for a given cgroup's children */

/**

 * cgroup_update_dfl_csses - update css assoc of a subtree in default hierarchy

 * @cgrp: root of the subtree to update csses for

 *

 * @cgrp's control masks have changed and its subtree's css associations

 * need to be updated accordingly.  This function looks up all css_sets

 * which are attached to the subtree, creates the matching updated css_sets

 * and migrates the tasks to the new ones.

 look up all csses currently attached to @cgrp's subtree */

 NULL dst indicates self on default hierarchy */

 all tasks in src_csets need to be migrated */

/**

 * cgroup_lock_and_drain_offline - lock cgroup_mutex and drain offlined csses

 * @cgrp: root of the target subtree

 *

 * Because css offlining is asynchronous, userland may try to re-enable a

 * controller while the previous css is still around.  This function grabs

 * cgroup_mutex and drains the previous css instances of @cgrp's subtree.

/**

 * cgroup_save_control - save control masks and dom_cgrp of a subtree

 * @cgrp: root of the target subtree

 *

 * Save ->subtree_control, ->subtree_ss_mask and ->dom_cgrp to the

 * respective old_ prefixed fields for @cgrp's subtree including @cgrp

 * itself.

/**

 * cgroup_propagate_control - refresh control masks of a subtree

 * @cgrp: root of the target subtree

 *

 * For @cgrp and its subtree, ensure ->subtree_ss_mask matches

 * ->subtree_control and propagate controller availability through the

 * subtree so that descendants don't have unavailable controllers enabled.

/**

 * cgroup_restore_control - restore control masks and dom_cgrp of a subtree

 * @cgrp: root of the target subtree

 *

 * Restore ->subtree_control, ->subtree_ss_mask and ->dom_cgrp from the

 * respective old_ prefixed fields for @cgrp's subtree including @cgrp

 * itself.

/**

 * cgroup_apply_control_enable - enable or show csses according to control

 * @cgrp: root of the target subtree

 *

 * Walk @cgrp's subtree and create new csses or make the existing ones

 * visible.  A css is created invisible if it's being implicitly enabled

 * through dependency.  An invisible css is made visible when the userland

 * explicitly enables it.

 *

 * Returns 0 on success, -errno on failure.  On failure, csses which have

 * been processed already aren't cleaned up.  The caller is responsible for

 * cleaning up with cgroup_apply_control_disable().

/**

 * cgroup_apply_control_disable - kill or hide csses according to control

 * @cgrp: root of the target subtree

 *

 * Walk @cgrp's subtree and kill and hide csses so that they match

 * cgroup_ss_mask() and cgroup_visible_mask().

 *

 * A css is hidden when the userland requests it to be disabled while other

 * subsystems are still depending on it.  The css must not actively control

 * resources and be in the vanilla state if it's made visible again later.

 * Controllers which may be depended upon should provide ->css_reset() for

 * this purpose.

/**

 * cgroup_apply_control - apply control mask updates to the subtree

 * @cgrp: root of the target subtree

 *

 * subsystems can be enabled and disabled in a subtree using the following

 * steps.

 *

 * 1. Call cgroup_save_control() to stash the current state.

 * 2. Update ->subtree_control masks in the subtree as desired.

 * 3. Call cgroup_apply_control() to apply the changes.

 * 4. Optionally perform other related operations.

 * 5. Call cgroup_finalize_control() to finish up.

 *

 * This function implements step 3 and propagates the mask changes

 * throughout @cgrp's subtree, updates csses accordingly and perform

 * process migrations.

	/*

	 * At this point, cgroup_e_css_by_mask() results reflect the new csses

	 * making the following cgroup_update_dfl_csses() properly update

	 * css associations of all tasks in the subtree.

/**

 * cgroup_finalize_control - finalize control mask update

 * @cgrp: root of the target subtree

 * @ret: the result of the update

 *

 * Finalize control mask update.  See cgroup_apply_control() for more info.

 if nothing is getting enabled, nothing to worry about */

 can @cgrp host any resources? */

 mixables don't care */

 can't enable domain controllers inside a thread subtree */

		/*

		 * Threaded controllers can handle internal competitions

		 * and are always allowed inside a (prospective) thread

		 * subtree.

	/*

	 * Controllers can't be enabled for a cgroup with tasks to avoid

	 * child cgroups competing against tasks.

 change the enabled child controllers for a cgroup in the default hierarchy */

	/*

	 * Parse input - space separated list of subsystem names prefixed

	 * with either + or -.

 a child has it enabled? */

 save and update control masks and prepare csses */

/**

 * cgroup_enable_threaded - make @cgrp threaded

 * @cgrp: the target cgroup

 *

 * Called when "threaded" is written to the cgroup.type interface file and

 * tries to make @cgrp threaded and join the parent's resource domain.

 * This function is never called on the root cgroup as cgroup.type doesn't

 * exist on it.

 noop if already threaded */

	/*

	 * If @cgroup is populated or has domain controllers enabled, it

	 * can't be switched.  While the below cgroup_can_be_thread_root()

	 * test can catch the same conditions, that's only when @parent is

	 * not mixable, so let's check it explicitly.

 we're joining the parent's domain, ensure its validity */

	/*

	 * The following shouldn't cause actual migrations and should

	 * always succeed.

 only switching to threaded mode is supported */

 drain dying csses before we re-apply (threaded) subtree control */

 threaded can only be enabled */

 CONFIG_PSI */

 CONFIG_PSI */

 Ignore kernel threads here. */

 Skip tasks that are already dying. */

	/*

	 * Killing is a process directed operation, i.e. the whole thread-group

	 * is taken down so act like we do for cgroup.procs and only make this

	 * writable in non-threaded cgroups.

	/*

	 * If namespaces are delegation boundaries, disallow writes to

	 * files in an non-init namespace root from inside the namespace

	 * except for the files explicitly marked delegatable -

	 * cgroup.procs and cgroup.subtree_control.

	/*

	 * kernfs guarantees that a file isn't deleted with operations in

	 * flight, which means that the matching css is and stays alive and

	 * doesn't need to be pinned.  The RCU locking is not necessary

	 * either.  It's just for the convenience of using cgroup_css().

 set uid and gid of cgroup dirs and files to that of the creator */

/**

 * cgroup_addrm_files - add or remove files to a cgroup directory

 * @css: the target css

 * @cgrp: the target cgroup (usually css->cgroup)

 * @cfts: array of cftypes to be added

 * @is_add: whether to add or remove

 *

 * Depending on @is_add, add or remove files defined by @cfts on @cgrp.

 * For removals, this function never fails.

 does cft->flags tell us to skip this file on @cgrp? */

 add/rm files for all cgroups created before */

 free copy for custom atomic_write_len, see init_cftypes() */

 revert flags set by cgroup core while adding @cfts */

		/*

		 * Ugh... if @cft wants a custom max_write_len, we need to

		 * make a copy of kf_ops to set its atomic_write_len.

/**

 * cgroup_rm_cftypes - remove an array of cftypes from a subsystem

 * @cfts: zero-length name terminated array of cftypes

 *

 * Unregister @cfts.  Files described by @cfts are removed from all

 * existing cgroups and all future cgroups won't have them either.  This

 * function can be called anytime whether @cfts' subsys is attached or not.

 *

 * Returns 0 on successful unregistration, -ENOENT if @cfts is not

 * registered.

/**

 * cgroup_add_cftypes - add an array of cftypes to a subsystem

 * @ss: target cgroup subsystem

 * @cfts: zero-length name terminated array of cftypes

 *

 * Register @cfts to @ss.  Files described by @cfts are created for all

 * existing cgroups to which @ss is attached and all future cgroups will

 * have them too.  This function can be called anytime whether @ss is

 * attached or not.

 *

 * Returns 0 on successful registration, -errno on failure.  Note that this

 * function currently returns 0 as long as @cfts registration is successful

 * even if some file creation attempts on existing cgroups fail.

/**

 * cgroup_add_dfl_cftypes - add an array of cftypes for default hierarchy

 * @ss: target cgroup subsystem

 * @cfts: zero-length name terminated array of cftypes

 *

 * Similar to cgroup_add_cftypes() but the added files are only used for

 * the default hierarchy.

/**

 * cgroup_add_legacy_cftypes - add an array of cftypes for legacy hierarchies

 * @ss: target cgroup subsystem

 * @cfts: zero-length name terminated array of cftypes

 *

 * Similar to cgroup_add_cftypes() but the added files are only used for

 * the legacy hierarchies.

/**

 * cgroup_file_notify - generate a file modified event for a cgroup_file

 * @cfile: target cgroup_file

 *

 * @cfile must have been obtained by setting cftype->file_offset.

/**

 * css_next_child - find the next child of a given css

 * @pos: the current position (%NULL to initiate traversal)

 * @parent: css whose children to walk

 *

 * This function returns the next child of @parent and should be called

 * under either cgroup_mutex or RCU read lock.  The only requirement is

 * that @parent and @pos are accessible.  The next sibling is guaranteed to

 * be returned regardless of their states.

 *

 * If a subsystem synchronizes ->css_online() and the start of iteration, a

 * css which finished ->css_online() is guaranteed to be visible in the

 * future iterations and will stay visible until the last reference is put.

 * A css which hasn't finished ->css_online() or already finished

 * ->css_offline() may show up during traversal.  It's each subsystem's

 * responsibility to synchronize against on/offlining.

	/*

	 * @pos could already have been unlinked from the sibling list.

	 * Once a cgroup is removed, its ->sibling.next is no longer

	 * updated when its next sibling changes.  CSS_RELEASED is set when

	 * @pos is taken off list, at which time its next pointer is valid,

	 * and, as releases are serialized, the one pointed to by the next

	 * pointer is guaranteed to not have started release yet.  This

	 * implies that if we observe !CSS_RELEASED on @pos in this RCU

	 * critical section, the one pointed to by its next pointer is

	 * guaranteed to not have finished its RCU grace period even if we

	 * have dropped rcu_read_lock() in-between iterations.

	 *

	 * If @pos has CSS_RELEASED set, its next pointer can't be

	 * dereferenced; however, as each css is given a monotonically

	 * increasing unique serial number and always appended to the

	 * sibling list, the next one can be found by walking the parent's

	 * children until the first css with higher serial number than

	 * @pos's.  While this path can be slower, it happens iff iteration

	 * races against release and the race window is very small.

	/*

	 * @next, if not pointing to the head, can be dereferenced and is

	 * the next sibling.

/**

 * css_next_descendant_pre - find the next descendant for pre-order walk

 * @pos: the current position (%NULL to initiate traversal)

 * @root: css whose descendants to walk

 *

 * To be used by css_for_each_descendant_pre().  Find the next descendant

 * to visit for pre-order traversal of @root's descendants.  @root is

 * included in the iteration and the first node to be visited.

 *

 * While this function requires cgroup_mutex or RCU read locking, it

 * doesn't require the whole traversal to be contained in a single critical

 * section.  This function will return the correct next descendant as long

 * as both @pos and @root are accessible and @pos is a descendant of @root.

 *

 * If a subsystem synchronizes ->css_online() and the start of iteration, a

 * css which finished ->css_online() is guaranteed to be visible in the

 * future iterations and will stay visible until the last reference is put.

 * A css which hasn't finished ->css_online() or already finished

 * ->css_offline() may show up during traversal.  It's each subsystem's

 * responsibility to synchronize against on/offlining.

 if first iteration, visit @root */

 visit the first child if exists */

 no child, visit my or the closest ancestor's next sibling */

/**

 * css_rightmost_descendant - return the rightmost descendant of a css

 * @pos: css of interest

 *

 * Return the rightmost descendant of @pos.  If there's no descendant, @pos

 * is returned.  This can be used during pre-order traversal to skip

 * subtree of @pos.

 *

 * While this function requires cgroup_mutex or RCU read locking, it

 * doesn't require the whole traversal to be contained in a single critical

 * section.  This function will return the correct rightmost descendant as

 * long as @pos is accessible.

 ->prev isn't RCU safe, walk ->next till the end */

/**

 * css_next_descendant_post - find the next descendant for post-order walk

 * @pos: the current position (%NULL to initiate traversal)

 * @root: css whose descendants to walk

 *

 * To be used by css_for_each_descendant_post().  Find the next descendant

 * to visit for post-order traversal of @root's descendants.  @root is

 * included in the iteration and the last node to be visited.

 *

 * While this function requires cgroup_mutex or RCU read locking, it

 * doesn't require the whole traversal to be contained in a single critical

 * section.  This function will return the correct next descendant as long

 * as both @pos and @cgroup are accessible and @pos is a descendant of

 * @cgroup.

 *

 * If a subsystem synchronizes ->css_online() and the start of iteration, a

 * css which finished ->css_online() is guaranteed to be visible in the

 * future iterations and will stay visible until the last reference is put.

 * A css which hasn't finished ->css_online() or already finished

 * ->css_offline() may show up during traversal.  It's each subsystem's

 * responsibility to synchronize against on/offlining.

 if first iteration, visit leftmost descendant which may be @root */

 if we visited @root, we're done */

 if there's an unvisited sibling, visit its leftmost descendant */

 no sibling left, visit parent */

/**

 * css_has_online_children - does a css have online children

 * @css: the target css

 *

 * Returns %true if @css has any online children; otherwise, %false.  This

 * function can be called from any context but the caller is responsible

 * for synchronizing against on/offlining as necessary.

 find the next threaded cset */

 find the next cset */

 initialize threaded css_set walking */

/**

 * css_task_iter_advance_css_set - advance a task iterator to the next css_set

 * @it: the iterator to advance

 *

 * Advance @it to the next css_set to walk.

 Advance to the next non-empty css_set and find first non-empty tasks list*/

	/*

	 * We don't keep css_sets locked across iteration steps and thus

	 * need to take steps to ensure that iteration can be resumed after

	 * the lock is re-acquired.  Iteration is performed at two levels -

	 * css_sets and tasks in them.

	 *

	 * Once created, a css_set never leaves its cgroup lists, so a

	 * pinned css_set is guaranteed to stay put and we can resume

	 * iteration afterwards.

	 *

	 * Tasks may leave @cset across iteration steps.  This is resolved

	 * by registering each iterator with the css_set currently being

	 * walked and making css_set_move_task() advance iterators whose

	 * next task is leaving.

		/*

		 * Advance iterator to find next entry. We go through cset

		 * tasks, mg_tasks and dying_tasks, when consumed we move onto

		 * the next cset.

 called from start, proceed to the first cset */

 if PROCS, skip over tasks which aren't group leaders */

 and dying leaders w/o live member threads */

 skip all dying ones */

/**

 * css_task_iter_start - initiate task iteration

 * @css: the css to walk tasks of

 * @flags: CSS_TASK_ITER_* flags

 * @it: the task iterator to use

 *

 * Initiate iteration through the tasks of @css.  The caller can call

 * css_task_iter_next() to walk through the tasks until the function

 * returns NULL.  On completion of iteration, css_task_iter_end() must be

 * called.

/**

 * css_task_iter_next - return the next task for the iterator

 * @it: the task iterator being iterated

 *

 * The "next" function for task iteration.  @it should have been

 * initialized via css_task_iter_start().  Returns NULL when the iteration

 * reaches the end.

 @it may be half-advanced by skips, finish advancing */

/**

 * css_task_iter_end - finish task iteration

 * @it: the task iterator to finish

 *

 * Finish task iteration started by css_task_iter_start().

	/*

	 * When a seq_file is seeked, it's always traversed sequentially

	 * from position 0, so we can simply keep iterating on !0 *pos.

	/*

	 * All processes of a threaded subtree belong to the domain cgroup

	 * of the subtree.  Only threads can be distributed across the

	 * subtree.  Reject reads on cgroup.procs in the subtree proper.

	 * They're always empty anyway.

 find the common ancestor */

 %current should be authorized to migrate to the common ancestor */

	/*

	 * If namespaces are delegation boundaries, %current must be able

	 * to see both source and destination cgroups from its namespace.

 find the source cgroup */

 process and thread migrations follow same delegation rule */

 cgroup core interface files for the default hierarchy */

 CONFIG_PSI */

 terminate */

/*

 * css destruction is four-stage process.

 *

 * 1. Destruction starts.  Killing of the percpu_ref is initiated.

 *    Implemented in kill_css().

 *

 * 2. When the percpu_ref is confirmed to be visible as killed on all CPUs

 *    and thus css_tryget_online() is guaranteed to fail, the css can be

 *    offlined by invoking offline_css().  After offlining, the base ref is

 *    put.  Implemented in css_killed_work_fn().

 *

 * 3. When the percpu_ref reaches zero, the only possible remaining

 *    accessors are inside RCU read sections.  css_release() schedules the

 *    RCU callback.

 *

 * 4. After the grace period, the css can be freed.  Implemented in

 *    css_free_work_fn().

 *

 * It is actually hairier because both step 2 and 4 require process context

 * and thus involve punting to css->destroy_work adding two additional

 * steps to the already complex sequence.

 css free path */

 cgroup free path */

			/*

			 * We get a ref to the parent, and put the ref when

			 * this cgroup is being freed, so it's guaranteed

			 * that the parent won't be destroyed before its

			 * children.

			/*

			 * This is root cgroup's refcnt reaching zero,

			 * which indicates that the root should be

			 * released.

 css release path */

 cgroup release path */

		/*

		 * There are two control paths which try to determine

		 * cgroup from dentry without going through kernfs -

		 * cgroupstats_build() and css_tryget_online_from_dir().

		 * Those are supported by RCU protecting clearing of

		 * cgrp->kn->priv backpointer.

 invoke ->css_online() on a new CSS and mark it online if successful */

 if the CSS is online, invoke ->css_offline() on it and mark it offline */

/**

 * css_create - create a cgroup_subsys_state

 * @cgrp: the cgroup new css will be associated with

 * @ss: the subsys of new css

 *

 * Create a new css associated with @cgrp - @ss pair.  On success, the new

 * css is online and installed in @cgrp.  This function doesn't create the

 * interface files.  Returns 0 on success, -errno on failure.

 @css is ready to be brought online now, make it visible */

/*

 * The returned cgroup is fully initialized including its control mask, but

 * it isn't associated with its kernfs_node and doesn't have the control

 * mask applied.

 allocate the cgroup and its ID, 0 is reserved for the root */

 create the directory */

	/*

	 * New cgroup inherits effective freeze counter, and

	 * if the parent has to be frozen, the child has too.

		/*

		 * Set the CGRP_FREEZE flag, so when a process will be

		 * attached to the child cgroup, it will become frozen.

		 * At this point the new cgroup is unpopulated, so we can

		 * consider it frozen immediately.

			/*

			 * If the new cgroup is frozen, all ancestor cgroups

			 * get a new frozen descendant, but their state can't

			 * change because of this.

 allocation complete, commit to creation */

	/*

	 * On the default hierarchy, a child doesn't automatically inherit

	 * subtree_control from the parent.  Each is configured manually.

 do not accept '\n' to prevent making /proc/<pid>/cgroup unparsable */

	/*

	 * This extra ref will be put in cgroup_free_fn() and guarantees

	 * that @cgrp->kn is always accessible.

 let's create and online css's */

/*

 * This is called when the refcnt of a css is confirmed to be killed.

 * css_tryget_online() is now guaranteed to fail.  Tell the subsystem to

 * initiate destruction and put the css ref from kill_css().

 @css can't go away while we're holding cgroup_mutex */

 css kill confirmation processing requires process context, bounce */

/**

 * kill_css - destroy a css

 * @css: css to destroy

 *

 * This function initiates destruction of @css by removing cgroup interface

 * files and putting its base reference.  ->css_offline() will be invoked

 * asynchronously once css_tryget_online() is guaranteed to fail and when

 * the reference count reaches zero, @css will be released.

	/*

	 * This must happen before css is disassociated with its cgroup.

	 * See seq_css() for details.

	/*

	 * Killing would put the base ref, but we need to keep it alive

	 * until after ->css_offline().

	/*

	 * cgroup core guarantees that, by the time ->css_offline() is

	 * invoked, no new css reference will be given out via

	 * css_tryget_online().  We can't simply call percpu_ref_kill() and

	 * proceed to offlining css's because percpu_ref_kill() doesn't

	 * guarantee that the ref is seen as killed on all CPUs on return.

	 *

	 * Use percpu_ref_kill_and_confirm() to get notifications as each

	 * css is confirmed to be seen as killed on all CPUs.

/**

 * cgroup_destroy_locked - the first stage of cgroup destruction

 * @cgrp: cgroup to be destroyed

 *

 * css's make use of percpu refcnts whose killing latency shouldn't be

 * exposed to userland and are RCU protected.  Also, cgroup core needs to

 * guarantee that css_tryget_online() won't succeed by the time

 * ->css_offline() is invoked.  To satisfy all the requirements,

 * destruction is implemented in the following two steps.

 *

 * s1. Verify @cgrp can be destroyed and mark it dying.  Remove all

 *     userland visible parts and start killing the percpu refcnts of

 *     css's.  Set up so that the next stage will be kicked off once all

 *     the percpu refcnts are confirmed to be killed.

 *

 * s2. Invoke ->css_offline(), mark the cgroup dead and proceed with the

 *     rest of destruction.  Once all cgroup references are gone, the

 *     cgroup is RCU-freed.

 *

 * This function implements s1.  After this step, @cgrp is gone as far as

 * the userland is concerned and a new cgroup with the same name may be

 * created.  As cgroup doesn't care about the names internally, this

 * doesn't cause any problem.

	/*

	 * Only migration can raise populated from zero and we're already

	 * holding cgroup_mutex.

	/*

	 * Make sure there's no live children.  We can't test emptiness of

	 * ->self.children as dead children linger on it while being

	 * drained; otherwise, "rmdir parent/child parent" may fail.

	/*

	 * Mark @cgrp and the associated csets dead.  The former prevents

	 * further task migration and child creation by disabling

	 * cgroup_lock_live_group().  The latter makes the csets ignored by

	 * the migration path.

 initiate massacre of all css's */

 clear and remove @cgrp dir, @cgrp has an extra ref on its kn */

		/*

		 * If the dying cgroup is frozen, decrease frozen descendants

		 * counters of ancestor cgroups.

 put the base reference */

 Create the root cgroup state for this subsystem */

 We don't handle early failures gracefully */

	/*

	 * Root csses are never destroyed and we can't initialize

	 * percpu_ref during early init.  Disable refcnting.

 allocation can't be done safely during early init */

	/* Update the init_css_set to contain a subsys

	 * pointer to this state - since the subsystem is

	 * newly registered, all tasks and hence the

	/* At system boot, before all subsystems have been

	 * registered, no tasks have been forked, so we don't

/**

 * cgroup_init_early - cgroup initialization at system boot

 *

 * Initialize cgroups at system boot, and initialize any

 * subsystems that request early init.

/**

 * cgroup_init - cgroup initialization

 *

 * Register cgroup filesystem and /proc file, and initialize

 * any subsystems that didn't request early init.

	/*

	 * The latency of the synchronize_rcu() is too high for cgroups,

	 * avoid it at the cost of forcing all readers into the slow path.

	/*

	 * Add init_css_set to the hash table so that dfl_root can link to

	 * it during init.

		/*

		 * Setting dfl_root subsys_mask needs to consider the

		 * disabled flag and cftype registration needs kmalloc,

		 * both of which aren't available during early_init.

 implicit controllers must be threaded too */

 init_css_set.subsys[] has been updated, re-hash */

	/*

	 * There isn't much point in executing destruction path in

	 * parallel.  Good chunk is serialized with cgroup_mutex anyway.

	 * Use 1 for @max_active.

	 *

	 * We would prefer to do this in cgroup_init() above, but that

	 * is called before init_workqueues(): so leave this until after.

/*

 * cgroup_get_from_id : get the cgroup associated with cgroup id

 * @id: cgroup id

 * On success return the cgrp, on failure return NULL

/*

 * proc_cgroup_show()

 *  - Print task's cgroup paths into seq_file, one line for each hierarchy

 *  - Used for /proc/<pid>/cgroup.

		/*

		 * On traditional hierarchies, all zombie tasks show up as

		 * belonging to the root cgroup.  On the default hierarchy,

		 * while a zombie doesn't show up in "cgroup.procs" and

		 * thus can't be migrated, its /proc/PID/cgroup keeps

		 * reporting the cgroup it belonged to before exiting.  If

		 * the cgroup is removed before the zombie is reaped,

		 * " (deleted)" is appended to the cgroup path.

/**

 * cgroup_fork - initialize cgroup related fields during copy_process()

 * @child: pointer to task_struct of forking parent process.

 *

 * A task is associated with the init_css_set until cgroup_post_fork()

 * attaches it to the target css_set.

/**

 * cgroup_css_set_fork - find or create a css_set for a child process

 * @kargs: the arguments passed to create the child process

 *

 * This functions finds or creates a new css_set which the child

 * process will be attached to in cgroup_post_fork(). By default,

 * the child process will be given the same css_set as its parent.

 *

 * If CLONE_INTO_CGROUP is specified this function will try to find an

 * existing css_set which includes the requested cgroup and if not create

 * a new css_set that the child will be attached to later. If this function

 * succeeds it will hold cgroup_threadgroup_rwsem on return. If

 * CLONE_INTO_CGROUP is requested this function will grab cgroup mutex

 * before grabbing cgroup_threadgroup_rwsem and will hold a reference

 * to the target cgroup.

	/*

	 * Verify that we the target cgroup is writable for us. This is

	 * usually done by the vfs layer but since we're not going through

	 * the vfs layer here we need to do it "manually".

/**

 * cgroup_css_set_put_fork - drop references we took during fork

 * @kargs: the arguments passed to create the child process

 *

 * Drop references to the prepared css_set and target cgroup if

 * CLONE_INTO_CGROUP was requested.

/**

 * cgroup_can_fork - called on a new task before the process is exposed

 * @child: the child process

 *

 * This prepares a new css_set for the child process which the child will

 * be attached to in cgroup_post_fork().

 * This calls the subsystem can_fork() callbacks. If the cgroup_can_fork()

 * callback returns an error, the fork aborts with that error code. This

 * allows for a cgroup subsystem to conditionally allow or deny new forks.

/**

 * cgroup_cancel_fork - called if a fork failed after cgroup_can_fork()

 * @child: the child process

 * @kargs: the arguments passed to create the child process

 *

 * This calls the cancel_fork() callbacks if a fork failed *after*

 * cgroup_can_fork() succeeded and cleans up references we took to

 * prepare a new css_set for the child process in cgroup_can_fork().

/**

 * cgroup_post_fork - finalize cgroup setup for the child process

 * @child: the child process

 *

 * Attach the child process to its css_set calling the subsystem fork()

 * callbacks.

 init tasks are special, only link regular threads */

			/*

			 * If the cgroup has to be frozen, the new task has

			 * too. Let's set the JOBCTL_TRAP_FREEZE jobctl bit to

			 * get the task into the frozen state.

			/*

			 * Calling cgroup_update_frozen() isn't required here,

			 * because it will be called anyway a bit later from

			 * do_freezer_trap(). So we avoid cgroup's transient

			 * switch from the frozen state and back.

		/*

		 * If the cgroup is to be killed notice it now and take the

		 * child down right after we finished preparing it for

		 * userspace.

	/*

	 * Call ss->fork().  This must happen after @child is linked on

	 * css_set; otherwise, @child might change state between ->fork()

	 * and addition to css_set.

 Make the new cset the root_cset of the new cgroup namespace. */

 Cgroup has to be killed so take down child immediately. */

/**

 * cgroup_exit - detach cgroup from exiting task

 * @tsk: pointer to task_struct of exiting process

 *

 * Description: Detach cgroup from @tsk.

 *

 see cgroup_post_fork() for details */

/**

 * css_tryget_online_from_dir - get corresponding css from a cgroup dentry

 * @dentry: directory dentry of interest

 * @ss: subsystem of interest

 *

 * If @dentry is a directory for a cgroup which has @ss enabled on it, try

 * to get the corresponding css and return it.  If such css doesn't exist

 * or can't be pinned, an ERR_PTR value is returned.

 is @dentry a cgroup dir? */

	/*

	 * This path doesn't originate from kernfs and @kn could already

	 * have been or be removed at any point.  @kn->priv is RCU

	 * protected for this access.  See css_release_work_fn() for details.

/**

 * css_from_id - lookup css by id

 * @id: the cgroup id

 * @ss: cgroup subsys to be looked into

 *

 * Returns the css if there's valid one with @id, otherwise returns NULL.

 * Should be called under rcu_read_lock().

/**

 * cgroup_get_from_path - lookup and get a cgroup from its default hierarchy path

 * @path: path on the default hierarchy

 *

 * Find the cgroup at @path on the default hierarchy, increment its

 * reference count and return it.  Returns pointer to the found cgroup on

 * success, ERR_PTR(-ENOENT) if @path doesn't exist or if the cgroup has already

 * been released and ERR_PTR(-ENOTDIR) if @path points to a non-directory.

/**

 * cgroup_get_from_fd - get a cgroup pointer from a fd

 * @fd: fd obtained by open(cgroup2_dir)

 *

 * Find the cgroup from a fd which should be obtained

 * by opening a cgroup directory.  Returns a pointer to the

 * cgroup on success. ERR_PTR is returned if the cgroup

 * cannot be found.

/**

 * cgroup_parse_float - parse a floating number

 * @input: input string

 * @dec_shift: number of decimal digits to shift

 * @v: output

 *

 * Parse a decimal floating point number in @input and store the result in

 * @v with decimal point right shifted @dec_shift times.  For example, if

 * @input is "12.3456" and @dec_shift is 3, *@v will be set to 12345.

 * Returns 0 on success, -errno otherwise.

 *

 * There's nothing cgroup specific about this function except that it's

 * currently the only user.

/*

 * sock->sk_cgrp_data handling.  For more info, see sock_cgroup_data

 * definition in cgroup-defs.h.

 Don't associate the sock with unrelated interrupted task's cgroup. */

	/*

	 * We might be cloning a socket which is left in an empty

	 * cgroup and the cgroup might have already been rmdir'd.

	 * Don't use cgroup_get_live().

 CONFIG_SOCK_CGROUP_DATA */

 CONFIG_SYSFS */

 SPDX-License-Identifier: GPL-2.0

/*

 * Debug controller

 *

 * WARNING: This controller is for cgroup core debugging only.

 * Its interfaces are unstable and subject to changes at any time.

/*

 * debug_taskcount_read - return the number of tasks in a cgroup.

 * @cgrp: the cgroup in question

	/*

	 * Print the css'es stored in the current css_set.

		/*

		 * Print out the proc_cset and threaded_cset relationship

		 * and highlight difference between refcount and task_count.

				/*

				 * Take out the one additional reference in

				 * init_css_set.

 show # of overflowed tasks */

 Show the parent CSS if applicable*/

 terminate */

 terminate */

/*

 * On v2, debug is an implicit controller enabled by "cgroup_debug" boot

 * parameter.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * pidlists linger the following amount before being destroyed.  The goal

 * is avoiding frequent destruction in the middle of consecutive read calls

 * Expiring in the middle is a performance problem not a correctness one.

 * 1 sec should be enough.

 Controllers blocked by the commandline in v1 */

 disable named v1 mounts */

/*

 * pidlist destructions need to be flushed on cgroup destruction.  Use a

 * separate workqueue as flush domain.

 protects cgroup_subsys->release_agent_path */

/**

 * cgroup_attach_task_all - attach task 'tsk' to all cgroups of task 'from'

 * @from: attach to all cgroups of a given task

 * @tsk: the task to be attached

 *

 * Return: %0 on success or a negative errno code on failure

/**

 * cgroup_transfer_tasks - move tasks from one cgroup to another

 * @to: cgroup to which the tasks will be moved

 * @from: cgroup in which the tasks currently reside

 *

 * Locking rules between cgroup_post_fork() and the migration path

 * guarantee that, if a task is forking while being migrated, the new child

 * is guaranteed to be either visible in the source cgroup after the

 * parent's migration is complete or put into the target cgroup.  No task

 * can slip out of migration through forking.

 *

 * Return: %0 on success or a negative errno code on failure

 all tasks in @from are being moved, all csets are source */

	/*

	 * Migrate tasks one-by-one until @from is empty.  This fails iff

	 * ->can_attach() fails.

/*

 * Stuff for reading the 'tasks'/'procs' files.

 *

 * Reading this file can return large amounts of data if a cgroup has

 * *lots* of attached tasks. So it may need several calls to read(),

 * but we cannot guarantee that the information we produce is correct

 * unless we produce it entirely atomically.

 *

 which pidlist file are we talking about? */

/*

 * A pidlist is a list of pids that virtually represents the contents of one

 * of the cgroup files ("procs" or "tasks"). We keep a list of such pidlists,

 * a pair (one each for procs, tasks) for each pid namespace that's relevant

 * to the cgroup.

	/*

	 * used to find which pidlist is wanted. doesn't change as long as

	 * this particular list stays in the list.

 array of xids */

 how many elements the above list has */

 each of these stored in a list by its cgroup */

 pointer to the cgroup we belong to, for list removal purposes */

 for delayed destruction */

/*

 * Used to destroy all pidlists lingering waiting for destroy timer.  None

 * should be left afterwards.

	/*

	 * Destroy iff we didn't get queued again.  The state won't change

	 * as destroy_dwork can only be queued while locked.

/*

 * pidlist_uniq - given a kmalloc()ed list, strip out all duplicate entries

 * Returns the number of unique elements.

	/*

	 * we presume the 0th element is unique, so i starts at 1. trivial

	 * edge cases first; no work needs to be done for either

 src and dest walk down the list; dest counts unique elements */

 find next unique element */

 dest always points to where the next unique element goes */

/*

 * The two pid files - task and cgroup.procs - guaranteed that the result

 * is sorted, which forced this whole pidlist fiasco.  As pid order is

 * different per namespace, each namespace needs differently sorted list,

 * making it impossible to use, for example, single rbtree of member tasks

 * sorted by task pointer.  As pidlists can be fairly large, allocating one

 * per open file is dangerous, so cgroup had to implement shared pool of

 * pidlists keyed by cgroup and namespace.

 don't need task_nsproxy() if we're looking at ourself */

/*

 * find the appropriate pidlist for our purpose (given procs vs tasks)

 * returns with the lock on that pidlist already held, and takes care

 * of the use count, or returns NULL with no locks held if we're out of

 * memory.

 entry not found; create a new one */

 don't need task_nsproxy() if we're looking at ourself */

/*

 * Load a cgroup's pidarray with either procs' tgids or tasks' pids

 used for populating the array */

	/*

	 * If cgroup gets more users after we read count, we won't have

	 * enough space - tough.  This race is indistinguishable to the

	 * caller from the case that the additional cgroup users didn't

	 * show up until sometime later on.

 now, populate the array */

 get tgid or pid for procs or tasks file respectively */

 make sure to only use valid results */

 now sort & (if procs) strip out duplicates */

 store array, freeing old if necessary */

/*

 * seq_file methods for the tasks/procs files. The seq_file position is the

 * next pid to display; the seq_file iterator is a pointer to the pid

 * in the cgroup->l->list array.

	/*

	 * Initially we receive a position value that corresponds to

	 * one more than the last pid shown (or 0 on the first call or

	 * after a seek to the start). Use a binary-search to find the

	 * next pid to display, if any

	/*

	 * !NULL @of->priv indicates that this isn't the first start()

	 * after open.  If the matching pidlist is around, we can use that.

	 * Look for it.  Note that @of->priv can't be used directly.  It

	 * could already have been destroyed.

	/*

	 * Either this is the first start() after open or the matching

	 * pidlist has been destroyed inbetween.  Create a new one.

 If we're off the end of the array, we're done */

 Update the abstract position to be the actual pid that we found */

	/*

	 * Advance to the next pid in the array. If this goes off the

	 * end, we're done

	/*

	 * Even if we're attaching all tasks in the thread group, we only

	 * need to check permissions on one of them.

 cgroup core interface files for the legacy hierarchies */

 terminate */

 Display information about each subsystem and each hierarchy */

	/*

	 * Grab the subsystems state racily. No need to add avenue to

	 * cgroup_mutex contention.

/**

 * cgroupstats_build - build and fill cgroupstats

 * @stats: cgroupstats to fill information into

 * @dentry: A dentry entry belonging to the cgroup for which stats have

 * been requested.

 *

 * Build and fill cgroupstats so that taskstats can export it to user

 * space.

 *

 * Return: %0 on success or a negative errno code on failure

 it should be kernfs_node belonging to cgroupfs and is a directory */

	/*

	 * We aren't being called from kernfs and there's no guarantee on

	 * @kn->priv's validity.  For this and css_tryget_online_from_dir(),

	 * @kn->priv is RCU safe.  Let's do the RCU dancing.

/*

 * Notify userspace when a cgroup is released, by running the

 * configured release agent with the name of the cgroup (path

 * relative to the root of cgroup file system) as the argument.

 *

 * Most likely, this user command will try to rmdir this cgroup.

 *

 * This races with the possibility that some other task will be

 * attached to this cgroup before it is removed, or that some other

 * user task will 'mkdir' a child cgroup of this cgroup.  That's ok.

 * The presumed 'rmdir' will fail quietly if this cgroup is no longer

 * unused, and this cgroup will be reprieved from its death sentence,

 * to continue to serve a useful existence.  Next time it's released,

 * we will get notified again, if it still has 'notify_on_release' set.

 *

 * The final arg to call_usermodehelper() is UMH_WAIT_EXEC, which

 * means only wait until the task is successfully execve()'d.  The

 * separate release agent task is forked by call_usermodehelper(),

 * then control in this thread returns here, without waiting for the

 * release agent task.  We don't bother to wait because the caller of

 * this routine has no use for the exit status of the release agent

 * task, so no sense holding our caller up for that.

 snoop agent path and exit early if empty */

 prepare argument buffers */

 minimal command environment */

/*

 * cgroup_rename - Only allow simple rename of directories in place.

 do not accept '\n' to prevent making /proc/<pid>/cgroup unparsable */

	/*

	 * We're gonna grab cgroup_mutex which nests outside kernfs

	 * active_ref.  kernfs_rename() doesn't require active_ref

	 * protection.  Break them before grabbing cgroup_mutex.

 Explicitly have no subsystems */

 Specifying two release agents is forbidden */

 blocked by boot param? */

 Can't specify an empty name */

 Must match [\w.-]+ */

 Specifying two names is forbidden */

	/*

	 * In absence of 'none', 'name=' and subsystem name options,

	 * let's default to 'all'.

 Mutually exclusive option 'all' + subsystem name */

 'all' => select all the subsystems */

	/*

	 * We either have to specify by name or by subsystems. (So all

	 * empty hierarchies must have a name).

	/*

	 * Option noprefix was introduced just for backward compatibility

	 * with the old cpuset, so we allow noprefix only if mounting just

	 * the cpuset subsystem.

 Can't specify "none" and some subsystems */

 See what subsystems are wanted */

 Don't allow flags or name to change at remount */

 remounting is not allowed for populated hierarchies */

/*

 * The guts of cgroup1 mount - find or create cgroup_root to use.

 * Called with cgroup_mutex held; returns 0 on success, -E... on

 * error and positive - in case when the candidate is busy dying.

 * On success it stashes a reference to cgroup_root into given

 * cgroup_fs_context; that reference is *NOT* counting towards the

 * cgroup_root refcount.

 First find the desired set of subsystems */

	/*

	 * Destruction of cgroup root is asynchronous, so subsystems may

	 * still be dying after the previous unmount.  Let's drain the

	 * dying subsystems.  We just need to ensure that the ones

	 * unmounted previously finish dying and don't care about new ones

	 * starting.  Testing ref liveliness is good enough.

 restart */

		/*

		 * If we asked for a name then it must match.  Also, if

		 * name matches but sybsys_mask doesn't, we should fail.

		 * Remember whether name matched.

		/*

		 * If we asked for subsystems (or explicitly for no

		 * subsystems) then they must match.

	/*

	 * No such thing, create a new one.  name= matching without subsys

	 * specification is allowed for already existing hierarchies but we

	 * can't create new one without subsys specification.

 Hierarchies may only be created in the initial cgroup namespace. */

 Check if the caller has permission to mount. */

 restart */

	/*

	 * Used to destroy pidlists and separate to serve as flush domain.

	 * Cap @max_active to 1 too.

 SPDX-License-Identifier: GPL-2.0

/*

 * KCSAN reporting.

 *

 * Copyright (C) 2019, Google LLC.

/*

 * Max. number of stack entries to show in the report.

 Common access info. */

/*

 * Other thread info: communicated from other racing thread to thread that set

 * up the watchpoint, which then prints the complete report atomically.

	/*

	 * Optionally pass @current. Typically we do not need to pass @current

	 * via @other_info since just @task_pid is sufficient. Passing @current

	 * has additional overhead.

	 *

	 * To safely pass @current, we must either use get_task_struct/

	 * put_task_struct, or stall the thread that populated @other_info.

	 *

	 * We cannot rely on get_task_struct/put_task_struct in case

	 * release_report() races with a task being released, and would have to

	 * free it in release_report(). This may result in deadlock if we want

	 * to use KCSAN on the allocators.

	 *

	 * Since we also want to reliably print held locks for

	 * CONFIG_KCSAN_VERBOSE, the current implementation stalls the thread

	 * that populated @other_info until it has been consumed.

/*

 * To never block any producers of struct other_info, we need as many elements

 * as we have watchpoints (upper bound on concurrent races to report).

/*

 * Information about reported races; used to rate limit reporting.

	/*

	 * The last time the race was reported.

	/*

	 * The frames of the 2 threads; if only 1 thread is known, one frame

	 * will be 0.

/*

 * Since we also want to be able to debug allocators with KCSAN, to avoid

 * deadlock, report_times cannot be dynamically resized with krealloc in

 * rate_limit_report.

 *

 * Therefore, we use a fixed-size array, which at most will occupy a page. This

 * still adequately rate limits reports, assuming that a) number of unique data

 * races is not excessive, and b) occurrence of unique races within the

 * same time window is limited.

/*

 * Spinlock serializing report generation, and access to @other_infos. Although

 * it could make sense to have a finer-grained locking story for @other_infos,

 * report generation needs to be serialized either way, so not much is gained.

/*

 * Checks if the race identified by thread frames frame1 and frame2 has

 * been reported since (now - KCSAN_REPORT_ONCE_IN_MS).

 Check if a matching race report exists. */

		/*

		 * Must always select an entry for use to store info as we

		 * cannot resize report_times; at the end of the scan, use_entry

		 * will be the oldest entry, which ideally also happened before

		 * KCSAN_REPORT_ONCE_IN_MS ago.

		/*

		 * Initially, no need to check any further as this entry as well

		 * as following entries have never been used.

 Check if entry expired. */

 before KCSAN_REPORT_ONCE_IN_MS ago */

 Reported recently, check if race matches. */

/*

 * Special rules to skip reporting.

 Should never get here if value_change==FALSE. */

	/*

	 * The first call to skip_report always has value_change==TRUE, since we

	 * cannot know the value written of an instrumented access. For the 2nd

	 * call there are 6 cases with CONFIG_KCSAN_REPORT_VALUE_CHANGE_ONLY:

	 *

	 * 1. read watchpoint, conflicting write (value_change==TRUE): report;

	 * 2. read watchpoint, conflicting write (value_change==MAYBE): skip;

	 * 3. write watchpoint, conflicting write (value_change==TRUE): report;

	 * 4. write watchpoint, conflicting write (value_change==MAYBE): skip;

	 * 5. write watchpoint, conflicting read (value_change==MAYBE): skip;

	 * 6. write watchpoint, conflicting read (value_change==TRUE): report;

	 *

	 * Cases 1-4 are intuitive and expected; case 5 ensures we do not report

	 * data races where the write may have rewritten the same value; case 6

	 * is possible either if the size is larger than what we check value

	 * changes for or the access type is KCSAN_ACCESS_ASSERT.

		/*

		 * The access is a write, but the data value did not change.

		 *

		 * We opt-out of this filter for certain functions at request of

		 * maintainers.

 Return thread description: in task or interrupt. */

 safe: protected by report_lock */

 Helper to skip KCSAN-related functions in stack-trace. */

 Never show tsan_* or {read,write}_once_size. */

 KCSAN runtime function. */

 KCSAN related test. */

		/*

		 * No match for runtime functions -- @skip entries to skip to

		 * get to first frame of interest.

/*

 * Skips to the first entry that matches the function of @ip, and then replaces

 * that entry with @ip, returning the entries to skip.

 Should not happen; the resulting stack trace is likely misleading. */

 Compares symbolized strings of addr1 and addr2. */

 Restore IRQ state trace for printing. */

 silence uninit warnings */

	/*

	 * Must check report filter rules before starting to print.

 @value_change is only known for the other thread */

 Print report header. */

		/*

		 * Order functions lexographically for consistent bug titles.

		 * Do not print offset of functions to keep title short.

 Print information about the racing accesses. */

 Print the other thread's stack trace. */

 Print stack trace of this thread. */

 Print observed value change. */

 Print report footer. */

	/*

	 * Use size to denote valid/invalid, since KCSAN entirely ignores

	 * 0-sized accesses.

/*

 * Sets @other_info->task and awaits consumption of @other_info.

 *

 * Precondition: report_lock is held.

 * Postcondition: report_lock is held.

	/*

	 * We may be instrumenting a code-path where current->state is already

	 * something other than TASK_RUNNING.

	/*

	 * To avoid deadlock in case we are in an interrupt here and this is a

	 * race with a task on the same CPU (KCSAN_INTERRUPT_WATCHER), provide a

	 * timeout to ensure this works in all contexts.

	 *

	 * Await approximately the worst case delay of the reporting thread (if

	 * we are not interrupted).

			/*

			 * Let lockdep know the real task is sleeping, to print

			 * the held locks (recall we turned lockdep off, so

			 * locking/unlocking @report_lock won't be recorded).

		/*

		 * We cannot call schedule() since we also cannot reliably

		 * determine if sleeping here is permitted -- see in_atomic().

			/*

			 * Abort. Reset @other_info->task to NULL, since it

			 * appears the other thread is still going to consume

			 * it. It will result in no verbose info printed for

			 * this task.

		/*

		 * If invalid, or @ptr nor @current matches, then @other_info

		 * has been consumed and we may continue. If not, retry.

 Populate @other_info; requires that the provided @other_info not in use. */

	/*

	 * The same @other_infos entry cannot be used concurrently, because

	 * there is a one-to-one mapping to watchpoint slots (@watchpoints in

	 * core.c), and a watchpoint is only released for reuse after reporting

	 * is done by the consumer of @other_info. Therefore, it is impossible

	 * for another concurrent prepare_report_producer() to set the same

	 * @other_info, and are guaranteed exclusivity for the @other_infos

	 * entry pointed to by @other_info.

	 *

	 * To check this property holds, size should never be non-zero here,

	 * because every consumer of struct other_info resets size to 0 in

	 * release_report().

 Awaits producer to fill @other_info and then returns. */

 Await valid @other_info. */

 Should always have a matching access based on watchpoint encoding. */

		/*

		 * If the actual accesses to not match, this was a false

		 * positive due to watchpoint encoding.

 Only replace stack entry with @ip if scoped access. */

 See kcsan_report_known_origin(). */

	/*

	 * Because we may generate reports when we're in scheduler code, the use

	 * of printk() could deadlock. Until such time that all printing code

	 * called in print_report() is scheduler-safe, accept the risk, and just

	 * get our message out. As such, also disable lockdep to hide the

	 * warning, and avoid disabling lockdep for the rest of the kernel.

	/*

	 * Never report if value_change is FALSE, only when it is

	 * either TRUE or MAYBE. In case of MAYBE, further filtering may

	 * be done once we know the full stack trace in print_report().

 See kcsan_report_known_origin(). */

 SPDX-License-Identifier: GPL-2.0

/*

 * KCSAN debugfs interface.

 *

 * Copyright (C) 2019, Google LLC.

/*

 * Addresses for filtering functions from reporting. This list can be used as a

 * whitelist or blacklist.

 array of addresses */

 current size */

 number of elements used */

 if elements are sorted */

 if list is a blacklist or whitelist */

 small initial size */

 default is blacklist */

/*

 * The microbenchmark allows benchmarking KCSAN core runtime only. To run

 * multiple threads, pipe 'microbench=<iters>' from multiple tasks into the

 * debugfs file. This will not generate any conflicts, and tests fast-path only.

 We may have been called from an atomic region; reset context. */

	/*

	 * Disable to benchmark fast-path for all accesses, and (expected

	 * negligible) call into slow-path, but never set up watchpoints.

 restore context */

 Get function start */

 Sort array if it is unsorted, and then do a binary search. */

 Returns 0 on success, error-code otherwise. */

 initial allocation */

 resize filterlist */

 leave filterlist itself untouched */

 Note: deduplicating should be done in userspace. */

 show stats */

 show filter functions, and filter type */

 SPDX-License-Identifier: GPL-2.0

/*

 * KCSAN test with various race scenarious to test runtime behaviour. Since the

 * interface with which KCSAN's reports are obtained is via the console, this is

 * the output we should verify. For each test case checks the presence (or

 * absence) of generated reports. Relies on 'console' tracepoint to capture

 * reports as they appear in the kernel log.

 *

 * Makes use of KUnit for test organization, and the Torture framework for test

 * thread control.

 *

 * Copyright (C) 2020, Google LLC.

 * Author: Marco Elver <elver@google.com>

 Points to current test-case memory access "kernels". */

 Lists of threads. */

 End time of test. */

 Report as observed from console. */

 Setup test checking loop. */

	/*

	 * Require at least as long as KCSAN_REPORT_ONCE_IN_MS, to ensure at

	 * least one race is reported.

 Signal start; release potential initialization of shared data. */

 End test checking loop. */

 Continue checking */

/*

 * Probe for console output: checks if a race was reported, and obtains observed

 * lines of interest.

	/*

	 * Note that KCSAN reports under a global lock, so we do not risk the

	 * possibility of having multiple reports interleaved. If that were the

	 * case, we'd expect tests to fail.

		/*

		 * KCSAN report and related to the test.

		 *

		 * The provided @buf is not NUL-terminated; copy no more than

		 * @len bytes and let strscpy() add the missing NUL-terminator.

 No second line of interest. */

 Publish new nlines. */

 Check if a report related to the test exists. */

 Report information we expect in a report. */

 Access information of both accesses. */

 Function pointer to expected function of top frame. */

 Address of access; unchecked if NULL. */

 Size of access; unchecked if @addr is NULL. */

 Access type, see KCSAN_ACCESS definitions. */

 Check observed report matches information in @r. */

 Doubled-checked locking. */

 Generate expected report contents. */

 Title */

 Expect lexographically sorted function names in title. */

 The exact offset won't match, remove it. */

 Access 1 */

 Access 1 & 2 */

 Access 2 */

 Dummy string if no second access is available. */

 Address is optional. */

 A new report is being captured. */

 Finally match expected output to what we actually observed. */

 Access info may appear in any order. */

 ===== Test kernels ===== */

 @test_array should be large enough to fall into multiple watchpoint slots. */

/*

 * Helper to avoid compiler optimizing out reads, and to generate source values

 * for writes.

 Suffixed by value-change exception filter. */

 Use builtin, so we can set up the "bad" atomic/non-atomic scenario. */

		/*

		 * Avoid race of unknown origin for this test, just pretend they

		 * are atomic.

/*

 * Scoped assertions do trigger anywhere in scope. However, the report should

 * still only point at the start of the scope.

 Unrelated accesses to scoped assert. */

 induce value change */

	/*

	 * Generate concurrent accesses, expecting no reports, ensuring KCSAN

	 * treats builtin atomics as actually atomic.

 Do not report data races between the read-writes. */

 ===== Test cases ===== */

 Simple test with normal data race. */

/*

 * Stress KCSAN with lots of concurrent races on different addresses until

 * timeout.

 NULL will match any address. */

 Sanity check matches exist. */

 Test the KCSAN_REPORT_VALUE_CHANGE_ONLY option. */

 Reset value. */

/*

 * Test that the rules where the KCSAN_REPORT_VALUE_CHANGE_ONLY option should

 * never apply work.

 Reset value. */

 Test that data races of unknown origin are reported. */

 Test KCSAN_ASSUME_PLAIN_WRITES_ATOMIC if it is selected. */

 induce value-change */

/*

 * Test that data races with writes larger than word-size are always reported,

 * even if KCSAN_ASSUME_PLAIN_WRITES_ATOMIC is selected.

/*

 * Test that data races where only one write is larger than word-size are always

 * reported, even if KCSAN_ASSUME_PLAIN_WRITES_ATOMIC is selected.

 Test that races with atomic accesses never result in reports. */

 Test that a race with an atomic and plain access result in reports. */

 Test that atomic RMWs generate correct report. */

 Zero-sized accesses should never cause data race reports. */

 Sanity check. */

 Test the data_race() macro. */

 This test requires a bit more time. */

/*

 * jiffies is special (declared to be volatile) and its accesses are typically

 * not marked; this test ensures that the compiler nor KCSAN gets confused about

 * jiffies's declaration on different architectures.

 Test that racing accesses in seqlock critical sections are not reported. */

/*

 * Test atomic builtins work and required instrumentation functions exist. We

 * also test that KCSAN understands they're atomic by racing with them via

 * test_kernel_atomic_builtins(), and expect no reports.

 *

 * The atomic builtins _SHOULD NOT_ be used in normal kernel code!

/*

 * Generate thread counts for all test cases. Values generated are in interval

 * [2, 5] followed by exponentially increasing thread counts from 8 to 32.

 *

 * The thread counts are chosen to cover potentially interesting boundaries and

 * corner cases (2 to 5), and then stress the system with larger counts.

 stop */

 initial value */

		/*

		 * Without any preemption, keep 2 CPUs free for other tasks, one

		 * of which is the main test case function checking for

		 * completion or failure.

 Use negative value to indicate last param. */

 ===== End test cases ===== */

 Concurrent accesses from interrupts. */

 Acquire potential initialization. */

 The main loop for each thread. */

 Iterate through all kernels. */

 Acquire potential initialization. */

/*

 * We only want to do tracepoints setup and teardown once, therefore we have to

 * customize the init and exit functions and cannot rely on kunit_test_suite().

	/*

	 * Because we want to be able to build the test as a module, we need to

	 * iterate through all known tracepoints, since the static registration

	 * won't work here.

 SPDX-License-Identifier: GPL-2.0

/*

 * KCSAN core runtime.

 *

 * Copyright (C) 2019, Google LLC.

 Per-CPU kcsan_ctx for interrupts */

/*

 * Helper macros to index into adjacent slots, starting from address slot

 * itself, followed by the right and left slots.

 *

 * The purpose is 2-fold:

 *

 *	1. if during insertion the address slot is already occupied, check if

 *	   any adjacent slots are free;

 *	2. accesses that straddle a slot boundary due to size that exceeds a

 *	   slot's range may check adjacent slots if any watchpoint matches.

 *

 * Note that accesses with very large size may still miss a watchpoint; however,

 * given this should be rare, this is a reasonable trade-off to make, since this

 * will avoid:

 *

 *	1. excessive contention between watchpoint checks and setup;

 *	2. larger number of simultaneous watchpoints without sacrificing

 *	   performance.

 *

 * Example: SLOT_IDX values for KCSAN_CHECK_ADJACENT=1, where i is [0, 1, 2]:

 *

 *   slot=0:  [ 1,  2,  0]

 *   slot=9:  [10, 11,  9]

 *   slot=63: [64, 65, 63]

/*

 * SLOT_IDX_FAST is used in the fast-path. Not first checking the address's primary

 * slot (middle) is fine if we assume that races occur rarely. The set of

 * indices {SLOT_IDX(slot, i) | i in [0, NUM_SLOTS)} is equivalent to

 * {SLOT_IDX_FAST(slot, i) | i in [0, NUM_SLOTS)}.

/*

 * Watchpoints, with each entry encoded as defined in encoding.h: in order to be

 * able to safely update and access a watchpoint without introducing locking

 * overhead, we encode each watchpoint as a single atomic long. The initial

 * zero-initialized state matches INVALID_WATCHPOINT.

 *

 * Add NUM_SLOTS-1 entries to account for overflow; this helps avoid having to

 * use more complicated SLOT_IDX_FAST calculation with modulo in the fast-path.

/*

 * Instructions to skip watching counter, used in should_watch(). We use a

 * per-CPU counter to avoid excessive contention.

 For kcsan_prandom_u32_max(). */

 Check if the watchpoint matches the access. */

 Check slot index logic, ensuring we stay within array bounds. */

 Try to acquire this slot. */

/*

 * Return true if watchpoint was successfully consumed, false otherwise.

 *

 * This may return false if:

 *

 *	1. another thread already consumed the watchpoint;

 *	2. the thread that set up the watchpoint already removed it;

 *	3. the watchpoint was removed and then re-used.

 Return true if watchpoint was not touched, false if already consumed. */

 Remove the watchpoint -- its slot may be reused after. */

	/*

	 * In interrupts, use raw_cpu_ptr to avoid unnecessary checks, that would

	 * also result in calls that generate warnings in uaccess regions.

 Check scoped accesses; never inline because this is a slow-path! */

 Avoid recursion. */

 Rules for generic atomic accesses. Called from fast-path. */

	/*

	 * Unless explicitly declared atomic, never consider an assertion access

	 * as atomic. This allows using them also in atomic regions, such as

	 * seqlocks, without implicitly changing their semantics.

 Assume aligned writes up to word size are atomic. */

		/*

		 * Because we do not have separate contexts for nested

		 * interrupts, in case atomic_next is set, we simply assume that

		 * the outer interrupt set atomic_next. In the worst case, we

		 * will conservatively consider operations as atomic. This is a

		 * reasonable trade-off to make, since this case should be

		 * extremely rare; however, even if extremely rare, it could

		 * lead to false positives otherwise.

 in task, or outer interrupt */

	/*

	 * Never set up watchpoints when memory operations are atomic.

	 *

	 * Need to check this first, before kcsan_skip check below: (1) atomics

	 * should not count towards skipped instructions, and (2) to actually

	 * decrement kcsan_atomic_next for consecutive instruction stream.

	/*

	 * NOTE: If we get here, kcsan_skip must always be reset in slow path

	 * via reset_kcsan_skip() to avoid underflow.

 this operation should be watched */

/*

 * Returns a pseudo-random number in interval [0, ep_ro). Simple linear

 * congruential generator, using constants from "Numerical Recipes".

 Introduce delay depending on context and configuration. */

 For certain access types, skew the random delay to be longer. */

/*

 * Pull everything together: check_access() below contains the performance

 * critical operations; the fast-path (including check_access) functions should

 * all be inlinable by the instrumentation functions.

 *

 * The slow-path (kcsan_found_watchpoint, kcsan_setup_watchpoint) are

 * non-inlinable -- note that, we prefix these with "kcsan_" to ensure they can

 * be filtered from the stacktrace, as well as give them unique names for the

 * UACCESS whitelist of objtool. Each function uses user_access_save/restore(),

 * since they do not access any user memory, but instrumentation is still

 * emitted in UACCESS regions.

	/*

	 * We know a watchpoint exists. Let's try to keep the race-window

	 * between here and finally consuming the watchpoint below as small as

	 * possible -- avoid unneccessarily complex code until consumed.

	/*

	 * The access_mask check relies on value-change comparison. To avoid

	 * reporting a race where e.g. the writer set up the watchpoint, but the

	 * reader has access_mask!=0, we have to ignore the found watchpoint.

	/*

	 * If the other thread does not want to ignore the access, and there was

	 * a value change as a result of this thread's operation, we will still

	 * generate a report of unknown origin.

	 *

	 * Use CONFIG_KCSAN_REPORT_RACE_UNKNOWN_ORIGIN=n to filter.

	/*

	 * Consuming the watchpoint must be guarded by kcsan_is_enabled() to

	 * avoid erroneously triggering reports if the context is disabled.

 keep this after try_consume_watchpoint */

		/*

		 * The other thread may not print any diagnostics, as it has

		 * already removed the watchpoint, or another thread consumed

		 * the watchpoint before this thread.

	/*

	 * Always reset kcsan_skip counter in slow-path to avoid underflow; see

	 * should_watch().

	/*

	 * Check to-ignore addresses after kcsan_is_enabled(), as we may access

	 * memory that is not yet initialized during early boot.

	/*

	 * Save and restore the IRQ state trace touched by KCSAN, since KCSAN's

	 * runtime is entered for every memory access, and potentially useful

	 * information is lost if dirtied by KCSAN.

		/*

		 * Out of capacity: the size of 'watchpoints', and the frequency

		 * with which should_watch() returns true should be tweaked so

		 * that this case happens very rarely.

	/*

	 * Read the current value, to later check and infer a race if the data

	 * was modified via a non-instrumented access, e.g. from a device.

 ignore; we do not diff the values */

	/*

	 * Delay this thread, to increase probability of observing a racy

	 * conflicting access.

	/*

	 * Re-read value, and check if it is as expected; if not, we infer a

	 * racy access.

 ignore; we do not diff the values */

	/*

	 * Check if we observed a value change.

	 *

	 * Also check if the data race should be ignored (the rules depend on

	 * non-zero diff); if it is to be ignored, the below rules for

	 * KCSAN_VALUE_CHANGE_MAYBE apply.

 Check if this access raced with another. */

		/*

		 * Depending on the access type, map a value_change of MAYBE to

		 * TRUE (always report) or FALSE (never report).

				/*

				 * For access with access_mask, we require a

				 * value-change, as it is likely that races on

				 * ~access_mask bits are expected.

 Always assume a value-change. */

		/*

		 * No need to increment 'data_races' counter, as the racing

		 * thread already did.

		 *

		 * Count 'assert_failures' for each failed ASSERT access,

		 * therefore both this thread and the racing thread may

		 * increment this counter.

 Inferring a race, since the value should not have changed. */

	/*

	 * Remove watchpoint; must be after reporting, since the slot may be

	 * reused after this point.

	/*

	 * Do nothing for 0 sized check; this comparison will be optimized out

	 * for constant sized instrumentation (__tsan_{read,write}N).

	/*

	 * Avoid user_access_save in fast-path: find_watchpoint is safe without

	 * user_access_save, as the address that ptr points to is only used to

	 * check if a watchpoint exists; ptr is never dereferenced.

	/*

	 * It is safe to check kcsan_is_enabled() after find_watchpoint in the

	 * slow-path, as long as no state changes that cause a race to be

	 * detected and reported have occurred until kcsan_is_enabled() is

	 * checked.

 Call only once in fast-path. */

 === Public interface ===================================================== */

	/*

	 * We are in the init task, and no other tasks should be running;

	 * WRITE_ONCE without memory barrier is sufficient.

 === Exported interface =================================================== */

		/*

		 * Warn if kcsan_enable_current() calls are unbalanced with

		 * kcsan_disable_current() calls, which causes disable_count to

		 * become negative and should not happen.

 restore to 0, KCSAN still enabled */

 disable to generate warning */

	/*

	 * Do *not* check and warn if we are in a flat atomic region: nestable

	 * and flat atomic regions are independent from each other.

	 * See include/linux/kcsan.h: struct kcsan_ctx comments for more

	 * comments.

		/*

		 * Warn if kcsan_nestable_atomic_end() calls are unbalanced with

		 * kcsan_nestable_atomic_begin() calls, which causes

		 * atomic_nest_count to become negative and should not happen.

 restore to 0 */

 disable to generate warning */

 Disable KCSAN, in case list debugging is on. */

 Lazy initialize list head. */

 Disable KCSAN, in case list debugging is on. */

		/*

		 * Ensure we do not enter kcsan_check_scoped_accesses()

		 * slow-path if unnecessary, and avoids requiring list_empty()

		 * in the fast-path (to avoid a READ_ONCE() and potential

		 * uaccess warning).

/*

 * KCSAN uses the same instrumentation that is emitted by supported compilers

 * for ThreadSanitizer (TSAN).

 *

 * When enabled, the compiler emits instrumentation calls (the functions

 * prefixed with "__tsan" below) for all loads and stores that it generated;

 * inline asm is not instrumented.

 *

 * Note that, not all supported compiler versions distinguish aligned/unaligned

 * accesses, but e.g. recent versions of Clang do. We simply alias the unaligned

 * version to the generic version, which can handle both.

/*

 * Use of explicit volatile is generally disallowed [1], however, volatile is

 * still used in various concurrent context, whether in low-level

 * synchronization primitives or for legacy reasons.

 * [1] https://lwn.net/Articles/233479/

 *

 * We only consider volatile accesses atomic if they are aligned and would pass

 * the size-check of compiletime_assert_rwonce_type().

/*

 * The below are not required by KCSAN, but can still be emitted by the

 * compiler.

/*

 * Instrumentation for atomic builtins (__atomic_*, __sync_*).

 *

 * Normal kernel code _should not_ be using them directly, but some

 * architectures may implement some or all atomics using the compilers'

 * builtins.

 *

 * Note: If an architecture decides to fully implement atomics using the

 * builtins, because they are implicitly instrumented by KCSAN (and KASAN,

 * etc.), implementing the ARCH_ATOMIC interface (to get instrumentation via

 * atomic-instrumented) is no longer necessary.

 *

 * TSAN instrumentation replaces atomic accesses with calls to any of the below

 * functions, whose job is to also execute the operation itself.

/*

 * Note: CAS operations are always classified as write, even in case they

 * fail. We cannot perform check_access() after a write, as it might lead to

 * false positives, in cases such as:

 *

 *	T0: __atomic_compare_exchange_n(&p->flag, &old, 1, ...)

 *

 *	T1: if (__atomic_load_n(&p->flag, ...)) {

 *		modify *p;

 *		p->flag = 0;

 *	    }

 *

 * The only downside is that, if there are 3 threads, with one CAS that

 * succeeds, another CAS that fails, and an unmarked racing operation, we may

 * point at the wrong CAS as the source of the race. However, if we assume that

 * all CAS can succeed in some other execution, the data race is still valid.

 SPDX-License-Identifier: GPL-2.0

/*

 * KCSAN short boot-time selftests.

 *

 * Copyright (C) 2019, Google LLC.

 Test requirements. */

 random should be initialized for the below tests */

/*

 * Test watchpoint encode and decode: check that encoding some access's info,

 * and then subsequent decode preserves the access's info.

 Check special watchpoints */

 Check decoding watchpoint returns same data */

 Test access matching function. */

	/*

	 * An access of size 0 could match another access, as demonstrated here.

	 * Rather than add more comparisons to 'matching_access()', which would

	 * end up in the fast-path for *all* checks, check_access() simply

	 * returns for all accesses of size 0.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * shadow.c - Shadow Variables

 *

 * Copyright (C) 2014 Josh Poimboeuf <jpoimboe@redhat.com>

 * Copyright (C) 2014 Seth Jennings <sjenning@redhat.com>

 * Copyright (C) 2017 Joe Lawrence <joe.lawrence@redhat.com>

/**

 * DOC: Shadow variable API concurrency notes:

 *

 * The shadow variable API provides a simple relationship between an

 * <obj, id> pair and a pointer value.  It is the responsibility of the

 * caller to provide any mutual exclusion required of the shadow data.

 *

 * Once a shadow variable is attached to its parent object via the

 * klp_shadow_*alloc() API calls, it is considered live: any subsequent

 * call to klp_shadow_get() may then return the shadow variable's data

 * pointer.  Callers of klp_shadow_*alloc() should prepare shadow data

 * accordingly.

 *

 * The klp_shadow_*alloc() API calls may allocate memory for new shadow

 * variable structures.  Their implementation does not call kmalloc

 * inside any spinlocks, but API callers should pass GFP flags according

 * to their specific needs.

 *

 * The klp_shadow_hash is an RCU-enabled hashtable and is safe against

 * concurrent klp_shadow_free() and klp_shadow_get() operations.

/*

 * klp_shadow_lock provides exclusive access to the klp_shadow_hash and

 * the shadow variables it references.

/**

 * struct klp_shadow - shadow variable structure

 * @node:	klp_shadow_hash hash table node

 * @rcu_head:	RCU is used to safely free this structure

 * @obj:	pointer to parent object

 * @id:		data identifier

 * @data:	data area

/**

 * klp_shadow_match() - verify a shadow variable matches given <obj, id>

 * @shadow:	shadow variable to match

 * @obj:	pointer to parent object

 * @id:		data identifier

 *

 * Return: true if the shadow variable matches.

/**

 * klp_shadow_get() - retrieve a shadow variable data pointer

 * @obj:	pointer to parent object

 * @id:		data identifier

 *

 * Return: the shadow variable data element, NULL on failure.

 Check if the shadow variable already exists */

	/*

	 * Allocate a new shadow variable.  Fill it with zeroes by default.

	 * More complex setting can be done by @ctor function.  But it is

	 * called only when the buffer is really used (under klp_shadow_lock).

 Look for <obj, id> again under the lock */

		/*

		 * Shadow variable was found, throw away speculative

		 * allocation.

 No <obj, id> found, so attach the newly allocated one */

/**

 * klp_shadow_alloc() - allocate and add a new shadow variable

 * @obj:	pointer to parent object

 * @id:		data identifier

 * @size:	size of attached data

 * @gfp_flags:	GFP mask for allocation

 * @ctor:	custom constructor to initialize the shadow data (optional)

 * @ctor_data:	pointer to any data needed by @ctor (optional)

 *

 * Allocates @size bytes for new shadow variable data using @gfp_flags.

 * The data are zeroed by default.  They are further initialized by @ctor

 * function if it is not NULL.  The new shadow variable is then added

 * to the global hashtable.

 *

 * If an existing <obj, id> shadow variable can be found, this routine will

 * issue a WARN, exit early and return NULL.

 *

 * This function guarantees that the constructor function is called only when

 * the variable did not exist before.  The cost is that @ctor is called

 * in atomic context under a spin lock.

 *

 * Return: the shadow variable data element, NULL on duplicate or

 * failure.

/**

 * klp_shadow_get_or_alloc() - get existing or allocate a new shadow variable

 * @obj:	pointer to parent object

 * @id:		data identifier

 * @size:	size of attached data

 * @gfp_flags:	GFP mask for allocation

 * @ctor:	custom constructor to initialize the shadow data (optional)

 * @ctor_data:	pointer to any data needed by @ctor (optional)

 *

 * Returns a pointer to existing shadow data if an <obj, id> shadow

 * variable is already present.  Otherwise, it creates a new shadow

 * variable like klp_shadow_alloc().

 *

 * This function guarantees that only one shadow variable exists with the given

 * @id for the given @obj.  It also guarantees that the constructor function

 * will be called only when the variable did not exist before.  The cost is

 * that @ctor is called in atomic context under a spin lock.

 *

 * Return: the shadow variable data element, NULL on failure.

/**

 * klp_shadow_free() - detach and free a <obj, id> shadow variable

 * @obj:	pointer to parent object

 * @id:		data identifier

 * @dtor:	custom callback that can be used to unregister the variable

 *		and/or free data that the shadow variable points to (optional)

 *

 * This function releases the memory for this <obj, id> shadow variable

 * instance, callers should stop referencing it accordingly.

 Delete <obj, id> from hash */

/**

 * klp_shadow_free_all() - detach and free all <*, id> shadow variables

 * @id:		data identifier

 * @dtor:	custom callback that can be used to unregister the variable

 *		and/or free data that the shadow variable points to (optional)

 *

 * This function releases the memory for all <*, id> shadow variable

 * instances, callers should stop referencing them accordingly.

 Delete all <*, id> from hash */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * transition.c - Kernel Live Patching transition functions

 *

 * Copyright (C) 2015-2016 Josh Poimboeuf <jpoimboe@redhat.com>

/*

 * This work can be performed periodically to finish patching or unpatching any

 * "straggler" tasks which failed to transition in the first attempt.

/*

 * This function is just a stub to implement a hard force

 * of synchronize_rcu(). This requires synchronizing

 * tasks even in userspace and idle.

/*

 * We allow to patch also functions where RCU is not watching,

 * e.g. before user_exit(). We can not rely on the RCU infrastructure

 * to do the synchronization. Instead hard force the sched synchronization.

 *

 * This approach allows to use RCU functions for manipulating func_stack

 * safely.

/*

 * The transition to the target patch state is complete.  Clean up the data

 * structures.

		/*

		 * All tasks have transitioned to KLP_UNPATCHED so we can now

		 * remove the new functions from the func_stack.

		/*

		 * Make sure klp_ftrace_handler() can no longer see functions

		 * from this patch on the ops->func_stack.  Otherwise, after

		 * func->transition gets cleared, the handler may choose a

		 * removed function.

 Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */

/*

 * This is called in the error path, to cancel a transition before it has

 * started, i.e. klp_init_transition() has been called but

 * klp_start_transition() hasn't.  If the transition *has* been started,

 * klp_reverse_transition() should be used instead.

/*

 * Switch the patched state of the task to the set of functions in the target

 * patch state.

 *

 * NOTE: If task is not 'current', the caller must ensure the task is inactive.

 * Otherwise klp_ftrace_handler() might read the wrong 'patch_state' value.

	/*

	 * A variant of synchronize_rcu() is used to allow patching functions

	 * where RCU is not watching, see klp_synchronize_transition().

	/*

	 * This test_and_clear_tsk_thread_flag() call also serves as a read

	 * barrier (smp_rmb) for two cases:

	 *

	 * 1) Enforce the order of the TIF_PATCH_PENDING read and the

	 *    klp_target_state read.  The corresponding write barrier is in

	 *    klp_init_transition().

	 *

	 * 2) Enforce the order of the TIF_PATCH_PENDING read and a future read

	 *    of func->transition, if klp_ftrace_handler() is called later on

	 *    the same CPU.  See __klp_disable_patch().

/*

 * Determine whether the given stack trace includes any references to a

 * to-be-patched or to-be-unpatched function.

			 /*

			  * Check for the to-be-unpatched function

			  * (the func itself).

			/*

			 * Check for the to-be-patched function

			 * (the previous func).

 original function */

 previously patched function */

/*

 * Determine whether it's safe to transition the task to the target patch state

 * by looking for any to-be-patched or to-be-unpatched functions on its stack.

/*

 * Try to safely switch a task to the target patch state.  If it's currently

 * running, or it's sleeping on a to-be-patched or to-be-unpatched function, or

 * if the stack is unreliable, return false.

 check if this task has already switched over */

	/*

	 * For arches which don't have reliable stack traces, we have to rely

	 * on other methods (e.g., switching tasks at kernel exit).

	/*

	 * Now try to check the stack for any to-be-patched or to-be-unpatched

	 * functions.  If all goes well, switch the task to the target patch

	 * state.

 success */

 klp_check_and_switch_task() */

 klp_check_and_switch_task() */

 klp_check_and_switch_task() */

/*

 * Sends a fake signal to all non-kthread tasks with TIF_PATCH_PENDING set.

 * Kthreads with TIF_PATCH_PENDING set are woken up.

		/*

		 * There is a small race here. We could see TIF_PATCH_PENDING

		 * set and decide to wake up a kthread or send a fake signal.

		 * Meanwhile the task could migrate itself and the action

		 * would be meaningless. It is not serious though.

			/*

			 * Wake up a kthread which sleeps interruptedly and

			 * still has not been migrated.

			/*

			 * Send fake signal to all non-kthread tasks which are

			 * still not migrated.

/*

 * Try to switch all remaining tasks to the target patch state by walking the

 * stacks of sleeping tasks and looking for any to-be-patched or

 * to-be-unpatched functions.  If such functions are found, the task can't be

 * switched yet.

 *

 * If any tasks are still stuck in the initial patch state, schedule a retry.

	/*

	 * Try to switch the tasks to the target patch state by walking their

	 * stacks and looking for any to-be-patched or to-be-unpatched

	 * functions.  If such functions are found on a stack, or if the stack

	 * is deemed unreliable, the task can't be switched yet.

	 *

	 * Usually this will transition most (or all) of the tasks on a system

	 * unless the patch includes changes to a very common function.

	/*

	 * Ditto for the idle "swapper" tasks.

 Make idle task go through the main loop. */

 offline idle tasks can be switched immediately */

		/*

		 * Some tasks weren't able to be switched over.  Try again

		 * later and/or wait for other methods like kernel exit

		 * switching.

 we're done, now cleanup the data structures */

	/*

	 * It would make more sense to free the unused patches in

	 * klp_complete_transition() but it is called also

	 * from klp_cancel_transition().

/*

 * Start the transition to the specified target patch state so tasks can begin

 * switching to it.

	/*

	 * Mark all normal tasks as needing a patch state update.  They'll

	 * switch either in klp_try_complete_transition() or as they exit the

	 * kernel.

	/*

	 * Mark all idle tasks as needing a patch state update.  They'll switch

	 * either in klp_try_complete_transition() or at the idle loop switch

	 * point.

/*

 * Initialize the global target patch state and all tasks to the initial patch

 * state, and initialize all function transition states to true in preparation

 * for patching or unpatching.

	/*

	 * Set the global target patch state which tasks will switch to.  This

	 * has no effect until the TIF_PATCH_PENDING flags get set later.

	/*

	 * Initialize all tasks to the initial patch state to prepare them for

	 * switching to the target state.

	/*

	 * Ditto for the idle "swapper" tasks.

	/*

	 * Enforce the order of the task->patch_state initializations and the

	 * func->transition updates to ensure that klp_ftrace_handler() doesn't

	 * see a func in transition with a task->patch_state of KLP_UNDEFINED.

	 *

	 * Also enforce the order of the klp_target_state write and future

	 * TIF_PATCH_PENDING writes to ensure klp_update_patch_state() doesn't

	 * set a task->patch_state to KLP_UNDEFINED.

	/*

	 * Set the func transition states so klp_ftrace_handler() will know to

	 * switch to the transition logic.

	 *

	 * When patching, the funcs aren't yet in the func_stack and will be

	 * made visible to the ftrace handler shortly by the calls to

	 * klp_patch_object().

	 *

	 * When unpatching, the funcs are already in the func_stack and so are

	 * already visible to the ftrace handler.

/*

 * This function can be called in the middle of an existing transition to

 * reverse the direction of the target patch state.  This can be done to

 * effectively cancel an existing enable or disable operation if there are any

 * tasks which are stuck in the initial patch state.

	/*

	 * Clear all TIF_PATCH_PENDING flags to prevent races caused by

	 * klp_update_patch_state() running in parallel with

	 * klp_start_transition().

 Let any remaining calls to klp_update_patch_state() complete */

 Called from copy_process() during fork */

 TIF_PATCH_PENDING gets copied in setup_thread_stack() */

/*

 * Drop TIF_PATCH_PENDING of all tasks on admin's request. This forces an

 * existing transition to finish.

 *

 * NOTE: klp_update_patch_state(task) requires the task to be inactive or

 * 'current'. This is not the case here and the consistency model could be

 * broken. Administrator, who is the only one to execute the

 * klp_force_transitions(), has to be aware of this.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * core.c - Kernel Live Patching Core

 *

 * Copyright (C) 2014 Seth Jennings <sjenning@redhat.com>

 * Copyright (C) 2014 SUSE

/*

 * klp_mutex is a coarse lock which serializes access to klp data.  All

 * accesses to klp-related variables and structures must have mutex protection,

 * except within the following functions which carefully avoid the need for it:

 *

 * - klp_ftrace_handler()

 * - klp_update_patch_state()

/*

 * Actively used patches: enabled or in transition. Note that replaced

 * or disabled patches are not listed even though the related kernel

 * module still can be loaded.

 sets obj->mod if object is not vmlinux and module is found */

	/*

	 * We do not want to block removal of patched modules and therefore

	 * we do not take a reference here. The patches are removed by

	 * klp_module_going() instead.

	/*

	 * Do not mess work of klp_module_coming() and klp_module_going().

	 * Note that the patch might still be needed before klp_module_going()

	 * is called. Module functions can be called even in the GOING state

	 * until mod->exit() finishes. This is especially important for

	 * patches that modify semantic of the functions.

	/*

	 * Finish the search when the symbol is found for the desired position

	 * or the position is not defined for a non-unique symbol.

	/*

	 * Ensure an address was found. If sympos is 0, ensure symbol is unique;

	 * otherwise ensure the symbol position count matches sympos.

	/*

	 * Since the field widths for sym_objname and sym_name in the sscanf()

	 * call are hard-coded and correspond to MODULE_NAME_LEN and

	 * KSYM_NAME_LEN respectively, we must make sure that MODULE_NAME_LEN

	 * and KSYM_NAME_LEN have the values we expect them to have.

	 *

	 * Because the value of MODULE_NAME_LEN can differ among architectures,

	 * we use the smallest/strictest upper bound possible (56, based on

	 * the current definition of MODULE_NAME_LEN) to prevent overflows.

 For each rela in this klp relocation section */

 Format: .klp.sym.sym_objname.sym_name,sympos */

		/*

		 * Prevent module-specific KLP rela sections from referencing

		 * vmlinux symbols.  This helps prevent ordering issues with

		 * module special section initializations.  Presumably such

		 * symbols are exported and normal relas can be used instead.

 klp_find_object_symbol() treats a NULL objname as vmlinux */

/*

 * At a high-level, there are two types of klp relocation sections: those which

 * reference symbols which live in vmlinux; and those which reference symbols

 * which live in other modules.  This function is called for both types:

 *

 * 1) When a klp module itself loads, the module code calls this function to

 *    write vmlinux-specific klp relocations (.klp.rela.vmlinux.* sections).

 *    These relocations are written to the klp module text to allow the patched

 *    code/data to reference unexported vmlinux symbols.  They're written as

 *    early as possible to ensure that other module init code (.e.g.,

 *    jump_label_apply_nops) can access any unexported vmlinux symbols which

 *    might be referenced by the klp module's special sections.

 *

 * 2) When a to-be-patched module loads -- or is already loaded when a

 *    corresponding klp module loads -- klp code calls this function to write

 *    module-specific klp relocations (.klp.rela.{module}.* sections).  These

 *    are written to the klp module text to allow the patched code/data to

 *    reference symbols which live in the to-be-patched module or one of its

 *    module dependencies.  Exported symbols are supported, in addition to

 *    unexported symbols, in order to enable late module patching, which allows

 *    the to-be-patched module to be loaded and patched sometime *after* the

 *    klp module is loaded.

	/*

	 * Format: .klp.rela.sec_objname.section_name

	 * See comment in klp_resolve_symbols() for an explanation

	 * of the selected field width value.

/*

 * Sysfs Interface

 *

 * /sys/kernel/livepatch

 * /sys/kernel/livepatch/<patch>

 * /sys/kernel/livepatch/<patch>/enabled

 * /sys/kernel/livepatch/<patch>/transition

 * /sys/kernel/livepatch/<patch>/force

 * /sys/kernel/livepatch/<patch>/<object>

 * /sys/kernel/livepatch/<patch>/<object>/<function,sympos>

 already in requested state */

	/*

	 * Allow to reverse a pending transition in both ways. It might be

	 * necessary to complete the transition without forcing and breaking

	 * the system integrity.

	 *

	 * Do not allow to re-enable a disabled patch.

	/*

	 * func->new_func is same as func->old_func. These addresses are

	 * set when the object is loaded, see klp_init_object_loaded().

/*

 * Add 'nop' functions which simply return to the caller to run

 * the original function. The 'nop' functions are added to a

 * patch to facilitate a 'replace' mode.

 Clean up when a patched object is unloaded */

/*

 * This function implements the free operations that can be called safely

 * under klp_mutex.

 *

 * The operation must be completed by calling klp_free_patch_finish()

 * outside klp_mutex.

/*

 * This function implements the free part that must be called outside

 * klp_mutex.

 *

 * It must be called after klp_free_patch_start(). And it has to be

 * the last function accessing the livepatch structures when the patch

 * gets disabled.

	/*

	 * Avoid deadlock with enabled_store() sysfs callback by

	 * calling this outside klp_mutex. It is safe because

	 * this is called when the patch gets disabled and it

	 * cannot get enabled again.

 Put the module after the last access to struct klp_patch. */

/*

 * The livepatch might be freed from sysfs interface created by the patch.

 * This work allows to wait until the interface is destroyed in a separate

 * context.

	/*

	 * NOPs get the address later. The patched module must be loaded,

	 * see klp_init_object_loaded().

	/* The format for the sysfs directory is <function,sympos> where sympos

	 * is the nth occurrence of this symbol in kallsyms for the patched

	 * object. If the user selects 0 for old_sympos, then 1 will be used

	 * since a unique symbol will be the first occurrence.

 parts of the initialization that is done only when the object is loaded */

		/*

		 * Only write module-specific relocations here

		 * (.klp.rela.{module}.*).  vmlinux-specific relocations were

		 * written earlier during the initialization of the klp module

		 * itself.

	/*

	 * Enforce the order of the func->transition writes in

	 * klp_init_transition() and the TIF_PATCH_PENDING writes in

	 * klp_start_transition().  In the rare case where klp_ftrace_handler()

	 * is called shortly after klp_update_patch_state() switches the task,

	 * this ensures the handler sees that func->transition is set.

	/*

	 * Enforce the order of the func->transition writes in

	 * klp_init_transition() and the ops->func_stack writes in

	 * klp_patch_object(), so that klp_ftrace_handler() will see the

	 * func->transition updates before the handler is registered and the

	 * new funcs become visible to the handler.

/**

 * klp_enable_patch() - enable the livepatch

 * @patch:	patch to be enabled

 *

 * Initializes the data structure associated with the patch, creates the sysfs

 * interface, performs the needed symbol lookups and code relocations,

 * registers the patched functions with ftrace.

 *

 * This function is supposed to be called from the livepatch module_init()

 * callback.

 *

 * Return: 0 on success, otherwise error

/*

 * This function unpatches objects from the replaced livepatches.

 *

 * We could be pretty aggressive here. It is called in the situation where

 * these structures are no longer accessed from the ftrace handler.

 * All functions are redirected by the klp_transition_patch. They

 * use either a new code or they are in the original code because

 * of the special nop function patches.

 *

 * The only exception is when the transition was forced. In this case,

 * klp_ftrace_handler() might still see the replaced patch on the stack.

 * Fortunately, it is carefully designed to work with removed functions

 * thanks to RCU. We only have to keep the patches on the system. Also

 * this is handled transparently by patch->module_put.

/*

 * This function removes the dynamically allocated 'nop' functions.

 *

 * We could be pretty aggressive. NOPs do not change the existing

 * behavior except for adding unnecessary delay by the ftrace handler.

 *

 * It is safe even when the transition was forced. The ftrace handler

 * will see a valid ops->func_stack entry thanks to RCU.

 *

 * We could even free the NOPs structures. They must be the last entry

 * in ops->func_stack. Therefore unregister_ftrace_function() is called.

 * It does the same as klp_synchronize_transition() to make sure that

 * nobody is inside the ftrace handler once the operation finishes.

 *

 * IMPORTANT: It must be called right after removing the replaced patches!

/*

 * Remove parts of patches that touch a given kernel module. The list of

 * patches processed might be limited. When limit is NULL, all patches

 * will be handled.

	/*

	 * Each module has to know that klp_module_coming()

	 * has been called. We never know what module will

	 * get patched by a new patch.

	/*

	 * If a patch is unsuccessfully applied, return

	 * error to the module loader.

	/*

	 * Each module has to know that klp_module_going()

	 * has been called. We never know what module will

	 * get patched by a new patch.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * patch.c - livepatch patching functions

 *

 * Copyright (C) 2014 Seth Jennings <sjenning@redhat.com>

 * Copyright (C) 2014 SUSE

 * Copyright (C) 2015 Josh Poimboeuf <jpoimboe@redhat.com>

	/*

	 * The ftrace_test_recursion_trylock() will disable preemption,

	 * which is required for the variant of synchronize_rcu() that is

	 * used to allow patching functions where RCU is not watching.

	 * See klp_synchronize_transition() for more details.

	/*

	 * func should never be NULL because preemption should be disabled here

	 * and unregister_ftrace_function() does the equivalent of a

	 * synchronize_rcu() before the func_stack removal.

	/*

	 * In the enable path, enforce the order of the ops->func_stack and

	 * func->transition reads.  The corresponding write barrier is in

	 * __klp_enable_patch().

	 *

	 * (Note that this barrier technically isn't needed in the disable

	 * path.  In the rare case where klp_update_patch_state() runs before

	 * this handler, its TIF_PATCH_PENDING read and this func->transition

	 * read need to be ordered.  But klp_update_patch_state() already

	 * enforces that.)

		/*

		 * Enforce the order of the func->transition and

		 * current->patch_state reads.  Otherwise we could read an

		 * out-of-date task state and pick the wrong function.  The

		 * corresponding write barrier is in klp_init_transition().

			/*

			 * Use the previously patched version of the function.

			 * If no previous patches exist, continue with the

			 * original function.

	/*

	 * NOPs are used to replace existing patches with original code.

	 * Do nothing! Setting pc would cause an infinite loop.

/*

 * Convert a function address into the appropriate ftrace location.

 *

 * Usually this is just the address of the function, but on some architectures

 * it's more complicated so allow them to provide a custom behaviour.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * system_state.c - State of the system modified by livepatches

 *

 * Copyright (C) 2019 SUSE

/**

 * klp_get_state() - get information about system state modified by

 *	the given patch

 * @patch:	livepatch that modifies the given system state

 * @id:		custom identifier of the modified system state

 *

 * Checks whether the given patch modifies the given system state.

 *

 * The function can be called either from pre/post (un)patch

 * callbacks or from the kernel code added by the livepatch.

 *

 * Return: pointer to struct klp_state when found, otherwise NULL.

/**

 * klp_get_prev_state() - get information about system state modified by

 *	the already installed livepatches

 * @id:		custom identifier of the modified system state

 *

 * Checks whether already installed livepatches modify the given

 * system state.

 *

 * The same system state can be modified by more non-cumulative

 * livepatches. It is expected that the latest livepatch has

 * the most up-to-date information.

 *

 * The function can be called only during transition when a new

 * livepatch is being enabled or when such a transition is reverted.

 * It is typically called only from pre/post (un)patch

 * callbacks.

 *

 * Return: pointer to the latest struct klp_state from already

 *	installed livepatches, NULL when not found.

 Check if the patch is able to deal with the existing system state. */

 A cumulative livepatch must handle all already modified states. */

/*

 * Check that the new livepatch will not break the existing system states.

 * Cumulative patches must handle all already modified states.

 * Non-cumulative patches can touch already modified states.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Queued spinlock

 *

 * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.

 * (C) Copyright 2013-2014,2018 Red Hat, Inc.

 * (C) Copyright 2015 Intel Corp.

 * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP

 *

 * Authors: Waiman Long <longman@redhat.com>

 *          Peter Zijlstra <peterz@infradead.org>

/*

 * Include queued spinlock statistics code

/*

 * The basic principle of a queue-based spinlock can best be understood

 * by studying a classic queue-based spinlock implementation called the

 * MCS lock. A copy of the original MCS lock paper ("Algorithms for Scalable

 * Synchronization on Shared-Memory Multiprocessors by Mellor-Crummey and

 * Scott") is available at

 *

 * https://bugzilla.kernel.org/show_bug.cgi?id=206115

 *

 * This queued spinlock implementation is based on the MCS lock, however to

 * make it fit the 4 bytes we assume spinlock_t to be, and preserve its

 * existing API, we must modify it somehow.

 *

 * In particular; where the traditional MCS lock consists of a tail pointer

 * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to

 * unlock the next pending (next->locked), we compress both these: {tail,

 * next->locked} into a single u32 value.

 *

 * Since a spinlock disables recursion of its own context and there is a limit

 * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there

 * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now

 * we can encode the tail by combining the 2-bit nesting level with the cpu

 * number. With one byte for the lock value and 3 bytes for the tail, only a

 * 32-bit word is now needed. Even though we only need 1 bit for the lock,

 * we extend it to a full byte to achieve better performance for architectures

 * that support atomic byte write.

 *

 * We also change the first spinner to spin on the lock bit instead of its

 * node; whereby avoiding the need to carry a node from lock to unlock, and

 * preserving existing lock API. This also makes the unlock code simpler and

 * faster.

 *

 * N.B. The current implementation only supports architectures that allow

 *      atomic operations on smaller 8-bit and 16-bit data types.

 *

/*

 * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in

 * size and four of them will fit nicely in one 64-byte cacheline. For

 * pvqspinlock, however, we need more space for extra data. To accommodate

 * that, we insert two more long words to pad it up to 32 bytes. IOW, only

 * two of them can fit in a cacheline in this case. That is OK as it is rare

 * to have more than 2 levels of slowpath nesting in actual use. We don't

 * want to penalize pvqspinlocks to optimize for a rare case in native

 * qspinlocks.

/*

 * The pending bit spinning loop count.

 * This heuristic is used to limit the number of lockword accesses

 * made by atomic_cond_read_relaxed when waiting for the lock to

 * transition out of the "== _Q_PENDING_VAL" state. We don't spin

 * indefinitely because there's no guarantee that we'll make forward

 * progress.

/*

 * Per-CPU queue node structures; we can never have more than 4 nested

 * contexts: task, softirq, hardirq, nmi.

 *

 * Exactly fits one 64-byte cacheline on a 64-bit architecture.

 *

 * PV doubles the storage and uses the second cacheline for PV state.

/*

 * We must be able to distinguish between no-tail and the tail at 0:0,

 * therefore increment the cpu number by one.

 assume < 4 */

/**

 * clear_pending - clear the pending bit.

 * @lock: Pointer to queued spinlock structure

 *

 * *,1,* -> *,0,*

/**

 * clear_pending_set_locked - take ownership and clear the pending bit.

 * @lock: Pointer to queued spinlock structure

 *

 * *,1,0 -> *,0,1

 *

 * Lock stealing is not allowed if this function is used.

/*

 * xchg_tail - Put in the new queue tail code word & retrieve previous one

 * @lock : Pointer to queued spinlock structure

 * @tail : The new queue tail code word

 * Return: The previous queue tail code word

 *

 * xchg(lock, tail), which heads an address dependency

 *

 * p,*,* -> n,*,* ; prev = xchg(lock, node)

	/*

	 * We can use relaxed semantics since the caller ensures that the

	 * MCS node is properly initialized before updating the tail.

 _Q_PENDING_BITS == 8 */

/**

 * clear_pending - clear the pending bit.

 * @lock: Pointer to queued spinlock structure

 *

 * *,1,* -> *,0,*

/**

 * clear_pending_set_locked - take ownership and clear the pending bit.

 * @lock: Pointer to queued spinlock structure

 *

 * *,1,0 -> *,0,1

/**

 * xchg_tail - Put in the new queue tail code word & retrieve previous one

 * @lock : Pointer to queued spinlock structure

 * @tail : The new queue tail code word

 * Return: The previous queue tail code word

 *

 * xchg(lock, tail)

 *

 * p,*,* -> n,*,* ; prev = xchg(lock, node)

		/*

		 * We can use relaxed semantics since the caller ensures that

		 * the MCS node is properly initialized before updating the

		 * tail.

 _Q_PENDING_BITS == 8 */

/**

 * queued_fetch_set_pending_acquire - fetch the whole lock value and set pending

 * @lock : Pointer to queued spinlock structure

 * Return: The previous lock value

 *

 * *,*,* -> *,1,*

/**

 * set_locked - Set the lock bit and own the lock

 * @lock: Pointer to queued spinlock structure

 *

 * *,*,0 -> *,0,1

/*

 * Generate the native code for queued_spin_unlock_slowpath(); provide NOPs for

 * all the PV callbacks.

 _GEN_PV_LOCK_SLOWPATH */

/**

 * queued_spin_lock_slowpath - acquire the queued spinlock

 * @lock: Pointer to queued spinlock structure

 * @val: Current value of the queued spinlock 32-bit word

 *

 * (queue tail, pending bit, lock value)

 *

 *              fast     :    slow                                  :    unlock

 *                       :                                          :

 * uncontended  (0,0,0) -:--> (0,0,1) ------------------------------:--> (*,*,0)

 *                       :       | ^--------.------.             /  :

 *                       :       v           \      \            |  :

 * pending               :    (0,1,1) +--> (0,1,0)   \           |  :

 *                       :       | ^--'              |           |  :

 *                       :       v                   |           |  :

 * uncontended           :    (n,x,y) +--> (n,0,0) --'           |  :

 *   queue               :       | ^--'                          |  :

 *                       :       v                               |  :

 * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :

 *   queue               :         ^--'                             :

	/*

	 * Wait for in-progress pending->locked hand-overs with a bounded

	 * number of spins so that we guarantee forward progress.

	 *

	 * 0,1,0 -> 0,0,1

	/*

	 * If we observe any contention; queue.

	/*

	 * trylock || pending

	 *

	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock

	/*

	 * If we observe contention, there is a concurrent locker.

	 *

	 * Undo and queue; our setting of PENDING might have made the

	 * n,0,0 -> 0,0,0 transition fail and it will now be waiting

	 * on @next to become !NULL.

 Undo PENDING if we set it. */

	/*

	 * We're pending, wait for the owner to go away.

	 *

	 * 0,1,1 -> 0,1,0

	 *

	 * this wait loop must be a load-acquire such that we match the

	 * store-release that clears the locked bit and create lock

	 * sequentiality; this is because not all

	 * clear_pending_set_locked() implementations imply full

	 * barriers.

	/*

	 * take ownership and clear the pending bit.

	 *

	 * 0,1,0 -> 0,0,1

	/*

	 * End of pending bit optimistic spinning and beginning of MCS

	 * queuing.

	/*

	 * 4 nodes are allocated based on the assumption that there will

	 * not be nested NMIs taking spinlocks. That may not be true in

	 * some architectures even though the chance of needing more than

	 * 4 nodes will still be extremely unlikely. When that happens,

	 * we fall back to spinning on the lock directly without using

	 * any MCS node. This is not the most elegant solution, but is

	 * simple enough.

	/*

	 * Keep counts of non-zero index values:

	/*

	 * Ensure that we increment the head node->count before initialising

	 * the actual node. If the compiler is kind enough to reorder these

	 * stores, then an IRQ could overwrite our assignments.

	/*

	 * We touched a (possibly) cold cacheline in the per-cpu queue node;

	 * attempt the trylock once more in the hope someone let go while we

	 * weren't watching.

	/*

	 * Ensure that the initialisation of @node is complete before we

	 * publish the updated tail via xchg_tail() and potentially link

	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.

	/*

	 * Publish the updated tail.

	 * We have already touched the queueing cacheline; don't bother with

	 * pending stuff.

	 *

	 * p,*,* -> n,*,*

	/*

	 * if there was a previous node; link it and wait until reaching the

	 * head of the waitqueue.

 Link @node into the waitqueue. */

		/*

		 * While waiting for the MCS lock, the next pointer may have

		 * been set by another lock waiter. We optimistically load

		 * the next pointer & prefetch the cacheline for writing

		 * to reduce latency in the upcoming MCS unlock operation.

	/*

	 * we're at the head of the waitqueue, wait for the owner & pending to

	 * go away.

	 *

	 * *,x,y -> *,0,0

	 *

	 * this wait loop must use a load-acquire such that we match the

	 * store-release that clears the locked bit and create lock

	 * sequentiality; this is because the set_locked() function below

	 * does not imply a full barrier.

	 *

	 * The PV pv_wait_head_or_lock function, if active, will acquire

	 * the lock and return a non-zero value. So we have to skip the

	 * atomic_cond_read_acquire() call. As the next PV queue head hasn't

	 * been designated yet, there is no way for the locked value to become

	 * _Q_SLOW_VAL. So both the set_locked() and the

	 * atomic_cmpxchg_relaxed() calls will be safe.

	 *

	 * If PV isn't active, 0 will be returned instead.

	 *

	/*

	 * claim the lock:

	 *

	 * n,0,0 -> 0,0,1 : lock, uncontended

	 * *,*,0 -> *,*,1 : lock, contended

	 *

	 * If the queue head is the only one in the queue (lock value == tail)

	 * and nobody is pending, clear the tail code and grab the lock.

	 * Otherwise, we only need to grab the lock.

	/*

	 * In the PV case we might already have _Q_LOCKED_VAL set, because

	 * of lock stealing; therefore we must also allow:

	 *

	 * n,0,1 -> 0,0,1

	 *

	 * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the

	 *       above wait condition, therefore any concurrent setting of

	 *       PENDING will make the uncontended transition fail.

 No contention */

	/*

	 * Either somebody is queued behind us or _Q_PENDING_VAL got set

	 * which will then detect the remaining tail and queue behind us

	 * ensuring we'll see a @next.

	/*

	 * contended path; wait for next if not observed yet, release.

	/*

	 * release the node

/*

 * Generate the paravirt code for queued_spin_unlock_slowpath().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/locking/mutex.c

 *

 * Mutexes: blocking mutual exclusion locks

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 * Many thanks to Arjan van de Ven, Thomas Gleixner, Steven Rostedt and

 * David Howells for suggestions and improvements.

 *

 *  - Adaptive spinning for mutexes by Peter Zijlstra. (Ported to mainline

 *    from the -rt tree, where it was originally implemented for rtmutexes

 *    by Steven Rostedt, based on work by Gregory Haskins, Peter Morreale

 *    and Sven Dietrich.

 *

 * Also see Documentation/locking/mutex-design.rst.

/*

 * @owner: contains: 'struct task_struct *' to the current lock owner,

 * NULL means not owned. Since task_struct pointers are aligned at

 * at least L1_CACHE_BYTES, we have low bits to store extra state.

 *

 * Bit0 indicates a non-empty waiter list; unlock must issue a wakeup.

 * Bit1 indicates unlock needs to hand the lock to the top-waiter

 * Bit2 indicates handoff has been done and we're waiting for pickup.

/*

 * Internal helper function; C doesn't allow us to hide it :/

 *

 * DO NOT USE (outside of mutex code).

/*

 * Returns: __mutex_owner(lock) on failure or NULL on success.

 must loop, can race against a flag */

/*

 * Trylock or set HANDOFF

/*

 * Actual trylock that will work on any unlocked state.

/*

 * Lockdep annotations are contained to the slow paths for simplicity.

 * There is nothing that would stop spreading the lockdep annotations outwards

 * except more code.

/*

 * Optimistic trylock that only works in the uncontended case. Make sure to

 * follow with a __mutex_trylock() before failing.

/*

 * Add @waiter to a given location in the lock wait_list and set the

 * FLAG_WAITERS flag if it's the first waiter.

/*

 * Give up ownership to a specific task, when @task = NULL, this is equivalent

 * to a regular unlock. Sets PICKUP on a handoff, clears HANDOFF, preserves

 * WAITERS. Provides RELEASE semantics like a regular unlock, the

 * __mutex_trylock() provides a matching ACQUIRE semantics for the handoff.

/*

 * We split the mutex lock/unlock logic into separate fastpath and

 * slowpath functions, to reduce the register pressure on the fastpath.

 * We also put the fastpath first in the kernel image, to make sure the

 * branch is predicted by the CPU as default-untaken.

/**

 * mutex_lock - acquire the mutex

 * @lock: the mutex to be acquired

 *

 * Lock the mutex exclusively for this task. If the mutex is not

 * available right now, it will sleep until it can get it.

 *

 * The mutex must later on be released by the same task that

 * acquired it. Recursive locking is not allowed. The task

 * may not exit without first unlocking the mutex. Also, kernel

 * memory where the mutex resides must not be freed with

 * the mutex still locked. The mutex must first be initialized

 * (or statically defined) before it can be locked. memset()-ing

 * the mutex to 0 is not allowed.

 *

 * (The CONFIG_DEBUG_MUTEXES .config option turns on debugging

 * checks that will enforce the restrictions and will also do

 * deadlock debugging)

 *

 * This function is similar to (but not equivalent to) down().

/*

 * Trylock variant that returns the owning task on failure.

	/*

	 * If ww->ctx is set the contents are undefined, only

	 * by acquiring wait_lock there is a guarantee that

	 * they are not invalid when reading.

	 *

	 * As such, when deadlock detection needs to be

	 * performed the optimistic spinning cannot be done.

	 *

	 * Check this in every inner iteration because we may

	 * be racing against another thread's ww_mutex_lock.

	/*

	 * If we aren't on the wait list yet, cancel the spin

	 * if there are waiters. We want  to avoid stealing the

	 * lock from a waiter with an earlier stamp, since the

	 * other thread may already own a lock that we also

	 * need.

	/*

	 * Similarly, stop spinning if we are no longer the

	 * first waiter.

/*

 * Look out! "owner" is an entirely speculative pointer access and not

 * reliable.

 *

 * "noinline" so that this function shows up on perf profiles.

		/*

		 * Ensure we emit the owner->on_cpu, dereference _after_

		 * checking lock->owner still matches owner. And we already

		 * disabled preemption which is equal to the RCU read-side

		 * crital section in optimistic spinning code. Thus the

		 * task_strcut structure won't go away during the spinning

		 * period

		/*

		 * Use vcpu_is_preempted to detect lock holder preemption issue.

/*

 * Initial check for entering the mutex spinning loop

	/*

	 * We already disabled preemption which is equal to the RCU read-side

	 * crital section in optimistic spinning code. Thus the task_strcut

	 * structure won't go away during the spinning period.

	/*

	 * As lock holder preemption issue, we both skip spinning if task is not

	 * on cpu or its cpu is preempted

	/*

	 * If lock->owner is not set, the mutex has been released. Return true

	 * such that we'll trylock in the spin path, which is a faster option

	 * than the blocking slow path.

/*

 * Optimistic spinning.

 *

 * We try to spin for acquisition when we find that the lock owner

 * is currently running on a (different) CPU and while we don't

 * need to reschedule. The rationale is that if the lock owner is

 * running, it is likely to release the lock soon.

 *

 * The mutex spinners are queued up using MCS lock so that only one

 * spinner can compete for the mutex. However, if mutex spinning isn't

 * going to happen, there is no point in going through the lock/unlock

 * overhead.

 *

 * Returns true when the lock was taken, otherwise false, indicating

 * that we need to jump to the slowpath and sleep.

 *

 * The waiter flag is set to true if the spinner is a waiter in the wait

 * queue. The waiter-spinner will spin on the lock directly and concurrently

 * with the spinner at the head of the OSQ, if present, until the owner is

 * changed to itself.

		/*

		 * The purpose of the mutex_can_spin_on_owner() function is

		 * to eliminate the overhead of osq_lock() and osq_unlock()

		 * in case spinning isn't possible. As a waiter-spinner

		 * is not going to take OSQ lock anyway, there is no need

		 * to call mutex_can_spin_on_owner().

		/*

		 * In order to avoid a stampede of mutex spinners trying to

		 * acquire the mutex all at once, the spinners need to take a

		 * MCS (queued) lock first before spinning on the owner field.

 Try to acquire the mutex... */

		/*

		 * There's an owner, wait for it to either

		 * release the lock or go to sleep.

		/*

		 * The cpu_relax() call is a compiler barrier which forces

		 * everything in this loop to be re-loaded. We don't need

		 * memory barriers as we'll eventually observe the right

		 * values at the cost of a few extra spins.

	/*

	 * If we fell out of the spin path because of need_resched(),

	 * reschedule now, before we try-lock the mutex. This avoids getting

	 * scheduled out right after we obtained the mutex.

		/*

		 * We _should_ have TASK_RUNNING here, but just in case

		 * we do not, make it so, otherwise we might get stuck.

/**

 * mutex_unlock - release the mutex

 * @lock: the mutex to be released

 *

 * Unlock a mutex that has been locked by this task previously.

 *

 * This function must not be used in interrupt context. Unlocking

 * of a not locked mutex is not allowed.

 *

 * This function is similar to (but not equivalent to) up().

/**

 * ww_mutex_unlock - release the w/w mutex

 * @lock: the mutex to be released

 *

 * Unlock a mutex that has been locked by this task previously with any of the

 * ww_mutex_lock* functions (with or without an acquire context). It is

 * forbidden to release the locks after releasing the acquire context.

 *

 * This function must not be used in interrupt context. Unlocking

 * of a unlocked mutex is not allowed.

/*

 * Lock a mutex (possibly interruptible), slowpath:

		/*

		 * Reset the wounded flag after a kill. No other process can

		 * race and wound us here since they can't have a valid owner

		 * pointer if we don't have any locks held.

 got the lock, yay! */

	/*

	 * After waiting to acquire the wait_lock, try again.

 add waiting tasks to the end of the waitqueue (FIFO): */

		/*

		 * Add in stamp order, waking up waiters that must kill

		 * themselves.

		/*

		 * Once we hold wait_lock, we're serialized against

		 * mutex_unlock() handing the lock off to us, do a trylock

		 * before testing the error conditions to make sure we pick up

		 * the handoff.

		/*

		 * Check for signals and kill conditions while holding

		 * wait_lock. This ensures the lock cancellation is ordered

		 * against mutex_unlock() and wake-ups do not go missing.

		/*

		 * Here we order against unlock; we must either see it change

		 * state back to RUNNING and fall through the next schedule(),

		 * or we must see its unlock and acquire.

		/*

		 * Wound-Wait; we stole the lock (!first_waiter), check the

		 * waiters as anyone might want to wound us.

 got the lock - cleanup and rejoice! */

/**

 * ww_mutex_trylock - tries to acquire the w/w mutex with optional acquire context

 * @ww: mutex to lock

 * @ww_ctx: optional w/w acquire context

 *

 * Trylocks a mutex with the optional acquire context; no deadlock detection is

 * possible. Returns 1 if the mutex has been acquired successfully, 0 otherwise.

 *

 * Unlike ww_mutex_lock, no deadlock handling is performed. However, if a @ctx is

 * specified, -EALREADY handling may happen in calls to ww_mutex_trylock.

 *

 * A mutex acquired with this function must be released with ww_mutex_unlock.

	/*

	 * Reset the wounded flag after a kill. No other process can

	 * race and wound us here, since they can't have a valid owner

	 * pointer if we don't have any locks held.

/*

 * Release the lock, slowpath:

	/*

	 * Release the lock before (potentially) taking the spinlock such that

	 * other contenders can get on with things ASAP.

	 *

	 * Except when HANDOFF, in that case we must not clear the owner field,

	 * but instead set it to the top waiter.

 get the first entry from the wait-list: */

/*

 * Here come the less common (and hence less performance-critical) APIs:

 * mutex_lock_interruptible() and mutex_trylock().

/**

 * mutex_lock_interruptible() - Acquire the mutex, interruptible by signals.

 * @lock: The mutex to be acquired.

 *

 * Lock the mutex like mutex_lock().  If a signal is delivered while the

 * process is sleeping, this function will return without acquiring the

 * mutex.

 *

 * Context: Process context.

 * Return: 0 if the lock was successfully acquired or %-EINTR if a

 * signal arrived.

/**

 * mutex_lock_killable() - Acquire the mutex, interruptible by fatal signals.

 * @lock: The mutex to be acquired.

 *

 * Lock the mutex like mutex_lock().  If a signal which will be fatal to

 * the current process is delivered while the process is sleeping, this

 * function will return without acquiring the mutex.

 *

 * Context: Process context.

 * Return: 0 if the lock was successfully acquired or %-EINTR if a

 * fatal signal arrived.

/**

 * mutex_lock_io() - Acquire the mutex and mark the process as waiting for I/O

 * @lock: The mutex to be acquired.

 *

 * Lock the mutex like mutex_lock().  While the task is waiting for this

 * mutex, it will be accounted as being in the IO wait state by the

 * scheduler.

 *

 * Context: Process context.

/**

 * mutex_trylock - try to acquire the mutex, without waiting

 * @lock: the mutex to be acquired

 *

 * Try to acquire the mutex atomically. Returns 1 if the mutex

 * has been acquired successfully, and 0 on contention.

 *

 * NOTE: this function follows the spin_trylock() convention, so

 * it is negated from the down_trylock() return values! Be careful

 * about this when converting semaphore users to mutexes.

 *

 * This function must not be used in interrupt context. The

 * mutex must be released by the same task that acquired it.

 !CONFIG_DEBUG_LOCK_ALLOC */

 !CONFIG_PREEMPT_RT */

/**

 * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0

 * @cnt: the atomic which we are to dec

 * @lock: the mutex to return holding if we dec to 0

 *

 * return true and hold lock if we dec to 0, return false otherwise

 dec if we can't possibly hit 0 */

 we might hit 0, so take the lock */

 when we actually did the dec, we didn't hit 0 */

 we hit 0, and we hold the lock */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Module-based torture test facility for locking

 *

 * Copyright (C) IBM Corporation, 2014

 *

 * Authors: Paul E. McKenney <paulmck@linux.ibm.com>

 *          Davidlohr Bueso <dave@stgolabs.net>

 *	Based on kernel/rcu/torture.c.

 Forward reference. */

/*

 * Operations vector for selecting different types of tests.

 for irq spinlocks */

 writer statistics */

 reader statistics */

/*

 * Definitions for lock torture testing.

 BUGGY, do not use in real life!!! */

 We want a long delay occasionally to force massive contention.  */

 Allow test to be preempted. */

 BUGGY, do not use in real life!!! */

 Only rtmutexes care about priority */

	/* We want a short delay mostly to emulate likely code, and

	 * we want a long delay occasionally to force massive contention.

 Allow test to be preempted. */

	/* We want a short delay mostly to emulate likely code, and

	 * we want a long delay occasionally to force massive contention.

	/* We want a short delay mostly to emulate likely code, and

	 * we want a long delay occasionally to force massive contention.

 We want a long delay occasionally to force massive contention.  */

 Allow test to be preempted. */

/*

 * The torture ww_mutexes should belong to the same lock class as

 * torture_ww_class to avoid lockdep problem. The ww_mutex_init()

 * function is called for initialization to ensure that.

 yes, quite arbitrary */

		/*

		 * Boost priority once every ~50k operations. When the

		 * task tries to take the lock, the rtmutex it will account

		 * for the new priority, and do any corresponding pi-dance.

 common case, do nothing */

		/*

		 * The task will remain boosted for another ~500k operations,

		 * then restored back to its original prio, and so forth.

		 *

		 * When @trsp is nil, we want to force-reset the task for

		 * stopping the kthread.

 common case, do nothing */

	/*

	 * We want a short delay mostly to emulate likely code, and

	 * we want a long delay occasionally to force massive contention.

 Allow test to be preempted. */

 We want a long delay occasionally to force massive contention.  */

 Allow test to be preempted. */

 We want a long delay occasionally to force massive contention.  */

 Allow test to be preempted. */

/*

 * Lock torture writer kthread.  Repeatedly acquires and releases

 * the lock, checking for duplicate acquisitions.

 rare, but... */

 reset prio */

/*

 * Lock torture reader kthread.  Repeatedly acquires and releases

 * the reader lock.

 rare, but... */

/*

 * Create an lock-torture-statistics message in the specified buffer.

/*

 * Print torture statistics.  Caller must ensure that there is only one

 * call to this function at a given time!!!  This is normally accomplished

 * by relying on the module system to only have one copy of the module

 * loaded, and then by giving the lock_torture_stats kthread full control

 * (or the init/cleanup functions when lock_torture_stats thread is not

 * running).

/*

 * Periodically prints torture statistics, if periodic statistics printing

 * was specified via the stat_interval module parameter.

 *

 * No need to worry about fullstop here, since this one doesn't reference

 * volatile state or register callbacks.

	/*

	 * Indicates early cleanup, meaning that the test has not run,

	 * such as when passing bogus args when loading the module.

	 * However cxt->cur_ops.init() may have been invoked, so beside

	 * perform the underlying torture-specific cleanups, cur_ops.exit()

	 * will be invoked if needed.

 -After- the stats thread is stopped! */

 Process args and tell the world that the torturer is on the job. */

 Initialize the statistics so that each run gets its own numbers. */

			/*

			 * By default distribute evenly the number of

			 * readers and writers. We still run the same number

			 * of threads as the writer-only locks default.

 user doesn't care */

 Prepare torture context. */

	/*

	 * Create the kthreads and start torturing (oh, those poor little locks).

	 *

	 * TODO: Note that we interleave writers with readers, giving writers a

	 * slight advantage, by creating its kthread first. This can be modified

	 * for very specific needs, or even let the user choose the policy, if

	 * ever wanted.

 Create writer. */

 Create reader. */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

/*

 * An MCS like lock especially tailored for optimistic spinning for sleeping

 * lock implementations (mutex, rwsem, etc).

 *

 * Using a single mcs node per CPU is safe because sleeping locks should not be

 * called from interrupt context and we have preemption disabled while

 * spinning.

/*

 * We use the value 0 to represent "no CPU", thus the encoded value

 * will be the CPU number incremented by 1.

/*

 * Get a stable @node->next pointer, either for unlock() or unqueue() purposes.

 * Can return NULL in case we were the last queued and we updated @lock instead.

	/*

	 * If there is a prev node in queue, then the 'old' value will be

	 * the prev node's CPU #, else it's set to OSQ_UNLOCKED_VAL since if

	 * we're currently last in queue, then the queue will then become empty.

			/*

			 * We were the last queued, we moved @lock back. @prev

			 * will now observe @lock and will complete its

			 * unlock()/unqueue().

		/*

		 * We must xchg() the @node->next value, because if we were to

		 * leave it in, a concurrent unlock()/unqueue() from

		 * @node->next might complete Step-A and think its @prev is

		 * still valid.

		 *

		 * If the concurrent unlock()/unqueue() wins the race, we'll

		 * wait for either @lock to point to us, through its Step-B, or

		 * wait for a new @node->next from its Step-C.

	/*

	 * We need both ACQUIRE (pairs with corresponding RELEASE in

	 * unlock() uncontended, or fastpath) and RELEASE (to publish

	 * the node fields we just initialised) semantics when updating

	 * the lock tail.

	/*

	 * osq_lock()			unqueue

	 *

	 * node->prev = prev		osq_wait_next()

	 * WMB				MB

	 * prev->next = node		next->prev = prev // unqueue-C

	 *

	 * Here 'node->prev' and 'next->prev' are the same variable and we need

	 * to ensure these stores happen in-order to avoid corrupting the list.

	/*

	 * Normally @prev is untouchable after the above store; because at that

	 * moment unlock can proceed and wipe the node element from stack.

	 *

	 * However, since our nodes are static per-cpu storage, we're

	 * guaranteed their existence -- this allows us to apply

	 * cmpxchg in an attempt to undo our queueing.

	/*

	 * Wait to acquire the lock or cancellation. Note that need_resched()

	 * will come with an IPI, which will wake smp_cond_load_relaxed() if it

	 * is implemented with a monitor-wait. vcpu_is_preempted() relies on

	 * polling, be careful.

 unqueue */

	/*

	 * Step - A  -- stabilize @prev

	 *

	 * Undo our @prev->next assignment; this will make @prev's

	 * unlock()/unqueue() wait for a next pointer since @lock points to us

	 * (or later).

		/*

		 * cpu_relax() below implies a compiler barrier which would

		 * prevent this comparison being optimized away.

		/*

		 * We can only fail the cmpxchg() racing against an unlock(),

		 * in which case we should observe @node->locked becoming

		 * true.

		/*

		 * Or we race against a concurrent unqueue()'s step-B, in which

		 * case its step-C will write us a new @node->prev pointer.

	/*

	 * Step - B -- stabilize @next

	 *

	 * Similar to unlock(), wait for @node->next or move @lock from @node

	 * back to @prev.

	/*

	 * Step - C -- unlink

	 *

	 * @prev is stable because its still waiting for a new @prev->next

	 * pointer, @next is stable because our @node->next pointer is NULL and

	 * it will wait in Step-A.

	/*

	 * Fast path for the uncontended case.

	/*

	 * Second most likely case.

 SPDX-License-Identifier: GPL-2.0

/*

 * kernel/lockdep_proc.c

 *

 * Runtime locking correctness validator

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra

 *

 * Code for /proc/lockdep and /proc/lockdep_stats:

 *

 CONFIG_PROVE_LOCKING */

	/*

	 * Total number of dependencies:

	 *

	 * All irq-safe locks may nest inside irq-unsafe locks,

	 * plus all the other known dependencies:

	/*

	 * Zapped classes and lockdep data buffers reuse statistics.

/*

 * sort on absolute number of contentions

 for display rounding */

 XXX truncates versions > 9 */

 CONFIG_LOCK_STAT */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (2004) Linus Torvalds

 *

 * Author: Zwane Mwaikambo <zwane@fsmlabs.com>

 *

 * Copyright (2004, 2005) Ingo Molnar

 *

 * This file contains the spinlock/rwlock implementations for the

 * SMP and the DEBUG_SPINLOCK cases. (UP-nondebug inlines them)

 *

 * Note that some architectures have special knowledge about the

 * stack frames of these functions in their profile_pc. If you

 * change anything significant here that could change the stack

 * frame contact the architecture maintainers.

/*

 * If lockdep is enabled then we use the non-preemption spin-ops

 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are

 * not re-enabled during lock-acquire (which the preempt-spin-ops do):

/*

 * The __lock_function inlines are taken from

 * spinlock : include/linux/spinlock_api_smp.h

 * rwlock   : include/linux/rwlock_api_smp.h

/*

 * Some architectures can relax in favour of the CPU owning the lock.

/*

 * We build the __lock_function inlines here. They are too large for

 * inlining all over the place, but here is only one user per function

 * which embeds them into the calling _lock_function below.

 *

 * This could be a long-held lock. We both prepare to spin for a long

 * time (making _this_ CPU preemptible if possible), and we also signal

 * towards that other CPU that it should break the lock ASAP.

							*/	\

 Careful: we must exclude softirqs too, hence the	*/	\

 irq-disabling. We use the generic preemption-aware	*/	\

 function:						*/	\

*/								\

/*

 * Build preemption-friendly versions of the following

 * lock-spinning functions:

 *

 *         __[spin|read|write]_lock()

 *         __[spin|read|write]_lock_irq()

 *         __[spin|read|write]_lock_irqsave()

 *         __[spin|read|write]_lock_bh()

 !CONFIG_PREEMPT_RT */

 Linker adds these: start and end of __lockfunc functions */

 SPDX-License-Identifier: GPL-2.0 */

/*

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 *

 * This program is distributed in the hope that it will be useful,

 * but WITHOUT ANY WARRANTY; without even the implied warranty of

 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the

 * GNU General Public License for more details.

 *

 * Authors: Waiman Long <waiman.long@hpe.com>

/*

 * Collect locking event counts

/*

 * When CONFIG_LOCK_EVENT_COUNTS is enabled, event counts of different

 * types of locks will be reported under the <debugfs>/lock_event_counts/

 * directory. See lock_events_list.h for the list of available locking

 * events.

 *

 * Writing to the special ".reset_counts" file will reset all the above

 * locking event counts. This is a very slow operation and so should not

 * be done frequently.

 *

 * These event counts are implemented as per-cpu variables which are

 * summed and computed whenever the corresponding debugfs files are read. This

 * minimizes added overhead making the counts usable even in a production

 * environment.

/*

 * Per-cpu counts

/*

 * The lockevent_read() function can be overridden.

	/*

	 * Get the counter ID stored in file->f_inode->i_private

/*

 * Function to handle write request

 *

 * When idx = reset_cnts, reset all the counts.

	/*

	 * Get the counter ID stored in file->f_inode->i_private

/*

 * Debugfs data structures

	/*

	 * Skip PV qspinlock events on bare metal.

/*

 * Initialize debugfs for the locking event counts.

	/*

	 * Create the debugfs files

	 *

	 * As reading from and writing to the stat files can be slow, only

	 * root is allowed to do the read/write to limit impact to system

	 * performance.

 SPDX-License-Identifier: GPL-2.0

/* kernel/rwsem.c: R/W semaphores, public implementation

 *

 * Written by David Howells (dhowells@redhat.com).

 * Derived from asm-i386/semaphore.h

 *

 * Writer lock-stealing by Alex Shi <alex.shi@intel.com>

 * and Michel Lespinasse <walken@google.com>

 *

 * Optimistic spinning by Tim Chen <tim.c.chen@intel.com>

 * and Davidlohr Bueso <davidlohr@hp.com>. Based on mutexes.

 *

 * Rwsem count bit fields re-definition and rwsem rearchitecture by

 * Waiman Long <longman@redhat.com> and

 * Peter Zijlstra <peterz@infradead.org>.

/*

 * The least significant 2 bits of the owner value has the following

 * meanings when set.

 *  - Bit 0: RWSEM_READER_OWNED - The rwsem is owned by readers

 *  - Bit 1: RWSEM_NONSPINNABLE - Cannot spin on a reader-owned lock

 *

 * When the rwsem is reader-owned and a spinning writer has timed out,

 * the nonspinnable bit will be set to disable optimistic spinning.



 * When a writer acquires a rwsem, it puts its task_struct pointer

 * into the owner field. It is cleared after an unlock.

 *

 * When a reader acquires a rwsem, it will also puts its task_struct

 * pointer into the owner field with the RWSEM_READER_OWNED bit set.

 * On unlock, the owner field will largely be left untouched. So

 * for a free or reader-owned rwsem, the owner value may contain

 * information about the last reader that acquires the rwsem.

 *

 * That information may be helpful in debugging cases where the system

 * seems to hang on a reader owned rwsem especially if only one reader

 * is involved. Ideally we would like to track all the readers that own

 * a rwsem, but the overhead is simply too big.

 *

 * A fast path reader optimistic lock stealing is supported when the rwsem

 * is previously owned by a writer and the following conditions are met:

 *  - rwsem is not currently writer owned

 *  - the handoff isn't set.

/*

 * On 64-bit architectures, the bit definitions of the count are:

 *

 * Bit  0    - writer locked bit

 * Bit  1    - waiters present bit

 * Bit  2    - lock handoff bit

 * Bits 3-7  - reserved

 * Bits 8-62 - 55-bit reader count

 * Bit  63   - read fail bit

 *

 * On 32-bit architectures, the bit definitions of the count are:

 *

 * Bit  0    - writer locked bit

 * Bit  1    - waiters present bit

 * Bit  2    - lock handoff bit

 * Bits 3-7  - reserved

 * Bits 8-30 - 23-bit reader count

 * Bit  31   - read fail bit

 *

 * It is not likely that the most significant bit (read fail bit) will ever

 * be set. This guard bit is still checked anyway in the down_read() fastpath

 * just in case we need to use up more of the reader bits for other purpose

 * in the future.

 *

 * atomic_long_fetch_add() is used to obtain reader lock, whereas

 * atomic_long_cmpxchg() will be used to obtain writer lock.

 *

 * There are three places where the lock handoff bit may be set or cleared.

 * 1) rwsem_mark_wake() for readers.

 * 2) rwsem_try_write_lock() for writers.

 * 3) Error path of rwsem_down_write_slowpath().

 *

 * For all the above cases, wait_lock will be held. A writer must also

 * be the first one in the wait_list to be eligible for setting the handoff

 * bit. So concurrent setting/clearing of handoff bit is not possible.

/*

 * All writes to owner are protected by WRITE_ONCE() to make sure that

 * store tearing can't happen as optimistic spinners may read and use

 * the owner value concurrently without lock. Read from owner, however,

 * may not need READ_ONCE() as long as the pointer value is only used

 * for comparison and isn't being dereferenced.

/*

 * Test the flags in the owner field.

/*

 * The task_struct pointer of the last owning reader will be left in

 * the owner field.

 *

 * Note that the owner value just indicates the task has owned the rwsem

 * previously, it may not be the real owner or one of the real owners

 * anymore when that field is examined, so take it with a grain of salt.

 *

 * The reader non-spinnable bit is preserved.

/*

 * Return true if the rwsem is owned by a reader.

	/*

	 * Check the count to see if it is write-locked.

/*

 * With CONFIG_DEBUG_RWSEMS configured, it will make sure that if there

 * is a task pointer in owner of a reader-owned rwsem, it will be the

 * real owner or one of the real owners. The only exception is when the

 * unlock is done by up_read_non_owner().

/*

 * Set the RWSEM_NONSPINNABLE bits if the RWSEM_READER_OWNED flag

 * remains set. Otherwise, the operation will be aborted.

/*

 * Return just the real task structure pointer of the owner

/*

 * Return the real task structure pointer of the owner and the embedded

 * flags in the owner. pflags must be non-NULL.

/*

 * Guide to the rw_semaphore's count field.

 *

 * When the RWSEM_WRITER_LOCKED bit in count is set, the lock is owned

 * by a writer.

 *

 * The lock is owned by readers when

 * (1) the RWSEM_WRITER_LOCKED isn't set in count,

 * (2) some of the reader bits are set in count, and

 * (3) the owner field has RWSEM_READ_OWNED bit set.

 *

 * Having some reader bits set is not enough to guarantee a readers owned

 * lock as the readers may be in the process of backing out from the count

 * and a writer has just released the lock. So another writer may steal

 * the lock immediately after that.

/*

 * Initialize an rwsem:

	/*

	 * Make sure we are not reinitializing a held semaphore:

 Wake whatever's at head of wait list */

 Wake readers only */

 Waker thread holds the read lock */

 Writer is not first in wait list */

 Writer is first in wait list     */

 Writer is first & handoff needed */

/*

 * The typical HZ value is either 250 or 1000. So set the minimum waiting

 * time to at least 4ms or 1 jiffy (if it is higher than 4ms) in the wait

 * queue before initiating the handoff protocol.

/*

 * Magic number to batch-wakeup waiting readers, even when writers are

 * also present in the queue. This both limits the amount of work the

 * waking thread must do and also prevents any potential counter overflow,

 * however unlikely.

/*

 * handle the lock release when processes blocked on it that can now run

 * - if we come here from up_xxxx(), then the RWSEM_FLAG_WAITERS bit must

 *   have been set.

 * - there must be someone on the queue

 * - the wait_lock must be held by the caller

 * - tasks are marked for wakeup, the caller must later invoke wake_up_q()

 *   to actually wakeup the blocked task(s) and drop the reference count,

 *   preferably when the wait_lock is released

 * - woken process blocks are discarded from the list after having task zeroed

 * - writers are only marked woken if downgrading is false

	/*

	 * Take a peek at the queue head waiter such that we can determine

	 * the wakeup(s) to perform.

			/*

			 * Mark writer at the front of the queue for wakeup.

			 * Until the task is actually later awoken later by

			 * the caller, other writers are able to steal it.

			 * Readers, on the other hand, will block as they

			 * will notice the queued writer.

	/*

	 * No reader wakeup if there are too many of them already.

	/*

	 * Writers might steal the lock before we grant it to the next reader.

	 * We prefer to do the first reader grant before counting readers

	 * so we can bail out early if a writer stole the lock.

			/*

			 * When we've been waiting "too" long (for writers

			 * to give up the lock), request a HANDOFF to

			 * force the issue.

		/*

		 * Set it to reader-owned to give spinners an early

		 * indication that readers now have the lock.

		 * The reader nonspinnable bit seen at slowpath entry of

		 * the reader is copied over.

	/*

	 * Grant up to MAX_READERS_WAKEUP read locks to all the readers in the

	 * queue. We know that the woken will be at least 1 as we accounted

	 * for above. Note we increment the 'active part' of the count by the

	 * number of readers before waking any processes up.

	 *

	 * This is an adaptation of the phase-fair R/W locks where at the

	 * reader phase (first waiter is a reader), all readers are eligible

	 * to acquire the lock at the same time irrespective of their order

	 * in the queue. The writers acquire the lock according to their

	 * order in the queue.

	 *

	 * We have to do wakeup in 2 passes to prevent the possibility that

	 * the reader count may be decremented before it is incremented. It

	 * is because the to-be-woken waiter may not have slept yet. So it

	 * may see waiter->task got cleared, finish its critical section and

	 * do an unlock before the reader count increment.

	 *

	 * 1) Collect the read-waiters in a separate list, count them and

	 *    fully increment the reader count in rwsem.

	 * 2) For each waiters in the new list, clear waiter->task and

	 *    put them into wake_q to be woken up later.

		/*

		 * Limit # of readers that can be woken up per wakeup call.

 hit end of list above */

	/*

	 * When we've woken a reader, we no longer need to force writers

	 * to give up the lock and we can clear HANDOFF.

 2nd pass */

		/*

		 * Ensure calling get_task_struct() before setting the reader

		 * waiter to nil such that rwsem_down_read_slowpath() cannot

		 * race with do_exit() by always holding a reference count

		 * to the task to wakeup.

		/*

		 * Ensure issuing the wakeup (either by us or someone else)

		 * after setting the reader waiter to nil.

/*

 * This function must be called with the sem->wait_lock held to prevent

 * race conditions between checking the rwsem wait list and setting the

 * sem->count accordingly.

 *

 * If wstate is WRITER_HANDOFF, it will make sure that either the handoff

 * bit is set or the lock is acquired with handoff bit cleared.

	/*

	 * We have either acquired the lock with handoff bit cleared or

	 * set the handoff bit.

/*

 * The rwsem_spin_on_owner() function returns the following 4 values

 * depending on the lock owner state.

 *   OWNER_NULL  : owner is currently NULL

 *   OWNER_WRITER: when owner changes and is a writer

 *   OWNER_READER: when owner changes and the new owner may be a reader.

 *   OWNER_NONSPINNABLE:

 *		   when optimistic spinning has to stop because either the

 *		   owner stops running, is unknown, or its timeslice has

 *		   been used up.

/*

 * Try to acquire write lock before the writer has been put on wait queue.

	/*

	 * As lock holder preemption issue, we both skip spinning if

	 * task is not on cpu or its cpu is preempted

	/*

	 * Disable preemption is equal to the RCU read-side crital section,

	 * thus the task_strcut structure won't go away.

	/*

	 * Don't check the read-owner as the entry may be stale.

		/*

		 * When a waiting writer set the handoff flag, it may spin

		 * on the owner as well. Once that writer acquires the lock,

		 * we can spin on it. So we don't need to quit even when the

		 * handoff bit is set.

		/*

		 * Ensure we emit the owner->on_cpu, dereference _after_

		 * checking sem->owner still matches owner, if that fails,

		 * owner might point to free()d memory, if it still matches,

		 * our spinning context already disabled preemption which is

		 * equal to RCU read-side crital section ensures the memory

		 * stays valid.

/*

 * Calculate reader-owned rwsem spinning threshold for writer

 *

 * The more readers own the rwsem, the longer it will take for them to

 * wind down and free the rwsem. So the empirical formula used to

 * determine the actual spinning time limit here is:

 *

 *   Spinning threshold = (10 + nr_readers/2)us

 *

 * The limit is capped to a maximum of 25us (30 readers). This is just

 * a heuristic and is subjected to change in the future.

 sem->wait_lock should not be held when doing optimistic spinning */

	/*

	 * Optimistically spin on the owner field and attempt to acquire the

	 * lock whenever the owner changes. Spinning will be stopped when:

	 *  1) the owning writer isn't running; or

	 *  2) readers own the lock and spinning time has exceeded limit.

		/*

		 * Try to acquire the lock

		/*

		 * Time-based reader-owned rwsem optimistic spinning

			/*

			 * Re-initialize rspin_threshold every time when

			 * the owner state changes from non-reader to reader.

			 * This allows a writer to steal the lock in between

			 * 2 reader phases and have the threshold reset at

			 * the beginning of the 2nd reader phase.

			/*

			 * Check time threshold once every 16 iterations to

			 * avoid calling sched_clock() too frequently so

			 * as to reduce the average latency between the times

			 * when the lock becomes free and when the spinner

			 * is ready to do a trylock.

		/*

		 * An RT task cannot do optimistic spinning if it cannot

		 * be sure the lock holder is running or live-lock may

		 * happen if the current task and the lock holder happen

		 * to run in the same CPU. However, aborting optimistic

		 * spinning while a NULL owner is detected may miss some

		 * opportunity where spinning can continue without causing

		 * problem.

		 *

		 * There are 2 possible cases where an RT task may be able

		 * to continue spinning.

		 *

		 * 1) The lock owner is in the process of releasing the

		 *    lock, sem->owner is cleared but the lock has not

		 *    been released yet.

		 * 2) The lock was free and owner cleared, but another

		 *    task just comes in and acquire the lock before

		 *    we try to get it. The new owner may be a spinnable

		 *    writer.

		 *

		 * To take advantage of two scenarios listed above, the RT

		 * task is made to retry one more time to see if it can

		 * acquire the lock or continue spinning on the new owning

		 * writer. Of course, if the time lag is long enough or the

		 * new owner is not a writer or spinnable, the RT task will

		 * quit spinning.

		 *

		 * If the owner is a writer, the need_resched() check is

		 * done inside rwsem_spin_on_owner(). If the owner is not

		 * a writer, need_resched() check needs to be done here.

		/*

		 * The cpu_relax() call is a compiler barrier which forces

		 * everything in this loop to be re-loaded. We don't need

		 * memory barriers as we'll eventually observe the right

		 * values at the cost of a few extra spins.

/*

 * Clear the owner's RWSEM_NONSPINNABLE bit if it is set. This should

 * only be called when the reader count reaches 0.

/*

 * Wait for the read lock to be granted

	/*

	 * To prevent a constant stream of readers from starving a sleeping

	 * waiter, don't attempt optimistic lock stealing if the lock is

	 * currently owned by readers.

	/*

	 * Reader optimistic lock stealing.

		/*

		 * Wake up other readers in the wait queue if it is

		 * the first reader.

		/*

		 * In case the wait queue is empty and the lock isn't owned

		 * by a writer or has the handoff bit set, this reader can

		 * exit the slowpath and return immediately as its

		 * RWSEM_READER_BIAS has already been set in the count.

 Provide lock ACQUIRE */

 we're now waiting on the lock, but no longer actively locking */

	/*

	 * If there are no active locks, wake the front queued process(es).

	 *

	 * If there are no writers and we are first in the queue,

	 * wake our own waiter to join the existing active readers !

 wait to be given the lock */

 Matches rwsem_mark_wake()'s smp_store_release(). */

 Ordered by sem->wait_lock against rwsem_mark_wake(). */

/*

 * Wait until we successfully acquire the write lock

 do optimistic spinning and steal lock if possible */

 rwsem_optimistic_spin() implies ACQUIRE on success */

	/*

	 * Optimistic spinning failed, proceed to the slowpath

	 * and block until we can acquire the sem.

 account for this before adding a new element to the list */

 we're now waiting on the lock */

		/*

		 * If there were already threads queued before us and:

		 *  1) there are no active locks, wake the front

		 *     queued process(es) as the handoff bit might be set.

		 *  2) there are no active writers and some readers, the lock

		 *     must be read owned; so we try to wake any read lock

		 *     waiters that were queued ahead of us.

			/*

			 * We want to minimize wait_lock hold time especially

			 * when a large number of readers are to be woken up.

 Used again, reinit */

 wait until we successfully acquire the lock */

 rwsem_try_write_lock() implies ACQUIRE on success */

		/*

		 * After setting the handoff bit and failing to acquire

		 * the lock, attempt to spin on owner to accelerate lock

		 * transfer. If the previous owner is a on-cpu writer and it

		 * has just released the lock, OWNER_NULL will be returned.

		 * In this case, we attempt to acquire the lock again

		 * without sleeping.

 Block until there are no active lockers. */

			/*

			 * If HANDOFF bit is set, unconditionally do

			 * a trylock.

			/*

			 * The setting of the handoff bit is deferred

			 * until rwsem_try_write_lock() is called.

/*

 * handle waking up a waiter on the semaphore

 * - up_read/up_write has decremented the active part of count if we come here

/*

 * downgrade a write lock into a read lock

 * - caller incremented waiting part of count and discovered it still negative

 * - just wake up any readers at the front of the queue

/*

 * lock for reading

	/*

	 * Optimize for the case when the rwsem is not locked at all.

/*

 * lock for writing

/*

 * unlock after reading

/*

 * unlock after writing

	/*

	 * sem->owner may differ from current if the ownership is transferred

	 * to an anonymous writer by setting the RWSEM_NONSPINNABLE bits.

/*

 * downgrade write lock to read lock

	/*

	 * When downgrading from exclusive to shared ownership,

	 * anything inside the write-locked region cannot leak

	 * into the read side. In contrast, anything in the

	 * read-locked region is ok to be re-ordered into the

	 * write side. As such, rely on RELEASE semantics.

 !CONFIG_PREEMPT_RT */

 Debug stubs for the common API */

 CONFIG_PREEMPT_RT */

/*

 * lock for reading

/*

 * trylock for reading -- returns 1 if successful, 0 if contention

/*

 * lock for writing

/*

 * lock for writing

/*

 * trylock for writing -- returns 1 if successful, 0 if contention

/*

 * release a read lock

/*

 * release a write lock

/*

 * downgrade write lock to read lock

 SPDX-License-Identifier: GPL-2.0-only

/*

 * RT-Mutexes: simple blocking mutual exclusion locks with PI support

 *

 * started by Ingo Molnar and Thomas Gleixner.

 *

 *  Copyright (C) 2004-2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>

 *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt

 *  Copyright (C) 2006 Esben Nielsen

 * Adaptive Spinlocks:

 *  Copyright (C) 2008 Novell, Inc., Gregory Haskins, Sven Dietrich,

 *				     and Peter Morreale,

 * Adaptive Spinlocks simplification:

 *  Copyright (C) 2008 Red Hat, Inc., Steven Rostedt <srostedt@redhat.com>

 *

 *  See Documentation/locking/rt-mutex-design.rst for details.

/*

 * lock->owner state tracking:

 *

 * lock->owner holds the task_struct pointer of the owner. Bit 0

 * is used to keep track of the "lock has waiters" state.

 *

 * owner	bit0

 * NULL		0	lock is free (fast acquire possible)

 * NULL		1	lock is free and has waiters and the top waiter

 *				is going to take the lock*

 * taskpointer	0	lock is held (fast release possible)

 * taskpointer	1	lock is held and has waiters**

 *

 * The fast atomic compare exchange based acquire and release is only

 * possible when bit 0 of lock->owner is 0.

 *

 * (*) It also can be a transitional state when grabbing the lock

 * with ->wait_lock is held. To prevent any fast path cmpxchg to the lock,

 * we need to set the bit0 before looking at the lock, and the owner may be

 * NULL in this small time, hence this can be a transitional state.

 *

 * (**) There is a small time when bit 0 is set but there are no

 * waiters. This can happen when grabbing the lock in the slow path.

 * To prevent a cmpxchg of the owner releasing the lock, we need to

 * set this bit before looking at the lock.

	/*

	 * The rbtree has no waiters enqueued, now make sure that the

	 * lock->owner still has the waiters bit set, otherwise the

	 * following can happen:

	 *

	 * CPU 0	CPU 1		CPU2

	 * l->owner=T1

	 *		rt_mutex_lock(l)

	 *		lock(l->lock)

	 *		l->owner = T1 | HAS_WAITERS;

	 *		enqueue(T2)

	 *		boost()

	 *		  unlock(l->lock)

	 *		block()

	 *

	 *				rt_mutex_lock(l)

	 *				lock(l->lock)

	 *				l->owner = T1 | HAS_WAITERS;

	 *				enqueue(T3)

	 *				boost()

	 *				  unlock(l->lock)

	 *				block()

	 *		signal(->T2)	signal(->T3)

	 *		lock(l->lock)

	 *		dequeue(T2)

	 *		deboost()

	 *		  unlock(l->lock)

	 *				lock(l->lock)

	 *				dequeue(T3)

	 *				 ==> wait list is empty

	 *				deboost()

	 *				 unlock(l->lock)

	 *		lock(l->lock)

	 *		fixup_rt_mutex_waiters()

	 *		  if (wait_list_empty(l) {

	 *		    l->owner = owner

	 *		    owner = l->owner & ~HAS_WAITERS;

	 *		      ==> l->owner = T1

	 *		  }

	 *				lock(l->lock)

	 * rt_mutex_unlock(l)		fixup_rt_mutex_waiters()

	 *				  if (wait_list_empty(l) {

	 *				    owner = l->owner & ~HAS_WAITERS;

	 * cmpxchg(l->owner, T1, NULL)

	 *  ===> Success (l->owner = NULL)

	 *

	 *				    l->owner = owner

	 *				      ==> l->owner = T1

	 *				  }

	 *

	 * With the check for the waiter bit in place T3 on CPU2 will not

	 * overwrite. All tasks fiddling with the waiters bit are

	 * serialized by l->lock, so nothing else can modify the waiters

	 * bit. If the bit is set then nothing can change l->owner either

	 * so the simple RMW is safe. The cmpxchg() will simply fail if it

	 * happens in the middle of the RMW because the waiters bit is

	 * still set.

/*

 * We can speed up the acquire/release, if there's no debugging state to be

 * set up.

/*

 * Callers must hold the ->wait_lock -- which is the whole purpose as we force

 * all future threads that attempt to [Rmw] the lock to the slowpath. As such

 * relaxed semantics suffice.

/*

 * Safe fastpath aware unlock:

 * 1) Clear the waiters bit

 * 2) Drop lock->wait_lock

 * 3) Try to unlock the lock with cmpxchg

	/*

	 * If a new waiter comes in between the unlock and the cmpxchg

	 * we have two situations:

	 *

	 * unlock(wait_lock);

	 *					lock(wait_lock);

	 * cmpxchg(p, owner, 0) == owner

	 *					mark_rt_mutex_waiters(lock);

	 *					acquire(lock);

	 * or:

	 *

	 * unlock(wait_lock);

	 *					lock(wait_lock);

	 *					mark_rt_mutex_waiters(lock);

	 *

	 * cmpxchg(p, owner, 0) != owner

	 *					enqueue_waiter();

	 *					unlock(wait_lock);

	 * lock(wait_lock);

	 * wake waiter();

	 * unlock(wait_lock);

	 *					lock(wait_lock);

	 *					acquire(lock);

/*

 * Simple slow path only version: lock->owner is protected by lock->wait_lock.

/*

 * Only use with rt_mutex_waiter_{less,equal}()

	/*

	 * If both waiters have dl_prio(), we check the deadlines of the

	 * associated tasks.

	 * If left waiter has a dl_prio(), and we didn't return 1 above,

	 * then right waiter has a dl_prio() too.

	/*

	 * If both waiters have dl_prio(), we check the deadlines of the

	 * associated tasks.

	 * If left waiter has a dl_prio(), and we didn't return 0 above,

	 * then right waiter has a dl_prio() too.

	/*

	 * Note that RT tasks are excluded from same priority (lateral)

	 * steals to prevent the introduction of an unbounded latency.

 NOTE: relies on waiter->ww_ctx being set before insertion */

 RT mutex specific wake_q wrappers */

 Pairs with preempt_disable() in mark_wakeup_next_waiter() */

/*

 * Deadlock detection is conditional:

 *

 * If CONFIG_DEBUG_RT_MUTEXES=n, deadlock detection is only conducted

 * if the detect argument is == RT_MUTEX_FULL_CHAINWALK.

 *

 * If CONFIG_DEBUG_RT_MUTEXES=y, deadlock detection is always

 * conducted independent of the detect argument.

 *

 * If the waiter argument is NULL this indicates the deboost path and

 * deadlock detection is disabled independent of the detect argument

 * and the config settings.

/*

 * Adjust the priority chain. Also used for deadlock detection.

 * Decreases task's usage by one - may thus free the task.

 *

 * @task:	the task owning the mutex (owner) for which a chain walk is

 *		probably needed

 * @chwalk:	do we have to carry out deadlock detection?

 * @orig_lock:	the mutex (can be NULL if we are walking the chain to recheck

 *		things for a task that has just got its priority adjusted, and

 *		is waiting on a mutex)

 * @next_lock:	the mutex on which the owner of @orig_lock was blocked before

 *		we dropped its pi_lock. Is never dereferenced, only used for

 *		comparison to detect lock chain changes.

 * @orig_waiter: rt_mutex_waiter struct for the task that has just donated

 *		its priority to the mutex owner (can be NULL in the case

 *		depicted above or if the top waiter is gone away and we are

 *		actually deboosting the owner)

 * @top_task:	the current top waiter

 *

 * Returns 0 or -EDEADLK.

 *

 * Chain walk basics and protection scope

 *

 * [R] refcount on task

 * [P] task->pi_lock held

 * [L] rtmutex->wait_lock held

 *

 * Step	Description				Protected by

 *	function arguments:

 *	@task					[R]

 *	@orig_lock if != NULL			@top_task is blocked on it

 *	@next_lock				Unprotected. Cannot be

 *						dereferenced. Only used for

 *						comparison.

 *	@orig_waiter if != NULL			@top_task is blocked on it

 *	@top_task				current, or in case of proxy

 *						locking protected by calling

 *						code

 *	again:

 *	  loop_sanity_check();

 *	retry:

 * [1]	  lock(task->pi_lock);			[R] acquire [P]

 * [2]	  waiter = task->pi_blocked_on;		[P]

 * [3]	  check_exit_conditions_1();		[P]

 * [4]	  lock = waiter->lock;			[P]

 * [5]	  if (!try_lock(lock->wait_lock)) {	[P] try to acquire [L]

 *	    unlock(task->pi_lock);		release [P]

 *	    goto retry;

 *	  }

 * [6]	  check_exit_conditions_2();		[P] + [L]

 * [7]	  requeue_lock_waiter(lock, waiter);	[P] + [L]

 * [8]	  unlock(task->pi_lock);		release [P]

 *	  put_task_struct(task);		release [R]

 * [9]	  check_exit_conditions_3();		[L]

 * [10]	  task = owner(lock);			[L]

 *	  get_task_struct(task);		[L] acquire [R]

 *	  lock(task->pi_lock);			[L] acquire [P]

 * [11]	  requeue_pi_waiter(tsk, waiters(lock));[P] + [L]

 * [12]	  check_exit_conditions_4();		[P] + [L]

 * [13]	  unlock(task->pi_lock);		release [P]

 *	  unlock(lock->wait_lock);		release [L]

 *	  goto again;

	/*

	 * The (de)boosting is a step by step approach with a lot of

	 * pitfalls. We want this to be preemptible and we want hold a

	 * maximum of two locks per step. So we have to check

	 * carefully whether things change under us.

	/*

	 * We limit the lock chain length for each invocation.

		/*

		 * Print this only once. If the admin changes the limit,

		 * print a new message when reaching the limit again.

	/*

	 * We are fully preemptible here and only hold the refcount on

	 * @task. So everything can have changed under us since the

	 * caller or our own code below (goto retry/again) dropped all

	 * locks.

	/*

	 * [1] Task cannot go away as we did a get_task() before !

	/*

	 * [2] Get the waiter on which @task is blocked on.

	/*

	 * [3] check_exit_conditions_1() protected by task->pi_lock.

	/*

	 * Check whether the end of the boosting chain has been

	 * reached or the state of the chain has changed while we

	 * dropped the locks.

	/*

	 * Check the orig_waiter state. After we dropped the locks,

	 * the previous owner of the lock might have released the lock.

	/*

	 * We dropped all locks after taking a refcount on @task, so

	 * the task might have moved on in the lock chain or even left

	 * the chain completely and blocks now on an unrelated lock or

	 * on @orig_lock.

	 *

	 * We stored the lock on which @task was blocked in @next_lock,

	 * so we can detect the chain change.

	/*

	 * There could be 'spurious' loops in the lock graph due to ww_mutex,

	 * consider:

	 *

	 *   P1: A, ww_A, ww_B

	 *   P2: ww_B, ww_A

	 *   P3: A

	 *

	 * P3 should not return -EDEADLK because it gets trapped in the cycle

	 * created by P1 and P2 (which will resolve -- and runs into

	 * max_lock_depth above). Therefore disable detect_deadlock such that

	 * the below termination condition can trigger once all relevant tasks

	 * are boosted.

	 *

	 * Even when we start with ww_mutex we can disable deadlock detection,

	 * since we would supress a ww_mutex induced deadlock at [6] anyway.

	 * Supressing it here however is not sufficient since we might still

	 * hit [6] due to adjustment driven iteration.

	 *

	 * NOTE: if someone were to create a deadlock between 2 ww_classes we'd

	 * utterly fail to report it; lockdep should.

	/*

	 * Drop out, when the task has no waiters. Note,

	 * top_waiter can be NULL, when we are in the deboosting

	 * mode!

		/*

		 * If deadlock detection is off, we stop here if we

		 * are not the top pi waiter of the task. If deadlock

		 * detection is enabled we continue, but stop the

		 * requeueing in the chain walk.

	/*

	 * If the waiter priority is the same as the task priority

	 * then there is no further priority adjustment necessary.  If

	 * deadlock detection is off, we stop the chain walk. If its

	 * enabled we continue, but stop the requeueing in the chain

	 * walk.

	/*

	 * [4] Get the next lock

	/*

	 * [5] We need to trylock here as we are holding task->pi_lock,

	 * which is the reverse lock order versus the other rtmutex

	 * operations.

	/*

	 * [6] check_exit_conditions_2() protected by task->pi_lock and

	 * lock->wait_lock.

	 *

	 * Deadlock detection. If the lock is the same as the original

	 * lock which caused us to walk the lock chain or if the

	 * current lock is owned by the task which initiated the chain

	 * walk, we detected a deadlock.

		/*

		 * When the deadlock is due to ww_mutex; also see above. Don't

		 * report the deadlock and instead let the ww_mutex wound/die

		 * logic pick which of the contending threads gets -EDEADLK.

		 *

		 * NOTE: assumes the cycle only contains a single ww_class; any

		 * other configuration and we fail to report; also, see

		 * lockdep.

	/*

	 * If we just follow the lock chain for deadlock detection, no

	 * need to do all the requeue operations. To avoid a truckload

	 * of conditionals around the various places below, just do the

	 * minimum chain walk checks.

		/*

		 * No requeue[7] here. Just release @task [8]

		/*

		 * [9] check_exit_conditions_3 protected by lock->wait_lock.

		 * If there is no owner of the lock, end of chain.

 [10] Grab the next task, i.e. owner of @lock */

		/*

		 * No requeue [11] here. We just do deadlock detection.

		 *

		 * [12] Store whether owner is blocked

		 * itself. Decision is made after dropping the locks

		/*

		 * Get the top waiter for the next iteration

 [13] Drop locks */

 If owner is not blocked, end of chain. */

	/*

	 * Store the current top waiter before doing the requeue

	 * operation on @lock. We need it for the boost/deboost

	 * decision below.

 [7] Requeue the waiter in the lock waiter tree. */

	/*

	 * Update the waiter prio fields now that we're dequeued.

	 *

	 * These values can have changed through either:

	 *

	 *   sys_sched_set_scheduler() / sys_sched_setattr()

	 *

	 * or

	 *

	 *   DL CBS enforcement advancing the effective deadline.

	 *

	 * Even though pi_waiters also uses these fields, and that tree is only

	 * updated in [11], we can do this here, since we hold [L], which

	 * serializes all pi_waiters access and rb_erase() does not care about

	 * the values of the node being removed.

 [8] Release the task */

	/*

	 * [9] check_exit_conditions_3 protected by lock->wait_lock.

	 *

	 * We must abort the chain walk if there is no lock owner even

	 * in the dead lock detection case, as we have nothing to

	 * follow here. This is the end of the chain we are walking.

		/*

		 * If the requeue [7] above changed the top waiter,

		 * then we need to wake the new top waiter up to try

		 * to get the lock.

 [10] Grab the next task, i.e. the owner of @lock */

 [11] requeue the pi waiters if necessary */

		/*

		 * The waiter became the new top (highest priority)

		 * waiter on the lock. Replace the previous top waiter

		 * in the owner tasks pi waiters tree with this waiter

		 * and adjust the priority of the owner.

		/*

		 * The waiter was the top waiter on the lock, but is

		 * no longer the top priority waiter. Replace waiter in

		 * the owner tasks pi waiters tree with the new top

		 * (highest priority) waiter and adjust the priority

		 * of the owner.

		 * The new top waiter is stored in @waiter so that

		 * @waiter == @top_waiter evaluates to true below and

		 * we continue to deboost the rest of the chain.

		/*

		 * Nothing changed. No need to do any priority

		 * adjustment.

	/*

	 * [12] check_exit_conditions_4() protected by task->pi_lock

	 * and lock->wait_lock. The actual decisions are made after we

	 * dropped the locks.

	 *

	 * Check whether the task which owns the current lock is pi

	 * blocked itself. If yes we store a pointer to the lock for

	 * the lock chain change detection above. After we dropped

	 * task->pi_lock next_lock cannot be dereferenced anymore.

	/*

	 * Store the top waiter of @lock for the end of chain walk

	 * decision below.

 [13] Drop the locks */

	/*

	 * Make the actual exit decisions [12], based on the stored

	 * values.

	 *

	 * We reached the end of the lock chain. Stop right here. No

	 * point to go back just to figure that out.

	/*

	 * If the current waiter is not the top waiter on the lock,

	 * then we can stop the chain walk here if we are not in full

	 * deadlock detection mode.

/*

 * Try to take an rt-mutex

 *

 * Must be called with lock->wait_lock held and interrupts disabled

 *

 * @lock:   The lock to be acquired.

 * @task:   The task which wants to acquire the lock

 * @waiter: The waiter that is queued to the lock's wait tree if the

 *	    callsite called task_blocked_on_lock(), otherwise NULL

	/*

	 * Before testing whether we can acquire @lock, we set the

	 * RT_MUTEX_HAS_WAITERS bit in @lock->owner. This forces all

	 * other tasks which try to modify @lock into the slow path

	 * and they serialize on @lock->wait_lock.

	 *

	 * The RT_MUTEX_HAS_WAITERS bit can have a transitional state

	 * as explained at the top of this file if and only if:

	 *

	 * - There is a lock owner. The caller must fixup the

	 *   transient state if it does a trylock or leaves the lock

	 *   function due to a signal or timeout.

	 *

	 * - @task acquires the lock and there are no other

	 *   waiters. This is undone in rt_mutex_set_owner(@task) at

	 *   the end of this function.

	/*

	 * If @lock has an owner, give up.

	/*

	 * If @waiter != NULL, @task has already enqueued the waiter

	 * into @lock waiter tree. If @waiter == NULL then this is a

	 * trylock attempt.

		/*

		 * If waiter is the highest priority waiter of @lock,

		 * or allowed to steal it, take it over.

			/*

			 * We can acquire the lock. Remove the waiter from the

			 * lock waiters tree.

		/*

		 * If the lock has waiters already we check whether @task is

		 * eligible to take over the lock.

		 *

		 * If there are no other waiters, @task can acquire

		 * the lock.  @task->pi_blocked_on is NULL, so it does

		 * not need to be dequeued.

 Check whether the trylock can steal it. */

			/*

			 * The current top waiter stays enqueued. We

			 * don't have to change anything in the lock

			 * waiters order.

			/*

			 * No waiters. Take the lock without the

			 * pi_lock dance.@task->pi_blocked_on is NULL

			 * and we have no waiters to enqueue in @task

			 * pi waiters tree.

	/*

	 * Clear @task->pi_blocked_on. Requires protection by

	 * @task->pi_lock. Redundant operation for the @waiter == NULL

	 * case, but conditionals are more expensive than a redundant

	 * store.

	/*

	 * Finish the lock acquisition. @task is the new owner. If

	 * other waiters exist we have to insert the highest priority

	 * waiter into @task->pi_waiters tree.

	/*

	 * This either preserves the RT_MUTEX_HAS_WAITERS bit if there

	 * are still waiters or clears it.

/*

 * Task blocks on lock.

 *

 * Prepare waiter and propagate pi chain

 *

 * This must be called with lock->wait_lock held and interrupts disabled

	/*

	 * Early deadlock detection. We really don't want the task to

	 * enqueue on itself just to untangle the mess later. It's not

	 * only an optimization. We drop the locks, so another waiter

	 * can come in before the chain walk detects the deadlock. So

	 * the other will detect the deadlock and return -EDEADLOCK,

	 * which is wrong, as the other waiter is not in a deadlock

	 * situation.

 Get the top priority waiter on the lock */

 Check whether the waiter should back out immediately */

 Store the lock on which owner is blocked or NULL */

	/*

	 * Even if full deadlock detection is on, if the owner is not

	 * blocked itself, we can avoid finding this out in the chain

	 * walk.

	/*

	 * The owner can't disappear while holding a lock,

	 * so the owner struct is protected by wait_lock.

	 * Gets dropped in rt_mutex_adjust_prio_chain()!

/*

 * Remove the top waiter from the current tasks pi waiter tree and

 * queue it up.

 *

 * Called with lock->wait_lock held and interrupts disabled.

	/*

	 * Remove it from current->pi_waiters and deboost.

	 *

	 * We must in fact deboost here in order to ensure we call

	 * rt_mutex_setprio() to update p->pi_top_task before the

	 * task unblocks.

	/*

	 * As we are waking up the top waiter, and the waiter stays

	 * queued on the lock until it gets the lock, this lock

	 * obviously has waiters. Just set the bit here and this has

	 * the added benefit of forcing all new tasks into the

	 * slow path making sure no task of lower priority than

	 * the top waiter can steal this lock.

	/*

	 * We deboosted before waking the top waiter task such that we don't

	 * run two tasks with the 'same' priority (and ensure the

	 * p->pi_top_task pointer points to a blocked task). This however can

	 * lead to priority inversion if we would get preempted after the

	 * deboost but before waking our donor task, hence the preempt_disable()

	 * before unlock.

	 *

	 * Pairs with preempt_enable() in rt_mutex_wake_up_q();

	/*

	 * try_to_take_rt_mutex() sets the lock waiters bit

	 * unconditionally. Clean this up.

/*

 * Slow path try-lock function:

	/*

	 * If the lock already has an owner we fail to get the lock.

	 * This can be done without taking the @lock->wait_lock as

	 * it is only being read, and this is a trylock anyway.

	/*

	 * The mutex has currently no owner. Lock the wait lock and try to

	 * acquire the lock. We use irqsave here to support early boot calls.

/*

 * Slow path to release a rt-mutex.

 irqsave required to support early boot calls */

	/*

	 * We must be careful here if the fast path is enabled. If we

	 * have no waiters queued we cannot set owner to NULL here

	 * because of:

	 *

	 * foo->lock->owner = NULL;

	 *			rtmutex_lock(foo->lock);   <- fast path

	 *			free = atomic_dec_and_test(foo->refcnt);

	 *			rtmutex_unlock(foo->lock); <- fast path

	 *			if (free)

	 *				kfree(foo);

	 * raw_spin_unlock(foo->lock->wait_lock);

	 *

	 * So for the fastpath enabled kernel:

	 *

	 * Nothing can set the waiters bit as long as we hold

	 * lock->wait_lock. So we do the following sequence:

	 *

	 *	owner = rt_mutex_owner(lock);

	 *	clear_rt_mutex_waiters(lock);

	 *	raw_spin_unlock(&lock->wait_lock);

	 *	if (cmpxchg(&lock->owner, owner, 0) == owner)

	 *		return;

	 *	goto retry;

	 *

	 * The fastpath disabled variant is simple as all access to

	 * lock->owner is serialized by lock->wait_lock:

	 *

	 *	lock->owner = NULL;

	 *	raw_spin_unlock(&lock->wait_lock);

 Drops lock->wait_lock ! */

 Relock the rtmutex and try again */

	/*

	 * The wakeup next waiter path does not suffer from the above

	 * race. See the comments there.

	 *

	 * Queue the next waiter for wakeup once we release the wait_lock.

 If owner changed, trylock again. */

		/*

		 * Ensure that @owner is dereferenced after checking that

		 * the lock owner still matches @owner. If that fails,

		 * @owner might point to freed memory. If it still matches,

		 * the rcu_read_lock() ensures the memory stays valid.

		/*

		 * Stop spinning when:

		 *  - the lock owner has been scheduled out

		 *  - current is not longer the top waiter

		 *  - current is requested to reschedule (redundant

		 *    for CONFIG_PREEMPT_RCU=y)

		 *  - the VCPU on which owner runs is preempted

/*

 * Functions required for:

 *	- rtmutex, futex on all kernels

 *	- mutex and rwsem substitutions on RT kernels

/*

 * Remove a waiter from a lock and give up

 *

 * Must be called with lock->wait_lock held and interrupts disabled. It must

 * have just failed to try_to_take_rt_mutex().

	/*

	 * Only update priority if the waiter was the highest priority

	 * waiter of the lock and there is an owner to update.

 Store the lock on which owner is blocked or NULL */

	/*

	 * Don't walk the chain, if the owner task is not blocked

	 * itself.

 gets dropped in rt_mutex_adjust_prio_chain()! */

/**

 * rt_mutex_slowlock_block() - Perform the wait-wake-try-to-take loop

 * @lock:		 the rt_mutex to take

 * @ww_ctx:		 WW mutex context pointer

 * @state:		 the state the task should block in (TASK_INTERRUPTIBLE

 *			 or TASK_UNINTERRUPTIBLE)

 * @timeout:		 the pre-initialized and started timer, or NULL for none

 * @waiter:		 the pre-initialized rt_mutex_waiter

 *

 * Must be called with lock->wait_lock held and interrupts disabled

 Try to acquire the lock: */

	/*

	 * If the result is not -EDEADLOCK or the caller requested

	 * deadlock detection, nothing to do here.

	/*

	 * Yell loudly and stop the task right here.

/**

 * __rt_mutex_slowlock - Locking slowpath invoked with lock::wait_lock held

 * @lock:	The rtmutex to block lock

 * @ww_ctx:	WW mutex context pointer

 * @state:	The task state for sleeping

 * @chwalk:	Indicator whether full or partial chainwalk is requested

 * @waiter:	Initializer waiter for blocking

 Try to acquire the lock again: */

 acquired the lock */

	/*

	 * try_to_take_rt_mutex() sets the waiter bit

	 * unconditionally. We might have to fix that up.

/*

 * rt_mutex_slowlock - Locking slowpath invoked when fast path fails

 * @lock:	The rtmutex to block lock

 * @ww_ctx:	WW mutex context pointer

 * @state:	The task state for sleeping

	/*

	 * Technically we could use raw_spin_[un]lock_irq() here, but this can

	 * be called in early boot if the cmpxchg() fast path is disabled

	 * (debug, no architecture support). In this case we will acquire the

	 * rtmutex with lock->wait_lock held. But we cannot unconditionally

	 * enable interrupts in that early boot case. So we need to use the

	 * irqsave/restore variants.

 RT_MUTEX_BUILD_MUTEX */

/*

 * Functions required for spin/rw_lock substitution on RT kernels

/**

 * rtlock_slowlock_locked - Slow path lock acquisition for RT locks

 * @lock:	The underlying RT mutex

 Save current state and set state to TASK_RTLOCK_WAIT */

 Try to acquire the lock again */

 Restore the task state */

	/*

	 * try_to_take_rt_mutex() sets the waiter bit unconditionally.

	 * We might have to fix that up:

 RT_MUTEX_BUILD_SPINLOCKS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * RT-specific reader/writer semaphores and reader/writer locks

 *

 * down_write/write_lock()

 *  1) Lock rtmutex

 *  2) Remove the reader BIAS to force readers into the slow path

 *  3) Wait until all readers have left the critical section

 *  4) Mark it write locked

 *

 * up_write/write_unlock()

 *  1) Remove the write locked marker

 *  2) Set the reader BIAS, so readers can use the fast path again

 *  3) Unlock rtmutex, to release blocked readers

 *

 * down_read/read_lock()

 *  1) Try fast path acquisition (reader BIAS is set)

 *  2) Take tmutex::wait_lock, which protects the writelocked flag

 *  3) If !writelocked, acquire it for read

 *  4) If writelocked, block on tmutex

 *  5) unlock rtmutex, goto 1)

 *

 * up_read/read_unlock()

 *  1) Try fast path release (reader count != 1)

 *  2) Wake the writer waiting in down_write()/write_lock() #3

 *

 * down_read/read_lock()#3 has the consequence, that rw semaphores and rw

 * locks on RT are not writer fair, but writers, which should be avoided in

 * RT tasks (think mmap_sem), are subject to the rtmutex priority/DL

 * inheritance mechanism.

 *

 * It's possible to make the rw primitives writer fair by keeping a list of

 * active readers. A blocked writer would force all newly incoming readers

 * to block on the rtmutex, but the rtmutex would have to be proxy locked

 * for one reader after the other. We can't use multi-reader inheritance

 * because there is no way to support that with SCHED_DEADLINE.

 * Implementing the one by one reader boosting/handover mechanism is a

 * major surgery for a very dubious value.

 *

 * The risk of writer starvation is there, but the pathological use cases

 * which trigger it are not necessarily the typical RT workloads.

 *

 * Fast-path orderings:

 * The lock/unlock of readers can run in fast paths: lock and unlock are only

 * atomic ops, and there is no inner lock to provide ACQUIRE and RELEASE

 * semantics of rwbase_rt. Atomic ops should thus provide _acquire()

 * and _release() (or stronger).

 *

 * Common code shared between RT rw_semaphore and rwlock

	/*

	 * Increment reader count, if sem->readers < 0, i.e. READER_BIAS is

	 * set.

	/*

	 * Allow readers, as long as the writer has not completely

	 * acquired the semaphore for write.

	/*

	 * Call into the slow lock path with the rtmutex->wait_lock

	 * held, so this can't result in the following race:

	 *

	 * Reader1		Reader2		Writer

	 *			down_read()

	 *					down_write()

	 *					rtmutex_lock(m)

	 *					wait()

	 * down_read()

	 * unlock(m->wait_lock)

	 *			up_read()

	 *			wake(Writer)

	 *					lock(m->wait_lock)

	 *					sem->writelocked=true

	 *					unlock(m->wait_lock)

	 *

	 *					up_write()

	 *					sem->writelocked=false

	 *					rtmutex_unlock(m)

	 *			down_read()

	 *					down_write()

	 *					rtmutex_lock(m)

	 *					wait()

	 * rtmutex_lock(m)

	 *

	 * That would put Reader1 behind the writer waiting on

	 * Reader2 to call up_read(), which might be unbound.

	/*

	 * For rwlocks this returns 0 unconditionally, so the below

	 * !ret conditionals are optimized out.

	/*

	 * On success the rtmutex is held, so there can't be a writer

	 * active. Increment the reader count and immediately drop the

	 * rtmutex again.

	 *

	 * rtmutex->wait_lock has to be unlocked in any case of course.

	/*

	 * Wake the writer, i.e. the rtmutex owner. It might release the

	 * rtmutex concurrently in the fast path (due to a signal), but to

	 * clean up rwb->readers it needs to acquire rtm->wait_lock. The

	 * worst case which can happen is a spurious wakeup.

 Pairs with the preempt_enable in rt_mutex_wake_up_q() */

	/*

	 * rwb->readers can only hit 0 when a writer is waiting for the

	 * active readers to leave the critical section.

	 *

	 * dec_and_test() is fully ordered, provides RELEASE.

	/*

	 * _release() is needed in case that reader is in fast path, pairing

	 * with atomic_try_cmpxchg_acquire() in rwbase_read_trylock().

 Release it and account current as reader */

 Can do without CAS because we're serialized by wait_lock. */

	/*

	 * _acquire is needed in case the reader is in the fast path, pairing

	 * with rwbase_read_unlock(), provides ACQUIRE.

 Take the rtmutex as a first step */

 Force readers into slow path */

 Optimized out for rwlocks */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2008 Intel Corporation

 * Author: Matthew Wilcox <willy@linux.intel.com>

 *

 * This file implements counting semaphores.

 * A counting semaphore may be acquired 'n' times before sleeping.

 * See mutex.c for single-acquisition sleeping locks which enforce

 * rules which allow code to be debugged more easily.

/*

 * Some notes on the implementation:

 *

 * The spinlock controls access to the other members of the semaphore.

 * down_trylock() and up() can be called from interrupt context, so we

 * have to disable interrupts when taking the lock.  It turns out various

 * parts of the kernel expect to be able to use down() on a semaphore in

 * interrupt context when they know it will succeed, so we have to use

 * irqsave variants for down(), down_interruptible() and down_killable()

 * too.

 *

 * The ->count variable represents how many more tasks can acquire this

 * semaphore.  If it's zero, there may be tasks waiting on the wait_list.

/**

 * down - acquire the semaphore

 * @sem: the semaphore to be acquired

 *

 * Acquires the semaphore.  If no more tasks are allowed to acquire the

 * semaphore, calling this function will put the task to sleep until the

 * semaphore is released.

 *

 * Use of this function is deprecated, please use down_interruptible() or

 * down_killable() instead.

/**

 * down_interruptible - acquire the semaphore unless interrupted

 * @sem: the semaphore to be acquired

 *

 * Attempts to acquire the semaphore.  If no more tasks are allowed to

 * acquire the semaphore, calling this function will put the task to sleep.

 * If the sleep is interrupted by a signal, this function will return -EINTR.

 * If the semaphore is successfully acquired, this function returns 0.

/**

 * down_killable - acquire the semaphore unless killed

 * @sem: the semaphore to be acquired

 *

 * Attempts to acquire the semaphore.  If no more tasks are allowed to

 * acquire the semaphore, calling this function will put the task to sleep.

 * If the sleep is interrupted by a fatal signal, this function will return

 * -EINTR.  If the semaphore is successfully acquired, this function returns

 * 0.

/**

 * down_trylock - try to acquire the semaphore, without waiting

 * @sem: the semaphore to be acquired

 *

 * Try to acquire the semaphore atomically.  Returns 0 if the semaphore has

 * been acquired successfully or 1 if it cannot be acquired.

 *

 * NOTE: This return value is inverted from both spin_trylock and

 * mutex_trylock!  Be careful about this when converting code.

 *

 * Unlike mutex_trylock, this function can be used from interrupt context,

 * and the semaphore can be released by any task or interrupt.

/**

 * down_timeout - acquire the semaphore within a specified time

 * @sem: the semaphore to be acquired

 * @timeout: how long to wait before failing

 *

 * Attempts to acquire the semaphore.  If no more tasks are allowed to

 * acquire the semaphore, calling this function will put the task to sleep.

 * If the semaphore is not released within the specified number of jiffies,

 * this function returns -ETIME.  It returns 0 if the semaphore was acquired.

/**

 * up - release the semaphore

 * @sem: the semaphore to release

 *

 * Release the semaphore.  Unlike mutexes, up() may be called from any

 * context and even by tasks which have never called down().

 Functions for the contended case */

/*

 * Because this function is inlined, the 'state' parameter will be

 * constant, and thus optimised away by the compiler.  Likewise the

 * 'timeout' parameter for the cases without timeouts.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * rtmutex API

	/*

	 * Reset the wounded flag after a kill. No other process can

	 * race and wound us here, since they can't have a valid owner

	 * pointer if we don't have any locks held.

		/*

		 * Reset the wounded flag after a kill. No other process can

		 * race and wound us here, since they can't have a valid owner

		 * pointer if we don't have any locks held.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Queued read/write locks

 *

 * (C) Copyright 2013-2014 Hewlett-Packard Development Company, L.P.

 *

 * Authors: Waiman Long <waiman.long@hp.com>

/**

 * queued_read_lock_slowpath - acquire read lock of a queue rwlock

 * @lock: Pointer to queue rwlock structure

	/*

	 * Readers come here when they cannot get the lock without waiting

		/*

		 * Readers in interrupt context will get the lock immediately

		 * if the writer is just waiting (not holding the lock yet),

		 * so spin with ACQUIRE semantics until the lock is available

		 * without waiting in the queue.

	/*

	 * Put the reader into the wait queue

	/*

	 * The ACQUIRE semantics of the following spinning code ensure

	 * that accesses can't leak upwards out of our subsequent critical

	 * section in the case that the lock is currently held for write.

	/*

	 * Signal the next one in queue to become queue head

/**

 * queued_write_lock_slowpath - acquire write lock of a queue rwlock

 * @lock : Pointer to queue rwlock structure

 Put the writer into the wait queue */

 Try to acquire the lock directly if no reader is present */

 Set the waiting flag to notify readers that a writer is pending */

 When no more readers or writers, set the locked flag */

/*

 * Copyright 2005, Red Hat, Inc., Ingo Molnar

 * Released under the General Public License (GPL).

 *

 * This file contains the spinlock/rwlock implementations for

 * DEBUG_SPINLOCK.

	/*

	 * Make sure we are not reinitializing a held lock:

	/*

	 * Make sure we are not reinitializing a held lock:

/*

 * We are now relying on the NMI watchdog to detect lockup instead of doing

 * the detection here with an unfair lock which can cause problem of its own.

	/*

	 * Must not happen on UP:

	/*

	 * Must not happen on UP:

	/*

	 * Must not happen on UP:

 !CONFIG_PREEMPT_RT */

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * XXX: temporary kludge. The error path in alloc_super()

	 * assumes that percpu_free_rwsem() is safe after kzalloc().

 catch use after free bugs */

	/*

	 * Due to having preemption disabled the decrement happens on

	 * the same CPU as the increment, avoiding the

	 * increment-on-one-CPU-and-decrement-on-another problem.

	 *

	 * If the reader misses the writer's assignment of sem->block, then the

	 * writer is guaranteed to see the reader's increment.

	 *

	 * Conversely, any readers that increment their sem->read_count after

	 * the writer looks are guaranteed to see the sem->block value, which

	 * in turn means that they are guaranteed to immediately decrement

	 * their sem->read_count, so that it doesn't matter that the writer

	 * missed them.

 A matches D */

	/*

	 * If !sem->block the critical section starts here, matched by the

	 * release in percpu_up_write().

 Prod writer to re-evaluate readers_active_check() */

/*

 * The return value of wait_queue_entry::func means:

 *

 *  <0 - error, wakeup is terminated and the error is returned

 *   0 - no wakeup, a next waiter is tried

 *  >0 - woken, if EXCLUSIVE, counted towards @nr_exclusive.

 *

 * We use EXCLUSIVE for both readers and writers to preserve FIFO order,

 * and play games with the return value to allow waking multiple readers.

 *

 * Specifically, we wake readers until we've woken a single writer, or until a

 * trylock fails.

 concurrent against percpu_down_write(), can get stolen */

 wake (readers until) 1 writer */

	/*

	 * Serialize against the wakeup in percpu_up_write(), if we fail

	 * the trylock, the wakeup must see us on the list.

 .reader = */ true);

/*

 * Return true if the modular sum of the sem->read_count per-CPU variable is

 * zero.  If this sum is zero, then it is stable due to the fact that if any

 * newly arriving readers increment a given counter, they will immediately

 * decrement that same counter.

 *

 * Assumes sem->block is set.

	/*

	 * If we observed the decrement; ensure we see the entire critical

	 * section.

 C matches B */

 Notify readers to take the slow path. */

	/*

	 * Try set sem->block; this provides writer-writer exclusion.

	 * Having sem->block set makes new readers block.

 .reader = */ false);

 smp_mb() implied by __percpu_down_write_trylock() on success -- D matches A */

	/*

	 * If they don't see our store of sem->block, then we are guaranteed to

	 * see their sem->read_count increment, and therefore will wait for

	 * them.

 Wait for all active readers to complete. */

	/*

	 * Signal the writer is done, no fast path yet.

	 *

	 * One reason that we cannot just immediately flip to readers_fast is

	 * that new readers might fail to see the results of this writer's

	 * critical section.

	 *

	 * Therefore we force it through the slow path which guarantees an

	 * acquire and thereby guarantees the critical section's consistency.

	/*

	 * Prod any pending reader/writer to make progress.

	/*

	 * Once this completes (at least one RCU-sched grace period hence) the

	 * reader fast path will be available again. Safe to use outside the

	 * exclusive write lock because its counting.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PREEMPT_RT substitution for spin/rw_locks

 *

 * spinlocks and rwlocks on RT are based on rtmutexes, with a few twists to

 * resemble the non RT semantics:

 *

 * - Contrary to plain rtmutexes, spinlocks and rwlocks are state

 *   preserving. The task state is saved before blocking on the underlying

 *   rtmutex, and restored when the lock has been acquired. Regular wakeups

 *   during that time are redirected to the saved state so no wake up is

 *   missed.

 *

 * - Non RT spin/rwlocks disable preemption and eventually interrupts.

 *   Disabling preemption has the side effect of disabling migration and

 *   preventing RCU grace periods.

 *

 *   The RT substitutions explicitly disable migration and take

 *   rcu_read_lock() across the lock held section.

/*

 * __might_resched() skips the state check as rtlocks are state

 * preserving. Take RCU nesting into account as spin/read/write_lock() can

 * legitimately nest into an RCU read side critical section.

/*

 * Wait for the lock to get unlocked: instead of polling for an unlock

 * (like raw spinlocks do), lock and unlock, to force the kernel to

 * schedule if there's contention:

/*

 * RT-specific reader/writer locks

/*

 * The common functions which get wrapped into the rwlock API.

/*

 * Debugging code for mutexes

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 * lock debugging, locking tree, deadlock detection started by:

 *

 *  Copyright (C) 2004, LynuxWorks, Inc., Igor Manyilov, Bill Huey

 *  Released under the General Public License (GPL).

/*

 * Must be called with lock->wait_lock held.

 Mark the current thread as blocked on the lock: */

	/*

	 * Make sure we are not reinitializing a held lock:

/***

 * mutex_destroy - mark a mutex unusable

 * @lock: the mutex to be destroyed

 *

 * This function marks the mutex uninitialized, and any subsequent

 * use of the mutex is forbidden. The mutex must not be locked when

 * this function is called.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Module-based API test facility for ww_mutexes

 restarts iteration */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * rtmutex API

/*

 * Max number of times we'll walk the boosting chain:

/*

 * Debug aware fast / slowpath lock,trylock,unlock

 *

 * The atomic acquire/release ops are compiled away, when either the

 * architecture does not support cmpxchg or when debugging is enabled.

/**

 * rt_mutex_lock_nested - lock a rt_mutex

 *

 * @lock: the rt_mutex to be locked

 * @subclass: the lockdep subclass

 !CONFIG_DEBUG_LOCK_ALLOC */

/**

 * rt_mutex_lock - lock a rt_mutex

 *

 * @lock: the rt_mutex to be locked

/**

 * rt_mutex_lock_interruptible - lock a rt_mutex interruptible

 *

 * @lock:		the rt_mutex to be locked

 *

 * Returns:

 *  0		on success

 * -EINTR	when interrupted by a signal

/**

 * rt_mutex_trylock - try to lock a rt_mutex

 *

 * @lock:	the rt_mutex to be locked

 *

 * This function can only be called in thread context. It's safe to call it

 * from atomic regions, but not from hard or soft interrupt context.

 *

 * Returns:

 *  1 on success

 *  0 on contention

/**

 * rt_mutex_unlock - unlock a rt_mutex

 *

 * @lock: the rt_mutex to be unlocked

/*

 * Futex variants, must not use fastpath.

/**

 * __rt_mutex_futex_unlock - Futex variant, that since futex variants

 * do not use the fast-path, can be simple and will not need to retry.

 *

 * @lock:	The rt_mutex to be unlocked

 * @wqh:	The wake queue head from which to get the next lock waiter

 done */

	/*

	 * We've already deboosted, mark_wakeup_next_waiter() will

	 * retain preempt_disabled when we drop the wait_lock, to

	 * avoid inversion prior to the wakeup.  preempt_disable()

	 * therein pairs with rt_mutex_postunlock().

 call postunlock() */

/**

 * __rt_mutex_init - initialize the rt_mutex

 *

 * @lock:	The rt_mutex to be initialized

 * @name:	The lock name used for debugging

 * @key:	The lock class key used for debugging

 *

 * Initialize the rt_mutex to unlocked state.

 *

 * Initializing of a locked rt_mutex is not allowed

/**

 * rt_mutex_init_proxy_locked - initialize and lock a rt_mutex on behalf of a

 *				proxy owner

 *

 * @lock:	the rt_mutex to be locked

 * @proxy_owner:the task to set as owner

 *

 * No locking. Caller has to do serializing itself

 *

 * Special API call for PI-futex support. This initializes the rtmutex and

 * assigns it to @proxy_owner. Concurrent operations on the rtmutex are not

 * possible at this point because the pi_state which contains the rtmutex

 * is not yet visible to other tasks.

	/*

	 * On PREEMPT_RT the futex hashbucket spinlock becomes 'sleeping'

	 * and rtmutex based. That causes a lockdep false positive, because

	 * some of the futex functions invoke spin_unlock(&hb->lock) with

	 * the wait_lock of the rtmutex associated to the pi_futex held.

	 * spin_unlock() in turn takes wait_lock of the rtmutex on which

	 * the spinlock is based, which makes lockdep notice a lock

	 * recursion. Give the futex/rtmutex wait_lock a separate key.

/**

 * rt_mutex_proxy_unlock - release a lock on behalf of owner

 *

 * @lock:	the rt_mutex to be locked

 *

 * No locking. Caller has to do serializing itself

 *

 * Special API call for PI-futex support. This just cleans up the rtmutex

 * (debugging) state. Concurrent operations on this rt_mutex are not

 * possible because it belongs to the pi_state which is about to be freed

 * and it is not longer visible to other tasks.

/**

 * __rt_mutex_start_proxy_lock() - Start lock acquisition for another task

 * @lock:		the rt_mutex to take

 * @waiter:		the pre-initialized rt_mutex_waiter

 * @task:		the task to prepare

 *

 * Starts the rt_mutex acquire; it enqueues the @waiter and does deadlock

 * detection. It does not wait, see rt_mutex_wait_proxy_lock() for that.

 *

 * NOTE: does _NOT_ remove the @waiter on failure; must either call

 * rt_mutex_wait_proxy_lock() or rt_mutex_cleanup_proxy_lock() after this.

 *

 * Returns:

 *  0 - task blocked on lock

 *  1 - acquired the lock for task, caller should wake it up

 * <0 - error

 *

 * Special API call for PI-futex support.

 We enforce deadlock detection for futexes */

		/*

		 * Reset the return value. We might have

		 * returned with -EDEADLK and the owner

		 * released the lock while we were walking the

		 * pi chain.  Let the waiter sort it out.

/**

 * rt_mutex_start_proxy_lock() - Start lock acquisition for another task

 * @lock:		the rt_mutex to take

 * @waiter:		the pre-initialized rt_mutex_waiter

 * @task:		the task to prepare

 *

 * Starts the rt_mutex acquire; it enqueues the @waiter and does deadlock

 * detection. It does not wait, see rt_mutex_wait_proxy_lock() for that.

 *

 * NOTE: unlike __rt_mutex_start_proxy_lock this _DOES_ remove the @waiter

 * on failure.

 *

 * Returns:

 *  0 - task blocked on lock

 *  1 - acquired the lock for task, caller should wake it up

 * <0 - error

 *

 * Special API call for PI-futex support.

/**

 * rt_mutex_wait_proxy_lock() - Wait for lock acquisition

 * @lock:		the rt_mutex we were woken on

 * @to:			the timeout, null if none. hrtimer should already have

 *			been started.

 * @waiter:		the pre-initialized rt_mutex_waiter

 *

 * Wait for the lock acquisition started on our behalf by

 * rt_mutex_start_proxy_lock(). Upon failure, the caller must call

 * rt_mutex_cleanup_proxy_lock().

 *

 * Returns:

 *  0 - success

 * <0 - error, one of -EINTR, -ETIMEDOUT

 *

 * Special API call for PI-futex support

 sleep on the mutex */

	/*

	 * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might

	 * have to fix that up.

/**

 * rt_mutex_cleanup_proxy_lock() - Cleanup failed lock acquisition

 * @lock:		the rt_mutex we were woken on

 * @waiter:		the pre-initialized rt_mutex_waiter

 *

 * Attempt to clean up after a failed __rt_mutex_start_proxy_lock() or

 * rt_mutex_wait_proxy_lock().

 *

 * Unless we acquired the lock; we're still enqueued on the wait-list and can

 * in fact still be granted ownership until we're removed. Therefore we can

 * find we are in fact the owner and must disregard the

 * rt_mutex_wait_proxy_lock() failure.

 *

 * Returns:

 *  true  - did the cleanup, we done.

 *  false - we acquired the lock after rt_mutex_wait_proxy_lock() returned,

 *          caller should disregards its return value.

 *

 * Special API call for PI-futex support

	/*

	 * Do an unconditional try-lock, this deals with the lock stealing

	 * state where __rt_mutex_futex_unlock() -> mark_wakeup_next_waiter()

	 * sets a NULL owner.

	 *

	 * We're not interested in the return value, because the subsequent

	 * test on rt_mutex_owner() will infer that. If the trylock succeeded,

	 * we will own the lock and it will have removed the waiter. If we

	 * failed the trylock, we're still not owner and we need to remove

	 * ourselves.

	/*

	 * Unless we're the owner; we're still enqueued on the wait_list.

	 * So check if we became owner, if not, take us off the wait_list.

	/*

	 * try_to_take_rt_mutex() sets the waiter bit unconditionally. We might

	 * have to fix that up.

/*

 * Recheck the pi chain, in case we got a priority setting

 *

 * Called from sched_setscheduler

 gets dropped in rt_mutex_adjust_prio_chain()! */

/*

 * Performs the wakeup of the top-waiter and re-enables preemption.

 Mutexes */

 CONFIG_DEBUG_LOCK_ALLOC */

 !CONFIG_DEBUG_LOCK_ALLOC */

 CONFIG_PREEMPT_RT */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kernel/lockdep.c

 *

 * Runtime locking correctness validator

 *

 * Started by Ingo Molnar:

 *

 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra

 *

 * this code maps all the lock dependencies as they occur in a live kernel

 * and will warn about the following classes of locking bugs:

 *

 * - lock inversion scenarios

 * - circular lock dependencies

 * - hardirq/softirq safe/unsafe locking bugs

 *

 * Bugs are reported even if the current locking scenario does not cause

 * any deadlock at this point.

 *

 * I.e. if anytime in the past two locks were taken in a different order,

 * even if it happened for another task, even if those were different

 * locks (but of the same class as this lock), this code will detect it.

 *

 * Thanks to Arjan van de Ven for coming up with the initial idea of

 * mapping lock dependencies runtime.

/*

 * lockdep_lock: protects the lockdep graph, the hashes and the

 *               class/list/hash allocators.

 *

 * This is one of the rare exceptions where it's justified

 * to use a raw spinlock - we really dont want the spinlock

 * code to recurse back into the lockdep code...

	/*

	 * Make sure that if another CPU detected a bug while

	 * walking the graph we dont change it (while the other

	 * CPU is busy printing out stuff with the graph lock

	 * dropped already)

/*

 * Turn lock debugging off and return with 0 if it was off already,

 * and also release the graph lock:

/*

 * All data structures here are protected by the global debug_lock.

 *

 * nr_lock_classes is the number of elements of lock_classes[] that is

 * in use.

 Don't re-read hlock->class_idx, can't use READ_ONCE() on bitfield */

		/*

		 * Someone passed in garbage, we give up.

	/*

	 * At this point, if the passed hlock->class_idx is still garbage,

	 * we just have to live with it

/*

 * We keep a global list of all lock classes. The list is only accessed with

 * the lockdep spinlock lock held. free_lock_classes is a list with free

 * elements. These elements are linked together by the lock_entry member in

 * struct lock_class.

/**

 * struct pending_free - information about data structures about to be freed

 * @zapped: Head of a list with struct lock_class elements.

 * @lock_chains_being_freed: Bitmap that indicates which lock_chains[] elements

 *	are about to be freed.

/**

 * struct delayed_free - data structures used for delayed freeing

 *

 * A data structure for delayed freeing of data structures that may be

 * accessed by RCU readers at the time these were freed.

 *

 * @rcu_head:  Used to schedule an RCU callback for freeing data structures.

 * @index:     Index of @pf to which freed data structures are added.

 * @scheduled: Whether or not an RCU callback has been scheduled.

 * @pf:        Array with information about data structures about to be freed.

/*

 * The lockdep classes are in a hash-table as well, for fast lookup:

/*

 * We put the lock dependency chains into a hash-table as well, to cache

 * their existence:

/*

 * the id of held_lock

/*

 * The hash key of the lock dependency chains is a hash itself too:

 * it's a hash of all locks taken up to that lock, including that lock.

 * It's a 64-bit hash, because it's important for the keys to be

 * unique.

 Macro that modifies arguments! */

 no locks held yet */

/*

 * Debugging switches:

/*

 * Quick filtering for interesting events:

 Example */

 Filter everything else. 1 would be to allow everything else */

/**

 * struct lock_trace - single stack backtrace

 * @hash_entry:	Entry in a stack_trace_hash[] list.

 * @hash:	jhash() of @entries.

 * @nr_entries:	Number of entries in @entries.

 * @entries:	Actual stack backtrace.

/*

 * Stack-trace: sequence of lock_trace structures. Protected by the graph_lock.

 Return the number of stack traces in the stack_trace[] array. */

 Return the number of stack hash chains that have at least one stack trace. */

/*

 * Various lockdep statistics:

/*

 * Locking printouts:

 abused as string storage for verify_lock_unused() */

	/*

	 * The usage character defaults to '.' (i.e., irqs disabled and not in

	 * irq context), which is the safest usage category.

	/*

	 * The order of the following usage checks matters, which will

	 * result in the outcome character as follows:

	 *

	 * - '+': irq is enabled and not in irq context

	 * - '-': in irq context and irq is disabled

	 * - '?': in irq context and irq is enabled

	/*

	 * We can be called locklessly through debug_show_all_locks() so be

	 * extra careful, the hlock might have been released and cleared.

	 *

	 * If this indeed happens, lets pretend it does not hurt to continue

	 * to print the lock unless the hlock class_idx does not point to a

	 * registered class. The rationale here is: since we don't attempt

	 * to distinguish whether we are in this situation, if it just

	 * happened we can't count on class_idx to tell either.

	/*

	 * It's not reliable to print a task's held locks if it's not sleeping

	 * and it's not the current task.

/*

 * Is this the address of a static object:

/*

 * Check if an address is part of freed initmem. After initmem is freed,

 * memory can be allocated from it, and such allocations would then have

 * addresses within the range [_stext, _end].

	/*

	 * static variable?

	/*

	 * in-kernel percpu var?

	/*

	 * module static or percpu var?

/*

 * To make lock name printouts unique, we calculate a unique

 * class->name_version generation counter. The caller must hold the graph

 * lock.

 used from NMI context -- must be lockless */

	/*

	 * If it is not initialised then it has never been locked,

	 * so it won't be present in the hash table.

	/*

	 * NOTE: the class-key must be unique. For dynamic locks, a static

	 * lock_class_key variable is passed in through the mutex_init()

	 * (or spin_lock_init()) call - which acts as the key. For static

	 * locks we use the lock object itself as the key.

	/*

	 * We do an RCU walk of the hash, see lockdep_free_key_range().

			/*

			 * Huh! same key, different name? Did someone trample

			 * on some memory? We're most confused.

/*

 * Static locks do not have their class-keys yet - for them the key is

 * the lock object itself. If the lock is in the per cpu area, the

 * canonical address of the lock (per cpu offset removed) is used.

	/*

	 * lockdep_free_key_range() assumes that struct lock_class_key

	 * objects do not overlap. Since we use the address of lock

	 * objects as class key for static objects, check whether the

	 * size of lock_class_key objects does not exceed the size of

	 * the smallest lock object.

 Debug-check: all keys must be persistent! */

 Check whether element @e occurs in list @h */

/*

 * Check whether entry @e occurs in any of the locks_after or locks_before

 * lists.

	/*

	 * The 'unsigned long long' casts avoid that a compiler warning

	 * is reported when building tools/lib/lockdep.

 Check whether all classes occur in a lock list. */

 Check whether all classes have valid lock lists. */

 Check the chain_key of all lock chains. */

	/*

	 * Check whether all list entries that are in use occur in a class

	 * lock list.

	/*

	 * Check whether all list entries that are not in use do not occur in

	 * a class lock list.

 CONFIG_DEBUG_LOCKDEP */

 CONFIG_DEBUG_LOCKDEP */

/*

 * Initialize the lock_classes[] array elements, the free_lock_classes list

 * and also the delayed_free structure.

 Register a dynamically allocated key. */

 Check whether a key has been registered as a dynamic key. */

	/*

	 * If lock debugging is disabled lock_keys_hash[] may contain

	 * pointers to memory that has already been freed. Avoid triggering

	 * a use-after-free in that case by returning early.

/*

 * Register a lock's class in the hash-table, if the class is not present

 * yet. Otherwise we look it up. We cache the result in the lock object

 * itself, so actual lookup of the hash should be once per lock object.

	/*

	 * We have to do the hash-walk again, to avoid races

	 * with another CPU:

 Allocate a new lock class and add it to the hash. */

	/*

	 * We use RCU's safe list-add method to make

	 * parallel walking of the hash-list safe:

	/*

	 * Remove the class from the free list and add it to the global list

	 * of classes.

	/*

	 * Hash collision, did we smoke some? We found a class with a matching

	 * hash but the subclass -- which is hashed in -- didn't match.

/*

 * Allocate a lockdep entry. (assumes the graph_lock held, returns

 * with NULL on failure)

/*

 * Add a new dependency to the head of the list:

	/*

	 * Lock not present yet - get a new dependency struct and

	 * add it to the list:

	/*

	 * Both allocation and removal are done under the graph lock; but

	 * iteration is under RCU-sched; see look_up_lock_class() and

	 * lockdep_free_key_range().

/*

 * For good efficiency of modular, we use power of 2

/*

 * The circular_queue and helpers are used to implement graph

 * breadth-first search (BFS) algorithm, by which we can determine

 * whether there is a path from a lock to another. In deadlock checks,

 * a path from the next lock to be acquired to a previous held lock

 * indicates that adding the <prev> -> <next> lock dependency will

 * produce a circle in the graph. Breadth-first search instead of

 * depth-first search is used in order to find the shortest (circular)

 * path.

/*

 * Dequeue an element from the circular_queue, return a lock_list if

 * the queue is not empty, or NULL if otherwise.

/*

 * Return the forward or backward dependency list.

 *

 * @lock:   the lock_list to get its class's dependency list

 * @offset: the offset to struct lock_class to determine whether it is

 *          locks_after or locks_before

/*

 * Return values of a bfs search:

 *

 * BFS_E* indicates an error

 * BFS_R* indicates a result (match or not)

 *

 * BFS_EINVALIDNODE: Find a invalid node in the graph.

 *

 * BFS_EQUEUEFULL: The queue is full while doing the bfs.

 *

 * BFS_RMATCH: Find the matched node in the graph, and put that node into

 *             *@target_entry.

 *

 * BFS_RNOMATCH: Haven't found the matched node and keep *@target_entry

 *               _unchanged_.

/*

 * bfs_result < 0 means error

/*

 * DEP_*_BIT in lock_list::dep

 *

 * For dependency @prev -> @next:

 *

 *   SR: @prev is shared reader (->read != 0) and @next is recursive reader

 *       (->read == 2)

 *   ER: @prev is exclusive locker (->read == 0) and @next is recursive reader

 *   SN: @prev is shared reader and @next is non-recursive locker (->read != 2)

 *   EN: @prev is exclusive locker and @next is non-recursive locker

 *

 * Note that we define the value of DEP_*_BITs so that:

 *   bit0 is prev->read == 0

 *   bit1 is next->read != 2

 0 */

 1 */

 2 */

 3 */

/*

 * calculate the dep_bit for backwards edges. We care about whether @prev is

 * shared and whether @next is recursive.

/*

 * Initialize a lock_list entry @lock belonging to @class as the root for a BFS

 * search.

/*

 * Initialize a lock_list entry @lock based on a lock acquisition @hlock as the

 * root for a BFS search.

 *

 * ->only_xr of the initial lock node is set to @hlock->read == 2, to make sure

 * that <prev> -> @hlock and @hlock -> <whatever __bfs() found> is not -(*R)->

 * and -(S*)->.

/*

 * Similar to bfs_init_root() but initialize the root for backwards BFS.

 *

 * ->only_xr of the initial lock node is set to @hlock->read != 0, to make sure

 * that <next> -> @hlock and @hlock -> <whatever backwards BFS found> is not

 * -(*S)-> and -(R*)-> (reverse order of -(*R)-> and -(S*)->).

/*

 * Breadth-First Search to find a strong path in the dependency graph.

 *

 * @source_entry: the source of the path we are searching for.

 * @data: data used for the second parameter of @match function

 * @match: match function for the search

 * @target_entry: pointer to the target of a matched path

 * @offset: the offset to struct lock_class to determine whether it is

 *          locks_after or locks_before

 *

 * We may have multiple edges (considering different kinds of dependencies,

 * e.g. ER and SN) between two nodes in the dependency graph. But

 * only the strong dependency path in the graph is relevant to deadlocks. A

 * strong dependency path is a dependency path that doesn't have two adjacent

 * dependencies as -(*R)-> -(S*)->, please see:

 *

 *         Documentation/locking/lockdep-design.rst

 *

 * for more explanation of the definition of strong dependency paths

 *

 * In __bfs(), we only traverse in the strong dependency path:

 *

 *     In lock_list::only_xr, we record whether the previous dependency only

 *     has -(*R)-> in the search, and if it does (prev only has -(*R)->), we

 *     filter out any -(S*)-> in the current dependency and after that, the

 *     ->only_xr is set according to whether we only have -(*R)-> left.

		/*

		 * Step 1: check whether we already finish on this one.

		 *

		 * If we have visited all the dependencies from this @lock to

		 * others (iow, if we have visited all lock_list entries in

		 * @lock->class->locks_{after,before}) we skip, otherwise go

		 * and visit all the dependencies in the list and mark this

		 * list accessed.

		/*

		 * Step 2: check whether prev dependency and this form a strong

		 *         dependency path.

 Parent exists, check prev dependency */

			/*

			 * Mask out all -(S*)-> if we only have *R in previous

			 * step, because -(*R)-> -(S*)-> don't make up a strong

			 * dependency.

 If nothing left, we skip */

 If there are only -(*R)-> left, set that for the next step */

		/*

		 * Step 3: we haven't visited this and there is a strong

		 *         dependency path to this, so check with @match.

		 *         If @skip is provide and returns true, we skip this

		 *         lock (and any path this lock is in).

		/*

		 * Step 4: if not match, expand the path by adding the

		 *         forward or backwards dependencies in the search

		 *

			/*

			 * Note we only enqueue the first of the list into the

			 * queue, because we can always find a sibling

			 * dependency from one (see __bfs_next()), as a result

			 * the space of queue is saved.

/*

 * Print a dependency chain entry (this is only done when a deadlock

 * has been detected):

	/*

	 * A direct locking problem where unsafe_class lock is taken

	 * directly by safe_class lock, then all we need to show

	 * is the deadlock scenario, as it is obvious that the

	 * unsafe lock is taken under the safe lock.

	 *

	 * But if there is a chain instead, where the safe lock takes

	 * an intermediate lock (middle_class) where this lock is

	 * not the same as the safe lock, then the lock chain is

	 * used to describe the problem. Otherwise we would need

	 * to show a different CPU case for each link in the chain

	 * from the safe_class lock to the unsafe_class lock.

/*

 * When a circular dependency is detected, print the

 * header first:

/*

 * We are about to add A -> B into the dependency graph, and in __bfs() a

 * strong dependency path A -> .. -> B is found: hlock_class equals

 * entry->class.

 *

 * If A -> .. -> B can replace A -> B in any __bfs() search (means the former

 * is _stronger_ than or equal to the latter), we consider A -> B as redundant.

 * For example if A -> .. -> B is -(EN)-> (i.e. A -(E*)-> .. -(*N)-> B), and A

 * -> B is -(ER)-> or -(EN)->, then we don't need to add A -> B into the

 * dependency graph, as any strong path ..-> A -> B ->.. we can get with

 * having dependency A -> B, we could already get a equivalent path ..-> A ->

 * .. -> B -> .. with A -> .. -> B. Therefore A -> B is redundant.

 *

 * We need to make sure both the start and the end of A -> .. -> B is not

 * weaker than A -> B. For the start part, please see the comment in

 * check_redundant(). For the end part, we need:

 *

 * Either

 *

 *     a) A -> B is -(*R)-> (everything is not weaker than that)

 *

 * or

 *

 *     b) A -> .. -> B is -(*N)-> (nothing is stronger than this)

 *

 Found A -> .. -> B */

 A -> B is -(*R)-> */

 A -> .. -> B is -(*N)-> */

/*

 * We are about to add B -> A into the dependency graph, and in __bfs() a

 * strong dependency path A -> .. -> B is found: hlock_class equals

 * entry->class.

 *

 * We will have a deadlock case (conflict) if A -> .. -> B -> A is a strong

 * dependency cycle, that means:

 *

 * Either

 *

 *     a) B -> A is -(E*)->

 *

 * or

 *

 *     b) A -> .. -> B is -(*N)-> (i.e. A -> .. -(*N)-> B)

 *

 * as then we don't have -(*R)-> -(S*)-> in the cycle.

 Found A -> .. -> B */

 B -> A is -(E*)-> */

 A -> .. -> B is -(*N)-> */

	/*

	 * Breadth-first-search failed, graph got corrupted?

/*

 * Check that the dependency graph starting at <src> can lead to

 * <target> or not.

/*

 * Prove that the dependency graph starting at <src> can not

 * lead to <target>. If it can, there is a circle when adding

 * <target> -> <src> dependency.

 *

 * Print an error and return BFS_RMATCH if it does.

			/*

			 * If save_trace fails here, the printing might

			 * trigger a WARN but because of the !nr_entries it

			 * should not do bad things.

/*

 * Forwards and backwards subgraph searching, for the purposes of

 * proving that two subgraphs can be connected by a new dependency

 * without creating any illegal irq-safe -> irq-unsafe lock dependency.

 *

 * A irq safe->unsafe deadlock happens with the following conditions:

 *

 * 1) We have a strong dependency path A -> ... -> B

 *

 * 2) and we have ENABLED_IRQ usage of B and USED_IN_IRQ usage of A, therefore

 *    irq can create a new dependency B -> A (consider the case that a holder

 *    of B gets interrupted by an irq whose handler will try to acquire A).

 *

 * 3) the dependency circle A -> ... -> B -> A we get from 1) and 2) is a

 *    strong circle:

 *

 *      For the usage bits of B:

 *        a) if A -> B is -(*N)->, then B -> A could be any type, so any

 *           ENABLED_IRQ usage suffices.

 *        b) if A -> B is -(*R)->, then B -> A must be -(E*)->, so only

 *           ENABLED_IRQ_*_READ usage suffices.

 *

 *      For the usage bits of A:

 *        c) if A -> B is -(E*)->, then B -> A could be any type, so any

 *           USED_IN_IRQ usage suffices.

 *        d) if A -> B is -(S*)->, then B -> A must be -(*N)->, so only

 *           USED_IN_IRQ_*_READ usage suffices.

/*

 * There is a strong dependency path in the dependency graph: A -> B, and now

 * we need to decide which usage bit of A should be accumulated to detect

 * safe->unsafe bugs.

 *

 * Note that usage_accumulate() is used in backwards search, so ->only_xr

 * stands for whether A -> B only has -(S*)-> (in this case ->only_xr is true).

 *

 * As above, if only_xr is false, which means A -> B has -(E*)-> dependency

 * path, any usage of A should be considered. Otherwise, we should only

 * consider _READ usage.

 Mask out _READ usage bits */

/*

 * There is a strong dependency path in the dependency graph: A -> B, and now

 * we need to decide which usage bit of B conflicts with the usage bits of A,

 * i.e. which usage bit of B may introduce safe->unsafe deadlocks.

 *

 * As above, if only_xr is false, which means A -> B has -(*N)-> dependency

 * path, any usage of B should be considered. Otherwise, we should only

 * consider _READ usage.

 Mask out _READ usage bits */

	/*

	 * Skip local_lock() for irq inversion detection.

	 *

	 * For !RT, local_lock() is not a real lock, so it won't carry any

	 * dependency.

	 *

	 * For RT, an irq inversion happens when we have lock A and B, and on

	 * some CPU we can have:

	 *

	 *	lock(A);

	 *	<interrupted>

	 *	  lock(B);

	 *

	 * where lock(B) cannot sleep, and we have a dependency B -> ... -> A.

	 *

	 * Now we prove local_lock() cannot exist in that dependency. First we

	 * have the observation for any lock chain L1 -> ... -> Ln, for any

	 * 1 <= i <= n, Li.inner_wait_type <= L1.inner_wait_type, otherwise

	 * wait context check will complain. And since B is not a sleep lock,

	 * therefore B.inner_wait_type >= 2, and since the inner_wait_type of

	 * local_lock() is 3, which is greater than 2, therefore there is no

	 * way the local_lock() exists in the dependency B -> ... -> A.

	 *

	 * As a result, we will skip local_lock(), when we search for irq

	 * inversion bugs.

/*

 * Find a node in the forwards-direction dependency sub-graph starting

 * at @root->class that matches @bit.

 *

 * Return BFS_MATCH if such a node exists in the subgraph, and put that node

 * into *@target_entry.

/*

 * Find a node in the backwards-direction dependency sub-graph starting

 * at @root->class that matches @bit.

/*

 * Dependency path printing:

 *

 * After BFS we get a lock dependency path (linked via ->parent of lock_list),

 * printing out each lock in the dependency path will help on understanding how

 * the deadlock could happen. Here are some details about dependency path

 * printing:

 *

 * 1)	A lock_list can be either forwards or backwards for a lock dependency,

 * 	for a lock dependency A -> B, there are two lock_lists:

 *

 * 	a)	lock_list in the ->locks_after list of A, whose ->class is B and

 * 		->links_to is A. In this case, we can say the lock_list is

 * 		"A -> B" (forwards case).

 *

 * 	b)	lock_list in the ->locks_before list of B, whose ->class is A

 * 		and ->links_to is B. In this case, we can say the lock_list is

 * 		"B <- A" (bacwards case).

 *

 * 	The ->trace of both a) and b) point to the call trace where B was

 * 	acquired with A held.

 *

 * 2)	A "helper" lock_list is introduced during BFS, this lock_list doesn't

 * 	represent a certain lock dependency, it only provides an initial entry

 * 	for BFS. For example, BFS may introduce a "helper" lock_list whose

 * 	->class is A, as a result BFS will search all dependencies starting with

 * 	A, e.g. A -> B or A -> C.

 *

 * 	The notation of a forwards helper lock_list is like "-> A", which means

 * 	we should search the forwards dependencies starting with "A", e.g A -> B

 * 	or A -> C.

 *

 * 	The notation of a bacwards helper lock_list is like "<- B", which means

 * 	we should search the backwards dependencies ending with "B", e.g.

 * 	B <- A or B <- C.

/*

 * printk the shortest lock dependencies from @root to @leaf in reverse order.

 *

 * We have a lock dependency path as follow:

 *

 *    @root                                                                 @leaf

 *      |                                                                     |

 *      V                                                                     V

 *	          ->parent                                   ->parent

 * | lock_list | <--------- | lock_list | ... | lock_list  | <--------- | lock_list |

 * |    -> L1  |            | L1 -> L2  | ... |Ln-2 -> Ln-1|            | Ln-1 -> Ln|

 *

 * , so it's natural that we start from @leaf and print every ->class and

 * ->trace until we reach the @root.

compute depth from generated tree by BFS*/

/*

 * printk the shortest lock dependencies from @leaf to @root.

 *

 * We have a lock dependency path (from a backwards search) as follow:

 *

 *    @leaf                                                                 @root

 *      |                                                                     |

 *      V                                                                     V

 *	          ->parent                                   ->parent

 * | lock_list | ---------> | lock_list | ... | lock_list  | ---------> | lock_list |

 * | L2 <- L1  |            | L3 <- L2  | ... | Ln <- Ln-1 |            |    <- Ln  |

 *

 * , so when we iterate from @leaf to @root, we actually print the lock

 * dependency path L1 -> L2 -> .. -> Ln in the non-reverse order.

 *

 * Another thing to notice here is that ->class of L2 <- L1 is L1, while the

 * ->trace of L2 <- L1 is the call trace of L2, in fact we don't have the call

 * trace of L1 in the dependency path, which is alright, because most of the

 * time we can figure out where L1 is held from the call trace of L2.

compute depth from generated tree by BFS*/

		/*

		 * Record the pointer to the trace for the next lock_list

		 * entry, see the comments for the function.

	/*

	 * A direct locking problem where unsafe_class lock is taken

	 * directly by safe_class lock, then all we need to show

	 * is the deadlock scenario, as it is obvious that the

	 * unsafe lock is taken under the safe lock.

	 *

	 * But if there is a chain instead, where the safe lock takes

	 * an intermediate lock (middle_class) where this lock is

	 * not the same as the safe lock, then the lock chain is

	 * used to describe the problem. Otherwise we would need

	 * to show a different CPU case for each link in the chain

	 * from the safe_class lock to the unsafe_class lock.

/*

 * The bit number is encoded like:

 *

 *  bit0: 0 exclusive, 1 read lock

 *  bit1: 0 used in irq, 1 irq enabled

 *  bit2-n: state

	/*

	 * keep state, bit flip the direction and strip read.

/*

 * Observe that when given a bitmask where each bitnr is encoded as above, a

 * right shift of the mask transforms the individual bitnrs as -1 and

 * conversely, a left shift transforms into +1 for the individual bitnrs.

 *

 * So for all bits whose number have LOCK_ENABLED_* set (bitnr1 == 1), we can

 * create the mask with those bit numbers using LOCK_USED_IN_* (bitnr1 == 0)

 * instead by subtracting the bit number by 2, or shifting the mask right by 2.

 *

 * Similarly, bitnr1 == 0 becomes bitnr1 == 1 by adding 2, or shifting left 2.

 *

 * So split the mask (note that LOCKF_ENABLED_IRQ_ALL|LOCKF_USED_IN_IRQ_ALL is

 * all bits set) and recompose with bitnr1 flipped.

 Invert dir */

/*

 * Note that a LOCK_ENABLED_IRQ_*_READ usage and a LOCK_USED_IN_IRQ_*_READ

 * usage may cause deadlock too, for example:

 *

 * P1				P2

 * <irq disabled>

 * write_lock(l1);		<irq enabled>

 *				read_lock(l2);

 * write_lock(l2);

 * 				<in irq>

 * 				read_lock(l1);

 *

 * , in above case, l1 will be marked as LOCK_USED_IN_IRQ_HARDIRQ_READ and l2

 * will marked as LOCK_ENABLE_IRQ_HARDIRQ_READ, and this is a possible

 * deadlock.

 *

 * In fact, all of the following cases may cause deadlocks:

 *

 * 	 LOCK_USED_IN_IRQ_* -> LOCK_ENABLED_IRQ_*

 * 	 LOCK_USED_IN_IRQ_*_READ -> LOCK_ENABLED_IRQ_*

 * 	 LOCK_USED_IN_IRQ_* -> LOCK_ENABLED_IRQ_*_READ

 * 	 LOCK_USED_IN_IRQ_*_READ -> LOCK_ENABLED_IRQ_*_READ

 *

 * As a result, to calculate the "exclusive mask", first we invert the

 * direction (USED_IN/ENABLED) of the original mask, and 1) for all bits with

 * bitnr0 set (LOCK_*_READ), add those with bitnr0 cleared (LOCK_*). 2) for all

 * bits with bitnr0 cleared (LOCK_*_READ), add those with bitnr0 set (LOCK_*).

/*

 * Retrieve the _possible_ original mask to which @mask is

 * exclusive. Ie: this is the opposite of exclusive_mask().

 * Note that 2 possible original bits can match an exclusive

 * bit: one has LOCK_USAGE_READ_MASK set, the other has it

 * cleared. So both are returned for each exclusive bit.

 Include read in existing usages */

/*

 * Find the first pair of bit match between an original

 * usage mask and an exclusive usage mask.

		/*

		 * exclusive_bit() strips the read bit, however,

		 * LOCK_ENABLED_IRQ_*_READ may cause deadlocks too, so we need

		 * to search excl | LOCK_USAGE_READ_MASK as well.

/*

 * Prove that the new dependency does not connect a hardirq-safe(-read)

 * lock with a hardirq-unsafe lock - to achieve this we search

 * the backwards-subgraph starting at <prev>, and the

 * forwards-subgraph starting at <next>:

	/*

	 * Step 1: gather all hard/soft IRQs usages backward in an

	 * accumulated usage mask.

	/*

	 * Step 2: find exclusive uses forward that match the previous

	 * backward accumulated mask.

	/*

	 * Step 3: we found a bad match! Now retrieve a lock from the backward

	 * list whose usage mask matches the exclusive usage mask from the

	 * lock found on the forward list.

	 *

	 * Note, we should only keep the LOCKF_ENABLED_IRQ_ALL bits, considering

	 * the follow case:

	 *

	 * When trying to add A -> B to the graph, we find that there is a

	 * hardirq-safe L, that L -> ... -> A, and another hardirq-unsafe M,

	 * that B -> ... -> M. However M is **softirq-safe**, if we use exact

	 * invert bits of M's usage_mask, we will find another lock N that is

	 * **softirq-unsafe** and N -> ... -> A, however N -> .. -> M will not

	 * cause a inversion deadlock.

	/*

	 * Step 4: narrow down to a pair of incompatible usage bits

	 * and report it.

 CONFIG_TRACE_IRQFLAGS */

/*

 * Check that the dependency graph starting at <src> can lead to

 * <target> or not. If it can, <src> -> <target> dependency is already

 * in the graph.

 *

 * Return BFS_RMATCH if it does, or BFS_RNOMATCH if it does not, return BFS_E* if

 * any error appears in the bfs search.

	/*

	 * Special setup for check_redundant().

	 *

	 * To report redundant, we need to find a strong dependency path that

	 * is equal to or stronger than <src> -> <target>. So if <src> is E,

	 * we need to let __bfs() only search for a path starting at a -(E*)->,

	 * we achieve this by setting the initial node's ->only_xr to true in

	 * that case. And if <prev> is S, we set initial ->only_xr to false

	 * because both -(S*)-> (equal) and -(E*)-> (stronger) are redundant.

	/*

	 * Note: we skip local_lock() for redundant check, because as the

	 * comment in usage_skip(), A -> local_lock() -> B and A -> B are not

	 * the same.

/*

 * Check whether we are holding such a class already.

 *

 * (Note that this has to be done separately, because the graph cannot

 * detect such classes of deadlocks.)

 *

 * Returns: 0 on deadlock detected, 1 on OK, 2 if another lock with the same

 * lock class is held but nest_lock is also held, i.e. we rely on the

 * nest_lock to avoid the deadlock.

		/*

		 * Allow read-after-read recursion of the same

		 * lock class (i.e. read_lock(lock)+read_lock(lock)):

		/*

		 * We're holding the nest_lock, which serializes this lock's

		 * nesting behaviour.

/*

 * There was a chain-cache miss, and we are about to add a new dependency

 * to a previous lock. We validate the following rules:

 *

 *  - would the adding of the <prev> -> <next> dependency create a

 *    circular dependency in the graph? [== circular deadlock]

 *

 *  - does the new prev->next dependency connect any hardirq-safe lock

 *    (in the full backwards-subgraph starting at <prev>) with any

 *    hardirq-unsafe lock (in the full forwards-subgraph starting at

 *    <next>)? [== illegal lock inversion with hardirq contexts]

 *

 *  - does the new prev->next dependency connect any softirq-safe lock

 *    (in the full backwards-subgraph starting at <prev>) with any

 *    softirq-unsafe lock (in the full forwards-subgraph starting at

 *    <next>)? [== illegal lock inversion with softirq contexts]

 *

 * any of these scenarios could lead to a deadlock.

 *

 * Then if all the validations pass, we add the forwards and backwards

 * dependency.

		/*

		 * The warning statements below may trigger a use-after-free

		 * of the class name. It is better to trigger a use-after free

		 * and to have the class name most of the time instead of not

		 * having the class name available.

	/*

	 * Prove that the new <prev> -> <next> dependency would not

	 * create a circular dependency in the graph. (We do this by

	 * a breadth-first search into the graph starting at <next>,

	 * and check whether we can reach <prev>.)

	 *

	 * The search is limited by the size of the circular queue (i.e.,

	 * MAX_CIRCULAR_QUEUE_SIZE) which keeps track of a breadth of nodes

	 * in the graph whose neighbours are to be checked.

	/*

	 * Is the <prev> -> <next> dependency already present?

	 *

	 * (this may occur even though this is a new chain: consider

	 *  e.g. the L1 -> L2 -> L3 -> L4 and the L5 -> L1 -> L2 -> L3

	 *  chains - the second one will be new, but L1 already has

	 *  L2 added to its dependency list, due to the first chain.)

			/*

			 * Also, update the reverse dependency in @next's

			 * ->locks_before list.

			 *

			 *  Here we reuse @entry as the cursor, which is fine

			 *  because we won't go to the next iteration of the

			 *  outer loop:

			 *

			 *  For normal cases, we return in the inner loop.

			 *

			 *  If we fail to return, we have inconsistency, i.e.

			 *  <prev>::locks_after contains <next> while

			 *  <next>::locks_before doesn't contain <prev>. In

			 *  that case, we return after the inner and indicate

			 *  something is wrong.

 <prev> is not found in <next>::locks_before */

	/*

	 * Is the <prev> -> <next> link redundant?

	/*

	 * Ok, all validations passed, add the new lock

	 * to the previous lock's dependency list:

/*

 * Add the dependency to all directly-previous locks that are 'relevant'.

 * The ones that are relevant are (in increasing distance from curr):

 * all consecutive trylock entries and the final non-trylock entry - or

 * the end of this context's lock-chain - whichever comes first.

	/*

	 * Debugging checks.

	 *

	 * Depth must not be zero for a non-head lock:

	/*

	 * At least two relevant locks must exist for this

	 * to be a head:

			/*

			 * Stop after the first non-trylock entry,

			 * as non-trylock entries have added their

			 * own direct dependencies already, so this

			 * lock is connected to them indirectly:

		/*

		 * End of lock-stack?

		/*

		 * Stop the search if we cross into another context:

	/*

	 * Clearly we all shouldn't be here, but since we made it we

	 * can reliable say we messed up our state. See the above two

	 * gotos for reasons why we could possibly end up here.

 Free chain_hlocks in buckets */

 Lost chain_hlocks */

 size > MAX_CHAIN_BUCKETS */

/*

 * The first 2 chain_hlocks entries in the chain block in the bucket

 * list contains the following meta data:

 *

 *   entry[0]:

 *     Bit    15 - always set to 1 (it is not a class index)

 *     Bits 0-14 - upper 15 bits of the next block index

 *   entry[1]    - lower 16 bits of next block index

 *

 * A next block index of all 1 bits means it is the end of the list.

 *

 * On the unsized bucket (bucket-0), the 3rd and 4th entries contain

 * the chain block size:

 *

 *   entry[2] - upper 16 bits of the chain block size

 *   entry[3] - lower 16 bits of the chain block size

/*

 * Iterate all the chain blocks in a bucket.

/*

 * next block or -1

/*

 * bucket-0 only

		/*

		 * We can't store single entries on the freelist. Leak them.

		 *

		 * One possible way out would be to uniquely mark them, other

		 * than with CHAIN_BLK_FLAG, such that we can recover them when

		 * the block before it is re-added.

		/*

		 * Variable sized, sort large to small.

	/*

	 * Fixed size, add to head.

/*

 * Only the first block in the list can be deleted.

 *

 * For the variable size bucket[0], the first block (the largest one) is

 * returned, broken up and put back into the pool. So if a chain block of

 * length > MAX_CHAIN_BUCKETS is ever used and zapped, it will just be

 * queued up after the primordial chain block and never be used until the

 * hlock entries in the primordial chain block is almost used up. That

 * causes fragmentation and reduce allocation efficiency. That can be

 * monitored by looking at the "large chain blocks" number in lockdep_stats.

/*

 * Return offset of a chain block of the right size or -1 if not found.

 *

 * Fairly simple worst-fit allocator with the addition of a number of size

 * specific free lists.

	/*

	 * We rely on the MSB to act as an escape bit to denote freelist

	 * pointers. Make sure this bit isn't set in 'normal' class_idx usage.

	/*

	 * We require a minimum of 2 (u16) entries to encode a freelist

	 * 'pointer'.

 Try bucket 0 */

	/*

	 * The variable sized freelist is sorted by size; the first entry is

	 * the largest. Use it if it fits.

	/*

	 * Last resort, split a block in a larger sized bucket.

/*

 * Returns the index of the first held_lock of the current chain

/*

 * Returns the next chain_key iteration

/*

 * Checks whether the chain and the current held locks are consistent

 * in depth and also in content. If they are not it most likely means

 * that there was a collision during the calculation of the chain_key.

 * Returns: 0 not passed, 1 passed

/*

 * Given an index that is >= -1, return the index of the next lock chain.

 * Return -2 if there is no next lock chain.

 Must be called with the graph lock held. */

/*

 * Adds a dependency chain into chain hashtable. And must be called with

 * graph_lock held.

 *

 * Return 0 if fail, and graph_lock is released.

 * Return 1 if succeed, with graph_lock held.

	/*

	 * The caller must hold the graph lock, ensure we've got IRQs

	 * disabled to make this an IRQ-safe lock.. for recursion reasons

	 * lockdep won't complain about its own locking errors.

/*

 * Look up a dependency chain. Must be called with either the graph lock or

 * the RCU read lock held.

/*

 * If the key is not present yet in dependency chain cache then

 * add it and return 1 - in this case the new dependency chain is

 * validated. If the key is already hashed, return 0.

 * (On return with 1 graph_lock is held.)

	/*

	 * We have to walk the chain again locked - to avoid duplicates:

	/*

	 * Trylock needs to maintain the stack of held locks, but it

	 * does not add new dependencies, because trylock can be done

	 * in any order.

	 *

	 * We look up the chain_key and do the O(N^2) check and update of

	 * the dependencies only if this is a new dependency chain.

	 * (If lookup_chain_cache_add() return with 1 it acquires

	 * graph_lock for us)

		/*

		 * Check whether last held lock:

		 *

		 * - is irq-safe, if this lock is irq-unsafe

		 * - is softirq-safe, if this lock is hardirq-unsafe

		 *

		 * And check whether the new lock's dependency graph

		 * could lead back to the previous lock:

		 *

		 * - within the current held-lock stack

		 * - across our accumulated lock dependency records

		 *

		 * any of these scenarios could lead to a deadlock.

		/*

		 * The simple case: does the current hold the same lock

		 * already?

		/*

		 * Add dependency only if this lock is not the head

		 * of the chain, and if the new lock introduces no more

		 * lock dependency (because we already hold a lock with the

		 * same lock class) nor deadlock (because the nest_lock

		 * serializes nesting locks), see the comments for

		 * check_deadlock().

 after lookup_chain_cache_add(): */

 CONFIG_PROVE_LOCKING */

/*

 * We are building curr_chain_key incrementally, so double-check

 * it from scratch, to make sure that it's done correctly:

			/*

			 * We got mighty confused, our chain keys don't match

			 * with what we expect, someone trample on our task state?

		/*

		 * hlock->class_idx can't go beyond MAX_LOCKDEP_KEYS, but is

		 * it registered lock class index?

		/*

		 * More smoking hash instead of calculating it, damn see these

		 * numbers float.. I bet that a pink elephant stepped on my memory.

/*

 * Print out an error if an invalid bit is set:

/*

 * print irq inversion bug:

 Find a middle lock (if one exists) */

/*

 * Prove that in the forwards-direction subgraph starting at <this>

 * there is no lock matching <mask>:

 Check whether write or read usage is the match */

/*

 * Prove that in the backwards-direction subgraph starting at <this>

 * there is no lock matching <mask>:

 Check whether write or read usage is the match */

	/*

	 * Validate that this particular lock does not have conflicting

	 * usage states.

	/*

	 * Check for read in write conflicts

	/*

	 * Validate that the lock dependencies don't have conflicting usage

	 * states.

		/*

		 * mark ENABLED has to look backwards -- to ensure no dependee

		 * has USED_IN state, which, again, would allow  recursion deadlocks.

		/*

		 * mark USED_IN has to look forwards -- to ensure no dependency

		 * has ENABLED state, which would allow recursion deadlocks.

/*

 * Mark all held locks with a usage bit:

/*

 * Hardirqs will be enabled:

	/*

	 * We are going to turn hardirqs on, so set the

	 * usage bit for all held locks:

	/*

	 * If we have softirqs enabled, then set the usage

	 * bit for all held locks. (disabled hardirqs prevented

	 * this bit from being set before)

/**

 * lockdep_hardirqs_on_prepare - Prepare for enabling interrupts

 * @ip:		Caller address

 *

 * Invoked before a possible transition to RCU idle from exit to user or

 * guest mode. This ensures that all RCU operations are done before RCU

 * stops watching. After the RCU transition lockdep_hardirqs_on() has to be

 * invoked to set the final state.

	/*

	 * NMIs do not (and cannot) track lock dependencies, nothing to do.

		/*

		 * Neither irq nor preemption are disabled here

		 * so this is racy by nature but losing one hit

		 * in a stat is not a big deal.

	/*

	 * We're enabling irqs and according to our state above irqs weren't

	 * already enabled, yet we find the hardware thinks they are in fact

	 * enabled.. someone messed up their IRQ state tracing.

	/*

	 * See the fine text that goes along with this variable definition.

	/*

	 * Can't allow enabling interrupts while in an interrupt handler,

	 * that's general bad form and such. Recursion, limited stack etc..

	/*

	 * NMIs can happen in the middle of local_irq_{en,dis}able() where the

	 * tracking state and hardware state are out of sync.

	 *

	 * NMIs must save lockdep_hardirqs_enabled() to restore IRQ state from,

	 * and not rely on hardware state like normal interrupts.

		/*

		 * Skip:

		 *  - recursion check, because NMI can hit lockdep;

		 *  - hardware state check, because above;

		 *  - chain_key check, see lockdep_hardirqs_on_prepare().

		/*

		 * Neither irq nor preemption are disabled here

		 * so this is racy by nature but losing one hit

		 * in a stat is not a big deal.

	/*

	 * We're enabling irqs and according to our state above irqs weren't

	 * already enabled, yet we find the hardware thinks they are in fact

	 * enabled.. someone messed up their IRQ state tracing.

	/*

	 * Ensure the lock stack remained unchanged between

	 * lockdep_hardirqs_on_prepare() and lockdep_hardirqs_on().

 we'll do an OFF -> ON transition: */

/*

 * Hardirqs were disabled:

	/*

	 * Matching lockdep_hardirqs_on(), allow NMIs in the middle of lockdep;

	 * they will restore the software state. This ensures the software

	 * state is consistent inside NMIs as well.

	/*

	 * So we're supposed to get called after you mask local IRQs, but for

	 * some reason the hardware doesn't quite think you did a proper job.

		/*

		 * We have done an ON -> OFF transition:

/*

 * Softirqs will be enabled:

	/*

	 * We fancy IRQs being disabled here, see softirq.c, avoids

	 * funny state and nesting things.

	/*

	 * We'll do an OFF -> ON transition:

	/*

	 * We are going to turn softirqs on, so set the

	 * usage bit for all held locks, if hardirqs are

	 * enabled too:

/*

 * Softirqs were disabled:

	/*

	 * We fancy IRQs being disabled here, see softirq.c

		/*

		 * We have done an ON -> OFF transition:

		/*

		 * Whoops, we wanted softirqs off, so why aren't they?

	/*

	 * If non-trylock use in a hardirq or softirq context, then

	 * mark the lock as used in these contexts:

 mark it as used: */

	/*

	 * Keep track of points where we cross into an interrupt context:

		/*

		 * If we cross into another context, reset the

		 * hash key (this also prevents the checking and the

		 * adding of the dependency to 'prev'):

/*

 * Mark a lock with a usage bit, and validate the state transition:

	/*

	 * If already set then do not dirty the cacheline,

	 * nor do any checks:

	/*

	 * Make sure we didn't race:

	/*

	 * We must printk outside of the graph_lock:

	/*

	 * Set appropriate wait type for the context; for IRQs we have to take

	 * into account force_irqthread as that is implied by PREEMPT_RT.

		/*

		 * Check if force_irqthreads will run us threaded.

		/*

		 * Softirqs are always threaded.

/*

 * Verify the wait_type context.

 *

 * This check validates we take locks in the right wait-type order; that is it

 * ensures that we do not take mutexes inside spinlocks and do not attempt to

 * acquire spinlocks inside raw_spinlocks and the sort.

 *

 * The entire thing is slightly more complex because of RCU, RCU is a lock that

 * can be taken from (pretty much) any context but also has constraints.

 * However when taken in a stricter environment the RCU lock does not loosen

 * the constraints.

 *

 * Therefore we must look for the strictest environment in the lock stack and

 * compare that to the lock we're trying to acquire.

	/*

	 * Find start of current irq_context..

			/*

			 * We can have a bigger inner than a previous one

			 * when outer is smaller than inner, as with RCU.

			 *

			 * Also due to trylocks.

 CONFIG_PROVE_LOCKING */

 CONFIG_PROVE_LOCKING */

/*

 * Initialize a lock instance's lock-class mapping info:

	/*

	 * Can't be having no nameless bastards around this place!

	/*

	 * No key, no joy, we need to hash something.

	/*

	 * Sanity check, the lock-class key must either have been allocated

	 * statically or must have been registered as a dynamic key.

/*

 * This gets called for every mutex_lock*()/spin_lock*() operation.

 * We maintain the dependency maps and validate the locking attempt:

 *

 * The callers must make sure that IRQs are disabled before calling it,

 * otherwise we could get an interrupt which would want to take locks,

 * which would end up in lockdep again.

	/*

	 * Not cached?

	/*

	 * Add the lock to the list of currently held locks.

	 * (we dont increase the depth just yet, up until the

	 * dependency checks are done)

	/*

	 * Ran out of static storage for our per-task lock stack again have we?

 we're holding locks */

 Overflow */

	/*

	 * Plain impossible, we just registered it and checked it weren't no

	 * NULL like.. I bet this mushroom I ate was good!

 Initialize the lock usage bit */

	/*

	 * Calculate the chain hash: it's the combined hash of all the

	 * lock keys along the dependency chain. We save the hash value

	 * at every step so that we can get the current hash easily

	 * after unlock. The chain hash is then used to cache dependency

	 * results.

	 *

	 * The 'key ID' is what is the most compact key value to drive

	 * the hash, not class->key.

	/*

	 * Whoops, we did it again.. class_idx is invalid.

		/*

		 * How can we have a chain hash when we ain't got no keys?!

		/*

		 * If look_up_lock_class() failed to find a class, we're trying

		 * to test if we hold a lock that has never yet been acquired.

		 * Clearly if the lock hasn't been acquired _ever_, we're not

		 * holding it either, so report failure.

		/*

		 * References, but not a lock we're actually ref-counting?

		 * State got messed up, follow the sites that change ->references

		 * and try to make sense of it.

 @depth must not be zero */

		/*

		 * We must not cross into another context:

	/*

	 * This function is about (re)setting the class of a held lock,

	 * yet we're not actually holding any locks. Naughty user!

	/*

	 * I took it apart and put it back together again, except now I have

	 * these 'spare' parts.. where shall I put them.

	/*

	 * This function is about (re)setting the class of a held lock,

	 * yet we're not actually holding any locks. Naughty user!

 Merging can't happen with unchanged classes.. */

	/*

	 * I took it apart and put it back together again, except now I have

	 * these 'spare' parts.. where shall I put them.

/*

 * Remove the lock from the list of currently held locks - this gets

 * called on mutex_unlock()/spin_unlock*() (or on a failed

 * mutex_lock_interruptible()).

	/*

	 * So we're all set to release this lock.. wait what lock? We don't

	 * own any locks, you've been drinking again?

	/*

	 * Check whether the lock exists in the current stack

	 * of held locks:

			/*

			 * We had, and after removing one, still have

			 * references, the current lock stack is still

			 * valid. We're done!

	/*

	 * We have the right lock to unlock, 'hlock' points to it.

	 * Now we remove it from the stack, and add back the other

	 * entries (if any), recalculating the hash along the way:

	/*

	 * The most likely case is when the unlock is on the innermost

	 * lock. In this case, we are done!

	/*

	 * We had N bottles of beer on the wall, we drank one, but now

	 * there's not N-1 bottles of beer left on the wall...

	 * Pouring two of the bottles together is acceptable.

	/*

	 * Since reacquire_held_locks() would have called check_chain_key()

	 * indirectly via __lock_acquire(), we don't need to do it again

	 * on return.

			/*

			 * Grab 16bits of randomness; this is sufficient to not

			 * be guessable and still allows some pin nesting in

			 * our u32 pin_count.

/*

 * Check whether we follow the irq-flags state precisely:

 Get the warning out..  */

	/*

	 * We dont accurately track softirq state in e.g.

	 * hardirq contexts (such as on 4KSTACKS), so only

	 * check if not in hardirq contexts:

 like the above, but with softirqs */

 lick the above, does it taste good? */

 NMI context !!! */

 if it doesn't have a class (yet), it certainly hasn't been used yet */

	/*

	 * READ locks only conflict with USED, such that if we only ever use

	 * READ locks, there is no deadlock possible -- RCU.

/*

 * read_lock() is recursive if:

 * 1. We force lockdep think this way in selftests or

 * 2. The implementation is not queued read/write lock or

 * 3. The locker is at an in_interrupt() context.

/*

 * We are not always called with irqs disabled - do that here,

 * and also avoid lockdep recursion:

 XXX allow trylock from NMI ?!? */

 XXX

	/*

	 * Avoid false negative lockdep_assert_held() and

	 * lockdep_assert_not_held().

	/*

	 * Whee, we contended on this lock, except it seems we're not

	 * actually trying to acquire anything much at all..

	/*

	 * Yay, we acquired ownership of this lock we didn't try to

	 * acquire, how the heck did that happen?

/*

 * Used by the testsuite, sanitize the validator state

 * after a simulated failure:

 Remove a class from a lock chain. Must be called with the graph lock held. */

		/*

		 * Each lock class occurs at most once in a lock chain so once

		 * we found a match we can break out of this loop.

 Since the chain has not been modified, return. */

 Overwrite the chain key for concurrent RCU readers. */

	/*

	 * Note: calling hlist_del_rcu() from inside a

	 * hlist_for_each_entry_rcu() loop is safe.

 Must be called with the graph lock held. */

/*

 * Remove all references to a lock class. The caller must hold the graph lock.

	/*

	 * Remove all dependencies this lock is

	 * involved in:

 The caller must hold the graph lock. */

/*

 * Schedule an RCU callback if no RCU callback is pending. Must be called with

 * the graph lock held.

 The caller must hold the graph lock. May be called from RCU context. */

 closed head */

	/*

	 * If there's anything on the open list, close and start a new callback.

/*

 * Remove all lock classes from the class hash table and from the

 * all_lock_classes list whose key or name is in the address range [start,

 * start + size). Move these lock classes to the zapped_classes list. Must

 * be called with the graph lock held.

 Unhash all classes that were created by a module. */

/*

 * Used in module.c to remove lock classes from memory that is going to be

 * freed; and possibly re-used by other modules.

 *

 * We will have had one synchronize_rcu() before getting here, so we're

 * guaranteed nobody will look up these exact classes -- they're properly dead

 * but still allocated.

	/*

	 * Wait for any possible iterators from look_up_lock_class() to pass

	 * before continuing to free the memory they refer to.

/*

 * Free all lockdep keys in the range [start, start+size). Does not sleep.

 * Ignores debug_locks. Must only be used by the lockdep selftests.

/*

 * Check whether any element of the @lock->class_cache[] array refers to a

 * registered lock class. The caller must hold either the graph lock or the

 * RCU read lock.

 The caller must hold the graph lock. Does not sleep. */

	/*

	 * Remove all classes this lock might have:

		/*

		 * If the class exists we look it up and zap it:

	/*

	 * Debug check: in the end all mapped classes should

	 * be gone.

/*

 * Remove all information lockdep has about a lock if debug_locks == 1. Free

 * released data structures from RCU context.

/*

 * Reset a lock. Does not sleep. Ignores debug_locks. Must only be used by the

 * lockdep selftests.

 Unregister a dynamically allocated key. */

 Wait until is_dynamic_key() has finished accessing k->hash_entry. */

/*

 * Called when kernel memory is freed (or unmapped), or if a lock

 * is destroyed or reinitialized - this code checks whether there is

 * any held lock in the memory range of <from> to <to>:

/*

 * Careful: only use this function if you are sure that

 * the task cannot run in parallel!

	/*

	 * The lock history for each syscall should be independent. So wipe the

	 * slate clean on return to userspace.

 Note: the following can be executed concurrently, so be careful. */

	/*

	 * If a CPU is in the RCU-free window in idle (ie: in the section

	 * between rcu_idle_enter() and rcu_idle_exit(), then RCU

	 * considers that CPU to be in an "extended quiescent state",

	 * which means that RCU will be completely ignoring that CPU.

	 * Therefore, rcu_read_lock() and friends have absolutely no

	 * effect on a CPU running in that state. In other words, even if

	 * such an RCU-idle CPU has called rcu_read_lock(), RCU might well

	 * delete data structures out from under it.  RCU really has no

	 * choice here: we need to keep an RCU-free window in idle where

	 * the CPU may possibly enter into low power mode. This way we can

	 * notice an extended quiescent state to other CPUs that started a grace

	 * period. Otherwise we would delay any grace period as long as we run

	 * in the idle task.

	 *

	 * So complain bitterly if someone does call rcu_read_lock(),

	 * rcu_read_lock_bh() and so on from extended quiescent states.

 SPDX-License-Identifier: GPL-2.0+

/*

 * RCU-based infrastructure for lightweight reader-writer locking

 *

 * Copyright (c) 2015, Red Hat, Inc.

 *

 * Author: Oleg Nesterov <oleg@redhat.com>

/**

 * rcu_sync_init() - Initialize an rcu_sync structure

 * @rsp: Pointer to rcu_sync structure to be initialized

/**

 * rcu_sync_enter_start - Force readers onto slow path for multiple updates

 * @rsp: Pointer to rcu_sync structure to use for synchronization

 *

 * Must be called after rcu_sync_init() and before first use.

 *

 * Ensures rcu_sync_is_idle() returns false and rcu_sync_{enter,exit}()

 * pairs turn into NO-OPs.

/**

 * rcu_sync_func() - Callback function managing reader access to fastpath

 * @rhp: Pointer to rcu_head in rcu_sync structure to use for synchronization

 *

 * This function is passed to call_rcu() function by rcu_sync_enter() and

 * rcu_sync_exit(), so that it is invoked after a grace period following the

 * that invocation of enter/exit.

 *

 * If it is called by rcu_sync_enter() it signals that all the readers were

 * switched onto slow path.

 *

 * If it is called by rcu_sync_exit() it takes action based on events that

 * have taken place in the meantime, so that closely spaced rcu_sync_enter()

 * and rcu_sync_exit() pairs need not wait for a grace period.

 *

 * If another rcu_sync_enter() is invoked before the grace period

 * ended, reset state to allow the next rcu_sync_exit() to let the

 * readers back onto their fastpaths (after a grace period).  If both

 * another rcu_sync_enter() and its matching rcu_sync_exit() are invoked

 * before the grace period ended, re-invoke call_rcu() on behalf of that

 * rcu_sync_exit().  Otherwise, set all state back to idle so that readers

 * can again use their fastpaths.

		/*

		 * We're at least a GP after the GP_IDLE->GP_ENTER transition.

		/*

		 * A new rcu_sync_exit() has happened; requeue the callback to

		 * catch a later GP.

		/*

		 * We're at least a GP after the last rcu_sync_exit(); everybody

		 * will now have observed the write side critical section.

		 * Let 'em rip!

/**

 * rcu_sync_enter() - Force readers onto slowpath

 * @rsp: Pointer to rcu_sync structure to use for synchronization

 *

 * This function is used by updaters who need readers to make use of

 * a slowpath during the update.  After this function returns, all

 * subsequent calls to rcu_sync_is_idle() will return false, which

 * tells readers to stay off their fastpaths.  A later call to

 * rcu_sync_exit() re-enables reader slowpaths.

 *

 * When called in isolation, rcu_sync_enter() must wait for a grace

 * period, however, closely spaced calls to rcu_sync_enter() can

 * optimize away the grace-period wait via a state machine implemented

 * by rcu_sync_enter(), rcu_sync_exit(), and rcu_sync_func().

		/*

		 * Note that we could simply do rcu_sync_call(rsp) here and

		 * avoid the "if (gp_state == GP_IDLE)" block below.

		 *

		 * However, synchronize_rcu() can be faster if rcu_expedited

		 * or rcu_blocking_is_gp() is true.

		 *

		 * Another reason is that we can't wait for rcu callback if

		 * we are called at early boot time but this shouldn't happen.

		/*

		 * See the comment above, this simply does the "synchronous"

		 * call_rcu(rcu_sync_func) which does GP_ENTER -> GP_PASSED.

 Not really needed, wait_event() would see GP_PASSED. */

/**

 * rcu_sync_exit() - Allow readers back onto fast path after grace period

 * @rsp: Pointer to rcu_sync structure to use for synchronization

 *

 * This function is used by updaters who have completed, and can therefore

 * now allow readers to make use of their fastpaths after a grace period

 * has elapsed.  After this grace period has completed, all subsequent

 * calls to rcu_sync_is_idle() will return true, which tells readers that

 * they can once again use their fastpaths.

/**

 * rcu_sync_dtor() - Clean up an rcu_sync structure

 * @rsp: Pointer to rcu_sync structure to be cleaned up

 SPDX-License-Identifier: GPL-2.0+

/*

 * Read-Copy Update module-based scalability-test facility

 *

 * Copyright (C) IBM Corporation, 2015

 *

 * Authors: Paul E. McKenney <paulmck@linux.ibm.com>

/*

 * The intended use cases for the nreaders and nwriters module parameters

 * are as follows:

 *

 * 1.	Specify only the nr_cpus kernel boot parameter.  This will

 *	set both nreaders and nwriters to the value specified by

 *	nr_cpus for a mixed reader/writer test.

 *

 * 2.	Specify the nr_cpus kernel boot parameter, but set

 *	rcuscale.nreaders to zero.  This will set nwriters to the

 *	value specified by nr_cpus for an update-only test.

 *

 * 3.	Specify the nr_cpus kernel boot parameter, but set

 *	rcuscale.nwriters to zero.  This will set nreaders to the

 *	value specified by nr_cpus for a read-only test.

 *

 * Various other use cases may of course be specified.

 *

 * Note that this test's readers are intended only as a test load for

 * the writers.  The reader scalability statistics will be overly

 * pessimistic due to the per-critical-section interrupt disabling,

 * test-end checks, and the pair of calls through pointers.

/*

 * Operations vector for selecting different types of tests.

/*

 * Definitions for rcu scalability testing.

/*

 * Definitions for srcu scalability testing.

/*

 * Definitions for RCU-tasks scalability testing.

/*

 * Definitions for RCU-tasks-trace scalability testing.

/*

 * If scalability tests complete, wait for shutdown to commence.

/*

 * RCU scalability reader kthread.  Repeatedly does empty RCU read-side

 * critical section, minimizing update-side interference.  However, the

 * point of this test is not to evaluate reader scalability, but instead

 * to serve as a test load for update-side scalability testing.

/*

 * Callback function for asynchronous grace periods from rcu_scale_writer().

/*

 * RCU scale writer kthread.  Repeatedly does a grace period.

	/*

	 * Wait until rcu_end_inkernel_boot() is called for normal GP tests

	 * so that RCU is not always expedited for normal GP tests.

	 * The system_state test is approximate, but works well in practice.

 Because we are stopping. */

 Assign before wake. */

	/*

	 * Would like warning at start, but everything is expedited

	 * during the mid-boot phase, so have to wait till the end.

 Do torture-type-specific cleanup operations.  */

/*

 * Return the number if non-negative.  If -1, the number of CPUs.

 * If less than -1, that much less than the number of CPUs, but

 * at least one.

/*

 * RCU scalability shutdown kthread.  Just waits to be awakened, then shuts

 * down system.

 Wake before output. */

/*

 * kfree_rcu() scalability tests: Start a kfree_rcu() loop on all CPUs for number

 * of iterations and measure total time and number of GP for all iterations to complete.

 By default kfree_rcu_test_single and kfree_rcu_test_double are

 initialized to false. If both have the same value (false or true)

 both are randomly tested, otherwise only the one with value true

 is tested.

 Assign before wake. */

/*

 * shutdown kthread.  Just waits to be awakened, then shuts down system.

 Wake before output. */

 Start up the kthreads. */

 Process args and announce that the scalability'er is on the job. */

 Start up the kthreads. */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Read-Copy Update module-based torture test facility

 *

 * Copyright (C) IBM Corporation, 2005, 2006

 *

 * Authors: Paul E. McKenney <paulmck@linux.ibm.com>

 *	  Josh Triplett <josh@joshtriplett.org>

 *

 * See also:  Documentation/RCU/torture.rst

 Bits for ->extendables field, extendables param, and related definitions. */

 Put SRCU index in upper bits. */

 Extend readers by disabling bh. */

  ... disabling interrupts. */

  ... disabling preemption. */

  ... rcu_read_lock_bh(). */

  ... rcu_read_lock_sched(). */

  ... entering another RCU reader. */

 Number of bits defined above. */

 Maximum reader extensions. */

 Must be power of two minus one. */

 Mailbox-like structure to check RCU global memory ordering.

 Update-side data structure used to check RCU readers.

 did rcu_barrier test succeed? */

 Record reader segment types and duration for first failing read. */

 #ifdef CONFIG_RCU_TRACE */

 #else #ifdef CONFIG_RCU_TRACE */

/*

 * Stop aggressive CPU-hog tests a bit before the end of the test in order

 * to avoid interfering with test shutdown.

 jiffies of next boost test start. */

 protect setting boost_starttime */

  and boost task create/destroy. */

 Barrier callbacks registered. */

 Test phase. */

 Barrier callbacks invoked. */

 Coordinate barrier testing. */

 Short rcu_torture_delay() delays. */

/*

 * Allocate an element from the rcu_tortures pool.

/*

 * Free an element to the rcu_tortures pool.

/*

 * Operations vector for selecting different types of tests.

/*

 * Definitions for rcu torture testing.

	/* We want a short delay sometimes to make a reader delay the grace

	 * period, and we want a long delay occasionally to trigger

 Avoid triggering BH limits. */

 QS only if preemptible. */

/*

 * Update callback in the pipe.  This should be invoked after a grace period.

 Pair with smp_load_acquire().

/*

 * Update all callbacks in the pipe.  Suitable for synchronous grace-period

 * primitives.

 Test is ending, just drop callbacks on the floor. */

 The next initialization will pick up the pieces. */

/*

 * Don't even think about trying any of these in real life!!!

 * The names includes "busted", and they really means it!

 * The only purpose of these functions is to provide a buggy RCU

 * implementation to make sure that rcutorture correctly emits

 * buggy-RCU error messages.

 This is a deliberate bug for testing purposes only! */

 This is a deliberate bug for testing purposes only! */

 This is a deliberate bug for testing purposes only! */

 just reuse rcu's version. */

/*

 * Definitions for srcu torture testing.

 We want there to be long-running readers, but not all the time. */

 In case of a later rcutorture run. */

 As above, but dynamically allocated. */

 As above, but broken due to inappropriate reader extension. */

/*

 * Definitions for RCU-tasks torture testing.

 just reuse rcu's version. */

/*

 * Definitions for trivial CONFIG_PREEMPT=n-only torture testing.

 * This implementation does not necessarily work well with CPU hotplug.

 just reuse rcu's version. */

/*

 * Definitions for rude RCU-tasks torture testing.

 just reuse rcu's version. */

/*

 * Definitions for tracing RCU-tasks torture testing.

 just reuse srcu's version. */

/*

 * RCU torture priority-boost testing.  Runs one real-time thread per

 * CPU for moderate bursts, repeatedly starting grace periods and waiting

 * for them to complete.  If a given grace period takes too long, we assume

 * that priority inversion has occurred.

	/*

	 * Disable RT throttling so that rcutorture's boost threads don't get

	 * throttled. Only possible if rcutorture is built-in otherwise the

	 * user should manually do this by setting the sched_rt_period_us and

	 * sched_rt_runtime sysctls.

 Recheck after checking time to avoid false positives.

 Time check before grace-period check.

 passed, though perhaps just barely

 At most one persisted message per boost test.

 passed on a technicality

 Recheck after print to flag grace period ending during splat.

 failed

 passed

 Set real-time priority. */

 Each pass through the following loop does one boost-test cycle. */

 Test failed already in this test interval

 Wait for the next test interval. */

 Do one boost-test interval.

 Has current GP gone too long?

 If we don't have a grace period in flight, start one.

 If the grace period already ended,

 we don't know when that happened, so

 start over.

 In case the grace period extended beyond the end of the loop.

		/*

		 * Set the start time of the next test interval.

		 * Yes, this is vulnerable to long delays, but such

		 * delays simply cause a false negative for the next

		 * interval.  Besides, we are running at RT priority,

		 * so delays should be relatively rare.

 Go do the stutter. */

 Clean up and exit. */

/*

 * RCU torture force-quiescent-state kthread.  Repeatedly induces

 * bursts of calls to force_quiescent_state(), increasing the probability

 * of occurrence of some important types of race conditions.

 Used by writers to randomly choose from the available grace-period

 primitives.  The only purpose of the initialization is to size the array.

/*

 * Determine which grace-period primitives are available.

 Initialize synctype[] array.  If none set, take default. */

/*

 * RCU torture writer kthread.  Repeatedly substitutes a new structure

 * for that pointed to by rcu_torture_current, freeing the old structure

 * after a series of grace periods (the "pipeline").

		/*

		 * No updates primitives, so don't try updating.

		 * The resulting test won't be testing much, hence the

		 * above WARN_ONCE().

 Mods to old_rp must follow rcu_assign_pointer() */

 Cycle through nesting levels of rcu_expedite_gp() calls. */

 Disabled during boot, recheck. */

 Let stats task know that we are done.

 Reset expediting back to unexpedited. */

/*

 * RCU torture fake writer kthread.  Repeatedly calls sync, with a random

 * delay between calls.

 Set up and carry out testing of RCU's global memory ordering

 Me.

 Assigned us to do checking.

 Reader being checked.

 Reader doing checking when not me.

 Don't try this from timer handlers.

 Increment my counter.

 Attempt to assign someone else some checking work.

 Pairs with smp_store_release below.

 Pairs with smp_store_release below.

 This gets set after the grace period ends.

 Back out.

 If assigned some completed work, do it!

 No work or work not yet ready.

 Someone else can assign us work.

 Assigner can again assign.

/*

 * Do one extension of an RCU read-side critical section using the

 * current reader state in readstate (set to zero for initial entry

 * to extended critical section), set the new state as specified by

 * newstate (set to zero for final exit from extended critical section),

 * and random-number-generator state in trsp.  If this is neither the

 * beginning or end of the critical section and if there was actually a

 * change, do a ->read_delay().

 First, put new protection in place to avoid critical-section gap. */

	/*

	 * Next, remove old protection, in decreasing order of strength

	 * to avoid unlock paths that aren't safe in the stronger

	 * context. Namely: BH can not be enabled with disabled interrupts.

	 * Additionally PREEMPT_RT requires that BH is enabled in preemptible

	 * context.

 Delay if neither beginning nor end and there was a change. */

 Update the reader state. */

 Return the biggest extendables mask given current RCU and boot parameters. */

 Return a random protection state mask, but with at least one bit set. */

 Mostly only one bit (need preemption!), sometimes lots of bits. */

	/*

	 * Can't enable bh w/irq disabled.

	/*

	 * Ideally these sequences would be detected in debug builds

	 * (regardless of RT), but until then don't stop testing

	 * them on non-RT.

 Can't modify BH in atomic context */

/*

 * Do a randomly selected number of extensions of an existing RCU read-side

 * critical section.

 -Existing- RCU read-side critsect! */

 Current RCU reader not extendable. */

 Bias towards larger numbers of loops. */

/*

 * Do one read-side critical section, returning false if there was

 * no data to read.  Can be invoked both from process context and

 * from a timer handler.

 Wait for rcu_torture_writer to get underway */

 Should not happen, but... */

 Should not happen, but... */

 This next splat is expected behavior if leakpointer, especially

 for CONFIG_RCU_STRICT_GRACE_PERIOD=y kernels.

 If error or close call, record the sequence of reader protections. */

/*

 * RCU torture reader from timer handler.  Dereferences rcu_torture_current,

 * incrementing the corresponding element of the pipeline array.  The

 * counter in the element should never be greater than 1, otherwise, the

 * RCU implementation is broken.

 Test call_rcu() invocation from interrupt handler. */

/*

 * RCU torture reader kthread.  Repeatedly dereferences rcu_torture_current,

 * incrementing the corresponding element of the pipeline array.  The

 * counter in the element should never be greater than 1, otherwise, the

 * RCU implementation is broken.

/*

 * Randomly Toggle CPUs' callback-offload state.  This uses hrtimers to

 * increase race probabilities and fuzzes the interval between toggling.

/*

 * Print torture statistics.  Caller must ensure that there is only

 * one call to this function at a given time!!!  This is normally

 * accomplished by relying on the module system to only have one copy

 * of the module loaded, and then by giving the rcu_torture_stats

 * kthread full control (or the init/cleanup functions when rcu_torture_stats

 * thread is not running).

 Statistic.

 rcu_barrier()

 no boost kthread

 can't set RT prio

 boost failed (TIMER_SOFTIRQ RT prio?)

 Too-short grace period

/*

 * Periodically prints torture statistics, if periodic statistics printing

 * was specified via the stat_interval module parameter.

 Test mem_dump_obj() and friends.  */

 This must be outside of the mutex, otherwise deadlock! */

 Already created, nothing more to do. */

 Don't allow time recalculation while creating a new task. */

/*

 * CPU-stall kthread.  It waits as specified by stall_cpu_holdoff, then

 * induces a CPU stall for the time specified by stall_cpu.

 RCU CPU stall is expected behavior in following code. */

 Spawn CPU-stall kthread, if stall_cpu specified. */

 State structure for forward-progress self-propagating RCU callback. */

/*

 * Forward-progress self-propagating RCU callback function.  Because

 * callbacks run from softirq, this function is an implicit RCU read-side

 * critical section.

 State for continuous-flood RCU callbacks. */

 Maximum CB test duration. */

 This many CB invocations to count. */

 Number of counted CBs. */

 Histogram buckets/second. */

 Callback function for continuous-flood RCU callbacks. */

 Give the scheduler a chance, even on nohz_full CPUs.

 Real call_rcu() floods hit userspace, so emulate that.

 No userspace emulation: CB invocation throttles call_rcu()

/*

 * Free all callbacks on the rcu_fwd_cb_head list, either because the

 * test is over or because we hit an OOM event.

 Carry out need_resched()/cond_resched() forward-progress testing. */

 Cannot do need_resched() forward progress testing without ->sync.

 Tight loop containing cond_resched(). */

 Later readers see above write. */

 Wait for running CB to complete. */

 Wait for queued callbacks. */

 Let kthreads recover. */

 Carry out call_rcu() forward-progress testing. */

 Get out of the way quickly, no GP wait! */

 Can't do call_rcu() fwd prog without ->call. */

 Loop continuously posting RCU callbacks. */

 Later readers see above write. */

 Hoist initialization for multi-kthread

 Wait for callbacks to be invoked. */

 Let CBs drain. */

/*

 * OOM notifier, but this only prints diagnostic information for the

 * current forward-progress test.

 Emergency stop before free and wait to avoid hangs. */

 Frees before return to avoid redoing OOM. */

 Forward progress CBs freed! */

 Carry out grace-period forward-progress testing. */

 Avoid slow periods, better to test when busy. */

 Short runs might not contain a valid forward-progress attempt. */

 If forward-progress checking is requested and feasible, spawn the thread. */

 Not requested, so don't do it. */

 In module, can fail back to user. */

 Make sure rcutorture notices conflict. */

 Callback function for RCU barrier testing. */

 IPI handler to get callback posted on desired CPU, if online. */

 kthread function to register callbacks used to test RCU barriers. */

		/*

		 * The above smp_load_acquire() ensures barrier_phase load

		 * is ordered before the following ->call().

 IPI failed, so use direct call from current CPU.

 kthread function to drive and coordinate RCU barrier testing. */

 Ensure barrier_phase ordered after prior assignments. */

 Implies smp_mb() for wait_event(). */

 Wait manually for the remaining callbacks

 Can't trust ordering if broken.

 Initialize RCU barrier testing. */

 Clean up after RCU barrier testing. */

 Child kthread which just does an rcutorture reader and exits.

 Minimize time between reading and exiting.

 Parent kthread which creates and destroys read-exit child kthreads.

 Allocate and initialize.

 Each pass through this loop does one read-exit episode.

 Wait for task_struct free, avoid OOM.

 Spawn child.

 Clean up and exit.

 After reaping.

 Store before wakeup.

 Above write before wait.

	/*

	 * Wait for all RCU callbacks to fire, then do torture-type-specific

	 * cleanup operations.

 -After- the stats thread is stopped! */

	/*

	 * This -might- happen due to race conditions, but is unlikely.

	 * The scenario that leads to this happening is that the

	 * first of the pair of duplicate callbacks is queued,

	 * someone else starts a grace period that includes that

	 * callback, then the second of the pair must wait for the

	 * next grace period.  Unlikely, but can happen.  If it

	 * does happen, the debug-objects subsystem won't have splatted.

 #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD */

/*

 * Verify that double-free causes debug-objects to complain, but only

 * if CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.  Otherwise, say that the test

 * cannot be carried out.

 Try to queue the rh2 pair of callbacks for the same grace period. */

 Prevent preemption from interrupting test. */

 Make it impossible to finish a grace period. */

 Start grace period. */

 Make it harder to start a new grace period. */

 Duplicate callback. */

 Another duplicate callback. */

 Wait for them all to get done so we can safely return. */

 #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD */

 #else #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD */

 Process args and tell the world that the torturer is on the job. */

 Set up the freelist. */

 Initialize the statistics so that each run gets its own numbers. */

 Start up the kthreads. */

 Create the fqs thread */

 Testing RCU priority boosting requires rcutorture do

 some serious abuse.  Counter this by running ksoftirqd

 at higher priority.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Read-Copy Update mechanism for mutual exclusion

 *

 * Copyright IBM Corporation, 2001

 *

 * Authors: Dipankar Sarma <dipankar@in.ibm.com>

 *	    Manfred Spraul <manfred@colorfullife.com>

 *

 * Based on the original work by Paul McKenney <paulmck@linux.ibm.com>

 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.

 * Papers:

 * http://www.rdrop.com/users/paulmck/paper/rclockpdcsproof.pdf

 * http://lse.sourceforge.net/locking/rclock_OLS.2001.05.01c.sc.pdf (OLS2001)

 *

 * For detailed explanation of Read-Copy Update mechanism see -

 *		http://lse.sourceforge.net/locking/rcupdate.html

 *

 #ifndef CONFIG_TINY_RCU */

/**

 * rcu_read_lock_held_common() - might we be in RCU-sched read-side critical section?

 * @ret:	Best guess answer if lockdep cannot be relied on

 *

 * Returns true if lockdep must be ignored, in which case ``*ret`` contains

 * the best guess described below.  Otherwise returns false, in which

 * case ``*ret`` tells the caller nothing and the caller should instead

 * consult lockdep.

 *

 * If CONFIG_DEBUG_LOCK_ALLOC is selected, set ``*ret`` to nonzero iff in an

 * RCU-sched read-side critical section.  In absence of

 * CONFIG_DEBUG_LOCK_ALLOC, this assumes we are in an RCU-sched read-side

 * critical section unless it can prove otherwise.  Note that disabling

 * of preemption (including disabling irqs) counts as an RCU-sched

 * read-side critical section.  This is useful for debug checks in functions

 * that required that they be called within an RCU-sched read-side

 * critical section.

 *

 * Check debug_lockdep_rcu_enabled() to prevent false positives during boot

 * and while lockdep is disabled.

 *

 * Note that if the CPU is in the idle loop from an RCU point of view (ie:

 * that we are in the section between rcu_idle_enter() and rcu_idle_exit())

 * then rcu_read_lock_held() sets ``*ret`` to false even if the CPU did an

 * rcu_read_lock().  The reason for this is that RCU ignores CPUs that are

 * in such a section, considering these as in extended quiescent state,

 * so such a CPU is effectively never in an RCU read-side critical section

 * regardless of what RCU primitives it invokes.  This state of affairs is

 * required --- we need to keep an RCU-free window in idle where the CPU may

 * possibly enter into low power mode. This way we can notice an extended

 * quiescent state to other CPUs that started a grace period. Otherwise

 * we would delay any grace period as long as we run in the idle task.

 *

 * Similarly, we avoid claiming an RCU read lock held if the current

 * CPU is offline.

/*

 * Should expedited grace-period primitives always fall back to their

 * non-expedited counterparts?  Intended for use within RCU.  Note

 * that if the user specifies both rcu_expedited and rcu_normal, then

 * rcu_normal wins.  (Except during the time period during boot from

 * when the first task is spawned until the rcu_set_runtime_mode()

 * core_initcall() is invoked, at which point everything is expedited.)

/*

 * Should normal grace-period primitives be expedited?  Intended for

 * use within RCU.  Note that this function takes the rcu_expedited

 * sysfs/boot variable and rcu_scheduler_active into account as well

 * as the rcu_expedite_gp() nesting.  So looping on rcu_unexpedite_gp()

 * until rcu_gp_is_expedited() returns false is a -really- bad idea.

/**

 * rcu_expedite_gp - Expedite future RCU grace periods

 *

 * After a call to this function, future calls to synchronize_rcu() and

 * friends act as the corresponding synchronize_rcu_expedited() function

 * had instead been called.

/**

 * rcu_unexpedite_gp - Cancel prior rcu_expedite_gp() invocation

 *

 * Undo a prior call to rcu_expedite_gp().  If all prior calls to

 * rcu_expedite_gp() are undone by a subsequent call to rcu_unexpedite_gp(),

 * and if the rcu_expedited sysfs/boot parameter is not set, then all

 * subsequent calls to synchronize_rcu() and friends will return to

 * their normal non-expedited behavior.

/*

 * Inform RCU of the end of the in-kernel boot sequence.

/*

 * Let rcutorture know when it is OK to turn it up to eleven.

 #ifndef CONFIG_TINY_RCU */

/*

 * Test each non-SRCU synchronous grace-period wait API.  This is

 * useful just after a change in mode for these primitives, and

 * during early boot.

/*

 * Switch to run-time mode once RCU has fully initialized.

 #if !defined(CONFIG_TINY_RCU) || defined(CONFIG_SRCU) */

 PREEMPT_RT implies PREEMPT_RCU */

 PREEMPT_RT makes BH preemptible. */

 Tell lockdep when RCU callbacks are being invoked.

/**

 * rcu_read_lock_held() - might we be in RCU read-side critical section?

 *

 * If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an RCU

 * read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,

 * this assumes we are in an RCU read-side critical section unless it can

 * prove otherwise.  This is useful for debug checks in functions that

 * require that they be called within an RCU read-side critical section.

 *

 * Checks debug_lockdep_rcu_enabled() to prevent false positives during boot

 * and while lockdep is disabled.

 *

 * Note that rcu_read_lock() and the matching rcu_read_unlock() must

 * occur in the same context, for example, it is illegal to invoke

 * rcu_read_unlock() in process context if the matching rcu_read_lock()

 * was invoked from within an irq handler.

 *

 * Note that rcu_read_lock() is disallowed if the CPU is either idle or

 * offline from an RCU perspective, so check for those as well.

/**

 * rcu_read_lock_bh_held() - might we be in RCU-bh read-side critical section?

 *

 * Check for bottom half being disabled, which covers both the

 * CONFIG_PROVE_RCU and not cases.  Note that if someone uses

 * rcu_read_lock_bh(), but then later enables BH, lockdep (if enabled)

 * will show the situation.  This is useful for debug checks in functions

 * that require that they be called within an RCU read-side critical

 * section.

 *

 * Check debug_lockdep_rcu_enabled() to prevent false positives during boot.

 *

 * Note that rcu_read_lock_bh() is disallowed if the CPU is either idle or

 * offline from an RCU perspective, so check for those as well.

 #ifdef CONFIG_DEBUG_LOCK_ALLOC */

/**

 * wakeme_after_rcu() - Callback function to awaken a task after grace period

 * @head: Pointer to rcu_head member within rcu_synchronize structure

 *

 * Awaken the corresponding task now that a grace period has elapsed.

 Initialize and register callbacks for each crcu_array element. */

 Wait for all callbacks to be invoked. */

/**

 * init_rcu_head_on_stack() - initialize on-stack rcu_head for debugobjects

 * @head: pointer to rcu_head structure to be initialized

 *

 * This function informs debugobjects of a new rcu_head structure that

 * has been allocated as an auto variable on the stack.  This function

 * is not required for rcu_head structures that are statically defined or

 * that are dynamically allocated on the heap.  This function has no

 * effect for !CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds.

/**

 * destroy_rcu_head_on_stack() - destroy on-stack rcu_head for debugobjects

 * @head: pointer to rcu_head structure to be initialized

 *

 * This function informs debugobjects that an on-stack rcu_head structure

 * is about to go out of scope.  As with init_rcu_head_on_stack(), this

 * function is not required for rcu_head structures that are statically

 * defined or that are dynamically allocated on the heap.  Also as with

 * init_rcu_head_on_stack(), this function has no effect for

 * !CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds.

 #ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD */

 Get rcutorture access to sched_setaffinity(). */

 !0 = suppress stall warnings.

 #ifdef CONFIG_RCU_STALL_COMMON */

 Suppress boot-time RCU CPU stall warnings and rcutorture writer stall

 warnings.  Also used by rcutorture even if stall warnings are excluded.

 !0 = suppress boot stalls.

/*

 * Early boot self test parameters.

 CONFIG_PROVE_RCU */

/*

 * Print any significant non-default boot-time settings.

 #ifndef CONFIG_TINY_RCU */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Sleepable Read-Copy Update mechanism for mutual exclusion.

 *

 * Copyright (C) IBM Corporation, 2006

 * Copyright (C) Fujitsu, 2012

 *

 * Authors: Paul McKenney <paulmck@linux.ibm.com>

 *	   Lai Jiangshan <laijs@cn.fujitsu.com>

 *

 * For detailed explanation of Read-Copy Update mechanism see -

 *		Documentation/RCU/ *.txt

 *

 Holdoff in nanoseconds for auto-expediting. */

 Overflow-check frequency.  N bits roughly says every 2**N grace periods. */

 Early-boot callback-management, so early that no lock is required! */

 Wrappers for lock acquisition and release, see raw_spin_lock_rcu_node(). */

/*

 * Initialize SRCU combining tree.  Note that statically allocated

 * srcu_struct structures might already have srcu_read_lock() and

 * srcu_read_unlock() running against them.  So if the is_static parameter

 * is set, don't initialize ->srcu_lock_count[] and ->srcu_unlock_count[].

 Initialize geometry if it has not already been initialized. */

 Work out the overall tree geometry. */

 Each pass through this loop initializes one srcu_node structure. */

 Root node, special case. */

 Non-root node. */

	/*

	 * Initialize the per-CPU srcu_data array, which feeds into the

	 * leaves of the srcu_node tree.

/*

 * Initialize non-compile-time initialized fields, including the

 * associated srcu_node and srcu_data structures.  The is_static

 * parameter is passed through to init_srcu_struct_nodes(), and

 * also tells us that ->sda has already been wired up to srcu_data.

 Init done. */

 Don't re-initialize a lock while it is held. */

 #ifdef CONFIG_DEBUG_LOCK_ALLOC */

/**

 * init_srcu_struct - initialize a sleep-RCU structure

 * @ssp: structure to initialize.

 *

 * Must invoke this on a given srcu_struct before passing that srcu_struct

 * to any other function.  Each srcu_struct represents a separate domain

 * of SRCU protection.

 #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */

/*

 * First-use initialization of statically allocated srcu_struct

 * structure.  Wiring up the combining tree is more than can be

 * done with compile-time initialization, so this check is added

 * to each update-side SRCU primitive.  Use ssp->lock, which -is-

 * compile-time initialized, to resolve races involving multiple

 * CPUs trying to garner first-use privileges.

 The smp_load_acquire() pairs with the smp_store_release(). */

^^^*/

 Already initialized. */

/*

 * Returns approximate total of the readers' ->srcu_lock_count[] values

 * for the rank of per-CPU counters specified by idx.

/*

 * Returns approximate total of the readers' ->srcu_unlock_count[] values

 * for the rank of per-CPU counters specified by idx.

/*

 * Return true if the number of pre-existing readers is determined to

 * be zero.

	/*

	 * Make sure that a lock is always counted if the corresponding

	 * unlock is counted. Needs to be a smp_mb() as the read side may

	 * contain a read from a variable that is written to before the

	 * synchronize_srcu() in the write side. In this case smp_mb()s

	 * A and B act like the store buffering pattern.

	 *

	 * This smp_mb() also pairs with smp_mb() C to prevent accesses

	 * after the synchronize_srcu() from being executed before the

	 * grace period ends.

 A */

	/*

	 * If the locks are the same as the unlocks, then there must have

	 * been no readers on this index at some time in between. This does

	 * not mean that there are no more readers, as one could have read

	 * the current index but not have incremented the lock counter yet.

	 *

	 * So suppose that the updater is preempted here for so long

	 * that more than ULONG_MAX non-nested readers come and go in

	 * the meantime.  It turns out that this cannot result in overflow

	 * because if a reader modifies its unlock count after we read it

	 * above, then that reader's next load of ->srcu_idx is guaranteed

	 * to get the new value, which will cause it to operate on the

	 * other bank of counters, where it cannot contribute to the

	 * overflow of these counters.  This means that there is a maximum

	 * of 2*NR_CPUS increments, which cannot overflow given current

	 * systems, especially not on 64-bit systems.

	 *

	 * OK, how about nesting?  This does impose a limit on nesting

	 * of floor(ULONG_MAX/NR_CPUS/2), which should be sufficient,

	 * especially on 64-bit systems.

/**

 * srcu_readers_active - returns true if there are readers. and false

 *                       otherwise

 * @ssp: which srcu_struct to count active readers (holding srcu_read_lock).

 *

 * Note that this is not an atomic primitive, and can therefore suffer

 * severe errors when invoked on an active srcu_struct.  That said, it

 * can be useful as an error check at cleanup time.

/*

 * Return grace-period delay, zero if there are expedited grace

 * periods pending, SRCU_INTERVAL otherwise.

/**

 * cleanup_srcu_struct - deconstruct a sleep-RCU structure

 * @ssp: structure to clean up.

 *

 * Must invoke this after you are finished using a given srcu_struct that

 * was initialized via init_srcu_struct(), else you leak memory.

 Just leak it! */

 Just leak it! */

 Forgot srcu_barrier(), so just leak it! */

 Caller forgot to stop doing call_srcu()? */

/*

 * Counts the new reader in the appropriate per-CPU element of the

 * srcu_struct.

 * Returns an index that must be passed to the matching srcu_read_unlock().

 B */  
/*

 * Removes the count for the old reader from the appropriate per-CPU

 * element of the srcu_struct.  Note that this may well be a different

 * CPU than that which was incremented by the corresponding srcu_read_lock().

 C */  
/*

 * We use an adaptive strategy for synchronize_srcu() and especially for

 * synchronize_srcu_expedited().  We spin for a fixed time period

 * (defined below) to allow SRCU readers to exit their read-side critical

 * sections.  If there are still some readers after a few microseconds,

 * we repeatedly block for 1-millisecond time periods.

/*

 * Start an SRCU grace period.

 Interrupts already disabled. */

 Interrupts remain disabled. */

 Order prior store to ->srcu_gp_seq_needed vs. GP start. */

/*

 * Schedule callback invocation for the specified srcu_data structure,

 * if possible, on the corresponding CPU.

/*

 * Schedule callback invocation for all srcu_data structures associated

 * with the specified srcu_node structure that have callbacks for the

 * just-completed grace period, the one corresponding to idx.  If possible,

 * schedule this invocation on the corresponding CPUs.

/*

 * Note the end of an SRCU grace period.  Initiates callback invocation

 * and starts a new grace period if needed.

 *

 * The ->srcu_cb_mutex acquisition does not protect any data, but

 * instead prevents more than one grace period from starting while we

 * are initiating callback invocation.  This allows the ->srcu_have_cbs[]

 * array to have a finite number of elements.

 Prevent more than one additional grace period. */

 End the current grace period. */

 A new grace period can start at this point.  But only one. */

 Initiate callback invocation as needed. */

 Occasionally prevent srcu_data counter wrap. */

 Callback initiation done, allow grace periods after next. */

 Start a new grace period if needed. */

/*

 * Funnel-locking scheme to scalably mediate many concurrent expedited

 * grace-period requests.  This function is invoked for the first known

 * expedited request for a grace period that has already been requested,

 * but without expediting.  To start a completely new grace period,

 * whether expedited or not, use srcu_funnel_gp_start() instead.

/*

 * Funnel-locking scheme to scalably mediate many concurrent grace-period

 * requests.  The winner has to do the work of actually starting grace

 * period s.  Losers must either ensure that their desired grace-period

 * number is recorded on at least their leaf srcu_node structure, or they

 * must take steps to invoke their own callbacks.

 *

 * Note that this function also does the work of srcu_funnel_exp_start(),

 * in some cases by directly invoking it.

 Each pass through the loop does one level of the srcu_node tree. */

 GP already done and CBs recorded. */

 Top of tree, must ensure the grace period will be started. */

		/*

		 * Record need for grace period s.  Pair with load

		 * acquire setting up for initialization.

^^^*/

 If grace period not already done and none in progress, start it. */

/*

 * Wait until all readers counted by array index idx complete, but

 * loop an additional time if there is an expedited grace period pending.

 * The caller must ensure that ->srcu_idx is not changed while checking.

/*

 * Increment the ->srcu_idx counter so that future SRCU readers will

 * use the other rank of the ->srcu_(un)lock_count[] arrays.  This allows

 * us to wait for pre-existing readers in a starvation-free manner.

	/*

	 * Ensure that if this updater saw a given reader's increment

	 * from __srcu_read_lock(), that reader was using an old value

	 * of ->srcu_idx.  Also ensure that if a given reader sees the

	 * new value of ->srcu_idx, this updater's earlier scans cannot

	 * have seen that reader's increments (which is OK, because this

	 * grace period need not wait on that reader).

 E */  
	/*

	 * Ensure that if the updater misses an __srcu_read_unlock()

	 * increment, that task's next __srcu_read_lock() will see the

	 * above counter update.  Note that both this memory barrier

	 * and the one in srcu_readers_active_idx_check() provide the

	 * guarantee for __srcu_read_lock().

 D */  
/*

 * If SRCU is likely idle, return true, otherwise return false.

 *

 * Note that it is OK for several current from-idle requests for a new

 * grace period from idle to specify expediting because they will all end

 * up requesting the same grace period anyhow.  So no loss.

 *

 * Note also that if any CPU (including the current one) is still invoking

 * callbacks, this function will nevertheless say "idle".  This is not

 * ideal, but the overhead of checking all CPUs' callback lists is even

 * less ideal, especially on large systems.  Furthermore, the wakeup

 * can happen before the callback is fully removed, so we have no choice

 * but to accept this type of error.

 *

 * This function is also subject to counter-wrap errors, but let's face

 * it, if this function was preempted for enough time for the counters

 * to wrap, it really doesn't matter whether or not we expedite the grace

 * period.  The extra overhead of a needlessly expedited grace period is

 * negligible when amortized over that time period, and the extra latency

 * of a needlessly non-expedited grace period is similarly negligible.

 If the local srcu_data structure has callbacks, not idle.  */

 Callbacks already present, so not idle. */

	/*

	 * No local callbacks, so probabilistically probe global state.

	 * Exact information would require acquiring locks, which would

	 * kill scalability, hence the probabilistic nature of the probe.

 First, see if enough time has passed since the last GP. */

 Too soon after last GP. */

 Next, check for probable idleness. */

 Order ->srcu_gp_seq with ->srcu_gp_seq_needed. */

 Grace period in progress, so not idle. */

 Order ->srcu_gp_seq with prior access. */

 GP # changed, so not idle. */

 With reasonable probability, idle! */

/*

 * SRCU callback function to leak a callback.

/*

 * Start an SRCU grace period, and also queue the callback if non-NULL.

/*

 * Enqueue an SRCU callback on the srcu_data structure associated with

 * the current CPU and the specified srcu_struct structure, initiating

 * grace-period processing if it is not already running.

 *

 * Note that all CPUs must agree that the grace period extended beyond

 * all pre-existing SRCU read-side critical section.  On systems with

 * more than one CPU, this means that when "func()" is invoked, each CPU

 * is guaranteed to have executed a full memory barrier since the end of

 * its last corresponding SRCU read-side critical section whose beginning

 * preceded the call to call_srcu().  It also means that each CPU executing

 * an SRCU read-side critical section that continues beyond the start of

 * "func()" must have executed a memory barrier after the call_srcu()

 * but before the beginning of that SRCU read-side critical section.

 * Note that these guarantees include CPUs that are offline, idle, or

 * executing in user mode, as well as CPUs that are executing in the kernel.

 *

 * Furthermore, if CPU A invoked call_srcu() and CPU B invoked the

 * resulting SRCU callback function "func()", then both CPU A and CPU

 * B are guaranteed to execute a full memory barrier during the time

 * interval between the call to call_srcu() and the invocation of "func()".

 * This guarantee applies even if CPU A and CPU B are the same CPU (but

 * again only if the system has more than one CPU).

 *

 * Of course, these guarantees apply only for invocations of call_srcu(),

 * srcu_read_lock(), and srcu_read_unlock() that are all passed the same

 * srcu_struct structure.

 Probable double call_srcu(), so leak the callback. */

/**

 * call_srcu() - Queue a callback for invocation after an SRCU grace period

 * @ssp: srcu_struct in queue the callback

 * @rhp: structure to be used for queueing the SRCU callback.

 * @func: function to be invoked after the SRCU grace period

 *

 * The callback function will be invoked some time after a full SRCU

 * grace period elapses, in other words after all pre-existing SRCU

 * read-side critical sections have completed.  However, the callback

 * function might well execute concurrently with other SRCU read-side

 * critical sections that started after call_srcu() was invoked.  SRCU

 * read-side critical sections are delimited by srcu_read_lock() and

 * srcu_read_unlock(), and may be nested.

 *

 * The callback will be invoked from process context, but must nevertheless

 * be fast and must not block.

/*

 * Helper function for synchronize_srcu() and synchronize_srcu_expedited().

	/*

	 * Make sure that later code is ordered after the SRCU grace

	 * period.  This pairs with the spin_lock_irq_rcu_node()

	 * in srcu_invoke_callbacks().  Unlike Tree RCU, this is needed

	 * because the current CPU might have been totally uninvolved with

	 * (and thus unordered against) that grace period.

/**

 * synchronize_srcu_expedited - Brute-force SRCU grace period

 * @ssp: srcu_struct with which to synchronize.

 *

 * Wait for an SRCU grace period to elapse, but be more aggressive about

 * spinning rather than blocking when waiting.

 *

 * Note that synchronize_srcu_expedited() has the same deadlock and

 * memory-ordering properties as does synchronize_srcu().

/**

 * synchronize_srcu - wait for prior SRCU read-side critical-section completion

 * @ssp: srcu_struct with which to synchronize.

 *

 * Wait for the count to drain to zero of both indexes. To avoid the

 * possible starvation of synchronize_srcu(), it waits for the count of

 * the index=((->srcu_idx & 1) ^ 1) to drain to zero at first,

 * and then flip the srcu_idx and wait for the count of the other index.

 *

 * Can block; must be called from process context.

 *

 * Note that it is illegal to call synchronize_srcu() from the corresponding

 * SRCU read-side critical section; doing so will result in deadlock.

 * However, it is perfectly legal to call synchronize_srcu() on one

 * srcu_struct from some other srcu_struct's read-side critical section,

 * as long as the resulting graph of srcu_structs is acyclic.

 *

 * There are memory-ordering constraints implied by synchronize_srcu().

 * On systems with more than one CPU, when synchronize_srcu() returns,

 * each CPU is guaranteed to have executed a full memory barrier since

 * the end of its last corresponding SRCU read-side critical section

 * whose beginning preceded the call to synchronize_srcu().  In addition,

 * each CPU having an SRCU read-side critical section that extends beyond

 * the return from synchronize_srcu() is guaranteed to have executed a

 * full memory barrier after the beginning of synchronize_srcu() and before

 * the beginning of that SRCU read-side critical section.  Note that these

 * guarantees include CPUs that are offline, idle, or executing in user mode,

 * as well as CPUs that are executing in the kernel.

 *

 * Furthermore, if CPU A invoked synchronize_srcu(), which returned

 * to its caller on CPU B, then both CPU A and CPU B are guaranteed

 * to have executed a full memory barrier during the execution of

 * synchronize_srcu().  This guarantee applies even if CPU A and CPU B

 * are the same CPU, but again only if the system has more than one CPU.

 *

 * Of course, these memory-ordering guarantees apply only when

 * synchronize_srcu(), srcu_read_lock(), and srcu_read_unlock() are

 * passed the same srcu_struct structure.

 *

 * Implementation of these memory-ordering guarantees is similar to

 * that of synchronize_rcu().

 *

 * If SRCU is likely idle, expedite the first request.  This semantic

 * was provided by Classic SRCU, and is relied upon by its users, so TREE

 * SRCU must also provide it.  Note that detecting idleness is heuristic

 * and subject to both false positives and negatives.

/**

 * get_state_synchronize_srcu - Provide an end-of-grace-period cookie

 * @ssp: srcu_struct to provide cookie for.

 *

 * This function returns a cookie that can be passed to

 * poll_state_synchronize_srcu(), which will return true if a full grace

 * period has elapsed in the meantime.  It is the caller's responsibility

 * to make sure that grace period happens, for example, by invoking

 * call_srcu() after return from get_state_synchronize_srcu().

 Any prior manipulation of SRCU-protected data must happen

 before the load from ->srcu_gp_seq.

/**

 * start_poll_synchronize_srcu - Provide cookie and start grace period

 * @ssp: srcu_struct to provide cookie for.

 *

 * This function returns a cookie that can be passed to

 * poll_state_synchronize_srcu(), which will return true if a full grace

 * period has elapsed in the meantime.  Unlike get_state_synchronize_srcu(),

 * this function also ensures that any needed SRCU grace period will be

 * started.  This convenience does come at a cost in terms of CPU overhead.

/**

 * poll_state_synchronize_srcu - Has cookie's grace period ended?

 * @ssp: srcu_struct to provide cookie for.

 * @cookie: Return value from get_state_synchronize_srcu() or start_poll_synchronize_srcu().

 *

 * This function takes the cookie that was returned from either

 * get_state_synchronize_srcu() or start_poll_synchronize_srcu(), and

 * returns @true if an SRCU grace period elapsed since the time that the

 * cookie was created.

 *

 * Because cookies are finite in size, wrapping/overflow is possible.

 * This is more pronounced on 32-bit systems where cookies are 32 bits,

 * where in theory wrapping could happen in about 14 hours assuming

 * 25-microsecond expedited SRCU grace periods.  However, a more likely

 * overflow lower bound is on the order of 24 days in the case of

 * one-millisecond SRCU grace periods.  Of course, wrapping in a 64-bit

 * system requires geologic timespans, as in more than seven million years

 * even for expedited SRCU grace periods.

 *

 * Wrapping/overflow is much more of an issue for CONFIG_SMP=n systems

 * that also have CONFIG_PREEMPTION=n, which selects Tiny SRCU.  This uses

 * a 16-bit cookie, which rcutorture routinely wraps in a matter of a

 * few minutes.  If this proves to be a problem, this counter will be

 * expanded to the same size as for Tree SRCU.

 Ensure that the end of the SRCU grace period happens before

 any subsequent code that the caller might execute.

 ^^^

/*

 * Callback function for srcu_barrier() use.

/**

 * srcu_barrier - Wait until all in-flight call_srcu() callbacks complete.

 * @ssp: srcu_struct on which to wait for in-flight callbacks.

 Force ordering following return. */

 Someone else did our work for us. */

 Initial count prevents reaching zero until all CBs are posted. */

	/*

	 * Each pass through this loop enqueues a callback, but only

	 * on CPUs already having callbacks enqueued.  Note that if

	 * a CPU already has callbacks enqueue, it must have already

	 * registered the need for a future grace period, so all we

	 * need do is enqueue a callback that will use the same

	 * grace period as the last callback already in the queue.

 Remove the initial count, at which point reaching zero can happen. */

/**

 * srcu_batches_completed - return batches completed.

 * @ssp: srcu_struct on which to report batch completion.

 *

 * Report the number of batches, correlated with, but not necessarily

 * precisely the same as, the number of grace periods that have elapsed.

/*

 * Core SRCU state machine.  Push state bits of ->srcu_gp_seq

 * to SRCU_STATE_SCAN2, and invoke srcu_gp_end() when scan has

 * completed in that state.

	/*

	 * Because readers might be delayed for an extended period after

	 * fetching ->srcu_idx for their index, at any point in time there

	 * might well be readers using both idx=0 and idx=1.  We therefore

	 * need to wait for readers to clear from both index values before

	 * invoking a callback.

	 *

	 * The load-acquire ensures that we see the accesses performed

	 * by the prior grace period.

 ^^^ */

 Someone else started the grace period. */

 readers present, retry later. */

		/*

		 * SRCU read-side critical sections are normally short,

		 * so check at least twice in quick succession after a flip.

 readers present, retry later. */

 Releases ->srcu_gp_mutex. */

/*

 * Invoke a limited number of SRCU callbacks that have passed through

 * their grace period.  If there are more to do, SRCU will reschedule

 * the workqueue.  Note that needed memory barriers have been executed

 * in this task's context by srcu_readers_active_idx_check().

 Someone else on the job or nothing to do. */

 We are on the job!  Extract and invoke ready callbacks. */

	/*

	 * Update counts, accelerate new callbacks, and if needed,

	 * schedule another round of callback invocation.

/*

 * Finished one round of SRCU grace period.  Start another if there are

 * more SRCU callbacks queued, otherwise put SRCU into not-running state.

 All requests fulfilled, time to go idle. */

 Outstanding request and no GP.  Start one. */

/*

 * This is the work-queue function that handles SRCU grace periods.

		/*

		 * Make sure that a lock is always counted if the corresponding

		 * unlock is counted.

	/*

	 * Once that is set, call_srcu() can follow the normal path and

	 * queue delayed work. This must follow RCU workqueues creation

	 * and timers initialization.

 Initialize any global-scope srcu_struct structures used by this module. */

 Clean up any global-scope srcu_struct structures used by this module. */

 Handle one module, either coming or going. */

 #ifdef CONFIG_MODULES */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.

 *

 * Copyright IBM Corporation, 2008

 *

 * Author: Paul E. McKenney <paulmck@linux.ibm.com>

 *

 * For detailed explanation of Read-Copy Update mechanism see -

 *		Documentation/RCU

 Global control variables for rcupdate callback mechanism. */

 List of pending callbacks (CBs). */

 ->next pointer of last "done" CB. */

 ->next pointer of last CB. */

 Grace-period counter. */

 Definition for rcupdate control block. */

 Record an rcu quiescent state.  */

/*

 * Check to see if the scheduling-clock interrupt came from an extended

 * quiescent state, and, if so, tell RCU about it.  This function must

 * be called from hardirq context.  It is normally called from the

 * scheduling-clock interrupt.

/*

 * Reclaim the specified callback, either by invoking it for non-kfree cases or

 * freeing it directly (for kfree). Return true if kfreeing, false otherwise.

 Invoke the RCU callbacks whose grace period has elapsed.  */

 Move the ready-to-invoke callbacks to a local list. */

 No callbacks ready, so just leave. */

 Invoke the callbacks on the local list. */

/*

 * Wait for a grace period to elapse.  But it is illegal to invoke

 * synchronize_rcu() from within an RCU read-side critical section.

 * Therefore, any legal call to synchronize_rcu() is a quiescent

 * state, and so on a UP system, synchronize_rcu() need do nothing.

 * (But Lai Jiangshan points out the benefits of doing might_sleep()

 * to reduce latency.)

 *

 * Cool, huh?  (Due to Josh Triplett.)

/*

 * Post an RCU callback to be invoked after the end of an RCU grace

 * period.  But since we have but one CPU, that would be after any

 * quiescent state.

 force scheduling for rcu_qs() */

/*

 * Return a grace-period-counter "cookie".  For more information,

 * see the Tree RCU header comment.

/*

 * Return a grace-period-counter "cookie" and ensure that a future grace

 * period completes.  For more information, see the Tree RCU header comment.

 force scheduling for rcu_qs() */

/*

 * Return true if the grace period corresponding to oldstate has completed

 * and false otherwise.  For more information, see the Tree RCU header

 * comment.

 SPDX-License-Identifier: GPL-2.0+

/*

 * Read-Copy Update mechanism for mutual exclusion (tree-based version)

 *

 * Copyright IBM Corporation, 2008

 *

 * Authors: Dipankar Sarma <dipankar@in.ibm.com>

 *	    Manfred Spraul <manfred@colorfullife.com>

 *	    Paul E. McKenney <paulmck@linux.ibm.com>

 *

 * Based on the original work by Paul McKenney <paulmck@linux.ibm.com>

 * and inputs from Rusty Russell, Andrea Arcangeli and Andi Kleen.

 *

 * For detailed explanation of Read-Copy Update mechanism see -

 *	Documentation/RCU

 Data structures. */

 Dump rcu_node combining tree at boot to verify correct setup. */

 By default, use RCU_SOFTIRQ instead of rcuc kthreads. */

 Control rcu_node-tree auto-balancing at boot time. */

 Increase (but not decrease) the RCU_FANOUT_LEAF at boot time. */

 Number of rcu_nodes at specified level. */

 Total # rcu_nodes in use. */

/*

 * The rcu_scheduler_active variable is initialized to the value

 * RCU_SCHEDULER_INACTIVE and transitions RCU_SCHEDULER_INIT just before the

 * first task is spawned.  So when this variable is RCU_SCHEDULER_INACTIVE,

 * RCU can assume that there is but one task, allowing RCU to (for example)

 * optimize synchronize_rcu() to a simple barrier().  When this variable

 * is RCU_SCHEDULER_INIT, RCU must actually do all the hard work required

 * to detect real grace periods.  This variable is also used to suppress

 * boot-time false positives from lockdep-RCU error checking.  Finally, it

 * transitions from RCU_SCHEDULER_INIT to RCU_SCHEDULER_RUNNING after RCU

 * is fully initialized, including all of its kthreads having been spawned.

/*

 * The rcu_scheduler_fully_active variable transitions from zero to one

 * during the early_initcall() processing, which is after the scheduler

 * is capable of creating new tasks.  So RCU processing (for example,

 * creating tasks for RCU priority boosting) must be delayed until after

 * rcu_scheduler_fully_active transitions from zero to one.  We also

 * currently delay invocation of any RCU callbacks until after this point.

 *

 * It might later prove better for people registering RCU callbacks during

 * early boot to take responsibility for these callbacks, but one step at

 * a time.

 rcuc/rcub kthread realtime priority */

 Delay in jiffies for grace-period initialization delays, debug only. */

 Add delay to rcu_read_unlock() for strict grace periods.

/*

 * This rcu parameter is runtime-read-only. It reflects

 * a minimum allowed number of objects which can be cached

 * per-CPU. Object size is equal to one page. This value

 * can be changed at boot time.

 A page shrinker can ask for pages to be freed to make them

 available for other parts of the system. This usually happens

 under low memory conditions, and in that case we should also

 defer page-cache filling for a short time period.



 The default value is 5 seconds, which is long enough to reduce

 interference with the shrinker while it asks other systems to

 drain their caches.

 Retrieve RCU kthreads priority for rcutorture */

/*

 * Number of grace periods between delays, normalized by the duration of

 * the delay.  The longer the delay, the more the grace periods between

 * each delay.  The reason for this normalization is that it means that,

 * for non-zero delays, the overall slowdown of grace periods is constant

 * regardless of the duration of the delay.  This arrangement balances

 * the need for long delays to increase some race probabilities with the

 * need for fast grace periods to increase other race probabilities.

 Number of grace periods between delays for debugging. */

/*

 * Compute the mask of online CPUs for the specified rcu_node structure.

 * This will not be stable unless the rcu_node structure's ->lock is

 * held, but the bit corresponding to the current CPU will be stable

 * in most contexts.

/*

 * Return true if an RCU grace period is in progress.  The READ_ONCE()s

 * permit this function to be invoked without holding the root rcu_node

 * structure's ->lock, but of course results can be subject to change.

/*

 * Return the number of callbacks queued on the specified CPU.

 * Handles both the nocbs and normal cases.

/*

 * Increment the current CPU's rcu_data structure's ->dynticks field

 * with ordering.  Return the new value.

/*

 * Record entry into an extended quiescent state.  This is only to be

 * called when not already in an extended quiescent state, that is,

 * RCU is watching prior to the call to this function and is no longer

 * watching upon return.

	/*

	 * CPUs seeing atomic_add_return() must see prior RCU read-side

	 * critical sections, and we also must force ordering with the

	 * next idle sojourn.

 Before ->dynticks update!

 RCU is no longer watching.  Better be in extended quiescent state!

/*

 * Record exit from an extended quiescent state.  This is only to be

 * called from an extended quiescent state, that is, RCU is not watching

 * prior to the call to this function and is watching upon return.

	/*

	 * CPUs seeing atomic_add_return() must see prior idle sojourns,

	 * and we also must force ordering with the next RCU read-side

	 * critical section.

 RCU is now watching.  Better not be in an extended quiescent state!

 After ->dynticks update!

/*

 * Reset the current CPU's ->dynticks counter to indicate that the

 * newly onlined CPU is no longer in an extended quiescent state.

 * This will either leave the counter unchanged, or increment it

 * to the next non-quiescent value.

 *

 * The non-atomic test/increment sequence works because the upper bits

 * of the ->dynticks counter are manipulated only by the corresponding CPU,

 * or when the corresponding CPU is offline.

/*

 * Is the current CPU in an extended quiescent state?

 *

 * No ordering, as we are sampling CPU-local information.

/*

 * Snapshot the ->dynticks counter with full ordering so as to allow

 * stable comparison of this counter with past and future snapshots.

 Fundamental RCU ordering guarantee.

/*

 * Return true if the snapshot returned from rcu_dynticks_snap()

 * indicates that RCU is in an extended quiescent state.

 Return true if the specified CPU is currently idle from an RCU viewpoint.  */

/*

 * Return true if the CPU corresponding to the specified rcu_data

 * structure has spent some time in an extended quiescent state since

 * rcu_dynticks_snap() returned the specified snapshot.

/*

 * Return true if the referenced integer is zero while the specified

 * CPU remains within a single extended quiescent state.

 If not quiescent, force back to earlier extended quiescent state.

 Order ->dynticks and *vp reads.

 Non-zero, so report failure;

 Order *vp read and ->dynticks re-read.

 If still in the same extended quiescent state, we are good!

/*

 * Let the RCU core know that this CPU has gone through the scheduler,

 * which is a quiescent state.  This is called when the need for a

 * quiescent state is urgent, so we burn an atomic operation and full

 * memory barriers to let the RCU core know about it, regardless of what

 * this CPU might (or might not) do in the near future.

 *

 * We inform the RCU core by emulating a zero-duration dyntick-idle period.

 *

 * The caller must have disabled interrupts and must not be idle.

 It is illegal to call this from idle state. */

/**

 * rcu_is_cpu_rrupt_from_idle - see if 'interrupted' from idle

 *

 * If the current CPU is idle and running at a first-level (not nested)

 * interrupt, or directly, from idle, return true.

 *

 * The caller must have at least disabled IRQs.

	/*

	 * Usually called from the tick; but also used from smp_function_call()

	 * for expedited grace periods. This latter can result in running from

	 * the idle task, instead of an actual IPI.

 Check for counter underflows */

 Are we at first interrupt nesting level? */

	/*

	 * If we're not in an interrupt, we must be in the idle task!

 Does CPU appear to be idle from an RCU standpoint? */

 Maximum callbacks per rcu_do_batch ...

 ... even during callback flood.

 If this many pending, ignore blimit.

 Once only this many pending, use blimit.

 If this many pending, hammer QS.

 No pre-initialization lock acquisitions!

 Force an exit from rcu_do_batch() after 3 milliseconds. */

/*

 * How long the grace period must be before we start recruiting

 * quiescent-state help from rcu_note_context_switch().

 See adjust_jiffies_till_sched_qs(). */

 Display only! */

/*

 * Make sure that we give the grace-period kthread time to detect any

 * idle CPUs before taking active measures to force quiescent states.

 * However, don't go below 100 milliseconds, adjusted upwards for really

 * large systems.

 If jiffies_till_sched_qs was specified, respect the request. */

 Otherwise, set to third fqs scan, but bound below on large system. */

/*

 * Return the number of RCU GPs completed thus far for debug & stats.

/*

 * Return the number of RCU expedited batches completed thus far for

 * debug & stats.  Odd numbers mean that a batch is in progress, even

 * numbers mean idle.  The value returned will thus be roughly double

 * the cumulative batches since boot.

/*

 * Return the root node of the rcu_state structure.

/*

 * Send along grace-period-related data for rcutorture diagnostics.

/*

 * Enter an RCU extended quiescent state, which can be either the

 * idle loop or adaptive-tickless usermode execution.

 *

 * We crowbar the ->dynticks_nmi_nesting field to zero to allow for

 * the possibility of usermode upcalls having messed up our count

 * of interrupt nesting level during the prior busy period.

 RCU will still be watching, so just do accounting and leave.

 instrumentation for the noinstr rcu_dynticks_eqs_enter()

 Avoid irq-access tearing. */

 RCU is watching here ...

 ... but is no longer watching here.

/**

 * rcu_idle_enter - inform RCU that current CPU is entering idle

 *

 * Enter idle mode, in other words, -leave- the mode in which RCU

 * read-side critical sections can occur.  (Though RCU read-side

 * critical sections can occur in irq handlers in idle, a possibility

 * handled by irq_enter() and irq_exit().)

 *

 * If you add or remove a call to rcu_idle_enter(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

/*

 * An empty function that will trigger a reschedule on

 * IRQ tail once IRQs get re-enabled on userspace/guest resume.

/*

 * If either:

 *

 * 1) the task is about to enter in guest mode and $ARCH doesn't support KVM generic work

 * 2) the task is about to enter in user mode and $ARCH doesn't support generic entry.

 *

 * In these cases the late RCU wake ups aren't supported in the resched loops and our

 * last resort is to fire a local irq_work that will trigger a reschedule once IRQs

 * get re-enabled again.

/**

 * rcu_user_enter - inform RCU that we are resuming userspace.

 *

 * Enter RCU idle mode right before resuming userspace.  No use of RCU

 * is permitted between this call and rcu_user_exit(). This way the

 * CPU doesn't need to maintain the tick for RCU maintenance purposes

 * when the CPU runs in userspace.

 *

 * If you add or remove a call to rcu_user_enter(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

	/*

	 * Other than generic entry implementation, we may be past the last

	 * rescheduling opportunity in the entry code. Trigger a self IPI

	 * that will fire and reschedule once we resume in user/guest mode.

 CONFIG_NO_HZ_FULL */

/**

 * rcu_nmi_exit - inform RCU of exit from NMI context

 *

 * If we are returning from the outermost NMI handler that interrupted an

 * RCU-idle period, update rdp->dynticks and rdp->dynticks_nmi_nesting

 * to let the RCU grace-period handling know that the CPU is back to

 * being RCU-idle.

 *

 * If you add or remove a call to rcu_nmi_exit(), be sure to test

 * with CONFIG_RCU_EQS_DEBUG=y.

	/*

	 * Check for ->dynticks_nmi_nesting underflow and bad ->dynticks.

	 * (We are exiting an NMI handler, so RCU better be paying attention

	 * to us!)

	/*

	 * If the nesting level is not 1, the CPU wasn't RCU-idle, so

	 * leave it in non-RCU-idle state.

 No store tearing. */

 This NMI interrupted an RCU-idle CPU, restore RCU-idleness. */

 Avoid store tearing. */

 instrumentation for the noinstr rcu_dynticks_eqs_enter()

 RCU is watching here ...

 ... but is no longer watching here.

/**

 * rcu_irq_exit - inform RCU that current CPU is exiting irq towards idle

 *

 * Exit from an interrupt handler, which might possibly result in entering

 * idle mode, in other words, leaving the mode in which read-side critical

 * sections can occur.  The caller must have disabled interrupts.

 *

 * This code assumes that the idle loop never does anything that might

 * result in unbalanced calls to irq_enter() and irq_exit().  If your

 * architecture's idle loop violates this assumption, RCU will give you what

 * you deserve, good and hard.  But very infrequently and irreproducibly.

 *

 * Use things like work queues to work around this limitation.

 *

 * You have been warned.

 *

 * If you add or remove a call to rcu_irq_exit(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

/**

 * rcu_irq_exit_check_preempt - Validate that scheduling is possible

 #ifdef CONFIG_PROVE_RCU */

/*

 * Wrapper for rcu_irq_exit() where interrupts are enabled.

 *

 * If you add or remove a call to rcu_irq_exit_irqson(), be sure to test

 * with CONFIG_RCU_EQS_DEBUG=y.

/*

 * Exit an RCU extended quiescent state, which can be either the

 * idle loop or adaptive-tickless usermode execution.

 *

 * We crowbar the ->dynticks_nmi_nesting field to DYNTICK_IRQ_NONIDLE to

 * allow for the possibility of usermode upcalls messing up our count of

 * interrupt nesting level during the busy period that is just now starting.

 RCU was already watching, so just do accounting and leave.

 RCU is not watching here ...

 ... but is watching here.

 instrumentation for the noinstr rcu_dynticks_eqs_exit()

/**

 * rcu_idle_exit - inform RCU that current CPU is leaving idle

 *

 * Exit idle mode, in other words, -enter- the mode in which RCU

 * read-side critical sections can occur.

 *

 * If you add or remove a call to rcu_idle_exit(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

/**

 * rcu_user_exit - inform RCU that we are exiting userspace.

 *

 * Exit RCU idle mode while entering the kernel because it can

 * run a RCU read side critical section anytime.

 *

 * If you add or remove a call to rcu_user_exit(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

/**

 * __rcu_irq_enter_check_tick - Enable scheduler tick on CPU if RCU needs it.

 *

 * The scheduler tick is not normally enabled when CPUs enter the kernel

 * from nohz_full userspace execution.  After all, nohz_full userspace

 * execution is an RCU quiescent state and the time executing in the kernel

 * is quite short.  Except of course when it isn't.  And it is not hard to

 * cause a large system to spend tens of seconds or even minutes looping

 * in the kernel, which can cause a number of problems, include RCU CPU

 * stall warnings.

 *

 * Therefore, if a nohz_full CPU fails to report a quiescent state

 * in a timely manner, the RCU grace-period kthread sets that CPU's

 * ->rcu_urgent_qs flag with the expectation that the next interrupt or

 * exception will invoke this function, which will turn on the scheduler

 * tick, which will enable RCU to detect that CPU's quiescent states,

 * for example, due to cond_resched() calls in CONFIG_PREEMPT=n kernels.

 * The tick will be disabled once a quiescent state is reported for

 * this CPU.

 *

 * Of course, in carefully tuned systems, there might never be an

 * interrupt or exception.  In that case, the RCU grace-period kthread

 * will eventually cause one to happen.  However, in less carefully

 * controlled environments, this function allows RCU to get what it

 * needs without creating otherwise useless interruptions.

 If we're here from NMI there's nothing to do.

 RCU doesn't need nohz_full help from this CPU, or it is

 already getting that help.

 We get here only when not in an extended quiescent state and

 from interrupts (as opposed to NMIs).  Therefore, (1) RCU is

 already watching and (2) The fact that we are in an interrupt

 handler and that the rcu_node lock is an irq-disabled lock

 prevents self-deadlock.  So we can safely recheck under the lock.

 Note that the nohz_full state currently cannot change.

 A nohz_full CPU is in the kernel and RCU needs a

 quiescent state.  Turn on the tick!

 CONFIG_NO_HZ_FULL */

/**

 * rcu_nmi_enter - inform RCU of entry to NMI context

 *

 * If the CPU was idle from RCU's viewpoint, update rdp->dynticks and

 * rdp->dynticks_nmi_nesting to let the RCU grace-period handling know

 * that the CPU is active.  This implementation permits nested NMIs, as

 * long as the nesting level does not overflow an int.  (You will probably

 * run out of stack space first.)

 *

 * If you add or remove a call to rcu_nmi_enter(), be sure to test

 * with CONFIG_RCU_EQS_DEBUG=y.

 Complain about underflow. */

	/*

	 * If idle from RCU viewpoint, atomically increment ->dynticks

	 * to mark non-idle and increment ->dynticks_nmi_nesting by one.

	 * Otherwise, increment ->dynticks_nmi_nesting by two.  This means

	 * if ->dynticks_nmi_nesting is equal to one, we are guaranteed

	 * to be in the outermost NMI handler that interrupted an RCU-idle

	 * period (observation due to Andy Lutomirski).

 RCU is not watching here ...

 ... but is watching here.

 instrumentation for the noinstr rcu_dynticks_curr_cpu_in_eqs()

 instrumentation for the noinstr rcu_dynticks_eqs_exit()

 Prevent store tearing. */

/**

 * rcu_irq_enter - inform RCU that current CPU is entering irq away from idle

 *

 * Enter an interrupt handler, which might possibly result in exiting

 * idle mode, in other words, entering the mode in which read-side critical

 * sections can occur.  The caller must have disabled interrupts.

 *

 * Note that the Linux kernel is fully capable of entering an interrupt

 * handler that it never exits, for example when doing upcalls to user mode!

 * This code assumes that the idle loop never does upcalls to user mode.

 * If your architecture's idle loop does do upcalls to user mode (or does

 * anything else that results in unbalanced calls to the irq_enter() and

 * irq_exit() functions), RCU will give you what you deserve, good and hard.

 * But very infrequently and irreproducibly.

 *

 * Use things like work queues to work around this limitation.

 *

 * You have been warned.

 *

 * If you add or remove a call to rcu_irq_enter(), be sure to test with

 * CONFIG_RCU_EQS_DEBUG=y.

/*

 * Wrapper for rcu_irq_enter() where interrupts are enabled.

 *

 * If you add or remove a call to rcu_irq_enter_irqson(), be sure to test

 * with CONFIG_RCU_EQS_DEBUG=y.

/*

 * If any sort of urgency was applied to the current CPU (for example,

 * the scheduler-clock interrupt was enabled on a nohz_full CPU) in order

 * to get to a quiescent state, disable it.

/**

 * rcu_is_watching - see if RCU thinks that the current CPU is not idle

 *

 * Return true if RCU is watching the running CPU, which means that this

 * CPU can safely enter RCU read-side critical sections.  In other words,

 * if the current CPU is not in its idle loop or is in an interrupt or

 * NMI handler, return true.

 *

 * Make notrace because it can be called by the internal functions of

 * ftrace, and making this notrace removes unnecessary recursion calls.

/*

 * If a holdout task is actually running, request an urgent quiescent

 * state from its CPU.  This is unsynchronized, so migrations can cause

 * the request to go to the wrong CPU.  Which is OK, all that will happen

 * is that the CPU's next context switch will be a bit slower and next

 * time around this task will generate another request.

 This task is not running on that CPU. */

/*

 * Is the current CPU online as far as RCU is concerned?

 *

 * Disable preemption to avoid false positives that could otherwise

 * happen due to the current CPU number being sampled, this task being

 * preempted, its old CPU being taken offline, resuming on some other CPU,

 * then determining that its old CPU is now offline.

 *

 * Disable checking if in an NMI handler because we cannot safely

 * report errors from NMI handlers anyway.  In addition, it is OK to use

 * RCU on an offline processor during initial boot, hence the check for

 * rcu_scheduler_fully_active.

 #if defined(CONFIG_PROVE_RCU) && defined(CONFIG_HOTPLUG_CPU) */

/*

 * When trying to report a quiescent state on behalf of some other CPU,

 * it is our responsibility to check for and handle potential overflow

 * of the rcu_node ->gp_seq counter with respect to the rcu_data counters.

 * After all, the CPU might be in deep idle state, and thus executing no

 * code whatsoever.

/*

 * Snapshot the specified CPU's dynticks counter so that we can later

 * credit them with an implicit quiescent state.  Return 1 if this CPU

 * is in dynticks idle mode, which is an extended quiescent state.

/*

 * Return true if the specified CPU has passed through a quiescent

 * state by virtue of being in or having passed through an dynticks

 * idle state since the last call to dyntick_save_progress_counter()

 * for this same CPU, or by virtue of having been offline.

	/*

	 * If the CPU passed through or entered a dynticks idle phase with

	 * no active irq/NMI handlers, then we can safely pretend that the CPU

	 * already acknowledged the request to pass through a quiescent

	 * state.  Either way, that CPU cannot possibly be in an RCU

	 * read-side critical section that started before the beginning

	 * of the current RCU grace period.

	/*

	 * Complain if a CPU that is considered to be offline from RCU's

	 * perspective has not yet reported a quiescent state.  After all,

	 * the offline CPU should have reported a quiescent state during

	 * the CPU-offline process, or, failing that, by rcu_gp_init()

	 * if it ran concurrently with either the CPU going offline or the

	 * last task on a leaf rcu_node structure exiting its RCU read-side

	 * critical section while all CPUs corresponding to that structure

	 * are offline.  This added warning detects bugs in any of these

	 * code paths.

	 *

	 * The rcu_node structure's ->lock is held here, which excludes

	 * the relevant portions the CPU-hotplug code, the grace-period

	 * initialization code, and the rcu_read_unlock() code paths.

	 *

	 * For more detail, please refer to the "Hotplug CPU" section

	 * of RCU's Requirements documentation.

 Break things loose after complaining. */

	/*

	 * A CPU running for an extended time within the kernel can

	 * delay RCU grace periods: (1) At age jiffies_to_sched_qs,

	 * set .rcu_urgent_qs, (2) At age 2*jiffies_to_sched_qs, set

	 * both .rcu_need_heavy_qs and .rcu_urgent_qs.  Note that the

	 * unsynchronized assignments to the per-CPU rcu_need_heavy_qs

	 * variable are safe because the assignments are repeated if this

	 * CPU failed to pass through a quiescent state.  This code

	 * also checks .jiffies_resched in case jiffies_to_sched_qs

	 * is set way high.

 Store rcu_need_heavy_qs before rcu_urgent_qs. */

	/*

	 * NO_HZ_FULL CPUs can run in-kernel without rcu_sched_clock_irq!

	 * The above code handles this, but only for straight cond_resched().

	 * And some in-kernel loops check need_resched() before calling

	 * cond_resched(), which defeats the above code for CPUs that are

	 * running in-kernel with scheduling-clock interrupts disabled.

	 * So hit them over the head with the resched_cpu() hammer!

	/*

	 * If more than halfway to RCU CPU stall-warning time, invoke

	 * resched_cpu() more frequently to try to loosen things up a bit.

	 * Also check to see if the CPU is getting hammered with interrupts,

	 * but only once per grace period, just to keep the IPIs down to

	 * a dull roar.

 Trace-event wrapper function for trace_rcu_future_grace_period.  */

/*

 * rcu_start_this_gp - Request the start of a particular grace period

 * @rnp_start: The leaf node of the CPU from which to start.

 * @rdp: The rcu_data corresponding to the CPU from which to start.

 * @gp_seq_req: The gp_seq of the grace period to start.

 *

 * Start the specified grace period, as needed to handle newly arrived

 * callbacks.  The required future grace periods are recorded in each

 * rcu_node structure's ->gp_seq_needed field.  Returns true if there

 * is reason to awaken the grace-period kthread.

 *

 * The caller must hold the specified rcu_node structure's ->lock, which

 * is why the caller is responsible for waking the grace-period kthread.

 *

 * Returns true if the GP thread needs to be awakened else false.

	/*

	 * Use funnel locking to either acquire the root rcu_node

	 * structure's lock or bail out if the need for this grace period

	 * has already been recorded -- or if that grace period has in

	 * fact already started.  If there is already a grace period in

	 * progress in a non-leaf node, no recording is needed because the

	 * end of the grace period will scan the leaf rcu_node structures.

	 * Note that rnp_start->lock must not be released.

			/*

			 * We just marked the leaf or internal node, and a

			 * grace period is in progress, which means that

			 * rcu_gp_cleanup() will see the marking.  Bail to

			 * reduce contention.

 At root, and perhaps also leaf. */

 If GP already in progress, just leave, otherwise start one. */

 Caller must wake GP kthread. */

 Push furthest requested GP to leaf node and rcu_data structure. */

/*

 * Clean up any old requests for the just-ended grace period.  Also return

 * whether any additional grace periods have been requested.

 Avoid counter wrap. */

/*

 * Awaken the grace-period kthread.  Don't do a self-awaken (unless in an

 * interrupt or softirq handler, in which case we just might immediately

 * sleep upon return, resulting in a grace-period hang), and don't bother

 * awakening when there is nothing for the grace-period kthread to do

 * (as in several CPUs raced to awaken, we lost), and finally don't try

 * to awaken a kthread that has not yet been created.  If all those checks

 * are passed, track some debug information and awaken.

 *

 * So why do the self-wakeup when in an interrupt or softirq handler

 * in the grace-period kthread's context?  Because the kthread might have

 * been interrupted just as it was going to sleep, and just after the final

 * pre-sleep check of the awaken condition.  In this case, a wakeup really

 * is required, and is therefore supplied.

/*

 * If there is room, assign a ->gp_seq number to any callbacks on this

 * CPU that have not already been assigned.  Also accelerate any callbacks

 * that were previously assigned a ->gp_seq number that has since proven

 * to be too conservative, which can happen if callbacks get assigned a

 * ->gp_seq number while RCU is idle, but with reference to a non-root

 * rcu_node structure.  This function is idempotent, so it does not hurt

 * to call it repeatedly.  Returns an flag saying that we should awaken

 * the RCU grace-period kthread.

 *

 * The caller must hold rnp->lock with interrupts disabled.

 If no pending (not yet ready to invoke) callbacks, nothing to do. */

	/*

	 * Callbacks are often registered with incomplete grace-period

	 * information.  Something about the fact that getting exact

	 * information requires acquiring a global lock...  RCU therefore

	 * makes a conservative estimate of the grace period number at which

	 * a given callback will become ready to invoke.	The following

	 * code checks this estimate and improves it when possible, thus

	 * accelerating callback invocation to an earlier grace-period

	 * number.

 Trace depending on how much we were able to accelerate. */

/*

 * Similar to rcu_accelerate_cbs(), but does not require that the leaf

 * rcu_node structure's ->lock be held.  It consults the cached value

 * of ->gp_seq_needed in the rcu_data structure, and if that indicates

 * that a new grace-period request be made, invokes rcu_accelerate_cbs()

 * while holding the leaf rcu_node structure's ->lock.

 Old request still live, so mark recent callbacks. */

 irqs already disabled. */

 irqs remain disabled. */

/*

 * Move any callbacks whose grace period has completed to the

 * RCU_DONE_TAIL sublist, then compact the remaining sublists and

 * assign ->gp_seq numbers to any callbacks in the RCU_NEXT_TAIL

 * sublist.  This function is idempotent, so it does not hurt to

 * invoke it repeatedly.  As long as it is not invoked -too- often...

 * Returns true if the RCU grace-period kthread needs to be awakened.

 *

 * The caller must hold rnp->lock with interrupts disabled.

 If no pending (not yet ready to invoke) callbacks, nothing to do. */

	/*

	 * Find all callbacks whose ->gp_seq numbers indicate that they

	 * are ready to invoke, and put them into the RCU_DONE_TAIL sublist.

 Classify any remaining callbacks. */

/*

 * Move and classify callbacks, but only if doing so won't require

 * that the RCU grace-period kthread be awakened.

/*

 * In CONFIG_RCU_STRICT_GRACE_PERIOD=y kernels, attempt to generate a

 * quiescent state.  This is intended to be invoked when the CPU notices

 * a new grace period.

/*

 * Update CPU-local rcu_data state to record the beginnings and ends of

 * grace periods.  The caller must hold the ->lock of the leaf rcu_node

 * structure corresponding to the current CPU, and must have irqs disabled.

 * Returns true if the grace-period kthread needs to be awakened.

 Nothing to do. */

 Handle the ends of any preceding grace periods first. */

 Advance CBs. */

 Recent CBs. */

 Now handle the beginnings of any new-to-this-CPU grace periods. */

		/*

		 * If the current grace period is waiting for this CPU,

		 * set up to detect a quiescent state, otherwise don't

		 * go looking for one.

 Remember new grace-period state. */

 w/out lock. */

 irqs already off, so later. */

 Allow rcutorture to stall the grace-period kthread. */

 Actually implement the aforementioned wait. */

/*

 * Handler for on_each_cpu() to invoke the target CPU's RCU core

 * processing.

/*

 * Initialize a new grace period.  Return false if no grace period required.

 Spurious wakeup, tell caller to go back to sleep.  */

 Clear all flags: New GP. */

		/*

		 * Grace period already in progress, don't start another.

		 * Not supposed to be able to happen.

 Advance to a new grace period and initialize state. */

 Record GP times before starting GP, hence rcu_seq_start(). */

	/*

	 * Apply per-leaf buffered online and offline operations to

	 * the rcu_node tree. Note that this new grace period need not

	 * wait for subsequent online CPUs, and that RCU hooks in the CPU

	 * offlining path, when combined with checks in this function,

	 * will handle CPUs that are currently going offline or that will

	 * go offline later.  Please also refer to "Hotplug CPU" section

	 * of RCU's Requirements documentation.

 Wait for CPU-hotplug operations that might have

 started before this grace period did.

 Pair with barriers used when updating ->ofl_seq to odd values.

 Can't wake unless RCU is watching.

 Pair with barriers used when updating ->ofl_seq to even values.

 Nothing to do on this leaf rcu_node structure. */

 Record old state, apply changes to ->qsmaskinit field. */

 If zero-ness of ->qsmaskinit changed, propagate up tree. */

 First online CPU for rcu_node. */

 Ever offline? */

 blocked tasks */

 Last offline CPU and can propagate. */

		/*

		 * If all waited-on tasks from prior grace period are

		 * done, and if all this rcu_node structure's CPUs are

		 * still offline, propagate up the rcu_node tree and

		 * clear ->wait_blkd_tasks.  Otherwise, if one of this

		 * rcu_node structure's CPUs has since come back online,

		 * simply clear ->wait_blkd_tasks.

 Races with CPU hotplug. */

	/*

	 * Set the quiescent-state-needed bits in all the rcu_node

	 * structures for all currently online CPUs in breadth-first

	 * order, starting from the root rcu_node structure, relying on the

	 * layout of the tree within the rcu_state.node[] array.  Note that

	 * other CPUs will access only the leaves of the hierarchy, thus

	 * seeing that no grace period is in progress, at least until the

	 * corresponding leaf node has been initialized.

	 *

	 * The grace period cannot complete until the initialization

	 * process finishes, because this kthread handles both.

 Quiescent states for tasks on any now-offline CPUs. */

 If strict, make all CPUs aware of new grace period.

/*

 * Helper function for swait_event_idle_exclusive() wakeup at force-quiescent-state

 * time.

 If under overload conditions, force an immediate FQS scan.

 Someone like call_rcu() requested a force-quiescent-state scan.

 The current grace period has completed.

/*

 * Do one round of quiescent-state forcing.

 Collect dyntick-idle snapshots. */

 Handle dyntick-idle and offline CPUs. */

 Clear flag to prevent immediate re-entry. */

/*

 * Loop doing repeated quiescent-state forcing until the grace period ends.

			/*

			 * jiffies_force_qs before RCU_GP_WAIT_FQS state

			 * update; required for stall checks.

 Locking provides needed memory barriers. */

 If grace period done, leave loop. */

 If time for quiescent-state forcing, do it. */

 Force full wait till next FQS. */

 Deal with stray signal. */

 Keep old FQS timing. */

/*

 * Clean up after the old grace period.

	/*

	 * We know the grace period is complete, but to everyone else

	 * it appears to still be ongoing.  But it is also the case

	 * that to everyone else it looks like there is nothing that

	 * they can do to advance the grace period.  It is therefore

	 * safe for us to drop the lock in order to mark the grace

	 * period as completed in all of the rcu_node structures.

	/*

	 * Propagate new ->gp_seq value to rcu_node structures so that

	 * other CPUs don't have to wait until the start of the next grace

	 * period to process their callbacks.  This also avoids some nasty

	 * RCU grace-period initialization races by forcing the end of

	 * the current grace period to be completely recorded in all of

	 * the rcu_node structures before the beginning of the next grace

	 * period is recorded in any of the rcu_node structures.

 smp_mb() provided by prior unlock-lock pair. */

 Reset overload indication for CPUs no longer overloaded

 GP before ->gp_seq update. */

 Declare grace period done, trace first to use old GP number. */

 Check for GP requests since above loop. */

 Advance CBs to reduce false positives below. */

 If strict, make all CPUs aware of the end of the old grace period.

/*

 * Body of kthread that handles grace periods.

 Handle grace-period start. */

 Locking provides needed memory barrier. */

 Handle quiescent-state forcing. */

 Handle grace-period end. */

/*

 * Report a full set of quiescent states to the rcu_state data structure.

 * Invoke rcu_gp_kthread_wake() to awaken the grace-period kthread if

 * another grace period is required.  Whether we wake the grace-period

 * kthread or it awakens itself for the next round of quiescent-state

 * forcing, that kthread will clean up after the just-completed grace

 * period.  Note that the caller must hold rnp->lock, which is released

 * before return.

/*

 * Similar to rcu_report_qs_rdp(), for which it is a helper function.

 * Allows quiescent states for a group of CPUs to be reported at one go

 * to the specified rcu_node structure, though all the CPUs in the group

 * must be represented by the same rcu_node structure (which need not be a

 * leaf rcu_node structure, though it often will be).  The gps parameter

 * is the grace-period snapshot, which means that the quiescent states

 * are valid only if rnp->gp_seq is equal to gps.  That structure's lock

 * must be held upon entry, and it is released before return.

 *

 * As a special case, if mask is zero, the bit-already-cleared check is

 * disabled.  This allows propagating quiescent state due to resumed tasks

 * during grace-period initialization.

 Walk up the rcu_node hierarchy. */

			/*

			 * Our bit has already been cleared, or the

			 * relevant grace period is already over, so done.

 Any child must be all zeroed! */

 Other bits still set at this level, so done. */

 No more levels.  Exit loop holding root lock. */

	/*

	 * Get here if we are the last CPU to pass through a quiescent

	 * state for this grace period.  Invoke rcu_report_qs_rsp()

	 * to clean up and start the next grace period if one is needed.

 releases rnp->lock. */

/*

 * Record a quiescent state for all tasks that were previously queued

 * on the specified rcu_node structure and that were blocking the current

 * RCU grace period.  The caller must hold the corresponding rnp->lock with

 * irqs disabled, and this lock is released upon return, but irqs remain

 * disabled.

 Still need more quiescent states! */

		/*

		 * Only one rcu_node structure in the tree, so don't

		 * try to report up to its nonexistent parent!

 Report up the rest of the hierarchy, tracking current ->gp_seq. */

 irqs remain disabled. */

 irqs already disabled. */

/*

 * Record a quiescent state for the specified CPU to that CPU's rcu_data

 * structure.  This must be called from the specified CPU.

		/*

		 * The grace period in which this quiescent state was

		 * recorded has ended, so don't report it upwards.

		 * We will instead need a new quiescent state that lies

		 * within the current grace period.

 need qs for new gp. */

		/*

		 * This GP can't end until cpu checks in, so all of our

		 * callbacks can be processed during the next GP.

 ^^^ Released rnp->lock */

/*

 * Check to see if there is a new grace period of which this CPU

 * is not yet aware, and if so, set up local rcu_data state for it.

 * Otherwise, see if this CPU has just passed through its first

 * quiescent state for this grace period, and record that fact if so.

 Check for grace-period ends and beginnings. */

	/*

	 * Does this CPU still need to do its part for current grace period?

	 * If no, return and let the other CPUs do their part as well.

	/*

	 * Was there a quiescent state since the beginning of the grace

	 * period? If no, then exit and wait for the next call.

	/*

	 * Tell RCU we are done (but rcu_report_qs_rdp() will be the

	 * judge of that).

/*

 * Near the end of the offline process.  Trace the fact that this CPU

 * is going offline.

/*

 * All CPUs for the specified rcu_node structure have gone offline,

 * and all tasks that were preempted within an RCU read-side critical

 * section while running on one of those CPUs have since exited their RCU

 * read-side critical section.  Some other CPU is reporting this fact with

 * the specified rcu_node structure's ->lock held and interrupts disabled.

 * This function therefore goes up the tree of rcu_node structures,

 * clearing the corresponding bits in the ->qsmaskinit fields.  Note that

 * the leaf rcu_node structure's ->qsmaskinit field has already been

 * updated.

 *

 * This function does check that the specified rcu_node structure has

 * all CPUs offline and no blocked tasks, so it is OK to invoke it

 * prematurely.  That said, invoking it after the fact will cost you

 * a needless lock acquisition.  So once it has done its work, don't

 * invoke it again.

 irqs already disabled. */

 Between grace periods, so better already be zero! */

 irqs remain disabled. */

 irqs remain disabled. */

/*

 * The CPU has been completely removed, and some other CPU is reporting

 * this fact from process context.  Do the remainder of the cleanup.

 * There can only be one CPU hotplug operation at a time, so no need for

 * explicit locking.

 Outgoing CPU's rdp & rnp. */

 Adjust any no-longer-needed kthreads. */

 Stop-machine done, so allow nohz_full to disable tick.

/*

 * Invoke any RCU callbacks that have made it to the end of their grace

 * period.  Throttle as specified by rdp->blimit.

 If no callbacks are ready, just return. */

	/*

	 * Extract the list of ready callbacks, disabling to prevent

	 * races with call_rcu() from interrupt handlers.  Leave the

	 * callback counts, as rcu_barrier() needs to be conservative.

 Invoke callbacks. */

		/*

		 * Stop only if limit reached and CPU has something to do.

 only call local_clock() every 32 callbacks */

 Exceeded the time limit, so leave. */

 Update counts and requeue any remaining callbacks. */

 Reinstate batch limit if we have worked down the excess. */

 Reset ->qlen_last_fqs_check trigger if enough CBs have drained. */

	/*

	 * The following usually indicates a double call_rcu().  To track

	 * this down, try building with CONFIG_DEBUG_OBJECTS_RCU_HEAD=y.

 Re-invoke RCU core processing if there are callbacks remaining. */

/*

 * This function is invoked from each scheduling-clock interrupt,

 * and checks to see if this CPU is in a non-context-switch quiescent

 * state, for example, user mode or idle loop.  It also schedules RCU

 * core processing.  If the current grace period has gone on too long,

 * it will ask the scheduler to manufacture a context switch for the sole

 * purpose of providing the needed quiescent state.

 The load-acquire pairs with the store-release setting to true. */

 Idle and userspace execution already are quiescent states. */

/*

 * Scan the leaf rcu_node structures.  For each structure on which all

 * CPUs have reported a quiescent state and on which there are tasks

 * blocking the current grace period, initiate RCU priority boosting.

 * Otherwise, invoke the specified function to check dyntick state for

 * each CPU that has not yet reported a quiescent state.

				/*

				 * No point in scanning bits because they

				 * are all zero.  But we might need to

				 * priority-boost blocked readers.

 rcu_initiate_boost() releases rnp->lock */

 Idle/offline CPUs, report (releases rnp->lock). */

 Nothing to do here, so just drop the lock. */

/*

 * Force quiescent states on reluctant CPUs, and also detect which

 * CPUs are in dyntick-idle mode.

 Funnel through hierarchy to reduce memory contention. */

 rnp_old == rcu_get_root(), rnp == NULL. */

 Reached the root of the rcu_node tree, acquire lock. */

 Someone beat us to it. */

 Workqueue handler for an RCU reader for kernels enforcing struct RCU

 grace periods.

 Perform RCU core processing work for the current CPU.  */

 Report any deferred quiescent states if preemption enabled. */

 Update RCU state based on any recent quiescent states. */

 No grace period and unregistered callbacks? */

 If there are callbacks ready, invoke them. */

 Do any needed deferred wakeups of rcuo kthreads. */

 If strict GPs, schedule an RCU reader in a clean environment.

	/*

	 * If the thread is yielding, only wake it when this

	 * is invoked from idle

/*

 * Wake up this CPU's rcuc kthread to do RCU core processing.

/*

 * Per-CPU kernel thread that invokes RCU callbacks.  This replaces

 * the RCU softirq used in configurations of RCU that do not support RCU

 * priority boosting.

/*

 * Spawn per-CPU RCU core processing kthreads.

/*

 * Handle any core-RCU processing required by a call_rcu() invocation.

	/*

	 * If called from an extended quiescent state, invoke the RCU

	 * core in order to force a re-evaluation of RCU's idleness.

 If interrupts were disabled or CPU offline, don't invoke RCU core. */

	/*

	 * Force the grace period if too many callbacks or too long waiting.

	 * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()

	 * if some other CPU has recently done so.  Also, don't bother

	 * invoking rcu_force_quiescent_state() if the newly enqueued callback

	 * is the only one waiting for a grace period to complete.

 Are we ignoring a completed grace period? */

 Start a new grace period if one not already started. */

 Give the grace period a kick. */

/*

 * RCU callback function to leak a callback.

/*

 * Check and if necessary update the leaf rcu_node structure's

 * ->cbovldmask bit corresponding to the current CPU based on that CPU's

 * number of queued RCU callbacks.  The caller must hold the leaf rcu_node

 * structure's ->lock.

 Early boot and wildcard value set.

/*

 * Check and if necessary update the leaf rcu_node structure's

 * ->cbovldmask bit corresponding to the current CPU based on that CPU's

 * number of queued RCU callbacks.  No locks need be held, but the

 * caller must have disabled interrupts.

 *

 * Note that this function ignores the possibility that there are a lot

 * of callbacks all of which have already seen the end of their respective

 * grace periods.  This omission is due to the need for no-CBs CPUs to

 * be holding ->nocb_lock to do this check, which is too heavy for a

 * common-case operation.

 Early boot wildcard value or already set correctly.

 Helper function for call_rcu() and friends.  */

 Misaligned rcu_head! */

		/*

		 * Probable double call_rcu(), so leak the callback.

		 * Use rcu:rcu_callback trace event to find the previous

		 * time callback was passed to __call_rcu().

 Add the callback to our list. */

 This can trigger due to call_rcu() from offline CPU:

 Very early boot, before rcu_init().  Initialize if needed

 and then drop through to queue the callback.

 Enqueued onto ->nocb_bypass, so just leave.

 If no-CBs CPU gets here, rcu_nocb_try_bypass() acquired ->nocb_lock.

 Go handle any RCU core processing required. */

 unlocks */

/**

 * call_rcu() - Queue an RCU callback for invocation after a grace period.

 * @head: structure to be used for queueing the RCU updates.

 * @func: actual callback function to be invoked after the grace period

 *

 * The callback function will be invoked some time after a full grace

 * period elapses, in other words after all pre-existing RCU read-side

 * critical sections have completed.  However, the callback function

 * might well execute concurrently with RCU read-side critical sections

 * that started after call_rcu() was invoked.

 *

 * RCU read-side critical sections are delimited by rcu_read_lock()

 * and rcu_read_unlock(), and may be nested.  In addition, but only in

 * v5.0 and later, regions of code across which interrupts, preemption,

 * or softirqs have been disabled also serve as RCU read-side critical

 * sections.  This includes hardware interrupt handlers, softirq handlers,

 * and NMI handlers.

 *

 * Note that all CPUs must agree that the grace period extended beyond

 * all pre-existing RCU read-side critical section.  On systems with more

 * than one CPU, this means that when "func()" is invoked, each CPU is

 * guaranteed to have executed a full memory barrier since the end of its

 * last RCU read-side critical section whose beginning preceded the call

 * to call_rcu().  It also means that each CPU executing an RCU read-side

 * critical section that continues beyond the start of "func()" must have

 * executed a memory barrier after the call_rcu() but before the beginning

 * of that RCU read-side critical section.  Note that these guarantees

 * include CPUs that are offline, idle, or executing in user mode, as

 * well as CPUs that are executing in the kernel.

 *

 * Furthermore, if CPU A invoked call_rcu() and CPU B invoked the

 * resulting RCU callback function "func()", then both CPU A and CPU B are

 * guaranteed to execute a full memory barrier during the time interval

 * between the call to call_rcu() and the invocation of "func()" -- even

 * if CPU A and CPU B are the same CPU (but again only if the system has

 * more than one CPU).

 *

 * Implementation of these memory-ordering guarantees is described here:

 * Documentation/RCU/Design/Memory-Ordering/Tree-RCU-Memory-Ordering.rst.

 Maximum number of jiffies to wait before draining a batch. */

/**

 * struct kvfree_rcu_bulk_data - single block to store kvfree_rcu() pointers

 * @nr_records: Number of active pointers in the array

 * @next: Next bulk object in the block chain

 * @records: Array of the kvfree_rcu() pointers

/*

 * This macro defines how many entries the "records" array

 * will contain. It is based on the fact that the size of

 * kvfree_rcu_bulk_data structure becomes exactly one page.

/**

 * struct kfree_rcu_cpu_work - single batch of kfree_rcu() requests

 * @rcu_work: Let queue_rcu_work() invoke workqueue handler after grace period

 * @head_free: List of kfree_rcu() objects waiting for a grace period

 * @bkvhead_free: Bulk-List of kvfree_rcu() objects waiting for a grace period

 * @krcp: Pointer to @kfree_rcu_cpu structure

/**

 * struct kfree_rcu_cpu - batch up kfree_rcu() requests for RCU grace period

 * @head: List of kfree_rcu() objects not yet waiting for a grace period

 * @bkvhead: Bulk-List of kvfree_rcu() objects not yet waiting for a grace period

 * @krw_arr: Array of batches of kfree_rcu() objects waiting for a grace period

 * @lock: Synchronize access to this structure

 * @monitor_work: Promote @head to @head_free after KFREE_DRAIN_JIFFIES

 * @monitor_todo: Tracks whether a @monitor_work delayed work is pending

 * @initialized: The @rcu_work fields have been initialized

 * @count: Number of objects for which GP not started

 * @bkvcache:

 *	A simple cache list that contains objects for reuse purpose.

 *	In order to save some per-cpu space the list is singular.

 *	Even though it is lockless an access has to be protected by the

 *	per-cpu lock.

 * @page_cache_work: A work to refill the cache when it is empty

 * @backoff_page_cache_fill: Delay cache refills

 * @work_in_progress: Indicates that page_cache_work is running

 * @hrtimer: A hrtimer for scheduling a page_cache_work

 * @nr_bkv_objs: number of allocated objects at @bkvcache.

 *

 * This is a per-CPU structure.  The reason that it is not included in

 * the rcu_data structure is to permit this code to be extracted from

 * the RCU files.  Such extraction could allow further optimization of

 * the interactions with the slab allocators.

 For safely calling this_cpu_ptr().

 Check the limit.

/*

 * This function is invoked in workqueue context after a grace period.

 * It frees all the objects queued on ->bkvhead_free or ->head_free.

 Channels 1 and 2.

 Channel 3.

 Handle the first two channels.

 kmalloc() / kfree().

 vmalloc() / vfree().

	/*

	 * This is used when the "bulk" path can not be used for the

	 * double-argument of kvfree_rcu().  This happens when the

	 * page-cache is empty, which means that objects are instead

	 * queued on a linked list through their rcu_head structures.

	 * This list is named "Channel 3".

/*

 * This function is invoked after the KFREE_DRAIN_JIFFIES timeout.

 Attempt to start a new batch.

 Try to detach bkvhead or head and attach it over any

 available corresponding free channel. It can be that

 a previous RCU batch is in progress, it means that

 immediately to queue another one is not possible so

 in that case the monitor work is rearmed.

 Channel 1 corresponds to the SLAB-pointer bulk path.

 Channel 2 corresponds to vmalloc-pointer bulk path.

 Channel 3 corresponds to both SLAB and vmalloc

 objects queued on the linked list.

 One work is per one batch, so there are three

 "free channels", the batch can handle. It can

 be that the work is in the pending state when

 channels have been detached following by each

 other.

 If there is nothing to detach, it means that our job is

 successfully done here. In case of having at least one

 of the channels that is still busy we should rearm the

 work to repeat an attempt. Because previous batches are

 still in progress.

 Record ptr in a page managed by krcp, with the pre-krc_this_cpu_lock()

 state specified by flags.  If can_alloc is true, the caller must

 be schedulable and not be holding any locks or mutexes that might be

 acquired by the memory allocator or anything that it might invoke.

 Returns true if ptr was successfully recorded, else the caller must

 use a fallback.

 Check if a new block is required. */

 __GFP_NORETRY - allows a light-weight direct reclaim

 what is OK from minimizing of fallback hitting point of

 view. Apart of that it forbids any OOM invoking what is

 also beneficial since we are about to release memory soon.



 __GFP_NOMEMALLOC - prevents from consuming of all the

 memory reserves. Please note we have a fallback path.



 __GFP_NOWARN - it is supposed that an allocation can

 be failed under low memory or high memory pressure

 scenarios.

 Initialize the new block. */

 Attach it to the head. */

 Finally insert. */

/*

 * Queue a request for lazy invocation of the appropriate free routine

 * after a grace period.  Please note that three paths are maintained,

 * two for the common case using arrays of pointers and a third one that

 * is used only when the main paths cannot be used, for example, due to

 * memory pressure.

 *

 * Each kvfree_call_rcu() request is added to a batch. The batch will be drained

 * every KFREE_DRAIN_JIFFIES number of jiffies. All the objects in the batch will

 * be free'd in workqueue context. This allows us to: batch requests together to

 * reduce the number of grace periods during heavy kfree_rcu()/kvfree_rcu() load.

		/*

		 * Please note there is a limitation for the head-less

		 * variant, that is why there is a clear rule for such

		 * objects: it can be used from might_sleep() context

		 * only. For other places please embed an rcu_head to

		 * your data.

 Queue the object but don't yet schedule the batch.

 Probable double kfree_rcu(), just leak.

 Mark as success and leave.

 Inline if kvfree_rcu(one_arg) call.

 Set timer to drain after KFREE_DRAIN_JIFFIES.

	/*

	 * Inline kvfree() after synchronize_rcu(). We can do

	 * it from might_sleep() context only, so the current

	 * CPU can pass the QS state.

 Snapshot count of all CPUs */

/*

 * During early boot, any blocking grace-period wait automatically

 * implies a grace period.  Later on, this is never the case for PREEMPTION.

 *

 * However, because a context switch is a grace period for !PREEMPTION, any

 * blocking grace-period wait automatically implies a grace period if

 * there is only one CPU online at any point time during execution of

 * either synchronize_rcu() or synchronize_rcu_expedited().  It is OK to

 * occasionally incorrectly indicate that there are multiple CPUs online

 * when there was in fact only one the whole time, as this just adds some

 * overhead: RCU still operates correctly.

 Check for RCU read-side critical section. */

	/*

	 * If the rcu_state.n_online_cpus counter is equal to one,

	 * there is only one CPU, and that CPU sees all prior accesses

	 * made by any CPU that was online at the time of its access.

	 * Furthermore, if this counter is equal to one, its value cannot

	 * change until after the preempt_enable() below.

	 *

	 * Furthermore, if rcu_state.n_online_cpus is equal to one here,

	 * all later CPUs (both this one and any that come online later

	 * on) are guaranteed to see all accesses prior to this point

	 * in the code, without the need for additional memory barriers.

	 * Those memory barriers are provided by CPU-hotplug code.

/**

 * synchronize_rcu - wait until a grace period has elapsed.

 *

 * Control will return to the caller some time after a full grace

 * period has elapsed, in other words after all currently executing RCU

 * read-side critical sections have completed.  Note, however, that

 * upon return from synchronize_rcu(), the caller might well be executing

 * concurrently with new RCU read-side critical sections that began while

 * synchronize_rcu() was waiting.

 *

 * RCU read-side critical sections are delimited by rcu_read_lock()

 * and rcu_read_unlock(), and may be nested.  In addition, but only in

 * v5.0 and later, regions of code across which interrupts, preemption,

 * or softirqs have been disabled also serve as RCU read-side critical

 * sections.  This includes hardware interrupt handlers, softirq handlers,

 * and NMI handlers.

 *

 * Note that this guarantee implies further memory-ordering guarantees.

 * On systems with more than one CPU, when synchronize_rcu() returns,

 * each CPU is guaranteed to have executed a full memory barrier since

 * the end of its last RCU read-side critical section whose beginning

 * preceded the call to synchronize_rcu().  In addition, each CPU having

 * an RCU read-side critical section that extends beyond the return from

 * synchronize_rcu() is guaranteed to have executed a full memory barrier

 * after the beginning of synchronize_rcu() and before the beginning of

 * that RCU read-side critical section.  Note that these guarantees include

 * CPUs that are offline, idle, or executing in user mode, as well as CPUs

 * that are executing in the kernel.

 *

 * Furthermore, if CPU A invoked synchronize_rcu(), which returned

 * to its caller on CPU B, then both CPU A and CPU B are guaranteed

 * to have executed a full memory barrier during the execution of

 * synchronize_rcu() -- even if CPU A and CPU B are the same CPU (but

 * again only if the system has more than one CPU).

 *

 * Implementation of these memory-ordering guarantees is described here:

 * Documentation/RCU/Design/Memory-Ordering/Tree-RCU-Memory-Ordering.rst.

 Context allows vacuous grace periods.

/**

 * get_state_synchronize_rcu - Snapshot current RCU state

 *

 * Returns a cookie that is used by a later call to cond_synchronize_rcu()

 * or poll_state_synchronize_rcu() to determine whether or not a full

 * grace period has elapsed in the meantime.

	/*

	 * Any prior manipulation of RCU-protected data must happen

	 * before the load from ->gp_seq.

 ^^^ */

/**

 * start_poll_synchronize_rcu - Snapshot and start RCU grace period

 *

 * Returns a cookie that is used by a later call to cond_synchronize_rcu()

 * or poll_state_synchronize_rcu() to determine whether or not a full

 * grace period has elapsed in the meantime.  If the needed grace period

 * is not already slated to start, notifies RCU core of the need for that

 * grace period.

 *

 * Interrupts must be enabled for the case where it is necessary to awaken

 * the grace-period kthread.

 irqs already disabled.

/**

 * poll_state_synchronize_rcu - Conditionally wait for an RCU grace period

 *

 * @oldstate: value from get_state_synchronize_rcu() or start_poll_synchronize_rcu()

 *

 * If a full RCU grace period has elapsed since the earlier call from

 * which oldstate was obtained, return @true, otherwise return @false.

 * If @false is returned, it is the caller's responsibility to invoke this

 * function later on until it does return @true.  Alternatively, the caller

 * can explicitly wait for a grace period, for example, by passing @oldstate

 * to cond_synchronize_rcu() or by directly invoking synchronize_rcu().

 *

 * Yes, this function does not take counter wrap into account.

 * But counter wrap is harmless.  If the counter wraps, we have waited for

 * more than 2 billion grace periods (and way more on a 64-bit system!).

 * Those needing to keep oldstate values for very long time periods

 * (many hours even on 32-bit systems) should check them occasionally

 * and either refresh them or set a flag indicating that the grace period

 * has completed.

 *

 * This function provides the same memory-ordering guarantees that

 * would be provided by a synchronize_rcu() that was invoked at the call

 * to the function that provided @oldstate, and that returned at the end

 * of this function.

 Ensure GP ends before subsequent accesses. */

/**

 * cond_synchronize_rcu - Conditionally wait for an RCU grace period

 *

 * @oldstate: value from get_state_synchronize_rcu() or start_poll_synchronize_rcu()

 *

 * If a full RCU grace period has elapsed since the earlier call to

 * get_state_synchronize_rcu() or start_poll_synchronize_rcu(), just return.

 * Otherwise, invoke synchronize_rcu() to wait for a full grace period.

 *

 * Yes, this function does not take counter wrap into account.  But

 * counter wrap is harmless.  If the counter wraps, we have waited for

 * more than 2 billion grace periods (and way more on a 64-bit system!),

 * so waiting for one additional grace period should be just fine.

 *

 * This function provides the same memory-ordering guarantees that

 * would be provided by a synchronize_rcu() that was invoked at the call

 * to the function that provided @oldstate, and that returned at the end

 * of this function.

/*

 * Check to see if there is any immediate RCU-related work to be done by

 * the current CPU, returning 1 if so and zero otherwise.  The checks are

 * in order of increasing expense: checks that can be carried out against

 * CPU-local state are performed first.  However, we must check for CPU

 * stalls first, else we might not get a chance.

 Check for CPU stalls, if enabled. */

 Does this CPU need a deferred NOCB wakeup? */

 Is this a nohz_full CPU in userspace or idle?  (Ignore RCU if so.) */

 Is the RCU core waiting for a quiescent state from this CPU? */

 Does this CPU have callbacks ready to invoke? */

 Has RCU gone idle with this CPU needing another grace period? */

 Have RCU grace period completed or started?  */

 outside lock */

 nothing to do */

/*

 * Helper function for rcu_barrier() tracing.  If tracing is disabled,

 * the compiler is expected to optimize this away.

/*

 * RCU callback function for rcu_barrier().  If we are last, wake

 * up the task executing rcu_barrier().

 *

 * Note that the value of rcu_state.barrier_sequence must be captured

 * before the atomic_dec_and_test().  Otherwise, if this CPU is not last,

 * other CPUs might count the value down to zero before this CPU gets

 * around to invoking rcu_barrier_trace(), which might result in bogus

 * data from the next instance of rcu_barrier().

/*

 * Called with preemption disabled, and from cross-cpu IRQ context.

/**

 * rcu_barrier - Wait until all in-flight call_rcu() callbacks complete.

 *

 * Note that this primitive does not necessarily wait for an RCU grace period

 * to complete.  For example, if there are no RCU callbacks queued anywhere

 * in the system, then rcu_barrier() is within its rights to return

 * immediately, without waiting for anything, much less an RCU grace period.

 Take mutex to serialize concurrent rcu_barrier() requests. */

 Did someone else do our work for us? */

 caller's subsequent code after above check. */

 Mark the start of the barrier operation. */

	/*

	 * Initialize the count to two rather than to zero in order

	 * to avoid a too-soon return to zero in case of an immediate

	 * invocation of the just-enqueued callback (or preemption of

	 * this task).  Exclude CPU-hotplug operations to ensure that no

	 * offline non-offloaded CPU has callbacks queued.

	/*

	 * Force each CPU with callbacks to register a new callback.

	 * When that callback is invoked, we will know that all of the

	 * corresponding CPU's preceding callbacks have been invoked.

	/*

	 * Now that we have an rcu_barrier_callback() callback on each

	 * CPU, and thus each counted, remove the initial count.

 Wait for all rcu_barrier_callback() callbacks to be invoked. */

 Mark the end of the barrier operation. */

 Other rcu_barrier() invocations can now safely proceed. */

/*

 * Propagate ->qsinitmask bits up the rcu_node tree to account for the

 * first CPU in a given leaf rcu_node structure coming online.  The caller

 * must hold the corresponding leaf rcu_node ->lock with interrupts

 * disabled.

 Interrupts already disabled. */

 Interrupts remain disabled. */

/*

 * Do boot-time initialization of a CPU's per-CPU RCU data.

 Set up local state, ensuring consistent view of global state. */

/*

 * Invoked early in the CPU-online process, when pretty much all services

 * are available.  The incoming CPU is not present.

 *

 * Initializes a CPU's per-CPU RCU data.  Note that only one online or

 * offline event can be happening at a given time.  Note also that we can

 * accept some slop in the rsp->gp_seq access due to the fact that this

 * CPU cannot possibly have any non-offloaded RCU callbacks in flight yet.

 * And any offloaded callbacks are being numbered elsewhere.

 Set up local state, ensuring consistent view of global state. */

 CPU not up, no tearing. */

 irqs remain disabled. */

	/*

	 * Only non-NOCB CPUs that didn't have early-boot callbacks need to be

	 * (re-)initialized.

 Re-enable callbacks. */

	/*

	 * Add CPU to leaf rcu_node pending-online bitmask.  Any needed

	 * propagation up the rcu_node tree will happen at the beginning

	 * of the next grace period.

 irqs already disabled. */

 We have now been online. */

/*

 * Update RCU priority boot kthread affinity for CPU-hotplug changes.

/*

 * Near the end of the CPU-online process.  Pretty much all services

 * enabled, and the CPU is now very much alive.

 Too early in boot for scheduler work. */

 Stop-machine done, so allow nohz_full to disable tick.

/*

 * Near the beginning of the process.  The CPU is still very much alive

 * with pretty much all services enabled.

 nohz_full CPUs need the tick for stop-machine to work quickly

/*

 * Mark the specified CPU as being online so that subsequent grace periods

 * (both expedited and normal) will wait on it.  Note that this means that

 * incoming CPUs are not allowed to use RCU read-side critical sections

 * until this function is called.  Failing to observe this restriction

 * will result in lockdep splats.

 *

 * Note that this function is special in that it is invoked directly

 * from the incoming CPU rather than from the cpuhp_step mechanism.

 * This is because this function must be invoked at a precise location.

 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier().

 Allow lockless access for expedited grace periods. */

 ^^^ */

 Offline-induced counter wrap? */

 An incoming CPU should never be blocking a grace period. */

 RCU waiting on incoming CPU? */

 Report QS -after- changing ->qsmaskinitnext! */

 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier().

 Ensure RCU read-side usage follows above initialization. */

/*

 * The outgoing function has no further need of RCU, so remove it from

 * the rcu_node tree's ->qsmaskinitnext bit masks.

 *

 * Note that this function is special in that it is invoked directly

 * from the outgoing CPU rather than from the cpuhp_step mechanism.

 * This is because this function must be invoked at a precise location.

 Outgoing CPU's rdp & rnp. */

 Do any dangling deferred wakeups.

 QS for any half-done expedited grace period. */

 Remove outgoing CPU from mask in the leaf rcu_node structure. */

 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier().

 Enforce GP memory-order guarantee. */

 RCU waiting on outgoing CPU? */

 Report quiescent state -before- changing ->qsmaskinitnext! */

 Pair with rcu_gp_cleanup()'s ->ofl_seq barrier().

/*

 * The outgoing CPU has just passed through the dying-idle state, and we

 * are being invoked from the CPU that was IPIed to continue the offline

 * operation.  Migrate the outgoing CPU's callbacks to the current CPU.

 No callbacks to migrate. */

 irqs already disabled. */

 irqs already disabled. */

 Leverage recent GPs and set GP for new callbacks. */

 irqs remain disabled. */

 irqs remain disabled. */

/*

 * On non-huge systems, use expedited RCU grace periods to make suspend

 * and hibernation run faster.

/*

 * Spawn the kthreads that handle RCU's grace periods.

 Force priority into range. */

 Reset .gp_activity and .gp_req_activity before setting .gp_kthread.

 ^^^ */

/*

 * This function is invoked towards the end of the scheduler's

 * initialization process.  Before this is called, the idle task might

 * contain synchronous grace-period primitives (during which time, this idle

 * task is booting the system, and such primitives are no-ops).  After this

 * function is called, any synchronous grace-period primitives are run as

 * expedited, with the requesting task driving the grace period forward.

 * A later core_initcall() rcu_set_runtime_mode() will switch to full

 * runtime RCU functionality.

/*

 * Helper function for rcu_init() that initializes the rcu_state structure.

 kids/node in each level. */

 Fix buf[] init! */

 Silence gcc 4.8 false positive about array index out of range. */

 Initialize the level-tracking arrays. */

 Initialize the elements themselves, starting from the leaves. */

/*

 * Compute the rcu_node tree geometry from kernel parameters.  This cannot

 * replace the definitions in tree.h because those are needed to size

 * the ->node array in the rcu_state structure.

		/*

		 * Warn if setup_nr_cpu_ids() had not yet been invoked,

		 * unless nr_cpus_ids == NR_CPUS, in which case who cares?

	/*

	 * Initialize any unspecified boot parameters.

	 * The default values of jiffies_till_first_fqs and

	 * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS

	 * value, which is a function of HZ, then adding one for each

	 * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.

 If the compile-time values are accurate, just leave. */

	/*

	 * The boot-time rcu_fanout_leaf parameter must be at least two

	 * and cannot exceed the number of bits in the rcu_node masks.

	 * Complain and fall back to the compile-time values if this

	 * limit is exceeded.

	/*

	 * Compute number of nodes that can be handled an rcu_node tree

	 * with the given number of levels.

	/*

	 * The tree must be able to accommodate the configured number of CPUs.

	 * If this limit is exceeded, fall back to the compile-time values.

 Calculate the number of levels in the tree. */

 Calculate the number of rcu_nodes at each level of the tree. */

 Calculate the total number of rcu_node structures. */

/*

 * Dump out the structure of the rcu_node combining tree associated

 * with the rcu_state structure.

 Clamp it to [0:100] seconds interval. */

	/*

	 * We don't need protection against CPU-hotplug here because

	 * this is called early in boot, before either interrupts

	 * or the scheduler are operational.

 Create workqueue for Tree SRCU and for expedited GPs. */

 Fill in default value for rcutree.qovld boot parameter. */

 -After- the rcu_node ->lock fields are initialized! */

 SPDX-License-Identifier: GPL-2.0+

/*

 * RCU segmented callback lists, function definitions

 *

 * Copyright IBM Corporation, 2017

 *

 * Authors: Paul E. McKenney <paulmck@linux.ibm.com>

 Initialize simple callback list. */

/*

 * Enqueue an rcu_head structure onto the specified callback list.

/*

 * Flush the second rcu_cblist structure onto the first one, obliterating

 * any contents of the first.  If rhp is non-NULL, enqueue it as the sole

 * element of the second rcu_cblist structure, but ensuring that the second

 * rcu_cblist structure, if initially non-empty, always appears non-empty

 * throughout the process.  If rdp is NULL, the second rcu_cblist structure

 * is instead initialized to empty.

/*

 * Dequeue the oldest rcu_head structure from the specified callback

 * list.

 Set the length of an rcu_segcblist structure. */

 Get the length of a segment of the rcu_segcblist structure. */

 Return number of callbacks in segmented callback list by summing seglen. */

 Set the length of a segment of the rcu_segcblist structure. */

 Increase the numeric length of a segment by a specified amount. */

 Move from's segment length to to's segment. */

 Increment segment's length. */

/*

 * Increase the numeric length of an rcu_segcblist structure by the

 * specified amount, which can be negative.  This can cause the ->len

 * field to disagree with the actual number of callbacks on the structure.

 * This increase is fully ordered with respect to the callers accesses

 * both before and after.

 *

 * So why on earth is a memory barrier required both before and after

 * the update to the ->len field???

 *

 * The reason is that rcu_barrier() locklessly samples each CPU's ->len

 * field, and if a given CPU's field is zero, avoids IPIing that CPU.

 * This can of course race with both queuing and invoking of callbacks.

 * Failing to correctly handle either of these races could result in

 * rcu_barrier() failing to IPI a CPU that actually had callbacks queued

 * which rcu_barrier() was obligated to wait on.  And if rcu_barrier()

 * failed to wait on such a callback, unloading certain kernel modules

 * would result in calls to functions whose code was no longer present in

 * the kernel, for but one example.

 *

 * Therefore, ->len transitions from 1->0 and 0->1 have to be carefully

 * ordered with respect with both list modifications and the rcu_barrier().

 *

 * The queuing case is CASE 1 and the invoking case is CASE 2.

 *

 * CASE 1: Suppose that CPU 0 has no callbacks queued, but invokes

 * call_rcu() just as CPU 1 invokes rcu_barrier().  CPU 0's ->len field

 * will transition from 0->1, which is one of the transitions that must

 * be handled carefully.  Without the full memory barriers after the ->len

 * update and at the beginning of rcu_barrier(), the following could happen:

 *

 * CPU 0				CPU 1

 *

 * call_rcu().

 *					rcu_barrier() sees ->len as 0.

 * set ->len = 1.

 *					rcu_barrier() does nothing.

 *					module is unloaded.

 * callback invokes unloaded function!

 *

 * With the full barriers, any case where rcu_barrier() sees ->len as 0 will

 * have unambiguously preceded the return from the racing call_rcu(), which

 * means that this call_rcu() invocation is OK to not wait on.  After all,

 * you are supposed to make sure that any problematic call_rcu() invocations

 * happen before the rcu_barrier().

 *

 *

 * CASE 2: Suppose that CPU 0 is invoking its last callback just as

 * CPU 1 invokes rcu_barrier().  CPU 0's ->len field will transition from

 * 1->0, which is one of the transitions that must be handled carefully.

 * Without the full memory barriers before the ->len update and at the

 * end of rcu_barrier(), the following could happen:

 *

 * CPU 0				CPU 1

 *

 * start invoking last callback

 * set ->len = 0 (reordered)

 *					rcu_barrier() sees ->len as 0

 *					rcu_barrier() does nothing.

 *					module is unloaded

 * callback executing after unloaded!

 *

 * With the full barriers, any case where rcu_barrier() sees ->len as 0

 * will be fully ordered after the completion of the callback function,

 * so that the module unloading operation is completely safe.

 *

 Read header comment above.

 Read header comment above.

 Read header comment above.

 Read header comment above.

/*

 * Increase the numeric length of an rcu_segcblist structure by one.

 * This can cause the ->len field to disagree with the actual number of

 * callbacks on the structure.  This increase is fully ordered with respect

 * to the callers accesses both before and after.

/*

 * Initialize an rcu_segcblist structure.

/*

 * Disable the specified rcu_segcblist structure, so that callbacks can

 * no longer be posted to it.  This structure must be empty.

/*

 * Mark the specified rcu_segcblist structure as offloaded.

/*

 * Does the specified rcu_segcblist structure contain callbacks that

 * are ready to be invoked?

/*

 * Does the specified rcu_segcblist structure contain callbacks that

 * are still pending, that is, not yet ready to be invoked?

/*

 * Return a pointer to the first callback in the specified rcu_segcblist

 * structure.  This is useful for diagnostics.

/*

 * Return a pointer to the first pending callback in the specified

 * rcu_segcblist structure.  This is useful just after posting a given

 * callback -- if that callback is the first pending callback, then

 * you cannot rely on someone else having already started up the required

 * grace period.

/*

 * Return false if there are no CBs awaiting grace periods, otherwise,

 * return true and store the nearest waited-upon grace period into *lp.

/*

 * Enqueue the specified callback onto the specified rcu_segcblist

 * structure, updating accounting as needed.  Note that the ->len

 * field may be accessed locklessly, hence the WRITE_ONCE().

 * The ->len field is used by rcu_barrier() and friends to determine

 * if it must post a callback on this structure, and it is OK

 * for rcu_barrier() to sometimes post callbacks needlessly, but

 * absolutely not OK for it to ever miss posting a callback.

/*

 * Entrain the specified callback onto the specified rcu_segcblist at

 * the end of the last non-empty segment.  If the entire rcu_segcblist

 * is empty, make no change, but return false.

 *

 * This is intended for use by rcu_barrier()-like primitives, -not-

 * for normal grace-period use.  IMPORTANT:  The callback you enqueue

 * will wait for all prior callbacks, NOT necessarily for a grace

 * period.  You have been warned.

 Ensure counts are updated before callback is entrained. */

/*

 * Extract only those callbacks ready to be invoked from the specified

 * rcu_segcblist structure and place them in the specified rcu_cblist

 * structure.

 Nothing to do. */

/*

 * Extract only those callbacks still pending (not yet ready to be

 * invoked) from the specified rcu_segcblist structure and place them in

 * the specified rcu_cblist structure.  Note that this loses information

 * about any callbacks that might have been partway done waiting for

 * their grace period.  Too bad!  They will have to start over.

 Nothing to do. */

/*

 * Insert counts from the specified rcu_cblist structure in the

 * specified rcu_segcblist structure.

/*

 * Move callbacks from the specified rcu_cblist to the beginning of the

 * done-callbacks segment of the specified rcu_segcblist.

 No callbacks to move. */

/*

 * Move callbacks from the specified rcu_cblist to the end of the

 * new-callbacks segment of the specified rcu_segcblist.

 Nothing to do. */

/*

 * Advance the callbacks in the specified rcu_segcblist structure based

 * on the current value passed in for the grace-period counter.

	/*

	 * Find all callbacks whose ->gp_seq numbers indicate that they

	 * are ready to invoke, and put them into the RCU_DONE_TAIL segment.

 If no callbacks moved, nothing more need be done. */

 Clean up tail pointers that might have been misordered above. */

	/*

	 * Callbacks moved, so clean up the misordered ->tails[] pointers

	 * that now point into the middle of the list of ready-to-invoke

	 * callbacks.  The overall effect is to copy down the later pointers

	 * into the gap that was created by the now-ready segments.

 No more callbacks. */

/*

 * "Accelerate" callbacks based on more-accurate grace-period information.

 * The reason for this is that RCU does not synchronize the beginnings and

 * ends of grace periods, and that callbacks are posted locally.  This in

 * turn means that the callbacks must be labelled conservatively early

 * on, as getting exact information would degrade both performance and

 * scalability.  When more accurate grace-period information becomes

 * available, previously posted callbacks can be "accelerated", marking

 * them to complete at the end of the earlier grace period.

 *

 * This function operates on an rcu_segcblist structure, and also the

 * grace-period sequence number seq at which new callbacks would become

 * ready to invoke.  Returns true if there are callbacks that won't be

 * ready to invoke until seq, false otherwise.

	/*

	 * Find the segment preceding the oldest segment of callbacks

	 * whose ->gp_seq[] completion is at or after that passed in via

	 * "seq", skipping any empty segments.  This oldest segment, along

	 * with any later segments, can be merged in with any newly arrived

	 * callbacks in the RCU_NEXT_TAIL segment, and assigned "seq"

	 * as their ->gp_seq[] grace-period completion sequence number.

	/*

	 * If all the segments contain callbacks that correspond to

	 * earlier grace-period sequence numbers than "seq", leave.

	 * Assuming that the rcu_segcblist structure has enough

	 * segments in its arrays, this can only happen if some of

	 * the non-done segments contain callbacks that really are

	 * ready to invoke.  This situation will get straightened

	 * out by the next call to rcu_segcblist_advance().

	 *

	 * Also advance to the oldest segment of callbacks whose

	 * ->gp_seq[] completion is at or after that passed in via "seq",

	 * skipping any empty segments.

	 *

	 * Note that segment "i" (and any lower-numbered segments

	 * containing older callbacks) will be unaffected, and their

	 * grace-period numbers remain unchanged.  For example, if i ==

	 * WAIT_TAIL, then neither WAIT_TAIL nor DONE_TAIL will be touched.

	 * Instead, the CBs in NEXT_TAIL will be merged with those in

	 * NEXT_READY_TAIL and the grace-period number of NEXT_READY_TAIL

	 * would be updated.  NEXT_TAIL would then be empty.

 Accounting: everything below i is about to get merged into i. */

	/*

	 * Merge all later callbacks, including newly arrived callbacks,

	 * into the segment located by the for-loop above.  Assign "seq"

	 * as the ->gp_seq[] value in order to correctly handle the case

	 * where there were no pending callbacks in the rcu_segcblist

	 * structure other than in the RCU_NEXT_TAIL segment.

/*

 * Merge the source rcu_segcblist structure into the destination

 * rcu_segcblist structure, then initialize the source.  Any pending

 * callbacks from the source get to start over.  It is best to

 * advance and accelerate both the destination and the source

 * before merging.

	/*

	 * No need smp_mb() before setting length to 0, because CPU hotplug

	 * lock excludes rcu_barrier.

 SPDX-License-Identifier: GPL-2.0+



 Scalability test comparing RCU vs other mechanisms

 for acquiring references on objects.



 Copyright (C) Google, 2020.



 Author: Joel Fernandes <joel@joelfernandes.org>

 Wait until there are multiple CPUs before starting test.

 Number of loops per experiment, all readers execute operations concurrently.

 Number of readers, with -1 defaulting to about 75% of the CPUs.

 Number of runs.

 Reader delay in nanoseconds, 0 for no delay.

 Number of readers that are part of the current experiment.

 Use to wait for all threads to start.

 Track which experiment is currently running.

 Operations vector for selecting different types of tests.

 Definitions for SRCU ref scale testing.

 Definitions for RCU Tasks ref scale testing: Empty read markers.

 These definitions also work for RCU Rude readers.

 Definitions for RCU Tasks Trace ref scale testing.

 Definitions for reference count

 Definitions for rwlock

 Definitions for rwsem

 Definitions for global spinlock

 Definitions for global irq-save spinlock

 Definitions acquire-release.

 Reader kthread.  Repeatedly does empty RCU read-side

 critical section, minimizing update-side interference.

 Wait for signal that this reader can start.

 Make sure that the CPU is affinitized appropriately during testing.

 To reduce noise, do an initial cache-warming invocation, check

 in, and then keep warming until everyone has checked in.

 Also keep interrupts disabled.  This also has the effect

 of preventing entries into slow path for rcu_read_unlock().

 To reduce runtime-skew noise, do maintain-load invocations until

 everyone is done.

 Print the results of each reader and return the sum of all their durations.

 The main_func is the main orchestrator, it performs a bunch of

 experiments.  For every experiment, it orders all the readers

 involved to start and waits for them to finish the experiment. It

 then reads their timestamps and starts the next experiment. Each

 experiment progresses from 1 concurrent reader to N of them at which

 point all the timestamps are printed.

 Wait for all threads to start.

 Start exp readers up per experiment

 Print the average of all experiments

 This will shutdown everything including us.

 Wait for torture to stop us

 Do scale-type-specific cleanup operations.

 Shutdown kthread.  Just waits to be awakened, then shuts down system.

 Wake before output.

 Shutdown task

 Reader tasks (default to ~75% of online CPUs).

 Main Task

 SPDX-License-Identifier: GPL-2.0+

/*

 * Sleepable Read-Copy Update mechanism for mutual exclusion,

 *	tiny version for non-preemptible single-CPU use.

 *

 * Copyright (C) IBM Corporation, 2017

 *

 * Author: Paul McKenney <paulmck@linux.ibm.com>

 Don't re-initialize a lock while it is held. */

 #ifdef CONFIG_DEBUG_LOCK_ALLOC */

/*

 * init_srcu_struct - initialize a sleep-RCU structure

 * @ssp: structure to initialize.

 *

 * Must invoke this on a given srcu_struct before passing that srcu_struct

 * to any other function.  Each srcu_struct represents a separate domain

 * of SRCU protection.

 #else #ifdef CONFIG_DEBUG_LOCK_ALLOC */

/*

 * cleanup_srcu_struct - deconstruct a sleep-RCU structure

 * @ssp: structure to clean up.

 *

 * Must invoke this after you are finished using a given srcu_struct that

 * was initialized via init_srcu_struct(), else you leak memory.

/*

 * Removes the count for the old reader from the appropriate element of

 * the srcu_struct.

/*

 * Workqueue handler to drive one grace period and invoke any callbacks

 * that become ready as a result.  Single-CPU and !PREEMPTION operation

 * means that we get away with murder on synchronization.  ;-)

 Already running or nothing to do. */

 Remove recently arrived callbacks and wait for readers. */

 srcu_read_unlock() wakes! */

 srcu_read_unlock() cheap. */

 Invoke the callbacks we removed above. */

	/*

	 * Enable rescheduling, and if there are more callbacks,

	 * reschedule ourselves.  This can race with a call_srcu()

	 * at interrupt level, but the ->srcu_gp_running checks will

	 * straighten that out.

/*

 * Enqueue an SRCU callback on the specified srcu_struct structure,

 * initiating grace-period processing if it is not already running.

/*

 * synchronize_srcu - wait for prior SRCU read-side critical-section completion

/*

 * get_state_synchronize_srcu - Provide an end-of-grace-period cookie

/*

 * start_poll_synchronize_srcu - Provide cookie and start grace period

 *

 * The difference between this and get_state_synchronize_srcu() is that

 * this function ensures that the poll_state_synchronize_srcu() will

 * eventually return the value true.

/*

 * poll_state_synchronize_srcu - Has cookie's grace period ended?

 Lockdep diagnostics.  */

/*

 * Queue work for srcu_struct structures with early boot callbacks.

 * The work won't actually execute until the workqueue initialization

 * phase that takes place after the scheduler starts.

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic wait-for-completion handler;

 *

 * It differs from semaphores in that their default case is the opposite,

 * wait_for_completion default blocks whereas semaphore default non-block. The

 * interface also makes it easy to 'complete' multiple waiting threads,

 * something which isn't entirely natural for semaphores.

 *

 * But more importantly, the primitive documents the usage. Semaphores would

 * typically be used for exclusion which gives rise to priority inversion.

 * Waiting for completion is a typically sync point, but not an exclusion point.

/**

 * complete: - signals a single thread waiting on this completion

 * @x:  holds the state of this particular completion

 *

 * This will wake up a single thread waiting on this completion. Threads will be

 * awakened in the same order in which they were queued.

 *

 * See also complete_all(), wait_for_completion() and related routines.

 *

 * If this function wakes up a task, it executes a full memory barrier before

 * accessing the task state.

/**

 * complete_all: - signals all threads waiting on this completion

 * @x:  holds the state of this particular completion

 *

 * This will wake up all threads waiting on this particular completion event.

 *

 * If this function wakes up a task, it executes a full memory barrier before

 * accessing the task state.

 *

 * Since complete_all() sets the completion of @x permanently to done

 * to allow multiple waiters to finish, a call to reinit_completion()

 * must be used on @x if @x is to be used again. The code must make

 * sure that all waiters have woken and finished before reinitializing

 * @x. Also note that the function completion_done() can not be used

 * to know if there are still waiters after complete_all() has been called.

/**

 * wait_for_completion: - waits for completion of a task

 * @x:  holds the state of this particular completion

 *

 * This waits to be signaled for completion of a specific task. It is NOT

 * interruptible and there is no timeout.

 *

 * See also similar routines (i.e. wait_for_completion_timeout()) with timeout

 * and interrupt capability. Also see complete().

/**

 * wait_for_completion_timeout: - waits for completion of a task (w/timeout)

 * @x:  holds the state of this particular completion

 * @timeout:  timeout value in jiffies

 *

 * This waits for either a completion of a specific task to be signaled or for a

 * specified timeout to expire. The timeout is in jiffies. It is not

 * interruptible.

 *

 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left

 * till timeout) if completed.

/**

 * wait_for_completion_io: - waits for completion of a task

 * @x:  holds the state of this particular completion

 *

 * This waits to be signaled for completion of a specific task. It is NOT

 * interruptible and there is no timeout. The caller is accounted as waiting

 * for IO (which traditionally means blkio only).

/**

 * wait_for_completion_io_timeout: - waits for completion of a task (w/timeout)

 * @x:  holds the state of this particular completion

 * @timeout:  timeout value in jiffies

 *

 * This waits for either a completion of a specific task to be signaled or for a

 * specified timeout to expire. The timeout is in jiffies. It is not

 * interruptible. The caller is accounted as waiting for IO (which traditionally

 * means blkio only).

 *

 * Return: 0 if timed out, and positive (at least 1, or number of jiffies left

 * till timeout) if completed.

/**

 * wait_for_completion_interruptible: - waits for completion of a task (w/intr)

 * @x:  holds the state of this particular completion

 *

 * This waits for completion of a specific task to be signaled. It is

 * interruptible.

 *

 * Return: -ERESTARTSYS if interrupted, 0 if completed.

/**

 * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))

 * @x:  holds the state of this particular completion

 * @timeout:  timeout value in jiffies

 *

 * This waits for either a completion of a specific task to be signaled or for a

 * specified timeout to expire. It is interruptible. The timeout is in jiffies.

 *

 * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,

 * or number of jiffies left till timeout) if completed.

/**

 * wait_for_completion_killable: - waits for completion of a task (killable)

 * @x:  holds the state of this particular completion

 *

 * This waits to be signaled for completion of a specific task. It can be

 * interrupted by a kill signal.

 *

 * Return: -ERESTARTSYS if interrupted, 0 if completed.

/**

 * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))

 * @x:  holds the state of this particular completion

 * @timeout:  timeout value in jiffies

 *

 * This waits for either a completion of a specific task to be

 * signaled or for a specified timeout to expire. It can be

 * interrupted by a kill signal. The timeout is in jiffies.

 *

 * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,

 * or number of jiffies left till timeout) if completed.

/**

 *	try_wait_for_completion - try to decrement a completion without blocking

 *	@x:	completion structure

 *

 *	Return: 0 if a decrement cannot be done without blocking

 *		 1 if a decrement succeeded.

 *

 *	If a completion is being used as a counting completion,

 *	attempt to decrement the counter without blocking. This

 *	enables us to avoid waiting if the resource the completion

 *	is protecting is not available.

	/*

	 * Since x->done will need to be locked only

	 * in the non-blocking case, we check x->done

	 * first without taking the lock so we can

	 * return early in the blocking case.

/**

 *	completion_done - Test to see if a completion has any waiters

 *	@x:	completion structure

 *

 *	Return: 0 if there are waiters (wait_for_completion() in progress)

 *		 1 if there are no waiters.

 *

 *	Note, this will always return true if complete_all() was called on @X.

	/*

	 * If ->done, we need to wait for complete() to release ->wait.lock

	 * otherwise we can end up freeing the completion before complete()

	 * is done referencing it.

 SPDX-License-Identifier: GPL-2.0

/*

 * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)

 *

 *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 *  Interactivity improvements by Mike Galbraith

 *  (C) 2007 Mike Galbraith <efault@gmx.de>

 *

 *  Various enhancements by Dmitry Adamushko.

 *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>

 *

 *  Group scheduling enhancements by Srivatsa Vaddagiri

 *  Copyright IBM Corporation, 2007

 *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>

 *

 *  Scaled math optimizations by Thomas Gleixner

 *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>

 *

 *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra

 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra

/*

 * Targeted preemption latency for CPU-bound tasks:

 *

 * NOTE: this latency value is not the same as the concept of

 * 'timeslice length' - timeslices in CFS are of variable length

 * and have no persistent notion like in traditional, time-slice

 * based scheduling concepts.

 *

 * (to see the precise effective timeslice length of your workload,

 *  run vmstat and monitor the context-switches (cs) field)

 *

 * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)

/*

 * The initial- and re-scaling of tunables is configurable

 *

 * Options are:

 *

 *   SCHED_TUNABLESCALING_NONE - unscaled, always *1

 *   SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus)

 *   SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus

 *

 * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))

/*

 * Minimal preemption granularity for CPU-bound tasks:

 *

 * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)

/*

 * Minimal preemption granularity for CPU-bound SCHED_IDLE tasks.

 * Applies only when SCHED_IDLE tasks compete with normal tasks.

 *

 * (default: 0.75 msec)

/*

 * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity

/*

 * After fork, child runs first. If set to 0 (default) then

 * parent will (try to) run first.

/*

 * SCHED_OTHER wake-up granularity.

 *

 * This option delays the preemption effects of decoupled workloads

 * and reduces their over-scheduling. Synchronous workloads will still

 * have immediate wakeup/sleep latencies.

 *

 * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)

/*

 * For asym packing, by default the lower numbered CPU has higher priority.

/*

 * The margin used when comparing utilization with CPU capacity.

 *

 * (default: ~20%)

/*

 * The margin used when comparing CPU capacities.

 * is 'cap1' noticeably greater than 'cap2'

 *

 * (default: ~5%)

/*

 * Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool

 * each time a cfs_rq requests quota.

 *

 * Note: in the case that the slice exceeds the runtime remaining (either due

 * to consumption or the quota being specified to be smaller than the slice)

 * we will always only issue the remaining available time.

 *

 * (default: 5 msec, units: microseconds)

/*

 * Increase the granularity value when there are more CPUs,

 * because with more CPUs the 'effective latency' as visible

 * to users decreases. But the relationship is not linear,

 * so pick a second-best guess by going with the log2 of the

 * number of CPUs.

 *

 * This idea comes from the SD scheduler of Con Kolivas:

/*

 * delta_exec * weight / lw.weight

 *   OR

 * (delta_exec * (weight * lw->inv_weight)) >> WMULT_SHIFT

 *

 * Either weight := NICE_0_LOAD and lw \e sched_prio_to_wmult[], in which case

 * we're guaranteed shift stays positive because inv_weight is guaranteed to

 * fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift >= 22.

 *

 * Or, weight =< lw.weight (because lw.weight is the runqueue weight), thus

 * weight/lw.weight <= 1, and therefore our shift will also be positive.

/**************************************************************

 * CFS operations on generic schedulable entities:

 Walk up scheduling entities hierarchy */

	/*

	 * Ensure we either appear before our parent (if already

	 * enqueued) or force our parent to appear after us when it is

	 * enqueued. The fact that we always enqueue bottom-up

	 * reduces this to two cases and a special case for the root

	 * cfs_rq. Furthermore, it also means that we will always reset

	 * tmp_alone_branch either when the branch is connected

	 * to a tree or when we reach the top of the tree

		/*

		 * If parent is already on the list, we add the child

		 * just before. Thanks to circular linked property of

		 * the list, this means to put the child at the tail

		 * of the list that starts by parent.

		/*

		 * The branch is now connected to its tree so we can

		 * reset tmp_alone_branch to the beginning of the

		 * list.

		/*

		 * cfs rq without parent should be put

		 * at the tail of the list.

		/*

		 * We have reach the top of a tree so we can reset

		 * tmp_alone_branch to the beginning of the list.

	/*

	 * The parent has not already been added so we want to

	 * make sure that it will be put after us.

	 * tmp_alone_branch points to the begin of the branch

	 * where we will add parent.

	/*

	 * update tmp_alone_branch to points to the new begin

	 * of the branch

		/*

		 * With cfs_rq being unthrottled/throttled during an enqueue,

		 * it can happen the tmp_alone_branch points the a leaf that

		 * we finally want to del. In this case, tmp_alone_branch moves

		 * to the prev element but it will point to rq->leaf_cfs_rq_list

		 * at the end of the enqueue.

 Iterate thr' all leaf cfs_rq's on a runqueue */

 Do the two (enqueued) entities belong to the same group ? */

	/*

	 * preemption test can be made between sibling entities who are in the

	 * same cfs_rq i.e who have a common parent. Walk up the hierarchy of

	 * both tasks until we find their ancestors who are siblings of common

	 * parent.

 First walk up until both entities are at same depth */

 !CONFIG_FAIR_GROUP_SCHED */

 CONFIG_FAIR_GROUP_SCHED */

/**************************************************************

 * Scheduling class tree data structure manipulation methods:

 non-empty tree */

 ensure we never gain time by being placed backwards. */

/*

 * Enqueue an entity into the rb-tree:

/**************************************************************

 * Scheduling class statistics methods:

/*

 * delta /= w

/*

 * The idea is to set a period in which each task runs once.

 *

 * When there are too many tasks (sched_nr_latency) we have to stretch

 * this period because otherwise the slices get too small.

 *

 * p = (nr <= nl) ? l : l*nr/nl

/*

 * We calculate the wall-time slice from the period by taking a part

 * proportional to the weight.

 *

 * s = p*P[w/rw]

/*

 * We calculate the vruntime slice of a to-be-inserted task.

 *

 * vs = s/w

 Give new sched_entity start runnable values to heavy its load in infant time */

	/*

	 * Tasks are initialized with full load to be seen as heavy tasks until

	 * they get a chance to stabilize to their real load level.

	 * Group entities are initialized with zero load to reflect the fact that

	 * nothing has been attached to the task group yet.

 when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */

/*

 * With new tasks being created, their initial util_avgs are extrapolated

 * based on the cfs_rq's current util_avg:

 *

 *   util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight

 *

 * However, in many cases, the above util_avg does not give a desired

 * value. Moreover, the sum of the util_avgs may be divergent, such

 * as when the series is a harmonic series.

 *

 * To solve this problem, we also cap the util_avg of successive tasks to

 * only 1/2 of the left utilization budget:

 *

 *   util_avg_cap = (cpu_scale - cfs_rq->avg.util_avg) / 2^n

 *

 * where n denotes the nth task and cpu_scale the CPU capacity.

 *

 * For example, for a CPU with 1024 of capacity, a simplest series from

 * the beginning would be like:

 *

 *  task  util_avg: 512, 256, 128,  64,  32,   16,    8, ...

 * cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ...

 *

 * Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap)

 * if util_avg > util_avg_cap.

		/*

		 * For !fair tasks do:

		 *

		update_cfs_rq_load_avg(now, cfs_rq);

		attach_entity_load_avg(cfs_rq, se);

		switched_from_fair(rq, p);

		 *

		 * such that the next switched_to_fair() has the

		 * expected state.

 !CONFIG_SMP */

 CONFIG_SMP */

/*

 * Update the current task's runtime statistics.

	/*

	 * When the sched_schedstat changes from 0 to 1, some sched se

	 * maybe already in the runqueue, the se->statistics.wait_start

	 * will be 0.So it will let the delta wrong. We need to avoid this

	 * scenario.

/*

 * Task is being enqueued - update stats:

	/*

	 * Are we enqueueing a waiting task? (for current tasks

	 * a dequeue/enqueue event is a NOP)

	/*

	 * Mark the end of the wait period if dequeueing a

	 * waiting task:

 XXX racy against TTWU */

/*

 * We are picking a new current task - update its stats:

	/*

	 * We are starting a new run period:

/**************************************************

 * Scheduling class queueing methods:

/*

 * Approximate time to scan a full NUMA task in ms. The task scan period is

 * calculated based on the tasks virtual memory size and

 * numa_balancing_scan_size.

 Portion of address space to scan in MB */

 Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */

 nr_tasks, tasks */

	/*

	 * faults[] array is split into two regions: faults_mem and faults_cpu.

	 *

	 * Faults_cpu is used to decide whether memory should move

	 * towards the CPU. As a consequence, these stats are weighted

	 * more by CPU use than by memory faults.

/*

 * For functions that can be called in multiple contexts that permit reading

 * ->numa_group (see struct task_struct for locking rules).

	/*

	 * Calculations based on RSS as non-present and empty pages are skipped

	 * by the PTE scanner and NUMA hinting faults should be trapped based

	 * on resident pages

 For sanity's sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */

 Scale the maximum scan period with the amount of shared memory. */

 Watch for min being lower than max due to floor calculations */

 Scale the maximum scan period with the amount of shared memory. */

 Shared or private faults. */

 Memory and CPU locality */

 Averaged statistics, and temporary buffers. */

/*

 * The averaged statistics, shared & private, memory & CPU,

 * occupy the first half of the array. The second half of the

 * array is for current counters, which are averaged into the

 * first set by task_numa_placement.

/*

 * A node triggering more than 1/3 as many NUMA faults as the maximum is

 * considered part of a numa group's pseudo-interleaving set. Migrations

 * between these nodes are slowed down, to allow things to settle down.

 Handle placement on systems where not all nodes are directly connected. */

	/*

	 * All nodes are directly connected, and the same distance

	 * from each other. No need for fancy placement algorithms.

	/*

	 * This code is called for each node, introducing N^2 complexity,

	 * which should be ok given the number of nodes rarely exceeds 8.

		/*

		 * The furthest away nodes in the system are not interesting

		 * for placement; nid was already counted.

		/*

		 * On systems with a backplane NUMA topology, compare groups

		 * of nodes, and move tasks towards the group with the most

		 * memory accesses. When comparing two nodes at distance

		 * "hoplimit", only nodes closer by than "hoplimit" are part

		 * of each group. Skip other nodes.

 Add up the faults from nearby nodes. */

		/*

		 * On systems with a glueless mesh NUMA topology, there are

		 * no fixed "groups of nodes". Instead, nodes that are not

		 * directly connected bounce traffic through intermediate

		 * nodes; a numa_group can occupy any set of nodes.

		 * The further away a node is, the less the faults count.

		 * This seems to result in good task placement.

/*

 * These return the fraction of accesses done by a particular task, or

 * task group, on a particular numa node.  The group weight is given a

 * larger multiplier, in order to group tasks together that are almost

 * evenly spread out between numa nodes.

	/*

	 * Allow first faults or private faults to migrate immediately early in

	 * the lifetime of a task. The magic number 4 is based on waiting for

	 * two full passes of the "multi-stage node selection" test that is

	 * executed below.

	/*

	 * Multi-stage node selection is used in conjunction with a periodic

	 * migration fault to build a temporal task<->page relation. By using

	 * a two-stage filter we remove short/unlikely relations.

	 *

	 * Using P(p) ~ n_p / n_t as per frequentist probability, we can equate

	 * a task's usage of a particular page (n_p) per total usage of this

	 * page (n_t) (in a given time-span) to a probability.

	 *

	 * Our periodic faults will sample this probability and getting the

	 * same result twice in a row, given these samples are fully

	 * independent, is then given by P(n)^2, provided our sample period

	 * is sufficiently short compared to the usage pattern.

	 *

	 * This quadric squishes small probabilities, making it less likely we

	 * act on an unlikely task<->page relation.

 Always allow migrate on private faults */

 A shared fault, but p->numa_group has not been set up yet. */

	/*

	 * Destination node is much more heavily used than the source

	 * node? Allow migration.

	/*

	 * Distribute memory according to CPU & memory use on each node,

	 * with 3/4 hysteresis to avoid unnecessary memory migrations:

	 *

	 * faults_cpu(dst)   3   faults_cpu(src)

	 * --------------- * - > ---------------

	 * faults_mem(dst)   4   faults_mem(src)

/*

 * 'numa_type' describes the node at the moment of load balancing.

 The node has spare capacity that can be used to run more tasks.  */

	/*

	 * The node is fully used and the tasks don't compete for more CPU

	 * cycles. Nevertheless, some tasks might wait before running.

	/*

	 * The node is overloaded and can't provide expected CPU cycles to all

	 * tasks.

 Cached statistics for all CPUs within a node */

 Total compute capacity of CPUs on a node */

 Forward declarations of select_idle_sibling helpers */

	/*

	 * Prefer cores instead of packing HT siblings

	 * and triggering future load balancing.

/*

 * Gather all necessary information to make NUMA balancing placement

 * decisions that are compatible with standard load balancer. This

 * borrows code and logic from update_sg_lb_stats but sharing a

 * common implementation is impractical.

 Check if run-queue part of active NUMA balance. */

 Find alternative idle CPU. */

 Failed to find an alternative idle CPU */

	/*

	 * Clear previous best_cpu/rq numa-migrate flag, since task now

	 * found a better CPU to move/swap.

	/*

	 * The load is corrected for the CPU capacity available on each node.

	 *

	 * src_load        dst_load

	 * ------------ vs ---------

	 * src_capacity    dst_capacity

 Would this change make things worse? */

/*

 * Maximum NUMA importance can be 1998 (2*999);

 * SMALLIMP @ 30 would be close to 1998/64.

 * Used to deter task migration.

/*

 * This checks if the overall compute and NUMA accesses of the system would

 * be improved if the source tasks was migrated to the target dst_cpu taking

 * into account that it might be best if task running on the dst_cpu should

 * be exchanged with the source task

	/*

	 * Because we have preemption enabled we can get migrated around and

	 * end try selecting ourselves (current == env->p) as a swap candidate.

 Skip this swap candidate if cannot move to the source cpu. */

	/*

	 * Skip this swap candidate if it is not moving to its preferred

	 * node and the best task is.

	/*

	 * "imp" is the fault differential for the source task between the

	 * source and destination node. Calculate the total differential for

	 * the source task and potential destination task. The more negative

	 * the value is, the more remote accesses that would be expected to

	 * be incurred if the tasks were swapped.

	 *

	 * If dst and source tasks are in the same NUMA group, or not

	 * in any group then look only at task weights.

		/*

		 * Add some hysteresis to prevent swapping the

		 * tasks within a group over tiny differences.

		/*

		 * Compare the group weights. If a task is all by itself

		 * (not part of a group), use the task weight instead.

 Discourage picking a task already on its preferred node */

	/*

	 * Encourage picking a task that moves to its preferred node.

	 * This potentially makes imp larger than it's maximum of

	 * 1998 (see SMALLIMP and task_weight for why) but in this

	 * case, it does not matter.

	/*

	 * Prefer swapping with a task moving to its preferred node over a

	 * task that is not.

	/*

	 * If the NUMA importance is less than SMALLIMP,

	 * task migration might only result in ping pong

	 * of tasks and also hurt performance due to cache

	 * misses.

	/*

	 * In the overloaded case, try and keep the load balanced.

 Evaluate an idle CPU for a task numa move. */

 Nothing cached so current CPU went idle since the search. */

		/*

		 * If the CPU is no longer truly idle and the previous best CPU

		 * is, keep using it.

	/*

	 * If a move to idle is allowed because there is capacity or load

	 * balance improves then stop the search. While a better swap

	 * candidate may exist, a search is not free.

	/*

	 * If a swap candidate must be identified and the current best task

	 * moves its preferred node then stop the search.

	/*

	 * If dst node has spare capacity, then check if there is an

	 * imbalance that would be overruled by the load balancer.

		/*

		 * Would movement cause an imbalance? Note that if src has

		 * more running tasks that the imbalance is ignored as the

		 * move improves the imbalance from the perspective of the

		 * CPU load balancer.

 Use idle CPU if there is no imbalance */

		/*

		 * If the improvement from just moving env->p direction is better

		 * than swapping tasks around, check if a move is possible.

 Skip this CPU if the source task cannot migrate */

	/*

	 * Pick the lowest SD_NUMA domain, as that would have the smallest

	 * imbalance and would be the first to start moving tasks about.

	 *

	 * And we want to avoid any moving of tasks about, as that would create

	 * random movement of tasks -- counter the numa conditions we're trying

	 * to satisfy here.

	/*

	 * Cpusets can break the scheduler domain tree into smaller

	 * balance domains, some of which do not cross NUMA boundaries.

	 * Tasks that are "trapped" in such domains cannot be migrated

	 * elsewhere, so there is no point in (re)trying.

 Try to find a spot on the preferred nid. */

	/*

	 * Look at other nodes in these cases:

	 * - there is no space available on the preferred_nid

	 * - the task is part of a numa_group that is interleaved across

	 *   multiple NUMA nodes; in order to better consolidate the group,

	 *   we need to check other locations.

 Only consider nodes where both task and groups benefit */

	/*

	 * If the task is part of a workload that spans multiple NUMA nodes,

	 * and is migrating into one of the workload's active nodes, remember

	 * this node as the task's preferred numa node, so the workload can

	 * settle down.

	 * A task that migrated to a second choice node will be better off

	 * trying for a better one later. Do not set the preferred node here.

 No better CPU than the current one was found. */

 Attempt to migrate a task to a CPU on the preferred node. */

 This task has no NUMA fault statistics yet */

 Periodically retry migrating the task to the preferred node */

 Success if task is already running on preferred CPU */

 Otherwise, try migrate to a CPU on the preferred node */

/*

 * Find out how many nodes the workload is actively running on. Do this by

 * tracking the nodes from which NUMA hinting faults are triggered. This can

 * be different from the set of nodes where the workload's memory is currently

 * located.

/*

 * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS

 * increments. The more local the fault statistics are, the higher the scan

 * period will be for the next scan window. If local/(local+remote) ratio is

 * below NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS)

 * the scan period will decrease. Aim for 70% local accesses.

/*

 * Increase the scan period (slow down scanning) if the majority of

 * our memory is already on our local node, or if the majority of

 * the page accesses are shared with other processes.

 * Otherwise, decrease the scan period.

	/*

	 * If there were no record hinting faults then either the task is

	 * completely idle or all activity is in areas that are not of interest

	 * to automatic numa balancing. Related to that, if there were failed

	 * migration then it implies we are migrating too quickly or the local

	 * node is overloaded. In either case, scan slower

	/*

	 * Prepare to scale scan period relative to the current period.

	 *	 == NUMA_PERIOD_THRESHOLD scan period stays the same

	 *       <  NUMA_PERIOD_THRESHOLD scan period decreases (scan faster)

	 *	 >= NUMA_PERIOD_THRESHOLD scan period increases (scan slower)

		/*

		 * Most memory accesses are local. There is no need to

		 * do fast NUMA scanning, since memory is already local.

		/*

		 * Most memory accesses are shared with other tasks.

		 * There is no point in continuing fast NUMA scanning,

		 * since other tasks may just move the memory elsewhere.

		/*

		 * Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS,

		 * yet they are not on the local NUMA node. Speed up

		 * NUMA scanning to get the memory moved over.

/*

 * Get the fraction of time the task has been running since the last

 * NUMA placement cycle. The scheduler keeps similar statistics, but

 * decays those on a 32ms period, which is orders of magnitude off

 * from the dozens-of-seconds NUMA balancing period. Use the scheduler

 * stats only if the task is so new there are no NUMA statistics yet.

 Use the start of this time slice to avoid calculations. */

 Avoid time going backwards, prevent potential divide error: */

/*

 * Determine the preferred nid for a task in a numa_group. This needs to

 * be done in a way that produces consistent results with group_weight,

 * otherwise workloads might not converge.

 Direct connections between all NUMA nodes. */

	/*

	 * On a system with glueless mesh NUMA topology, group_weight

	 * scores nodes according to the number of NUMA hinting faults on

	 * both the node itself, and on nearby nodes.

	/*

	 * Finding the preferred nid in a system with NUMA backplane

	 * interconnect topology is more involved. The goal is to locate

	 * tasks from numa_groups near each other in the system, and

	 * untangle workloads from different sides of the system. This requires

	 * searching down the hierarchy of node groups, recursively searching

	 * inside the highest scoring group of nodes. The nodemask tricks

	 * keep the complexity of the search down.

 Are there nodes at this distance from each other? */

 Sum group's NUMA faults; includes a==b case. */

 Remember the top group. */

				/*

				 * subtle: at the smallest distance there is

				 * just one node left in each "group", the

				 * winner is the preferred nid.

 Next round, evaluate the nodes within max_group. */

	/*

	 * The p->mm->numa_scan_seq field gets updated without

	 * exclusive access. Use READ_ONCE() here to ensure

	 * that the field is read in a single access:

 If the task is part of a group prevent parallel updates to group stats */

 Find the node with the highest number of faults */

 Keep track of the offsets in numa_faults array */

 Decay existing window, copy faults since last scan */

			/*

			 * Normalize the faults_from, so all tasks in a group

			 * count according to CPU use, instead of by the raw

			 * number of faults. Tasks with little runtime have

			 * little over-all impact on throughput, and thus their

			 * faults are less important.

				/*

				 * safe because we can only change our own group

				 *

				 * mem_idx represents the offset for a given

				 * nid and priv in a specific region because it

				 * is at the beginning of the numa_faults array.

 Set the new preferred node */

	/*

	 * Only join the other group if its bigger; if we're the bigger group,

	 * the other task will join us.

	/*

	 * Tie-break on the grp address.

 Always join threads in the same process. */

 Simple filter to avoid false positives due to PID collisions */

 Update priv based on whether false sharing was detected */

/*

 * Get rid of NUMA statistics associated with a task (either current or dead).

 * If @final is set, the task is dead and has reached refcount zero, so we can

 * safely free all relevant data structures. Otherwise, there might be

 * concurrent reads from places like load balancing and procfs, and we should

 * reset the data back to default state without freeing ->numa_faults.

 safe: p either is current or is being freed by current */

/*

 * Got a PROT_NONE fault for a page on @node.

 for example, ksmd faulting in a user's mm */

 Allocate buffer to track faults on a per-node basis */

	/*

	 * First accesses are treated as private, otherwise consider accesses

	 * to be private if the accessing pid has not changed

	/*

	 * If a workload spans multiple NUMA nodes, a shared fault that

	 * occurs wholly within the set of nodes that the workload is

	 * actively using should be counted as local. This allows the

	 * scan rate to slow down when a workload has settled down.

	/*

	 * Retry to migrate task to preferred node periodically, in case it

	 * previously failed, or the scheduler moved us.

	/*

	 * We only did a read acquisition of the mmap sem, so

	 * p->mm->numa_scan_seq is written to without exclusive access

	 * and the update is not guaranteed to be atomic. That's not

	 * much of an issue though, since this is just used for

	 * statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not

	 * expensive, to avoid any form of compiler optimizations:

/*

 * The expensive part of numa migration is done from task_work context.

 * Triggered from task_tick_numa().

	/*

	 * Who cares about NUMA placement when they're dying.

	 *

	 * NOTE: make sure not to dereference p->mm before this check,

	 * exit_task_work() happens _after_ exit_mm() so we could be called

	 * without p->mm even though we still had it when we enqueued this

	 * work.

	/*

	 * Enforce maximal scan/migration frequency..

	/*

	 * Delay this task enough that another task of this mm will likely win

	 * the next time around.

 MB in pages */

 Scan up to this much virtual space */

		/*

		 * Shared library pages mapped by multiple processes are not

		 * migrated as it is expected they are cache replicated. Avoid

		 * hinting faults in read-only file-backed mappings or the vdso

		 * as migrating the pages will be of marginal benefit.

		/*

		 * Skip inaccessible VMAs to avoid any confusion between

		 * PROT_NONE and NUMA hinting ptes

			/*

			 * Try to scan sysctl_numa_balancing_size worth of

			 * hpages that have at least one present PTE that

			 * is not already pte-numa. If the VMA contains

			 * areas that are unused or already full of prot_numa

			 * PTEs, scan up to virtpages, to skip through those

			 * areas faster.

	/*

	 * It is possible to reach the end of the VMA list but the last few

	 * VMAs are not guaranteed to the vma_migratable. If they are not, we

	 * would find the !migratable VMA on the next scan but not reset the

	 * scanner to the start so check it now.

	/*

	 * Make sure tasks use at least 32x as much time to run other code

	 * than they used here, to limit NUMA PTE scanning overhead to 3% max.

	 * Usually update_task_scan_period slows down scanning enough; on an

	 * overloaded system we need to limit overhead on a per task basis.

 Protect against double add, see task_tick_numa and task_numa_work */

 New address space, reset the preferred nid */

	/*

	 * New thread, keep existing numa_preferred_nid which should be copied

	 * already by arch_dup_task_struct but stagger when scans start.

/*

 * Drive the periodic memory faults..

	/*

	 * We don't care about NUMA placement if we don't have memory.

	/*

	 * Using runtime rather than walltime has the dual advantage that

	 * we (mostly) drive the selection from busy threads and that the

	 * task needs to have done some actual work before we bother with

	 * NUMA placement.

	/*

	 * Allow resets if faults have been trapped before one scan

	 * has completed. This is most likely due to a new task that

	 * is pulled cross-node due to wakeups or load balancing.

		/*

		 * Avoid scan adjustments if moving to the preferred

		 * node or if the task was not previously running on

		 * the preferred node.

 CONFIG_NUMA_BALANCING */

/*

 * Signed add and clamp on underflow.

 *

 * Explicitly do a load-store to ensure the intermediate value never hits

 * memory. This allows lockless observations without ever seeing the negative

 * values.

/*

 * Unsigned subtract and clamp on underflow.

 *

 * Explicitly do a load-store to ensure the intermediate value never hits

 * memory. This allows lockless observations without ever seeing the negative

 * values.

/*

 * Remove and clamp on negative, from a local variable.

 *

 * A variant of sub_positive(), which does not use explicit load-store

 * and is thus optimized for local variable updates.

 commit outstanding execution time */

/*

 * All this does is approximate the hierarchical proportion which includes that

 * global sum we all love to hate.

 *

 * That is, the weight of a group entity, is the proportional share of the

 * group weight based on the group runqueue weights. That is:

 *

 *                     tg->weight * grq->load.weight

 *   ge->load.weight = -----------------------------               (1)

 *                       \Sum grq->load.weight

 *

 * Now, because computing that sum is prohibitively expensive to compute (been

 * there, done that) we approximate it with this average stuff. The average

 * moves slower and therefore the approximation is cheaper and more stable.

 *

 * So instead of the above, we substitute:

 *

 *   grq->load.weight -> grq->avg.load_avg                         (2)

 *

 * which yields the following:

 *

 *                     tg->weight * grq->avg.load_avg

 *   ge->load.weight = ------------------------------              (3)

 *                             tg->load_avg

 *

 * Where: tg->load_avg ~= \Sum grq->avg.load_avg

 *

 * That is shares_avg, and it is right (given the approximation (2)).

 *

 * The problem with it is that because the average is slow -- it was designed

 * to be exactly that of course -- this leads to transients in boundary

 * conditions. In specific, the case where the group was idle and we start the

 * one task. It takes time for our CPU's grq->avg.load_avg to build up,

 * yielding bad latency etc..

 *

 * Now, in that special case (1) reduces to:

 *

 *                     tg->weight * grq->load.weight

 *   ge->load.weight = ----------------------------- = tg->weight   (4)

 *                         grp->load.weight

 *

 * That is, the sum collapses because all other CPUs are idle; the UP scenario.

 *

 * So what we do is modify our approximation (3) to approach (4) in the (near)

 * UP case, like:

 *

 *   ge->load.weight =

 *

 *              tg->weight * grq->load.weight

 *     ---------------------------------------------------         (5)

 *     tg->load_avg - grq->avg.load_avg + grq->load.weight

 *

 * But because grq->load.weight can drop to 0, resulting in a divide by zero,

 * we need to use grq->avg.load_avg as its lower bound, which then gives:

 *

 *

 *                     tg->weight * grq->load.weight

 *   ge->load.weight = -----------------------------		   (6)

 *                             tg_load_avg'

 *

 * Where:

 *

 *   tg_load_avg' = tg->load_avg - grq->avg.load_avg +

 *                  max(grq->load.weight, grq->avg.load_avg)

 *

 * And that is shares_weight and is icky. In the (near) UP case it approaches

 * (4) while in the normal case it approaches (3). It consistently

 * overestimates the ge->load.weight and therefore:

 *

 *   \Sum ge->load.weight >= tg->weight

 *

 * hence icky!

 Ensure tg_weight >= load */

	/*

	 * MIN_SHARES has to be unscaled here to support per-CPU partitioning

	 * of a group with small tg->shares value. It is a floor value which is

	 * assigned as a minimum load.weight to the sched_entity representing

	 * the group on a CPU.

	 *

	 * E.g. on 64-bit for a group with tg->shares of scale_load(15)=15*1024

	 * on an 8-core system with 8 tasks each runnable on one CPU shares has

	 * to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In

	 * case no task is runnable on a CPU MIN_SHARES=2 should be returned

	 * instead of 0.

 CONFIG_SMP */

/*

 * Recomputes the group entity based on the current state of its group

 * runqueue.

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_FAIR_GROUP_SCHED */

		/*

		 * There are a few boundary cases this might miss but it should

		 * get called often enough that that should (hopefully) not be

		 * a real problem.

		 *

		 * It will not get called when we go idle, because the idle

		 * thread is a different class (!fair), nor will the utilization

		 * number include things like RT tasks.

		 *

		 * As is, the util number is not freq-invariant (we'd have to

		 * implement arch_scale_freq_capacity() for that).

		 *

		 * See cpu_util().

/*

 * Because list_add_leaf_cfs_rq always places a child cfs_rq on the list

 * immediately before a parent cfs_rq, and cfs_rqs are removed from the list

 * bottom-up, we only have to test whether the cfs_rq before us on the list

 * is our child.

 * If cfs_rq is not on the list, test whether a child needs its to be added to

 * connect a branch to the tree  * (see list_add_leaf_cfs_rq() for details).

	/*

	 * _avg must be null when _sum are null because _avg = _sum / divider

	 * Make sure that rounding and/or propagation of PELT values never

	 * break this.

/**

 * update_tg_load_avg - update the tg's load avg

 * @cfs_rq: the cfs_rq whose avg changed

 *

 * This function 'ensures': tg->load_avg := \Sum tg->cfs_rq[]->avg.load.

 * However, because tg->load_avg is a global value there are performance

 * considerations.

 *

 * In order to avoid having to look at the other cfs_rq's, we use a

 * differential update where we store the last value we propagated. This in

 * turn allows skipping updates if the differential is 'small'.

 *

 * Updating tg's load_avg is necessary before update_cfs_share().

	/*

	 * No need to update load_avg for root_task_group as it is not used.

/*

 * Called within set_task_rq() right before setting a task's CPU. The

 * caller only guarantees p->pi_lock is held; no other assumptions,

 * including the state of rq->lock, should be made.

	/*

	 * We are supposed to update the task to "current" time, then its up to

	 * date and ready to go to new CPU/cfs_rq. But we have difficulty in

	 * getting what current time is, so simply throw away the out-of-date

	 * time. This will result in the wakee task is less decayed, but giving

	 * the wakee more load sounds not bad.

/*

 * When on migration a sched_entity joins/leaves the PELT hierarchy, we need to

 * propagate its contribution. The key to this propagation is the invariant

 * that for each group:

 *

 *   ge->avg == grq->avg						(1)

 *

 * _IFF_ we look at the pure running and runnable sums. Because they

 * represent the very same entity, just at different points in the hierarchy.

 *

 * Per the above update_tg_cfs_util() and update_tg_cfs_runnable() are trivial

 * and simply copies the running/runnable sum over (but still wrong, because

 * the group entity and group rq do not have their PELT windows aligned).

 *

 * However, update_tg_cfs_load() is more complex. So we have:

 *

 *   ge->avg.load_avg = ge->load.weight * ge->avg.runnable_avg		(2)

 *

 * And since, like util, the runnable part should be directly transferable,

 * the following would _appear_ to be the straight forward approach:

 *

 *   grq->avg.load_avg = grq->load.weight * grq->avg.runnable_avg	(3)

 *

 * And per (1) we have:

 *

 *   ge->avg.runnable_avg == grq->avg.runnable_avg

 *

 * Which gives:

 *

 *                      ge->load.weight * grq->avg.load_avg

 *   ge->avg.load_avg = -----------------------------------		(4)

 *                               grq->load.weight

 *

 * Except that is wrong!

 *

 * Because while for entities historical weight is not important and we

 * really only care about our future and therefore can consider a pure

 * runnable sum, runqueues can NOT do this.

 *

 * We specifically want runqueues to have a load_avg that includes

 * historical weights. Those represent the blocked load, the load we expect

 * to (shortly) return to us. This only works by keeping the weights as

 * integral part of the sum. We therefore cannot decompose as per (3).

 *

 * Another reason this doesn't work is that runnable isn't a 0-sum entity.

 * Imagine a rq with 2 tasks that each are runnable 2/3 of the time. Then the

 * rq itself is runnable anywhere between 2/3 and 1 depending on how the

 * runnable section of these tasks overlap (or not). If they were to perfectly

 * align the rq as a whole would be runnable 2/3 of the time. If however we

 * always have at least 1 runnable task, the rq as a whole is always runnable.

 *

 * So we'll have to approximate.. :/

 *

 * Given the constraint:

 *

 *   ge->avg.running_sum <= ge->avg.runnable_sum <= LOAD_AVG_MAX

 *

 * We can construct a rule that adds runnable to a rq by assuming minimal

 * overlap.

 *

 * On removal, we'll assume each task is equally runnable; which yields:

 *

 *   grq->avg.runnable_sum = grq->avg.load_sum / grq->load.weight

 *

 * XXX: only do this for the part of runnable > running ?

 *

 Nothing to update */

	/*

	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.

	 * See ___update_load_avg() for details.

 Set new sched_entity's utilization */

 Update parent cfs_rq utilization */

 Nothing to update */

	/*

	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.

	 * See ___update_load_avg() for details.

 Set new sched_entity's runnable */

 Update parent cfs_rq runnable */

	/*

	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.

	 * See ___update_load_avg() for details.

		/*

		 * Add runnable; clip at LOAD_AVG_MAX. Reflects that until

		 * the CPU is saturated running == runnable.

		/*

		 * Estimate the new unweighted runnable_sum of the gcfs_rq by

		 * assuming all tasks are equally runnable.

 But make sure to not inflate se's runnable */

	/*

	 * runnable_sum can't be lower than running_sum

	 * Rescale running sum to be in the same range as runnable sum

	 * running_sum is in [0 : LOAD_AVG_MAX <<  SCHED_CAPACITY_SHIFT]

	 * runnable_sum is in [0 : LOAD_AVG_MAX]

 Update task and its cfs_rq load average */

/*

 * Check if we need to update the load and the utilization of a blocked

 * group_entity:

	/*

	 * If sched_entity still have not zero load or utilization, we have to

	 * decay it:

	/*

	 * If there is a pending propagation, we have to update the load and

	 * the utilization of the sched_entity:

	/*

	 * Otherwise, the load and the utilization of the sched_entity is

	 * already zero and there is no pending propagation, so it will be a

	 * waste of time to try to decay it:

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_FAIR_GROUP_SCHED */

/**

 * update_cfs_rq_load_avg - update the cfs_rq's load/util averages

 * @now: current time, as per cfs_rq_clock_pelt()

 * @cfs_rq: cfs_rq to update

 *

 * The cfs_rq avg is the direct sum of all its entities (blocked and runnable)

 * avg. The immediate corollary is that all (fair) tasks must be attached, see

 * post_init_entity_util_avg().

 *

 * cfs_rq->avg is used for task_h_load() and update_cfs_share() for example.

 *

 * Returns true if the load decayed or we removed load.

 *

 * Since both these conditions indicate a changed cfs_rq->avg.load we should

 * call update_tg_load_avg() when this function returns true.

		/*

		 * removed_runnable is the unweighted version of removed_load so we

		 * can use it to estimate removed_load_sum.

/**

 * attach_entity_load_avg - attach this entity to its cfs_rq load avg

 * @cfs_rq: cfs_rq to attach to

 * @se: sched_entity to attach

 *

 * Must call update_cfs_rq_load_avg() before this, since we rely on

 * cfs_rq->avg.last_update_time being current.

	/*

	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.

	 * See ___update_load_avg() for details.

	/*

	 * When we attach the @se to the @cfs_rq, we must align the decay

	 * window because without that, really weird and wonderful things can

	 * happen.

	 *

	 * XXX illustrate

	/*

	 * Hell(o) Nasty stuff.. we need to recompute _sum based on the new

	 * period_contrib. This isn't strictly correct, but since we're

	 * entirely outside of the PELT hierarchy, nobody cares if we truncate

	 * _sum a little.

/**

 * detach_entity_load_avg - detach this entity from its cfs_rq load avg

 * @cfs_rq: cfs_rq to detach from

 * @se: sched_entity to detach

 *

 * Must call update_cfs_rq_load_avg() before this, since we rely on

 * cfs_rq->avg.last_update_time being current.

	/*

	 * cfs_rq->avg.period_contrib can be used for both cfs_rq and se.

	 * See ___update_load_avg() for details.

/*

 * Optional action to be done while updating the load average

 Update task and its cfs_rq load average */

	/*

	 * Track task load average for carrying it to new CPU after migrated, and

	 * track group sched_entity load average for task_h_load calc in migration

		/*

		 * DO_ATTACH means we're here from enqueue_entity().

		 * !last_update_time means we've passed through

		 * migrate_task_rq_fair() indicating we migrated.

		 *

		 * IOW we're enqueueing a task on a new CPU.

/*

 * Synchronize entity load avg of dequeued entity without locking

 * the previous rq.

/*

 * Task first catches up with cfs_rq, and then subtract

 * itself from the cfs_rq (task must be off the queue now).

	/*

	 * tasks cannot exit without having gone through wake_up_new_task() ->

	 * post_init_entity_util_avg() which will have added things to the

	 * cfs_rq, so we can remove unconditionally.

 Update root cfs_rq's estimated utilization */

 Update root cfs_rq's estimated utilization */

/*

 * Check if a (signed) value is within a specified (unsigned) margin,

 * based on the observation that:

 *

 *     abs(x) < y := (unsigned)(x + y - 1) < (2 * y - 1)

 *

 * NOTE: this only works when value + margin < INT_MAX.

	/*

	 * Skip update of task's estimated utilization when the task has not

	 * yet completed an activation, e.g. being migrated.

	/*

	 * If the PELT values haven't changed since enqueue time,

	 * skip the util_est update.

	/*

	 * Reset EWMA on utilization increases, the moving average is used only

	 * to smooth utilization decreases.

	/*

	 * Skip update of task's estimated utilization when its members are

	 * already ~1% close to its last activation value.

	/*

	 * To avoid overestimation of actual task utilization, skip updates if

	 * we cannot grant there is idle time in this CPU.

	/*

	 * Update Task's estimated utilization

	 *

	 * When *p completes an activation we can consolidate another sample

	 * of the task size. This is done by storing the current PELT value

	 * as ue.enqueued and by using this value to update the Exponential

	 * Weighted Moving Average (EWMA):

	 *

	 *  ewma(t) = w *  task_util(p) + (1-w) * ewma(t-1)

	 *          = w *  task_util(p) +         ewma(t-1)  - w * ewma(t-1)

	 *          = w * (task_util(p) -         ewma(t-1)) +     ewma(t-1)

	 *          = w * (      last_ewma_diff            ) +     ewma(t-1)

	 *          = w * (last_ewma_diff  +  ewma(t-1) / w)

	 *

	 * Where 'w' is the weight of new samples, which is configured to be

	 * 0.25, thus making w=1/4 ( >>= UTIL_EST_WEIGHT_SHIFT)

	/*

	 * Make sure that misfit_task_load will not be null even if

	 * task_h_load() returns 0.

 CONFIG_SMP */

 CONFIG_SMP */

	/*

	 * The 'current' period is already promised to the current tasks,

	 * however the extra weight of the new task will slow them down a

	 * little, place the new task so that it fits in the slot that

	 * stays open at the end.

 sleeps up to a single latency don't count. */

		/*

		 * Halve their sleep time's effect, to allow

		 * for a gentler effect of sleepers:

 ensure we never gain time by being placed backwards. */

/*

 * MIGRATION

 *

 *	dequeue

 *	  update_curr()

 *	    update_min_vruntime()

 *	  vruntime -= min_vruntime

 *

 *	enqueue

 *	  update_curr()

 *	    update_min_vruntime()

 *	  vruntime += min_vruntime

 *

 * this way the vruntime transition between RQs is done when both

 * min_vruntime are up-to-date.

 *

 * WAKEUP (remote)

 *

 *	->migrate_task_rq_fair() (p->state == TASK_WAKING)

 *	  vruntime -= min_vruntime

 *

 *	enqueue

 *	  update_curr()

 *	    update_min_vruntime()

 *	  vruntime += min_vruntime

 *

 * this way we don't have the most up-to-date min_vruntime on the originating

 * CPU and an up-to-date min_vruntime on the destination CPU.

	/*

	 * If we're the current task, we must renormalise before calling

	 * update_curr().

	/*

	 * Otherwise, renormalise after, such that we're placed at the current

	 * moment in time, instead of some random moment in the past. Being

	 * placed in the past could significantly boost this task to the

	 * fairness detriment of existing tasks.

	/*

	 * When enqueuing a sched_entity, we must:

	 *   - Update loads to have both entity and cfs_rq synced with now.

	 *   - Add its load to cfs_rq->runnable_avg

	 *   - For group_entity, update its weight to reflect the new share of

	 *     its group cfs_rq

	 *   - Add its new weight to cfs_rq->load.weight

	/*

	 * When bandwidth control is enabled, cfs might have been removed

	 * because of a parent been throttled but cfs->nr_running > 1. Try to

	 * add it unconditionally.

	/*

	 * Update run-time statistics of the 'current'.

	/*

	 * When dequeuing a sched_entity, we must:

	 *   - Update loads to have both entity and cfs_rq synced with now.

	 *   - Subtract its load from the cfs_rq->runnable_avg.

	 *   - Subtract its previous weight from cfs_rq->load.weight.

	 *   - For group entity, update its weight to reflect the new share

	 *     of its group cfs_rq.

	/*

	 * Normalize after update_curr(); which will also have moved

	 * min_vruntime if @se is the one holding it back. But before doing

	 * update_min_vruntime() again, which will discount @se's position and

	 * can move min_vruntime forward still more.

 return excess runtime on last dequeue */

	/*

	 * Now advance min_vruntime if @se was the entity holding it back,

	 * except when: DEQUEUE_SAVE && !DEQUEUE_MOVE, in this case we'll be

	 * put back on, and if we advance min_vruntime, we'll be placed back

	 * further than we started -- ie. we'll be penalized.

/*

 * Preempt the current task with a newly woken task if needed:

		/*

		 * The current task ran long enough, ensure it doesn't get

		 * re-elected due to buddy favours.

	/*

	 * Ensure that a task that missed wakeup preemption by a

	 * narrow margin doesn't have to wait for a full slice.

	 * This also mitigates buddy induced latencies under load.

 'current' is not kept within the tree. */

		/*

		 * Any task has to be enqueued before it get to execute on

		 * a CPU. So account for the time it spent waiting on the

		 * runqueue.

	/*

	 * Track our maximum slice length, if the CPU's load is at

	 * least twice that of our own weight (i.e. dont track it

	 * when there are only lesser-weight tasks around):

/*

 * Pick the next process, keeping these things in mind, in this order:

 * 1) keep things fair between processes/task groups

 * 2) pick the "next" process, since someone really wants that to run

 * 3) pick the "last" process, for cache locality

 * 4) do not run the "skip" process, if something else is available

	/*

	 * If curr is set we have to see if its left of the leftmost entity

	 * still in the tree, provided there was anything in the tree at all.

 ideally we run the leftmost entity */

	/*

	 * Avoid running the skip buddy, if running something else can

	 * be done without getting too unfair.

		/*

		 * Someone really wants this to run. If it's not unfair, run it.

		/*

		 * Prefer last buddy, try to return the CPU to a preempted task.

	/*

	 * If still on the runqueue then deactivate_task()

	 * was not called and update_curr() has to be done:

 throttle cfs_rqs exceeding runtime */

 Put 'current' back into the tree. */

 in !on_rq case, update occurred at dequeue */

	/*

	 * Update run-time statistics of the 'current'.

	/*

	 * Ensure that runnable average is periodically updated.

	/*

	 * queued ticks are scheduled to match the slice, so don't bother

	 * validating it and just reschedule.

	/*

	 * don't let the period tick interfere with the hrtick preemption

/**************************************************

 * CFS bandwidth control machinery

 CONFIG_JUMP_LABEL */

 CONFIG_JUMP_LABEL */

/*

 * default period for cfs group bandwidth.

 * default: 0.1s, units: nanoseconds

/*

 * Replenish runtime according to assigned quota. We use sched_clock_cpu

 * directly instead of rq->clock to avoid adding additional synchronization

 * around rq->lock.

 *

 * requires cfs_b->lock

 returns 0 on failure to allocate runtime */

 note: this is a positive sum as runtime_remaining <= 0 */

 returns 0 on failure to allocate runtime */

 dock delta_exec before expiring quota (as it could span periods) */

	/*

	 * if we're unable to extend our runtime we resched so that the active

	 * hierarchy can be throttled

 check whether cfs_rq, or any parent, is throttled */

/*

 * Ensure that neither of the group entities corresponding to src_cpu or

 * dest_cpu are members of a throttled hierarchy when performing group

 * load-balance operations.

 Add cfs_rq with load or one or more already running entities to the list */

 group is entering throttled state, stop time */

 This will start the period timer if necessary */

		/*

		 * We have raced with bandwidth becoming available, and if we

		 * actually throttled the timer might not unthrottle us for an

		 * entire period. We additionally needed to make sure that any

		 * subsequent check_cfs_rq_runtime calls agree not to throttle

		 * us, as we may commit to do cfs put_prev+pick_next, so we ask

		 * for 1ns of runtime rather than just check cfs_b.

 Throttle no longer required. */

 freeze hierarchy runnable averages while throttled */

 throttled entity or throttle-on-deactivate */

 Avoid re-evaluating load for this entity: */

 throttled entity or throttle-on-deactivate */

 At this point se is NULL and we are at root level*/

	/*

	 * Note: distribution will already see us throttled via the

	 * throttled-list.  rq->lock protects completion.

 update hierarchical throttle state */

 Nothing to run but something to decay (on_list)? Complete the branch */

 end evaluation on encountering a throttled cfs_rq */

 end evaluation on encountering a throttled cfs_rq */

		/*

		 * One parent has been throttled and cfs_rq removed from the

		 * list. Add it back to not break the leaf list.

 At this point se is NULL and we are at root level*/

	/*

	 * The cfs_rq_throttled() breaks in the above iteration can result in

	 * incomplete leaf list maintenance, resulting in triggering the

	 * assertion below.

 Determine whether we need to wake up potentially idle CPU: */

 By the above check, this should never be true */

 we check whether we're throttled above */

/*

 * Responsible for refilling a task_group's bandwidth and unthrottling its

 * cfs_rqs as appropriate. If there has been no activity within the last

 * period the timer is deactivated until scheduling resumes; cfs_b->idle is

 * used to track this state.

 no need to continue the timer with no bandwidth constraint */

 Refill extra burst quota even if cfs_b->idle */

	/*

	 * idle depends on !throttled (for the case of a large deficit), and if

	 * we're going inactive then everything else can be deferred

 mark as potentially idle for the upcoming period */

 account preceding periods in which throttling occurred */

	/*

	 * This check is repeated as we release cfs_b->lock while we unthrottle.

 we can't nest cfs_b->lock while distributing bandwidth */

	/*

	 * While we are ensured activity in the period following an

	 * unthrottle, this also covers the case in which the new bandwidth is

	 * insufficient to cover the existing bandwidth deficit.  (Forcing the

	 * timer to remain active while there are any throttled entities.)

 a cfs_rq won't donate quota below this amount */

 minimum remaining period time to redistribute slack quota */

 how long we wait to gather additional slack before distributing */

/*

 * Are we near the end of the current quota period?

 *

 * Requires cfs_b->lock for hrtimer_expires_remaining to be safe against the

 * hrtimer base being cleared by hrtimer_start. In the case of

 * migrate_hrtimers, base is never cleared, so we are fine.

 if the call-back is running a quota refresh is already occurring */

 is a quota refresh about to occur? */

 if there's a quota refresh soon don't bother with slack */

 don't push forwards an existing deferred unthrottle */

 we know any runtime found here is valid as update_curr() precedes return */

 we are under rq->lock, defer unthrottling using a timer */

 even if it's not valid for return we don't want to try again */

/*

 * This is done with a timer (instead of inline with bandwidth return) since

 * it's necessary to juggle rq->locks to unthrottle their respective cfs_rqs.

 confirm we're still not at a refresh boundary */

/*

 * When a group wakes up we want to make sure that its quota is not already

 * expired/exceeded, otherwise it may be allowed to steal additional ticks of

 * runtime as update_curr() throttling can not trigger until it's on-rq.

 an active group must be handled by the update_curr()->put() path */

 ensure the group is not already throttled */

 update runtime allocation */

 conditionally throttle active cfs_rq's from put_prev_entity() */

	/*

	 * it's possible for a throttled entity to be forced into a running

	 * state (e.g. set_curr_task), in this case we're finished.

			/*

			 * Grow period by a factor of 2 to avoid losing precision.

			 * Precision loss in the quota/period ratio can cause __cfs_schedulable

			 * to fail.

 reset count so we don't come right back in here */

 init_cfs_bandwidth() was not called */

/*

 * Both these CPU hotplug callbacks race against unregister_fair_sched_group()

 *

 * The race is harmless, since modifying bandwidth settings of unhooked group

 * bits doesn't do much.

 cpu online callback */

 cpu offline callback */

		/*

		 * clock_task is not advancing so we just need to make sure

		 * there's some valid quota amount

		/*

		 * Offline rq is schedulable till CPU is completely disabled

		 * in take_cpu_down(), so we prevent new cfs throttling here.

 CONFIG_CFS_BANDWIDTH */

 CONFIG_CFS_BANDWIDTH */

/**************************************************

 * CFS operations on tasks:

/*

 * called from enqueue/dequeue and updates the hrtick when the

 * current task is from our class and nr_running is low enough

 * to matter.

 !CONFIG_SCHED_HRTICK */

 Runqueue only has SCHED_IDLE tasks enqueued */

/*

 * Returns true if cfs_rq only has SCHED_IDLE entities enqueued. Note the use

 * of idle_nr_running, which does not consider idle descendants of normal

 * entities.

/*

 * The enqueue_task method is called before nr_running is

 * increased. Here we update the fair scheduling stats and

 * then put the task into the rbtree:

	/*

	 * The code below (indirectly) updates schedutil which looks at

	 * the cfs_rq utilization to select a frequency.

	 * Let's add the task's estimated utilization to the cfs_rq's

	 * estimated utilization, before we update schedutil.

	/*

	 * If in_iowait is set, the code below may not trigger any cpufreq

	 * utilization updates, so do it here explicitly with the IOWAIT flag

	 * passed.

 end evaluation on encountering a throttled cfs_rq */

 end evaluation on encountering a throttled cfs_rq */

               /*

                * One parent has been throttled and cfs_rq removed from the

                * list. Add it back to not break the leaf list.

 At this point se is NULL and we are at root level*/

	/*

	 * Since new tasks are assigned an initial util_avg equal to

	 * half of the spare capacity of their CPU, tiny tasks have the

	 * ability to cross the overutilized threshold, which will

	 * result in the load balancer ruining all the task placement

	 * done by EAS. As a way to mitigate that effect, do not account

	 * for the first enqueue operation of new tasks during the

	 * overutilized flag detection.

	 *

	 * A better way of solving this problem would be to wait for

	 * the PELT signals of tasks to converge before taking them

	 * into account, but that is not straightforward to implement,

	 * and the following generally works well enough in practice.

		/*

		 * When bandwidth control is enabled; the cfs_rq_throttled()

		 * breaks in the above iteration can result in incomplete

		 * leaf list maintenance, resulting in triggering the assertion

		 * below.

/*

 * The dequeue_task method is called before nr_running is

 * decreased. We remove the task from the rbtree and

 * update the fair scheduling stats:

 end evaluation on encountering a throttled cfs_rq */

 Don't dequeue parent if it has other entities besides us */

 Avoid re-evaluating load for this entity: */

			/*

			 * Bias pick_next to pick a task from this cfs_rq, as

			 * p is sleeping when it is within its sched_slice.

 end evaluation on encountering a throttled cfs_rq */

 At this point se is NULL and we are at root level*/

 balance early to pull high priority tasks */

 Working cpumask for: load_balance, load_balance_newidle. */

 Idle CPUS has blocked load */

 Newly idle CPUs need their next_balance collated */

 in jiffy units */

 Next update of blocked load in jiffies */

 CONFIG_NO_HZ_COMMON */

/*

 * cpu_load_without - compute CPU load without any contributions from *p

 * @cpu: the CPU which load is requested

 * @p: the task which load should be discounted

 *

 * The load of a CPU is defined by the load of tasks currently enqueued on that

 * CPU as well as tasks which are currently sleeping after an execution on that

 * CPU.

 *

 * This method returns the load of the specified CPU by discounting the load of

 * the specified task, whenever the task is currently contributing to the CPU

 * load.

 Task has no contribution or is new */

 Discount task's util from CPU's util */

 Task has no contribution or is new */

 Discount task's runnable from CPU's runnable */

	/*

	 * Only decay a single time; tasks that have less then 1 wakeup per

	 * jiffy will not have built up many flips.

/*

 * Detect M:N waker/wakee relationships via a switching-frequency heuristic.

 *

 * A waker of many should wake a different task than the one last awakened

 * at a frequency roughly N times higher than one of its wakees.

 *

 * In order to determine whether we should let the load spread vs consolidating

 * to shared cache, we look for a minimum 'flip' frequency of llc_size in one

 * partner, and a factor of lls_size higher frequency in the other.

 *

 * With both conditions met, we can be relatively sure that the relationship is

 * non-monogamous, with partner count exceeding socket size.

 *

 * Waker/wakee being client/server, worker/dispatcher, interrupt source or

 * whatever is irrelevant, spread criteria is apparent partner count exceeds

 * socket size.

/*

 * The purpose of wake_affine() is to quickly determine on which CPU we can run

 * soonest. For the purpose of speed we only consider the waking and previous

 * CPU.

 *

 * wake_affine_idle() - only considers 'now', it check if the waking CPU is

 *			cache-affine and is (or	will be) idle.

 *

 * wake_affine_weight() - considers the weight to reflect the average

 *			  scheduling latency of the CPUs. This seems to work

 *			  for the overloaded case.

	/*

	 * If this_cpu is idle, it implies the wakeup is from interrupt

	 * context. Only allow the move if cache is shared. Otherwise an

	 * interrupt intensive workload could force all tasks onto one

	 * node depending on the IO topology or IRQ affinity settings.

	 *

	 * If the prev_cpu is idle and cache affine then avoid a migration.

	 * There is no guarantee that the cache hot data from an interrupt

	 * is more important than cache hot data on the prev_cpu and from

	 * a cpufreq perspective, it's better to have higher utilisation

	 * on one CPU.

	/*

	 * If sync, adjust the weight of prev_eff_load such that if

	 * prev_eff == this_eff that select_idle_sibling() will consider

	 * stacking the wakee on top of the waker if no other CPU is

	 * idle.

/*

 * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group.

 Check if we have any choice: */

 Traverse only the allowed CPUs */

				/*

				 * We give priority to a CPU whose idle state

				 * has the smallest exit latency irrespective

				 * of any idle timestamp.

				/*

				 * If equal or no active idle state, then

				 * the most recently idled CPU might have

				 * a warmer cache.

	/*

	 * We need task's util for cpu_util_without, sync it up to

	 * prev_cpu's last_update_time.

 Now try balancing at a lower domain level of 'cpu': */

 Now try balancing at a lower domain level of 'new_cpu': */

/*

 * Scans the local SMT mask to see if the entire core is idle, and records this

 * information in sd_llc_shared->has_idle_cores.

 *

 * Since SMT siblings share all cache levels, inspecting this limited remote

 * state should be fairly cheap.

/*

 * Scan the entire LLC domain for idle cores; this dynamically switches off if

 * there are no idle cores left in the system; tracked through

 * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above.

/*

 * Scan the local SMT mask for idle CPUs.

 CONFIG_SCHED_SMT */

 CONFIG_SCHED_SMT */

/*

 * Scan the LLC domain for idle CPUs; this is dynamically regulated by

 * comparing the average scan cost (tracked in sd->avg_scan_cost) against the

 * average idle time for this rq (as found in rq->avg_idle).

		/*

		 * If we're busy, the assumption that the last idle period

		 * predicts the future is flawed; age away the remaining

		 * predicted idle time.

		/*

		 * Account for the scan cost of wakeups against the average

		 * idle time.

/*

 * Scan the asym_capacity domain for idle CPUs; pick the first idle one on which

 * the task fits. If no CPU is big enough, but there are idle ones, try to

 * maximize capacity.

/*

 * Try and locate an idle core/thread in the LLC cache domain.

	/*

	 * On asymmetric system, update task utilization because we will check

	 * that the task fits with cpu's capacity.

	/*

	 * per-cpu select_idle_mask usage

	/*

	 * If the previous CPU is cache affine and idle, don't be stupid:

	/*

	 * Allow a per-cpu kthread to stack with the wakee if the

	 * kworker thread and the tasks previous CPUs are the same.

	 * The assumption is that the wakee queued work for the

	 * per-cpu kthread that is now complete and the wakeup is

	 * essentially a sync wakeup. An obvious example of this

	 * pattern is IO completions.

 Check a recently used CPU as a potential idle candidate: */

	/*

	 * For asymmetric CPU capacity systems, our domain of interest is

	 * sd_asym_cpucapacity rather than sd_llc.

		/*

		 * On an asymmetric CPU capacity system where an exclusive

		 * cpuset defines a symmetric island (i.e. one unique

		 * capacity_orig value through the cpuset), the key will be set

		 * but the CPUs within that cpuset will not have a domain with

		 * SD_ASYM_CPUCAPACITY. These should follow the usual symmetric

		 * capacity path.

/**

 * cpu_util - Estimates the amount of capacity of a CPU used by CFS tasks.

 * @cpu: the CPU to get the utilization of

 *

 * The unit of the return value must be the one of capacity so we can compare

 * the utilization with the capacity of the CPU that is available for CFS task

 * (ie cpu_capacity).

 *

 * cfs_rq.avg.util_avg is the sum of running time of runnable tasks plus the

 * recent utilization of currently non-runnable tasks on a CPU. It represents

 * the amount of utilization of a CPU in the range [0..capacity_orig] where

 * capacity_orig is the cpu_capacity available at the highest frequency

 * (arch_scale_freq_capacity()).

 * The utilization of a CPU converges towards a sum equal to or less than the

 * current capacity (capacity_curr <= capacity_orig) of the CPU because it is

 * the running time on this CPU scaled by capacity_curr.

 *

 * The estimated utilization of a CPU is defined to be the maximum between its

 * cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks

 * currently RUNNABLE on that CPU.

 * This allows to properly represent the expected utilization of a CPU which

 * has just got a big task running since a long sleep period. At the same time

 * however it preserves the benefits of the "blocked utilization" in

 * describing the potential for other tasks waking up on the same CPU.

 *

 * Nevertheless, cfs_rq.avg.util_avg can be higher than capacity_curr or even

 * higher than capacity_orig because of unfortunate rounding in

 * cfs.avg.util_avg or just after migrating tasks and new task wakeups until

 * the average stabilizes with the new running time. We need to check that the

 * utilization stays within the range of [0..capacity_orig] and cap it if

 * necessary. Without utilization capping, a group could be seen as overloaded

 * (CPU0 utilization at 121% + CPU1 utilization at 80%) whereas CPU1 has 20% of

 * available capacity. We allow utilization to overshoot capacity_curr (but not

 * capacity_orig) as it useful for predicting the capacity required after task

 * migrations (scheduler-driven DVFS).

 *

 * Return: the (estimated) utilization for the specified CPU

/*

 * cpu_util_without: compute cpu utilization without any contributions from *p

 * @cpu: the CPU which utilization is requested

 * @p: the task which utilization should be discounted

 *

 * The utilization of a CPU is defined by the utilization of tasks currently

 * enqueued on that CPU as well as tasks which are currently sleeping after an

 * execution on that CPU.

 *

 * This method returns the utilization of the specified CPU by discounting the

 * utilization of the specified task, whenever the task is currently

 * contributing to the CPU utilization.

 Task has no contribution or is new */

 Discount task's util from CPU's util */

	/*

	 * Covered cases:

	 *

	 * a) if *p is the only task sleeping on this CPU, then:

	 *      cpu_util (== task_util) > util_est (== 0)

	 *    and thus we return:

	 *      cpu_util_without = (cpu_util - task_util) = 0

	 *

	 * b) if other tasks are SLEEPING on this CPU, which is now exiting

	 *    IDLE, then:

	 *      cpu_util >= task_util

	 *      cpu_util > util_est (== 0)

	 *    and thus we discount *p's blocked utilization to return:

	 *      cpu_util_without = (cpu_util - task_util) >= 0

	 *

	 * c) if other tasks are RUNNABLE on that CPU and

	 *      util_est > cpu_util

	 *    then we use util_est since it returns a more restrictive

	 *    estimation of the spare capacity on that CPU, by just

	 *    considering the expected utilization of tasks already

	 *    runnable on that CPU.

	 *

	 * Cases a) and b) are covered by the above code, while case c) is

	 * covered by the following code when estimated utilization is

	 * enabled.

		/*

		 * Despite the following checks we still have a small window

		 * for a possible race, when an execl's select_task_rq_fair()

		 * races with LB's detach_task():

		 *

		 *   detach_task()

		 *     p->on_rq = TASK_ON_RQ_MIGRATING;

		 *     ---------------------------------- A

		 *     deactivate_task()                   \

		 *       dequeue_task()                     + RaceTime

		 *         util_est_dequeue()              /

		 *     ---------------------------------- B

		 *

		 * The additional check on "current == p" it's required to

		 * properly fix the execl regression and it helps in further

		 * reducing the chances for the above race.

	/*

	 * Utilization (estimated) can exceed the CPU capacity, thus let's

	 * clamp to the maximum CPU capacity to ensure consistency with

	 * the cpu_util call.

/*

 * Predicts what cpu_util(@cpu) would return if @p was migrated (and enqueued)

 * to @dst_cpu.

	/*

	 * If @p migrates from @cpu to another, remove its contribution. Or,

	 * if @p migrates from another CPU to @cpu, add its contribution. In

	 * the other cases, @cpu is not impacted by the migration, so the

	 * util_avg should already be correct.

		/*

		 * During wake-up, the task isn't enqueued yet and doesn't

		 * appear in the cfs_rq->avg.util_est.enqueued of any rq,

		 * so just add it (if needed) to "simulate" what will be

		 * cpu_util() after the task has been enqueued.

/*

 * compute_energy(): Estimates the energy that @pd would consume if @p was

 * migrated to @dst_cpu. compute_energy() predicts what will be the utilization

 * landscape of @pd's CPUs after the task migration, and uses the Energy Model

 * to compute what would be the energy if we decided to actually migrate that

 * task.

	/*

	 * The capacity state of CPUs of the current rd can be driven by CPUs

	 * of another rd if they belong to the same pd. So, account for the

	 * utilization of these CPUs too by masking pd with cpu_online_mask

	 * instead of the rd span.

	 *

	 * If an entire pd is outside of the current rd, it will not appear in

	 * its pd list and will not be accounted by compute_energy().

		/*

		 * When @p is placed on @cpu:

		 *

		 * util_running = max(cpu_util, cpu_util_est) +

		 *		  max(task_util, _task_util_est)

		 *

		 * while cpu_util_next is: max(cpu_util + task_util,

		 *			       cpu_util_est + _task_util_est)

		/*

		 * Busy time computation: utilization clamping is not

		 * required since the ratio (sum_util / cpu_capacity)

		 * is already enough to scale the EM reported power

		 * consumption at the (eventually clamped) cpu_capacity.

		/*

		 * Performance domain frequency: utilization clamping

		 * must be considered since it affects the selection

		 * of the performance domain frequency.

		 * NOTE: in case RT tasks are running, by default the

		 * FREQUENCY_UTIL's utilization can be max OPP.

/*

 * find_energy_efficient_cpu(): Find most energy-efficient target CPU for the

 * waking task. find_energy_efficient_cpu() looks for the CPU with maximum

 * spare capacity in each performance domain and uses it as a potential

 * candidate to execute the task. Then, it uses the Energy Model to figure

 * out which of the CPU candidates is the most energy-efficient.

 *

 * The rationale for this heuristic is as follows. In a performance domain,

 * all the most energy efficient CPU candidates (according to the Energy

 * Model) are those for which we'll request a low frequency. When there are

 * several CPUs for which the frequency request will be the same, we don't

 * have enough data to break the tie between them, because the Energy Model

 * only includes active power costs. With this model, if we assume that

 * frequency requests follow utilization (e.g. using schedutil), the CPU with

 * the maximum spare capacity in a performance domain is guaranteed to be among

 * the best candidates of the performance domain.

 *

 * In practice, it could be preferable from an energy standpoint to pack

 * small tasks on a CPU in order to let other CPUs go in deeper idle states,

 * but that could also hurt our chances to go cluster idle, and we have no

 * ways to tell with the current Energy Model if this is actually a good

 * idea or not. So, find_energy_efficient_cpu() basically favors

 * cluster-packing, and spreading inside a cluster. That should at least be

 * a good thing for latency, and this is consistent with the idea that most

 * of the energy savings of EAS come from the asymmetry of the system, and

 * not so much from breaking the tie between identical CPUs. That's also the

 * reason why EAS is enabled in the topology code only for systems where

 * SD_ASYM_CPUCAPACITY is set.

 *

 * NOTE: Forkees are not accepted in the energy-aware wake-up path because

 * they don't have any useful utilization data yet and it's not possible to

 * forecast their impact on energy consumption. Consequently, they will be

 * placed by find_idlest_cpu() on the least loaded CPU, which might turn out

 * to be energy-inefficient in some use-cases. The alternative would be to

 * bias new tasks towards specific types of CPUs first, or to try to infer

 * their util_avg from the parent task, but those heuristics could hurt

 * other use-cases too. So, until someone finds a better way to solve this,

 * let's keep things simple by re-using the existing slow path.

	/*

	 * Energy-aware wake-up happens on the lowest sched_domain starting

	 * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu.

			/*

			 * Skip CPUs that cannot satisfy the capacity request.

			 * IOW, placing the task there would make the CPU

			 * overutilized. Take uclamp into account to see how

			 * much capacity we can get out of the CPU; this is

			 * aligned with sched_cpu_util().

 Always use prev_cpu as a candidate. */

				/*

				 * Find the CPU with the maximum spare capacity

				 * in the performance domain.

 Compute the 'base' energy of the pd, without @p */

 Evaluate the energy impact of using prev_cpu. */

 Evaluate the energy impact of using max_spare_cap_cpu. */

	/*

	 * Pick the best CPU if prev_cpu cannot be used, or if it saves at

	 * least 6% of the energy used by prev_cpu.

/*

 * select_task_rq_fair: Select target runqueue for the waking task in domains

 * that have the relevant SD flag set. In practice, this is SD_BALANCE_WAKE,

 * SD_BALANCE_FORK, or SD_BALANCE_EXEC.

 *

 * Balances load by selecting the idlest CPU in the idlest group, or under

 * certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set.

 *

 * Returns the target CPU number.

 SD_flags and WF_flags share the first nibble */

	/*

	 * required for stable ->cpus_allowed

		/*

		 * If both 'cpu' and 'prev_cpu' are part of this domain,

		 * cpu is a valid SD_WAKE_AFFINE target.

 Prefer wake_affine over balance flags */

 Slow path */

 XXX always ? */

 Fast path */

/*

 * Called immediately before a task is migrated to a new CPU; task_cpu(p) and

 * cfs_rq_of(p) references at time of call are still valid and identify the

 * previous CPU. The caller guarantees p->pi_lock or task_rq(p)->lock is held.

	/*

	 * As blocked tasks retain absolute vruntime the migration needs to

	 * deal with this by subtracting the old and adding the new

	 * min_vruntime -- the latter is done by enqueue_entity() when placing

	 * the task on the new runqueue.

		/*

		 * In case of TASK_ON_RQ_MIGRATING we in fact hold the 'old'

		 * rq->lock and can modify state directly.

		/*

		 * We are supposed to update the task to "current" time, then

		 * its up to date and ready to go to new CPU/cfs_rq. But we

		 * have difficulty in getting what current time is, so simply

		 * throw away the out-of-date time. This will result in the

		 * wakee task is less decayed, but giving the wakee more load

		 * sounds not bad.

 Tell new CPU we are migrated */

 We have migrated, no longer consider this task hot */

 CONFIG_SMP */

	/*

	 * Since its curr running now, convert the gran from real-time

	 * to virtual-time in his units.

	 *

	 * By using 'se' instead of 'curr' we penalize light tasks, so

	 * they get preempted easier. That is, if 'se' < 'curr' then

	 * the resulting gran will be larger, therefore penalizing the

	 * lighter, if otoh 'se' > 'curr' then the resulting gran will

	 * be smaller, again penalizing the lighter task.

	 *

	 * This is especially important for buddies when the leftmost

	 * task is higher priority than the buddy.

/*

 * Should 'se' preempt 'curr'.

 *

 *             |s1

 *        |s2

 *   |s3

 *         g

 *      |<--->|c

 *

 *  w(c, s1) = -1

 *  w(c, s2) =  0

 *  w(c, s3) =  1

 *

/*

 * Preempt the current task with a newly woken task if needed:

	/*

	 * This is possible from callers such as attach_tasks(), in which we

	 * unconditionally check_preempt_curr() after an enqueue (which may have

	 * lead to a throttle).  This both saves work and prevents false

	 * next-buddy nomination below.

	/*

	 * We can come here with TIF_NEED_RESCHED already set from new task

	 * wake up path.

	 *

	 * Note: this also catches the edge-case of curr being in a throttled

	 * group (e.g. via set_curr_task), since update_curr() (in the

	 * enqueue of curr) will have resulted in resched being set.  This

	 * prevents us from potentially nominating it as a false LAST_BUDDY

	 * below.

 Idle tasks are by definition preempted by non-idle tasks. */

	/*

	 * Batch and idle tasks do not preempt non-idle tasks (their preemption

	 * is driven by the tick):

	/*

	 * Preempt an idle group in favor of a non-idle group (and don't preempt

	 * in the inverse case).

		/*

		 * Bias pick_next to pick the sched entity that is

		 * triggering this preemption.

	/*

	 * Only set the backward buddy when the current task is still

	 * on the rq. This can happen when a wakeup gets interleaved

	 * with schedule on the ->pre_schedule() or idle_balance()

	 * point, either of which can * drop the rq lock.

	 *

	 * Also, during early boot the idle thread is in the fair class,

	 * for obvious reasons its a bad idea to schedule back to it.

 When we pick for a remote RQ, we'll not have done put_prev_entity() */

	/*

	 * Because of the set_next_buddy() in dequeue_task_fair() it is rather

	 * likely that a next task is from the same cgroup as the current.

	 *

	 * Therefore attempt to avoid putting and setting the entire cgroup

	 * hierarchy, only change the part that actually changes.

		/*

		 * Since we got here without doing put_prev_entity() we also

		 * have to consider cfs_rq->curr. If it is still a runnable

		 * entity, update_curr() will update its vruntime, otherwise

		 * forget we've ever seen it.

			/*

			 * This call to check_cfs_rq_runtime() will do the

			 * throttle and dequeue its entity in the parent(s).

			 * Therefore the nr_running test will indeed

			 * be correct.

	/*

	 * Since we haven't yet done put_prev_entity and if the selected task

	 * is a different task than we started out with, try and touch the

	 * least amount of cfs_rqs.

	/*

	 * Move the next running task to the front of

	 * the list, so our cfs_tasks list becomes MRU

	 * one.

	/*

	 * Because newidle_balance() releases (and re-acquires) rq->lock, it is

	 * possible for any higher priority task to appear. In that case we

	 * must re-start the pick_next_entity() loop.

	/*

	 * rq is about to be idle, check if we need to update the

	 * lost_idle_time of clock_pelt

/*

 * Account for a descheduled task:

/*

 * sched_yield() is very simple

 *

 * The magic of dealing with the ->skip buddy is in pick_next_entity.

	/*

	 * Are we the only task in the tree?

		/*

		 * Update run-time statistics of the 'current'.

		/*

		 * Tell update_rq_clock() that we've just updated,

		 * so we don't do microscopic update in schedule()

		 * and double the fastpath cost.

 throttled hierarchies are not runnable */

 Tell the scheduler that we'd really like pse to run next. */

/**************************************************

 * Fair scheduling class load-balancing methods.

 *

 * BASICS

 *

 * The purpose of load-balancing is to achieve the same basic fairness the

 * per-CPU scheduler provides, namely provide a proportional amount of compute

 * time to each task. This is expressed in the following equation:

 *

 *   W_i,n/P_i == W_j,n/P_j for all i,j                               (1)

 *

 * Where W_i,n is the n-th weight average for CPU i. The instantaneous weight

 * W_i,0 is defined as:

 *

 *   W_i,0 = \Sum_j w_i,j                                             (2)

 *

 * Where w_i,j is the weight of the j-th runnable task on CPU i. This weight

 * is derived from the nice value as per sched_prio_to_weight[].

 *

 * The weight average is an exponential decay average of the instantaneous

 * weight:

 *

 *   W'_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0               (3)

 *

 * C_i is the compute capacity of CPU i, typically it is the

 * fraction of 'recent' time available for SCHED_OTHER task execution. But it

 * can also include other factors [XXX].

 *

 * To achieve this balance we define a measure of imbalance which follows

 * directly from (1):

 *

 *   imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j }    (4)

 *

 * We them move tasks around to minimize the imbalance. In the continuous

 * function space it is obvious this converges, in the discrete case we get

 * a few fun cases generally called infeasible weight scenarios.

 *

 * [XXX expand on:

 *     - infeasible weights;

 *     - local vs global optima in the discrete case. ]

 *

 *

 * SCHED DOMAINS

 *

 * In order to solve the imbalance equation (4), and avoid the obvious O(n^2)

 * for all i,j solution, we create a tree of CPUs that follows the hardware

 * topology where each level pairs two lower groups (or better). This results

 * in O(log n) layers. Furthermore we reduce the number of CPUs going up the

 * tree to only the first of the previous level and we decrease the frequency

 * of load-balance at each level inv. proportional to the number of CPUs in

 * the groups.

 *

 * This yields:

 *

 *     log_2 n     1     n

 *   \Sum       { --- * --- * 2^i } = O(n)                            (5)

 *     i = 0      2^i   2^i

 *                               `- size of each group

 *         |         |     `- number of CPUs doing load-balance

 *         |         `- freq

 *         `- sum over all levels

 *

 * Coupled with a limit on how many tasks we can migrate every balance pass,

 * this makes (5) the runtime complexity of the balancer.

 *

 * An important property here is that each CPU is still (indirectly) connected

 * to every other CPU in at most O(log n) steps:

 *

 * The adjacency matrix of the resulting graph is given by:

 *

 *             log_2 n

 *   A_i,j = \Union     (i % 2^k == 0) && i / 2^(k+1) == j / 2^(k+1)  (6)

 *             k = 0

 *

 * And you'll find that:

 *

 *   A^(log_2 n)_i,j != 0  for all i,j                                (7)

 *

 * Showing there's indeed a path between every CPU in at most O(log n) steps.

 * The task movement gives a factor of O(m), giving a convergence complexity

 * of:

 *

 *   O(nm log n),  n := nr_cpus, m := nr_tasks                        (8)

 *

 *

 * WORK CONSERVING

 *

 * In order to avoid CPUs going idle while there's still work to do, new idle

 * balancing is more aggressive and has the newly idle CPU iterate up the domain

 * tree itself instead of relying on other CPUs to bring it work.

 *

 * This adds some complexity to both (5) and (8) but it reduces the total idle

 * time.

 *

 * [XXX more?]

 *

 *

 * CGROUPS

 *

 * Cgroups make a horror show out of (2), instead of a simple sum we get:

 *

 *                                s_k,i

 *   W_i,0 = \Sum_j \Prod_k w_k * -----                               (9)

 *                                 S_k

 *

 * Where

 *

 *   s_k,i = \Sum_j w_i,j,k  and  S_k = \Sum_i s_k,i                 (10)

 *

 * w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i.

 *

 * The big problem is S_k, its a global sum needed to compute a local (W_i)

 * property.

 *

 * [XXX write more on how we solve this.. _after_ merging pjt's patches that

 *      rewrite all of this once again.]

/*

 * 'group_type' describes the group of CPUs at the moment of load balancing.

 *

 * The enum is ordered by pulling priority, with the group with lowest priority

 * first so the group_type can simply be compared when selecting the busiest

 * group. See update_sd_pick_busiest().

 The group has spare capacity that can be used to run more tasks.  */

	/*

	 * The group is fully used and the tasks don't compete for more CPU

	 * cycles. Nevertheless, some tasks might wait before running.

	/*

	 * SD_ASYM_CPUCAPACITY only: One task doesn't fit with CPU's capacity

	 * and must be migrated to a more powerful CPU.

	/*

	 * SD_ASYM_PACKING only: One local CPU with higher capacity is available,

	 * and the task should be migrated to it instead of running on the

	 * current CPU.

	/*

	 * The tasks' affinity constraints previously prevented the scheduler

	 * from balancing the load across the system.

	/*

	 * The CPU is overloaded and can't provide expected CPU cycles to all

	 * tasks.

 The set of CPUs under consideration for load-balancing */

/*

 * Is this task likely cache-hot:

 SMT siblings share cache */

	/*

	 * Buddy candidates are cache hot:

	/*

	 * Don't migrate task if the task's cookie does not match

	 * with the destination CPU's core cookie.

/*

 * Returns 1, if task migration degrades locality

 * Returns 0, if task migration improves locality i.e migration preferred.

 * Returns -1, if task migration is not affected by locality.

 Migrating away from the preferred node is always bad. */

 Encourage migration to the preferred node. */

 Leaving a core idle is often worse than degrading locality. */

/*

 * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?

	/*

	 * We do not migrate tasks that are:

	 * 1) throttled_lb_pair, or

	 * 2) cannot be migrated to this CPU due to cpus_ptr, or

	 * 3) running (obviously), or

	 * 4) are cache-hot on their current CPU.

 Disregard pcpu kthreads; they are where they need to be. */

		/*

		 * Remember if this task can be migrated to any other CPU in

		 * our sched_group. We may want to revisit it if we couldn't

		 * meet load balance goals by pulling other tasks on src_cpu.

		 *

		 * Avoid computing new_dst_cpu

		 * - for NEWLY_IDLE

		 * - if we have already computed one in current iteration

		 * - if it's an active balance

 Prevent to re-select dst_cpu via env's CPUs: */

 Record that we found at least one task that could run on dst_cpu */

	/*

	 * Aggressive migration if:

	 * 1) active balance

	 * 2) destination numa is preferred

	 * 3) task is cache cold, or

	 * 4) too many balance attempts have failed.

/*

 * detach_task() -- detach the task for the migration specified in env

/*

 * detach_one_task() -- tries to dequeue exactly one task from env->src_rq, as

 * part of active balancing operations within "domain".

 *

 * Returns a task if successful and NULL otherwise.

		/*

		 * Right now, this is only the second place where

		 * lb_gained[env->idle] is updated (other is detach_tasks)

		 * so we can safely collect stats here rather than

		 * inside detach_tasks().

/*

 * detach_tasks() -- tries to detach up to imbalance load/util/tasks from

 * busiest_rq, as part of a balancing operation within domain "sd".

 *

 * Returns number of detached tasks if successful and 0 otherwise.

	/*

	 * Source run queue has been emptied by another CPU, clear

	 * LBF_ALL_PINNED flag as we will not test any task.

		/*

		 * We don't want to steal all, otherwise we may be treated likewise,

		 * which could at worst lead to a livelock crash.

 We've more or less seen every task there is, call it quits */

 take a breather every nr_migrate tasks */

			/*

			 * Depending of the number of CPUs and tasks and the

			 * cgroup hierarchy, task_h_load() can return a null

			 * value. Make sure that env->imbalance decreases

			 * otherwise detach_tasks() will stop only after

			 * detaching up to loop_max tasks.

			/*

			 * Make sure that we don't migrate too much load.

			 * Nevertheless, let relax the constraint if

			 * scheduler fails to find a good waiting task to

			 * migrate.

 This is not a misfit task */

		/*

		 * NEWIDLE balancing is a source of latency, so preemptible

		 * kernels will stop after the first task is detached to minimize

		 * the critical section.

		/*

		 * We only want to steal up to the prescribed amount of

		 * load/util/tasks.

	/*

	 * Right now, this is one of only two places we collect this stat

	 * so we can safely collect detach_one_task() stats here rather

	 * than inside detach_one_task().

/*

 * attach_task() -- attach the task detached by detach_task() to its new rq.

/*

 * attach_one_task() -- attaches the task returned from detach_one_task() to

 * its new rq.

/*

 * attach_tasks() -- attaches all tasks detached by detach_tasks() to their

 * new rq.

	/*

	 * update_load_avg() can call cpufreq_update_util(). Make sure that RT,

	 * DL and IRQ signals have been updated before updating CFS.

	/*

	 * Iterates the task_group tree in a bottom up fashion, see

	 * list_add_leaf_cfs_rq() for details.

 Propagate pending load changes to the parent, if any: */

		/*

		 * There can be a lot of idle CPU cgroups.  Don't let fully

		 * decayed cfs_rqs linger on the list.

 Don't need periodic decay once load/util_avg are null */

/*

 * Compute the hierarchical load factor for cfs_rq and all its ascendants.

 * This needs to be done in a top-down fashion because the load of a child

 * group is a fraction of its parents load.

********* Helpers for find_busiest_group ************************/

/*

 * sg_lb_stats - stats of a sched_group required for load_balancing

Avg load across the CPUs of the group */

 Total load over the CPUs of the group */

 Total utilization over the CPUs of the group */

 Total runnable time over the CPUs of the group */

 Nr of tasks running in the group */

 Nr of CFS tasks running in the group */

 Tasks should be moved to preferred CPU */

 A CPU has a task too big for its capacity */

/*

 * sd_lb_stats - Structure to store the statistics of a sched_domain

 *		 during load balancing.

 Busiest group in this sd */

 Local group in this sd */

 Total load of all groups in sd */

 Total capacity of all groups in sd */

 Average load across all groups in sd */

 tasks should go to sibling first */

 Statistics of the busiest group */

 Statistics of the local group */

	/*

	 * Skimp on the clearing to avoid duplicate work. We can avoid clearing

	 * local_stat because update_sg_lb_stats() does a full clear/assignment.

	 * We must however set busiest_stat::group_type and

	 * busiest_stat::idle_cpus to the worst busiest group because

	 * update_sd_pick_busiest() reads these before assignment.

	/*

	 * avg_rt.util_avg and avg_dl.util_avg track binary signals

	 * (running and not running) with weights 0 and 1024 respectively.

	 * avg_thermal.load_avg tracks thermal pressure and the weighted

	 * average uses the actual delta max capacity(load).

		/*

		 * SD_OVERLAP domains cannot assume that child groups

		 * span the current group.

		/*

		 * !SD_OVERLAP domains can assume that child groups

		 * span the current group.

/*

 * Check whether the capacity of the rq has been noticeably reduced by side

 * activity. The imbalance_pct is used for the threshold.

 * Return true is the capacity is reduced

/*

 * Check whether a rq has a misfit task and if it looks like we can actually

 * help that task: we can migrate the task to a CPU of higher capacity, or

 * the task's current CPU is heavily pressured.

/*

 * Group imbalance indicates (and tries to solve) the problem where balancing

 * groups is inadequate due to ->cpus_ptr constraints.

 *

 * Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a

 * cpumask covering 1 CPU of the first group and 3 CPUs of the second group.

 * Something like:

 *

 *	{ 0 1 2 3 } { 4 5 6 7 }

 *	        *     * * *

 *

 * If we were to balance group-wise we'd place two tasks in the first group and

 * two tasks in the second group. Clearly this is undesired as it will overload

 * cpu 3 and leave one of the CPUs in the second group unused.

 *

 * The current solution to this issue is detecting the skew in the first group

 * by noticing the lower domain failed to reach balance and had difficulty

 * moving tasks due to affinity constraints.

 *

 * When this is so detected; this group becomes a candidate for busiest; see

 * update_sd_pick_busiest(). And calculate_imbalance() and

 * find_busiest_group() avoid some of the usual balance conditions to allow it

 * to create an effective group imbalance.

 *

 * This is a somewhat tricky proposition since the next run might not find the

 * group imbalance and decide the groups need to be balanced again. A most

 * subtle and fragile situation.

/*

 * group_has_capacity returns true if the group has spare capacity that could

 * be used by some tasks.

 * We consider that a group has spare capacity if the  * number of task is

 * smaller than the number of CPUs or if the utilization is lower than the

 * available capacity for CFS tasks.

 * For the latter, we use a threshold to stabilize the state, to take into

 * account the variance of the tasks' load and to return true if the available

 * capacity in meaningful for the load balancer.

 * As an example, an available capacity of 1% can appear but it doesn't make

 * any benefit for the load balance.

/*

 *  group_is_overloaded returns true if the group has more tasks than it can

 *  handle.

 *  group_is_overloaded is not equals to !group_has_capacity because a group

 *  with the exact right number of tasks, has no more spare capacity but is not

 *  overloaded so both group_has_capacity and group_is_overloaded return

 *  false.

/**

 * asym_smt_can_pull_tasks - Check whether the load balancing CPU can pull tasks

 * @dst_cpu:	Destination CPU of the load balancing

 * @sds:	Load-balancing data with statistics of the local group

 * @sgs:	Load-balancing statistics of the candidate busiest group

 * @sg:		The candidate busiest group

 *

 * Check the state of the SMT siblings of both @sds::local and @sg and decide

 * if @dst_cpu can pull tasks.

 *

 * If @dst_cpu does not have SMT siblings, it can pull tasks if two or more of

 * the SMT siblings of @sg are busy. If only one CPU in @sg is busy, pull tasks

 * only if @dst_cpu has higher priority.

 *

 * If both @dst_cpu and @sg have SMT siblings, and @sg has exactly one more

 * busy CPU than @sds::local, let @dst_cpu pull tasks if it has higher priority.

 * Bigger imbalances in the number of busy CPUs will be dealt with in

 * update_sd_pick_busiest().

 *

 * If @sg does not have SMT siblings, only pull tasks if all of the SMT siblings

 * of @dst_cpu are idle and @sg has lower priority.

		/*

		 * If we are here, @dst_cpu is idle and does not have SMT

		 * siblings. Pull tasks if candidate group has two or more

		 * busy CPUs.

 implies sg_is_smt */

		/*

		 * @dst_cpu does not have SMT siblings. @sg may have SMT

		 * siblings and only one is busy. In such case, @dst_cpu

		 * can help if it has higher priority and is idle (i.e.,

		 * it has no running tasks).

 @dst_cpu has SMT siblings. */

	/*

	 * @sg does not have SMT siblings. Ensure that @sds::local does not end

	 * up with more than one busy SMT sibling and only pull tasks if there

	 * are not busy CPUs (i.e., no CPU has running tasks).

 Always return false so that callers deal with non-SMT cases. */

 Only do SMT checks if either local or candidate have SMT siblings */

/**

 * update_sg_lb_stats - Update sched_group's statistics for load balancing.

 * @env: The load balancing environment.

 * @group: sched_group whose statistics are to be updated.

 * @sgs: variable to hold the statistics for this group.

 * @sg_status: Holds flag indicating the status of the sched_group

		/*

		 * No need to call idle_cpu() if nr_running is not 0

 Idle cpu can't have misfit task */

 Check for a misfit task on the cpu */

 Check if dst CPU is idle and preferred to this group */

 Computing avg_load makes sense only when group is overloaded */

/**

 * update_sd_pick_busiest - return 1 on busiest group

 * @env: The load balancing environment.

 * @sds: sched_domain statistics

 * @sg: sched_group candidate to be checked for being the busiest

 * @sgs: sched_group statistics

 *

 * Determine if @sg is a busier group than the previously selected

 * busiest group.

 *

 * Return: %true if @sg is a busier group than the previously selected

 * busiest group. %false otherwise.

 Make sure that there is at least one task to pull */

	/*

	 * Don't try to pull misfit tasks we can't help.

	 * We can use max_capacity here as reduction in capacity on some

	 * CPUs in the group should either be possible to resolve

	 * internally or be covered by avg_load imbalance (eventually).

	/*

	 * The candidate and the current busiest group are the same type of

	 * group. Let check which one is the busiest according to the type.

 Select the overloaded group with highest avg_load. */

		/*

		 * Select the 1st imbalanced group as we don't have any way to

		 * choose one more than another.

 Prefer to move from lowest priority CPU's work */

		/*

		 * If we have more than one misfit sg go with the biggest

		 * misfit.

		/*

		 * Select the fully busy group with highest avg_load. In

		 * theory, there is no need to pull task from such kind of

		 * group because tasks have all compute capacity that they need

		 * but we can still improve the overall throughput by reducing

		 * contention when accessing shared HW resources.

		 *

		 * XXX for now avg_load is not computed and always 0 so we

		 * select the 1st one.

		/*

		 * Select not overloaded group with lowest number of idle cpus

		 * and highest number of running tasks. We could also compare

		 * the spare capacity which is more stable but it can end up

		 * that the group has less spare capacity but finally more idle

		 * CPUs which means less opportunity to pull tasks.

	/*

	 * Candidate sg has no more than one task per CPU and has higher

	 * per-CPU capacity. Migrating tasks to less capable CPUs may harm

	 * throughput. Maximize throughput, power/energy consequences are not

	 * considered.

 CONFIG_NUMA_BALANCING */

/*

 * task_running_on_cpu - return 1 if @p is running on @cpu.

 Task has no contribution or is new */

/**

 * idle_cpu_without - would a given CPU be idle without p ?

 * @cpu: the processor on which idleness is tested.

 * @p: task which should be ignored.

 *

 * Return: 1 if the CPU would be idle. 0 otherwise.

	/*

	 * rq->nr_running can't be used but an updated version without the

	 * impact of p on cpu must be used instead. The updated nr_running

	 * be computed and tested before calling idle_cpu_without().

/*

 * update_sg_wakeup_stats - Update sched_group's statistics for wakeup.

 * @sd: The sched_domain level to look for idlest group.

 * @group: sched_group whose statistics are to be updated.

 * @sgs: variable to hold the statistics for this group.

 * @p: The task for which we look for the idlest group/CPU.

		/*

		 * No need to call idle_cpu_without() if nr_running is not 0

 Check if task fits in the group */

	/*

	 * Computing avg_load makes sense only when group is fully busy or

	 * overloaded

	/*

	 * The candidate and the current idlest group are the same type of

	 * group. Let check which one is the idlest according to the type.

 Select the group with lowest avg_load. */

 Those types are not used in the slow wakeup path */

 Select group with the highest max capacity */

 Select group with most idle CPUs */

 Select group with lowest group_util */

/*

 * Allow a NUMA imbalance if busy CPUs is less than 25% of the domain.

 * This is an approximation as the number of running tasks may not be

 * related to the number of busy CPUs due to sched_setaffinity.

/*

 * find_idlest_group() finds and returns the least busy CPU group within the

 * domain.

 *

 * Assumes p is allowed on at least one CPU in sd.

 Skip over this group if it has no CPUs allowed */

 Skip over this group if no cookie matched */

 There is no idlest group to push tasks to */

 The local group has been skipped because of CPU affinity */

	/*

	 * If the local group is idler than the selected idlest group

	 * don't try and push the task.

	/*

	 * If the local group is busier than the selected idlest group

	 * try and push the task.

 Calculate allowed imbalance based on load */

		/*

		 * When comparing groups across NUMA domains, it's possible for

		 * the local domain to be very lightly loaded relative to the

		 * remote domains but "imbalance" skews the comparison making

		 * remote CPUs look much more favourable. When considering

		 * cross-domain, add imbalance to the load on the remote node

		 * and consider staying local.

		/*

		 * If the local group is less loaded than the selected

		 * idlest group don't try and push any tasks.

 Those type are not used in the slow wakeup path */

 Select group with the highest max capacity */

			/*

			 * If there is spare capacity at NUMA, try to select

			 * the preferred node

			/*

			 * Otherwise, keep the task on this node to stay close

			 * its wakeup source and improve locality. If there is

			 * a real need of migration, periodic load balance will

			 * take care of it.

		/*

		 * Select group with highest number of idle CPUs. We could also

		 * compare the utilization which is more stable but it can end

		 * up that the group has less spare capacity but finally more

		 * idle CPUs which means more opportunity to run task.

/**

 * update_sd_lb_stats - Update sched_domain's statistics for load balancing.

 * @env: The load balancing environment.

 * @sds: variable to hold the statistics for this sched_domain.

 Now, start updating sd_lb_stats */

 Tag domain that child domain prefers tasks go to siblings first */

 update overload indicator if we are at root domain */

 Update over-utilization (tipping point, U >= 0) indicator */

	/*

	 * Allow a small imbalance based on a simple pair of communicating

	 * tasks that remain local when the destination is lightly loaded.

/**

 * calculate_imbalance - Calculate the amount of imbalance present within the

 *			 groups of a given sched_domain during load balance.

 * @env: load balance environment

 * @sds: statistics of the sched_domain whose imbalance is to be calculated.

 Set imbalance to allow misfit tasks to be balanced. */

		/*

		 * In case of asym capacity, we will try to migrate all load to

		 * the preferred CPU.

		/*

		 * In the group_imb case we cannot rely on group-wide averages

		 * to ensure CPU-load equilibrium, try to move any task to fix

		 * the imbalance. The next load balance will take care of

		 * balancing back the system.

	/*

	 * Try to use spare capacity of local group without overloading it or

	 * emptying busiest.

			/*

			 * If busiest is overloaded, try to fill spare

			 * capacity. This might end up creating spare capacity

			 * in busiest or busiest still being overloaded but

			 * there is no simple way to directly compute the

			 * amount of load to migrate in order to balance the

			 * system.

			/*

			 * In some cases, the group's utilization is max or even

			 * higher than capacity because of migrations but the

			 * local CPU is (newly) idle. There is at least one

			 * waiting task in this overloaded busiest group. Let's

			 * try to pull it.

			/*

			 * When prefer sibling, evenly spread running tasks on

			 * groups.

			/*

			 * If there is no overload, we just want to even the number of

			 * idle cpus.

 Consider allowing a small imbalance between NUMA groups */

	/*

	 * Local is fully busy but has to take more load to relieve the

	 * busiest group

		/*

		 * Local will become overloaded so the avg_load metrics are

		 * finally needed.

		/*

		 * If the local group is more loaded than the selected

		 * busiest group don't try to pull any tasks.

	/*

	 * Both group are or will become overloaded and we're trying to get all

	 * the CPUs to the average_load, so we don't want to push ourselves

	 * above the average load, nor do we wish to reduce the max loaded CPU

	 * below the average load. At the same time, we also don't want to

	 * reduce the group load below the group capacity. Thus we look for

	 * the minimum possible imbalance.

****** find_busiest_group() helpers end here *********************/

/*

 * Decision matrix according to the local and busiest group type:

 *

 * busiest \ local has_spare fully_busy misfit asym imbalanced overloaded

 * has_spare        nr_idle   balanced   N/A    N/A  balanced   balanced

 * fully_busy       nr_idle   nr_idle    N/A    N/A  balanced   balanced

 * misfit_task      force     N/A        N/A    N/A  force      force

 * asym_packing     force     force      N/A    N/A  force      force

 * imbalanced       force     force      N/A    N/A  force      force

 * overloaded       force     force      N/A    N/A  force      avg_load

 *

 * N/A :      Not Applicable because already filtered while updating

 *            statistics.

 * balanced : The system is balanced for these 2 groups.

 * force :    Calculate the imbalance as load migration is probably needed.

 * avg_load : Only if imbalance is significant enough.

 * nr_idle :  dst_cpu is not busy and the number of idle CPUs is quite

 *            different in groups.

/**

 * find_busiest_group - Returns the busiest group within the sched_domain

 * if there is an imbalance.

 *

 * Also calculates the amount of runnable load which should be moved

 * to restore balance.

 *

 * @env: The load balancing environment.

 *

 * Return:	- The busiest group if imbalance exists.

	/*

	 * Compute the various statistics relevant for load balancing at

	 * this level.

 There is no busy sibling group to pull tasks from */

 Misfit tasks should be dealt with regardless of the avg load */

 ASYM feature bypasses nice load balance check */

	/*

	 * If the busiest group is imbalanced the below checks don't

	 * work because they assume all things are equal, which typically

	 * isn't true due to cpus_ptr constraints and the like.

	/*

	 * If the local group is busier than the selected busiest group

	 * don't try and pull any tasks.

	/*

	 * When groups are overloaded, use the avg_load to ensure fairness

	 * between tasks.

		/*

		 * If the local group is more loaded than the selected

		 * busiest group don't try to pull any tasks.

 XXX broken for overlapping NUMA groups */

		/*

		 * Don't pull any tasks if this group is already above the

		 * domain average load.

		/*

		 * If the busiest group is more loaded, use imbalance_pct to be

		 * conservative.

 Try to move all excess tasks to child's sibling domain */

			/*

			 * If the busiest group is not overloaded (and as a

			 * result the local one too) but this CPU is already

			 * busy, let another idle CPU try to pull task.

			/*

			 * If the busiest group is not overloaded

			 * and there is no imbalance between this and busiest

			 * group wrt idle CPUs, it is balanced. The imbalance

			 * becomes significant if the diff is greater than 1

			 * otherwise we might end up to just move the imbalance

			 * on another group. Of course this applies only if

			 * there is more than 1 CPU per group.

			/*

			 * busiest doesn't have any tasks waiting to run

 Looks like there is an imbalance. Compute it */

/*

 * find_busiest_queue - find the busiest runqueue among the CPUs in the group.

		/*

		 * We classify groups/runqueues into three groups:

		 *  - regular: there are !numa tasks

		 *  - remote:  there are numa tasks that run on the 'wrong' node

		 *  - all:     there is no distinction

		 *

		 * In order to avoid migrating ideally placed numa tasks,

		 * ignore those when there's better options.

		 *

		 * If we ignore the actual busiest queue to migrate another

		 * task, the next balance pass can still reduce the busiest

		 * queue by moving tasks around inside the node.

		 *

		 * If we cannot move enough load due to this classification

		 * the next pass will adjust the group classification and

		 * allow migration of more tasks.

		 *

		 * Both cases only affect the total convergence complexity.

		/*

		 * For ASYM_CPUCAPACITY domains, don't pick a CPU that could

		 * eventually lead to active_balancing high->low capacity.

		 * Higher per-CPU capacity is considered better than balancing

		 * average load.

 Make sure we only pull tasks from a CPU of lower priority */

			/*

			 * When comparing with load imbalance, use cpu_load()

			 * which is not scaled with the CPU capacity.

			/*

			 * For the load comparisons with the other CPUs,

			 * consider the cpu_load() scaled with the CPU

			 * capacity, so that the load can be moved away

			 * from the CPU that is potentially running at a

			 * lower capacity.

			 *

			 * Thus we're looking for max(load_i / capacity_i),

			 * crosswise multiplication to rid ourselves of the

			 * division works out to:

			 * load_i * capacity_j > load_j * capacity_i;

			 * where j is our previous maximum.

			/*

			 * Don't try to pull utilization from a CPU with one

			 * running task. Whatever its utilization, we will fail

			 * detach the task.

			/*

			 * For ASYM_CPUCAPACITY domains with misfit tasks we

			 * simply seek the "biggest" misfit task.

/*

 * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but

 * so long as it is large enough.

	/*

	 * ASYM_PACKING needs to force migrate tasks from busy but

	 * lower priority CPUs in order to pack all tasks in the

	 * highest priority CPUs.

	/*

	 * The imbalanced case includes the case of pinned tasks preventing a fair

	 * distribution of the load on the system but also the even distribution of the

	 * threads on a system with spare capacity

	/*

	 * The dst_cpu is idle and the src_cpu CPU has only 1 CFS task.

	 * It's worth migrating the task if the src_cpu's capacity is reduced

	 * because of other sched_class or IRQs if more capacity stays

	 * available on dst_cpu.

	/*

	 * Ensure the balancing environment is consistent; can happen

	 * when the softirq triggers 'during' hotplug.

	/*

	 * In the newly idle case, we will allow all the CPUs

	 * to do the newly idle load balance.

 Try to find first idle CPU */

 Are we the first idle CPU? */

 Are we the first CPU of this group ? */

/*

 * Check this_cpu to ensure it is balanced within domain. Attempt to move

 * tasks if there is an imbalance.

 Clear this flag as soon as we find a pullable task */

		/*

		 * Attempt to move tasks. If find_busiest_group has found

		 * an imbalance but busiest->nr_running <= 1, the group is

		 * still unbalanced. ld_moved simply stays zero, so it is

		 * correctly treated as an imbalance.

		/*

		 * cur_ld_moved - load moved in current iteration

		 * ld_moved     - cumulative load moved across iterations

		/*

		 * We've detached some tasks from busiest_rq. Every

		 * task is masked "TASK_ON_RQ_MIGRATING", so we can safely

		 * unlock busiest->lock, and we are able to be sure

		 * that nobody can manipulate the tasks in parallel.

		 * See task_rq_lock() family for the details.

		/*

		 * Revisit (affine) tasks on src_cpu that couldn't be moved to

		 * us and move them to an alternate dst_cpu in our sched_group

		 * where they can run. The upper limit on how many times we

		 * iterate on same src_cpu is dependent on number of CPUs in our

		 * sched_group.

		 *

		 * This changes load balance semantics a bit on who can move

		 * load to a given_cpu. In addition to the given_cpu itself

		 * (or a ilb_cpu acting on its behalf where given_cpu is

		 * nohz-idle), we now have balance_cpu in a position to move

		 * load to given_cpu. In rare situations, this may cause

		 * conflicts (balance_cpu and given_cpu/ilb_cpu deciding

		 * _independently_ and at _same_ time to move some load to

		 * given_cpu) causing excess load to be moved to given_cpu.

		 * This however should not happen so much in practice and

		 * moreover subsequent load balance cycles should correct the

		 * excess load moved.

 Prevent to re-select dst_cpu via env's CPUs */

			/*

			 * Go back to "more_balance" rather than "redo" since we

			 * need to continue with same src_cpu.

		/*

		 * We failed to reach balance because of affinity.

 All tasks on this runqueue were pinned by CPU affinity */

			/*

			 * Attempting to continue load balancing at the current

			 * sched_domain level only makes sense if there are

			 * active CPUs remaining as possible busiest CPUs to

			 * pull load from which are not contained within the

			 * destination group that is receiving any migrated

			 * load.

		/*

		 * Increment the failure counter only on periodic balance.

		 * We do not want newidle balance, which can be very

		 * frequent, pollute the failure counter causing

		 * excessive cache_hot migrations and active balances.

			/*

			 * Don't kick the active_load_balance_cpu_stop,

			 * if the curr task on busiest CPU can't be

			 * moved to this_cpu:

 Record that we found at least one task that could run on this_cpu */

			/*

			 * ->active_balance synchronizes accesses to

			 * ->active_balance_work.  Once set, it's cleared

			 * only after active load balance is finished.

 We were unbalanced, so reset the balancing interval */

	/*

	 * We reach balance although we may have faced some affinity

	 * constraints. Clear the imbalance flag only if other tasks got

	 * a chance to move and fix the imbalance.

	/*

	 * We reach balance because all tasks are pinned at this level so

	 * we can't migrate them. Let the imbalance flag set so parent level

	 * can try to migrate them.

	/*

	 * newidle_balance() disregards balance intervals, so we could

	 * repeatedly reach this code, which would lead to balance_interval

	 * skyrocketing in a short amount of time. Skip the balance_interval

	 * increase logic to avoid that.

 tune up the balancing interval */

 scale ms to jiffies */

	/*

	 * Reduce likelihood of busy balancing at higher domains racing with

	 * balancing at lower domains by preventing their balancing periods

	 * from being multiples of each other.

 used by idle balance, so cpu_busy = 0 */

/*

 * active_load_balance_cpu_stop is run by the CPU stopper. It pushes

 * running tasks off the busiest CPU onto idle CPUs. It requires at

 * least 1 task to be running on each physical CPU where possible, and

 * avoids physical / logical imbalances.

	/*

	 * Between queueing the stop-work and running it is a hole in which

	 * CPUs can become inactive. We should not move tasks from or to

	 * inactive CPUs.

 Make sure the requested CPU hasn't gone down in the meantime: */

 Is there any task to move? */

	/*

	 * This condition is "impossible", if it occurs

	 * we need to fix it. Originally reported by

	 * Bjorn Helgaas on a 128-CPU setup.

 Search for an sd spanning us and the target CPU. */

 Active balancing done, reset the failure counter. */

/*

 * Scale the max load_balance interval with the number of CPUs in the system.

 * This trades load-balance latency on larger machines for less cross talk.

		/*

		 * Track max cost of a domain to make sure to not delay the

		 * next wakeup on the CPU.

		/*

		 * Decay the newidle max times by ~1% per second to ensure that

		 * it is not outdated and the current max cost is actually

		 * shorter.

/*

 * It checks each scheduling domain to see if it is due to be balanced,

 * and initiates a balancing operation if so.

 *

 * Balancing parameters are set up in init_sched_domains.

 Earliest time when we have to do rebalance again */

		/*

		 * Decay the newidle max times here because this is a regular

		 * visit to all the domains.

		/*

		 * Stop the load balance at this level. There is another

		 * CPU in our sched group which is doing load balancing more

		 * actively.

				/*

				 * The LBF_DST_PINNED logic could have changed

				 * env->dst_cpu, so we can't know our idle

				 * state even if we migrated tasks. Update it.

		/*

		 * Ensure the rq-wide value also decays but keep it at a

		 * reasonable floor to avoid funnies with rq->avg_idle.

	/*

	 * next_balance will be updated only when there is a need.

	 * When the cpu is attached to null domain for ex, it will not be

	 * updated.

/*

 * idle load balancing details

 * - When one of the busy CPUs notice that there may be an idle rebalancing

 *   needed, they will kick the idle load balancer, which then does idle

 *   load balancing for all the idle CPUs.

 * - HK_FLAG_MISC CPUs are used for this task, because HK_FLAG_SCHED not set

 *   anywhere yet.

/*

 * Kick a CPU to do the nohz balancing, if it is time for it. We pick any

 * idle CPU in the HK_FLAG_MISC housekeeping set (if there is one).

	/*

	 * Increase nohz.next_balance only when if full ilb is triggered but

	 * not if we only update stats.

	/*

	 * Access to rq::nohz_csd is serialized by NOHZ_KICK_MASK; he who sets

	 * the first flag owns it; cleared by nohz_csd_func().

	/*

	 * This way we generate an IPI on the target CPU which

	 * is idle. And the softirq performing nohz idle load balance

	 * will be run before returning from the IPI.

/*

 * Current decision point for kicking the idle load balancer in the presence

 * of idle CPUs in the system.

	/*

	 * We may be recently in ticked or tickless idle mode. At the first

	 * busy tick after returning from idle, we will update the busy stats.

	/*

	 * None are in tickless mode and hence no need for NOHZ idle load

	 * balancing.

		/*

		 * If there's a CFS task and the current CPU has reduced

		 * capacity; kick the ILB to see if there's a better CPU to run

		 * on.

		/*

		 * When ASYM_PACKING; see if there's a more preferred CPU

		 * currently idle; in which case, kick the ILB to move tasks

		 * around.

		/*

		 * When ASYM_CPUCAPACITY; see if there's a higher capacity CPU

		 * to run the misfit task on.

		/*

		 * For asymmetric systems, we do not want to nicely balance

		 * cache use, instead we want to embrace asymmetry and only

		 * ensure tasks have enough CPU capacity.

		 *

		 * Skip the LLC logic because it's not relevant in that case.

		/*

		 * If there is an imbalance between LLC domains (IOW we could

		 * increase the overall cache use), we need some less-loaded LLC

		 * domain to pull some load. Likewise, we may need to spread

		 * load within the current LLC domain (e.g. packed SMT cores but

		 * other CPUs are idle). We can't really know from here how busy

		 * the others are - so just get a nohz balance going if it looks

		 * like this LLC domain has tasks we could move.

/*

 * This routine will record that the CPU is going idle with tick stopped.

 * This info will be used in performing idle load balancing in the future.

 If this CPU is going down, then nothing needs to be done: */

 Spare idle load balancing on CPUs that don't want to be disturbed: */

	/*

	 * Can be set safely without rq->lock held

	 * If a clear happens, it will have evaluated last additions because

	 * rq->lock is held during the check and the clear

	/*

	 * The tick is still stopped but load could have been added in the

	 * meantime. We set the nohz.has_blocked flag to trig a check of the

	 * *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear

	 * of nohz.has_blocked can only happen after checking the new load

 If we're a completely isolated CPU, we don't play: */

	/*

	 * Ensures that if nohz_idle_balance() fails to observe our

	 * @idle_cpus_mask store, it must observe the @has_blocked

	 * and @needs_update stores.

	/*

	 * Each time a cpu enter idle, we assume that it has blocked load and

	 * enable the periodic update of the load of idle cpus

/*

 * Internal function that runs load balance for all idle cpus. The load balance

 * can be a simple update of blocked load or a complete load balance with

 * tasks movement depending of flags.

 Earliest time when we have to do rebalance again */

	/*

	 * We assume there will be no idle load after this update and clear

	 * the has_blocked flag. If a cpu enters idle in the mean time, it will

	 * set the has_blocked flag and trigger another update of idle load.

	 * Because a cpu that becomes idle, is added to idle_cpus_mask before

	 * setting the flag, we are sure to not clear the state and not

	 * check the load of an idle cpu.

	 *

	 * Same applies to idle_cpus_mask vs needs_update.

	/*

	 * Ensures that if we miss the CPU, we must see the has_blocked

	 * store from nohz_balance_enter_idle().

	/*

	 * Start with the next CPU after this_cpu so we will end with this_cpu and let a

	 * chance for other idle cpu to pull load.

		/*

		 * If this CPU gets work to do, stop the load balancing

		 * work being done for other CPUs. Next load

		 * balancing owner will pick it up.

		/*

		 * If time for next balance is due,

		 * do the balance.

	/*

	 * next_balance will be updated only when there is a need.

	 * When the CPU is attached to null domain for ex, it will not be

	 * updated.

 There is still blocked load, enable periodic update */

/*

 * In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the

 * rebalancing for all the cpus for whom scheduler ticks are stopped.

/*

 * Check if we need to run the ILB for updating blocked load before entering

 * idle state.

	/*

	 * Update the blocked load only if no SCHED_SOFTIRQ is about to happen

	 * (ie NOHZ_STATS_KICK set) and will do the same.

	/*

	 * This CPU doesn't want to be disturbed by scheduler

	 * housekeeping

 Will wake up very soon. No time for doing anything else*/

 Don't need to update blocked load of idle CPUs*/

	/*

	 * Set the need to trigger ILB in order to update blocked load

	 * before entering idle state.

 !CONFIG_NO_HZ_COMMON */

 CONFIG_NO_HZ_COMMON */

/*

 * newidle_balance is called by schedule() if this_cpu is about to become

 * idle. Attempts to pull tasks from other CPUs.

 *

 * Returns:

 *   < 0 - we released the lock and there are !fair tasks present

 *     0 - failed, no new tasks

 *   > 0 - success, new (fair) tasks present

	/*

	 * There is a task waiting to run. No need to search for one.

	 * Return 0; the task will be enqueued when switching to idle.

	/*

	 * We must set idle_stamp _before_ calling idle_balance(), such that we

	 * measure the duration of idle_balance() as idle time.

	/*

	 * Do not pull tasks towards !active CPUs...

	/*

	 * This is OK, because current is on_cpu, which avoids it being picked

	 * for load-balance and preemption/IRQs are still disabled avoiding

	 * further scheduler activity on it and we're being very careful to

	 * re-start the picking loop.

		/*

		 * Stop searching for tasks to pull if there are

		 * now runnable tasks on this rq.

	/*

	 * While browsing the domains, we released the rq lock, a task could

	 * have been enqueued in the meantime. Since we're not going idle,

	 * pretend we pulled a task.

 Is there a task of a high priority class? */

 Move the next balance forward */

/*

 * run_rebalance_domains is triggered when needed from the scheduler tick.

 * Also triggered for nohz idle balancing (with nohz_balancing_kick set).

	/*

	 * If this CPU has a pending nohz_balance_kick, then do the

	 * balancing on behalf of the other idle CPUs whose ticks are

	 * stopped. Do nohz_idle_balance *before* rebalance_domains to

	 * give the idle CPUs a chance to load balance. Else we may

	 * load balance only within the local sched_domain hierarchy

	 * and abort nohz_idle_balance altogether if we pull some load.

 normal load balance */

/*

 * Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing.

	/*

	 * Don't need to rebalance while attached to NULL domain or

	 * runqueue CPU is not active

 Ensure any throttled groups are reachable by pick_next_task */

 CONFIG_SMP */

	/*

	 * If runqueue has only one task which used up its slice and

	 * if the sibling is forced idle, then trigger schedule to

	 * give forced idle task a chance.

	 *

	 * sched_slice() considers only this active rq and it gets the

	 * whole slice. But during force idle, we have siblings acting

	 * like a single runqueue and hence we need to consider runnable

	 * tasks on this CPU and the forced idle CPU. Ideally, we should

	 * go through the forced idle rq, but that would be a perf hit.

	 * We can assume that the forced idle CPU has at least

	 * MIN_NR_TASKS_DURING_FORCEIDLE - 1 tasks and use that to check

	 * if we need to give up the CPU.

/*

 * se_fi_update - Update the cfs_rq->min_vruntime_fi in a CFS hierarchy if needed.

	/*

	 * Find an se in the hierarchy for tasks a and b, such that the se's

	 * are immediate siblings.

	/*

	 * Find delta after normalizing se's vruntime with its cfs_rq's

	 * min_vruntime_fi, which would have been updated in prior calls

	 * to se_fi_update().

/*

 * scheduler tick hitting a task of our scheduling class.

 *

 * NOTE: This function can be called remotely by the tick offload that

 * goes along full dynticks. Therefore no local assumption can be made

 * and everything must be accessed through the @rq and @curr passed in

 * parameters.

/*

 * called on fork with the child task as argument from the parent's context

 *  - child not yet on the tasklist

 *  - preemption disabled

		/*

		 * Upon rescheduling, sched_class::put_prev_task() will place

		 * 'current' within the tree based on its new key value.

/*

 * Priority of the task has changed. Check to see if we preempt

 * the current task.

	/*

	 * Reschedule if we are currently running on this runqueue and

	 * our priority decreased, or if we are not currently running on

	 * this runqueue and our priority is higher than the current's

	/*

	 * In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases,

	 * the dequeue_entity(.flags=0) will already have normalized the

	 * vruntime.

	/*

	 * When !on_rq, vruntime of the task has usually NOT been normalized.

	 * But there are some cases where it has already been normalized:

	 *

	 * - A forked child which is waiting for being woken up by

	 *   wake_up_new_task().

	 * - A task which has been woken up by try_to_wake_up() and

	 *   waiting for actually being woken up by sched_ttwu_pending().

/*

 * Propagate the changes of the sched_entity across the tg tree to make it

 * visible to the root

 Start to propagate at parent */

 Catch up with the cfs_rq and remove our load when we leave */

	/*

	 * Since the real-depth could have been changed (only FAIR

	 * class maintain depth value), reset depth properly.

 Synchronize entity with its cfs_rq */

		/*

		 * Fix up our vruntime so that the current sleep doesn't

		 * cause 'unlimited' sleep bonus.

		/*

		 * We were most likely switched from sched_rt, so

		 * kick off the schedule if running, otherwise just see

		 * if we can still preempt the current task.

/* Account for a task changing its policy or group.

 *

 * This routine is mostly called to set cfs_rq->curr field when a task

 * migrates between groups/classes.

		/*

		 * Move the next running task to the front of the list, so our

		 * cfs_tasks list becomes MRU one.

 ensure bandwidth has been allocated on our new cfs_rq */

 Tell se's cfs_rq has been changed -- migrated */

		/*

		 * Only empty task groups can be destroyed; so we can speculatively

		 * check on_list without danger of it being re-added.

 se could be NULL for root_task_group */

 guarantee group entities always have weight */

	/*

	 * We can't change the weight of the root cgroup.

 Propagate contribution to hierarchy */

 Already accounted at parent level and above. */

 Idle groups have minimum weight. */

 CONFIG_FAIR_GROUP_SCHED */

 CONFIG_FAIR_GROUP_SCHED */

	/*

	 * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise

	 * idle runqueue:

/*

 * All the scheduling class methods:

 CONFIG_NUMA_BALANCING */

 CONFIG_SCHED_DEBUG */

 SMP */

/*

 * Helper functions to facilitate extracting info from tracepoints.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sched_clock() for unstable CPU clocks

 *

 *  Copyright (C) 2008 Red Hat, Inc., Peter Zijlstra

 *

 *  Updates and enhancements:

 *    Copyright (C) 2008 Red Hat, Inc. Steven Rostedt <srostedt@redhat.com>

 *

 * Based on code by:

 *   Ingo Molnar <mingo@redhat.com>

 *   Guillaume Chazarain <guichaz@gmail.com>

 *

 *

 * What this file implements:

 *

 * cpu_clock(i) provides a fast (execution time) high resolution

 * clock with bounded drift between CPUs. The value of cpu_clock(i)

 * is monotonic for constant i. The timestamp returned is in nanoseconds.

 *

 * ######################### BIG FAT WARNING ##########################

 * # when comparing cpu_clock(i) to cpu_clock(j) for i != j, time can #

 * # go backwards !!                                                  #

 * ####################################################################

 *

 * There is no strict promise about the base, although it tends to start

 * at 0 on boot (but people really shouldn't rely on that).

 *

 * cpu_clock(i)       -- can be used from any context, including NMI.

 * local_clock()      -- is cpu_clock() on the current CPU.

 *

 * sched_clock_cpu(i)

 *

 * How it is implemented:

 *

 * The implementation either uses sched_clock() when

 * !CONFIG_HAVE_UNSTABLE_SCHED_CLOCK, which means in that case the

 * sched_clock() is assumed to provide these properties (mostly it means

 * the architecture provides a globally synchronized highres time source).

 *

 * Otherwise it tries to create a semi stable clock from a mixture of other

 * clocks, including:

 *

 *  - GTOD (clock monotonic)

 *  - sched_clock()

 *  - explicit idle events

 *

 * We use GTOD as base and use sched_clock() deltas to improve resolution. The

 * deltas are filtered to provide monotonicity and keeping it within an

 * expected window.

 *

 * Furthermore, explicit sleep and wakeup hooks allow us to account for time

 * that is otherwise invisible (TSC gets stopped).

 *

/*

 * Scheduler clock - returns current time in nanosec units.

 * This is default implementation.

 * Architectures and sub-architectures can override this.

/*

 * We must start with !__sched_clock_stable because the unstable -> stable

 * transition is accurate, while the stable -> unstable transition is not.

 *

 * Similarly we start with __sched_clock_stable_early, thereby assuming we

 * will become stable, such that there's only a single 1 -> 0 transition.

/*

 * We want: ktime_get_ns() + __gtod_offset == sched_clock() + __sched_clock_offset

	/*

	 * Since we're still unstable and the tick is already running, we have

	 * to disable IRQs in order to get a consistent scd->tick* reading.

	/*

	 * Attempt to make the (initial) unstable->stable transition continuous.

/*

 * If we ever get here, we're screwed, because we found out -- typically after

 * the fact -- that TSC wasn't good. This means all our clocksources (including

 * ktime) could have reported wrong values.

 *

 * What we do here is an attempt to fix up and continue sort of where we left

 * off in a coherent manner.

 *

 * The only way to fully avoid random clock jumps is to boot with:

 * "tsc=unstable".

 take a current timestamp and set 'now' */

 clone to all CPUs */

 matches sched_clock_init_late() */

	/*

	 * Set __gtod_offset such that once we mark sched_clock_running,

	 * sched_clock_tick() continues where sched_clock() left off.

	 *

	 * Even if TSC is buggered, we're still UP at this point so it

	 * can't really be out of sync.

/*

 * We run this as late_initcall() such that it runs after all built-in drivers,

 * notably: acpi_processor and intel_idle, which can mark the TSC as unstable.

	/*

	 * Ensure that it is impossible to not do a static_key update.

	 *

	 * Either {set,clear}_sched_clock_stable() must see sched_clock_running

	 * and do the update, or we must see their __sched_clock_stable_early

	 * and do the update, or both.

 matches {set,clear}_sched_clock_stable() */

/*

 * min, max except they take wrapping into account

/*

 * update the percpu scd from the raw @now value

 *

 *  - filter out backward motion

 *  - use the GTOD tick value to create a window to filter crazy TSC values

	/*

	 * scd->clock = clamp(scd->tick_gtod + delta,

	 *		      max(scd->tick_gtod, scd->clock),

	 *		      scd->tick_gtod + TICK_NSEC);

	/*

	 * Careful here: The local and the remote clock values need to

	 * be read out atomic as we need to compare the values and

	 * then update either the local or the remote side. So the

	 * cmpxchg64 below only protects one readout.

	 *

	 * We must reread via sched_clock_local() in the retry case on

	 * 32-bit kernels as an NMI could use sched_clock_local() via the

	 * tracer and hit between the readout of

	 * the low 32-bit and the high 32-bit portion.

	/*

	 * We must enforce atomic readout on 32-bit, otherwise the

	 * update on the remote CPU can hit inbetween the readout of

	 * the low 32-bit and the high 32-bit portion.

	/*

	 * On 64-bit kernels the read of [my]scd->clock is atomic versus the

	 * update, so we can avoid the above 32-bit dance.

	/*

	 * Use the opportunity that we have both locks

	 * taken to couple the two clocks: we take the

	 * larger time as the latest time for both

	 * runqueues. (this creates monotonic movement)

		/*

		 * Should be rare, but possible:

/*

 * Similar to cpu_clock(), but requires local IRQs to be disabled.

 *

 * See cpu_clock().

	/*

	 * Called under watchdog_lock.

	 *

	 * The watchdog just found this TSC to (still) be stable, so now is a

	 * good moment to update our __gtod_offset. Because once we find the

	 * TSC to be unstable, any computation will be computing crap.

/*

 * We are going deep-idle (irqs are disabled):

/*

 * We just idled; resync with ktime.

 CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */

 CONFIG_HAVE_UNSTABLE_SCHED_CLOCK */

/*

 * Running clock - returns the time that has elapsed while a guest has been

 * running.

 * On a guest this value should be local_clock minus the time the guest was

 * suspended by the hypervisor (for any reason).

 * On bare metal this function should return the same as local_clock.

 * Architectures and sub-architectures can override this.

 SPDX-License-Identifier: GPL-2.0

/*

 * stop-task scheduling class.

 *

 * The stop task is the highest priority task in the system, it preempts

 * everything and will be preempted by nothing.

 *

 * See kernel/stop_machine.c

 stop tasks as never migrate */

 CONFIG_SMP */

 we're never preempted */

 the stop task should never yield, its pointless. */

/*

 * scheduler tick hitting a task of our scheduling class.

 *

 * NOTE: This function can be called remotely by the tick offload that

 * goes along full dynticks. Therefore no local assumption can be made

 * and everything must be accessed through the @rq and @curr passed in

 * parameters.

 its impossible to change to this class */

 how!?, what priority? */

/*

 * Simple, special scheduling class for the per-CPU stop tasks:

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic entry points for the idle threads and

 * implementation of the idle task scheduling class.

 *

 * (NOTE: these are not related to SCHED_IDLE batch scheduled

 *        tasks which are handled in sched/fair.c )

 Linker adds these: start and end of __cpuidle functions */

/**

 * sched_idle_set_state - Record idle state for the current CPU.

 * @idle_state: State to record.

 Weak implementations for optional arch specific functions */

/**

 * default_idle_call - Default CPU idle routine.

 *

 * To use when the cpuidle framework cannot be used.

		/*

		 * arch_cpu_idle() is supposed to enable IRQs, however

		 * we can't do that because of RCU and tracing.

		 *

		 * Trace IRQs enable here, then switch off RCU, and have

		 * arch_cpu_idle() use raw_local_irq_enable(). Note that

		 * rcu_idle_enter() relies on lockdep IRQ state, so switch that

		 * last -- this is very similar to the entry code.

		/*

		 * OK, so IRQs are enabled here, but RCU needs them disabled to

		 * turn itself back on.. funny thing is that disabling IRQs

		 * will cause tracing, which needs RCU. Jump through hoops to

		 * make it 'work'.

	/*

	 * The idle task must be scheduled, it is pointless to go to idle, just

	 * update no idle residency and return.

	/*

	 * Enter the idle state previously returned by the governor decision.

	 * This function will block until an interrupt occurs and will take

	 * care of re-enabling the local interrupts

/**

 * cpuidle_idle_call - the main idle function

 *

 * NOTE: no locks or semaphores should be used here

 *

 * On architectures that support TIF_POLLING_NRFLAG, is called with polling

 * set, and it returns with polling set.  If it ever stops polling, it

 * must clear the polling bit.

	/*

	 * Check if the idle task must be rescheduled. If it is the

	 * case, exit the function after re-enabling the local irq.

	/*

	 * The RCU framework needs to be told that we are entering an idle

	 * section, so no more rcu read side critical sections and one more

	 * step to the grace period

	/*

	 * Suspend-to-idle ("s2idle") is a system state in which all user space

	 * has been frozen, all I/O devices have been suspended and the only

	 * activity happens here and in interrupts (if any). In that case bypass

	 * the cpuidle governor and go straight for the deepest idle state

	 * available.  Possibly also suspend the local tick and the entire

	 * timekeeping to prevent timer interrupts from kicking us out of idle

	 * until a proper wakeup interrupt happens.

		/*

		 * Ask the cpuidle framework to choose a convenient idle state.

		/*

		 * Give the governor an opportunity to reflect on the outcome

	/*

	 * It is up to the idle functions to reenable local interrupts

/*

 * Generic idle loop implementation

 *

 * Called with polling cleared.

	/*

	 * Check if we need to update blocked load

	/*

	 * If the arch has a polling bit, we maintain an invariant:

	 *

	 * Our polling bit is clear if we're not scheduled (i.e. if rq->curr !=

	 * rq->idle). This means that, if rq->idle has the polling bit set,

	 * then setting need_resched is guaranteed to cause the CPU to

	 * reschedule.

		/*

		 * In poll mode we reenable interrupts and spin. Also if we

		 * detected in the wakeup from idle path that the tick

		 * broadcast device expired for us, we don't want to go deep

		 * idle as we know that the IPI is going to arrive right away.

	/*

	 * Since we fell out of the loop above, we know TIF_NEED_RESCHED must

	 * be set, propagate it into PREEMPT_NEED_RESCHED.

	 *

	 * This is required because for polling idle loops we will not have had

	 * an IPI to fold the state for us.

	/*

	 * We promise to call sched_ttwu_pending() and reschedule if

	 * need_resched() is set while polling is set. That means that clearing

	 * polling needs to be visible before doing these things.

	/*

	 * RCU relies on this call to be done outside of an RCU read-side

	 * critical section.

	/*

	 * Only FIFO tasks can disable the tick since they don't need the forced

	 * preemption.

/*

 * idle-task scheduling class.

 IDLE tasks as never migrated */

/*

 * Idle tasks are unconditionally rescheduled:

/*

 * It is not legal to sleep in the idle task - print a warning

 * message if some code attempts to do it:

/*

 * scheduler tick hitting a task of our scheduling class.

 *

 * NOTE: This function can be called remotely by the tick offload that

 * goes along full dynticks. Therefore no local assumption can be made

 * and everything must be accessed through the @rq and @curr passed in

 * parameters.

/*

 * Simple, special scheduling class for the per-CPU idle tasks:

 no enqueue/yield_task for idle tasks */

 dequeue is not valid, we print a debug message there: */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  kernel/sched/cpupri.c

 *

 *  CPU priority management

 *

 *  Copyright (C) 2007-2008 Novell

 *

 *  Author: Gregory Haskins <ghaskins@novell.com>

 *

 *  This code tracks the priority of each CPU so that global migration

 *  decisions are easy to calculate.  Each CPU can be in a state as follows:

 *

 *                 (INVALID), NORMAL, RT1, ... RT99, HIGHER

 *

 *  going from the lowest priority to the highest.  CPUs in the INVALID state

 *  are not eligible for routing.  The system maintains this state with

 *  a 2 dimensional bitmap (the first for priority class, the second for CPUs

 *  in that class).  Therefore a typical application without affinity

 *  restrictions can find a suitable CPU with O(1) complexity (e.g. two bit

 *  searches).  For tasks with affinity restrictions, the algorithm has a

 *  worst case complexity of O(min(101, nr_domcpus)), though the scenario that

 *  yields the worst case search is fairly contrived.

/*

 * p->rt_priority   p->prio   newpri   cpupri

 *

 *				  -1       -1 (CPUPRI_INVALID)

 *

 *				  99        0 (CPUPRI_NORMAL)

 *

 *		1        98       98        1

 *	      ...

 *	       49        50       50       49

 *	       50        49       49       50

 *	      ...

 *	       99         0        0       99

 *

 *				 100	  100 (CPUPRI_HIGHER)

 -1 */

 1 ... 99 */

  0 */

 100 */

	/*

	 * When looking at the vector, we need to read the counter,

	 * do a memory barrier, then read the mask.

	 *

	 * Note: This is still all racy, but we can deal with it.

	 *  Ideally, we only want to look at masks that are set.

	 *

	 *  If a mask is not set, then the only thing wrong is that we

	 *  did a little more work than necessary.

	 *

	 *  If we read a zero count but the mask is set, because of the

	 *  memory barriers, that can only happen when the highest prio

	 *  task for a run queue has left the run queue, in which case,

	 *  it will be followed by a pull. If the task we are processing

	 *  fails to find a proper place to go, that pull request will

	 *  pull this task if the run queue is running at a lower

	 *  priority.

 Need to do the rmb for every iteration */

		/*

		 * We have to ensure that we have at least one bit

		 * still set in the array, since the map could have

		 * been concurrently emptied between the first and

		 * second reads of vec->mask.  If we hit this

		 * condition, simply act as though we never hit this

		 * priority level and continue on.

/**

 * cpupri_find_fitness - find the best (lowest-pri) CPU in the system

 * @cp: The cpupri context

 * @p: The task

 * @lowest_mask: A mask to fill in with selected CPUs (or NULL)

 * @fitness_fn: A pointer to a function to do custom checks whether the CPU

 *              fits a specific criteria so that we only return those CPUs.

 *

 * Note: This function returns the recommended CPUs as calculated during the

 * current invocation.  By the time the call returns, the CPUs may have in

 * fact changed priorities any number of times.  While not ideal, it is not

 * an issue of correctness since the normal rebalancer logic will correct

 * any discrepancies created by racing against the uncertainty of the current

 * priority configuration.

 *

 * Return: (int)bool - CPUs were found

 Ensure the capacity of the CPUs fit the task */

		/*

		 * If no CPU at the current priority can fit the task

		 * continue looking

	/*

	 * If we failed to find a fitting lowest_mask, kick off a new search

	 * but without taking into account any fitness criteria this time.

	 *

	 * This rule favours honouring priority over fitting the task in the

	 * correct CPU (Capacity Awareness being the only user now).

	 * The idea is that if a higher priority task can run, then it should

	 * run even if this ends up being on unfitting CPU.

	 *

	 * The cost of this trade-off is not entirely clear and will probably

	 * be good for some workloads and bad for others.

	 *

	 * The main idea here is that if some CPUs were over-committed, we try

	 * to spread which is what the scheduler traditionally did. Sys admins

	 * must do proper RT planning to avoid overloading the system if they

	 * really care.

/**

 * cpupri_set - update the CPU priority setting

 * @cp: The cpupri context

 * @cpu: The target CPU

 * @newpri: The priority (INVALID,NORMAL,RT1-RT99,HIGHER) to assign to this CPU

 *

 * Note: Assumes cpu_rq(cpu)->lock is locked

 *

 * Returns: (void)

	/*

	 * If the CPU was currently mapped to a different value, we

	 * need to map it to the new value then remove the old value.

	 * Note, we must add the new value first, otherwise we risk the

	 * cpu being missed by the priority loop in cpupri_find.

		/*

		 * When adding a new vector, we update the mask first,

		 * do a write memory barrier, and then update the count, to

		 * make sure the vector is visible when count is set.

		/*

		 * Because the order of modification of the vec->count

		 * is important, we must make sure that the update

		 * of the new prio is seen before we decrement the

		 * old prio. This makes sure that the loop sees

		 * one or the other when we raise the priority of

		 * the run queue. We don't care about when we lower the

		 * priority, as that will trigger an rt pull anyway.

		 *

		 * We only need to do a memory barrier if we updated

		 * the new priority vec.

		/*

		 * When removing from the vector, we decrement the counter first

		 * do a memory barrier and then clear the mask.

/**

 * cpupri_init - initialize the cpupri structure

 * @cp: The cpupri context

 *

 * Return: -ENOMEM on memory allocation failure.

/**

 * cpupri_cleanup - clean up the cpupri structure

 * @cp: The cpupri context

 SPDX-License-Identifier: GPL-2.0

/*

 * Deadline Scheduling Class (SCHED_DEADLINE)

 *

 * Earliest Deadline First (EDF) + Constant Bandwidth Server (CBS).

 *

 * Tasks that periodically executes their instances for less than their

 * runtime won't miss any of their deadlines.

 * Tasks that are not periodic or sporadic or that tries to execute more

 * than their reserved bandwidth will be slowed down (and may potentially

 * miss some of their deadlines), and won't affect any other task.

 *

 * Copyright (C) 2012 Dario Faggioli <raistlin@linux.it>,

 *                    Juri Lelli <juri.lelli@gmail.com>,

 *                    Michael Trimarchi <michael@amarulasolutions.com>,

 *                    Fabio Checconi <fchecconi@gmail.com>

/*

 * XXX Fix: If 'rq->rd == def_root_domain' perform AC against capacity

 * of the CPU the task is running on rather rd's \Sum CPU capacity.

 overflow */

 kick cpufreq (see the comment in kernel/sched/sched.h). */

 underflow */

 kick cpufreq (see the comment in kernel/sched/sched.h). */

 overflow */

 underflow */

		/*

		 * If the timer handler is currently running and the

		 * timer cannot be canceled, inactive_task_timer()

		 * will see that dl_not_contending is not set, and

		 * will not touch the rq's active utilization,

		 * so we are still safe.

/*

 * The utilization of a task cannot be immediately removed from

 * the rq active utilization (running_bw) when the task blocks.

 * Instead, we have to wait for the so called "0-lag time".

 *

 * If a task blocks before the "0-lag time", a timer (the inactive

 * timer) is armed, and running_bw is decreased when the timer

 * fires.

 *

 * If the task wakes up again before the inactive timer fires,

 * the timer is canceled, whereas if the task wakes up after the

 * inactive timer fired (and running_bw has been decreased) the

 * task's utilization has to be added to running_bw again.

 * A flag in the deadline scheduling entity (dl_non_contending)

 * is used to avoid race conditions between the inactive timer handler

 * and task wakeups.

 *

 * The following diagram shows how running_bw is updated. A task is

 * "ACTIVE" when its utilization contributes to running_bw; an

 * "ACTIVE contending" task is in the TASK_RUNNING state, while an

 * "ACTIVE non contending" task is a blocked task for which the "0-lag time"

 * has not passed yet. An "INACTIVE" task is a task for which the "0-lag"

 * time already passed, which does not contribute to running_bw anymore.

 *                              +------------------+

 *             wakeup           |    ACTIVE        |

 *          +------------------>+   contending     |

 *          | add_running_bw    |                  |

 *          |                   +----+------+------+

 *          |                        |      ^

 *          |                dequeue |      |

 * +--------+-------+                |      |

 * |                |   t >= 0-lag   |      | wakeup

 * |    INACTIVE    |<---------------+      |

 * |                | sub_running_bw |      |

 * +--------+-------+                |      |

 *          ^                        |      |

 *          |              t < 0-lag |      |

 *          |                        |      |

 *          |                        V      |

 *          |                   +----+------+------+

 *          | sub_running_bw    |    ACTIVE        |

 *          +-------------------+                  |

 *            inactive timer    |  non contending  |

 *            fired             +------------------+

 *

 * The task_non_contending() function is invoked when a task

 * blocks, and checks if the 0-lag time already passed or

 * not (in the first case, it directly updates running_bw;

 * in the second case, it arms the inactive timer).

 *

 * The task_contending() function is invoked when a task wakes

 * up, and checks if the task is still in the "ACTIVE non contending"

 * state or not (in the second case, it updates running_bw).

	/*

	 * If this is a non-deadline task that has been boosted,

	 * do nothing

	/*

	 * Using relative times instead of the absolute "0-lag time"

	 * allows to simplify the code

	/*

	 * If the "0-lag time" already passed, decrease the active

	 * utilization now, instead of starting a timer

	/*

	 * If this is a non-deadline task that has been boosted,

	 * do nothing

		/*

		 * If the timer handler is currently running and the

		 * timer cannot be canceled, inactive_task_timer()

		 * will see that dl_not_contending is not set, and

		 * will not touch the rq's active utilization,

		 * so we are still safe.

		/*

		 * Since "dl_non_contending" is not set, the

		 * task's utilization has already been removed from

		 * active utilization (either when the task blocked,

		 * when the "inactive timer" fired).

		 * So, add it back.

 zero means no -deadline tasks */

	/*

	 * Must be visible before the overload count is

	 * set (as in sched_rt.c).

	 *

	 * Matched by the barrier in pull_dl_task().

/*

 * The list of pushable -deadline task is not a plist, like in

 * sched_rt.c, it is an rb-tree with tasks ordered by deadline.

		/*

		 * If we cannot preempt any rq, fall back to pick any

		 * online CPU:

			/*

			 * Failed to find any suitable CPU.

			 * The task will never come back!

			/*

			 * If admission control is disabled we

			 * try a little harder to let the task

			 * run.

		/*

		 * Inactive timer is armed (or callback is running, but

		 * waiting for us to release rq locks). In any case, when it

		 * will fire (or continue), it will see running_bw of this

		 * task migrated to later_rq (and correctly handle it).

	/*

	 * And we finally need to fixup root_domain(s) bandwidth accounting,

	 * since p is still hanging out in the old (now moved to default) root

	 * domain.

 CONFIG_SMP */

/*

 * We are being explicitly informed that a new instance is starting,

 * and this means that:

 *  - the absolute deadline of the entity has to be placed at

 *    current time + relative deadline;

 *  - the runtime of the entity has to be set to the maximum value.

 *

 * The capability of specifying such event is useful whenever a -deadline

 * entity wants to (try to!) synchronize its behaviour with the scheduler's

 * one, and to (try to!) reconcile itself with its own scheduling

 * parameters.

	/*

	 * We are racing with the deadline timer. So, do nothing because

	 * the deadline timer handler will take care of properly recharging

	 * the runtime and postponing the deadline

	/*

	 * We use the regular wall clock time to set deadlines in the

	 * future; in fact, we must consider execution overheads (time

	 * spent on hardirq context, etc.).

/*

 * Pure Earliest Deadline First (EDF) scheduling does not deal with the

 * possibility of a entity lasting more than what it declared, and thus

 * exhausting its runtime.

 *

 * Here we are interested in making runtime overrun possible, but we do

 * not want a entity which is misbehaving to affect the scheduling of all

 * other entities.

 * Therefore, a budgeting strategy called Constant Bandwidth Server (CBS)

 * is used, in order to confine each entity within its own bandwidth.

 *

 * This function deals exactly with that, and ensures that when the runtime

 * of a entity is replenished, its deadline is also postponed. That ensures

 * the overrunning entity can't interfere with other entity in the system and

 * can't make them miss their deadlines. Reasons why this kind of overruns

 * could happen are, typically, a entity voluntarily trying to overcome its

 * runtime, or it just underestimated it during sched_setattr().

	/*

	 * This could be the case for a !-dl task that is boosted.

	 * Just go with full inherited parameters.

	/*

	 * We keep moving the deadline away until we get some

	 * available runtime for the entity. This ensures correct

	 * handling of situations where the runtime overrun is

	 * arbitrary large.

	/*

	 * At this point, the deadline really should be "in

	 * the future" with respect to rq->clock. If it's

	 * not, we are, for some reason, lagging too much!

	 * Anyway, after having warn userspace abut that,

	 * we still try to keep the things running by

	 * resetting the deadline and the budget of the

	 * entity.

/*

 * Here we check if --at time t-- an entity (which is probably being

 * [re]activated or, in general, enqueued) can use its remaining runtime

 * and its current deadline _without_ exceeding the bandwidth it is

 * assigned (function returns true if it can't). We are in fact applying

 * one of the CBS rules: when a task wakes up, if the residual runtime

 * over residual deadline fits within the allocated bandwidth, then we

 * can keep the current (absolute) deadline and residual budget without

 * disrupting the schedulability of the system. Otherwise, we should

 * refill the runtime and set the deadline a period in the future,

 * because keeping the current (absolute) deadline of the task would

 * result in breaking guarantees promised to other tasks (refer to

 * Documentation/scheduler/sched-deadline.rst for more information).

 *

 * This function returns true if:

 *

 *   runtime / (deadline - t) > dl_runtime / dl_deadline ,

 *

 * IOW we can't recycle current parameters.

 *

 * Notice that the bandwidth check is done against the deadline. For

 * task with deadline equal to period this is the same of using

 * dl_period instead of dl_deadline in the equation above.

	/*

	 * left and right are the two sides of the equation above,

	 * after a bit of shuffling to use multiplications instead

	 * of divisions.

	 *

	 * Note that none of the time values involved in the two

	 * multiplications are absolute: dl_deadline and dl_runtime

	 * are the relative deadline and the maximum runtime of each

	 * instance, runtime is the runtime left for the last instance

	 * and (deadline - t), since t is rq->clock, is the time left

	 * to the (absolute) deadline. Even if overflowing the u64 type

	 * is very unlikely to occur in both cases, here we scale down

	 * as we want to avoid that risk at all. Scaling down by 10

	 * means that we reduce granularity to 1us. We are fine with it,

	 * since this is only a true/false check and, anyway, thinking

	 * of anything below microseconds resolution is actually fiction

	 * (but still we want to give the user that illusion >;).

/*

 * Revised wakeup rule [1]: For self-suspending tasks, rather then

 * re-initializing task's runtime and deadline, the revised wakeup

 * rule adjusts the task's runtime to avoid the task to overrun its

 * density.

 *

 * Reasoning: a task may overrun the density if:

 *    runtime / (deadline - t) > dl_runtime / dl_deadline

 *

 * Therefore, runtime can be adjusted to:

 *     runtime = (dl_runtime / dl_deadline) * (deadline - t)

 *

 * In such way that runtime will be equal to the maximum density

 * the task can use without breaking any rule.

 *

 * [1] Luca Abeni, Giuseppe Lipari, and Juri Lelli. 2015. Constant

 * bandwidth server revisited. SIGBED Rev. 11, 4 (January 2015), 19-24.

	/*

	 * If the task has deadline < period, and the deadline is in the past,

	 * it should already be throttled before this check.

	 *

	 * See update_dl_entity() comments for further details.

/*

 * Regarding the deadline, a task with implicit deadline has a relative

 * deadline == relative period. A task with constrained deadline has a

 * relative deadline <= relative period.

 *

 * We support constrained deadline tasks. However, there are some restrictions

 * applied only for tasks which do not have an implicit deadline. See

 * update_dl_entity() to know more about such restrictions.

 *

 * The dl_is_implicit() returns true if the task has an implicit deadline.

/*

 * When a deadline entity is placed in the runqueue, its runtime and deadline

 * might need to be updated. This is done by a CBS wake up rule. There are two

 * different rules: 1) the original CBS; and 2) the Revisited CBS.

 *

 * When the task is starting a new period, the Original CBS is used. In this

 * case, the runtime is replenished and a new absolute deadline is set.

 *

 * When a task is queued before the begin of the next period, using the

 * remaining runtime and deadline could make the entity to overflow, see

 * dl_entity_overflow() to find more about runtime overflow. When such case

 * is detected, the runtime and deadline need to be updated.

 *

 * If the task has an implicit deadline, i.e., deadline == period, the Original

 * CBS is applied. the runtime is replenished and a new absolute deadline is

 * set, as in the previous cases.

 *

 * However, the Original CBS does not work properly for tasks with

 * deadline < period, which are said to have a constrained deadline. By

 * applying the Original CBS, a constrained deadline task would be able to run

 * runtime/deadline in a period. With deadline < period, the task would

 * overrun the runtime/period allowed bandwidth, breaking the admission test.

 *

 * In order to prevent this misbehave, the Revisited CBS is used for

 * constrained deadline tasks when a runtime overflow is detected. In the

 * Revisited CBS, rather than replenishing & setting a new absolute deadline,

 * the remaining runtime of the task is reduced to avoid runtime overflow.

 * Please refer to the comments update_dl_revised_wakeup() function to find

 * more about the Revised CBS rule.

/*

 * If the entity depleted all its runtime, and if we want it to sleep

 * while waiting for some new execution time to become available, we

 * set the bandwidth replenishment timer to the replenishment instant

 * and try to activate it.

 *

 * Notice that it is important for the caller to know if the timer

 * actually started or not (i.e., the replenishment instant is in

 * the future or in the past).

	/*

	 * We want the timer to fire at the deadline, but considering

	 * that it is actually coming from rq->clock and not from

	 * hrtimer's time base reading.

	/*

	 * If the expiry time already passed, e.g., because the value

	 * chosen as the deadline is too small, don't even try to

	 * start the timer in the past!

	/*

	 * !enqueued will guarantee another callback; even if one is already in

	 * progress. This ensures a balanced {get,put}_task_struct().

	 *

	 * The race against __run_timer() clearing the enqueued state is

	 * harmless because we're holding task_rq()->lock, therefore the timer

	 * expiring after we've done the check will wait on its task_rq_lock()

	 * and observe our state.

/*

 * This is the bandwidth enforcement timer callback. If here, we know

 * a task is not on its dl_rq, since the fact that the timer was running

 * means the task is throttled and needs a runtime replenishment.

 *

 * However, what we actually do depends on the fact the task is active,

 * (it is on its rq) or has been removed from there by a call to

 * dequeue_task_dl(). In the former case we must issue the runtime

 * replenishment and add the task back to the dl_rq; in the latter, we just

 * do nothing but clearing dl_throttled, so that runtime and deadline

 * updating (and the queueing back to dl_rq) will be done by the

 * next call to enqueue_task_dl().

	/*

	 * The task might have changed its scheduling policy to something

	 * different than SCHED_DEADLINE (through switched_from_dl()).

	/*

	 * The task might have been boosted by someone else and might be in the

	 * boosting/deboosting path, its not throttled.

	/*

	 * Spurious timer due to start_dl_timer() race; or we already received

	 * a replenishment from rt_mutex_setprio().

	/*

	 * If the throttle happened during sched-out; like:

	 *

	 *   schedule()

	 *     deactivate_task()

	 *       dequeue_task_dl()

	 *         update_curr_dl()

	 *           start_dl_timer()

	 *         __dequeue_task_dl()

	 *     prev->on_rq = 0;

	 *

	 * We can be both throttled and !queued. Replenish the counter

	 * but do not enqueue -- wait for our wakeup to do that.

		/*

		 * If the runqueue is no longer available, migrate the

		 * task elsewhere. This necessarily changes rq.

		/*

		 * Now that the task has been migrated to the new RQ and we

		 * have that locked, proceed as normal and enqueue the task

		 * there.

	/*

	 * Queueing this task back might have overloaded rq, check if we need

	 * to kick someone away.

		/*

		 * Nothing relies on rq->lock after this, so its safe to drop

		 * rq->lock.

	/*

	 * This can free the task_struct, including this hrtimer, do not touch

	 * anything related to that after this.

/*

 * During the activation, CBS checks if it can reuse the current task's

 * runtime and period. If the deadline of the task is in the past, CBS

 * cannot use the runtime, and so it replenishes the task. This rule

 * works fine for implicit deadline tasks (deadline == period), and the

 * CBS was designed for implicit deadline tasks. However, a task with

 * constrained deadline (deadline < period) might be awakened after the

 * deadline, but before the next period. In this case, replenishing the

 * task would allow it to run for runtime / deadline. As in this case

 * deadline < period, CBS enables a task to run for more than the

 * runtime / period. In a very loaded system, this can cause a domino

 * effect, making other tasks miss their deadlines.

 *

 * To avoid this problem, in the activation of a constrained deadline

 * task after the deadline but before the next period, throttle the

 * task and set the replenishing timer to the begin of the next period,

 * unless it is boosted.

/*

 * This function implements the GRUB accounting rule:

 * according to the GRUB reclaiming algorithm, the runtime is

 * not decreased as "dq = -dt", but as

 * "dq = -max{u / Umax, (1 - Uinact - Uextra)} dt",

 * where u is the utilization of the task, Umax is the maximum reclaimable

 * utilization, Uinact is the (per-runqueue) inactive utilization, computed

 * as the difference between the "total runqueue utilization" and the

 * runqueue active utilization, and Uextra is the (per runqueue) extra

 * reclaimable utilization.

 * Since rq->dl.running_bw and rq->dl.this_bw contain utilizations

 * multiplied by 2^BW_SHIFT, the result has to be shifted right by

 * BW_SHIFT.

 * Since rq->dl.bw_ratio contains 1 / Umax multiplied by 2^RATIO_SHIFT,

 * dl_bw is multiped by rq->dl.bw_ratio and shifted right by RATIO_SHIFT.

 * Since delta is a 64 bit variable, to have an overflow its value

 * should be larger than 2^(64 - 20 - 8), which is more than 64 seconds.

 * So, overflow is not an issue here.

 Utot - Uact */

	/*

	 * Instead of computing max{u * bw_ratio, (1 - u_inact - u_extra)},

	 * we compare u_inact + rq->dl.extra_bw with

	 * 1 - (u * rq->dl.bw_ratio >> RATIO_SHIFT), because

	 * u_inact + rq->dl.extra_bw can be larger than

	 * 1 * (so, 1 - u_inact - rq->dl.extra_bw would be negative

	 * leading to wrong results)

/*

 * Update the current task's runtime statistics (provided it is still

 * a -deadline task and has not been removed from the dl_rq).

	/*

	 * Consumed budget is computed considering the time as

	 * observed by schedulable tasks (excluding time spent

	 * in hardirq context, etc.). Deadlines are instead

	 * computed using hard walltime. This seems to be the more

	 * natural solution, but the full ramifications of this

	 * approach need further study.

	/*

	 * For tasks that participate in GRUB, we implement GRUB-PA: the

	 * spare reclaimed bandwidth is used to clock down frequency.

	 *

	 * For the others, we still need to scale reservation parameters

	 * according to current frequency and CPU maximum capacity.

 If requested, inform the user about runtime overruns. */

	/*

	 * Because -- for now -- we share the rt bandwidth, we need to

	 * account our runtime there too, otherwise actual rt tasks

	 * would be able to exceed the shared quota.

	 *

	 * Account to the root rt group for now.

	 *

	 * The solution we're working towards is having the RT groups scheduled

	 * using deadline servers -- however there's a few nasties to figure

	 * out before that can happen.

		/*

		 * We'll let actual RT tasks worry about the overflow here, we

		 * have our own CBS to keep us inline; only account when RT

		 * bandwidth is relevant.

	/*

	 * Since we may have removed our earliest (and/or next earliest)

	 * task we must recompute them.

 CONFIG_SMP */

	/*

	 * If this is a wakeup or a new instance, the scheduling

	 * parameters of the task might need updating. Otherwise,

	 * we want a replenishment of its runtime.

		/*

		 * Because of delays in the detection of the overrun of a

		 * thread's runtime, it might be the case that a thread

		 * goes to sleep in a rt mutex with negative runtime. As

		 * a consequence, the thread will be throttled.

		 *

		 * While waiting for the mutex, this thread can also be

		 * boosted via PI, resulting in a thread that is throttled

		 * and boosted at the same time.

		 *

		 * In this case, the boost overrides the throttle.

			/*

			 * The replenish timer needs to be canceled. No

			 * problem if it fires concurrently: boosted threads

			 * are ignored in dl_task_timer().

		/*

		 * Special case in which we have a !SCHED_DEADLINE task that is going

		 * to be deboosted, but exceeds its runtime while doing so. No point in

		 * replenishing it, as it's going to return back to its original

		 * scheduling class after this. If it has been throttled, we need to

		 * clear the flag, otherwise the task may wake up as throttled after

		 * being boosted again with no means to replenish the runtime and clear

		 * the throttle.

	/*

	 * Check if a constrained deadline task was activated

	 * after the deadline but before the next period.

	 * If that is the case, the task will be throttled and

	 * the replenishment timer will be set to the next period.

	/*

	 * If p is throttled, we do not enqueue it. In fact, if it exhausted

	 * its budget it needs a replenishment and, since it now is on

	 * its rq, the bandwidth timer callback (which clearly has not

	 * run yet) will take care of this.

	 * However, the active utilization does not depend on the fact

	 * that the task is on the runqueue or not (but depends on the

	 * task's state - in GRUB parlance, "inactive" vs "active contending").

	 * In other words, even if a task is throttled its utilization must

	 * be counted in the active utilization; hence, we need to call

	 * add_running_bw().

	/*

	 * This check allows to start the inactive timer (or to immediately

	 * decrease the active utilization, if needed) in two cases:

	 * when the task blocks and when it is terminating

	 * (p->state == TASK_DEAD). We can handle the two cases in the same

	 * way, because from GRUB's point of view the same thing is happening

	 * (the task moves from "active contending" to "active non contending"

	 * or "inactive")

/*

 * Yield task semantic for -deadline tasks is:

 *

 *   get off from the CPU until our next instance, with

 *   a new runtime. This is of little use now, since we

 *   don't have a bandwidth reclaiming mechanism. Anyway,

 *   bandwidth reclaiming is planned for the future, and

 *   yield_task_dl will indicate that some spare budget

 *   is available for other task instances to use it.

	/*

	 * We make the task go to sleep until its current deadline by

	 * forcing its runtime to zero. This way, update_curr_dl() stops

	 * it and the bandwidth timer will wake it up and will give it

	 * new scheduling parameters (thanks to dl_yielded=1).

	/*

	 * Tell update_rq_clock() that we've just updated,

	 * so we don't do microscopic update in schedule()

	 * and double the fastpath cost.

 unlocked access */

	/*

	 * If we are dealing with a -deadline task, we must

	 * decide where to wake it up.

	 * If it has a later deadline and the current task

	 * on this rq can't move (provided the waking task

	 * can!) we prefer to send it somewhere else. On the

	 * other hand, if it has a shorter deadline, we

	 * try to make it stay here, it might be important.

	/*

	 * Take the capacity of the CPU into account to

	 * ensure it fits the requirement of the task.

	/*

	 * Since p->state == TASK_WAKING, set_task_cpu() has been called

	 * from try_to_wake_up(). Hence, p->pi_lock is locked, but

	 * rq->lock is not... So, lock it

		/*

		 * If the timer handler is currently running and the

		 * timer cannot be canceled, inactive_task_timer()

		 * will see that dl_not_contending is not set, and

		 * will not touch the rq's active utilization,

		 * so we are still safe.

	/*

	 * Current can't be migrated, useless to reschedule,

	 * let's hope p can move out.

	/*

	 * p is migratable, so let's not schedule it and

	 * see if it is pushed or pulled somewhere else.

		/*

		 * This is OK, because current is on_cpu, which avoids it being

		 * picked for load-balance and preemption/IRQs are still

		 * disabled avoiding further scheduler activity on it and we've

		 * not yet started the picking loop.

 CONFIG_SMP */

/*

 * Only called when both the current and waking task are -deadline

 * tasks.

	/*

	 * In the unlikely case current and p have the same deadline

	 * let us try to decide what's the best thing to do...

 CONFIG_SMP */

 !CONFIG_SCHED_HRTICK */

 You can't push away the running task */

/*

 * scheduler tick hitting a task of our scheduling class.

 *

 * NOTE: This function can be called remotely by the tick offload that

 * goes along full dynticks. Therefore no local assumption can be made

 * and everything must be accessed through the @rq and @curr passed in

 * parameters.

	/*

	 * Even when we have runtime, update_curr_dl() might have resulted in us

	 * not being the leftmost task anymore. In that case NEED_RESCHED will

	 * be set and schedule() will start a new hrtick for the next task.

	/*

	 * SCHED_DEADLINE tasks cannot fork and this is achieved through

	 * sched_fork()

 Only try algorithms three times */

/*

 * Return the earliest pushable rq's task, which is suitable to be executed

 * on the CPU, NULL otherwise:

 Make sure the mask is initialized first */

	/*

	 * We have to consider system topology and task affinity

	 * first, then we can look for a suitable CPU.

	/*

	 * If we are here, some targets have been found, including

	 * the most suitable which is, among the runqueues where the

	 * current tasks have later deadlines than the task's one, the

	 * rq with the latest possible one.

	 *

	 * Now we check how well this matches with task's

	 * affinity and system topology.

	 *

	 * The last CPU where the task run is our first

	 * guess, since it is most likely cache-hot there.

	/*

	 * Check if this_cpu is to be skipped (i.e., it is

	 * not in the mask) or not.

			/*

			 * If possible, preempting this_cpu is

			 * cheaper than migrating.

			/*

			 * Last chance: if a CPU being in both later_mask

			 * and current sd span is valid, that becomes our

			 * choice. Of course, the latest possible CPU is

			 * already under consideration through later_mask.

	/*

	 * At this point, all our guesses failed, we just return

	 * 'something', and let the caller sort the things out.

 Locks the rq it finds */

			/*

			 * Target rq has tasks of equal or earlier deadline,

			 * retrying does not release any lock and is unlikely

			 * to yield a different result.

 Retry if something changed. */

		/*

		 * If the rq we found has no -deadline task, or

		 * its earliest one has a later deadline than our

		 * task, the rq is a good one.

 Otherwise we try again. */

/*

 * See if the non running -deadline tasks on this rq

 * can be sent to some other CPU where they can preempt

 * and start executing.

	/*

	 * If next_task preempts rq->curr, and rq->curr

	 * can move away, it makes sense to just reschedule

	 * without going further in pushing next_task.

 We might release rq lock */

 Will lock the rq it'll find */

		/*

		 * We must check all this again, since

		 * find_lock_later_rq releases rq->lock and it is

		 * then possible that next_task has migrated.

			/*

			 * The task is still there. We don't try

			 * again, some other CPU will pull it when ready.

 No more tasks */

	/*

	 * Update the later_rq clock here, because the clock is used

	 * by the cpufreq_update_util() inside __add_running_bw().

 push_dl_task() will return true if it moved a -deadline task */

	/*

	 * Match the barrier from dl_set_overloaded; this guarantees that if we

	 * see overloaded we must also see the dlo_mask bit.

		/*

		 * It looks racy, abd it is! However, as in sched_rt.c,

		 * we are fine with this.

 Might drop this_rq->lock */

		/*

		 * If there are no more pullable tasks on the

		 * rq, we're done with it.

		/*

		 * We found a task to be pulled if:

		 *  - it preempts our current (if there's one),

		 *  - it will preempt the last one we pulled (if any).

			/*

			 * Then we pull iff p has actually an earlier

			 * deadline than the current task of its runqueue.

 Is there any other task even earlier? */

/*

 * Since the task is not running and a reschedule is not going to happen

 * anytime soon on its runqueue, we try pushing it away now.

	/*

	 * Migrating a SCHED_DEADLINE task between exclusive

	 * cpusets (different root_domains) entails a bandwidth

	 * update. We already made space for us in the destination

	 * domain (see cpuset_can_attach()).

		/*

		 * We now free resources of the root_domain we are migrating

		 * off. In the worst case, sched_setattr() may temporary fail

		 * until we complete the update.

 Assumes rq->lock is held */

 Assumes rq->lock is held */

 CONFIG_SMP */

	/*

	 * task_non_contending() can start the "inactive timer" (if the 0-lag

	 * time is in the future). If the task switches back to dl before

	 * the "inactive timer" fires, it can continue to consume its current

	 * runtime using its current deadline. If it stays outside of

	 * SCHED_DEADLINE until the 0-lag time passes, inactive_task_timer()

	 * will reset the task parameters.

		/*

		 * Inactive timer is armed. However, p is leaving DEADLINE and

		 * might migrate away from this rq while continuing to run on

		 * some other class. We need to remove its contribution from

		 * this rq running_bw now, or sub_rq_bw (below) will complain.

	/*

	 * We cannot use inactive_task_timer() to invoke sub_running_bw()

	 * at the 0-lag time, because the task could have been migrated

	 * while SCHED_OTHER in the meanwhile.

	/*

	 * Since this might be the only -deadline task on the rq,

	 * this is the right place to try to pull some other one

	 * from an overloaded CPU, if any.

/*

 * When switching to -deadline, we may overload the rq, then

 * we try to push someone off, if possible.

 If p is not queued we will update its parameters at next wakeup. */

/*

 * If the scheduling parameters of a -deadline task changed,

 * a push or pull operation might be needed.

		/*

		 * This might be too much, but unfortunately

		 * we don't have the old deadline value, and

		 * we can't argue if the task is increasing

		 * or lowering its prio, so...

		/*

		 * If we now have a earlier deadline task than p,

		 * then reschedule, provided p is still on this

		 * runqueue.

		/*

		 * Again, we don't know if p has a earlier

		 * or later deadline, so let's blindly set a

		 * (maybe not needed) rescheduling point.

 CONFIG_SMP */

 Used for dl_bw check and update, used under sched_rt_handler()::mutex */

	/*

	 * Here we want to check the bandwidth not being set to some

	 * value smaller than the currently allocated bandwidth in

	 * any of the root_domains.

/*

 * We must be sure that accepting a new task (or allowing changing the

 * parameters of an existing one) is consistent with the bandwidth

 * constraints. If yes, this function also accordingly updates the currently

 * allocated bandwidth to reflect the new situation.

 *

 * This function is called while holding p's rq->lock.

 !deadline task may carry old deadline bandwidth */

	/*

	 * Either if a task, enters, leave, or stays -deadline but changes

	 * its parameters, we may need to update accordingly the total

	 * allocated bandwidth of the container.

		/*

		 * XXX this is slightly incorrect: when the task

		 * utilization decreases, we should delay the total

		 * utilization change until the task's 0-lag point.

		 * But this would require to set the task's "inactive

		 * timer" when the task is not inactive.

		/*

		 * Do not decrease the total deadline utilization here,

		 * switched_from_dl() will take care to do it at the correct

		 * (0-lag) time.

/*

 * This function initializes the sched_dl_entity of a newly becoming

 * SCHED_DEADLINE task.

 *

 * Only the static values are considered here, the actual runtime and the

 * absolute deadline will be properly calculated when the task is enqueued

 * for the first time with its new policy.

/*

 * Default limits for DL period; on the top end we guard against small util

 * tasks still getting ridiculously long effective runtimes, on the bottom end we

 * guard against timer DoS.

 ~4 seconds */

 100 us */

/*

 * This function validates the new parameters of a -deadline task.

 * We ask for the deadline not being zero, and greater or equal

 * than the runtime, as well as the period of being zero or

 * greater than deadline. Furthermore, we have to be sure that

 * user parameters are above the internal resolution of 1us (we

 * check sched_runtime only since it is always the smaller one) and

 * below 2^63 ns (we have to check both sched_deadline and

 * sched_period, as the latter can be zero).

 special dl tasks don't actually use any parameter */

 deadline != 0 */

	/*

	 * Since we truncate DL_SCALE bits, make sure we're at least

	 * that big.

	/*

	 * Since we use the MSB for wrap-around and sign issues, make

	 * sure it's not set (mind that period can be equal to zero).

 runtime <= deadline <= period (if period != 0) */

/*

 * This function clears the sched_dl_entity static params.

		/*

		 * We reserve space for this task in the destination

		 * root_domain, as we can't fail after this point.

		 * We will free resources in the source root_domain

		 * later on (see set_cpus_allowed_dl()).

 CONFIG_SCHED_DEBUG */

 SPDX-License-Identifier: GPL-2.0

/*

 * Per Entity Load Tracking

 *

 *  Copyright (C) 2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>

 *

 *  Interactivity improvements by Mike Galbraith

 *  (C) 2007 Mike Galbraith <efault@gmx.de>

 *

 *  Various enhancements by Dmitry Adamushko.

 *  (C) 2007 Dmitry Adamushko <dmitry.adamushko@gmail.com>

 *

 *  Group scheduling enhancements by Srivatsa Vaddagiri

 *  Copyright IBM Corporation, 2007

 *  Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>

 *

 *  Scaled math optimizations by Thomas Gleixner

 *  Copyright (C) 2007, Thomas Gleixner <tglx@linutronix.de>

 *

 *  Adaptive scheduling granularity, math enhancements by Peter Zijlstra

 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra

 *

 *  Move PELT related code from fair.c into this pelt.c file

 *  Author: Vincent Guittot <vincent.guittot@linaro.org>

/*

 * Approximate:

 *   val * y^n,    where y^32 ~= 0.5 (~1 scheduling period)

 after bounds checking we can collapse to 32-bit */

	/*

	 * As y^PERIOD = 1/2, we can combine

	 *    y^n = 1/2^(n/PERIOD) * y^(n%PERIOD)

	 * With a look-up table which covers y^n (n<PERIOD)

	 *

	 * To achieve constant time decay_load.

 y^0 == 1 */

	/*

	 * c1 = d1 y^p

	/*

	 *            p-1

	 * c2 = 1024 \Sum y^n

	 *            n=1

	 *

	 *              inf        inf

	 *    = 1024 ( \Sum y^n - \Sum y^n - y^0 )

	 *              n=0        n=p

/*

 * Accumulate the three separate parts of the sum; d1 the remainder

 * of the last (incomplete) period, d2 the span of full periods and d3

 * the remainder of the (incomplete) current period.

 *

 *           d1          d2           d3

 *           ^           ^            ^

 *           |           |            |

 *         |<->|<----------------->|<--->|

 * ... |---x---|------| ... |------|-----x (now)

 *

 *                           p-1

 * u' = (u + d1) y^p + 1024 \Sum y^n + d3 y^0

 *                           n=1

 *

 *    = u y^p +					(Step 1)

 *

 *                     p-1

 *      d1 y^p + 1024 \Sum y^n + d3 y^0		(Step 2)

 *                     n=1

 p == 0 -> delta < 1024 */

 A period is 1024us (~1ms) */

	/*

	 * Step 1: decay old *_sum if we crossed period boundaries.

		/*

		 * Step 2

			/*

			 * This relies on the:

			 *

			 * if (!load)

			 *	runnable = running = 0;

			 *

			 * clause from ___update_load_sum(); this results in

			 * the below usage of @contrib to disappear entirely,

			 * so no point in calculating it.

/*

 * We can represent the historical contribution to runnable average as the

 * coefficients of a geometric series.  To do this we sub-divide our runnable

 * history into segments of approximately 1ms (1024us); label the segment that

 * occurred N-ms ago p_N, with p_0 corresponding to the current period, e.g.

 *

 * [<- 1024us ->|<- 1024us ->|<- 1024us ->| ...

 *      p0            p1           p2

 *     (now)       (~1ms ago)  (~2ms ago)

 *

 * Let u_i denote the fraction of p_i that the entity was runnable.

 *

 * We then designate the fractions u_i as our co-efficients, yielding the

 * following representation of historical load:

 *   u_0 + u_1*y + u_2*y^2 + u_3*y^3 + ...

 *

 * We choose y based on the with of a reasonably scheduling period, fixing:

 *   y^32 = 0.5

 *

 * This means that the contribution to load ~32ms ago (u_32) will be weighted

 * approximately half as much as the contribution to load within the last ms

 * (u_0).

 *

 * When a period "rolls over" and we have new u_0`, multiplying the previous

 * sum again by y is sufficient to update:

 *   load_avg = u_0` + y*(u_0 + u_1*y + u_2*y^2 + ... )

 *            = u_0 + u_1*y + u_2*y^2 + ... [re-labeling u_i --> u_{i+1}]

	/*

	 * This should only happen when time goes backwards, which it

	 * unfortunately does during sched clock init when we swap over to TSC.

	/*

	 * Use 1024ns as the unit of measurement since it's a reasonable

	 * approximation of 1us and fast to compute.

	/*

	 * running is a subset of runnable (weight) so running can't be set if

	 * runnable is clear. But there are some corner cases where the current

	 * se has been already dequeued but cfs_rq->curr still points to it.

	 * This means that weight will be 0 but not running for a sched_entity

	 * but also for a cfs_rq if the latter becomes idle. As an example,

	 * this happens during idle_balance() which calls

	 * update_blocked_averages().

	 *

	 * Also see the comment in accumulate_sum().

	/*

	 * Now we know we crossed measurement unit boundaries. The *_avg

	 * accrues by two steps:

	 *

	 * Step 1: accumulate *_sum since last_update_time. If we haven't

	 * crossed period boundaries, finish.

/*

 * When syncing *_avg with *_sum, we must take into account the current

 * position in the PELT segment otherwise the remaining part of the segment

 * will be considered as idle time whereas it's not yet elapsed and this will

 * generate unwanted oscillation in the range [1002..1024[.

 *

 * The max value of *_sum varies with the position in the time segment and is

 * equals to :

 *

 *   LOAD_AVG_MAX*y + sa->period_contrib

 *

 * which can be simplified into:

 *

 *   LOAD_AVG_MAX - 1024 + sa->period_contrib

 *

 * because LOAD_AVG_MAX*y == LOAD_AVG_MAX-1024

 *

 * The same care must be taken when a sched entity is added, updated or

 * removed from a cfs_rq and we need to update sched_avg. Scheduler entities

 * and the cfs rq, to which they are attached, have the same position in the

 * time segment because they use the same clock. This means that we can use

 * the period_contrib of cfs_rq when updating the sched_avg of a sched_entity

 * if it's more convenient.

	/*

	 * Step 2: update *_avg.

/*

 * sched_entity:

 *

 *   task:

 *     se_weight()   = se->load.weight

 *     se_runnable() = !!on_rq

 *

 *   group: [ see update_cfs_group() ]

 *     se_weight()   = tg->weight * grq->load_avg / tg->load_avg

 *     se_runnable() = grq->h_nr_running

 *

 *   runnable_sum = se_runnable() * runnable = grq->runnable_sum

 *   runnable_avg = runnable_sum

 *

 *   load_sum := runnable

 *   load_avg = se_weight(se) * load_sum

 *

 * cfq_rq:

 *

 *   runnable_sum = \Sum se->avg.runnable_sum

 *   runnable_avg = \Sum se->avg.runnable_avg

 *

 *   load_sum = \Sum se_weight(se) * se->avg.load_sum

 *   load_avg = \Sum se->avg.load_avg

/*

 * rt_rq:

 *

 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked

 *   util_sum = cpu_scale * load_sum

 *   runnable_sum = util_sum

 *

 *   load_avg and runnable_avg are not supported and meaningless.

 *

/*

 * dl_rq:

 *

 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked

 *   util_sum = cpu_scale * load_sum

 *   runnable_sum = util_sum

 *

 *   load_avg and runnable_avg are not supported and meaningless.

 *

/*

 * thermal:

 *

 *   load_sum = \Sum se->avg.load_sum but se->avg.load_sum is not tracked

 *

 *   util_avg and runnable_load_avg are not supported and meaningless.

 *

 * Unlike rt/dl utilization tracking that track time spent by a cpu

 * running a rt/dl task through util_avg, the average thermal pressure is

 * tracked through load_avg. This is because thermal pressure signal is

 * time weighted "delta" capacity unlike util_avg which is binary.

 * "delta capacity" =  actual capacity  -

 *			capped capacity a cpu due to a thermal event.

/*

 * irq:

 *

 *   util_sum = \Sum se->avg.util_sum but se->avg.util_sum is not tracked

 *   util_sum = cpu_scale * load_sum

 *   runnable_sum = util_sum

 *

 *   load_avg and runnable_avg are not supported and meaningless.

 *

	/*

	 * We can't use clock_pelt because irq time is not accounted in

	 * clock_task. Instead we directly scale the running time to

	 * reflect the real amount of computation

	/*

	 * We know the time that has been used by interrupt since last update

	 * but we don't when. Let be pessimistic and assume that interrupt has

	 * happened just before the update. This is not so far from reality

	 * because interrupt will most probably wake up task and trig an update

	 * of rq clock during which the metric is updated.

	 * We start to decay with normal context time and then we add the

	 * interrupt context time.

	 * We can safely remove running from rq->clock because

	 * rq->clock += delta with delta >= running

/*

 * Pressure stall information for CPU, memory and IO

 *

 * Copyright (c) 2018 Facebook, Inc.

 * Author: Johannes Weiner <hannes@cmpxchg.org>

 *

 * Polling support by Suren Baghdasaryan <surenb@google.com>

 * Copyright (c) 2018 Google, Inc.

 *

 * When CPU, memory and IO are contended, tasks experience delays that

 * reduce throughput and introduce latencies into the workload. Memory

 * and IO contention, in addition, can cause a full loss of forward

 * progress in which the CPU goes idle.

 *

 * This code aggregates individual task delays into resource pressure

 * metrics that indicate problems with both workload health and

 * resource utilization.

 *

 *			Model

 *

 * The time in which a task can execute on a CPU is our baseline for

 * productivity. Pressure expresses the amount of time in which this

 * potential cannot be realized due to resource contention.

 *

 * This concept of productivity has two components: the workload and

 * the CPU. To measure the impact of pressure on both, we define two

 * contention states for a resource: SOME and FULL.

 *

 * In the SOME state of a given resource, one or more tasks are

 * delayed on that resource. This affects the workload's ability to

 * perform work, but the CPU may still be executing other tasks.

 *

 * In the FULL state of a given resource, all non-idle tasks are

 * delayed on that resource such that nobody is advancing and the CPU

 * goes idle. This leaves both workload and CPU unproductive.

 *

 * Naturally, the FULL state doesn't exist for the CPU resource at the

 * system level, but exist at the cgroup level, means all non-idle tasks

 * in a cgroup are delayed on the CPU resource which used by others outside

 * of the cgroup or throttled by the cgroup cpu.max configuration.

 *

 *	SOME = nr_delayed_tasks != 0

 *	FULL = nr_delayed_tasks != 0 && nr_running_tasks == 0

 *

 * The percentage of wallclock time spent in those compound stall

 * states gives pressure numbers between 0 and 100 for each resource,

 * where the SOME percentage indicates workload slowdowns and the FULL

 * percentage indicates reduced CPU utilization:

 *

 *	%SOME = time(SOME) / period

 *	%FULL = time(FULL) / period

 *

 *			Multiple CPUs

 *

 * The more tasks and available CPUs there are, the more work can be

 * performed concurrently. This means that the potential that can go

 * unrealized due to resource contention *also* scales with non-idle

 * tasks and CPUs.

 *

 * Consider a scenario where 257 number crunching tasks are trying to

 * run concurrently on 256 CPUs. If we simply aggregated the task

 * states, we would have to conclude a CPU SOME pressure number of

 * 100%, since *somebody* is waiting on a runqueue at all

 * times. However, that is clearly not the amount of contention the

 * workload is experiencing: only one out of 256 possible execution

 * threads will be contended at any given time, or about 0.4%.

 *

 * Conversely, consider a scenario of 4 tasks and 4 CPUs where at any

 * given time *one* of the tasks is delayed due to a lack of memory.

 * Again, looking purely at the task state would yield a memory FULL

 * pressure number of 0%, since *somebody* is always making forward

 * progress. But again this wouldn't capture the amount of execution

 * potential lost, which is 1 out of 4 CPUs, or 25%.

 *

 * To calculate wasted potential (pressure) with multiple processors,

 * we have to base our calculation on the number of non-idle tasks in

 * conjunction with the number of available CPUs, which is the number

 * of potential execution threads. SOME becomes then the proportion of

 * delayed tasks to possible threads, and FULL is the share of possible

 * threads that are unproductive due to delays:

 *

 *	threads = min(nr_nonidle_tasks, nr_cpus)

 *	   SOME = min(nr_delayed_tasks / threads, 1)

 *	   FULL = (threads - min(nr_running_tasks, threads)) / threads

 *

 * For the 257 number crunchers on 256 CPUs, this yields:

 *

 *	threads = min(257, 256)

 *	   SOME = min(1 / 256, 1)             = 0.4%

 *	   FULL = (256 - min(257, 256)) / 256 = 0%

 *

 * For the 1 out of 4 memory-delayed tasks, this yields:

 *

 *	threads = min(4, 4)

 *	   SOME = min(1 / 4, 1)               = 25%

 *	   FULL = (4 - min(3, 4)) / 4         = 25%

 *

 * [ Substitute nr_cpus with 1, and you can see that it's a natural

 *   extension of the single-CPU model. ]

 *

 *			Implementation

 *

 * To assess the precise time spent in each such state, we would have

 * to freeze the system on task changes and start/stop the state

 * clocks accordingly. Obviously that doesn't scale in practice.

 *

 * Because the scheduler aims to distribute the compute load evenly

 * among the available CPUs, we can track task state locally to each

 * CPU and, at much lower frequency, extrapolate the global state for

 * the cumulative stall times and the running averages.

 *

 * For each runqueue, we track:

 *

 *	   tSOME[cpu] = time(nr_delayed_tasks[cpu] != 0)

 *	   tFULL[cpu] = time(nr_delayed_tasks[cpu] && !nr_running_tasks[cpu])

 *	tNONIDLE[cpu] = time(nr_nonidle_tasks[cpu] != 0)

 *

 * and then periodically aggregate:

 *

 *	tNONIDLE = sum(tNONIDLE[i])

 *

 *	   tSOME = sum(tSOME[i] * tNONIDLE[i]) / tNONIDLE

 *	   tFULL = sum(tFULL[i] * tNONIDLE[i]) / tNONIDLE

 *

 *	   %SOME = tSOME / period

 *	   %FULL = tFULL / period

 *

 * This gives us an approximation of pressure that is practical

 * cost-wise, yet way more sensitive and accurate than periodic

 * sampling of the aggregate task states would be.

 Running averages - we need to be higher-res than loadavg */

 2 sec intervals */

 1/exp(2s/10s) as fixed-point */

 1/exp(2s/60s) */

 1/exp(2s/300s) */

 PSI trigger definitions */

 Min window size is 500ms */

 Max window size is 10s */

 10 updates per window */

 Sampling frequency in nanoseconds */

 System-level pressure and stall tracking */

 Init trigger-related members */

 Snapshot a coherent view of the CPU state */

 Calculate state time deltas against the previous snapshot */

		/*

		 * In addition to already concluded states, we also

		 * incorporate currently active states on the CPU,

		 * since states may last for many sampling periods.

		 *

		 * This way we keep our delta sampling buckets small

		 * (u32) and our reported pressure close to what's

		 * actually happening.

 Fill in zeroes for periods of no activity */

 Sample the most recent active period */

	/*

	 * Collect the per-cpu time buckets and average them into a

	 * single time sample that is normalized to wallclock time.

	 *

	 * For averaging, each CPU is weighted by its non-idle time in

	 * the sampling period. This eliminates artifacts from uneven

	 * loading, or even entirely idle CPUs.

	/*

	 * Integrate the sample into the running statistics that are

	 * reported to userspace: the cumulative stall times and the

	 * decaying averages.

	 *

	 * Pressure percentages are sampled at PSI_FREQ. We might be

	 * called more often when the user polls more frequently than

	 * that; we might be called less often when there is no task

	 * activity, thus no data, and clock ticks are sporadic. The

	 * below handles both.

 total= */

 avgX= */

	/*

	 * The periodic clock tick can get delayed for various

	 * reasons, especially on loaded systems. To avoid clock

	 * drift, we schedule the clock in fixed psi_period intervals.

	 * But the deltas we sample out of the per-cpu buckets above

	 * are based on the actual time elapsing between clock ticks.

		/*

		 * Due to the lockless sampling of the time buckets,

		 * recorded time deltas can slip into the next period,

		 * which under full pressure can result in samples in

		 * excess of the period length.

		 *

		 * We don't want to report non-sensical pressures in

		 * excess of 100%, nor do we want to drop such events

		 * on the floor. Instead we punt any overage into the

		 * future until pressure subsides. By doing this we

		 * don't underreport the occurring pressure curve, we

		 * just report it delayed by one period length.

		 *

		 * The error isn't cumulative. As soon as another

		 * delta slips from a period P to P+1, by definition

		 * it frees up its time T in P.

	/*

	 * If there is task activity, periodically fold the per-cpu

	 * times and feed samples into the running averages. If things

	 * are idle and there is no data to process, stop the clock.

	 * Once restarted, we'll catch up the running averages in one

	 * go - see calc_avgs() and missed_periods.

 Trigger tracking window manipulations */

/*

 * PSI growth tracking window update and growth calculation routine.

 *

 * This approximates a sliding tracking window by interpolating

 * partially elapsed windows using historical growth data from the

 * previous intervals. This minimizes memory requirements (by not storing

 * all the intermediate values in the previous window) and simplifies

 * the calculations. It works well because PSI signal changes only in

 * positive direction and over relatively small window sizes the growth

 * is close to linear.

	/*

	 * After each tracking window passes win->start_value and

	 * win->start_time get reset and win->prev_growth stores

	 * the average per-window growth of the previous window.

	 * win->prev_growth is then used to interpolate additional

	 * growth from the previous window assuming it was linear.

	/*

	 * On subsequent updates, calculate growth deltas and let

	 * watchers know when their specified thresholds are exceeded.

 Check for stall activity */

		/*

		 * Multiple triggers might be looking at the same state,

		 * remember to update group->polling_total[] once we've

		 * been through all of them. Also remember to extend the

		 * polling time if we see new stall activity.

 Calculate growth since last update */

 Limit event signaling to once per window */

 Generate an event */

 Schedule polling if it's not already scheduled. */

	/*

	 * Do not reschedule if already scheduled.

	 * Possible race with a timer scheduled after this check but before

	 * mod_timer below can be tolerated because group->polling_next_update

	 * will keep updates on schedule.

	/*

	 * kworker might be NULL in case psi_trigger_destroy races with

	 * psi_task_change (hotpath) which can't use locks

 Initialize trigger windows when entering polling mode */

		/*

		 * Keep the monitor active for at least the duration of the

		 * minimum tracking window as long as monitor states are

		 * changing.

	/*

	 * First we assess the aggregate resource states this CPU's

	 * tasks have been in since the last change, and account any

	 * SOME and FULL time these may have resulted in.

	 *

	 * Then we update the task counts according to the state

	 * change requested through the @clear and @set bits.

 Calculate state mask representing active states */

	/*

	 * Since we care about lost potential, a memstall is FULL

	 * when there are no other working tasks, but also when

	 * the CPU is actively reclaiming and nothing productive

	 * could run even if it were runnable. So when the current

	 * task in a cgroup is in_memstall, the corresponding groupc

	 * on that cpu is in PSI_MEM_FULL state.

	/*

	 * Periodic aggregation shuts off if there is a period of no

	 * task changes, so we wake it back up if necessary. However,

	 * don't do this if the task change is the aggregation worker

	 * itself going to sleep, or we'll ping-pong forever.

		/*

		 * When switching between tasks that have an identical

		 * runtime state, the cgroup that contains both tasks

		 * runtime state, the cgroup that contains both tasks

		 * we reach the first common ancestor. Iterate @next's

		 * ancestors only until we encounter @prev's ONCPU.

		/*

		 * When we're going to sleep, psi_dequeue() lets us handle

		 * TSK_RUNNING and TSK_IOWAIT here, where we can combine it

		 * with TSK_ONCPU and save walking common ancestors twice.

		/*

		 * TSK_ONCPU is handled up to the common ancestor. If we're tasked

		 * with dequeuing too, finish that for the rest of the hierarchy.

/**

 * psi_memstall_enter - mark the beginning of a memory stall section

 * @flags: flags to handle nested sections

 *

 * Marks the calling task as being stalled due to a lack of memory,

 * such as waiting for a refault or performing reclaim.

	/*

	 * in_memstall setting & accounting needs to be atomic wrt

	 * changes to the task's scheduling state, otherwise we can

	 * race with CPU migration.

/**

 * psi_memstall_leave - mark the end of an memory stall section

 * @flags: flags to handle nested memdelay sections

 *

 * Marks the calling task as no longer stalled due to lack of memory.

	/*

	 * in_memstall clearing & accounting needs to be atomic wrt

	 * changes to the task's scheduling state, otherwise we could

	 * race with CPU migration.

 All triggers must be removed by now */

/**

 * cgroup_move_task - move task to a different cgroup

 * @task: the task

 * @to: the target css_set

 *

 * Move task to a new cgroup and safely migrate its associated stall

 * state between the different groups.

 *

 * This function acquires the task's rq lock to lock out concurrent

 * changes to the task's scheduling state and - in case the task is

 * running - concurrent changes to its stall state.

		/*

		 * Lame to do this here, but the scheduler cannot be locked

		 * from the outside, so we move cgroups from inside sched/.

	/*

	 * We may race with schedule() dropping the rq lock between

	 * deactivating prev and switching to next. Because the psi

	 * updates from the deactivation are deferred to the switch

	 * callback to save cgroup tree updates, the task's scheduling

	 * state here is not coherent with its psi state:

	 *

	 * schedule()                   cgroup_move_task()

	 *   rq_lock()

	 *   deactivate_task()

	 *     p->on_rq = 0

	 *     psi_dequeue() // defers TSK_RUNNING & TSK_IOWAIT updates

	 *   pick_next_task()

	 *     rq_unlock()

	 *                                rq_lock()

	 *                                psi_task_change() // old cgroup

	 *                                task->cgroups = to

	 *                                psi_task_change() // new cgroup

	 *                                rq_unlock()

	 *     rq_lock()

	 *   psi_sched_switch() // does deferred updates in new cgroup

	 *

	 * Don't rely on the scheduling state. Use psi_flags instead.

 See comment above */

 CONFIG_CGROUPS */

 Update averages before reporting them */

 Check threshold */

	/*

	 * Wakeup waiters to stop polling. Can happen if cgroup is deleted

	 * from under a polling process.

 reset min update period for the remaining triggers */

 Destroy poll_task when the last trigger is destroyed */

	/*

	 * Wait for both *trigger_ptr from psi_trigger_replace and

	 * poll_task RCUs to complete their read-side critical sections

	 * before destroying the trigger and optionally the poll_task

	/*

	 * Stop kthread 'psimon' after releasing trigger_lock to prevent a

	 * deadlock while waiting for psi_poll_work to acquire trigger_lock

		/*

		 * After the RCU grace period has expired, the worker

		 * can no longer be found through group->poll_task.

 Take seq->lock to protect seq->private from concurrent writes */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Generic waiting primitives.

 *

 * (C) 2004 Nadia Yvette Chambers, Oracle

/*

 * Scan threshold to break wait queue walk.

 * This allows a waker to take a break from holding the

 * wait queue lock during the wait queue walk.

/*

 * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just

 * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve

 * number) then we wake that number of exclusive tasks, and potentially all

 * the non-exclusive tasks. Normally, exclusive tasks will be at the end of

 * the list and any non-exclusive tasks will be woken first. A priority task

 * may be at the head of the list, and can consume the event without any other

 * tasks being woken.

 *

 * There are circumstances in which we can try to wake a task which has already

 * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns

 * zero in this (rare) case, and we handle it by continuing to scan the queue.

/**

 * __wake_up - wake up threads blocked on a waitqueue.

 * @wq_head: the waitqueue

 * @mode: which threads

 * @nr_exclusive: how many wake-one or wake-many threads to wake up

 * @key: is directly passed to the wakeup function

 *

 * If this function wakes up a task, it executes a full memory barrier before

 * accessing the task state.

/*

 * Same as __wake_up but called with the spinlock in wait_queue_head_t held.

/**

 * __wake_up_sync_key - wake up threads blocked on a waitqueue.

 * @wq_head: the waitqueue

 * @mode: which threads

 * @key: opaque value to be passed to wakeup targets

 *

 * The sync wakeup differs that the waker knows that it will schedule

 * away soon, so while the target thread will be woken up, it will not

 * be migrated to another CPU - ie. the two threads are 'synchronized'

 * with each other. This can prevent needless bouncing between CPUs.

 *

 * On UP it can prevent extra preemption.

 *

 * If this function wakes up a task, it executes a full memory barrier before

 * accessing the task state.

/**

 * __wake_up_locked_sync_key - wake up a thread blocked on a locked waitqueue.

 * @wq_head: the waitqueue

 * @mode: which threads

 * @key: opaque value to be passed to wakeup targets

 *

 * The sync wakeup differs in that the waker knows that it will schedule

 * away soon, so while the target thread will be woken up, it will not

 * be migrated to another CPU - ie. the two threads are 'synchronized'

 * with each other. This can prevent needless bouncing between CPUs.

 *

 * On UP it can prevent extra preemption.

 *

 * If this function wakes up a task, it executes a full memory barrier before

 * accessing the task state.

/*

 * __wake_up_sync - see __wake_up_sync_key()

 For internal use only */

/*

 * Note: we use "set_current_state()" _after_ the wait-queue add,

 * because we need a memory barrier there on SMP, so that any

 * wake-function that tests for the wait-queue being active

 * will be guaranteed to see waitqueue addition _or_ subsequent

 * tests in this thread will see the wakeup having taken place.

 *

 * The spin_unlock() itself is semi-permeable and only protects

 * one way (it only protects stuff inside the critical region and

 * stops them from bleeding out - it would still allow subsequent

 * loads to move into the critical region).

 Returns true if we are the first waiter in the queue, false otherwise. */

		/*

		 * Exclusive waiter must not fail if it was selected by wakeup,

		 * it should "consume" the condition we were waiting for.

		 *

		 * The caller will recheck the condition and return success if

		 * we were already woken up, we can not miss the event because

		 * wakeup locks/unlocks the same wq_head->lock.

		 *

		 * But we need to ensure that set-condition + wakeup after that

		 * can't see us, it should wake up another exclusive waiter if

		 * we fail.

/*

 * Note! These two wait functions are entered with the

 * wait-queue lock held (and interrupts off in the _irq

 * case), so there is no race with testing the wakeup

 * condition in the caller before they add the wait

 * entry to the wake queue.

/**

 * finish_wait - clean up after waiting in a queue

 * @wq_head: waitqueue waited on

 * @wq_entry: wait descriptor

 *

 * Sets current thread back to running state and removes

 * the wait descriptor from the given waitqueue if still

 * queued.

	/*

	 * We can check for list emptiness outside the lock

	 * IFF:

	 *  - we use the "careful" check that verifies both

	 *    the next and prev pointers, so that there cannot

	 *    be any half-pending updates in progress on other

	 *    CPU's that we haven't seen yet (and that might

	 *    still change the stack area.

	 * and

	 *  - all other users take the lock (ie we can only

	 *    have _one_ other CPU that looks at or modifies

	 *    the list).

/*

 * DEFINE_WAIT_FUNC(wait, woken_wake_func);

 *

 * add_wait_queue(&wq_head, &wait);

 * for (;;) {

 *     if (condition)

 *         break;

 *

 *     // in wait_woken()			// in woken_wake_function()

 *

 *     p->state = mode;				wq_entry->flags |= WQ_FLAG_WOKEN;

 *     smp_mb(); // A				try_to_wake_up():

 *     if (!(wq_entry->flags & WQ_FLAG_WOKEN))	   <full barrier>

 *         schedule()				   if (p->state & mode)

 *     p->state = TASK_RUNNING;			      p->state = TASK_RUNNING;

 *     wq_entry->flags &= ~WQ_FLAG_WOKEN;	~~~~~~~~~~~~~~~~~~

 *     smp_mb(); // B				condition = true;

 * }						smp_mb(); // C

 * remove_wait_queue(&wq_head, &wait);		wq_entry->flags |= WQ_FLAG_WOKEN;

	/*

	 * The below executes an smp_mb(), which matches with the full barrier

	 * executed by the try_to_wake_up() in woken_wake_function() such that

	 * either we see the store to wq_entry->flags in woken_wake_function()

	 * or woken_wake_function() sees our store to current->state.

 A */

	/*

	 * The below executes an smp_mb(), which matches with the smp_mb() (C)

	 * in woken_wake_function() such that either we see the wait condition

	 * being true or the store to wq_entry->flags in woken_wake_function()

	 * follows ours in the coherence order.

 B */

 Pairs with the smp_store_mb() in wait_woken(). */

 C */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  kernel/sched/cpudl.c

 *

 *  Global CPU deadline management

 *

 *  Author: Juri Lelli <j.lelli@sssup.it>

 adapted from lib/prio_heap.c */

 pull largest child onto idx */

 actual push down of saved original values orig_* */

 pull parent onto idx */

 actual push up of saved original values orig_* */

/*

 * cpudl_find - find the best (later-dl) CPU in the system

 * @cp: the cpudl max-heap context

 * @p: the task

 * @later_mask: a mask to fill in with the selected CPUs (or NULL)

 *

 * Returns: int - CPUs were found

 Ensure the capacity of the CPUs fits the task. */

/*

 * cpudl_clear - remove a CPU from the cpudl max-heap

 * @cp: the cpudl max-heap context

 * @cpu: the target CPU

 *

 * Notes: assumes cpu_rq(cpu)->lock is locked

 *

 * Returns: (void)

		/*

		 * Nothing to remove if old_idx was invalid.

		 * This could happen if a rq_offline_dl is

		 * called for a CPU without -dl tasks running.

/*

 * cpudl_set - update the cpudl max-heap

 * @cp: the cpudl max-heap context

 * @cpu: the target CPU

 * @dl: the new earliest deadline for this CPU

 *

 * Notes: assumes cpu_rq(cpu)->lock is locked

 *

 * Returns: (void)

/*

 * cpudl_set_freecpu - Set the cpudl.free_cpus

 * @cp: the cpudl max-heap context

 * @cpu: rd attached CPU

/*

 * cpudl_clear_freecpu - Clear the cpudl.free_cpus

 * @cp: the cpudl max-heap context

 * @cpu: rd attached CPU

/*

 * cpudl_init - initialize the cpudl structure

 * @cp: the cpudl max-heap context

/*

 * cpudl_cleanup - clean up the cpudl structure

 * @cp: the cpudl max-heap context

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Housekeeping management. Manage the targets for routine code that can run on

 *  any CPU: unbound workqueues, timers, kthreads and any offloadable work.

 *

 * Copyright (C) 2017 Red Hat, Inc., Frederic Weisbecker

 * Copyright (C) 2017-2018 SUSE, Frederic Weisbecker

 *

 We need at least one CPU to handle housekeeping work */

		/*

		 * Skip unknown sub-parameter and validate that it is not

		 * containing an invalid character.

 Default behaviour for isolcpus without flags */

 SPDX-License-Identifier: GPL-2.0

/*

 * Scheduler topology setup/handling methods

 Protected by sched_domains_mutex: */

 !CONFIG_SCHED_DEBUG */

 CONFIG_SCHED_DEBUG */

 Generate a mask of SD flags with the SDF_NEEDS_GROUPS metaflag */

 Following flags need at least 2 groups */

 Following flags don't use groups */

 Flags needing groups don't count if only 1 group in parent */

/*

 * EAS can be used on a root domain if it meets all the following conditions:

 *    1. an Energy Model (EM) is available;

 *    2. the SD_ASYM_CPUCAPACITY flag is set in the sched_domain hierarchy.

 *    3. no SMT is detected.

 *    4. the EM complexity is low enough to keep scheduling overheads low;

 *    5. schedutil is driving the frequency of all CPUs of the rd;

 *    6. frequency invariance support is present;

 *

 * The complexity of the Energy Model is defined as:

 *

 *              C = nr_pd * (nr_cpus + nr_ps)

 *

 * with parameters defined as:

 *  - nr_pd:    the number of performance domains

 *  - nr_cpus:  the number of CPUs

 *  - nr_ps:    the sum of the number of performance states of all performance

 *              domains (for example, on a system with 2 performance domains,

 *              with 10 performance states each, nr_ps = 2 * 10 = 20).

 *

 * It is generally not a good idea to use such a model in the wake-up path on

 * very complex platforms because of the associated scheduling overheads. The

 * arbitrary constraint below prevents that. It makes EAS usable up to 16 CPUs

 * with per-CPU DVFS and less than 8 performance states each, for example.

 EAS is enabled for asymmetric CPU capacity topologies. */

 EAS definitely does *not* handle SMT */

 Skip already covered CPUs. */

 Do not attempt EAS if schedutil is not being used. */

 Create the new pd and add it to the local list. */

		/*

		 * Count performance domains and performance states for the

		 * complexity check.

 Bail out if the Energy Model complexity is too high. */

 Attach the new list of performance domains to the root domain. */

 CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL*/

		/*

		 * If we dont want to free the old_rd yet then

		 * set old_rd to NULL to skip the freeing later

		 * in this function:

/*

 * By default the system creates a single root-domain with all CPUs as

 * members (mimicking the global state we have today).

	/*

	 * A normal sched domain may have multiple group references, an

	 * overlapping domain, having private groups, only one.  Iterate,

	 * dropping group/capacity references, freeing where none remain.

/*

 * Keep a special pointer to the highest sched_domain that has

 * SD_SHARE_PKG_RESOURCE set (Last Level Cache Domain) for this

 * allows us to avoid some pointer chasing select_idle_sibling().

 *

 * Also keep a unique ID per domain (we use the first CPU number in

 * the cpumask of the domain), this allows us to quickly tell if

 * two CPUs are in the same cache domain, see cpus_share_cache().

/*

 * Attach the domain 'sd' to 'cpu' as its base domain. Callers must

 * hold the hotplug lock.

 Remove the sched domains which do not contribute to scheduling. */

			/*

			 * Transfer SD_PREFER_SIBLING down in case of a

			 * degenerate parent; the spans match for this

			 * so the property transfers.

			/*

			 * sched groups hold the flags of the child sched

			 * domain for convenience. Clear such flags since

			 * the child is being destroyed.

/*

 * Return the canonical balance CPU for this group, this is the first CPU

 * of this group that's also in the balance mask.

 *

 * The balance mask are all those CPUs that could actually end up at this

 * group. See build_balance_mask().

 *

 * Also see should_we_balance().

/*

 * NUMA topology (first read the regular topology blurb below)

 *

 * Given a node-distance table, for example:

 *

 *   node   0   1   2   3

 *     0:  10  20  30  20

 *     1:  20  10  20  30

 *     2:  30  20  10  20

 *     3:  20  30  20  10

 *

 * which represents a 4 node ring topology like:

 *

 *   0 ----- 1

 *   |       |

 *   |       |

 *   |       |

 *   3 ----- 2

 *

 * We want to construct domains and groups to represent this. The way we go

 * about doing this is to build the domains on 'hops'. For each NUMA level we

 * construct the mask of all nodes reachable in @level hops.

 *

 * For the above NUMA topology that gives 3 levels:

 *

 * NUMA-2	0-3		0-3		0-3		0-3

 *  groups:	{0-1,3},{1-3}	{0-2},{0,2-3}	{1-3},{0-1,3}	{0,2-3},{0-2}

 *

 * NUMA-1	0-1,3		0-2		1-3		0,2-3

 *  groups:	{0},{1},{3}	{0},{1},{2}	{1},{2},{3}	{0},{2},{3}

 *

 * NUMA-0	0		1		2		3

 *

 *

 * As can be seen; things don't nicely line up as with the regular topology.

 * When we iterate a domain in child domain chunks some nodes can be

 * represented multiple times -- hence the "overlap" naming for this part of

 * the topology.

 *

 * In order to minimize this overlap, we only build enough groups to cover the

 * domain. For instance Node-0 NUMA-2 would only get groups: 0-1,3 and 1-3.

 *

 * Because:

 *

 *  - the first group of each domain is its child domain; this

 *    gets us the first 0-1,3

 *  - the only uncovered node is 2, who's child domain is 1-3.

 *

 * However, because of the overlap, computing a unique CPU for each group is

 * more complicated. Consider for instance the groups of NODE-1 NUMA-2, both

 * groups include the CPUs of Node-0, while those CPUs would not in fact ever

 * end up at those groups (they would end up in group: 0-1,3).

 *

 * To correct this we have to introduce the group balance mask. This mask

 * will contain those CPUs in the group that can reach this group given the

 * (child) domain tree.

 *

 * With this we can once again compute balance_cpu and sched_group_capacity

 * relations.

 *

 * XXX include words on how balance_cpu is unique and therefore can be

 * used for sched_group_capacity links.

 *

 *

 * Another 'interesting' topology is:

 *

 *   node   0   1   2   3

 *     0:  10  20  20  30

 *     1:  20  10  20  20

 *     2:  20  20  10  20

 *     3:  30  20  20  10

 *

 * Which looks a little like:

 *

 *   0 ----- 1

 *   |     / |

 *   |   /   |

 *   | /     |

 *   2 ----- 3

 *

 * This topology is asymmetric, nodes 1,2 are fully connected, but nodes 0,3

 * are not.

 *

 * This leads to a few particularly weird cases where the sched_domain's are

 * not of the same number for each CPU. Consider:

 *

 * NUMA-2	0-3						0-3

 *  groups:	{0-2},{1-3}					{1-3},{0-2}

 *

 * NUMA-1	0-2		0-3		0-3		1-3

 *

 * NUMA-0	0		1		2		3

 *

/*

 * Build the balance mask; it contains only those CPUs that can arrive at this

 * group and should be considered to continue balancing.

 *

 * We do this during the group creation pass, therefore the group information

 * isn't complete yet, however since each group represents a (child) domain we

 * can fully construct this using the sched_domain bits (which are already

 * complete).

		/*

		 * Can happen in the asymmetric case, where these siblings are

		 * unused. The mask will not be empty because those CPUs that

		 * do have the top domain _should_ span the domain.

 If we would not end up here, we can't continue from here */

 We must not have empty masks here */

/*

 * XXX: This creates per-node group entries; since the load-balancer will

 * immediately access remote memory to construct this group's load-balance

 * statistics having the groups node local is of dubious benefit.

	/*

	 * Initialize sgc->capacity such that even if we mess up the

	 * domains and no possible iteration will get us here, we won't

	 * die on a /0 trap.

	/*

	 * The proper descendant would be the one whose child won't span out

	 * of sd

	/*

	 * As we are referencing sgc across different topology level, we need

	 * to go down to skip those sched_domains which don't contribute to

	 * scheduling because they will be degenerated in cpu_attach_domain

		/*

		 * Asymmetric node setups can result in situations where the

		 * domain tree is of unequal depth, make sure to skip domains

		 * that already cover the entire range.

		 *

		 * In that case build_sched_domains() will have terminated the

		 * iteration early and our sibling sd spans will be empty.

		 * Domains should always include the CPU they're built on, so

		 * check that.

		/*

		 * Usually we build sched_group by sibling's child sched_domain

		 * But for machines whose NUMA diameter are 3 or above, we move

		 * to build sched_group by sibling's proper descendant's child

		 * domain because sibling's child sched_domain will span out of

		 * the sched_domain being built as below.

		 *

		 * Smallest diameter=3 topology is:

		 *

		 *   node   0   1   2   3

		 *     0:  10  20  30  40

		 *     1:  20  10  20  30

		 *     2:  30  20  10  20

		 *     3:  40  30  20  10

		 *

		 *   0 --- 1 --- 2 --- 3

		 *

		 * NUMA-3       0-3             N/A             N/A             0-3

		 *  groups:     {0-2},{1-3}                                     {1-3},{0-2}

		 *

		 * NUMA-2       0-2             0-3             0-3             1-3

		 *  groups:     {0-1},{1-3}     {0-2},{2-3}     {1-3},{0-1}     {2-3},{0-2}

		 *

		 * NUMA-1       0-1             0-2             1-3             2-3

		 *  groups:     {0},{1}         {1},{2},{0}     {2},{3},{1}     {3},{2}

		 *

		 * NUMA-0       0               1               2               3

		 *

		 * The NUMA-2 groups for nodes 0 and 3 are obviously buggered, as the

		 * group span isn't a subset of the domain span.

/*

 * Package topology (also see the load-balance blurb in fair.c)

 *

 * The scheduler builds a tree structure to represent a number of important

 * topology features. By default (default_topology[]) these include:

 *

 *  - Simultaneous multithreading (SMT)

 *  - Multi-Core Cache (MC)

 *  - Package (DIE)

 *

 * Where the last one more or less denotes everything up to a NUMA node.

 *

 * The tree consists of 3 primary data structures:

 *

 *	sched_domain -> sched_group -> sched_group_capacity

 *	    ^ ^             ^ ^

 *          `-'             `-'

 *

 * The sched_domains are per-CPU and have a two way link (parent & child) and

 * denote the ever growing mask of CPUs belonging to that level of topology.

 *

 * Each sched_domain has a circular (double) linked list of sched_group's, each

 * denoting the domains of the level below (or individual CPUs in case of the

 * first domain level). The sched_group linked by a sched_domain includes the

 * CPU of that sched_domain [*].

 *

 * Take for instance a 2 threaded, 2 core, 2 cache cluster part:

 *

 * CPU   0   1   2   3   4   5   6   7

 *

 * DIE  [                             ]

 * MC   [             ] [             ]

 * SMT  [     ] [     ] [     ] [     ]

 *

 *  - or -

 *

 * DIE  0-7 0-7 0-7 0-7 0-7 0-7 0-7 0-7

 * MC	0-3 0-3 0-3 0-3 4-7 4-7 4-7 4-7

 * SMT  0-1 0-1 2-3 2-3 4-5 4-5 6-7 6-7

 *

 * CPU   0   1   2   3   4   5   6   7

 *

 * One way to think about it is: sched_domain moves you up and down among these

 * topology levels, while sched_group moves you sideways through it, at child

 * domain granularity.

 *

 * sched_group_capacity ensures each unique sched_group has shared storage.

 *

 * There are two related construction problems, both require a CPU that

 * uniquely identify each group (for a given domain):

 *

 *  - The first is the balance_cpu (see should_we_balance() and the

 *    load-balance blub in fair.c); for each group we only want 1 CPU to

 *    continue balancing at a higher domain.

 *

 *  - The second is the sched_group_capacity; we want all identical groups

 *    to share a single sched_group_capacity.

 *

 * Since these topologies are exclusive by construction. That is, its

 * impossible for an SMT thread to belong to multiple cores, and cores to

 * be part of multiple caches. There is a very clear and unique location

 * for each CPU in the hierarchy.

 *

 * Therefore computing a unique CPU for each group is trivial (the iteration

 * mask is redundant and set all 1s; all CPUs in a group will end up at _that_

 * group), we can simply pick the first CPU in each group.

 *

 *

 * [*] in other words, the first group of each domain is its child domain.

 Increase refcounts for claim_allocations: */

 sgc visits should follow a similar trend as sg */

 If we have already visited that group, it's already initialized. */

/*

 * build_sched_groups will build a circular linked list of the groups

 * covered by the given span, will set each group's ->cpumask correctly,

 * and will initialize their ->sgc.

 *

 * Assumes the sched_domain tree is fully constructed

/*

 * Initialize sched groups cpu_capacity.

 *

 * cpu_capacity indicates the capacity of sched group, which is used while

 * distributing the load between different sched groups in a sched domain.

 * Typically cpu_capacity for all the groups in a sched domain will be same

 * unless there are asymmetries in the topology. If there are asymmetries,

 * group having more cpu_capacity will pickup more load compared to the

 * group having less cpu_capacity.

/*

 * Asymmetric CPU capacity bits

/*

 * Set of available CPUs grouped by their corresponding capacities

 * Each list entry contains a CPU mask reflecting CPUs that share the same

 * capacity.

 * The lifespan of data is unlimited.

/*

 * Verify whether there is any CPU capacity asymmetry in a given sched domain.

 * Provides sd_flags reflecting the asymmetry scope.

	/*

	 * Count how many unique CPU capacities this domain spans across

	 * (compare sched_domain CPUs mask with ones representing  available

	 * CPUs capacities). Take into account CPUs that might be offline:

	 * skip those.

 No asymmetry detected */

 Some of the available CPU capacity values have not been detected */

 Full asymmetry */

/*

 * Build-up/update list of CPUs grouped by their capacities

 * An update requires explicit request to rebuild sched domains

 * with state indicating CPU topology changes.

	/*

	 * Only one capacity value has been detected i.e. this system is symmetric.

	 * No need to keep this data around.

/*

 * Initializers for schedule domains

 * Non-inlined to reduce accumulated stack pressure in build_sched_domains()

 Turn off idle balance on this domain: */

/*

 * NULL the sd_data elements we've used to build the sched_domain and

 * sched_group structure so that the subsequent __free_domain_allocs()

 * will not free the data we're using.

/*

 * SD_flags allowed in topology descriptions.

 *

 * These flags are purely descriptive of the topology and do not prescribe

 * behaviour. Behaviour is artificial and mapped in the below sd_init()

 * function:

 *

 *   SD_SHARE_CPUCAPACITY   - describes SMT topologies

 *   SD_SHARE_PKG_RESOURCES - describes shared caches

 *   SD_NUMA                - describes NUMA topologies

 *

 * Odd one out, which beside describing the topology has a quirk also

 * prescribes the desired behaviour that goes along with it:

 *

 *   SD_ASYM_PACKING        - describes SMT quirks

	/*

	 * Ugly hack to pass state to sd_numa_mask()...

	/*

	 * Convert topological properties into behaviour.

 Don't attempt to spread across CPUs of different capacities. */

	/*

	 * For all levels sharing cache; connect a sched_domain_shared

	 * instance.

/*

 * Topology list, bottom-up.

/*

 * A system can have three types of NUMA topology:

 * NUMA_DIRECT: all nodes are directly connected, or not a NUMA system

 * NUMA_GLUELESS_MESH: some nodes reachable through intermediary nodes

 * NUMA_BACKPLANE: nodes can reach other nodes through a backplane

 *

 * The difference between a glueless mesh topology and a backplane

 * topology lies in whether communication between not directly

 * connected nodes goes through intermediary nodes (where programs

 * could run), or through backplane controllers. This affects

 * placement of programs.

 *

 * The type of topology can be discerned with the following tests:

 * - If the maximum distance between any nodes is 1 hop, the system

 *   is directly connected.

 * - If for two nodes A and B, located N > 1 hops away from each other,

 *   there is an intermediary node C, which is < N hops away from both

 *   nodes A and B, the system is a glueless mesh.

 Find two nodes furthest removed from each other. */

 Is there an intermediary node between a and b? */

	/*

	 * O(nr_nodes^2) deduplicating selection sort -- in order to find the

	 * unique distances in the node_distance() table.

	/*

	 * We can now figure out how many unique distance values there are and

	 * allocate memory accordingly.

	/*

	 * 'nr_levels' contains the number of unique distances

	 *

	 * The sched_domains_numa_distance[] array includes the actual distance

	 * numbers.

	/*

	 * Here, we should temporarily reset sched_domains_numa_levels to 0.

	 * If it fails to allocate memory for array sched_domains_numa_masks[][],

	 * the array will contain less then 'nr_levels' members. This could be

	 * dangerous when we use it to iterate array sched_domains_numa_masks[][]

	 * in other functions.

	 *

	 * We reset it to 'nr_levels' at the end of this function.

	/*

	 * Now for each level, construct a mask per node which contains all

	 * CPUs of nodes that are that many hops away from us.

				/*

				 * Distance information can be unreliable for

				 * offline nodes, defer building the node

				 * masks to its bringup.

				 * This relies on all unique distance values

				 * still being visible at init time.

 Compute default topology size */

	/*

	 * Copy the default topology bits..

	/*

	 * Add the NUMA identity distance, aka single NODE.

	/*

	 * .. and append 'j' levels of NUMA goodness.

	/*

	 * NUMA masks are not built for offline nodes in sched_init_numa().

	 * Thus, when a CPU of a never-onlined-before node gets plugged in,

	 * adding that new CPU to the right NUMA masks is not sufficient: the

	 * masks of that CPU's node must also be updated.

 Add remote nodes in our masks */

	/*

	 * A new node has been brought up, potentially changing the topology

	 * classification.

	 *

	 * Note that this is racy vs any use of sched_numa_topology_type :/

 Set ourselves in the remote node's masks */

/*

 * sched_numa_find_closest() - given the NUMA topology, find the cpu

 *                             closest to @cpu from @cpumask.

 * cpumask: cpumask to find a cpu from

 * cpu: cpu to be close to

 *

 * returns: cpu, or nr_cpu_ids when nothing found.

 CONFIG_NUMA */

 Fixup, ensure @sd has at least @child CPUs. */

/*

 * Ensure topology masks are sane, i.e. there are no conflicts (overlaps) for

 * any two given CPUs at this (non-NUMA) topology level.

 NUMA levels are allowed to overlap */

	/*

	 * Non-NUMA levels cannot partially overlap - they must be either

	 * completely equal or completely disjoint. Otherwise we can end up

	 * breaking the sched_group lists - i.e. a later get_group() pass

	 * breaks the linking done for an earlier span.

		/*

		 * We should 'and' all those masks with 'cpu_map' to exactly

		 * match the topology we're about to build, but that can only

		 * remove CPUs, which only lessens our ability to detect

		 * overlaps

/*

 * Build sched domains for a given set of CPUs and attach the sched domains

 * to the individual CPUs

 Set up domains for CPUs specified by the cpu_map: */

 Build the groups for the domains */

 Calculate CPU capacity for physical packages and nodes */

 Attach the domains */

 Use READ_ONCE()/WRITE_ONCE() to avoid load/store tearing: */

 Current sched domains: */

 Number of sched domains in 'doms_cur': */

 Attributes of custom domains in 'doms_cur' */

/*

 * Special case: If a kmalloc() of a doms_cur partition (array of

 * cpumask) fails, then fallback to a single sched domain,

 * as determined by the single cpumask fallback_doms.

/*

 * arch_update_cpu_topology lets virtualized architectures update the

 * CPU core maps. It is supposed to return 1 if the topology changed

 * or 0 if it stayed the same.

/*

 * Set up scheduler domains and groups.  For now this just excludes isolated

 * CPUs, but could be used to exclude other special cases in the future.

/*

 * Detach sched domains from a group of CPUs specified in cpu_map

 * These CPUs will now be attached to the NULL domain

 handle null as "default" */

 Fast path: */

/*

 * Partition sched domains as specified by the 'ndoms_new'

 * cpumasks in the array doms_new[] of cpumasks. This compares

 * doms_new[] to the current sched domain partitioning, doms_cur[].

 * It destroys each deleted domain and builds each new domain.

 *

 * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.

 * The masks don't intersect (don't overlap.) We should setup one

 * sched domain for each mask. CPUs not in any of the cpumasks will

 * not be load balanced. If the same cpumask appears both in the

 * current 'doms_cur' domains and in the new 'doms_new', we can leave

 * it as it is.

 *

 * The passed in 'doms_new' should be allocated using

 * alloc_sched_domains.  This routine takes ownership of it and will

 * free_sched_domains it when done with it. If the caller failed the

 * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,

 * and partition_sched_domains() will fallback to the single partition

 * 'fallback_doms', it also forces the domains to be rebuilt.

 *

 * If doms_new == NULL it will be replaced with cpu_online_mask.

 * ndoms_new == 0 is a special case for destroying existing domains,

 * and it will not create the default domain.

 *

 * Call with hotplug lock and sched_domains_mutex held

 Let the architecture update CPU core mappings: */

 Trigger rebuilding CPU capacity asymmetry data */

 Destroy deleted domains: */

				/*

				 * This domain won't be destroyed and as such

				 * its dl_bw->total_bw needs to be cleared.  It

				 * will be recomputed in function

				 * update_tasks_root_domain().

 No match - a current sched domain not in new doms_new[] */

 Build new domains: */

 No match - add a new doms_new */

 Build perf. domains: */

 No match - add perf. domains for a new rd */

 Remember the new sched domains: */

/*

 * Call with hotplug lock held

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2010-2017 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>

 *

 * membarrier system call

/*

 * For documentation purposes, here are some membarrier ordering

 * scenarios to keep in mind:

 *

 * A) Userspace thread execution after IPI vs membarrier's memory

 *    barrier before sending the IPI

 *

 * Userspace variables:

 *

 * int x = 0, y = 0;

 *

 * The memory barrier at the start of membarrier() on CPU0 is necessary in

 * order to enforce the guarantee that any writes occurring on CPU0 before

 * the membarrier() is executed will be visible to any code executing on

 * CPU1 after the IPI-induced memory barrier:

 *

 *         CPU0                              CPU1

 *

 *         x = 1

 *         membarrier():

 *           a: smp_mb()

 *           b: send IPI                       IPI-induced mb

 *           c: smp_mb()

 *         r2 = y

 *                                           y = 1

 *                                           barrier()

 *                                           r1 = x

 *

 *                     BUG_ON(r1 == 0 && r2 == 0)

 *

 * The write to y and load from x by CPU1 are unordered by the hardware,

 * so it's possible to have "r1 = x" reordered before "y = 1" at any

 * point after (b).  If the memory barrier at (a) is omitted, then "x = 1"

 * can be reordered after (a) (although not after (c)), so we get r1 == 0

 * and r2 == 0.  This violates the guarantee that membarrier() is

 * supposed by provide.

 *

 * The timing of the memory barrier at (a) has to ensure that it executes

 * before the IPI-induced memory barrier on CPU1.

 *

 * B) Userspace thread execution before IPI vs membarrier's memory

 *    barrier after completing the IPI

 *

 * Userspace variables:

 *

 * int x = 0, y = 0;

 *

 * The memory barrier at the end of membarrier() on CPU0 is necessary in

 * order to enforce the guarantee that any writes occurring on CPU1 before

 * the membarrier() is executed will be visible to any code executing on

 * CPU0 after the membarrier():

 *

 *         CPU0                              CPU1

 *

 *                                           x = 1

 *                                           barrier()

 *                                           y = 1

 *         r2 = y

 *         membarrier():

 *           a: smp_mb()

 *           b: send IPI                       IPI-induced mb

 *           c: smp_mb()

 *         r1 = x

 *         BUG_ON(r1 == 0 && r2 == 1)

 *

 * The writes to x and y are unordered by the hardware, so it's possible to

 * have "r2 = 1" even though the write to x doesn't execute until (b).  If

 * the memory barrier at (c) is omitted then "r1 = x" can be reordered

 * before (b) (although not before (a)), so we get "r1 = 0".  This violates

 * the guarantee that membarrier() is supposed to provide.

 *

 * The timing of the memory barrier at (c) has to ensure that it executes

 * after the IPI-induced memory barrier on CPU1.

 *

 * C) Scheduling userspace thread -> kthread -> userspace thread vs membarrier

 *

 *           CPU0                            CPU1

 *

 *           membarrier():

 *           a: smp_mb()

 *                                           d: switch to kthread (includes mb)

 *           b: read rq->curr->mm == NULL

 *                                           e: switch to user (includes mb)

 *           c: smp_mb()

 *

 * Using the scenario from (A), we can show that (a) needs to be paired

 * with (e). Using the scenario from (B), we can show that (c) needs to

 * be paired with (d).

 *

 * D) exit_mm vs membarrier

 *

 * Two thread groups are created, A and B.  Thread group B is created by

 * issuing clone from group A with flag CLONE_VM set, but not CLONE_THREAD.

 * Let's assume we have a single thread within each thread group (Thread A

 * and Thread B).  Thread A runs on CPU0, Thread B runs on CPU1.

 *

 *           CPU0                            CPU1

 *

 *           membarrier():

 *             a: smp_mb()

 *                                           exit_mm():

 *                                             d: smp_mb()

 *                                             e: current->mm = NULL

 *             b: read rq->curr->mm == NULL

 *             c: smp_mb()

 *

 * Using scenario (B), we can show that (c) needs to be paired with (d).

 *

 * E) kthread_{use,unuse}_mm vs membarrier

 *

 *           CPU0                            CPU1

 *

 *           membarrier():

 *           a: smp_mb()

 *                                           kthread_unuse_mm()

 *                                             d: smp_mb()

 *                                             e: current->mm = NULL

 *           b: read rq->curr->mm == NULL

 *                                           kthread_use_mm()

 *                                             f: current->mm = mm

 *                                             g: smp_mb()

 *           c: smp_mb()

 *

 * Using the scenario from (A), we can show that (a) needs to be paired

 * with (g). Using the scenario from (B), we can show that (c) needs to

 * be paired with (d).

/*

 * Bitmask made from a "or" of all commands within enum membarrier_cmd,

 * except MEMBARRIER_CMD_QUERY.

 IPIs should be serializing but paranoid. */

	/*

	 * The smp_mb() in membarrier after all the IPIs is supposed to

	 * ensure that memory on remote CPUs that occur before the IPI

	 * become visible to membarrier()'s caller -- see scenario B in

	 * the big comment at the top of this file.

	 *

	 * A sync_core() would provide this guarantee, but

	 * sync_core_before_usermode() might end up being deferred until

	 * after membarrier()'s smp_mb().

 IPIs should be serializing but paranoid. */

	/*

	 * Ensure that all stores done by the calling thread are visible

	 * to the current task before the current task resumes.  We could

	 * probably optimize this away on most architectures, but by the

	 * time we've already sent an IPI, the cost of the extra smp_mb()

	 * is negligible.

	/*

	 * Issue a memory barrier after setting

	 * MEMBARRIER_STATE_GLOBAL_EXPEDITED in the current runqueue to

	 * guarantee that no memory access following registration is reordered

	 * before registration.

	/*

	 * Issue a memory barrier before clearing membarrier_state to

	 * guarantee that no memory access prior to exec is reordered after

	 * clearing this state.

	/*

	 * Keep the runqueue membarrier_state in sync with this mm

	 * membarrier_state.

	/*

	 * Matches memory barriers around rq->curr modification in

	 * scheduler.

 system call entry is not a mb. */

		/*

		 * Skipping the current CPU is OK even through we can be

		 * migrated at any point. The current CPU, at the point

		 * where we read raw_smp_processor_id(), is ensured to

		 * be in program order with respect to the caller

		 * thread. Therefore, we can skip this CPU from the

		 * iteration.

		/*

		 * Skip the CPU if it runs a kernel thread which is not using

		 * a task mm.

	/*

	 * Memory barrier on the caller thread _after_ we finished

	 * waiting for the last IPI. Matches memory barriers around

	 * rq->curr modification in scheduler.

 exit from system call is not a mb */

	/*

	 * Matches memory barriers around rq->curr modification in

	 * scheduler.

 system call entry is not a mb. */

		/*

		 * smp_call_function_single() will call ipi_func() if cpu_id

		 * is the calling CPU.

		/*

		 * For regular membarrier, we can save a few cycles by

		 * skipping the current cpu -- we're about to do smp_mb()

		 * below, and if we migrate to a different cpu, this cpu

		 * and the new cpu will execute a full barrier in the

		 * scheduler.

		 *

		 * For SYNC_CORE, we do need a barrier on the current cpu --

		 * otherwise, if we are migrated and replaced by a different

		 * task in the same mm just before, during, or after

		 * membarrier, we will end up with some thread in the mm

		 * running without a core sync.

		 *

		 * For RSEQ, don't rseq_preempt() the caller.  User code

		 * is not supposed to issue syscalls at all from inside an

		 * rseq critical section.

	/*

	 * Memory barrier on the caller thread _after_ we finished

	 * waiting for the last IPI. Matches memory barriers around

	 * rq->curr modification in scheduler.

 exit from system call is not a mb */

		/*

		 * For single mm user, we can simply issue a memory barrier

		 * after setting MEMBARRIER_STATE_GLOBAL_EXPEDITED in the

		 * mm and in the current runqueue to guarantee that no memory

		 * access following registration is reordered before

		 * registration.

	/*

	 * For mm with multiple users, we need to ensure all future

	 * scheduler executions will observe @mm's new membarrier

	 * state.

	/*

	 * For each cpu runqueue, if the task's mm match @mm, ensure that all

	 * @mm's membarrier state set bits are also set in the runqueue's

	 * membarrier state. This ensures that a runqueue scheduling

	 * between threads which are users of @mm has its membarrier state

	 * updated.

	/*

	 * We need to consider threads belonging to different thread

	 * groups, which use the same mm. (CLONE_VM but not

	 * CLONE_THREAD).

/**

 * sys_membarrier - issue memory barriers on a set of threads

 * @cmd:    Takes command values defined in enum membarrier_cmd.

 * @flags:  Currently needs to be 0 for all commands other than

 *          MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ: in the latter

 *          case it can be MEMBARRIER_CMD_FLAG_CPU, indicating that @cpu_id

 *          contains the CPU on which to interrupt (= restart)

 *          the RSEQ critical section.

 * @cpu_id: if @flags == MEMBARRIER_CMD_FLAG_CPU, indicates the cpu on which

 *          RSEQ CS should be interrupted (@cmd must be

 *          MEMBARRIER_CMD_PRIVATE_EXPEDITED_RSEQ).

 *

 * If this system call is not implemented, -ENOSYS is returned. If the

 * command specified does not exist, not available on the running

 * kernel, or if the command argument is invalid, this system call

 * returns -EINVAL. For a given command, with flags argument set to 0,

 * if this system call returns -ENOSYS or -EINVAL, it is guaranteed to

 * always return the same value until reboot. In addition, it can return

 * -ENOMEM if there is not enough memory available to perform the system

 * call.

 *

 * All memory accesses performed in program order from each targeted thread

 * is guaranteed to be ordered with respect to sys_membarrier(). If we use

 * the semantic "barrier()" to represent a compiler barrier forcing memory

 * accesses to be performed in program order across the barrier, and

 * smp_mb() to represent explicit memory barriers forcing full memory

 * ordering across the barrier, we have the following ordering table for

 * each pair of barrier(), sys_membarrier() and smp_mb():

 *

 * The pair ordering is detailed as (O: ordered, X: not ordered):

 *

 *                        barrier()   smp_mb() sys_membarrier()

 *        barrier()          X           X            O

 *        smp_mb()           X           O            O

 *        sys_membarrier()   O           O            O

 MEMBARRIER_CMD_GLOBAL is not compatible with nohz_full. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR

 * policies)

 More than 4 hours if BW_SHIFT equals 20. */

		/*

		 * SCHED_DEADLINE updates the bandwidth, as a run away

		 * RT task with a DL task could hog a CPU. But DL does

		 * not reset the period. If a deadline task was running

		 * without an RT task running, it can cause RT tasks to

		 * throttle when they start up. Kick the timer right away

		 * to update the period.

 delimiter for bitsearch: */

 CONFIG_SMP */

 We start is dequeued state, because no RT tasks are queued */

 CONFIG_RT_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

 Try to pull RT tasks here if we lower this rq's prio */

	/*

	 * Make sure the mask is visible before we set

	 * the overload count. That is checked to determine

	 * if we should look at the mask. It would be a shame

	 * if we looked at the mask, but the mask was not

	 * updated yet.

	 *

	 * Matched by the barrier in pull_rt_task().

 the order here really doesn't matter */

 Update the highest prio pushable task */

 Update the new highest prio pushable task */

 CONFIG_SMP */

/*

 * Verify the fitness of task @p to run on @cpu taking into account the uclamp

 * settings.

 *

 * This check is only important for heterogeneous systems where uclamp_min value

 * is higher than the capacity of a @cpu. For non-heterogeneous system this

 * function will always return true.

 *

 * The function will return true if the capacity of the @cpu is >= the

 * uclamp_min and false otherwise.

 *

 * Note that uclamp_min will be clamped to uclamp_max if uclamp_min

 * > uclamp_max.

 Only heterogeneous systems can benefit from this check */

 Kick cpufreq (see the comment in kernel/sched/sched.h). */

 !CONFIG_RT_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

/*

 * We ran out of runtime, see if we can borrow some from our neighbours.

		/*

		 * Either all rqs have inf runtime and there's nothing to steal

		 * or __disable_runtime() below sets a specific rq to inf to

		 * indicate its been disabled and disallow stealing.

		/*

		 * From runqueues with spare time, take 1/n part of their

		 * spare time, but no more than our period.

/*

 * Ensure this RQ takes back all the runtime it lend to its neighbours.

		/*

		 * Either we're all inf and nobody needs to borrow, or we're

		 * already disabled and thus have nothing to do, or we have

		 * exactly the right amount of runtime to take out.

		/*

		 * Calculate the difference between what we started out with

		 * and what we current have, that's the amount of runtime

		 * we lend and now have to reclaim.

		/*

		 * Greedy reclaim, take back as much as we can.

			/*

			 * Can't reclaim from ourselves or disabled runqueues.

		/*

		 * We cannot be left wanting - that would mean some runtime

		 * leaked out of the system.

		/*

		 * Disable all the borrow logic by pretending we have inf

		 * runtime - in which case borrowing doesn't make sense.

 Make rt_rq available for pick_next_task() */

	/*

	 * Reset each runqueue's bandwidth settings

 !CONFIG_SMP */

 CONFIG_SMP */

	/*

	 * FIXME: isolated CPUs should really leave the root task group,

	 * whether they are isolcpus or were isolated via cpusets, lest

	 * the timer run on a CPU which does not service all runqueues,

	 * potentially leaving other CPUs indefinitely throttled.  If

	 * isolation is really required, the user will turn the throttle

	 * off to kill the perturbations it causes anyway.  Meanwhile,

	 * this maintains functionality for boot and/or troubleshooting.

		/*

		 * When span == cpu_online_mask, taking each rq->lock

		 * can be time-consuming. Try to avoid it when possible.

				/*

				 * When we're idle and a woken (rt) task is

				 * throttled check_preempt_curr() will set

				 * skip_update and the time between the wakeup

				 * and this unthrottle will get accounted as

				 * 'runtime'.

		/*

		 * Don't actually throttle groups that have no runtime assigned

		 * but accrue some time due to boosting.

			/*

			 * In case we did anyway, make it go away,

			 * replenishment is a joke, since it will replenish us

			 * with exactly 0 ns.

/*

 * Update the current task's runtime statistics. Skip current tasks that

 * are not in our scheduling class.

 Kick cpufreq (see the comment in kernel/sched/sched.h). */

	/*

	 * Change rq's cpupri only if rt_rq is the top queue.

	/*

	 * Change rq's cpupri only if rt_rq is the top queue.

 CONFIG_SMP */

 CONFIG_SMP */

		/*

		 * This may have been our highest task, and therefore

		 * we may have some recomputation to do

 CONFIG_SMP || CONFIG_RT_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

/*

 * Change rt_se->run_list location unless SAVE && !MOVE

 *

 * assumes ENQUEUE/DEQUEUE flags match

 schedstats is not supported for rt group. */

	/*

	 * Don't enqueue the group if its throttled, or when empty.

	 * The latter is a consequence of the former when a child group

	 * get throttled and the current group doesn't have any other

	 * active members.

/*

 * Because the prio of an upper entry depends on the lower

 * entries, we must remove entries top - down.

/*

 * Adding/removing a task to/from a priority array:

/*

 * Put task to the head or the end of the run list without the overhead of

 * dequeue followed by enqueue.

 For anything but wake ups, just return the task_cpu */

 unlocked access */

	/*

	 * If the current task on @p's runqueue is an RT task, then

	 * try to see if we can wake this RT task up on another

	 * runqueue. Otherwise simply start this RT task

	 * on its current runqueue.

	 *

	 * We want to avoid overloading runqueues. If the woken

	 * task is a higher priority, then it will stay on this CPU

	 * and the lower prio task should be moved to another CPU.

	 * Even though this will probably make the lower prio task

	 * lose its cache, we do not want to bounce a higher task

	 * around just because it gave up its CPU, perhaps for a

	 * lock?

	 *

	 * For equal prio tasks, we just let the scheduler sort it out.

	 *

	 * Otherwise, just let it ride on the affined RQ and the

	 * post-schedule router will push the preempted task away

	 *

	 * This test is optimistic, if we get it wrong the load-balancer

	 * will have to sort it out.

	 *

	 * We take into account the capacity of the CPU to ensure it fits the

	 * requirement of the task - which is only important on heterogeneous

	 * systems like big.LITTLE.

		/*

		 * Bail out if we were forcing a migration to find a better

		 * fitting CPU but our search failed.

		/*

		 * Don't bother moving it if the destination CPU is

		 * not running a lower priority task.

	/*

	 * Current can't be migrated, useless to reschedule,

	 * let's hope p can move out.

	/*

	 * p is migratable, so let's not schedule it and

	 * see if it is pushed or pulled somewhere else.

	/*

	 * There appear to be other CPUs that can accept

	 * the current task but none can run 'p', so lets reschedule

	 * to try and push the current task away:

		/*

		 * This is OK, because current is on_cpu, which avoids it being

		 * picked for load-balance and preemption/IRQs are still

		 * disabled avoiding further scheduler activity on it and we've

		 * not yet started the picking loop.

 CONFIG_SMP */

/*

 * Preempt the current task with a newly woken task if needed:

	/*

	 * If:

	 *

	 * - the newly woken task is of equal priority to the current task

	 * - the newly woken task is non-migratable while current is migratable

	 * - current will be preempted on the next reschedule

	 *

	 * we should check to see if current can readily move to a different

	 * cpu.  If so, we will reschedule to allow the push logic to try

	 * to move current somewhere else, making room for our non-migratable

	 * task.

 The running task is never eligible for pushing */

	/*

	 * If prev task was rt, put_prev_task() has already updated the

	 * utilization. We only care of the case where we start to schedule a

	 * rt task

	/*

	 * The previous task needs to be made eligible for pushing

	 * if it is still active

 Only try algorithms three times */

/*

 * Return the highest pushable rq's task, which is suitable to be executed

 * on the CPU, NULL otherwise

 Make sure the mask is initialized first */

 No other targets possible */

	/*

	 * If we're on asym system ensure we consider the different capacities

	 * of the CPUs when searching for the lowest_mask.

 No targets found */

	/*

	 * At this point we have built a mask of CPUs representing the

	 * lowest priority tasks in the system.  Now we want to elect

	 * the best one based on our affinity and topology.

	 *

	 * We prioritize the last CPU that the task executed on since

	 * it is most likely cache-hot in that location.

	/*

	 * Otherwise, we consult the sched_domains span maps to figure

	 * out which CPU is logically closest to our hot cache data.

 Skip this_cpu opt if not among lowest */

			/*

			 * "this_cpu" is cheaper to preempt than a

			 * remote processor.

	/*

	 * And finally, if there were no matches within the domains

	 * just give the caller *something* to work with from the compatible

	 * locations.

 Will lock the rq it finds */

			/*

			 * Target rq has tasks of equal or higher priority,

			 * retrying does not release any lock and is unlikely

			 * to yield a different result.

 if the prio of this runqueue changed, try again */

			/*

			 * We had to unlock the run queue. In

			 * the mean time, task could have

			 * migrated already or had its affinity changed.

			 * Also make sure that it wasn't scheduled on its rq.

 If this rq is still suitable use it. */

 try again */

/*

 * If the current CPU has more than one RT task, see if the non

 * running task can migrate over to a CPU that is running a task

 * of lesser priority.

		/*

		 * Given we found a CPU with lower priority than @next_task,

		 * therefore it should be running. However we cannot migrate it

		 * to this other CPU, instead attempt to push the current

		 * running task on this CPU away.

	/*

	 * It's possible that the next_task slipped in of

	 * higher priority than current. If that's the case

	 * just reschedule current.

 We might release rq lock */

 find_lock_lowest_rq locks the rq if found */

		/*

		 * find_lock_lowest_rq releases rq->lock

		 * so it is possible that next_task has migrated.

		 *

		 * We need to make sure that the task is still on the same

		 * run-queue and is also still the next task eligible for

		 * pushing.

			/*

			 * The task hasn't migrated, and is still the next

			 * eligible task, but we failed to find a run-queue

			 * to push it to.  Do not retry in this case, since

			 * other CPUs will pull from us when ready.

 No more tasks, just exit */

		/*

		 * Something has shifted, try again.

 push_rt_task will return true if it moved an RT */

/*

 * When a high priority task schedules out from a CPU and a lower priority

 * task is scheduled in, a check is made to see if there's any RT tasks

 * on other CPUs that are waiting to run because a higher priority RT task

 * is currently running on its CPU. In this case, the CPU with multiple RT

 * tasks queued on it (overloaded) needs to be notified that a CPU has opened

 * up that may be able to run one of its non-running queued RT tasks.

 *

 * All CPUs with overloaded RT tasks need to be notified as there is currently

 * no way to know which of these CPUs have the highest priority task waiting

 * to run. Instead of trying to take a spinlock on each of these CPUs,

 * which has shown to cause large latency when done on machines with many

 * CPUs, sending an IPI to the CPUs to have them push off the overloaded

 * RT tasks waiting to run.

 *

 * Just sending an IPI to each of the CPUs is also an issue, as on large

 * count CPU machines, this can cause an IPI storm on a CPU, especially

 * if its the only CPU with multiple RT tasks queued, and a large number

 * of CPUs scheduling a lower priority task at the same time.

 *

 * Each root domain has its own irq work function that can iterate over

 * all CPUs with RT overloaded tasks. Since all CPUs with overloaded RT

 * task must be checked if there's one or many CPUs that are lowering

 * their priority, there's a single irq work iterator that will try to

 * push off RT tasks that are waiting to run.

 *

 * When a CPU schedules a lower priority task, it will kick off the

 * irq work iterator that will jump to each CPU with overloaded RT tasks.

 * As it only takes the first CPU that schedules a lower priority task

 * to start the process, the rto_start variable is incremented and if

 * the atomic result is one, then that CPU will try to take the rto_lock.

 * This prevents high contention on the lock as the process handles all

 * CPUs scheduling lower priority tasks.

 *

 * All CPUs that are scheduling a lower priority task will increment the

 * rt_loop_next variable. This will make sure that the irq work iterator

 * checks all RT overloaded CPUs whenever a CPU schedules a new lower

 * priority task, even if the iterator is in the middle of a scan. Incrementing

 * the rt_loop_next will cause the iterator to perform another scan.

 *

	/*

	 * When starting the IPI RT pushing, the rto_cpu is set to -1,

	 * rt_next_cpu() will simply return the first CPU found in

	 * the rto_mask.

	 *

	 * If rto_next_cpu() is called with rto_cpu is a valid CPU, it

	 * will return the next CPU found in the rto_mask.

	 *

	 * If there are no more CPUs left in the rto_mask, then a check is made

	 * against rto_loop and rto_loop_next. rto_loop is only updated with

	 * the rto_lock held, but any CPU may increment the rto_loop_next

	 * without any locking.

 When rto_cpu is -1 this acts like cpumask_first() */

		/*

		 * ACQUIRE ensures we see the @rto_mask changes

		 * made prior to the @next value observed.

		 *

		 * Matches WMB in rt_set_overload().

 Keep the loop going if the IPI is currently active */

 Only one CPU can initiate a loop at a time */

	/*

	 * The rto_cpu is updated under the lock, if it has a valid CPU

	 * then the IPI is still running and will continue due to the

	 * update to loop_next, and nothing needs to be done here.

	 * Otherwise it is finishing up and an ipi needs to be sent.

 Make sure the rd does not get freed while pushing */

 Called from hardirq context */

	/*

	 * We do not need to grab the lock to check for has_pushable_tasks.

	 * When it gets updated, a check is made if a push is possible.

 Pass the IPI to the next rt overloaded queue */

 Try the next RT overloaded CPU */

 HAVE_RT_PUSH_IPI */

	/*

	 * Match the barrier from rt_set_overloaded; this guarantees that if we

	 * see overloaded we must also see the rto_mask bit.

 If we are the only overloaded CPU do nothing */

		/*

		 * Don't bother taking the src_rq->lock if the next highest

		 * task is known to be lower-priority than our current task.

		 * This may look racy, but if this value is about to go

		 * logically higher, the src_rq will push this task away.

		 * And if its going logically lower, we do not care

		/*

		 * We can potentially drop this_rq's lock in

		 * double_lock_balance, and another CPU could

		 * alter this_rq

		/*

		 * We can pull only a task, which is pushable

		 * on its rq, and no others.

		/*

		 * Do we have an RT task that preempts

		 * the to-be-scheduled task?

			/*

			 * There's a chance that p is higher in priority

			 * than what's currently running on its CPU.

			 * This is just that p is waking up and hasn't

			 * had a chance to schedule. We only pull

			 * p if it is lower in priority than the

			 * current task on the run queue

			/*

			 * We continue with the search, just in

			 * case there's an even higher prio task

			 * in another runqueue. (low likelihood

			 * but possible)

/*

 * If we are not running and we are not going to reschedule soon, we should

 * try to push tasks away now

 Assumes rq->lock is held */

 Assumes rq->lock is held */

/*

 * When switch from the rt queue, we bring ourselves to a position

 * that we might want to pull RT tasks from other runqueues.

	/*

	 * If there are other RT tasks then we will reschedule

	 * and the scheduling of the other RT tasks will handle

	 * the balancing. But if we are the last RT task

	 * we may need to handle the pulling of RT tasks

	 * now.

 CONFIG_SMP */

/*

 * When switching a task to RT, we may overload the runqueue

 * with RT tasks. In this case we try to push them off to

 * other runqueues.

	/*

	 * If we are running, update the avg_rt tracking, as the running time

	 * will now on be accounted into the latter.

	/*

	 * If we are not running we may need to preempt the current

	 * running task. If that current running task is also an RT task

	 * then see if we can move to another run queue.

 CONFIG_SMP */

/*

 * Priority of the task has changed. This may cause

 * us to initiate a push or pull.

		/*

		 * If our priority decreases while running, we

		 * may need to pull tasks to this runqueue.

		/*

		 * If there's a higher priority task waiting to run

		 * then reschedule.

 For UP simply resched on drop of prio */

 CONFIG_SMP */

		/*

		 * This task is not running, but if it is

		 * greater than the current running task

		 * then reschedule.

 max may change after cur was read, this will be fixed next tick */

/*

 * scheduler tick hitting a task of our scheduling class.

 *

 * NOTE: This function can be called remotely by the tick offload that

 * goes along full dynticks. Therefore no local assumption can be made

 * and everything must be accessed through the @rq and @curr passed in

 * parameters.

	/*

	 * RR tasks need a special form of timeslice management.

	 * FIFO tasks have no timeslices.

	/*

	 * Requeue to the end of queue if we (and all of our ancestors) are not

	 * the only element on the queue

	/*

	 * Time slice is 0 for SCHED_FIFO tasks

/*

 * Ensure that the real time constraints are schedulable.

	/*

	 * Autogroups do not have RT tasks; see autogroup_create().

	/*

	 * Cannot have more runtime than the period.

	/*

	 * Ensure we don't starve existing RT tasks if runtime turns zero.

	/*

	 * Nobody can have more than the global setting allows.

	/*

	 * The sum of our children's runtime should not exceed our own.

	/*

	 * Disallowing the root group RT runtime is BAD, it would disallow the

	 * kernel creating (and or operating) RT threads.

 No period doesn't make any sense. */

	/*

	 * Bound quota to defend quota against overflow during bandwidth shift.

 Don't accept realtime tasks when there is no way for them to run */

 !CONFIG_RT_GROUP_SCHED */

 CONFIG_RT_GROUP_SCHED */

	/*

	 * Make sure that internally we keep jiffies.

	 * Also, writing zero resets the timeslice to default:

 CONFIG_SCHED_DEBUG */

