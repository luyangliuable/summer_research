 SPDX-License-Identifier: GPL-2.0

 Direct 8 bit.   */

 Direct 12 bit.  */

 Direct 16 bit.  */

 Direct 20 bit.  */

 DL */

 DH */

 Direct 32 bit.  */

 Direct 64 bit.  */

 PC relative 16 bit.	*/

 PC relative 16 bit shifted by 1.  */

 PC relative 32 bit shifted by 1.  */

 PC relative 32 bit.	*/

 PC relative 64 bit.	*/

 SPDX-License-Identifier: GPL-2.0

/*

 *    Copyright IBM Corp. 2007, 2009

 *    Author(s): Hongjie Yang <hongjie@us.ibm.com>,

 *		 Heiko Carstens <heiko.carstens@de.ibm.com>

 TOD clock not running. Set the clock to Unix Epoch. */

/*

 * Initialize storage key for kernel pages

 Check current-configuration-level */

 Get virtual-machine cpu information. */

 Detect known hypervisors */

 Remove leading, trailing and double whitespace. */

 Disable low address protection before storing into lowcore. */

 Enabled signed clock comparator comparisons */

 the control bit is set during PCI initialization */

 copy arch command line */

 SPDX-License-Identifier: GPL-2.0+

/*

 *  Kernel Probes (KProbes)

 *

 * Copyright IBM Corp. 2002, 2006

 *

 * s390 port, used ppc64 as template. Mike Grundy <grundym@us.ibm.com>

		/*

		 * For pc-relative instructions in RIL-b or RIL-c format patch

		 * the RI2 displacement field. We have already made sure that

		 * the insn slot for the patched instruction is within the same

		 * 2GB area as the original instruction (either kernel image or

		 * module area). Therefore the new displacement will always fit.

	/*

	 * Get an insn slot that is within the same 2GB area like the original

	 * instruction. That way instructions with a 32bit signed displacement

	 * field can be patched and executed within the insn slot.

 Check if paddr is at an instruction boundary */

 Decode instructions */

				/*

				 * Note that QEMU inserts opcode 0x0000 to implement

				 * software breakpoints for guests. Since the size of

				 * the original instruction is unknown, stop following

				 * instructions and prevent setting a kprobe.

			/*

			 * Check if the instruction has been modified by another

			 * kprobe, in which case the original instruction is

			 * decoded.

 not a kprobe */

 Make sure the probe isn't going on a difficult instruction */

 Set up the PER control registers %cr9-%cr11 */

 Save control regs and psw mask */

 Set PER control regs, turns on single step for the given address */

 Restore control regs and psw mask, set new psw address */

/*

 * Activate a kprobe by storing its pointer to current_kprobe. The

 * previous kprobe is stored in kcb->prev_kprobe. A stack of up to

 * two kprobes can be active, see KPROBE_REENTER.

/*

 * Deactivate a kprobe by backing up to the previous state. If the

 * current state is KPROBE_REENTER prev_kprobe.kp will be non-NULL,

 * for any other state prev_kprobe.kp will be NULL.

 Replace the return addr with trampoline addr */

		/*

		 * A kprobe on the code path to single step an instruction

		 * is a BUG. The code path resides in the .kprobes.text

		 * section and is executed with interrupts disabled.

	/*

	 * We want to disable preemption for the entire duration of kprobe

	 * processing. That includes the calls to the pre/post handlers

	 * and single stepping the kprobe instruction.

			/*

			 * We have hit a kprobe while another is still

			 * active. This can happen in the pre and post

			 * handler. Single step the instruction of the

			 * new probe but do not call any handler function

			 * of this secondary kprobe.

			 * push_kprobe and pop_kprobe saves and restores

			 * the currently active kprobe.

			/*

			 * If we have no pre-handler or it returned 0, we

			 * continue with single stepping. If we have a

			 * pre-handler and it returned non-zero, it prepped

			 * for changing execution path, so get out doing

			 * nothing more here.

	} /* else:

	   * No kprobe at this address and no active kprobe. The trap has

	   * not been caused by a kprobe breakpoint. The race of breakpoint

	   * vs. kprobe remove does not exist because on s390 as we use

	   * stop_machine to arm/disarm the breakpoints.

/*

 * Function return probe trampoline:

 *	- init_kprobes() establishes a probepoint here

 *	- When the probed function returns, this probe

 *		causes the handlers to fire

/*

 * Called when the probe at kretprobe trampoline is hit

	/*

	 * By returning a non-zero value, we are telling

	 * kprobe_handler() that we don't want the post_handler

	 * to run (and have re-enabled preemption)

/*

 * Called after single-stepping.  p->addr is the address of the

 * instruction whose first byte has been replaced by the "breakpoint"

 * instruction.  To avoid the SMP problems that can occur when we

 * temporarily put back the original opcode to single-step, we

 * single-stepped a copy of the instruction.  The address of this

 * copy is p->ainsn.insn.

	/*

	 * if somebody else is singlestepping across a probe point, psw mask

	 * will have PER set, in which case, continue the remaining processing

	 * of do_single_step, as if this is not a probe hit.

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe and the nip points back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * In case the user-specified fault handler returned

		 * zero, try to fix up.

		/*

		 * fixup_exception() could not handle it,

		 * Let do_page_fault() fix it.

/*

 * Wrapper routine to for handling exceptions.

 SPDX-License-Identifier: GPL-2.0

/*

 *  S390 version

 *    Copyright IBM Corp. 1999, 2007

 *    Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com),

 *               Christian Borntraeger (cborntra@de.ibm.com),

/*

 * __cpcmd has some restrictions over cpcmd

 *  - __cpcmd is unlocked and therefore not SMP-safe

 SPDX-License-Identifier: GPL-2.0

/*

 * S390 kdump implementation

 *

 * Copyright IBM Corp. 2011

 * Author(s): Michael Holzheu <holzheu@linux.vnet.ibm.com>

/*

 * Allocate a save area

/*

 * Return the address of the save area for the boot CPU

/*

 * Copy CPU registers into the save area

/*

 * Copy vector registers into the save area

 Copy lower halves of vector registers 0-15 */

 Copy vector registers 16-31 */

/*

 * Return physical address for virtual address

/*

 * Copy memory of the old, dumped system to a kernel space virtual address

 Copy from zfcp/nvme dump HSA area */

 Check for swapped kdump oldmem areas */

/*

 * Copy memory of the old, dumped system to a user space virtual address

 Copy from zfcp/nvme dump HSA area */

 Check for swapped kdump oldmem areas */

/*

 * Copy one page from "oldmem"

/*

 * Remap "oldmem" for kdump

 *

 * For the kdump reserved memory this functions performs a swap operation:

 * [0 - OLDMEM_SIZE] is mapped to [OLDMEM_BASE - OLDMEM_BASE + OLDMEM_SIZE]

/*

 * Remap "oldmem" for zfcp/nvme dump

 *

 * We only map available memory above HSA size. Memory below HSA size

 * is read on demand using the copy_oldmem_page() function.

/*

 * Remap "oldmem" for kdump or zfcp/nvme dump

/*

 * Initialize ELF note

/*

 * Calculate the size of ELF note

/*

 * Fill ELF notes for one CPU with save area registers

 Prepare prstatus note */

 Prepare fpregset (floating point) note */

 Create ELF notes for the CPU */

/*

 * Calculate size of ELF notes per cpu

/*

 * Initialize prpsinfo note (new kernel)

/*

 * Get vmcoreinfo using lowcore->vmcore_info (new kernel)

/*

 * Initialize vmcoreinfo note (new kernel)

/*

 * Initialize final note (needed for /proc/vmcore code)

/*

 * Initialize ELF header (new kernel)

/*

 * Return CPU count for ELF header (new kernel)

/*

 * Return memory chunk count for ELF header (new kernel)

/*

 * Initialize ELF loads (new kernel)

/*

 * Initialize notes (new kernel)

 PT_NOTES */

 nt_prpsinfo */

 regsets */

 nt_vmcoreinfo */

 nt_final */

 PT_LOADS */

/*

 * Create ELF core header (new kernel)

 If we are not in kdump or zfcp/nvme dump mode return */

 If we cannot get HSA size for zfcp/nvme dump return error */

 For kdump, exclude previous crashkernel memory */

	/* Without elfcorehdr /proc/vmcore cannot be created. Thus creating

	 * a dump with this crash kernel will fail. Panic now to allow other

	 * dump mechanisms to take over.

 Init elf header */

 Init program headers */

 Init notes */

 Init loads */

/*

 * Free ELF core header (new kernel)

/*

 * Read from ELF header

/*

 * Read from ELF notes data

 SPDX-License-Identifier: GPL-2.0+

/*

 *  Kernel module help for s390.

 *

 *  S390 version

 *    Copyright IBM Corp. 2002, 2003

 *    Author(s): Arnd Bergmann (arndb@de.ibm.com)

 *		 Martin Schwidefsky (schwidefsky@de.ibm.com)

 *

 *  based on i386 version

 *    Copyright (C) 2001 Rusty Russell.

 12 bit GOT offset.  */

 16 bit GOT offset.  */

 20 bit GOT offset.  */

 32 bit GOT offset.  */

 64 bit GOT offset.  */

 32 bit PC rel. to GOT entry shifted by 1. */

 12 bit offset to jump slot.	*/

 16 bit offset to jump slot.  */

 20 bit offset to jump slot.  */

 32 bit offset to jump slot.  */

 64 bit offset to jump slot.	*/

 32 bit rel. offset to jump slot >> 1. */

 16 bit PC rel. PLT shifted by 1.  */

 32 bit PC rel. PLT shifted by 1.  */

 32 bit PC relative PLT address.  */

 64 bit PC relative PLT address.  */

 16 bit offset from GOT to PLT. */

 32 bit offset from GOT to PLT. */

 16 bit offset from GOT to PLT. */

		/* Only needed if we want to support loading of 

/*

 * Account for GOT and PLT relocations. We can't add sections for

 * got and plt but we can increase the core module size.

 Find symbol table and string table. */

 Allocate one syminfo structure per symbol. */

 "Define" it as absolute. */

 Search for got/plt relocations. */

	/* Increase core size by size of got & plt and set start

 This is where to make the change */

	/* This is the symbol it is referring to.  Note that all

 No relocation.  */

 Direct 8 bit.   */

 Direct 12 bit.  */

 Direct 16 bit.  */

 Direct 20 bit.  */

 Direct 32 bit.  */

 Direct 64 bit.  */

 PC relative 16 bit.  */

 PC relative 16 bit shifted by 1.  */

 PC relative 32 bit shifted by 1.  */

 PC relative 32 bit.  */

 PC relative 64 bit.	*/

 12 bit GOT offset.  */

 16 bit GOT offset.  */

 20 bit GOT offset.  */

 32 bit GOT offset.  */

 64 bit GOT offset.  */

 32 bit PC rel. to GOT entry shifted by 1. */

 12 bit offset to jump slot.	*/

 20 bit offset to jump slot.  */

 16 bit offset to jump slot.  */

 32 bit offset to jump slot.  */

 64 bit offset to jump slot.	*/

 32 bit rel. offset to jump slot >> 1. */

 16 bit PC rel. PLT shifted by 1.  */

 32 bit PC rel. PLT shifted by 1.  */

 32 bit PC relative PLT address.  */

 64 bit PC relative PLT address.  */

 16 bit offset from GOT to PLT. */

 32 bit offset from GOT to PLT. */

 16 bit offset from GOT to PLT. */

 basr 1,0  */

 lg	1,10(1) */

 j __jump_r1 */

 br %r1 */

 16 bit offset to GOT.  */

 32 bit offset to GOT.  */

 64 bit offset to GOT. */

 32 bit PC relative offset to GOT. */

 32 bit PC rel. off. to GOT shifted by 1. */

 Create GOT entry.  */

 Create PLT entry.  */

 Adjust by program base.  */

		/* Only needed if we want to support loading of 

 CONFIG_FUNCTION_TRACER */

 exrl	%r0,.+10	*/

 j	.		*/

 br	%r1		*/

 j	.		*/

 patch .altinstructions */

 CONFIG_FUNCTION_TRACER */

 SPDX-License-Identifier: GPL-2.0

/*

 * vdso setup for s390

 *

 *  Copyright IBM Corp. 2008

 *  Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com)

	/*

	 * VM_PFNMAP | VM_IO protect .fault() handler from being called

	 * through interfaces like /proc/$pid/mem or

	 * process_vm_{readv,writev}() as long as there's no .access()

	 * in special_mapping_vmops().

	 * For more details check_vma_flags() and __access_remote_vm()

/*

 * The VVAR page layout depends on whether a task belongs to the root or

 * non-root time namespace. Whenever a task changes its namespace, the VVAR

 * page tables are cleared and then they will be re-faulted with a

 * corresponding layout.

 * See also the comment near timens_setup_vdso_data() for details.

			/*

			 * Fault in VVAR page too, since it will be accessed

			 * to get clock data anyway.

		/*

		 * If a task belongs to a time namespace then a namespace

		 * specific VVAR is mapped with the VVAR_DATA_PAGE_OFFSET and

		 * the real VVAR page is mapped with the VVAR_TIMENS_PAGE_OFFSET

		 * offset.

		 * See also the comment near timens_setup_vdso_data().

 CONFIG_TIME_NS */

 Must be called before SMP init */

 VM_MAYWRITE for COW so gdb can set breakpoints */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * The scan order should be from start to end. A later scanned

	 * alternative code can overwrite previously scanned alternative code.

 SPDX-License-Identifier: GPL-2.0

/*

 *   Machine check handler

 *

 *    Copyright IBM Corp. 2000, 2009

 *    Author(s): Ingo Adlung <adlung@de.ibm.com>,

 *		 Martin Schwidefsky <schwidefsky@de.ibm.com>,

 *		 Cornelia Huck <cornelia.huck@de.ibm.com>,

 *		 Heiko Carstens <heiko.carstens@de.ibm.com>,

/*

 * The initial machine check extended save area for the boot CPU.

 * It will be replaced by nmi_init() with an allocated structure.

 * The structure is required for machine check happening early in

 * the boot process.

 create slab cache for the machine-check-extended-save-areas */

 The pointer is stored with mcesa_bits ORed in */

 disable lowcore protection */

 Replace boot_mcesa on the boot CPU */

 The pointer is stored with mcesa_bits ORed in */

/*

 * Main machine check handler function. Will be called with interrupts disabled

 * and machine checks enabled.

	/*

	 * Disable machine checks and get the current state of accumulated

	 * machine checks. Afterwards delete the old state and enable machine

	 * checks again.

	/*

	 * A warning may remain for a prolonged period on the bare iron.

	 * (actually until the machine is powered off, or the problem is gone)

	 * So we just stop listening for the WARNING MCH and avoid continuously

	 * being interrupted.  One caveat is however, that we must do this per

	 * processor and cannot use the smp version of ctl_clear_bit().

	 * On VM we only get one interrupt per virtally presented machinecheck.

	 * Though one suffices, we may get one interrupt per (virtual) cpu.

 WARNING pending ? */

 Use single cpu clear, as we cannot handle smp here. */

 Disable WARNING MCH */

/*

 * returns 0 if all required registers are available

 * returns 1 otherwise

		/*

		 * General purpose registers couldn't be restored and have

		 * unknown contents. Stop system or terminate process.

		/*

		 * Floating point registers can't be restored. If the

		 * kernel currently uses floating point registers the

		 * system is stopped. If the process has its floating

		 * pointer registers loaded it is terminated.

		/*

		 * Floating point control register can't be restored.

		 * If the kernel currently uses the floating pointer

		 * registers and needs the FPC register the system is

		 * stopped. If the process has its floating pointer

		 * registers loaded it is terminated. Otherwise the

		 * FPC is just validated.

 Validate floating point registers */

 Validate vector registers */

			/*

			 * Vector registers can't be restored. If the kernel

			 * currently uses vector registers the system is

			 * stopped. If the process has its vector registers

			 * loaded it is terminated. Otherwise just validate

			 * the registers.

 vlm 0,15,0(1) */

 vlm 16,31,256(1) */

 Validate access registers */

		/*

		 * Access registers have unknown contents.

		 * Terminating task.

 Validate guarded storage registers */

			/*

			 * Guarded storage register can't be restored and

			 * the current processes uses guarded storage.

			 * It has to be terminated.

	/*

	 * The getcpu vdso syscall reads CPU number from the programmable

	 * field of the TOD clock. Disregard the TOD programmable register

	 * validity bit and load the CPU number into the TOD programmable

	 * field unconditionally.

 Validate clock comparator register */

/*

 * Backup the guest's machine check info to its description block

 r14 contains the sie block, which was set in sie64a */

 Something's seriously wrong, stop system. */

 5 minutes */

 External damage STP island check */

 External damage STP sync check */

/*

 * machine check handler.

	/*

	 * Reinject the instruction processing damages' machine checks

	 * including Delayed Access Exception into the guest

	 * instead of damaging the host if they happen in the guest.

 Processing backup -> verify if we can survive this */

			/*

			 * Nullifying exigent condition, therefore we might

			 * retry this instruction.

 Processing damage -> stopping machine */

		/*

		 * Couldn't restore all register contents for the

		 * user space process -> mark task for termination.

	/*

	 * Backup the machine check's info if it happens when the guest

	 * is running.

 Timing facility damage */

 External damage */

 Channel report word pending */

 Warning pending */

	/*

	 * If there are only Channel Report Pending and External Damage

	 * machine checks, they will not be reinjected into the guest

	 * because they refer to host conditions only.

 Set exit reason code for host's later handling */

 enable external damage MCH */

 enable system recovery MCH */

 enable warning MCH */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Copyright IBM Corp. 2004, 2011

 *    Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>,

 *		 Holger Smolinski <Holger.Smolinski@de.ibm.com>,

 *		 Thomas Spatzier <tspat@de.ibm.com>,

 *

 * This file contains interrupt related functions.

/*

 * The list of "main" irq classes on s390. This is the list of interrupts

 * that appear both in /proc/stat ("intr" line) and /proc/interrupts.

 * Historically only external and I/O interrupts have been part of /proc/stat.

 * We can't add the split external and I/O sub classes since the first field

 * in the "intr" line in /proc/stat is supposed to be the sum of all other

 * fields.

 * Since the external and I/O interrupt fields are already sums we would end

 * up with having a sum which accounts each interrupt twice.

/*

 * The list of split external and I/O interrupts that appear only in

 * /proc/interrupts.

 * In addition this list contains non external / I/O events like NMIs.

 Serve timer interrupts first. */

/*

 * show_interrupts is needed by /proc/interrupts.

/*

 * ext_int_hash[index] is the list head for all external interrupts that hash

 * to this index.

 ext_int_hash_lock protects the handler lists for external interrupts */

 SPDX-License-Identifier: GPL-2.0

/*

 * Tracepoint definitions for s390

 *

 * Copyright IBM Corp. 2015

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 Avoid lockdep recursion. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common Ultravisor functions and initialization

 *

 * Copyright IBM Corp. 2019, 2020

 the bootdata_preserved fields come from ones in arch/s390/boot/uv.c */

/*

 * Requests the Ultravisor to pin the page in the shared state. This will

 * cause an intercept when the guest attempts to unshare the pinned page.

/*

 * Requests the Ultravisor to destroy a guest page and make it

 * accessible to the host. The destroy clears the page instead of

 * exporting.

 *

 * @paddr: Absolute host address of page to be destroyed

		/*

		 * Older firmware uses 107/d as an indication of a non secure

		 * page. Let us emulate the newer variant (no-op).

/*

 * The caller must already hold a reference to the page

/*

 * Requests the Ultravisor to encrypt a guest page and make it

 * accessible to the host for paging (export).

 *

 * @paddr: Absolute host address of page to be exported

/*

 * The caller must already hold a reference to the page

/*

 * Calculate the expected ref_count for a page that would otherwise have no

 * further pins. This was cribbed from similar functions in other places in

 * the kernel, but with some slight modifications. We know that a secure

 * page can not be a huge page for example.

	/*

	 * If the UVC does not succeed or fail immediately, we don't want to

	 * loop for long, or we might get stall notifications.

	 * On the other hand, this is a complex scenario and we are holding a lot of

	 * locks, so we can't easily sleep and reschedule. We try only once,

	 * and if the UVC returned busy or partial completion, we return

	 * -EAGAIN and we let the callers deal with it.

	/*

	 * Return -ENXIO if the page was not mapped, -EINVAL for other errors.

	 * If busy or partially completed, return -EAGAIN.

/*

 * Requests the Ultravisor to make a page accessible to a guest.

 * If it's brought in the first time, it will be cleared. If

 * it has been exported before, it will be decrypted and integrity

 * checked.

	/*

	 * Secure pages cannot be huge and userspace should not combine both.

	 * In case userspace does it anyway this will result in an -EFAULT for

	 * the unpack. The guest is thus never reaching secure mode. If

	 * userspace is playing dirty tricky with mapping huge pages later

	 * on this will result in a segmentation fault.

		/*

		 * If we are here because the UVC returned busy or partial

		 * completion, this is just a useless check, but it is safe.

		/*

		 * If we have tried a local drain and the page refcount

		 * still does not match our expected safe value, try with a

		 * system wide drain. This is needed if the pagevecs holding

		 * the page are on a different CPU.

 We give up here, and let the caller try again */

		/*

		 * We are here if the page refcount does not match the

		 * expected safe value. The main culprits are usually

		 * pagevecs. With lru_add_drain() we drain the pagevecs

		 * on the local CPU so that hopefully the refcount will

		 * reach the expected safe value.

 And now we try again immediately after draining */

/*

 * To be called with the page locked or with an extra reference! This will

 * prevent gmap_make_secure from touching the page concurrently. Having 2

 * parallel make_page_accessible is fine, as the UV calls will become a

 * no-op if the page is already exported.

 Hugepage cannot be protected, so nothing to do */

	/*

	 * PG_arch_1 is used in 3 places:

	 * 1. for kernel page tables during early boot

	 * 2. for storage keys of huge pages and KVM

	 * 3. As an indication that this page might be secure. This can

	 *    overindicate, e.g. we set the bit before calling

	 *    convert_to_secure.

	 * As secure pages are never huge, all 3 variants can co-exists.

 SPDX-License-Identifier: GPL-2.0

/*

 *    Copyright IBM Corp. 2000, 2006

 *    Author(s): Denis Joseph Barrow (djbarrow@de.ibm.com,barrow_dj@yahoo.com)

 *               Gerhard Tonn (ton@de.ibm.com)                  

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  1997-11-28  Modified for POSIX.1b signals by Richard Henderson

 Offset of svc_insn is NOT fixed! */

 Store registers needed to create the signal frame */

 Load registers after signal return */

 Alwys make any pending restarted system call return -EINTR */

 Test the floating-point-control word. */

 Use regs->psw.mask instead of PSW_USER_BITS to preserve PER bit. */

 Check for invalid user address space control. */

 No longer in a system call */

 Save high gprs to signal stack */

 Save vector registers to signal stack */

 Restore high gprs from signal stack */

 Restore vector registers from signal stack */

/*

 * Set up a signal frame.

/*

 * Determine which stack to use..

 Default to using normal stack */

 Overflow on alternate signal stack gives SIGSEGV. */

 This is the X/Open sanctioned signal stack switching.  */

	/*

	 * gprs_high are always present for 31-bit compat tasks.

	 * The space for vector registers is only allocated if

	 * the machine supports it

 Set up backchain. */

 Create struct sigcontext32 on the signal stack */

 Store registers needed to create the signal frame */

 Create _sigregs32 on the signal stack */

 Place signal number on stack to allow backtrace from handler.  */

 Create _sigregs_ext32 on the signal stack */

	/* Set up to return from userspace.  If provided, use a stub

 Set up registers for signal handler */

 Force 31 bit amode and default user address space control. */

	/* We forgot to include these in the sigcontext.

 set extra registers only for synchronous signals */

	/*

	 * gprs_high are always present for 31-bit compat tasks.

	 * The space for vector registers is only allocated if

	 * the machine supports it

 Set up backchain. */

	/* Set up to return from userspace.  If provided, use a stub

 Create siginfo on the signal stack */

 Store registers needed to create the signal frame */

 Create ucontext on the signal stack. */

 Set up registers for signal handler */

 Force 31 bit amode and default user address space control. */

/*

 * OK, we're invoking a handler

 Set up the stack frame */

 SPDX-License-Identifier: GPL-2.0

/*

 * Generate definitions needed by assembly language modules.

 * This code generates raw asm output which is post-processed to extract

 * and format the required data.

 task struct offsets */

 thread struct offsets */

 thread info offsets */

 pt_regs offsets */

 stack_frame offsets */

 idle data offsets */

 hardware defined lowcore locations 0x000 - 0x1ff */

 software defined lowcore locations 0x200 - 0xdff*/

 software defined ABI-relevant lowcore locations 0xe00 - 0xe20 */

 hardware defined lowcore locations 0x1000 - 0x18ff */

 gmap/sie offsets */

 kexec_sha_region */

 sizeof kernel parameter area */

 kernel parameter area offsets */

 SPDX-License-Identifier: GPL-2.0

 Copyright IBM Corp. 2020 */

 CPU number is stored in the programmable field of the TOD clock */

 NUMA node is always zero */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Optimized xor_block operation for RAID4/5

 *

 * Copyright IBM Corp. 2016

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * MSB0 numbered special bitops handling.

 *

 * The bits are numbered:

 *   |0..............63|64............127|128...........191|192...........255|

 *

 * The reason for this bit numbering is the fact that the hardware sets bits

 * in a bitmap starting at bit 0 (MSB) and we don't want to scan the bitmap

 * from the 'wrong end'.

 Are any bits set? */

 Nope. */

 Are any bits set? */

 Nope. */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Out of line spinlock code.

 *

 *    Copyright IBM Corp. 2004, 2006

 *    Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com)

/*

 * spin_retry= parameter

 NIAI 4 */

 NIAI 8 */

 0 -> no target cpu */

 cpu + 1 */

 Enqueue the node for this CPU in the spinlock wait queue */

			/*

			 * The lock is free but there may be waiters.

			 * With no waiters simply take the lock, if there

			 * are waiters try to steal the lock. The lock may

			 * be stolen three times before the next queued

			 * waiter will get the lock.

 Got the lock */

 lock passing in progress */

 Make the node of this CPU the new tail. */

 Set the 'next' pointer of the tail node in the queue */

 Pass the virtual CPU to the lock holder if it is not running */

 Spin on the CPU local node->prev pointer */

 Query running state of lock holder again. */

 Spin on the lock value in the spinlock_t */

 Got the lock */

 Pass lock_spin job to next CPU in the queue */

 Wait until the next CPU has set up the 'next' pointer */

 cpu + 1 */

 Pass the virtual CPU to the lock holder if it is not running */

 Try to get the lock if it is free. */

 Got the lock */

 Try to get the lock if it is free. */

 Remove this reader again to allow recursive read locking */

 Put the reader into the wait queue */

 Now add this reader to the count value again */

 Loop until the writer is done */

 Add this CPU to the write waiters */

 Put the writer into the wait queue */

 Got the lock */

 SPDX-License-Identifier: GPL-2.0+

	/*

	 * Emulate 'br 14'. 'regs' is captured by kprobes on entry to some

	 * kernel function.

 SPDX-License-Identifier: GPL-2.0

/*

 *    Optimized string functions

 *

 *  S390 version

 *    Copyright IBM Corp. 2004

 *    Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com)

/*

 * Helper functions to find the end of a string

/**

 * strlen - Find the length of a string

 * @s: The string to be sized

 *

 * returns the length of @s

/**

 * strnlen - Find the length of a length-limited string

 * @s: The string to be sized

 * @n: The maximum number of bytes to search

 *

 * returns the minimum of the length of @s and @n

/**

 * strcpy - Copy a %NUL terminated string

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 *

 * returns a pointer to @dest

/**

 * strncpy - Copy a length-limited, %NUL-terminated string

 * @dest: Where to copy the string to

 * @src: Where to copy the string from

 * @n: The maximum number of bytes to copy

 *

 * The result is not %NUL-terminated if the source exceeds

 * @n bytes.

/**

 * strcat - Append one %NUL-terminated string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

 *

 * returns a pointer to @dest

/**

 * strlcat - Append a length-limited, %NUL-terminated string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

 * @n: The size of the destination buffer.

/**

 * strncat - Append a length-limited, %NUL-terminated string to another

 * @dest: The string to be appended to

 * @src: The string to append to it

 * @n: The maximum numbers of bytes to copy

 *

 * returns a pointer to @dest

 *

 * Note that in contrast to strncpy, strncat ensures the result is

 * terminated.

/**

 * strcmp - Compare two strings

 * @s1: One string

 * @s2: Another string

 *

 * returns   0 if @s1 and @s2 are equal,

 *	   < 0 if @s1 is less than @s2

 *	   > 0 if @s1 is greater than @s2

/**

 * strstr - Find the first substring in a %NUL terminated string

 * @s1: The string to be searched

 * @s2: The string to search for

/**

 * memchr - Find a character in an area of memory.

 * @s: The memory area

 * @c: The byte to search for

 * @n: The size of the area.

 *

 * returns the address of the first occurrence of @c, or %NULL

 * if @c is not found

/**

 * memcmp - Compare two areas of memory

 * @s1: One area of memory

 * @s2: Another area of memory

 * @n: The size of the area.

/**

 * memscan - Find a character in an area of memory.

 * @s: The memory area

 * @c: The byte to search for

 * @n: The size of the area.

 *

 * returns the address of the first occurrence of @c, or 1 byte past

 * the area if @c is not found

 SPDX-License-Identifier: GPL-2.0

/*

 *  Standard user space access functions based on mvcp/mvcs and doing

 *  interesting things in the secondary space mode.

 *

 *    Copyright IBM Corp. 2006,2014

 *    Author(s): Martin Schwidefsky (schwidefsky@de.ibm.com),

 *		 Gerald Schaefer (gerald.schaefer@de.ibm.com)

CONFIG_DEBUG_ENTRY */

 %4 = ptr + 4095 */

 %4 = (ptr + 4095) & -4096 */

 copy crosses next page boundary? */

 %4 = ptr + 255 */

 %4 = (ptr + 255) & -4096 */

 copy crosses next page boundary? */

 %4 = ptr + 4095 */

 %4 = (ptr + 4095) & -4096 */

 copy crosses next page boundary? */

 %4 = ptr + 255 */

 %4 = (ptr + 255) & -4096 */

 copy crosses next page boundary? */

 %4 = to + 4095 */

 %4 = (to + 4095) & -4096 */

 copy crosses next page boundary? */

 %2 = ptr + 255 */

 %2 = (ptr + 255) & -4096 */

 clear crosses next page boundary? */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Test module for unwind_for_each_frame

/*

 * To avoid printk line limit split backtrace by lines

/*

 * Calls unwind_for_each_frame(task, regs, sp) and verifies that the result

 * contains unwindme_func2 followed by unwindme_func1.

 Unwind. */

 Check the results. */

 State of the task being unwound. */

 Values of unwindme.flags. */

 Unwind a separate task. */

 Pass regs to test_unwind(). */

 Pass sp to test_unwind(). */

 Unwind starting from caller. */

 Use call_on_stack. */

 Unwind from irq context. */

 Unwind from program check handler. */

 This function may or may not appear in the backtrace. */

		/*

		 * trigger specification exception

 This function may or may not appear in the backtrace. */

 This function must appear in the backtrace. */

 This function must follow unwindme_func2 in the backtrace. */

 Spawns a task and passes it to test_unwind(). */

 Initialize thread-related fields. */

	/*

	 * Start the task and wait until it reaches unwindme_func4() and sleeps

	 * in (task_ready, unwind_done] range.

	/*

	 * Make sure task reaches unwindme_func4 before parking it,

	 * we might park it before kthread function has been executed otherwise

 Unwind. */

/*

 * Create required parameter list for tests

/*

 * Parameter description generator: required for KUNIT_ARRAY_PARAM()

/*

 * Create test_unwind_gen_params

 SPDX-License-Identifier: GPL-2.0

/*

 *    Common helper functions for kprobes and uprobes

 *

 *    Copyright IBM Corp. 2014

 bassm */

 bsm	 */

 diag  */

 ex	 */

 stnsm */

 stosm */

 exrl   */

 pr	 */

 bsa	 */

 bakr  */

 bsg	 */

 pc	 */

 pt	 */

 epsw	 */

 tbegin */

 tbeginc */

 tend	 */

 default fixup method */

 balr	*/

 basr */

 if r2 = 0, no branch will be taken */

 bctr	*/

 bcr	*/

 bal	*/

 bas	*/

 bc	*/

 bct	*/

 bxh	*/

 bxle	*/

 lpsw	*/

 lpswe */

 bras	*/

 brasl */

 bxhg  */

 bxleg */

 bctg	*/

 clgrb */

 cgrb  */

 crb   */

 clrb  */

 cgib  */

 cglib */

 cib   */

 clib  */

	/* Check if we have a RIL-b or RIL-c format instruction which

 larl */

 llhrl  */

 lghrl  */

 lhrl   */

 llghrl */

 sthrl  */

 lgrl   */

 stgrl  */

 lgfrl  */

 lrl    */

 llgfrl */

 strl   */

 pfdrl  */

 cghrl  */

 chrl   */

 clghrl */

 clhrl  */

 cgrl   */

 clgrl  */

 cgfrl  */

 crl    */

 clgfrl */

 clrl   */

 SPDX-License-Identifier: GPL-2.0+

 SPDX-License-Identifier: GPL-2.0

/*

 *    Precise Delay Loops for S390

 *

 *    Copyright IBM Corp. 1999, 2008

 *    Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>,

 *		 Heiko Carstens <heiko.carstens@de.ibm.com>,

        /*

         * To end the bloody studid and useless discussion about the

         * BogoMips number I took the liberty to define the __delay

         * function in a way that that resulting BogoMips number will

         * yield the megahertz number of the cpu. The important function

         * is udelay and that is done using the tod clock. -- martin.

 SPDX-License-Identifier: GPL-1.0+

/*

 *    Hypervisor filesystem for Linux on s390.

 *

 *    Copyright IBM Corp. 2006, 2008

 *    Author(s): Michael Holzheu <holzheu@de.ibm.com>

 ASCII 'hyp' */

 size of temporary buffers */

 uid used for files and dirs */

 gid used for files and dirs */

 file to trigger update */

 last update, CLOCK_MONOTONIC time */

 lock to protect update process */

 start of list of all dentries, which have to be deleted on update */

 directory tree removal functions */

	/*

	 * Currently we only allow one update per second for two reasons:

	 * 1. diag 204 is VERY expensive

	 * 2. If several processes do updates in parallel and then read the

	 *    hypfs data, the likelihood of collisions is reduced, if we restrict

	 *    the minimum update interval. A collision occurs, if during the

	 *    data gathering of one process another process triggers an update

	 *    If the first process wants to ensure consistent data, it has

	 *    to restart data collection in this case.

	/*

	 * We do not put the update file on the 'delete' list with

	 * hypfs_add_dentry(), since it should not be removed when the tree

	 * is updated.

 SPDX-License-Identifier: GPL-2.0

/*

 * Hypervisor filesystem for Linux on s390

 *

 * Diag 0C implementation

 *

 * Copyright IBM Corp. 2014

/*

 * Get hypfs_diag0c_entry from CPU vector and store diag0c data

/*

 * Allocate buffer and store diag 0c data

 Note: Diag 0c needs 8 byte alignment and real storage */

 Fill CPU vector for each online CPU */

 Collect data all CPUs */

/*

 * Hypfs DBFS callback: Free diag 0c data

/*

 * Hypfs DBFS callback: Create diag 0c data

/*

 * Hypfs DBFS file structure

/*

 * Initialize diag 0c interface for z/VM

/*

 * Shutdown diag 0c interface for z/VM

 SPDX-License-Identifier: GPL-2.0

/*

 * Hypervisor filesystem for Linux on s390 - debugfs interface

 *

 * Copyright IBM Corp. 2010

 * Author(s): Michael Holzheu <holzheu@linux.vnet.ibm.com>

 SPDX-License-Identifier: GPL-2.0

/*

 *    Hypervisor filesystem for Linux on s390. z/VM implementation.

 *

 *    Copyright IBM Corp. 2006

 *    Author(s): Michael Holzheu <holzheu@de.ibm.com>

/*

 * Allocate buffer for "query" and store diag 2fc at "offset"

 guest dir */

 logical cpu information */

	/*

	 * Note: The "weight_min" attribute got the wrong name.

	 * The value represents the number of non-stopped (operating)

	 * CPUS.

 memory information */

 samples */

 Hpervisor Info */

 physical cpus */

 guests */

 Length of d2fc buffer without header */

 Version of header */

 TOD clock for d2fc */

 Number of VM guests in d2fc buffer */

 64 byte header */

 d2fc buffer */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Hypervisor filesystem for Linux on s390.

 *    Set Partition-Resource Parameter interface.

 *

 *    Copyright IBM Corp. 2013

 *    Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 unknown ioctl number */

 SPDX-License-Identifier: GPL-2.0

/*

 *    Hypervisor filesystem for Linux on s390. Diag 204 and 224

 *    implementation.

 *

 *    Copyright IBM Corp. 2006, 2008

 *    Author(s): Michael Holzheu <holzheu@de.ibm.com>

 size of temporary buffers */

 diag 224 name table */

 used subcode for store */

 used diag 204 data format */

 4K aligned buffer for diag204 data */

 vmalloc pointer for diag204 data */

 number of pages for diag204 data */

/*

 * DIAG 204 member access functions.

 *

 * Since we have two different diag 204 data formats for old and new s390

 * machines, we do not access the structs directly, but use getter functions for

 * each struct member instead. This should make the code more readable.

 Time information block */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 Partition header */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 CPU info block */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 online_time not available in simple info */

 DIAG204_INFO_EXT */

 Physical header */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 Physical CPU info block */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 DIAG204_INFO_EXT */

 Diagnose 204 functions */

/*

 * For the old diag subcode 4 with simple data format we have to use real

 * memory. If we use subcode 6 or 7 with extended data format, we can (and

 * should) use vmalloc, since we need a lot of memory in that case. Currently

 * up to 93 pages!

 The buffer has to be page aligned! */

 DIAG204_INFO_EXT */

/*

 * diag204_probe() has to find out, which type of diagnose 204 implementation

 * we have on our machine. Currently there are three possible scanarios:

 *   - subcode 4   + simple data format (only one page)

 *   - subcode 4-6 + extended data format

 *   - subcode 4-7 + extended data format

 *

 * Subcode 5 is used to retrieve the size of the data, provided by subcodes

 * 6 and 7. Subcode 7 basically has the same function as subcode 6. In addition

 * to subcode 6 it provides also information about secondary cpus.

 * In order to get as much information as possible, we first try

 * subcode 7, then 6 and if both fail, we use subcode 4.

 subcodes 6 and 7 failed, now try subcode 4 */

 Diagnose 224 functions */

 memory must be below 2GB */

 Length of d204 buffer without header */

 Version of header */

 Used subcode */

 64 byte header */

 d204 buffer */

/*

 * Functions to create the directory structure

 * *******************************************

 SPDX-License-Identifier: GPL-2.0

/*

 * BPF Jit compiler for s390.

 *

 * Minimum build requirements:

 *

 *  - HAVE_MARCH_Z196_FEATURES: laal, laalg

 *  - HAVE_MARCH_Z10_FEATURES: msfi, cgrj, clgrj

 *  - HAVE_MARCH_Z9_109_FEATURES: alfi, llilf, clfi, oilf, nilf

 *  - PACK_STACK

 *  - 64BIT

 *

 * Copyright IBM Corp. 2012,2015

 *

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 *	      Michael Holzheu <holzheu@linux.vnet.ibm.com>

 Flags to remember seen eBPF instructions */

 Array to remember which registers are used */

 Array with relative instruction addresses */

 Start of program */

 Size of program and literal pool */

 Size of program */

 Current position in program */

 Start of 32-bit literal pool */

 Current position in 32-bit literal pool */

 Start of 64-bit literal pool */

 Current position in 64-bit literal pool */

 Base address for literal pool */

 Address of exit */

 Address of expoline thunk for 'br %r1' */

 Address of expoline thunk for 'br %r14' */

 Tail call start offset */

 Number of exception table entries */

 use mem[] for temporary storage */

 code uses literals */

 calls C functions */

 code uses tail calls */

/*

 * s390 registers

 Work register 1 (even) */

 Work register 2 (odd) */

 Literal pool register */

 Register 15 */

 Register 0 */

 Register 1 */

 Register 2 */

 Register 14 */

/*

 * Mapping of BPF registers to s390 registers

 Return code */

 Function parameters */

 Call saved registers */

 BPF stack pointer */

 Register for blinding */

 Work registers for s390x backend */

/*

 * EMIT macros for code generation

 llgfr %dst,%dst (zero extend to 64 bit) */	\

/*

 * Return whether this is the first pass. The first pass is special, since we

 * don't know any sizes yet, and thus must be conservative.

/*

 * Return whether this is the code generation pass. The code generation pass is

 * special, since we should change as little as possible.

/*

 * Return whether "rel" can be encoded as a short PC-relative offset

/*

 * Return whether "off" can be reached using a short PC-relative offset

/*

 * Return whether given displacement can be encoded using

 * Long-Displacement Facility

/*

 * Return whether the next 32-bit literal pool entry can be referenced using

 * Long-Displacement Facility

/*

 * Return whether the next 64-bit literal pool entry can be referenced using

 * Long-Displacement Facility

/*

 * Fill whole space with illegal instructions

/*

 * Save registers from "rs" (register start) to "re" (register end) on stack

 stg %rs,off(%r15) */

 stmg %rs,%re,off(%r15) */

/*

 * Restore registers from "rs" (register start) to "re" (register end) on stack

 lg %rs,off(%r15) */

 lmg %rs,%re,off(%r15) */

/*

 * Return first seen register (from start)

/*

 * Return last seen register (from start) (gap >= 2)

/*

 * Save and restore clobbered registers (6-15) on stack.

 * We save/restore registers in chunks with gap >= 2 registers.

		/*

		 * We don't know yet which registers are used. Reserve space

		 * conservatively.

 brcl 0xf,size */

 brc 0xf,size */

 bcr 0,%0 */

/*

 * Emit function prologue

 *

 * Save registers and create stack frame if necessary.

 * See stack frame layout desription in "bpf_jit.h"!

 xc STK_OFF_TCCNT(4,%r15),STK_OFF_TCCNT(%r15) */

		/*

		 * There are no tail calls. Insert nops in order to have

		 * tail_call_start at a predictable offset.

 Tail calls have to skip above initialization */

 Save registers */

 Setup literal pool */

 basr %l,0 */

 larl %l,lit32_start */

 Setup stack and backchain */

 lgr %w1,%r15 (backchain) */

 la %bfp,STK_160_UNUSED(%r15) (BPF frame pointer) */

 aghi %r15,-STK_OFF */

 stg %w1,152(%r15) (backchain) */

/*

 * Function epilogue

 Load exit code: lgr %r2,%b0 */

 Restore registers */

 Generate __s390_indirect_jump_r14 thunk */

 exrl %r0,.+10 */

 larl %r1,.+14 */

 ex 0,0(%r1) */

 j . */

 br %r14 */

 Generate __s390_indirect_jump_r1 thunk */

 exrl %r0,.+10 */

 j . */

 br %r1 */

 ex 0,S390_lowcore.br_r1_tampoline */

 j . */

	/*

	 * insn must point to llgc, llgh, llgf or lg, which have destination

	 * register at the same position.

 common llgc, llgh, llgf and lg prefix */

 llgc */

 llgh */

 llgf */

 lg */

 JIT bug - unexpected instruction. */

 Do nothing during early JIT passes. */

 JIT bug - unexpected probe instruction. */

 JIT bug - gap between probe and nop instructions. */

 Verifier bug - not enough entries. */

 Add extable entries for probe and nop instructions. */

 JIT bug - code and extable must be close. */

		/*

		 * Always land on the nop. Note that extable infrastructure

		 * ignores fixup field, it is handled by ex_handler_bpf().

 JIT bug - landing pad and extable must be close. */

/*

 * Compile one eBPF instruction into s390x code

 *

 * NOTE: Use noinline because for gcov (-fprofile-arcs) gcc allocates a lot of

 * stack space for the large switch statement.

	/*

	 * BPF_MOV

 dst = (u32) src */

 llgfr %dst,%src */

 dst = src */

 lgr %dst,%src */

 dst = (u32) imm */

 llilf %dst,imm */

 dst = imm */

 lgfi %dst,imm */

	/*

	 * BPF_LD 64

 dst = (u64) imm */

 16 byte instruction that uses two 'struct bpf_insn' */

 lgrl %dst,imm */

	/*

	 * BPF_ADD

 dst = (u32) dst + (u32) src */

 ar %dst,%src */

 dst = dst + src */

 agr %dst,%src */

 dst = (u32) dst + (u32) imm */

 alfi %dst,imm */

 dst = dst + imm */

 agfi %dst,imm */

	/*

	 * BPF_SUB

 dst = (u32) dst - (u32) src */

 sr %dst,%src */

 dst = dst - src */

 sgr %dst,%src */

 dst = (u32) dst - (u32) imm */

 alfi %dst,-imm */

 dst = dst - imm */

 algfi %dst,0x80000000 */

 agfi %dst,-imm */

	/*

	 * BPF_MUL

 dst = (u32) dst * (u32) src */

 msr %dst,%src */

 dst = dst * src */

 msgr %dst,%src */

 dst = (u32) dst * (u32) imm */

 msfi %r5,imm */

 dst = dst * imm */

 msgfi %dst,imm */

	/*

	 * BPF_DIV / BPF_MOD

 dst = (u32) dst / (u32) src */

 dst = (u32) dst % (u32) src */

 lhi %w0,0 */

 lr %w1,%dst */

 dlr %w0,%src */

 llgfr %dst,%rc */

 dst = dst / src */

 dst = dst % src */

 lghi %w0,0 */

 lgr %w1,%dst */

 dlgr %w0,%dst */

 lgr %dst,%rc */

 dst = (u32) dst / (u32) imm */

 dst = (u32) dst % (u32) imm */

 lhgi %dst,0 */

 lhi %w0,0 */

 lr %w1,%dst */

 dl %w0,<d(imm)>(%l) */

 lgfrl %dst,imm */

 dlr %w0,%dst */

 llgfr %dst,%rc */

 dst = dst / imm */

 dst = dst % imm */

 lhgi %dst,0 */

 lghi %w0,0 */

 lgr %w1,%dst */

 dlg %w0,<d(imm)>(%l) */

 lgrl %dst,imm */

 dlgr %w0,%dst */

 lgr %dst,%rc */

	/*

	 * BPF_AND

 dst = (u32) dst & (u32) src */

 nr %dst,%src */

 dst = dst & src */

 ngr %dst,%src */

 dst = (u32) dst & (u32) imm */

 nilf %dst,imm */

 dst = dst & imm */

 ng %dst,<d(imm)>(%l) */

 lgrl %w0,imm */

 ngr %dst,%w0 */

	/*

	 * BPF_OR

 dst = (u32) dst | (u32) src */

 or %dst,%src */

 dst = dst | src */

 ogr %dst,%src */

 dst = (u32) dst | (u32) imm */

 oilf %dst,imm */

 dst = dst | imm */

 og %dst,<d(imm)>(%l) */

 lgrl %w0,imm */

 ogr %dst,%w0 */

	/*

	 * BPF_XOR

 dst = (u32) dst ^ (u32) src */

 xr %dst,%src */

 dst = dst ^ src */

 xgr %dst,%src */

 dst = (u32) dst ^ (u32) imm */

 xilf %dst,imm */

 dst = dst ^ imm */

 xg %dst,<d(imm)>(%l) */

 lgrl %w0,imm */

 xgr %dst,%w0 */

	/*

	 * BPF_LSH

 dst = (u32) dst << (u32) src */

 sll %dst,0(%src) */

 dst = dst << src */

 sllg %dst,%dst,0(%src) */

 dst = (u32) dst << (u32) imm */

 sll %dst,imm(%r0) */

 dst = dst << imm */

 sllg %dst,%dst,imm(%r0) */

	/*

	 * BPF_RSH

 dst = (u32) dst >> (u32) src */

 srl %dst,0(%src) */

 dst = dst >> src */

 srlg %dst,%dst,0(%src) */

 dst = (u32) dst >> (u32) imm */

 srl %dst,imm(%r0) */

 dst = dst >> imm */

 srlg %dst,%dst,imm(%r0) */

	/*

	 * BPF_ARSH

 ((s32) dst) >>= src */

 sra %dst,%dst,0(%src) */

 ((s64) dst) >>= src */

 srag %dst,%dst,0(%src) */

 ((s32) dst >> imm */

 sra %dst,imm(%r0) */

 ((s64) dst) >>= imm */

 srag %dst,%dst,imm(%r0) */

	/*

	 * BPF_NEG

 dst = (u32) -dst */

 lcr %dst,%dst */

 dst = -dst */

 lcgr %dst,%dst */

	/*

	 * BPF_FROM_BE/LE

 s390 is big endian, therefore only clear high order bytes */

 dst = (u16) cpu_to_be16(dst) */

 llghr %dst,%dst */

 dst = (u32) cpu_to_be32(dst) */

 llgfr %dst,%dst */

 dst = (u64) cpu_to_be64(dst) */

 dst = (u16) cpu_to_le16(dst) */

 lrvr %dst,%dst */

 srl %dst,16(%r0) */

 llghr %dst,%dst */

 dst = (u32) cpu_to_le32(dst) */

 lrvr %dst,%dst */

 llgfr %dst,%dst */

 dst = (u64) cpu_to_le64(dst) */

 lrvgr %dst,%dst */

	/*

	 * BPF_NOSPEC (speculation barrier)

	/*

	 * BPF_ST(X)

 *(u8 *)(dst + off) = src_reg */

 stcy %src,off(%dst) */

 (u16 *)(dst + off) = src */

 sthy %src,off(%dst) */

 *(u32 *)(dst + off) = src */

 sty %src,off(%dst) */

 (u64 *)(dst + off) = src */

 stg %src,off(%dst) */

 *(u8 *)(dst + off) = imm */

 lhi %w0,imm */

 stcy %w0,off(dst) */

 (u16 *)(dst + off) = imm */

 lhi %w0,imm */

 sthy %w0,off(dst) */

 *(u32 *)(dst + off) = imm */

 llilf %w0,imm  */

 sty %w0,off(%dst) */

 *(u64 *)(dst + off) = imm */

 lgfi %w0,imm */

 stg %w0,off(%dst) */

	/*

	 * BPF_ATOMIC

 {op32|op64} {%w0|%src},%src,off(%dst) */

 {laal|laalg} */

 {lan|lang} */

 {lao|laog} */

 {lax|laxg} */

 {ly|lg} %w0,off(%dst) */

 0: {csy|csg} %w0,%src,off(%dst) */

 brc 4,0b */

 {llgfr|lgr} %src,%w0 */

 0: {csy|csg} %b0,%src,off(%dst) */

	/*

	 * BPF_LDX

 dst = *(u8 *)(ul) (src + off) */

 llgc %dst,0(off,%src) */

 dst = *(u16 *)(ul) (src + off) */

 llgh %dst,0(off,%src) */

 dst = *(u32 *)(ul) (src + off) */

 llgf %dst,off(%src) */

 dst = *(u64 *)(ul) (src + off) */

 lg %dst,0(off,%src) */

	/*

	 * BPF_JMP / CALL

 lgrl %w1,func */

 brasl %r14,__s390_indirect_jump_r1 */

 basr %r14,%w1 */

 lgr %b0,%r2: load return value into %b0 */

		/*

		 * Implicit input:

		 *  B1: pointer to ctx

		 *  B2: pointer to bpf_array

		 *  B3: index in bpf_array

		/*

		 * if (index >= array->map.max_entries)

		 *         goto out;

 llgf %w1,map.max_entries(%b2) */

 if ((u32)%b3 >= (u32)%w1) goto out; */

 clrj %b3,%w1,0xa,out */

		/*

		 * if (tail_call_cnt++ > MAX_TAIL_CALL_CNT)

		 *         goto out;

 lhi %w0,1 */

 laal %w1,%w0,off(%r15) */

 clij %w1,MAX_TAIL_CALL_CNT,0x2,out */

		/*

		 * prog = array->ptrs[index];

		 * if (prog == NULL)

		 *         goto out;

 llgfr %r1,%b3: %r1 = (u32) index */

 sllg %r1,%r1,3: %r1 *= 8 */

 ltg %r1,prog(%b2,%r1) */

 brc 0x8,out */

		/*

		 * Restore registers before calling function

		/*

		 * goto *(prog->bpf_func + tail_call_start);

 lg %r1,bpf_func(%r1) */

 bc 0xf,tail_call_start(%r1) */

 out: */

 return b0 */

 brc 0xf, <exit> */

 brcl 0xf, <exit> */

	/*

	 * Branch relative (number of skipped instructions) to offset on

	 * condition.

	 *

	 * Condition code to mask mapping:

	 *

	 * CC | Description	   | Mask

	 * ------------------------------

	 * 0  | Operands equal	   |	8

	 * 1  | First operand low  |	4

	 * 2  | First operand high |	2

	 * 3  | Unused		   |	1

	 *

	 * For s390x relative branches: ip = ip + off_bytes

	 * For BPF relative branches:	insn = insn + off_insns + 1

	 *

	 * For example for s390x with offset 0 we jump to the branch

	 * instruction itself (loop) and for BPF with offset 0 we

	 * branch to the instruction behind the branch.

 if (true) */

 j */

 ((s64) dst > (s64) imm) */

 ((s32) dst > (s32) imm) */

 jh */

 ((s64) dst < (s64) imm) */

 ((s32) dst < (s32) imm) */

 jl */

 ((s64) dst >= (s64) imm) */

 ((s32) dst >= (s32) imm) */

 jhe */

 ((s64) dst <= (s64) imm) */

 ((s32) dst <= (s32) imm) */

 jle */

 (dst_reg > imm) */

 ((u32) dst_reg > (u32) imm) */

 jh */

 (dst_reg < imm) */

 ((u32) dst_reg < (u32) imm) */

 jl */

 (dst_reg >= imm) */

 ((u32) dst_reg >= (u32) imm) */

 jhe */

 (dst_reg <= imm) */

 ((u32) dst_reg <= (u32) imm) */

 jle */

 (dst_reg != imm) */

 ((u32) dst_reg != (u32) imm) */

 jne */

 (dst_reg == imm) */

 ((u32) dst_reg == (u32) imm) */

 je */

 (dst_reg & imm) */

 ((u32) dst_reg & (u32) imm) */

 jnz */

 llilf %w1,imm (load zero extend imm) */

 nr %w1,%dst */

 lgfi %w1,imm (load sign extend imm) */

 ngr %w1,%dst */

 ((s64) dst > (s64) src) */

 ((s32) dst > (s32) src) */

 jh */

 ((s64) dst < (s64) src) */

 ((s32) dst < (s32) src) */

 jl */

 ((s64) dst >= (s64) src) */

 ((s32) dst >= (s32) src) */

 jhe */

 ((s64) dst <= (s64) src) */

 ((s32) dst <= (s32) src) */

 jle */

 (dst > src) */

 ((u32) dst > (u32) src) */

 jh */

 (dst < src) */

 ((u32) dst < (u32) src) */

 jl */

 (dst >= src) */

 ((u32) dst >= (u32) src) */

 jhe */

 (dst <= src) */

 ((u32) dst <= (u32) src) */

 jle */

 (dst != src) */

 ((u32) dst != (u32) src) */

 jne */

 (dst == src) */

 ((u32) dst == (u32) src) */

 je */

 (dst & src) */

 ((u32) dst & (u32) src) */

 jnz */

 nrk or ngrk %w1,%dst,%src */

 cfi or cgfi %dst,imm */

 brc mask,off */

 brcl mask,off */

 lgfi %w1,imm (load sign extend imm) */

 crj or cgrj %dst,%src,mask,off */

 cr or cgr %dst,%src */

 brcl mask,off */

 clrj or clgrj %dst,%src,mask,off */

 clr or clgr %dst,%src */

 brcl mask,off */

 brc mask,off */

 brcl mask,off */

 too complex, give up */

		/*

		 * Handlers of certain exceptions leave psw.addr pointing to

		 * the instruction directly after the failing one. Therefore,

		 * create two exception table entries and also add a nop in

		 * case two probing instructions come directly after each

		 * other.

 bcr 0,%0 */

/*

 * Return whether new i-th instruction address does not violate any invariant

 On the first pass anything goes */

 The codegen pass must not change anything */

 Passes in between must not increase code size */

/*

 * Update the address of i-th instruction

/*

 * Compile eBPF program into s390x code

 Next instruction address */

 Verifier bug - too many entries. */

 We need two entries per insn. */

/*

 * Compile eBPF program "fp"

	/*

	 * If blinding was requested and we failed during blinding,

	 * we must fall back to the interpreter.

	/*

	 * Three initial passes:

	 *   - 1/2: Determine clobbered registers

	 *   - 3:   Calculate program size and addrs arrray

	/*

	 * Final pass: Allocate and generate program

 SPDX-License-Identifier: GPL-2.0

/*

 *  IBM System z PNET ID Support

 *

 *    Copyright IBM Corp. 2018

#define PNETIDS_LEN		64	/* Total utility string length in bytes

					 * to cover up to 4 PNETIDs of 16 bytes

					 * for up to 4 device ports

 Max.length of a single port PNETID */

 Max. # of ports with a PNETID */

/*

 * Get the PNETIDs from a device.

 * s390 hardware supports the definition of a so-called Physical Network

 * Identifier (short PNETID) per network device port. These PNETIDs can be

 * used to identify network devices that are attached to the same physical

 * network (broadcast domain).

 *

 * The device can be

 * - a ccwgroup device with all bundled subchannels having the same PNETID

 * - a PCI attached network device

 *

 * Returns:

 * 0:		PNETIDs extracted from device.

 * -ENOMEM:	No memory to extract utility string.

 * -EOPNOTSUPP: Device type without utility string support

/*

 * Extract the pnetid for a device port.

 *

 * Return 0 if a pnetid is found and -ENOENT otherwise.

 SPDX-License-Identifier: GPL-2.0

/*

 * Purgatory code running between two kernels.

 *

 * Copyright IBM Corp. 2018

 *

 * Author(s): Philipp Rudo <prudo@linux.vnet.ibm.com>

 SPDX-License-Identifier: GPL-2.0

 arch function */

 SPDX-License-Identifier: GPL-2.0

/*

 * Data gathering module for Linux-VM Monitor Stream, Stage 1.

 * Collects data related to memory management.

 *

 * Copyright IBM Corp. 2003, 2006

 *

 * Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>

 Converts #Pages to KB */

/*

 * Memory data

 *

 * This is accessed as binary data by z/VM. If changes to it can't be avoided,

 * the structure version (product ID, see appldata_base.c) needs to be changed

 * as well and all documentation and z/VM applications using it must be

 * updated.

 after VM collected the record data, */

	u32 sync_count_2;	/* sync_count_1 and sync_count_2 should be the

				   same. If not, the record has been updated on

				   the Linux side while VM was collecting the

 data read from disk  */

 data written to disk */

 pages swapped in  */

 pages swapped out */

 sharedram is currently set to 0 */

 total main memory size */

 free main memory size  */

 total high memory size */

 free high memory size  */

 memory reserved for buffers, free cache */

 size of (used) cache, w/o buffers */

 total swap space size */

 free swap space */

 New in 2.6 -->

 page allocations */

 page faults (major+minor) */

 page faults (major only) */

 <-- New in 2.6

/*

 * appldata_get_mem_data()

 *

 * gather memory data

	/*

	 * don't put large structures on the stack, we are

	 * serialized through the appldata_ops_mutex and can use static

 EBCDIC "00" */

/*

 * appldata_mem_init()

 *

 * init_data, register ops

/*

 * appldata_mem_exit()

 *

 * unregister ops

 SPDX-License-Identifier: GPL-2.0

/*

 * Data gathering module for Linux-VM Monitor Stream, Stage 1.

 * Collects misc. OS related data (CPU utilization, running processes).

 *

 * Copyright IBM Corp. 2003, 2006

 *

 * Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>

/*

 * OS data

 *

 * This is accessed as binary data by z/VM. If changes to it can't be avoided,

 * the structure version (product ID, see appldata_base.c) needs to be changed

 * as well and all documentation and z/VM applications using it must be

 * updated.

 timer ticks spent in user mode   */

 ... spent with modified priority */

 ... spent in kernel mode         */

 ... spent in idle mode           */

 New in 2.6 */

 ... spent in interrupts          */

 ... spent in softirqs            */

 ... spent while waiting for I/O  */

 New in modification level 01 */

 ... stolen by hypervisor	    */

 number of this CPU		    */

 after VM collected the record data, */

	u32 sync_count_2;	/* sync_count_1 and sync_count_2 should be the

				   same. If not, the record has been updated on

				   the Linux side while VM was collecting the

 number of (virtual) CPUs        */

 size of the per-cpu data struct */

 offset of the first per-cpu data struct */

 number of runnable threads      */

 number of threads               */

 average nr. of running processes during */

 the last 1, 5 and 15 minutes */

 New in 2.6 */

	u32 nr_iowait;		/* number of blocked threads

 per cpu data */

 EBCDIC "01" */

/*

 * appldata_get_os_data()

 *

 * gather OS data

/*

 * appldata_os_init()

 *

 * init data, register ops

/*

 * appldata_os_exit()

 *

 * unregister ops

 SPDX-License-Identifier: GPL-2.0

/*

 * Data gathering module for Linux-VM Monitor Stream, Stage 1.

 * Collects accumulated network statistics (Packets received/transmitted,

 * dropped, errors, ...).

 *

 * Copyright IBM Corp. 2003, 2006

 *

 * Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>

/*

 * Network data

 *

 * This is accessed as binary data by z/VM. If changes to it can't be avoided,

 * the structure version (product ID, see appldata_base.c) needs to be changed

 * as well and all documentation and z/VM applications using it must be updated.

 after VM collected the record data, */

	u32 sync_count_2;	/* sync_count_1 and sync_count_2 should be the

				   same. If not, the record has been updated on

				   the Linux side while VM was collecting the

 nr. of network interfaces being monitored */

 next value is 64-bit aligned, so these */

 4 byte would be padded out by compiler */

 total packets received        */

 total packets transmitted     */

 total bytes received          */

 total bytes transmitted       */

 bad packets received          */

 packet transmit problems      */

 no space in linux buffers     */

 no space available in linux   */

 collisions while transmitting */

/*

 * appldata_get_net_sum_data()

 *

 * gather accumulated network statistics

 EBCDIC "00" */

/*

 * appldata_net_init()

 *

 * init data, register ops

/*

 * appldata_net_exit()

 *

 * unregister ops

 SPDX-License-Identifier: GPL-2.0

/*

 * Base infrastructure for Linux-z/VM Monitor Stream, Stage 1.

 * Exports appldata_register_ops() and appldata_unregister_ops() for the

 * data gathering modules.

 *

 * Copyright IBM Corp. 2003, 2009

 *

 * Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>

#define APPLDATA_CPU_INTERVAL	10000		/* default (CPU) time for

						   sampling interval in

#define TOD_MICRO	0x01000			/* nr. of TOD clock units

/*

 * /proc entries (sysctl)

/*

 * Timer

/*

 * Work queue

/*

 * Ops list

************************** timer, work, DIAG *******************************/

/*

 * appldata_timer_function()

 *

 * schedule work and reschedule timer

/*

 * appldata_work_fn()

 *

 * call data gathering function for each (active) module

 "LINUXKR" */

 "NL" */

 "26" */

 "01" */

/*

 * appldata_diag()

 *

 * prepare parameter list, issue DIAG 0xDC

*********************** timer, work, DIAG <END> ****************************/

***************************** /proc stuff **********************************/

/*

 * __appldata_vtimer_setup()

 *

 * Add, delete or modify virtual timers on all online cpus.

 * The caller needs to get the appldata_timer_lock spinlock.

/*

 * appldata_timer_handler()

 *

 * Start/Stop timer, show status of timer (0 = not active, 1 = active)

/*

 * appldata_interval_handler()

 *

 * Set (CPU) timer interval for collection of data (in milliseconds), show

 * current timer interval.

/*

 * appldata_generic_handler()

 *

 * Generic start/stop monitoring and DIAG, show status of

 * monitoring (0 = not in process, 1 = in process)

 protect this function

 protect work queue callback

 init record

************************** /proc stuff <END> *******************************/

************************ module-ops management *****************************/

/*

 * appldata_register_ops()

 *

 * update ops list, register /proc/sys entries

/*

 * appldata_unregister_ops()

 *

 * update ops list, unregister /proc entries, stop DIAG if necessary

********************* module-ops management <END> **************************/

*************************** suspend / resume *******************************/

 init record

************************ suspend / resume <END> ****************************/

****************************** init / exit *********************************/

/*

 * appldata_init()

 *

 * init timer, register /proc entries

*************************** init / exit <END> ******************************/

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Find the length for the IPL report boot data

	/*

	 * Start from safe_addr to find a free memory area large

	 * enough for the IPL report boot data. This area is used

	 * for ipl_cert_list_addr/ipl_cert_list_size and

	 * early_ipl_comp_list_addr/early_ipl_comp_list_size. It must

	 * not overlap with any component or any certificate.

	/*

	 * Check if there is a IPL report by looking at the copy

	 * of the IPL parameter information block.

	/*

	 * There is an IPL report, to find it load the pointer to the

	 * IPL parameter information block from lowcore and skip past

	 * the IPL parameter list, then align the address to a double

	 * word boundary.

 Walk through the IPL report blocks in the IPL Report list */

	/*

	 * With either the component list or the certificate list

	 * missing the kernel will stay ignorant of secure IPL.

	/*

	 * Copy component and certificate list to a safe area

	 * where the decompressed kernel can find them.

 SPDX-License-Identifier: GPL-2.0

/*

 *    Copyright IBM Corp. 2016

/*

 * The code within this file will be called very early. It may _not_

 * access anything within the bss section, since that is not cleared

 * yet and may contain data (e.g. initrd) that must be saved by other

 * code.

 * For temporary objects the stack (16k) should be used.

			/*

			 * Make sure we stay within one line. Consider that

			 * each facility bit adds up to five characters and

			 * z/VM adds a four character prefix.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SCLP early buffer must stay page-aligned and below 2GB */

 SPDX-License-Identifier: GPL-2.0

 Works only for digits and letters, but small and fast */

/**

 * simple_strtoull - convert a string to an unsigned long long

 * @cp: The start of the string

 * @endp: A pointer to the end of the parsed string will be placed here

 * @base: The number base to use

 SPDX-License-Identifier: GPL-2.0

			/*

			 * 0 == undefined symbol table index (STN_UNDEF),

			 * used for R_390_RELATIVE, only add KASLR offset

/*

 * Merge information from several sources into a single ident_map_size value.

 * "ident_map_size" represents the upper limit of physical memory we may ever

 * reach. It might not be all online memory, but also include standby (offline)

 * memory. "ident_map_size" could be lower then actual standby or even online

 * memory present, due to limiting factors. We should never go above this limit.

 * It is the size of our identity mapping.

 *

 * Consider the following factors:

 * 1. max_physmem_end - end of physical memory online or standby.

 *    Always <= end of the last online memory block (get_mem_detect_end()).

 * 2. CONFIG_MAX_PHYSMEM_BITS - the maximum size of physical memory the

 *    kernel is able to support.

 * 3. "mem=" kernel command line option which limits physical memory usage.

 * 4. OLDMEM_BASE which is a kdump memory limit when the kernel is executed as

 *    crash kernel.

 * 5. "hsa" size which is a memory limit when the kernel is executed during

 *    zfcp/nvme dump.

 vmemmap contains a multiple of PAGES_PER_SECTION struct pages */

 choose kernel address space layout: 4 or 3 levels. */

	/*

	 * forcing modules and vmalloc area under the ultravisor

	 * secure storage limit, so that any vmalloc allocation

	 * we do could be used to back secure guest storage.

 force vmalloc and modules below kasan shadow */

 allow vmalloc area to occupy up to about 1/2 of the rest virtual space left */

 split remaining virtual space between 1:1 mapping & vmemmap array */

 keep vmemmap_start aligned to a top level region table entry */

 vmemmap_start is the future VMEM_MAX_PHYS, make sure it is within MAX_PHYSMEM */

 make sure identity map doesn't overlay with vmemmap */

 make sure vmemmap doesn't overlay with vmalloc area */

/*

 * This function clears the BSS section of the decompressed Linux kernel and NOT the decompressor's.

/*

 * Set vmalloc area size to an 8th of (potential) physical memory

 * size, unless size has been set by kernel command line parameter.

		/*

		 * Save KASLR offset for early dumps, before vmcore_info is set.

		 * Mark as uneven to distinguish from real vmcore_info pointer.

 Clear non-relocated kernel */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 up to 256 storage elements, 1020 subincrements each */

/*

 * To avoid corrupting old kernel memory during dump, find lowest memory

 * chunk possible either right after the kernel end (decompressed kernel) or

 * after initrd (if it is present and there is no hole between the kernel end

 * and initrd)

/*

 * sequential calls to add_mem_detect_block with adjacent memory areas

 * are merged together into single memory block.

 storage configuration */

 fail */

 VM supports up to 8 extends */

 in 1MB blocks */

 SPDX-License-Identifier: GPL-2.0

 symbol entries are in a form "10000 c4 startup\0" */

 reserve 15 bytes for offset/len in symbol+0x1234/0x1234 */

 make sure buf is 0 terminated */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2019

 initial parameter block for tdes mode, copied from libica */

 add entropy */

/*

 * To randomize kernel base address we have to consider several facts:

 * 1. physical online memory might not be continuous and have holes. mem_detect

 *    info contains list of online memory ranges we should consider.

 * 2. we have several memory regions which are occupied and we should not

 *    overlap and destroy them. Currently safe_addr tells us the border below

 *    which all those occupied regions are. We are safe to use anything above

 *    safe_addr.

 * 3. the upper limit might apply as well, even if memory above that limit is

 *    online. Currently those limitations are:

 *    3.1. Limit set by "mem=" kernel command line option

 *    3.2. memory reserved at the end for kasan initialization.

 * 4. kernel base address must be aligned to THREAD_SIZE (kernel stack size).

 *    Which is required for CONFIG_CHECK_STACK. Currently THREAD_SIZE is 4 pages

 *    (16 pages when the kernel is built with kasan enabled)

 * Assumptions:

 * 1. kernel size (including .bss size) and upper memory limit are page aligned.

 * 2. mem_detect memory region start is THREAD_SIZE aligned / end is PAGE_SIZE

 *    aligned (in practice memory configurations granularity on z/VM and LPAR

 *    is 1mb).

 *

 * To guarantee uniform distribution of kernel base address among all suitable

 * addresses we generate random value just once. For that we need to build a

 * continuous range in which every value would be suitable. We can build this

 * range by simply counting all suitable addresses (let's call them positions)

 * which would be valid as kernel base address. To count positions we iterate

 * over online memory ranges. For each range which is big enough for the

 * kernel image we count all suitable addresses we can put the kernel image at

 * that is

 * (end - start - kernel_size) / THREAD_SIZE + 1

 * Two functions count_valid_kernel_positions and position_to_address help

 * to count positions in memory range given and then convert position back

 * to address.

	/*

	 * Avoid putting kernel in the end of physical memory

	 * which kasan will use for shadow memory and early pgtable

	 * mapping allocations.

 we need a value in the range [1, base_pos] inclusive */

 SPDX-License-Identifier: GPL-2.0

 '\0' character position */

 append right after '\0' */

 replace '\0' with space */

 convert arch command line to ascii if necessary */

 copy arch command line */

 append IPL PARM data to the boot command line */

 SPDX-License-Identifier: GPL-2.0

 will be used in arch/s390/kernel/uv.c */

 rc==0x100 means that there is additional data we do not process */

 disable if no prot_virt=1 given on command-line */

 disable if protected guest virtualization is enabled */

 disable if no hardware support */

 disable if kdump */

 disable if stand-alone dump */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Definitions and wrapper functions for kernel decompressor

 *

 * Copyright IBM Corp. 2010

 *

 * Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

/*

 * gzip declarations

	/*

	 * due to 4MB HEAD_SIZE for bzip2

	 * 'decompress_offset + vmlinux.image_size' could be larger than

	 * kernel at final position + its .bss, so take the larger of two

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the SHA512 and SHA38 Secure Hash Algorithm.

 *

 * Copyright IBM Corp. 2007

 * Author(s): Jan Glauber (jang@de.ibm.com)

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the SHA256 and SHA224 Secure Hash Algorithm.

 *

 * s390 Version:

 *   Copyright IBM Corp. 2005, 2011

 *   Author(s): Jan Glauber (jang@de.ibm.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2006, 2015

 * Author(s): Jan Glauber <jan.glauber@de.ibm.com>

 *	      Harald Freudenberger <freude@de.ibm.com>

 * Driver for the s390 pseudo random number generator

/*

 * Any one who considers arithmetical methods of producing random digits is,

 * of course, in a state of sin. -- John von Neumann

 initial parameter block for tdes mode, copied from libica */

** helper functions ***/

/*

 * generate_entropy:

 * This function fills a given buffer with random bytes. The entropy within

 * the random bytes given back is assumed to have at least 50% - meaning

 * a 64 bytes buffer has at least 64 * 8 / 2 = 256 bits of entropy.

 * Within the function the entropy generation is done in junks of 64 bytes.

 * So the caller should also ask for buffer fill in multiples of 64 bytes.

 * The generation of the entropy is based on the assumption that every stckf()

 * invocation produces 0.5 bits of entropy. To accumulate 256 bits of entropy

 * at least 512 stckf() values are needed. The entropy relevant part of the

 * stckf value is bit 51 (counting starts at the left with bit nr 0) so

 * here we use the lower 4 bytes and exor the values into 2k of bufferspace.

 * To be on the save side, if there is ever a problem with stckf() the

 * other half of the page buffer is filled with bytes from urandom via

 * get_random_bytes(), so this function consumes 2k of urandom for each

 * requested 64 bytes output data. Finally the buffer page is condensed into

 * a 64 byte value by hashing with a SHA512 hash.

 8 x 64 bit init values */

 128 bit counter total message bit length */

 allocate one page stckf buffer */

 fill the ebuf in chunks of 64 byte each */

 fill lower 2k with urandom bytes */

 exor upper 2k with 512 stckf values, offset 4 bytes each */

 hash over the filled page */

** tdes functions ***/

 Add the entropy */

 memory allocation, prng_data struct init, mutex init */

 initialize the PRNG, add 128 bits of entropy */

** sha512 functions ***/

 NIST DRBG testvector for Hash Drbg, Sha-512, Count #0 */

 initial seed */

 check working states V and C */

 generate random bytes */

 check against expected data */

 memory allocation, prng_data struct init, mutex init */

 selftest */

 generate initial seed, we need at least  256 + 128 bits entropy. */

		/*

		 * Trng available, so use it. The trng works in chunks of

		 * 32 bytes and produces 100% entropy. So we pull 64 bytes

		 * which gives us 512 bits entropy.

		/*

		 * No trng available, so use the generate_entropy() function.

		 * This function works in 64 byte junks and produces

		 * 50% entropy. So we pull 2*64 bytes which gives us 512 bits

		 * of entropy.

 append the seed by 16 bytes of unique nonce */

 now initial seed of the prno drng */

	/* if fips mode is enabled, generate a first block of random

 We need at least 256 bits of fresh entropy for reseeding */

 trng produces 256 bits entropy in 32 bytes */

 generate_entropy() produces 256 bits entropy in 64 bytes */

 do a reseed of the prno drng with this bytestring */

 reseed needed ? */

 PRNO generate */

 FIPS 140-2 Conditional Self Test */

** file io functions ***/

 lock prng_data struct */

 give mutex free before calling schedule() */

 occopy mutex again */

		/*

		 * we lose some random bytes if an attacker issues

		 * reads < 8 bytes, but we don't care

 PRNG only likes multiples of 8 bytes */

 if the CPU supports PRNG stckf is present too */

		/*

		 * Beside the STCKF the input for the TDES-EDE is the output

		 * of the last operation. We differ here from X9.17 since we

		 * only store one timestamp into the buffer. Padding the whole

		 * buffer with timestamps does not improve security, since

		 * successive stckf have nearly constant offsets.

		 * If an attacker knows the first timestamp it would be

		 * trivial to guess the additional values. One timestamp

		 * is therefore enough and still guarantees unique input values.

		 *

		 * Note: you can still get strict X9.17 conformity by setting

		 * prng_chunk_size to 8 bytes.

 unlock prng_data struct */

 if errorflag is set do nothing and return 'broken pipe' */

 lock prng_data struct */

 give mutex free before calling schedule() */

 occopy mutex again */

 push left over random bytes from the previous read */

 generate one chunk of random bytes into read buf */

 unlock prng_data struct */

** sysfs stuff ***/

 chunksize attribute (ro) */

 counter attribute (ro) */

 errorflag attribute (ro) */

 mode attribute (ro) */

 reseed attribute (w) */

 reseed limit attribute (rw) */

 strength attribute (ro) */

** module init and exit ***/

 check if the CPU has a PRNG */

 check if TRNG subfunction is available */

 choose prng mode */

 check for MSA5 support for PRNO operations */

 SHA512 mode */

 TDES mode */

 SPDX-License-Identifier: GPL-2.0

/*

 * Cryptographic API.

 *

 * s390 implementation of the AES Cipher Algorithm with protected keys.

 *

 * s390 Version:

 *   Copyright IBM Corp. 2017,2020

 *   Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>

 *		Harald Freudenberger <freude@de.ibm.com>

/*

 * Key blobs smaller/bigger than these defines are rejected

 * by the common code even before the individual setkey function

 * is called. As paes can handle different kinds of key blobs

 * and padding is also possible, the limits need to be generous.

	/*

	 * Small keys will be stored in the keybuf. Larger keys are

	 * stored in extra allocated memory. In both cases does

	 * key point to the memory where the key is stored.

	 * The code distinguishes by checking keylen against

	 * sizeof(keybuf). See the two following helper functions.

 clear key value, prepare pkey clear key token in keybuf */

 TOKVER_CLEAR_KEY */

 other key material, let pkey handle this */

 try three times in case of failure */

 Pick the correct function code based on the protected key type */

 Check if the function code is available */

 only use complete blocks */

 combo: aes + ecb + 1 */

 Pick the correct function code based on the protected key type */

 Check if the function code is available */

 only use complete blocks */

 ecb-paes-s390 + 1 */

 Pick the correct function code based on the protected key type */

 Check if the function code is available */

	/*

	 * xts_check_key verifies the key length is not odd and makes

	 * sure that the two keys are not the same. This can be done

	 * on the two protected keys as well

 key + verification pattern */

 key + verification pattern */

 only use complete blocks */

 ecb-paes-s390 + 1 */

 Pick the correct function code based on the protected key type */

 Check if the function code is available */

 only use complete blocks, max. PAGE_SIZE */

	/*

	 * final block may be < AES_BLOCK_SIZE, copy only nbytes

 ecb-paes-s390 + 1 */

 Query available functions for KM, KMC and KMCTR */

 SPDX-License-Identifier: GPL-2.0

/*

 * s390 arch random implementation.

 *

 * Copyright IBM Corp. 2017, 2020

 * Author(s): Harald Freudenberger

 *

 * The s390_arch_random_generate() function may be called from random.c

 * in interrupt context. So this implementation does the best to be very

 * fast. There is a buffer of random data which is asynchronously checked

 * and filled by a workqueue thread.

 * If there are enough bytes in the buffer the s390_arch_random_generate()

 * just delivers these bytes. Otherwise false is returned until the

 * worker thread refills the buffer.

 * The worker fills the rng buffer by pulling fresh entropy from the

 * high quality (but slow) true hardware random generator. This entropy

 * is then spread over the buffer with an pseudo random generator PRNG.

 * As the arch_get_random_seed_long() fetches 8 bytes and the calling

 * function add_interrupt_randomness() counts this as 1 bit entropy the

 * distribution needs to make sure there is in fact 1 bit entropy contained

 * in 8 bytes of the buffer. The current values pull 32 byte entropy

 * and scatter this into a 2048 byte buffer. So 8 byte in the buffer

 * will contain 1 bit of entropy.

 * The worker thread is rescheduled based on the charge level of the

 * buffer but at least with 500 ms delay to avoid too much CPU consumption.

 * So the max. amount of rng data delivered via arch_get_random_seed is

 * limited to 4k bytes per second.

 max hunk is ARCH_RNG_BUF_SIZE */

 lock rng buffer */

 try to resolve the requested amount of bytes from the buffer */

 not enough bytes in rng buffer, refill is done asynchronously */

 buffer is exhausted and needs refill */

 fetch ARCH_PRNG_SEED_SIZE bytes of entropy */

 blow this entropy up to ARCH_RNG_BUF_SIZE with PRNG */

 kick next check */

/*

 * Here follows the implementation of s390_arch_get_random_long().

 *

 * The random longs to be pulled by arch_get_random_long() are

 * prepared in an 4K buffer which is filled from the NIST 800-90

 * compliant s390 drbg. By default the random long buffer is refilled

 * 256 times before the drbg itself needs a reseed. The reseed of the

 * drbg is done with 32 bytes fetched from the high quality (but slow)

 * trng which is assumed to deliver 100% entropy. So the 32 * 8 = 256

 * bits of entropy are spread over 256 * 4KB = 1MB serving 131072

 * arch_get_random_long() invocations before reseeded.

 *

 * How often the 4K random long buffer is refilled with the drbg

 * before the drbg is reseeded can be adjusted. There is a module

 * parameter 's390_arch_rnd_long_drbg_reseed' accessible via

 *   /sys/module/arch_random/parameters/rndlong_drbg_reseed

 * or as kernel command line parameter

 *   arch_random.rndlong_drbg_reseed=<value>

 * This parameter tells how often the drbg fills the 4K buffer before

 * it is re-seeded by fresh entropy from the trng.

 * A value of 16 results in reseeding the drbg at every 16 * 4 KB = 64

 * KB with 32 bytes of fresh entropy pulled from the trng. So a value

 * of 16 would result in 256 bits entropy per 64 KB.

 * A value of 256 results in 1MB of drbg output before a reseed of the

 * drbg is done. So this would spread the 256 bits of entropy among 1MB.

 * Setting this parameter to 0 forces the reseed to take place every

 * time the 4K buffer is depleted, so the entropy rises to 256 bits

 * entropy per 4K or 0.5 bit entropy per arch_get_random_long().  With

 * setting this parameter to negative values all this effort is

 * disabled, arch_get_random long() returns false and thus indicating

 * that the arch_get_random_long() feature is disabled at all.

 need to re-seed the drbg */

 fetch seed from trng */

 seed drbg */

 re-init counter for drbg */

 fill the arch_get_random_long buffer from drbg */

 arch_get_random_long() disabled ? */

 try to lock the random long lock */

 deliver next long value from the buffer */

 buffer is depleted and needs refill */

 delay refill in interrupt context to next caller */

 refill random long buffer */

 and provide one random long */

 all the needed PRNO subfunctions available ? */

 alloc arch random working buffer */

 kick worker queue job to fill the random buffer */

 enable arch random to the outside world */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the SHA256 and SHA224 Secure Hash Algorithm.

 *

 * s390 Version:

 *   Copyright IBM Corp. 2019

 *   Author(s): Joerg Schmidbauer (jschmidb@de.ibm.com)

 = 32 */

 same as for 256 */

 function code different! */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the SHA1 Secure Hash Algorithm.

 *

 * Derived from cryptoapi implementation, adapted for in-place

 * scatterlist interface.  Originally based on the public domain

 * implementation written by Steve Reid.

 *

 * s390 Version:

 *   Copyright IBM Corp. 2003, 2007

 *   Author(s): Thomas Spatzier

 *		Jan Glauber (jan.glauber@de.ibm.com)

 *

 * Derived from "crypto/sha1_generic.c"

 *   Copyright (c) Alan Smithee.

 *   Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>

 *   Copyright (c) Jean-Francois Dive <jef@linuxbe.org>

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 generic implementation of the SHA Secure Hash Algorithms.

 *

 * Copyright IBM Corp. 2007

 * Author(s): Jan Glauber (jang@de.ibm.com)

 how much is already in the buffer? */

 process one stored block */

 process as many blocks as possible */

 set total msg bit length (mbl) in CPACF parmblock */

		/*

		 * the SHA512 parmblock has a 128-bit mbl field, clear

		 * high-order u64 field, copy bits to low-order u64 field

 copy digest to out */

 wipe context */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the DES Cipher Algorithm.

 *

 * Copyright IBM Corp. 2003, 2011

 * Author(s): Thomas Spatzier

 *	      Jan Glauber (jan.glauber@de.ibm.com)

 only use complete blocks */

 only use complete blocks */

 combo: des + ecb */

 combo: des + cbc */

/*

 * RFC2451:

 *

 *   For DES-EDE3, there is no known need to reject weak or

 *   complementation keys.  Any weakness is obviated by the use of

 *   multiple keys.

 *

 *   However, if the first two or last two independent 64-bit keys are

 *   equal (k1 == k2 or k2 == k3), then the DES3 operation is simply the

 *   same as DES.  Implementers MUST reject keys that exhibit this

 *   property.

 *

 *   In fips mode additinally check for all 3 keys are unique.

 *

 combo: des3 + ecb */

 combo: des3 + cbc */

 align to block size, max. PAGE_SIZE */

 final block may be < DES_BLOCK_SIZE, copy only nbytes */

 combo: des + ctr */

 combo: des3 + ede */

 Query available functions for KM, KMC and KMCTR */

 SPDX-License-Identifier: GPL-2.0

/*

 * Cryptographic API.

 *

 * s390 implementation of the GHASH algorithm for GCM (Galois/Counter Mode).

 *

 * Copyright IBM Corp. 2011

 * Author(s): Gerald Schaefer <gerald.schaefer@de.ibm.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Crypto-API module for CRC-32 algorithms implemented with the

 * z/Architecture Vector Extension Facility.

 *

 * Copyright IBM Corp. 2015

 * Author(s): Hendrik Brueckner <brueckner@linux.vnet.ibm.com>

 Prototypes for functions in assembly files */

/*

 * DEFINE_CRC32_VX() - Define a CRC-32 function using the vector extension

 *

 * Creates a function to perform a particular CRC-32 computation. Depending

 * on the message buffer, the hardware-accelerated or software implementation

 * is used.   Note that the message buffer is aligned to improve fetch

 * operations of VECTOR LOAD MULTIPLE instructions.

 *

	/*

	 * Perform a final XOR with 0xFFFFFFFF to be in sync

	 * with the generic crc32c shash implementation.

	/*

	 * Perform a final XOR with 0xFFFFFFFF to be in sync

	 * with the generic crc32c shash implementation.

 CRC-32 LE */

 CRC-32 BE */

 CRC-32C LE */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the AES Cipher Algorithm.

 *

 * s390 Version:

 *   Copyright IBM Corp. 2005, 2017

 *   Author(s): Jan Glauber (jang@de.ibm.com)

 *		Sebastian Siewior (sebastian@breakpoint.cc> SW-Fallback

 *		Patrick Steuer <patrick.steuer@de.ibm.com>

 *		Harald Freudenberger <freude@de.ibm.com>

 *

 * Derived from "crypto/aes_generic.c"

 Pick the correct function code based on the key length */

 Check if the function code is available */

 Pick the correct function code based on the key length */

 Check if the function code is available */

 only use complete blocks */

 combo: aes + ecb + 1 */

 Pick the correct function code based on the key length */

 Check if the function code is available */

 only use complete blocks */

 ecb-aes-s390 + 1 */

 In fips mode only 128 bit or 256 bit keys are valid */

 Pick the correct function code based on the key length */

 Check if the function code is available */

 Split the XTS key into the two subkeys */

 only use complete blocks */

 ecb-aes-s390 + 1 */

 Pick the correct function code based on the key length */

 Check if the function code is available */

 only use complete blocks, max. PAGE_SIZE */

	/*

	 * final block may be < AES_BLOCK_SIZE, copy only nbytes

 ecb-aes-s390 + 1 */

 reserved */

 Counter Value */

 Tag */

 Hash-subkey */

 Total AAD Length */

 Total Plain-/Cipher-text Length */

 initial counter value */

 Key */

	/*

	 * encrypt

	 *   req->src: aad||plaintext

	 *   req->dst: aad||ciphertext||tag

	 * decrypt

	 *   req->src: aad||ciphertext||tag

	 *   req->dst: aad||plaintext, return 0 or -EBADMSG

	 * aad, plaintext and ciphertext may be empty.

 Query available functions for KM, KMC, KMCTR and KMA */

 SPDX-License-Identifier: GPL-2.0+

/*

 * Cryptographic API.

 *

 * s390 implementation of the SHA512 and SHA384 Secure Hash Algorithm.

 *

 * Copyright IBM Corp. 2019

 * Author(s): Joerg Schmidbauer (jschmidb@de.ibm.com)

 same as for 512 */

 function code different! */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2020

 *

 * Author(s):

 *   Niklas Schnelle <schnelle@linux.ibm.com>

 *

 Linux' vfid's start at 0 vfn at 1 */

 Linux' vfid's start at 0 vfn at 1*/

	/* If the parent PF for the given VF is also configured in the

	 * instance, it must be on the same zbus.

	 * We can then identify the parent PF by checking what

	 * devfn the VF would have if it belonged to that PF using the PF's

	 * stride and offset. Only if this candidate devfn matches the

	 * actual devfn will we link both functions.

 balance pci_get_slot() */

 balance pci_get_slot() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2012

 *

 * Author(s):

 *   Jan Glauber <jang@linux.vnet.ibm.com>

 *

 * The System z PCI code is a rewrite from a prototype by

 * the following people (Kudoz!):

 *   Alexander Schmidt

 *   Christoph Raisch

 *   Hannes Hering

 *   Hoang-Nam Nguyen

 *   Jan-Bernd Themann

 *   Stefan Roscher

 *   Thomas Klein

 list of all detected zpci devices */

 Modify PCI: Register I/O address translation parameters */

 Modify PCI: Unregister I/O address translation parameters */

 Modify PCI: Set PCI function measurement parameters */

 reset software counters */

 Modify PCI: Disable PCI function measurement */

 Function measurement is disabled if fmb address is zero */

 Function already gone. */

 combine single writes by using store-block insn */

 Create a virtual mapping cookie for a PCI BAR */

 Detect overrun */

 Detect underrun */

 only MMIO is supported */

 The pdev has a reference to the zdev via its bus */

	/*

	 * We can always auto allocate domains below ZPCI_NR_DEVICES.

	 * There is either a free domain or we have reached the maximum in

	 * which case we would have bailed earlier.

 Function is already disabled - update handle */

/**

 * zpci_hot_reset_device - perform a reset of the given zPCI function

 * @zdev: the slot which should be reset

 *

 * Performs a low level reset of the zPCI function. The reset is low level in

 * the sense that the zPCI function can be reset without detaching it from the

 * common PCI subsystem. The reset may be performed while under control of

 * either DMA or IOMMU APIs in which case the existing DMA/IOMMU translation

 * table is reinstated at the end of the reset.

 *

 * After the reset the functions internal state is reset to an initial state

 * equivalent to its state during boot when first probing a driver.

 * Consequently after reset the PCI function requires re-initialization via the

 * common PCI code including re-enabling IRQs via pci_alloc_irq_vectors()

 * and enabling the function via e.g.pci_enablde_device_flags().The caller

 * must guard against concurrent reset attempts.

 *

 * In most cases this function should not be called directly but through

 * pci_reset_function() or pci_reset_bus() which handle the save/restore and

 * locking.

 *

 * Return: 0 on success and an error value otherwise

 Disables device access, DMAs and IRQs (reset state) */

		/*

		 * Due to a z/VM vs LPAR inconsistency in the error state the

		 * FH may indicate an enabled device but disable says the

		 * device is already disabled don't treat it as an error here.

/**

 * zpci_create_device() - Create a new zpci_dev and add it to the zbus

 * @fid: Function ID of the device to be created

 * @fh: Current Function Handle of the device to be created

 * @state: Initial state after creation either Standby or Configured

 *

 * Creates a new zpci device and adds it to its, possibly newly created, zbus

 * as well as zpci_list.

 *

 * Returns: the zdev on success or an error pointer otherwise

 FID and Function Handle are the static/dynamic identifiers */

 Query function properties and update zdev */

/**

 * zpci_scan_configured_device() - Scan a freshly configured zpci_dev

 * @zdev: The zpci_dev to be configured

 * @fh: The general function handle supplied by the platform

 *

 * Given a device in the configuration state Configured, enables, scans and

 * adds it to the common code PCI subsystem if possible. If the PCI device is

 * parked because we can not yet create a PCI bus because we have not seen

 * function 0, it is ignored but will be scanned once function 0 appears.

 * If any failure occurs, the zpci_dev is left disabled.

 *

 * Return: 0 on success, or an error code otherwise

 the PCI function will be scanned once function 0 appears */

	/* For function 0 on a multi-function bus scan whole bus as we might

	 * have to pick up existing functions waiting for it to allow creating

	 * the PCI bus

/**

 * zpci_deconfigure_device() - Deconfigure a zpci_dev

 * @zdev: The zpci_dev to configure

 *

 * Deconfigure a zPCI function that is currently configured and possibly known

 * to the common code PCI subsystem.

 * If any failure occurs the device is left as is.

 *

 * Return: 0 on success, or an error code otherwise

/**

 * zpci_device_reserved() - Mark device as resverved

 * @zdev: the zpci_dev that was reserved

 *

 * Handle the case that a given zPCI function was reserved by another system.

 * After a call to this function the zpci_dev can not be found via

 * get_zdev_by_fid() anymore but may still be accessible via existing

 * references though it will not be functional anymore.

	/*

	 * Remove device from zpci_list as it is going away. This also

	 * makes sure we ignore subsequent zPCI events for this device.

/**

 * zpci_clear_error_state() - Clears the zPCI error state of the device

 * @zdev: The zdev for which the zPCI error state should be reset

 *

 * Clear the zPCI error state of the device. If clearing the zPCI error state

 * fails the device is left in the error state. In this case it may make sense

 * to call zpci_io_perm_failure() on the associated pdev if it exists.

 *

 * Returns: 0 on success, -EIO otherwise

/**

 * zpci_reset_load_store_blocked() - Re-enables L/S from error state

 * @zdev: The zdev for which to unblock load/store access

 *

 * Re-enables load/store access for a PCI function in the error state while

 * keeping DMA blocked. In this state drivers can poke MMIO space to determine

 * if error recovery is possible while catching any rogue DMA access from the

 * device.

 *

 * Returns: 0 on success, -EIO otherwise

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2012

 *

 * Author(s):

 *   Jan Glauber <jang@linux.vnet.ibm.com>

	/* Can't use device_remove_self() here as that would lead us to lock

	 * the pci_rescan_remove_lock while holding the device' kernfs lock.

	 * This would create a possible deadlock with disable_slot() which is

	 * not directly protected by the device' kernfs lock but takes it

	 * during the device removal which happens under

	 * pci_rescan_remove_lock.

	 *

	 * This is analogous to sdev_store_delete() in

	 * drivers/scsi/scsi_sysfs.c

	/* device_remove_file() serializes concurrent calls ignoring all but

	 * the first

	/* A concurrent call to recover_store() may slip between

	 * sysfs_break_active_protection() and the sysfs file removal.

	 * Once it unblocks from pci_lock_rescan_remove() the original pdev

	 * will already be removed.

			/*

			 * Due to a z/VM vs LPAR inconsistency in the error

			 * state the FH may indicate an enabled device but

			 * disable says the device is already disabled don't

			 * treat it as an error here.

 analogous to smbios index */

 SPDX-License-Identifier: GPL-2.0

/*

 * Access to PCI I/O memory from user space programs.

 *

 * Copyright IBM Corp. 2014

 * Author(s): Alexey Ishchuk <aishchuk@linux.vnet.ibm.com>

	/*

	 * copy 0 < @len <= 8 bytes from @src into the right most bytes of

	 * a register, then store it to PCI at @ioaddr while in secondary

	 * address space. pcistg then uses the user mappings.

 did we read everything from user memory? */

 main path */

	/*

	 * We only support write access to MIO capable devices if we are on

	 * a MIO enabled system. Otherwise we would have to check for every

	 * address if it is a special ZPCI_ADDR and would have to do

	 * a pfn lookup which we don't need for MIO capable devices.  Currently

	 * ISM devices are the only devices without MIO support and there is no

	 * known need for accessing these from userspace.

	/*

	 * read 0 < @len <= 8 bytes from the PCI memory mapped at @ioaddr (in

	 * user space) into a register using pcilg then store these bytes at

	 * user address @dst

 did we write everything to the user space buffer? */

	/*

	 * We only support read access to MIO capable devices if we are on

	 * a MIO enabled system. Otherwise we would have to check for every

	 * address if it is a special ZPCI_ADDR and would have to do

	 * a pfn lookup which we don't need for MIO capable devices.  Currently

	 * ISM devices are the only devices without MIO support and there is no

	 * known need for accessing these from userspace.

 SPDX-License-Identifier: GPL-2.0

/*

 * s390 specific pci instructions

 *

 * Copyright IBM Corp. 2013

 1 microsecond */

 Modify PCI Function Controls */

 Refresh PCI Translations */

 Set Interruption Controls */

 PCI Load */

 PCI Store */

 PCI Store Block */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2012

 *

 * Author(s):

 *   Jan Glauber <jang@linux.vnet.ibm.com>

/*

 * Call Logical Processor with c=1, lps=0 and command 1

 * to get the bit mask of installed logical processors

/*

 * Call Logical Processor with c=0, the give constant lps and an lpcb request.

/**

 * clp_set_pci_fn() - Execute a command on a PCI function

 * @zdev: Function that will be affected

 * @fh: Out parameter for updated function handle

 * @nr_dma_as: DMA address space number

 * @command: The command code to execute

 *

 * Returns: 0 on success, < 0 for Linux errors (e.g. -ENOMEM), and

 * > 0 for non-success platform responses

 store as many entries as possible */

 Get PCI function handle list */

/*

 * Get the current function handle of the function matching @fid

 store logical-processor characteristics */

 store logical-processor characteristics */

 list PCI functions */

 query PCI function */

 query PCI function group */

 Command code 0: test for a specific processor */

 Command code 1: return bit mask of installed processors */

 SPDX-License-Identifier: GPL-2.0

/*

 * summary bit vector

 * FLOATING - summary bit per function

 * DIRECTED - summary bit per cpu (only used in fallback path)

/*

 * interrupt bit vectors

 * FLOATING - interrupt bit vector per function

 * DIRECTED - interrupt bit vector per cpu

 Modify PCI: Register floating adapter interruptions */

 enable summary notifications */

 each zdev has its own interrupt vector */

 Modify PCI: Unregister floating adapter interruptions */

 Function already gone or IRQs already deregistered. */

 Modify PCI: Register CPU directed interruptions */

 Modify PCI: Unregister CPU directed interruptions */

 Function already gone or IRQs already deregistered. */

 Register adapter interruptions */

 Clear adapter interruptions */

 Scan the directed IRQ bit vector */

 End of second scan with interrupts on. */

 First scan complete, reenable interrupts. */

 End of second scan with interrupts on. */

 First scan complete, reenable interrupts. */

 Scan adapter summary indicator bit vector */

 End of second scan with interrupts on. */

 First scan complete, reenable interrupts. */

 Scan the adapter interrupt vector for this device. */

 Allocate cpu vector bits */

 Allocate adapter summary indicator bit */

 Create adapter interrupt vector */

 Wire up shortcut pointer */

 Each function has its own interrupt vector */

 Request MSI interrupts */

 Disable interrupts */

 Release MSI interrupts */

		/*

		 * Per CPU IRQ vectors look the same but bit-allocation

		 * is only done on the first vector.

 Set summary to 1 to be called every time for the ISC. */

	/*

	 * Enable floating IRQs (with suppression after one IRQ). When using

	 * directed IRQs this enables the fallback path.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2020

 *

 * Author(s):

 *   Pierre Morel <pmorel@linux.ibm.com>

 *

/* zpci_bus_prepare_device - Prepare a zPCI function for scanning

 * @zdev: the zPCI function to be prepared

 *

 * The PCI resources for the function are set up and added to its zbus and the

 * function is enabled. The function must be added to a zbus which must have

 * a PCI bus created. If an error occurs the zPCI function is not enabled.

 *

 * Return: 0 on success, an error code otherwise

/* zpci_bus_scan_device - Scan a single device adding it to the PCI core

 * @zdev: the zdev to be scanned

 *

 * Scans the PCI function making it available to the common PCI code.

 *

 * Return: 0 on success, an error value otherwise

/* zpci_bus_remove_device - Removes the given zdev from the PCI core

 * @zdev: the zdev to be removed from the PCI core

 * @set_error: if true the device's error state is set to permanent failure

 *

 * Sets a zPCI device to a configured but offline state; the zPCI

 * device is still accessible through its hotplug slot and the zPCI

 * API but is removed from the common code PCI bus, making it

 * no longer available to drivers.

 balance pci_get_slot */

 balance pci_get_slot */

/* zpci_bus_scan_bus - Scan all configured zPCI functions on the bus

 * @zbus: the zbus to be scanned

 *

 * Enables and scans all PCI functions on the bus making them available to the

 * common PCI code. If there is no function 0 on the zbus nothing is scanned. If

 * a function does not have a slot yet because it was added to the zbus before

 * function 0 the slot is created. If a PCI function fails to be initialized

 * an error will be returned but attempts will still be made for all other

 * functions on the bus.

 *

 * Return: 0 on success, an error value otherwise

/* zpci_bus_scan_busses - Scan all registered busses

 *

 * Scan all available zbusses

 *

/* zpci_bus_create_pci_bus - Create the PCI bus associated with this zbus

 * @zbus: the zbus holding the zdevices

 * @f0: function 0 of the bus

 * @ops: the pci operations

 *

 * Function zero is taken as a parameter as this is used to determine the

 * domain, multifunction property and maximum bus speed of the entire bus.

 *

 * Return: 0 on success, an error code otherwise

	/*

	 * Note that the zbus->resources are taken over and zbus->resources

	 * is empty after a successful call

	/*

	 * With pdev->no_vf_scan the common PCI probing code does not

	 * perform PF/VF linking.

/* zpci_bus_create_hotplug_slots - Add hotplug slot(s) for device added to bus

 * @zdev: the zPCI device that was newly added

 *

 * Add the hotplug slot(s) for the newly added PCI function. Normally this is

 * simply the slot for the function itself. If however we are adding the

 * function 0 on a zbus, it might be that we already registered functions on

 * that zbus but could not create their hotplug slots yet so add those now too.

 *

 * Return: 0 on success, an error code otherwise

		/* Now that function 0 is there we can finally create the

		 * hotplug slots for those functions with devfn != 0 that have

		 * been parked in zbus->function[] waiting for us to be able to

		 * create the PCI bus.

 Hotplug slot will be created once function 0 appears */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright IBM Corp. 2012

 *

 *  Author(s):

 *    Jan Glauber <jang@linux.vnet.ibm.com>

 Content Code Description for PCI Function Error */

 function handle */

 function id */

 expected table type */

 MSI vector number */

 DMA address space */

 event qualifier */

 read/write */

 failing address */

 PCI event code */

 Content Code Description for PCI Function Availability */

 function handle */

 function id */

 PCI event code */

 Let's try a full reset instead */

 Let's try a full reset instead */

/* zpci_event_attempt_error_recovery - Try to recover the given PCI function

 * @pdev: PCI function to recover currently in the error state

 *

 * We follow the scheme outlined in Documentation/PCI/pci-error-recovery.rst.

 * With the simplification that recovery always happens per function

 * and the platform determines which functions are affected for

 * multi-function devices.

	/*

	 * Ensure that the PCI function is not removed concurrently, no driver

	 * is unbound or probed and that userspace can't access its

	 * configuration space while we perform recovery.

/* zpci_event_io_failure - Report PCI channel failure state to driver

 * @pdev: PCI function for which to report

 * @es: PCI channel failure state to report

	/**

	 * While vfio-pci's error_detected callback notifies user-space QEMU

	 * reacts to this by freezing the guest. In an s390 environment PCI

	 * errors are rarely fatal so this is overkill. Instead in the future

	 * we will inject the error event and let the guest recover the device

	 * itself.

 Service Action or Error Recovery Successful */

		/*

		 * Mark as frozen not permanently failed because the device

		 * could be subsequently recovered by the platform.

	/* Give the driver a hint that the function is

	 * already unusable.

	/* Even though the device is already gone we still

	 * need to free zPCI resources as part of the disable.

 Reserved|Standby -> Configured */

 the configuration request may be stale */

 Reserved -> Standby */

 Deconfiguration requested */

			/* The event may have been queued before we confirgured

			 * the device.

 Configured -> Standby|Reserved */

			/* The event may have been queued before we confirgured

			 * the device.:

 The 0x0304 event may immediately reserve the device */

 0x308 or 0x302 for multiple devices */

 Standby -> Reserved */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright IBM Corp. 2012

 *

 * Author(s):

 *   Jan Glauber <jang@linux.vnet.ibm.com>

	/*

	 * With zdev->tlb_refresh == 0, rpcit is not required to establish new

	 * translations when previously invalid translation-table entries are

	 * validated. With lazy unmap, rpcit is skipped for previously valid

	 * entries, but a global rpcit is then required before any address can

	 * be re-used, i.e. after each iommu bitmap wrap-around.

 enable the hypervisor to free some resources */

 global flush before DMA addresses are reused */

 wrap-around */

 This rounds up number of pages based on size and offset */

 Use rounded up size */

 Map a segment into a contiguous dma address area */

	/*

	 * At this point, if the device is part of an IOMMU domain, this would

	 * be a strong hint towards a bug in the IOMMU API (common) code and/or

	 * simultaneous access via IOMMU and DMA API. So let's issue a warning.

	/*

	 * Restrict the iommu bitmap size to the minimum of the following:

	 * - s390_iommu_aperture which defaults to high_memory

	 * - 3-level pagetable address limit minus start_dma offset

	 * - DMA address range allowed by the hardware (clp query pci fn)

	 *

	 * Also set zdev->end_dma to the actual end address of the usable

	 * range, instead of the theoretical maximum as reported by hardware.

	 *

	 * This limits the number of concurrently usable DMA mappings since

	 * for each DMA mapped memory address we need a DMA address including

	 * extra DMA addresses for multiple mappings of the same memory address.

	/*

	 * At this point, if the device is part of an IOMMU domain, this would

	 * be a strong hint towards a bug in the IOMMU API (common) code and/or

	 * simultaneous access via IOMMU and DMA API. So let's issue a warning.

	/*

	 * cc == 3 indicates the function is gone already. This can happen

	 * if the function was deconfigured/disabled suddenly and we have not

	 * received a new handle yet.

 dma_supported is unconditionally true without a callback */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright IBM Corp. 2012,2015

 *

 *  Author(s):

 *    Jan Glauber <jang@linux.vnet.ibm.com>

 header */

 event trace buffer */

 error log */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * We need to ensure that shared mappings are correctly aligned to

 * avoid aliasing issues with VIPT caches.  We need to ensure that

 * a specific page of an object is always mapped at a multiple of

 * SHMLBA bytes.

 *

 * We unconditionally provide this function for all cases.

	/*

	 * We only need to do colour alignment if either the I or D

	 * caches alias.

	/*

	 * We enforce the MAP_FIXED case.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * Get byte-value from addr and set it to *valp.

 *

 * Success: return 0

 * Failure: return 1

/*

 * Put byte-value to addr.

 *

 * Success: return 0

 * Failure: return 1

/*

 * Get half-word from [rx + imm]

 *

 * Success: return 0

 * Failure: return 1

/*

 * Store half-word to [rx + imm]

 *

 * Success: return 0

 * Failure: return 1

/*

 * Get word from [rx + imm]

 *

 * Success: return 0

 * Failure: return 1

/*

 * Store word to [rx + imm]

 *

 * Success: return 0

 * Failure: return 1

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 for L1-cache */

 for L2-cache */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 for L1-cache */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * One C-SKY MMU TLB entry contain two PFN/page entry, ie:

 * 1VPN -> 2PFN

/*

 * MMU operation regs only could invalid tlb entry in jtlb and we

 * need change asid field to invalid I-utlb & D-utlb.

 SPDX-License-Identifier: GPL-2.0

/*

 * Generic ASID allocator.

 *

 * Based on arch/arm/mm/context.c

 *

 * Copyright (C) 2002-2003 Deep Blue Solutions Ltd, all rights reserved.

 * Copyright (C) 2012 ARM Ltd.

 Update the list of reserved ASIDs and the ASID bitmap. */

		/*

		 * If this CPU has already been through a

		 * rollover, but hasn't run another task in

		 * the meantime, we must preserve its reserved

		 * ASID, as this is the only trace we have of

		 * the process it is still running.

	/*

	 * Queue a TLB invalidation for each CPU to perform on next

	 * context-switch

	/*

	 * Iterate over the set of reserved ASIDs looking for a match.

	 * If we find one, then we can update our mm to use newasid

	 * (i.e. the same ASID in the current generation) but we can't

	 * exit the loop early, since we need to ensure that all copies

	 * of the old ASID are updated to reflect the mm. Failure to do

	 * so could result in us missing the reserved ASID in a future

	 * generation.

		/*

		 * If our current ASID was active during a rollover, we

		 * can continue to use it and this was just a false alarm.

		/*

		 * We had a valid ASID in a previous life, so try to re-use

		 * it if possible.

	/*

	 * Allocate a free ASID. If we can't find one, take a note of the

	 * currently active ASIDs and mark the TLBs as requiring flushes.  We

	 * always count from ASID #2 (index 1), as we use ASID #0 when setting

	 * a reserved TTBR0 for the init_mm and we allocate ASIDs in even/odd

	 * pairs.

 We're out of ASIDs, so increment the global generation count */

 We have more ASIDs than CPUs, so this will always succeed */

/*

 * Generate a new ASID for the context.

 *

 * @pasid: Pointer to the current ASID batch allocated. It will be updated

 * with the new ASID batch.

 * @cpu: current CPU ID. Must have been acquired through get_cpu()

 Check that our ASID belongs to the current generation. */

/*

 * Initialize the ASID allocator

 *

 * @info: Pointer to the asid allocator structure

 * @bits: Number of ASIDs available

 * @asid_per_ctxt: Number of ASIDs to allocate per-context. ASIDs are

 * allocated contiguously for a given context. This value should be a power of

 * 2.

	/*

	 * Expect allocation after rollover to fail if we don't have at least

	 * one more ASID than CPUs. ASID #0 is always reserved.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 FIXME not sure about */

 Setup page mask to 4k */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 Are we prepared to handle this kernel fault? */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

		/*

		 * We ran out of memory, call the OOM killer, and return the userspace

		 * (which will retry the fault, or kill us if we got oom-killed).

 Kernel mode? Handle exceptions or die */

	/*

	 * Something tried to access memory that isn't in our memory map.

	 * Fix it, but check if it's kernel or user first.

 User mode accesses just cause a SIGSEGV */

 User mode accesses just cause a SIGSEGV */

	/*

	 * Synchronize this task's top level page-table

	 * with the 'reference' page table.

	 *

	 * Do _not_ use "tsk" here. We might be inside

	 * an interrupt in the middle of a task switch..

/*

 * This routine handles page faults.  It determines the address and the

 * problem, and then passes it off to one of the appropriate routines.

	/*

	 * Fault-in kernel-space virtual memory on-demand.

	 * The 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

 Enable interrupts if they were enabled in the parent context. */

	/*

	 * If we're in an interrupt, have no user context, or are running

	 * in an atomic region, then we must not take the fault.

	/*

	 * Ok, we have a good vm_area for this memory access, so

	 * we can handle it.

	/*

	 * If for any reason at all we could not handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

	/*

	 * If we need to retry but a fatal signal is pending, handle the

	 * signal first. We do not need to release the mmap_lock because it

	 * would already be released in __lock_page_or_retry in mm/filemap.c.

		/*

		 * No need to mmap_read_unlock(mm) as we would

		 * have already released it in __lock_page_or_retry

		 * in mm/filemap.c.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * Some archs flush debug and FPU info here

 setup thread.sp for switch_to !!! */

 Fill in the fpu structure for a core dump.  */

 NOTE: usp is error value. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * for abiv1 the 64bits args should be even th, So we need mov the advice

 * forward.

 SPDX-License-Identifier: GPL-2.0

 task blocked in __switch_to */

 Validate frame pointer */

 Unwind stack frame */

 !CONFIG_FRAME_POINTER */

 task blocked in __switch_to */

 CONFIG_FRAME_POINTER */

/*

 * Save stack-backtrace addresses into a stack_trace buffer.

 CONFIG_STACKTRACE */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 Defined in entry.S */

 Defined in head.S */

 setup trap0 trap2 trap3 */

 setup MMU TLB exception */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 Clean up bss section */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * Gcc-csky with -pg will insert stub in function prologue:

 *	push	lr

 *	jbsr	_mcount

 *	nop32

 *	nop32

 *

 * If the (callee - current_pc) is less then 64MB, we'll use bsr:

 *	push	lr

 *	bsr	_mcount

 *	nop32

 *	nop32

 * else we'll use (movih + ori + jsr):

 *	push	lr

 *	movih	r26, ...

 *	ori	r26, ...

 *	jsr	r26

 *

 * (r26 is our reserved link-reg)

 *

 CONFIG_DYNAMIC_FTRACE */

		/*

		 * For csky-gcc function has sub-call:

		 * subi	sp,	sp, 8

		 * stw	r8,	(sp, 0)

		 * mov	r8,	sp

		 * st.w r15,	(sp, 0x4)

		 * push	r15

		 * jl	_mcount

		 * We only need set *parent for resume

		 *

		 * For csky-gcc function has no sub-call:

		 * subi	sp,	sp, 4

		 * stw	r8,	(sp, 0)

		 * mov	r8,	sp

		 * push	r15

		 * jl	_mcount

		 * We need set *parent and *(frame_pointer + 4) for resume,

		 * because lr is resumed twice.

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_FUNCTION_GRAPH_TRACER */

 CONFIG_DYNAMIC_FTRACE */

 _mcount is defined in abi's mcount.S */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 PMU Control reg */

 Start PC reg */

 End PC reg */

 Soft Counter reg */

 Count Enable reg */

 Interrupt Enable reg */

 Interrupt Status reg */

 The events for a given PMU register set. */

	/*

	 * The events that are active on the PMU for the given index.

	/*

	 * A 1 bit for an index indicates that the counter is being used for

	 * an event. A 0 means that the counter can be used.

 cycle counter */

 instruction counter */

 l1 icache access counter */

 l1 icache miss counter */

 l1 dcache access counter */

 l1 dcache miss counter */

 l2 cache access counter */

 l2 cache miss counter */

 I-UTLB miss counter */

 D-UTLB miss counter */

 JTLB miss counter */

 software counter */

 conditional branch mispredict counter */

 conditional branch instruction counter */

 indirect branch mispredict counter */

 indirect branch instruction counter */

 LSU spec fail counter */

 store instruction counter */

 dcache read access counter */

 dcache read miss counter */

 dcache write access counter */

 dcache write miss counter */

 l2cache read access counter */

 l2cache read miss counter */

 l2cache write access counter */

 l2cache write miss counter */

	/*

	 * The hw event starts counting from this event offset,

	 * mark it to be able to extract future "deltas":

	/*

	 * Sign extend count value to 64bit, otherwise delta calculation

	 * would be incorrect when overflow occurs.

	/*

	 * We aren't afraid of hwc->prev_count changing beneath our feet

	 * because there's no way for us to re-enter this function anytime.

 starts all counters */

 stops all counters */

 allocate hardware counter and optionally start counting */

	/*

	 * Did an overflow occur?

	/*

	 * Handle the counter(s) overflow(s)

 Ignore if we don't have an event. */

		/*

		 * We have a single interrupt for all counters. Check that

		 * each counter has overflowed before we process it.

	/*

	 * Handle the pending perf events.

	 *

	 * Note: this call *must* be run with interrupts disabled. For

	 * platforms that can have the PMU interrupts raised as an NMI, this

	 * will not work.

 Ensure the PMU has sane values out of reset. */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Because other CPUs are in reset status, we must flush data

	 * from cache to out and secondary CPUs use them in

	 * csky_start_secondary(void)

 Enable cpu in SMP reset ctrl reg */

 Wait for the cpu online */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * pad[3] is compatible with the same struct defined in

	 * gcc/libgcc/config/csky/linux-unwind.h

 sc_pt_regs is structured the same as the start of pt_regs */

 BIT(0) of regs->sr is Condition Code/Carry bit */

 Restore the floating-point state. */

 Always make any pending restarted system calls return -EINTR */

 Default to using normal stack */

	/*

	 * If we are on the alternate signal stack and would overflow it, don't.

	 * Return an always-bogus address instead so we will die with SIGSEGV.

 This is the X/Open sanctioned signal stack switching. */

 Align the stack frame. */

 Create the ucontext. */

 Set up to return from userspace. */

	/*

	 * Set up registers for signal handler.

	 * Registers that we don't modify keep the value they had from

	 * user-space at the time we took the signal.

	 * We always pass siginfo and mcontext, regardless of SA_SIGINFO,

	 * since some things rely on this (e.g. glibc's debug/segfault.c).

 a0: signal number */

 a1: siginfo pointer */

 a2: ucontext pointer */

 Are we from a system call? */

 Avoid additional syscall restarting via ret_from_exception */

 If so, check system call restarting.. */

 Set up the stack frame */

 Actually deliver the signal */

 Did we come from a system call? */

 Avoid additional syscall restarting via ret_from_exception */

 Restart the system call - no handlers present */

	/*

	 * If there is no signal to deliver, we just put the saved

	 * sigmask back.

/*

 * notification of userspace execution resumption

 * - triggered by the _TIF_WORK_MASK flags

 Handle pending signal delivery */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 read processor id, max is 100 */

 some CPU only has one id reg */

 cpid index is 31-28, reset */

 CPU feature regs, setup by bootloader or gdbinit */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 sets the trace bits. */

/*

 * Make sure the single step bit is not set.

 Enable irq */

 Disable irq */

/*

 * Make sure the single step bit is set.

 Abiv1 regs->tls is fake and we need sync here. */

 BIT(0) of regs.sr is Condition Code/Carry bit */

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_within_kernel_stack() - check the address in the stack

 * @regs:      pt_regs which contains kernel stack pointer.

 * @addr:      address which is checked.

 *

 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).

 * If @addr is within the kernel stack, it returns true. If not, returns false.

/**

 * regs_get_kernel_stack_nth() - get Nth entry of the stack

 * @regs:	pt_regs which contains kernel stack pointer.

 * @n:		stack entry number.

 *

 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which

 * is specified by @regs. If the @n th entry is NOT in the kernel stack,

 * this returns 0.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 jsri 0x...  --> lrw r26, 0x... */

 lsli r0, r0 --> jsr r26 */

 This is where to make the change */

 We add the value into the location given */

 Add the value, subtract its postition */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * The vDSO data page.

	/*

	 * Put vDSO base into mm struct. We need to do this before calling

	 * install_special_mapping or the perf counter mmap tracking code

	 * will fail to recognise it as a vDSO (since arch_vma_name fails).

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Hangzhou C-SKY Microsystems co.,ltd.

 Kernel callchain */

/*

 * Get the return address for a single stackframe and return a pointer to the

 * next frame tail.

 Check accessibility of one struct frame_tail beyond */

/*

 * This will be called when the target is in user mode

 * This function will only be called when we use

 * "PERF_SAMPLE_CALLCHAIN" in

 * kernel/events/core.c:perf_prepare_sample()

 *

 * How to trigger perf_callchain_[user/kernel] :

 * $ perf record -e cpu-clock --call-graph fp ./program

 * $ perf report --call-graph

 *

 * On C-SKY platform, the program being sampled and the C library

 * need to be compiled with * -mbacktrace, otherwise the user

 * stack will not contain function frame.

 C-SKY does not support virtualization. */

	/*

	 * While backtrace from leaf function, lr is normally

	 * not saved inside frame on C-SKY, so get lr from pt_regs

	 * at the sample point. However, lr value can be incorrect if

	 * lr is used as temp register

 C-SKY does not support virtualization. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 offsets into the task struct */

 offsets into the thread struct */

 offsets into the thread_info struct */

 offsets into the pt_regs */

 offsets into the irq_cpustat_t struct */

 signal defines */

 SPDX-License-Identifier: GPL-2.0+

/* Return:

 *   INSN_REJECTED     If instruction is one not allowed to kprobe,

 *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.

 SPDX-License-Identifier: GPL-2.0

 Ftrace callback handler for kprobes -- called under preepmt disabled */

			/*

			 * Emulate singlestep (and also recover regs->pc)

			 * as if there is a nop

		/*

		 * If pre_handler returns !0, it changes regs->pc. We have to

		 * skip emulating post_handler.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014-2016 Pratyush Anand <panand@redhat.com>

	/*

	 * Task has received a fatal signal, so reset back to probbed

	 * address.

 SPDX-License-Identifier: GPL-2.0+

 copy instruction */

 decode instruction */

 insn not supported */

 insn need simulation */

 instruction uses slot */

 prepare the instruction */

 install breakpoint in text */

 remove breakpoint from text */

/*

 * Interrupts need to be disabled before single-step mode is set, and not

 * reenabled until after single-step mode ends.

 * Without disabling interrupt on local CPU, there is a chance of

 * interrupt occurrence in the period of exception return and  start of

 * out-of-line single-step, that result in wrongly single stepping

 * into the interrupt handler.

 prepare for single stepping */

 mark pending ss */

 IRQs and single stepping do not mix well. */

 insn simulation */

 return addr restore if non-branching insn */

 restore back original saved kprobe variables and continue */

 call post handler */

		/* post_handler can hit breakpoint and single step

		 * again, so we enable D-flag for recursive exception.

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe and the ip points back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * In case the user-specified fault handler returned

		 * zero, try to fix up.

 Probe hit */

			/*

			 * If we have no pre-handler or it returned 0, we

			 * continue with normal processing.  If we have a

			 * pre-handler and it returned non-zero, it will

			 * modify the execution path and no need to single

			 * stepping. Let's just reset current kprobe and exit.

			 *

			 * pre_handler can hit a breakpoint and can step thru

			 * before return.

	/*

	 * The breakpoint instruction was removed right

	 * after we hit it.  Another cpu has removed

	 * either a probepoint or a debugger breakpoint

	 * at this address.  In either case, no further

	 * handling of this interrupt is appropriate.

	 * Return back to original instruction, and continue.

 clear pending ss */

/*

 * Provide a blacklist of symbols identifying ranges which cannot be kprobed.

 * This blacklist is exposed to userspace via debugfs (kprobes/blacklist).

 SPDX-License-Identifier: GPL-2.0+

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 4W */

 1W */

 1B */

/*

 * __clear_user: - Zero a block of memory in user space, with less checking.

 * @to:   Destination address, in user space.

 * @n:    Number of bytes to zero.

 *

 * Zero a block of memory in user space.  Caller must check

 * the specified block with access_ok() before calling this function.

 *

 * Returns number of bytes that could not be cleared.

 * On success, this will be zero.

 4W */

 1W */

 1B */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

/*

 * fpu_libc_helper() is to help libc to excute:

 *  - mfcr %a, cr<1, 2>

 *  - mfcr %a, cr<2, 2>

 *  - mtcr %a, cr<1, 2>

 *  - mtcr %a, cr<2, 2>

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.

		/*

		 * Ensure the remote hart's writes are visible to this hart.

		 * This pairs with a barrier in flush_icache_mm.

 Mark every hart's icache as needing a flush for this MM. */

 Flush this hart's I$ now, and mark it as flushed. */

	/*

	 * Flush the I$ of other harts concurrently executing, and mark them as

	 * flushed.

 SPDX-License-Identifier: GPL-2.0+

/*

 *	DMA support for Broadcom SiByte platforms.

 *

 *	Copyright (c) 2018  Maciej W. Rozycki

 SPDX-License-Identifier: GPL-2.0

 XXXKW can/should this ever happen? */

 XXXKW think about interaction with 'console=' cmdline arg */

 If none of the console options are configured, the build will break. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2001, 2002, 2003 Broadcom Corporation

 * Copyright (C) 2007 Ralf Baechle <ralf@linux-mips.org>

 * Copyright (C) 2007 MIPS Technologies, Inc.

 *    written by Ralf Baechle <ralf@linux-mips.org>

 ioctls */

/*

 * Routines for using 40-bit SCD cycle counter

 *

 * Client responsible for either handling interrupts or making sure

 * the cycles counter never saturates, e.g., by doing

 * zclk_timer_init(0) at least every 2^40 - 1 ZCLKs.

/*

 * Configures SCD counter 0 to count ZCLKs starting from val;

 * Configures SCD counters1,2,3 to count nothing.

 * Must not be called while gathering ZBbus profiles.

 SCD perf_cnt_cfg */ \

 write val to counter0 */ \

 config counter0 for zclks*/ \

 no outputs */ \

 enable, counter0 */ \

 inputs */ "r"(val), "r" ((1ULL << 33) | 1ULL) \

 modifies */ "$8" )

/* Reads SCD counter 0 and puts result in value

 SCD perf_cnt_cfg */ \

 write val to counter0 */ \

 outputs */ "=r"(val) \

 inputs */ \

 modifies */ "$8" )

/*

 * Support for ZBbus sampling using the trace buffer

 *

 * We use the SCD performance counter interrupt, caused by a Zclk counter

 * overflow, to trigger the start of tracing.

 *

 * We set the trace buffer to sample everything and freeze on

 * overflow.

 *

 * We map the interrupt for trace_buffer_freeze to handle it on CPU 0.

 *

	/*

	 * Generate an SCD_PERFCNT interrupt in TB_PERIOD Zclks to

	 * trigger start of trace.  XXX vary sampling period

	/*

	 * Unfortunately, in Pass 2 we must clear all counters to knock down

	 * a previous interrupt request.  This means that bus profiling

	 * requires ALL of the SCD perf counters.

 keep counters 0,2,3,4,5,6,7 as is */

 counter 1 counts cycles */

 enable counting */

 clear all counters */

 counter 1 counts cycles */

 keep counters 0,2,3 as is */

 enable counting */

 clear all counters */

 counter 1 counts cycles */

 Reset the trace buffer */

 XXXKW may want to expose control to the data-collector */

 XXX should use XKPHYS to make writes bypass L2 */

 Read out trace */

 Loop runs backwards because bundles are read out in reverse order */

 Subscripts decrease to put bundle in the order */

   t0 lo, t0 hi, t1 lo, t1 hi, t2 lo, t2 hi */

 read t2 hi */

 read t2 lo */

 read t1 hi */

 read t1 lo */

 read t0 hi */

 read t0 lo */

 knock down current interrupt and get another one later */

 No more trace buffer samples */

/*

 * Requires: Already called zclk_timer_init with a value that won't

 *	     saturate 40 bits.	No subsequent use of SCD performance counters

 *	     or trace buffer.

 Make sure there isn't a perf-cnt interrupt waiting */

 Disable and clear counters, override SRC_1 */

	/*

	 * We grab this interrupt to prevent others from trying to use

	 * it, even though we don't want to service the interrupts

	 * (they only feed into the trace-on-interrupt mechanism)

	/*

	 * I need the core to mask these, but the interrupt mapper to

	 *  pass them through.	I am exploiting my knowledge that

	 *  cp0_status masks out IP[5]. krw

 Initialize address traps */

 Initialize Trace Event 0-7 */

				when interrupt	*/

 Initialize Trace Sequence 0-7 */

				     Start on event 0 (interrupt) */

			  dsamp when d used | asamp when a used */

 Now indicate the PERF_CNT interrupt as a trace-relevant interrupt */

		/*

		 * XXXKW there is a window here where the intr handler may run,

		 * see the disable, and do the wake_up before this sleep

		 * happens.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2002,2003 Broadcom Corporation

/*

 * The Bus Watcher monitors internal bus transactions and maintains

 * counts of transactions with error status, logging details and

 * causing one of several interrupts.  This driver provides a handler

 * for those interrupts which aggregates the counts (to avoid

 * saturating the 8-bit counters) and provides a presence in

 * /proc/bus_watcher if PROC_FS is on.

/*

 * check_bus_watcher is exported for use in situations where we want

 * to see the most recent status of the bus watcher, which might have

 * already been destructively read out of the registers.

 *

 * notes: this is currently used by the cache error handler

 *	  should provide locking against the interrupt handler

 Use non-destructive register */

 Use non-destructive register */

 Same as 1250 except BUS_ERR_STATUS_DEBUG is in a different place. */

/* For simplicity, I want to assume a single read is required each

	/* XXXKW indicate multiple errors between printings, or stats

 CONFIG_PROC_FS */

/*

 * sibyte_bw_int - handle bus watcher interrupts and accumulate counts

 *

 * notes: possible re-entry due to multiple sources

 *	  should check/indicate saturation

 Destructive read, clears register and interrupt */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001, 2002, 2003 Broadcom Corporation

 Max ram addressable in 32-bit segments */

 Don't repeat the process from another CPU */

 Get CPU 0 to do the cfe_exit */

 regardless of PHYS_ADDR_T_64BIT */

 INITRD */

			/*

			 * See if this block contains (any portion of) the

			 * ramdisk

				/*

				 * memcpy/__copy_user prefetch, which

				 * will cause a bus error for

				 * KSEG/KUSEG addrs not backed by RAM.

				 * Hence, reserve some padding for the

				 * prefetch distance.

				/*

				 * Too many regions.  Need to configure more

 Make a copy of the initrd argument so we can smash it up here */

	/*

	 *Initrd location comes in the form "<hex size of ramdisk in bytes>@<location in memory>"

	 *  e.g. initrd=3abfd@80010000.	 This is set up by the loader.

/*

 * prom_init is called just after the cpu type is determined, from setup_arch()

	/*

	 * Check if a loader was used; if NOT, the 4 arguments are

	 * what CFE gives us (handle, 0, EPT and EPTSEAL)

			/*

			 * Old loader; all it gives us is the handle,

			 * so use the "known" entrypoint and assume

			 * the seal.

			/*

			 * Newer loaders bundle the handle/ept/eptseal

			 * Note: prom_vec is in the loader's useg

			 * which is still alive in the TLB.

 too early for panic to do any good */

	/*

	 * Get the handle for (at least) prom_putchar, possibly for

	 * boot console

 The loader should have set the command line */

 too early for panic to do any good */

		/* Need to find out early whether we've got an initrd.	So scan

 CONFIG_BLK_DEV_INITRD */

 Not sure this is needed, but it's the safe way. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001, 2002, 2003 Broadcom Corporation

 XXXKW don't overload PASS defines? */

 XXXKW different war_pass? */

 Early build didn't have revid set */

 Setup code likely to be common to all SiByte platforms */

 Pass 2 - easiest as default for now - so many numbers */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001 Broadcom Corporation

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001, 2002, 2003 Broadcom Corporation

/*

 * SMP init and finish on secondary CPUs

 Set interrupt mask, but don't enable */

/*

 * These are routines for dealing with the sb1250 smp capabilities

 * independent of board/firmware

/*

 * Simple enough; everything is set up, so just poke the appropriate mailbox

 * register, and we should be set

/*

 * Code to run on secondary just after probing the CPU

/*

 * Do any tidying up before marking online and running the idle

 * loop

/*

 * Setup the PC, SP, and GP of a secondary processor and start it

 * running!

/*

 * Use CFE to find out how many CPUs are available, setting up

 * cpu_possible_mask and the logical/physical mappings.

 * XXXKW will the boot CPU ever not be physical 0?

 *

 * Common setup before any secondaries are started

 Load the mailbox register to figure out what we're supposed to do */

 Clear the mailbox to clear the interrupt */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001, 2002, 2003 Broadcom Corporation

/*

 * These are the routines that handle all the low level interrupt stuff.

 * Actions handled here are: initialization of the interrupt map, requesting of

 * interrupt lines by handlers, dispatching if interrupts to handlers, probing

 * for interrupt lines

 Store the CPU id (not the logical number) */

 Convert logical CPU to physical CPU */

 Protect against other affinity changers and IMR manipulation */

 Swizzle each CPU's IMR (but leave the IP selection alone) */

 If it was on, mask it */

 unmask for the new CPU */

	/*

	 * If the interrupt was an HT interrupt, now is the time to

	 * clear it.  NOTE: we assume the HT bridge was set up to

	 * deliver the interrupts to all CPUs (which makes affinity

	 * changing easier for us)

			/*

			 * Clear for all CPUs so an affinity switch

			 * doesn't find an old status

		/*

		 * Generate EOI.  For Pass 1 parts, EOI is a nop.  For

		 * Pass 2, the LDT world may be edge-triggered, but

		 * this EOI shouldn't hurt.  If they are

		 * level-sensitive, the EOI is required.

/*

 *  arch_init_irq is called early in the boot sequence from init/main.c via

 *  init_IRQ.  It is responsible for setting up the interrupt mapper and

 *  installing the handler that will be responsible for dispatching interrupts

 *  to the "right" place.

/*

 * For now, map all interrupts to IP[2].  We could save

 * some cycles by parceling out system interrupts to different

 * IP lines, but keep it simple for bringup.  We'll also direct

 * all interrupts to a single CPU; we should probably route

 * PCI and LDT to one cpu and everything else to the other

 * to balance the load a bit.

 *

 * On the second cpu, everything is set to IP5, which is

 * ignored, EXCEPT the mailbox interrupt.  That one is

 * set to IP[2] so it is handled.  This is needed so we

 * can do cross-cpu function calls, as required by SMP

 Default everything to IP2 */

 was I0 */

	/*

	 * Map the high 16 bits of the mailbox registers to IP[3], for

	 * inter-cpu messages

 Was I1 */

 Clear the mailboxes.	 The firmware may leave them dirty */

 Mask everything except the mailbox registers for both cpus */

	/*

	 * Note that the timer interrupts are also mapped, but this is

	 * done in sb1250_time_init().	Also, the profiling driver

	 * does its own management of IP7.

 Enable necessary IPs, disable the rest */

	/*

	 * Default...we've hit an IP[2] interrupt, which means we've got to

	 * check the 1250 interrupt registers to figure out what to do.	 Need

	 * to detect which CPU we're on, now that smp_affinity is supported.

	/*

	 * What a pain. We have to be really careful saving the upper 32 bits

	 * of any * register across function calls if we don't want them

	 * trashed--since were running in -o32, the calling routing never saves

	 * the full 64 bits of a register across a function call.  Being the

	 * interrupt handler, we're guaranteed that interrupts are disabled

	 * during this code so we don't have to worry about random interrupts

	 * blasting the high 32 bits.

 CPU performance counter interrupt */

 sb1250_timer_interrupt() */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001, 2002, 2003, 2004 Broadcom Corporation

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * Setup code for the SWARM board

 Data bus error - print PA */

 XXXKW for CFE, get lines/cols from environment */

 XXXKW need to detect Monterey/LittleSur/etc */

 LEDS_PHYS */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001 Broadcom Corporation

 *

 * Copyright (C) 2002 MontaVista Software Inc.

 * Author: jsun@mvista.com or jsun@junsun.net

 Xicor 1241 definitions */

/*

 * Register bits

 currently on battery power */

 r/w latch is enabled, can write RTC */

 r/w latch is unlocked, can enable r/w now */

 clock failed */

 block protect 2 */

 block protect 1 */

 block protect 0 */

 military time format */

/*

 * Register numbers

 block protect bits */

  */

 Seconds */

 Minutes */

 Hours */

 Day of month */

 Month */

 Year */

 Day of Week */

 Year 2K */

 Status register */

 Clear error bit by writing a 1 */

 Clear error bit by writing a 1 */

 unlock writes to the CCR */

 trivial ones */

 tm_mon starts from 0, *ick* */

 year is split */

 hour is the most tricky one */

 24 hour format */

 12 hour format, with 0x2 for pm */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001 Broadcom Corporation

 *

 * Copyright (C) 2002 MontaVista Software Inc.

 * Author: jsun@mvista.com or jsun@junsun.net

 M41T81 definitions */

/*

 * Register bits

 stop bit */

 century bit */

 century enable bit */

 sign bit */

 frequency test bit */

 output level */

 watchdog resolution bit 0 */

 watchdog resolution bit 1 */

 watchdog multiplier bit 0 */

 watchdog multiplier bit 1 */

 watchdog multiplier bit 2 */

 watchdog multiplier bit 3 */

 watchdog multiplier bit 4 */

 alarm in "battery back-up mode" enable bit */

 square wave enable */

 alarm flag enable flag */

 alarm repeat mode bit 5 */

 alarm repeat mode bit 4 */

 alarm repeat mode bit 3 */

 halt update bit */

 alarm repeat mode bit 2 */

 alarm repeat mode bit 1 */

 alarm flag (read only) */

 watchdog flag (read only) */

 sqw frequency bit 0 */

 sqw frequency bit 1 */

 sqw frequency bit 2 */

 sqw frequency bit 3 */

/*

 * Register numbers

 tenths/hundredths of second */

 seconds */

 minute */

 hour/century */

 day of week */

 date of month */

 month */

 year */

 control */

 watchdog */

 alarm: month */

 alarm: date */

 alarm: hour */

 alarm: minute */

 alarm: second */

 flags */

 square wave register */

 Clear error bit by writing a 1 */

 Clear error bit by writing a 1 */

 read the same byte again to make sure it is written */

 Note we don't care about the century */

	/*

	 * Note the write order matters as it ensures the correctness.

	 * When we write sec, 10th sec is clear.  It is reasonable to

	 * believe we should finish writing min within a second.

 tm_wday starts from 0 to 6 */

 tm_mon starts from 0, *ick* */

 we don't do century, everything is beyond 2000 */

	/*

	 * min is valid if two reads of sec are the same.

 enable chip if it is not enabled yet */

 SPDX-License-Identifier: GPL-2.0

 grumble */

 defined(CONFIG_SIBYTE_SWARM) || defined(CONFIG_SIBYTE_LITTLESUR) */

 Set the number of available units based on the SOC type.  */

 Hybrid */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Broadcom BCM91250A (SWARM), etc. I2C platform setup.

 *

 *	Copyright (c) 2008  Maciej W. Rozycki

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000,2001,2002,2003,2004 Broadcom Corporation

 Setup code likely to be common to all SiByte platforms */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000,2001,2004 Broadcom Corporation

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001,2002,2004 Broadcom Corporation

/*

 * These are routines for dealing with the bcm1480 smp capabilities

 * independent of board/firmware

/*

 * SMP init and finish on secondary CPUs

 Set interrupt mask, but don't enable */

/*

 * These are routines for dealing with the sb1250 smp capabilities

 * independent of board/firmware

/*

 * Simple enough; everything is set up, so just poke the appropriate mailbox

 * register, and we should be set

/*

 * Code to run on secondary just after probing the CPU

/*

 * Do any tidying up before marking online and running the idle

 * loop

/*

 * Setup the PC, SP, and GP of a secondary processor and start it

 * running!

/*

 * Use CFE to find out how many CPUs are available, setting up

 * cpu_possible_mask and the logical/physical mappings.

 * XXXKW will the boot CPU ever not be physical 0?

 *

 * Common setup before any secondaries are started

 Load the mailbox register to figure out what we're supposed to do */

 Clear the mailbox to clear the interrupt */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000,2001,2002,2003,2004 Broadcom Corporation

/*

 * These are the routines that handle all the low level interrupt stuff.

 * Actions handled here are: initialization of the interrupt map, requesting of

 * interrupt lines by handlers, dispatching if interrupts to handlers, probing

 * for interrupt lines

 Store the CPU id (not the logical number) */

 Convert logical CPU to physical CPU */

 Protect against other affinity changers and IMR manipulation */

 Swizzle each CPU's IMR (but leave the IP selection alone) */

 Loop through high and low interrupt mask register */

 If it was on, mask it */

 unmask for the new CPU */

****************************************************************************/

	/*

	 * If the interrupt was an HT interrupt, now is the time to

	 * clear it.  NOTE: we assume the HT bridge was set up to

	 * deliver the interrupts to all CPUs (which makes affinity

	 * changing easier for us)

 Loop through high and low LDT interrupts */

				/*

				 * Clear for all CPUs so an affinity switch

				 * doesn't find an old status

			/*

			 * Generate EOI.  For Pass 1 parts, EOI is a nop.  For

			 * Pass 2, the LDT world may be edge-triggered, but

			 * this EOI shouldn't hurt.  If they are

			 * level-sensitive, the EOI is required.

/*

 *  init_IRQ is called early in the boot sequence from init/main.c.  It

 *  is responsible for setting up the interrupt mapper and installing the

 *  handler that will be responsible for dispatching interrupts to the

 *  "right" place.

/*

 * For now, map all interrupts to IP[2].  We could save

 * some cycles by parceling out system interrupts to different

 * IP lines, but keep it simple for bringup.  We'll also direct

 * all interrupts to a single CPU; we should probably route

 * PCI and LDT to one cpu and everything else to the other

 * to balance the load a bit.

 *

 * On the second cpu, everything is set to IP5, which is

 * ignored, EXCEPT the mailbox interrupt.  That one is

 * set to IP[2] so it is handled.  This is needed so we

 * can do cross-cpu function calls, as required by SMP

 Default everything to IP2 */

 Start with _high registers which has no bit 0 interrupt source */

 was I0 */

 Now do _low registers */

	/*

	 * Map the high 16 bits of mailbox_0 registers to IP[3], for

	 * inter-cpu messages

 Was I1 */

 Clear the mailboxes.	 The firmware may leave them dirty */

 Mask everything except the high 16 bit of mailbox_0 registers for all cpus */

	/*

	 * Note that the timer interrupts are also mapped, but this is

	 * done in bcm1480_time_init().	 Also, the profiling driver

	 * does its own management of IP7.

 Enable necessary IPs, disable the rest */

	/*

	 * Default...we've hit an IP[2] interrupt, which means we've got to

	 * check the 1480 interrupt registers to figure out what to do.	 Need

	 * to detect which CPU we're on, now that smp_affinity is supported.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011 Zhang, Keguang <keguang.zhang@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011 Zhang, Keguang <keguang.zhang@gmail.com>

 *

 * Modified from arch/mips/pnx833x/common/prom.c.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2014 Zhang, Keguang <keguang.zhang@gmail.com>

	/*

	 * Although our caller may have the read side of xtime_lock,

	 * this is now a seqlock, and we are cheating in this routine

	 * by having side effects on state that we cannot undo if

	 * there is a collision on the seqlock and our caller has to

	 * retry.  (Namely, old_jifs and old_count.)  So we must treat

	 * jiffies as volatile despite the lock.  We read jiffies

	 * before latching the timer count to guarantee that although

	 * the jiffies value might be older than the count (that is,

	 * the counter may underflow between the last point where

	 * jiffies was incremented and the point where we latch the

	 * count), it cannot be newer.

 read the count */

	/*

	 * It's possible for count to appear to go the wrong way for this

	 * reason:

	 *

	 *  The timer counter underflows, but we haven't handled the resulting

	 *  interrupt and incremented jiffies yet.

	 *

	 * Previous attempts to handle these cases intelligently were buggy, so

	 * we just do the simple thing now.

 CONFIG_CEVT_CSRC_LS1X */

 initialize LS1X clocks */

 setup LS1X PWM timer */

 setup mips r4k timer */

 CONFIG_CEVT_CSRC_LS1X */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011-2016 Zhang, Keguang <keguang.zhang@gmail.com>

 8250/16550 compatible UART */

 CPUFreq */

 Synopsys Ethernet GMAC */

 CONFIG_LOONGSON1_LS1B */

 GPIO */

 USB EHCI */

 Real Time Clock */

 Watchdog */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011 Zhang, Keguang <keguang.zhang@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011 Zhang, Keguang <keguang.zhang@gmail.com>

 Get pending sources, masked by current enables */

 INT0 */

 INT1 */

 INT2 */

 INT3 */

 INT4 */

	/* Disable interrupts and clear pending,

	 * setup all IRQs as high level triggered

 set DMA0, DMA1 and DMA2 to edge trigger */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2011-2016 Zhang, Keguang <keguang.zhang@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (c) 2016 Yang Ling <gnaygnil@gmail.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * setup.c - boot time setup code

 just jump to the reset vector */

 Enable PCI interrupts in EPLD Mask register */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  RouterBoard 500 specific prom routines

 *

 *  Copyright (C) 2003, Peter Sadik <peter.sadik@idt.com>

 *  Copyright (C) 2005-2006, P.Christeas <p_christ@hol.gr>

 *  Copyright (C) 2007, Gabor Juhos <juhosg@openwrt.org>

 *			Felix Fietkau <nbd@openwrt.org>

 *			Florian Fainelli <florian@openwrt.org>

		/* Note: it is common that parameters start

		 * at argv[1] and not argv[0],

 parses out the "mem=xx" arg */

	/* give all RAM to boot allocator,

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  RouterBoard 500 Platform devices

 *

 *  Copyright (C) 2006 Felix Fietkau <nbd@openwrt.org>

 *  Copyright (C) 2007 Florian Fainelli <florian@openwrt.org>

 149 */

 Resources and device for NAND */

 NAND definitions */

 Setup NAND specific settings */

 Look for the CF card reader */

 disable cf_slot0 at index 2 */

 Read the NAND resources from the device controller */

 Read and map device controller 3 */

 Initialise the NAND device */

 set the uart clock to the current cpu frequency */

 CONFIG_NET */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 *  Setting up the clock on the MIPS boards.

/*

 * Figure out the r4k offset, the amount to increment the compare

 * register for each time tick. There is no RTC available.

 *

 * The RC32434 counts at half the CPU *core* speed.

 round */

/*

 *  Miscellaneous functions for IDT EB434 board

 *

 *  Copyright 2004 IDT Inc. (rischelp@idt.com)

 *  Copyright 2006 Phil Sutter <n0-1@freewrt.org>

 *  Copyright 2007 Florian Fainelli <florian@openwrt.org>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/* rb532_set_bit - sanely set a bit

 *

 * bitval: new value for the bit

 * offset: bit index in the 4 byte address range

 * ioaddr: 4 byte aligned address being altered

 unset bit if bitval == 0 */

 set bit if bitval == 1 */

/* rb532_get_bit - read a bit

 *

 * returns the boolean state of the bit, which may be > 1

/*

/*

 * Set output GPIO level

/*

 * Set GPIO direction to input

 disable alternate function in case it's set */

/*

 * Set GPIO direction to output

 disable alternate function in case it's set */

 set the initial output value */

/*

 * Set GPIO interrupt level

/*

 * Set GPIO interrupt status

/*

 * Configure GPIO alternate function

 Register our GPIO chip */

/*

 *  BRIEF MODULE DESCRIPTION

 *     Serial port initialisation.

 *

 *  Copyright 2004 IDT Inc. (rischelp@idt.com)

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/*

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 *

 * Copyright 2002 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *		stevel@mvista.com or source@mvista.com

 mask of valid bits in pending/mask registers */

 There is a maximum of 14 GPIO interrupts */

		/*

		 * if there are no more interrupts enabled in this

		 * group, disable corresponding IP

 Main Interrupt dispatcher */

 only unmasked interrupts */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * double precision: MIN{,A}.f

 * MIN : Scalar Floating-Point Minimum

 * MINA: Scalar Floating-Point argument with Minimum Absolute Value

 *

 * MIN.D : FPR[fd] = minNum(FPR[fs],FPR[ft])

 * MINA.D: FPR[fd] = maxNumMag(FPR[fs],FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare signs */

 Signs of inputs are equal, let's compare exponents */

 Inputs are both positive */

 Inputs are both negative */

 Signs and exponents of inputs are equal, let's compare mantissas */

 Inputs are both positive, with equal signs and exponents */

 Inputs are both negative, with equal signs and exponents */

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare exponent */

 Compare mantissa */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * double precision: MIN{,A}.f

 * MIN : Scalar Floating-Point Minimum

 * MINA: Scalar Floating-Point argument with Minimum Absolute Value

 *

 * MIN.D : FPR[fd] = minNum(FPR[fs],FPR[ft])

 * MINA.D: FPR[fd] = maxNumMag(FPR[fs],FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare signs */

 Signs of inputs are the same, let's compare exponents */

 Inputs are both positive */

 Inputs are both negative */

 Signs and exponents of inputs are equal, let's compare mantissas */

 Inputs are both positive, with equal signs and exponents */

 Inputs are both negative, with equal signs and exponents */

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare exponent */

 Compare mantissa */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * single precision: MADDF.f (Fused Multiply Add)

 * MADDF.fmt: FPR[fd] = FPR[fd] + (FPR[fs] x FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * Handle the cases when at least one of x, y or z is a NaN.

	 * Order of precedence is sNaN, qNaN and z, x, y.

 ZERO z cases are handled separately below */

	/*

	 * Infinity handling

			/*

			 * Cases of addition of infinities with opposite signs

			 * or subtraction of infinities with same signs.

		/*

		 * z is here either not an infinity, or an infinity having the

		 * same sign as product (x*y). The result must be an infinity,

		 * and its sign is determined only by the sign of product (x*y).

 Handle cases +0 + (-0) and similar ones. */

				/*

				 * Cases of addition of zeros of equal signs

				 * or subtraction of zeroes of opposite signs.

				 * The sign of the resulting zero is in any

				 * such case determined only by the sign of z.

 x*y is here 0, and z is not 0, so just return z */

 continue to real computations */

 Finally get to do some computation */

	/*

	 * Do the multiplication bit first

	 *

	 * rm = xm * ym, re = xe + ye basically

	 *

	 * At this point xm and ym should have been normalized.

 rm = xm * ym, re = xe+ye basically */

 Multiple 24 bit xm and ym to give 48 bit results */

 Shunt to top of word */

 Put explicit bit at bit 62 if necessary */

		/*

		 * Move explicit bit from bit 62 to bit 26 since the

		 * ieee754sp_format code expects the mantissa to be

		 * 27 bits wide (24 + 3 rounding bits).

 Move explicit bit from bit 23 to bit 62 */

 Make the exponents the same */

		/*

		 * Have to shift r fraction right to align.

		/*

		 * Have to shift z fraction right to align.

 Do the addition */

		/*

		 * Generate 64 bit result by adding two 63 bit numbers

		 * leaving result in zm64, zs and ze.

 carry out */

		/*

		 * Put explicit bit at bit 62 if necessary.

	/*

	 * Move explicit bit from bit 62 to bit 26 since the

	 * ieee754sp_format code expects the mantissa to be

	 * 27 bits wide (24 + 3 rounding bits).

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 * Copyright (C) 2017 Imagination Technologies, Ltd.

 * Author: Aleksandar Markovic <aleksandar.markovic@imgtec.com>

 <-- DP needed for 64-bit mantissa tmp */

 toward nearest */

 toward zero */

 toward +infinity */

 toward -infinity */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision square root

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 take care of Inf and NaN */

 x == INF or NAN? */

 sqrt(Nan) = Nan */

 sqrt(0) = 0 */

 sqrt(-Inf) = Nan */

 sqrt(+Inf) = Inf */

 sqrt(-x) = Nan */

 normalize x */

 subnormal x */

 unbias exponent */

 odd m, double x to make it even */

 m = [m/2] */

 generate sqrt(x) bit by bit */

 q = sqrt(x) */

 r = moving bit from right to left */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

/*

 * Raise the Invalid Operation IEEE 754 exception

 * and convert the signaling NaN supplied to a quiet NaN.

	/* inexact must round of 3 bits

 xm += (xm&0x8)?0x4:0x3 */

 toward +Infinity */

 ?? */

 toward -Infinity */

 ?? */

/* generate a normal/denormal number with over,under handling

 * sn is sign

 * xe is an unbiased exponent

 * xm is 3bit extended precision value.

 we don't gen exact zeros (probably should) */

 no excess */

 strip lower bits */

 toward +Infinity */

 toward -Infinity */

 Not tiny after rounding */

 Clear grs bits */

			/* sticky right shift es bits

		/* inexact must round of 3 bits

		/* adjust exponent for rounding add overflowing

 add causes mantissa overflow */

 strip grs bits */

 no excess */

 -O can be table indexed by (rm,sn) */

 toward +Infinity */

 toward -Infinity */

 gen norm/denorm/zero */

 we underflow (tiny/zero) */

 no excess */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

 quick fix up */

 flip sign of y and handle as add */

 provide guard,round and stick bit space */

		/*

		 * have to shift y fraction right to align

		/*

		 * have to shift x fraction right to align

		/* generate 28 bit result of adding two 27 bit numbers

 carry out */

 shift preserving sticky */

 round negative inf. => sign = -1 */

 other round modes   => sign = 1 */

		/* normalize to rounding precision

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 max neg can't be safely negated */

 normalize - result can never be inexact or overflow */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * single precision: MIN{,A}.f

 * MIN : Scalar Floating-Point Minimum

 * MINA: Scalar Floating-Point argument with Minimum Absolute Value

 *

 * MIN.S : FPR[fd] = minNum(FPR[fs],FPR[ft])

 * MINA.S: FPR[fd] = maxNumMag(FPR[fs],FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare signs */

 Signs of inputs are the same, let's compare exponents */

 Inputs are both positive */

 Inputs are both negative */

 Signs and exponents of inputs are equal, let's compare mantissas */

 Inputs are both positive, with equal signs and exponents */

 Inputs are both negative, with equal signs and exponents */

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare exponent */

 Compare mantissa */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

 rm = xm * ym, re = xe+ye basically */

 shunt to top of word */

	/*

	 * Multiply 32 bits xm, ym to give high 32 bits rm with stickness.

 16 * 16 => 32 */

 16 * 16 => 32 */

 16 * 16 => 32 */

 16 * 16 => 32 */

	/*

	 * Sticky shift down to normal rounding precision.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 look for valid corner case */

		/* Set invalid. We will only use overflow for floating

 oh gawd */

			/* Shifting a u32 32 times does not work,

			* so we do it in two steps. Be aware that xe

 toward +Infinity */

 toward -Infinity */

 This can happen after rounding */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * cp1emu.c: a MIPS coprocessor 1 (FPU) instruction emulator

 *

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 *

 * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000  MIPS Technologies, Inc.

 *

 * A complete emulator for MIPS coprocessor 1 instructions.  This is

 * required for #float(switch) or #float(trap), where it catches all

 * COP1 instructions via the "CoProcessor Unusable" exception.

 *

 * More surprisingly it is also required for #float(ieee), to help out

 * the hardware FPU at the boundaries of the IEEE-754 representation

 * (denormalised values, infinities, underflow, etc).  It is made

 * quite nasty because emulation of some non-COP1 instructions is

 * required, e.g. in branch delay slots.

 *

 * Note if you know that you won't have an FPU, then you'll get much

 * better performance by compiling with -msoft-float!

 Function which emulates a floating point instruction. */

 Control registers */

 $0  = revision id */

 $25 = fccr */

 $26 = fexr */

 $28 = fenr */

 $31 = csr */

 convert condition code register number to csr bit */

 (microMIPS) Convert certain microMIPS instructions to MIPS32 format. */

/*

 * This functions translates a 32-bit microMIPS instruction

 * into a 32-bit MIPS32 instruction. Returns 0 on success

 * and SIGILL otherwise.

 NOTE: offset is << by 1 if in microMIPS mode. */

 Invalid */

 Invalid */

 Invalid */

 Invalid */

 POOL32FXF */

 c.cond.fmt */

/*

 * Redundant with logic already in kernel/branch.c,

 * embedded in compute_return_epc.  At some point,

 * a single subroutine should be used across both

 * modules.

 For R6, JR already emulated in jalr_op */

 Set microMIPS mode bit: XOR for jalx. */

		/*

		 * Compact branches for R6 for the

		 * blez and blezl opcodes.

		 * BLEZ  | rs = 0 | rt != 0  == BLEZALC

		 * BLEZ  | rs = rt != 0      == BGEZALC

		 * BLEZ  | rs != 0 | rt != 0 == BGEUC

		 * BLEZL | rs = 0 | rt != 0  == BLEZC

		 * BLEZL | rs = rt != 0      == BGEZC

		 * BLEZL | rs != 0 | rt != 0 == BGEC

		 *

		 * For real BLEZ{,L}, rt is always 0.

		/*

		 * Compact branches for R6 for the

		 * bgtz and bgtzl opcodes.

		 * BGTZ  | rs = 0 | rt != 0  == BGTZALC

		 * BGTZ  | rs = rt != 0      == BLTZALC

		 * BGTZ  | rs != 0 | rt != 0 == BLTUC

		 * BGTZL | rs = 0 | rt != 0  == BGTZC

		 * BGTZL | rs = rt != 0      == BLTZC

		 * BGTZL | rs != 0 | rt != 0 == BLTC

		 *

		 * *ZALC varint for BGTZ &&& rt != 0

		 * For real GTZ{,L}, rt is always 0.

 This is bbit0 on Octeon */

 This is bbit032 on Octeon */

 This is bbit1 on Octeon */

 This is bbit132 on Octeon */

		/*

		 * Only valid for MIPS R6 but we can still end up

		 * here from a broken userland so just tell emulator

		 * this is not a branch and let it break later on.

 Need to check for R6 bc1nez and bc1eqz branches */

 R2/R6 compatible cop1 instruction */

 bc1f */

 bc1fl */

 bc1t */

 bc1tl */

/*

 * In the Linux kernel, we support selection of FPR format on the

 * basis of the Status.FR bit.	If an FPU is not present, the FR bit

 * is hardwired to zero, which would imply a 32-bit FPU even for

 * 64-bit CPUs so we rather look at TIF_32BIT_FPREGS.

 * FPU emu is slow and bulky and optimizing this function offers fairly

 * sizeable benefits so we try to be clever and make this function return

 * a constant whenever possible, that is on 64-bit kernels without O32

 * compatibility enabled and on 32-bit without 64-bit FPU support.

/*

 * Emulate a CFC1 instruction.

/*

 * Emulate a CTC1 instruction.

 Preserve read-only bits.  */

/*

 * Emulate the single floating point instruction pointed at by EPC.

 * Two instructions if the instruction is in a branch delay slot.

	/*

	 * These are giving gcc a gentle hint about what to expect in

	 * dec_inst in order to do better optimization.

 XXX NEC Vr54xx bug workaround */

		/*

		 * The instruction to be emulated is in a branch delay slot

		 * which means that we have to	emulate the branch instruction

		 * BEFORE we do the cop1 instruction.

		 *

		 * This branch could be a COP1 branch, but in that case we

		 * would have had a trap for that instruction, and would not

		 * come through this route.

		 *

		 * Linux MIPS branch emulator operates on context, updating the

		 * cp0_epc.

 process delay slot instr */

 process current instr */

	/*

	 * Since microMIPS FPU instructios are a subset of MIPS32 FPU

	 * instructions, we want to convert microMIPS FPU instructions

	 * into MIPS32 instructions so that we could reuse all of the

	 * FPU emulation code.

	 *

	 * NOTE: We cannot do this for branch instructions since they

	 *       are not a subset. Example: Cannot emulate a 16-bit

	 *       aligned target address with a MIPS32 instruction.

		/*

		 * If next instruction is a 16-bit instruction, then it

		 * it cannot be a FPU instruction. This could happen

		 * since we can be called for non-FPU instructions.

 copregister fs -> gpr[rt] */

 copregister fs <- rt */

 copregister rd -> gpr[rt] */

 copregister rd <- gpr[rt] */

 copregister rd -> gpr[rt] */

 copregister rd <- rt */

 cop control register rd -> gpr[rt] */

 copregister rd <- rt */

				/*

				 * Branch taken: emulate dslot instruction

				/*

				 * Remember EPC at the branch to point back

				 * at so that any delay-slot instruction

				 * signal is not silently ignored.

 If 16-bit instruction, not FPU. */

						/*

						 * Since this instruction will

						 * be put on the stack with

						 * 32-bit words, get around

						 * this problem by putting a

						 * NOP16 as the second one.

						/*

						 * Single step the non-CP1

						 * instruction in the dslot.

						/*

						 * SIGILL forces out of

						 * the emulation loop.

 its one of ours */

				/*

				 * Single step the non-cp1

				 * instruction in the dslot

 SIGILL forces out of the emulation loop.  */

 branch not taken */

				/*

				 * branch likely nullifies

				 * dslot if not taken

				/*

				 * else continue & execute

				 * dslot as normal insn

 a real fpu computation instruction */

 we did it !! */

/*

 * Conversion table from MIPS compare ops 48-63

 * cond = ieee754dp_cmp(x,y,IEEE754_UN,sig);

 cmp_0 (sig) cmp_sf */

 cmp_un (sig) cmp_ngle */

 cmp_eq (sig) cmp_seq */

 cmp_ueq (sig) cmp_ngl  */

 cmp_olt (sig) cmp_lt */

 cmp_ult (sig) cmp_nge */

 cmp_ole (sig) cmp_le */

 cmp_ule (sig) cmp_ngt */

 Reserved */

 Reserved */

/*

 * Additional MIPS4 instructions

 resulting csr */

 0 */

				/*printk ("SIGFPE: FPU csr = %08x\n",

 1 */

 ignore prefx operation */

/*

 * Emulate a single COP1 arithmetic instruction.

 resulting format */

 resulting csr */

 resulting value */

 0 */

 binary ops */

 unary  ops */

		/*

		 * Note that on some MIPS IV implementations such as the

		 * R5000 and R8000 the FSQRT and FRECIP instructions do not

		 * achieve full IEEE-754 accuracy - however this emulator does.

 an easy one */

 binary op on handler */

 unary conv ops */

 not defined */

 binary ops */

 unary  ops */

		/*

		 * Note that on some MIPS IV implementations such as the

		 * R5000 and R8000 the FSQRT and FRECIP instructions do not

		 * achieve full IEEE-754 accuracy - however this emulator does.

 an easy one */

 binary op on handler */

		/*

		 * unary conv ops

 not defined */

 wrong */

 convert word to single precision real */

 convert word to double precision real */

 Emulating the new CMP.condn.fmt R6 instruction */

 This is an R6 only instruction */

 fmt is w_fmt for single precision so fix it */

 default to false */

 CMP.condn.S */

 positive predicates */

 true, all 1s */

 negative predicates */

 true, all 1s */

 Reserved R6 ops */

 convert long to single precision real */

 convert long to double precision real */

 Emulating the new CMP.condn.fmt R6 instruction */

 fmt is l_fmt for double precision so fix it */

 default to false */

 CMP.condn.D */

 positive predicates */

 true, all 1s */

 negative predicates */

 true, all 1s */

 Reserved R6 ops */

	/*

	 * Update the fpu CSR register for this operation.

	 * If an exception is required, generate a tidy SIGFPE exception,

	 * without updating the result register.

	 * Note: cause exception bits do not accumulate, they are rewritten

	 * for each op; only the flag/sticky bits accumulate.

printk ("SIGFPE: FPU csr = %08x\n",ctx->fcr31); */

	/*

	 * Now we can safely write the result back to the register file.

/*

 * Emulate FPU instructions.

 *

 * If we use FPU hardware, then we have been typically called to handle

 * an unimplemented operation, such as where an operand is a NaN or

 * denormalized.  In that case exit the emulation loop after a single

 * iteration so as to let hardware execute any subsequent instructions.

 *

 * If we have no FPU hardware or it has been disabled, then continue

 * emulating floating-point instructions until one of these conditions

 * has occurred:

 *

 * - a non-FPU instruction has been encountered,

 *

 * - an attempt to emulate has ended with a signal,

 *

 * - the ISA mode has been switched.

 *

 * We need to terminate the emulation loop if we got switched to the

 * MIPS16 mode, whether supported or not, so that we do not attempt

 * to emulate a MIPS16 instruction as a regular MIPS FPU instruction.

 * Similarly if we got switched to the microMIPS mode and only the

 * regular MIPS mode is supported, so that we do not attempt to emulate

 * a microMIPS instruction as a regular MIPS FPU instruction.  Or if

 * we got switched to the regular MIPS mode and only the microMIPS mode

 * is supported, so that we do not attempt to emulate a regular MIPS

 * instruction that should cause an Address Error exception instead.

 * For simplicity we always terminate upon an ISA mode switch.

	/*

	 * Initialize context if it hasn't been used already, otherwise ensure

	 * it has been saved to struct thread_struct.

			/*

			 * Get next 2 microMIPS instructions and convert them

			 * into 32-bit instructions.

 Get first instruction. */

 Duplicate the half-word. */

 16-bit instruction. */

 32-bit instruction. */

 Get second instruction. */

 Duplicate the half-word. */

 16-bit instruction. */

 32-bit instruction. */

 Skip NOPs */

			/*

			 * The 'ieee754_csr' is an alias of ctx->fcr31.

			 * No need to copy ctx->fcr31 to ieee754_csr.

		/*

		 * We have to check for the ISA bit explicitly here,

		 * because `get_isa16_mode' may return 0 if support

		 * for code compression has been globally disabled,

		 * or otherwise we may produce the wrong signal or

		 * even proceed successfully where we must not.

 SIGILL indicates a non-fpu instruction */

 but if EPC has advanced, then ignore it */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

	/*

	 * Provide guard, round and stick bit space.

		/*

		 * Have to shift y fraction right to align.

		/*

		 * Have to shift x fraction right to align.

		/*

		 * Generate 28 bit result of adding two 27 bit numbers

		 * leaving result in xm, xs and xe.

 carry out */

		/*

		 * Normalize in extended single precision

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

		/* Set invalid. We will only use overflow for floating

 oh gawd */

		/* Note: At this point upper 32 bits of xm are guaranteed

 toward +Infinity */

 toward -Infinity */

 look for valid corner case 0x80000000 */

 This can happen after rounding */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 can't possibly be sp representable */

	/*

	 * Convert from DP_FBITS to SP_FBITS+3 with sticky right shift.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * single precision: MAX{,A}.f

 * MAX : Scalar Floating-Point Maximum

 * MAXA: Scalar Floating-Point argument with Maximum Absolute Value

 *

 * MAX.S : FPR[fd] = maxNum(FPR[fs],FPR[ft])

 * MAXA.S: FPR[fd] = maxNumMag(FPR[fs],FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare signs */

 Signs of inputs are equal, let's compare exponents */

 Inputs are both positive */

 Inputs are both negative */

 Signs and exponents of inputs are equal, let's compare mantissas */

 Inputs are both positive, with equal signs and exponents */

 Inputs are both negative, with equal signs and exponents */

	/*

	 * Quiet NaN handling

	/*

	 *    The case of both inputs quiet NaNs

	/*

	 *    The cases of exactly one input quiet NaN (numbers

	 *    are here preferred as returned values to NaNs)

	/*

	 * Infinity and zero handling

 Finally get to do some computation */

 Compare exponent */

 Compare mantissa */

 SPDX-License-Identifier: GPL-2.0

/**

 * struct emuframe - The 'emulation' frame structure

 * @emul:	The instruction to 'emulate'.

 * @badinst:	A break instruction to cause a return to the kernel.

 *

 * This structure defines the frames placed within the delay slot emulation

 * page in response to a call to mips_dsemul(). Each thread may be allocated

 * only one frame at any given time. The kernel stores within it the

 * instruction to be 'emulated' followed by a break instruction, then

 * executes the frame in user mode. The break causes a trap to the kernel

 * which leads to do_dsemulret() being called unless the instruction in

 * @emul causes a trap itself, is a branch, or a signal is delivered to

 * the thread. In these cases the allocated frame will either be reused by

 * a subsequent delay slot 'emulation', or be freed during signal delivery or

 * upon thread exit.

 *

 * This approach is used because:

 *

 * - Actually emulating all instructions isn't feasible. We would need to

 *   be able to handle instructions from all revisions of the MIPS ISA,

 *   all ASEs & all vendor instruction set extensions. This would be a

 *   whole lot of work & continual maintenance burden as new instructions

 *   are introduced, and in the case of some vendor extensions may not

 *   even be possible. Thus we need to take the approach of actually

 *   executing the instruction.

 *

 * - We must execute the instruction within user context. If we were to

 *   execute the instruction in kernel mode then it would have access to

 *   kernel resources without very careful checks, leaving us with a

 *   high potential for security or stability issues to arise.

 *

 * - We used to place the frame on the users stack, but this requires

 *   that the stack be executable. This is bad for security so the

 *   per-process page is now used instead.

 *

 * - The instruction in @emul may be something entirely invalid for a

 *   delay slot. The user may (intentionally or otherwise) place a branch

 *   in a delay slot, or a kernel mode instruction, or something else

 *   which generates an exception. Thus we can't rely upon the break in

 *   @badinst always being hit. For this reason we track the index of the

 *   frame allocated to each thread, allowing us to clean it up at later

 *   points such as signal delivery or thread exit.

 *

 * - The user may generate a fake struct emuframe if they wish, invoking

 *   the BRK_MEMU break instruction themselves. We must therefore not

 *   trust that BRK_MEMU means there's actually a valid frame allocated

 *   to the thread, and must not allow the user to do anything they

 *   couldn't already.

 Ensure we have an allocation bitmap */

 Attempt to allocate a single bit/frame */

		/*

		 * Failed to allocate a frame. We'll wait until one becomes

		 * available. We unlock the page so that other threads actually

		 * get the opportunity to free their frames, which means

		 * technically the result of bitmap_full may be incorrect.

		 * However the worst case is that we repeat all this and end up

		 * back here again.

 Received a fatal signal - just give in */

 Success! */

 If some thread is waiting for a frame, now's its chance */

 Clear any allocated frame, retrieving its index */

 If no frame was allocated, we're done */

 Free the frame that this thread had allocated */

 Do nothing if we're not executing from a frame */

 Find the frame being executed */

	/*

	 * If the PC is at the emul instruction, roll back to the branch. If

	 * PC is at the badinst (break) instruction, we've already emulated the

	 * instruction so progress to the continue PC. If it's anything else

	 * then something is amiss & the user has branched into some other area

	 * of the emupage - we'll free the allocated frame anyway.

 NOP is easy */

 microMIPS instructions */

 NOP16 aka MOVE16 $0, $0 */

 ADDIUPC */

 Allocate a frame if we don't already have one */

 Retrieve the appropriately encoded break instruction */

 Write the instructions to the frame */

 Write the frame to user memory */

 Record the PC of the branch, PC to continue from & frame index */

 Change user register context to execute the frame */

 Cleanup the allocated frame, returning if there wasn't one */

 Set EPC to return to post-branch instruction */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 max neg can't be safely negated */

 normalize */

 shunt out overflow bits */

 normalize in grs extended double precision */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

 provide rounding space */

 now the dirty work */

 have remainder, set sticky */

	/*

	 * Normalise rm to rounding precision ?

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision square root

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 x == INF or NAN? */

 sqrt(Nan) = Nan */

 sqrt(0) = 0 */

 sqrt(-Inf) = Nan */

 sqrt(+Inf) = Inf */

 sqrt(-x) = Nan */

 save old csr; switch off INX enable & flag; set RN rounding */

 adjust exponent to prevent overflow */

 x > 2**-512? */

 x = x / 2**512 */

 x < 2**-512? */

 x = x * 2**512 */

 magic initial approximation to almost 8 sig. bits */

 Heron's rule once with correction to improve to ~18 sig. bits */

 t=x/y; y=y+t; py[n0]=py[n0]-0x00100006; py[n1]=0; */

 triple to almost 56 sig. bits: y ~= sqrt(x) to within 1 ulp */

 t=y*y; z=t;	pt[n0]+=0x00100000; t+=z; z=(x-z)*y; */

 t=z/(t+x) ;	pt[n0]+=0x00100000; y+=t; */

 twiddle last bit to force y correctly rounded */

 set RZ, clear INEX flag */

 t=x/y; ...chopped quotient, possibly inexact */

 t = t-ulp */

 add inexact to result status */

 y=y+t; ...chopped sum */

 adjust scalx for correctly rounded sqrt(x) */

 py[n0]=py[n0]+scalx; ...scale back y */

 restore rounding mode, possibly set inexact */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 Even clear inexact flag here */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 normalize */

	/*

	 * Can't possibly overflow,underflow, or need rounding

 drop the hidden bit */

 SPDX-License-Identifier: GPL-2.0

/*

 * Used to obtain names for a debugfs instruction counter, given field name

 * in fpuemustats structure. For example, for input "cmp_sueq_d", the output

 * would be "cmp.sueq.d". This is needed since dots are not allowed to be

 * used in structure field names, and are, on the other hand, desired to be

 * used in debugfs item names to be clearly associated to corresponding

 * MIPS FPU instructions.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 <-- need 64-bit mantissa tmp */

 look for valid corner case */

		/* Set invalid. We will only use overflow for floating

 oh gawd */

 toward +Infinity */

 toward -Infinity */

 This can happen after rounding */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

 quick fix up */

 normalize ym,ye */

 normalize xm,xe */

 flip sign of y and handle as add */

 provide guard,round and stick bit dpace */

		/*

		 * Have to shift y fraction right to align

		/*

		 * Have to shift x fraction right to align

		/* generate 28 bit result of adding two 27 bit numbers

 carry out */

 shift preserving sticky */

 round negative inf. => sign = -1 */

 other round modes   => sign = 1 */

		/* normalize to rounding precision

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 look for valid corner case */

		/* Set invalid. We will only use overflow for floating

 oh gawd */

			/* Shifting a u64 64 times does not work,

			* so we do it in two steps. Be aware that xe

 toward +Infinity */

 toward -Infinity */

 This can happen after rounding */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 Even clear inexact flag here */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * double precision: CLASS.f

 * FPR[fd] = class(FPR[fs])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * 10 bit mask as follows:

	 *

	 * bit0 = SNAN

	 * bit1 = QNAN

	 * bit2 = -INF

	 * bit3 = -NORM

	 * bit4 = -DNORM

	 * bit5 = -ZERO

	 * bit6 = INF

	 * bit7 = NORM

	 * bit8 = DNORM

	 * bit9 = ZERO

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Some debug functions

 *

 * MIPS floating point support

 *

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 *

 *  Nov 7, 2000

 *  Modified to build and operate in Linux kernel environment.

 *

 *  Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 *  Copyright (C) 2000 MIPS Technologies, Inc. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

 provide rounding space */

 now the dirty work */

 have remainder, set sticky */

	/* normalise rm to rounding precision ?

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * double precision: MADDF.f (Fused Multiply Add)

 * MADDF.fmt: FPR[fd] = FPR[fd] + (FPR[fs] x FPR[ft])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

 128 bits shift right logical with rounding. */

	/*

	 * Handle the cases when at least one of x, y or z is a NaN.

	 * Order of precedence is sNaN, qNaN and z, x, y.

 ZERO z cases are handled separately below */

	/*

	 * Infinity handling

			/*

			 * Cases of addition of infinities with opposite signs

			 * or subtraction of infinities with same signs.

		/*

		 * z is here either not an infinity, or an infinity having the

		 * same sign as product (x*y). The result must be an infinity,

		 * and its sign is determined only by the sign of product (x*y).

 Handle cases +0 + (-0) and similar ones. */

				/*

				 * Cases of addition of zeros of equal signs

				 * or subtraction of zeroes of opposite signs.

				 * The sign of the resulting zero is in any

				 * such case determined only by the sign of z.

 x*y is here 0, and z is not 0, so just return z */

 continue to real computations */

 Finally get to do some computation */

	/*

	 * Do the multiplication bit first

	 *

	 * rm = xm * ym, re = xe + ye basically

	 *

	 * At this point xm and ym should have been normalized.

 shunt to top of word */

	/*

	 * Multiply 64 bits xm and ym to give 128 bits result in hrm:lrm.

 Put explicit bit at bit 126 if necessary */

		/*

		 * Move explicit bit from bit 126 to bit 55 since the

		 * ieee754dp_format code expects the mantissa to be

		 * 56 bits wide (53 + 3 rounding bits).

 Move explicit bit from bit 52 to bit 126 */

 Make the exponents the same */

		/*

		 * Have to shift y fraction right to align.

		/*

		 * Have to shift x fraction right to align.

 Do the addition */

		/*

		 * Generate 128 bit result by adding two 127 bit numbers

		 * leaving result in hzm:lzm, zs and ze.

 carry out */

		/*

		 * Put explicit bit at bit 126 if necessary.

 left shift by 63 or 64 bits */

 MSB of lzm is the explicit bit */

	/*

	 * Move explicit bit from bit 126 to bit 55 since the

	 * ieee754dp_format code expects the mantissa to be

	 * 56 bits wide (53 + 3 rounding bits).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * IEEE754 floating point arithmetic

 * single precision: CLASS.f

 * FPR[fd] = class(FPR[fs])

 *

 * MIPS floating point support

 * Copyright (C) 2015 Imagination Technologies, Ltd.

 * Author: Markos Chandras <markos.chandras@imgtec.com>

	/*

	 * 10 bit mask as follows:

	 *

	 * bit0 = SNAN

	 * bit1 = QNAN

	 * bit2 = -INF

	 * bit3 = -NORM

	 * bit4 = -DNORM

	 * bit5 = -ZERO

	 * bit6 = INF

	 * bit7 = NORM

	 * bit8 = DNORM

	 * bit9 = ZERO

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 * Copyright (C) 2017 Imagination Technologies, Ltd.

 * Author: Aleksandar Markovic <aleksandar.markovic@imgtec.com>

 toward nearest */

 toward zero */

 toward +infinity */

 toward -infinity */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

 rm = xm * ym, re = xe+ye basically */

 shunt to top of word */

	/*

	 * Multiply 64 bits xm, ym to give high 64 bits rm with stickness.

	/*

	 * Sticky shift down to normal rounding precision.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * double precision: common utilities

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

	/*

	 * Infinity handling

	/*

	 * Zero handling

	/*

	 * Provide guard,round and stick bit space.

		/*

		 * Have to shift y fraction right to align.

		/*

		 * Have to shift x fraction right to align.

		/*

		 * Generate 28 bit result of adding two 27 bit numbers

		 * leaving result in xm, xs and xe.

 carry out */

		/*

		 * Normalize to rounding precision.

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

/*

 * Raise the Invalid Operation IEEE 754 exception

 * and convert the signaling NaN supplied to a quiet NaN.

	/* inexact must round of 3 bits

 xm += (xm&0x8)?0x4:0x3 */

 toward +Infinity */

 ?? */

 toward -Infinity */

 ?? */

/* generate a normal/denormal number with over,under handling

 * sn is sign

 * xe is an unbiased exponent

 * xm is 3bit extended precision value.

 we don't gen exact zeros (probably should) */

 no excess */

 strip lower bits */

 toward +Infinity */

 toward -Infinity */

 Not tiny after rounding */

 Clear grs bits */

			/* sticky right shift es bits

		/* inexact must round of 3 bits

		/* adjust exponent for rounding add overflowing

 add causes mantissa overflow */

 strip grs bits */

 no excess */

 -O can be table indexed by (rm,sn) */

 toward +Infinity */

 toward -Infinity */

 gen norm/denorm/zero */

 we underflow (tiny/zero) */

 no excess */

 SPDX-License-Identifier: GPL-2.0-only

/* ieee754 floating point arithmetic

 * single and double precision

 *

 * BUGS

 * not much dp done

 * doesn't generate IEEE754_INEXACT

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

/*

 * Special constants

/*

 * Older GCC requires the inner braces for initialization of union ieee754dp's

 * anonymous struct member.  Without an error will result.

 + zero   */

 - zero   */

 + 1.0   */

 - 1.0   */

 + 10.0   */

 - 10.0   */

 + infinity */

 - infinity */

 + ind legacy qNaN */

 + indef 2008 qNaN */

 + max */

 - max */

 + min normal */

 - min normal */

 + min denormal */

 - min denormal */

 + 1.0e31 */

 + 1.0e63 */

 + zero   */

 - zero   */

 + 1.0   */

 - 1.0   */

 + 10.0   */

 - 10.0   */

 + infinity */

 - infinity */

 + indef legacy quiet NaN */

 + indef 2008 quiet NaN */

 + max normal */

 - max normal */

 + min normal */

 - min normal */

 + min denormal */

 - min denormal */

 + 1.0e31 */

 + 1.0e63 */

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 max neg can't be safely negated */

		/* shunt out overflow bits

		/* normalize in grs extended single precision

 SPDX-License-Identifier: GPL-2.0-only

/* IEEE754 floating point arithmetic

 * single precision

/*

 * MIPS floating point support

 * Copyright (C) 1994-2000 Algorithmics Ltd.

 <--- need 64-bit mantissa temp */

 max neg can't be safely negated */

		/* shunt out overflow bits

 normalize in grs extended single precision */

 SPDX-License-Identifier: (GPL-2.0 OR MIT)

/*

 * Microsemi MIPS SoC support

 *

 * Copyright (c) 2017 Microsemi Corporation

 Look for the TLB entry set up by redboot before trying to use it */

 A TLB entry exists, lets assume its usable and check the CHIP ID */

 Copy command line from bootloader early for Initrd detection */

 ignore all built-in args if any f/w args given */

 This has to be done so late because ioremap needs to work */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 Already set up */

		/*

		 * We have been provided with the appropriate device tree for

		 * the board. Make use of it & search for any machine struct

		 * based upon the root compatible string.

		/*

		 * We weren't booted using the UHI boot protocol, but do

		 * support some number of boards with legacy boot protocols.

		 * Attempt to find the right one.

		/*

		 * If we don't recognise the machine then we can't continue, so

		 * die here.

 Retrieve the machine's FDT */

	/*

	 * reset fdt as the cached value would point to the location

	 * before relocations happened and update the location argument

	 * if it was passed using UHI

 CONFIG_RELOCATABLE */

 The counter runs at the CPU clock rate */

 The counter runs at half the CPU clock rate */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support for Ingenic SoCs

 *

 * Copyright (C) 2009-2010, Lars-Peter Clausen <lars@metafoo.de>

 * Copyright (C) 2011, Maarten ter Huurne <maarten@treewalker.org>

 * Copyright (C) 2020 Paul Cercueil <paul@crapouillou.net>

	/*

	 * Old devicetree files for the qi,lb60 board did not have a /memory

	 * node. Hardcode the memory info here.

		/*

		 * Unconditionally enable the clock for the first CPU.

		 * This makes sure that the PLL that feeds the CPU won't be

		 * stopped while the kernel is running.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Support code for virtual Ranchu board for MIPS.

 *

 * Author: Miodrag Dinic <miodrag.dinic@mips.com>

	/*

	 * Reading the low address latches the high value

	 * as well so there is no fear that we may read

	 * inaccurate high value.

	/*

	 * Poll the nanosecond resolution RTC for one

	 * second to calibrate the CPU frequency.

	/*

	 * Make sure the frequency will be a round number.

	 * Without this correction, the returned value may vary

	 * between subsequent emulation executions.

	 *

	 * TODO: Set this value using device tree.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

/*

 * Maximum 384MB RAM at physical address 0, preceding any I/O.

 start	size */

 leave the GIC node intact if a GIC is present */

 if this isn't SEAD3, something went wrong */

 get original sample */

 wait for transition */

 flip the bit */

 wait 1 second (the sampling clock transitions every 10ms) */

 wait for transition */

 flip the bit */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 find or add chosen node */

 How much of the remaining RAM fits in the next region? */

 Emit a memory region */

 Discard the next mr->discard bytes */

 find memory size from the bootloader environment */

 default to using all available RAM */

 allow the user to override the usable memory */

 if the user says there's more RAM than we thought, believe them */

 find or add a memory node */

 find or add chosen node */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 by Ralf Baechle

 * Copyright (C) 2009, 2012 Cavium, Inc.

	/*

	 * For __ndelay we divide by 2^16, so the factor is multiplied

	 * by the same amount.

 CPU clock */

 I/O clock */

 CPU clock */

 I/O clock */

/*

 * Set the current core's cvmcount counter to the value of the

 * IPD_CLK_COUNT.  We do this on all cores as they are brought

 * on-line.  This allows for a read from a local cpu register to

 * access a synchronized counter.

 *

 * On CPU_CAVIUM_OCTEON2 the IPD_CLK_COUNT is scaled by rdiv/sdiv.

 Clobber loops so GCC will not unroll the following while loop. */

	/*

	 * Loop several times so we are executing from the cache,

	 * which should give more deterministic timing.

 64-bit arithmatic can overflow, so use 128-bit.  */

/**

 * octeon_io_clk_delay - wait for a given number of io clock cycles to pass.

 *

 * We scale the wait by the clock ratio, and then wait for the

 * corresponding number of core clocks.

 *

 * @count: The number of clocks to wait.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000  Ani Joshi <ajoshi@unixbox.com>

 * Copyright (C) 2000, 2001  Ralf Baechle <ralf@gnu.org>

 * Copyright (C) 2005 Ilya A. Volynets-Evenbakh <ilya@total-knowledge.com>

 * swiped from i386, and cloned for MIPS by Geert, polished by Ralf.

 * IP32 changes by Ilya.

 * Copyright (C) 2010 Cavium Networks, Inc.

 Anything in the BAR1 hole or above goes via BAR2 */

 Anything not in the BAR1 range goes via BAR2 */

 CONFIG_PCI */

 These addresses map low for PCI. */

	/*

	 * For OCTEON_DMA_BAR_TYPE_SMALL, size the iotlb at 1/4 memory

	 * size to a maximum of 64MB

		/*

		 * Otherwise only allocate a big iotlb if there is

		 * memory past the BAR1 hole.

 OCTEON II ohci is only 32-bit. */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2007 Cavium Networks

 * Copyright (C) 2008, 2009 Wind River Systems

 *   written by Ralf Baechle <ralf@linux-mips.org>

 for memset */

/*

 * TRUE for devices having registers with little-endian byte

 * order, FALSE for registers with native-endian byte order.

 * PCI mandates little-endian, USB and SATA are configuraable,

 * but we chose little-endian for these.

 bootbus/CF */

 PCI mmio window */

 PCI mmio window */

 PCI mmio window */

 PCI mmio window */

 OCTEON III USB */

 OCTEON III USB */

 OCTEON III SATA */

 OCTEON II USB */

/*

 * Wait for relocation code is prepared and send

 * secondary CPUs to spin until kernel is relocated.

			/*

			 * convert command line string to array

			 * of parameters (as bootloader does).

	/*

	 * Information about segments will be needed during pre-boot memory

	 * initialization.

 disable watchdogs */

		/*

		 * Mark all memory (except low 0x100000 bytes) as free.

		 * It is the same thing that bootloader does.

		/*

		 * Allocate all segments to avoid their corruption during boot.

		/*

		 * Do not mark all memory as free. Free only named sections

		 * leaving the rest of memory unchanged.

 running on octeon_main_processor */

 running on secondary cpu */

 disable watchdogs */

 CONFIG_KEXEC */

/* crashkernel cmdline parameter is parsed _after_ memory setup

/**

 * octeon_is_simulation - Return non-zero if we are currently running

 * in the Octeon simulator

 *

 * Return: non-0 if running in the Octeon simulator, 0 otherwise

/**

 * octeon_is_pci_host - Return true if Octeon is in PCI Host mode. This means

 * Linux can control the PCI bus.

 *

 * Return: Non-zero if Octeon is in host mode.

/**

 * octeon_get_clock_rate - Get the clock rate of Octeon

 *

 * Return: Clock rate in HZ

/**

 * octeon_write_lcd - Write to the LCD display connected to the bootbus.

 * @s:	    String to write

 *

 * This display exists on most Cavium evaluation boards. If it doesn't exist,

 * then this function doesn't do anything.

/**

 * octeon_get_boot_uart - Return the console uart passed by the bootloader

 *

 * Return: uart number (0 or 1)

/**

 * octeon_get_boot_coremask - Get the coremask Linux was booted on.

 *

 * Return: Core mask

/**

 * octeon_check_cpu_bist - Check the hardware BIST results for a CPU

 Check BIST results for COP0 registers */

/**

 * octeon_restart - Reboot Octeon

 *

 * @command: Command to pass to the bootloader. Currently ignored.

 Disable all watchdogs before soft reset. They don't get cleared */

/**

 * octeon_kill_core - Permanently stop a core.

 *

 * @arg: Ignored.

 A break instruction causes the simulator stop a core */

 Disable watchdog on this core. */

 Spin in a low power mode. */

/**

 * octeon_halt - Halt the system

 Driving a 1 to GPIO 12 shuts off this board */

/**

 * octeon_board_type_string - Return a string representing the system type

 *

 * Return: system type string

 Get the current settings for CP0_CVMMEMCTL_REG */

	/* R/W If set, marked write-buffer entries time out the same

	 * as as other entries; if clear, marked write-buffer entries

	/* R/W If set, a merged store does not clear the write-buffer

	/* R/W Two bits that are the MSBs of the resultant CVMSEG LM

	 * word location for an IOBDMA. The other 8 bits come from the

	/* R/W If set, SYNCWS and SYNCS only order marked stores; if

	 * clear, SYNCWS and SYNCS only order unmarked

	 * stores. SYNCWSMARKED has no effect when DISSYNCWS is

 R/W If set, SYNCWS acts as SYNCW and SYNCS acts as SYNC. */

 R/W If set, no stall happens on write buffer full. */

	/* R/W If set (and SX set), supervisor-level loads/stores can

	/* R/W If set (and UX set), user-level loads/stores can use

	/* R/W If set (and SX set), supervisor-level loads/stores can

	/* R/W If set (and UX set), user-level loads/stores can use

	/* R/W If set, all stores act as SYNCW (NOMERGE must be set

	/* R/W If set, no stores merge, and all stores reach the

	/* R/W Selects the bit in the counter used for DID time-outs 0

	 * = 231, 1 = 230, 2 = 229, 3 = 214. Actual time-out is

	 * between 1x and 2x this interval. For example, with

 R/W If set, the (mem) CSR clock never turns off. */

 R/W If set, mclk never turns off. */

	/* R/W Selects the bit in the counter used for write buffer

	 * flush time-outs (WBFLT+11) is the bit position in an

	 * internal counter used to determine expiration. The write

	 * buffer expires between 1x and 2x this interval. For

	 * example, with WBFLT = 0, a write buffer expires between 2K

 R/W If set, do not put Istream in the L2 cache. */

	/*

	 * R/W The write buffer threshold. As per erratum Core-14752

	 * for CN63XX, a sc/scd might fail if the write buffer is

	 * full.  Lowering WBTHRESH greatly lowers the chances of the

	 * write buffer ever being full and triggering the erratum.

	/* R/W If set, CVMSEG is available for loads/stores in

	/* R/W If set, CVMSEG is available for loads/stores in

	/* R/W If set, CVMSEG is available for loads/stores in user

 Setup of CVMSEG is done in kernel-entry-init.h */

 Set a default for the hardware timeouts */

 Disable tagwait FAU timeout */

 4096 cycles */

/**

 * prom_init - Early entry point for arch setup

	/*

	 * The bootloader passes a pointer to the boot descriptor in

	 * $a3, this is available as fw_arg3.

 Some broken u-boot pass garbage in upper bits, clear them out */

 I/O clock runs at a different rate than the CPU. */

 I/O clock runs at a different rate than the CPU. */

		/*

		 * Setup the multiplier save/restore code if

		 * CvmCtl[NOMUL] clear.

	/*

	 * Only enable the LED controller if we're running on a CN38XX, CN58XX,

	 * or CN56XX. The CN30XX and CN31XX don't have an LED controller.

 TLB refill */

 General exception */

 Interrupt handler */

	/*

	 * BIST should always be enabled when doing a soft reset. L2

	 * Cache locking for instance is not cleared unless BIST is

	 * enabled.  Unfortunately due to a chip errata G-200 for

	 * Cn38XX and CN31XX, BIST must be disabled on these parts.

 Default to 64MB in the simulator to speed things up */

			/*

			 * To do: switch parsing to new style, something like:

			 * parse_crashkernel(arg, sysinfo->system_dram_size,

			 *		  &crashk_size, &crashk_base);

 Exclude a single page from the regions obtained in plat_mem_setup. */

 CONFIG_CRASH_DUMP */

	/*

	 * The Mips memory init uses the first memory location for

	 * some memory vectors. When SPARSEMEM is in use, it doesn't

	 * verify that the size is big enough for the final

	 * vectors. Making the smallest chuck 4MB seems to be enough

	 * to consistently work.

 Crashkernel ignores bootmem list. It relies on mem=X@Y option */

	/*

	 * When allocating memory, we want incrementing addresses,

	 * which is handled by memblock

			/*

			 * exclude a page at the beginning and end of

			 * the 256MB PCIe 'hole' so the kernel will not

			 * try to allocate multi-page buffers that

			 * span the discontinuity.

			/*

			 * This function automatically merges address regions

			 * next to each other if they are received in

			 * incrementing order

 region is fully in */

				/*

				 * Entire memory region is within the new

				 *  kernel's memory, ignore it.

				/*

				 * Overlap with the beginning of the region,

				 * reserve the beginning.

				/*

				 * Overlap with the beginning of the region,

				 * chop of end.

 Recovering mem_alloc_size */

 CONFIG_CRASH_DUMP */

/*

 * Emit one character to the boot UART.	 Exported for use by the

 * watchdog timer.

 Spin until there is room */

 Write the byte */

 Check for presence of Core-14449 fix.  */

	/*

	 * Initially assume there is no PCI. The PCI/PCIe platform code will

	 * later re-initialize these to correct values if they are present.

	/*

	 * Release the allocated memory if a real IO space is there.

/*

 *   Octeon Bootbus flash setup

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007, 2008 Cavium Networks

/*

 * Module/ driver initialization.

 *

 * Returns Zero on success

	/*

	 * Read the bootbus region 0 setup to determine the base

	 * address of the flash.

		/*

		 * The bootloader always takes the flash and sets its

		 * address so the entire flash fits below

		 * 0x1fc00000. This way the flash aliases to

		 * 0x1fc00000 for booting. Software can access the

		 * full flash at the true address, while core boot can

		 * access 4MB.

 Use this name so old part lines work */

 8-bit bus (0 + 1) or 16-bit bus (1 + 1) */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2009 Wind River Systems,

 *   written by Ralf Baechle <ralf@linux-mips.org>

 Don't call default notifier */

 Let default notifier send signals */

 SPDX-License-Identifier: GPL-2.0-only

	/* interval in milli seconds after which the interrupt will

	 * be triggered

		/* Calculating by the amounts io clock and cpu clock would

		 *  increment in interval amount of ms

 Read it back to force wait until register is written. */

 Read it back to force immediate write of timer register*/

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2017 Cavium, Inc.

 * Copyright (C) 2008 Wind River Systems

	/*

	 * Step 1: Wait for voltages stable.  That surely happened

	 * before starting the kernel.

	 *

	 * Step 2: Enable  SCLK of UCTL by writing UCTL0_IF_ENA[EN] = 1

 Set txvreftune to 15 to obtain compliant 'eye' diagram. */

 Step 3: Configure the reference clock, PHY, and HCLK */

	/*

	 * If the UCTL looks like it has already been started, skip

	 * the initialization, otherwise bus errors are obtained.

 3a */

 3b */

 3c */

 Read it back, */

 3d */

 3e: delay 64 io clocks */

	/*

	 * Step 4: Program the power-on reset field in the UCTL

	 * clock-reset-control register.

 Step 5:    Wait 3 ms for the PHY clock to start. */

 Steps 6..9 for ATE only, are skipped. */

 Step 10: Configure the OHCI_CLK48 and OHCI_CLK12 clocks. */

 10a */

 10b */

 10c */

	/*

	 * Step 11: Program the PHY reset field:

	 * UCTL0_CLK_RST_CTL[P_PRST] = 1

 Step 11b */

 Step 11c */

 Step 11d */

 Step 11e */

 Step 12: Wait 1 uS. */

 Step 13: Program the HRESET_N field: UCTL0_CLK_RST_CTL[HRST] = 1 */

 Set uSOF cycle period to 60,000 bits. */

 Octeon EHCI matches CPU endianness. */

	/*

	 * We can DMA from anywhere. But the descriptors must be in

	 * the lower 4GB.

 Use 64-bit addressing. */

 Byte swapped. */

 Byte swapped. */

 not swapped. */

 not swapped. */

 Octeon OHCI matches CPU endianness. */

 Byte swapped. */

 Byte swapped. */

 not swapped. */

 not swapped. */

 CONFIG_USB */

 Octeon Random Number Generator.  */

 Port 1 on these boards is always gigabit. */

 Ports 0 and 1 connect to the switch. */

 Delete the PHY things */

 This one may fail */

 Use the alt phy node instead.*/

				/*

				 * Boards with gigabit WAN ports need a

				 * different setting that is compatible with

				 * 100 Mbit settings

 Different config for switch port. */

 I2C */

 SMI/MDIO */

 Serial */

 Right now CN52XX is the only chip with a third uart */

 uart2 */

 Compact Flash */

 Find CS0 region. */

 cs and cs + 1 are CS0 and CS1, both must be less than 8. */

			/*

			 * Boot loader signals availability of DMA (true_ide

			 * mode) by setting low order bits of base_ptr to

			 * zero.

 Asume that CS1 immediately follows. */

 8 char LED */

 Find CS0 region. */

 OHCI/UHCI USB */

 Missing "refclk-type" defaults to crystal. */

 DWC2 USB */

 Missing "refclk-type" defaults to external. */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2016 Cavium, Inc.

/*

 * The 8 most significant bits of the intsn identify the interrupt major block.

 * Each major block might use its own interrupt domain. Thus 256 domains are

 * needed.

 Information for each ciu3 in the system */

 Each ciu3 in the system uses its own data (one ciu3 per node) */

 number of sum registers (2 or 3). */

 Register offsets from ciu3_addr */

 only used for ciu3 */

 only used for ciu/ciu2 */

 Next CPU expected to take this irq */

 NUMA node number of the CIU */

	/*

	 * We don't need to disable IRQs to make these atomic since

	 * they are already disabled earlier in the low level

	 * interrupt code.

 The two user interrupts must be cleared manually. */

	/*

	 * We don't need to disable IRQs to make these atomic since

	 * they are already disabled earlier in the low level

	 * interrupt code.

	/*

	 * Interrupts are already disabled, so these are atomic.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

/*

 * Enable the irq on the next core in the affinity set for chips that

 * have the EN*_W1{S,C} registers.

	/*

	 * Called under the desc lock, so these should never get out

	 * of sync.

/*

 * Enable the irq in the sum2 registers.

/*

 * Disable the irq in the sum2 registers.

/*

 * Enable the irq on the current CPU for chips that

 * have the EN*_W1{S,C} registers.

/*

 * Write to the W1C bit in CVMX_CIU_INTX_SUM0 to clear the irq.

/*

 * Disable the irq on the all cores for chips that have the EN*_W1{S,C}

 * registers.

/*

 * Enable the irq on the all cores for chips that have the EN*_W1{S,C}

 * registers.

 140 nS glitch filter*/

		/*

		 * It has multi CPU affinity, just remove this CPU

		 * from the affinity set.

 Otherwise, put it on lowest numbered online CPU. */

	/*

	 * For non-v2 CIU, we will allow only single CPU affinity.

	 * This removes the need to do locking in the .ack/.eoi

	 * functions.

		/*

		 * Must be visible to octeon_irq_ip{2,3}_ciu() before

		 * enabling the irq.

/*

 * Set affinity for the irq for chips that have the EN*_W1{S,C}

 * registers.

	/* ack any pending edge-irq at startup, so there is

	 * an _edge_ to fire on when the event reappears.

/*

 * Newer octeon chips have support for lockless CIU operation.

/*

 * Newer octeon chips have support for lockless CIU operation.

 The mbox versions don't do any affinity or round-robin. */

/*

 * Watchdog interrupts are special.  They are associated with a single

 * core, so we hardwire the affinity to that core.

 Bit 0-63 of EN1 */

	/*

	 * Must be visible to octeon_irq_ip{2,3}_ciu() before enabling

	 * the irq.

/*

 * Watchdog interrupts are special.  They are associated with a single

 * core, so we hardwire the affinity to that core.

 GMX DRP */

 IPD_DRP */

 Timers */

 MPI */

 line == 1 */

 PTP */

	/*

	 * Default to handle_level_irq. If the DT contains a different

	 * trigger type, it will call the irq_set_type callback and

	 * the handler gets updated.

	/*

	 * Disable All CIU Interrupts. The ones we need will be

	 * enabled later.  Read the SUM register so we know the write

	 * completed.

	/*

	 * Disable All CIU2 Interrupts. The ones we need will be

	 * enabled later.  Read the SUM register so we know the write

	 * completed.

	 *

	 * There are 9 registers and 3 IPX levels with strides 0x1000

	 * and 0x200 respectivly.  Use loops to clear them.

 Enable the CIU lines */

 Enable the CIU lines */

 Mips internal */

 CIU_0 */

 CIU_1 */

 Enable the CIU lines */

 gpio domain host_data is the base hwirq number. */

	/*

	 * Clear the OF_POPULATED flag that was set by of_irq_init()

	 * so that all GPIO devices will be probed.

/*

 * Watchdog interrupts are special.  They are associated with a single

 * core, so we hardwire the affinity to that core.

 MIO */

 IPD_DRP */

 Timers */

 PTP */

 PKT */

 ILK_DRP */

 GMX_DRP */

	/*

	 * Don't map irq if it is reserved for GPIO.

	 * (Line 7 are the GPIO lines.)

	/* CN68XX pass 1.x has an errata that accessing the ACK registers

	/* CN68XX pass 1.x has an errata that accessing the ACK registers

 Mips internal */

 CUI2 */

 Enable the CIU lines */

 unofficial value, but we might as well let it work. */

 official value for level triggering. */

 official value for edge triggering. */

 Nothing else is acceptable. */

 Chain to real handler. */

 If edge, acknowledge the bit we will be sending. */

 disable all IRQs */

 ack any outstanding */

 Software handled separately. */

 official value for level triggering. */

 unofficial value, but we might as well let it work. */

 official value for edge triggering. */

 Nothing else is acceptable. */

	/*

	 * We use a single irq_chip, so we have to do nothing to ack a

	 * level interrupt.

	/*

	 * We use a single irq_chip, so only ack an edge (!level)

	 * interrupt.

 Get the domain to use from the major block */

/*

 * 10 mbox per core starting from zero.

 * Base mbox is core * 10

 SW (mbox) are 0x04 in bits 12..19 */

	/*

	 * 4 idt per core starting from 1 because zero is reserved.

	 * Base idt per core is 4 * core + 1

 ip2 interrupts for this CPU */

 ip3 interrupts for this CPU */

 ip4 interrupts for this CPU */

 Enable the CIU lines */

 of_node_to_nid(ciu_node); */

 Mips internal */

 Only do per CPU things if it is the CIU of the boot node. */

	/*

	 * Initialize all domains to use the default domain. Specific major

	 * blocks will overwrite the default domain as needed.

 Only do per CPU things if it is the CIU of the boot node. */

 Enable the CIU lines */

 Set the default affinity to the boot cpu. */

 CONFIG_HOTPLUG_CPU */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2008, 2009, 2010 Cavium Networks

 CONFIG_RELOCATABLE */

	/*

	 * Make sure the function array initialization remains

	 * correct.

	/*

	 * Load the mailbox register to figure out what we're supposed

	 * to do.

 Clear the mailbox to clear the interrupt */

/*

 * Cause the function described by call_data to be executed on the passed

 * cpu.	 When the function has finished, increment the finished field of

 * call_data.

	/*

	pr_info("SMP: Mailbox send cpu=%d, coreid=%d, action=%u\n", cpu,

	       coreid, action);

/*

 * Detect available CPUs, populate cpu_possible_mask

 The present CPUs are initially just the boot cpu (CPU 0). */

 The present CPUs get the lowest CPU numbers. */

	/*

	 * The possible CPUs are all those present on the chip.	 We

	 * will assign CPU numbers for possible cores as well.	Cores

	 * are always consecutively numberd from 0.

 Send secondaries into relocated kernel */

 CONFIG_RELOCATABLE */

/*

 * Firmware CPU startup hook

 Waiting for processor to get the SP and GP */

/*

 * After we've done initial boot, this function is called to allow the

 * board code to clean up state, if needed

/*

 * Callout to firmware before smp_init

	/*

	 * Only the low order mailbox bits are used for IPIs, leave

	 * the other bits alone.

/*

 * Last chance for the board code to finish SMP initialization before

 * the CPU is "online".

 to generate the first CPU timer interrupt */

 State of each CPU. */

	/*

	 * This is a bit complicated strategics of getting/settig available

	 * cores mask, copied from bootloader

 LINUX_APP_BOOT_BLOCK is initialized in bootoct binary */

 alternative, already initialized */

 core will be reset here */

 set a2 = 0 for secondary core */

 alternative, already initialized */

 core not available, assume, that caught by simple-executive */

 CONFIG_HOTPLUG_CPU */

/*

 * Callout to firmware before smp_init

/*

 * XHCI HCD glue for Cavium Octeon III SOCs.

 *

 * Copyright (C) 2010-2017 Cavium Networks

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 USB Control Register */

 1 = BIST and set all USB RAMs to 0x0, 0 = BIST */

 1 = Start BIST and cleared by hardware */

	/* Reference clock select for SuperSpeed and HighSpeed PLLs:

	 *	0x0 = Both PLLs use DLMC_REF_CLK0 for reference clock

	 *	0x1 = Both PLLs use DLMC_REF_CLK1 for reference clock

	 *	0x2 = SuperSpeed PLL uses DLMC_REF_CLK0 for reference clock &

	 *	      HighSpeed PLL uses PLL_REF_CLK for reference clck

	 *	0x3 = SuperSpeed PLL uses DLMC_REF_CLK1 for reference clock &

	 *	      HighSpeed PLL uses PLL_REF_CLK for reference clck

 1 = Spread-spectrum clock enable, 0 = SS clock disable */

	/* Spread-spectrum clock modulation range:

	 *	0x0 = -4980 ppm downspread

	 *	0x1 = -4492 ppm downspread

	 *	0x2 = -4003 ppm downspread

	 *	0x3 - 0x7 = Reserved

	/* Enable non-standard oscillator frequencies:

	 *	[55:53] = modules -1

	 *	[52:47] = 2's complement push amount, 0 = Feature disabled

	/* Reference clock multiplier for non-standard frequencies:

	 *	0x19 = 100MHz on DLMC_REF_CLK* if REF_CLK_SEL = 0x0 or 0x1

	 *	0x28 = 125MHz on DLMC_REF_CLK* if REF_CLK_SEL = 0x0 or 0x1

	 *	0x32 =  50MHz on DLMC_REF_CLK* if REF_CLK_SEL = 0x0 or 0x1

	 *	Other Values = Reserved

	/* Enable reference clock to prescaler for SuperSpeed functionality.

	 * Should always be set to "1"

	/* Divide the reference clock by 2 before entering the

	 * REF_CLK_FSEL divider:

	 *	If REF_CLK_SEL = 0x0 or 0x1, then only 0x0 is legal

	 *	If REF_CLK_SEL = 0x2 or 0x3, then:

	 *		0x1 = DLMC_REF_CLK* is 125MHz

	 *		0x0 = DLMC_REF_CLK* is another supported frequency

	/* Select reference clock freqnuency for both PLL blocks:

	 *	0x27 = REF_CLK_SEL is 0x0 or 0x1

	 *	0x07 = REF_CLK_SEL is 0x2 or 0x3

 Reserved */

 Controller clock enable. */

	/* Select bypass input to controller clock divider:

	 *	0x0 = Use divided coprocessor clock from H_CLKDIV

	 *	0x1 = Use clock from GPIO pins

 Reset controller clock divider. */

 Reserved */

	/* Clock divider select:

	 *	0x0 = divide by 1

	 *	0x1 = divide by 2

	 *	0x2 = divide by 4

	 *	0x3 = divide by 6

	 *	0x4 = divide by 8

	 *	0x5 = divide by 16

	 *	0x6 = divide by 24

	 *	0x7 = divide by 32

 Reserved */

 USB3 port permanently attached: 0x0 = No, 0x1 = Yes */

 USB2 port permanently attached: 0x0 = No, 0x1 = Yes */

 Reserved */

 Disable SuperSpeed PHY: 0x0 = No, 0x1 = Yes */

 Reserved */

 Disable HighSpeed PHY: 0x0 = No, 0x1 = Yes */

 Reserved */

 Enable PHY SuperSpeed block power: 0x0 = No, 0x1 = Yes */

 Reserved */

 Enable PHY HighSpeed block power: 0x0 = No, 0x1 = Yes */

 Reserved */

 Enable USB UCTL interface clock: 0xx = No, 0x1 = Yes */

 Controller mode: 0x0 = Host, 0x1 = Device */

 PHY reset */

 Software reset UAHC */

 Software resets UCTL */

 UAHC Configuration Register */

 Reserved */

 Indicates minimum value of all received BELT values */

 Reserved */

 HS jitter adjustment */

 Reserved */

 Bus-master enable: 0x0 = Disabled (stall DMAs), 0x1 = enabled */

 Overcurrent protection enable: 0x0 = unavailable, 0x1 = available */

	/* Overcurrent sene selection:

	 *	0x0 = Overcurrent indication from off-chip is active-low

	 *	0x1 = Overcurrent indication from off-chip is active-high

 Port power control enable: 0x0 = unavailable, 0x1 = available */

	/* Port power control sense selection:

	 *	0x0 = Port power to off-chip is active-low

	 *	0x1 = Port power to off-chip is active-high

 Reserved */

 UCTL Shim Features Register */

 Out-of-bound UAHC register access: 0 = read, 1 = write */

 Reserved */

	/* SRCID error log for out-of-bound UAHC register access:

	 *	[59:58] = chipID

	 *	[57] = Request source: 0 = core, 1 = NCB-device

	 *	[56:51] = Core/NCB-device number, [56] always 0 for NCB devices

	 *	[50:48] = SubID

 Error log for bad UAHC DMA access: 0 = Read log, 1 = Write log */

 Reserved */

 Encoded error type for bad UAHC DMA */

 Reserved */

 Select the IOI read command used by DMA accesses */

 Reserved */

	/* Select endian format for DMA accesses to the L2c:

	 *	0x0 = Little endian

	 *`	0x1 = Big endian

	 *	0x2 = Reserved

	 *	0x3 = Reserved

 Reserved */

	/* Select endian format for IOI CSR access to UAHC:

	 *	0x0 = Little endian

	 *`	0x1 = Big endian

	 *	0x2 = Reserved

	 *	0x3 = Reserved

 Enable XHCI power control and set if active high or low. */

 Disable XHCI power control and set if active high. */

	/*

	 * Step 1: Wait for all voltages to be stable...that surely

	 *         happened before starting the kernel. SKIP

 Step 2: Select GPIO for overcurrent indication, if desired. SKIP */

 Step 3: Assert all resets. */

 Step 4a: Reset the clock dividers. */

 Step 4b: Select controller clock frequency. */

 Step 4c: Deassert the controller clock divider reset. */

 Step 5a: Reference clock configuration. */

 Step 5b: Configure and enable spread-spectrum for SuperSpeed. */

 Step 5c: Enable SuperSpeed. */

 Step 5d: Cofngiure PHYs. SKIP */

 Step 6a & 6b: Power up PHYs. */

 Step 7: Wait 10 controller-clock cycles to take effect. */

 Step 8a: Deassert UCTL reset signal. */

 Step 8b: Wait 10 controller-clock cycles. */

 Steo 8c: Setup power-power control. */

 Step 8d: Deassert UAHC reset signal. */

 Step 8e: Wait 10 controller-clock cycles. */

 Step 9: Enable conditional coprocessor clock of UCTL. */

Step 10: Set for host mode only. */

	/*

	 * There should only be three universal controllers, "uctl"

	 * in the device tree. Two USB and a SATA, which we ignore.

			/*

			 * The code below maps in the registers necessary for

			 * setting up the clocks and reseting PHYs. We must

			 * release the resources so the dwc3 subsystem doesn't

			 * know the difference.

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Simple allocate only memory allocator.  Used to allocate memory at

 * application start time.

#define DEBUG */

 See header file for descriptions of functions */

/*

 * This macro returns a member of the

 * cvmx_bootmem_named_block_desc_t structure. These members can't

 * be directly addressed as they might be in memory not directly

 * reachable. In the case where bootmem is compiled with

 * LINUX_HOST, the structure itself might be located on a remote

 * Octeon. The argument "field" is the member name of the

 * cvmx_bootmem_named_block_desc_t to read. Regardless of the type

 * of the field, the return type is always a uint64_t. The "addr"

 * parameter is the physical address of the structure.

/*

 * This function is the implementation of the get macros defined

 * for individual structure members. The argument are generated

 * by the macros inorder to read only the needed memory.

 *

 * @param base   64bit physical address of the complete structure

 * @param offset Offset from the beginning of the structure to the member being

 *               accessed.

 * @param size   Size of the structure member.

 *

 * @return Value of the structure member promoted into a uint64_t.

/*

 * Wrapper functions are provided for reading/writing the size and

 * next block values as these may not be directly addressible (in 32

 * bit applications, for instance.)  Offsets of data elements in

 * bootmem list, must match cvmx_bootmem_block_header_t.

/*

 * Allocate a block of memory from the free list that was

 * passed to the application by the bootloader within a specified

 * address range. This is an allocate-only algorithm, so

 * freeing memory is not possible. Allocation will fail if

 * memory cannot be allocated in the requested range.

 *

 * @size:      Size in bytes of block to allocate

 * @min_addr:  defines the minimum address of the range

 * @max_addr:  defines the maximum address of the range

 * @alignment: Alignment required - must be power of 2

 * Returns pointer to block of memory, NULL on error

	/* Here we set the global pointer to the bootmem descriptor

	 * block.  This pointer will be used directly, so we will set

	 * it up to be directly usable by the application.  It is set

	 * up as follows for the various runtime/ABI combinations:

	 *

	 * Linux 64 bit: Set XKPHYS bit

	 * Linux 32 bit: use mmap to create mapping, use virtual address

	 * CVMX 64 bit:	 use physical address directly

	 * CVMX 32 bit:	 use physical address directly

	 *

	 * Note that the CVMX environment assumes the use of 1-1 TLB

	 * mappings so that the physical addresses can be used

	 * directly

 Set XKPHYS bit */

/*

 * The cvmx_bootmem_phy* functions below return 64 bit physical

 * addresses, and expose more features that the cvmx_bootmem_functions

 * above.  These are required for full memory space access in 32 bit

 * applications, as well as for using some advance features.  Most

 * applications should not need to use these.

 points to previous list entry, NULL current entry is head of list */

	/*

	 * Do a variety of checks to validate the arguments.  The

	 * allocator code will later assume that these checks have

	 * been made.  We validate that the requested constraints are

	 * not self-contradictory before we look through the list of

	 * available memory.

 0 is not a valid req_size for this allocator */

 Round req_size up to mult of minimum alignment bytes */

	/*

	 * Convert !0 address_min and 0 address_max to special case of

	 * range that specifies an exact memory block to allocate.  Do

	 * this before other checks and adjustments so that this

	 * tranformation will be validated.

 If no limits given, use max limits */

	/*

	 * Enforce minimum alignment (this also keeps the minimum free block

	 * req_size the same as the alignment req_size.

	/*

	 * Adjust address minimum based on requested alignment (round

	 * up to meet alignment).  Do this here so we can reject

	 * impossible requests up front. (NOP for address_min == 0)

	/*

	 * Reject inconsistent args.  We have adjusted these, so this

	 * may fail due to our internal changes even if this check

	 * would pass for the values the user supplied.

 Walk through the list entries - first fit found is returned */

		/*

		 * Determine if this is an entry that can satisify the

		 * request Check to make sure entry is large enough to

		 * satisfy request.

		/*

		 * We should be able to allocate block at address

		 * usable_base.

		/*

		 * Determine if request can be satisfied from the

		 * current entry.

		/*

		 * We have found an entry that has room to satisfy the

		 * request, so allocate it from this entry.  If end

		 * CVMX_BOOTMEM_FLAG_END_ALLOC set, then allocate from

		 * the end of this block rather than the beginning.

			/*

			 * Align desired address down to required

			 * alignment.

 Match at start of entry */

				/*

				 * big enough to create a new block

				 * from top portion of block.

				/*

				 * Adjust next pointer as following

				 * code uses this.

			/*

			 * adjust prev ptr or head to remove this

			 * entry from list.

				/*

				 * head of list being returned, so

				 * update head ptr.

		/*

		 * block returned doesn't start at beginning of entry,

		 * so we know that we will be splitting a block off

		 * the front of this one.  Create a new block from the

		 * beginning, add to list, and go to top of loop

		 * again.

		 *

		 * create new block from high portion of

		 * block, so that top block starts at desired

		 * addr.

 Loop again to handle actual alloc from new block */

 We didn't find anything, so return error */

 zero is invalid */

 0 is not a valid size for this allocator */

 add at front of list - special case with changing head ptr */

 error, overlapping section */

 Add to front of existing first block */

 New block before first block.  OK if cur_addr is 0 */

 Find place in list to add block */

		/*

		 * We have reached the end of the list, add on to end,

		 * checking to see if we need to combine with last

		 * block

		/*

		 * insert between prev and cur nodes, checking for

		 * merge with either/both.

 Merge with previous */

 Also merge with current */

 Merge with current */

 It is a standalone block, add in between prev and cur */

/*

 * Finds a named memory block by name.

 * Also used for finding an unused entry in the named block table.

 *

 * @name: Name of memory block to find.	 If NULL pointer given, then

 *	  finds unused descriptor, if available.

 *

 * @flags: Flags to control options for the allocation.

 *

 * Returns Pointer to memory block descriptor, NULL if not found.

 *	   If NULL returned when name parameter is NULL, then no memory

 *	   block descriptors are available.

	/*

	 * Lock the structure to make sure that it is not being

	 * changed while we are examining it.

 Use XKPHYS for 64 bit linux */

/*

 * Frees a named block.

 *

 * @name:   name of block to free

 * @flags:  flags for passing options

 *

 * Returns 0 on failure

 *	   1 on success

	/*

	 * Take lock here, as name lookup/block free/name free need to

	 * be atomic.

 Set size to zero to indicate block not used. */

 0 on failure, 1 on success */

	/*

	 * Take lock here, as name lookup/block alloc/name add need to

	 * be atomic.

 Get pointer to first available named block descriptor */

	/*

	 * Check to see if name already in use, return error if name

	 * not available or no more room for blocks.

	/*

	 * Round size up to mult of minimum alignment bytes We need

	 * the actual size allocated to allow for blocks to be

	 * coalesced when they are freed. The alloc routine does the

	 * same rounding up on all allocations.

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 *

 * Helper functions for common, but complicated tasks.

 *

 Port count per interface */

/**

 * Return the number of interfaces the chip has. Each interface

 * may have multiple ports. Most chips support two interfaces,

 * but the CNX0XX and CNX1XX are exceptions. These only support

 * one interface.

 *

 * Returns Number of interfaces on chip

/**

 * Return the number of ports on an interface. Depending on the

 * chip and configuration, this can be 1-16. A value of 0

 * specifies that the interface doesn't exist or isn't usable.

 *

 * @interface: Interface to get the port count for

 *

 * Returns Number of ports on interface. Can be Zero.

/**

 * @INTERNAL

 * Return interface mode for CN68xx.

 QLM is disabled when QLM SPD is 15. */

 QLM is disabled when QLM SPD is 15. */

 QLM is disabled when QLM SPD is 15. */

/**

 * @INTERNAL

 * Return interface mode for an Octeon II

 Only present in CN63XX & CN66XX Octeon model */

 QLM2 is SGMII0 and QLM1 is SGMII1 */

/**

 * @INTERNAL

 * Return interface mode for CN7XXX.

 TODO: Implement support for AGL (RGMII). */

/**

 * Get the operating mode of an interface. Depending on the Octeon

 * chip and configuration, this function returns an enumeration

 * of the type of packet I/O supported by an interface.

 *

 * @interface: Interface to probe

 *

 * Returns Mode of the interface. Unknown or unsupported interfaces return

 *	   DISABLED.

	/*

	 * OCTEON III models

	/*

	 * Octeon II models

	/*

	 * Octeon and Octeon Plus models

 Interface 1 is always disabled on CN31XX and CN30XX */

/**

 * Configure the IPD/PIP tagging and QoS options for a specific

 * port. This function determines the POW work queue entry

 * contents for a port. The setup performed here is controlled by

 * the defines in executive-config.h.

 *

 * @ipd_port: Port to configure. This follows the IPD numbering, not the

 *		   per interface numbering

 *

 * Returns Zero on success, negative on failure

 Have each port go to a different POW queue */

 Process the headers and place the IP header in the work queue */

 Put all packets in group 0. Other groups can be used by the app */

/**

 * This function sets the interface_port_count[interface] correctly,

 * without modifying any hardware configuration.  Hardware setup of

 * the ports will be performed later.

 *

 * @interface: Interface to probe

 *

 * Returns Zero on success, negative on failure

 These types don't support ports to IPD/PKO */

 XAUI is a single high speed port */

		/*

		 * RGMII/GMII/MII are all treated about the same. Most

		 * functions refer to these ports as RGMII.

		/*

		 * SPI4 can have 1-16 ports depending on the device at

		 * the other end.

		/*

		 * SGMII can have 1-4 ports depending on how many are

		 * hooked up.

 PCI target Network Packet Interface */

		/*

		 * Special loopback only ports. These are not the same

		 * as other ports in loopback mode.

 Make sure all global variables propagate to other cores */

/**

 * This function probes an interface to determine the actual

 * number of hardware ports connected to it. It doesn't setup the

 * ports or enable them. The main goal here is to set the global

 * interface_port_count[interface] correctly. Hardware setup of the

 * ports will be performed later.

 *

 * @interface: Interface to probe

 *

 * Returns Zero on success, negative on failure

	/* At this stage in the game we don't want packets to be moving yet.

	   The following probe calls should perform hardware setup

 These types don't support ports to IPD/PKO */

 XAUI is a single high speed port */

		/*

		 * RGMII/GMII/MII are all treated about the same. Most

		 * functions refer to these ports as RGMII.

		/*

		 * SPI4 can have 1-16 ports depending on the device at

		 * the other end.

		/*

		 * SGMII can have 1-4 ports depending on how many are

		 * hooked up.

 PCI target Network Packet Interface */

		/*

		 * Special loopback only ports. These are not the same

		 * as other ports in loopback mode.

 Make sure all global variables propagate to other cores */

/**

 * Setup the IPD/PIP for the ports on an interface. Packet

 * classification and tagging are set for every port on the

 * interface. The number of ports on the interface must already

 * have been probed.

 *

 * @interface: Interface to setup IPD/PIP for

 *

 * Returns Zero on success, negative on failure

/**

 * Setup global setting for IPD/PIP not related to a specific

 * interface or port. This must be called before IPD is enabled.

 *

 * Returns Zero on success, negative on failure.

 Setup the global packet input options */

 The +8 is to account for the next ptr */

 The +8 is to account for the next ptr */

/**

 * Setup the PKO for the ports on an interface. The number of

 * queues per port and the priority of each PKO output queue

 * is set here. PKO must be disabled when this function is called.

 *

 * @interface: Interface to setup PKO for

 *

 * Returns Zero on success, negative on failure

	/*

	 * Each packet output queue has an associated priority. The

	 * higher the priority, the more often it can send a packet. A

	 * priority of 8 means it can send in all 8 rounds of

	 * contention. We're going to make each queue one less than

	 * the last.  The vector of priorities has been extended to

	 * support CN5xxx CPUs, where up to 16 queues can be

	 * associated to a port.  To keep backward compatibility we

	 * don't change the initial 8 priorities and replicate them in

	 * the second half.  With per-core PKO queues (PKO lockless

	 * operation) all queues have the same priority.

	/*

	 * Setup the IPD/PIP and PKO for the ports discovered

	 * above. Here packet classification, tagging and output

	 * priorities are set.

/**

 * Setup global setting for PKO not related to a specific

 * interface or port. This must be called before PKO is enabled.

 *

 * Returns Zero on success, negative on failure.

	/*

	 * Disable tagwait FAU timeout. This needs to be done before

	 * anyone might start packet output using tags.

/**

 * Setup global backpressure setting.

 *

 * Returns Zero on success, negative on failure

 Disable backpressure if configured to do so */

 Disable backpressure (pause frame) generation */

/**

 * Enable packet input/output from the hardware. This function is

 * called after all internal setup is complete and IPD is enabled.

 * After this function completes, packets will be accepted from the

 * hardware ports. PKO should still be disabled to make sure packets

 * aren't sent out partially setup hardware.

 *

 * @interface: Interface to enable

 *

 * Returns Zero on success, negative on failure

 These types don't support ports to IPD/PKO */

 Nothing to do */

 XAUI is a single high speed port */

		/*

		 * RGMII/GMII/MII are all treated about the same. Most

		 * functions refer to these ports as RGMII

		/*

		 * SPI4 can have 1-16 ports depending on the device at

		 * the other end

		/*

		 * SGMII can have 1-4 ports depending on how many are

		 * hooked up

 PCI target Network Packet Interface */

		/*

		 * Special loopback only ports. These are not the same

		 * as other ports in loopback mode

/**

 * Function to adjust internal IPD pointer alignments

 *

 * Returns 0 on success

 *	   !0 on failure

 Ports 0-15 are interface 0, 16-31 are interface 1 */

 Save values for restore at end */

 Configure port to gig FDX as required for loopback mode */

	/*

	 * Disable reception on all ports so if traffic is present it

	 * will not interfere.

 Build the PKO command */

 Free packet */

 Return CSR configs to saved values */

/**

 * Called after all internal packet IO paths are setup. This

 * function enables IPD/PIP and begins packet input and output.

 *

 * Returns Zero on success, negative on failure

 Enable IPD */

	/*

	 * Time to enable hardware ports packet input and output. Note

	 * that at this point IPD/PIP must be fully functional and PKO

	 * must be disabled

 Finally enable PKO now that the entire path is up and running */

/**

 * Initialize the PIP, IPD, and PKO hardware to support

 * simple priority based queues for the ethernet ports. Each

 * port is configured with a number of priority queues based

 * on CVMX_PKO_QUEUES_PER_PORT_* where each queue is lower

 * priority than the previous.

 *

 * Returns Zero on success, non-zero on failure

	/*

	 * CN52XX pass 1: Due to a bug in 2nd order CDR, it needs to

	 * be disabled.

	/*

	 * Tell L2 to give the IOB statically higher priority compared

	 * to the cores. This avoids conditions where IO blocks might

	 * be starved under very high L2 loads.

 Enable any flow control and backpressure */

/**

 * Return the link state of an IPD/PKO port as returned by

 * auto negotiation. The result of this function may not match

 * Octeon's link config if auto negotiation has changed since

 * the last call to cvmx_helper_link_set().

 *

 * @ipd_port: IPD/PKO port to query

 *

 * Returns Link state

	/* The default result will be a down link unless the code below

 Network links are not supported */

 Network links are not supported */

/**

 * Configure an IPD/PKO port for the specified link state. This

 * function does not influence auto negotiation at the PHY level.

 * The passed link state must always match the link state returned

 * by cvmx_helper_link_get().

 *

 * @ipd_port:  IPD/PKO port to configure

 * @link_info: The new link state

 *

 * Returns Zero on success, negative on failure

		/*

		 * RGMII/GMII/MII are all treated about the same. Most

		 * functions refer to these ports as RGMII.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2017 Cavium, Inc.

/*

  We install this program at the bootvector:

------------------------------------

	.set noreorder

	.set nomacro

	.set noat

reset_vector:

	dmtc0	$k0, $31, 0	# Save $k0 to DESAVE

	dmtc0	$k1, $31, 3	# Save $k1 to KScratch2



	mfc0	$k0, $12, 0	# Status

	mfc0	$k1, $15, 1	# Ebase



	ori	$k0, 0x84	# Enable 64-bit addressing, set

				# ERL (should already be set)

	andi	$k1, 0x3ff	# mask out core ID



	mtc0	$k0, $12, 0	# Status

	sll	$k1, 5



	lui	$k0, 0xbfc0

	cache	17, 0($0)	# Core-14345, clear L1 Dcache virtual

				# tags if the core hit an NMI



	ld	$k0, 0x78($k0)	# k0 <- (bfc00078) pointer to the reset vector

	synci	0($0)		# Invalidate ICache to get coherent

				# view of target code.



	daddu	$k0, $k0, $k1

	nop



	ld	$k0, 0($k0)	# k0 <- core specific target address

	dmfc0	$k1, $31, 3	# Restore $k1 from KScratch2



	beqz	$k0, wait_loop	# Spin in wait loop

	nop



	jr	$k0

	nop



	nop			# NOPs needed here to fill delay slots

	nop			# on endian reversal of previous instructions



wait_loop:

	wait

	nop



	b	wait_loop

	nop



	nop

	nop

------------------------------------



0000000000000000 <reset_vector>:

   0:	40baf800	dmtc0	k0,c0_desave

   4:	40bbf803	dmtc0	k1,c0_kscratch2



   8:	401a6000	mfc0	k0,c0_status

   c:	401b7801	mfc0	k1,c0_ebase



  10:	375a0084	ori	k0,k0,0x84

  14:	337b03ff	andi	k1,k1,0x3ff



  18:	409a6000	mtc0	k0,c0_status

  1c:	001bd940	sll	k1,k1,0x5



  20:	3c1abfc0	lui	k0,0xbfc0

  24:	bc110000	cache	0x11,0(zero)



  28:	df5a0078	ld	k0,120(k0)

  2c:	041f0000	synci	0(zero)



  30:	035bd02d	daddu	k0,k0,k1

  34:	00000000	nop



  38:	df5a0000	ld	k0,0(k0)

  3c:	403bf803	dmfc0	k1,c0_kscratch2



  40:	13400005	beqz	k0,58 <wait_loop>

  44:	00000000	nop



  48:	03400008	jr	k0

  4c:	00000000	nop



  50:	00000000	nop

  54:	00000000	nop



0000000000000058 <wait_loop>:

  58:	42000020	wait

  5c:	00000000	nop



  60:	1000fffd	b	58 <wait_loop>

  64:	00000000	nop



  68:	00000000	nop

  6c:	00000000	nop



 patch low order 8-bits if no KScratch*/

 patch low order 8-bits if no KScratch*/

 To be filled in with address of vector block*/

 2^10 CPUs */

 KScratch not availble. */

/**

 * Get a pointer to the per-core table of reset vector pointers

 *

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/**

 *

 * Helper utilities for qlm_jtag.

 *

/**

 * Initialize the internal QLM JTAG logic to allow programming

 * of the JTAG chain by the cvmx_helper_qlm_jtag_*() functions.

 * These functions should only be used at the direction of Cavium

 * Networks. Programming incorrect values into the JTAG chain

 * can cause chip damage.

 Convert the divisor into a power of 2 shift */

	/*

	 * Clock divider for QLM JTAG operations.  eclk is divided by

	 * 2^(CLK_DIV + 2)

/**

 * Write up to 32bits into the QLM jtag chain. Bits are shifted

 * into the MSB and out the LSB, so you should shift in the low

 * order bits followed by the high order bits. The JTAG chain is

 * 4 * 268 bits long, or 1072.

 *

 * @qlm:    QLM to shift value into

 * @bits:   Number of bits to shift in (1-32).

 * @data:   Data to shift in. Bit 0 enters the chain first, followed by

 *		 bit 1, etc.

 *

 * Returns The low order bits of the JTAG chain that shifted out of the

 *	   circle.

/**

 * Shift long sequences of zeros into the QLM JTAG chain. It is

 * common to need to shift more than 32 bits of zeros into the

 * chain. This function is a convience wrapper around

 * cvmx_helper_qlm_jtag_shift() to shift more than 32 bits of

 * zeros at a time.

 *

 * @qlm:    QLM to shift zeros into

 * @bits:

/**

 * Program the QLM JTAG chain into all lanes of the QLM. You must

 * have already shifted in 268*4, or 1072 bits into the JTAG

 * chain. Updating invalid values can possibly cause chip damage.

 *

 * @qlm:    QLM to program

 Update the new data */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Support library for the hardware Packet Output unit.

/*

 * Internal state of packet output

	/*

	 * Initialize every iport with the invalid eid.

 Invalid */

	/*

	 * Set up PKO_MEM_IPORT_PTRS

	/*

	 * Initialize queues

/*

 * Call before any other calls to initialize the packet

 * output system.  This does chip global config, and should only be

 * done by one core.

	/*

	 * Set the size of the PKO command buffers to an odd number of

	 * 64bit words. This allows the normal two word send to stay

	 * aligned and never span a command word buffer.

	/*

	 * Chip-specific setup.

	/*

	 * If we aren't using all of the queues optimize PKO's

	 * internal memory.

/*

 * Enables the packet output hardware. It must already be

 * configured.

	/*

	 * always enable big endian for 3-word command. Does nothing

	 * for 2-word.

/*

 * Disables the packet output. Does not affect any configuration.

/*

 * Reset the packet output.

/*

 * Shutdown and free resources required by packet output.

/*

 * Configure a output port and the associated queues for use.

 *

 * @port:	Port to configure.

 * @base_queue: First queue number to associate with this port.

 * @num_queues: Number of queues to associate with this port

 * @priority:	Array of priority levels for each queue. Values are

 *		     allowed to be 0-8. A value of 8 get 8 times the traffic

 *		     of a value of 1.  A value of 0 indicates that no rounds

 *		     will be participated in. These priorities can be changed

 *		     on the fly while the pko is enabled. A priority of 9

 *		     indicates that static priority should be used.  If static

 *		     priority is used all queues with static priority must be

 *		     contiguous starting at the base_queue, and lower numbered

 *		     queues have higher priority than higher numbered queues.

 *		     There must be num_queues elements in the array.

		/*

		 * Validate the static queue priority setup and set

		 * static_priority_base and static_priority_end

		 * accordingly.

 Find first queue of static priority */

 Find last queue of static priority */

 all queues are static priority */

			/*

			 * Check to make sure all static priority

			 * queues are contiguous.  Also catches some

			 * cases of static priorites not starting at

			 * queue 0.

	/*

	 * At this point, static_priority_base and static_priority_end

	 * are either both -1, or are valid start/end queue

	 * numbers.

		/*

		 * Convert the priority into an enable bit field. Try

		 * to space the bits out evenly so the packet don't

		 * get grouped up

 to the error case, when Pass 1 */

/*

 * Show map of ports -> queues for different cores.

/*

 * Rate limit a PKO port to a max packets/sec. This function is only

 * supported on CN51XX and higher, excluding CN58XX.

 *

 * @port:      Port to rate limit

 * @packets_s: Maximum packet/sec

 * @burst:     Maximum number of packets to burst in a row before rate

 *		    limiting cuts in.

 *

 * Returns Zero on success, negative on failure

 No cost per word since we are limited by packets/sec, not bits/sec */

/*

 * Rate limit a PKO port to a max bits/sec. This function is only

 * supported on CN51XX and higher, excluding CN58XX.

 *

 * @port:   Port to rate limit

 * @bits_s: PKO rate limit in bits/sec

 * @burst:  Maximum number of bits to burst before rate

 *		 limiting cuts in.

 *

 * Returns Zero on success, negative on failure

	/*

	 * Each packet has a 12 bytes of interframe gap, an 8 byte

	 * preamble, and a 4 byte CRC. These are not included in the

	 * per word count. Multiply by 8 to covert to bits and divide

	 * by 256 for limit granularity.

 Each 8 byte word has 64bits */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Support functions for managing command queues used for

 * various hardware blocks.

/*

 * This application uses this pointer to access the global queue

 * state. It points to a bootmem named block.

/*

 * Initialize the Global queue state pointer.

 *

 * Returns CVMX_CMD_QUEUE_SUCCESS or a failure code

/*

 * Initialize a command queue for use. The initial FPA buffer is

 * allocated and the hardware unit is configured to point to the

 * new command queue.

 *

 * @queue_id:  Hardware command queue to initialize.

 * @max_depth: Maximum outstanding commands that can be queued.

 * @fpa_pool:  FPA pool the command queues should come from.

 * @pool_size: Size of each buffer in the FPA pool (bytes)

 *

 * Returns CVMX_CMD_QUEUE_SUCCESS or a failure code

	/*

	 * We artificially limit max_depth to 1<<20 words. It is an

	 * arbitrary limit.

 See if someone else has already initialized the queue */

		/*

		 * We zeroed the now serving field so we need to also

		 * zero the ticket.

/*

 * Shutdown a queue a free it's command buffers to the FPA. The

 * hardware connected to the queue must be stopped before this

 * function is called.

 *

 * @queue_id: Queue to shutdown

 *

 * Returns CVMX_CMD_QUEUE_SUCCESS or a failure code

/*

 * Return the number of command words pending in the queue. This

 * function may be relatively slow for some hardware units.

 *

 * @queue_id: Hardware command queue to query

 *

 * Returns Number of outstanding commands

	/*

	 * The cast is here so gcc with check that all values in the

	 * cvmx_cmd_queue_id_t enumeration are here.

		/*

		 * FIXME: Need atomic lock on

		 * CVMX_PKO_REG_READ_IDX. Right now we are normally

		 * called with the queue lock, so that is a SLIGHT

		 * amount of protection.

 FIXME: Implement other lengths */

/*

 * Return the command buffer to be written to. The purpose of this

 * function is to allow CVMX routine access t othe low level buffer

 * for initial hardware setup. User applications should not call this

 * function directly.

 *

 * @queue_id: Command queue to query

 *

 * Returns Command buffer or NULL on failure

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Utility functions to decode Octeon's RSL_INT_BLOCKS

 * interrupts into error messages.

/**

 * Enable ASX error interrupts that exist on CN3XXX, CN50XX, and

 * CN58XX.

 *

 * @block:  Interface to enable 0-1

	/*

	 * CN38XX and CN58XX have two interfaces with 4 ports per

	 * interface. All other chips have a max of 3 ports on

	 * interface 0

 Set enables for 4 ports */

 Set enables for 3 ports */

 Enable interface interrupts */

/**

 * Enable GMX error reporting for the supplied interface

 *

 * @interface: Interface to enable

 XAUI */

 SGMII */

 PICMG */

 Disabled */

				/*

				 * SPI on CN38XX and CN58XX report all

				 * errors through port 0.  RGMII needs

				 * to check all 4 ports

				/*

				 * CN30XX, CN31XX, and CN50XX have two

				 * or three ports. GMII and MII has 2,

				 * RGMII has three

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Small helper utilities.

/**

 * Convert a interface mode into a human readable string

 *

 * @mode:   Mode to convert

 *

 * Returns String

/**

 * Setup Random Early Drop on a specific input queue

 *

 * @queue:  Input queue to setup RED on (0-7)

 * @pass_thresh:

 *		 Packets will begin slowly dropping when there are less than

 *		 this many packet buffers free in FPA 0.

 * @drop_thresh:

 *		 All incoming packets will be dropped when there are less

 *		 than this many free packet buffers in FPA 0.

 * Returns Zero on success. Negative on failure

	/* Set RED to begin dropping packets when there are pass_thresh buffers

	   left. It will linearly drop more packets until reaching drop_thresh

 Use the actual queue 0 counter, not the average */

/**

 * Setup Random Early Drop to automatically begin dropping packets.

 *

 * @pass_thresh:

 *		 Packets will begin slowly dropping when there are less than

 *		 this many packet buffers free in FPA 0.

 * @drop_thresh:

 *		 All incoming packets will be dropped when there are less

 *		 than this many free packet buffers in FPA 0.

 * Returns Zero on success. Negative on failure

 Disable backpressure based on queued buffers. It needs SW support */

	/* Shutoff the dropping based on the per port page count. SW isn't

/**

 * Setup the common GMX settings that determine the number of

 * ports. These setting apply to almost all configurations of all

 * chips.

 *

 * @interface: Interface to configure

 * @num_ports: Number of ports on the interface

 *

 * Returns Zero on success, negative on failure

 Tell GMX the number of TX ports on this interface */

	/* Tell GMX the number of RX ports on this interface.  This only

 Skip setting CVMX_PKO_REG_GMX_PORT_MODE on 30XX, 31XX, and 50XX */

 Tell PKO the number of ports on this interface */

	/*

	 * Set GMX to buffer as much data as possible before starting

	 * transmit.  This reduces the chances that we have a TX under

	 * run due to memory contention. Any packet that fits entirely

	 * in the GMX FIFO can never have an under run regardless of

	 * memory load.

 These chips have a fixed max threshold of 0x40 */

 Choose the max value for the number of ports */

	/*

	 * SPI and XAUI can have lots of ports but the GMX hardware

	 * only ever has a max of 4.

/**

 * Returns the IPD/PKO port number for a port on the given

 * interface.

 *

 * @interface: Interface to use

 * @port:      Port on the interface

 *

 * Returns IPD/PKO port number

/**

 * Returns the interface number for an IPD/PKO port number.

 *

 * @ipd_port: IPD/PKO port number

 *

 * Returns Interface number

/**

 * Returns the interface index number for an IPD/PKO port

 * number.

 *

 * @ipd_port: IPD/PKO port number

 *

 * Returns Interface index number

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/**

 *

 * Fixes and workaround for Octeon chip errata. This file

 * contains functions called by cvmx-helper to workaround known

 * chip errata. For the most part, code doesn't need to call

 * these functions directly.

 *

/**

 * Due to errata G-720, the 2nd order CDR circuit on CN52XX pass

 * 1 doesn't work properly. The following code disables 2nd order

 * CDR for the specified QLM.

 *

 * @qlm:    QLM to disable 2nd order CDR for.

 We need to load all four lanes of the QLM, a total of 1072 bits */

		/*

		 * Each lane has 268 bits. We need to set

		 * cfg_cdr_incx<67:64> = 3 and cfg_cdr_secord<77> =

		 * 1. All other bits are zero. Bits go in LSB first,

		 * so start off with the zeros for bits <63:0>.

 cfg_cdr_incx<67:64>=3 */

 Zeros for bits <76:68> */

 cfg_cdr_secord<77>=1 */

 Zeros for bits <267:78> */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (C) 2003-2018 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for SPI initialization, configuration,

 * and monitoring.

/*

 * CVMX_HELPER_SPI_TIMEOUT is used to determine how long the SPI

 * initialization routines wait for SPI training. You can override the

 * value using executive-config.h if necessary.

/**

 * Probe a SPI interface and determine the number of ports

 * connected to it. The SPI interface should still be down after

 * this call.

 *

 * @interface: Interface to probe

 *

 * Returns Number of ports on the interface. Zero to disable.

		/*

		 * Unlike the SPI4000, most SPI devices don't

		 * automatically put on the L2 CRC. For everything

		 * except for the SPI4000 have PKO append the L2 CRC

		 * to the packet.

/**

 * Bringup and enable a SPI interface. After this call packet I/O

 * should be fully functional. This is called with IPD enabled but

 * PKO disabled.

 *

 * @interface: Interface to bring up

 *

 * Returns Zero on success, negative on failure

	/*

	 * Normally the ethernet L2 CRC is checked and stripped in the

	 * GMX block.  When you are using SPI, this isn' the case and

	 * IPD needs to check the L2 CRC.

/**

 * Return the link state of an IPD/PKO port as returned by

 * auto negotiation. The result of this function may not match

 * Octeon's link config if auto negotiation has changed since

 * the last call to cvmx_helper_link_set().

 *

 * @ipd_port: IPD/PKO port to query

 *

 * Returns Link state

 The simulator gives you a simulated full duplex link */

 10 Mbps */

 100 Mbps */

 1 Gbps */

 Illegal */

		/* For generic SPI we can't determine the link, just return some

/**

 * Configure an IPD/PKO port for the specified link state. This

 * function does not influence auto negotiation at the PHY level.

 * The passed link state must always match the link state returned

 * by cvmx_helper_link_get().

 *

 * @ipd_port:  IPD/PKO port to configure

 * @link_info: The new link state

 *

 * Returns Zero on success, negative on failure

	/* Nothing to do. If we have a SPI4000 then the setup was already performed

	   by cvmx_spi4000_check_speed(). If not then there isn't any link

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (C) 2003-2018 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for SGMII initialization, configuration,

 * and monitoring.

/**

 * Perform initialization required only once for an SGMII port.

 *

 * @interface: Interface to init

 * @index:     Index of prot on the interface

 *

 * Returns Zero on success, negative on failure

 Disable GMX */

	/*

	 * Write PCS*_LINK*_TIMER_COUNT_REG[COUNT] with the

	 * appropriate value. 1000BASE-X specifies a 10ms

	 * interval. SGMII specifies a 1.6ms interval.

 1000BASE-X */

 SGMII */

	/*

	 * Write the advertisement register to be used as the

	 * tx_Config_Reg<D15:D0> of the autonegotiation.  In

	 * 1000BASE-X mode, tx_Config_Reg<D15:D0> is PCS*_AN*_ADV_REG.

	 * In SGMII PHY mode, tx_Config_Reg<D15:D0> is

	 * PCS*_SGM*_AN_ADV_REG.  In SGMII MAC mode,

	 * tx_Config_Reg<D15:D0> is the fixed value 0x4001, so this

	 * step can be skipped.

 1000BASE-X */

 PHY Mode */

 MAC Mode - Nothing to do */

/**

 * Initialize the SERTES link for the first time or after a loss

 * of link.

 *

 * @interface: Interface to init

 * @index:     Index of prot on the interface

 *

 * Returns Zero on success, negative on failure

	/*

	 * Take PCS through a reset sequence.

	 * PCS*_MR*_CONTROL_REG[PWR_DN] should be cleared to zero.

	 * Write PCS*_MR*_CONTROL_REG[RESET]=1 (while not changing the

	 * value of the other PCS*_MR*_CONTROL_REG bits).  Read

	 * PCS*_MR*_CONTROL_REG[RESET] until it changes value to

	 * zero.

	/*

	 * Write PCS*_MR*_CONTROL_REG[RST_AN]=1 to ensure a fresh

	 * sgmii negotiation starts.

	/*

	 * Wait for PCS*_MR*_STATUS_REG[AN_CPT] to be set, indicating

	 * that sgmii autonegotiation is complete. In MAC mode this

	 * isn't an ethernet link, but a link between Octeon and the

	 * PHY.

 cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index); */

/**

 * Configure an SGMII link to the specified speed after the SERTES

 * link is up.

 *

 * @interface: Interface to init

 * @index:     Index of prot on the interface

 * @link_info: Link state to configure

 *

 * Returns Zero on success, negative on failure

 Disable GMX before we make any changes. Remember the enable state */

 Wait for GMX to be idle */

 Read GMX CFG again to make sure the disable completed */

	/*

	 * Get the misc control for PCS. We will need to set the

	 * duplication amount.

	/*

	 * Use GMXENO to force the link down if the status we get says

	 * it should be down.

 Only change the duplex setting if the link is up */

 Do speed based setting for GMX */

 Setting from GMX-603 */

 Write the new misc control for PCS */

 Write the new GMX settings with the port still disabled */

 Read GMX CFG again to make sure the config completed */

 Restore the enabled / disabled state */

/**

 * Bring up the SGMII interface to be ready for packet I/O but

 * leave I/O disabled using the GMX override. This function

 * follows the bringup documented in 10.6.3 of the manual.

 *

 * @interface: Interface to bringup

 * @num_ports: Number of ports on the interface

 *

 * Returns Zero on success, negative on failure

		/* Linux kernel driver will call ....link_set with the

		 * proper link state. In the simulator there is no

		 * link state polling and hence it is set from

		 * here.

/**

 * Probe a SGMII interface and determine the number of ports

 * connected to it. The SGMII interface should still be down after

 * this call.

 *

 * @interface: Interface to probe

 *

 * Returns Number of ports on the interface. Zero to disable.

	/*

	 * Due to errata GMX-700 on CN56XXp1.x and CN52XXp1.x, the

	 * interface needs to be enabled before IPD otherwise per port

	 * backpressure may not work properly

/**

 * Bringup and enable a SGMII interface. After this call packet

 * I/O should be fully functional. This is called with IPD

 * enabled but PKO disabled.

 *

 * @interface: Interface to bring up

 *

 * Returns Zero on success, negative on failure

/**

 * Return the link state of an IPD/PKO port as returned by

 * auto negotiation. The result of this function may not match

 * Octeon's link config if auto negotiation has changed since

 * the last call to cvmx_helper_link_set().

 *

 * @ipd_port: IPD/PKO port to query

 *

 * Returns Link state

 The simulator gives you a simulated 1Gbps full duplex link */

 Force 1Gbps full duplex link for internal loopback */

 1000BASE-X */

 FIXME */

 PHY Mode */

			/*

			 * Don't bother continuing if the SERTES low

			 * level link is down

 Read the autoneg results */

				/*

				 * Auto negotiation is complete. Set

				 * status accordingly.

				/*

				 * Auto negotiation isn't

				 * complete. Return link down.

 MAC Mode */

/**

 * Configure an IPD/PKO port for the specified link state. This

 * function does not influence auto negotiation at the PHY level.

 * The passed link state must always match the link state returned

 * by cvmx_helper_link_get().

 *

 * @ipd_port:  IPD/PKO port to configure

 * @link_info: The new link state

 *

 * Returns Zero on success, negative on failure

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * This module provides system/board/application information obtained

 * by the bootloader.

/*

 * This structure defines the private state maintained by sysinfo module.

 system information */

/*

 * Returns the application information as obtained

 * by the bootloader.  This provides the core mask of the cores

 * running the same application image, as well as the physical

 * memory regions available to the core.

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2017 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/**

 * Read a byte of fuse data

 * @byte_addr:	 address to read

 *

 * Returns fuse value: 0 or 1

/*

 * Version of octeon_model_get_string() that takes buffer as argument,

 * as running early in u-boot static/global variables don't work when

 * running from flash.

 Make sure the non existent devices look disabled */

 CN50XX */

 CN30XX */

 CN57XX or CN56XX */

 Make a guess at the suffix */

 NSP = everything */

 EXP = No crypto */

 SCP = No DFA, No zip */

 CP = No DFA, No crypto, No zip */

	/*

	 * Assume pass number is encoded using <5:3><2:0>. Exceptions

	 * will be fixed later.

	/*

	 * Use the number of cores to determine the last 2 digits of

	 * the model number. There are some exceptions that are fixed

	 * later.

 Now figure out the family, the first two digits */

 CN38XX, CN37XX or CN36XX */

			/*

			 * For some unknown reason, the 16 core one is

			 * called 37 instead of 36.

		/*

		 * This series of chips didn't follow the standard

		 * pass numbering.

 CN31XX or CN3020 */

		/*

		 * This series of chips didn't follow the standard

		 * pass numbering.

 CN3010 or CN3005 */

 A chip with half cache is an 05 */

		/*

		 * This series of chips didn't follow the standard

		 * pass numbering.

 CN58XX */

 Special case. 4 core, half cache (CP with half cache) */

 Pass 1 uses different encodings for pass numbers */

 CN57XX, CN56XX, CN55XX, CN54XX */

 CN50XX */

 CN52XX */

 CN61XX */

 CN63XX */

 Other core counts match generic */

 CN66XX */

 Other core counts match generic */

 CN68XX */

 CNF71XX */

 CN78XX */

 Other core counts match generic */

 CN70XX */

 CN73XX */

 Other core counts match generic */

 CN75XX */

 Check for model in fuses, overrides normal decode */

 This is _not_ valid for Octeon CN3XXX models */

 Have both number and suffix in fuses, so both */

 Only have suffix, so add suffix to 'normal' model number */

 Don't have suffix, so just use model from fuses */

/**

 * Given the chip processor ID from COP0, this function returns a

 * string representing the chip model number. The string is of the

 * form CNXXXXpX.X-FREQ-SUFFIX.

 * - XXXX = The chip model number

 * - X.X = Chip pass number

 * - FREQ = Current frequency in Mhz

 * - SUFFIX = NSP, EXP, SCP, SSP, or CP

 *

 * @chip_id: Chip ID

 *

 * Returns Model string

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for LOOP initialization, configuration,

 * and monitoring.

/**

 * Probe a LOOP interface and determine the number of ports

 * connected to it. The LOOP interface should still be down

 * after this call.

 *

 * @interface: Interface to probe

 *

 * Returns Number of ports on the interface. Zero to disable.

	/* We need to disable length checking so packet < 64 bytes and jumbo

 Disable FCS stripping for loopback ports */

/**

 * Bringup and enable a LOOP interface. After this call packet

 * I/O should be fully functional. This is called with IPD

 * enabled but PKO disabled.

 *

 * @interface: Interface to bring up

 *

 * Returns Zero on success, negative on failure

 Do nothing. */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2009 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 *

 * Automatically generated functions useful for enabling

 * and decoding RSL_INT_BLOCKS interrupts.

 *

/**

 * __cvmx_interrupt_gmxx_rxx_int_en_enable - enable all interrupt bits in cvmx_gmxx_rxx_int_en_t

 * @index: interrupt register offset

 * @block: interrupt register block_id

 Skipping gmx_rx_int_en.s.reserved_29_63 */

 Skipping gmx_rx_int_en.s.reserved_16_18 */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Skipping gmx_rx_int_en.s.reserved_9_9 */

 Skipping gmx_rx_int_en.s.reserved_5_6 */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_2_2 */

 Skipping gmx_rx_int_en.s.reserved_0_0 */

 Skipping gmx_rx_int_en.s.reserved_19_63 */

gmx_rx_int_en.s.phy_dupx = 1; */

gmx_rx_int_en.s.phy_spd = 1; */

gmx_rx_int_en.s.phy_link = 1; */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Length errors are handled when we get work */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_20_63 */

gmx_rx_int_en.s.phy_dupx = 1; */

gmx_rx_int_en.s.phy_spd = 1; */

gmx_rx_int_en.s.phy_link = 1; */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Skipping gmx_rx_int_en.s.reserved_6_6 */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_2_2 */

 Skipping gmx_rx_int_en.s.reserved_0_0 */

 Skipping gmx_rx_int_en.s.reserved_19_63 */

gmx_rx_int_en.s.phy_dupx = 1; */

gmx_rx_int_en.s.phy_spd = 1; */

gmx_rx_int_en.s.phy_link = 1; */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Length errors are handled when we get work */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_19_63 */

gmx_rx_int_en.s.phy_dupx = 1; */

gmx_rx_int_en.s.phy_spd = 1; */

gmx_rx_int_en.s.phy_link = 1; */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Length errors are handled when we get work */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_20_63 */

gmx_rx_int_en.s.phy_dupx = 1; */

gmx_rx_int_en.s.phy_spd = 1; */

gmx_rx_int_en.s.phy_link = 1; */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Length errors are handled when we get work */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_29_63 */

 Skipping gmx_rx_int_en.s.reserved_16_18 */

gmx_rx_int_en.s.ifgerr = 1; */

 Collision detect */

 False carrier error or extend error after slottime */

 RGMII reserved opcodes */

 Bad Preamble / Protocol */

 Skipping gmx_rx_int_en.s.reserved_9_9 */

 Skipping gmx_rx_int_en.s.reserved_5_6 */

 FCS errors are handled when we get work */

 Skipping gmx_rx_int_en.s.reserved_2_2 */

 Skipping gmx_rx_int_en.s.reserved_0_0 */

/**

 * __cvmx_interrupt_pcsx_intx_en_reg_enable - enable all interrupt bits in cvmx_pcsx_intx_en_reg_t

 * @index: interrupt register offset

 * @block: interrupt register block_id

 Skipping pcs_int_en_reg.s.reserved_12_63 */

 This happens during normal operation */

 This happens during normal operation */

 This happens during normal operation */

 This happens during normal operation */

 Skipping pcs_int_en_reg.s.reserved_12_63 */

 This happens during normal operation */

 This happens during normal operation */

 This happens during normal operation */

 This happens during normal operation */

/**

 * __cvmx_interrupt_pcsxx_int_en_reg_enable - enable all interrupt bits in cvmx_pcsxx_int_en_reg_t

 * @index: interrupt register block_id

 Skipping pcsx_int_en_reg.s.reserved_6_63 */

 Skipping pcsx_int_en_reg.s.reserved_6_63 */

 Happens if XAUI module is not installed */

/**

 * __cvmx_interrupt_spxx_int_msk_enable - enable all interrupt bits in cvmx_spxx_int_msk_t

 * @index: interrupt register block_id

 Skipping spx_int_msk.s.reserved_12_63 */

 Skipping spx_int_msk.s.reserved_2_3 */

 Skipping spx_int_msk.s.reserved_12_63 */

 Skipping spx_int_msk.s.reserved_2_3 */

/**

 * __cvmx_interrupt_stxx_int_msk_enable - enable all interrupt bits in cvmx_stxx_int_msk_t

 * @index: interrupt register block_id

 Skipping stx_int_msk.s.reserved_8_63 */

 Skipping stx_int_msk.s.reserved_8_63 */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 *

 * Helper functions to abstract board specific data about

 * network ports from the rest of the cvmx-helper files.

/*

 * Return the MII PHY address associated with the given IPD

 * port. A result of -1 means there isn't a MII capable PHY

 * connected to this port. On chips supporting multiple MII

 * busses the bus number is encoded in bits <15:8>.

 *

 * This function must be modified for every new Octeon board.

 * Internally it uses switch statements based on the cvmx_sysinfo

 * data to determine board types and revisions. It replies on the

 * fact that every Octeon board receives a unique board type

 * enumeration from the bootloader.

 *

 * @ipd_port: Octeon IPD port to get the MII address for.

 *

 * Returns MII PHY address and bus number or -1.

 Simulator doesn't have MII */

 Interface 0 is SPI4, interface 1 is RGMII */

		/*

		 * Port 0 is WAN connected to a PHY, Port 1 is GMII

		 * connected to a switch

 Board has 8 RGMII ports PHYs are 0-7 */

 Board has dual SPI4 and no PHYs */

 Board has 2 management ports */

		/*

		 * Board has 4 SGMII ports. The PHYs start right after the MII

		 * ports MII0 = 0, MII1 = 1, SGMII = 2-5.

 Board has 1 management port */

		/*

		 * Board has 8 SGMII ports. 4 connect out, two connect

		 * to a switch, and 2 loop to each other

 Board has 4 SGMII ports. connected QLM3(interface 1) */

		/*

		 * No PHYs are connected to Octeon, everything is

		 * through switch.

 Some unknown board. Somebody forgot to update this function... */

/*

 * This function is the board specific method of determining an

 * ethernet ports link speed. Most Octeon boards have Marvell PHYs

 * and are handled by the fall through case. This function must be

 * updated for boards that don't have the normal Marvell PHYs.

 *

 * This function must be modified for every new Octeon board.

 * Internally it uses switch statements based on the cvmx_sysinfo

 * data to determine board types and revisions. It relies on the

 * fact that every Octeon board receives a unique board type

 * enumeration from the bootloader.

 *

 * @ipd_port: IPD input port associated with the port we want to get link

 *		   status for.

 *

 * Returns The ports link status. If the link isn't fully resolved, this must

 *	   return zero.

 Unless we fix it later, all links are defaulted to down */

 The simulator gives you a simulated 1Gbps full duplex link */

		/*

		 * We don't have a PHY address, so attempt to use

		 * in-band status. It is really important that boards

		 * not supporting in-band status never get

		 * here. Reading broken in-band status tends to do bad

		 * things

 10 Mbps */

 100 Mbps */

 1 Gbps */

 Illegal */

		/*

		 * We don't have a PHY address and we don't have

		 * in-band status. There is no way to determine the

		 * link speed. Return down assuming this port isn't

		 * wired

 If link is down, return all fields as zero. */

/*

 * This function is called by cvmx_helper_interface_probe() after it

 * determines the number of ports Octeon can support on a specific

 * interface. This function is the per board location to override

 * this value. It is called with the number of ports Octeon might

 * support and should return the number of actual ports on the

 * board.

 *

 * This function must be modifed for every new Octeon board.

 * Internally it uses switch statements based on the cvmx_sysinfo

 * data to determine board types and revisions. It relys on the

 * fact that every Octeon board receives a unique board type

 * enumeration from the bootloader.

 *

 * @interface: Interface to probe

 * @supported_ports:

 *		    Number of ports Octeon supports.

 *

 * Returns Number of ports the actual board supports. Many times this will

 *	   simple be "support_ports".

		/* The 2nd interface on the EBH5600 is connected to the Marvel switch,

/*

 * Get the clock type used for the USB block based on board type.

 * Used by the USB code for auto configuration of clock type.

 *

 * Return USB clock type enumeration

 Most boards except NIC10e use a 12MHz crystal */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2017 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Implementation of the Level 2 Cache (L2C) control,

 * measurement, and debugging facilities.

/*

 * This spinlock is used internally to ensure that only one core is

 * performing certain L2 operations at a time.

 *

 * NOTE: This only protects calls from within a single application -

 * if multiple applications or operating systems are running, then it

 * is up to the user program to coordinate between them.

 Validate the core number */

	/*

	 * Use the lower two bits of the coreNumber to determine the

	 * bit offset of the UMSK[] field in the L2C_SPAR register.

	/*

	 * Return the UMSK[] field from the appropriate L2C_SPAR

	 * register based on the coreNumber.

 A UMSK setting which blocks all L2C Ways is an error on some chips */

 Validate the core number */

	/*

	 * Use the lower two bits of core to determine the bit offset of the

	 * UMSK[] field in the L2C_SPAR register.

	/*

	 * Assign the new mask setting to the UMSK[] field in the appropriate

	 * L2C_SPAR register based on the core_num.

	 *

 A UMSK setting which blocks all L2C Ways is an error on some chips */

/*

 * @INTERNAL

 * Helper function use to fault in cache lines for L2 cache locking

 *

 * @addr:   Address of base of memory region to read into L2 cache

 * @len:    Length (in bytes) of region to fault in

	/*

	 * Adjust addr and length so we get all cache lines even for

	 * small ranges spanning two cache lines.

	/*

	 * Invalidate L1 cache to make sure all loads result in data

	 * being in L2.

 Make sure we were able to lock the line */

 make sure CVMX_L2C_TADX_TAG is updated */

 Check if a valid line is found */

 cvmx_dprintf("ERROR: cvmx_l2c_lock_line: line not found for locking at 0x%llx address\n", (unsigned long long)addr); */

 Check if lock bit is not set */

 cvmx_dprintf("ERROR: cvmx_l2c_lock_line: Not able to lock at 0x%llx address\n", (unsigned long long)addr); */

 Clear l2t error bits if set */

 Set this core as debug core */

 Only lock 1 line at a time */

 Make sure it gets there */

 Make sure it gets there */

 Stop being debug core */

 We were unable to lock the line */

 Round start/end to cache line boundaries */

 These may look like constants, but they aren't... */

		/*

		 * For 63XX, we can flush a line by using the physical

		 * address directly, so finding the cache line used by

		 * the address is only required to provide the proper

		 * return value for the function.

 Compute portion of address that is stored in tag */

 Round start/end to cache line boundaries */

/*

 * Internal l2c tag types.  These are converted to a generic structure

 * that can be used on all chips.

 Line valid */

 Line dirty */

 Line locked */

 Use, LRU eviction */

 Phys addr (33..14) */

 Line valid */

 Line dirty */

 Line locked */

 Use, LRU eviction */

 Phys addr (33..15) */

 Line valid */

 Line dirty */

 Line locked */

 Use, LRU eviction */

 Phys addr (33..16) */

 Line valid */

 Line dirty */

 Line locked */

 Use, LRU eviction */

 Phys addr (33..17) */

 Line valid */

 Line dirty */

 Line locked */

 Use, LRU eviction */

 Phys addr (33..18) */

 2048 sets */

 512 sets */

/*

 * @INTERNAL

 * Function to read a L2C tag.  This code make the current core

 * the 'debug core' for the L2.  This code must only be executed by

 * 1 core at a time.

 *

 * @assoc:  Association (way) of the tag to dump

 * @index:  Index of the cacheline

 *

 * Returns The Octeon model specific tag structure.  This is

 *	   translated by a wrapper function to a generic form that is

 *	   easier for applications to use.

	/*

	 * For low core count parts, the core number is always small

	 * enough to stay in the correct field and not set any

	 * reserved bits.

	/*

	 * Make sure core is quiet (no prefetches, etc.) before

	 * entering debug mode.

 Flush L1 to make sure debug load misses L1 */

	/*

	 * The following must be done in assembly as when in debug

	 * mode all data loads from L2 return special debug data, not

	 * normal memory contents.  Also, interrupts must be disabled,

	 * since if an interrupt occurs while in debug mode the ISR

	 * will get debug data from all its memory * reads instead of

	 * the contents of memory.

 Enter debug mode, wait for store */

 Read L2C tag data */

 Exit debug mode, wait for store */

 Invalidate dcache to discard debug data */

		/*

		 * Use L2 cache Index load tag cache instruction, as

		 * hardware loads the virtual tag for the L2 cache

		 * block with the contents of L2C_TAD0_TAG

		 * register.

 make sure CVMX_L2C_TADX_TAG is updated */

 __read_l2_tag is intended for internal use only */

		/*

		 * Convert all tag structure types to generic version,

		 * as it can represent all models.

/*

 * Return log base 2 of the number of sets in the L2 cache

 2048 sets */

 1024 sets */

 512 sets */

 256 sets */

 128 sets */

 2048 sets */

 Return the number of sets in the L2 Cache */

 Return the number of associations in the L2 Cache */

 Check to see if part of the cache is disabled */

		/*

		 * cvmx_mio_fus_dat3.s.l2c_crip fuses map as follows

		 * <2> will be not used for 63xx

		 * <1> disables 1/2 ways

		 * <0> disables 1/4 ways

		 * They are cumulative, so for 63xx:

		 * <1> <0>

		 * 0 0 16-way 2MB cache

		 * 0 1 12-way 1.5MB cache

		 * 1 0 8-way 1MB cache

		 * 1 1 4-way 512KB cache

		/*

		 * Using shifts here, as bit position names are

		 * different for each model but they all mean the

		 * same.

/*

 * Flush a line from the L2 cache

 * This should only be called from one core at a time, as this routine

 * sets the core to the 'debug' core in order to flush the line.

 *

 * @assoc:  Association (or way) to flush

 * @index:  Index to flush

 Check the range of the index. */

 Check the range of association. */

		/* Create the address based on index and association.

		 * Bits<20:17> select the way of the cache block involved in

		 *	       the operation

		 * Bits<16:7> of the effect address select the index

		/*

		 * Enter debug mode, and make sure all other writes

		 * complete before we enter debug mode

 Exit debug mode */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (C) 2003-2018 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for RGMII/GMII/MII initialization, configuration,

 * and monitoring.

/*

 * Probe RGMII ports and determine the number present

 *

 * @interface: Interface to probe

 *

 * Returns Number of RGMII/GMII/MII ports (0-4).

			/*

			 * On these chips "type" says we're in

			 * GMII/MII mode. This limits us to 2 ports

/*

 * Put an RGMII interface in loopback mode. Internal packets sent

 * out will be received back again on the same port. Externally

 * received packets will echo back out.

 *

 * @port:   IPD port number to loop.

/*

 * Workaround ASX setup errata with CN38XX pass1

 *

 * @interface: Interface to setup

 * @port:      Port to setup (0..3)

 * @cpu_clock_hz:

 *		    Chip frequency in Hertz

 *

 * Returns Zero on success, negative on failure

 Set hi water mark as per errata GMX-4 */

/*

 * Configure all of the ASX, GMX, and PKO registers required

 * to get RGMII to function on the supplied interface.

 *

 * @interface: PKO Interface to configure (0 or 1)

 *

 * Returns Zero on success

 Ignore SPI interfaces */

 Configure the ASX registers needed to use the RGMII ports */

 Configure the GMX registers needed to use the RGMII ports */

		/* Setting of CVMX_GMXX_TXX_THRESH has been moved to

			/*

			 * Configure more flexible RGMII preamble

			 * checking. Pass 1 doesn't support this

			 * feature.

 New field, so must be compile time */

		/*

		 * Each pause frame transmitted will ask for about 10M

		 * bit times before resume.  If buffer space comes

		 * available before that time has expired, an XON

		 * pause frame (0 time) will be transmitted to restart

		 * the flow.

 enable the ports now */

/*

 * Return the link state of an IPD/PKO port as returned by

 * auto negotiation. The result of this function may not match

 * Octeon's link config if auto negotiation has changed since

 * the last call to cvmx_helper_link_set().

 *

 * @ipd_port: IPD/PKO port to query

 *

 * Returns Link state

 Force 1Gbps full duplex on internal loopback */

/*

 * Configure an IPD/PKO port for the specified link state. This

 * function does not influence auto negotiation at the PHY level.

 * The passed link state must always match the link state returned

 * by cvmx_helper_link_get().

 *

 * @ipd_port:  IPD/PKO port to configure

 * @link_info: The new link state

 *

 * Returns Zero on success, negative on failure

 Ignore speed sets in the simulator */

 Read the current settings so we know the current enable state */

 Disable the lowest level RX */

 Disable all queues so that TX should become idle */

 Disable backpressure */

	/*

	 * Poll the GMX state machine waiting for it to become

	 * idle. Preferably we should only change speed when it is

	 * idle. If it doesn't become idle we will still do the speed

	 * change, but there is a slight chance that GMX will

	 * lockup.

 Disable the port before we make any changes */

 Set full/half duplex */

 Half duplex is broken for 38XX Pass 1 */

 Force full duplex on down links */

 Set the link speed. Anything unknown is set to 1Gbps */

 Adjust the clocks */

	/*

	 * Port	 .en  .type  .p0mii  Configuration

	 * ----	 ---  -----  ------  -----------------------------------------

	 *  X	   0	 X	X    All links are disabled.

	 *  0	   1	 X	0    Port 0 is RGMII

	 *  0	   1	 X	1    Port 0 is MII

	 *  1	   1	 0	X    Ports 1 and 2 are configured as RGMII ports.

	 *  1	   1	 1	X    Port 1: GMII/MII; Port 2: disabled. GMII or

	 *			     MII port is selected by GMX_PRT1_CFG[SPEED].

 In MII mode, CLK_CNT = 1. */

 Do a read to make sure all setup stuff is complete */

 Save the new GMX setting without enabling the port */

 Enable the lowest level RX */

 Re-enable the TX path */

 Restore backpressure */

 Restore the GMX enable state. Port config is complete */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (C) 2003-2018 Cavium, Inc.

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for XAUI initialization, configuration,

 * and monitoring.

 *

 If HiGig2 is enabled return 16 ports, otherwise return 1 port */

/*

 * Probe a XAUI interface and determine the number of ports

 * connected to it. The XAUI interface should still be down

 * after this call.

 *

 * @interface: Interface to probe

 *

 * Returns Number of ports on the interface. Zero to disable.

	/*

	 * Due to errata GMX-700 on CN56XXp1.x and CN52XXp1.x, the

	 * interface needs to be enabled before IPD otherwise per port

	 * backpressure may not work properly.

	/*

	 * Setup PKO to support 16 ports for HiGig2 virtual

	 * ports. We're pointing all of the PKO packet ports for this

	 * interface to the XAUI. This allows us to use HiGig2

	 * backpressure per port.

		/*

		 * We set each PKO port to have equal priority in a

		 * round robin fashion.

 All PKO ports map to the same XAUI hardware port */

/*

 * Bringup and enable a XAUI interface. After this call packet

 * I/O should be fully functional. This is called with IPD

 * enabled but PKO disabled.

 *

 * @interface: Interface to bring up

 *

 * Returns Zero on success, negative on failure

 Setup PKND */

 (1) Interface has already been enabled. */

 (2) Disable GMX. */

 (3) Disable GMX and PCSX interrupts. */

 (4) Bring up the PCSX and GMX reconciliation layer. */

 (4)a Set polarity and lane swapping. */

 (4)b */

 Enable better IFG packing and improves performance */

 (4)c Aply reset sequence */

 Issuing a reset here seems to hang some CN68XX chips. */

 Wait for PCS to come out of reset */

 Wait for PCS to be aligned */

 Wait for RX to be ready */

 (6) Configure GMX */

 Wait for GMX RX to be idle */

 Wait for GMX TX to be idle */

 GMX configure */

 (7) Clear out any error state */

 Wait for receive link */

 (8) Enable packet reception */

/*

 * Return the link state of an IPD/PKO port as returned by

 * auto negotiation. The result of this function may not match

 * Octeon's link config if auto negotiation has changed since

 * the last call to cvmx_helper_link_set().

 *

 * @ipd_port: IPD/PKO port to query

 *

 * Returns Link state

 Only return a link if both RX and TX are happy */

 Disable GMX and PCSX interrupts. */

/*

 * Configure an IPD/PKO port for the specified link state. This

 * function does not influence auto negotiation at the PHY level.

 * The passed link state must always match the link state returned

 * by cvmx_helper_link_get().

 *

 * @ipd_port:  IPD/PKO port to configure

 * @link_info: The new link state

 *

 * Returns Zero on success, negative on failure

 If the link shouldn't be up, then just return */

 Do nothing if both RX and TX are happy */

 Bring the link up */

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 *

 * Support library for the SPI

/* Default callbacks, can be overridden

 *  using cvmx_spi_get_callbacks/cvmx_spi_set_callbacks

/*

 * Get current SPI4 initialization callbacks

 *

 * @callbacks:	Pointer to the callbacks structure.to fill

 *

 * Returns Pointer to cvmx_spi_callbacks_t structure.

/*

 * Set new SPI4 initialization callbacks

 *

 * @new_callbacks:  Pointer to an updated callbacks structure.

/*

 * Initialize and start the SPI interface.

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @timeout:   Timeout to wait for clock synchronization in seconds

 * @num_ports: Number of SPI ports to configure

 *

 * Returns Zero on success, negative of failure.

 Callback to perform SPI4 reset */

 Callback to perform calendar setup */

 Callback to perform clock detection */

 Callback to perform SPI4 link training */

 Callback to perform calendar sync */

 Callback to handle interface coming up */

/*

 * This routine restarts the SPI interface after it has lost synchronization

 * with its correspondent system.

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @timeout:   Timeout to wait for clock synchronization in seconds

 *

 * Returns Zero on success, negative of failure.

 Callback to perform SPI4 reset */

 NOTE: Calendar setup is not performed during restart */

	 Refer to cvmx_spi_start_interface() for the full sequence */

 Callback to perform clock detection */

 Callback to perform SPI4 link training */

 Callback to perform calendar sync */

 Callback to handle interface coming up */

/*

 * Callback to perform SPI4 reset

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

 Disable SPI error events while we run BIST */

 Run BIST in the SPI interface */

 Clear the calendar table after BIST to fix parity errors */

 Re enable reporting of error interrupts */

 Setup the CLKDLY right in the middle */

 This should always be on the opposite edge as statdrv */

 Reset SRX0 DLL */

 Waiting for Inf0 Spi4 RX DLL to lock */

 Enable dynamic alignment */

/*

 * Callback to setup calendar and miscellaneous settings before clock detection

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @num_ports: Number of ports to configure on SPI

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

 SRX0 number of Ports */

 SRX0 Calendar Table. This round robbins through all ports */

 STX0 Config */

 STX0 Training Control */

Minimum needed by dynamic alignment */

Minimum interval is 0x20 */

 STX0 Calendar Table. This round robbins through all ports */

/*

 * Callback to perform clock detection

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @timeout:   Timeout to wait for clock synchronization in seconds

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

	/*

	 * Regardless of operating mode, both Tx and Rx clocks must be

	 * present for the SPI interface to operate.

	/*

	 * Require 100 clock transitions in order to avoid any noise

	 * in the beginning.

			/*

			 * We've seen a clock transition, so decrement

			 * the number we still need.

	/*

	 * Require 100 clock transitions in order to avoid any noise in the

	 * beginning.

			/*

			 * We've seen a clock transition, so decrement

			 * the number we still need

/*

 * Callback to perform link training

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @timeout:   Timeout to wait for link to be trained (in seconds)

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

 SRX0 & STX0 Inf0 Links are configured - begin training */

 This should always be on the opposite edge as statdrv */

 SRX0 clear the boot bit */

 Wait for the training sequence to complete */

 Wait a really long time here */

	/*

	 * The HRM says we must wait for 34 + 16 * MAXDIST training sequences.

	 * We'll be pessimistic and wait for a lot more.

/*

 * Callback to perform calendar data synchronization

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 * @timeout:   Timeout to wait for calendar data in seconds

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

 SRX0 interface should be good, send calendar data */

 STX0 has achieved sync */

 The corespondant board should be sending calendar data */

 Enable the STX0 STAT receiver. */

 Waiting for calendar sync on STX0 STAT */

 SPX0_CLK_STAT - SPX0_CLK_STAT[STXCAL] should be 1 (bit10) */

/*

 * Callback to handle interface up

 *

 * @interface: The identifier of the packet interface to configure and

 *		    use as a SPI interface.

 * @mode:      The operating mode for the SPI interface. The interface

 *		    can operate as a full duplex (both Tx and Rx data paths

 *		    active) or as a halfplex (either the Tx data path is

 *		    active or the Rx data path is active, but not both).

 *

 * Returns Zero on success, non-zero error code on failure (will cause

 * SPI initialization to abort)

/***********************license start***************

 * Author: Cavium Networks

 *

 * Contact: support@caviumnetworks.com

 * This file is part of the OCTEON SDK

 *

 * Copyright (c) 2003-2008 Cavium Networks

 *

 * This file is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License, Version 2, as

 * published by the Free Software Foundation.

 *

 * This file is distributed in the hope that it will be useful, but

 * AS-IS and WITHOUT ANY WARRANTY; without even the implied warranty

 * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE, TITLE, or

 * NONINFRINGEMENT.  See the GNU General Public License for more

 * details.

 *

 * You should have received a copy of the GNU General Public License

 * along with this file; if not, write to the Free Software

 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA

 * or visit http://www.gnu.org/licenses/.

 *

 * This file may also be available under a different license from Cavium.

 * Contact Cavium Networks for more information

/*

 * Functions for NPI initialization, configuration,

 * and monitoring.

/**

 * Probe a NPI interface and determine the number of ports

 * connected to it. The NPI interface should still be down

 * after this call.

 *

 * @interface: Interface to probe

 *

 * Returns Number of ports on the interface. Zero to disable.

 The packet engines didn't exist before pass 2 */

 The packet engines didn't exist before pass 2 */

/**

 * Bringup and enable a NPI interface. After this call packet

 * I/O should be fully functional. This is called with IPD

 * enabled but PKO disabled.

 *

 * @interface: Interface to bring up

 *

 * Returns Zero on success, negative on failure

	/*

	 * On CN50XX, CN52XX, and CN56XX we need to disable length

	 * checking so packet < 64 bytes and jumbo frames don't get

	 * errors.

 Enables are controlled by the remote host, so nothing to do here */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Cryptographic API.

 *

 * SHA-512 and SHA-384 Secure Hash Algorithm.

 *

 * Adapted for OCTEON by Aaro Koskinen <aaro.koskinen@iki.fi>.

 *

 * Based on crypto/sha512_generic.c, which is:

 *

 * Copyright (c) Jean-Luc Cooke <jlcooke@certainkey.com>

 * Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>

 * Copyright (c) 2003 Kyle McMartin <kyle@debian.org>

/*

 * We pass everything as 64-bit. OCTEON can handle misaligned data.

 Compute number of bytes mod 128. */

 Update number of bytes. */

 Transform as many times as possible. */

 Buffer remaining input. */

	/*

	 * Small updates never reach the crypto engine, so the generic sha512 is

	 * faster because of the heavyweight octeon_crypto_enable() /

	 * octeon_crypto_disable().

 Save number of bits. */

 Pad out to 112 mod 128. */

 Append length (before padding). */

 Store state in digest. */

 Zeroize sensitive information. */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License. See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004-2012 Cavium Networks

/**

 * Enable access to Octeon's COP2 crypto hardware for kernel use. Wrap any

 * crypto operations in calls to octeon_crypto_enable/disable in order to make

 * sure the state of COP2 isn't corrupted if userspace is also performing

 * hardware crypto operations. Allocate the state parameter on the stack.

 * Returns with preemption disabled.

 *

 * @state: Pointer to state structure to store current COP2 state in.

 *

 * Returns: Flags to be passed to octeon_crypto_disable()

/**

 * Disable access to Octeon's COP2 crypto hardware in the kernel. This must be

 * called after an octeon_crypto_enable() before any context switch or return to

 * userspace.

 *

 * @state:	Pointer to COP2 state to restore

 * @flags:	Return value from octeon_crypto_enable()

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Cryptographic API.

 *

 * SHA1 Secure Hash Algorithm.

 *

 * Adapted for OCTEON by Aaro Koskinen <aaro.koskinen@iki.fi>.

 *

 * Based on crypto/sha1_generic.c, which is:

 *

 * Copyright (c) Alan Smithee.

 * Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>

 * Copyright (c) Jean-Francois Dive <jef@linuxbe.org>

/*

 * We pass everything as 64-bit. OCTEON can handle misaligned data.

	/*

	 * Small updates never reach the crypto engine, so the generic sha1 is

	 * faster because of the heavyweight octeon_crypto_enable() /

	 * octeon_crypto_disable().

 Save number of bits. */

 Pad out to 56 mod 64. */

 Append length (before padding). */

 Store state in digest */

 Zeroize sensitive information. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Cryptographic API.

 *

 * SHA-224 and SHA-256 Secure Hash Algorithm.

 *

 * Adapted for OCTEON by Aaro Koskinen <aaro.koskinen@iki.fi>.

 *

 * Based on crypto/sha256_generic.c, which is:

 *

 * Copyright (c) Jean-Luc Cooke <jlcooke@certainkey.com>

 * Copyright (c) Andrew McDonald <andrew@mcdonald.org.uk>

 * Copyright (c) 2002 James Morris <jmorris@intercode.com.au>

 * SHA224 Support Copyright 2007 Intel Corporation <jonathan.lynch@intel.com>

/*

 * We pass everything as 64-bit. OCTEON can handle misaligned data.

	/*

	 * Small updates never reach the crypto engine, so the generic sha256 is

	 * faster because of the heavyweight octeon_crypto_enable() /

	 * octeon_crypto_disable().

 Save number of bits. */

 Pad out to 56 mod 64. */

 Append length (before padding). */

 Store state in digest */

 Zeroize sensitive information. */

/*

 * Cryptographic API.

 *

 * MD5 Message Digest Algorithm (RFC1321).

 *

 * Adapted for OCTEON by Aaro Koskinen <aaro.koskinen@iki.fi>.

 *

 * Based on crypto/md5.c, which is:

 *

 * Derived from cryptoapi implementation, originally based on the

 * public domain implementation written by Colin Plumb in 1993.

 *

 * Copyright (c) Cryptoapi developers.

 * Copyright (c) 2002 James Morris <jmorris@intercode.com.au>

 *

 * This program is free software; you can redistribute it and/or modify it

 * under the terms of the GNU General Public License as published by the Free

 * Software Foundation; either version 2 of the License, or (at your option)

 * any later version.

/*

 * We pass everything as 64-bit. OCTEON can handle misaligned data.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Registration of Cobalt LCD platform device.

 *

 *  Copyright (C) 2008  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Setup pointers to hardware dependent routines.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 1997, 2004, 05 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2001, 2002, 2003 by Liam Davies (ldavies@agile.tv)

 *

/*

 * Cobalt doesn't have PS/2 keyboard/mouse interfaces,

 * keyboard controller is never used.

 * Also PCI-ISA bridge DMA controller is never used.

 dma1 */

 keyboard */

 dma page reg */

 dma2 */

 I/O port resource */

 These resources have been reserved by VIA SuperI/O chip. */

/*

 * Prom init. We read our one and only communication with the firmware.

 * Grab the amount of installed memory.

 * Better boot loaders (CoLo) pass a command line too :-)

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Registration of Cobalt LED platform device.

 *

 *  Copyright (C) 2007	Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Cobalt time initialization.

 *

 *  Copyright (C) 2007  Yoichi Yuasa <yuasa@linux-mips.org>

 50MHz */

	/*

	 * MIPS counter frequency is measured during a 100msec interval

	 * using GT64111 timer0.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Registration of Cobalt UART platform device.

 *

 *  Copyright (C) 2007  Yoichi Yuasa <yuasa@linux-mips.org>

	/*

	 * Cobalt Qube1 has no UART.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Registration of Cobalt RTC platform device.

 *

 *  Copyright (C) 2007  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Registration of Cobalt MTD device.

 *

 *  Copyright (C) 2006  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Cobalt buttons platform device.

 *

 *  Copyright (C) 2007  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Register PCI controller.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 1997, 2004, 05 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2001, 2002, 2003 by Liam Davies (ldavies@agile.tv)

 *

/*

 * Cobalt Reset operations

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995, 1996, 1997 by Ralf Baechle

 * Copyright (C) 2001 by Liam Davies (ldavies@agile.tv)

	/*

	 * turn on power off LED on RaQ

 we should never get here */

/*

 * IRQ vector handles

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995, 1996, 1997, 2003 by Ralf Baechle

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.

/*

 * Initializes basic routines and structures pointers, memory size (as

 * given by the bios and saves the command line.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 * Putting things on the screen/serial line using YAMONs facilities.

/*



Well-known variable (num is looked up in table above for matching variable name)

Example: cpufrequency=211968000

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---

| 01 |CTRL|CHECKSUM | 01 | _2 | _1 | _1 | _9 | _6 | _8 | _0 | _0 | _0 | \0 | FF

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---



Name=Value pair in a single chunk

Example: NAME=VALUE

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---

| 00 |CTRL|CHECKSUM | 01 | _N | _A | _M | _E | _0 | _V | _A | _L | _U | _E | \0

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---



Name=Value pair in 2 chunks (len is the number of chunks)

Example: bootloaderVersion=1.3.7.15

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---

| 00 |CTRL|CHECKSUM | 02 | _b | _o | _o | _t | _l | _o | _a | _d | _e | _r | _V

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---

| _e | _r | _s | _i | _o | _n | \0 | _1 | _. | _3 | _. | _7 | _. | _1 | _5 | \0

+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+---



Data is padded with 0xFF



 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 * Setting up the clock on the MIPS boards.

 Initialize ar7 clocks so the CPU clock frequency is correct */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2007 Eugene Konev <ejka@openwrt.org>

 * Copyright (C) 2009 Florian Fainelli <florian@openwrt.org>

 nop */

 nop */

 nop */

 Async */

 Sync */

 2:1 */

 1:1 */

 adjust vbus clock rate */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2007 Eugene Konev <ejka@openwrt.org>

 * Copyright (C) 2009-2010 Florian Fainelli <florian@openwrt.org>

 reg, start bit, mux value */

 Check the mux status */

 Set the pin sel value */

 Perform minimal Titan GPIO configuration */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2006,2007 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2006,2007 Eugene Konev <ejka@openwrt.org>

/*****************************************************************************

 * VLYNQ Bus

/*****************************************************************************

 * Flash

/*****************************************************************************

 * Ethernet

/*****************************************************************************

 * USB

/*****************************************************************************

 * LEDs

 Default LEDs */

 FIXME: the whole thing is unreliable */

 If we can't get the product id from PROM, use the default LEDs */

/*****************************************************************************

 * Watchdog

 Filled at runtime */

 Filled at runtime */

/*****************************************************************************

 * Init

 Only TNETD73xx have a second serial port */

 Set vlynq0 data */

 Set vlynq1 data */

 Set vlynq0 resources */

 Set vlynq1 resources */

 Set cpmac0 data */

 Set cpmac1 data */

 Set cpmac0 resources */

 Set cpmac1 resources */

 Register watchdog only if enabled in hardware */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2006,2007 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2006,2007 Eugene Konev <ejka@openwrt.org>

 0x80 */

 0x10 */

 0x88 */

 0x20 */

 0x90 */

 0x30 */

 0x98 */

 0x50 */

 0x60 */

	/*

	 * Disable interrupts and clear pending

 Primary IRQ's */

 Secondary IRQ's */

 Primary IRQ's */

 Secondary IRQ's are cascaded through primary '0' */

 cpu timer */

 int0 hardware line */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2007 Eugene Konev <ejka@openwrt.org>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Yann Le Du <ledu@kymasys.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS TLB handling, this file is part of the Linux host kernel so that

 * TLB handlers run from KSEG0

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

 GuestID management */

/**

 * clear_root_gid() - Set GuestCtl1.RID for normal root operation.

/**

 * set_root_gid_to_guest_gid() - Set GuestCtl1.RID to match GuestCtl1.ID.

 *

 * Sets the root GuestID to match the current guest GuestID, for TLB operation

 * on the GPA->RPA mappings in the root TLB.

 *

 * The caller must be sure to disable HTW while the root GID is set, and

 * possibly longer if TLB registers are modified.

 Set root GuestID for root probe and write of guest TLB entry */

	/*

	 * We don't want to get reserved instruction exceptions for missing tlb

	 * entries.

/**

 * kvm_vz_guest_tlb_lookup() - Lookup a guest VZ TLB mapping.

 * @vcpu:	KVM VCPU pointer.

 * @gpa:	Guest virtual address in a TLB mapped guest segment.

 * @gpa:	Ponter to output guest physical address it maps to.

 *

 * Converts a guest virtual address in a guest TLB mapped segment to a guest

 * physical address, by probing the guest TLB.

 *

 * Returns:	0 if guest TLB mapping exists for @gva. *@gpa will have been

 *		written.

 *		-EFAULT if no guest TLB mapping exists for @gva. *@gpa may not

 *		have been written.

 Probe the guest TLB for a mapping */

 Set root GuestID for root probe of guest TLB entry */

 No match, fail */

 Match! read the TLB entry */

 Select one of the EntryLo values and interpret the GPA */

	/*

	 * TLB entry may have become invalid since TLB probe if physical FTLB

	 * entries are shared between threads (e.g. I6400).

	/*

	 * Note, this doesn't take guest MIPS32 XPA into account, where PFN is

	 * split with XI/RI in the middle.

/**

 * kvm_vz_local_flush_roottlb_all_guests() - Flush all root TLB entries for

 * guests.

 *

 * Invalidate all entries in root tlb which are GPA mappings.

 TLBR may clobber EntryHi.ASID, PageMask, and GuestCtl1.RID */

	/*

	 * Invalidate guest entries in root TLB while leaving root entries

	 * intact when possible.

 Don't invalidate non-guest (RVA) mappings in the root TLB */

 Make sure all entries differ. */

/**

 * kvm_vz_local_flush_guesttlb_all() - Flush all guest TLB entries.

 *

 * Invalidate all entries in guest tlb irrespective of guestid.

 Preserve all clobbered guest registers */

 Inhibit machine check due to multiple matching TLB entries */

 Invalidate guest entries in guest TLB */

 Make sure all entries differ. */

/**

 * kvm_vz_save_guesttlb() - Save a range of guest TLB entries.

 * @buf:	Buffer to write TLB entries into.

 * @index:	Start index.

 * @count:	Number of entries to save.

 *

 * Save a range of guest TLB entries. The caller must ensure interrupts are

 * disabled.

 Save registers we're about to clobber */

 Set root GuestID for root probe */

 Read each entry from guest TLB */

 Entry invalid or belongs to another guest */

 Entry belongs to the right guest */

 Clear root GuestID again */

 Restore clobbered registers */

/**

 * kvm_vz_load_guesttlb() - Save a range of guest TLB entries.

 * @buf:	Buffer to read TLB entries from.

 * @index:	Start index.

 * @count:	Number of entries to load.

 *

 * Load a range of guest TLB entries. The caller must ensure interrupts are

 * disabled.

 Save registers we're about to clobber */

 Set root GuestID for root probe */

 Write each entry to guest TLB */

 Clear root GuestID again */

 Restore clobbered registers */

 Set root GuestID for root probe and write of guest TLB entry */

 Set root GuestID for root probe and write of guest TLB entry */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: Interrupt delivery

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: Support for hardware virtualization extensions

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Yann Le Du <ledu@kymasys.com>

 Pointers to last VCPU loaded on each physical CPU */

 Pointers to last VCPU executed on each physical CPU */

/*

 * Number of guest VTLB entries to use, so we can catch inconsistency between

 * CPUs.

	/*

	 * First write with WG=1 to write upper bits, then write again in case

	 * WG should be left at 0.

	 * write_gc0_ebase_64() is no longer UNDEFINED since R6.

/*

 * These Config bits may be writable by the guest:

 * Config:	[K23, KU] (!TLB), K0

 * Config1:	(none)

 * Config2:	[TU, SU] (impl)

 * Config3:	ISAOnExc

 * Config4:	FTLBPageSize

 * Config5:	K, CV, MSAEn, UFE, FRE, SBRI, UFR

 no need to be exact */

 Permit MSAEn changes if MSA supported and enabled */

	/*

	 * Permit guest FPU mode changes if FPU is enabled and the relevant

	 * feature exists according to FIR register.

/*

 * VZ optionally allows these additional Config bits to be written by root:

 * Config:	M, [MT]

 * Config1:	M, [MMUSize-1, C2, MD, PC, WR, CA], FP

 * Config2:	M

 * Config3:	M, MSAP, [BPG], ULRI, [DSP2P, DSPP], CTXTC, [ITL, LPA, VEIC,

 *		VInt, SP, CDMM, MT, SM, TL]

 * Config4:	M, [VTLBSizeExt, MMUSizeExt]

 * Config5:	MRP

 Permit FPU to be present if FPU is supported */

 Permit MSA to be present if MSA is supported */

 VZ guest has already converted gva to gpa */

	/*

	 * timer expiry is asynchronous to vcpu execution therefore defer guest

	 * cp0 accesses

	/*

	 * timer expiry is asynchronous to vcpu execution therefore defer guest

	 * cp0 accesses

	/*

	 * interrupts are asynchronous to vcpu execution therefore defer guest

	 * cp0 accesses

	/*

	 * interrupts are asynchronous to vcpu execution therefore defer guest

	 * cp0 accesses

		/*

		 * Explicitly clear irq associated with Cause.IP[IPTI]

		 * if GuestCtl2 virtual interrupt register not

		 * supported or if not using GuestCtl2 Hardware Clear.

 Clear GuestCtl2.VIP irq if not using Hardware Clear */

/*

 * VZ guest timer handling.

/**

 * kvm_vz_should_use_htimer() - Find whether to use the VZ hard guest timer.

 * @vcpu:	Virtual CPU.

 *

 * Returns:	true if the VZ GTOffset & real guest CP0_Count should be used

 *		instead of software emulation of guest timer.

 *		false otherwise.

 Chosen frequency must match real frequency */

 We don't support a CP0_GTOffset with fewer bits than CP0_Count */

/**

 * _kvm_vz_restore_stimer() - Restore soft timer state.

 * @vcpu:	Virtual CPU.

 * @compare:	CP0_Compare register value, restored by caller.

 * @cause:	CP0_Cause register to restore.

 *

 * Restore VZ state relating to the soft timer. The hard timer can be enabled

 * later.

	/*

	 * Avoid spurious counter interrupts by setting Guest CP0_Count to just

	 * after Guest CP0_Compare.

/**

 * _kvm_vz_restore_htimer() - Restore hard timer state.

 * @vcpu:	Virtual CPU.

 * @compare:	CP0_Compare register value, restored by caller.

 * @cause:	CP0_Cause register to restore.

 *

 * Restore hard timer Guest.Count & Guest.Cause taking care to preserve the

 * value of Guest.CP0_Cause.TI while restoring Guest.CP0_Cause.

	/*

	 * Freeze the soft-timer and sync the guest CP0_Count with it. We do

	 * this with interrupts disabled to avoid latency.

 restore guest CP0_Cause, as TI may already be set */

	/*

	 * The above sequence isn't atomic and would result in lost timer

	 * interrupts if we're not careful. Detect if a timer interrupt is due

	 * and assert it.

/**

 * kvm_vz_restore_timer() - Restore timer state.

 * @vcpu:	Virtual CPU.

 *

 * Restore soft timer state from saved context.

/**

 * kvm_vz_acquire_htimer() - Switch to hard timer state.

 * @vcpu:	Virtual CPU.

 *

 * Restore hard timer state on top of existing soft timer state if possible.

 *

 * Since hard timer won't remain active over preemption, preemption should be

 * disabled by the caller.

 enable guest access to hard timer */

/**

 * _kvm_vz_save_htimer() - Switch to software emulation of guest timer.

 * @vcpu:	Virtual CPU.

 * @compare:	Pointer to write compare value to.

 * @cause:	Pointer to write cause value to.

 *

 * Save VZ guest timer state and switch to software emulation of guest CP0

 * timer. The hard timer must already be in use, so preemption should be

 * disabled.

	/*

	 * Record the CP0_Count *prior* to saving CP0_Cause, so we have a time

	 * at which no pending timer interrupt is missing.

	/*

	 * Record a final CP0_Count which we will transfer to the soft-timer.

	 * This is recorded *after* saving CP0_Cause, so we don't get any timer

	 * interrupts from just after the final CP0_Count point.

	/*

	 * The above sequence isn't atomic, so we could miss a timer interrupt

	 * between reading CP0_Cause and end_count. Detect and record any timer

	 * interrupt due between before_count and end_count.

	/*

	 * Restore soft-timer, ignoring a small amount of negative drift due to

	 * delay between freeze_hrtimer and setting CP0_GTOffset.

/**

 * kvm_vz_save_timer() - Save guest timer state.

 * @vcpu:	Virtual CPU.

 *

 * Save VZ guest timer state and switch to soft guest timer if hard timer was in

 * use.

 disable guest use of hard timer */

 save hard timer state */

 save timer-related state to VCPU context */

/**

 * kvm_vz_lose_htimer() - Ensure hard guest timer is not in use.

 * @vcpu:	Virtual CPU.

 *

 * Transfers the state of the hard guest timer to the soft guest timer, leaving

 * guest state intact so it can continue to be used with the soft timer.

 disable guest use of timer */

 switch to soft timer */

 leave soft timer in usable state */

/**

 * is_eva_access() - Find whether an instruction is an EVA memory accessor.

 * @inst:	32-bit instruction encoding.

 *

 * Finds whether @inst encodes an EVA memory access instruction, which would

 * indicate that emulation of it should access the user mode address space

 * instead of the kernel mode address space. This matters for MUSUK segments

 * which are TLB mapped for user mode but unmapped for kernel mode.

 *

 * Returns:	Whether @inst encodes an EVA accessor instruction.

/**

 * is_eva_am_mapped() - Find whether an access mode is mapped.

 * @vcpu:	KVM VCPU state.

 * @am:		3-bit encoded access mode.

 * @eu:		Segment becomes unmapped and uncached when Status.ERL=1.

 *

 * Decode @am to find whether it encodes a mapped segment for the current VCPU

 * state. Where necessary @eu and the actual instruction causing the fault are

 * taken into account to make the decision.

 *

 * Returns:	Whether the VCPU faulted on a TLB mapped address.

	/*

	 * Interpret access control mode. We assume address errors will already

	 * have been caught by the guest, leaving us with:

	 *      AM      UM  SM  KM  31..24 23..16

	 * UK    0 000          Unm   0      0

	 * MK    1 001          TLB   1

	 * MSK   2 010      TLB TLB   1

	 * MUSK  3 011  TLB TLB TLB   1

	 * MUSUK 4 100  TLB TLB Unm   0      1

	 * USK   5 101      Unm Unm   0      0

	 * -     6 110                0      0

	 * UUSK  7 111  Unm Unm Unm   0      0

	 *

	 * We shift a magic value by AM across the sign bit to find if always

	 * TLB mapped, and if not shift by 8 again to find if it depends on KM.

		/*

		 * MK, MSK, MUSK

		 * Always TLB mapped, unless SegCtl.EU && ERL

			/*

			 * MUSUK

			 * TLB mapped if not in kernel mode

			/*

			 * EVA access instructions in kernel

			 * mode access user address space.

/**

 * kvm_vz_gva_to_gpa() - Convert valid GVA to GPA.

 * @vcpu:	KVM VCPU state.

 * @gva:	Guest virtual address to convert.

 * @gpa:	Output guest physical address.

 *

 * Convert a guest virtual address (GVA) which is valid according to the guest

 * context, to a guest physical address (GPA).

 *

 * Returns:	0 on success.

 *		-errno on failure.

 Handle canonical 32-bit virtual address */

 CFG5 (1GB) */

 CFG4 (1GB) */

 CFG3 (512MB) */

 CFG2 (512MB) */

 CFG1 (512MB) */

 CFG0 (512MB) */

				/*

				 * GCC 4.9 isn't smart enough to figure out that

				 * segctl and mask are always initialised.

 Unmapped, find guest physical address */

 legacy unmapped KSeg0 or KSeg1 */

 XKPHYS */

			/*

			 * Each of the 8 regions can be overridden by SegCtl2.XR

			 * to use SegCtl1.XAM.

		/*

		 * Traditionally fully unmapped.

		 * Bits 61:59 specify the CCA, which we can just mask off here.

		 * Bits 58:PABITS should be zero, but we shouldn't have got here

		 * if it wasn't.

/**

 * kvm_vz_badvaddr_to_gpa() - Convert GVA BadVAddr from root exception to GPA.

 * @vcpu:	KVM VCPU state.

 * @badvaddr:	Root BadVAddr.

 * @gpa:	Output guest physical address.

 *

 * VZ implementations are permitted to report guest virtual addresses (GVA) in

 * BadVAddr on a root exception during guest execution, instead of the more

 * convenient guest physical addresses (GPA). When we get a GVA, this function

 * converts it to a GPA, taking into account guest segmentation and guest TLB

 * state.

 *

 * Returns:	0 on success.

 *		-errno on failure.

 If BadVAddr is GPA, then all is well in the world */

 Otherwise we'd expect it to be GVA ... */

 ... and we need to perform the GVA->GPA translation in software */

	/*

	 *  Fetch the instruction.

 Mask off unused bits */

 Set or clear VH */

 clear VH */

 set VH to match VL */

	/*

	 * Update PC and hold onto current PC in case there is

	 * an error and we want to rollback the PC

 Count */

 Compare */

 LLAddr */

 MAAR */

 MAARI must be in range */

 PRid */

 CDMMBase */

 CMGCRBase */

 SRSCtl */

 SRSMap */

 Config6 */

 Config7 */

 MAARI */

 ErrCtl */

 Diag */

 Sign extend */

 Count */

 Compare */

 LLAddr */

				/*

				 * P5600 generates GPSI on guest MTC0 LLAddr.

				 * Only allow the guest to clear LLB.

 MAAR */

 MAARI must be in range */

 MAARI */

 ErrCtl */

 ignore the written value */

 Diag */

 Flush BTB */

 Flush ITLB */

 Flush DTLB */

 Flush VTLB */

 Flush FTLB */

 Rollback PC only if emulation was unsuccessful */

	/*

	 * Update PC and hold onto current PC in case there is

	 * an error and we want to rollback the PC

 Secondary or tirtiary cache ops ignored */

 We can just flush entire icache */

 So far, other platforms support guest hit cache ops */

 Rollback PC */

	/*

	 * Update PC and hold onto current PC in case there is

	 * an error and we want to rollback the PC

 Read CPUCFG */

 Don't export any other advanced features to guest */

 Rollback PC only if emulation was unsuccessful */

	/*

	 *  Fetch the instruction.

 Read count register */

	/*

	 *  Fetch the instruction.

 complete MTC0 on behalf of guest and advance EPC */

 FR bit should read as zero if no FPU */

			/*

			 * Also don't allow FR to be set if host doesn't support

			 * it.

				/*

				 * FPU and Vector register state is made

				 * UNPREDICTABLE by a change of FR, so don't

				 * even bother saving it.

			/*

			 * If MSA state is already live, it is undefined how it

			 * interacts with FR=0 FPU state, and we don't want to

			 * hit reserved instruction exceptions trying to save

			 * the MSA state later when CU=1 && FR=1, so play it

			 * safe and save it first.

 DC bit enabling/disabling timer? */

 Only certain bits are RW to the guest */

 WP can only be cleared */

 IntCtl */

 Handle changes in FPU/MSA modes */

			/*

			 * Propagate FRE changes immediately if the FPU

			 * context is already loaded.

	/*

	 * Presumably this is due to MC (guest mode change), so lets trace some

	 * relevant info.

	/*

	 * Update PC and hold onto current PC in case there is

	 * an error and we want to rollback the PC

	/*

	 *  Fetch the instruction.

/**

 * kvm_trap_vz_handle_cop_unusuable() - Guest used unusable coprocessor.

 * @vcpu:	Virtual CPU context.

 *

 * Handle when the guest attempts to use a coprocessor which hasn't been allowed

 * by the root context.

		/*

		 * If guest FPU not present, the FPU operation should have been

		 * treated as a reserved instruction!

		 * If FPU already in use, we shouldn't get this at all.

 other coprocessors not handled */

/**

 * kvm_trap_vz_handle_msa_disabled() - Guest used MSA while disabled in root.

 * @vcpu:	Virtual CPU context.

 *

 * Handle when the guest attempts to use MSA when it is disabled in the root

 * context.

	/*

	 * If MSA not present or not exposed to guest or FR=0, the MSA operation

	 * should have been treated as a reserved instruction!

	 * Same if CU1=1, FR=0.

	 * If MSA already in use, we shouldn't get this at all.

 A code fetch fault doesn't count as an MMIO */

 Fetch the instruction */

 Treat as MMIO */

 Just try the access again if we couldn't do the translation */

 Fetch the instruction */

 Treat as MMIO */

		/*

		 * KVM API exposes 64-bit version of the register, so move the

		 * RI/XI bits up into place.

		/*

		 * KVM API exposes 64-bit versiono of the register, so move the

		 * RI/XI bits down into place.

 Octeon III has a read-only guest.PRid */

		/*

		 * If the timer is stopped or started (DC bit) it must look

		 * atomic with changes to the timer interrupt pending bit (TI).

		 * A timer interrupt should not happen in between.

 disable timer first */

 enable timer last */

 Octeon III has a guest.PRid, but its read-only */

 fix version if needed */

 guestid 0 reserved for root */

 start new guestid cycle */

 Returns 1 if the guest TLB may be clobbered */

 Drop all GuestIDs for this VCPU */

 This will clobber guest TLB contents too */

		/*

		 * For Root ASID Dealias (RAD) we don't do anything here, but we

		 * still need the request to ensure we recheck asid_flush_mask.

		 * We can still return 0 as only the root TLB will be affected

		 * by a root ASID flush.

 Expand the wired TLB array if necessary */

 Save whatever we can */

 Save wired entries from the guest TLB */

 Invalidate any dropped entries since last time */

 Load wired entries into the guest TLB */

	/*

	 * Are we entering guest context on a different CPU to last time?

	 * If so, the VCPU's guest TLB state on this CPU may be stale.

	/*

	 * A vcpu's GuestID is set in GuestCtl1.ID when the vcpu is loaded and

	 * remains set until another vcpu is loaded in.  As a rule GuestRID

	 * remains zeroed when in root context unless the kernel is busy

	 * manipulating guest tlb entries.

		/*

		 * Check if our GuestID is of an older version and thus invalid.

		 *

		 * We also discard the stored GuestID if we've executed on

		 * another CPU, as the guest mappings may have changed without

		 * hypervisor knowledge.

 Restore GuestID */

		/*

		 * The Guest TLB only stores a single guest's TLB state, so

		 * flush it if another VCPU has executed on this CPU.

		 *

		 * We also flush if we've executed on another CPU, as the guest

		 * mappings may have changed without hypervisor knowledge.

		/*

		 * Root ASID dealiases guest GPA mappings in the root TLB.

		 * Allocate new root ASID if needed.

	/*

	 * Have we migrated to a different CPU?

	 * If so, any old guest TLB state may be stale.

	/*

	 * Was this the last VCPU to run on this CPU?

	 * If not, any old guest state from this VCPU will have been clobbered.

	/*

	 * Restore CP0_Wired unconditionally as we clear it after use, and

	 * restore wired guest TLB entries (while in guest context).

	/*

	 * Restore timer state regardless, as e.g. Cause.TI can change over time

	 * if left unmaintained.

 Set MC bit if we want to trace guest mode changes */

 Don't bother restoring registers multiple times unless necessary */

	/*

	 * Restore config registers first, as some implementations restrict

	 * writes to other registers when the corresponding feature bits aren't

	 * set. For example Status.CU1 cannot be set unless Config1.FP is set.

 restore KScratch registers if enabled in guest */

 restore HTW registers */

 restore Root.GuestCtl2 from unused Guest guestctl2 register */

	/*

	 * We should clear linked load bit to break interrupted atomics. This

	 * prevents a SC on the next VCPU from succeeding by matching a LL on

	 * the previous VCPU.

 allow wired TLB entries to be overwritten */

 only save implemented config registers */

 save KScratch registers if enabled in guest */

 save HTW registers if enabled in guest */

 save Root.GuestCtl2 in unused Guest guestctl2 register */

/**

 * kvm_vz_resize_guest_vtlb() - Attempt to resize guest VTLB.

 * @size:	Number of guest VTLB entries (0 < @size <= root VTLB entries).

 *

 * Attempt to resize the guest VTLB by writing guest Config registers. This is

 * necessary for cores with a shared root/guest TLB to avoid overlap with wired

 * entries in the root VTLB.

 *

 * Returns:	The resulting guest VTLB size.

 Write MMUSize - 1 into guest Config registers */

	/*

	 * Set Guest.Wired.Limit = 0 (no limit up to Guest.MMUSize-1), unless it

	 * would exceed Root.Wired.Limit (clearing Guest.Wired.Wired so write

	 * not dropped)

 Read back MMUSize - 1 */

 Set up guest timer/perfcount IRQ lines */

 No I/O hole translation. */

 Halve the root MMU size */

 Update our records */

 Flush moved entries in new (guest) context */

		/*

		 * ImgTec cores tend to use a shared root/guest TLB. To avoid

		 * overlap of root wired and guest entries, the guest TLB may

		 * need resizing.

 Try switching to maximum guest VTLB size for flush */

		/*

		 * Reduce to make space for root wired entries and at least 2

		 * root non-wired entries. This does assume that long-term wired

		 * entries won't be added later.

		/*

		 * Write the VTLB size, but if another CPU has already written,

		 * check it matches or we won't provide a consistent view to the

		 * guest. If this ever happens it suggests an asymmetric number

		 * of wired entries.

	/*

	 * Enable virtualization features granting guest direct control of

	 * certain features:

	 * CP0=1:	Guest coprocessor 0 context.

	 * AT=Guest:	Guest MMU.

	 * CG=1:	Hit (virtual address) CACHE operations (optional).

	 * CF=1:	Guest Config registers.

	 * CGI=1:	Indexed flush CACHE operations (optional).

 clear any pending injected virtual guest interrupts */

 Control guest CCA attribute */

 Flush any remaining guest TLB entries */

		/*

		 * Allocate whole TLB for root. Existing guest TLB entries will

		 * change ownership to the root TLB. We should be safe though as

		 * they've already been flushed above while in guest TLB.

 Update our records */

 Flush moved entries in new (root) context */

 we wouldn't be here unless cpu_has_vz */

 We support 64-bit registers/operations and addresses */

	/*

	 * If the VCPU is freed and reused as another VCPU, we don't want the

	 * matching pointer wrongly hanging around in last_vcpu[] or

	 * last_exec_vcpu[].

 default to 100 MHz */

	/*

	 * Start off the timer at the same frequency as the host timer, but the

	 * soft timer doesn't handle frequencies greater than 1GHz yet.

	/*

	 * Initialize guest register state to valid architectural reset state.

 PageGrain */

 Wired */

 Status */

 IntCtl */

 PRId */

 EBase */

 Config */

 architecturally writable (e.g. from guest) */

 architecturally read only, but maybe writable from root */

 Config1 */

 architecturally read only, but maybe writable from root */

 Config2 */

 Config3 */

 architecturally writable (e.g. from guest) */

 architecturally read only, but maybe writable from root */

 Config4 */

 Config5 */

 architecturally writable (e.g. from guest) */

 architecturally read only, but maybe writable from root */

 ContextConfig */

 XContextConfig */

 bits SEGBITS-13+3:4 set */

 Implementation dependent, use the legacy layout */

 SegCtl0, SegCtl1, SegCtl2 */

 reset HTW registers */

 PWField */

 PWSize */

 start with no pending virtual guest interrupts */

 Put PC at reset vector */

		/*

		 * For each CPU there is a single GPA ASID used by all VCPUs in

		 * the VM, so it doesn't make sense for the VCPUs to handle

		 * invalidation of these ASIDs individually.

		 *

		 * Instead mark all CPUs as needing ASID invalidation in

		 * asid_flush_mask, and kvm_flush_remote_tlbs(kvm) will

		 * kick any running VCPUs so they check asid_flush_mask.

 Check if we have any exceptions/interrupts pending */

	/*

	 * VZ requires at least 2 KScratch registers, so it should have been

	 * possible to allocate pgd_reg.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: MIPS specific KVM APIs

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

 for EI/VI mode */

/*

 * XXXKYMA: We are simulatoring a processor that has the WII bit set in

 * Config7, so we are "runnable" if interrupts are pending

 Unsupported KVM type */

 Allocate page table to map GPA -> RPA */

 It should always be safe to remove after flushing the whole range */

 Flush whole GPA */

	/*

	 * The slot has been made invalid (ready for moving or deletion), so we

	 * need to ensure that it can no longer be accessed by any guest VCPUs.

 Flush slot from GPA */

	/*

	 * If dirty page logging is enabled, write protect all pages in the slot

	 * ready for dirty logging.

	 *

	 * There is no need to do this in any of the following cases:

	 * CREATE:	No dirty mappings will already exist.

	 * MOVE/DELETE:	The old mappings will already have been cleaned up by

	 *		kvm_arch_flush_shadow_memslot()

 Write protect GPA page table entries */

 low level hrtimer wake routine */

	/*

	 * Allocate space for host mode exception handlers that handle

	 * guest mode exits

	/*

	 * Check new ebase actually fits in CP0_EBase. The lack of a write gate

	 * limits us to the low 512MB of physical address space. If the memory

	 * we allocate is out of range, just give up now.

 Save new ebase */

 Build guest exception vectors dynamically in unmapped memory */

 TLB refill (or XTLB refill on 64-bit VZ where KX=1) */

 General Exception Entry point */

 For vectored interrupts poke the exception code @ all offsets 0-7 */

 General exit handler */

 Guest entry routine */

 Dump the generated code */

 Invalidate the icache for these ranges */

 Init */

 Initial guest state */

	/*

	 * Make sure the read of VCPU requests in vcpu_run() callback is not

	 * reordered ahead of the write to vcpu->mode, or we could miss a TLB

	 * flush request while the requester sees the VCPU as outside of guest

	 * mode and not needing an IPI.

 odd doubles */

 skip odd doubles if no F64 */

 General purpose registers */

 Floating point registers */

 Odd singles in top of even double when FR=0 */

 Can't access odd doubles in FR=0 mode */

 MIPS SIMD Architecture (MSA) registers */

 Can't access MSA registers in FR=0 mode */

 least significant byte first */

 most significant byte first */

 registers to be handled specially */

 General purpose registers */

 Silently ignore requests to set $0 */

 Floating point registers */

 Odd singles in top of even double when FR=0 */

 Can't access odd doubles in FR=0 mode */

 Read-only */

 MIPS SIMD Architecture (MSA) registers */

 least significant byte first */

 most significant byte first */

 Read-only */

 registers to be handled specially */

 We don't handle systems with inconsistent cpu_has_fpu */

		/*

		 * We don't support MSA vector partitioning yet:

		 * 1) It would require explicit support which can't be tested

		 *    yet due to lack of support in current hardware.

		 * 2) It extends the state that would need to be saved/restored

		 *    by e.g. QEMU for migration.

		 *

		 * When vector partitioning hardware becomes available, support

		 * could be added by requiring a flag when enabling

		 * KVM_CAP_MIPS_MSA capability to indicate that userland knows

		 * to save/restore the appropriate extra state.

 zero is special, and cannot be set. */

/*

 * Return value is in the form (errcode<<2 | RESUME_FLAG_HOST | RESUME_FLAG_NV)

 Set a default exit reason */

	/*

	 * Set the appropriate status bits based on host CPU features,

	 * before we hit the scheduler

 XXXKYMA: Might need to return to user space */

 defer exit accounting to handler */

 Only check for signals if not already exiting to userspace */

		/*

		 * Make sure the read of VCPU requests in vcpu_reenter()

		 * callback is not reordered ahead of the write to vcpu->mode,

		 * or we could miss a TLB flush request while the requester sees

		 * the VCPU as outside of guest mode and not needing an IPI.

		/*

		 * If FPU / MSA are enabled (i.e. the guest's FPU / MSA context

		 * is live), restore FCR31 / MSACSR.

		 *

		 * This should be before returning to the guest exception

		 * vector, as it may well cause an [MSA] FP exception if there

		 * are pending exception bits unmasked. (see

		 * kvm_mips_csr_die_notifier() for how that is handled).

 Enable FPU for guest and restore context */

	/*

	 * If MSA state is already live, it is undefined how it interacts with

	 * FR=0 FPU state, and we don't want to hit reserved instruction

	 * exceptions trying to save the MSA state later when CU=1 && FR=1, so

	 * play it safe and save it first.

	/*

	 * Enable FPU for guest

	 * We set FR and FRE according to guest context

 If guest FPU state not active, restore it now */

 Enable MSA for guest and restore context */

	/*

	 * Enable FPU if enabled in guest, since we're restoring FPU context

	 * anyway. We set FR and FRE according to guest context.

		/*

		 * If FR=0 FPU state is already live, it is undefined how it

		 * interacts with MSA state, so play it safe and save it first.

 Enable MSA for guest */

		/*

		 * Guest FPU state already loaded, only restore upper MSA state

 Neither FPU or MSA already active, restore full MSA state */

 Drop FPU & MSA without saving it */

 Save and disable FPU & MSA */

	/*

	 * With T&E, FPU & MSA get disabled in root context (hardware) when it

	 * is disabled in guest context (software), but the register state in

	 * the hardware may still be in use.

	 * This is why we explicitly re-enable the hardware before saving.

 Disable MSA & FPU */

 Disable FPU */

/*

 * Step over a specific ctc1 to FCSR and a specific ctcmsa to MSACSR which are

 * used to restore guest FCSR/MSACSR state and may trigger a "harmless" FP/MSAFP

 * exception if cause bits are set in the value being written.

 Only interested in FPE and MSAFPE */

 Return immediately if guest context isn't active */

 Should never get here from user mode */

 match 2nd instruction in __kvm_restore_fcsr */

 match 2nd/3rd instruction in __kvm_restore_msacsr */

 Move PC forward a little and continue executing */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: COP0 access histogram

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: Instruction/Exception emulation

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

/*

 * Compute the return address and do emulate branch simulation, if required.

 * This function should be called only in branch delay slot active.

 Read the instruction */

 jr and jalr are in r_format format. */

		/*

		 * This group contains:

		 * bltz_op, bgez_op, bltzl_op, bgezl_op,

		 * bltzal_op, bgezal_op, bltzall_op, bgezall_op.

 These are unconditional and in j_format. */

 These are conditional and in i_format. */

 POP06 */

 removed in R6 */

 POP07 */

 removed in R6 */

 And now the FPA/cp1 branch instructions. */

 R6 added the following compact branches with forbidden slots */

 POP26 */

 POP27 */

 only rt == 0 isn't compact branch */

 only rs == rt == 0 is reserved, rest are compact branches */

 only rs == 0 isn't compact branch */

		/*

		 * If we've hit an exception on the forbidden slot, then

		 * the branch must not have been taken.

 Fall through - Compact branches not supported before R6 */

/**

 * kvm_get_badinstr() - Get bad instruction encoding.

 * @opc:	Guest pointer to faulting instruction.

 * @vcpu:	KVM VCPU information.

 *

 * Gets the instruction encoding of the faulting instruction, using the saved

 * BadInstr register value if it exists, otherwise falling back to reading guest

 * memory at @opc.

 *

 * Returns:	The instruction encoding of the faulting instruction.

/**

 * kvm_get_badinstrp() - Get bad prior instruction encoding.

 * @opc:	Guest pointer to prior faulting instruction.

 * @vcpu:	KVM VCPU information.

 *

 * Gets the instruction encoding of the prior faulting instruction (the branch

 * containing the delay slot which faulted), using the saved BadInstrP register

 * value if it exists, otherwise falling back to reading guest memory at @opc.

 *

 * Returns:	The instruction encoding of the prior faulting instruction.

/**

 * kvm_mips_count_disabled() - Find whether the CP0_Count timer is disabled.

 * @vcpu:	Virtual CPU.

 *

 * Returns:	1 if the CP0_Count timer is disabled by either the guest

 *		CP0_Cause.DC bit or the count_ctl.DC bit.

 *		0 otherwise (in which case CP0_Count timer is running).

/**

 * kvm_mips_ktime_to_count() - Scale ktime_t to a 32-bit count.

 *

 * Caches the dynamic nanosecond bias in vcpu->arch.count_dyn_bias.

 *

 * Assumes !kvm_mips_count_disabled(@vcpu) (guest CP0_Count timer is running).

 If delta is out of safe range the bias needs adjusting */

 Recalculate delta with new bias */

	/*

	 * We've ensured that:

	 *   delta < count_period

	 *

	 * Therefore the intermediate delta*count_hz will never overflow since

	 * at the boundary condition:

	 *   delta = count_period

	 *   delta = NSEC_PER_SEC * 2^32 / count_hz

	 *   delta * count_hz = NSEC_PER_SEC * 2^32

/**

 * kvm_mips_count_time() - Get effective current time.

 * @vcpu:	Virtual CPU.

 *

 * Get effective monotonic ktime. This is usually a straightforward ktime_get(),

 * except when the master disable bit is set in count_ctl, in which case it is

 * count_resume, i.e. the time that the count was disabled.

 *

 * Returns:	Effective monotonic ktime for CP0_Count.

/**

 * kvm_mips_read_count_running() - Read the current count value as if running.

 * @vcpu:	Virtual CPU.

 * @now:	Kernel time to read CP0_Count at.

 *

 * Returns the current guest CP0_Count register at time @now and handles if the

 * timer interrupt is pending and hasn't been handled yet.

 *

 * Returns:	The current value of the guest CP0_Count register.

 Calculate the biased and scaled guest CP0_Count */

	/*

	 * Find whether CP0_Count has reached the closest timer interrupt. If

	 * not, we shouldn't inject it.

	/*

	 * The CP0_Count we're going to return has already reached the closest

	 * timer interrupt. Quickly check if it really is a new interrupt by

	 * looking at whether the interval until the hrtimer expiry time is

	 * less than 1/4 of the timer period.

		/*

		 * Cancel it while we handle it so there's no chance of

		 * interference with the timeout handler.

 Nothing should be waiting on the timeout */

		/*

		 * Restart the timer if it was running based on the expiry time

		 * we read, so that we don't push it back 2 periods.

/**

 * kvm_mips_read_count() - Read the current count value.

 * @vcpu:	Virtual CPU.

 *

 * Read the current guest CP0_Count value, taking into account whether the timer

 * is stopped.

 *

 * Returns:	The current guest CP0_Count value.

 If count disabled just read static copy of count */

/**

 * kvm_mips_freeze_hrtimer() - Safely stop the hrtimer.

 * @vcpu:	Virtual CPU.

 * @count:	Output pointer for CP0_Count value at point of freeze.

 *

 * Freeze the hrtimer safely and return both the ktime and the CP0_Count value

 * at the point it was frozen. It is guaranteed that any pending interrupts at

 * the point it was frozen are handled, and none after that point.

 *

 * This is useful where the time/CP0_Count is needed in the calculation of the

 * new parameters.

 *

 * Assumes !kvm_mips_count_disabled(@vcpu) (guest CP0_Count timer is running).

 *

 * Returns:	The ktime at the point of freeze.

 stop hrtimer before finding time */

 find count at this point and handle pending hrtimer */

/**

 * kvm_mips_resume_hrtimer() - Resume hrtimer, updating expiry.

 * @vcpu:	Virtual CPU.

 * @now:	ktime at point of resume.

 * @count:	CP0_Count at point of resume.

 *

 * Resumes the timer and updates the timer expiry based on @now and @count.

 * This can be used in conjunction with kvm_mips_freeze_timer() when timer

 * parameters need to be changed.

 *

 * It is guaranteed that a timer interrupt immediately after resume will be

 * handled, but not if CP_Compare is exactly at @count. That case is already

 * handled by kvm_mips_freeze_timer().

 *

 * Assumes !kvm_mips_count_disabled(@vcpu) (guest CP0_Count timer is running).

 Calculate timeout (wrap 0 to 2^32) */

 Update hrtimer to use new timeout */

/**

 * kvm_mips_restore_hrtimer() - Restore hrtimer after a gap, updating expiry.

 * @vcpu:	Virtual CPU.

 * @before:	Time before Count was saved, lower bound of drift calculation.

 * @count:	CP0_Count at point of restore.

 * @min_drift:	Minimum amount of drift permitted before correction.

 *		Must be <= 0.

 *

 * Restores the timer from a particular @count, accounting for drift. This can

 * be used in conjunction with kvm_mips_freeze_timer() when a hardware timer is

 * to be used for a period of time, but the exact ktime corresponding to the

 * final Count that must be restored is not known.

 *

 * It is gauranteed that a timer interrupt immediately after restore will be

 * handled, but not if CP0_Compare is exactly at @count. That case should

 * already be handled when the hardware timer state is saved.

 *

 * Assumes !kvm_mips_count_disabled(@vcpu) (guest CP0_Count timer is not

 * stopped).

 *

 * Returns:	Amount of correction to count_bias due to drift.

 Calculate expected count at before */

	/*

	 * Detect significantly negative drift, where count is lower than

	 * expected. Some negative drift is expected when hardware counter is

	 * set after kvm_mips_freeze_timer(), and it is harmless to allow the

	 * time to jump forwards a little, within reason. If the drift is too

	 * significant, adjust the bias to avoid a big Guest.CP0_Count jump.

 Calculate expected count right now */

	/*

	 * Detect positive drift, where count is higher than expected, and

	 * adjust the bias to avoid guest time going backwards.

 Subtract nanosecond delta to find ktime when count was read */

 Resume using the calculated ktime */

/**

 * kvm_mips_write_count() - Modify the count and update timer.

 * @vcpu:	Virtual CPU.

 * @count:	Guest CP0_Count value to set.

 *

 * Sets the CP0_Count value and updates the timer accordingly.

 Calculate bias */

 The timer's disabled, adjust the static count */

 Update timeout */

/**

 * kvm_mips_init_count() - Initialise timer.

 * @vcpu:	Virtual CPU.

 * @count_hz:	Frequency of timer.

 *

 * Initialise the timer to the specified frequency, zero it, and set it going if

 * it's enabled.

 Starting at 0 */

/**

 * kvm_mips_set_count_hz() - Update the frequency of the timer.

 * @vcpu:	Virtual CPU.

 * @count_hz:	Frequency of CP0_Count timer in Hz.

 *

 * Change the frequency of the CP0_Count timer. This is done atomically so that

 * CP0_Count is continuous and no timer interrupt is lost.

 *

 * Returns:	-EINVAL if @count_hz is out of range.

 *		0 on success.

 ensure the frequency is in a sensible range... */

 ... and has actually changed */

 Safely freeze timer so we can keep it continuous */

 Update the frequency */

 Calculate adjusted bias so dynamic count is unchanged */

 Update and resume hrtimer */

/**

 * kvm_mips_write_compare() - Modify compare and update timer.

 * @vcpu:	Virtual CPU.

 * @compare:	New CP0_Compare value.

 * @ack:	Whether to acknowledge timer interrupt.

 *

 * Update CP0_Compare to a new value and update the timeout.

 * If @ack, atomically acknowledge any pending timer interrupt, otherwise ensure

 * any pending timer interrupt is preserved.

 silence bogus GCC warning */

 if unchanged, must just be an ack */

	/*

	 * If guest CP0_Compare moves forward, CP0_GTOffset should be adjusted

	 * too to prevent guest CP0_Count hitting guest CP0_Compare.

	 *

	 * The new GTOffset corresponds to the new value of CP0_Compare, and is

	 * set prior to it being written into the guest context. We disable

	 * preemption until the new value is written to prevent restore of a

	 * GTOffset corresponding to the old CP0_Compare value.

 freeze_hrtimer() takes care of timer interrupts <= count */

		/*

		 * With VZ, writing CP0_Compare acks (clears) CP0_Cause.TI, so

		 * preserve guest CP0_Cause.TI if we don't want to ack it.

 resume_hrtimer() takes care of timer interrupts > count */

	/*

	 * If guest CP0_Compare is moving backward, we delay CP0_GTOffset change

	 * until after the new CP0_Compare is written, otherwise new guest

	 * CP0_Count could hit new guest CP0_Compare.

/**

 * kvm_mips_count_disable() - Disable count.

 * @vcpu:	Virtual CPU.

 *

 * Disable the CP0_Count timer. A timer interrupt on or before the final stop

 * time will be handled but not after.

 *

 * Assumes CP0_Count was previously enabled but now Guest.CP0_Cause.DC or

 * count_ctl.DC has been set (count disabled).

 *

 * Returns:	The time that the timer was stopped.

 Stop hrtimer */

 Set the static count from the dynamic count, handling pending TI */

/**

 * kvm_mips_count_disable_cause() - Disable count using CP0_Cause.DC.

 * @vcpu:	Virtual CPU.

 *

 * Disable the CP0_Count timer and set CP0_Cause.DC. A timer interrupt on or

 * before the final stop time will be handled if the timer isn't disabled by

 * count_ctl.DC, but not after.

 *

 * Assumes CP0_Cause.DC is clear (count enabled).

/**

 * kvm_mips_count_enable_cause() - Enable count using CP0_Cause.DC.

 * @vcpu:	Virtual CPU.

 *

 * Enable the CP0_Count timer and clear CP0_Cause.DC. A timer interrupt after

 * the start time will be handled if the timer isn't disabled by count_ctl.DC,

 * potentially before even returning, so the caller should be careful with

 * ordering of CP0_Cause modifications so as not to lose it.

 *

 * Assumes CP0_Cause.DC is set (count disabled).

	/*

	 * Set the dynamic count to match the static count.

	 * This starts the hrtimer if count_ctl.DC allows it.

	 * Otherwise it conveniently updates the biases.

/**

 * kvm_mips_set_count_ctl() - Update the count control KVM register.

 * @vcpu:	Virtual CPU.

 * @count_ctl:	Count control register new value.

 *

 * Set the count control KVM register. The timer is updated accordingly.

 *

 * Returns:	-EINVAL if reserved bits are set.

 *		0 on success.

 Only allow defined bits to be changed */

 Apply new value */

 Master CP0_Count disable */

 Is CP0_Cause.DC already disabling CP0_Count? */

 Just record the current time */

 disable timer and record current time */

			/*

			 * Calculate timeout relative to static count at resume

			 * time (wrap 0 to 2^32).

 Handle pending interrupt */

 Nothing should be waiting on the timeout */

 Resume hrtimer without changing bias */

/**

 * kvm_mips_set_count_resume() - Update the count resume KVM register.

 * @vcpu:		Virtual CPU.

 * @count_resume:	Count resume register new value.

 *

 * Set the count resume KVM register.

 *

 * Returns:	-EINVAL if out of valid range (0..now).

 *		0 on success.

	/*

	 * It doesn't make sense for the resume time to be in the future, as it

	 * would be possible for the next interrupt to be more than a full

	 * period in the future.

/**

 * kvm_mips_count_timeout() - Push timer forward on timeout.

 * @vcpu:	Virtual CPU.

 *

 * Handle an hrtimer event by push the hrtimer forward a period.

 *

 * Returns:	The hrtimer_restart value to return to the hrtimer subsystem.

 Add the Count period to the current expiry time */

		/*

		 * We we are runnable, then definitely go off to user space to

		 * check if any I/O interrupts are pending.

	/*

	 * Update PC and hold onto current PC in case there is

	 * an error and we want to rollback the PC

		/*

		 * Loongson-3 overridden sdc2 instructions.

		 * opcode1              instruction

		 *   0x0          gssbx: store 1 bytes from GPR

		 *   0x1          gsshx: store 2 bytes from GPR

		 *   0x2          gsswx: store 4 bytes from GPR

		 *   0x3          gssdx: store 8 bytes from GPR

 Rollback PC if emulation was unsuccessful */

	/*

	 * Find the resume PC now while we have safe and easy access to the

	 * prior branch instruction, and save it for

	 * kvm_mips_complete_mmio_load() to restore later.

 signed */

 unsigned */

 unsigned */

 unsigned */

 1 byte */

 2 bytes */

 3 bytes */

 4 bytes */

 4 bytes */

 3 bytes */

 2 bytes */

 1 byte */

 1 byte */

 2 bytes */

 3 bytes */

 4 bytes */

 5 bytes */

 6 bytes */

 7 bytes */

 8 bytes */

 8 bytes */

 7 bytes */

 6 bytes */

 5 bytes */

 4 bytes */

 3 bytes */

 2 bytes */

 1 byte */

		/*

		 * Loongson-3 overridden ldc2 instructions.

		 * opcode1              instruction

		 *   0x0          gslbx: store 1 bytes from GPR

		 *   0x1          gslhx: store 2 bytes from GPR

		 *   0x2          gslwx: store 4 bytes from GPR

		 *   0x3          gsldx: store 8 bytes from GPR

 signed */

 signed */

 signed */

 signed */

 Restore saved resume PC */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS: Hypercall handling.

 *

 * Copyright (C) 2015  Imagination Technologies Ltd.

 Report unimplemented hypercall to guest */

 read hypcall number and arguments */

 v0 */

 a0 */

 a1 */

 a2 */

 a3 */

 v0 */);

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Generation of main entry point for the guest, exception handling.

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

 *

 * Copyright (C) 2016 Imagination Technologies Ltd.

 Register names */

 _MIPS_SIM == _MIPS_SIM_ABI32 */

 _MIPS_SIM == _MIPS_SIM_ABI64 || _MIPS_SIM == _MIPS_SIM_NABI32 */

 Some CP0 registers */

/*

 * The version of this function in tlbex.c uses current_cpu_type(), but for KVM

 * we assume symmetry.

/**

 * kvm_mips_entry_setup() - Perform global setup for entry code.

 *

 * Perform global setup for entry code, such as choosing a scratch register.

 *

 * Returns:	0 on success.

 *		-errno on failure.

	/*

	 * We prefer to use KScratchN registers if they are available over the

	 * defaults above, which may not work on all cores.

 Pick a scratch register for storing VCPU */

 Pick a scratch register to use as a temp for saving state */

 Save the VCPU scratch register value in cp0_epc of the stack frame */

 Save the temp scratch register value in cp0_cause of stack frame */

	/*

	 * Restore host scratch register values saved by

	 * kvm_mips_build_save_scratch().

/**

 * build_set_exc_base() - Assemble code to write exception base address.

 * @p:		Code buffer pointer.

 * @reg:	Source register (generated code may set WG bit in @reg).

 *

 * Assemble code to modify the exception base address in the EBase register,

 * using the appropriately sized access and setting the WG bit if necessary.

 Set WG so that all the bits get written */

/**

 * kvm_mips_build_vcpu_run() - Assemble function to start running a guest VCPU.

 * @addr:	Address to start writing code.

 *

 * Assemble the start of the vcpu_run function to run a guest VCPU. The function

 * conforms to the following prototype:

 *

 * int vcpu_run(struct kvm_vcpu *vcpu);

 *

 * The exit from the guest and return to the caller is handled by the code

 * generated by kvm_mips_build_ret_to_host().

 *

 * Returns:	Next address after end of written function.

	/*

	 * A0: vcpu

 k0/k1 not being used in host kernel context */

 Save host status */

 Save scratch registers, will be used to store pointer to vcpu etc */

 VCPU scratch register has pointer to vcpu */

 Offset into vcpu->arch */

	/*

	 * Save the host stack to VCPU, used for exception processing

	 * when we exit from the Guest

 Save the kernel gp as well */

	/*

	 * Setup status register for running the guest in UM, interrupts

	 * are disabled

 load up the new EBASE */

	/*

	 * Now that the new EBASE has been loaded, unset BEV, set

	 * interrupt mask as it was but make sure that timer interrupts

	 * are enabled

/**

 * kvm_mips_build_enter_guest() - Assemble code to resume guest execution.

 * @addr:	Address to start writing code.

 *

 * Assemble the code to resume guest execution. This code is common between the

 * initial entry into the guest from the host, and returning from the exit

 * handler back to the guest.

 *

 * Returns:	Next address after end of written function.

 Set Guest EPC */

 Save normal linux process pgd (VZ guarantees pgd_reg is set) */

	/*

	 * Set up KVM GPA pgd.

	 * This does roughly the same as TLBMISS_HANDLER_SETUP_PGD():

	 * - call tlbmiss_handler_setup_pgd(mm->pgd)

	 * - write mm->pgd into CP0_PWBase

	 *

	 * We keep S0 pointing at struct kvm so we can load the ASID below.

 delay slot */

 Set GM bit to setup eret to VZ guest context */

		/*

		 * Set root mode GuestID, so that root TLB refill handler can

		 * use the correct GuestID in the root TLB.

 Get current GuestID */

 Set GuestCtl1.RID = GuestCtl1.ID */

 GuestID handles dealiasing so we don't need to touch ASID */

 Root ASID Dealias (RAD) */

 Save host ASID */

 Set the root ASID for the Guest */

 t1: contains the base of the ASID array, need to get the cpu id  */

 smp_processor_id */

 index the ASID array */

	/*

	 * reuse ASID array offset

	 * cpuinfo_mips is a multiple of sizeof(long)

 Set up KVM VZ root ASID (!guestid) */

 Disable RDHWR access */

 load the guest context from VCPU and return */

 Guest k0/k1 loaded later */

 Restore hi/lo */

 Restore the guest's k0/k1 registers */

 Jump to guest */

/**

 * kvm_mips_build_tlb_refill_exception() - Assemble TLB refill handler.

 * @addr:	Address to start writing code.

 * @handler:	Address of common handler (within range of @addr).

 *

 * Assemble TLB refill exception fast path handler for guest execution.

 *

 * Returns:	Next address after end of written function.

 Save guest k1 into scratch register */

 Get the VCPU pointer from the VCPU scratch register */

 Save guest k0 into VCPU structure */

	/*

	 * Some of the common tlbex code uses current_cpu_type(). For KVM we

	 * assume symmetry and just disable preemption to silence the warning.

 global page dir */

 middle page dir */

 even */

 odd */

	/*

	 * Now for the actual refill bit. A lot of this can be common with the

	 * Linux TLB refill handler, however we don't need to handle so many

	 * cases. We only need to handle user mode refills, and user mode runs

	 * with 32-bit addressing.

	 *

	 * Therefore the branch to label_vmalloc generated by build_get_pmde64()

	 * that isn't resolved should never actually get taken and is harmless

	 * to leave in place for now.

 get pmd in K1 */

 get pgd in K1 */

 we don't support huge pages yet */

 Get the VCPU pointer from the VCPU scratch register again */

 Restore the guest's k0/k1 registers */

 Jump to guest */

/**

 * kvm_mips_build_exception() - Assemble first level guest exception handler.

 * @addr:	Address to start writing code.

 * @handler:	Address of common handler (within range of @addr).

 *

 * Assemble exception vector code for guest execution. The generated vector will

 * branch to the common exception handler generated by kvm_mips_build_exit().

 *

 * Returns:	Next address after end of written function.

 Save guest k1 into scratch register */

 Get the VCPU pointer from the VCPU scratch register */

 Save guest k0 into VCPU structure */

 Branch to the common handler */

/**

 * kvm_mips_build_exit() - Assemble common guest exit handler.

 * @addr:	Address to start writing code.

 *

 * Assemble the generic guest exit handling code. This is called by the

 * exception vectors (generated by kvm_mips_build_exception()), and calls

 * kvm_mips_handle_exit(), then either resumes the guest or returns to the host

 * depending on the return value.

 *

 * Returns:	Next address after end of written function.

	/*

	 * Generic Guest exception handler. We end up here when the guest

	 * does something that causes a trap to kernel mode.

	 *

	 * Both k0/k1 registers will have already been saved (k0 into the vcpu

	 * structure, and k1 into the scratch_tmp register).

	 *

	 * The k1 register will already contain the kvm_vcpu_arch pointer.

 Start saving Guest context to VCPU */

 Guest k0/k1 saved later */

 We need to save hi/lo and restore them on the way out */

 Finally save guest k1 to VCPU */

 Now that context has been saved, we can use other registers */

 Restore vcpu */

	/*

	 * Save Host level EPC, BadVaddr and Cause to VCPU, useful to process

	 * the exception

 Now restore the host state just enough to run the handlers */

 Switch EBASE to the one used by Linux */

 load up the host EBASE */

		/*

		 * If FPU is enabled, save FCR31 and clear it so that later

		 * ctc1's don't trigger FPE for pending exceptions.

		/*

		 * If MSA is enabled, save MSACSR and clear it so that later

		 * instructions don't trigger MSAFPE for pending exceptions.

 MIPS_CONF5_MSAEN */

 Restore host ASID */

	/*

	 * Set up normal Linux process pgd.

	 * This does roughly the same as TLBMISS_HANDLER_SETUP_PGD():

	 * - call tlbmiss_handler_setup_pgd(mm->pgd)

	 * - write mm->pgd into CP0_PWBase

 delay slot */

 Clear GM bit so we don't enter guest mode when EXL is cleared */

 Save GuestCtl0 so we can access GExcCode after CPU migration */

		/*

		 * Clear root mode GuestID, so that root TLB operations use the

		 * root GuestID in the root TLB.

 Set GuestCtl1.RID = MIPS_GCTL1_ROOT_GUESTID (i.e. 0) */

 Now that the new EBASE has been loaded, unset BEV and KSU_USER */

 Load up host GP */

 Need a stack before we can jump to "C" */

 Saved host state */

	/*

	 * XXXKYMA do we need to load the host ASID, maybe not because the

	 * kernel entries are marked GLOBAL, need to verify

 Restore host scratch registers, as we'll have clobbered them */

 Restore RDHWR access */

 Jump to handler */

	/*

	 * XXXKYMA: not sure if this is safe, how large is the stack??

	 * Now jump to the kvm_mips_handle_exit() to see if we can deal

	 * with this in the kernel

/**

 * kvm_mips_build_ret_from_exit() - Assemble guest exit return handler.

 * @addr:	Address to start writing code.

 *

 * Assemble the code to handle the return from kvm_mips_handle_exit(), either

 * resuming the guest or returning to the host depending on the return value.

 *

 * Returns:	Next address after end of written function.

 Return from handler Make sure interrupts are disabled */

	/*

	 * XXXKYMA: k0/k1 could have been blown away if we processed

	 * an exception while we were handling the exception from the

	 * guest, reload k1

	/*

	 * Check return value, should tell us if we are returning to the

	 * host (handle I/O etc)or resuming the guest

/**

 * kvm_mips_build_ret_to_guest() - Assemble code to return to the guest.

 * @addr:	Address to start writing code.

 *

 * Assemble the code to handle return from the guest exit handler

 * (kvm_mips_handle_exit()) back to the guest.

 *

 * Returns:	Next address after end of written function.

 Put the saved pointer to vcpu (s0) back into the scratch register */

 Load up the Guest EBASE to minimize the window where BEV is set */

 Switch EBASE back to the one used by KVM */

 Setup status register for running guest in UM */

/**

 * kvm_mips_build_ret_to_host() - Assemble code to return to the host.

 * @addr:	Address to start writing code.

 *

 * Assemble the code to handle return from the guest exit handler

 * (kvm_mips_handle_exit()) back to the host, i.e. to the caller of the vcpu_run

 * function generated by kvm_mips_build_vcpu_run().

 *

 * Returns:	Next address after end of written function.

 EBASE is already pointing to Linux */

	/*

	 * r2/v0 is the return code, shift it down by 2 (arithmetic)

	 * to recover the err code

 Load context saved on the host stack */

 Restore RDHWR access */

 Restore RA, which is the address we will return to */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * KVM/MIPS MMU handling in the KVM module.

 *

 * Copyright (C) 2012  MIPS Technologies, Inc.  All rights reserved.

 * Authors: Sanjay Lal <sanjayl@kymasys.com>

/*

 * KVM_MMU_CACHE_MIN_PAGES is the number of GPA page table translation levels

 * for which pages need to be cached.

/**

 * kvm_pgd_init() - Initialise KVM GPA page directory.

 * @page:	Pointer to page directory (PGD) for KVM GPA.

 *

 * Initialise a KVM GPA page directory with pointers to the invalid table, i.e.

 * representing no mappings. This is similar to pgd_init(), however it

 * initialises all the page directory pointers, not just the ones corresponding

 * to the userland address space (since it is for the guest physical address

 * space rather than a virtual address space).

/**

 * kvm_pgd_alloc() - Allocate and initialise a KVM GPA page directory.

 *

 * Allocate a blank KVM GPA page directory (PGD) for representing guest physical

 * to host physical page mappings.

 *

 * Returns:	Pointer to new KVM GPA page directory.

 *		NULL on allocation failure.

/**

 * kvm_mips_walk_pgd() - Walk page table with optional allocation.

 * @pgd:	Page directory pointer.

 * @addr:	Address to index page table using.

 * @cache:	MMU page cache to allocate new page tables from, or NULL.

 *

 * Walk the page tables pointed to by @pgd to find the PTE corresponding to the

 * address @addr. If page tables don't exist for @addr, they will be created

 * from the MMU cache if @cache is not NULL.

 *

 * Returns:	Pointer to pte_t corresponding to @addr.

 *		NULL if a page table doesn't exist for @addr and !@cache.

 *		NULL if a page table allocation failed.

 Not used on MIPS yet */

 Caller must hold kvm->mm_lock */

/*

 * kvm_mips_flush_gpa_{pte,pmd,pud,pgd,pt}.

 * Flush a range of guest physical address space from the VM's GPA page tables.

/**

 * kvm_mips_flush_gpa_pt() - Flush a range of guest physical addresses.

 * @kvm:	KVM pointer.

 * @start_gfn:	Guest frame number of first page in GPA range to flush.

 * @end_gfn:	Guest frame number of last page in GPA range to flush.

 *

 * Flushes a range of GPA mappings from the GPA page tables.

 *

 * The caller must hold the @kvm->mmu_lock spinlock.

 *

 * Returns:	Whether its safe to remove the top level page directory because

 *		all lower levels have been removed.

 returns true if anything was done */					\

/*

 * kvm_mips_mkclean_gpa_pt.

 * Mark a range of guest physical address space clean (writes fault) in the VM's

 * GPA page table to allow dirty page tracking.

/**

 * kvm_mips_mkclean_gpa_pt() - Make a range of guest physical addresses clean.

 * @kvm:	KVM pointer.

 * @start_gfn:	Guest frame number of first page in GPA range to flush.

 * @end_gfn:	Guest frame number of last page in GPA range to flush.

 *

 * Make a range of GPA mappings clean so that guest writes will fault and

 * trigger dirty page logging.

 *

 * The caller must hold the @kvm->mmu_lock spinlock.

 *

 * Returns:	Whether any GPA mappings were modified, which would require

 *		derived mappings (GVA page tables & TLB enties) to be

 *		invalidated.

/**

 * kvm_arch_mmu_enable_log_dirty_pt_masked() - write protect dirty pages

 * @kvm:	The KVM pointer

 * @slot:	The memory slot associated with mask

 * @gfn_offset:	The gfn offset in memory slot

 * @mask:	The mask of dirty pages at offset 'gfn_offset' in this memory

 *		slot to be write protected

 *

 * Walks bits set in mask write protects the associated pte's. Caller must

 * acquire @kvm->mmu_lock.

/*

 * kvm_mips_mkold_gpa_pt.

 * Mark a range of guest physical address space old (all accesses fault) in the

 * VM's GPA page table to allow detection of commonly used pages.

 Mapping may need adjusting depending on memslot flags */

 Replacing an absent or old page doesn't need flushes */

 Pages swapped, aged, moved, or cleaned require flushes */

/**

 * _kvm_mips_map_page_fast() - Fast path GPA fault handler.

 * @vcpu:		VCPU pointer.

 * @gpa:		Guest physical address of fault.

 * @write_fault:	Whether the fault was due to a write.

 * @out_entry:		New PTE for @gpa (written on success unless NULL).

 * @out_buddy:		New PTE for @gpa's buddy (written on success unless

 *			NULL).

 *

 * Perform fast path GPA fault handling, doing all that can be done without

 * calling into KVM. This handles marking old pages young (for idle page

 * tracking), and dirtying of clean pages (for dirty page logging).

 *

 * Returns:	0 on success, in which case we can update derived mappings and

 *		resume guest execution.

 *		-EFAULT on failure due to absent GPA mapping or write to

 *		read-only page, in which case KVM must be consulted.

 silence bogus GCC warning */

 Fast path - just check GPA page table for an existing entry */

 Track access to pages marked old */

 call kvm_set_pfn_accessed() after unlock */

 Track dirtying of writeable pages */

/**

 * kvm_mips_map_page() - Map a guest physical page.

 * @vcpu:		VCPU pointer.

 * @gpa:		Guest physical address of fault.

 * @write_fault:	Whether the fault was due to a write.

 * @out_entry:		New PTE for @gpa (written on success unless NULL).

 * @out_buddy:		New PTE for @gpa's buddy (written on success unless

 *			NULL).

 *

 * Handle GPA faults by creating a new GPA mapping (or updating an existing

 * one).

 *

 * This takes care of marking pages young or dirty (idle/dirty page tracking),

 * asking KVM for the corresponding PFN, and creating a mapping in the GPA page

 * tables. Derived mappings (GVA page tables and TLBs) must be handled by the

 * caller.

 *

 * Returns:	0 on success, in which case the caller may use the @out_entry

 *		and @out_buddy PTEs to update derived mappings and resume guest

 *		execution.

 *		-EFAULT if there is no memory region at @gpa or a write was

 *		attempted to a read-only memory region. This is usually handled

 *		as an MMIO access.

 Try the fast path to handle old / clean pages */

 We need a minimum of cached pages ready for page table creation */

	/*

	 * Used to check for invalidations in progress, of the pfn that is

	 * returned by pfn_to_pfn_prot below.

	/*

	 * Ensure the read of mmu_notifier_seq isn't reordered with PTE reads in

	 * gfn_to_pfn_prot() (which calls get_user_pages()), so that we don't

	 * risk the page we get a reference to getting unmapped before we have a

	 * chance to grab the mmu_lock without mmu_notifier_retry() noticing.

	 *

	 * This smp_rmb() pairs with the effective smp_wmb() of the combination

	 * of the pte_unmap_unlock() after the PTE is zapped, and the

	 * spin_lock() in kvm_mmu_notifier_invalidate_<page|range_end>() before

	 * mmu_notifier_seq is incremented.

 Slow path - ask KVM core whether we can access this GPA */

 Check if an invalidation has taken place since we got pfn */

		/*

		 * This can happen when mappings are changed asynchronously, but

		 * also synchronously if a COW is triggered by

		 * gfn_to_pfn_prot().

 Ensure page tables are allocated */

 Set up the PTE */

 Write the PTE */

 Invalidate this entry in the TLB */

/**

 * kvm_mips_migrate_count() - Migrate timer.

 * @vcpu:	Virtual CPU.

 *

 * Migrate CP0_Count hrtimer to the current CPU by cancelling and restarting it

 * if it was running prior to being cancelled.

 *

 * Must be called when the VCPU is migrated to a different CPU to ensure that

 * timer expiry during guest execution interrupts the guest and causes the

 * interrupt to be delivered in a timely manner.

 Restore ASID once we are scheduled back after preemption */

		/*

		 * Migrate the timer interrupt to the current CPU so that it

		 * always interrupts the guest and synchronously triggers a

		 * guest timer interrupt.

 restore guest state to registers */

 ASID can change if another task is scheduled during preemption */

 save guest state in registers */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Loongson-3 Virtual IPI interrupt support.

 *

 * Copyright (C) 2019  Loongson Technologies, Inc.  All rights reserved.

 *

 * Authors: Chen Zhu <zhuchen@loongson.cn>

 * Authors: Huacai Chen <chenhc@lemote.com>

 Assume len == 4 */

 Assume len == 4 */

	/*

	 * Initialize IPI device

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright MontaVista Software Inc

 * Copyright (C) 2003 Atheros Communications, Inc.,  All Rights Reserved.

 * Copyright (C) 2006 FON Technology, SL.

 * Copyright (C) 2006 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2006 Felix Fietkau <nbd@openwrt.org>

/*

 * Prom setup file for AR5312/AR231x SoCs

 SPDX-License-Identifier: GPL-2.0

 CONFIG_SERIAL_8250_CONSOLE */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2010 Gabor Juhos <juhosg@openwrt.org>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 Atheros Communications, Inc.,  All Rights Reserved.

 * Copyright (C) 2006 FON Technology, SL.

 * Copyright (C) 2006 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2006-2009 Felix Fietkau <nbd@openwrt.org>

 offset for flash magic */

 config magic found */

	/* broken board data detected, use radio data to find the

	/*

	 * Now find the start of Radio Configuration data, using heuristics:

	 * Search forward from Board Configuration data by 0x1000 bytes

	 * at a time until we find non-0xffffffff.

 AR2316 relocates radio config to new location */

/*

 * NB: Search region size could be larger than the actual flash size,

 * but this shouldn't be a problem here, because the flash

 * will simply be mapped multiple times.

	/* Copy the board and radio data to RAM, because accessing the mapped

 Try to find valid board and radio data */

 If that fails, try to at least find valid radio data */

	/* Radio config starts 0x100 bytes after board config, regardless

 Disable data watchpoints */

 Initialize interrupt controllers */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 Atheros Communications, Inc.,  All Rights Reserved.

 * Copyright (C) 2006 FON Technology, SL.

 * Copyright (C) 2006 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2006 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2012 Alexandros C. Couloumbis <alex@ozo.com>

/*

 * Platform devices for Atheros AR2315 SoCs

 Catastrophic failure */

/*

 * Called when an interrupt is received, this function

 * determines exactly which interrupt it was, and it

 * invokes the appropriate handler.

 *

 * Implicitly, we also define interrupt priority by

 * choosing which to dispatch first.

 Find board configuration */

 try reset the system via reset control */

	/* Cold reset does not work on the AR2315/6, use the GPIO reset bits

	 * a workaround. Give it some time to attempt a gpio based hardware

 TODO: implement the GPIO reset workaround */

	/* Some boards (e.g. Senao EOC-2610) don't implement the reset logic

	 * workaround. Attempt to jump to the mips reset location -

/*

 * This table is indexed by bits 5..4 of the CLOCKCTL1 register

 * to determine the predevisor value.

 clkm input selected */

 Detect memory size */

 Detect the hardware based on the device ID */

 Need to check */

 Clear any lingering AHB errors */

 Reset PCI DMA logic */

 Configure endians */

 Configure as PCI host with DMA */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 Atheros Communications, Inc.,  All Rights Reserved.

 * Copyright (C) 2006 FON Technology, SL.

 * Copyright (C) 2006 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2006-2009 Felix Fietkau <nbd@openwrt.org>

 * Copyright (C) 2012 Alexandros C. Couloumbis <alex@ozo.com>

/*

 * Platform devices for Atheros AR5312 SoCs

 clears error */

 clears error */

 Catastrophic failure */

 Enable the specified AR5312_MISC_IRQ interrupt */

 Disable the specified AR5312_MISC_IRQ interrupt */

 flush write buffer */

 fixup flash width */

	/*

	 * Configure flash bank 0.

	 * Assume 8M window size. Flash will be aliased if it's smaller

 Disable other flash banks */

 Locate board/radio config data */

 AR2313 has CPU minor rev. 10 */

 AR2312 shares the same Silicon ID as AR5312 */

 Everything else is probably AR5312 or compatible */

 reset the system */

/*

 * This table is indexed by bits 5..4 of the CLOCKCTL1 register

 * to determine the predevisor value.

 Trust the bootrom's idea of cpu frequency. */

 AR5312 and AR2312 */

	/*

	 * Clocking is derived from a fixed 40MHz input clock.

	 *

	 *  cpu_freq = input_clock * MULT (where MULT is PLL multiplier)

	 *  sys_freq = cpu_freq / 4	  (used for APB clock, serial,

	 *				   flash, Timer, Watchdog Timer)

	 *

	 *  cnt_freq = cpu_freq / 2	  (use for CPU count/compare)

	 *

	 * So, for example, with a PLL multiplier of 5, we have

	 *

	 *  cpu_freq = 200MHz

	 *  sys_freq = 50MHz

	 *  cnt_freq = 100MHz

	 *

	 * We compute the CPU frequency, based on PLL settings.

 Detect memory size */

 Clear any lingering AHB errors */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001, 2002 Broadcom Corporation

/*

 *

 * Broadcom Common Firmware Environment (CFE)

 *

 * This module contains device function stubs (small routines to

 * call the standard "iocb" interface entry point to CFE).

 * There should be one routine here per iocb function call.

 *

 * Authors:  Mitch Lichtenberg, Chris Demetriou

 Cast from a native pointer to a cfe_xptr_t and back.	 */

/*

 * Declare the dispatch function with args of "intptr_t".

 * This makes sure whatever model we're compiling in

 * puts the pointers in a single register.  For example,

 * combining -mlong64 and -mips1 or -mips2 would lead to

 * trouble, since the handle and IOCB pointer will be

 * passed in two registers each, and CFE expects one.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2012 MIPS Technologies, Inc.  All rights reserved.

 Validate command line parameters. */

 Validate environment pointer. */

		/*

		 * Return a pointer to the given environment variable.

		 * YAMON uses "name", "value" pairs, while U-Boot uses

		 * "name=value".

 Increment array index. */

/*

 * Big Endian PROM code for SNI RM machines

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005-2006 Florian Lohoff (flo@rfc822.org)

 * Copyright (C) 2005-2006 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

 special SNI prom calls */

/*

 * This does not exist in all proms - SINIX compares

 * the prom env variable "version" against "2.0008"

 * or greater. If lesser it tries to probe interesting

 * registers

 O32 stack has to be 8-byte aligned. */

/*

 * /proc/cpuinfo system type

 *

 MemSIZE from prom in 16MByte chunks */

 get memory bank layout from prom */

 copy prom cmdline parameters to kernel cmdline */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * identify.c: identify machine by looking up system identifier

 *

 * Copyright (C) 1998 Thomas Bogendoerfer

 *

 * This code is based on arch/mips/sgi/kernel/system.c, which is

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

	/*

	 * The root component tells us what machine architecture we have here.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * cmdline.c: Kernel command line creation using ARCS argc/argv.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

/*

 * A 32-bit ARC PROM pass arguments and environment as 32-bit pointer.

 * These macro take care of sign extension.

 Always ignore argv[0] */

 Ok, we want it. First append the replacement... */

 ... and now the argument */

 Always ignore argv[0] */

	/*

	 * Move ARC variables to the beginning to make sure they can be

	 * overridden by later arguments.

 Ok, we want it. */

 get rid of trailing space */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Miscellaneous ARCS PROM routines.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1999 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999 Silicon Graphics, Inc.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996 David S. Miller (dm@sgi.com)

 * Compatibility with board caches, Ulf Carlsson

/*

 * For 64bit kernels working with a 32bit ARC PROM pointer arguments

 * for ARC calls need to reside in CKEG0/1. But as soon as the kernel

 * switches to it's first kernel thread stack is set to an address in

 * XKPHYS, so anything on stack can't be used anymore. This is solved

 * by using a * static declartion variables are put into BSS, which is

 * linked to a CKSEG0 address. Since this is only used on UP platforms

 * there is not spinlock needed

/*

 * IP22 boardcache is not compatible with board caches.	 Thus we disable it

 * during romvec action.  Since r4xx0.c is always compiled and linked with your

 * kernel, this shouldn't cause any harm regardless what MIPS processor you

 * have.

 *

 * The ARC write and read functions seem to interfere with the serial lines

 * in some way. You should be careful with them.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * PROM library initialisation code.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 Master romvec interface. */

 stack for calling 32bit ARC prom */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * ARC firmware interface.

 *

 * Copyright (C) 1994, 1995, 1996, 1999 Ralf Baechle

 * Copyright (C) 1999 Silicon Graphics, Inc.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * env.c: ARCS environment variable routines.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 SPDX-License-Identifier: GPL-2.0

/*

 * Wrap-around code for a console using the

 * ARC io-routines.

 *

 * Copyright (c) 1998 Harald Koerfgen

 * Copyright (c) 2001 Ralf Baechle

 * Copyright (c) 2002 Thiemo Seufer

 Do each character */

/*

 *    Register console.

 SPDX-License-Identifier: GPL-2.0

/*

 * memory.c: PROM library functions for acquiring/using memory descriptors

 *	     given to us from the ARCS firmware.

 *

 * Copyright (C) 1996 by David S. Miller

 * Copyright (C) 1999, 2000, 2001 by Ralf Baechle

 * Copyright (C) 1999, 2000 by Silicon Graphics, Inc.

 *

 * PROM library functions for acquiring/using memory descriptors given to us

 * from the ARCS firmware.  This is only used when CONFIG_ARC_MEMORY is set

 * because on some machines like SGI IP27 the ARC memory configuration data

 * completely bogus and alternate easier to use mechanisms are available.

/*

 * For ARC firmware memory functions the unit of meassuring memory is always

 * a 4k page of memory

 convenient for debugging */

 Nuke warning.  */

 Nuke warning.  */

 SGI is ``different'' ... */

 ignore mirrored RAM on IP28/IP30 */

	/*

	 * at this point it isn't safe to call PROM functions

	 * give platforms a way to do PROM cleanups

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 magic write/read */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 Input PPS Registers */

 Output PPS Registers */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 Default early console parameters */

 UART1(x == 0) - UART6(x == 5) */

	/*

	 * arch_mem_init() has not been called yet, so we don't have a real

	 * command line setup if using CONFIG_CMDLINE_BOOL.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson, joshua.henderson@microchip.com

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

	/*

	 * Load the builtin device tree. This causes the chosen node to be

	 * parsed resulting in our memory appearing.

 sentinel */}

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Purna Chandra Mandal, purna.mandal@microchip.com

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 Boot Status */

 Device Inforation */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Joshua Henderson <joshua.henderson@microchip.com>

 * Copyright (C) 2015 Microchip Technology Inc.  All rights reserved.

 Oscillators, PLL & clocks */

 SPDX-License-Identifier: GPL-2.0-only

 If libc provides le{16,32,64}toh() then we'll use them */

 MIPS opcodes, in bits 31:26 of an instruction */

 Bits 20:16 of OP_REGIMM instructions */

 Bits 5:0 of OP_SPECIAL instructions */

 Bits 31:11 should all be zeroes */

 Bits 5:0 specify the SYNC special encoding */

	/*

	 * Every LL must be preceded by a sync instruction in order to ensure

	 * that instruction reordering doesn't allow a prior memory access to

	 * execute after the LL & cause erroneous results.

 Find the matching SC instruction */

	/*

	 * Check branches within the LL/SC loop target sync instructions,

	 * ensuring that speculative execution can't generate memory accesses

	 * due to instructions outside of the loop.

		/*

		 * If the branch target is within the LL/SC loop then we don't

		 * need to worry about it.

 If the branch targets a sync instruction we're all good... */

 ...but if not, we have a problem */

	/*

	 * Skip the first instructionm allowing check_ll to look backwards

	 * unconditionally.

 Now scan through the code looking for LL instructions */

 SPDX-License-Identifier: GPL-2.0

 If libc provides [bl]e{32,64}toh() then we'll use them */

 Sign extend to form a canonical address */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip27-irq.c: Highlevel interrupt handling for IP27 architecture.

 *

 * Copyright (C) 1999, 2000 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 1999 - 2001 Kanoj Sarcar

 use CPU connected to nearest hub */

 Make sure it's not already pending when we connect it. */

/*

 * This code is unnecessarily complex, because we do

 * intr enabling. Basically, once we grab the set of intrs we need

 * to service, we must mask _all_ these interrupts; firstly, to make

 * sure the same intr does not intr again, causing recursion that

 * can lead to stack overflow. Secondly, we can not just mask the

 * one intr we are do_IRQing, because the non-masked intrs in the

 * first set might intr again, causing multiple servicings of the

 * same intr. This effect is mostly seen for intercpu intrs.

 * Kanoj 05.13.00

 copied from Irix intpend0() */

 Pick intrs we should look at */

 copied from Irix intpend0() */

 Pick intrs we should look at */

	/*

	 * Some interrupts are reserved by hardware or by software convention.

	 * Mark these as reserved right away so they won't be used accidentally

	 * later.

 SPDX-License-Identifier: GPL-2.0

/*

 * Let's see what else we need to do here. Set up sp, gp?

/*

 * Copy the cpu registers which have been saved in the IP27prom format

 * into the eframe format for the node under consideration.

 Get the pointer to the current cpu's register set. */

	/*

	 * Saved main processor registers

	/*

	 * Saved cp0 registers

 Slice A */

 Slice B */

/*

 * Copy the cpu registers which have been saved in the IP27prom format

 * into the eframe format for the node under consideration.

 Save the registers into eframe for each cpu */

/*

 * Save the nmi cpu registers for all cpus in the system.

	/*

	 * Only allow 1 cpu to proceed

	/*

	 * Wait up to 15 seconds for the other cpus to respond to the NMI.

	 * If a cpu has not responded after 10 sec, send it 1 additional NMI.

	 * This is for 2 reasons:

	 *	- sometimes a MMSC fail to NMI all cpus.

	 *	- on 512p SN0 system, the MMSC will only send NMIs to

	 *	  half the cpus. Unfortunately, we don't know which cpus may be

	 *	  NMIed - it depends on how the site chooses to configure.

	 *

	 * Note: it has been measure that it takes the MMSC up to 2.3 secs to

	 * send NMIs to all cpus on a 256p system.

						/*

						 * cputonasid, cputoslice

						 * needs kernel cpuid

	/*

	 * Save the nmi cpu registers for all cpu in the eframe format.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999, 2000 Silcon Graphics, Inc.

 * Copyright (C) 2004 Christoph Hellwig.

 *

 * Generic XTALK initialization code

 Xbow in Xbridge */

 Lowest external port */

	/*

	 * found xbow, so may have multiple bridges

	 * need to probe xbow

	/*

	 * Okay, here's a xbow. Let's arbitrate and find

	 * out if we should initialize it. Set enabled

	 * hub connected at highest or lowest widget as

	 * master.

 check whether the link is up */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1999, 2000, 05, 06 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

	/*

	 * Ack

/*

 * This is a hack; we really need to figure these values out dynamically

 *

 * Since 800 ns works very well with various HUB frequencies, such as

 * 360, 380, 390 and 400 MHZ, we use 800 ns rtc cycle time.

 *

 * Ralf: which clock rate is used to feed the counter?

	/*

	 * We only need to initialize the current node.

	 * If this is not the current node then it is a cpuless

	 * node and timeouts will not happen there.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Reset an IP27.

 *

 * Copyright (C) 1997, 1998, 1999, 2000, 06 by Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 Silence gcc.	 */

 XXX How to pass the reboot command to the firmware??? */

 To do ...  */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1999, 2000 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 next component */

 Didn't find it. */

 Search all boards stored on this node. */

 Didn't find it. */

 Search all boards stored on this node. */

 Didn't find it. */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000, 05 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2000 by Silicon Graphics, Inc.

 * Copyright (C) 2004 by Christoph Hellwig

 *

 * On SGI IP27 the ARC memory configuration data is completely bogus but

 * alternate easier to use mechanisms are available.

 Figure out which routers nodes in question are connected to */

 Find the node board */

 Get the memory bank structure */

 Size in _Megabytes_ */

 hack for 128 dimm banks */

 size in bytes */

	/*

	 * Probe for all CPUs - this creates the cpumask and sets up the

	 * mapping tables.  We need to do this as early as possible.

	/*

	 * Set all nodes' calias sizes to 8k

		/*

		 * Always have node 0 in the region mask, otherwise

		 * CALIAS accesses get exceptions since the hub

		 * thinks it is a node 0 address.

		/*

		 * Set up all hubs to have a big window pointing at

		 * widget 0. Memory mode, widget 0, offset 0

 Hack to detect problem configs */

			/*

			 * We need to refine the hack when we have replicated

			 * kernel text.

	/*

	 * Allocate the node data structures on the node first.

/*

 * A node with nothing.	 We use it to avoid any special casing in

 * cpumask_of_node

/*

 * Currently, the intranode memory hole support assumes that each slot

 * contains at least 32 MBytes of memory. We assume all bootmem data

 * fits on the first slot.

 This comes from node 0 */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994, 1995, 1996, 1999, 2000 by Ralf Baechle

 * Copyright (C) 1999, 2000 by Silicon Graphics

 * Copyright (C) 2002  Maciej W. Rozycki

 for SIGBUS */

 schow_regs(), force_sig() */

 XXX Initialize all the Hub & Bridge error handling here.  */

 Disable error stack */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001, 2002 Ralf Baechle

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1992-1997, 2000-2003 Silicon Graphics, Inc.

 * Copyright (C) 2004 Christoph Hellwig.

 *

 * Support functions for the HUB ASIC - mostly PIO mapping related.

/**

 * hub_pio_map	-  establish a HUB PIO mapping

 *

 * @hub:	hub to perform PIO mapping on

 * @widget:	widget ID to perform PIO mapping for

 * @xtalk_addr: xtalk_address that needs to be mapped

 * @size:	size of the PIO mapping

 *

 use small-window mapping if possible */

		/*

		 * The code below does a PIO write to setup an ITTE entry.

		 *

		 * We need to prevent other CPUs from seeing our updated

		 * memory shadow of the ITTE (in the piomap) until the ITTE

		 * entry is actually set up; otherwise, another CPU might

		 * attempt a PIO prematurely.

		 *

		 * Also, the only way we can know that an entry has been

		 * received  by the hub and can be used by future PIO reads/

		 * writes is by reading back the ITTE entry after writing it.

		 *

		 * For these two reasons, we PIO read back the ITTE entry

		 * after we write it.

/*

 * hub_setup_prb(nasid, prbnum, credits, conveyor)

 *

 *	Put a PRB into fire-and-forget mode if conveyor isn't set.  Otherwise,

 *	put it into conveyor belt mode with the specified number of credits.

	/*

	 * Get the current register value.

	/*

	 * Clear out some fields.

	/*

	 * Enable or disable fire-and-forget mode.

	/*

	 * Set the appropriate number of PIO credits for the widget.

	/*

	 * Store the new value to the register.

/**

 * hub_set_piomode  -  set pio mode for a given hub

 *

 * @nasid:	physical node ID for the hub in question

 *

 * Put the hub into either "PIO conveyor belt" mode or "fire-and-forget" mode.

 * To do this, we have to make absolutely sure that no PIOs are in progress

 * so we turn off access to all widgets for the duration of the function.

 *

 * XXX - This code should really check what kind of widget we're talking

 * to.	Bridges can only handle three requests, but XG will do more.

 * How many can crossbow handle to widget 0?  We're assuming 1.

 *

 * XXX - There is a bug in the crossbow that link reset PIOs do not

 * return write responses.  The easiest solution to this problem is to

 * leave widget 0 (xbow) in fire-and-forget mode at all times.	This

 * only affects pio's to xbow registers, which should be rare.

		/*

		 * Assume a bridge here.

		/*

		 * Assume a crossbow here.

	/*

	 * XXX - Here's where we should take the widget type into

	 * when account assigning credits.

/*

 * hub_pio_init	 -  PIO-related hub initialization

 *

 * @hub:	hubinfo structure for our hub

 initialize big window piomaps for this hub */

/*

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * Copyright (C) 2000 - 2001 by Kanoj Sarcar (kanoj@sgi.com)

 * Copyright (C) 2000 - 2001 by Silicon Graphics, Inc.

 Only let it join in if it's marked enabled */

	/*

	 * Set the interrupt bit associated with the CPU we want to

	 * send the interrupt to.

/*

 * Launch a slave into smp_bootstrap().	 It doesn't take an argument, and we

 * set sp to the kernel stack of the newly created idle process, gp to the proc

 * struct so that current_thread_info() will work.

	/*

	 * PROM sets up system, that boot cpu is always first CPU on nasid 0

 We already did everything necessary earlier */

/*

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file "COPYING" in the main directory of this

 * archive for more details.

 *

 * Copyright (C) 2000 - 2001 by Kanoj Sarcar (kanoj@sgi.com)

 * Copyright (C) 2000 - 2001 by Silicon Graphics, Inc.

	/*

	 * Set CRB timeout at 5ms, (< PI timeout of 10ms)

 copy exception handlers from first node to current node */

 switch to node local exception handlers */

 Install our NMI handler if symmon hasn't installed one. */

	/*

	 * hub_rtc init and cpu clock intr enabled for later calibrate_delay.

	/*

	 * Try to catch kernel missconfigurations and give user an

	 * indication what option to select.

 SPDX-License-Identifier: GPL-2.0

/*

 * Ported from IRIX to Linux by Kanoj Sarcar, 06/08/00.

 * Copyright 2000 - 2001 Silicon Graphics, Inc.

 * Copyright 2000 - 2001 Kanoj Sarcar (kanoj@sgi.com)

/*

 * XXX - This needs to be much smarter about where it puts copies of the

 * kernel.  For example, we should never put a copy on a headless node,

 * and we should respect the topology of the machine.

 Set only the master cnode's bit.  The master cnode is always 0. */

 Advertise that we have a copy of the kernel */

 Set up a GDA pointer to the replication mask. */

 XXX - When the BTE works, we should use it instead of this. */

 Record where the master node should get its kernel text */

 Check if this node should get a copy of the kernel */

 Record where this node should get its kernel text */

/*

 * Return pfn of first free page of memory on a node. PROM may allocate

 * data structures on the first couple of pages of the first slot of each

 * node. If this is the case, getfirstfree(node) > getslotstart(node, 0).

/*

 *  Copyright (C) 2004 Florian Schirmer <jolt@tuxbox.org>

 *  Copyright (C) 2006 Felix Fietkau <nbd@openwrt.org>

 *  Copyright (C) 2006 Michael Buesch <m@bues.ch>

 *  Copyright (C) 2010 Waldemar Brodkorb <wbx@openadk.org>

 *  Copyright (C) 2010-2012 Hauke Mehrtens <hauke@hauke-m.de>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 Set the watchdog timer to reset immediately */

 Disable interrupts and watchdog and spin forever */

 swap serial ports */

/*

 * Memory setup is done in the early part of MIPS's arch_mem_init. It's supposed

 * to detect memory and record it with memblock_add.

 * Any extra initializaion performed here must not use kmalloc or bootmem.

/*

 * This finishes bus initialization doing things that were not possible without

 * kmalloc. Make sure to call it late enough (after mm_init).

 With bus initialized we can access NVRAM and detect the board */

 Nothing to do */

		/* The BCM4706 has a problem with the CPU wait instruction.

		 * When r4k_wait or r4k_wait_irqoff is used will just hang and

		 * not return from a msleep(). Removing the cpu_wait

		 * functionality is a workaround for this problem. The BCM4716

		 * does not have this problem.

 Nothing to do */

/*

 *  Copyright (C) 2004 Florian Schirmer <jolt@tuxbox.org>

 *  Copyright (C) 2007 Aurelien Jarno <aurelien@aurel32.net>

 *  Copyright (C) 2010-2012 Hauke Mehrtens <hauke@hauke-m.de>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

	/* Figure out memory size by finding aliases.

	 *

	 * We should theoretically use the mapping from CFE using cfe_enummem().

	 * However as the BCM47XX is mostly used on low-memory systems, we

	 * want to reuse the memory used by CFE (around 4MB). That means cfe_*

	 * functions stop to work at some point during the boot, we should only

	 * call them at the beginning of the boot.

	 *

	 * BCM47XX uses 128MB for addressing the ram, if the system contains

	 * less that that amount of ram it remaps the ram more often into the

	 * available space.

 Physical address, without mapping to any kernel segment */

 Accessing memory after 128 MiB will cause an exception */

 Loop condition may be not enough, off may be over 1 MiB */

	/* Ignoring the last page when ddr size is 128M. Cached

	 * accesses to last page is causing the processor to prefetch

	 * using address above 128M stepping out of the ddr address

	 * space.

/*

 * This is the first serial on the chip common core, it is at this position

 * for sb (ssb) and ai (bcma) bus.

/* Stripped version of tlb_init, with the call to build_tlb_refill_handler

 * dropped. Calling it at this stage causes a hang.

	/* Add one temporary TLB entry to map SDRAM Region 2.

	 *      Physical        Virtual

	 *      0x80000000      0xc0000000      (1st: 256MB)

	 *      0x90000000      0xd0000000      (2nd: 256MB)

 TODO: Register extra memory */

 defined(CONFIG_BCM47XX_BCMA) && defined(CONFIG_HIGHMEM) */

/*

 *  Copyright (C) 2004 Florian Schirmer <jolt@tuxbox.org>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

	/*

	 * Use deterministic values for initial counter interrupt

	 * so that calibrate delay avoids encountering a counter wrap.

 Set MIPS counter frequency for fixed_rate_gettimeoffset() */

/*

 * 8250 UART probe driver for the BCM47XX platforms

 * Author: Aurelien Jarno

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 Aurelien Jarno <aurelien@aurel32.net>

 SPDX-License-Identifier: GPL-2.0

/**************************************************

 * Database

 Asus */

 Router mode */

 Repeater mode */

 AP mode */

 Hard disk power switch */

 EZSetup */

 Copy data from USB to internal disk */

 Hard reset */

 Huawei */

 Belkin */

 Buffalo */

 Router / AP mode swtich */

 Router / AP mode swtich */

 Router / AP mode swtich */

 Dell */

 D-Link */

 Linksys */

 Luxul */

 Microsoft */

 Motorola */

 Netgear */

 SimpleTech */

/**************************************************

 * Init

 Copy data from __initconst */

 SPDX-License-Identifier: GPL-2.0

 model_name */

 hardware_version */

 hardware_version, boardnum */

 productid */

 ModelId */

 melco_id or buf1falo_id */

 boot_hw_model, boot_hw_ver */

 like WRT160N v3.0 */

 like WRT310N v2.0 */

 like WRT160N v3.0 */

 like WRT610N v2.0 */

 board_id */

 boardtype, boardnum, boardrev */

 boardtype, boardrev */

/*

 * Some devices don't use any common NVRAM entry for identification and they

 * have only one model specific variable.

 * They don't deserve own arrays, let's group them there using key-value array.

 buffalo hardware, check id for specific hardware matches */

 check if the nvram is available */

 init of nvram failed, probably too early now */

 SPDX-License-Identifier: GPL-2.0

 No workaround(s) needed */

 SPDX-License-Identifier: GPL-2.0

/**************************************************

 * Database

 Asus */

 TODO: Add "wlan" LED */

 Labeled "READY" (there is no "power" LED). Originally ON, flashing on USB activity. */

 Belkin */

 Buffalo */

 Dell */

 D-Link */

 Originally blinking when device is ready, separated from "power" LED */

 Originally blinking when device is ready, separated from "power" LED */

 Huawei */

 Linksys */

 Verified on: WRT54GS V1.0 */

 Verified on: WRT54GL V1.1 */

 Luxul */

 Microsoft */

 Motorola */

 There are only 3 LEDs: Power, Wireless and Device (ethernet) */

 Netgear */

 Siemens */

 SimpleTech */

 "Ready" LED */

/**************************************************

 * Init

/*

 *  Copyright (C) 2004 Florian Schirmer <jolt@tuxbox.org>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

	/*

	 * This is the first arch callback after mm_init (we can use kmalloc),

	 * so let's finish bus initialization now.

		/*

		 * the kernel reads the timer irq from some register and thinks

		 * it's #5, but we offset it by 2 and route to #7

/*

 * System-specific setup, especially interrupts.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1998 Harald Koerfgen

 * Copyright (C) 2000, 2001, 2002, 2003, 2005, 2020  Maciej W. Rozycki

/*

 * IRQ routing and priority tables.  Priorites are set as follows:

 *

 *		KN01	KN230	KN02	KN02-BA	KN02-CA	KN03

 *

 * MEMORY	CPU	CPU	CPU	ASIC	CPU	CPU

 * RTC		CPU	CPU	CPU	ASIC	CPU	CPU

 * DMA		-	-	-	ASIC	ASIC	ASIC

 * SERIAL0	CPU	CPU	CSR	ASIC	ASIC	ASIC

 * SERIAL1	-	-	-	ASIC	-	ASIC

 * SCSI		CPU	CPU	CSR	ASIC	ASIC	ASIC

 * ETHERNET	CPU	*	CSR	ASIC	ASIC	ASIC

 * other	-	-	-	ASIC	-	-

 * TC2		-	-	CSR	CPU	ASIC	ASIC

 * TC1		-	-	CSR	CPU	ASIC	ASIC

 * TC0		-	-	CSR	CPU	ASIC	ASIC

 * other	-	CPU	-	CPU	ASIC	ASIC

 * other	-	-	-	-	CPU	CPU

 *

 * * -- shared with SCSI

/*

 * Bus error (DBE/IBE exceptions and bus interrupts) handling setup.

 DS2100/DS3100 Pmin/Pmax */

 DS5000/1xx 3min */

 DS5000/xx Maxine */

 DS5000/200 3max */

 DS5000/240 3max+ */

 DS5900 bigmax */

 Stay away from the firmware working memory area for now. */

/*

 * Machine-specific initialisation for KN01, aka DS2100 (aka Pmin)

 * or DS3100 (aka Pmax).

 IRQ routing. */

 CPU IRQ priorities. */

 dec_init_kn01 */

/*

 * Machine-specific initialisation for KN230, aka DS5100, aka MIPSmate.

 IRQ routing. */

 CPU IRQ priorities. */

 dec_init_kn230 */

/*

 * Machine-specific initialisation for KN02, aka DS5000/200, aka 3max.

 IRQ routing. */

 CPU IRQ priorities. */

 KN02 CSR IRQ priorities. */

 dec_init_kn02 */

/*

 * Machine-specific initialisation for KN02-BA, aka DS5000/1xx

 * (xx = 20, 25, 33), aka 3min.  Also applies to KN04(-BA), aka

 * DS5000/150, aka 4min.

 IRQ routing. */

 CPU IRQ priorities. */

 I/O ASIC IRQ priorities. */

 dec_init_kn02ba */

/*

 * Machine-specific initialisation for KN02-CA, aka DS5000/xx,

 * (xx = 20, 25, 33), aka MAXine.  Also applies to KN04(-CA), aka

 * DS5000/50, aka 4MAXine.

 IRQ routing. */

 CPU IRQ priorities. */

 I/O ASIC IRQ priorities. */

 dec_init_kn02ca */

/*

 * Machine-specific initialisation for KN03, aka DS5000/240,

 * aka 3max+ and DS5900, aka BIGmax.  Also applies to KN05, aka

 * DS5000/260, aka 4max+ and DS5900/260.

 IRQ routing. */

 CPU IRQ priorities. */

 I/O ASIC IRQ priorities. */

 dec_init_kn03 */

 DS2100/DS3100 Pmin/Pmax */

 DS5100 MIPSmate */

 DS5000/200 3max */

 DS5000/1xx 3min */

 DS5000/240 3max+ */

 DS5900 bigmax */

 Personal DS5000/xx */

 DS5800 Isis */

 DS5400 MIPSfair */

 DS5500 MIPSfair-2 */

 Free the FPU interrupt if the exception is present. */

 Free the halt interrupt unused on R4k systems.  */

 Register board interrupts: FPU and cascade. */

 Register the bus error interrupt. */

 Register the HALT interrupt. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1991, 1992, 1995  Linus Torvalds

 *  Copyright (C) 2000, 2003  Maciej W. Rozycki

 *

 * This file contains the time handling details for PC-style clocks as

 * found in some MIPS systems.

 *

		/*

		 * The PROM will reset the year to either '72 or '73.

		 * Therefore we store the real year separately, in one

		 * of unused BBU RAM locations.

/*

 * In order to set the CMOS clock precisely, update_persistent_clock64 has to

 * be called 500 ms after the second nowtime has started, because when

 * nowtime is written into the registers of the CMOS clock, it will

 * jump to the next second precisely 500 ms later.  Check the Dallas

 * DS1287 data sheet for details.

 irq are locally disabled here */

 tell the clock it's being set */

 stop and reset prescaler */

	/*

	 * since we're only adjusting minutes and seconds,

	 * don't interfere with hour overflow. This avoids

	 * messing with unknown time zones but requires your

	 * RTC not to be off by more than 15 minutes

 correct for half hour time zone */

	/* The following flags have to be released exactly in this order,

	 * otherwise the DS1287 will not reset the oscillator and will not

	 * update precisely 500 ms later.  You won't find this mentioned

	 * in the Dallas Semiconductor data sheets, but who believes data

	 * sheets anyway ...                           -- Markus Kuhn

 Set up the rate of periodic DS1287 interrupts. */

 On some I/O ASIC systems we have the I/O ASIC's counter.  */

		/*

		 * All R4k DECstations suffer from the CP0 Count erratum,

		 * so we can't use the timer as a clock source, and a clock

		 * event both at a time.  An accurate wall clock is more

		 * important than a high-precision interval timer so only

		 * use the timer as a clock source, and not a clock event

		 * if there's no I/O ASIC counter available to serve as a

		 * clock source.

/*

 * Setup the right wbflush routine for the different DECstations.

 *

 * Created with information from:

 *	DECstation 3100 Desktop Workstation Functional Specification

 *	DECstation 5000/200 KN02 System Module Functional Specification

 *	mipsel-linux-objdump --disassemble vmunix | grep "wbflush" :-)

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1998 Harald Koerfgen

 * Copyright (C) 2002 Maciej W. Rozycki

 DS5000 3max */

 DS5100 MIPSMATE */

 DS5000/100 3min */

 Personal DS5000/2x */

 DS5000/240 3max+ */

 DS5900 bigmax */

/*

 * For the DS3100 and DS5000/200 the R2020/R3220 writeback buffer functions

 * as part of Coprocessor 0.

/*

 * For the DS5100 the writeback buffer seems to be a part of Coprocessor 3.

 * But CP3 has to enabled first.

/*

 * I/O ASIC systems use a standard writeback buffer that gets flushed

 * upon an uncached read.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bus error event handling code for systems equipped with ECC

 *	handling logic, i.e. DECstation/DECsystem 5000/200 (KN02),

 *	5000/240 (KN03), 5000/260 (KN05) and DECsystem 5900 (KN03),

 *	5900/260 (KN05) systems.

 *

 *	Copyright (c) 2003, 2005  Maciej W. Rozycki

 any write clears the IRQ */

 For non-ECC ack ASAP, so that any subsequent errors get caught. */

 No idea what happened. */

 An ECC error on a CPU or DMA transaction. */

 A CPU timeout or a DMA overrun. */

 For ECC errors on reads adjust for MT pipelining. */

 Only CPU errors are fixable. */

 Low bank. */

 High bank. */

 Ack now, no rewrite will happen. */

			/*

			 * Multibit errors may be tagged incorrectly;

			 * check the syndrome explicitly.

 Rewrite. */

 Ack now, now we've rewritten (or not). */

	/*

	 * FIXME: Find the affected processes and kill them, otherwise

	 * we must die.

	 *

	 * The interrupt is asynchronously delivered thus EPC and RA

	 * may be irrelevant, but are printed for a reference.

/*

 * Initialization differs a bit between KN02 and KN03/KN05, so we

 * need two variants.  Once set up, all systems can be handled the

 * same way.

 Preset write-only bits of the Control Register cache. */

 Set normal ECC detection and generation. */

 Enable ECC correction. */

	/*

	 * Set normal ECC detection and generation, enable ECC correction.

	 * For KN05 we also need to make sure EE (?) is enabled in the MB.

	 * Otherwise DBE/IBE exceptions would be masked but bus error

	 * interrupts would still arrive, resulting in an inevitable crash

	 * if get_dbe() triggers one.

 Clear any leftover errors from the firmware. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	DECstation 5000/200 (KN02) Control and Status Register

 *	interrupts.

 *

 *	Copyright (c) 2002, 2003, 2005  Maciej W. Rozycki

/*

 * Bits 7:0 of the Control Register are write-only -- the

 * corresponding bits of the Status Register have a different

 * meaning.  Hence we use a cache.  It speeds up things a bit

 * as well.

 *

 * There is no default value -- it has to be initialized.

 Mask interrupts. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bus error event handling code for 5000-series systems equipped

 *	with parity error detection logic, i.e. DECstation/DECsystem

 *	5000/120, /125, /133 (KN02-BA), 5000/150 (KN04-BA) and Personal

 *	DECstation/DECsystem 5000/20, /25, /33 (KN02-CA), 5000/50

 *	(KN04-CA) systems.

 *

 *	Copyright (c) 2005  Maciej W. Rozycki

 Clear errors; keep the ARC IRQ. */

 Any write clears the bus IRQ. */

 Ack ASAP, so that any subsequent errors get caught. */

 No DMA errors? */

 Low 256MB is decoded as memory, high -- as TC. */

	/*

	 * FIXME: Find the affected processes and kill them, otherwise

	 * we must die.

	 *

	 * The interrupt is asynchronously delivered thus EPC and RA

	 * may be irrelevant, but are printed for a reference.

 For KN04 we need to make sure EE (?) is enabled in the MB.  */

 Clear any leftover errors from the firmware. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	DEC platform devices.

 *

 *	Copyright (c) 2014  Maciej W. Rozycki

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	DEC I/O ASIC interrupts.

 *

 *	Copyright (c) 2002, 2003, 2013  Maciej W. Rozycki

/*

 * I/O ASIC implements two kinds of DMA interrupts, informational and

 * error interrupts.

 *

 * The formers do not stop DMA and should be cleared as soon as possible

 * so that if they retrigger before the handler has completed, usually as

 * a side effect of actions taken by the handler, then they are reissued.

 * These use the `handle_edge_irq' handler that clears the request right

 * away.

 *

 * The latters stop DMA and do not resume it until the interrupt has been

 * cleared.  This cannot be done until after a corrective action has been

 * taken and this also means they will not retrigger.  Therefore they use

 * the `handle_fasteoi_irq' handler that only clears the request on the

 * way out.  Because MIPS processor interrupt inputs, one of which the I/O

 * ASIC is cascaded to, are level-triggered it is recommended that error

 * DMA interrupt action handlers are registered with the IRQF_ONESHOT flag

 * set so that they are run with the interrupt line masked.

 *

 * This mask has `1' bits in the positions of informational interrupts.

 Mask interrupts. */

/*

 *	TURBOchannel architecture calls.

 *

 *	Copyright (c) Harald Koerfgen, 1998

 *	Copyright (c) 2001, 2003, 2005, 2006  Maciej W. Rozycki

 *	Copyright (c) 2005  James Simmons

 *

 *	This file is subject to the terms and conditions of the GNU

 *	General Public License.  See the file "COPYING" in the main

 *	directory of this archive for more details.

/*

 * Protected read byte from TURBOchannel slot space.

/*

 * Get TURBOchannel bus information as specified by the spec, plus

 * the slot space base address and the number of slots.

/*

 * Get the IRQ for the specified slot.

	/*

	 * Yuck! DS5000/200 onboard devices

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Bus error event handling code for DECstation/DECsystem 3100

 *	and 2100 (KN01) systems equipped with parity error detection

 *	logic.

 *

 *	Copyright (c) 2005  Maciej W. Rozycki

 CP0 hazard avoidance. */

/*

 * Bits 7:0 of the Control Register are write-only -- the

 * corresponding bits of the Status Register have a different

 * meaning.  Hence we use a cache.  It speeds up things a bit

 * as well.

 *

 * There is no default value -- it has to be initialized.

 Clear bus IRQ. */

 Ack ASAP, so that any subsequent errors get caught. */

 Bloody hardware doesn't record the address for reads... */

 This never faults. */

 Peek at what physical address the CPU used. */

 No need to check for presence. */

 Treat low 256MB as memory, high -- as I/O. */

 Must have been video. */

	/*

	 * FIXME: Find the affected processes and kill them, otherwise

	 * we must die.

	 *

	 * The interrupt is asynchronously delivered thus EPC and RA

	 * may be irrelevant, but are printed for a reference.

 Preset write-only bits of the Control Register cache. */

 Enable parity error detection. */

 Clear any leftover errors from the firmware. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Reset a DECstation machine.

 *

 * Copyright (C) 199x  the Anonymous

 * Copyright (C) 2001, 2002, 2003  Maciej W. Rozycki

 DECstations don't have a software power switch */

 SPDX-License-Identifier: GPL-2.0

/*

 * identify.c: machine identification code.

 *

 * Copyright (C) 1998 Harald Koerfgen and Paul M. Antoine

 * Copyright (C) 2002, 2003, 2004, 2005  Maciej W. Rozycki

/*

 * Setup essential system-specific memory addresses.  We need them

 * early.  Semantically the functions belong to prom/init.c, but they

 * are compact enough we want them inlined. --macro

	/*

	 * FIXME: This may not be an exhaustive list of DECStations/Servers!

	 * Put all model-specific initialisation calls here.

 DS5100 MIPSMATE */

 DS5000 3max */

 DS5000/100 3min */

 DS5000/240 3max+ or DS5900 bigmax */

 Personal DS5000/xx maxine */

 DS5800 Isis */

 DS5400 MIPSfair */

 DS5500 MIPSfair-2 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	DECstation PROM-based early console support.

 *

 *	Copyright (C) 2004, 2007  Maciej W. Rozycki

 SPDX-License-Identifier: GPL-2.0

/*

 * cmdline.c: read the command line passed to us by the PROM.

 *

 * Copyright (C) 1998 Harald Koerfgen

 * Copyright (C) 2002, 2004  Maciej W. Rozycki

	/*

	 * collect args and prepare cmd_line

 SPDX-License-Identifier: GPL-2.0

/*

 * init.c: PROM library initialisation code.

 *

 * Copyright (C) 1998 Harald Koerfgen

 * Copyright (C) 2002, 2004  Maciej W. Rozycki

/*

 * Detect which PROM the DECSTATION has, and set the callback vectors

 * appropriately.

	/*

	 * No sign of the REX PROM's magic number means we assume a non-REX

	 * machine (i.e. we're on a DS2100/3100, DS5100 or DS5000/2xx)

		/*

		 * Set up prom abstraction structure with REX entry points.

		/*

		 * Set up prom abstraction structure with non-REX entry points.

	/*

	 * Determine which PROM we have

	 * (and therefore which machine we're on!)

 Register the early console.  */

 Were we compiled with the right CPU option? */

 SPDX-License-Identifier: GPL-2.0

/*

 * memory.c: memory initialisation code.

 *

 * Copyright (C) 1998 Harald Koerfgen, Frieder Streffer and Paul M. Antoine

 * Copyright (C) 2000, 2002  Maciej W. Rozycki

 So we know an error occurred */

/*

 * Probe memory in 4MB chunks, waiting for an error to tell us we've fallen

 * off the end of real memory.  Only suitable for the 2100/3100's (PMAX).

 Install exception handler */

	/* read unmapped and uncached (KSEG1)

	 * DECstations have at least 4MB RAM

	 * Assume less than 480MB of RAM, as this is max for 5000/2xx

	 * FIXME this should be replaced by the first free page!

/*

 * Use the REX prom calls to get hold of the memory bitmap, and thence

 * determine memory size.

 some free 64k */

 FIXME: very simplistically only add full sets of pages */

	/*

	 * Free everything below the kernel itself but leave

	 * the first page reserved for the exception handlers.

	/*

	 * Leave 128 KB reserved for Lance memory for

	 * IOASIC DECstations.

	 *

	 * XXX: save this address for use in dec_lance.c?

 SPDX-License-Identifier: GPL-2.0

/*

 * sc-rm7k.c: RM7000 cache management functions.

 *

 * Copyright (C) 1997, 2001, 2003, 2004 Ralf Baechle (ralf@linux-mips.org)

 for run_uncached() */

 Primary cache parameters. */

 Secondary cache parameters. */

 Fixed to 256KiB on RM7000 */

 Tertiary cache parameters */

/*

 * Writeback and invalidate the primary cache dcache before DMA.

 * (XXX These need to be fixed ...)

 Catch bad driver code */

 Page_Invalidate_T */

 Catch bad driver code */

 Page_Invalidate_T */

/*

 * This function is executed in uncached address space.

/*

 * This function is executed in uncached address space.

/*

 * This is a probing function like the one found in c-r4k.c, we look for the

 * wrap around point with different addresses.

 Fill size-multiple lines with a valid tag */

 Load first line with a 0 tag, to check after */

 Look for the wrap-around */

	/*

	 * While we're at it let's deal with the tertiary cache.

	/*

	 * No efficient way to ask the hardware for the size of the tcache,

	 * so must probe for it.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * This function is specific to ASIDs, and should not be called when

	 * MMIDs are in use.

 start new asid cycle */

	/*

	 * This function is specific to ASIDs, and should not be called when

	 * MMIDs are in use.

 Check if our ASID is of an older version and thus invalid */

 Update the list of reserved MMIDs and the MMID bitmap */

 Reserve an MMID for kmap/wired entries */

		/*

		 * If this CPU has already been through a

		 * rollover, but hasn't run another task in

		 * the meantime, we must preserve its reserved

		 * MMID, as this is the only trace we have of

		 * the process it is still running.

	/*

	 * Queue a TLB invalidation for each CPU to perform on next

	 * context-switch

	/*

	 * Iterate over the set of reserved MMIDs looking for a match.

	 * If we find one, then we can update our mm to use newmmid

	 * (i.e. the same MMID in the current generation) but we can't

	 * exit the loop early, since we need to ensure that all copies

	 * of the old MMID are updated to reflect the mm. Failure to do

	 * so could result in us missing the reserved MMID in a future

	 * generation.

		/*

		 * If our current MMID was active during a rollover, we

		 * can continue to use it and this was just a false alarm.

		/*

		 * We had a valid MMID in a previous life, so try to re-use

		 * it if possible.

 Allocate a free MMID */

 We're out of MMIDs, so increment the global version */

 Note currently active MMIDs & mark TLBs as requiring flushes */

 We have more MMIDs than CPUs, so this will always succeed */

	/*

	 * MMID switch fast-path, to avoid acquiring cpu_mmid_lock when it's

	 * unnecessary.

	 *

	 * The memory ordering here is subtle. If our active_mmids is non-zero

	 * and the MMID matches the current version, then we update the CPU's

	 * asid_cache with a relaxed cmpxchg. Racing with a concurrent rollover

	 * means that either:

	 *

	 * - We get a zero back from the cmpxchg and end up waiting on

	 *   cpu_mmid_lock in check_mmu_context(). Taking the lock synchronises

	 *   with the rollover and so we are forced to see the updated

	 *   generation.

	 *

	 * - We get a valid MMID back from the cmpxchg, which means the

	 *   relaxed xchg in flush_context will treat us as reserved

	 *   because atomic RmWs are totally ordered for a given location.

	/*

	 * Invalidate the local TLB if needed. Note that we must only clear our

	 * bit in tlb_flush_pending after this is complete, so that the

	 * cpu_has_shared_ftlb_entries case below isn't misled.

	/*

	 * If this CPU shares FTLB entries with its siblings and one or more of

	 * those siblings hasn't yet invalidated its TLB following a version

	 * increase then we need to invalidate any TLB entries for our MMID

	 * that we might otherwise pick up from a sibling.

	 *

	 * We ifdef on CONFIG_SMP because cpu_sibling_map isn't defined in

	 * CONFIG_SMP=n kernels.

 Ensure we operate on the new MMID */

		/*

		 * Invalidate all TLB entries associated with the new

		 * MMID, and wait for the invalidation to complete.

	/*

	 * Expect allocation after rollover to fail if we don't have at least

	 * one more MMID than CPUs.

 Reserve an MMID for kmap/wired entries */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * A small micro-assembler. It is intentionally kept simple, does only

 * support a subset of instructions, and does not try to hide pipeline

 * effects like branch delay slots.

 *

 * Copyright (C) 2004, 2005, 2006, 2008	 Thiemo Seufer

 * Copyright (C) 2005, 2007  Maciej W. Rozycki

 * Copyright (C) 2006  Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2012, 2013  MIPS Technologies, Inc.  All rights reserved.

 This macro sets the non-variable bits of an instruction. */

 This macro sets the non-variable bits of an R6 instruction. */

/*

 * The order of opcode arguments is implicitly left to right,

 * starting with RS and ending with FUNC or IMM.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 1997, 2001 Ralf Baechle (ralf@gnu.org),

 * derived from r4xx0.c by David S. Miller (davem@davemloft.net).

 Secondary cache size in bytes, if present.  */

 Catch bad driver code */

	/* On the R5000 secondary cache we cannot

	 * invalidate less than a page at a time.

	 * The secondary cache is physically indexed, write-through.

 SPDX-License-Identifier: GPL-2.0

/*

 * r2300.c: R2000 and R3000 specific mmu/cache code.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 *

 * with a lot of changes to make this thing work for R3000s

 * Tx39XX R4k style caches added. HK

 * Copyright (C) 1998, 1999, 2000 Harald Koerfgen

 * Copyright (C) 1998 Gleb Raiko & Vladimir Roganov

 For R3000 cores with R4000 style caches */

 Size in bytes */

 This sequence is required to ensure icache is disabled immediately */

 TX39H-style cache flush routines. */

 disable icache (set ICE#) */

 Catch bad driver code */

 TX39H2,TX39H3 */

 disable icache (set ICE#) */

 disable icache (set ICE#) */

 disable icache (set ICE#) */

	/*

	 * If ownes no valid ASID yet, cannot possibly have gotten

	 * this page into the cache.

	/*

	 * If the page isn't marked valid, the page cannot possibly be

	 * in the cache.

	/*

	 * Doing flushes for another ASID than the current one is

	 * too difficult since stupid R4k caches do a TLB translation

	 * for every cache flush operation.  So we do indexed flushes

	 * in that case, which doesn't overly flush the cache too much.

	/*

	 * Do indexed flush, too much work to get the (possible) TLB refills

	 * to work correctly.

 disable icache (set ICE#) */

 TX39/H core (writethru direct-map cache) */

 TX39/H2,H3 core (writeback 2way-set-associative cache) */

 board-dependent init code may set WBON */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001,2002,2003 Broadcom Corporation

/*

 * We'd like to dump the L2_ECC_TAG register on errors, but errata make

 * that unsafe... So for now we don't.	(BCM1250/BCM112x erratum SOC-48.)

 SB1 definitions */

 XXX should come from config1 XXX */

 Destructive read, clears register and interrupt */

 Bit 31 is always on, but there's no #define for that */

 Freeze the trace buffer now */

 Check index of EPC, allowing for delay slot */

	/*

	 * Calling panic() when a fatal cache error occurs scrambles the

	 * state of the system (and the cache), making it difficult to

	 * investigate after the fact.	However, if you just stall the CPU,

	 * the other CPU may keep on running, which is typically very

	 * undesirable.

 Parity lookup table. */

 Masks to select bits for Hamming parity, mask_72_64[i] for bit[i] */

 Calculate the parity on a range of bits */

 Calculate the 4-bit even byte-parity for an instruction */

 Index-load-tag-I */

 bank */

 index */

 (hit all banks and ways) */

 Index-load-data-I */

 XXXKW should/could check predecode bits themselves */

 Compute the ECC for a data doubleword */

 Index-load-tag-D */

 bank */

 index */

 Index-load-data-D */

 Index-load-data-D */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2006 Chris Dearman (chris@mips.com),

/*

 * MIPS32/MIPS64 L2 cache handling

/*

 * Writeback and invalidate the secondary cache before DMA.

/*

 * Invalidate the secondary cache before DMA.

 L2 cache is permanently enabled */

 L2 cache is permanently enabled */

	/*

	 * If there is one or more L2 prefetch unit present then enable

	 * prefetching for both code & data, for all ports.

/*

 * Check if the L2 cache controller is activated on a particular platform.

 * MTI's L2 controller and the L2 cache controller of Broadcom's BMIPS

 * cores both use c0_config2's bit 12 as "L2 Bypass" bit, that is the

 * cache being disabled.  However there is no guarantee for this to be

 * true on all platforms.  In an act of stupidity the spec defined bits

 * 12..15 as implementation defined so below function will eventually have

 * to be replaced by a platform specific probe.

 Check the bypass bit (L2B) */

 Mark as not present until probe completed */

 Ignore anything but MIPSxx processors */

 Does this MIPS32/MIPS64 CPU have a config2 register? */

		/*

		 * According to config2 it would be 5-ways, but that is

		 * contradicted by all documentation.

		/*

		 * According to config2 it would be 5-ways and 512-sets,

		 * but that is contradicted by all documentation.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2011 Wind River Systems,

 *   written by Ralf Baechle <ralf@linux-mips.org>

 Sane caches */

 Even MAP_FIXED mappings must reside within TASK_SIZE */

		/*

		 * We do not accept a shared mapping if it would violate

		 * cache aliasing constraints.

 requesting a specific address */

		/*

		 * A failed mmap() very likely causes application failure,

		 * so fall back to the bottom-up function here. This scenario

		 * can happen with large stack limits and large mmap()

		 * allocations.

/*

 * There is no need to export this but sched.h declares the function as

 * extern so making it static here results in an error.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003, 04, 05 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2007  Maciej W. Rozycki

 * Copyright (C) 2008  Thiemo Seufer

 * Copyright (C) 2012  MIPS Technologies, Inc.

 Registers used in the assembled routines. */

 Handle labels (which must be positive integers). */

 We need one branch and therefore one relocation per target label. */

/*

 * R6 has a limited offset of the pref instruction.

 * Skip it if the offset is more than 9 bits.

	/*

	 * The pref's used here are using "streaming" hints, which cause the

	 * copied data to be kicked out of the cache sooner.  A page copy often

	 * ends up copying a lot more data than is commonly used, so this seems

	 * to make sense in terms of reducing cache pollution, but I've no real

	 * performance data to back this up.

		/*

		 * XXX: Most prefetch bias values in here are based on

		 * guesswork.

 These processors only support the Pref_Load. */

			/*

			 * Those values have been experimentally tuned for an

			 * Origin 200.

			/*

			 * SB1 pass1 Pref_LoadStreamed/Pref_StoreStreamed

			 * hints are broken.

 Loongson-3 only support the Pref_Load/Pref_Store. */

				/*

				 * Bit 30 (Pref_PrepareForStore) has been

				 * removed from MIPS R6. Use bit 5

				 * (Pref_StoreStreamed).

	/*

	 * Too much unrolling will overflow the available space in

	 * clear_space_array / copy_page_array.

	/*

	 * This algorithm makes the following assumptions:

	 *   - The prefetch bias is a multiple of 2 words.

	 *   - The prefetch bias is less than one page.

	/*

	 * This algorithm makes the following assumptions:

	 *   - All prefetch biases are multiples of 8 words.

	 *   - The prefetch biases are less than one page.

	 *   - The store prefetch bias isn't greater than the load

	 *     prefetch bias.

/*

 * Pad descriptors to cacheline, since each is exclusively owned by a

 * particular CPU.

 if the page is not in KSEG0, use old way */

	/*

	 * Don't really want to do it this way, but there's no

	 * reliable way to delay completion detection.

 if any page is not in KSEG0, use old way */

	/*

	 * Don't really want to do it this way, but there's no

	 * reliable way to delay completion detection.

 CONFIG_SIBYTE_DMA_PAGEOPS */

 SPDX-License-Identifier: GPL-2.0

	/* high_memory does not get immediately defined, and there

	 * are early callers of __pa() against PAGE_OFFSET

	/*

	 * MAX_DMA_ADDRESS is a virtual address that may not correspond to an

	 * actual physical address. Enough code relies on

	 * virt_to_phys(MAX_DMA_ADDRESS) that we just need to work around it

	 * and always return true.

	/* This is bounds checking against the kernel image only.

	 * __pa_symbol should only be used on kernel symbol addresses.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005-2007 Cavium Networks

/*

 * Octeon automatically flushes the dcache on tlb changes, so

 * from Linux's viewpoint it acts much like a physically

 * tagged cache. No flushing is needed

 *

 Nothing to do */

/*

 * Flush local I-cache for the specified range.

/**

 * octeon_flush_icache_all_cores -  Flush caches as necessary for all cores

 * affected by a vma. If no vma is supplied, all cores are flushed.

 *

 * @vma:    VMA to flush or NULL to flush all icaches.

	/*

	 * If we have a vma structure, we only need to worry about

	 * cores it has been used on

/*

 * Called to flush the icache on all cores

/**

 * octeon_flush_cache_mm - flush all memory associated with a memory context.

 *

 * @mm:	    Memory context to flush

	/*

	 * According to the R4K version of this file, CPUs without

	 * dcache aliases don't need to do anything here

/*

 * Flush a range of kernel addresses out of the icache

 *

/**

 * octeon_flush_cache_range - Flush a range out of a vma

 *

 * @vma:    VMA to flush

 * @start:  beginning address for flush

 * @end:    ending address for flush

/**

 * octeon_flush_cache_page - Flush a specific page of a vma

 *

 * @vma:    VMA to flush page for

 * @page:   Page to flush

 * @pfn:    Page frame number

/*

 * Probe Octeon's caches

 *

 CN5XXX has two Dcache sets */

 CN3XXX has one Dcache set */

 compute a couple of other cache variables */

/*

 * Setup the Octeon cache flush routines

 *

/*

 * Handle a cache error exception

/*

 * Called when the the exception is recoverable

/*

 * Called when the the exception is not recoverable

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994 - 2000 Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.

/*

 * We have up to 8 empty zeroed pages so we can map one of the right colour

 * when needed.	 This is necessary only on R4000 / R4400 SC and MC versions

 * where we have to avoid VCED / VECI exceptions for good performance at

 * any price.  Since page is never written to after the initialization we

 * don't have to care about aliases on other CPUs.

/*

 * Not static inline because used by IP27 special magic initialization code

 Make sure this page is cleared on other CPU's too before using it */

 MAAR registers hold physical addresses right shifted by 4 bits */

 Fill in the MAAR config entry */

 Ensure we don't overflow the cfg array */

 Detect the number of MAARs */

 MAARs should be in pairs */

 Set MAARs using values we recorded already */

 Configure the required MAARs */

 Disable any further MAARs */

 Record the setup for use on secondary CPUs */

	/*

	 * When _PFN_SHIFT is greater than PAGE_SHIFT we won't have enough PTE

	 * bits to hold a full 32b physical address on MIPS32 systems.

 Setup zeroed pages.  */

		/* The -4 is a hack so that user tools don't have to handle

 !CONFIG_NUMA */

 nothing to do */

	/*

	 * Let the platform define a specific function to free the

	 * init section since EVA may have used any possible mapping

	 * between virtual and physical addresses.

	/*

	 * Always reserve area for module percpu variables.  That's

	 * what the legacy allocator did.

/*

 * Align swapper_pg_dir in to 64K, allows its address to be loaded

 * with a single LUI instruction in the TLB handlers.  If we used

 * __aligned(64K), its size would get rounded up to the alignment

 * size, and waste space.  So we place it in its own section and align

 * it in the linker script.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * A small micro-assembler. It is intentionally kept simple, does only

 * support a subset of instructions, and does not try to hide pipeline

 * effects like branch delay slots.

 *

 * Copyright (C) 2004, 2005, 2006, 2008	 Thiemo Seufer

 * Copyright (C) 2005, 2007  Maciej W. Rozycki

 * Copyright (C) 2006  Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2012, 2013  MIPS Technologies, Inc.  All rights reserved.

 insn_invalid must be last */

		/*

		 * As per erratum Core-14449, replace prefetches 0-4,

		 * 6-24 with 'pref 28'.

 Handle labels. */

 Is this address in 32bit compat space? */

 Handle relocations. */

 Convenience functions for labeled branches. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 by Ralf Baechle

 defined(CONFIG_TRANSPARENT_HUGEPAGE) */

 Initialize the entire pgd.  */

	/*

	 * Fixed mappings:

	/*

	 * Permanent kmaps:

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994 - 2003, 06, 07 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2007 MIPS Technologies, Inc.

 Cache operations. */

 MIPS specific cache operations */

 DMA cache operations. */

 CONFIG_DMA_NONCOHERENT */

/*

 * We could optimize the case where the cache argument is not BCACHE but

 * that seems very atypical use ...

	/*

	 * We could delay the flush for the !page_mapping case too.  But that

	 * case is for exec env/arg pages and those are %99 certainly going to

	 * get faulted into the tlb (and thus flushed) anyways.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995 - 2000 by Ralf Baechle

 For VMALLOC_END */

/*

 * This routine handles page faults.  It determines the address,

 * and the problem, and then passes it off to one of the appropriate

 * routines.

	/*

	 * This is to notify the fault handler of the kprobes.

	/*

	 * We fault-in kernel-space virtual memory on-demand. The

	 * 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

	/*

	 * If we're in an interrupt or have no user

	 * context, we must not take the fault..

/*

 * Ok, we have a good vm_area for this memory access, so

 * we can handle it..

	/*

	 * If for any reason at all we couldn't handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

			/*

			 * No need to mmap_read_unlock(mm) as we would

			 * have already released it in __lock_page_or_retry

			 * in mm/filemap.c.

/*

 * Something tried to access memory that isn't in our memory map..

 * Fix it, but check if it's kernel or user first..

 User mode accesses just cause a SIGSEGV */

 Are we prepared to handle this kernel fault?	 */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

	/*

	 * We ran out of memory, call the OOM killer, and return the userspace

	 * (which will retry the fault, or kill us if we got oom-killed).

 Kernel mode? Handle exceptions or die */

	/*

	 * Send a sigbus, regardless of whether we were in kernel

	 * or user mode.

		/*

		 * Synchronize this task's top level page-table

		 * with the 'reference' page table.

		 *

		 * Do _not_ use "tsk" here. We might be inside

		 * an interrupt in the middle of a task switch..

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1997, 1998, 1999, 2000 Ralf Baechle ralf@gnu.org

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2002 MIPS Technologies, Inc.  All rights reserved.

/*

 * LOONGSON-2 has a 4 entry itlb which is a subset of jtlb, LOONGSON-3 has

 * a 4 entry itlb and a 4 entry dtlb which are subsets of jtlb. Unfortunately,

 * itlb/dtlb are not totally transparent to software.

 Save old context and create impossible VPN2 value */

	/*

	 * Blast 'em all away.

	 * If there are any wired entries, fall back to iterating

 invalidate VTLB */

 invalidate one FTLB set */

 Make sure all entries differ. */

 Make sure all entries differ. */

 Make sure all entries differ. */

 Make sure all entries differ. */

/*

 * This one is only used for pages with the global bit set so we don't care

 * much about the ASID.

 Make sure all entries differ. */

/*

 * We will need multiple versions of update_mmu_cache(), one that just

 * updates the TLB with the new pte(s), and another which also checks

 * for the R4k "end of page" hardware bug and does the needy.

	/*

	 * Handle debugger faulting in for debugee.

 this could be a huge page  */

 Save old context and create impossible VPN2 value */

 What is the hazard here? */

 What is the hazard here? */

 first call comes during __init */

 CONFIG_TRANSPARENT_HUGEPAGE  */

/*

 * Used for loading TLB entries before trap_init() has started, when we

 * don't actually want to add a wired entry which remains throughout the

 * lifetime of the system

 Save old context and create impossible VPN2 value */

/*

 * Configure TLB (for init or after a CPU has been powered off).

	/*

	 * You should never change this register:

	 *   - On R4600 1.7 the tlbp never hits for pages smaller than

	 *     the value in the c0_pagemask register.

	 *   - The entire mm handling assumes the c0_pagemask register to

	 *     be set to fixed-size pages.

		/*

		 * Enable the no read, no exec bits, and enable large physical

		 * address.

 From this point on the ARC firmware is dead.	 */

 Did I tell you that ARC SUCKS?  */

 SPDX-License-Identifier: GPL-2.0

/*

 * r2300.c: R2000 and R3000 specific mmu/cache code.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 *

 * with a lot of changes to make this thing work for R3000s

 * Tx39XX R4k style caches added. HK

 * Copyright (C) 1998, 1999, 2000 Harald Koerfgen

 * Copyright (C) 1998 Gleb Raiko & Vladimir Roganov

 * Copyright (C) 2002  Ralf Baechle

 * Copyright (C) 2002  Maciej W. Rozycki

 CP0 hazard avoidance. */

 Should be in cpu_data? */

 TLB operations. */

 BARRIER */

 BARRIER */

 BARRIER */

 BARRIER */

 BARRIER */

 BARRIER */

	/*

	 * Handle debugger faulting in for debugee.

 BARRIER */

 TX39XX */

 Save old context and create impossible VPN2 value */

 BARRIER */

 Set to 8 on reset... */

/*

 * MIPS Huge TLB Page Support for Kernel.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2002, Rohit Seth <rohit.seth@intel.com>

 * Copyright 2005, Embedded Alley Solutions, Inc.

 * Matt Porter <mporter@embeddedalley.com>

 * Copyright (C) 2008, 2009 Cavium Networks, Inc.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 for dma_default_coherent */

 for run_uncached() */

/*

 * Bits describing what cache ops an SMP callback function may perform.

 *

 * R4K_HIT   -	Virtual user or kernel address based cache operations. The

 *		active_mm must be checked before using user addresses, falling

 *		back to kmap.

 * R4K_INDEX -	Index based cache operations.

/**

 * r4k_op_needs_ipi() - Decide if a cache op needs to be done on every core.

 * @type:	Type of cache operations (R4K_HIT or R4K_INDEX).

 *

 * Decides whether a cache op needs to be performed on every core in the system.

 * This may change depending on the @type of cache operation, as well as the set

 * of online CPUs, so preemption should be disabled by the caller to prevent CPU

 * hotplug from changing the result.

 *

 * Returns:	1 if the cache operation @type should be done on every core in

 *		the system.

 *		0 if the cache operation @type is globalized and only needs to

 *		be performed on a simple CPU.

 The MIPS Coherence Manager (CM) globalizes address-based cache ops */

	/*

	 * Hardware doesn't globalize the required cache ops, so SMP calls may

	 * be needed, but only if there are foreign CPUs (non-siblings with

	 * separate caches).

 cpu_foreign_map[] undeclared when !CONFIG_SMP */

/*

 * Special Variant of smp_call_function for use by cache functions:

 *

 *  o No return value

 *  o collapses to normal function call on UP kernels

 *  o collapses to normal function call on systems with a single shared

 *    primary cache.

 *  o doesn't disable interrupts on the local CPU

/*

 * Must die.

/*

 * Dummy cache handling routines for machines without boardcaches

 force code alignment (used for CONFIG_WAR_TX49XX_ICACHE_INDEX_INV) */

 32 * 32 = 1024 */

 I'm in even chunk.  blast odd chunks */

 I'm in odd chunk.  blast even chunks */

 I'm in even chunk.  blast odd chunks */

 I'm in odd chunk.  blast even chunks */

		/*

		 * These caches are inclusive caches, that is, if something

		 * is not cached in the S-cache, we know it also won't be

		 * in one of the primary caches.

 Use get_ebase_cpunum() for both NUMA=y/n */

/**

 * has_valid_asid() - Determine if an mm already has an ASID.

 * @mm:		Memory map.

 * @type:	R4K_HIT or R4K_INDEX, type of cache op.

 *

 * Determines whether @mm already has an ASID on any of the CPUs which cache ops

 * of type @type within an r4k_on_each_cpu() call will affect. If

 * r4k_on_each_cpu() does an SMP call to a single VPE in each core, then the

 * scope of the operation is confined to sibling CPUs, otherwise all online CPUs

 * will need to be checked.

 *

 * Must be called in non-preemptive context.

 *

 * Returns:	1 if the CPUs affected by @type cache ops have an ASID for @mm.

 *		0 otherwise.

 cpu_sibling_map[] undeclared when !CONFIG_SMP */

	/*

	 * If r4k_on_each_cpu does SMP calls, it does them to a single VPE in

	 * each foreign core, so we only need to worry about siblings.

	 * Otherwise we need to worry about all present CPUs.

/*

 * Note: flush_tlb_range() assumes flush_cache_range() sufficiently flushes

 * whole caches when vma is executable.

	/*

	 * If dcache can alias, we must blast it since mapping is changing.

	 * If executable, we must ensure any dirty lines are written back far

	 * enough to be visible to icache.

 If executable, blast stale lines from icache */

	/*

	 * Kludge alert.  For obscure reasons R4000SC and R4400SC go nuts if we

	 * only flush the primary caches but R1x000 behave sane ...

	 * R4000SC and R4400SC indexed S-cache ops also invalidate primary

	 * caches, so we can bail out early.

	/*

	 * If owns no valid ASID yet, cannot possibly have gotten

	 * this page into the cache.

	/*

	 * If the page isn't marked valid, the page cannot possibly be

	 * in the cache.

		/*

		 * Use kmap_coherent or kmap_atomic to do flushes for

		 * another ASID than the current one.

	/*

	 * Indexed cache ops require an SMP call.

	 * Consider if that can or should be avoided.

		/*

		 * If address-based cache ops don't require an SMP call, then

		 * use them exclusively for small flushes.

 Catch bad driver code */

	/*

	 * Either no secondary cache or the available caches don't have the

	 * subset property so we have to flush the primary caches

	 * explicitly.

	 * If we would need IPI to perform an INDEX-type operation, then

	 * we have to use the HIT-type alternative as IPI cannot be used

	 * here due to interrupts possibly being disabled.

 Catch bad driver code */

			/*

			 * There is no clearly documented alignment requirement

			 * for the cache instruction on MIPS processors and

			 * some processors, among them the RM5200 and RM7000

			 * QED processors will throw an address error for cache

			 * hit ops with insufficient alignment.	 Solved by

			 * aligning the address to cache line size.

 CONFIG_DMA_NONCOHERENT */

	/*

	 * Aliases only affect the primary caches so don't bother with

	 * S-caches or T-caches.

	/*

	 * Aliases only affect the primary caches so don't bother with

	 * S-caches or T-caches.

 RM7000 erratum #31. The icache is screwed at startup. */

	/*

	 * Early versions of the 74K do not update the cache tags on a

	 * vtag miss/ptag hit which can occur in the case of KSEG0/KUSEG

	 * aliases.  In this case it is better to treat the cache as always

	 * having aliases.  Also disable the synonym tag update feature

	 * where available.  In this case no opportunistic tag update will

	 * happen where a load causes a virtual address miss but a physical

	 * address hit during a D-cache look-up.

 QED style two way caches? */

 doesn't matter */

 does not matter */

 Workaround for cache instruction bug of VR4131 */

 doesn't matter */

 does not matter */

 For now lie about the number of ways. */

		/*

		 * So we seem to be a MIPS32 or MIPS64 CPU

		 * So let's probe the I-cache ...

 IL == 7 is reserved */

		/*

		 * Now probe the MIPS32 / MIPS64 data cache.

 DL == 7 is reserved */

	/*

	 * Processor configuration sanity check for the R4000SC erratum

	 * #5.	With page sizes larger than 32kB there is no possibility

	 * to get a VCE exception anymore so we don't care about this

	 * misconfiguration.  The case is rather theoretical anyway;

	 * presumably no vendor is shipping his hardware in the "bad"

	 * configuration.

 compute a couple of other cache variables */

	/*

	 * R1x000 P-caches are odd in a positive way.  They're 32kB 2-way

	 * virtually indexed so normally would suffer from aliases.  So

	 * normally they'd suffer from aliases but magic in the hardware deals

	 * with that for us so we don't need to take care ourselves.

			/*

			 * Effectively physically indexed dcache,

			 * thus no virtual aliases.

 Physically indexed caches don't suffer from virtual aliasing */

	/*

	 * In systems with CM the icache fills from L2 or closer caches, and

	 * thus sees remote stores without needing to write them back any

	 * further than that.

		/*

		 * Some older 20Kc chips doesn't have the 'VI' bit in

		 * the config register.

 Cache aliases are handled in hardware; allow HIGHMEM */

		/*

		 * LOONGSON2 has 4 way icache, but when using indexed cache op,

		 * one op will act on all 4 ways

/*

 * If you even _breathe_ on this function, look at the gcc output and make sure

 * it does not pop things on and off the stack for the cache sizing loop that

 * executes in KSEG1 space or else you will crash and burn badly.  You have

 * been warned.

	/*

	 * This is such a bitch, you'd think they would make it easy to do

	 * this.  Away you daemons of stupidity!

 Fill each size-multiple cache line with a valid tag. */

 whee... */

 Load first line with zero (therefore invalid) tag. */

 avoid the hazard */

 Now search for the wrap around point. */

 hazard... */

 does not matter */

 Loongson-3 has 4-Scache banks, while Loongson-2K have only 2 banks */

	/*

	 * Do the probing thing on R4000SC and R4400SC processors.  Other

	 * processors don't have a S-cache that would be relevant to the

	 * Linux memory management.

 don't need to worry about L2, fully coherent */

 compute a couple of other cache variables */

	/*

	 * c0_config.od (bit 19) was write only (and read as 0)

	 * on the early revisions of Alchemy SOCs.  It disables the bus

	 * transaction overlapping and needs to be set to fix various errata.

 Au1000 DA */

 Au1000 HA */

 Au1000 HB */

 Au1500 AB */

	/*

	 * Au1100 errata actually keeps silence about this bit, so we set it

	 * just in case for those revisions that require it to be set according

	 * to the (now gone) cpu table.

 Au1100 AB */

 Au1100 BA */

 Au1100 BC */

 CP0 hazard avoidance. */

 clear all three cache coherency fields */

	/*

	 * c0_status.cu=0 specifies that updates by the sc instruction use

	 * the coherency mode specified by the TLB; 1 means cachable

	 * coherent update on write will be used.  Not all processors have

	 * this bit and; some wire it to zero, others like Toshiba had the

	 * silly idea of putting something else there ...

	/*

	 * We need to catch the early Alchemy SOCs with

	 * the write-only co_config.od bit and set it back to one on:

	 * Au1000 rev DA, HA, HB;  Au1100 AB, BA, BC, Au1500 AB

	/*

	 * Some MIPS32 and MIPS64 processors have physically indexed caches.

	 * This code supports virtually indexed processors and will be

	 * unnecessarily inefficient on physically indexed processors.

 CONFIG_DMA_NONCOHERENT */

	/*

	 * We want to run CMP kernels on core with and without coherent

	 * caches. Therefore, do not use CONFIG_MIPS_CMP to decide whether

	 * or not to flush caches.

	/*

	 * Per-CPU overrides

 No IPI is needed because all CPUs share the same D$ */

 We lose our superpowers if L2 is disabled */

 I$ fills from D$ just by emptying the write buffers */

 Optimization: an L2 flush implicitly flushes the L1 */

 Loongson-3 maintains cache coherency by hardware */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000  Ani Joshi <ajoshi@unixbox.com>

 * Copyright (C) 2000, 2001, 06	 Ralf Baechle <ralf@linux-mips.org>

 * swiped from i386, and cloned for MIPS by Geert, polished by Ralf.

/*

 * The affected CPUs below in 'cpu_needs_post_dma_flush()' can speculatively

 * fill random cachelines with stale data at any time, requiring an extra

 * flush post-DMA.

 *

 * Warning on the terminology - Linux calls an uncached area coherent;  MIPS

 * terminology calls memory areas with hardware maintained coherency coherent.

 *

 * Note that the R14000 and R16000 should also be checked for in this condition.

 * However this function is only called on non-I/O-coherent systems and only the

 * R10000 and R12000 are used in such systems, the SGI IP28 Indigo² rsp.

 * SGI IP32 aka O2.

		/*

		 * Presence of MAARs suggests that the CPU supports

		 * speculatively prefetching data, and therefore requires

		 * the post-DMA flush/invalidate.

/*

 * A single sg entry may refer to multiple physically contiguous pages.  But

 * we still need to process highmem pages individually.  If highmem is not

 * configured then the bulk of this loop gets optimized out.

 SPDX-License-Identifier: GPL-2.0

/*

 * r2300.c: R2000 and R3000 specific mmu/cache code.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 *

 * with a lot of changes to make this thing work for R3000s

 * Tx39XX R4k style caches added. HK

 * Copyright (C) 1998, 1999, 2000 Harald Koerfgen

 * Copyright (C) 1998 Gleb Raiko & Vladimir Roganov

 * Copyright (C) 2001, 2004, 2007  Maciej W. Rozycki

 Size in bytes */

 Size in bytes */

 isolate cache space */

 isolate cache space */

 isolate cache space */

 isolate cache space */

 No ASID => no such page in the cache.  */

 Invalid => no such page in the cache.  */

 Catch bad driver code */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Synthesize TLB refill handlers at runtime.

 *

 * Copyright (C) 2004, 2005, 2006, 2008	 Thiemo Seufer

 * Copyright (C) 2005, 2007, 2008, 2009	 Maciej W. Rozycki

 * Copyright (C) 2006  Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2008, 2009 Cavium Networks, Inc.

 * Copyright (C) 2011  MIPS Technologies, Inc.

 *

 * ... and the days got worse and worse and now you see

 * I've gone completely out of my mind.

 *

 * They're coming to take me a away haha

 * they're coming to take me a away hoho hihi haha

 * to the funny farm where code is beautiful all the time ...

 *

 * (Condolences to Napoleon XIV)

/*

 * TLB load/store/modify handlers.

 *

 * Only the fastpath gets synthesized at runtime, the slowpath for

 * do_page_fault remains normal asm.

 XXX: We should probe for the presence of this bug, but we don't. */

 XXX: We should probe for the presence of this bug, but we don't. */

	/*

	 * CVMSEG starts at address -32768 and extends for

	 * CAVIUM_OCTEON_CVMSEG_SIZE 128 byte cache lines.

 Kernel use starts at the top and works down. */

 Really unreachable, but evidently some GCC want this. */

/*

 * Found by experiment: At least some revisions of the 4kc throw under

 * some circumstances a machine check exception, triggered by invalid

 * values in the index register.  Delaying the tlbp instruction until

 * after the next branch,  plus adding an additional nop in front of

 * tlbwi/tlbwr avoids the invalid index register values. Nobody knows

 * why; it's not an issue caused by the core RTL.

 *

 Handle labels (which must be positive integers). */

 _tlbw_hazard_x is handled differently.  */

/*

 * pgtable bits are assigned dynamically depending on processor feature

 * and statically based on kernel configuration.  This spits out the actual

 * values the kernel is using.	Required to make sense from disassembled

 * TLB exception handlers.

 The only general purpose registers allowed in TLB handlers. */

 Some CP0 registers */

/* The worst case length of the handler is around 18 instructions for

 * R3000-style TLBs and up to 63 instructions for R4000-style TLBs.

 * Maximum space available is 32 instructions for R3000 and 64

 * instructions for R4000.

 *

 * We deliberately chose a buffer size of 128, so we won't scribble

 * over anything important on overflow before we panic.

 simply assume worst case size for labels and relocs */

 make it zero based */

 Save in CPU local C0_KScratch? */

 Get smp_processor_id */

 handler_reg_save index in K0 */

 K0 now points to save area, save $1 and $2  */

 K0 already points to save area, restore $1 and $2  */

/*

 * CONFIG_MIPS_PGD_C0_CONTEXT implies 64 bit and lack of pgd_current,

 * we cannot do r3000 under these circumstances.

 *

 * The R3000 TLB handler is simple.

 cp0 delay */

 load delay */

 cp0 delay */

 load delay */

 load delay */

 cp0 delay */

 cp0 delay */

 branch delay */

 CONFIG_MIPS_PGD_C0_CONTEXT */

/*

 * The R4000 TLB handler is much more complicated. We have two

 * consecutive handler areas with 32 instructions space each.

 * Since they aren't used at the same time, we can overflow in the

 * other one.To keep things simple, we first assume linear space,

 * then we relocate it to the final handler layout as needed.

/*

 * Hazards

 *

 * From the IDT errata for the QED RM5230 (Nevada), processor revision 1.0:

 * 2. A timing hazard exists for the TLBP instruction.

 *

 *	stalling_instruction

 *	TLBP

 *

 * The JTLB is being read for the TLBP throughout the stall generated by the

 * previous instruction. This is not really correct as the stalling instruction

 * can modify the address used to access the JTLB.  The failure symptom is that

 * the TLBP instruction will use an address created for the stalling instruction

 * and not the address held in C0_ENHI and thus report the wrong results.

 *

 * The software work-around is to not allow the instruction preceding the TLBP

 * to stall - make it an NOP or some other instruction guaranteed not to stall.

 *

 * Errata 2 will not be fixed.	This errata is also on the R5000.

 *

 * As if we MIPS hackers wouldn't know how to nop pipelines happy ...

 Found by experiment: R4600 v2.0/R4700 needs this, too.  */

		/*

		 * This branch uses up a mtc0 hazard nop slot and saves

		 * two nops after the tlbw instruction.

 QED specifies 2 nops hazard */

 QED specifies 2 nops hazard */

 pte_t is already in EntryLo format */

		/*

		 * Ensure the MFC0 below observes the value written to the

		 * KScratch register by the prior MTC0.

 Reset default page size */

 Reset default page size */

 Set huge page tlb entry size */

/*

 * Check if Huge PTE is present, if so then jump to LABEL.

	/*

	 * A huge PTE describes an area the size of the

	 * configured huge page size. This is twice the

	 * of the large TLB entry size we intend to use.

	 * A TLB entry half the size of the configured

	 * huge page size is configured into entrylo0

	 * and entrylo1 to cover the contiguous huge PTE

	 * address space.

 We can clobber tmp.	It isn't used after this.*/

 load it */

 convert to entrylo1 */

 load it */

 Needed because SC killed our PTE */

 CONFIG_MIPS_HUGE_TLB_SUPPORT */

/*

 * TMP and PTR are scratch.

 * TMP will be clobbered, PTR will hold the pmd entry.

	/*

	 * The vmalloc handling is not in the hotpath.

		/*

		 * The kernel currently implicitely assumes that the

		 * MIPS SEGBITS parameter for the processor is

		 * (PGDIR_SHIFT+PGDIR_BITS) or less, and will never

		 * allocate virtual addresses outside the maximum

		 * range for SEGBITS = (PGDIR_SHIFT+PGDIR_BITS). But

		 * that doesn't prevent user code from accessing the

		 * higher xuseg addresses.  Here, we make sure that

		 * everything but the lower xuseg addresses goes down

		 * the module_alloc/vmalloc path.

 No uasm_i_nop needed here, since the next insn doesn't touch TMP. */

 pgd is in pgd_reg */

		/*

		 * &pgd << 11 stored in CONTEXT [23..63].

 Clear lower 23 bits of context. */

 insert bit[63:59] of CAC_BASE into bit[11:6] of ptr */

 get pgd offset in bytes */

 add in pgd offset */

 get faulting address */

 get pud pointer */

 get pud offset in bytes */

 add in pud offset */

 get faulting address */

 get pmd pointer */

 get pmd offset in bytes */

 add in pmd offset */

/*

 * BVADDR is the faulting address, PTR is scratch.

 * PTR will hold the pgd for vmalloc.

 fall through */

		/*

		 * We get here if we are an xsseg address, or if we are

		 * an xuseg address above (PGDIR_SHIFT+PGDIR_BITS) boundary.

		 *

		 * Ignoring xsseg (assume disabled so would generate

		 * (address errors?), the only remaining possibility

		 * is the upper xuseg addresses.  On processors with

		 * TLB_SEGBITS <= PGDIR_SHIFT+PGDIR_BITS, these

		 * addresses would have taken an address error. We try

		 * to mimic that here by taking a load/istream page

		 * fault.

 !CONFIG_64BIT */

/*

 * TMP and PTR are scratch.

 * TMP will be clobbered, PTR will hold the pgd entry.

 pgd is in pgd_reg */

 get faulting address */

 32 bit SMP has smp_processor_id() stored in CONTEXT. */

 get faulting address */

 get pgd only bits */

 add in pgd offset */

 !CONFIG_64BIT */

	/*

	 * Bug workaround for the Nevada. It seems as if under certain

	 * circumstances the move from cp0_context might produce a

	 * bogus result when the mfc0 instruction and its consumer are

	 * in a different cacheline or a load instruction, probably any

	 * memory reference, is between them.

 get context reg */

 get context reg */

 add in offset */

 The low 32 bits of EntryLo is stored in pte_high */

 even pte */

 odd pte */

 get even pte */

 get odd pte */

 load it */

 load it */

 Our extra working register */

 Clear lower 23 bits of context. */

 Clear lower 23 bits of context. */

 insert bit[63:59] of CAC_BASE into bit[11:6] of ptr */

 get pgd offset in bytes */

	/*

	 *			   tmp		ptr

	 * fall-through case =	 badvaddr  *pgd_current

	 * vmalloc case	     =	 badvaddr  swapper_pg_dir

 get pgd offset in bytes */

 get context reg */

 add in pgd offset */

 get pmd pointer */

 get pud offset in bytes */

 add in pmd offset */

 ptr contains a pointer to PMD entry */

 tmp contains the address */

 get pmd offset in bytes */

 get context reg */

 add in pmd offset */

 Adjust the context during the load latency. */

	/*

	 * The in the LWX case we don't want to do the load in the

	 * delay slot.	It cannot issue in the same cycle and may be

	 * speculative and unneeded.

 CONFIG_MIPS_HUGE_TLB_SUPPORT */

 build_update_entries */

 add in offset */

 get even pte */

 get odd pte */

 load it */

 load it */

 load it */

 return from trap */

/*

 * For a 64-bit kernel, we are using the 64-bit XTLB refill exception

 * because EXL == 0.  If we wrap, we can also use the 32 instruction

 * slots before the XTLB refill exception handler which belong to the

 * unused TLB refill exception.

		/*

		 * create the plain linear handler

 No need for uasm_i_nop */

 get pmd in K1 */

 get pgd in K1 */

 return from trap */

	/*

	 * Overflow check: For the 64bit handler, we need at least one

	 * free instruction slot for the wrap-around branch. In worst

	 * case, if the intended insertion point is a delay slot, we

	 * need three, with the second nop'ed and the third being

	 * unused.

 Loongson2 ebase is different than r4k, we have more space */

			/*

			 * Now fold the handler in the TLB refill handler space.

 Simplest case, just copy the handler. */

			/*

			 * Now fold the handler in the TLB refill handler space.

 Just copy the handler. */

				/*

				 * See if we have overflown one way or the other.

					/*

					 * Split two instructions before the end.  One

					 * for the branch and one for the instruction

					 * in the delay slot.

					/*

					 * If the branch would fall in a delay slot,

					 * we must back up an additional instruction

					 * so that it is no longer in a delay slot.

 Copy first part of the handler. */

 Insert branch. */

 Copy the rest of the handler. */

 bit used to indicate huge page */

 1st level PGD */

 2nd level PMD */

 3rd level PTE */

 Set PWDirExt */

 KScratch6 is used for KPGD */

 global page dir */

 middle page dir */

 even */

 odd */

 restore page mask */

 PGD << 11 in c0_Context */

		/*

		 * If it is a ckseg0 address, convert to a physical

		 * address.  Shifting right by 29 and adding 4 will

		 * result in zero for these addresses.

		 *

 PGD in c0_KScratch */

 Save PGD to pgd_current[smp_processor_id()] */

 SMP */

 if pgd_reg is allocated, save PGD also to scratch register */

 no uasm_i_nop needed */

 no uasm_i_nop needed */

/*

 * Check if PTE is present, if not then jump to LABEL. PTR points to

 * the page table where this PTE is located, PTE will be re-loaded

 * with it's original value.

 You lose the SMP race :-(*/

 You lose the SMP race :-(*/

 Make PTE valid, store result in PTR. */

/*

 * Check if PTE can be written to, if not branch to LABEL. Regardless

 * restore PTE with value from PTR when done.

 You lose the SMP race :-(*/

/* Make PTE writable, update software status bits as well, then store

 * at PTR.

/*

 * Check if PTE can be modified, if not branch to LABEL. Regardless

 * restore PTE with value from PTR when done.

 You lose the SMP race :-(*/

/*

 * R3000 style TLB load/store/modify handlers.

/*

 * This places the pte into ENTRYLO0 and writes it with tlbwi.

 * Then it returns.

 cp0 delay */

 cp0 delay */

 branch delay */

/*

 * This places the pte into ENTRYLO0 and writes it with tlbwi

 * or tlbwr as appropriate.  This is because the index register

 * may have the probe fail bit set as a result of a trap on a

 * kseg2 access, i.e. without refill.  Then it returns.

 cp0 delay */

 cp0 delay */

 branch delay */

 cp0 delay */

 branch delay */

 cp0 delay */

 branch delay */

 cp0 delay */

 load delay */

 cp0 delay */

 load delay */

 load delay */

 load delay */

 load delay */

 load delay */

 CONFIG_MIPS_PGD_C0_CONTEXT */

	/*

	 * When a Hardware Table Walker is running it can replace TLB entries

	 * at any time, leading to a race between it & the CPU.

	/*

	 * If the CPU shares FTLB RAM with its siblings then our entry may be

	 * replaced at any time by a sibling performing a write to the FTLB.

 In all other cases there ought to be no race condition to handle */

/*

 * R4000 style TLB load/store/modify handlers.

 get pmd in ptr */

 get pgd in ptr */

	/*

	 * For huge tlb entries, pmd doesn't contain an address but

	 * instead contains the tlb pte. Check the PAGE_HUGE bit and

	 * see if we need to jump to huge tlb processing.

 get even pte */

 race condition happens, leaving */

 return from trap */

 No need for uasm_i_nop */

		/*

		 * If the page is not _PAGE_VALID, RI or XI could not

		 * have triggered it.  Skip the expensive test..

		/*

		 * Warn if something may race with us & replace the TLB entry

		 * before we read it here. Everything with such races should

		 * also have dedicated RiXi exception handlers, so this

		 * shouldn't be hit.

 Examine  entrylo 0 or 1 based on ptr. */

 load it in the delay slot*/

 load it if ptr is odd */

		/*

		 * If the entryLo (now in wr.r3) is valid (bit 1), RI or

		 * XI must have triggered it.

	/*

	 * This is the entry point when build_r4000_tlbchange_handler_head

	 * spots a huge page.

		/*

		 * If the page is not _PAGE_VALID, RI or XI could not

		 * have triggered it.  Skip the expensive test..

		/*

		 * Warn if something may race with us & replace the TLB entry

		 * before we read it here. Everything with such races should

		 * also have dedicated RiXi exception handlers, so this

		 * shouldn't be hit.

 Examine  entrylo 0 or 1 based on ptr. */

 load it in the delay slot*/

 load it if ptr is odd */

		/*

		 * If the entryLo (now in wr.r3) is valid (bit 1), RI or

		 * XI must have triggered it.

		/*

		 * We clobbered C0_PAGEMASK, restore it.  On the other branch

		 * it is restored in build_huge_tlb_write_entry.

	/*

	 * This is the entry point when

	 * build_r4000_tlbchange_handler_head spots a huge page.

 Present and writable bits set, set accessed and dirty bits. */

	/*

	 * This is the entry point when

	 * build_r4000_tlbchange_handler_head spots a huge page.

	/*

	 * We are using 2-level page tables, so we only need to

	 * setup GDW and PTW appropriately. UDW and MDW will remain 0.

	 * The default value of GDI/UDI/MDI/PTI is 0xc. It is illegal to

	 * write values less than 0xc in these fields because the entire

	 * write will be dropped. As a result of which, we must preserve

	 * the original reset values and overwrite only what we really want.

 re-initialize the GDI field */

 re-initialize the PTI field including the even/odd bit */

 Set the PTEI right shift */

 Check whether the PTEI value is supported */

		/*

		 * Drop option to avoid HTW being enabled via another path

		 * (eg htw_reset())

 Set pointer size to size of directory pointers */

 PTEs may be multiple pointers long (e.g. with XPA) */

 Make sure everything is set before we enable the HTW */

	/*

	 * Enable HTW (and only for XUSeg on 64-bit), and disable the rest of

	 * the pwctl fields.

		/*

		 * We'll only be making use of the fact that we can rotate bits

		 * into the fill if the CPU supports RIXI, so don't bother

		 * probing this for CPUs which don't.

 clear all non-PFN bits */

 find a lower bound on PABITS, and upper bound on fill bits */

 minus the RI & XI bits */

	/*

	 * The refill handler is generated per-CPU, multi-node systems

	 * may have local storage for it. The other handlers are only

	 * needed once.

 SPDX-License-Identifier: GPL-2.0-only

 highest bit set means kernel space */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1997, 99, 2001 - 2004 Ralf Baechle <ralf@linux-mips.org>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * (C) Copyright 1995 1996 Linus Torvalds

 * (C) Copyright 2001, 2002 Ralf Baechle

/*

 * ioremap_prot     -   map bus memory into CPU space

 * @phys_addr:    bus address of the memory

 * @size:      size of the resource to map

 *

 * ioremap_prot gives the caller control over cache coherency attributes (CCA)

 Don't allow wraparound or zero size */

	/*

	 * Map uncached objects in the low 512mb of address space using KSEG1,

	 * otherwise map using page tables.

	/*

	 * Don't allow anybody to remap RAM that may be allocated by the page

	 * allocator, since that could lead to races & data clobbering.

	/*

	 * Mappings have to be page-aligned

	/*

	 * Ok, go for it..

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1999, 2000 by Silicon Graphics

 * Copyright (C) 2003 by Ralf Baechle

 Initialize the entire pgd.  */

	/*

	 * Fixed mappings:

 SPDX-License-Identifier: GPL-2.0

/*

 * sc-ip22.c: Indy cache management functions.

 *

 * Copyright (C) 1997, 2001 Ralf Baechle (ralf@gnu.org),

 * derived from r4xx0.c by David S. Miller (davem@davemloft.net).

 Secondary cache size in bytes, if present.  */

 Catch bad driver code */

 Which lines to flush?  */

 This is really cool... */

/* XXX Check with wje if the Indy caches can differentiate between

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * A small micro-assembler. It is intentionally kept simple, does only

 * support a subset of instructions, and does not try to hide pipeline

 * effects like branch delay slots.

 *

 * Copyright (C) 2004, 2005, 2006, 2008	 Thiemo Seufer

 * Copyright (C) 2005, 2007  Maciej W. Rozycki

 * Copyright (C) 2006  Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2012, 2013   MIPS Technologies, Inc.  All rights reserved.

 This macro sets the non-variable bits of an instruction. */

/*

 * The order of opcode arguments is implicitly left to right,

 * starting with RS and ending with FUNC or IMM.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (c) 2004 MIPS Inc

 * Author: chris@mips.com

 *

 * Copyright (C) 2004, 06 Ralf Baechle <ralf@linux-mips.org>

 mask off an interrupt */

 unmask an interrupt */

/*

 * Masks and ACKs an IRQ

/*

 * Masks and ACKs an IRQ

/*

 * Interrupt handler for interrupts coming from SOC-it.

 read the interrupt vector register */

 Ignore spurious interrupt */

 Reset interrupt controller - initialises all registers to 0 */

 Enable interrupt generation */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994 - 1999, 2000 by Ralf Baechle and others.

 * Copyright (C) 2005, 2006 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2004 Thiemo Seufer

 * Copyright (C) 2013  Imagination Technologies Ltd.

 New thread loses kernel privileges. */

	/*

	 * User threads may have allocated a delay slot emulation frame.

	 * If so, clean up that allocation.

	/*

	 * Save any process state which is live in hardware registers to the

	 * parent context prior to duplication. This prevents the new child

	 * state becoming stale if the parent is preempted before copy_thread()

	 * gets a chance to save the parent's live hardware registers to the

	 * child context.

/*

 * Copy architecture-specific thread state

 set up new TSS. */

  Put the stack after the struct pt_regs.  */

 kernel thread */

 fn */

 user thread */

 Clear error flag */

 Child gets zero as return value */

	/*

	 * New tasks lose permission to use the fpu. This accelerates context

	 * switching for most programs since they don't use the fpu.

 CONFIG_MIPS_MT_FPAFF */

	/*

	 * jr16 ra

	 * jr ra

	/*

	 * swsp ra,offset

	 * swm16 reglist,offset(sp)

	 * swm32 reglist,offset(sp)

	 * sw32 ra,offset(sp)

	 * jradiussp - NOT SUPPORTED

	 *

	 * microMIPS is way more fun...

 sw / sd $ra, offset($sp) */

	/*

	 * jr16,jrc,jalr16,jalr16

	 * jal

	 * jalr/jr,jalr.hb/jr.hb,jalrs,jalrs.hb

	 * jraddiusp - NOT SUPPORTED

	 *

	 * microMIPS is kind of more fun...

	/*

	 * addiusp -imm

	 * addius5 sp,-imm

	 * addiu32 sp,sp,-imm

	 * jradiussp - NOT SUPPORTED

	 *

	 * microMIPS is not more fun...

 0x0,0x1,0x1fe,0x1ff are special */

 addiu/daddiu sp,sp,-imm */

			/*

			 * If we see a jump instruction, we are finished

			 * with the frame save.

			 *

			 * Some functions can have a shortcut return at

			 * the beginning of the function, so don't start

			 * looking for jump instruction until we see the

			 * frame setup.

			 *

			 * The RA save instruction can get put into the

			 * delay slot of the jump instruction, so look

			 * at the next instruction, too.

 nested */

 leaf */

 prologue seems bogus... */

	/*

	 * Without schedule() frame info, result given by

	 * thread_saved_pc() and __get_wchan() are not reliable.

/*

 * Return saved PC of a blocked thread.

 New born processes are a special case */

 generic stack unwinding function */

	/*

	 * IRQ stacks start at IRQ_STACK_START

	 * task stacks at THREAD_SIZE - 32

	/*

	 * If we reached the top of the interrupt stack, start unwinding

	 * the interrupted task stack.

		/*

		 * Check that the pointer saved in the IRQ stack head points to

		 * something within the stack of the current task

		/*

		 * Follow pointer to tasks kernel stack frame where interrupted

		 * state was saved.

	/*

	 * Return ra if an exception occurred at the first instruction

 analyze from start to ofs */

		/*

		 * For some extreme cases, get_frame_info() can

		 * consider wrongly a nested function as a leaf

		 * one. In that cases avoid to return always the

		 * same value.

 used by show_backtrace() */

/*

 * __get_wchan - a maintenance nightmare^W^Wpain in the ass ...

 One page for branch delay slot "emulation" */

 Space for the VDSO, data page & GIC user page */

 Space for cache colour alignment */

 Space to randomize the VDSO base */

/*

 * Don't forget that the stack pointer must be aligned on a 8 bytes

 * boundary for 32-bits ABI and 16 bytes for 64-bits ABI.

		/*

		 * If we previously sent an IPI to the target CPU & it hasn't

		 * cleared its bit in the busy cpumask then it didn't handle

		 * our previous IPI & it's not safe for us to reuse the

		 * call_single_data_t.

	/*

	 * This is icky, but we use this to simply ensure that all CPUs have

	 * context switched, regardless of whether they were previously running

	 * kernel or user code. This ensures that no CPU that a mode-switching

	 * program may execute on keeps its FPU enabled (& in the old mode)

	 * throughout the mode switch.

 If nothing to change, return right away, successfully.  */

 Only accept a mode change if 64-bit FP enabled for o32.  */

 And only for o32 tasks.  */

 Check the value is valid */

 Setting FRE without FR is not supported.  */

 Avoid inadvertently triggering emulation */

 FR = 0 not supported in MIPS R6 */

 Indicate the new FP mode in each thread */

 Update desired FP register width */

 Update desired FP single layout */

	/*

	 * We need to ensure that all threads in the process have switched mode

	 * before returning, in order to allow userland to not worry about

	 * races. We can do this by forcing all CPUs that any thread in the

	 * process may be running on to schedule something else - in this case

	 * prepare_for_fp_mode_switch().

	 *

	 * We begin by generating a mask of all CPUs that any thread in the

	 * process may be running on.

	/*

	 * Now we schedule prepare_for_fp_mode_switch() on each of those CPUs.

	 *

	 * The CPUs may have rescheduled already since we switched mode or

	 * generated the cpumask, but that doesn't matter. If the task in this

	 * process is scheduled out then our scheduling

	 * prepare_for_fp_mode_switch() will simply be redundant. If it's

	 * scheduled in then it will already have picked up the new FP mode

	 * whilst doing so.

 k0/k1 are copied as zero. */

 CONFIG_32BIT || CONFIG_MIPS32_O32 */

 k0/k1 are copied as zero. */

 CONFIG_64BIT */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2014 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 Whether to accept legacy-NaN and 2008-NaN user binaries.  */

 FPU modes */

/**

 * struct mode_req - ABI FPU mode requirements

 * @single:	The program being loaded needs an FPU but it will only issue

 *		single precision instructions meaning that it can execute in

 *		either FR0 or FR1.

 * @soft:	The soft(-float) requirement means that the program being

 *		loaded needs has no FPU dependency at all (i.e. it has no

 *		FPU instructions).

 * @fr1:	The program being loaded depends on FPU being in FR=1 mode.

 * @frdefault:	The program being loaded depends on the default FPU mode.

 *		That is FR0 for O32 and FR1 for N32/N64.

 * @fre:	The program being loaded depends on FPU with FRE=1. This mode is

 *		a bridge which uses FR=1 whilst still being able to maintain

 *		full compatibility with pre-existing code using the O32 FP32

 *		ABI.

 *

 * More information about the FP ABIs can be found here:

 *

 * https://dmz-portal.mips.com/wiki/MIPS_O32_ABI_-_FR0_and_FR1_Interlinking#10.4.1._Basic_mode_set-up

 *

/*

 * Mode requirements when .MIPS.abiflags is not present in the ELF.

 * Not present means that everything is acceptable except FR1.

 Let's see if this is an O32 ELF */

			/*

			 * Set MIPS_ABI_FP_OLD_64 for EF_MIPS_FP64. We will override it

			 * later if needed

 Record the required FP ABIs for use by mips_check_elf */

	/*

	 * Determine the NaN personality, reject the binary if not allowed.

	 * Also ensure that any interpreter matches the executable.

 Default to a mode capable of running code expecting FR=0 */

 Allow all ABIs we know about */

 MIPS64 code always uses FR=1, thus the default is easy */

 Disallow access to the various FPXX & FP64 ABIs */

 It's time to determine the FPU mode requirements */

	/*

	 * Check whether the program's and interp's ABIs have a matching FPU

	 * mode requirement.

	/*

	 * Determine the desired FPU mode

	 *

	 * Decision making:

	 *

	 * - We want FR_FRE if FRE=1 and both FR=1 and FR=0 are false. This

	 *   means that we have a combination of program and interpreter

	 *   that inherently require the hybrid FP mode.

	 * - If FR1 and FRDEFAULT is true, that means we hit the any-abi or

	 *   fpxx case. This is because, in any-ABI (or no-ABI) we have no FPU

	 *   instructions so we don't care about the mode. We will simply use

	 *   the one preferred by the hardware. In fpxx case, that ABI can

	 *   handle both FR=1 and FR=0, so, again, we simply choose the one

	 *   preferred by the hardware. Next, if we only use single-precision

	 *   FPU instructions, and the default ABI FPU mode is not good

	 *   (ie single + any ABI combination), we set again the FPU mode to the

	 *   one is preferred by the hardware. Next, if we know that the code

	 *   will only use single-precision instructions, shown by single being

	 *   true but frdefault being false, then we again set the FPU mode to

	 *   the one that is preferred by the hardware.

	 * - We want FP_FR1 if that's the only matching mode and the default one

	 *   is not good.

	 * - Return with -ELIBADD if we can't find a matching FPU mode.

 Make sure 64-bit MIPS III/IV/64R1 will not pick FR1 */

	/*

	 * This function is only ever called for O32 ELFs so we should

	 * not be worried about N32/N64 binaries.

/*

 * Select the IEEE 754 NaN encoding and ABS.fmt/NEG.fmt execution mode

 * in FCSR according to the ELF NaN personality.

 CONFIG_MIPS_FP_SUPPORT */

 The binary doesn't request a non-executable stack */

 The CPU doesn't support non-executable memory */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2014 Imagination Technologies Ltd.

 *

 * CPU PM notifiers for saving/restoring general CPU state.

 Used by PM helper macros in asm/pm.h */

/**

 * mips_cpu_save() - Save general CPU state.

 * Ensures that general CPU context is saved, notably FPU and DSP.

 Save FPU state */

 Save DSP state */

/**

 * mips_cpu_restore() - Restore general CPU state.

 * Restores important CPU context.

 Restore ASID */

 Restore DSP state */

 Restore UserLocal */

 Restore watch registers */

/**

 * mips_pm_notifier() - Notifier for preserving general CPU context.

 * @self:	Notifier block.

 * @cmd:	CPU PM event.

 * @v:		Private data (unused).

 *

 * This is called when a CPU power management event occurs, and is used to

 * ensure that important CPU context is preserved across a CPU power down.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995, 1996, 1997, 2000, 2001, 05 by Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2001 MIPS Technologies, Inc.

/*

 * For historic reasons the pipe(2) syscall on MIPS has an unusual calling

 * convention.	It returns results in registers $v0 / $v1 which means there

 * is no need for it to do verify the validity of a userspace pointer

 * argument.  Historically that used to be expensive in Linux.	These days

 * the performance advantage is negligible.

 No error */

	/*

	 * Don't let your children do this ...

 no outputs */

 unreached.  Honestly.  */

/*

 * mips_atomic_set() normally returns directly via syscall_exit potentially

 * clobbering static registers, so be sure to preserve them.

/*

 * No implemented yet ...

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stack trace management functions

 *

 *  Copyright (C) 2006 Atsushi Nemoto <anemo@mba.ocn.ne.jp>

/*

 * Save stack-backtrace addresses into a stack_trace buffer:

/*

 * Save stack-backtrace addresses into a stack_trace buffer.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000,2001,2004 Broadcom Corporation

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994 - 1999, 2000, 01, 06 Ralf Baechle

 * Copyright (C) 1995, 1996 Paul M. Antoine

 * Copyright (C) 1998 Ulf Carlsson

 * Copyright (C) 1999 Silicon Graphics, Inc.

 * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2002, 2003, 2004, 2005, 2007  Maciej W. Rozycki

 * Copyright (C) 2000, 2001, 2012 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2014, Imagination Technologies Ltd.

/*

 * This routine abuses get_user()/put_user() to reference pointers

 * with at least a bit of error checking ...

	/*

	 * Saved main processor registers

	/*

	 * Saved cp0 registers

/*

 * FIXME: really the generic show_regs should take a const pointer argument.

 Given an address, look for it in the exception tables. */

 XXX For now.	 Fixme, this searches the wrong table ...  */

	/*

	 * Assume it would be too dangerous to continue ...

/*

 * ll/sc, rdhwr, sync emulation

  microMIPS definitions   */

/*

 * The ll_bit is cleared by r*_switch.S

	/*

	 * analyse the ll instruction that just caused a ri exception

	 * and put the referenced address to addr.

 sign extend offset */

	/*

	 * analyse the sc instruction that just caused a ri exception

	 * and put the referenced address to addr.

 sign extend offset */

/*

 * ll uses the opcode of lwc0 and sc uses the opcode of swc0.  That is both

 * opcodes are supposed to result in coprocessor unusable exceptions if

 * executed on ll/sc-less processors.  That's the theory.  In practice a

 * few processors such as NEC's VR4100 throw reserved instruction exceptions

 * instead, so we're doing the emulation thing in both exception handlers.

 Must be something else ... */

/*

 * Simulate trapping 'rdhwr' instructions to provide user accessible

 * registers not implemented in hardware.

 CPU number */

 SYNCI length */

 Read count register */

 Count register resolution */

 Read UserLocal register */

 Not ours.  */

 Not ours.  */

 Must be something else ... */

/*

 * Loongson-3 CSR instructions emulation

 Do not emulate on unsupported core models. */

 Not ours.  */

 CONFIG_CPU_LOONGSON3_CPUCFG_EMULATION */

/*

 * Send SIGFPE according to FCSR Cause bits, which must have already

 * been masked against Enable bits.  This is impotant as Inexact can

 * happen together with Overflow or Underflow, and `ptrace' can set

 * any bits.

 If it's obviously not an FP instruction, skip it */

	/*

	 * do_ri skipped over the instruction via compute_return_epc, undo

	 * that for the FPU emulator.

 Run the emulator */

	/*

	 * We can't allow the emulated instruction to leave any

	 * enabled Cause bits set in $fcr31.

 Restore the hardware register state */

 Send a signal if required.  */

/*

 * XXX Delayed fp exceptions when doing a lazy ctx switch XXX

 Clear FCSR.Cause before enabling interrupts */

		/*

		 * Unimplemented operation exception.  If we've got the full

		 * software emulator on-board, let's use it...

		 *

		 * Force FPU to dump state into task/thread context.  We're

		 * moving a lot of data here for what is probably a single

		 * instruction, but the alternative is to pre-decode the FP

		 * register operands before invoking the emulator, which seems

		 * a bit extreme for what should be an infrequent event.

 Run the emulator */

		/*

		 * We can't allow the emulated instruction to leave any

		 * enabled Cause bits set in $fcr31.

 Restore the hardware register state */

 Using the FPU again.	 */

 Send a signal if required.  */

/*

 * MIPS MT processors may have fewer FPU contexts than CPU threads. If we've

 * emulated more than some threshold number of instructions, force migration to

 * a "CPU" that has FP support.

		/*

		 * If there's no FPU present, or if the application has already

		 * restricted the allowed set to exclude any CPUs with FPUs,

		 * we'll skip the procedure.

 CONFIG_MIPS_MT_FPAFF */

 !CONFIG_MIPS_FP_SUPPORT */

 !CONFIG_MIPS_FP_SUPPORT */

 CONFIG_KGDB_LOW_LEVEL_TRAP */

	/*

	 * A short test says that IRIX 5.3 sends SIGTRAP for all trap

	 * insns, even for trap and break codes that indicate arithmetic

	 * failures.  Weird ...

	 * But should we continue the brokenness???  --macro

		/*

		 * This breakpoint code is used by the FPU emulator to retake

		 * control of the CPU after executing the instruction from the

		 * delay slot of an emulated branch.

		 *

		 * Terminate if exception was recognized as a delay slot return

		 * otherwise handle as normal.

 MIPS16e mode */

 16-bit microMIPS BREAK */

 32-bit microMIPS BREAK */

	/*

	 * There is the ancient bug in the MIPS assemblers that the break

	 * code starts left to bit 16 instead to bit 6 in the opcode.

	 * Gas is bug-compatible, but not always, grrr...

	 * We handle both cases with a simple heuristics.  --macro

	/*

	 * notify the kprobe handlers, if instruction is likely to

	 * pertain to them.

 Immediate versions don't provide a code.  */

 Immediate versions don't provide a code.  */

	/*

	 * Avoid any kernel code. Just emulate the R2 instruction

	 * as quickly as possible.

 Undo skip-over.  */

/*

 * No lock; only written during early bootup by CPU 0.

 Initialize context if it hasn't been used already */

			/*

			 * with MSA enabled, userspace can see MSACSR

			 * and MSA regs, but the values in them are from

			 * other task before current task, restore them

			 * from saved fp/msa context

			/*

			 * own_fpu_inatomic(1) just restore low 64bit,

			 * fix the high 64bit

	/*

	 * This task has formerly used the FP context.

	 *

	 * If this thread has no live MSA vector context then we can simply

	 * restore the scalar FP context. If it has live MSA vector context

	 * (that is, it has or may have used MSA since last performing a

	 * function call) then we'll need to restore the vector context. This

	 * applies even if we're currently only executing a scalar FP

	 * instruction. This is because if we were to later execute an MSA

	 * instruction then we'd either have to:

	 *

	 *  - Restore the vector context & clobber any registers modified by

	 *    scalar FP instructions between now & then.

	 *

	 * or

	 *

	 *  - Not restore the vector context & lose the most significant bits

	 *    of all vector registers.

	 *

	 * Neither of those options is acceptable. We cannot restore the least

	 * significant bits of the registers now & only restore the most

	 * significant bits later because the most significant bits of any

	 * vector registers whose aliased FP register is modified now will have

	 * been zeroed. We'd have no way to know that when restoring the vector

	 * context & thus may load an outdated value for the most significant

	 * bits of a vector register.

	/*

	 * This task is using or has previously used MSA. Thus we require

	 * that Status.FR == 1.

	/*

	 * If this is the first time that the task is using MSA and it has

	 * previously used scalar FP in this time slice then we already nave

	 * FP context which we shouldn't clobber. We do however need to clear

	 * the upper 64b of each vector register so that this task has no

	 * opportunity to see data left behind by another.

		/*

		 * Restore the least significant 64b of each vector register

		 * from the existing scalar FP context.

		/*

		 * The task has not formerly used MSA, so clear the upper 64b

		 * of each vector register such that it cannot see data left

		 * behind by another task.

 We need to restore the vector context. */

 Restore the scalar FP control & status register */

 !CONFIG_MIPS_FP_SUPPORT */

 CONFIG_MIPS_FP_SUPPORT */

 Undo skip-over.  */

		/*

		 * The COP3 opcode space and consequently the CP0.Status.CU3

		 * bit and the CP0.Cause.CE=3 encoding have been removed as

		 * of the MIPS III ISA.  From the MIPS IV and MIPS32r2 ISAs

		 * up the space has been reused for COP1X instructions, that

		 * are enabled by the CP0.Status.CU1 bit and consequently

		 * use the CP0.Cause.CE=1 encoding for Coprocessor Unusable

		 * exceptions.  Some FPU-less processors that implement one

		 * of these ISAs however use this code erroneously for COP1X

		 * instructions.  Therefore we redirect this trap to the FP

		 * emulator too.

		/*

		 * We can't allow the emulated instruction to leave

		 * any enabled Cause bits set in $fcr31.

 Send a signal if required.  */

 CONFIG_MIPS_FP_SUPPORT */

 CONFIG_MIPS_FP_SUPPORT */

 Clear MSACSR.Cause before enabling interrupts */

/*

 * Called with interrupts disabled.

	/*

	 * Clear WP (bit 22) bit of cause register so we don't loop

	 * forever.

	/*

	 * If the current thread has the watch registers loaded, save

	 * their values and send SIGTRAP.  Otherwise another thread

	 * left the registers set, clear them and continue.

	/*

	 * Some chips may have other causes of machine check (e.g. SB1

	 * graduation timer)

	/*

	 * Game over - no way to handle this if it ever occurs.	 Most probably

	 * caused by a new unknown cpu type or after another deadly

	 * hard/software error.

/*

 * Some MIPS CPUs can enable/disable for cache parity detection, but do

 * it different ways.

		/*

		 * With CM3 systems we need to ensure that the L1 & L2

		 * parity enables are set to the same value, since this

		 * is presumed by the hardware engineers.

		 *

		 * If the user disabled either of L1 or L2 ECC checking,

		 * disable both.

 Probe L1 ECC support */

 Probe L2 ECC support */

			/*

			 * One of L1 or L2 ECC checking isn't supported,

			 * so we cannot enable either.

 Configure L1 ECC checking */

 Configure L2 ECC checking */

 probe L1 parity support */

 probe L2 parity support */

 No parity available */

 Set the PE bit (bit 31) in the c0_errctl register. */

 Clear the DE bit (bit 16) in the c0_status register. */

 For the moment, report the problem and hang. */

 For the moment, report the problem and hang. */

 Just print the cacheerr bits for now */

		/* Undocumented exception, will trigger on certain

		 * also-undocumented instructions accessible from userspace.

		 * Processor state is not otherwise corrupted, but currently

		 * we don't know how to proceed. Maybe there is some

		 * undocumented control flag to enable the instructions?

		/* None of the other exceptions, documented or not, have

		 * further details given; none are encountered in the wild

		 * either. Panic in case some of them turn out to be fatal.

/*

 * SDBBP EJTAG debug exception handler.

 * We skip the instruction and return to the next instruction.

		/*

		 * In branch delay slot.

		 * We cheat a little bit here and use EPC to calculate the

		 * debug return address (DEPC). EPC is restored after the

		 * calculation.

/*

 * NMI exception handler.

 * No lock; only written during early bootup by CPU 0.

	/*

	 * Only the TLB handlers are cache aligned with an even

	 * address. All other handlers are on an odd address and

	 * require no modification. Otherwise, MIPS32 mode will

	 * be entered when handling any TLB exceptions. That

	 * would be bad...since we must stay in microMIPS mode.

 SRSMap is only defined if shadow sets are implemented */

		/*

		 * If no shadow set is selected then use the default handler

		 * that does normal register saving and standard interrupt exit

			/*

			 * Sigh... panicing won't help as the console

			 * is probably not configured :(

		/*

		 * In other cases jump directly to the interrupt handler. It

		 * is the handler's responsibility to save registers if required

		 * (eg hi/lo) and return from the exception using "eret".

 j handler */

/*

 * Timer interrupt

/*

 * Performance counter IRQ or -1 if shared with timer

/*

 * Fast debug channel IRQ or -1 if not present

 configure STATUS register */

	/*

	 * Disable coprocessors and select 32-bit or 64-bit addressing

	 * and the 16/32 or 32/32 FPR register model.  Reset the BEV

	 * flag that some firmware may have left set and the TS bit (for

	 * IP27).  Set XX for ISA IV code to work.

 configure HWRENA register */

 If available, use WG to set top bits of EBASE */

 Setting vector spacing enables EI/VI mode  */

	/*

	 * Before R2 both interrupt numbers were fixed to 7, so on R2 only:

	 *

	 *  o read IntCtl.IPTI to determine the timer interrupt

	 *  o read IntCtl.IPPCI to determine the performance counter interrupt

	 *  o read IntCtl.IPFDC to determine the fast debug channel interrupt

 Boot CPU's cache setup in setup_arch(). */

 Install CPU exception handler */

/*

 * Install uncached CPU exception handler.

 * This is suitable only for the cache error exception which is the only

 * exception handler that is being run uncached.

		/*

		 * Try to ensure ebase resides in KSeg0 if possible.

		 *

		 * It shouldn't generally be in XKPhys on MIPS64 to avoid

		 * hitting a poorly defined exception base for Cache Errors.

		 * The allocation is likely to be in the low 512MB of physical,

		 * in which case we should be able to convert to KSeg0.

		 *

		 * EVA is special though as it allows segments to be rearranged

		 * and to become uncached during cache error handling.

	/*

	 * Copy the generic exception handlers to their final destination.

	 * This will be overridden later as suitable for a particular

	 * configuration.

	/*

	 * Setup default vectors

	/*

	 * Copy the EJTAG debug exception vector handler code to it's final

	 * destination.

	/*

	 * Only some CPUs have the watch exceptions.

	/*

	 * Initialise interrupt handlers

	/*

	 * Some CPUs can enable/disable for cache parity detection, but does

	 * it different ways.

	/*

	 * The Data Bus Errors / Instruction Bus Errors are signaled

	 * by external hardware.  Therefore these two exceptions

	 * may have board specific handlers.

 Special exception: R4[04]00 uses also the divec space. */

 Run last  */

 Restore register with CPU number for TLB handlers */

 SPDX-License-Identifier: GPL-2.0

/*

 * MIPS specific sysrq operations.

 *

 * Copyright (C) 2015 Imagination Technologies Ltd.

/*

 * Dump TLB entries on all CPUs.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995 Linus Torvalds

 * Copyright (C) 1995 Waldorf Electronics

 * Copyright (C) 1994, 95, 96, 97, 98, 99, 2000, 01, 02, 03  Ralf Baechle

 * Copyright (C) 1996 Stoned Elipot

 * Copyright (C) 1999 Silicon Graphics, Inc.

 * Copyright (C) 2000, 2001, 2002, 2007	 Maciej W. Rozycki

 CONFIG_MIPS_ELF_APPENDED_DTB */

/*

 * Setup information

 *

 * These are initialized so they are in the .data section

/*

 * mips_io_port_base is the begin of the address space to which x86 style

 * I/O ports are mapped.

/*

 * Manage initrd

 Guess if the sign extension was forgotten by bootloader */

 it returns the next free pfn after initrd */

	/*

	 * Board specific code or command line parser should have

	 * already set up initrd_start and initrd_end. In these cases

	 * perfom sanity checks and use them if all looks good.

	/*

	 * Sanitize initrd addresses. For example firmware

	 * can't guess if they need to pass them through

	 * 64-bits values if the kernel has been built in pure

	 * 32-bit. We need also to switch from KSEG0 to XKPHYS

	 * addresses now, so the code can now safely use __pa().

/* In some conditions (e.g. big endian bootloader with a little endian

   kernel), the initrd might appear byte swapped.  Try to detect this and

 Check for CPIO signature */

 Check for compressed initrd */

 Try again with a byte swapped header */

 !CONFIG_BLK_DEV_INITRD */

/*

 * Initialize the bootmem allocator. It also setup initrd related data

 * if needed.

 !CONFIG_SGI_IP27 */

	/*

	 * Sanity check any INITRD first. We don't take it into account

	 * for bootmem setup initially, rely on the end-of-kernel-code

	 * as our memory range starting point. Once bootmem is inited we

	 * will reserve the area used for the initrd.

 Reserve memory occupied by kernel. */

 max_low_pfn is not a number of pages but the end pfn of low mem */

	/*

	 * Reserve any memory between the start of RAM and PHYS_OFFSET

		/*

		 * Skip highmem here so we get an accurate max_low_pfn if low

		 * memory stops short of high memory.

		 * If the region overlaps HIGHMEM_START, end is clipped so

		 * max_pfn excludes the highmem portion.

	/*

	 * Reserve initrd memory if needed.

 CONFIG_SGI_IP27 */

	/*

	 * If a user specifies memory size, we

	 * blow away any automatically generated

	 * size.

				/*

				 * Reserve from the elf core header to the end of

				 * the memory segment, that should all be kdump

				 * reserved memory.

 64M alignment for crash kernel regions */

 !defined(CONFIG_KEXEC)		*/

 !defined(CONFIG_KEXEC)  */

 CONFIG_OF_EARLY_FLATTREE */

	/*

	 * If CMDLINE_OVERRIDE is enabled then initializing the command line is

	 * trivial - we simply use the built-in command line unconditionally &

	 * unmodified.

	/*

	 * If the user specified a built-in command line &

	 * MIPS_CMDLINE_BUILTIN_EXTEND, then the built-in command line is

	 * prepended to arguments from the bootloader or DT so we'll copy them

	 * to the start of boot_command_line here. Otherwise, empty

	 * boot_command_line to undo anything early_init_dt_scan_chosen() did.

	/*

	 * If we're configured to take boot arguments from DT, look for those

	 * now.

	/*

	 * If we didn't get any arguments from DT (regardless of whether that's

	 * because we weren't configured to look for them, or because we looked

	 * & found none) then we'll take arguments from the bootloader.

	 * plat_mem_setup() should have filled arcs_cmdline with arguments from

	 * the bootloader.

	/*

	 * If the user specified a built-in command line & we didn't already

	 * prepend it, we append it to boot_command_line here.

/*

 * arch_mem_init - initialize memory management subsystem

 *

 *  o plat_mem_setup() detects the memory configuration and will record detected

 *    memory areas using memblock_add.

 *

 * At this stage the memory configuration of the system is known to the

 * kernel but generic memory management system is still entirely uninitialized.

 *

 *  o bootmem_init()

 *  o sparse_init()

 *  o paging_init()

 *  o dma_contiguous_reserve()

 *

 * At this stage the bootmem allocator is ready to use.

 *

 * NOTE: historically plat_mem_setup did the entire platform initialization.

 *	 This was rather impractical because it meant plat_mem_setup had to

 * get away without any kind of memory allocator.  To keep old code from

 * breaking plat_setup was just renamed to plat_mem_setup and a second platform

 * initialization hook for anything else was introduced.

 call board setup routine */

	/*

	 * Prevent memblock from allocating high memory.

	 * This cannot be done before max_low_pfn is detected, so up

	 * to this point is possible to only reserve physical memory

	 * with memblock_reserve; memblock_alloc* can be used

	 * only after this point

	/*

	 * In order to reduce the possibility of kernel panic when failed to

	 * get IO TLB memory under CONFIG_SWIOTLB, it is better to allocate

	 * low memory as small as possible before plat_swiotlb_setup(), so

	 * make sparse_init() using top-down allocation.

 Reserve for hibernation. */

		/*

		 * In memblock, end points to the first byte after the

		 * range while in resourses, end points to the last byte in

		 * the range.

		/*

		 *  We don't know which RAM region contains kernel data,

		 *  so we try it repeatedly and let the resource manager

		 *  test it.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

 initialise the wait queues */

 set up notifiers */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Processor capabilities determination functions.

 *

 * Copyright (C) xxxx  the Anonymous

 * Copyright (C) 1994 - 2006 Ralf Baechle

 * Copyright (C) 2003, 2004  Maciej W. Rozycki

 * Copyright (C) 2001, 2004, 2011, 2012	 MIPS Technologies, Inc.

 Hardware capabilities */

/*

 * Probe whether cpu has config register by trying to play with

 * alternate cache bit and see whether it matters.

 * It's used by cpu_probe to distinguish between R3000A and R3081.

	/*

	 * Set a default elf platform, cpu probe may later

	 * overwrite it with a more precise value

	/*

	 * Platform code can force the cpu type to optimize code

	 * generation. In that case be sure the cpu type is correctly

	 * manually setup otherwise it could trigger some nasty bugs.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MIPS support for CONFIG_OF device tree support

 *

 * Copyright (C) 2010 Cisco Systems Inc. <dediao@cisco.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004, 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

 The number of TCs and VPEs physically available on the core */

 We are prepared so configure and start the VPE... */

 check we are the Master VPE */

 Put MVPE's into 'configuration state' */

 should check it is halted, and not activated */

	/*

	 * Write the address we want it to start running from in the TCPC

	 * register.

	/*

	 * Mark the TC as activated, not interrupt exempt and not dynamically

	 * allocatable

	/*

	 * The sde-kit passes 'memsize' to __start in $a3, so set something

	 * here...  Or set $a3 to zero and define DFLT_STACK_SIZE and

	 * DFLT_HEAP_SIZE when you compile your program

 set up VPE1 */

	/*

	 * bind the TC to VPE 1 as late as possible so we only have the final

	 * VPE registers to set up, and so an EJTAG probe can trigger on it

 Set up the XTC bit in vpeconf0 to point at our tc */

 enable this VPE */

 clear out any left overs from a previous program */

 take system out of configuration state */

	/*

	 * SMVP kernels manage VPE enable independently, but uniprocessor

	 * kernels need to turn it on, even if that wasn't the pre-dvpe() state.

 Put MVPE's into 'configuration state' */

 mark not allocated and not dynamically allocatable */

 interrupt exempt */

 module wrapper entry points */

 give me a vpe */

 find a vpe */

 start running from here */

 halt it for now */

 I've done with it thank you */

 Put MVPE's into 'configuration state' */

 halt the TC */

 mark the TC unallocated */

 Put MVPE's into 'configuration state' */

		/*

		 * Must re-enable multithreading temporarily or in case we

		 * reschedule send IPIs or similar we might hang.

 VPE's */

 add the tc to the list of this vpe's tc's. */

 deactivate all but vpe0 */

 master VPE */

 disable multi-threading with TC's */

				/*

				 * Set config to be the same as vpe0,

				 * particularly kseg0 coherency alg

 TC's */

 set the parent vpe */

			/*

			 * A TC that is bound to any other VPE gets bound to

			 * VPE0, ideally I'd like to make it homeless but it

			 * doesn't appear to let me bind a TC to a non-existent

			 * VPE. Which is perfectly reasonable.

			 *

			 * The (un)bound state is visible to an EJTAG probe so

			 * may notify GDB...

 tc is bound >vpe0 */

 set the parent vpe */

 halt the TC */

 mark not activated and not dynamically allocatable */

 interrupt exempt */

 release config state */

 No locking needed here */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Support for Kernel relocation at boot time

 *

 * Copyright (C) 2015, Imagination Technologies Ltd.

 * Authors: Matt Redfearn (matt.redfearn@mips.com)

 End kernel image / start relocation table */

 End relocation table */

 Start exception table */

 End exception table */

/*

 * This function may be defined for a platform to perform any post-relocation

 * fixup necessary.

 * Return non-zero to abort relocation

 no output */

 Completion barrier */

 Original target address */

 Get the new target address */

 high 16bits of target */

 Sentinel for last relocation */

/*

 * The exception table is filled in by the relocs tool after vmlinux is linked.

 * It must be relocated separately since there will not be any relocation

 * information for it filled in by the linker.

 Rotate by odd number of bits and XOR. */

 Attempt to create a simple but unpredictable starting entropy. */

 Add in any runtime entropy we can get */

 Get any additional entropy passed in device tree */

 CONFIG_USE_OF */

 Choose a new address for the kernel */

	/*

	 * Choose a new address for the kernel

	 * For now we'll hard code the destination

 Inappropriately aligned new location */

 New location overlaps original kernel */

 Default to original kernel entry point */

 Get the command line */

 Deal with the device tree */

 Boot command line was passed in device tree */

 CONFIG_USE_OF */

 Sanity check relocation address */

 Reset the command line now so we don't end up with a duplicate */

		/*

		 * If built-in dtb is used then it will have been relocated

		 * during kernel _text relocation. If appended DTB is used

		 * then it will not be relocated, but it should remain

		 * intact in the original location. If dtb is loaded by

		 * the bootloader then it may need to be moved if it crosses

		 * the target memory area

 CONFIG_USE_OF */

 Copy the kernel to it's new location */

 Perform relocations on the new kernel */

 Sync the caches ready for execution of new kernel */

		/*

		 * The original .bss has already been cleared, and

		 * some variables such as command line parameters

		 * stored to it so make a copy in the new location.

		/*

		 * If fdt was stored outside of the kernel image and

		 * had to be moved then update platform's state data

		 * with the new fdt location

		/*

		 * Last chance for the platform to abort relocation.

		 * This may also be used by the platform to perform any

		 * initialisation required now that the new kernel is

		 * resident in memory and ready to be executed.

 The current thread is now within the relocated image */

 Return the new kernel's entry point */

 Error may occur before, so keep it at last */

/*

 * Show relocation information on panic.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 MIPS Technologies, Inc.

 * Copyright (C) 2007 Ralf Baechle <ralf@linux-mips.org>

/**

 * calculate_min_delta() - Calculate a good minimum delta for mips_next_event().

 *

 * Running under virtualisation can introduce overhead into mips_next_event() in

 * the form of hypervisor emulation of CP0_Count/CP0_Compare registers,

 * potentially with an unnatural frequency, which makes a fixed min_delta_ns

 * value inappropriate as it may be too small.

 *

 * It can also introduce occasional latency from the guest being descheduled.

 *

 * This function calculates a good minimum delta based roughly on the 75th

 * percentile of the time taken to do the mips_next_event() sequence, in order

 * to handle potentially higher overhead while also eliminating outliers due to

 * unpredictable hypervisor latency (which can be handled by retries).

 *

 * Return:	An appropriate minimum delta for the clock event device.

	/*

	 * Calculate the median of 5 75th percentiles of 5 samples of how long

	 * it takes to set CP0_Compare = CP0_Count + delta.

			/*

			 * This is like the code in mips_next_event(), and

			 * directly measures the borderline "safe" delta.

 Sorted insert into buf1 */

 Sorted insert of 75th percentile into buf2 */

 Use 2 * median of 75th percentiles */

 Don't go too low */

/*

 * Possibly handle a performance counter interrupt.

 * Return true if the timer interrupt should not be checked

	/*

	 * The performance counter overflow interrupt may be shared with the

	 * timer interrupt (cp0_perfcount_irq < 0). If it is and a

	 * performance counter has overflowed (perf_irq() == IRQ_HANDLED)

	 * and we can't reliably determine if a counter interrupt has also

	 * happened (!r2) then don't check for a timer interrupt.

	/*

	 * Suckage alert:

	 * Before R2 of the architecture there was no way to see if a

	 * performance counter interrupt was pending, so we have to run

	 * the performance counter interrupt handler anyway.

	/*

	 * The same applies to performance counter interrupts.	But with the

	 * above we now know that the reason we got here must be a timer

	 * interrupt.  Being the paranoiacs we are we check anyway.

 Clear Count/Compare Interrupt */

	/*

	 * IRQF_SHARED: The timer interrupt may be shared with other interrupts

	 * such as perf counter and FDC interrupts.

/*

 * FIXME: This doesn't hold for the relocated E9000 compare interrupt.

 When cpu_has_mips_r2, this checks Cause.TI instead of Cause.IP7 */

/*

 * Compare interrupt can be routed and latched outside the core,

 * so wait up to worst case number of cycle counter ticks for timer interrupt

 * changes to propagate to the cause register.

	/*

	 * IP7 already pending?	 Try to clear it by acking the timer.

 increase delta if the timer was already expired */

 Wait for expiry  */

	/*

	 * Feels like a real count / compare timer.

 !CONFIG_CPU_FREQ */

	/*

	 * With vectored interrupts things are getting platform specific.

	 * get_c0_compare_int is a hook to allow a platform to return the

	 * interrupt number of its liking.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004, 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

 *

 * VPE spport module for loading a MIPS SP program into VPE1. The SP

 * environment is rather simple since there are no TLBs. It needs

 * to be relocatable (or partiall linked). Initialize your stack in

 * the startup-code. The loader looks for the symbol __start and sets

 * up the execution to resume from there. To load and run, simply do

 * a cat SP 'binary' to the /dev/vpe1 device.

 If this is set, the section belongs in the init part of the module */

 get the vpe associated with this minor */

 get the vpe associated with this minor */

 allocate a vpe and associate it with this minor (or index) */

 allocate a tc. At startup only tc0 is running, all other can be halted. */

 clean up and free everything */

 Find some VPE program space */

	/*

	 * This means you must tell Linux to use less memory than you

	 * physically have, for example by passing a mem= boot argument.

 simple grab some mem for now */

 Update size with this section: return offset. */

/* Lay out the SHF_ALLOC sections in a way not dissimilar to how ld

   might -- code, read-only data, read-write data, small data.	Tally

   sizes, and place the offsets into sh_entsize fields: high bit means it

		/* NOTE: all executable code must be the first section

		 * in this array; otherwise modify the text_size

 from module-elf32.c, but subverted a little */

 .sbss + gp(relative) + offset */

 kludge! */

 because the offset is in _instructions_ not bytes. */

 and one instruction less due to the branch delay slot. */

/*

 * Not desperately convinced this is a good check of an overflow condition

 * anyway. But it gets in the way of handling undefined weak symbols which

 * we want to set to zero.

 * if ((v & 0xf0000000) != (((unsigned long)location + 4) & 0xf0000000)) {

 * printk(KERN_ERR

 * "module %s: relocation overflow\n",

 * me->name);

 * return -ENOEXEC;

 * }

	/*

	 * We cannot relocate this one now because we don't know the value of

	 * the carry we need to add.  Save the information, and let LO16 do the

	 * actual relocation.

 Sign extend the addend we extract from the lo insn.	*/

			/*

			 * The value for the HI16 had best be the same.

			/*

			 * Do the HI16 relocation.  Note that we actually don't

			 * need to know anything about the LO16 itself, except

			 * where to find the low 16 bits of the addend needed

			 * by the LO16.

			/*

			 * Account for the sign extension that will happen in

			 * the low bits.

	/*

	 * Ok, we're done with the HI16 relocs.	 Now deal with the LO16.

 This is where to make the change */

 This is the symbol it is referring to */

 just print the warning, dont barf */

 end module-elf32.c */

 Change all symbols so that sh_value encodes the pointer directly. */

 find the .bss section for COMMON symbols */

			/* Allocate space for the symbol in the .bss section.

			   st_value is currently size.

 Don't need to do anything */

 ret = -ENOENT; */

 .sbss section */

/*

 * Allocates a VPE with some program code space(the load address), copies the

 * contents of the program (p)buffer performing relocatations/etc, free's it

 * when finished.

 so we can re-use the relocations code */

	/* Sanity checks against insmoding binaries or wrong arch,

 Convenience variables */

 And these should exist, but gcc whinges if we don't init them */

			/* Mark all sections sh_addr with their address in the

 Internal symbols and strings. */

 Update sh_addr to point to copy in image. */

 Fix up syms, so that st_value is a pointer to location. */

 Now do relocations. */

 Not a valid relocation section? */

 Don't bother with non-allocated sections */

 Internal symbols and strings. */

				/*

				 * mark symtab's address for when we try

				 * to find the magic symbols

 make sure it's physically written out */

 checks VPE is unused and gets ready to load program	*/

 assume only 1 device at the moment. */

 this of-course trashes what was there before... */

	/* It's good to be able to run the SP and if it chokes have a look at

	   the /dev/rt?. But if we reset the pointer to the shared struct we

	   lose what has happened. So perhaps if garbage is sent to the vpe

	   device, use it as a trigger for the reset. Hopefully a nice

 SPDX-License-Identifier: GPL-2.0-only

/*

 * MIPS cacheinfo support

 Populates leaf and increments to next leaf */

	/*

	 * If Dcache is not set, we assume the cache structures

	 * are not properly initialized.

 I/D caches are per core */

 Vcache is per core as well */

 Scache is per cluster */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: Jun Sun, jsun@mvista.com or jsun@junsun.net

 * Copyright (c) 2003, 2004  Maciej W. Rozycki

 *

 * Common time service routines for MIPS machines.

	/*

	 * Skip lpj numbers adjustment if the CPU-freq transition is safe for

	 * the loops delay. (Is this possible?)

 Save the initial values of the lpjes for future scaling. */

	/*

	 * Adjust global lpj variable and per-CPU udelay_val number in

	 * accordance with the new CPU frequency.

 CONFIG_CPU_FREQ */

/*

 * forward reference

/*

 * time_init() - it does the following things.

 *

 * 1) plat_time_init() -

 *	a) (optional) set up RTC routines,

 *	b) (optional) calibrate and set the mips_hpt_frequency

 *	    (only needed if you intended to use cpu counter as timer interrupt

 *	     source)

 * 2) calculate a couple of cached variables for later usage

		/*

		 * V3.0 is documented as suffering from the mfc0 from count bug.

		 * Afaik this is the last version of the R4000.	 Later versions

		 * were marketed as R4400.

		/*

		 * The published errata for the R4400 up to 3.0 say the CPU

		 * has the mfc0 from count bug.

		/*

		 * we assume newer revisions are ok

	/*

	 * The use of the R4k timer as a clock event takes precedence;

	 * if reading the Count register might interfere with the timer

	 * interrupt, then we don't use the timer as a clock source.

	 * We may still use the timer as a clock source though if the

	 * timer interrupt isn't reliable; the interference doesn't

	 * matter then, because we don't use the interrupt.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001 Broadcom Corporation

 max value */

/*

 * The HPT is free running from SB1250_HPT_VALUE down to 0 then starts over

 * again.

 Setup hpt using timer #3 but do not enable irq for it */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MIPS idle loop and WAIT instruction support.

 *

 * Copyright (C) xxxx  the Anonymous

 * Copyright (C) 1994 - 2006 Ralf Baechle

 * Copyright (C) 2003, 2004  Maciej W. Rozycki

 * Copyright (C) 2001, 2004, 2011, 2012	 MIPS Technologies, Inc.

/*

 * Not all of the MIPS CPUs have the "wait" instruction available. Moreover,

 * the implementation of the "wait" feature differs between CPU families. This

 * points to the function that implements CPU specific wait.

 * The wait instruction stops the pipeline and reduces the power consumption of

 * the CPU very much.

/*

 * This variant is preferable as it allows testing need_resched and going to

 * sleep depending on the outcome atomically.  Unfortunately the "It is

 * implementation-dependent whether the pipeline restarts when a non-enabled

 * interrupt is requested" restriction in the MIPS32/MIPS64 architecture makes

 * using this version a gamble.

/*

 * The RM7000 variant has to handle erratum 38.	 The workaround is to not

 * have any pending stores when the WAIT instruction is executed.

/*

 * Au1 'wait' is only useful when the 32kHz counter is used as timer,

 * since coreclock (and the cp0 counter) stops upon executing it. Only an

 * interrupt can wake it, so they must be enabled before entering idle modes.

 irqs on */

 wr c0status */

	/*

	 * MIPSr6 specifies that masked interrupts should unblock an executing

	 * wait instruction, and thus that it is safe for us to use

	 * r4k_wait_irqoff. Yippee!

	case CPU_R4300: */

		/*

		 * Incoming Fast Debug Channel (FDC) data during a wait

		 * instruction causes the wait never to resume, even if an

		 * interrupt is received. Avoid using wait at all if FDC data is

		 * likely to be received.

		/*

		 * WAIT on Rev1.0 has E1, E2, E3 and E16.

		 * WAIT on Rev2.0 and Rev3.0 has E16.

		 * Rev3.1 WAIT is nop, why bother

		/*

		 * Another rev is incremeting c0_count at a reduced clock

		 * rate while in WAIT mode.  So we basically have the choice

		 * between using the cp0 timer as clocksource or avoiding

		 * the WAIT instruction.  Until more details are known,

		 * disable the use of WAIT for 20Kc entirely.

		   cpu_wait = r4k_wait;

 SPDX-License-Identifier: GPL-2.0

/*

 * i8253.c  8253/PIT functions

 *

 PIT does not scale! */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2013 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

/**

 * mips_cpc_phys_base - retrieve the physical base address of the CPC

 *

 * This function returns the physical base address of the Cluster Power

 * Controller memory mapped registers, or 0 if no Cluster Power Controller

 * is present.

 If the CPC is already enabled, leave it so */

 Otherwise, use the default address */

 Enable the CPC, mapped at the default address */

 Systems with CM >= 3 lock the CPC via mips_cm_lock_other */

	/*

	 * Ensure the core-other region reflects the appropriate core &

	 * VP before any accesses to it occur.

 Systems with CM >= 3 lock the CPC via mips_cm_lock_other */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 97, 2000, 2001 by Ralf Baechle

 * Copyright (C) 2001 MIPS Technologies, Inc.

/*

 * Calculate and return exception PC in case of branch delay slot

 * for microMIPS and MIPS16e. It does not clear the ISA mode bit.

 Calculate exception PC in branch delay slot. */

 This should never happen because delay slot was checked. */

 (microMIPS) Convert 16-bit register encoding to 32-bit register encoding. */

 Not mm_jr */

 CONFIG_MIPS_FP_SUPPORT */

/*

 * Compute return address and emulate branch in microMIPS mode after an

 * exception only. It does not handle compact branches/jumps and cannot

 * be used in interrupt context. (Compact branches/jumps do not cause

 * exceptions.)

 This load never faults. */

/*

 * Compute return address and emulate branch in MIPS16e mode after an

 * exception only. It does not handle compact branches/jumps and cannot

 * be used in interrupt context. (Compact branches/jumps do not cause

 * exceptions.)

 Read the instruction. */

		/*

		 *  JAL and JALX in MIPS16e mode

		/*

		 * JAL:5 X:1 TARGET[20-16]:5 TARGET[25:21]:5 TARGET[15:0]:16

		 *

		 * ......TARGET[15:0].................TARGET[20:16]...........

		 * ......TARGET[25:21]

 Set ISA mode bit. */

		/*

		 *  J(AL)R(C)

	/*

	 * All other cases have no branch delay slot and are 16-bits.

	 * Branches do not cause an exception.

/**

 * __compute_return_epc_for_insn - Computes the return address and do emulate

 *				    branch simulation, if required.

 *

 * @regs:	Pointer to pt_regs

 * @insn:	branch instruction to decode

 * Return:	-EFAULT on error and forces SIGILL, and on success

 *		returns 0 or BRANCH_LIKELY_TAKEN as appropriate after

 *		evaluating the branch.

 *

 * MIPS R6 Compact branches and forbidden slots:

 *	Compact branches do not throw exceptions because they do

 *	not have delay slots. The forbidden slot instruction ($PC+4)

 *	is only executed if the branch was not taken. Otherwise the

 *	forbidden slot is skipped entirely. This means that the

 *	only possible reason to be here because of a MIPS R6 compact

 *	branch instruction is that the forbidden slot has thrown one.

 *	In that case the branch was not taken, so the EPC can be safely

 *	set to EPC + 8.

	/*

	 * jr and jalr are in r_format format.

	/*

	 * This group contains:

	 * bltz_op, bgez_op, bltzl_op, bgezl_op,

	 * bltzal_op, bgezal_op, bltzall_op, bgezall_op.

			/*

			 * OK we are here either because we hit a NAL

			 * instruction or because we are emulating an

			 * old bltzal{,l} one. Let's figure out what the

			 * case really is.

				/*

				 * NAL or BLTZAL with rs == 0

				 * Doesn't matter if we are R6 or not. The

				 * result is the same

 Now do the real thing for non-R6 BLTZAL{,L} */

			/*

			 * OK we are here either because we hit a BAL

			 * instruction or because we are emulating an

			 * old bgezal{,l} one. Let's figure out what the

			 * case really is.

				/*

				 * BAL or BGEZAL with rs == 0

				 * Doesn't matter if we are R6 or not. The

				 * result is the same

 Now do the real thing for non-R6 BGEZAL{,L} */

	/*

	 * These are unconditional and in j_format.

	/*

	 * These are conditional and in i_format.

 not really i_format */

		/*

		 * Compact branches for R6 for the

		 * blez and blezl opcodes.

		 * BLEZ  | rs = 0 | rt != 0  == BLEZALC

		 * BLEZ  | rs = rt != 0      == BGEZALC

		 * BLEZ  | rs != 0 | rt != 0 == BGEUC

		 * BLEZL | rs = 0 | rt != 0  == BLEZC

		 * BLEZL | rs = rt != 0      == BGEZC

		 * BLEZL | rs != 0 | rt != 0 == BGEC

		 *

		 * For real BLEZ{,L}, rt is always 0.

 rt field assumed to be zero */

		/*

		 * Compact branches for R6 for the

		 * bgtz and bgtzl opcodes.

		 * BGTZ  | rs = 0 | rt != 0  == BGTZALC

		 * BGTZ  | rs = rt != 0      == BLTZALC

		 * BGTZ  | rs != 0 | rt != 0 == BLTUC

		 * BGTZL | rs = 0 | rt != 0  == BGTZC

		 * BGTZL | rs = rt != 0      == BLTZC

		 * BGTZL | rs != 0 | rt != 0 == BLTC

		 *

		 * *ZALC varint for BGTZ &&& rt != 0

		 * For real GTZ{,L}, rt is always 0.

 rt field assumed to be zero */

	/*

	 * And now the FPA/cp1 branch instructions.

 bc1f */

 bc1fl */

 bc1t */

 bc1tl */

 CONFIG_MIPS_FP_SUPPORT */

 This is bbit0 on Octeon */

 This is bbit032 on Octeon */

 This is bbit1 on Octeon */

 This is bbit132 on Octeon */

 Only valid for MIPS R6 */

 Compact branch: BALC */

 Compact branch: BEQZC || JIC */

 Compact branch: BNEZC || JIALC */

 JIALC: set $31/ra */

 Only valid for MIPS R6 */

		/*

		 * Compact branches:

		 * bovc, beqc, beqzalc, bnvc, bnec, bnezlac

	/*

	 * Read the instruction

		/*

		 * blez[l] and bgtz[l] opcodes with non-zero rt

		 * are MIPS R6 compact branches

 CONFIG_KPROBES || CONFIG_UPROBES */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 David Daney

/*

 * Install the watch registers for the current thread.	A maximum of

 * four registers are installed although the machine may have more.

 Trap all ASIDs */

 Clear result bits */

/*

 * Read back the watchhi registers so the user space debugger has

 * access to the I, R, and W bits.  A maximum of four registers are

 * read although the machine may have more.

		/* Pathological case of release 1 architecture that

		 * doesn't set the condition bits.  We assume that

		 * since we got here, the watch condition was met and

		 * signal that the conditions requested in watchlo

/*

 * Disable all watch registers.	 Although only four registers are

 * installed, all are cleared to eliminate the possibility of endless

 * looping in the watch handler.

	/*

	 * Check which of the I,R and W bits are supported, then

	 * disable the register.

	/* Write the mask bits and read them back to determine which

 We use at most 4, but probe and report up to 8. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * machine_kexec.c for kexec

 * Created by <nschichan@corp.free.fr> on Thu Oct 12 15:15:06 2006

	/*

	 * In case DTB file is not passed to the new kernel, a flat device

	 * tree will be created by kexec tool. It holds modified command

	 * line for the new kernel.

 CONFIG_UHI_BOOT */

 We won't be sent IPIs any more. */

 NOTREACHED */

	/*

	 * We know we were online, and there will be no incoming IPIs at

	 * this point. Mark online again before rebooting so that the crash

	 * analysis tool will see us correctly.

 Ensure remote CPUs observe that we're online before rebooting. */

		/*

		 * Instead of cpu_relax() or wait, this is needed for kexec

		 * smp reboot. Kdump usually doesn't require an smp new

		 * kernel, but kexec may do.

 NOTREACHED */

	/*

	 * Make sure we get correct instructions written by the

	 * machine_kexec() CPU.

	/*

	 * The generic kexec code builds a page list with physical

	 * addresses. they are directly accessible through KSEG0 (or

	 * CKSEG0 or XPHYS if on 64bit system), hence the

	 * phys_to_virt() call.

 Mark offline BEFORE disabling local irq. */

	/*

	 * we do not want to be bothered.

 Make reboot code buffer available to the boot CPU. */

 All secondary cpus now may jump to kexec_wait cycle */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2013 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 Detect & record VPE topology */

 Use the number of VPEs in cluster 0 core 0 for smp_num_siblings */

 Indicate present CPUs (CPU being synonymous with VPE) */

 Set a coherent default CCA (CWB) */

 Core 0 is powered up (we're running on it) */

 Initialise core 0 */

 Make core 0 coherent with everything */

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

 Detect whether the CCA is unsuited to multi-core SMP */

 CWBE */

 CWB */

 The CCA is coherent, multi-core is fine */

 CCA is not coherent, multi-core is not usable */

 Warn the user if the CCA prevents multi-core */

	/*

	 * Patch the start of mips_cps_core_entry to provide:

	 *

	 * s0 = kseg0 CCA

 Allocate core boot configuration structs */

 Allocate VPE boot configuration structs */

 Mark this CPU as booted */

 Clean up allocations */

 Effectively disable SMP by declaring CPUs not present */

 Select the appropriate core */

 Set its reset vector */

 Ensure its coherency is disabled */

 Start it with the legacy memory map and exception base */

 Ensure the core can access the GCRs */

 Reset the core */

 Run only the requested VP following the reset */

			/*

			 * Ensure that the VP_RUN register is written before the

			 * core leaves reset.

 U6 == coherent execution, ie. the core is up */

 Delay a little while before we start warning */

 Take the core out of reset */

 The core is now powered up */

 We don't yet support booting CPUs in other clusters */

 Boot a VPE on a powered down core */

 Boot a VPE on another powered up core */

 Boot a VPE on this core */

 Disable MT - we only want to run 1 TC per VPE */

		/*

		 * Ensure that our calculation of the VP ID matches up with

		 * what the GIC reports, otherwise we'll have configured

		 * interrupts incorrectly.

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

 Halt this TC */

 Ensure that the VP_STOP register is written */

 Power down the core */

 CONFIG_KEXEC */

 CONFIG_HOTPLUG_CPU || CONFIG_KEXEC */

 Look for another online VPE within the core */

			/*

			 * There is an online VPE within the core. Just halt

			 * this TC and leave the core alone.

 This CPU has chosen its way out */

 This should never be reached */

 Wait for the cpu to choose its way out */

	/*

	 * Now wait for the CPU to actually offline. Without doing this that

	 * offlining may race with one or more of:

	 *

	 *   - Onlining the CPU again.

	 *   - Powering down the core if another VPE within it is offlined.

	 *   - A sibling VPE entering a non-coherent state.

	 *

	 * In the non-MT halt case (ie. infinite loop) the CPU is doing nothing

	 * with which we could race, so do nothing.

		/*

		 * Wait for the core to enter a powered down or clock gated

		 * state, the latter happening when a JTAG probe is connected

		 * in which case the CPC will refuse to power down the core.

			/*

			 * The core ought to have powered down, but didn't &

			 * now we don't really know what state it's in. It's

			 * likely that its _pwr_up pin has been wired to logic

			 * 1 & it powered back up as soon as we powered it

			 * down...

			 *

			 * The best we can do is warn the user & continue in

			 * the hope that the core is doing nothing harmful &

			 * might behave properly if we online it later.

 Indicate the core is powered off */

		/*

		 * Have a CPU with access to the offlined CPUs registers wait

		 * for its TC to halt.

 CONFIG_HOTPLUG_CPU */

 check we have a GIC - we need one for IPIs */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2002, 2003, 06, 07 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2007 MIPS Technologies, Inc.

 *   written by Ralf Baechle (ralf@linux-mips.org)

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2013 Imagination Technologies Ltd.

 Segment access mode. */

	/*

	 * Access modes MK, MSK and MUSK are mapped segments. Therefore

	 * there is no direct physical address mapping unless it becomes

	 * unmapped uncached at error level due to EU.

 Exception configuration. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2017 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 Check that ptr is naturally aligned */

 Mask value to the correct size. */

	/*

	 * Calculate a shift & mask that correspond to the value we wish to

	 * exchange within the naturally aligned 4 byte integerthat includes

	 * it.

	/*

	 * Calculate a pointer to the naturally aligned 4 byte integer that

	 * includes our byte of interest, and load its value.

 Check that ptr is naturally aligned */

 Mask inputs to the correct size. */

	/*

	 * Calculate a shift & mask that correspond to the value we wish to

	 * compare & exchange within the naturally aligned 4 byte integer

	 * that includes it.

	/*

	 * Calculate a pointer to the naturally aligned 4 byte integer that

	 * includes our byte of interest, and load its value.

		/*

		 * Ensure the byte we want to exchange matches the expected

		 * old value, and if not then bail.

		/*

		 * Calculate the old & new values of the naturally aligned

		 * 4 byte integer that include the byte we want to exchange.

		 * Attempt to exchange the old value for the new value, and

		 * return if we succeed.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2007 MIPS Technologies, Inc.

 *    Chris Dearman (chris@mips.com)

 Assume GIC is present */

 Enable per-cpu interrupts: platform specific */

 CDFIXME: remove this? */

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

/*

 * Setup the PC, SP, and GP of a secondary processor and start it running

 * smp_bootstrap is the place to resume from

 * __KSTK_TOS(idle) is apparently the stack pointer

 * (unsigned long)idle->thread_info the gp

 Needed? */

/*

 * Common setup before any secondaries are started

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

	/*

	 * FIXME: some of these options are per-system, some per-core and

	 * some per-cpu

/*

 * Based on linux/arch/mips/jmr3927/rbhma3100/irq.c,

 *	    linux/arch/mips/tx4927/common/tx4927_irq.c,

 *	    linux/arch/mips/tx4938/common/irq.c

 *

 * Copyright 2001, 2003-2005 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *	   ahennessy@mvista.com

 *	   source@mvista.com

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 IRCER : Int. Control Enable */

 IRCR : Int. Control */

 IRSCR : Int. Status Control */

 IRCSR : Int. Current Status */

 update IRCSR */

 update IRCSR */

 flush write buffer */

 clear edge detection */

 middle level */

 mask all IRC interrupts */

 setup IRC interrupt mode (Low Active) */

 enable interrupt control */

 SPDX-License-Identifier: GPL-2.0

 CONFIG_NUMA */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 2014 Imagination Technologies Ltd.

 * Author: Leonid Yegoshin <Leonid.Yegoshin@imgtec.com>

 * Author: Markos Chandras <markos.chandras@imgtec.com>

 *

 *      MIPS R2 user space instruction emulator for MIPS R6

 *

 CONFIG_64BIT */

/**

 * mipsr6_emul - Emulate some frequent R2/R5/R6 instructions in delay slot

 * for performance instead of the traditional way of using a stack trampoline

 * which is rather slow.

 * @regs: Process register set

 * @ir: Instruction

 FPU instructions in delay slot */

/**

 * movf_func - Emulate a MOVF instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * movt_func - Emulate a MOVT instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * jr_func - Emulate a JR instruction.

 * @pt_regs: Process register set

 * @ir: Instruction

 *

 * Returns SIGILL if JR was in delay slot, SIGEMT if we

 * can't compute the EPC, SIGSEGV if we can't access the

 * userland instruction or 0 on success.

 EPC after the RI/JR instruction */

 Roll back to the reserved R2 JR instruction */

 Computed EPC */

 Get DS instruction */

 If nir == 0(NOP), then nothing else to do */

		/*

		 * Negative err means FPU instruction in BD-slot,

		 * Zero err means 'BD-slot emulation done'

		 * For anything else we go back to trampoline emulation.

/**

 * movz_func - Emulate a MOVZ instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * movn_func - Emulate a MOVZ instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mfhi_func - Emulate a MFHI instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mthi_func - Emulate a MTHI instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mflo_func - Emulate a MFLO instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mtlo_func - Emulate a MTLO instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mult_func - Emulate a MULT instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * multu_func - Emulate a MULTU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * div_func - Emulate a DIV instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * divu_func - Emulate a DIVU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * dmult_func - Emulate a DMULT instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 on success or SIGILL for 32-bit kernels.

/**

 * dmultu_func - Emulate a DMULTU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 on success or SIGILL for 32-bit kernels.

/**

 * ddiv_func - Emulate a DDIV instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 on success or SIGILL for 32-bit kernels.

/**

 * ddivu_func - Emulate a DDIVU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 on success or SIGILL for 32-bit kernels.

 R6 removed instructions for the SPECIAL opcode */

/**

 * madd_func - Emulate a MADD instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * maddu_func - Emulate a MADDU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * msub_func - Emulate a MSUB instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * msubu_func - Emulate a MSUBU instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * mul_func - Emulate a MUL instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * clz_func - Emulate a CLZ instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * clo_func - Emulate a CLO instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * dclz_func - Emulate a DCLZ instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

/**

 * dclo_func - Emulate a DCLO instruction

 * @regs: Process register set

 * @ir: Instruction

 *

 * Returns 0 since it always succeeds.

 R6 removed instructions for the SPECIAL2 opcode */

/**

 * mipsr2_decoder: Decode and emulate a MIPS R2 instruction

 * @regs: Process register set

 * @inst: Instruction to decode and emulate

 * @fcr31: Floating Point Control and Status Register Cause bits returned

 FPU instruction under JR */

			/*

			 * This will probably be optimized away when

			 * CONFIG_DEBUG_FS is not enabled

			/*

			 * This will probably be optimized away when

			 * CONFIG_DEBUG_FS is not enabled

		/*

		 * For BLEZL and BGTZL, rt field must be set to 0. If this

		 * is not the case, this may be an encoding of a MIPS R6

		 * instruction, so return to CPU execution if this occurs

		/*

		 * This will probably be optimized away when

		 * CONFIG_DEBUG_FS is not enabled

		/*

		 * We can't allow the emulated instruction to leave any

		 * enabled Cause bits set in $fcr31.

		/*

		 * this is a tricky issue - lose_fpu() uses LL/SC atomics

		 * if FPU is owned and effectively cancels user level LL/SC.

		 * So, it could be logical to don't restore FPU ownership here.

		 * But the sequence of multiple FPU instructions is much much

		 * more often than LL-FPU-SC and I prefer loop here until

		 * next scheduler cycle cancels FPU ownership

 Restore FPU state. */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

 !CONFIG_CPU_LITTLE_ENDIAN */

 CONFIG_CPU_LITTLE_ENDIAN */

			/*

			 * An LL/SC block can't be safely emulated without

			 * a Config5/LLB availability. So it's probably time to

			 * kill our process before things get any worse. This is

			 * because Config5/LLB allows us to use ERETNC so that

			 * the LLAddr/LLB bit is not cleared when we return from

			 * an exception. MIPS R2 LL/SC instructions trap with an

			 * RI exception so once we emulate them here, we return

			 * back to userland with ERETNC. That preserves the

			 * LLAddr/LLB so the subsequent SC instruction will

			 * succeed preserving the atomic semantics of the LL/SC

			 * block. Without that, there is no safe way to emulate

			 * an LL/SC block in MIPSR2 userland.

			/*

			 * An LL/SC block can't be safely emulated without

			 * a Config5/LLB availability. So it's probably time to

			 * kill our process before things get any worse. This is

			 * because Config5/LLB allows us to use ERETNC so that

			 * the LLAddr/LLB bit is not cleared when we return from

			 * an exception. MIPS R2 LL/SC instructions trap with an

			 * RI exception so once we emulate them here, we return

			 * back to userland with ERETNC. That preserves the

			 * LLAddr/LLB so the subsequent SC instruction will

			 * succeed preserving the atomic semantics of the LL/SC

			 * block. Without that, there is no safe way to emulate

			 * an LL/SC block in MIPSR2 userland.

			/*

			 * An LL/SC block can't be safely emulated without

			 * a Config5/LLB availability. So it's probably time to

			 * kill our process before things get any worse. This is

			 * because Config5/LLB allows us to use ERETNC so that

			 * the LLAddr/LLB bit is not cleared when we return from

			 * an exception. MIPS R2 LL/SC instructions trap with an

			 * RI exception so once we emulate them here, we return

			 * back to userland with ERETNC. That preserves the

			 * LLAddr/LLB so the subsequent SC instruction will

			 * succeed preserving the atomic semantics of the LL/SC

			 * block. Without that, there is no safe way to emulate

			 * an LL/SC block in MIPSR2 userland.

			/*

			 * An LL/SC block can't be safely emulated without

			 * a Config5/LLB availability. So it's probably time to

			 * kill our process before things get any worse. This is

			 * because Config5/LLB allows us to use ERETNC so that

			 * the LLAddr/LLB bit is not cleared when we return from

			 * an exception. MIPS R2 LL/SC instructions trap with an

			 * RI exception so once we emulate them here, we return

			 * back to userland with ERETNC. That preserves the

			 * LLAddr/LLB so the subsequent SC instruction will

			 * succeed preserving the atomic semantics of the LL/SC

			 * block. Without that, there is no safe way to emulate

			 * an LL/SC block in MIPSR2 userland.

 skip it */

	/*

	 * Let's not return to userland just yet. It's costly and

	 * it's likely we have more R2 instructions to emulate

 Likely a MIPS R6 compatible instruction */

 CONFIG_DEBUG_FS */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2004, 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

 APRP can only reserve one TC in a VPE and no more. */

 VPE */

 add the tc to the list of this vpe's tc's. */

 TC */

 set the parent vpe */

 No locking needed here */

 SPDX-License-Identifier: GPL-2.0

/*

 * Count register synchronisation.

 *

 * All CPUs will have their count registers synchronised to the CPU0 next time

 * value. This can cause a small timewarp for CPU0. All other CPU's should

 * not have done anything significant (but they may have had interrupts

 * enabled briefly - prom_smp_finish() should not be responsible for enabling

 * interrupts...)

	/*

	 * We loop a few times to get a primed instruction cache,

	 * then the last pass is more or less synchronised and

	 * the master and slaves each set their cycle counters to a known

	 * value all at once. This reduces the chance of having random offsets

	 * between the processors, and guarantees that the maximum

	 * delay between the cycle counters is never bigger than

	 * the latency of information-passing (cachelines) between

	 * two CPUs.

 slaves loop on '!= 2' */

 Let the slave writes its count register */

 Count will be initialised to current timer */

		/*

		 * Everyone initialises count in the last loop:

		/*

		 * Wait for slave to leave the synchronization point:

 Arrange for an interrupt in a short while */

	/*

	 * i386 code reported the skew here, but the

	 * count registers were almost certainly out of sync

	 * so no point in alarming people

	/*

	 * Not every cpu is online at the time this gets called,

	 * so we first wait for the master to say everyone is ready

		/*

		 * Everyone initialises count in the last loop:

 Arrange for an interrupt in a short while */

 SPDX-License-Identifier: GPL-2.0

/*

 * Conversion between 32-bit and 64-bit native system calls.

 *

 * Copyright (C) 2000 Silicon Graphics, Inc.

 * Written by Ulf Carlsson (ulfc@engr.sgi.com)

/* From the Single Unix Spec: pread & pwrite act like lseek to pos + op +

   lseek back to original location.  They fail just like lseek does on

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 2010 Cavium Networks, Inc.

/*

 * Define parameters for the standard MIPS and the microMIPS jump

 * instruction encoding respectively:

 *

 * - the ISA bit of the target, either 0 or 1 respectively,

 *

 * - the amount the jump target address is shifted right to fit in the

 *   immediate field of the machine instruction, either 2 or 1,

 *

 * - the mask determining the size of the jump region relative to the

 *   delay-slot instruction, either 256MB or 128MB,

 *

 * - the jump target alignment, either 4 or 2 bytes.

 Target must have the right alignment and ISA must be preserved. */

			/*

			 * The branch offset must fit in the instruction's 26

			 * bit field.

			/*

			 * Jump only works within an aligned region its delay

			 * slot is in.

 nop */

 SPDX-License-Identifier: GPL-2.0

/*

 * Code for replacing ftrace calls with jumps.

 *

 * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>

 * Copyright (C) 2009, 2010 DSLab, Lanzhou University, China

 * Author: Wu Zhangjin <wuzhangjin@gmail.com>

 *

 * Thanks goes to Steven Rostedt for writing the original x86 version.

 Arch override because MIPS doesn't need to run this from stop_machine() */

 jump & link: ip --> ra, jump to target */

  op_code|addr : 31...26|25 ....0 */

 nop */

 la v1, _mcount */

 jal (ftrace_caller + 8), jump over the first two instruction */

 j ftrace_graph_caller */

 *(unsigned int *)ip = new_code; */

/*

 * The details about the calling site of mcount on MIPS

 *

 * 1. For kernel:

 *

 * move at, ra

 * jal _mcount		--> nop

 *  sub sp, sp, 8	--> nop  (CONFIG_32BIT)

 *

 * 2. For modules:

 *

 * 2.1 For KBUILD_MCOUNT_RA_ADDRESS and CONFIG_32BIT

 *

 * lui v1, hi_16bit_of_mcount	     --> b 1f (0x10000005)

 * addiu v1, v1, low_16bit_of_mcount --> nop  (CONFIG_32BIT)

 * move at, ra

 * move $12, ra_address

 * jalr v1

 *  sub sp, sp, 8

 *				    1: offset = 5 instructions

 * 2.2 For the Other situations

 *

 * lui v1, hi_16bit_of_mcount	     --> b 1f (0x10000004)

 * addiu v1, v1, low_16bit_of_mcount --> nop  (CONFIG_32BIT)

 * move at, ra

 * jalr v1

 *  nop | move $12, ra_address | sub sp, sp, 8

 *				    1: offset = 4 instructions

	/*

	 * If ip is in kernel space, no long call, otherwise, long call is

	 * needed.

	/*

	 * On 32 bit MIPS platforms, gcc adds a stack adjust

	 * instruction in the delay slot after the branch to

	 * mcount and expects mcount to restore the sp on return.

	 * This is based on a legacy API and does nothing but

	 * waste instructions so it's being removed at runtime.

 Encode the instructions when booting */

 Remove "b ftrace_stub" to ensure ftrace_caller() is executed */

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_DYNAMIC_FTRACE */

 s{d,w} ra, offset(sp) */

 s{d,w} R, offset(sp) */

 stack offset range: 0 ~ PT_SIZE */

	/*

	 * For module, move the ip from the return address after the

	 * instruction "lui v1, hi_16bit_of_mcount"(offset is 24), but for

	 * kernel, move after the instruction "move ra, at"(offset is 16)

	/*

	 * search the text until finding the non-store instruction or "s{d,w}

	 * ra, offset(sp)" instruction

 get the code at "ip": code = *(unsigned int *)ip; */

		/*

		 * If we hit the non-store instruction before finding where the

		 * ra is stored, then this is a leaf function and it does not

		 * store the ra on the stack

 Move to the next instruction */

 tmp = *(unsigned long *)sp; */

 !KBUILD_MCOUNT_RA_ADDRESS */

/*

 * Hook the return address and push it in the stack of return addrs

 * in current thread info.

	/*

	 * "parent_ra_addr" is the stack address where the return address of

	 * the caller of _mcount is saved.

	 *

	 * If gcc < 4.5, a leaf function does not save the return address

	 * in the stack address, so we "emulate" one in _mcount's stack space,

	 * and hijack it directly.

	 * For a non-leaf function, it does save the return address to its own

	 * stack space, so we can not hijack it directly, but need to find the

	 * real stack address, which is done by ftrace_get_parent_addr().

	 *

	 * If gcc >= 4.5, with the new -mmcount-ra-address option, for a

	 * non-leaf function, the location of the return address will be saved

	 * to $12 for us.

	 * For a leaf function, it just puts a zero into $12, so we handle

	 * it in ftrace_graph_caller() of mcount.S.

 old_parent_ra = *parent_ra_addr; */

	/*

	 * If fails when getting the stack address of the non-leaf function's

	 * ra, stop function graph tracer and return

 *parent_ra_addr = return_hooker; */

	/*

	 * Get the recorded ip of the current mcount calling site in the

	 * __mcount_loc section, which will be used to filter the function

	 * entries configured through the tracing/set_graph_function interface.

 CONFIG_FUNCTION_GRAPH_TRACER */

 CONFIG_FTRACE_SYSCALLS */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Based on linux/arch/mips/kernel/cevt-r4k.c,

 *	    linux/arch/mips/jmr3927/rbhma3100/setup.c

 *

 * Copyright 2001 MontaVista Software Inc.

 * Copyright (C) 2000-2001 Toshiba Corporation

 * Copyright (C) 2007 MIPS Technologies, Inc.

 * Copyright (C) 2007 Ralf Baechle <ralf@linux-mips.org>

 1/2 */

 Use 1 bit smaller width to use full bits in that width */

 stop and reset counter */

 clear pending interrupt */

 start timer */

 start timer */

 ack interrupt */

 Start once to make CounterResetEnable effective */

 Stop and reset the counter */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  DS1287 clockevent driver

 *

 *  Copyright (C) 2008	Yoichi Yuasa <yuasa@linux-mips.org>

 Ack the RTC interrupt. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2004, 05, 06 MIPS Technologies, Inc.

 *    Elizabeth Clarke (beth@mips.com)

 *    Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2006 Ralf Baechle (ralf@linux-mips.org)

 set config to be the same as vpe0, particularly kseg0 coherency alg */

 make sure there are no software interrupts pending */

 Propagate Config7 */

 Deactivate all but VPE 0 */

 master VPE */

 Record this as available CPU */

 Disable multi-threading with TC's */

	/* bind a TC to each VPE, May as well put all excess TC's

 and set XTC */

 mark not allocated and not dynamically allocatable */

 interrupt exempt */

 This is Malta specific: IPI,performance and timer interrupts */

 CDFIXME: remove this? */

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

/*

 * Setup the PC, SP, and GP of a secondary processor and start it

 * running!

 * smp_bootstrap is the place to resume from

 * __KSTK_TOS(idle) is apparently the stack pointer

 * (unsigned long)idle->thread_info the gp

 * assumes a 1:1 mapping of TC => VPE

 restart */

 enable the tc this vpe/cpu will be running */

 enable the VPE */

 stack pointer */

 global pointer */

 finally out of configuration and into chaos */

/*

 * Common setup before any secondaries are started

 * Make sure all CPU's are in a sensible state before we boot any of the

 * secondaries

 If we have an FPU, enroll ourselves in the FPU-full mask */

 CONFIG_MIPS_MT_FPAFF */

 disable MT so we can configure */

 Put MVPE's into 'configuration state' */

	/* we'll always have more TC's than VPE's, so loop setting everything

 Release config state */

 We'll wait until starting the secondaries before starting MVPE */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Processor capabilities determination functions.

 *

 * Copyright (C) xxxx  the Anonymous

 * Copyright (C) 1994 - 2006 Ralf Baechle

 * Copyright (C) 2003, 2004  Maciej W. Rozycki

 * Copyright (C) 2001, 2004, 2011, 2012	 MIPS Technologies, Inc.

/*

 * Get the FPU Implementation/Revision.

/*

 * Check if the CPU has an external FPU.

/*

 * Determine the FCSR mask for FPU hardware.

/*

 * Determine the IEEE 754 NaN encodings and ABS.fmt/NEG.fmt execution modes

 * supported by FPU hardware.

			/*

			 * MAC2008 toolchain never landed in real world, so

			 * we're only testing whether it can be disabled and

			 *  don't try to enabled it.

				/*

				 * The bit for MAC2008 might be reused by R6

				 * in future, so we only test for R2-R5.

/*

 * IEEE 754 conformance mode to use.  Affects the NaN encoding and the

 * ABS.fmt/NEG.fmt execution mode.

/*

 * Set the IEEE 754 NaN encodings and the ABS.fmt/NEG.fmt execution modes

 * to support by the FPU emulator according to the IEEE 754 conformance

 * mode selected.  Note that "relaxed" straps the emulator so that it

 * allows 2008-NaN binaries even for legacy processors.

/*

 * Override the IEEE 754 NaN encoding and ABS.fmt/NEG.fmt execution mode

 * according to the "ieee754=" parameter.

/*

 * IEEE 754 NaN encoding and ABS.fmt/NEG.fmt execution mode override

 * settings:

 *

 * strict:  accept binaries that request a NaN encoding supported by the FPU

 * legacy:  only accept legacy-NaN binaries

 * 2008:    only accept 2008-NaN binaries

 * relaxed: accept any binaries regardless of whether supported by the FPU

/*

 * Set the FIR feature flags for the FPU emulator.

 Determined FPU emulator mask to use for the boot CPU with "nofpu".  */

/*

 * Set options for FPU hardware.

/*

 * Set options for the FPU emulator.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  DEC I/O ASIC's counter clocksource

 *

 *  Copyright (C) 2008	Yoichi Yuasa <yuasa@linux-mips.org>

 An early revision of the I/O ASIC didn't have the counter.  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Linux performance counter support for MIPS.

 *

 * Copyright (C) 2010 MIPS Technologies, Inc.

 * Author: Deng-Cheng Zhu

 *

 * This code is based on the implementation for ARM, which is in turn

 * based on the sparc64 perf event code and the x86 code. Performance

 * counter access is based on the MIPS Oprofile code. And the callchain

 * support references the code of MIPS stacktrace.c.

 Callchain handling code. */

/*

 * Leave userspace callchain empty for now. When we find a way to trace

 * the user stack callchains, we will add it here.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1991, 1992  Linus Torvalds

 * Copyright (C) 1994 - 2000, 2006  Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2016, Imagination Technologies Ltd.

 32-bit compatibility types */

/*

 * Atomically swap in the new signal mask, and wait for a signal.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

/*

 * Interrupt handler may be called before rtlx_init has otherwise had

 * a chance to run.

 initialise the wait queues */

 set up notifiers */

 SPDX-License-Identifier: GPL-2.0

 This keeps a track of which one is crashing cpu. */

	/*

	 * If we are passed registers, use those.  Otherwise get the

	 * regs from the last interrupt, which should be correct, as

	 * we are in an interrupt.  But if the regs are not there,

	 * pull them from the top of the stack.  They are probably

	 * wrong, but we need something to keep from crashing again.

 We won't be sent IPIs any more. */

 NOTREACHED */

 Excluding the panic cpu */

	/*

	 * The crash CPU sends an IPI and wait for other CPUs to

	 * respond. Delay of at least 10 seconds.

 Override the weak function in kernel/panic.c */

 !defined(CONFIG_SMP)  */

 !defined(CONFIG_SMP)	*/

 SPDX-License-Identifier: GPL-2.0

/*

 * General MIPS MT support routines, usable in AP/SP and SMVP.

 * Copyright (C) 2005 Mips Technologies, Inc

/*

 * CPU mask used to set process affinity for MT VPEs/TCs with FPUs

/*

 * Replacement functions for the sys_sched_setaffinity() and

 * sys_sched_getaffinity() system calls, so that we can integrate

 * FPU affinity with the user's requested processor affinity.

 * This code is 98% identical with the sys_sched_setaffinity()

 * and sys_sched_getaffinity() system calls, and should be

 * updated when kernel/sched/core.c changes.

/*

 * find_process_by_pid - find a process with a matching PID value.

 * used in sys_sched_set/getaffinity() in kernel/sched/core.c, so

 * cloned here.

/*

 * check the target process has a UID that matches the current process's

/*

 * mipsmt_sys_sched_setaffinity - set the cpu affinity of a process

 Prevent p going away */

 Record new user-specified CPU set for future reference */

 Compute new global allowed CPU set if necessary */

			/*

			 * We must have raced with a concurrent cpuset

			 * update. Just reset the cpus_allowed to the

			 * cpuset's cpus_allowed

/*

 * mipsmt_sys_sched_getaffinity - get the cpu affinity of a process

/*

 * FPU Use Factor empirically derived from experiments on 34K

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000, 2001 Broadcom Corporation

/*

 * The general purpose timer ticks at 1MHz independent if

 * the rest of the system

 Stop the timer until we actually program a shot */

 ACK interrupt */

 Only have 4 general purpose timers, and we use last one as hpt */

	/*

	 * Map the timer interrupt to IP[4] of this cpu

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2000, 2001 Kanoj Sarcar

 * Copyright (C) 2000, 2001 Ralf Baechle

 * Copyright (C) 2000, 2001 Silicon Graphics, Inc.

 * Copyright (C) 2000, 2001, 2003 Broadcom Corporation

 Map physical to logical */

 Map logical to physical */

 Number of TCs (or siblings in Intel speak) per CPU core */

 representing the TCs (or siblings in Intel speak) of each logical CPU */

 representing the core map of multi-core chips of each logical CPU */

/*

 * A logical cpu mask containing only one VPE per core to

 * reduce the number of IPIs on large MT systems.

 representing cpus for which sibling maps can be computed */

 representing cpus for which core maps can be computed */

/*

 * Calculate a new cpu_foreign_map mask whenever a

 * new cpu appears or disappears.

 Re-calculate the mask */

	/*

	 * Some platforms have half DT setup. So if we found irq node but

	 * didn't find an ipidomain, try to search for one that is not in the

	 * DT.

	/*

	 * There are systems which use IPI IRQ domains, but only have one

	 * registered when some runtime condition is met. For example a Malta

	 * kernel may include support for GIC & CPU interrupt controller IPI

	 * IRQ domains, but if run on a system with no GIC & no MT ASE then

	 * neither will be supported or registered.

	 *

	 * We only have a problem if we're actually using multiple CPUs so fail

	 * loudly if that is the case. Otherwise simply return, skipping IPI

	 * setup, if we're running with only a single CPU.

	/*

	 * Some platforms have half DT setup. So if we found irq node but

	 * didn't find an ipidomain, try to search for one that is not in the

	 * DT.

/*

 * First C code run on the secondary CPUs after being started up by

 * the master.

	/*

	 * XXX parity protection should be folded in here when it's converted

	 * to an option instead of something based on .cputype

 Notify boot CPU that we're starting & ready to sync counters */

 The CPU is running and counters synchronised, now mark it online */

	/*

	 * Notify boot CPU that we're up & online and it can safely return

	 * from __cpu_up

	/*

	 * irq will be enabled in ->smp_finish(), enabling it too early

	 * is dangerous.

	/*

	 * Remove this CPU:

 called from main before smp_init() */

 preload SMP state for boot cpu */

 Wait for CPU to start and be ready to sync counters */

 Wait for CPU to finish startup & mark itself online before return */

 Not really SMP stuff ... */

/*

 * Special Variant of smp_call_function for use by TLB functions:

 *

 *  o No return value

 *  o collapses to normal function call on UP kernels

 *  o collapses to normal function call on systems with a single shared

 *    primary cache.

/*

 * The following tlb flush calls are invoked when old translations are

 * being torn down, or pte attributes are changing. For single threaded

 * address spaces, a new context is obtained on the current cpu, and tlb

 * context on other cpus are invalidated to force a new context allocation

 * at switch_mm time, should the mm ever be used on other cpus. For

 * multithreaded address spaces, inter-CPU interrupts have to be sent.

 * Another case where inter-CPU interrupts are required is when the target

 * mm might be active on another cpu (eg debuggers doing the flushes on

 * behalf of debugees, kswapd stealing pages from another process etc).

 * Kanoj 07/00.

		/*

		 * No need to worry about other CPUs - the ginvt in

		 * drop_mmu_context() will be globalized.

			/*

			 * flush_cache_range() will only fully flush icache if

			 * the VMA is executable, otherwise we must invalidate

			 * ASID without it appearing to has_valid_asid() as if

			 * mm has been completely unused by that CPU.

			/*

			 * flush_cache_page() only does partial flushes, so

			 * invalidate ASID without it appearing to

			 * has_valid_asid() as if mm has been completely unused

			 * by that CPU.

 CONFIG_GENERIC_CLOCKEVENTS_BROADCAST */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2011 by Kevin Cernekee (cernekee@gmail.com)

 *

 * SMP support for BMIPS

 these may be configured by the platform code */

 initial $sp, $gp - used by arch/mips/kernel/bmips_vec.S */

 SW interrupts 0,1 are used for interprocessor signaling */

 arbitration priority */

 NBK and weak order flags */

 Find out if we are running on TP0 or TP1 */

		/*

		 * MIPS interrupts 0,1 (SW INT 0,1) cross over to the other

		 * thread

		 * MIPS interrupt 2 (HW INT 0) is the CPU0 L1 controller output

		 * MIPS interrupt 3 (HW INT 1) is the CPU1 L1 controller output

 single core, 2 threads (2 pipelines) */

 enable raceless SW interrupts */

 route HW interrupt 0 to CPU0, HW interrupt 1 to CPU1 */

 N cores, 2 threads per core */

 clear any pending SW interrupts */

 this can be overridden by the BSP */

/*

 * IPI IRQ setup - runs on CPU0

/*

 * Tell the hardware to boot CPUx - runs on CPU0

	/*

	 * Initial boot sequence for secondary CPU:

	 *   bmips_reset_nmi_vec @ a000_0000 ->

	 *   bmips_smp_entry ->

	 *   plat_wired_tlb_setup (cached function call; optional) ->

	 *   start_secondary (cached jump)

	 *

	 * Warm restart sequence:

	 *   play_dead WAIT loop ->

	 *   bmips_smp_int_vec @ BMIPS_WARM_RESTART_VEC ->

	 *   eret to play_dead ->

	 *   bmips_secondary_reentry ->

	 *   start_secondary

 kseg1 might not exist if this CPU enabled XKS01 */

 Reset slave TP1 if booting from TP0 */

/*

 * Early setup - runs on secondary CPU after cache probe

/*

 * Late setup - runs on secondary CPU before entering the idle loop

 make sure there won't be a timer interrupt for a little while */

/*

 * BMIPS5000 raceless IPIs

 *

 * Each CPU has two inbound SW IRQs which are independent of all other CPUs.

 * IPI0 is used for SMP_RESCHEDULE_YOURSELF

 * IPI1 is used for SMP_CALL_FUNCTION

/*

 * BMIPS43xx racey IPIs

 *

 * We use one inbound SW IRQ for each CPU.

 *

 * A spinlock must be held in order to keep CPUx from accidentally clearing

 * an incoming IPI when it writes CP0 CAUSE to raise an IPI on CPUy.  The

 * same spinlock is used to protect the action masks.

 flush data cache */

	/*

	 * Wakeup is on SW0 or SW1; disable everything else

	 * Use BEV !IV (BMIPS_WARM_RESTART_VEC) to avoid the regular Linux

	 * IRQ handlers; this clears ST0_IE and returns immediately.

	/*

	 * wait for SW interrupt from bmips_boot_secondary(), then jump

	 * back to start_secondary()

 CONFIG_HOTPLUG_CPU */

 CONFIG_SMP */

/***********************************************************************

 * BMIPS vector relocation

 * This is primarily used for SMP boot, but it is applicable to some

 * UP BMIPS systems as well.

 BMIPS5200 "should" use mask/shift, but it's buggy */

 this needs to run from CPU0 (which is always online) */

		/*

		 * BMIPS4350 cannot relocate the normal vectors, but it

		 * can relocate the BEV=1 vectors.  So CPU1 starts up at

		 * the relocated BEV=1, IV=0 general exception vector @

		 * 0xa000_0380.

		 *

		 * set_uncached_handler() is used here because:

		 *  - CPU1 will run this from uncached space

		 *  - None of the cacheflush functions are set up yet

		/*

		 * 0x8000_0000: reset/NMI (initially in kseg1)

		 * 0x8000_0400: normal vectors

		/*

		 * 0x8000_0000: reset/NMI (initially in kseg1)

		 * 0x8000_1000: normal vectors

	/*

	 * Called when starting/restarting a secondary CPU.

	 * Kernel stacks and other important data might only be accessible

	 * once the wired entries are present.

 Set BIU to async mode */

 put the BIU back in sync mode */

 clear BHTD to enable branch history table */

 Flush and enable RAC */

 CBG workaround for early BMIPS4380 CPUs */

 clear BHTD to enable branch history table */

 XI/ROTR enable */

 enable RDHWR, BRDHWR */

 Disable JTB */

 mtc0	t0, $22, 15 */

 mfc0	t0, $22, 8 */

 mtc0	t0, $22, 8 */

 mtc0	t0, $22, 15 */

 XI enable */

 enable MIPS32R2 ROR instruction for XI TLB handlers */

 mtc0 $8, $22, 15 */

 mfc0 $8, $22, 8 */

 mtc0 $8, $22, 8 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2003 Broadcom Corporation

/*

 * Including <asm/unistd.h> would give use the 64-bit syscall numbers ...

 mask last for extensibility */

 argument save space for o32 */

 Was: signal trampoline */

	/*

	 * Don't let your children do this ...

 no outputs */

 Unreached */

 Create siginfo.  */

 Create the ucontext.	 */

	/*

	 * Arguments to signal handler:

	 *

	 *   a0 = signal number

	 *   a1 = 0 (should be cause)

	 *   a2 = pointer to ucontext

	 *

	 * $25 and c0_epc point to the signal handler, $29 points to

	 * the struct rt_sigframe.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1991, 1992  Linus Torvalds

 * Copyright (C) 1994 - 2000  Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2014, Imagination Technologies Ltd.

 argument save space for o32 */

 Was: signal trampoline */

 Matches struct ucontext from its uc_mcontext field onwards */

 argument save space for o32 */

 Was: signal trampoline */

/*

 * Thread saved context copy to/from a signal context presumed to be on the

 * user stack, and therefore accessed with appropriate macros from uaccess.h.

 !CONFIG_MIPS_FP_SUPPORT */

 !CONFIG_MIPS_FP_SUPPORT */

/*

 * Wrappers for the assembly _{save,restore}_fp_context functions.

/*

 * Extended context handling.

	/*

	 * We can just pretend the sigcontext is always embedded in a struct

	 * ucontext here, because the offset from sigcontext to extended

	 * context is the same in the struct sigframe case.

	/*

	 * Ensure that we can't lose the live MSA context between checking

	 * for it & writing it to memory.

		/*

		 * There are no EVA versions of the vector register load/store

		 * instructions, so MSA context has to be saved to kernel memory

		 * and then copied to user memory. The save to kernel memory

		 * should already have been done when handling scalar FP

		 * context.

		/*

		 * There are no EVA versions of the vector register load/store

		 * instructions, so MSA context has to be copied to kernel

		 * memory and later loaded to registers. The same is true of

		 * scalar FP context, so FPU & MSA should have already been

		 * disabled whilst handling scalar FP context.

 !CONFIG_CPU_HAS_MSA */

 !CONFIG_CPU_HAS_MSA */

 If no context was saved then trivially return */

 Write the end marker */

/*

 * Helper routines

	/*

	 * EVA does not have userland equivalents of ldc1 or sdc1, so

	 * save to the kernel FP context & copy that to userland below.

 touch the sigcontext and try again */

 really bad sigcontext */

	/*

	 * The signal handler may have used FPU; give it up if the program

	 * doesn't want it following sigreturn.

	/*

	 * EVA does not have userland equivalents of ldc1 or sdc1, so we

	 * disable the FPU here such that the code below simply copies to

	 * the kernel FP context.

 touch the sigcontext and try again */

 really bad sigcontext */

	/*

	 * Save FPU state to signal context. Signal handler

	 * will "inherit" current FPU state.

	/*

	 * The assumption here is that between this point & the point at which

	 * the extended context is saved the size of the context should only

	 * ever be able to shrink (if the task is preempted), but never grow.

	 * That is, what this function returns is an upper bound on the size of

	 * the extended context for the current task at the current time.

 If any context is saved then we'll append the end marker */

	/*

	 * If the signal handler set some FPU exceptions, clear it and

	 * send SIGFPE.

 Always make any pending restarted system calls return -EINTR */

 Leave space for potential extended context */

 Default to using normal stack */

	/*

	 * FPU emulator may have it's own trampoline active just

	 * above the user stack, 16-bytes before the next lowest

	 * 16 byte boundary.  Try to avoid trashing it.

/*

 * Atomically swap in the new signal mask, and wait for a signal.

	/*

	 * Don't let your children do this ...

 no outputs */

 Unreached */

 CONFIG_TRAD_SIGNALS */

	/*

	 * Don't let your children do this ...

 no outputs */

 Unreached */

	/*

	 * Arguments to signal handler:

	 *

	 *   a0 = signal number

	 *   a1 = 0 (should be cause)

	 *   a2 = pointer to struct sigcontext

	 *

	 * $25 and c0_epc point to the signal handler, $29 points to the

	 * struct sigframe.

 Create siginfo.  */

 Create the ucontext.	 */

	/*

	 * Arguments to signal handler:

	 *

	 *   a0 = signal number

	 *   a1 = 0 (should be cause)

	 *   a2 = pointer to ucontext

	 *

	 * $25 and c0_epc point to the signal handler, $29 points to

	 * the struct rt_sigframe.

	/*

	 * If we were emulating a delay slot instruction, exit that frame such

	 * that addresses in the sigframe are as expected for userland and we

	 * don't have a problem if we reuse the thread's frame for an

	 * instruction within the signal handler.

 Don't deal with this again.	*/

 Whee!  Actually deliver the signal.	*/

 Don't deal with this again.	*/

	/*

	 * If there's no signal to deliver, we just put the saved sigmask

	 * back

/*

 * notification of userspace execution resumption

 * - triggered by the TIF_WORK_MASK flags

 deal with pending signal delivery */

	/*

	 * The offset from sigcontext to extended context should be the same

	 * regardless of the type of signal, such that userland can always know

	 * where to look if it wishes to find the extended context structures.

 For now just do the cpu_has_fpu check when the functions are invoked */

 CONFIG_SMP */

/*

 *  Originally written by Glenn Engel, Lake Stevens Instrument Division

 *

 *  Contributed by HP Systems

 *

 *  Modified for Linux/MIPS (and MIPS in general) by Andreas Busse

 *  Send complaints, suggestions etc. to <andy@waldorf-gmbh.de>

 *

 *  Copyright (C) 1995 Andreas Busse

 *

 *  Copyright (C) 2003 MontaVista Software Inc.

 *  Author: Jun Sun, jsun@mvista.com or jsun@junsun.net

 *

 *  Copyright (C) 2004-2005 MontaVista Software Inc.

 *  Author: Manish Lachwani, mlachwani@mvista.com or manish@koffee-break.com

 *

 *  Copyright (C) 2007-2008 Wind River Systems, Inc.

 *  Author/Maintainer: Jason Wessel, jason.wessel@windriver.com

 *

 *  This file is licensed under the terms of the GNU General Public License

 *  version 2. This program is licensed "as is" without any warranty of any

 *  kind, whether express or implied.

 for linux pt_regs struct */

 Trap type code for MIPS R3xxx and R4xxx */

 Signal that we map this trap into */

 instruction bus error */

 data bus error */

 break */

	{ 11, SIGILL }, */	
 overflow */

 trap */

 virtual instruction cache coherency */

 floating point exception */

 watch */

 virtual data cache coherency */

 Must be last */

 FP registers 38 -> 69 */

 Process the fcr31/fsr (register 70) */

 Ignore the fir (register 71) */

 First 38 registers */

 FP registers 38 -> 69 */

 Process the fcr31/fsr (register 70) */

 Ignore the fir (register 71) */

 default for things we don't know about */

/*

 * Similar to regs_to_gdb_regs() except that process is sleeping and so

 * we may not be able to get all the info.

 S0 - S7 */

 GP, SP, FP, RA */

 lo, hi */

	/*

	 * BadVAddr, Cause

	 * Ideally these would come from the last exception frame up the stack

	 * but that requires unwinding, otherwise we can't know much for sure.

	/*

	 * PC

	 * use return address (RA), i.e. the moment after return from resume()

/*

 * Calls linux_debug_hook before the kernel dies. If KGDB is enabled,

 * then try to fall into the debugger

	/*

	 * Return immediately if the kprobes fault notifier has set

	 * DIE_PAGE_FAULT.

 CONFIG_KPROBES */

 Userspace events, ignore. */

 In SMP mode, __flush_cache_all does IPI */

 CONFIG_KGDB_LOW_LEVEL_TRAP */

/*

 * Handle the 'c' command

 handle the optional parameter */

/*

 *	kgdb_arch_exit - Perform any architecture specific uninitalization.

 *

 *	This function will handle the uninitalization of any architecture

 *	specific callbacks, for dynamic registration and unregistration.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Processor capabilities determination functions.

 *

 * Copyright (C) xxxx  the Anonymous

 * Copyright (C) 1994 - 2006 Ralf Baechle

 * Copyright (C) 2003, 2004  Maciej W. Rozycki

 * Copyright (C) 2001, 2004, 2011, 2012	 MIPS Technologies, Inc.

 Hardware capabilities */

	/*

	 * If the core hasn't done any FTLB configuration, there is nothing

	 * for us to do here.

 Disable it in the boot cpu */

 Check that FTLB has been disabled */

 MMUSIZEEXT == VTLB ON, FTLB OFF */

 This should never happen */

	/*

	 * noftlb is mainly used for debug purposes so print

	 * an informative message instead of using pr_debug()

	/*

	 * Some of these bits are duplicated in the decode_config4.

	 * MIPS_CONF4_MMUEXTDEF_MMUSIZEEXT is the only possible case

	 * once FTLB has been disabled so undo what decode_config4 did.

/*

 * Check if the CPU has per tc perf counters

		/*

		 * Erratum "RPS May Cause Incorrect Instruction Execution"

		 * This code only handles VPE0, any SMP/RTOS code

		 * making use of VPE1 will be responsable for that VPE.

/*

 * Probe whether cpu has config register by trying to play with

 * alternate cache bit and see whether it matters.

 * It's used by cpu_probe to distinguish between R3000A and R3081.

 R6 incompatible with everything else */

 Break here so we don't add incompatible ISAs */

	/*

	 * 0 = All TLBWR instructions go to FTLB

	 * 1 = 15:1: For every 16 TBLWR instructions, 15 go to the

	 * FTLB and 1 goes to the VTLB.

	 * 2 = 7:1: As above with 7:1 ratio.

	 * 3 = 3:1: As above with 3:1 ratio.

	 *

	 * Use the linear midpoint as the probability threshold.

		/*

		 * So FTLB is less than 4 times bigger than VTLB.

		 * A 3:1 ratio can still be useful though.

 It's implementation dependent how the FTLB can be enabled */

 proAptiv & related cores use Config6 to enable the FTLB */

 There's no way to disable the FTLB */

 Flush ITLB, DTLB, VTLB and FTLB */

 Loongson-3 cores use Config6 to enable the FTLB */

 Enable FTLB */

 Disable FTLB */

	/*

	 * It's implementation dependent what type of write-merge is supported

	 * and whether it can be enabled/disabled. If it is settable lets make

	 * the merging allowed by default. Some platforms might have

	 * write-through caching unsupported. In this case just ignore the

	 * CP0.Config.MM bit field value.

	/*

	 * Look for Standard TLB or Dual VTLB and FTLB

		/*

		 * R6 has dropped the MMUExtDef field from config4.

		 * On R6 the fields always describe the FTLB, and only if it is

		 * present according to Config.MT.

 Switch FTLB off */

	/*

	 * Warn if the computed ASID mask doesn't match the mask the kernel

	 * is built for. This may indicate either a serious problem or an

	 * easy optimisation opportunity, but either way should be addressed.

 Ensure the write to config5 above takes effect */

 Check whether we successfully enabled MMID support */

		/*

		 * Warn if we've hardcoded cpu_has_mmid to a value unsuitable

		 * for the CPU we're running on, or if CPUs in an SMP system

		 * have inconsistent MMID support.

			/*

			 * We maintain a bitmap to track MMID allocation, and

			 * need a sensible upper bound on the size of that

			 * bitmap. The initial CPU with MMID support (I6500)

			 * supports 16 bit MMIDs, which gives us an 8KiB

			 * bitmap. The architecture recommends that hardware

			 * support 32 bit MMIDs, which would give us a 512MiB

			 * bitmap - that's too big in most cases.

			 *

			 * Cap MMID width at 16 bits for now & we can revisit

			 * this if & when hardware supports anything wider.

 MIPS32 or MIPS64 compliant CPU.  */

 Enable FTLB if present and not disabled */

 Read Config registers.  */

 Arch spec violation!	 */

 Probe the EBase.WG bit */

 {read,write}_c0_ebase_64() may be UNDEFINED prior to r6 */

 WG bit already set, we can avoid the clumsy probe */

 Its UNDEFINED to change EBase while BEV=0 */

			/*

			 * On pre-r6 cores, this may well clobber the upper bits

			 * of EBase. This is hard to avoid without potentially

			 * hitting UNDEFINED dm*c0 behaviour if EBase is 32-bit.

 Restore BEV */

 configure the FTLB write probability */

/*

 * Probe for certain guest capabilities by writing config bits and reading back.

 * Finally write back the original value.

/*

 * Probe for dynamic guest capabilities by changing certain config bits and

 * reading back to see if they change. Finally write back the original value.

		/*

		 * Probe for Direct Root to Guest (DRG). Set GuestCtl1.RID = 0

		 * first, otherwise all data accesses will be fully virtualised

		 * as if they were performed by guest mode.

 determine the number of bits of GuestID available */

 determine the number of bits of GTOffset available */

			/*

			 * SC and MC versions can't be reliably told apart,

			 * but only the latter support coherent caching

			 * modes so assume the firmware has set the KSEG0

			 * coherency attribute reasonably (if uncached, we

			 * assume SC).

		/*

		 * This processor doesn't have an MMU, so it's not

		 * "real easy" to run Linux on it. It is left purely

		 * for documentation.  Commented out because it shares

		 * it's c0_prid id number with the TX3900.

		/*

		 * Undocumented RM7000:	 Bit 29 in the info register of

		 * the RM7000 v2.0 indicates if the TLB has 48 or 64

		 * entries.

		 *

		 * 29	   1 =>	   64 entry JTLB

		 *	   0 =>	   48 entry JTLB

 Loongson-2/3 */

 Loongson-1 */

	/* Recent MIPS cores use the implementation-dependent ExcCode 16 for

	 * cache/FTLB parity exceptions.

 FPU in pass1 is known to have issues. */

 All Loongson processors covered here define ExcCode 16 as GSExc. */

 Loongson-64 Reduced */

 Loongson-3 Classic */

		/*

		 * Loongson-3 Classic did not implement MIPS standard TLBINV

		 * but implemented TLBINVF and EHINV. As currently we're only

		 * using these two features, enable MIPS_CPU_TLBINV as well.

		 *

		 * Also some early Loongson-3A2000 had wrong TLB type in Config

		 * register, we correct it here.

 VZ of Loongson-3A2000/3000 is incomplete */

	/*

	 * XBurst misses a config2 register, so config3 decode was skipped in

	 * decode_configs().

 XBurst does not implement the CP0 counter. */

 XBurst has virtually tagged icache */

 XBurst®1 with MXU1.0/MXU1.1 SIMD ISA */

		/*

		 * The XBurst core by default attempts to avoid branch target

		 * buffer lookups by detecting & special casing loops. This

		 * feature will cause BogoMIPS and lpj calculate in error.

		 * Set cp0 config7 bit 4 to disable this feature.

		/*

		 * The config0 register in the XBurst CPUs with a processor ID of

		 * PRID_COMP_INGENIC_D0 report themselves as MIPS32r2 compatible,

		 * but they don't actually support this ISA.

 FPU is not properly detected on JZ4760(B). */

		/*

		 * The config0 register in the XBurst CPUs with a processor ID of

		 * PRID_COMP_INGENIC_D0 or PRID_COMP_INGENIC_D1 has an abandoned

		 * huge page tlb mode, this mode is not compatible with the MIPS

		 * standard, it will cause tlbmiss and into an infinite loop

		 * (line 21 in the tlb-funcs.S) when starting the init process.

		 * After chip reset, the default is HPTLB mode, Write 0xa9000000

		 * to cp0 register 5 sel 4 to switch back to VTLB mode to prevent

		 * getting stuck.

 XBurst®1 with MXU2.0 SIMD ISA */

 Ingenic uses the WA bit to achieve write-combine memory writes */

 XBurst®2 with MXU2.1 SIMD ISA */

 For use by uaccess.h */

	/*

	 * Set a default elf platform, cpu probe may later

	 * overwrite it with a more precise value

	/*

	 * Platform code can force the cpu type to optimize code

	 * generation. In that case be sure the cpu type is correctly

	 * manually setup otherwise it could trigger some nasty bugs.

 Enable the RIXI exceptions */

 Verify the IEC bit is set */

 R2 has Performance Counter Interrupt indicator */

	/* Synthesize CPUCFG data if running on Loongson processors;

	 * no-op otherwise.

	 *

	 * This looks at previously probed features, so keep this at bottom.

 Ensure the core number fits in the field */

 Ensure the core number fits in the field */

 Ensure the VP(E) ID fits in the field */

 Ensure we're not using VP(E)s without support */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1992 Ross Biro

 * Copyright (C) Linus Torvalds

 * Copyright (C) 1994, 95, 96, 97, 98, 2000 Ralf Baechle

 * Copyright (C) 1996 David S. Miller

 * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999 MIPS Technologies, Inc.

 * Copyright (C) 2000 Ulf Carlsson

 *

 * At this time Linux/MIPS64 only supports syscall tracing, even for 32-bit

 * binaries.

/*

 * Called by kernel/ptrace.c when detaching..

 *

 * Make sure single step bits etc are not set.

 Don't load the watchpoint registers for the ex-child. */

/*

 * Read a general register set.	 We always use the 64-bit format, even

 * for 32-bit kernels and for 32-bit processes on a 64-bit kernel.

 * Registers are sign extended to fill the available space.

/*

 * Write a general register set.  As for PTRACE_GETREGS, we always use

 * the 64-bit format.  On a 32-bit kernel only the lower order half

 * (according to endianness) will be used.

 badvaddr, status, and cause may not be written.  */

 System call number may have been changed */

 Check the values. */

 Install them. */

 Set the G bit. */

 regset get/set implementations */

		/*

		 * Cast all values to signed here so that if this is a 64-bit

		 * kernel, the supplied 32-bit values will be sign extended.

 k0/k1 are ignored. */

 System call number may have been changed */

 CONFIG_32BIT || CONFIG_MIPS32_O32 */

 k0/k1 are ignored. */

 System call number may have been changed */

 CONFIG_64BIT */

/*

 * Poke at FCSR according to its mask.  Set the Cause bits even

 * if a corresponding Enable bit is set.  This will be noticed at

 * the time the thread is switched to and SIGFPE thrown accordingly.

 FIR may not be written.  */

/*

 * Copy the floating-point context to the supplied NT_PRFPREG buffer,

 * !CONFIG_CPU_HAS_MSA variant.  FP context's general register slots

 * correspond 1:1 to buffer slots.  Only general registers are copied.

/*

 * Copy the floating-point context to the supplied NT_PRFPREG buffer,

 * CONFIG_CPU_HAS_MSA variant.  Only lower 64 bits of FP context's

 * general register slots are copied to buffer slots.  Only general

 * registers are copied.

/*

 * Copy the floating-point context to the supplied NT_PRFPREG buffer.

 * Choose the appropriate helper for general registers, and then copy

 * the FCSR and FIR registers separately.

/*

 * Copy the supplied NT_PRFPREG buffer to the floating-point context,

 * !CONFIG_CPU_HAS_MSA variant.   Buffer slots correspond 1:1 to FP

 * context's general register slots.  Only general registers are copied.

/*

 * Copy the supplied NT_PRFPREG buffer to the floating-point context,

 * CONFIG_CPU_HAS_MSA variant.  Buffer slots are copied to lower 64

 * bits only of FP context's general register slots.  Only general

 * registers are copied.

/*

 * Copy the supplied NT_PRFPREG buffer to the floating-point context.

 * Choose the appropriate helper for general registers, and then copy

 * the FCSR register separately.  Ignore the incoming FIR register

 * contents though, as the register is read-only.

 *

 * We optimize for the case where `count % sizeof(elf_fpreg_t) == 0',

 * which is supposed to have been guaranteed by the kernel before

 * calling us, e.g. in `ptrace_regset'.  We enforce that requirement,

 * so that we can safely avoid preinitializing temporaries for

 * partial register writes.

 Copy the FP mode setting to the supplied NT_MIPS_FP_MODE buffer.  */

/*

 * Copy the supplied NT_MIPS_FP_MODE buffer to the FP mode setting.

 *

 * We optimize for the case where `count % sizeof(int) == 0', which

 * is supposed to have been guaranteed by the kernel before calling

 * us, e.g. in `ptrace_regset'.  We enforce that requirement, so

 * that we can safely avoid preinitializing temporaries for partial

 * mode writes.

 CONFIG_MIPS_FP_SUPPORT */

 The task hasn't used FP or MSA, fill with 0xff */

 Copy scalar FP context, fill the rest with 0xff */

 Trivially copy the vector registers */

 Copy as much context as possible, fill the rest with 0xff */

 Trivially copy the vector registers */

 Copy as much context as possible */

 CONFIG_CPU_HAS_MSA */

/*

 * Copy the DSP context to the supplied 32-bit NT_MIPS_DSP buffer.

/*

 * Copy the supplied 32-bit NT_MIPS_DSP buffer to the DSP context.

 CONFIG_32BIT || CONFIG_MIPS32_O32 */

/*

 * Copy the DSP context to the supplied 64-bit NT_MIPS_DSP buffer.

/*

 * Copy the supplied 64-bit NT_MIPS_DSP buffer to the DSP context.

 CONFIG_64BIT */

/*

 * Determine whether the DSP context is present.

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:       the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

 CONFIG_32BIT || CONFIG_MIPS32_O32 */

 CONFIG_MIPS32_N32 */

 CONFIG_64BIT */

 when I and D space are separate, these will need to be fixed. */

 read word at location addr. */

 Read the word at location addr in the USER area. */

 Default return value. */

 FP not yet used */

				/*

				 * The odd registers are actually the high

				 * order bits of the values stored in the even

				 * registers.

 implementation / version register */

 when I and D space are separate, this will have to be fixed. */

 write the word at location addr. */

 System call number may have been changed */

				/*

				 * The odd registers are actually the high

				 * order bits of the values stored in the even

				 * registers.

 The rest are not allowed. */

/*

 * Notification of system call entry/exit

 * - triggered by current->work.syscall_trace

	/*

	 * Negative syscall numbers are mistaken for rejected syscalls, but

	 * won't have had the return value set appropriately, so we do so now.

/*

 * Notification of system call entry/exit

 * - triggered by current->work.syscall_trace

        /*

	 * We may come here right after calling schedule_user()

	 * or do_notify_resume(), in which case we can be in RCU

	 * user mode.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Linux performance counter support for MIPS.

 *

 * Copyright (C) 2010 MIPS Technologies, Inc.

 * Copyright (C) 2011 Cavium Networks, Inc.

 * Author: Deng-Cheng Zhu

 *

 * This code is based on the implementation for ARM, which is in turn

 * based on the sparc64 perf event code and the x86 code. Performance

 * counter access is based on the MIPS Oprofile code. And the callchain

 * support references the code of MIPS stacktrace.c.

 For perf_irq */

 Array of events on this cpu. */

	/*

	 * Set the bit (indexed by the counter number) when the counter

	 * is used for an event.

	/*

	 * Software copy of the control register for each performance counter.

	 * MIPS CPUs vary in performance counters. They use this differently,

	 * and even may not use it.

 The description of MIPS performance events. */

	/*

	 * MIPS performance counters are indexed starting from 0.

	 * CNTR_EVEN indicates the indexes of the counters to be used are

	 * even numbers.

 !CONFIG_CPU_BMIPS5000 */

 CONFIG_CPU_BMIPS5000 */

 Copied from op_model_mipsxx.c */

 !CONFIG_MIPS_PERF_SHARED_TC_COUNTERS */

 CONFIG_MIPS_PERF_SHARED_TC_COUNTERS */

/* 0: Not Loongson-3

 * 1: Loongson-3A1000/3B1000/3B1500

 * 2: Loongson-3A2000/3A3000

 * 3: Loongson-3A4000+

		/*

		 * The counters are unsigned, we must cast to truncate

		 * off the high bits.

	/*

	 * We only need to care the counter mask. The range has been

	 * checked definitely.

		/*

		 * Note that some MIPS perf events can be counted by both

		 * even and odd counters, wheresas many other are only by

		 * even _or_ odd counters. This introduces an issue that

		 * when the former kind of event takes the counter the

		 * latter kind of event wants to use, then the "counter

		 * allocation" for the latter event will fail. In fact if

		 * they can be dynamically swapped, they both feel happy.

		 * But here we leave this issue alone for now.

 Make sure interrupt enabled. */

 Make sure interrupt enabled. */

 enable the counter for the calling thread */

 The counter is processor wide. Set it up to count all TCs. */

		/*

		 * Set up the counter for a particular CPU when event->cpu is

		 * a valid CPU number. Otherwise set up the counter for the CPU

		 * scheduling this thread.

	/*

	 * We do not actually let the counter run. Leave it until start().

 left underflowed by more than period. */

 left underflowed by less than period. */

 Set the period for the event. */

 Enable the event. */

 We are working on a local event. */

 To look for a free counter for this event. */

	/*

	 * If there is an event in the counter we are going to use then

	 * make sure it is disabled.

 Propagate our changes to the userspace mapping. */

 Don't read disabled counters! */

/*

 * MIPS performance counters can be per-TC. The control registers can

 * not be directly accessed across CPUs. Hence if we want to do global

 * control, we need cross CPU calls. on_each_cpu() can help us, but we

 * can not make sure this function is called with interrupts enabled. So

 * here we pause local counters and then grab a rwlock and leave the

 * counters on other CPUs alone. If any counter interrupt raises while

 * we own the write lock, simply pause local counters on that CPU and

 * spin in the handler. Also we know we won't be switched to another

 * CPU after pausing local counters and before grabbing the lock.

 Request my own irq handler. */

		/*

		 * We are sharing the irq number with the timer interrupt.

/*

 * mipsxx/rm9000/loongson2 have different performance counters, they have

 * specific low-level init routines.

		/*

		 * We must not call the destroy function with interrupts

		 * disabled.

 does not support taken branch sampling */

/*

 * Top 8 bits for range, next 16 bits for cntr_mask, lowest 8 bits for

 * event_id.

 CONFIG_MIPS_MT_SMP */

 This is needed by specific irq handlers in perf_event_*.c */

 24K/34K/1004K/interAptiv/loongson1 cores share the same event map. */

 74K/proAptiv core has different branch event code. */

 These only count dcache, not icache */

 24K/34K/1004K/interAptiv/loongson1 cores share the same cache event map. */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

		/*

		 * Note that MIPS has only "hit" events countable for

		 * the prefetch operation.

 Using the same code for *HW_BRANCH* */

 74K/proAptiv core has completely different cache event map. */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

		/*

		 * Note that MIPS has only "hit" events countable for

		 * the prefetch operation.

/*

 * 74K core does not have specific DTLB events. proAptiv core has

 * "speculative" DTLB events which are numbered 0x63 (even/odd) and

 * not included here. One can use raw events if really needed.

 Using the same code for *HW_BRANCH* */

 Can't distinguish read & write */

 Conditional branches / mispredicted */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

 Using the same code for *HW_BRANCH* */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

 Using the same code for *HW_BRANCH* */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

 Using the same code for *HW_BRANCH* */

 BMIPS5000 */

	/*

	 * Like some other architectures (e.g. ARM), the performance

	 * counters don't differentiate between read and write

	 * accesses/misses, so this isn't strictly correct, but it's the

	 * best we can do. Writes and reads get combined.

		/*

		 * Note that MIPS has only "hit" events countable for

		 * the prefetch operation.

 Using the same code for *HW_BRANCH* */

	/*

	 * Only general DTLB misses are counted use the same event for

	 * read and write.

 Returning MIPS event descriptor for generic perf event. */

 We are working on the global raw event. */

 The event type is not (yet) supported. */

	/*

	 * We allow max flexibility on how each individual counter shared

	 * by the single CPU operates (the mode exclusion and the range).

 MIPS kernel mode: KSU == 00b || EXL == 1 || ERL == 1 */

	/*

	 * The event can belong to another cpu. We do not assign a local

	 * counter for it for now.

	/*

	 * First we pause the local counters, so that when we are locked

	 * here, the counters are all paused. When it gets locked due to

	 * perf_disable(), the timer interrupt handler will be delayed.

	 *

	 * See also mipsxx_pmu_start().

	/*

	 * Do all the work for the pending perf events. We can do this

	 * in here because the performance counter interrupt is a regular

	 * interrupt, not NMI.

 24K */

 34K */

 74K */

 proAptiv */

 P5600 */

 1004K */

 interAptiv */

 The P/V/T info is not provided for "(b) == 38" in SUM, assume P. */

 BMIPS5000 */

/*

 * For most cores the user can use 0-255 raw events, where 0-127 for the events

 * of even counters, and 128-255 for odd counters. Note that bit 7 is used to

 * indicate the even/odd bank selector. So, for example, when user wants to take

 * the Event Num of 15 for odd counters (by referring to the user manual), then

 * 128 needs to be added to 15 as the input for the event config, i.e., 143 (0x8F)

 * to be used.

 *

 * Some newer cores have even more events, in which case the user can use raw

 * events 0-511, where 0-255 are for the events of even counters, and 256-511

 * are for odd counters, so bit 8 is used to indicate the even/odd bank selector.

 currently most cores have 7-bit event numbers */

		/*

		 * This is actually doing nothing. Non-multithreading

		 * CPUs will not check and calculate the range.

 8-bit event numbers */

 8-bit event numbers */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Some parts derived from x86 version of this file.

 *

 * Copyright (C) 2013 Cavium, Inc.

 Must be CONFIG_64BIT */

 CONFIG_32BIT */

 Sign extend if 32-bit. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  GT641xx IRQ routines.

 *

 *  Copyright (C) 2007	Yoichi Yuasa <yuasa@linux-mips.org>

	/*

	 * bit0 : logical or of all the interrupt bits.

	 * bit30: logical or of bits[29:26,20:1].

	 * bit31: logical or of bits[25:1].

	/*

	 * bit0 : logical or of all the interrupt bits.

	 * bit30: logical or of bits[29:26,20:1].

	 * bit31: logical or of bits[25:1].

 SPDX-License-Identifier: GPL-2.0

/**

 * arch_uprobe_analyze_insn - instruction analysis including validity and fixups.

 * @mm: the probed address space.

 * @arch_uprobe: the probepoint information.

 * @addr: virtual address at which to install the probepoint

 * Return 0 on success or a -ve number on error.

	/*

	 * For the time being this also blocks attempts to use uprobes with

	 * MIPS16 and microMIPS.

 NOP  */

/**

 * is_trap_insn - check if the instruction is a trap variant

 * @insn: instruction to be checked.

 * Returns true if @insn is a trap variant.

 *

 * This definition overrides the weak definition in kernel/events/uprobes.c.

 * and is needed for the case where an architecture has multiple trap

 * instructions (like PowerPC or MIPS).  We treat BREAK just like the more

 * modern conditional trap instructions.

 Yes, really ...  */

/*

 * arch_uprobe_pre_xol - prepare to execute out of line.

 * @auprobe: the probepoint information.

 * @regs: reflects the saved user state of current task.

	/*

	 * Now find the EPC where to resume after the breakpoint has been

	 * dealt with.  This may require emulation of a branch.

/*

 * If xol insn itself traps and generates a signal(Say,

 * SIGILL/SIGSEGV/etc), then detect the case where a singlestepped

 * instruction jumps back to its own address. It is assumed that anything

 * like do_page_fault/do_trap/etc sets thread.trap_nr != -1.

 *

 * arch_uprobe_pre_xol/arch_uprobe_post_xol save/restore thread.trap_nr,

 * arch_uprobe_xol_was_trapped() simply checks that ->trap_nr is not equal to

 * UPROBE_TRAP_NR == -1 set by arch_uprobe_pre_xol().

 regs == NULL is a kernel bug */

 We are only interested in userspace traps */

/*

 * This function gets called when XOL instruction either gets trapped or

 * the thread has a fatal signal. Reset the instruction pointer to its

 * probed address for the potential restart or for post mortem analysis.

 Replace the return address with the trampoline address */

/**

 * set_swbp - store breakpoint at a given address.

 * @auprobe: arch specific probepoint information.

 * @mm: the probed process address space.

 * @vaddr: the virtual address to insert the opcode.

 *

 * For mm @mm, store the breakpoint instruction at @vaddr.

 * Return 0 (success) or a negative errno.

 *

 * This version overrides the weak version in kernel/events/uprobes.c.

 * It is required to handle MIPS16 and microMIPS.

 Initialize the slot */

/**

 * uprobe_get_swbp_addr - compute address of swbp given post-swbp regs

 * @regs: Reflects the saved state of the task after it has hit a breakpoint

 * instruction.

 * Return the address of the breakpoint instruction.

 *

 * This overrides the weak version in kernel/events/uprobes.c.

/*

 * See if the instruction can be emulated.

 * Returns true if instruction was emulated, false otherwise.

 *

 * For now we always emulate so this function just returns false.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2006, 07 by Ralf Baechle (ralf@linux-mips.org)

 *

 * Symmetric Uniprocessor (TM) Support

/*

 * Send inter-processor interrupt

/*

 *  After we've done initial boot, this function is called to allow the

 *  board code to clean up state, if needed

/*

 * Firmware CPU startup hook

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2005, 06 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2013 Imagination Technologies Ltd.

 call when we have the address of the shared structure from the SP side. */

 notifications */

 force a reload of rtlx */

 wake up any sleeping rtlx_open's */

 data available to read? */

		/*

		 * Never fill the buffer completely, so indexes are always

		 * equal if empty and only empty, or !equal if data available

 find out how much in total */

 then how much from the read pointer onwards */

 and if there is anything left at the beginning of the buffer */

 total number of bytes to copy */

 first bit from write pointer to the end of the buffer, or count */

 if there's any left copy to the beginning of the buffer */

 data available to read? */

 space to write */

 data available? */

 -EAGAIN makes 'cat' whine */

 any space left... */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1991, 1992  Linus Torvalds

 * Copyright (C) 1994 - 2000, 2006  Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2016, Imagination Technologies Ltd.

/*

 * Including <asm/unistd.h> would give use the 64-bit syscall numbers ...

 argument save space for o32 */

 Was: signal trampoline */

 mask last for extensibility */

 argument save space for o32 */

 Was: signal trampoline */

	/*

	 * Save FPU state to signal context.  Signal handler

	 * will "inherit" current FPU state.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Arguments to signal handler:

	 *

	 *   a0 = signal number

	 *   a1 = 0 (should be cause)

	 *   a2 = pointer to struct sigcontext

	 *

	 * $25 and c0_epc point to the signal handler, $29 points to the

	 * struct sigframe.

	/*

	 * Don't let your children do this ...

 no outputs */

 Unreached */

 Convert (siginfo_t -> compat_siginfo_t) and copy to user. */

 Create the ucontext.	 */

	/*

	 * Arguments to signal handler:

	 *

	 *   a0 = signal number

	 *   a1 = 0 (should be cause)

	 *   a2 = pointer to ucontext

	 *

	 * $25 and c0_epc point to the signal handler, $29 points to

	 * the struct rt_sigframe32.

/*

 * o32 compatibility on 64-bit kernels, without DSP ASE

	/*

	 * Don't let your children do this ...

 no outputs */

 Unreached */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1992 Ross Biro

 * Copyright (C) Linus Torvalds

 * Copyright (C) 1994, 95, 96, 97, 98, 2000 Ralf Baechle

 * Copyright (C) 1996 David S. Miller

 * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999 MIPS Technologies, Inc.

 * Copyright (C) 2000 Ulf Carlsson

 *

 * At this time Linux/MIPS64 only supports syscall tracing, even for 32-bit

 * binaries.

/*

 * Tracing a 32-bit process with a 64-bit strace and vice versa will not

 * work.  I don't know how to fix this.

	/*

	 * Read 4 bytes of the other process' storage

	 *  data is a pointer specifying where the user wants the

	 *	4 bytes copied into

	 *  addr is a pointer in the user's storage that contains an 8 byte

	 *	address in the other process of the 4 bytes that is to be read

	 * (this is run in a 32-bit process looking at a 64-bit process)

	 * when I and D space are separate, these will need to be fixed.

 Get the addr in the other process that we want to read */

 Read the word at location addr in the USER area. */

 Default return value. */

 FP not yet used */

				/*

				 * The odd registers are actually the high

				 * order bits of the values stored in the even

				 * registers.

 implementation / version register */

 CONFIG_MIPS_FP_SUPPORT */

	/*

	 * Write 4 bytes into the other process' storage

	 *  data is the 4 bytes that the user wants written

	 *  addr is a pointer in the user's storage that contains an

	 *	8 byte address in the other process where the 4 bytes

	 *	that is to be written

	 * (this is run in a 32-bit process looking at a 64-bit process)

	 * when I and D space are separate, these will need to be fixed.

 Get the addr in the other process that we want to write into */

 System call number may have been changed */

 FP not yet used  */

				/*

				 * The odd registers are actually the high

				 * order bits of the values stored in the even

				 * registers.

 CONFIG_MIPS_FP_SUPPORT */

 The rest are not allowed. */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Copyright (C) 1995, 1996, 2001  Ralf Baechle

 *  Copyright (C) 2001, 2004  MIPS Technologies, Inc.

 *  Copyright (C) 2004	Maciej W. Rozycki

/*

 * No lock; only written during early bootup by CPU 0.

	/*

	 * For the first processor also print the system type

 SPDX-License-Identifier: GPL-2.0

/*

 * General MIPS MT support routines, usable in AP/SP and SMVP.

 * Copyright (C) 2005 Mips Technologies, Inc

/*

 * Dump new MIPS MT state for the core. Does not leave TCs halted.

 * Takes an argument which taken to be a pre-call MVPControl value.

 Next VPE */

 Are we dumping ourself?  */

 Then we're not halted, and mustn't be */

 And pre-dump TCStatus is flags */

		/*

		 * Configure ITC mapping.  This code is very

		 * specific to the 34K core family, which uses

		 * a special mode bit ("ITC") in the ErrCtl

		 * register to enable access to ITC control

		 * registers via cache "tag" operations.

 ErrCtl register is known as "ecc" to Linux */

 Read "cache tag" for Dcache pseudo-index 8 */

 Set for 128 byte pitch of ITC cells */

 Stage in Tag register */

 Write out to ITU with CACHE op */

 Now set base address, and turn ITC on with 0x1 bit */

 Write out to ITU with CACHE op */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Kernel Probes (KProbes)

 *  arch/mips/kernel/kprobes.c

 *

 *  Copyright 2006 Sony Corp.

 *  Copyright 2010 Cavium Networks

 *

 *  Some portions copied from the powerpc version.

 *

 *   Copyright (C) IBM Corporation, 2002, 2004

/*

 * insn_has_ll_or_sc function checks whether instruction is ll or sc

 * one; putting breakpoint on top of atomic ll/sc pair is bad idea;

 * so we need to prevent it and refuse kprobes insertion for such

 * instructions; cannot do much about breakpoint in the middle of

 * ll/sc pair; it is upto user to avoid those places

 insn: must be on special executable page on mips. */

	/*

	 * In the kprobe->ainsn.insn[] array we store the original

	 * instruction at index zero and a break trap instruction at

	 * index one.

	 *

	 * On MIPS arch if the instruction at probed address is a

	 * branch instruction, we need to execute the instruction at

	 * Branch Delayslot (BD) at the time of probe hit. As MIPS also

	 * doesn't have single stepping support, the BD instruction can

	 * not be executed in-line and it would be executed on SSOL slot

	 * using a normal breakpoint instruction in the next slot.

	 * So, read the instruction and save it for later execution.

/**

 * evaluate_branch_instrucion -

 *

 * Evaluate the branch instruction at probed address during probe hit. The

 * result of evaluation would be the updated epc. The insturction in delayslot

 * would actually be single stepped using a normal breakpoint) on SSOL slot.

 *

 * The result is also saved in the kprobe control block for later use,

 * in case we need to execute the delayslot instruction. The latter will be

 * false for NOP instruction in dealyslot and the branch-likely instructions

 * when the branch is taken. And for those cases we set a flag as

 * SKIP_DELAYSLOT in the kprobe control block

 single step inline if the instruction is a break */

/*

 * Called after single-stepping.  p->addr is the address of the

 * instruction whose first byte has been replaced by the "break 0"

 * instruction.	 To avoid the SMP problems that can occur when we

 * temporarily put back the original opcode to single-step, we

 * single-stepped a copy of the instruction.  The address of this

 * copy is p->ainsn.insn.

 *

 * This function prepares to return from the post-single-step

 * breakpoint trap. In case of branch instructions, the target

 * epc to be restored.

	/*

	 * We don't want to be preempted for the entire

	 * duration of kprobe processing

 Check we're not actually recursing */

			/*

			 * We have reentered the kprobe_handler(), since

			 * another probe was hit while within the handler.

			 * We here save the original kprobes variables and

			 * just single step on the instruction of the new probe

			 * without calling any user handlers.

			/*

			 * The breakpoint instruction was removed by

			 * another cpu right after we hit, no further

			 * handling of this interrupt is appropriate

			/*

			 * The breakpoint instruction was removed right

			 * after we hit it.  Another cpu has removed

			 * either a probepoint or a debugger breakpoint

			 * at this address.  In either case, no further

			 * handling of this interrupt is appropriate.

 Not one of ours: let kernel handle it */

 handler has already set things up, so skip ss setup */

 Restore back the original saved kprobes variables and continue. */

/*

 * Wrapper routine for handling exceptions.

 kprobe_running() needs smp_processor_id() */

/*

 * Function return probe trampoline:

 *	- init_kprobes() establishes a probepoint here

 *	- When the probed function returns, this probe causes the

 *	  handlers to fire

 Keep the assembler from reordering and placing JR here. */

 Replace the return addr with trampoline addr */

/*

 * Called when the probe at kretprobe trampoline is hit

	/*

	 * By returning a non-zero value, we are telling

	 * kprobe_handler() that we don't want the post_handler

	 * to run (and have re-enabled preemption)

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  GT641xx clockevent routines.

 *

 *  Copyright (C) 2007	Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0

/**

 * copy_oldmem_page - copy one page from "oldmem"

 * @pfn: page frame number to be copied

 * @buf: target memory address for the copy; this can be in kernel address

 *	space or user address space (see @userbuf)

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page (based on pfn) to begin the copy

 * @userbuf: if set, @buf is in user address space, use copy_to_user(),

 *	otherwise @buf is in kernel address space, use memcpy().

 *

 * Copy a page from "oldmem". For this page, there is no pte mapped

 * in the current kernel.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 *  Copyright (C) 2001 Rusty Russell.

 *  Copyright (C) 2003, 2004 Ralf Baechle (ralf@linux-mips.org)

 *  Copyright (C) 2005 Thiemo Seufer

	/*

	 * We cannot relocate this one now because we don't know the value of

	 * the carry we need to add.  Save the information, and let LO16 do the

	 * actual relocation.

 Sign extend the addend we extract from the lo insn.	*/

			/*

			 * The value for the HI16 had best be the same.

			/*

			 * Do the HI16 relocation.  Note that we actually don't

			 * need to know anything about the LO16 itself, except

			 * where to find the low 16 bits of the addend needed

			 * by the LO16.

			/*

			 * Account for the sign extension that will happen in

			 * the low bits.

	/*

	 * Ok, we're done with the HI16 relocs.	 Now deal with the LO16.

 retrieve & sign extend implicit addend if any */

 check the sign bit onwards are identical - ie. we didn't overflow */

/**

 * reloc_handler() - Apply a particular relocation to a module

 * @type: type of the relocation to apply

 * @me: the module to apply the reloc to

 * @location: the address at which the reloc is to be applied

 * @base: the existing value at location for REL-style; 0 for RELA-style

 * @v: the value of the reloc, with addend for RELA-style

 * @rela: indication of is this a RELA (true) or REL (false) relocation

 *

 * Each implemented relocation function applies a particular type of

 * relocation to the module @me. Relocs that may be found in either REL or RELA

 * variants can be handled by making use of the @base & @v parameters which are

 * set to values which abstract the difference away from the particular reloc

 * implementations.

 *

 * Return: 0 upon success, else -ERRNO

 This is where to make the change */

 This is the symbol it is referring to */

 Ignore unresolved weak symbol */

	/*

	 * Normally the hi16 list should be deallocated at this point. A

	 * malformed binary however could contain a series of R_MIPS_HI16

	 * relocations not followed by a R_MIPS_LO16 relocation, or if we hit

	 * an error processing a reloc we might have gotten here before

	 * reaching the R_MIPS_LO16. In either case, free up the list and

	 * return an error.

 CONFIG_MODULES_USE_ELF_RELA */

 Given an address, look for it in the module exception tables. */

	/* Now, if we found one, we are running inside it now, hence

 Put in dbe list if necessary. */

 Make jump label nops. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * A gpio chip driver for TXx9 SoCs

 *

 * Copyright (C) 2008 Atsushi Nemoto <anemo@mba.ocn.ne.jp>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Imagination Technologies

 * Author: Alex Smith <alex.smith@imgtec.com>

 Kernel-provided data used by the VDSO. */

/*

 * Mapping for the VDSO data/GIC pages. The real pages are mapped manually, as

 * what we map and where within the area they are mapped is determined at

 * runtime.

 Skip the delay slot emulation page */

 Map delay slot emulation page */

	/*

	 * Determine total area size. This includes the VDSO data itself, the

	 * data page, and the GIC user page if present. Always create a mapping

	 * for the GIC user area if the GIC is present regardless of whether it

	 * is the current clocksource, in case it comes into use later on. We

	 * only map a page even though the total area is 64K, as we only need

	 * the counter registers at the start.

	/*

	 * Find a region that's large enough for us to perform the

	 * colour-matching alignment below.

	/*

	 * If we suffer from dcache aliasing, ensure that the VDSO data page

	 * mapping is coloured the same as the kernel's mapping of that memory.

	 * This ensures that when the kernel updates the VDSO data userland

	 * will observe it without requiring cache invalidations.

 Map GIC user page. */

 Map data page. */

 Map VDSO image. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2003, 2004, 2007  Maciej W. Rozycki

	/*

	 * We want the multiply and the shift to be isolated from the

	 * rest of the code to disable gcc optimizations.  Hence the

	 * asm statements that execute nothing, but make gcc not know

	 * what the values of m1, m2 and s are and what lv2 and p are

	 * used for.

	/*

	 * The following code leads to a wrong result of the first

	 * dsll32 when executed on R4000 rev. 2.2 or 3.0 (PRId

	 * 00000422 or 00000430, respectively).

	 *

	 * See "MIPS R4000PC/SC Errata, Processor Revision 2.2 and

	 * 3.0" by MIPS Technologies, Inc., errata #16 and #28 for

	 * details.  I got no permission to duplicate them here,

	 * sigh... --macro

	/*

	 * The trailing nop is needed to fulfill the two-instruction

	 * requirement between reading hi/lo and staring a mult/div.

	 * Leaving it out may cause gas insert a nop itself breaking

	 * the desired alignment of the next chunk.

	/* We have to use single integers for m1 and m2 and a double

	 * one for p to be sure the mulsidi3 gcc's RTL multiplication

	 * instruction has the workaround applied.  Older versions of

	 * gcc have correct umulsi3 and mulsi3, but other

	 * multiplication variants lack the workaround.

	/*

	 * Testing discovered false negatives for certain code offsets

	 * into cache lines.  Hence we test all possible offsets for

	 * the worst assumption of an R4000 I-cache line width of 32

	 * bytes.

	 *

	 * We can't use a loop as alignment directives need to be

	 * immediates.

	/*

	 * The following code fails to trigger an overflow exception

	 * when executed on R4000 rev. 2.2 or 3.0 (PRId 00000422 or

	 * 00000430, respectively).

	 *

	 * See "MIPS R4000PC/SC Errata, Processor Revision 2.2 and

	 * 3.0" by MIPS Technologies, Inc., erratum #23 for details.

	 * I got no permission to duplicate it here, sigh... --macro

	/*

	 * The following code leads to a wrong result of daddiu when

	 * executed on R4400 rev. 1.0 (PRId 00000440).

	 *

	 * See "MIPS R4400PC/SC Errata, Processor Revision 1.0" by

	 * MIPS Technologies, Inc., erratum #7 for details.

	 *

	 * According to "MIPS R4000PC/SC Errata, Processor Revision

	 * 2.2 and 3.0" by MIPS Technologies, Inc., erratum #41 this

	 * problem affects R4000 rev. 2.2 and 3.0 (PRId 00000422 and

	 * 00000430, respectively), too.  Testing failed to trigger it

	 * so far.

	 *

	 * I got no permission to duplicate the errata here, sigh...

	 * --macro

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2000,2001,2004 Broadcom Corporation

/*

 * The general purpose timer ticks at 1MHz independent if

 * the rest of the system

 Stop the timer until we actually program a shot */

 ACK interrupt */

 Only have 4 general purpose timers */

	/*

	 * Map the timer interrupt to IP[4] of this cpu

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  8250/16550-type serial ports prom_putchar()

 *

 *  Copyright (C) 2010  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2014 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

/*

 * cps_nc_entry_fn - type of a generated non-coherent state entry function

 * @online: the count of online coupled VPEs

 * @nc_ready_count: pointer to a non-coherent mapping of the core ready_count

 *

 * The code entering & exiting non-coherent states is generated at runtime

 * using uasm, in order to ensure that the compiler cannot insert a stray

 * memory access at an unfortunate time and to allow the generation of optimal

 * core-specific code particularly for cache routines. If coupled_coherence

 * is non-zero and this is the entry function for the CPS_PM_NC_WAIT state,

 * returns the number of VPEs that were in the wait state at the point this

 * VPE left it. Returns garbage if coupled_coherence is zero or this is not

 * the entry function for CPS_PM_NC_WAIT.

/*

 * The entry point of the generated non-coherent idle state entry/exit

 * functions. Actually per-core rather than per-CPU.

 Bitmap indicating which states are supported by the system */

/*

 * Indicates the number of coupled VPEs ready to operate in a non-coherent

 * state. Actually per-core rather than per-CPU.

 Indicates online CPUs coupled with the current CPU */

/*

 * Used to synchronize entry to deep idle states. Actually per-core rather

 * than per-CPU.

 Saved CPU state across the CPS_PM_POWER_GATED state */

 A somewhat arbitrary number of labels & relocs for uasm */

	/*

	 * This function is effectively the same as

	 * cpuidle_coupled_parallel_barrier, which can't be used here since

	 * there's no cpuidle device.

 Check that there is an entry function for this state */

 Calculate which coupled CPUs (VPEs) are online */

 Setup the VPE to run mips_cps_pm_restore when started again */

 Power gating relies upon CPS SMP */

 Indicate that this CPU might not be coherent */

 Create a non-coherent mapping of the core ready_count */

 Ensure ready_count is zero-initialised before the assembly runs */

 Run the generated entry code */

 Remove the non-coherent mapping of ready_count */

 Indicate that this CPU is definitely coherent */

	/*

	 * If this VPE is the first to leave the non-coherent wait state then

	 * it needs to wake up any coupled VPEs still running their wait

	 * instruction so that they return to cpuidle, which can then complete

	 * coordination between the coupled VPEs & provide the governor with

	 * a chance to reflect on the length of time the VPEs were in the

	 * idle state.

 If the cache isn't present this function has it easy */

 Load base address */

 Calculate end address */

 Start of cache op loop */

 Generate the cache ops */

 Update the base address */

 Loop if we haven't reached the end address yet */

	/*

	 * Determine whether this CPU requires an FSB flush, and if so which

	 * performance counter/event reflect stalls due to a full FSB.

 Newer proAptiv cores don't require this workaround */

 On older ones it's unavailable */

 Assume that the CPU does not need this workaround */

	/*

	 * Ensure that the fill/store buffer (FSB) is not holding the results

	 * of a prefetch, since if it is then the CPC sequencer may become

	 * stuck in the D3 (ClrBus) state whilst entering a low power state.

 Preserve perf counter setup */

 PerfCtlN */

 PerfCntN */

 Setup perf counter to count FSB full pipeline stalls */

 PerfCtlN */

 PerfCntN */

 Base address for loads */

 Start of clear loop */

 Perform some loads to fill the FSB */

	/*

	 * Invalidate the new D-cache entries so that the cache will need

	 * refilling (via the FSB) if the loop is executed again.

 Barrier ensuring previous cache invalidates are complete */

 Check whether the pipeline stalled due to the FSB being full */

 PerfCntN */

 Loop if it didn't */

 Restore perf counter 1. The count may well now be wrong... */

 PerfCtlN */

 PerfCntN */

 Allocate a buffer to hold the generated code */

 Clear labels & relocs ready for (re)use */

 Power gating relies upon CPS SMP */

		/*

		 * Save CPU state. Note the non-standard calling convention

		 * with the return address placed in v0 to avoid clobbering

		 * the ra register before it is saved.

	/*

	 * Load addresses of required CM & CPC registers. This is done early

	 * because they're needed in both the enable & disable coherence steps

	 * but in the coupled case the enable step will only run on one VPE.

 Increment ready_count */

 Barrier ensuring all CPUs see the updated r_nc_count value */

		/*

		 * If this is the last VPE to become ready for non-coherence

		 * then it should branch below.

			/*

			 * Otherwise this is not the last VPE to become ready

			 * for non-coherence. It needs to wait until coherence

			 * has been disabled before proceeding, which it will do

			 * by polling for the top bit of ready_count being set.

			/*

			 * The core will lose power & this VPE will not continue

			 * so it can simply halt here.

 Halt the VPE via C0 tchalt register */

 Halt the VP via the CPC VP_STOP register */

	/*

	 * This is the point of no return - this VPE will now proceed to

	 * disable coherence. At this point we *must* be sure that no other

	 * VPE within the core will interfere with the L1 dcache.

 Invalidate the L1 icache */

 Writeback & invalidate the L1 dcache */

 Barrier ensuring previous cache invalidates are complete */

		/*

		* Disable all but self interventions. The load from COHCTL is

		* defined by the interAptiv & proAptiv SUMs as ensuring that the

		*  operation resulting from the preceding store is complete.

 Barrier to ensure write to coherence control is complete */

 Disable coherence */

 Determine the CPC command to issue */

 Issue the CPC command */

 If anything goes wrong just hang */

			/*

			 * There's no point generating more code, the core is

			 * powered down & if powered back up will run from the

			 * reset vector not from here.

 Barrier to ensure write to CPC command is complete */

		/*

		 * At this point it is safe for all VPEs to proceed with

		 * execution. This VPE will set the top bit of ready_count

		 * to indicate to the other VPEs that they may continue.

		/*

		 * VPEs which did not disable coherence will continue

		 * executing, after coherence has been disabled, from this

		 * point.

 Now perform our wait */

	/*

	 * Re-enable coherence. Note that for CPS_PM_NC_WAIT all coupled VPEs

	 * will run this. The first will actually re-enable coherence & the

	 * rest will just be performing a rather unusual nop.

 Barrier to ensure write to coherence control is complete */

 Decrement ready_count */

 Barrier ensuring all CPUs see the updated r_nc_count value */

		/*

		 * At this point it is safe for all VPEs to proceed with

		 * execution. This VPE will set the top bit of ready_count

		 * to indicate to the other VPEs that they may continue.

		/*

		 * This core will be reliant upon another core sending a

		 * power-up command to the CPC in order to resume operation.

		 * Thus an arbitrary VPE can't trigger the core leaving the

		 * idle state and the one that disables coherence might as well

		 * be the one to re-enable it. The rest will continue from here

		 * after that has been done.

 Barrier ensuring all CPUs see the updated r_nc_count value */

 The core is coherent, time to return to C code */

 Ensure the code didn't exceed the resources allocated for it */

 Patch branch offsets */

 Flush the icache */

		/*

		 * If we're attempting to suspend the system and power down all

		 * of the cores, the JTAG detect bit indicates that the CPC will

		 * instead put the cores into clock-off state. In this state

		 * a connected debugger can cause the CPU to attempt

		 * interactions with the powered down system. At best this will

		 * fail. At worst, it can hang the NoC, requiring a hard reset.

		 * To avoid this, just block system suspend if a JTAG probe

		 * is detected.

 A CM is required for all non-coherent states */

	/*

	 * If interrupts were enabled whilst running a wait instruction on a

	 * non-coherent core then the VPE may end up processing interrupts

	 * whilst non-coherent. That would be bad.

 Detect whether a CPC is present */

 Detect whether clock gating is implemented */

 Power gating is available with CPS SMP & any CPC */

 SPDX-License-Identifier: GPL-2.0

 spin */

 spin */

 spin */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 by Ralf Baechle

	/*

	 * Older QEMUs have a broken implementation of RDHWR for the CP0 count

	 * which always returns a constant value. Try to identify this and don't

	 * use it in the VDSO if it is broken. This workaround can be removed

	 * once the fix has been in QEMU stable for a reasonable amount of time.

 !CONFIG_CPU_FREQ */

 Calculate a somewhat reasonable rating value */

	/*

	 * R2 onwards makes the count accessible to user mode so it can be used

	 * by the VDSO (HWREna is configured by configure_hwrena()).

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001, 06 by Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2001 MIPS Technologies, Inc.

/*

 * Urgs ...  Too many MIPS machines to handle this in a generic way.

 * So handle all using function pointers to machine specific

 * functions.

	/*

	 * We're hanging the system so we don't want to be interrupted anymore.

	 * Any interrupt handlers that ran would at best be useless & at worst

	 * go awry because the system isn't in a functional state.

	/*

	 * Mask all interrupts, giving us a better chance of remaining in the

	 * low power wait state.

			/*

			 * We know that the wait instruction is supported so

			 * make use of it directly, leaving interrupts

			 * disabled.

			/*

			 * Try the cpu_wait() callback. This isn't ideal since

			 * it'll re-enable interrupts, but that ought to be

			 * harmless given that they're all masked.

			/*

			 * We're going to burn some power running round the

			 * loop, but we don't really have a choice. This isn't

			 * a path we should expect to run for long during

			 * typical use anyway.

		/*

		 * In most modern MIPS CPUs interrupts will cause the wait

		 * instruction to graduate even when disabled, and in some

		 * cases even when masked. In order to prevent a timer

		 * interrupt from continuously taking us out of the low power

		 * wait state, we clear any pending timer interrupt here.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Code to handle x86 style IRQs plus some generic interrupt stuff.

 *

 * Copyright (C) 1992 Linus Torvalds

 * Copyright (C) 1994 - 2000 Ralf Baechle

/*

 * 'what should we do if we get a hw irq event on an illegal vector'.

 * each architecture has to answer this themselves.

	/*

	 * Check for stack overflow: is there less than STACK_WARN free?

	 * STACK_WARN is defined as 1/8 of THREAD_SIZE by default.

/*

 * do_IRQ handles all normal device IRQ's (the special

 * SMP cross-CPU interrupts have their own specific

 * handlers).

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2013 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 CM3 Tag ECC transaction type */

 CM3 Tag ECC command type */

 CM3 Tag ECC command group */

 Check the CMGCRBase register is implemented */

 Read the address from CMGCRBase */

	/*

	 * If the L2-only sync region is already enabled then leave it at it's

	 * current location.

 Default to following the CM */

 L2-only sync was introduced with CM major revision 6 */

 Find a location for the L2 sync region */

 Set the region base address & enable it */

 Map the region */

	/*

	 * No need to probe again if we have already been

	 * here before.

 sanity check that we're looking at a CM */

 set default target to memory */

 disable CM regions */

 probe for an L2-only sync region */

 determine register width for this CM */

		/*

		 * We need to disable interrupts in SMP systems in order to

		 * ensure that we don't interrupt the caller with code which

		 * may modify the redirect register. We do so here in a

		 * slightly obscure way by using a spin lock, since this has

		 * the neat property of also catching any nested uses of

		 * mips_cm_lock_other() leading to a deadlock or a nice warning

		 * with lockdep enabled.

		/*

		 * We only have a GCR_CL_OTHER per core in systems with

		 * CM 2.5 & older, so have to ensure other VP(E)s don't

		 * race with us.

	/*

	 * Ensure the core-other region reflects the appropriate core &

	 * VP before any accesses to it occur.

 CM2 */

 glob state & sresp together */

 CM3 */

 Used by cause == {1,2,3} */

 Tag ECC */

 reprime cause register */

/*

 * Handle unaligned accesses by emulation.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 1998, 1999, 2002 by Ralf Baechle

 * Copyright (C) 1999 Silicon Graphics, Inc.

 * Copyright (C) 2014 Imagination Technologies Ltd.

 *

 * This file contains exception handler for address error exception with the

 * special capability to execute faulting instructions in software.  The

 * handler does not try to handle the case when the program counter points

 * to an address not aligned to a word boundary.

 *

 * Putting data to unaligned addresses is a bad practice even on Intel where

 * only the performance is affected.  Much worse is that such code is non-

 * portable.  Due to several programs that die on MIPS due to alignment

 * problems I decided to implement this handler anyway though I originally

 * didn't intend to do this at all for user code.

 *

 * For now I enable fixing of address errors by default to make life easier.

 * I however intend to disable this somewhen in the future when the alignment

 * problems with user programs have been fixed.	 For programmers this is the

 * right way to go.

 *

 * Fixing address errors is a per process option.  The option is inherited

 * across fork(2) and execve(2) calls.	If you really want to use the

 * option in your user programs - I discourage the use of the software

 * emulation strongly - use the following code in your userland stuff:

 *

 * #include <sys/sysmips.h>

 *

 * ...

 * sysmips(MIPS_FIXADE, x);

 * ...

 *

 * The argument x is 0 for disabling software emulation, enabled otherwise.

 *

 * Below a little program to play around with this feature.

 *

 * #include <stdio.h>

 * #include <sys/sysmips.h>

 *

 * struct foo {

 *	   unsigned char bar[8];

 * };

 *

 * main(int argc, char *argv[])

 * {

 *	   struct foo x = {0, 1, 2, 3, 4, 5, 6, 7};

 *	   unsigned int *p = (unsigned int *) (x.bar + 3);

 *	   int i;

 *

 *	   if (argc > 1)

 *		   sysmips(MIPS_FIXADE, atoi(argv[1]));

 *

 *	   printf("*p = %08lx\n", *p);

 *

 *	   *p = 0xdeadface;

 *

 *	   for(i = 0; i <= 7; i++)

 *	   printf("%02x ", x.bar[i]);

 *	   printf("\n");

 * }

 *

 * Coprocessor loads are not supported; I think this case is unimportant

 * in the practice.

 *

 * TODO: Handle ndc (attempted store to doubleword in uncached memory)

 *	 exception for the R6000.

 *	 A store crossing a page boundary might be executed only partially.

 *	 Undo the partial store in this case.

	/*

	 * This load never faults.

		/*

		 * These are instructions that a compiler doesn't generate.  We

		 * can assume therefore that the code is MIPS-aware and

		 * really buggy.  Emulating these instructions would break the

		 * semantics anyway.

		/*

		 * For these instructions the only way to create an address

		 * error is an attempted access to kernel/supervisor address

		 * space.

		/*

		 * The remaining opcodes are the ones that are really of

		 * interest.

			/*

			 * we can land here only from kernel accessing user

			 * memory, so we need to "switch" the address limit to

			 * user space, so that address check can work properly.

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

 Restore FPU state. */

 Signal if something went wrong. */

 CONFIG_MIPS_FP_SUPPORT */

		/*

		 * If we've reached this point then userland should have taken

		 * the MSA disabled exception & initialised vector context at

		 * some point in the past.

				/*

				 * If we have live MSA context keep track of

				 * whether we get preempted in order to avoid

				 * the register context we load being clobbered

				 * by the live context as it's saved during

				 * preemption. If we don't have live context

				 * then it can't be saved to clobber the value

				 * we load.

				/*

				 * Update the hardware register if it is in use

				 * by the task in this quantum, in order to

				 * avoid having to save & restore the whole

				 * vector context.

			/*

			 * Update from the hardware register if it is in use by

			 * the task in this quantum, in order to avoid having to

			 * save & restore the whole vector context.

 CONFIG_CPU_HAS_MSA */

	/*

	 * COP2 is available to implementor for application specific use.

	 * It's up to applications to register a notifier chain and do

	 * whatever they have to do, including possible sending of signals.

	 *

	 * This instruction has been reallocated in Release 6

		/*

		 * Pheeee...  We encountered an yet unknown instruction or

		 * cache coherence problem.  Die sucker, die ...

 roll back jump/branch */

 Did we have an exception handler installed? */

 Recode table from 16-bit register notation to 32-bit GPR. */

 Recode table from 16-bit STORE register notation to 32-bit GPR. */

	/*

	 * This load never faults.

  Parse instruction to find what to do */

 CONFIG_64BIT */

 CONFIG_64BIT */

 CONFIG_64BIT */

 CONFIG_64BIT */

  LWC2, SWC2, LDC2, SDC2 are not serviced */

  LL,SC,LLD,SCD are not serviced */

 roll back jump/branch */

 restore FPU state */

 If something went wrong, signal */

 CONFIG_MIPS_FP_SUPPORT */

	/*

	 * A 32-bit kernel might be running on a 64-bit processor.  But

	 * if we're on a 32-bit processor and an i-cache incoherency

	 * or race makes us see a 64-bit instruction here the sdl/sdr

	 * would blow up, so for now we don't handle unaligned 64-bit

	 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

	/*

	 * A 32-bit kernel might be running on a 64-bit processor.  But

	 * if we're on a 32-bit processor and an i-cache incoherency

	 * or race makes us see a 64-bit instruction here the sdl/sdr

	 * would blow up, so for now we don't handle unaligned 64-bit

	 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

	/*

	 * A 32-bit kernel might be running on a 64-bit processor.  But

	 * if we're on a 32-bit processor and an i-cache incoherency

	 * or race makes us see a 64-bit instruction here the sdl/sdr

	 * would blow up, so for now we don't handle unaligned 64-bit

	 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

 advance or branch */

 roll back jump/branch */

 Did we have an exception handler installed? */

	/*

	 * This load never faults.

 skip EXTEND instruction */

  skip jump instructions */

  JAL/JALX are 32 bits but have OPCODE in first short int */

 I64 or RI64 instruction */

 I64/RI64 func field check */

 GPRSP */

 SWSP */

 SWGP */

 SHGP */

 LWSP */

 LWGP */

 LHGP */

 LHUGP */

 GPRSP */

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

 actually - MIPS16e_swrasp_func */

		/*

		 * A 32-bit kernel might be running on a 64-bit processor.  But

		 * if we're on a 32-bit processor and an i-cache incoherency

		 * or race makes us see a 64-bit instruction here the sdl/sdr

		 * would blow up, so for now we don't handle unaligned 64-bit

		 * instructions on 32-bit kernels.

 CONFIG_64BIT */

 Cannot handle 64-bit instructions in 32-bit kernel */

		/*

		 * Pheeee...  We encountered an yet unknown instruction or

		 * cache coherence problem.  Die sucker, die ...

 roll back jump/branch */

 Did we have an exception handler installed? */

	/*

	 * Did we catch a fault trying to load an instruction?

	/*

	 * Do branch emulation only if we didn't forward the exception.

	 * This is all so but ugly ...

	/*

	 * Are we running in microMIPS mode?

		/*

		 * Did we catch a fault trying to load an instruction in

		 * 16-bit mode?

	/*

	 * XXX On return from the signal handler we should advance the epc

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MIPS SPRAM support

 *

 * Copyright (C) 2007, 2008 MIPS Technologies, Inc.

/*

 * These definitions are correct for the 24K/34K/74K SPRAM sample

 * implementation. The 4KS interpreted the tags differently...

 errctl access */

/*

 * Different semantics to the set_c0_* function built by __BUILD_SET_C0

 enable SPRAM tag access */

 enable SPRAM tag access */

 enable SPRAM tag access */

	/*

	 * The limit is arbitrary but avoids the loop running away if

	 * the SPRAM tags are implemented differently

 tags may repeat... */

 Align base with size */

 reprogram the base address base address and enable */

 reread the tag */

 FIXME: addresses are Malta specific */

 SPDX-License-Identifier: GPL-2.0

/*

 * asm-offsets.c: Calculate pt_regs and task_struct offsets.

 *

 * Copyright (C) 1996 David S. Miller

 * Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002, 2003 Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 *

 * Kevin Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000 MIPS Technologies, Inc.

 CONFIG_CPU_CAVIUM_OCTEON */

 SPDX-License-Identifier: GPL-2.0

/*

 * Configuration file for O32 and N32 binaries.

 * Note: To be included before lib/vdso/gettimeofday.c

/*

 * In case of a 32 bit VDSO for a 64 bit kernel fake a 32 bit kernel

 * configuration.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Imagination Technologies

 * Author: Alex Smith <alex.smith@imgtec.com>

/*

 * This tool is used to generate the real VDSO images from the raw image. It

 * first patches up the MIPS ABI flags and GNU attributes sections defined in

 * elf.S to have the correct name and type. It then generates a C source file

 * to be compiled into the kernel containing the VDSO image data and a

 * mips_vdso_image struct for it, including symbol offsets extracted from the

 * image.

 *

 * We need to be passed both a stripped and unstripped VDSO image. The stripped

 * image is compiled into the kernel, but we must also patch up the unstripped

 * image's ABI flags sections so that it can be installed and used for

 * debugging.

 Define these in case the system elf.h is not new enough to have them. */

 Symbols the kernel requires offsets for. */

/*

 * Include genvdso.h twice with ELF_BITS defined differently to get functions

 * for both ELF32 and ELF64.

 ELF32/64 header formats are the same for the bits we're checking. */

 Patch both the VDSOs' ABI flags sections. */

 Automatically generated - do not edit */\n");

 Write out the stripped VDSO data. */

 Preallocate a page array. */

 Calculate and write symbol offsets to <output file> */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MIPS64 and compat userspace implementations of gettimeofday()

 * and similar.

 *

 * Copyright (C) 2015 Imagination Technologies

 * Copyright (C) 2018 ARM Limited

 *

/*

 * This is behind the ifdef so that we don't provide the symbol when there's no

 * possibility of there being a usable clocksource, because there's nothing we

 * can do without it. When libc fails the symbol lookup it should fall back on

 * the standard syscall path.

 CONFIG_MIPS_CLOCK_VSYSCALL */

/*

 * This is behind the ifdef so that we don't provide the symbol when there's no

 * possibility of there being a usable clocksource, because there's nothing we

 * can do without it. When libc fails the symbol lookup it should fall back on

 * the standard syscall path.

 CONFIG_MIPS_CLOCK_VSYSCALL */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/**

 * __ioread64_copy - copy data from MMIO space, in 64-bit units

 * @to: destination (must be 64-bit aligned)

 * @from: source, in MMIO space (must be 64-bit aligned)

 * @count: number of 64-bit quantities to copy

 *

 * Copy data from MMIO space to kernel space, in units of 32 or 64 bits at a

 * time.  Order of access is not guaranteed, nor is a memory barrier

 * performed afterwards.

 SPDX-License-Identifier: GPL-2.0

/*

 * GCC 7 & older can suboptimally generate __multi3 calls for mips64r6, so for

 * that specific case only we implement that intrinsic here.

 *

 * See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=82981

 multiply 64-bit values, low 64-bits returned */

 multiply 64-bit unsigned values, high 64-bits of 128-bit result returned */

 multiply 128-bit values, low 128-bits returned */

	/*

	 * a * b =           (a.lo * b.lo)

	 *         + 2^64  * (a.hi * b.lo + a.lo * b.hi)

	 *        [+ 2^128 * (a.hi * b.hi)]

 64BIT && CPU_MIPSR6 && GCC7 */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005 Thiemo Seufer

 * Copyright (C) 2005  MIPS Technologies, Inc.	All rights reserved.

 *	Author: Maciej W. Rozycki <macro@mips.com>

/*

 * FUNC is executed in one of the uncached segments, depending on its

 * original address as follows:

 *

 * 1. If the original address is in CKSEG0 or CKSEG1, then the uncached

 *    segment used is CKSEG1.

 * 2. If the original address is in XKPHYS, then the uncached segment

 *    used is XKPHYS(2).

 * 3. Otherwise it's a bug.

 *

 * The same remapping is done with the stack pointer.  Stack handling

 * works because we don't handle stack arguments or more complex return

 * values, so we can avoid sharing the same stack area between a cached

 * and the uncached mode.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (c) 1994-1997, 99, 2000, 06, 07 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (c) 1999, 2000  Silicon Graphics, Inc.

/**

 * __mips_set_bit - Atomically set a bit in memory.  This is called by

 * set_bit() if it cannot find a faster solution.

 * @nr: the bit to set

 * @addr: the address to start counting from

/**

 * __mips_clear_bit - Clears a bit in memory.  This is called by clear_bit() if

 * it cannot find a faster solution.

 * @nr: Bit to clear

 * @addr: Address to start counting from

/**

 * __mips_change_bit - Toggle a bit in memory.	This is called by change_bit()

 * if it cannot find a faster solution.

 * @nr: Bit to change

 * @addr: Address to start counting from

/**

 * __mips_test_and_set_bit_lock - Set a bit and return its old value.  This is

 * called by test_and_set_bit_lock() if it cannot find a faster solution.

 * @nr: Bit to set

 * @addr: Address to count from

/**

 * __mips_test_and_clear_bit - Clear a bit and return its old value.  This is

 * called by test_and_clear_bit() if it cannot find a faster solution.

 * @nr: Bit to clear

 * @addr: Address to count from

/**

 * __mips_test_and_change_bit - Change a bit and return its old value.	This is

 * called by test_and_change_bit() if it cannot find a faster solution.

 * @nr: Bit to change

 * @addr: Address to count from

 SPDX-License-Identifier: GPL-2.0

/*

 * Dump R3000 TLB for debugging purposes.

 *

 * Copyright (C) 1994, 1995 by Waldorf Electronics, written by Ralf Baechle.

 * Copyright (C) 1999 by Silicon Graphics, Inc.

 * Copyright (C) 1999 by Harald Koerfgen

 Unused entries have a virtual address of KSEG0.  */

			/*

			 * Only print entries in use

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994, 95, 96, 97, 98, 99, 2003 by Ralf Baechle

 * Copyright (C) 1996 by Paul M. Antoine

 * Copyright (C) 1999 Silicon Graphics

 * Copyright (C) 2000 MIPS Technologies, Inc.

/*

 * For cli() we have to insert nops to make sure that the new value

 * has actually arrived in the status register before the end of this

 * macro.

 * R4000/R4400 need three nops, the R4600 two nops and the R10000 needs

 * no nops at all.

/*

 * For TX49, operating only IE bit is not enough.

 *

 * If mfc0 $12 follows store and the mfc0 is last instruction of a

 * page and fetching the next instruction causes TLB miss, the result

 * of the mfc0 might wrongly contain EXL bit.

 *

 * ERT-TX49H2-027, ERT-TX49H3-012, ERT-TX49HL3-006, ERT-TX49H4-008

 *

 * Workaround: mask EXL bit of the result or place a nop before mfc0.

 no outputs */

 no inputs */

 no inputs */

 !CONFIG_CPU_HAS_DIEI */

 SPDX-License-Identifier: GPL-2.0

/*

 * Dump R4x00 TLB for debugging purposes.

 *

 * Copyright (C) 1994, 1995 by Waldorf Electronics, written by Ralf Baechle.

 * Copyright (C) 1999 by Silicon Graphics, Inc.

 EHINV bit marks entire entry as invalid */

		/*

		 * Prior to tlbinv, unused entries have a virtual address of

		 * CKSEG0.

		/*

		 * ASID takes effect in absence of G (global) bit.

		 * We check both G bits, even though architecturally they should

		 * match one another, because some revisions of the SB1 core may

		 * leave only a single G bit set after a machine check exception

		 * due to duplicate TLB entry.

		/*

		 * Only print entries in use

 RI/XI are in awkward places, so mask them off separately */

 RI/XI are in awkward places, so mask them off separately */

 SPDX-License-Identifier: GPL-2.0

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994 by Waldorf Electronics

 * Copyright (C) 1995 - 2000, 01, 03 by Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Copyright (C) 2007, 2014 Maciej W. Rozycki

/*

 * Division by multiplication: you don't have to worry about

 * loss of precision.

 *

 * Use only for very small delays ( < 1 msec).	Should probably use a

 * lookup table, really, as the multiplications take much too long with

 * short delays.  This is a "reasonable" implementation, though (and the

 * first constant multiplications gets optimized away if the delay is

 * a constant)

 SPDX-License-Identifier: GPL-2.0

/*

 * Implement the default iomap interfaces

 *

 * (C) Copyright 2004 Linus Torvalds

 * (C) Copyright 2006 Ralf Baechle <ralf@linux-mips.org>

 * (C) Copyright 2007 MIPS Technologies, Inc.

 *     written by Ralf Baechle <ralf@linux-mips.org>

 This will eventually become a BUG_ON but for now be gentle */

 CONFIG_PCI_DRIVERS_LEGACY */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2009 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2010 Joonas Lahtinen <joonas.lahtinen@gmail.com>

 *  Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2011 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2015 Nikolay Martynov <mar.kolya@gmail.com>

 * Copyright (C) 2015 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2011-2012 Gabor Juhos <juhosg@openwrt.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2015 Nikolay Martynov <mar.kolya@gmail.com>

 * Copyright (C) 2015 John Crispin <john@phrozen.org>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2013 by John Crispin <john@phrozen.org>

 route systick irq to mips irq 7 instead of the r4k-timer */

 enable the counter */

 noting to do here */

		/*

		 * cevt-r4k uses 300, make sure systick

		 * gets used if available

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2008-2009 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

	/*

	 * Load the builtin devicetree. This causes the chosen node to be

	 * parsed resulting in our memory appearing.

 make sure that the reset controller is setup early */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Ralink RT2880 timer

 * Author: John Crispin

 *

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Parts of this file are based on Ralink's 2.6.21 BSP

 *

 * Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 analog */

 digital */

 clock scaling */

 EFUSE bits */

 DRAM type bit */

 does the board have sdram or ddram */

		/*

		 * When the CPU goes into sleep mode, the BUS clock will be

		 * too low for USB to function properly. Adjust the busses

		 * fractional divider to fix this

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Parts of this file are based on Ralink's 2.6.21 BSP

 *

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2015 Nikolay Martynov <mar.kolya@gmail.com>

 * Copyright (C) 2015 John Crispin <john@phrozen.org>

 Early detection of CMP support */

		/*

		 * mips_cm_probe() wipes out bootloader

		 * config for CM regions and we have to configure them

		 * again. This SoC cannot talk to pamlbus devices

		 * witout proper iocu region set up.

		 *

		 * FIXME: it would be better to do this with values

		 * from DT, but we need this very early because

		 * without this we cannot talk to pretty much anything

		 * including serial.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Parts of this file are based on Ralink's 2.6.21 BSP

 *

 * Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 somehow this driver breaks on RT5350 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2008-2009 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 Reset Control */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2009 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 we have a cascade of 8 irqs */

 we have 32 SoC irqs */

 disable all interrupts */

 route all INTC interrupts to MIPS HW0 interrupt */

 tell the kernel which irq is used for performance monitoring */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Parts of this file are based on Ralink's 2.6.21 BSP

 *

 * Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 * Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 * Copyright (C) 2013 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * loongson-specific suspend support

 *

 *  Copyright (C) 2009 Lemote Inc.

 *  Author: Wu Zhangjin <wuzhangjin@gmail.com>

 i8259A */

 bonito */

 disable all mips events */

 disable all events of i8259A */

 disable all events of bonito */

 enable all mips events */

 only enable the cached events of i8259A */

 enable all cached events of bonito */

/*

 * Setup the board-specific events for waking up loongson from wait mode

/*

 * Check wakeup events

/*

 * If the events are really what we want to wakeup the CPU, wake it up

 * otherwise put the CPU asleep again.

/*

 * Stop all perf counters

 *

 * $24 is the control register of Loongson perf counter

 setup wakeup events via enabling the IRQs */

 Put CPU into wait mode */

 wait for the given events to wakeup cpu from wait mode */

 processor specific suspend */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 setup mips r4k timer */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 Ralf Baechle (ralf@linux-mips.org)

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Yan hua (yanhua@lemote.com)

 * Author: Wu Zhangjin (wuzhangjin@gmail.com)

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Lemote Fuloong platform support

 *

 *  Copyright(c) 2010 Arnaud Patard <apatard@mandriva.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 raw */

 ioremapped */

 The CPU provided serial port (LPC) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: Jun Sun, jsun@mvista.com or jsun@junsun.net

 * Copyright (C) 2000, 2001 Ralf Baechle (ralf@gnu.org)

 *

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

	/*

	 * local to PCI mapping for CPU accessing PCI space

	 * CPU address space [256M,448M] is window for accessing pci space

	 * we set pcimap_lo[0,1,2] to map it to pci space[0M,64M], [320M,448M]

	 *

	 * pcimap: PCI_MAP2  PCI_Mem_Lo2 PCI_Mem_Lo1 PCI_Mem_Lo0

	 *	     [<2G]   [384M,448M] [320M,384M] [0M,64M]

	/*

	 * PCI-DMA to local mapping: [2G,2G+256M] -> [0M,256M]

 base: 2G -> mmap: 0M */

 size: 256M, burst transmission, pre-fetch enable, 64bit */

 set this BAR as invalid */

 set this BAR as invalid */

 avoid deadlock of PCI reading/writing lock operation */

	/* can not change gnt to break pci transfer when device's gnt not

	/*

	 * set cpu addr window2 to map CPU address space to PCI address space

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 Loongson CPU address windows config space base address */

 init base address of io space */

init the uart base address */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Based on Ocelot Linux port, which is

 * Copyright 2001 MontaVista Software Inc.

 * Author: jsun@mvista.com or jsun@junsun.net

 *

 * Copyright 2003 ICT CAS

 * Author: Michael Guo <guoyi@ict.ac.cn>

 *

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 pmon passes arguments in 32bit pointers */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 Only 2F revision and it's successors support CPUFreq */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 set cpu window3 to map CPU to DDR: 2G -> 2G */

 !CONFIG_CPU_SUPPORTS_ADDRWINCFG */

 !CONFIG_64BIT */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Zhangjin Wu, wuzhangjin@gmail.com

 No outputs */

 do preparation for reboot */

 reboot via jumping to boot base address */

	/*

	 * It needs a wait loop here, but mips/kernel/reset.c already calls

	 * a generic delay loop, machine_hang(), so simply return.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

/*

 * the first level int-handler will jump here if it is a bonito irq

 workaround the IO dma problem: let cpu looping to allow DMA finish */

 Get pending sources, masked by current enables */

 machine-specific plat_irq_dispatch */

	/*

	 * Clear all of the interrupts while we change the able around a bit.

	 * int-handler is not on bootstrap

 no steer */

	/*

	 * Mask out all interrupt by writing "1" to all bit position in

	 * the interrupt reset reg.

 machine specific irq init */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 *

 * Copyright (c) 2009 Zhang Le <r0bertz@gentoo.org>

 please ensure the length of the machtype string is less than 50 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * the EHCI Virtual Support Module of AMD CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * the ACC Virtual Support Module of AMD CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 disable all the usb interrupt in PIC */

 enable all the acc interrupt in PIC */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * the OHCI Virtual Support Module of AMD CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 enable all the usb interrupt in PIC */

 32bit mem */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * the ISA Virtual Support Module of AMD CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 common variables for PCI_ISA_READ/WRITE_BAR */

/*

 * enable the divil module bar space.

 *

 * For all the DIVIL module LBAR, you should control the DIVIL LBAR reg

 * and the RCONFx(0~5) reg to use the modules.

	/*

	 * The DIVIL IRQ is not used yet. and make the RCONF0 reserved.

/*

 * disable the divil module bar space.

/*

 * BAR write: write value to the n BAR

 NATIVE reg */

 RCONFx is 4bytes in units for I/O space */

/*

 * BAR read: read the n BAR

/*

 * isa_write: ISA write transfer

 *

 * We assume that this is not a bus master transfer.

 disable uart1 interrupt in PIC */

 enable uart1 interrupt in PIC */

 disable uart2 interrupt in PIC */

 enable uart2 interrupt in PIC */

 enable the TARGET ABORT/MASTER ABORT etc. */

 ALL OTHER PCI CONFIG SPACE HEADER IS NOT IMPLEMENTED. */

/*

 * isa_read: ISA read transfers

 *

 * We assume that this is not a bus master transfer.

 we just check the first LBAR for the IO enable bit, */

 maybe we should changed later. */

		/*

		 * we only use the LBAR of DIVIL, no RCONF used.

		 * all of them are IO space.

 no interrupt used here */

/*

 * The mfgpt timer interrupt is running early, so we must keep the south bridge

 * mmio always enabled. Otherwise we may race with the PCI configuration which

 * may temporarily disable it. When that happens and the timer interrupt fires,

 * we are not able to clear it and the system will hang.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * the IDE Virtual Support Module of AMD CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * read/write operation to the PCI config space of CS5536

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author : jlliu, liujl@lemote.com

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 *

 *	the Virtual Support Module(VSM) for virtulizing the PCI

 *	configure space are defined in cs5536_modulename.c respectively,

 *

 *	after this virtulizing, user can access the PCI configure space

 *	directly as a normal multi-function PCI device which follows

 *	the PCI-2.2 spec.

/*

 * write to PCI config space and transfer it to MSR write.

/*

 * read PCI config space and transfer it to MSR access.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * CS5536 General timer functions

 *

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Yanhua, yanh@lemote.com

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu zhangjin, wuzhangjin@gmail.com

 *

 * Reference: AMD Geode(TM) CS5536 Companion Device Data Book

/*

 * Initialize the MFGPT timer.

 *

 * This is also called after resume to bring the MFGPT into operation again.

 disable counter */

 enable counter, comparator2 to event mode, 14.318MHz clock */

 set comparator2 */

 set counter to 0 */

 The oneshot mode have very high deviation, don't use it! */

	/*

	 * get MFGPT base address

	 *

	 * NOTE: do not remove me, it's need for the value of mfgpt_base is

	 * variable

 ack */

/*

 * Initialize the conversion factor and the min/max deltas of the clock event

 * structure and register the clock event source with the framework.

 Enable MFGPT0 Comparator 2 Output to the Interrupt Mapper */

 Enable Interrupt Gate 5 */

 get MFGPT base address */

/*

 * Since the MFGPT overflows every tick, its not very useful

 * to just read by itself. So use jiffies to emulate a free

 * running counter:

	/*

	 * Although our caller may have the read side of xtime_lock,

	 * this is now a seqlock, and we are cheating in this routine

	 * by having side effects on state that we cannot undo if

	 * there is a collision on the seqlock and our caller has to

	 * retry.  (Namely, old_jifs and old_count.)  So we must treat

	 * jiffies as volatile despite the lock.  We read jiffies

	 * before latching the timer count to guarantee that although

	 * the jiffies value might be older than the count (that is,

	 * the counter may underflow between the last point where

	 * jiffies was incremented and the point where we latch the

	 * count), it cannot be newer.

 read the count */

	/*

	 * It's possible for count to appear to go the wrong way for this

	 * reason:

	 *

	 *  The timer counter underflows, but we haven't handled the resulting

	 *  interrupt and incremented jiffies yet.

	 *

	 * Previous attempts to handle these cases intelligently were buggy, so

	 * we just do the simple thing now.

 Functional for real use, but not desired */

 MFGPT does not scale! */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/* Board-specific reboot/shutdown routines

 * Copyright (c) 2009 Philippe Vachon <philippe@cowpig.ca>

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 perf counter loverflow */

	/* init all controller

	 *   0-15	  ------> i8259 interrupt

	 *   16-23	  ------> mips cpu interrupt

	 *   32-63	  ------> bonito irq

 most bonito irq should be level triggered */

 Sets the first-level interrupt dispatcher. */

 bonito irq at IP2 */

 8259 irq at IP5 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Lemote loongson2f family machines' specific suspend support

 *

 *  Copyright (C) 2009 Lemote Inc.

 *  Author: Wu Zhangjin <wuzhangjin@gmail.com>

 open the keyboard irq in i8259A */

 enable keyboard port */

 Wakeup CPU via SCI lid open event */

 yeeloong_report_lid_status will be implemented in yeeloong_laptop.c */

 query the interrupt number */

 query the event number */

 check the LID status */

 wakeup cpu when people open the LID */

				/* If we call it directly here, the WARNING

				 * will be sent out by getnstimeofday

				 * via "WARN_ON(timekeeping_suspended);"

				 * because we can not schedule in suspend mode.

/*

 * Copyright (C) 2006 - 2008 Lemote Inc. & Institute of Computing Technology

 * Author: Yanhua, yanh@lemote.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Basic KB3310B Embedded Controller support for the YeeLoong 2F netbook

 *

 *  Copyright (C) 2008 Lemote Inc.

 *  Author: liujl <liujl@lemote.com>, 2008-04-20

  flush the write action */

/*

 * This function is used for EC command writes and corresponding status queries.

 make chip goto reset mode */

 check if the command is received by ec */

/*

 * Send query command to EC to get the proper event number

/*

 * Get event number from EC

 *

 * NOTE: This routine must follow the query_event_num function in the

 * interrupt.

 SPDX-License-Identifier: GPL-2.0-or-later

/* Board-specific reboot/shutdown routines

 *

 * Copyright (c) 2009 Philippe Vachon <philippe@cowpig.ca>

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

	/*

	 * reset cpu to full speed, this is needed when enabling cpu frequency

	 * scalling

 reset support for fuloong2f */

	/* send a reset signal to south bridge.

	 *

	 * NOTE: if enable "Power Management" in kernel, rtl8169 will not reset

	 * normally with this reset operation and it will not work in PMON, but

	 * you can type halt command and then reboot, seems the hardware reset

	 * logic not work normally.

 get gpio base */

 make cs5536 gpio13 output enable */

 make cs5536 gpio13 output low level voltage. */

 reset support for yeeloong2f and mengloong2f notebook */

 sending an reset signal to EC(embedded controller) */

 menglong(7inches) laptop has different shutdown logic from 8.9inches */

 need enough wait here... how many microseconds needs? */

 cpu-gpio0 output low */

 cpu-gpio0 as output */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote Inc.

 * Author: Fuxin Zhang, zhangfx@lemote.com

 cpu timer */

 bonito */

 cpu serial port */

 i8259 */

/*

 * The generic i8259_irq() make the kernel hang on booting.  Since we cannot

 * get the irq via the IRR directly, we access the ISR instead.

			/*

			 * This may be a spurious interrupt.

			 *

			 * Read the interrupt status register (ISR). If the most

			 * significant bit is not set then there is no valid

			 * interrupt.

 ISR register */

 North Bridge, Perf counter */

 CPU UART */

 South Bridge */

	/* init all controller

	 *   0-15	  ------> i8259 interrupt

	 *   16-23	  ------> mips cpu interrupt

	 *   32-63	  ------> bonito irq

 setup cs5536 as high level trigger */

 Sets the first-level interrupt dispatcher. */

 setup north bridge irq (bonito) */

 setup source bridge irq (i8259) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

	/* We share the same kernel image file among Lemote 2F family

	 * of machines, and provide the machtype= kernel command line

	 * to users to indicate their machine, this command line will

	 * be passed by the latest PMON automatically. and fortunately,

	 * up to now, we can get the machine type from the PMON_VER=

	 * commandline directly except the NAS machine, In the old

	 * machines, this will help the users a lot.

	 *

	 * If no "machtype=" passed, get machine type from "PMON_VER=".

	 *	PMON_VER=LM8089		Lemote 8.9'' netbook

	 *		 LM8101		Lemote 10.1'' netbook

	 *	(The above two netbooks have the same kernel support)

	 *		 LM6XXX		Lemote FuLoong(2F) box series

	 *		 LM9XXX		Lemote LynLoong PC series

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2010 John Crispin <john@phrozen.org>

 access to the ebu needs to be locked between different drivers */

/*

 * This is needed by the VPE loader code, just set it to 0 and assume

 * that the firmware hardcodes this value to something useful.

/*

 * this struct is filled by the soc specific detection code and holds

 * information about the specific soc type, revision and name

	/*

	 * Load the devicetree. This causes the chosen node to be

	 * parsed resulting in our memory appearing

 call the soc specific detetcion code and get it to fill soc_info */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2010 Thomas Langer <thomas.langer@lantiq.com>

 * Copyright (C) 2010 John Crispin <john@phrozen.org>

 lantiq socs have 3 static clocks */

 no input */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2010 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2010 John Crispin <john@phrozen.org>

 * Copyright (C) 2010 Thomas Langer <thomas.langer@lantiq.com>

 register definitions - internal irqs */

 register definitions - external irqs */

 number of external interrupts */

 the performance counter */

/*

 * irqs generated by devices attached to the EBU need to be acked in

 * a special manner

 we have a cascade of 8 irqs */

 This shouldn't be even possible, maybe during CPU hotplug spam */

 by default we are low level triggered */

 clear all pending */

 enable */

 disable */

	/*

	 * silicon bug causes only the msb set to 1 to be valid. all

	 * other bits might be bogus

 if this is a EBU irq, we need to ack it or get a deadlock */

 load register regions of available ICUs */

 turn off all irqs by default */

 make sure all irqs are turned off by default */

 clear all possibly pending interrupts */

 clear resend */

 tell oprofile which irq to use */

 the external interrupts are optional and xway only */

 find out how many external irq sources we have */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2010 John Crispin <john@phrozen.org>

 *  Copyright (C) 2013-2015 Lantiq Beteiligungs-GmbH & Co.KG

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2010 John Crispin <john@phrozen.org>

 *  Copyright (C) 2013-2015 Lantiq Beteiligungs-GmbH & Co.KG

 legacy xway clock */

 vr9, ar10/grx390 clock */

 OCP ratio 1 */

 OCP ratio 2 */

 OCP ratio 2.5 */

 OCP ratio 3 */

 fpi clock is derived from ddr_clk */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2012 John Crispin <john@phrozen.org>

 *  Copyright (C) 2010 Sameer Ahmad, Lantiq GmbH

 Bias and regulator Setup Register */

 Bias and regulator Setup Register */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *   Copyright (C) 2011 John Crispin <john@phrozen.org>

 channel number */

 descriptor complete irq */

 TX channel direction */

 channel on / off bit */

 enable packet drop */

 channel on / off bit */

 channel on / off bit */

 IRQ status register */

 turn on channel polling */

 polling clock divider */

 2 word burst length */

 4 word burst length */

 8 word burst length */

 tx burst shift */

 rx burst shift */

 endianness swap etop channels */

 default channel wheight */

		/*

		 * Tell the DMA engine to swap the endianness of data frames and

		 * drop packets if the channel arbitration fails.

 power up and reset the dma engine */

 disable all interrupts */

 reset/configure each channel */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2011-2012 John Crispin <john@phrozen.org>

 *  Copyright (C) 2013-2015 Lantiq Beteiligungs-GmbH & Co.KG

 clock control register for legacy */

 system clock register for legacy */

 pci control register */

 ephy configuration register */

 Legacy PMU register for ar9, ase, danube */

 power control register */

 power status register */

 power control register */

 power status register */

 power control register */

 power status register */

 PMU register for ar10 and grx390 */

 First register set */

 status */

 Enable */

 Disable */

 Second register set */

 status */

 Enable */

 Disable */

 Third register set */

 status */

 Enable */

 Disable */

 Status */

 Enable */

 Disable */

 clock gates that we can en/disable */

 ASE special */

 ase */

 from vr9 until grx390 */

 vr9 */

 danube, ar9, vr9 */

 grx390 */

 ar10, xrx390 */

 ar10, xrx390 */

 ar10, xrx390 */

 vr9-specific,moved in ar10/grx390 */

 legacy function kept alive to ease clkdev transition */

 legacy function kept alive to ease clkdev transition */

 enable a hw clock */

 disable a hw clock */

 enable a clock gate */

 disable a clock gate */

 the pci enable helper */

 set bus clock speed */

 62.5M */

 62.5M */

 enable the external clock as a source */

 disable the external clock as a source */

 enable a clockout source */

 get the correct rate */

 manage the clock gates via PMU */

		/*

		 * Disable it during the initialization. Module should enable

		 * when used

 manage the clock generator */

 pci needs its own enable function as the setup is a bit more complex */

 main pci clock */

 use internal/external bus clock */

 xway socs can generate clocks on gpio pins */

 bring up all register ranges that we need for basic system control */

 check if all the core register ranges are available */

 make sure to unprotect the memory region where flash is located */

 add our generic xway clocks */

 add the soc dependent clocks */

 rc 0 */

 rc 1 */

 rc 2 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2012 John Crispin <john@phrozen.org>

 *  Copyright (C) 2012 Lantiq GmbH

 the magic ID byte of the core */

 clock control register */

 id register */

 interrupt node enable */

 interrupt control register */

 interrupt capture register */

 there are 3 identical blocks of 2 timers. calculate register offsets */

 timer control register */

 timer auto reload register */

 timer manual reload register */

 timer count register */

 GPTU_CON(x) */

 GPTU_RUN(x) */

 set clock to runmode */

 bring core out of suspend */

 the disable bit */

 remap gptu register range */

 enable our clock */

 power up the core */

 the gptu has a ID register */

 register the clocks */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2012 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2012 Thomas Langer <thomas.langer@lantiq.com>

 * Copyright (C) 2012 John Crispin <john@phrozen.org>

 reset, nmi and ejtag exception vectors */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2011 Thomas Langer <thomas.langer@lantiq.com>

 * Copyright (C) 2011 John Crispin <john@phrozen.org>

 infrastructure control register */

 Configuration fuses for drivers and pll */

 GPE frequency selection */

 Clock status register */

 Clock enable register */

 Clock clear register */

 Activation Status Register */

 Activation Register */

 Deactivation Register */

 reboot Register */

 CPU0 Clock Control Register */

 HRST_OUT_N Control Register */

 clock divider bit */

 Activation Status Register */

 enable the ONU core */

 if if the clock is already enabled */

 use 625MHz on unfused chip */

 apply new frequency */

 enable new frequency */

 check if all the core register ranges are available */

 get our 3 static rates for cpu, fpi and io clocks */

 add our clock domains */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2012 Thomas Langer <thomas.langer@lantiq.com>

 * Copyright (C) 2012 John Crispin <john@phrozen.org>

/*

 * Dummy implementation.  Used to allow platform code to find out what

 * source was booted from

 reboot magic */

 'LTQ\0' */

 '\0QTL' */

 reset Bootreg RVEC */

 watchdog magic */

 PWL */

 CLKDIV */

 enable */

 reload */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip30-smp.c: SMP on IP30 architecture.

 * Based off of the original IP30 SMP code, with inspiration from ip27-smp.c

 * and smp-bmips.c.

 *

 * Copyright (C) 2005-2007 Stanislaw Skowronek <skylark@unaligned.org>

 *               2006-2007, 2014-2015 Joshua Kinard <kumba@gentoo.org>

 *               2009 Johannes Dickgreber <tanzy@gmx.de>

 HEART can theoretically do 4 CPUs, but only 2 are physically possible */

 Poke the other CPU -- it's got mail! */

 Scan the MPCONF structure and enumerate available CPUs. */

	/*

	 * Set the coherency algorithm to '5' (cacheable coherent

	 * exclusive on write).  This is needed on IP30 SMP, especially

	 * for R14000 CPUs, otherwise, instruction bus errors will

	 * occur upon reaching userland.

 nothing to do here */

 Stack pointer (sp). */

 Global pointer (gp). */

 make sure stack and lparm are written */

 Boot CPUx. */

 CPUx now executes smp_bootstrap, then ip30_smp_finish */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip30-power.c: Software powerdown and reset handling for IP30 architecture.

 *

 * Copyright (C) 2004-2007 Stanislaw Skowronek <skylark@unaligned.org>

 *               2014 Joshua Kinard <kumba@gentoo.org>

 *               2009 Johannes Dickgreber <tanzy@gmx.de>

	/*

	 * Execute HEART cold reset

	 *   Yes, it's cold-HEARTed!

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * SGI IP30 miscellaneous setup bits.

 *

 * Copyright (C) 2004-2007 Stanislaw Skowronek <skylark@unaligned.org>

 *               2007 Joshua Kinard <kumba@gentoo.org>

 *               2009 Johannes Dickgreber <tanzy@gmx.de>

 Structure of accessible HEART registers located in XKPHYS space. */

/*

 * ARCS will report up to the first 1GB of

 * memory if queried.  Anything beyond that

 * is marked as reserved.

/*

 * Memory in the Octane starts at 512MB

/*

 * If using ARCS to probe for memory, then

 * remaining memory will start at this offset.

/**

 * ip30_cpu_time_init - platform time initialization.

 Disable all interrupts. */

/**

 * plat_mem_setup - despite the name, misc setup happens here.

 XXX: Hard lock on /sbin/init if this flag isn't specified. */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip30-xtalk.c - Very basic Crosstalk (XIO) detection support.

 *   Copyright (C) 2004-2007 Stanislaw Skowronek <skylark@unaligned.org>

 *   Copyright (C) 2009 Johannes Dickgreber <tanzy@gmx.de>

 *   Copyright (C) 2007, 2014-2016 Joshua Kinard <kumba@gentoo.org>

 XBow is always 0 */

 HEART is always 8 */

 BaseIO PCI is always 15 */

	/*

	 * Walk widget IDs backwards so that BaseIO is probed first.  This

	 * ensures that the BaseIO IOC3 is always detected as eth0.

 SPDX-License-Identifier: GPL-2.0

/*

 * ip30-irq.c: Highlevel interrupt handling for IP30 architecture.

 Bail if there's nothing to process (how did we get here, then?) */

 Prevent any of the error IRQs from firing again. */

 Ack all error IRQs. */

	/*

	 * If we also have a cause value, then something happened, so loop

	 * through the error IRQs and report a "heart attack" for each one

	 * and print the value of the HEART cause register.  This is really

	 * primitive right now, but it should hopefully work until a more

	 * robust error handling routine can be put together.

	 *

	 * Refer to heart.h for the HC_* macros to work out the cause

	 * that got us here.

 i = 63; i >= 51; i-- */

 XXX: Seems possible to loop forever here, so panic(). */

 Unmask the error IRQs. */

 Mask all IRQs. */

 Ack everything. */

 Enable specific HEART error IRQs for each CPU. */

	/*

	 * Some HEART bits are reserved by hardware or by software convention.

	 * Mark these as reserved right away so they won't be accidentally

	 * used later.

 Reserve the error interrupts (#51 to #63). */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip30-timer.c: Clocksource/clockevent support for the

 *               HEART chip in SGI Octane (IP30) systems.

 *

 * Copyright (C) 2004-2007 Stanislaw Skowronek <skylark@unaligned.org>

 * Copyright (C) 2009 Johannes Dickgreber <tanzy@gmx.de>

 * Copyright (C) 2011 Joshua Kinard <kumba@gentoo.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-setup.c: SGI specific setup, including init of the feature struct.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1997, 1998 Ralf Baechle (ralf@gnu.org)

	/* Init the INDY HPC I/O controller.  Need to call this before

	 * fucking with the memory controller because it needs to know the

	 * boardID and whether this is a Guiness or a FullHouse machine.

 Init INDY memory controller. */

 Now enable boardcaches, if any. */

	/* Set EISA IO port base for Indigo2

	/* ARCS console environment variable is set to "g?" for

	 * graphics console, it is set to "d" for the first serial

	 * line and "d2" for the second serial line.

	 *

	 * Need to check if the case is 'g' but no keyboard:

	 * (ConsoleIn/Out = serial)

 Use ARC if we don't want serial ('d') or graphics ('g'). */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-mc.c: Routines for manipulating SGI Memory Controller.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1999 Andrew R. Baker (andrewb@uab.edu) - Indigo2 changes

 * Copyright (C) 2003 Ladislav Michl  (ladis@linux-mips.org)

 * Copyright (C) 2004 Peter Fuerst    (pf@net.alphadv.de) - IP28

 prom detects all usable memory */

/*

 * Detect installed memory, which PROM misses

 ioremap can't fail */

	/* Place the MC into a known state.  This must be done before

	 * interrupts are first enabled etc.

	/* Step 0: Make sure we turn off the watchdog in case it's

	 *	   still running (which might be the case after a

	 *	   soft reboot).

	/* Step 1: The CPU/GIO error status registers will not latch

	 *	   up a new error status until the register has been

	 *	   cleared by the cpu.	These status registers are

	 *	   cleared by writing any value to them.

	/* Step 2: Enable all parity checking in cpu control register

	 *	   zero.

 don't touch parity settings for IP28 */

	/* Step 3: Setup the MC write buffer depth, this is controlled

	 *	   in cpu control register 1 in the lower 4 bits.

	/* Step 4: Initialize the RPSS divider register to run as fast

	 *	   as it can correctly operate.	 The register is laid

	 *	   out as follows:

	 *

	 *	   ----------------------------------------

	 *	   |  RESERVED	|   INCREMENT	| DIVIDER |

	 *	   ----------------------------------------

	 *	    31	      16 15	       8 7	 0

	 *

	 *	   DIVIDER determines how often a 'tick' happens,

	 *	   INCREMENT determines by how the RPSS increment

	 *	   registers value increases at each 'tick'. Thus,

	 *	   for IP22 we get INCREMENT=1, DIVIDER=1 == 0x101

	/* Step 5: Initialize GIO64 arbitrator configuration register.

	 *

	 * NOTE: HPC init code in sgihpc_init() must run before us because

	 *	 we need to know Guiness vs. FullHouse and the board

	 *	 revision on this machine. You have been warned.

 First the basic invariants across all GIO64 implementations. */

 keep gfx 64bit settings */

 All 1st HPC's interface at 64bits */

 Only one physical GIO bus exists */

 Fullhouse specific settings. */

 2nd HPC at 64bits */

 exp0 pipelines */

 exp1 masters */

 exp0 is realtime */

 2nd HPC 64bits */

 exp[01] pipelined */

 EISA masters */

 Guiness specific settings. */

 MC talks to EISA at 64bits */

 EISA bus can act as master */

 poof */

	/*

	 * because ARCS accesses memory uncached we wait until ARCS

	 * isn't needed any longer, before we switch from slow to

	 * normal mode

 map ECC register */

 switch to normal mode */

 reduce WR_COL */

 restore old config */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-berr.c: Bus error handling.

 *

 * Copyright (C) 2002, 2003 Ladislav Michl (ladis@linux-mips.org)

 Status reg for CPU */

 Status reg for GIO */

 Error address reg for CPU */

 Error address reg for GIO */

 Bus error interrupt status */

 save status registers */

/*

 * MC sends an interrupt whenever bus or parity errors occur. In addition,

 * if the error happened during a CPU read, it also asserts the bus error

 * pin on the R4K. Code in bus error handler save the MC bus error registers

 * and then clear the interrupt when this happens.

 Assume it would be too dangerous to continue ... */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Time operations for IP22 machines. Original code may come from

 * Ralf Baechle or David S. Miller (sorry guys, i'm really not sure)

 *

 * Copyright (C) 2001 by Ladislav Michl

 * Copyright (C) 2003, 06 Ralf Baechle (ralf@linux-mips.org)

 Start the counter. */

 Get initial counter invariant */

 Latch and spin until top byte of counter2 is zero */

 Stop the counter. */

	/*

	 * Return the difference, this is how far the r4k counter increments

	 * for every 1/HZ seconds. We round off the nearest 1 MHz of master

	 * clock (= 1000000 / HZ / 2).

/*

 * Here we need to calibrate the cycle counter to at least be close.

	/*

	 * Figure out the r4k offset, the algorithm is very simple and works in

	 * _all_ cases as long as the 8254 counter register itself works ok (as

	 * an interrupt driving timer it does not because of bug, this is why

	 * we are using the onchip r4k counter/compare register to serve this

	 * purpose, but for r4k_offset calculation it will work ok for us).

	 * There are other very complicated ways of performing this calculation

	 * but this one works just fine so I am not going to futz around. ;-)

 Prime cache. */

 Prime cache. */

 Zero is NOT an option. */

 Generic SGI handler for (spurious) 8254 interrupts */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-nvram.c: NVRAM and serial EEPROM handling.

 *

 * Copyright (C) 2003 Ladislav Michl (ladis@linux-mips.org)

 Control opcode for serial eeprom  */

 serial memory read */

 write enable before prog modes */

 serial memory write */

 write all registers */

 disable all programming */

 read protect register */

 enable protect register mode */

 clear protect register */

 write protect register */

 disable protect register, forever */

 Protect register enable */

 Chip select */

 EEPROM clock */

 Data out */

 Data in */

 We need to use these functions early... */

/*

 * clock in the nvram command and the register number. For the

 * national semiconductor nv ram chip the op code is 3 bits and

 * the address is 6/8 bits.

 if high order bit set */

 see data sheet timing diagram */

 clock the data ouf of serial mem */

/*

 * Read specified register from main NVRAM

		/* IP22 (Indigo2 aka FullHouse) stores env variables into

 IP24 (Indy aka Guiness) uses DS1386 8K version */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1997, 1998, 2001, 03, 05, 06 by Ralf Baechle

/*

 * Just powerdown if init hasn't done after POWERDOWN_TIMEOUT seconds.

 * I'm not sure if this feature is a good idea, for now it's here just to

 * make the power button make behave just like under IRIX.

/*

 * Blink frequency during reboot grace period and when panicked.

 Disable watchdog */

 Good bye cruel world ...  */

		/* If we're still running, we probably got sent an alarm

 XXX fix this for fullhouse  */

 Interrupt still being sent. */

 0.05s	*/

 No init process or button pressed twice.  */

 Wait until interrupt goes away */

	/* Power button was pressed

	 * ioc.ps page 22: "The Panel Register is called Power Control by Full

	 * House. Only lowest 2 bits are used. Guiness uses upper four bits

	 * for volume control". This is not true, all bits are pulled high

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-hpc.c: Routines for generic manipulation of the HPC controllers.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1998 Ralf Baechle

 We need software copies of these because they are write only. */

 ioremap can't fail */

 IOC lives in PBUS PIO channel 6 */

		/* Full House comes with INT2 which lives in PBUS PIO

 Guiness comes with INT3 which is part of IOC */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip28-berr.c: Bus error handling.

 *

 * Copyright (C) 2002, 2003 Ladislav Michl (ladis@linux-mips.org)

 * Copyright (C) 2005 Peter Fuerst (pf@net.alphadv.de) - IP28

 Status reg for CPU */

 Status reg for GIO */

 Error address reg for CPU */

 Error address reg for GIO */

 Bus error interrupt status */

 Way 0/1 */

	/*

	 * Starting with a bus-address, save secondary cache (indexed by

	 * PA[23..18:7..6]) tags first.

 PA[35:18], VA[13:12] */

 PA[39:36] */

 PA[35:18], VA[13:12] */

 PA[39:36] */

	/*

	 * Save all primary data cache (indexed by VA[13:5]) tags which

	 * might fit to this bus-address, knowing that VA[11:0] == PA[11:0].

	 * Saving all tags and evaluating them later is easier and safer

	 * than relying on VA[13:12] from the secondary cache tags to pick

	 * matching primary tags here already.

 PA[35:12] */

 PA[39:36] */

 PA[35:12] */

 PA[39:36] */

	/*

	 * Save primary instruction cache (indexed by VA[13:6]) tags

	 * the same way.

 PA[35:12] */

 PA[39:36] */

 PA[35:12] */

 PA[39:36] */

 save status registers */

 HPC3_SCTRL_ACTIVE ? */

 HPC3_SCTRL_ACTIVE ? */

 HPC3_ERXCTRL_ACTIVE ? */

 HPC3_ETXCTRL_ACTIVE ? */

 HPC3_PDMACTRL_ISACT ? */

 PA[31:12] shifted to PTag0 (PA[35:12]) format */

 for each possible VA[13:12] value */

 for each possible VA[13:12] value */

 scblksize = 2^[7..6] */

 scwaysize = 2^[24..19] / 2 */

 This is likely rather similar to correct code ;-) */

 Doc. states that top bit is ignored */

 If tlb-entry is valid and VPN-high (bits [30:21] ?) matches... */

 16k:4k */

 PTEIndex is VPN-low (bits [22:14]/[20:12] ?) */

 PTEBase */

				/*

				 * Note: Since DMA hardware does look up

				 * translation on its own, this PTE *must*

				 * match the TLB/EntryLo-register format !

 PFN */

 Xlate-bit clear ? */

/*

 * MC sends an interrupt whenever bus or parity errors occur. In addition,

 * if the error happened during a CPU read, it also asserts the bus error

 * pin on the R4K. Code in bus error handler save the MC bus error registers

 * and then clear the interrupt when this happens.

	/*

	 * Try to find out, whether we got here by a mispredicted speculative

	 * load/store operation.  If so, it's not fatal, we can go on.

 Any cause other than "Interrupt" (ExcCode 0) is fatal. */

 Any cause other than "Bus error interrupt" (IP6) is weird. */

 Any state other than "Memory bus error" is fatal. */

 GIO errors other than timeouts are fatal */

	/*

	 * Now we have an asynchronous bus error, speculatively or DMA caused.

	 * Need to search all DMA descriptors for the error address.

 Check MC's virtual DMA stuff. */

 A speculative bus error... */

 Assume it would be too dangerous to continue ... */

	/*

	 * We arrive here only in the unusual case of do_be() invocation,

	 * i.e. by a bus error exception without a bus error interrupt.

 SPDX-License-Identifier: GPL-2.0-only

 fake IDs */

/**

 * gio_match_device - Tell if an of_device structure has a matching

 * gio_match structure

 * @ids: array of of device match structures to search in

 * @dev: the of device structure to match against

 *

 * Used by a driver to check whether an of_device present in the

 * system is in its list of supported devices.

/**

 * gio_release_dev - free an gio device structure when all users of it are finished.

 * @dev: device that's been disconnected

 *

 * Will be called only by the device core when all users of this gio device are

 * done.

 initialize common driver fields */

 register with core */

		/*

		 * We got no DBE, but this doesn't mean anything.

		 * If GIO is pipelined (which can't be disabled

		 * for GFX slot) we don't get a DBE, but we see

		 * the transfer size as data. So we do an 8bit

		 * and a 16bit access and check whether the common

		 * data matches

			/*

			 * 32bit access worked, but 8bit doesn't

			 * so we don't see phantom reads on

			 * a pipelined bus, but a real card which

			 * doesn't support 8 bit reads

 nothing here */

 HQ2 only allows 32bit accesses */

 first look for GR2/GR3 by checking mystery register */

			/*

			 * no GIO signature at start address of slot

			 * since Newport doesn't have one, we check if

			 * user status register is readable

 Indigo2 */

 Indy/Challenge S */

 SPDX-License-Identifier: GPL-2.0

/*

 * ip22-int.c: Routines for generic manipulation of the INT[23] ASIC

 *	       found on INDY and Indigo2 workstations.

 *

 * Copyright (C) 1996 David S. Miller (davem@davemloft.net)

 * Copyright (C) 1997, 1998 Ralf Baechle (ralf@gnu.org)

 * Copyright (C) 1999 Andrew R. Baker (andrewb@uab.edu)

 *		      - Indigo2 changes

 *		      - Interrupt handling fixes

 * Copyright (C) 2001, 2003 Ladislav Michl (ladis@linux-mips.org)

 So far nothing hangs here */

	/* don't allow mappable interrupt to be enabled from setup_irq,

	/* don't allow mappable interrupt to be enabled from setup_irq,

	/*

	 * workaround for INT2 bug; if irq == 0, INT2 has seen a fifo full

	 * irq, but failed to latch it into status register

 if irq == 0, then the interrupt has already been cleared */

/*

 * IRQs on the INDY look basically (barring software IRQs which we don't use

 * at all) like:

 *

 *	MIPS IRQ	Source

 *	--------	------

 *	       0	Software (ignored)

 *	       1	Software (ignored)

 *	       2	Local IRQ level zero

 *	       3	Local IRQ level one

 *	       4	8254 Timer zero

 *	       5	8254 Timer one

 *	       6	Bus Error

 *	       7	R4k timer (what we use)

 *

 * We handle the IRQ according to _our_ priority which is:

 *

 * Highest ----	    R4k Timer

 *		    Local IRQ zero

 *		    Local IRQ one

 *		    Bus Error

 *		    8254 Timer zero

 * Lowest  ----	    8254 Timer one

 *

 * then we just return, if multiple IRQs are pending then we will just take

 * another exception, big deal.

	/*

	 * First we check for r4k counter/timer IRQ.

 Init local mask --> irq tables. */

 Mask out all interrupts. */

 init CPU irqs */

 vector handler. this register the IRQ as non-sharable */

 cascade in cascade. i love Indy ;-) */

 Only Indigo-2 has EISA stuff */

/*

 * Basic EISA bus support for the SGI Indigo-2.

 *

 * (C) 2002 Pascal Dameme <netinet@freesurf.fr>

 *	and Marc Zyngier <mzyngier@freesurf.fr>

 *

 * This code is released under both the GPL version 2 and BSD

 * licenses.  Either license may be used.

 *

 * This code offers a very basic support for this EISA bus present in

 * the SGI Indigo-2. It currently only supports PIO (forget about DMA

 * for the time being). This is enough for a low-end ethernet card,

 * but forget about your favorite SCSI card...

 *

 * TODO :

 * - Fix bugs...

 * - Add ISA support

 * - Add DMA (yeah, right...).

 * - Fix more bugs.

 I2 has four EISA slots. */

 Oops, Bad Stuff Happened... */

	/* Warning : BlackMagicAhead(tm).

 First say hello to the EIU */

 Now be nice to the EISA chipset */

 Wait long enough for the dust to settle */

 SPDX-License-Identifier: GPL-2.0

/*

 * Create a platform device for the GPI port that receives the

 * image data from the embedded camera.

/*

 * Create a platform device for the GPI port that receives the

 * image data from the embedded camera.

 Second HPC is missing? */

 interrupt/config register on Challenge S Mezz board */

 full house has no volume buttons */

 SPDX-License-Identifier: GPL-2.0

/*

 *  Nintendo 64 init.

 *

 *  Copyright (C) 2021	Lauri Kasanen

/*

 * System-specifc irq names for clarity

 The framebuffer needs 64-byte alignment */

 setup IORESOURCE_MEM as framebuffer memory */

 Bootloader blocks the 4mb config */

 93.75 MHz cpu, count register runs at half rate */

 SPDX-License-Identifier: GPL-2.0

/*

 *  N64 IRQ

 *

 *  Copyright (C) 2021 Lauri Kasanen

/*

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * 2003-2005 (c) MontaVista Software, Inc.

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 EBUSC settings of TX4927, etc. */

 "CEn" */

 pcode, internal register */

 clocks */

 don't enable by default - see errata */

 search board_vecs table */

 Always ignore the "-c" at argv[0] */

		/*

		 * argc is not a valid number, or argv32 is not a valid

		 * pointer

 flush all cache on very early stage (before 4k_cache_init) */

 4way, waybit=0 */

 flush and disable */

 enable cache */

 flush all cache on very early stage (before tx39_cache_init) */

 2way, waybit=0 */

 flush and disable */

 enable cache */

 first, determine by "board=" argument in preprocess_cmdline() */

 next, determine by "board" envvar */

 select "default" board */

 YAMON style ("name", "value" pairs) */

				/*

				 * Clear counter interrupt while it

				 * breaks WAIT instruction even if

				 * masked.

 Watchdog support */

 disable watch dog timer */

 kick watchdog */

 immediate */

 SPI support */

HAVE_CTS_LINE*/;

USE_SCLK*/;

 CONFIG_SERIAL_TXX9 */

 CONFIG_EARLY_PRINTK */

 wrappers */

 no limit */

 no limit */

 fallback restart/halt routines */

 see include/asm-mips/mach-tx39xx/mangle-port.h, for example. */

 If this area contained boot area, make separate partition */

 CONFIG_LEDS_GPIO */

 CONFIG_LEDS_GPIO */

/*

 * spi_eeprom.c

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

 register board information for at25 driver */

 1.5Mbps */

 Mode 0: High-Active, Sample-Then-Shift */

 simple temporary spi driver to provide early access to seeprom. */

 spi_write_then_read can only work with small chunk */

 AT25_READ */

/*

 * Common tx4927 irq handler

 *

 * Author: MontaVista Software, Inc.

 *	   source@mvista.com

 *

 *  under the terms of the GNU General Public License as published by the

 *  Free Software Foundation; either version 2 of the License, or (at your

 *  option) any later version.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,

 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS

 *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND

 *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

 *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the GNU General Public License along

 *  with this program; if not, write to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 raise priority for errors, timers, SIO */

/*

 * TX4927 setup routines

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * 2003-2005 (c) MontaVista Software, Inc.

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 report watchdog reset status */

 clear WatchDogReset (W1C) */

 do reset on watchdog */

 clear watchdog status */

 W1C */

 External WDRST failed.  Do internal watchdog reset */

 fallback */

 SDRAMC,EBUSC are configured by PROM */

 disabled */

 clocks */

 calculate gbus_clock and cpu_clock from master_clock */

 200MHz */

 calculate gbus_clock and master_clock from cpu_clock */

 change default value to udelay/mdelay take reasonable time */

 CCFG */

 clear BusErrorOnWrite flag (W1C) */

 enable Timeout BusError */

 DMA selection */

 Use external clock for external arbiter */

 disabled */

 TMR */

 disable all timers */

 PIO */

 disabled */

 setup DMASEL (playback:ACLC ch0, capture:ACLC ch1) */

/*

 * Interface for smsc fdc48m81x Super IO chip

 *

 * Author: MontaVista Software, Inc. source@mvista.com

 *

 * 2001-2003 (c) MontaVista Software, Inc. This file is licensed under

 * the terms of the GNU General Public License version 2. This program

 * is licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Copyright 2004 (c) MontaVista Software, Inc.

 Common Registers */

 Logical device numbers */

 Logical device Config Registers */

 Chip Config Values */

/*

 * TX4938/4937 setup routines

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * 2003-2005 (c) MontaVista Software, Inc.

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 report watchdog reset status */

 clear WatchDogReset (W1C) */

 do reset on watchdog */

 clear watchdog status */

 W1C */

 External WDRST failed.  Do internal watchdog reset */

 fallback */

 SDRAMC,EBUSC are configured by PROM */

 disabled */

 clocks */

 calculate gbus_clock and cpu_clock from master_clock */

 300MHz */

 calculate gbus_clock and master_clock from cpu_clock */

 change default value to udelay/mdelay take reasonable time */

 CCFG */

 clear BusErrorOnWrite flag (W1C) */

 enable Timeout BusError */

 DMA selection */

 Use external clock for external arbiter */

 disabled */

 SRAM */

 TMR */

 disable all timers */

 PIO */

 set PCIC1 reset */

 at least 128 cpu clock */

 clear PCIC1 reset */

 stop PCIC1 */

 disable SIO1 by PCFG setting */

 disabled */

 .start and .end are filled in later */

		/*

		 * The IDE driver should not change bus timings if other ISA

		 * devices existed.

 check EBCCRn.ISA, EBCCRn.BSZ, EBCCRn.ME */

/*

 * linux/arch/mips/txx9/pci.c

 *

 * Based on linux/arch/mips/txx9/rbtx4927/setup.c,

 *	    linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright 2001-2005 MontaVista Software Inc.

 * Copyright (C) 1996, 97, 2001, 04  Ralf Baechle (ralf@linux-mips.org)

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Fake a parent bus structure. */

 It seems SLC90E66 needs some time after PCI reset... */

 check 66MHz capability */

/*

 * allocate pci_controller and resources.

 * mem_base, io_base: physical address.	 0 for auto assignment.

 * mem_size and io_size means max size on auto assignment.

 * pcic must be &txx9_primary_pcic or NULL.

	/*

	 * for auto assignment, first search a (big) region for PCI

	 * MEM, then search a region for PCI IO.

 low 512MB */

 default size for auto assignment */

 mem:512M(max) */

 mem:128M(max) */

 search free region for PCI MEM */

 default size for auto assignment */

 io:16M(max) */

 search free region for PCI IO in low 512MB */

 map ioport 0 to PCI I/O space address 0 */

 busaddr == ioaddr */

 physaddr to ioaddr */

 busaddr == physaddr */

 register_pci_controller() will request MEM resource */

 reseve legacy I/O space */

 IRQ/IDSEL mapping */

 PCI/ISA Bridge interrupt */

 INTA */

 serial irq control */

 serial irq pin */

 ide irq on isa14 */

 SMSC SLC90E66 IDE uses irq 14, 15 (default) */

 enable SMSC SLC90E66 IDE */

	/*

	 * !!! DO NOT REMOVE THIS COMMENT IT IS REQUIRED BY SMSC !!!

	 *

	 * This line of code is intended to provide the user with a work

	 * around solution to the anomalies cited in SMSC's anomaly sheet

	 * entitled, "SLC90E66 Functional Rev.J_0.1 Anomalies"".

	 *

	 * !!! DO NOT REMOVE THIS COMMENT IT IS REQUIRED BY SMSC !!!

 CONFIG_TOSHIBA_FPCIB0 */

 This device may have PM registers but not they are not supported. */

 Do build-in self test */

 timeout after 2 sec */

		/* PICMG compliant backplane (TOSHIBA JMB-PICMG-ATX

		/* non-PICMG compliant backplane (TOSHIBA

 "auto" */

/*

 * TX3927 setup routines

 * Based on linux/arch/mips/txx9/jmr3927/setup.c

 *

 * Copyright 2001 MontaVista Software Inc.

 * Copyright (C) 2000-2001 Toshiba Corporation

 * Copyright (C) 2007 Ralf Baechle (ralf@linux-mips.org)

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 SDRAMC,ROMC are configured by PROM */

 disabled */

 clocks */

 change default value to udelay/mdelay take reasonable time */

 CCFG */

 enable Timeout BusError */

 clear BusErrorOnWrite flag */

 Disable PCI snoop */

 Enable PCI SNOOP - with write through only */

 do reset on watchdog */

 TMR */

 DMA */

 reset channel */

 enable DMA */

 PIO */

 disabled */

/*

 * 7 Segment LED routines

 * Based on RBTX49xx patch from CELF patch archive.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * (C) Copyright TOSHIBA CORPORATION 2005-2007

 * All Rights Reserved.

/*

 * TX4939 setup routines

 * Based on linux/arch/mips/txx9/generic/setup_tx4938.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * 2003-2005 (c) MontaVista Software, Inc.

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 report watchdog reset status */

 clear WatchDogReset (W1C) */

 do reset on watchdog */

 clear watchdog status */

 W1C */

 External WDRST failed.  Do internal watchdog reset */

 fallback */

 SDRAMC,EBUSC are configured by PROM */

 disabled */

 clocks */

 calculate cpu_clock from master_clock */

 / 6 *  8 */; break;

 / 6 *  9 */; break;

 / 6 * 10 */; break;

 / 6 * 12 */; break;

 / 6 * 14 */; break;

 / 6 * 15 */; break;

 400MHz */

 calculate master_clock from cpu_clock */

 * 2 / 20 */

 calculate gbus_clock from cpu_clock */

 change default value to udelay/mdelay take reasonable time */

 CCFG */

 clear BusErrorOnWrite flag (W1C) */

 enable Timeout BusError */

 DMA selection */

 Use external clock for external arbiter */

 disabled */

 SRAM */

 TMR */

 disable all timers */

 set PCIC1 reset (required to prevent hangup on BIST) */

 at least 128 cpu clock */

 clear PCIC1 reset */

 stop PCIC1 */

 only SIO0 have RTS/CTS */

 disable SIO0 RTS/CTS by PCFG setting */

 disable SIO2 by PCFG setting */

 disable SIO3 by PCFG setting */

 default 100Mbps */

 disabled */

/*

 * Common tx3927 irq handler

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright 2001 MontaVista Software Inc.

 * Copyright (C) 2000-2001 Toshiba Corporation

 raise priority for timers, sio */

/*

 * linux/arch/mips/tx4938/common/irq.c

 *

 * Common tx4938 irq handler

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

 raise priority for errors, timers, SIO */

/*

 * TX4939 irq routines

 * Based on linux/arch/mips/kernel/irq_txx9.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright 2001, 2003-2005 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *	   ahennessy@mvista.com

 *	   source@mvista.com

 * Copyright (C) 2000-2001,2005-2007 Toshiba Corporation

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * TX4939 defines 64 IRQs.

 * Similer to irq_txx9.c but different register layouts.

 IRCER : Int. Control Enable */

 IRCR : Int. Control */

 IRSCR : Int. Status Control */

 IRCSR : Int. Current Status */

 clear edge detection */

 disable interrupt control */

 irq_base + 0 is not used */

 middle level */

 mask all IRC interrupts */

 setup IRC interrupt mode (Low Active) */

 enable interrupt control */

 raise priority for errors, timers, sio */

/*

 * common tx4927 memory interface

 *

 * Author: MontaVista Software, Inc.

 *	   source@mvista.com

 *

 * Copyright 2001-2002 MontaVista Software Inc.

 *

 *  This program is free software; you can redistribute it and/or modify it

 *  under the terms of the GNU General Public License as published by the

 *  Free Software Foundation; either version 2 of the License, or (at your

 *  option) any later version.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,

 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS

 *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND

 *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

 *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the GNU General Public License along

 *  with this program; if not, write to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 MVMCP -- need #defs for these bits masks */

/*

 * Setup pointers to hardware-dependent routines.

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

 fallback */

 already configured */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reinitialize PCIC */

 Reset PCIC1 */

 PCI1DMD==0 => PCI1CLK==GBUSCLK/2 => PCI66 */

 clear PCIC1 reset */

 mem:64K(max), io:64K(max) (enough for ETH0,ETH1) */

 CONFIG_PCI */

 SPI support */

 chip select for SPI devices */

 PIO7 */

 IOC */

 IOC */

 IOC */

 0-3: "MAC\0", 4-9:eth0, 10-15:eth1, 16:sum */

 CONFIG_PCI */

 set SPI_SEL */

 25MHz */

 updated */

 fixup piosel */

 1.0Mbps @ Vdd 2.0V */

 Mode 1 (High-Active, Shift-Then-Sample), High Avtive CS  */

 Boot */

 System */

 System */

 Boot */

 Ext */

 System */

 Boot */

 Boot */

 System */

 TC58DVM82A1FT: tDH=10ns, tWP=tRP=tREADID=35ns */

/*

 * rbtx4938 specific prom routines

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

/*

 * Toshiba RBTX4938 specific interrupt handlers

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

/*

 * MIPS_CPU_IRQ_BASE+00 Software 0

 * MIPS_CPU_IRQ_BASE+01 Software 1

 * MIPS_CPU_IRQ_BASE+02 Cascade TX4938-CP0

 * MIPS_CPU_IRQ_BASE+03 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+04 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+05 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+06 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+07 CPU TIMER

 *

 * TXX9_IRQ_BASE+00

 * TXX9_IRQ_BASE+01

 * TXX9_IRQ_BASE+02 Cascade RBTX4938-IOC

 * TXX9_IRQ_BASE+03 RBTX4938 RTL-8019AS Ethernet

 * TXX9_IRQ_BASE+04

 * TXX9_IRQ_BASE+05 TX4938 ETH1

 * TXX9_IRQ_BASE+06 TX4938 ETH0

 * TXX9_IRQ_BASE+07

 * TXX9_IRQ_BASE+08 TX4938 SIO 0

 * TXX9_IRQ_BASE+09 TX4938 SIO 1

 * TXX9_IRQ_BASE+10 TX4938 DMA0

 * TXX9_IRQ_BASE+11 TX4938 DMA1

 * TXX9_IRQ_BASE+12 TX4938 DMA2

 * TXX9_IRQ_BASE+13 TX4938 DMA3

 * TXX9_IRQ_BASE+14

 * TXX9_IRQ_BASE+15

 * TXX9_IRQ_BASE+16 TX4938 PCIC

 * TXX9_IRQ_BASE+17 TX4938 TMR0

 * TXX9_IRQ_BASE+18 TX4938 TMR1

 * TXX9_IRQ_BASE+19 TX4938 TMR2

 * TXX9_IRQ_BASE+20

 * TXX9_IRQ_BASE+21

 * TXX9_IRQ_BASE+22 TX4938 PCIERR

 * TXX9_IRQ_BASE+23

 * TXX9_IRQ_BASE+24

 * TXX9_IRQ_BASE+25

 * TXX9_IRQ_BASE+26

 * TXX9_IRQ_BASE+27

 * TXX9_IRQ_BASE+28

 * TXX9_IRQ_BASE+29

 * TXX9_IRQ_BASE+30

 * TXX9_IRQ_BASE+31 TX4938 SPI

 *

 * RBTX4938_IRQ_IOC+00 PCI-D

 * RBTX4938_IRQ_IOC+01 PCI-C

 * RBTX4938_IRQ_IOC+02 PCI-B

 * RBTX4938_IRQ_IOC+03 PCI-A

 * RBTX4938_IRQ_IOC+04 RTC

 * RBTX4938_IRQ_IOC+05 ATA

 * RBTX4938_IRQ_IOC+06 MODEM

 * RBTX4938_IRQ_IOC+07 SWINT

 must use fls so onboard ATA has priority */

 Now, interrupt control disabled, */

 all IRC interrupts are masked, */

 all IRC interrupt mode are Low Active. */

 mask all IOC interrupts */

 clear SoftInt interrupts */

 Onboard 10M Ether: High Active */

/*

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 *

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *              ahennessy@mvista.com

 *

 * Copyright (C) 2000-2001 Toshiba Corporation

 * Copyright (C) 2007 Ralf Baechle (ralf@linux-mips.org)

 Resetting PCI bus */

 flush WB */

 fallback */

 cache setup */

 initialize board */

 ch1: noCTS */

 Reset PCI Bus */

 CONFIG_PCI */

 SDRAMC are configured by PROM */

 ROMC */

 Pin selection */

 PIO[15:12] connected to LEDs */

 SIO0 DTR on */

 This trick makes rtc-ds1742 driver usable as is. */

/*

 * BRIEF MODULE DESCRIPTION

 *    PROM library initialisation code, assuming a version of

 *    pmon is the boot code.

 *

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *              ahennessy@mvista.com

 *

 * Based on arch/mips/au1000/common/prom.c

 *

 * This file was derived from Carsten Langgaard's

 * arch/mips/mips-boards/xx files.

 *

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 CCFG */

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *              ahennessy@mvista.com

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/*

 * CP0_STATUS is a thread's resource (saved/restored on context switch).

 * So disable_irq/enable_irq MUST handle IOC/IRC registers.

 0: mask */

 flush write buffer */

 0: mask */

 flush write buffer */

 Now, interrupt control disabled, */

 all IRC interrupts are masked, */

 all IRC interrupt mode are Low Active. */

 mask all IOC interrupts */

 setup IOC interrupt mode (SOFT:High Active, Others:Low Active) */

 clear PCI Soft interrupts */

 clear PCI Reset interrupts */

 setup IOC interrupt 1 (PCI, MODEM) */

/*

 * Toshiba rbtx4927 specific setup

 *

 * Author: MontaVista Software, Inc.

 *	   source@mvista.com

 *

 * Copyright 2001-2002 MontaVista Software Inc.

 *

 * Copyright (C) 1996, 97, 2001, 04  Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2000 RidgeRun, Inc.

 * Author: RidgeRun, Inc.

 *   glonnon@ridgerun.com, skranz@ridgerun.com, stevej@ridgerun.com

 *

 * Copyright 2001 MontaVista Software Inc.

 * Author: jsun@mvista.com or jsun@junsun.net

 *

 * Copyright 2002 MontaVista Software Inc.

 * Author: Michael Pruznick, michael_pruznick@mvista.com

 *

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * Copyright (C) 2004 MontaVista Software Inc.

 * Author: Manish Lachwani, mlachwani@mvista.com

 *

 *  This program is free software; you can redistribute it and/or modify it

 *  under the terms of the GNU General Public License as published by the

 *  Free Software Foundation; either version 2 of the License, or (at your

 *  option) any later version.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,

 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS

 *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND

 *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

 *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the GNU General Public License along

 *  with this program; if not, write to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 for TX4937 */

 already configured */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reinitialize PCIC */

 already configured */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reset PCI Bus */

 Reset PCIC */

 clear PCIC reset */

 Reinitialize PCIC */

 CONFIG_PCI */

 TX4927-SIO DTR on (PIO[15]) */

 enable the s/w reset register */

 wait for enable to be seen */

 do a s/w reset */

 fallback */

	/*

	 * ASSUMPTION: PCIDIVMODE is configured for PCI 33MHz or 66MHz.

	 *

	 * For TX4927:

	 * PCIDIVMODE[12:11]'s initial value is given by S9[4:3] (ON:0, OFF:1).

	 * CPU 166MHz: PCI 66MHz : PCIDIVMODE: 00 (1/2.5)

	 * CPU 200MHz: PCI 66MHz : PCIDIVMODE: 01 (1/3)

	 * CPU 166MHz: PCI 33MHz : PCIDIVMODE: 10 (1/5)

	 * CPU 200MHz: PCI 33MHz : PCIDIVMODE: 11 (1/6)

	 * i.e. S9[3]: ON (83MHz), OFF (100MHz)

 166MHz */

 200MHz */

	/*

	 * ASSUMPTION: PCIDIVMODE is configured for PCI 33MHz or 66MHz.

	 *

	 * For TX4937:

	 * PCIDIVMODE[12:11]'s initial value is given by S1[5:4] (ON:0, OFF:1)

	 * PCIDIVMODE[10] is 0.

	 * CPU 266MHz: PCI 33MHz : PCIDIVMODE: 000 (1/8)

	 * CPU 266MHz: PCI 66MHz : PCIDIVMODE: 001 (1/4)

	 * CPU 300MHz: PCI 33MHz : PCIDIVMODE: 010 (1/9)

	 * CPU 300MHz: PCI 66MHz : PCIDIVMODE: 011 (1/4.5)

	 * CPU 333MHz: PCI 33MHz : PCIDIVMODE: 100 (1/10)

	 * CPU 333MHz: PCI 66MHz : PCIDIVMODE: 101 (1/5)

 266MHz */

 300MHz */

 333MHz */

/*

 * rbtx4927 specific prom routines

 *

 * Author: MontaVista Software, Inc.

 *	   source@mvista.com

 *

 * Copyright 2001-2002 MontaVista Software Inc.

 *

 * Copyright (C) 2004 MontaVista Software Inc.

 * Author: Manish Lachwani, mlachwani@mvista.com

 *

 *  This program is free software; you can redistribute it and/or modify it

 *  under the terms of the GNU General Public License as published by the

 *  Free Software Foundation; either version 2 of the License, or (at your

 *  option) any later version.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,

 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS

 *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND

 *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

 *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the GNU General Public License along

 *  with this program; if not, write to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/*

 * Toshiba RBTX4927 specific interrupt handlers

 *

 * Author: MontaVista Software, Inc.

 *	   source@mvista.com

 *

 * Copyright 2001-2002 MontaVista Software Inc.

 *

 *  This program is free software; you can redistribute it and/or modify it

 *  under the terms of the GNU General Public License as published by the

 *  Free Software Foundation; either version 2 of the License, or (at your

 *  option) any later version.

 *

 *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED

 *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

 *  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,

 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS

 *  OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND

 *  ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR

 *  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE

 *  USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the GNU General Public License along

 *  with this program; if not, write to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/*

 * I8259A_IRQ_BASE+00

 * I8259A_IRQ_BASE+01 PS2/Keyboard

 * I8259A_IRQ_BASE+02 Cascade RBTX4927-ISA (irqs 8-15)

 * I8259A_IRQ_BASE+03

 * I8259A_IRQ_BASE+04

 * I8259A_IRQ_BASE+05

 * I8259A_IRQ_BASE+06

 * I8259A_IRQ_BASE+07

 * I8259A_IRQ_BASE+08

 * I8259A_IRQ_BASE+09

 * I8259A_IRQ_BASE+10

 * I8259A_IRQ_BASE+11

 * I8259A_IRQ_BASE+12 PS2/Mouse (not supported at this time)

 * I8259A_IRQ_BASE+13

 * I8259A_IRQ_BASE+14 IDE

 * I8259A_IRQ_BASE+15

 *

 * MIPS_CPU_IRQ_BASE+00 Software 0

 * MIPS_CPU_IRQ_BASE+01 Software 1

 * MIPS_CPU_IRQ_BASE+02 Cascade TX4927-CP0

 * MIPS_CPU_IRQ_BASE+03 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+04 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+05 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+06 Multiplexed -- do not use

 * MIPS_CPU_IRQ_BASE+07 CPU TIMER

 *

 * TXX9_IRQ_BASE+00

 * TXX9_IRQ_BASE+01

 * TXX9_IRQ_BASE+02

 * TXX9_IRQ_BASE+03 Cascade RBTX4927-IOC

 * TXX9_IRQ_BASE+04

 * TXX9_IRQ_BASE+05 RBTX4927 RTL-8019AS ethernet

 * TXX9_IRQ_BASE+06

 * TXX9_IRQ_BASE+07

 * TXX9_IRQ_BASE+08 TX4927 SerialIO Channel 0

 * TXX9_IRQ_BASE+09 TX4927 SerialIO Channel 1

 * TXX9_IRQ_BASE+10

 * TXX9_IRQ_BASE+11

 * TXX9_IRQ_BASE+12

 * TXX9_IRQ_BASE+13

 * TXX9_IRQ_BASE+14

 * TXX9_IRQ_BASE+15

 * TXX9_IRQ_BASE+16 TX4927 PCI PCI-C

 * TXX9_IRQ_BASE+17

 * TXX9_IRQ_BASE+18

 * TXX9_IRQ_BASE+19

 * TXX9_IRQ_BASE+20

 * TXX9_IRQ_BASE+21

 * TXX9_IRQ_BASE+22 TX4927 PCI PCI-ERR

 * TXX9_IRQ_BASE+23 TX4927 PCI PCI-PMA (not used)

 * TXX9_IRQ_BASE+24

 * TXX9_IRQ_BASE+25

 * TXX9_IRQ_BASE+26

 * TXX9_IRQ_BASE+27

 * TXX9_IRQ_BASE+28

 * TXX9_IRQ_BASE+29

 * TXX9_IRQ_BASE+30

 * TXX9_IRQ_BASE+31

 *

 * RBTX4927_IRQ_IOC+00 FPCIB0 PCI-D (SouthBridge)

 * RBTX4927_IRQ_IOC+01 FPCIB0 PCI-C (SouthBridge)

 * RBTX4927_IRQ_IOC+02 FPCIB0 PCI-B (SouthBridge/IDE/pin=1,INTR)

 * RBTX4927_IRQ_IOC+03 FPCIB0 PCI-A (SouthBridge/USB/pin=4)

 * RBTX4927_IRQ_IOC+04

 * RBTX4927_IRQ_IOC+05

 * RBTX4927_IRQ_IOC+06

 * RBTX4927_IRQ_IOC+07

 *

 * NOTES:

 * SouthBridge/INTR is mapped to SouthBridge/A=PCI-B/#58

 * SouthBridge/ISA/pin=0 no pci irq used by this device

 * SouthBridge/IDE/pin=1 no pci irq used by this device, using INTR

 * via ISA IRQ14

 * SouthBridge/USB/pin=4 using pci irq SouthBridge/D=PCI-A=#59

 * SouthBridge/PMC/pin=0 no pci irq used by this device

 * SuperIO/PS2/Keyboard, using INTR via ISA IRQ1

 * SuperIO/PS2/Mouse, using INTR via ISA IRQ12 (mouse not currently supported)

 * JP7 is not bus master -- do NOT use -- only 4 pci bus master's

 * allowed -- SouthBridge, JP4, JP5, JP6

 mask all IOC interrupts */

 clear SoftInt interrupts */

 cpu timer */

 tx4927 pic */

 user line 0 */

 user line 1 */

 Onboard 10M Ether: High Active */

/*

 * Toshiba RBTX4939 setup routines.

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright (C) 2000-2001,2005-2007 Toshiba Corporation

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 __BIG_ENDIAN && CONFIG_SMC91X */

 mem:64K(max), io:64K(max) (enough for ETH0,ETH1) */

 CONFIG_PCI */

 64M ROM */

 1M IOC */

 ISA */

 use user-configured speed */

 initialise by myself */

 Use "dot" in 7seg LEDs */

 bit7: reserved for LED class */

 convert from map_to_seg7() notation */

 special mapping for boot rom */

 BOOT Mode: USER ROM1 / USER ROM2 */

 rotate A[23:22] */

 BOOT Mode: Monitor ROM */

 swap A[22] */

 see inline_map_write() in mtd/map.h */

 BOOT Mode: USER ROM1 / USER ROM2 */

 BOOT Mode: Monitor ROM */

 BOOT Mode: USER ROM1 / USER ROM2 */

 BOOT Mode: Monitor ROM */

 BOOT Mode: ROM Emulator */

 override default irq flag defined in smc91x.h */

 TC58DVM82A1FT: tDH=10ns, tWP=tRP=tREADID=35ns */

 ch1:8bit, ch2:16bit */

 always enable ATA0 */

/*

 * rbtx4939 specific prom routines

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

/*

 * Toshiba RBTX4939 interrupt routines

 * Based on linux/arch/mips/txx9/rbtx4938/irq.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright (C) 2000-2001,2005-2006 Toshiba Corporation

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

/*

 * RBTX4939 IOC controller definition

 redirect IOC interrupts */

 mask all IOC interrupts */

 clear SoftInt interrupts */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Virtual EISA root driver.

 * Acts as a placeholder if we don't have a proper EISA bridge.

 *

 * (C) 2003 Marc Zyngier <maz@wild-wind.fr.eu.org>

 * modified for SNI usage by Thomas Bogendoerfer

/* The default EISA device parent (virtual root device).

		/* A real bridge may have been registered before

/*

 * Setup pointers to hardware-dependent routines.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 97, 98, 2000, 03, 04, 06 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2006,2007 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

	ioport_resource.end = sni_io_resource.end;

	/*

	 * Setup (E)ISA I/O memory access stuff

	/*

	 * firmware doesn't set the ram size correct, so we

	 * need to do it here, otherwise we get screen corruption

	 * on older Cirrus chips

 unlock all extension registers */

/*

 * A20R specific code

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2006 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

 16bit mpu port access */

/*

 * Trigger chipset to update CPU's CAUSE IP field

/*

 * hwint 0 receive all interrupts

 FIXME, remove if not needed */

/*

 * RM200 specific code

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2006,2007 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

 *

 * i8259 parts ripped out of arch/mips/kernel/i8259.c

/*

 * RM200 has an ISA and an EISA bus. The iSA bus is only used

 * for onboard devices and also has twi i8259 PICs. Since these

 * PICs are no accessible via inb/outb the following code uses

 * readb/writeb to access them

 i8259A PIC related value */

/*

 * This contains the irq mask for both 8259A irq controllers,

 ISR register */

/*

 * Careful! The 8259A is a fragile beast, it pretty

 * much _has_ to be done exactly like this (mask it

 * first, _then_ send the EOI, and the order of EOI

 * to the two 8259s is important!

	/*

	 * Lightweight spurious IRQ detection. We do not want

	 * to overdo spurious IRQ handling - it's usually a sign

	 * of hardware problems, so we only do the checks we can

	 * do without slowing down good hardware unnecessarily.

	 *

	 * Note that IRQ7 and IRQ15 (the two spurious IRQs

	 * usually resulting from the 8259A-1|2 PICs) occur

	 * even if the IRQ is masked in the 8259A. Thus we

	 * can check spurious 8259A IRQs without doing the

	 * quite slow i8259A_irq_real() call for every IRQ.

	 * This does not cover 100% of spurious interrupts,

	 * but should be enough to warn the user that there

	 * is something bad going on ...

	/*

	 * this is the slow path - should happen rarely.

		/*

		 * oops, the IRQ _is_ in service according to the

		 * 8259A - not spurious, go handle it.

		/*

		 * At this point we can be sure the IRQ is spurious,

		 * let's ACK and report it. [once per IRQ]

		/*

		 * Theoretically we do not have to handle this IRQ,

		 * but in Linux this does not cause problems and is

		 * simpler for us.

/*

 * Do the traditional i8259 interrupt polling thing.  This is for the few

 * cases where no better interrupt acknowledge method is available and we

 * absolutely must touch the i8259.

 Perform an interrupt acknowledge cycle on controller 1. */

 prepare for poll */

		/*

		 * Interrupt is cascaded so perform interrupt

		 * acknowledge on controller 2.

 prepare for poll */

		/*

		 * This may be a spurious interrupt.

		 *

		 * Read the interrupt status register (ISR). If the most

		 * significant bit is not set then there is no valid

		 * interrupt.

 ISR register */

 wait for 8259A to initialize */

/*

 * IRQ2 is cascade interrupt to second interrupt controller

 ISA irq handler */

 Actually we've got more interrupts to handle ...  */

 SPDX-License-Identifier: GPL-2.0

 .mult, .shift, .max_delta_ns and .min_delta_ns left uninitialized */

/*

 * a20r platform uses 2 counters to divide the input frequency.

 * Counter 2 output is connected to Counter 0 & 1 input.

 Start the counter. */

 Get initial counter invariant */

 Latch and spin until top byte of counter0 is zero */

 Stop the counter. */

	/*

	 * Return the difference, this is how far the r4k counter increments

	 * for every 1/HZ seconds. We round off the nearest 1 MHz of master

	 * clock (= 1000000 / HZ / 2).

return (ct1 - ct0 + (500000/HZ/2)) / (500000/HZ) * (500000/HZ);*/

/*

 * Here we need to calibrate the cycle counter to at least be close.

	/*

	 * Figure out the r4k offset, the algorithm is very simple and works in

	 * _all_ cases as long as the 8254 counter register itself works ok (as

	 * an interrupt driving timer it does not because of bug, this is why

	 * we are using the onchip r4k counter/compare register to serve this

	 * purpose, but for r4k_offset calculation it will work ok for us).

	 * There are other very complicated ways of performing this calculation

	 * but this one works just fine so I am not going to futz around. ;-)

 Prime cache. */

 Prime cache. */

 Zero is NOT an option. */

/*

 * PCIMT specific code

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 97, 98, 2000, 03, 04, 06 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2006,2007 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

/*

 * A bit more gossip about the iron we're running on ...

		/*

		 * this region should only be 4 bytes long,

		 * but it's 16MB on all RM300C I've checked

 request I/O space for devices used on all i[345]86 PCs */

 request MEM space for devices used on all i[345]86 PCs */

/*

 * hwint0 should deal with MP agent, ASIC PCI, EISA NMI and debug

 * button interrupts.  Later ...

/*

 * hwint 1 deals with EISA and SCSI interrupts,

 *

 * The EISA_INT bit in CSITPEND is high active, all others are low active.

		/*

		 * Note: ASIC PCI's builtin interrupt acknowledge feature is

		 * broken.  Using it may result in loss of some or all i8259

		 * interrupts, so don't use PCIMT_INT_ACKNOWLEDGE ...

/*

 * hwint 3 should deal with the PCI A - D interrupts,

 Actually we've got more interrupts to handle ...  */

/*

 * PCI Tower specific code

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2006 Thomas Bogendoerfer (tsbogend@alpha.franken.de)

 request I/O space for devices used on all i[345]86 PCs */

 CONFIG_PCI */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/mips/sni/process.c

 *

 *  Reset a SNI machine.

/*

 * This routine reboots the machine by asking the keyboard

 * controller to pulse the reset-line low. We try that for a while,

 * and if it doesn't work, we do some other stupid things.

 XXX This ends up at the ARC firmware prompt ...  */

	/* This does a normal via the keyboard controller like a PC.

 pulse reset low */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1992 Linus Torvalds

 * Copyright (C) 1994 - 2000 Ralf Baechle

 * Copyright (C) 2006 Thomas Bogendoerfer

 ISA irq handler */

/*

 * On systems with i8259-style interrupt controllers we assume for

 * driver compatibility reasons interrupts 0 - 15 to be the i8295

 * interrupts even if the hardware uses a different interrupt numbering.

 Integrated i8259  */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Florian Fainelli <florian@openwrt.org>

 filled at runtime */

 filled at runtime */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2008 Florian Fainelli <florian@openwrt.org>

 * Copyright (C) 2012 Jonas Gorski <jonas.gorski@gmail.com>

 extract nvram data */

 check checksum before using data */

	/* Cable modems have a different NVRAM which is embedded in the eCos

	 * firmware and not easily extractible, give at least a MAC address

	 * pool.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 soft reset all blocks */

 Jump to the power on address. */

 set high vectors (base at 0xbfc00000 */

 run uncached in kseg0 */

 remove all wired TLB entries */

 mask and clear all external irq */

/*

 * return system type in /proc/cpuinfo

 register gpiochip */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 stop any running watchdog */

 disable all hardware blocks clock for now */

 do low level board init */

 set up SMP */

		/*

		 * BCM6328 might not have its second CPU enabled, while BCM3368

		 * and BCM6358 need special handling for their shared TLB, so

		 * disable SMP for now.

		/*

		 * The bootloader has set up the CPU1 reset vector at

		 * 0xa000_0200.

		 * This conflicts with the special interrupt vector (IV).

		 * The bootloader has also set up CPU1 to respond to the wrong

		 * IPI interrupt.

		 * Here we will start up CPU1 in the background and ask it to

		 * reconfigure itself then go back to sleep.

		/*

		 * FIXME: we really should have some sort of hazard barrier here

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 filled at runtime */

 start & end filled at runtime */

 start filled at runtime */

 start filled at runtime */

 copy given platform data */

 adjust them in case internal phy is used */

 internal phy only exists for enet0 */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * Ethernet MAC "misc" clock: dma clocks and main clock on 6348

 BCMCPU_IS_6358 */

/*

 * Ethernet MAC clocks: only relevant on 6358, silently enable misc

 * clocks

/*

 * Ethernet PHY clock

/*

 * Ethernet switch SAR clock

/*

 * Ethernet switch USB clock

/*

 * Ethernet switch clock

 reset switch core afer clock change */

/*

 * PCM clock

/*

 * USB host clock

/*

 * USB device clock

/*

 * SPI clock

 BCMCPU_IS_6368 */

/*

 * HSSPI clock

/*

 * HSSPI PLL

/*

 * XTM clock

 reset sar core afer clock change */

/*

 * IPsec clock

/*

 * PCIe clock

/*

 * Internal peripheral clock

/*

 * Linux clock API implementation

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

 fixed rate clocks */

 gated clocks */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2008-2011 Florian Fainelli <florian@openwrt.org>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 wait for any previous char to be transmitted */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2009 Florian Fainelli <florian@openwrt.org>

 BCM6338 has a fixed 240 Mhz frequency */

 BCM6345 has a fixed 140Mhz frequency */

 16MHz * (N1 + 1) * (N2 + 2) / (M1_CPU + 1) */

 16MHz * N1 * N2 / M1_CPU */

 (64MHz / P1) * P2 * NDIV / M1_CPU */

/*

 * attempt to detect the amount of memory installed

 0 => 11 address bits ... 2 => 13 address bits */

 0 => 8 address bits ... 2 => 10 address bits */

 soc registers location depends on cpu type */

	/*

	 * really early to panic, but delaying panic would not help since we

	 * will never get any working console

 read out CPU type */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * check if given chip select exists

/*

 * Configure chipselect base address and size (bytes).

 * Size must be a power of two between 8k and 256M.

 sanity check on size */

 8k => 0 - 256M => 15 */

/*

 * configure chipselect timing (ns)

/*

 * configure other chipselect parameter (data bus size, ...)

 none of this fields apply to pcmcia */

/*

 * set cs status (enable/disable)

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2009-2011 Florian Fainelli <florian@openwrt.org>

 * Copyright (C) 2010 Tanguy Bouzeloc <tanguy.bouzeloc@efixo.com>

 filled at runtime */

 filled at runtime */

 filled at runtime */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2011 Florian Fainelli <florian@openwrt.org>

 filled at runtime */

 filled at runtime */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 pcmcia registers */

 start & end filled at runtime */

 pcmcia memory zone resources */

 PCMCIA irq */

 start filled at runtime */

 declare PCMCIA IO resource also */

 use correct pcmcia ready gpio depending on processor */

 configure pcmcia chip selects */

/*

 * Broadcom BCM63xx flash registration

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2008 Florian Fainelli <florian@openwrt.org>

 * Copyright (C) 2012 Jonas Gorski <jonas.gorski@gmail.com>

 filled at runtime */

 filled at runtime */

 no way to auto detect so assume parallel */

 read base address of boot chip select (0) */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2012 Jonas Gorski <jonas.gorski@gmail.com>

 filled at runtime */

 filled at runtime */

 filled at runtime */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2012 Jonas Gorski <jonas.gorski@gmail.com>

/*

 * core reset bits

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2008 Nicolas Schichan <nschichan@freebox.fr>

/*

 * dispatch internal devices IRQ (uart, enet, watchdog, ...). do not

 * prioritize any interrupt relatively to another. the static counter

 * will resume the loop where it ended the last time we left this

 * function.

 read registers in reverse order */				\

/*

 * internal IRQs operations: only mask/unmask on PERF irq mask

 * register.

/*

 * external IRQs operations: mask/unmask and clear on PERF external

 * irq control register.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2012 Kevin Cernekee <cernekee@gmail.com>

 * Copyright (C) 2012 Broadcom Corporation

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 start & end filled at runtime */

 start filled at runtime */

 start & end filled at runtime */

 start filled at runtime */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2008 Florian Fainelli <florian@openwrt.org>

/*

 * known 3368 boards

 CONFIG_BCM63XX_CPU_3368 */

/*

 * known 6328 boards

 CONFIG_BCM63XX_CPU_6328 */

/*

 * known 6338 boards

 CONFIG_BCM63XX_CPU_6338 */

/*

 * known 6345 boards

 CONFIG_BCM63XX_CPU_6345 */

/*

 * known 6348 boards

 CONFIG_BCM63XX_CPU_6348 */

/*

 * known 6358 boards

 CONFIG_BCM63XX_CPU_6358 */

/*

 * all boards

 CONFIG_BCM63XX_CPU_3368 */

 CONFIG_BCM63XX_CPU_6328 */

 CONFIG_BCM63XX_CPU_6338 */

 CONFIG_BCM63XX_CPU_6345 */

 CONFIG_BCM63XX_CPU_6348 */

 CONFIG_BCM63XX_CPU_6358 */

/*

 * Register a sane SPROMv2 to make the on-board

 * bcm4318 WLAN work

 CONFIG_SSB_PCIHOST */

/*

 * return board name for /proc/cpuinfo

/*

 * early init callback, read nvram data from flash and checksum it

	/* read base address of boot chip select (0)

	 * 6328/6362 do not have MPI but boot from a fixed address

 dump cfe version */

 find board by name */

 copy, board desc array is marked initdata */

 bail out if board is not found, will complain later */

	/* setup pin multiplexing depending on board enabled device,

	 * this has to be done this early since PCI init is done

 CONFIG_PCI */

/*

 * second stage init callback, good time to panic if we couldn't

 * identify on which board we're running since early printk is working

 make sure we're running on expected cpu */

/*

 * third stage init callback, register all board devices.

	/* Generate MAC address for WLAN and register our SPROM,

	 * do this after registering enet devices

 CONFIG_SSB_PCIHOST */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

 * Copyright (C) 2014 Kevin Cernekee <cernekee@gmail.com>

	/*

	 * Some experimental CM boxes are set up to let CM own the Viper TP0

	 * and let Linux own TP1.  This requires moving the kernel

	 * load address to a non-conflicting region (e.g. via

	 * CONFIG_PHYSICAL_START) and supplying an alternate DTB.

	 * If we detect this condition, we need to move the MIPS exception

	 * vectors up to an area that we own.

	 *

	 * This is distinct from the OTHER special case mentioned in

	 * smp-bmips.c (boot on TP1, but enable SMP, then TP0 becomes our

	 * logical CPU#1).  For the Viper TP1 case, SMP is off limits.

	 *

	 * Also note that many BMIPS435x CPUs do not have a

	 * BMIPS_RELO_VECTOR_CONTROL_1 register, so it isn't safe to just

	 * write VMLINUX_LOAD_ADDRESS into that register on every SoC.

	/*

	 * The bootloader has set up the CPU1 reset vector at

	 * 0xa000_0200.

	 * This conflicts with the special interrupt vector (IV).

	 * The bootloader has also set up CPU1 to respond to the wrong

	 * IPI interrupt.

	 * Here we will start up CPU1 in the background and ask it to

	 * reconfigure itself then go back to sleep.

 Check CPU1 status in OTP (it is usually disabled) */

	/*

	 * BCM3368/BCM6358 need special handling for their shared TLB, so

	 * disable SMP for now

 intended to somewhat resemble ARM; see Documentation/arm/booting.rst */

 Disable SMP boot unless both CPUs are listed in DT and !disabled */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2014 Kevin Cernekee <cernekee@gmail.com>

/*

 * BCM338x has configurable address translation windows which allow the

 * peripherals' DMA addresses to be different from the Zephyr-visible

 * physical addresses.  e.g. usb_dma_addr = zephyr_pa ^ 0x08000000

 *

 * If the "brcm,ubus" node has a "dma-ranges" property we will enable this

 * translation globally using the provided information.  This implements a

 * very limited subset of "dma-ranges" support and it will probably be

 * replaced by a more generic version later.

 Flush stale data out of the readahead cache */

 add a dummy (zero) entry at the end as a sentinel */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2014 Broadcom Corporation

 * Author: Kevin Cernekee <cernekee@gmail.com>

 Only these controllers support SMP IRQ affinity */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 Ralf Baechle (ralf@linux-mips.org)

 RTC registers on IP32 are each padded by 256 bytes (0x100). */

/*

 * Code to handle IP32 IRQs

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000 Harald Koerfgen

 * Copyright (C) 2001 Keith M Wesolowski

 issue a PIO read to make sure no PIO writes are pending */

/*

 * O2 irq map

 *

 * IP0 -> software (ignored)

 * IP1 -> software (ignored)

 * IP2 -> (irq0) C crime 1.1 all interrupts; crime 1.5 ???

 * IP3 -> (irq1) X unknown

 * IP4 -> (irq2) X unknown

 * IP5 -> (irq3) X unknown

 * IP6 -> (irq4) X unknown

 * IP7 -> (irq5) 7 CPU count/compare timer (system timer)

 *

 * crime: (C)

 *

 * CRIME_INT_STAT 31:0:

 *

 * 0  ->  8  Video in 1

 * 1  ->  9 Video in 2

 * 2  -> 10  Video out

 * 3  -> 11  Mace ethernet

 * 4  -> S  SuperIO sub-interrupt

 * 5  -> M  Miscellaneous sub-interrupt

 * 6  -> A  Audio sub-interrupt

 * 7  -> 15  PCI bridge errors

 * 8  -> 16  PCI SCSI aic7xxx 0

 * 9  -> 17 PCI SCSI aic7xxx 1

 * 10 -> 18 PCI slot 0

 * 11 -> 19 unused (PCI slot 1)

 * 12 -> 20 unused (PCI slot 2)

 * 13 -> 21 unused (PCI shared 0)

 * 14 -> 22 unused (PCI shared 1)

 * 15 -> 23 unused (PCI shared 2)

 * 16 -> 24 GBE0 (E)

 * 17 -> 25 GBE1 (E)

 * 18 -> 26 GBE2 (E)

 * 19 -> 27 GBE3 (E)

 * 20 -> 28 CPU errors

 * 21 -> 29 Memory errors

 * 22 -> 30 RE empty edge (E)

 * 23 -> 31 RE full edge (E)

 * 24 -> 32 RE idle edge (E)

 * 25 -> 33 RE empty level

 * 26 -> 34 RE full level

 * 27 -> 35 RE idle level

 * 28 -> 36 unused (software 0) (E)

 * 29 -> 37 unused (software 1) (E)

 * 30 -> 38 unused (software 2) - crime 1.5 CPU SysCorError (E)

 * 31 -> 39 VICE

 *

 * S, M, A: Use the MACE ISA interrupt register

 * MACE_ISA_INT_STAT 31:0

 *

 * 0-7 -> 40-47 Audio

 * 8 -> 48 RTC

 * 9 -> 49 Keyboard

 * 10 -> X Keyboard polled

 * 11 -> 51 Mouse

 * 12 -> X Mouse polled

 * 13-15 -> 53-55 Count/compare timers

 * 16-19 -> 56-59 Parallel (16 E)

 * 20-25 -> 60-62 Serial 1 (22 E)

 * 26-31 -> 66-71 Serial 2 (28 E)

 *

 * Note that this means IRQs 12-14, 50, and 52 do not exist.  This is a

 * different IRQ map than IRIX uses, but that's OK as Linux irq handling

 * is quite different anyway.

 Some initial interrupts to set up */

/*

 * This is for pure CRIME interrupts - ie not MACE.  The advantage?

 * We get to split the register in half and do faster lookups.

 Edge triggered interrupts must be cleared. */

/*

 * This is for MACE PCI interrupts.  We can decrease bus traffic by masking

 * as close to the source as possible.	This also means we can take the

 * next chunk of the CRIME register in one piece.

/* This is used for MACE ISA interrupts.  That means bits 4-6 in the

 * CRIME register.

 edge triggered */

/* This is used for regular non-ISA, non-PCI MACE interrupts.  That means

 * bits 0-3 and 7 in the CRIME register.

 CRIME 1.1 appears to deliver all interrupts to this one pin. */

 change this to loop over all edge-triggered irqs, exception masked out ones */

	/*

	 * Sanity check interrupt numbering enum.

	 * MACE got 32 interrupts and there are 32 MACE ISA interrupts daisy

	 * chained.

 crime sometime delivers spurious interrupts, ignore them */

	/* Install our interrupt handler, then clear and disable all

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001, 2003 Keith M Wesolowski

 * Copyright (C) 2005 Ilya A. Volynets <ilya@total-knowledge.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1994, 1995, 1996, 1999, 2000 by Ralf Baechle

 * Copyright (C) 1999, 2000 by Silicon Graphics

 * Copyright (C) 2002  Maciej W. Rozycki

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2001 Keith M Wesolowski

 * Copyright (C) 2001 Paul Mundt

 * Copyright (C) 2003 Guido Guenther <agx@sigxcpu.org>

/*

 * Blink frequency during reboot grace period and when panicked.

 If the first __symbol_get failed, our module wasn't loaded. */

 No init process or button pressed twice.  */

 turn off the green LED */

 turn on the green led only */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2006  Ralf Baechle <ralf@linux-mips.org>

/*

 * Few notes.

 * 1. CPU sees memory as two chunks: 0-256M@0x0, and the rest @0x40000000+256M

 * 2. PCI sees memory as one big chunk @0x0 (or we could use 0x40000000 for

 *    native-endian)

 * 3. All other devices see memory as one big chunk at 0x40000000

 * 4. Non-PCI devices will pass NULL as struct device*

 *

 * Thus we translate differently, depending on device.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 Keith M Wesolowski

 * Copyright (C) 2005 Ilya A. Volynets (Total Knowledge)

/*

 * IP32 basic setup

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000 Harald Koerfgen

 * Copyright (C) 2002, 2003, 2005 Ilya A. Volynets

 * Copyright (C) 2006 Ralf Baechle <ralf@linux-mips.org>

/*

 * This is taken care of in here 'cause they say using Arc later on is

 * problematic

 foo */

 An arbitrary time; this can be decreased if reliability looks good */

/*

 * Setup pointers to hardware-dependent routines.

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1996, 1997, 1998, 2001, 07, 08 by Ralf Baechle

 * Copyright (C) 2001 MIPS Technologies, Inc.

 * Copyright (C) 2007 by Thomas Bogendoerfer

 Map 0xe0000000 -> 0x0:800005C0, 0xe0010000 -> 0x1:30000580 */

 Map 0xe2000000 -> 0x0:900005C0, 0xe3010000 -> 0x0:910005C0 */

 Map 0xe4000000 -> 0x0:600005C0, 0xe4100000 -> 400005C0 */

 request I/O space for devices used on all i[345]86 PCs */

 The RTC is outside the port address space */

/* Some Jazz machines seem to have an 8MHz crystal clock but I don't know

 ( 3072000 / 16) */

 SPDX-License-Identifier: GPL-2.0

/*

 * Mips Jazz DMA controller support

 * Copyright (C) 1995, 1996 by Andreas Busse

 *

 * NOTE: Some of the argument checking could be removed when

 * things have settled down. Also, instead of returning 0xffffffff

 * on failure of vdma_alloc() one could leave page #0 unused

 * and return the more usual NULL pointer as logical address.

/*

 * Set this to one to enable additional vdma debug code.

/*

 * Debug stuff

/*

 * Initialize the pagetable with a one-to-one mapping of

 * the first 16 Mbytes of main memory and declare all

 * entries to be unused. Using this method will at least

 * allow some early device driver operations to work.

/*

 * Initialize the Jazz R4030 dma controller

	/*

	 * Allocate 32k of memory for DMA page tables.	This needs to be page

	 * aligned and should be uncached to avoid cache flushing after every

	 * update.

	/*

	 * Clear the R4030 translation table

/*

 * Allocate DMA pagetables using a simple first-fit algorithm

 check arguments */

 invalid physical address */

 invalid physical address */

	/*

	 * Find free chunk

 nothing free */

 found */

	/*

	 * Mark pages as allocated

	/*

	 * Update translation table and return logical start address

/*

 * Free previously allocated dma translation pages

 * Note that this does NOT change the translation table,

 * it just marks the free'd pages as unused!

/*

 * Translate a physical address to a logical address.

 * This will return the logical address of the first

 * match.

/*

 * Translate a logical DMA address to a physical address

/*

 * Print DMA statistics

/*

 * DMA transfer functions

/*

 * Enable a DMA channel. Also clear any error conditions.

	/*

	 * Check error conditions first

	/*

	 * Clear all interrupt flags

	/*

	 * Enable the desired channel

/*

 * Disable a DMA channel

	/*

	 * After disabling a DMA channel a remote bus register should be

	 * read to ensure that the current DMA acknowledge cycle is completed.

/*

 * Set DMA mode. This function accepts the mode values used

 * to set a PC-style DMA controller. For the SCSI and FDC

 * channels, we also set the default modes each time we're

 * called.

 * NOTE: The FAST and BURST dma modes are supported by the

 * R4030 Rev. 2 and PICA chipsets only. I leave them disabled

 * for now.

 scsi */

			  R4030_MODE_FAST | */

			  R4030_MODE_BURST | */

 floppy */

			  R4030_MODE_FAST | */

			  R4030_MODE_BURST | */

/*

 * Set Transfer Address

/*

 * Set Transfer Count

/*

 * Get Residual

/*

 * Get DMA channel enable register

 SPDX-License-Identifier: GPL-2.0

/*

 * Reset a Jazz machine.

 *

 * We don't trust the firmware so we do it the classic way by poking and

 * stabbing at the keyboard controller ...

 Keyboard input buffer full */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1992 Linus Torvalds

 * Copyright (C) 1994 - 2001, 2003, 07 Ralf Baechle

 clear pending IRQs */

 clear error bits */

/*

 * On systems with i8259-style interrupt controllers we assume for

 * driver compatibility reasons interrupts 0 - 15 to be the i8259

 * interrupts even if the hardware uses a different interrupt numbering.

	/*

	 * this is a hack to get back the still needed wired mapping

	 * killed by init_mm()

 Map 0xe0000000 -> 0x0:800005C0, 0xe0010000 -> 0x1:30000580 */

 Map 0xe2000000 -> 0x0:900005C0, 0xe3010000 -> 0x0:910005C0 */

 Map 0xe4000000 -> 0x0:600005C0, 0xe4100000 -> 400005C0 */

 Integrated i8259  */

	/*

	 * Set clock to 100Hz.

	 *

	 * The R4030 timer receives an input clock of 1kHz which is divieded by

	 * a programmable 4-bit divider.  This makes it fairly inflexible.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Just-In-Time compiler for eBPF bytecode on MIPS.

 * Implementation of JIT functions common to 32-bit and 64-bit CPUs.

 *

 * Copyright (c) 2021 Anyfi Networks AB.

 * Author: Johan Almbladh <johan.almbladh@gmail.com>

 *

 * Based on code and ideas from

 * Copyright (c) 2017 Cavium, Inc.

 * Copyright (c) 2017 Shubham Bansal <illusionist.neo@gmail.com>

 * Copyright (c) 2011 Mircea Gherzan <mgherzan@gmail.com>

/*

 * Code overview

 * =============

 *

 * - bpf_jit_comp.h

 *   Common definitions and utilities.

 *

 * - bpf_jit_comp.c

 *   Implementation of JIT top-level logic and exported JIT API functions.

 *   Implementation of internal operations shared by 32-bit and 64-bit code.

 *   JMP and ALU JIT control code, register control code, shared ALU and

 *   JMP/JMP32 JIT operations.

 *

 * - bpf_jit_comp32.c

 *   Implementation of functions to JIT prologue, epilogue and a single eBPF

 *   instruction for 32-bit MIPS CPUs. The functions use shared operations

 *   where possible, and implement the rest for 32-bit MIPS such as ALU64

 *   operations.

 *

 * - bpf_jit_comp64.c

 *   Ditto, for 64-bit MIPS CPUs.

 *

 * Zero and sign extension

 * ========================

 * 32-bit MIPS instructions on 64-bit MIPS registers use sign extension,

 * but the eBPF instruction set mandates zero extension. We let the verifier

 * insert explicit zero-extensions after 32-bit ALU operations, both for

 * 32-bit and 64-bit MIPS JITs. Conditional JMP32 operations on 64-bit MIPs

 * are JITed with sign extensions inserted when so expected.

 *

 * ALU operations

 * ==============

 * ALU operations on 32/64-bit MIPS and ALU64 operations on 64-bit MIPS are

 * JITed in the following steps. ALU64 operations on 32-bit MIPS are more

 * complicated and therefore only processed by special implementations in

 * step (3).

 *

 * 1) valid_alu_i:

 *    Determine if an immediate operation can be emitted as such, or if

 *    we must fall back to the register version.

 *

 * 2) rewrite_alu_i:

 *    Convert BPF operation and immediate value to a canonical form for

 *    JITing. In some degenerate cases this form may be a no-op.

 *

 * 3) emit_alu_{i,i64,r,64}:

 *    Emit instructions for an ALU or ALU64 immediate or register operation.

 *

 * JMP operations

 * ==============

 * JMP and JMP32 operations require an JIT instruction offset table for

 * translating the jump offset. This table is computed by dry-running the

 * JIT without actually emitting anything. However, the computed PC-relative

 * offset may overflow the 18-bit offset field width of the native MIPS

 * branch instruction. In such cases, the long jump is converted into the

 * following sequence.

 *

 *    <branch> !<cond> +2    Inverted PC-relative branch

 *    nop                    Delay slot

 *    j <offset>             Unconditional absolute long jump

 *    nop                    Delay slot

 *

 * Since this converted sequence alters the offset table, all offsets must

 * be re-calculated. This may in turn trigger new branch conversions, so

 * the process is repeated until no further changes are made. Normally it

 * completes in 1-2 iterations. If JIT_MAX_ITERATIONS should reached, we

 * fall back to converting every remaining jump operation. The branch

 * conversion is independent of how the JMP or JMP32 condition is JITed.

 *

 * JMP32 and JMP operations are JITed as follows.

 *

 * 1) setup_jmp_{i,r}:

 *    Convert jump conditional and offset into a form that can be JITed.

 *    This form may be a no-op, a canonical form, or an inverted PC-relative

 *    jump if branch conversion is necessary.

 *

 * 2) valid_jmp_i:

 *    Determine if an immediate operations can be emitted as such, or if

 *    we must fall back to the register version. Applies to JMP32 for 32-bit

 *    MIPS, and both JMP and JMP32 for 64-bit MIPS.

 *

 * 3) emit_jmp_{i,i64,r,r64}:

 *    Emit instructions for an JMP or JMP32 immediate or register operation.

 *

 * 4) finish_jmp_{i,r}:

 *    Emit any instructions needed to finish the jump. This includes a nop

 *    for the delay slot if a branch was emitted, and a long absolute jump

 *    if the branch was converted.

 Convenience macros for descriptor access */

/*

 * Push registers on the stack, starting at a given depth from the stack

 * pointer and increasing. The next depth to be written is returned.

 sizeof(long) == 8 */

/*

 * Pop registers from the stack, starting at a given depth from the stack

 * pointer and increasing. The next depth to be read is returned.

 sizeof(long) == 8 */

 Compute the 28-bit jump target address from a BPF program location */

 Compute the PC-relative offset to relative BPF program offset */

 dst = imm (register width) */

 dst = src (register width) */

 Validate ALU immediate range */

 All legal eBPF values are valid */

 imm must be 16 bits */

 -imm must be 16 bits */

 imm must be 16 bits unsigned */

 imm must be zero or a positive power of two */

 imm must be an 17-bit power of two */

 Rewrite ALU immediate operation */

 imm == 0 is a no-op */

 dst * 1 is a no-op */

 dst * 0 is dst & 0 */

 dst * (1 << n) is dst << n */

 dst / 1 is a no-op */

 dst / (1 << n) is dst >> n */

 dst % (1 << n) is dst & ((1 << n) - 1) */

 ALU immediate operation (32-bit) */

 dst = -dst */

 dst = dst & imm */

 dst = dst | imm */

 dst = dst ^ imm */

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst >> imm (arithmetic) */

 dst = dst + imm */

 dst = dst - imm */

 ALU register operation (32-bit) */

 dst = dst & src */

 dst = dst | src */

 dst = dst ^ src */

 dst = dst << src */

 dst = dst >> src */

 dst = dst >> src (arithmetic) */

 dst = dst + src */

 dst = dst - src */

 dst = dst * src */

 dst = dst / src */

 dst = dst % src */

 Atomic read-modify-write (32-bit) */

 Delay slot */

 Atomic compare-and-exchange (32-bit) */

 Delay slot */

 Delay slot */

 Swap bytes and truncate a register word or half word */

 Swap bytes in a word */

 tmp  = dst << 16 */

 dst = dst >> 16  */

 dst = dst | tmp  */

 msk = 0x00ff0000 */

 msk = msk | 0xff */

 tmp = dst & msk  */

 tmp = tmp << 8   */

 dst = dst >> 8   */

 dst = dst & msk  */

 reg = dst | tmp  */

 Swap bytes in a half word */

 t = d & 0xff00 */

 t = t >> 8     */

 d = d & 0x00ff */

 d = d << 8     */

 d = d | t      */

 Validate jump immediate range */

 Immediate value not used */

 No immediate operation */

 imm must be 16 bits unsigned */

 imm must be 16 bits */

 imm + 1 must be 16 bits */

 Invert a conditional jump operation */

 Prepare a PC-relative jump operation */

 Do not compute offsets on the first pass */

 Skip jumps never taken */

 Convert jumps always taken */

	/*

	 * Current ctx->jit_index points to the start of the branch preamble.

	 * Since the preamble differs among different branch conditionals,

	 * the current index cannot be used to compute the branch offset.

	 * Instead, we use the offset table value for the next instruction,

	 * which gives the index immediately after the branch delay slot.

	/*

	 * The PC-relative branch offset field on MIPS is 18 bits signed,

	 * so if the computed offset is larger than this we generate a an

	 * absolute jump that we skip with an inverted conditional branch.

 Prepare a PC-relative jump operation with immediate conditional */

 Prepare a PC-relative jump operation with register conditional */

 Finish a PC-relative jump operation */

 Emit conditional branch delay slot */

	/*

	 * Emit an absolute long jump with delay slot,

	 * if the PC-relative branch was converted.

 Jump immediate (32-bit) */

 No-op, used internally for branch optimization */

 PC += off if dst & imm */

 PC += off if (dst & imm) == 0 (not in BPF, used for long jumps) */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 Jump register (32-bit) */

 No-op, used internally for branch optimization */

 PC += off if dst == src */

 PC += off if dst != src */

 PC += off if dst & src */

 PC += off if (dst & imm) == 0 (not in BPF, used for long jumps) */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 Jump always */

 Jump to epilogue */

 Build the program body from eBPF bytecode */

 Store the end offset, where the epilogue begins */

 Set the branch conversion flag on all instructions */

 We are guaranteed to have aligned memory. */

 Increments p */

	/*

	 * If BPF JIT was not enabled then we must fall back to

	 * the interpreter.

	/*

	 * If constant blinding was enabled and we failed during blinding

	 * then we must fall back to the interpreter. Otherwise, we save

	 * the new JITed code.

	/*

	 * Not able to allocate memory for descriptors[], then

	 * we must fall back to the interpreter

 First pass discovers used resources */

	/*

	 * Second pass computes instruction offsets.

	 * If any PC-relative branches are out of range, a sequence of

	 * a PC-relative branch + a jump is generated, and we have to

	 * try again from the beginning to generate the new offsets.

	 * This is done until no additional conversions are necessary.

	 * The last two iterations are done with all branches being

	 * converted, to guarantee offset table convergence within a

	 * fixed number of iterations.

 Now we know the size of the structure to make */

	/*

	 * Not able to allocate memory for the structure then

	 * we must fall back to the interpretation

 Actual pass to generate final JIT code */

	/*

	 * If building the JITed code fails somehow,

	 * we fall back to the interpretation.

 Populate line info meta data */

 Set as read-only exec and flush instruction cache */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Just-In-Time compiler for eBPF bytecode on MIPS.

 * Implementation of JIT functions for 32-bit CPUs.

 *

 * Copyright (c) 2021 Anyfi Networks AB.

 * Author: Johan Almbladh <johan.almbladh@gmail.com>

 *

 * Based on code and ideas from

 * Copyright (c) 2017 Cavium, Inc.

 * Copyright (c) 2017 Shubham Bansal <illusionist.neo@gmail.com>

 * Copyright (c) 2011 Mircea Gherzan <mgherzan@gmail.com>

 MIPS a4-a7 are not available in the o32 ABI */

 Stack is 8-byte aligned in o32 ABI */

/*

 * The top 16 bytes of a stack frame is reserved for the callee in O32 ABI.

 * This corresponds to stack space for register arguments a0-a3.

 Temporary 64-bit register used by JIT */

/*

 * Number of prologue bytes to skip when doing a tail call.

 * Tail call count (TCC) initialization (8 bytes) always, plus

 * R0-to-v0 assignment (4 bytes) if big endian.

 CPU registers holding the callee return value */

 CPU registers arguments passed to callee directly */

 CPU register arguments passed to callee on stack */

 Caller-saved CPU registers */

 Callee-saved CPU registers */

/*

 * Mapping of 64-bit eBPF registers to 32-bit native MIPS registers.

 *

 * 1) Native register pairs are ordered according to CPU endiannes, following

 *    the MIPS convention for passing 64-bit arguments and return values.

 * 2) The eBPF return value, arguments and callee-saved registers are mapped

 *    to their native MIPS equivalents.

 * 3) Since the 32 highest bits in the eBPF FP register are always zero,

 *    only one general-purpose register is actually needed for the mapping.

 *    We use the fp register for this purpose, and map the highest bits to

 *    the MIPS register r0 (zero).

 * 4) We use the MIPS gp and at registers as internal temporary registers

 *    for constant blinding. The gp register is callee-saved.

 * 5) One 64-bit temporary register is mapped for use when sign-extending

 *    immediate operands. MIPS registers t6-t9 are available to the JIT

 *    for as temporaries when implementing complex 64-bit operations.

 *

 * With this scheme all eBPF registers are being mapped to native MIPS

 * registers without having to use any stack scratch space. The direct

 * register mapping (2) simplifies the handling of function calls.

 Return value from in-kernel function, and exit value from eBPF */

 Arguments from eBPF program to in-kernel function */

 Remaining arguments, to be passed on the stack per O32 ABI */

 Callee-saved registers that in-kernel function will preserve */

 Read-only frame pointer to access the eBPF stack */

 Temporary register for blinding constants */

 Temporary register for internal JIT use */

 Get low CPU register for a 64-bit eBPF register mapping */

 Get high CPU register for a 64-bit eBPF register mapping */

/*

 * Mark a 64-bit CPU register pair as clobbered, it needs to be

 * saved/restored by the program if callee-saved.

 dst = imm (sign-extended) */

 Zero extension, if verifier does not do it for us  */

 Load delay slot, if ISA mandates it */

 ALU immediate operation (64-bit) */

	/*

	 * ADD/SUB with all but the max negative imm can be handled by

	 * inverting the operation and the imm value, saving one insn.

 Move immediate to temporary register */

 dst = dst + imm */

 dst = dst - imm */

 dst = dst | imm */

 dst = dst & imm */

 dst = dst ^ imm */

 ALU register operation (64-bit) */

 dst = dst + src */

 dst = dst - src */

 dst = dst | src */

 dst = dst & src */

 dst = dst ^ src */

 ALU invert (64-bit) */

 ALU shift immediate (64-bit) */

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst >> imm (arithmetic) */

 ALU shift register (64-bit) */

 t1 = src & 32          */

 PC += 16 if t1 == 0    */

 t2 = ~src (delay slot) */

 dst = dst << src */

 Next: shift >= 32 */

 dh = dl << src */

 dl = 0         */

 PC += 20       */

 +16: shift < 32 */

 t1 = dl >> 1   */

 t1 = t1 >> t2  */

 dl = dl << src */

 dh = dh << src */

 dh = dh | t1   */

 dst = dst >> src */

 Next: shift >= 32 */

 dl = dh >> src */

 dh = 0         */

 PC += 20       */

 +16: shift < 32 */

 t1 = dl << 1   */

 t1 = t1 << t2  */

 dl = dl >> src */

 dh = dh >> src */

 dl = dl | t1   */

 dst = dst >> src (arithmetic) */

 Next: shift >= 32 */

 dl = dh >>a src */

 dh = dh >>a 31  */

 PC += 20        */

 +16: shift < 32 */

 t1 = dl << 1    */

 t1 = t1 << t2   */

 dl = dl >>a src */

 dh = dh >> src  */

 dl = dl | t1    */

 +20: Done */

 ALU mul immediate (64x32-bit) */

 dst = dst * 1 is a no-op */

 dst = dst * -1 */

 Full 64x32 multiply */

 hi(dst) = hi(dst) * src(imm) */

 hi(dst) = hi(dst) - lo(dst) */

 tmp = lo(dst) * src(imm) >> 32 */

 lo(dst) = lo(dst) * src(imm) */

 hi(dst) += tmp */

 ALU mul register (64x64-bit) */

 acc = hi(dst) * lo(src) */

 tmp = lo(dst) * hi(src) */

 acc += tmp */

 tmp = lo(dst) * lo(src) >> 32 */

 lo(dst) = lo(dst) * lo(src) */

 hi(dst) = acc + tmp */

 Helper function for 64-bit modulo */

 ALU div/mod register (64-bit) */

 Mapped to v0-v1 */

 Mapped to a0-a1 */

 Mapped to a2-a3 */

 Push caller-saved registers on stack */

 Put 64-bit arguments 1 and 2 in registers a0-a3 */

 Emit function call */

 dst = dst / src */

 dst = dst % src */

 Delay slot */

 Store the 64-bit result in dst */

 Restore caller-saved registers, excluding the computed result */

 Swap bytes in a register word */

 tmp = src & 0x00ff00ff */

 tmp = tmp << 8         */

 dst = src >> 8         */

 dst = dst & 0x00ff00ff */

 dst = dst | tmp        */

 Swap half words in a register word */

 tmp = src << 16 */

 dst = src >> 16 */

 dst = dst | tmp */

 Swap bytes and truncate a register double word, word or half word */

 Swap bytes in a double word */

 tmp = 0x00ff0000 */

 tmp = 0x00ff00ff */

 Swap bytes in a word */

 Swap bytes in a half word */

 Truncate a register double word, word or half word */

 Zero-extend a word */

 Zero-extend a half word */

 Load operation: dst = *(size*)(src + off) */

 Load a byte */

 Load a half word */

 Load a word */

 Load a double word */

 Store operation: *(size *)(dst + off) = src */

 Store a byte */

 Store a half word */

 Store a word */

 Store a double word */

 Atomic read-modify-write (32-bit, non-ll/sc fallback) */

 Push caller-saved registers on stack */

	/*

	 * Argument 1: dst+off if xchg, otherwise src, passed in register a0

	 * Argument 2: src if xchg, othersize dst+off, passed in register a1

 Emit function call */

 Delay slot */

 Update src register with old value, if specified */

 Restore caller-saved registers, except any fetched value */

 Helper function for 64-bit atomic exchange */

 Atomic read-modify-write (64-bit) */

 Mapped to v0-v1 */

 Mapped to a0-a1 */

 Push caller-saved registers on stack */

	/*

	 * Argument 1: 64-bit src, passed in registers a0-a1

	 * Argument 2: 32-bit dst+off, passed in register a2

 Emit function call */

 Delay slot */

 Update src register with old value, if specified */

 Restore caller-saved registers, except any fetched value */

 Atomic compare-and-exchange (32-bit, non-ll/sc fallback) */

 Push caller-saved registers on stack */

	/*

	 * Argument 1: 32-bit dst+off, passed in register a0

	 * Argument 2: 32-bit r0, passed in register a1

	 * Argument 3: 32-bit src, passed in register a2

 Emit function call */

 Delay slot */

 Restore caller-saved registers, except the return value */

 Atomic compare-and-exchange (64-bit) */

 Push caller-saved registers on stack */

	/*

	 * Argument 1: 32-bit dst+off, passed in register a0 (a1 unused)

	 * Argument 2: 64-bit r0, passed in registers a2-a3

	 * Argument 3: 64-bit src, passed on stack

 Emit function call */

 Delay slot */

 Restore caller-saved registers, except the return value */

/*

 * Conditional movz or an emulated equivalent.

 * Note that the rs register may be modified.

 rd = rt ? rd : rs  */

 rs = 0 if rt == 0  */

 rd = 0 if rt != 0  */

 rd = rd | rs       */

 PC += 8 if rd != 0 */

 +0: delay slot     */

 +4: rd = rs        */

/*

 * Conditional movn or an emulated equivalent.

 * Note that the rs register may be modified.

 rd = rt ? rs : rd  */

 rs = 0 if rt == 0  */

 rd = 0 if rt != 0  */

 rd = rd | rs       */

 PC += 8 if rd == 0 */

 +0: delay slot     */

 +4: rd = rs        */

 Emulation of 64-bit sltiu rd, rs, imm, where imm may be S32_MAX + 1 */

 rd = imm        */

 rd = rsl < rd   */

 tmp = rsh < ~0U */

 rd = rd | tmp   */

 imm >= 0 */

 rd = imm       */

 rd = rsl < rd  */

 rd = rsl < imm */

 rd = 0 if rsh  */

 Emulation of 64-bit sltu rd, rs, rt */

 rd = rsl < rtl     */

 tmp = rsh - rth    */

 rd = 0 if tmp != 0 */

 tmp = rsh < rth    */

 rd = rd | tmp      */

 Emulation of 64-bit slti rd, rs, imm, where imm may be S32_MAX + 1 */

	/*

	 * if ((rs < 0) ^ (imm < 0)) t1 = imm >u rsl

	 * else                      t1 = rsl <u imm

 t1 = rsl <u imm   */

 t2 = imm <u rsl   */

 rd = rsh >> 31    */

 t1 = rd ? t1 : t2 */

 t1 = rd ? t2 : t1 */

	/*

	 * if ((imm < 0 && rsh != 0xffffffff) ||

	 *     (imm >= 0 && rsh != 0))

	 *      t1 = 0

 rd = rsh + 1 */

 imm >= 0 */

 t1 = 0 if cmp != 0 */

	/*

	 * if (imm < 0) rd = rsh < -1

	 * else         rd = rsh != 0

	 * rd = rd | t1

 rd = rsh < hi(imm) */

 rd = rd | t1       */

 Emulation of 64-bit(slt rd, rs, rt) */

	/*

	 * if ((rs < 0) ^ (rt < 0)) t1 = rtl <u rsl

	 * else                     t1 = rsl <u rtl

	 * if (rsh == rth)          t1 = 0

 t1 = rsl <u rtl   */

 t2 = rtl <u rsl   */

 t3 = rlh ^ rth    */

 rd = t3 >> 31     */

 t1 = rd ? t2 : t1 */

 t1 = 0 if t3 != 0 */

 rd = (rsh < rth) | t1 */

 rd = rsh <s rth   */

 rd = rd | t1      */

 Jump immediate (64-bit) */

 No-op, used internally for branch optimization */

 PC += off if dst == imm */

 PC += off if dst != imm */

 Register fallback */

 Compare sign extension */

 Compare zero extension */

 BPF_JNE */

 PC += off if dst & imm */

 PC += off if (dst & imm) == 0 (not in BPF, used for long jumps) */

 Register fallback */

 Sign-extension pulls in high word */

 JIT_JNSET */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 Jump register (64-bit) */

 No-op, used internally for branch optimization */

 PC += off if dst == src */

 PC += off if dst != src */

 BPF_JNE */

 PC += off if dst & src */

 PC += off if (dst & imm) == 0 (not in BPF, used for long jumps) */

 JIT_JNSET */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 Function call */

 Decode the call address */

 Push stack arguments */

 Emit function call */

 Delay slot */

 Function tail call */

	/*

	 * Tail call:

	 * eBPF R1   - function argument (context ptr), passed in a0-a1

	 * eBPF R2   - ptr to object with array of function entry points

	 * eBPF R3   - array index of function to be called

	 * stack[sz] - remaining tail call count, initialized in prologue

 if (ind >= ary->map.max_entries) goto out */

 t1 = ary->map.max_entries*/

 Load delay slot          */

 t1 = ind < t1            */

 PC += off(1) if t1 == 0  */

 (next insn delay slot)   */

 if (TCC-- <= 0) goto out */

 t2 = *(SP + size) */

 Load delay slot         */

 PC += off(1) if t2 <= 0 */

 t2-- (delay slot)       */

 *(SP + size) = t2 */

 prog = ary->ptrs[ind] */

 t1 = ind << 2           */

 t1 += ary               */

 t2 = *(t1 + off)        */

 Load delay slot         */

 if (prog == 0) goto out */

 PC += off(1) if t2 == 0 */

 Delay slot              */

 func = prog->bpf_func + 8 (prologue skip offset) */

 t1 = *(t2 + off)       */

 Load delay slot        */

 t1 += skip (8 or 12)   */

 goto func */

/*

 * Stack frame layout for a JITed program (stack grows down).

 *

 * Higher address  : Caller's stack frame       :

 *                 :----------------------------:

 *                 : 64-bit eBPF args r3-r5     :

 *                 :----------------------------:

 *                 : Reserved / tail call count :

 *                 +============================+  <--- MIPS sp before call

 *                 | Callee-saved registers,    |

 *                 | including RA and FP        |

 *                 +----------------------------+  <--- eBPF FP (MIPS zero,fp)

 *                 | Local eBPF variables       |

 *                 | allocated by program       |

 *                 +----------------------------+

 *                 | Reserved for caller-saved  |

 *                 | registers                  |

 *                 +----------------------------+

 *                 | Reserved for 64-bit eBPF   |

 *                 | args r3-r5 & args passed   |

 *                 | on stack in kernel calls   |

 * Lower address   +============================+  <--- MIPS sp

 Build program prologue to set up the stack and registers */

	/*

	 * The first two instructions initialize TCC in the reserved (for us)

	 * 16-byte area in the parent's stack frame. On a tail call, the

	 * calling function jumps into the prologue after these instructions.

	/*

	 * Register eBPF R1 contains the 32-bit context pointer argument.

	 * A 32-bit argument is always passed in MIPS register a0, regardless

	 * of CPU endianness. Initialize R1 accordingly and zero-extend.

 === Entry-point for tail calls === */

 Zero-extend the 32-bit argument */

 If the eBPF frame pointer was accessed it must be saved */

 Compute the stack space needed for callee-saved registers */

 Stack space used by eBPF program local data */

	/*

	 * If we are emitting function calls, reserve extra stack space for

	 * caller-saved registers and function arguments passed on the stack.

	 * The required space is computed automatically during resource

	 * usage discovery (pass 1).

 Allocate the stack frame */

 Store callee-saved registers on stack */

 Initialize the eBPF frame pointer if accessed */

 Build the program epilogue to restore the stack and registers */

 Restore callee-saved registers from stack */

	/*

	 * A 32-bit return value is always passed in MIPS register v0,

	 * but on big-endian targets the low part of R0 is mapped to v1.

 Jump to the return address and adjust the stack pointer */

 Build one eBPF instruction */

 ALU operations */

 dst = imm */

 dst = src */

 Special mov32 for zext */

 dst = -dst */

 dst = dst & imm */

 dst = dst | imm */

 dst = dst ^ imm */

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst >> imm (arithmetic) */

 dst = dst + imm */

 dst = dst - imm */

 dst = dst * imm */

 dst = dst / imm */

 dst = dst % imm */

 dst = dst & src */

 dst = dst | src */

 dst = dst ^ src */

 dst = dst << src */

 dst = dst >> src */

 dst = dst >> src (arithmetic) */

 dst = dst + src */

 dst = dst - src */

 dst = dst * src */

 dst = dst / src */

 dst = dst % src */

 dst = imm (64-bit) */

 dst = src (64-bit) */

 dst = -dst (64-bit) */

 dst = dst & imm (64-bit) */

 dst = dst | imm (64-bit) */

 dst = dst ^ imm (64-bit) */

 dst = dst + imm (64-bit) */

 dst = dst - imm (64-bit) */

 dst = dst << imm (64-bit) */

 dst = dst >> imm (64-bit) */

 dst = dst >> imm (64-bit, arithmetic) */

 dst = dst * imm (64-bit) */

 dst = dst / imm (64-bit) */

 dst = dst % imm (64-bit) */

		/*

		 * Sign-extend the immediate value into a temporary register,

		 * and then do the operation on this register.

 dst = dst & src (64-bit) */

 dst = dst | src (64-bit) */

 dst = dst ^ src (64-bit) */

 dst = dst + src (64-bit) */

 dst = dst - src (64-bit) */

 dst = dst << src (64-bit) */

 dst = dst >> src (64-bit) */

 dst = dst >> src (64-bit, arithmetic) */

 dst = dst * src (64-bit) */

 dst = dst / src (64-bit) */

 dst = dst % src (64-bit) */

 dst = htole(dst) */

 dst = htobe(dst) */

 dst = imm64 */

 LDX: dst = *(size *)(src + off) */

 ST: *(size *)(dst + off) = imm */

 Sign-extend immediate value into temporary reg */

 STX: *(size *)(dst + off) = src */

 Speculation barrier */

 Atomics */

 Non-ll/sc fallback */

 Non-ll/sc fallback */

 Result zero-extension inserted by verifier */

 Atomics (64-bit) */

 PC += off if dst == src */

 PC += off if dst != src */

 PC += off if dst & src */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 PC += off if dst == imm */

 PC += off if dst != imm */

 PC += off if dst & imm */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 Move large immediate to register */

 PC += off if dst == src */

 PC += off if dst != src */

 PC += off if dst & src */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 PC += off if dst == imm */

 PC += off if dst != imm */

 PC += off if dst & imm */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 PC += off */

 Tail call */

 Function call */

 Function return */

		/*

		 * Optimization: when last instruction is EXIT

		 * simply continue to epilogue.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Just-In-Time compiler for eBPF bytecode on MIPS.

 * Implementation of JIT functions for 64-bit CPUs.

 *

 * Copyright (c) 2021 Anyfi Networks AB.

 * Author: Johan Almbladh <johan.almbladh@gmail.com>

 *

 * Based on code and ideas from

 * Copyright (c) 2017 Cavium, Inc.

 * Copyright (c) 2017 Shubham Bansal <illusionist.neo@gmail.com>

 * Copyright (c) 2011 Mircea Gherzan <mgherzan@gmail.com>

 MIPS t0-t3 are not available in the n64 ABI */

 Stack is 16-byte aligned in n64 ABI */

 Extra 64-bit eBPF registers used by JIT */

 Number of prologue bytes to skip when doing a tail call */

 Callee-saved CPU registers that the JIT must preserve */

 Caller-saved CPU registers available for JIT use */

/*

 * Mapping of 64-bit eBPF registers to 64-bit native MIPS registers.

 * MIPS registers t4 - t7 may be used by the JIT as temporary registers.

 * MIPS registers t8 - t9 are reserved for single-register common functions.

 Return value from in-kernel function, and exit value from eBPF */

 Arguments from eBPF program to in-kernel function */

 Callee-saved registers that in-kernel function will preserve */

 Read-only frame pointer to access the eBPF stack */

 Temporary register for blinding constants */

 Tail call count register, caller-saved */

 Constant for register zero-extension */

/*

 * MIPS 32-bit operations on 64-bit registers generate a sign-extended

 * result. However, the eBPF ISA mandates zero-extension, so we rely on the

 * verifier to add that for us (emit_zext_ver). In addition, ALU arithmetic

 * operations, right shift and byte swap require properly sign-extended

 * operands or the result is unpredictable. We emit explicit sign-extensions

 * in those cases.

 Sign extension */

 Zero extension */

 We need the ZX register */

 Zero extension, if verifier does not do it for us  */

 dst = imm (64-bit) */

 ALU immediate operation (64-bit) */

 dst = dst | imm */

 dst = dst ^ imm */

 dst = -dst */

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst >> imm (arithmetic) */

 dst = dst + imm */

 dst = dst - imm */

 Width-generic operations */

 ALU register operation (64-bit) */

 dst = dst << src */

 dst = dst >> src */

 dst = dst >> src (arithmetic) */

 dst = dst + src */

 dst = dst - src */

 dst = dst * src */

 dst = dst / src */

 dst = dst % src */

 Width-generic operations */

 Swap sub words in a register double word */

 tmp = dst & mask  */

 tmp = tmp << bits */

 dst = dst >> bits */

 dst = dst & mask  */

 dst = dst | tmp   */

 Swap bytes and truncate a register double word, word or half word */

 Swap bytes in a double word */

 t2 = dst << 32    */

 dst = dst >> 32   */

 dst = dst | t2    */

 t1 = t2 << 32     */

 t1 = t1 | t2      */

 dst = swap16(dst) */

 t2 = 0x00ff0000   */

 t2 = t2 | 0x00ff  */

 t1 = t2 << 32     */

 t1 = t1 | t2      */

 dst = swap8(dst)  */

 Swap bytes in a half word */

 Swap bytes in a word */

 Truncate a register double word, word or half word */

 Zero-extend a word */

 Zero-extend a half word */

 Load operation: dst = *(size*)(src + off) */

 Load a byte */

 Load a half word */

 Load a word */

 Load a double word */

 Store operation: *(size *)(dst + off) = src */

 Store a byte */

 Store a half word */

 Store a word */

 Store a double word */

 Atomic read-modify-write */

 Delay slot */

 Atomic compare-and-exchange */

 Delay slot */

 Delay slot */

 Function call */

 Decode the call address */

 Push caller-saved registers on stack */

 Emit function call */

 Delay slot */

 Restore caller-saved registers */

 Re-initialize the JIT zero-extension register if accessed */

 Function tail call */

	/*

	 * Tail call:

	 * eBPF R1 - function argument (context ptr), passed in a0-a1

	 * eBPF R2 - ptr to object with array of function entry points

	 * eBPF R3 - array index of function to be called

 if (ind >= ary->map.max_entries) goto out */

 tmp = ary->map.max_entrs*/

 tmp = ind < t1          */

 PC += off(1) if tmp == 0*/

 if (--TCC < 0) goto out */

 tcc-- (delay slot)      */

 PC += off(1) if tcc < 0 */

 (next insn delay slot)  */

 prog = ary->ptrs[ind] */

 tmp = ind << 3          */

 tmp += ary              */

 tmp = *(tmp + off)      */

 if (prog == 0) goto out */

 PC += off(1) if tmp == 0*/

 Delay slot              */

 func = prog->bpf_func + 8 (prologue skip offset) */

 tmp = *(tmp + off)   */

 tmp += skip (4)      */

 goto func */

/*

 * Stack frame layout for a JITed program (stack grows down).

 *

 * Higher address  : Previous stack frame      :

 *                 +===========================+  <--- MIPS sp before call

 *                 | Callee-saved registers,   |

 *                 | including RA and FP       |

 *                 +---------------------------+  <--- eBPF FP (MIPS fp)

 *                 | Local eBPF variables      |

 *                 | allocated by program      |

 *                 +---------------------------+

 *                 | Reserved for caller-saved |

 *                 | registers                 |

 * Lower address   +===========================+  <--- MIPS sp

 Build program prologue to set up the stack and registers */

	/*

	 * The first instruction initializes the tail call count register.

	 * On a tail call, the calling function jumps into the prologue

	 * after this instruction.

 === Entry-point for tail calls === */

	/*

	 * If the eBPF frame pointer and tail call count registers were

	 * accessed they must be preserved. Mark them as clobbered here

	 * to save and restore them on the stack as needed.

 Compute the stack space needed for callee-saved registers */

 Stack space used by eBPF program local data */

	/*

	 * If we are emitting function calls, reserve extra stack space for

	 * caller-saved registers needed by the JIT. The required space is

	 * computed automatically during resource usage discovery (pass 1).

 Allocate the stack frame */

 Store callee-saved registers on stack */

 Initialize the eBPF frame pointer if accessed */

 Initialize the ePF JIT zero-extension register if accessed */

 Build the program epilogue to restore the stack and registers */

 Restore callee-saved registers from stack */

 Release the stack frame */

 Jump to return address and sign-extend the 32-bit return value */

 Delay slot */

 Build one eBPF instruction */

 ALU operations */

 dst = imm */

 dst = src */

 Special mov32 for zext */

 dst = -dst */

 dst = dst & imm */

 dst = dst | imm */

 dst = dst ^ imm */

 dst = dst << imm */

 dst = dst >> imm */

 dst = dst >> imm (arithmetic) */

 dst = dst + imm */

 dst = dst - imm */

 dst = dst * imm */

 dst = dst / imm */

 dst = dst % imm */

 dst = dst & src */

 dst = dst | src */

 dst = dst ^ src */

 dst = dst << src */

 dst = dst >> src */

 dst = dst >> src (arithmetic) */

 dst = dst + src */

 dst = dst - src */

 dst = dst * src */

 dst = dst / src */

 dst = dst % src */

 dst = imm (64-bit) */

 dst = src (64-bit) */

 dst = -dst (64-bit) */

 dst = dst & imm (64-bit) */

 dst = dst | imm (64-bit) */

 dst = dst ^ imm (64-bit) */

 dst = dst << imm (64-bit) */

 dst = dst >> imm (64-bit) */

 dst = dst >> imm ((64-bit, arithmetic) */

 dst = dst + imm (64-bit) */

 dst = dst - imm (64-bit) */

 dst = dst * imm (64-bit) */

 dst = dst / imm (64-bit) */

 dst = dst % imm (64-bit) */

 dst = dst & src (64-bit) */

 dst = dst | src (64-bit) */

 dst = dst ^ src (64-bit) */

 dst = dst << src (64-bit) */

 dst = dst >> src (64-bit) */

 dst = dst >> src (64-bit, arithmetic) */

 dst = dst + src (64-bit) */

 dst = dst - src (64-bit) */

 dst = dst * src (64-bit) */

 dst = dst / src (64-bit) */

 dst = dst % src (64-bit) */

 dst = htole(dst) */

 dst = htobe(dst) */

 dst = imm64 */

 LDX: dst = *(size *)(src + off) */

 ST: *(size *)(dst + off) = imm */

 STX: *(size *)(dst + off) = src */

 Speculation barrier */

 Atomics */

 Don't overwrite dst */

 32-bit, no fetch */

 Don't overwrite dst */

 Restore result */

 Result zext inserted by verifier */

 PC += off if dst == src */

 PC += off if dst != src */

 PC += off if dst & src */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 Sign-extended dst */

 Sign-extended src */

 PC += off if dst == imm */

 PC += off if dst != imm */

 PC += off if dst & imm */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 Sign-extended dst */

 Move large immediate to register, sign-extended */

 PC += off if dst == src */

 PC += off if dst != src */

 PC += off if dst & src */

 PC += off if dst > src */

 PC += off if dst >= src */

 PC += off if dst < src */

 PC += off if dst <= src */

 PC += off if dst > src (signed) */

 PC += off if dst >= src (signed) */

 PC += off if dst < src (signed) */

 PC += off if dst <= src (signed) */

 PC += off if dst == imm */

 PC += off if dst != imm */

 PC += off if dst & imm */

 PC += off if dst > imm */

 PC += off if dst >= imm */

 PC += off if dst < imm */

 PC += off if dst <= imm */

 PC += off if dst > imm (signed) */

 PC += off if dst >= imm (signed) */

 PC += off if dst < imm (signed) */

 PC += off if dst <= imm (signed) */

 Move large immediate to register */

 PC += off */

 Tail call */

 Function call */

 Function return */

		/*

		 * Optimization: when last instruction is EXIT

		 * simply continue to epilogue.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR71XX/AR724X/AR913X specific setup

 *

 *  Copyright (C) 2010-2011 Jaiganesh Narayanan <jnarayanan@atheros.com>

 *  Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 *

 *  Parts of this file are based on Atheros' 2.6.15/2.6.31 BSP

 for mips_hpt_frequency */

 for _machine_{restart,halt} */

 Get the position of the FDT passed by the bootloader */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR71XX/AR724X/AR913X specific prom routines

 *

 *  Copyright (C) 2015 Laurent Fasnacht <l@libres.ch>

 *  Copyright (C) 2008-2010 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 Read the initrd address from the firmware environment */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR71XX/AR724X/AR913X common routines

 *

 *  Copyright (C) 2010-2011 Jaiganesh Narayanan <jnarayanan@atheros.com>

 *  Copyright (C) 2011 Gabor Juhos <juhosg@openwrt.org>

 *

 *  Parts of this file are based on Atheros' 2.6.15/2.6.31 BSP

	/*

	 * QCA956x timer init workaround has to be applied right before setting

	 * up the clock. Else, there will be no jiffies

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR7XXX/AR9XXX SoC early printk support

 *

 *  Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 nothing to do */

 TODO */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR71XX/AR724X/AR913X common routines

 *

 *  Copyright (C) 2010-2011 Jaiganesh Narayanan <jnarayanan@atheros.com>

 *  Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 *

 *  Parts of this file are based on Atheros' 2.6.15/2.6.31 BSP

 Flush the DDR write buffer. */

 It must be run twice. */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  NEC VR4100 series SIU platform device.

 *

 *  Copyright (C) 2007-2008  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  type.c, System type for NEC VR4100 series.

 *

 *  Copyright (C) 2005	Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  NEC VR4100 series RTC platform device.

 *

 *  Copyright (C) 2007	Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  NEC VR4100 series GIU platform device.

 *

 *  Copyright (C) 2007	Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  cmu.c, Clock Mask Unit routines for the NEC VR4100 series.

 *

 *  Copyright (C) 2001-2002  MontaVista Software Inc.

 *    Author: Yoichi Yuasa <source@mvista.com>

 *  Copyright (C) 2003-2005  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Changes:

 *  MontaVista Software Inc. <source@mvista.com>

 *  - New creation, NEC VR4122 and VR4131 are supported.

 *  - Added support for NEC VR4111 and VR4121.

 *

 *  Yoichi Yuasa <yuasa@linux-mips.org>

 *  - Added support for NEC VR4133.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  pmu.c, Power Management Unit routines for NEC VR4100 series.

 *

 *  Copyright (C) 2003-2007  Yoichi Yuasa <yuasa@linux-mips.org>

		/*

		 * "standby" sets IE bit of the CP0_STATUS to 1.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  init.c, Common initialization routines for NEC VR4100 series.

 *

 *  Copyright (C) 2003-2009  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  icu.c, Interrupt Control Unit routines for the NEC VR4100 series.

 *

 *  Copyright (C) 2001-2002  MontaVista Software Inc.

 *    Author: Yoichi Yuasa <source@mvista.com>

 *  Copyright (C) 2003-2006  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Changes:

 *  MontaVista Software Inc. <source@mvista.com>

 *  - New creation, NEC VR4122 and VR4131 are supported.

 *  - Added support for NEC VR4111 and VR4121.

 *

 *  Yoichi Yuasa <yuasa@linux-mips.org>

 *  - Coped with INTASSIGN of NEC VR4133.

 Pin 0-15 */

 Pin 0-15 */

 Int0-4 -> IRQ2-6 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  Interrupt handing routines for NEC VR4100 series.

 *

 *  Copyright (C) 2005-2007  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  bcu.c, Bus Control Unit routines for the NEC VR4100 series.

 *

 *  Copyright (C) 2002	MontaVista Software Inc.

 *    Author: Yoichi Yuasa <source@mvista.com>

 *  Copyright (C) 2003-2005  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Changes:

 *  MontaVista Software Inc. <source@mvista.com>

 *  - New creation, NEC VR4122 and VR4131 are supported.

 *  - Added support for NEC VR4111 and VR4121.

 *

 *  Yoichi Yuasa <yuasa@linux-mips.org>

 *  - Added support for NEC VR4133.

 The NEC VR4111 doesn't have the VTClock. */

 DIVVT == 9 Divide by 1.5 . VTClock = (PClock * 6) / 9 */

 DIVVT == 10 Divide by 2.5 . VTClock = (PClock * 4) / 10 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  setup.c, Setup for the IBM WorkPad z50.

 *

 *  Copyright (C) 2002-2006  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  setup.c, Setup for the CASIO CASSIOPEIA E-11/15/55/65.

 *

 *  Copyright (C) 2002-2006  Yoichi Yuasa <yuasa@linux-mips.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * loongson-specific suspend support

 *

 *  Copyright (C) 2009 Lemote Inc.

 *  Author: Wu Zhangjin <wuzhangjin@gmail.com>

 i8259A */

 bonito */

 disable all mips events */

 disable all events of i8259A */

 disable all events of bonito */

 enable all mips events */

 only enable the cached events of i8259A */

 enable all cached events of bonito */

/*

 * Setup the board-specific events for waking up loongson from wait mode

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 setup mips r4k timer */

 SPDX-License-Identifier: GPL-2.0+

 SPDX-License-Identifier: GPL-2.0

	/* We extract 2bit node id (bit 44~47, only bit 44~45 used now) from

	/* We extract 2bit node id (bit 44~47, only bit 44~45 used now) from

 SPDX-License-Identifier: GPL-2.0

 UCA is already enabled. */

 See if UCAC bit can be flipped on. This should be safe. */

 Only engage the logic on Loongson processors. */

 CPUs with CPUCFG support don't need to synthesize anything. */

 Add CPUCFG features non-discoverable otherwise. */

		/* It is possible that some future Loongson cores still do

		 * not have CPUCFG, so do not emulate anything for these

		 * cores.

	/* This feature is set by firmware, but all known Loongson-64 systems

	 * are configured this way.

 Patch in dynamically probed bits. */

	/* We have usable CPUCFG now, emulated or not.

	 * Announce CPUCFG availability to userspace via hwcap.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 Otherwise come from DTB */

 Parse memory information and activate */

 init base address of io space */

 Hardcode to CPU UART 0 */

 Legacy ISA must placed at the start of PCI_IOBASE */

 SPDX-License-Identifier: GPL-2.0

 Do nothing on Loongson-3 */

 stop counter */

 enables the timer0 to generate a periodic interrupt */

 set the comparator */

 start counter */

	/*

	 * set timer0 type

	 * 1 : periodic interrupt

	 * 0 : non-periodic(oneshot) interrupt

 clear the TIMER0 irq status register */

/*

 * hpet address assignation and irq setting should be done in bios.

 * but pmon don't do this, we just setup here directly.

 * The operation under is normal. unfortunately, hpet_setup process

 * is before pci initialize.

 *

 * {

 *	struct pci_dev *pdev;

 *

 *	pdev = pci_get_device(PCI_VENDOR_ID_ATI, PCI_DEVICE_ID_ATI_SBX00_SMBUS, NULL);

 *	pci_write_config_word(pdev, SMBUS_PCI_REGB4, HPET_ADDR);

 *

 *	...

 * }

 set hpet base address */

 enable decoding of access to HPET MMIO*/

 HPET irq enable */

 mips clocksource rating is less than 300, so hpet is better. */

 oneshot mode work normal with this flag */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2010 Loongson Inc. & Lemote Inc. &

 *                    Institute of Computing Technology

 * Author:  Xiang Gao, gaoxiang@ict.ac.cn

 *          Huacai Chen, chenhc@lemote.com

 *          Xiaofu Meng, Shuangshuang Zhang

 kernel start address */

 kernel end address */

 used by finalize_initrd() */

 Reserve the kernel text/data/bss */

 Reserve 0xfe000000~0xffffffff for RS780E integrated GPU */

 Reserve pfn range 0~node[0]->node_start_pfn */

 This comes from node 0 */

 All PCI device belongs to logical Node-0 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Based on Ocelot Linux port, which is

 * Copyright 2001 MontaVista Software Inc.

 * Author: jsun@mvista.com or jsun@junsun.net

 *

 * Copyright 2003 ICT CAS

 * Author: Michael Guo <guoyi@ict.ac.cn>

 *

 * Copyright (C) 2007 Lemote Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin, wuzhangjin@gmail.com

 firmware arguments are initialized in head.S */

 One chip has 2 nodes */

 Read the ID of PCI host bridge to detect bridge type */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2014 Lemote Corporation.

 *   written by Huacai Chen <chenhc@lemote.com>

 *

 * based on arch/mips/cavium-octeon/cpu.c

 * Copyright (C) 2009 Wind River Systems,

 *   written by Ralf Baechle <ralf@linux-mips.org>

 If FPU is owned, we needn't init or restore fp */

 Don't call default notifier */

 gslq */

 gslqc1 */

 Don't call default notifier */

 gssq */

 write upper 8 bytes first */

 gssqc1 */

 Don't call default notifier */

		/*

		 * Loongson-3 overridden ldc2 instructions.

		 * opcode1              instruction

		 *   0x1          gslhx: load 2 bytes to GPR

		 *   0x2          gslwx: load 4 bytes to GPR

		 *   0x3          gsldx: load 8 bytes to GPR

		 *   0x6	  gslwxc1: load 4 bytes to FPR

		 *   0x7	  gsldxc1: load 8 bytes to FPR

 Don't call default notifier */

		/*

		 * Loongson-3 overridden sdc2 instructions.

		 * opcode1              instruction

		 *   0x1          gsshx: store 2 bytes from GPR

		 *   0x2          gsswx: store 4 bytes from GPR

		 *   0x3          gssdx: store 8 bytes from GPR

		 *   0x6          gsswxc1: store 4 bytes from FPR

		 *   0x7          gssdxc1: store 8 bytes from FPR

 Don't call default notifier */

 Let default notifier send signals */

 roll back jump/branch */

 Did we have an exception handler installed? */

 Don't call default notifier */

 Don't call default notifier */

 Don't call default notifier */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2010, 2011, 2012, Lemote, Inc.

 * Author: Chen Huacai, chenhc@lemote.com

 read a 32bit value from ipi register */

 read a 64bit value from ipi register */

 write a 32bit value to ipi register */

 write a 64bit value to ipi register */

 send mail via Mail_Send register for 3A4000+ CPU */

 send high 32 bits */

 send low 32 bits */

 Load the ipi register to figure out what we're supposed to do */

 Clear the ipi register to clear the interrupt */

 startargs[] are initial PC, SP and GP for secondary CPU */

 Load the ipi register to figure out what we're supposed to do */

 Clear the ipi register to clear the interrupt */

 startargs[] are initial PC, SP and GP for secondary CPU */

/*

 * Simple enough, just poke the appropriate ipi register

 Let others see the result ASAP */

/*

 * SMP init and finish on secondary CPUs

 Set interrupt mask, but don't enable */

 Local access is faster for loops */

 i: physical id, num: logical id */

	/* For unified kernel, NR_CPUS is the maximum possible value,

	 * loongson_sysconf.nr_cpus is the really present value

 Reserved physical CPU cores */

 Loongson processors are always grouped by 4 */

/*

 * Setup the PC, SP, and GP of a secondary processor and start it runing!

/* To shutdown a core in Loongson 3, the target core should go to CKSEG1 and

 * flush all L1 entries at first. Then, another core (usually Core 0) can

 * safely disable the clock of the target core. loongson3_play_dead() is

 * called via CKSEG1 (uncached and unmmaped)

 KSEG0 */

 flush L1 ICache */

 flush L1 DCache */

 *state_addr = CPU_DEAD; */

 flush entry of *state_addr */

 get core id */

 get node id */

 wait for init loop */

 limit mailbox access */

 get PC via mailbox */

 get SP via mailbox */

 get GP via mailbox */

 jump to initial PC */

 No Input */

 KSEG0 */

 flush L1 ICache */

 flush L1 DCache */

 *state_addr = CPU_DEAD; */

 flush entry of *state_addr */

 get core id */

 get node id */

 15:14 */

 wait for init loop */

 limit mailbox access */

 get PC via mailbox */

 get SP via mailbox */

 get GP via mailbox */

 jump to initial PC */

 No Input */

 KSEG0 */

 flush L1 ICache */

 flush L1 DCache */

 KSEG0 */

 flush L1 VCache */

 *state_addr = CPU_DEAD; */

 flush entry of *state_addr */

 get core id */

 get node id */

 wait for init loop */

 limit mailbox access */

 check lower 32-bit as jump indicator */

 get PC (whole 64-bit) via mailbox */

 get SP via mailbox */

 get GP via mailbox */

 jump to initial PC */

 No Input */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Zhangjin Wu, wuzhangjin@gmail.com

 0X80000000~0X80200000 is safe */

 argv at offset 0, argv[] at offset KEXEC_ARGV_SIZE/2 */

			/*

			 * convert command line string to array

			 * of parameters (as bootloader does).

 kexec/kdump need a safe page to save reboot_code_buffer */

 All CPUs go to reboot_code_buffer */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2013 Imagination Technologies Ltd.

 *

 * Arbitrary Monitor Interface

 Target must see parameters before go */

 Target must see go before we poll  */

 Target will be updating flags soon */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 * Setting up the clock on the MIPS boards.

/*

 * Estimate CPU and GIC frequencies.

	/*

	 * Read counters exactly on rising edge of update flag.

	 * This helps get an accurate reading under virtualisation.

 Wait for falling edge before reading RTC. */

 Read counters again exactly on rising edge of update flag. */

 Wait for falling edge before reading RTC again. */

	/*

	 * Some cores claim the FDC is routable through the GIC, but it doesn't

	 * actually seem to be connected for those Malta bitstreams.

 Set 32KHz time base if not already set */

 Ensure SET bit is clear so RTC can run */

 Only Malta has a PIT. */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000, 2001, 2004 MIPS Technologies, Inc.

 * Copyright (C) 2001 Ralf Baechle

 * Copyright (C) 2013 Imagination Technologies Ltd.

 *

 * Routines for generic manipulation of the interrupts found on the MIPS

 * Malta board. The interrupt controller is located in the South Bridge

 * a PIIX4 device with two internal 82C95 interrupt controllers.

	/*

	 * Determine highest priority pending interrupt by performing

	 * a PCI Interrupt Acknowledge cycle.

		/* The following will generate a PCI IACK cycle on the

		 * Bonito controller. It's a little bit kludgy, but it

		 * was the easiest way to implement it in hardware at

		 * the given time.

 Flush Bonito register block */

 sync */

 sync */

	/* Read all the registers and then print them as there is a

	   problem with interspersed printk's upsetting the Bonito controller.

	   Do it for the others too.

	/*

	 * Preallocate the i8259's expected virq's here. Since irqchip_init()

	 * will probe the irqchips in hierarchial order, i8259 is probed last.

	 * If anything allocates a virq before the i8259 is probed, it will

	 * be given one of the i8259's expected range and consequently setup

	 * of the i8259 will fail.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2015 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 determined physical memory size, not overridden by command line args	 */

		/*

		 * The current Malta EVA configuration is "special" in that it

		 * always makes use of addresses in the upper half of the 32 bit

		 * physical address map, which gives it a contiguous region of

		 * DDR but limits it to 2GB.

		/*

		 * We have a flat 32 bit physical memory map with DDR filling

		 * all 4GB of the memory map, apart from the I/O region which

		 * obscures 256MB from 0x10000000-0x1fffffff.

		 *

		 * Therefore we discard the 256MB behind the I/O region.

 Make use of the memory following the I/O region */

		/*

		 * We have a 32 bit physical memory map with a 2GB DDR region

		 * aliased in the upper & lower halves of it. The I/O region

		 * obscures 256MB from 0x10000000-0x1fffffff in the low alias

		 * but the DDR it obscures is accessible via the high alias.

		 *

		 * Simply access everything beyond the lowest 256MB of DDR using

		 * the high alias.

 if a memory node already exists, leave it alone */

 find memory size from the bootloader environment */

		/*

		 * SOC-it swaps, or perhaps doesn't swap, when DMA'ing

		 * the last word of physical memory.

 default to using all available RAM */

 allow the user to override the usable memory */

 if the user says there's more RAM than we thought, believe them */

 detect the memory map in use */

 ROCit has a register indicating the memory map in use */

 if not using ROCit, presume the v1 memory map */

 append memory to the DT */

 if we have a CM which reports a GIC is present, leave the DT alone */

		/*

		 * On systems using the RocIT system controller a GIC may be

		 * present without a CM. Detect whether that is the case.

 enable the GIC at the system controller level */

 if this isn't Malta, leave the DT alone */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2006, 07 MIPS Technologies, Inc.

 *   written by Ralf Baechle (ralf@linux-mips.org)

 *     written by Ralf Baechle <ralf@linux-mips.org>

 *

 * Copyright (C) 2008 Wind River Systems, Inc.

 *   updated by Tiejun Chen <tiejun.chen@windriver.com>

 *

 * 1. Probe driver for the Malta's UART ports:

 *

 *   o 2 ports in the SMC SuperIO

 *   o 1 port in the CBUS UART, a discrete 16550 which normally is only used

 *     for bringups.

 *

 * We don't use 8250_platform.c on Malta as it would result in the CBUS

 * UART becoming ttyS0.

 *

 * 2. Register RTC-CMOS platform device on Malta.

 The CBUS UART */

 Twice the usual clk! */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * PROM library functions for acquiring/using memory descriptors given to

 * us from the YAMON.

 *

 * Copyright (C) 1999,2000,2012  MIPS Technologies, Inc.

 * All rights reserved.

 * Authors: Carsten Langgaard <carstenl@mips.com>

 *          Steven J. Hill <sjhill@mips.com>

 determined physical memory size, not overridden by command line args	 */

 This address is "typically unused" */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.

 * Copyright (C) 2008 Dmitri Vorobiev

 for dma_default_coherent */

	/*

	 * Activate Floppy Controller in the SMSC FDC37M817 Super I/O

	 * Controller.

	 * Done by YAMON 2.00 onwards

 Entering config state. */

 Activate floppy controller. */

 Exit config state. */

 Nothing special needs to be done to enable coherency */

	/*

	 * If user passed a pci_clock= option, don't tack on another one

 EVA has already been configured in mach-malta/kernel-init.h */

 Request I/O space for devices used on the Malta board. */

	/*

	 * Enable DMA channel 4 (cascade channel) in the PIIX4 south bridge.

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * PROM library initialisation code.

 *

 * Copyright (C) 1999,2000,2004,2005,2012  MIPS Technologies, Inc.

 * All rights reserved.

 * Authors: Carsten Langgaard <carstenl@mips.com>

 *         Maciej W. Rozycki <macro@mips.com>

 *          Steven J. Hill <sjhill@mips.com>

 Bonito64 system controller register base. */

 GT64120 system controller register base */

 MIPS System controller register base */

	/*

	 * early setup of _pcictrl_bonito so that we can determine

	 * the system controller on a CORE_EMUL board

			/*

			 * SOCit/ROCit support is essentially identical

			 * but make an attempt to distinguish them

 See above */

		/*

		 * Setup the North bridge to do Master byte-lane swapping

		 * when running in bigendian.

 Fix up PCI I/O mapping if necessary (for Atlas).  */

		/*

		 * Disable Bonito IOBC.

		/*

		 * Setup the North bridge to do Master byte-lane swapping

		 * when running in bigendian.

 Fix up lane swapping.  */

		/*

		 * Setup the Malta max (2GB) memory for PCI DMA in host bridge

		 * in transparent addressing mode.

 Don't handle target retries indefinitely.  */

 Unknown system controller */

 We die here... */

 Early detection of CMP support */

/*

 * Copyright (c) 1995

 *	Ted Lemon (hereinafter referred to as the author)

 *

 * Redistribution and use in source and binary forms, with or without

 * modification, are permitted provided that the following conditions

 * are met:

 * 1. Redistributions of source code must retain the above copyright

 *    notice, this list of conditions and the following disclaimer.

 * 2. Redistributions in binary form must reproduce the above copyright

 *    notice, this list of conditions and the following disclaimer in the

 *    documentation and/or other materials provided with the distribution.

 * 3. The name of the author may not be used to endorse or promote products

 *    derived from this software without specific prior written permission.

 *

 * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND

 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE

 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE

 * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE

 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL

 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS

 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)

 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT

 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY

 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF

 * SUCH DAMAGE.

/* elf2ecoff.c



   This program converts an elf executable to an ECOFF executable.

   No symbol table is retained.	  This is useful primarily in building

   net-bootable kernels for machines (e.g., DECstation and Alpha) which

/*

 * Some extra ELF definitions

 Register usage information */

 Records ABI related flags  */

 -------------------------------------------------------------------- */

 Go to the start of the ELF symbol table... */

/*

 * Combine two segments, which must be contiguous.   If pad is true, it's

 * okay for there to be padding between.

 Check args... */

 Try the input file... */

 Read the header, which is at the beginning of the file... */

 Read the program headers... */

 Read the section headers... */

	/* Figure out if we can cram the program header into an ECOFF

	   header...  Basically, we can't handle anything but loadable

	   segments, but we can ignore some kinds of segments.	We can't

	   handle holes in the address space.  Segments may be out of order,

 Section types we can ignore... */

 Writable (data) segment? */

 Remember the lowest segment start address. */

 Section types we can't handle... */

 Sections must be in order to be converted... */

	/* If there's a data section but no text section, then the loader

	   combined everything into one section.   That needs to be the

	   text section, so just make the data section zero length following

	/* If there is a gap between text and data, we'll fill it when we copy

	   the data, so update the length of the text segment as represented in

	   a.out to reflect that, since a.out doesn't allow gaps in the program

 We now have enough information to cons up an a.out header... */

 unused. */

 bogus */

 Stripped, not sharable. */

 Make the output file... */

 Write the headers... */

	/*

	 * Copy the loadable sections.	 Zero-fill any gaps less than 64k;

	 * complain about any zero-filling, and die if we're asked to zero-fill

	 * more than 64k.

		/* Unprocessable sections were handled above, so just verify that

	/*

	 * Write a page of padding for boot PROMS that read entire pages.

	 * Without this, they may attempt to read past the end of the

	 * data section, incur an error, and refuse to boot.

 Looks like we won... */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 This is included from relocs_32/64.c */

 Symbols matching these regex's should never be relocated */

 Convert the fields to native endian */

 Set offset into kernel image */

			/* Convert MIPS64 RELA format - only the symbol

			 * index needs converting to native endianness

		/* Set relocation section size to 0, effectively removing it.

		 * This is necessary due to lack of support for relocations

		 * in objcopy when creating 32bit elf from 64bit elf.

	/* Relocation representation in binary table:

	 * |76543210|76543210|76543210|76543210|

	 * |  Type  |  offset from _text >> 2  |

 Walk through the relocations */

 Don't relocate weak symbols without a target */

		/*

		 * NONE can be ignored and PC relative relocations don't

		 * need to be adjusted.

		/* We support relocating within the same 4Gb segment only,

		 * thus leaving the top 32bits unchanged

		/* We support relocating by 64k jumps only

		 * thus leaving the bottom 16bits unchanged

 Collect up the relocations */

 Print the relocations */

		/* Print the relocations in a form suitable that

		 * gas will like.

 Output text to stdout */

 Output raw binary to stdout */

		/* Seek to offset of the relocation section.

		* Each relocation is then written into the

		* vmlinux kernel image.

 Print a stop, but only if we've actually written some relocs */

		/* Die, but suggest a value for CONFIG_RELOCATION_TABLE_SIZE

		 * which will fix this problem and allow a bit of headroom

		 * if more kernel features are enabled

/*

 * As an aid to debugging problems with different linkers

 * print summary information about the relocs.

 * Since different linkers tend to emit the sections in

 * different orders we use the section names in the output.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 Symbol index.  */

 Special symbol.  */

 Third relocation.  */

 Second relocation.  */

 First relocation.  */

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

/*

 * 16550 compatible uart based serial debug support for zboot

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: Matt Porter <mporter@mvista.com>

 *

 * Copyright (C) 2009 Lemote, Inc.

 * Author: Wu Zhangjin <wuzhangjin@gmail.com>

/*

 * These two variables specify the free mem region

 * that can be used for temporary malloc area

 The linker tells us where the image is. */

 debug interfaces  */

 Halt */

 activate the code for pre-boot environment */

 This area are prepared for mallocing when decompressing */

 Display standard Linux/MIPS boot prompt */

 Decompress the kernel with according algorithm */

 last four bytes is always image size in little endian */

 The device tree's address must be properly aligned  */

 copy dtb to where the booted kernel will expect it */

 FIXME: should we flush cache here? */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/mips/boot/compressed/string.c

 *

 * Very small subset of simple string routines

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2010 "Wu Zhangjin" <wuzhangjin@gmail.com>

 Convert hex characters to dec number */

	/*

	 * Align with 64KB: KEXEC needs load sections to be aligned to PAGE_SIZE,

	 * which may be as large as 64KB depending on the kernel configuration.

 SPDX-License-Identifier: GPL-2.0

/*

 * MIPS-specific debug support for pre-boot environment

 *

 * NOTE: putc() is board specific, if your board have a 16550 compatible uart,

 * please select SYS_SUPPORTS_ZBOOT_UART16550 for your machine. othewise, you

 * need to implement your own putc().

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0-only

 SPDX-License-Identifier: GPL-2.0

 Avoid TLB mismatch during and after kernel resume */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Suspend support specific for mips.

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Hu Hongbing <huhb@lemote.com>

 *	   Wu Zhangjin <wuzhangjin@gmail.com>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * BRIEF MODULE DESCRIPTION

 *	MyCable XXS1500 board support

 *

 * Copyright 2003, 2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 Jump to the reset vector */

 Set multiple use pins (UART3/GPIO) to UART (it's used as UART too) */

 Enable UART */

 Enable DTR (MCR bit 0) = USB power up */

*****************************************************************************/

 CF irq */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * MTX-1 platform devices registration (Au1500)

 *

 * Copyright (C) 2007-2009, Florian Fainelli <florian@openwrt.org>

 Jump to the reset vector */

 Enable USB power switch */

 IS_ENABLED(CONFIG_USB_OHCI_HCD) */

 Initialize sys_pinfunc */

 Initialize GPIO */

 Disable M66EN (PCI 66MHz) */

 Disable PCI CLKRUN# */

 Enable EXT_IO3 */

 Disable eth PHY TX_ER */

 Enable LED and set it to green */

 green on */

 red off */

*****************************************************************************/

 Global number 215 is offset 15 on Alchemy GPIO 2 */

	/* This function is only necessary to support a proprietary Cardbus

	 * adapter on the mtx-1 "singleboard" variant. It triggers a custom

	 * logic chip connected to EXT_IO3 (GPIO1) to suppress IDSEL signals.

 Suppress signal to Cardbus */

 set EXT_IO3 OFF */

 set EXT_IO3 ON */

 IDSEL 00 - AdapterA-Slot0 (top) */

 IDSEL 01 - AdapterA-Slot1 (bottom) */

 IDSEL 02 - AdapterB-Slot0 (top) */

 IDSEL 03 - AdapterB-Slot1 (bottom) */

 IDSEL 04 - AdapterC-Slot0 (top) */

 IDSEL 05 - AdapterC-Slot1 (bottom) */

 IDSEL 06 - AdapterD-Slot0 (top) */

 IDSEL 07 - AdapterD-Slot1 (bottom) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * GPR board platform device registration (Au1550)

 *

 * Copyright (C) 2010 Wolfgang Grandegger <wg@denx.de>

 switch System-LED to orange (red# and green# on) */

 trigger watchdog to reset board in 200ms */

 Enable UART1/3 */

 Take away Reset of UMTS-card */

/*

 * Watchdog

/*

 * FLASH

 *

 * 0x00000000-0x00200000 : "kernel"

 * 0x00200000-0x00a00000 : "rootfs"

 * 0x01d00000-0x01f00000 : "config"

 * 0x01c00000-0x01d00000 : "yamon"

 * 0x01d00000-0x01d40000 : "yamon env vars"

 * 0x00000000-0x00a00000 : "kernel+rootfs"

/*

 * LEDs

 green */

 red */

/*

 * I2C

		/*

		 * This should be on "GPIO2" which has base at 200 so

		 * the global numbers 209 and 210 should correspond to

		 * local offsets 9 and 10.

	/*

	 * The open drain mode is hardwired somewhere or an electrical

	 * property of the alchemy GPIO controller.

 ~100 kHz */

 must be arch_initcall; MIPS PCI scans busses in a subsys_initcall */

/*

 * Copyright 2000, 2007-2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com

 *

 * Updates to 2.6, Pete Popov, Embedded Alley Solutions, Inc.

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 for dma_default_coherent */

 Au1200 AB USB does not support coherent memory */

 Various early Au1xx0 errata corrected by this */

 Set Config[OD] */

 Clear to obtain best system bus performance */

 Clear Config[OD] */

 board specific setup */

 IO/MEM resources. */

 This routine should be valid for all Au1x based boards */

 Don't fixup 36-bit addresses */

 Check for PCI memory window */

 default nop */

 CONFIG_MIPS_FIXUP_BIGPHYS_ADDR */

/*

 *

 * BRIEF MODULE DESCRIPTION

 *    PROM library initialisation code, supports YAMON and U-Boot.

 *

 * Copyright 2000-2001, 2006, 2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 *

 * This file was derived from Carsten Langgaard's

 * arch/mips/mips-boards/xx files.

 *

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

	/*

	 * Return a pointer to the given environment variable.

	 * YAMON uses "name", "value" pairs, while U-Boot uses "name=value".

 minimum memsize is 64MB RAM */

 foo */

 Check the environment variables first */

 Check command line */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008-2009 Manuel Lauss <manuel.lauss@gmail.com>

 *

 * Previous incarnations were:

 * Copyright (C) 2001, 2006, 2008 MontaVista Software, <source@mvista.com>

 * Copied and modified Carsten Langgaard's time.c

 *

 * Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 1999,2000 MIPS Technologies, Inc.  All rights reserved.

 *

 * ########################################################################

 *

 * ########################################################################

 *

 * Clocksource/event using the 32.768kHz-clocked Counter1 ('RTC' in the

 * databooks).  Firmware/Board init code must enable the counters in the

 * counter control register, otherwise the CP0 counter clocksource/event

 * will be installed instead (and use of 'wait' instruction is prohibited).

 32kHz clock enabled and detected */

 wait for register access */

	/* Check if firmware (YAMON, ...) has enabled 32kHz and clock

	 * has been detected.  If so install the rtcmatch2 clocksource,

	 * otherwise don't bother.  Note that both bits being set is by

	 * no means a definite guarantee that the counters actually work

	 * (the 32S bit seems to be stuck set to 1 once a single clock-

	 * edge is detected, hence the timeouts).

	/*

	 * setup counter 1 (RTC) to tick at full speed

 32.768 kHz */

 register counter1 clocksource and event device */

 ~0.28ms */

 wait doesn't work with r4k timer */

 SPDX-License-Identifier: GPL-2.0

/*

 * Alchemy clocks.

 *

 * Exposes all configurable internal clock sources to the clk framework.

 *

 * We have:

 *  - Root source, usually 12MHz supplied by an external crystal

 *  - 3 PLLs which generate multiples of root rate [AUX, CPU, AUX2]

 *

 * Dividers:

 *  - 6 clock dividers with:

 *   * selectable source [one of the PLLs],

 *   * output divided between [2 .. 512 in steps of 2] (!Au1300)

 *     or [1 .. 256 in steps of 1] (Au1300),

 *   * can be enabled individually.

 *

 * - up to 6 "internal" (fixed) consumers which:

 *   * take either AUXPLL or one of the above 6 dividers as input,

 *   * divide this input by 1, 2, or 4 (and 3 on Au1300).

 *   * can be disabled separately.

 *

 * Misc clocks:

 * - sysbus clock: CPU core clock (CPUPLL) divided by 2, 3 or 4.

 *    depends on board design and should be set by bootloader, read-only.

 * - peripheral clock: half the rate of sysbus clock, source for a lot

 *    of peripheral blocks, read-only.

 * - memory clock: clk rate to main memory chips, depends on board

 *    design and is read-only,

 * - lrclk: the static bus clock signal for synchronous operation.

 *    depends on board design, must be set by bootloader,

 *    but may be required to correctly configure devices attached to

 *    the static bus. The Au1000/1500/1100 manuals call it LCLK, on

 *    later models it's called RCLK.

/* Base clock: 12MHz is the default in all databooks, and I haven't

 * found any board yet which uses a different rate.

/*

 * the internal sources which can be driven by the PLLs and dividers.

 * Names taken from the databooks, refer to them for more information,

 * especially which ones are share a clock line.

/* aliases for a few on-chip sources which are either shared

 * or have gone through name changes.

 access locks to SYS_FREQCTRL0/1 and SYS_CLKSRC registers */

 CPU Core clock *****************************************************/

	/*

	 * On early Au1000, sys_cpupll was write-only. Since these

	 * silicon versions of Au1000 are not sold, we don't bend

	 * over backwards trying to determine the frequency.

 AUXPLLs ************************************************************/

 au1300 has also AUXPLL2 */

 max multiplier */

 minimum is 84MHz, max is 756-1032 depending on variant */

 sysbus_clk *********************************************************/

 Peripheral Clock ***************************************************/

 Peripheral clock runs at half the rate of sysbus clk */

 mem clock **********************************************************/

 lrclk: external synchronous static bus clock ***********************/

	/* Au1000, Au1500: MEM_STCFG0[11]: If bit is set, lrclk=pclk/5,

	 * otherwise lrclk=pclk/4.

	 * All other variants: MEM_STCFG0[15:13] = divisor.

	 * L/RCLK = periph_clk / (divisor + 1)

	 * On Au1000, Au1500, Au1100 it's called LCLK,

	 * on later models it's called RCLK, but it's the same thing.

 all other models */

 Clock dividers and muxes *******************************************/

 data for fgen and csrc mux-dividers */

 register lock		  */

 SYS_FREQCTRL0/1		  */

 offset in register		  */

 parent before disable [Au1300] */

 is it enabled?		  */

 dividertable for csrc	  */

 only div-by-multiple-of-2 possible */

 stay <=prate */

 value to write to register */

	/* look at the rates each enabled parent supplies and select

	 * the one that gets closest to but not over the requested rate.

		/* if this parent is currently unused, remember it.

		 * XXX: we would actually want clk_has_active_children()

		 * but this is a good-enough approximation for now.

 what can hardware actually provide */

	/* if we couldn't get the exact rate we wanted from the enabled

	 * parents, maybe we can tell an available disabled/inactive one

	 * to give us a rate we can divide down to the requested rate.

 Au1000, Au1100, Au15x0, Au12x0 */

 enable by setting the previous parent clock */

 set input mux to "disabled" state */

 value to write to register */

/* fg0-2 and fg4-6 share a "scale"-bit. With this bit cleared, the

 * dividers behave exactly as on previous models (dividers are multiples

 * of 2); with the bit set, dividers are multiples of 1, halving their

 * range, but making them also much more flexible.

 test "scale" bit */

 test scale bit */

 Au1300 larger input mux, no separate disable bit, flexible divider */

		/* default to first parent if bootloader has set

		 * the mux to disabled state.

 internal sources muxes *********************************************/

 enable by setting the previous parent clock */

 mux to "disabled" state */

 value to write to register */

 oops */

 au1300 check */

 disabled at index 0 */ ALCHEMY_AUXPLL_CLK,

 divider tables */

 rest */

 Au1300 */

		/* default to first parent clock if mux is initially

		 * set to disabled state.

*********************************************************************/

 Root of the Alchemy clock tree: external 12MHz crystal osc */

 CPU core clock */

 AUXPLLs: max 1GHz on Au1300, 748MHz on older models */

 sysbus clock: cpu core clock divided by 2, 3 or 4 */

 peripheral clock: runs at half rate of sysbus clk */

 SDR/DDR memory clock */

 L/RCLK: external static bus clock for synchronous mode */

 Frequency dividers 0-5 */

 diving muxes for internal sources */

 set up aliases drivers might look for */

/*

 *

 * BRIEF MODULE DESCRIPTION

 *      The Descriptor Based DMA channel manager that first appeared

 *	on the Au1550.  I started with dma.c, but I think all that is

 *	left is this initial comment :-)

 *

 * Copyright 2004 Embedded Edge, LLC

 *	dan@embeddededge.com

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 *

/*

 * The Descriptor Based DMA supports up to 16 channels.

 *

 * There are 32 devices defined. We keep an internal structure

 * of devices using these channels, along with additional

 * information.

 *

 * We allocate the descriptors and allow access to them through various

 * functions.  The drivers allocate the data buffers and assign them

 * to the descriptors.

 I couldn't find a macro that did this... */

 UARTS */

 EXT DMA */

 USB DEV */

 PSCs */

 PCI */

 NAND */

 MAC 0 */

 MAC 1 */

 32 predefined plus 32 custom */

 Allocate a channel and return a non-zero descriptor if successful. */

	/*

	 * We do the initialization on the first channel allocation.

	 * We have to wait because of the interrupt handler initialization

	 * which can't be done successfully during board set up.

 Check to see if we can get both channels. */

 Got source */

 Got destination */

 Can't get dest.  Release src. */

 Let's see if we can allocate a channel for it. */

			/*

			 * If kmalloc fails, it is caught below same

			 * as a channel not available.

 Initialize channel configuration. */

 drain writebuffer */

		/*

		 * Return a non-zero value that can be used to find the channel

		 * information in subsequent operations.

 Release devices */

/*

 * Set the device width if source or destination is a FIFO.

 * Should be 8, 16, or 32 bits.

 Source in fifo */

 Destination out fifo */

 Allocate a descriptor ring, initializing as much as possible. */

	/*

	 * I guess we could check this to be within the

	 * range of the table......

	/*

	 * The descriptors must be 32-byte aligned.  There is a

	 * possibility the allocation will give us such an address,

	 * and if we try that first we are likely to not waste larger

	 * slabs of memory.

		/*

		 * Lost....do it again, allocate extra, and round

		 * the address base.

 Keep track of the base descriptor. */

 Initialize the rings with as much information as we know. */

 Is it mem to mem transfer? */

	/*

	 * If the device is marked as an in/out FIFO, ensure it is

	 * set non-coherent.

 Source in FIFO */

 Destination out FIFO */

	/*

	 * Set up source1.  For now, assume no stride and increment.

	 * A channel attribute update can change this later.

 If source input is FIFO, set static address. */

	/*

	 * Set up dest1.  For now, assume no stride and increment.

	 * A channel attribute update can change this later.

 If destination output is FIFO, set static address. */

 Make last descrptor point to the first. */

/*

 * Put a source buffer into the DMA ring.

 * This updates the source pointer and byte count.  Normally used

 * for memory to fifo transfers.

	/*

	 * I guess we could check this to be within the

	 * range of the table......

	/*

	 * We should have multiple callers for a particular channel,

	 * an interrupt doesn't affect this pointer nor the descriptor,

	 * so no locking should be needed.

	/*

	 * If the descriptor is valid, we are way ahead of the DMA

	 * engine, so just return an error condition.

 Load up buffer address and byte count. */

 Check flags */

	/*

	 * There is an errata on the Au1200/Au1550 parts that could result

	 * in "stale" data being DMA'ed. It has to do with the snoop logic on

	 * the cache eviction buffer.  DMA_NONCOHERENT is on by default for

	 * these parts. If it is fixed in the future, these dma_cache_inv will

	 * just be nothing more than empty macros. See io.h.

 Let it rip */

 drain writebuffer */

 Get next descriptor pointer. */

 Return something non-zero. */

/* Put a destination buffer into the DMA ring.

 * This updates the destination pointer and byte count.  Normally used

 * to place an empty buffer into the ring for fifo to memory transfers.

	/* I guess we could check this to be within the

	 * range of the table......

	/* We should have multiple callers for a particular channel,

	 * an interrupt doesn't affect this pointer nor the descriptor,

	 * so no locking should be needed.

	/* If the descriptor is valid, we are way ahead of the DMA

	 * engine, so just return an error condition.

 Load up buffer address and byte count */

 Check flags  */

	/*

	 * There is an errata on the Au1200/Au1550 parts that could result in

	 * "stale" data being DMA'ed. It has to do with the snoop logic on the

	 * cache eviction buffer.  DMA_NONCOHERENT is on by default for these

	 * parts. If it is fixed in the future, these dma_cache_inv will just

	 * be nothing more than empty macros. See io.h.

 Let it rip */

 drain writebuffer */

 Get next descriptor pointer. */

 Return something non-zero. */

/*

 * Get a destination buffer into the DMA ring.

 * Normally used to get a full buffer from the ring during fifo

 * to memory transfers.  This does not set the valid bit, you will

 * have to put another destination buffer to keep the DMA going.

	/*

	 * I guess we could check this to be within the

	 * range of the table......

	/*

	 * We should have multiple callers for a particular channel,

	 * an interrupt doesn't affect this pointer nor the descriptor,

	 * so no locking should be needed.

	/*

	 * If the descriptor is valid, we are way ahead of the DMA

	 * engine, so just return an error condition.

 Return buffer address and byte count. */

 Get next descriptor pointer. */

 Return something non-zero. */

 Disable channel */

 drain writebuffer */

 clear current desc valid and doorbell */

 drain writebuffer */

/*

 * Start using the current descriptor pointer.  If the DBDMA encounters

 * a non-valid descriptor, it will stop.  In this case, we can just

 * continue by adding a buffer to the list and starting again.

 Enable channel */

 drain writebuffer */

 drain writebuffer */

 Run through the descriptors and reset the valid indicator. */

		/*

		 * Reset our software status -- this is used to determine

		 * if a descriptor is in use by upper level software. Since

		 * posting can reset 'V' bit.

 This is only valid if the channel is stopped. */

 drain writebuffer */

 drain writebuffer */

 Reset interrupt. */

 drain writebuffer */

 Run through the descriptors */

/* Put a descriptor into the DMA ring.

 * This updates the source/destination pointers and byte count.

	/*

	 * I guess we could check this to be within the

	 * range of the table......

	/*

	 * We should have multiple callers for a particular channel,

	 * an interrupt doesn't affect this pointer nor the descriptor,

	 * so no locking should be needed.

	/*

	 * If the descriptor is valid, we are way ahead of the DMA

	 * engine, so just return an error condition.

 Load up buffer addresses and byte count. */

 Allow the caller to specify if an interrupt is generated */

 Get next descriptor pointer. */

 Return something non-zero. */

 save channel configurations */

 halt channel */

 next channel base */

 disable channel interrupts */

 restore channel configurations */

 next channel base */

 drain writebuffer */

/*

 *

 * BRIEF MODULE DESCRIPTION

 *      A DMA channel allocator for Au1x00. API is modeled loosely off of

 *      linux/kernel/dma.c.

 *

 * Copyright 2000, 2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 * Copyright (C) 2005 Ralf Baechle (ralf@linux-mips.org)

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 *

/*

 * A note on resource allocation:

 *

 * All drivers needing DMA channels, should allocate and release them

 * through the public routines `request_dma()' and `free_dma()'.

 *

 * In order to avoid problems, all processes should allocate resources in

 * the same sequence and release them in the reverse order.

 *

 * So, when allocating DMAs and IRQs, first allocate the DMA, then the IRQ.

 * When releasing them, first release the IRQ, then release the DMA. The

 * main reason for this order is that, if you are requesting the DMA buffer

 * done interrupt, you won't know the irq number until the DMA channel is

 * returned from request_dma.

 DMA Channel register block spacing */

 Device FIFO addresses and default DMA modes */

 UART0_TX */

 UART0_RX */

 DMA_REQ0 */

 DMA_REQ1 */

 AC97 TX c */

 AC97 RX c */

 UART3_TX */

 UART3_RX */

 EP0RD */

 EP0WR */

 EP2WR */

 EP3WR */

 EP4RD */

 EP5RD */

 on Au1500, these 2 are DMA_REQ2/3 (GPIO208/209) instead! */

 I2S TX */

 I2S RX */

 Device FIFO addresses and default DMA modes - 2nd bank */

 coherent */

 coherent */

 coherent */

 coherent */

/*

 * Finds a free channel, and binds the requested device to it.

 * Returns the allocated channel number, or negative on error.

 * Requests the DMA done IRQ if irqhandler != NULL.

 fill it in */

 initialize the channel before returning */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * USB block power/access management abstraction.

 *

 * Au1000+: The OHCI block control register is at the far end of the OHCI memory

 *	    area. Au1550 has OHCI on different base address. No need to handle

 *	    UDC here.

 * Au1200:  one register to control access and clocks to O/EHCI, UDC and OTG

 *	    as well as the PHY for EHCI and UDC.

 *

 control register offsets */

 Au1000 USB block config bits */

 OHCI reset-done indicator */

 OHCI block clock enable */

 OHCI block enable */

 OHCI block coherency bit */

 OHCI Big-Endian */

 Au1200 USB config bits */

 prefetch enable (undoc) */

 read combining (undoc) */

 unknown, leave this way */

 serial short detect en */

 HS PHY PLL */

 UDC clock enable */

 EHCI clock enable */

 OHCI clock enable */

 coherent access (undoc) */

 OTG mem access */

 UDC busmaster enable */

 UDC mem enable */

 EHCI busmaster enable */

 EHCI mem enable */

 OHCI busmaster enable */

 OHCI mem enable */

 Au1300 USB config registers */

 set to DISable OTG */

 set to ENable EHCI */

 set to ENable UDC */

 set to enable PHY1 */

 set to enable PHY0 */

 set to enable PHY */

 coherent access */

 simply enable all PHYs */

 no USB block active, do disable all PHYs */

 start OHCI clock */

 enable OHCI block */

 power up the PHYs */

 reset the OHCI start clock bit */

	/* set some sane defaults.  Note: we don't fiddle with DWC_CTRL4

	 * here at all: Port 2 routing (EHCI or UDC) must be set either

	 * by boot firmware or platform init code; I can't autodetect

	 * a sane setting.

 disable all USB irqs */

 disable all clocks */

 clear all errors */

 clear int status */

 set coherent access bit */

 UDC also off? */

 yes: disable HS PHY PLL */

 EHCI also off? */

 yes: disable HS PHY PLL */

 initialize USB block(s) to a known working state */

 48MHz check. Don't init if no one can provide it */

 wait for reset complete (read reg twice: au1500 erratum) */

/*

 * alchemy_usb_control - control Alchemy on-chip USB blocks

 * @block:	USB block to target

 * @enable:	set 1 to enable a block, 0 to disable

 There appears to be some undocumented reset register.... */

 save OTG_CAP/MUX registers which indicate port routing */

 FIXME: write an OTG driver to do that */

 restore access to all MMIO areas */

 restore OTG_CAP/MUX registers */

 remember Port2 routing */

/*

 * BRIEF MODULE DESCRIPTION

 *	Au1xx0 Power Management routines.

 *

 * Copyright 2001, 2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 *

 *  Some of the routines are right out of init/main.c, whose

 *  copyrights apply here.

 *

 *  This program is free software; you can redistribute	 it and/or modify it

 *  under  the terms of	 the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the	License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED	  ``AS	IS'' AND   ANY	EXPRESS OR IMPLIED

 *  WARRANTIES,	  INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO	EVENT  SHALL   THE AUTHOR  BE	 LIABLE FOR ANY	  DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED	  TO, PROCUREMENT OF  SUBSTITUTE GOODS	OR SERVICES; LOSS OF

 *  USE, DATA,	OR PROFITS; OR	BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN	 CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

/*

 * We need to save/restore a bunch of core registers that are

 * either volatile or reset to some state across a processor sleep.

 * If reading a register doesn't provide a proper result for a

 * later restore, we have to provide a function for loading that

 * register and save a copy.

 *

 * We only have to save/restore registers that aren't otherwise

 * done as part of a driver pm_* function.

 Clocks and PLLs. */

 pin mux config */

 Save the static memory controller configuration. */

	/* restore clock configuration.  Writing CPUPLL last will

	 * stall a bit and stabilize other clocks (unless this is

	 * one of those Au1000 with a write-only PLL, where we dont

	 * have a valid value)

 Restore the static memory controller configuration. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Au1300 media block power gating (VSS)

 *

 * This is a stop-gap solution until I have the clock framework integration

 * ready. This stuff here really must be handled transparently when clocks

 * for various media blocks are enabled/disabled.

 gate wait timers */

 clock/block control */

 footers */

 enable a block as outlined in the databook */

 enable clock, assert reset */

 maximum setup time */

 enable footers in sequence */

 start FSM too */

 deassert reset */

 enable isolation cells */

 disable a block as outlined in the databook */

 disable isolation cells */

 disable FSM */

 assert reset */

 disable clock */

 disable all footers */

 only one block at a time */

/*

 * Platform device support for Au1x00 SoCs.

 *

 * Copyright 2004, Matt Porter <mporter@kernel.crashing.org>

 *

 * (C) Copyright Embedded Alley Solutions, Inc 2005

 * Author: Pantelis Antoniou <pantelis@embeddedalley.com>

 *

 * This file is licensed under the terms of the GNU General Public

 * License version 2.  This program is licensed "as is" without any

 * warranty of any kind, whether express or implied.

 power off */

 Fill up uartclk. */

 Power on callback for the ehci platform driver */

 Power off/suspend callback for the ehci platform driver */

 Power on callback for the ohci platform driver */

 Power off/suspend callback for the ohci platform driver */

 setup OHCI0.  Every variant has one */

 setup EHCI0: Au1200/Au1300 */

 Au1300: OHCI1 */

 Macro to help defining the Ethernet MAC resources */

 MAC regs, MAC en, MAC INT, MACDMA regs */

 Handle 1st MAC */

 Handle 2nd MAC */

 next addr for 2nd MAC */

 Register second MAC if enabled in pinfunc */

/*

 * Copyright 2001, 2007-2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 *

 * Copyright (C) 2007 Ralf Baechle (ralf@linux-mips.org)

 *

 *  This program is free software; you can redistribute	 it and/or modify it

 *  under  the terms of	 the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the	License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED	  ``AS	IS'' AND   ANY	EXPRESS OR IMPLIED

 *  WARRANTIES,	  INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO	EVENT  SHALL   THE AUTHOR  BE	 LIABLE FOR ANY	  DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED	  TO, PROCUREMENT OF  SUBSTITUTE GOODS	OR SERVICES; LOSS OF

 *  USE, DATA,	OR PROFITS; OR	BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN	 CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 Interrupt Controller register offsets */

 per-processor fixed function irqs */

 linux IRQ number */

 IRQ_TYPE_ */

 irq priority, 0 highest, 3 lowest */

 GPIC: internal source (no ext. pin)? */

/* NOTE on interrupt priorities: The original writers of this code said:

 *

 * Because of the tight timing of SETUP token to reply transactions,

 * the USB devices-side packet complete interrupt (USB_DEV_REQ_INT)

 * needs the highest priority.

 multifunction: gpio pin or device */

 au1300 internal */

 terminator */

*****************************************************************************/

	/*

	 * This may assume that we don't get interrupts from

	 * both edges at once, or if we do, that we don't care.

	/*

	 * This may assume that we don't get interrupts from

	 * both edges at once, or if we do, that we don't care.

	/* only GPIO 0-7 can act as wakeup source.  Fortunately these

	 * are wired up identically on all supported variants.

/*

 * irq_chips for both ICs; this way the mask handlers can be

 * as short as possible.

 cfgregs 2:1:0 */

 0:0:1 */

 0:1:0 */

 0:1:1 */

 1:0:1 */

 1:1:0 */

 0:0:0 */

*****************************************************************************/

/*

 * au1300_gpic_chgcfg - change PIN configuration.

 * @gpio:	pin to change (0-based GPIO number from datasheet).

 * @clr:	clear all bits set in 'clr'.

 * @set:	set these bits.

 *

 * modifies a pins' configuration register, bits set in @clr will

 * be cleared in the register, bits in @set will be set.

 offset into pin config array */

/*

 * au1300_pinfunc_to_gpio - assign a pin as GPIO input (GPIO ctrl).

 * @pin:	pin (0-based GPIO number from datasheet).

 *

 * Assigns a GPIO pin to the GPIO controller, so its level can either

 * be read or set through the generic GPIO functions.

 * If you need a GPOUT, use au1300_gpio_set_value(pin, 0/1).

 * REVISIT: is this function really necessary?

/*

 * au1300_pinfunc_to_dev - assign a pin to the device function.

 * @pin:	pin (0-based GPIO number from datasheet).

 *

 * Assigns a GPIO pin to its associated device function; the pin will be

 * driven by the device and not through GPIO functions.

/*

 * au1300_set_irq_priority -  set internal priority of IRQ.

 * @irq:	irq to set priority (linux irq number).

 * @p:		priority (0 = highest, 3 = lowest).

/*

 * au1300_set_dbdma_gpio - assign a gpio to one of the DBDMA triggers.

 * @dchan:	dbdma trigger select (0, 1).

 * @gpio:	pin to assign as trigger.

 *

 * DBDMA controller has 2 external trigger sources; this function

 * assigns a GPIO to the selected trigger.

 ack */

 mask */

 ack */

*****************************************************************************/

 initialize interrupt controller to a safe state */

 shut it up too while at it */

 save 4 interrupt mask status registers */

 save misc register(s) */

 molto silenzioso */

 save pin/int-type configuration */

 disable all first */

 restore pin/int-type configurations */

 restore misc register(s) */

 finally restore masks */

*****************************************************************************/

 create chained handlers for the 4 IC requests to the MIPS IRQ ctrl */

*****************************************************************************/

	/* register all 64 possible IC0+IC1 irq sources as type "none".

	 * Use set_irq_type() to set edge/level behaviour at runtime.

	/*

	 * Initialize IC0, which is fixed per processor.

 disable & ack all possible interrupt sources */

 register an irq_chip for them, with 2nd highest priority */

 setup known on-chip sources */

*****************************************************************************/

/*

 *  Copyright (C) 2007-2009, OpenWrt.org, Florian Fainelli <florian@openwrt.org>

 *	GPIOLIB support for Alchemy chips.

 *

 *  This program is free software; you can redistribute	 it and/or modify it

 *  under  the terms of	 the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the	License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED	  ``AS	IS'' AND   ANY	EXPRESS OR IMPLIED

 *  WARRANTIES,	  INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO	EVENT  SHALL   THE AUTHOR  BE	 LIABLE FOR ANY	  DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED	  TO, PROCUREMENT OF  SUBSTITUTE GOODS	OR SERVICES; LOSS OF

 *  USE, DATA,	OR PROFITS; OR	BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN	 CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 *

 *  Notes :

 *	This file must ONLY be built when CONFIG_GPIOLIB=y and

 *	 CONFIG_ALCHEMY_GPIO_INDIRECT=n, otherwise compilation will fail!

 *	au1000 SoC have only one GPIO block : GPIO1

 *	Au1100, Au15x0, Au12x0 have a second one : GPIO2

 *	Au1300 is totally different: 1 block with up to 128 GPIOs

 SPDX-License-Identifier: GPL-2.0

/*

 * Alchemy Development Board example suspend userspace interface.

 *

 * (c) 2008 Manuel Lauss <mano@roarinelk.homelinux.net>

/*

 * Generic suspend userspace interface for Alchemy development boards.

 * This code exports a few sysfs nodes under /sys/power/db1x/ which

 * can be used by userspace to en/disable all au1x-provided wakeup

 * sources and configure the timeout after which the the TOYMATCH2 irq

 * is to trigger a wakeup.

 save CPLD regs */

 shut off hexleds */

 enable GPIO based wakeup */

 clear and setup wake cause and source */

 setup 1Hz-timer-based wakeup: wait for reg access */

 wait for value to really hit the register */

 ...and now the sandman can come! */

 restore CPLD regs */

 restore CPLD int registers */

 light up hexleds */

	/* read and store wakeup source, the clear the register. To

	 * be able to clear it, WAKEMSK must be cleared first.

 GPIO-based wakeup enable */

 TOYMATCH2-based wakeup enable */

 timer-based wakeup timeout value, in seconds */

 contents of SYS_WAKESRC after last wakeup */

 direct access to SYS_WAKEMSK */

 terminator */

/*

 * Initialize suspend interface

	/* init TOY to tick at 1Hz if not already done. No need to wait

	 * for confirmation since there's plenty of time from here to

	 * the next suspend cycle.

 SPDX-License-Identifier: GPL-2.0

/*

 * DBAu1300 init and platform device setup.

 *

 * (c) 2009 Manuel Lauss <manuel.lauss@googlemail.com>

 KEY_* codes */

 FPGA (external mux) interrupt sources */

 SMSC9210 CS */

 ATA CS */

 NAND CS */

 I2S audio codec */

 adm1025-compat hwmon */

 multifunction pins to assign to GPIO controller */

 terminator */

 multifunction pins to assign to device functions */

 wake-from-str pins 0-3 */

 external clock sources for PSC0 */

 8bit MMC interface on SD0: 6-9 */

 UART1 pins: 11-18 */

 UART0 pins: 19-24 */

 UART2: 25-26 */

 UART3: 27-28 */

 LCD controller PWMs, ext pixclock: 30-31 */

 SD1 interface: 32-37 */

 SD2 interface: 38-43 */

 PSC0/1 clocks: 44-45 */

 PSCs: 46-49/50-53/54-57/58-61 */

 PCMCIA interface: 62-70 */

 camera interface H/V sync inputs: 71-72 */

 PSC2/3 clocks: 73-74 */

 terminator */

 implies pin_to_gpio */

*********************************************************************/

 assume we want to r/w real data  by default */

*********************************************************************/

*********************************************************************/

 PSC ID. match with AC97 codec ID! */

*********************************************************************/

 PSC ID */

*********************************************************************/

 bus number */

*********************************************************************/

/* proper key assignments when facing the LCD panel.  For key assignments

 * according to the schematics swap up with down and left with right.

 * I chose to use it to emulate the arrow keys of a keyboard.

*********************************************************************/

*********************************************************************/

	/* link against CONFIG_MMC=m.  We can only be called once MMC core has

	 * initialized the controller, so symbol_get() should always succeed.

 debounce */

 it uses SD1 interface, but the DB1200's SD0 bit in the CPLD */

 insertion irq signal */

*********************************************************************/

 disable for now, it doesn't work yet */

*********************************************************************/

 ID of PSC for AC97 audio, see asoc glue! */

 PSC ID */

 PSC ID */

*********************************************************************/

 DB1300_800x480 */

 Apply power (Vee/Vdd logic is inverted on Panel DB1300_800x480) */

 Remove power (Vee/Vdd logic is inverted on Panel DB1300_800x480) */

*********************************************************************/

 external pendown indicator */

 internal "virtual" pendown gpio */

*********************************************************************/

 setup CPLD IRQ muxer */

	/* insert/eject IRQs: one always triggers so don't enable them

	 * when doing request_irq() on them.  DB1200 has this bug too.

	/*

	 * setup board

 Audio PSC clock is supplied by codecs (PSC1, 2) */

 I2C driver wants 50MHz, get as close as possible */

 enable power to USB ports */

	/* although it is socket #0, it uses the CPLD bits which previous boards

	 * have used for socket #1.

 enable UARTs, YAMON only enables #2 */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DBAu1000/1500/1100 PBAu1100/1500 board support

 *

 * Copyright 2000, 2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 initialize board register space */

*****************************************************************************/

 link against CONFIG_MMC=m */

 PB1100 SD0 CD# */

 PB1100 SD1 CD# */

 testing suggests that this bit is inverted */

 stabilization time */

 stabilization time */

*****************************************************************************/

/*

 * Alchemy GPIO 2 has its base at 200 so the GPIO lines

 * 207 thru 210 are GPIOs at offset 7 thru 10 at this chip.

 GPIO number, NOT irq! */

 GPIO number, NOT irq! */

 GPIO number, NOT irq! */

 GPIO number, NOT irq! */

 sd0 cd# */

 sd1 cd# */

 spi_gpio on SSI0 pins */

 SSI0 pins as GPIOs */

 link LCD clock to AUXPLL */

 GPIO number, NOT irq! */

 GPIO number, NOT irq! */

 GPIO number, NOT irq! */

 RTC and daughtercard irqs */

		/* EPSON S1D13806 0x1b000000

		 * SRAM 1MB/2MB	  0x1a000000

		 * DS1693 RTC	  0x0c000000

 GPIO number, NOT irq! */

 pendown, rtc, daughtercard irqs */

		/* EPSON S1D13806 0x1b000000

		 * SRAM 1MB/2MB	  0x1a000000

		 * DiskOnChip	  0x0d000000

		 * DS1693 RTC	  0x0c000000

 unknown board, no further dev setup to do */

s0*/0, 0, 0);

s1*/0, 0, 1);

 32bit */, F_SWAPPED);

 SPDX-License-Identifier: GPL-2.0-only

/*

 * bcsr.h -- Db1xxx/Pb1xxx Devboard CPLD registers ("BCSR") abstraction.

 *

 * All Alchemy development boards (except, of course, the weird PB1000)

 * have a few registers in a CPLD with standardised layout; they mostly

 * only differ in base address.

 * All registers are 16bits wide with 32bit spacing.

 KSEG1 addr of BCSR base */

 linux-irq of first cascaded irq */

/*

 * DB1200/PB1200 CPLD IRQ muxer

 ack */

 mask & enable & ack all */

 SPDX-License-Identifier: GPL-2.0

/*

 * devoard misc stuff.

 sit and spin */

 register a pcmcia socket */

 NOR flash ends at 0x20000000, regardless of size */

	/* partition setup.  Most Develboards have a switch which allows

	 * to swap the physical locations of the 2 NOR flash banks.

 first NOR chip */

 SPDX-License-Identifier: GPL-2.0

/*

 * Alchemy DB/PB1xxx board support.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * DBAu1200/PBAu1200 board platform device registration

 *

 * Copyright (C) 2008-2011 Manuel Lauss

 try the DB1200 first */

 okay, try the PB1200 then */

 it's neither */

*****************************************************************************/

 TI TMP121AIDBVR temp sensor */

 Spansion S25FL001D0FMA SPI flash */

 AT24C04-10 I2C eeprom */

 adm1025-compat hwmon */

 I2S audio codec WM8731 */

*********************************************************************/

 assume we want to r/w real data  by default */

*********************************************************************/

*********************************************************************/

*********************************************************************/

/* SD carddetects:  they're supposed to be edge-triggered, but ack

 * doesn't seem to work (CPLD Rev 2).  Instead, the screaming one

 * is disabled and its counterpart enabled.  The 200ms timeout is

 * because the carddetect usually triggers twice, after debounce.

 link against CONFIG_MMC=m */

 debounce */

 stabilization time */

 -- */

 link against CONFIG_MMC=m */

 debounce */

 stabilization time */

*********************************************************************/

 Apply power */

 Remove power */

*********************************************************************/

 bus number */

 PSC0 clock */

 bus number */

 AC97 or I2S device */

 name assigned later based on switch setting */

 PSC ID */

 DB1200 ASoC card device */

 name assigned later based on switch setting */

 PSC ID */

 on PSC1 */

 PSC ID */

 PSC0, selected by S6.8 */

 Some peripheral base addresses differ on the PB1200 */

 CPLD Revs earlier than 4 cause problems */

 GPIO7 is low-level triggered CPLD cascade */

 SMBus/SPI on PSC0, Audio on PSC1 */

 SPI is configured later */

 get 50MHz for I2C driver on PSC0 */

	/* insert/eject pairs: one of both is always screaming.	 To avoid

	 * issues they must not be automatically enabled when initially

	 * requested.

	/* SWITCHES:	S6.8 I2C/SPI selector  (OFF=I2C	 ON=SPI)

	 *		S6.7 AC97/I2S selector (OFF=AC97 ON=I2S)

	 *		or S12 on the PB1200.

	/* NOTE: GPIO215 controls OTG VBUS supply.  In SPI mode however

	 * this pin is claimed by PSC0 (unused though, but pinmux doesn't

	 * allow to free it without crippling the SPI interface).

	 * As a result, in SPI mode, OTG simply won't work (PSC0 uses

	 * it as an input pin which is pulled high on the boards).

 switch off OTG VBUS supply */

 GPIO2 block owns GPIO215 */

 PSC0 owns GPIO215 */

	/* Audio: DIP7 selects I2S(0)/AC97(1), but need I2C for I2S!

	 * so: DIP7=1 || DIP8=0 => AC97, DIP7=0 && DIP8=1 => I2S

 Audio PSC clock is supplied externally. (FIXME: platdata!!) */

DB1200_PC0_STSCHG_INT*/0, DB1200_PC0_EJECT_INT, 0);

DB1200_PC1_STSCHG_INT*/0, DB1200_PC1_EJECT_INT, 1);

 PB1200 is a DB1200 with a 2nd MMC and Camera connector */

 SPDX-License-Identifier: GPL-2.0

/*

 * Alchemy Db1550/Pb1550 board support

 *

 * (c) 2011 Manuel Lauss <manuel.lauss@googlemail.com>

	/* complete pin setup: assign GPIO16 to PSC0_SYNC1 (SPI cs# line)

	 * as well as PSC1_SYNC for AC97 on PB1550.

	/* reset the AC97 codec now, the reset time in the psc-ac97 driver

	 * is apparently too short although it's ridiculous as it is.

 PB1550 hexled offset differs */

****************************************************************************/

 TI TMP121AIDBVR temp sensor */

 Spansion S25FL001D0FMA SPI flash */

 AT24C04-10 I2C eeprom */

 adm1025-compat hwmon */

 I2S audio codec WM8731 */

*********************************************************************/

 assume we want to r/w real data  by default */

 x8 NAND default, needs fixing up */

 de-assert NAND CS# */

 x16 NAND Flash */

 x8 NAND, already set up */

*********************************************************************/

 PSC0 clock: max. 2.4MHz SPI clk */

 bus number */

*********************************************************************/

 PSC ID */

 bus number */

*********************************************************************/

 PSC ID */

*********************************************************************/

 on PSC1 */

 on PSC3 */

 on PSC3 */

*********************************************************************/

*********************************************************************/

 must be arch_initcall; MIPS PCI scans busses in a subsys_initcall */

 red led on */

 CD0# */

 CD1# */

 CARD0# */

 CARD1# */

 STSCHG0# */

 STSCHG1# */

AU1550_GPIO21_INT*/0, 0, 0);

AU1550_GPIO22_INT*/0, 0, 1);

 green led on */

 enable both PCMCIA card irqs in the shared line */

 socket 0 card irq */

 socket 1 card irq */

	/* Pb1550, like all others, also has statuschange irqs; however they're

	* wired up on one of the Au1550's shared GPIO201_205 line, which also

	* services the PCMCIA card interrupts.	So we ignore statuschange and

	* use the GPIO201_205 exclusively for card interrupts, since a) pcmcia

	* drivers are used to shared irqs and b) statuschange isn't really use-

	* ful anyway.

 Audio PSC clock is supplied by codecs (PSC1, 3) FIXME: platdata!! */

 SPI/I2C use internally supplied 50MHz source */

 SPDX-License-Identifier: GPL-2.0

/*

 * MIPS accelerated ChaCha and XChaCha stream ciphers,

 * including ChaCha20 (RFC7539)

 *

 * Copyright (C) 2019 Linaro, Ltd. <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0

/*

 * crc32-mips.c - CRC32 and CRC32C using optional MIPSr6 instructions

 *

 * Module based on arm64/crypto/crc32-arm.c

 *

 * Copyright (C) 2014 Linaro Ltd <yazen.ghannam@linaro.org>

 * Copyright (C) 2018 MIPS Tech, LLC

 !TOOLCHAIN_SUPPORTS_CRC */

 !CONFIG_64BIT */

 !CONFIG_64BIT */

/*

 * Setting the seed allows arbitrary accumulators and flexible XOR policy

 * If your algorithm starts with ~0, then XOR with ~0 before you set

 * the seed.

 SPDX-License-Identifier: GPL-2.0

/*

 * OpenSSL/Cryptogams accelerated Poly1305 transform for MIPS

 *

 * Copyright (C) 2019 Linaro Ltd. <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR71xx PCI host controller driver

 *

 *  Copyright (C) 2008-2011 Gabor Juhos <juhosg@openwrt.org>

 *  Copyright (C) 2008 Imre Kaloz <kaloz@openwrt.org>

 *

 *  Parts of this file are based on Atheros' 2.6.15 BSP

 Byte lane enable bits */

 type 0 */

 type 1 */

 clear PCI error status */

 clear AHB error status */

 flush write */

 flush write */

 setup COMMAND register */

 clear bus errors */

/*

 *  BRIEF MODULE DESCRIPTION

 *     PCI initialization for IDT EB434 board

 *

 *  Copyright 2004 IDT Inc. (rischelp@idt.com)

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 define an unsigned array for the PCI registers */

 Not in Host Mode, return ERROR */

 Enables the Idle Grant mode, Arbiter Parking */

 Enable the PCI bus Interface */

 Zero out the PCI status & PCI Status Mask */

 Zero out the PCI decoupled registers */

	rc32434_pci->pcidac = 0;	/*

					 * disable PCI decoupled accesses at

					 * initialization

 clear the status */

 Mask all the interrupts */

 Mask PCI Messaging Interrupts */

 Setup PCILB0 as Memory Window */

 setup the PCI map address as same as the local address */

 Setup PCILBA1 as MEM */

 flush the CPU write Buffers */

 setup PCILBA2 as IO Window */

 flush the CPU write Buffers */

 setup PCILBA2 as IO Window */

 flush the CPU write Buffers */

 Setup PCILBA3 as IO Window */

 flush the CPU write Buffers */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001,2002,2005 Broadcom Corporation

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * BCM1x80/1x55-specific PCI support

 *

 * This module provides the glue between Linux's PCI subsystem

 * and the hardware.  We basically provide glue for accessing

 * configuration space, and set up the translation for I/O

 * space accesses.

 *

 * To access configuration space, we use ioremap.  In the 32-bit

 * kernel, this consumes either 4 or 8 page table pages, and 16MB of

 * kernel mapped memory.  Hopefully neither of these should be a huge

 * problem.

 *

 * XXX: AT THIS TIME, ONLY the NATIVE PCI-X INTERFACE IS SUPPORTED.

/*

 * Macros for calculating offsets into config space given a device

 * structure or dev/fun/reg

/*

 * Read/write 32-bit values in config space.

 Do platform specific device initialization at pci_enable_device() time */

/*

 * Some checks before doing config cycles:

 * In PCI Device Mode, hide everything on bus 0 except the LDT host

 * bridge.  Otherwise, access is controlled by bridge MasterEn bits.

/*

 * Read/write access functions for various sizes of values

 * in config space.  Return all 1's for disallowed accesses

 * for a kludgy but adequate simulation of master aborts.

 CFE will assign PCI resources */

 Avoid ISA compat ranges.  */

 Set I/O resource limits. - unlimited for now to accommodate HT */

	/*

	 * See if the PCI bus has been configured by the firmware.

 XXX */

 turn on ExpMemEn */

	/*

	 * Establish mappings in KSEG2 (kernel virtual) to PCI I/O

	 * space.  Use "match bytes" policy to make everything look

	 * little-endian.  So, you need to also set

	 * CONFIG_SWAP_IO_SPACE, but this is the combination that

	 * works correctly with most of Linux's drivers.

	 * XXX ehs: Should this happen in PCI Device mode?

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  fixup-tb0219.c, The TANBAC TB0219 specific PCI fixups.

 *

 *  Copyright (C) 2003	Megasolution Inc. <matsu@megasolution.jp>

 *  Copyright (C) 2004-2005  Yoichi Yuasa <yuasa@linux-mips.org>

 Do platform specific device initialization at pci_enable_device() time */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000, 2001 Keith M Wesolowski

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * Handle errors from the bridge.  This includes master and target aborts,

 * various command and address errors, and the interrupt test.	This gets

 * registered on the bridge error irq.	It's conceivable that some of these

 * conditions warrant a panic.	Anybody care to say which ones?

 Clear any outstanding errors and enable interrupts */

 extend memory resources */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000, 2004  MIPS Technologies, Inc.

 *	All rights reserved.

 *	Authors: Carsten Langgaard <carstenl@mips.com>

 *		 Maciej W. Rozycki <macro@mips.com>

 *

 * MIPS boards specific PCI support.

 Type 0 configuration for onboard PCI bus */

 Type 1 configuration for offboard PCI bus */

 Clear aborts */

 Flush Bonito register block */

 Wait till done */

 Detect Master/Target abort */

 Error occurred */

 Clear bits */

/*

 * We can't address 8 and 16 bit words directly.  Instead we have to

 * read/write a 32bit word and mask/modify the data we actually want.

/*

 * Based on linux/arch/mips/txx9/rbtx4939/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright 2001, 2003-2005 MontaVista Software Inc.

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 IRQ rotation */

 0-3 */

 1-4 */

/*

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright 2001, 2003-2005 MontaVista Software Inc.

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Assert M66EN */

 Double PCICLK (if possible) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  ops-vr41xx.c, PCI configuration routines for the PCIU of NEC VR4100 series.

 *

 *  Copyright (C) 2001-2003 MontaVista Software Inc.

 *    Author: Yoichi Yuasa <source@mvista.com>

 *  Copyright (C) 2004-2005  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Changes:

 *  MontaVista Software Inc. <source@mvista.com>

 *  - New creation, NEC VR4122 and VR4131 are supported.

		/*

		 * Type 0 configuration

		/*

		 * Type 1 configuration

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * SNI specific PCI support for RM200/RM300.

 *

 * Copyright (C) 1997 - 2000, 2003 Ralf Baechle <ralf@linux-mips.org>

/*

 * It seems that on the RM200 only lower 3 bits of the 5 bit PCI device

 * address are decoded.	 We therefore manually have to reject attempts at

 * reading outside this range.	Being on the paranoid side we only do this

 * test for bus 0 and hope forwarding and decoding work properly for any

 * subordinated busses.

 *

 * ASIC PCI only supports type 1 config cycles.

	/*

	 * on bus 0 we need to check, whether there is a device answering

	 * for the devfn by doing a config write and checking the result. If

	 * we don't do it, we will get a data bus error

/*

 * Toshiba rbtx4938 pci routines

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * 2003-2005 (c) MontaVista Software, Inc. This file is licensed under the

 * terms of the GNU General Public License version 2. This program is

 * licensed "as is" without any warranty of any kind, whether express

 * or implied.

 *

 * Support for TX4938 in 2.6 - Manish Lachwani (mlachwani@mvista.com)

 IRQ rotation */

 0-3 */

 PCI CardSlot (IDSEL=A23) */

 PCIA => PCIA (IDSEL=A23) */

 PCI Backplane */

 1-4 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000, 2004  MIPS Technologies, Inc.

 *	All rights reserved.

 *	Authors: Carsten Langgaard <carstenl@mips.com>

 *		 Maciej W. Rozycki <macro@mips.com>

 *

 * Copyright (C) 2009 Lemote Inc.

 * Author: Wu Zhangjin <wuzhangjin@gmail.com>

		/* board-specific part,currently,only fuloong2f,yeeloong2f

		 * use CS5536, fuloong2e use via686b, gdium has no

		 * south bridge

		/* cs5536_pci_conf_read4/write4() will call _rdmsr/_wrmsr() to

		 * access the regsters PCI_MSR_ADDR, PCI_MSR_DATA_LO,

		 * PCI_MSR_DATA_HI, which is bigger than PCI_MSR_CTRL, so, it

		 * will not go this branch, but the others. so, no calling dead

		 * loop here.

 Type 0 configuration for onboard PCI bus */

 Type 1 configuration for offboard PCI bus */

 Clear aborts */

 Flush Bonito register block */

 Detect Master/Target abort */

 Error occurred */

 Clear bits */

/*

 * We can't address 8 and 16 bit words directly.  Instead we have to

 * read/write a 32bit word and mask/modify the data we actually want.

/*

 *  Copyright (C) 2008 Aurelien Jarno <aurelien@aurel32.net>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 IRQ-0 and IRQ-1 are software interrupts. */

 IRQ-0 and IRQ-1 are software interrupts. */

/*

 *  BRIEF MODULE DESCRIPTION

 *     pci_ops for IDT EB434 board

 *

 *  Copyright 2004 IDT Inc. (rischelp@idt.com)

 *  Copyright 2006 Felix Fietkau <nbd@openwrt.org>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 Setup address */

/*

 * We can't address 8 and 16 bit words directly.  Instead we have to

 * read/write a 32bit word and mask/modify the data we actually want.

	/*

	 * Don't scan too far, else there will be errors with plugged in

	 * daughterboard (rb564).

	/*

	 * Certain devices react delayed at device scan time, this

	 * gives them time to settle

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  fixup-cappcela.c, The ZAO Networks Capcella specific PCI fixups.

 *

 *  Copyright (C) 2002,2004  Yoichi Yuasa <yuasa@linux-mips.org>

/*

 * Shortcuts

 Do platform specific device initialization at pci_enable_device() time */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2010 John Crispin <john@phrozen.org>

 BAR11MASK value depends on available memory on system. */

 get our clocks */

 read the bus speed that we want */

 and enable the clocks */

 setup reset gpio used by pci */

 enable auto-switching between PCI and EBU */

 busy, i.e. configuration is not done, PCI access has to be retried */

 BUS Master/IO/MEM access */

 enable external 2 PCI masters */

 setup the request mask */

 enable internal arbiter */

 enable internal PCI master reqest */

 enable EBU request */

 enable all external masters request */

 setup BAR memory regions */

 both TX and RX endian swap are enabled */

use 8 dw burst length */

 setup irq line */

 toggle reset pin */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2012 John Crispin <john@phrozen.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *	Copyright (C) 2004, 2006  MIPS Technologies, Inc.  All rights reserved.

 *	    Author:	Maciej W. Rozycki <macro@mips.com>

 *	Copyright (C) 2018  Maciej W. Rozycki

/*

 * Set the BCM1250, etc. PCI host bridge's TRDY timeout

 * to the finite max.

/*

 * The BCM1250, etc. PCI host bridge does not support DAC on its 32-bit

 * bus, so we set the bus's DMA limit accordingly.  However the HT link

 * down the artificial PCI-HT bridge supports 40-bit addressing and the

 * SP1011 HT-PCI bridge downstream supports both DAC and a 64-bit bus

 * width, so we record the PCI-HT bridge's secondary and subordinate bus

 * numbers and do not set the limit for devices present in the inclusive

 * range of those.

/*

 * The BCM1250, etc. PCI/HT bridge reports as a host bridge.

/*

 * Set the SP1011 HT/PCI bridge's TRDY timeout to the finite max.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2016 Imagination Technologies

 * Author: Paul Burton <paul.burton@mips.com>

 *

 * pcibios_align_resource taken from arch/arm/kernel/bios32.c.

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

 *

 * Why? Because some silly external IO cards only decode

 * the low 10 bits of the IO address. The 0x00-0xff region

 * is reserved for motherboard devices that decode all 16

 * bits, so it's ok to allocate at, say, 0x2800-0x28ff,

 * but we want to try to avoid allocating at 0x2900-0x2bff

 * which might have be mirrored at 0x0100-0x03ff..

 SPDX-License-Identifier: GPL-2.0

 PCI interrupt pins */

 This table is filled in by interrogating the PIIX4 chip */

	INTA	INTB	INTC	INTD */

  0: GT64120 PCI bridge */

  1: Unused */

  2: Unused */

  3: Unused */

  4: Unused */

  5: Unused */

  6: Unused */

  7: Unused */

  8: Unused */

  9: Unused */

 10: PIIX4 USB */

 11: AMD 79C973 Ethernet */

 12: Crystal 4281 Sound */

 13: Unused */

 14: Unused */

 15: Unused */

 16: Unused */

 17: Bonito/SOC-it PCI Bridge*/

 18: PCI Slot 1 */

 19: PCI Slot 2 */

 20: PCI Slot 3 */

 21: PCI Slot 4 */

 Do platform specific device initialization at pci_enable_device() time */

 Set a sane PM I/O base address */

 Enable access to the PM I/O region */

 PIIX PIRQC[A:D] irq mappings */

 Interrogate PIIX4 to get PCI IRQ mapping */

 Disabled */

 Done by YAMON 2.00 onwards */

		/*

		 * Set top of main memory accessible by ISA or DMA

		 * devices to 16 Mb.

 Mux SERIRQ to its pin */

 Enable SERIRQ */

 Enable response to special cycles */

 Done by YAMON 2.02 onwards */

		/*

		 * IDE Decode enable.

 Enable PCI 2.1 compatibility in PIIX4 */

 Enable passive releases and delayed transaction */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2003 Christoph Hellwig (hch@lst.de)

 * Copyright (C) 1999, 2000, 04 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

/*

 * Common phys<->dma mapping for platforms using pci xtalk bridge

/*

 * Most of the IOC3 PCI config register aren't present

 * we emulate what is needed for a normal PCI enumeration

 emulate sane interrupt pin value */

 Turn off byte swapping */

 Flush */

/*

 * The Bridge ASIC supports both type 0 and type 1 access.  Type 1 is

 * not really documented, so right now I can't write code which uses it.

 * Therefore we use type 0 accesses for now even though they won't work

 * correctly for PCI-to-PCI bridges.

 *

 * The function is complicated by the ultimate brokenness of the IOC3 chip

 * which is used in SGI systems.  The IOC3 can only handle 32-bit PCI

 * accesses and does only decode parts of it's address space.

	/*

	 * IOC3 is broken beyond belief ...  Don't even give the

	 * generic PCI code a chance to look at it for real ...

	/*

	 * IOC3 is broken beyond belief ...  Don't even give the

	 * generic PCI code a chance to look at it for real ...

	/*

	 * IOC3 is broken beyond belief ...  Don't even give the

	 * generic PCI code a chance to look at it for real ...

	/*

	 * IOC3 is broken beyond belief ...  Don't even give the

	 * generic PCI code a chance to look at it for real ...

 more stuff in int_enable */

	/*

	 * Enable sending of an interrupt clear packet to the hub on a high to

	 * low transition of the interrupt pin.

	 *

	 * IRIX sets additional bits in the address which are documented as

	 * reserved in the bridge docs.

	/*

	 * We assume the bridge to have a 1:1 mapping between devices

	 * (slots) and intr pins.

/*

 * All observed requests have pin == 1. We could have a global here, that

 * gets incremented and returned every time - unfortunately, pci_map_irq

 * may be called on the same device over and over, and need to return the

 * same value. On O2000, pin can be 0 or 1, and PCI slots can be [0..7].

 *

 * A given PCI device, in general, should be able to intr any of the cpus

 * on any one of the hubs connected to its xbow.

 Assemble part number */

 get part number from one wire prom */

 not available yet */

	/*

	 * Clear all pending interrupts.

	/*

	 * Until otherwise set up, assume all interrupts are from slot 0

	/*

	 * disable swapping for big windows

 16kB or larger */

	/*

	 * Hmm...  IRIX sets additional bits in the address which

	 * are documented as reserved in the bridge docs.

 DMA */

 default interrupt pin mapping */

 wait until Bridge PIO complete */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Ralink MT7620A SoC PCI support

 *

 *  Copyright (C) 2007-2013 Bruce Chang (Mediatek)

 *  Copyright (C) 2013-2016 John Crispin <john@phrozen.org>

 PCI Bridge registers */

 PCI RC registers */

 bypass PCIe DLL */

 Elastic buffer control */

 put core into reset */

 disable power and all clocks */

 bring core out of reset */

 power up the bus */

 bring the core out of reset */

 enable the pci clk */

 voodoo from the SDK driver */

 bring up the pci core */

 enable write access */

 check if there is a card present */

 setup ranges */

 enable interrupts */

 voodoo from the SDK driver */

 configure the cache line size to 0x14 */

 configure latency timer to 0xff */

 setup the slot */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2003 Christoph Hellwig (hch@lst.de)

 * Copyright (C) 1999, 2000, 04 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 CONFIG_NUMA */

 only needed on second module */

 enable ethernet PHY on IP29 systemboard */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * SNI specific PCI support for RM200/RM300.

 *

 * Copyright (C) 1997 - 2000, 2003, 04 Ralf Baechle (ralf@linux-mips.org)

/*

 * PCIMT Shortcuts ...

/*

 * Device 0: PCI EISA Bridge	(directly routed)

 * Device 1: NCR53c810 SCSI	(directly routed)

 * Device 2: PCnet32 Ethernet	(directly routed)

 * Device 3: VGA		(routed to INTB)

 * Device 4: Unused

 * Device 5: Slot 2

 * Device 6: Slot 3

 * Device 7: Slot 4

 *

 * Documentation says the VGA is device 5 and device 3 is unused but that

 * seem to be a documentation error.  At least on my RM200C the Cirrus

 * Logic CL-GD5434 VGA is device 3.

	 INTA  INTB  INTC  INTD */

 EISA bridge */

 SCSI */

 Ethernet */

 VGA */

 Unused */

 Slot 2 */

 Slot 3 */

 Slot 4 */

/*

 * In Revision D of the RM300 Device 2 has become a normal purpose Slot 1

 *

 * The VGA card is optional for RM300 systems.

	 INTA  INTB  INTC  INTD */

 EISA bridge */

 SCSI */

 Slot 1 */

 VGA */

 Unused */

 Slot 2 */

 Slot 3 */

 Slot 4 */

	 INTA  INTB  INTC  INTD */

 HOST bridge */

 SCSI */

 Bridge/i960 */

 Slot 1 */

 Slot 2 */

/*

 * PCIT Shortcuts ...

	 INTA  INTB  INTC  INTD */

 HOST bridge */

 SCSI */

 SCSI */

 Ethernet */

 PCI-PCI bridge */

 Unused */

 Unused */

 Unused */

 Slot 1 */

 Slot 2 */

 Slot 3 */

 Slot 4 */

 Slot 5 */

	 INTA  INTB  INTC  INTD */

 HOST bridge */

 PCI Slot 9 */

 PCI-EISA */

 Unused */

 PCI-PCI bridge */

 fixup */

			/*

			 * SNI messed up interrupt wiring for onboard

			 * PCI bus 1; we need to fix this up here

 Do platform specific device initialization at pci_enable_device() time */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2000, 2001 Keith M Wesolowski

/*

 * O2 has up to 5 PCI devices connected into the MACE bridge.  The device

 * map looks like this:

 *

 * 0  aic7xxx 0

 * 1  aic7xxx 1

 * 2  expansion slot

 * 3  N/C

 * 4  N/C

 disable master aborts interrupts during config read */

 ack possible master abort */

	/*

	 * someone forgot to set the ultra bit for the onboard

	 * scsi chips; we fake it here

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2003, 04, 11 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2011 Wind River Systems,

 *   written by Ralf Baechle (ralf@linux-mips.org)

	/*

	 * Set PCI cacheline size to that of the highest level in the

	 * cache hierarchy.

/*

 * Cobalt Qube/Raq PCI support

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 1995, 1996, 1997, 2002, 2003 by Ralf Baechle

 * Copyright (C) 2001, 2002, 2003 by Liam Davies (ldavies@agile.tv)

/*

 * PCI slot numbers

/*

 * The Cobalt board ID information.  The boards have an ID number wired

 * into the VIA that is available in the high nibble of register 94.

/*

 * Default value of PCI Class Code on GT64111 is PCI_CLASS_MEMORY_OTHER (0x0580)

 * instead of PCI_CLASS_BRIDGE_HOST (0x0600). Galileo explained this choice in

 * document "GT-64111 System Controller for RC4640, RM523X and VR4300 CPUs",

 * section "6.5.3 PCI Autoconfiguration at RESET":

 *

 *   Some PCs refuse to configure host bridges if they are found plugged into

 *   a PCI slot (ask the BIOS vendors why...). The "Memory Controller" Class

 *   Code does not cause a problem for these non-compliant BIOSes, so we used

 *   this as the default in the GT-64111.

 *

 * So fix the incorrect default value of PCI Class Code. More details are on:

 * https://lore.kernel.org/r/20211102154831.xtrlgrmrizl5eidl@pali/

 * https://lore.kernel.org/r/20211102150201.GA11675@alpha.franken.de/

 Enable Bus Mastering and fast back to back. */

 Enable both ide interfaces. ROM only enables primary one.  */

 Set latency timer to reasonable value. */

	/* Fix PCI latency-timer and cache-line-size values in Galileo

	 * host bridge.

	/*

	 * The code described by the comment below has been removed

	 * as it causes bus mastering by the Ethernet controllers

	 * to break under any kind of network load. We always set

	 * the retry timeouts to their maximum.

	 *

	 * --x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--x--

	 *

	 * On all machines prior to Q2, we had the STOP line disconnected

	 * from Galileo to VIA on PCI.	The new Galileo does not function

	 * correctly unless we have it connected.

	 *

	 * Therefore we must set the disconnect/retry cycle values to

	 * something sensible when using the new Galileo.

 New Galileo, assumes PCI stop line to VIA is connected. */

 XXX WE MUST DO THIS ELSE GALILEO LOCKS UP! -DaveM */

 Old Galileo, assumes PCI STOP line to VIA is disconnected. */

 retry count */

 timeout 1   */

 timeout 0   */

 enable PCI retry exceeded interrupt */

 Do platform specific device initialization at pci_enable_device() time */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000, 2004, 2005	 MIPS Technologies, Inc.

 *    All rights reserved.

 *    Authors: Carsten Langgaard <carstenl@mips.com>

 *	       Maciej W. Rozycki <macro@mips.com>

 * Copyright (C) 2005 Ralf Baechle (ralf@linux-mips.org)

 *

 * MIPS boards specific PCI support.

/*

 *  PCI configuration cycle AD bus definition

 Type 0 */

 Type 1 */

 Clear status register bits. */

 Perform access */

 Detect Master/Target abort */

 Error occurred */

 Clear bits */

/*

 * We can't address 8 and 16 bit words directly.  Instead we have to

 * read/write a 32bit word and mask/modify the data we actually want.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2018 John Crispin <john@phrozen.org>

#include <linux/of_irq.h>

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *         	stevel@mvista.com or source@mvista.com

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 disable prefetched memory range */

/*

 * The fixup applies to both the IDT and VIA devices present on the board

 Do platform specific device initialization at pci_enable_device() time */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * Allow PCI to be disabled at runtime depending on board nvram

 * configuration

/*

 * We handle cardbus  via a fake Cardbus bridge,  memory and io spaces

 * have to be  clearly separated from PCI one  since we have different

 * memory decoder.

 enable SERDES */

 reset the PCIe core */

 enable clock */

 configure the PCIe bridge */

 setup the interrupts */

 enable credit checking and error checking */

 set device bus/func for the pcie device */

 setup class code as bridge */

 disable bar1 size */

 set bar0 to little endian */

	/*

	 * configuration  access are  done through  IO space,  remap 4

	 * first bytes to access it from CPU.

	 *

	 * this means that  no io access from CPU  should happen while

	 * we do a configuration cycle,	 but there's no way we can add

	 * a spinlock for each io access, so this is currently kind of

	 * broken on SMP.

 setup local bus to PCI access (PCI memory) */

	/* set Cardbus IDSEL (type 0 cfg access on primary bus for

 setup local bus to PCI access (Cardbus memory) */

 disable second access windows */

	/* setup local bus  to PCI access (IO memory),	we have only 1

	 * IO window  for both PCI  and cardbus, but it	 cannot handle

	 * both	 at the	 same time,  assume standard  PCI for  now, if

	 * cardbus card has  IO zone, PCI fixup will  change window to

 enable PCI related GPIO pins */

	/* setup PCI to local bus access, used by PCI device to target

	/* 6348 before rev b0 exposes only 16 MB of RAM memory through

 setup sp0 range to local RAM size */

	/* change  host bridge	retry  counter to  infinite number  of

	 * retry,  needed for  some broadcom  wifi cards  with Silicon

 enable memory decoder and bus mastering */

	/* enable read prefetching & disable byte swapping for bus

 enable pci interrupt */

 mark memory space used for IO mapping as reserved */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2007, 2008, 2009, 2010, 2011 Cavium Networks

 128 byte Max Read Request Size */

 128 byte Max Packet Size (Limit of most PCs) */

 1024 byte Max Read Request Size */

 128 byte Max Packet Size (Limit of most PCs) */

 Module parameter to disable PCI probing */

 Normally 2 for XKPHYS */

 Must be zero */

 1 for IO space access */

 PCIe DID = 3 */

 PCIe SubDID = 1 */

 Must be zero */

 Endian swap = 1 */

 PCIe port 0,1 */

 Must be zero */

		/*

		 * Selects the type of the configuration request (0 = type 0,

		 * 1 = type 1).

 Target bus number sent in the ID in the request. */

		/*

		 * Target device number sent in the ID in the

		 * request. Note that Dev must be zero for type 0

		 * configuration requests.

 Target function number sent in the ID in the request. */

		/*

		 * Selects a register in the configuration space of

		 * the target.

 Normally 2 for XKPHYS */

 Must be zero */

 1 for IO space access */

 PCIe DID = 3 */

 PCIe SubDID = 2 */

 Must be zero */

 Endian swap = 1 */

 PCIe port 0,1 */

 PCIe IO address */

 Normally 2 for XKPHYS */

 Must be zero */

 1 for IO space access */

 PCIe DID = 3 */

 PCIe SubDID = 3-6 */

 Must be zero */

 PCIe Mem address */

/**

 * Return the Core virtual base address for PCIe IO access. IOs are

 * read/written as an offset from this address.

 *

 * @pcie_port: PCIe port the IO is for

 *

 * Returns 64bit Octeon IO base address for read/write

/**

 * Size of the IO address region returned at address

 * cvmx_pcie_get_io_base_address()

 *

 * @pcie_port: PCIe port the IO is for

 *

 * Returns Size of the IO window

/**

 * Return the Core virtual base address for PCIe MEM access. Memory is

 * read/written as an offset from this address.

 *

 * @pcie_port: PCIe port the IO is for

 *

 * Returns 64bit Octeon IO base address for read/write

/**

 * Size of the Mem address region returned at address

 * cvmx_pcie_get_mem_base_address()

 *

 * @pcie_port: PCIe port the IO is for

 *

 * Returns Size of the Mem window

/**

 * Read a PCIe config space register indirectly. This is used for

 * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.

 *

 * @pcie_port:	PCIe port to read from

 * @cfg_offset: Address to read

 *

 * Returns Value read

/**

 * Write a PCIe config space register indirectly. This is used for

 * registers of the form PCIEEP_CFG??? and PCIERC?_CFG???.

 *

 * @pcie_port:	PCIe port to write to

 * @cfg_offset: Address to write

 * @val:	Value to write

/**

 * Build a PCIe config space request address for a device

 *

 * @pcie_port: PCIe port to access

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 *

 * Returns 64bit Octeon IO address

/**

 * Read 8bits from a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 *

 * Returns Result of the read

/**

 * Read 16bits from a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 *

 * Returns Result of the read

/**

 * Read 32bits from a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 *

 * Returns Result of the read

/**

 * Write 8bits to a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 * @val:       Value to write

/**

 * Write 16bits to a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 * @val:       Value to write

/**

 * Write 32bits to a Device's config space

 *

 * @pcie_port: PCIe port the device is on

 * @bus:       Sub bus

 * @dev:       Device ID

 * @fn:	       Device sub function

 * @reg:       Register to access

 * @val:       Value to write

/**

 * Initialize the RC config space CSRs

 *

 * @pcie_port: PCIe port to initialize

 Max Payload Size (PCIE*_CFG030[MPS]) */

 Max Read Request Size (PCIE*_CFG030[MRRS]) */

 Relaxed-order, no-snoop enables (PCIE*_CFG030[RO_EN,NS_EN] */

 Error Message Enables (PCIE*_CFG030[CE_EN,NFE_EN,FE_EN,UR_EN]) */

	/*

	 * Enable relaxed order processing. This will allow devices to

	 * affect read response ordering.

 Enable no snoop processing. Not used by Octeon */

 Correctable error reporting enable. */

 Non-fatal error reporting enable. */

 Fatal error reporting enable. */

 Unsupported request reporting enable. */

		/*

		 * Max Payload Size (NPEI_CTL_STATUS2[MPS]) must match

		 * PCIE*_CFG030[MPS].  Max Read Request Size

		 * (NPEI_CTL_STATUS2[MRRS]) must not exceed

		 * PCIE*_CFG030[MRRS]

 Max payload size = 128 bytes for best Octeon DMA performance */

 Max read request size = 128 bytes for best Octeon DMA performance */

 Port1 BAR1 Size 256MB */

 Port0 BAR1 Size 256MB */

		/*

		 * Max Payload Size (DPI_SLI_PRTX_CFG[MPS]) must match

		 * PCIE*_CFG030[MPS].  Max Read Request Size

		 * (DPI_SLI_PRTX_CFG[MRRS]) must not exceed

		 * PCIE*_CFG030[MRRS].

 Max outstanding load request. */

 ECRC Generation (PCIE*_CFG070[GE,CE]) */

 ECRC generation enable. */

 ECRC check enable. */

	/*

	 * Access Enables (PCIE*_CFG001[MSAE,ME])

	 * ME and MSAE should always be set.

	 * Interrupt Disable (PCIE*_CFG001[I_DIS])

	 * System Error Message Enable (PCIE*_CFG001[SEE])

 Memory space enable. */

 Bus master enable. */

 INTx assertion disable. */

 SERR# enable */

 Advanced Error Recovery Message Enables */

 (PCIE*_CFG066,PCIE*_CFG067,PCIE*_CFG069) */

 Use CVMX_PCIERCX_CFG067 hardware default */

 Active State Power Management (PCIE*_CFG032[ASLPC]) */

 Active state Link PM control. */

	/*

	 * Link Width Mode (PCIERCn_CFG452[LME]) - Set during

	 * cvmx_pcie_rc_initialize_link()

	 *

	 * Primary Bus Number (PCIERCn_CFG006[PBNUM])

	 *

	 * We set the primary bus number to 1 so IDT bridges are

	 * happy. They don't like zero.

	/*

	 * Memory-mapped I/O BAR (PCIERCn_CFG008)

	 * Most applications should disable the memory-mapped I/O BAR by

	 * setting PCIERCn_CFG008[ML_ADDR] < PCIERCn_CFG008[MB_ADDR]

	/*

	 * Prefetchable BAR (PCIERCn_CFG009,PCIERCn_CFG010,PCIERCn_CFG011)

	 * Most applications should disable the prefetchable BAR by setting

	 * PCIERCn_CFG011[UMEM_LIMIT],PCIERCn_CFG009[LMEM_LIMIT] <

	 * PCIERCn_CFG010[UMEM_BASE],PCIERCn_CFG009[LMEM_BASE]

	/*

	 * System Error Interrupt Enables (PCIERCn_CFG035[SECEE,SEFEE,SENFEE])

	 * PME Interrupt Enables (PCIERCn_CFG035[PMEIE])

 System error on correctable error enable. */

 System error on fatal error enable. */

 System error on non-fatal error enable. */

 PME interrupt enable. */

	/*

	 * Advanced Error Recovery Interrupt Enables

	 * (PCIERCn_CFG075[CERE,NFERE,FERE])

 Correctable error reporting enable. */

 Non-fatal error reporting enable. */

 Fatal error reporting enable. */

	/*

	 * HP Interrupt Enables (PCIERCn_CFG034[HPINT_EN],

	 * PCIERCn_CFG034[DLLS_EN,CCINT_EN])

 Hot-plug interrupt enable. */

 Data Link Layer state changed enable */

 Command completed interrupt enable. */

/**

 * Initialize a host mode PCIe gen 1 link. This function takes a PCIe

 * port from reset to a link up state. Software can then begin

 * configuring the rest of the link.

 *

 * @pcie_port: PCIe port to initialize

 *

 * Returns Zero on success

 Set the lane width */

 We're in 8 lane (56XX) or 4 lane (54XX) mode */

 We're in 4 lane (56XX) or 2 lane (52XX) mode */

	/*

	 * CN52XX pass 1.x has an errata where length mismatches on UR

	 * responses can cause bus errors on 64bit memory

	 * reads. Turning off length error checking fixes this.

 Lane swap needs to be manually enabled for CN52XX */

 Bring up the link */

	/*

	 * CN52XX pass 1.0: Due to a bug in 2nd order CDR, it needs to

	 * be disabled.

 Wait for the link to come up */

 Clear all pending errors */

	/*

	 * Update the Replay Time Limit. Empirically, some PCIe

	 * devices take a little longer to respond than expected under

	 * load. As a workaround for this we configure the Replay Time

	 * Limit to the value expected for a 512 byte MPS instead of

	 * our actual 256 byte MPS. The numbers below are directly

	 * from the PCIe spec table 3-4.

 1 lane */

 2 lanes */

 4 lanes */

 8 lanes */

/**

 * Initialize a PCIe gen 1 port for use in host(RC) mode. It doesn't

 * enumerate the bus.

 *

 * @pcie_port: PCIe port to initialize

 *

 * Returns Zero on success

	/*

	 * Make sure we aren't trying to setup a target mode interface

	 * in host mode.

	/*

	 * Make sure a CN52XX isn't trying to bring up port 1 when it

	 * is disabled.

	/*

	 * PCIe switch arbitration mode. '0' == fixed priority NPEI,

	 * PCIe0, then PCIe1. '1' == round robin.

 Allow up to 0x20 config retries */

	/*

	 * CN52XX pass1.x has an errata where P0_NTAGS and P1_NTAGS

	 * don't reset.

 Bring the PCIe out of reset */

		/*

		 * The EBH5200 board swapped the PCIe reset lines on

		 * the board. As a workaround for this bug, we bring

		 * both PCIe ports out of reset at the same time

		 * instead of on separate calls. So for port 0, we

		 * bring both out of reset and do nothing on port 1

			/*

			 * After a chip reset the PCIe will also be in

			 * reset. If it isn't, most likely someone is

			 * trying to init it again without a proper

			 * PCIe reset.

 Reset the ports */

 Wait until pcie resets the ports. */

		/*

		 * The normal case: The PCIe ports are completely

		 * separate and can be brought out of reset

		 * independently.

		/*

		 * After a chip reset the PCIe will also be in

		 * reset. If it isn't, most likely someone is trying

		 * to init it again without a proper PCIe reset.

 Reset the port */

 Wait until pcie resets the ports. */

	/*

	 * Wait for PCIe reset to complete. Due to errata PCIE-700, we

	 * don't poll PESCX_CTL_STATUS2[PCIERST], but simply wait a

	 * fixed number of cycles.

	/*

	 * PESCX_BIST_STATUS2[PCLK_RUN] was missing on pass 1 of

	 * CN56XX and CN52XX, so we only probe it on newer chips

 Clear PCLK_RUN so we can check if the clock is running */

		/* Now that we cleared PCLK_RUN, wait for it to be set

		 * again telling us the clock is running

	/*

	 * Check and make sure PCIe came out of reset. If it doesn't

	 * the board probably hasn't wired the clocks up and the

	 * interface should be skipped.

	/*

	 * Check BIST2 status. If any bits are set skip this

	 * interface. This is an attempt to catch PCIE-813 on pass 1

	 * parts.

 Check BIST status */

 Initialize the config space CSRs */

 Bring the link up */

 Store merge control (NPEI_MEM_ACCESS_CTL[TIMER,MAX_WORD]) */

 Allow 16 words to combine */

 Wait up to 127 cycles for more data */

 Setup Mem access SubDIDs */

 Port the request is sent to. */

 Due to an errata on pass 1 chips, no merging is allowed. */

 Endian-swap for Reads. */

 Endian-swap for Writes. */

 Enable Snooping for Reads. Octeon doesn't care, but devices might want this more conservative setting */

 Enable Snoop for Writes. */

 Disable Relaxed Ordering for Reads. */

 Disable Relaxed Ordering for Writes. */

 PCIe Adddress Bits <63:34>. */

	/*

	 * Setup mem access 12-15 for port 0, 16-19 for port 1,

	 * supplying 36 bits of address space.

 Set each SUBID to extend the addressable range */

	/*

	 * Disable the peer to peer forwarding register. This must be

	 * setup by the OS after it enumerates the bus and assigns

	 * addresses to the PCIe busses.

 Set Octeon's BAR0 to decode 0-16KB. It overlaps with Bar2 */

 BAR1 follows BAR2 with a gap so it has the same address as for gen2. */

 Not Cached */

 Endian Swap mode */

 Valid entry */

 Big endian swizzle for 32-bit PEXP_NCB register. */

 256MB / 16 >> 22 == 4 */

	/*

	 * Set Octeon's BAR2 to decode 0-2^39. Bar0 and Bar1 take

	 * precedence where they overlap. It also overlaps with the

	 * device addresses, so make sure the peer to peer forwarding

	 * is set right.

	/*

	 * Setup BAR2 attributes

	 *

	 * Relaxed Ordering (NPEI_CTL_PORTn[PTLP_RO,CTLP_RO, WAIT_COM])

	 * - PTLP_RO,CTLP_RO should normally be set (except for debug).

	 * - WAIT_COM=0 will likely work for all applications.

	 *

	 * Load completion relaxed ordering (NPEI_CTL_PORTn[WAITL_COM]).

	/*

	 * Both pass 1 and pass 2 of CN52XX and CN56XX have an errata

	 * that causes TLP ordering to not be preserved after multiple

	 * PCIe port resets. This code detects this fault and corrects

	 * it by aligning the TLP counters properly. Another link

	 * reset is then performed. See PCIE-13340

		/*

		 * Choose a write address of 1MB. It should be

		 * harmless as all bars haven't been setup.

		/*

		 * Make sure at least in_p_offset have been executed before we try and

		 * read in_fif_p_count

		/*

		 * Read the IN_FIF_P_COUNT from the debug

		 * select. IN_FIF_P_COUNT can be unstable sometimes so

		 * read it twice with a write between the reads.  This

		 * way we can tell the value is good as it will

		 * increment by one due to the write

 Update in_fif_p_count for it's offset with respect to out_p_count */

 Read the OUT_P_COUNT from the debug select */

 Check that the two counters are aligned */

			/*

			 * The EBH5200 board swapped the PCIe reset

			 * lines on the board. This means we must

			 * bring both links down and up, which will

			 * cause the PCIe0 to need alignment

			 * again. Lots of messages will be displayed,

			 * but everything should work

 Rety bringing this port up */

 Display the link status */

/**

  * Initialize a host mode PCIe gen 2 link. This function takes a PCIe

 * port from reset to a link up state. Software can then begin

 * configuring the rest of the link.

 *

 * @pcie_port: PCIe port to initialize

 *

 * Return Zero on success.

 Bring up the link */

 Wait for the link to come up */

	/*

	 * Update the Replay Time Limit. Empirically, some PCIe

	 * devices take a little longer to respond than expected under

	 * load. As a workaround for this we configure the Replay Time

	 * Limit to the value expected for a 512 byte MPS instead of

	 * our actual 256 byte MPS. The numbers below are directly

	 * from the PCIe spec table 3-4

 1 lane */

 2 lanes */

 4 lanes */

 8 lanes */

/**

 * Initialize a PCIe gen 2 port for use in host(RC) mode. It doesn't enumerate

 * the bus.

 *

 * @pcie_port: PCIe port to initialize

 *

 * Returns Zero on success.

 Make sure this interface isn't SRIO */

			/*

			 * The CN66XX requires reading the

			 * MIO_QLMX_CFG register to figure out the

			 * port type.

 SRIO 1x4 short */

 SRIO 1x4 long */

 SRIO 2x2 short */

 SRIO 2x2 long */

 SGMII */

 XAUI */

 PCIE gen2 */

 PCIE gen2 (alias) */

 PCIE gen1 */

 PCIE gen1 (alias) */

 This code is so that the PCIe analyzer is able to see 63XX traffic */

 Make sure we aren't trying to setup a target mode interface in host mode */

 CN63XX Pass 1.0 errata G-14395 requires the QLM De-emphasis be programmed */

 Bring the PCIe out of reset */

	/*

	 * After a chip reset the PCIe will also be in reset. If it

	 * isn't, most likely someone is trying to init it again

	 * without a proper PCIe reset

 Reset the port */

 Wait until pcie resets the ports. */

 Wait for PCIe reset to complete */

	/*

	 * Check and make sure PCIe came out of reset. If it doesn't

	 * the board probably hasn't wired the clocks up and the

	 * interface should be skipped.

 Check BIST status */

 Errata PCIE-14766 may cause the lower 6 bits to be randomly set on CN63XXp1 */

 Initialize the config space CSRs */

 Enable gen2 speed selection */

 Bring the link up */

		/*

		 * Some gen1 devices don't handle the gen 2 training

		 * correctly. Disable gen2 and try again with only

		 * gen1

 Store merge control (SLI_MEM_ACCESS_CTL[TIMER,MAX_WORD]) */

 Allow 16 words to combine */

 Wait up to 127 cycles for more data */

 Setup Mem access SubDIDs */

 Port the request is sent to. */

 Allow merging as it works on CN6XXX. */

 Endian-swap for Reads. */

 Endian-swap for Writes. */

 "No snoop" and "Relaxed ordering" are not set */

 "No snoop" and "Relaxed ordering" are not set */

 PCIe Adddress Bits <63:34>. */

	/*

	 * Setup mem access 12-15 for port 0, 16-19 for port 1,

	 * supplying 36 bits of address space.

 Set each SUBID to extend the addressable range */

	/*

	 * Disable the peer to peer forwarding register. This must be

	 * setup by the OS after it enumerates the bus and assigns

	 * addresses to the PCIe busses.

 Set Octeon's BAR0 to decode 0-16KB. It overlaps with Bar2 */

	/*

	 * Set Octeon's BAR2 to decode 0-2^41. Bar0 and Bar1 take

	 * precedence where they overlap. It also overlaps with the

	 * device addresses, so make sure the peer to peer forwarding

	 * is set right.

	/*

	 * Setup BAR2 attributes

	 * Relaxed Ordering (NPEI_CTL_PORTn[PTLP_RO,CTLP_RO, WAIT_COM])

	 * - PTLP_RO,CTLP_RO should normally be set (except for debug).

	 * - WAIT_COM=0 will likely work for all applications.

	 * Load completion relaxed ordering (NPEI_CTL_PORTn[WAITL_COM])

 256MB BAR1*/

 BAR1 follows BAR2 */

 Not Cached */

 Endian Swap mode */

 Valid entry */

 256MB / 16 >> 22 == 4 */

	/*

	 * Allow config retries for 250ms. Count is based off the 5Ghz

	 * SERDES clock.

 Display the link status */

/**

 * Initialize a PCIe port for use in host(RC) mode. It doesn't enumerate the bus.

 *

 * @pcie_port: PCIe port to initialize

 *

 * Returns Zero on success

 Above was cvmx-pcie.c, below original pcie.c */

/**

 * Map a PCI device to the appropriate interrupt line

 *

 * @dev:    The Linux PCI device structure for the device to map

 * @slot:   The slot number for this device on __BUS 0__. Linux

 *		 enumerates through all the bridges and figures out the

 *		 slot on Bus 0 where this device eventually hooks to.

 * @pin:    The PCI interrupt pin read from the device, then swizzled

 *		 as it goes through each bridge.

 * Returns Interrupt number for the device

	/*

	 * The EBH5600 board with the PCI to PCIe bridge mistakenly

	 * wires the first slot for both device id 2 and interrupt

	 * A. According to the PCI spec, device id 2 should be C. The

	 * following kludge attempts to fix this.

		/*

		 * Iterate all the way up the device chain and find

		 * the root bus.

		/*

		 * If the root bus is number 0 and the PEX 8114 is the

		 * root, assume we are behind the miswired bus. We

		 * need to correct the swizzle level by two. Yuck.

			/*

			 * The pin field is one based, not zero. We

			 * need to swizzle it by minus two.

	/*

	 * The -1 is because pin starts with one, not zero. It might

	 * be that this equation needs to include the slot number, but

	 * I don't have hardware to check that against.

/*

 * Read a value from configuration space

 *

	/*

	 * For the top level bus make sure our hardware bus number

	 * matches the software one

	/*

	 * PCIe only has a single device connected to Octeon. It is

	 * always device ID 0. Don't bother doing reads for other

	 * device IDs on the first segment.

	/*

	 * The following is a workaround for the CN57XX, CN56XX,

	 * CN55XX, and CN54XX errata with PCIe config reads from non

	 * existent devices.  These chips will hang the PCIe link if a

	 * config read is performed that causes a UR response.

		/*

		 * For our EBH5600 board, port 0 has a bridge with two

		 * PCI-X slots. We need a new special checks to make

		 * sure we only probe valid stuff.  The PCIe->PCI-X

		 * bridge only respondes to device ID 0, function

		 * 0-1

		/*

		 * The PCI-X slots are device ID 2,3. Choose one of

		 * the below "if" blocks based on what is plugged into

		 * the board.

 Use this option if you aren't using either slot */

		/*

		 * Use this option if you are using the first slot but

		 * not the second.

		/*

		 * Use this option if you are using the second slot

		 * but not the first.

 Use this opion if you are using both slots */

		/* The following #if gives a more complicated example. This is

		   the required checks for running a Nitrox CN16XX-NHBX in the

		   slot of the EBH5600. This card has a PLX PCIe bridge with

 PLX bridge with 4 ports */

 Nitrox behind PLX 1 */

 Nitrox behind PLX 2 */

 Nitrox behind PLX 3 */

 Nitrox behind PLX 4 */

		/*

		 * Shorten the DID timeout so bus errors for PCIe

		 * config reads from non existent devices happen

		 * faster. This allows us to continue booting even if

		 * the above "if" checks are wrong.  Once one of these

		 * errors happens, the PCIe port is dead.

/*

 * Write a value to PCI configuration space

/**

 * Initialize the Octeon PCIe controllers

 *

 * Returns

 These chips don't have PCIe */

 No PCIe simulation */

 Disable PCI if instructed on the command line */

 Point pcibios_map_irq() to the PCIe version of it */

	/*

	 * PCIe I/O range. It is based on port 0 but includes up until

	 * port 1's end.

	/*

	 * Create a dummy PCIe controller to swallow up bus 0. IDT bridges

	 * don't work if the primary bus number is zero. Here we add a fake

	 * PCIe controller that the kernel will give bus 0. This allows

	 * us to not change the normal kernel bus enumeration

 CN63XX pass 1_x/2.0 errata PCIe-15205 */

 Port is SRIO */

 Memory offsets are physical addresses */

 IO offsets are Mips virtual addresses */

			/*

			 * To keep things similar to PCI, we start

			 * device addresses at the same place as PCI

			 * uisng big bar support. This normally

			 * translates to 4GB-256MB, which is the same

			 * as most x86 PCs.

			/*

			 * Ports must be above 16KB for the ISA bus

			 * filtering in the PCI-X to PCI bridge.

 Some devices need extra time */

 CN63XX pass 1_x/2.0 errata PCIe-15205 */

 Skip the 2nd port on CN52XX if port 0 is in 4 lane mode */

 CN63XX pass 1_x/2.0 errata PCIe-15205 */

 Port is SRIO */

 Memory offsets are physical addresses */

			/*

			 * To calculate the address for accessing the 2nd PCIe device,

			 * either 'io_map_base' (pci_iomap()), or 'mips_io_port_base'

			 * (ioport_map()) value is added to

			 * pci_resource_start(dev,bar)). The 'mips_io_port_base' is set

			 * only once based on first PCIe. Also changing 'io_map_base'

			 * based on first slot's value so that both the routines will

			 * work properly.

 IO offsets are Mips virtual addresses */

			/*

			 * To keep things similar to PCI, we start device

			 * addresses at the same place as PCI uisng big bar

			 * support. This normally translates to 4GB-256MB,

			 * which is the same as most x86 PCs.

			/*

			 * Ports must be above 16KB for the ISA bus filtering

			 * in the PCI-X to PCI bridge.

 Some devices need extra time */

 CN63XX pass 1_x/2.0 errata PCIe-15205  */

	/*

	 * CN63XX pass 1_x/2.0 errata PCIe-15205 requires setting all

	 * of SRIO MACs SLI_CTL_PORT*[INT*_MAP] to similar value and

	 * all of PCIe Macs SLI_CTL_PORT*[INT*_MAP] to different value

	 * from the previous set values

 SPDX-License-Identifier: GPL-2.0

/*

 * Alchemy PCI host mode support.

 *

 * Copyright 2001-2003, 2007-2008 MontaVista Software Inc.

 * Author: MontaVista Software, Inc. <source@mvista.com>

 *

 * Support for all devices (greater than 16) added by David Gathright.

 for dma_default_coherent */

 leave as first member! */

 ctrl base */

 tools for wired entry for config space access */

/* for syscore_ops. There's only one PCI controller on Alchemy chips, so this

 * should suffice for now.

/* IO/MEM resources for PCI. Keep the memres in sync with fixup_bigphys_addr

 * in arch/mips/alchemy/common/setup.c

 Save old context and create impossible VPN2 value */

	/* Allow board vendors to implement their own off-chip IDSEL.

	 * If it doesn't succeed, may as well bail out at this point.

 Setup the config window */

 Setup the lower bits of the 36-bit address */

 Pick up any address that falls below the page mask */

 Page boundary */

	/* To improve performance, if the current device is the same as

	 * the last device accessed, we don't touch the TLB.

 check for errors, master abort */

 clear errors */

 Take away the IDSEL. */

 success */

 save PCI controller register contents. */

	/* YAMON on all db1xxx boards wipes the TLB and writes zero to C0_wired

	 * on resume, making it necessary to recreate it as soon as possible.

 impossibly high value */

 install it */

 need at least PCI IRQ mapping table */

 map parts of the PCI IO area */

	/* REVISIT: if this changes with a newer variant (doubt it) make this

	 * a platform resource.

 Au1500 revisions older than AD have borked coherent PCI */

 fill in relevant pci_controller members */

	/* we can't ioremap the entire pci config space because it's too large,

	 * nor can we dynamically ioremap it because some drivers use the

	 * PCI config routines from within atomic contex and that becomes a

	 * problem in get_vm_area().  Instead we use one wired TLB entry to

	 * handle all config accesses for all busses.

 impossibly high value */

 install it */

 board may want to modify bits in the config register, do it now */

 clear disable bit */

 Au1500/Au1550 have PCI */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 *  Copyright (C) 2010 John Crispin <john@phrozen.org>

	/* we support slot from 0 to 15 dev_fn & 0x68 (AD29) is the

 Perform access */

 clean possible Master abort */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000, 2004  MIPS Technologies, Inc.

 *	All rights reserved.

 *	Authors: Carsten Langgaard <carstenl@mips.com>

 *		 Maciej W. Rozycki <macro@mips.com>

/*

 *  PCI configuration cycle AD bus definition

 Type 0 */

 Type 1 */

 Because of a bug in the galileo (for slot 31). */

 Clear cause register bits */

 Setup address */

			/*

			 * The Galileo system controller is acting

			 * differently than other devices.

			/*

			 * The Galileo system controller is acting

			 * differently than other devices.

 Check for master or target abort */

 Error occurred */

 Clear bits */

/*

 * We can't address 8 and 16 bit words directly.  Instead we have to

 * read/write a 32bit word and mask/modify the data we actually want.

/*

 * Based on linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * Copyright 2001, 2003-2005 MontaVista Software Inc.

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Assert M66EN */

 Double PCICLK (if possible) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 * Copyright (C) 2003, 04, 11 Ralf Baechle (ralf@linux-mips.org)

 * Copyright (C) 2011 Wind River Systems,

 *   written by Ralf Baechle (ralf@linux-mips.org)

/*

 * If PCI_PROBE_ONLY in pci_flags is set, we don't change any PCI resource

 * assignments.

/*

 * The PCI controller list.

/*

 * We need to avoid collisions with `mirrored' VGA ports

 * and other strange ISA hardware, so we always want the

 * addresses to be allocated in the 0x000-0x0ff region

 * modulo 0x400.

 *

 * Why? Because some silly external IO cards only decode

 * the low 10 bits of the IO address. The 0x00-0xff region

 * is reserved for motherboard devices that decode all 16

 * bits, so it's ok to allocate at, say, 0x2800-0x28ff,

 * but we want to try to avoid allocating at 0x2900-0x2bff

 * which might have be mirrored at 0x0100-0x03ff..

 Make sure we start at our min on all hoses */

		/*

		 * Put everything into 0x00-0xff region modulo 0x400

 Make sure we start at our min on all hoses */

	/* Don't allow 8-bit bus number overflow inside the hose -

	/*

	 * We insert PCI resources into the iomem_resource and

	 * ioport_resource trees in either pci_bus_claim_resources()

	 * or pci_bus_assign_resources().

	/*

	 * Do not panic here but later - this might happen before console init.

	/*

	 * Scan the bus if it is register after the PCI subsystem

	 * initialization.

 Scan all of the recorded PCI controllers.  */

 Only set up the requested stuff */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005-2009, 2010 Cavium Networks

/*

 * Each bit in msi_free_irq_bitmask represents a MSI interrupt that is

 * in use.

/*

 * Each bit in msi_multiple_irq_bitmask tells that the device using

 * this bit in msi_free_irq_bitmask is also using the next bit. This

 * is used so we can disable all of the MSI interrupts when a device

 * uses multiple.

/*

 * This lock controls updates to msi_free_irq_bitmask and

 * msi_multiple_irq_bitmask.

/*

 * Number of MSI IRQs used. This variable is set up in

 * the module init time.

/**

 * Called when a driver request MSI interrupts instead of the

 * legacy INT A-D. This routine will allocate multiple interrupts

 * for MSI devices that support them. A device can override this by

 * programming the MSI control bits [6:4] before calling

 * pci_enable_msi().

 *

 * @dev:    Device requesting MSI interrupts

 * @desc:   MSI descriptor

 *

 * Returns 0 on success.

	/*

	 * Read the MSI config to figure out how many IRQs this device

	 * wants.  Most devices only want 1, which will give

	 * configured_private_bits and request_private_bits equal 0.

	/*

	 * If the number of private bits has been configured then use

	 * that value instead of the requested number. This gives the

	 * driver the chance to override the number of interrupts

	 * before calling pci_enable_msi().

 Nothing is configured, so use the hardware requested size */

		/*

		 * Use the number of configured bits, assuming the

		 * driver wanted to override the hardware request

		 * value.

	/*

	 * The PCI 2.3 spec mandates that there are at most 32

	 * interrupts. If this device asks for more, only give it one.

	/*

	 * The IRQs have to be aligned on a power of two based on the

	 * number being requested.

 Mask with one bit for each IRQ */

	/*

	 * We're going to search msi_free_irq_bitmask_lock for zero

	 * bits. This represents an MSI interrupt number that isn't in

	 * use.

 Make sure the search for available interrupts didn't fail */

 MSI interrupts start at logical IRQ OCTEON_IRQ_MSI_BIT0 */

 When not using big bar, Bar 0 is based at 128MB */

 When using big bar, Bar 0 is based at 0 */

 When using PCIe, Bar 0 is based at 0 */

 FIXME CVMX_NPEI_MSI_RCV* other than 0? */

 When using PCIe2, Bar 0 is based at 0 */

 Update the number of IRQs the device has available to it */

	/*

	 * MSI-X is not supported.

	/*

	 * If an architecture wants to support multiple MSI, it needs to

	 * override arch_setup_msi_irqs()

/**

 * Called when a device no longer needs its MSI interrupts. All

 * MSI interrupts for the device are freed.

 *

 * @irq:    The devices first irq number. There may be multple in sequence.

	/*

	 * Count the number of IRQs we need to free by looking at the

	 * msi_multiple_irq_bitmask. Each bit set means that the next

	 * IRQ is also owned by this device.

 Mask with one bit for each IRQ */

 Shift the mask to the correct bit location */

 Checks are done, update the in use bitmask */

	/*

	 * Octeon PCI doesn't have the ability to mask/unmask MSI

	 * interrupts individually. Instead of masking/unmasking them

	 * in groups of 16, we simple assume MSI devices are well

	 * behaved. MSI interrupts are always enable and the ACK is

	 * assumed to be enough

 See comment in enable */

/*

 * Called by the interrupt handling code when an MSI interrupt

 * occurs.

 Acknowledge it first. */

/*

 * Create octeon_msi_interrupt{0-3} function body

/*

 * Initializes the MSI interrupt handling code

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  fixup-mpc30x.c, The Victor MP-C303/304 specific PCI fixups.

 *

 *  Copyright (C) 2002,2004  Yoichi Yuasa <yuasa@linux-mips.org>

 Do platform specific device initialization at pci_enable_device() time */

/*

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *              ahennessy@mvista.com

 *

 * Copyright (C) 2000-2001 Toshiba Corporation

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 *

 * Based on arch/mips/ddb5xxx/ddb5477/pci_ops.c

 *

 *     Define the pci_ops for TX3927.

 *

 * Much of the code is derived from the original DDB5074 port by

 * Geert Uytterhoeven <geert@linux-m68k.org>

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 clear M_ABORT and Disable M_ABORT Int. */

 flush write buffer */

 Disable External PCI Config. Access */

 LB->PCI mappings */

 PCI->LB mappings */

 Enable Direct mapping Address Space Decoder */

 Clear All Local Bus Status */

 Enable All Local Bus Interrupts */

 Clear All PCI Status Error */

 Enable All PCI Status Error Interrupts */

 PCIC Int => IRC IRQ10 */

 Target Control (per errata) */

 Enable Bus Arbiter */

 clear all pci errors */

/*

 *

 * BRIEF MODULE DESCRIPTION

 *	Board specific pci fixups.

 *

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *         	ppopov@mvista.com or source@mvista.com

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 IRQ rotation (PICMG) */

 0-3 */

 PCI CardSlot (IDSEL=A23, DevNu=12) */

 PCIA => PCIC (IDSEL=A23) */

 NOTE: JMR3927 JP1 must be set to OPEN */

 PCI CardSlot (IDSEL=A22, DevNu=11) */

 PCIA => PCIA (IDSEL=A22) */

 NOTE: JMR3927 JP1 must be set to OPEN */

 PCI Backplane */

 1-4 */

 Check OnBoard Ethernet (IDSEL=A24, DevNu=13) */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001,2002,2005 Broadcom Corporation

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * BCM1480/1455-specific HT support (looking like PCI)

 *

 * This module provides the glue between Linux's PCI subsystem

 * and the hardware.  We basically provide glue for accessing

 * configuration space, and set up the translation for I/O

 * space accesses.

 *

 * To access configuration space, we use ioremap.  In the 32-bit

 * kernel, this consumes either 4 or 8 page table pages, and 16MB of

 * kernel mapped memory.  Hopefully neither of these should be a huge

 * problem.

 *

/*

 * Macros for calculating offsets into config space given a device

 * structure or dev/fun/reg

/*

 * HT's level-sensitive interrupts require EOI, which is generated

 * through a 4MB memory-mapped region

/*

 * Read/write 32-bit values in config space.

/*

 * Some checks before doing config cycles:

 * In PCI Device Mode, hide everything on bus 0 except the LDT host

 * bridge.  Otherwise, access is controlled by bridge MasterEn bits.

/*

 * Read/write access functions for various sizes of values

 * in config space.  Return all 1's for disallowed accesses

 * for a kludgy but adequate simulation of master aborts.

 CFE doesn't always init all HT paths, so we always scan */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  fixup-tb0287.c, The TANBAC TB0287 specific PCI fixups.

 *

 *  Copyright (C) 2005	Yoichi Yuasa <yuasa@linux-mips.org>

 Do platform specific device initialization at pci_enable_device() time */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  pci-vr41xx.c, PCI Control Unit routines for the NEC VR4100 series.

 *

 *  Copyright (C) 2001-2003 MontaVista Software Inc.

 *    Author: Yoichi Yuasa <source@mvista.com>

 *  Copyright (C) 2004-2008  Yoichi Yuasa <yuasa@linux-mips.org>

 *  Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * Changes:

 *  MontaVista Software Inc. <source@mvista.com>

 *  - New creation, NEC VR4122 and VR4131 are supported.

 Disable PCI interrupt */

 Supply VTClock to PCIU */

 Dummy write, waiting for supply of VTClock. */

 Select PCI clock */

 Supply PCI clock by PCI bus */

 Clear bus error */

/*

 *

 * BRIEF MODULE DESCRIPTION

 *      Board specific pci fixups for the Toshiba rbtx4927

 *

 * Copyright 2001 MontaVista Software Inc.

 * Author: MontaVista Software, Inc.

 *              ppopov@mvista.com or source@mvista.com

 *

 * Copyright (C) 2000-2001 Toshiba Corporation

 *

 * Copyright (C) 2004 MontaVista Software Inc.

 * Author: Manish Lachwani (mlachwani@mvista.com)

 *

 *  This program is free software; you can redistribute  it and/or modify it

 *  under  the terms of  the GNU General  Public License as published by the

 *  Free Software Foundation;  either version 2 of the  License, or (at your

 *  option) any later version.

 *

 *  THIS  SOFTWARE  IS PROVIDED   ``AS  IS'' AND   ANY  EXPRESS OR IMPLIED

 *  WARRANTIES,   INCLUDING, BUT NOT  LIMITED  TO, THE IMPLIED WARRANTIES OF

 *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN

 *  NO  EVENT  SHALL   THE AUTHOR  BE    LIABLE FOR ANY   DIRECT, INDIRECT,

 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT

 *  NOT LIMITED   TO, PROCUREMENT OF  SUBSTITUTE GOODS  OR SERVICES; LOSS OF

 *  USE, DATA,  OR PROFITS; OR  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON

 *  ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT

 *  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF

 *  THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

 *

 *  You should have received a copy of the  GNU General Public License along

 *  with this program; if not, write  to the Free Software Foundation, Inc.,

 *  675 Mass Ave, Cambridge, MA 02139, USA.

 IRQ rotation */

 0-3 */

 PCI CardSlot (IDSEL=A23) */

 PCIA => PCIA */

 PCI Backplane */

 1-4 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1999, 2000, 2004, 2005	 MIPS Technologies, Inc.

 *	All rights reserved.

 *	Authors: Carsten Langgaard <carstenl@mips.com>

 *		 Maciej W. Rozycki <macro@mips.com>

 *

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 *

 * MIPS boards specific PCI support.

		/*

		 * Due to a bug in the Galileo system controller, we need

		 * to setup the PCI BAR for the Galileo internal registers.

		 * This should be done in the bios/bootprom and will be

		 * fixed in a later revision of YAMON (the MIPS boards

		 * boot prom).

 Local bus */

 GT64120 dev */

 Function 0*/

 BAR 4*/

 Perform the write */

 Set up resource ranges from the controller's registers.  */

 Cannot support multiple windows, use the wider.  */

 We don't support remapping with a discontiguous mask.  */

 Addresses are 36-bit, so do shifts in the destinations.  */

 We don't support remapping with a discontiguous mask.  */

 Addresses are 36-bit, so do shifts in the destinations.  */

 Set up resource ranges from the controller's registers.  */

 Combine as many adjacent windows as possible.  */

 Set up resource ranges from the controller's registers.  */

 If ranges overlap I/O takes precedence.  */

 Use the larger space.  */

 PIIX4 ACPI starts at 0x1000 */

 64 GB */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Ralink RT288x SoC PCI register definitions

 *

 *  Copyright (C) 2009 John Crispin <john@phrozen.org>

 *  Copyright (C) 2009 Gabor Juhos <juhosg@openwrt.org>

 *

 *  Parts of this file are based on Ralink's 2.6.21 BSP

	/*

	 * Nobody seems to initialize slot 0, but this platform requires it, so

	 * do it once when some other slot is being enabled. The PCI subsystem

	 * should configure other slots properly, so no need to do anything

	 * special for those.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2004 ICT CAS

 * Author: Li xiaoyu, ICT CAS

 *   lixy@ict.ac.cn

 *

 * Copyright (C) 2007 Lemote, Inc. & Institute of Computing Technology

 * Author: Fuxin Zhang, zhangfx@lemote.com

 South bridge slot number is set by the pci probe process */

 Do platform specific device initialization at pci_enable_device() time */

 Configures port 1, 2, 3, 4 to be validate*/

 System clock is 48-MHz Oscillator. */

  Enable I/O Recovery time */

  Enable ISA refresh */

  disable ISA line buffer */

  Gate INTR, and flush line buffer */

  Disable PCI Delay Transaction, Enable EISA ports 4D0/4D1. */

 pci_write_config_byte(pdev, 0x47, 0x20); */

	/*

	 *  enable PCI Delay Transaction, Enable EISA ports 4D0/4D1.

	 *  enable time-out timer

	/*

	 * enable level trigger on pci irqs: 9,10,11,13

	 * important! without this PCI interrupts won't work

  512 K PCI Decode */

  Wait for PGNT before grant to ISA Master/DMA */

	/*

	 * Plug'n'Play

	 *

	 *  Parallel DRQ 3, Floppy DRQ 2 (default)

	/*

	 * IRQ Routing for Floppy and Parallel port

	 *

	 *  IRQ 6 for floppy, IRQ 7 for parallel port

 IRQ Routing for serial ports (take IRQ 3 and 4) */

  All IRQ's level triggered. */

 route PIRQA-D irq */

 bit 7-4, PIRQA */

 bit 7-4, PIRQC; */

 3-0, PIRQB */

 bit 7-4, PIRQD */

 enable function 5/6, audio/modem */

 Modify IDE controller setup */

 legacy mode */

 play safe, otherwise we may see notebook's usb keyboard lockup */

 disable read prefetch/write post buffers */

 use 3/4 as fifo thresh hold	*/

 irq routing */

 irq routing */

 enable IO */

 route ac97 IRQ */

 link control: enable link & SGD PCM output */

 disable game port, FM, midi, sb, enable write to reg2c-2f */

 we are using Avance logic codec */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2001,2002,2003 Broadcom Corporation

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

/*

 * BCM1250-specific PCI support

 *

 * This module provides the glue between Linux's PCI subsystem

 * and the hardware.  We basically provide glue for accessing

 * configuration space, and set up the translation for I/O

 * space accesses.

 *

 * To access configuration space, we use ioremap.  In the 32-bit

 * kernel, this consumes either 4 or 8 page table pages, and 16MB of

 * kernel mapped memory.  Hopefully neither of these should be a huge

 * problem.

/*

 * Macros for calculating offsets into config space given a device

 * structure or dev/fun/reg

/*

 * HT's level-sensitive interrupts require EOI, which is generated

 * through a 4MB memory-mapped region

/*

 * Read/write 32-bit values in config space.

 Do platform specific device initialization at pci_enable_device() time */

/*

 * Some checks before doing config cycles:

 * In PCI Device Mode, hide everything on bus 0 except the LDT host

 * bridge.  Otherwise, access is controlled by bridge MasterEn bits.

/*

 * Read/write access functions for various sizes of values

 * in config space.  Return all 1's for disallowed accesses

 * for a kludgy but adequate simulation of master aborts.

 CFE will assign PCI resources */

 Avoid ISA compat ranges.  */

 Set I/O resource limits.  */

 32MB accessible by sb1250 */

 no HT support yet */

	/*

	 * See if the PCI bus has been configured by the firmware.

	/*

	 * Establish mappings in KSEG2 (kernel virtual) to PCI I/O

	 * space.  Use "match bytes" policy to make everything look

	 * little-endian.  So, you need to also set

	 * CONFIG_SWAP_IO_SPACE, but this is the combination that

	 * works correctly with most of Linux's drivers.

	 * XXX ehs: Should this happen in PCI Device mode?

	/*

	 * Also check the LDT bridge's enable, just in case we didn't

	 * initialize that one.

		/*

		 * Need bits 23:16 to convey vector number.  Note that

		 * this consumes 4MB of kernel-mapped memory

		 * (Kseg2/Kseg3) for 32-bit kernel.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Ralink RT3662/RT3883 SoC PCI support

 *

 *  Copyright (C) 2011-2013 Gabor Juhos <juhosg@openwrt.org>

 *

 *  Parts of this file are based on Ralink's 2.6.21 BSP

 flush write */

 flush write */

 disable all interrupts */

 setup PCI PAD drive mode */

	/*

	 * setup the device number of the P2P bridge

	 * and de-assert the reset line

 flush write */

 reset the PCIe block */

 turn off PCIe clock */

 enable PCI arbiter */

 find the interrupt controller child node */

 find the PCI host bridge child node */

 Load PCI I/O and memory resources from DT */

 PCI */

 PCIe */

 PCIe */

 PCI */

 flush write */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2008 Lemote Technology

 * Copyright (C) 2004 ICT CAS

 * Author: Li xiaoyu, lixy@ict.ac.cn

 *

 * Copyright (C) 2007 Lemote, Inc.

 * Author: Fuxin Zhang, zhangfx@lemote.com

/* PCI interrupt pins

 *

 * These should not be changed, or you should consider loongson2f interrupt

 * register and your pci card dispatch

 all the pci device has the PCIA pin, check the datasheet. */

	INTA	INTB	INTC	INTD */

  11: Unused */

  12: Unused */

  13: Unused */

  14: Unused */

  15: Unused */

  16: Unused */

  17: RTL8110-0 */

  18: RTL8110-1 */

  19: SiI3114 */

  20: 3-ports nec usb */

  21: PCI-SLOT */

  22: Unused */

  23: Unused */

  24: Unused */

  25: Unused */

  26: Unused */

  27: Unused */

  cs5536 */

  for IDE */

  for AUDIO */

  for OHCI */

  for EHCI */

  for UDC */

  for OTG */

 Do platform specific device initialization at pci_enable_device() time */

 CS5536 SPEC. fixup */

 the uart1 and uart2 interrupt in PIC is enabled as default */

 setting the mutex pin as IDE function */

 enable the AUDIO interrupt in PIC  */

 enable the OHCI interrupt in PIC */

 THE OHCI, EHCI, UDC, OTG are shared with interrupt in PIC */

 Serial short detect enable */

 setting the USB2.0 micro frame length */

 Only 2 port be used */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2008 Maxime Bizon <mbizon@freebox.fr>

/*

 * swizzle 32bits data to return only the needed part

/*

 * setup hardware for a configuration cycle with given parameters

 sanity check */

 ok, setup config access */

 type 0 cycle for local bus, type 1 cycle for anything else */

 FIXME: how to specify bus ??? */

	/* two phase cycle, first we write address, then read data at

	 * another location, caller already has a spinlock so no need

 restore IO space normal behaviour */

	/* two phase cycle, first we write address, then write data to

	 * another location, caller already has a spinlock so no need

 no way to know the access is done, we have to wait */

 restore IO space normal behaviour */

/*

 * emulate configuration read access on a cardbus bridge

 create dummy vendor/device id from our cpu id */

 bridge control */

 pin:intA line:0xff */

 | 1 for 32bits io support */

 | 1 for 32bits io support */

/*

 * emulate configuration write access on a cardbus bridge

 disable memory prefetch support */

	/* snoop access to slot 0x1e on root bus, we fake a cardbus

	/* a  configuration  cycle for	the  device  behind the	 cardbus

	 * bridge is  actually done as a  type 0 cycle	on the primary

	 * bus. This means that only  one device can be on the cardbus

/*

 * only one IO window, so it  cannot be shared by PCI and cardbus, use

 * fixup to choose and detect unhandled configuration

 look for any io resource */

 skip our fake bus with only cardbus bridge on it */

 find on which bus the device is */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

/**

 * Both AR2315 and AR2316 chips have PCI interface unit, which supports DMA

 * and interrupt. PCI interface supports MMIO access method, but does not

 * seem to support I/O ports.

 *

 * Read/write operation in the region 0x80000000-0xBFFFFFFF causes

 * a memory read/write command on the PCI bus. 30 LSBs of address on

 * the bus are taken from memory read/write request and 2 MSBs are

 * determined by PCI unit configuration.

 *

 * To work with the configuration space instead of memory is necessary set

 * the CFG_SEL bit in the PCI_MISC_CONFIG register.

 *

 * Devices on the bus can perform DMA requests via chip BAR1. PCI host

 * controller BARs are programmend as if an external device is programmed.

 * Which means that during configuration, IDSEL pin of the chip should be

 * asserted.

 *

 * We know (and support) only one board that uses the PCI interface -

 * Fonera 2.0g (FON2202). It has a USB EHCI controller connected to the

 * AR2315 PCI bus. IDSEL pin of USB controller is connected to AD[13] line

 * and IDSEL pin of AR2315 is connected to AD[16] line.

/*

 * PCI Bus Interface Registers

 # of AHB clk cycles in 1ms */

 Enable TXD for fragments */

 Mem or Config cycles */

 bits 31-30 for pci req */

 4:5=0 rst is input */

 4:5=1 rst to GND */

 4:5=2 rst to VDD */

 6:7=0 early grant en */

 6:7=1 grant waits 4 frame */

 6:7=2 grant waits 4 idle */

 6:7=2 grant waits 4 idle */

#define AR2315_PCICACHE_DIS	0x00001000	/* PCI external access cache

 Enable chain 0 */

 Enable chain 1 */

 Enable chain 2 */

 Enable chain 3 */

 Disable chain 0 */

 Disable chain 1 */

 Disable chain 2 */

 Disable chain 3 */

 Enable chain 0 */

 Disable chain 0 */

 PCI interrupt status (write one to clear) */

 Desc In Completed */

 Desc In OK */

 Desc In ERR */

 Desc In End-of-List */

 Desc Out Completed */

 Desc Out OK */

 Desc Out ERR */

 Desc Out EOL */

 Desc In Out-of-Desc */

 Desc Mask */

 Extern PCI INTA */

 PCI bus abort event */

 PCI interrupt mask */

 Global PCI interrupt enable */

 disable pci interrupts */

 enable pci interrupts */

/*

 * PCI interrupts, which share IP5

 * Keep ordered according to AR2315_PCI_INT_XXX bits

 Arbitrary size of memory region to access the configuration space */

/*

 * We need some arbitrary non-zero value to be programmed to the BAR1 register

 * of PCI host controller to enable DMA. The same value should be used as the

 * offset to calculate the physical address of DMA buffer for PCI devices.

 ??? access BAR */

 RAM access BAR */

 ??? access BAR */

 Prevent access past the remapped area */

 Clear pending errors */

 Select Configuration access */

 PCI must see space change before we begin */

 Select Memory access */

 Program MBARs */

 Run */

	/* Clear any pending Abort or external Interrupts

 Remap PCI config space */

 Reset the PCI bus by setting bits 5-4 in PCI_MCFG */

 Bring the PCI out of reset */

 1GB uncached */

 Enable uncached */

 Base: 0x80000000 */);

 PCI controller does not support I/O ports */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Define the pci_ops for the PCIC on Toshiba TX4927, TX4938, etc.

 *

 * Based on linux/arch/mips/pci/ops-tx4938.c,

 *	    linux/arch/mips/pci/fixup-rbtx4938.c,

 *	    linux/arch/mips/txx9/rbtx4938/setup.c,

 *	    and RBTX49xx patch from CELF patch archive.

 *

 * 2003-2005 (c) MontaVista Software, Inc.

 * Copyright (C) 2004 by Ralf Baechle (ralf@linux-mips.org)

 * (C) Copyright TOSHIBA CORPORATION 2000-2001, 2004-2007

 TX4938 has 2 pcic */

 clear M_ABORT and Disable M_ABORT Int. */

 wait write cycle completion before checking error status */

 flush write buffer */

 4064 GBUSCLK for CCFG.GTOT=0b11 */

 Disable All Initiator Space */

 GB->PCI mappings */

 PCI->GB mappings (I/O 256B) */

 256B */

 PCI->GB mappings (MEM 512MB (64MB on R1.x)) */

 PCI->GB mappings (MEM 16MB) */

 PCI->GB mappings (MEM 1MB) */

 1MB */

 Clear all (including IRBER) except for GBWC */

 Enable Initiator Memory Space */

 Enable Initiator I/O Space */

 Enable Initiator Config */

 Do not use MEMMUL, MEMINF: YMFPCI card causes M_ABORT. */

 Clear All Local Bus Status */

 Enable All Local Bus Interrupts */

 Clear All Initiator Status */

 Enable All Initiator Interrupts */

 Clear All PCI Status Error */

 Enable All PCI Status Error Interrupts */

 Reset Bus Arbiter */

 Enable Bus Arbiter */

 skip registers with side-effects */

 clear all pci errors */

 Reset Bus Arbiter */

		/*

		 * swap reqBP and reqXP (raise priority of SLC90E66).

		 * SLC90E66(PCI-ISA bridge) is connected to REQ2 on

		 * PCI Backplane board.

 Use Fixed ParkMaster (required by SLC90E66) */

 Enable Bus Arbiter */

/*

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 *

 * Copyright (C) 2005-2009 Cavium Networks

/*

 * Octeon's PCI controller uses did=3, subdid=2 for PCI IO

 * addresses. Use PCI endian swapping 1 so no address swapping is

 * necessary. The Linux io routines will endian swap the data.

 Octeon't PCI controller uses did=3, subdid=3 for PCI memory. */

/**

 * This is the bit decoding used for the Octeon PCI controller addresses

/**

 * Map a PCI device to the appropriate interrupt line

 *

 * @dev:    The Linux PCI device structure for the device to map

 * @slot:   The slot number for this device on __BUS 0__. Linux

 *		 enumerates through all the bridges and figures out the

 *		 slot on Bus 0 where this device eventually hooks to.

 * @pin:    The PCI interrupt pin read from the device, then swizzled

 *		 as it goes through each bridge.

 * Returns Interrupt number for the device

/*

 * Called to perform platform specific PCI setup

	/*

	 * Force the Cache line setting to 64 bytes. The standard

	 * Linux bus scan doesn't seem to set it. Octeon really has

	 * 128 byte lines, but Intel bridges get really upset if you

	 * try and set values above 64 bytes. Value is specified in

	 * 32bit words.

 Set latency timers for all devices */

 Enable reporting System errors and parity errors on all devices */

 Enable parity checking and error reporting */

 Set latency timers on sub bridges */

 More bridge error detection */

 Enable the PCIe normal error reporting */

 Correctable Error Reporting */

 Non-Fatal Error Reporting */

 Fatal Error Reporting */

 Unsupported Request */

 Find the Advanced Error Reporting capability */

 Clear Uncorrectable Error Status */

 Enable reporting of all uncorrectable errors */

 Uncorrectable Error Mask - turned on bits disable errors */

		/*

		 * Leave severity at HW default. This only controls if

		 * errors are reported as uncorrectable or

		 * correctable, not if the error is reported.

 PCI_ERR_UNCOR_SEVER - Uncorrectable Error Severity */

 Clear Correctable Error Status */

 Enable reporting of all correctable errors */

 Correctable Error Mask - turned on bits disable errors */

 Advanced Error Capabilities */

 ECRC Generation Enable */

 ECRC Check Enable */

 PCI_ERR_HEADER_LOG - Header Log Register (16 bytes) */

 Report all errors to the root complex */

 Clear the Root status register */

/**

 * Return the mapping of PCI device number to IRQ line. Each

 * character in the return string represents the interrupt

 * line for the device at that position. Device 1 maps to the

 * first character, etc. The characters A-D are used for PCI

 * interrupts.

 *

 * Returns PCI interrupt mapping

	/*

	 * Returning an empty string causes the interrupts to be

	 * routed based on the PCI specification. From the PCI spec:

	 *

	 * INTA# of Device Number 0 is connected to IRQW on the system

	 * board.  (Device Number has no significance regarding being

	 * located on the system board or in a connector.) INTA# of

	 * Device Number 1 is connected to IRQX on the system

	 * board. INTA# of Device Number 2 is connected to IRQY on the

	 * system board. INTA# of Device Number 3 is connected to IRQZ

	 * on the system board. The table below describes how each

	 * agent's INTx# lines are connected to the system board

	 * interrupt lines. The following equation can be used to

	 * determine to which INTx# signal on the system board a given

	 * device's INTx# line(s) is connected.

	 *

	 * MB = (D + I) MOD 4 MB = System board Interrupt (IRQW = 0,

	 * IRQX = 1, IRQY = 2, and IRQZ = 3) D = Device Number I =

	 * Interrupt Number (INTA# = 0, INTB# = 1, INTC# = 2, and

	 * INTD# = 3)

 This is really the NAC38 */

/**

 * Map a PCI device to the appropriate interrupt line

 *

 * @dev:    The Linux PCI device structure for the device to map

 * @slot:   The slot number for this device on __BUS 0__. Linux

 *		 enumerates through all the bridges and figures out the

 *		 slot on Bus 0 where this device eventually hooks to.

 * @pin:    The PCI interrupt pin read from the device, then swizzled

 *		 as it goes through each bridge.

 * Returns Interrupt number for the device

 Get the board specific interrupt mapping */

/*

 * Read a value from configuration space

/*

 * Write a value to PCI configuration space

/*

 * PCI ports must be above 16KB so the ISA bus filtering in the PCI-X to PCI

 * bridge

/*

 * Low level initialize the Octeon PCI controller

 Reset the PCI Bus */

 Hold PCI reset for 2 ms */

 cvmx_read_csr(CVMX_NPI_CTL_STATUS); */

	/* Deassert PCI reset and advertize PCX Host Mode Device Capability

 Wait 2 ms after deasserting PCI reset */

	ctl_status_2.s.tsr_hwm = 1;	/* Initializes to 0.  Must be set

 Enable BAR2 */

 Don't use L2 */

 Round robin priority */

 BAR1 hole */

 BAR1 is 2GB */

 Don't use L2 with big bars */

 Big bar in byte swap mode */

 BAR1 is big */

 BAR0 is big */

 Wait 2 ms before doing PCI reads */

	/*

	 * TDOMC must be set to one in PCI mode. TDOMC should be set to 4

	 * in PCI-X mode to allow four outstanding splits. Otherwise,

	 * should not change from its reset value. Don't write PCI_CFG19

	 * in PCI mode (0x82000001 reset value), write it to 0x82000004

	 * after PCI-X mode is known. MRBCI,MDWE,MDRE -> must be zero.

	 * MRBCM -> must be one.

		/*

		 * Target Delayed/Split request outstanding maximum

		 * count. [1..31] and 0=32.  NOTE: If the user

		 * programs these bits beyond the Designed Maximum

		 * outstanding count, then the designed maximum table

		 * depth will be used instead.	No additional

		 * Deferred/Split transactions will be accepted if

		 * this outstanding maximum count is

		 * reached. Furthermore, no additional deferred/split

		 * transactions will be accepted if the I/O delay/ I/O

		 * Split Request outstanding maximum is reached.

		/*

		 * Master Deferred Read Request Outstanding Max Count

		 * (PCI only).	CR4C[26:24] Max SAC cycles MAX DAC

		 * cycles 000 8 4 001 1 0 010 2 1 011 3 1 100 4 2 101

		 * 5 2 110 6 3 111 7 3 For example, if these bits are

		 * programmed to 100, the core can support 2 DAC

		 * cycles, 4 SAC cycles or a combination of 1 DAC and

		 * 2 SAC cycles. NOTE: For the PCI-X maximum

		 * outstanding split transactions, refer to

		 * CRE0[22:20].

		/*

		 * Master Request (Memory Read) Byte Count/Byte Enable

		 * select. 0 = Byte Enables valid. In PCI mode, a

		 * burst transaction cannot be performed using Memory

		 * Read command=4?h6. 1 = DWORD Byte Count valid

		 * (default). In PCI Mode, the memory read byte

		 * enables are automatically generated by the

		 * core. Note: N3 Master Request transaction sizes are

		 * always determined through the

		 * am_attr[<35:32>|<7:0>] field.

 Memory Space Access Enable */

 Master Enable */

 PERR# Enable */

 System Error Enable */

 Fast Back to Back Transaction Enable */

	/*

	 * When OCTEON is a PCI host, most systems will use OCTEON's

	 * internal arbiter, so must enable it before any PCI/PCI-X

	 * traffic can occur.

 Internal arbiter enable */

 USE_OCTEON_INTERNAL_ARBITER */

	/*

	 * Preferably written to 1 to set MLTD. [RDSATI,TRTAE,

	 * TWTAE,TMAE,DPPMR -> must be zero. TILT -> must not be set to

	 * 1..7.

 Master Latency Timer Disable */

	/*

	 * Should be written to 0x4ff00. MTTV -> must be zero.

	 * FLUSH -> must be 1. MRV -> should be 0xFF.

 Master Retry Value [1..255] and 0=infinite */

	/*

	 * AM_DO_FLUSH_I control NOTE: This bit MUST BE ONE for proper

	 * N3K operation.

	/*

	 * MOST Indicates the maximum number of outstanding splits (in -1

	 * notation) when OCTEON is in PCI-X mode.  PCI-X performance is

	 * affected by the MOST selection.  Should generally be written

	 * with one of 0x3be807, 0x2be807, 0x1be807, or 0x0be807,

	 * depending on the desired MOST of 3, 2, 1, or 0, respectively.

 RO - PCI-X Capability ID */

 RO - Next Capability Pointer */

 Data Parity Error Recovery Enable */

 Relaxed Ordering Enable */

	cfg56.s.mmbc = 1;	/* Maximum Memory Byte Count

	cfg56.s.most = 3;	/* Maximum outstanding Split transactions [0=1

	/*

	 * Affects PCI performance when OCTEON services reads to its

	 * BAR1/BAR2. Refer to Section 10.6.1.	The recommended values are

	 * 0x22, 0x33, and 0x33 for PCI_READ_CMD_6, PCI_READ_CMD_C, and

	 * PCI_READ_CMD_E, respectively. Unfortunately due to errata DDR-700,

	 * these values need to be changed so they won't possibly prefetch off

	 * of the end of memory if PCI is DMAing a buffer at the end of

	 * memory. Note that these values differ from their reset values.

/*

 * Initialize the Octeon PCI controller

 Only these chips have PCI */

 Point pcibios_map_irq() to the PCI version of it */

 Only use the big bars on chips that support it */

 PCI I/O and PCI MEM values */

 Endian-Swap on read. */

 Endian-Swap on write. */

 No-Snoop on read. */

 No-Snoop on write. */

 Relax Read on read. */

 Relax Order on write. */

 PCI Address bits [63:36]. */

	/*

	 * Remap the Octeon BAR 2 above all 32 bit devices

	 * (0x8000000000ul).  This is done here so it is remapped

	 * before the readl()'s below. We don't want BAR2 overlapping

	 * with BAR0/BAR1 during these reads.

 Remap the Octeon BAR 0 to 0-2GB */

		/*

		 * Remap the Octeon BAR 1 to map 2GB-4GB (minus the

		 * BAR 1 hole).

 BAR1 movable mappings set for identity mapping */

 Address bits[35:22] sent to L2C */

 Don't put PCI accesses in L2. */

 Endian Swap Mode */

 Set '1' when the selected address range is valid. */

 Devices go after BAR1 */

 Remap the Octeon BAR 0 to map 128MB-(128MB+4KB) */

 Remap the Octeon BAR 1 to map 0-128MB */

 BAR1 movable regions contiguous to cover the swiotlb */

 Address bits[35:22] sent to L2C */

 Don't put PCI accesses in L2. */

 Endian Swap Mode */

 Set '1' when the selected address range is valid. */

 Devices go after BAR0 */

	/*

	 * Clear any errors that might be pending from before the bus

	 * was setup properly.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Atheros AR724X PCI host controller driver

 *

 *  Copyright (C) 2011 René Bolldorf <xsecute@googlemail.com>

 *  Copyright (C) 2009-2011 Gabor Juhos <juhosg@openwrt.org>

 flush write */

 use the cached value */

			/*

			 * WAR for a hw issue. If the BAR0 register of the

			 * device is set to the proper base address, the

			 * memory space of the device is not accessible.

			 *

			 * Cache the intended value so it can be read back,

			 * and write a SoC specific constant value to the

			 * BAR0 register in order to make the device memory

			 * accessible.

 flush write */

 flush write */

 flush write */

 flush write */

 deassert PCIe host controller and PCIe PHY reset */

 remove the reset of the PCIE PLL */

 deassert bypass for the PCIE PLL */

 set PCIE Application Control to ready */

 wait up to 100ms for PHY link up */

	/*

	 * Do the full PCIE Root Complex Initialization Sequence if the PCIe

	 * host controller is in reset.

 SPDX-License-Identifier: GPL-2.0

/*

 * O2 has up to 5 PCI devices connected into the MACE bridge.  The device

 * map looks like this:

 *

 * 0  aic7xxx 0

 * 1  aic7xxx 1

 * 2  expansion slot

 * 3  N/C

 * 4  N/C

 Dummy	INT#A  INT#B  INT#C  INT#D */

 This is placeholder row - never used */

/*

 * Given a PCI slot number (a la PCI_SLOT(...)) and the interrupt pin of

 * the device (1-4 => A-D), tell what irq to use.  Note that we don't

 * in theory have slots 4 and 5, and we never normally use the shared

 * irqs.  I suppose a device without a pin A will thank us for doing it

 * right if there exists such a broken piece of crap.

 Do platform specific device initialization at pci_enable_device() time */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *  fixup-tb0226.c, The TANBAC TB0226 specific PCI fixups.

 *

 *  Copyright (C) 2002-2005  Yoichi Yuasa <yuasa@linux-mips.org>

 Do platform specific device initialization at pci_enable_device() time */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Memory fault handling for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * Page fault handling for the Hexagon Virtual Machine.

 * Can also be called by a native port emulating the HVM

 * execptions.

/*

 * Decode of hardware exception sends us to one of several

 * entry points.  At each, we generate canonical arguments

 * for handling by the abstract memory management code.

/*

 * Canonical page fault handler

	/*

	 * If we're in an interrupt or have no user context,

	 * then must not take the fault.

 Address space is OK.  Now check access rights. */

 The most common case -- we are done. */

 Handle copyin/out exception cases */

	/* User-mode address is in the memory map, but we are

	 * unable to fix up the page fault.

 Address is not in the memory map */

 Kernel-mode fault falls through */

 Things are looking very, very bad now */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Memory subsystem initialization for Hexagon

 *

 * Copyright (c) 2010-2013, The Linux Foundation. All rights reserved.

/*

 * Define a startpg just past the end of the kernel image and a lastpg

 * that corresponds to the end of real or simulated platform memory.

  Should be set by platform code  */

  physical kernel offset >> 12  */

  Set as variable to limit PMD copies  */

  indicate pfn's of high memory  */

 Default cache attribute for newly created page tables */

/*

 * The current "generation" of kernel map, which should not roll

 * over until Hell freezes over.  Actual bound in years needs to be

 * calculated to confirm.

  checkpatch says don't init this to 0.  */

/*

 * mem_init - initializes memory

 *

 * Frees up bootmem

 * Fixes up more stuff for HIGHMEM

 * Calculates and displays memory available/used

  No idea where this is actually declared.  Seems to evade LXR.  */

	/*

	 *  To-Do:  someone somewhere should wipe out the bootmem map

	 *  after we're done?

	/*

	 * This can be moved to some more virtual-memory-specific

	 * initialization hook at some point.  Set the init_mm

	 * descriptors "context" value to point to the initial

	 * kernel segment table's physical address.

/*

 * In order to set up page allocator "nodes",

 * somebody has to call free_area_init() for UMA.

 *

 * In this mode, we only have one pg_data_t

 * structure: contig_mem_data.

	/*

	 *  This is not particularly well documented anywhere, but

	 *  give ZONE_NORMAL all the memory, including the big holes

	 *  left by the kernel+bootmem_map which are already left as reserved

	 *  in the bootmem_map; free_area_init should see those bits and

	 *  adjust accordingly.

  sets up the zonelists and mem_map  */

	/*

	 * Start of high memory area.  Will probably need something more

	 * fancy if we...  get more fancy.

/*

 * Pick out the memory size.  We look for mem=size,

 * where size is "size[KkMm]"

  XXX Todo: this probably should be cleaned up  */

	/*

	 * Set up boot memory allocator

	 *

	 * The Gorman book also talks about these functions.

	 * This needs to change for highmem setups.

  Prior to this, bootmem_lastpg is actually mem size  */

 Memory size needs to be a multiple of 16M */

 Reserve kernel text/data/bss */

	/*

	 * Reserve the top DMA_RESERVE bytes of RAM for DMA (uncached)

	 * memory allocation

	/*

	 * The default VM page tables (will be) populated with

	 * VA=PA+PAGE_OFFSET mapping.  We go in and invalidate entries

	 * higher than what we have memory for.

  this is pointer arithmetic; each entry covers 4MB  */

  this actually only goes to the end of the first gig  */

	/*

	 * Move forward to the start of empty pages; take into account

	 * phys_offset shift.

 stop the pointer at the device I/O 4MB page  */

  Other half of the early device table from vm_init_segtable. */

	/*

	 *  The bootmem allocator seemingly just lives to feed memory

	 *  to the paging system

  See Gorman Book, 2.3  */

	/*

	 *  At this point, the page allocator is kind of initialized, but

	 *  apparently no pages are available (just like with the bootmem

	 *  allocator), and need to be freed themselves via mem_init(),

	 *  which is called by start_kernel() later on in the process

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * Support for user memory access from kernel.  This will

 * probably be inlined for performance at some point, but

 * for ease of debug, and to a lesser degree for code size,

 * we implement here as subroutines.

/*

 * For clear_user(), exploit previously defined copy_to_user function

 * and the fact that we've got a handy zero page defined in kernel/head.S

 *

 * dczero here would be even faster.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Cache management functions for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * This is just really brutal and shouldn't be used anyways,

 * especially on V2.  Left here just in case.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Hexagon Virtual Machine TLB functions

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * The Hexagon Virtual Machine conceals the real workings of

 * the TLB, but there are one or two functions that need to

 * be instantiated for it, differently from a native build.

/*

 * Initial VM implementation has only one map active at a time, with

 * TLB purgings on changes.  So either we're nuking the current map,

 * or it's a no-op.  This operation is messy on true SMPs where other

 * processors must be induced to flush the copies in their local TLBs,

 * but Hexagon thread-based virtual processors share the same MMU.

/*

 * Flush a page from the kernel virtual map - used by highmem

/*

 * Flush all TLBs across all CPUs, virtual or real.

 * A single Hexagon core has 6 thread contexts but

 * only one TLB.

  should probably use that fixaddr end or whateve label  */

/*

 * Flush TLB entries associated with a given mm_struct mapping.

 Current Virtual Machine has only one map active at a time */

/*

 * Flush TLB state associated with a page of a vma.

/*

 * Flush TLB entries associated with a kernel address range.

 * Like flush range, but without the check on the vma->vm_mm.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I/O remap functions for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

  Wrapping not allowed  */

  Rounds up to next page size, including whole-page offset */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Process creation support for Hexagon

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

/*

 * Program thread launch.  Often defined as a macro in processor.h,

 * but we're shooting for a small footprint and it's not an inner-loop

 * performance-critical operation.

 *

 * The Hexagon ABI specifies that R28 is zero'ed before program launch,

 * so that gets automatically done here.  If we ever stop doing that here,

 * we'll probably want to define the ELF_PLAT_INIT macro.

 We want to zero all data-containing registers. Is this overkill? */

 We might want to also zero all Processor registers here */

/*

 *  Spin, or better still, do a hardware or VM wait instruction

 *  If hardware or VM offer wait termination even though interrupts

 *  are disabled.

  interrupts wake us up, but irqs are still disabled */

/*

 * Copy architecture-specific thread state

	/*

	 * Establish kernel stack pointer and initial PC for new thread

	 * Note that unlike the usual situation, we do not copy the

	 * parent's callee-saved here; those are in pt_regs and whatever

	 * we leave here will be overridden on return to userland.

 r24 <- fn, r25 <- arg */

 Child sees zero return value */

	/*

	 * The clone syscall has the C signature:

	 * int [r0] clone(int flags [r0],

	 *           void *child_frame [r1],

	 *           void *parent_tid [r2],

	 *           void *child_tid [r3],

	 *           void *thread_control_block [r4]);

	 * ugp is used to provide TLS support.

	/*

	 * Parent sees new pid -- not necessary, not even possible at

	 * this point in the fork process

	 * Might also want to set things like ti->addr_limit

/*

 * Release any architecture-specific resources locked by thread

/*

 * Some archs flush debug and FPU info here

/*

 * The "wait channel" terminology is archaic, but what we want

 * is an identification of the point at which the scheduler

 * was invoked by a blocked thread.

/*

 * Called on the exit path of event entry; see vm_entry.S

 *

 * Interrupts will already be disabled.

 *

 * Returns 0 if there's no need to re-check for more work.

 shortcut -- no work to be done */

 Should not even reach here */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stacktrace support for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * Save stack-backtrace addresses into a stack_trace buffer.

		/*

		 * The next frame must be at a higher address than the

		 * current frame.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel traps/events for Hexagon processor

 *

 * Copyright (c) 2010-2014, The Linux Foundation. All rights reserved.

 Maybe should resemble arch/sh/kernel/traps.c ?? */

 CONFIG_GENERIC_BUG */

 Saved link reg is one word above FP */

 Expect kernel stack to be in-bounds */

 Attempt to continue past exception. */

 really want to see more ... */

 If link reg is null, we are done. */

 If newfp isn't larger, we're tracing garbage. */

 Saved link reg is one word above FP */

/*

 * It's not clear that misaligned fetches are ever recoverable.

/*

 * Misaligned loads and stores, on the other hand, can be

 * emulated, and probably should be, some day.  But for now

 * they will be considered fatal.

/*

 * Precise bus errors may be recoverable with a a retry,

 * but for now, treat them as irrecoverable.

/*

 * If anything is to be done here other than panic,

 * it will probably be complex and migrate to another

 * source module.  For now, just die.

/*

 * General exception handler

	/*

	 * Decode Cause and Dispatch

 Halt and catch fire */

 Indirect system call dispatch */

 System call is trap0 #1 */

 allow strace to catch syscall args  */

  return -ENOSYS somewhere?  */

 Interrupts should be re-enabled for syscall processing */

		/*

		 * System call number is in r6, arguments in r0..r5.

		 * Fortunately, no Linux syscall has more than 6 arguments,

		 * and Hexagon ABI passes first 6 arguments in registers.

		 * 64-bit arguments are passed in odd/even register pairs.

		 * Fortunately, we have no system calls that take more

		 * than three arguments with more than one 64-bit value.

		 * Should that change, we'd need to redesign to copy

		 * between user and kernel stacks.

		/*

		 * GPR R0 carries the first parameter, and is also used

		 * to report the return value.  We need a backup of

		 * the user's value in case we need to do a late restart

		 * of the system call.

 allow strace to get the syscall return state  */

 Trap0 0xdb is debug breakpoint */

			/*

			 * Some architecures add some per-thread state

			 * to distinguish between breakpoint traps and

			 * trace traps.  We may want to do that, and

			 * set the si_code value appropriately, or we

			 * may want to use a different trap0 flavor.

 Ignore other trap0 codes for now, especially 0 (Angel calls) */

/*

 * Machine check exception handler

 Halt and catch fire */

/*

 * Treat this like the old 0xdb trap.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Arch related setup for Hexagon

 *

 * Copyright (c) 2010-2013, The Linux Foundation. All rights reserved.

/*

 * setup_arch -  high level architectural setup routine

 * @cmdline_p: pointer to pointer to command-line arguments

	/*

	 * These will eventually be pulled in via either some hypervisor

	 * or devicetree description.  Hardwiring for now.

	/*

	 * Set up event bindings to handle exceptions and interrupts.

	/*

	 * Simulator has a few differences from the hardware.

	 * For now, check uninitialized-but-mapped memory

	 * prior to invoking setup_arch_memory().

	/*

	 * boot_command_line and the value set up by setup_arch

	 * are both picked up by the init code. If no reason to

	 * make them different, pass the same pointer back.

/*

 * Functions for dumping CPU info via /proc

 * Probably should move to kernel/proc.c or something.

/*

 * Eventually this will dump information about

 * CPU properties like ISA level, TLB size, etc.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Time related functions for Hexagon architecture

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * For the clocksource we need:

 *	pcycle frequency (600MHz)

 * For the loops_per_jiffy we need:

 *	thread/cpu frequency (100MHz)

 * And for the timer, we need:

 *	sleep clock rate

/*

 * 8x50 HDD Specs 5-8.  Simulator co-sim not fixed until

 * release 1.1, and then it's "adjustable" and probably not defaulted.

  A lot of this stuff should move into a platform specific section.  */

  Match value  */

  [1] - CLR_ON_MATCH_EN, [0] - EN  */

  one-shot register that clears the count  */

  Look for "TCX0" for related constants.  */

  Assuming the timer will be disabled when we enter here.  */

  Broadcast mechanism  */

 XXX Implement set_state_shutdown() */

  Called from smp.c for each CPU's timer ipi call  */

 CONFIG_SMP */

/*

 * time_init_deferred - called by start_kernel to set up timer/clock source

 *

 * Install the IRQ handler for the clock, setup timers.

 * This is done late, as that way, we can use ioremap().

 *

 * This runs just before the delay loop is calibrated, and

 * is used for delay calibration.

  ioremap here means this has to run later, after paging init  */

  Note: the sim generic RTOS clock is apparently really 18750Hz  */

	/*

	 * Last arg is some guaranteed seconds for which the conversion will

	 * work without overflow.

/*

 * This could become parametric or perhaps even computed at run-time,

 * but for now we take the observed simulator jitter.

 Maybe lower if kernel optimized. */

  not sure how this improves readability  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * DMA implementation for Hexagon

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

/*

 * Our max_low_pfn should have been backed off by 16MB in mm/init.c to create

 * DMA coherent space.  Use that for the pool.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * System call table for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Mostly IRQ support for Hexagon

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

/*

 * show_regs - print pt_regs structure

 * @regs: pointer to pt_regs

 *

 * To-do:  add all the accessor definitions to registers.h

 *

 * Will make this routine a lot easier to write.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Export of symbols defined in assembly files and/or libgcc.

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

 Additional functions */

 Additional variables */

 Symbols found in libgcc that assorted kernel modules need */

 Additional functions */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * First-level interrupt controller model for Hexagon.

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

  This is actually all we need for handle_fasteoi_irq  */

/* Power mamangement wake call. We don't need this, however,

 * if this is absent, then an -ENXIO error is returned to the

 * msm_serial driver, and it fails to correctly initialize.

 * This is a bug in the msm_serial driver, but, for now, we

 * work around it here, by providing this bogus handler.

 * XXX FIXME!!! remove this when msm_serial is fixed.

/**

 * The hexagon core comes with a first-level interrupt controller

 * with 32 total possible interrupts.  When the core is embedded

 * into different systems/platforms, it is typically wrapped by

 * macro cells that provide one or more second-level interrupt

 * controllers that are cascaded into one or more of the first-level

 * interrupts handled here. The precise wiring of these other

 * irqs varies from platform to platform, and are set up & configured

 * in the platform-specific files.

 *

 * The first-level interrupt controller is wrapped by the VM, which

 * virtualizes the interrupt controller for us.  It provides a very

 * simple, fast & efficient API, and so the fasteoi handler is

 * appropriate for this case.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SMP support for Hexagon

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

  timer_interrupt  */

/*

 * cpu_possible_mask needs to be filled out prior to setup_per_cpu_areas

 * (which is prior to any of our smp_prepare_cpu crap), in order to set

 * up the...  per_cpu areas.

			/*

			 * call vmstop()

  Used for IPI call from other CPU's to unmask int  */

/*

 * This is based on Alpha's IPI stuff.

 * Supposed to take (int, void*) as args now.

 * Specifically, first arg is irq, second is the irq_desc.

  Possible barrier here  */

/*

 * interrupts should already be disabled from the VM

 * SP should already be correct; need to set THREADINFO_REG

 * to point to current thread info

  Calculate thread_info pointer from stack pointer  */

  Set the memory struct  */

  Register the clock_event dummy  */

/*

 * called once for each present cpu

 * apparently starts up the CPU and then

 * maintains control until "cpu_online(cpu)" is set.

  Boot to the head.  */

	/*

	 * should eventually have some sort of machine

	 * descriptor that has this stuff

  Right now, let's just fake it. */

  Also need to register the interrupts for IPI  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Signal support for Hexagon processor

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

/*

 * Setup signal stack frame with siginfo structure

	/* The on-stack signal trampoline is no longer executed;

	 * however, the libgcc signal frame unwinding code checks for

	 * the presence of these two numeric magic values.

 Load r0/r1 pair with signumber/siginfo pointer... */

/*

 * Setup invocation of signal handler

	/*

	 * If we're handling a signal that aborted a system call,

	 * set up the error return value before adding the signal

	 * frame to the stack.

	/*

	 * Set up the stack frame; not doing the SA_SIGINFO thing.  We

	 * only set up the rt_frame flavor.

 If there was an error on setup, no signal was delivered. */

/*

 * Called from return-from-event code.

	/*

	 * No (more) signals; if we came from a system call, handle the restart.

 If there's no signal to deliver, put the saved sigmask back */

/*

 * Architecture-specific wrappers for signal-related system calls

 Always make any pending restarted system calls return -EINTR */

 Restore the user's stack as well */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/hexagon/kernel/kgdb.c - Hexagon KGDB Support

 *

 * Copyright (c) 2011-2012, The Linux Foundation. All rights reserved.

 All registers are 4 bytes, for now */

/* The register names are used during printing of the regs;

 trap0(#0xDB) 0x0cdb0054 */

  Not yet working  */

 Initialize to zero */

 Otherwise, we have only some registers from switch_to() */

/**

 * kgdb_arch_handle_exception - Handle architecture specific GDB packets.

 * @vector: The error vector of the exception that happened.

 * @signo: The signal number of the exception that happened.

 * @err_code: The error code of the exception that happened.

 * @remcom_in_buffer: The buffer of the packet we have read.

 * @remcom_out_buffer: The buffer of %BUFMAX bytes to write a packet into.

 * @regs: The &struct pt_regs of the current process.

 *

 * This function MUST handle the 'c' and 's' command packets,

 * as well packets to set / remove a hardware breakpoint, if used.

 * If there are additional packets which the hardware needs to handle,

 * they are handled here.  The code should return -1 if it wants to

 * process more packets, and a %0 or %1 if it wants to exit from the

 * kgdb callback.

 *

 * Not yet working.

 Stay in the debugger. */

 cpu roundup */

	/*

	 * Lowest-prio notifier priority, we want to be notified last:

/**

 * kgdb_arch_init - Perform any architecture specific initialization.

 *

 * This function will handle the initialization of any architecture

 * specific callbacks.

/**

 * kgdb_arch_exit - Perform any architecture specific uninitalization.

 *

 * This function will handle the uninitalization of any architecture

 * specific callbacks, for dynamic registration and unregistration.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Ptrace support for Hexagon

 *

 * Copyright (c) 2010-2013, The Linux Foundation. All rights reserved.

  Both called from ptrace_resume  */

	/* The general idea here is that the copyout must happen in

	 * exactly the same order in which the userspace expects these

	 * regs. Now, the sequence in userspace does not match the

	 * sequence in the kernel, so everything past the 32 gprs

	 * happens one at a time.

 Must be exactly same sequence as struct user_regs_struct */

 pc

 cause

 badva

 Must be exactly same sequence as struct user_regs_struct */

 CAUSE and BADVA aren't writeable. */

 Ignore the rest, if needed */

	/*

	 * This is special; SP is actually restored by the VM via the

	 * special event record which is set by the special trap.

 Boilerplate - resolves to null inline if no HW single-step */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel module loader for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

/*

 * module_frob_arch_sections - tweak got/plt sections.

 * @hdr - pointer to elf header

 * @sechdrs - pointer to elf load section headers

 * @secstrings - symbol names

 * @mod - pointer to module

 Look for .plt and/or .got.plt and/or .init.plt sections */

 At this time, we don't support modules comiled with -shared */

  return -ENOEXEC;  */

/*

 * apply_relocate_add - perform rela relocations.

 * @sechdrs - pointer to section headers

 * @strtab - some sort of start address?

 * @symindex - symbol index offset or something?

 * @relsec - address to relocate to?

 * @module - pointer to module

 *

 * Perform rela relocations.

 Symbol to relocate */

 Where to make the change */

 `Everything is relative'. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * vDSO implementation for Hexagon

 *

 * Copyright (c) 2011, The Linux Foundation. All rights reserved.

/* Create a vDSO page holding the signal trampoline.

 * We want this for a non-executable stack.

	/* Install the signal trampoline; currently looks like this:

	 *	r6 = #__NR_rt_sigreturn;

	 *	trap0(#1);

/*

 * Called from binfmt_elf.  Create a VMA for the vDSO page.

 Try to get it loaded right near ld.so/glibc. */

 MAYWRITE to allow gdb to COW and set breakpoints. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 1996 David S. Miller

 * Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002, 2003 Ralf Baechle

 * Copyright (C) 1999, 2000 Silicon Graphics, Inc.

 * Kevin Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com

 * Copyright (C) 2000 MIPS Technologies, Inc.

 *

 * Copyright (c) 2010-2012, The Linux Foundation. All rights reserved.

/*  This file is used to produce asm/linkerscript constants from header

  might get these from somewhere else.  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * I/O access functions for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

  These are all FIFO routines!  */

/*

 * __raw_readsw - read words a short at a time

 * @addr:  source address

 * @data:  data address

 * @len: number of shorts to read

/*

 * __raw_writesw - read words a short at a time

 * @addr:  source address

 * @data:  data address

 * @len: number of shorts to read

  Pretty sure len is pre-adjusted for the length of the access already */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Checksum functions for Hexagon

 *

 * Copyright (c) 2010-2011, The Linux Foundation. All rights reserved.

  This was derived from arch/alpha/lib/checksum.c  */

  Vector value operations  */

 optimized HEXAGON V3 intrinsic version */

/*

 * computes the checksum of the TCP/UDP pseudo-header

 * returns a 16-bit checksum, already complemented.

	/* Fold down to 32-bits so we don't lose in the typedef-less

 64 to 33 */

 33 to 32 */

/*

 * Do a 64-bit checksum on an arbitrary memory area..

 *

 * This isn't a great routine, but it's not _horrible_ either. The

 * inner loop could be unrolled a bit further, and there are better

 * ways to do the carry, but this is reasonable.

 optimized HEXAGON intrinsic version, with over read fixed */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

 Mark this VCPU never ran */

 Setup ISA features available to VCPU */

 Setup reset state of shadow SSTATUS and HSTATUS CSRs */

 Setup VCPU timer */

 Reset VCPU */

 Cleanup VCPU timer */

 Flush the pages pre-allocated for Stage2 page table mappings */

 Read current HVIP and VSIE CSRs */

 Sync-up HVIP.VSSIP bit changes does by Guest */

 TODO; To be implemented later. */

				/*

				 * Awaken to handle a signal, request to

				 * sleep again later.

 Mark this VCPU ran at least once */

 Process MMIO value returned from user-space */

 Process SBI value returned from user-space */

 Check conditions before entering the guest */

		/*

		 * Exit if we have a signal pending so that we can deliver

		 * the signal to user space.

		/*

		 * Ensure we set mode to IN_GUEST_MODE after we disable

		 * interrupts and before the final VCPU requests check.

		 * See the comment in kvm_vcpu_exiting_guest_mode() and

		 * Documentation/virt/kvm/vcpu-requests.rst

		/*

		 * We might have got VCPU interrupts updated asynchronously

		 * so update it in HW.

 Update HVIP CSR for current CPU */

		/*

		 * Save SCAUSE, STVAL, HTVAL, and HTINST because we might

		 * get an interrupt between __kvm_riscv_switch_to() and

		 * local_irq_enable() which can potentially change CSRs.

 Syncup interrupts state with HW */

		/*

		 * We may have taken a host interrupt in VS/VU-mode (i.e.

		 * while executing the guest). This interrupt is still

		 * pending, as we haven't serviced it yet!

		 *

		 * We're now back in HS-mode with interrupts disabled

		 * so enabling the interrupts now will have the effect

		 * of taking the interrupt again, in HS-mode this time.

		/*

		 * We do local_irq_enable() before calling guest_exit() so

		 * that if a timer interrupt hits while running the guest

		 * we account that tick as being spent in the guest. We

		 * enable preemption after calling guest_exit() so that if

		 * we get preempted we make sure ticks after that is not

		 * counted as guest time.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Atish Patra <atish.patra@wdc.com>

 *     Anup Patel <anup.patel@wdc.com>

 No need to check host sstatus as it can be modified outside */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

 Redirect trap to Guest VCPU */

 Determine trapped instruction */

		/*

		 * Bit[0] == 1 implies trapped instruction value is

		 * transformed instruction or custom instruction.

		/*

		 * Bit[0] == 0 implies trapped instruction value is

		 * zero or special value.

 Redirect trap if we failed to read instruction */

 Decode length of MMIO and shift */

 Fault address should be aligned to length of MMIO */

 Save instruction decode info */

 Update MMIO details in kvm_run struct */

 Try to handle MMIO access in the kernel */

 Successfully handled MMIO access in the kernel so resume */

 Exit to userspace for MMIO emulation */

 Determine trapped instruction */

		/*

		 * Bit[0] == 1 implies trapped instruction value is

		 * transformed instruction or custom instruction.

		/*

		 * Bit[0] == 0 implies trapped instruction value is

		 * zero or special value.

 Redirect trap if we failed to read instruction */

 Fault address should be aligned to length of MMIO */

 Save instruction decode info */

 Copy data to kvm_run instance */

 Update MMIO details in kvm_run struct */

 Try to handle MMIO access in the kernel */

 Successfully handled MMIO access in the kernel so resume */

 Exit to userspace for MMIO emulation */

/**

 * kvm_riscv_vcpu_unpriv_read -- Read machine word from Guest memory

 *

 * @vcpu: The VCPU pointer

 * @read_insn: Flag representing whether we are reading instruction

 * @guest_addr: Guest address to read

 * @trap: Output pointer to trap details

		/*

		 * HLVX.HU instruction

		 * 0110010 00011 rs1 100 rd 1110011

			/*

			 * HLVX.HU %[val], (%[addr])

			 * HLVX.HU t0, (t2)

			 * 0110010 00011 00111 100 00101 1110011

			/*

			 * HLVX.HU %[tmp], (%[addr])

			 * HLVX.HU t1, (t2)

			 * 0110010 00011 00111 100 00110 1110011

		/*

		 * HLV.D instruction

		 * 0110110 00000 rs1 100 rd 1110011

		 *

		 * HLV.W instruction

		 * 0110100 00000 rs1 100 rd 1110011

			/*

			 * HLV.D %[val], (%[addr])

			 * HLV.D t0, (t2)

			 * 0110110 00000 00111 100 00101 1110011

			/*

			 * HLV.W %[val], (%[addr])

			 * HLV.W t0, (t2)

			 * 0110100 00000 00111 100 00101 1110011

/**

 * kvm_riscv_vcpu_trap_redirect -- Redirect trap to Guest

 *

 * @vcpu: The VCPU pointer

 * @trap: Trap details

 Change Guest SSTATUS.SPP bit */

 Change Guest SSTATUS.SPIE bit */

 Clear Guest SSTATUS.SIE bit */

 Update Guest SSTATUS */

 Update Guest SCAUSE, STVAL, and SEPC */

 Set Guest PC to Guest exception vector */

/**

 * kvm_riscv_vcpu_mmio_return -- Handle MMIO loads after user space emulation

 *			     or in-kernel IO emulation

 *

 * @vcpu: The VCPU pointer

 * @run:  The VCPU run struct containing the mmio data

 Move to next instruction */

/*

 * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on

 * proper exit to userspace.

 If we got host interrupt then do nothing */

 Handle guest traps */

 Print details in-case of error */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Atish Patra <atish.patra@wdc.com>

 Handle SBI return only once */

 Update return values */

 Move to next instruction */

		/*

		 * The CONSOLE_GETCHAR/CONSOLE_PUTCHAR SBI calls cannot be

		 * handled in kernel so we forward these to user-space

 Return error for unsupported SBI calls */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

 Figure-out number of VMID bits in HW */

 We polluted local TLB so flush all guest TLB */

 We don't use VMID bits if they are not sufficient */

 Mark the initial VMID and VMID version invalid */

	/*

	 * We need to re-check the vmid_version here to ensure that if

	 * another vcpu already allocated a valid vmid for this vm.

 First user of a new VMID version? */

		/*

		 * We ran out of VMIDs so we increment vmid_version and

		 * start assigning VMIDs from 1.

		 *

		 * This also means existing VMIDs assignement to all Guest

		 * instances is invalid and we have force VMID re-assignement

		 * for all Guest instances. The Guest instances that were not

		 * running will automatically pick-up new VMIDs because will

		 * call kvm_riscv_stage2_vmid_update() whenever they enter

		 * in-kernel run loop. For Guest instances that are already

		 * running, we force VM exits on all host CPUs using IPI and

		 * flush all Guest TLBs.

 Request stage2 page table update for all VCPUs */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Atish Patra <atish.patra@wdc.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

	/*

	 * TODO: Instead of cpu_online_mask, we should only target CPUs

	 * where the Guest/VM is running.

	/*

	 * A RISC-V implementation can choose to either:

	 * 1) Update 'A' and 'D' PTE bits in hardware

	 * 2) Generate page fault when 'A' and/or 'D' bits are not set

	 *    PTE so that software can update these bits.

	 *

	 * We support both options mentioned above. To achieve this, we

	 * always set 'A' and 'D' PTE bits at time of creating stage2

	 * mapping. To support KVM dirty page logging with both options

	 * mentioned above, we will write-protect stage2 PTEs to track

	 * dirty pages.

 Nothing */

 Clear/Unmap */

 Write-protect */

		/*

		 * If the range is too large, release the kvm->mmu_lock

		 * to prevent starvation and lockup detector warnings.

	/*

	 * At this point memslot has been committed and there is an

	 * allocated dirty_bitmap[], dirty pages will be tracked while

	 * the memory slot is write protected.

	/*

	 * Prevent userspace from creating a memory region outside of the GPA

	 * space addressable by the KVM guest GPA space.

	/*

	 * A memory region could potentially cover multiple VMAs, and

	 * any holes between them, so iterate over all of them to find

	 * out if we can map any of them right now.

	 *

	 *     +--------------------------------------------+

	 * +---------------+----------------+   +----------------+

	 * |   : VMA 1     |      VMA 2     |   |    VMA 3  :    |

	 * +---------------+----------------+   +----------------+

	 *     |               memory region                |

	 *     +--------------------------------------------+

		/*

		 * Mapping a read-only VMA is only allowed if the

		 * memory region is configured as read-only.

 Take the intersection of this VMA with the memory region */

 IO region dirty page logging not allowed */

 We need minimum second+third level pages */

	/*

	 * If logging is active then we allow writable pages only

	 * for write faults.

 Try Sv48x4 stage2 mode */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 *

 * Authors:

 *     Anup Patel <anup.patel@wdc.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2017 SiFive

 * Copyright (C) 2021 Western Digital Corporation or its affiliates.

	/*

	 * Iterate over the set of reserved CONTEXT looking for a match.

	 * If we find one, then we can update our mm to use new CONTEXT

	 * (i.e. the same CONTEXT in the current_version) but we can't

	 * exit the loop early, since we need to ensure that all copies

	 * of the old CONTEXT are updated to reflect the mm. Failure to do

	 * so could result in us missing the reserved CONTEXT in a future

	 * version.

 Must be called with context_lock held */

 Update the list of reserved ASIDs and the ASID bitmap. */

 Mark already active ASIDs as used */

		/*

		 * If this CPU has already been through a rollover, but

		 * hasn't run another task in the meantime, we must preserve

		 * its reserved CONTEXT, as this is the only trace we have of

		 * the process it is still running.

 Mark ASID #0 as used because it is used at boot-time */

 Queue a TLB invalidation for each CPU on next context-switch */

 Must be called with context_lock held */

		/*

		 * If our current CONTEXT was active during a rollover, we

		 * can continue to use it and this was just a false alarm.

		/*

		 * We had a valid CONTEXT in a previous life, so try to

		 * re-use it if possible.

	/*

	 * Allocate a free ASID. If we can't find one then increment

	 * current_version and flush all ASIDs.

 We're out of ASIDs, so increment current_version */

 Flush everything  */

 We have more ASIDs than CPUs, so this will always succeed */

	/*

	 * If our active_context is non-zero and the context matches the

	 * current_version, then we update the active_context entry with a

	 * relaxed cmpxchg.

	 *

	 * Following is how we handle racing with a concurrent rollover:

	 *

	 * - We get a zero back from the cmpxchg and end up waiting on the

	 *   lock. Taking the lock synchronises with the rollover and so

	 *   we are forced to see the updated verion.

	 *

	 * - We get a valid context back from the cmpxchg then we continue

	 *   using old ASID because __flush_context() would have marked ASID

	 *   of active_context as used and next context switch we will

	 *   allocate new context.

 Check that our ASID belongs to the current_version. */

 Switch the page table and blindly nuke entire local TLB */

 Figure-out number of ASID bits in HW */

	/*

	 * In the process of determining number of ASID bits (above)

	 * we polluted the TLB of current HART so let's do TLB flushed

	 * to remove unwanted TLB enteries.

 Pre-compute ASID details */

	/*

	 * Use ASID allocator only if number of HW ASIDs are

	 * at-least twice more than CPUs

 Nothing to do here when there is no MMU */

/*

 * When necessary, performs a deferred icache flush for the given MM context,

 * on the local CPU.  RISC-V has no direct mechanism for instruction cache

 * shoot downs, so instead we send an IPI that informs the remote harts they

 * need to flush their local instruction caches.  To avoid pathologically slow

 * behavior in a common case (a bunch of single-hart processes on a many-hart

 * machine, ie 'make -j') we avoid the IPIs for harts that are not currently

 * executing a MM context and instead schedule a deferred local instruction

 * cache flush to be performed before execution resumes on each hart.  This

 * actually performs that local instruction cache flush, which implicitly only

 * refers to the current hart.

 *

 * The "cpu" argument must be the current local CPU number.

		/*

		 * Ensure the remote hart's writes are visible to this hart.

		 * This pairs with a barrier in flush_icache_mm.

	/*

	 * Mark the current MM context as inactive, and the next as

	 * active.  This is at least used by the icache flushing

	 * routines in order to determine who should be flushed.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Andes Technology Corporation

 init for swapper_pg_dir */

	/*

	 * Wait for the whole PGD to be populated before setting the PGD in

	 * the page table, otherwise, if we did set the PGD before populating

	 * it entirely, memblock could allocate a page at a physical address

	 * where KASAN is not populated yet and then we'd get a page fault.

		/*

		 * pgdp can't be none since kasan_early_init initialized all KASAN

		 * shadow region with kasan_early_shadow_pmd: if this is stillthe case,

		 * that means we can try to allocate a hugepage as a replacement.

 Populate the linear mapping */

 Populate kernel, BPF, modules mapping */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 SiFive

/*

 * The page dumper groups page table entries of the same type into a single

 * description. It uses pg_state to track the range information while

 * iterating over the pte entries. When the continuity is broken it then

 * dumps out a description of the range.

 Address marker */

 Private information for debugfs */

 Page Table Entry */

 Page Level */

 pgd */

 p4d */

 pud */

 pmd */

 pte */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2019 SiFive

 Nothing to do here */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Boundary checking aginst the kernel linear mapping space.

	/*

	 * Boundary checking aginst the kernel image mapping.

	 * __pa_symbol should only be used on kernel symbol addresses.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2019 Western Digital Corporation or its affiliates.

 * Copyright (C) 2020 FORTH-ICS/CARV

 *  Nick Kossifidis <mick@ics.forth.gr>

 CONFIG_DEBUG_VM */

 CONFIG_FLATMEM */

/*

 * The default maximal physical memory size is -PAGE_OFFSET for 32-bit kernel,

 * whereas for 64-bit kernel, the end of the virtual address space is occupied

 * by the modules/BPF/kernel mappings which reduces the available size of the

 * linear mapping.

 * Limit the memory size via mem.

	/*

	 * Reserve from the start of the kernel to the end of the kernel

	/*

	 * Make sure we align the reservation on PMD_SIZE since we will

	 * map the kernel in the linear mapping as read-only: we do not want

	 * any allocation to happen between _end and the next pmd aligned page.

	/*

	 * memblock allocator is not aware of the fact that last 4K bytes of

	 * the addressable memory can not be mapped because of IS_ERR_VALUE

	 * macro. Make sure that last 4k bytes are not usable by memblock

	 * if end of dram is equal to maximum addressable memory.  For 64-bit

	 * kernel, this problem can't happen here as the end of the virtual

	 * address space is occupied by the kernel mapping then this check must

	 * be done as soon as the kernel mapping base address is determined.

	/*

	 * If DTB is built in, no need to reserve its memblock.

	 * Otherwise, do reserve it but avoid using

	 * early_init_fdt_reserve_self() since __pa() does

	 * not work for DTB pointers that are fixmap addresses

 CONFIG_XIP_KERNEL */

	/*

	 * We only create PMD or PGD early mappings so we

	 * should never reach here with MMU disabled.

 CONFIG_XIP_KERNEL */

 Before MMU is enabled */

 Upgrade to PMD_SIZE mappings whenever possible */

 called from head.S with MMU off */

	/*

	 * In 64-bit kernel, the kernel mapping is outside the linear mapping so

	 * we must protect its linear mapping alias from being executed and

	 * written.

	 * And rodata section is marked readonly in mark_rodata_ro.

 CONFIG_STRICT_KERNEL_RWX */

/*

 * setup_vm() is called from head.S with MMU-off.

 *

 * Following requirements should be honoured for setup_vm() to work

 * correctly:

 * 1) It should use PC-relative addressing for accessing kernel symbols.

 *    To achieve this we always use GCC cmodel=medany.

 * 2) The compiler instrumentation for FTRACE will not work for setup_vm()

 *    so disable compiler instrumentation when FTRACE is enabled.

 *

 * Currently, the above requirements are honoured by using custom CFLAGS

 * for init.o in mm/Makefile.

 Map the flash resident part */

 Map the data in RAM */

/*

 * Setup a 4MB mapping that encompasses the device tree: for 64-bit kernel,

 * this means 2 PMD entries whereas for 32-bit kernel, this is only 1 PGDIR

 * entry.

	/*

	 * For 64-bit kernel, __va can't be used since it would return a linear

	 * mapping address whereas dtb_early_va will be used before

	 * setup_vm_final installs the linear mapping. For 32-bit kernel, as the

	 * kernel is mapped in the linear mapping, that makes no difference.

 Sanity check alignment and size */

	/*

	 * The last 4K bytes of the addressable memory can not be mapped because

	 * of IS_ERR_VALUE macro.

 Setup early PGD for fixmap */

 Setup fixmap PMD */

 Setup trampoline PGD and PMD */

 Setup trampoline PGD */

	/*

	 * Setup early PGD covering entire kernel which will allow

	 * us to reach paging_init(). We map all memory banks later

	 * in setup_vm_final() below.

 Setup early mapping for FDT early scan */

	/*

	 * Bootime fixmap only can handle PMD_SIZE mapping. Thus, boot-ioremap

	 * range can not span multiple pmds.

	/*

	 * Early ioremap fixmap is already created as it lies within first 2MB

	 * of fixmap region. We always map PMD_SIZE. Thus, both FIX_BTMAP_END

	 * FIX_BTMAP_BEGIN should lie in the same pmd. Verify that and warn

	 * the user if not.

	/**

	 * MMU is enabled at this point. But page table setup is not complete yet.

	 * fixmap page table alloc functions should be used at this point

 Setup swapper PGD for fixmap */

 Map all memory banks in the linear mapping */

 Map the kernel */

 Clear fixmap PTE and PMD mappings */

 Move to swapper page table */

 generic page allocation functions must be used to setup page table */

 CONFIG_MMU */

/*

 * reserve_crashkernel() - reserves memory for crash kernel

 *

 * This function reserves memory area given in "crashkernel=" kernel command

 * line parameter. The memory reserved is used by dump capture kernel when

 * primary kernel is crashing.

	/*

	 * Don't reserve a region for a crash kernel on a crash kernel

	 * since it doesn't make much sense and we have limited memory

	 * resources.

	/*

	 * Current riscv boot protocol requires 2MB alignment for

	 * RV64 and 4MB alignment for RV32 (hugepage size)

 CONFIG_KEXEC_CORE */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.

 *  Lennox Wu <lennox.wu@sunplusct.com>

 *  Chen Liqin <liqin.chen@sunplusct.com>

 * Copyright (C) 2012 Regents of the University of California

 Are we prepared to handle this kernel fault? */

	/*

	 * Oops. The kernel tried to access some bad page. We'll have to

	 * terminate things with extreme prejudice.

		/*

		 * We ran out of memory, call the OOM killer, and return the userspace

		 * (which will retry the fault, or kill us if we got oom-killed).

 Kernel mode? Handle exceptions or die */

	/*

	 * Something tried to access memory that isn't in our memory map.

	 * Fix it, but check if it's kernel or user first.

 User mode accesses just cause a SIGSEGV */

 User mode accesses just cause a SIGSEGV */

	/*

	 * Synchronize this task's top level page-table

	 * with the 'reference' page table.

	 *

	 * Do _not_ use "tsk->active_mm->pgd" here.

	 * We might be inside an interrupt in the middle

	 * of a task switch.

	/*

	 * Since the vmalloc area is global, it is unnecessary

	 * to copy individual PTEs

	/*

	 * Make sure the actual PTE exists as well to

	 * catch kernel vmalloc-area accesses to non-mapped

	 * addresses. If we don't do this, this will just

	 * silently loop forever.

	/*

	 * The kernel assumes that TLBs don't cache invalid

	 * entries, but in RISC-V, SFENCE.VMA specifies an

	 * ordering constraint, not a cache flush; it is

	 * necessary even after writing invalid entries.

/*

 * This routine handles page faults.  It determines the address and the

 * problem, and then passes it off to one of the appropriate routines.

	/*

	 * Fault-in kernel-space virtual memory on-demand.

	 * The 'reference' page table is init_mm.pgd.

	 *

	 * NOTE! We MUST NOT take any locks for this case. We may

	 * be in an interrupt or a critical region, and should

	 * only copy the information from the master page table,

	 * nothing more.

	/*

	 * Modules in 64bit kernels lie in their own virtual region which is not

	 * in the vmalloc region, but dealing with page faults in this region

	 * or the vmalloc region amounts to doing the same thing: checking that

	 * the mapping exists in init_mm.pgd and updating user page table, so

	 * just use vmalloc_fault.

 Enable interrupts if they were enabled in the parent context. */

	/*

	 * If we're in an interrupt, have no user context, or are running

	 * in an atomic region, then we must not take the fault.

	/*

	 * Ok, we have a good vm_area for this memory access, so

	 * we can handle it.

	/*

	 * If for any reason at all we could not handle the fault,

	 * make sure we exit gracefully rather than endlessly redo

	 * the fault.

	/*

	 * If we need to retry but a fatal signal is pending, handle the

	 * signal first. We do not need to release the mmap_lock because it

	 * would already be released in __lock_page_or_retry in mm/filemap.c.

		/*

		 * No need to mmap_read_unlock(mm) as we would

		 * have already released it in __lock_page_or_retry

		 * in mm/filemap.c.

 SPDX-License-Identifier: GPL-2.0

 With CONTIG_ALLOC, we can allocate gigantic pages at runtime */

 SPDX-License-Identifier: GPL-2.0

 check if the tlbflush needs to be sent to other CPUs */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 SiFive

/*

 * Performs an icache flush for the given MM context.  RISC-V has no direct

 * mechanism for instruction cache shoot downs, so instead we send an IPI that

 * informs the remote harts they need to flush their local instruction caches.

 * To avoid pathologically slow behavior in a common case (a bunch of

 * single-hart processes on a many-hart machine, ie 'make -j') we avoid the

 * IPIs for harts that are not currently executing a MM context and instead

 * schedule a deferred local instruction cache flush to be performed before

 * execution resumes on each hart.

 Mark every hart's icache as needing a flush for this MM. */

 Flush this hart's I$ now, and mark it as flushed. */

	/*

	 * Flush the I$ of other harts concurrently executing, and mark them as

	 * flushed.

		/*

		 * It's assumed that at least one strongly ordered operation is

		 * performed on this hart between setting a hart's cpumask bit

		 * and scheduling this MM context on that hart.  Sending an SBI

		 * remote message will do this, but in the case where no

		 * messages are sent we still need to order this hart's writes

		 * with flush_icache_deferred().

 CONFIG_SMP */

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.

 *  Lennox Wu <lennox.wu@sunplusct.com>

 *  Chen Liqin <liqin.chen@sunplusct.com>

 * Copyright (C) 2013 Regents of the University of California

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.

 *  Chen Liqin <liqin.chen@sunplusct.com>

 *  Lennox Wu <lennox.wu@sunplusct.com>

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2017 SiFive

		/*

		 * Restore the initial value to the FP register

		 * before starting the user program.

	/*

	 * Reset FPU state and context

	 *	frm: round to nearest, ties to even (IEEE default)

	 *	fflags: accrued exceptions cleared

 p->thread holds context to be restored by __switch_to() */

 Kernel thread */

 Supervisor/Machine, irqs on: */

 fn */

 User fork */

 Return value of fork() */

 kernel sp */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SMP initialisation and IPI support

 * Based on arch/arm64/kernel/smp.c

 *

 * Copyright (C) 2012 ARM Ltd.

 * Copyright (C) 2015 Regents of the University of California

 * Copyright (C) 2017 SiFive

 This covers non-smp usecase mandated by "nosmp" option */

/*

 * C entry point for a secondary processor.

 All kernel threads share the same mm context.  */

	/*

	 * Remote TLB flushes are ignored while the CPU is offline, so emit

	 * a local TLB flush right now just in case.

	/*

	 * Disable preemption before enabling interrupts, so we don't try to

	 * schedule a CPU that hasn't actually started yet.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2008 ARM Limited

 * Copyright (C) 2014 Regents of the University of California

 task blocked in __switch_to */

 Validate frame pointer */

 Unwind stack frame */

 !CONFIG_FRAME_POINTER */

 task blocked in __switch_to */

 CONFIG_FRAME_POINTER */

 CONFIG_STACKTRACE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 CONFIG_GENERIC_BUG */

/*

 * shadow stack, handled_ kernel_ stack_ overflow(in kernel/entry.S) is used

 * to get per-cpu overflow stack(get_overflow_stack).

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.

 *  Chen Liqin <liqin.chen@sunplusct.com>

 *  Lennox Wu <lennox.wu@sunplusct.com>

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2020 FORTH-ICS/CARV

 *  Nick Kossifidis <mick@ics.forth.gr>

/*

 * The lucky hart to first increment this variable will boot the other cores.

 * This is used before the kernel initializes the BSS so it can't be in the

 * BSS.

/*

 * Place kernel memory regions on the resource tree so that

 * kexec-tools can retrieve them from /proc/iomem. While there

 * also add "System RAM" regions for compatibility with other

 * archs, and the rest of the known regions for completeness.

	/*

	 * The memory region of the kernel image is continuous and

	 * was reserved on setup_bootmem, register it here as a

	 * resource, with the various segments of the image as

	 * child nodes.

 + 1 as memblock_alloc() might increase memblock.reserved.cnt */

	/*

	 * Start by adding the reserved regions, if they overlap

	 * with /memory regions, insert_resource later on will take

	 * care of it.

		/*

		 * Ignore any other reserved regions within

		 * system memory.

 Re-use this pre-allocated resource */

 Add /memory regions to the resource tree */

 Clean-up any unused pre-allocated resources */

 Better an empty resource tree than an inconsistent one */

 Early scan of device tree from init memory */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Western Digital Corporation or its affiliates.

 * Adapted from arch/arm64/kernel/efi.c

/*

 * Only regions of type EFI_RUNTIME_SERVICES_CODE need to be

 * executable, everything else can be mapped with the XN bits

 * set. Also take the new (optional) RO/XP bits into account.

 R-- */

 R-X */

 RW- */

 RWX */

 RISC-V maps one page at a time */

	/*

	 * Calling apply_to_page_range() is only safe on regions that are

	 * guaranteed to be mapped down to pages. Since we are only called

	 * for regions that have been mapped using efi_create_mapping() above

	 * (and this is checked by the generic Memory Attributes table parsing

	 * routines), there is no need to check that again here.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SBI initialilization and all extension implementation.

 *

 * Copyright (c) 2020 Western Digital Corporation or its affiliates.

 default SBI version is 0.1 */

/**

 * sbi_console_putchar() - Writes given character to the console device.

 * @ch: The data to be written to the console.

 *

 * Return: None

/**

 * sbi_console_getchar() - Reads a byte from console device.

 *

 * Returns the value read from console.

/**

 * sbi_shutdown() - Remove all the harts from executing supervisor code.

 *

 * Return: None

/**

 * sbi_clear_ipi() - Clear any pending IPIs for the calling hart.

 *

 * Return: None

/**

 * __sbi_set_timer_v01() - Program the timer for next timer event.

 * @stime_value: The value after which next timer event should fire.

 *

 * Return: None

 v0.2 function IDs are equivalent to v0.1 extension IDs */

 CONFIG_RISCV_SBI_V01 */

/**

 * sbi_set_timer() - Program the timer for next timer event.

 * @stime_value: The value after which next timer event should fire.

 *

 * Return: None.

/**

 * sbi_send_ipi() - Send an IPI to any hart.

 * @hart_mask: A cpu mask containing all the target harts.

 *

 * Return: 0 on success, appropriate linux error code otherwise.

/**

 * sbi_remote_fence_i() - Execute FENCE.I instruction on given remote harts.

 * @hart_mask: A cpu mask containing all the target harts.

 *

 * Return: 0 on success, appropriate linux error code otherwise.

/**

 * sbi_remote_sfence_vma() - Execute SFENCE.VMA instructions on given remote

 *			     harts for the specified virtual address range.

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the virtual address

 * @size: Total size of the virtual address range.

 *

 * Return: 0 on success, appropriate linux error code otherwise.

/**

 * sbi_remote_sfence_vma_asid() - Execute SFENCE.VMA instructions on given

 * remote harts for a virtual address range belonging to a specific ASID.

 *

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the virtual address

 * @size: Total size of the virtual address range.

 * @asid: The value of address space identifier (ASID).

 *

 * Return: 0 on success, appropriate linux error code otherwise.

/**

 * sbi_remote_hfence_gvma() - Execute HFENCE.GVMA instructions on given remote

 *			   harts for the specified guest physical address range.

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the guest physical address

 * @size: Total size of the guest physical address range.

 *

 * Return: None

/**

 * sbi_remote_hfence_gvma_vmid() - Execute HFENCE.GVMA instructions on given

 * remote harts for a guest physical address range belonging to a specific VMID.

 *

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the guest physical address

 * @size: Total size of the guest physical address range.

 * @vmid: The value of guest ID (VMID).

 *

 * Return: 0 if success, Error otherwise.

/**

 * sbi_remote_hfence_vvma() - Execute HFENCE.VVMA instructions on given remote

 *			     harts for the current guest virtual address range.

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the current guest virtual address

 * @size: Total size of the current guest virtual address range.

 *

 * Return: None

/**

 * sbi_remote_hfence_vvma_asid() - Execute HFENCE.VVMA instructions on given

 * remote harts for current guest virtual address range belonging to a specific

 * ASID.

 *

 * @hart_mask: A cpu mask containing all the target harts.

 * @start: Start of the current guest virtual address

 * @size: Total size of the current guest virtual address range.

 * @asid: The value of address space identifier (ASID).

 *

 * Return: None

/**

 * sbi_probe_extension() - Check if an SBI extension ID is supported or not.

 * @extid: The extension ID to be probed.

 *

 * Return: Extension specific nonzero value f yes, -ENOTSUPP otherwise.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2020 Western Digital Corporation or its affiliates.

	/*

	 * In this protocol, all cpus boot on their own accord.  _start

	 * selects the first cpu to boot the kernel and causes the remainder

	 * of the cpus to spin in a loop waiting for their stack pointer to be

	 * setup by that main cpu.  Writing to bootdata

	 * (i.e __cpu_up_stack_pointer) signals to the spinning cpus that they

	 * can continue the boot process.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 SiFive

	/*

	 * Using raw_smp_processor_id() elides a preemptability check, but this

	 * is really indicative of a larger problem: the cacheinfo UABI assumes

	 * that cores have a homonogenous view of the cache hierarchy.  That

	 * happens to be the case for the current set of RISC-V systems, but

	 * likely won't be true in general.  Since there's no way to provide

	 * correct information for these systems via the current UABI we're

	 * just eliding the check for now.

	/*

	 * If the cache is fully associative, there is no need to

	 * check the other properties.

	/*

	 * Set the ways number for n-ways associative, make sure

	 * all properties are big than zero.

 Level 1 caches in cpu node */

 Next level caches in cache nodes */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2014 Darius Rad <darius@bluespec.com>

 * Copyright (C) 2017 SiFive

	/*

	 * Note that the shift for mmap2 is constant (12),

	 * regardless of PAGE_SIZE

 !CONFIG_64BIT */

/*

 * Allows the instruction cache to be flushed from userspace.  Despite RISC-V

 * having a direct 'fence.i' instruction available to userspace (which we

 * can't trap!), that's not actually viable when running on Linux because the

 * kernel might schedule a process on another hart.  There is no way for

 * userspace to handle this without invoking the kernel (as it doesn't know the

 * thread->hart mappings), so we've defined a RISC-V specific system call to

 * flush the instruction cache.

 *

 * sys_riscv_flush_icache() is defined to flush the instruction cache over an

 * address range, with the flush applying to either all threads or just the

 * caller.  We don't currently do anything with the address range, that's just

 * in there for forwards compatibility.

 Check the reserved flags. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2017 SiFive

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Zihao Yu

/*

 * Assembly functions that may be used (directly or indirectly) by modules

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2019 FORTH-ICS/CARV

 *  Nick Kossifidis <mick@ics.forth.gr>

 For riscv_kexec_* symbol defines */

 For smp_send_stop () */

 For local_flush_icache_all() */

 For smp_wmb() */

 For PAGE_MASK */

 For fdt_check_header() */

 For set_memory_x() */

 For unreachable() */

 For cpu_down() */

/*

 * kexec_image_info - Print received image details

/*

 * machine_kexec_prepare - Initialize kexec

 *

 * This function is called from do_kexec_load, when the user has

 * provided us with an image to be loaded. Its goal is to validate

 * the image and prepare the control code buffer as needed.

 * Note that kimage_alloc_init has already been called and the

 * control buffer has already been allocated.

 Find the Flattened Device Tree and save its physical address */

 Copy the assembler code for relocation to the control page */

 Mark the control page executable */

/*

 * machine_kexec_cleanup - Cleanup any leftovers from

 *			   machine_kexec_prepare

 *

 * This function is called by kimage_free to handle any arch-specific

 * allocations done on machine_kexec_prepare. Since we didn't do any

 * allocations there, this is just an empty function. Note that the

 * control buffer is freed by kimage_free.

/*

 * machine_shutdown - Prepare for a kexec reboot

 *

 * This function is called by kernel_kexec just before machine_kexec

 * below. Its goal is to prepare the rest of the system (the other

 * harts and possibly devices etc) for a kexec reboot.

	/*

	 * No more interrupts on this hart

	 * until we are back up.

/*

 * machine_crash_shutdown - Prepare to kexec after a kernel crash

 *

 * This function is called by crash_kexec just before machine_kexec

 * below and its goal is similar to machine_shutdown, but in case of

 * a kernel crash. Since we don't handle such cases yet, this function

 * is empty.

/*

 * machine_kexec - Jump to the loaded kimage

 *

 * This function is called by kernel_kexec which is called by the

 * reboot system call when the reboot cmd is LINUX_REBOOT_CMD_KEXEC,

 * or by crash_kernel which is called by the kernel's arch-specific

 * trap handler in case of a kernel panic. It's the final stage of

 * the kexec process where the pre-loaded kimage is ready to be

 * executed. We assume at this point that all other harts are

 * suspended and this hart will be the new boot hart.

 Make sure the relocation code is visible to the hart */

 Jump to the relocation code */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

/*

 * Returns the hart ID of the given device tree node, or -ENODEV if the node

 * isn't an enabled and valid RISC-V hart node.

/*

 * Find hart ID of the CPU DT node under which given DT node falls.

 *

 * To achieve this, we walk up the DT tree until we find an active

 * RISC-V core (HART) node and extract the cpuid from it.

 Print the entire ISA as it is */

 CONFIG_PROC_FS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Emil Renner Berthing

 *

 * Based on arch/arm64/kernel/jump_label.c

	/*

	 * We use the same instructions in the arch_static_branch and

	 * arch_static_branch_jump inline functions, so there's no

	 * need to patch them up here.

	 * The core will call arch_jump_label_transform  when those

	 * instructions need to be replaced.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2013 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

 * Copyright (C) 2017 Andes Technology Corporation

 we expect nops at the hook position */

	/*

	 * Read the text we want to modify;

	 * return must be -EFAULT on read error

	/*

	 * Make sure it is what we expect it to be;

	 * return must be -EINVAL on failed comparison

 Replace the auipc-jalr pair at once. Return -EPERM on write error. */

/*

 * Put 5 instructions with 16 bytes at the front of function within

 * patchable function entry nops' area.

 *

 * 0: REG_S  ra, -SZREG(sp)

 * 1: auipc  ra, 0x?

 * 2: jalr   -?(ra)

 * 3: REG_L  ra, -SZREG(sp)

 *

 * So the opcodes is:

 * 0: 0xfe113c23 (sd)/0xfe112e23 (sw)

 * 1: 0x???????? -> auipc

 * 2: 0x???????? -> jalr

 * 3: 0xff813083 (ld)/0xffc12083 (lw)

/*

 * This is called early on, and isn't wrapped by

 * ftrace_arch_code_modify_{prepare,post_process}() and therefor doesn't hold

 * text_mutex, which triggers a lockdep failure.  SMP isn't running so we could

 * just directly poke the text, but it's simpler to just take the lock

 * ourselves.

/*

 * Most of this function is copied from arm64.

	/*

	 * We don't suffer access faults, so no extra fault-recovery assembly

	 * is needed here.

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_FUNCTION_GRAPH_TRACER */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copied from arch/arm64/kernel/cpufeature.c

 *

 * Copyright (C) 2015 ARM Ltd.

 * Copyright (C) 2017 SiFive

 Host ISA bitmap */

/**

 * riscv_isa_extension_base() - Get base extension word

 *

 * @isa_bitmap: ISA bitmap to use

 * Return: base extension word as unsigned long value

 *

 * NOTE: If isa_bitmap is NULL then Host ISA bitmap will be used.

/**

 * __riscv_isa_extension_available() - Check whether given extension

 * is available or not

 *

 * @isa_bitmap: ISA bitmap to use

 * @bit: bit position of the desired extension

 * Return: true or false

 *

 * NOTE: If isa_bitmap is NULL then Host ISA bitmap will be used.

			/*

			 * TODO: X, Y and Z extension parsing for Host ISA

			 * bitmap will be added in-future.

		/*

		 * All "okay" hart should have same isa. Set HWCAP based on

		 * common capabilities of every "okay" hart, in case they don't

		 * have.

	/* We don't support systems with F but without D, so mask those out

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HSM extension and cpu_ops implementation.

 *

 * Copyright (c) 2020 Western Digital Corporation or its affiliates.

 SPDX-License-Identifier: GPL-2.0 */

/*

 * Copyright (C) 2008 Thomas Gleixner <tglx@linutronix.de>

 * Copyright (C) 2008-2009 Red Hat, Inc., Ingo Molnar

 * Copyright (C) 2009 Jaswinder Singh Rajput

 * Copyright (C) 2009 Advanced Micro Devices, Inc., Robert Richter

 * Copyright (C) 2008-2009 Red Hat, Inc., Peter Zijlstra

 * Copyright (C) 2009 Intel Corporation, <markus.t.metzger@intel.com>

 * Copyright (C) 2009 Google, Inc., Stephane Eranian

 * Copyright 2014 Tilera Corporation. All Rights Reserved.

 * Copyright (C) 2018 Andes Technology Corporation

 *

 * Perf_events support for RISC-V platforms.

 *

 * Since the spec. (as of now, Priv-Spec 1.10) does not provide enough

 * functionality for perf event to fully work, this file provides

 * the very basic framework only.

 *

 * For platform portings, please check Documentations/riscv/pmu.txt.

 *

 * The Copyright line includes x86 and tile ones.

/*

 * Hardware & cache maps and their methods

/*

 * Low-level functions: reading/writing counters

 currently not supported */

/*

 * pmu->read: read and update the counter

 *

 * Other architectures' implementation often have a xxx_perf_event_update

 * routine, which can return counter values when called in the IRQ, but

 * return void when being called by the pmu->read method.

	/*

	 * delta is the value to update the counter we maintain in the kernel.

	/*

	 * Something like local64_sub(delta, &hwc->period_left) here is

	 * needed if there is an interrupt for perf.

/*

 * State transition functions:

 *

 * stop()/start() & add()/del()

/*

 * pmu->stop: stop the counter

/*

 * pmu->start: start the event.

		/*

		 * Set the counter to the period to the next interrupt here,

		 * if you have any.

	/*

	 * Since we cannot write to counters, this serves as an initialization

	 * to the delta-mechanism in pmu->read(); otherwise, the delta would be

	 * wrong when pmu->read is called for the first time.

/*

 * pmu->add: add the event to PMU.

	/*

	 * We don't have general conunters, so no binding-event-to-counter

	 * process here.

	 *

	 * Indexing using hwc->config generally not works, since config may

	 * contain extra information, but here the only info we have in

	 * hwc->config is the event index.

/*

 * pmu->del: delete the event from PMU.

/*

 * Interrupt: a skeletion for reference.

/*

 * Event Initialization/Finalization

	/*

	 * idx is set to -1 because the index of a general event should not be

	 * decided until binding to some counter in pmu->add().

	 *

	 * But since we don't have such support, later in pmu->add(), we just

	 * use hwc->config as the index instead.

/*

 * Initialization

 This means this PMU has no IRQ. */

 sentinel value */ }

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Western Digital Corporation or its affiliates.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SMP initialisation and IPI support

 * Based on arch/arm64/kernel/smp.c

 *

 * Copyright (C) 2012 ARM Ltd.

 * Copyright (C) 2015 Regents of the University of California

 * Copyright (C) 2017 SiFive

 A collection of single bit ipi messages.  */

 Unsupported */

 Order bit clearing and data access. */

 Order data access and bit testing. */

 Wait up to one second for other CPUs to stop */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.

 *  Chen Liqin <liqin.chen@sunplusct.com>

 *  Lennox Wu <lennox.wu@sunplusct.com>

 * Copyright (C) 2012 Regents of the University of California

 We support no other extension state at this time. */

 We support no other extension state at this time. */

 sc_regs is structured the same as the start of pt_regs */

 Restore the floating-point state. */

 Always make any pending restarted system calls return -EINTR */

 sc_regs is structured the same as the start of pt_regs */

 Save the floating-point state. */

 Default to using normal stack */

	/*

	 * If we are on the alternate signal stack and would overflow it, don't.

	 * Return an always-bogus address instead so we will die with SIGSEGV.

 This is the X/Open sanctioned signal stack switching. */

 Align the stack frame. */

 Create the ucontext. */

 Set up to return from userspace. */

	/*

	 * For the nommu case we don't have a VDSO.  Instead we push two

	 * instructions to call the rt_sigreturn syscall onto the user stack.

 CONFIG_MMU */

	/*

	 * Set up registers for signal handler.

	 * Registers that we don't modify keep the value they had from

	 * user-space at the time we took the signal.

	 * We always pass siginfo and mcontext, regardless of SA_SIGINFO,

	 * since some things rely on this (e.g. glibc's debug/segfault.c).

 a0: signal number */

 a1: siginfo pointer */

 a2: ucontext pointer */

 Are we from a system call? */

 Avoid additional syscall restarting via ret_from_exception */

 If so, check system call restarting.. */

 Set up the stack frame */

 Actually deliver the signal */

 Did we come from a system call? */

 Avoid additional syscall restarting via ret_from_exception */

 Restart the system call - no handlers present */

	/*

	 * If there is no signal to deliver, we just put the saved

	 * sigmask back.

/*

 * notification of userspace execution resumption

 * - triggered by the _TIF_WORK_MASK flags

 Handle pending signal delivery */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 SiFive

 C.JAL is an RV32C-only instruction */

 Calculate the new address for after a step */

 Determine where the target instruction will send us to */

 Store the op code in the stepped address */

 Replace the op code with the break instruction */

 Flush and return */

 Undo a single step */

 Initialize to zero */

/*

 * Global data

 c.ebreak */

 ebreak */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2010 Tilera Corporation. All Rights Reserved.

 * Copyright 2015 Regents of the University of California

 * Copyright 2017 SiFive

 *

 * Copied from arch/tile/kernel/ptrace.c

 explicitly pad

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_within_kernel_stack() - check the address in the stack

 * @regs:      pt_regs which contains kernel stack pointer.

 * @addr:      address which is checked.

 *

 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).

 * If @addr is within the kernel stack, it returns true. If not, returns false.

/**

 * regs_get_kernel_stack_nth() - get Nth entry of the stack

 * @regs:	pt_regs which contains kernel stack pointer.

 * @n:		stack entry number.

 *

 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which

 * is specified by @regs. If the @n th entry is NOT in the kernel stack,

 * this returns 0.

/*

 * Allows PTRACE_SYSCALL to work.  These are called from entry.S in

 * {handle,ret_from}_syscall.

	/*

	 * Do the secure computing after ptrace; failures should be fast.

	 * If this fails we might have return value in a0 from seccomp

	 * (via SECCOMP_RET_ERRNO/TRACE).

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Western Digital Corporation or its affiliates.

/*

 * __cpu_disable runs on the processor to be shutdown.

/*

 * Called on the thread which is asking for a CPU to be shutdown.

 Verify from the firmware if the cpu is really stopped*/

/*

 * Called from the idle thread for the CPU which has been shutdown.

 It should never reach here */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Hangzhou C-SKY Microsystems co.,ltd. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2020 Western Digital Corporation or its affiliates.

 Make sure tidle is updated */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 SiFive

/*

 * The fix_to_virt(, idx) needs a const value (not a dynamic variable of

 * reg-a0) or BUILD_BUG_ON failed with "idx >= __end_of_fixed_addresses".

 * So use '__always_inline' and 'const unsigned int fixmap' here.

	/*

	 * Before reaching here, it was expected to lock the text_mutex

	 * already, so we don't need to give another lock here and could

	 * ensure that it was safe between each cores.

 CONFIG_MMU */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Copyright (C) 2020 Western Digital Corporation or its affiliates.

/*

 * This is called extremly early, before parse_dtb(), to allow initializing

 * SoC hardware before memory or any device driver initialization.

 SPDX-License-Identifier: GPL-2.0

/*

 * This code comes from arch/arm64/kernel/crash_dump.c

 * Created by: AKASHI Takahiro <takahiro.akashi@linaro.org>

 * Copyright (C) 2017 Linaro Limited

/**

 * copy_oldmem_page() - copy one page from old kernel memory

 * @pfn: page frame number to be copied

 * @buf: buffer where the copied page is placed

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page

 * @userbuf: if set, @buf is in a user address space

 *

 * This function copies one page from old kernel memory into buffer pointed by

 * @buf. If @buf is in userspace, set @userbuf to %1. Returns number of bytes

 * copied or negative error in case of failure.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 *

 *  Copyright (C) 2017 Zihao Yu

	/*

	 * v is the lo12 value to fill. It is calculated before calling this

	 * handler.

	/*

	 * v is the lo12 value to fill. It is calculated before calling this

	 * handler.

 Skip medlow checking because of filtering by HI20 already */

 Skip medlow checking because of filtering by HI20 already */

 Always emit the got entry */

 Only emit the plt entry if offset over 32-bit range */

 This is where to make the change */

 This is the symbol it is referring to */

 Ignore unresolved weak symbol */

 Find the corresponding HI20 relocation entry */

 Calculate lo12 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2004 Benjamin Herrenschmidt, IBM Corp.

 *                    <benh@kernel.crashing.org>

 * Copyright (C) 2012 ARM Limited

 * Copyright (C) 2015 Regents of the University of California

/*

 * The vDSO data page.

 Data Mapping */

 Code Mapping */

 Grab the vDSO code pages. */

/*

 * The vvar mapping contains data for a specific time namespace, so when a task

 * changes namespace we must unmap its vvar data for the old namespace.

 * Subsequent faults will map in data for the new namespace.

 *

 * For more details see timens_setup_vdso_data().

	/*

	 * VM_PFNMAP | VM_IO protect .fault() handler from being called

	 * through interfaces like /proc/$pid/mem or

	 * process_vm_{readv,writev}() as long as there's no .access()

	 * in special_mapping_vmops.

	 * For more details check_vma_flags() and __access_remote_vm()

		/*

		 * If a task belongs to a time namespace then a namespace

		 * specific VVAR is mapped with the VVAR_DATA_PAGE_OFFSET and

		 * the real VVAR page is mapped with the VVAR_TIMENS_PAGE_OFFSET

		 * offset.

		 * See also the comment near timens_setup_vdso_data().

 CONFIG_TIME_NS */

 Be sure to map the data page */

/* SPDX-License-Identifier: GPL-2.0

 *

 * Copyright (C) 2014-2017 Linaro Ltd. <ard.biesheuvel@linaro.org>

 *

 * Copyright (C) 2018 Andes Technology Corporation <zong@andestech.com>

 There is no duplicate entry, create a new one */

 There is no duplicate entry, create a new one */

	/*

	 * Find the empty .got and .plt sections.

 Calculate the maxinum number of entries */

 ignore relocations that operate on non-exec sections */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2009 Arnd Bergmann <arnd@arndb.de>

 * Copyright (C) 2012 Regents of the University of California

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Hangzhou C-SKY Microsystems co.,ltd. */

/*

 * Get the return address for a single stackframe and return a pointer to the

 * next frame tail.

 Check accessibility of one struct frame_tail beyond */

/*

 * This will be called when the target is in user mode

 * This function will only be called when we use

 * "PERF_SAMPLE_CALLCHAIN" in

 * kernel/events/core.c:perf_prepare_sample()

 *

 * How to trigger perf_callchain_[user/kernel] :

 * $ perf record -e cpu-clock --call-graph fp ./program

 * $ perf report --call-graph

 *

 * On RISC-V platform, the program being sampled and the C library

 * need to be compiled with -fno-omit-frame-pointer, otherwise

 * the user stack will not contain function frame.

 RISC-V does not support perf in guest mode. */

 RISC-V does not support perf in guest mode. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2017 SiFive

 * Copyright (C) 2018 Christoph Hellwig

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

 * Copyright (C) 2017 SiFive

 F extension */

 D extension */

	/*

	 * THREAD_{F,X}* might be larger than a S-type offset can handle, but

	 * these are used in performance-sensitive assembly so we can't resort

	 * to loading the long immediate every time.

	/*

	 * We allocate a pt_regs on the stack when entering the kernel.  This

	 * ensures the alignment is sane.

 SPDX-License-Identifier: GPL-2.0+

/* Return:

 *   INSN_REJECTED     If instruction is one not allowed to kprobe,

 *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.

	/*

	 * Reject instructions list:

	/*

	 * Simulate instructions list:

	 * TODO: the REJECTED ones below need to be implemented

 SPDX-License-Identifier: GPL-2.0

 Ftrace callback handler for kprobes -- called under preepmt disabled */

			/*

			 * Emulate singlestep (and also recover regs->pc)

			 * as if there is a nop

		/*

		 * If pre_handler returns !0, it changes regs->pc. We have to

		 * skip emulating post_handler.

 SPDX-License-Identifier: GPL-2.0-only

	/*

	 * Task has received a fatal signal, so reset back to probbed

	 * address.

 Initialize the slot */

 Add ebreak behind opcode to simulate singlestep */

	/*

	 * We probably need flush_icache_user_page() but it needs vma.

	 * This should work on most of architectures by default. If

	 * architecture needs to do something different it can define

	 * its own version of the function.

 SPDX-License-Identifier: GPL-2.0+

 copy instruction */

 decode instruction */

 insn not supported */

 insn need simulation */

 instruction uses slot */

 prepare the instruction */

 install breakpoint in text */

 remove breakpoint from text */

/*

 * Interrupts need to be disabled before single-step mode is set, and not

 * reenabled until after single-step mode ends.

 * Without disabling interrupt on local CPU, there is a chance of

 * interrupt occurrence in the period of exception return and  start of

 * out-of-line single-step, that result in wrongly single stepping

 * into the interrupt handler.

 prepare for single stepping */

 IRQs and single stepping do not mix well. */

 insn simulation */

 return addr restore if non-branching insn */

 restore back original saved kprobe variables and continue */

 call post handler */

		/* post_handler can hit breakpoint and single step

		 * again, so we enable D-flag for recursive exception.

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe and the ip points back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * In case the user-specified fault handler returned

		 * zero, try to fix up.

 Probe hit */

			/*

			 * If we have no pre-handler or it returned 0, we

			 * continue with normal processing.  If we have a

			 * pre-handler and it returned non-zero, it will

			 * modify the execution path and no need to single

			 * stepping. Let's just reset current kprobe and exit.

			 *

			 * pre_handler can hit a breakpoint and can step thru

			 * before return.

	/*

	 * The breakpoint instruction was removed right

	 * after we hit it.  Another cpu has removed

	 * either a probepoint or a debugger breakpoint

	 * at this address.  In either case, no further

	 * handling of this interrupt is appropriate.

	 * Return back to original instruction, and continue.

 not ours, kprobes should ignore it */

/*

 * Provide a blacklist of symbols identifying ranges which cannot be kprobed.

 * This blacklist is exposed to userspace via debugfs (kprobes/blacklist).

 SPDX-License-Identifier: GPL-2.0+

	/*

	 *     31    30       21    20     19        12 11 7 6      0

	 * imm [20] | imm[10:1] | imm[11] | imm[19:12] | rd | opcode

	 *     1         10          1           8       5    JAL/J

	/*

	 * 31          20 19 15 14 12 11 7 6      0

	 *  offset[11:0] | rs1 | 010 | rd | opcode

	 *      12         5      3    5    JALR/JR

	/*

	 * auipc instruction:

	 *  31        12 11 7 6      0

	 * | imm[31:12] | rd | opcode |

	 *        20       5     7

	/*

	 * branch instructions:

	 *      31    30       25 24 20 19 15 14    12 11       8    7      6      0

	 * | imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1] | imm[11] | opcode |

	 *     1           6        5     5      3         4         1         7

	 *     imm[12|10:5]        rs2   rs1    000       imm[4:1|11]       1100011  BEQ

	 *     imm[12|10:5]        rs2   rs1    001       imm[4:1|11]       1100011  BNE

	 *     imm[12|10:5]        rs2   rs1    100       imm[4:1|11]       1100011  BLT

	 *     imm[12|10:5]        rs2   rs1    101       imm[4:1|11]       1100011  BGE

	 *     imm[12|10:5]        rs2   rs1    110       imm[4:1|11]       1100011  BLTU

	 *     imm[12|10:5]        rs2   rs1    111       imm[4:1|11]       1100011  BGEU

 SPDX-License-Identifier: GPL-2.0

/*

 * Copied from arch/arm64/kernel/vdso/vgettimeofday.c

 *

 * Copyright (C) 2018 ARM Ltd.

 * Copyright (C) 2020 SiFive

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 Regents of the University of California

/*

 * This is copies from arch/arm/include/asm/delay.h

 *

 * Loop (or tick) based delay:

 *

 * loops = loops_per_jiffy * jiffies_per_sec * delay_us / us_per_sec

 *

 * where:

 *

 * jiffies_per_sec = HZ

 * us_per_sec = 1000000

 *

 * Therefore the constant part is HZ / 1000000 which is a small

 * fractional number. To make this usable with integer math, we

 * scale up this constant by 2^31, perform the actual multiplication,

 * and scale the result back down by 2^31 with a simple shift:

 *

 * loops = (loops_per_jiffy * delay_us * UDELAY_MULT) >> 31

 *

 * where:

 *

 * UDELAY_MULT = 2^31 * HZ / 1000000

 *             = (2^31 / 1000000) * HZ

 *             = 2147.483648 * HZ

 *             = 2147 * HZ + 483648 * HZ / 1000000

 *

 * 31 is the biggest scale shift value that won't overflow 32 bits for

 * delay_us * UDELAY_MULT assuming HZ <= 1000 and delay_us <= 2000.

/*

 * RISC-V supports both UDELAY and NDELAY.  This is largely the same as above,

 * but with different constants.  I added 10 bits to the shift to get this, but

 * the result is that I need a 64-bit multiply, which is slow on 32-bit

 * platforms.

 *

 * NDELAY_MULT = 2^41 * HZ / 1000000000

 *             = (2^41 / 1000000000) * HZ

 *             = 2199.02325555 * HZ

 *             = 2199 * HZ + 23255550 * HZ / 1000000000

 *

 * The maximum here is to avoid 64-bit overflow, but it isn't checked as it

 * won't happen.

	/*

	 * This doesn't bother checking for overflow, as it won't happen (it's

	 * an hour) of delay.

 SPDX-License-Identifier: GPL-2.0

/*

 * BPF JIT compiler for RV32G

 *

 * Copyright (c) 2020 Luke Nelson <luke.r.nels@gmail.com>

 * Copyright (c) 2020 Xi Wang <xi.wang@gmail.com>

 *

 * The code is based on the BPF JIT compiler for RV64G by Björn Töpel and

 * the BPF JIT compiler for 32-bit ARM by Shubham Bansal and Mircea Gherzan.

/*

 * Stack layout during BPF program execution:

 *

 *                     high

 *     RV32 fp =>  +----------+

 *                 | saved ra |

 *                 | saved fp | RV32 callee-saved registers

 *                 |   ...    |

 *                 +----------+ <= (fp - 4 * NR_SAVED_REGISTERS)

 *                 |  hi(R6)  |

 *                 |  lo(R6)  |

 *                 |  hi(R7)  | JIT scratch space for BPF registers

 *                 |  lo(R7)  |

 *                 |   ...    |

 *  BPF_REG_FP =>  +----------+ <= (fp - 4 * NR_SAVED_REGISTERS

 *                 |          |        - 4 * BPF_JIT_SCRATCH_REGS)

 *                 |          |

 *                 |   ...    | BPF program stack

 *                 |          |

 *     RV32 sp =>  +----------+

 *                 |          |

 *                 |   ...    | Function call stack

 *                 |          |

 *                 +----------+

 *                     low

 Stack layout - these are offsets from top of JIT scratch space. */

 Stack space for BPF_REG_6 through BPF_REG_9 and BPF_REG_AX. */

 Number of callee-saved registers stored to stack: ra, fp, s1--s7. */

 Offset from fp for BPF registers stored on stack. */

 Return value from in-kernel function, and exit value from eBPF. */

 Arguments from eBPF program to in-kernel function. */

	/*

	 * Callee-saved registers that in-kernel function will preserve.

	 * Stored on the stack.

 Read-only frame pointer to access BPF stack. */

 Temporary register for blinding constants. Stored on the stack. */

	/*

	 * Temporary registers used by the JIT to operate on registers stored

	 * on the stack. Save t0 and t1 to be used as temporaries in generated

	 * code.

 Emit immediate into lower bits. */

 Sign-extend into upper bits. */

 Set return value if not tail call. */

 Restore callee-saved registers. */

		/*

		 * goto *(t0 + 4);

		 * Skips first instruction of prologue which initializes tail

		 * call counter. Assumes t0 contains address of target program,

		 * see emit_bpf_tail_call.

 Do nothing. */

 Do nothing. */

 Do nothing. */

	/*

	 * NO_JUMP skips over the rest of the instructions and the

	 * emit_jump_and_link, meaning the BPF branch is not taken.

	 * JUMP skips directly to the emit_jump_and_link, meaning

	 * the BPF branch is taken.

	 *

	 * The fallthrough case results in the BPF branch being taken.

 Adjust for extra insns. */

		/*

		 * BPF_JSET is a special case: it has no inverse so we always

		 * treat it as a far branch.

	/*

	 * For a far branch, the condition is negated and we jump over the

	 * branch itself, and the two instructions from emit_jump_and_link.

	 * For a near branch, just use rvoff.

 Adjust for extra insns. */

 Adjust for extra insns. */

 R1-R4 already in correct registers---need to push R5 to stack. */

 Backup TCC. */

	/*

	 * Use lui/jalr pair to jump to absolute address. Don't use emit_imm as

	 * the number of emitted instructions should not depend on the value of

	 * addr.

 Restore TCC. */

 Set return value and restore stack. */

	/*

	 * R1 -> &ctx

	 * R2 -> &array

	 * R3 -> index

 max_entries = array->map.max_entries; */

	/*

	 * if (index >= max_entries)

	 *   goto out;

	/*

	 * temp_tcc = tcc - 1;

	 * if (tcc < 0)

	 *   goto out;

	/*

	 * prog = array->ptrs[index];

	 * if (!prog)

	 *   goto out;

	/*

	 * tcc = temp_tcc;

	 * goto *(prog->bpf_func + 4);

 Epilogue jumps to *(t0 + 4). */

 Only BPF_ADD supported */

 Special mov32 for zext. */

		/*

		 * mul,div,mod are handled in the BPF_X case since there are

		 * no RISC-V I-type equivalents.

		/*

		 * src is ignored---choose tmp2 as a dummy register since it

		 * is not on the stack.

 Do nothing. */

 Swap upper and lower halves. */

 Swap each half. */

 speculation barrier */

 No hardware support for 8-byte atomics in RV32. */

 Fallthrough. */

 Make space for callee-saved registers. */

 Make space for BPF registers on stack. */

 Make space for BPF stack. */

 Round up for stack alignment. */

	/*

	 * The first instruction sets the tail-call-counter (TCC) register.

	 * This instruction is skipped by tail calls.

 Save callee-save registers. */

 Set fp: used as the base address for stacked BPF registers. */

 Set up BPF frame pointer. */

 Set up BPF context pointer. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Common functionality for RV32 and RV64 BPF JIT compilers

 *

 * Copyright (c) 2019 Björn Töpel <bjorn.topel@gmail.com>

 *

 Number of iterations to try until offsets converge. */

 BPF_LD | BPF_IMM | BPF_DW: skip the next instruction. */

 obtain the actual image size */

			/*

			 * Now, when the image is allocated, the image can

			 * potentially shrink more (auipc/jalr -> jal).

 SPDX-License-Identifier: GPL-2.0

/* BPF JIT compiler for RV64G

 *

 * Copyright(c) 2019 Björn Töpel <bjorn.topel@gmail.com>

 *

 Store A6 in S6 if program do calls */

	/*

	 * auipc+jalr can reach any signed PC-relative offset in the range

	 * [-2^31 - 2^11, 2^31 - 2^11).

	/* Note that the immediate from the add is sign-extended,

	 * which means that we need to compensate this by adding 2^12,

	 * when the 12th bit is set. A simpler way of doing this, and

	 * getting rid of the check, is to just add 2**11 before the

	 * shift. The "Loading a 32-Bit constant" example from the

	 * "Computer Organization and Design, RISC-V edition" book by

	 * Patterson/Hennessy highlights this fact.

	 *

	 * This also means that we need to process LSB to MSB.

	/* Sign-extend lower 12 bits to 64 bits since immediates for li, addiw,

	 * and addi are signed and RVC checks will perform signed comparisons.

 Set return value. */

 skip TCC init */

 Adjust for jal */

	/* Transform, e.g.:

	 *   bne rd,rs,foo

	 * to

	 *   beq rd,rs,<.L1>

	 *   (auipc foo)

	 *   jal(r) foo

	 * .L1

	/* 32b No need for an additional rvoff adjustment, since we

	 * get that from the auipc at PC', where PC = PC' + 4.

	/* a0: &ctx

	 * a1: &array

	 * a2: index

	 *

	 * if (index >= array->map.max_entries)

	 *	goto out;

	/* if (TCC-- < 0)

	 *     goto out;

	/* prog = array->ptrs[index];

	 * if (!prog)

	 *     goto out;

 goto *(prog->bpf_func + 4); */

 For accesses to BTF pointers, add an entry to the exception table */

	/*

	 * Since the extable follows the program, the fixup offset is always

	 * negative and limited to BPF_JIT_REGION_SIZE. Store a positive value

	 * to keep things simple, and put the destination register in the upper

	 * bits. We don't need to worry about buildtime or runtime sort

	 * modifying the upper bits because the table is already sorted, and

	 * isn't part of the main exception table.

 dst = src */

 Special mov32 for zext */

 dst = dst OP src */

 dst = -dst */

 dst = BSWAP##imm(dst) */

 Do nothing */

 dst = imm */

 dst = dst OP imm */

 JUMP off */

 IF (dst COND src) JUMP off */

 Adjust for extra insns */

 Adjust for and */

 IF (dst COND imm) JUMP off */

 If imm is 0, simply use zero register. */

 Adjust for extra insns */

		/* For jset32, we should clear the upper 32 bits of t1, but

		 * sign-extension is sufficient here and saves one instruction,

		 * as t1 is used only in comparison against zero.

 function call */

 tail call */

 function return */

 dst = imm64 */

 LDX: dst = *(size *)(src + off) */

 speculation barrier */

 ST: *(size *)(dst + off) = imm */

 STX: *(size *)(dst + off) = src */

		/* atomic_add: lock *(u32 *)(dst + off) += src

		 * atomic_add: lock *(u64 *)(dst + off) += src

 RV_REG_FP */

	/* First instruction is always setting the tail-call-counter

	 * (TCC) register. This instruction is skipped for tail calls.

	 * Force using a 4-byte (non-compressed) instruction.

	/* Program contains calls and tail calls, so RV_REG_TCC need

	 * to be saved across calls.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * alternative runtime patching

 * inspired by the ARM64 and x86 version

 *

 * Copyright (C) 2021 Sifive.

/*

 * This is called very early in the boot process (directly after we run

 * a feature detect on the boot CPU). No need to worry about other CPUs

 * here.

 If called on non-boot cpu things could go wrong */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2021 Sifive.

	/*

	 * Affected cores:

	 * Architecture ID: 0x8000000000000007

	 * Implement ID: 0x20181004 <= impid <= 0x20191105

	/*

	 * Affected cores:

	 * Architecture ID: 0x8000000000000007 or 0x1

	 * Implement ID: mimpid[23:0] <= 0x200630 and mimpid != 0x01200626

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/* Kill off a ptraced child by all means available.  kill it normally first,

 * then PTRACE_KILL it, then PTRACE_CONT it in case it's in a run state from

 * which it can't exit directly.

/* Don't use the glibc version, which caches the result in TLS. It misses some

 * syscalls, and also breaks with clone(), which does not unshare the TLS.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Anton Ivanov (aivanov@{brocade.com,kot-begemot.co.uk})

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2012-2014 Cisco Systems

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike{addtoit,linux.intel}.com)

/**

 * os_timer_create() - create an new posix (interval) timer

 we cheat here

/**

 * os_timer_disable() - disable the posix (interval) timer

/**

 * os_idle_sleep() - sleep until interrupted

 block SIGALRM while we analyze the timer state */

 check the timer, and if it'll fire then wait for it */

 either way, restore the signal mask */

 SPDX-License-Identifier: GPL-2.0

/* Some of this are builtin function (some are not but could in the future),

 * so I *must* declare good prototypes for them and then EXPORT them.

 * The kernel code uses the macro defined by include/linux/string.h,

 * so I undef macros; the userspace code does not include that and I

 * add an EXPORT for the glibc one.

 If it's not defined, the export is included in lib/string.c.*/

/* Here, instead, I can provide a fake prototype. Yes, someone cares: genksyms.

 * However, the modules will use the CRC defined *here*, no matter if it is

 * good; so the versions of these symbols will always match

 Export symbols used by GCC for the stack protector. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Changed by set_umid, which is run early in boot */

 Changed by set_uml_dir and make_uml_dir, which are run early in boot */

/*

 * Unlinks the files contained in @dir and then removes @dir.

 * Doesn't handle directory trees, so it's not like rm -rf, but almost such. We

 * ignore ENOENT errors for anything (they happen, strangely enough - possibly

 * due to races between multiple dying UML threads).

/*

 * This says that there isn't already a user of the specified directory even if

 * there are errors during the checking.  This is because if these errors

 * happen, the directory is unusable by the pre-existing UML, so we might as

 * well take it over.  This could happen either by

 * 	the existing UML somehow corrupting its umid directory

 * 	something other than UML sticking stuff in the directory

 *	this boot racing with a shutdown of the other UML

 * In any of these cases, the directory isn't useful for anything else.

 *

 * Boolean return: 1 if in use, 0 otherwise.

/*

 * Try to remove the directory @dir unless it's in use.

 * Precondition: @dir exists.

 * Returns 0 for success, < 0 for failure in removal or if the directory is in

 * use.

 Changed in make_umid, which is called during early boot */

		/*

		 * There's a nice tiny little race between this unlink and

		 * the mkdir below.  It'd be nice if there were a mkstemp

		 * for directories.

	/*

	 * If initializing with the given umid failed, then try again with

	 * a random one.

		/*

		 * Return 0 here because do_initcalls doesn't look at

		 * the return value.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 device */

 inode */

 protection */

 number of hard links */

 user ID of owner */

 group ID of owner */

 total size, in bytes */

 blocksize for filesys I/O */

 number of blocks allocated */

 time of last access */

 time of last modification */

 time of last change */

 FIXME? required only by hostaudio (because it passes ioctls verbatim) */

 FIXME: ensure namebuf in os_get_if_name is big enough */

 currently need 2 FDs at most so avoid dynamic allocation */

 Return the index of the available FD */

 SPDX-License-Identifier: GPL-2.0

/*

 *  arch/um/kernel/elf_aux.c

 *

 *  Scan the Elf auxiliary vector provided by the host to extract

 *  information about vsyscall-page, etc.

 *

 *  Copyright (C) 2004 Fujitsu Siemens Computers GmbH

 *  Author: Bodo Stroesser (bodo.stroesser@fujitsu-siemens.com)

 These are initialized very early in boot and never changed */

 See if the page is under TASK_SIZE */

 See if the page is under TASK_SIZE */

                                /* elf.h removed the pointer elements from

                                 * a_un, so we have to use a_val, which is

                                 * all that's left.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

	/*

	 * XXX tcsetattr could have applied only some changes

	 * (and cfmakeraw() is a set of changes)

/*

 * We cannot use glibc's abort(). It makes use of tgkill() which

 * has no effect within UML's kernel threads.

 * After that glibc would execute an invalid instruction to kill

 * the calling process and UML crashes with SIGSEGV.

/*

 * UML helper threads must not handle SIGWINCH/INT/TERM

	/*

	 * We are about to SIGTERM this entire process group to ensure that

	 * nothing is around to run after the kernel exits.  The

	 * kernel wants to abort, not die through SIGTERM, so we

	 * ignore it here.

	/*

	 * Most of the other processes associated with this UML are

	 * likely sTopped, so give them a SIGCONT so they see the

	 * SIGTERM.

	/*

	 * Now, having sent signals to everyone but us, make sure they

	 * die by ptrace.  Processes can survive what's been done to

	 * them so far - the mechanism I understand is receiving a

	 * SIGSEGV and segfaulting immediately upon return.  There is

	 * always a SIGSEGV pending, and (I'm guessing) signals are

	 * processed in numeric order so the SIGTERM (signal 15 vs

	 * SIGSEGV being signal 11) is never handled.

	 *

	 * Run a waitpid loop until we get some kind of error.

	 * Hopefully, it's ECHILD, but there's not a lot we can do if

	 * it's something else.  Tell os_kill_ptraced_process not to

	 * wait for the child to report its death because there's

	 * nothing reasonable to do if that fails.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Anton Ivanov (aivanov@{brocade.com,kot-begemot.co.uk})

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2004 PathScale, Inc

 * Copyright (C) 2004 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 For segfaults, we want the data from the sigcontext. */

 enable signals if sig isn't IRQ signal */

/*

 * These are the asynchronous signals.  SIGPROF is excluded because we want to

 * be able to profile all of UML, not just the non-critical sections.  If

 * profiling is not thread-safe, then that is not my problem.  We can disable

 * profiling when SMP is enabled in that case.

		/*

		 * In TT_MODE_EXTERNAL, need to still call time-travel

		 * handlers unless signals are also blocked for the

		 * external time message processing. This will mark

		 * signals_pending by itself (only if necessary.)

		/*

		 * pending comes back with one bit set for each

		 * interrupt that arrived while setting up the stack,

		 * plus a bit for this interrupt, plus the zero bit is

		 * set if this is a nested interrupt.

		 * If bail is true, then we interrupted another

		 * handler setting up the stack.  In this case, we

		 * have to return, and the upper handler will deal

		 * with this interrupt.

		/*

		 * Again, pending comes back with a mask of signals

		 * that arrived while tearing down the stack.  If this

		 * is non-zero, we just go back, set up the stack

		 * again, and handle the new interrupts.

 block irq ones */

 if it's an irq signal */

	/*

	 * This must return with signals disabled, so this barrier

	 * ensures that writes are flushed out before the return.

	 * This might matter if gcc figures out how to inline this and

	 * decides to shuffle this code into the caller.

	/*

	 * We loop because the IRQ handler returns with interrupts off.  So,

	 * interrupts may have arrived and we need to re-enable them and

	 * recheck signals_pending.

		/*

		 * Save and reset save_pending after enabling signals.  This

		 * way, signals_pending won't be changed while we're reading it.

		 *

		 * Setting signals_enabled and reading signals_pending must

		 * happen in this order, so have the barrier here.

		/*

		 * We have pending interrupts, so disable signals, as the

		 * handlers expect them off when they are called.  They will

		 * be enabled again above. We need to trace this, as we're

		 * expected to be enabling interrupts already, but any more

		 * tracing that happens inside the handlers we call for the

		 * pending signals will mess up the tracing state.

		/*

		 * Deal with SIGIO first because the alarm handler might

		 * schedule, leaving the pending SIGIO stranded until we come

		 * back here.

		 *

		 * SIGIO's handler doesn't use siginfo or mcontext,

		 * so they can be NULL.

 Do not reenter the handler */

 Rerun the loop only if there is still pending SIGIO and not in TIMER handler */

 Re-enable signals and trace that we're doing so. */

 Must be set to 0 before we check the pending bits etc. */

 this is a bit inefficient, but that's not really important */

 we need to run time-travel handlers even if not enabled */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2004 PathScale, Inc

 * Copyright (C) 2004 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 This is set once at boot time and not changed thereafter */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2008 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * Protected by sigio_lock(), also used by sigio_cleanup, which is an

 * exitcall.

/*

 * These arrays are initialized before the sigio thread is started, and

 * the descriptors closed after it is killed.  So, it can't see them change.

 * On the UML side, they are changed under the sigio_lock.

/*

 * Protected by sigio_lock().  Used by the sigio thread, but the UML thread

 * synchronizes with it.

/*

 * Must be called with sigio_lock held, because it's needed by the marked

 * critical section.

 Critical section start */

 Critical section end */

	/*

	 * This is called from exitcalls elsewhere in UML - if

	 * sigio_cleanup has already run, then update_thread will hang

	 * or fail because the thread is no longer running.

 We call this *tons* of times - and most ones we must just fail. */

	/*

	 * Did we race? Don't try to optimize this, please, it's not so likely

	 * to happen, and no more than once at the boot.

 Changed during early boot */

 Used as a flag during SIGIO testing early in boot */

 Not now, but complain so we now where we failed. */

 Here because it only does the SIGIO testing for now */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 If the exec succeeds, we don't get here */

 Returns either the pid of the child process we run or -E* on failure. */

	/*

	 * Read the errno value from the child, if the exec failed, or get 0 if

	 * the exec succeeded because the pipe fd was set as close-on-exec.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Set by make_tempfile() during early boot. */

 Check if dir is on tmpfs. Return 0 if yes, -1 if no or error. */

/*

 * Choose the tempdir to use. We want something on tmpfs so that our memory is

 * not subject to the host's vm.dirty_ratio. If a tempdir is specified in the

 * environment, we use that even if it's not on tmpfs, but we warn the user.

 * Otherwise, we try common tmpfs locations, and if no tmpfs directory is found

 * then we fall back to /tmp.

 Make a copy since getenv results may not remain valid forever. */

/*

 * Create an unlinked tempfile in a suitable tempdir. template must be the

 * basename part of the template with a leading '/'.

	/*

	 * If the running system does not support O_TMPFILE flag then retry

	 * without it.

	/*

	 * Seek to len - 1 because writing a character there will

	 * increase the file size by one byte, to the desired length.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Calling os_getpid because some libcs cached getpid incorrectly */

	/*

	 * This syscall will be intercepted by the parent. Don't call more than

	 * once, please.

 Nothing modified by the parent, we are running normally. */

		/*

		 * Expected in check_ptrace and check_sysemu when they succeed

		 * in modifying the stack frame

		/* Serious trouble! This could be caused by a bug in host 2.6

		 * SKAS3/2.6 patch before release -V6, together with a bug in

		 * the UML code itself.

/* When testing for SYSEMU support, if it is one of the broken versions, we

 * must just avoid using sysemu, not panic, but only if SYSEMU features are

 * broken.

 * So only for SYSEMU features we test mustpanic, while normal host features

 * must work anyway!

 Changed only during early boot */

perso.wanadoo.fr/laurent.vivier/UML/ for further \n"

 Print out the core dump limits early */

	/* Need to check this early because mmapping happens before the

	 * kernel is running.

/* Copyright (C) 2006 by Paolo Giarrusso - modified from glibc' execvp.c.

   Original copyright notice follows:



   Copyright (C) 1991,92,1995-99,2002,2004 Free Software Foundation, Inc.

   This file is part of the GNU C Library.



   The GNU C Library is free software; you can redistribute it and/or

   modify it under the terms of the GNU Lesser General Public

   License as published by the Free Software Foundation; either

   version 2.1 of the License, or (at your option) any later version.



   The GNU C Library is distributed in the hope that it will be useful,

   but WITHOUT ANY WARRANTY; without even the implied warranty of

   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

   Lesser General Public License for more details.



   You should have received a copy of the GNU Lesser General Public

   License along with the GNU C Library; if not, write to the Free

   Software Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA

/* Execute FILE, searching in the `PATH' environment variable if it contains

 Don't search when it contains a slash.  */

 Copy the file name at the top.  */

 And add the slash.  */

Let's avoid this GNU extension.

p = strchrnul (path, ':');

				/* Two adjacent colons, or a colon at the beginning or the end

 Try to execute this name.  If it works, execv will not return.  */

			/*

			if (errno == ENOEXEC) {

			}

					/* Record the we got a `Permission denied' error.  If we end

					   up finding no executable we can use, we want to diagnose

					/* Those errors indicate the file is missing or not executable

					   by us, in which case we want to just try the next path

					/* Some strange filesystems like AFS return even

					   stranger error numbers.  They cannot reasonably mean

					/* We won't go searching for the shell

					 * if it is not executable - the Linux

					 * kernel already handles this enough,

					/* Some other error means we found an executable file, but

					   something went wrong executing it; return the error to our

 We tried every element and none of them worked.  */

			/* At least one failure was due to permissions, so report that

 Return the error from the last attempt (probably ENOENT).  */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 - Cambridge Greys Ltd

 * Copyright (C) 2011 - 2014 Cisco Systems Inc

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Epoll support */

/* Helper to return an Epoll data pointer from an epoll event structure.

 * We need to keep this one on the userspace side to keep includes separate

/* Helper to compare events versus the events in the epoll structure.

 * Same as above - needs to be on the userspace side

/* Helper to set the event mask.

 * The event mask is opaque to the kernel side, because it does not have

 * access to the right includes/defines for EPOLL constants.

/*

 * Initial Epoll Setup

/*

 * Helper to run the actual epoll_wait

/*

 * Helper to add a fd to epoll

/*

 * Helper to mod the fd event mask and/or data backreference

/*

 * Helper to delete the epoll fd

	/* This is quiet as we use this as IO ON/OFF - so it is often

	 * invoked on a non-existent fd

 Needed so we do not leak an fd when rebooting */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 All signals are enabled in this handler ... */

	/*

	 * ... including the signal being handled, plus we want the

	 * handler reset to the default behavior, so that if an exit

	 * handler is hanging for some reason, the UML will just die

	 * after this signal is sent a second time.

	/*

	 * if no PATH variable is set or it has an empty value

	 * just use the default + /usr/lib/uml

 append /usr/lib/uml to the existing path */

	/*

	 * Allow these signals to bring down a UML if all other

	 * methods of control fail.

	/*

	 * Disable SIGPROF - I have no idea why libc doesn't do this or turn

	 * off the profiling time, but UML dies with a SIGPROF just before

	 * exiting when profiling is active.

	/*

	 * This signal stuff used to be in the reboot case.  However,

	 * sometimes a timer signal can come in when we're halting (reproducably

	 * when writing out gcov information, presumably because that takes

	 * some time) and cause a segfault.

 stop timers and set timer signal to be ignored */

 disable SIGIO for the fds and set SIGIO to be ignored */

	/*

	 * Let any pending signals fire now.  This ensures

	 * that they won't be delivered after the exec, when

	 * they are definitely not expected.

 Reboot */

 finding contiguous pages can be hard*/

	/*

	 * glibc people insist that if malloc fails, errno should be

	 * set by malloc as well. So we do.

	/*

	 * We need to know how the allocation happened, so it can be correctly

	 * freed.  This is done by seeing what region of memory the pointer is

	 * in -

	 * 	physical memory - kmalloc/kfree

	 *	kernel virtual memory - vmalloc/vfree

	 * 	anywhere else - malloc/free

	 * If kmalloc is not yet possible, then either high_physmem and/or

	 * end_vm are still 0 (as at startup), in which case we call free, or

	 * we have set them, but anyway addr has not been allocated from those

	 * areas. So, in both cases __real_free is called.

	 *

	 * CAN_KMALLOC is checked because it would be bad to free a buffer

	 * with kmalloc/vmalloc after they have been turned off during

	 * shutdown.

	 * XXX: However, we sometimes shutdown CAN_KMALLOC temporarily, so

	 * there is a possibility for memory leaks.

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 by various other people who didn't put their name here.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2002- 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * Signals that are OK to receive in the stub - we'll just continue it.

 * SIGWINCH will happen when UML is inside a detached screen.

 Signals that the stub will finish with - anything else is an error */

	/*

	 * faultinfo is prepared by the stub_segv_handler at start of

	 * the stub stack page. We just have to copy it.

/*

 * To use the same value of using_sysemu as the caller, ask it that value

 * (in local_using_sysemu

/**

 * userspace_tramp() - userspace trampoline

 * @stack:	pointer to the new userspace stack page, can be NULL, if? FIXME:

 *

 * The userspace trampoline is used to setup a new userspace process in start_userspace() after it was clone()'ed.

 * This function will run on a temporary stack page.

 * It ptrace()'es itself, then

 * Two pages are mapped into the userspace address space:

 * - STUB_CODE (with EXEC), which contains the skas stub code

 * - STUB_DATA (with R/W), which contains a data page that is used to transfer certain data between the UML userspace process and the UML kernel.

 * Also for the userspace process a SIGSEGV handler is installed to catch pagefaults in the userspace process.

 * And last the process stops itself to give control to the UML kernel for this userspace process.

 *

 * Return: Always zero, otherwise the current userspace process is ended with non null exit() call

/**

 * start_userspace() - prepare a new userspace process

 * @stub_stack:	pointer to the stub stack. Can be NULL, if? FIXME:

 *

 * Setups a new temporary stack page that is used while userspace_tramp() runs

 * Clones the kernel process into a new userspace process, with FDs only.

 *

 * Return: When positive: the process id of the new userspace process,

 *         when negative: an error number.

 * FIXME: can PIDs become negative?!

 setup a temporary stack page */

 set stack pointer to the end of the stack page, so it can grow downwards */

 clone into new userspace process */

 To prevent races if using_sysemu changes under us.*/

 Handle any immediate reschedules or signals */

		/*

		 * This can legitimately fail if the process loads a

		 * bogus value into a segment register.  It will

		 * segfault and PTRACE_GETREGS will read that value

		 * out of the process.  However, PTRACE_SETREGS will

		 * fail.  In this case, there is nothing to do but

		 * just kill the process.

 Now we set local_using_sysemu to be used for one loop */

 Assume: It's not a syscall */

			/* These signal handlers need the si argument.

			 * The SIGIO and SIGALARM handlers which constitute the

			 * majority of invocations, do not use it.

 Avoid -ERESTARTSYS handling in host */

 Set parent's instruction pointer to start of clone-stub */

	/*

	 * prepare offset and fd of child's stack as argument for parent's

	 * and child's mmap2 calls

	/*

	 * Wait, until parent has finished its work: read child's pid from

	 * parent's stack, and check, if bad result.

	/*

	 * Wait, until child has finished too: read child's result from

	 * child's stack and check it.

 XXX Make these percpu */

	/*

	 * Can't use UML_SETJMP or UML_LONGJMP here because they save

	 * and restore signals, with the possible side-effect of

	 * trying to handle any signals which came when they were

	 * blocked, which can't be done on this stack.

	 * Signals must be blocked when jumping back here and restored

	 * after returning to the jumper.

 unreachable */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

	/*

	 * When the stub stops, we find the following values on the

	 * beginning of the stack:

	 * (long )return_value

	 * (long )offset to failed sycall-data (0, if no error)

	/*

	 * If *addr still is uninitialized, it *must* contain NULL.

	 * Thus in this case do_syscall_stub correctly won't be called.

 in case of error, don't overwrite data on stack */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Do reliable error handling as this fails frequently enough. */

 Avoid hang as we won't get data in failure case. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * Returns the free space inside the ring buffer of this line.

 *

 * Should be called while holding line->lock (this does not modify data).

 This is for the case where the buffer is wrapped! */

 The other case */

 write_room subtracts 1 for the needed NULL, so we readd it.*/

/*

 * This copies the content of buf into the circular buffer associated with

 * this line.

 * The return value is the number of characters actually copied, i.e. the ones

 * for which there was space: this function is not supposed to ever flush out

 * the circular buffer.

 *

 * Must be called while holding line->lock!

 The circular buffer is wrapping */

/*

 * Flushes the ring buffer to the output channels. That is, write_chan is

 * called, passing it line->head as buffer, and an appropriate count.

 *

 * On exit, returns 1 when the buffer is empty,

 * 0 when the buffer is not empty on exit,

 * and -errno when an error occurred.

 *

 line->buffer + LINE_BUFSIZE is the end of the buffer! */

			/*

			 * We have flushed from ->head to buffer end, now we

			 * must flush only from the beginning to ->tail.

/*

 * We map both ->flush_chars and ->put_char (which go in pair) onto

 * ->flush_buffer and ->write. Hope it's not that bad.

	/*

	 * Interrupts are disabled here because genirq keep irqs disabled when

	 * calling the action handler.

/*

 * Common setup code for both startup command line and mconsole initialization.

 * @lines contains the array (of size @num) to modify;

 * @init is the setup string;

 * @error_out is an error string in the case of failure;

		/*

		 * We said con=/ssl= instead of con#=, so we are configuring all

		 * consoles at once.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 CONFIG_NOCONFIG_CHAN */

/* Items are added in IRQ context, when free_irq can't be called, and

 * removed in process context, when it can.

 * This handles interrupt sources which disappear, and which need to

 * be permanently disabled.  This is discovered in IRQ context, but

 * the freeing of the IRQ must be done later.

	/* Close in reverse order as open in case more than one of them

	 * refers to the same device and they save and restore that device's

	 * state.  Then, the first one opened will have the original state,

	 * so it must be the last closed.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

/*

 * The drop_skb is used when we can't allocate an skb.  The

 * packet is read into drop_skb in order to get the data off the

 * connection to the host.

 * It is reallocated whenever a maximum packet size is seen which is

 * larger than any seen before.  update_drop_skb is called from

 * eth_configure when a new interface is added.

 If we can't allocate memory, try again next round. */

 Read a packet into drop_skb and don't do anything with it. */

		/* dev_close can't be called in interrupt context, and takes

		 * again lp->lock.

		 * And dev_close() can be safely called multiple times on the

		 * same device, since it tests for (dev->flags & IFF_UP). So

		 * there's no harm in delaying the device shutdown.

		 * Furthermore, the workqueue will not re-enqueue an already

	/* clear buffer - it can happen that the host side of the interface

	 * is full when we get here.  In this case, new data is never queued,

	 * SIGIOs never arrive, and the net never works.

 this is normally done in the interrupt when tx finishes */

/*

 * Ensures that platform_driver_register is called only once by

 * eth_configure.  Will be set in an initcall.

	/* If this name ends up conflicting with an existing registered

	 * netdevice, that is OK, register_netdev{,ice}() will notice this

	 * and fail.

	/* This points to the transport private data. It's still clear, but we

 sysfs register */

	/*

	 * These just fill in a data structure, so there's no failure

	 * to be worried about.

 platform_device_unregister frees dev and device */

 Filled in during early boot */

	/* This string is broken up and the pieces used by the underlying

	 * driver.  So, it is freed only if eth_setup_common fails.

 uml_net_init shouldn't be called twice on two CPUs at the same time */

	/* Devices may have been opened already, so the uml_inetaddr_notifier

	 * didn't get a chance to run for them.  This fakes it so that

	 * addresses which have already been set up get handled properly.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 Steve Schmidtke

/*

 * Changed either at boot time or module load time.  At boot, this is

 * single-threaded; at module load, multiple modules would each have

 * their own copy of these variables.

 /dev/dsp file operations */

 /dev/mixer file operations */

 kernel module operations */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 - Cambridge Greys Limited

 * Copyright (C) 2011 - 2014 Cisco Systems Inc

 fix for ipv4 raw */;

	/* we do not do a strict check for "data" packets as per

	 * the RFC spec because the pure IP spec does not have

	 * that anyway.

 fix for ipv4 raw */;

 do not try to enable tap too if raw failed */

 "Pure" tap uses the same fd for rx and tx */

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Virtio vhost-user driver

 *

 * Copyright(c) 2019 Intel Corporation

 *

 * This driver allows virtio devices to be used over a vhost-user socket.

 *

 * Guest devices can be instantiated by kernel module or command line

 * parameters. One device will be created for each parameter. Syntax:

 *

 *		virtio_uml.device=<socket>:<virtio_id>[:<platform_id>]

 * where:

 *		<socket>	:= vhost-user socket path to connect

 *		<virtio_id>	:= virtio device id (as in virtio_ids.h)

 *		<platform_id>	:= (optional) platform device id

 *

 * example:

 *		virtio_uml.device=/var/uml.socket:1

 *

 * Based on Virtio MMIO driver by Pawel Moll, copyright 2011-2014, ARM Ltd.

 Vhost-user protocol */

	/*

	 * In virtio time-travel mode, we're handling all the vhost-user

	 * FDs by polling them whenever appropriate. However, we may get

	 * into a situation where we're sending out an interrupt message

	 * to a device (e.g. a net device) and need to handle a simulation

	 * time message while doing so, e.g. one that tells us to update

	 * our idea of how long we can run without scheduling.

	 *

	 * Thus, we need to not just read() from the given fd, but need

	 * to also handle messages for the simulation time - this function

	 * does that for us while waiting for the given fd to be readable.

	/*

	 * The need_response flag indicates that we already need a response,

	 * e.g. to read the features. In these cases, don't request an ACK as

	 * it is meaningless. Also request an ACK only if supported.

 not supported - VIRTIO_F_ACCESS_PLATFORM */

 not supported - VHOST_USER_PROTOCOL_F_HOST_NOTIFIER */

 ignored */, vq);

 Use a pipe for slave req fd, SIGIO is not supported for eventfd */

 Close unused write end of request fds */

 Ensure mapping is valid for the entire region */

	/*

	 * This is a bit tricky, see also the comment with setup_physmem().

	 *

	 * Essentially, setup_physmem() uses a file to mmap() our physmem,

	 * but the code and data we *already* have is omitted. To us, this

	 * is no difference, since they both become part of our address

	 * space and memory consumption. To somebody looking in from the

	 * outside, however, it is different because the part of our memory

	 * consumption that's already part of the binary (code/data) is not

	 * mapped from the file, so it's not visible to another mmap from

	 * the file descriptor.

	 *

	 * Thus, don't advertise this space to the vhost-user slave. This

	 * means that the slave will likely abort or similar when we give

	 * it an address from the hidden range, since it's not marked as

	 * a valid address, but at least that way we detect the issue and

	 * don't just have the slave read an all-zeroes buffer from the

	 * shared memory file, or write something there that we can never

	 * see (depending on the direction of the virtqueue traffic.)

	 *

	 * Since we usually don't want to use .text for virtio buffers,

	 * this effectively means that you cannot use

	 *  1) global variables, which are in the .bss and not in the shm

	 *     file-backed memory

	 *  2) the stack in some processes, depending on where they have

	 *     their stack (or maybe only no interrupt stack?)

	 *

	 * The stack is already not typically valid for DMA, so this isn't

	 * much of a restriction, but global variables might be encountered.

	 *

	 * It might be possible to fix it by copying around the data that's

	 * between bss_start and where we map the file now, but it's not

	 * something that you typically encounter with virtio drivers, so

	 * it didn't seem worthwhile.

 Virtio interface */

 Note: reverse order as a workaround to a decoding bug in snabb */

 Ensure previous messages have been processed */

 no call FD needed/desired in this case */

 Use a pipe for call fd, since SIGIO is not supported for eventfd */

 Close (unused) write end of call fds */

 not supported for now */

 might not have been opened due to not negotiating the feature */

 Platform device */

 Command line device list */

 Platform driver */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 This is a locked semaphore... */

	/* ... so here we wait for an xterm interrupt.

	 *

	 * XXX Note, if the xterm doesn't work for some reason (eg. DISPLAY

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org)

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

	/*

	 * With uts namespaces, uts information becomes process-specific, so

	 * we need a process context.  If we try handling this in interrupt

	 * context, we may hit an exiting process without a valid uts

	 * namespace.

 Initialized in mconsole_init, which is an initcall */

 Unversioned request */

	/*

	 * XXX This is a stack consumption problem.  It'd be nice to

	 * make it global and serialize access to it, but there are a

	 * ton of callers to this function.

 err can only be true on the first packet */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

/*

 * _XOPEN_SOURCE is needed for pread, but we define _GNU_SOURCE, which defines

 * that.

 unsigned time_t works until year 2106 */

/*

 * Define PATH_LEN_V3 as the usual value of MAXPATHLEN, just hard-code it in

 * case other systems have different values for MAXPATHLEN.

 *

 * The same must hold for V2 - we want file format compatibility, not anything

 * else.

/*

 * Changes from V2 -

 *	PATH_LEN_V3 as described above

 *	Explicitly specify field bit lengths for systems with different

 *		lengths for the usual C types.  Not sure whether char or

 *		time_t should be changed, this can be changed later without

 *		breaking compatibility

 *	Add alignment field so that different alignments can be used for the

 *		bitmap and data

 * 	Add cow_format field to allow for the possibility of different ways

 *		of specifying the COW blocks.  For now, the only value is 0,

 * 		for the traditional COW bitmap.

 *	Move the backing_file field to the end of the header.  This allows

 *		for the possibility of expanding it into the padding required

 *		by the bitmap alignment.

 * 	The bitmap and data portions of the file will be aligned as specified

 * 		by the alignment field.  This is to allow COW files to be

 *		put on devices with restrictions on access alignments, such as

 *		/dev/raw, with a 512 byte alignment restriction.  This also

 *		allows the data to be more aligned more strictly than on

 *		sector boundaries.  This is needed for ubd-mmap, which needs

 *		the data to be page aligned.

 *	Fixed (finally!) the rounding bug

/*

 * Until Dec2005, __attribute__((packed)) was left out from the below

 * definition, leading on 64-bit systems to 4 bytes of padding after mtime, to

 * align size to 8-byte alignment.  This shifted all fields above (no padding

 * was present on 32-bit, no other padding was added).

 *

 * However, this _can be detected_: it means that cow_format (always 0 until

 * now) is shifted onto the first 4 bytes of backing_file, where it is otherwise

 This is the broken layout used by some 64-bit binaries. */

 COW format definitions - for now, we have only the usual COW bitmap */

 MOOO */

 Below, %zd is for a size_t value */

 XXX Need to sanity-check the values read from the header */

 No error printed because the non-COW case comes through here */

 This is very subtle - see above at union cow_header definition */

		/*

		 * this was used until Dec2005 - 64bits are needed to represent

		 * 2106+. I.e. we can safely do this truncating cast.

		 *

		 * Additionally, we must use be32toh() instead of be64toh(), since

		 * the program used to use the former (tested - I got mtime

		 * mismatch "0 vs whatever").

		 *

	/*

	 * does not really matter how much we write it is just to set EOF

	 * this also sets the entire COW bitmap

	 * to zero without having to allocate it

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Copyright (C) 2005 - 2008 Jeff Dike <jdike@{linux.intel,addtoit}.com> */

/* Much of this ripped from drivers/char/hw_random.c, see there for other

 * copyright.

 *

 * This software may be used and distributed according to the terms

 * of the GNU General Public License, incorporated herein by reference.

/*

 * core module information

/* Changed at init time, in the non-modular case, and at module load

 * time, in the module case.  Presumably, the module subsystem

 * protects against a module being loaded twice at the same time.

/*

 * rng_init - initialize RNG module

/*

 * rng_cleanup - shutdown RNG module

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 by various other people who didn't put their name here.

 We will free this pointer. If it contains crap we're burned. */

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2000, 2001 Jeff Dike (jdike@karaya.com)

 Almost const, except that xterm_title may be changed in an initcall */

 Const, except for .mc.list */

/* The array is initialized by line_init, at initcall time.  The

 * elements are locked individually as needed.

 Set in an initcall, checked in an exitcall */

 No locking for register_console call - relies on single-threaded initcalls */

 console= option specifies tty */

/* UML hardware watchdog, shamelessly stolen from:

 *

 *	SoftDog	0.05:	A Software Watchdog Device

 *

 *	(c) Copyright 1996 Alan Cox <alan@redhat.com>, All Rights Reserved.

 *				http://www.redhat.com

 *

 *	This program is free software; you can redistribute it and/or

 *	modify it under the terms of the GNU General Public License

 *	as published by the Free Software Foundation; either version

 *	2 of the License, or (at your option) any later version.

 *

 *	Neither Alan Cox nor CymruNet Ltd. admit liability nor provide

 *	warranty for any of this software. This material is provided

 *	"AS-IS" and at no charge.

 *

 *	(c) Copyright 1995    Alan Cox <alan@lxorguk.ukuu.org.uk>

 *

 *	Software only watchdog driver. Unlike its big brother the WDT501P

 *	driver this won't always recover a failed machine.

 *

 *  03/96: Angelo Haritsis <ah@doc.ic.ac.uk> :

 *	Modularised.

 *	Added soft_margin; use upon insmod to change the timer delay.

 *	NB: uses same minor as wdt (WATCHDOG_MINOR); we could use separate

 *	    minors.

 *

 *  19980911 Alan Cox

 *	Made SMP safe for 2.3.x

 *

 *  20011127 Joel Becker (jlbec@evilplan.org>

 *	Added soft_noboot; Allows testing the softdog trigger without

 *	requiring a recompile.

 *	Added WDIOC_GETTIMEOUT and WDIOC_SETTIMOUT.

/*

 *	Allow only one person to hold it open

	/*

	 *	Shut off the timer.

	/*

	 *	Refresh the timer.

 SPDX-License-Identifier: GPL-2.0

/*

 * user-mode-linux networking multicast transport

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 by Harald Welte <laforge@gnumonks.org>

 *

 * based on the existing uml-networking code, which is

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

 *

 *

 set ttl according to config */

 set LOOP, so data does get fed back to local sockets */

 bind socket to the address */

 subscribe to the multicast group */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 XXX should be simpler... */

 no command line given after MAC addr */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/* This is very ugly and brute force lookup, but it is done

 * only once at initialization so not worth doing hashes or

 * anything more intelligent

/*

 * Socket/FD configuration functions. These return an structure

 * of rx and tx descriptors to cover cases where these are not

 * the same (f.e. read via raw socket and write via tap).

 TAP */

 TAP */

 RAW */

 Turn off RX on this fd */

 ld	[8] */

 jeq	#0xMAC[2-6] jt 2 jf 5*/

 ldh	[6] */

 jeq	#0xMAC[0-1] jt 4 jf 5 */

 ret	#0 */

 ret	#0x40000 */

 Note - this function requires a valid mac being passed as an arg */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 This address is used only as a unique identifier */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/um/drivers/mmapper_kern.c

 *

 * BRIEF MODULE DESCRIPTION

 *

 * Copyright (C) 2000 RidgeRun, Inc.

 * Author: RidgeRun, Inc.

 *         Greg Lonnon glonnon@ridgerun.com or info@ridgerun.com

 *

 These are set in mmapper_init, which is called at boot time */

	/*

	 * XXX A comment above remap_pfn_range says it should only be

	 * called when the mm semaphore is held

/*

 * No locking needed - only used (and modified) by below initcall and exitcall.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2018 Cambridge Greys Ltd

 * Copyright (C) 2015-2016 Anton Ivanov (aivanov@brocade.com)

 * Copyright (C) 2000 Jeff Dike (jdike@karaya.com)

/* 2001-09-28...2002-04-17

 * Partition stuff by James_McMechan@hotmail.com

 * old style ubd by setting UBD_SHIFT to 0

 * 2002-09-27...2002-10-18 massive tinkering for 2.5

 * partitions have changed in 2.5

 * 2003-01-29 more tinkering for 2.5.59-1

 * This should now address the sysfs problems and has

 * the symlink for devfs to allow for booting with

 * the common /dev/ubd/discX/... names rather than

 * only /dev/ubdN/discN this version also has lots of

 * clean ups preparing for ubd-many.

 * James McMechan

 Max request size is determined by sector mask - 32K */

 io_desc has to be the last element of the struct */

End stuff from ubd_user.h*/

 replaces BKL, might not be needed */

 Protected by ubd_lock */

 backing file name */

 backing file fd */

	/* name (and fd, below) of the file opened for writing, either the

 Protected by ubd_lock */

/* If *index_out == -1 at exit, the passed option was a general one;

 * otherwise, the str pointer is used (and owned) inside ubd_devs array, so it

 * should not be freed on exit.

 Only changed by ubd_init, which is an initcall. */

/* Function to read several request pointers at a time

* handling fractional reads if (and as) needed

			/*

			* Read somehow returned not a multiple of dword

			* theoretically possible, but never observed in the

			* wild, so read routine must be able to handle it

 Called without dev->lock held, and only in interrupt context. */

 Only changed by ubd_init, which is an initcall. */

		/*__u64 can be a long on AMD64 and with %lu GCC complains; so

 Successful return case! */

 Allow switching only if no mismatch. */

	/* This string is possibly broken up and stored, so it's only

	 * freed if ubd_setup_common fails, or if only general options

	 * were set.

 you cannot remove a open disk */

/* All these are called by mconsole in process context and without

 * ubd-specific locks.  The structure itself is const except for .list.

 Used in ubd_init, which is an initcall */

 Set by CONFIG_BLK_DEV_UBD_SYNC or ubd=sync.*/

		/* Letting ubd=sync be like using ubd#s= instead of ubd#= is

	/* This should no more be needed. And it didn't work anyway to exclude

	/*if((mode & FMODE_WRITE) && !ubd_dev->openflags.w){

	        if(--ubd_dev->count == 0) ubd_close_dev(ubd_dev);

	        err = -EROFS;

	/* This takes care of the case where we're exactly at the end of the

	 * device, and *cow_offset + 1 is off the end.  So, just back it up

	 * by one word.  Thanks to Lynn Kerby for the fix and James McMechan

	 * for the original diagnosis.

/*

 * Everything from here onwards *IS NOT PART OF THE KERNEL*

 *

 * The following functions are part of UML hypervisor code.

 * All functions from here onwards are executed as a helper

 * thread and are not allowed to execute any kernel functions.

 *

 * Any communication must occur strictly via shared memory and IPC.

 *

 * Do not add printks, locks, kernel memory operations, etc - it

 * will result in unpredictable behaviour and/or crashes.

 FLUSH is really a special case, we cannot "case" it with others */

 fds[0] is always either the rw image or our cow file */

/* Changed in start_io_thread, which is serialized by being called only

 * from ubd_init, which is an initcall.

 Only changed by the io thread. XXX: currently unused. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 XXX The os_getpid() is not SMP correct */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

		/* This is done here because freeing an IRQ can't be done

		 * within the IRQ handler.  So, pipe_interrupt always ups

		 * the semaphore regardless of whether it got a successful

		 * connection.  Then we loop here throwing out failed

		 * connections until a good one is found.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Luca Bigliardi (shammash@artha.org).

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Did we hit the end ? */

 verify slave side is usable */

 SPDX-License-Identifier: GPL-2.0

/*

 * user-mode-linux networking multicast transport

 * Copyright (C) 2001 by Harald Welte <laforge@gnumonks.org>

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 *

 * based on the existing uml-networking code, which is

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Intel Corporation

 * Author: Johannes Berg <johannes@sipsolutions.net>

 apparently timerfd won't send SIGIO, use workaround */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 XXX Trivial wrapper around write */

		/*

		 * The terminal becomes a bit less raw, to handle \n also as

		 * "Carriage Return", not only as "New Line". Otherwise, the new

		 * line won't start at the first column.

	/*

	 * Restore raw mode, in any case; we *must* ignore any error apart

	 * EINTR, except for debug.

/*

 * UML SIGWINCH handling

 *

 * The point of this is to handle SIGWINCH on consoles which have host

 * ttys and relay them inside UML to whatever might be running on the

 * console and cares about the window size (since SIGWINCH notifies

 * about terminal size changes).

 *

 * So, we have a separate thread for each host tty attached to a UML

 * device (side-issue - I'm annoyed that one thread can't have

 * multiple controlling ttys for the purpose of handling SIGWINCH, but

 * I imagine there are other reasons that doesn't make any sense).

 *

 * SIGWINCH can't be received synchronously, so you have to set up to

 * receive it as a signal.  That being the case, if you are going to

 * wait for it, it is convenient to sit in sigsuspend() and wait for

 * the signal to bounce you out of it (see below for how we make sure

 * to exit only on SIGWINCH).

	/*

	 * We are not using SIG_IGN on purpose, so don't fix it as I thought to

	 * do! If using SIG_IGN, the sigsuspend() call below would not stop on

	 * SIGWINCH.

 Block all signals possible. */

 In sigsuspend(), block anything else than SIGWINCH. */

	/*

	 * These are synchronization calls between various UML threads on the

	 * host - since they are not different kernel threads, we cannot use

	 * kernel semaphores. We don't use SysV semaphores because they are

	 * persistent.

		/*

		 * This will be interrupted by SIGWINCH only, since

		 * other signals are blocked.

	/*

	 * CLONE_FILES so this thread doesn't hold open files which are open

	 * now, but later closed in a different thread.  This is a

	 * problem with /dev/net/tun, which if held open by this

	 * thread, prevents the TUN/TAP device from being reused.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org)

 * Copyright (C) 2001 - 2008 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/* Safe without explicit locking for now.  Tasklets provide their own

 * locking, and the interrupt handler is safe because it can't interrupt

 * itself and it can only happen on CPU 0.

 long to avoid size mismatch warnings from gcc */

 that's OK */

 Begin the file content on his own line. */

	/*

	 * With 'b', the system will shut down without a chance to reply,

	 * so in this case, we reply first.

/*

 * Mconsole stack trace

 *  Added by Allan Graves, Jeff Dike

 *  Dumps a stacks registers to the linux console.

 *  Usage stack <pid>.

	/*

	 * Would be nice:

	 * 1) Send showregs output to mconsole.

	 * 2) Add a way to stack dump all pids.

	/*

	 * Should really check for multiple pids or reject bad args here

 What do the arguments in mconsole_reply mean? */

/*

 * Changed by mconsole_setup, which is __setup, and called before SMP is

 * active.

 long to avoid size mismatch warnings from gcc */

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2000, 2002 Jeff Dike (jdike@karaya.com)

 Almost const, except that xterm_title may be changed in an initcall */

 Const, except for .mc.list */

/* The array is initialized by line_init, at initcall time.  The

 * elements are locked individually as needed.

/* Changed by ssl_init and referenced by ssl_exit, which are both serialized

 * by being an initcall and exitcall, respectively.

 No locking for register_console call - relies on single-threaded initcalls */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Intel Corporation

 * Author: Johannes Berg <johannes@sipsolutions.net>

 for MSI-X we have a 32-bit payload */

 for now just standard BARs */

 in PCI, writes are posted, so don't wait */

 try without allocating memory */

 add to internal virtio queue */

 kick and poll for getting a response on the queue */

 buf->data is maximum size - we may only use parts of it */

 maximum size - we may only use parts of it */

 buf->data is maximum size - we may only use parts of it */

 maximum size - we may only use parts of it */

 not allowing functions for now ... */

 we should properly chain interrupts, but on ARCH=um we don't care */

 our MSI message is just the interrupt number */

 nothing to do - we already woke up due to the message */

 recycle the message buffer */

	/*

	 * In order to do suspend-resume properly, don't allow VQs

	 * to be suspended.

 Stop all virtqueues */

		/*

		 * must be the whole or part of the resource,

		 * not allowed to only overlap

 no need to continue */

 we want the full address here */

	/*

	 * This is a very low address and not actually valid 'physical' memory

	 * in UML, so we can simply map MSI(-X) vectors to there, it cannot be

	 * legitimately written to by the device in any other way.

	 * We use the (virtual) IRQ number here as the message to simplify the

	 * code that receives the message, where for now we simply trust the

	 * device to send the correct message.

 Yes, we map all pins to the same IRQ ... doesn't matter for now. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Only changed by xterm_setup, which is a setup */

	/*

	 * Check that DISPLAY is set, this doesn't guarantee the xterm

	 * will work but w/o it we can be pretty sure it won't.

	/*

	 * This business of getting a descriptor to a temp file,

	 * deleting the file and closing the descriptor is just to get

	 * a known-unused name for the Unix socket that we really

	 * want.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 Intel Corporation

 * Author: Johannes Berg <johannes@sipsolutions.net>

 Use this to get correct time in time-travel mode */

 enable or update */

 alarm triggered, it's now off */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 - 2019 Cambridge Greys Limited

 * Copyright (C) 2011 - 2014 Cisco Systems Inc

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2001 Lennert Buytenhek (buytenh@gnu.org) and

 * James Leu (jleu@mindspring.net).

 * Copyright (C) 2001 by various other people who didn't put their name here.

/*

 * Adapted from network devices with the following major changes:

 * All transports are static - simplifies the code significantly

 * Multiple FDs/IRQs per device

 * Vector IO optionally used for read/write, falling back to legacy

 * based on configuration and/or availability

 * Configuration is no longer positional - L2TPv3 and GRE require up to

 * 10 parameters, passing this as positional is not fit for purpose.

 * Only socket transports are supported

/* Argument accessors to set variables (and/or set default values)

 * mtu, buffer sizing, default headroom, etc

/* A mini-buffer for packet drop read

 * All of our supported transports are datagram oriented and we always

 * read using recvmsg or recvmmsg. If we pass a buffer which is smaller

 * than the packet size it still counts as full packet read and will

 * clean the incoming stream to keep sigio/epoll happy

/* Array backed queues optimized for bulk enqueue/dequeue and

 * 1:N (small values of N) or 1:1 enqueuer/dequeuer ratios.

 * For more details and full design rationale see

 * http://foswiki.cambridgegreys.com/Main/EatYourTailAndEnjoyIt

/*

 * Advance the mmsg queue head by n = advance. Resets the queue to

 * maximum enqueue/dequeue-at-once capacity if possible. Called by

 * dequeuers. Caller must hold the head_lock!

	/* we are at 0, use this to

	 * reset head and tail so we can use max size vectors

/*	Advance the queue tail by n = advance.

 *	This is called by enqueuers which should hold the

 *	head lock already

/*

 * Generic vector enqueue with support for forming headers using transport

 * specific callback. Allows GRE, L2TPv3, RAW and other transports

 * to use a common enqueue procedure in vector mode

		/* mark as empty to ensure correct destruction if

		 * needed

/*

 * Generic vector deque via sendmmsg with support for forming headers

 * using transport specific callback. Allows GRE, L2TPv3, RAW and

 * other transports to use a common dequeue procedure in vector mode

 update queue_depth to current value */

 Calculate the start of the vector */

 Adjust vector size if wraparound */

 Try to TX as many packets as possible */

				/* For some of the sendmmsg error scenarios

				 * we may end being unsure in the TX success

				 * for all packets. It is safer to declare

				 * them all TX-ed and blame the network.

					/* This is equivalent to an TX IRQ.

					 * Restart the upper layers to feed us

					 * more packets.

				/* if TX is busy, break out of the send loop,

				 *  poll write IRQ will reschedule xmit for us

/* Queue destructor. Deliberately stateless so we can use

 * it in queue cleanup if initialization fails.

	/* deallocate any skbuffs - we rely on any unused to be

	 * set to NULL.

 deallocate matching IOV structures including header buffs */

/*

 * Queue constructor. Create a queue with a given side.

 further failures can be handled safely by destroy_queue*/

		/* Clear all pointers - we use non-NULL as marking on

		 * what to free on destruction

/*

 * We do not use the RX queue as a proper wraparound queue for now

 * This is not necessary because the consumption via netif_rx()

 * happens in-line. While we can try using the return code of

 * netif_rx() for flow control there are no drivers doing this today.

 * For this RX specific use we ignore the tail/head locks and

 * just read into a prepared queue filled with skbuffs.

 Prepare queue for recvmmsg one-shot rx - fill with fresh sk_buffs*/

		/* it is OK if allocation fails - recvmmsg with NULL data in

		 * iov argument still performs an RX, just drops the packet

		 * This allows us stop faffing around with a "drop buffer"

	/* This string is broken up and the pieces used by the underlying

	 * driver. We should copy it to make sure things do not go wrong

	 * later.

/*

 * There is no shared per-transport initialization code, so

 * we will just initialize each interface one by one and

 * add them to a list

/* Bog standard recv using recvmsg - not used normally unless the user

 * explicitly specifies not to use recvmmsg vector RX.

 header + data use case only */

		/* Read a packet into drop_buffer and don't do

		 * anything with it.

/*

 * Packet at a time TX which falls back to vector TX if the

 * underlying transport is busy.

/*

 * Receive as many messages as we can in one call using the special

 * mmsg vector matched to an skb vector which we prepared earlier.

	/* Refresh the vector and make sure it is with new skbs and the

	 * iovs are updated to point to them.

 Fire the Lazy Gun - get as many packets as we can in one go. */

	/* We treat packet processing as enqueue, buffer refresh as dequeue

	 * The queue_depth tells us how many buffers have been used and how

	 * many do we need to prep the next time prep_queue_for_rx() is called.

				/* Overlay header failed to verify - discard.

				 * We can actually keep this skb and reuse it,

				 * but that will make the prep logic too

				 * complex.

			/*

			 * We do not need to lock on updating stats here

			 * The interrupt loop is non-reentrant.

			/* Overlay header too short to do anything - discard.

			 * We can actually keep this skb and reuse it,

			 * but that will make the prep logic too complex.

 Move to the next buffer element */

	/* We do BQL only in the vector path, no point doing it in

	 * packet at a time mode as there is no device queue

	/* if the device queue is full, stop the upper layers and

	 * flush it.

	/* We need to pay attention to it only if we got

	 * -EAGAIN or -ENOBUFFS from sendmmsg. Otherwise

	 * we ignore it. In the future, it may be worth

	 * it to improve the IRQ controller a bit to make

	 * tweaking the IRQ mask less costly

 Disable and free all IRQS */

 TX tasklet */

 READ IRQ */

 WRITE IRQ - we need it only if we have vector TX */

	/* clear buffer - it can happen that the host side of the interface

	 * is full when we get here. In this case, new data is never queued,

	 * SIGIOs never arrive, and the net never works.

 TODO: - we can do some BPF games here */

	/* Adjust buffer sizes for GSO/GRO. Unfortunately, there is

	 * no way to negotiate it on raw sockets, so we can change

	 * only our side.

 All new frame buffers will be GRO-sized */

 All new frame buffers will be normal sized */

	/* If this name ends up conflicting with an existing registered

	 * netdevice, that is OK, register_netdev{,ice}() will notice this

	 * and fail.

 sysfs register */

		/* TODO - we need to calculate headroom so that ip header

		 * is 16 byte aligned all the time

 FIXME */

 primary IRQ - fixme */

 we will adjust this once opened */

/*

 * Invoked late in the init

/* Invoked at initial argument parsing, only stores

 * arguments until a proper vector_init is called

 * later

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2016 Anton Ivanov (aivanov@brocade.com)

 * Copyright (C) 2000, 2001, 2002 Jeff Dike (jdike@karaya.com)

 * Copyright (C) 2001 Ridgerun,Inc (glonnon@ridgerun.com)

 SPDX-License-Identifier: GPL-2.0

 ----------------------------------------------------------------------------- */

 trivial console driver -- simply dump everything to stderr                    */

/*

 * Don't register by default -- as this registers very early in the

 * boot process it becomes the default console.

 *

 * Initialized at init time.

 stderr */, string, len, NULL);

/* The previous behavior of not unregistering led to /dev/console being

 * impossible to open.  My FC5 filesystem started having init die, and the

 * system panicing because of this.  Unregistering causes the real

 * console to become the default console, and /dev/console can then be

 * opened.  Making this an initcall makes this happen late enough that

 * there is no added value in dumping everything to stderr, and the

 * normal console is good enough to show you all available output.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2007 Luca Bigliardi (shammash@artha.org).

 *

 * Transport usage:

 *  ethN=vde,<vde_switch>,<mac addr>,<port>,<group>,<mode>,<description>

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Anton Ivanov (aivanov@{brocade.com,kot-begemot.co.uk})

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright 2003 PathScale, Inc.

/*

 * This is a per-cpu array.  A processor only modifies its entry and it only

 * cares about its entry, so it's OK if another processor is modifying its

 * entry.

 FIXME: Need to look up userspace_pid by cpu */

/*

 * This is called magically, by its address being stuffed in a jmp_buf

 * and being longjmp-d to.

	/*

	 * callback returns only if the kernel thread execs a process

 Called magically, see new_thread_handler above */

	/*

	 * XXX: if interrupt_end() calls schedule, this call to

	 * arch_switch_to isn't needed. We could want to apply this to

	 * improve performance. -bb

		/*

		 * Set a new TLS for the child thread?

 Is in_interrupt() really needed? */

 We use the first char, but pretend to write everything */

/*

 * Only x86 and x86_64 have an arch_align_stack().

 * All other arches have "#define arch_align_stack(x) (x)"

 * in their asm/exec.h

 * As this is included in UML from asm-um/system-generic.h,

 * we can use it to behave as the subarch does.

 Bail if the process has no kernel stack for some reason */

	/*

	 * Bail if the stack pointer is below the bottom of the kernel

	 * stack for some reason

 Ignore everything until we're above the scheduler */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2013 Richard Weinberger <richard@nod.at>

 * Copyright (C) 2014 Google Inc., Author: Daniel Walter <dwalter@google.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2013 Richard Weinberger <richrd@nod.at>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Changed during early boot */

/**

 * setup_physmem() - Setup physical memory for UML

 * @start:	Start address of the physical kernel memory,

 *		i.e start address of the executable image.

 * @reserve_end:	end address of the physical kernel memory.

 * @len:	Length of total physical memory that should be mapped/made

 *		available, in bytes.

 * @highmem:	Number of highmem bytes that should be mapped/made available.

 *

 * Creates an unlinked temporary file of size (len + highmem) and memory maps

 * it on the last executable image address (uml_reserved).

 *

 * The offset is needed as the length of the total physical memory

 * (len + highmem) includes the size of the memory used be the executable image,

 * but the mapped-to address is the last address of the executable image

 * (uml_reserved == end address of executable image).

 *

 * The memory mapped memory of the temporary file is used as backing memory

 * of all user space processes/kernel tasks.

	/*

	 * Special kludge - This page will be mapped in to userspace processes

	 * from physmem_fd, so it needs to be written out there.

/*

 * This list is constructed in parse_iomem and addresses filled in in

 * setup_iomem, both of which run during early boot.  Afterwards, it's

 * unchanged.

 Initialized in parse_iomem and unchanged thereafter */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Anton Ivanov (aivanov@{brocade.com,kot-begemot.co.uk})

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2012-2014 Cisco Systems

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Copyright (C) 2019 Intel Corporation

	/*

	 * We can't unlock here, but interrupt signals with a timetravel_handler

	 * (see um_request_irq_tt) get to the timetravel_handler anyway.

 nothing */

	/*

	 * We need to block even the timetravel handlers of SIGIO here and

	 * only restore their use when we got the ACK - otherwise we may

	 * (will) get interrupted by that, try to queue the IRQ for future

	 * processing and thus send another request while we're still waiting

	 * for an ACK, but the peer doesn't know we got interrupted and will

	 * send the ACKs in the same order as the message, but we'd need to

	 * see them in the opposite order ...

	 *

	 * This wouldn't matter *too* much, but some ACKs carry the

	 * current time (for UM_TIMETRAVEL_GET) and getting another

	 * ACK without a time would confuse us a lot!

	 *

	 * The sequence number assignment that happens here lets us

	 * debug such message handling issues more easily.

 asked for exactly this time previously */

	/*

	 * if we're running and are allowed to run past the request

	 * then we don't need to update it either

 returns true if we must do a wait to the simtime device */

	/*

	 * If we received an external sync point ("free until") then we

	 * don't have to request/wait for anything until then, unless

	 * we're already waiting.

	/*

	 * Here we are deep in the idle loop, so we have to break out of the

	 * kernel abstraction in a sense and implement this in terms of the

	 * UML system waiting on the VQ interrupt while sleeping, when we get

	 * the signal it'll call time_travel_ext_vq_notify_done() completing the

	 * call.

 we might request more stuff while polling - reset when we run */

		/*

		 * Add the new entry before one with higher time,

		 * or if they're equal and both on stack, because

		 * in that case we need to unwind the stack in the

		 * right order, and the later event (timer sleep

		 * or such) must be dequeued first.

	/*

	 * Don't do anything for most cases. Note that because here we have

	 * to disable IRQs (and re-enable later) we'll actually recurse at

	 * the end of the function, so this is strictly necessary.

		/*

		 * deliver_alarm() does the irq_enter/irq_exit

		 * by itself, so must handle it specially here

		/*

		 * set pending again, it was set to false when the

		 * event was deleted from the original list, but

		 * now it's still pending until we deliver the IRQ.

 add it without a handler - we deal with that specifically below */

 new events may have been inserted while we were waiting */

	/*

	 * We could model interrupt latency here, for now just

	 * don't have any latency at all and request the exact

	 * same time (again) to run the interrupt...

	/*

	 * Wait "forever" (using S64_MAX because there are some potential

	 * wrapping issues, especially with the current TT_MODE_EXTERNAL

	 * controller application.

			/*

			 * This is somewhat wrong - we should get the first

			 * one sooner like the os_timer_one_shot() below...

 controller gave us the *current* time, so adjust by that */

 we just read the host clock with os_persistent_clock_emulation() */

 CONFIG_UML_TIME_TRAVEL_SUPPORT */

 fail link if this actually gets used */

 these are empty macros so the struct/fn need not exist */

 externally not usable - redefine here so we can */

	/*

	 * In basic time-travel mode we still get real interrupts

	 * (signals) but since we don't read time from the OS, we

	 * must update the simulated time here to the expiry when

	 * we get a signal.

	 * This is not the case in inf-cpu mode, since there we

	 * never get any real signals from the OS.

 microsecond resolution should be enough for anyone, same as 640K RAM

 userspace - relay signal, results in correct userspace timers */

		/*

		 * We make reading the timer cost a bit so that we don't get

		 * stuck in loops that expect time to move more than the

		 * exact requested sleep amount, e.g. python's socket server,

		 * see https://bugs.python.org/issue37026.

		 *

		 * However, don't do that when we're in interrupt or such as

		 * then we might recurse into our own processing, and get to

		 * even more waiting, and that's not good - it messes up the

		 * "what do I do next" and onstack event we use to know when

		 * to return from time_travel_update_time().

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 This is not an else because ret is modified above */

	/*

	 * Don't bother flushing if this address space is about to be

	 * destroyed.

	/*

	 * Don't bother flushing if this address space is about to be

	 * destroyed.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2011 Richard Weinberger <richrd@nod.at>

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * If read and write race, the read will still atomically read a valid

 * value.

	/*

	 * Save uml_exitcode in a local so that we don't need to guarantee

	 * that sprintf accesses it atomically.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2001 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Changed by set_umid_arg */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * OK, we're invoking a handler

 Did we come from a system call? */

 If so, check system call restarting.. */

 Whee!  Actually deliver the signal.  */

 Did we come from a system call? */

 Restart the system call - no handlers present */

	/*

	 * This closes a way to execute a system call on the host.  If

	 * you set a breakpoint on a system call instruction and singlestep

	 * from it, the tracing thread used to PTRACE_SINGLESTEP the process

	 * rather than PTRACE_SYSCALL it, allowing the system call to execute

	 * on the host.  The tracing thread will check this flag and

	 * PTRACE_SYSCALL if necessary.

	/*

	 * if there's no signal to deliver, we just put the saved sigmask

	 * back

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Changed in add_arg and setup_arch, which run before SMP is started */

/*

 * These fields are initialized at boot time and not changed.

 * XXX This structure is used only in the non-SMP case.  Maybe this

 * should be moved to smp.c.

 Changed in setup_arch, which is called in early boot */

 Set in linux_main */

 Also modified in mem_init */

 Set in uml_ncpus_setup */

 Set in early boot */

 Set in uml_mem_setup and modified in linux_main */

user-mode-linux.sourceforge.net/\n\n";

 Explicitly use printf() to show version in stdout */

 Explicitly use printf() to show help in stdout */

 Set during early boot */

 reserve two pages for the stubs */

	/*

	 * TASK_SIZE needs to be PGDIR_SIZE aligned or else exit_mmap craps

	 * out

 OS sanity checks that need to happen before the kernel runs */

	/*

	 * Increase physical memory size for exec-shield users

	 * so they actually get what they asked for. This should

	 * add zero for non-exec shield users

 Reserve up to 4M after the current brk */

	/*

	 * Zones have to begin on a 1 << MAX_ORDER page boundary,

	 * so this makes sure that's true for highmem

	/*

	 * In UML, the only reference to this function is in

	 * apply_relocate_add(), which shouldn't ever actually call this

	 * because UML doesn't have live patching.

	/*

	 * This is identical to the idle sleep, but we've just

	 * (during suspend) turned off all interrupt sources

	 * except for the ones we want, so now we can only wake

	 * up on something we actually want to wake up on. All

	 * timing has also been suspended.

	/*

	 * In external time-travel mode we can't use signals to wake up

	 * since that would mess with the scheduling. We'll have to do

	 * some additional work to support wakeup on virtio devices or

	 * similar, perhaps implementing a fake RTC controller that can

	 * trigger wakeup (and request the appropriate scheduling from

	 * the external scheduler when going to suspend.)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 Changed by uml_initrd_setup, which is a setup */

	/*

	 * This is necessary because alloc_bootmem craps out if you

	 * ask for no memory.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * Called by kernel/ptrace.c when detaching..

 read the word at location addr in the USER area. */

 write the word at location addr in the USER area */

 Get all gp regs from the child. */

 Set all gp regs in the child. */

 Send us the fake SIGTRAP */

 User-mode eip? */

/*

 * XXX Check PT_DTRACE vs TIF_SINGLESTEP for singlestepping check and

 * PT_PTRACED vs TIF_SYSCALL_TRACE for syscall tracing check

 Fake a debug trap */

 force do_signal() --> is_syscall() */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{linux.intel,addtoit}.com)

 Protected by sigio_lock() called from write_sigio_workaround */

 These are called from os-Linux/sigio.c to protect its pollfds arrays. */

 SPDX-License-Identifier: GPL-2.0

 only dump kmsg when no console is available */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013 Richard Weinberger <richrd@nod.at>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 allocated in paging_init, zeroed in mem_init, and unchanged thereafter */

/*

 * Initialized during boot, and readonly for initializing page tables

 * afterwards

 Initialized at boot time, and readonly after that */

 Used during early boot */

 clear the zero-page */

	/* Map in the area just after the brk now that kmalloc is about

	 * to be turned on.

 this will put all low memory onto the freelists */

/*

 * Create a page table and place a pointer to it in a middle page

 * directory entry.

	/*

	 * Fixed mappings, only the page table structure has to be

	 * created - mappings will be set by set_fixmap():

/*

 * This can't do anything because nothing in the kernel image can be freed

 * since it's not in kernel physical memory.

 Allocate and free page tables. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * Note this is constrained to return 0, -EFAULT, -EACCES, -ENOMEM by

 * segv().

	/*

	 * If the fault was with pagefaults disabled, don't take the fault, just

	 * fail.

 Don't require VM_READ|VM_EXEC for write faults! */

	/*

	 * The below warning was added in place of

	 *	pte_mkyoung(); if (is_write) pte_mkdirty();

	 * If it's triggered, we'd see normally a hang here (a clean pte is

	 * marked read-only to emulate the dirty bit).

	 * However, the generic code can mark a PTE writable but clean on a

	 * concurrent read fault, triggering this harmlessly. So comment it out.

	/*

	 * We ran out of memory, call the OOM killer, and return the userspace

	 * (which will retry the fault, or kill us if we got oom-killed).

	/*

	 * This is to tell gcc that we're not returning - do_signal

	 * can, in general, return, but in this case, it's not, since

	 * we just got a fatal SIGSEGV queued.

/**

 * segv_handler() - the SIGSEGV handler

 * @sig:	the signal number

 * @unused_si:	the signal info struct; unused in this handler

 * @regs:	the ptrace register information

 *

 * The handler first extracts the faultinfo from the UML ptrace regs struct.

 * If the userfault did not happen in an UML userspace process, bad_segv is called.

 * Otherwise the signal did happen in a cloned userspace process, handle it.

/*

 * We give a *copy* of the faultinfo in the regs to segv.

 * This must be done, since nesting SEGVs could overwrite

 * the info in the regs. A pointer to the info then would

 * give us bad data!

		/*

		 * A thread accessed NULL, we get a fault, but CR2 is invalid.

		 * This code is used in __do_copy_from_user() of TT mode.

		 * XXX tt mode is gone, so maybe this isn't needed any more

	/* Is the signal layout for the signal known?

	 * Signal data must be scrubbed to prevent information leaks.

 SPDX-License-Identifier: GPL-2.0

/* 

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2021 Intel Corporation

 * Author: Johannes Berg <johannes@sipsolutions.net>

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2017 - Cambridge Greys Ltd

 * Copyright (C) 2011 - 2014 Cisco Systems Inc

 * Copyright (C) 2000 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 * Derived (i.e. mostly copied) from arch/i386/kernel/irq.c:

 *	Copyright (C) 1992, 1998 Linus Torvalds, Ingo Molnar

/* When epoll triggers we do not know why it did so

 * we can also have different IRQs for read and write.

 * This is why we keep a small irq_reg array for each fd -

 * one entry per IRQ type

 it's cheaper to store this than to query it */

/*

 * irq->active guards against reentry

 * irq->pending accumulates pending requests

 * if pending is raised the irq_handler is re-run

 * until pending is cleared

 do nothing if suspended - just to cause a wakeup */

	/*

	 * Handle all messages - we might get multiple even while

	 * interrupts are already suspended, due to suspend order

	 * etc. Note that time_travel_add_irq_event() will not add

	 * an event twice, if it's pending already "first wins".

	/*

	 * If we're called to only run time-travel handlers then don't

	 * actually proceed but mark sigio as pending (if applicable).

	 * For suspend/resume, timetravel_handlers_only may be true

	 * despite time-travel not being configured and used.

		/* This is now lockless - epoll keeps back-referencesto the irqs

		 * which have trigger it so there is no need to walk the irq

		 * list and lock it every time. We avoid locking by turning off

		 * IO for a specific fd by executing os_del_epoll_fd(fd) before

		 * we do any changes to the actual data structures

 will modify (instead of add) if needed */

 cannot register the same FD twice with the same type */

 temporarily disable to avoid IRQ-side locking */

/*

 * Remove the entry or entries for a specific FD, if you

 * don't want to remove all the possible entries then use

 * um_free_irq() or deactivate_fd() instead.

/*

 * Called just before shutdown in order to provide a clean exec

 * environment in case the system is rebooting.  No locking because

 * that would cause a pointless shutdown hang if something hadn't

 * released the lock.

	/* Stop IO. The IRQ loop has no lock so this is our

	 * only way of making sure we are safe to dispose

	 * of all IRQ handlers

 we can no longer call kfree() here so just deactivate */

/*

 * do_IRQ handles all normal device IRQs (the special

 * SMP cross-CPU interrupts have their own specific

 * handlers).

			/*

			 * For the SIGIO_WRITE_IRQ, which is used to handle the

			 * SIGIO workaround thread, we need special handling:

			 * enable wake for it itself, but below we tell it about

			 * any FDs that should be suspended.

	/*

	 * We don't need to lock anything here since we're in resume

	 * and nothing else is running, but have disabled IRQs so we

	 * don't try anything else with the interrupt list from there.

/*

 * irq_chip must define at least enable/disable and ack when

 * the edge handler is used.

 This is used for everything other than the timer. */

 Initialize EPOLL Loop */

/*

 * IRQ stack entry and exit:

 *

 * Unlike i386, UML doesn't receive IRQs on the normal kernel stack

 * and switch over to the IRQ stack after some preparation.  We use

 * sigaltstack to receive signals on a separate stack from the start.

 * These two functions make sure the rest of the kernel won't be too

 * upset by being on a different stack.  The IRQ stack has a

 * thread_info structure at the bottom so that current et al continue

 * to work.

 *

 * to_irq_stack copies the current task's thread_info to the IRQ stack

 * thread_info and sets the tasks's stack to point to the IRQ stack.

 *

 * from_irq_stack copies the thread_info struct back (flags may have

 * been modified) and resets the task's stack pointer.

 *

 * Tricky bits -

 *

 * What happens when two signals race each other?  UML doesn't block

 * signals with sigprocmask, SA_DEFER, or sa_mask, so a second signal

 * could arrive while a previous one is still setting up the

 * thread_info.

 *

 * There are three cases -

 *     The first interrupt on the stack - sets up the thread_info and

 * handles the interrupt

 *     A nested interrupt interrupting the copying of the thread_info -

 * can't handle the interrupt, as the stack is in an unknown state

 *     A nested interrupt not interrupting the copying of the

 * thread_info - doesn't do any setup, just handles the interrupt

 *

 * The first job is to figure out whether we interrupted stack setup.

 * This is done by xchging the signal mask with thread_info->pending.

 * If the value that comes back is zero, then there is no setup in

 * progress, and the interrupt can be handled.  If the value is

 * non-zero, then there is stack setup in progress.  In order to have

 * the interrupt handled, we leave our signal in the mask, and it will

 * be handled by the upper handler after it has set up the stack.

 *

 * Next is to figure out whether we are the outer handler or a nested

 * one.  As part of setting up the stack, thread_info->real_thread is

 * set to non-NULL (and is reset to NULL on exit).  This is the

 * nesting indicator.  If it is non-NULL, then the stack is already

 * set up and the handler can run.

		/*

		 * If any interrupts come in at this point, we want to

		 * make sure that their bits aren't lost by our

		 * putting our bit in.  So, this loop accumulates bits

		 * until xchg returns the same value that we put in.

		 * When that happens, there were no new interrupts,

		 * and pending_mask contains a bit for each interrupt

		 * that came in.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

	/*

	 * If we have infinite CPU resources, then make every syscall also a

	 * preemption point, since we don't have any other preemption in this

	 * case, and kernel threads would basically never run until userspace

	 * went to sleep, even if said userspace interacts with the kernel in

	 * various ways.

 Initialize the syscall number and default return value. */

 Do the seccomp check after ptrace; failures should be fast. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/*

 * This is in a separate file because it needs to be compiled with any

 * extraneous gcc flags (-pg, -fprofile-arcs, -ftest-coverage) disabled

 *

 * Use UM_KERN_PAGE_SIZE instead of PAGE_SIZE because that calls getpagesize

 * on some systems.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

/**

 * arch_futex_atomic_op_inuser() - Atomic arithmetic operation with constant

 *			  argument and comparison of the previous

 *			  futex value with another constant.

 *

 * @encoded_op:	encoded operation to execute

 * @uaddr:	pointer to user space address

 *

 * Return:

 * 0 - On success

 * -EFAULT - User access resulted in a page fault

 * -EAGAIN - Atomic operation was unable to complete due to contention

 * -ENOSYS - Operation not supported

/**

 * futex_atomic_cmpxchg_inatomic() - Compare and exchange the content of the

 *				uaddr with newval if the current value is

 *				oldval.

 * @uval:	pointer to store content of @uaddr

 * @uaddr:	pointer to user space address

 * @oldval:	old value

 * @newval:	new value to store to @uaddr

 *

 * Return:

 * 0 - On success

 * -EFAULT - User access resulted in a page fault

 * -EAGAIN - Atomic operation was unable to complete due to contention

 * -ENOSYS - Function not implemented (only if !HAVE_FUTEX_CMPXCHG)

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)

 * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)

	/*

	 * If init_new_context wasn't called, this will be

	 * zero, resulting in a kill(0), which will result in the

	 * whole UML suddenly dying.  Also, cover negative and

	 * 1 cases, since they shouldn't happen either.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/h8300/mm/init.c

 *

 *  Copyright (C) 1998  D. Jeff Dionne <jeff@lineo.ca>,

 *                      Kenneth Albanowski <kjahds@kjahds.com>,

 *  Copyright (C) 2000  Lineo, Inc.  (www.lineo.com)

 *

 *  Based on:

 *

 *  linux/arch/m68knommu/mm/init.c

 *  linux/arch/m68k/mm/init.c

 *

 *  Copyright (C) 1995  Hamish Macdonald

 *

 *  JAN/1999 -- hacked to support ColdFire (gerg@snapgear.com)

 *  DEC/2000 -- linux 2.4 support <davidm@snapgear.com>

/*

 * ZERO_PAGE is a special page that is used for zero-initialized

 * data and COW.

/*

 * paging_init() continues the virtual memory environment setup which

 * was begun by the code in arch/head.S.

 * The parameters are pointers to where to stick the starting and ending

 * addresses of available kernel virtual memory.

	/*

	 * Make sure start_mem is page aligned,  otherwise bootmem and

	 * page_alloc get different views og the world.

	/*

	 * Initialize the bad page table and bad page to point

	 * to a couple of allocated pages.

	/*

	 * Set up SFC/DFC registers (user data space).

 this will put all low memory onto the freelists */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/h8300/mm/fault.c

 *

 *  Copyright (C) 1998  D. Jeff Dionne <jeff@lineo.ca>,

 *  Copyright (C) 2000  Lineo, Inc.  (www.lineo.com)

 *

 *  Based on:

 *

 *  linux/arch/m68knommu/mm/fault.c

 *  linux/arch/m68k/mm/fault.c

 *

 *  Copyright (C) 1995  Hamish Macdonald

/*

 * This routine handles page faults.  It determines the problem, and

 * then passes it off to one of the appropriate routines.

 *

 * error_code:

 *	bit 0 == 0 means no page found, 1 means protection fault

 *	bit 1 == 0 means read, 1 means write

 *

 * If this routine detects a bad access, it returns 1, otherwise it

 * returns 0.

/*

 * Oops. The kernel tried to access some bad page. We'll have to

 * terminate things with extreme prejudice.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/h8300/mm/memory.c

 *

 *  Copyright (C) 2002  Yoshinori Sato <ysato@users.sourceforge.jp>,

 *

 *  Based on:

 *

 *  linux/arch/m68knommu/mm/memory.c

 *

 *  Copyright (C) 1998  Kenneth Albanowski <kjahds@kjahds.com>,

 *  Copyright (C) 1999-2002, Greg Ungerer (gerg@snapgear.com)

 *

 *  Based on:

 *

 *  linux/arch/m68k/mm/memory.c

 *

 *  Copyright (C) 1995  Hamish Macdonald

/*

 * Map some physical address range into the kernel address space.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/h8300/kernel/process.c

 *

 * Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 *  Based on:

 *

 *  linux/arch/m68knommu/kernel/process.c

 *

 *  Copyright (C) 1998  D. Jeff Dionne <jeff@ryeham.ee.ryerson.ca>,

 *                      Kenneth Albanowski <kjahds@kjahds.com>,

 *                      The Silver Hammer Group, Ltd.

 *

 *  linux/arch/m68k/kernel/process.c

 *

 *  Copyright (C) 1995  Hamish Macdonald

 *

 *  68060 fixes by Jesper Skov

/*

 * This file handles the architecture-dependent parts of process handling..

/*

 * The idle loop on an H8/300..

 arg */

 fn */

 generic sys_clone is not enough registers */

/*

 *    ptrace cpu depend helper functions

 *

 *  Copyright 2003, 2015 Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of

 * this archive for more details.

 trapa #3 */

 disable singlestep */

 calculate next pc */

 normal instruction */

 absolute address jump */

 indirect address jump */

 return to subrutine */

 register indexed jump */

 pc relative jump (byte offset) */

 pc relative jump (word offset) */

/* opcode decode table define

   ptn: opcode pattern

   msk: opcode bitmask

   len: instruction length (<0 next table index)

 0x00 */

 0x01 */

 0x02-0x03 */

 0x04-0x05/0x14-0x15 */

 0x06-0x07 */

 0x08-0x09/0x0c-0x0d/0x18-0x19/0x1c-0x1d */

 0x0a-0x0b/0x1a-0x1b */

 0x0e-0x0f/0x1e-0x1f */

 0x10-0x13 */

 0x16-0x17 */

 0x20-0x3f */

 0x40-0x4f */

 0x50-0x53 */

 0x54/0x56 */

 0x55 */

 0x57 */

 0x58/0x5c */

 0x59/0x5b */

 0x5a/0x5e */

 0x5b/0x5f */

 0x60-0x67/0x70-0x77 */

 0x68-0x69/0x6c-0x6d */

 0x6a-0x6b */

 0x6e-0x6f */

 0x78 */

 0x79 */

 0x7a */

 0x7b */

 0x7c-0x7f */

 0x80-0xff */

 0x0100 */

 0x0140-0x14f */

 0x0180-0x018f */

 0x01c0-0x01ff */

 0x6a0?/0x6a8?/0x6b0?/0x6b8? */

 0x6a2?/0x6aa?/0x6b2?/0x6ba? */

 0x010069/0x01006d/014069/0x01406d */

 0x01006b/0x01406b */

 0x01006f/0x01406f */

 0x010078/0x014078 */

/* 0x0100690?/0x01006d0?/0140690?/0x01406d0?/

/* 0x0100692?/0x01006d2?/0140692?/0x01406d2?/

 encode complex conditions */

	/* B4: N^V

	   B5: Z|(N^V)

		/* user stack frames

		   |   er0  | temporary saved

		   +--------+

		   |   exp  | exception stack frames

		   +--------+

		   | ret pc | userspace return address

 skip myself */

 skip myself */

 Set breakpoint(s) to simulate a single step from the current PC.  */

/*

 * linux/arch/h8300/boot/traps.c -- general exception handling code

 * H8/300 support Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * Cloned from Linux/m68k.

 *

 * No original Copyright holder listed,

 * Probable original (C) Roman Zippel (assigned DJD, 1999)

 *

 * Copyright 1999-2000 D. Jeff Dionne, <jeff@rt-control.com>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file COPYING in the main directory of this archive

 * for more details.

/*

 * this must be called very early as the kernel might

 * use some instruction that are emulated on the 060

/*

 *	Generic dumping code. Used for panic and debug.

		/*

		 * If the address is either in the text segment of the

		 * kernel, or in the region which contains vmalloc'ed

		 * memory, it *may* be the address of a calling

		 * routine; if so, print it so that someone tracing

		 * down the cause of the crash will be able to figure

		 * out the call path that was taken.

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/h8300/kernel/setup.c

 *

 *  Copyright (C) 2001-2014 Yoshinori Sato <ysato@users.sourceforge.jp>

/*

 * This file handles the architecture-dependent parts of system setup

 Find main memory where is the kernel */

 setup bootmem globals (we use no_bootmem, but mm still depends on this) */

	/*

	 * get kmalloc into gear

/*

 *	Get CPU information for use by the procfs.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/h8300/kernel/sim-console.c

 *

 *  Copyright (C) 2015 Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 er0 = 1 (stdout) */

 jsr @0xc7 (sys_write) */

/*

 *  linux/arch/h8300/kernel/signal.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file COPYING in the main directory of this archive

 * for more details.

/*

 * uClinux H8/300 support by Yoshinori Sato <ysato@users.sourceforge.jp>

 *                and David McCullough <davidm@snapgear.com>

 *

 * Based on

 * Linux/m68k by Hamish Macdonald

/*

 * ++roman (07/09/96): implemented signal stacks (specially for tosemu on

 * Atari :-) Current limitation: Only one sigstack can be active at one time.

 * If a second signal with SA_ONSTACK set arrives while working on a sigstack,

 * SA_ONSTACK is ignored. This behaviour avoids lots of trouble with nested

 * signal handlers!

/*

 * Do a signal return; undo the signal stack.

 *

 * Keep the return code on the stack quadword aligned!

 * That makes the cache flush below easier.

 Always make any pending restarted system calls return -EINTR */

 restore passed registers */

 disable syscall checks */

 Create the ucontext.  */

 Set up to return from userspace.  */

 sub.l er0,er0; mov.b #__NR_rt_sigreturn,r0l; trapa #0 */

 Set up registers for signal handler */

 GOT base */

/*

 * OK, we're invoking a handler

 are we from a system call? */

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 Whee!  Actually deliver the signal.  */

 Did we come from a system call? */

 If there's no signal to deliver, we just restore the saved mask.  */

/*

 * H8/300 KGDB support

 *

 * Copyright (C) 2015 Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 handle the optional parameters */

 this means that we do not want to exit from the handler */

 Nothing to do */

 Breakpoint instruction: trapa #2 */

/*

 *  linux/arch/h8300/kernel/ptrace.c

 *

 *  Copyright 2015 Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of

 * this archive for more details.

 mode/imask not set */

 modify only T */

/* Mapping from PT_xxx to the stack offset at which the register is

   saved.  Notice that usp has no stack-slot and needs to be treated

 read register */

 build user regs in buffer */

 write back to pt_regs */

		/*

		 * Tracing decided this syscall should not happen.

		 * We'll return a bogus call number to get an ENOSYS

		 * error, but leave the original number in regs->regs[0].

 SPDX-License-Identifier: GPL-2.0

/*

 * libgcc functions - functions that are used internally by the

 * compiler...  (prototypes are not correct though, but that

 * doesn't really matter since they're not versioned).

 gcc lib functions */

 SPDX-License-Identifier: GPL-2.0

 This is where to make the change */

		/* This is the symbol it is referring to.  Note that all

 SPDX-License-Identifier: GPL-2.0

/*

 * linux/arch/h8300/kernel/irq.c

 *

 * Copyright 2014-2015 Yoshinori Sato <ysato@users.sourceforge.jp>

 check romvector format */

 ramvector base address */

 writerble? */

 create redirect table */

 noting do */

/*

 *  linux/arch/h8300/kernel/ptrace_h8s.c

 *    ptrace cpu depend helper functions

 *

 *  Yoshinori Sato <ysato@users.sourceforge.jp>

 *

 * This file is subject to the terms and conditions of the GNU General

 * Public License.  See the file COPYING in the main directory of

 * this archive for more details.

 disable singlestep */

 enable singlestep */

 SPDX-License-Identifier: GPL-2.0

/*

 * This program is used to generate definitions needed by

 * assembly language modules.

 *

 * We use the technique used in the OSF Mach kernel code:

 * generate asm statements containing #defines,

 * compile this file to assembler, and then extract the

 * #defines from the assembly-language output.

 offsets into the task struct */

 offsets into the irq_cpustat_t struct */

 offsets into the thread struct */

 offsets into the pt_regs struct */

 offsets in thread_info structure */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

 w.s.high = 1..1 or 0..0 */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0

/*

 * delay loops

 *

 * Copyright (C) 2015 Yoshinori Sato

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/h8300/boot/compressed/misc.c

 *

 * This is a collection of several routines from gzip-1.0.3

 * adapted for Linux.

 *

 * malloc by Hannu Savolainen 1993 and Matthias Urlichs 1994

 *

 * Adapted for h8300 by Yoshinori Sato 2006

/*

 * gzip declarations

 Halt */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on the x86 implementation.

 *

 * Copyright (C) 2012 ARM Ltd.

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 ARM Ltd.

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * The LSB of the HYP VA tag

/*

 * The HYP VA tag value with the region bit

/*

 * Compute HYP VA by using the same computation as kern_hyp_va().

/*

 * Store a hyp VA <-> PA offset into a EL2-owned variable.

 Compute the offset from the hyp VA and PA of a random symbol. */

/*

 * We want to generate a hyp VA with the following format (with V ==

 * vabits_actual):

 *

 *  63 ... V |     V-1    | V-2 .. tag_lsb | tag_lsb - 1 .. 0

 *  ---------------------------------------------------------

 * | 0000000 | hyp_va_msb |   random tag   |  kern linear VA |

 *           |--------- tag_val -----------|----- va_mask ---|

 *

 * which does not conflict with the idmap regions.

 Where is my RAM region? */

 We have some free bits to insert a random tag. */

/*

 * The .hyp.reloc ELF section contains a list of kimg positions that

 * contains kimg VAs but will be accessed only in hyp execution context.

 * Convert them to hyp VAs. See gen-hyprel.c for more details.

		/*

		 * Each entry contains a 32-bit relative offset from itself

		 * to a kimg VA position.

 Read the kimg VA value at the relocation address. */

 Convert to hyp VA and store back to the relocation address. */

 ROR is a variant of EXTR with Rm = Rn */

 ROR is a variant of EXTR with Rm = Rn */

		/*

		 * VHE doesn't need any address translation, let's NOP

		 * everything.

		 *

		 * Alternatively, if the tag is zero (because the layout

		 * dictates it and we don't have any spare bits in the

		 * address), NOP everything after masking the kernel VA.

	/*

	 * Compute HYP VA by using the same computation as kern_hyp_va()

 Use PC[10:7] to branch to the same vector in KVM */

	/*

	 * Branch over the preamble in order to avoid the initial store on

	 * the stack (which we already perform in the hardening vectors).

 movz x0, #(addr & 0xffff) */

 movk x0, #((addr >> 16) & 0xffff), lsl #16 */

 movk x0, #((addr >> 32) & 0xffff), lsl #32 */

 br x0 */

 Compute target register */

 movz rd, #(val & 0xffff) */

 movk rd, #((val >> 16) & 0xffff), lsl #16 */

 movk rd, #((val >> 32) & 0xffff), lsl #32 */

 movk rd, #((val >> 48) & 0xffff), lsl #48 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Fault injection for both 32 and 64bit guests.

 *

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Based on arch/arm/kvm/emulate.c

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

	/*

	 * Build an {i,d}abort, depending on the level and the

	 * instruction set. Report an external synchronous abort.

	/*

	 * Here, the guest runs in AArch64 mode when in EL1. If we get

	 * an AArch32 fault, it means we managed to trap an EL0 fault.

	/*

	 * Build an unknown exception, depending on the instruction

	 * set.

/*

 * Modelled after TakeDataAbortException() and TakePrefetchAbortException

 * pseudocode.

 Give the guest an IMPLEMENTATION DEFINED exception */

 no need to shuffle FS[4] into DFSR[10] as its 0 */

 !iabt */

/**

 * kvm_inject_dabt - inject a data abort into the guest

 * @vcpu: The VCPU to receive the data abort

 * @addr: The address to report in the DFAR

 *

 * It is assumed that this code is called from the VCPU thread and that the

 * VCPU therefore is not currently executing guest code.

/**

 * kvm_inject_pabt - inject a prefetch abort into the guest

 * @vcpu: The VCPU to receive the prefetch abort

 * @addr: The address to report in the DFAR

 *

 * It is assumed that this code is called from the VCPU thread and that the

 * VCPU therefore is not currently executing guest code.

/**

 * kvm_inject_undefined - inject an undefined instruction into the guest

 * @vcpu: The vCPU in which to inject the exception

 *

 * It is assumed that this code is called from the VCPU thread and that the

 * VCPU therefore is not currently executing guest code.

/**

 * kvm_inject_vabt - inject an async abort / SError into the guest

 * @vcpu: The VCPU to receive the exception

 *

 * It is assumed that this code is called from the VCPU thread and that the

 * VCPU therefore is not currently executing guest code.

 *

 * Systems with the RAS Extensions specify an imp-def ESR (ISV/IDS = 1) with

 * the remaining ISS all-zeros so that this error is not interpreted as an

 * uncategorized RAS error. Without the RAS Extensions we can't specify an ESR

 * value, so the CPU generates an imp-def value.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

 The VMID used in the VTTBR */

	/*

	 * The default is to expose CSV2 == 1 if the HW isn't affected.

	 * Although this is a per-CPU feature, we make it global because

	 * asymmetric systems are just a nuisance.

	 *

	 * Userspace can override this as long as it doesn't promise

	 * the impossible.

/**

 * kvm_arch_init_vm - initializes a VM data structure

 * @kvm:	pointer to the KVM struct

 The maximum number of VCPUs is limited by the host's GIC model */

/**

 * kvm_arch_destroy_vm - destroy the VM data structure

 * @kvm:	pointer to the KVM struct

		/*

		 * ARM64 treats KVM_CAP_NR_CPUS differently from all other

		 * architectures, as it does not always bound it to

		 * KVM_CAP_MAX_VCPUS. It should not matter much because

		 * this is just an advisory value.

		/*

		 * 1: EL1_VTIMER, EL1_PTIMER, and PMU.

		 * (bump this number if adding more devices)

 Force users to call KVM_ARM_VCPU_INIT */

 Set up the timer */

	/*

	 * If we're about to block (most likely because we've just hit a

	 * WFI), we need to sync back the state of the GIC CPU interface

	 * so that we have the latest PMR and group enables. This ensures

	 * that kvm_arch_vcpu_runnable has up-to-date data to decide

	 * whether we have pending interrupts.

	 *

	 * For the same reason, we want to tell GICv4 that we need

	 * doorbells to be signalled, should an interrupt become pending.

	/*

	 * We guarantee that both TLBs and I-cache are private to each

	 * vcpu. If detecting that a vcpu from the same VM has

	 * previously run on the same physical CPU, call into the

	 * hypervisor code to nuke the relevant contexts.

	 *

	 * We might get preempted before the vCPU actually runs, but

	 * over-invalidation doesn't affect correctness.

/**

 * kvm_arch_vcpu_runnable - determine if the vcpu can be scheduled

 * @v:		The VCPU pointer

 *

 * If the guest CPU is not waiting for interrupts or an interrupt line is

 * asserted, the CPU is by definition runnable.

 Just ensure a guest exit from a particular CPU */

/**

 * need_new_vmid_gen - check that the VMID is still valid

 * @vmid: The VMID to check

 *

 * return true if there is a new generation of VMIDs being used

 *

 * The hardware supports a limited set of values with the value zero reserved

 * for the host, so we check if an assigned value belongs to a previous

 * generation, which requires us to assign a new value. If we're the first to

 * use a VMID for the new generation, we must flush necessary caches and TLBs

 * on all CPUs.

 Orders read of kvm_vmid_gen and kvm->arch.vmid */

/**

 * update_vmid - Update the vmid with a valid VMID for the current generation

 * @vmid: The stage-2 VMID information struct

	/*

	 * We need to re-check the vmid_gen here to ensure that if another vcpu

	 * already allocated a valid vmid for this vm, then this vcpu should

	 * use the same vmid.

 First user of a new VMID generation? */

		/*

		 * On SMP we know no other CPUs can use this CPU's or each

		 * other's VMID after force_vm_exit returns since the

		 * kvm_vmid_lock blocks them from reentry to the guest.

		/*

		 * Now broadcast TLB + ICACHE invalidation over the inner

		 * shareable domain to make sure all data structures are

		 * clean.

		/*

		 * Map the VGIC hardware resources before running a vcpu the

		 * first time on this VM.

		/*

		 * Tell the rest of the code that there are userspace irqchip

		 * VMs in the wild.

	/*

	 * Initialize traps for protected VMs.

	 * NOTE: Move to run in EL2 directly, rather than via a hypercall, once

	 * the code is in place for first run initialization at EL2.

 Awaken to handle a signal, request we sleep again later. */

	/*

	 * Make sure we will observe a potential reset request if we've

	 * observed a change to the power state. Pairs with the smp_wmb() in

	 * kvm_psci_vcpu_on().

		/*

		 * Clear IRQ_PENDING requests that were made to guarantee

		 * that a VCPU sees new virtual interrupts.

 The distributor enable bits were changed */

/**

 * kvm_vcpu_exit_request - returns true if the VCPU should *not* enter the guest

 * @vcpu:	The VCPU pointer

 * @ret:	Pointer to write optional return code

 *

 * Returns: true if the VCPU needs to return to a preemptible + interruptible

 *	    and skip guest entry.

 *

 * This function disambiguates between two different types of exits: exits to a

 * preemptible + interruptible kernel context and exits to userspace. For an

 * exit to userspace, this function will write the return code to ret and return

 * true. For an exit to preemptible + interruptible kernel context (i.e. check

 * for pending work and re-enter), return true without writing to ret.

	/*

	 * If we're using a userspace irqchip, then check if we need

	 * to tell a userspace irqchip about timer or PMU level

	 * changes and if so, exit to userspace (the actual level

	 * state gets updated in kvm_timer_update_run and

	 * kvm_pmu_update_run below).

/**

 * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code

 * @vcpu:	The VCPU pointer

 *

 * This function is called through the VCPU_RUN ioctl called from user space. It

 * will execute VM code in a loop until the time slice for the process is used

 * or some emulation is needed from user space in which case the function will

 * return with return value 0 and with the kvm_run structure filled in with the

 * required data for the requested emulation.

		/*

		 * Check conditions before entering the guest

		/*

		 * Preparing the interrupts to be injected also

		 * involves poking the GIC, which must be done in a

		 * non-preemptible context.

		/*

		 * Ensure we set mode to IN_GUEST_MODE after we disable

		 * interrupts and before the final VCPU requests check.

		 * See the comment in kvm_vcpu_exiting_guest_mode() and

		 * Documentation/virt/kvm/vcpu-requests.rst

 Ensure work in x_flush_hwstate is committed */

		/**************************************************************

		 * Enter the guest

		/*

		 * Back from guest

		/*

		 * We must sync the PMU state before the vgic state so

		 * that the vgic can properly sample the updated state of the

		 * interrupt line.

		/*

		 * Sync the vgic state before syncing the timer state because

		 * the timer code needs to know if the virtual timer

		 * interrupts are active.

		/*

		 * Sync the timer hardware state before enabling interrupts as

		 * we don't want vtimer interrupts to race with syncing the

		 * timer virtual interrupt state.

		/*

		 * We may have taken a host interrupt in HYP mode (ie

		 * while executing the guest). This interrupt is still

		 * pending, as we haven't serviced it yet!

		 *

		 * We're now back in SVC mode, with interrupts

		 * disabled.  Enabling the interrupts now will have

		 * the effect of taking the interrupt again, in SVC

		 * mode this time.

		/*

		 * We do local_irq_enable() before calling guest_exit() so

		 * that if a timer interrupt hits while running the guest we

		 * account that tick as being spent in the guest.  We enable

		 * preemption after calling guest_exit() so that if we get

		 * preempted we make sure ticks after that is not counted as

		 * guest time.

 Exit types that need handling before we can be preempted */

		/*

		 * The ARMv8 architecture doesn't give the hypervisor

		 * a mechanism to prevent a guest from dropping to AArch32 EL0

		 * if implemented by the CPU. If we spot the guest in such

		 * state and that we decided it wasn't supposed to do so (like

		 * with the asymmetric AArch32 case), return to userspace with

		 * a fatal error.

			/*

			 * As we have caught the guest red-handed, decide that

			 * it isn't fit for purpose anymore by making the vcpu

			 * invalid. The VMM can try and fix it by issuing  a

			 * KVM_ARM_VCPU_INIT if it really wants to.

 Tell userspace about in-kernel device output levels */

	/*

	 * In the unlikely event that we are returning to userspace

	 * with pending exceptions or PC adjustment, commit these

	 * adjustments in order to give userspace a consistent view of

	 * the vcpu state. Note that this relies on __kvm_adjust_pc()

	 * being preempt-safe on VHE.

 KVM_ARM_IRQ_CPU_FIQ */

	/*

	 * If we didn't change anything, no need to wake up or kick other CPUs

	/*

	 * The vcpu irq_lines field was updated, wake up sleeping VCPUs and

	 * trigger a world-switch round on the running physical CPU to set the

	 * virtual IRQ/FIQ fields in the HCR appropriately.

	/*

	 * Secondary and subsequent calls to KVM_ARM_VCPU_INIT must

	 * use the same target.

 -ENOENT for unknown features, -EINVAL for invalid combinations. */

		/*

		 * Secondary and subsequent calls to KVM_ARM_VCPU_INIT must

		 * use the same feature set.

 Now we know what it is, we can reset it. */

	/*

	 * Ensure a rebooted VM will fault in RAM pages and detect if the

	 * guest MMU is turned off and flush the caches as needed.

	 *

	 * S2FWB enforces all memory accesses to RAM being cacheable,

	 * ensuring that the data side is always coherent. We still

	 * need to invalidate the I-cache though, as FWB does *not*

	 * imply CTR_EL0.DIC.

	/*

	 * Handle the "start in power-off" case.

 check whether the reserved field is zero */

 check whether the pad field is zero */

		/*

		 * We could owe a reset due to PSCI. Handle the pending reset

		 * here to ensure userspace register accesses are ordered after

		 * the reset.

 A lookup table holding the hypervisor VA for each vector slot */

	/*

	 * Calculate the raw per-cpu offset without a translation from the

	 * kernel's mapping to the linear mapping, and store it in tpidr_el2

	 * so that we can use adr_l to access per-cpu variables in EL2.

	 * Also drop the KASAN tag which gets in the way...

	/*

	 * The ID map may be configured to use an extended virtual address

	 * range. This is only the case if system RAM is out of range for the

	 * currently configured page size and VA_BITS, in which case we will

	 * also need the extended virtual range for the HYP ID map, or we won't

	 * be able to enable the EL2 MMU.

	 *

	 * However, at EL2, there is only one TTBR register, and we can't switch

	 * between translation tables *and* update TCR_EL2.T0SZ at the same

	 * time. Bottom line: we need to use the extended range with *both* our

	 * translation tables.

	 *

	 * So use the same T0SZ value we use for the ID map.

	/*

	 * Flush the init params from the data cache because the struct will

	 * be read while the MMU is off.

 Switch from the HYP stub to our own HYP init vector */

	/*

	 * Call initialization code, and switch to the full blown HYP code.

	 * If the cpucaps haven't been finalized yet, something has gone very

	 * wrong, and hyp will crash and burn when it uses any

	 * cpus_have_const_cap() wrapper.

	/*

	 * Disabling SSBD on a non-VHE system requires us to enable SSBS

	 * at EL2.

/*

 * EL2 vectors can be mapped and rerouted in a number of ways,

 * depending on the kernel configuration and CPU present:

 *

 * - If the CPU is affected by Spectre-v2, the hardening sequence is

 *   placed in one of the vector slots, which is executed before jumping

 *   to the real vectors.

 *

 * - If the CPU also has the ARM64_SPECTRE_V3A cap, the slot

 *   containing the hardening sequence is mapped next to the idmap page,

 *   and executed before jumping to the real vectors.

 *

 * - If the CPU only has the ARM64_SPECTRE_V3A cap, then an

 *   empty slot is selected, mapped next to the idmap page, and

 *   executed before jumping to the real vectors.

 *

 * Note that ARM64_SPECTRE_V3A is somewhat incompatible with

 * VHE, as we don't have hypervisor-specific mappings. If the system

 * is VHE and yet selects this capability, it will be ignored.

	/*

	 * kvm_arm_hardware_enabled is left with its old value over

	 * PM_ENTER->PM_EXIT. It is used to indicate PM_EXIT should

	 * re-enable hyp.

			/*

			 * don't update kvm_arm_hardware_enabled here

			 * so that the hardware will be re-enabled

			 * when we resume. See below.

 The hardware was enabled before suspend. */

	/*

	 * Copy the MPIDR <-> logical CPU ID mapping to hyp.

	 * Only copy the set of online CPUs whose features have been chacked

	 * against the finalized system capabilities. The hypervisor will not

	 * allow any other CPUs from the `possible` set to boot.

	/*

	 * If PSCI has not been initialized, protected KVM cannot install

	 * itself on newly booted CPUs.

	/*

	 * Enable hardware so that subsystem initialisation can access EL2.

	/*

	 * Register CPU lower-power notifier

	/*

	 * Init HYP view of VGIC

	/*

	 * Init HYP architected timer support

	/*

	 * The stub hypercalls are now disabled, so set our local flag to

	 * prevent a later re-init attempt in kvm_arch_hardware_enable().

/**

 * Inits Hyp-mode on all online CPUs

	/*

	 * The protected Hyp-mode cannot be initialized if the memory pool

	 * allocation has failed.

	/*

	 * Allocate Hyp PGD and setup Hyp identity mapping

	/*

	 * Allocate stack pages for Hypervisor-mode

	/*

	 * Allocate and initialize pages for Hypervisor-mode percpu regions.

	/*

	 * Map the Hyp-code called directly from the host

	/*

	 * .hyp.bss is guaranteed to be placed at the beginning of the .bss

	 * section thanks to an assertion in the linker script. Map it RW and

	 * the rest of .bss RO.

	/*

	 * Map the Hyp stack pages

 Map Hyp percpu pages */

 Prepare the CPU initialization parameters */

	/*

	 * Flip the static key upfront as that may no longer be possible

	 * once the host stage 2 is installed.

	/*

	 * Exclude HYP BSS from kmemleak so that it doesn't get peeked

	 * at, which would end badly once the section is inaccessible.

	 * None of other sections should ever be introspected.

/**

 * Initialize Hyp-mode and memory mappings on all CPUs.

 NOP: Compiling as a module not supported */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 ARM Ltd.

 * Author: Marc Zyngier <marc.zyngier@arm.com>

	/*

	 * We may see a timer interrupt after vcpu_put() has been called which

	 * sets the CPU's vcpu pointer to NULL, because even though the timer

	 * has been disabled in timer_save_state(), the hardware interrupt

	 * signal may not have been retired from the interrupt controller yet.

/*

 * Returns the earliest expiration time in ns among guest timers.

 * Note that it will return 0 if none of timers can fire.

 If none of timers can fire, then return 0 */

	/*

	 * Check that the timer has really expired from the guest's

	 * PoV (NTP on the host may have forced it to expire

	 * early). If we should have slept longer, restart it.

	/*

	 * Check that the timer has really expired from the guest's

	 * PoV (NTP on the host may have forced it to expire

	 * early). If not ready, schedule for a later time.

 GCC is braindead */

/*

 * Reflect the timer output level into the kvm_run structure

 Populate the device bitmap with the timer states */

 Only called for a fully emulated timer */

	/*

	 * If the timer can fire now, we don't need to have a soft timer

	 * scheduled for the future.  If the timer cannot fire at all,

	 * then we also don't need a soft timer.

 Disable the timer */

 Disable the timer */

/*

 * Schedule the background timer before calling kvm_vcpu_block, so that this

 * thread is removed from its waitqueue and made runnable when there's a timer

 * interrupt to handle.

	/*

	 * If no timers are capable of raising interrupts (disabled or

	 * masked), then there's no more work for us to do.

	/*

	 * At least one guest time will expire. Schedule a background timer.

	 * Set the earliest expiration time among the guest timers.

	/*

	 * Update the timer output so that it is likely to match the

	 * state we're about to restore. If the timer expires between

	 * this point and the register restoration, we'll take the

	 * interrupt anyway.

	/*

	 * Update the timer output so that it is likely to match the

	 * state we're about to restore. If the timer expires between

	 * this point and the register restoration, we'll take the

	 * interrupt anyway.

	/*

	 * When using a userspace irqchip with the architected timers and a

	 * host interrupt controller that doesn't support an active state, we

	 * must still prevent continuously exiting from the guest, and

	 * therefore mask the physical interrupt by disabling it on the host

	 * interrupt controller when the virtual level is high, such that the

	 * guest can make forward progress.  Once we detect the output level

	 * being de-asserted, we unmask the interrupt again so that we exit

	 * from the guest when the timer fires.

	/*

	 * Cancel soft timer emulation, because the only case where we

	 * need it after a vcpu_put is in the context of a sleeping VCPU, and

	 * in that case we already factor in the deadline for the physical

	 * timer when scheduling the bg_timer.

	 *

	 * In any case, we re-schedule the hrtimer for the physical timer when

	 * coming back to the VCPU thread in kvm_timer_vcpu_load().

	/*

	 * The kernel may decide to run userspace after calling vcpu_put, so

	 * we reset cntvoff to 0 to ensure a consistent read between user

	 * accesses to the virtual counter and kernel access to the physical

	 * counter of non-VHE case. For VHE, the virtual counter uses a fixed

	 * virtual offset of zero, so no need to zero CNTVOFF_EL2 register.

/*

 * With a userspace irqchip we have to check if the guest de-asserted the

 * timer and if so, unmask the timer irq signal on the host interrupt

 * controller to ensure that we see future timer signals.

	/*

	 * The bits in CNTV_CTL are architecturally reset to UNKNOWN for ARMv8

	 * and to 0 for ARMv7.  We provide an implementation that always

	 * resets the timer to be disabled and unmasked and is compliant with

	 * the ARMv7 architecture.

 Make the updates of cntvoff for all vtimer contexts atomic */

	/*

	 * When called from the vcpu create path, the CPU being created is not

	 * included in the loop above, so we just set it here as well.

 Synchronize cntvoff across all vtimers of a VM. */

	/*

	 * Set ISTATUS bit if it's expired.

	 * Note that according to ARMv8 ARM Issue A.k, ISTATUS bit is

	 * UNKNOWN when ENABLE bit is 0, so we chose to set ISTATUS bit

	 * regardless of ENABLE bit for our implementation convenience.

 Assume both vtimer and ptimer in the same parent */

 First, do the virtual EL1 timer irq */

 Now let's do the physical EL1 timer irq */

 Without a VGIC we do not map virtual IRQs to physical IRQs */

	/*

	 * At this stage, we have the guarantee that the vgic is both

	 * available and initialized.

/*

 * On VHE system, we only need to configure the EL2 timer trap register once,

 * not for every world switch.

 * The host kernel runs at EL2 with HCR_EL2.TGE == 1,

 * and this makes those bits have no effect for the host kernel execution.

 When HCR_EL2.E2H ==1, EL1PCEN and EL1PCTEN are shifted by 10 */

	/*

	 * VHE systems allow the guest direct access to the EL1 physical

	 * timer/counter.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VGIC system registers handling functions for AArch64 mode

		/*

		 * Disallow restoring VM state if not supported by this

		 * hardware.

		/*

		 * Here set VMCR.CTLR in ICC_CTLR_EL1 layout.

		 * The vgic_set_vmcr() will convert to ICH_VMCR layout.

		/*

		 * The VMCR.CTLR value is in ICC_CTLR_EL1 layout.

		 * Extract it directly using ICC_CTLR_EL1 reg definitions.

 Validate SRE bit */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 Linaro Ltd.

 * Author: Shannon Zhao <shannon.zhao@linaro.org>

 Shouldn't be here, just for sanity */

/**

 * kvm_pmu_idx_is_64bit - determine if select_idx is a 64bit counter

 * @vcpu: The vcpu pointer

 * @select_idx: The counter index

/**

 * kvm_pmu_pmc_is_chained - determine if the pmc is chained

 * @pmc: The PMU counter pointer

/**

 * kvm_pmu_idx_is_high_counter - determine if select_idx is a high/low counter

 * @select_idx: The counter index

/**

 * kvm_pmu_get_canonical_pmc - obtain the canonical pmc

 * @pmc: The PMU counter pointer

 *

 * When a pair of PMCs are chained together we use the low counter (canonical)

 * to hold the underlying perf event.

/**

 * kvm_pmu_idx_has_chain_evtype - determine if the event type is chain

 * @vcpu: The vcpu pointer

 * @select_idx: The counter index

/**

 * kvm_pmu_get_pair_counter_value - get PMU counter value

 * @vcpu: The vcpu pointer

 * @pmc: The PMU counter pointer

	/*

	 * The real counter value is equal to the value of counter register plus

	 * the value perf event counts.

/**

 * kvm_pmu_get_counter_value - get PMU counter value

 * @vcpu: The vcpu pointer

 * @select_idx: The counter index

/**

 * kvm_pmu_set_counter_value - set PMU counter value

 * @vcpu: The vcpu pointer

 * @select_idx: The counter index

 * @val: The counter value

 Recreate the perf event to reflect the updated sample_period */

/**

 * kvm_pmu_release_perf_event - remove the perf event

 * @pmc: The PMU counter pointer

/**

 * kvm_pmu_stop_counter - stop PMU counter

 * @pmc: The PMU counter pointer

 *

 * If this counter has been configured to monitor some event, release it here.

/**

 * kvm_pmu_vcpu_init - assign pmu counter idx for cpu

 * @vcpu: The vcpu pointer

 *

/**

 * kvm_pmu_vcpu_reset - reset pmu state for cpu

 * @vcpu: The vcpu pointer

 *

/**

 * kvm_pmu_vcpu_destroy - free perf event of PMU for cpu

 * @vcpu: The vcpu pointer

 *

/**

 * kvm_pmu_enable_counter_mask - enable selected PMU counters

 * @vcpu: The vcpu pointer

 * @val: the value guest writes to PMCNTENSET register

 *

 * Call perf_event_enable to start counting the perf event

 A change in the enable state may affect the chain state */

 At this point, pmc must be the canonical */

/**

 * kvm_pmu_disable_counter_mask - disable selected PMU counters

 * @vcpu: The vcpu pointer

 * @val: the value guest writes to PMCNTENCLR register

 *

 * Call perf_event_disable to stop counting the perf event

 A change in the enable state may affect the chain state */

 At this point, pmc must be the canonical */

/*

 * Reflect the PMU overflow interrupt output level into the kvm_run structure

 Populate the timer bitmap for user space */

/**

 * kvm_pmu_flush_hwstate - flush pmu state to cpu

 * @vcpu: The vcpu pointer

 *

 * Check if the PMU has overflowed while we were running in the host, and inject

 * an interrupt if that was the case.

/**

 * kvm_pmu_sync_hwstate - sync pmu state from cpu

 * @vcpu: The vcpu pointer

 *

 * Check if the PMU has overflowed while we were running in the guest, and

 * inject an interrupt if that was the case.

/**

 * When perf interrupt is an NMI, we cannot safely notify the vcpu corresponding

 * to the event.

 * This is why we need a callback to do it once outside of the NMI context.

/**

 * When the perf event overflows, set the overflow status and inform the vcpu.

	/*

	 * Reset the sample period to the architectural limit,

	 * i.e. the point where the counter overflows.

/**

 * kvm_pmu_software_increment - do software increment

 * @vcpu: The vcpu pointer

 * @val: the value guest writes to PMSWINC register

 Weed out disabled counters */

 PMSWINC only applies to ... SW_INC! */

 increment this even SW_INC counter */

 no overflow on the low part */

 increment the high counter */

 mark overflow on the high counter */

 mark overflow on low counter */

/**

 * kvm_pmu_handle_pmcr - handle PMCR register

 * @vcpu: The vcpu pointer

 * @val: the value guest writes to PMCR register

/**

 * kvm_pmu_create_perf_event - create a perf event for a counter

 * @vcpu: The vcpu pointer

 * @select_idx: The number of selected counter

	/*

	 * For chained counters the event type and filtering attributes are

	 * obtained from the low/even counter. We also use this counter to

	 * determine if the event is enabled/disabled.

 Software increment event doesn't need to be backed by a perf event */

	/*

	 * If we have a filter in place and that the event isn't allowed, do

	 * not install a perf event either.

 Don't count EL2 events */

 Don't count host events */

		/**

		 * The initial sample period (overflow count) of an event. For

		 * chained counters we only support overflow interrupts on the

		 * high counter.

 The initial sample period (overflow count) of an event. */

/**

 * kvm_pmu_update_pmc_chained - update chained bitmap

 * @vcpu: The vcpu pointer

 * @select_idx: The number of selected counter

 *

 * Update the chained bitmap based on the event type written in the

 * typer register and the enable state of the odd register.

		/*

		 * During promotion from !chained to chained we must ensure

		 * the adjacent counter is stopped and its event destroyed

/**

 * kvm_pmu_set_counter_event_type - set selected counter to monitor some event

 * @vcpu: The vcpu pointer

 * @data: The data guest writes to PMXEVTYPER_EL0

 * @select_idx: The number of selected counter

 *

 * When OS accesses PMXEVTYPER_EL0, that means it wants to set a PMC to count an

 * event with given hardware event number. Here we call perf_event API to

 * emulate this action and create a kernel perf event for it.

	/*

	 * Create a dummy event that only counts user cycles. As we'll never

	 * leave this function with the event being live, it will never

	 * count anything. But it allows us to probe some of the PMU

	 * details. Yes, this is terrible.

		/*

		 * Don't advertise STALL_SLOT, as PMMIR_EL0 is handled

		 * as RAZ

	/*

	 * A valid interrupt configuration for the PMU is either to have a

	 * properly configured interrupt number and using an in-kernel

	 * irqchip, or to not have an in-kernel GIC and not set an IRQ.

		/*

		 * If we are using an in-kernel vgic, at this point we know

		 * the vgic will be initialized, so we can check the PMU irq

		 * number against the dimensions of the vgic and make sure

		 * it's valid.

 One-off reload of the PMU on first run */

		/*

		 * If using the PMU with an in-kernel virtual GIC

		 * implementation, we require the GIC to be already

		 * initialized when initializing the PMU.

/*

 * For one VM the interrupt type must be same for each vcpu.

 * As a PPI, the interrupt number is the same for all vcpus,

 * while as an SPI it must be a separate number per vcpu.

 The PMU overflow interrupt can be a PPI or a valid SPI. */

			/*

			 * The default depends on the first applied filter.

			 * If it allows events, the default is to deny.

			 * Conversely, if the first filter denies a set of

			 * events, the default is to allow.

 SPDX-License-Identifier: GPL-2.0

/*

 * arch/arm64/kvm/fpsimd.c: Guest/host FPSIMD context coordination helpers

 *

 * Copyright 2018 Arm Limited

 * Author: Dave Martin <Dave.Martin@arm.com>

/*

 * Called on entry to KVM_RUN unless this vcpu previously ran at least

 * once and the most recent prior KVM_RUN for this vcpu was called from

 * the same task as current (highly likely).

 *

 * This is guaranteed to execute before kvm_arch_vcpu_load_fp(vcpu),

 * such that on entering hyp the relevant parts of current are already

 * mapped.

	/*

	 * Make sure the host task thread flags and fpsimd state are

	 * visible to hyp:

/*

 * Prepare vcpu for saving the host's FPSIMD state and loading the guest's.

 * The actual loading is done by the FPSIMD access trap taken to hyp.

 *

 * Here, we just set the correct metadata to indicate that the FPSIMD

 * state in the cpu regs (if any) belongs to current on the host.

 *

 * TIF_SVE is backed up here, since it may get clobbered with guest state.

 * This flag is restored by kvm_arch_vcpu_put_fp(vcpu).

/*

 * If the guest FPSIMD state was loaded, update the host's context

 * tracking data mark the CPU FPSIMD regs as dirty and belonging to vcpu

 * so that they will be written back if the kernel clobbers them due to

 * kernel-mode NEON before re-entry into the guest.

/*

 * Write back the vcpu FPSIMD regs if they are dirty, and invalidate the

 * cpu FPSIMD regs so that they can't be spuriously reused if this vcpu

 * disappears and another task or vcpu appears that recycles the same

 * struct fpsimd_state.

 Restore the VL that was saved when bound to the CPU */

		/*

		 * The FPSIMD/SVE state in the CPU has not been touched, and we

		 * have SVE (and VHE): CPACR_EL1 (alias CPTR_EL2) has been

		 * reset to CPACR_EL1_DEFAULT by the Hyp code, disabling SVE

		 * for EL0.  To avoid spurious traps, restore the trap state

		 * seen by kvm_arch_vcpu_load_fp():

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Derived from arch/arm/kvm/handle_exit.c:

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

	/*

	 * "If an SMC instruction executed at Non-secure EL1 is

	 * trapped to EL2 because HCR_EL2.TSC is 1, the exception is a

	 * Trap exception, not a Secure Monitor Call exception [...]"

	 *

	 * We need to advance the PC after the trap, as it would

	 * otherwise return to the same address...

/*

 * Guest access to FP/ASIMD registers are routed to this handler only

 * when the system doesn't support FP/ASIMD.

/**

 * kvm_handle_wfx - handle a wait-for-interrupts or wait-for-event

 *		    instruction executed by a guest

 *

 * @vcpu:	the vcpu pointer

 *

 * WFE: Yield the CPU and come back to this vcpu when the scheduler

 * decides to.

 * WFI: Simply call kvm_vcpu_block(), which will halt execution of

 * world-switches and schedule other host processes until there is an

 * incoming IRQ or FIQ to the VM.

/**

 * kvm_handle_guest_debug - handle a debug exception instruction

 *

 * @vcpu:	the vcpu pointer

 *

 * We route all debug exceptions through the same handler. If both the

 * guest and host are using the same debug facilities it will be up to

 * userspace to re-inject the correct exception for guest delivery.

 *

 * @return: 0 (while setting vcpu->run->exit_reason)

 Until SVE is supported for guests: */

/*

 * Guest usage of a ptrauth instruction (which the guest EL1 did not turn into

 * a NOP). If we get here, it is that we didn't fixup ptrauth on exit, and all

 * that we can do is give the guest an UNDEF.

/*

 * We may be single-stepping an emulated instruction. If the emulation

 * has been completed in the kernel, we can return to userspace with a

 * KVM_EXIT_DEBUG, otherwise userspace needs to complete its

 * emulation first.

	/*

	 * See ARM ARM B1.14.1: "Hyp traps on instructions

	 * that fail their condition code check"

/*

 * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on

 * proper exit to userspace.

		/*

		 * EL2 has been reset to the hyp-stub. This happens when a guest

		 * is pre-empted by kvm_reboot()'s shutdown call.

		/*

		 * We attempted an illegal exception return.  Guest state must

		 * have been corrupted somehow.  Give up.

 For exit types that need handling before we can be preempted */

	/*

	 * The nVHE hyp symbols are not included by kallsyms to avoid issues

	 * with aliasing. That means that the symbols cannot be printed with the

	 * "%pS" format specifier, so fall back to the vmlinux address if

	 * there's no better option.

 All hyp bugs, including warnings, are treated as fatal. */

	/*

	 * Hyp has panicked and we're going to handle that by panicking the

	 * kernel. The kernel offset will be revealed in the panic so we're

	 * also safe to reveal the hyp offset as a debugging aid for translating

	 * hyp VAs to vmlinux addresses.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

/**

 * kvm_handle_mmio_return -- Handle MMIO loads after user space emulation

 *			     or in-kernel IO emulation

 *

 * @vcpu: The VCPU pointer

 Detect an already handled MMIO return */

	/*

	 * The MMIO instruction is emulated and should not be re-executed

	 * in the guest.

	/*

	 * No valid syndrome? Ask userspace for help if it has

	 * volunteered to do so, and bail out otherwise.

	/*

	 * Prepare MMIO operation. First decode the syndrome data we get

	 * from the CPU. Then try if some in-kernel emulation feels

	 * responsible, otherwise let user space do its magic.

 Now prepare kvm_run for the potential return to userland. */

 We handled the access successfully in the kernel. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Derived from arch/arm/kvm/guest.c:

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

	/*

	 * The KVM_REG_ARM64_SVE regs must be used instead of

	 * KVM_REG_ARM_CORE for accessing the FPSIMD V-registers on

	 * SVE-enabled vcpus:

	/*

	 * Because the kvm_regs structure is a mix of 32, 64 and

	 * 128bit fields, we index it as if it was a 32bit

	 * array. Hence below, nr_regs is the number of entries, and

	 * off the index in the "array".

 Our ID is an index into the kvm_regs struct. */

 Our ID is an index into the kvm_regs struct. */

		/*

		 * Either we are dealing with user mode, and only the

		 * first 15 registers (+ PC) must be narrowed to 32bit.

		 * AArch32 r0-r14 conveniently map to AArch64 x0-x14.

		/*

		 * Otherwide, this is a priviledged mode, and *all* the

		 * registers must be narrowed to 32bit.

 too late! */

	/*

	 * Vector lengths supported by the host can't currently be

	 * hidden from the guest individually: instead we can only set a

	 * maximum via ZCR_EL2.LEN.  So, make sure the available vector

	 * lengths match the set requested exactly up to the requested

	 * maximum:

 Can't run with no vector lengths at all: */

 vcpu->arch.sve_state will be alloc'd by kvm_vcpu_finalize_sve() */

/*

 * Number of register slices required to cover each whole SVE register.

 * NOTE: Only the first slice every exists, for now.

 * If you are tempted to modify this, you must also rework sve_reg_to_region()

 * to match:

 Bounds of a single SVE register slice within vcpu->arch.sve_state */

 offset into sve_state in kernel memory */

 length in kernel memory */

 extra trailing padding in user memory */

/*

 * Validate SVE register ID and get sanitised bounds for user/kernel SVE

 * register copy

 reg ID ranges for Z- registers */

 reg ID ranges for P- registers and FFR (which are contiguous) */

 User-requested offset and length */

 Maximum permitted length */

 Verify that the P-regs and FFR really do have contiguous IDs: */

 Verify that we match the UAPI header: */

 Handle the KVM_REG_ARM64_SVE_VLS pseudo-reg as a special case: */

 Try to interpret reg ID as an architectural SVE register... */

 Handle the KVM_REG_ARM64_SVE_VLS pseudo-reg as a special case: */

 Try to interpret reg ID as an architectural SVE register... */

/**

 * ARM64 versions of the TIMER registers, always available on arm64

 Policed by KVM_GET_REG_LIST: */

 FFR */)

 KVM_REG_ARM64_SVE_VLS */

 Policed by KVM_GET_REG_LIST: */

	/*

	 * Enumerate this first, so that userspace can save/restore in

	 * the order reported by KVM_GET_REG_LIST:

/**

 * kvm_arm_num_regs - how many registers do we present via KVM_GET_ONE_REG

 *

 * This is for all registers.

/**

 * kvm_arm_copy_reg_indices - get indices of all registers.

 *

 * We do core registers right here, then we append system regs.

 We currently use nothing arch-specific in upper 32 bits */

 We currently use nothing arch-specific in upper 32 bits */

	/*

	 * We never return a pending ext_dabt here because we deliver it to

	 * the virtual CPU directly when setting the event and it's no longer

	 * 'pending' at this point.

 Return a default generic target */

	/*

	 * For now, we don't return any features.

	 * In future, we might use features to return target

	 * specific features available for the preferred

	 * target type.

/**

 * kvm_arch_vcpu_ioctl_set_guest_debug - set up guest debugging

 * @kvm:	pointer to the KVM struct

 * @kvm_guest_debug: the ioctl data buffer

 *

 * This sets up and enables the VM for guest debugging. Userspace

 * passes in a control flag to enable different debug types and

 * potentially other architecture specific information in the rest of

 * the structure.

 Hardware assisted Break and Watch points */

 If not enabled clear all flags */

 Reject ZONE_DEVICE memory */

 No tags in memory, so write zeros */

			/*

			 * Set the flag after checking the write

			 * completed fully

 If some data has been copied report the number of bytes copied */

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright 2019 Arm Limited

 * Author: Andrew Murray <Andrew.Murray@arm.com>

/*

 * Given the perf event attributes and system type, determine

 * if we are going to need to switch counters at guest entry/exit.

	/**

	 * With VHE the guest kernel runs at EL1 and the host at EL2,

	 * where user (EL0) is excluded then we have no reason to switch

	 * counters.

 Only switch if attributes are different */

/*

 * Add events to track that we may want to switch at guest entry/exit

 * time.

/*

 * Stop tracking events

/*

 * Read a value direct from PMEVTYPER<idx> where idx is 0-30

 * or PMCCFILTR_EL0 where idx is ARMV8_PMU_CYCLE_IDX (31).

/*

 * Write a value direct to PMEVTYPER<idx> where idx is 0-30

 * or PMCCFILTR_EL0 where idx is ARMV8_PMU_CYCLE_IDX (31).

/*

 * Modify ARMv8 PMU events to include EL0 counting

/*

 * Modify ARMv8 PMU events to exclude EL0 counting

/*

 * On VHE ensure that only guest events have EL0 counting enabled.

 * This is called from both vcpu_{load,put} and the sysreg handling.

 * Since the latter is preemptible, special care must be taken to

 * disable preemption.

/*

 * On VHE ensure that only host events have EL0 counting enabled

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Debug and Guest Debug support

 *

 * Copyright (C) 2015 - Linaro Ltd

 * Author: Alex Bennée <alex.bennee@linaro.org>

 These are the bits of MDSCR_EL1 we may manipulate */

/**

 * save/restore_guest_debug_regs

 *

 * For some debug operations we need to tweak some guest registers. As

 * a result we need to save the state of those registers before we

 * make those modifications.

 *

 * Guest access to MDSCR_EL1 is trapped by the hypervisor and handled

 * after we have restored the preserved value to the main context.

/**

 * kvm_arm_init_debug - grab what we need for debug

 *

 * Currently the sole task of this function is to retrieve the initial

 * value of mdcr_el2 so we can preserve MDCR_EL2.HPMN which has

 * presumably been set-up by some knowledgeable bootcode.

 *

 * It is called once per-cpu during CPU hyp initialisation.

/**

 * kvm_arm_setup_mdcr_el2 - configure vcpu mdcr_el2 value

 *

 * @vcpu:	the vcpu pointer

 *

 * This ensures we will trap access to:

 *  - Performance monitors (MDCR_EL2_TPM/MDCR_EL2_TPMCR)

 *  - Debug ROM Address (MDCR_EL2_TDRA)

 *  - OS related registers (MDCR_EL2_TDOSA)

 *  - Statistical profiler (MDCR_EL2_TPMS/MDCR_EL2_E2PB)

 *  - Self-hosted Trace Filter controls (MDCR_EL2_TTRF)

 *  - Self-hosted Trace (MDCR_EL2_TTRF/MDCR_EL2_E2TB)

	/*

	 * This also clears MDCR_EL2_E2PB_MASK and MDCR_EL2_E2TB_MASK

	 * to disable guest access to the profiling and trace buffers

 Is the VM being debugged by userspace? */

 Route all software debug exceptions to EL2 */

	/*

	 * Trap debug register access when one of the following is true:

	 *  - Userspace is using the hardware to debug the guest

	 *  (KVM_GUESTDBG_USE_HW is set).

	 *  - The guest is not using debug (KVM_ARM64_DEBUG_DIRTY is clear).

/**

 * kvm_arm_vcpu_init_debug - setup vcpu debug traps

 *

 * @vcpu:	the vcpu pointer

 *

 * Set vcpu initial mdcr_el2 value.

/**

 * kvm_arm_reset_debug_ptr - reset the debug ptr to point to the vcpu state

/**

 * kvm_arm_setup_debug - set up debug related stuff

 *

 * @vcpu:	the vcpu pointer

 *

 * This is called before each entry into the hypervisor to setup any

 * debug related registers.

 *

 * Additionally, KVM only traps guest accesses to the debug registers if

 * the guest is not actively using them (see the KVM_ARM64_DEBUG_DIRTY

 * flag on vcpu->arch.flags).  Since the guest must not interfere

 * with the hardware state when debugging the guest, we must ensure that

 * trapping is enabled whenever we are debugging the guest using the

 * debug registers.

 Is Guest debugging in effect? */

 Save guest debug state */

		/*

		 * Single Step (ARM ARM D2.12.3 The software step state

		 * machine)

		 *

		 * If we are doing Single Step we need to manipulate

		 * the guest's MDSCR_EL1.SS and PSTATE.SS. Once the

		 * step has occurred the hypervisor will trap the

		 * debug exception and we return to userspace.

		 *

		 * If the guest attempts to single step its userspace

		 * we would have to deal with a trapped exception

		 * while in the guest kernel. Because this would be

		 * hard to unwind we suppress the guest's ability to

		 * do so by masking MDSCR_EL.SS.

		 *

		 * This confuses guest debuggers which use

		 * single-step behind the scenes but everything

		 * returns to normal once the host is no longer

		 * debugging the system.

		/*

		 * HW Breakpoints and watchpoints

		 *

		 * We simply switch the debug_ptr to point to our new

		 * external_debug_state which has been populated by the

		 * debug ioctl. The existing KVM_ARM64_DEBUG_DIRTY

		 * mechanism ensures the registers are updated on the

		 * world switch.

 Enable breakpoints/watchpoints */

 If KDE or MDE are set, perform a full save/restore cycle. */

 Write mdcr_el2 changes since vcpu_load on VHE systems */

		/*

		 * If we were using HW debug we need to restore the

		 * debug_ptr to the guest debug state.

 For VHE, there is nothing to do */

	/*

	 * If SPE is present on this CPU and is available at current EL,

	 * we may need to check if the host state needs to be saved.

 Check if we have TRBE implemented and available at the host */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2020 Arm Ltd.

 Those values are deliberately separate from the generic SMCCC definitions. */

 get as many bits as we need to fulfil the request */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * This is an implementation of the Power State Coordination Interface

 * as described in ARM document number ARM DEN 0022A.

	/*

	 * NOTE: For simplicity, we make VCPU suspend emulation to be

	 * same-as WFI (Wait-for-interrupt) emulation.

	 *

	 * This means for KVM the wakeup events are interrupts and

	 * this is consistent with intended use of StateID as described

	 * in section 5.4.1 of PSCI v0.2 specification (ARM DEN 0022A).

	 *

	 * Further, we also treat power-down request to be same as

	 * stand-by request as-per section 5.4.2 clause 3 of PSCI v0.2

	 * specification (ARM DEN 0022A). This means all suspend states

	 * for KVM will preserve the register state.

	/*

	 * Make sure the caller requested a valid CPU and that the CPU is

	 * turned off.

 Propagate caller endianness */

	/*

	 * NOTE: We always update r0 (or x0) because for PSCI v0.1

	 * the general purpose registers are undefined upon CPU_ON.

	/*

	 * Make sure the reset request is observed if the change to

	 * power_state is observed.

 Determine target affinity mask */

 Ignore other bits of target affinity */

	/*

	 * If one or more VCPU matching target affinity are running

	 * then ON else OFF

	/*

	 * The KVM ABI specifies that a system event exit may call KVM_RUN

	 * again and may perform shutdown/reboot at a later time that when the

	 * actual request is made.  Since we are implementing PSCI and a

	 * caller of PSCI reboot and shutdown expects that the system shuts

	 * down or reboots immediately, let's make sure that VCPUs are not run

	 * after this call is handled and before the VCPUs have been

	 * re-initialized.

	/*

	 * Zero the input registers' upper 32 bits. They will be fully

	 * zeroed on exit, so we're fine changing them in place.

 Disallow these functions for 32bit guests */

		/*

		 * Bits[31:16] = Major Version = 0

		 * Bits[15:0] = Minor Version = 2

		/*

		 * Trusted OS is MP hence does not require migration

	         * or

		 * Trusted OS is not present

		/*

		 * We shouldn't be going back to guest VCPU after

		 * receiving SYSTEM_OFF request.

		 *

		 * If user space accidentally/deliberately resumes

		 * guest VCPU after SYSTEM_OFF request then guest

		 * VCPU should see internal failure from PSCI return

		 * value. To achieve this, we preload r0 (or x0) with

		 * PSCI return value INTERNAL_FAILURE.

		/*

		 * Same reason as SYSTEM_OFF for preloading r0 (or x0)

		 * with PSCI return value INTERNAL_FAILURE.

/**

 * kvm_psci_call - handle PSCI call if r0 value is in range

 * @vcpu: Pointer to the VCPU struct

 *

 * Handle PSCI calls from guests through traps from HVC instructions.

 * The calling convention is similar to SMC calls to the secure world

 * where the function number is placed in r0.

 *

 * This function returns: > 0 (success), 0 (success but exit to user

 * space), and < 0 (errors)

 *

 * Errors:

 * -EINVAL: Unrecognized PSCI function

 PSCI version and two workaround registers */

/*

 * Convert the workaround level into an easy-to-compare number, where higher

 * values mean better protection.

			/*

			 * As for the hypercall discovery, we pretend we

			 * don't have any FW mitigation if SSBS is there at

			 * all times.

 The enabled bit must not be set unless the level is AVAIL. */

		/*

		 * Map all the possible incoming states to the only two we

		 * really want to deal with.

		/*

		 * We can deal with NOT_AVAIL on NOT_REQUIRED, but not the

		 * other way around.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Arm Ltd.

	/*

	 * Start counting stolen time from the time the guest requests

	 * the feature enabled.

 Check the address is in a valid memslot */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

/*

 * Release kvm_mmu_lock periodically if the memory region is large. Otherwise,

 * we may see kernel panics with CONFIG_DETECT_HUNG_TASK,

 * CONFIG_LOCKUP_DETECTOR, CONFIG_LOCKDEP. Additionally, holding the lock too

 * long will also starve other vCPUs. We have to also make sure that the page

 * tables are not freed while we released the lock.

/**

 * kvm_flush_remote_tlbs() - flush all VM TLB entries for v7/8

 * @kvm:	pointer to kvm structure.

 *

 * Interface to HYP function to flush all VM TLB entries

 Allocated with __GFP_ZERO, so no need to zero */

/*

 * Unmapping vs dcache management:

 *

 * If a guest maps certain memory pages as uncached, all writes will

 * bypass the data cache and go directly to RAM.  However, the CPUs

 * can still speculate reads (not writes) and fill cache lines with

 * data.

 *

 * Those cache lines will be *clean* cache lines though, so a

 * clean+invalidate operation is equivalent to an invalidate

 * operation, because no cache lines are marked dirty.

 *

 * Those clean cache lines could be filled prior to an uncached write

 * by the guest, and the cache coherent IO subsystem would therefore

 * end up writing old data to disk.

 *

 * This is why right after unmapping a page/section and invalidating

 * the corresponding TLBs, we flush to make sure the IO subsystem will

 * never hit in the cache.

 *

 * This is all avoided on systems that have ARM64_HAS_STAGE2_FWB, as

 * we then fully enforce cacheability of RAM, no matter what the guest

 * does.

/**

 * unmap_stage2_range -- Clear stage2 page table entries to unmap a range

 * @mmu:   The KVM stage-2 MMU pointer

 * @start: The intermediate physical base address of the range to unmap

 * @size:  The size of the area to unmap

 * @may_block: Whether or not we are permitted to block

 *

 * Clear a range of stage-2 mappings, lowering the various ref-counts.  Must

 * be called while holding mmu_lock (unless for freeing the stage2 pgd before

 * destroying the VM), otherwise another faulting VCPU may come in and mess

 * with things behind our backs.

/**

 * stage2_flush_vm - Invalidate cache for pages mapped in stage 2

 * @kvm: The struct kvm pointer

 *

 * Go through the stage 2 page tables and invalidate any cache lines

 * backing memory already mapped to the VM.

/**

 * free_hyp_pgds - free Hyp-mode page tables

	/*

	 * This can happen at boot time when __create_hyp_mappings() is called

	 * after the hyp protection has been enabled, but the static key has

	 * not been flipped yet.

/**

 * create_hyp_mappings - duplicate a kernel virtual address range in Hyp mode

 * @from:	The virtual kernel start address of the range

 * @to:		The virtual kernel end address of the range (exclusive)

 * @prot:	The protection to be applied to this range

 *

 * The same virtual address as the kernel virtual address is also used

 * in Hyp-mode mapping (modulo HYP_PAGE_OFFSET) to the same underlying

 * physical pages.

	/*

	 * This assumes that we have enough space below the idmap

	 * page to allocate our VAs. If not, the check below will

	 * kick. A potential alternative would be to detect that

	 * overflow and switch to an allocation above the idmap.

	 *

	 * The allocated size is always a multiple of PAGE_SIZE.

	/*

	 * Verify that BIT(VA_BITS - 1) hasn't been flipped by

	 * allocating the new area, as it would indicate we've

	 * overflowed the idmap/IO address range.

/**

 * create_hyp_io_mappings - Map IO into both kernel and HYP

 * @phys_addr:	The physical start address which gets mapped

 * @size:	Size of the region being mapped

 * @kaddr:	Kernel VA for this mapping

 * @haddr:	HYP VA for this mapping

/**

 * create_hyp_exec_mappings - Map an executable range into HYP

 * @phys_addr:	The physical start address which gets mapped

 * @size:	Size of the region being mapped

 * @haddr:	HYP VA for this mapping

 We shouldn't need any other callback to walk the PT */

 Keep GCC quiet... */

/**

 * kvm_init_stage2_mmu - Initialise a S2 MMU strucrure

 * @kvm:	The pointer to the KVM structure

 * @mmu:	The pointer to the s2 MMU structure

 *

 * Allocates only the stage-2 HW PGD level table(s).

 * Note we don't need locking here as this is only called when the VM is

 * created, which can only be done once.

	/*

	 * A memory region could potentially cover multiple VMAs, and any holes

	 * between them, so iterate over all of them to find out if we should

	 * unmap any of them.

	 *

	 *     +--------------------------------------------+

	 * +---------------+----------------+   +----------------+

	 * |   : VMA 1     |      VMA 2     |   |    VMA 3  :    |

	 * +---------------+----------------+   +----------------+

	 *     |               memory region                |

	 *     +--------------------------------------------+

		/*

		 * Take the intersection of this VMA with the memory region

/**

 * stage2_unmap_vm - Unmap Stage-2 RAM mappings

 * @kvm: The struct kvm pointer

 *

 * Go through the memregions and unmap any regular RAM

 * backing memory already mapped to the VM.

/**

 * kvm_phys_addr_ioremap - map a device range to guest IPA

 *

 * @kvm:	The KVM pointer

 * @guest_ipa:	The IPA at which to insert the mapping

 * @pa:		The physical address of the device

 * @size:	The size of the mapping

 * @writable:   Whether or not to create a writable mapping

/**

 * stage2_wp_range() - write protect stage2 memory region range

 * @mmu:        The KVM stage-2 MMU pointer

 * @addr:	Start address of range

 * @end:	End address of range

/**

 * kvm_mmu_wp_memory_region() - write protect stage 2 entries for memory slot

 * @kvm:	The KVM pointer

 * @slot:	The memory slot to write protect

 *

 * Called to start logging dirty pages after memory region

 * KVM_MEM_LOG_DIRTY_PAGES operation is called. After this function returns

 * all present PUD, PMD and PTEs are write protected in the memory region.

 * Afterwards read of dirty page log can be called.

 *

 * Acquires kvm_mmu_lock. Called with kvm->slots_lock mutex acquired,

 * serializing operations for VM memory regions.

/**

 * kvm_mmu_write_protect_pt_masked() - write protect dirty pages

 * @kvm:	The KVM pointer

 * @slot:	The memory slot associated with mask

 * @gfn_offset:	The gfn offset in memory slot

 * @mask:	The mask of dirty pages at offset 'gfn_offset' in this memory

 *		slot to be write protected

 *

 * Walks bits set in mask write protects the associated pte's. Caller must

 * acquire kvm_mmu_lock.

/*

 * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected

 * dirty pages.

 *

 * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to

 * enable dirty logging for them.

 The memslot and the VMA are guaranteed to be aligned to PAGE_SIZE */

	/*

	 * Pages belonging to memslots that don't have the same alignment

	 * within a PMD/PUD for userspace and IPA cannot be mapped with stage-2

	 * PMD/PUD entries, because we'll end up mapping the wrong pages.

	 *

	 * Consider a layout like the following:

	 *

	 *    memslot->userspace_addr:

	 *    +-----+--------------------+--------------------+---+

	 *    |abcde|fgh  Stage-1 block  |    Stage-1 block tv|xyz|

	 *    +-----+--------------------+--------------------+---+

	 *

	 *    memslot->base_gfn << PAGE_SHIFT:

	 *      +---+--------------------+--------------------+-----+

	 *      |abc|def  Stage-2 block  |    Stage-2 block   |tvxyz|

	 *      +---+--------------------+--------------------+-----+

	 *

	 * If we create those stage-2 blocks, we'll end up with this incorrect

	 * mapping:

	 *   d -> f

	 *   e -> g

	 *   f -> h

	/*

	 * Next, let's make sure we're not trying to map anything not covered

	 * by the memslot. This means we have to prohibit block size mappings

	 * for the beginning and end of a non-block aligned and non-block sized

	 * memory slot (illustrated by the head and tail parts of the

	 * userspace view above containing pages 'abcde' and 'xyz',

	 * respectively).

	 *

	 * Note that it doesn't matter if we do the check using the

	 * userspace_addr or the base_gfn, as both are equally aligned (per

	 * the check above) and equally sized.

/*

 * Check if the given hva is backed by a transparent huge page (THP) and

 * whether it can be mapped using block mapping in stage2. If so, adjust

 * the stage2 PFN and IPA accordingly. Only PMD_SIZE THPs are currently

 * supported. This will need to be updated to support other THP sizes.

 *

 * Returns the size of the mapping.

	/*

	 * Make sure the adjustment is done only for THP pages. Also make

	 * sure that the HVA and IPA are sufficiently aligned and that the

	 * block map is contained within the memslot.

		/*

		 * The address we faulted on is backed by a transparent huge

		 * page.  However, because we map the compound huge page and

		 * not the individual tail page, we need to transfer the

		 * refcount to the head page.  We have to be careful that the

		 * THP doesn't start to split while we are adjusting the

		 * refcounts.

		 *

		 * We are sure this doesn't happen, because mmu_notifier_retry

		 * was successful and we are holding the mmu_lock, so if this

		 * THP is trying to split, it will be blocked in the mmu

		 * notifier before touching any of the pages, specifically

		 * before being able to call __split_huge_page_refcount().

		 *

		 * We can therefore safely transfer the refcount from PG_tail

		 * to PG_head and switch the pfn from a tail page to the head

		 * page accordingly.

 Use page mapping if we cannot use block mapping. */

/*

 * The page will be mapped in stage 2 as Normal Cacheable, so the VM will be

 * able to see the page's tags and therefore they must be initialised first. If

 * PG_mte_tagged is set, tags have already been initialised.

 *

 * The race in the test/set of the PG_mte_tagged flag is handled by:

 * - preventing VM_SHARED mappings in a memslot with MTE preventing two VMs

 *   racing to santise the same page

 * - mmap_lock protects between a VM faulting a page in and the VMM performing

 *   an mprotect() to add VM_MTE

	/*

	 * pfn_to_online_page() is used to reject ZONE_DEVICE pages

	 * that may not support tags.

	/*

	 * Let's check if we will get back a huge page backed by hugetlbfs, or

	 * get block mapping for device MMIO region.

	/*

	 * logging_active is guaranteed to never be true for VM_PFNMAP

	 * memslots.

	/*

	 * Permission faults just need to update the existing leaf entry,

	 * and so normally don't require allocations from the memcache. The

	 * only exception to this is when dirty logging is enabled at runtime

	 * and a write fault needs to collapse a block entry into a table.

	/*

	 * Ensure the read of mmu_notifier_seq happens before we call

	 * gfn_to_pfn_prot (which calls get_user_pages), so that we don't risk

	 * the page we just got a reference to gets unmapped before we have a

	 * chance to grab the mmu_lock, which ensure that if the page gets

	 * unmapped afterwards, the call to kvm_unmap_gfn will take it away

	 * from us again properly. This smp_rmb() interacts with the smp_wmb()

	 * in kvm_mmu_notifier_invalidate_<page|range_end>.

	 *

	 * Besides, __gfn_to_pfn_memslot() instead of gfn_to_pfn_prot() is

	 * used to avoid unnecessary overhead introduced to locate the memory

	 * slot because it's always fixed even @gfn is adjusted for huge pages.

		/*

		 * If the page was identified as device early by looking at

		 * the VMA flags, vma_pagesize is already representing the

		 * largest quantity we can map.  If instead it was mapped

		 * via gfn_to_pfn_prot(), vma_pagesize is set to PAGE_SIZE

		 * and must not be upgraded.

		 *

		 * In both cases, we don't let transparent_hugepage_adjust()

		 * change things at the last minute.

		/*

		 * Only actually map the page as writable if this was a write

		 * fault.

	/*

	 * If we are not forced to use page mapping, check if we are

	 * backed by a THP and thus use block mapping if possible.

 Check the VMM hasn't introduced a new VM_SHARED VMA */

	/*

	 * Under the premise of getting a FSC_PERM fault, we just need to relax

	 * permissions only if vma_pagesize equals fault_granule. Otherwise,

	 * kvm_pgtable_stage2_map() should be called to change block size.

 Mark the page dirty only if the fault is handled successfully */

 Resolve the access fault by making the page young again. */

/**

 * kvm_handle_guest_abort - handles all 2nd stage aborts

 * @vcpu:	the VCPU pointer

 *

 * Any abort that gets to the host is almost guaranteed to be caused by a

 * missing second stage translation table entry, which can mean that either the

 * guest simply needs more memory and we must allocate an appropriate page or it

 * can mean that the guest tried to access I/O memory, which is emulated by user

 * space. The distinction is based on the IPA causing the fault and whether this

 * memory region has been registered as standard RAM by user space.

 Synchronous External Abort? */

		/*

		 * For RAS the host kernel may handle this abort.

		 * There is no need to pass the error into the guest.

 Check the stage-2 fault is trans. fault or write fault */

		/*

		 * The guest has put either its instructions or its page-tables

		 * somewhere it shouldn't have. Userspace won't be able to do

		 * anything about this (there's no syndrome for a start), so

		 * re-inject the abort back into the guest.

		/*

		 * Check for a cache maintenance operation. Since we

		 * ended-up here, we know it is outside of any memory

		 * slot. But we can't find out if that is for a device,

		 * or if the guest is just being stupid. The only thing

		 * we know for sure is that this range cannot be cached.

		 *

		 * So let's assume that the guest is just being

		 * cautious, and skip the instruction.

		/*

		 * The IPA is reported as [MAX:12], so we need to

		 * complement it with the bottom 12 bits from the

		 * faulting VA. This is always 12 bits, irrespective

		 * of the page size.

 Userspace should not be able to register out-of-bounds IPAs */

	/*

	 * We've moved a page around, probably through CoW, so let's treat

	 * it just like a translation fault and the map handler will clean

	 * the cache to the PoC.

	 *

	 * The MMU notifiers will have unmapped a huge PMD before calling

	 * ->change_pte() (which in turn calls kvm_set_spte_gfn()) and

	 * therefore we never need to clear out a huge PMD through this

	 * calling path and a memcache is not required.

	/*

	 * We rely on the linker script to ensure at build time that the HYP

	 * init code does not cross a page boundary.

		/*

		 * The idmap page is intersecting with the VA space,

		 * it is not safe to continue further.

	/*

	 * At this point memslot has been committed and there is an

	 * allocated dirty_bitmap[], dirty pages will be tracked while the

	 * memory slot is write protected.

		/*

		 * If we're with initial-all-set, we don't need to write

		 * protect any pages because they're all reported as dirty.

		 * Huge pages and normal pages will be write protect gradually.

	/*

	 * Prevent userspace from creating a memory region outside of the IPA

	 * space addressable by the KVM guest IPA space.

	/*

	 * A memory region could potentially cover multiple VMAs, and any holes

	 * between them, so iterate over all of them.

	 *

	 *     +--------------------------------------------+

	 * +---------------+----------------+   +----------------+

	 * |   : VMA 1     |      VMA 2     |   |    VMA 3  :    |

	 * +---------------+----------------+   +----------------+

	 *     |               memory region                |

	 *     +--------------------------------------------+

		/*

		 * VM_SHARED mappings are not allowed with MTE to avoid races

		 * when updating the PG_mte_tagged page flag, see

		 * sanitise_mte_tags for more details.

 IO region dirty page logging not allowed */

/*

 * See note at ARMv7 ARM B1.14.4 (TL;DR: S/W ops are not easily virtualized).

 *

 * Main problems:

 * - S/W ops are local to a CPU (not broadcast)

 * - We have line migration behind our back (speculation)

 * - System caches don't support S/W at all (damn!)

 *

 * In the face of the above, the best we can do is to try and convert

 * S/W ops to VA ops. Because the guest is not allowed to infer the

 * S/W to PA mapping, it can only use S/W to nuke the whole cache,

 * which is a rather good thing for us.

 *

 * Also, it is only used when turning caches on/off ("The expected

 * usage of the cache maintenance instructions that operate by set/way

 * is associated with the cache maintenance instructions associated

 * with the powerdown and powerup of caches, if this is required by

 * the implementation.").

 *

 * We use the following policy:

 *

 * - If we trap a S/W operation, we enable VM trapping to detect

 *   caches being turned on/off, and do a full clean.

 *

 * - We flush the caches on both caches being turned on and off.

 *

 * - Once the caches are enabled, we stop trapping VM ops.

	/*

	 * If this is the first time we do a S/W operation

	 * (i.e. HCR_TVM not set) flush the whole memory, and set the

	 * VM trapping.

	 *

	 * Otherwise, rely on the VM trapping to wait for the MMU +

	 * Caches to be turned off. At that point, we'll be able to

	 * clean the caches again.

	/*

	 * If switching the MMU+caches on, need to invalidate the caches.

	 * If switching it off, need to clean the caches.

	 * Clean + invalidate does the trick always.

 Caches are now on, stop trapping VM ops (until a S/W op) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Derived from arch/arm/kvm/reset.c

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

 Maximum phys_shift supported for any VM on this host */

/*

 * ARMv8 Reset Values

		/*

		 * The get_sve_reg()/set_sve_reg() ioctl interface will need

		 * to be extended with multiple register slice support in

		 * order to support vector lengths greater than

		 * SVE_VL_ARCH_MAX:

		/*

		 * Don't even try to make use of vector lengths that

		 * aren't available on all CPUs, for now:

	/*

	 * Userspace can still customize the vector lengths by writing

	 * KVM_REG_ARM64_SVE_VLS.  Allocation is deferred until

	 * kvm_arm_vcpu_finalize(), which freezes the configuration.

/*

 * Finalize vcpu's maximum SVE vector length, allocating

 * vcpu->arch.sve_state as necessary.

	/*

	 * Responsibility for these properties is shared between

	 * kvm_arm_init_arch_resources(), kvm_vcpu_enable_sve() and

	 * set_sve_vls().  Double-check here just to be sure:

	/*

	 * For now make sure that both address/generic pointer authentication

	 * features are requested by the userspace together and the system

	 * supports these capabilities.

 MTE is incompatible with AArch32 */

 Check that the vcpus are either all 32bit or all 64bit */

/**

 * kvm_reset_vcpu - sets core registers and sys_regs to reset value

 * @vcpu: The VCPU pointer

 *

 * This function finds the right table above and sets the registers on

 * the virtual CPU struct to their architecturally defined reset

 * values, except for registers whose reset is deferred until

 * kvm_arm_vcpu_finalize().

 *

 * Note: This function can be called from two paths: The KVM_ARM_VCPU_INIT

 * ioctl or as part of handling a request issued by another VCPU in the PSCI

 * handling code.  In the first case, the VCPU will not be loaded, and in the

 * second case the VCPU will be loaded.  Because this function operates purely

 * on the memory-backed values of system registers, we want to do a full put if

 * we were loaded (handling a request) and load the values back at the end of

 * the function.  Otherwise we leave the state alone.  In both cases, we

 * disable preemption around the vcpu reset as we would otherwise race with

 * preempt notifiers which also call put/load.

 Reset PMU outside of the non-preemptible section */

 Reset core registers */

 Reset system registers */

	/*

	 * Additional reset state handling that PSCI may have imposed on us.

	 * Must be done after all the sys_reg reset.

 Gracefully handle Thumb2 entry point */

 Propagate caller endianness */

 Reset timer */

	/*

	 * IPA size beyond 48 bits could not be supported

	 * on either 4K or 16K page size. Hence let's cap

	 * it to 48 bits, in case it's reported as larger

	 * on the system.

	/*

	 * Check with ARMv8.5-GTG that our PAGE_SIZE is supported at

	 * Stage-2. If not, things will stop very quickly.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Derived from arch/arm/kvm/coproc.c:

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Authors: Rusty Russell <rusty@rustcorp.com.au>

 *          Christoffer Dall <c.dall@virtualopensystems.com>

/*

 * All of this file is extremely similar to the ARM coproc.c, but the

 * types are different. My gut feeling is that it should be pretty

 * easy to merge, but that would be an ABI breakage -- again. VFP

 * would also need to be abstracted.

 *

 * For AArch32, we only take care of what is being trapped. Anything

 * that has to do with init and userspace access has to go via the

 * 64bit interface.

 3 bits per cache level, as per CLIDR, but non-existent caches always 0 */

 CSSELR values; used to index KVM_REG_ARM_DEMUX_ID_CCSIDR */

 Which cache CCSIDR represents depends on CSSELR value. */

 Make sure noone else changes CSSELR during this! */

/*

 * See note at ARMv7 ARM B1.14.4 (TL;DR: S/W ops are not easily virtualized).

	/*

	 * Only track S/W ops if we don't have FWB. It still indicates

	 * that the guest is a bit broken (S/W operations should only

	 * be done by firmware, knowing that there is only a single

	 * CPU left in the system, and certainly not from non-secure

	 * software).

/*

 * Generic accessor for VM registers. Only called as long as HCR_TVM

 * is set. If the guest enables the MMU, we stop trapping the VM

 * sys_regs and leave it in complete control of the caches.

/*

 * Trap handler for the GICv3 SGI generation system register.

 * Forward the request to the VGIC emulation.

 * The cp15_64 code makes sure this automatically works

 * for both AArch64 and AArch32 accesses.

	/*

	 * In a system where GICD_CTLR.DS=1, a ICC_SGI0R_EL1 access generates

	 * Group0 SGIs only, while ICC_SGI1R_EL1 can generate either group,

	 * depending on the SGI configuration. ICC_ASGI1R_EL1 is effectively

	 * equivalent to ICC_SGI0R_EL1, as there is no "alternative" secure

	 * group.

 AArch32 */

 Keep GCC quiet */

 ICC_SGI1R */

 ICC_ASGI1R */

 ICC_SGI0R */

 AArch64 */

 Keep GCC quiet */

 ICC_SGI1R_EL1 */

 ICC_ASGI1R_EL1 */

 ICC_SGI0R_EL1 */

/*

 * ARMv8.1 mandates at least a trivial LORegion implementation, where all the

 * RW registers are RES0 (which we can implement as RAZ/WI). On an ARMv8.0

 * system, these registers should UNDEF. LORID_EL1 being a RO register, we

 * treat it separately.

/*

 * We want to avoid world-switching all the DBG registers all the

 * time:

 *

 * - If we've touched any debug register, it is likely that we're

 *   going to touch more of them. It then makes sense to disable the

 *   traps and start doing the save/restore dance

 * - If debug is active (DBG_MDSCR_KDE or DBG_MDSCR_MDE set), it is

 *   then mandatory to save/restore the registers, as the guest

 *   depends on them.

 *

 * For this, we use a DIRTY bit, indicating the guest has modified the

 * debug registers, used as follow:

 *

 * On guest entry:

 * - If the dirty bit is set (because we're coming back from trapping),

 *   disable the traps, save host registers, restore guest registers.

 * - If debug is actively in use (DBG_MDSCR_KDE or DBG_MDSCR_MDE set),

 *   set the dirty bit, disable the traps, save host registers,

 *   restore guest registers.

 * - Otherwise, enable the traps

 *

 * On guest exit:

 * - If the dirty bit is set, save guest registers, restore host

 *   registers and clear the dirty bit. This ensure that the host can

 *   now use the debug registers.

/*

 * reg_to_dbg/dbg_to_reg

 *

 * A 32 bit write to a debug register leave top bits alone

 * A 32 bit read from a debug register only returns the bottom bits

 *

 * All writes will set the KVM_ARM64_DEBUG_DIRTY flag to ensure the

 * hyp.S code switches between host and guest values in future.

	/*

	 * Map the vcpu_id into the first three affinity level fields of

	 * the MPIDR. We limit the number of VCPUs in level 0 due to a

	 * limitation to 16 CPUs in that level in the ICC_SGIxR registers

	 * of the GICv3 to be able to address each CPU directly when

	 * sending IPIs.

 No PMU available, any PMU reg may UNDEF... */

 No PMU available, PMCR_EL0 may UNDEF... */

	/*

	 * Writable bits of PMCR_EL0 (ARMV8_PMU_PMCR_MASK) are reset to UNKNOWN

	 * except PMCR.E resetting to zero.

 Only update writeable bits of PMCR */

 PMCR.P & PMCR.C are RAZ */

 return PMSELR.SEL field */

 PMXEVCNTR_EL0 */

 PMCCNTR_EL0 */

 PMCCNTR */

 PMEVCNTRn_EL0 */

 Catch any decoding mistake */

 PMXEVTYPER_EL0 */

 PMEVTYPERn_EL0 */

 accessing PMCNTENSET_EL0 */

 accessing PMCNTENCLR_EL0 */

 accessing PMINTENSET_EL1 */

 accessing PMINTENCLR_EL1 */

 accessing PMOVSSET_EL0 */

 accessing PMOVSCLR_EL0 */

 Silly macro to expand the DBG{BCR,BVR,WVR,WCR}n_EL1 registers in one go */

 Macro to expand the PMEVCNTRn_EL0 register */

 Macro to expand the PMEVTYPERn_EL0 register */

 Macro to expand the AMU counter and type registers*/

/*

 * If we land here on a PtrAuth access, that is because we didn't

 * fixup the access on exit by allowing the PtrAuth sysregs. The only

 * way this happens is when the guest does not have PtrAuth support

 * enabled.

 Read a sanitised cpufeature ID register by sys_reg_desc */

 Limit debug to ARMv8.0 */

 Limit guests to PMUv3 for ARMv8.4 */

 Hide SPE from guests */

 Limit guests to PMUv3 for ARMv8.4 */

 cpufeature ID register access trap handlers */

 Visibility overrides for SVE-specific control registers */

	/*

	 * Allow AA64PFR0_EL1.CSV2 to be set from userspace as long as

	 * it doesn't promise more than what is actually provided (the

	 * guest could otherwise be covered in ectoplasmic residue).

 Same thing for CSV3 */

 We can only differ with CSV[23], and anything else is an error */

/*

 * cpufeature ID register user accessors

 *

 * For now, these registers are immutable for userspace, so no values

 * are stored, and for set_id_reg() we don't allow the effective value

 * to be changed.

 This is what we mean by invariant: you can't change it. */

 Perform the access even if we are going to ignore the value */

	/*

	 * Guests should not be doing cache operations by set/way at all, and

	 * for this reason, we trap them and attempt to infer the intent, so

	 * that we can flush the entire guest's address space at the appropriate

	 * time.

	 * To prevent this trapping from causing performance problems, let's

	 * expose the geometry of all data and unified caches (which are

	 * guaranteed to be PIPT and thus non-aliasing) as 1 set and 1 way.

	 * [If guests should attempt to infer aliasing properties from the

	 * geometry (which is not permitted by the architecture), they would

	 * only do so for virtually indexed caches.]

 data or unified cache

 sys_reg_desc initialiser for known cpufeature ID registers */

/*

 * sys_reg_desc initialiser for architecturally unallocated cpufeature ID

 * register with encoding Op0=3, Op1=0, CRn=0, CRm=crm, Op2=op2

 * (1 <= crm < 8, 0 <= Op2 < 8).

/*

 * sys_reg_desc initialiser for known ID registers that we hide from guests.

 * For now, these are exposed just like unallocated ID regs: they appear

 * RAZ for the guest.

/*

 * Architected system registers.

 * Important: Must be sorted ascending by Op0, Op1, CRn, CRm, Op2

 *

 * Debug handling: We do trap most, if not all debug related system

 * registers. The implementation is good enough to ensure that a guest

 * can use these with minimal performance degradation. The drawback is

 * that we don't implement any of the external debug, none of the

 * OSlock protocol. This should be revisited if we ever encounter a

 * more demanding guest...

 DBGDTR[TR]X_EL0 share the same encoding

	/*

	 * ID regs: all ID_SANITISED() entries here must have corresponding

	 * entries in arm64_ftr_regs[].

 AArch64 mappings of the AArch32 ID registers */

 CRm=1 */

 CRm=2 */

 CRm=3 */

 AArch64 ID registers */

 CRm=4 */

 CRm=5 */

 CRm=6 */

 CRm=7 */

 PMBIDR_EL1 is not trapped */

	/*

	 * PM_SWINC_EL0 is exposed to userspace as RAZ/WI, as it was

	 * previously (and pointlessly) advertised in the past...

	/*

	 * PMUSERENR_EL0 resets as unknown in 64bit mode while it resets as zero

	 * in 32bit mode. Here we choose to reset it as zero for consistency.

 PMEVCNTRn_EL0 */

 PMEVTYPERn_EL0 */

	/*

	 * PMCCFILTR_EL0 resets as unknown in 64bit mode while it resets as zero

	 * in 32bit mode. Here we choose to reset it as zero for consistency.

/*

 * AArch32 debug register mappings

 *

 * AArch32 DBGBVRn is mapped to DBGBVRn_EL1[31:0]

 * AArch32 DBGBXVRn is mapped to DBGBVRn_EL1[63:32]

 *

 * None of the other registers share their location, so treat them as

 * if they were 64bit.

 DBGBVRn */							      \

 DBGBCRn */							      \

 DBGWVRn */							      \

 DBGWCRn */							      \

/*

 * Trapped cp14 registers. We generally ignore most of the external

 * debug, on the principle that they don't really make sense to a

 * guest. Revisit this one day, would this principle change.

 DBGDIDR */

 DBGDTRRXext */

 DBGDSCRint */

 DBGDCCINT */

 DBGDSCRext */

 DBGDTR[RT]Xint */

 DBGDTR[RT]Xext */

 DBGWFAR */

 DBGOSECCR */

 DBGVCR */

 DBGDRAR (32bit) */

 DBGOSLAR */

 DBGOSLSR */

 DBGOSDLR */

 DBGPRCR */

 DBGDSAR (32bit) */

 DBGDEVID2 */

 DBGDEVID1 */

 DBGDEVID */

 DBGCLAIMSET */

 DBGCLAIMCLR */

 DBGAUTHSTATUS */

 Trapped cp14 64bit registers */

 DBGDRAR (64bit) */

 DBGDSAR (64bit) */

 Macro to expand the PMEVCNTRn register */

 PMEVCNTRn */							\

 Macro to expand the PMEVTYPERn register */

 PMEVTYPERn */						\

/*

 * Trapped cp15 registers. TTBR0/TTBR1 get a double encoding,

 * depending on the way they are accessed (as a 32bit or a 64bit

 * register).

 ACTLR */

 ACTLR2 */

 TTBCR */

 TTBCR2 */

 DFSR */

 ADFSR */

 AIFSR */

 DFAR */

 IFAR */

	/*

	 * DC{C,I,CI}SW operations:

 PMU */

 PMMIR */

 PRRR/MAIR0 */

 NMRR/MAIR1 */

 AMAIR0 */

 AMAIR1 */

 ICC_SRE */

 Arch Tmers */

 PMEVCNTRn */

 PMEVTYPERn */

 PMCCFILTR */

 ICC_SGI1R */

 ICC_ASGI1R */

 ICC_SGI0R */

 Check for regs disabled by runtime config */

	/*

	 * Not having an accessor means that we have configured a trap

	 * that we don't know how to handle. This certainly qualifies

	 * as a gross bug that should be fixed right away.

 Skip instruction if instructed so */

/*

 * emulate_cp --  tries to match a sys_reg access in a handling table, and

 *                call the corresponding trap handler.

 *

 * @params: pointer to the descriptor of the access

 * @table: array of trap descriptors

 * @num: size of the trap descriptor array

 *

 * Return 0 if the access has been handled, and -1 if not.

 Not handled */

 Not handled */

/**

 * kvm_handle_cp_64 -- handles a mrrc/mcrr trap on a guest CP14/CP15 access

 * @vcpu: The VCPU pointer

 * @run:  The kvm_run struct

	/*

	 * Make a 64-bit value out of Rt and Rt2. As we use the same trap

	 * backends between AArch32 and AArch64, we get away with it.

	/*

	 * If the table contains a handler, handle the

	 * potential register operation in the case of a read and return

	 * with success.

 Split up the value between registers for the read side */

/**

 * kvm_handle_cp_32 -- handles a mrc/mcr trap on a guest CP14/CP15 access

 * @vcpu: The VCPU pointer

 * @run:  The kvm_run struct

 See ARM DDI 0487E.a, section D12.3.2

/**

 * kvm_reset_sys_regs - sets system registers to reset value

 * @vcpu: The VCPU pointer

 *

 * This function finds the right table above and sets the registers on the

 * virtual CPU struct to their architecturally defined reset values.

/**

 * kvm_handle_sys_reg -- handles a mrs/msr trap on a guest sys_reg access

 * @vcpu: The VCPU pointer

/******************************************************************************

 * Userspace API

 Any unused index bits means it's not valid. */

 Decode an index value, and find the sys_reg_desc entry. */

 We only do sys_reg for now. */

 Not saved in the sys_reg array and not otherwise accessible? */

/*

 * These are the invariant sys_reg registers: we let the guest see the

 * host versions of these, so they're part of the guest state.

 *

 * A future CPU may provide a mechanism to present different values to

 * the guest, or a future kvm may trap them.

 ->val is filled in by kvm_sys_reg_table_init() */

 Make sure high bits are 0 for 32-bit regs */

 This is what we mean by invariant: you can't change it. */

 Bottom bit is Instruction or Data bit.  Next 3 bits are level. */

 No cache */

 Instruction cache only */

 Data cache only */

 Unified cache */

 Separate instruction and data caches */

 Reserved: we can't know instruction or data. */

 Fail if we have unknown bits set. */

 Fail if we have unknown bits set. */

 This is also invariant: you can't change it. */

 Check for regs disabled by runtime config */

 Check for regs disabled by runtime config */

	/*

	 * Ignore registers we trap but don't save,

	 * and for which no custom user accessor is provided.

 Assumed ordered tables, see kvm_sys_reg_table_init. */

 Then give them all the invariant registers' indices. */

 Make sure tables are unique and in order. */

 We abuse the reset function to overwrite the table itself. */

	/*

	 * CLIDR format is awkward, so clean it up.  See ARM B4.1.20:

	 *

	 *   If software reads the Cache Type fields from Ctype1

	 *   upwards, once it has seen a value of 0b000, no caches

	 *   exist at further-out levels of the hierarchy. So, for

	 *   example, if Ctype3 is the first Cache Type field with a

	 *   value of 0b000, the values of Ctype4 to Ctype7 must be

	 *   ignored.

 Ugly... */

 Clear all higher bits. */

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2019 Arm Ltd.

	/*

	 * system time and counter value must captured at the same

	 * time to keep consistency and precision.

	/*

	 * This is only valid if the current clocksource is the

	 * architected counter, as this is the only one the guest

	 * can see.

	/*

	 * The guest selects one of the two reference counters

	 * (virtual or physical) with the first argument of the SMCCC

	 * call. In case the identifier is not supported, error out.

	/*

	 * This relies on the top bit of val[0] never being set for

	 * valid values of system time, because that is *really* far

	 * in the future (about 292 years from 1970, and at that stage

	 * nobody will give a damn about it).

				/*

				 * SSBS everywhere: Indicate no firmware

				 * support, as the SSBS support will be

				 * indicated to the guest and the default is

				 * safe.

				 *

				 * Otherwise, expose a permanent mitigation

				 * to the guest, and hide SSBS so that the

				 * guest stays protected.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VGICv3 MMIO handling functions

 extract @num bytes at @offset bytes offset in data */

 allows updates of any half of a 64-bit register (or the whole thing) */

/*

 * The Revision field in the IIDR have the following meanings:

 *

 * Revision 2: Interrupt groups are guest-configurable and signaled using

 * 	       their configured groups.

 Not a GICv4.1? No HW SGIs */

 Dist stays enabled? nASSGIreq is RO */

 Switching HW SGIs? */

 This is at best for documentation purposes... */

 Not a GICv4.1? No HW SGIs */

 The upper word is RAZ for us. */

 The upper word is WI for us since we don't implement Aff3. */

 We only care about and preserve Aff0, Aff1 and Aff2. */

		/*

		 * the rdist is the last one of the redist region,

		 * check whether there is no other contiguous rdist region

 report a GICv3 compliant implementation */

	/*

	 * pending state of interrupt is latched in pending_latch variable.

	 * Userspace will save and restore pending state and line_level

	 * separately.

	 * Refer to Documentation/virt/kvm/devices/arm-vgic-v3.rst

	 * for handling of ISPENDR and ICPENDR.

			/*

			 * pending_latch is set irrespective of irq type

			 * (level or edge) to avoid dependency that VM should

			 * restore irq config before pending info.

 We want to avoid outer shareable. */

 Avoid any inner non-cacheable mapping. */

 Non-cacheable or same-as-inner are OK. */

 Storing a value with LPIs already enabled is undefined */

 Storing a value with LPIs already enabled is undefined */

/*

 * The GICv3 per-IRQ registers are split to control PPIs and SGIs in the

 * redistributors, while SPIs are covered by registers in the distributor

 * block. Trying to set private IRQs in this block gets ignored.

 * We take some special care here to fix the calculation of the register

 * offset.

 RD_base registers */

 SGI_base registers */

/**

 * vgic_register_redist_iodev - register a single redist iodev

 * @vcpu:    The VCPU to which the redistributor belongs

 *

 * Register a KVM iodev for this VCPU's redistributor using the address

 * provided.

 *

 * Return 0 on success, -ERRNO otherwise.

	/*

	 * We may be creating VCPUs before having set the base address for the

	 * redistributor region, in which case we will come back to this

	 * function for all VCPUs when the base address is set.  Just return

	 * without doing any work for now.

 The current c failed, so we start with the previous one. */

/**

 * vgic_v3_alloc_redist_region - Allocate a new redistributor region

 *

 * Performs various checks before inserting the rdist region in the list.

 * Those tests depend on whether the size of the rdist region is known

 * (ie. count != 0). The list is sorted by rdist region index.

 *

 * @kvm: kvm handle

 * @index: redist region index

 * @base: base of the new rdist region

 * @count: number of redistributors the region is made of (0 in the old style

 * single region, whose size is induced from the number of vcpus)

 *

 * Return 0 on success, < 0 otherwise

 cross the end of memory ? */

 Don't mix single region and discrete redist regions */

	/*

	 * For legacy single-region redistributor regions (!count),

	 * check that the redistributor region does not overlap with the

	 * distributor's address space.

 collision with any other rdist region? */

	/*

	 * Register iodevs for each existing VCPU.  Adding more VCPUs

	 * afterwards will register the iodevs when needed.

 We only support aligned 32-bit accesses. */

/*

 * Compare a given affinity (level 1-3 and a level 0 mask, from the SGI

 * generation register ICC_SGI1R_EL1) with a given VCPU.

 * If the VCPU's MPIDR matches, return the level0 affinity, otherwise

 * return -1.

	/*

	 * Split the current VCPU's MPIDR into affinity level 0 and the

	 * rest as this is what we have to compare against.

 bail out if the upper three levels don't match */

 Is this VCPU's bit set in the mask ? */

/*

 * The ICC_SGI* registers encode the affinity differently from the MPIDR,

 * so provide a wrapper to use the existing defines to isolate a certain

 * affinity level.

/**

 * vgic_v3_dispatch_sgi - handle SGI requests from VCPUs

 * @vcpu: The VCPU requesting a SGI

 * @reg: The value written into ICC_{ASGI1,SGI0,SGI1}R by that VCPU

 * @allow_group1: Does the sysreg access allow generation of G1 SGIs

 *

 * With GICv3 (and ARE=1) CPUs trigger SGIs by writing to a system register.

 * This will trap in sys_regs.c and call this function.

 * This ICC_SGI1R_EL1 register contains the upper three affinity levels of the

 * target processors as well as a bitmask of 16 Aff0 CPUs.

 * If the interrupt routing mode bit is not set, we iterate over all VCPUs to

 * check for matching ones. If this bit is set, we signal all, but not the

 * calling VCPU.

	/*

	 * We iterate over all VCPUs to find the MPIDRs matching the request.

	 * If we have handled one CPU, we clear its bit to detect early

	 * if we are already finished. This avoids iterating through all

	 * VCPUs when most of the times we just signal a single VCPU.

 Exit early if we have dealt with all requested CPUs */

 Don't signal the calling VCPU */

 remove this matching VCPU from the mask */

		/*

		 * An access targeting Group0 SGIs can only generate

		 * those, while an access targeting Group1 SGIs can

		 * generate interrupts of either group.

 HW SGI? Ask the GIC to inject it */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VGIC MMIO handling functions

 Ignore */

 Ignore */

 Loop over all IRQs affected by this read */

/*

 * Read accesses to both GICD_ICENABLER and GICD_ISENABLER return the value

 * of the enabled bit, so there is only one function for both here.

 Loop over all IRQs affected by this read */

			/*

			 * We need to update the state of the interrupt because

			 * the guest might have changed the state of the device

			 * while the interrupt was disabled at the VGIC level.

			/*

			 * Deactivate the physical interrupt so the GIC will let

			 * us know when it is asserted again.

 Loop over all IRQs affected by this read */

 GICD_ISPENDR0 SGI bits are WI */

 HW SGI? Ask the GIC to inject it */

		/*

		 * GICv2 SGIs are terribly broken. We can't restore

		 * the source of the interrupt, so just pick the vcpu

		 * itself as the source...

 Must be called with irq->irq_lock held */

	/*

	 * We don't want the guest to effectively mask the physical

	 * interrupt by doing a write to SPENDR followed by a write to

	 * CPENDR for HW interrupts, so we clear the active state on

	 * the physical side if the virtual interrupt is not active.

	 * This may lead to taking an additional interrupt on the

	 * host, but that should not be a problem as the worst that

	 * can happen is an additional vgic injection.  We also clear

	 * the pending state to maintain proper semantics for edge HW

	 * interrupts.

 GICD_ICPENDR0 SGI bits are WI */

 HW SGI? Ask the GIC to clear its pending bit */

		/*

		 * More fun with GICv2 SGIs! If we're clearing one of them

		 * from userspace, which source vcpu to clear? Let's not

		 * even think of it, and blow the whole set.

/*

 * If we are fiddling with an IRQ's active state, we have to make sure the IRQ

 * is not queued on some running VCPU's LRs, because then the change to the

 * active state can be overwritten when the VCPU's state is synced coming back

 * from the guest.

 *

 * For shared interrupts as well as GICv3 private interrupts, we have to

 * stop all the VCPUs because interrupts can be migrated while we don't hold

 * the IRQ locks and we don't want to be chasing moving targets.

 *

 * For GICv2 private interrupts we don't have to do anything because

 * userspace accesses to the VGIC state already require all VCPUs to be

 * stopped, and only the VCPU itself can modify its private interrupts

 * active state, which guarantees that the VCPU is not running.

 See vgic_access_active_prepare */

 Loop over all IRQs affected by this read */

		/*

		 * Even for HW interrupts, don't evaluate the HW state as

		 * all the guest is interested in is the virtual state.

 Must be called with irq->irq_lock held */

		/*

		 * GICv4.1 VSGI feature doesn't track an active state,

		 * so let's not kid ourselves, there is nothing we can

		 * do here.

		/*

		 * The GICv2 architecture indicates that the source CPUID for

		 * an SGI should be provided during an EOI which implies that

		 * the active state is stored somewhere, but at the same time

		 * this state is not architecturally exposed anywhere and we

		 * have no way of knowing the right source.

		 *

		 * This may lead to a VCPU not being able to receive

		 * additional instances of a particular SGI after migration

		 * for a GICv2 VM on some GIC implementations.  Oh well.

/*

 * We currently don't handle changing the priority of an interrupt that

 * is already pending on a VCPU. If there is a need for this, we would

 * need to make this VCPU exit and re-evaluate the priorities, potentially

 * leading to this interrupt getting presented now to the guest (if it has

 * been masked by the priority mask before).

 Narrow the priority range to what we actually support */

		/*

		 * The configuration cannot be changed for SGIs in general,

		 * for PPIs this is IMPLEMENTATION DEFINED. The arch timer

		 * code relies on PPIs being level triggered, so we also

		 * make them read-only here.

		/*

		 * Line level is set irrespective of irq type

		 * (level or edge) to avoid dependency that VM should

		 * restore irq config before line level.

/*

 * kvm_mmio_read_buf() returns a value in a format where it can be converted

 * to a byte array and be directly observed as the guest wanted it to appear

 * in memory if it had done the store itself, which is LE for the GIC, as the

 * guest knows the GIC is always LE.

 *

 * We convert this value to the CPUs native format to deal with it as a data

 * value.

/*

 * kvm_mmio_write_buf() expects a value in a format such that if converted to

 * a byte array it is observed as the guest would see it if it could perform

 * the load directly.  Since the GIC is LE, and the guest knows this, the

 * guest expects a value in little endian format.

 *

 * We convert the data value from the CPUs native format to LE so that the

 * value is returned in the proper format.

 Do we access a non-allocated IRQ? */

/*

 * Userland access to VGIC registers.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Linaro

 * Author: Christoffer Dall <christoffer.dall@linaro.org>

/*

 * Structure to control looping through the entire vgic state.  We start at

 * zero for each field and move upwards.  So, if dist_id is 0 we print the

 * distributor info.  When dist_id is 1, we have already printed it and move

 * on.

 *

 * When vcpu_id < nr_cpus we print the vcpu info until vcpu_id == nr_cpus and

 * so on.

 Fast forward to the right position if needed */

	/*

	 * If the seq file wasn't properly opened, there's nothing to clearn

	 * up.

 SPDX-License-Identifier: GPL-2.0-only

 Notify fds when the guest EOI'ed a level-triggered IRQ */

 An LPI could have been unmapped. */

 Always preserve the active bit, note deactivation */

 Edge is the only case where we preserve the pending bit */

		/*

		 * Clear soft pending state when level irqs have been acked.

 Handle resampling for mapped interrupts if required */

 Requires the irq to be locked already */

		/*

		 * Never set pending+active on a HW interrupt, as the

		 * pending state is kept at the physical distributor

		 * level.

			/*

			 * Software resampling doesn't work very well

			 * if we allow P+A, so let's not do that.

	/*

	 * Level-triggered mapped IRQs are special because we only observe

	 * rising edges as input to the VGIC.  We therefore lower the line

	 * level here, so that we can take new virtual IRQs.  See

	 * vgic_v3_fold_lr_state for more info.

		/*

		 * When emulating GICv3 on GICv3 with SRE=1 on the

		 * VFIQEn bit is RES1 and the VAckCtl bit is RES0.

		/*

		 * When emulating GICv3 on GICv3 with SRE=1 on the

		 * VFIQEn bit is RES1 and the VAckCtl bit is RES0.

	/*

	 * By forcing VMCR to zero, the GIC will restore the binary

	 * points to their reset values. Anything else resets to zero

	 * anyway.

	/*

	 * If we are emulating a GICv3, we do it in an non-GICv2-compatible

	 * way, so we force SRE to 1 to demonstrate this to the guest.

	 * Also, we don't support any form of IRQ/FIQ bypass.

	 * This goes with the spec allowing the value to be RAO/WI.

 Get the show on the road... */

 clear consumed data */

/*

 * The deactivation of the doorbell interrupt will trigger the

 * unmapping of the associated vPE.

/**

 * vgic_v3_save_pending_tables - Save the pending tables into guest RAM

 * kvm lock and all vcpu lock must be held

	/*

	 * A preparation for getting any VLPI states.

	 * The above vgic initialized check also ensures that the allocation

	 * and enabling of the doorbells have already been done.

/**

 * vgic_v3_rdist_overlap - check if a region overlaps with any

 * existing redistributor region

 *

 * @kvm: kvm handle

 * @base: base of the region

 * @size: size of region

 *

 * Return: true if there is an overlap

/*

 * Check for overlapping regions and for regions crossing the end of memory

 * for base addresses which have already been set.

/**

 * vgic_v3_rdist_free_slot - Look up registered rdist regions and identify one

 * which has free space to put a new rdist region.

 *

 * @rd_regions: redistributor region list head

 *

 * A redistributor regions maps n redistributors, n = region size / (2 x 64kB).

 * Stride between redistributors is 0 and regions are filled in the index order.

 *

 * Return: the redist region handle, if any, that has space to map a new rdist

 * region.

	/*

	 * For a VGICv3 we require the userland to explicitly initialize

	 * the VGIC before we need to use it.

/**

 * vgic_v3_probe - probe for a VGICv3 compatible interrupt controller

 * @info:	pointer to the GIC description

 *

 * Returns 0 if the VGICv3 has been probed successfully, returns an error code

 * otherwise

	/*

	 * The ListRegs field is 5 bits, but there is an architectural

	 * maximum of 16 list registers. Just ignore bit 4...

 GICv4 support? */

	/*

	 * If dealing with a GICv2 emulation on GICv3, VMCR_EL2.VFIQen

	 * is dependent on ICC_SRE_EL1.SRE, and we have to perform the

	 * VMCR_EL2 save/restore in the world switch.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * GICv3 ITS emulation

 *

 * Copyright (C) 2015,2016 ARM Ltd.

 * Author: Andre Przywara <andre.przywara@arm.com>

/*

 * Creates a new (reference to a) struct vgic_irq for a given LPI.

 * If this LPI is already mapped on another ITS, we increase its refcount

 * and return a pointer to the existing structure.

 * If this is a "new" LPI, we allocate and initialize a new struct vgic_irq.

 * This function returns a pointer to the _unlocked_ structure.

 In this case there is no put, since we keep the reference. */

	/*

	 * There could be a race with another vgic_add_lpi(), so we need to

	 * check that we don't add a second list entry with the same LPI.

 Someone was faster with adding this LPI, lets use that. */

		/*

		 * This increases the refcount, the caller is expected to

		 * call vgic_put_irq() on the returned pointer once it's

		 * finished with the IRQ.

	/*

	 * We "cache" the configuration table entries in our struct vgic_irq's.

	 * However we only have those structs for mapped IRQs, so we read in

	 * the respective config data from memory here upon mapping the LPI.

	 *

	 * Should any of these fail, behave as if we couldn't create the LPI

	 * by dropping the refcount and returning the error.

 the head for the list of ITTEs */

/**

 * struct vgic_its_abi - ITS abi ops and settings

 * @cte_esz: collection table entry size

 * @dte_esz: device table entry size

 * @ite_esz: interrupt translation table entry size

 * @save tables: save the ITS tables into guest RAM

 * @restore_tables: restore the ITS internal structs from tables

 *  stored in guest RAM

 * @commit: initialize the registers which expose the ABI settings,

 *  especially the entry sizes

/*

 * Find and returns a device in the device table for an ITS.

 * Must be called with the its_lock mutex held.

/*

 * Find and returns an interrupt translation table entry (ITTE) for a given

 * Device ID/Event ID pair on an ITS.

 * Must be called with the its_lock mutex held.

 To be used as an iterator this macro misses the enclosing parentheses */

/*

 * Finds and returns a collection in the ITS collection table.

 * Must be called with the its_lock mutex held.

/*

 * Reads the configuration data for a given LPI from guest memory and

 * updates the fields in struct vgic_irq.

 * If filter_vcpu is not NULL, applies only if the IRQ is targeting this

 * VCPU. Unconditionally applies if filter_vcpu is NULL.

/*

 * Create a snapshot of the current LPIs targeting @vcpu, so that we can

 * enumerate those LPIs without holding any lock.

 * Returns their number and puts the kmalloc'ed array into intid_ptr.

	/*

	 * There is an obvious race between allocating the array and LPIs

	 * being mapped/unmapped. If we ended up here as a result of a

	 * command, we're safe (locks are held, preventing another

	 * command). If coming from another path (such as enabling LPIs),

	 * we must be careful not to overrun the array.

 We don't need to "get" the IRQ, as we hold the list lock. */

/*

 * Promotes the ITS view of affinity of an ITTE (which redistributor this LPI

 * is targeting) to the VGIC's view, which deals with target VCPUs.

 * Needs to be called whenever either the collection for a LPIs has

 * changed or the collection itself got retargeted.

/*

 * Updates the target VCPU for every LPI targeting this collection.

 * Must be called with the its_lock mutex held.

/*

 * Sync the pending table pending bit of LPIs targeting @vcpu

 * with our own data structures. This relies on the LPI being

 * mapped before.

		/*

		 * For contiguously allocated LPIs chances are we just read

		 * this very same byte in the last iteration. Reuse that.

	/*

	 * We use linear CPU numbers for redistributor addressing,

	 * so GITS_TYPER.PTA is 0.

	 * Also we force all PROPBASER registers to be the same, so

	 * CommonLPIAff is 0 as well.

	 * To avoid memory waste in the guest, we keep the number of IDBits and

	 * DevBits low - as least for the time being.

 part number, bits[7:0] */

 part number, bits[11:8] */

 This is a 64K software visible page */

 The following are the ID registers for (any) GIC. */

		/*

		 * If we hit a NULL entry, there is nothing after this

		 * point.

		/*

		 * Move this entry to the head, as it is the most

		 * recently used.

 Do not cache a directly injected interrupt */

	/*

	 * We could have raced with another CPU caching the same

	 * translation behind our back, so let's check it is not in

	 * already

 Always reuse the last entry (LRU policy) */

	/*

	 * Caching the translation implies having an extra reference

	 * to the interrupt, so drop the potential reference on what

	 * was in the cache, and increment it on the new interrupt.

 Move the new translation to the head of the list */

		/*

		 * If we hit a NULL entry, there is nothing after this

		 * point.

/*

 * Find the target VCPU and the LPI number for a given devid/eventid pair

 * and make this IRQ pending, possibly injecting it.

 * Must be called with the its_lock mutex held.

 * Returns 0 on success, a positive error value for any ITS mapping

 * related errors and negative error values for generic errors.

/*

 * Queries the KVM IO bus framework to get the ITS pointer from the given

 * doorbell address.

 * We then call vgic_its_trigger_msi() with the decoded data.

 * According to the KVM_SIGNAL_MSI API description returns 1 on success.

	/*

	 * KVM_SIGNAL_MSI demands a return value > 0 for success and 0

	 * if the guest has blocked the MSI. So we map any LPI mapping

	 * related error to that.

 Requires the its_lock to be held. */

 This put matches the get in vgic_add_lpi. */

/*

 * The DISCARD command frees an Interrupt Translation Table Entry (ITTE).

 * Must be called with the its_lock mutex held.

		/*

		 * Though the spec talks about removing the pending state, we

		 * don't bother here since we clear the ITTE anyway and the

		 * pending state is a property of the ITTE struct.

/*

 * The MOVI command moves an ITTE to a different collection.

 * Must be called with the its_lock mutex held.

/*

 * Check whether an ID can be stored into the corresponding guest table.

 * For a direct table this is pretty easy, but gets a bit nasty for

 * indirect tables. We check whether the resulting guest physical address

 * is actually valid (covered by a memslot and guest accessible).

 * For this we have to read the respective first level entry.

 as GITS_TYPER.CIL == 0, ITS supports 16-bit collection ID */

 calculate and check the index into the 1st level */

 Each 1st level entry is represented by a 64-bit value. */

 check the valid bit of the first level entry */

 Mask the guest physical address and calculate the frame number. */

 Find the address of the actual entry */

	/*

	 * Clearing the mapping for that collection ID removes the

	 * entry from the list. If there wasn't any before, we can

	 * go home early.

 Must be called with its_lock mutex held */

/*

 * The MAPTI and MAPI commands map LPIs to ITTEs.

 * Must be called with its_lock mutex held.

 If there is an existing mapping, behavior is UNPREDICTABLE. */

 Requires the its_lock to be held. */

	/*

	 * The spec says that unmapping a device with still valid

	 * ITTEs associated is UNPREDICTABLE. We remove all ITTEs,

	 * since we cannot leave the memory unreferenced.

 its lock must be held */

 its lock must be held */

 Must be called with its_lock mutex held */

/*

 * MAPD maps or unmaps a device ID to Interrupt Translation Tables (ITTs).

 * Must be called with the its_lock mutex held.

	/*

	 * The spec says that calling MAPD on an already mapped device

	 * invalidates all cached data for this device. We implement this

	 * by removing the mapping and re-establishing it.

	/*

	 * The spec does not say whether unmapping a not-mapped device

	 * is an error, so we are done in any case.

/*

 * The MAPC command maps collection IDs to redistributors.

 * Must be called with the its_lock mutex held.

/*

 * The CLEAR command removes the pending state for a particular LPI.

 * Must be called with the its_lock mutex held.

/*

 * The INV command syncs the configuration bits from the memory table.

 * Must be called with the its_lock mutex held.

/*

 * The INVALL command requests flushing of all IRQ data in this collection.

 * Find the VCPU mapped to that collection, then iterate over the VM's list

 * of mapped LPIs and update the configuration for each IRQ which targets

 * the specified vcpu. The configuration will be read from the in-memory

 * configuration table.

 * Must be called with the its_lock mutex held.

/*

 * The MOVALL command moves the pending state of all IRQs targeting one

 * redistributor to another. We don't hold the pending state in the VCPUs,

 * but in the IRQs instead, so there is really not much to do for us here.

 * However the spec says that no IRQ must target the old redistributor

 * afterwards, so we make sure that no LPI is using the associated target_vcpu.

 * This command affects all LPIs in the system that target that redistributor.

/*

 * The INT command injects the LPI associated with that DevID/EvID pair.

 * Must be called with the its_lock mutex held.

/*

 * This function is called with the its_cmd lock held, but the ITS data

 * structure lock dropped.

 we ignore this command: we are in sync all of the time */

 We support only one (ITS) page size: 64K */

 Sanitise the physical address to be 64k aligned. */

 When GITS_CTLR.Enable is 1, this register is RO. */

	/*

	 * CWRITER is architecturally UNKNOWN on reset, but we need to reset

	 * it to CREADR to make sure we start with an empty command buffer.

 Must be called with the cmd_lock held. */

 Commands are only processed when the ITS is enabled. */

		/*

		 * If kvm_read_guest() fails, this could be due to the guest

		 * programming a bogus value in CBASER or something else going

		 * wrong from which we cannot easily recover.

		 * According to section 6.3.2 in the GICv3 spec we can just

		 * ignore that command then.

/*

 * By writing to CWRITER the guest announces new commands to be processed.

 * To avoid any races in the first place, we take the its_cmd lock, which

 * protects our ring buffer variables, so that there is only one user

 * per ITS handling commands at a given time.

 When GITS_CTLR.Enable is 1, we ignore write accesses. */

 Take the its_lock to prevent a race with a save/restore */

	/*

	 * It is UNPREDICTABLE to enable the ITS if any of the CBASER or

	 * device/collection BASER are invalid

	/*

	 * Try to process any pending commands. This function bails out early

	 * if the ITS is disabled or no commands have been queued.

 Ignore */

 This is called on setting the LPI enable bit in the redistributor. */

 Default is 16 cached LPIs per vcpu */

 An allocation failure is not fatal */

 alloc by kvm_ioctl_create_device, free by .destroy */

	/*

	 * Although the spec supports upper/lower 32-bit accesses to

	 * 64-bit ITS registers, the userspace ABI requires 64-bit

	 * accesses to all 64-bit wide registers. We therefore only

	 * support 32-bit accesses to GITS_CTLR, GITS_IIDR and GITS ID

	 * registers

/**

 * entry_fn_t - Callback called on a table entry restore path

 * @its: its handle

 * @id: id of the entry

 * @entry: pointer to the entry

 * @opaque: pointer to an opaque data

 *

 * Return: < 0 on error, 0 if last element was identified, id offset to next

 * element otherwise

/**

 * scan_its_table - Scan a contiguous table in guest RAM and applies a function

 * to each entry

 *

 * @its: its handle

 * @base: base gpa of the table

 * @size: size of the table in bytes

 * @esz: entry size in bytes

 * @start_id: the ID of the first entry in the table

 * (non zero for 2d level tables)

 * @fn: function to apply on each entry

 *

 * Return: < 0 on error, 0 if last element was identified, 1 otherwise

 * (the last element may not be found on second level tables)

/**

 * vgic_its_save_ite - Save an interrupt translation entry at @gpa

/**

 * vgic_its_restore_ite - restore an interrupt translation entry

 * @event_id: id used for indexing

 * @ptr: pointer to the ITE entry

 * @opaque: pointer to the its_device

 invalid entry, no choice but to scan next entry */

		/*

		 * If an LPI carries the HW bit, this means that this

		 * interrupt is controlled by GICv4, and we do not

		 * have direct access to that state without GICv4.1.

		 * Let's simply fail the save operation...

/**

 * vgic_its_restore_itt - restore the ITT of a device

 *

 * @its: its handle

 * @dev: device handle

 *

 * Return 0 on success, < 0 on error

 scan_its_table returns +1 if all ITEs are invalid */

/**

 * vgic_its_save_dte - Save a device table entry at a given GPA

 *

 * @its: ITS handle

 * @dev: ITS device

 * @ptr: GPA

/**

 * vgic_its_restore_dte - restore a device table entry

 *

 * @its: its handle

 * @id: device id the DTE corresponds to

 * @ptr: kernel VA where the 8 byte DTE is located

 * @opaque: unused

 *

 * Return: < 0 on error, 0 if the dte is the last one, id offset to the

 * next dte otherwise

 dte entry is valid */

/**

 * vgic_its_save_device_tables - Save the device table and all ITT

 * into guest RAM

 *

 * L1/L2 handling is hidden by vgic_its_check_id() helper which directly

 * returns the GPA of the device entry

/**

 * handle_l1_dte - callback used for L1 device table entries (2 stage case)

 *

 * @its: its handle

 * @id: index of the entry in the L1 table

 * @addr: kernel VA

 * @opaque: unused

 *

 * L1 table entries are scanned by steps of 1 entry

 * Return < 0 if error, 0 if last dte was found when scanning the L2

 * table, +1 otherwise (meaning next L1 entry must be scanned)

/**

 * vgic_its_restore_device_tables - Restore the device table and all ITT

 * from guest RAM to internal data structs

 scan_its_table returns +1 if all entries are invalid */

/**

 * vgic_its_save_collection_table - Save the collection table into

 * guest RAM

	/*

	 * table is not fully filled, add a last dummy element

	 * with valid bit unset

/**

 * vgic_its_restore_collection_table - reads the collection table

 * in guest memory and restores the ITS internal state. Requires the

 * BASER registers to be restored before.

/**

 * vgic_its_save_tables_v0 - Save the ITS tables into guest ARM

 * according to v0 ABI

/**

 * vgic_its_restore_tables_v0 - Restore the ITS tables from guest RAM

 * to internal data structs according to V0 ABI

 *

 We need to keep the ABI specific field values */

 Nothing to do */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VGICv2 MMIO handling functions

/*

 * The Revision field in the IIDR have the following meanings:

 *

 * Revision 1: Report GICv2 interrupts as group 0 instead of group 1

 * Revision 2: Interrupt groups are guest-configurable and signaled using

 * 	       their configured groups.

 Nothing to do */

		/*

		 * If we observe a write to GICD_IIDR we know that userspace

		 * has been updated and has had a chance to cope with older

		 * kernels (VGICv2 IIDR.Revision == 0) incorrectly reporting

		 * interrupts as group 1, and therefore we now allow groups to

		 * be user writable.  Doing this by default would break

		 * migration from old kernels to new kernels with legacy

		 * userspace.

 as specified by targets */

 all, ... */

 but self */

 this very vCPU only */

 reserved */

 GICD_ITARGETSR[0-7] are read-only */

 These are for userland accesses only, there is no guest-facing emulation. */

		/*

		 * Our KVM_DEV_TYPE_ARM_VGIC_V2 device ABI exports the

		 * PMR field as GICH_VMCR.VMPriMask rather than

		 * GICC_PMR.Priority, so we expose the upper five bits of

		 * priority mask to userspace using the lower bits in the

		 * unsigned long.

		/*

		 * Our KVM_DEV_TYPE_ARM_VGIC_V2 device ABI exports the

		 * PMR field as GICH_VMCR.VMPriMask rather than

		 * GICC_PMR.Priority, so we expose the upper five bits of

		 * priority mask to userspace using the lower bits in the

		 * unsigned long.

 which APRn is this */

 GICv2 hardware systems support max. 32 groups */

 GICv3 only uses ICH_AP1Rn for memory mapped (GICv2) guests */

 which APRn is this */

 GICv2 hardware systems support max. 32 groups */

 GICv3 only uses ICH_AP1Rn for memory mapped (GICv2) guests */

 We only support aligned 32-bit accesses. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015, 2016 ARM Ltd.

/*

 * Initialization rules: there are multiple stages to the vgic

 * initialization, both for the distributor and the CPU interfaces.  The basic

 * idea is that even though the VGIC is not functional or not requested from

 * user space, the critical path of the run loop can still call VGIC functions

 * that just won't do anything, without them having to check additional

 * initialization flags to ensure they don't look at uninitialized data

 * structures.

 *

 * Distributor:

 *

 * - kvm_vgic_early_init(): initialization of static data that doesn't

 *   depend on any sizing information or emulation type. No allocation

 *   is allowed there.

 *

 * - vgic_init(): allocation and initialization of the generic data

 *   structures that depend on sizing information (number of CPUs,

 *   number of interrupts). Also initializes the vcpu specific data

 *   structures. Can be executed lazily for GICv2.

 *

 * CPU Interface:

 *

 * - kvm_vgic_vcpu_init(): initialization of static data that

 *   doesn't depend on any sizing information or emulation type. No

 *   allocation is allowed there.

 EARLY INIT */

/**

 * kvm_vgic_early_init() - Initialize static VGIC VCPU data structures

 * @kvm: The VM whose VGIC districutor should be initialized

 *

 * Only do initialization of static structures that don't require any

 * allocation or sizing information from userspace.  vgic_init() called

 * kvm_vgic_dist_init() which takes care of the rest.

 CREATION */

/**

 * kvm_vgic_create: triggered by the instantiation of the VGIC device by

 * user space, either through the legacy KVM_CREATE_IRQCHIP ioctl (v2 only)

 * or through the generic KVM_CREATE_DEVICE API ioctl.

 * irqchip_in_kernel() tells you if this function succeeded or not.

 * @kvm: kvm struct pointer

 * @type: KVM_DEV_TYPE_ARM_VGIC_V[23]

	/*

	 * This function is also called by the KVM_CREATE_IRQCHIP handler,

	 * which had no chance yet to check the availability of the GICv2

	 * emulation. So check this here again. KVM_CREATE_DEVICE does

	 * the proper checks already.

 INIT/DESTROY */

/**

 * kvm_vgic_dist_init: initialize the dist data structures

 * @kvm: kvm struct pointer

 * @nr_spis: number of spis, frozen by caller

	/*

	 * In the following code we do not take the irq struct lock since

	 * no other action on irq structs can happen while the VGIC is

	 * not initialized yet:

	 * If someone wants to inject an interrupt or does a MMIO access, we

	 * require prior initialization in case of a virtual GICv3 or trigger

	 * initialization when using a virtual GICv2.

/**

 * kvm_vgic_vcpu_init() - Initialize static VGIC VCPU data

 * structures and register VCPU-specific KVM iodevs

 *

 * @vcpu: pointer to the VCPU being created and initialized

 *

 * Only do initialization, but do not actually enable the

 * VGIC CPU interface

	/*

	 * Enable and configure all SGIs to be edge-triggered and

	 * configure all PPIs as level-triggered.

 SGIs */

 PPIs */

	/*

	 * If we are creating a VCPU with a GICv3 we must also register the

	 * KVM io device for the redistributor that belongs to this VCPU.

/*

 * vgic_init: allocates and initializes dist and vcpu data structures

 * depending on two dimensioning parameters:

 * - the number of spis

 * - the number of vcpus

 * The function is generally called when nr_spis has been explicitly set

 * by the guest through the KVM DEVICE API. If not nr_spis is set to 256.

 * vgic_initialized() returns true when this function has succeeded.

 * Must be called with kvm->lock held!

 Are we also in the middle of creating a VCPU? */

 freeze the number of spis */

 Initialize groups on CPUs created before the VGIC type was known */

	/*

	 * If we have GICv4.1 enabled, unconditionnaly request enable the

	 * v4 support so that we get HW-accelerated vSGIs. Otherwise, only

	 * enable it if we present a virtual ITS to the guest.

	/*

	 * Retire all pending LPIs on this vcpu anyway as we're

	 * going to destroy it.

 To be called with kvm->lock held */

/**

 * vgic_lazy_init: Lazy init is only allowed if the GIC exposed to the guest

 * is a GICv2. A GICv3 must be explicitly initialized by the guest using the

 * KVM_DEV_ARM_VGIC_GRP_CTRL KVM_DEVICE group.

 * @kvm: kvm struct pointer

		/*

		 * We only provide the automatic initialization of the VGIC

		 * for the legacy case of a GICv2. Any other type must

		 * be explicitly initialized once setup with the respective

		 * KVM device call.

 RESOURCE MAPPING */

/**

 * Map the MMIO regions depending on the VGIC model exposed to the guest

 * called on the first VCPU run.

 * Also map the virtual CPU interface into the VM.

 * v2 calls vgic_init() if not already done.

 * v3 and derivatives return an error if the VGIC is not initialized.

 * vgic_ready() returns true if this function has succeeded.

 * @kvm: kvm struct pointer

 GENERIC PROBE */

	/*

	 * We cannot rely on the vgic maintenance interrupt to be

	 * delivered synchronously. This means we can only use it to

	 * exit the VM, and we perform the handling of EOIed

	 * interrupts on the exit path (see vgic_fold_lr_state).

/**

 * kvm_vgic_init_cpu_hardware - initialize the GIC VE hardware

 *

 * For a specific CPU, initialize the GIC VE hardware.

	/*

	 * We want to make sure the list registers start out clear so that we

	 * only have the program the used registers.

/**

 * kvm_vgic_hyp_init: populates the kvm_vgic_global_state variable

 * according to the host GIC model. Accordingly calls either

 * vgic_v2/v3_probe which registers the KVM_DEVICE that can be

 * instantiated by a guest later on .

	/*

	 * If we get one of these oddball non-GICs, taint the kernel,

	 * as we have no idea of how they *really* behave.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015, 2016 ARM Ltd.

/**

 * vgic_irqfd_set_irq: inject the IRQ corresponding to the

 * irqchip routing entry

 *

 * This is the entry point for irqfd IRQ injection

/**

 * kvm_set_routing_entry: populate a kvm routing entry

 * from a user routing entry

 *

 * @kvm: the VM this entry is applied to

 * @e: kvm kernel routing entry handle

 * @ue: user api routing entry handle

 * return 0 on success, -EINVAL on errors.

/**

 * kvm_set_msi: inject the MSI corresponding to the

 * MSI routing entry

 *

 * This is the entry point for irqfd MSI injection

 * and userspace MSI injection.

/**

 * kvm_arch_set_irq_inatomic: fast-path for irqfd injection

		/*

		 * Injecting SPIs is always possible in atomic context

		 * as long as the damn vgic is initialized.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015, 2016 ARM Ltd.

/*

 * Locking order is always:

 * kvm->lock (mutex)

 *   its->cmd_lock (mutex)

 *     its->its_lock (mutex)

 *       vgic_cpu->ap_list_lock		must be taken with IRQs disabled

 *         kvm->lpi_list_lock		must be taken with IRQs disabled

 *           vgic_irq->irq_lock		must be taken with IRQs disabled

 *

 * As the ap_list_lock might be taken from the timer interrupt handler,

 * we have to disable IRQs before taking this lock and everything lower

 * than it.

 *

 * If you need to take multiple locks, always take the upper lock first,

 * then the lower ones, e.g. first take the its_lock, then the irq_lock.

 * If you are already holding a lock and need to take a higher one, you

 * have to drop the lower ranking lock first and re-aquire it after having

 * taken the upper one.

 *

 * When taking more than one ap_list_lock at the same time, always take the

 * lowest numbered VCPU's ap_list_lock first, so:

 *   vcpuX->vcpu_id < vcpuY->vcpu_id:

 *     raw_spin_lock(vcpuX->arch.vgic_cpu.ap_list_lock);

 *     raw_spin_lock(vcpuY->arch.vgic_cpu.ap_list_lock);

 *

 * Since the VGIC must support injecting virtual interrupts from ISRs, we have

 * to use the raw_spin_lock_irqsave/raw_spin_unlock_irqrestore versions of outer

 * spinlocks for any lock that may be taken while injecting an interrupt.

/*

 * Iterate over the VM's list of mapped LPIs to find the one with a

 * matching interrupt ID and return a reference to the IRQ structure.

		/*

		 * This increases the refcount, the caller is expected to

		 * call vgic_put_irq() later once it's finished with the IRQ.

/*

 * This looks up the virtual interrupt ID to get the corresponding

 * struct vgic_irq. It also increases the refcount, so any caller is expected

 * to call vgic_put_irq() once it's finished with this IRQ.

 SGIs and PPIs */

 SPIs */

 LPIs */

/*

 * We can't do anything in here, because we lack the kvm pointer to

 * lock and remove the item from the lpi_list. So we keep this function

 * empty and use the return value of kref_put() to trigger the freeing.

/*

 * Drop the refcount on the LPI. Must be called with lpi_list_lock held.

 Set/Clear the physical active state */

/**

 * kvm_vgic_target_oracle - compute the target vcpu for an irq

 *

 * @irq:	The irq to route. Must be already locked.

 *

 * Based on the current state of the interrupt (enabled, pending,

 * active, vcpu and target_vcpu), compute the next vcpu this should be

 * given to. Return NULL if this shouldn't be injected at all.

 *

 * Requires the IRQ lock to be held.

 If the interrupt is active, it must stay on the current vcpu */

	/*

	 * If the IRQ is not active but enabled and pending, we should direct

	 * it to its configured target VCPU.

	 * If the distributor is disabled, pending interrupts shouldn't be

	 * forwarded.

	/* If neither active nor pending and enabled, then this IRQ should not

	 * be queued to any VCPU.

/*

 * The order of items in the ap_lists defines how we'll pack things in LRs as

 * well, the first items in the list being the first things populated in the

 * LRs.

 *

 * A hard rule is that active interrupts can never be pushed out of the LRs

 * (and therefore take priority) since we cannot reliably trap on deactivation

 * of IRQs and therefore they have to be present in the LRs.

 *

 * Otherwise things should be sorted by the priority field and the GIC

 * hardware support will take care of preemption of priority groups etc.

 *

 * Return negative if "a" sorts before "b", 0 to preserve order, and positive

 * to sort "b" before "a".

	/*

	 * list_sort may call this function with the same element when

	 * the list is fairly long.

 Both pending and enabled, sort by priority */

 Must be called with the ap_list_lock held */

/*

 * Only valid injection if changing level for level-triggered IRQs or for a

 * rising edge, and in-kernel connected IRQ lines can only be controlled by

 * their owner.

/*

 * Check whether an IRQ needs to (and can) be queued to a VCPU's ap list.

 * Do the queuing if necessary, taking the right locks in the right order.

 * Returns true when the IRQ was queued, false otherwise.

 *

 * Needs to be entered with the IRQ lock already held, but will return

 * with all locks dropped.

		/*

		 * If this IRQ is already on a VCPU's ap_list, then it

		 * cannot be moved or modified and there is no more work for

		 * us to do.

		 *

		 * Otherwise, if the irq is not pending and enabled, it does

		 * not need to be inserted into an ap_list and there is also

		 * no more work for us to do.

		/*

		 * We have to kick the VCPU here, because we could be

		 * queueing an edge-triggered interrupt for which we

		 * get no EOI maintenance interrupt. In that case,

		 * while the IRQ is already on the VCPU's AP list, the

		 * VCPU could have EOI'ed the original interrupt and

		 * won't see this one until it exits for some other

		 * reason.

	/*

	 * We must unlock the irq lock to take the ap_list_lock where

	 * we are going to insert this new pending interrupt.

 someone can do stuff here, which we re-check below */

	/*

	 * Did something change behind our backs?

	 *

	 * There are two cases:

	 * 1) The irq lost its pending state or was disabled behind our

	 *    backs and/or it was queued to another VCPU's ap_list.

	 * 2) Someone changed the affinity on this irq behind our

	 *    backs and we are now holding the wrong ap_list_lock.

	 *

	 * In both cases, drop the locks and retry.

	/*

	 * Grab a reference to the irq to reflect the fact that it is

	 * now in the ap_list.

/**

 * kvm_vgic_inject_irq - Inject an IRQ from a device to the vgic

 * @kvm:     The VM structure pointer

 * @cpuid:   The CPU for PPIs

 * @intid:   The INTID to inject a new state to.

 * @level:   Edge-triggered:  true:  to trigger the interrupt

 *			      false: to ignore the call

 *	     Level-sensitive  true:  raise the input signal

 *			      false: lower the input signal

 * @owner:   The opaque pointer to the owner of the IRQ being raised to verify

 *           that the caller is allowed to inject this IRQ.  Userspace

 *           injections will have owner == NULL.

 *

 * The VGIC is not concerned with devices being active-LOW or active-HIGH for

 * level-sensitive interrupts.  You can think of the level parameter as 1

 * being HIGH and 0 being LOW and all devices being active-HIGH.

 Nothing to see here, move along... */

 @irq->irq_lock must be held */

	/*

	 * Find the physical IRQ number corresponding to @host_irq

 @irq->irq_lock must be held */

/**

 * kvm_vgic_reset_mapped_irq - Reset a mapped IRQ

 * @vcpu: The VCPU pointer

 * @vintid: The INTID of the interrupt

 *

 * Reset the active and pending states of a mapped interrupt.  Kernel

 * subsystems injecting mapped interrupts should reset their interrupt lines

 * when we are doing a reset of the VM.

/**

 * kvm_vgic_set_owner - Set the owner of an interrupt for a VM

 *

 * @vcpu:   Pointer to the VCPU (used for PPIs)

 * @intid:  The virtual INTID identifying the interrupt (PPI or SPI)

 * @owner:  Opaque pointer to the owner

 *

 * Returns 0 if intid is not already used by another in-kernel device and the

 * owner is set, otherwise returns an error code.

 SGIs and LPIs cannot be wired up to any device */

/**

 * vgic_prune_ap_list - Remove non-relevant interrupts from the list

 *

 * @vcpu: The VCPU pointer

 *

 * Go over the list of "interesting" interrupts, and prune those that we

 * won't have to consider in the near future.

			/*

			 * We don't need to process this interrupt any

			 * further, move it off the list.

			/*

			 * This vgic_put_irq call matches the

			 * vgic_get_irq_kref in vgic_queue_irq_unlock,

			 * where we added the LPI to the ap_list. As

			 * we remove the irq from the list, we drop

			 * also drop the refcount.

 We're on the right CPU */

 This interrupt looks like it has to be migrated. */

		/*

		 * Ensure locking order by always locking the smallest

		 * ID first.

		/*

		 * If the affinity has been preserved, move the

		 * interrupt around. Otherwise, it means things have

		 * changed while the interrupt was unlocked, and we

		 * need to replay this.

		 *

		 * In all cases, we cannot trust the list not to have

		 * changed, so we restart from the beginning.

 Requires the irq_lock to be held. */

 Requires the ap_list_lock to be held. */

 GICv2 SGIs can count for more than one... */

 Requires the VCPU's ap_list_lock to be held. */

		/*

		 * If we have multi-SGIs in the pipeline, we need to

		 * guarantee that they are all seen before any IRQ of

		 * lower priority. In that case, we need to filter out

		 * these interrupts by exiting early. This is easy as

		 * the AP list has been sorted already.

 Nuke remaining LRs */

	/*

	 * GICv2 can always be accessed from the kernel because it is

	 * memory-mapped, and VHE systems can access GICv3 EL2 system

	 * registers.

 Sync back the hardware VGIC state into our emulation after a guest's run. */

 An empty ap_list_head implies used_lrs == 0 */

 Flush our emulation state into the GIC hardware before entering the guest. */

	/*

	 * If there are no virtual interrupts active or pending for this

	 * VCPU, then there is no work to do and we can bail out without

	 * taking any lock.  There is a potential race with someone injecting

	 * interrupts to the VCPU, but it is a benign race as the VCPU will

	 * either observe the new interrupt before or after doing this check,

	 * and introducing additional synchronization mechanism doesn't change

	 * this.

	 *

	 * Note that we still need to go through the whole thing if anything

	 * can be directly injected (GICv4).

	/*

	 * We've injected an interrupt, time to find out who deserves

	 * a good kick...

/*

 * Level-triggered mapped IRQs are special because we only observe rising

 * edges as input to the VGIC.

 *

 * If the guest never acked the interrupt we have to sample the physical

 * line and set the line level, because the device state could have changed

 * or we simply need to process the still pending interrupt later.

 *

 * We could also have entered the guest with the interrupt active+pending.

 * On the next exit, we need to re-evaluate the pending state, as it could

 * otherwise result in a spurious interrupt by injecting a now potentially

 * stale pending state.

 *

 * If this causes us to lower the level, we have to also clear the physical

 * active state, since we will otherwise never be told when the interrupt

 * becomes asserted again.

 *

 * Another case is when the interrupt requires a helping hand on

 * deactivation (no HW deactivation, for example).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015, 2016 ARM Ltd.

/*

 * transfer the content of the LRs back into the corresponding ap_list:

 * - active bit is transferred as is

 * - pending bit is

 *   - transferred as is in case of edge sensitive IRQs

 *   - set to the line-level (resample time) for level sensitive IRQs

 Extract the source vCPU id from the LR */

 Notify fds when the guest EOI'ed a level-triggered SPI */

 Always preserve the active bit, note deactivation */

 Edge is the only case where we preserve the pending bit */

		/*

		 * Clear soft pending state when level irqs have been acked.

 Handle resampling for mapped interrupts if required */

/*

 * Populates the particular LR with the state of a given IRQ:

 * - for an edge sensitive IRQ the pending state is cleared in struct vgic_irq

 * - for a level sensitive IRQ the pending state value is unchanged;

 *   it is dictated directly by the input level

 *

 * If @irq describes an SGI with multiple sources, we choose the

 * lowest-numbered source VCPU and clear that bit in the source bitmap.

 *

 * The irq_lock must be held by the caller.

		/*

		 * Never set pending+active on a HW interrupt, as the

		 * pending state is kept at the physical distributor

		 * level.

			/*

			 * Software resampling doesn't work very well

			 * if we allow P+A, so let's not do that.

	/*

	 * Level-triggered mapped IRQs are special because we only observe

	 * rising edges as input to the VGIC.  We therefore lower the line

	 * level here, so that we can take new virtual IRQs.  See

	 * vgic_v2_fold_lr_state for more info.

 The GICv2 LR only holds five bits of priority. */

	/*

	 * By forcing VMCR to zero, the GIC will restore the binary

	 * points to their reset values. Anything else resets to zero

	 * anyway.

 Get the show on the road... */

 check for overlapping regions and for regions crossing the end of memory */

	/*

	 * Initialize the vgic if this hasn't already been done on demand by

	 * accessing the vgic state from userspace.

/**

 * vgic_v2_probe - probe for a VGICv2 compatible interrupt controller

 * @info:	pointer to the GIC description

 *

 * Returns 0 if the VGICv2 has been probed successfully, returns an error code

 * otherwise

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 ARM Ltd.

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * How KVM uses GICv4 (insert rude comments here):

 *

 * The vgic-v4 layer acts as a bridge between several entities:

 * - The GICv4 ITS representation offered by the ITS driver

 * - VFIO, which is in charge of the PCI endpoint

 * - The virtual ITS, which is the only thing the guest sees

 *

 * The configuration of VLPIs is triggered by a callback from VFIO,

 * instructing KVM that a PCI device has been configured to deliver

 * MSIs to a vITS.

 *

 * kvm_vgic_v4_set_forwarding() is thus called with the routing entry,

 * and this is used to find the corresponding vITS data structures

 * (ITS instance, device, event and irq) using a process that is

 * extremely similar to the injection of an MSI.

 *

 * At this stage, we can link the guest's view of an LPI (uniquely

 * identified by the routing entry) and the host irq, using the GICv4

 * driver mapping operation. Should the mapping succeed, we've then

 * successfully upgraded the guest's LPI to a VLPI. We can then start

 * with updating GICv4's view of the property table and generating an

 * INValidation in order to kickstart the delivery of this VLPI to the

 * guest directly, without software intervention. Well, almost.

 *

 * When the PCI endpoint is deconfigured, this operation is reversed

 * with VFIO calling kvm_vgic_v4_unset_forwarding().

 *

 * Once the VLPI has been mapped, it needs to follow any change the

 * guest performs on its LPI through the vITS. For that, a number of

 * command handlers have hooks to communicate these changes to the HW:

 * - Any invalidation triggers a call to its_prop_update_vlpi()

 * - The INT command results in a irq_set_irqchip_state(), which

 *   generates an INT on the corresponding VLPI.

 * - The CLEAR command results in a irq_set_irqchip_state(), which

 *   generates an CLEAR on the corresponding VLPI.

 * - DISCARD translates into an unmap, similar to a call to

 *   kvm_vgic_v4_unset_forwarding().

 * - MOVI is translated by an update of the existing mapping, changing

 *   the target vcpu, resulting in a VMOVI being generated.

 * - MOVALL is translated by a string of mapping updates (similar to

 *   the handling of MOVI). MOVALL is horrible.

 *

 * Note that a DISCARD/MAPTI sequence emitted from the guest without

 * reprogramming the PCI endpoint after MAPTI does not result in a

 * VLPI being mapped, as there is no callback from VFIO (the guest

 * will get the interrupt via the normal SW injection). Fixing this is

 * not trivial, and requires some horrible messing with the VFIO

 * internals. Not fun. Don't do that.

 *

 * Then there is the scheduling. Each time a vcpu is about to run on a

 * physical CPU, KVM must tell the corresponding redistributor about

 * it. And if we've migrated our vcpu from one CPU to another, we must

 * tell the ITS (so that the messages reach the right redistributor).

 * This is done in two steps: first issue a irq_set_affinity() on the

 * irq corresponding to the vcpu, then call its_make_vpe_resident().

 * You must be in a non-preemptible context. On exit, a call to

 * its_make_vpe_non_resident() tells the redistributor that we're done

 * with the vcpu.

 *

 * Finally, the doorbell handling: Each vcpu is allocated an interrupt

 * which will fire each time a VLPI is made pending whilst the vcpu is

 * not running. Each time the vcpu gets blocked, the doorbell

 * interrupt gets enabled. When the vcpu is unblocked (for whatever

 * reason), the doorbell interrupt is disabled.

 We got the message, no need to fire again */

	/*

	 * The v4.1 doorbell can fire concurrently with the vPE being

	 * made non-resident. Ensure we only update pending_last

	 * *after* the non-residency sequence has completed.

	/*

	 * With GICv4.1, every virtual SGI can be directly injected. So

	 * let's pretend that they are HW interrupts, tied to a host

	 * IRQ. The SGI code will do its magic.

 Transfer the full irq state to the vPE */

 Transfer pending state */

 Must be called with the kvm lock held */

/*

 * Must be called with GICv4.1 and the vPE unmapped, which

 * indicates the invalidation of any VPT caches associated

 * with the vPE, thus we can get the VLPI state by peeking

 * at the VPT.

/**

 * vgic_v4_init - Initialize the GICv4 data structures

 * @kvm:	Pointer to the VM being initialized

 *

 * We may be called each time a vITS is created, or when the

 * vgic is initialized. This relies on kvm->lock to be

 * held. In both cases, the number of vcpus should now be

 * fixed.

 Nothing to see here... move along. */

		/*

		 * Don't automatically enable the doorbell, as we're

		 * flipping it back and forth when the vcpu gets

		 * blocked. Also disable the lazy disabling, as the

		 * doorbell could kick us out of the guest too

		 * early...

		 *

		 * On GICv4.1, the doorbell is managed in HW and must

		 * be left enabled.

			/*

			 * Trick: adjust the number of vpes so we know

			 * how many to nuke on teardown...

/**

 * vgic_v4_teardown - Free the GICv4 data structures

 * @kvm:	Pointer to the VM being destroyed

 *

 * Relies on kvm->lock to be held.

	/*

	 * Before making the VPE resident, make sure the redistributor

	 * corresponding to our current CPU expects us here. See the

	 * doc in drivers/irqchip/irq-gic-v4.c to understand how this

	 * turns into a VMOVP command at the ITS level.

	/*

	 * Now that the VPE is resident, let's get rid of a potential

	 * doorbell interrupt that would still be pending. This is a

	 * GICv4.0 only "feature"...

	/*

	 * No need to wait for the vPE to be ready across a shallow guest

	 * exit, as only a vcpu_put will invalidate it.

	/*

	 * Get the ITS, and escape early on error (not a valid

	 * doorbell for any of our vITSs).

 Perform the actual DevID/EventID -> LPI translation. */

	/*

	 * Emit the mapping request. If it fails, the ITS probably

	 * isn't v4 compatible, so let's silently bail out. Holding

	 * the ITS lock should ensure that nothing can modify the

	 * target vcpu.

 Transfer pending state */

		/*

		 * Clear pending_latch and communicate this state

		 * change via vgic_queue_irq_unlock.

	/*

	 * Get the ITS, and escape early on error (not a valid

	 * doorbell for any of our vITSs).

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VGIC: KVM DEVICE API

 *

 * Copyright (C) 2015 ARM Ltd.

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 common helpers */

/**

 * kvm_vgic_addr - set or get vgic VM base addresses

 * @kvm:   pointer to the vm struct

 * @type:  the VGIC addr type, one of KVM_VGIC_V[23]_ADDR_TYPE_XXX

 * @addr:  pointer to address value

 * @write: if true set the address in the VM address space, if false read the

 *          address

 *

 * Set or get the vgic base addresses for the distributor and the virtual CPU

 * interface in the VM physical address space.  These addresses are properties

 * of the emulated core/SoC and therefore user space initially knows this

 * information.

 * Check them for sanity (alignment, double assignment). We can't check for

 * overlapping regions in case of a virtual GICv3 here, since we don't know

 * the number of VCPUs yet, so we defer this check to map_resources().

		/*

		 * We require:

		 * - at least 32 SPIs on top of the 16 SGIs and 16 PPIs

		 * - at most 1024 interrupts

		 * - a multiple of 32 interrupts

 unlocks vcpus from @vcpu_lock_idx and smaller */

 Returns true if all vcpus were locked, false otherwise */

	/*

	 * Any time a vcpu is run, vcpu_load is called which tries to grab the

	 * vcpu->mutex.  By grabbing the vcpu->mutex of all VCPUs we ensure

	 * that no other VCPUs are run and fiddle with the vgic state while we

	 * access it.

/**

 * vgic_v2_attr_regs_access - allows user space to access VGIC v2 state

 *

 * @dev:      kvm device handle

 * @attr:     kvm device attribute

 * @reg:      address the value is read or written

 * @is_write: true if userspace is writing a register

	/*

	 * For KVM_DEV_ARM_VGIC_GRP_DIST_REGS group,

	 * attr might not hold MPIDR. Hence assume vcpu0.

/*

 * vgic_v3_attr_regs_access - allows user space to access VGIC v3 state

 *

 * @dev:      kvm device handle

 * @attr:     kvm device attribute

 * @reg:      address the value is read or written

 * @is_write: true if userspace is writing a register

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyp portion of the (not much of an) Emulation layer for 32bit guests.

 *

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * based on arch/arm/kvm/emulate.c

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

/*

 * stolen from arch/arm/kernel/opcodes.c

 *

 * condition code lookup table

 * index into the table is test code: EQ, NE, ... LT, GT, AL, NV

 *

 * bit position in short is condition code: NZCV

 EQ == Z set            */

 NE                     */

 CS == C set            */

 CC                     */

 MI == N set            */

 PL                     */

 VS == V set            */

 VC                     */

 HI == C set && Z clear */

 LS == C clear || Z set */

 GE == (N==V)           */

 LT == (N!=V)           */

 GT == (!Z && (N==V))   */

 LE == (Z || (N!=V))    */

 AL always              */

 NV                     */

/*

 * Check if a trapped instruction should have been executed or not.

 Top two bits non-zero?  Unconditional. */

 Is condition field valid? */

 This can happen in Thumb mode: examine IT state. */

 it == 0 => unconditional. */

 The cond for this insn works out as the top 4 bits. */

/**

 * adjust_itstate - adjust ITSTATE when emulating instructions in IT-block

 * @vcpu:	The VCPU pointer

 *

 * When exceptions occur while instructions are executed in Thumb IF-THEN

 * blocks, the ITSTATE field of the CPSR is not advanced (updated), so we have

 * to do this little bit of work manually. The fields map like this:

 *

 * IT[7:0] -> CPSR[26:25],CPSR[15:10]

 Perform ITAdvance (see page A2-52 in ARM DDI 0406C) */

/**

 * kvm_skip_instr - skip a trapped instruction and proceed to the next

 * @vcpu: The vcpu pointer

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2020 - Google LLC

 * Author: Quentin Perret <qperret@google.com>

	/*

	 * The hyp_vmemmap needs to be backed by pages, but these pages

	 * themselves need to be present in the vmemmap, so compute the number

	 * of pages needed by looking for a fixed point.

	/*

	 * Try to allocate a PMD-aligned region to reduce TLB pressure once

	 * this is unmapped from the host stage-2, and fallback to PAGE_SIZE.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * __vgic_v2_perform_cpuif_access -- perform a GICV access on behalf of the

 *				     guest.

 *

 * @vcpu: the offending vcpu

 *

 * Returns:

 *  1: GICV access successfully performed

 *  0: Not a GICV access

 * -1: Illegal GICV access successfully performed

 Build the full address */

 If not for GICV, move on */

 Reject anything but a 32bit access */

 Not aligned? Don't bother */

 guest pre-swabbed data, undo this for writel() */

 guest expects swabbed data */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

	/*

	 * Make sure stores to the GIC via the memory mapped interface

	 * are now visible to the system register interface when reading the

	 * LRs, and when reading back the VMCR on non-VHE systems.

	/*

	 * Ensure that writes to the LRs, and on non-VHE systems ensure that

	 * the write to the VMCR in __vgic_v3_activate_traps(), will have

	 * reached the (re)distributors. This ensure the guest will read the

	 * correct values from the memory-mapped interface.

	/*

	 * VFIQEn is RES1 if ICC_SRE_EL1.SRE is 1. This causes a

	 * Group0 interrupt (as generated in GICv2 mode) to be

	 * delivered as a FIQ to the guest, with potentially fatal

	 * consequences. So we must make sure that ICC_SRE_EL1 has

	 * been actually programmed with the value we want before

	 * starting to mess with the rest of the GIC, and VMCR_EL2 in

	 * particular.  This logic must be called before

	 * __vgic_v3_restore_state().

			/*

			 * Ensure that the write to the VMCR will have reached

			 * the (re)distributors. This ensure the guest will

			 * read the correct values from the memory-mapped

			 * interface.

	/*

	 * Prevent the guest from touching the GIC system registers if

	 * SRE isn't enabled for GICv3 emulation.

	/*

	 * If we need to trap system registers, we must write

	 * ICH_HCR_EL2 anyway, even if no interrupts are being

	 * injected,

 Make sure ENABLE is set at EL2 before setting SRE at EL1 */

	/*

	 * If we were trapping system registers, we enabled the VGIC even if

	 * no interrupts were being injected, and we disable it again here.

/*

 * Return the GIC CPU configuration:

 * - [31:0]  ICH_VTR_EL2

 * - [62:32] RES0

 * - [63]    MMIO (GICv2) capable

	/*

	 * To check whether we have a MMIO-based (GICv2 compatible)

	 * CPU interface, we need to disable the system register

	 * view. To do that safely, we have to prevent any interrupt

	 * from firing (which would be deadly).

	 *

	 * Note that this only makes sense on VHE, as interrupts are

	 * already masked for nVHE as part of the exception entry to

	 * EL2.

	/*

	 * Table 11-2 "Permitted ICC_SRE_ELx.SRE settings" indicates

	 * that to be able to set ICC_SRE_EL1.SRE to 0, all the

	 * interrupt overrides must be set. You've got to love this.

 See Pseudocode for VPriorityGroup */

 Not pending in the state? */

 Group-0 interrupt, but Group-0 disabled? */

 Group-1 interrupt, but Group-1 disabled? */

 Not the highest priority? */

 This is a candidate */

		/*

		 * The ICH_AP0Rn_EL2 and ICH_AP1Rn_EL2 registers

		 * contain the active priority levels for this VCPU

		 * for the maximum number of supported priority

		 * levels, and we return the full priority level only

		 * if the BPR is programmed to its minimum, otherwise

		 * we return a combination of the priority level and

		 * subpriority, as determined by the setting of the

		 * BPR, but without the full subpriority.

/*

 * Convert a priority to a preemption level, taking the relevant BPR

 * into account by zeroing the sub-priority bits.

/*

 * The priority value is independent of any of the BPR values, so we

 * normalize it using the minimal BPR value. This guarantees that no

 * matter what the guest does with its BPR, we can always set/get the

 * same value of a priority.

 Always clear the LSB, which is the highest priority */

 Rescale to 8 bits of priority */

 EOImode == 0, nothing to be done here */

 No deactivate to be performed on an LPI */

 Drop priority in any case */

 Do not bump EOIcount for LPIs that aren't in the LRs */

 EOImode == 1 and not an LPI, nothing to be done here */

 If priorities or group do not match, the guest has fscked-up. */

 Let's now perform the deactivation */

 Enforce BPR limiting */

 Enforce BPR limiting */

 PRIbits */

 IDbits */

 A3V */

 EOImode */

 CBPR */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stand-alone page-table allocator for hyp stage-1 and guest stage-2.

 * No bombay mix was harmed in the writing of this file.

 *

 * Copyright (C) 2020 Google LLC

 * Author: Will Deacon <will@kernel.org>

 May underflow */

	/*

	 * Tolerate KVM recreating the exact same mapping, or changing software

	 * bits if the existing mapping was valid.

 Force mappings to page granularity */

	/*

	 * Use a minimum 2 level page table to prevent splitting

	 * host PMD huge pages at stage2.

	/*

	 * Enable the Hardware Access Flag management, unconditionally

	 * on all CPUs. The features is RES0 on CPUs without the support

	 * and must be ignored by the CPUs.

 Set the vmid bits */

	/*

	 * The refcount tracks valid entries as well as invalid entries if they

	 * encode ownership of a page to another entity than the page-table

	 * owner, whose id is 0.

	/*

	 * Clear the existing PTE, and perform break-before-make with

	 * TLB maintenance if it was valid.

		/*

		 * Skip updating the PTE if we are trying to recreate the exact

		 * same mapping or only change the access permissions. Instead,

		 * the vCPU will exit one more time from guest if still needed

		 * and then go through the path of relaxing permissions.

 Perform CMOs before installation of the guest stage-2 PTE */

	/*

	 * Invalidate the whole stage-2, as we may have numerous leaf

	 * entries below us which would otherwise need invalidating

	 * individually.

	/*

	 * If we've run into an existing block mapping then replace it with

	 * a table. Accesses beyond 'end' that fall within the new table

	 * will be mapped lazily.

/*

 * This is a little fiddly, as we use all three of the walk flags. The idea

 * is that the TABLE_PRE callback runs for table entries on the way down,

 * looking for table entries which we could conceivably replace with a

 * block entry for this mapping. If it finds one, then it sets the 'anchor'

 * field in 'struct stage2_map_data' to point at the table entry, before

 * clearing the entry to zero and descending into the now detached table.

 *

 * The behaviour of the LEAF callback then depends on whether or not the

 * anchor has been set. If not, then we're not using a block mapping higher

 * up the table and we perform the mapping at the existing leaves instead.

 * If, on the other hand, the anchor _is_ set, then we drop references to

 * all valid leaves so that the pages beneath the anchor can be freed.

 *

 * Finally, the TABLE_POST callback does nothing if the anchor has not

 * been set, but otherwise frees the page-table pages while walking back up

 * the page-table, installing the block entry when it revisits the anchor

 * pointer and clearing the anchor to NULL.

	/*

	 * This is similar to the map() path in that we unmap the entire

	 * block entry and rely on the remaining portions being faulted

	 * back lazily.

	/*

	 * We may race with the CPU trying to set the access flag here,

	 * but worst-case the access flag update gets lost and will be

	 * set on the next access instead.

		/*

		 * Invalidate instruction cache before updating the guest

		 * stage-2 PTE if we are going to add executable permission.

	/*

	 * "But where's the TLBI?!", you scream.

	 * "Over in the core code", I sigh.

	 *

	 * See the '->clear_flush_young()' callback on the KVM mmu notifier.

 Ensure zeroed PGD pages are visible to the hardware walker */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Fault injection for both 32 and 64bit guests.

 *

 * Copyright (C) 2012,2013 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 *

 * Based on arch/arm/kvm/emulate.c

 * Copyright (C) 2012 - Virtual Open Systems and Columbia University

 * Author: Christoffer Dall <c.dall@virtualopensystems.com>

/*

 * This performs the exception entry at a given EL (@target_mode), stashing PC

 * and PSTATE into ELR and SPSR respectively, and compute the new PC/PSTATE.

 * The EL passed to this function *must* be a non-secure, privileged mode with

 * bit 0 being set (PSTATE.SP == 1).

 *

 * When an exception is taken, most PSTATE fields are left unchanged in the

 * handler. However, some are explicitly overridden (e.g. M[4:0]). Luckily all

 * of the inherited bits have the same position in the AArch64/AArch32 SPSR_ELx

 * layouts, so we don't need to shuffle these for exceptions from AArch32 EL0.

 *

 * For the SPSR_ELx layout for AArch64, see ARM DDI 0487E.a page C5-429.

 * For the SPSR_ELx layout for AArch32, see ARM DDI 0487E.a page C5-426.

 *

 * Here we manipulate the fields in order of the AArch64 SPSR_ELx layout, from

 * MSB to LSB.

 Don't do that */

 PSTATE.UAO is set to zero upon any exception to AArch64

 See ARM DDI 0487E.a, page D5-2579.

 PSTATE.PAN is unchanged unless SCTLR_ELx.SPAN == 0b0

 SCTLR_ELx.SPAN is RES1 when ARMv8.1-PAN is not implemented

 See ARM DDI 0487E.a, page D5-2578.

 PSTATE.SS is set to zero upon any exception to AArch64

 See ARM DDI 0487E.a, page D2-2452.

 PSTATE.IL is set to zero upon any exception to AArch64

 See ARM DDI 0487E.a, page D1-2306.

 PSTATE.SSBS is set to SCTLR_ELx.DSSBS upon any exception to AArch64

 See ARM DDI 0487E.a, page D13-3258

 PSTATE.BTYPE is set to zero upon any exception to AArch64

 See ARM DDI 0487E.a, pages D1-2293 to D1-2294.

/*

 * When an exception is taken, most CPSR fields are left unchanged in the

 * handler. However, some are explicitly overridden (e.g. M[4:0]).

 *

 * The SPSR/SPSR_ELx layouts differ, and the below is intended to work with

 * either format. Note: SPSR.J bit doesn't exist in SPSR_ELx, but this bit was

 * obsoleted by the ARMv7 virtualization extensions and is RES0.

 *

 * For the SPSR layout seen from AArch32, see:

 * - ARM DDI 0406C.d, page B1-1148

 * - ARM DDI 0487E.a, page G8-6264

 *

 * For the SPSR_ELx layout for AArch32 seen from AArch64, see:

 * - ARM DDI 0487E.a, page C5-426

 *

 * Here we manipulate the fields in order of the AArch32 SPSR_ELx layout, from

 * MSB to LSB.

 CPSR.IT[7:0] are set to zero upon any exception

 See ARM DDI 0487E.a, section G1.12.3

 See ARM DDI 0406C.d, section B1.8.3

 CPSR.SSBS is set to SCTLR.DSSBS upon any exception

 See ARM DDI 0487E.a, page G8-6244

 CPSR.PAN is unchanged unless SCTLR.SPAN == 0b0

 SCTLR.SPAN is RES1 when ARMv8.1-PAN is not implemented

 See ARM DDI 0487E.a, page G8-6246

 SS does not exist in AArch32, so ignore

 CPSR.IL is set to zero upon any exception

 See ARM DDI 0487E.a, page G1-5527

 CPSR.IT[7:0] are set to zero upon any exception

 See prior comment above

 CPSR.E is set to SCTLR.EE upon any exception

 See ARM DDI 0487E.a, page G8-6245

 See ARM DDI 0406C.d, page B4-1701

 CPSR.A is unchanged upon an exception to Undefined, Supervisor

 CPSR.A is set upon an exception to other modes

 See ARM DDI 0487E.a, pages G1-5515 to G1-5516

 See ARM DDI 0406C.d, page B1-1182

 CPSR.I is set upon any exception

 See ARM DDI 0487E.a, pages G1-5515 to G1-5516

 See ARM DDI 0406C.d, page B1-1182

 CPSR.F is set upon an exception to FIQ

 CPSR.F is unchanged upon an exception to other modes

 See ARM DDI 0487E.a, pages G1-5515 to G1-5516

 See ARM DDI 0406C.d, page B1-1182

 CPSR.T is set to SCTLR.TE upon any exception

 See ARM DDI 0487E.a, page G8-5514

 See ARM DDI 0406C.d, page B1-1181

/*

 * Table taken from ARMv8 ARM DDI0487B-B, table G1-10.

 Reset, unused */

 Undefined */

 SVC, unused */

 Prefetch abort */

 Data abort */

 HVC, unused */

 IRQ, unused */

 FIQ, unused */

 KVM only enters the ABT and UND modes, so only deal with those */

 Branch to exception vector */

 always have security exceptions */

 Err... */

			/*

			 * Only EL1_SYNC makes sense so far, EL2_{SYNC,IRQ}

			 * will be implemented at some point. Everything

			 * else gets silently ignored.

/*

 * Adjust the guest PC (and potentially exception state) depending on

 * flags provided by the emulation code.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 - Google Inc

 * Author: Andrew Scull <ascull@google.com>

	/*

	 * __pkvm_init() will return only if an error occurred, otherwise it

	 * will tail-call in __pkvm_init_finalise() which will have to deal

	 * with the host context directly.

 ___kvm_hyp_init */

	/*

	 * If pKVM has been initialised then reject any calls to the

	 * early "privileged" hypercalls. Note that we cannot reject

	 * calls to __pkvm_prot_finalize for two reasons: (1) The static

	 * key used to determine initialisation must be toggled prior to

	 * finalisation and (2) finalisation is performed on a per-CPU

	 * basis. This is all fine, however, since __pkvm_prot_finalize

	 * returns -EPERM after the first call for a given CPU.

 SMC was trapped, move ELR past the current PC. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google LLC

 * Author: Quentin Perret <qperret@google.com>

 Recreate the hyp page-table using the early page allocator */

	/*

	 * Map the host's .bss and .rodata sections RO in the hypervisor, but

	 * transfer the ownership from the host to the hypervisor itself to

	 * make sure it can't be donated or shared with another entity.

	 *

	 * The ownership transition requires matching changes in the host

	 * stage-2. This will be done later (see finalize_host_mappings()) once

	 * the hyp_vmemmap is addressable.

	/*

	 * Adjust the host stage-2 mappings to match the ownership attributes

	 * configured in the hypervisor stage-1.

 Now that the vmemmap is backed, install the full-fledged allocator */

	/*

	 * We tail-called to here from handle___pkvm_init() and will not return,

	 * so make sure to propagate the return value to the host.

 Jump in the idmap page to switch to the new page-tables */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 Non-VHE specific context */

		/*

		 * At this stage, and thanks to the above isb(), S2 is

		 * configured and enabled. We can now restore the guest's S1

		 * configuration: SCTLR, and only then TCR.

		/*

		 * Set the TCR and SCTLR registers in the exact opposite

		 * sequence as __activate_traps (first prevent walks,

		 * then force the MMU on). A generous sprinkling of isb()

		 * ensure that things happen in this exact order.

 Save VGICv3 state on non-VHE systems */

 Restore VGICv3 state on non_VEH systems */

/**

 * Disable host events, enable guest events

/**

 * Disable guest events, enable host events

/**

 * Handler for protected VM MSR, MRS or System instruction execution in AArch64.

 *

 * Returns true if the hypervisor has handled the exit, and control should go

 * back to the guest, or false if it hasn't.

	/*

	 * Make sure we handle the exit for workarounds and ptrauth

	 * before the pKVM handling, as the latter could decide to

	 * UNDEF.

/**

 * Handler for protected floating-point and Advanced SIMD accesses.

 *

 * Returns true if the hypervisor has handled the exit, and control should go

 * back to the guest, or false if it hasn't.

 Linux guests assume support for floating-point and Advanced SIMD. */

/*

 * Some guests (e.g., protected VMs) are not be allowed to run in AArch32.

 * The ARMv8 architecture does not give the hypervisor a mechanism to prevent a

 * guest from dropping to AArch32 EL0 if implemented by the CPU. If the

 * hypervisor spots a guest in such a state ensure it is handled, and don't

 * trust the host to spot or fix it.  The check below is based on the one in

 * kvm_arch_vcpu_ioctl_run().

 *

 * Returns false if the guest ran in AArch32 when it shouldn't have, and

 * thus should exit to the host, or true if a the guest run loop can continue.

		/*

		 * As we have caught the guest red-handed, decide that it isn't

		 * fit for purpose anymore by making the vcpu invalid. The VMM

		 * can try and fix it by re-initializing the vcpu with

		 * KVM_ARM_VCPU_INIT, however, this is likely not possible for

		 * protected VMs.

 Switch to the guest for legacy non-VHE systems */

	/*

	 * Having IRQs masked via PMR when entering the guest means the GIC

	 * will not signal the CPU of interrupts of lower priority, and the

	 * only way to get out will be via guest exceptions.

	 * Naturally, we want to avoid this.

	/*

	 * We must flush and disable the SPE buffer for nVHE, as

	 * the translation regime(EL1&0) is going to be loaded with

	 * that of the guest. And we must do this before we change the

	 * translation regime to EL2 (via MDCR_EL2_E2PB == 0) and

	 * before we load guest Stage1.

	/*

	 * We must restore the 32-bit state before the sysregs, thanks

	 * to erratum #852523 (Cortex-A57) or #853709 (Cortex-A72).

	 *

	 * Also, and in order to be able to deal with erratum #1319537 (A57)

	 * and #1319367 (A72), we must ensure that all VM-related sysreg are

	 * restored before we enable S2 translation.

 Jump in the fire! */

 And we're baaack! */

	/*

	 * This must come after restoring the host sysregs, since a non-VHE

	 * system may enable SPE here and make use of the TTBRs.

 Returning to host will clear PSR.I, remask PMR if needed */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 - Google LLC

 * Author: David Brazdil <dbrazdil@google.com>

 *

 * Generates relocation information used by the kernel to convert

 * absolute addresses in hyp data from kernel VAs to hyp VAs.

 *

 * This is necessary because hyp code is linked into the same binary

 * as the kernel but executes under different memory mappings.

 * If the compiler used absolute addressing, those addresses need to

 * be converted before they are used by hyp code.

 *

 * The input of this program is the relocatable ELF object containing

 * all hyp code/data, not yet linked into vmlinux. Hyp section names

 * should have been prefixed with `.hyp` at this point.

 *

 * The output (printed to stdout) is an assembly file containing

 * an array of 32-bit integers and static relocations that instruct

 * the linker of `vmlinux` to populate the array entries with offsets

 * to positions in the kernel binary containing VAs used by hyp code.

 *

 * Note that dynamic relocations could be used for the same purpose.

 * However, those are only generated if CONFIG_RELOCATABLE=y.

/*

 * AArch64 relocation type constants.

 * Included in case these are not defined in the host toolchain.

 Global state of the processed ELF. */

/*

 * Return a pointer of a given type at a given offset from

 * the beginning of the ELF file.

 Iterate over all sections in the ELF. */

 Iterate over all Elf64_Rela relocations in a given section. */

 True if a string starts with a given prefix. */

 Returns a string containing the name of a given section. */

 Returns a pointer to the first byte of section data. */

 Find a section by its offset from the beginning of the file. */

 Find a section by its index. */

/*

 * Memory-map the given ELF file, perform sanity checks, and

 * populate global state.

 Store path in the global struct for error printing. */

 Open the ELF file. */

 Get status of ELF file to obtain its size. */

 mmap() the entire ELF file read-only at an arbitrary address. */

 mmap() was successful, close the FD. */

 Get pointer to the ELF header. */

 Check the ELF magic. */

 Sanity check that this is an ELF64 relocatable object for AArch64. */

 Populate fields of the global struct. */

 Print the prologue of the output ASM file. */

 Print ASM statements needed as a prologue to a processed hyp section. */

 Declare the hyp section symbol. */

/*

 * Print ASM statements to create a hyp relocation entry for a given

 * R_AARCH64_ABS64 relocation.

 *

 * The linker of vmlinux will populate the position given by `rela` with

 * an absolute 64-bit kernel VA. If the kernel is relocatable, it will

 * also generate a dynamic relocation entry so that the kernel can shift

 * the address at runtime for KASLR.

 *

 * Emit a 32-bit offset from the current address to the position given

 * by `rela`. This way the kernel can iterate over all kernel VAs used

 * by hyp at runtime and convert them to hyp VAs. However, that offset

 * will not be known until linking of `vmlinux`, so emit a PREL32

 * relocation referencing a symbol that the hyp linker script put at

 * the beginning of the relocated section + the offset from `rela`.

 Offset of this reloc from the beginning of HYP_RELOC_SECTION. */

 Create storage for the 32-bit offset. */

	/*

	 * Create a PREL32 relocation which instructs the linker of `vmlinux`

	 * to insert offset to position <base> + <offset>, where <base> is

	 * a symbol at the beginning of the relocated section, and <offset>

	 * is `rela->r_offset`.

 Print the epilogue of the output ASM file. */

/*

 * Iterate over all RELA relocations in a given section and emit

 * hyp relocation data for all absolute addresses in hyp code/data.

 *

 * Static relocations that generate PC-relative-addressing are ignored.

 * Failure is reported for unexpected relocation types.

 Skip all non-hyp sections. */

 Check that rela points inside the relocated section. */

		/*

		 * Data relocations to generate absolute addressing.

		 * Emit a hyp relocation.

 Allow position-relative data relocations. */

 Allow relocations to generate PC-relative addressing. */

 Allow relative relocations for control-flow instructions. */

 Allow group relocations to create PC-relative offset inline. */

 Iterate over all sections and emit hyp relocation data for RELA sections. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 - Google LLC

 * Author: David Brazdil <dbrazdil@google.com>

 Config options set by the host. */

 SMCCC reserves IDs 0x00-1F with the given 32/64-bit base for PSCI. */

 Reject invalid MPIDRs */

	/*

	 * Find the logical CPU ID for the given MPIDR. The search set is

	 * the set of CPUs that were online at the point of KVM initialization.

	 * Booting other CPUs is rejected because their cpufeatures were not

	 * checked against the finalized capabilities. This could be relaxed

	 * by doing the feature checks in hyp.

 Check if the target CPU is already being booted. */

 If successful, the lock will be released by the target CPU. */

	/*

	 * No need to acquire a lock before writing to boot_args because a core

	 * can only suspend itself. Racy CPU_ON calls use a separate struct.

	/*

	 * Will either return if shallow sleep state, or wake up into the entry

	 * point if it is a deep sleep state.

	/*

	 * No need to acquire a lock before writing to boot_args because a core

	 * can only suspend itself. Racy CPU_ON calls use a separate struct.

 Will only return on error. */

	/*

	 * SYSTEM_OFF/RESET should not return according to the spec.

	 * Allow it so as to stay robust to broken firmware.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2021 Google LLC

 * Author: Fuad Tabba <tabba@google.com>

/*

 * Set trap register values based on features in ID_AA64PFR0.

 Protected KVM does not support AArch32 guests. */

	/*

	 * Linux guests assume support for floating-point and Advanced SIMD. Do

	 * not change the trapping behavior for these from the KVM default.

 Trap RAS unless all current versions are supported */

 Trap AMU */

 Trap SVE */

/*

 * Set trap register values based on features in ID_AA64PFR1.

 Memory Tagging: Trap and Treat as Untagged if not supported. */

/*

 * Set trap register values based on features in ID_AA64DFR0.

 Trap/constrain PMU */

 Trap Debug */

 Trap OS Double Lock */

 Trap SPE */

 Trap Trace Filter */

 Trap Trace */

/*

 * Set trap register values based on features in ID_AA64MMFR0.

 Trap Debug Communications Channel registers */

/*

 * Set trap register values based on features in ID_AA64MMFR1.

 Trap LOR */

/*

 * Set baseline trap register values.

	/*

	 * Always trap:

	 * - Feature id registers: to control features exposed to guests

	 * - Implementation-defined features

 Clear res0 and set res1 bits to trap potential new features. */

/*

 * Initialize trap register values for protected VMs.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google LLC

 * Author: Quentin Perret <qperret@google.com>

	/*

	 * The size of concatenated PGDs is always a power of two of PAGE_SIZE,

	 * so there should be no need to free any of the tail pages to make the

	 * allocation exact.

 The host stage 2 is id-mapped, so use parange for T0SZ */

	/*

	 * Make sure to have an ISB before the TLB maintenance below but only

	 * when __load_stage2() doesn't include one already.

 Invalidate stale HCR bits that may be cached in TLBs */

 Unmap all non-memory regions to recycle the pages */

 The list of memblock regions is sorted, binary search it */

/*

 * The pool has been provided with enough pages to cover all of memory with

 * page granularity, but it is difficult to know how much of the MMIO range

 * we will need to cover upfront, so we may need to 'recycle' the pages if we

 * run out.

	/*

	 * Block mappings must be used with care in the host stage-2 as a

	 * kvm_pgtable_stage2_map() operation targeting a page in the range of

	 * an existing block will delete the block under the assumption that

	 * mappings in the rest of the block range can always be rebuilt lazily.

	 * That assumption is correct for the host stage-2 with RWX mappings

	 * targeting memory or RW mappings targeting MMIO ranges (see

	 * host_stage2_idmap() below which implements some of the host memory

	 * abort logic). However, this is not safe for any other mappings where

	 * the host stage-2 page-table is in fact the only place where this

	 * state is stored. In all those cases, it is safer to use page-level

	 * mappings, hence avoiding to lose the state because of side-effects in

	 * kvm_pgtable_stage2_map().

	/*

	 * Check attributes in the host stage-2 PTE. We need the page to be:

	 *  - mapped RWX as we're sharing memory;

	 *  - not borrowed, as that implies absence of ownership.

	 * Otherwise, we can't let it got through

	/*

	 * Tolerate double-sharing the same page, but this requires

	 * cross-checking the hypervisor stage-1.

	/*

	 * If the page has been shared with the hypervisor, it must be

	 * already mapped as SHARED_BORROWED in its stage-1.

	/*

	 * If the page is not yet shared, adjust mappings in both page-tables

	 * while both locks are held.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

		/*

		 * For CPUs that are affected by ARM 1319367, we need to

		 * avoid a host Stage-1 walk while we have the guest's

		 * VMID set in the VTTBR in order to invalidate TLBs.

		 * We're guaranteed that the S1 MMU is enabled, so we can

		 * simply set the EPD bits to avoid any further TLB fill.

	/*

	 * __load_stage2() includes an ISB only when the AT

	 * workaround is applied. Take care of the opposite condition,

	 * ensuring that we always have an ISB, but not two ISBs back

	 * to back.

 Ensure write of the host VMID */

 Restore the host's TCR_EL1 */

 Switch to requested VMID */

	/*

	 * We could do so much better if we had the VA as well.

	 * Instead, we invalidate Stage-2 for this IPA, and the

	 * whole of Stage-1. Weep...

	/*

	 * We have to ensure completion of the invalidation at Stage-2,

	 * since a table walk on another CPU could refill a TLB with a

	 * complete (S1 + S2) walk based on the old Stage-2 mapping if

	 * the Stage-1 invalidation happened first.

	/*

	 * If the host is running at EL1 and we have a VPIPT I-cache,

	 * then we must perform I-cache maintenance at EL2 in order for

	 * it to have an effect on the guest. Since the guest cannot hit

	 * I-cache lines allocated with a different VMID, we don't need

	 * to worry about junk out of guest reset (we nuke the I-cache on

	 * VMID rollover), but we do need to be careful when remapping

	 * executable pages for the same guest. This can happen when KSM

	 * takes a CoW fault on an executable page, copies the page into

	 * a page that was previously mapped in the guest and then needs

	 * to invalidate the guest view of the I-cache for that page

	 * from EL1. To solve this, we invalidate the entire I-cache when

	 * unmapping a page from a guest if we have a VPIPT I-cache but

	 * the host is running at EL1. As above, we could do better if

	 * we had the VA.

	 *

	 * The moral of this story is: if you have a VPIPT I-cache, then

	 * you should be running with VHE enabled.

 Switch to requested VMID */

 Switch to requested VMID */

	/*

	 * VIPT and PIPT caches are not affected by VMID, so no maintenance

	 * is necessary across a VMID rollover.

	 *

	 * VPIPT caches constrain lookup and maintenance to the active VMID,

	 * so we need to invalidate lines with a stale VMID to avoid an ABA

	 * race after multiple rollovers.

	 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google LLC

 * Author: Quentin Perret <qperret@google.com>

 Are we overflowing on the vmemmap ? */

	/*

	 * One half of the VA space is reserved to linearly map portions of

	 * memory -- see va_layout.c for more details. The other half of the VA

	 * space contains the trampoline page, and needs some care. Split that

	 * second half in two and find the quarter of VA space not conflicting

	 * with the idmap to place the IOs and the vmemmap. IOs use the lower

	 * half of the quarter and the vmemmap the upper half.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * Non-VHE: Both host and guest must save everything.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * Should only be called on non-VHE systems.

 * VHE systems use EL2 timers and configure EL1 timers in kvm_timer_init_vhe().

 Allow physical timer/counter access for the host */

/*

 * Should only be called on non-VHE systems.

 * VHE systems use EL2 timers and configure EL1 timers in kvm_timer_init_vhe().

	/*

	 * Disallow physical timer access for the guest

	 * Physical counter access is allowed

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 - Google LLC

 * Author: David Brazdil <dbrazdil@google.com>

/*

 * nVHE copy of data structures tracking available CPU cores.

 * Only entries for CPUs that were online at KVM init are populated.

 * Other CPUs should not be allowed to boot because their features were

 * not checked against the finalized system capabilities.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google LLC

 * Author: Quentin Perret <qperret@google.com>

/*

 * Index the hyp_vmemmap to find a potential buddy page, but make no assumption

 * about its current state.

 *

 * Example buddy-tree for a 4-pages physically contiguous pool:

 *

 *                 o : Page 3

 *                /

 *               o-o : Page 2

 *              /

 *             /   o : Page 1

 *            /   /

 *           o---o-o : Page 0

 *    Order  2   1 0

 *

 * Example of requests on this pool:

 *   __find_buddy_nocheck(pool, page 0, order 0) => page 1

 *   __find_buddy_nocheck(pool, page 0, order 1) => page 2

 *   __find_buddy_nocheck(pool, page 1, order 0) => page 0

 *   __find_buddy_nocheck(pool, page 2, order 0) => page 3

	/*

	 * Don't return a page outside the pool range -- it belongs to

	 * something else and may not be mapped in hyp_vmemmap.

 Find a buddy page currently available for allocation */

/*

 * Pages that are available for allocation are tracked in free-lists, so we use

 * the pages themselves to store the list nodes to avoid wasting space. As the

 * allocator always returns zeroed pages (which are zeroed on the hyp_put_page()

 * path to optimize allocation speed), we also need to clean-up the list node in

 * each page when we take it out of the list.

	/*

	 * Only the first struct hyp_page of a high-order page (otherwise known

	 * as the 'head') should have p->order set. The non-head pages should

	 * have p->order = HYP_NO_ORDER. Here @p may no longer be the head

	 * after coallescing, so make sure to mark it HYP_NO_ORDER proactively.

 Take the buddy out of its list, and coallesce with @p */

 Mark the new head, and insert it */

		/*

		 * The buddy of order n - 1 currently has HYP_NO_ORDER as it

		 * is covered by a higher-level page (whose head is @p). Use

		 * __find_buddy_nocheck() to find it and inject it in the

		 * free_list[n - 1], effectively splitting @p in half.

/*

 * Changes to the buddy tree and page refcounts must be done with the hyp_pool

 * lock held. If a refcount change requires an update to the buddy tree (e.g.

 * hyp_put_page()), both operations must be done within the same critical

 * section to guarantee transient states (e.g. a page with null refcount but

 * not yet attached to a free list) can't be observed by well-behaved readers.

 Look for a high-enough-order page */

 Extract it from the tree at the right order */

 Init the vmemmap portion */

 Attach the unused pages to the buddy tree */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stubs for out-of-line function calls caused by re-using kernel

 * infrastructure at EL2.

 *

 * Copyright (C) 2020 - Google LLC

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 Clear pmscr in case of early return */

	/*

	 * At this point, we know that this CPU implements

	 * SPE and is available to the host.

	 * Check if the host is actually using it ?

 Yes; save the control register and disable data generation */

 Now drain all buffered data to memory */

 The host page table is installed, but not yet synchronised */

 Re-enable data generation */

 Check if the TRBE is enabled */

	/*

	 * Prohibit trace generation while we are in guest.

	 * Since access to TRFCR_EL1 is trapped, the guest can't

	 * modify the filtering set by the host.

 Drain the trace buffer to memory */

 Restore trace filter controls */

 Disable and flush SPE data generation */

 Disable and flush Self-Hosted Trace generation */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 Google LLC

 * Author: Quentin Perret <qperret@google.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2021 Google LLC

 * Author: Fuad Tabba <tabba@google.com>

/*

 * Copies of the host's CPU features registers holding sanitized values at hyp.

/*

 * Inject an unknown/undefined exception to an AArch64 guest while most of its

 * sysregs are live.

/*

 * Returns the restricted features values of the feature register based on the

 * limitations in restrict_fields.

 * A feature id field value of 0b0000 does not impose any restrictions.

 * Note: Use only for unsigned feature field values.

	/*

	 * According to the Arm Architecture Reference Manual, feature fields

	 * use increasing values to indicate increases in functionality.

	 * Iterate over the restricted feature fields and calculate the minimum

	 * unsigned value between the one supported by the system, and what the

	 * value is being restricted to.

/*

 * Functions that return the value of feature id registers for protected VMs

 * based on allowed features, system features, and KVM support.

 Spectre and Meltdown mitigation in KVM */

	/*

	 * No support for Scalable Vectors, therefore, hyp has no sanitized

	 * copy of the feature id register.

	/*

	 * No support for debug, including breakpoints, and watchpoints,

	 * therefore, pKVM has no sanitized copy of the feature id register.

	/*

	 * No support for debug, therefore, hyp has no sanitized copy of the

	 * feature id register.

	/*

	 * No support for implementation defined features, therefore, hyp has no

	 * sanitized copy of the feature id register.

	/*

	 * No support for implementation defined features, therefore, hyp has no

	 * sanitized copy of the feature id register.

 Read a sanitized cpufeature ID register by its encoding */

		/*

		 * Should never happen because all cases are covered in

		 * pvm_sys_reg_descs[].

 Handler to RAZ/WI sysregs */

/*

 * Accessor for AArch32 feature id registers.

 *

 * The value of these registers is "unknown" according to the spec if AArch32

 * isn't supported.

	/*

	 * No support for AArch32 guests, therefore, pKVM has no sanitized copy

	 * of AArch32 feature id registers.

/*

 * Accessor for AArch64 feature id registers.

 *

 * If access is allowed, set the regval to the protected VM's view of the

 * register and return true.

 * Otherwise, inject an undefined exception and return false.

 pVMs only support GICv3. 'nuf said. */

 Mark the specified system register as an AArch32 feature id register. */

 Mark the specified system register as an AArch64 feature id register. */

 Mark the specified system register as Read-As-Zero/Write-Ignored */

 Mark the specified system register as not being handled in hyp. */

/*

 * Architected system registers.

 * Important: Must be sorted ascending by Op0, Op1, CRn, CRm, Op2

 *

 * NOTE: Anything not explicitly listed here is *restricted by default*, i.e.,

 * it will lead to injecting an exception into the guest.

 Cache maintenance by set/way operations are restricted. */

 Debug and Trace Registers are restricted. */

 AArch64 mappings of the AArch32 ID registers */

 CRm=1 */

 CRm=2 */

 CRm=3 */

 AArch64 ID registers */

 CRm=4 */

 Scalable Vector Registers are restricted. */

 Performance Monitoring Registers are restricted. */

 Limited Ordering Regions Registers are restricted. */

 Performance Monitoring Registers are restricted. */

 Activity Monitoring Registers are restricted. */

 Performance Monitoring Registers are restricted. */

/*

 * Checks that the sysreg table is unique and in-order.

 *

 * Returns 0 if the table is consistent, or 1 otherwise.

/*

 * Handler for protected VM MSR, MRS or System instruction execution.

 *

 * Returns true if the hypervisor has handled the exit, and control should go

 * back to the guest, or false if it hasn't, to be handled by the host.

 Undefined (RESTRICTED). */

 Handled by the host (HOST_HANDLED) */

 Handled by hyp: skip instruction if instructed to do so. */

/*

 * Handler for protected VM restricted exceptions.

 *

 * Inject an undefined exception into the guest and return true to indicate that

 * the hypervisor has handled the exit, and control should go back to the guest.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 VHE specific context */

	/*

	 * With VHE (HCR.E2H == 1), accesses to CPACR_EL1 are routed to

	 * CPTR_EL2. In general, CPACR_EL1 has the same layout as CPTR_EL2,

	 * except for some missing controls, such as TAM.

	 * In this case, CPTR_EL2.TAM has the same position with or without

	 * VHE (HCR.E2H == 1) which allows us to use here the CPTR_EL2.TAM

	 * shift value for trapping the AMU accesses.

 kernel exception vectors */

	/*

	 * ARM errata 1165522 and 1530923 require the actual execution of the

	 * above before we can switch to the EL2/EL0 translation regime used by

	 * the host.

 Switch to the guest for VHE systems running in EL2 */

	/*

	 * ARM erratum 1165522 requires us to configure both stage 1 and

	 * stage 2 translation for the guest context before we clear

	 * HCR_EL2.TGE.

	 *

	 * We have already configured the guest's stage 1 translation in

	 * kvm_vcpu_load_sysregs_vhe above.  We must now call

	 * __load_stage2 before __activate_traps, because

	 * __load_stage2 configures stage 2 translation, and

	 * __activate_traps clear HCR_EL2.TGE (among other things).

 Jump in the fire! */

 And we're baaack! */

	/*

	 * Having IRQs masked via PMR when entering the guest means the GIC

	 * will not signal the CPU of interrupts of lower priority, and the

	 * only way to get out will be via guest exceptions.

	 * Naturally, we want to avoid this.

	 *

	 * local_daif_mask() already sets GIC_PRIO_PSR_I_SET, we just need a

	 * dsb to ensure the redistributor is forwards EL2 IRQs to the CPU.

	/*

	 * local_daif_restore() takes care to properly restore PSTATE.DAIF

	 * and the GIC PMR if the host is using IRQ priorities.

	/*

	 * When we exit from the guest we change a number of CPU configuration

	 * parameters, such as traps.  Make sure these changes take effect

	 * before running the host or additional guests.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

		/*

		 * For CPUs that are affected by ARM errata 1165522 or 1530923,

		 * we cannot trust stage-1 to be in a correct state at that

		 * point. Since we do not want to force a full load of the

		 * vcpu state, we prevent the EL1 page-table walker to

		 * allocate new TLBs. This is done by setting the EPD bits

		 * in the TCR_EL1 register. We also need to prevent it to

		 * allocate IPA->PA walks, so we enable the S1 MMU...

	/*

	 * With VHE enabled, we have HCR_EL2.{E2H,TGE} = {1,1}, and

	 * most TLB operations target EL2/EL0. In order to affect the

	 * guest TLBs (EL1/EL0), we need to change one of these two

	 * bits. Changing E2H is impossible (goodbye TTBR1_EL2), so

	 * let's flip TGE before executing the TLB operation.

	 *

	 * ARM erratum 1165522 requires some special handling (again),

	 * as we need to make sure both stages of translation are in

	 * place before clearing TGE. __load_stage2() already

	 * has an ISB in order to deal with this.

	/*

	 * We're done with the TLB operation, let's restore the host's

	 * view of HCR_EL2.

 Restore the registers to what they were */

 Switch to requested VMID */

	/*

	 * We could do so much better if we had the VA as well.

	 * Instead, we invalidate Stage-2 for this IPA, and the

	 * whole of Stage-1. Weep...

	/*

	 * We have to ensure completion of the invalidation at Stage-2,

	 * since a table walk on another CPU could refill a TLB with a

	 * complete (S1 + S2) walk based on the old Stage-2 mapping if

	 * the Stage-1 invalidation happened first.

 Switch to requested VMID */

 Switch to requested VMID */

	/*

	 * VIPT and PIPT caches are not affected by VMID, so no maintenance

	 * is necessary across a VMID rollover.

	 *

	 * VPIPT caches constrain lookup and maintenance to the active VMID,

	 * so we need to invalidate lines with a stale VMID to avoid an ABA

	 * race after multiple rollovers.

	 *

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

/*

 * VHE: Host and guest must save mdscr_el1 and sp_el0 (and the PC and

 * pstate, which are handled as part of the el2 return state) on every

 * switch (sp_el0 is being dealt with in the assembly code).

 * tpidr_el0 and tpidrro_el0 only need to be switched when going

 * to host userspace or a different VCPU.  EL1 registers only need to be

 * switched when potentially going to run a different VCPU.  The latter two

 * classes are handled as part of kvm_arch_vcpu_load and kvm_arch_vcpu_put.

/**

 * kvm_vcpu_load_sysregs_vhe - Load guest system registers to the physical CPU

 *

 * @vcpu: The VCPU pointer

 *

 * Load system registers that do not affect the host's execution, for

 * example EL1 system registers on a VHE system where the host kernel

 * runs at EL2.  This function is called from KVM's vcpu_load() function

 * and loading system register state early avoids having to load them on

 * every entry to the VM.

	/*

	 * Load guest EL1 and user state

	 *

	 * We must restore the 32-bit state before the sysregs, thanks

	 * to erratum #852523 (Cortex-A57) or #853709 (Cortex-A72).

/**

 * kvm_vcpu_put_sysregs_vhe - Restore host system registers to the physical CPU

 *

 * @vcpu: The VCPU pointer

 *

 * Save guest system registers that do not affect the host's execution, for

 * example EL1 system registers on a VHE system where the host kernel

 * runs at EL2.  This function is called from KVM's vcpu_put() function

 * and deferring saving system register state until we're no longer running the

 * VCPU avoids having to save them on every exit from the VM.

 Restore host user state */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012-2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2015 - ARM Ltd

 * Author: Marc Zyngier <marc.zyngier@arm.com>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/context.c

 *

 * Copyright (C) 2002-2003 Deep Blue Solutions Ltd, all rights reserved.

 * Copyright (C) 2012 ARM Ltd.

 Get the ASIDBits supported by the current CPU */

 Check if the current cpu's ASIDBits is compatible with asid_bits */

		/*

		 * We cannot decrease the ASID size at runtime, so panic if we support

		 * fewer ASID bits than the boot CPU.

	/*

	 * In case of KPTI kernel/user ASIDs are allocated in

	 * pairs, the bottom bit distinguishes the two: if it

	 * is set, then the ASID will map only userspace. Thus

	 * mark even as reserved for kernel.

 Update the list of reserved ASIDs and the ASID bitmap. */

		/*

		 * If this CPU has already been through a

		 * rollover, but hasn't run another task in

		 * the meantime, we must preserve its reserved

		 * ASID, as this is the only trace we have of

		 * the process it is still running.

	/*

	 * Queue a TLB invalidation for each CPU to perform on next

	 * context-switch

	/*

	 * Iterate over the set of reserved ASIDs looking for a match.

	 * If we find one, then we can update our mm to use newasid

	 * (i.e. the same ASID in the current generation) but we can't

	 * exit the loop early, since we need to ensure that all copies

	 * of the old ASID are updated to reflect the mm. Failure to do

	 * so could result in us missing the reserved ASID in a future

	 * generation.

		/*

		 * If our current ASID was active during a rollover, we

		 * can continue to use it and this was just a false alarm.

		/*

		 * If it is pinned, we can keep using it. Note that reserved

		 * takes priority, because even if it is also pinned, we need to

		 * update the generation into the reserved_asids.

		/*

		 * We had a valid ASID in a previous life, so try to re-use

		 * it if possible.

	/*

	 * Allocate a free ASID. If we can't find one, take a note of the

	 * currently active ASIDs and mark the TLBs as requiring flushes.  We

	 * always count from ASID #2 (index 1), as we use ASID #0 when setting

	 * a reserved TTBR0 for the init_mm and we allocate ASIDs in even/odd

	 * pairs.

 We're out of ASIDs, so increment the global generation count */

 We have more ASIDs than CPUs, so this will always succeed */

	/*

	 * The memory ordering here is subtle.

	 * If our active_asids is non-zero and the ASID matches the current

	 * generation, then we update the active_asids entry with a relaxed

	 * cmpxchg. Racing with a concurrent rollover means that either:

	 *

	 * - We get a zero back from the cmpxchg and end up waiting on the

	 *   lock. Taking the lock synchronises with the rollover and so

	 *   we are forced to see the updated generation.

	 *

	 * - We get a valid ASID back from the cmpxchg, which means the

	 *   relaxed xchg in flush_context will treat us as reserved

	 *   because atomic RmWs are totally ordered for a given location.

 Check that our ASID belongs to the current generation. */

	/*

	 * Defer TTBR0_EL1 setting for user threads to uaccess_enable() when

	 * emulating PAN.

		/*

		 * We went through one or more rollover since that ASID was

		 * used. Ensure that it is still valid, or generate a new one.

 Set the equivalent of USER_ASID_BIT */

 Errata workaround post TTBRx_EL1 update. */

 Skip CNP for the reserved ASID */

 SW PAN needs a copy of the ASID in TTBR0 for entry */

 Set ASID in TTBR1 since TCR.A1 is set */

	/*

	 * Expect allocation after rollover to fail if we don't have at least

	 * one more ASID than CPUs. ASID #0 is reserved for init_mm.

	/*

	 * There must always be an ASID available after rollover. Ensure that,

	 * even if all CPUs have a reserved ASID and the maximum number of ASIDs

	 * are pinned, there still is at least one empty slot in the ASID map.

	/*

	 * We cannot call set_reserved_asid_bits() here because CPU

	 * caps are not finalized yet, so it is safer to assume KPTI

	 * and reserve kernel ASID's from beginning.

 SPDX-License-Identifier: GPL-2.0

/*

 * Transitional page tables for kexec and hibernate

 *

 * This file derived from: arch/arm64/kernel/hibernate.c

 *

 * Copyright (c) 2021, Microsoft Corporation.

 * Pasha Tatashin <pasha.tatashin@soleen.com>

 *

/*

 * Transitional tables are used during system transferring from one world to

 * another: such as during hibernate restore, and kexec reboots. During these

 * phases one cannot rely on page table not being overwritten. This is because

 * hibernate and kexec can overwrite the current page tables during transition.

		/*

		 * Resume will overwrite areas that may be marked

		 * read only (code, rodata). Clear the RDONLY bit from

		 * the temporary mappings we use during restore.

		/*

		 * debug_pagealloc will removed the PTE_VALID bit if

		 * the page isn't in use by the resume kernel. It may have

		 * been in use by the original kernel, in which case we need

		 * to put it back in our copy to do the restore.

		 *

		 * Before marking this entry valid, check the pfn should

		 * be mapped.

/*

 * Create trans_pgd and copy linear map.

 * info:	contains allocator and its argument

 * dst_pgdp:	new page table that is created, and to which map is copied.

 * start:	Start of the interval (inclusive).

 * end:		End of the interval (exclusive).

 *

 * Returns 0 on success, and -ENOMEM on failure.

/*

 * The page we want to idmap may be outside the range covered by VA_BITS that

 * can be built using the kernel's p?d_populate() helpers. As a one off, for a

 * single page, we build these page tables bottom up and just assume that will

 * need the maximum T0SZ.

 *

 * Returns 0 on success, and -ENOMEM on failure.

 * On success trans_ttbr0 contains page table with idmapped page, t0sz is set to

 * maximum T0SZ for this page.

/*

 * Create a copy of the vector table so we can call HVC_SET_VECTORS or

 * HVC_SOFT_RESTART from contexts where the table may be overwritten.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * This file contains kasan initialization code for ARM64.

 *

 * Copyright (c) 2015 Samsung Electronics Co., Ltd.

 * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>

/*

 * The p*d_populate functions call virt_to_phys implicitly so they can't be used

 * directly on kernel symbols (bm_p*d). All the early functions are called too

 * early to use lm_alias so __p*d_populate functions must be used to populate

 * with the physical address from __pa_symbol.

 The early shadow maps everything to a single page of zeroes */

 Set up full kasan mappings, ensuring that the mapped pages are zeroed */

/*

 * Copy the current shadow region into a new pgdir.

	/*

	 * Remove references to kasan page tables from

	 * swapper_pg_dir. pgd_clear() can't be used

	 * here because it's nop on 2,3-level pagetable setups

	/*

	 * We are going to perform proper setup of shadow memory.

	 * At first we should unmap early shadow (clear_pgds() call below).

	 * However, instrumented code couldn't execute without shadow memory.

	 * tmp_pg_dir used to keep early shadow mapped until full shadow

	 * setup will be finished.

	/*

	 * KAsan may reuse the contents of kasan_early_shadow_pte directly,

	 * so we should make sure that it maps the zero page read-only.

 CONFIG_KASAN_SW_TAGS also requires kasan_init_sw_tags(). */

 CONFIG_KASAN_GENERIC || CONFIG_KASAN_SW_TAGS */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

 * Debug helper to dump the current kernel pagetables of the system

 * so that we can see what the various memory ranges are set to.

 *

 * Derived from x86 and arm implementation:

 * (C) Copyright 2008 Intel Corporation

 *

 * Author: Arjan van de Ven <arjan@linux.intel.com>

 PAGE_END */,		"Linear Mapping end" },

 KASAN_SHADOW_START */,	"Kasan shadow start" },

/*

 * The page dumper groups page table entries of the same type into a single

 * description. It uses pg_state to track the range information while

 * iterating over the pte entries. When the continuity is broken it then

 * dumps out a description of the range.

 pgd */

 p4d */

 pud */

 pmd */

 pte */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * PGD allocation/freeing

 *

 * Copyright (C) 2012 ARM Ltd.

 * Author: Catalin Marinas <catalin.marinas@arm.com>

	/*

	 * With 52-bit physical addresses, the architecture requires the

	 * top-level table to be aligned to at least 64 bytes.

	/*

	 * Naturally aligned pgds required by the architecture.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/mmap.c

 *

 * Copyright (C) 2012 ARM Ltd.

/*

 * You really shouldn't be using read() or write() on /dev/mem.  This might go

 * away in the future.

	/*

	 * Check whether addr is covered by a memory region without the

	 * MEMBLOCK_NOMAP attribute, and whether that region covers the

	 * entire range. In theory, this could lead to false negatives

	 * if the range is covered by distinct but adjacent memory regions

	 * that only differ in other attributes. However, few of such

	 * attributes have been defined, and it is debatable whether it

	 * follows that /dev/mem read() calls should be able traverse

	 * such boundaries.

/*

 * Do not allow /dev/mem mappings beyond the supported physical range.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (c) 2014, The Linux Foundation. All rights reserved.

/*

 * This function assumes that the range is mapped with PAGE_SIZE pages.

	/*

	 * Kernel VA mappings are always live, and splitting live section

	 * mappings into page mappings may cause TLB conflicts. This means

	 * we have to ensure that changing the permission bits of the range

	 * we are operating on does not result in such splitting.

	 *

	 * Let's restrict ourselves to mappings created by vmalloc (or vmap).

	 * Those are guaranteed to consist entirely of page mappings, and

	 * splitting is never needed.

	 *

	 * So check whether the [addr, addr + size) interval is entirely

	 * covered by precisely one VM area that has the VM_ALLOC flag set.

	/*

	 * If we are manipulating read-only permissions, apply the same

	 * change to the linear mapping of the pages that back this VM area.

	/*

	 * Get rid of potentially aliasing lazily unmapped vm areas that may

	 * have permissions set that deviate from the ones we are setting here.

 CONFIG_DEBUG_PAGEALLOC */

/*

 * This function is used to determine if a linear map page has been marked as

 * not-valid. Walk the page table and check the PTE_VALID bit. This is based

 * on kern_addr_valid(), which almost does what we need.

 *

 * Because this is only called on the kernel linear map,  p?d_sect() implies

 * p?d_present(). When debug_pagealloc is enabled, sections mappings are

 * disabled.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * This is bounds checking against the kernel image only.

	 * __pa_symbol should only be used on kernel symbol addresses.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/init.c

 *

 * Copyright (C) 1995-2005 Russell King

 * Copyright (C) 2012 ARM Ltd.

/*

 * We need to be able to catch inadvertent references to memstart_addr

 * that occur (potentially in generic code) before arm64_memblock_init()

 * executes, which assigns it its actual value. So use a default value

 * that cannot be mistaken for a real physical address.

/*

 * If the corresponding config options are enabled, we create both ZONE_DMA

 * and ZONE_DMA32. By default ZONE_DMA covers the 32-bit addressable memory

 * unless restricted on specific platforms (e.g. 30-bit on Raspberry Pi 4).

 * In such case, ZONE_DMA32 covers the rest of the 32-bit addressable memory,

 * otherwise it is empty.

/*

 * reserve_crashkernel() - reserves memory for crash kernel

 *

 * This function reserves memory area given in "crashkernel=" kernel command

 * line parameter. The memory reserved is used by dump capture kernel when

 * primary kernel is crashing.

 no crashkernel= or invalid value specified */

 User specifies base address explicitly. */

 Current arm64 boot protocol requires 2MB alignment */

	/*

	 * The crashkernel memory will be removed from the kernel linear

	 * map. Inform kmemleak so that it won't try to access it.

 CONFIG_KEXEC_CORE */

/*

 * Return the maximum physical address for a zone accessible by the given bits

 * limit. If DRAM starts above 32-bit, expand the zone to the maximum

 * available memory, otherwise cap it at 32-bit.

 avoid false positives for bogus PFNs, see comment in pfn_valid() */

/*

 * Limit the memory size that was specified via FDT.

	/*

	 * Corner case: 52-bit VA capable systems running KVM in nVHE mode may

	 * be limited in their ability to support a linear map that exceeds 51

	 * bits of VA space, depending on the placement of the ID map. Given

	 * that the placement of the ID map may be randomized, let's simply

	 * limit the kernel's linear map to 51 bits as well if we detect this

	 * configuration.

 Remove memory above our supported physical address size */

	/*

	 * Select a suitable value for the base of physical memory.

	/*

	 * Remove the memory that we will not be able to cover with the

	 * linear mapping. Take care not to clip the kernel which may be

	 * high in memory.

 ensure that memstart_addr remains sufficiently aligned */

	/*

	 * If we are running with a 52-bit kernel VA config on a system that

	 * does not support it, we have to place the available physical

	 * memory in the 48-bit addressable part of the linear region, i.e.,

	 * we have to move it upward. Since memstart_addr represents the

	 * physical address of PAGE_OFFSET, we have to *subtract* from it.

	/*

	 * Apply the memory limit if it was set. Since the kernel may be loaded

	 * high up in memory, add back the kernel region that must be accessible

	 * via the linear mapping.

		/*

		 * Add back the memory we just removed if it results in the

		 * initrd to become inaccessible via the linear mapping.

		 * Otherwise, this is a no-op

		/*

		 * We can only add back the initrd memory if we don't end up

		 * with more memory than we can address via the linear mapping.

		 * It is up to the bootloader to position the kernel and the

		 * initrd reasonably close to each other (i.e., within 32 GB of

		 * each other) so that all granule/#levels combinations can

		 * always access both.

 clear MEMBLOCK_ flags */

		/*

		 * If the size of the linear region exceeds, by a sufficient

		 * margin, the size of the region that the physical memory can

		 * span, randomize the linear region as well.

	/*

	 * Register the kernel text, kernel data, initrd, and initial

	 * pagetables with memblock.

 the generic initrd code expects virtual addresses */

	/*

	 * must be done after arch_numa_init() which calls numa_init() to

	 * initialize node_online_map that gets used in hugetlb_cma_reserve()

	 * while allocating required CMA size across online nodes.

	/*

	 * sparse_init() tries to allocate memory from memblock, so must be

	 * done after the fixed reservations

	/*

	 * Reserve the CMA area after arm64_dma_phys_limit was initialised.

	/*

	 * request_standard_resources() depends on crashkernel's memory being

	 * reserved, so do it here.

/*

 * mem_init() marks the free areas in the mem_map and tells us how much memory

 * is free.  This is done after various parts of the system have claimed their

 * memory after the kernel image.

 this will put all unused low memory onto the freelists */

	/*

	 * Check boundaries twice: Some fundamental inconsistencies can be

	 * detected at build time already.

	/*

	 * Selected page table levels should match when derived from

	 * scratch using the virtual address range and page size.

		/*

		 * On a machine this small we won't get anywhere without

		 * overcommit, so turn it on by default.

	/*

	 * Unmap the __init region but leave the VM area in place. This

	 * prevents the region from being reused for kernel modules, which

	 * is not supported by kallsyms.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/flush.c

 *

 * Copyright (C) 1995-2002 Russell King

 * Copyright (C) 2012 ARM Ltd.

		/*

		 * Don't issue kick_all_cpus_sync() after I-cache invalidation

		 * for user mappings.

/*

 * Copy user data from/to a page which is mapped into a different processes

 * address space.  Really, we want to allow our "user space" model to handle

 * this.

/*

 * This function is called when a page has been modified by the kernel. Mark

 * it as dirty for later flushing when mapped in user space (if executable,

 * see __sync_icache_dcache).

/*

 * Additional functions defined in assembly.

 Ensure order against any prior non-cacheable writes */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/fault.c

 *

 * Copyright (C) 1995  Linus Torvalds

 * Copyright (C) 1995-2004 Russell King

 * Copyright (C) 2012 ARM Ltd.

 Either init_pg_dir or swapper_pg_dir */

/*

 * Dump out the page tables associated with 'addr' in the currently active mm.

 TTBR0 */

 TTBR1 */

/*

 * This function sets the access flags (dirty, accessed), as well as write

 * permission, and only to a more permissive setting.

 *

 * It needs to cope with hardware update of the accessed/dirty state by other

 * agents in the system and can safely skip the __sync_icache_dcache() call as,

 * like set_pte_at(), the PTE is never changed from no-exec to exec here.

 *

 * Returns whether or not the PTE actually changed.

 only preserve the access flags and write permission */

	/*

	 * Setting the flags must be done atomically to avoid racing with the

	 * hardware update of the access/dirty state. The PTE_RDONLY bit must

	 * be set to the most permissive (lowest value) of *ptep and entry

	 * (calculated as: a & b == ~(~a | ~b)).

 Invalidate a stale read-only entry */

	/*

	 * If we now have a valid translation, treat the translation fault as

	 * spurious.

	/*

	 * If we got a different type of fault from the AT instruction,

	 * treat the translation fault as spurious.

	/*

	 * SAS bits aren't set for all faults reported in EL1, so we can't

	 * find out access size.

 Tag faults aren't enabled without CONFIG_KASAN_HW_TAGS. */

	/*

	 * Disable MTE Tag Checking on the local CPU for the current EL.

	 * It will be done lazily on the other CPUs when they will hit a

	 * tag fault.

	/*

	 * Are we prepared to handle this kernel fault?

	 * We are almost certainly not prepared to handle instruction faults.

	/*

	 * If the faulting address is in the kernel, we must sanitize the ESR.

	 * From userspace's point of view, kernel-only mappings don't exist

	 * at all, so we report them as level 0 translation faults.

	 * (This is not quite the way that "no mapping there at all" behaves:

	 * an alignment fault not caused by the memory type would take

	 * precedence over translation fault for a real access to empty

	 * space. Unfortunately we can't easily distinguish "alignment fault

	 * not caused by memory type" from "alignment fault caused by memory

	 * type", so we ignore this wrinkle and just return the translation

	 * fault.)

			/*

			 * These bits provide only information about the

			 * faulting instruction, which userspace knows already.

			 * We explicitly clear bits which are architecturally

			 * RES0 in case they are given meanings in future.

			 * We always report the ESR as if the fault was taken

			 * to EL1 and so ISV and the bits in ISS[23:14] are

			 * clear. (In fact it always will be a fault to EL1.)

			/*

			 * Claim a level 0 translation fault.

			 * All other bits are architecturally RES0 for faults

			 * reported with that DFSC value, so we clear them.

			/*

			 * This should never happen (entry.S only brings us

			 * into this code for insn and data aborts from a lower

			 * exception level). Fail safe by not providing an ESR

			 * context record at all.

	/*

	 * If we are in kernel mode at this point, we have no context to

	 * handle this fault with.

	/*

	 * Ok, we have a good vm_area for this memory access, so we can handle

	 * it.

	/*

	 * Check that the permissions on the VMA allow for the fault which

	 * occurred.

/*

 * Note: not valid for EL1 DC IVAC, but we never use that such that it

 * should fault. EL0 cannot issue DC IVAC (undef).

	/*

	 * If we're in an interrupt or have no user context, we must not take

	 * the fault.

	/*

	 * vm_flags tells us what bits we must have in vma->vm_flags

	 * for the fault to be benign, __do_page_fault() would check

	 * vma->vm_flags & vm_flags and returns an error if the

	 * intersection is empty

 It was exec fault */

 It was write fault */

 It was read fault */

 Write implies read */

 If EPAN is absent then exec implies read */

	/*

	 * As per x86, we may deadlock here. However, since the kernel only

	 * validly references user space from well defined areas of the code,

	 * we can bug out early if this is from code which shouldn't.

		/*

		 * The above mmap_read_trylock() might have succeeded in which

		 * case, we'll have missed the might_sleep() from down_read().

 Quick path to respond to signals */

	/*

	 * Handle the "normal" (no error) case first.

	/*

	 * If we are in kernel mode at this point, we have no context to

	 * handle this fault with.

		/*

		 * We ran out of memory, call the OOM killer, and return to

		 * userspace (which will retry the fault, or kill us if we got

		 * oom-killed).

		/*

		 * We had some memory, but were unable to successfully fix up

		 * this page fault.

		/*

		 * Something tried to access memory that isn't in our memory

		 * map.

 "fault" */

		/*

		 * APEI claimed this as a firmware-first notification.

		 * Some processing deferred to task_work before ret_to_user().

		/*

		 * The architecture specifies that the tag bits of FAR_EL1 are

		 * UNKNOWN for synchronous external aborts. Mask them out now

		 * so that userspace doesn't see them.

	/*

	 * The architecture specifies that bits 63:60 of FAR_EL1 are UNKNOWN

	 * for tag check faults. Set them to corresponding bits in the untagged

	 * address.

 Reserved when RAS is implemented

 Reserved when RAS is implemented

 Reserved when RAS is implemented

 Reserved when RAS is implemented

 Reserved when RAS is implemented

	/*

	 * At this point we have an unrecognized fault type whose tag bits may

	 * have been defined as UNKNOWN. Therefore we only expose the untagged

	 * address to the signal handler.

/*

 * __refdata because early_brk64 is __init, but the reference to it is

 * clobbered at arch_initcall time.

 * See traps.c and debug-monitors.c:debug_traps_init().

/*

 * In debug exception context, we explicitly disable preemption despite

 * having interrupts disabled.

 * This serves two purposes: it makes it much less likely that we would

 * accidentally schedule in exception context and it will force a warning

 * if we somehow manage to schedule by accident.

 This code is a bit fragile.  Test it. */

/*

 * Used during anonymous page fault handling.

	/*

	 * If the page is mapped with PROT_MTE, initialise the tags at the

	 * point of allocation and page zeroing as this is usually faster than

	 * separate DC ZVA and STGM.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/mm/hugetlbpage.c

 *

 * Copyright (C) 2013 Linaro Ltd.

 *

 * Based on arch/x86/mm/hugetlbpage.c.

/*

 * HugeTLB Support Matrix

 *

 * ---------------------------------------------------

 * | Page Size | CONT PTE |  PMD  | CONT PMD |  PUD  |

 * ---------------------------------------------------

 * |     4K    |   64K    |   2M  |    32M   |   1G  |

 * |    16K    |    2M    |  32M  |     1G   |       |

 * |    64K    |    2M    | 512M  |    16G   |       |

 * ---------------------------------------------------

/*

 * Reserve CMA areas for the largest supported gigantic

 * huge page when requested. Any other smaller gigantic

 * huge pages could still be served from those areas.

	/*

	 * HugeTLB CMA reservation is required for gigantic

	 * huge pages which could not be allocated via the

	 * page allocator. Just warn if there is any change

	 * breaking this assumption.

 CONFIG_CMA */

/*

 * Select all bits except the pfn

/*

 * Changing some bits of contiguous entries requires us to follow a

 * Break-Before-Make approach, breaking the whole contiguous set

 * before we can change any entries. See ARM DDI 0487A.k_iss10775,

 * "Misprogramming of the Contiguous bit", page D4-1762.

 *

 * This helper performs the break step.

		/*

		 * If HW_AFDBM is enabled, then the HW could turn on

		 * the dirty or accessed bit for any page in the set,

		 * so check them all.

/*

 * Changing some bits of contiguous entries requires us to follow a

 * Break-Before-Make approach, breaking the whole contiguous set

 * before we can change any entries. See ARM DDI 0487A.k_iss10775,

 * "Misprogramming of the Contiguous bit", page D4-1762.

 *

 * This helper performs the break step for use cases where the

 * original pte is not needed.

	/*

	 * Code needs to be expanded to handle huge swap and migration

	 * entries. Needed for HUGETLB and MEMORY_FAILURE.

		/*

		 * Note that if this code were ever ported to the

		 * 32-bit arm platform then it will cause trouble in

		 * the case where CONFIG_HIGHPTE is set, since there

		 * will be no pte_unmap() to correspond with this

		 * pte_alloc_map().

 hugepage or swap? */

 table; check the next level */

/*

 * huge_ptep_set_access_flags will update access flags (dirty, accesssed)

 * and write permission.

 *

 * For a contiguous huge pte range we need to check whether or not write

 * permission has to change only on the first pte in the set. Then for

 * all the contiguous ptes we need to check whether or not there is a

 * discrepancy between dirty or young.

 Make sure we don't lose the dirty or young state */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/copypage.c

 *

 * Copyright (C) 2002 Deep Blue Solutions Ltd, All Rights Reserved.

 * Copyright (C) 2012 ARM Ltd.

		/*

		 * We need smp_wmb() in between setting the flags and clearing the

		 * tags because if another thread reads page->flags and builds a

		 * tagged address out of it, there is an actual dependency to the

		 * memory access, but on the current thread we do not guarantee that

		 * the new page->flags are visible before the tags were updated.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2012 ARM Ltd.

 * Author: Catalin Marinas <catalin.marinas@arm.com>

 SPDX-License-Identifier: GPL-2.0

/*

 * Based on arch/arm/mm/extable.c

 SPDX-License-Identifier: GPL-2.0-only

 tags granule is 16 bytes, 2 tags stored per byte */

 page_private contains the swap entry.val set in do_swap_page */

 Entry is being replaced, free the old entry */

	/*

	 * We need smp_wmb() in between setting the flags and clearing the

	 * tags because if another thread reads page->flags and builds a

	 * tagged address out of it, there is an actual dependency to the

	 * memory access, but on the current thread we do not guarantee that

	 * the new page->flags are visible before the tags were updated.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/ioremap.c

 *

 * (C) Copyright 1995 1996 Linus Torvalds

 * Hacked for ARM by Phil Blundell <philb@gnu.org>

 * Hacked to allow all architectures to build, and various cleanups

 * by Russell King

 * Copyright (C) 2012 ARM Ltd.

	/*

	 * Page align the mapping address and size, taking account of any

	 * offset.

	/*

	 * Don't allow wraparound, zero size or outside PHYS_MASK.

	/*

	 * Don't allow RAM to be mapped.

	/*

	 * We could get an address outside vmalloc range in case

	 * of ioremap_cache() reusing a RAM mapping.

 For normal memory we already have a cacheable mapping. */

/*

 * Must be called after early_fixmap_init

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/mm/mmu.c

 *

 * Copyright (C) 1995-2005 Russell King

 * Copyright (C) 2012 ARM Ltd.

 assumes FEAT_HPDS is not used */

/*

 * Empty_zero_page is a special page that is used for zero-initialized data

 * and COW.

	/*

	 * We need dsb(ishst) here to ensure the page-table-walker sees

	 * our new entry before set_p?d() returns. The fixmap's

	 * flush_tlb_kernel_range() via clear_fixmap() does this for us.

	/*

	 * The FIX_{PGD,PUD,PMD} slots may be in active use, but the FIX_PTE

	 * slot will be free, so we can (ab)use the FIX_PTE slot to initialise

	 * any level of table.

	/*

	 * Implicit barriers also ensure the zeroed page is visible to the page

	 * table walker

	/*

	 * The following mapping attributes may be updated in live

	 * kernel mappings without the need for break-before-make.

 creating or taking down mappings is always safe */

 live contiguous mappings may not be manipulated at all */

 Transitioning from Non-Global to Global is unsafe */

	/*

	 * Changing the memory type between Normal and Normal-Tagged is safe

	 * since Tagged is considered a permission attribute from the

	 * mismatched attribute aliases perspective.

		/*

		 * After the PTE entry has been populated once, we

		 * only allow updates to the permission attributes.

 use a contiguous mapping if the range is suitably aligned */

 try section mapping first */

			/*

			 * After the PMD entry has been populated once, we

			 * only allow updates to the permission attributes.

	/*

	 * Check for initial section mappings in the pgd/pud.

 use a contiguous mapping if the range is suitably aligned */

		/*

		 * For 4K granule only, attempt to put down a 1GB block

			/*

			 * After the PUD entry has been populated once, we

			 * only allow updates to the permission attributes.

	/*

	 * If the virtual and physical address don't have the same offset

	 * within a page, we cannot map the region as the caller expects.

 Ensure the zeroed page is visible to the page table walker */

	/*

	 * Call proper page table ctor in case later we need to

	 * call core mm functions like apply_to_page_range() on

	 * this pre-allocated page table.

	 *

	 * We don't select ARCH_ENABLE_SPLIT_PMD_PTLOCK if pmd is

	 * folded, and if so pgtable_pmd_page_ctor() becomes nop.

/*

 * This function can only be used to modify existing table entries,

 * without allocating new levels of table. Note that this permits the

 * creation of new section or page entries.

 flush the TLBs after updating live kernel mappings */

	/*

	 * Remove the write permissions from the linear alias of .text/.rodata

	/*

	 * Proper parameter parsing is done by reserve_crashkernel(). We only

	 * need to know if the linear map has to avoid block mappings so that

	 * the crashkernel reservations can be unmapped later.

	/*

	 * Setting hierarchical PXNTable attributes on table entries covering

	 * the linear region is only possible if it is guaranteed that no table

	 * entries at any level are being shared between the linear region and

	 * the vmalloc region. Check whether this is true for the PGD level, in

	 * which case it is guaranteed to be true for all other levels as well.

	/*

	 * Take care not to create a writable alias for the

	 * read-only text and rodata sections of the kernel image.

	 * So temporarily mark them as NOMAP to skip mappings in

	 * the following for-loop

 map all the memory banks */

		/*

		 * The linear map must allow allocation tags reading/writing

		 * if MTE is present. Otherwise, it has the same attributes as

		 * PAGE_KERNEL.

	/*

	 * Map the linear alias of the [_stext, __init_begin) interval

	 * as non-executable now, and remove the write permission in

	 * mark_linear_text_alias_ro() below (which will be called after

	 * alternative patching has completed). This makes the contents

	 * of the region accessible to subsystems such as hibernate,

	 * but protects it from inadvertent modification or execution.

	 * Note that contiguous mappings cannot be remapped in this way,

	 * so we should avoid them here.

	/*

	 * mark .rodata as read only. Use __init_begin rather than __end_rodata

	 * to cover NOTES and EXCEPTION_TABLE.

 permit 'full' in addition to boolean options */

 The trampoline is always mapped and can therefore be global */

 Map only the text into the trampoline page table */

 Map both the text and data into the kernel page table */

/*

 * Open coded check for BTI, only for use to determine configuration

 * for early mappings for before the cpufeature code has run.

/*

 * Create fine-grained mappings for the kernel.

	/*

	 * External debuggers may need to write directly to the text

	 * mapping to install SW breakpoints. Allow this (only) when

	 * explicitly requested with rodata=off.

	/*

	 * If we have a CPU that supports BTI and a kernel built for

	 * BTI then mark the kernel executable text as guarded pages

	 * now so we don't have to rewrite the page tables later.

	/*

	 * Only rodata will be remapped with different permissions later on,

	 * all other segments are allowed to use contiguous mappings.

		/*

		 * The fixmap falls in a separate pgd to the kernel, and doesn't

		 * live in the carveout for the swapper_pg_dir. We can simply

		 * re-use the existing dir for the fixmap.

		/*

		 * The fixmap shares its top level pgd entry with the kernel

		 * mapping. This can really only occur when we are running

		 * with 16k/4 levels, so we can simply reuse the pud level

		 * entry instead.

/*

 * Check whether a kernel address is valid (derived from arch/x86/).

			/*

			 * One TLBI should be sufficient here as the PMD_SIZE

			 * range is mapped with a single block entry.

			/*

			 * One TLBI should be sufficient here as the PUD_SIZE

			 * range is mapped with a single block entry.

	/*

	 * altmap can only be used as vmemmap mapping backing memory.

	 * In case the backing memory itself is not being freed, then

	 * altmap is irrelevant. Warn about this inconsistency when

	 * encountered.

		/*

		 * This is just a sanity check here which verifies that

		 * pte clearing has been done by earlier unmap loops.

	/*

	 * Check whether we can free the pte page if the rest of the

	 * entries are empty. Overlap with other regions have been

	 * handled by the floor/ceiling check.

	/*

	 * Check whether we can free the pmd page if the rest of the

	 * entries are empty. Overlap with other regions have been

	 * handled by the floor/ceiling check.

	/*

	 * Check whether we can free the pud page if the rest of the

	 * entries are empty. Overlap with other regions have been

	 * handled by the floor/ceiling check.

 !ARM64_KERNEL_USES_PMD_MAPS */

 !ARM64_KERNEL_USES_PMD_MAPS */

 CONFIG_MEMORY_HOTPLUG */

/*

 * The p*d_populate functions call virt_to_phys implicitly so they can't be used

 * directly on kernel symbols (bm_p*d). This function is called too early to use

 * lm_alias so __p*d_populate functions must be used to populate with the

 * physical address from __pa_symbol.

		/*

		 * We only end up here if the kernel mapping and the fixmap

		 * share the top level pgd entry, which should only happen on

		 * 16k/4 levels configurations.

	/*

	 * The boot-ioremap range spans multiple pmds, for which

	 * we are not prepared:

/*

 * Unusually, this is also called in IRQ context (ghes_iounmap_irq) so if we

 * ever need to use IPIs for TLB broadcasting, then we're in trouble here.

	/*

	 * Check whether the physical FDT address is set and meets the minimum

	 * alignment requirement. Since we are relying on MIN_FDT_ALIGN to be

	 * at least 8 bytes so that we can always access the magic and size

	 * fields of the FDT header after mapping the first chunk, double check

	 * here if that is indeed the case.

	/*

	 * Make sure that the FDT region can be mapped without the need to

	 * allocate additional translation table pages, so that it is safe

	 * to call create_mapping_noalloc() this early.

	 *

	 * On 64k pages, the FDT will be mapped using PTEs, so we need to

	 * be in the same PMD as the rest of the fixmap.

	 * On 4k pages, we'll use section mappings for the FDT so we only

	 * have to be in the same PUD.

 map the first chunk so we can read the size from the header */

 Only allow permission changes for now */

 Only allow permission changes for now */

		/*

		 * Check for a wrap, it is possible because of randomized linear

		 * mapping the start physical address is actually bigger than

		 * the end physical address. In this case set start to zero

		 * because [0, end_linear_pa] range must still be able to cover

		 * all addressable physical addresses.

	/*

	 * Linear mapping region is the range [PAGE_OFFSET..(PAGE_END - 1)]

	 * accommodating both its ends but excluding PAGE_END. Max physical

	 * range which can be mapped inside this linear mapping range, must

	 * also be derived from its end points.

	/*

	 * KFENCE requires linear map to be mapped at page granularity, so that

	 * it is possible to protect/unprotect single pages in the KFENCE pool.

/*

 * This memory hotplug notifier helps prevent boot memory from being

 * inadvertently removed as it blocks pfn range offlining process in

 * __offline_pages(). Hence this prevents both offlining as well as

 * removal process for boot memory which is initially always online.

 * In future if and when boot memory could be removed, this notifier

 * should be dropped and free_hotplug_page_range() should handle any

 * reserved pages allocated during boot.

			/*

			 * Boot memory removal is not supported. Prevent

			 * it via blocking any attempted offline request

			 * for the boot memory and just report it.

			/*

			 * This should have never happened. Boot memory

			 * offlining should have been prevented by this

			 * very notifier. Probably some memory removal

			 * procedure might have changed which would then

			 * require further debug.

			/*

			 * Core memory hotplug does not process a return

			 * code from the notifier for MEM_OFFLINE events.

			 * The error condition has been reported. Return

			 * from here as if ignored.

/*

 * This ensures that boot memory sections on the platform are online

 * from early boot. Memory sections could not be prevented from being

 * offlined, unless for some reason they are not online to begin with.

 * This helps validate the basic assumption on which the above memory

 * event notifier works to prevent boot memory section offlining and

 * its possible removal.

	/*

	 * Scanning across all memblock might be expensive

	 * on some big memory systems. Hence enable this

	 * validation only with DEBUG_VM.

			/*

			 * All memory ranges in the system at this point

			 * should have been marked as early sections.

			/*

			 * Memory notifier mechanism here to prevent boot

			 * memory offlining depends on the fact that each

			 * early section memory on the system is initially

			 * online. Otherwise a given memory section which

			 * is already offline will be overlooked and can

			 * be removed completely. Call out such sections.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/process.c

 *

 * Original Copyright (C) 1995  Linus Torvalds

 * Copyright (C) 1996-2000 Russell King - Converted to ARM.

 * Copyright (C) 2012 ARM Ltd.

/*

 * Function pointers to optional machine specific functions

/*

 * Called by kexec, immediately prior to machine_kexec().

 *

 * This must completely disable all secondary CPUs; simply causing those CPUs

 * to execute e.g. a RAM-based pin loop is not sufficient. This allows the

 * kexec'd kernel to use any and all RAM as it sees fit, without having to

 * avoid any code or data used by any SW CPU pin loop. The CPU hotplug

 * functionality embodied in smpt_shutdown_nonboot_cpus() to achieve this.

/*

 * Halting simply requires that the secondary CPUs stop performing any

 * activity (executing tasks, handling interrupts). smp_send_stop()

 * achieves this.

/*

 * Power-off simply requires that the secondary CPUs stop performing any

 * activity (executing tasks, handling interrupts). smp_send_stop()

 * achieves this. When the system power is turned off, it will take all CPUs

 * with it.

/*

 * Restart requires that the secondary CPUs stop performing any activity

 * while the primary CPU resets the system. Systems with multiple CPUs must

 * provide a HW restart implementation, to ensure that all CPUs reset at once.

 * This is required so that any code running after reset on the primary CPU

 * doesn't have to co-ordinate with other CPUs to ensure they aren't still

 * executing pre-reset code, and using RAM that the primary CPU's code wishes

 * to use. Implementing such co-ordination would be essentially impossible.

 Disable interrupts first */

	/*

	 * UpdateCapsule() depends on the system being reset via

	 * ResetSystem().

 Now call the architecture specific reboot code. */

	/*

	 * Whoops - the architecture was unable to reboot.

		/*

		 * We need to ensure ordering between the shadow state and the

		 * hardware state, so that we don't corrupt the hardware state

		 * with a stale shadow state during context switch.

 We rely on the above assignment to initialize dst's thread_flags: */

	/*

	 * Detach src's sve_state (if any) from dst so that it does not

	 * get erroneously used or freed prematurely.  dst's sve_state

	 * will be allocated on demand later on if dst uses SVE.

	 * For consistency, also clear TIF_SVE here: this could be done

	 * later in copy_process(), but to avoid tripping up future

	 * maintainers it is best not to leave TIF_SVE and sve_state in

	 * an inconsistent state, even temporarily.

 clear any pending asynchronous tag fault raised by the parent */

	/*

	 * In case p was allocated the same task_struct pointer as some

	 * other recently-exited task, make sure p is disassociated from

	 * any cpu that may have run that now-exited task recently.

	 * Otherwise we could erroneously skip reloading the FPSIMD

	 * registers for p.

		/*

		 * Read the current TLS pointer from tpidr_el0 as it may be

		 * out-of-sync with the saved value.

		/*

		 * If a TLS pointer was passed to clone, use it for the new

		 * thread.

		/*

		 * A kthread has no context to ERET to, so ensure any buggy

		 * ERET is treated as an illegal exception return.

		 *

		 * When a user task is created from a kthread, childregs will

		 * be initialized by start_thread() or start_compat_thread().

	/*

	 * For the benefit of the unwinder, set up childregs->stackframe

	 * as the final frame for the new task.

/*

 * Force SSBS state on context-switch, since it may be lost after migrating

 * from a CPU which treats the bit as RES0 in a heterogeneous system.

	/*

	 * Nothing to do for kernel threads, but 'regs' may be junk

	 * (e.g. idle task) so check the flags and bail early.

	/*

	 * If all CPUs implement the SSBS extension, then we just need to

	 * context-switch the PSTATE field.

/*

 * We store our current task in sp_el0, which is clobbered by userspace. Keep a

 * shadow copy so that we can restore this upon entry from userspace.

 *

 * This is *only* for exception entry from EL0, and is not valid until we

 * __switch_to() a user task.

/*

 * ARM erratum 1418040 handling, affecting the 32bit view of CNTVCT.

 * Assuming the virtual counter is enabled at the beginning of times:

 *

 * - disable access when switching from a 64bit task to a 32bit task

 * - enable access when switching from a 32bit task to a 64bit task

/*

 * __switch_to() checks current->thread.sctlr_user as an optimisation. Therefore

 * this function must be called with preemption disabled and the update to

 * sctlr_user must be made in the same preemption disabled block so that

 * __switch_to() does not see the variable update before the SCTLR_EL1 one.

	/*

	 * EnIA must not be cleared while in the kernel as this is necessary for

	 * in-kernel PAC. It will be cleared on kernel exit if needed.

 ISB required for the kernel uaccess routines when setting TCF0. */

/*

 * Thread switching.

	/*

	 * Complete any pending TLB or cache maintenance on this CPU in case

	 * the thread migrates to a different CPU.

	 * This full barrier is also required by the membarrier system

	 * call.

	/*

	 * MTE thread switching must happen after the DSB above to ensure that

	 * any asynchronous tag check faults have been logged in the TFSR*_EL1

	 * registers.

 avoid expensive SCTLR_EL1 accesses if no change */

 the actual thread switch */

	/*

	 * Prevent execve() of a 32-bit program from a deadline task

	 * if the restricted affinity mask would be inadmissible on an

	 * asymmetric system.

/*

 * Called from setup_new_exec() after (COMPAT_)SET_PERSONALITY.

		/*

		 * Restrict the CPU affinity mask for a 32-bit task so that

		 * it contains only 32-bit-capable CPUs.

		 *

		 * From the perspective of the task, this looks similar to

		 * what would happen if the 64-bit-only CPUs were hot-unplugged

		 * at the point of execve(), although we try a bit harder to

		 * honour the cpuset hierarchy.

/*

 * Control the relaxed ABI allowing tagged user addresses into the kernel.

	/*

	 * Do not allow the enabling of the tagged address ABI if globally

	 * disabled via sysctl abi.tagged_addr_disabled.

/*

 * Global sysctl to disable the tagged user addresses support. This control

 * only prevents the tagged address ABI enabling via prctl() and does not

 * disable it for tasks that already opted in to the relaxed ABI.

 CONFIG_ARM64_TAGGED_ADDR_ABI */

	/*

	 * For dynamically linked executables the interpreter is

	 * responsible for setting PROT_BTI on everything except

	 * itself.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AArch64-specific system calls implementation

 *

 * Copyright (C) 2012 ARM Ltd.

 * Author: Catalin Marinas <catalin.marinas@arm.com>

/*

 * Wrappers to pass the pt_regs argument.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Record and handle CPU attributes.

 *

 * Copyright (C) 2014 ARM Ltd.

/*

 * In case the boot CPU is hotpluggable, we record its initial state and

 * current state separately. Certain system registers may contain different

 * values depending on configuration at or after reset.

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 Not possible on arm64 */

 CONFIG_COMPAT */

		/*

		 * glibc reads /proc/cpuinfo to determine the number of

		 * online processors, looking for lines beginning with

		 * "processor".  Give glibc what it expects.

		/*

		 * Dump out the common processor features in a single line.

		 * Userspace should read the hwcaps with getauxval(AT_HWCAP)

		 * rather than attempting to parse this, but there's a body of

		 * software which does already (at least for 32-bit).

					/*

					 * Warn once if any feature should not

					 * have been present on arm64 platform.

 CONFIG_COMPAT */

/*

 * The ARM ARM uses the phrase "32-bit register" to describe a register

 * whose upper 32 bits are RES0 (per C5.1.1, ARM DDI 0487A.i), however

 * no statement is made as to whether the upper 32 bits will or will not

 * be made use of in future, and between ARM DDI 0487A.c and ARM DDI

 * 0487A.d CLIDR_EL1 was expanded from 32-bit to 64-bit.

 *

 * Thus, while both MIDR_EL1 and REVIDR_EL1 are described as 32-bit

 * registers, we expose them both as 64 bit values to cater for possible

 * future expansion without an ABI break.

 Assume aliasing */

	/*

	 * Use the effective value of the CTR_EL0 than the raw value

	 * exposed by the CPU. CTR_EL0.IDC field value must be interpreted

	 * with the CLIDR_EL1 fields to avoid triggering false warnings

	 * when there is a mismatch across the CPUs. Keep track of the

	 * effective value of the CTR_EL0 in our internal records for

	 * accurate sanity check and feature enablement.

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Ultimately, this value will get limited by KSTACK_OFFSET_MAX(),

	 * but not enough for arm64 stack utilization comfort. To keep

	 * reasonable stack head room, reduce the maximum offset to 9 bits.

	 *

	 * The actual entropy will be further reduced by the compiler when

	 * applying stack alignment constraints: the AAPCS mandates a

	 * 16-byte (i.e. 4-bit) aligned SP at function boundaries.

	 *

	 * The resulting 5 bits of entropy is seen in SP[8:4].

	/*

	 * BTI note:

	 * The architecture does not guarantee that SPSR.BTYPE is zero

	 * on taking an SVC, so we could return to userspace with a

	 * non-zero BTYPE after the syscall.

	 *

	 * This shouldn't matter except when userspace is explicitly

	 * doing something stupid, such as setting PROT_BTI on a page

	 * that lacks conforming BTI/PACIxSP instructions, falling

	 * through from one executable page to another with differing

	 * PROT_BTI, or messing with BTYPE via ptrace: in such cases,

	 * userspace should not be surprised if a SIGILL occurs on

	 * syscall return.

	 *

	 * So, don't touch regs->pstate & PSR_BTYPE_MASK here.

	 * (Similarly for HVC and SMC elsewhere.)

		/*

		 * Process the asynchronous tag check fault before the actual

		 * syscall. do_notify_resume() will send a signal to userspace

		 * before the syscall is restarted.

		/*

		 * The de-facto standard way to skip a system call using ptrace

		 * is to set the system call to -1 (NO_SYSCALL) and set x0 to a

		 * suitable error code for consumption by userspace. However,

		 * this cannot be distinguished from a user-issued syscall(-1)

		 * and so we must set x0 to -ENOSYS here in case the tracer doesn't

		 * issue the skip and we fall into trace_exit with x0 preserved.

		 *

		 * This is slightly odd because it also means that if a tracer

		 * sets the system call number to -1 but does not initialise x0,

		 * then x0 will be preserved for all system calls apart from a

		 * user-issued syscall(-1). However, requesting a skip and not

		 * setting the return value is unlikely to do anything sensible

		 * anyway.

	/*

	 * The tracing status may have changed under our feet, so we have to

	 * check again. However, if we were tracing entry, then we always trace

	 * exit regardless, as the old entry assembly did.

	/*

	 * task_fpsimd_load() won't be called to update CPACR_EL1 in

	 * ret_to_user unless TIF_FOREIGN_FPSTATE is still set, which only

	 * happens if a context switch or kernel_neon_begin() or context

	 * modification (sigreturn, ptrace) intervenes.

	 * So, ensure that CPACR_EL1 is already correct for the fast-path case.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Spin Table SMP initialisation

 *

 * Copyright (C) 2013 ARM Ltd.

/*

 * Write secondary_holding_pen_release in a way that is guaranteed to be

 * visible to all observers, irrespective of whether they're taking part

 * in coherency or not.  This is necessary for the hotplug code to work

 * reliably.

	/*

	 * Determine the address from which the CPU is polling.

	/*

	 * The cpu-release-addr may or may not be inside the linear mapping.

	 * As ioremap_cache will either give us a new mapping or reuse the

	 * existing linear mapping, we can use it to cover both cases. In

	 * either case the memory will be MT_NORMAL.

	/*

	 * We write the release address as LE regardless of the native

	 * endianness of the kernel. Therefore, any boot-loaders that

	 * read this address need to convert this address to the

	 * boot-loader's endianness before jumping. This is mandated by

	 * the boot protocol.

	/*

	 * Send an event to wake up the secondary CPU.

	/*

	 * Update the pen release flag.

	/*

	 * Send an event, causing the secondaries to read pen_release.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Stack tracing support

 *

 * Copyright (C) 2012 ARM Ltd.

/*

 * AArch64 PCS assigns the frame pointer to x29.

 *

 * A simple function prologue looks like this:

 * 	sub	sp, sp, #0x10

 *   	stp	x29, x30, [sp]

 *	mov	x29, sp

 *

 * A simple function epilogue looks like this:

 *	mov	sp, x29

 *	ldp	x29, x30, [sp]

 *	add	sp, sp, #0x10

	/*

	 * Prime the first unwind.

	 *

	 * In unwind_frame() we'll check that the FP points to a valid stack,

	 * which can't be STACK_TYPE_UNKNOWN, and the first unwind will be

	 * treated as a transition to whichever stack that happens to be. The

	 * prev_fp value won't be used, but we set it to 0 such that it is

	 * definitely not an accessible stack address.

/*

 * Unwind from one frame record (A) to the next frame record (B).

 *

 * We terminate early if the location of B indicates a malformed chain of frame

 * records (e.g. a cycle), determined based on the location and fp value of A

 * and the location (but not the fp value) of B.

 Final frame; nothing to unwind */

	/*

	 * As stacks grow downward, any valid record on the same stack must be

	 * at a strictly higher address than the prior record.

	 *

	 * Stacks can nest in several valid orders, e.g.

	 *

	 * TASK -> IRQ -> OVERFLOW -> SDEI_NORMAL

	 * TASK -> SDEI_NORMAL -> SDEI_CRITICAL -> OVERFLOW

	 *

	 * ... but the nesting itself is strict. Once we transition from one

	 * stack to another, it's never valid to unwind back to that first

	 * stack.

	/*

	 * Record this frame record's values and location. The prev_fp and

	 * prev_type are only meaningful to the next unwind_frame() invocation.

		/*

		 * This is a case where function graph tracer has

		 * modified a return address (LR) in a stack frame

		 * to hook a function return.

		 * So replace it to an original value.

 CONFIG_FUNCTION_GRAPH_TRACER */

		/*

		 * task blocked in __switch_to

 skip until specified stack frame */

			/*

			 * Mostly, this is the case where this function is

			 * called in panic/abort. As exception handler's

			 * stack frame does not contain the corresponding pc

			 * at which an exception has taken place, use regs->pc

			 * instead.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/traps.c

 *

 * Copyright (C) 1995-2009 Russell King

 * Copyright (C) 2012 ARM Ltd.

 PSR_C_BIT &= ~PSR_Z_BIT */

 PSR_C_BIT &= ~PSR_Z_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

 PSR_N_BIT ^= PSR_V_BIT */

PSR_N_BIT ^= PSR_V_BIT */

PSR_N_BIT |= PSR_Z_BIT */

PSR_N_BIT ^= PSR_V_BIT */

PSR_N_BIT |= PSR_Z_BIT */

/*

 * Note that the ARMv8 ARM calls condition code 0b1111 "nv", but states that

 * it behaves identically to 0b1110 ("al").

 trap and error numbers are mostly meaningless on ARM */

/*

 * This function is protected against re-entrancy.

 Leave if the signal won't be shown */

 ARM mode */

	/*

	 * If this is the last instruction of the block, wipe the IT

	 * state. Otherwise advance it.

	/*

	 * If we were single stepping, we want to get the step exception after

	 * we return from the trap.

 16-bit Thumb instruction */

 32-bit ARM instruction */

 Force signals we don't understand to SIGKILL */

/*

 * Set up process info to signal segmentation fault - called on access error.

 check for AArch32 breakpoint instructions */

	/*

	 * Unexpected FPAC exception or pointer authentication failure in

	 * the kernel: kill the task before it does any more harm.

 DC CVAU, gets promoted */

 DC CVAC, gets promoted */

 DC CVADP */

 DC CVAP */

 DC CIVAC */

 IC IVAU */

 Hide DIC so that we can trap the unnecessary maintenance...*/

 ... and fake IminLine to reduce the number of traps. */

 Trap read access to CTR_EL0 */

 Trap read access to CNTVCT_EL0 */

 Trap read access to CNTVCTSS_EL0 */

 Trap read access to CNTFRQ_EL0 */

 Trap read access to CPUID registers */

 Trap WFI instructions executed in userspace */

 Only a T32 instruction can trap without CV being set */

		/*

		 * There is no T16 variant of a CP access, so we

		 * always advance PC by 4 bytes.

	/*

	 * New cp15 instructions may previously have been undefined at

	 * EL0. Fall back to our usual undefined instruction handler

	 * so that we handle these consistently.

	/*

	 * New SYS instructions may previously have been undefined at EL0. Fall

	 * back to our usual undefined instruction handler so that we handle

	 * these consistently.

/*

 * bad_el0_sync handles unexpected, but potentially recoverable synchronous

 * exceptions taken from EL0.

	/*

	 * We use nmi_panic to limit the potential for recusive overflows, and

	 * to get a better stack trace.

 corrected error */

 restartable, not yet consumed */

		/*

		 * The CPU can make progress. We may take UEO again as

		 * a more severe error.

 Uncorrected Unrecoverable */

 Uncorrected Recoverable */

		/*

		 * The CPU can't make progress. The exception may have

		 * been imprecise.

		 *

		 * Neoverse-N1 #1349291 means a non-KVM SError reported as

		 * Unrecoverable should be treated as Uncontainable. We

		 * call arm64_serror_panic() in both cases.

 Uncontainable or Uncategorized error */

 Error has been silently propagated */

 non-RAS errors are not containable */

 GENERIC_BUG traps */

	/*

	 * bug_handler() only called for BRK #BUG_BRK_IMM.

	 * So the answer is trivial -- any spurious instances with no

	 * bug table entry will be rejected by report_bug() and passed

	 * back to the debug-monitors code and handled as a fatal

	 * unexpected debug exception.

 unknown/unrecognised bug trap type */

 If thread survives, skip over the BUG instruction and continue: */

 We cannot handle this */

	/*

	 * The instrumentation allows to control whether we can proceed after

	 * a crash was detected. This is done by passing the -recover flag to

	 * the compiler. Disabling recovery allows to generate more compact

	 * code.

	 *

	 * Unfortunately disabling recovery doesn't work for the kernel right

	 * now. KASAN reporting is disabled in some contexts (for example when

	 * the allocator accesses slab object metadata; this is controlled by

	 * current->kasan_depth). All these accesses are detected by the tool,

	 * even though the reports for them are not printed.

	 *

	 * This is something that might be fixed at some point in the future.

 If thread survives, skip over the brk instruction and continue: */

/*

 * Initial handler for AArch64 BRK exceptions

 * This handler only used until debug_traps_init().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/setup.c

 *

 * Copyright (C) 1995-2001 Russell King

 * Copyright (C) 2012 ARM Ltd.

/*

 * Standard memory resources

/*

 * The recorded values of x0 .. x3 upon kernel entry.

/**

 * smp_build_mpidr_hash - Pre-compute shifts required at each affinity

 *			  level in order to build a linear index from an

 *			  MPIDR value. Resulting algorithm is a collision

 *			  free hash carried out through shifting and ORing

	/*

	 * Pre-scan the list of MPIDRS and filter out bits that do

	 * not contribute to affinity levels, ie they never toggle.

	/*

	 * Find and stash the last and first bit set at all affinity levels to

	 * check how many bits are required to represent them.

		/*

		 * Find the MSB bit and LSB bits position

		 * to determine how many bits are required

		 * to express the affinity level.

	/*

	 * An index can be created from the MPIDR_EL1 by isolating the

	 * significant bits at each affinity level and by shifting

	 * them in order to compress the 32 bits values space to a

	 * compressed set of values. This is equivalent to hashing

	 * the MPIDR_EL1 through shifting and ORing. It is a collision free

	 * hash though not minimal since some levels might contain a number

	 * of CPUs that is not an exact power of 2 and their bit

	 * representation might contain holes, eg MPIDR_EL1[7:0] = {0x2, 0x80}.

	/*

	 * 4x is an arbitrary value used to warn on a hash table much bigger

	 * than expected on most systems.

 Early fixups are done, map the FDT as read-only now */

 Userspace will find "Crash kernel" region in /proc/iomem. */

	/*

	 * If know now we are going to need KPTI then use non-global

	 * mappings from the start, avoiding the cost of rewriting

	 * everything later.

	/*

	 * Initialise the static keys early as they may be enabled by the

	 * cpufeature code and early parameters.

	/*

	 * Unmask asynchronous aborts and fiq after bringing up possible

	 * earlycon. (Report possible System Errors once we can report this

	 * occurred).

	/*

	 * TTBR0 is only used for the identity mapping at this stage. Make it

	 * point to zero page to avoid speculatively fetching new entries.

 Parse the ACPI tables for possible boot-time configuration */

 Init percpu seeds for random tags after cpus are set up. */

	/*

	 * Make sure init_thread_info.ttbr0 always generates translation

	 * faults in case uaccess_enable() is inadvertently called by the init

	 * thread.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Extensible Firmware Interface

 *

 * Based on Extensible Firmware Interface Specification version 2.4

 *

 * Copyright (C) 2013, 2014 Linaro Ltd.

/*

 * Only regions of type EFI_RUNTIME_SERVICES_CODE need to be

 * executable, everything else can be mapped with the XN bits

 * set. Also take the new (optional) RO/XP bits into account.

		/*

		 * If the region is not aligned to the page size of the OS, we

		 * can not use strict permissions, since that would also affect

		 * the mapping attributes of the adjacent regions.

 R-- */

 R-X */

 RW- */

 RWX */

 we will fill this structure from the stub, so don't put it in .bss */

		/*

		 * If the end address of this region is not aligned to page

		 * size, the mapping is rounded up, and may end up sharing a

		 * page frame with the next UEFI memory region. If we create

		 * a block entry now, we may need to split it again when mapping

		 * the next region, and support for that is going to be removed

		 * from the MMU routines. So avoid block mappings altogether in

		 * that case.

	/*

	 * Calling apply_to_page_range() is only safe on regions that are

	 * guaranteed to be mapped down to pages. Since we are only called

	 * for regions that have been mapped using efi_create_mapping() above

	 * (and this is checked by the generic Memory Attributes table parsing

	 * routines), there is no need to check that again here.

/*

 * UpdateCapsule() depends on the system being shutdown via

 * ResetSystem().

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/io.c

 *

 * Copyright (C) 2012 ARM Ltd.

/*

 * Copy data from IO memory space to "real" memory space.

/*

 * Copy data from "real" memory space to IO memory space.

/*

 * "memset" on IO memory space.

 SPDX-License-Identifier: GPL-2.0-only

/*:

 * Hibernate support specific for ARM64

 *

 * Derived from work on ARM hibernation support by:

 *

 * Ubuntu project, hibernation support for mach-dove

 * Copyright (C) 2010 Nokia Corporation (Hiroshi Doyu)

 * Copyright (C) 2010 Texas Instruments, Inc. (Teerth Reddy et al.)

 *  https://lkml.org/lkml/2010/6/18/4

 *  https://lists.linux-foundation.org/pipermail/linux-pm/2010-June/027422.html

 *  https://patchwork.kernel.org/patch/96442/

 *

 * Copyright (C) 2006 Rafael J. Wysocki <rjw@sisk.pl>

/*

 * Hibernate core relies on this value being 0 on resume, and marks it

 * __nosavedata assuming it will keep the resume kernel's '0' value. This

 * doesn't happen with either KASLR.

 *

 * defined as "__visible int in_suspend __nosavedata" in

 * kernel/power/hibernate.c

 Do we need to reset el2? */

 hyp-stub vectors, used to restore el2 during resume from hibernate. */

/*

 * The logical cpu number we should resume on, initialised to a non-cpu

 * number.

/*

 * Values that may not change over hibernate/resume. We put the build number

 * and date in here so that we guarantee not to resume with a different

 * kernel.

 These values need to be know across a hibernate/restore. */

 These are needed to find the relocated kernel if built with kaslr */

	/*

	 * We need to know where the __hyp_stub_vectors are after restore to

	 * re-configure el2.

 We can't use __hyp_get_vectors() because kvm may still be loaded */

 Save the mpidr of the cpu we called cpu_suspend() on... */

/*

 * Copies length bytes, starting at src_start into an new page,

 * perform cache maintenance, then maps it at the specified address low

 * address as executable.

 *

 * This is used by hibernate to copy the code it needs to execute when

 * overwriting the kernel text. This function generates a new set of page

 * tables, which it loads into ttbr0.

 *

 * Length is provided as we probably only want 4K of data, even on a 64K

 * page system.

		/*

		 * It is not required to invoke page_kasan_tag_reset(page)

		 * at this point since the tags stored in page->flags are

		 * already restored.

 CONFIG_ARM64_MTE */

 CONFIG_ARM64_MTE */

 make the crash dump kernel image visible/saveable */

 Clean kernel core startup/idle code to PoC*/

 Clean kvm setup code to PoC? */

 make the crash dump kernel image protected again */

		/*

		 * Tell the hibernation core that we've just restored

		 * the memory

		/*

		 * Just in case the boot kernel did turn the SSBD

		 * mitigation off behind our back, let's set the state

		 * to what we expect it to be.

/*

 * Setup then Resume from the hibernate image using swsusp_arch_suspend_exit().

 *

 * Memory allocated by get_safe_page() will be dealt with by the hibernate code,

 * we don't need to free it here.

	/*

	 * Restoring the memory image will overwrite the ttbr1 page tables.

	 * Create a second copy of just the linear map, and use this when

	 * restoring.

	/*

	 * We need a zero page that is zero before & after resume in order to

	 * to break before make on the ttbr1 page tables.

	/*

	 * Copy swsusp_arch_suspend_exit() to a safe page. This will generate

	 * a new set of ttbr0 page tables and load them.

	/*

	 * KASLR will cause the el2 vectors to be in a different location in

	 * the resumed kernel. Load hibernate's temporary copy into el2.

	 *

	 * We can skip this step if we booted at EL1, or are running with VHE.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2020 ARM Ltd.

/*

 * The asynchronous and asymmetric MTE modes have the same behavior for

 * store operations. This flag is set when either of these modes is enabled.

	/*

	 * We need smp_wmb() in between setting the flags and clearing the

	 * tags because if another thread reads page->flags and builds a

	 * tagged address out of it, there is an actual dependency to the

	 * memory access, but on the current thread we do not guarantee that

	 * the new page->flags are visible before the tags were updated.

 Early out if there's nothing to do */

 if PG_mte_tagged is set, tags have already been initialised */

	/*

	 * If the page content is identical but at least one of the pages is

	 * tagged, return non-zero to avoid KSM merging. If only one of the

	 * pages is tagged, set_pte_at() may zero or change the tags of the

	 * other page via mte_sync_tags().

 Enable MTE Sync Mode for EL1. */

	/*

	 * Make sure we enter this function when no PE has set

	 * async mode previously.

	/*

	 * MTE async mode is set system wide by the first PE that

	 * executes this function.

	 *

	 * Note: If in future KASAN acquires a runtime switching

	 * mode in between sync and async, this strategy needs

	 * to be reviewed.

		/*

		 * MTE asymm mode behaves as async mode for store

		 * operations. The mode is set system wide by the

		 * first PE that executes this function.

		 *

		 * Note: If in future KASAN acquires a runtime switching

		 * mode in between sync and async, this strategy needs

		 * to be reviewed.

		/*

		 * If the CPU does not support MTE asymmetric mode the

		 * kernel falls back on synchronous mode which is the

		 * default for kasan=on.

		/*

		 * Note: isb() is not required after this direct write

		 * because there is no indirect read subsequent to it

		 * (per ARM DDI 0487F.c table D13-1).

	/*

	 * This must be called with preemption disabled and can only be called

	 * on the current or next task since the CPU must match where the thread

	 * is going to run. The caller is responsible for calling

	 * update_sctlr_el1() later in the same preemption disabled block.

	/*

	 * SYS_GCR_EL1 will be set to current->thread.mte_ctrl value by

	 * mte_set_user_gcr() in kernel_exit, but only if KASAN is enabled.

 Branch -> NOP */

 clear any pending asynchronous tag fault */

 disable tag checking and reset tag generation mask */

	/*

	 * Check if an async tag exception occurred at EL1.

	 *

	 * Note: On the context switch path we rely on the dsb() present

	 * in __switch_to() to guarantee that the indirect writes to TFSR_EL1

	 * are synchronized before this point.

	/*

	 * The barriers are required to guarantee that the indirect writes

	 * to TFSR_EL1 are synchronized before we report the state.

 Report SYS_TFSR_EL1 before suspend entry */

/*

 * Access MTE tags in another process' address space as given in mm. Update

 * the number of tags copied. Return 0 if any tags copied, error otherwise.

 * Inspired by __access_remote_vm().

		/*

		 * Only copy tags if the page has been mapped as PROT_MTE

		 * (PG_mte_tagged set). Otherwise the tags are not valid and

		 * not accessible to user. Moreover, an mprotect(PROT_MTE)

		 * would cause the existing tags to be cleared if the page

		 * was never mapped with PROT_MTE.

 limit access to the end of the page */

 error accessing the tracer's buffer */

 return an error if no tags copied */

 check for error accessing the tracee's address space */

/*

 * Copy MTE tags in another process' address space at 'addr' to/from tracer's

 * iovec buffer. Return 0 on success. Inspired by ptrace_access_vm().

 align addr to the MTE tag granule */

 SPDX-License-Identifier: GPL-2.0

/*

 * ACPI 5.1 based NUMA setup for ARM64

 * Lots of code was borrowed from arch/x86/mm/srat.c

 *

 * Copyright 2004 Andi Kleen, SuSE Labs.

 * Copyright (C) 2013-2016, Linaro Ltd.

 *		Author: Hanjun Guo <hanjun.guo@linaro.org>

 *

 * Reads the ACPI SRAT table to figure out what memory belongs to which CPUs.

 *

 * Called from acpi_numa_init while reading the SRAT and SLIT tables.

 * Assumes all memory regions belonging to a single proximity domain

 * are in one chunk. Holes between them will be included in the node.

	/*

	 * If we can't map the UID to a logical cpu this

	 * means that the UID is not part of possible cpus

	 * so we do not need a NUMA mapping for it, skip

	 * the SRAT entry and keep parsing.

 Callback for Proximity Domain -> ACPI processor UID mapping */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ARM64 cacheinfo support

 *

 *  Copyright (C) 2015 ARM Ltd.

 *  All Rights Reserved

 Max 7 level supported */

 Ctypen, bits[3(n - 1) + 2 : 3(n - 1)], for n = 1 to 7 */

 Separate instruction and data caches */

		/*

		 * some external caches not specified in CLIDR_EL1

		 * the information may be available in the device tree

		 * only unified external caches are considered here

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/time.c

 *

 * Copyright (C) 1991, 1992, 1995  Linus Torvalds

 * Modifications for ARM (C) 1994-2001 Russell King

 * Copyright (C) 2012 ARM Ltd.

 Calibrate the delay loop directly */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Low-level idle sequences

/*

 *	cpu_do_idle()

 *

 *	Idle the processor (wait for interrupt).

 *

 *	If the CPU supports priority masking we must do additional work to

 *	ensure that interrupts are not masked at the PMR (because the core will

 *	not wake up if we block the wake up signal in the interrupt controller).

/*

 * This is our default idle handler.

	/*

	 * This should do all the clock switching and wait for interrupt

	 * tricks

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  Copyright (C) 2014 ARM Limited

/*

 * The runtime support for deprecated instruction support can be in one of

 * following three states -

 *

 * 0 = undef

 * 1 = emulate (software emulation)

 * 2 = hw (supported in hardware)

 Run set_hw_mode(mode) on all active CPUs */

/*

 * Run set_hw_mode for all insns on a starting CPU.

 * Returns:

 *  0 		- If all the hooks ran successfully.

 * -EINVAL	- At least one hook is not supported by the CPU.

 Nothing to be done */

 Disable the HW mode if it was turned on at early boot time */

 Register any handlers if required */

 Mode change failed, revert to previous mode. */

/*

 *  Implement emulation of the SWP/SWPB instructions using load-exclusive and

 *  store-exclusive.

 *

 *  Syntax of SWP{B} instruction: SWP{B}<c> <Rt>, <Rt2>, [<Rn>]

 *  Where: Rt  = destination

 *	   Rt2 = source

 *	   Rn  = address

/*

 * Error-checking SWP macros implemented using ldxr{b}/stxr{b}

 Arbitrary constant to ensure forward-progress of the LL/SC loop */

/*

 * Bit 22 of the instruction encoding distinguishes between

 * the SWP and SWPB variants (bit set means SWPB).

 SWP to unaligned address not permitted */

/*

 * swp_handler logs the id of calling process, dissects the instruction, sanity

 * checks the memory location, calls emulate_swpX for the actual operation and

 * deals with fixup/error handling before returning

 Condition failed - return to next instruction */

 If unconditional encoding - not a SWP, undef */

 Check access in reasonable access range for both SWP and SWPB */

/*

 * Only emulate SWP/SWPB executed in ARM state/User mode.

 * The kernel must be SWP free and SWP{B} does not exist in Thumb.

 Condition failed - return to next instruction */

 If unconditional encoding - not a barrier instruction */

		/*

		 * dmb - mcr p15, 0, Rt, c7, c10, 5

		 * dsb - mcr p15, 0, Rt, c7, c10, 4

		/*

		 * isb - mcr p15, 0, Rt, c7, c5, 4

		 *

		 * Taking an exception or returning from one acts as an

		 * instruction barrier. So no explicit barrier needed here.

 Thumb mode */

/*

 * Invoked as core_initcall, which guarantees that the instruction

 * emulation is ready for userspace.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * HW_breakpoint: a unified kernel/user-space hardware breakpoint facility,

 * using the CPU's debug registers.

 *

 * Copyright (C) 2012 ARM Limited

 * Author: Will Deacon <will.deacon@arm.com>

 Breakpoint currently in use for each BRP. */

 Watchpoint currently in use for each WRP. */

 Currently stepping a per-CPU kernel breakpoint. */

 Number of BRP/WRP registers on this CPU. */

	/*

	 * We can be called early, so don't rely on

	 * our static variables being initialised.

/*

 * Convert a breakpoint privilege level to the corresponding exception

 * level.

	/*

	 * tsk can be NULL for per-cpu (non-ptrace) breakpoints.

	 * In this case, use the native interface, since we don't have

	 * the notion of a "compat CPU" and could end up relying on

	 * deprecated behaviour if we use unaligned watchpoints in

	 * AArch64 state.

/**

 * hw_breakpoint_slot_setup - Find and setup a perf slot according to

 *			      operations

 *

 * @slots: pointer to array of slots

 * @max_slots: max number of slots

 * @bp: perf_event to setup

 * @ops: operation to be carried out on the slot

 *

 * Return:

 *	slot index on success

 *	-ENOSPC if no slot is available/matches

 *	-EINVAL on wrong operations parameter

 Breakpoint */

 Watchpoint */

		/*

		 * Ensure debug monitors are enabled at the correct exception

		 * level.

 Setup the address register. */

 Setup the control register. */

 Reset the control register. */

		/*

		 * Release the debug monitors for the correct exception

		 * level.

/*

 * Install a perf counter breakpoint.

/*

 * Check whether bp virtual address is in kernel space.

/*

 * Extract generic type and length encodings from an arch_hw_breakpoint_ctrl.

 * Hopefully this will disappear when ptrace can bypass the conversion

 * to generic breakpoint descriptions.

 Type */

 Len */

/*

 * Construct an arch_hw_breakpoint from a perf_event.

 Type */

 Len */

	/*

	 * On AArch64, we only permit breakpoints of length 4, whereas

	 * AArch32 also requires breakpoints of length 2 for Thumb.

	 * Watchpoints can be of length 1, 2, 4 or 8 bytes.

			/*

			 * FIXME: Some tools (I'm looking at you perf) assume

			 *	  that breakpoints should be sizeof(long). This

			 *	  is nonsense. For now, we fix up the parameter

			 *	  but we should probably return -EINVAL instead.

 Address */

	/*

	 * Privilege

	 * Note that we disallow combined EL0/EL1 breakpoints because

	 * that would complicate the stepping code.

 Enabled? */

/*

 * Validate the arch-specific HW Breakpoint register settings.

 Build the arch_hw_breakpoint. */

	/*

	 * Check address alignment.

	 * We don't do any clever alignment correction for watchpoints

	 * because using 64-bit unaligned addresses is deprecated for

	 * AArch64.

	 *

	 * AArch32 tasks expect some simple alignment fixups, so emulate

	 * that here.

 Aligned */

 Allow halfword watchpoints and breakpoints. */

 Allow single byte watchpoint. */

	/*

	 * Disallow per-task kernel breakpoints since these would

	 * complicate the stepping code.

/*

 * Enable/disable all of the breakpoints active at the specified

 * exception level at the register level.

 * This is used when single-stepping after a breakpoint exception.

/*

 * Debug exception handlers.

 Check if the breakpoint value matches. */

 Possible match, check the byte address select to confirm. */

 Do we need to handle the stepping? */

 If we're already stepping a watchpoint, just return. */

/*

 * Arm64 hardware does not always report a watchpoint hit address that matches

 * one of the watchpoints set. It can also report an address "near" the

 * watchpoint if a single instruction access both watched and unwatched

 * addresses. There is no straight-forward way, short of disassembling the

 * offending instruction, to map that address back to the watchpoint. This

 * function computes the distance of the memory access from the watchpoint as a

 * heuristic for the likelyhood that a given access triggered the watchpoint.

 *

 * See Section D2.10.5 "Determining the memory location that caused a Watchpoint

 * exception" of ARMv8 Architecture Reference Manual for details.

 *

 * The function returns the distance of the address from the bytes watched by

 * the watchpoint. In case of an exact match, it returns 0.

	/*

	 * If we triggered a user watchpoint from a uaccess routine, then

	 * handle the stepping ourselves since userspace really can't help

	 * us with this.

	/*

	 * Find all watchpoints that match the reported address. If no exact

	 * match is found. Attribute the hit to the closest watchpoint.

		/*

		 * Check that the access type matches.

		 * 0 => load, otherwise => store

 Check if the watchpoint value and byte select match. */

 Is this an exact match? */

 No exact match found? */

	/*

	 * We always disable EL0 watchpoints because the kernel can

	 * cause these to fire via an unprivileged access.

 If we're already stepping a breakpoint, just return. */

/*

 * Handle single-step exception.

	/*

	 * Called from single-step exception handler.

	 * Return 0 if execution can resume, 1 if a SIGTRAP should be

	 * reported.

 Allow exception handling to fall-through. */

/*

 * Context-switcher for restoring suspended breakpoints.

	/*

	 *           current        next

	 * disabled: 0              0     => The usual case, NOTIFY_DONE

	 *           0              1     => Disable the registers

	 *           1              0     => Enable the registers

	 *           1              1     => NOTIFY_DONE. per-task bps will

	 *                                   get taken care of by perf.

 Update breakpoints. */

 Update watchpoints. */

/*

 * CPU initialisation.

	/*

	 * When a CPU goes through cold-boot, it does not have any installed

	 * slot, so it is safe to share the same function for restoring and

	 * resetting breakpoints; when a CPU is hotplugged in, it goes

	 * through the slots, which are all empty, hence it just resets control

	 * and value for debug registers.

	 * When this function is triggered on warm-boot through a CPU PM

	 * notifier some slots might be initialized; if so they are

	 * reprogrammed according to the debug slots content.

/*

 * One-time initialisation.

 Register debug fault handlers. */

	/*

	 * Reset the breakpoint resources. We assume that a halting

	 * debugger will leave the world in a nice state for us.

 Register cpu_suspend hw breakpoint restore hook */

/*

 * Dummy function to register with die_notifier.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * kexec for arm64

 *

 * Copyright (C) Linaro.

 * Copyright (C) Huawei Futurewei Technologies.

/**

 * kexec_image_info - For debugging output.

 Empty routine needed to avoid build errors. */

/**

 * machine_kexec_prepare - Prepare for a kexec reboot.

 *

 * Called from the core kexec code when a kernel image is loaded.

 * Forbid loading a kexec kernel if we have no way of hotplugging cpus or cpus

 * are stuck in the kernel. This avoids a panic once we hit machine_kexec().

/**

 * kexec_segment_flush - Helper to flush the kimage segments to PoC.

 Allocates pages for kexec page table */

 If in place, relocation is not used, only flush next kernel */

 Create a copy of the linear map */

 Flush the reloc_code in preparation for its execution. */

/**

 * machine_kexec - Do the kexec reboot.

 *

 * Called from the core kexec code for a sys_reboot with LINUX_REBOOT_CMD_KEXEC.

	/*

	 * New cpus may have become stuck_in_kernel after we loaded the image.

	/*

	 * Both restart and kernel_reloc will shutdown the MMU, disable data

	 * caches. However, restart will start new kernel or purgatory directly,

	 * kernel_reloc contains the body of arm64_relocate_new_kernel

	 * In kexec case, kimage->start points to purgatory assuming that

	 * kernel entry and dtb address are embedded in purgatory by

	 * userspace (kexec-tools).

	 * In kexec_file case, the kernel starts directly without purgatory.

 Should never get here. */

		/*

		 * First try to remove the active state. If this

		 * fails, try to EOI the interrupt.

/**

 * machine_crash_shutdown - shutdown non-crashing cpus and save registers

 shutdown non-crashing cpus */

 for crashing cpu */

/*

 * To preserve the crash dump kernel image, the relevant memory segments

 * should be mapped again around the hibernation.

/*

 * crash_is_nosave

 *

 * Return true only if a page is part of reserved memory for crash dump kernel,

 * but does not hold any data of loaded kernel image.

 *

 * Note that all the pages in crash dump kernel memory have been initially

 * marked as Reserved as memory was allocated via memblock_reserve().

 *

 * In hibernation, the pages which are Reserved and yet "nosave" are excluded

 * from the hibernation iamge. crash_is_nosave() does thich check for crash

 * dump kernel and will reduce the total size of hibernation image.

 in reserved memory? */

 not part of loaded kernel image? */

 CONFIG_HIBERNATION */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * FP/SIMD context switching and fault handling

 *

 * Copyright (C) 2012 ARM Ltd.

 * Author: Catalin Marinas <catalin.marinas@arm.com>

/*

 * (Note: in this discussion, statements about FPSIMD apply equally to SVE.)

 *

 * In order to reduce the number of times the FPSIMD state is needlessly saved

 * and restored, we need to keep track of two things:

 * (a) for each task, we need to remember which CPU was the last one to have

 *     the task's FPSIMD state loaded into its FPSIMD registers;

 * (b) for each CPU, we need to remember which task's userland FPSIMD state has

 *     been loaded into its FPSIMD registers most recently, or whether it has

 *     been used to perform kernel mode NEON in the meantime.

 *

 * For (a), we add a fpsimd_cpu field to thread_struct, which gets updated to

 * the id of the current CPU every time the state is loaded onto a CPU. For (b),

 * we add the per-cpu variable 'fpsimd_last_state' (below), which contains the

 * address of the userland FPSIMD state of the task that was loaded onto the CPU

 * the most recently, or NULL if kernel mode NEON has been performed after that.

 *

 * With this in place, we no longer have to restore the next FPSIMD state right

 * when switching between tasks. Instead, we can defer this check to userland

 * resume, at which time we verify whether the CPU's fpsimd_last_state and the

 * task's fpsimd_cpu are still mutually in sync. If this is the case, we

 * can omit the FPSIMD restore.

 *

 * As an optimization, we use the thread_info flag TIF_FOREIGN_FPSTATE to

 * indicate whether or not the userland FPSIMD state of the current task is

 * present in the registers. The flag is set unless the FPSIMD registers of this

 * CPU currently contain the most recent userland FPSIMD state of the current

 * task.

 *

 * In order to allow softirq handlers to use FPSIMD, kernel_neon_begin() may

 * save the task's FPSIMD context back to task_struct from softirq context.

 * To prevent this from racing with the manipulation of the task's FPSIMD state

 * from task context and thereby corrupting the state, it is necessary to

 * protect any manipulation of a task's fpsimd_state or TIF_FOREIGN_FPSTATE

 * flag with {, __}get_cpu_fpsimd_context(). This will still allow softirqs to

 * run but prevent them to use FPSIMD.

 *

 * For a certain task, the sequence may look something like this:

 * - the task gets scheduled in; if both the task's fpsimd_cpu field

 *   contains the id of the current CPU, and the CPU's fpsimd_last_state per-cpu

 *   variable points to the task's fpsimd_state, the TIF_FOREIGN_FPSTATE flag is

 *   cleared, otherwise it is set;

 *

 * - the task returns to userland; if TIF_FOREIGN_FPSTATE is set, the task's

 *   userland FPSIMD state is copied from memory to the registers, the task's

 *   fpsimd_cpu field is set to the id of the current CPU, the current

 *   CPU's fpsimd_last_state pointer is set to this task's fpsimd_state and the

 *   TIF_FOREIGN_FPSTATE flag is cleared;

 *

 * - the task executes an ordinary syscall; upon return to userland, the

 *   TIF_FOREIGN_FPSTATE flag will still be cleared, so no FPSIMD state is

 *   restored;

 *

 * - the task executes a syscall which executes some NEON instructions; this is

 *   preceded by a call to kernel_neon_begin(), which copies the task's FPSIMD

 *   register contents to memory, clears the fpsimd_last_state per-cpu variable

 *   and sets the TIF_FOREIGN_FPSTATE flag;

 *

 * - the task gets preempted after kernel_neon_end() is called; as we have not

 *   returned from the 2nd syscall yet, TIF_FOREIGN_FPSTATE is still set so

 *   whatever is in the FPSIMD registers is not saved to memory, but discarded.

 Default VL for tasks */

 ! CONFIG_ARM64_SVE */

 Dummy declaration for code that will be optimised out: */

 ! CONFIG_ARM64_SVE */

/*

 * Claim ownership of the CPU FPSIMD context for use by the calling context.

 *

 * The caller may freely manipulate the FPSIMD context metadata until

 * put_cpu_fpsimd_context() is called.

 *

 * The double-underscore version must only be called if you know the task

 * can't be preempted.

 No matching get_cpu_fpsimd_context()? */

/*

 * Release the CPU FPSIMD context.

 *

 * Must be called from a context in which get_cpu_fpsimd_context() was

 * previously called, with no call to put_cpu_fpsimd_context() in the

 * meantime.

/*

 * Call __sve_free() directly only if you know task can't be scheduled

 * or preempted.

/*

 * TIF_SVE controls whether a task can use SVE without trapping while

 * in userspace, and also the way a task's FPSIMD/SVE state is stored

 * in thread_struct.

 *

 * The kernel uses this flag to track whether a user task is actively

 * using SVE, and therefore whether full SVE register state needs to

 * be tracked.  If not, the cheaper FPSIMD context handling code can

 * be used instead of the more costly SVE equivalents.

 *

 *  * TIF_SVE set:

 *

 *    The task can execute SVE instructions while in userspace without

 *    trapping to the kernel.

 *

 *    When stored, Z0-Z31 (incorporating Vn in bits[127:0] or the

 *    corresponding Zn), P0-P15 and FFR are encoded in in

 *    task->thread.sve_state, formatted appropriately for vector

 *    length task->thread.sve_vl.

 *

 *    task->thread.sve_state must point to a valid buffer at least

 *    sve_state_size(task) bytes in size.

 *

 *    During any syscall, the kernel may optionally clear TIF_SVE and

 *    discard the vector state except for the FPSIMD subset.

 *

 *  * TIF_SVE clear:

 *

 *    An attempt by the user task to execute an SVE instruction causes

 *    do_sve_acc() to be called, which does some preparation and then

 *    sets TIF_SVE.

 *

 *    When stored, FPSIMD registers V0-V31 are encoded in

 *    task->thread.uw.fpsimd_state; bits [max : 128] for each of Z0-Z31 are

 *    logically zero but not stored anywhere; P0-P15 and FFR are not

 *    stored and have unspecified values from userspace's point of

 *    view.  For hygiene purposes, the kernel zeroes them on next use,

 *    but userspace is discouraged from relying on this.

 *

 *    task->thread.sve_state does not need to be non-NULL, valid or any

 *    particular size: it must not be dereferenced.

 *

 *  * FPSR and FPCR are always stored in task->thread.uw.fpsimd_state

 *    irrespective of whether TIF_SVE is clear or set, since these are

 *    not vector length dependent.

/*

 * Update current's FPSIMD/SVE registers from thread_struct.

 *

 * This function should be called only when the FPSIMD/SVE state in

 * thread_struct is known to be up to date, when preparing to enter

 * userspace.

/*

 * Ensure FPSIMD/SVE storage in memory for the loaded context is up to

 * date with respect to the CPU registers.

 set by fpsimd_bind_task_to_cpu() or fpsimd_bind_state_to_cpu() */

			/*

			 * Can't save the user regs, so current would

			 * re-enter user with corrupt state.

			 * There's no way to recover, so kill it:

/*

 * All vector length selection from userspace comes through here.

 * We're on a slow path, so some sanity-checks are included.

 * If things go wrong there's a bug somewhere, but try to fall back to a

 * safe choice.

 Writing -1 has the special meaning "set to max": */

 ! (CONFIG_ARM64_SVE && CONFIG_SYSCTL) */

 ! (CONFIG_ARM64_SVE && CONFIG_SYSCTL) */

/*

 * Transfer the FPSIMD state in task->thread.uw.fpsimd_state to

 * task->thread.sve_state.

 *

 * Task can be a non-runnable task, or current.  In the latter case,

 * the caller must have ownership of the cpu FPSIMD context before calling

 * this function.

 * task->thread.sve_state must point to at least sve_state_size(task)

 * bytes of allocated kernel memory.

 * task->thread.uw.fpsimd_state must be up to date before calling this

 * function.

/*

 * Transfer the SVE state in task->thread.sve_state to

 * task->thread.uw.fpsimd_state.

 *

 * Task can be a non-runnable task, or current.  In the latter case,

 * the caller must have ownership of the cpu FPSIMD context before calling

 * this function.

 * task->thread.sve_state must point to at least sve_state_size(task)

 * bytes of allocated kernel memory.

 * task->thread.sve_state must be up to date before calling this function.

/*

 * Return how many bytes of memory are required to store the full SVE

 * state for task, given task's currently configured vector length.

/*

 * Ensure that task->thread.sve_state is allocated and sufficiently large.

 *

 * This function should be used only in preparation for replacing

 * task->thread.sve_state with new data.  The memory is always zeroed

 * here to prevent stale data from showing through: this is done in

 * the interest of testability and predictability: except in the

 * do_sve_acc() case, there is no ABI requirement to hide stale data

 * written previously be task.

 This is a small allocation (maximum ~8KB) and Should Not Fail. */

/*

 * Ensure that task->thread.sve_state is up to date with respect to

 * the user task, irrespective of when SVE is in use or not.

 *

 * This should only be called by ptrace.  task must be non-runnable.

 * task->thread.sve_state must point to at least sve_state_size(task)

 * bytes of allocated kernel memory.

/*

 * Ensure that task->thread.uw.fpsimd_state is up to date with respect to

 * the user task, irrespective of whether SVE is in use or not.

 *

 * This should only be called by ptrace.  task must be non-runnable.

 * task->thread.sve_state must point to at least sve_state_size(task)

 * bytes of allocated kernel memory.

/*

 * Ensure that task->thread.sve_state is up to date with respect to

 * the task->thread.uw.fpsimd_state.

 *

 * This should only be called by ptrace to merge new FPSIMD register

 * values into a task for which SVE is currently active.

 * task must be non-runnable.

 * task->thread.sve_state must point to at least sve_state_size(task)

 * bytes of allocated kernel memory.

 * task->thread.uw.fpsimd_state must already have been initialised with

 * the new FPSIMD register values to be merged in.

	/*

	 * Clamp to the maximum vector length that VL-agnostic SVE code can

	 * work with.  A flag may be assigned in the future to allow setting

	 * of larger vector lengths without confusing older software.

 Reset VL to system default on next exec: */

 Only actually set the VL if not deferred: */

	/*

	 * To ensure the FPSIMD bits of the SVE vector registers are preserved,

	 * write any live register state back to task_struct, and convert to a

	 * non-SVE thread.

	/*

	 * Force reallocation of task SVE state to the correct size

	 * on next use:

/*

 * Encode the current vector length and flags for return.

 * This is only required for prctl(): ptrace has separate fields

 *

 * flags are as for sve_set_vector_length().

 PR_SVE_SET_VL */

 PR_SVE_GET_VL */

 self-syncing */

 skip intervening lengths */

/*

 * Initialise the set of known supported VQs for the boot CPU.

 * This is called during kernel boot, before secondary CPUs are brought up.

/*

 * If we haven't committed to the set of supported VQs yet, filter out

 * those not supported by the current CPU.

 * This function is called during the bring-up of early secondary CPUs only.

/*

 * Check whether the current CPU supports all VQs in the committed set.

 * This function is called during the bring-up of late secondary CPUs only.

	/*

	 * For KVM, it is necessary to ensure that this CPU doesn't

	 * support any vector length that guests may have probed as

	 * unsupported.

 Recover the set of supported VQs: */

 Find VQs supported that are not globally supported: */

 Find the lowest such VQ, if any: */

 no mismatches */

	/*

	 * Mismatches above sve_max_virtualisable_vl are fine, since

	 * no guest is allowed to configure ZCR_EL2.LEN to exceed this:

	/*

	 * alloc_percpu() warns and prints a backtrace if this goes wrong.

	 * This is evidence of a crippled system and we are returning void,

	 * so no attempt is made to handle this situation here.

/*

 * Enable SVE for EL1.

 * Intended for use by the cpufeatures code during CPU boot.

/*

 * Read the pseudo-ZCR used by cpufeatures to identify the supported SVE

 * vector length.

 *

 * Use only if SVE is present.

 * This function clobbers the SVE vector length.

	/*

	 * Set the maximum possible VL, and write zeroes to all other

	 * bits to see if they stick.

 find sticky 1s outside LEN field */

 set LEN field to maximum effective value */

	/*

	 * The SVE architecture mandates support for 128-bit vectors,

	 * so sve_vq_map must have at least SVE_VQ_MIN set.

	 * If something went wrong, at least try to patch it up:

	/*

	 * Sanity-check that the max VL we determined through CPU features

	 * corresponds properly to sve_vq_map.  If not, do our best:

	/*

	 * For the default VL, pick the maximum supported value <= 64.

	 * VL == 64 is guaranteed not to grow the signal frame.

 No non-virtualisable VLs found */

 No virtualisable VLs?  This is architecturally forbidden. */

 b + 1 < SVE_VQ_MAX */

 KVM decides whether to support mismatched systems. Just warn here: */

/*

 * Called from the put_task_struct() path, which cannot get here

 * unless dead_task is really dead and not schedulable.

 CONFIG_ARM64_SVE */

/*

 * Trapped SVE access

 *

 * Storage is allocated for the full SVE state, the current FPSIMD

 * register contents are migrated across, and the access trap is

 * disabled.

 *

 * TIF_SVE should be clear on entry: otherwise, fpsimd_restore_current_state()

 * would have disabled the SVE access trap for userspace during

 * ret_to_user, making an SVE access trap impossible in that case.

 Even if we chose not to use SVE, the hardware could still trap: */

 SVE access shouldn't have trapped */

	/*

	 * Convert the FPSIMD state to SVE, zeroing all the state that

	 * is not shared with FPSIMD. If (as is likely) the current

	 * state is live in the registers then do this there and

	 * update our metadata for the current task including

	 * disabling the trap, otherwise update our in-memory copy.

/*

 * Trapped FP/ASIMD access.

 TODO: implement lazy context saving/restoring */

/*

 * Raise a SIGFPE for the current process.

 Save unsaved fpsimd state, if any: */

	/*

	 * Fix up TIF_FOREIGN_FPSTATE to correctly describe next's

	 * state.  For kernel threads, FPSIMD registers are never loaded

	 * and wrong_task and wrong_cpu will always be true.

	/*

	 * Reset the task vector length as required.  This is where we

	 * ensure that all user tasks have a valid vector length

	 * configured: no kernel task can become a user task without

	 * an exec and hence a call to this function.  By the time the

	 * first call to this function is made, all early hardware

	 * probing is complete, so __sve_default_vl should be valid.

	 * If a bug causes this to go wrong, we make some noise and

	 * try to fudge thread.sve_vl to a safe value here.

	/*

	 * If the task is not set to inherit, ensure that the vector

	 * length will be reset by a subsequent exec:

/*

 * Save the userland FPSIMD state of 'current' to memory, but only if the state

 * currently held in the registers does in fact belong to 'current'

/*

 * Like fpsimd_preserve_current_state(), but ensure that

 * current->thread.uw.fpsimd_state is updated so that it can be copied to

 * the signal frame.

/*

 * Associate current's FPSIMD context with this cpu

 * The caller must have ownership of the cpu FPSIMD context before calling

 * this function.

 Toggle SVE trapping for userspace if needed */

 Serialised by exception return to user */

/*

 * Load the userland FPSIMD state of 'current' from memory, but only if the

 * FPSIMD state already held in the registers is /not/ the most recent FPSIMD

 * state of 'current'

	/*

	 * For the tasks that were created before we detected the absence of

	 * FP/SIMD, the TIF_FOREIGN_FPSTATE could be set via fpsimd_thread_switch(),

	 * e.g, init. This could be then inherited by the children processes.

	 * If we later detect that the system doesn't support FP/SIMD,

	 * we must clear the flag for  all the tasks to indicate that the

	 * FPSTATE is clean (as we can't have one) to avoid looping for ever in

	 * do_notify_resume().

/*

 * Load an updated userland FPSIMD state for 'current' from memory and set the

 * flag that indicates that the FPSIMD register contents are the most recent

 * FPSIMD state of 'current'

/*

 * Invalidate live CPU copies of task t's FPSIMD state

 *

 * This function may be called with preemption enabled.  The barrier()

 * ensures that the assignment to fpsimd_cpu is visible to any

 * preemption/softirq that could race with set_tsk_thread_flag(), so

 * that TIF_FOREIGN_FPSTATE cannot be spuriously re-cleared.

 *

 * The final barrier ensures that TIF_FOREIGN_FPSTATE is seen set by any

 * subsequent code.

	/*

	 * If we don't support fpsimd, bail out after we have

	 * reset the fpsimd_cpu for this task and clear the

	 * FPSTATE.

/*

 * Invalidate any task's FPSIMD state that is present on this cpu.

 * The FPSIMD context should be acquired with get_cpu_fpsimd_context()

 * before calling this function.

/*

 * Save the FPSIMD state to memory and invalidate cpu view.

 * This function must be called with preemption disabled.

/*

 * Kernel-side NEON support functions

/*

 * kernel_neon_begin(): obtain the CPU FPSIMD registers for use by the calling

 * context

 *

 * Must not be called unless may_use_simd() returns true.

 * Task context in the FPSIMD registers is saved back to memory as necessary.

 *

 * A matching call to kernel_neon_end() must be made before returning from the

 * calling context.

 *

 * The caller may freely use the FPSIMD registers until kernel_neon_end() is

 * called.

 Save unsaved fpsimd state, if any: */

 Invalidate any task state remaining in the fpsimd regs: */

/*

 * kernel_neon_end(): give the CPU FPSIMD registers back to the current task

 *

 * Must be called from a context in which kernel_neon_begin() was previously

 * called, with no call to kernel_neon_end() in the meantime.

 *

 * The caller must not use the FPSIMD registers after this function is called,

 * unless kernel_neon_begin() is called again in the meantime.

/*

 * EFI runtime services support functions

 *

 * The ABI for EFI runtime services allows EFI to use FPSIMD during the call.

 * This means that for EFI (and only for EFI), we have to assume that FPSIMD

 * is always used rather than being an optional accelerator.

 *

 * These functions provide the necessary support for ensuring FPSIMD

 * save/restore in the contexts from which EFI is used.

 *

 * Do not use them for any other purpose -- if tempted to do so, you are

 * either doing something wrong or you need to propose some refactoring.

/*

 * __efi_fpsimd_begin(): prepare FPSIMD for making an EFI runtime services call

		/*

		 * If !efi_sve_state, SVE can't be in use yet and doesn't need

		 * preserving:

/*

 * __efi_fpsimd_end(): clean up FPSIMD after an EFI runtime services call

 CONFIG_EFI */

 CONFIG_KERNEL_MODE_NEON */

 CONFIG_CPU_PM */

/*

 * FP/SIMD support code initialisation.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Handle detection, reporting and mitigation of Spectre v1, v2, v3a and v4, as

 * detailed at:

 *

 *   https://developer.arm.com/support/arm-security-updates/speculative-processor-vulnerability

 *

 * This code was originally written hastily under an awful lot of stress and so

 * aspects of it are somewhat hacky. Unfortunately, changing anything in here

 * instantly makes me feel ill. Thanks, Jann. Thann.

 *

 * Copyright (C) 2018 ARM Ltd, All Rights Reserved.

 * Copyright (C) 2020 Google LLC

 *

 * "If there's something strange in your neighbourhood, who you gonna call?"

 *

 * Authors: Will Deacon <will@kernel.org> and Marc Zyngier <maz@kernel.org>

/*

 * We try to ensure that the mitigation state can never change as the result of

 * onlining a late CPU.

 Userspace almost certainly can't deal with this. */

/*

 * Spectre v1.

 *

 * The kernel can't protect userspace for this one: it's each person for

 * themselves. Advertise what we're doing and be done with it.

/*

 * Spectre v2.

 *

 * This one sucks. A CPU is either:

 *

 * - Mitigated in hardware and advertised by ID_AA64PFR0_EL1.CSV2.

 * - Mitigated in hardware and listed in our "safe list".

 * - Mitigated in software by firmware.

 * - Mitigated in software by a CPU-specific dance in the kernel and a

 *   firmware call at EL2.

 * - Vulnerable.

 *

 * It's not unlikely for different CPUs in a big.LITTLE system to fall into

 * different camps.

 sentinel */ }

 If the CPU has CSV2 set, we're safe */

 Alternatively, we have a list of unaffected CPUs */

	/*

	 * Vinz Clortho takes the hyp_vecs start/end "keys" at

	 * the door when we're a guest. Skip the hyp-vectors work.

	/*

	 * Prefer a CPU-specific workaround if it exists. Note that we

	 * still rely on firmware for the mitigation at EL2.

/*

 * Spectre-v3a.

 *

 * Phew, there's not an awful lot to do here! We just instruct EL2 to use

 * an indirect trampoline for the hyp vectors so that guests can't read

 * VBAR_EL2 to defeat randomisation of the hypervisor VA layout.

/*

 * Spectre v4.

 *

 * If you thought Spectre v2 was nasty, wait until you see this mess. A CPU is

 * either:

 *

 * - Mitigated in hardware and listed in our "safe list".

 * - Mitigated in hardware via PSTATE.SSBS.

 * - Mitigated in software by firmware (sometimes referred to as SSBD).

 *

 * Wait, that doesn't sound so bad, does it? Keep reading...

 *

 * A major source of headaches is that the software mitigation is enabled both

 * on a per-task basis, but can also be forced on for the kernel, necessitating

 * both context-switch *and* entry/exit hooks. To make it even worse, some CPUs

 * allow EL0 to toggle SSBS directly, which can end up with the prctl() state

 * being stale when re-entering the kernel. The usual big.LITTLE caveats apply,

 * so you can have systems that have both firmware and SSBS mitigations. This

 * means we actually have to reject late onlining of CPUs with mitigations if

 * all of the currently onlined CPUs are safelisted, as the mitigation tends to

 * be opt-in for userspace. Yes, really, the cure is worse than the disease.

 *

 * The only good part is that if the firmware mitigation is present, then it is

 * present for all CPUs, meaning we don't have to worry about late onlining of a

 * vulnerable CPU if one of the boot CPUs is using the firmware mitigation.

 *

 * Give me a VAX-11/780 any day of the week...

 This is the per-cpu state tracking whether we need to talk to firmware */

/*

 * Because this was all written in a rush by people working in different silos,

 * we've ended up with multiple command line options to control the same thing.

 * Wrap these up in some helpers, which prefer disabling the mitigation if faced

 * with contradictory parameters. The mitigation is always either "off",

 * "dynamic" or "on".

 Do we need to toggle the mitigation state on entry to/exit from the kernel? */

 sentinel */ },

 CPU features are detected first */

	/*

	 * If the system is mitigated but this CPU doesn't have SSBS, then

	 * we must be on the safelist and there's nothing more to do.

 SCTLR_EL1.DSSBS was initialised to 0 during boot */

/*

 * Patch a branch over the Spectre-v4 mitigation code with a NOP so that

 * we fallthrough and check whether firmware needs to be called on this CPU.

 Branch -> NOP */

/*

 * Patch a NOP in the Spectre-v4 mitigation code with an SMC/HVC instruction

 * to call into firmware to adjust the mitigation state.

 NOP -> HVC/SMC */

/*

 * The Spectre-v4 mitigation can be controlled via a prctl() from userspace.

 * This is interesting because the "speculation disabled" behaviour can be

 * configured so that it is preserved across exec(), which means that the

 * prctl() may be necessary even when PSTATE.SSBS can be toggled directly

 * from userspace.

 Enable speculation: disable mitigation */

		/*

		 * Force disabled speculation prevents it from being

		 * re-enabled.

		/*

		 * If the mitigation is forced on, then speculation is forced

		 * off and we again prevent it from being re-enabled.

 Force disable speculation: force enable mitigation */

		/*

		 * If the mitigation is forced off, then speculation is forced

		 * on and we prevent it from being disabled.

 Disable speculation: enable mitigation */

 Same as PR_SPEC_FORCE_DISABLE */

 Disable speculation until execve(): enable mitigation */

		/*

		 * If the mitigation state is forced one way or the other, then

		 * we must fail now before we try to toggle it on execve().

 Mitigations are disabled, so we're vulnerable. */

 Check the mitigation state for this task */

/*

 * arch/arm64/kernel/topology.c

 *

 * Copyright (C) 2011,2013,2014 Linaro Limited.

 *

 * Based on the arm32 version written by Vincent Guittot in turn based on

 * arch/sh/kernel/topology.c

 *

 * This file is subject to the terms and conditions of the GNU General Public

 * License.  See the file "COPYING" in the main directory of this archive

 * for more details.

 Uniprocessor systems can rely on default topology values */

	/*

	 * This would be the place to create cpu topology based on MPIDR.

	 *

	 * However, it cannot be trusted to depict the actual topology; some

	 * pieces of the architecture enforce an artificial cap on Aff0 values

	 * (e.g. GICv3's ICC_SGI1R_EL1 limits it to 15), leading to an

	 * artificial cycling of Aff1, Aff2 and Aff3 values. IOW, these end up

	 * having absolutely no relationship to the actual underlying system

	 * topology, and cannot be reasonably used as core / package ID.

	 *

	 * If the MT bit is set, Aff0 *could* be used to define a thread ID, but

	 * we still wouldn't be able to obtain a sane core ID. This means we

	 * need to entirely ignore MPIDR for any topology deduction.

	/*

	 * if the PPTT doesn't have thread information, assume a homogeneous

	 * machine and return the current CPU's thread state.

/*

 * Propagate the topology information of the processor_topology_node tree to the

 * cpu_topology array.

			/*

			 * this is the only part of cpu_topology that has

			 * a direct relationship with the cache topology

	/*

	 * Pre-compute the fixed ratio between the frequency of the constant

	 * reference counter and the maximum frequency of the CPU.

	 *

	 *			    ref_rate

	 * arch_max_freq_scale =   ---------- * SCHED_CAPACITY_SCALE²

	 *			    max_rate

	 *

	 * We use a factor of 2 * SCHED_CAPACITY_SHIFT -> SCHED_CAPACITY_SCALE²

	 * in order to ensure a good resolution for arch_max_freq_scale for

	 * very low reference frequencies (down to the KHz range which should

	 * be unlikely).

	/*

	 *	    /\core    arch_max_freq_scale

	 * scale =  ------- * --------------------

	 *	    /\const   SCHED_CAPACITY_SCALE

	 *

	 * See validate_cpu_freq_invariance_counters() for details on

	 * arch_max_freq_scale and the use of SCHED_CAPACITY_SHIFT.

 We are already set since the last insmod of cpufreq driver */

	/*

	 * We don't need to handle CPUFREQ_REMOVE_POLICY event as the AMU

	 * counters don't have any dependency on cpufreq driver once we have

	 * initialized AMU support and enabled invariance. The AMU counters will

	 * keep on working just fine in the absence of the cpufreq driver, and

	 * for the CPUs for which there are no counters available, the last set

	 * value of arch_freq_scale will remain valid as that is the frequency

	 * those CPUs are running at.

	/*

	 * Abort call on counterless CPU or when interrupts are

	 * disabled - can lead to deadlock in smp sync call.

/*

 * Refer to drivers/acpi/cppc_acpi.c for the description of the functions

 * below.

 CONFIG_ACPI_CPPC_LIB */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ARMv8 single-step debug support and mdscr context switching.

 *

 * Copyright (C) 2012 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 Determine debug architecture. */

/*

 * MDSCR access routines.

/*

 * Allow root to disable self-hosted debug from userspace.

 * This is useful if you want to connect an external JTAG debugger.

/*

 * Keep track of debug users on each core.

 * The ref counts are per-cpu so we use a local_t type.

/*

 * OS lock clearing.

/*

 * Single step API and exception handling.

/*

 * Call registered single step handlers

 * There is no Syndrome info to check for determining the handler.

 * So we call all the registered handlers, until the right handler is

 * found which returns zero.

	/*

	 * Since single-step exception disables interrupt, this function is

	 * entirely not preemptible, and we can use rcu list safely here.

	/*

	 * If we are stepping a pending breakpoint, call the hw_breakpoint

	 * handler first.

		/*

		 * ptrace will disable single step unless explicitly

		 * asked to re-enable it. For other clients, it makes

		 * sense to leave it enabled (i.e. rewind the controls

		 * to the active-not-pending state).

		/*

		 * Re-enable stepping since we know that we will be

		 * returning to regs.

	/*

	 * Since brk exception disables interrupt, this function is

	 * entirely not preemptible, and we can use rcu list safely here.

 get 16-bit Thumb instruction */

 get second half of 32-bit Thumb-2 instruction */

 32-bit ARM instruction */

 Re-enable single step for syscall restarting. */

	/*

	 * If single step is active for this thread, then set SPSR.SS

	 * to 1 to avoid returning to the active-pending state.

 Kernel API */

 ptrace API */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013 Huawei Ltd.

 * Author: Jiang Liu <liuj97@gmail.com>

 *

 * Based on arch/arm/kernel/jump_label.c

	/*

	 * We use the architected A64 NOP in arch_static_branch, so there's no

	 * need to patch an identical A64 NOP over the top of it here. The core

	 * will call arch_jump_label_transform from a module notifier if the

	 * NOP needs to be replaced by a branch.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/ftrace.c

 *

 * Copyright (C) 2013 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

/*

 * Replace a single instruction, which may be a branch or NOP.

 * If @validate == true, a replaced instruction is checked against 'old'.

	/*

	 * Note:

	 * We are paranoid about modifying text, as if a bug were to happen, it

	 * could cause us to read or write to someplace that could cause harm.

	 * Carefully read and modify the code with aarch64_insn_*() which uses

	 * probe_kernel_*(), and make sure what we read is what we expected it

	 * to be before modifying it.

/*

 * Replace tracer function in ftrace_caller()

/*

 * Turn on the call to ftrace_caller() in instrumented function

		/*

		 * On kernels that support module PLTs, the offset between the

		 * branch instruction and its target may legally exceed the

		 * range of an ordinary relative 'bl' opcode. In this case, we

		 * need to branch via a trampoline in the module.

		 *

		 * NOTE: __module_text_address() must be called with preemption

		 * disabled, but we can rely on ftrace_lock to ensure that 'mod'

		 * retains its validity throughout the remainder of this code.

/*

 * The compiler has inserted two NOPs before the regular function prologue.

 * All instrumented functions follow the AAPCS, so x0-x8 and x19-x30 are live,

 * and x9-x18 are free for our use.

 *

 * At runtime we want to be able to swing a single NOP <-> BL to enable or

 * disable the ftrace call. The BL requires us to save the original LR value,

 * so here we insert a <MOV X9, LR> over the first NOP so the instructions

 * before the regular prologue are:

 *

 * | Compiled | Disabled   | Enabled    |

 * +----------+------------+------------+

 * | NOP      | MOV X9, LR | MOV X9, LR |

 * | NOP      | NOP        | BL <entry> |

 *

 * The LR value will be recovered by ftrace_regs_entry, and restored into LR

 * before returning to the regular function prologue. When a function is not

 * being traced, the MOV is not harmful given x9 is not live per the AAPCS.

 *

 * Note: ftrace_process_locs() has pre-adjusted rec->ip to be the address of

 * the BL.

/*

 * Turn off the call to ftrace_caller() in instrumented function

		/*

		 * 'mod' is only set at module load time, but if we end up

		 * dealing with an out-of-range condition, we can assume it

		 * is due to a module being loaded far away from the kernel.

		/*

		 * The instruction we are about to patch may be a branch and

		 * link instruction that was redirected via a PLT entry. In

		 * this case, the normal validation will fail, but we can at

		 * least check that we are dealing with a branch and link

		 * instruction that points into the right module.

 CONFIG_DYNAMIC_FTRACE */

/*

 * function_graph tracer expects ftrace_return_to_handler() to be called

 * on the way back to parent. For this purpose, this function is called

 * in _mcount() or ftrace_caller() to replace return address (*parent) on

 * the call stack to return_to_handler.

 *

 * Note that @frame_pointer is used only for sanity check later.

	/*

	 * Note:

	 * No protection against faulting at *parent, which may be seen

	 * on other archs. It's unlikely on AArch64.

/*

 * Turn on/off the call to ftrace_graph_caller() in ftrace_caller()

 * depending on @enable.

 CONFIG_DYNAMIC_FTRACE */

 CONFIG_FUNCTION_GRAPH_TRACER */

 SPDX-License-Identifier: GPL-2.0-only

 discarded with init text/data */

/*

 * In ARMv8-A, A64 instructions have a fixed length of 32 bits and are always

 * little-endian.

 A64 instructions must be word aligned */

 The first CPU becomes master */

 Notify other processors with an additional increment. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Contains CPU specific errata definitions

 *

 * Copyright (C) 2014 ARM Ltd.

	/*

	 * We want to make sure that all the CPUs in the system expose

	 * a consistent CTR_EL0 to make sure that applications behaves

	 * correctly with migration.

	 *

	 * If a CPU has CTR_EL0.IDC but does not advertise it via CTR_EL0 :

	 *

	 * 1) It is safe if the system doesn't support IDC, as CPU anyway

	 *    reports IDC = 0, consistent with the rest.

	 *

	 * 2) If the system has IDC, it is still safe as we trap CTR_EL0

	 *    access on this CPU via the ARM64_HAS_CACHE_IDC capability.

	 *

	 * So, we need to make sure either the raw CTR_EL0 or the effective

	 * CTR_EL0 matches the system's copy to allow a secondary CPU to boot.

 Trap CTR_EL0 access on this CPU, only if it has a mismatch */

 ... or if the system is affected by an erratum */

 Errata affecting a range of revisions of  given model variant */

 Errata affecting a single variant/revision of a model */

 Errata affecting all variants/revisions of a given a model */

 Errata affecting a list of midr ranges, with same work around */

 Cavium ThunderX, T88 pass 1.x - 2.1 */

 Cavium ThunderX, T81 pass 1.0 */

 Cavium ThunderX, T88 pass 1.x - 2.2 */

 Cavium ThunderX, T81 pass 1.0 - 1.2 */

 Cavium ThunderX, T83 pass 1.0 */

 Cortex-A53 r0p[012]: ARM errata 826319, 827319, 824069 */

 Cortex-A53 r0p[01] : ARM errata 819472 */

/*

 * - 1188873 affects r0p0 to r2p0

 * - 1418040 affects r0p0 to r3p1

 Cortex-A76 r0p0 to r3p1 */

 Neoverse-N1 r0p0 to r3p1 */

 Kryo4xx Gold (rcpe to rfpf) => (r0p0 to r3p1) */

 Cortex-A53 r0p[01234] */

 Brahma-B53 r0p[0] */

 Kryo2XX Silver rAp4 */

 Cortex-A53 r0p[01234] */

 Brahma-B53 r0p[0] */

 Cortex A76 r0p0 to r2p0 */

 Cortex A55 r0p0 to r2p0 */

 Kryo4xx Silver (rdpe => r1p0) */

 Cortex-A76 r0p0 - r3p1 */

 Kryo4xx Gold (rcpe to rfpf) => (r0p0 to r3p1) */

 CONFIG_ARM64_WORKAROUND_TRBE_OVERWRITE_FILL_MODE */

 CONFIG_ARM64_WORKAROUND_TSB_FLUSH_FAILURE */

 CONFIG_ARM64_WORKAROUND_TRBE_WRITE_OUT_OF_RANGE */

 Cortex-A57 r0p0 - r1p2 */

 Cortex-A57 r0p0 - r1p2 */

 Cavium ThunderX, pass 1.x */

 Cortex-A73 all versions */

 Must come after the Spectre-v2 entry */

		/*

		 * We need to allow affected CPUs to come in late, but

		 * also need the non-affected CPUs to be able to come

		 * in at any point in time. Wonderful.

 we depend on the firmware portion for correctness */

 we depend on the firmware portion for correctness */

 NVIDIA Carmel */

		/*

		 * The erratum work around is handled within the TRBE

		 * driver and can be applied per-cpu. So, we can allow

		 * a late CPU to come online with this erratum.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 Linaro, Ltd. <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Contains CPU feature definitions

 *

 * Copyright (C) 2015 ARM Ltd.

 *

 * A note for the weary kernel hacker: the code here is confusing and hard to

 * follow! That's partly because it's solving a nasty problem, but also because

 * there's a little bit of over-abstraction that tends to obscure what's going

 * on behind a maze of helper functions and macros.

 *

 * The basic problem is that hardware folks have started gluing together CPUs

 * with distinct architectural features; in some cases even creating SoCs where

 * user-visible instructions are available only on a subset of the available

 * cores. We try to address this by snapshotting the feature registers of the

 * boot CPU and comparing these with the feature registers of each secondary

 * CPU when bringing them up. If there is a mismatch, then we update the

 * snapshot state to indicate the lowest-common denominator of the feature,

 * known as the "safe" value. This snapshot state can be queried to view the

 * "sanitised" value of a feature register.

 *

 * The sanitised register values are used to decide which capabilities we

 * have in the system. These may be in the form of traditional "hwcaps"

 * advertised to userspace or internal "cpucaps" which are used to configure

 * things like alternative patching and static keys. While a feature mismatch

 * may result in a TAINT_CPU_OUT_OF_SPEC kernel taint, a capability mismatch

 * may prevent a CPU from being onlined at all.

 *

 * Some implementation details worth remembering:

 *

 * - Mismatched features are *always* sanitised to a "safe" value, which

 *   usually indicates that the feature is not supported.

 *

 * - A mismatched feature marked with FTR_STRICT will cause a "SANITY CHECK"

 *   warning when onlining an offending CPU and the kernel will be tainted

 *   with TAINT_CPU_OUT_OF_SPEC.

 *

 * - Features marked as FTR_VISIBLE have their sanitised value visible to

 *   userspace. FTR_VISIBLE features in registers that are only visible

 *   to EL0 by trapping *must* have a corresponding HWCAP so that late

 *   onlining of CPUs cannot lead to features disappearing at runtime.

 *

 * - A "feature" is typically a 4-bit register field. A "capability" is the

 *   high-level description derived from the sanitised field value.

 *

 * - Read the Arm ARM (DDI 0487F.a) section D13.1.3 ("Principles of the ID

 *   scheme for fields in ID registers") to understand when feature fields

 *   may be signed or unsigned (FTR_SIGNED and FTR_UNSIGNED accordingly).

 *

 * - KVM exposes its own view of the feature registers to guest operating

 *   systems regardless of FTR_VISIBLE. This is typically driven from the

 *   sanitised register values to allow virtual CPUs to be migrated between

 *   arbitrary physical CPUs, but some features not present on the host are

 *   also advertised and emulated. Look at sys_reg_descs[] for the gory

 *   details.

 *

 * - If the arm64_ftr_bits[] for a register has a missing field, then this

 *   field is treated as STRICT RES0, including for read_sanitised_ftr_reg().

 *   This is stronger than FTR_HIDDEN and can be used to hide features from

 *   KVM guests.

 Kernel representation of AT_HWCAP and AT_HWCAP2 */

 Need also bit for ARM64_CB_PATCH */

/*

 * Permit PER_LINUX32 and execve() of 32-bit binaries even if not all CPUs

 * support it?

/*

 * Static branch enabled only if allow_mismatched_32bit_el0 is set and we have

 * seen at least one CPU capable of 32-bit EL0.

/*

 * Mask of CPUs supporting 32-bit EL0.

 * Only valid if arm64_mismatched_32bit_el0 is enabled.

/*

 * Flag to indicate if we have computed the system wide

 * capabilities based on the boot time active CPUs. This

 * will be used to determine if a new booting CPU should

 * go through the verification process to make sure that it

 * supports the system capabilities, without using a hotplug

 * notifier. This is also used to decide if we could use

 * the fast path for checking constant CPU caps.

 file-wide pr_fmt adds "CPU features: " prefix */

 Define a feature with unsigned values */

 Define a feature with a signed value */

/*

 * NOTE: Any changes to the visibility of features should be kept in

 * sync with the documentation of the CPU feature register ABI.

	/*

	 * Page size not being supported at Stage-2 is not fatal. You

	 * just give up KVM if PAGE_SIZE isn't supported there. Go fix

	 * your favourite nesting hypervisor.

	 *

	 * There is a small corner case where the hypervisor explicitly

	 * advertises a given granule size at Stage-2 (value 2) on some

	 * vCPUs, and uses the fallback to Stage-1 (value 0) for other

	 * vCPUs. Although this is not forbidden by the architecture, it

	 * indicates that the hypervisor is being silly (or buggy).

	 *

	 * We make no effort to cope with this and pretend that if these

	 * fields are inconsistent across vCPUs, then it isn't worth

	 * trying to bring KVM up.

	/*

	 * We already refuse to boot CPUs that don't support our configured

	 * page size, so we can only detect mismatches for a page size other

	 * than the one we're currently using. Unfortunately, SoCs like this

	 * exist in the wild so, even though we don't like it, we'll have to go

	 * along with it and treat them as non-strict.

 Linux shouldn't care about secure memory */

	/*

	 * Differing PARange is fine as long as all peripherals and memory are mapped

	 * within the minimum PARange of all CPUs

 RES1 */

	/*

	 * Linux can handle differing I-cache policies. Userspace JITs will

	 * make use of *minLine.

	 * If we have differing I-cache policies, report it as the weakest - VIPT.

 L1Ip */

	/*

	 * We can instantiate multiple PMU instances with different levels

	 * of support.

	/*

	 * SpecSEI = 1 indicates that the PE might generate an SError on an

	 * external abort on speculative read. It is safe to assume that an

	 * SError might be generated than it will not be. Hence it has been

	 * classified as FTR_HIGHER_SAFE.

 [31:28] TraceFilt */

 LEN */

/*

 * Common ftr bits for a 32bit register with all hidden, strict

 * attributes, with 4bit feature fields and a default safe value of

 * 0. Covers the following 32bit registers:

 * id_isar[1-4], id_mmfr[1-3], id_pfr1, mvfr[0-1]

 Table for a single 32bit feature value */

 Op1 = 0, CRn = 0, CRm = 1 */

 Op1 = 0, CRn = 0, CRm = 2 */

 Op1 = 0, CRn = 0, CRm = 3 */

 Op1 = 0, CRn = 0, CRm = 4 */

 Op1 = 0, CRn = 0, CRm = 5 */

 Op1 = 0, CRn = 0, CRm = 6 */

 Op1 = 0, CRn = 0, CRm = 7 */

 Op1 = 0, CRn = 1, CRm = 2 */

 Op1 = 1, CRn = 0, CRm = 0 */

 Op1 = 3, CRn = 0, CRm = 0 */

 Op1 = 3, CRn = 14, CRm = 0 */

/*

 * get_arm64_ftr_reg_nowarn - Looks up a feature register entry using

 * its sys_reg() encoding. With the array arm64_ftr_regs sorted in the

 * ascending order of sys_id, we use binary search to find a matching

 * entry.

 *

 * returns - Upon success,  matching ftr_reg entry for id.

 *         - NULL on failure. It is upto the caller to decide

 *	     the impact of a failure.

/*

 * get_arm64_ftr_reg - Looks up a feature register entry using

 * its sys_reg() encoding. This calls get_arm64_ftr_reg_nowarn().

 *

 * returns - Upon success,  matching ftr_reg entry for id.

 *         - NULL on failure but with an WARN_ON().

	/*

	 * Requesting a non-existent register search is an error. Warn

	 * and let the caller handle it.

		/*

		 * Features here must be sorted in descending order with respect

		 * to their shift values and should not overlap with each other.

			/*

			 * Skip the first feature. There is nothing to

			 * compare against for now.

		/*

		 * Skip the first register. There is nothing to

		 * compare against for now.

		/*

		 * Registers here must be sorted in ascending order with respect

		 * to sys_id for subsequent binary search in get_arm64_ftr_reg()

		 * to work correctly.

/*

 * Initialise the CPU feature register from Boot CPU values.

 * Also initiliases the strict_mask for the register.

 * Any bits that are not covered by an arm64_ftr_bits entry are considered

 * RES0 for the system-wide value, and must strictly match.

 Unsafe, remove the override */

 Override was valid */

 Override was the safe value */

 Before we start using the tables, make sure it is sorted */

	/*

	 * Initialize the indirect array of CPU hwcaps capabilities pointers

	 * before we handle the boot CPU below.

	/*

	 * Detect and enable early CPU capabilities based on the boot CPU,

	 * after we have initialised the CPU feature infrastructure.

 Find a safe value */

 Bogus field? */

	/*

	 * If we don't have AArch32 at EL1, then relax the strictness of

	 * EL1-dependent register fields to avoid spurious sanity check fails.

	/*

	 * Regardless of the value of the AuxReg field, the AIFSR, ADFSR, and

	 * ACTLR formats could differ across CPUs and therefore would have to

	 * be trapped for virtualization anyway.

/*

 * Update system wide CPU feature registers with the values from a

 * non-boot CPU. Also performs SANITY checks to make sure that there

 * aren't any insane variations from that of the boot CPU.

	/*

	 * The kernel can handle differing I-cache policies, but otherwise

	 * caches should look identical. Userspace JITs will make use of

	 * *minLine.

	/*

	 * Userspace may perform DC ZVA instructions. Mismatched block sizes

	 * could result in too much or too little memory being zeroed if a

	 * process is preempted and migrated between CPUs.

 If different, timekeeping will be broken (especially with KVM) */

	/*

	 * The kernel uses self-hosted debug features and expects CPUs to

	 * support identical debug features. We presently need CTX_CMPs, WRPs,

	 * and BRPs to be identical.

	 * ID_AA64DFR1 is currently RES0.

	/*

	 * Even in big.LITTLE, processors should be identical instruction-set

	 * wise.

	/*

	 * Differing PARange support is fine as long as all peripherals and

	 * memory are mapped within the minimum PARange of all CPUs.

	 * Linux should not care about secure memory.

 Probe vector lengths, unless we already gave up on SVE */

	/*

	 * The kernel uses the LDGM/STGM instructions and the number of tags

	 * they read/write depends on the GMID_EL1.BS field. Check that the

	 * value is the same on all CPUs.

	/*

	 * If we don't have AArch32 at all then skip the checks entirely

	 * as the register values may be UNKNOWN and we're not going to be

	 * using them for anything.

	 *

	 * This relies on a sanitised view of the AArch64 ID registers

	 * (e.g. SYS_ID_AA64PFR0_EL1), so we call it last.

	/*

	 * Mismatched CPU features are a recipe for disaster. Don't even

	 * pretend to support them.

/*

 * __read_sysreg_by_encoding() - Used by a STARTING cpu before cpuinfo is populated.

 * Read the system register on the current CPU

 Cavium ThunderX pass 1.x and 2.x */

	/*

	 * If the CPU exposes raw CTR_EL0.IDC = 0, while effectively

	 * CTR_EL0.IDC = 1 (from CLIDR values), we need to trap accesses

	 * to the CTR_EL0 on this CPU and emulate it with the real/safe

	 * value.

	/*

	 * Kdump isn't guaranteed to power-off all secondary CPUs, CNP

	 * may share TLB entries with a CPU stuck in the crashed

	 * kernel.

/*

 * This check is triggered during the early boot before the cpufeature

 * is initialised. Checking the status on the local CPU allows the boot

 * CPU to detect the need for non-global mappings and thus avoiding a

 * pagetable re-write after all the CPUs are booted. This check will be

 * anyway run on individual CPUs, allowing us to get the consistent

 * state once the SMP CPUs are up and thus make the switch to non-global

 * mappings if required.

	/*

	 * E0PD does a similar job to KPTI so can be used instead

	 * where available.

	/*

	 * Systems affected by Cavium erratum 24756 are incompatible

	 * with KPTI.

 0: not forced, >0: forced on, <0: forced off */

 List of CPUs that are not vulnerable and don't need KPTI */

 sentinel */ }

 Defer to CPU feature registers */

	/*

	 * For reasons that aren't entirely clear, enabling KPTI on Cavium

	 * ThunderX leads to apparent I-cache corruption of kernel text, which

	 * ends as well as you might imagine. Don't even try. We cannot rely

	 * on the cpus_have_*cap() helpers here to detect the CPU erratum

	 * because cpucap detection order may change. However, since we know

	 * affected CPUs are always in a homogeneous configuration, it is

	 * safe to rely on this_cpu_has_cap() here.

 Useful for KASLR robustness */

 Forced? */

	/*

	 * We don't need to rewrite the page-tables if either we've done

	 * it already or we have KASLR enabled and therefore have not

	 * created any global mappings at all.

 CONFIG_UNMAP_KERNEL_AT_EL0 */

 List of CPUs which have broken DBM support. */

 Kryo4xx Silver (rdpe => r1p0) */

	/*

	 * DBM is a non-conflicting feature. i.e, the kernel can safely

	 * run a mix of CPUs with and without the feature. So, we

	 * unconditionally enable the capability to allow any late CPU

	 * to use the feature. We only enable the control bits on the

	 * CPU, if it actually supports.

	 *

	 * We have to make sure we print the "feature" detection only

	 * when at least one CPU actually uses it. So check if this CPU

	 * can actually use it and print the message exactly once.

	 *

	 * This is safe as all CPUs (including secondary CPUs - due to the

	 * LOCAL_CPU scope - and the hotplugged CPUs - via verification)

	 * goes through the "matches" check exactly once. Also if a CPU

	 * matches the criteria, it is guaranteed that the CPU will turn

	 * the DBM on, as the capability is unconditionally enabled.

/*

 * The "amu_cpus" cpumask only signals that the CPU implementation for the

 * flagged CPUs supports the Activity Monitors Unit (AMU) but does not provide

 * information regarding all the events that it supports. When a CPU bit is

 * set in the cpumask, the user of this feature can only rely on the presence

 * of the 4 fixed counters for that CPU. But this does not guarantee that the

 * counters are enabled or access to these counters is enabled by code

 * executed at higher exception levels (firmware).

	/*

	 * The AMU extension is a non-conflicting feature: the kernel can

	 * safely run a mix of CPUs with and without support for the

	 * activity monitors extension. Therefore, unconditionally enable

	 * the capability to allow any late CPU to use the feature.

	 *

	 * With this feature unconditionally enabled, the cpu_enable

	 * function will be called for all CPUs that match the criteria,

	 * including secondary and hotplugged, marking this feature as

	 * present on that respective CPU. The enable function will also

	 * print a detection message.

	/*

	 * Copy register values that aren't redirected by hardware.

	 *

	 * Before code patching, we only set tpidr_el1, all CPUs need to copy

	 * this value to tpidr_el2 before we patch the code. Once we've done

	 * that, freshly-onlined CPUs will set tpidr_el2, so we don't need to

	 * do anything here.

 Check that CLIDR_EL1.LOU{U,IS} are both 0 */

	/*

	 * We modify PSTATE. This won't work from irq context as the PSTATE

	 * is discarded once we return from the exception.

 CONFIG_ARM64_PAN */

 Firmware may have left a deferred SError in this register. */

 CONFIG_ARM64_RAS_EXTN */

 We don't expect to be called with SCOPE_SYSTEM */

	/*

	 * The ptr-auth feature levels are not intercompatible with lower

	 * levels. Hence we must match ptr-auth feature level of the secondary

	 * CPUs with that of the boot CPU. The level of boot cpu is fetched

	 * from the sanitised register whereas direct register read is done for

	 * the secondary CPUs.

	 * The sanitised feature state is guaranteed to match that of the

	 * boot CPU as a mismatched secondary CPU is parked before it gets

	 * a chance to update the state, with the capability.

 Now check for the secondary CPUs with SCOPE_LOCAL_CPU scope */

 CONFIG_ARM64_PTR_AUTH */

 CONFIG_ARM64_E0PD */

	/*

	 * Use of X16/X17 for tail-calls and trampolines that jump to

	 * function entry points using BR is a requirement for

	 * marking binaries with GNU_PROPERTY_AARCH64_FEATURE_1_BTI.

	 * So, be strict and forbid other BRs using other registers to

	 * jump onto a PACIxSP instruction:

 CONFIG_ARM64_BTI */

	/*

	 * Clear the tags in the zero page. This needs to be done via the

	 * linear map which has the Tagged attribute.

 CONFIG_ARM64_MTE */

 CONFIG_KVM */

 Internal helper functions to match cpu capability type */

 CONFIG_ARM64_PAN */

 CONFIG_ARM64_EPAN */

 CONFIG_ARM64_LSE_ATOMICS */

		/*

		 * The ID feature fields below are used to indicate that

		 * the CPU doesn't need KPTI. See unmap_kernel_at_el0 for

		 * more details.

 FP/SIMD is not implemented */

 CONFIG_ARM64_SVE */

 CONFIG_ARM64_RAS_EXTN */

		/*

		 * The feature is enabled by default if CONFIG_ARM64_AMU_EXTN=y.

		 * Therefore, don't provide .desc as we don't want the detection

		 * message to be shown until at least one CPU is detected to

		 * support the feature.

 CONFIG_ARM64_AMU_EXTN */

		/*

		 * Since we turn this on always, we don't want the user to

		 * think that the feature is available when it may not be.

		 * So hide the description.

		 *

		 * .desc = "Hardware pagetable Dirty Bit Management",

		 *

 CONFIG_ARM64_PTR_AUTH */

		/*

		 * Depends on having GICv3

 CONFIG_ARM64_MTE */

 CONFIG_ARM64_MTE */

	/*

	 * Check that all of MVFR1_EL1.{SIMDSP, SIMDInt, SIMDLS} are available,

	 * in line with that of arm32 as in vfp_init(). We make sure that the

	 * check is future proof, by making sure value is non-zero.

 Arm v8 mandates MVFR0.FPDP == {0, 2}. So, piggy back on this for the presence of VFP support */

 Check if we have a particular HWCAP enabled */

 We support emulation of accesses to CPU ID feature registers */

/*

 * Enable all the available capabilities on this CPU. The capabilities

 * with BOOT_CPU scope are handled separately and hence skipped here.

/*

 * Run through the enabled capabilities and enable() it on all active

 * CPUs

 Ensure cpus_have_const_cap(num) works */

			/*

			 * Capabilities with SCOPE_BOOT_CPU scope are finalised

			 * before any secondary CPU boots. Thus, each secondary

			 * will enable the capability as appropriate via

			 * check_local_cpu_capabilities(). The only exception is

			 * the boot CPU, for which the capability must be

			 * enabled here. This approach avoids costly

			 * stop_machine() calls for this case.

	/*

	 * For all non-boot scope capabilities, use stop_machine()

	 * as it schedules the work allowing us to modify PSTATE,

	 * instead of on_each_cpu() which uses an IPI, giving us a

	 * PSTATE that disappears when we return.

/*

 * Run through the list of capabilities to check for conflicts.

 * If the system has already detected a capability, take necessary

 * action on this CPU.

			/*

			 * Check if the new CPU misses an advertised feature,

			 * which is not safe to miss.

			/*

			 * We have to issue cpu_enable() irrespective of

			 * whether the CPU has it or not, as it is enabeld

			 * system wide. It is upto the call back to take

			 * appropriate action on this CPU.

			/*

			 * Check if the CPU has this capability if it isn't

			 * safe to have when the system doesn't.

/*

 * Check for CPU features that are used in early boot

 * based on the Boot CPU value.

 Add checks on other ZCR bits here if necessary */

 Verify VMID bits */

 Verify IPA range */

/*

 * Run through the enabled system capabilities and enable() it on this CPU.

 * The capabilities were decided based on the available CPUs at the boot time.

 * Any new CPU should match the system wide status of the capability. If the

 * new CPU doesn't have a capability which the system now has enabled, we

 * cannot do anything to fix it up and could cause unexpected failures. So

 * we park the CPU.

	/*

	 * The capabilities with SCOPE_BOOT_CPU are checked from

	 * check_early_cpu_features(), as they need to be verified

	 * on all secondary CPUs.

	/*

	 * All secondary CPUs should conform to the early CPU features

	 * in use by the kernel based on boot CPU.

	/*

	 * If we haven't finalised the system capabilities, this CPU gets

	 * a chance to update the errata work arounds and local features.

	 * Otherwise, this CPU should verify that it has all the system

	 * advertised capabilities.

 Detect capabilities with either SCOPE_BOOT_CPU or SCOPE_LOCAL_CPU */

 Enable the SCOPE_BOOT_CPU capabilities alone right away */

/*

 * This helper function is used in a narrow window when,

 * - The system wide safe registers are set with all the SMP CPUs and,

 * - The SYSTEM_FEATURE cpu_hwcaps may not have been set.

 * In all other cases cpus_have_{const_}cap() should be used.

	/*

	 * We currently only populate the first 32 bits of AT_HWCAP. Please

	 * note that for userspace compatibility we guarantee that bits 62

	 * and 63 will always be returned as 0.

	/*

	 * We have finalised the system-wide safe feature

	 * registers, finalise the capabilities that depend

	 * on it. Also enable all the available capabilities,

	 * that are not enabled already.

 Advertise that we have computed the system capabilities */

	/*

	 * Check for sane CTR_EL0.CWG value.

	/*

	 * The first 32-bit-capable CPU we detected and so can no longer

	 * be offlined by userspace. -1 indicates we haven't yet onlined

	 * a 32-bit-capable CPU.

	/*

	 * We've detected a mismatch. We need to keep one of our CPUs with

	 * 32-bit EL0 online so that is_cpu_allowed() doesn't end up rejecting

	 * every CPU in the system for a 32-bit task.

/*

 * We emulate only the following system register space.

 * Op0 = 0x3, CRn = 0x0, Op1 = 0x0, CRm = [0, 4 - 7]

 * See Table C5-6 System instruction encodings for System register accesses,

 * ARMv8 ARM(ARM DDI 0487A.f) for more details.

/*

 * With CRm == 0, reg should be one of :

 * MIDR_EL1, MPIDR_EL1 or REVIDR_EL1.

 IMPLEMENTATION DEFINED values are emulated with 0 */

		/*

		 * The untracked registers are either IMPLEMENTATION DEFINED

		 * (e.g, ID_AFR0_EL1) or reserved RAZ.

	/*

	 * sys_reg values are defined as used in mrs/msr instruction.

	 * shift the imm value to get the encoding.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Code borrowed from powerpc/kernel/pci-common.c

 *

 * Copyright (C) 2003 Anton Blanchard <anton@au.ibm.com>, IBM

 * Copyright (C) 2014 ARM Ltd.

/*

 * Try to assign the IRQ number when probing a new device

/*

 * raw_pci_read/write - Platform-specific PCI config space access.

 config space mapping */

	/*

	 * On Hyper-V there is no corresponding ACPI device for a root bridge,

	 * therefore ->parent is set as NULL by the driver. And set 'adev' as

	 * NULL in this case because there is no proper ACPI device.

/*

 * Lookup the bus range for the domain in MCFG, and set up config space

 * mapping.

 release_info: free resources allocated by init_info */

 Interface called from ACPI code to setup PCI host controller */

 If we must preserve the resource configuration, claim now */

	/*

	 * Assign whatever was left unassigned. If we didn't claim above,

	 * this will reassign everything.

 SPDX-License-Identifier: GPL-2.0

/*

 * kexec_file for arm64

 *

 * Copyright (C) 2018 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

 *

 * Most code is derived from arm64 port of kexec-tools

 for exclusion of crashkernel region */

 Exclude crashkernel region */

/*

 * Tries to add the initrd and DTB to the image. If it is not possible to find

 * valid locations, this function will undo changes to the image and return non

 * zero.

 not allocate anything below the kernel */

 load elf core header */

 largest supported page size */

 load initrd */

 within 1GB-aligned window of up to 32GB in size */

 load dtb */

 trim it */

 not across 2MB boundary */

 SPDX-License-Identifier: GPL-2.0

/*

 * Kexec image loader



 * Copyright (C) 2018 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

	/*

	 * We require a kernel with an unambiguous Image header. Per

	 * Documentation/arm64/booting.rst, this is the case when image_size

	 * is non-zero (practically speaking, since v3.17).

 Check cpu features */

 Load the kernel */

 Adjust kernel segment with TEXT_OFFSET */

	/*

	 * The location of the kernel segment may make it impossible to satisfy

	 * the other segment requirements, so we try repeatedly to find a

	 * location that will work.

 Try to load additional data */

		/*

		 * We couldn't find space for the other segments; erase the

		 * kernel segment and try the next available hole.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ARMv8 PMUv3 Performance Events handling code.

 *

 * Copyright (C) 2012 ARM Limited

 * Author: Will Deacon <will.deacon@arm.com>

 *

 * This code is based heavily on the ARMv7 perf event code.

 ARMv8 Cortex-A53 specific event types. */

 ARMv8 Cavium ThunderX specific event types. */

/*

 * ARMv8 Architectural defined events, not all of these may

 * be supported on any given implementation. Unsupported events will

 * be disabled at run-time based on the PMCEID registers.

 Don't expose the chain event in /sys, since it's useless in isolation */

 Encoded as Log2(number of bytes), plus one */

/*

 * Perf Events' indices

/*

 * We unconditionally enable ARMv8.5-PMU long event counter support

 * (64-bit events) where supported. Indicate if this arm_pmu has long

 * event counter support.

/*

 * We must chain two programmable counters for 64 bit events,

 * except when we have allocated the 64bit cycle counter (for CPU

 * cycles event). This must be called only when the event has

 * a counter allocated.

/*

 * ARMv8 low level PMU access

/*

 * Perf Event to low level counters mapping

/*

 * This code is really good

/*

 * The cycle counter is always a 64-bit counter. When ARMV8_PMU_PMCR_LP

 * is set the event counters also become 64-bit counters. Unless the

 * user has requested a long counter (attr.config1) then we want to

 * interrupt upon 32-bit overflow - we achieve this by applying a bias.

	/*

	 * For chained events, the low counter is programmed to count

	 * the event of interest and the high counter is programmed

	 * with CHAIN event code with filters set to count at all ELs.

	/*

	 * Make sure event configuration register writes are visible before we

	 * enable the counter.

 We rely on the hypervisor switch code to enable guest counters */

	/*

	 * Make sure the effects of disabling the counter are visible before we

	 * start configuring the event.

 We rely on the hypervisor switch code to disable guest counters */

 Clear the overflow flag in case an interrupt is pending. */

 Read */

 Write to clear flags */

	/*

	 * Enable counter and interrupt, and set the counter to count

	 * the event that we're interested in.

	/*

	 * Disable counter

	/*

	 * Set event.

	/*

	 * Enable interrupt for this counter

	/*

	 * Enable counter

	/*

	 * Disable counter

	/*

	 * Disable interrupt for this counter

 Enable all counters */

 Disable all counters */

	/*

	 * Get and reset the IRQ flags

	/*

	 * Did an overflow occur?

	/*

	 * Handle the counter(s) overflow(s)

	/*

	 * Stop the PMU while processing the counter overflows

	 * to prevent skews in group events.

 Ignore if we don't have an event. */

		/*

		 * We have a single interrupt for all counters. Check that

		 * each counter has overflowed before we process it.

		/*

		 * Perf event overflow will queue the processing of the event as

		 * an irq_work which will be taken care of in the handling of

		 * IPI_IRQ_WORK.

	/*

	 * Chaining requires two consecutive event counters, where

	 * the lower idx must be even.

 Check if the preceding even counter is available */

 Release the Odd counter */

 Always prefer to place a cycle counter into the cycle counter. */

	/*

	 * Otherwise use events counters

/*

 * Add an event filter to a given event.

	/*

	 * If we're running in hyp mode, then we *are* the hypervisor.

	 * Therefore we ignore exclude_hv in this configuration, since

	 * there's no hypervisor to sample anyway. This is consistent

	 * with other architectures (x86 and Power).

	/*

	 * Filter out !VHE kernels and guest kernels

	/*

	 * Install the filter into config_base as this is used to

	 * construct the event type.

 The counter and interrupt enable registers are unknown at reset. */

 Clear the counters we flip at guest entry/exit */

	/*

	 * Initialize & Reset PMNC. Request overflow interrupt for

	 * 64 bit cycle counter but cheat in armv8pmu_write_counter().

 Enable long event counter support where available */

 Only expose micro/arch events supported by this PMU */

 Read the nb of CNTx counters supported from PMNC */

 Add the CPU cycles counter */

 store PMMIR_EL1 register for sysfs */

		/*

		 * Subtract the cycle base, such that software that

		 * doesn't know about cap_user_time_short still 'works'

		 * assuming no wraps.

	/*

	 * time_shift is not expected to be greater than 31 due to

	 * the original published conversion algorithm shifting a

	 * 32-bit value (now specifies a 64-bit value) - refer

	 * perf_event_mmap_page documentation in perf_event.h.

	/*

	 * Internal timekeeping for enabled/running/stopped times

	 * is always computed with the sched_clock.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/signal.c

 *

 * Copyright (C) 1995-2009 Russell King

 * Copyright (C) 2012 ARM Ltd.

 * Modified by Will Deacon <will.deacon@arm.com>

 Something that isn't a valid magic number for any coprocessor.  */

/*

 * VFP save/restore code.

 *

 * We have to be careful with endianness, since the fpsimd context-switch

 * code operates on 128-bit (Q) register values whereas the compat ABI

 * uses an array of 64-bit (D) registers. Consequently, we need to swap

 * the two halves of each Q register when running on a big-endian CPU.

	/*

	 * Save the hardware registers to the fpsimd_state structure.

	 * Note that this also saves V16-31, which aren't visible

	 * in AArch32.

 Place structure header on the stack */

	/*

	 * Now copy the FP registers. Since the registers are packed,

	 * we can copy the prefix we want (V0-V15) as it is.

 Create an AArch32 fpscr from the fpsr and the fpcr. */

	/*

	 * The exception register aren't available so we fake up a

	 * basic FPEXC and zero everything else.

 Copy the FP registers into the start of the fpsimd_state. */

 Extract the fpsr and the fpcr from the fpscr */

	/*

	 * We don't need to touch the exception register, so

	 * reload the hardware state.

	/*

	 * Avoid compat_sys_sigreturn() restarting.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Since we stacked the signal on a 64-bit boundary,

	 * then 'sp' should be word aligned here.  If it's

	 * not, then the user is trying to mess with us.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Since we stacked the signal on a 64-bit boundary,

	 * then 'sp' should be word aligned here.  If it's

	 * not, then the user is trying to mess with us.

	/*

	 * ATPCS B01 mandates 8-byte alignment

	/*

	 * Check that we can actually write to the signal frame.

 Check if the handler is written for ARM or Thumb */

 The IT state must be cleared for both ARM and Thumb-2 */

 Restore the original endianness */

 Set up sigreturn pointer */

 set the compat FSR WnR */

/*

 * 32-bit signal handling routines called from signal.c

/*

 * Compile-time assertions for siginfo_t offsets. Check NSIG* as well, as

 * changes likely come with new fields that should be added below.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) Linaro.

 * Copyright (C) Huawei Futurewei Technologies.

 Please note VMCOREINFO_NUMBER() uses "%d", not "%x" */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/sys32.c

 *

 * Copyright (C) 2015 ARM Ltd.

/*

 * Needed to avoid conflicting __NR_* macros between uapi/asm/unistd.h and

 * asm/unistd32.h.

	/*

	 * 32-bit ARM applies an OABI compatibility fixup to statfs64 and

	 * fstatfs64 regardless of whether OABI is in use, and therefore

	 * arbitrary binaries may rely upon it, so we must do the same.

	 * For more details, see commit:

	 *

	 * 713c481519f19df9 ("[ARM] 3108/2: old ABI compat: statfs64 and

	 * fstatfs64")

 see aarch32_statfs64 */

/*

 * Note: off_4k is always in units of 4K. If we can't do the

 * requested offset because it is not page-aligned, we return -EINVAL.

 SPDX-License-Identifier: GPL-2.0

 Copyright (C) 2017 Arm Ltd.

/*

 * VMAP'd stacks checking for stack overflow on exception using sp as a scratch

 * register, meaning SDEI has to switch to its own stack. We need two stacks as

 * a critical event may interrupt a normal event that has just taken a

 * synchronous exception, and is using sp as scratch register. For a critical

 * event interrupting a normal event, we can't reliably tell if we were on the

 * sdei stack.

 * For now, we allocate stacks when the driver is probed.

	/*

	 * SDEI works between adjacent exception levels. If we booted at EL1 we

	 * assume a hypervisor is marshalling events. If we booted at EL2 and

	 * dropped to EL1 because we don't support VHE, then we can't support

	 * SDEI.

 CONFIG_UNMAP_KERNEL_AT_EL0 */

/*

 * do_sdei_event() returns one of:

 *  SDEI_EV_HANDLED -  success, return to the interrupted context.

 *  SDEI_EV_FAILED  -  failure, return this error code to firmare.

 *  virtual-address -  success, return to this address.

 +SPSel */

 Retrieve the missing registers values */

 from within the handler, this call always succeeds */

		/*

		 * We took a synchronous exception from the SDEI handler.

		 * This could deadlock, and if you interrupt KVM it will

		 * hyp-panic instead.

	/*

	 * If we interrupted the kernel with interrupts masked, we always go

	 * back to wherever we came from.

	/*

	 * Otherwise, we pretend this was an IRQ. This lets user space tasks

	 * receive signals before we return to them, and KVM to invoke it's

	 * world switch to do the same.

	 *

	 * See DDI0487B.a Table D1-7 'Vector offsets from vector table base

	 * address'.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014-2017 Linaro Ltd. <ard.biesheuvel@linaro.org>

	/*

	 * Check whether both entries refer to the same target:

	 * do the cheapest checks first.

	 * If the 'add' or 'br' opcodes are different, then the target

	 * cannot be the same.

	/*

	 * If the 'adrp' opcodes are the same then we just need to check

	 * that they refer to the same 4k region.

	/*

	 * Check if the entry we just created is a duplicate. Given that the

	 * relocations are sorted, this will be the last entry we allocated.

	 * (if one exists).

 get the destination register of the ADRP instruction */

 sort by type, symbol index and addend */

	/*

	 * Entries are sorted by type, symbol index and addend. That means

	 * that, if a duplicate entry exists, it must be in the preceding

	 * slot.

			/*

			 * We only have to consider branch targets that resolve

			 * to symbols that are defined in a different section.

			 * This is not simply a heuristic, it is a fundamental

			 * limitation, since there is no guaranteed way to emit

			 * PLT entries sufficiently close to the branch if the

			 * section size exceeds the range of a branch

			 * instruction. So ignore relocations against defined

			 * symbols if they live in the same section as the

			 * relocation target.

			/*

			 * Jump relocations with non-zero addends against

			 * undefined symbols are supported by the ELF spec, but

			 * do not occur in practice (e.g., 'jump n bytes past

			 * the entry point of undefined function symbol f').

			 * So we need to support them, but there is no need to

			 * take them into consideration when trying to optimize

			 * this code. So let's only check for duplicates when

			 * the addend is zero: this allows us to record the PLT

			 * entry address in the symbol table itself, rather than

			 * having to search the list for duplicates each time we

			 * emit one.

			/*

			 * Determine the minimal safe alignment for this ADRP

			 * instruction: the section alignment at which it is

			 * guaranteed not to appear at a vulnerable offset.

			 *

			 * This comes down to finding the least significant zero

			 * bit in bits [11:3] of the section offset, and

			 * increasing the section's alignment so that the

			 * resulting address of this instruction is guaranteed

			 * to equal the offset in that particular bit (as well

			 * as all less signficant bits). This ensures that the

			 * address modulo 4 KB != 0xfff8 or 0xfffc (which would

			 * have all ones in bits [11:3])

			/*

			 * Allocate veneer space for each ADRP that may appear

			 * at a vulnerable offset nonetheless. At relocation

			 * time, some of these will remain unused since some

			 * ADRP instructions can be patched to ADR instructions

			 * instead.

		/*

		 * Add some slack so we can skip PLT slots that may trigger

		 * the erratum due to the placement of the ADRP instruction.

 Group branch PLT relas at the front end of the array. */

	/*

	 * Find the empty .plt section so we can expand it to store the PLT

	 * entries. Record the symtab address as well.

 ignore relocations that operate on non-exec sections */

		/*

		 * sort branch relocations requiring a PLT by type, symbol index

		 * and addend

 SPDX-License-Identifier: GPL-2.0-only

/*

 * SMP initialisation and IPI support

 * Based on arch/arm/kernel/smp.c

 *

 * Copyright (C) 2012 ARM Ltd.

/*

 * as from 2.5, kernels no longer have an init_tasks structure

 * so we need some other way of telling a new secondary core

 * where to place its SVC stack

 Number of CPUs which aren't online, but looping in kernel text. */

/*

 * Boot a secondary CPU, and assign it the specified idle task.

 * This also gives us the initial stack to use for this CPU.

	/*

	 * We need to tell the secondary core where to find its stack and the

	 * page tables.

 Now bring the CPU into our world */

	/*

	 * CPU was successfully started, wait for it to come online or

	 * time out.

/*

 * This is the secondary CPU boot entry.  We're using this CPUs

 * idle thread stack, but a set of temporary page tables.

	/*

	 * All kernel threads share the same mm context; grab a

	 * reference and switch to it.

	/*

	 * TTBR0 is only used for the identity mapping at this stage. Make it

	 * point to zero page to avoid speculatively fetching new entries.

	/*

	 * If the system has established the capabilities, make sure

	 * this CPU ticks all of those. If it doesn't, the CPU will

	 * fail to come online.

	/*

	 * Log the CPU info before it is marked online and might get read.

	/*

	 * Enable GIC and timers.

	/*

	 * OK, now it's safe to let the boot CPU continue.  Wait for

	 * the CPU migration code to notice that the CPU is online

	 * before we continue.

	/*

	 * OK, it's off to the idle thread for us

	/*

	 * If we don't have a cpu_die method, abort before we reach the point

	 * of no return. CPU0 may not have an cpu_ops, so test for it.

	/*

	 * We may need to abort a hot unplug for some other mechanism-specific

	 * reason.

/*

 * __cpu_disable runs on the processor to be shutdown.

	/*

	 * Take this CPU offline.  Once we clear this, we can't return,

	 * and we must not schedule until we're ready to give up the cpu.

	/*

	 * OK - migrate IRQs away from this CPU

	/*

	 * If we have no means of synchronising with the dying CPU, then assume

	 * that it is really dead. We can only wait for an arbitrary length of

	 * time and hope that it's dead, so let's skip the wait and just hope.

/*

 * called on the thread which is asking for a CPU to be shutdown -

 * waits until shutdown has completed, or it is timed out.

	/*

	 * Now that the dying CPU is beyond the point of no return w.r.t.

	 * in-kernel synchronisation, try to get the firwmare to help us to

	 * verify that it has really left the kernel before we consider

	 * clobbering anything it might still be using.

/*

 * Called from the idle thread for the CPU which has been shutdown.

 *

 Tell __cpu_die() that this CPU is now safe to dispose of */

	/*

	 * Actually shutdown the CPU. This must never fail. The specific hotplug

	 * mechanism must perform all required cache maintenance to ensure that

	 * no dirty lines are lost in the process of shutting down the CPU.

/*

 * Kill the calling secondary CPU, early in bringup before it is turned

 * online.

 Mark this CPU absent */

	/*

	 * The runtime per-cpu areas have been allocated by

	 * setup_per_cpu_areas(), and CPU0's boot time per-cpu area will be

	 * freed shortly, so we must move over to the runtime per-cpu area.

	/*

	 * We now know enough about the boot CPU to apply the

	 * alternatives that cannot wait until interrupt handling

	 * and/or scheduling is enabled.

 Conditionally switch to GIC PMR for interrupt masking */

/*

 * Duplicate MPIDRs are a recipe for disaster. Scan all initialized

 * entries and check for duplicates. If any is found just ignore the

 * cpu. cpu_logical_map was initialized to INVALID_HWID to avoid

 * matching valid MPIDR values.

/*

 * Initialize cpu operations for a logical cpu and

 * set it in the possible mask on success

/*

 * acpi_map_gic_cpu_interface - parse processor MADT entry

 *

 * Carry out sanity checks on MADT processor entry and initialize

 * cpu_logical_map on success

 Check if GICC structure of boot CPU is available in the MADT */

 map the logical cpu id to cpu MPIDR */

	/*

	 * Set-up the ACPI parking protocol cpu entries

	 * while initializing the cpu_logical_map to

	 * avoid parsing MADT entries multiple times for

	 * nothing (ie a valid cpu_logical_map entry should

	 * contain a valid parking protocol data set to

	 * initialize the cpu if the parking protocol is

	 * the only available enable method).

	/*

	 * do a walk of MADT to determine how many CPUs

	 * we have including disabled CPUs, and get information

	 * we need for SMP init.

	/*

	 * In ACPI, SMP and CPU NUMA information is provided in separate

	 * static tables, namely the MADT and the SRAT.

	 *

	 * Thus, it is simpler to first create the cpu logical map through

	 * an MADT walk and then map the logical cpus to their node ids

	 * as separate steps.

/*

 * Enumerate the possible CPU set from the device tree and build the

 * cpu logical map array containing MPIDR values related to logical

 * cpus. Assumes that cpu_logical_map(0) has already been initialized.

		/*

		 * The numbering scheme requires that the boot CPU

		 * must be assigned logical id 0. Record it so that

		 * the logical map built from DT is validated and can

		 * be used.

			/*

			 * cpu_logical_map has already been

			 * initialized and the boot cpu doesn't need

			 * the enable-method so continue without

			 * incrementing cpu.

/*

 * Enumerate the possible CPU set from the device tree or ACPI and build the

 * cpu logical map array containing MPIDR values related to logical

 * cpus. Assumes that cpu_logical_map(0) has already been initialized.

	/*

	 * We need to set the cpu_logical_map entries before enabling

	 * the cpus so that cpu processor description entries (DT cpu nodes

	 * and ACPI MADT entries) can be retrieved by matching the cpu hwid

	 * with entries in cpu_logical_map while initializing the cpus.

	 * If the cpu set-up fails, invalidate the cpu_logical_map entry.

	/*

	 * If UP is mandated by "nosmp" (which implies "maxcpus=0"), don't set

	 * secondary CPUs present.

	/*

	 * Initialise the present map (which describes the set of CPUs

	 * actually populated at the present time) and release the

	 * secondaries from the bootloader.

/*

 * We need to implement panic_smp_self_stop() for parallel panic() calls, so

 * that cpu_online_mask gets correctly updated and smp_send_stop() can skip

 * CPUs that have already stopped themselves.

 just in case */

/*

 * Main handler for inter-processor interrupts

 Setup the boot CPU immediately */

/*

 * The number of CPUs online, not counting this CPU (which may not be

 * fully online and so not counted in num_online_cpus()).

 Wait up to one second for other CPUs to stop */

	/*

	 * This function can be called twice in panic path, but obviously

	 * we execute this only once.

	/*

	 * If this cpu is the only one alive at this point in time, online or

	 * not, there are no stop messages to be sent around, so just back out.

 Wait up to one second for other CPUs to stop */

/*

 * not supported here

 SPDX-License-Identifier: GPL-2.0

/*

 * Early cpufeature override framework

 *

 * Copyright (C) 2020 Google LLC

 * Author: Marc Zyngier <maz@kernel.org>

	/*

	 * If we ever reach this point while running VHE, we're

	 * guaranteed to be on one of these funky, VHE-stuck CPUs. If

	 * the user was trying to force nVHE on us, proceed with

	 * attitude adjustment.

			/*

			 * If an override gets filtered out, advertise

			 * it by setting the value to 0xf, but

			 * clearing the mask... Yes, this is fragile.

 Keep checkers quiet */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/signal.c

 *

 * Copyright (C) 1995-2009 Russell King

 * Copyright (C) 2012 ARM Ltd.

/*

 * Do a signal return; undo the signal stack. These are aligned to 128-bit.

 size of allocated sigframe data */

 largest allowed size */

 Reserve space for extension and terminator ^ */

/*

 * Sanity limit on the approximate maximum size of signal frame we'll

 * try to generate.  Stack alignment padding and the frame record are

 * not taken into account.  This limit is not a guarantee and is

 * NOT ABI.

 Reserve space for the __reserved[] terminator */

		/*

		 * Allow expansion up to SIGFRAME_MAXSZ, ensuring space for

		 * the terminator:

 Still not enough space?  Bad luck! */

/*

 * Allocate space for an optional record of <size> bytes in the user

 * signal frame.  The offset from the signal frame base address to the

 * allocated block is assigned to *offset.

 Allocate the null terminator record and prevent further allocations */

 Un-reserve the space reserved for the terminator: */

 Prevent further allocation: */

 copy the FP and status/control registers */

 copy the magic/size information */

 check the magic/size information */

 copy the FP and status/control registers */

 load the hardware registers from the fpsimd_state structure */

		/*

		 * This assumes that the SVE state has already been saved to

		 * the task struct by calling the function

		 * fpsimd_signal_preserve_current_state().

	/*

	 * Careful: we are about __copy_from_user() directly into

	 * thread.sve_state with preemption enabled, so protection is

	 * needed to prevent a racing context switch from writing stale

	 * registers back over the new data.

 From now, fpsimd_thread_switch() won't touch thread.sve_state */

 copy the FP and status/control registers */

 restore_sigframe() already checked that user->fpsimd != NULL. */

 load the hardware registers from the fpsimd_state structure */

 ! CONFIG_ARM64_SVE */

 Turn any non-optimised out attempts to use these into a link error: */

 ! CONFIG_ARM64_SVE */

 ignore */

 Check for the dummy terminator in __reserved[]: */

 Prevent looping/repeated parsing of extra_context */

 Reject "unreasonably large" frames: */

			/*

			 * Ignore trailing terminator in __reserved[]

			 * and start parsing extra data:

	/*

	 * Avoid sys_rt_sigreturn() restarting.

 Always make any pending restarted system calls return -EINTR */

	/*

	 * Since we stacked the signal on a 128-bit boundary, then 'sp' should

	 * be word aligned here.

/*

 * Determine the layout of optional records in the signal frame

 *

 * add_all: if true, lays out the biggest possible signal frame for

 *	this task; otherwise, generates a layout for the current state

 *	of the task.

 fault information, if valid */

 set up the stack frame for unwinding */

 fault information, if valid */

 Scalable Vector Extension state, if present */

		/*

		 * extra_datap is just written to the signal frame.

		 * The value gets cast back to a void __user *

		 * during sigreturn.

 Add the terminator */

 set the "end" magic */

	/*

	 * Check that we can actually write to the signal frame.

	/*

	 * Signal delivery is a (wacky) indirect function call in

	 * userspace, so simulate the same setting of BTYPE as a BLR

	 * <register containing the signal handler entry point>.

	 * Signal delivery to a location in a PROT_BTI guarded page

	 * that is not a function entry point will now trigger a

	 * SIGILL in userspace.

	 *

	 * If the signal handler entry point is not in a PROT_BTI

	 * guarded page, this is harmless.

 TCO (Tag Check Override) always cleared for signal handlers */

/*

 * OK, we're invoking a handler

	/*

	 * Set up the stack frame

	/*

	 * Check that the resulting registers are actually sane.

 Step into the signal handler if we are stepping */

/*

 * Note that 'init' is a special process: it doesn't get signals it doesn't

 * want to handle. Thus you cannot kill init even with a SIGKILL even by

 * mistake.

 *

 * Note that we go through the signals twice: once to check the signals that

 * the kernel can handle, and then we build all the user-level signal handling

 * stack-frames in one go after that.

	/*

	 * If we were from a system call, check for system call restarting...

		/*

		 * Avoid additional syscall restarting via ret_to_user.

		/*

		 * Prepare for system call restart. We do this here so that a

		 * debugger will see the already changed PC.

	/*

	 * Get the signal to deliver. When running under ptrace, at this point

	 * the debugger may change all of our registers.

		/*

		 * Depending on the signal settings, we may need to revert the

		 * decision to restart the system call, but skip this if a

		 * debugger has chosen to restart at a different PC.

	/*

	 * Handle restarting a different system call. As above, if a debugger

	 * has chosen to restart at a different PC, ignore the restart.

 Unmask Debug and SError for the next task */

/*

 * Determine the stack space required for guaranteed signal devliery.

 * This function is used to populate AT_MINSIGSTKSZ at process startup.

 * cpufeatures setup is assumed to be complete.

	/*

	 * If this fails, SIGFRAME_MAXSZ needs to be enlarged.  It won't

	 * be big enough, but it's our best guess:

 max alignment padding */

/*

 * Compile-time assertions for siginfo_t offsets. Check NSIG* as well, as

 * changes likely come with new fields that should be added below.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AArch64 KGDB support

 *

 * Based on arch/arm/kernel/kgdb.c

 *

 * Copyright (C) 2013 Cavium Inc.

 * Author: Vijaya Kumar K <vijaya.kumar@caviumnetworks.com>

	/*

	 * struct pt_regs thinks PSTATE is 64-bits wide but gdb remote

	 * protocol disagrees. Therefore we must extract only the lower

	 * 32-bits. Look for the big comment in asm/kgdb.h for more

	 * detail.

 Initialize to zero */

		/*

		 * Packet D (Detach), k (kill). No special handling

		 * is required here. Handle same as c packet.

		/*

		 * Packet c (Continue) to continue executing.

		 * Set pc to required address.

		 * Try to read optional parameter and set pc.

		 * If this was a compiled breakpoint, we need to move

		 * to the next instruction else we will just breakpoint

		 * over and over again.

		/*

		 * Received continue command, disable single step

		/*

		 * Update step address value with address passed

		 * with step packet.

		 * On debug exception return PC is copied to ELR

		 * So just update PC.

		 * If no step address is passed, resume from the address

		 * pointed by PC. Do not update PC

		/*

		 * Enable single step handling

	/*

	 * Want to be lowest priority

/*

 * kgdb_arch_init - Perform any architecture specific initialization.

 * This function will handle the initialization of any architecture

 * specific callbacks.

/*

 * kgdb_arch_exit - Perform any architecture specific uninitalization.

 * This function will handle the uninitalization of any architecture

 * specific callbacks, for dynamic registration and unregistration.

 SPDX-License-Identifier: GPL-2.0

/*

 * Exception handling code

 *

 * Copyright (C) 2019 ARM Ltd.

/*

 * Handle IRQ/context state management when entering from kernel mode.

 * Before this function is called it is not safe to call regular kernel code,

 * intrumentable code, or any code which may trigger an exception.

 *

 * This is intended to match the logic in irqentry_enter(), handling the kernel

 * mode transitions only.

/*

 * Handle IRQ/context state management when exiting to kernel mode.

 * After this function returns it is not safe to call regular kernel code,

 * intrumentable code, or any code which may trigger an exception.

 *

 * This is intended to match the logic in irqentry_exit(), handling the kernel

 * mode transitions only, and with preemption handled elsewhere.

/*

 * Handle IRQ/context state management when entering from user mode.

 * Before this function is called it is not safe to call regular kernel code,

 * intrumentable code, or any code which may trigger an exception.

/*

 * Handle IRQ/context state management when exiting to user mode.

 * After this function returns it is not safe to call regular kernel code,

 * intrumentable code, or any code which may trigger an exception.

/*

 * Handle IRQ/context state management when entering an NMI from user/kernel

 * mode. Before this function is called it is not safe to call regular kernel

 * code, intrumentable code, or any code which may trigger an exception.

/*

 * Handle IRQ/context state management when exiting an NMI from user/kernel

 * mode. After this function returns it is not safe to call regular kernel

 * code, intrumentable code, or any code which may trigger an exception.

/*

 * Handle IRQ/context state management when entering a debug exception from

 * kernel mode. Before this function is called it is not safe to call regular

 * kernel code, intrumentable code, or any code which may trigger an exception.

/*

 * Handle IRQ/context state management when exiting a debug exception from

 * kernel mode. After this function returns it is not safe to call regular

 * kernel code, intrumentable code, or any code which may trigger an exception.

	/*

	 * DAIF.DA are cleared at the start of IRQ/FIQ handling, and when GIC

	 * priority masking is used the GIC irqchip driver will clear DAIF.IF

	 * using gic_arch_enable_irqs() for normal IRQs. If anything is set in

	 * DAIF we must have handled an NMI, so skip preemption.

	/*

	 * Preempting a task from an IRQ means we leave copies of PSTATE

	 * on the stack. cpufeature's enable calls may modify PSTATE, but

	 * resuming one of these preempted tasks would undo those changes.

	 *

	 * Only allow a task to be preempted once cpufeatures have been

	 * enabled.

 We will have taken a single-step exception by this point */

	/*

	 * We've taken a dummy step exception from the kernel to ensure

	 * that interrupts are re-enabled on the syscall path. Return back

	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions

	 * masked so that we can safely restore the mdscr and get on with

	 * handling the syscall.

 CONFIG_ARM64_ERRATUM_1463225 */

 CONFIG_ARM64_ERRATUM_1463225 */

	/*

	 * We don't handle ESR_ELx_EC_SP_ALIGN, since we will have hit a

	 * recursive exception when trying to push the initial pt_regs.

	/*

	 * Note: thread_info::preempt_count includes both thread_info::count

	 * and thread_info::need_resched, and is not equivalent to

	 * preempt_count().

	/*

	 * We've taken an instruction abort from userspace and not yet

	 * re-enabled IRQs. If the address is a kernel address, apply

	 * BP hardening prior to enabling IRQs and pre-emption.

 Only watchpoints write FAR_EL1, otherwise its UNKNOWN */

 CONFIG_COMPAT */

 CONFIG_COMPAT */

 CONFIG_VMAP_STACK */

	/*

	 * We didn't take an exception to get here, so the HW hasn't

	 * set/cleared bits in PSTATE that we may rely on.

	 *

	 * The original SDEI spec (ARM DEN 0054A) can be read ambiguously as to

	 * whether PSTATE bits are inherited unchanged or generated from

	 * scratch, and the TF-A implementation always clears PAN and always

	 * clears UAO. There are no other known implementations.

	 *

	 * Subsequent revisions (ARM DEN 0054B) follow the usual rules for how

	 * PSTATE is modified upon architectural exceptions, and so PAN is

	 * either inherited or set per SCTLR_ELx.SPAN, and UAO is always

	 * cleared.

	 *

	 * We must explicitly reset PAN to the expected state, including

	 * clearing it when the host isn't using it, in case a VM had it set.

 CONFIG_ARM_SDE_INTERFACE */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/ptrace.c

 *

 * By Ross Biro 1/23/92

 * edited by Linus Torvalds

 * ARM modifications Copyright (C) 2000 Russell King

 * Copyright (C) 2012 ARM Ltd.

/**

 * regs_query_register_offset() - query register offset from its name

 * @name:	the name of a register

 *

 * regs_query_register_offset() returns the offset of a register in struct

 * pt_regs from its name. If the name is invalid, this returns -EINVAL;

/**

 * regs_within_kernel_stack() - check the address in the stack

 * @regs:      pt_regs which contains kernel stack pointer.

 * @addr:      address which is checked.

 *

 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).

 * If @addr is within the kernel stack, it returns true. If not, returns false.

/**

 * regs_get_kernel_stack_nth() - get Nth entry of the stack

 * @regs:	pt_regs which contains kernel stack pointer.

 * @n:		stack entry number.

 *

 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which

 * is specified by @regs. If the @n th entry is NOT in the kernel stack,

 * this returns 0.

/*

 * TODO: does not yet catch signals sent when the child dies.

 * in exit.c or in signal.c.

/*

 * Called by kernel/ptrace.c when detaching..

	/*

	 * This would be better off in core code, but PTRACE_DETACH has

	 * grown its fair share of arch-specific worts and changing it

	 * is likely to cause regressions on obscure architectures.

/*

 * Handle hitting a HW-breakpoint.

/*

 * Unregister breakpoints from this task and reset the pointers in

 * the thread_struct.

	/*

	 * Initialise fields to sane defaults

	 * (i.e. values that will pass validation).

 Resource info */

 (address, ctrl) registers */

 Resource info and pad */

 (address, ctrl) registers */

 CONFIG_HAVE_HW_BREAKPOINT */

/*

 * TODO: update fp accessors for lazy context switching (sync/flush hwstate)

	/*

	 * Ensure target->thread.uw.fpsimd_state is up to date, so that a

	 * short copyin can't resurrect stale data.

 Header */

 Registers: FPSIMD-only case */

 Otherwise: full SVE case */

	/*

	 * Copy fpsr, and fpcr which must follow contiguously in

	 * struct fpsimd_state:

 Header */

	/*

	 * Apart from SVE_PT_REGS_MASK, all SVE_PT_* flags are consumed by

	 * sve_set_vector_length(), which will also validate them for us:

 Actual VL set may be less than the user asked for: */

 Registers: FPSIMD-only case */

 Otherwise: full SVE case */

	/*

	 * If setting a different VL from the requested VL and there is

	 * register data, the data layout will be wrong: don't even

	 * try to set the registers in this case.

	/*

	 * Ensure target->thread.sve_state is up to date with target's

	 * FPSIMD regs, so that a short copyin leaves trailing registers

	 * unmodified.

	/*

	 * Copy fpsr, and fpcr which must follow contiguously in

	 * struct fpsimd_state:

 CONFIG_ARM64_SVE */

	/*

	 * The PAC bits can differ across data and instruction pointers

	 * depending on TCR_EL1.TBID*, which we may make use of in future, so

	 * we expose separate masks.

 CONFIG_CHECKPOINT_RESTORE */

 CONFIG_ARM64_PTR_AUTH */

		/*

		 * We pretend we have 32-bit registers because the fpsr and

		 * fpcr are 32-bits wide.

 Scalable Vector Extension */

 this cannot be set dynamically */

 Calculate the number of AArch32 registers contained in count */

 Convert pos into an register number */

	/*

	 * The VFP registers are packed into the fpsimd_state, so they all sit

	 * nicely together for us. We just need to create the fpscr separately.

/*

 * Convert a virtual register number into an index for a thread_info

 * breakpoint array. Breakpoints are identified using positive numbers

 * whilst watchpoints are negative. The registers are laid out as pairs

 * of (address, control), each pair mapping to a unique hw_breakpoint struct.

 * Register 0 is reserved for describing resource information.

 Watchpoint */

 Resource info */

 Breakpoint */

 CONFIG_HAVE_HW_BREAKPOINT */

 CONFIG_COMPAT */

	/*

	 * Core dumping of 32-bit tasks or compat ptrace requests must use the

	 * user_aarch32_view compatible with arm32. Native ptrace requests on

	 * 32-bit children use an extended user_aarch32_ptrace_view to allow

	 * access to the TLS register.

	/*

	 * We have some ABI weirdness here in the way that we handle syscall

	 * exit stops because we indicate whether or not the stop has been

	 * signalled from syscall entry or syscall exit by clobbering a general

	 * purpose register (ip/r12 for AArch32, x7 for AArch64) in the tracee

	 * and restoring its old value after the stop. This means that:

	 *

	 * - Any writes by the tracer to this register during the stop are

	 *   ignored/discarded.

	 *

	 * - The actual value of the register is not available during the stop,

	 *   so the tracer cannot save it and restore it later.

	 *

	 * - Syscall stops behave differently to seccomp and pseudo-step traps

	 *   (the latter do not nobble any registers).

		/*

		 * Signal a pseudo-step exception since we are stepping but

		 * tracer modifications to the registers may have rewound the

		 * state machine.

 Do the secure computing after ptrace; failures should be fast. */

/*

 * SPSR_ELx bits which are always architecturally RES0 per ARM DDI 0487D.a.

 * We permit userspace to set SSBS (AArch64 bit 12, AArch32 bit 23) which is

 * not described in ARM DDI 0487D.a.

 * We treat PAN and UAO as RES0 bits, as they are meaningless at EL0, and may

 * be allocated an EL0 meaning in future.

 * Userspace cannot use these until they have an architectural meaning.

 * Note that this follows the SPSR_ELx format, not the AArch32 PSR format.

 * We also reserve IL for the kernel; SS is handled dynamically.

	/*

	 * Force PSR to a valid 32-bit EL0t, preserving the same bits as

	 * arch/arm.

 Force PSR to a valid 64-bit EL0t */

/*

 * Are the current registers suitable for user mode? (used to maintain

 * security in signal handlers)

lore.kernel.org/lkml/20191118131525.GA4180@willie-the-truck */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 Citrix Systems

 *

 * Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

 return stolen time in ns by asking the hypervisor */

	/*

	 * paravirt_steal_clock() may be called before the CPU

	 * online notification callback runs. Until the callback

	 * has run we just return zero.

 To detect the presence of PV time support we require SMCCC 1.1+ */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Our handling of compat tasks (PERF_SAMPLE_REGS_ABI_32) is weird, but

	 * we're stuck with it for ABI compatibility reasons.

	 *

	 * For a 32-bit consumer inspecting a 32-bit task, then it will look at

	 * the first 16 registers (see arch/arm/include/uapi/asm/perf_regs.h).

	 * These correspond directly to a prefix of the registers saved in our

	 * 'struct pt_regs', with the exception of the PC, so we copy that down

	 * (x15 corresponds to SP_hyp in the architecture).

	 *

	 * So far, so good.

	 *

	 * The oddity arises when a 64-bit consumer looks at a 32-bit task and

	 * asks for registers beyond PERF_REG_ARM_MAX. In this case, we return

	 * SP_usr, LR_usr and PC in the positions where the AArch64 SP, LR and

	 * PC registers would normally live. The initial idea was to allow a

	 * 64-bit unwinder to unwind a 32-bit task and, although it's not clear

	 * how well that works in practice, somebody might be relying on it.

	 *

	 * At the time we make a sample, we don't know whether the consumer is

	 * 32-bit or 64-bit, so we have to cater for both possibilities.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * CPU kernel entry/exit control

 *

 * Copyright (C) 2013 ARM Ltd.

			/*

			 * The boot CPU may not have an enable method (e.g.

			 * when spin-table is used for secondaries).

			 * Don't warn spuriously.

			/*

			 * In ACPI systems the boot CPU does not require

			 * checking the enable method since for some

			 * boot protocol (ie parking protocol) it need not

			 * be initialized. Don't warn spuriously.

/*

 * Read a cpu's enable method and record it in cpu_ops.

 SPDX-License-Identifier: GPL-2.0-only

/*

 *

 * Copyright (C) 2013 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 Fail early if we don't have CPU_OFF support */

 Trusted OS will deny CPU_OFF */

	/*

	 * There are no known implementations of PSCI actually using the

	 * power state field, pass a sensible default for now.

	/*

	 * cpu_kill could race with cpu_die and we can

	 * potentially end up declaring this cpu undead

	 * while it is dying. So, try again a few times.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2016 Linaro Ltd <ard.biesheuvel@linaro.org>

/*

 * This routine will be executed with the kernel mapped at its default virtual

 * address, and if it returns successfully, the kernel will be remapped, and

 * start_kernel() will be executed from a randomized virtual offset. The

 * relocation will result in all absolute references (e.g., static variables

 * containing function pointers) to be reinitialized, and zero-initialized

 * .bss variables will be reset to 0.

	/*

	 * Set a reasonable default for module_alloc_base in case

	 * we end up running with module randomization disabled.

	/*

	 * Try to map the FDT early. If this fails, we simply bail,

	 * and proceed with KASLR disabled. We will make another

	 * attempt at mapping the FDT in setup_machine()

	/*

	 * Retrieve (and wipe) the seed from the FDT

	/*

	 * Check if 'nokaslr' appears on the command line, and

	 * return 0 if that is the case.

	/*

	 * Mix in any entropy obtainable architecturally if enabled

	 * and supported.

	/*

	 * OK, so we are proceeding with KASLR enabled. Calculate a suitable

	 * kernel image offset from the seed. Let's place the kernel in the

	 * middle half of the VMALLOC area (VA_BITS_MIN - 2), and stay clear of

	 * the lower and upper quarters to avoid colliding with other

	 * allocations.

	 * Even if we could randomize at page granularity for 16k and 64k pages,

	 * let's always round to 2 MB so we don't interfere with the ability to

	 * map using contiguous PTEs

 use the top 16 bits to randomize the linear region */

		/*

		 * KASAN without KASAN_VMALLOC does not expect the module region

		 * to intersect the vmalloc region, since shadow memory is

		 * allocated for each module at load time, whereas the vmalloc

		 * region is shadowed by KASAN zero pages. So keep modules

		 * out of the vmalloc region if KASAN is enabled without

		 * KASAN_VMALLOC, and put the kernel well within 4 GB of the

		 * module region.

		/*

		 * Randomize the module region over a 2 GB window covering the

		 * kernel. This reduces the risk of modules leaking information

		 * about the address of the kernel itself, but results in

		 * branches between modules and the core kernel that are

		 * resolved via PLTs. (Branches between modules will be

		 * resolved normally.)

		/*

		 * Randomize the module region by setting module_alloc_base to

		 * a PAGE_SIZE multiple in the range [_etext - MODULES_VSIZE,

		 * _stext) . This guarantees that the resulting region still

		 * covers [_stext, _etext], and that all relative branches can

		 * be resolved without veneers unless this region is exhausted

		 * and we fall back to a larger 2GB window in module_alloc()

		 * when ARM64_MODULE_PLTS is enabled.

 use the lower 21 bits to randomize the base of the module region */

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  ARM64 Specific Low-Level ACPI Boot Support

 *

 *  Copyright (C) 2013-2014, Linaro Ltd.

 *	Author: Al Stone <al.stone@linaro.org>

 *	Author: Graeme Gregory <graeme.gregory@linaro.org>

 *	Author: Hanjun Guo <hanjun.guo@linaro.org>

 *	Author: Tomasz Nowicki <tomasz.nowicki@linaro.org>

 *	Author: Naresh Bhat <naresh.bhat@linaro.org>

 skip ACPI IRQ initialization */

 skip ACPI PCI scan and IRQ initialization */

 "acpi=off" disables both ACPI table parsing and interpreter */

 prefer ACPI over DT */

 force ACPI to be enabled */

 Core will print when we return error */

	/*

	 * Ignore anything not directly under the root node; we'll

	 * catch its parent instead.

	/*

	 * This node at depth 1 is neither a chosen node nor a xen node,

	 * which we do not expect.

/*

 * __acpi_map_table() will be called before page_init(), so early_ioremap()

 * or early_memremap() should be called here to for ACPI table mapping.

 Whether HVC must be used instead of SMC as the PSCI conduit */

/*

 * acpi_fadt_sanity_check() - Check FADT presence and carry out sanity

 *			      checks on it

 *

 * Return 0 on success,  <0 on failure

	/*

	 * FADT is required on arm64; retrieve it to check its presence

	 * and carry out revision and ACPI HW reduced compliancy tests

	/*

	 * Revision in table header is the FADT Major revision, and there

	 * is a minor revision of FADT which was introduced by ACPI 5.1,

	 * we only deal with ACPI 5.1 or newer revision to get GIC and SMP

	 * boot protocol configuration data.

	/*

	 * acpi_get_table() creates FADT table mapping that

	 * should be released after parsing and before resuming boot

/*

 * acpi_boot_table_init() called from setup_arch(), always.

 *	1. find RSDP and get its address, and then find XSDT

 *	2. extract all tables and checksums them all

 *	3. check ACPI FADT revision

 *	4. check ACPI FADT HW reduced flag

 *

 * We can parse ACPI boot-time tables such as MADT after

 * this function is called.

 *

 * On return ACPI is enabled if either:

 *

 * - ACPI tables are initialized and sanity checks passed

 * - acpi=force was passed in the command line and ACPI was not disabled

 *   explicitly through acpi=off command line parameter

 *

 * ACPI is disabled on function return otherwise

	/*

	 * Enable ACPI instead of device tree unless

	 * - ACPI has been disabled explicitly (acpi=off), or

	 * - the device tree is not empty (it has more than just a /chosen node,

	 *   and a /hypervisor node when running on Xen)

	 *   and ACPI has not been [force] enabled (acpi=on|force)

	/*

	 * ACPI is disabled at this point. Enable it in order to parse

	 * the ACPI tables and carry out sanity checks

	/*

	 * If ACPI tables are initialized and FADT sanity checks passed,

	 * leave ACPI enabled and carry on booting; otherwise disable ACPI

	 * on initialization error.

	 * If acpi=force was passed on the command line it forces ACPI

	 * to be enabled even if its initialization failed.

	/*

	 * Although UEFI specifies the use of Normal Write-through for

	 * EFI_MEMORY_WT, it is seldom used in practice and not implemented

	 * by most (all?) CPUs. Rather than allocate a MAIR just for this

	 * purpose, emit a warning and use Normal Non-cacheable instead.

	/*

	 * According to "Table 8 Map: EFI memory types to AArch64 memory

	 * types" of UEFI 2.5 section 2.3.6.1, each EFI memory type is

	 * mapped to a corresponding MAIR attribute encoding.

	 * The EFI memory attribute advises all possible capabilities

	 * of a memory region.

	/*

	 * It is fine for AML to remap regions that are not represented in the

	 * EFI memory map at all, as it only describes normal memory, and MMIO

	 * regions that require a virtual mapping to make them accessible to

	 * the EFI runtime services.

			/*

			 * Mapping kernel memory is permitted if the region in

			 * question is covered by a single memblock with the

			 * NOMAP attribute set: this enables the use of ACPI

			 * table overrides passed via initramfs, which are

			 * reserved in memory using arch_reserve_mem_area()

			 * below. As this particular use case only requires

			 * read access, fall through to the R/O mapping case.

			/*

			 * This would be unusual, but not problematic per se,

			 * as long as we take care not to create a writable

			 * mapping for executable code.

			/*

			 * ACPI reclaim memory is used to pass firmware tables

			 * and other data that is intended for consumption by

			 * the OS only, which may decide it wants to reclaim

			 * that memory and use it for something else. We never

			 * do that, but we usually add it to the linear map

			 * anyway, in which case we should use the existing

			 * mapping.

/*

 * Claim Synchronous External Aborts as a firmware first notification.

 *

 * Used by KVM and the arch do_sea handler.

 * @regs may be NULL when called from process context.

 current_flags isn't useful here as daif doesn't tell us about pNMI */

	/*

	 * SEA can interrupt SError, mask it and describe this as an NMI so

	 * that APEI defers the handling.

	/*

	 * APEI NMI-like notifications are deferred to irq_work. Unless

	 * we interrupted irqs-masked code, we can do that now.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Routines for doing kexec-based kdump

 *

 * Copyright (C) 2017 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

/**

 * copy_oldmem_page() - copy one page from old kernel memory

 * @pfn: page frame number to be copied

 * @buf: buffer where the copied page is placed

 * @csize: number of bytes to copy

 * @offset: offset in bytes into the page

 * @userbuf: if set, @buf is in a user address space

 *

 * This function copies one page from old kernel memory into buffer pointed by

 * @buf. If @buf is in userspace, set @userbuf to %1. Returns number of bytes

 * copied or negative error in case of failure.

/**

 * elfcorehdr_read - read from ELF core header

 * @buf: buffer where the data is placed

 * @count: number of bytes to read

 * @ppos: address in the memory

 *

 * This function reads @count bytes from elf core header which exists

 * on crash dump kernel's memory.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * AArch64 loadable module support.

 *

 * Copyright (C) 2012 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 Silence the initial allocation */

 don't exceed the static module region - see below */

		/*

		 * KASAN without KASAN_VMALLOC can only deal with module

		 * allocations being served from the reserved module region,

		 * since the remainder of the vmalloc region is already

		 * backed by zero shadow pages, and punching holes into it

		 * is non-trivial. Since the module region is not randomized

		 * when KASAN is enabled without KASAN_VMALLOC, it is even

		 * less likely that the module region gets exhausted, so we

		 * can simply omit this fallback in that case.

	/*

	 * The ELF psABI for AArch64 documents the 16-bit and 32-bit place

	 * relative and absolute relocations as having a range of [-2^15, 2^16)

	 * or [-2^31, 2^32), respectively. However, in order to be able to

	 * detect overflows reliably, we have to choose whether we interpret

	 * such quantities as signed or as unsigned, and stick with it.

	 * The way we organize our address space requires a signed

	 * interpretation of 32-bit relative references, so let's use that

	 * for all R_AARCH64_PRELxx relocations. This means our upper

	 * bound for overflow detection should be Sxx_MAX rather than Uxx_MAX.

		/*

		 * For signed MOVW relocations, we have to manipulate the

		 * instruction encoding depending on whether or not the

		 * immediate is less than zero.

 >=0: Set the instruction to MOVZ (opcode 10b). */

			/*

			 * <0: Set the instruction to MOVN (opcode 00b).

			 *     Since we've masked the opcode already, we

			 *     don't need to do anything other than

			 *     inverting the new immediate field.

 Update the instruction with the new encoding. */

 Calculate the relocation value. */

 Extract the value bits and shift them to bit 0. */

 Update the instruction's immediate field. */

	/*

	 * Extract the upper value bits (including the sign bit) and

	 * shift them to bit 0.

	/*

	 * Overflow has occurred if the upper bits are not all equal to

	 * the sign bit of the value.

 patch ADRP to ADR if it is in range */

 out of range for ADR -> emit a veneer */

 loc corresponds to P in the AArch64 ELF document. */

 sym is the ELF symbol we're referring to. */

 val corresponds to (S + A) in the AArch64 ELF document. */

 Check for overflow by default. */

 Perform the static relocation. */

 Null relocations. */

 Data relocations. */

 MOVW instruction relocations. */

 We're using the top bits so we can't overflow. */

 We're using the top bits so we can't overflow. */

 Immediate instruction relocations. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ARM64 CPU idle arch support

 *

 * Copyright (C) 2014 ARM Ltd.

 * Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

/**

 * arm_cpuidle_suspend() - function to enter a low-power idle state

 * @index: argument to pass to CPU suspend operations

 *

 * Return: 0 on success, -EOPNOTSUPP if CPU suspend hook not initialized, CPU

 * operations back-end error code otherwise.

	/*

	 * If the PSCI cpu_suspend function hook has not been initialized

	 * idle states must not be enabled, so bail out

		/*

		 * Only bits[31:0] represent a PSCI power_state while

		 * bits[63:32] must be 0x0 as per ARM ACPI FFH Specification

 SPDX-License-Identifier: GPL-2.0-only

/*

 * VDSO implementations.

 *

 * Copyright (C) 2012 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 Data Mapping */

 Code Mapping */

 CONFIG_COMPAT_VDSO */

/*

 * The vDSO data page.

 Grab the vDSO code pages. */

/*

 * The vvar mapping contains data for a specific time namespace, so when a task

 * changes namespace we must unmap its vvar data for the old namespace.

 * Subsequent faults will map in data for the new namespace.

 *

 * For more details see timens_setup_vdso_data().

	/*

	 * VM_PFNMAP | VM_IO protect .fault() handler from being called

	 * through interfaces like /proc/$pid/mem or

	 * process_vm_{readv,writev}() as long as there's no .access()

	 * in special_mapping_vmops.

	 * For more details check_vma_flags() and __access_remote_vm()

		/*

		 * If a task belongs to a time namespace then a namespace

		 * specific VVAR is mapped with the VVAR_DATA_PAGE_OFFSET and

		 * the real VVAR page is mapped with the VVAR_TIMENS_PAGE_OFFSET

		 * offset.

		 * See also the comment near timens_setup_vdso_data().

 CONFIG_TIME_NS */

 Be sure to map the data page */

/*

 * Create and map the vectors page for AArch32 tasks.

 kuser helpers */

 ABI */

 ABI */

	/*

	 * Avoid VM_MAYWRITE for compatibility with arch/arm/, where it's

	 * not safe to CoW the page containing the CPU exception vectors.

	/*

	 * VM_MAYWRITE is required to allow gdb to Copy-on-Write and

	 * set breakpoints.

 CONFIG_COMPAT */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * ARM64 ACPI Parking Protocol implementation

 *

 * Authors: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

 *	    Mark Salter <msalter@redhat.com>

	/*

	 * Map mailbox memory with attribute device nGnRE (ie ioremap -

	 * this deviates from the parking protocol specifications since

	 * the mailboxes are required to be mapped nGnRnE; the attribute

	 * discrepancy is harmless insofar as the protocol specification

	 * is concerned).

	 * If the mailbox is mistakenly allocated in the linear mapping

	 * by FW ioremap will fail since the mapping will be prevented

	 * by the kernel (it clashes with the linear mapping attributes

	 * specifications).

	/*

	 * Check if firmware has set-up the mailbox entry properly

	 * before kickstarting the respective cpu.

	/*

	 * stash the mailbox address mapping to use it for further FW

	 * checks in the postboot method

	/*

	 * We write the entry point and cpu id as LE regardless of the

	 * native endianness of the kernel. Therefore, any boot-loaders

	 * that read this address need to convert this address to the

	 * Boot-Loader's endianness before jumping.

	/*

	 * Check if firmware has cleared the entry_point as expected

	 * by the protocol specification.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * alternative runtime patching

 * inspired by the x86 version

 *

 * Copyright (C) 2014 ARM Ltd.

 Volatile, as we may be patching the guts of READ_ONCE() */

/*

 * Check if the target PC is within an alternative block.

		/*

		 * If we're branching inside the alternate sequence,

		 * do not rewrite the instruction, as it is already

		 * correct. Otherwise, generate the new instruction.

		/*

		 * If we're replacing an adrp instruction, which uses PC-relative

		 * immediate addressing, adjust the offset to reflect the new

		 * PC. adrp operates on 4K aligned addresses.

		/*

		 * Disallow patching unhandled instructions using PC relative

		 * literal addresses

/*

 * We provide our own, private D-cache cleaning function so that we don't

 * accidentally call into the cache.S code, which is patched by us at

 * runtime.

		/*

		 * We must clean+invalidate to the PoC in order to avoid

		 * Cortex-A53 errata 826319, 827319, 824069 and 819472

		 * (this corresponds to ARM64_WORKAROUND_CLEAN_CACHE)

 Use ARM64_CB_PATCH as an unconditional patch */

	/*

	 * The core module code takes care of cache maintenance in

	 * flush_module_icache().

 Ignore ARM64_CB bit from feature mask */

/*

 * We might be patching the stop_machine state machine, so implement a

 * really simple polling protocol here.

 We always have a CPU 0 at this point (__init) */

 Barriers provided by the cache flushing */

 better not try code patching on a live SMP system */

/*

 * This is called very early in the boot process (directly after we run

 * a feature detect on the boot CPU). No need to worry about other CPUs

 * here.

 If called on non-boot cpu things could go wrong */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/return_address.c

 *

 * Copyright (C) 2013 Linaro Limited

 * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arm64 callchain support

 *

 * Copyright (C) 2015 ARM Limited

/*

 * Get the return address for a single stackframe and return a pointer to the

 * next frame tail.

 Also check accessibility of one struct frame_tail beyond */

	/*

	 * Frame pointers should strictly progress back up the stack

	 * (towards higher addresses).

/*

 * The registers we're interested in are at the end of the variable

 * length saved register structure. The fp points at the end of this

 * structure so the address of this struct is:

 * (struct compat_frame_tail *)(xxx->fp)-1

 *

 * This code has been adapted from the ARM OProfile support.

 a (struct compat_frame_tail *) in compat mode */

 Also check accessibility of one struct frame_tail beyond */

	/*

	 * Frame pointers should strictly progress back up the stack

	 * (towards higher addresses).

 CONFIG_COMPAT */

 We don't support guest os callchain now */

 AARCH64 mode */

 AARCH32 compat mode */

/*

 * Gets called by walk_stackframe() for every stackframe. This will be called

 * whist unwinding the stackframe and is like a subroutine return so we use

 * the PC.

 We don't support guest os callchain now */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/irq.c

 *

 * Copyright (C) 1992 Linus Torvalds

 * Modifications for ARM processor Copyright (C) 1995-2000 Russell King.

 * Support for Dynamic Tick Timer Copyright (C) 2004-2005 Nokia Corporation.

 * Dynamic Tick Timer written by Tony Lindgren <tony@atomide.com> and

 * Tuukka Tikkanen <tuukka.tikkanen@elektrobit.com>.

 * Copyright (C) 2012 ARM Ltd.

 Only access this in an NMI enter/exit */

 irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned. */

		/*

		 * Now that we have a stack for our IRQ handler, set

		 * the PMR/PSR pair to a consistent state.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/sys_arm.c

 *

 * Copyright (C) People who wrote linux/arch/i386/kernel/sys_i386.c

 * Copyright (C) 1995, 1996 Russell King.

 * Copyright (C) 2012 ARM Ltd.

			/*

			 * The workaround requires an inner-shareable tlbi.

			 * We pick the reserved-ASID to minimise the impact.

/*

 * Handle all unrecognised system calls.

	/*

	 * Flush a region from virtual address 'r0' to virtual address 'r1'

	 * _exclusive_.  There is no alignment requirement on either address;

	 * user space does not need to know the hardware cache layout.

	 *

	 * r2 contains flags.  It should ALWAYS be passed as ZERO until it

	 * is defined to be something else.  For now we ignore it, but may

	 * the fires of hell burn in your belly if you break this rule. ;)

	 *

	 * (at a later date, we may want to allow this call to not flush

	 * various aspects of the cache.  Passing '0' will guarantee that

	 * everything necessary gets flushed to maintain consistency in

	 * the specified region).

		/*

		 * Protect against register corruption from context switch.

		 * See comment in tls_thread_flush.

		/*

		 * Calls 0xf0xxx..0xf07ff are defined to return -ENOSYS

		 * if not implemented, rather than raising SIGILL. This

		 * way the calling program can gracefully determine whether

		 * a feature is supported.

 SPDX-License-Identifier: GPL-2.0

/*

 * This is allocated by cpu_suspend_init(), and used to store a pointer to

 * the 'struct sleep_stack_data' the contains a particular CPUs state.

/*

 * This hook is provided so that cpu_suspend code can restore HW

 * breakpoints as early as possible in the resume path, before reenabling

 * debug exceptions. Code cannot be run from a CPU PM notifier since by the

 * time the notifier runs debug exceptions might have been enabled already,

 * with HW breakpoints registers content still in an unknown state.

 Prevent multiple restore hook initializations */

	/*

	 * We are resuming from reset with the idmap active in TTBR0_EL1.

	 * We must uninstall the idmap and restore the expected MMU

	 * state before we can possibly return to userspace.

 Restore CnP bit in TTBR1_EL1 */

	/*

	 * PSTATE was not saved over suspend/resume, re-enable any detected

	 * features that might not have been set correctly.

	/*

	 * Restore HW breakpoint registers to sane values

	 * before debug exceptions are possibly reenabled

	 * by cpu_suspend()s local_daif_restore() call.

	/*

	 * On resume, firmware implementing dynamic mitigation will

	 * have turned the mitigation on. If the user has forcefully

	 * disabled it, make sure their wishes are obeyed.

 Restore additional feature-specific configuration */

/*

 * cpu_suspend

 *

 * arg: argument to pass to the finisher function

 * fn: finisher function pointer

 *

 Report any MTE async fault before going to suspend */

	/*

	 * From this point debug exceptions are disabled to prevent

	 * updates to mdscr register (saved and restored along with

	 * general purpose registers) from kernel debuggers.

	/*

	 * Function graph tracer state gets inconsistent when the kernel

	 * calls functions that never return (aka suspend finishers) hence

	 * disable graph tracing during their execution.

	/*

	 * Switch to using DAIF.IF instead of PMR in order to reliably

	 * resume if we're using pseudo-NMIs.

 Call the suspend finisher */

		/*

		 * Never gets here, unless the suspend finisher fails.

		 * Successful cpu_suspend() should return from cpu_resume(),

		 * returning through this code path is considered an error

		 * If the return value is set to 0 force ret = -EOPNOTSUPP

		 * to make sure a proper error condition is propagated

	/*

	 * Restore pstate flags. OS lock and mdscr have been already

	 * restored, so from this point onwards, debugging is fully

	 * renabled if it was enabled when core started shutdown.

 ctx_ptr is an array of physical addresses */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Based on arch/arm/kernel/asm-offsets.c

 *

 * Copyright (C) 1995-2003 Russell King

 *               2001-2002 Keith Owens

 * Copyright (C) 2012 ARM Ltd.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/probes/decode-insn.c

 *

 * Copyright (C) 2013 Linaro Limited.

	/*

	 * Branch instructions will write a new value into the PC which is

	 * likely to be relative to the XOL address and therefore invalid.

	 * Deliberate generation of an exception during stepping is also not

	 * currently safe. Lastly, MSR instructions can do any number of nasty

	 * things we can't handle during single-stepping.

		/*

		 * The MRS instruction may not return a correct value when

		 * executing in the single-stepping environment. We do make one

		 * exception, for reading the DAIF bits.

		/*

		 * The HINT instruction is steppable only if it is in whitelist

		 * and the rest of other such instructions are blocked for

		 * single stepping as they may cause exception or other

		 * unintended behaviour.

	/*

	 * Instructions which load PC relative literals are not going to work

	 * when executed from an XOL slot. Instructions doing an exclusive

	 * load/store are not going to complete successfully when single-step

	 * exception handling happens in the middle of the sequence.

/* Return:

 *   INSN_REJECTED     If instruction is one not allowed to kprobe,

 *   INSN_GOOD         If instruction is supported and uses instruction slot,

 *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.

	/*

	 * Instructions reading or modifying the PC won't work from the XOL

	 * slot.

		/*

		 * Instruction cannot be stepped out-of-line and we don't

		 * (yet) simulate it.

		/*

		 * atomic region starts from exclusive load and ends with

		 * exclusive store.

	/*

	 * If there's a symbol defined in front of and near enough to

	 * the probe address assume it is the entry point to this

	 * code and use it to further limit how far back we search

	 * when determining if we're in an atomic sequence. If we could

	 * not find any symbol skip the atomic test altogether as we

	 * could otherwise end up searching irrelevant text/literals.

	 * KPROBES depends on KALLSYMS so this last case should never

	 * happen.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2014-2016 Pratyush Anand <panand@redhat.com>

 Initialize the slot */

 flush caches (dcache/icache) */

 TODO: Currently we do not support AARCH32 instruction probing */

 Initialize with an invalid fault code to detect if ol insn trapped */

 Instruction points to execute ol */

 Instruction points to execute next to breakpoint address */

	/*

	 * Between arch_uprobe_pre_xol and arch_uprobe_post_xol, if an xol

	 * insn itself is trapped, then detect the case with the help of

	 * invalid fault code which is being set in arch_uprobe_pre_xol

	/*

	 * Task has received a fatal signal, so reset back to probbed

	 * address.

	/*

	 * If a simple branch instruction (B) was called for retprobed

	 * assembly label then return true even when regs->sp and ret->stack

	 * are same. It will ensure that cleanup and reporting of return

	 * instances corresponding to callee label is done when

	 * handle_trampoline for called function is executed.

 Replace the return addr with trampoline addr */

 uprobe breakpoint handler hook */

 uprobe single step handler hook */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/probes/kprobes.c

 *

 * Kprobes support for ARM64

 *

 * Copyright (C) 2013 Linaro Limited.

 * Author: Sandeepa Prabhu <sandeepa.prabhu@linaro.org>

 prepare insn slot */

	/*

	 * Needs restoring of return address after stepping xol.

 This instructions is not executed xol. No need to adjust the PC */

 single step simulated, now go for post processing */

 copy instruction */

 decode instruction */

 insn not supported */

 insn need simulation */

 instruction uses slot */

 prepare the instruction */

 arm kprobe: install breakpoint in text */

 disarm kprobe: remove breakpoint from text */

/*

 * Mask all of DAIF while executing the instruction out-of-line, to keep things

 * simple and avoid nesting exceptions. Interrupts do have to be disabled since

 * the kprobe state is per-CPU and doesn't get migrated.

 prepare for single stepping */

 insn simulation */

 return addr restore if non-branching insn */

 restore back original saved kprobe variables and continue */

 call post handler */

		/*

		 * We are here because the instruction being single

		 * stepped caused a page fault. We reset the current

		 * kprobe and the ip points back to the probe address

		 * and allow the page fault handler to continue as a

		 * normal page fault.

		/*

		 * In case the user-specified fault handler returned

		 * zero, try to fix up.

 Probe hit */

			/*

			 * If we have no pre-handler or it returned 0, we

			 * continue with normal processing.  If we have a

			 * pre-handler and it returned non-zero, it will

			 * modify the execution path and no need to single

			 * stepping. Let's just reset current kprobe and exit.

	/*

	 * The breakpoint instruction was removed right

	 * after we hit it.  Another cpu has removed

	 * either a probepoint or a debugger breakpoint

	 * at this address.  In either case, no further

	 * handling of this interrupt is appropriate.

	 * Return back to original instruction, and continue.

 not ours, kprobes should ignore it */

/*

 * Provide a blacklist of symbols identifying ranges which cannot be kprobed.

 * This blacklist is exposed to userspace via debugfs (kprobes/blacklist).

 replace return addr (x30) with trampoline */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/kernel/probes/simulate-insn.c

 *

 * Copyright (C) 2013 Linaro Limited.

/*

 * instruction simulation functions

 Link register is x30 */

 update pc first in case we're doing a "blr lr" */

 Link register is x30 */

 x0-x30 */

 w0-w30 */

 SPDX-License-Identifier: GPL-2.0

/*

 * ARM64 userspace implementations of gettimeofday() and similar.

 *

 * Copyright (C) 2018 ARM Limited

 *

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (C) 2012-2018 ARM Limited

 *

 * This supplies .note.* sections to go into the PT_NOTE inside the vDSO text.

 * Here we can supply some information useful to userland.

 SPDX-License-Identifier: GPL-2.0

/*

 * ARM64 compat userspace implementations of gettimeofday() and similar.

 *

 * Copyright (C) 2018 ARM Limited

 *

 Avoid unresolved references emitted by GCC */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * 'regs' represents the state on entry of a predefined function in

	 * the kernel/module and which is captured on a kprobe.

	 *

	 * When kprobe returns back from exception it will override the end

	 * of probed function and directly return to the predefined

	 * function's caller.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * arch/arm64/lib/xor-neon.c

 *

 * Authors: Jackie Liu <liuyun01@kylinos.cn>

 * Copyright (C) 2018,Tianjin KYLIN Information Technology Co., Ltd.

 p1 ^= p2 */

 store */

 p1 ^= p2 */

 p1 ^= p3 */

 store */

 p1 ^= p2 */

 p1 ^= p3 */

 p1 ^= p4 */

 store */

 p1 ^= p2 */

 p1 ^= p3 */

 p1 ^= p4 */

 p1 ^= p5 */

 store */

 SPDX-License-Identifier: GPL-2.0-only

 Copyright (C) 2019-2020 Arm Ltd.

 Looks dumb, but generates nice-ish code */

/*

 * We over-read the buffer and this makes KASAN unhappy. Instead, disable

 * instrumentation and call kasan explicitly.

	/*

	 * This is to all intents and purposes safe, since rounding down cannot

	 * result in a different page or cache line being accessed, and @buff

	 * should absolutely not be pointing to anything read-sensitive. We do,

	 * however, have to be careful not to piss off KASAN, which means using

	 * unchecked reads to accommodate the head and tail, for which we'll

	 * compensate with an explicit check up-front.

	/*

	 * Head: zero out any excess leading bytes. Shifting back by the same

	 * amount should be at least as fast as any other way of handling the

	 * odd/even alignment, and means we can ignore it until the very end.

	/*

	 * Body: straightforward aligned loads from here on (the paired loads

	 * underlying the quadword type still only need dword alignment). The

	 * main loop strictly excludes the tail, so the second loop will always

	 * run at least once.

 This is the "don't dump the carry flag into a GPR" idiom */

	/*

	 * Tail: zero any over-read bytes similarly to the head, again

	 * preserving odd/even alignment.

 Finally, folding */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2017 ARM Ltd.

	/*

	 * We assume this should not be called with @dst pointing to

	 * non-cacheable memory, such that we don't need an explicit

	 * barrier to order the cache maintenance against the memcpy.

 See above */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Delay loops based on the OpenRISC implementation.

 *

 * Copyright (C) 2012 ARM Limited

 *

 * Author: Will Deacon <will.deacon@arm.com>

 2**32 / 1000000 (rounded up) */

 2**32 / 1000000000 (rounded up) */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright (C) 2013 Huawei Ltd.

 * Author: Jiang Liu <liuj97@gmail.com>

 *

 * Copyright (C) 2014-2016 Zi Shen Lim <zlim.lnx@gmail.com>

 ldr/ldrsw (literal), prfm */

 b, bl, cb*, tb*, ret*, b.cond, br*, blr* */

 Update the immediate field. */

	/*

	 * B/BL support [-128M, 128M) offset

	 * ARM64 virtual address arrangement guarantees all kernel and module

	 * texts are within +/-128M.

	/*

	 * STADD is simply encoded as an alias for LDADD with XZR as

	 * the destination register.

 In this case, imm5 is encoded into Rt field. */

 We can't encode more than a 24bit value (12bit + 12bit shift) */

 If we have something in the top 12 bits... */

 ... and in the low 12 bits -> error */

/*

 * MOV (register) is architecturally an alias of ORR (shifted register) where

 * MOV <*d>, <*m> is equivalent to ORR <*d>, <*ZR>, <*m>

/*

 * Decode the imm field of a branch, and return the byte offset as a

 * signed value (so it can be used when computing a new branch

 * target).

 Unhandled instruction */

/*

 * Encode the displacement of a branch in the imm field and return the

 * updated instruction.

 Unhandled instruction */

/*

 * Extract the Op/CR data from a msr/mrs instruction.

/*

 * Macros/defines for extracting register numbers from instruction.

 Doesn't handle full ones or full zeroes */

 One of Sean Eron Anderson's bithack tricks */

 Can't encode full zeroes, full ones, or value wider than the mask */

	/*

	 * Inverse of Replicate(). Try to spot a repeating pattern

	 * with a pow2 stride.

 N is only set if we're encoding a 64bit value */

 Trim imm to the element size */

 That's how many ones we need to encode */

	/*

	 * imms is set to (ones - 1), prefixed with a string of ones

	 * and a zero if they fit. Cap it to 6 bits.

 Compute the rotation */

		/*

		 * Pattern: 0..01..10..0

		 *

		 * Compute how many rotate we need to align it right

		/*

		 * Pattern: 0..01..10..01..1

		 *

		 * Fill the unused top bits with ones, and check if

		 * the result is a valid immediate (all ones with a

		 * contiguous ranges of zeroes).

		/*

		 * Compute the rotation to get a continuous set of

		 * ones, with the first bit set at position 0

	/*

	 * immr is the number of bits we need to rotate back to the

	 * original set of ones. Note that this is relative to the

	 * element size...

 SPDX-License-Identifier: GPL-2.0-only

/*

 * BPF JIT compiler for ARM64

 *

 * Copyright (C) 2014-2016 Zi Shen Lim <zlim.lnx@gmail.com>

 Map BPF registers to A64 registers */

 return value from in-kernel function, and exit value from eBPF */

 arguments from eBPF program to in-kernel function */

 callee saved registers that in-kernel function will preserve */

 read-only frame pointer to access stack */

 temporary registers for internal BPF JIT */

 tail_call_cnt */

 temporary register for blinding constants */

/*

 * Kernel addresses in the vmalloc space use at most 48 bits, and the

 * remaining bits are guaranteed to be 0x1. So we can compose the address

 * with a fixed length movn/movk/movk sequence.

 BPF JMP offset is relative to the next instruction */

	/*

	 * Whereas arm64 branch instructions encode the offset

	 * from the branch itself, so we must subtract 1 from the

	 * instruction offset.

 We are guaranteed to have aligned memory. */

 Either imm12 or shifted imm12. */

 Tail call offset to jump into */

	/*

	 * BPF prog stack layout

	 *

	 *                         high

	 * original A64_SP =>   0:+-----+ BPF prologue

	 *                        |FP/LR|

	 * current A64_FP =>  -16:+-----+

	 *                        | ... | callee saved registers

	 * BPF fp register => -64:+-----+ <= (BPF_FP)

	 *                        |     |

	 *                        | ... | BPF prog stack

	 *                        |     |

	 *                        +-----+ <= (BPF_FP - prog->aux->stack_depth)

	 *                        |RSVD | padding

	 * current A64_SP =>      +-----+ <= (BPF_FP - ctx->stack_size)

	 *                        |     |

	 *                        | ... | Function call stack

	 *                        |     |

	 *                        +-----+

	 *                          low

	 *

 BTI landing pad */

 Save FP and LR registers to stay align with ARM64 AAPCS */

 Save callee-saved registers */

 Set up BPF prog stack base register */

 Initialize tail_call_cnt */

 BTI landing pad for the tail call, done with a BR */

 Stack must be multiples of 16B */

 Set up function call stack */

 initialized on the first pass of build_body() */

 bpf_tail_call(void *prog_ctx, struct bpf_array *array, u64 index) */

	/* if (index >= array->map.max_entries)

	 *     goto out;

	/* if (tail_call_cnt > MAX_TAIL_CALL_CNT)

	 *     goto out;

	 * tail_call_cnt++;

	/* prog = array->ptrs[index];

	 * if (prog == NULL)

	 *     goto out;

 goto *(prog->bpf_func + prologue_offset); */

 out: */

 We're done with BPF stack */

 Restore fs (x25) and x26 */

 Restore callee-saved register */

 Restore FP/LR registers */

 Set return value */

 For accesses to BTF pointers, add an entry to the exception table */

 First pass */

	/*

	 * Since the extable follows the program, the fixup offset is always

	 * negative and limited to BPF_JIT_REGION_SIZE. Store a positive value

	 * to keep things simple, and put the destination register in the upper

	 * bits. We don't need to worry about buildtime or runtime sort

	 * modifying the upper bits because the table is already sorted, and

	 * isn't part of the main exception table.

/* JITs an eBPF instruction.

 * Returns:

 * 0  - successfully JITed an 8-byte eBPF instruction.

 * >0 - successfully JITed a 16-byte eBPF instruction.

 * <0 - failed to JIT.

 dst = src */

 dst = dst OP src */

 dst = -dst */

 dst = BSWAP##imm(dst) */

 !CONFIG_CPU_BIG_ENDIAN */

 zero-extend 16 bits into 64 bits */

 upper 32 bits already cleared */

 zero-extend 16 bits into 64 bits */

 zero-extend 32 bits into 64 bits */

 nop */

 dst = imm */

 dst = dst OP imm */

 JUMP off */

 IF (dst COND src) JUMP off */

 IF (dst COND imm) JUMP off */

 function call */

 tail call */

 function return */

		/* Optimization: when last instruction is EXIT,

 dst = imm64 */

 LDX: dst = *(size *)(src + off) */

 speculation barrier */

		/*

		 * Nothing required here.

		 *

		 * In case of arm64, we rely on the firmware mitigation of

		 * Speculative Store Bypass as controlled via the ssbd kernel

		 * parameter. Whenever the mitigation is enabled, it works

		 * for all of the kernel code with no need to provide any

		 * additional instructions.

 ST: *(size *)(dst + off) = imm */

 Load imm to a register then store it */

 STX: *(size *)(dst + off) = src */

		/* STX XADD: lock *(u32 *)(dst + off) += src

		 * and

		 * STX XADD: lock *(u64 *)(dst + off) += src

	/*

	 * - offset[0] offset of the end of prologue,

	 *   start of the 1st instruction.

	 * - offset[1] - offset of the end of 1st instruction,

	 *   start of the 2nd instruction

	 * [....]

	 * - offset[3] - offset of the end of 3rd instruction,

	 *   start of 4th instruction

	/*

	 * offset is allocated with prog->len + 1 so fill in

	 * the last element with the offset after the last

	 * instruction (end of program)

	/* If blinding was requested and we failed during blinding,

	 * we must fall back to the interpreter.

 1. Initial fake pass to compute ctx->idx. */

 Fake pass to fill in ctx->offset. */

 Now we know the actual image size. */

 2. Now, the actual pass. */

 3. Extra pass to validate JITed code. */

 And we're done. */

 SPDX-License-Identifier: GPL-2.0

/*

 * Core routines for interacting with Microsoft's Hyper-V hypervisor,

 * including hypervisor initialization.

 *

 * Copyright (C) 2021, Microsoft, Inc.

 *

 * Author : Michael Kelley <mikelley@microsoft.com>

	/*

	 * Allow for a kernel built with CONFIG_HYPERV to be running in

	 * a non-Hyper-V environment, including on DT instead of ACPI.

	 * In such cases, do nothing and return success.

 Setup the guest ID */

 Get the features and hints from Hyper-V */

 Get information about the Hyper-V host version */

 SPDX-License-Identifier: GPL-2.0

/*

 * Low level utility routines for interacting with Hyper-V.

 *

 * Copyright (C) 2021, Microsoft, Inc.

 *

 * Author : Michael Kelley <mikelley@microsoft.com>

/*

 * hv_do_hypercall- Invoke the specified hypercall

/*

 * hv_do_fast_hypercall8 -- Invoke the specified hypercall

 * with arguments in registers instead of physical memory.

 * Avoids the overhead of virt_to_phys for simple hypercalls.

/*

 * Set a single VP register to a 64-bit value.

	/*

	 * Something is fundamentally broken in the hypervisor if

	 * setting a VP register fails. There's really no way to

	 * continue as a guest VM, so panic.

/*

 * Get the value of a single VP register.  One version

 * returns just 64 bits and another returns the full 128 bits.

 * The two versions are separate to avoid complicating the

 * calling sequence for the more frequently used 64 bit version.

	/*

	 * Use the SMCCC 1.2 interface because the results are in registers

	 * beyond X0-X3.

	/*

	 * Something is fundamentally broken in the hypervisor if

	 * getting a VP register fails. There's really no way to

	 * continue as a guest VM, so panic.

/*

 * hyperv_report_panic - report a panic to Hyper-V.  This function uses

 * the older version of the Hyper-V interface that admittedly doesn't

 * pass enough information to be useful beyond just recording the

 * occurrence of a panic. The parallel hv_kmsg_dump() uses the

 * new interface that allows reporting 4 Kbytes of data, which is much

 * more useful. Hyper-V on ARM64 always supports the newer interface, but

 * we retain support for the older version because the sysadmin is allowed

 * to disable the newer version via sysctl in case of information security

 * concerns about the more verbose version.

 Don't report a panic to Hyper-V if we're not going to panic */

	/*

	 * We prefer to report panic on 'die' chain as we have proper

	 * registers to report, but if we miss it (e.g. on BUG()) we need

	 * to report it on 'panic'.

	 *

	 * Calling code in the 'die' and 'panic' paths ensures that only

	 * one CPU is running this code, so no atomicity is needed.

	/*

	 * Hyper-V provides the ability to store only 5 values.

	 * Pick the passed in error value, the guest_id, the PC,

	 * and the SP.

	/*

	 * Let Hyper-V know there is crash data available

 SPDX-License-Identifier: GPL-2.0-only

/*

 * linux/arch/arm64/crypto/aes-glue.c - wrapper code for ARMv8 AES

 *

 * Copyright (C) 2013 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

 defined in aes-modes.S */

 handle ciphertext stealing */

 handle ciphertext stealing */

			/*

			 * The final partial block could not be returned using

			 * an overlapping store, so it was passed via buf[]

			 * instead.

 encrypt the zero vector */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sha2-ce-glue.c - SHA-224/SHA-256 using ARMv8 Crypto Extensions

 *

 * Copyright (C) 2014 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

	/*

	 * Allow the asm code to perform the finalization if there is no

	 * partial data and the input is a round multiple of the block size.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sha1-ce-glue.c - SHA-1 secure hash using ARMv8 Crypto Extensions

 *

 * Copyright (C) 2014 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

	/*

	 * Allow the asm code to perform the finalization if there is no

	 * partial data and the input is a round multiple of the block size.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Scalar AES core transform

 *

 * Copyright (C) 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/arm64 port of the OpenSSL SHA256 implementation for AArch64

 *

 * Copyright (c) 2016 Linaro Ltd. <ard.biesheuvel@linaro.org>

		/*

		 * Don't hog the CPU for the entire time it takes to process all

		 * input when running on a preemptible kernel, but process the

		 * data block by block instead.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Bit sliced AES using NEON instructions

 *

 * Copyright (C) 2016 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

 borrowed from aes-neon-blk.ko */

 fall back to the non-bitsliced NEON implementation */

 ensure that the cts tail is covered by a single step */

 plain NEON is faster otherwise */

 handle ciphertext stealing */

/*

 * ARM NEON and scalar accelerated ChaCha and XChaCha stream ciphers,

 * including ChaCha20 (RFC7539)

 *

 * Copyright (C) 2016 - 2017 Linaro, Ltd. <ard.biesheuvel@linaro.org>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 *

 * Based on:

 * ChaCha20 256-bit cipher algorithm, RFC7539, SIMD glue code

 *

 * Copyright (C) 2015 Martin Willi

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License as published by

 * the Free Software Foundation; either version 2 of the License, or

 * (at your option) any later version.

 SPDX-License-Identifier: GPL-2.0 */

/*

 * sha512-ce-glue.c - SHA-384/SHA-512 using ARMv8 Crypto Extensions

 *

 * Copyright (C) 2018 Linaro Ltd <ard.biesheuvel@linaro.org>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * aes-ce-cipher.c - core AES cipher using ARMv8 Crypto Extensions

 *

 * Copyright (C) 2013 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

	/*

	 * # of rounds specified by AES:

	 * 128 bit key		10 rounds

	 * 192 bit key		12 rounds

	 * 256 bit key		14 rounds

	 * => n byte key	=> 6 + (n/4) rounds

	/*

	 * The AES key schedule round constants

	/*

	 * Generate the decryption keys for the Equivalent Inverse Cipher.

	 * This involves reversing the order of the round keys, and applying

	 * the Inverse Mix Columns transformation on all but the first and

	 * the last one.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Accelerated GHASH implementation with ARMv8 PMULL instructions.

 *

 * Copyright (C) 2014 - 2018 Linaro Ltd. <ard.biesheuvel@linaro.org>

 avoid hogging the CPU for too long */

 needed for the fallback */

	/*

	 * # of rounds specified by AES:

	 * 128 bit key		10 rounds

	 * 192 bit key		12 rounds

	 * 256 bit key		14 rounds

	 * => n byte key	=> 6 + (n/4) rounds

 needed for the fallback */

 copy authtag to end of dst */

 SPDX-License-Identifier: GPL-2.0

/*

 * OpenSSL/Cryptogams accelerated Poly1305 transform for arm64

 *

 * Copyright (C) 2019 Linaro Ltd. <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0 */

/*

 * sha3-ce-glue.c - core SHA-3 transform using v8.2 Crypto Extensions

 *

 * Copyright (C) 2018 Linaro Ltd <ard.biesheuvel@linaro.org>

 *

 * This program is free software; you can redistribute it and/or modify

 * it under the terms of the GNU General Public License version 2 as

 * published by the Free Software Foundation.

 SPDX-License-Identifier: GPL-2.0-or-later

/*

 * Linux/arm64 port of the OpenSSL SHA512 implementation for AArch64

 *

 * Copyright (c) 2016 Linaro Ltd. <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * aes-ccm-glue.c - AES-CCM transform for ARMv8 with Crypto Extensions

 *

 * Copyright (C) 2013 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

	/*

	 * # of rounds specified by AES:

	 * 128 bit key		10 rounds

	 * 192 bit key		12 rounds

	 * 256 bit key		14 rounds

	 * => n byte key	=> 6 + (n/4) rounds

 verify that CCM dimension 'L' is set correctly in the IV */

 verify that msglen can in fact be represented in L bytes */

	/*

	 * Even if the CCM spec allows L values of up to 8, the Linux cryptoapi

	 * uses a u32 type to represent msglen so the top 4 bytes are always 0.

	/*

	 * Meaning of byte 0 according to CCM spec (RFC 3610/NIST 800-38C)

	 * - bits 0..2	: max # of bytes required to represent msglen, minus 1

	 *                (already set by caller)

	 * - bits 3..5	: size of auth tag (1 => 4 bytes, 2 => 6 bytes, etc)

	 * - bit 6	: indicates presence of authenticate-only data

 prepend the AAD with a length tag */

 yield NEON at least every 4k */

 preserve the original iv for the final round */

 copy authtag to end of dst */

 preserve the original iv for the final round */

 compare calculated auth tag with the stored one */

 SPDX-License-Identifier: GPL-2.0

/*

 * NHPoly1305 - ε-almost-∆-universal hash function for Adiantum

 * (ARM64 NEON accelerated version)

 *

 * Copyright 2018 Google LLC

 wrapper to avoid indirect call to assembly, which doesn't work with CFI */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * sm3-ce-glue.c - SM3 secure hash using ARMv8.2 Crypto Extensions

 *

 * Copyright (C) 2018 Linaro Ltd <ard.biesheuvel@linaro.org>

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Accelerated CRC-T10DIF using arm64 NEON and Crypto Extensions instructions

 *

 * Copyright (C) 2016 - 2017 Linaro Ltd <ard.biesheuvel@linaro.org>

 only register the first array element */

 SPDX-License-Identifier: GPL-2.0

/*

 *  linux/arch/x86_64/ia32/ia32_signal.c

 *

 *  Copyright (C) 1991, 1992  Linus Torvalds

 *

 *  1997-11-28  Modified for POSIX.1b signals by Richard Henderson

 *  2000-06-20  Pentium III FXSR, SSE support by Gareth Hughes

 *  2000-12-*   x86-64 compatibility mode signal handling by Andi Kleen

/*

 * Do a signal return; undo the signal stack.

 Always make any pending restarted system calls return -EINTR */

 Get only the ia32 registers. */

 Get CS/SS and force CPL3 */

 disable syscall checks */

	/*

	 * Reload fs and gs if they have changed in the signal

	 * handler.  This does not handle long fs/gs base changes in

	 * the handler, but does not clobber them at least in the

	 * normal case.

/*

 * Set up a signal frame.

 non-iBCS2 extensions.. */

/*

 * Determine which stack to use..

 Default to using normal stack */

 This is the X/Open sanctioned signal stack switching.  */

 This is the legacy signal stack switching. */

	/* Align the stack pointer according to the i386 ABI,

 copy_to_user optimizes that into a single 8 byte store */

 popl %eax ; movl $...,%eax */

 int $0x80 */

 Return stub is in 32bit vsyscall page */

	/*

	 * These are actually not used anymore, but left because some

	 * gdb versions depend on them as a marker.

 Set up registers for signal handler */

 Make -mregparm=3 work */

 unsafe_put_user optimizes that into a single 8 byte store */

 Create the ucontext.  */

	/*

	 * Not actually used anymore, but left because some gdb

	 * versions need it.

 Set up registers for signal handler */

 Make -mregparm=3 work */

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 *  a.out loader for x86-64

 *

 *  Copyright (C) 1991, 1992, 1996  Linus Torvalds

 *  Hacked together by Andi Kleen

/*

 * create_aout_tables() parses the env- and arg-strings in new user

 * memory and creates the pointer tables from them, and puts their

 * addresses on the "stack", returning the new stack pointer value.

/*

 * These are the functions used to load a.out style executables and shared

 * libraries.  There is no binary dependent code anywhere else.

 exec-header */

	/* Check initial limits. This avoids letting people circumvent

	 * size limits imposed on them by creating programs with large

	 * arrays in the data or bss.

 Flush all traces of the currently running executable */

 OK, This is the point of no return */

 start thread */

 We come in here for the regular a.out style of shared libraries */

	/* For  QMAGIC, the starting address is 0x20 into the page.  We mask

 Now use mmap to map the library into memory. */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_ld_str.c                                                             |

 |                                                                           |

 | All of the functions which transfer data between user memory and FPU_REGs.|

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1996,1997                                    |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | Note:                                                                     |

 |    The file contains code which accesses user memory.                     |

 |    Emulator static data may change when user memory is accessed, due to   |

 |    other processes using the emulator while swapping is in progress.      |

 largest valid exponent */

 smallest valid exponent */

 largest valid exponent */

 smallest valid exponent */

 The number is a de-normal or pseudodenormal. */

 Is an Infinity, a NaN, or an unsupported data type. */

 Unsupported data type. */

 Valid numbers have the ms bit set to 1. */

 Unnormal. */

 Get a long double from user memory */

 Get a double from user memory */

 Infinity or NaN */

 +- infinity */

 Must be a signaling or quiet NaN */

 The calling function must look for NaNs */

 Zero or de-normal */

 Zero */

 De-normal */

 Get a float from user memory */

 Zero */

 De-normals */

 Infinity or NaN */

 +- infinity */

 Must be a signaling or quiet NaN */

 The calling function must look for NaNs */

 Set the sign. */

 Get a long long from user memory */

 Get a long from user memory */

 Get a short from user memory */

 Cast as short to get the sign extended. */

 Get a packed bcd array from user memory */

 Set the sign. */

===========================================================================*/

 Put a long double into user memory */

	/*

	   The only exception raised by an attempt to store to an

	   extended format is the Invalid Stack exception, i.e.

	   attempting to store from an empty register.

 Empty register (stack underflow) */

 The masked response */

 Put out the QNaN indefinite */

 Put a double into user memory */

 avoid gcc warnings */

 It may be a denormal */

 largest exp to be 51 */

 Did it round to a non-denormal ? */

				/* This behaviour might be regarded as peculiar, it appears

				   that the 80486 rounds to the dest precision, then

 PECULIAR_486 */

					/* This is a special case: see sec 16.2.5.1 of

 Rounding can get a little messy.. */

 nearest */

 odd -> even */

 towards -infinity */

 towards +infinity */

 Truncate the mantissa */

 the sigl part overflows */

 The sigh part overflows */

 We only need to increment sigl */

 This is a special case: see sec 16.2.5.1 of the 80486 book */

 Overflow to infinity */

 Set to + INF */

 Add the exponent */

 Number is zero */

 A denormal will always underflow. */

			/* An 80486 is supposed to be able to generate

 Underflow has priority. */

 PECULIAR_486 */

 Is it really a NaN ? */

 See if we can get a valid NaN from the FPU_REG */

 It is a signalling NaN */

 It is an unsupported data type */

 Empty register (stack underflow) */

 The masked response */

 Put out the QNaN indefinite */

 Put a float into user memory */

 avoid gcc warnings */

 largest exp to be 22 */

 Did it round to a non-denormal ? */

				/* This behaviour might be regarded as peculiar, it appears

				   that the 80486 rounds to the dest precision, then

 PECULIAR_486 */

					/* This is a special case: see sec 16.2.5.1 of

 more than half */

 more than half */

 round to even */

 towards -infinity */

 towards +infinity */

 Truncate part of the mantissa */

 The sigh part overflows */

 Finish the truncation */

 This is a special case: see sec 16.2.5.1 of the 80486 book. */

 Masked response is overflow to infinity. */

 Add the exponent */

 A denormal will always underflow. */

			/* An 80486 is supposed to be able to generate

 Underflow has priority. */

 PECULIAR_486 */

 Is it really a NaN ? */

 See if we can get a valid NaN from the FPU_REG */

 It is a signalling NaN */

 It is an unsupported data type */

 Empty register (stack underflow) */

 The masked response */

 Put out the QNaN indefinite */

 Put a long long into user memory */

 Empty register (stack underflow) */

 This is a special case: see sec 16.2.5.1 of the 80486 book */

 Produce something like QNaN "indefinite" */

 Put a long into user memory */

 Empty register (stack underflow) */

 This is a special case: see sec 16.2.5.1 of the 80486 book */

 Produce something like QNaN "indefinite" */

 Put a short into user memory */

 Empty register (stack underflow) */

 This is a special case: see sec 16.2.5.1 of the 80486 book */

 Produce something like QNaN "indefinite" */

 Put a packed bcd array into user memory */

 Empty register (stack underflow) */

 Check for overflow, by comparing with 999999999999999999 decimal. */

 This is a special case: see sec 16.2.5.1 of the 80486 book */

 Produce the QNaN "indefinite" */

 These bytes "undefined" */

 This byte "undefined" */

 Precision loss doesn't stop the data transfer */

===========================================================================*/

/* r gets mangled such that sig is int, sign: 

/* The return value (in eax) is zero if the result is exact,

   if bits are changed due to rounding, truncation, etc, then

/* Overflow is signaled by a non-zero return value (in eax).

   In the case of overflow, the returned significand always has the

 Make sure that zero is returned */

 o.k. */

 The largest representable number */

 overflow */

 test for 0xfff...fff */

 nearest */

 odd -> even */

 overflow */

 overflow */

 overflow */

===========================================================================*/

 PECULIAR_486 */

 New tag is empty.  Accept it */

			/* Old tag is empty and new tag is not empty.  New tag is determined

 An Un-normal */

		/* Else old tag is not empty and new tag is not empty.  Old tag

 Copy all registers in stack order. */

 The loaded data over-rides all other cases. */

 PECULIAR_486 */

 An 80486 sets nearly all of the reserved bits to 1. */

 PECULIAR_486 */

 Copy all registers in stack order. */

===========================================================================*/

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_divide.c                                                             |

 |                                                                           |

 | Divide one FPU_REG by another and put the result in a destination FPU_REG.|

 |                                                                           |

 | Copyright (C) 1996                                                        |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@jacobi.maths.monash.edu.au                |

 |                                                                           |

 |    Return value is the tag of the answer, or-ed with FPU_Exception if     |

 |    one was raised, or -1 on internal error.                               |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | The destination may be any FPU_REG, including one of the source FPU_REGs. |

/*

  Divide one register by another and put the result into a third register.

 Both regs Valid, this should be the most common case. */

 Want to find Zero/Valid */

 The result is zero. */

 We have an exception condition, either 0/0 or Valid/Zero. */

 0/0 */

 Valid/Zero */

 Must have infinities, NaNs, etc */

 infinity/infinity */

 tagb must be Valid or Zero */

			/* Infinity divided by Zero or Valid does

 The result is zero. */

 PARANOID */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_constant.c                                                           |

 |                                                                           |

 | All of the constant FPU_REGs                                              |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                     W. Metzenthen, 22 Parker St, Ormond, Vic 3163,        |

 |                     Australia.  E-mail   billm@suburbia.net               |

 |                                                                           |

 |                                                                           |

  0  */

 Extra bits to take pi/2 to more than 128 bits precision. */

 Only the sign (and tag) is used in internal zeroes */

 Only the sign and significand (and tag) are used in internal NaNs */

/* The 80486 never generates one of these

FPU_REG const CONST_SNAN = MAKE_REG(POS, EXP_OVER, 0x00000001, 0x80000000);

 This is the real indefinite QNaN */

 Only the sign (and tag) is used in internal infinities */

	st_new_ptr->sigl += adj;	/* For all our fldxxx constants, we don't need to

/* A fast way to find out whether x is one of RC_DOWN or RC_CHOP

   (and not one of RC_RND or RC_UP).

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_trig.c                                                               |

 |                                                                           |

 | Implementation of the FPU "transcendental" functions.                     |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997,1999                                    |

 |                       W. Metzenthen, 22 Parker St, Ormond, Vic 3163,      |

 |                       Australia.  E-mail   billm@melbpc.org.au            |

 |                                                                           |

 |                                                                           |

 Used only by fptan, fsin, fcos, and fsincos. */

/* This routine produces very accurate results, similar to

/* Limited measurements show no results worse than 64 bit precision

   except for the results for arguments close to 2^63, where the

 Reduction incomplete. */

	FPU_round_to_int(&tmp, tag);	/* Fortunately, this can't overflow

		/* So far, the results are exact but based upon a 64 bit

		   precision approximation to pi/2. The technique used

		   now is equivalent to using an approximation to pi/2 which

			/* This code gives the effect of having pi/2 to better than

				/* CONST_PI2extra is negative, so the result of the addition

				   can be negative. This means that the argument is actually

				   in a different quadrant. The correction is always < pi/2,

 BETTER_THAN_486 */

		/* So far, the results are exact but based upon a 64 bit

		   precision approximation to pi/2. The technique used

		   now is equivalent to using an approximation to pi/2 which

			/* This code gives the effect of having p/2 to better than

 This must return TAG_Valid */

				/* CONST_PI2extra is negative, so the result of the

				   subtraction can be larger than pi/2. This means

				   that the argument is actually in a different quadrant.

				   The correction is always < pi/2, so it can't overflow

 BETTER_THAN_486 */

 Reduction complete. */

 Convert a long to register */

 Puts a QNaN in st(0) */

 return with a NaN in st(0) */

 PARANOID */

 Signaling ? */

 The masked response */

 Convert to a QNaN */

 A QNaN */

 pseudoNaN or other unsupported */

 The masked response */

 return with a NaN in st(0) */

 PARANOID */

---------------------------------------------------------------------------*/

 For an 80486 FPU, the result is undefined if the arg is >= 1.0 */

 poly_2xm1(x) requires 0 < st(0) < 1. */

 80486 appears to always do this */

 -infinity gives -1 (p16-10) */

 Stack underflow has higher priority */

 Puts a QNaN in st(0) */

 Puts a QNaN in the new st(0) */

 Operand is out of range */

 We do not really know if up or down */

 For a small arg, the result == the argument */

 Underflow may happen */

 The 80486 treats infinity as an invalid operand */

 anticipate */

 Needed if arg was a denormal */

 Is this the correct behaviour? */

 PARANOID */

 sqrt(negative) is invalid */

 make st(0) in  [1.0 .. 4.0) */

 Do the computation, the sign of the result will be positive. */

 sqrt(-Infinity) is invalid */

 sqrt(negative) is invalid */

 Fortunately, this can't overflow to 2^64 */

 Operand is out of range */

 We do not really know if up or down */

 For a small arg, the result == the argument */

 Must be up. */

 For a small arg, the result == the argument */

 Underflow may happen */

 The 80486 treats infinity as an invalid operand */

 We do not really know if up or down */

 We do not really know if up or down */

 Operand is out of range */

 80486 appears to do this. */

 Must be up. */

 PECULIAR_486 */

 The 80486 treats infinity as an invalid operand */

 requires st0_ptr == &st(0) */

 Stack underflow has higher priority */

 Puts a QNaN in st(0) */

 Puts a QNaN in the new st(0) */

 The 80486 treats infinity as an invalid operand */

 Masked response */

 An error, so restore st(0) */

---------------------------------------------------------------------------*/

 The following all require two arguments: st(0) and st(1) */

/* A lean, mean kernel for the fprem instructions. This relies upon

   the division and rounding to an integer in do_fprem giving an

   exact result. Because of this, rem_kernel() needs to deal only with

   the least significant 64 bits, the more significant bits of the

   result must be zero.

 Do the required multiplication and subtraction in the one operation */

 lsw x -= lsw st1 * lsw q */

 msw x -= msw st1 * lsw q */

 msw x -= lsw st1 * msw q */

 Remainder of st(0) / st(1) */

/* This routine produces exact results, i.e. there is never any

 Convert registers for internal use. */

		/* We want the status following the denorm tests, but don't want

 This should be the most common case */

					FPU_round_to_int(&tmp, tag);	/* Fortunately, this can't

					/* We may need to subtract st(1) once more,

 expdif is 1 */

 or equi-distant (from 0 & st(1)) and q is odd */

 There is a large exponent difference ( >= 64 ) */

			/* To make much sense, the code in this section should

 prevent overflow here */

 N is 'a number between 32 and 63' (p26-113) */

			N = (expdif & 0x0000001f) + 32;	/* This choice gives results

			FPU_round_to_int(&tmp, tag);	/* Fortunately, this can't

			/* It is possible for the operation to be complete here.

			   What does the IEEE standard say? The Intel 80486 manual

			   implies that the operation will never be completed at this

			   point, and the behaviour of a real 80486 confirms this.

 The result is zero */

 PECULIAR_486 */

		/* The only condition to be looked for is underflow,

 fprem(?,0) always invalid */

 fprem(Valid,Zero) is invalid */

 fprem(Valid,Infinity) is o.k. */

 fprem(Infinity,?) is invalid */

 One of the registers must contain a NaN if we got here. */

 PARANOID */

 ST(1) <- ST(1) * log ST;  pop ST */

 Both regs are Valid or Denormal */

 Convert st(0) for internal use. */

 Special case. The result can be precise. */

 The usual case */

 Convert st(1) for internal use. */

 negative */

 Both args zero is invalid */

 st(1) contains zero, st(0) valid <> 0 */

 Zero is the valid answer */

 log(negative) */

 One or both operands are denormals. */

 One or both arg must be an infinity */

 log(-infinity) or 0*log(infinity) */

 st(1) must be infinity here */

 st(0) holds 1.0 */

 infinity*log(1) */

 else st(0) is positive and > 1.0 */

 st(0) is positive and < 1.0 */

 st(0) must be zero or negative */

 This should be invalid, but a real 80486 is happy with it. */

 PECULIAR_486 */

 log(negative) */

 An 80486 preserves the sign */

 st(1) is infinity, st(0) not infinity */

 st(0) must be valid or zero */

 An 80486 preserves the sign */

 st(1) must be TAG_Valid here */

 PARANOID */

 We do not really know if up or down */

 Infinity*log(1) */

 PARANOID */

 st(0) holds <= -1.0 */

 Stupid 80486 doesn't worry about log(negative). */

 PECULIAR_486 */

 st(0) holds < -1.0 */

 Stupid 80486 doesn't worry about log(negative). */

 PECULIAR_486 */

 This should have higher priority than denormals, but... */

 log(-infinity) */

 PECULIAR_486 */

 Denormal operands actually get higher priority */

 log(-infinity) */

 PECULIAR_486 */

 log(infinity) */

 st(1) must be valid here. */

		/* The Manual says that log(Infinity) is invalid, but a real

 PARANOID */

 Convert register for internal use. */

 2^31 is far too large, would require 2^(2^30) or 2^(-2^30) */

 This can never overflow here */

 Use FPU_round() to properly detect under/overflow etc */

 st(0) must be a denormal */

 Will not be left on stack */

 Zero scaled by +Infinity */

 Infinity scaled by -Infinity */

 At least one of st(0), st(1) must be empty */

---------------------------------------------------------------------------*/

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  poly_sin.c                                                               |

 |                                                                           |

 |  Computation of an approximation of the sin function and the cosine       |

 |  function by a polynomial.                                                |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997,1999                                    |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@melbpc.org.au                             |

 |                                                                           |

 |                                                                           |

/*--- poly_sine() -----------------------------------------------------------+

 |                                                                           |

 Split into two ranges, for arguments below and above 1.0 */

 The boundary between upper and lower is approx 0.88309101259 */

 The argument is <= 0.88309101259 */

 Divide by four */

 Add 1.0 */

 Divide by four, FPU_REG compatible, etc */

 The minimum exponent difference is 3 */

 The argument is > 0.88309101259 */

 We use sin(st(0)) = cos(pi/2-st(0)) */

 The argument is >= 1.0 */

 Put the binary point at the left. */

 pi/2 in hex is: 1.921fb54442d18469 898CC51701B839A2 52049C1 */

 There is a special case which arises due to rounding, to fix here. */

 A zero accumulator here would cause problems */

		/* The basic computation is complete. Now fix the answer to

		   compensate for the error due to the approximation used for

		   pi/2

 This has an exponent of -65 */

 The fix-up needs to be improved for larger args */

 Get about 32 bit precision in these: */

 temp save */

 PARANOID */

/*--- poly_cos() ------------------------------------------------------------+

 |                                                                           |

 PARANOID */

 arg is < 0.687705 */

 shift the argument right by the required places */

		/* It doesn't matter if accumulator is all zero here, the

 The result is 1.0 */

 will be a valid positive nr with expon = -1 */

 The argument is >= 1.0 */

 Put the binary point at the left. */

 pi/2 in hex is: 1.921fb54442d18469 898CC51701B839A2 52049C1 */

 There is a special case which arises due to rounding, to fix here. */

		/* A shift is needed here only for a narrow range of arguments,

 shift the argument right by the required places */

 Divide by four */

 Add 1.0 */

 Divide by four, FPU_REG compatible, etc */

 The minimum exponent difference is 3 */

		/* The basic computation is complete. Now fix the answer to

		   compensate for the error due to the approximation used for

		   pi/2

 This has an exponent of -65 */

 The fix-up needs to be improved for larger args */

 Get about 32 bit precision in these: */

 Prevent overflow */

 PARANOID */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  errors.c                                                                 |

 |                                                                           |

 |  The error handling functions for wm-FPU-emu                              |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1996                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@jacobi.maths.monash.edu.au                |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | Note:                                                                     |

 |    The file contains code which accesses user memory.                     |

 |    Emulator static data may change when user memory is accessed, due to   |

 |    other processes using the emulator while swapping is in progress.      |

 */

 */

 No need to check access_ok(), we have previously fetched these bytes. */

  0  */

/*

   Called for opcodes which are illegal and which are known to result in a

   SIGILL with a real 80486.

 No need to check access_ok(), we have previously fetched these bytes. */

 DEBUGGING */

 busy */

 stack top pointer */

 Error summary status */

 Stack flag */

 cc */

 cc */

 Update tagi for the printk below */

/*

 EX_INTERNAL is always given with a code which indicates where the

 error was detected.



 Internal error types:

       0x14   in fpu_etc.c

       0x1nn  in a *.c file:

              0x101  in reg_add_sub.c

              0x102  in reg_mul.c

              0x104  in poly_atan.c

              0x105  in reg_mul.c

              0x107  in fpu_trig.c

	      0x108  in reg_compare.c

	      0x109  in reg_compare.c

	      0x110  in reg_add_sub.c

	      0x111  in fpe_entry.c

	      0x112  in fpu_trig.c

	      0x113  in errors.c

	      0x115  in fpu_trig.c

	      0x116  in fpu_trig.c

	      0x117  in fpu_trig.c

	      0x118  in fpu_trig.c

	      0x119  in fpu_trig.c

	      0x120  in poly_atan.c

	      0x121  in reg_compare.c

	      0x122  in reg_compare.c

	      0x123  in reg_compare.c

	      0x125  in fpu_trig.c

	      0x126  in fpu_entry.c

	      0x127  in poly_2xm1.c

	      0x128  in fpu_entry.c

	      0x129  in fpu_entry.c

	      0x130  in get_address.c

	      0x131  in get_address.c

	      0x132  in get_address.c

	      0x133  in get_address.c

	      0x140  in load_store.c

	      0x141  in load_store.c

              0x150  in poly_sin.c

              0x151  in poly_sin.c

	      0x160  in reg_ld_str.c

	      0x161  in reg_ld_str.c

	      0x162  in reg_ld_str.c

	      0x163  in reg_ld_str.c

	      0x164  in reg_ld_str.c

	      0x170  in fpu_tags.c

	      0x171  in fpu_tags.c

	      0x172  in fpu_tags.c

	      0x180  in reg_convert.c

       0x2nn  in an *.S file:

              0x201  in reg_u_add.S

              0x202  in reg_u_div.S

              0x203  in reg_u_div.S

              0x204  in reg_u_div.S

              0x205  in reg_u_mul.S

              0x206  in reg_u_sub.S

              0x207  in wm_sqrt.S

	      0x208  in reg_div.S

              0x209  in reg_u_sub.S

              0x210  in reg_u_sub.S

              0x211  in reg_u_sub.S

              0x212  in reg_u_sub.S

	      0x213  in wm_sqrt.S

	      0x214  in wm_sqrt.S

	      0x215  in wm_sqrt.S

	      0x220  in reg_norm.S

	      0x221  in reg_norm.S

	      0x230  in reg_round.S

	      0x231  in reg_round.S

	      0x232  in reg_round.S

	      0x233  in reg_round.S

	      0x234  in reg_round.S

	      0x235  in reg_round.S

	      0x236  in reg_round.S

	      0x240  in div_Xsig.S

	      0x241  in div_Xsig.S

	      0x242  in div_Xsig.S

 Needed only to stop compiler warnings */

 Set lots of exception bits! */

 Extract only the bits which we use to set the status word */

 Set the corresponding exception bit */

 Set summary bits iff exception isn't masked */

				/* This bit distinguishes over- from underflow for a stack fault,

 Get a name string for error reporting */

 PRINT_MESSAGES */

 PRINT_MESSAGES */

		/*

		 * The 80486 generates an interrupt on the next non-control FPU

		 * instruction. So we need some means of flagging it.

		 * We use the ES (Error Summary) bit for this.

 __DEBUG__ */

 Real operation attempted on a NaN. */

 Returns < 0 if the exception is unmasked */

	/* The default result for the case of two "equal" NaNs (signs may

 pseudo-NaN, or other unsupported? */

 Masked response */

 The masked response */

 pseudo-NaN ? */

 ensure a Quiet NaN */

 Real operation attempted on two operands, one a NaN. */

 Returns < 0 if the exception is unmasked */

 TW_NaN is also used for unsupported data types. */

 Masked response */

				/* The default result for the case of two "equal" NaNs (signs may

 return the quiet version of the NaN in a */

 PARANOID */

 PARANOID */

 pseudo-NaN ? */

 ensure a Quiet NaN */

 Invalid arith operation on Valid registers */

 Returns < 0 if the exception is unmasked */

 The masked response */

 Divide a finite number by zero */

 The masked response */

 This may be called often, so keep it lean */

 The masked response */

 This may be called often, so keep it lean */

 The masked response */

 This may be called often, so keep it lean */

 The masked response */

 The masked response */

 The masked response */

 ###### The response here depends upon the rounding mode */

 Subtract the magic number from the exponent */

 The overflow exception is masked. */

		/* By definition, precision is lost.

		   The roundup bit (C1) is also set because we have

 The masked response */

 Round down. */

 Add the magic number to the exponent. */

 The underflow exception is masked. */

 The masked response */

 The masked response */

 The masked response */

 The masked response */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_compare.c                                                            |

 |                                                                           |

 | Compare two floating point registers                                      |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | compare() is the core FPU_REG comparison function                         |

 The 80486 book says that infinities can be equal! */

 Fall through to the NaN code */

 Fall through to the NaN code */

		/* The only possibility now should be that one of the arguments

 Neither is a signaling NaN */

 PARANOID */

		diff = st0_ptr->sigh - b->sigh;	/* Works only if ms bits are

 This function requires that st(0) is not empty */

 PARANOID */

 Stack fault */

 PARANOID */

 Stack fault */

 PARANOID */

 Stack fault */

		if (c & COMP_SNaN) {	/* This is the only difference between

 PARANOID */

 Stack fault */

		if (c & COMP_SNaN) {	/* This is the only difference between

 PARANOID */

---------------------------------------------------------------------------*/

 fcom st(i) */

 fcomp st(i) */

 fcompp */

 fucom st(i) */

 fucomp st(i) */

 fucompp */

 P6+ compare-to-EFLAGS ops */

 fcomi st(i) */

 fcomip st(i) */

 fucomi st(i) */

 fucomip st(i) */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  poly_atan.c                                                              |

 |                                                                           |

 | Compute the arctan of a FPU_REG, using a polynomial approximation.        |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

 odd poly, negative terms */

 Dummy (not for - 1.0) */

 odd poly, positive terms */

  0xaaaaaaaaaaaaaaabLL,  transferred to fixedpterm[] */

/*--- poly_atan() -----------------------------------------------------------+

 |                                                                           |

 This gives non-compatible stack contents... */

 This gives non-compatible stack contents... */

 The argument is greater than sqrt(2)-1 (=0.414213562...) */

 Convert the argument by an identity for atan */

 There must be a logic error */

 PARANOID */

 Make the transformed arg -> 0.0 */

	/* Now have argSq etc with binary point at the left

 Do the basic fixed point polynomial evaluation */

 compute pi/4 - accumulator */

 compute pi/2 - accumulator */

 compute pi - accumulator */

	set_precision_flag_up();	/* We do not really know if up or down,

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  poly_2xm1.c                                                              |

 |                                                                           |

 | Function to compute 2^x-1 by a polynomial approximation.                  |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

 This term done separately as 12 bytes */

/* Four slices: 0.0 : 0.25 : 0.50 : 0.75 : 1.0,

   These numbers are 2^(1/4), 2^(1/2), and 2^(3/4)

/*--- poly_2xm1() -----------------------------------------------------------+

 | Requires st(0) which is TAG_Valid and < 1.                                |

 Don't want a |number| >= 1.0 */

 Number negative, too large, or not Valid. */

 PARANOID */

 subtract 0.5 or 0.75 */

 subtract 0.25 */

 Shift the argument right by the required places. */

 round up */

 The leading term */

		/* The argument is large, use the identity:

		   f(x+a) = f(a) * (f(x) + 1) - 1;

 add 1.0 */

 subtract 1.0 */

		/* The argument is negative, use the identity:

		   f(-x) = -f(x) / (1 + f(x))

 exponent must be 1 here */

 add 1.0 */

 Convert to 64 bit signed-compatible */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_add_sub.c                                                            |

 |                                                                           |

 | Functions to add or subtract two registers and put the result in a third. |

 |                                                                           |

 | Copyright (C) 1992,1993,1997                                              |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 |  For each function, the destination may be any FPU_REG, including one of  |

 | the source FPU_REGs.                                                      |

 |  Each function returns 0 if the answer is o.k., otherwise a non-zero      |

 | value is returned, indicating either an exception condition or an         |

 | internal error.                                                           |

/*

  Operates on st(0) and st(n), or on st(0) and temporary data.

  The destination must be one of the source st(x).

 Both registers are valid */

 signs are the same */

 The signs are different, so do a subtraction */

				diff = a->sigh - b->sigh;	/* This works only if the ms bits

 sign depends upon rounding mode */

 Subtract b from a.  (a-b) -> dest */

 Both registers are valid */

 Works only if ms bits are identical */

 P - P */

 N - N */

 |a| > |b| */

 sign depends upon rounding mode */

 P - N */

 N - P */

 Both are zero, result will be zero. */

 Signs are different. */

 Sign of answer depends upon rounding mode. */

 signa may differ from the sign of a. */

 A pseudoDenormal, convert it. */

 signb may differ from the sign of b. */

 A pseudoDenormal */

 signa may differ from the sign of a. */

 signa may differ from the sign of a. */

 Infinity-Infinity is undefined. */

 signb may differ from the sign of b. */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  poly_l2.c                                                                |

 |                                                                           |

 | Compute the base 2 log of a FPU_REG, using a polynomial approximation.    |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

/*--- poly_l2() -------------------------------------------------------------+

 |   Base 2 logarithm by a polynomial approximation.                         |

 From st0_ptr, make a number > sqrt(2)/2 and < sqrt(2) */

 Treat as  sqrt(2)/2 < st0_ptr < 1 */

 Treat as  1 <= st0_ptr < sqrt(2) */

 80486 appears to always do this */

/*--- poly_l2p1() -----------------------------------------------------------+

 |   Base 2 logarithm by a polynomial approximation.                         |

 |   log2(x+1)                                                               |

 80486 appears to always do this */

 The magnitude of st0_ptr is far too large. */

 Trying to get the log of a negative number. */

 Stupid 80486 doesn't worry about log(negative). */

 PECULIAR_486 */

 80486 appears to do this */

/*--- log2_kernel() ---------------------------------------------------------+

 |   Base 2 logarithm by a polynomial approximation.                         |

 |   log2(x+1)                                                               |

 Denom must be 1.0 */

 Should check here that  |local_arg|  is within the valid range */

 The argument is too large */

 PECULIAR_486 */

 Do the basic fixed point polynomial evaluation */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_etc.c                                                                |

 |                                                                           |

 | Implement a few FPU instructions.                                         |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                       W. Metzenthen, 22 Parker St, Ormond, Vic 3163,      |

 |                       Australia.  E-mail   billm@suburbia.net             |

 |                                                                           |

 |                                                                           |

 This is weird! */

 PECULIAR_486 */

 Operand is not comparable */

 Operand is not comparable */

 Denormal */

 We also use NaN for unsupported types. */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_convert.c                                                            |

 |                                                                           |

 |  Convert register representation.                                         |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1996,1997                                    |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

 Set up the exponent as a 16 bit quantity. */

 The number is a de-normal or pseudodenormal. */

 We only deal with the significand and exponent. */

 Is a pseudodenormal. */

			/* This is non-80486 behaviour because the number

 Is a denormal. */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_arith.c                                                              |

 |                                                                           |

 | Code to implement the FPU register/register arithmetic instructions       |

 |                                                                           |

 | Copyright (C) 1992,1993,1997                                              |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

 fadd st,st(i) */

 fmul st,st(i) */

 fsub st,st(i) */

 fsubr st,st(i) */

 fdiv st,st(i) */

 fdivr st,st(i) */

 fadd st(i),st */

 fmul st(i),st */

 fsubr st(i),st */

 fsub st(i),st */

 fdivr st(i),st */

 fdiv st(i),st */

 faddp st(i),st */

 fmulp st(i),st */

 fsubrp st(i),st */

 fsubp st(i),st */

 fdivrp st(i),st */

 fdivp st(i),st */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  reg_mul.c                                                                |

 |                                                                           |

 | Multiply one FPU_REG by another, put the result in a destination FPU_REG. |

 |                                                                           |

 | Copyright (C) 1992,1993,1997                                              |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 | Returns the tag of the result if no exceptions or errors occurred.        |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | The destination may be any FPU_REG, including one of the source FPU_REGs. |

/*

  Multiply two registers to give a register result.

  The sources are st(deststnr) and (b,tagb,signb).

  The destination is st(deststnr).

 This routine must be called with non-empty source registers */

 Both regs Valid, this should be the most common case. */

		/* Must have either both arguments == zero, or

		   one valid and the other zero.

		/* The 80486 book says that the answer is +0, but a real

		   80486 behaves this way.

 Must have infinities, NaNs, etc */

 Zero*Infinity is invalid */

 PARANOID */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_entry.c                                                              |

 |                                                                           |

 | The entry functions for wm-FPU-emu                                        |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1996,1997                                    |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 | See the files "README" and "COPYING" for further copyright and warranty   |

 | information.                                                              |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | Note:                                                                     |

 |    The file contains code which accesses user memory.                     |

 |    Emulator static data may change when user memory is accessed, due to   |

 |    other processes using the emulator while swapping is in progress.      |

/*---------------------------------------------------------------------------+

 | math_emulate(), restore_i387_soft() and save_i387_soft() are the only     |

 | entry points for wm-FPU-emu.                                              |

 Illegal on an 80486, causes SIGILL */

 fcmovCC and f(u)comi(p) are enabled if CPUID(1).EDX(15) "cmov" is set */

/* WARNING: "u" entries are not documented by Intel in their 80486 manual

   and may not work on FPU clones or later Intel FPUs.

 Opcode:	d8		d9		da		db */

		dc		dd		de		df */

 c0..7 */	fadd__,		fld_i_,		fcmovb,		fcmovnb,

 c0..7 */	fadd_i,		ffree_,		faddp_,		ffreep,
 c8..f */	fmul__,		fxch_i,		fcmove,		fcmovne,

 c8..f */	fmul_i,		fxch_i,u*/

 d0..7 */	fcom_st,	fp_nop,		fcmovbe,	fcmovnbe,

 d0..7 */	fcom_st,u*/	fstp_i,
 d8..f */	fcompst,	fstp_i,
 d8..f */	fcompst,u*/

 e0..7 */	fsub__,		FPU_etc,	__BAD__,	finit_,

 e0..7 */	fsubri,		fucom_,		fsubrp,		fstsw_,

 e8..f */	fsubr_,		fconst,		fucompp,	fucomi_,

 e8..f */	fsub_i,		fucomp,		fsubp_,		fucomip,

 f0..7 */	fdiv__,		FPU_triga,	__BAD__,	fcomi_,

 f0..7 */	fdivri,		__BAD__,	fdivrp,		fcomip,

 f8..f */	fdivr_,		FPU_trigb,	__BAD__,	__BAD__,

 f8..f */	fdiv_i,		__BAD__,	fdivp_,		__BAD__,

 Take no special action */

 Need to check for not empty st(0) */

 Need to check for not empty st(0) and st(rm) */

 Uses st(rm) */

 Need to check for space to push onto stack */

 Function illegal or not implemented */

 Uses st(0) and st(rm), result to st(rm) */

 Uses st(0) and st(rm), result to st(rm) then pop */

 Compare st(0) and st(rm) */

 Uses st(0) and st(rm), but handle checks later */

 Opcode:	d8	d9	da	db	dc	dd	de	df */

 c0..7 */	_REGI_, _NONE_, _REGIn, _REGIn, _REGIi, _REGi_, _REGIp, _REGi_,

 c8..f */	_REGI_, _REGIn, _REGIn, _REGIn, _REGIi, _REGI_, _REGIp, _REGI_,

 d0..7 */	_REGIc, _NONE_, _REGIn, _REGIn, _REGIc, _REG0_, _REGIc, _REG0_,

 d8..f */	_REGIc, _REG0_, _REGIn, _REGIn, _REGIc, _REG0_, _REGIc, _REG0_,

 e0..7 */	_REGI_, _NONE_, _null_, _NONE_, _REGIi, _REGIc, _REGIp, _NONE_,

 e8..f */	_REGI_, _NONE_, _REGIc, _REGIc, _REGIi, _REGIc, _REGIp, _REGIc,

 f0..7 */	_REGI_, _NONE_, _null_, _REGIc, _REGIi, _null_, _REGIp, _REGIc,

 f8..f */	_REGI_, _NONE_, _null_, _null_, _REGIi, _null_, _REGIp, _null_,

 RE_ENTRANT_CHECKING */

 Initialized to stop compiler warnings */

 RE_ENTRANT_CHECKING */

 Virtual 8086 mode */

 Assumes code_base <= 0xffff0000 */

 Must be in the LDT */

			/* Can only handle segmented addressing via the LDT

 The above test may be wrong, the book is not clear */

 Segmented 32 bit protected mode */

 16 bit protected mode */

 We have fetched the prefix and first code bytes. */

		/* This checks for the minimum instruction bytes.

 PARANOID */

		/* Ignore the error for now if the current instruction is a no-wait

		/* The 80486 manual contradicts itself on this topic,

		   but a real 80486 uses the following instructions:

		   fninit, fnstenv, fnsave, fnstsw, fnstenv, fnclex.

 fnclex, fninit, fnstsw */

		       (((code & 0x3003) == 0x3001) &&	/* fnsave, fnstcw, fnstenv,

			/*

			 *  We need to simulate the action of the kernel to FPU

			 *  interrupts here.

 Point to current FPU instruction. */

 All of these instructions use the mod/rm byte to get a data address */

 Stack underflow has priority */

 This table works for 16 and 32 bit protected mode */

 Do this here to stop compiler warnings. */

 Used here to suppress gcc warnings. */

				/* No more access to user memory, it is safe

 NaN operands have the next priority. */

				/* We have to delay looking at st(0) until after

					/* Restore the status word; we might have loaded a

 fcom or fcomp */

 fcomp, masked, so we pop. */

						/* This is not really needed, but gives behaviour

 fdiv or fsub */

 PECULIAR_486 */

 fadd, fdivr, fmul, or fsubr */

 Is not a comparison instruction. */

 fdivr */

								/* We use the fact here that the unmasked

								   exception in the loaded data was for a

 Restore the state of the denormal op bit */

 fadd */

 fmul */

 fcom */

 fcomp */

 fsub */

 fsubr */

 fdiv */

 fdivr */

						partial_status = status1;	/* Undo any denorm tag,

 The instruction is fcom or fcomp */

 fcomp */

 None of these instructions access user memory */

		/* This is supposed to be undefined, but a real 80486 seems

 PECULIAR_486 */

 also _REGIc: _REGIn */

 Only used by the fld st(i) instruction */

 DEBUG */

/* Support for prefix bytes is not yet complete. To properly handle

   all prefix bytes, further changes are needed in the emulator code

   which accesses user address space. Access to separate segments is

 defaults */

/* lock is not a valid prefix for FPU instructions,

	case PREFIX_LOCK: */

 rep.. prefixes have no meaning for FPU instructions */

				/* Not a valid sequence of prefix bytes followed by

 Needed for error message. */

 PARANOID */

 Copy all registers in stack order. */

 The tags may need to be corrected now. */

 The loaded data over-rides all other cases. */

 An 80486 sets nearly all of the reserved bits to 1. */

 PECULIAR_486 */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  load_store.c                                                             |

 |                                                                           |

 | This file contains most of the code to interpret the FPU instructions     |

 | which load and store from user memory.                                    |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                       W. Metzenthen, 22 Parker St, Ormond, Vic 3163,      |

 |                       Australia.  E-mail   billm@suburbia.net             |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | Note:                                                                     |

 |    The file contains code which accesses user memory.                     |

 |    Emulator static data may change when user memory is accessed, due to   |

 |    other processes using the emulator while swapping is in progress.      |

 st0_ptr etc not needed */

 Will be storing st(0) */

 Need to check for space to push onto stack */

 Function illegal or not implemented */

 index is a 5-bit value: (3-bit FPU_modrm.reg field | opcode[2,1]) */

 /0: d9:fld f32,  db:fild m32,  dd:fld f64,  df:fild m16 */

 /1: d9:undef,    db,dd,df:fisttp m32/64/16 */

 /2: d9:fst f32,  db:fist m32,  dd:fst f64,  df:fist m16 */

 /3: d9:fstp f32, db:fistp m32, dd:fstp f64, df:fistp m16 */

 /1: d9:undef, db,dd,df:fisttp */

 /1: d9:undef, db,dd,df:fisttp */

 This is just to stop a gcc warning. */

 Initialized just to stop compiler warnings. */

 PARANOID */

		st0_ptr = &st(0);	/* Some of these instructions pop after

 PARANOID */

 type is a 5-bit value: (3-bit FPU_modrm.reg field | opcode[2,1]) */

 fld m32real (d9 /0) */

 fild m32int (db /0) */

 fld m64real (dd /0) */

 fild m16int (df /0) */

 case 004: undefined (d9 /1) */

 fisttp are enabled if CPUID(1).ECX(0) "sse3" is set */

 fisttp m32int (db /1) */

			pop_0();	/* pop only if the number was actually stored

 fisttp m64int (dd /1) */

			pop_0();	/* pop only if the number was actually stored

 fisttp m16int (df /1) */

			pop_0();	/* pop only if the number was actually stored

 fst m32real */

 fist m32int */

 fst m64real */

 fist m16int */

 fstp m32real */

			pop_0();	/* pop only if the number was actually stored

 fistp m32int */

			pop_0();	/* pop only if the number was actually stored

 fstp m64real */

			pop_0();	/* pop only if the number was actually stored

 fistp m16int */

			pop_0();	/* pop only if the number was actually stored

 fldenv  m14/28byte */

		/* Ensure that the values just loaded are not changed by

 frstor m94/108byte */

		/* Ensure that the values just loaded are not changed by

 fbld m80dec */

 fldcw */

 An 80486 appears to always set this bit */

 PECULIAR_486 */

 fld m80real */

 fild m64int */

 fstenv  m14/28byte */

 fsave */

 fbstp m80dec */

			pop_0();	/* pop only if the number was actually stored

 fstcw m16int */

 fstp m80real */

			pop_0();	/* pop only if the number was actually stored

 fstsw m2byte */

 fistp m64int */

			pop_0();	/* pop only if the number was actually stored

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_tags.c                                                               |

 |                                                                           |

 |  Set FPU register tags.                                                   |

 |                                                                           |

 | Copyright (C) 1997                                                        |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@jacobi.maths.monash.edu.au                |

 |                                                                           |

 |                                                                           |

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  fpu_aux.c                                                                |

 |                                                                           |

 | Code to implement some of the FPU auxiliary instructions.                 |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                  W. Metzenthen, 22 Parker St, Ormond, Vic 3163, Australia |

 |                  E-mail   billm@suburbia.net                              |

 |                                                                           |

 |                                                                           |

 Needs to be externally visible */

 We don't keep top in the status word internally. */

	/* The behaviour is different from that detailed in

/*

 * These are nops on the i387..

 fld st(i) */

 The masked response */

 fxch st(i) */

 Masked response */

 Masked response */

 fcmovCC st(i) */

 ffree st(i) */

 ffree st(i) + pop - unofficial code */

 fst st(i) */

 fstp st(i) */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  get_address.c                                                            |

 |                                                                           |

 | Get the effective address from an FPU instruction.                        |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997                                         |

 |                       W. Metzenthen, 22 Parker St, Ormond, Vic 3163,      |

 |                       Australia.  E-mail   billm@suburbia.net             |

 |                                                                           |

 |                                                                           |

/*---------------------------------------------------------------------------+

 | Note:                                                                     |

 |    The file contains code which accesses user memory.                     |

 |    Emulator static data may change when user memory is accessed, due to   |

 |    other processes using the emulator while swapping is in progress.      |

 dummy, not saved on stack */

 Decode the SIB byte. This function assumes mod != 0 */

 The SIB byte */

 No base register */

 No index register */

 A non-zero ss is illegal */

 8 bit signed displacement */

 The second condition also has mod==0 */

 32 bit displacement */

 PARANOID */

 This should work for 16 and 32 bit protected mode. */

 segment is unsigned, so this also detects if segment was 0: */

 PARANOID */

 user gs handling can be lazy, use special accessors */

/*

       MOD R/M byte:  MOD == 3 has a special use for the FPU

                      SIB byte used iff R/M = 100b



       7   6   5   4   3   2   1   0

       .....   .........   .........

        MOD    OPCODE(2)     R/M



       SIB byte



       7   6   5   4   3   2   1   0

       .....   .........   .........

        SS      INDEX        BASE



 Initialized just to stop compiler warnings. */

	/* Memory accessed via the cs selector is write protected

 Default, for 32 bit non-segmented mode. */

 Special case: disp32 */

				address = *cpu_reg_ptr;	/* Just return the contents

 8 bit signed displacement */

 32 bit displacement */

 Not legal for the FPU */

 Default used for mod == 0 */

	/* Memory accessed via the cs selector is write protected

 Default, for 32 bit non-segmented mode. */

 Special case: disp16 */

 8 bit signed displacement */

 16 bit displacement */

 Not legal for the FPU */

 SPDX-License-Identifier: GPL-2.0

/*---------------------------------------------------------------------------+

 |  poly_tan.c                                                               |

 |                                                                           |

 | Compute the tan of a FPU_REG, using a polynomial approximation.           |

 |                                                                           |

 | Copyright (C) 1992,1993,1994,1997,1999                                    |

 |                       W. Metzenthen, 22 Parker St, Ormond, Vic 3163,      |

 |                       Australia.  E-mail   billm@melbpc.org.au            |

 |                                                                           |

 |                                                                           |

 odd poly, positive terms */

 odd poly, negative terms */

 even poly, positive terms */

 even poly, negative terms */

/*--- poly_tan() ------------------------------------------------------------+

 |                                                                           |

 Can't hack a number < 0.0 */

 Need a positive number */

 PARANOID */

 Split the problem into two domains, smaller and larger than pi/4 */

 The argument is greater than (approx) pi/4 */

 The argument is >= 1.0 */

 Put the binary point at the left. */

 pi/2 in hex is: 1.921fb54442d18469 898CC51701B839A2 52049C1 */

 This is a special case which arises due to rounding. */

 shift the argument right by the required places */

 round up */

 Compute the negative terms for the numerator polynomial */

 Add the positive terms */

 Compute the positive terms for the denominator polynomial */

 Add the negative terms */

 Multiply by arg^2 */

 de-normalize and divide by 2 */

 This does 1 - accumulator */

 Now find the ratio. */

		/* accumulatoro must contain 1.0 here, (actually, 0) but it

		   really doesn't matter what value we use because it will

		   have negligible effect in later calculations

 Multiply by 1/3 * arg^3 */

 tan(arg) = arg + accum */

		/* We now have the value of tan(pi_2 - arg) where pi_2 is an

		   approximation for pi/2

		/* The next step is to fix the answer to compensate for the

		   error due to the approximation used for pi/2

		/* This is (approx) delta, the error in our approx for pi/2

		   (see above). It has an exponent of -65

			adj = 0xffffffff;	/* We want approx 1.0 here, but

 tan */

 tan^2 */

 delta * tan^2 */

 did fix_up overflow ? */

 Yes, we need to add an msb */

		/* accum now contains tan(pi/2 - arg).

		   Use tan(arg) = 1.0 / tan(pi/2 - arg)

 Transfer the result */

 Result is positive. */

 SPDX-License-Identifier: GPL-2.0

 System call table for x86-64. */

 SPDX-License-Identifier: GPL-2.0

 System call table for x32 ABI. */

 SPDX-License-Identifier: GPL-2.0

 System call table for i386. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * common.c - C code for kernel entry and exit

 * Copyright (c) 2015 Andrew Lutomirski

 *

 * Based on asm and ptrace code by many authors.  The code here originated

 * in ptrace.c and signal.c.

	/*

	 * Convert negative numbers to very high and thus out of range

	 * numbers for comparisons.

	/*

	 * Adjust the starting offset of the table, and convert numbers

	 * < __X32_SYSCALL_BIT to very high and thus out of range

	 * numbers for comparisons.

 Invalid system call, but still a system call. */

/*

 * Invoke a 32-bit syscall.  Called with IRQs on in CONTEXT_KERNEL.

	/*

	 * Convert negative numbers to very high and thus out of range

	 * numbers for comparisons.

 Handles int $0x80 */

	/*

	 * Subtlety here: if ptrace pokes something larger than 2^31-1 into

	 * orig_ax, the int return value truncates it. This matches

	 * the semantics of syscall_get_nr().

	/*

	 * This cannot use syscall_enter_from_user_mode() as it has to

	 * fetch EBP before invoking any of the syscall entry work

	 * functions.

 Fetch EBP from where the vDSO stashed it. */

		/*

		 * Micro-optimization: the pointer we're following is

		 * explicitly 32 bits, so it can't be out of range.

 User code screwed up. */

 Now this is just like a normal syscall. */

 Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */

	/*

	 * Called using the internal vDSO SYSENTER/SYSCALL32 calling

	 * convention.  Adjust regs so it looks like we entered using int80.

	/*

	 * SYSENTER loses EIP, and even SYSCALL32 needs us to skip forward

	 * so that 'regs->ip -= 2' lands back on an int $0x80 instruction.

	 * Fix it up.

 Invoke the syscall. If it failed, keep it simple: use IRET. */

	/*

	 * Opportunistic SYSRETL: if possible, try to return using SYSRETL.

	 * SYSRETL is available on all 64-bit CPUs, so we don't need to

	 * bother with SYSEXIT.

	 *

	 * Unlike 64-bit opportunistic SYSRET, we can't check that CX == IP,

	 * because the ECX fixup above will ensure that this is essentially

	 * never the case.

	/*

	 * Opportunistic SYSEXIT: if possible, try to return using SYSEXIT.

	 *

	 * Unlike 64-bit opportunistic SYSRET, we can't check that CX == IP,

	 * because the ECX fixup above will ensure that this is essentially

	 * never the case.

	 *

	 * We don't allow syscalls at all from VM86 mode, but we still

	 * need to check VM, because we might be returning from sys_vm86.

 Returns 0 to return using IRET or 1 to return using SYSEXIT/SYSRETL. */

 SYSENTER loses RSP, but the vDSO saved it in RBP. */

 SYSENTER clobbers EFLAGS.IF.  Assume it was set in usermode. */

/*

 * Some hypercalls issued by the toolstack can take many 10s of

 * seconds. Allow tasks running hypercalls via the privcmd driver to

 * be voluntarily preempted even if full kernel preemption is

 * disabled.

 *

 * Such preemptible hypercalls are bracketed by

 * xen_preemptible_hcall_begin() and xen_preemptible_hcall_end()

 * calls.

/*

 * In case of scheduling the flag must be cleared and restored after

 * returning from schedule as the task might move to a different CPU.

 CONFIG_XEN_PV */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2006 Andi Kleen, SUSE Labs.

 *

 * Fast user context implementation of getcpu()

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Copyright 2007 Andi Kleen, SUSE Labs.

 *

 * This contains most of the x86 vDSO kernel-side code.

 Fixing userspace landing - look at do_fast_syscall_32 */

	/*

	 * VM_PFNMAP | VM_IO protect .fault() handler from being called

	 * through interfaces like /proc/$pid/mem or

	 * process_vm_{readv,writev}() as long as there's no .access()

	 * in special_mapping_vmops().

	 * For more details check_vma_flags() and __access_remote_vm()

/*

 * The vvar page layout depends on whether a task belongs to the root or

 * non-root time namespace. Whenever a task changes its namespace, the VVAR

 * page tables are cleared and then they will re-faulted with a

 * corresponding layout.

 * See also the comment near timens_setup_vdso_data() for details.

	/*

	 * Sanity check: a symbol offset of zero means that the page

	 * does not exist for this vdso image, not that the page is at

	 * offset zero relative to the text mapping.  This should be

	 * impossible here, because sym_offset should only be zero for

	 * the page past the end of the vvar mapping.

		/*

		 * If a task belongs to a time namespace then a namespace

		 * specific VVAR is mapped with the sym_vvar_page offset and

		 * the real VVAR page is mapped with the sym_timens_page

		 * offset.

		 * See also the comment near timens_setup_vdso_data().

			/*

			 * Optimization: inside time namespace pre-fault

			 * VVAR page too. As on timens page there are only

			 * offsets for clocks on VVAR, it'll be faulted

			 * shortly by VDSO code.

/*

 * Add vdso and vvar mappings to current process.

 * @image          - blob to map

 * @addr           - request a specific address (zero to map at free addr)

	/*

	 * MAYWRITE to allow gdb to COW and set breakpoints

/*

 * Put the vdso above the (randomized) stack with another randomized

 * offset.  This way there is no hole in the middle of address space.

 * To save memory make sure it is still in the same PTE as the stack

 * top.  This doesn't give that many random bits.

 *

 * Note that this algorithm is imperfect: the distribution of the vdso

 * start address within a PMD is biased toward the end.

 *

 * Only used for the 64-bit and x32 vdsos.

	/*

	 * Round up the start address.  It can start out unaligned as a result

	 * of stack start randomization.

 Round the lowest possible end address up to a PMD boundary. */

	/*

	 * Forcibly align the final address in case we have a hardware

	 * issue that requires alignment for performance reasons.

	/*

	 * Check if we have already mapped vdso blob - fail to prevent

	 * abusing from userspace install_special_mapping, which may

	 * not do accounting and rlimit right.

	 * We could search vma near context.vdso, but it's a slowpath,

	 * so let's explicitly check all VMAs to be completely sure.

 Other values all mean "disabled" */

 CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Fast user context implementation of clock_gettime, gettimeofday, and time.

 *

 * Copyright 2006 Andi Kleen, SUSE Labs.

 * Copyright 2019 ARM Limited

 *

 * 32 Bit compat layer by Stefani Seibold <stefani@seibold.net>

 *  sponsored by Rohde & Schwarz GmbH & Co. KG Munich/Germany

 both 64-bit and x32 use these */

 i386 only */

 SPDX-License-Identifier: GPL-2.0

/*

 * (C) Copyright 2002 Linus Torvalds

 * Portions based on the vdso-randomization code from exec-shield:

 * Copyright(C) 2005-2006, Red Hat, Inc., Ingo Molnar

 *

 * This file contains the needed initializations to support sysenter.

/*

 * Should the kernel map a VDSO page into processes and pass its

 * address down to glibc upon exec()?

/*

 * For consistency, the argument vdso32=[012] affects the 32-bit vDSO

 * behavior on both 64-bit and 32-bit kernels.

 * On 32-bit kernels, vdso=[012] means the same thing.

 Register vsyscall32 into the ABI table */

 CONFIG_SYSCTL */

 CONFIG_X86_64 */

 SPDX-License-Identifier: GPL-2.0

	/*

	 * Do not attempt to fixup #DB or #BP.  It's impossible to identify

	 * whether or not a #DB/#BP originated from within an SGX enclave and

	 * SGX enclaves are currently the only use case for vDSO fixup.

/*

 * vdso2c - A vdso image preparation tool

 * Copyright (c) 2014 Andy Lutomirski and others

 * Licensed under the GPL v2

 *

 * vdso2c requires stripped and unstripped input.  It would be trivial

 * to fully strip the input in here, but, for reasons described below,

 * we need to write a section table.  Doing this is more or less

 * equivalent to dropping all non-allocatable sections, but it's

 * easier to let objcopy handle that instead of doing it ourselves.

 * If we ever need to do something fancier than what objcopy provides,

 * it would be straightforward to add here.

 *

 * We're keep a section table for a few reasons:

 *

 * The Go runtime had a couple of bugs: it would read the section

 * table to try to figure out how many dynamic symbols there were (it

 * shouldn't have looked at the section table at all) and, if there

 * were no SHT_SYNDYM section table entry, it would use an

 * uninitialized value for the number of symbols.  An empty DYNSYM

 * table would work, but I see no reason not to write a valid one (and

 * keep full performance for old Go programs).  This hack is only

 * needed on x86_64.

 *

 * The bug was introduced on 2012-08-31 by:

 * https://code.google.com/p/go/source/detail?r=56ea40aac72b

 * and was fixed on 2014-06-13 by:

 * https://code.google.com/p/go/source/detail?r=fc1cd5e12595

 *

 * Binutils has issues debugging the vDSO: it reads the section table to

 * find SHT_NOTE; it won't look at PT_NOTE for the in-memory vDSO, which

 * would break build-id if we removed the section table.  Binutils

 * also requires that shstrndx != 0.  See:

 * https://sourceware.org/bugzilla/show_bug.cgi?id=17064

 *

 * elfutils might not look for PT_NOTE if there is a section table at

 * all.  I don't know whether this matters for any practical purpose.

 *

 * For simplicity, rather than hacking up a partial section table, we

 * just write a mostly complete one.  We omit non-dynamic symbols,

 * though, since they're rather large.

 *

 * Once binutils gets fixed, we might be able to drop this for all but

 * the 64-bit vdso, since build-id only works in kernel RPMs, and

 * systems that update to new enough kernel RPMs will likely update

 * binutils in sync.  build-id has never worked for home-built kernel

 * RPMs without manual symlinking, and I suspect that no one ever does

 * that.

 Symbols that we need in vdso2c. */

/*

 * Evil macros for little-endian reads and writes

	/*

	 * Figure out the struct name.  If we're writing to a .so file,

	 * generate raw output instead.

 SPDX-License-Identifier: GPL-2.0

/*

 * in case of a 32 bit VDSO for a 64 bit kernel fake a 32 bit kernel

 * configuration

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright (c) 2012-2014 Andy Lutomirski <luto@amacapital.net>

 *

 * Based on the original implementation which is:

 *  Copyright (C) 2001 Andrea Arcangeli <andrea@suse.de> SuSE

 *  Copyright 2003 Andi Kleen, SuSE Labs.

 *

 *  Parts of the original code have been moved to arch/x86/vdso/vma.c

 *

 * This file implements vsyscall emulation.  vsyscalls are a legacy ABI:

 * Userspace can request certain kernel services by calling fixed

 * addresses.  This concept is problematic:

 *

 * - It interferes with ASLR.

 * - It's awkward to write code that lives in kernel addresses but is

 *   callable by userspace at fixed addresses.

 * - The whole concept is impossible for 32-bit compat userspace.

 * - UML cannot easily virtualize a vsyscall.

 *

 * As of mid-2014, I believe that there is no new userspace code that

 * will use a vsyscall if the vDSO is present.  I hope that there will

 * soon be no new userspace code that will ever use a vsyscall.

 *

 * The code in this file emulates vsyscalls when notified of a page

 * fault to a vsyscall address.

	/*

	 * XXX: if access_ok, get_user, and put_user handled

	 * sig_on_uaccess_err, this could go away.

 Write faults or kernel-privilege faults never get fixed up. */

 Failed vsyscall read */

		/*

		 * User code tried and failed to read the vsyscall page.

	/*

	 * No point in checking CS -- the only way to get here is a user mode

	 * trap to a high address, which means that we're in 64-bit user code.

	/*

	 * Check for access_ok violations and find the syscall nr.

	 *

	 * NULL is a valid user pointer (in the access_ok sense) on 32-bit and

	 * 64-bit, so we don't need to special-case it here.  For all the

	 * vsyscalls, NULL means "don't write anything" not "write it at

	 * address 0".

	/*

	 * Handle seccomp.  regs->ip must be the original value.

	 * See seccomp_send_sigsys and Documentation/userspace-api/seccomp_filter.rst.

	 *

	 * We could optimize the seccomp disabled case, but performance

	 * here doesn't matter.

 skip requested */

	/*

	 * With a real vsyscall, page faults cause SIGSEGV.  We want to

	 * preserve that behavior to make writing exploits harder.

 this decodes regs->di and regs->si on its own */

 this decodes regs->di on its own */

 while we could clobber regs->dx, we didn't in the past... */

 this decodes regs->di, regs->si and regs->dx on its own */

 Bad news -- userspace fed a bad pointer to a vsyscall. */

		/*

		 * If we failed to generate a signal for any reason,

		 * generate one here.  (This should be impossible.)

 Don't emulate the ret. */

 Emulate a ret instruction. */

/*

 * A pseudo VMA to allow ptrace access for the vsyscall page.  This only

 * covers the 64bit vsyscall page now. 32bit has a real VMA now and does

 * not need special handling anymore:

/*

 * Use this when you have no reliable mm, typically from interrupt

 * context. It is less reliable than using a task's mm and may give

 * false positives.

/*

 * The VSYSCALL page is the only user-accessible page in the kernel address

 * range.  Normally, the kernel page tables can have _PAGE_USER clear, but

 * the tables covering VSYSCALL_ADDR need _PAGE_USER set if vsyscalls

 * are enabled.

 *

 * Some day we may create a "minimal" vsyscall mode in which we emulate

 * vsyscalls but leave the page not present.  If so, we skip calling

 * this.

	/*

	 * For full emulation, the page needs to exist for real.  In

	 * execute-only mode, there is no PTE at all backing the vsyscall

	 * page.

 SPDX-License-Identifier: GPL-2.0

/*

 * Copyright © 2019 Oracle and/or its affiliates. All rights reserved.

 * Copyright © 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.

 *

 * KVM Xen emulation

 Paranoia checks on the 32-bit struct layout */

 32-bit location by default */

 Paranoia checks on the 64-bit struct layout */

	/*

	 * Time waiting for the scheduler isn't "stolen" if the

	 * vCPU wasn't running anyway.

	/*

	 * The only difference is alignment of uint64_t in 32-bit.

	 * So the first field 'state' is accessed directly using

	 * offsetof() (where its offset happens to be zero), while the

	 * remaining fields which are all uint64_t, start at 'offset'

	 * which we tweak here by adding 4.

	/*

	 * First write the updated state_entry_time at the appropriate

	 * location determined by 'offset'.

	/*

	 * Next, write the new runstate. This is in the *same* place

	 * for 32-bit and 64-bit guests, asserted here for paranoia.

	/*

	 * Write the actual runstate times immediately after the

	 * runstate_entry_time.

	/*

	 * Finally, clear the XEN_RUNSTATE_UPDATE bit in the guest's

	 * runstate_entry_time field.

	/*

	 * If the global upcall vector (HVMIRQ_callback_vector) is set and

	 * the vCPU's evtchn_upcall_pending flag is set, the IRQ is pending.

 No need for compat handling here */

	/*

	 * For efficiency, this mirrors the checks for using the valid

	 * cache in kvm_read_guest_offset_cached(), but just uses

	 * __get_user() instead. And falls back to the slow path.

 Fast path */

 Slow path */

	/*

	 * This function gets called from kvm_vcpu_block() after setting the

	 * task to TASK_INTERRUPTIBLE, to see if it needs to wake immediately

	 * from a HLT. So we really mustn't sleep. If the page ended up absent

	 * at that point, just return 1 in order to trigger an immediate wake,

	 * and we'll end up getting called again from a context where we *can*

	 * fault in the page and wait for it.

 No compat necessary here. */

 The adjustment must add up */

 Latch long_mode for shared_info pages etc. */

	/*

	 * If Xen hypercall intercept is enabled, fill the hypercall

	 * page with VMCALL/VMMCALL instructions since that's what

	 * we catch. Else the VMM has provided the hypercall pages

	 * with instructions of its own choosing, so use those.

 mov imm32, %eax */

 vmcall / vmmcall */

 ret */

 int3 to pad */

		/*

		 * Note, truncation is a non-issue as 'lm' is guaranteed to be

		 * false for a 32-bit kernel, i.e. when hva_t is only 4 bytes.

	/*

	 * With hypercall interception the kernel generates its own

	 * hypercall page so it must not be provided.

 Hyper-V hypercalls get bit 31 set in EAX */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 * cpuid support routines

 *

 * derived from arch/x86/kvm/x86.c

 *

 * Copyright 2011 Red Hat, Inc. and/or its affiliates.

 * Copyright IBM Corporation, 2008

/*

 * Unlike "struct cpuinfo_x86.x86_capability", kvm_cpu_caps doesn't need to be

 * aligned to sizeof(unsigned long) because it's not accessed via bitops.

/*

 * This one is tied to SSB in the user API, and not

 * visible in /proc/cpuinfo.

 Predictive Store Forwarding Disable */

	/*

	 * The existing code assumes virtual address is 48-bit or 57-bit in the

	 * canonical address checks; exit if it is ever changed.

	/*

	 * save the feature bitmap to avoid cpuid lookup for every PV

	 * operation

 Update OSXSAVE bit */

	/*

	 * Bits 127:0 of the allowed SECS.ATTRIBUTES (CPUID.0x12.0x1) enumerate

	 * the supported XSAVE Feature Request Mask (XFRM), i.e. the enclave's

	 * requested XCR0 value.  The enclave's XFRM must be a subset of XCRO

	 * at the time of EENTER, thus adjust the allowed XFRM by the guest's

	 * supported XCR0.  Similar to XCR0 handling, FP and SSE are forced to

	 * '1' even on CPUs that don't support XSAVE.

 Invoke the vendor callback only after the above state is updated. */

	/*

	 * Except for the MMU, which needs to do its thing any vendor specific

	 * adjustments to the reserved GPA bits.

/*

 * This "raw" version returns the reserved GPA bits without any adjustments for

 * encryption technologies that usurp bits.  The raw mask should be used if and

 * only if hardware does _not_ strip the usurped bits, e.g. in virtual MTRRs.

 when an old userspace process fills a new kernel module */

 Mask kvm_cpu_caps for @leaf with the raw CPUID capabilities of this CPU. */

 Use kvm_cpu_cap_mask for non-scattered leafs. */

 Use kvm_cpu_cap_init_scattered for scattered leafs. */

		/*

		 * NOTE: MONITOR (and MWAIT) are emulated as NOP, but *not*

		 * advertised to guests via CPUID!

 DTES64, MONITOR */ |

 DS-CPL, VMX, SMX, EST */ |

 TM2 */ | F(SSSE3) | 0  Reserved */ |

 xTPR Update */ | F(PDCM) |

 Reserved, DCA */ | F(XMM4_1) |

 Reserved*/ | F(AES) | F(XSAVE) | 0 
 KVM emulates x2apic in software irrespective of host support. */

 Reserved */ | F(SEP) |

 PSN */ | F(CLFLUSH) |

 Reserved, DS, ACPI */ | F(MMX) |

 HTT, TM, Reserved, PBE */

MPX*/ | F(RDSEED) |

INTEL_PT*/

OSPKE*/ | F(RDPID) |

WAITPKG*/ |

 Set LA57 based on hardware capability. */

	/*

	 * PKU not yet implemented for shadow paging and requires OSPKE

	 * to be set on the host. Clear it if that is not the case

 TSC_ADJUST and ARCH_CAPABILITIES are emulated in software. */

SVM*/ | 0 
 IBS */ | F(XOP) |

 SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |

 Reserved */ | F(SYSCALL) |

 Reserved */ |

 Reserved */ | F(MMXEXT) | F(MMX) |

 Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW)

	/*

	 * AMD has separate bits for each SPEC_CTRL bit.

	 * arch/x86/kernel/cpu/bugs.c is kind enough to

	 * record that in cpufeatures so use them.

	/*

	 * The preference is to use SPEC CTRL MSR instead of the

	 * VIRT_SPEC MSR.

	/*

	 * Hide all SVM features by default, SVM will set the cap bits for

	 * features it emulates and/or exposes for L1.

 SME */ | F(SEV) | 0 
	/*

	 * Hide RDTSCP and RDPID if either feature is reported as supported but

	 * probing MSR_TSC_AUX failed.  This is purely a sanity check and

	 * should never happen, but the guest will likely crash if RDTSCP or

	 * RDPID is misreported, and KVM has botched MSR_TSC_AUX emulation in

	 * the past.  For example, the sanity check may fire if this instance of

	 * KVM is running as L1 on top of an older, broken KVM.

 all calls to cpuid_count() should be made on the same cpu */

 Limited to the highest leaf implemented in KVM. */

		/*

		 * On ancient CPUs, function 2 entries are STATEFUL.  That is,

		 * CPUID(function=2, index=0) may return different results each

		 * time, with the least-significant byte in EAX enumerating the

		 * number of times software should do CPUID(2, 0).

		 *

		 * Modern CPUs, i.e. every CPU KVM has *ever* run on are less

		 * idiotic.  Intel's SDM states that EAX & 0xff "will always

		 * return 01H. Software should ignore this value and not

		 * interpret it as an informational descriptor", while AMD's

		 * APM states that CPUID(2) is reserved.

		 *

		 * WARN if a frankenstein CPU that supports virtualization and

		 * a stateful CPUID.0x2 is encountered.

 functions 4 and 0x8000001d have additional index. */

		/*

		 * Read entries until the cache type in the previous entry is

		 * zero, i.e. indicates an invalid entry.

 Thermal management */

 allow ARAT */

 function 7 has additional index. */

 KVM only supports 0x7.0 and 0x7.1, capped above via min(). */

 Architectural Performance Monitoring */

		/*

		 * Only support guest architectural pmu on a host

		 * with architectural pmu.

	/*

	 * Per Intel's SDM, the 0x1f is a superset of 0xb,

	 * thus they can be handled by common code.

		/*

		 * Populate entries until the level type (ECX[15:8]) of the

		 * previous entry is zero.  Note, CPUID EAX.{0x1f,0xb}.0 is

		 * the starting entry, filled by the primary do_host_cpuid().

			/*

			 * The supported check above should have filtered out

			 * invalid sub-leafs.  Only valid sub-leafs should

			 * reach this point, and they should have a non-zero

			 * save state size.  Furthermore, check whether the

			 * processor agrees with supported_xcr0/supported_xss

			 * on whether this is an XCR0- or IA32_XSS-managed area.

 Intel SGX */

		/*

		 * Index 0: Sub-features, MISCSELECT (a.k.a extended features)

		 * and max enclave sizes.   The SGX sub-features and MISCSELECT

		 * are restricted by kernel and KVM capabilities (like most

		 * feature flags), while enclave size is unrestricted.

		/*

		 * Index 1: SECS.ATTRIBUTES.  ATTRIBUTES are restricted a la

		 * feature flags.  Advertise all supported flags, including

		 * privileged attributes that require explicit opt-in from

		 * userspace.  ATTRIBUTES.XFRM is not adjusted as userspace is

		 * expected to derive it from supported XCR0.

 Intel PT */

 L2 cache and TLB: pass through host info. */

 Advanced power management */

 invariant TSC is CPUID.80000007H:EDX[8] */

 mask against host */

		/*

		 * If TDP (NPT) is disabled use the adjusted host MAXPHYADDR as

		 * the guest operates in the same PA space as the host, i.e.

		 * reductions in MAXPHYADDR for memory encryption affect shadow

		 * paging, too.

		 *

		 * If TDP is enabled but an explicit guest MAXPHYADDR is not

		 * provided, use the raw bare metal MAXPHYADDR as reductions to

		 * the HPAs do not affect GPAs.

 SVM revision 1 */

		entry->ebx = 8; /* Lets support 8 ASIDs in case we add proper

 Reserved */

			/*

			 * Enumerate '0' for "PA bits reduction", the adjusted

			 * MAXPHYADDR is enumerated directly (see 0x80000008).

Add support for Centaur's CPUID instruction*/

Just support up to 0xC0000004 now*/

 Processor serial number */

 MONITOR/MWAIT */

	/*

	 * We want to make sure that ->padding is being passed clean from

	 * userspace in case we want to use it for something in the future.

	 *

	 * Sadly, this wasn't enforced for KVM_GET_SUPPORTED_CPUID and so we

	 * have to give ourselves satisfied only with the emulated side. /me

	 * sheds a tear.

/*

 * Intel CPUID semantics treats any query for an out-of-range leaf as if the

 * highest basic leaf (i.e. CPUID.0H:EAX) were requested.  AMD CPUID semantics

 * returns all zeroes for any undefined leaf, whether or not the leaf is in

 * range.  Centaur/VIA follows Intel semantics.

 *

 * A leaf is considered out-of-range if its function is higher than the maximum

 * supported leaf of its associated class or if its associated class does not

 * exist.

 *

 * There are three primary classes to be considered, with their respective

 * ranges described as "<base> - <top>[,<base2> - <top2>] inclusive.  A primary

 * class exists if a guest CPUID entry for its <base> leaf exists.  For a given

 * class, CPUID.<base>.EAX contains the max supported leaf for the class.

 *

 *  - Basic:      0x00000000 - 0x3fffffff, 0x50000000 - 0x7fffffff

 *  - Hypervisor: 0x40000000 - 0x4fffffff

 *  - Extended:   0x80000000 - 0xbfffffff

 *  - Centaur:    0xc0000000 - 0xcfffffff

 *

 * The Hypervisor class is further subdivided into sub-classes that each act as

 * their own independent class associated with a 0x100 byte range.  E.g. if Qemu

 * is advertising support for both HyperV and KVM, the resulting Hypervisor

 * CPUID sub-classes are:

 *

 *  - HyperV:     0x40000000 - 0x400000ff

 *  - KVM:        0x40000100 - 0x400001ff

	/*

	 * Leaf specific adjustments are also applied when redirecting to the

	 * max basic entry, e.g. if the max basic leaf is 0xb but there is no

	 * entry for CPUID.0xb.index (see below), then the output value for EDX

	 * needs to be pulled from CPUID.0xb.1.

	/*

	 * The class does not exist or the requested function is out of range;

	 * the effective CPUID entry is the max basic leaf.  Note, the index of

	 * the original requested leaf is observed!

		/*

		 * When leaf 0BH or 1FH is defined, CL is pass-through

		 * and EDX is always the x2APIC ID, even for undefined

		 * subleaves. Index 1 will exist iff the leaf is

		 * implemented, so we pass through CL iff leaf 1

		 * exists. EDX can be copied from any existing index.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * derived from drivers/kvm/kvm_main.c

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright (C) 2008 Qumranet, Inc.

 * Copyright IBM Corporation, 2008

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Avi Kivity   <avi@qumranet.com>

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Amit Shah    <amit.shah@qumranet.com>

 *   Ben-Ami Yassour <benami@il.ibm.com>

/* EFER defaults:

 * - enable syscall per default because its emulated by KVM

 * - enable LME and LMA per default on 64 bit KVM

 tsc tolerance in parts per million - default to 1/2 of the NTP threshold */

/*

 * lapic timer advance (tscdeadline mode only) in nanoseconds.  '-1' enables

 * adaptive tuning starting from default advancement of 1000ns.  '0' disables

 * advancement entirely.  Any other value is used as-is and disables adaptive

 * tuning, i.e. allows privileged userspace to set an exact advancement time.

/*

 * Restoring the host value for MSRs that are only consumed when running in

 * usermode, e.g. SYSCALL MSRs and TSC_AUX, can be deferred until the CPU

 * returns to userspace, i.e. the kernel can run with the guest's value.

/*

 * When called, it means the previous get/set msr reached an invalid msr.

 * Return true if we want to ignore/silent this failed msr access.

 Mask the error */

	/*

	 * Disabling irqs at this point since the following code could be

	 * interrupted and executed through kvm_arch_hardware_disable()

/*

 * Handle a fault on a hardware virtualization (VMX or SVM) instruction.

 *

 * Hardware virtualization extension instructions may fault if a reboot turns

 * off virtualization while processes are running.  Usually after catching the

 * fault we just panic; during reboot instead the instruction is ignored.

 Fault while not rebooting.  We want the trace. */

 #DB is trap, as instruction watchpoints are handled elsewhere */

 Reserved exceptions will result in fault */

		/*

		 * "Certain debug exceptions may clear bit 0-3.  The

		 * remaining contents of the DR6 register are never

		 * cleared by the processor".

		/*

		 * In order to reflect the #DB exception payload in guest

		 * dr6, three components need to be considered: active low

		 * bit, FIXED_1 bits and active high bits (e.g. DR6_BD,

		 * DR6_BS and DR6_BT)

		 * DR6_ACTIVE_LOW contains the FIXED_1 and active low bits.

		 * In the target guest dr6:

		 * FIXED_1 bits should always be set.

		 * Active low bits should be cleared if 1-setting in payload.

		 * Active high bits should be set if 1-setting in payload.

		 *

		 * Note, the payload is compatible with the pending debug

		 * exceptions/exit qualification under VMX, that active_low bits

		 * are active high in payload.

		 * So they need to be flipped for DR6.

		/*

		 * The #DB payload is defined as compatible with the 'pending

		 * debug exceptions' field under VMX, not DR6. While bit 12 is

		 * defined in the 'pending debug exceptions' field (enabled

		 * breakpoint), it is reserved and must be zero in DR6.

			/*

			 * On vmentry, vcpu->arch.exception.pending is only

			 * true if an event injection was blocked by

			 * nested_run_pending.  In that case, however,

			 * vcpu_enter_guest requests an immediate exit,

			 * and the guest shouldn't proceed far enough to

			 * need reinjection.

				/*

				 * A reinjected event has already

				 * delivered its payload.

 to check exception */

 triple fault -> shutdown */

		/*

		 * Generate double fault per SDM Table 5-5.  Set

		 * exception.pending = true so that the double fault

		 * can trigger a nested vmexit.

		/* replace previous exception with a new one in a hope

		   that instruction re-execution will regenerate lost

	/*

	 * Invalidate the TLB entry for the faulting address, if it exists,

	 * else the access will fault indefinitely (and to emulate hardware).

/*

 * Checks if cpl <= required_cpl; if true, return true.  Otherwise queue

 * a #GP and return false.

/*

 * Load the pae pdptrs.  Return 1 if they are all valid, 0 otherwise.

	/*

	 * If the MMU is nested, CR3 holds an L2 GPA and needs to be translated

	 * to an L1 GPA.

 Note the offset, PDPTRs are 32 byte aligned when using PAE paging. */

 Only support XCR_XFEATURE_ENABLED_MASK(xcr0) now  */

	/*

	 * Do not allow the guest to set bits that we do not support

	 * saving.  However, xcr0 bit 0 is always set, even if the

	 * emulated CPU does not support XSAVE (see kvm_vcpu_reset()).

	/*

	 * If any role bit is changed, the MMU needs to be reset.

	 *

	 * If CR4.PCIDE is changed 1 -> 0, the guest TLB must be flushed.

	 * If CR4.PCIDE is changed 0 -> 1, there is no need to flush the TLB

	 * according to the SDM; however, stale prev_roots could be reused

	 * incorrectly in the future after a MOV to CR3 with NOFLUSH=1, so we

	 * free them all.  KVM_REQ_MMU_RELOAD is fit for the both cases; it

	 * is slow, but changing CR4.PCIDE is a rare case.

	 *

	 * If CR4.PGE is changed, the guest TLB must be flushed.

	 *

	 * Note: resetting MMU is a superset of KVM_REQ_MMU_RELOAD and

	 * KVM_REQ_MMU_RELOAD is a superset of KVM_REQ_TLB_FLUSH_GUEST, hence

	 * the usage of "else if".

 PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */

	/*

	 * MOV CR3 and INVPCID are usually not intercepted when using TDP, but

	 * this is reachable when running EPT=1 and unrestricted_guest=0,  and

	 * also via the emulator.  KVM's TDP page tables are not in the scope of

	 * the invalidation, but the guest's TLB entries need to be flushed as

	 * the CPU may have cached entries in its TLB for the target PCID.

	/*

	 * If neither the current CR3 nor any of the prev_roots use the given

	 * PCID, then nothing needs to be done here because a resync will

	 * happen anyway before switching to any other CR3.

	/*

	 * If PCID is disabled, there is no need to free prev_roots even if the

	 * PCIDs for them are also 0, because MOV to CR3 always flushes the TLB

	 * with PCIDE=0.

 PDPTRs are always reloaded for PAE paging. */

	/*

	 * Do not condition the GPA check on long mode, this helper is used to

	 * stuff CR3, e.g. for RSM emulation, and there is no guarantee that

	 * the current vCPU mode is accurate.

	/*

	 * A load of CR3 that flushes the TLB flushes only the current PCID,

	 * even if PCID is disabled, in which case PCID=0 is flushed.  It's a

	 * moot point in the end because _disabling_ PCID will flush all PCIDs,

	 * and it's impossible to use a non-zero PCID when PCID is disabled,

	 * i.e. only PCID=0 can be relevant.

 #GP */

 7 */

 #GP */

 7 */

/*

 * List of msr numbers which we expose to userspace through KVM_GET_MSRS

 * and KVM_SET_MSRS, and KVM_GET_MSR_INDEX_LIST.

 *

 * The three MSR lists(msrs_to_save, emulated_msrs, msr_based_features)

 * extract the supported MSRs from the related const lists.

 * msrs_to_save is selected from the msrs_to_save_all to reflect the

 * capabilities of the host cpu. This capabilities test skips MSRs that are

 * kvm-specific. Those are put in emulated_msrs_all; filtering of emulated_msrs

 * may depend on host virtualization features rather than host cpu features.

	/*

	 * The following list leaves out MSRs whose values are determined

	 * by arch/x86/kvm/vmx/nested.c based on CPUID or other MSRs.

	 * We always support the "true" VMX control MSRs, even if the host

	 * processor does not, so I am putting these registers here rather

	 * than in msrs_to_save_all.

/*

 * List of msr numbers which are used to expose MSR-based features that

 * can be used by a hypervisor to validate requested CPU features.

	/*

	 * If nx_huge_pages is enabled, KVM's shadow paging will ensure that

	 * the nested hypervisor runs with NX huge pages.  If it is not,

	 * L1 is anyway vulnerable to ITLB_MULTIHIT exploits from other

	 * L1 guests, so it need not worry about its own (L2) guests.

	/*

	 * If we're doing cache flushes (either "always" or "cond")

	 * we will do one whenever the guest does a vmlaunch/vmresume.

	 * If an outer hypervisor is doing the cache flush for us

	 * (VMENTER_L1D_FLUSH_NESTED_VM), we can safely pass that

	 * capability to the guest too, and if EPT is disabled we're not

	 * vulnerable.  Overall, only VMENTER_L1D_FLUSH_NEVER will

	 * require a nested hypervisor to do a flush of its own.

		/*

		 * If RTM=0 because the kernel has disabled TSX, the host might

		 * have TAA_NO or TSX_CTRL.  Clear TAA_NO (the guest sees RTM=0

		 * and therefore knows that there cannot be TAA) but keep

		 * TSX_CTRL: some buggy userspaces leave it set on tsx=on hosts,

		 * and we want to allow migrating those guests to tsx=off hosts.

		/*

		 * Nothing to do here; we emulate TSX_CTRL if present on the

		 * host so the guest can choose between disabling TSX or

		 * using VERW to clear CPU buffers.

 Unconditionally clear the output for simplicity */

 Update reserved bits */

 x2APIC MSRs do not support filtering. */

/*

 * Write @data into the MSR specified by @index.  Select MSR specific fault

 * checks are bypassed if @host_initiated is %true.

 * Returns 0 on success, non-0 otherwise.

 * Assumes vcpu_load() was already called.

		/*

		 * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if

		 * non-canonical address is written on Intel but not on

		 * AMD (which ignores the top 32-bits, because it does

		 * not implement 64-bit SYSENTER).

		 *

		 * 64-bit code should hence be able to write a non-canonical

		 * value on AMD.  Making the address canonical ensures that

		 * vmentry does not fail on Intel after writing a non-canonical

		 * value, and that something deterministic happens if the guest

		 * invokes 64-bit SYSENTER.

		/*

		 * Per Intel's SDM, bits 63:32 are reserved, but AMD's APM has

		 * incomplete and conflicting architectural behavior.  Current

		 * AMD CPUs completely ignore bits 63:32, i.e. they aren't

		 * reserved and always read as zeros.  Enforce Intel's reserved

		 * bits check if and only if the guest CPU is Intel, and clear

		 * the bits in all other cases.  This ensures cross-vendor

		 * migration will provide consistent behavior for the guest.

/*

 * Read the MSR specified by @index into @data.  Select MSR specific fault

 * checks are bypassed if @host_initiated is %true.

 * Returns 0 on success, non-0 otherwise.

 * Assumes vcpu_load() was already called.

 Unconditionally clear *data for simplicity */

 Check if the user wanted to know about this MSR fault */

 MSR read failed? See if we should ask user space */

 Bounce to user space */

 MSR write failed? See if we should ask user space */

 Bounce to user space */

 Signal all other negative errors to userspace */

 Treat an INVD instruction as a NOP and just skip it. */

/*

 * The fast path for frequent and performance sensitive wrmsr emulation,

 * i.e. the sending of IPI, sending IPI early in the VM-Exit flow reduces

 * the latency of virtual IPI by avoiding the expensive bits of transitioning

 * from guest to host, e.g. reacquiring KVM's SRCU lock. In contrast to the

 * other cases which must be called after interrupts are enabled on the host.

/*

 * Adapt set_msr() to msr_io()'s calling convention

 extract of a clocksource struct */

 extract of a clocksource struct */

 copy pvclock gtod data */

 Count up from boot time, but with the frequency of the raw clock.  */

 Master clock not used, so we can just use CLOCK_BOOTTIME.  */

 first time write, random junk */

	/*

	 * The guest calculates current wall clock time by adding

	 * system time (updated by kvm_guest_time_update below) to the

	 * wall clock specified here.  We do the reverse here.

 overflow in 2106 guest time */

 we verify if the enable bit is set... */

 Guest TSC same frequency as host TSC? */

 TSC scaling supported? */

 TSC scaling required  - calculate ratio */

 tsc_khz can be zero if TSC calibration fails */

 set tsc_scaling_ratio to a safe value */

 Compute a scale to convert nanoseconds in TSC cycles */

	/*

	 * Compute the variation in TSC rate which is acceptable

	 * within the range of tolerance and decide if the

	 * rate being applied is within that bounds of the hardware

	 * rate.  If so, no scaling or compensation need be done.

	/*

	 * Once the masterclock is enabled, always perform request in

	 * order to update it.

	 *

	 * In order to enable masterclock, the host clocksource must be TSC

	 * and the vcpus need to have matched TSCs.  When that happens,

	 * perform request to enable masterclock.

/*

 * Multiply tsc by a fixed point number represented by ratio.

 *

 * The most significant 64-N bits (mult) of ratio represent the

 * integral part of the fixed point number; the remaining N bits

 * (frac) represent the fractional part, ie. ratio represents a fixed

 * point number (mult + frac * 2^(-N)).

 *

 * N equals to kvm_tsc_scaling_ratio_frac_bits.

	/*

	 * If we are here because L1 chose not to trap WRMSR to TSC then

	 * according to the spec this should set L1's TSC (as opposed to

	 * setting L1's offset for L2).

 Userspace is changing the multiplier while L2 is active */

	/*

	 * TSC is marked unstable when we're running on Hyper-V,

	 * 'TSC page' clocksource is good.

/*

 * Infers attempts to synchronize the guest's tsc from host writes. Sets the

 * offset for the vcpu and tracks the TSC matching generation that the vcpu

 * participates in.

	/*

	 * We also track th most recent recorded KHZ, write and time to

	 * allow the matching interval to be extended at each write.

		/*

		 * We split periods of matched TSC writes into generations.

		 * For each generation, we track the original measured

		 * nanosecond time, offset, and write, so if TSCs are in

		 * sync, we can match exact offset, and if not, we can match

		 * exact software computation in compute_guest_tsc()

		 *

		 * These values are tracked in kvm->arch.cur_xxx variables.

 Keep track of which generation this VCPU has synchronized to */

			/*

			 * detection of vcpu initialization -- need to sync

			 * with other vCPUs. This particularly helps to keep

			 * kvm_clock stable after CPU hotplug

			/*

			 * Special case: TSC write with a small delta (1 second)

			 * of virtual cycle time against real time is

			 * interpreted as an attempt to synchronize the CPU.

	/*

	 * For a reliable TSC, we can match TSC offsets, and for an unstable

	 * TSC, we add elapsed time in this computation.  We could let the

	 * compensation code attempt to catch up if we fall behind, but

	 * it's better to try to match offsets from the beginning.

	/*

	 * GCC likes to generate cmov here, but this branch is extremely

	 * predictable (it's just a function of time and the likely is

	 * very likely) and there's a data dependence, so force GCC

	 * to generate a branch instead.  I don't barrier() because

	 * we don't actually need a barrier, and if this function

	 * ever gets inlined it will generate worse code.

 TSC page valid */

 TSC page invalid */

 returns true if host is using TSC based clocksource */

 checked again under seqlock below */

 returns true if host is using TSC based clocksource */

 checked again under seqlock below */

/*

 *

 * Assuming a stable TSC across physical CPUS, and a stable TSC

 * across virtual CPUs, the following condition is possible.

 * Each numbered line represents an event visible to both

 * CPUs at the next numbered event.

 *

 * "timespecX" represents host monotonic time. "tscX" represents

 * RDTSC value.

 *

 * 		VCPU0 on CPU0		|	VCPU1 on CPU1

 *

 * 1.  read timespec0,tsc0

 * 2.					| timespec1 = timespec0 + N

 * 					| tsc1 = tsc0 + M

 * 3. transition to guest		| transition to guest

 * 4. ret0 = timespec0 + (rdtsc - tsc0) |

 * 5.				        | ret1 = timespec1 + (rdtsc - tsc1)

 * 				        | ret1 = timespec0 + N + (rdtsc - (tsc0 + M))

 *

 * Since ret0 update is visible to VCPU1 at time 5, to obey monotonicity:

 *

 * 	- ret0 < ret1

 *	- timespec0 + (rdtsc - tsc0) < timespec0 + N + (rdtsc - (tsc0 + M))

 *		...

 *	- 0 < N - M => M < N

 *

 * That is, when timespec0 != timespec1, M < N. Unfortunately that is not

 * always the case (the difference between two distinct xtime instances

 * might be smaller then the difference between corresponding TSC reads,

 * when updating guest vcpus pvclock areas).

 *

 * To avoid that problem, do not allow visibility of distinct

 * system_timestamp/tsc_timestamp values simultaneously: use a master

 * copy of host monotonic time values. Update that master copy

 * in lockstep.

 *

 * Rely on synchronization of host TSCs and guest TSCs for monotonicity.

 *

	/*

	 * If the host uses TSC clock, then passthrough TSC as stable

	 * to the guest.

 no guest entries from this point */

 guest entries allowed */

 Called within read_seqcount_begin/retry for kvm->pvclock_sc.  */

 both __this_cpu_read() and rdtsc() should be on the same cpu */

	/* This VCPU is paused, but it's legal for a guest to read another

	 * VCPU's kvmclock, so we really have to follow the specification where

	 * it says that version is odd if data is being modified, and even after

	 * it is consistent.

	 *

	 * Version field updates must be kept separate.  This is because

	 * kvm_write_guest_cached might use a "rep movs" instruction, and

	 * writes within a string instruction are weakly ordered.  So there

	 * are three writes overall.

	 *

	 * As a small optimization, only write the version field in the first

	 * and third write.  The vcpu->pv_time cache is still valid, because the

	 * version field is the first in the struct.

 first time write, random junk */

 retain PVCLOCK_GUEST_STOPPED if set in guest copy */

	/*

	 * If the host uses TSC clock, then passthrough TSC as stable

	 * to the guest.

 Keep irq disabled to prevent changes to the clock */

	/*

	 * We may have to catch up the TSC to match elapsed wall clock

	 * time for two reasons, even if kvmclock is used.

	 *   1) CPU could have been running below the maximum TSC rate

	 *   2) Broken TSC compensation resets the base at each VCPU

	 *      entry to avoid unknown leaps of TSC even when running

	 *      again on the same CPU.  This may cause apparent elapsed

	 *      time to disappear, and the guest to stand still or run

	 *	very slowly.

 With all the info we got, fill in the values */

 If the host uses TSC clocksource, then it is stable */

/*

 * kvmclock updates which are isolated to a given vcpu, such as

 * vcpu->cpu migration, should not allow system_timestamp from

 * the rest of the vcpus to remain static. Otherwise ntp frequency

 * correction applies to one vcpu's system_timestamp but not

 * the others.

 *

 * So in those cases, request a kvmclock update for all vcpus.

 * We need to rate-limit these requests though, as they can

 * considerably slow guests that have a large number of vcpus.

 * The time for a remote vcpu to update its kvmclock is bound

 * by the delay we use to rate-limit the updates.

/*

 * On AMD, HWCR[McStatusWrEn] controls whether setting MCi_STATUS results in #GP.

 McStatusWrEn enabled? */

			/* only 0 or all 1s can be written to IA32_MCi_CTL

			 * some Linux kernels though clear bit 10 in bank 4 to

			 * workaround a BIOS/GART TBL issue on AMD K8s, ignore

			 * this to avoid an uncatched #GP in the guest

 MCi_STATUS */

 Bits 4:5 are reserved, Should be zero */

 Bits 8-63 are reserved */

		/*

		 * A TLB flush on behalf of the guest is equivalent to

		 * INVPCID(all), toggling CR4.PGE, etc., which requires

		 * a forced sync of the shadow page tables.  Ensure all the

		 * roots are synced and the guest TLB in hardware is clean.

 We rely on the fact that it fits in a single page. */

	/*

	 * Doing a TLB flush here, on the guest's behalf, can avoid

	 * expensive IPIs.

 first time write, random junk */

 ignore flush filter disable */

 ignore ignne emulation enable */

 ignore TLB cache disable */

 Handle McStatusWrEn */

				/* Before back to guest, tsc_timestamp must be adjusted

				 * as well, otherwise guest's percpu pvclock time could jump.

		/*

		 * KVM supports exposing PT to the guest, but does not support

		 * IA32_XSS[bit 8]. Guests have to use RDMSR/WRMSR rather than

		 * XSAVES/XRSTORS to save/restore PT MSRs.

 only enable bit supported */

		/*

		 * Ignore all writes to this no longer documented MSR.

		 * Writes are only relevant for old K7 processors,

		 * all pre-dating SVM, but a recommended workaround from

		 * AMD for these chips. It is possible to specify the

		 * affected processor models on the command line, hence

		 * the need to ignore the workaround.

		/* Drop writes to this legacy MSR -- see rdmsr

		 * counterpart for further detail.

	/*

	 * Intel Sandy Bridge CPUs must support the RAPL (running average power

	 * limit) MSRs. Just return 0, as we do not want to expose the host

	 * data here. Do not conditionalize this on CPUID, as KVM does not do

	 * so for existing CPU-specific MSRs.

 Power plane 0 (core) */

 Power plane 1 (graphics uncore) */

 Total package */

 DRAM controller */

		/*

		 * Intel SDM states that MSR_IA32_TSC read adds the TSC offset

		 * even when not intercepted. AMD manual doesn't explicitly

		 * state this but appears to behave the same.

		 *

		 * On userspace reads and writes, however, we unconditionally

		 * return L1's TSC value to ensure backwards-compatible

		 * behavior for migration.

 fsb frequency */

		/*

		 * MSR_EBC_FREQUENCY_ID

		 * Conservative value valid for even the basic CPU models.

		 * Models 0,1: 000 in bits 23:21 indicating a bus speed of

		 * 100MHz, model 2 000 in bits 18:16 indicating 100MHz,

		 * and 266MHz for model 3, or 4. Set Core Clock

		 * Frequency to System Bus Frequency Ratio to 1 (bits

		 * 31:24) even though these are only valid for CPU

		 * models > 2, however guests may end up dividing or

		 * multiplying by zero otherwise.

 TSC increment by tick */

 CPU multiplier */

		/*

		 * Provide expected ramp-up count for K7. All other

		 * are set to zero, indicating minimum divisors for

		 * every field.

		 *

		 * This prevents guest kernels on AMD host with CPU

		 * type 6, model 8 and higher from exploding due to

		 * the rdmsr failing.

		/* This legacy MSR exists but isn't fully documented in current

		 * silicon.  It is however accessed by winxp in very narrow

		 * scenarios where it sets bit #19, itself documented as

		 * a "reserved" bit.  Best effort attempt to source coherent

		 * read data here should the balance of the register be

		 * interpreted by the guest:

		 *

		 * L2 cache control register 3: 64GB range, 256KB size,

		 * enabled, latency 0x1, configured

/*

 * Read or write a bunch of msrs. All parameters are kernel addresses.

 *

 * @return number of msrs set successfully.

/*

 * Read or write a bunch of msrs. Parameters are user addresses.

 *

 * @return number of msrs set successfully.

		/* SMBASE is usually relocated above 1M on modern chipsets,

		 * and SMM handlers might indeed rely on 4G segment limits,

		 * so do not report SMM to be available if real mode is

		 * emulated via vm86 mode.  Still, do not go to great lengths

		 * to avoid userspace's usage of the feature, because it is a

		 * fringe case that is not enabled except via specific settings

		 * of the module parameters.

 obsolete */

 Address WBINVD may be executed by guest */

 Save host pkru register if supported */

 Apply any externally detected TSC adjustments (due to suspend) */

		/*

		 * On a host with synchronized TSC, there is no need to update

		 * kvmclock on vcpu->cpu migration

 This happens on process exit */

	/*

	 * Take the srcu lock as memslots will be accessed to check the gfn

	 * cache generation against the memslots generation.

	/*

	 * We can accept userspace's request for interrupt injection

	 * as long as we have a place to store the interrupt number.

	 * The actual injection will happen when the CPU is able to

	 * deliver the interrupt.

 Acknowledging ExtINT does not happen if LINT0 is masked.  */

	/*

	 * Do not cause an interrupt window exit if an exception

	 * is pending or an event needs reinjection; userspace

	 * might want to inject the interrupt manually using KVM_SET_REGS

	 * or KVM_SET_SREGS.  For that to work, we must be at an

	 * instruction boundary and with no events half-injected.

	/*

	 * With in-kernel LAPIC, we only use this to inject EXTINT, so

	 * fail for in-kernel 8259.

 Init IA32_MCG_CTL to all 1s */

 Init IA32_MCi_CTL to all 1s */

	/*

	 * if IA32_MCG_CTL is not all 1s, the uncorrected error

	 * reporting is disabled

	/*

	 * if IA32_MCi_CTL is not all 1s, the uncorrected error

	 * reporting is disabled for the bank

	/*

	 * In guest mode, payload delivery should be deferred,

	 * so that the L1 hypervisor can intercept #PF before

	 * CR2 is modified (or intercept #DB before DR6 is

	 * modified under nVMX). Unless the per-VM capability,

	 * KVM_CAP_EXCEPTION_PAYLOAD, is set, we may not defer the delivery of

	 * an exception payload and handle after a KVM_GET_VCPU_EVENTS. Since we

	 * opportunistically defer the exception payload, deliver it if the

	 * capability hasn't been requested before processing a

	 * KVM_GET_VCPU_EVENTS.

	/*

	 * The API doesn't provide the instruction length for software

	 * exceptions, so don't report them. As long as the guest RIP

	 * isn't advanced, we should expect to encounter the exception

	 * again.

		/*

		 * For ABI compatibility, deliberately conflate

		 * pending and injected exceptions when

		 * KVM_CAP_EXCEPTION_PAYLOAD isn't enabled.

 never valid when reporting to user space */

 INITs are latched while in SMM */

 Only support XCR0 currently */

/*

 * kvm_set_guest_paused() indicates to the guest kernel that it has been

 * stopped by the hypervisor.  This function will be called from the host only.

 * EINVAL is returned when the host attempts to set the flag for a guest that

 * does not support pv clocks.

 nested_run_pending implies guest_mode.  */

	/* pit->pit_state.lock was overloaded to prevent userspace from getting

	 * an inconsistent state after running multiple KVM_REINJECT_CONTROL

	 * ioctls in parallel.  Use a separate lock if that ioctl isn't rare.

	/*

	 * Flush all CPUs' dirty log buffers to the  dirty_bitmap.  Called

	 * before reporting dirty_bitmap to userspace.  KVM flushes the buffers

	 * on all VM-Exits, thus we only need to kick running vCPUs to force a

	 * VM-Exit.

 Pairs with irqchip_in_kernel. */

 KVM only supports the PROVISIONKEY privileged attribute. */

 The per-VM filter is protected by kvm->lock... */

 CONFIG_HAVE_KVM_PM_NOTIFIER */

	/*

	 * Only KVM_CLOCK_REALTIME is used, but allow passing the

	 * result of KVM_GET_CLOCK back to KVM_SET_CLOCK.

	/*

	 * This pairs with kvm_guest_time_update(): when masterclock is

	 * in use, we use master_kernel_ns + kvmclock_offset to set

	 * unsigned 'system_time' so if we use get_kvmclock_ns() (which

	 * is slightly ahead) here we risk going negative on unsigned

	 * 'system_time' when 'data.clock' is very small.

		/*

		 * Avoid stepping the kvmclock backwards.

	/*

	 * This union makes it completely explicit to gcc-3.x

	 * that these two variables' stack usage should be

	 * combined, not added together.

 Write kvm->irq_routing before enabling irqchip_in_kernel. */

 0: PIC master, 1: PIC slave, 2: IOAPIC */

 0: PIC master, 1: PIC slave, 2: IOAPIC */

		/*

		 * Even MSRs that are valid in the host may not be exposed

		 * to the guests in some cases.

 NPT walks are always user-walks */

 uses this to access any guest's mapped memory without checking CPL */

 used for instruction fetching */

 Inline kvm_read_guest_virt_helper for speed.  */

	/*

	 * FIXME: this should call handle_emulation_failure if X86EMUL_IO_NEEDED

	 * is returned, but our callers are not ready for that and they blindly

	 * call kvm_inject_page_fault.  Ensure that they at least do not leak

	 * uninitialized kernel stack memory into cr2 and error code.

 kvm_write_guest_virt_system can pull in tons of pages. */

 ud2; .ascii "kvm" */

 For APIC access vmexit */

	/*

	 * currently PKRU is only applied to ept enabled guest so

	 * there is no pkey in EPT page table for L1 guest or EPT

	 * shadow page table for L2 guest.

	/*

	 * If the exit was due to a NPF we may already have a GPA.

	 * If the GPA is present, use it to avoid the GVA to GPA table walk.

	 * Note, this cannot be used on string operations since string

	 * operation using rep will only have the initial GPA from the NPF

	 * occurred.

	/*

	 * Is this MMIO handled locally?

 Crossing a page boundary? */

 guests cmpxchg8b have to be emulated atomically */

	/*

	 * Emulate the atomic as a straight write to avoid #AC if SLD is

	 * enabled in the host and the access splits a cache line.

 Complete previous iteration.  */

 Results already available, fall through.  */

 Bounce to user space */

 Bounce to user space */

	/*

	 * an sti; sti; sequence only disable interrupts for the first

	 * instruction. So, if the last instruction, be it emulated or

	 * not, left the system with the INT_STI flag enabled, it

	 * means that the last instruction is an sti. We should not

	 * leave the flag on in this case. The same goes for mov ss

	/*

	 * Zero the whole array used to retrieve the exit info, as casting to

	 * u32 for select entries will leave some chunks uninitialized.

	/*

	 * There's currently space for 13 entries, but 5 are used for the exit

	 * reason and info.  Restrict to 4 to reduce the maintenance burden

	 * when expanding kvm_run.emulation_failure in the future.

 Always include the flags as a 'data' entry. */

		/*

		 * Write permission should be allowed since only

		 * write access need to be emulated.

		/*

		 * If the mapping is invalid in guest, let cpu retry

		 * it to generate fault.

	/*

	 * Do not retry the unhandleable instruction if it faults on the

	 * readonly host memory, otherwise it will goto a infinite loop:

	 * retry instruction -> write #PF -> emulation fail -> retry

	 * instruction -> ...

	/*

	 * If the instruction failed on the error pfn, it can not be fixed,

	 * report the error to userspace.

 The instructions are well-emulated on direct mmu. */

	/*

	 * if emulation was due to access to shadowed page table

	 * and it failed try to unshadow page and re-enter the

	 * guest to let CPU execute the instruction.

	/*

	 * If the access faults on its page table, it can not

	 * be fixed by unprotecting shadow page and it should

	 * be reported to userspace.

	/*

	 * If the emulation is caused by #PF and it is non-page_table

	 * writing instruction, it means the VM-EXIT is caused by shadow

	 * page protected, we can zap the shadow page and retry this

	 * instruction directly.

	 *

	 * Note: if the guest uses a non-page-table modifying instruction

	 * on the PDE that points to the instruction, then we will unmap

	 * the instruction and go to an infinite loop. So, we cache the

	 * last retried eip and the last fault address, if we meet the eip

	 * and the address again, we can break out of the potential infinite

	 * loop.

 Process a latched INIT or SMI, if any.  */

		/*

		 * Even if KVM_SET_SREGS2 loaded PDPTRs out of band,

		 * on SMM exit we still need to reload them from

		 * guest memory

	/*

	 * rflags is the old, "raw" value of the flags.  The new value has

	 * not been saved yet.

	 *

	 * This is correct even for TF set by the guest, because "the

	 * processor will not generate this exception after the instruction

	 * that sets the TF flag".

 IN */

 OUT */

 INS */

 OUTS */

 RDPMC */

/*

 * Decode to be emulated instruction. Return EMULATION_OK if success.

	/*

	 * We will reenter on the same instruction since we do not set

	 * complete_userspace_io. This does not handle watchpoints yet,

	 * those would be handled in the emulate_ops.

	/*

	 * Clear write_fault_to_shadow_pgtable here to ensure it is

	 * never reused.

				/*

				 * #UD should result in just EMULATION_FAILED, and trap-like

				 * exception should not be encountered during decode.

	/*

	 * Note, EMULTYPE_SKIP is intended for use *only* by vendor callbacks

	 * for kvm_skip_emulated_instruction().  The caller is responsible for

	 * updating interruptibility state and injecting single-step #DBs.

	/* this is needed for vmware backdoor interface to work since it

 Save the faulting GPA (cr2) in the address field */

 With shadow page tables, cr2 contains a GVA or nGPA. */

 Sanitize the address out of an abundance of paranoia. */

 FIXME: return into emulator if single-stepping.  */

		/*

		 * For STI, interrupts are shadowed; so KVM_REQ_EVENT will

		 * do nothing, and it will be requested again as soon as

		 * the shadow expires.  But we still need to check here,

		 * because POPF has no interrupt shadow.

	/*

	 * Workaround userspace that relies on old KVM behavior of %rip being

	 * incremented prior to exiting to userspace to handle "OUT 0x7e".

 We should only ever be called with arch.pio.count equal to 1 */

 For size less than 4 we merge, else we zero extend */

	/*

	 * Since vcpu->arch.pio.count == 1 let emulator_pio_in perform

	 * the copy and tracing

 For size less than 4 we merge, else we zero extend */

 no guest entries from this point */

 TSC frequency always matches when on Hyper-V */

	/*

	 * We allow guests to temporarily run on slowing clocks,

	 * provided we notify them after, or to run on accelerating

	 * clocks, provided we notify them before.  Thus time never

	 * goes backwards.

	 *

	 * However, we have a problem.  We can't atomically update

	 * the frequency of a given CPU from this function; it is

	 * merely a notifier, which can be called from any CPU.

	 * Changing the TSC frequency at arbitrary points in time

	 * requires a recomputation of local variables related to

	 * the TSC for each VCPU.  We must flag these local variables

	 * to be updated and be sure the update takes place with the

	 * new frequency before any guests proceed.

	 *

	 * Unfortunately, the combination of hotplug CPU and frequency

	 * change creates an intractable locking scenario; the order

	 * of when these callouts happen is undefined with respect to

	 * CPU hotplug, and they can race with each other.  As such,

	 * merely setting per_cpu(cpu_tsc_khz) = X during a hotadd is

	 * undefined; you can actually have a CPU frequency change take

	 * place in between the computation of X and the setting of the

	 * variable.  To protect against this problem, all updates of

	 * the per_cpu tsc_khz variable are done in an interrupt

	 * protected IPI, and all callers wishing to update the value

	 * must wait for a synchronous IPI to complete (which is trivial

	 * if the caller is on the CPU already).  This establishes the

	 * necessary total order on variable updates.

	 *

	 * Note that because a guest time update may take place

	 * anytime after the setting of the VCPU's request bit, the

	 * correct TSC value must be set before the request.  However,

	 * to ensure the update actually makes it to any guest which

	 * starts running in hardware virtualization between the set

	 * and the acquisition of the spinlock, we must also ping the

	 * CPU after setting the request bit.

	 *

		/*

		 * We upscale the frequency.  Must make the guest

		 * doesn't see old kvmclock values while running with

		 * the new frequency, otherwise we risk the guest sees

		 * time go backwards.

		 *

		 * In case we update the frequency for another cpu

		 * (which might be in guest context) send an interrupt

		 * to kick the cpu out of guest context.  Next time

		 * guest context is entered kvmclock will be updated,

		 * so the guest will not see stale values.

/*

 * Indirection to move queue_work() out of the tk_core.seq write held

 * region to prevent possible deadlocks against time accessors which

 * are invoked with work related locks held.

/*

 * Notification about pvclock gtod data update.

	/*

	 * Disable master clock if host does not trust, or does not use,

	 * TSC based clocksource. Delegate queue_work() to irq_work as

	 * this is invoked with tk_core.seq write held.

	/*

	 * KVM explicitly assumes that the guest has an FPU and

	 * FXSAVE/FXRSTOR. For example, the KVM_GET_FPU explicitly casts the

	 * vCPU's FPU state as a fxregs_state struct.

	/*

	 * TODO: we might be squashing a GUESTDBG_SINGLESTEP-triggered

	 * KVM_EXIT_DEBUG here.

/*

 * kvm_pv_kick_cpu_op:  Kick a vcpu.

 *

 * @apicid - apicid of vcpu to be kicked.

 Ignore requests to yield to self */

	/*

	 * if_flag is obsolete and useless, so do not bother

	 * setting it for SEV-ES guests.  Userspace can just

	 * use kvm_run->ready_for_interrupt_injection.

	/*

	 * The call to kvm_ready_for_interrupt_injection() may end up in

	 * kvm_xen_has_interrupt() which may require the srcu lock to be

	 * held, to protect against changes in the vcpu_info address.

 try to reinject previous events if any */

	/*

	 * Do not inject an NMI or interrupt if there is a pending

	 * exception.  Exceptions and interrupts are recognized at

	 * instruction boundaries, i.e. the start of an instruction.

	 * Trap-like exceptions, e.g. #DB, have higher priority than

	 * NMIs and interrupts, i.e. traps are recognized before an

	 * NMI/interrupt that's pending on the same instruction.

	 * Fault-like exceptions, e.g. #GP and #PF, are the lowest

	 * priority, but are only generated (pended) during instruction

	 * execution, i.e. a pending fault-like exception means the

	 * fault occurred on the *previous* instruction and must be

	 * serviced prior to recognizing any new events in order to

	 * fully complete the previous instruction.

	/*

	 * Call check_nested_events() even if we reinjected a previous event

	 * in order for caller to determine if it should require immediate-exit

	 * from L2 to L1 due to pending L1 events which require exit

	 * from L2 to L1.

 try to inject new event if pending */

 Don't inject interrupts if the user asked to avoid doing so */

	/*

	 * Finally, inject interrupt events.  If an event cannot be injected

	 * due to architectural conditions (e.g. IF=0) a window-open exit

	 * will re-request KVM_REQ_EVENT.  Sometimes however an event is pending

	 * and can architecturally be injected, but we cannot do it right now:

	 * an interrupt could have arrived just now and we have to inject it

	 * as a vmexit, or there could already an event in the queue, which is

	 * indicated by can_inject.  In that case we request an immediate exit

	 * in order to make progress and get back here for another iteration.

	 * The kvm_x86_ops hooks communicate this by returning -EBUSY.

	/*

	 * x86 is limited to one NMI running, and one NMI pending after it.

	 * If an NMI is already in progress, limit further NMIs to just one.

	 * Otherwise, allow two (and we'll inject the first one immediately).

 revision id */

 revision id */

	/*

	 * Give enter_smm() a chance to make ISA-specific changes to the vCPU

	 * state (e.g. leave guest mode) after we've saved the state into the

	 * SMM state-save area.

 Undocumented: IDT limit is set to zero on entry to SMM.  */

	/*

	 * When APICv gets disabled, we may still have injected interrupts

	 * pending. At the same time, KVM_REQ_EVENT may not be set as APICv was

	 * still active when the interrupt got accepted. Make sure

	 * inject_pending_event() is called to check for that.

		/*

		 * Kick all vCPUs before setting apicv_inhibit_reasons to avoid

		 * false positives in the sanity check WARN in svm_vcpu_run().

		 * This task will wait for all vCPUs to ack the kick IRQ before

		 * updating apicv_inhibit_reasons, and all other vCPUs will

		 * block on acquiring apicv_update_lock so that vCPUs can't

		 * redo svm_vcpu_run() without seeing the new inhibit state.

		 *

		 * Note, holding apicv_update_lock and taking it in the read

		 * side (handling the request) also prevents other vCPUs from

		 * servicing the request with a stale apicv_inhibit_reasons.

	/*

	 * The physical address of apic access page is stored in the VMCS.

	 * Update it when it becomes invalid.

/*

 * Returns 1 to let vcpu_run() continue the guest execution loop without

 * exiting to the userspace.  Otherwise, the value will be returned to the

 * userspace.

 Forbid vmenter if vcpu dirty ring is soft-full */

 Flushing all ASIDs flushes the current ASID... */

 Page is swapped out. Do synthetic halt */

		/*

		 * KVM_REQ_HV_STIMER has to be processed after

		 * KVM_REQ_CLOCK_UPDATE, because Hyper-V SynIC timers

		 * depend on the guest clock being up-to-date

	/*

	 * Disable IRQs before setting IN_GUEST_MODE.  Posted interrupt

	 * IPI are then delayed after guest entry, which ensures that they

	 * result in virtual interrupt delivery.

	/*

	 * 1) We should set ->mode before checking ->requests.  Please see

	 * the comment in kvm_vcpu_exiting_guest_mode().

	 *

	 * 2) For APICv, we should set ->mode before checking PID.ON. This

	 * pairs with the memory barrier implicit in pi_test_and_set_on

	 * (see vmx_deliver_posted_interrupt).

	 *

	 * 3) This also orders the write to mode from any reads to the page

	 * tables done while the VCPU is running.  Please see the comment

	 * in kvm_flush_remote_tlbs.

	/*

	 * This handles the case where a posted interrupt was

	 * notified with kvm_vcpu_kick.

		/*

		 * Assert that vCPU vs. VM APICv state is consistent.  An APICv

		 * update must kick and wait for all vCPUs before toggling the

		 * per-VM state, and responsing vCPUs must wait for the update

		 * to complete before servicing KVM_REQ_APICV_UPDATE.

	/*

	 * Do this here before restoring debug registers on the host.  And

	 * since we do this before handling the vmexit, a DR access vmexit

	 * can (a) read the correct value of the debug registers, (b) set

	 * KVM_DEBUGREG_WONT_EXIT again.

	/*

	 * If the guest has used debug registers, at least dr7

	 * will be disabled while returning to the host.

	 * If we don't have active breakpoints in the host, we don't

	 * care about the messed up debug address registers. But if

	 * we have some of them active, restore the old state.

	/*

	 * Consume any pending interrupts, including the possible source of

	 * VM-Exit on SVM and any ticks that occur between VM-Exit and now.

	 * An instruction is required after local_irq_enable() to fully unblock

	 * interrupts on processors that implement an interrupt shadow, the

	 * stat.exits increment will do nicely.

	/*

	 * Wait until after servicing IRQs to account guest time so that any

	 * ticks that occurred while running the guest are properly accounted

	 * to the guest.  Waiting until IRQs are enabled degrades the accuracy

	 * of accounting via context tracking, but the loss of accuracy is

	 * acceptable for all known use cases.

	/*

	 * Profile KVM exit RIPs:

/*

 * Implements the following, as a state machine:

 *

 * read:

 *   for each fragment

 *     for each mmio piece in the fragment

 *       write gpa, len

 *       exit

 *       copy data

 *   execute insn

 *

 * write:

 *   for each fragment

 *     for each mmio piece in the fragment

 *       write gpa, len

 *       copy data

 *       exit

 Complete previous fragment */

 Switch to the next fragment. */

 Go forward to the next mmio piece. */

 FIXME: return into emulator if single-stepping.  */

 Swap (qemu) user FPU context for the guest FPU context. */

	/*

	 * Exclude PKRU from restore as restored separately in

	 * kvm_x86_ops.run().

 When vcpu_run ends, restore user space FPU context. */

 re-sync apic's tpr */

		/*

		 * We are here if userspace calls get_regs() in the middle of

		 * instruction emulation. Registers state needs to be copied

		 * back from emulation context to vcpu. Userspace shouldn't do

		 * that usually, but some bad designed PV devices (vmware

		 * backdoor interface) need this to work

	/*

	 * KVM_MP_STATE_INIT_RECEIVED means the processor is in

	 * INIT state; latched init should be reported using

	 * KVM_SET_VCPU_EVENTS, so reject it here.

		/*

		 * When EFER.LME and CR0.PG are set, the processor is in

		 * 64-bit mode (though maybe in a 32-bit code segment).

		 * CR4.PAE and EFER.LMA must be set.

		/*

		 * Not in 64-bit mode: EFER.LMA is clear and the code

		 * segment cannot be 64-bit.

 Older userspace won't unhalt the vcpu on reset. */

	/*

	 * Read rflags as long as potentially injected trace flags are still

	 * filtered out.

	/*

	 * Trigger an rflags update that will inject or remove the trace

	 * flags.

/*

 * Translate a guest virtual address to a guest physical address.

 poll control enabled by default */

	/*

	 * Several of the "set" flows, e.g. ->set_cr0(), read other registers

	 * to handle side effects.  RESET emulation hits those flows and relies

	 * on emulated/virtualized registers, including those that are loaded

	 * into hardware, to be zeroed at vCPU creation.  Use CRs as a sentinel

	 * to detect improper or missing initialization.

		/*

		 * To avoid have the INIT path from kvm_apic_has_events() that be

		 * called with loaded FPU and does not let userspace fix the state.

 All GPRs except RDX (handled below) are zeroed on RESET/INIT. */

	/*

	 * Fall back to KVM's default Family/Model/Stepping of 0x600 (P6/Athlon)

	 * if no CPUID match is found.  Note, it's impossible to get a match at

	 * RESET since KVM emulates RESET before exposing the vCPU to userspace,

	 * i.e. it's impossible for kvm_find_cpuid_entry() to find a valid entry

	 * on RESET.  But, go through the motions in case that's ever remedied.

	/*

	 * CR0.CD/NW are set on RESET, preserved on INIT.  Note, some versions

	 * of Intel's SDM list CD/NW as being set on INIT, but they contradict

	 * (or qualify) that with a footnote stating that CD/NW are preserved.

	/*

	 * Reset the MMU context if paging was enabled prior to INIT (which is

	 * implied if CR0.PG=1 as CR0 will be '0' prior to RESET).  Unlike the

	 * standard CR0/CR4/EFER modification paths, only CR0.PG needs to be

	 * checked because it is unconditionally cleared on INIT and all other

	 * paging related bits are ignored if paging is disabled, i.e. CR0.WP,

	 * CR4, and EFER changes are all irrelevant if CR0.PG was '0'.

	/*

	 * Intel's SDM states that all TLB entries are flushed on INIT.  AMD's

	 * APM states the TLBs are untouched by INIT, but it also states that

	 * the TLBs are flushed on "External initialization of the processor."

	 * Flush the guest TLB regardless of vendor, there is no meaningful

	 * benefit in relying on the guest to flush the TLB immediately after

	 * INIT.  A spurious TLB flush is benign and likely negligible from a

	 * performance perspective.

	/*

	 * Sometimes, even reliable TSCs go backwards.  This happens on

	 * platforms that reset TSC during suspend or hibernate actions, but

	 * maintain synchronization.  We must compensate.  Fortunately, we can

	 * detect that condition here, which happens early in CPU bringup,

	 * before any KVM threads can be running.  Unfortunately, we can't

	 * bring the TSCs fully up to date with real time, as we aren't yet far

	 * enough into CPU bringup that we know how much real time has actually

	 * elapsed; our helper function, ktime_get_boottime_ns() will be using boot

	 * variables that haven't been updated yet.

	 *

	 * So we simply find the maximum observed TSC above, then record the

	 * adjustment to TSC in each VCPU.  When the VCPU later gets loaded,

	 * the adjustment will be applied.  Note that we accumulate

	 * adjustments, in case multiple suspend cycles happen before some VCPU

	 * gets a chance to run again.  In the event that no KVM threads get a

	 * chance to run, we will miss the entire elapsed period, as we'll have

	 * reset last_host_tsc, so VCPUs will not have the TSC adjusted and may

	 * loose cycle time.  This isn't too big a deal, since the loss will be

	 * uniform across all VCPUs (not to mention the scenario is extremely

	 * unlikely). It is possible that a second hibernate recovery happens

	 * much faster than a first, causing the observed TSC here to be

	 * smaller; this would require additional padding adjustment, which is

	 * why we set last_host_tsc to the local tsc observed here.

	 *

	 * N.B. - this code below runs only on platforms with reliable TSC,

	 * as that is the only way backwards_tsc is set above.  Also note

	 * that this runs for ALL vcpus, which is not a bug; all VCPUs should

	 * have the same delta_cyc adjustment applied if backwards_tsc

	 * is detected.  Note further, this adjustment is only done once,

	 * as we reset last_host_tsc on all VCPUs to stop this from being

	 * called multiple times (one for each physical CPU bringup).

	 *

	 * Platforms with unreliable TSCs don't have to deal with this, they

	 * will be compensated by the logic in vcpu_load, which sets the TSC to

	 * catchup mode.  This will catchup all VCPUs to real time, but cannot

	 * guarantee that they stay in perfect synchronization.

			/*

			 * We have to disable TSC offset matching.. if you were

			 * booting a VM while issuing an S4 host suspend....

			 * you may have some problem.  Solving this issue is

			 * left as an exercise to the reader.

		/*

		 * Make sure the user can only configure tsc_khz values that

		 * fit into a signed integer.

		 * A min value is not calculated because it will always

		 * be 1 on all machines.

 Reserve bit 0 of irq_sources_bitmap for userspace irq source */

 Reserve bit 1 of irq_sources_bitmap for irqfd-resampler */

	/*

	 * Unpin any mmu pages first.

/**

 * __x86_set_memory_region: Setup KVM internal memory slot

 *

 * @kvm: the kvm pointer to the VM.

 * @id: the slot ID to setup.

 * @gpa: the GPA to install the slot (unused when @size == 0).

 * @size: the size of the slot. Set to zero to uninstall a slot.

 *

 * This function helps to setup a KVM internal memory slot.  Specify

 * @size > 0 to install a new slot, while @size == 0 to uninstall a

 * slot.  The return code can be one of the following:

 *

 *   HVA:           on success (uninstall will return a bogus HVA)

 *   -errno:        on error

 *

 * The caller should always use IS_ERR() to check the return value

 * before use.  Note, the KVM internal memory slots are guaranteed to

 * remain valid and unchanged until the VM is destroyed, i.e., the

 * GPA->HVA translation will not change.  However, the HVA is a user

 * address, i.e. its accessibility is not guaranteed, and must be

 * accessed via __copy_{to,from}_user().

 Called with kvm->slots_lock held.  */

		/*

		 * MAP_SHARED to prevent internal slot pages from being moved

		 * by fork()/COW.

		/*

		 * Free memory regions allocated on behalf of userspace,

		 * unless the the memory map has changed due to process exit

		 * or fd copying.

	/*

	 * Clear out the previous array pointers for the KVM_MR_MOVE case.  The

	 * old arrays will be freed by __kvm_set_memory_region() if installing

	 * the new memslot is successful.

		/*

		 * If the gfn and userspace address are not aligned wrt each

		 * other, disable large page support for this slot.

	/*

	 * memslots->generation has been incremented.

	 * mmio generation may have reached its maximum value.

 Force re-initialization of steal_time cache */

	/*

	 * Update CPU dirty logging if dirty logging is being toggled.  This

	 * applies to all operations.

	/*

	 * Nothing more to do for RO slots (which can't be dirtied and can't be

	 * made writable) or CREATE/MOVE/DELETE of a slot.

	 *

	 * For a memslot with dirty logging disabled:

	 * CREATE:      No dirty mappings will already exist.

	 * MOVE/DELETE: The old mappings will already have been cleaned up by

	 *		kvm_arch_flush_shadow_memslot()

	 *

	 * For a memslot with dirty logging enabled:

	 * CREATE:      No shadow pages exist, thus nothing to write-protect

	 *		and no dirty bits to clear.

	 * MOVE/DELETE: The old mappings will already have been cleaned up by

	 *		kvm_arch_flush_shadow_memslot().

	/*

	 * READONLY and non-flags changes were filtered out above, and the only

	 * other flag is LOG_DIRTY_PAGES, i.e. something is wrong if dirty

	 * logging isn't being toggled on or off.

		/*

		 * Dirty logging tracks sptes in 4k granularity, meaning that

		 * large sptes have to be split.  If live migration succeeds,

		 * the guest in the source machine will be destroyed and large

		 * sptes will be created in the destination.  However, if the

		 * guest continues to run in the source machine (for example if

		 * live migration fails), small sptes will remain around and

		 * cause bad performance.

		 *

		 * Scan sptes if dirty logging has been stopped, dropping those

		 * which can be collapsed into a single large-page spte.  Later

		 * page faults will create the large-page sptes.

		/*

		 * Initially-all-set does not require write protecting any page,

		 * because they're all assumed to be dirty.

 Free the arrays associated with the old memslot. */

 Can't read the RIP when guest state is protected, just return 0 */

			/*

			 * k lies cyclically in ]i,j]

			 * |    i.k.j |

			 * |....j i.k.| or  |.k..j i...|

	/*

	 * If interrupts are off we cannot even use an artificial

	 * halt state.

		/*

		 * It is not possible to deliver a paravirtualized asynchronous

		 * page fault, but putting the guest in an artificial halt state

		 * can be beneficial nevertheless: if an interrupt arrives, we

		 * can deliver it timely and perhaps the guest will schedule

		 * another process.  When the instruction that triggered a page

		 * fault is retried, hopefully the page will be ready in the host.

 broadcast wakeup */

	/*

	 * When producer of consumer is unregistered, we change back to

	 * remapped mode, so we can re-use the current implementation

	 * when the irq is masked/disabled or the consumer side (KVM

	 * int this case doesn't want to receive the interrupts.

	/*

	 * test that setting IA32_SPEC_CTRL to given value

	 * is allowed by the host processor

		/*

		 * If vcpu->arch.walk_mmu->gva_to_gpa succeeded, the page

		 * tables probably do not match the TLB.  Just proceed

		 * with the error code that the processor gave.

/*

 * Handles kvm_read/write_guest_virt*() result and either injects #PF or returns

 * KVM_EXIT_INTERNAL_ERROR for cases not currently handled by KVM. Return value

 * indicates whether exit to userspace is needed.

	/*

	 * In case kvm_read/write_guest_virt*() failed with X86EMUL_IO_NEEDED

	 * while handling a VMX instruction KVM could've handled the request

	 * correctly by exiting to userspace and performing I/O but there

	 * doesn't seem to be a real use-case behind such requests, just return

	 * KVM_EXIT_INTERNAL_ERROR for now.

		/*

		 * Currently, KVM doesn't mark global entries in the shadow

		 * page tables, so a non-global flush just degenerates to a

		 * global flush. If needed, we could optimize this later by

		 * keeping track of global entries in shadow page tables.

 Complete previous fragment */

 Switch to the next fragment. */

 Go forward to the next mmio piece. */

 VMG change, at this point, we're always done

 RIP has already been advanced

 More MMIO is needed

TODO: Check if need to increment number of frags */

TODO: Check if need to increment number of frags */

 memcpy done already by emulator_pio_out.  */

 Emulation done by the kernel.  */

 Emulation done by the kernel.  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * KVM Microsoft Hyper-V emulation

 *

 * derived from arch/x86/kvm/x86.c

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright (C) 2008 Qumranet, Inc.

 * Copyright IBM Corporation, 2008

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 * Copyright (C) 2015 Andrey Smetanin <asmetanin@virtuozzo.com>

 *

 * Authors:

 *   Avi Kivity   <avi@qumranet.com>

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Amit Shah    <amit.shah@qumranet.com>

 *   Ben-Ami Yassour <benami@il.ibm.com>

 *   Andrey Smetanin <asmetanin@virtuozzo.com>

 "Hv#1" signature */

	/*

	 * Valid vectors are 16-255, however, nested Hyper-V attempts to write

	 * default '0x10000' value on boot and this should not #GP. We need to

	 * allow zero-initing the register from host as well.

	/*

	 * Guest may configure multiple SINTs to use the same vector, so

	 * we maintain a bitmap of vectors handled by synic, and a

	 * bitmap of vectors with auto-eoi behavior.  The bitmaps are

	 * updated here, and atomically queried on fast paths.

 Load SynIC vectors into EOI exit bitmap */

 Try to deliver pending Hyper-V SynIC timers messages */

	/*

	 * Fall back to get_kvmclock_ns() when TSC page hasn't been set up,

	 * is broken, disabled or being updated.

/*

 * stimer_start() assumptions:

 * a) stimer->count is not equal to 0

 * b) stimer->config has HV_STIMER_ENABLE flag

		/*

		 * Expire timer according to Hypervisor Top-Level Functional

		 * specification v4(15.3.1):

		 * "If a one shot is enabled and the specified count is in

		 * the past, it will expire immediately."

	/*

	 * Strictly following the spec-mandated ordering would assume setting

	 * .msg_pending before checking .message_type.  However, this function

	 * is only called in vcpu context so the entire update is atomic from

	 * guest POV and thus the exact order here doesn't matter.

	/*

	 * To avoid piling up periodic ticks, don't retry message

	 * delivery for them (within "lazy" lost ticks policy).

/*

 * The kvmclock and Hyper-V TSC page use similar formulas, and converting

 * between them is possible:

 *

 * kvmclock formula:

 *    nsec = (ticks - tsc_timestamp) * tsc_to_system_mul * 2^(tsc_shift-32)

 *           + system_time

 *

 * Hyper-V formula:

 *    nsec/100 = ticks * scale / 2^64 + offset

 *

 * When tsc_timestamp = system_time = 0, offset is zero in the Hyper-V formula.

 * By dividing the kvmclock formula by 100 and equating what's left we get:

 *    ticks * scale / 2^64 = ticks * tsc_to_system_mul * 2^(tsc_shift-32) / 100

 *            scale / 2^64 =         tsc_to_system_mul * 2^(tsc_shift-32) / 100

 *            scale        =         tsc_to_system_mul * 2^(32+tsc_shift) / 100

 *

 * Now expand the kvmclock formula and divide by 100:

 *    nsec = ticks * tsc_to_system_mul * 2^(tsc_shift-32)

 *           - tsc_timestamp * tsc_to_system_mul * 2^(tsc_shift-32)

 *           + system_time

 *    nsec/100 = ticks * tsc_to_system_mul * 2^(tsc_shift-32) / 100

 *               - tsc_timestamp * tsc_to_system_mul * 2^(tsc_shift-32) / 100

 *               + system_time / 100

 *

 * Replace tsc_to_system_mul * 2^(tsc_shift-32) / 100 by scale / 2^64:

 *    nsec/100 = ticks * scale / 2^64

 *               - tsc_timestamp * scale / 2^64

 *               + system_time / 100

 *

 * Equate with the Hyper-V formula so that ticks * scale / 2^64 cancels out:

 *    offset = system_time / 100 - tsc_timestamp * scale / 2^64

 *

 * These two equivalencies are implemented in this function.

	/*

	 * check if scale would overflow, if so we use the time ref counter

	 *    tsc_to_system_mul * 2^(tsc_shift+32) / 100 >= 2^64

	 *    tsc_to_system_mul / 100 >= 2^(32-tsc_shift)

	 *    tsc_to_system_mul >= 100 * 2^(32-tsc_shift)

	/*

	 * Otherwise compute the scale and offset according to the formulas

	 * derived above.

/*

 * Don't touch TSC page values if the guest has opted for TSC emulation after

 * migration. KVM doesn't fully support reenlightenment notifications and TSC

 * access emulation and Hyper-V is known to expect the values in TSC page to

 * stay constant before TSC access emulation is disabled from guest side

 * (HV_X64_MSR_TSC_EMULATION_STATUS). KVM userspace is expected to preserve TSC

 * frequency and guest visible TSC value across migration (and prevent it when

 * TSC scaling is unsupported).

	/*

	 * Because the TSC parameters only vary when there is a

	 * change in the master clock, do not bother with caching.

	/*

	 * While we're computing and writing the parameters, force the

	 * guest to use the time reference count MSR.

 Ensure sequence is zero before writing the rest of the struct.  */

	/*

	 * Now switch to the TSC page mechanism by writing the sequence.

 Write the struct entirely before the non-zero sequence.  */

 Preserve HV_TSC_PAGE_GUEST_CHANGED/HV_TSC_PAGE_HOST_CHANGED states */

	/*

	 * Take the srcu lock as memslots will be accessed to check the gfn

	 * cache generation against the memslots generation.

 setting guest os id to zero disables hypercall page */

 if guest os id is not set hypercall should remain disabled */

		/*

		 * If Xen and Hyper-V hypercalls are both enabled, disambiguate

		 * the same way Xen itself does, by setting the bit 31 of EAX

		 * which is RsvdZ in the 32-bit Hyper-V hypercall ABI and just

		 * going to be clobbered on 64-bit.

 orl $0x80000000, %eax */

 vmcall/vmmcall */

 ret */

 Send notification about crash to user space */

 read-only, but still ignore it if host-initiated */

 Calculate cpu time spent by current task in 100ns units */

		/*

		 * The VP index is initialized to vcpu_index by

		 * kvm_hv_vcpu_postcreate so they initially match.  Now the

		 * VP index is changing, adjust num_mismatched_vp_indexes if

		 * it now matches or no longer matches vcpu_idx.

		/*

		 * Clear apic_assist portion of struct hv_vp_assist_page

		 * only, there can be valuable data in the rest which needs

		 * to be preserved e.g. on migration.

 read-only, but still ignore it if host-initiated */

 for all vcpus vp_index == vcpu_idx */

		/*

		 * Work around possible WS2012 bug: it sends hypercalls

		 * with processor_mask = 0x0 and HV_FLUSH_ALL_PROCESSORS clear,

		 * while also expecting us to flush something and crashing if

		 * we don't. Let's treat processor_mask == 0 same as

		 * HV_FLUSH_ALL_PROCESSORS.

	/*

	 * vcpu->arch.cr3 may not be up-to-date for running vCPUs so we can't

	 * analyze it here, flush TLB regardless of the specified address space.

 We always do full TLB flush, set 'Reps completed' = 'Rep Count' */

 We fail only when APIC is disabled */

 'reserved' part of hv_send_ipi should be 0 */

	/*

	 * Per spec, bits 32-47 contain the extra "flag number".  However, we

	 * have no use for it, and in all known usecases it is zero, so just

	 * report lookup failure if it isn't.

 remaining bits are reserved-zero */

 the eventfd is protected by vcpu->kvm->srcu, but conn_to_evt isn't */

		/*

		 * Return 'true' when SynDBG is disabled so the resulting code

		 * will be HV_STATUS_INVALID_HYPERCALL_CODE.

	/*

	 * hypercall generates UD from non zero cpl and real mode

	 * per HYPER-V spec

 maybe userspace knows this conn_id */

 don't bother userspace if it has no way to handle it */

 Skip NESTED_FEATURES if eVMCS is not supported */

			/*

			 * We implement some Hyper-V 2016 functions so let's use

			 * this version.

			/*

			 * Direct Synthetic timers only make sense with in-kernel

			 * LAPIC

			/*

			 * Default number of spinlock retry attempts, matches

			 * HyperV 2016.

 Maximum number of virtual processors */

			/*

			 * Maximum number of logical processors, matches

			 * HyperV 2016.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine -- Performance Monitoring Unit support

 *

 * Copyright 2015 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Avi Kivity   <avi@redhat.com>

 *   Gleb Natapov <gleb@redhat.com>

 *   Wei Huang    <wei@redhat.com>

 This is enough to filter the vast majority of currently defined events. */

/* NOTE:

 * - Each perf counter is defined as "struct kvm_pmc";

 * - There are two types of perf counters: general purpose (gp) and fixed.

 *   gp counters are stored in gp_counters[] and fixed counters are stored

 *   in fixed_counters[] respectively. Both of them are part of "struct

 *   kvm_pmu";

 * - pmu.c understands the difference between gp counters and fixed counters.

 *   However AMD doesn't support fixed-counters;

 * - There are three types of index to access perf counters (PMC):

 *     1. MSR (named msr): For example Intel has MSR_IA32_PERFCTRn and AMD

 *        has MSR_K7_PERFCTRn.

 *     2. MSR Index (named idx): This normally is used by RDPMC instruction.

 *        For instance AMD RDPMC instruction uses 0000_0003h in ECX to access

 *        C001_0007h (MSR_K7_PERCTR3). Intel has a similar mechanism, except

 *        that it also supports fixed counters. idx can be used to as index to

 *        gp and fixed counters.

 *     3. Global PMC Index (named pmc): pmc is an index specific to PMU

 *        code. Each pmc, stored in kvm_pmc.idx field, is unique across

 *        all perf counters (both gp and fixed). The mapping relationship

 *        between pmc and perf counters is as the following:

 *        * Intel: [0 .. INTEL_PMC_MAX_GENERIC-1] <=> gp counters

 *                 [INTEL_PMC_IDX_FIXED .. INTEL_PMC_IDX_FIXED + 2] <=> fixed

 *        * AMD:   [0 .. AMD64_NUM_COUNTERS-1] <=> gp counters

		/*

		 * Inject PMI. If vcpu was in a guest mode during NMI PMI

		 * can be ejected on a guest mode re-entry. Otherwise we can't

		 * be sure that vcpu wasn't executing hlt instruction at the

		 * time of vmexit and is not going to re-enter guest mode until

		 * woken up. So we should wake it, but this is impossible from

		 * NMI context. Do it from irq work instead.

		/*

		 * HSW_IN_TX_CHECKPOINTED is not supported with nonzero

		 * period. Just clear the sample period so at least

		 * allocating the counter doesn't fail.

 update counter, reset event value to avoid redundant accumulation */

 recalibrate sample period and check if it's accepted by perf core */

 reuse perf_event to serve as pmc_reprogram_counter() does*/

 exclude user */

 exclude kernel */

	/*

	 * Unused perf_events are only released if the corresponding MSRs

	 * weren't accessed during the last vCPU time slice. kvm_arch_sched_in

	 * triggers KVM_REQ_PMU if cleanup is needed.

 check if idx is a valid index to access PMU */

/* refresh PMU settings. This function generally is called when underlying

 * settings are changed (such as changes of PMU CPUID by guest VMs), which

 * should rarely happen.

 Release perf_events for vPMCs that have been unused for a full time slice.  */

 Ensure nevents can't be changed between the user copies. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * Copyright 2016 Red Hat, Inc. and/or its affiliates.

/*

 * This covers statistics <1024 (11=log(1024)+1), which should be enough to

 * cover RMAP_RECYCLE_THRESHOLD.

 Still small enough to be on the stack */

 index=0 counts no rmap; index=1 counts 1 rmap */

/*

 *  Copyright (C) 2001  MandrakeSoft S.A.

 *  Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 *    MandrakeSoft S.A.

 *    43, rue d'Aboukir

 *    75002 Paris - France

 *    http://www.linux-mandrake.com/

 *    http://www.mandrakesoft.com/

 *

 *  This library is free software; you can redistribute it and/or

 *  modify it under the terms of the GNU Lesser General Public

 *  License as published by the Free Software Foundation; either

 *  version 2 of the License, or (at your option) any later version.

 *

 *  This library is distributed in the hope that it will be useful,

 *  but WITHOUT ANY WARRANTY; without even the implied warranty of

 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU

 *  Lesser General Public License for more details.

 *

 *  You should have received a copy of the GNU Lesser General Public

 *  License along with this library; if not, write to the Free Software

 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA

 *

 *  Yunhong Jiang <yunhong.jiang@intel.com>

 *  Yaozu (Eddie) Dong <eddie.dong@intel.com>

 *  Based on Xen 3.1 code.

 RTC special handling */

 coalesced */

		/*

		 * If no longer has pending EOI in LAPICs, update

		 * EOI for this vector.

	/*

	 * AMD SVM AVIC accelerate EOI write iff the interrupt is edge

	 * triggered, in which case the in-kernel IOAPIC will not be able

	 * to receive the EOI.  In this case, we do a lazy update of the

	 * pending EOI when trying to set IOAPIC irq.

	/*

	 * Return 0 for coalesced interrupts; for edge-triggered interrupts,

	 * this only happens if a previous edge has not been delivered due

	 * to masking.  For level interrupts, the remote_irr field tells

	 * us if the interrupt is waiting for an EOI.

	 *

	 * RTC is special: it is edge-triggered, but userspace likes to know

	 * if it has been already ack-ed via EOI because coalesced RTC

	 * interrupts lead to time drift in Windows guests.  So we track

	 * EOI manually for the RTC interrupt.

 Make sure we see any missing RTC EOI */

 Writes are ignored. */

 Preserve read-only fields */

		/*

		 * Some OSes (Linux, Xen) assume that Remote IRR bit will

		 * be cleared by IOAPIC hardware when the entry is configured

		 * as edge-triggered. This behavior is used to simulate an

		 * explicit EOI on IOAPICs that don't have the EOI register.

				/*

				 * Update vcpu_bitmap with vcpus specified in

				 * the previous request as well. This is done to

				 * keep ioapic_handled_vectors synchronized.

		/*

		 * pending_eoi cannot ever become negative (see

		 * rtc_status_pending_eoi_check_valid) and the caller

		 * ensures that it is only called if it is >= zero, namely

		 * if rtc_irq_check_coalesced returns false).

	/*

	 * We are dropping lock while calling ack notifiers because ack

	 * notifier callbacks for assigned devices call into IOAPIC

	 * recursively. Since remote_irr is cleared only after call

	 * to notifiers if the same vector will be delivered while lock

	 * is dropped it will be put into irr and will be delivered

	 * after ack notifier returns.

			/*

			 * Real hardware does not deliver the interrupt

			 * immediately during eoi broadcast, and this

			 * lets a buggy guest make slow progress

			 * even if it does not correctly handle a

			 * level-triggered interrupt.  Emulate this

			 * behavior if we detect an interrupt storm.

 check alignment */

 check alignment */

 8-bit register */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Local APIC virtualization

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright (C) 2007 Novell

 * Copyright (C) 2007 Intel

 * Copyright 2009 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Dor Laor <dor.laor@qumranet.com>

 *   Gregory Haskins <ghaskins@novell.com>

 *   Yaozu (Eddie) Dong <eddie.dong@intel.com>

 *

 * Based on Xen 3.1 code, Copyright (c) 2004, Intel Corporation.

 14 is the version for Xeon and Pentium 8.4.8*/

 followed define is not in apicdef.h */

 clock cycles */

 clock cycles */

 step-by-step approximation to mitigate fluctuation */

 Not optimized. */

/*

 * CLEAN -> DIRTY and UPDATE_IN_PROGRESS -> DIRTY changes happen without a lock.

 *

 * DIRTY -> UPDATE_IN_PROGRESS and UPDATE_IN_PROGRESS -> CLEAN happen with

 * apic_map_lock_held.

 enough space for any xAPIC ID */

 Read kvm->arch.apic_map_dirty before kvm->arch.apic_map.  */

	/*

	 * Read kvm->arch.apic_map_dirty before kvm->arch.apic_map

	 * (if clean) or the APIC registers (if dirty).

 Someone else has updated the map. */

 Hotplug hack: see kvm_apic_match_physical_addr(), ... */

		/*

		 * ... xAPIC ID of VCPUs with APIC ID > 0xff will wrap-around,

		 * prevent them from masking VCPUs with APIC ID <= 0xff.

	/*

	 * Write kvm->arch.apic_map before clearing apic->apic_map_dirty.

	 * If another update has come in, leave it DIRTY.

 Check if there are APF page ready requests pending */

	/*

	 * KVM emulates 82093AA datasheet (with in-kernel IOAPIC implementation)

	 * which doesn't have EOI register; Some buggy OSes (e.g. Windows with

	 * Hyper-V role) disable EOI broadcast in lapic not checking for IOAPIC

	 * version first and level-triggered interrupts never get EOIed in

	 * IOAPIC.

 part LVTT mask, timer mode mask added at runtime */

 LVTTHMR */

 LVTPC */

 LVT0-1 */

 LVTERR */

	/*

	 * Note that irr_pending is just a hint. It will be always

	 * true with virtual interrupt delivery enabled.

 need to update RVI */

	/*

	 * With APIC virtualization enabled, all caching is disabled

	 * because the processor can modify ISR under the hood.  Instead

	 * just set SVI.

		/*

		 * ISR (in service register) bit is set when injecting an interrupt.

		 * The highest vector is injected. Thus the latest bit set matches

		 * the highest bit in ISR.

	/*

	 * Note that isr_count is always 1, and highest_isr_cache

	 * is always -1, with APIC virtualization enabled.

	/*

	 * We do get here for APIC virtualization enabled if the guest

	 * uses the Hyper-V APIC enlightenment.  In this case we may need

	 * to trigger a new interrupt delivery by writing the SVI field;

	 * on the other hand isr_count and highest_isr_cache are unused

	 * and must be left alone.

	/* This may race with setting of irr in __apic_accept_irq() and

	 * value returned may be wrong, but kvm_vcpu_kick() in __apic_accept_irq

	 * will cause vmexit immediately and the value will be recalculated

	 * on the next vmentry.

	/*

	 * Hotplug hack: Make LAPIC in xAPIC mode also accept interrupts as if

	 * it were in x2APIC mode.  Hotplugged VCPUs start in xAPIC mode and

	 * this allows unique addressing of VCPUs with APIC ID over 0xff.

	 * The 0xff condition is needed because writeable xAPIC ID.

/* The KVM local APIC implementation has two quirks:

 *

 *  - Real hardware delivers interrupts destined to x2APIC ID > 0xff to LAPICs

 *    in xAPIC mode if the "destination & 0xff" matches its xAPIC ID.

 *    KVM doesn't do that aliasing.

 *

 *  - in-kernel IOAPIC messages have to be delivered directly to

 *    x2APIC, because the kernel does not support interrupt remapping.

 *    In order to support broadcast without interrupt remapping, x2APIC

 *    rewrites the destination of non-IPI messages from APIC_BROADCAST

 *    to X2APIC_BROADCAST.

 *

 * The broadcast quirk can be disabled with KVM_CAP_X2APIC_API.  This is

 * important when userspace wants to use x2APIC-format MSIs, because

 * APIC_BROADCAST (0xff) is a legal route for "cluster 0, CPUs 0-7".

/* Return true if the interrupt can be handled by using *bitmap as index mask

 * for valid destinations in *dst array.

 * Return false if kvm_apic_map_get_dest_lapic did nothing useful.

 * Note: we may have zero kvm_lapic destinations when we return true, which

 * means that the interrupt should be dropped.  In this case, *bitmap would be

 * zero and *dst undefined.

/*

 * This routine tries to handle interrupts in posted mode, here is how

 * it deals with different cases:

 * - For single-destination interrupts, handle it in posted mode

 * - Else if vector hashing is enabled and it is a lowest-priority

 *   interrupt, handle it in posted mode and use the following mechanism

 *   to find the destination vCPU.

 *	1. For lowest-priority interrupts, store all the possible

 *	   destination vCPUs in an array.

 *	2. Use "guest vector % max number of destination vCPUs" to find

 *	   the right destination vCPU in the array for the lowest-priority

 *	   interrupt.

 * - Otherwise, use remapped mode to inject the interrupt.

/*

 * Add a pending IRQ into lapic.

 * Return 1 if successfully added and 0 if discarded.

 FIXME add logic for vcpu on reset */

 assumes that there are only KVM_APIC_INIT/SIPI */

 make sure sipi_vector is visible for the receiver */

		/*

		 * Should only be called by kvm_apic_local_deliver() with LVT0,

		 * before NMI watchdog was enabled. Already handled by

		 * kvm_apic_accept_pic_intr().

/*

 * This routine identifies the destination vcpus mask meant to receive the

 * IOAPIC interrupts. It either uses kvm_apic_map_get_dest_lapic() to find

 * out the destination vcpus array and set the bitmap or it traverses to

 * each available vcpu to identify the same.

 Eoi the ioapic only if the ioapic doesn't own the vector. */

 Request a KVM exit to inform the userspace IOAPIC. */

	/*

	 * Not every write EOI will has corresponding ISR,

	 * one example is when Kernel check timer on setup_IO_APIC

/*

 * this interface assumes a trap-like exit, which has already finished

 * desired side effect including vISR and vPPR update.

 if initial count is 0, current count should also be 0 */

 Timer CCR */

 this bitmask has a bit cleared for each reserved register */

 ARBPRI is not valid on x2APIC */

	/*

	 * Do not allow the guest to program periodic timers with small

	 * interval, since the hrtimers are not throttled by the host

	 * scheduler.

/*

 * On APICv, this test will cause a busy wait

 * during a higher-priority task.

	/*

	 * If the guest TSC is running at a different ratio than the host, then

	 * convert the delay to nanoseconds to achieve an accurate delay.  Note

	 * that __delay() uses delay_tsc whenever the hardware has TSC, thus

	 * always for VMX enabled hardware.

 Do not adjust for tiny fluctuations or large random spikes. */

 too early */

 too late */

		/*

		 * If the timer fired early, reread the TSC to account for the

		 * overhead of the above adjustment to avoid waiting longer

		 * than is necessary.

		/*

		 * Ensure the guest's timer has truly expired before posting an

		 * interrupt.  Open code the relevant checks to avoid querying

		 * lapic_timer_int_injected(), which will be false since the

		 * interrupt isn't yet injected.  Waiting until after injecting

		 * is not an option since that won't help a posted interrupt.

	/*

	 * Synchronize both deadlines to the same time source or

	 * differences in the periods (caused by differences in the

	 * underlying clocks or numerical approximation errors) will

	 * cause the two to drift apart over time as the errors

	 * accumulate.

	/*

	 * To simplify handling the periodic timer, leave the hv timer running

	 * even if the deadline timer has expired, i.e. rely on the resulting

	 * VM-Exit to recompute the periodic timer's target expiration.

		/*

		 * Cancel the hv timer if the sw timer fired while the hv timer

		 * was being programmed, or if the hv timer itself expired.

 If the preempt notifier has already run, it also called apic_timer_expired */

 Possibly the TSC deadline timer is not enabled yet */

 Local APIC ID */

 No delay here, so we always clear the pending bit */

 TODO: Check vector */

	/*

	 * APIC register must be aligned on 128-bits boundary.

	 * 32/64/128 bits registers must be accessed thru 32 bits.

	 * Refer SDM 8.4.1

 emulate APIC access in a trap manner */

 hw has done the conditional check and inst decode */

 TODO: optimize to just emulate side effect w/o one more write */

/*

 *----------------------------------------------------------------------

 * LAPIC interface

 *----------------------------------------------------------------------

 update jump label if enable bit changes */

 Check if there are APF page ready requests pending */

 irr_pending is always true when apicv is activated. */

 Stop the timer in case it's a reset to an active apic */

 The xAPIC ID is set at RESET even if the APIC was already enabled. */

/*

 *----------------------------------------------------------------------

 * timer interface

 *----------------------------------------------------------------------

	/*

	 * Stuff the APIC ENABLE bit in lieu of temporarily incrementing

	 * apic_hw_disabled; the full RESET value is set by kvm_lapic_reset().

 sw disabled at reset */

	/*

	 * We get here even with APIC virtualization enabled, if doing

	 * nested virtualization and L1 runs with the "acknowledge interrupt

	 * on exit" mode.  Then we cannot inject the interrupt via RVI,

	 * because the process would deliver it through the IDT.

		/*

		 * For auto-EOI interrupts, there might be another pending

		 * interrupt above PPR, so check whether to raise another

		 * KVM_REQ_EVENT.

		/*

		 * For normal interrupts, PPR has been raised and there cannot

		 * be a higher-priority pending interrupt---except if there was

		 * a concurrent interrupt injection, but that would have

		 * triggered KVM_REQ_EVENT already.

 In x2APIC mode, the LDR is fixed and based on the id */

	/*

	 * Get calculated timer current count for remaining timer period (if

	 * any) and store it in the returned register set.

 set SPIV separately to get count of SW disabled APICs right */

/*

 * apic_sync_pv_eoi_from_guest - called on vmexit or cancel interrupt

 *

 * Detect whether guest triggered PV EOI since the

 * last entry. If yes, set EOI on guests's behalf.

 * Clear PV EOI in guest memory in any case.

	/*

	 * PV EOI state is derived from KVM_APIC_PV_EOI_PENDING in host

	 * and KVM_PV_EOI_ENABLED in guest memory as follows:

	 *

	 * KVM_APIC_PV_EOI_PENDING is unset:

	 * 	-> host disabled PV EOI.

	 * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is set:

	 * 	-> host enabled PV EOI, guest did not execute EOI yet.

	 * KVM_APIC_PV_EOI_PENDING is set, KVM_PV_EOI_ENABLED is unset:

	 * 	-> host enabled PV EOI, guest executed EOI.

	/*

	 * Clear pending bit in any case: it will be set again on vmentry.

	 * While this might not be ideal from performance point of view,

	 * this makes sure pv eoi is only enabled when we know it's safe.

/*

 * apic_sync_pv_eoi_to_guest - called before vmentry

 *

 * Detect whether it's safe to enable PV EOI and

 * if yes do so.

 IRR set or many bits in ISR: could be nested. */

 Cache not set: could be safe but we don't bother. */

 Need EOI to update ioapic. */

		/*

		 * PV EOI was disabled by apic_sync_pv_eoi_from_guest

		 * so we need not do anything here.

 if this is ICR write vector before command */

 if this is ICR write vector before command */

	/*

	 * Read pending events before calling the check_events

	 * callback.

		/*

		 * If an event has happened and caused a vmexit,

		 * we know INITs are latched and therefore

		 * we will not incorrectly deliver an APIC

		 * event instead of a vmexit.

	/*

	 * INITs are latched while CPU is in specific states

	 * (SMM, VMX root mode, SVM with GIF=0).

	 * Because a CPU cannot be in these states immediately

	 * after it has processed an INIT signal (and thus in

	 * KVM_MP_STATE_INIT_RECEIVED state), just eat SIPIs

	 * and leave the INIT pending.

 evaluate pending_events before reading the vector */

 SPDX-License-Identifier: GPL-2.0-only

/******************************************************************************

 * emulate.c

 *

 * Generic x86 (32-bit and 64-bit) instruction decoder and emulator.

 *

 * Copyright (c) 2005 Keir Fraser

 *

 * Linux coding style, mod r/m decoder, segment base fixes, real-mode

 * privileged instructions:

 *

 * Copyright (C) 2006 Qumranet

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 *   Avi Kivity <avi@qumranet.com>

 *   Yaniv Kamay <yaniv@qumranet.com>

 *

 * From: xen-unstable 10676:af9809f51f81a3c43f276f00c81a52ef558afda4

/*

 * Operand types

 No generic decode */

 Register */

 Memory */

 Accumulator: AL/AX/EAX/RAX */

 ES:DI/EDI/RDI */

 Memory, 64-bit */

 Zero-extended 8-bit immediate */

 DX register */

 CL register (for shifts) */

 8-bit sign extended immediate */

 Implied 1 */

 Sign extended up to 32-bit immediate */

 Memory operand (16-bit). */

 Memory operand (32-bit). */

 Immediate operand, zero extended */

 SI/ESI/RSI */

 Immediate far address */

 Far address in memory */

 Immediate operand, 16 bits, zero extended */

 ES */

 CS */

 SS */

 DS */

 FS */

 GS */

 8-bit zero extended memory operand */

 Sign extended 16/32/64-bit immediate */

 memory at BX/EBX/RBX + zero-extended AL */

 Low part of extended acc (AX/AX/EAX/RAX) */

 High part of extended acc (-/DX/EDX/RDX) */

 Width of operand field */

/*

 * Opcode effective-address decode tables.

 * Note that we only emulate instructions that have at least one memory

 * operand (excluding implicit stack references). We assume that stack

 * references and instruction fetches will never occur in special memory

 * areas that require emulation. So, for example, 'mov <imm>,<reg>' need

 * not be handled.

 Operand sizes: 8-bit operands or specified/overridden size. */

 8-bit operands. */

 Destination operand type. */

 Source operand type. */

 Memory operand is absolute displacement */

 String instruction (rep capable) */

 Stack instruction (push/pop) */

 Opcode uses one of the group mechanisms */

 Bits 3:5 of modrm byte extend opcode */

 Alternate decoding of mod == 3 */

 Instruction varies with 66/f2/f3 prefix */

 Opcode extension in ModRM r/m if mod == 3 */

 Escape to coprocessor instruction */

 Alternate instruction decoding of mod == 3 */

 Different instruction for 32/64 bit */

 SSE Vector instruction */

 Generic ModRM decode. */

 Destination is only written; never read. */

 Misc flags */

 instruction generates #UD if not in prot-mode */

 Emulate if unsupported by the host */

 Don't access memory (lea/invlpg/verr etc) */

 Operand is 64b in long mode, 32b otherwise */

 No Such Instruction */

 lock prefix is allowed for the instruction */

 instruction generates #GP if current CPL != 0 */

 instruction used to write page table */

 instruction is not implemented */

 Source 2 operand type */

 MMX Vector instruction */

 Explicitly aligned (e.g. MOVDQA) */

 Explicitly unaligned (e.g. MOVDQU) */

 Advanced Vector Extensions */

 Aligned to 16 byte boundary (e.g. FXSAVE) */

 Use opcode::u.fastop */

 No writeback */

 Write back src operand */

 Mod field is ignored */

 Has valid intercept field */

 Has valid check_perm field */

 #UD instead of #GP on CPL > 0 */

 Near branches */

 No 16 bit operand */

 SP is incremented before ModRM calc */

 Instruction has two memory operand */

/*

 * These EFLAGS bits are restored from saved value during emulation, and

 * any changes are written back to the saved value after emulation.

/*

 * fastop functions have a special calling convention:

 *

 * dst:    rax        (in/out)

 * src:    rdx        (in/out)

 * src2:   rcx        (in)

 * flags:  rflags     (in/out)

 * ex:     rsi        (in:fastop pointer, out:zero if exception)

 *

 * Moreover, they are all exactly FASTOP_SIZE bytes long, so functions for

 * different operand sizes can be reached by calculation, rather than a jump

 * table (which would be bigger than the code).

 1-operand, using src2 (for MUL/DIV r/m) */

 1-operand, using src2 (for MUL/DIV r/m), with exceptions */

 2 operand, word only */

 2 operand, src is CL */

 2 operand, src and dest are reversed */

 3-operand, word-only, src2=cl */

 Special case for SETcc - 1 instruction per cc */

/*

 * XXX: inoutclob user must know where the argument is being expanded.

 *      Relying on CONFIG_CC_HAS_ASM_GOTO would allow us to remove _fault.

 The 4-byte case *is* correct: in 64-bit mode we zero-extend. */

 64b: zero-extend */

 d=0: 0xffff; d=1: 0xffffffff */

 Access/update address held in a register, based on addressing mode. */

/*

 * x86 defines three classes of vector instructions: explicitly

 * aligned, explicitly unaligned, and the rest, which change behaviour

 * depending on whether they're AVX encoded or not.

 *

 * Also included is CMPXCHG16B which is not a vector instruction, yet it is

 * subject to the same check.  FXSAVE and FXRSTOR are checked here too as their

 * 512 bytes of data must be aligned to a 16 byte boundary.

 code segment in protected mode or read-only data segment */

 unreadable code segment */

 expand-down segment */

 temporary value */

/*

 * Prefetch the remaining bytes of the instruction without crossing page

 * boundary if they are not in fetch_cache yet.

	/*

	 * We do not know exactly how many bytes will be needed, and

	 * __linearize is expensive, so fetch as much as possible.  We

	 * just have to avoid going beyond the 15 byte limit, the end

	 * of the segment, or the end of the page.

	 *

	 * __linearize is called with size 0 so that it does not do any

	 * boundary check itself.  Instead, we use max_size to check

	 * against op_size.

	/*

	 * One instruction can only straddle two pages,

	 * and one has been loaded at the beginning of

	 * x86_decode_insn.  So, if not enough bytes

	 * still, we must have hit the 15-byte boundary.

 Fetch next part of the instruction being emulated. */

/*

 * Given the 'reg' portion of a ModRM byte, and a register block, return a

 * pointer into the block that addresses the relevant register.

 * @highbyte_regs specifies whether to decode AH,CH,DH,BH.

 If src is zero, do not writeback, but update flags */

 If src is zero, do not writeback, but update flags */

 REX.R */

 REX.X */

 REX.B */

 16-bit ModR/M decode. */

 32/64-bit ModR/M decode. */

 Increment ESP on POP [ESP] */

 only subword offset */

 refill pio read ahead */

 what if limit > 65535? */

 allowed just for 8 bytes segments */

 allowed just for 8 bytes segments */

 0000-0003 are null */

		/* set real mode segment descriptor (keep limit etc. for

 VM86 needs a clean new segment descriptor */

 TR should be in GDT only */

 NULL selector is not valid for TR, CS and (except for long mode) SS */

			/*

			 * ctxt->ops->set_segment expects the CPL to be in

			 * SS.DPL, so fake an expand-up 32-bit data segment.

 Skip all following checks */

 can't load system descriptor into segment selector */

		/*

		 * segment is not a writable data segment or segment

		 * selector's RPL != CPL or segment selector's RPL != CPL

 conforming */

 nonconforming */

 in long-mode d/b must be clear if l is set */

 CS(RPL) <- CPL */

 busy */

  DS, ES, FS, or GS */

		/*

		 * segment is not a data or readable code segment or

		 * ((segment is a data or nonconforming code segment)

		 * and (both RPL and CPL > DPL))

 mark segment as accessed */

	/*

	 * None of MOV, POP and LSS can load a NULL selector in CPL=3, but

	 * they can load it at CPL<3 (Intel's manual says only LSS can,

	 * but it's wrong).

	 *

	 * However, the Intel manual says that putting IST=1/DPL=3 in

	 * an interrupt gate will result in SS=3 (the AMD manual instead

	 * says it doesn't), so allow SS=3 in __load_segment_descriptor

	 * and only forbid it here.

 no writeback */

 Disable writeback. */

 real mode */

 TODO: Add limit checks */

 Protected mode interrupts unimplemented yet */

 TODO: Add stack limit check */

 Clear reserved zeros */

 iret from protected mode unimplemented yet */

 Error handling is not implemented. */

 Outer-privilege level return is not implemented */

 Error handling is not implemented. */

 Save real source value, then compare EAX against destination. */

 Success: write back to memory; no update of EAX */

 Failure: write the value we saw to EAX. */

 Create write-cycle to dest by writing the same value */

 In order to later set CR4.PCIDE, CR3[11:0] must be zero.  */

	/*

	 * First enable PAE, long mode needs it before CR0.PG = 1 is set.

	 * Then enable protected mode.	However, PCID cannot be enabled

	 * if EFER.LMA=0, so set it separately.

	/*

	 * Get back to real mode, to prepare a safe state in which to load

	 * CR0/CR3/CR4/EFER.  It's all a bit more complicated if the vCPU

	 * supports long mode.

 Zero CR4.PCIDE before CR0.PG.  */

 A 32-bit code segment is required to clear EFER.LMA.  */

 For the 64-bit case, this will clear EFER.LMA.  */

 Clear CR4.PAE before clearing EFER.LME. */

 And finally go back to 32-bit mode.  */

	/*

	 * Give leave_smm() a chance to make ISA-specific changes to the vCPU

	 * state (e.g. enter guest mode) before loading state from the SMM

	 * state-save area.

	/*

	 * Note, the ctxt->ops callbacks are responsible for handling side

	 * effects when writing MSRs and CRs, e.g. MMU context resets, CPUID

	 * runtime updates, etc...  If that changes, e.g. this flow is moved

	 * out of the emulator to make it look more like enter_smm(), then

	 * those side effects need to be explicitly handled for both success

	 * and shutdown.

 will be adjusted later */

 flat segment */

 4kb granularity */

 4GB limit */

 Read, Execute, Accessed */

 will be adjusted later */

 flat segment */

 4GB limit */

 4kb granularity */

 Read/Write, Accessed */

 32bit stack segment */

	/*

	 * syscall should always be enabled in longmode - so only become

	 * vendor specific (cpuid) if other modes are active...

	/*

	 * remark: Intel CPUs only support "syscall" in 64bit longmode. Also a

	 * 64bit guest with a 32bit compat-app running will #UD !! While this

	 * behaviour can be fixed (by emulating) into AMD response - CPUs of

	 * AMD can't behave like Intel.

	/*

	 * default: (not Intel, not AMD, not Hygon), apply Intel's

	 * stricter rules...

 syscall is not available in real mode */

 legacy mode */

 inject #GP if in real mode */

	/*

	 * Not recognized on AMD in compat mode (but is recognized in legacy

	 * mode).

 sysenter/sysexit have not been tested in 64bit mode. */

 inject #GP if in real mode or Virtual 8086 mode */

	/*

	 * VMware allows access to these ports even if denied

	 * by TSS I/O permission bitmap. Mimic behavior.

	/*

	 * Intel CPUs mask the counter and pointers in quite strange

	 * manner when ECX is zero due to REP-string optimizations.

 movsb */

 movsd/w */

 stosb */

 stosd/w */

	/*

	 * SDM says that segment selectors are loaded before segment

	 * descriptors

	/*

	 * Now load segment descriptors. If fault happens at this stage

	 * it is handled in a context of new task

 CR3 and ldt selector are not saved intentionally */

 General purpose registers */

	/*

	 * SDM says that segment selectors are loaded before segment

	 * descriptors.  This is important because CPL checks will

	 * use CS.RPL.

	/*

	 * If we're switching between Protected Mode and VM86, we need to make

	 * sure to update the mode before loading the segment descriptors so

	 * that the selectors are interpreted correctly.

	/*

	 * Now load segment descriptors. If fault happens at this stage

	 * it is handled in a context of new task

 Only GP registers and segment selectors are saved */

 FIXME: old_tss_base == ~0 ? */

 FIXME: check that next_tss_desc is tss */

	/*

	 * Check privileges. The three cases are task switch caused by...

	 *

	 * 1. jmp/call/int to task gate: Check against DPL of the task gate

	 * 2. Exception/IRQ/iret: No check is performed

	 * 3. jmp/call to TSS/task-gate: No check is performed since the

	 *    hardware checks it before exiting.

 Software interrupts */

 clear busy flag */

	/* set back link to prev task only if NT bit is set in eflags

 set busy flag */

 Set PF, ZF, SF */

 Set PF, ZF, SF */

 Set PF, ZF, SF */

	/* If we failed, we tainted the memory, but the very least we should

 Write back the register source. */

 Write back the memory destination with implicit LOCK prefix. */

		/*

		 * From MOVBE definition: "...When the operand size is 16 bits,

		 * the upper word of the destination register remains unchanged

		 * ..."

		 *

		 * Both casting ->valptr and ->val to u16 breaks strict aliasing

		 * rules so we have to do the operation almost per hand.

 Disable writeback. */

 #UD condition is already handled. */

 Disable writeback. */

 Disable writeback. */

 Disable writeback. */

 Disable writeback. */

 Disable writeback. */

 Let the processor re-execute the fixed hypercall */

 Disable writeback. */

 Disable writeback. */

 Disable writeback. */

 Disable writeback. */

 emulating clflush regardless of cpuid */

 emulating clflushopt regardless of cpuid */

	/*

	 * Don't emulate a case that should never be hit, instead of working

	 * around a lack of fxsave64/fxrstor64 on old compilers.

/*

 * Hardware doesn't save and restore XMM 0-7 without CR4.OSFXSR, but does save

 * and restore MXCSR.

/*

 * FXSAVE and FXRSTOR have 4 different formats depending on execution mode,

 *  1) 16 bit mode

 *  2) 32 bit mode

 *     - like (1), but FIP and FDP (foo) are only 16 bit.  At least Intel CPUs

 *       preserve whole 32 bit values, though, so (1) and (2) are the same wrt.

 *       save and restore

 *  3) 64-bit mode with REX.W prefix

 *     - like (2), but XMM 8-15 are being saved and restored

 *  4) 64-bit mode without REX.W prefix

 *     - like (3), but FIP and FDP are 64 bit

 *

 * Emulation uses (3) for (1) and (2) and preserves XMM 8-15 to reach the

 * desired result.  (4) is not emulated.

 *

 * Note: Guest and host CPUID.(EAX=07H,ECX=0H):EBX[bit 13] (deprecate FPU CS

 * and FPU DS) should match.

/*

 * FXRSTOR might restore XMM registers not provided by the guest. Fill

 * in the host registers (via FXSAVE) instead, so they won't be modified.

 * (preemption has to stay disabled until FXRSTOR).

 *

 * Use noinline to keep the stack for other functions called by callers small.

 Check if DR7.Global_Enable is set */

 Valid physical address? */

	/*

	 * VMware allows access to these Pseduo-PMCs even when read via RDPMC

	 * in Ring3 when CR4.PCE=0.

	/*

	 * If CR4.PCE is set, the SDM requires CPL=0 or CR0.PE=0.  The CR0.PE

	 * check however is unnecessary because CPL is always 0 outside

	 * protected mode.

/*

 * The "memory" destination is actually always a register, since we come

 * from the register case of group9.

 0xC0 - 0xC7 */

 0xC8 - 0xCF */

 0xD0 - 0xC7 */

 0xD8 - 0xDF */

 0xE0 - 0xE7 */

 0xE8 - 0xEF */

 0xF0 - 0xF7 */

 0xF8 - 0xFF */

 0xC0 - 0xC7 */

 0xC8 - 0xCF */

 0xD0 - 0xC7 */

 0xD8 - 0xDF */

 0xE0 - 0xE7 */

 0xE8 - 0xEF */

 0xF0 - 0xF7 */

 0xF8 - 0xFF */

 0xC0 - 0xC7 */

 0xC8 - 0xCF */

 0xD0 - 0xC7 */

 0xD8 - 0xDF */

 0xE0 - 0xE7 */

 0xE8 - 0xEF */

 0xF0 - 0xF7 */

 0xF8 - 0xFF */

 0x00 - 0x07 */

 0x08 - 0x0F */

 0x10 - 0x17 */

 0x18 - 0x1F */

 0x20 - 0x27 */

 0x28 - 0x2F */

 0x30 - 0x37 */

 0x38 - 0x3F */

 0x40 - 0x4F */

 0x50 - 0x57 */

 0x58 - 0x5F */

 0x60 - 0x67 */

 0x68 - 0x6F */

 insb, insw/insd */

 outsb, outsw/outsd */

 0x70 - 0x7F */

 0x80 - 0x87 */

 0x88 - 0x8F */

 0x90 - 0x97 */

 0x98 - 0x9F */

 0xA0 - 0xA7 */

 0xA8 - 0xAF */

 0xB0 - 0xB7 */

 0xB8 - 0xBF */

 0xC0 - 0xC7 */

 0xC8 - 0xCF */

 0xD0 - 0xD7 */

 0xD8 - 0xDF */

 0xE0 - 0xE7 */

 0xE8 - 0xEF */

 0xF0 - 0xF7 */

 0xF8 - 0xFF */

 0x00 - 0x0F */

 0x10 - 0x1F */

 4 * prefetch + 4 * reserved NOP */

 8 * reserved NOP */

 8 * reserved NOP */

 8 * reserved NOP */

 NOP + 7 * reserved NOP */

 0x20 - 0x2F */

 0x30 - 0x3F */

 0x40 - 0x4F */

 0x50 - 0x5F */

 0x60 - 0x6F */

 0x70 - 0x7F */

 0x80 - 0x8F */

 0x90 - 0x9F */

 0xA0 - 0xA7 */

 0xA8 - 0xAF */

 0xB0 - 0xB7 */

 0xB8 - 0xBF */

 0xC0 - 0xC7 */

 0xC8 - 0xCF */

 0xD0 - 0xDF */

 0xE0 - 0xEF */

 0xF0 - 0xFF */

/*

 * Insns below are selected by the prefix which indexed by the third opcode

 * byte.

 0x00 - 0x7f */

 0x80 - 0xef */

 0xf0 - 0xf1 */

 0xf2 - 0xff */

 NB. Immediates are sign-extended as necessary. */

 Special instructions do their own operand decoding. */

 Disable writeback. */

 Legacy prefixes. */

 operand-size override */

 switch between 2/4 bytes */

 address-size override */

 switch between 4/8 bytes */

 switch between 2/4 bytes */

 ES override */

 CS override */

 SS override */

 DS override */

 FS override */

 GS override */

 REX */

 LOCK */

 REPNE/REPNZ */

 REP/REPE/REPZ */

 Any legacy prefix after a REX prefix nullifies its effect. */

 REX prefix. */

 REX.W */

 Opcode byte(s). */

 Two-byte opcode? */

 0F_38 opcode map */

 vex-prefix instructions are not implemented */

 Unrecognised? */

		/*

		 * These are copied unconditionally here, and checked unconditionally

		 * in x86_emulate_insn.

 ModRM and SIB bytes. */

	/*

	 * Decode and fetch the source operand: register, memory

	 * or immediate.

	/*

	 * Decode and fetch the second source operand: register, memory

	 * or immediate.

 Decode and fetch the destination operand: register or memory. */

	/* The second termination condition only applies for REPE

	 * and REPNE. Test if the repeat string operation prefix is

	 * REPE/REPZ or REPNE/REPNZ and if it's the case it tests the

	 * corresponding termination condition according to:

	 * 	- if REPE/REPZ and ZF = 0 then done

	 * 	- if REPNE/REPNZ and ZF = 1 then done

 exception is returned in fop variable */

 LOCK prefix is allowed only with some instructions */

			/*

			 * Now that we know the fpu is exception safe, we can fetch

			 * operands from it.

 Instruction can only be executed in protected mode */

 Privileged instruction can be executed only in CPL=0 */

 Do instruction specific permission checks */

 All REP prefixes have the same first termination condition */

 optimisation - avoid slow emulated read if Mov */

 Copy full 64-bit value for CMPXCHG8B.  */

 jcc (short) */

 lea r16/r32, m */

 nop / xchg reg, rax */

 cbw/cwde/cdqe */

 int3 */

 int n */

 into */

 jmp rel */

 jmp rel short */

 Disable writeback. */

 hlt */

 cmc */

 complement carry flag from eflags reg */

 clc */

 stc */

 cld */

 std */

	/*

	 * restore dst type in case the decoding will be reused

	 * (happens for string instruction )

			/*

			 * Re-enter guest when pio read ahead buffer is empty

			 * or, if it is not used, after each 1024 iteration.

				/*

				 * Reset read cache. Usually happens before

				 * decode, but since instruction is restarted

				 * we have to do it here.

 skip rip writeback */

 wbinvd */

 invd */

 GrpP (prefetch) */

 Grp16 (prefetch/nop) */

 nop */

 mov cr, reg */

 mov from dr to reg */

 cmov */

 no writeback */

 jnz rel, etc*/

 setcc r/m8 */

 movzx */

 movsx */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * irq_comm.c: Common API for in kernel interrupt controller

 * Copyright (c) 2007, Intel Corporation.

 *

 * Authors:

 *   Yaozu (Eddie) Dong <Eddie.dong@intel.com>

 *

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

	/* We can't check irqchip_in_kernel() here as some callers are

	 * currently initializing the irqchip. Other callers should therefore

	 * check kvm_arch_can_set_irq_routing() before calling this function.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * KVM L1 hypervisor optimizations on Hyper-V.

		/*

		 * Flush all valid roots, and see if all vCPUs have converged

		 * on a common root, in which case future flushes can skip the

		 * loop and flush the common root.

			/*

			 * Set the tracked root to the first valid root.  Keep

			 * this root for the entirety of the loop even if more

			 * roots are encountered as a low effort optimization

			 * to avoid flushing the same (first) root again.

			/*

			 * Stop processing roots if a failure occurred and

			 * multiple valid roots have already been detected.

		/*

		 * The optimized flush of a single root can't be used if there

		 * are multiple valid roots (obviously).

/*

 * 8253/8254 interval timer emulation

 *

 * Copyright (c) 2003-2004 Fabrice Bellard

 * Copyright (c) 2006 Intel Corporation

 * Copyright (c) 2007 Keir Fraser, XenSource Inc

 * Copyright (c) 2008 Intel Corporation

 * Copyright 2009 Red Hat, Inc. and/or its affiliates.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a copy

 * of this software and associated documentation files (the "Software"), to deal

 * in the Software without restriction, including without limitation the rights

 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell

 * copies of the Software, and to permit persons to whom the Software is

 * furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN

 * THE SOFTWARE.

 *

 * Authors:

 *   Sheng Yang <sheng.yang@intel.com>

 *   Based on QEMU and Xen.

 XXX: just disable/enable counting */

 Restart counting on rising edge. */

	/*

	 * The Counter does not stop when it reaches zero. In

	 * Modes 0, 1, 4, and 5 the Counter ``wraps around'' to

	 * the highest count, either FFFF hex for binary counting

	 * or 9999 for BCD counting, and continues counting.

	 * Modes 2 and 3 are periodic; the Counter reloads

	 * itself with the initial count and continues counting

	 * from there.

 XXX: may be incorrect for odd counts */

 TODO: Return NULL COUNT (bit 6). */

	/* irq_ack should be set before pending is read.  Order accesses with

	 * inc(pending) in pit_timer_fn and xchg(irq_ack, 0) in pit_do_work.

 Somewhat arbitrarily make vcpu0 the owner of the PIT. */

	/*

	 * Provides NMI watchdog support via Virtual Wire mode.

	 * The route is: PIT -> LVT0 in NMI mode.

	 *

	 * Note: Our Virtual Wire implementation does not follow

	 * the MP specification.  We propagate a PIT interrupt to all

	 * VCPUs and only when LVT0 is in NMI mode.  The interrupt can

	 * also be simultaneously delivered through PIC and IOAPIC.

	/*

	 * AMD SVM AVIC accelerates EOI write and does not trap.

	 * This cause in-kernel PIT re-inject mode to fail

	 * since it checks ps->irq_ack before kvm_set_irq()

	 * and relies on the ack notifier to timely queue

	 * the pt->worker work iterm and reinject the missed tick.

	 * So, deactivate APICv when PIT is in reinject mode.

 The initial state is preserved while ps->reinject == 0. */

 TODO The new value only affected after the retriggered */

	/*

	 * Do not allow the guest to program periodic timers with small

	 * interval, since the hrtimers are not throttled by the host

	 * scheduler.

	/*

	 * The largest possible initial count is 0; this is equivalent

	 * to 216 for binary counting and 104 for BCD counting.

	/* Two types of timer

 FIXME: enhance mode 4 precision */

 save existing mode for later reenablement */

 disable timer */

 Read-Back Command. */

 Select Counter <channel>. */

 Write Count. */

 Refresh clock toggles at about 15us. We approximate as 2^14ns. */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * irq.c: API for in kernel interrupt controller

 * Copyright (c) 2007, Intel Corporation.

 * Copyright 2009 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Yaozu (Eddie) Dong <Eddie.dong@intel.com>

/*

 * check if there are pending timer events

 * to be processed.

/*

 * check if there is a pending userspace external interrupt

/*

 * check if there is pending interrupt from

 * non-APIC source without intack.

	/*

	 * FIXME: interrupt.injected represents an interrupt whose

	 * side-effects have already been applied (e.g. bit from IRR

	 * already moved to ISR). Therefore, it is incorrect to rely

	 * on interrupt.injected to know if there is a pending

	 * interrupt in the user-mode LAPIC.

	 * This leads to nVMX/nSVM not be able to distinguish

	 * if it should exit from L2 to L1 on EXTERNAL_INTERRUPT on

	 * pending interrupt or should re-inject an injected

	 * interrupt.

/*

 * check if there is injectable interrupt:

 * when virtual interrupt delivery enabled,

 * interrupt from apic will handled by hardware,

 * we don't need to check it here.

 LAPIC */

/*

 * check if there is pending interrupt without

 * intack.

 LAPIC */

/*

 * Read pending interrupt(from non-APIC source)

 * vector and intack.

 PIC */

/*

 * Read pending interrupt vector and intack.

 PIC */

 APIC */

/*

 * 8259 interrupt controller emulation

 *

 * Copyright (c) 2003-2004 Fabrice Bellard

 * Copyright (c) 2007 Intel Corporation

 * Copyright 2009 Red Hat, Inc. and/or its affiliates.

 *

 * Permission is hereby granted, free of charge, to any person obtaining a copy

 * of this software and associated documentation files (the "Software"), to deal

 * in the Software without restriction, including without limitation the rights

 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell

 * copies of the Software, and to permit persons to whom the Software is

 * furnished to do so, subject to the following conditions:

 *

 * The above copyright notice and this permission notice shall be included in

 * all copies or substantial portions of the Software.

 *

 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR

 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,

 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL

 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER

 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,

 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN

 * THE SOFTWARE.

 * Authors:

 *   Yaozu (Eddie) Dong <Eddie.dong@intel.com>

 *   Port from Qemu.

	/*

	 * We are dropping lock while calling ack notifiers since ack

	 * notifier callbacks for assigned devices call into PIC recursively.

	 * Other interrupt may be delivered to PIC while lock is dropped but

	 * it should be safe since PIC state is already updated at this stage.

/*

 * set irq level. If an edge is detected, then the IRR is set to 1

 level triggered */

 edge triggered */

/*

 * return the highest priority found in mask (highest = smallest

 * number). Return 8 if no irq

/*

 * return the pic wanted interrupt. return -1 if none

	/*

	 * compute current priority. If special fully nested mode on the

	 * master, the IRQ coming from the slave is not taken into account

	 * for the priority computation.

		/*

		 * higher priority found: an irq should be generated

/*

 * raise irq to CPU if necessary. must be called every time the active

 * irq may change

		/*

		 * if irq request by slave pic, signal master PIC

/*

 * acknowledge interrupt 'irq'

	/*

	 * We don't clear a level sensitive interrupt here

				/*

				 * spurious IRQ on slave controller

		/*

		 * spurious IRQ on host controller

 end of interrupt */

 no operation */

 normal mode */

/*

 * callback when PIC0 irq status changed

	/*

	 * Initialize PIO device

 SPDX-License-Identifier: GPL-2.0-only

/*

 * vMTRR implementation

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 * Copyright(C) 2015 Intel Corporation.

 *

 * Authors:

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Avi Kivity   <avi@qumranet.com>

 *   Marcelo Tosatti <mtosatti@redhat.com>

 *   Paolo Bonzini <pbonzini@redhat.com>

 *   Xiao Guangrong <guangrong.xiao@linux.intel.com>

 0, 1, 4, 5, 6 */

 variable MTRRs */

 MTRR base */

 MTRR mask */

	/*

	 * Intel SDM 11.11.2.2: all MTRRs are disabled when

	 * IA32_MTRR_DEF_TYPE.E bit is cleared, and the UC

	 * memory type is applied to all of physical memory.

	 *

	 * However, virtual machines can be run with CPUID such that

	 * there are no MTRRs.  In that case, the firmware will never

	 * enable MTRRs and it is obviously undesirable to run the

	 * guest entirely with UC memory and we use WB.

/*

* Three terms are used in the following code:

* - segment, it indicates the address segments covered by fixed MTRRs.

* - unit, it corresponds to the MSR entry in the segment.

* - range, a range is covered in one memory cache type.

 the start position in kvm_mtrr.fixed_ranges[]. */

 MSR_MTRRfix64K_00000, 1 unit. 64K fixed mtrr. */

 64K */

	/*

	 * MSR_MTRRfix16K_80000 ... MSR_MTRRfix16K_A0000, 2 units,

	 * 16K fixed mtrr.

 16K */

	/*

	 * MSR_MTRRfix4K_C0000 ... MSR_MTRRfix4K_F8000, 8 units,

	 * 4K fixed mtrr.

 12K */

/*

 * The size of unit is covered in one MSR, one MSR entry contains

 * 8 ranges so that unit size is always 8 * 2^range_shift.

 each unit has 8 ranges. */

	/* This cannot overflow because writing to the reserved bits of

	 * variable MTRRs causes a #GP.

 fixed MTRRs. */

 variable range MTRRs. */

 remove the entry if it's in the list. */

	/*

	 * Set all illegal GPA bits in the mask, since those bits must

	 * implicitly be 0.  The bits are then cleared when reading them.

 add it to the list if it's enabled. */

 MSR_MTRRcap is a readonly MSR. */

		/*

		 * SMRR = 0

		 * WC = 1

		 * FIX = 1

		 * VCNT = KVM_NR_VAR_MTRR

 Variable MTRRs */

 input fields. */

 output fields. */

 mtrr is completely disabled? */

 [start, end) is not fully covered in MTRRs? */

 private fields. */

 used for fixed MTRRs. */

 used for var MTRRs. */

 max address has been covered in var MTRRs. */

		/*

		 * the function is called when we do kvm_mtrr.head walking.

		 * Range has the minimum base address which interleaves

		 * [looker->start_max, looker->end).

 update the max address has been covered. */

 terminate the lookup. */

 have looked up for all fixed MTRRs. */

 switch to next segment. */

		/*

		 * Please refer to Intel SDM Volume 3: 11.11.4.1 MTRR

		 * Precedences.

		/*

		 * If two or more variable memory ranges match and the

		 * memory types are identical, then that memory type is

		 * used.

		/*

		 * If two or more variable memory ranges match and one of

		 * the memory types is UC, the UC memory type used.

		/*

		 * If two or more variable memory ranges match and the

		 * memory types are WT and WB, the WT memory type is used.

		/*

		 * For overlaps not defined by the above rules, processor

		 * behavior is undefined.

 We use WB for this undefined behavior. :( */

 not contained in any MTRRs. */

	/*

	 * We just check one page, partially covered by MTRRs is

	 * impossible.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Support KVM gust page tracking

 *

 * This feature allows us to track page access in guest. Currently, only

 * write access is tracked.

 *

 * Copyright(C) 2015 Intel Corporation.

 *

 * Author:

 *   Xiao Guangrong <guangrong.xiao@linux.intel.com>

/*

 * add guest page to the tracking pool so that corresponding access on that

 * page will be intercepted.

 *

 * It should be called under the protection both of mmu-lock and kvm->srcu

 * or kvm->slots_lock.

 *

 * @kvm: the guest instance we are interested in.

 * @slot: the @gfn belongs to.

 * @gfn: the guest page.

 * @mode: tracking mode, currently only write track is supported.

	/*

	 * new track stops large page mapping for the

	 * tracked page.

/*

 * remove the guest page from the tracking pool which stops the interception

 * of corresponding access on that page. It is the opposed operation of

 * kvm_slot_page_track_add_page().

 *

 * It should be called under the protection both of mmu-lock and kvm->srcu

 * or kvm->slots_lock.

 *

 * @kvm: the guest instance we are interested in.

 * @slot: the @gfn belongs to.

 * @gfn: the guest page.

 * @mode: tracking mode, currently only write track is supported.

	/*

	 * allow large page mapping for the tracked page

	 * after the tracker is gone.

/*

 * check if the corresponding access on the specified guest page is tracked.

/*

 * register the notifier so that event interception for the tracked guest

 * pages can be received.

/*

 * stop receiving the event interception. It is the opposed operation of

 * kvm_page_track_register_notifier().

/*

 * Notify the node that write access is intercepted and write emulation is

 * finished at this time.

 *

 * The node should figure out if the written page is the one that node is

 * interested in by itself.

/*

 * Notify the node that memory slot is being removed or moved so that it can

 * drop write-protection for the pages in the memory slot.

 *

 * The node should figure out it has any write-protected pages in this slot

 * by itself.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * mmu_audit.c:

 *

 * Audit code for KVM MMU

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Avi Kivity   <avi@qumranet.com>

 *   Marcelo Tosatti <mtosatti@redhat.com>

 *   Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>

 SPDX-License-Identifier: GPL-2.0

 Initializes the TDP MMU for the VM, if enabled. */

 This should not be changed for the lifetime of the VM. */

	/*

	 * Ensure that all the outstanding RCU callbacks to free shadow pages

	 * can run before the VM is torn down.

/*

 * This is called through call_rcu in order to free TDP page table memory

 * safely with respect to other kernel threads that may be operating on

 * the memory.

 * By only accessing TDP MMU page table memory in an RCU read critical

 * section, and freeing it after a grace period, lockless access to that

 * memory won't use it after it is freed.

/*

 * Finds the next valid root after root (or the first valid root if root

 * is NULL), takes a reference on it, and returns that next root. If root

 * is not NULL, this thread should have already taken a reference on it, and

 * that reference will be dropped. If no valid root is found, this

 * function will return NULL.

/*

 * Note: this iterator gets and puts references to the roots it iterates over.

 * This makes it safe to release the MMU lock and yield within the loop, but

 * if exiting the loop early, the caller must drop the reference to the most

 * recent root. (Unless keeping a live reference is desirable.)

 *

 * If shared is set, this function is operating under the MMU lock in read

 * mode. In the unlikely event that this thread must free a root, the lock

 * will be temporarily dropped and reacquired in write mode.

 Check for an existing root before allocating a new one. */

/**

 * tdp_mmu_link_page - Add a new page to the list of pages used by the TDP MMU

 *

 * @kvm: kvm instance

 * @sp: the new page

 * @account_nx: This page replaces a NX large page and should be marked for

 *		eventual reclaim.

/**

 * tdp_mmu_unlink_page - Remove page from the list of pages used by the TDP MMU

 *

 * @kvm: kvm instance

 * @sp: the page to be removed

 * @shared: This operation may not be running under the exclusive use of

 *	    the MMU lock and the operation must synchronize with other

 *	    threads that might be adding or removing pages.

/**

 * handle_removed_tdp_mmu_page - handle a pt removed from the TDP structure

 *

 * @kvm: kvm instance

 * @pt: the page removed from the paging structure

 * @shared: This operation may not be running under the exclusive use

 *	    of the MMU lock and the operation must synchronize with other

 *	    threads that might be modifying SPTEs.

 *

 * Given a page table that has been removed from the TDP paging structure,

 * iterates through the page table to clear SPTEs and free child page tables.

 *

 * Note that pt is passed in as a tdp_ptep_t, but it does not need RCU

 * protection. Since this thread removed it from the paging structure,

 * this thread will be responsible for ensuring the page is freed. Hence the

 * early rcu_dereferences in the function.

			/*

			 * Set the SPTE to a nonpresent value that other

			 * threads will not overwrite. If the SPTE was

			 * already marked as removed then another thread

			 * handling a page fault could overwrite it, so

			 * set the SPTE until it is set from some other

			 * value to the removed SPTE value.

			/*

			 * If the SPTE is not MMU-present, there is no backing

			 * page associated with the SPTE and so no side effects

			 * that need to be recorded, and exclusive ownership of

			 * mmu_lock ensures the SPTE can't be made present.

			 * Note, zapping MMIO SPTEs is also unnecessary as they

			 * are guarded by the memslots generation, not by being

			 * unreachable.

			/*

			 * Marking the SPTE as a removed SPTE is not

			 * strictly necessary here as the MMU lock will

			 * stop other threads from concurrently modifying

			 * this SPTE. Using the removed SPTE value keeps

			 * the two branches consistent and simplifies

			 * the function.

/**

 * __handle_changed_spte - handle bookkeeping associated with an SPTE change

 * @kvm: kvm instance

 * @as_id: the address space of the paging structure the SPTE was a part of

 * @gfn: the base GFN that was mapped by the SPTE

 * @old_spte: The value of the SPTE before the change

 * @new_spte: The value of the SPTE after the change

 * @level: the level of the PT the SPTE is part of in the paging structure

 * @shared: This operation may not be running under the exclusive use of

 *	    the MMU lock and the operation must synchronize with other

 *	    threads that might be modifying SPTEs.

 *

 * Handle bookkeeping that might result from the modification of a SPTE.

 * This function must be called for all TDP SPTE modifications.

	/*

	 * If this warning were to trigger it would indicate that there was a

	 * missing MMU notifier or a race with some notifier handler.

	 * A present, leaf SPTE should never be directly replaced with another

	 * present leaf SPTE pointing to a different PFN. A notifier handler

	 * should be zapping the SPTE before the main MM's page table is

	 * changed, or the SPTE should be zeroed, and the TLBs flushed by the

	 * thread before replacement.

		/*

		 * Crash the host to prevent error propagation and guest data

		 * corruption.

	/*

	 * The only times a SPTE should be changed from a non-present to

	 * non-present state is when an MMIO entry is installed/modified/

	 * removed. In that case, there is nothing to do here.

		/*

		 * If this change does not involve a MMIO SPTE or removed SPTE,

		 * it is unexpected. Log the change, though it should not

		 * impact the guest since both the former and current SPTEs

		 * are nonpresent.

	/*

	 * Recursively handle child PTs if the change removed a subtree from

	 * the paging structure.

/*

 * tdp_mmu_set_spte_atomic - Set a TDP MMU SPTE atomically

 * and handle the associated bookkeeping.  Do not mark the page dirty

 * in KVM's dirty bitmaps.

 *

 * @kvm: kvm instance

 * @iter: a tdp_iter instance currently on the SPTE that should be set

 * @new_spte: The value the SPTE should be set to

 * Returns: true if the SPTE was set, false if it was not. If false is returned,

 *	    this function will have no side-effects.

	/*

	 * Do not change removed SPTEs. Only the thread that froze the SPTE

	 * may modify it.

	/*

	 * Note, fast_pf_fix_direct_spte() can also modify TDP MMU SPTEs and

	 * does not hold the mmu_lock.

	/*

	 * Freeze the SPTE by setting it to a special,

	 * non-present value. This will stop other threads from

	 * immediately installing a present entry in its place

	 * before the TLBs are flushed.

	/*

	 * No other thread can overwrite the removed SPTE as they

	 * must either wait on the MMU lock or use

	 * tdp_mmu_set_spte_atomic which will not overwrite the

	 * special removed SPTE value. No bookkeeping is needed

	 * here since the SPTE is going from non-present

	 * to non-present.

/*

 * __tdp_mmu_set_spte - Set a TDP MMU SPTE and handle the associated bookkeeping

 * @kvm: kvm instance

 * @iter: a tdp_iter instance currently on the SPTE that should be set

 * @new_spte: The value the SPTE should be set to

 * @record_acc_track: Notify the MM subsystem of changes to the accessed state

 *		      of the page. Should be set unless handling an MMU

 *		      notifier for access tracking. Leaving record_acc_track

 *		      unset in that case prevents page accesses from being

 *		      double counted.

 * @record_dirty_log: Record the page as dirty in the dirty bitmap if

 *		      appropriate for the change being made. Should be set

 *		      unless performing certain dirty logging operations.

 *		      Leaving record_dirty_log unset in that case prevents page

 *		      writes from being double counted.

	/*

	 * No thread should be using this function to set SPTEs to the

	 * temporary removed SPTE value.

	 * If operating under the MMU lock in read mode, tdp_mmu_set_spte_atomic

	 * should be used. If operating under the MMU lock in write mode, the

	 * use of the removed SPTE should not be necessary.

/*

 * Yield if the MMU lock is contended or this thread needs to return control

 * to the scheduler.

 *

 * If this function should yield and flush is set, it will perform a remote

 * TLB flush before yielding.

 *

 * If this function yields, it will also reset the tdp_iter's walk over the

 * paging structure and the calling function should skip to the next

 * iteration to allow the iterator to continue its traversal from the

 * paging structure root.

 *

 * Return true if this function yielded and the iterator's traversal was reset.

 * Return false if a yield was not needed.

 Ensure forward progress has been made before yielding. */

/*

 * Tears down the mappings for the range of gfns, [start, end), and frees the

 * non-root pages mapping GFNs strictly within that range. Returns true if

 * SPTEs have been cleared and a TLB flush is needed before releasing the

 * MMU lock.

 *

 * If can_yield is true, will release the MMU lock and reschedule if the

 * scheduler needs the CPU or there is contention on the MMU lock. If this

 * function cannot yield, it will not release the MMU lock or reschedule and

 * the caller must ensure it does not supply too large a GFN range, or the

 * operation can cause a soft lockup.

 *

 * If shared is true, this thread holds the MMU lock in read mode and must

 * account for the possibility that other threads are modifying the paging

 * structures concurrently. If shared is false, this thread should hold the

 * MMU lock in write mode.

	/*

	 * No need to try to step down in the iterator when zapping all SPTEs,

	 * zapping the top-level non-leaf SPTEs will recurse on their children.

	/*

	 * Bound the walk at host.MAXPHYADDR, guest accesses beyond that will

	 * hit a #PF(RSVD) and never get to an EPT Violation/Misconfig / #NPF,

	 * and so KVM will never install a SPTE for such addresses.

		/*

		 * If this is a non-last-level SPTE that covers a larger range

		 * than should be zapped, continue, and zap the mappings at a

		 * lower level, except when zapping all SPTEs.

			/*

			 * The iter must explicitly re-read the SPTE because

			 * the atomic cmpxchg failed.

/*

 * Tears down the mappings for the range of gfns, [start, end), and frees the

 * non-root pages mapping GFNs strictly within that range. Returns true if

 * SPTEs have been cleared and a TLB flush is needed before releasing the

 * MMU lock.

/*

 * Since kvm_tdp_mmu_zap_all_fast has acquired a reference to each

 * invalidated root, they will not be freed until this function drops the

 * reference. Before dropping that reference, tear down the paging

 * structure so that whichever thread does drop the last reference

 * only has to do a trivial amount of work. Since the roots are invalid,

 * no new SPTEs should be created under them.

		/*

		 * Put the reference acquired in

		 * kvm_tdp_mmu_invalidate_roots

/*

 * Mark each TDP MMU root as invalid so that other threads

 * will drop their references and allow the root count to

 * go to 0.

 *

 * Also take a reference on all roots so that this thread

 * can do the bulk of the work required to free the roots

 * once they are invalidated. Without this reference, a

 * vCPU thread might drop the last reference to a root and

 * get stuck with tearing down the entire paging structure.

 *

 * Roots which have a zero refcount should be skipped as

 * they're already being torn down.

 * Already invalid roots should be referenced again so that

 * they aren't freed before kvm_tdp_mmu_zap_all_fast is

 * done with them.

 *

 * This has essentially the same effect for the TDP MMU

 * as updating mmu_valid_gen does for the shadow MMU.

/*

 * Installs a last-level SPTE to handle a TDP page fault.

 * (NPT/EPT violation/misconfiguration)

	/*

	 * If the page fault was caused by a write but the page is write

	 * protected, emulation is needed. If the emulation was skipped,

	 * the vCPU would have the same fault again.

 If a MMIO SPTE is installed, the MMIO will need to be emulated. */

	/*

	 * Increase pf_fixed in both RET_PF_EMULATE and RET_PF_FIXED to be

	 * consistent with legacy MMU behavior.

/*

 * Handle a TDP page fault (NPT/EPT violation/misconfiguration) by installing

 * page tables and SPTEs to translate the faulting guest physical address.

		/*

		 * If there is an SPTE mapping a large page at a higher level

		 * than the target, that SPTE must be cleared and replaced

		 * with a non-leaf SPTE.

			/*

			 * The iter must explicitly re-read the spte here

			 * because the new value informs the !present

			 * path below.

			/*

			 * If SPTE has been frozen by another thread, just

			 * give up and retry, avoiding unnecessary page table

			 * allocation and free.

	/*

	 * Don't support rescheduling, none of the MMU notifiers that funnel

	 * into this helper allow blocking; it'd be dead, wasteful code.

/*

 * Mark the SPTEs range of GFNs [start, end) unaccessed and return non-zero

 * if any of the GFNs in the range have been accessed.

 If we have a non-accessed entry we don't need to change the pte. */

		/*

		 * Capture the dirty status of the page, so that it doesn't get

		 * lost when the SPTE is marked for access tracking.

 Huge pages aren't expected to be modified without first being zapped. */

	/*

	 * Note, when changing a read-only SPTE, it's not strictly necessary to

	 * zero the SPTE before setting the new PFN, but doing so preserves the

	 * invariant that the PFN of a present * leaf SPTE can never change.

	 * See __handle_changed_spte().

/*

 * Handle the changed_pte MMU notifier for the TDP MMU.

 * data is a pointer to the new pte_t mapping the HVA specified by the MMU

 * notifier.

 * Returns non-zero if a flush is needed before releasing the MMU lock.

 FIXME: return 'flush' instead of flushing here. */

/*

 * Remove write access from all SPTEs at or above min_level that map GFNs

 * [start, end). Returns true if an SPTE has been changed and the TLBs need to

 * be flushed.

			/*

			 * The iter must explicitly re-read the SPTE because

			 * the atomic cmpxchg failed.

/*

 * Remove write access from all the SPTEs mapping GFNs in the memslot. Will

 * only affect leaf SPTEs down to min_level.

 * Returns true if an SPTE has been changed and the TLBs need to be flushed.

/*

 * Clear the dirty status of all the SPTEs mapping GFNs in the memslot. If

 * AD bits are enabled, this will involve clearing the dirty bit on each SPTE.

 * If AD bits are not enabled, this will require clearing the writable bit on

 * each SPTE. Returns true if an SPTE has been changed and the TLBs need to

 * be flushed.

			/*

			 * The iter must explicitly re-read the SPTE because

			 * the atomic cmpxchg failed.

/*

 * Clear the dirty status of all the SPTEs mapping GFNs in the memslot. If

 * AD bits are enabled, this will involve clearing the dirty bit on each SPTE.

 * If AD bits are not enabled, this will require clearing the writable bit on

 * each SPTE. Returns true if an SPTE has been changed and the TLBs need to

 * be flushed.

/*

 * Clears the dirty status of all the 4k SPTEs mapping GFNs for which a bit is

 * set in mask, starting at gfn. The given memslot is expected to contain all

 * the GFNs represented by set bits in the mask. If AD bits are enabled,

 * clearing the dirty status will involve clearing the dirty bit on each SPTE

 * or, if AD bits are not enabled, clearing the writable bit on each SPTE.

/*

 * Clears the dirty status of all the 4k SPTEs mapping GFNs for which a bit is

 * set in mask, starting at gfn. The given memslot is expected to contain all

 * the GFNs represented by set bits in the mask. If AD bits are enabled,

 * clearing the dirty status will involve clearing the dirty bit on each SPTE

 * or, if AD bits are not enabled, clearing the writable bit on each SPTE.

/*

 * Clear leaf entries which could be replaced by large mappings, for

 * GFNs within the slot.

			/*

			 * The iter must explicitly re-read the SPTE because

			 * the atomic cmpxchg failed.

/*

 * Clear non-leaf entries (and free associated page tables) which could

 * be replaced by large mappings, for GFNs within the slot.

/*

 * Removes write access on the last level SPTE mapping this GFN and unsets the

 * MMU-writable bit to ensure future writes continue to be intercepted.

 * Returns true if an SPTE was set and a TLB flush is needed.

/*

 * Removes write access on the last level SPTE mapping this GFN and unsets the

 * MMU-writable bit to ensure future writes continue to be intercepted.

 * Returns true if an SPTE was set and a TLB flush is needed.

/*

 * Return the level of the lowest level SPTE added to sptes.

 * That SPTE may be non-present.

 *

 * Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.

/*

 * Returns the last level spte pointer of the shadow page walk for the given

 * gpa, and sets *spte to the spte value. This spte may be non-preset. If no

 * walk could be performed, returns NULL and *spte does not contain valid data.

 *

 * Contract:

 *  - Must be called between kvm_tdp_mmu_walk_lockless_{begin,end}.

 *  - The returned sptep must not be used after kvm_tdp_mmu_walk_lockless_end.

 *

 * WARNING: This function is only intended to be called during fast_page_fault.

	/*

	 * Perform the rcu_dereference to get the raw spte pointer value since

	 * we are passing it up to fast_page_fault, which is shared with the

	 * legacy MMU and thus does not retain the TDP MMU-specific __rcu

	 * annotation.

	 *

	 * This is safe since fast_page_fault obeys the contracts of this

	 * function as well as all TDP MMU contracts around modifying SPTEs

	 * outside of mmu_lock.

 SPDX-License-Identifier: GPL-2.0

/*

 * Recalculates the pointer to the SPTE for the current GFN and level and

 * reread the SPTE.

/*

 * Return the TDP iterator to the root PT and allow it to continue its

 * traversal over the paging structure from there.

/*

 * Sets a TDP iterator to walk a pre-order traversal of the paging structure

 * rooted at root_pt, starting with the walk to translate next_last_level_gfn.

/*

 * Given an SPTE and its level, returns a pointer containing the host virtual

 * address of the child page table referenced by the SPTE. Returns null if

 * there is no such entry.

	/*

	 * There's no child entry if this entry isn't present or is a

	 * last-level entry.

/*

 * Steps down one level in the paging structure towards the goal GFN. Returns

 * true if the iterator was able to step down a level, false otherwise.

	/*

	 * Reread the SPTE before stepping down to avoid traversing into page

	 * tables that are no longer linked from this entry.

/*

 * Steps to the next entry in the current page table, at the current page table

 * level. The next entry could point to a page backing guest memory or another

 * page table, or it could be non-present. Returns true if the iterator was

 * able to step to the next entry in the page table, false if the iterator was

 * already at the end of the current page table.

	/*

	 * Check if the iterator is already at the end of the current page

	 * table.

/*

 * Tries to traverse back up a level in the paging structure so that the walk

 * can continue from the next entry in the parent page table. Returns true on a

 * successful step up, false if already in the root page.

/*

 * Step to the next SPTE in a pre-order traversal of the paging structure.

 * To get to the next SPTE, the iterator either steps down towards the goal

 * GFN, if at a present, non-last-level SPTE, or over to a SPTE mapping a

 * highter GFN.

 *

 * The basic algorithm is as follows:

 * 1. If the current SPTE is a non-last-level SPTE, step down into the page

 *    table it points to.

 * 2. If the iterator cannot step down, it will try to step to the next SPTE

 *    in the current page of the paging structure.

 * 3. If the iterator cannot step to the next entry in the current page, it will

 *    try to step up to the parent paging structure page. In this case, that

 *    SPTE will have already been visited, and so the iterator must also step

 *    to the side again.

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * This module enables machines with Intel VT-x extensions to run virtual

 * machines without emulation or binary translation.

 *

 * MMU support

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2010 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Yaniv Kamay  <yaniv@qumranet.com>

 *   Avi Kivity   <avi@qumranet.com>

 Recovery can cause latency spikes, disable it for PREEMPT_RT.  */

/*

 * When setting this variable to true it enables Two-Dimensional-Paging

 * where the hardware walks 2 page tables:

 * 1. the guest-virtual to guest-physical

 * 2. while doing 1. it walks guest-physical to host-physical

 * If the hardware supports that we don't need to do shadow paging.

 make pte_list_desc fit well in cache lines */

/*

 * Slight optimization of cacheline layout, by putting `more' and `spte_count'

 * at the start; then accessing it will only use one single cacheline for

 * either full (entries==PTE_LIST_EXT) case or entries<=6.

	/*

	 * Stores number of entries stored in the pte_list_desc.  No need to be

	 * u64 but just for easier alignment.  When PTE_LIST_EXT, means full.

/*

 * Yes, lot's of underscores.  They're a hint that you probably shouldn't be

 * reading from the role_regs.  Once the mmu_role is constructed, it becomes

 * the single source of truth for the MMU's state.

/*

 * The MMU itself (with a valid role) is the single source of truth for the

 * MMU.  Do not use the regs used to build the MMU/role, nor the vCPU.  The

 * regs don't account for dependencies, e.g. clearing CR4 bits if CR0.PG=1,

 * and the vCPU may be incorrect/irrelevant.

 Ensure the spte is completely set before we increase the count */

	/*

	 * If we map the spte from nonpresent to present, We should store

	 * the high bits firstly, then set present bit, so cpu can not

	 * fetch this spte while we are setting the spte.

	/*

	 * If we map the spte from present to nonpresent, we should clear

	 * present bit firstly to avoid vcpu fetch the old high bits.

 xchg acts as a barrier before the setting of the high bits */

/*

 * The idea using the light way get the spte on x86_32 guest is from

 * gup_get_pte (mm/gup.c).

 *

 * An spte tlb flush may be pending, because kvm_set_pte_rmapp

 * coalesces them and we are running out of the MMU lock.  Therefore

 * we need to protect against in-progress updates of the spte.

 *

 * Reading the spte while an update is in progress may get the old value

 * for the high part of the spte.  The race is fine for a present->non-present

 * change (because the high part of the spte is ignored for non-present spte),

 * but for a present->present change we must reread the spte.

 *

 * All such changes are done in two steps (present->non-present and

 * non-present->present), hence it is enough to count the number of

 * present->non-present updates: if it changed while reading the spte,

 * we might have hit the race.  This is done using clear_spte_count.

	/*

	 * Always atomically update spte if it can be updated

	 * out of mmu-lock, it can ensure dirty bit is not lost,

	 * also, it can help us to get a stable is_writable_pte()

	 * to ensure tlb flush is not missed.

/* Rules for using mmu_spte_set:

 * Set the sptep from nonpresent to present.

 * Note: the sptep being assigned *must* be either not present

 * or in a state where the hardware will not attempt to update

 * the spte.

/*

 * Update the SPTE (excluding the PFN), but do not track changes in its

 * accessed/dirty status.

/* Rules for using mmu_spte_update:

 * Update the state bits, it means the mapped pfn is not changed.

 *

 * Whenever we overwrite a writable spte with a read-only one we

 * should flush remote TLBs. Otherwise rmap_write_protect

 * will find a read-only spte, even though the writable spte

 * might be cached on a CPU's TLB, the return value indicates this

 * case.

 *

 * Returns true if the TLB needs to be flushed

	/*

	 * For the spte updated out of mmu-lock is safe, since

	 * we always atomically update it, see the comments in

	 * spte_has_volatile_bits().

	/*

	 * Flush TLB when accessed/dirty states are changed in the page tables,

	 * to guarantee consistency between TLB and page tables.

/*

 * Rules for using mmu_spte_clear_track_bits:

 * It sets the sptep from present to nonpresent, and track the

 * state bits, it is used to clear the last level sptep.

 * Returns the old PTE.

	/*

	 * KVM does not hold the refcount of the page used by

	 * kvm mmu, before reclaiming the page, we should

	 * unmap it from mmu first.

/*

 * Rules for using mmu_spte_clear_no_track:

 * Directly clear spte without caring the state bits of sptep,

 * it is used to set the upper level spte.

 Restore an acc-track PTE back to a regular PTE */

 Returns the Accessed status of the PTE and resets it at the same time. */

		/*

		 * Capture the dirty status of the page, so that it doesn't get

		 * lost when the SPTE is marked for access tracking.

		/*

		 * Prevent page table teardown by making any free-er wait during

		 * kvm_flush_remote_tlbs() IPI to all active vcpus.

		/*

		 * Make sure a following spte read is not reordered ahead of the write

		 * to vcpu->mode.

		/*

		 * Make sure the write to vcpu->mode is not reordered in front of

		 * reads to sptes.  If it does, kvm_mmu_commit_zap_page() can see us

		 * OUTSIDE_GUEST_MODE and proceed to free the shadow page table.

 1 rmap, 1 parent PTE per level, and the prefetched rmaps. */

/*

 * Return the pointer to the large page information for a given gfn,

 * handling slots that are not large page aligned.

 the non-leaf shadow pages are keeping readonly. */

/*

 * About rmap_head encoding:

 *

 * If the bit zero of rmap_head->val is clear, then it points to the only spte

 * in this rmap chain. Otherwise, (rmap_head->val & ~1) points to a struct

 * pte_list_desc containing more mappings.

/*

 * Returns the number of pointers in the rmap chain, not counting the new one.

 Return true if rmap existed, false otherwise */

 rmap_head is meaningless now, remember to reset it */

	/*

	 * Unlike rmap_add, rmap_remove does not run in the context of a vCPU

	 * so we have to determine which memslots to use based on context

	 * information in sp->role.

/*

 * Used by the following functions to iterate through the sptes linked by a

 * rmap.  All fields are private and not assumed to be used outside.

 private fields */

 holds the sptep if not NULL */

 index of the sptep */

/*

 * Iteration must be started by this function.  This should also be used after

 * removing/dropping sptes from the rmap link because in such cases the

 * information in the iterator may not be valid.

 *

 * Returns sptep if found, NULL otherwise.

/*

 * Must be used with a valid iterator: e.g. after rmap_get_first().

 *

 * Returns sptep if found, NULL otherwise.

 desc->sptes[0] cannot be NULL */

/*

 * Write-protect on the specified @sptep, @pt_protect indicates whether

 * spte write-protection is caused by protecting shadow page table.

 *

 * Note: write protection is difference between dirty logging and spte

 * protection:

 * - for dirty logging, the spte can be set to writable at anytime if

 *   its dirty bitmap is properly set.

 * - for spte protection, the spte can be writable only after unsync-ing

 *   shadow page.

 *

 * Return true if tlb need be flushed.

/*

 * Gets the GFN ready for another round of dirty logging by clearing the

 *	- D bit on ad-enabled SPTEs, and

 *	- W bit on ad-disabled SPTEs.

 * Returns true iff any D or W bits were cleared.

/**

 * kvm_mmu_write_protect_pt_masked - write protect selected PT level pages

 * @kvm: kvm instance

 * @slot: slot to protect

 * @gfn_offset: start of the BITS_PER_LONG pages we care about

 * @mask: indicates which pages we should protect

 *

 * Used when we do not need to care about huge page mappings.

 clear the first set bit */

/**

 * kvm_mmu_clear_dirty_pt_masked - clear MMU D-bit for PT level pages, or write

 * protect the page if the D-bit isn't supported.

 * @kvm: kvm instance

 * @slot: slot to clear D-bit

 * @gfn_offset: start of the BITS_PER_LONG pages we care about

 * @mask: indicates which pages we should clear D-bit

 *

 * Used for PML to re-log the dirty GPAs after userspace querying dirty_bitmap.

 clear the first set bit */

/**

 * kvm_arch_mmu_enable_log_dirty_pt_masked - enable dirty logging for selected

 * PT level pages.

 *

 * It calls kvm_mmu_write_protect_pt_masked to write protect selected pages to

 * enable dirty logging for them.

 *

 * We need to care about huge page mappings: e.g. during dirty logging we may

 * have such mappings.

	/*

	 * Huge pages are NOT write protected when we start dirty logging in

	 * initially-all-set mode; must write protect them here so that they

	 * are split to 4K on the first write.

	 *

	 * The gfn_offset is guaranteed to be aligned to 64, but the base_gfn

	 * of memslot has no such restriction, so the range can cross two large

	 * pages.

 Cross two large pages? */

 Now handle 4K PTEs.  */

 input fields. */

 output fields. */

 private field. */

/*

 * This value is the sum of all of the kvm instances's

 * kvm->arch.n_used_mmu_pages values.  We need a global,

 * aggregate version in order to make the slab shrinker

 * faster

	/*

	 * active_mmu_pages must be a FIFO list, as kvm_zap_obsolete_pages()

	 * depends on valid pages being added to the head of the list.  See

	 * comments in kvm_zap_obsolete_pages().

	/* Also set up a sentinel.  Further entries in pvec are all

	 * children of sp, so this element is never overwritten.

			/*

			 * If the guest is creating an upper-level page, zap

			 * unsync pages for the same gfn.  While it's possible

			 * the guest is using recursive page tables, in all

			 * likelihood the guest has stopped using the unsync

			 * page and is installing a completely unrelated page.

			 * Unsync pages must not be left as is, because the new

			 * upper-level page will be write-protected.

			/*

			 * The page is good, but is stale.  kvm_sync_page does

			 * get the latest guest state, but (unlike mmu_unsync_children)

			 * it doesn't write-protect the page or mark it synchronized!

			 * This way the validity of the mapping is ensured, but the

			 * overhead of write protection is not incurred until the

			 * guest invalidates the TLB mapping.  This allows multiple

			 * SPs for a single gfn to be unsync.

			 *

			 * If the sync fails, the page is zapped.  If so, break

			 * in order to rebuild it.

		/*

		 * prev_root is currently only used for 64-bit hosts. So only

		 * the active root_hpa is valid here.

		/*

		 * For the direct sp, if the guest pte's dirty bit

		 * changed form clean to dirty, it will corrupt the

		 * sp's access: allow writable in the read-only sp,

		 * so we should update the spte at this point to get

		 * a new sp with the correct access.

 Returns the number of zapped non-leaf child shadow pages. */

			/*

			 * Recursively zap nested TDP SPs, parentless SPs are

			 * unlikely to be used again in the near future.  This

			 * avoids retaining a large number of stale nested SPs.

 Zapping children means active_mmu_pages has become unstable. */

 Count self */

		/*

		 * Already invalid pages (previously active roots) are not on

		 * the active page list.  See list_del() in the "else" case of

		 * !sp->root_count.

		/*

		 * Remove the active root from the active page list, the root

		 * will be explicitly freed when the root_count hits zero.

		/*

		 * Obsolete pages cannot be used on any vCPUs, see the comment

		 * in kvm_mmu_zap_all_fast().  Note, is_obsolete_sp() also

		 * treats invalid shadow pages as being obsolete.

	/*

	 * We need to make sure everyone sees our modifications to

	 * the page tables and see changes to vcpu->mode here. The barrier

	 * in the kvm_flush_remote_tlbs() achieves this. This pairs

	 * with vcpu_enter_guest and walk_shadow_page_lockless_begin/end.

	 *

	 * In addition, kvm_flush_remote_tlbs waits for all vcpus to exit

	 * guest mode and/or lockless shadow page table walks.

		/*

		 * Don't zap active root pages, the page itself can't be freed

		 * and zapping it will just force vCPUs to realloc and reload.

	/*

	 * Note, this check is intentionally soft, it only guarantees that one

	 * page is available, while the caller may end up allocating as many as

	 * four pages, e.g. for PAE roots or for 5-level paging.  Temporarily

	 * exceeding the (arbitrary by default) limit will not harm the host,

	 * being too aggressive may unnecessarily kill the guest, and getting an

	 * exact count is far more trouble than it's worth, especially in the

	 * page fault paths.

/*

 * Changing the number of mmu pages allocated to the vm

 * Note: if goal_nr_mmu_pages is too small, you will get dead lock

/*

 * Attempt to unsync any shadow pages that can be reached by the specified gfn,

 * KVM is creating a writable mapping for said gfn.  Returns 0 if all pages

 * were marked unsync (or if there is no shadow page), -EPERM if the SPTE must

 * be write-protected.

	/*

	 * Force write-protection if the page is being tracked.  Note, the page

	 * track machinery is used to write-protect upper-level shadow pages,

	 * i.e. this guards the role.level == 4K assertion below!

	/*

	 * The page is not write-tracked, mark existing shadow pages unsync

	 * unless KVM is synchronizing an unsync SP (can_unsync = false).  In

	 * that case, KVM must complete emulation of the guest TLB flush before

	 * allowing shadow pages to become unsync (writable by the guest).

		/*

		 * TDP MMU page faults require an additional spinlock as they

		 * run with mmu_lock held for read, not write, and the unsync

		 * logic is not thread safe.  Take the spinklock regardless of

		 * the MMU type to avoid extra conditionals/parameters, there's

		 * no meaningful penalty if mmu_lock is held for write.

			/*

			 * Recheck after taking the spinlock, a different vCPU

			 * may have since marked the page unsync.  A false

			 * positive on the unprotected check above is not

			 * possible as clearing sp->unsync _must_ hold mmu_lock

			 * for write, i.e. unsync cannot transition from 0->1

			 * while this CPU holds mmu_lock for read (or write).

	/*

	 * We need to ensure that the marking of unsync pages is visible

	 * before the SPTE is updated to allow writes because

	 * kvm_mmu_sync_roots() checks the unsync flags without holding

	 * the MMU lock and so can race with this. If the SPTE was updated

	 * before the page had been marked as unsync-ed, something like the

	 * following could happen:

	 *

	 * CPU 1                    CPU 2

	 * ---------------------------------------------------------------------

	 * 1.2 Host updates SPTE

	 *     to be writable

	 *                      2.1 Guest writes a GPTE for GVA X.

	 *                          (GPTE being in the guest page table shadowed

	 *                           by the SP from CPU 1.)

	 *                          This reads SPTE during the page table walk.

	 *                          Since SPTE.W is read as 1, there is no

	 *                          fault.

	 *

	 *                      2.2 Guest issues TLB flush.

	 *                          That causes a VM Exit.

	 *

	 *                      2.3 Walking of unsync pages sees sp->unsync is

	 *                          false and skips the page.

	 *

	 *                      2.4 Guest accesses GVA X.

	 *                          Since the mapping in the SP was not updated,

	 *                          so the old mapping for GVA X incorrectly

	 *                          gets used.

	 * 1.1 Host marks SP

	 *     as unsync

	 *     (sp->unsync = true)

	 *

	 * The write barrier below ensures that 1.1 happens before 1.2 and thus

	 * the situation in 2.4 does not arise.  It pairs with the read barrier

	 * in is_unsync_root(), placed between 2.1's load of SPTE.W and 2.3.

 Prefetching always gets a writable pfn.  */

		/*

		 * If we overwrite a PTE page pointer with a 2MB PMD, unlink

		 * the parent of the now unreachable PTE.

	/*

	 * Without accessed bits, there's no way to distinguish between

	 * actually accessed translations and prefetched, so disable pte

	 * prefetch if accessed bits aren't available.

	/*

	 * If addresses are being invalidated, skip prefetching to avoid

	 * accidentally prefetching those addresses.

	/*

	 * Note, using the already-retrieved memslot and __gfn_to_hva_memslot()

	 * is not solely for performance, it's also necessary to avoid the

	 * "writable" check in __gfn_to_hva_many(), which will always fail on

	 * read-only memslots due to gfn_to_hva() assuming writes.  Earlier

	 * page fault steps have already verified the guest isn't writing a

	 * read-only memslot.

	/*

	 * Enforce the iTLB multihit workaround after capturing the requested

	 * level, which will be used to do precise, accurate accounting.

	/*

	 * mmu_notifier_retry() was successful and mmu_lock is held, so

	 * the pmd can't be split from under us.

		/*

		 * A small SPTE exists for this pfn, but FNAME(fetch)

		 * and __direct_map would like to create a large PTE

		 * instead: just force them to go down another level,

		 * patching back for them into pfn the next 9 bits of

		 * the address.

		/*

		 * We cannot overwrite existing page tables with an NX

		 * large page, as the leaf could be executable.

	/*

	 * Do not cache the mmio info caused by writing the readonly gfn

	 * into the spte otherwise read access on readonly gfn also can

	 * caused mmio page fault and treat it as mmio access.

 The pfn is invalid, report the error! */

		/*

		 * If MMIO caching is disabled, emulate immediately without

		 * touching the shadow page tables as attempting to install an

		 * MMIO SPTE will just be an expensive nop.

	/*

	 * Do not fix the mmio spte with invalid generation number which

	 * need to be updated by slow page fault path.

 See if the page fault is due to an NX violation */

	/*

	 * #PF can be fast if:

	 * 1. The shadow page table entry is not present, which could mean that

	 *    the fault is potentially caused by access tracking (if enabled).

	 * 2. The shadow page table entry is present and the fault

	 *    is caused by write-protect, that means we just need change the W

	 *    bit of the spte which can be done out of mmu-lock.

	 *

	 * However, if access tracking is disabled we know that a non-present

	 * page must be a genuine page fault where we have to create a new SPTE.

	 * So, if access tracking is disabled, we return true only for write

	 * accesses to a present page.

/*

 * Returns true if the SPTE was fixed successfully. Otherwise,

 * someone else modified the SPTE from its original value.

	/*

	 * Theoretically we could also set dirty bit (and flush TLB) here in

	 * order to eliminate unnecessary PML logging. See comments in

	 * set_spte. But fast_page_fault is very unlikely to happen with PML

	 * enabled, so we do not do this. This might result in the same GPA

	 * to be logged in PML buffer again when the write really happens, and

	 * eventually to be called by mark_page_dirty twice. But it's also no

	 * harm. This also avoids the TLB flush needed after setting dirty bit

	 * so non-PML cases won't be impacted.

	 *

	 * Compare with set_spte where instead shadow_dirty_mask is set.

 Fault was on Read access */

/*

 * Returns the last level spte pointer of the shadow page walk for the given

 * gpa, and sets *spte to the spte value. This spte may be non-preset. If no

 * walk could be performed, returns NULL and *spte does not contain valid data.

 *

 * Contract:

 *  - Must be called between walk_shadow_page_lockless_{begin,end}.

 *  - The returned sptep must not be used after walk_shadow_page_lockless_end.

/*

 * Returns one of RET_PF_INVALID, RET_PF_FIXED or RET_PF_SPURIOUS.

		/*

		 * Check whether the memory access that caused the fault would

		 * still cause it if it were to be performed right now. If not,

		 * then this is a spurious fault caused by TLB lazily flushed,

		 * or some other CPU has already fixed the PTE after the

		 * current CPU took the fault.

		 *

		 * Need not check the access of upper level table entries since

		 * they are always ACC_ALL.

		/*

		 * Currently, to simplify the code, write-protection can

		 * be removed in the fast path only if the SPTE was

		 * write-protected for dirty-logging or access tracking.

			/*

			 * Do not fix write-permission on the large spte when

			 * dirty logging is enabled. Since we only dirty the

			 * first page into the dirty-bitmap in

			 * fast_pf_fix_direct_spte(), other pages are missed

			 * if its slot has dirty logging enabled.

			 *

			 * Instead, we let the slow page fault path create a

			 * normal spte to fix the access.

 Verify that the fault can be handled in the fast path */

		/*

		 * Currently, fast page fault only works for direct mapping

		 * since the gfn is not stable for indirect shadow page. See

		 * Documentation/virt/kvm/locking.rst to get more detail.

 roots_to_free must be some combination of the KVM_MMU_ROOT_* flags */

 Before acquiring the MMU lock, see if we need to do any real work. */

	/*

	 * This should not be called while L2 is active, L2 can't invalidate

	 * _only_ its own roots, e.g. INVVPID unconditionally exits.

 root_pgd is ignored for direct MMUs. */

	/*

	 * Check if this is the first shadow root being allocated before

	 * taking the lock.

 Recheck, under the lock, whether this is the first shadow root. */

	/*

	 * Check if anything actually needs to be allocated, e.g. all metadata

	 * will be allocated upfront if TDP is disabled.

			/*

			 * Both of these functions are no-ops if the target is

			 * already allocated, so unconditionally calling both

			 * is safe.  Intentionally do NOT free allocations on

			 * failure to avoid having to track which allocations

			 * were made now versus when the memslot was created.

			 * The metadata is guaranteed to be freed when the slot

			 * is freed, and will be kept/used if userspace retries

			 * KVM_RUN instead of killing the VM.

	/*

	 * Ensure that shadow_root_allocated becomes true strictly after

	 * all the related pointers are set.

	/*

	 * On SVM, reading PDPTRs might access guest memory, which might fault

	 * and thus might sleep.  Grab the PDPTRs before acquiring mmu_lock.

	/*

	 * Do we shadow a long mode page table? If so we need to

	 * write-protect the guests page table root.

	/*

	 * We shadow a 32 bit page table. This may be a legacy 2-level

	 * or a PAE 3-level page table. In either case we need to be aware that

	 * the shadow page table may be a PAE or a long mode page table.

	/*

	 * When shadowing 32-bit or PAE NPT with 64-bit NPT, the PML4 and PDP

	 * tables are allocated and initialized at root creation as there is no

	 * equivalent level in the guest's NPT to shadow.  Allocate the tables

	 * on demand, as running a 32-bit L1 VMM on 64-bit KVM is very rare.

	/*

	 * NPT, the only paging mode that uses this horror, uses a fixed number

	 * of levels for the shadow page tables, e.g. all MMUs are 4-level or

	 * all MMus are 5-level.  Thus, this can safely require that pml5_root

	 * is allocated if the other roots are valid and pml5 is needed, as any

	 * prior MMU would also have required pml5.

	/*

	 * The special roots should always be allocated in concert.  Yell and

	 * bail if KVM ends up in a state where only one of the roots is valid.

	/*

	 * Unlike 32-bit NPT, the PDP table doesn't need to be in low mem, and

	 * doesn't need to be decrypted.

	/*

	 * The read barrier orders the CPU's read of SPTE.W during the page table

	 * walk before the reads of sp->unsync/sp->unsync_children here.

	 *

	 * Even if another CPU was marking the SP as unsync-ed simultaneously,

	 * any guest page table changes are not guaranteed to be visible anyway

	 * until this VCPU issues a TLB flush strictly after those changes are

	 * made.  We only need to ensure that the other CPU sets these flags

	 * before any actual changes to the page tables are made.  The comments

	 * in mmu_try_to_unsync_pages() describe what could go wrong if this

	 * requirement isn't satisfied.

 sync prev_roots by simply freeing them */

	/*

	 * A nested guest cannot use the MMIO cache if it is using nested

	 * page tables, because cr2 is a nGPA while the cache stores GPAs.

/*

 * Return the level of the lowest level SPTE added to sptes.

 * That SPTE may be non-present.

 *

 * Must be called between walk_shadow_page_lockless_{begin,end}.

 return true if reserved bit(s) are detected on a valid, non-MMIO SPTE. */

	/*

	 * Skip reserved bits checks on the terminal leaf if it's not a valid

	 * SPTE.  Note, this also (intentionally) skips MMIO SPTEs, which, by

	 * design, always have reserved bits set.  The purpose of the checks is

	 * to detect reserved bits on non-MMIO SPTEs. i.e. buggy SPTEs.

	/*

	 * If the page table is zapped by other cpus, let CPU fault again on

	 * the address.

	/*

	 * guest is writing the page which is write tracked which can

	 * not be fixed by page fault handler.

	/*

	 * Retry the page fault if the gfn hit a memslot that is being deleted

	 * or moved.  This ensures any existing SPTEs for the old memslot will

	 * be zapped before KVM inserts a new MMIO SPTE for the gfn.

 Don't expose private memslots to L2. */

		/*

		 * If the APIC access page exists but is disabled, go directly

		 * to emulation without caching the MMIO access or creating a

		 * MMIO SPTE.  That way the cache doesn't need to be purged

		 * when the AVIC is re-enabled.

 *pfn has correct page already */

 This path builds a PAE pagetable, we can map 2mb pages at maximum. */

 A 64-bit CR2 should be impossible on 32-bit KVM. */

/*

 * Find out if a previously cached root matching the new pgd/role is available.

 * The current root is also inserted into the cache.

 * If a matching root was found, it is assigned to kvm_mmu->root_hpa and true is

 * returned.

 * Otherwise, the LRU root from the cache is assigned to kvm_mmu->root_hpa and

 * false is returned. This root should now be freed by the caller.

	/*

	 * For now, limit the fast switch to 64-bit hosts+VMs in order to avoid

	 * having to deal with PDPTEs. We may add support for 32-bit hosts/VMs

	 * later if necessary.

	/*

	 * It's possible that the cached previous root page is obsolete because

	 * of a change in the MMU generation number. However, changing the

	 * generation number is accompanied by KVM_REQ_MMU_RELOAD, which will

	 * free the root set here and allocate a new one.

	/*

	 * The last MMIO access's GVA and GPA are cached in the VCPU. When

	 * switching to a new CR3, that GVA->GPA mapping may no longer be

	 * valid. So clear any cached MMIO info even when we don't need to sync

	 * the shadow page tables.

	/*

	 * If this is a direct root page, it doesn't have a write flooding

	 * count. Otherwise, clear the write flooding count.

 arbitrary */

 Note, NX doesn't exist in PDPTEs, this is handled below. */

	/*

	 * Non-leaf PML4Es and PDPEs reserve bit 8 (which would be the G bit for

	 * leaf entries) on AMD CPUs only.

 no rsvd bits for 2 level 4K page table entries */

 36bits PSE 4MB page */

 32 bits PSE 4MB page */

 PDPTE */

 PDE */

 PTE */

 large page */

 large page */

	/*

	 * If TDP is enabled, let the guest use GBPAGES if they're supported in

	 * hardware.  The hardware page walker doesn't let KVM disable GBPAGES,

	 * i.e. won't treat them as reserved, and KVM doesn't redo the GVA->GPA

	 * walk for performance and complexity reasons.  Not to mention KVM

	 * _can't_ solve the problem because GVA->GPA walks aren't visible to

	 * KVM once a TDP translation is installed.  Mimic hardware behavior so

	 * that KVM's is at least consistent, i.e. doesn't randomly inject #PF.

 large page */

 bits 3..5 must not be 2 */

 bits 3..5 must not be 3 */

 bits 3..5 must not be 7 */

 bits 0..2 must not be 010 */

 bits 0..2 must not be 110 */

 bits 0..2 must not be 100 unless VMX capabilities allow it */

/*

 * the page table on host is the shadow page table for the page

 * table in guest or amd nested guest, its mmu features completely

 * follow the features in guest.

	/*

	 * KVM uses NX when TDP is disabled to handle a variety of scenarios,

	 * notably for huge SPTEs if iTLB multi-hit mitigation is enabled and

	 * to generate correct permissions for CR0.WP=0/CR4.SMEP=1/EFER.NX=0.

	 * The iTLB multi-hit workaround can be toggled at any time, so assume

	 * NX can be used by any non-nested shadow MMU to avoid having to reset

	 * MMU contexts.  Note, KVM forces EFER.NX=1 when TDP is disabled.

 @amd adds a check on bit of SPTEs, which KVM shouldn't use anyways. */

 KVM doesn't use 2-level page tables for the shadow MMU. */

/*

 * the direct page table on host, use as much mmu features as

 * possible, however, kvm currently does not do execution-protection.

/*

 * as the comments in reset_shadow_zero_bits_mask() except it

 * is the shadow page table for intel nested guest.

		/*

		 * Each "*f" variable has a 1 bit for each UWX value

		 * that causes a fault with the given PFEC.

 Faults from writes to non-writable pages */

 Faults from user mode accesses to supervisor pages */

 Faults from fetches of non-executable pages*/

 Faults from kernel mode fetches of user pages */

 Faults from kernel mode accesses of user pages */

 Faults from kernel mode accesses to user pages */

 Not really needed: !nx will cause pte.nx to fault */

 Allow supervisor writes if !cr0.wp */

 Disallow supervisor fetches of user code if cr4.smep */

			/*

			 * SMAP:kernel-mode data accesses from user-mode

			 * mappings should fault. A fault is considered

			 * as a SMAP violation if all of the following

			 * conditions are true:

			 *   - X86_CR4_SMAP is set in CR4

			 *   - A user page is accessed

			 *   - The access is not a fetch

			 *   - Page fault in kernel mode

			 *   - if CPL = 3 or X86_EFLAGS_AC is clear

			 *

			 * Here, we cover the first three conditions.

			 * The fourth is computed dynamically in permission_fault();

			 * PFERR_RSVD_MASK bit will be set in PFEC if the access is

			 * *not* subject to SMAP restrictions.

/*

* PKU is an additional mechanism by which the paging controls access to

* user-mode addresses based on the value in the PKRU register.  Protection

* key violations are reported through a bit in the page fault error code.

* Unlike other bits of the error code, the PK bit is not known at the

* call site of e.g. gva_to_gpa; it must be computed directly in

* permission_fault based on two bits of PKRU, on some machine state (CR4,

* CR0, EFER, CPL), and on other bits of the error code and the page tables.

*

* In particular the following conditions come from the error code, the

* page tables and the machine state:

* - PK is always zero unless CR4.PKE=1 and EFER.LMA=1

* - PK is always zero if RSVD=1 (reserved bit set) or F=1 (instruction fetch)

* - PK is always zero if U=0 in the page tables

* - PKRU.WD is ignored if CR0.WP=0 and the access is a supervisor access.

*

* The PKRU bitmask caches the result of these four conditions.  The error

* code (minus the P bit) and the page table's U bit form an index into the

* PKRU bitmask.  Two bits of the PKRU bitmask are then extracted and ANDed

* with the two bits of the PKRU register corresponding to the protection key.

* For the first three conditions above the bits will be 00, thus masking

* away both AD and WD.  For all reads or if the last condition holds, WD

* only will be masked away.

 PFEC.RSVD is replaced by ACC_USER_MASK. */

		/*

		 * Only need to check the access which is not an

		 * instruction fetch and is to a user page.

		/*

		 * write access is controlled by PKRU if it is a

		 * user access or CR0.WP = 1.

 PKRU.AD stops both read and write access. */

 PKRU.WD stops write access. */

 PKEY and LA57 are active iff long mode is active. */

 tdp_root_level is architecture forced level, use it if nonzero */

 Use 5-level TDP if and only if it's useful/necessary. */

 SMM flag is inherited from root_mmu */

 EPT, and thus nested EPT, does not consume CR0, CR4, nor EFER. */

	/*

	 * Nested MMUs are used only for walking L2's gva->gpa, they never have

	 * shadow pages of their own and so "direct" has no meaning.   Set it

	 * to "true" to try to detect bogus usage of the nested MMU.

	/*

	 * L2 page tables are never shadowed, so there is no need to sync

	 * SPTEs.

	/*

	 * Note that arch.mmu->gva_to_gpa translates l2_gpa to l1_gpa using

	 * L1's nested page tables (e.g. EPT12). The nested translation

	 * of l2_gva to l1_gpa is done by arch.nested_mmu.gva_to_gpa using

	 * L2's page tables as the first level of translation and L1's

	 * nested page tables as the second level of translation. Basically

	 * the gva_to_gpa functions between mmu and nested_mmu are swapped.

	/*

	 * Invalidate all MMU roles to force them to reinitialize as CPUID

	 * information is factored into reserved bit calculations.

	/*

	 * KVM does not correctly handle changing guest CPUID after KVM_RUN, as

	 * MAXPHYADDR, GBPAGES support, AMD reserved bit behavior, etc.. aren't

	 * tracked in kvm_mmu_page_role.  As a result, KVM may miss guest page

	 * faults due to reusing SPs/SPTEs.  Alert userspace, but otherwise

	 * sweep the problem under the rug.

	 *

	 * KVM's horrific CPUID ABI makes the problem all but impossible to

	 * solve, as correctly handling multiple vCPU models (with respect to

	 * paging and physical address properties) in a single VM would require

	 * tracking all relevant CPUID information in kvm_mmu_page_role.  That

	 * is very undesirable as it would double the memory requirements for

	 * gfn_track (see struct kvm_mmu_page_role comments), and in practice

	 * no sane VMM mucks with the core vCPU model on the fly.

	/*

	 * Assume that the pte write on a page table of the same type

	 * as the current vcpu paging mode since we update the sptes only

	 * when they have the same mode.

 Handle a 32-bit guest writing two halves of a 64-bit gpte */

/*

 * If we're seeing too many writes to a page, it may no longer be a page table,

 * or we may be forking, in which case it is better to unmap the page.

	/*

	 * Skip write-flooding detected for the sp whose level is 1, because

	 * it can become unsync, then the guest page is not write-protected.

/*

 * Misaligned accesses are too much trouble to fix up; also, they usually

 * indicate a page is not used as a page table.

	/*

	 * Sometimes, the OS only writes the last one bytes to update status

	 * bits, for example, in linux, andb instruction is used in clear_bit().

 32->64 */

		/*

		 * A 32-bit pde maps 4MB while the shadow pdes map

		 * only 2MB.  So we need to double the offset again

		 * and zap two pdes instead of one.

 kill rounding error */

	/*

	 * If we don't have indirect shadow pages, it means no page is

	 * write-protected, so we can exit simply.

	/*

	 * No need to care whether allocation memory is successful

	 * or not since pte prefetch is skipped if it does not have

	 * enough objects in the cache.

	/*

	 * Before emulating the instruction, check if the error code

	 * was due to a RO violation while translating the guest page.

	 * This can occur when using nested virtualization with nested

	 * paging in both guests. If true, we simply unprotect the page

	 * and resume the guest.

	/*

	 * vcpu->arch.mmu.page_fault returned RET_PF_EMULATE, but we can still

	 * optimistically try to just unprotect the page and let the processor

	 * re-execute the instruction that caused the page fault.  Do not allow

	 * retrying MMIO emulation, as it's not only pointless but could also

	 * cause us to enter an infinite loop because the processor will keep

	 * faulting on the non-existent MMIO address.  Retrying an instruction

	 * from a nested guest is also pointless and dangerous as we are only

	 * explicitly shadowing L1's page tables, i.e. unprotecting something

	 * for L1 isn't going to magically fix whatever issue cause L2 to fail.

 It's actually a GPA for vcpu->arch.guest_mmu.  */

 INVLPG on a non-canonical address is a NOP according to the SDM.  */

		/*

		 * INVLPG is required to invalidate any global mappings for the VA,

		 * irrespective of PCID. Since it would take us roughly similar amount

		 * of work to determine whether any of the prev_root mappings of the VA

		 * is marked global, or to just sync it blindly, so we might as well

		 * just always sync it.

		 *

		 * Mappings not reachable via the current cr3 or the prev_roots will be

		 * synced when switching to that cr3, so nothing needs to be done here

		 * for them.

	/*

	 * Mappings not reachable via the current cr3 or the prev_roots will be

	 * synced when switching to that cr3, so nothing needs to be done here

	 * for them.

	/*

	 * max_huge_page_level reflects KVM's MMU capabilities irrespective

	 * of kernel support, e.g. KVM may be capable of using 1GB pages when

	 * the kernel is not.  But, KVM never creates a page size greater than

	 * what is used by the kernel for any given HVA, i.e. the kernel's

	 * capabilities are ultimately consulted by kvm_mmu_hugepage_adjust().

 The return value indicates if tlb flush on all vcpus is needed. */

 The caller should hold mmu-lock before calling this function. */

	/*

	 * When using PAE paging, the four PDPTEs are treated as 'root' pages,

	 * while the PDP table is a per-vCPU construct that's allocated at MMU

	 * creation.  When emulating 32-bit mode, cr3 is only 32 bits even on

	 * x86_64.  Therefore we need to allocate the PDP table in the first

	 * 4GB of memory, which happens to fit the DMA32 zone.  TDP paging

	 * generally doesn't use PAE paging and can skip allocating the PDP

	 * table.  The main exception, handled here, is SVM's 32-bit NPT.  The

	 * other exception is for shadowing L1's 32-bit or PAE NPT on 64-bit

	 * KVM; that horror is handled on-demand by mmu_alloc_shadow_roots().

	/*

	 * CR3 is only 32 bits when PAE paging is used, thus it's impossible to

	 * get the CPU to treat the PDPTEs as encrypted.  Decrypt the page so

	 * that KVM's writes and the CPU's reads get along.  Note, this is

	 * only necessary when using shadow paging, as 64-bit NPT can get at

	 * the C-bit even when shadowing 32-bit NPT, and SME isn't supported

	 * by 32-bit kernels (when KVM itself uses 32-bit NPT).

		/*

		 * No obsolete valid page exists before a newly created page

		 * since active_mmu_pages is a FIFO list.

		/*

		 * Invalid pages should never land back on the list of active

		 * pages.  Skip the bogus page, otherwise we'll get stuck in an

		 * infinite loop if the page gets put back on the list (again).

		/*

		 * No need to flush the TLB since we're only zapping shadow

		 * pages with an obsolete generation number and all vCPUS have

		 * loaded a new root, i.e. the shadow pages being zapped cannot

		 * be in active use by the guest.

	/*

	 * Trigger a remote TLB flush before freeing the page tables to ensure

	 * KVM is not in the middle of a lockless shadow page table walk, which

	 * may reference the pages.

/*

 * Fast invalidate all shadow pages and use lock-break technique

 * to zap obsolete pages.

 *

 * It's required when memslot is being deleted or VM is being

 * destroyed, in these cases, we should ensure that KVM MMU does

 * not use any resource of the being-deleted slot or all slots

 * after calling the function.

	/*

	 * Toggle mmu_valid_gen between '0' and '1'.  Because slots_lock is

	 * held for the entire duration of zapping obsolete pages, it's

	 * impossible for there to be multiple invalid generations associated

	 * with *valid* shadow pages at any given time, i.e. there is exactly

	 * one valid generation and (at most) one invalid generation.

	/* In order to ensure all threads see this change when

	 * handling the MMU reload signal, this must happen in the

	 * same critical section as kvm_reload_remote_mmus, and

	 * before kvm_zap_obsolete_pages as kvm_zap_obsolete_pages

	 * could drop the MMU lock and yield.

	/*

	 * Notify all vcpus to reload its shadow page table and flush TLB.

	 * Then all vcpus will switch to new shadow page table with the new

	 * mmu_valid_gen.

	 *

	 * Note: we need to do this under the protection of mmu_lock,

	 * otherwise, vcpu would purge shadow page but miss tlb flush.

/*

 * Invalidate (zap) SPTEs that cover GFNs from gfn_start and up to gfn_end

 * (not including it)

	/*

	 * We can flush all the TLBs out of the mmu lock without TLB

	 * corruption since we just change the spte from writable to

	 * readonly so that we only need to care the case of changing

	 * spte from present to present (changing the spte from present

	 * to nonpresent will flush all the TLBs immediately), in other

	 * words, the only case we care is mmu_spte_update() where we

	 * have checked Host-writable | MMU-writable instead of

	 * PT_WRITABLE_MASK, that means it does not depend on PT_WRITABLE_MASK

	 * anymore.

		/*

		 * We cannot do huge page mapping for indirect shadow pages,

		 * which are found on the last rmap (level = 1) when not using

		 * tdp; such shadow pages are synced with the page table in

		 * the guest, and the guest page table is using 4K page size

		 * mapping if the indirect sp has level = 1.

		/*

		 * Zap only 4k SPTEs since the legacy MMU only supports dirty

		 * logging at a 4k granularity and never creates collapsible

		 * 2m SPTEs during dirty logging.

	/*

	 * All current use cases for flushing the TLBs for a specific memslot

	 * related to dirty logging, and many do the TLB flush out of mmu_lock.

	 * The interaction between the various operations on memslot must be

	 * serialized by slots_locks to ensure the TLB flush from one operation

	 * is observed by any other operation on the same memslot.

		/*

		 * Clear dirty bits only on 4k SPTEs since the legacy MMU only

		 * support dirty logging at a 4k granularity.

	/*

	 * It's also safe to flush TLBs out of mmu lock here as currently this

	 * function is only used for dirty logging, in which case flushing TLB

	 * out of mmu lock also guarantees no dirty pages will be lost in

	 * dirty_bitmap.

	/*

	 * Generation numbers are incremented in multiples of the number of

	 * address spaces in order to provide unique generations across all

	 * address spaces.  Strip what is effectively the address space

	 * modifier prior to checking for a wrap of the MMIO generation so

	 * that a wrap in any address space is detected.

	/*

	 * The very rare case: if the MMIO generation number has wrapped,

	 * zap all shadow pages.

		/*

		 * Never scan more than sc->nr_to_scan VM instances.

		 * Will not hit this condition practically since we do not try

		 * to shrink more than one VM and it is very unlikely to see

		 * !n_used_mmu_pages so many times.

		/*

		 * n_used_mmu_pages is accessed without holding kvm->mmu_lock

		 * here. We may skip a VM instance errorneosly, but we do not

		 * want to shrink a VM that only started to populate its MMU

		 * anyway.

		/*

		 * unfair on small ones

		 * per-vm shrinkers cry out

		 * sadness comes quickly

 Return true when CPU has the bug, and mitigations are ON */

 In "auto" mode deploy workaround only if CPU has the bug. */

	/*

	 * MMU roles use union aliasing which is, generally speaking, an

	 * undefined behavior. However, we supposedly know how compilers behave

	 * and the current status quo is unlikely to change. Guardians below are

	 * supposed to let us know if the assumption becomes false.

/*

 * Calculate mmu pages needed for kvm.

		/*

		 * We use a separate list instead of just using active_mmu_pages

		 * because the number of lpage_disallowed pages is expected to

		 * be relatively small compared to the total.

 Make sure the period is not less than one second.  */

 SPDX-License-Identifier: GPL-2.0-only

/*

 * Kernel-based Virtual Machine driver for Linux

 *

 * Macros and functions to access KVM PTEs (also known as SPTEs)

 *

 * Copyright (C) 2006 Qumranet, Inc.

 * Copyright 2020 Red Hat, Inc. and/or its affiliates.

 mutual exclusive with nx_mask */

			/*

			 * Some reserved pages, such as those from NVDIMM

			 * DAX devices, are not for MMIO, and can be mapped

			 * with cached memory type for better performance.

			 * However, the above check misconceives those pages

			 * as MMIO, and results in KVM mapping them with UC

			 * memory type, which would hurt the performance.

			 * Therefore, we check the host memory type in addition

			 * and only treat UC/UC-/WC pages as MMIO.

	/*

	 * For the EPT case, shadow_present_mask is 0 if hardware

	 * supports exec-only page table entries.  In that case,

	 * ACC_USER_MASK and shadow_user_mask are used to represent

	 * read access.  See FNAME(gpte_access) in paging_tmpl.h.

		/*

		 * Optimization: for pte sync, if spte was writable the hash

		 * lookup is unnecessary (and expensive). Write protection

		 * is responsibility of kvm_mmu_get_page / kvm_mmu_sync_roots.

		 * Same reasoning can be applied to dirty page accounting.

		/*

		 * Unsync shadow pages that are reachable by the new, writable

		 * SPTE.  Write-protect the SPTE if the page can't be unsync'd,

		 * e.g. it's write-tracked (upper-level SPs) or has one or more

		 * shadow pages and unsync'ing pages is not allowed.

 Enforced by kvm_mmu_hugepage_adjust. */

	/*

	 * boot_cpu_data.x86_phys_bits is reduced when MKTME or SME are detected

	 * in CPU detection code, but the processor treats those reduced bits as

	 * 'keyID' thus they are not reserved bits. Therefore KVM needs to look at

	 * the physical address bits reported by CPUID.

	/*

	 * Quite weird to have VMX or SVM but not MAXPHYADDR; probably a VM with

	 * custom CPUID.  Proceed with whatever the kernel found since these features

	 * aren't virtualizable (SME/SEV also require CPUIDs higher than 0x80000008).

	/*

	 * Making an Access Tracking PTE will result in removal of write access

	 * from the PTE. So, verify that we will be able to restore the write

	 * access in the fast page fault path later on.

	/*

	 * Disable MMIO caching if the MMIO value collides with the bits that

	 * are used to hold the relocated GFN when the L1TF mitigation is

	 * enabled.  This should never fire as there is no known hardware that

	 * can trigger this condition, e.g. SME/SEV CPUs that require a custom

	 * MMIO value are not susceptible to L1TF.

	/*

	 * The masked MMIO value must obviously match itself and a removed SPTE

	 * must not get a false positive.  Removed SPTEs and MMIO SPTEs should

	 * never collide as MMIO must set some RWX bits, and removed SPTEs must

	 * not set any RWX bits.

	/*

	 * EPT Misconfigurations are generated if the value of bits 2:0

	 * of an EPT paging-structure entry is 110b (write/execute).

	/*

	 * If the CPU has 46 or less physical address bits, then set an

	 * appropriate mask to guard against L1TF attacks. Otherwise, it is

	 * assumed that the CPU is not vulnerable to L1TF.

	 *

	 * Some Intel CPUs address the L1 cache using more PA bits than are

	 * reported by CPUID. Use the PA width of the L1 cache when possible

	 * to achieve more effective mitigation, e.g. if system RAM overlaps

	 * the most significant bits of legal physical address space.

	/*

	 * Set a reserved PA bit in MMIO SPTEs to generate page faults with

	 * PFEC.RSVD=1 on MMIO accesses.  64-bit PTEs (PAE, x86-64, and EPT

	 * paging) support a maximum of 52 bits of PA, i.e. if the CPU supports

	 * 52-bit physical addresses then there are no reserved PA bits in the

	 * PTEs and so the reserved PA approach must be disabled.

 SPDX-License-Identifier: GPL-2.0

  Copyright(c) 2021 Intel Corporation. */

 Initial value of guest's virtual SGX_LEPUBKEYHASHn MSRs */

/*

 * ENCLS's memory operands use a fixed segment (DS) and a fixed

 * address size based on the mode.  Related prefixes are ignored.

 Skip vmcs.GUEST_DS retrieval for 64-bit mode to avoid VMREADs. */

	/*

	 * A non-EPCM #PF indicates a bad userspace HVA.  This *should* check

	 * for PFEC.SGX and not assume any #PF on SGX2 originated in the EPC,

	 * but the error code isn't (yet) plumbed through the ENCLS helpers.

	/*

	 * If the guest thinks it's running on SGX2 hardware, inject an SGX

	 * #PF if the fault matches an EPCM fault signature (#GP on SGX1,

	 * #PF on SGX2).  The assumption is that EPCM faults are much more

	 * likely than a bad userspace address.

 Enforce restriction of access to the PROVISIONKEY. */

 Enforce CPUID restrictions on MISCSELECT, ATTRIBUTES and XFRM. */

 Enforce CPUID restriction on max enclave size. */

	/*

	 * sgx_virt_ecreate() returns:

	 *  1) 0:	ECREATE was successful

	 *  2) -EFAULT:	ECREATE was run but faulted, and trapnr was set to the

	 *		exception number.

	 *  3) -EINVAL:	access_ok() on @secs_hva failed. This should never

	 *		happen as KVM checks host addresses at memslot creation.

	 *		sgx_virt_ecreate() has already warned in this case.

	/*

	 * Copy the PAGEINFO to local memory, its pointers need to be

	 * translated, i.e. we need to do a deep copy/translate.

	/*

	 * Translate the SECINFO, SOURCE and SECS pointers from GVA to GPA.

	 * Resume the guest on failure to inject a #PF.

	/*

	 * ...and then to HVA.  The order of accesses isn't architectural, i.e.

	 * KVM doesn't have to fully process one address at a time.  Exit to

	 * userspace if a GPA is invalid.

	/*

	 * Copy contents into kernel memory to prevent TOCTOU attack. E.g. the

	 * guest could do ECREATE w/ SECS.SGX_ATTR_PROVISIONKEY=0, and

	 * simultaneously set SGX_ATTR_PROVISIONKEY to bypass the check to

	 * enforce restriction of access to the PROVISIONKEY.

 Exit to userspace if copying from a host userspace address fails. */

	/*

	 * Translate the SIGSTRUCT, SECS and TOKEN pointers from GVA to GPA.

	 * Resume the guest on failure to inject a #PF.

	/*

	 * ...and then to HVA.  The order of accesses isn't architectural, i.e.

	 * KVM doesn't have to fully process one address at a time.  Exit to

	 * userspace if a GPA is invalid.  Note, all structures are aligned and

	 * cannot split pages.

	/*

	 * sgx_virt_einit() returns -EINVAL when access_ok() fails on @sig_hva,

	 * @token_hva or @secs_hva. This should never happen as KVM checks host

	 * addresses at memslot creation. sgx_virt_einit() has already warned

	 * in this case, so just return.

	/*

	 * Use Intel's default value for Skylake hardware if Launch Control is

	 * not supported, i.e. Intel's hash is hardcoded into silicon, or if

	 * Launch Control is supported and enabled, i.e. mimic the reset value

	 * and let the guest write the MSRs at will.  If Launch Control is

	 * supported but disabled, then use the current MSR values as the hash

	 * MSRs exist but are read-only (locked and not writable).

 MSR_IA32_SGXLEPUBKEYHASH0 is read above */

/*

 * ECREATE must be intercepted to enforce MISCSELECT, ATTRIBUTES and XFRM

 * restrictions if the guest's allowed-1 settings diverge from hardware.

	/*

	 * There is no software enable bit for SGX that is virtualized by

	 * hardware, e.g. there's no CR4.SGXE, so when SGX is disabled in the

	 * guest (either by the host or by the guest's BIOS) but enabled in the

	 * host, trap all ENCLS leafs and inject #UD/#GP as needed to emulate

	 * the expected system behavior for ENCLS.

 Nothing to do if hardware doesn't support SGX */

		/*

		 * Trap and execute EINIT if launch control is enabled in the

		 * host using the guest's values for launch control MSRs, even

		 * if the guest's values are fixed to hardware default values.

		 * The MSRs are not loaded/saved on VM-Enter/VM-Exit as writing

		 * the MSRs is extraordinarily expensive.

 SPDX-License-Identifier: GPL-2.0

 SPDX-License-Identifier: GPL-2.0-only

/*

 * KVM PMU support for Intel CPUs

 *

 * Copyright 2011 Red Hat, Inc. and/or its affiliates.

 *

 * Authors:

 *   Avi Kivity   <avi@redhat.com>

 *   Gleb Natapov <gleb@redhat.com>

 Index must match CPUID 0x0A.EBX bit vector */

 mapping between fixed pmc index and intel_arch_events array */

 function is called when global control register has been updated. */

 check if a PMC is enabled by comparing it with globl_ctrl bits. */

	/*

	 * As a first step, a guest could only enable LBR feature if its

	 * cpu model is the same as the host because the LBR registers

	 * would be pass-through to the guest and they're model specific.

	/*

	 * The perf_event_attr is constructed in the minimum efficient way:

	 * - set 'pinned = true' to make it task pinned so that if another

	 *   cpu pinned event reclaims LBR, the event->oncpu will be set to -1;

	 * - set '.exclude_host = true' to record guest branches behavior;

	 *

	 * - set '.config = INTEL_FIXED_VLBR_EVENT' to indicates host perf

	 *   schedule the event without a real HW counter but a fake one;

	 *   check is_guest_lbr_event() and __intel_get_event_constraints();

	 *

	 * - set 'sample_type = PERF_SAMPLE_BRANCH_STACK' and

	 *   'branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |

	 *   PERF_SAMPLE_BRANCH_USER' to configure it as a LBR callstack

	 *   event, which helps KVM to save/restore guest LBR records

	 *   during host context switches and reduces quite a lot overhead,

	 *   check branch_user_callstack() and intel_pmu_lbr_sched_task();

/*

 * It's safe to access LBR msrs from guest when they have not

 * been passthrough since the host would help restore or reset

 * the LBR msrs records when the guest LBR event is scheduled in.

	/*

	 * Disable irq to ensure the LBR feature doesn't get reclaimed by the

	 * host at the time the value is read from the msr, and this avoids the

	 * host LBR value to be leaked to the guest. If LBR has been reclaimed,

	 * return 0 on guest reads.

 RO MSR */

/*

 * Emulate LBR_On_PMI behavior for 1 < pmu.version < 4.

 *

 * If Freeze_LBR_On_PMI = 1, the LBR is frozen on PMI and

 * the KVM emulates to clear the LBR bit (bit 0) in IA32_DEBUGCTL.

 *

 * Guest needs to re-enable LBR to resume branches recording.

/*

 * Higher priority host perf events (e.g. cpu pinned) could reclaim the

 * pmu resources (e.g. LBR) that were assigned to the guest. This is

 * usually done via ipi calls (more details in perf_install_in_context).

 *

 * Before entering the non-root mode (with irq disabled here), double

 * confirm that the pmu features enabled to the guest are not reclaimed

 * by higher priority host events. Otherwise, disallow vcpu's access to

 * the reclaimed features.

 SPDX-License-Identifier: GPL-2.0

/*

 * Hyper-V requires all of these, so mark them as supported even though

 * they are just treated the same as all-context.

		/*

		 * PML and the preemption timer can be emulated, but the

		 * processor cannot vmwrite to fields that don't exist

		 * on bare metal.

/*

 * The following 3 functions, nested_vmx_succeed()/failValid()/failInvalid(),

 * set the success or error code of an emulated VMX instruction (as specified

 * by Vol 2B, VMX Instruction Reference, "Conventions"), and skip the emulated

 * instruction.

	/*

	 * We don't need to force sync to shadow VMCS because

	 * VM_INSTRUCTION_ERROR is not shadowed. Enlightened VMCS 'shadows' all

	 * fields and thus must be synced.

	/*

	 * failValid writes the error number to the current VMCS, which

	 * can't be done if there isn't a current VMCS.

 TODO: not to reset guest simply here. */

/*

 * Free whatever needs to be freed from vmx->nested when L1 goes down, or

 * just stops using VMX.

 Unpin physical memory we referred to in the vmcs02 */

/*

 * Ensure that the current vmcs of the logical processor is the

 * vmcs01 of the vcpu before calling free_nested().

		/*

		 * Although the caller (kvm_inject_emulated_page_fault) would

		 * have already synced the faulting address in the shadow EPT

		 * tables for the current EPTP12, we also need to sync it for

		 * any other cached EPTP02s based on the same EP4TA, since the

		 * TLB associates mappings to the EP4TA rather than the full EPTP.

/*

 * KVM wants to inject page-faults which it got to the guest. This function

 * checks whether in a nested guest, we need to inject them to L1 or L2.

/*

 * For x2APIC MSRs, ignore the vmcs01 bitmap.  L1 can enable x2APIC without L1

 * itself utilizing x2APIC.  All MSRs were previously set to be intercepted,

 * only the "disable intercept" case needs to be handled.

/*

 * Merge L0's and L1's MSR bitmap, return false to indicate that

 * we do not use the hardware.

 Nothing to do if the MSR bitmap is not in use.  */

	/*

	 * To keep the control flow simple, pay eight 8-byte writes (sixteen

	 * 4-byte writes on 32-bit systems) up front to enable intercepts for

	 * the x2APIC MSR range and selectively toggle those relevant to L2.

			/*

			 * L0 need not intercept reads for MSRs between 0x800

			 * and 0x8ff, it just lets the processor take the value

			 * from the virtual-APIC page; take those 256 bits

			 * directly from the L1 bitmap.

	/*

	 * Always check vmcs01's bitmap to honor userspace MSR filters and any

	 * other runtime changes to vmcs01's bitmap, e.g. dynamic pass-through.

/*

 * In nested virtualization, check if L1 has set

 * VM_EXIT_ACK_INTR_ON_EXIT

	/*

	 * If virtualize x2apic mode is enabled,

	 * virtualize apic access must be disabled.

	/*

	 * If virtual interrupt delivery is enabled,

	 * we must exit on external interrupts.

	/*

	 * bits 15:8 should be zero in posted_intr_nv,

	 * the descriptor address has been already checked

	 * in nested_get_vmcs12_pages.

	 *

	 * bits 5:0 of posted_intr_desc_addr should be zero.

 tpr shadow is needed by all apicv features. */

 x2APIC MSR accesses are not allowed */

 SDM Table 35-2 */

 SMM is not supported */

 SMM is not supported */

/*

 * Load guest's/host's msr at nested entry/exit.

 * return 0 for success, entry index for failure.

 *

 * One of the failure modes for MSR load/store is when a list exceeds the

 * virtual hardware's capacity. To maintain compatibility with hardware inasmuch

 * as possible, process all valid entries before failing rather than precheck

 * for a capacity violation.

 Note, max_msr_list_size is at most 4096, i.e. this can't wrap. */

	/*

	 * If the L0 hypervisor stored a more accurate value for the TSC that

	 * does not include the time taken for emulation of the L2->L1

	 * VM-exit in L0, use the more accurate value.

			/*

			 * Emulated VMEntry does not fail here.  Instead a less

			 * accurate value will be returned by

			 * nested_vmx_get_vmexit_msr_value() using kvm_get_msr()

			 * instead of reading the value from the vmcs02 VMExit

			 * MSR-store area.

/*

 * Load guest's/host's cr3 at nested entry/exit.  @nested_ept is true if we are

 * emulating VM-Entry into a guest with EPT enabled.  On failure, the expected

 * Exit Qualification (for a VM-Entry consistency check VM-Exit) is assigned to

 * @entry_failure_code.

	/*

	 * If PAE paging and EPT are both on, CR3 is not used by the CPU and

	 * must not be dereferenced.

 Re-initialize the MMU, e.g. to pick up CR4 MMU role changes. */

/*

 * Returns if KVM is able to config CPU to tag TLB entries

 * populated by L2 differently than TLB entries populated

 * by L1.

 *

 * If L0 uses EPT, L1 and L2 run with different EPTP because

 * guest_mode is part of kvm_mmu_page_role. Thus, TLB entries

 * are tagged with different EPTP.

 *

 * If L1 uses VPID and we allocated a vpid02, TLB entries are tagged

 * with different VPID (L1 entries are tagged with vmx->vpid

 * while L2 entries are tagged with vmx->nested.vpid02).

	/*

	 * If vmcs12 doesn't use VPID, L1 expects linear and combined mappings

	 * for *all* contexts to be flushed on VM-Enter/VM-Exit, i.e. it's a

	 * full TLB flush from the guest's perspective.  This is required even

	 * if VPID is disabled in the host as KVM may need to synchronize the

	 * MMU in response to the guest TLB flush.

	 *

	 * Note, using TLB_FLUSH_GUEST is correct even if nested EPT is in use.

	 * EPT is a special snowflake, as guest-physical mappings aren't

	 * flushed on VPID invalidations, including VM-Enter or VM-Exit with

	 * VPID disabled.  As a result, KVM _never_ needs to sync nEPT

	 * entries on VM-Enter because L1 can't rely on VM-Enter to flush

	 * those mappings.

 L2 should never have a VPID if VPID is disabled. */

	/*

	 * If VPID is enabled and used by vmc12, but L2 does not have a unique

	 * TLB tag (ASID), i.e. EPT is disabled and KVM was unable to allocate

	 * a VPID for L2, flush the current context as the effective ASID is

	 * common to both L1 and L2.

	 *

	 * Defer the flush so that it runs after vmcs02.EPTP has been set by

	 * KVM_REQ_LOAD_MMU_PGD (if nested EPT is enabled) and to avoid

	 * redundant flushes further down the nested pipeline.

	 *

	 * If a TLB flush isn't required due to any of the above, and vpid12 is

	 * changing then the new "virtual" VPID (vpid12) will reuse the same

	 * "real" VPID (vpid02), and so needs to be flushed.  There's no direct

	 * mapping between vpid02 and vpid12, vpid02 is per-vCPU and reused for

	 * all nested vCPUs.  Remember, a flush on VM-Enter does not invalidate

	 * guest-physical mappings, so there is no need to sync the nEPT MMU.

 feature (except bit 48; see below) */

 reserved */

	/*

	 * KVM does not emulate a version of VMX that constrains physical

	 * addresses of VMX structures (e.g. VMCS) to 32-bits.

 Check must-be-1 bits are still 1. */

 Check must-be-0 bits are still 0. */

 feature */

 reserved */

 Every bit is either reserved or a feature bit. */

	/*

	 * 1 bits (which indicates bits which "must-be-1" during VMX operation)

	 * must be 1 in the restored value.

/*

 * Called when userspace is restoring VMX MSRs.

 *

 * Returns 0 on success, non-0 otherwise.

	/*

	 * Don't allow changes to the VMX capability MSRs while the vCPU

	 * is in VMX operation.

		/*

		 * The "non-true" VMX capability MSRs are generated from the

		 * "true" MSRs, so we do not support restoring them directly.

		 *

		 * If userspace wants to emulate VMX_BASIC[55]=0, userspace

		 * should restore the "true" MSRs with the must-be-1 bits

		 * set according to the SDM Vol 3. A.2 "RESERVED CONTROLS AND

		 * DEFAULT SETTINGS".

		/*

		 * These MSRs are generated based on the vCPU's CPUID, so we

		 * do not support restoring them directly.

		/*

		 * The rest of the VMX capability MSRs do not support restore.

 Returns 0 on success, non-0 otherwise. */

/*

 * Copy the writable VMCS shadow fields back to the VMCS12, in case they have

 * been modified by the L1 guest.  Note, "writable" in this context means

 * "writable by the guest", i.e. tagged SHADOW_FIELD_RW; the set of

 * fields tagged SHADOW_FIELD_RO may or may not align with the "read-only"

 * VM-exit information fields (which are actually writable if the vCPU is

 * configured to support "VMWRITE to any supported field in the VMCS").

 HV_VMX_ENLIGHTENED_CLEAN_FIELD_NONE */

	/*

	 * Not used?

	 * vmcs12->vm_exit_msr_store_addr = evmcs->vm_exit_msr_store_addr;

	 * vmcs12->vm_exit_msr_load_addr = evmcs->vm_exit_msr_load_addr;

	 * vmcs12->vm_entry_msr_load_addr = evmcs->vm_entry_msr_load_addr;

	 * vmcs12->page_fault_error_code_mask =

	 *		evmcs->page_fault_error_code_mask;

	 * vmcs12->page_fault_error_code_match =

	 *		evmcs->page_fault_error_code_match;

	 * vmcs12->cr3_target_count = evmcs->cr3_target_count;

	 * vmcs12->vm_exit_msr_store_count = evmcs->vm_exit_msr_store_count;

	 * vmcs12->vm_exit_msr_load_count = evmcs->vm_exit_msr_load_count;

	 * vmcs12->vm_entry_msr_load_count = evmcs->vm_entry_msr_load_count;

	/*

	 * Read only fields:

	 * vmcs12->guest_physical_address = evmcs->guest_physical_address;

	 * vmcs12->vm_instruction_error = evmcs->vm_instruction_error;

	 * vmcs12->vm_exit_reason = evmcs->vm_exit_reason;

	 * vmcs12->vm_exit_intr_info = evmcs->vm_exit_intr_info;

	 * vmcs12->vm_exit_intr_error_code = evmcs->vm_exit_intr_error_code;

	 * vmcs12->idt_vectoring_info_field = evmcs->idt_vectoring_info_field;

	 * vmcs12->idt_vectoring_error_code = evmcs->idt_vectoring_error_code;

	 * vmcs12->vm_exit_instruction_len = evmcs->vm_exit_instruction_len;

	 * vmcs12->vmx_instruction_info = evmcs->vmx_instruction_info;

	 * vmcs12->exit_qualification = evmcs->exit_qualification;

	 * vmcs12->guest_linear_address = evmcs->guest_linear_address;

	 *

	 * Not present in struct vmcs12:

	 * vmcs12->exit_io_instruction_ecx = evmcs->exit_io_instruction_ecx;

	 * vmcs12->exit_io_instruction_esi = evmcs->exit_io_instruction_esi;

	 * vmcs12->exit_io_instruction_edi = evmcs->exit_io_instruction_edi;

	 * vmcs12->exit_io_instruction_eip = evmcs->exit_io_instruction_eip;

	/*

	 * Should not be changed by KVM:

	 *

	 * evmcs->host_es_selector = vmcs12->host_es_selector;

	 * evmcs->host_cs_selector = vmcs12->host_cs_selector;

	 * evmcs->host_ss_selector = vmcs12->host_ss_selector;

	 * evmcs->host_ds_selector = vmcs12->host_ds_selector;

	 * evmcs->host_fs_selector = vmcs12->host_fs_selector;

	 * evmcs->host_gs_selector = vmcs12->host_gs_selector;

	 * evmcs->host_tr_selector = vmcs12->host_tr_selector;

	 * evmcs->host_ia32_pat = vmcs12->host_ia32_pat;

	 * evmcs->host_ia32_efer = vmcs12->host_ia32_efer;

	 * evmcs->host_cr0 = vmcs12->host_cr0;

	 * evmcs->host_cr3 = vmcs12->host_cr3;

	 * evmcs->host_cr4 = vmcs12->host_cr4;

	 * evmcs->host_ia32_sysenter_esp = vmcs12->host_ia32_sysenter_esp;

	 * evmcs->host_ia32_sysenter_eip = vmcs12->host_ia32_sysenter_eip;

	 * evmcs->host_rip = vmcs12->host_rip;

	 * evmcs->host_ia32_sysenter_cs = vmcs12->host_ia32_sysenter_cs;

	 * evmcs->host_fs_base = vmcs12->host_fs_base;

	 * evmcs->host_gs_base = vmcs12->host_gs_base;

	 * evmcs->host_tr_base = vmcs12->host_tr_base;

	 * evmcs->host_gdtr_base = vmcs12->host_gdtr_base;

	 * evmcs->host_idtr_base = vmcs12->host_idtr_base;

	 * evmcs->host_rsp = vmcs12->host_rsp;

	 * sync_vmcs02_to_vmcs12() doesn't read these:

	 * evmcs->io_bitmap_a = vmcs12->io_bitmap_a;

	 * evmcs->io_bitmap_b = vmcs12->io_bitmap_b;

	 * evmcs->msr_bitmap = vmcs12->msr_bitmap;

	 * evmcs->ept_pointer = vmcs12->ept_pointer;

	 * evmcs->xss_exit_bitmap = vmcs12->xss_exit_bitmap;

	 * evmcs->vm_exit_msr_store_addr = vmcs12->vm_exit_msr_store_addr;

	 * evmcs->vm_exit_msr_load_addr = vmcs12->vm_exit_msr_load_addr;

	 * evmcs->vm_entry_msr_load_addr = vmcs12->vm_entry_msr_load_addr;

	 * evmcs->tpr_threshold = vmcs12->tpr_threshold;

	 * evmcs->virtual_processor_id = vmcs12->virtual_processor_id;

	 * evmcs->exception_bitmap = vmcs12->exception_bitmap;

	 * evmcs->vmcs_link_pointer = vmcs12->vmcs_link_pointer;

	 * evmcs->pin_based_vm_exec_control = vmcs12->pin_based_vm_exec_control;

	 * evmcs->vm_exit_controls = vmcs12->vm_exit_controls;

	 * evmcs->secondary_vm_exec_control = vmcs12->secondary_vm_exec_control;

	 * evmcs->page_fault_error_code_mask =

	 *		vmcs12->page_fault_error_code_mask;

	 * evmcs->page_fault_error_code_match =

	 *		vmcs12->page_fault_error_code_match;

	 * evmcs->cr3_target_count = vmcs12->cr3_target_count;

	 * evmcs->virtual_apic_page_addr = vmcs12->virtual_apic_page_addr;

	 * evmcs->tsc_offset = vmcs12->tsc_offset;

	 * evmcs->guest_ia32_debugctl = vmcs12->guest_ia32_debugctl;

	 * evmcs->cr0_guest_host_mask = vmcs12->cr0_guest_host_mask;

	 * evmcs->cr4_guest_host_mask = vmcs12->cr4_guest_host_mask;

	 * evmcs->cr0_read_shadow = vmcs12->cr0_read_shadow;

	 * evmcs->cr4_read_shadow = vmcs12->cr4_read_shadow;

	 * evmcs->vm_exit_msr_store_count = vmcs12->vm_exit_msr_store_count;

	 * evmcs->vm_exit_msr_load_count = vmcs12->vm_exit_msr_load_count;

	 * evmcs->vm_entry_msr_load_count = vmcs12->vm_entry_msr_load_count;

	 *

	 * Not present in struct vmcs12:

	 * evmcs->exit_io_instruction_ecx = vmcs12->exit_io_instruction_ecx;

	 * evmcs->exit_io_instruction_esi = vmcs12->exit_io_instruction_esi;

	 * evmcs->exit_io_instruction_edi = vmcs12->exit_io_instruction_edi;

	 * evmcs->exit_io_instruction_eip = vmcs12->exit_io_instruction_eip;

/*

 * This is an equivalent of the nested hypervisor executing the vmptrld

 * instruction.

		/*

		 * Currently, KVM only supports eVMCS version 1

		 * (== KVM_EVMCS_VERSION) and thus we expect guest to set this

		 * value to first u32 field of eVMCS which should specify eVMCS

		 * VersionNumber.

		 *

		 * Guest should be aware of supported eVMCS versions by host by

		 * examining CPUID.0x4000000A.EAX[0:15]. Host userspace VMM is

		 * expected to set this CPUID leaf according to the value

		 * returned in vmcs_version from nested_enable_evmcs().

		 *

		 * However, it turns out that Microsoft Hyper-V fails to comply

		 * to their own invented interface: When Hyper-V use eVMCS, it

		 * just sets first u32 field of eVMCS to revision_id specified

		 * in MSR_IA32_VMX_BASIC. Instead of used eVMCS version number

		 * which is one of the supported versions specified in

		 * CPUID.0x4000000A.EAX[0:15].

		 *

		 * To overcome Hyper-V bug, we accept here either a supported

		 * eVMCS version or VMCS12 revision_id as valid values for first

		 * u32 field of eVMCS.

		/*

		 * Unlike normal vmcs12, enlightened vmcs12 is not fully

		 * reloaded from guest's memory (read only fields, fields not

		 * present in struct hv_enlightened_vmcs, ...). Make sure there

		 * are no leftovers.

	/*

	 * Clean fields data can't be used on VMLAUNCH and when we switch

	 * between different L2 guests as KVM keeps a single VMCS12 per L1.

	/*

	 * A timer value of zero is architecturally guaranteed to cause

	 * a VMExit prior to executing any instructions in the guest.

	/*

	 * If vmcs02 hasn't been initialized, set the constant vmcs02 state

	 * according to L0's settings (vmcs12 is irrelevant here).  Host

	 * fields that come from L0 and are not constant, e.g. HOST_CR3,

	 * will be set as needed prior to VMLAUNCH/VMRESUME.

	/*

	 * We don't care what the EPTP value is we just need to guarantee

	 * it's valid so we don't get a false positive when doing early

	 * consistency checks.

 All VMFUNCs are currently emulated through L0 vmexits.  */

	/*

	 * PML is emulated for L2, but never enabled in hardware as the MMU

	 * handles A/D emulation.  Disabling PML for L2 also avoids having to

	 * deal with filtering out L2 GPAs from the buffer.

	/*

	 * Set the MSR load/store lists to match L0's settings.  Only the

	 * addresses are constant (for vmcs02), the counts can change based

	 * on L2's behavior, e.g. switching to/from long mode.

	/*

	 * PIN CONTROLS

 Posted interrupts setting is only taken from vmcs12.  */

	/*

	 * EXEC CONTROLS

 L0's desires */

	/*

	 * A vmexit (to either L1 hypervisor or L0 userspace) is always needed

	 * for I/O port accesses.

	/*

	 * This bit will be computed in nested_get_vmcs12_pages, because

	 * we do not have access to L1's MSR bitmap yet.  For now, keep

	 * the same bit as before, hoping to avoid multiple VMWRITEs that

	 * only set/clear this bit.

	/*

	 * SECONDARY EXEC CONTROLS

 Take the following fields only from vmcs12 */

 PML is emulated and never enabled in hardware for L2. */

 VMCS shadowing for L2 is emulated for now */

		/*

		 * Preset *DT exiting when emulating UMIP, so that vmx_set_cr4()

		 * will not have to rewrite the controls just for this bit.

	/*

	 * ENTRY CONTROLS

	 *

	 * vmcs12's VM_{ENTRY,EXIT}_LOAD_IA32_EFER and VM_ENTRY_IA32E_MODE

	 * are emulated by vmx_set_efer() in prepare_vmcs02(), but speculate

	 * on the related bits (if supported by the CPU) in the hope that

	 * we can avoid VMWrites during vmx_set_efer().

	/*

	 * EXIT CONTROLS

	 *

	 * L2->L1 exit controls are emulated - the hardware exit is to L0 so

	 * we should use its exit controls. Note that VM_EXIT_LOAD_IA32_EFER

	 * bits may be modified by vmx_set_efer() in prepare_vmcs02().

	/*

	 * Interrupt/Exception Fields

		/*

		 * L1 may access the L2's PDPTR, so save them to construct

		 * vmcs12

	/*

	 * Whether page-faults are trapped is determined by a combination of

	 * 3 settings: PFEC_MASK, PFEC_MATCH and EXCEPTION_BITMAP.PF.  If L0

	 * doesn't care about page faults then we should set all of these to

	 * L1's desires. However, if L0 does care about (some) page faults, it

	 * is not easy (if at all possible?) to merge L0 and L1's desires, we

	 * simply ask to exit on each and every L2 page fault. This is done by

	 * setting MASK=MATCH=0 and (see below) EB.PF=1.

	 * Note that below we don't need special code to set EB.PF beyond the

	 * "or"ing of the EB of vmcs01 and vmcs12, because when enable_ept,

	 * vmcs01's EB.PF is 0 so the "or" will take vmcs12's value, and when

	 * !enable_ept, EB.PF is 1, so the "or" will always be 1.

		/*

		 * TODO: if both L0 and L1 need the same MASK and MATCH,

		 * go ahead and use it?

	/*

	 * Make sure the msr_autostore list is up to date before we set the

	 * count in the vmcs02.

/*

 * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested

 * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function "merges" it

 * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2

 * guest in a way that will both be appropriate to L1's requests, and our

 * needs. In addition to modifying the active vmcs (which is vmcs02), this

 * function also has additional necessary side-effects, like setting various

 * vcpu->arch fields.

 * Returns 0 on success, 1 on failure. Invalid state exit qualification code

 * is assigned to entry_failure_code on failure.

	/* EXCEPTION_BITMAP and CR0_GUEST_HOST_MASK should basically be the

	 * bitwise-or of what L1 wants to trap for L2, and what we want to

	 * trap. Note that CR0.TS also needs updating - we do this later.

	/*

	 * This sets GUEST_CR0 to vmcs12->guest_cr0, possibly modifying those

	 * bits which we consider mandatory enabled.

	 * The CR0_READ_SHADOW is what L2 should have expected to read given

	 * the specifications by L1; It's not enough to take

	 * vmcs12->cr0_read_shadow because on our cr0_guest_host_mask we we

	 * have more bits than L1 expected.

 Note: may modify VM_ENTRY/EXIT_CONTROLS and GUEST/HOST_IA32_EFER */

	/*

	 * Guest state is invalid and unrestricted guest is disabled,

	 * which means L1 attempted VMEntry to L2 with invalid state.

	 * Fail the VMEntry.

	 *

	 * However when force loading the guest state (SMM exit or

	 * loading nested state after migration, it is possible to

	 * have invalid guest state now, which will be later fixed by

	 * restoring L2 register state

 Shadow page tables on either EPT or shadow page tables. */

	/*

	 * Immediately write vmcs02.GUEST_CR3.  It will be propagated to vmcs12

	 * on nested VM-Exit, which can occur without actually running L2 and

	 * thus without hitting vmx_load_mmu_pgd(), e.g. if L1 is entering L2 with

	 * vmcs12.GUEST_ACTIVITYSTATE=HLT, in which case KVM will intercept the

	 * transition to HLT instead of running L2.

 Late preparation of GUEST_PDPTRs now that EFER and CRs are set. */

	/*

	 * It was observed that genuine Hyper-V running in L1 doesn't reset

	 * 'hv_clean_fields' by itself, it only sets the corresponding dirty

	 * bits when it changes a field in eVMCS. Mark all fields as clean

	 * here.

 Check for memory type validity */

 Page-walk levels validity. */

 Reserved bits should not be set */

 AD, if set, should be supported */

/*

 * Checks related to VM-Execution Control Fields

/*

 * Checks related to VM-Exit Control Fields

/*

 * Checks related to VM-Entry Control Fields

	/*

	 * From the Intel SDM, volume 3:

	 * Fields relevant to VM-entry event injection must be set properly.

	 * These fields are the VM-entry interruption-information field, the

	 * VM-entry exception error code, and the VM-entry instruction length.

 VM-entry interruption-info field: interruption type */

 VM-entry interruption-info field: vector */

 VM-entry interruption-info field: deliver error code */

 VM-entry exception error code */

 VM-entry interruption-info field: reserved bits */

 VM-entry instruction length */

	/*

	 * If the load IA32_EFER VM-exit control is 1, bits reserved in the

	 * IA32_EFER MSR must be 0 in the field for that register. In addition,

	 * the values of the LMA and LME bits in the field must each be that of

	 * the host address-space size VM-exit control.

/*

 * Checks related to Guest Non-register State

	/*

	 * If the load IA32_EFER VM-entry control is 1, the following checks

	 * are performed on the field for the IA32_EFER MSR:

	 * - Bits reserved in the IA32_EFER MSR must be 0.

	 * - Bit 10 (corresponding to IA32_EFER.LMA) must equal the value of

	 *   the IA-32e mode guest VM-exit control. It must also be identical

	 *   to bit 8 (LME) if bit 31 in the CR0 field (corresponding to

	 *   CR0.PG) is 1.

	/*

	 * Induce a consistency check VMExit by clearing bit 1 in GUEST_RFLAGS,

	 * which is reserved to '1' by hardware.  GUEST_RFLAGS is guaranteed to

	 * be written (by prepare_vmcs02()) before the "real" VMEnter, i.e.

	 * there is no need to preserve other bits or save/restore the field.

	/*

	 * VMExit clears RFLAGS.IF and DR7, even on a consistency check.

	/*

	 * A non-failing VMEntry means we somehow entered guest mode with

	 * an illegal RIP, and that's just the tip of the iceberg.  There

	 * is no telling what memory has been modified or what state has

	 * been exposed to unknown code.  Hitting this all but guarantees

	 * a (very critical) hardware issue.

	/*

	 * hv_evmcs may end up being not mapped after migration (when

	 * L2 was running), map it here to make sure vmcs12 changes are

	 * properly reflected.

		/*

		 * Post migration VMCS12 always provides the most actual

		 * information, copy it to eVMCS upon entry.

		/*

		 * Reload the guest's PDPTRs since after a migration

		 * the guest CR3 might be restored prior to setting the nested

		 * state which can lead to a load of wrong PDPTRs.

		/*

		 * Translate L1 physical address to host physical

		 * address for vmcs02. Keep the page pinned, so this

		 * physical address remains valid. We keep a reference

		 * to it so we can release it later.

 shouldn't happen */

			/*

			 * The processor will never use the TPR shadow, simply

			 * clear the bit from the execution control.  Such a

			 * configuration is useless, but it happens in tests.

			 * For any other configuration, failing the vm entry is

			 * _not_ what the processor does but it's basically the

			 * only possibility we have.

			/*

			 * Write an illegal value to VIRTUAL_APIC_PAGE_ADDR to

			 * force VM-Entry to fail.

			/*

			 * Defer the KVM_INTERNAL_EXIT until KVM tries to

			 * access the contents of the VMCS12 posted interrupt

			 * descriptor. (Note that KVM may do this when it

			 * should not, per the architectural specification.)

	/*

	 * Check if PML is enabled for the nested guest. Whether eptp bit 6 is

	 * set is already checked as part of A/D emulation.

/*

 * Intel's VMX Instruction Reference specifies a common set of prerequisites

 * for running VMX instructions (except VMXON, whose prerequisites are

 * slightly different). It also specifies what exception to inject otherwise.

 * Note that many of these exceptions have priority over VM exits, so they

 * don't have to be checked again here.

/*

 * If from_vmentry is false, this is being called from state restore (either RSM

 * or KVM_SET_NESTED_STATE).  Otherwise it's called from vmlaunch/vmresume.

 *

 * Returns:

 *	NVMX_VMENTRY_SUCCESS: Entered VMX non-root mode

 *	NVMX_VMENTRY_VMFAIL:  Consistency check VMFail

 *	NVMX_VMENTRY_VMEXIT:  Consistency check VMExit

 *	NVMX_VMENTRY_KVM_INTERNAL_ERROR: KVM internal error

	/*

	 * Overwrite vmcs01.GUEST_CR3 with L1's CR3 if EPT is disabled *and*

	 * nested early checks are disabled.  In the event of a "late" VM-Fail,

	 * i.e. a VM-Fail detected by hardware but not KVM, KVM must unwind its

	 * software model to the pre-VMEntry host state.  When EPT is disabled,

	 * GUEST_CR3 holds KVM's shadow CR3, not L1's "real" CR3, which causes

	 * nested_vmx_restore_host_state() to corrupt vcpu->arch.cr3.  Stuffing

	 * vmcs01.GUEST_CR3 results in the unwind naturally setting arch.cr3 to

	 * the correct value.  Smashing vmcs01.GUEST_CR3 is safe because nested

	 * VM-Exits, and the unwind, reset KVM's MMU, i.e. vmcs01.GUEST_CR3 is

	 * guaranteed to be overwritten with a shadow CR3 prior to re-entering

	 * L1.  Don't stuff vmcs01.GUEST_CR3 when using nested early checks as

	 * KVM modifies vcpu->arch.cr3 if and only if the early hardware checks

	 * pass, and early VM-Fails do not reset KVM's MMU, i.e. the VM-Fail

	 * path would need to manually save/restore vmcs01.GUEST_CR3.

		/*

		 * The MMU is not initialized to point at the right entities yet and

		 * "get pages" would need to read data from the guest (i.e. we will

		 * need to perform gpa to hpa translation). Request a call

		 * to nested_get_vmcs12_pages before the next VM-entry.  The MSRs

		 * have already been set at vmentry time and should not be reset.

	/*

	 * If L1 had a pending IRQ/NMI until it executed

	 * VMLAUNCH/VMRESUME which wasn't delivered because it was

	 * disallowed (e.g. interrupts disabled), L0 needs to

	 * evaluate if this pending event should cause an exit from L2

	 * to L1 or delivered directly to L2 (e.g. In case L1 don't

	 * intercept EXTERNAL_INTERRUPT).

	 *

	 * Usually this would be handled by the processor noticing an

	 * IRQ/NMI window request, or checking RVI during evaluation of

	 * pending virtual interrupts.  However, this setting was done

	 * on VMCS01 and now VMCS02 is active instead. Thus, we force L0

	 * to perform pending event evaluation by requesting a KVM_REQ_EVENT.

	/*

	 * Do not start the preemption timer hrtimer until after we know

	 * we are successful, so that only nested_vmx_vmexit needs to cancel

	 * the timer.

	/*

	 * Note no nested_vmx_succeed or nested_vmx_fail here. At this point

	 * we are no longer running L1, and VMLAUNCH/VMRESUME has not yet

	 * returned as far as L1 is concerned. It will only return (and set

	 * the success flag) when L2 exits (see nested_vmx_vmexit()).

	/*

	 * A failed consistency check that leads to a VMExit during L1's

	 * VMEnter to L2 is a variation of a normal VMexit, as explained in

	 * 26.7 "VM-entry failures during or after loading guest state".

/*

 * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1

 * for running an L2 nested guest.

	/*

	 * Can't VMLAUNCH or VMRESUME a shadow VMCS. Despite the fact

	 * that there *is* a valid VMCS pointer, RFLAGS.CF is set

	 * rather than RFLAGS.ZF, and no error number is stored to the

	 * VM-instruction error field.

 Enlightened VMCS doesn't have launch state */

	/*

	 * The nested entry process starts with enforcing various prerequisites

	 * on vmcs12 as required by the Intel SDM, and act appropriately when

	 * they fail: As the SDM explains, some conditions should cause the

	 * instruction to fail, while others will cause the instruction to seem

	 * to succeed, but return an EXIT_REASON_INVALID_STATE.

	 * To speed up the normal (success) code path, we should avoid checking

	 * for misconfigurations which will anyway be caught by the processor

	 * when using the merged vmcs02.

	/*

	 * We're finally done with prerequisite checking, and can start with

	 * the nested entry.

 Emulate processing of posted interrupts on VM-Enter. */

 Hide L1D cache contents from the nested guest.  */

	/*

	 * Must happen outside of nested_vmx_enter_non_root_mode() as it will

	 * also be used as part of restoring nVMX state for

	 * snapshot restore (migration).

	 *

	 * In this flow, it is assumed that vmcs12 cache was

	 * transferred as part of captured nVMX state and should

	 * therefore not be read from guest memory (which may not

	 * exist on destination host yet).

		/*

		 * If we're entering a halted L2 vcpu and the L2 vcpu won't be

		 * awakened by event injection or by an NMI-window VM-exit or

		 * by an interrupt-window VM-exit, halt the vcpu.

/*

 * On a nested exit from L2 to L1, vmcs12.guest_cr0 might not be up-to-date

 * because L2 may have changed some cr0 bits directly (CR0_GUEST_HOST_MASK).

 * This function returns the new value we should put in vmcs12.guest_cr0.

 * It's not enough to just return the vmcs02 GUEST_CR0. Rather,

 *  1. Bits that neither L0 nor L1 trapped, were set directly by L2 and are now

 *     available in vmcs02 GUEST_CR0. (Note: It's enough to check that L0

 *     didn't trap the bit, because if L1 did, so would L0).

 *  2. Bits that L1 asked to trap (and therefore L0 also did) could not have

 *     been modified by L2, and L1 knows it. So just leave the old value of

 *     the bit from vmcs12.guest_cr0. Note that the bit from vmcs02 GUEST_CR0

 *     isn't relevant, because if L0 traps this bit it can set it to anything.

 *  3. Bits that L1 didn't trap, but L0 did. L1 believes the guest could have

 *     changed these bits, and therefore they need to be updated, but L0

 *     didn't necessarily allow them to be changed in GUEST_CR0 - and rather

 *     put them in vmcs02 CR0_READ_SHADOW. So take these bits from there.

1*/	(vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |

2*/	(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask) |

3*/	(vmcs_readl(CR0_READ_SHADOW) & ~(vmcs12->cr0_guest_host_mask |

1*/	(vmcs_readl(GUEST_CR4) & vcpu->arch.cr4_guest_owned_bits) |

2*/	(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask) |

3*/	(vmcs_readl(CR4_READ_SHADOW) & ~(vmcs12->cr4_guest_host_mask |

	/*

	 * Don't need to mark the APIC access page dirty; it is never

	 * written to by the CPU during APIC virtualization.

/*

 * Returns true if a debug trap is pending delivery.

 *

 * In KVM, debug traps bear an exception payload. As such, the class of a #DB

 * exception may be inferred from the presence of an exception payload.

/*

 * Certain VM-exits set the 'pending debug exceptions' field to indicate a

 * recognized #DB (data or single-step) that has yet to be delivered. Since KVM

 * represents these debug traps with a payload that is said to be compatible

 * with the 'pending debug exceptions' field, write the payload to the VMCS

 * field if a VM-exit is delivered before the debug trap.

	/*

	 * Clear the MTF state. If a higher priority VM-exit is delivered first,

	 * this state is discarded.

	/*

	 * Process any exceptions that are not debug traps before MTF.

	 *

	 * Note that only a pending nested run can block a pending exception.

	 * Otherwise an injected NMI/interrupt should either be

	 * lost or delivered to the nested hypervisor in the IDT_VECTORING_INFO,

	 * while delivering the pending exception.

		/*

		 * The NMI-triggered VM exit counts as injection:

		 * clear this one and block further NMIs.

/*

 * Update the guest state fields of vmcs12 to reflect changes that

 * occurred while L2 was running. (The "IA-32e mode guest" bit of the

 * VM-entry controls is also updated, since this is really a guest

 * state bit.)

	/*

	 * In some cases (usually, nested EPT), L2 is allowed to change its

	 * own CR3 without exiting. If it has changed it, we must keep it.

	 * Of course, if L0 is using shadow page tables, GUEST_CR3 was defined

	 * by L0, not L1 or L2, so we mustn't unconditionally copy it to vmcs12.

	 *

	 * Additionally, restore L2's PDPTR to vmcs12.

/*

 * prepare_vmcs12 is part of what we need to do when the nested L2 guest exits

 * and we want to prepare to run its L1 parent. L1 keeps a vmcs for L2 (vmcs12),

 * and this function updates it to reflect the changes to the guest state while

 * L2 was running (and perhaps made some exits which were handled directly by L0

 * without going back to L1), and to reflect the exit reason.

 * Note that we do not have to copy here all VMCS fields, just those that

 * could have changed by the L2 guest or the exit - i.e., the guest-state and

 * exit-information fields only. Other fields are modified by L1 with VMWRITE,

 * which already writes to vmcs12 directly.

 update exit information fields: */

		/* vm_entry_intr_info_field is cleared on exit. Emulate this

		/*

		 * Transfer the event that L0 or L1 may wanted to inject into

		 * L2 to IDT_VECTORING_INFO_FIELD.

		/*

		 * According to spec, there's no need to store the guest's

		 * MSRs if the exit is due to a VM-entry failure that occurs

		 * during or after loading the guest state. Since this exit

		 * does not fall in that category, we need to save the MSRs.

	/*

	 * Drop what we picked up for L2 via vmx_complete_interrupts. It is

	 * preserved above and would only end up incorrectly in L1.

/*

 * A part of what we need to when the nested L2 guest exits and we want to

 * run its L1 parent, is to reset L1's guest state to the host state specified

 * in vmcs12.

 * This function is to be called not only on normal nested exit, but also on

 * a nested entry failure, as explained in Intel's spec, 3B.23.7 ("VM-Entry

 * Failures During or After Loading Guest State").

 * This function should be called when the active VMCS is L1's (vmcs01).

	/*

	 * Note that calling vmx_set_cr0 is important, even if cr0 hasn't

	 * actually changed, because vmx_set_cr0 refers to efer set above.

	 *

	 * CR0_GUEST_HOST_MASK is already set in the original vmcs01

	 * (KVM doesn't change it);

 Same as above - no reason to call set_cr4_guest_host_mask().  */

	/*

	 * Only PDPTE load can fail as the value of cr3 was checked on entry and

	 * couldn't have changed.

 If not VM_EXIT_CLEAR_BNDCFGS, the L2 value propagates to L1.  */

	/* Set L1 segment info according to Intel SDM

		/*

		 * L1's host DR7 is lost if KVM_GUESTDBG_USE_HW_BP is set

		 * as vmcs01.GUEST_DR7 contains a userspace defined value

		 * and vcpu->arch.dr7 is not squirreled away before the

		 * nested VMENTER (not worth adding a variable in nested_vmx).

	/*

	 * Note that calling vmx_set_{efer,cr0,cr4} is important as they

	 * handle a variety of side effects to KVM's software model.

	/*

	 * Use ept_save_pdptrs(vcpu) to load the MMU's cached PDPTRs

	 * from vmcs01 (if necessary).  The PDPTRs are not loaded on

	 * VMFail, like everything else we just need to ensure our

	 * software model is up-to-date.

	/*

	 * This nasty bit of open coding is a compromise between blindly

	 * loading L1's MSRs using the exit load lists (incorrect emulation

	 * of VMFail), leaving the nested VM's MSRs in the software model

	 * (incorrect behavior) and snapshotting the modified MSRs (too

	 * expensive since the lists are unbound by hardware).  For each

	 * MSR that was (prematurely) loaded from the nested VMEntry load

	 * list, reload it from the exit load list if it exists and differs

	 * from the guest value.  The intent is to stuff host state as

	 * silently as possible, not to fully process the exit load list.

/*

 * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1

 * and modify vmcs12 to make it see what it would expect to see there if

 * L2 was its real guest. Must only be called when in L2 (is_guest_mode())

 trying to cancel vmlaunch/vmresume is a bug */

 Similarly, triple faults in L2 should never escape. */

		/*

		 * KVM_REQ_GET_NESTED_STATE_PAGES is also used to map

		 * Enlightened VMCS after migration and we still need to

		 * do that when something is forcing L2->L1 exit prior to

		 * the first L2 run.

 Service the TLB flush request for L2 before switching to L1. */

	/*

	 * VCPU_EXREG_PDPTR will be clobbered in arch/x86/kvm/vmx/vmx.h between

	 * now and the new vmentry.  Ensure that the VMCS02 PDPTR fields are

	 * up-to-date before switching to L1.

		/*

		 * Must happen outside of sync_vmcs02_to_vmcs12() as it will

		 * also be used to capture vmcs12 cache as part of

		 * capturing nVMX state for snapshot (migration).

		 *

		 * Otherwise, this flush will dirty guest memory at a

		 * point it is already assumed by user-space to be

		 * immutable.

		/*

		 * The only expected VM-instruction error is "VM entry with

		 * invalid control field(s)." Anything else indicates a

		 * problem with L0.  And we should never get here with a

		 * VMFail of any type if early consistency checks are enabled.

 Update any VMCS fields that might have changed while L2 ran */

 Unpin physical memory we referred to in vmcs02 */

 in case we halted in L2 */

	/*

	 * After an early L2 VM-entry failure, we're now back

	 * in L1 which thinks it just finished a VMLAUNCH or

	 * VMRESUME instruction, so we need to set the failure

	 * flag and the VM-instruction error field of the VMCS

	 * accordingly, and skip the emulated instruction.

	/*

	 * Restore L1's host state to KVM's software model.  We're here

	 * because a consistency check was caught by hardware, which

	 * means some amount of guest state has been propagated to KVM's

	 * model and needs to be unwound to the host's state.

/*

 * Decode the memory-address operand of a vmx instruction, as recorded on an

 * exit caused by such an instruction (run by a guest hypervisor).

 * On success, returns 0. When the operand is invalid, returns 1 and throws

 * #UD, #GP, or #SS.

	/*

	 * According to Vol. 3B, "Information for VM Exits Due to Instruction

	 * Execution", on an exit, vmx_instruction_info holds most of the

	 * addressing components of the operand. Only the displacement part

	 * is put in exit_qualification (see 3B, "Basic VM-Exit Information").

	 * For how an actual address is calculated from all these components,

	 * refer to Vol. 1, "Operand Addressing".

 Addr = segment_base + offset */

 offset = base + [index * scale] + displacement */

 holds the displacement */

	/*

	 * The effective address, i.e. @off, of a memory operand is truncated

	 * based on the address size of the instruction.  Note that this is

	 * the *effective address*, i.e. the address prior to accounting for

	 * the segment's base.

 32 bit */

 16 bit */

 Checks for #GP/#SS exceptions. */

		/*

		 * The virtual/linear address is never truncated in 64-bit

		 * mode, e.g. a 32-bit address size can yield a 64-bit virtual

		 * address when using FS/GS with a non-zero base.

		/* Long mode: #GP(0)/#SS(0) if the memory address is in a

		 * non-canonical form. This is the only check on the memory

		 * destination for long mode!

		/*

		 * When not in long mode, the virtual/linear address is

		 * unconditionally truncated to 32 bits regardless of the

		 * address size.

		/* Protected mode: apply checks for segment validity in the

		 * following order:

		 * - segment type check (#GP(0) may be thrown)

		 * - usability check (#GP(0)/#SS(0))

		 * - limit check (#GP(0)/#SS(0))

			/* #GP(0) if the destination operand is located in a

			 * read-only data segment or any code segment.

			/* #GP(0) if the source operand is located in an

			 * execute-only code segment

		/* Protected mode: #GP(0)/#SS(0) if the segment is unusable.

		/*

		 * Protected mode: #GP(0)/#SS(0) if the memory operand is

		 * outside the segment limit.  All CPUs that support VMX ignore

		 * limit checks for flat segments, i.e. segments with base==0,

		 * limit==0xffffffff and of type expand-up data or code.

/*

 * Allocate a shadow VMCS and associate it with the currently loaded

 * VMCS, unless such a shadow VMCS already exists. The newly allocated

 * VMCS is also VMCLEARed, so that it is ready for use.

	/*

	 * We should allocate a shadow vmcs for vmcs01 only when L1

	 * executes VMXON and free it when L1 executes VMXOFF.

	 * As it is invalid to execute VMXON twice, we shouldn't reach

	 * here when vmcs01 already have an allocated shadow vmcs.

 Emulate the VMXON instruction. */

	/*

	 * The Intel VMX Instruction Reference lists a bunch of bits that are

	 * prerequisite to running VMXON, most notably cr4.VMXE must be set to

	 * 1 (see vmx_is_valid_cr4() for when we allow the guest to set this).

	 * Otherwise, we should fail with #UD.  But most faulting conditions

	 * have already been checked by hardware, prior to the VM-exit for

	 * VMXON.  We do test guest cr4.VMXE because processor CR4 always has

	 * that bit set to 1 in non-root mode.

 CPL=0 must be checked manually. */

	/*

	 * SDM 3: 24.11.5

	 * The first 4 bytes of VMXON region contain the supported

	 * VMCS revision identifier

	 *

	 * Note - IA32_VMX_BASIC[48] will never be 1 for the nested case;

	 * which replaces physical address width with 32

		/* copy to memory all shadowed fields in case

 Flush VMCS12 to guest memory */

 Emulate the VMXOFF instruction */

 Process a latched INIT during time CPU was in VMX operation */

 Emulate the VMCLEAR instruction */

	/*

	 * When Enlightened VMEntry is enabled on the calling CPU we treat

	 * memory area pointer by vmptr as Enlightened VMCS (as there's no good

	 * way to distinguish it from VMCS12) and we must not corrupt it by

	 * writing to the non-existent 'launch_state' field. The area doesn't

	 * have to be the currently active EVMCS on the calling CPU and there's

	 * nothing KVM has to do to transition it from 'active' to 'non-active'

	 * state. It is possible that the area will stay mapped as

	 * vmx->nested.hv_evmcs but this shouldn't be a problem.

 Emulate the VMLAUNCH instruction */

 Emulate the VMRESUME instruction */

	/*

	 * In VMX non-root operation, when the VMCS-link pointer is INVALID_GPA,

	 * any VMREAD sets the ALU flags for VMfailInvalid.

 Decode instruction info and find the field to read */

 Read the field, zero-extended to a u64 value */

	/*

	 * Now copy part of this value to register or memory, as requested.

	 * Note that the number of bits actually copied is 32 or 64 depending

	 * on the guest's mode (32 or 64 bit), not on the given field's length.

 _system ok, nested_vmx_check_permission has verified cpl=0 */

	/*

	 * The value to write might be 32 or 64 bits, depending on L1's long

	 * mode, and eventually we need to write that into a field of several

	 * possible lengths. The code below first zero-extends the value to 64

	 * bit (value), and then copies only the appropriate number of

	 * bits into the vmcs12 field.

	/*

	 * In VMX non-root operation, when the VMCS-link pointer is INVALID_GPA,

	 * any VMWRITE sets the ALU flags for VMfailInvalid.

	/*

	 * If the vCPU supports "VMWRITE to any supported field in the

	 * VMCS," then the "read-only" fields are actually read/write.

	/*

	 * Ensure vmcs12 is up-to-date before any VMWRITE that dirties

	 * vmcs12, else we may crush a field or consume a stale value.

	/*

	 * Some Intel CPUs intentionally drop the reserved bits of the AR byte

	 * fields on VMWRITE.  Emulate this behavior to ensure consistent KVM

	 * behavior regardless of the underlying hardware, e.g. if an AR_BYTE

	 * field is intercepted for VMWRITE but not VMREAD (in L1), then VMREAD

	 * from L1 will return a different value than VMREAD from L2 (L1 sees

	 * the stripped down value, L2 sees the full value as stored by KVM).

	/*

	 * Do not track vmcs12 dirty-state if in guest-mode as we actually

	 * dirty shadow vmcs12 instead of vmcs12.  Fields that can be updated

	 * by L1 without a vmexit are always updated in the vmcs02, i.e. don't

	 * "dirty" vmcs12, all others go down the prepare_vmcs02() slow path.

		/*

		 * L1 can read these fields without exiting, ensure the

		 * shadow VMCS is up-to-date.

 Emulate the VMPTRLD instruction */

 Forbid normal VMPTRLD if Enlightened version was used */

			/*

			 * Reads from an unbacked page return all 1s,

			 * which means that the 32 bits located at the

			 * given physical address won't match the required

			 * VMCS12_REVISION identifier.

		/*

		 * Load VMCS12 from guest memory since it is not already

		 * cached.

 Emulate the VMPTRST instruction */

 *_system ok, nested_vmx_check_permission has verified cpl=0 */

 Emulate the INVEPT instruction */

	/* According to the Intel VMX instruction reference, the memory

	 * operand is read even if it isn't needed (e.g., for type==global)

	/*

	 * Nested EPT roots are always held through guest_mmu,

	 * not root_mmu.

	/* according to the intel vmx instruction reference, the memory

	 * operand is read even if it isn't needed (e.g., for type==global)

	/*

	 * Sync the shadow page tables if EPT is disabled, L1 is invalidating

	 * linear mappings for L2 (tagged with L2's VPID).  Free all guest

	 * roots as VPIDs are not tracked in the MMU role.

	 *

	 * Note, this operates on root_mmu, not guest_mmu, as L1 and L2 share

	 * an MMU when EPT is disabled.

	 *

	 * TODO: sync only the affected SPTEs for INVDIVIDUAL_ADDR.

	/*

	 * If the (L2) guest does a vmfunc to the currently

	 * active ept pointer, we don't have to do anything else

	/*

	 * VMFUNC is only supported for nested guests, but we always enable the

	 * secondary control for simplicity; for non-nested mode, fake that we

	 * didn't by injecting #UD.

	/*

	 * #UD on out-of-bounds function has priority over VM-Exit, and VMFUNC

	 * is enabled in vmcs02 if and only if it's enabled in vmcs12.

	/*

	 * This is effectively a reflected VM-Exit, as opposed to a synthesized

	 * nested VM-Exit.  Pass the original exit reason, i.e. don't hardcode

	 * EXIT_REASON_VMFUNC as the exit reason.

/*

 * Return true if an IO instruction with the specified port and size should cause

 * a VM-exit into L1.

/*

 * Return 1 if we should exit from L2 to L1 to handle an MSR access,

 * rather than handle it ourselves in L0. I.e., check whether L1 expressed

 * disinterest in the current event (read or write a specific MSR) by using an

 * MSR bitmap. This may be the case even when L0 doesn't use MSR bitmaps.

	/*

	 * The MSR_BITMAP page is divided into four 1024-byte bitmaps,

	 * for the four combinations of read/write and low/high MSR numbers.

	 * First we need to figure out which of the four to use:

 Then read the msr_index'th bit from this bitmap: */

 let L1 handle the wrong parameter */

/*

 * Return 1 if we should exit from L2 to L1 to handle a CR access exit,

 * rather than handle it ourselves in L0. I.e., check if L1 wanted to

 * intercept (via guest_host_mask etc.) the current event.

 mov to cr */

 clts */

 mov from cr */

 lmsw */

		/*

		 * lmsw can change bits 1..3 of cr0, and only set bit 0 of

		 * cr0. Other attempted changes are ignored, with no exit.

 Decode instruction info and find the field to access */

 Out-of-range fields always cause a VM exit from L2 to L1 */

	/*

	 * An MTF VM-exit may be injected into the guest by setting the

	 * interruption-type to 7 (other event) and the vector field to 0. Such

	 * is the case regardless of the 'monitor trap flag' VM-execution

	 * control.

/*

 * Return true if L0 wants to handle an exit from L2 regardless of whether or not

 * L1 wants the exit.  Only call this when in is_guest_mode (L2).

		/*

		 * L0 always deals with the EPT violation. If nested EPT is

		 * used, and the nested mmu code discovers that the address is

		 * missing in the guest EPT table (EPT12), the EPT violation

		 * will be injected with nested_ept_inject_page_fault()

		/*

		 * L2 never uses directly L1's EPT, but rather L0's own EPT

		 * table (shadow on EPT) or a merged EPT table that L0 built

		 * (EPT on EPT). So any problems with the structure of the

		 * table is L0's fault.

		/*

		 * PML is emulated for an L1 VMM and should never be enabled in

		 * vmcs02, always "handle" PML_FULL by exiting to userspace.

 VM functions are emulated through L2->L0 vmexits. */

		/*

		 * At present, bus lock VM exit is never exposed to L1.

		 * Handle L2's bus locks in L0 directly.

/*

 * Return 1 if L1 wants to intercept an exit from L2.  Only call this when in

 * is_guest_mode (L2).

		/*

		 * VMX instructions trap unconditionally. This allows L1 to

		 * emulate them for its L2 guest, i.e., allows 3-level nesting!

		/*

		 * The controls for "virtualize APIC accesses," "APIC-

		 * register virtualization," and "virtual-interrupt

		 * delivery" only come from vmcs12.

		/*

		 * This should never happen, since it is not possible to

		 * set XSS to a non-zero value---neither in L1 nor in L2.

		 * If if it were, XSS would have to be checked against

		 * the XSS exit bitmap in vmcs12.

/*

 * Conditionally reflect a VM-Exit into L1.  Returns %true if the VM-Exit was

 * reflected into L1.

	/*

	 * Late nested VM-Fail shares the same flow as nested VM-Exit since KVM

	 * has already loaded L2's state.

 If L0 (KVM) wants the exit, it trumps L1's desires. */

 If L1 doesn't want the exit, handle it in L0. */

	/*

	 * vmcs.VM_EXIT_INTR_INFO is only valid for EXCEPTION_NMI exits.  For

	 * EXTERNAL_INTERRUPT, the value for vmcs12->vm_exit_intr_info would

	 * need to be synthesized by querying the in-kernel LAPIC, but external

	 * interrupts are never reflected to L1 so it's a non-issue.

 'hv_evmcs_vmptr' can also be EVMPTR_MAP_PENDING here */

	/*

	 * When running L2, the authoritative vmcs12 state is in the

	 * vmcs02. When running L1, the authoritative vmcs12 state is

	 * in the shadow or enlightened vmcs linked to vmcs01, unless

	 * need_vmcs12_to_shadow_sync is set, in which case, the authoritative

	 * vmcs12 state is in the vmcs12 already.

				/*

				 * L1 hypervisor is not obliged to keep eVMCS

				 * clean fields data always up-to-date while

				 * not in guest mode, 'hv_clean_fields' is only

				 * supposed to be actual upon vmentry so we need

				 * to ignore it here and do full copy.

	/*

	 * Copy over the full allocated size of vmcs12 rather than just the size

	 * of the struct.

/*

 * Forcibly leave nested mode in order to be able to reset the VCPU later on.

		/*

		 * KVM_STATE_NESTED_EVMCS used to signal that KVM should

		 * enable eVMCS capability on vCPU. However, since then

		 * code was changed such that flag signals vmcs12 should

		 * be copied into eVMCS in guest memory.

		 *

		 * To preserve backwards compatability, allow user

		 * to set this flag even when there is no VMXON region.

	/*

	 * SMM temporarily disables VMX, so we cannot be in guest mode,

	 * nor can VMLAUNCH/VMRESUME be pending.  Outside SMM, SMM flags

	 * must be zero.

 Empty 'VMXON' state is permitted if no VMCS loaded */

 See vmx_has_valid_vmcs12.  */

		/*

		 * nested_vmx_handle_enlightened_vmptrld() cannot be called

		 * directly from here as HV_X64_MSR_VP_ASSIST_PAGE may not be

		 * restored yet. EVMCS will be mapped from

		 * nested_get_vmcs12_pages().

/*

 * Indexing into the vmcs12 uses the VMCS encoding rotated left by 6.  Undo

 * that madness to get the encoding for comparison.

	/*

	 * Note these are the so called "index" of the VMCS field encoding, not

	 * the index into vmcs12.

	/*

	 * For better or worse, KVM allows VMREAD/VMWRITE to all fields in

	 * vmcs12, regardless of whether or not the associated feature is

	 * exposed to L1.  Simply find the field with the highest index.

 The vmcs12 table is very, very sparsely populated. */

/*

 * nested_vmx_setup_ctls_msrs() sets up variables containing the values to be

 * returned for the various VMX controls MSRs when nested VMX is enabled.

 * The same values should also be used to verify that vmcs12 control fields are

 * valid during nested entry from L1 to L2.

 * Each of these control msrs has a low and high 32-bit half: A low bit is on

 * if the corresponding bit in the (32-bit) control field *must* be on, and a

 * bit in the high half is on if the corresponding bit in the control field

 * may be on. See also vmx_control_verify().

	/*

	 * Note that as a general rule, the high half of the MSRs (bits in

	 * the control fields which may be 1) should be initialized by the

	 * intersection of the underlying hardware's MSR (i.e., features which

	 * can be supported) and the list of features we want to expose -

	 * because they are known to be properly supported in our code.

	 * Also, usually, the low half of the MSRs (bits which must be 1) can

	 * be set to 0, meaning that L1 may turn off any of these bits. The

	 * reason is that if one of these bits is necessary, it will appear

	 * in vmcs01 and prepare_vmcs02, when it bitwise-or's the control

	 * fields of vmcs01 and vmcs02, will turn these bits off - and

	 * nested_vmx_l1_wants_exit() will not pass related exits to L1.

	 * These rules have exceptions below.

 pin-based controls */

 exit controls */

 We support free control of debug control saving. */

 entry controls */

 We support free control of debug control loading. */

 cpu-based controls */

	/*

	 * We can allow some features even when not supported by the

	 * hardware. For example, L1 can specify an MSR bitmap - and we

	 * can use it to avoid exits to L1 - even when L0 runs L2

	 * without MSR bitmaps.

 We support free control of CR3 access interception. */

	/*

	 * secondary cpu-based controls.  Do not include those that

	 * depend on CPUID bits, they are added later by

	 * vmx_vcpu_after_set_cpuid.

	/*

	 * We can emulate "VMCS shadowing," even if the hardware

	 * doesn't support it.

 nested EPT: emulate EPT also to L1 */

		/*

		 * Advertise EPTP switching unconditionally

		 * since we emulate it

	/*

	 * Old versions of KVM use the single-context version without

	 * checking for support, so declare that it is supported even

	 * though it is treated as global context.  The alternative is

	 * not failing the single-context invvpid, and it is worse.

 miscellaneous data */

	/*

	 * This MSR reports some information about VMX support. We

	 * should return information about the VMX we emulate for the

	 * guest, and the VMCS structure we give it - not about the

	 * VMX support of the underlying hardware.

	/*

	 * These MSRs specify bits which the guest must keep fixed on

	 * while L1 is in VMXON mode (in L1's root mode, or running an L2).

	 * We picked the standard core2 setting.

 These MSRs specify bits which the guest must keep fixed off. */

			/*

			 * The vmx_bitmap is not tied to a VM and so should

			 * not be charged to a memcg.

